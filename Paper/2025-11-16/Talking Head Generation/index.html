<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Talking Head Generation">
    <meta name="description" content="Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  Reinforcing Trustworthiness in Multimodal Emotional Support Systems">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Talking Head Generation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-8e3adb44bf8ed190c0ffe932bf3bbb6d')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Talking Head Generation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                <span class="chip bg-color">Talking Head Generation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                Talking Head Generation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-16-æ›´æ–°"><a href="#2025-11-16-æ›´æ–°" class="headerlink" title="2025-11-16 æ›´æ–°"></a>2025-11-16 æ›´æ–°</h1><h2 id="Reinforcing-Trustworthiness-in-Multimodal-Emotional-Support-Systems"><a href="#Reinforcing-Trustworthiness-in-Multimodal-Emotional-Support-Systems" class="headerlink" title="Reinforcing Trustworthiness in Multimodal Emotional Support Systems"></a>Reinforcing Trustworthiness in Multimodal Emotional Support Systems</h2><p><strong>Authors:Huy M. Le, Dat Tien Nguyen, Ngan T. T. Vo, Tuan D. Q. Nguyen, Nguyen Binh Le, Duy Minh Ho Nguyen, Daniel Sonntag, Lizi Liao, Binh T. Nguyen</strong></p>
<p>In todayâ€™s world, emotional support is increasingly essential, yet it remains challenging for both those seeking help and those offering it. Multimodal approaches to emotional support show great promise by integrating diverse data sources to provide empathetic, contextually relevant responses, fostering more effective interactions. However, current methods have notable limitations, often relying solely on text or converting other data types into text, or providing emotion recognition only, thus overlooking the full potential of multimodal inputs. Moreover, many studies prioritize response generation without accurately identifying critical emotional support elements or ensuring the reliability of outputs. To overcome these issues, we introduce \textsc{ MultiMood}, a new framework that (i) leverages multimodal embeddings from video, audio, and text to predict emotional components and to produce responses responses aligned with professional therapeutic standards. To improve trustworthiness, we (ii) incorporate novel psychological criteria and apply Reinforcement Learning (RL) to optimize large language models (LLMs) for consistent adherence to these standards. We also (iii) analyze several advanced LLMs to assess their multimodal emotional support capabilities. Experimental results show that MultiMood achieves state-of-the-art on MESC and DFEW datasets while RL-driven trustworthiness improvements are validated through human and LLM evaluations, demonstrating its superior capability in applying a multimodal framework in this domain.</p>
<blockquote>
<p>åœ¨å¦‚ä»Šçš„ä¸–ç•Œä¸­ï¼Œæƒ…æ„Ÿæ”¯æŒå˜å¾—æ„ˆå‘é‡è¦ï¼Œä½†å¯¹äºå¯»æ±‚å¸®åŠ©å’Œæä¾›å¸®åŠ©çš„äººæ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æƒ…æ„Ÿæ”¯æŒçš„å¤šæ¨¡å¼æ–¹æ³•æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œå®ƒé€šè¿‡æ•´åˆå„ç§æ•°æ®æºæ¥æä¾›å¯Œæœ‰åŒæƒ…å¿ƒå’Œè¯­å¢ƒç›¸å…³çš„å›åº”ï¼Œä¿ƒè¿›æ›´æœ‰æ•ˆçš„äº’åŠ¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•å­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ï¼Œé€šå¸¸ä»…ä¾èµ–æ–‡æœ¬æˆ–å°†å…¶ä»–æ•°æ®ç±»å‹è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œæˆ–è€…ä»…æä¾›æƒ…ç»ªè¯†åˆ«ï¼Œä»è€Œå¿½ç•¥äº†å¤šæ¨¡å¼è¾“å…¥çš„å…¨é¢æ½œåŠ›ã€‚æ­¤å¤–ï¼Œè®¸å¤šç ”ç©¶ä¼˜å…ˆäºå“åº”ç”Ÿæˆï¼Œæœªèƒ½å‡†ç¡®è¯†åˆ«å…³é”®çš„æƒ…æ„Ÿæ”¯æŒå…ƒç´ æˆ–ç¡®ä¿è¾“å‡ºçš„å¯é æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†\textbf{MultiMood}æ–°æ¡†æ¶ï¼Œå®ƒï¼ˆiï¼‰åˆ©ç”¨è§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬çš„è·¨æ¨¡å¼åµŒå…¥æ¥é¢„æµ‹æƒ…æ„Ÿæˆåˆ†ï¼Œå¹¶äº§ç”Ÿä¸ä¸“ä¸šæ²»ç–—æ ‡å‡†ç›¸ç¬¦çš„å“åº”ã€‚ä¸ºäº†æé«˜å¯ä¿¡åº¦ï¼Œæˆ‘ä»¬ï¼ˆiiï¼‰èå…¥æ–°é¢–çš„å¿ƒç†æ ‡å‡†å¹¶åº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä½¿å…¶å§‹ç»ˆç¬¦åˆè¿™äº›æ ‡å‡†ã€‚æˆ‘ä»¬è¿˜ï¼ˆiiiï¼‰åˆ†æäº†å‡ ç§å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥è¯„ä¼°å…¶åœ¨å¤šæ¨¡å¼æƒ…æ„Ÿæ”¯æŒæ–¹é¢çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMultiMoodåœ¨MESCå’ŒDFEWæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œé€šè¿‡äººç±»å’Œå¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°éªŒè¯äº†å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„å¯ä¿¡æ€§æ”¹è¿›ï¼Œè¯æ˜äº†å…¶åœ¨è¯¥é¢†åŸŸåº”ç”¨å¤šæ¨¡å¼æ¡†æ¶çš„å“è¶Šèƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10011v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>å½“å‰æƒ…æ„Ÿæ”¯æŒçš„éœ€æ±‚æ—¥ç›Šå¢é•¿ï¼Œä½†å¯»æ±‚å¸®åŠ©å’Œæä¾›å¸®åŠ©çš„äººéƒ½é¢ä¸´æŒ‘æˆ˜ã€‚å¤šæ¨¡å¼æƒ…æ„Ÿæ”¯æŒæ–¹æ³•é€šè¿‡æ•´åˆå„ç§æ•°æ®æºæ¥æä¾›å¯Œæœ‰åŒæƒ…å¿ƒå’Œæƒ…å¢ƒç›¸å…³çš„å›åº”ï¼Œå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨è¯¸å¤šå±€é™ï¼Œå¦‚è¿‡äºä¾èµ–æ–‡æœ¬æˆ–è½¬æ¢å…¶ä»–æ•°æ®ç±»å‹ä¸ºæ–‡æœ¬ï¼Œæˆ–ä»…æä¾›æƒ…ç»ªè¯†åˆ«ï¼Œå¿½ç•¥äº†å¤šæ¨¡å¼è¾“å…¥çš„å…¨é¢æ½œåŠ›ã€‚ä¸ºå…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºMultiMoodæ¡†æ¶ï¼Œåˆ©ç”¨è§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬çš„å¤šå…ƒåµŒå…¥æ¥é¢„æµ‹æƒ…ç»ªæˆåˆ†å¹¶ç”Ÿæˆä¸ä¸“ä¸šæ²»ç–—æ ‡å‡†å¯¹é½çš„å›åº”ã€‚ä¸ºæé«˜å¯ä¿¡åº¦ï¼Œæˆ‘ä»¬èå…¥æ–°çš„å¿ƒç†æ ‡å‡†å¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç¡®ä¿å…¶æŒç»­ç¬¦åˆæ ‡å‡†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMultiMoodåœ¨MESCå’ŒDFEWæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼ŒåŒæ—¶RLé©±åŠ¨çš„ä¿¡ä»»åº¦æ”¹è¿›é€šè¿‡äººç±»å’ŒLLMè¯„ä¼°å¾—åˆ°éªŒè¯ï¼Œè¯æ˜å…¶åœ¨è¯¥é¢†åŸŸåº”ç”¨å¤šæ¨¡å¼æ¡†æ¶çš„å“è¶Šèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å½“å‰æƒ…æ„Ÿæ”¯æŒçš„é‡è¦æ€§åŠå…¶æŒ‘æˆ˜ã€‚</li>
<li>å¤šæ¨¡å¼æƒ…æ„Ÿæ”¯æŒæ–¹æ³•é€šè¿‡æ•´åˆä¸åŒæ•°æ®æºå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•çš„å±€é™åœ¨äºè¿‡äºä¾èµ–æ–‡æœ¬æˆ–ä»…æä¾›æƒ…ç»ªè¯†åˆ«ï¼Œå¿½ç•¥äº†å¤šæ¨¡å¼æ•°æ®çš„å…¨é¢æ½œåŠ›ã€‚</li>
<li>MultiMoodæ¡†æ¶åˆ©ç”¨è§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬çš„å¤šå…ƒåµŒå…¥æ¥é¢„æµ‹æƒ…ç»ªæˆåˆ†ã€‚</li>
<li>MultiMoodæ¡†æ¶ç”Ÿæˆä¸ä¸“ä¸šæ²»ç–—æ ‡å‡†å¯¹é½çš„å›åº”ã€‚</li>
<li>é€šè¿‡èå…¥æ–°çš„å¿ƒç†æ ‡å‡†å’Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæé«˜MultiMoodæ¡†æ¶çš„å¯ä¿¡åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10011">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-981a56c0085f01c82780cc4eee11405b" align="middle">
<img src="https://picx.zhimg.com/v2-fa1b85e6bd0e01fde72e10565094b7bc" align="middle">
<img src="https://picx.zhimg.com/v2-9093b195770eee5e6b22cd46940dcf9b" align="middle">
<img src="https://picx.zhimg.com/v2-0c2703c6b802b1193664bc8caa69159e" align="middle">
<img src="https://picx.zhimg.com/v2-12611d4f7057fa539d97e15f3af67f40" align="middle">
<img src="https://picx.zhimg.com/v2-2dba4d36757c09dea058137fda73cda9" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HI-TransPA-Hearing-Impairments-Translation-Personal-Assistant"><a href="#HI-TransPA-Hearing-Impairments-Translation-Personal-Assistant" class="headerlink" title="HI-TransPA: Hearing Impairments Translation Personal Assistant"></a>HI-TransPA: Hearing Impairments Translation Personal Assistant</h2><p><strong>Authors:Zhiming Ma, Shiyu Gan, Junhao Zhao, Xianming Li, Qingyun Pan, Peidong Wang, Mingjun Pan, Yuhao Mo, Jiajie Cheng, Chengxin Chen, Zhonglun Cao, Chonghan Liu, Shi Cheng</strong></p>
<p>To provide a unified and flexible solution for daily communication among hearing-impaired individuals, we introduce the Omni-Model paradigm into assistive technology and present HI-TransPA, an instruction-driven audio-visual personal assistant. The model fuses indistinct speech with high-frame-rate lip dynamics, enabling both translation and dialogue within a single multimodal framework. To tackle the challenges of noisy and heterogeneous raw data and the limited adaptability of existing Omni-Models to hearing-impaired speech, we construct a comprehensive preprocessing and curation pipeline that detects facial landmarks, isolates and stabilizes the lip region, and quantitatively assesses multimodal sample quality. These quality scores guide a curriculum learning strategy that first trains on clean, high-confidence samples and progressively incorporates harder cases to strengthen model robustness. We further adopt a SigLIP encoder combined with a Unified 3D-Resampler to efficiently encode high-frame-rate lip motion. Experiments on our purpose-built HI-Dialogue dataset show that HI-TransPA achieves state-of-the-art performance in both literal accuracy and semantic fidelity. This work establishes a foundation for applying Omni-Models to assistive communication technology, providing an end-to-end modeling framework and essential processing tools for future research.</p>
<blockquote>
<p>ä¸ºäº†ä¸ºå¬åŠ›å—æŸäººå£«çš„æ—¥å¸¸æ²Ÿé€šæä¾›ç»Ÿä¸€å’Œçµæ´»è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬å°†Omni-ModelèŒƒå¼å¼•å…¥è¾…åŠ©æŠ€æœ¯ï¼Œå¹¶æ¨å‡ºäº†HI-TransPAè¿™ä¸€æŒ‡ä»¤é©±åŠ¨çš„è§†å¬ä¸ªäººåŠ©ç†ã€‚è¯¥æ¨¡å‹èåˆäº†æ¨¡ç³Šè¯­éŸ³å’Œé«˜å¸§ç‡å”‡åŠ¨æ€ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€çš„å¤šæ¨¡å¼æ¡†æ¶å†…å®ç°ç¿»è¯‘å’Œå¯¹è¯ã€‚ä¸ºäº†åº”å¯¹åŸå§‹æ•°æ®çš„å˜ˆæ‚å’Œå¼‚è´¨æ€§ä»¥åŠç°æœ‰Omni-Modelså¯¹å¬åŠ›å—æŸè¯­éŸ³çš„æœ‰é™é€‚åº”æ€§ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå…¨é¢çš„é¢„å¤„ç†å’Œç­›é€‰ç®¡é“ï¼Œç”¨äºæ£€æµ‹é¢éƒ¨åœ°æ ‡ã€éš”ç¦»å’Œç¨³å®šå”‡éƒ¨åŒºåŸŸï¼Œå¹¶å®šé‡è¯„ä¼°å¤šæ¨¡å¼æ ·æœ¬è´¨é‡ã€‚è¿™äº›è´¨é‡åˆ†æ•°å¼•å¯¼äº†ä¸€ç§è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é¦–å…ˆä»¥å¹²å‡€ã€é«˜ä¿¡å¿ƒçš„æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œå¹¶é€æ­¥åŠ å…¥æ›´å›°éš¾çš„æ¡ˆä¾‹ä»¥å¢å¼ºæ¨¡å‹çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é‡‡ç”¨SigLIPç¼–ç å™¨ç»“åˆUnified 3D-Resampleré«˜æ•ˆç¼–ç é«˜å¸§ç‡å”‡åŠ¨ã€‚åœ¨æˆ‘ä»¬ä¸“é—¨æ„å»ºçš„HI-Dialogueæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒHI-TransPAåœ¨å­—é¢å‡†ç¡®ç‡å’Œè¯­ä¹‰ä¿çœŸåº¦æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚è¿™é¡¹å·¥ä½œä¸ºå°†Omni-Modelsåº”ç”¨äºè¾…åŠ©é€šä¿¡æŠ€æœ¯å¥ å®šäº†åŸºç¡€ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†ç«¯åˆ°ç«¯çš„å»ºæ¨¡æ¡†æ¶å’Œå¿…è¦çš„å¤„ç†å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09915v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§èåˆäº†è§†å¬ä¿¡æ¯çš„å¤šæ¨¡æ€äº¤æµåŠ©æ‰‹HI-TransPAé—®ä¸–ï¼Œä¸“é—¨ä¸ºå¬éšœäººç¾¤æä¾›æ—¥å¸¸äº¤æµçš„è§£å†³æ–¹æ¡ˆã€‚å®ƒé€šè¿‡æ•æ‰ä¸æ˜“è¾¨è¯†çš„è¯­éŸ³ä¿¡å·ä¸å˜´å”‡åŠ¨ä½œçš„é«˜å¸§ç‡æ•°æ®ï¼Œç»“åˆé¢éƒ¨å®šä½å’Œæ·±åº¦å­¦ä¹ ç®—æ³•è¿›è¡Œè¯­éŸ³è¯†åˆ«ä¸å¯¹è¯ï¼Œå½¢æˆå•æ¨¡æ€å’Œå¤šæ¨¡æ€ç»¼åˆå¯¹è¯çš„æ–°æ¨¡å¼ã€‚æœ¬æ–‡è¯¦ç»†æè¿°äº†ä¸ºè§£å†³è¯¥åº”ç”¨è¿‡ç¨‹ä¸­çš„æŒ‘æˆ˜å¦‚å¤„ç†åŸå§‹æ•°æ®çš„å™ªå£°å’Œå¤šæ ·æ€§é—®é¢˜ã€Omniæ¨¡å‹å¯¹å¬éšœäººç¾¤çš„é€‚åº”æ€§é™åˆ¶ç­‰æ‰€æ„å»ºçš„é¢„å¤„ç†å’Œç­›é€‰ç®¡é“ï¼ŒåŒ…æ‹¬é¢éƒ¨åœ°æ ‡æ£€æµ‹ã€å”‡éƒ¨åŒºåŸŸéš”ç¦»å’Œç¨³å®šç­‰å…³é”®æ­¥éª¤ã€‚é‡‡ç”¨åŸºäºSigLIPç¼–ç å™¨å’Œç»Ÿä¸€ä¸‰ç»´é‡é‡‡æ ·æŠ€æœ¯çš„ç­–ç•¥æœ‰æ•ˆæé«˜äº†é«˜å¸§ç‡å”‡éƒ¨è¿åŠ¨çš„ç¼–ç æ•ˆç‡ã€‚åœ¨å®šåˆ¶åŒ–çš„HI-Dialogueæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜HI-TransPAåœ¨å‡†ç¡®åº¦å’Œè¯­ä¹‰ä¿çœŸåº¦ä¸Šè¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ã€‚æ­¤ç ”ç©¶ä¸ºOmniæ¨¡å‹åœ¨è¾…åŠ©é€šè®¯æŠ€æœ¯ä¸­çš„åº”ç”¨å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HI-TransPAç»“åˆäº†è§†å¬ä¿¡æ¯ï¼Œä¸ºå¬éšœäººç¾¤æä¾›æ—¥å¸¸äº¤æµè§£å†³æ–¹æ¡ˆã€‚</li>
<li>HI-TransPAèåˆè¯­éŸ³ä¿¡å·å’Œå˜´å”‡åŠ¨ä½œæ•°æ®ä»¥æ”¯æŒå¯¹è¯ç¿»è¯‘å’Œå¯¹è¯­éŸ³çš„å¤„ç†åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09915">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-17f6d527a09b05f27d65ecd776cac619" align="middle">
<img src="https://picx.zhimg.com/v2-7bdbafb5b6cb0deae546215572971c4e" align="middle">
<img src="https://picx.zhimg.com/v2-39b1e5664eab3cd18ce0474fdfe71e3d" align="middle">
<img src="https://picx.zhimg.com/v2-dc926741fa46e4bd0b6db86f7281859d" align="middle">
<img src="https://picx.zhimg.com/v2-1bfa9b94382767ca6363eab686894e94" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Fairness-Aware-Few-Shot-Learning-for-Audio-Visual-Stress-Detection"><a href="#Fairness-Aware-Few-Shot-Learning-for-Audio-Visual-Stress-Detection" class="headerlink" title="Fairness-Aware Few-Shot Learning for Audio-Visual Stress Detection"></a>Fairness-Aware Few-Shot Learning for Audio-Visual Stress Detection</h2><p><strong>Authors:Anushka Sanjay Shelke, Aditya Sneh, Arya Adyasha, Haroon R. Lone</strong></p>
<p>Fairness in AI-driven stress detection is critical for equitable mental healthcare, yet existing models frequently exhibit gender bias, particularly in data-scarce scenarios. To address this, we propose FairM2S, a fairness-aware meta-learning framework for stress detection leveraging audio-visual data. FairM2S integrates Equalized Odds constraints during both meta-training and adaptation phases, employing adversarial gradient masking and fairness-constrained meta-updates to effectively mitigate bias. Evaluated against five state-of-the-art baselines, FairM2S achieves 78.1% accuracy while reducing the Equal Opportunity to 0.06, demonstrating substantial fairness gains. We also release SAVSD, a smartphone-captured dataset with gender annotations, designed to support fairness research in low-resource, real-world contexts. Together, these contributions position FairM2S as a state-of-the-art approach for equitable and scalable few-shot stress detection in mental health AI. We release our dataset and FairM2S publicly with this paper.</p>
<blockquote>
<p>äººå·¥æ™ºèƒ½é©±åŠ¨çš„åº”æ¿€æ£€æµ‹ä¸­çš„å…¬å¹³æ€§å¯¹äºå…¬å¹³çš„å¿ƒç†å¥åº·æŠ¤ç†è‡³å…³é‡è¦ï¼Œç„¶è€Œç°æœ‰çš„æ¨¡å‹ç»å¸¸è¡¨ç°å‡ºæ€§åˆ«åè§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FairM2Sï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨è§†å¬æ•°æ®çš„åº”æ¿€æ£€æµ‹å…¬å¹³æ€§æ„ŸçŸ¥å…ƒå­¦ä¹ æ¡†æ¶ã€‚FairM2Såœ¨å…ƒè®­ç»ƒå’Œé€‚åº”é˜¶æ®µéƒ½æ•´åˆäº†å‡è¡¡æœºä¼šçº¦æŸï¼Œé‡‡ç”¨å¯¹æŠ—æ€§æ¢¯åº¦æ©è”½å’Œå…¬å¹³æ€§çº¦æŸå…ƒæ›´æ–°æ¥æœ‰æ•ˆç¼“è§£åè§ã€‚ä¸äº”ä¸ªæœ€å…ˆè¿›çš„åŸºçº¿ç›¸æ¯”ï¼ŒFairM2Sè¾¾åˆ°äº†78.1%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶å°†å¹³ç­‰æœºä¼šé™è‡³0.06ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å…¬å¹³æ€§æå‡ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†SAVSDï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¦æœ‰æ€§åˆ«æ³¨é‡Šçš„æ™ºèƒ½æ‰‹æœºæ•æ‰æ•°æ®é›†ï¼Œæ—¨åœ¨æ”¯æŒä½èµ„æºã€ç°å®ä¸–ç•Œçš„å…¬å¹³æ€§ç ”ç©¶ã€‚FairM2Sçš„è¿™äº›è´¡çŒ®ä½¿å…¶æˆä¸ºå¿ƒç†å¥åº·äººå·¥æ™ºèƒ½é¢†åŸŸä¸­å…ˆè¿›ã€å…¬å¹³ä¸”å¯æ‰©å±•çš„å°‘æ•°é•œå¤´åº”æ¿€æ£€æµ‹çš„é¦–é€‰æ–¹æ³•ã€‚æˆ‘ä»¬éšè®ºæ–‡å…¬å¼€å‘å¸ƒæˆ‘ä»¬çš„æ•°æ®é›†å’ŒFairM2Sã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09039v1">PDF</a> </p>
<p><strong>Summary</strong><br>äººå·¥æ™ºèƒ½é©±åŠ¨çš„å…¬å¹³å‹åŠ›æ£€æµ‹å¯¹å…¬å¹³ç²¾ç¥å«ç”Ÿä¿å¥è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ¨¡å‹åœ¨æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸­ç»å¸¸å‡ºç°æ€§åˆ«åè§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FairM2Sï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨è§†å¬æ•°æ®çš„å‹åŠ›æ£€æµ‹å…¬å¹³æ€§æ„ŸçŸ¥å…ƒå­¦ä¹ æ¡†æ¶ã€‚FairM2Såœ¨å…ƒè®­ç»ƒå’Œé€‚åº”é˜¶æ®µéƒ½æ•´åˆäº†å‡è¡¡æœºä¼šçº¦æŸï¼Œé‡‡ç”¨å¯¹æŠ—æ€§æ¢¯åº¦æ©è”½å’Œå…¬å¹³æ€§çº¦æŸå…ƒæ›´æ–°æ¥æœ‰æ•ˆç¼“è§£åè§ã€‚ä¸äº”ç§æœ€æ–°æŠ€æœ¯åŸºçº¿ç›¸æ¯”ï¼ŒFairM2Så®ç°äº†78.1%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶å°†å…¬å¹³æœºä¼šé™è‡³0.06ï¼Œæ˜¾ç¤ºå‡ºå®è´¨æ€§çš„å…¬å¹³æ€§è¿›å±•ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†SAVSDæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†é‡‡ç”¨æ™ºèƒ½æ‰‹æœºæ•è·å¹¶å¸¦æœ‰æ€§åˆ«æ³¨é‡Šï¼Œæ—¨åœ¨æ”¯æŒä½èµ„æºã€ç°å®ç¯å¢ƒä¸­çš„å…¬å¹³æ€§ç ”ç©¶ã€‚æ€»ä½“è€Œè¨€ï¼ŒFairM2Sè¢«è®¤ä¸ºæ˜¯å¿ƒç†å¥åº·äººå·¥æ™ºèƒ½é¢†åŸŸä¸­å…¬å¹³ä¸”å¯æ‰©å±•çš„å°‘æ•°å‹åŠ›æ£€æµ‹çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†æ•°æ®é›†å’ŒFairM2Sã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½åœ¨å‹åŠ›æ£€æµ‹ä¸­çš„å…¬å¹³æ€§å¯¹ç²¾ç¥å«ç”Ÿä¿å¥çš„å…¬å¹³æ€§è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨æ•°æ®ç¨€ç¼ºçš„åœºæ™¯ä¸­å­˜åœ¨æ€§åˆ«åè§é—®é¢˜ã€‚</li>
<li>FairM2Sæ˜¯ä¸€ä¸ªåˆ©ç”¨è§†å¬æ•°æ®çš„å‹åŠ›æ£€æµ‹å…¬å¹³æ€§æ„ŸçŸ¥å…ƒå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>FairM2Sé€šè¿‡æ•´åˆå‡è¡¡æœºä¼šçº¦æŸåœ¨å…ƒè®­ç»ƒå’Œé€‚åº”é˜¶æ®µå®ç°å…¬å¹³æ€§ã€‚</li>
<li>FairM2Så–å¾—äº†è¾ƒé«˜çš„å‡†ç¡®ç‡ï¼Œå¹¶æ˜¾è‘—é™ä½äº†å…¬å¹³æœºä¼šæŒ‡æ•°ã€‚</li>
<li>å‘å¸ƒäº†SAVSDæ•°æ®é›†ï¼Œç”¨äºæ”¯æŒä½èµ„æºã€ç°å®ç¯å¢ƒä¸­çš„å…¬å¹³æ€§ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ec14e903eebb1db2e1eb51ac802b30f" align="middle">
<img src="https://picx.zhimg.com/v2-880245b64d2b6a8645b848fc7fc8622e" align="middle">
<img src="https://picx.zhimg.com/v2-31bafba33ab8a0d56e0601db0c2721fe" align="middle">
<img src="https://picx.zhimg.com/v2-11e045d9059fb05d113f9206da276661" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TiDAR-Think-in-Diffusion-Talk-in-Autoregression"><a href="#TiDAR-Think-in-Diffusion-Talk-in-Autoregression" class="headerlink" title="TiDAR: Think in Diffusion, Talk in Autoregression"></a>TiDAR: Think in Diffusion, Talk in Autoregression</h2><p><strong>Authors:Jingyu Liu, Xin Dong, Zhifan Ye, Rishabh Mehta, Yonggan Fu, Vartika Singh, Jan Kautz, Ce Zhang, Pavlo Molchanov</strong></p>
<p>Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can we achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. We introduce TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. We extensively evaluate TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second.</p>
<blockquote>
<p>æ‰©æ•£è¯­è¨€æ¨¡å‹å…·æœ‰å¿«é€Ÿå¹¶è¡Œç”Ÿæˆçš„æ½œåŠ›ï¼Œè€Œè‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹é€šå¸¸å› å…¶è‡ªç„¶ç¬¦åˆè¯­è¨€å»ºæ¨¡çš„å› æœç»“æ„è€Œåœ¨è´¨é‡ä¸Šè¡¨ç°å‡ºè‰²ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªæ ¹æœ¬æ€§çš„é—®é¢˜ï¼šæˆ‘ä»¬èƒ½å¦å®ç°é«˜ååé‡ã€æ›´é«˜çš„GPUåˆ©ç”¨ç‡å’ŒARçº§è´¨é‡çš„ååŒï¼Ÿç°æœ‰æ–¹æ³•æ— æ³•æœ‰æ•ˆåœ°å¹³è¡¡è¿™ä¸¤æ–¹é¢ï¼Œè¦ä¹ˆä½¿ç”¨è¾ƒå¼±çš„æ¨¡å‹è¿›è¡Œé¡ºåºèµ·è‰ï¼ˆæ¨æµ‹è§£ç ï¼‰ï¼Œä¼˜å…ˆè€ƒè™‘ARï¼Œå¯¼è‡´èµ·è‰æ•ˆç‡é™ä½ï¼Œè¦ä¹ˆé‡‡ç”¨æŸç§å½¢å¼çš„è‡ªå·¦è‡³å³ï¼ˆARç±»ä¼¼ï¼‰è§£ç é€»è¾‘è¿›è¡Œæ‰©æ•£ï¼Œè¿™ä»ç„¶ä¼šå¯¼è‡´è´¨é‡ä¸‹é™å¹¶ä¸§å¤±äº†å…¶æ½œåœ¨çš„å¹¶è¡Œæ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†TiDARï¼Œè¿™æ˜¯ä¸€ä¸ªåºåˆ—çº§æ··åˆæ¶æ„ï¼Œåœ¨æ‰©æ•£ä¸­è¿›è¡Œç¬¦å·ï¼ˆæ€è€ƒï¼‰èµ·è‰ï¼Œå¹¶ä»¥è‡ªå›å½’æ–¹å¼ï¼ˆè¯´è¯ï¼‰é‡‡æ ·æœ€ç»ˆè¾“å‡ºâ€”â€”æ‰€æœ‰è¿™äº›éƒ½ä½¿ç”¨ä¸“é—¨è®¾è®¡çš„ç»“æ„åŒ–æ³¨æ„åŠ›æ©ç åœ¨ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­å®Œæˆã€‚è¿™ç§è®¾è®¡åˆ©ç”¨äº†å…è´¹çš„GPUè®¡ç®—å¯†åº¦ï¼Œåœ¨èµ·è‰å’ŒéªŒè¯å®¹é‡ä¹‹é—´å®ç°äº†å¼ºå¤§çš„å¹³è¡¡ã€‚æ­¤å¤–ï¼ŒTiDARè¢«è®¾è®¡ä¸ºä½œä¸ºç‹¬ç«‹æ¨¡å‹å‹å¥½æœåŠ¡ï¼ˆä½å¼€é”€ï¼‰ã€‚æˆ‘ä»¬å¯¹TiDARè¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œä¸ARæ¨¡å‹ã€æ¨æµ‹è§£ç ä»¥åŠæ‰©æ•£å˜ä½“åœ¨ç”Ÿæˆå’Œå¯èƒ½æ€§ä»»åŠ¡ä¸Šçš„1.5Bå’Œ8Bè§„æ¨¡è¿›è¡Œäº†æ¯”è¾ƒã€‚ç”±äºå¹¶è¡Œèµ·è‰å’Œé‡‡æ ·ä»¥åŠç²¾ç¡®çš„KVç¼“å­˜æ”¯æŒï¼ŒTiDARåœ¨è¡¡é‡ååé‡ä¸Šä¼˜äºæ¨æµ‹è§£ç ï¼Œå¹¶ä¸”åœ¨æ•ˆç‡å’Œè´¨é‡ä¸Šè¶…è¿‡äº†å¦‚Dreamå’ŒLladaç­‰æ‰©æ•£æ¨¡å‹ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒTiDARæ˜¯é¦–ä¸ªåœ¨è´¨é‡ä¸Šç¼©å°ä¸ARæ¨¡å‹å·®è·çš„æ¶æ„ï¼ŒåŒæ—¶æ¯ç§’å¤„ç†ä»¤ç‰Œçš„æ•°é‡æé«˜äº†4.71å€è‡³5.91å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08923v1">PDF</a> NVIDIA-Tech Report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸è‡ªå›å½’æ¨¡å‹çš„èåˆé—®é¢˜ï¼Œæ—¨åœ¨å®ç°å¿«é€Ÿå¹¶è¡Œç”Ÿæˆä¸é«˜è´¨è¾“å‡ºã€‚ä¸ºæ­¤ï¼Œæå‡ºä¸€ç§åä¸ºTiDARçš„åºåˆ—çº§æ··åˆæ¶æ„ï¼Œé€šè¿‡ä¸€æ¬¡å‰å‘ä¼ é€’å®ç°æ‰©æ•£ä¸­çš„ä»¤ç‰Œè‰ç¨¿å’Œæœ€ç»ˆè¾“å‡ºçš„è‡ªå›å½’é‡‡æ ·ã€‚è¯¥è®¾è®¡åˆ©ç”¨å…è´¹GPUè®¡ç®—å¯†åº¦ï¼Œåœ¨è‰ç¨¿å’ŒéªŒè¯å®¹é‡ä¹‹é—´å®ç°å¹³è¡¡ã€‚ç»è¿‡è¯„ä¼°ï¼ŒTiDARåœ¨ç”Ÿæˆå’Œå¯èƒ½æ€§ä»»åŠ¡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä»¥å¹¶è¡Œè‰ç¨¿å’Œé‡‡æ ·ä»¥åŠç²¾ç¡®KVç¼“å­˜æ”¯æŒä¸ºç‰¹ç‚¹ï¼Œåœ¨ååé‡å’Œæ•ˆç‡æ–¹é¢ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯TiDARé¦–æ¬¡ç¼©å°äº†ä¸è‡ªå›å½’æ¨¡å‹çš„å“è´¨å·®è·ï¼ŒåŒæ—¶æé«˜äº†æ¯ç§’ä»¤ç‰Œç”Ÿæˆé€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸è‡ªå›å½’æ¨¡å‹çš„èåˆæ—¨åœ¨å®ç°å¿«é€Ÿå¹¶è¡Œç”Ÿæˆä¸é«˜è´¨è¾“å‡ºã€‚</li>
<li>TiDARæ˜¯ä¸€ç§åºåˆ—çº§æ··åˆæ¶æ„ï¼Œç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’é‡‡æ ·çš„ä¼˜ç‚¹ã€‚</li>
<li>TiDARé€šè¿‡ä¸€æ¬¡å‰å‘ä¼ é€’å®ç°ä»¤ç‰Œè‰ç¨¿å’Œæœ€ç»ˆè¾“å‡ºçš„è‡ªå›å½’é‡‡æ ·ã€‚</li>
<li>TiDARåˆ©ç”¨GPUè®¡ç®—èµ„æºï¼Œåœ¨è‰ç¨¿å’ŒéªŒè¯å®¹é‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚</li>
<li>TiDARåœ¨ç”Ÿæˆå’Œå¯èƒ½æ€§ä»»åŠ¡æ–¹é¢è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨ååé‡å’Œæ•ˆç‡æ–¹é¢ã€‚</li>
<li>TiDARç¼©å°äº†ä¸è‡ªå›å½’æ¨¡å‹çš„å“è´¨å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08923">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e72cf6555fad3dc5f6f48cd11c5f634" align="middle">
<img src="https://picx.zhimg.com/v2-3bac1fd6c53530efc2fe11e48d06522d" align="middle">
<img src="https://picx.zhimg.com/v2-112f52131edca348f8307c3523859223" align="middle">
<img src="https://picx.zhimg.com/v2-7d5fc72deb258c7c490dff90d0731015" align="middle">
<img src="https://picx.zhimg.com/v2-8c08b04b5451ef0fc6c52a7ae5aaf5a7" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Is-It-Truly-Necessary-to-Process-and-Fit-Minutes-Long-Reference-Videos-for-Personalized-Talking-Face-Generation"><a href="#Is-It-Truly-Necessary-to-Process-and-Fit-Minutes-Long-Reference-Videos-for-Personalized-Talking-Face-Generation" class="headerlink" title="Is It Truly Necessary to Process and Fit Minutes-Long Reference Videos for Personalized Talking Face Generation?"></a>Is It Truly Necessary to Process and Fit Minutes-Long Reference Videos for Personalized Talking Face Generation?</h2><p><strong>Authors:Rui-Qing Sun, Ang Li, Zhijing Wu, Tian Lan, Qianyu Lu, Xingshan Yao, Chen Xu, Xian-Ling Mao</strong></p>
<p>Talking Face Generation (TFG) aims to produce realistic and dynamic talking portraits, with broad applications in fields such as digital education, film and television production, e-commerce live streaming, and other related areas. Currently, TFG methods based on Neural Radiated Field (NeRF) or 3D Gaussian sputtering (3DGS) are received widespread attention. They learn and store personalized features from reference videos of each target individual to generate realistic speaking videos. To ensure models can capture sufficient 3D information and successfully learns the lip-audio mapping, previous studies usually require meticulous processing and fitting several minutes of reference video, which always takes hours. The computational burden of processing and fitting long reference videos severely limits the practical application value of these methods.However, is it really necessary to fit such minutes of reference video? Our exploratory case studies show that using some informative reference video segments of just a few seconds can achieve performance comparable to or even better than the full reference video. This indicates that video informative quality is much more important than its length. Inspired by this observation, we propose the ISExplore (short for Informative Segment Explore), a simple-yet-effective segment selection strategy that automatically identifies the informative 5-second reference video segment based on three key data quality dimensions: audio feature diversity, lip movement amplitude, and number of camera views. Extensive experiments demonstrate that our approach increases data processing and training speed by more than 5x for NeRF and 3DGS methods, while maintaining high-fidelity output. Project resources are available at xx.</p>
<blockquote>
<p>é¢éƒ¨ç”Ÿæˆï¼ˆTFGï¼‰æ—¨åœ¨ç”Ÿæˆé€¼çœŸä¸”åŠ¨æ€çš„è¯´è¯è‚–åƒï¼Œåœ¨æ•°å­—æ•™è‚²ã€å½±è§†åˆ¶ä½œã€ç”µå•†ç›´æ’­ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚ç›®å‰ï¼ŒåŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æˆ–ä¸‰ç»´é«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰çš„TFGæ–¹æ³•å—åˆ°å¹¿æ³›å…³æ³¨ã€‚å®ƒä»¬ä»ç›®æ ‡ä¸ªä½“çš„å‚è€ƒè§†é¢‘ä¸­å­¦ä¹ å¹¶å­˜å‚¨ä¸ªæ€§åŒ–ç‰¹å¾ï¼Œä»¥ç”Ÿæˆé€¼çœŸçš„è¯´è¯è§†é¢‘ã€‚ä¸ºç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæ•è·è¶³å¤Ÿçš„ä¸‰ç»´ä¿¡æ¯å¹¶æˆåŠŸå­¦ä¹ å”‡éŸ³æ˜ å°„ï¼Œä¹‹å‰çš„ç ”ç©¶é€šå¸¸éœ€è¦ç²¾ç»†å¤„ç†å’Œæ‹Ÿåˆå‡ åˆ†é’Ÿçš„å‚è€ƒè§†é¢‘ï¼Œè¿™ä¸€è¿‡ç¨‹å¾€å¾€éœ€è¦æ•°å°æ—¶ã€‚å¤„ç†å’Œæ‹Ÿåˆé•¿å‚è€ƒè§†é¢‘çš„è®¡ç®—è´Ÿæ‹…ä¸¥é‡é™åˆ¶äº†è¿™äº›æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„ä»·å€¼ã€‚ä½†æ˜¯ï¼ŒçœŸçš„æœ‰å¿…è¦æ‹Ÿåˆè¿™ä¹ˆä¹…çš„å‚è€ƒè§†é¢‘å—ï¼Ÿæˆ‘ä»¬çš„æ¢ç´¢æ€§æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨å‡ ç§’çš„å‚è€ƒè§†é¢‘ç‰‡æ®µå°±èƒ½è¾¾åˆ°ä¸å®Œæ•´å‚è€ƒè§†é¢‘ç›¸å½“ç”šè‡³æ›´å¥½çš„æ€§èƒ½ï¼Œè¿™è¡¨æ˜è§†é¢‘çš„ä¿¡æ¯è´¨é‡è¿œæ¯”å…¶é•¿åº¦é‡è¦ã€‚å—æ­¤è§‚å¯Ÿå¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ISExploreï¼ˆä¿¡æ¯ç‰‡æ®µæ¢ç´¢çš„ç®€ç§°ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„ç‰‡æ®µé€‰æ‹©ç­–ç•¥ï¼Œå®ƒè‡ªåŠ¨ç¡®å®šä¿¡æ¯ä¸°å¯Œçš„5ç§’å‚è€ƒè§†é¢‘ç‰‡æ®µï¼ŒåŸºäºä¸‰ä¸ªå…³é”®æ•°æ®è´¨é‡ç»´åº¦ï¼šéŸ³é¢‘ç‰¹å¾å¤šæ ·æ€§ã€å˜´å”‡è¿åŠ¨å¹…åº¦å’Œæ‘„åƒå¤´æ•°é‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æé«˜NeRFå’Œ3DGSæ–¹æ³•çš„æ•°æ®å¤„ç†å’Œè®­ç»ƒé€Ÿåº¦çš„åŒæ—¶ï¼Œä¿æŒäº†é«˜ä¿çœŸè¾“å‡ºã€‚é¡¹ç›®èµ„æºå¯åœ¨xxå¤„è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07940v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æˆ–3Dé«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰çš„è¯´è¯é¢å­”ç”Ÿæˆï¼ˆTFGï¼‰æŠ€æœ¯æ­£å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œç”¨äºåˆ›å»ºé€¼çœŸçš„åŠ¨æ€è‚–åƒï¼Œå¹¿æ³›åº”ç”¨äºæ•°å­—æ•™è‚²ã€å½±è§†åˆ¶ä½œã€ç”µå•†ç›´æ’­ç­‰é¢†åŸŸã€‚ä¼ ç»Ÿæ–¹æ³•éœ€è¦å¤„ç†å¹¶é€‚é…å‡ åˆ†é’Ÿçš„å‚è€ƒè§†é¢‘ä»¥ç¡®ä¿æ•æ‰åˆ°è¶³å¤Ÿçš„3Dä¿¡æ¯å’Œå”‡éŸ³æ˜ å°„ï¼Œè¿™ä¸€è¿‡ç¨‹ååˆ†ç¹çå¹¶é™åˆ¶äº†å…¶å®é™…åº”ç”¨ä»·å€¼ã€‚æœ¬ç ”ç©¶å‘ç°ï¼Œä»…ä½¿ç”¨å‡ ç§’çš„å‚è€ƒè§†é¢‘ç‰‡æ®µä¾¿èƒ½å®ç°ä¸å®Œæ•´è§†é¢‘ç›¸è¿‘æˆ–æ›´ä½³çš„æ€§èƒ½ï¼Œçªæ˜¾è§†é¢‘ä¿¡æ¯è´¨é‡è¾ƒè§†é¢‘é•¿åº¦æ›´ä¸ºé‡è¦ã€‚å—æ­¤å¯å‘ï¼Œç ”ç©¶æå‡ºISExploreç­–ç•¥ï¼Œå¯è‡ªåŠ¨è¾¨è¯†æœ€å…·ä¿¡æ¯é‡çš„5ç§’å‚è€ƒè§†é¢‘ç‰‡æ®µï¼ŒåŸºäºéŸ³é¢‘ç‰¹å¾å¤šæ ·æ€§ã€å”‡éƒ¨åŠ¨ä½œå¹…åº¦å’Œæ‘„åƒå¤´è§†è§’æ•°é‡ä¸‰ä¸ªå…³é”®æ•°æ®è´¨é‡ç»´åº¦ã€‚å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å¯æå‡NeRFå’Œ3DGSæ–¹æ³•çš„æ•°æ®å¤„ç†é€Ÿåº¦å’Œè®­ç»ƒé€Ÿåº¦è‡³5å€ä»¥ä¸Šï¼ŒåŒæ—¶ä¿æŒé«˜ä¿çœŸè¾“å‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯´è¯é¢å­”ç”ŸæˆæŠ€æœ¯å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œåº”ç”¨äºå¤šä¸ªé¢†åŸŸã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•éœ€å¤„ç†é•¿æ—¶é—´å‚è€ƒè§†é¢‘ï¼Œå…·æœ‰è®¡ç®—è´Ÿæ‹…ã€‚</li>
<li>ç ”ç©¶å‘ç°ä½¿ç”¨å‡ ç§’çš„ä¿¡æ¯é‡å¤§çš„è§†é¢‘ç‰‡æ®µå¯è¾¾åˆ°ç”šè‡³è¶…è¶Šä½¿ç”¨å®Œæ•´è§†é¢‘çš„æ€§èƒ½ã€‚</li>
<li>è§†é¢‘ä¿¡æ¯è´¨é‡æ¯”é•¿åº¦æ›´é‡è¦ã€‚</li>
<li>ISExploreç­–ç•¥å¯è‡ªåŠ¨é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„5ç§’è§†é¢‘ç‰‡æ®µã€‚</li>
<li>ISExploreç­–ç•¥æå‡äº†æ•°æ®å¤„ç†å’Œè®­ç»ƒé€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07940">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-62a424c1a5296b7152788a243d566878" align="middle">
<img src="https://picx.zhimg.com/v2-da11e51d7c84f5733717486d55c20df4" align="middle">
<img src="https://picx.zhimg.com/v2-8dcca5f3a09cad2c41089af9d662173d" align="middle">
<img src="https://picx.zhimg.com/v2-fc3f9bcbefee193f1c713f94789218ea" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SpikCommander-A-High-performance-Spiking-Transformer-with-Multi-view-Learning-for-Efficient-Speech-Command-Recognition"><a href="#SpikCommander-A-High-performance-Spiking-Transformer-with-Multi-view-Learning-for-Efficient-Speech-Command-Recognition" class="headerlink" title="SpikCommander: A High-performance Spiking Transformer with Multi-view Learning for Efficient Speech Command Recognition"></a>SpikCommander: A High-performance Spiking Transformer with Multi-view Learning for Efficient Speech Command Recognition</h2><p><strong>Authors:Jiaqi Wang, Liutao Yu, Xiongri Shen, Sihang Guo, Chenlin Zhou, Leilei Zhao, Yi Zhong, Zhiguo Zhang, Zhengyu Ma</strong></p>
<p>Spiking neural networks (SNNs) offer a promising path toward energy-efficient speech command recognition (SCR) by leveraging their event-driven processing paradigm. However, existing SNN-based SCR methods often struggle to capture rich temporal dependencies and contextual information from speech due to limited temporal modeling and binary spike-based representations. To address these challenges, we first introduce the multi-view spiking temporal-aware self-attention (MSTASA) module, which combines effective spiking temporal-aware attention with a multi-view learning framework to model complementary temporal dependencies in speech commands. Building on MSTASA, we further propose SpikCommander, a fully spike-driven transformer architecture that integrates MSTASA with a spiking contextual refinement channel MLP (SCR-MLP) to jointly enhance temporal context modeling and channel-wise feature integration. We evaluate our method on three benchmark datasets: the Spiking Heidelberg Dataset (SHD), the Spiking Speech Commands (SSC), and the Google Speech Commands V2 (GSC). Extensive experiments demonstrate that SpikCommander consistently outperforms state-of-the-art (SOTA) SNN approaches with fewer parameters under comparable time steps, highlighting its effectiveness and efficiency for robust speech command recognition.</p>
<blockquote>
<p>è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSpiking Neural Networksï¼Œç®€ç§°SNNsï¼‰é€šè¿‡åˆ©ç”¨å…¶äº‹ä»¶é©±åŠ¨çš„å¤„ç†æ¨¡å¼ï¼Œä¸ºå®ç°èƒ½æºé«˜æ•ˆçš„è¯­éŸ³æŒ‡ä»¤è¯†åˆ«ï¼ˆSpeech Command Recognitionï¼Œç®€ç§°SCRï¼‰æä¾›äº†æœ‰å‰æ™¯çš„è·¯å¾„ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºSNNçš„SCRæ–¹æ³•åœ¨æ•æ‰è¯­éŸ³ä¸­çš„ä¸°å¯Œæ—¶é—´ä¾èµ–æ€§å’Œä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢å¸¸å¸¸é‡åˆ°å›°éš¾ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºæœ‰é™çš„æ—¶é—´å»ºæ¨¡å’ŒåŸºäºäºŒè¿›åˆ¶çš„è„‰å†²è¡¨ç¤ºã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†å¤šè§†è§’è„‰å†²æ—¶é—´æ„ŸçŸ¥è‡ªæ³¨æ„åŠ›ï¼ˆMulti-view Spiking Temporal-aware Self-Attentionï¼Œç®€ç§°MSTASAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—ç»“åˆäº†æœ‰æ•ˆçš„è„‰å†²æ—¶é—´æ„ŸçŸ¥æ³¨æ„åŠ›ä¸å¤šè§†è§’å­¦ä¹ æ¡†æ¶ï¼Œä»¥æ¨¡æ‹Ÿè¯­éŸ³æŒ‡ä»¤ä¸­çš„äº’è¡¥æ—¶é—´ä¾èµ–æ€§ã€‚åŸºäºMSTASAï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†SpikCommanderï¼Œè¿™æ˜¯ä¸€ç§å®Œå…¨ç”±è„‰å†²é©±åŠ¨çš„å˜å‹å™¨æ¶æ„ï¼Œå®ƒå°†MSTASAä¸è„‰å†²ä¸Šä¸‹æ–‡ç»†åŒ–é€šé“MLPï¼ˆSCR-MLPï¼‰ç›¸ç»“åˆï¼Œä»¥å…±åŒå¢å¼ºæ—¶é—´ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œé€šé“çº§ç‰¹å¾é›†æˆã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼šè„‰å†²æµ·å¾·å ¡æ•°æ®é›†ï¼ˆSpiking Heidelberg Datasetï¼Œç®€ç§°SHDï¼‰ã€è„‰å†²è¯­éŸ³æŒ‡ä»¤ï¼ˆSpiking Speech Commandsï¼Œç®€ç§°SSCï¼‰å’Œè°·æ­Œè¯­éŸ³æŒ‡ä»¤V2ï¼ˆGoogle Speech Commands V2ï¼Œç®€ç§°GSCï¼‰ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSpikCommanderåœ¨å‚æ•°è¾ƒå°‘çš„æƒ…å†µä¸‹å§‹ç»ˆä¼˜äºæœ€æ–°çš„SNNæ–¹æ³•ï¼Œå¹¶ä¸”åœ¨ç›¸ä¼¼çš„æ—¶é—´æ­¥é•¿ä¸‹è¡¨ç°å‡ºå…¶æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ï¼Œä¸ºç¨³å¥çš„è¯­éŸ³æŒ‡ä»¤è¯†åˆ«æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07883v2">PDF</a> Accepted by The Fortieth AAAI Conference on Artificial Intelligence (AAAI 2026)</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†åˆ©ç”¨è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰å®ç°èƒ½æ•ˆé«˜çš„è¯­éŸ³æŒ‡ä»¤è¯†åˆ«ï¼ˆSCRï¼‰ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•éš¾ä»¥æ•æ‰è¯­éŸ³ä¸­çš„ä¸°å¯Œæ—¶é—´ä¾èµ–æ€§å’Œä¸Šä¸‹æ–‡ä¿¡æ¯çš„é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†å¤šè§†è§’è„‰å†²æ—¶åºæ„ŸçŸ¥è‡ªæ³¨æ„åŠ›ï¼ˆMSTASAï¼‰æ¨¡å—å’ŒSpikCommanderæ–¹æ³•ã€‚MSTASAç»“åˆäº†æœ‰æ•ˆçš„è„‰å†²æ—¶åºæ„ŸçŸ¥æ³¨æ„åŠ›å’Œå¤šè§†è§’å­¦ä¹ æ¡†æ¶ï¼Œä»¥æ¨¡æ‹Ÿè¯­éŸ³æŒ‡ä»¤ä¸­çš„äº’è¡¥æ—¶é—´ä¾èµ–æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼ŒSpikCommanderæ˜¯ä¸€ä¸ªå®Œå…¨ç”±è„‰å†²é©±åŠ¨çš„è½¬æ¢å™¨æ¶æ„ï¼Œå®ƒå°†MSTASAä¸è„‰å†²ä¸Šä¸‹æ–‡ç»†åŒ–é€šé“MLPï¼ˆSCR-MLPï¼‰ç›¸ç»“åˆï¼Œå…±åŒå¢å¼ºäº†æ—¶é—´ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œé€šé“çº§ç‰¹å¾é›†æˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSpikCommanderåœ¨å‚æ•°è¾ƒå°‘ã€æ—¶é—´æ­¥é•¿ç›¸ä¼¼çš„æƒ…å†µä¸‹ï¼Œå§‹ç»ˆä¼˜äºæœ€æ–°çš„SNNæ–¹æ³•ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨ç¨³å¥è¯­éŸ³æŒ‡ä»¤è¯†åˆ«æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰ä¸ºå®ç°èƒ½æ•ˆé«˜çš„è¯­éŸ³æŒ‡ä»¤è¯†åˆ«ï¼ˆSCRï¼‰æä¾›äº†å‰æ™¯ã€‚</li>
<li>ç°æœ‰SNN-based SCRæ–¹æ³•é¢ä¸´æ•æ‰è¯­éŸ³ä¸­ä¸°å¯Œçš„æ—¶é—´ä¾èµ–æ€§å’Œä¸Šä¸‹æ–‡ä¿¡æ¯çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥MSTASAæ¨¡å—ï¼Œç»“åˆè„‰å†²æ—¶åºæ„ŸçŸ¥æ³¨æ„åŠ›å’Œå¤šè§†è§’å­¦ä¹ æ¡†æ¶ï¼Œä»¥æ¨¡æ‹Ÿè¯­éŸ³æŒ‡ä»¤ä¸­çš„äº’è¡¥æ—¶é—´ä¾èµ–æ€§ã€‚</li>
<li>æå‡ºSpikCommanderæ–¹æ³•ï¼Œæ˜¯ä¸€ä¸ªå®Œå…¨ç”±è„‰å†²é©±åŠ¨çš„è½¬æ¢å™¨æ¶æ„ï¼Œç»“åˆäº†MSTASAå’Œè„‰å†²ä¸Šä¸‹æ–‡ç»†åŒ–é€šé“MLPï¼ˆSCR-MLPï¼‰ã€‚</li>
<li>SpikCommanderå¢å¼ºäº†æ—¶é—´ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œé€šé“çº§ç‰¹å¾é›†æˆã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSpikCommanderåœ¨å‚æ•°å’Œæ—¶é—´æ­¥é•¿ç›¸ä¼¼çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¼˜äºæœ€æ–°çš„SNNæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-63872aacd6d2e0ddbd6a5815818f20e4" align="middle">
<img src="https://picx.zhimg.com/v2-0976e4820e6f416d1c2ce573f5ad7f61" align="middle">
<img src="https://picx.zhimg.com/v2-798bf4f22b0837fb5ede846f4a4a56aa" align="middle">
<img src="https://picx.zhimg.com/v2-523b738b64c7afc5623d61134e2b5ea3" align="middle">
<img src="https://picx.zhimg.com/v2-96612df7e723143d3fba33392d74bf93" align="middle">
<img src="https://picx.zhimg.com/v2-fabf6eaa28e6987f6a2743a98133791e" align="middle">
<img src="https://picx.zhimg.com/v2-a378219636170b7b1a2587fe36124342" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CAVER-Curious-Audiovisual-Exploring-Robot"><a href="#CAVER-Curious-Audiovisual-Exploring-Robot" class="headerlink" title="CAVER: Curious Audiovisual Exploring Robot"></a>CAVER: Curious Audiovisual Exploring Robot</h2><p><strong>Authors:Luca Macesanu, Boueny Folefack, Samik Singh, Ruchira Ray, Ben Abbatematteo, Roberto MartÃ­n-MartÃ­n</strong></p>
<p>Multimodal audiovisual perception can enable new avenues for robotic manipulation, from better material classification to the imitation of demonstrations for which only audio signals are available (e.g., playing a tune by ear). However, to unlock such multimodal potential, robots need to learn the correlations between an objectâ€™s visual appearance and the sound it generates when they interact with it. Such an active sensorimotor experience requires new interaction capabilities, representations, and exploration methods to guide the robot in efficiently building increasingly rich audiovisual knowledge. In this work, we present CAVER, a novel robot that builds and utilizes rich audiovisual representations of objects. CAVER includes three novel contributions: 1) a novel 3D printed end-effector, attachable to parallel grippers, that excites objectsâ€™ audio responses, 2) an audiovisual representation that combines local and global appearance information with sound features, and 3) an exploration algorithm that uses and builds the audiovisual representation in a curiosity-driven manner that prioritizes interacting with high uncertainty objects to obtain good coverage of surprising audio with fewer interactions. We demonstrate that CAVER builds rich representations in different scenarios more efficiently than several exploration baselines, and that the learned audiovisual representation leads to significant improvements in material classification and the imitation of audio-only human demonstrations. <a target="_blank" rel="noopener" href="https://caver-bot.github.io/">https://caver-bot.github.io/</a></p>
<blockquote>
<p>å¤šæ¨¡æ€è§†å¬æ„ŸçŸ¥èƒ½ä¸ºæœºå™¨äººæ“ä½œå¼€è¾Ÿæ–°çš„é€”å¾„ï¼Œä»æ›´å¥½çš„ææ–™åˆ†ç±»åˆ°æ¨¡ä»¿åªæœ‰éŸ³é¢‘ä¿¡å·çš„æ¼”ç¤ºï¼ˆä¾‹å¦‚ï¼Œå‡­è€³åŠ›æ¼”å¥æ›²è°ƒï¼‰ã€‚ç„¶è€Œï¼Œä¸ºäº†è§£é”è¿™ç§å¤šæ¨¡æ€æ½œåŠ›ï¼Œæœºå™¨äººéœ€è¦å­¦ä¹ ç‰©ä½“å¤–è§‚ä¸å…¶äº¤äº’æ—¶äº§ç”Ÿçš„å£°éŸ³ä¹‹é—´çš„å…³è”ã€‚è¿™ç§æ´»è·ƒçš„æ„Ÿè§‰è¿åŠ¨ç»éªŒéœ€è¦æ–°çš„äº¤äº’èƒ½åŠ›ã€è¡¨ç¤ºæ–¹æ³•å’Œæ¢ç´¢æ–¹æ³•æ¥æŒ‡å¯¼æœºå™¨äººæœ‰æ•ˆåœ°æ„å»ºæ—¥ç›Šä¸°å¯Œçš„è§†å¬çŸ¥è¯†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CAVERï¼Œè¿™æ˜¯ä¸€ç§æ„å»ºå’Œåˆ©ç”¨ç‰©ä½“ä¸°å¯Œè§†å¬è¡¨ç¤ºçš„æ–°å‹æœºå™¨äººã€‚CAVERåŒ…æ‹¬ä¸‰ä¸ªæ–°é¢–çš„è´¡çŒ®ï¼š1ï¼‰ä¸€ç§æ–°å‹3Dæ‰“å°æœ«ç«¯æ‰§è¡Œå™¨ï¼Œå¯é™„åŠ åˆ°å¹³è¡Œå¤¹æŒå™¨ä¸Šï¼Œå¯æ¿€å‘ç‰©ä½“çš„éŸ³é¢‘å“åº”ï¼›2ï¼‰ä¸€ç§è§†å¬è¡¨ç¤ºï¼Œå®ƒå°†å±€éƒ¨å’Œå…¨å±€å¤–è§‚ä¿¡æ¯ä¸å£°éŸ³ç‰¹å¾ç›¸ç»“åˆï¼›3ï¼‰ä¸€ç§æ¢ç´¢ç®—æ³•ï¼Œå®ƒä»¥å¥½å¥‡å¿ƒé©±åŠ¨çš„æ–¹å¼ä½¿ç”¨å’Œæ„å»ºè§†å¬è¡¨ç¤ºï¼Œä¼˜å…ˆä¸é«˜åº¦ä¸ç¡®å®šçš„å¯¹è±¡è¿›è¡Œäº¤äº’ï¼Œä»¥è¾ƒå°‘çš„äº¤äº’è·å¾—ä»¤äººæƒŠè®¶çš„éŸ³é¢‘çš„è‰¯å¥½è¦†ç›–ã€‚æˆ‘ä»¬è¯æ˜ï¼ŒCAVERåœ¨ä¸åŒçš„åœºæ™¯ä¸­æ›´æœ‰æ•ˆåœ°æ„å»ºä¸°å¯Œçš„è¡¨ç¤ºå½¢å¼ï¼Œå¹¶ä¸”æ‰€å­¦çš„è§†å¬è¡¨ç¤ºåœ¨ææ–™åˆ†ç±»å’Œä»…ä½¿ç”¨éŸ³é¢‘çš„äººç±»æ¼”ç¤ºæ¨¡ä»¿æ–¹é¢å¸¦æ¥äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æƒ³äº†è§£æ›´å¤šä¿¡æ¯è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://caver-bot.github.io/">CAVERæœºå™¨äººå®˜ç½‘</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07619v1">PDF</a> 9 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸€ç§åä¸ºCAVERçš„æ–°å‹æœºå™¨äººæŠ€æœ¯ï¼Œå®ƒèƒ½å¤Ÿæ„å»ºå’Œåˆ©ç”¨ä¸°å¯Œçš„è§†å¬å¯¹è±¡è¡¨ç¤ºã€‚é€šè¿‡ä¸‰é¡¹æ–°è´¡çŒ®ï¼ŒåŒ…æ‹¬å¯é™„ç€äºå¹³è¡Œå¤¹å…·çš„3Dæ‰“å°æœ«ç«¯æ‰§è¡Œå™¨ã€ç»“åˆå±€éƒ¨å’Œå…¨å±€å¤–è§‚ä¿¡æ¯ä¸å£°éŸ³ç‰¹å¾çš„è§†å¬è¡¨ç¤ºï¼Œä»¥åŠå¥½å¥‡å¿ƒé©±åŠ¨çš„æ¢ç´¢ç®—æ³•ï¼ŒCAVERæœºå™¨äººèƒ½å¤Ÿåœ¨ä¸åŒåœºæ™¯ä¸‹æ›´æœ‰æ•ˆåœ°æ„å»ºä¸°å¯Œçš„è§†å¬çŸ¥è¯†è¡¨ç¤ºï¼Œæ˜¾è‘—æé«˜ææ–™åˆ†ç±»å’Œä»…å¬éŸ³é¢‘çš„äººç±»æ¼”ç¤ºæ¨¡ä»¿çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€è§†å¬æ„ŸçŸ¥èƒ½åŠ›ä¸ºæœºå™¨äººæ“ä½œå¸¦æ¥äº†æ–°çš„å¯èƒ½æ€§ï¼Œå¦‚æ›´å¥½çš„ææ–™åˆ†ç±»å’Œä»…é€šè¿‡éŸ³é¢‘ä¿¡å·è¿›è¡Œç¤ºèŒƒæ¨¡ä»¿ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡å¬è§‰æ¨¡ä»¿å¼¹å¥ä¹æ›²ï¼‰ã€‚</li>
<li>ä¸ºäº†å®ç°è¿™ç§å¤šæ¨¡æ€æ½œåŠ›ï¼Œæœºå™¨äººéœ€è¦å­¦ä¹ å¯¹è±¡è§†è§‰å¤–è§‚ä¸å…¶äº¤äº’æ—¶äº§ç”Ÿçš„å£°éŸ³ä¹‹é—´çš„å…³è”ã€‚</li>
<li>CAVERæœºå™¨äººæ˜¯ä¸€ç§æ–°å‹æœºå™¨äººæŠ€æœ¯ï¼Œèƒ½å¤Ÿæ„å»ºå’Œåˆ©ç”¨ä¸°å¯Œçš„è§†å¬å¯¹è±¡è¡¨ç¤ºã€‚</li>
<li>CAVERåŒ…æ‹¬ä¸‰é¡¹æ–°è´¡çŒ®ï¼š3Dæ‰“å°æœ«ç«¯æ‰§è¡Œå™¨ã€è§†å¬è¡¨ç¤ºæ³•å’Œæ¢ç´¢ç®—æ³•ã€‚</li>
<li>3Dæ‰“å°æœ«ç«¯æ‰§è¡Œå™¨å¯æ¿€å‘å¯¹è±¡çš„éŸ³é¢‘å“åº”ã€‚</li>
<li>è§†å¬è¡¨ç¤ºæ³•ç»“åˆäº†å±€éƒ¨å’Œå…¨å±€å¤–è§‚ä¿¡æ¯ä¸å£°éŸ³ç‰¹å¾ã€‚</li>
<li>æ¢ç´¢ç®—æ³•é‡‡ç”¨å¥½å¥‡å¿ƒé©±åŠ¨çš„æ–¹å¼ï¼Œä¼˜å…ˆä¸ä¸ç¡®å®šæ€§è¾ƒé«˜çš„å¯¹è±¡è¿›è¡Œäº¤äº’ï¼Œä»¥ç”¨è¾ƒå°‘çš„äº¤äº’è·å¾—å‡ºäººæ„æ–™çš„éŸ³é¢‘è¦†ç›–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07619">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cc36b7900a0b7feb88adb4d1a97090fe" align="middle">
<img src="https://picx.zhimg.com/v2-ce9391900d11675982c3bb33faee96eb" align="middle">
<img src="https://picx.zhimg.com/v2-ac60c3efe21a04c055122ffa18c30f6a" align="middle">
<img src="https://picx.zhimg.com/v2-ec2edbcc192720260f0dda6299ebdc2e" align="middle">
<img src="https://picx.zhimg.com/v2-8d9b77f6f5b4cc85c85ad78772a77550" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="E2E-VGuard-Adversarial-Prevention-for-Production-LLM-based-End-To-End-Speech-Synthesis"><a href="#E2E-VGuard-Adversarial-Prevention-for-Production-LLM-based-End-To-End-Speech-Synthesis" class="headerlink" title="E2E-VGuard: Adversarial Prevention for Production LLM-based End-To-End Speech Synthesis"></a>E2E-VGuard: Adversarial Prevention for Production LLM-based End-To-End Speech Synthesis</h2><p><strong>Authors:Zhisheng Zhang, Derui Wang, Yifan Mi, Zhiyong Wu, Jie Gao, Yuxin Cao, Kai Ye, Minhui Xue, Jie Hao</strong></p>
<p>Recent advancements in speech synthesis technology have enriched our daily lives, with high-quality and human-like audio widely adopted across real-world applications. However, malicious exploitation like voice-cloning fraud poses severe security risks. Existing defense techniques struggle to address the production large language model (LLM)-based speech synthesis. While previous studies have considered the protection for fine-tuning synthesizers, they assume manually annotated transcripts. Given the labor intensity of manual annotation, end-to-end (E2E) systems leveraging automatic speech recognition (ASR) to generate transcripts are becoming increasingly prevalent, e.g., voice cloning via commercial APIs. Therefore, this E2E speech synthesis also requires new security mechanisms. To tackle these challenges, we propose E2E-VGuard, a proactive defense framework for two emerging threats: (1) production LLM-based speech synthesis, and (2) the novel attack arising from ASR-driven E2E scenarios. Specifically, we employ the encoder ensemble with a feature extractor to protect timbre, while ASR-targeted adversarial examples disrupt pronunciation. Moreover, we incorporate the psychoacoustic model to ensure perturbative imperceptibility. For a comprehensive evaluation, we test 16 open-source synthesizers and 3 commercial APIs across Chinese and English datasets, confirming E2E-VGuardâ€™s effectiveness in timbre and pronunciation protection. Real-world deployment validation is also conducted. Our code and demo page are available at <a target="_blank" rel="noopener" href="https://wxzyd123.github.io/e2e-vguard/">https://wxzyd123.github.io/e2e-vguard/</a>.</p>
<blockquote>
<p>æœ€è¿‘è¯­éŸ³åˆæˆæŠ€æœ¯çš„è¿›å±•ä¸°å¯Œäº†æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ï¼Œé«˜è´¨é‡ã€äººæ€§åŒ–çš„éŸ³é¢‘åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­å¾—åˆ°äº†å¹¿æ³›é‡‡ç”¨ã€‚ç„¶è€Œï¼Œè¯¸å¦‚è¯­éŸ³å…‹éš†æ¬ºè¯ˆç­‰æ¶æ„åˆ©ç”¨è¡Œä¸ºå¸¦æ¥äº†ä¸¥é‡çš„å®‰å…¨é£é™©ã€‚ç°æœ‰çš„é˜²å¾¡æŠ€æœ¯å¾ˆéš¾åº”å¯¹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­éŸ³åˆæˆã€‚è™½ç„¶ä»¥å‰çš„ç ”ç©¶å·²ç»è€ƒè™‘äº†å¯¹å¾®è°ƒåˆæˆå™¨çš„ä¿æŠ¤ï¼Œä½†å®ƒä»¬å‡è®¾ä½¿ç”¨æ‰‹åŠ¨æ³¨é‡Šçš„æ–‡æœ¬ã€‚è€ƒè™‘åˆ°æ‰‹åŠ¨æ³¨é‡Šçš„åŠ³åŠ¨å¯†é›†åº¦ï¼Œåˆ©ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç”Ÿæˆæ–‡æœ¬ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰ç³»ç»Ÿæ­£å˜å¾—è¶Šæ¥è¶Šæ™®éï¼Œä¾‹å¦‚é€šè¿‡å•†ä¸šAPIè¿›è¡Œè¯­éŸ³å…‹éš†ã€‚å› æ­¤ï¼Œè¿™ç§ç«¯åˆ°ç«¯çš„è¯­éŸ³åˆæˆä¹Ÿéœ€è¦æ–°çš„å®‰å…¨æœºåˆ¶ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç«¯åˆ°ç«¯VGuardï¼ˆE2E-VGuardï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸¤ä¸ªæ–°å…´å¨èƒçš„ç§¯æé˜²å¾¡æ¡†æ¶ï¼šï¼ˆ1ï¼‰åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­éŸ³åˆæˆç”Ÿäº§ï¼Œï¼ˆ2ï¼‰ç”±ASRé©±åŠ¨çš„ç«¯åˆ°ç«¯åœºæ™¯äº§ç”Ÿçš„æ–°å‹æ”»å‡»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨ç¼–ç å™¨é›†ä¸ç‰¹å¾æå–å™¨æ¥ä¿æŠ¤éŸ³è‰²ï¼ŒåŒæ—¶é’ˆå¯¹ASRè®¾è®¡çš„å¯¹æŠ—æ€§ç¤ºä¾‹ä¼šç ´åå‘éŸ³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆäº†å¿ƒç†å£°å­¦æ¨¡å‹æ¥ç¡®ä¿æ‰°åŠ¨çš„ä¸æ˜“å¯Ÿè§‰æ€§ã€‚ä¸ºäº†è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬æµ‹è¯•äº†16ä¸ªå¼€æºåˆæˆå™¨å’Œè·¨ä¸­æ–‡å’Œè‹±æ–‡æ•°æ®é›†çš„3ä¸ªå•†ä¸šAPIï¼Œè¯å®äº†E2E-VGuardåœ¨éŸ³è‰²å’Œå‘éŸ³ä¿æŠ¤æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿˜è¿›è¡Œäº†ç°å®ä¸–ç•Œéƒ¨ç½²éªŒè¯ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¼”ç¤ºé¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://wxzyd123.github.io/e2e-vguard/%E8%AE%BF%E9%97%AE%E3%80%82">https://wxzyd123.github.io/e2e-vguard/è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07099v1">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¿‘æœŸè¯­éŸ³åˆæˆæŠ€æœ¯çš„è¿›å±•åŠå…¶åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å¹¿æ³›åº”ç”¨ï¼ŒåŒæ—¶æŒ‡å‡ºæ¶æ„åˆ©ç”¨å¦‚è¯­éŸ³å…‹éš†æ¬ºè¯ˆç­‰ä¸¥é‡å®‰å…¨é£é™©ã€‚ç°æœ‰é˜²å¾¡æŠ€æœ¯éš¾ä»¥åº”å¯¹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­éŸ³åˆæˆï¼Œè€Œç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç²¾ç»†è°ƒæ•´åˆæˆå™¨çš„ä¿æŠ¤æ–¹é¢ï¼Œå¹¶å‡è®¾æ‰‹åŠ¨æ³¨é‡Šçš„æ–‡æœ¬ã€‚è€ƒè™‘åˆ°æ‰‹åŠ¨æ³¨é‡Šçš„åŠ³åŠ¨å¯†é›†æ€§ï¼Œé‡‡ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç”Ÿæˆæ–‡æœ¬çš„ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰ç³»ç»Ÿè¶Šæ¥è¶Šæ™®éã€‚å› æ­¤ï¼Œè¿™ç§E2Eè¯­éŸ³åˆæˆä¹Ÿéœ€è¦æ–°çš„å®‰å…¨æœºåˆ¶ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†E2E-VGuardï¼Œä¸€ä¸ªé’ˆå¯¹ä¸¤ç§æ–°å…´å¨èƒçš„ä¸»åŠ¨é˜²å¾¡æ¡†æ¶ï¼šä¸€æ˜¯åŸºäºç”Ÿäº§LLMçš„è¯­éŸ³åˆæˆï¼ŒäºŒæ˜¯æ¥è‡ªASRé©±åŠ¨çš„E2Eåœºæ™¯çš„å…¨æ–°æ”»å‡»ã€‚é€šè¿‡ç¼–ç å™¨ç»„åˆã€ç‰¹å¾æå–å™¨ä¿æŠ¤éŸ³è‰²ã€é’ˆå¯¹ASRçš„å¯¹æŠ—æ€§å®ä¾‹å¹²æ‰°å‘éŸ³ï¼Œå¹¶ç»“åˆå¿ƒç†å£°å­¦æ¨¡å‹ç¡®ä¿æ‰°åŠ¨ä¸å¯å¯Ÿè§‰ã€‚å®éªŒæµ‹è¯•äº†å¤šç§å¼€æºåˆæˆå™¨å’Œå•†ä¸šAPIï¼ŒéªŒè¯äº†E2E-VGuardåœ¨éŸ³è‰²å’Œå‘éŸ³ä¿æŠ¤æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸè¯­éŸ³åˆæˆæŠ€æœ¯å¹¿æ³›åº”ç”¨äºæ—¥å¸¸ç”Ÿæ´»ï¼Œä½†å­˜åœ¨æ¶æ„åˆ©ç”¨å¦‚è¯­éŸ³å…‹éš†æ¬ºè¯ˆç­‰å®‰å…¨é£é™©ã€‚</li>
<li>ç°æœ‰é˜²å¾¡æŠ€æœ¯éš¾ä»¥åº”å¯¹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­éŸ³åˆæˆå’Œç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰ç³»ç»Ÿçš„æ–°å…´å¨èƒã€‚</li>
<li>E2E-VGuardæ˜¯ä¸€ä¸ªé’ˆå¯¹LLM-basedè¯­éŸ³åˆæˆå’ŒE2Eåœºæ™¯çš„æ–°å‹æ”»å‡»çš„ä¸»åŠ¨é˜²å¾¡æ¡†æ¶ã€‚</li>
<li>E2E-VGuardé€šè¿‡ç¼–ç å™¨ç»„åˆã€ç‰¹å¾æå–å™¨ä¿æŠ¤éŸ³è‰²ï¼Œé€šè¿‡ASRå¯¹æŠ—æ€§å®ä¾‹å¹²æ‰°å‘éŸ³ã€‚</li>
<li>ç»“åˆå¿ƒç†å£°å­¦æ¨¡å‹ç¡®ä¿æ‰°åŠ¨ä¸å¯å¯Ÿè§‰ï¼Œå¢å¼ºé˜²å¾¡ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å®éªŒæµ‹è¯•äº†å¤šç§å¼€æºåˆæˆå™¨å’Œå•†ä¸šAPIï¼ŒéªŒè¯äº†E2E-VGuardåœ¨éŸ³è‰²å’Œå‘éŸ³ä¿æŠ¤æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07099">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1df851e3f56855ab509ba5b887f5dc87" align="middle">
<img src="https://picx.zhimg.com/v2-2d402c2a82596a186957255426ab5c62" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Reperio-rPPG-Relational-Temporal-Graph-Neural-Networks-for-Periodicity-Learning-in-Remote-Physiological-Measurement"><a href="#Reperio-rPPG-Relational-Temporal-Graph-Neural-Networks-for-Periodicity-Learning-in-Remote-Physiological-Measurement" class="headerlink" title="Reperio-rPPG: Relational Temporal Graph Neural Networks for Periodicity Learning in Remote Physiological Measurement"></a>Reperio-rPPG: Relational Temporal Graph Neural Networks for Periodicity Learning in Remote Physiological Measurement</h2><p><strong>Authors:Ba-Thinh Nguyen, Thach-Ha Ngoc Pham, Hoang-Long Duc Nguyen, Thi-Duyen Ngo, Thanh-Ha Le</strong></p>
<p>Remote photoplethysmography (rPPG) is an emerging contactless physiological sensing technique that leverages subtle color variations in facial videos to estimate vital signs such as heart rate and respiratory rate. This non-invasive method has gained traction across diverse domains, including telemedicine, affective computing, driver fatigue detection, and health monitoring, owing to its scalability and convenience. Despite significant progress in remote physiological signal measurement, a crucial characteristic - the intrinsic periodicity - has often been underexplored or insufficiently modeled in previous approaches, limiting their ability to capture fine-grained temporal dynamics under real-world conditions. To bridge this gap, we propose Reperio-rPPG, a novel framework that strategically integrates Relational Convolutional Networks with a Graph Transformer to effectively capture the periodic structure inherent in physiological signals. Additionally, recognizing the limited diversity of existing rPPG datasets, we further introduce a tailored CutMix augmentation to enhance the modelâ€™s generalizability. Extensive experiments conducted on three widely used benchmark datasets - PURE, UBFC-rPPG, and MMPD - demonstrate that Reperio-rPPG not only achieves state-of-the-art performance but also exhibits remarkable robustness under various motion (e.g., stationary, rotation, talking, walking) and illumination conditions (e.g., nature, low LED, high LED). The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/deconasser/Reperio-rPPG">https://github.com/deconasser/Reperio-rPPG</a>.</p>
<blockquote>
<p>è¿œç¨‹å…‰å®¹ç§¯æ³•ï¼ˆrPPGï¼‰æ˜¯ä¸€ç§æ–°å…´çš„éæ¥è§¦å¼ç”Ÿç†ä¼ æ„ŸæŠ€æœ¯ï¼Œå®ƒåˆ©ç”¨é¢éƒ¨è§†é¢‘ä¸­çš„ç»†å¾®è‰²å½©å˜åŒ–æ¥ä¼°è®¡å¿ƒç‡å’Œå‘¼å¸ç‡ç­‰ç”Ÿå‘½ä½“å¾ã€‚è¿™ç§éä¾µå…¥å¼çš„æ–¹æ³•åœ¨è¿œç¨‹åŒ»ç–—ã€æƒ…æ„Ÿè®¡ç®—ã€é©¾é©¶å‘˜ç–²åŠ³æ£€æµ‹ä»¥åŠå¥åº·ç›‘æµ‹ç­‰å¤šä¸ªé¢†åŸŸéƒ½å—åˆ°äº†å¹¿æ³›çš„å…³æ³¨ï¼Œå› ä¸ºå®ƒå…·æœ‰å¯æ‰©å±•æ€§å’Œä¾¿æ·æ€§ã€‚å°½ç®¡åœ¨è¿œç¨‹ç”Ÿç†ä¿¡å·æµ‹é‡æ–¹é¢å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»¥å‰çš„æ–¹æ³•å¸¸å¸¸å¿½ç•¥æˆ–æœªèƒ½å……åˆ†å»ºæ¨¡ä¸€ä¸ªé‡è¦ç‰¹å¾â€”â€”å›ºæœ‰å‘¨æœŸæ€§ï¼Œè¿™åœ¨ç°å®æ¡ä»¶ä¸‹é™åˆ¶äº†å®ƒä»¬æ•æ‰ç»†å¾®æ—¶é—´åŠ¨æ€çš„èƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ä¸è¶³ï¼Œæˆ‘ä»¬æå‡ºäº†Reperio-rPPGè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒæˆ˜ç•¥æ€§åœ°ç»“åˆäº†å…³ç³»å·ç§¯ç½‘ç»œå’Œå›¾å˜æ¢å™¨ï¼Œä»¥æœ‰æ•ˆåœ°æ•æ‰ç”Ÿç†ä¿¡å·ä¸­å›ºæœ‰çš„å‘¨æœŸæ€§ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„è¯†åˆ°ç°æœ‰rPPGæ•°æ®é›†å¤šæ ·æ€§æœ‰é™çš„é—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†å®šåˆ¶çš„CutMixå¢å¼ºæŠ€æœ¯æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æ•°æ®é›†â€”â€”PUREã€UBFC-rPPGå’ŒMMPDä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒReperio-rPPGä¸ä»…è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè€Œä¸”åœ¨å„ç§è¿åŠ¨ï¼ˆå¦‚é™æ­¢ã€æ—‹è½¬ã€è¯´è¯ã€è¡Œèµ°ï¼‰å’Œç…§æ˜æ¡ä»¶ï¼ˆå¦‚è‡ªç„¶å…‰ã€ä½LEDã€é«˜LEDï¼‰ä¸‹ä¹Ÿè¡¨ç°å‡ºäº†æƒŠäººçš„ç¨³å¥æ€§ã€‚ä»£ç å…¬å¼€å¯è®¿é—®äº <a target="_blank" rel="noopener" href="https://github.com/deconasser/Reperio-rPPG%E3%80%82">https://github.com/deconasser/Reperio-rPPGã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05946v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿œç¨‹å…‰å®¹ç§¯è„‰ææ³¢æ³•ï¼ˆrPPGï¼‰æ˜¯ä¸€ç§æ–°å…´çš„æ— æ¥è§¦ç”Ÿç†ä¼ æ„ŸæŠ€æœ¯ï¼Œå®ƒé€šè¿‡é¢éƒ¨è§†é¢‘ä¸­çš„ç»†å¾®è‰²å½©å˜åŒ–æ¥ä¼°è®¡å¿ƒç‡å’Œå‘¼å¸ç‡ç­‰ç”Ÿå‘½ä½“å¾ã€‚ç”±äºå…¶å¯æ‰©å±•æ€§å’Œä¾¿åˆ©æ€§ï¼Œå®ƒåœ¨è¿œç¨‹ç”Ÿç†ä¿¡å·æµ‹é‡é¢†åŸŸå–å¾—äº†è¿›å±•ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºè¿œç¨‹åŒ»ç–—ã€æƒ…æ„Ÿè®¡ç®—ã€é©¾é©¶å‘˜ç–²åŠ³æ£€æµ‹å’Œå¥åº·ç›‘æµ‹ç­‰é¢†åŸŸã€‚é’ˆå¯¹ä»¥å¾€ç ”ç©¶ä¸­å¿½ç•¥æˆ–å»ºæ¨¡ä¸è¶³çš„å›ºæœ‰å‘¨æœŸæ€§ç‰¹å¾ï¼Œæˆ‘ä»¬æå‡ºäº†Reperio-rPPGæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å…³ç³»å·ç§¯ç½‘ç»œå’Œå›¾å˜æ¢å™¨ï¼Œä»¥æœ‰æ•ˆæ•æ‰ç”Ÿç†ä¿¡å·ä¸­çš„å‘¨æœŸæ€§ç»“æ„ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥å®šåˆ¶çš„CutMixå¢å¼ºæ–¹æ³•ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å¤šä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒReperio-rPPGä¸ä»…è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œè¿˜åœ¨å„ç§è¿åŠ¨å’Œå…‰ç…§æ¡ä»¶ä¸‹è¡¨ç°å‡ºäº†å“è¶Šçš„é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿œç¨‹å…‰å®¹ç§¯è„‰ææ³¢æ³•ï¼ˆrPPGï¼‰æ˜¯ä¸€ç§åˆ©ç”¨é¢éƒ¨è§†é¢‘ä¸­çš„è‰²å½©å˜åŒ–ä¼°è®¡ç”Ÿå‘½ä½“å¾çš„éæ¥è§¦ç”Ÿç†ä¼ æ„ŸæŠ€æœ¯ã€‚</li>
<li>rPPGæŠ€æœ¯åœ¨è¿œç¨‹åŒ»ç–—ã€æƒ…æ„Ÿè®¡ç®—ã€é©¾é©¶å‘˜ç–²åŠ³æ£€æµ‹å’Œå¥åº·ç›‘æµ‹ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>ä»¥å¾€rPPGç ”ç©¶å¸¸å¿½ç•¥æˆ–å»ºæ¨¡ä¸è¶³çš„å›ºæœ‰å‘¨æœŸæ€§ç‰¹å¾é™åˆ¶äº†å…¶åœ¨çœŸå®ä¸–ç•Œæ¡ä»¶ä¸‹çš„ç²¾ç»†æ—¶é—´åŠ¨æ€æ•æ‰èƒ½åŠ›ã€‚</li>
<li>Reperio-rPPGæ¡†æ¶ç»“åˆäº†å…³ç³»å·ç§¯ç½‘ç»œå’Œå›¾å˜æ¢å™¨ï¼Œä»¥æœ‰æ•ˆæ•æ‰ç”Ÿç†ä¿¡å·ä¸­çš„å‘¨æœŸæ€§ç»“æ„ã€‚</li>
<li>Reperio-rPPGé€šè¿‡å¼•å…¥å®šåˆ¶çš„CutMixå¢å¼ºæ–¹æ³•æé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒReperio-rPPGè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05946">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9535999e61685ba242c0e17a508944ed" align="middle">
<img src="https://picx.zhimg.com/v2-4100e71179de0f86719e7eba90d56152" align="middle">
<img src="https://picx.zhimg.com/v2-31219039494c05d300f161363d25ddb3" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Long-Grounded-Thoughts-Distilling-Compositional-Visual-Reasoning-Chains-at-Scale"><a href="#Long-Grounded-Thoughts-Distilling-Compositional-Visual-Reasoning-Chains-at-Scale" class="headerlink" title="Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale"></a>Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale</h2><p><strong>Authors:David Acuna, Chao-Han Huck Yang, Yuntian Deng, Jaehun Jung, Ximing Lu, Prithviraj Ammanabrolu, Hyunwoo Kim, Yuan-Hong Liao, Yejin Choi</strong></p>
<p>Recent progress in multimodal reasoning has been driven largely by undisclosed datasets and proprietary data synthesis recipes, leaving open questions about how to systematically build large-scale, vision-centric reasoning datasets, particularly for tasks that go beyond visual math. In this work, we introduce a new reasoning data generation framework spanning diverse skills and levels of complexity with over 1M high-quality synthetic vision-centric questions. The dataset also includes preference data and instruction prompts supporting both offline and online RL. Our synthesis framework proceeds in two stages: (1) scale; and (2) complexity. Reasoning traces are then synthesized through a two-stage process that leverages VLMs and reasoning LLMs, producing CoT traces for VLMs that capture the richness and diverse cognitive behaviors found in frontier reasoning models. Remarkably, we show that finetuning Qwen2.5-VL-7B on our data outperforms all open-data baselines across all evaluated vision-centric benchmarks, and even surpasses strong closed-data models such as MiMo-VL-7B-RL on V* Bench, CV-Bench and MMStar-V. Perhaps most surprising, despite being entirely vision-centric, our data transfers positively to text-only reasoning (MMLU-Pro) and audio reasoning (MMAU), demonstrating its effectiveness. Similarly, despite not containing videos or embodied visual data, we observe notable gains when evaluating on a single-evidence embodied QA benchmark (NiEH). Finally, we use our data to analyze the entire VLM post-training pipeline. Our empirical analysis highlights that (i) SFT on high-quality data with non-linear reasoning traces is essential for effective online RL, (ii) staged offline RL matches online RLâ€™s performance while reducing compute demands, and (iii) careful SFT on high quality data can substantially improve out-of-domain, cross-modality transfer.</p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€æ¨ç†è¿›å±•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¾—ç›Šäºæœªå…¬å¼€çš„æ•°æ®é›†å’Œä¸“æœ‰æ•°æ®åˆæˆæ–¹æ³•ï¼Œå…³äºå¦‚ä½•ç³»ç»Ÿåœ°æ„å»ºå¤§è§„æ¨¡ã€ä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„æ¨ç†æ•°æ®é›†ä»å­˜åœ¨ç–‘é—®ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè¶…è¶Šè§†è§‰æ•°å­¦çš„ä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ¨ç†æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ¶µç›–å„ç§æŠ€èƒ½å’Œå¤æ‚ç¨‹åº¦ï¼ŒåŒ…å«è¶…è¿‡100ä¸‡é«˜è´¨é‡çš„åˆæˆè§†è§‰ä¸­å¿ƒé—®é¢˜ã€‚è¯¥æ•°æ®é›†è¿˜åŒ…æ‹¬æ”¯æŒç¦»çº¿å’Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„åå¥½æ•°æ®å’ŒæŒ‡ä»¤æç¤ºã€‚æˆ‘ä»¬çš„åˆæˆæ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šï¼ˆ1ï¼‰è§„æ¨¡ï¼›ï¼ˆ2ï¼‰å¤æ‚æ€§ã€‚ç„¶åé€šè¿‡ä¸¤é˜¶æ®µè¿‡ç¨‹åˆæˆæ¨ç†è½¨è¿¹ï¼Œè¯¥è¿‡ç¨‹åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä¸ºVLMsç”Ÿæˆæ•è·å‰æ²¿æ¨ç†æ¨¡å‹ä¸­ä¸°å¯Œå’Œå¤šæ ·åŒ–çš„è®¤çŸ¥è¡Œä¸ºçš„CoTè½¨è¿¹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šå¾®è°ƒQwen2.5-VL-7Bçš„è¡¨ç°è¶…è¿‡äº†æ‰€æœ‰å…¬å¼€æ•°æ®åŸºå‡†æµ‹è¯•çš„æ‰€æœ‰è§†è§‰ä¸­å¿ƒåŸºå‡†æµ‹è¯•ï¼Œç”šè‡³è¶…è¶Šäº†MiMo-VL-7B-RLç­‰å¼ºå¤§çš„å°é—­æ•°æ®æ¨¡å‹åœ¨V*Benchã€CV-Benchå’ŒMMStar-Vä¸Šçš„è¡¨ç°ã€‚æœ€ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå°½ç®¡å®Œå…¨ä»¥è§†è§‰ä¸ºä¸­å¿ƒï¼Œæˆ‘ä»¬çš„æ•°æ®å¯¹æ–‡æœ¬æ¨ç†ï¼ˆMMLU-Proï¼‰å’ŒéŸ³é¢‘æ¨ç†ï¼ˆMMAUï¼‰äº§ç”Ÿäº†ç§¯æçš„å½±å“ã€‚åŒæ ·ï¼Œå°½ç®¡ä¸åŒ…å«è§†é¢‘æˆ–èº«ä¸´å…¶å¢ƒçš„è§†è§‰æ•°æ®ï¼Œä½†åœ¨å•ä¸€è¯æ®èº«ä¸´å…¶å¢ƒçš„é—®ç­”åŸºå‡†æµ‹è¯•ï¼ˆNiEHï¼‰ä¸Šè¯„ä¼°æ—¶ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°äº†æ˜¾è‘—çš„æ”¶ç›Šã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®åˆ†æäº†æ•´ä¸ªVLMåè®­ç»ƒç®¡é“ã€‚æˆ‘ä»¬çš„å®è¯åˆ†æè¡¨æ˜ï¼šï¼ˆiï¼‰åœ¨å…·æœ‰éçº¿æ€§æ¨ç†è½¨è¿¹çš„é«˜è´¨é‡æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒå¯¹äºæœ‰æ•ˆçš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ è‡³å…³é‡è¦ï¼Œï¼ˆiiï¼‰åˆ†é˜¶æ®µçš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ä¸åœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ç›¸åŒ¹é…ï¼ŒåŒæ—¶é™ä½äº†è®¡ç®—éœ€æ±‚ï¼Œï¼ˆiiiï¼‰åœ¨é«˜è´¨é‡æ•°æ®ä¸Šè¿›è¡Œä»”ç»†çš„å¾®è°ƒå¯ä»¥æå¤§åœ°æé«˜è·¨é¢†åŸŸçš„è·¨æ¨¡æ€è¿ç§»èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05705v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://nvlabs.github.io/LongGroundedThoughts/">https://nvlabs.github.io/LongGroundedThoughts/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå…¨æ–°çš„æ¨ç†æ•°æ®ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ¶µç›–å„é¡¹æŠ€èƒ½å’Œä¸åŒå¤æ‚åº¦çº§åˆ«ï¼ŒåŒ…å«è¶…è¿‡100ä¸‡é«˜è´¨é‡åˆæˆè§†è§‰ä¸­å¿ƒé—®é¢˜ã€‚æ•°æ®é›†åŒ…å«æ”¯æŒç¦»çº¿åœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„åå¥½æ•°æ®å’ŒæŒ‡ä»¤æç¤ºã€‚åˆæˆæ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šè§„æ¨¡å’Œå¤æ‚åº¦ã€‚é€šè¿‡åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œè¿›è¡Œä¸¤é˜¶æ®µæ¨ç†è½¨è¿¹åˆæˆï¼Œæ•æ‰å‰æ²¿æ¨ç†æ¨¡å‹çš„ä¸°å¯Œæ€§å’Œå¤šæ ·åŒ–è®¤çŸ¥è¡Œä¸ºã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨æœ¬æ–‡æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹è¡¨ç°ä¼˜è¶Šï¼Œè¶…è¿‡æ‰€æœ‰å…¬å¼€æ•°æ®åŸºå‡†æµ‹è¯•ï¼Œå¹¶åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šå°é—­æ•°æ®æ¨¡å‹ã€‚æœ¬æ–‡æ•°æ®åœ¨è§†è§‰ä¸­å¿ƒä»»åŠ¡å¤–ï¼Œå¦‚æ–‡æœ¬å’ŒéŸ³é¢‘æ¨ç†ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºç§¯æè¿ç§»æ•ˆæœã€‚æœ€åï¼Œä½¿ç”¨æœ¬æ–‡æ•°æ®åˆ†ææ•´ä¸ªVLMåè®­ç»ƒç®¡é“ï¼Œå¼ºè°ƒé«˜è´¨é‡æ•°æ®çš„é‡è¦æ€§ä»¥åŠåˆ†é˜¶æ®µç¦»çº¿RLåœ¨å‡å°‘è®¡ç®—éœ€æ±‚çš„åŒæ—¶åŒ¹é…åœ¨çº¿RLæ€§èƒ½çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ä¸ªæ¶µç›–å¤šç§æŠ€èƒ½å’Œå¤æ‚åº¦çº§åˆ«çš„å¤§è§„æ¨¡ã€é«˜è´¨é‡åˆæˆè§†è§‰ä¸ºä¸­å¿ƒçš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬è¶…è¿‡ä¸€ç™¾ä¸‡çš„é—®é¢˜ã€‚</li>
<li>åˆæˆæ¡†æ¶åˆ†ä¸ºè§„æ¨¡å’Œå¤æ‚åº¦ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œæ•°æ®ç”Ÿæˆã€‚</li>
<li>åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥ç”ŸæˆåŒ…å«ä¸°å¯Œè®¤çŸ¥è¡Œä¸ºçš„æ¨ç†è½¨è¿¹ã€‚</li>
<li>æ¨¡å‹åœ¨è¯¥æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡æ‰€æœ‰å…¬å¼€åŸºå‡†æµ‹è¯•ï¼Œå¹¶ä¸”åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ä½¿ç”¨å°é—­æ•°æ®çš„æ¨¡å‹ã€‚</li>
<li>æ•°æ®å±•ç°å‡ºç§¯æè¿ç§»æ•ˆåº”ï¼Œé€‚ç”¨äºè§†è§‰ä¸­å¿ƒä»»åŠ¡ä»¥å¤–çš„å…¶ä»–é¢†åŸŸï¼Œå¦‚æ–‡æœ¬å’ŒéŸ³é¢‘æ¨ç†ä»»åŠ¡ã€‚</li>
<li>å®è¯åˆ†ææ˜¾ç¤ºï¼Œé«˜è´¨é‡æ•°æ®å¯¹äºæœ‰æ•ˆå¼ºåŒ–å­¦ä¹ è‡³å…³é‡è¦ã€‚åˆ†é˜¶æ®µç¦»çº¿å¼ºåŒ–å­¦ä¹ èƒ½åœ¨å‡å°‘è®¡ç®—éœ€æ±‚çš„åŒæ—¶ä¸åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ€§èƒ½ç›¸åŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05705">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-28a698ec5c79a6ef5c91470c113f6f31" align="middle">
<img src="https://picx.zhimg.com/v2-511893eb209d865cefe7559bdc48cf28" align="middle">
<img src="https://picx.zhimg.com/v2-d20d9873e99e2529fee5a3d04132ef3e" align="middle">
<img src="https://picx.zhimg.com/v2-c5a0645d4139c0797f9ace168bc8f904" align="middle">
<img src="https://picx.zhimg.com/v2-edd2101866e0d675d37da8015f0f6eff" align="middle">
<img src="https://picx.zhimg.com/v2-07bee50687481f771c986f71fb80a1c7" align="middle">
<img src="https://picx.zhimg.com/v2-8766bebd677579acf81039d4513aa028" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="psiUnity-A-Platform-for-Multimodal-Data-Driven-XR"><a href="#psiUnity-A-Platform-for-Multimodal-Data-Driven-XR" class="headerlink" title="psiUnity: A Platform for Multimodal Data-Driven XR"></a>psiUnity: A Platform for Multimodal Data-Driven XR</h2><p><strong>Authors:Akhil Ajikumar, Sahil Mayenkar, Steven Yoo, Sakib Reza, Mohsen Moghaddam</strong></p>
<p>Extended reality (XR) research increasingly relies on the ability to stream and synchronize multimodal data between headsets and immersive applications for data-driven interaction and experimentation. However, developers face a critical gap: the Platform for Situated Intelligence (psi), which excels at deterministic temporal alignment and multimodal data management, has been largely inaccessible to the dominant Unity&#x2F;MRTK ecosystem used for HoloLens development. We introduce psiUnity, an open-source C# integration that bridges psiâ€™s .NET libraries with Unity 2022.3 and MRTK3 for HoloLens 2. psiUnity enables bidirectional, real-time streaming of head pose, hand tracking, gaze, IMU, audio, and depth sensor data (AHAT and long-throw) with microsecond-level temporal precision, allowing Unity applications to both consume and produce synchronized multimodal data streams. By embedding psiâ€™s native serialization, logging, and temporal coordination directly within Unityâ€™s architecture, psiUnity extends psi beyond its previous StereoKit limitations and empowers the HRI, HCI, and embodied-AI communities to develop reproducible, data-driven XR interactions and experiments within the familiar Unity environment. The integration is available at <a target="_blank" rel="noopener" href="https://github.com/sailgt/psiUnity">https://github.com/sailgt/psiUnity</a>.</p>
<blockquote>
<p>æ‰©å±•ç°å®ï¼ˆXRï¼‰ç ”ç©¶è¶Šæ¥è¶Šä¾èµ–äºåœ¨å¤´æˆ´è®¾å¤‡å’Œæ²‰æµ¸å¼åº”ç”¨ç¨‹åºä¹‹é—´æµå¼ä¼ è¾“å’ŒåŒæ­¥å¤šæ¨¡æ€æ•°æ®çš„èƒ½åŠ›ï¼Œä»¥å®ç°æ•°æ®é©±åŠ¨çš„äº¤äº’å’Œå®éªŒã€‚ç„¶è€Œï¼Œå¼€å‘è€…é¢ä¸´ä¸€ä¸ªå…³é”®çš„é¸¿æ²Ÿï¼šæ“…é•¿ç¡®å®šæ€§æ—¶é—´å¯¹é½å’Œå¤šæ¨¡æ€æ•°æ®ç®¡ç†çš„æƒ…å¢ƒæ™ºèƒ½å¹³å°ï¼ˆpsiï¼‰åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ— æ³•è¢«ç”¨äºHoloLenså¼€å‘çš„Unity&#x2F;MRTKç”Ÿæ€ç³»ç»Ÿæ‰€è®¿é—®ã€‚æˆ‘ä»¬æ¨å‡ºäº†psiUnityï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„C#é›†æˆï¼Œæ—¨åœ¨å°†psiçš„.NETåº“ä¸Unity 2022.3å’ŒMRTK3ï¼ˆç”¨äºHoloLens 2ï¼‰è¿›è¡Œé›†æˆã€‚psiUnityä½¿åŒå‘å®æ—¶æµå¼ä¼ è¾“å¤´éƒ¨å§¿æ€ã€æ‰‹éƒ¨è·Ÿè¸ªã€æ³¨è§†ã€IMUã€éŸ³é¢‘å’Œæ·±åº¦ä¼ æ„Ÿå™¨æ•°æ®ï¼ˆAHATå’Œé•¿è·ç¦»ï¼‰æˆä¸ºå¯èƒ½ï¼Œå…·æœ‰å¾®ç§’çº§çš„æ—¶é—´ç²¾åº¦ï¼Œå…è®¸Unityåº”ç”¨ç¨‹åºåŒæ—¶æ¶ˆè´¹å’Œç”Ÿäº§åŒæ­¥å¤šæ¨¡æ€æ•°æ®æµã€‚é€šè¿‡å°†psiçš„åŸç”Ÿåºåˆ—åŒ–ã€æ—¥å¿—è®°å½•å’Œä¸´æ—¶åè°ƒç›´æ¥åµŒå…¥Unityçš„æ¶æ„ä¸­ï¼ŒpsiUnityè¶…è¶Šäº†å…¶ä¹‹å‰çš„StereoKité™åˆ¶ï¼Œå¹¶ä½¿HRIã€HCIå’Œå®ä½“AIç¤¾åŒºèƒ½å¤Ÿåœ¨ç†Ÿæ‚‰çš„Unityç¯å¢ƒä¸­å¼€å‘å¯é‡å¤çš„ã€æ•°æ®é©±åŠ¨çš„XRäº¤äº’å’Œå®éªŒã€‚è¯¥é›†æˆå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sailgt/psiUnity">https://github.com/sailgt/psiUnity</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05304v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†Extended Realityï¼ˆXRï¼‰ç ”ç©¶ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå³éœ€è¦æµåŒ–å’ŒåŒæ­¥å¤´æˆ´è®¾å¤‡å’Œæ²‰æµ¸å¼åº”ç”¨ç¨‹åºä¹‹é—´çš„å¤šæ¨¡æ€æ•°æ®ä»¥å®ç°æ•°æ®é©±åŠ¨çš„äº¤äº’å’Œå®éªŒã€‚ä¸ºæ­¤ï¼Œè¯¥æ–‡ä»‹ç»äº†psiUnityè¿™ä¸€å¼€æºC#é›†æˆè§£å†³æ–¹æ¡ˆï¼Œå®ƒæˆåŠŸåœ°å°†psiçš„.NETåº“ä¸Unity 2022.3å’ŒMRTK3è¿æ¥èµ·æ¥ï¼Œä½¿å¾—ä¸ºHoloLens 2å¼€å‘çš„ç¨‹åºèƒ½å¤Ÿè¿›è¡ŒåŒå‘å®æ—¶æµåŒ–åŒ…æ‹¬å¤´éƒ¨å§¿æ€ã€æ‰‹éƒ¨è¿½è¸ªã€æ³¨è§†ã€IMUã€éŸ³é¢‘å’Œæ·±åº¦ä¼ æ„Ÿå™¨æ•°æ®ç­‰ï¼ˆAHATå’Œé•¿è·ç¦»æŠ•å°„ï¼‰ï¼Œå¹¶è¾¾åˆ°å¾®ç§’çº§çš„æ—¶åºç²¾åº¦ã€‚è¿™ä¸ºUnityåº”ç”¨ç¨‹åºæä¾›äº†æ¶ˆè´¹å’Œç”Ÿäº§åŒæ­¥å¤šæ¨¡æ€æ•°æ®æµçš„èƒ½åŠ›ã€‚é€šè¿‡å°†psiçš„åŸç”Ÿåºåˆ—åŒ–ã€æ—¥å¿—è®°å½•å’Œæ—¶åºåè°ƒç›´æ¥åµŒå…¥Unityæ¶æ„ä¸­ï¼ŒpsiUnityä¸ä»…è¶…è¶Šäº†ä¹‹å‰çš„StereoKité™åˆ¶ï¼Œè¿˜ä¸ºHRIã€HCIå’Œembodied-AIç¤¾åŒºæä¾›äº†åœ¨ç†Ÿæ‚‰çš„Unityç¯å¢ƒä¸­å¼€å‘å¯é‡å¤çš„ã€æ•°æ®é©±åŠ¨çš„XRäº¤äº’å’Œå®éªŒçš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Extended Reality (XR) ç ”ç©¶éœ€è¦æµåŒ–å’ŒåŒæ­¥å¤´æˆ´è®¾å¤‡å’Œæ²‰æµ¸å¼åº”ç”¨ä¹‹é—´çš„å¤šæ¨¡æ€æ•°æ®ã€‚</li>
<li>å­˜åœ¨ä¸€ä¸ªå…³é”®å·®è·ï¼šç”¨äºHoloLenså¼€å‘çš„psiå¹³å°æœªèƒ½ä¸ä¸»æµçš„Unity&#x2F;MRTKç”Ÿæ€ç³»ç»Ÿå®ç°èåˆã€‚</li>
<li>psiUnityæ˜¯é¦–ä¸ªå®ç°è¿™ä¸€èåˆçš„è§£å†³æ–¹æ¡ˆï¼Œå®ƒæ˜¯ä¸€ä¸ªå¼€æºçš„C#é›†æˆè§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨è¿æ¥psiçš„.NETåº“ä¸Unity 2022.3å’ŒMRTK3ã€‚</li>
<li>psiUnityèƒ½å¤Ÿå®ç°å¤´éƒ¨å§¿æ€ã€æ‰‹éƒ¨è¿½è¸ªç­‰å¤šç§æ•°æ®çš„åŒå‘å®æ—¶æµåŒ–ã€‚è¿™ç§æµåŒ–èƒ½å¤Ÿè¾¾åˆ°å¾®ç§’çº§çš„æ—¶åºç²¾åº¦ã€‚</li>
<li>psiUnityä½¿å¾—Unityåº”ç”¨ç¨‹åºæ—¢èƒ½æ¶ˆè´¹ä¹Ÿèƒ½äº§ç”ŸåŒæ­¥çš„å¤šæ¨¡æ€æ•°æ®æµã€‚</li>
<li>psiUnityé€šè¿‡å°†åŸç”Ÿåºåˆ—åŒ–ç­‰åŠŸèƒ½ç›´æ¥åµŒå…¥Unityæ¶æ„ä¸­ï¼Œæ‰©å±•äº†psiçš„åŠŸèƒ½å¹¶è¶…è¶Šäº†ä¹‹å‰çš„StereoKité™åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05304">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e0f5fa6612127ef340505918de276075" align="middle">
<img src="https://picx.zhimg.com/v2-ba68c2d777eff86014fb1cc26be8412d" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Enhancing-Public-Speaking-Skills-in-Engineering-Students-Through-AI"><a href="#Enhancing-Public-Speaking-Skills-in-Engineering-Students-Through-AI" class="headerlink" title="Enhancing Public Speaking Skills in Engineering Students Through AI"></a>Enhancing Public Speaking Skills in Engineering Students Through AI</h2><p><strong>Authors:Amol Harsh, Brainerd Prince, Siddharth Siddharth, Deepan Raj Prabakar Muthirayan, Kabir S Bhalla, Esraaj Sarkar Gupta, Siddharth Sahu</strong></p>
<p>This research-to-practice full paper was inspired by the persistent challenge in effective communication among engineering students. Public speaking is a necessary skill for future engineers as they have to communicate technical knowledge with diverse stakeholders. While universities offer courses or workshops, they are unable to offer sustained and personalized training to students. Providing comprehensive feedback on both verbal and non-verbal aspects of public speaking is time-intensive, making consistent and individualized assessment impractical. This study integrates research on verbal and non-verbal cues in public speaking to develop an AI-driven assessment model for engineering students. Our approach combines speech analysis, computer vision, and sentiment detection into a multi-modal AI system that provides assessment and feedback. The model evaluates (1) verbal communication (pitch, loudness, pacing, intonation), (2) non-verbal communication (facial expressions, gestures, posture), and (3) expressive coherence, a novel integration ensuring alignment between speech and body language. Unlike previous systems that assess these aspects separately, our model fuses multiple modalities to deliver personalized, scalable feedback. Preliminary testing demonstrated that our AI-generated feedback was moderately aligned with expert evaluations. Among the state-of-the-art AI models evaluated, all of which were Large Language Models (LLMs), including Gemini and OpenAI models, Gemini Pro emerged as the best-performing, showing the strongest agreement with human annotators. By eliminating reliance on human evaluators, this AI-driven public speaking trainer enables repeated practice, helping students naturally align their speech with body language and emotion, crucial for impactful and professional communication.</p>
<blockquote>
<p>æœ¬æ–‡æ˜¯ä¸€ç¯‡ä»ç ”ç©¶åˆ°å®è·µçš„å…¨æ–‡ï¼Œçµæ„Ÿæ¥æºäºå·¥ç¨‹å­¦ç”Ÿä¹‹é—´æœ‰æ•ˆæ²Ÿé€šçš„æŒä¹…æŒ‘æˆ˜ã€‚å…¬å…±æ¼”è®²æ˜¯æœªæ¥å·¥ç¨‹å¸ˆå¿…å¤‡çš„æŠ€èƒ½ï¼Œå› ä¸ºä»–ä»¬éœ€è¦ä¸å„ç§åˆ©ç›Šç›¸å…³è€…äº¤æµæŠ€æœ¯çŸ¥è¯†ã€‚è™½ç„¶å¤§å­¦æä¾›è¯¾ç¨‹æˆ–ç ”è®¨ä¼šï¼Œä½†å®ƒä»¬æ— æ³•ä¸ºå­¦ç”Ÿæä¾›æŒç»­å’Œä¸ªæ€§åŒ–çš„åŸ¹è®­ã€‚å¯¹å…¬å…±æ¼”è®²çš„è¨€è¯­å’Œéè¨€è¯­æ–¹é¢æä¾›å…¨é¢çš„åé¦ˆæ˜¯éå¸¸è€—æ—¶çš„ï¼Œä½¿å¾—ä¸€è‡´å’Œä¸ªæ€§åŒ–çš„è¯„ä¼°ä¸åˆ‡å®é™…ã€‚æœ¬ç ”ç©¶æ•´åˆäº†å…¬å…±æ¼”è®²ä¸­çš„è¨€è¯­å’Œéè¨€è¯­çº¿ç´¢çš„ç ”ç©¶ï¼Œä»¥å¼€å‘ä¸€ä¸ªç”¨äºå·¥ç¨‹å­¦ç”Ÿçš„AIé©±åŠ¨è¯„ä¼°æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†è¯­éŸ³åˆ†æã€è®¡ç®—æœºè§†è§‰å’Œæƒ…æ„Ÿæ£€æµ‹ï¼Œå½¢æˆä¸€ä¸ªå¤šæ¨¡å¼AIç³»ç»Ÿï¼Œæä¾›è¯„ä¼°å’Œåé¦ˆã€‚è¯¥æ¨¡å‹è¯„ä¼°ï¼ˆ1ï¼‰è¨€è¯­äº¤æµï¼ˆéŸ³è°ƒã€éŸ³é‡ã€è¯­é€Ÿã€è¯­è°ƒï¼‰ã€ï¼ˆ2ï¼‰éè¨€è¯­äº¤æµï¼ˆé¢éƒ¨è¡¨æƒ…ã€æ‰‹åŠ¿ã€å§¿åŠ¿ï¼‰ï¼Œä»¥åŠï¼ˆ3ï¼‰è¡¨è¾¾ä¸€è‡´æ€§ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹é›†æˆï¼Œç¡®ä¿è¨€è¯­å’Œè‚¢ä½“è¯­è¨€ä¹‹é—´çš„å¯¹é½ã€‚ä¸ä»¥å‰åˆ†åˆ«è¯„ä¼°è¿™äº›æ–¹é¢çš„ç³»ç»Ÿä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹èåˆäº†å¤šç§æ¨¡å¼ï¼Œä»¥æä¾›ä¸ªæ€§åŒ–ã€å¯æ‰©å±•çš„åé¦ˆã€‚åˆæ­¥æµ‹è¯•è¡¨æ˜ï¼Œæˆ‘ä»¬AIç”Ÿæˆçš„åé¦ˆä¸ä¸“å®¶è¯„ä¼°ä¸­åº¦å¯¹é½ã€‚åœ¨è¯„ä¼°çš„å…ˆè¿›AIæ¨¡å‹ä¸­ï¼ŒåŒ…æ‹¬åŒå­åº§å’ŒOpenAIæ¨¡å‹ç­‰æ‰€æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼ŒåŒå­åº§ä¸“ä¸šç‰ˆè¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œä¸äººç±»æ³¨é‡Šè€…çš„å¥‘åˆåº¦æœ€é«˜ã€‚é€šè¿‡æ¶ˆé™¤å¯¹äººç±»è¯„ä¼°è€…çš„ä¾èµ–ï¼Œè¿™ä¸ªAIé©±åŠ¨çš„å…¬å…±æ¼”è®²è®­ç»ƒå™¨å¯ä»¥é‡å¤ç»ƒä¹ ï¼Œå¸®åŠ©å­¦ç”Ÿè‡ªç„¶åœ°è°ƒæ•´ä»–ä»¬çš„æ¼”è®²ä¸è‚¢ä½“è¯­è¨€å’Œæƒ…æ„Ÿçš„ä¸€è‡´æ€§ï¼Œè¿™å¯¹äºæœ‰å†²å‡»åŠ›å’Œä¸“ä¸šçš„æ²Ÿé€šè‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04995v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶ç»“åˆå·¥ç¨‹å­¦ç”Ÿåœ¨æ²Ÿé€šæ–¹é¢æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§äººå·¥æ™ºèƒ½é©±åŠ¨çš„è¯„ä¼°æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå¯¹å­¦ç”Ÿçš„æ¼”è®²æŠ€èƒ½è¿›è¡Œå…¨é¢è¯„ä¼°å¹¶ç»™å‡ºåé¦ˆã€‚æ¨¡å‹èåˆäº†è¯­éŸ³åˆ†æã€è®¡ç®—æœºè§†è§‰å’Œæƒ…æ„Ÿæ£€æµ‹ç­‰æŠ€æœ¯ï¼Œä»å¤šä¸ªç»´åº¦å¯¹å­¦ç”Ÿçš„å£å¤´å’Œéå£å¤´æ²Ÿé€šèƒ½åŠ›è¿›è¡Œè¯„ä¼°ï¼Œå¹¶æä¾›ä¸ªæ€§åŒ–çš„åé¦ˆã€‚åˆæ­¥æµ‹è¯•è¡¨æ˜ï¼Œè¯¥æ¨¡å‹çš„è¯„ä¼°ç»“æœä¸ä¸“å®¶è¯„ä»·åŸºæœ¬ä¸€è‡´ã€‚åœ¨å¤šç§å…ˆè¿›çš„AIæ¨¡å‹ä¸­ï¼ŒGemini Proè¡¨ç°æœ€ä½³ï¼Œä¸äººç±»è¯„ä»·è€…çš„è¯„ä¼°ç»“æœæœ€ä¸ºæ¥è¿‘ã€‚è¯¥æ¨¡å‹å¯å¸®åŠ©å­¦ç”Ÿæé«˜æ²Ÿé€šæŠ€èƒ½ï¼Œä¸ºæœªæ¥çš„å·¥ç¨‹å¸ˆæä¾›ä¸“ä¸šä¸”æœ‰å½±å“åŠ›çš„æ²Ÿé€šæŠ€å·§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å·¥ç¨‹å­¦ç”Ÿåœ¨æ²Ÿé€šæ–¹é¢çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å…¬å…±æ¼”è®²æŠ€èƒ½çš„åŸ¹å…»ã€‚</li>
<li>æ¨¡å‹èåˆäº†è¯­éŸ³åˆ†æã€è®¡ç®—æœºè§†è§‰å’Œæƒ…æ„Ÿæ£€æµ‹ç­‰æŠ€æœ¯ï¼Œå¤šç»´åº¦è¯„ä¼°å­¦ç”Ÿçš„å£å¤´è¡¨è¾¾èƒ½åŠ›ã€‚</li>
<li>åé¦ˆæœºåˆ¶æ³¨é‡å­¦ç”Ÿçš„éè¯­è¨€æ²Ÿé€šèƒ½åŠ›ï¼ŒåŒ…æ‹¬é¢éƒ¨è¡¨æƒ…ã€å§¿åŠ¿å’Œæ‰‹åŠ¿ç­‰ã€‚</li>
<li>åˆ›æ–°æ€§åœ°æå‡ºäº†â€œè¡¨è¾¾ä¸€è‡´æ€§â€çš„è¯„ä¼°æ ‡å‡†ï¼Œç¡®ä¿å£è¯­å’Œè‚¢ä½“è¯­è¨€ä¹‹é—´çš„åè°ƒã€‚</li>
<li>åœ¨åˆæ­¥æµ‹è¯•ä¸­ï¼ŒAIæ¨¡å‹çš„è¯„ä¼°ç»“æœä¸ä¸“å®¶è¯„ä»·ä¿æŒä¸€è‡´ã€‚</li>
<li>åœ¨å¤šä¸ªAIæ¨¡å‹ä¸­ï¼ŒGemini Proè¡¨ç°æœ€ä½³ï¼Œä¸äººç±»è¯„ä»·è€…çš„è¯„ä¼°ç»“æœæœ€ä¸ºæ¥è¿‘ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f612fcb7b3dde2f0c294d313821addbe" align="middle">
<img src="https://picx.zhimg.com/v2-911e5823c37d840663aca72e6e62cc61" align="middle">
<img src="https://picx.zhimg.com/v2-0ab215c9ed43a58d37c50130e9cd1b94" align="middle">
<img src="https://picx.zhimg.com/v2-463cf84b92b9272b6c5b3038d711d6bb" align="middle">
<img src="https://picx.zhimg.com/v2-2bdbb6ed48be3339d0101405ab9cce6f" align="middle">
<img src="https://picx.zhimg.com/v2-2919b2a0db7b2e7293dc27d3bbcb5878" align="middle">
<img src="https://picx.zhimg.com/v2-269983832e2a2976b63a2e45ec49a8a9" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Caption-Injection-for-Optimization-in-Generative-Search-Engine"><a href="#Caption-Injection-for-Optimization-in-Generative-Search-Engine" class="headerlink" title="Caption Injection for Optimization in Generative Search Engine"></a>Caption Injection for Optimization in Generative Search Engine</h2><p><strong>Authors:Xiaolu Chen, Yong Liao</strong></p>
<p>Generative Search Engines (GSEs) leverage Retrieval-Augmented Generation (RAG) techniques and Large Language Models (LLMs) to integrate multi-source information and provide users with accurate and comprehensive responses. Unlike traditional search engines that present results in ranked lists, GSEs shift usersâ€™ attention from sequential browsing to content-driven subjective perception, driving a paradigm shift in information retrieval. In this context, enhancing the subjective visibility of content through Generative Search Engine Optimization (G-SEO) methods has emerged as a new research focus. With the rapid advancement of Multimodal Retrieval-Augmented Generation (MRAG) techniques, GSEs can now efficiently integrate text, images, audio, and video, producing richer responses that better satisfy complex information needs. Existing G-SEO methods, however, remain limited to text-based optimization and fail to fully exploit multimodal data. To address this gap, we propose Caption Injection, the first multimodal G-SEO approach, which extracts captions from images and injects them into textual content, integrating visual semantics to enhance the subjective visibility of content in generative search scenarios. We systematically evaluate Caption Injection on MRAMG, a benchmark for MRAG, under both unimodal and multimodal settings. Experimental results show that Caption Injection significantly outperforms text-only G-SEO baselines under the G-Eval metric, demonstrating the necessity and effectiveness of multimodal integration in G-SEO to improve user-perceived content visibility.</p>
<blockquote>
<p>ç”Ÿæˆå¼æœç´¢å¼•æ“ï¼ˆGSEï¼‰åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ•´åˆå¤šæºä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®è€Œå…¨é¢çš„å›ç­”ã€‚ä¸ä¼ ç»Ÿçš„ä»…å‘ˆç°æ’åç»“æœåˆ—è¡¨çš„æœç´¢å¼•æ“ä¸åŒï¼ŒGSEå°†ç”¨æˆ·çš„æ³¨æ„åŠ›ä»é¡ºåºæµè§ˆè½¬å˜ä¸ºå†…å®¹é©±åŠ¨çš„ä¸»è§‚æ„ŸçŸ¥ï¼Œæ¨åŠ¨äº†ä¿¡æ¯æ£€ç´¢çš„èŒƒå¼è½¬å˜ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œé€šè¿‡ç”Ÿæˆå¼æœç´¢å¼•æ“ä¼˜åŒ–ï¼ˆG-SEOï¼‰æ–¹æ³•æ¥æé«˜å†…å®¹çš„ä¸»è§‚å¯è§æ€§å·²æˆä¸ºæ–°çš„ç ”ç©¶é‡ç‚¹ã€‚éšç€å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMRAGï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼ŒGSEç°åœ¨å¯ä»¥æœ‰æ•ˆåœ°æ•´åˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ï¼Œäº§ç”Ÿæ›´ä¸°å¯Œã€æ›´èƒ½æ»¡è¶³å¤æ‚ä¿¡æ¯éœ€æ±‚çš„å›ç­”ã€‚ç„¶è€Œï¼Œç°æœ‰çš„G-SEOæ–¹æ³•ä»ç„¶å±€é™äºåŸºäºæ–‡æœ¬çš„ä¼˜åŒ–ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€æ•°æ®ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†â€œCaption Injectionâ€è¿™ä¸€é¦–ä¸ªå¤šæ¨¡æ€G-SEOæ–¹æ³•ï¼Œå®ƒä»å›¾åƒä¸­æå–å­—å¹•å¹¶å°†å…¶æ³¨å…¥æ–‡æœ¬å†…å®¹ä¸­ï¼Œæ•´åˆè§†è§‰è¯­ä¹‰æ¥æé«˜ç”Ÿæˆåœºæ™¯ä¸­å†…å®¹çš„ä¸»è§‚å¯è§æ€§ã€‚æˆ‘ä»¬åœ¨MRAMGè¿™ä¸€MRAGåŸºå‡†æµ‹è¯•ä¸‹ç³»ç»Ÿåœ°è¯„ä¼°äº†Caption Injectionåœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€è®¾ç½®ä¸‹çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨G-EvalæŒ‡æ ‡ä¸‹ï¼ŒCaption Injectionæ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨æ–‡æœ¬çš„G-SEOåŸºçº¿ï¼Œè¯æ˜äº†åœ¨å¤šæ¨¡æ€é›†æˆåœ¨G-SEOä¸­çš„å¿…è¦æ€§å’Œæœ‰æ•ˆæ€§ï¼Œä»¥æé«˜ç”¨æˆ·æ„ŸçŸ¥çš„å†…å®¹å¯è§æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04080v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç”Ÿæˆå¼æœç´¢å¼•æ“ï¼ˆGSEï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæºä¿¡æ¯èåˆæŠ€æœ¯ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥è·å¾—ç²¾å‡†å…¨é¢çš„å“åº”ã€‚ä¸åŒäºä¼ ç»Ÿæœç´¢å¼•æ“çš„ç»“æœæ’åå±•ç¤ºæ–¹å¼ï¼ŒGSEå°†ç”¨æˆ·çš„æ³¨æ„åŠ›ä»é¡ºåºæµè§ˆè½¬å˜ä¸ºå†…å®¹é©±åŠ¨çš„æ„Ÿæ€§è®¤çŸ¥ï¼Œå®ç°äº†ä¿¡æ¯æ£€ç´¢çš„èŒƒå¼è½¬å˜ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œé€šè¿‡ç”Ÿæˆå¼æœç´¢å¼•æ“ä¼˜åŒ–ï¼ˆG-SEOï¼‰æ–¹æ³•æå‡å†…å®¹çš„æ„Ÿæ€§å¯è§åº¦å·²æˆä¸ºæ–°çš„ç ”ç©¶ç„¦ç‚¹ã€‚éšç€å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆMRAGï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼ŒGSEèƒ½å¤Ÿé«˜æ•ˆèåˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ï¼Œäº§ç”Ÿæ›´ä¸°å¯Œã€æ›´æ»¡è¶³å¤æ‚ä¿¡æ¯éœ€æ±‚çš„å“åº”ã€‚ç„¶è€Œï¼Œç°æœ‰çš„G-SEOæ–¹æ³•ä»…é™äºæ–‡æœ¬ä¼˜åŒ–ï¼Œæœªèƒ½å……åˆ†åˆ©ç”¨å¤šæ¨¡æ€æ•°æ®ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºâ€œCaption Injectionâ€ä½œä¸ºé¦–ä¸ªå¤šæ¨¡æ€G-SEOæ–¹æ³•ï¼Œé€šè¿‡æå–å›¾åƒå­—å¹•å¹¶æ³¨å…¥æ–‡æœ¬å†…å®¹ï¼Œæ•´åˆè§†è§‰è¯­ä¹‰ä¿¡æ¯ï¼Œæå‡ç”Ÿæˆåœºæ™¯ä¸­å†…å®¹çš„æ„Ÿæ€§å¯è§åº¦ã€‚ç³»ç»Ÿè¯„ä¼°è¡¨æ˜ï¼Œåœ¨MRAGçš„åŸºå‡†æµ‹è¯•MRAMGä¸Šï¼Œæ— è®ºæ˜¯å•æ¨¡æ€è¿˜æ˜¯å¤šæ¨¡æ€ç¯å¢ƒä¸‹ï¼ŒCaption Injectionå‡æ˜¾è‘—ä¼˜äºä»…æ–‡æœ¬çš„G-SEOåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GSEsæ•´åˆå¤šæºä¿¡æ¯ï¼Œæä¾›ç²¾å‡†å…¨é¢çš„å“åº”ã€‚</li>
<li>ä¼ ç»Ÿæœç´¢å¼•æ“ä¸»è¦ä¾èµ–ç»“æœæ’åå±•ç¤ºæ–¹å¼ï¼Œè€ŒGSEsæ¨åŠ¨ä¿¡æ¯æ£€ç´¢ä»é¡ºåºæµè§ˆå‘å†…å®¹æ„ŸçŸ¥è½¬å˜ã€‚</li>
<li>G-SEOæ–¹æ³•æ—¨åœ¨æå‡å†…å®¹çš„æ„Ÿæ€§å¯è§åº¦ã€‚</li>
<li>MRAGæŠ€æœ¯ä½¿å¾—GSEèƒ½å¤Ÿèåˆå¤šç§æ¨¡æ€æ•°æ®ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ï¼‰ã€‚</li>
<li>ç°æœ‰G-SEOæ–¹æ³•ä¸»è¦å±€é™äºæ–‡æœ¬ä¼˜åŒ–ã€‚</li>
<li>Caption Injectionä½œä¸ºé¦–ä¸ªå¤šæ¨¡æ€G-SEOæ–¹æ³•ï¼Œé€šè¿‡æ•´åˆè§†è§‰è¯­ä¹‰ä¿¡æ¯æå‡å†…å®¹å¯è§åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4172dd5f0c752eea261681f7cf87342a" align="middle">
<img src="https://picx.zhimg.com/v2-8fc08329e389c3018cbf224890336e95" align="middle">
<img src="https://picx.zhimg.com/v2-e9522808b3eb3cbee18da9aed8841be5" align="middle">
<img src="https://picx.zhimg.com/v2-bc92a0d88585d8dd87e5b2ba6df7dde2" align="middle">
<img src="https://picx.zhimg.com/v2-433c60a0b485cdf60c3d12f09961c4bd" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Laugh-Relate-Engage-Stylized-Comment-Generation-for-Short-Videos"><a href="#Laugh-Relate-Engage-Stylized-Comment-Generation-for-Short-Videos" class="headerlink" title="Laugh, Relate, Engage: Stylized Comment Generation for Short Videos"></a>Laugh, Relate, Engage: Stylized Comment Generation for Short Videos</h2><p><strong>Authors:Xuan Ouyang, Senan Wang, Bouzhou Wang, Siyuan Xiahou, Jinrong Zhou, Yuekang Li</strong></p>
<p>Short-video platforms have become a central medium in the modern Internet landscape, where efficient information delivery and strong interactivity are reshaping user engagement and cultural dissemination. Among the various forms of user interaction, comments play a vital role in fostering community participation and enabling content re-creation. However, generating comments that are both compliant with platform guidelines and capable of exhibiting stylistic diversity and contextual awareness remains a significant challenge. We introduce LOLGORITHM, a modular multi-agent system (MAS) designed for controllable short-video comment generation. The system integrates video segmentation, contextual and affective analysis, and style-aware prompt construction. It supports six distinct comment styles: puns (homophones), rhyming, meme application, sarcasm (irony), plain humor, and content extraction. Powered by a multimodal large language model (MLLM), LOLGORITHM directly processes video inputs and achieves fine-grained style control through explicit prompt markers and few-shot examples. To support development and evaluation, we construct a bilingual dataset using official APIs from Douyin (Chinese) and YouTube (English), covering five popular video genres: comedy skits, daily life jokes, funny animal clips, humorous commentary, and talk shows. Evaluation combines automated metrics originality, relevance, and style conformity with a large-scale human preference study involving 40 videos and 105 participants. Results show that LOLGORITHM significantly outperforms baseline models, achieving preference rates of over 90% on Douyin and 87.55% on YouTube. This work presents a scalable and culturally adaptive framework for stylized comment generation on short-video platforms, offering a promising path to enhance user engagement and creative interaction.</p>
<blockquote>
<p>çŸ­è§†é¢‘å¹³å°å·²æˆä¸ºç°ä»£äº’è”ç½‘æ™¯è§‚ä¸­ä¸€ç§é‡è¦çš„åª’ä»‹ï¼Œé«˜æ•ˆçš„ä¿¡æ¯ä¼ é€’å’Œå¼ºå¤§çš„äº’åŠ¨æ€§æ­£åœ¨é‡å¡‘ç”¨æˆ·å‚ä¸å’Œæ–‡åŒ–ä¼ æ’­çš„æ–¹å¼ã€‚åœ¨å¤šç§å½¢å¼çš„ç”¨æˆ·äº’åŠ¨ä¸­ï¼Œè¯„è®ºå¯¹äºä¿ƒè¿›ç¤¾åŒºå‚ä¸å’Œæ¨åŠ¨å†…å®¹å†åˆ›ä½œèµ·åˆ°äº†è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç”Ÿæˆæ—¢ç¬¦åˆå¹³å°æŒ‡å—åˆå…·å¤‡é£æ ¼å¤šæ ·æ€§å’Œä¸Šä¸‹æ–‡æ„è¯†çš„è¯„è®ºä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†LOLORITHMï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºå¯æ§çŸ­è§†é¢‘è¯„è®ºç”Ÿæˆè®¾è®¡çš„æ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ã€‚è¯¥ç³»ç»Ÿèåˆäº†è§†é¢‘åˆ†å‰²ã€ä¸Šä¸‹æ–‡å’Œæƒ…æ„Ÿåˆ†æä»¥åŠé£æ ¼æ„ŸçŸ¥æç¤ºæ„å»ºã€‚å®ƒæ”¯æŒå…­ç§ä¸åŒçš„è¯„è®ºé£æ ¼ï¼šåŒå…³è¯­ã€æŠ¼éŸµã€æ¨¡å› åº”ç”¨ã€è®½åˆºã€æ™®é€šå¹½é»˜å’Œå†…å®¹æå–ã€‚LOLORITHMç”±å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰é©±åŠ¨ï¼Œç›´æ¥å¤„ç†è§†é¢‘è¾“å…¥ï¼Œå¹¶é€šè¿‡æ˜ç¡®çš„æç¤ºæ ‡è®°å’Œå°‘é‡ç¤ºä¾‹å®ç°ç²¾ç»†çš„é£æ ¼æ§åˆ¶ã€‚ä¸ºäº†æ”¯æŒå’Œè¯„ä¼°ï¼Œæˆ‘ä»¬ä½¿ç”¨æŠ–éŸ³ï¼ˆä¸­æ–‡ï¼‰å’ŒYouTubeï¼ˆè‹±æ–‡ï¼‰çš„å®˜æ–¹APIæ„å»ºäº†ä¸€ä¸ªåŒè¯­æ•°æ®é›†ï¼Œæ¶µç›–äº†äº”ç§æµè¡Œçš„è§†é¢‘ç±»å‹ï¼šå–œå‰§çŸ­ç‰‡ã€æ—¥å¸¸ç”Ÿæ´»ç¬‘è¯ã€æœ‰è¶£çš„åŠ¨ç‰©å‰ªè¾‘ã€å¹½é»˜è¯„è®ºå’Œè°ˆè¯èŠ‚ç›®ã€‚è¯„ä¼°ç»“åˆäº†è‡ªåŠ¨åŒ–æŒ‡æ ‡ï¼ˆå¦‚åŸåˆ›æ€§ã€ç›¸å…³æ€§å’Œé£æ ¼ä¸€è‡´æ€§ï¼‰ä»¥åŠæ¶‰åŠ40ä¸ªè§†é¢‘å’Œ105åå‚ä¸è€…çš„å¤§è§„æ¨¡äººç±»åå¥½ç ”ç©¶ã€‚ç»“æœè¡¨æ˜ï¼ŒLOLORITHMæ˜¾è‘—ä¼˜äºåŸºå‡†æ¨¡å‹ï¼Œåœ¨æŠ–éŸ³ä¸Šçš„åå¥½ç‡è¶…è¿‡90%ï¼Œåœ¨YouTubeä¸Šçš„åå¥½ç‡ä¸º87.55%ã€‚è¿™é¡¹å·¥ä½œä¸ºçŸ­è§†é¢‘å¹³å°ä¸Šçš„é£æ ¼åŒ–è¯„è®ºç”Ÿæˆæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•ä¸”æ–‡åŒ–é€‚åº”çš„æ¡†æ¶ï¼Œä¸ºå¢å¼ºç”¨æˆ·å‚ä¸åº¦å’Œåˆ›é€ æ€§äº’åŠ¨æä¾›äº†æœ‰å¸Œæœ›çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03757v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>çŸ­è§†é¢‘å¹³å°å·²æˆä¸ºç°ä»£äº’è”ç½‘æ™¯è§‚ä¸­çš„æ ¸å¿ƒåª’ä»‹ï¼Œé«˜æ•ˆçš„ä¿¡æ¯ä¼ é€’å’Œå¼ºå¤§çš„äº¤äº’æ€§æ­£åœ¨é‡å¡‘ç”¨æˆ·å‚ä¸å’Œæ–‡åŒ–ä¼ æ’­ã€‚è¯„è®ºåœ¨ä¿ƒè¿›ç¤¾åŒºå‚ä¸å’Œå†…å®¹å†åˆ›ä½œæ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç”Ÿæˆæ—¢ç¬¦åˆå¹³å°è§„èŒƒåˆå…·å¤‡é£æ ¼å¤šæ ·æ€§å’Œä¸Šä¸‹æ–‡æ„è¯†çš„è¯„è®ºä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†LOLGORITHMï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºå¯æ§çŸ­è§†é¢‘è¯„è®ºç”Ÿæˆè®¾è®¡çš„æ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ã€‚è¯¥ç³»ç»Ÿèåˆäº†è§†é¢‘åˆ†å‰²ã€ä¸Šä¸‹æ–‡å’Œæƒ…æ„Ÿåˆ†æä»¥åŠé£æ ¼æ„ŸçŸ¥æç¤ºæ„å»ºã€‚å®ƒæ”¯æŒå…­ç§ä¸åŒçš„è¯„è®ºé£æ ¼ï¼šåŒå…³è¯­ã€æŠ¼éŸµã€æ¨¡å› åº”ç”¨ã€è®½åˆºã€æ™®é€šå¹½é»˜å’Œå†…å®¹æå–ã€‚å€ŸåŠ©å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼ŒLOLGORITHMç›´æ¥å¤„ç†è§†é¢‘è¾“å…¥ï¼Œå¹¶é€šè¿‡æ˜ç¡®çš„æç¤ºæ ‡è®°å’Œå°‘é‡ç¤ºä¾‹å®ç°ç²¾ç»†é£æ ¼æ§åˆ¶ã€‚ä¸ºäº†æ”¯æŒå’Œè¯„ä¼°ï¼Œæˆ‘ä»¬ä½¿ç”¨æŠ–éŸ³ï¼ˆä¸­æ–‡ï¼‰å’ŒYouTubeï¼ˆè‹±æ–‡ï¼‰çš„å®˜æ–¹APIæ„å»ºäº†ä¸€ä¸ªåŒè¯­æ•°æ®é›†ï¼Œæ¶µç›–äº†äº”ç§æµè¡Œçš„è§†é¢‘ç±»å‹ï¼šå–œå‰§çŸ­ç‰‡ã€æ—¥å¸¸ç”Ÿæ´»ç¬‘è¯ã€æœ‰è¶£çš„åŠ¨ç‰©å‰ªè¾‘ã€å¹½é»˜è¯„è®ºå’Œè„±å£ç§€ã€‚è¯„ä¼°ç»“åˆäº†è‡ªåŠ¨åŒ–æŒ‡æ ‡åŸåˆ›æ€§ã€ç›¸å…³æ€§å’Œé£æ ¼ä¸€è‡´æ€§ä»¥åŠå¤§è§„æ¨¡çš„äººç±»åå¥½ç ”ç©¶ï¼Œæ¶‰åŠ40ä¸ªè§†é¢‘å’Œ11åå‚ä¸è€…ã€‚ç»“æœè¡¨æ˜ï¼ŒLOLGORITHMæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œåœ¨æŠ–éŸ³ä¸Šçš„åå¥½ç‡è¾¾åˆ°9%ä»¥ä¸Šï¼ŒYouTubeä¸Šçš„åå¥½ç‡è¾¾åˆ°87.5%ã€‚è¿™é¡¹å·¥ä½œä¸ºçŸ­è§†é¢‘å¹³å°ä¸Šçš„é£æ ¼åŒ–è¯„è®ºç”Ÿæˆæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•å’Œæ–‡åŒ–é€‚åº”çš„æ¡†æ¶ï¼Œä¸ºå¢å¼ºç”¨æˆ·å‚ä¸åº¦å’Œåˆ›é€ æ€§äº’åŠ¨æä¾›äº†æœ‰å¸Œæœ›çš„é€”å¾„ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>çŸ­è§†é¢‘å¹³å°å·²æˆä¸ºç°ä»£äº’è”ç½‘çš„æ ¸å¿ƒï¼Œé«˜æ•ˆçš„ä¿¡æ¯ä¼ é€’å’Œå¼ºå¤§çš„äº¤äº’æ€§é‡å¡‘äº†ç”¨æˆ·å‚ä¸å’Œæ–‡åŒ–ä¼ æ’­ã€‚</li>
<li>è¯„è®ºåœ¨ä¿ƒè¿›ç¤¾åŒºå‚ä¸å’Œå†…å®¹å†åˆ›ä½œæ–¹é¢å‘æŒ¥å…³é”®ä½œç”¨ï¼Œä½†ç”Ÿæˆç¬¦åˆå¹³å°è§„èŒƒçš„è¯„è®ºå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>LOLGORITHMæ˜¯ä¸€ä¸ªä¸ºå¯æ§çŸ­è§†é¢‘è¯„è®ºç”Ÿæˆè®¾è®¡çš„æ¨¡å—åŒ–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰ã€‚</li>
<li>ç³»ç»Ÿæ”¯æŒå…­ç§è¯„è®ºé£æ ¼ï¼Œç»“åˆè§†é¢‘åˆ†å‰²ã€ä¸Šä¸‹æ–‡å’Œæƒ…æ„Ÿåˆ†æä»¥åŠé£æ ¼æ„ŸçŸ¥æç¤ºæ„å»ºæŠ€æœ¯ã€‚</li>
<li>ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å®ç°ç²¾ç»†é£æ ¼æ§åˆ¶ï¼Œèƒ½ç›´æ¥å¤„ç†è§†é¢‘è¾“å…¥ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªåŒè¯­æ•°æ®é›†ç”¨äºç ”ç©¶å’Œè¯„ä¼°ï¼Œæ¶µç›–äº†äº”ç§æµè¡Œçš„è§†é¢‘ç±»å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03757">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cbb0888c70871e54b39051c4f9b12d0a" align="middle">
<img src="https://picx.zhimg.com/v2-57a95f995d4fcc0e90b2f4b370e6f5a9" align="middle">
<img src="https://picx.zhimg.com/v2-b6b5028f968eff3eab48140abe589a18" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SigmaCollab-An-Application-Driven-Dataset-for-Physically-Situated-Collaboration"><a href="#SigmaCollab-An-Application-Driven-Dataset-for-Physically-Situated-Collaboration" class="headerlink" title="SigmaCollab: An Application-Driven Dataset for Physically Situated Collaboration"></a>SigmaCollab: An Application-Driven Dataset for Physically Situated Collaboration</h2><p><strong>Authors:Dan Bohus, Sean Andrist, Ann Paradiso, Nick Saw, Tim Schoonbeek, Maia Stiber</strong></p>
<p>We introduce SigmaCollab, a dataset enabling research on physically situated human-AI collaboration. The dataset consists of a set of 85 sessions in which untrained participants were guided by a mixed-reality assistive AI agent in performing procedural tasks in the physical world. SigmaCollab includes a set of rich, multimodal data streams, such as the participant and system audio, egocentric camera views from the head-mounted device, depth maps, head, hand and gaze tracking information, as well as additional annotations performed post-hoc. While the dataset is relatively small in size (~ 14 hours), its application-driven and interactive nature brings to the fore novel research challenges for human-AI collaboration, and provides more realistic testing grounds for various AI models operating in this space. In future work, we plan to use the dataset to construct a set of benchmarks for physically situated collaboration in mixed-reality task assistive scenarios. SigmaCollab is available at <a target="_blank" rel="noopener" href="https://github.com/microsoft/SigmaCollab">https://github.com/microsoft/SigmaCollab</a>.</p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†SigmaCollabæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ”¯æŒå¯¹ç‰©ç†ç¯å¢ƒä¸­äººç±»ä¸äººå·¥æ™ºèƒ½åˆä½œçš„ç ”ç©¶ã€‚æ•°æ®é›†åŒ…å«85ä¸ªä¼šè¯é›†ï¼Œå…¶ä¸­æœªç»è¿‡è®­ç»ƒçš„å‚ä¸è€…åœ¨ä¸€ä¸ªæ··åˆç°å®è¾…åŠ©äººå·¥æ™ºèƒ½ä»£ç†çš„æŒ‡å¯¼ä¸‹ï¼Œåœ¨ç‰©ç†ä¸–ç•Œä¸­æ‰§è¡Œç¨‹åºä»»åŠ¡ã€‚SigmaCollabåŒ…å«ä¸€ç»„ä¸°å¯Œçš„å¤šæ¨¡å¼æ•°æ®æµï¼Œå¦‚å‚ä¸è€…å’Œç³»ç»ŸéŸ³é¢‘ã€å¤´æˆ´è®¾å¤‡ä¸Šçš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ç›¸æœºè§†å›¾ã€æ·±åº¦å›¾ã€å¤´éƒ¨ã€æ‰‹éƒ¨ä»¥åŠæ³¨è§†è¿½è¸ªä¿¡æ¯ï¼Œä»¥åŠäº‹åè¿›è¡Œçš„é¢å¤–æ³¨é‡Šã€‚è™½ç„¶æ•°æ®é›†å¤§å°ç›¸å¯¹è¾ƒå°ï¼ˆçº¦14å°æ—¶ï¼‰ï¼Œä½†å…¶åº”ç”¨é©±åŠ¨å’Œäº¤äº’æ€§ä¸ºé’ˆå¯¹äººç±»ä¸äººå·¥æ™ºèƒ½åˆä½œçš„ç ”ç©¶å¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ï¼Œå¹¶ä¸ºå„ç§åœ¨æ­¤ç©ºé—´è¿è¡Œçš„AIæ¨¡å‹æä¾›äº†æ›´ç°å®çš„æµ‹è¯•å¹³å°ã€‚æœªæ¥ï¼Œæˆ‘ä»¬è®¡åˆ’ä½¿ç”¨è¯¥æ•°æ®é›†ä¸ºæ··åˆç°å®ä»»åŠ¡è¾…åŠ©åœºæ™¯ä¸­çš„ç‰©ç†ç¯å¢ƒåˆä½œæ„å»ºä¸€å¥—åŸºå‡†æµ‹è¯•ã€‚SigmaCollabå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/microsoft/SigmaCollab%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/microsoft/SigmaCollabè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02560v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>SigmaCollabæ•°æ®é›†ç”¨äºç ”ç©¶ç‰©ç†ç¯å¢ƒä¸­çš„äººç±»-äººå·¥æ™ºèƒ½åä½œã€‚è¯¥æ•°æ®é›†åŒ…å«85ä¸ªä¼šè¯ï¼Œå…¶ä¸­æœªç»è®­ç»ƒçš„å‚ä¸è€…åœ¨æ··åˆç°å®è¾…åŠ©AIä»£ç†çš„æŒ‡å¯¼ä¸‹å®Œæˆç‰©ç†ä¸–ç•Œçš„ç¨‹åºä»»åŠ¡ã€‚SigmaCollabåŒ…å«ä¸°å¯Œçš„å¤šæ¨¡å¼æ•°æ®æµï¼Œå¦‚å‚ä¸è€…å’Œç³»ç»ŸéŸ³é¢‘ã€æ¥è‡ªå¤´æˆ´è®¾å¤‡çš„ä¸ªäººè§†è§’ç›¸æœºè§†å›¾ã€æ·±åº¦å›¾ã€å¤´éƒ¨ã€æ‰‹éƒ¨ä»¥åŠæ³¨è§†è¿½è¸ªä¿¡æ¯ç­‰ï¼Œè¿˜åŒ…æ‹¬äº‹åè¿›è¡Œçš„é¢å¤–æ³¨é‡Šã€‚è™½ç„¶æ•°æ®é›†å¤§å°ç›¸å¯¹è¾ƒå°ï¼ˆçº¦14å°æ—¶ï¼‰ï¼Œä½†å…¶ä»¥åº”ç”¨ä¸ºå¯¼å‘å’Œäº¤äº’æ€§ä¸ºäººç±»ä¸äººå·¥æ™ºèƒ½çš„åä½œå¸¦æ¥äº†æ–°é¢–çš„ç ”ç©¶æŒ‘æˆ˜ï¼Œå¹¶ä¸ºè¿™ä¸€é¢†åŸŸçš„å„ç§äººå·¥æ™ºèƒ½æ¨¡å‹æä¾›äº†æ›´ç°å®çš„æµ‹è¯•ç¯å¢ƒã€‚æœªæ¥ï¼Œæˆ‘ä»¬è®¡åˆ’ä½¿ç”¨è¯¥æ•°æ®é›†ä¸ºæ··åˆç°å®ä»»åŠ¡è¾…åŠ©åœºæ™¯ä¸­çš„ç‰©ç†åä½œæ„å»ºä¸€ç³»åˆ—åŸºå‡†æµ‹è¯•ã€‚SigmaCollabæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/microsoft/SigmaCollab%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/microsoft/SigmaCollabè·å–ã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>SigmaCollabæ˜¯ä¸€ä¸ªç”¨äºç ”ç©¶ç‰©ç†ç¯å¢ƒä¸­äººç±»-AIåä½œçš„æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†åŒ…å«85ä¸ªä¼šè¯ï¼Œæ¶‰åŠå‚ä¸è€…åœ¨AIçš„æŒ‡å¯¼ä¸‹å®Œæˆç‰©ç†ä»»åŠ¡ã€‚</li>
<li>æ•°æ®é›†åŒ…å«ä¸°å¯Œçš„å¤šæ¨¡å¼æ•°æ®ï¼ŒåŒ…æ‹¬éŸ³é¢‘ã€è§†é¢‘ã€æ·±åº¦å›¾ä»¥åŠå¤´éƒ¨ã€æ‰‹éƒ¨è·Ÿè¸ªä¿¡æ¯ç­‰ã€‚</li>
<li>æ•°æ®é›†è™½ç„¶è§„æ¨¡å°ï¼Œä½†ä¸ºäººå·¥æ™ºèƒ½æ¨¡å‹åœ¨æ··åˆç°å®ä»»åŠ¡ä¸­çš„çœŸå®è¡¨ç°æä¾›äº†æµ‹è¯•ç¯å¢ƒã€‚</li>
<li>æ•°æ®é›†ä¸ºæœªæ¥æ„å»ºç‰©ç†åä½œçš„åŸºå‡†æµ‹è¯•æä¾›äº†æ½œåŠ›ã€‚</li>
<li>SigmaCollabæ•°æ®é›†å¯ç”¨äºç ”ç©¶äººç±»ä¸AIäº¤äº’çš„æ–°æŒ‘æˆ˜å’Œæœºä¼šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02560">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe1d545e152c99503784669a8ce549c6" align="middle">
<img src="https://picx.zhimg.com/v2-0239e991742b4d2a602e02af5084c135" align="middle">
<img src="https://picx.zhimg.com/v2-0ff068bdb4d16edab33feb65a3596809" align="middle">
<img src="https://picx.zhimg.com/v2-f33956e23c83b9a9b5ecce6e0c7604de" align="middle">
<img src="https://picx.zhimg.com/v2-6c480c55fc82d42d69672c618b65d41e" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Evaluating-Emotion-Recognition-in-Spoken-Language-Models-on-Emotionally-Incongruent-Speech"><a href="#Evaluating-Emotion-Recognition-in-Spoken-Language-Models-on-Emotionally-Incongruent-Speech" class="headerlink" title="Evaluating Emotion Recognition in Spoken Language Models on Emotionally Incongruent Speech"></a>Evaluating Emotion Recognition in Spoken Language Models on Emotionally Incongruent Speech</h2><p><strong>Authors:Pedro CorrÃªa, JoÃ£o Lima, Victor Moreno, Lucas Ueda, Paula Dornhofer Paro Costa</strong></p>
<p>Advancements in spoken language processing have driven the development of spoken language models (SLMs), designed to achieve universal audio understanding by jointly learning text and audio representations for a wide range of tasks. Although promising results have been achieved, there is growing discussion regarding these modelsâ€™ generalization capabilities and the extent to which they truly integrate audio and text modalities in their internal representations. In this work, we evaluate four SLMs on the task of speech emotion recognition using a dataset of emotionally incongruent speech samples, a condition under which the semantic content of the spoken utterance conveys one emotion while speech expressiveness conveys another. Our results indicate that SLMs rely predominantly on textual semantics rather than speech emotion to perform the task, indicating that text-related representations largely dominate over acoustic representations. We release both the code and the Emotionally Incongruent Synthetic Speech dataset (EMIS) to the community.</p>
<blockquote>
<p>éšç€å£è¯­å¤„ç†æŠ€æœ¯çš„è¿›æ­¥ï¼Œå£è¯­æ¨¡å‹ï¼ˆSLMsï¼‰çš„å‘å±•æ—¨åœ¨é€šè¿‡è”åˆå­¦ä¹ æ–‡æœ¬å’ŒéŸ³é¢‘è¡¨ç¤ºæ¥å¹¿æ³›å®ç°å„ç§ä»»åŠ¡çš„é€šç”¨éŸ³é¢‘ç†è§£ã€‚è™½ç„¶å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœï¼Œä½†å¯¹äºè¿™äº›æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä»¥åŠå®ƒä»¬å†…éƒ¨è¡¨ç¤ºä¸­çœŸæ­£æ•´åˆéŸ³é¢‘å’Œæ–‡æœ¬æ¨¡å¼çš„ç¨‹åº¦ï¼Œæ­£åœ¨å‡ºç°è¶Šæ¥è¶Šå¤šçš„è®¨è®ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æƒ…æ„Ÿä¸ä¸€è‡´çš„è¯­éŸ³æ ·æœ¬æ•°æ®é›†è¯„ä¼°äº†å››ç§SLMåœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¿™æ˜¯ä¸€ç§æƒ…å†µä¸‹ï¼Œå£å¤´è¯è¯­çš„è¯­ä¹‰å†…å®¹ä¼ è¾¾äº†ä¸€ç§æƒ…æ„Ÿï¼Œè€Œè¯­éŸ³è¡¨ç°åŠ›ä¼ è¾¾äº†å¦ä¸€ç§æƒ…æ„Ÿã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒSLMä¸»è¦ä¾èµ–äºæ–‡æœ¬è¯­ä¹‰è€Œä¸æ˜¯è¯­éŸ³æƒ…æ„Ÿæ¥å®Œæˆä»»åŠ¡ï¼Œè¿™è¡¨æ˜æ–‡æœ¬ç›¸å…³è¡¨ç¤ºåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå æ®äº†ä¸»å¯¼åœ°ä½ï¼Œè¶…è¿‡äº†å£°å­¦è¡¨ç¤ºã€‚æˆ‘ä»¬å‘ç¤¾åŒºå‘å¸ƒäº†ä»£ç å’Œæƒ…ç»ªä¸ä¸€è‡´åˆæˆè¯­éŸ³æ•°æ®é›†ï¼ˆEMISï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25054v2">PDF</a> Submitted to IEEE ICASSP 2026. Copyright 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses</p>
<p><strong>Summary</strong></p>
<p>éšç€è¯­éŸ³è¯†åˆ«æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œå£å¤´è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰å¾—ä»¥å‘å±•ï¼Œæ—¨åœ¨é€šè¿‡è”åˆå­¦ä¹ æ–‡æœ¬å’ŒéŸ³é¢‘è¡¨ç¤ºæ¥å®ç°é€šç”¨éŸ³é¢‘ç†è§£ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºå„ç§ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå…³äºè¿™äº›æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå…¶åœ¨å†…éƒ¨è¡¨ç¤ºä¸­çœŸæ­£èåˆéŸ³é¢‘å’Œæ–‡æœ¬æ¨¡æ€çš„ç¨‹åº¦ï¼Œå­˜åœ¨è¶Šæ¥è¶Šå¤šçš„è®¨è®ºã€‚æœ¬ç ”ç©¶åœ¨æƒ…æ„Ÿä¸åŒ¹é…çš„è¯­éŸ³æ ·æœ¬æ•°æ®é›†ä¸Šè¯„ä¼°äº†å››ç§SLMï¼Œåœ¨è¿™ç§æƒ…å¢ƒä¸‹ï¼Œå£å¤´è¯è¯­çš„è¯­ä¹‰å†…å®¹ä¼ è¾¾äº†ä¸€ç§æƒ…æ„Ÿï¼Œè€Œè¯­éŸ³è¡¨è¾¾åˆ™ä¼ è¾¾äº†å¦ä¸€ç§æƒ…æ„Ÿã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒSLMä¸»è¦ä¾èµ–äºæ–‡æœ¬è¯­ä¹‰è€Œéè¯­éŸ³æƒ…æ„Ÿæ¥å®Œæˆä»»åŠ¡ï¼Œè¿™æ„å‘³ç€æ–‡æœ¬ç›¸å…³çš„è¡¨ç¤ºåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¼˜äºå£°éŸ³è¡¨ç¤ºã€‚æˆ‘ä»¬å‘ç¤¾åŒºå‘å¸ƒäº†ä»£ç å’Œæƒ…æ„Ÿä¸åŒ¹é…çš„åˆæˆè¯­éŸ³æ•°æ®é›†ï¼ˆEMISï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å£å¤´è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰é€šè¿‡è”åˆå­¦ä¹ æ–‡æœ¬å’ŒéŸ³é¢‘è¡¨ç¤ºï¼Œå®ç°äº†é€šç”¨éŸ³é¢‘ç†è§£ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºå„ç§ä»»åŠ¡ã€‚</li>
<li>SLMsåœ¨æƒ…æ„Ÿä¸åŒ¹é…çš„è¯­éŸ³æ ·æœ¬æ•°æ®é›†ä¸Šçš„è¡¨ç°è¢«è¯„ä¼°ã€‚</li>
<li>SLMsä¸»è¦ä¾èµ–äºæ–‡æœ¬è¯­ä¹‰å®Œæˆè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä»»åŠ¡ï¼Œè€Œéè¯­éŸ³æƒ…æ„Ÿã€‚</li>
<li>æ–‡æœ¬ç›¸å…³çš„è¡¨ç¤ºåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¼˜äºå£°éŸ³è¡¨ç¤ºã€‚</li>
<li>SLMsçš„æ³›åŒ–èƒ½åŠ›æ˜¯ä¸€ä¸ªé‡è¦çš„é—®é¢˜ï¼Œéœ€è¦æ›´å¤šçš„ç ”ç©¶ã€‚</li>
<li>éŸ³é¢‘å’Œæ–‡æœ¬æ¨¡æ€åœ¨SLMså†…éƒ¨è¡¨ç¤ºä¸­çš„èåˆç¨‹åº¦æœ‰é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25054">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e3adb44bf8ed190c0ffe932bf3bbb6d" align="middle">
<img src="https://picx.zhimg.com/v2-80c2cd127c4a1cd33c3859228e497a5c" align="middle">
<img src="https://picx.zhimg.com/v2-05fd82ce21ac5988b22f3aa51e847edc" align="middle">
<img src="https://picx.zhimg.com/v2-bebfe724336ec10aeb2a85642bdd8ecd" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Leveraging-Large-Language-Models-to-Identify-Conversation-Threads-in-Collaborative-Learning"><a href="#Leveraging-Large-Language-Models-to-Identify-Conversation-Threads-in-Collaborative-Learning" class="headerlink" title="Leveraging Large Language Models to Identify Conversation Threads in Collaborative Learning"></a>Leveraging Large Language Models to Identify Conversation Threads in Collaborative Learning</h2><p><strong>Authors:Prerna Ravi, Dong Won Lee, Beatriz Flamia, Jasmine David, Brandon Hanks, Cynthia Breazeal, Emma Anderson, Grace Lin</strong></p>
<p>Understanding how ideas develop and flow in small-group conversations is critical for analyzing collaborative learning. A key structural feature of these interactions is threading, the way discourse talk naturally organizes into interwoven topical strands that evolve over time. While threading has been widely studied in asynchronous text settings, detecting threads in synchronous spoken dialogue remains challenging due to overlapping turns and implicit cues. At the same time, large language models (LLMs) show promise for automating discourse analysis but often struggle with long-context tasks that depend on tracing these conversational links. In this paper, we investigate whether explicit thread linkages can improve LLM-based coding of relational moves in group talk. We contribute a systematic guidebook for identifying threads in synchronous multi-party transcripts and benchmark different LLM prompting strategies for automated threading. We then test how threading influences performance on downstream coding of conversational analysis frameworks, that capture core collaborative actions such as agreeing, building, and eliciting. Our results show that providing clear conversational thread information improves LLM coding performance and underscores the heavy reliance of downstream analysis on well-structured dialogue. We also discuss practical trade-offs in time and cost, emphasizing where human-AI hybrid approaches can yield the best value. Together, this work advances methods for combining LLMs and robust conversational thread structures to make sense of complex, real-time group interactions.</p>
<blockquote>
<p>ç†è§£å°ç»„å¯¹è¯ä¸­æƒ³æ³•çš„äº§ç”Ÿå’Œå‘å±•å¯¹äºåˆ†æåä½œå­¦ä¹ è‡³å…³é‡è¦ã€‚è¿™äº›äº’åŠ¨çš„ä¸€ä¸ªå…³é”®ç»“æ„ç‰¹å¾æ˜¯â€œçº¿ç´¢ä¸²è”â€ï¼Œå³è¯è¯­è‡ªç„¶ç»„ç»‡æˆç›¸äº’äº¤ç»‡çš„ä¸»é¢˜çº¿ç´¢ï¼Œéšç€æ—¶é—´çš„æ¨ç§»è€Œå‘å±•ã€‚è™½ç„¶çº¿ç´¢ä¸²è”åœ¨å¼‚æ­¥æ–‡æœ¬ç¯å¢ƒä¸­å·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œä½†åœ¨åŒæ­¥å¯¹è¯ä¸­æ£€æµ‹çº¿ç´¢ä¸²è”ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå­˜åœ¨é‡å çš„å‘è¨€å’Œéšå«çš„çº¿ç´¢ã€‚åŒæ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–è¯è¯­åˆ†ææ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨ä¾èµ–äºè¿½è¸ªè¿™äº›å¯¹è¯é“¾æ¥çš„é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡æ–¹é¢å¾€å¾€è¡¨ç°æŒ£æ‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.22844v1">PDF</a> In Submission: Journal of Educational Data Mining (jEDM) 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å°ç¾¤ç»„å¯¹è¯ä¸­æ€æƒ³å‘å±•ä¸æµåŠ¨çš„é‡è¦æ€§ï¼Œå¼ºè°ƒåˆ†æåä½œå­¦ä¹ çš„å…³é”®ã€‚æ–‡ç« ç ”ç©¶äº†å¯¹è¯ä¸­çš„â€œçº¿ç¨‹â€ç»“æ„ï¼Œå³è¯è¯­è‡ªç„¶ç»„ç»‡æˆäº¤ç»‡çš„ä¸»é¢˜çº¿ç´¢ï¼Œéšæ—¶é—´å‘å±•æ¼”å˜ã€‚å°½ç®¡çº¿ç¨‹åœ¨å¼‚æ­¥æ–‡æœ¬ç¯å¢ƒä¸­å·²è¢«å¹¿æ³›ç ”ç©¶ï¼Œä½†åœ¨åŒæ­¥å¯¹è¯ä¸­æ£€æµ‹çº¿ç¨‹ä»å…·æŒ‘æˆ˜æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨è¯è¯­åˆ†ææ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨ä¾èµ–è¿½è¸ªè¿™äº›å¯¹è¯é“¾æ¥çš„é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡æ–¹é¢å¾€å¾€è¡¨ç°æŒ£æ‰ã€‚æœ¬æ–‡è°ƒæŸ¥äº†æ˜ç¡®çº¿ç¨‹é“¾æ¥æ˜¯å¦èƒ½æ”¹è¿›åŸºäºLLMçš„å…³ç³»åŠ¨ä½œç¼–ç ã€‚æˆ‘ä»¬ä¸ºåŒæ­¥å¤šæ–¹å¯¹è¯ä¸­çº¿ç¨‹çš„è¯†åˆ«æä¾›äº†ç³»ç»ŸæŒ‡å—ï¼Œå¹¶å¯¹ä¸åŒçš„LLMæç¤ºç­–ç•¥è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ç„¶åæµ‹è¯•äº†çº¿ç¨‹å¯¹ä¸‹æ¸¸ä¼šè¯åˆ†ææ¡†æ¶ç¼–ç çš„å½±å“ï¼Œè¿™äº›æ¡†æ¶æ•æ‰æ ¸å¿ƒåä½œåŠ¨ä½œï¼Œå¦‚åŒæ„ã€æ„å»ºå’Œæ¿€å‘ã€‚ç»“æœè¡¨æ˜ï¼Œæä¾›æ¸…æ™°çš„å¯¹è¯çº¿ç¨‹ä¿¡æ¯æé«˜äº†LLMç¼–ç æ€§èƒ½ï¼Œå¹¶å¼ºè°ƒäº†ä¸‹æ¸¸åˆ†æå¯¹ç»“æ„è‰¯å¥½å¯¹è¯çš„ä¾èµ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°ç»„å¯¹è¯ä¸­çš„æ€æƒ³å‘å±•ä¸æµåŠ¨å¯¹åä½œå­¦ä¹ åˆ†æè‡³å…³é‡è¦ã€‚</li>
<li>å¯¹è¯ä¸­çš„â€œçº¿ç¨‹â€ç»“æ„æ˜¯åˆ†æé‡ç‚¹ï¼ŒæŒ‡è¯è¯­è‡ªç„¶äº¤ç»‡çš„ä¸»é¢˜çº¿ç´¢éšæ—¶é—´å‘å±•æ¼”å˜ã€‚</li>
<li>åŒæ­¥å¯¹è¯ä¸­çš„çº¿ç¨‹æ£€æµ‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› å­˜åœ¨é‡å çš„å‘è¨€å’Œéšå«çš„çº¿ç´¢ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨è¯è¯­åˆ†ææ–¹é¢æœ‰æ½œåŠ›ï¼Œä½†åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸Šè¡¨ç°å¯èƒ½å—é™ã€‚</li>
<li>æ˜ç¡®çº¿ç¨‹é“¾æ¥èƒ½æ”¹è¿›åŸºäºLLMçš„å…³ç³»åŠ¨ä½œç¼–ç ã€‚</li>
<li>æä¾›æ¸…æ™°çš„å¯¹è¯çº¿ç¨‹ä¿¡æ¯å¯¹æé«˜LLMç¼–ç æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.22844">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e9d9a4064c9540b4c90de047c303561" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Beyond-touch-based-HMI-Control-your-machines-in-natural-language-by-utilizing-large-language-models-and-OPC-UA"><a href="#Beyond-touch-based-HMI-Control-your-machines-in-natural-language-by-utilizing-large-language-models-and-OPC-UA" class="headerlink" title="Beyond touch-based HMI: Control your machines in natural language by utilizing large language models and OPC UA"></a>Beyond touch-based HMI: Control your machines in natural language by utilizing large language models and OPC UA</h2><p><strong>Authors:Bernd Hofmann, Sven Kreitlein, Joerg Franke, Patrick Bruendl</strong></p>
<p>This paper proposes an agent-based approach toward a more natural interface between humans and machines. Large language models equipped with tools and the communication standard OPC UA are utilized to control machines in natural language. Instead of touch interaction, which is currently the state-of-the-art medium for interaction in operations, the proposed approach enables operators to talk or text with machines. This allows commands such as â€˜Please decrease the temperature by 20 % in machine 1 and set the motor speed to 5000 rpm in machine 2.â€™ The large language model receives the user input and selects one of three predefined tools that connect to an OPC UA server and either change or read the value of a node. Afterwards, the result of the tool execution is passed back to the language model, which then provides a final response to the user. The approach is universally designed and can therefore be applied to any machine that supports the OPC UA standard. The large language model is neither fine-tuned nor requires training data, only the relevant machine credentials and a parameter dictionary are included within the system prompt. The approach is evaluated on a Siemens S7-1500 programmable logic controller with four machine parameters in a case study of fifty synthetically generated commands on five different models. The results demonstrate high success rate, with proprietary GPT 5 models achieving accuracies between 96.0 % and 98.0 %, and open-weight models reaching up to 90.0 %. The proposed approach of this empirical study contributes to advancing natural interaction in industrial human-machine interfaces.</p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä»£ç†çš„æ–¹æ³•ï¼Œä»¥å®ç°æ›´è‡ªç„¶çš„äººæœºäº¤äº’ç•Œé¢ã€‚åˆ©ç”¨é…å¤‡å·¥å…·å’Œé€šä¿¡æ ‡å‡†OPC UAçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥è‡ªç„¶è¯­è¨€æ§åˆ¶æœºå™¨ï¼Œè€Œæ— éœ€ä½¿ç”¨å½“å‰çš„è§¦æ‘¸äº¤äº’ä½œä¸ºæ“ä½œäº¤äº’çš„ä¸»æµåª’ä»‹ã€‚æ‰€æå‡ºçš„æ–¹æ³•ä½¿æ“ä½œè€…å¯ä»¥ä¸æœºå™¨è¿›è¡Œå¯¹è¯æˆ–æ–‡æœ¬äº¤æµï¼Œä¾‹å¦‚ä¸‹è¾¾æŒ‡ä»¤ï¼šâ€œè¯·åœ¨æœºå™¨1ä¸­å°†æ¸©åº¦é™ä½20%å¹¶å°†æœºå™¨2çš„ç”µæœºé€Ÿåº¦è®¾ç½®ä¸º5000è½¬&#x2F;åˆ†ã€‚â€å¤§å‹è¯­è¨€æ¨¡å‹æ¥æ”¶ç”¨æˆ·è¾“å…¥ï¼Œå¹¶é€‰æ‹©ä¸‰ç§é¢„å®šä¹‰å·¥å…·ä¹‹ä¸€è¿æ¥åˆ°OPC UAæœåŠ¡å™¨ï¼Œä»¥æ›´æ”¹æˆ–è¯»å–èŠ‚ç‚¹çš„å€¼ã€‚ç„¶åï¼Œå·¥å…·æ‰§è¡Œçš„ç»“æœå°†åé¦ˆç»™è¯­è¨€æ¨¡å‹ï¼Œç„¶åä¸ºç”¨æˆ·æä¾›æœ€ç»ˆå“åº”ã€‚è¯¥æ–¹æ³•è®¾è®¡é€šç”¨ï¼Œå¯åº”ç”¨äºæ”¯æŒOPC UAæ ‡å‡†çš„ä»»ä½•æœºå™¨ã€‚å¤§å‹è¯­è¨€æ¨¡å‹æ—¢ä¸éœ€è¦å¾®è°ƒä¹Ÿä¸éœ€è¦è®­ç»ƒæ•°æ®ï¼Œç³»ç»Ÿä¸­åªéœ€åŒ…å«ç›¸å…³çš„æœºå™¨å‡­æ®å’Œå‚æ•°è¯å…¸å³å¯ã€‚è¯¥æ–¹æ³•åœ¨è¥¿é—¨å­S7-1500å¯ç¼–ç¨‹é€»è¾‘æ§åˆ¶å™¨ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œåœ¨ä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶ä¸­ä½¿ç”¨äº”åä¸ªåˆæˆç”Ÿæˆçš„å‘½ä»¤å¯¹äº”ä¸ªä¸åŒæ¨¡å‹è¿›è¡Œæµ‹è¯•ã€‚ç»“æœæ˜¾ç¤ºæˆåŠŸç‡å¾ˆé«˜ï¼Œä¸“æœ‰GPT 5æ¨¡å‹çš„å‡†ç¡®ç‡åœ¨96.0%è‡³98.0%ä¹‹é—´ï¼Œå¼€æ”¾æƒé‡æ¨¡å‹çš„å‡†ç¡®ç‡é«˜è¾¾90.0%ã€‚æœ¬å®è¯ç ”ç©¶æ‰€æå‡ºçš„æ–¹æ³•æœ‰åŠ©äºæ¨åŠ¨å·¥ä¸šäººæœºç•Œé¢ä¸­çš„è‡ªç„¶äº¤äº’å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11300v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºä»£ç†çš„è‡ªç„¶äººæœºäº¤äº’æ–¹å¼ã€‚è¯¥ç ”ç©¶ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åŠOPC UAé€šä¿¡æ ‡å‡†æ¥æ§åˆ¶æœºå™¨çš„è‡ªç„¶è¯­è¨€äº¤äº’ã€‚æ­¤æ–¹å¼å…è®¸æ“ä½œè€…é€šè¿‡å¯¹è¯æˆ–æ–‡æœ¬ä¸æœºå™¨æ²Ÿé€šï¼Œæ›¿ä»£å½“å‰ä¸»æµçš„è§¦æ‘¸äº¤äº’æ–¹å¼ã€‚ç ”ç©¶å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨è¯­è¨€æ¨¡å‹é€‰æ‹©å·¥å…·ä¸OPC UAæœåŠ¡å™¨è¿æ¥ï¼Œæ”¹å˜æˆ–è¯»å–èŠ‚ç‚¹å€¼ï¼Œå¹¶åé¦ˆæ‰§è¡Œç»“æœã€‚è¯¥æ–¹å¼å¯å¹¿æ³›åº”ç”¨äºæ”¯æŒOPC UAæ ‡å‡†çš„ä»»ä½•æœºå™¨ã€‚è¯¥è¯­è¨€æ¨¡å‹æ— éœ€å¾®è°ƒï¼Œä¹Ÿä¸éœ€è®­ç»ƒæ•°æ®ï¼Œåªéœ€ç³»ç»Ÿæç¤ºä¸­è¾“å…¥ç›¸å…³æœºå™¨å‡­è¯å’Œå‚æ•°è¯å…¸å³å¯ã€‚åœ¨ä¸€ä¸ªä½¿ç”¨Siemens S7-1500å¯ç¼–ç¨‹é€»è¾‘æ§åˆ¶å™¨å’Œäº”ä¸ªä¸åŒæ¨¡å‹çš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œè¯¥æ–¹æ³•çš„æˆåŠŸç‡å¾ˆé«˜ï¼Œä¸“æœ‰GPT 5æ¨¡å‹çš„å‡†ç¡®ç‡åœ¨96.0%è‡³98.0%ä¹‹é—´ï¼Œå¼€æ”¾æƒé‡æ¨¡å‹çš„å‡†ç¡®ç‡é«˜è¾¾90.0%ã€‚æ­¤ç ”ç©¶æœ‰åŠ©äºæ¨åŠ¨å·¥ä¸šäººæœºç•Œé¢ä¸­çš„è‡ªç„¶äº¤äº’å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºä»£ç†çš„è‡ªç„¶äººæœºäº¤äº’æ–¹å¼ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’ŒOPC UAé€šä¿¡æ ‡å‡†æ¥æ§åˆ¶æœºå™¨ã€‚</li>
<li>æ›¿ä»£äº†å½“å‰ä¸»æµçš„è§¦æ‘¸äº¤äº’æ–¹å¼ï¼Œå…è®¸æ“ä½œè€…é€šè¿‡å¯¹è¯æˆ–æ–‡æœ¬ä¸æœºå™¨è¿›è¡Œæ²Ÿé€šã€‚</li>
<li>è¯­è¨€æ¨¡å‹èƒ½å¤Ÿé€‰æ‹©å·¥å…·ä¸OPC UAæœåŠ¡å™¨è¿æ¥ï¼Œå®ç°æ”¹å˜æˆ–è¯»å–èŠ‚ç‚¹å€¼çš„åŠŸèƒ½ï¼Œå¹¶åé¦ˆæ‰§è¡Œç»“æœã€‚</li>
<li>è¯¥æ–¹å¼å…·æœ‰é€šç”¨æ€§ï¼Œå¯åº”ç”¨äºä»»ä½•æ”¯æŒOPC UAæ ‡å‡†çš„æœºå™¨ã€‚</li>
<li>è¯­è¨€æ¨¡å‹æ— éœ€å¾®è°ƒï¼Œä¸”ä¸ä¾èµ–è®­ç»ƒæ•°æ®ï¼Œåªéœ€ç³»ç»Ÿæç¤ºä¸­è¾“å…¥ç›¸å…³æœºå™¨å‡­è¯å’Œå‚æ•°è¯å…¸å³å¯æ“ä½œã€‚</li>
<li>åœ¨Siemens S7-1500å¯ç¼–ç¨‹é€»è¾‘æ§åˆ¶å™¨ä¸Šçš„æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼Œè¯¥æ–¹æ³•æˆåŠŸç‡å¾ˆé«˜ï¼Œä¸“æœ‰GPT 5æ¨¡å‹çš„å‡†ç¡®ç‡åœ¨96.0%è‡³98.0%ä¹‹é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11300">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1be99a868b06391abe83c212a812d0e1" align="middle">
<img src="https://picx.zhimg.com/v2-1268a47c3634ea62732321834f55656d" align="middle">
<img src="https://picx.zhimg.com/v2-968ee11ee081fb710fcc11f75d74deef" align="middle">
<img src="https://picx.zhimg.com/v2-50136b82f1ccf7b46dcb6caf48d2f2ac" align="middle">
<img src="https://picx.zhimg.com/v2-90079ae0868ddddb8b1dbe5b1aeebd94" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Back-to-Ear-Perceptually-Driven-High-Fidelity-Music-Reconstruction"><a href="#Back-to-Ear-Perceptually-Driven-High-Fidelity-Music-Reconstruction" class="headerlink" title="Back to Ear: Perceptually Driven High Fidelity Music Reconstruction"></a>Back to Ear: Perceptually Driven High Fidelity Music Reconstruction</h2><p><strong>Authors:Kangdi Wang, Zhiyue Wu, Dinghao Zhou, Rui Lin, Junyu Dai, Tao Jiang</strong></p>
<p>Variational Autoencoders (VAEs) are essential for large-scale audio tasks like diffusion-based generation. However, existing open-source models often neglect auditory perceptual aspects during training, leading to weaknesses in phase accuracy and stereophonic spatial representation. To address these challenges, we propose Îµar-VAE, an open-source music signal reconstruction model that rethinks and optimizes the VAE training paradigm. Our contributions are threefold: (i) A K-weighting perceptual filter applied prior to loss calculation to align the objective with auditory perception. (ii) Two novel phase losses: a Correlation Loss for stereo coherence, and a Phase Loss using its derivativesâ€“Instantaneous Frequency and Group Delayâ€“for precision. (iii) A new spectral supervision paradigm where magnitude is supervised by all four Mid&#x2F;Side&#x2F;Left&#x2F;Right components, while phase is supervised only by the LR components. Experiments show Îµar-VAE at 44.1kHz substantially outperforms leading open-source models across diverse metrics, showing particular strength in reconstructing high-frequency harmonics and the spatial characteristics.</p>
<blockquote>
<p>å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰å¯¹äºåŸºäºæ‰©æ•£ç”Ÿæˆçš„å¤§å‹éŸ³é¢‘ä»»åŠ¡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¼€æºæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¾€å¾€å¿½è§†äº†å¬è§‰æ„ŸçŸ¥æ–¹é¢ï¼Œå¯¼è‡´ç›¸ä½å‡†ç¡®æ€§å’Œç«‹ä½“å£°ç©ºé—´è¡¨å¾çš„å¼±ç‚¹ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Îµar-VAEï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„éŸ³ä¹ä¿¡å·é‡å»ºæ¨¡å‹ï¼Œå®ƒé‡æ–°æ€è€ƒå’Œä¼˜åŒ–äº†VAEè®­ç»ƒèŒƒå¼ã€‚æˆ‘ä»¬çš„è´¡çŒ®ä¸»è¦ä½“ç°åœ¨ä¸‰ä¸ªæ–¹é¢ï¼šï¼ˆiï¼‰åœ¨æŸå¤±è®¡ç®—ä¹‹å‰åº”ç”¨K-åŠ æƒæ„ŸçŸ¥æ»¤æ³¢å™¨ï¼Œä»¥ä½¿ç›®æ ‡ä¸å¬è§‰æ„ŸçŸ¥ç›¸ç¬¦ã€‚ï¼ˆiiï¼‰ä¸¤ç§æ–°å‹ç›¸ä½æŸå¤±ï¼šç”¨äºç«‹ä½“å£°è¿è´¯æ€§çš„ç›¸å…³æ€§æŸå¤±ï¼Œä»¥åŠä½¿ç”¨å…¶å¯¼æ•°ï¼ˆå³æ—¶é¢‘ç‡å’Œç¾¤å»¶è¿Ÿï¼‰çš„ç›¸ä½æŸå¤±ï¼Œä»¥æé«˜ç²¾åº¦ã€‚ï¼ˆiiiï¼‰ä¸€ç§æ–°çš„é¢‘è°±ç›‘ç£èŒƒå¼ï¼Œå…¶ä¸­å¹…åº¦ç”±å››ä¸ªä¸­&#x2F;ä¾§&#x2F;å·¦&#x2F;å³ç»„ä»¶è¿›è¡Œç›‘ç£ï¼Œè€Œç›¸ä½ä»…ç”±LRç»„ä»¶è¿›è¡Œç›‘ç£ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨44.1kHzä¸‹ï¼ŒÎµar-VAEåœ¨å¤šç§æŒ‡æ ‡ä¸Šçš„è¡¨ç°å‡ä¼˜äºé¢†å…ˆçš„å¼€æºæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨é‡å»ºé«˜é¢‘è°æ³¢å’Œç©ºé—´ç‰¹æ€§æ–¹é¢è¡¨ç°å‡ºç‰¹åˆ«çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.14912v2">PDF</a> Check the Code here: <a target="_blank" rel="noopener" href="https://github.com/Eps-Acoustic-Revolution-Lab/EAR_VAE">https://github.com/Eps-Acoustic-Revolution-Lab/EAR_VAE</a> and Model Weights here: <a target="_blank" rel="noopener" href="https://huggingface.co/earlab/EAR_VAE">https://huggingface.co/earlab/EAR_VAE</a></p>
<p><strong>Summary</strong></p>
<p>Îµar-VAEæ¨¡å‹é’ˆå¯¹å¤§å‹éŸ³é¢‘ä»»åŠ¡ä¸­çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰è¿›è¡Œæ”¹è¿›å’Œä¼˜åŒ–ï¼Œä»¥æå‡åœ¨éŸ³é¢‘ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚é€šè¿‡åº”ç”¨å¬è§‰æ„ŸçŸ¥è¿‡æ»¤å™¨å’Œåˆ›æ–°é˜¶æ®µæŸå¤±ç­–ç•¥ï¼Œä»¥åŠæ–°çš„é¢‘è°±ç›‘ç£æ¨¡å¼ï¼ŒÎµar-VAEèƒ½æ›´æœ‰æ•ˆåœ°å¤„ç†éŸ³ä¹ä¿¡å·é‡å»ºé—®é¢˜ï¼Œå°¤å…¶åœ¨é‡å»ºé«˜é¢‘å’Œè°éŸ³å’Œç©ºé—´ç‰¹æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚è¯¥æ¨¡å‹å¯¹ç°æœ‰å¼€æºæ¨¡å‹è¿›è¡Œäº†å®è´¨æ€§æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Îµar-VAEæ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§å‹éŸ³é¢‘ä»»åŠ¡çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰æ”¹è¿›æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹é€šè¿‡åº”ç”¨K-weightingæ„ŸçŸ¥è¿‡æ»¤å™¨æ¥å¯¹é½ç›®æ ‡å¬è§‰æ„ŸçŸ¥ã€‚</li>
<li>æ¨¡å‹å¼•å…¥ä¸¤ç§æ–°çš„é˜¶æ®µæŸå¤±ç­–ç•¥ï¼šç”¨äºç«‹ä½“å£°è¿è´¯æ€§çš„ç›¸å…³æ€§æŸå¤±å’Œç”¨äºç²¾åº¦çš„å³æ—¶é¢‘ç‡å’Œç¾¤å»¶è¿Ÿçš„é˜¶æ®µæŸå¤±ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨æ–°çš„é¢‘è°±ç›‘ç£æ¨¡å¼ï¼Œå¯¹å¹…åº¦è¿›è¡Œå…¨é¢ç›‘ç£å¹¶å¯¹é˜¶æ®µè¿›è¡Œç›‘ç£è¿›è¡Œäº†ç»†åŒ–ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14912">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-49d5eedf2e4fef5a8a8f449ec51685c6" align="middle">
<img src="https://picx.zhimg.com/v2-1ee8b3570ebd6f41c6136e6a2e21b756" align="middle">
<img src="https://picx.zhimg.com/v2-7f7546299777d75a1fdd0f80d6b2cf90" align="middle">
<img src="https://picx.zhimg.com/v2-6e3ad6f73a7dfa4cf251e0aceb905422" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="FSR-VLN-Fast-and-Slow-Reasoning-for-Vision-Language-Navigation-with-Hierarchical-Multi-modal-Scene-Graph"><a href="#FSR-VLN-Fast-and-Slow-Reasoning-for-Vision-Language-Navigation-with-Hierarchical-Multi-modal-Scene-Graph" class="headerlink" title="FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph"></a>FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph</h2><p><strong>Authors:Xiaolin Zhou, Tingyang Xiao, Liu Liu, Yucheng Wang, Maiyue Chen, Xinrui Meng, Xinjie Wang, Wei Feng, Wei Sui, Zhizhong Su</strong></p>
<p>Visual-Language Navigation (VLN) is a fundamental challenge in robotic systems, with broad applications for the deployment of embodied agents in real-world environments. Despite recent advances, existing approaches are limited in long-range spatial reasoning, often exhibiting low success rates and high inference latency, particularly in long-range navigation tasks. To address these limitations, we propose FSR-VLN, a vision-language navigation system that combines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow Navigation Reasoning (FSR). The HMSG provides a multi-modal map representation supporting progressive retrieval, from coarse room-level localization to fine-grained goal view and object identification. Building on HMSG, FSR first performs fast matching to efficiently select candidate rooms, views, and objects, then applies VLM-driven refinement for final goal selection. We evaluated FSR-VLN across four comprehensive indoor datasets collected by humanoid robots, utilizing 87 instructions that encompass a diverse range of object categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all datasets, measured by the retrieval success rate (RSR), while reducing the response time by 82% compared to VLM-based methods on tour videos by activating slow reasoning only when fast intuition fails. Furthermore, we integrate FSR-VLN with speech interaction, planning, and control modules on a Unitree-G1 humanoid robot, enabling natural language interaction and real-time navigation.</p>
<blockquote>
<p>è§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰æ˜¯æœºå™¨äººç³»ç»Ÿä¸­çš„ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼Œå…·æœ‰åœ¨ç°å®ä¸–ç•Œçš„ç¯å¢ƒä¸­éƒ¨ç½²å®ä½“ä»£ç†çš„å¹¿æ³›åº”ç”¨ã€‚å°½ç®¡æœ€è¿‘æœ‰è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨è¿œç¨‹ç©ºé—´æ¨ç†æ–¹é¢ä»å­˜åœ¨å±€é™ï¼Œé€šå¸¸è¡¨ç°å‡ºæˆåŠŸç‡ä½å’Œæ¨ç†å»¶è¿Ÿé«˜çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è¿œç¨‹å¯¼èˆªä»»åŠ¡ä¸­ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†FSR-VLNï¼Œä¸€ç§ç»“åˆåˆ†å±‚å¤šæ¨¡å¼åœºæ™¯å›¾ï¼ˆHMSGï¼‰å’Œå¿«æ…¢å¯¼èˆªæ¨ç†ï¼ˆFSRï¼‰çš„è§†è§‰è¯­è¨€å¯¼èˆªç³»ç»Ÿã€‚HMSGæä¾›äº†ä¸€ç§å¤šæ¨¡å¼åœ°å›¾è¡¨ç¤ºï¼Œæ”¯æŒä»ç²—ç³™çš„æˆ¿é—´çº§å®šä½åˆ°ç²¾ç»†çš„ç›®æ ‡è§†å›¾å’Œå¯¹è±¡è¯†åˆ«çš„æ¸è¿›æ£€ç´¢ã€‚åŸºäºHMSGï¼ŒFSRé¦–å…ˆè¿›è¡Œå¿«é€ŸåŒ¹é…ï¼Œä»¥æœ‰æ•ˆåœ°é€‰æ‹©å€™é€‰æˆ¿é—´ã€è§†å›¾å’Œå¯¹è±¡ï¼Œç„¶ååº”ç”¨VLMé©±åŠ¨çš„ç»†åŒ–è¿›è¡Œæœ€ç»ˆç›®æ ‡é€‰æ‹©ã€‚æˆ‘ä»¬åœ¨ç”±äººå½¢æœºå™¨äººæ”¶é›†çš„å››ä¸ªç»¼åˆå®¤å†…æ•°æ®é›†ä¸Šè¯„ä¼°äº†FSR-VLNï¼Œåˆ©ç”¨87æ¡æŒ‡ä»¤æ¶µç›–äº†å„ç§å¯¹è±¡ç±»åˆ«ã€‚FSR-VLNåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„æ£€ç´¢æˆåŠŸç‡ï¼ˆRSRï¼‰å‡è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼ŒåŒæ—¶åœ¨æ¿€æ´»ä»…åœ¨å¿«é€Ÿç›´è§‰å¤±è´¥æ—¶æ‰å¯åŠ¨çš„æ…¢é€Ÿæ¨ç†æ—¶ï¼Œä¸åŸºäºVLMçš„æ–¹æ³•ç›¸æ¯”ï¼Œå°†å“åº”æ—¶é—´å‡å°‘äº†82%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†FSR-VLNä¸Unitree-G1äººå½¢æœºå™¨äººçš„è¯­éŸ³äº¤äº’ã€è§„åˆ’å’Œæ§åˆ¶æ¨¡å—é›†æˆï¼Œå®ç°è‡ªç„¶è¯­è¨€äº¤äº’å’Œå®æ—¶å¯¼èˆªã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13733v2">PDF</a> 8 pages</p>
<p><strong>Summary</strong>ï¼š</p>
<p>VLNï¼ˆè§†è§‰è¯­è¨€å¯¼èˆªï¼‰æ˜¯æœºå™¨äººç³»ç»Ÿçš„ä¸€é¡¹åŸºæœ¬æŒ‘æˆ˜ï¼Œå¹¿æ³›åº”ç”¨äºçœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„æ™ºèƒ½ä»£ç†éƒ¨ç½²ã€‚ä¸ºè§£å†³ç°æœ‰æ–¹æ³•åœ¨é•¿è·ç¦»ç©ºé—´æ¨ç†ä¸Šçš„å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†FSR-VLNç³»ç»Ÿï¼Œå®ƒç»“åˆäº†åˆ†å±‚å¤šæ¨¡å¼åœºæ™¯å›¾ï¼ˆHMSGï¼‰å’Œå¿«æ…¢å¯¼èˆªæ¨ç†ï¼ˆFSRï¼‰ã€‚HMSGæä¾›äº†å¤šæ¨¡å¼åœ°å›¾è¡¨ç¤ºï¼Œæ”¯æŒä»ç²—ç•¥çš„æˆ¿é—´çº§å®šä½åˆ°ç²¾ç»†çš„ç›®æ ‡è§†å›¾å’Œå¯¹è±¡è¯†åˆ«çš„æ¸è¿›æ£€ç´¢ã€‚FSRé¦–å…ˆè¿›è¡Œå¿«é€ŸåŒ¹é…ï¼Œä»¥é«˜æ•ˆé€‰æ‹©å€™é€‰æˆ¿é—´ã€è§†å›¾å’Œå¯¹è±¡ï¼Œç„¶ååº”ç”¨VLMé©±åŠ¨çš„ç»†åŒ–è¿›è¡Œæœ€ç»ˆç›®æ ‡é€‰æ‹©ã€‚åœ¨ç”±äººå½¢æœºå™¨äººæ”¶é›†çš„å››ä¸ªç»¼åˆå®¤å†…æ•°æ®é›†ä¸Šè¯„ä¼°FSR-VLNï¼Œä½¿ç”¨87æ¡æŒ‡ä»¤æ¶µç›–å„ç§å¯¹è±¡ç±»åˆ«ã€‚FSR-VLNåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„æ£€ç´¢æˆåŠŸç‡ï¼ˆRSRï¼‰è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå¹¶ä¸”é€šè¿‡å°†å“åº”æ—¶é—´å‡å°‘82ï¼…ï¼Œæé«˜äº†åŸºäºVLMçš„æ–¹æ³•çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†FSR-VLNä¸è¯­éŸ³äº¤äº’ã€è§„åˆ’å’Œæ§åˆ¶æ¨¡å—é›†æˆåœ¨Unitree-G1äººå½¢æœºå™¨äººä¸Šï¼Œå®ç°è‡ªç„¶è¯­è¨€äº¤äº’å’Œå®æ—¶å¯¼èˆªã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>VLNåœ¨æœºå™¨äººç³»ç»Ÿä¸­å…·æœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨é•¿è·ç¦»ç©ºé—´æ¨ç†çš„å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€å¯¼èˆªç³»ç»ŸFSR-VLNï¼Œç»“åˆäº†åˆ†å±‚å¤šæ¨¡å¼åœºæ™¯å›¾ï¼ˆHMSGï¼‰å’Œå¿«æ…¢å¯¼èˆªæ¨ç†ï¼ˆFSRï¼‰ã€‚</li>
<li>HMSGæä¾›äº†å¤šæ¨¡å¼åœ°å›¾è¡¨ç¤ºï¼Œæ”¯æŒä»ç²—ç•¥åˆ°ç²¾ç»†çš„æ¸è¿›æ£€ç´¢ã€‚</li>
<li>FSRé€šè¿‡å¿«é€ŸåŒ¹é…é€‰æ‹©å€™é€‰ï¼Œç„¶ååº”ç”¨VLMç»†åŒ–ç›®æ ‡é€‰æ‹©ã€‚</li>
<li>åœ¨å››ä¸ªå®¤å†…æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒFSR-VLNè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œæ˜¾è‘—æé«˜æ£€ç´¢æˆåŠŸç‡ï¼ˆRSRï¼‰ï¼Œå¹¶å¤§å¤§å‡å°‘å“åº”æ—¶é—´ã€‚</li>
<li>FSR-VLNä¸äººå½¢æœºå™¨äººé›†æˆï¼Œå®ç°è‡ªç„¶è¯­è¨€äº¤äº’å’Œå®æ—¶å¯¼èˆªã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb54b658ab79ae302fe030e463902437" align="middle">
<img src="https://picx.zhimg.com/v2-0e1d36df6d621f855b24b782503c3444" align="middle">
<img src="https://picx.zhimg.com/v2-3f09a637e0bdfc57b99e478299fca2ed" align="middle">
<img src="https://picx.zhimg.com/v2-dc3221f7385dcc63777681da65c3f99d" align="middle">
<img src="https://picx.zhimg.com/v2-946f1f8d372eda11f5262a3f7a5b16d5" align="middle">
<img src="https://picx.zhimg.com/v2-995fe8c28510324e1b4448ef87d38c8d" align="middle">
<img src="https://picx.zhimg.com/v2-bf3f0abdfb81cbfb04dba8f1f51ac6dd" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/Talking%20Head%20Generation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/Talking%20Head%20Generation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                    <span class="chip bg-color">Talking Head Generation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-17/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1a57e3c01ad6a4f9e38dd8e77c42443a" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-17  From 2D to 3D Without Extra Baggage Data-Efficient Cancer Detection in Digital Breast Tomosynthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3c65115714e83519b9c6ee5a8950d323" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  Time-to-Move Training-Free Motion Controlled Video Generation via Dual-Clock Denoising
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
