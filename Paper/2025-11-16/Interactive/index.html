<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Interactive">
    <meta name="description" content="Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  Persona-Aware Alignment Framework for Personalized Dialogue Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Interactive | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c5ed64d6b004d1e6854d030fe95a638b')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Interactive</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Interactive/">
                                <span class="chip bg-color">Interactive</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                Interactive
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    7.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    32 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-16-æ›´æ–°"><a href="#2025-11-16-æ›´æ–°" class="headerlink" title="2025-11-16 æ›´æ–°"></a>2025-11-16 æ›´æ–°</h1><h2 id="Persona-Aware-Alignment-Framework-for-Personalized-Dialogue-Generation"><a href="#Persona-Aware-Alignment-Framework-for-Personalized-Dialogue-Generation" class="headerlink" title="Persona-Aware Alignment Framework for Personalized Dialogue Generation"></a>Persona-Aware Alignment Framework for Personalized Dialogue Generation</h2><p><strong>Authors:Guanrong Li, Xinyu Liu, Zhen Wu, Xinyu Dai</strong></p>
<p>Personalized dialogue generation aims to leverage persona profiles and dialogue history to generate persona-relevant and consistent responses. Mainstream models typically rely on token-level language model training with persona dialogue data, such as Next Token Prediction, to implicitly achieve personalization, making these methods tend to neglect the given personas and generate generic responses. To address this issue, we propose a novel Persona-Aware Alignment Framework (PAL), which directly treats persona alignment as the training objective of dialogue generation. Specifically, PAL employs a two-stage training method including Persona-aware Learning and Persona Alignment, equipped with an easy-to-use inference strategy Select then Generate, to improve persona sensitivity and generate more persona-relevant responses at the semantics level. Through extensive experiments, we demonstrate that our framework outperforms many state-of-the-art personalized dialogue methods and large language models.</p>
<blockquote>
<p>ä¸ªæ€§åŒ–å¯¹è¯ç”Ÿæˆæ—¨åœ¨åˆ©ç”¨äººç‰©è§’è‰²æè¿°å’Œå¯¹è¯å†å²æ¥ç”Ÿæˆä¸äººç‰©è§’è‰²ç›¸å…³ä¸”è¿è´¯çš„å“åº”ã€‚ä¸»æµæ¨¡å‹é€šå¸¸ä¾èµ–äºåŸºäºä»¤ç‰Œçº§åˆ«çš„è¯­è¨€æ¨¡å‹è®­ç»ƒä¸ä¸ªæ€§åŒ–å¯¹è¯æ•°æ®ï¼ˆå¦‚ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹ï¼‰ï¼Œä»¥éšå«æ–¹å¼å®ç°ä¸ªæ€§åŒ–ï¼Œè¿™ä½¿å¾—è¿™äº›æ–¹æ³•å¾€å¾€å¿½ç•¥ç»™å®šçš„ä¸ªäººè§’è‰²å¹¶ç”Ÿæˆé€šç”¨å“åº”ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸ªæ€§æ„ŸçŸ¥å¯¹é½æ¡†æ¶ï¼ˆPALï¼‰ï¼Œå®ƒç›´æ¥å°†ä¸ªæ€§å¯¹é½è§†ä¸ºå¯¹è¯ç”Ÿæˆçš„è®­ç»ƒç›®æ ‡ã€‚å…·ä½“æ¥è¯´ï¼ŒPALé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸ªæ€§åŒ–æ„ŸçŸ¥å­¦ä¹ å’Œä¸ªæ€§åŒ–å¯¹é½ï¼Œå¹¶é…å¤‡ç®€å•æ˜“ç”¨çš„æ¨ç†ç­–ç•¥â€œå…ˆé€‰æ‹©åç”Ÿæˆâ€ï¼Œä»¥æé«˜å¯¹äººç‰©è§’è‰²çš„æ•æ„Ÿæ€§ï¼Œå¹¶åœ¨è¯­ä¹‰å±‚é¢ä¸Šç”Ÿæˆæ›´å¤šä¸äººç‰©è§’è‰²ç›¸å…³çš„å“åº”ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ¡†æ¶ä¼˜äºè®¸å¤šæœ€æ–°çš„ä¸ªæ€§åŒ–å¯¹è¯æ–¹æ³•å’Œå¤§å‹è¯­è¨€æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10215v1">PDF</a> Pre-MIT Press publication version</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ä¸ªæ€§åŒ–å¯¹è¯ç”Ÿæˆæ¡†æ¶â€”â€”Persona-Aware Alignment Frameworkï¼ˆPALï¼‰ï¼Œæ—¨åœ¨åˆ©ç”¨äººç‰©è§’è‰²ç‰¹å¾å’Œå¯¹è¯å†å²ç”Ÿæˆä¸äººç‰©è§’è‰²ç›¸å…³ä¸”è¿è´¯çš„å“åº”ã€‚PALæ¡†æ¶é€šè¿‡ç›´æ¥ä»¥äººç‰©è§’è‰²å¯¹é½ä½œä¸ºå¯¹è¯ç”Ÿæˆçš„è®­ç»ƒç›®æ ‡ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•å’Œâ€œé€‰æ‹©åç”Ÿæˆâ€æ¨ç†ç­–ç•¥ï¼Œæé«˜äº†å¯¹äººç‰©è§’è‰²çš„æ•æ„Ÿåº¦ï¼Œå¹¶åœ¨è¯­ä¹‰å±‚é¢ç”Ÿæˆäº†æ›´å¤šä¸äººç‰©è§’è‰²ç›¸å…³çš„å“åº”ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶ä¼˜äºå¤šç§å…ˆè¿›çš„ä¸ªæ€§åŒ–å¯¹è¯æ–¹æ³•å’Œå¤§å‹è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸ªæ€§åŒ–å¯¹è¯ç”Ÿæˆçš„ç›®æ ‡æ˜¯åˆ©ç”¨äººç‰©è§’è‰²ç‰¹å¾å’Œå¯¹è¯å†å²ç”Ÿæˆç›¸å…³ä¸”è¿è´¯çš„å“åº”ã€‚</li>
<li>ä¸»æµæ¨¡å‹é€šå¸¸ä¾èµ–ä»¤ç‰Œçº§è¯­è¨€æ¨¡å‹è®­ç»ƒï¼Œä»¥éšå«æ–¹å¼å®ç°ä¸ªæ€§åŒ–ï¼Œä½†å®¹æ˜“ç”Ÿæˆé€šç”¨å“åº”ã€‚</li>
<li>PALæ¡†æ¶ç›´æ¥ä»¥äººç‰©è§’è‰²å¯¹é½ä½œä¸ºå¯¹è¯ç”Ÿæˆè®­ç»ƒç›®æ ‡ï¼Œå¼ºè°ƒäººç‰©è§’è‰²çš„é‡è¦æ€§ã€‚</li>
<li>PALé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•å’Œâ€œé€‰æ‹©åç”Ÿæˆâ€æ¨ç†ç­–ç•¥ï¼Œæé«˜è¯­ä¹‰å±‚é¢çš„ä¸ªæ€§åŒ–å“åº”è´¨é‡ã€‚</li>
<li>PALæ¡†æ¶åœ¨å®éªŒä¸­è¡¨ç°å‡ºä¼˜äºå…¶ä»–å…ˆè¿›ä¸ªæ€§åŒ–å¯¹è¯æ–¹æ³•å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>PALæ¡†æ¶æœ‰åŠ©äºè§£å†³ä¸»æµæ¨¡å‹å¿½è§†äººç‰©è§’è‰²ç‰¹å¾çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10215">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2ed906dbeaa204c7be0f9b5f96a415fd" align="middle">
<img src="https://picx.zhimg.com/v2-86031fab95a3e14845d6af84a0c60e54" align="middle">
<img src="https://picx.zhimg.com/v2-19d73cd2f3547c5c428ae57ae07b77af" align="middle">
<img src="https://picx.zhimg.com/v2-e7da0d5d8e259f5bf8b9c99cb6f21e6f" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Beyond-ReAct-A-Planner-Centric-Framework-for-Complex-Tool-Augmented-LLM-Reasoning"><a href="#Beyond-ReAct-A-Planner-Centric-Framework-for-Complex-Tool-Augmented-LLM-Reasoning" class="headerlink" title="Beyond ReAct: A Planner-Centric Framework for Complex Tool-Augmented LLM Reasoning"></a>Beyond ReAct: A Planner-Centric Framework for Complex Tool-Augmented LLM Reasoning</h2><p><strong>Authors:Xiaolong Wei, Yuehu Dong, Xingliang Wang, Xingyu Zhang, Zhejun Zhao, Dongdong Shen, Long Xia, Dawei Yin</strong></p>
<p>Existing tool-augmented large language models (LLMs) encounter significant challenges when processing complex queries. Current frameworks such as ReAct are prone to local optimization traps due to their reliance on incremental decision-making processes. To address these limitations, we propose a novel Planner-centric Plan-Execute paradigm that fundamentally resolves local optimization bottlenecks through architectural innovation. Central to our approach is a novel Planner model that performs global Directed Acyclic Graph (DAG) planning for complex queries, enabling optimized execution beyond conventional tool coordination. We also introduce ComplexTool-Plan, a large-scale benchmark dataset featuring complex queries that demand sophisticated multi-tool composition and coordination capabilities. Additionally, we develop a two-stage training methodology that integrates Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO), systematically enhancing the Plannerâ€™s tool selection accuracy and global planning awareness through structured DAG-based planning. When integrated with a capable executor, our framework achieves state-of-the-art performance on the StableToolBench benchmark for complex user queries, demonstrating superior end-to-end execution capabilities and robust handling of intricate multi-tool workflows.</p>
<blockquote>
<p>ç°æœ‰çš„å·¥å…·å¢å¼ºå‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚å½“å‰æ¡†æ¶ï¼ˆå¦‚ReActï¼‰ç”±äºä¾èµ–å¢é‡å†³ç­–åˆ¶å®šè¿‡ç¨‹ï¼Œå®¹æ˜“é™·å…¥å±€éƒ¨ä¼˜åŒ–é™·é˜±ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ä»¥è§„åˆ’è€…ä¸ºä¸­å¿ƒçš„è§„åˆ’-æ‰§è¡ŒèŒƒå¼ï¼Œé€šè¿‡æ¶æ„åˆ›æ–°ä»æ ¹æœ¬ä¸Šè§£å†³å±€éƒ¨ä¼˜åŒ–ç“¶é¢ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªæ–°å‹è§„åˆ’å™¨æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯¹å¤æ‚æŸ¥è¯¢è¿›è¡Œå…¨å±€æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰è§„åˆ’ï¼Œèƒ½å¤Ÿå®ç°è¶…è¶Šä¼ ç»Ÿå·¥å…·åè°ƒçš„ä¼˜åŒ–æ‰§è¡Œã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ComplexTool-Planï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•æ•°æ®é›†ï¼ŒåŒ…å«éœ€è¦å¤æ‚çš„å¤šå·¥å…·ç»„åˆå’Œåè°ƒèƒ½åŠ›çš„å¤æ‚æŸ¥è¯¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œå°†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç›¸ç»“åˆï¼Œé€šè¿‡ç»“æ„åŒ–çš„DAGè§„åˆ’ï¼Œç³»ç»Ÿåœ°æé«˜äº†è§„åˆ’å™¨çš„å·¥å…·é€‰æ‹©å‡†ç¡®æ€§å’Œå…¨å±€è§„åˆ’æ„è¯†ã€‚å½“ä¸åŠŸèƒ½å¼ºå¤§çš„æ‰§è¡Œå™¨ç»“åˆæ—¶ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨StableToolBenchåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å¤æ‚ç”¨æˆ·æŸ¥è¯¢çš„å“è¶Šæ€§èƒ½ï¼Œå±•ç°äº†å‡ºè‰²çš„ç«¯åˆ°ç«¯æ‰§è¡Œèƒ½åŠ›å’Œå¤„ç†å¤æ‚å¤šå·¥å…·å·¥ä½œæµç¨‹çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10037v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹ç°æœ‰å·¥å…·å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„Planner-centric Plan-ExecuteèŒƒå¼ã€‚è¯¥èŒƒå¼é€šè¿‡æ¶æ„åˆ›æ–°ä»æ ¹æœ¬ä¸Šè§£å†³å±€éƒ¨ä¼˜åŒ–ç“¶é¢ˆé—®é¢˜ã€‚æ ¸å¿ƒåœ¨äºä¸€ä¸ªæ–°é¢–çš„Planneræ¨¡å‹ï¼Œå®ƒè¿›è¡Œå…¨å±€æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰è§„åˆ’ï¼Œä¸ºå¤æ‚æŸ¥è¯¢æä¾›ä¼˜åŒ–æ‰§è¡Œæ–¹æ¡ˆï¼Œè¶…è¶Šä¼ ç»Ÿå·¥å…·åè°ƒã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ComplexTool-Planå¤§å‹åŸºå‡†æ•°æ®é›†ï¼Œä»¥åŠä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œæ•´åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæå‡Plannerçš„å·¥å…·é€‰æ‹©å‡†ç¡®æ€§å’Œå…¨å±€è§„åˆ’æ„è¯†ã€‚ç»“åˆé«˜æ•ˆæ‰§è¡Œå™¨ï¼Œè¯¥æ¡†æ¶åœ¨StableToolBenchåŸºå‡†æµ‹è¯•ä¸­å®ç°å¤æ‚ç”¨æˆ·æŸ¥è¯¢çš„å“è¶Šæ€§èƒ½ï¼Œå±•ç°å‡ºå¼ºå¤§çš„ç«¯åˆ°ç«¯æ‰§è¡Œèƒ½åŠ›å’Œå¤„ç†å¤æ‚å¤šå·¥å…·å·¥ä½œæµç¨‹çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å·¥å…·å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„Planner-centric Plan-ExecuteèŒƒå¼ï¼Œé€šè¿‡æ¶æ„åˆ›æ–°è§£å†³å±€éƒ¨ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>Planneræ¨¡å‹è¿›è¡Œå…¨å±€æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰è§„åˆ’ï¼Œä¸ºå¤æ‚æŸ¥è¯¢æä¾›ä¼˜åŒ–æ‰§è¡Œæ–¹æ¡ˆã€‚</li>
<li>å¼•å…¥äº†ComplexTool-Planå¤§å‹åŸºå‡†æ•°æ®é›†ï¼Œç”¨äºæµ‹è¯•æ¨¡å‹å¤„ç†å¤æ‚æŸ¥è¯¢çš„èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œæ•´åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œæå‡Plannerçš„æ€§èƒ½ã€‚</li>
<li>ç»“åˆé«˜æ•ˆæ‰§è¡Œå™¨ï¼Œè¯¥æ¡†æ¶åœ¨StableToolBenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10037">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15543e81b15b4a0ea53087f38ed55c93" align="middle">
<img src="https://picx.zhimg.com/v2-79115e6d9dc041d63cef97edca328fef" align="middle">
<img src="https://picx.zhimg.com/v2-23826a8e0bc143dc3c67bfb451962950" align="middle">
<img src="https://picx.zhimg.com/v2-88155cd74f6e6f794742f0c62b09c694" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TinyChemVL-Advancing-Chemical-Vision-Language-Models-via-Efficient-Visual-Token-Reduction-and-Complex-Reaction-Tasks"><a href="#TinyChemVL-Advancing-Chemical-Vision-Language-Models-via-Efficient-Visual-Token-Reduction-and-Complex-Reaction-Tasks" class="headerlink" title="TinyChemVL: Advancing Chemical Vision-Language Models via Efficient Visual Token Reduction and Complex Reaction Tasks"></a>TinyChemVL: Advancing Chemical Vision-Language Models via Efficient Visual Token Reduction and Complex Reaction Tasks</h2><p><strong>Authors:Xuanle Zhao, Shuxin Zeng, Yinyuan Cai, Xiang Cheng, Duzhen Zhang, Xiuyi Chen, Bo Xu</strong></p>
<p>While Vision Language Models (VLMs) have demonstrated remarkable capabilities in general visual understanding, their application in the chemical domain has been limited, with previous works predominantly focusing on text and thus overlooking critical visual information, such as molecular structures. Current approaches that directly adopt standard VLMs for chemical tasks suffer from two primary issues: (i) computational inefficiency of processing entire chemical images with non-informative backgrounds. (ii) a narrow scope on molecular-level tasks that restricts progress in chemical reasoning. In this work, we propose \textbf{TinyChemVL}, an efficient and powerful chemical VLM that leverages visual token reduction and reaction-level tasks to improve model efficiency and reasoning capacity. Also, we propose \textbf{ChemRxn-V}, a reaction-level benchmark for assessing vision-based reaction recognition and prediction tasks. Directly predicting reaction products from molecular images poses a non-trivial challenge, as it requires models to integrate both recognition and reasoning capacities. Our results demonstrate that with only 4B parameters, TinyChemVL achieves superior performance on both molecular and reaction tasks while demonstrating faster inference and training speeds compared to existing models. Notably, TinyChemVL outperforms ChemVLM while utilizing only 1&#x2F;16th of the visual tokens. This work builds efficient yet powerful VLMs for chemical domains by co-designing model architecture and task complexity.</p>
<blockquote>
<p>è™½ç„¶è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ä¸€èˆ¬è§†è§‰ç†è§£æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨åŒ–å­¦é¢†åŸŸçš„åº”ç”¨å´å—åˆ°é™åˆ¶ã€‚ä¹‹å‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬ä¸Šï¼Œä»è€Œå¿½ç•¥äº†å…³é”®è§†è§‰ä¿¡æ¯ï¼Œå¦‚åˆ†å­ç»“æ„ã€‚å½“å‰ç›´æ¥é‡‡ç”¨æ ‡å‡†VLMsè¿›è¡ŒåŒ–å­¦ä»»åŠ¡çš„æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š(i)å¤„ç†å¸¦æœ‰éä¿¡æ¯èƒŒæ™¯çš„æ•´ä¸ªåŒ–å­¦å›¾åƒçš„è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚(ii)ä»»åŠ¡å±€é™äºåˆ†å­å±‚é¢ï¼Œé™åˆ¶äº†åŒ–å­¦æ¨ç†çš„è¿›å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>TinyChemVL</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆä¸”å¼ºå¤§çš„åŒ–å­¦VLMï¼Œå®ƒåˆ©ç”¨è§†è§‰æ ‡è®°å‡å°‘å’Œååº”çº§ä»»åŠ¡æ¥æé«˜æ¨¡å‹æ•ˆç‡å’Œæ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†<strong>ChemRxn-V</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°åŸºäºè§†è§‰çš„ååº”è¯†åˆ«å’Œé¢„æµ‹ä»»åŠ¡çš„ååº”çº§åŸºå‡†æµ‹è¯•ã€‚ç›´æ¥ä»åˆ†å­å›¾åƒé¢„æµ‹ååº”äº§ç‰©æ„æˆäº†ä¸€ä¸ªä¸å°çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒè¦æ±‚æ¨¡å‹åŒæ—¶å…·å¤‡è¯†åˆ«å’Œæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒTinyChemVLä»…æœ‰4Bå‚æ•°ï¼Œå°±èƒ½åœ¨åˆ†å­å’Œååº”ä»»åŠ¡ä¸Šå®ç°å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶åœ¨æ¨ç†å’Œè®­ç»ƒé€Ÿåº¦ä¸Šæ¯”ç°æœ‰æ¨¡å‹æ›´å¿«ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒTinyChemVLåœ¨åˆ©ç”¨åªæœ‰åå…­åˆ†ä¹‹ä¸€çš„è§†è§‰æ ‡è®°æ—¶ï¼Œä»ç„¶è¶…è¶Šäº†ChemVLMã€‚è¿™é¡¹å·¥ä½œé€šè¿‡å…±åŒè®¾è®¡æ¨¡å‹æ¶æ„å’Œä»»åŠ¡å¤æ‚åº¦ï¼Œä¸ºåŒ–å­¦é¢†åŸŸæ„å»ºäº†é«˜æ•ˆä¸”å¼ºå¤§çš„VLMsã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06283v1">PDF</a> Accepted by AAAI 2026, Preprint Version</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ–å­¦é¢†åŸŸçš„åº”ç”¨åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„åŒ–å­¦VLMâ€”â€”TinyChemVLï¼Œå®ƒé€šè¿‡è§†è§‰ä»¤ç‰Œå‡å°‘å’Œååº”çº§ä»»åŠ¡æ¥æé«˜æ¨¡å‹æ•ˆç‡å’Œæ¨ç†èƒ½åŠ›ã€‚åŒæ—¶ï¼Œè¿˜æå‡ºäº†ååº”çº§åŸºå‡†æµ‹è¯•ChemRxn-Vï¼Œç”¨äºè¯„ä¼°åŸºäºè§†è§‰çš„ååº”è¯†åˆ«å’Œé¢„æµ‹ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTinyChemVLåœ¨åˆ†å­å’Œååº”ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼Œå…·æœ‰æ›´å¿«çš„æ¨ç†å’Œè®­ç»ƒé€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨åŒ–å­¦é¢†åŸŸçš„åº”ç”¨é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¤„ç†åŒ–å­¦å›¾åƒæ—¶å­˜åœ¨è®¡ç®—æ•ˆç‡ä½ä¸‹å’Œä»»åŠ¡èŒƒå›´ç‹­çª„çš„é—®é¢˜ã€‚</li>
<li>TinyChemVLè¢«æå‡ºä½œä¸ºä¸€ç§é«˜æ•ˆçš„åŒ–å­¦VLMï¼Œé€šè¿‡è§†è§‰ä»¤ç‰Œå‡å°‘æé«˜æ¨¡å‹æ•ˆç‡ã€‚</li>
<li>TinyChemVLé€šè¿‡ç»“åˆååº”çº§ä»»åŠ¡ï¼Œæé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ChemRxn-VåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°åŸºäºè§†è§‰çš„ååº”è¯†åˆ«å’Œé¢„æµ‹ä»»åŠ¡ã€‚</li>
<li>ç›´æ¥ä»åˆ†å­å›¾åƒé¢„æµ‹ååº”äº§ç‰©éœ€è¦æ¨¡å‹å…·å¤‡è¯†åˆ«å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>TinyChemVLåœ¨åˆ†å­å’Œååº”ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç›¸æ¯”ç°æœ‰æ¨¡å‹å…·æœ‰æ›´å¿«çš„æ¨ç†å’Œè®­ç»ƒé€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06283">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b17d8f60546b708e5146e9005555cfd3" align="middle">
<img src="https://picx.zhimg.com/v2-c5ed64d6b004d1e6854d030fe95a638b" align="middle">
<img src="https://picx.zhimg.com/v2-bc578a43bc60197221a58bfa536910ef" align="middle">
<img src="https://picx.zhimg.com/v2-217e93f3a516b7a65182aa39cdf5c0cb" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="The-silence-of-the-weights-an-investigation-of-structural-pruning-strategies-for-attention-based-audio-signal-architectures"><a href="#The-silence-of-the-weights-an-investigation-of-structural-pruning-strategies-for-attention-based-audio-signal-architectures" class="headerlink" title="The silence of the weights: an investigation of structural pruning strategies for attention-based audio signal architectures"></a>The silence of the weights: an investigation of structural pruning strategies for attention-based audio signal architectures</h2><p><strong>Authors:Andrea Diecidue, Carlo Alberto Barbano, Piero Fraternali, Mathieu Fontaine, Enzo Tartaglione</strong></p>
<p>Transformer-based models have become the state of the art across multiple domains, from natural language processing to machine listening, thanks to attention mechanisms. However, the attention layers require a large number of parameters and high-end hardware for both training and inference. We propose a novel pruning technique targeted explicitly at the attention mechanism, where we decouple the pruning of the four layers in the attention block, namely: query, keys, values and outputsâ€™ projection matrices. We also investigate pruning strategies to prune along the head and channel dimensions, and compare the performance of the Audio Spectrogram Transformer (AST) model under different pruning scenarios. Our results show that even by pruning 50% of the attention parameters we incur in performance degradation of less than 1%</p>
<blockquote>
<p>åŸºäºTransformerçš„æ¨¡å‹å¾—ç›Šäºæ³¨æ„åŠ›æœºåˆ¶ï¼Œå·²ç»åœ¨å¤šä¸ªé¢†åŸŸæˆä¸ºå‰æ²¿æŠ€æœ¯ï¼Œæ— è®ºæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†è¿˜æ˜¯æœºå™¨å¬è§‰ã€‚ç„¶è€Œï¼Œæ³¨æ„åŠ›å±‚éœ€è¦å¤§é‡çš„å‚æ•°å’Œé«˜æ€§èƒ½ç¡¬ä»¶æ¥è¿›è¡Œè®­ç»ƒå’Œæ¨ç†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹æ³¨æ„åŠ›æœºåˆ¶çš„æ–°å‹å‰ªææŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯ä»¥è§£è€¦æ³¨æ„åŠ›å—ä¸­çš„å››å±‚å‰ªæï¼Œå³æŸ¥è¯¢ã€é”®ã€å€¼å’Œè¾“å‡ºæŠ•å½±çŸ©é˜µçš„å‰ªæã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†æ²¿ç€å¤´éƒ¨å’Œé€šé“ç»´åº¦çš„å‰ªæç­–ç•¥ï¼Œå¹¶æ¯”è¾ƒäº†ä¸åŒå‰ªæåœºæ™¯ä¸‹éŸ³é¢‘å…‰è°±å›¾Transformerï¼ˆASTï¼‰æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå³ä½¿å‰ªæ50%çš„æ³¨æ„åŠ›å‚æ•°ï¼Œæ€§èƒ½ä¸‹é™ä¹Ÿå°äº1%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.26207v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºTransformerçš„æ¨¡å‹å› æ³¨æ„åŠ›æœºåˆ¶è€Œåœ¨å¤šä¸ªé¢†åŸŸæˆä¸ºæœ€æ–°æŠ€æœ¯ã€‚ç„¶è€Œï¼Œæ³¨æ„åŠ›å±‚éœ€è¦å¤§é‡å‚æ•°å’Œé«˜æ€§èƒ½ç¡¬ä»¶è¿›è¡Œè®­ç»ƒå’Œæ¨ç†ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§é’ˆå¯¹æ³¨æ„åŠ›æœºåˆ¶çš„æ–°å‹å‰ªææŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯ä»¥è§£è€¦æ³¨æ„åŠ›å—ä¸­å››å±‚ï¼ˆæŸ¥è¯¢ã€é”®ã€å€¼å’Œè¾“å‡ºæŠ•å½±çŸ©é˜µï¼‰çš„å‰ªæã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜æ¢è®¨äº†æ²¿ç€å¤´éƒ¨å’Œé€šé“ç»´åº¦è¿›è¡Œå‰ªæçš„ç­–ç•¥ï¼Œå¹¶æ¯”è¾ƒäº†ä¸åŒå‰ªæåœºæ™¯ä¸‹Audio Spectrogram Transformerï¼ˆASTï¼‰æ¨¡å‹çš„æ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿å‰ªæ50%çš„æ³¨æ„åŠ›å‚æ•°ï¼Œæ€§èƒ½ä¸‹é™ä¹Ÿä¸åˆ°1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¨¡å‹å› æ³¨æ„åŠ›æœºåˆ¶åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å“è¶Šã€‚</li>
<li>æ³¨æ„åŠ›å±‚éœ€è¦å¤§é‡å‚æ•°å’Œé«˜æ€§èƒ½ç¡¬ä»¶èµ„æºã€‚</li>
<li>æå‡ºäº†ä¸€ç§é’ˆå¯¹æ³¨æ„åŠ›æœºåˆ¶çš„å‰ªææŠ€æœ¯ï¼Œå¯ç‹¬ç«‹å‰ªææ³¨æ„åŠ›å—ä¸­çš„å››å±‚ã€‚</li>
<li>æ¢è®¨äº†æ²¿å¤´éƒ¨å’Œé€šé“ç»´åº¦è¿›è¡Œå‰ªæçš„ç­–ç•¥ã€‚</li>
<li>å¯¹æ¯”äº†ä¸åŒå‰ªæåœºæ™¯ä¸‹çš„Audio Spectrogram Transformerï¼ˆASTï¼‰æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å‰ªæ50%çš„æ³¨æ„åŠ›å‚æ•°åï¼Œæ¨¡å‹æ€§èƒ½ä¸‹é™ä¸åˆ°1%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26207">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5819d3ef4795e2a51d22b048fdf6c25e" align="middle">
<img src="https://picx.zhimg.com/v2-0ddb3bdaa7a48b88f0f341d3d316bff6" align="middle">
<img src="https://picx.zhimg.com/v2-d12e65d9adba261dc110528e0c742d76" align="middle">
<img src="https://picx.zhimg.com/v2-0b66a7da19eb540bba552a12703f2961" align="middle">
<img src="https://picx.zhimg.com/v2-d279ec3617491f5f42a8546602e001c8" align="middle">
<img src="https://picx.zhimg.com/v2-eeec671a3873a7267e4031aa4da1decf" align="middle">
<img src="https://picx.zhimg.com/v2-6295f4e50f778c8e9cf30aeb01786b83" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="F2RVLM-Boosting-Fine-grained-Fragment-Retrieval-for-Multi-Modal-Long-form-Dialogue-with-Vision-Language-Model"><a href="#F2RVLM-Boosting-Fine-grained-Fragment-Retrieval-for-Multi-Modal-Long-form-Dialogue-with-Vision-Language-Model" class="headerlink" title="F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model"></a>F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model</h2><p><strong>Authors:Hanbo Bi, Zhiqiang Yuan, Zexi Jia, Jiapei Zhang, Chongyang Li, Peixiang Luo, Ying Deng, Xiaoyue Duan, Jinchao Zhang</strong></p>
<p>Traditional dialogue retrieval aims to select the most appropriate utterance or image from recent dialogue history. However, they often fail to meet usersâ€™ actual needs for revisiting semantically coherent content scattered across long-form conversations. To fill this gap, we define the Fine-grained Fragment Retrieval (FFR) task, requiring models to locate query-relevant fragments, comprising both utterances and images, from multimodal long-form dialogues. As a foundation for FFR, we construct MLDR, the longest-turn multimodal dialogue retrieval dataset to date, averaging 25.45 turns per dialogue, with each naturally spanning three distinct topics. To evaluate generalization in real-world scenarios, we curate and annotate a WeChat-based test set comprising real-world multimodal dialogues with an average of 75.38 turns. Building on these resources, we explore existing generation-based Vision-Language Models (VLMs) on FFR and observe that they often retrieve incoherent utterance-image fragments. While optimized for generating responses from visual-textual inputs, these models lack explicit supervision to ensure semantic coherence within retrieved fragments. To this end, we propose F2RVLM, a generative retrieval model trained in a two-stage paradigm: (1) supervised fine-tuning to inject fragment-level retrieval knowledge, and (2) GRPO-based reinforcement learning with multi-objective rewards promoting semantic precision, relevance, and contextual coherence. To handle varying intra-fragment complexity, from locally dense to sparsely distributed, we introduce difficulty-aware curriculum sampling that ranks training instances by model-predicted difficulty and gradually exposes the model to harder samples. This boosts reasoning ability in long, multi-turn contexts. F2RVLM outperforms popular VLMs in both in-domain and real-domain settings, demonstrating superior retrieval performance.</p>
<blockquote>
<p>ä¼ ç»Ÿå¯¹è¯æ£€ç´¢æ—¨åœ¨ä»æœ€è¿‘çš„å¯¹è¯å†å²ä¸­é€‰æ‹©æœ€åˆé€‚çš„è¯­å¥æˆ–å›¾åƒã€‚ç„¶è€Œï¼Œå®ƒä»¬å¾€å¾€ä¸èƒ½æ»¡è¶³ç”¨æˆ·é‡æ–°è®¿é—®é•¿ç¯‡å¯¹è¯ä¸­åˆ†æ•£çš„è¯­ä¹‰ç›¸å…³å†…å®¹çš„å®é™…éœ€æ±‚ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å®šä¹‰äº†ç²¾ç»†ç‰‡æ®µæ£€ç´¢ï¼ˆFFRï¼‰ä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹å®šä½ä¸æŸ¥è¯¢ç›¸å…³çš„ç‰‡æ®µï¼Œè¿™äº›ç‰‡æ®µåŒ…å«è¯­å¥å’Œå›¾åƒï¼Œæ¥è‡ªå¤šæ¨¡æ€é•¿ç¯‡å¯¹è¯ã€‚ä½œä¸ºFFRçš„åŸºç¡€ï¼Œæˆ‘ä»¬æ„å»ºäº†MLDRï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€é•¿çš„å¤šæ¨¡æ€å¯¹è¯æ£€ç´¢æ•°æ®é›†ï¼Œæ¯ä¸ªå¯¹è¯å¹³å‡åŒ…å«25.45è½®ï¼Œè‡ªç„¶æ¶µç›–ä¸‰ä¸ªä¸åŒçš„è¯é¢˜ã€‚ä¸ºäº†è¯„ä¼°åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬æ•´ç†å’Œæ³¨é‡Šäº†ä¸€ä¸ªåŸºäºå¾®ä¿¡çš„æµ‹è¯•é›†ï¼ŒåŒ…å«ç°å®ä¸–ç•Œçš„å¤šæ¨¡æ€å¯¹è¯ï¼Œå¹³å‡æ¯å¯¹è¯75.38è½®ã€‚åŸºäºè¿™äº›èµ„æºï¼Œæˆ‘ä»¬æ¢ç´¢äº†ç°æœ‰çš„åŸºäºç”Ÿæˆèƒ½åŠ›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨FFRä¸Šçš„åº”ç”¨ï¼Œå¹¶è§‚å¯Ÿåˆ°å®ƒä»¬ç»å¸¸æ£€ç´¢åˆ°ä¸è¿è´¯çš„è¯­å¥-å›¾åƒç‰‡æ®µã€‚è™½ç„¶è¿™äº›æ¨¡å‹ç»è¿‡ä¼˜åŒ–ï¼Œå¯ä»¥ä»è§†è§‰æ–‡æœ¬è¾“å…¥ä¸­ç”Ÿæˆå“åº”ï¼Œä½†å®ƒä»¬ç¼ºä¹æ˜ç¡®çš„ç›‘ç£æ¥ç¡®ä¿æ£€ç´¢åˆ°çš„ç‰‡æ®µä¸­çš„è¯­ä¹‰è¿è´¯æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†F2RVLMï¼Œä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒçš„ç”Ÿæˆæ£€ç´¢æ¨¡å‹ï¼šï¼ˆ1ï¼‰ç›‘ç£å¾®è°ƒæ³¨å…¥ç‰‡æ®µçº§æ£€ç´¢çŸ¥è¯†ï¼Œï¼ˆ2ï¼‰åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ ä¸å¤šç›®æ ‡å¥–åŠ±ç›¸ç»“åˆï¼Œä¿ƒè¿›è¯­ä¹‰ç²¾åº¦ã€ç›¸å…³æ€§å’Œä¸Šä¸‹æ–‡è¿è´¯æ€§ã€‚ä¸ºäº†è§£å†³ç‰‡æ®µå†…å¤æ‚æ€§çš„å·®å¼‚ï¼Œä»å±€éƒ¨å¯†é›†åˆ°ç¨€ç–åˆ†å¸ƒä¸ç­‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†éš¾åº¦æ„ŸçŸ¥è¯¾ç¨‹é‡‡æ ·ï¼Œè¯¥é‡‡æ ·é€šè¿‡æ¨¡å‹é¢„æµ‹çš„éš¾åº¦å¯¹è®­ç»ƒå®ä¾‹è¿›è¡Œæ’åï¼Œå¹¶é€æ¸æš´éœ²æ¨¡å‹äºæ›´å›°éš¾çš„æ ·æœ¬ã€‚è¿™æé«˜äº†é•¿ç¯‡å¤šè½®ä¸Šä¸‹æ–‡ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚F2RVLMåœ¨åŸŸå†…å’Œç°å®ä¸–ç•Œç¯å¢ƒä¸­å‡è¡¨ç°å‡ºä¼˜äºæµè¡ŒVLMsçš„æ£€ç´¢æ€§èƒ½ï¼Œä½“ç°äº†å…¶å“è¶Šçš„æ£€ç´¢èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17714v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¼ ç»Ÿå¯¹è¯æ£€ç´¢çš„å±€é™æ€§ï¼Œå¹¶ä»‹ç»é’ˆå¯¹æ­¤é—®é¢˜æå‡ºçš„ç²¾ç»†ç‰‡æ®µæ£€ç´¢ï¼ˆFFRï¼‰ä»»åŠ¡ã€‚ä¸ºæ”¯æŒFFRï¼Œæ„å»ºäº†MLDRæ•°æ®é›†ï¼Œå¼ºè°ƒå…¶æ˜¯å¤šæ¨¡æ€é•¿å¯¹è¯ä¸­å¹³å‡å¯¹è¯è½®æ¬¡æœ€å¤šçš„æ•°æ®é›†ã€‚åŒæ—¶ï¼Œæ–‡ç« æ¢è®¨äº†ç°æœ‰ç”Ÿæˆå¼è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†FFRä»»åŠ¡æ—¶çš„ä¸è¶³ï¼Œå¹¶æå‡ºäº†F2RVLMæ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ³•ï¼Œæ³¨é‡ç‰‡æ®µçº§æ£€ç´¢çŸ¥è¯†çš„æ³¨å…¥ä»¥åŠè¯­ä¹‰ç²¾åº¦ã€ç›¸å…³æ€§å’Œä¸Šä¸‹æ–‡è¿è´¯æ€§çš„æå‡ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†éš¾åº¦æ„ŸçŸ¥çš„è¯¾ç¨‹é‡‡æ ·ï¼Œä»¥æé«˜æ¨¡å‹åœ¨ä¸åŒå¤æ‚åº¦çš„ç‰‡æ®µå†…çš„æ¨ç†èƒ½åŠ›ã€‚æœ€ç»ˆç»“æœæ˜¾ç¤ºï¼ŒF2RVLMåœ¨åŸŸå†…å’ŒçœŸå®ç¯å¢ƒä¸‹çš„æ€§èƒ½å‡ä¼˜äºæµè¡Œçš„VLMsã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿå¯¹è¯æ£€ç´¢æ–¹æ³•ä¸»è¦å…³æ³¨é€‰æ‹©æœ€è¿‘çš„å¯¹è¯å†å²ä¸­çš„åˆé€‚è¯è¯­æˆ–å›¾åƒï¼Œä½†åœ¨é•¿å¯¹è¯ä¸­éš¾ä»¥æ»¡è¶³ç”¨æˆ·å¯¹è¯­ä¹‰è¿è´¯å†…å®¹çš„éœ€æ±‚ã€‚</li>
<li>æå‡ºç²¾ç»†ç‰‡æ®µæ£€ç´¢ï¼ˆFFRï¼‰ä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹ä»å¤šæ¨¡æ€é•¿å¯¹è¯ä¸­æ‰¾åˆ°ä¸æŸ¥è¯¢ç›¸å…³çš„ç‰‡æ®µã€‚</li>
<li>æ„å»ºMLDRæ•°æ®é›†ï¼Œæˆä¸ºè¿„ä»Šä¸ºæ­¢æœ€é•¿çš„å¤šæ¨¡æ€å¯¹è¯æ£€ç´¢æ•°æ®é›†ã€‚</li>
<li>ç°æœ‰ç”Ÿæˆå¼è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†FFRä»»åŠ¡æ—¶å­˜åœ¨ä¸è¶³ï¼Œä¸»è¦é—®é¢˜åœ¨äºè¯­ä¹‰è¿è´¯æ€§ã€‚</li>
<li>F2RVLMæ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ³•ï¼šé¦–å…ˆæ³¨å…¥ç‰‡æ®µçº§æ£€ç´¢çŸ¥è¯†ï¼Œç„¶åæå‡è¯­ä¹‰ç²¾åº¦ã€ç›¸å…³æ€§å’Œä¸Šä¸‹æ–‡è¿è´¯æ€§ã€‚</li>
<li>ä¸ºå¤„ç†ä¸åŒå¤æ‚åº¦çš„ç‰‡æ®µå†…é—®é¢˜ï¼Œå¼•å…¥äº†éš¾åº¦æ„ŸçŸ¥çš„è¯¾ç¨‹é‡‡æ ·æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17714">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f485d51b82392f052b94d67fa861e57f" align="middle">
<img src="https://picx.zhimg.com/v2-c7f62a80a58496d0377961aa4a9a78ad" align="middle">
<img src="https://picx.zhimg.com/v2-8750b0b23ac5a225d02c164926d1cd87" align="middle">
<img src="https://picx.zhimg.com/v2-bd6698a7e3b23471e2ae7df5771beb5e" align="middle">
<img src="https://picx.zhimg.com/v2-ea623fc1e485e22258b73ffb7867d2cf" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DIFFA-Large-Language-Diffusion-Models-Can-Listen-and-Understand"><a href="#DIFFA-Large-Language-Diffusion-Models-Can-Listen-and-Understand" class="headerlink" title="DIFFA: Large Language Diffusion Models Can Listen and Understand"></a>DIFFA: Large Language Diffusion Models Can Listen and Understand</h2><p><strong>Authors:Jiaming Zhou, Hongjie Chen, Shiwan Zhao, Jian Kang, Jie Li, Enzhi Wang, Yujie Guo, Haoqin Sun, Hui Wang, Aobo Kong, Yong Qin, Xuelong Li</strong></p>
<p>Recent advances in large language models (LLMs) have shown remarkable capabilities across textual and multimodal domains. In parallel, diffusion-based language models have emerged as a promising alternative to the autoregressive paradigm, offering improved controllability, bidirectional context modeling, and robust generation. However, their application to the audio modality remains underexplored. In this work, we introduce \textbf{DIFFA}, the first diffusion-based large audio-language model designed to perform spoken language understanding. DIFFA integrates a frozen diffusion language model with a lightweight dual-adapter architecture that bridges speech understanding and natural language reasoning. We employ a two-stage training pipeline: first, aligning semantic representations via an ASR objective; then, learning instruction-following abilities through synthetic audio-caption pairs automatically generated by prompting LLMs. Despite being trained on only 960 hours of ASR and 127 hours of synthetic instruction data, DIFFA demonstrates competitive performance on major benchmarks, including MMSU, MMAU, and VoiceBench, outperforming several autoregressive open-source baselines. Our results reveal the potential of diffusion-based language models for efficient and scalable audio understanding, opening a new direction for speech-driven AI. Our code will be available at <a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/DIFFA.git">https://github.com/NKU-HLT/DIFFA.git</a>.</p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥åœ¨æ–‡æœ¬å’Œå¤šæ¨¡æ€é¢†åŸŸè¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚ä¸æ­¤åŒæ—¶ï¼ŒåŸºäºæ‰©æ•£çš„è¯­è¨€æ¨¡å‹ä½œä¸ºè‡ªå›å½’èŒƒå¼çš„æœ‰å‰é€”çš„æ›¿ä»£å“è€Œå‡ºç°ï¼Œæä¾›äº†æ›´å¥½çš„å¯æ§æ€§ã€åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œç¨³å¥çš„ç”Ÿæˆã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨éŸ³é¢‘æ¨¡æ€çš„åº”ç”¨ä»ç„¶è¢«æ¢ç´¢å¾—å¾ˆå°‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†\textbf{DIFFA}ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºæ‰©æ•£çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è¿›è¡Œå£è¯­ç†è§£ã€‚DIFFAå°†å†»ç»“çš„æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸è½»é‡çº§çš„åŒé€‚é…å™¨æ¶æ„ç›¸ç»“åˆï¼Œè¯¥æ¶æ„æ¶èµ·äº†è¯­éŸ³ç†è§£å’Œè‡ªç„¶è¯­è¨€æ¨ç†ä¹‹é—´çš„æ¡¥æ¢ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ï¼šé¦–å…ˆï¼Œé€šè¿‡ASRç›®æ ‡å¯¹é½è¯­ä¹‰è¡¨ç¤ºï¼›ç„¶åï¼Œé€šè¿‡LLMæç¤ºè‡ªåŠ¨ç”Ÿæˆçš„åˆæˆéŸ³é¢‘å­—å¹•å¯¹æ¥å­¦ä¹ éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ã€‚å°½ç®¡åªåœ¨960å°æ—¶çš„ASRå’Œ127å°æ—¶çš„åˆæˆæŒ‡ä»¤æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒDIFFAåœ¨ä¸»è¦åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼ŒåŒ…æ‹¬MMSUã€MMAUå’ŒVoiceBenchï¼Œè¶…è¶Šäº†å¤šä¸ªè‡ªå›å½’å¼€æºåŸºå‡†ã€‚æˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†æ‰©æ•£å¼è¯­è¨€æ¨¡å‹åœ¨é«˜æ•ˆå’Œå¯æ‰©å±•çš„éŸ³é¢‘ç†è§£æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºè¯­éŸ³é©±åŠ¨çš„AIæ‰“å¼€äº†æ–°çš„ç ”ç©¶æ–¹å‘ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/DIFFA.git%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/NKU-HLT/DIFFA.gitä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18452v3">PDF</a> Accepted by AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å’Œå¤šæ¨¡æ€é¢†åŸŸå±•ç°å‡ºæ˜¾è‘—èƒ½åŠ›ï¼Œè€ŒåŸºäºæ‰©æ•£çš„è¯­è¨€æ¨¡å‹ä¸ºéŸ³é¢‘æ¨¡æ€çš„ç†è§£æä¾›äº†æ–°çš„æ–¹å‘ã€‚æœ¬ç ”ç©¶å¼•å…¥DIFFAï¼Œé¦–ä¸ªæ‰©æ•£å¤§éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œç”¨äºè¿›è¡Œå£è¯­ç†è§£ã€‚DIFFAç»“åˆå†»ç»“çš„æ‰©æ•£è¯­è¨€æ¨¡å‹å’Œè½»é‡çº§åŒé€‚é…å™¨æ¶æ„ï¼Œå®ç°äº†è¯­éŸ³ç†è§£ä¸è‡ªç„¶è¯­è¨€æ¨ç†çš„æ¡¥æ¢ä½œç”¨ã€‚é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ï¼Œé¦–å…ˆå¯¹è¯­ä¹‰è¡¨ç¤ºè¿›è¡Œå¯¹é½ï¼Œç„¶åå­¦ä¹ æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚åœ¨ä¸»è¦åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå±•ç°æ‰©æ•£æ¨¡å‹åœ¨éŸ³é¢‘ç†è§£æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å’Œå¤šæ¨¡æ€é¢†åŸŸå…·æœ‰æ˜¾è‘—èƒ½åŠ›ã€‚</li>
<li>åŸºäºæ‰©æ•£çš„è¯­è¨€æ¨¡å‹åœ¨éŸ³é¢‘æ¨¡æ€ç†è§£æ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>DIFFAæ˜¯é¦–ä¸ªæ‰©æ•£å¤§éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå£è¯­ç†è§£ã€‚</li>
<li>DIFFAç»“åˆå†»ç»“çš„æ‰©æ•£è¯­è¨€æ¨¡å‹å’Œè½»é‡çº§åŒé€‚é…å™¨æ¶æ„ã€‚</li>
<li>DIFFAé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ï¼ŒåŒ…æ‹¬è¯­ä¹‰è¡¨ç¤ºå¯¹é½å’ŒæŒ‡ä»¤è·Ÿéšèƒ½åŠ›å­¦ä¹ ã€‚</li>
<li>DIFFAåœ¨ä¸»è¦åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç«äº‰åŠ›è¶…è¿‡ä¸€äº›è‡ªåŠ¨å›å½’å¼€æºåŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18452">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-93d8135b03b15afd680975b0da8f7335" align="middle">
<img src="https://picx.zhimg.com/v2-a7077f3af8f05b29dfe8c6de4def1a13" align="middle">
<img src="https://picx.zhimg.com/v2-e818b91acaa3785be5b6a44bc29b9e90" align="middle">
<img src="https://picx.zhimg.com/v2-4a1220d90ca9cc844fe9d58293966c16" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Conversational-Intent-Driven-GraphRAG-Enhancing-Multi-Turn-Dialogue-Systems-through-Adaptive-Dual-Retrieval-of-Flow-Patterns-and-Context-Semantics"><a href="#Conversational-Intent-Driven-GraphRAG-Enhancing-Multi-Turn-Dialogue-Systems-through-Adaptive-Dual-Retrieval-of-Flow-Patterns-and-Context-Semantics" class="headerlink" title="Conversational Intent-Driven GraphRAG: Enhancing Multi-Turn Dialogue Systems through Adaptive Dual-Retrieval of Flow Patterns and Context Semantics"></a>Conversational Intent-Driven GraphRAG: Enhancing Multi-Turn Dialogue Systems through Adaptive Dual-Retrieval of Flow Patterns and Context Semantics</h2><p><strong>Authors:Ziqi Zhu, Tao Hu, Honglong Zhang, Dan Yang, HanGeng Chen, Mengran Zhang, Xilun Chen</strong></p>
<p>We present CID-GraphRAG (Conversational Intent-Driven Graph Retrieval Augmented Generation), a novel framework that addresses the limitations of existing dialogue systems in maintaining both contextual coherence and goal-oriented progression in multi-turn customer service conversations. Unlike traditional RAG systems that rely solely on semantic similarity (Conversation RAG) or standard knowledge graphs (GraphRAG), CID-GraphRAG constructs dynamic intent transition graphs from goal achieved historical dialogues and implements a dual-retrieval mechanism that adaptively balances intent-based graph traversal with semantic search. This approach enables the system to simultaneously leverage both conversional intent flow patterns and contextual semantics, significantly improving retrieval quality and response quality. In extensive experiments on real-world customer service dialogues, we employ both automatic metrics and LLM-as-judge assessments, demonstrating that CID-GraphRAG significantly outperforms both semantic-based Conversation RAG and intent-based GraphRAG baselines across all evaluation criteria. Quantitatively, CID-GraphRAG demonstrates substantial improvements over Conversation RAG across automatic metrics, with relative gains of 11% in BLEU, 5% in ROUGE-L, 6% in METEOR, and most notably, a 58% improvement in response quality according to LLM-as-judge evaluations. These results demonstrate that the integration of intent transition structures with semantic retrieval creates a synergistic effect that neither approach achieves independently, establishing CID-GraphRAG as an effective framework for addressing the challenges of maintaining contextual coherence and goal-oriented progression in knowledge-intensive multi-turn dialogues.</p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†CID-GraphRAGï¼ˆä¼šè¯æ„å›¾é©±åŠ¨çš„å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰å¯¹è¯ç³»ç»Ÿåœ¨ç»´æŒå¤šè½®å®¢æˆ·æœåŠ¡å¯¹è¯ä¸­çš„ä¸Šä¸‹æ–‡è¿è´¯æ€§å’Œç›®æ ‡å¯¼å‘è¿›å±•æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ä¼ ç»Ÿçš„ä»…ä¾èµ–è¯­ä¹‰ç›¸ä¼¼æ€§çš„RAGç³»ç»Ÿï¼ˆä¼šè¯RAGï¼‰æˆ–æ ‡å‡†çŸ¥è¯†å›¾è°±ï¼ˆGraphRAGï¼‰ä¸åŒï¼ŒCID-GraphRAGä»å·²å®ç°çš„å†å²å¯¹è¯ä¸­æ„å»ºåŠ¨æ€æ„å›¾è½¬ç§»å›¾ï¼Œå¹¶å®ç°äº†åŒæ£€ç´¢æœºåˆ¶ï¼Œè¯¥æœºåˆ¶è‡ªé€‚åº”åœ°å¹³è¡¡äº†åŸºäºæ„å›¾çš„å›¾éå†å’Œè¯­ä¹‰æœç´¢ã€‚è¿™ç§æ–¹æ³•ä½¿ç³»ç»Ÿèƒ½å¤ŸåŒæ—¶åˆ©ç”¨ä¼šè¯æ„å›¾æµåŠ¨æ¨¡å¼å’Œä¸Šä¸‹æ–‡è¯­ä¹‰ï¼Œä»è€Œå¤§å¤§æé«˜äº†æ£€ç´¢è´¨é‡å’Œå“åº”è´¨é‡ã€‚æˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œçš„å®¢æˆ·æœåŠ¡å¯¹è¯ä¸­è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œé‡‡ç”¨äº†è‡ªåŠ¨æŒ‡æ ‡å’ŒLLMè¯„ä¼°æ–¹æ³•ï¼Œè¯æ˜äº†CID-GraphRAGåœ¨å„é¡¹è¯„ä¼°æ ‡å‡†ä¸Šéƒ½æ˜¾è‘—ä¼˜äºè¯­ä¹‰åŸºç¡€çš„ä¼šè¯RAGå’ŒåŸºäºæ„å›¾çš„GraphRAGåŸºå‡†ã€‚å®šé‡ä¸Šï¼ŒCID-GraphRAGåœ¨è‡ªåŠ¨æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºä¼šè¯RAGï¼ŒBLEUå¾—åˆ†æé«˜11%ï¼ŒROUGE-Læé«˜5%ï¼ŒMETEORæé«˜6%ï¼Œæœ€å€¼å¾—ä¸€æçš„æ˜¯ï¼Œæ ¹æ®LLMè¯„ä¼°ï¼Œå“åº”è´¨é‡æé«˜äº†58%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ„å›¾è½¬ç§»ç»“æ„ä¸è¯­ä¹‰æ£€ç´¢çš„ç»“åˆäº§ç”Ÿäº†ååŒä½œç”¨ï¼Œå•ç‹¬ä»»ä½•ä¸€ç§æ–¹æ³•éƒ½æ— æ³•è¾¾åˆ°è¿™ç§æ•ˆæœï¼Œè¿™ä½¿å¾—CID-GraphRAGæˆä¸ºè§£å†³çŸ¥è¯†å¯†é›†å‹å¤šè½®å¯¹è¯ä¸­ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§å’Œç›®æ ‡å¯¼å‘è¿›å±•çš„æŒ‘æˆ˜çš„æœ‰æ•ˆæ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19385v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>CID-GraphRAGï¼ˆä¼šè¯æ„å›¾é©±åŠ¨çš„å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œè§£å†³äº†ç°æœ‰å¯¹è¯ç³»ç»Ÿåœ¨ç»´æŒå¤šè½®å®¢æˆ·æœåŠ¡å¯¹è¯ä¸­çš„ä¸Šä¸‹æ–‡è¿è´¯æ€§å’Œç›®æ ‡å¯¼å‘è¿›å±•çš„å±€é™æ€§ã€‚ä¸ä¼ ç»Ÿçš„ä»…ä¾èµ–è¯­ä¹‰ç›¸ä¼¼æ€§çš„RAGç³»ç»Ÿæˆ–æ ‡å‡†çŸ¥è¯†å›¾è°±ä¸åŒï¼ŒCID-GraphRAGä»å·²å®ç°çš„ç›®æ ‡å¯¹è¯ä¸­æ„å»ºåŠ¨æ€æ„å›¾è½¬æ¢å›¾ï¼Œå¹¶å®ç°äº†åŒæ£€ç´¢æœºåˆ¶ï¼Œè‡ªé€‚åº”åœ°å¹³è¡¡åŸºäºæ„å›¾çš„å›¾éå†å’Œè¯­ä¹‰æœç´¢ã€‚æ­¤æ–¹æ³•ä½¿ç³»ç»Ÿèƒ½å¤ŸåŒæ—¶åˆ©ç”¨ä¼šè¯æ„å›¾æµæ¨¡å¼å’Œä¸Šä¸‹æ–‡è¯­ä¹‰ï¼Œæ˜¾è‘—æé«˜äº†æ£€ç´¢è´¨é‡å’Œå“åº”è´¨é‡ã€‚åœ¨çœŸå®ä¸–ç•Œçš„å®¢æˆ·æœåŠ¡å¯¹è¯ä¸­çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCID-GraphRAGåœ¨è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å’Œäººç±»è¯„ä¼°ä¸­å‡æ˜¾è‘—ä¼˜äºåŸºäºè¯­ä¹‰çš„Conversation RAGå’ŒåŸºäºæ„å›¾çš„GraphRAGåŸºçº¿ã€‚å®šé‡æ•°æ®æ˜¾ç¤ºï¼ŒCID-GraphRAGåœ¨BLEUã€ROUGE-Lã€METEORç­‰è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ä¸Šç›¸å¯¹æé«˜äº†11%ã€5%ã€6%ï¼Œæœ€å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨äººç±»è¯„ä¼°ä¸­å“åº”è´¨é‡æé«˜äº†58%ã€‚ç»“æœè¡¨æ˜ï¼Œæ„å›¾è½¬æ¢ç»“æ„ä¸è¯­ä¹‰æ£€ç´¢çš„ç»“åˆäº§ç”Ÿäº†ååŒä½œç”¨ï¼Œå•ç‹¬ä»»ä½•ä¸€ç§æ–¹æ³•éƒ½æ— æ³•å®ç°ï¼Œå› æ­¤CID-GraphRAGæˆä¸ºåº”å¯¹çŸ¥è¯†å¯†é›†å‹å¤šè½®å¯¹è¯ä¸­ç»´æŒä¸Šä¸‹æ–‡è¿è´¯æ€§å’Œç›®æ ‡å¯¼å‘è¿›å±•æŒ‘æˆ˜çš„æœ‰æ•ˆæ¡†æ¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>CID-GraphRAGæ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œç”¨äºæ”¹è¿›å¤šè½®å¯¹è¯ç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>å®ƒé€šè¿‡æ„å»ºåŠ¨æ€æ„å›¾è½¬æ¢å›¾ï¼Œç»“åˆäº†ä¼šè¯æ„å›¾å’Œä¸Šä¸‹æ–‡è¯­ä¹‰ã€‚</li>
<li>CID-GraphRAGå®ç°äº†åŒæ£€ç´¢æœºåˆ¶ï¼Œå¹³è¡¡äº†åŸºäºæ„å›¾çš„å›¾éå†å’Œè¯­ä¹‰æœç´¢ã€‚</li>
<li>ä¸ä¼ ç»Ÿçš„RAGç³»ç»Ÿå’ŒçŸ¥è¯†å›¾è°±ç›¸æ¯”ï¼ŒCID-GraphRAGåœ¨ç»´æŒå¯¹è¯çš„ä¸Šä¸‹æ–‡è¿è´¯æ€§å’Œç›®æ ‡å¯¼å‘è¿›å±•æ–¹é¢æ›´ä¸ºæœ‰æ•ˆã€‚</li>
<li>åœ¨çœŸå®ä¸–ç•Œçš„å®¢æˆ·æœåŠ¡å¯¹è¯ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒCID-GraphRAGåœ¨è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å’Œäººç±»è¯„ä¼°ä¸­å‡è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>CID-GraphRAGåœ¨å“åº”è´¨é‡æ–¹é¢æœ‰æ˜æ˜¾çš„æ”¹è¿›ï¼Œè¿™å½’åŠŸäºå…¶ç»“åˆæ„å›¾è½¬æ¢ç»“æ„å’Œè¯­ä¹‰æ£€ç´¢çš„èƒ½åŠ›ã€‚</li>
<li>æ­¤æ¡†æ¶çš„å‡ºç°è§£å†³äº†çŸ¥è¯†å¯†é›†å‹å¤šè½®å¯¹è¯ä¸­çš„æŒ‘æˆ˜ï¼Œå¦‚ä¸Šä¸‹æ–‡è¿è´¯æ€§å’Œç›®æ ‡å¯¼å‘è¿›å±•çš„ç»´æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-92c723c6377d1b3dce4b6107fae977c6" align="middle">
<img src="https://picx.zhimg.com/v2-0b9072ee09f76760909c054e792ff209" align="middle">
<img src="https://picx.zhimg.com/v2-e6397f4006ce187bfb888309232a1ba5" align="middle">
<img src="https://picx.zhimg.com/v2-5691d6f6e6eed2def5bdb4a980c3ab6c" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Enhancing-Speech-to-Speech-Dialogue-Modeling-with-End-to-End-Retrieval-Augmented-Generation"><a href="#Enhancing-Speech-to-Speech-Dialogue-Modeling-with-End-to-End-Retrieval-Augmented-Generation" class="headerlink" title="Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation"></a>Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation</h2><p><strong>Authors:Pengchao Feng, Ziyang Ma, Wenxi Chen, Yao Li, Sheng Wang, Kai Yu, Xie Chen</strong></p>
<p>End-to-end speech-to-speech (S2S) dialogue systems have recently garnered increasing research attention for their lower latency and more natural integration of nonverbal cues such as emotion and speaker identity. However, these systems face key challenges, particularly in incorporating external knowledge, a capability commonly addressed by Retrieval-Augmented Generation (RAG) in text-based large language models (LLMs). The core difficulty lies in the modality gap between input speech and retrieved textual knowledge, which hinders effective integration of information. To address this issue, we propose a novel end-to-end RAG framework that directly retrieves relevant textual knowledge from speech queries. Experimental results demonstrate that our method significantly improves the performance of end-to-end S2S dialogue systems while achieving higher retrieval efficiency. Although the overall performance still lags behind the SOTA cascaded models, our framework offers a promising direction for enhancing knowledge integration in end-to-end S2S systems. Our code and dataset are released.</p>
<blockquote>
<p>ç«¯åˆ°ç«¯è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆS2Sï¼‰å¯¹è¯ç³»ç»Ÿå› å…¶è¾ƒä½çš„å»¶è¿Ÿå’Œæ›´è‡ªç„¶åœ°æ•´åˆéè¯­è¨€çº¿ç´¢ï¼ˆå¦‚æƒ…æ„Ÿå’Œè¯´è¯äººèº«ä»½ï¼‰è€Œæœ€è¿‘å¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶å…³æ³¨ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿé¢ä¸´å…³é”®æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•´åˆå¤–éƒ¨çŸ¥è¯†æ–¹é¢ï¼Œè¿™æ˜¯åŸºäºæ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­é€šå¸¸é€šè¿‡å¢å¼ºæ£€ç´¢ï¼ˆRAGï¼‰æ¥è§£å†³çš„èƒ½åŠ›ã€‚æ ¸å¿ƒå›°éš¾åœ¨äºè¾“å…¥è¯­éŸ³å’Œæ£€ç´¢åˆ°çš„æ–‡æœ¬çŸ¥è¯†ä¹‹é—´çš„æ¨¡å¼å·®è·ï¼Œè¿™é˜»ç¢äº†ä¿¡æ¯çš„æœ‰æ•ˆæ•´åˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç«¯åˆ°ç«¯RAGæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç›´æ¥ä»è¯­éŸ³æŸ¥è¯¢ä¸­æ£€ç´¢ç›¸å…³çš„æ–‡æœ¬çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç«¯åˆ°ç«¯S2Så¯¹è¯ç³»ç»Ÿçš„æ€§èƒ½ä¸Šæœ‰äº†æ˜¾è‘—çš„æå‡ï¼ŒåŒæ—¶æé«˜äº†æ£€ç´¢æ•ˆç‡ã€‚å°½ç®¡æ€»ä½“æ€§èƒ½ä»ç„¶è½åäºæœ€æ–°çš„çº§è”æ¨¡å‹ï¼Œä½†æˆ‘ä»¬çš„æ¡†æ¶ä¸ºå¢å¼ºç«¯åˆ°ç«¯S2Sç³»ç»Ÿä¸­çš„çŸ¥è¯†æ•´åˆæä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å·²ç»å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.00028v2">PDF</a> Accepted to EMNLP 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆS2Sï¼‰å¯¹è¯ç³»ç»Ÿå› å…¶ä½å»¶è¿Ÿå’Œèƒ½æ›´è‡ªç„¶åœ°æ•´åˆéè¨€è¯­çº¿ç´¢ï¼ˆå¦‚æƒ…æ„Ÿå’Œè¯´è¯äººèº«ä»½ï¼‰è€Œå¤‡å—ç ”ç©¶å…³æ³¨ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿåœ¨æ•´åˆå¤–éƒ¨çŸ¥è¯†æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨¡æ€é—´éš™é—®é¢˜ä¸Šã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç«¯åˆ°ç«¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥ä»è¯­éŸ³æŸ¥è¯¢ä¸­æ£€ç´¢ç›¸å…³æ–‡æœ¬çŸ¥è¯†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†ç«¯åˆ°ç«¯S2Så¯¹è¯ç³»ç»Ÿçš„æ€§èƒ½ï¼ŒåŒæ—¶æé«˜äº†æ£€ç´¢æ•ˆç‡ã€‚å°½ç®¡æ•´ä½“æ€§èƒ½ä»è½åäºå½“å‰å…ˆè¿›çš„çº§è”æ¨¡å‹ï¼Œä½†æˆ‘ä»¬çš„æ¡†æ¶ä¸ºå¢å¼ºç«¯åˆ°ç«¯S2Sç³»ç»Ÿä¸­çš„çŸ¥è¯†æ•´åˆæä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆS2Sï¼‰å¯¹è¯ç³»ç»Ÿæ­£å—åˆ°ç ”ç©¶å…³æ³¨ï¼Œå› å…¶ä½å»¶è¿Ÿå’Œèƒ½è‡ªç„¶æ•´åˆéè¨€è¯­çº¿ç´¢ã€‚</li>
<li>é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯æ•´åˆå¤–éƒ¨çŸ¥è¯†ï¼Œç‰¹åˆ«æ˜¯å¤„ç†æ¨¡æ€é—´éš™é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹ç«¯åˆ°ç«¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¡†æ¶ï¼Œèƒ½ä»è¯­éŸ³æŸ¥è¯¢ä¸­ç›´æ¥æ£€ç´¢ç›¸å…³æ–‡æœ¬çŸ¥è¯†ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•æé«˜äº†S2Så¯¹è¯ç³»ç»Ÿçš„æ€§èƒ½å’Œæ£€ç´¢æ•ˆç‡ã€‚</li>
<li>å°½ç®¡æ•´ä½“æ€§èƒ½ä»è½åäºå½“å‰å…ˆè¿›çš„çº§è”æ¨¡å‹ï¼Œä½†è¯¥æ¡†æ¶ä¸ºå¢å¼ºçŸ¥è¯†æ•´åˆæä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</li>
<li>æ¡†æ¶çš„ä»£ç å’Œæ•°æ®é›†å·²ç»å…¬å¼€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.00028">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2932519ab27ab5fc89d46b369ebe0457" align="middle">
<img src="https://picx.zhimg.com/v2-1ae1a7d681c82eabae8df366e112243c" align="middle">
<img src="https://picx.zhimg.com/v2-18b529b2b32009ba5044debfb0dbddb8" align="middle">
<img src="https://picx.zhimg.com/v2-f456b538c57c6cb91bbe43c021502568" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/Interactive/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/Interactive/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Interactive/">
                                    <span class="chip bg-color">Interactive</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3c65115714e83519b9c6ee5a8950d323" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  Time-to-Move Training-Free Motion Controlled Video Generation via Dual-Clock Denoising
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f56b9e6ef23d90258166f86d0b6688f1" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
