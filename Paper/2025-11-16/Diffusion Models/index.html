<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  VLF-MSC Vision-Language Feature-Based Multimodal Semantic Communication System">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-78be9d29ee5d64ce3b0b6b95e241ffc0')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    69 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-16-æ›´æ–°"><a href="#2025-11-16-æ›´æ–°" class="headerlink" title="2025-11-16 æ›´æ–°"></a>2025-11-16 æ›´æ–°</h1><h2 id="VLF-MSC-Vision-Language-Feature-Based-Multimodal-Semantic-Communication-System"><a href="#VLF-MSC-Vision-Language-Feature-Based-Multimodal-Semantic-Communication-System" class="headerlink" title="VLF-MSC: Vision-Language Feature-Based Multimodal Semantic Communication System"></a>VLF-MSC: Vision-Language Feature-Based Multimodal Semantic Communication System</h2><p><strong>Authors:Gwangyeon Ahn, Jiwan Seo, Joonhyuk Kang</strong></p>
<p>We propose Vision-Language Feature-based Multimodal Semantic Communication (VLF-MSC), a unified system that transmits a single compact vision-language representation to support both image and text generation at the receiver. Unlike existing semantic communication techniques that process each modality separately, VLF-MSC employs a pre-trained vision-language model (VLM) to encode the source image into a vision-language semantic feature (VLF), which is transmitted over the wireless channel. At the receiver, a decoder-based language model and a diffusion-based image generator are both conditioned on the VLF to produce a descriptive text and a semantically aligned image. This unified representation eliminates the need for modality-specific streams or retransmissions, improving spectral efficiency and adaptability. By leveraging foundation models, the system achieves robustness to channel noise while preserving semantic fidelity. Experiments demonstrate that VLF-MSC outperforms text-only and image-only baselines, achieving higher semantic accuracy for both modalities under low SNR with significantly reduced bandwidth.</p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†åŸºäºè§†è§‰è¯­è¨€ç‰¹å¾çš„å¤šåª’ä½“è¯­ä¹‰é€šä¿¡ï¼ˆVLF-MSCï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€ç³»ç»Ÿï¼Œå®ƒå°†å•ä¸ªç´§å‡‘çš„è§†è§‰è¯­è¨€è¡¨ç¤ºä¼ è¾“åˆ°æ¥æ”¶å™¨ï¼Œä»¥æ”¯æŒå›¾åƒå’Œæ–‡æœ¬çš„ç”Ÿæˆã€‚ä¸åŒäºç°æœ‰çš„è¯­ä¹‰é€šä¿¡æŠ€æœ¯ï¼Œåè€…æ˜¯åˆ†åˆ«å¤„ç†æ¯ç§æ¨¡æ€çš„ï¼ŒVLF-MSCåˆ™é‡‡ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¯¹æºå›¾åƒè¿›è¡Œç¼–ç ï¼Œç”Ÿæˆè§†è§‰è¯­è¨€è¯­ä¹‰ç‰¹å¾ï¼ˆVLFï¼‰ï¼Œå¹¶é€šè¿‡æ— çº¿ä¿¡é“è¿›è¡Œä¼ è¾“ã€‚åœ¨æ¥æ”¶å™¨ç«¯ï¼ŒåŸºäºè§£ç å™¨çš„è¯­è¨€æ¨¡å‹å’ŒåŸºäºæ‰©æ•£çš„å›¾åƒç”Ÿæˆå™¨éƒ½ä»¥VLFä¸ºæ¡ä»¶ï¼Œç”Ÿæˆæè¿°æ€§æ–‡æœ¬å’Œè¯­ä¹‰å¯¹é½çš„å›¾åƒã€‚è¿™ç§ç»Ÿä¸€è¡¨ç¤ºæ¶ˆé™¤äº†å¯¹ç‰¹å®šæ¨¡æ€çš„æµæˆ–é‡ä¼ çš„éœ€æ±‚ï¼Œæé«˜äº†é¢‘è°±æ•ˆç‡å’Œé€‚åº”æ€§ã€‚é€šè¿‡åˆ©ç”¨åŸºç¡€æ¨¡å‹ï¼Œè¯¥ç³»ç»Ÿåœ¨ä¿æŒè¯­ä¹‰ä¿çœŸåº¦çš„åŒæ—¶ï¼Œå®ç°äº†å¯¹ä¿¡é“å™ªå£°çš„é²æ£’æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒVLF-MSCä¼˜äºä»…ä½¿ç”¨æ–‡æœ¬æˆ–å›¾åƒçš„åŸºçº¿æ–¹æ³•ï¼Œåœ¨ä¿¡å™ªæ¯”ä½çš„æƒ…å†µä¸‹ï¼Œå¯¹ä¸¤ç§æ¨¡æ€éƒ½å®ç°äº†æ›´é«˜çš„è¯­ä¹‰å‡†ç¡®æ€§ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†å¸¦å®½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10074v1">PDF</a> To appear in the AI4NextG Workshop at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>VLF-MSCæ˜¯ä¸€ç§åŸºäºè§†è§‰è¯­è¨€ç‰¹å¾çš„å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡ç³»ç»Ÿï¼Œå®ƒèƒ½ä¼ è¾“å•ä¸€ç´§å‡‘çš„è§†è§‰è¯­è¨€è¡¨ç¤ºï¼Œæ”¯æŒæ¥æ”¶ç«¯çš„å›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å¯¹æºå›¾åƒè¿›è¡Œç¼–ç ï¼Œç”Ÿæˆè§†è§‰è¯­è¨€è¯­ä¹‰ç‰¹å¾ï¼ˆVLFï¼‰ï¼Œå¹¶é€šè¿‡æ— çº¿ä¿¡é“ä¼ è¾“ã€‚æ¥æ”¶ç«¯ä½¿ç”¨è§£ç å™¨å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£å‹å›¾åƒç”Ÿæˆå™¨ï¼ŒåŸºäºVLFç”Ÿæˆæè¿°æ€§æ–‡æœ¬å’Œè¯­ä¹‰å¯¹é½çš„å›¾åƒã€‚æ­¤ç»Ÿä¸€è¡¨ç¤ºæé«˜äº†é¢‘è°±æ•ˆç‡å’Œé€‚åº”æ€§ï¼Œå®ç°äº†å¯¹é€šé“å™ªå£°çš„é²æ£’æ€§ï¼ŒåŒæ—¶ä¿æŒäº†è¯­ä¹‰ä¿çœŸåº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒVLF-MSCåœ¨ä½ä¿¡å™ªæ¯”ä¸‹å®ç°äº†è¾ƒé«˜çš„è¯­ä¹‰å‡†ç¡®æ€§ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†å¸¦å®½ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLF-MSCç³»ç»Ÿæå‡ºä¸€ç§åŸºäºè§†è§‰è¯­è¨€ç‰¹å¾çš„å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡æ–¹æ³•ã€‚</li>
<li>ç³»ç»Ÿé‡‡ç”¨é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹å¯¹æºå›¾åƒè¿›è¡Œç¼–ç ï¼Œç”Ÿæˆè§†è§‰è¯­è¨€è¯­ä¹‰ç‰¹å¾ï¼ˆVLFï¼‰ã€‚</li>
<li>æ¥æ”¶ç«¯ä½¿ç”¨è§£ç å™¨å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£å‹å›¾åƒç”Ÿæˆå™¨ï¼ŒåŸºäºVLFç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒã€‚</li>
<li>æ­¤ç³»ç»Ÿå®ç°äº†ç»Ÿä¸€è¡¨ç¤ºï¼Œæé«˜äº†é¢‘è°±æ•ˆç‡å’Œé€‚åº”æ€§ã€‚</li>
<li>åˆ©ç”¨åŸºç¡€æ¨¡å‹ï¼Œç³»ç»Ÿå®ç°äº†å¯¹é€šé“å™ªå£°çš„é²æ£’æ€§ï¼Œä¿æŒäº†è¯­ä¹‰ä¿çœŸåº¦ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒVLF-MSCåœ¨ä½ä¿çœŸç¯å¢ƒä¸‹è¡¨ç°å‡ºè¾ƒé«˜çš„è¯­ä¹‰å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-86f83d99752cf757a735b9eb51d71b7a" align="middle">
<img src="https://picx.zhimg.com/v2-a6355b664db71d360f1c483ec34e6b7b" align="middle">
<img src="https://picx.zhimg.com/v2-394a461b54db361646163b166a7264ff" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="STELLAR-Scene-Text-Editor-for-Low-Resource-Languages-and-Real-World-Data"><a href="#STELLAR-Scene-Text-Editor-for-Low-Resource-Languages-and-Real-World-Data" class="headerlink" title="STELLAR: Scene Text Editor for Low-Resource Languages and Real-World Data"></a>STELLAR: Scene Text Editor for Low-Resource Languages and Real-World Data</h2><p><strong>Authors:Yongdeuk Seo, Hyun-seok Min, Sungchul Choi</strong></p>
<p>Scene Text Editing (STE) is the task of modifying text content in an image while preserving its visual style, such as font, color, and background. While recent diffusion-based approaches have shown improvements in visual quality, key limitations remain: lack of support for low-resource languages, domain gap between synthetic and real data, and the absence of appropriate metrics for evaluating text style preservation. To address these challenges, we propose STELLAR (Scene Text Editor for Low-resource LAnguages and Real-world data). STELLAR enables reliable multilingual editing through a language-adaptive glyph encoder and a multi-stage training strategy that first pre-trains on synthetic data and then fine-tunes on real images. We also construct a new dataset, STIPLAR(Scene Text Image Pairs of Low-resource lAnguages and Real-world data), for training and evaluation. Furthermore, we propose Text Appearance Similarity (TAS), a novel metric that assesses style preservation by independently measuring font, color, and background similarity, enabling robust evaluation even without ground truth. Experimental results demonstrate that STELLAR outperforms state-of-the-art models in visual consistency and recognition accuracy, achieving an average TAS improvement of 2.2% across languages over the baselines.</p>
<blockquote>
<p>åœºæ™¯æ–‡æœ¬ç¼–è¾‘ï¼ˆSTEï¼‰çš„ä»»åŠ¡æ˜¯åœ¨ä¿æŒå›¾åƒè§†è§‰é£æ ¼ï¼ˆå¦‚å­—ä½“ã€é¢œè‰²å’ŒèƒŒæ™¯ï¼‰çš„åŒæ—¶ä¿®æ”¹å›¾åƒä¸­çš„æ–‡æœ¬å†…å®¹ã€‚è™½ç„¶æœ€è¿‘çš„æ‰©æ•£æ–¹æ³•åœ¨æé«˜è§†è§‰è´¨é‡æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ä»å­˜åœ¨å…³é”®é™åˆ¶ï¼šä¸æ”¯æŒä½èµ„æºè¯­è¨€ã€åˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œä»¥åŠç¼ºä¹è¯„ä¼°æ–‡æœ¬é£æ ¼ä¿æŒæƒ…å†µçš„åˆé€‚æŒ‡æ ‡ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†STELLARï¼ˆåœºæ™¯æ–‡æœ¬ç¼–è¾‘å™¨ï¼Œé€‚ç”¨äºä½èµ„æºè¯­è¨€å’ŒçœŸå®ä¸–ç•Œæ•°æ®ï¼‰ã€‚STELLARé€šè¿‡è¯­è¨€è‡ªé€‚åº”çš„ç¬¦å·ç¼–ç å™¨ä»¥åŠåˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå®ç°åœ¨åˆæˆæ•°æ®ä¸Šå…ˆè¿›è¡Œé¢„è®­ç»ƒï¼Œå†åœ¨çœŸå®å›¾åƒä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»è€Œå®ç°å¯é çš„å¤šè¯­è¨€ç¼–è¾‘ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ç”¨äºè®­ç»ƒå’Œè¯„ä¼°çš„æ–°æ•°æ®é›†STIPLARï¼ˆåœºæ™¯æ–‡æœ¬å›¾åƒå¯¹ä½èµ„æºè¯­è¨€å’ŒçœŸå®ä¸–ç•Œæ•°æ®ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æ–‡æœ¬å¤–è§‚ç›¸ä¼¼æ€§ï¼ˆTASï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æŒ‡æ ‡ï¼Œé€šè¿‡ç‹¬ç«‹æµ‹é‡å­—ä½“ã€é¢œè‰²å’ŒèƒŒæ™¯ç›¸ä¼¼æ€§æ¥è¯„ä¼°é£æ ¼ä¿æŒæƒ…å†µï¼Œå³ä½¿åœ¨ç¼ºä¹çœŸå®å€¼çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å®ç°ç¨³å¥è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTELLARåœ¨è§†è§‰ä¸€è‡´æ€§å’Œè¯†åˆ«å‡†ç¡®æ€§æ–¹é¢ä¼˜äºæœ€å…ˆè¿›æ¨¡å‹ï¼Œåœ¨è¯­è¨€åŸºçº¿çš„åŸºç¡€ä¸Šå¹³å‡æé«˜äº†2.2%çš„TASã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09977v1">PDF</a> Accepted to AAAI Workshop (Artificial Intelligence with Biased or Scarce Data)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºåœºæ™¯æ–‡æœ¬ç¼–è¾‘ï¼ˆSTEï¼‰çš„ä»»åŠ¡æ˜¯ä¿®æ”¹å›¾åƒä¸­çš„æ–‡æœ¬å†…å®¹ï¼ŒåŒæ—¶ä¿ç•™å…¶è§†è§‰é£æ ¼ï¼Œå¦‚å­—ä½“ã€é¢œè‰²å’ŒèƒŒæ™¯ã€‚é’ˆå¯¹ä½èµ„æºè¯­è¨€ã€åˆæˆä¸ç°å®æ•°æ®ä¹‹é—´çš„é¢†åŸŸå·®è·ä»¥åŠç¼ºä¹é€‚å½“çš„æ–‡æœ¬é£æ ¼ä¿å­˜è¯„ä¼°æŒ‡æ ‡ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†STELLARï¼ˆåœºæ™¯æ–‡æœ¬ç¼–è¾‘å™¨ï¼Œç”¨äºä½èµ„æºè¯­è¨€å’ŒçœŸå®ä¸–ç•Œæ•°æ®ï¼‰ã€‚STELLARé€šè¿‡è¯­è¨€è‡ªé€‚åº”çš„ç¬¦å·ç¼–ç å™¨å’Œå¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå®ç°äº†å¯é çš„å¤šè¯­è¨€ç¼–è¾‘ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†STIPLARï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æŒ‡æ ‡Text Appearance Similarityï¼ˆTASï¼‰ï¼Œä»¥ç‹¬ç«‹æµ‹é‡å­—ä½“ã€é¢œè‰²å’ŒèƒŒæ™¯ç›¸ä¼¼æ€§ï¼Œä»è€Œåœ¨æ²¡æœ‰çœŸå®å€¼çš„æƒ…å†µä¸‹è¿›è¡Œç¨³å¥è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTELLARåœ¨è§†è§‰ä¸€è‡´æ€§å’Œè¯†åˆ«å‡†ç¡®æ€§æ–¹é¢ä¼˜äºæœ€æ–°æ¨¡å‹ï¼Œåœ¨å„ç§è¯­è¨€ä¸Šå¹³å‡æé«˜äº†2.2%çš„TASã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Scene Text Editing (STE)çš„ä»»åŠ¡æ˜¯ä¿®æ”¹å›¾åƒä¸­çš„æ–‡æœ¬å†…å®¹ï¼ŒåŒæ—¶ä¿ç•™å…¶è§†è§‰é£æ ¼ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ–¹æ³•åœ¨å¤„ç†æ–‡æœ¬å›¾åƒç¼–è¾‘æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚æ”¯æŒä½èµ„æºè¯­è¨€ã€åˆæˆä¸ç°å®æ•°æ®é—´çš„é¢†åŸŸå·®è·ç­‰ã€‚</li>
<li>æå‡ºäº†STELLARæ–¹æ³•ï¼Œé€šè¿‡è¯­è¨€è‡ªé€‚åº”çš„ç¬¦å·ç¼–ç å™¨å’Œå¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>æ„å»ºæ–°çš„æ•°æ®é›†STIPLARç”¨äºè®­ç»ƒå’Œè¯„ä¼°ã€‚</li>
<li>æå‡ºäº†æ–°è¯„ä¼°æŒ‡æ ‡Text Appearance Similarity (TAS)ï¼Œç”¨äºç‹¬ç«‹æµ‹é‡å­—ä½“ã€é¢œè‰²å’ŒèƒŒæ™¯ç›¸ä¼¼æ€§ã€‚</li>
<li>STELLARåœ¨è§†è§‰ä¸€è‡´æ€§å’Œè¯†åˆ«å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-54d43973a00d11f955811eafa3fbc2bc" align="middle">
<img src="https://picx.zhimg.com/v2-f9bf46ca7d66fba301745ac493ae8101" align="middle">
<img src="https://picx.zhimg.com/v2-55d27f2bac797deafff96fa8c996dfe4" align="middle">
<img src="https://picx.zhimg.com/v2-c5db1faee7467269ad81cf16729fde90" align="middle">
<img src="https://picx.zhimg.com/v2-3b3efe257d648440fc2f52a020495990" align="middle">
<img src="https://picx.zhimg.com/v2-202ae2f6204a7cb25b147630c6ebb71f" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Equivariant-Sampling-for-Improving-Diffusion-Model-based-Image-Restoration"><a href="#Equivariant-Sampling-for-Improving-Diffusion-Model-based-Image-Restoration" class="headerlink" title="Equivariant Sampling for Improving Diffusion Model-based Image Restoration"></a>Equivariant Sampling for Improving Diffusion Model-based Image Restoration</h2><p><strong>Authors:Chenxu Wu, Qingpeng Kong, Peiang Zhao, Wendi Yang, Wenxin Ma, Fenghe Tang, Zihang Jiang, S. Kevin Zhou</strong></p>
<p>Recent advances in generative models, especially diffusion models, have significantly improved image restoration (IR) performance. However, existing problem-agnostic diffusion model-based image restoration (DMIR) methods face challenges in fully leveraging diffusion priors, resulting in suboptimal performance. In this paper, we address the limitations of current problem-agnostic DMIR methods by analyzing their sampling process and providing effective solutions. We introduce EquS, a DMIR method that imposes equivariant information through dual sampling trajectories. To further boost EquS, we propose the Timestep-Aware Schedule (TAS) and introduce EquS$^+$. TAS prioritizes deterministic steps to enhance certainty and sampling efficiency. Extensive experiments on benchmarks demonstrate that our method is compatible with previous problem-agnostic DMIR methods and significantly boosts their performance without increasing computational costs. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/FouierL/EquS">https://github.com/FouierL/EquS</a>.</p>
<blockquote>
<p>æœ€è¿‘ç”Ÿæˆæ¨¡å‹ï¼Œå°¤å…¶æ˜¯æ‰©æ•£æ¨¡å‹çš„è¿›å±•ï¼Œå·²ç»æ˜¾è‘—æé«˜äº†å›¾åƒæ¢å¤ï¼ˆIRï¼‰çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¸é—®é¢˜æ— å…³çš„æ‰©æ•£æ¨¡å‹å›¾åƒæ¢å¤ï¼ˆDMIRï¼‰æ–¹æ³•åœ¨é¢å¯¹å……åˆ†åˆ©ç”¨æ‰©æ•£å…ˆéªŒçš„æŒ‘æˆ˜æ—¶ï¼Œè¡¨ç°å¹¶ä¸ç†æƒ³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡åˆ†æç°æœ‰ä¸é—®é¢˜æ— å…³çš„DMIRæ–¹æ³•çš„é‡‡æ ·è¿‡ç¨‹ï¼Œæå‡ºæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†å…¶å±€é™æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†EquSï¼Œä¸€ç§é€šè¿‡åŒé‡‡æ ·è½¨è¿¹æ–½åŠ ç­‰ä»·ä¿¡æ¯çš„DMIRæ–¹æ³•ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡EquSçš„æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†æ—¶åºæ„ŸçŸ¥è°ƒåº¦ï¼ˆTASï¼‰å¹¶å¼•å…¥äº†EquS+ã€‚TASé€šè¿‡ä¼˜å…ˆæ‰§è¡Œç¡®å®šæ€§æ­¥éª¤æ¥æé«˜ç¡®å®šæ€§å’Œé‡‡æ ·æ•ˆç‡ã€‚åœ¨åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä¹‹å‰çš„ä¸é—®é¢˜æ— å…³çš„DMIRæ–¹æ³•å…¼å®¹ï¼Œå¹¶èƒ½æ˜¾è‘—æé«˜å®ƒä»¬çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸å¢åŠ è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/FouierL/EquS">https://github.com/FouierL/EquS</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09965v1">PDF</a> 12 pages, 9 figures</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒä¿®å¤ï¼ˆIRï¼‰é¢†åŸŸçš„åº”ç”¨å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰çš„é—®é¢˜æ— å…³çš„æ‰©æ•£æ¨¡å‹å›¾åƒä¿®å¤æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨æ‰©æ•£å…ˆéªŒä¿¡æ¯ï¼Œæ€§èƒ½æœ‰å¾…æé«˜ã€‚æœ¬æ–‡é€šè¿‡åˆ†æç°æœ‰æ–¹æ³•çš„é‡‡æ ·è¿‡ç¨‹ï¼Œæå‡ºä¸€ç§åä¸ºEquSçš„æ‰©æ•£æ¨¡å‹å›¾åƒä¿®å¤æ–¹æ³•ï¼Œé€šè¿‡åŒé‡‡æ ·è½¨è¿¹å¼•å…¥ç­‰ä»·ä¿¡æ¯æ¥æé«˜æ€§èƒ½ã€‚ä¸ºè¿›ä¸€æ­¥å¢å¼ºEquSçš„æ•ˆæœï¼Œæœ¬æ–‡è¿˜æå‡ºäº†æ—¶é—´è¡¨æ„ŸçŸ¥è°ƒåº¦ï¼ˆTASï¼‰ï¼Œå¹¶åœ¨EquSçš„åŸºç¡€ä¸Šæ¨å‡ºäº†EquS+ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¸ç°æœ‰é—®é¢˜æ— å…³çš„æ‰©æ•£æ¨¡å‹å›¾åƒä¿®å¤æ–¹æ³•å…¼å®¹ï¼Œèƒ½æ˜¾è‘—æé«˜æ€§èƒ½ä¸”ä¸ä¼šå¢åŠ è®¡ç®—æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒä¿®å¤é¢†åŸŸçš„æ€§èƒ½æ˜¾è‘—æå‡ã€‚</li>
<li>ç°æœ‰çš„é—®é¢˜æ— å…³çš„æ‰©æ•£æ¨¡å‹å›¾åƒä¿®å¤æ–¹æ³•åœ¨åˆ©ç”¨æ‰©æ•£å…ˆéªŒä¿¡æ¯æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>EquSæ–¹æ³•é€šè¿‡åŒé‡‡æ ·è½¨è¿¹å¼•å…¥ç­‰ä»·ä¿¡æ¯ï¼Œä»¥æé«˜å›¾åƒä¿®å¤æ€§èƒ½ã€‚</li>
<li>TASè°ƒåº¦ç­–ç•¥èƒ½å¢å¼ºEquSçš„æ•ˆæœï¼Œæé«˜ç¡®å®šæ€§å’Œé‡‡æ ·æ•ˆç‡ã€‚</li>
<li>EquS+æ˜¯EquSçš„å‡çº§ç‰ˆï¼Œä¸ç°æœ‰æ–¹æ³•å…¼å®¹ï¼Œèƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚</li>
<li>å¤§é‡å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09965">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f7b1051e7502d6caea4d4b12998466de" align="middle">
<img src="https://picx.zhimg.com/v2-c9fda94da934c88ff7847e2e28090611" align="middle">
<img src="https://picx.zhimg.com/v2-a0e6a38a4aa1e0936f320e4a7933624e" align="middle">
<img src="https://picx.zhimg.com/v2-6bca5c897a3098698115a2a2dafa667f" align="middle">
<img src="https://picx.zhimg.com/v2-44e9a77105d1cb913c899364add69de3" align="middle">
<img src="https://picx.zhimg.com/v2-7fa9ff800c3a2e6b1f5b5944accc1dd8" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="GPDM-Generation-Prior-Diffusion-Model-for-Accelerated-Direct-Attenuation-and-Scatter-Correction-of-Whole-body-18F-FDG-PET"><a href="#GPDM-Generation-Prior-Diffusion-Model-for-Accelerated-Direct-Attenuation-and-Scatter-Correction-of-Whole-body-18F-FDG-PET" class="headerlink" title="GPDM: Generation-Prior Diffusion Model for Accelerated Direct Attenuation and Scatter Correction of Whole-body 18F-FDG PET"></a>GPDM: Generation-Prior Diffusion Model for Accelerated Direct Attenuation and Scatter Correction of Whole-body 18F-FDG PET</h2><p><strong>Authors:Min Jeong Cho, Hyeong Seok Shim, Sungyu Kim, Jae Sung Lee</strong></p>
<p>Accurate attenuation and scatter corrections are crucial in positron emission tomography (PET) imaging for accurate visual interpretation and quantitative analysis. Traditional methods relying on computed tomography (CT) or magnetic resonance imaging (MRI) have limitations in accuracy, radiation exposure, and applicability. Deep neural networks provide potential approaches to estimating attenuation and scatter-corrected (ASC) PET from non-attenuation and non-scatter-corrected (NASC) PET images based on VAE or CycleGAN. However, the limitations inherent to conventional GAN-based methods, such as unstable training and mode collapse, need further advancements. To address these limitations and achieve more accurate attenuation and scatter corrections, we propose a novel framework for generating high-quality ASC PET images from NASC PET images: Generation-Prior Diffusion Model (GPDM). Our GPDM framework is based on the Denoising Diffusion Probabilistic Model (DDPM), but instead of starting sampling from an entirely different image distribution, it begins from a distribution similar to the target images we aim to generate. This similar distribution is referred to as the Generation-Prior. By leveraging this Generation-Prior, the GPDM framework effectively reduces the number of sampling steps and generates more refined ASC PET images. Our experimental results demonstrate that GPDM outperforms existing methods in generating ASC PET images, achieving superior accuracy while significantly reducing sampling time. These findings highlight the potential of GPDM to address the limitations of conventional methods and establish a new standard for efficient and accurate attenuation and scatter correction in PET imaging.</p>
<blockquote>
<p>å‡†ç¡®çš„è¡°å‡å’Œæ•£å°„æ ¡æ­£å¯¹äºæ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰æˆåƒçš„å‡†ç¡®è§†è§‰è§£è¯»å’Œå®šé‡åˆ†æè‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ä¾èµ–äºè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æˆ–ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ï¼Œåœ¨å‡†ç¡®æ€§ã€è¾å°„æš´éœ²å’Œé€‚ç”¨æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æ·±åº¦ç¥ç»ç½‘ç»œæä¾›äº†åŸºäºå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æˆ–CycleGANä»éè¡°å‡å’Œéæ•£å°„æ ¡æ­£ï¼ˆNASCï¼‰PETå›¾åƒä¼°è®¡è¡°å‡å’Œæ•£å°„æ ¡æ­£ï¼ˆASCï¼‰PETçš„æ½œåœ¨æ–¹æ³•ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ–¹æ³•å›ºæœ‰çš„å±€é™æ€§ï¼Œå¦‚è®­ç»ƒä¸ç¨³å®šå’Œæ¨¡å¼å´©æºƒï¼Œéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œå®ç°æ›´å‡†ç¡®çš„è¡°å‡å’Œæ•£å°„æ ¡æ­£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»NASC PETå›¾åƒç”Ÿæˆé«˜è´¨é‡ASC PETå›¾åƒçš„æ–°å‹æ¡†æ¶ï¼šç”Ÿæˆä¼˜å…ˆæ‰©æ•£æ¨¡å‹ï¼ˆGPDMï¼‰ã€‚æˆ‘ä»¬çš„GPDMæ¡†æ¶åŸºäºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ï¼Œä½†ä¸åŒäºä»ä¸€ä¸ªå®Œå…¨ä¸åŒçš„å›¾åƒåˆ†å¸ƒå¼€å§‹é‡‡æ ·ï¼Œå®ƒä»ä¸€ä¸ªç±»ä¼¼äºæˆ‘ä»¬æ—¨åœ¨ç”Ÿæˆçš„ç›®æ ‡å›¾åƒåˆ†å¸ƒå¼€å§‹ã€‚è¿™ç§ç±»ä¼¼çš„åˆ†å¸ƒè¢«ç§°ä¸ºç”Ÿæˆä¼˜å…ˆã€‚é€šè¿‡åˆ©ç”¨è¿™ç§ç”Ÿæˆä¼˜å…ˆæƒï¼ŒGPDMæ¡†æ¶æœ‰æ•ˆåœ°å‡å°‘äº†é‡‡æ ·æ­¥éª¤çš„æ•°é‡ï¼Œå¹¶ç”Ÿæˆäº†æ›´ç²¾ç»†çš„ASC PETå›¾åƒã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGPDMåœ¨ç”ŸæˆASC PETå›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†é‡‡æ ·æ—¶é—´ã€‚è¿™äº›å‘ç°çªå‡ºäº†GPDMè§£å†³ä¼ ç»Ÿæ–¹æ³•å±€é™æ€§çš„æ½œåŠ›ï¼Œå¹¶ä¸ºPETæˆåƒä¸­çš„é«˜æ•ˆå’Œå‡†ç¡®è¡°å‡åŠæ•£å°„æ ¡æ­£å»ºç«‹äº†æ–°æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09941v1">PDF</a> 25 pages, 10 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ ä¸­çš„ç”Ÿæˆå…ˆéªŒæ‰©æ•£æ¨¡å‹ï¼ˆGPDMï¼‰ï¼Œæå‡ºäº†ä¸€ç§ä»éè¡°å‡å’Œéæ•£å°„æ ¡æ­£ï¼ˆNASCï¼‰PETå›¾åƒç”Ÿæˆé«˜è´¨é‡è¡°å‡å’Œæ•£å°„æ ¡æ­£ï¼ˆASCï¼‰PETå›¾åƒçš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ï¼Œä»ç±»ä¼¼äºç›®æ ‡å›¾åƒåˆ†å¸ƒçš„ç”Ÿæˆå…ˆéªŒå¼€å§‹ï¼Œæœ‰æ•ˆå‡å°‘é‡‡æ ·æ­¥éª¤ï¼Œç”Ÿæˆæ›´ç²¾ç»†çš„ASC PETå›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGPDMåœ¨ç”ŸæˆASC PETå›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ˜¾è‘—çš„é‡‡æ ·æ—¶é—´å‡å°‘ã€‚è¿™ä¸ºè§£å†³ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§å¹¶ç¡®ç«‹PETæˆåƒä¸­é«˜æ•ˆå‡†ç¡®çš„è¡°å‡å’Œæ•£å°„æ ¡æ­£æ–°æ ‡å‡†æä¾›äº†æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å‡†ç¡®è¡°å‡å’Œæ•£å°„æ ¡æ­£åœ¨PETæˆåƒä¸­è‡³å…³é‡è¦ï¼Œå½±å“è§†è§‰è§£é‡Šå’Œå®šé‡åˆ†æã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚ä½¿ç”¨CTæˆ–MRIï¼‰åœ¨å‡†ç¡®æ€§ã€è¾å°„æš´éœ²å’Œé€‚ç”¨æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æ·±åº¦ç¥ç»ç½‘ç»œä¸ºä»éè¡°å‡å’Œéæ•£å°„æ ¡æ­£çš„PETå›¾åƒä¼°è®¡è¡°å‡å’Œæ•£å°„æ ¡æ­£PETå›¾åƒæä¾›äº†æ½œåœ¨æ–¹æ³•ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆé«˜è´¨é‡ASC PETå›¾åƒæ¡†æ¶â€”â€”åŸºäºç”Ÿæˆå…ˆéªŒçš„æ‰©æ•£æ¨¡å‹ï¼ˆGPDMï¼‰ã€‚</li>
<li>GPDMæ¡†æ¶åŸºäºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ï¼Œä»ç±»ä¼¼ç›®æ ‡å›¾åƒåˆ†å¸ƒçš„ç”Ÿæˆå…ˆéªŒå¼€å§‹ï¼Œå‡å°‘é‡‡æ ·æ­¥éª¤å¹¶ç”Ÿæˆæ›´ç²¾ç»†çš„å›¾åƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒGPDMåœ¨ç”ŸæˆASC PETå›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ã€‚</li>
<li>GPDMæœ‰æ½œåŠ›è§£å†³ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶ä¸ºPETæˆåƒä¸­é«˜æ•ˆå‡†ç¡®çš„è¡°å‡å’Œæ•£å°„æ ¡æ­£è®¾å®šæ–°æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09941">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f1497ac36e0b2c01d64b9bd50044c4bb" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Debiased-Dual-Invariant-Defense-for-Adversarially-Robust-Person-Re-Identification"><a href="#Debiased-Dual-Invariant-Defense-for-Adversarially-Robust-Person-Re-Identification" class="headerlink" title="Debiased Dual-Invariant Defense for Adversarially Robust Person Re-Identification"></a>Debiased Dual-Invariant Defense for Adversarially Robust Person Re-Identification</h2><p><strong>Authors:Yuhang Zhou, Yanxiang Zhao, Zhongyun Hua, Zhipu Liu, Zhaoquan Gu, Qing Liao, Leo Yu Zhang</strong></p>
<p>Person re-identification (ReID) is a fundamental task in many real-world applications such as pedestrian trajectory tracking. However, advanced deep learning-based ReID models are highly susceptible to adversarial attacks, where imperceptible perturbations to pedestrian images can cause entirely incorrect predictions, posing significant security threats. Although numerous adversarial defense strategies have been proposed for classification tasks, their extension to metric learning tasks such as person ReID remains relatively unexplored. Moreover, the several existing defenses for person ReID fail to address the inherent unique challenges of adversarially robust ReID. In this paper, we systematically identify the challenges of adversarial defense in person ReID into two key issues: model bias and composite generalization requirements. To address them, we propose a debiased dual-invariant defense framework composed of two main phases. In the data balancing phase, we mitigate model bias using a diffusion-model-based data resampling strategy that promotes fairness and diversity in training data. In the bi-adversarial self-meta defense phase, we introduce a novel metric adversarial training approach incorporating farthest negative extension softening to overcome the robustness degradation caused by the absence of classifier. Additionally, we introduce an adversarially-enhanced self-meta mechanism to achieve dual-generalization for both unseen identities and unseen attack types. Experiments demonstrate that our method significantly outperforms existing state-of-the-art defenses.</p>
<blockquote>
<p>è¡Œäººå†è¯†åˆ«ï¼ˆReIDï¼‰æ˜¯è®¸å¤šç°å®ä¸–ç•Œåº”ç”¨ï¼ˆå¦‚è¡Œäººè½¨è¿¹è·Ÿè¸ªï¼‰ä¸­çš„åŸºæœ¬ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå…ˆè¿›çš„åŸºäºæ·±åº¦å­¦ä¹ çš„ReIDæ¨¡å‹å¾ˆå®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ï¼Œå…¶ä¸­è¡Œäººå›¾åƒçš„å¾®å°æ‰°åŠ¨å¯èƒ½å¯¼è‡´å®Œå…¨é”™è¯¯çš„é¢„æµ‹ï¼Œä»è€Œæ„æˆé‡å¤§å®‰å…¨å¨èƒã€‚è™½ç„¶é’ˆå¯¹åˆ†ç±»ä»»åŠ¡å·²ç»æå‡ºäº†è®¸å¤šå¯¹æŠ—æ€§é˜²å¾¡ç­–ç•¥ï¼Œä½†å®ƒä»¬æ‰©å±•åˆ°åº¦é‡å­¦ä¹ ä»»åŠ¡ï¼ˆå¦‚è¡ŒäººReIDï¼‰ä»ç„¶ç›¸å¯¹æœªè¢«æ¢ç´¢ã€‚è€Œä¸”ï¼Œç°æœ‰çš„é’ˆå¯¹è¡ŒäººReIDçš„å‡ ç§é˜²å¾¡æ–¹æ³•æœªèƒ½è§£å†³å¯¹æŠ—é²æ£’ReIDçš„å›ºæœ‰ç‹¬ç‰¹æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å¯¹æŠ—æ€§é˜²å¾¡åœ¨è¡ŒäººReIDä¸­çš„æŒ‘æˆ˜ç³»ç»Ÿåœ°å½’ä¸ºä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šæ¨¡å‹åè§å’Œå¤åˆæ³›åŒ–è¦æ±‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå»ååŒä¸å˜é˜²å¾¡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç”±ä¸¤ä¸ªé˜¶æ®µç»„æˆã€‚åœ¨æ•°æ®å¹³è¡¡é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºæ‰©æ•£æ¨¡å‹çš„æ•°æ®é‡é‡‡æ ·ç­–ç•¥æ¥ç¼“è§£æ¨¡å‹åè§ï¼Œè¯¥ç­–ç•¥ä¿ƒè¿›è®­ç»ƒæ•°æ®çš„å…¬å¹³æ€§å’Œå¤šæ ·æ€§ã€‚åœ¨åŒå‘å¯¹æŠ—æ€§è‡ªæˆ‘å…ƒé˜²å¾¡é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„åº¦é‡å¯¹æŠ—è®­ç»ƒæ–¹æ³•æ¥å…‹æœç”±äºç¼ºä¹åˆ†ç±»å™¨è€Œå¯¼è‡´çš„é²æ£’æ€§ä¸‹é™é—®é¢˜ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†æœ€è¿œè´Ÿæ‰©å±•è½¯åŒ–æŠ€æœ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†å¯¹æŠ—å¢å¼ºçš„è‡ªæˆ‘å…ƒæœºåˆ¶ï¼Œä»¥å®ç°é’ˆå¯¹æœªè§èº«ä»½å’Œæœªè§æ”»å‡»ç±»å‹çš„åŒé‡æ³›åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„é˜²å¾¡æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09933v1">PDF</a> Accepted by AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è¡Œäººå†è¯†åˆ«ï¼ˆReIDï¼‰ä»»åŠ¡åœ¨é¢ä¸´å¯¹æŠ—æ€§æ”»å‡»æ—¶çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå»ååŒä¸å˜é˜²å¾¡æ¡†æ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬æ•°æ®å¹³è¡¡å’ŒåŒå‘å¯¹æŠ—è‡ªå…ƒé˜²å¾¡ä¸¤ä¸ªé˜¶æ®µã€‚é€šè¿‡æ‰©æ•£æ¨¡å‹è¿›è¡Œæ•°æ®é‡é‡‡æ ·ï¼Œä¿ƒè¿›è®­ç»ƒçš„å…¬å¹³æ€§å’Œå¤šæ ·æ€§ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„åº¦é‡å¯¹æŠ—è®­ç»ƒæ–¹æ³•æ¥å…‹æœé²æ£’æ€§é€€åŒ–é—®é¢˜ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„é˜²å¾¡æ‰‹æ®µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡Œäººå†è¯†åˆ«ï¼ˆReIDï¼‰åœ¨é¢ä¸´å¯¹æŠ—æ€§æ”»å‡»æ—¶å­˜åœ¨æ˜¾è‘—çš„å®‰å…¨å¨èƒã€‚</li>
<li>å½“å‰é’ˆå¯¹ReIDçš„å¯¹æŠ—æ€§é˜²å¾¡ç­–ç•¥é¢ä¸´æ¨¡å‹åå·®å’Œå¤åˆæ³›åŒ–è¦æ±‚ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„å»ååŒä¸å˜é˜²å¾¡æ¡†æ¶åŒ…æ‹¬æ•°æ®å¹³è¡¡å’ŒåŒå‘å¯¹æŠ—è‡ªå…ƒé˜²å¾¡ä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>æ•°æ®å¹³è¡¡é˜¶æ®µä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œæ•°æ®é‡é‡‡æ ·ï¼Œå¢å¼ºè®­ç»ƒçš„å…¬å¹³æ€§å’Œå¤šæ ·æ€§ã€‚</li>
<li>åŒå‘å¯¹æŠ—è‡ªå…ƒé˜²å¾¡é˜¶æ®µå¼•å…¥æ–°çš„åº¦é‡å¯¹æŠ—è®­ç»ƒæ–¹æ³•æ¥å…‹æœé²æ£’æ€§é€€åŒ–é—®é¢˜ã€‚</li>
<li>æå‡ºçš„æœºåˆ¶å®ç°äº†å¯¹æœªè§èº«ä»½å’Œæœªè§æ”»å‡»ç±»å‹çš„åŒé‡æ³›åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09933">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-655f1643249e31293f53c4222a15bfb7" align="middle">
<img src="https://picx.zhimg.com/v2-5d317d779900c0e6ed6b65f6bdc506b8" align="middle">
<img src="https://picx.zhimg.com/v2-588b3f7cb16d2c9fcebed4d1f99409f2" align="middle">
<img src="https://picx.zhimg.com/v2-ed22a87c1df582e975cd3c13ce0e0ab9" align="middle">
<img src="https://picx.zhimg.com/v2-3432838e6a70b4e0536afd9f2414ce4a" align="middle">
<img src="https://picx.zhimg.com/v2-634178881122f7cc094f066755a80f76" align="middle">
<img src="https://picx.zhimg.com/v2-9c9da430b0338f0afef4c8b904b4e4cf" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="FlowCast-Advancing-Precipitation-Nowcasting-with-Conditional-Flow-Matching"><a href="#FlowCast-Advancing-Precipitation-Nowcasting-with-Conditional-Flow-Matching" class="headerlink" title="FlowCast: Advancing Precipitation Nowcasting with Conditional Flow Matching"></a>FlowCast: Advancing Precipitation Nowcasting with Conditional Flow Matching</h2><p><strong>Authors:Bernardo Perrone Ribeiro, Jana Faganeli Pucer</strong></p>
<p>Radar-based precipitation nowcasting, the task of forecasting short-term precipitation fields from previous radar images, is a critical problem for flood risk management and decision-making. While deep learning has substantially advanced this field, two challenges remain fundamental: the uncertainty of atmospheric dynamics and the efficient modeling of high-dimensional data. Diffusion models have shown strong promise by producing sharp, reliable forecasts, but their iterative sampling process is computationally prohibitive for time-critical applications. We introduce FlowCast, the first model to apply Conditional Flow Matching (CFM) to precipitation nowcasting. Unlike diffusion, CFM learns a direct noise-to-data mapping, enabling rapid, high-fidelity sample generation with drastically fewer function evaluations. Our experiments demonstrate that FlowCast establishes a new state-of-the-art in predictive accuracy. A direct comparison further reveals the CFM objective is both more accurate and significantly more efficient than a diffusion objective on the same architecture, maintaining high performance with significantly fewer sampling steps. This work positions CFM as a powerful and practical alternative for high-dimensional spatiotemporal forecasting.</p>
<blockquote>
<p>é›·è¾¾åŸºé™æ°´é¢„æŠ¥ï¼ˆRadar-based precipitation nowcastingï¼‰æ˜¯æŒ‡åˆ©ç”¨å‰æœŸé›·è¾¾å›¾åƒå¯¹çŸ­æœŸé™æ°´åœºè¿›è¡Œé¢„æµ‹çš„ä»»åŠ¡ï¼Œå¯¹äºæ´ªç¾é£é™©ç®¡ç†å’Œå†³ç­–åˆ¶å®šæ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ å·²ç»å¤§å¤§æ¨åŠ¨äº†è¿™ä¸€é¢†åŸŸçš„å‘å±•ï¼Œä½†ä»ç„¶å­˜åœ¨ä¸¤ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼šå¤§æ°”åŠ¨åŠ›å­¦çš„ä¸ç¡®å®šæ€§å’Œé«˜ç»´æ•°æ®çš„æœ‰æ•ˆå»ºæ¨¡ã€‚æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion modelsï¼‰å±•ç°å‡ºå¼ºçƒˆçš„æ½œåŠ›ï¼Œèƒ½å¤Ÿäº§ç”Ÿæ¸…æ™°å¯é çš„é¢„æµ‹ï¼Œä½†å…¶è¿­ä»£é‡‡æ ·è¿‡ç¨‹å¯¹äºæ—¶é—´å…³é”®å‹åº”ç”¨æ¥è¯´è®¡ç®—ä¸Šæ˜¯ä¸åˆç®—çš„ã€‚æˆ‘ä»¬å¼•å…¥äº†FlowCastï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåº”ç”¨äºé™æ°´é¢„æŠ¥çš„æ¡ä»¶æµåŒ¹é…ï¼ˆCFMï¼‰æ¨¡å‹ã€‚ä¸æ‰©æ•£ä¸åŒï¼ŒCFMå­¦ä¹ ç›´æ¥ä»å™ªå£°åˆ°æ•°æ®çš„æ˜ å°„ï¼Œèƒ½å¤Ÿå®ç°å¿«é€Ÿã€é«˜ä¿çœŸåº¦çš„æ ·æœ¬ç”Ÿæˆï¼Œæ‰€éœ€çš„åŠŸèƒ½è¯„ä¼°æ¬¡æ•°å¤§å¤§å‡å°‘ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒFlowCaståœ¨é¢„æµ‹ç²¾åº¦æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚è¿›ä¸€æ­¥çš„ç›´æ¥æ¯”è¾ƒæ˜¾ç¤ºï¼Œåœ¨åŒä¸€æ¶æ„ä¸Šï¼ŒCFMç›®æ ‡æ—¢æ›´å‡†ç¡®ä¹Ÿæ˜¾è‘—æ›´é«˜æ•ˆï¼Œåœ¨å‡å°‘é‡‡æ ·æ­¥éª¤çš„åŒæ—¶ä¿æŒé«˜æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œä½¿CFMæˆä¸ºé«˜ç»´æ—¶ç©ºé¢„æµ‹çš„å¼ºå¤§å®ç”¨æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09731v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>é›·è¾¾é™æ°´é¢„æŠ¥çŸ­æ—¶é™æ°´åœºä»å‰æœŸé›·è¾¾å›¾åƒè¿›è¡Œé¢„æµ‹ï¼Œå¯¹æ´ªæ°´é£é™©ç®¡ç†å’Œå†³ç­–åˆ¶å®šè‡³å…³é‡è¦ã€‚æ·±åº¦å­¦ä¹ å·²å¤§å¤§æ¨åŠ¨è¯¥é¢†åŸŸå‘å±•ï¼Œä½†ä»é¢ä¸´å¤§æ°”åŠ¨åŠ›ä¸ç¡®å®šæ€§å’Œé«˜ç»´æ•°æ®é«˜æ•ˆå»ºæ¨¡ä¸¤å¤§æŒ‘æˆ˜ã€‚æ‰©æ•£æ¨¡å‹è™½å±•ç°å‡ºé”åŒ–å¯é é¢„æµ‹çš„å¼ºå¤§æ½œåŠ›ï¼Œä½†å…¶è¿­ä»£é‡‡æ ·è¿‡ç¨‹å¯¹æ—¶é—´å…³é”®åº”ç”¨è€Œè¨€è®¡ç®—ä¸Šè¿‡äºæ˜‚è´µã€‚æœ¬ç ”ç©¶å¼•å…¥FlowCastæ¨¡å‹ï¼Œé¦–æ¬¡åº”ç”¨æ¡ä»¶æµåŒ¹é…ï¼ˆCFMï¼‰äºé™æ°´é¢„æŠ¥ã€‚ä¸åŒäºæ‰©æ•£æ¨¡å‹ï¼ŒCFMå­¦ä¹ ä»å™ªå£°åˆ°æ•°æ®çš„ç›´æ¥æ˜ å°„ï¼Œèƒ½å¤Ÿå®ç°å¿«é€Ÿã€é«˜ä¿çœŸæ ·æœ¬ç”Ÿæˆï¼Œå¤§å¹…å‡å°‘åŠŸèƒ½è¯„ä¼°æ¬¡æ•°ã€‚å®éªŒè¯æ˜FlowCaståœ¨é¢„æµ‹ç²¾åº¦ä¸Šæ ‘ç«‹æ–°æ ‡æ†ã€‚ä¸æ‰©æ•£ç›®æ ‡ç›´æ¥æ¯”è¾ƒæ˜¾ç¤ºï¼ŒCFMç›®æ ‡åœ¨ç›¸åŒæ¶æ„ä¸Šæ—¢æ›´å‡†ç¡®åˆæ›´æœ‰æ•ˆç‡ï¼Œåœ¨è¾ƒå°‘çš„é‡‡æ ·æ­¥éª¤ä¸­ä¿æŒé«˜æ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œå°†CFMå®šä½ä¸ºé«˜ç»´æ—¶ç©ºé¢„æµ‹çš„å¼ºå¤§å®ç”¨æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é›·è¾¾é™æ°´é¢„æŠ¥åœ¨æ´ªæ°´é£é™©ç®¡ç†å’Œå†³ç­–åˆ¶å®šä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨é›·è¾¾é™æ°´é¢„æŠ¥ä¸­çš„åº”ç”¨ä»é¢ä¸´å¤§æ°”åŠ¨åŠ›ä¸ç¡®å®šæ€§å’Œé«˜ç»´æ•°æ®å»ºæ¨¡æŒ‘æˆ˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹è™½èƒ½ç”Ÿæˆé”åŒ–é¢„æµ‹ï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œä¸é€‚ç”¨äºæ—¶é—´å…³é”®åº”ç”¨ã€‚</li>
<li>FlowCastæ¨¡å‹é¦–æ¬¡å°†æ¡ä»¶æµåŒ¹é…ï¼ˆCFMï¼‰åº”ç”¨äºé™æ°´é¢„æŠ¥ã€‚</li>
<li>CFMé€šè¿‡ç›´æ¥å™ªå£°åˆ°æ•°æ®çš„æ˜ å°„ï¼Œå®ç°å¿«é€Ÿã€é«˜ä¿çœŸæ ·æœ¬ç”Ÿæˆã€‚</li>
<li>å®éªŒè¯æ˜FlowCaståœ¨é¢„æµ‹ç²¾åº¦ä¸Šè¶…è¶Šç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09731">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1a606c861c7299a2eaed1f9be9cbc462" align="middle">
<img src="https://picx.zhimg.com/v2-3471d1511202f2811c30477ec628e043" align="middle">
<img src="https://picx.zhimg.com/v2-0473fb269f5df5d7527388e85a33d0a8" align="middle">
<img src="https://picx.zhimg.com/v2-1f571f8ff9dee0be431150eceb4a9ba0" align="middle">
<img src="https://picx.zhimg.com/v2-675f403ed7c69ad2d74c83b2547cc7f4" align="middle">
<img src="https://picx.zhimg.com/v2-121280023e5cb2d3b5926cd895d7060f" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MMaDA-Parallel-Multimodal-Large-Diffusion-Language-Models-for-Thinking-Aware-Editing-and-Generation"><a href="#MMaDA-Parallel-Multimodal-Large-Diffusion-Language-Models-for-Thinking-Aware-Editing-and-Generation" class="headerlink" title="MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation"></a>MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation</h2><p><strong>Authors:Ye Tian, Ling Yang, Jiongfan Yang, Anran Wang, Yu Tian, Jiani Zheng, Haochen Wang, Zhiyang Teng, Zhuochen Wang, Yinjie Wang, Yunhai Tong, Mengdi Wang, Xiangtai Li</strong></p>
<p>While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at <a target="_blank" rel="noopener" href="https://github.com/tyfeld/MMaDA-Parallel">https://github.com/tyfeld/MMaDA-Parallel</a></p>
<blockquote>
<p>å½“æˆ‘ä»¬è€ƒè™‘è®¤çŸ¥æ„ŸçŸ¥ç”Ÿæˆæ—¨åœ¨æé«˜å¤æ‚ä»»åŠ¡çš„æ€§èƒ½æ—¶ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ç§å…³é”®çš„å¤±è´¥æ¨¡å¼ï¼Œå³ç°æœ‰çš„åºåˆ—å¼ã€è‡ªå›å½’æ–¹æ³•ä¼šç”±äºé”™è¯¯ä¼ æ’­è€Œæ‚–è®ºæ€§åœ°å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†ç³»ç»Ÿåœ°åˆ†æè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ParaBenchï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ–‡æœ¬å’Œå›¾åƒè¾“å‡ºæ¨¡æ€çš„è¯„ä¼°ã€‚æˆ‘ä»¬åˆ©ç”¨ParaBenchè¿›è¡Œçš„åˆ†æè¡¨æ˜ï¼Œè¿™ç§æ€§èƒ½ä¸‹é™ä¸ç”Ÿæˆæ¨ç†å’Œæœ€ç»ˆå›¾åƒä¹‹é—´çš„å¯¹é½ä¸ä½³ä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09611v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://tyfeld.github.io/mmadaparellel.github.io/">https://tyfeld.github.io/mmadaparellel.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æŒ‡å‡ºï¼Œè™½ç„¶æ€è€ƒæ„ŸçŸ¥ç”Ÿæˆæ—¨åœ¨æé«˜å¤æ‚ä»»åŠ¡çš„æ€§èƒ½ï¼Œä½†ç°æœ‰åºè´¯çš„è‡ªå›å½’æ–¹æ³•ç”±äºè¯¯å·®ä¼ é€’ï¼Œä¼šå‡ºç°æ€§èƒ½æ‚–è®ºæ€§ä¸‹é™çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥æ–‡æå‡ºParaBenchåŸºå‡†æµ‹è¯•ï¼Œç”¨ä»¥è¯„ä¼°æ–‡æœ¬å’Œå›¾åƒè¾“å‡ºæ¨¡å¼ã€‚åˆ†ææ˜¾ç¤ºï¼Œæ€§èƒ½ä¸‹é™ä¸ç”Ÿæˆæ¨ç†å’Œæœ€ç»ˆå›¾åƒä¹‹é—´çš„å¯¹é½ä¸è‰¯å¯†åˆ‡ç›¸å…³ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œè¯¥æ–‡æå‡ºå¹¶è¡Œå¤šæ¨¡æ€æ‰©æ•£æ¡†æ¶MMaDA-Parallelï¼Œå®ç°æ–‡æœ¬å’Œå›¾åƒåœ¨æ•´ä¸ªå»å™ªè½¨è¿¹ä¸­çš„æŒç»­åŒå‘äº¤äº’ã€‚é€šè¿‡ç›‘ç£å¾®è°ƒè®­ç»ƒMMaDA-Parallelï¼Œå¹¶é‡‡ç”¨Parallel Reinforcement Learningï¼ˆParaRLï¼‰è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œæ²¿è½¨è¿¹åº”ç”¨è¯­ä¹‰å¥–åŠ±ä»¥åŠ å¼ºè·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚å®éªŒéªŒè¯ï¼Œè¯¥æ¨¡å‹æ˜¾è‘—æé«˜äº†è·¨æ¨¡æ€å¯¹é½å’Œè¯­ä¹‰ä¸€è‡´æ€§ï¼Œåœ¨ParaBenchä¸Šçš„è¾“å‡ºå¯¹é½åº¦è¾ƒBagelæ¨¡å‹æé«˜äº†6.9%ï¼Œä¸ºæ€è€ƒæ„ŸçŸ¥å›¾åƒåˆæˆå»ºç«‹äº†æ›´ç¨³å¥çš„èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰åºè´¯è‡ªå›å½’æ–¹æ³•åœ¨å¤æ‚ä»»åŠ¡ä¸­ä¼šå‡ºç°æ€§èƒ½æ‚–è®ºæ€§ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>ParaBenchåŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°æ–‡æœ¬å’Œå›¾åƒè¾“å‡ºæ¨¡å¼ï¼Œæ­ç¤ºäº†æ€§èƒ½ä¸‹é™ä¸ç”Ÿæˆæ¨ç†å’Œå›¾åƒå¯¹é½ä¹‹é—´çš„å…³è”ã€‚</li>
<li>MMaDA-Parallelæ¡†æ¶å®ç°æ–‡æœ¬å’Œå›¾åƒåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æŒç»­åŒå‘äº¤äº’ã€‚</li>
<li>MMaDA-Parallelé€šè¿‡ç›‘ç£å¾®è°ƒè®­ç»ƒï¼Œå¹¶é‡‡ç”¨Parallel Reinforcement Learningï¼ˆParaRLï¼‰è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚</li>
<li>è¯­ä¹‰å¥–åŠ±æ²¿ç”Ÿæˆè½¨è¿¹åº”ç”¨ï¼Œä»¥åŠ å¼ºè·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMMaDA-Parallelæ¨¡å‹åœ¨è·¨æ¨¡æ€å¯¹é½å’Œè¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09611">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7bbf5398f9ab3ddc1462c21b84fddfdc" align="middle">
<img src="https://picx.zhimg.com/v2-d60d55e15b807652b0a47e5cd811faf4" align="middle">
<img src="https://picx.zhimg.com/v2-84fd4c7d240f3acd55f25cb46fbb39fb" align="middle">
<img src="https://picx.zhimg.com/v2-23b5121eaf775eb73b369eb65d47504e" align="middle">
<img src="https://picx.zhimg.com/v2-f32b92308ede5f0449e0f4317581be18" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Bridging-the-Data-Gap-Spatially-Conditioned-Diffusion-Model-for-Anomaly-Generation-in-Photovoltaic-Electroluminescence-Images"><a href="#Bridging-the-Data-Gap-Spatially-Conditioned-Diffusion-Model-for-Anomaly-Generation-in-Photovoltaic-Electroluminescence-Images" class="headerlink" title="Bridging the Data Gap: Spatially Conditioned Diffusion Model for Anomaly Generation in Photovoltaic Electroluminescence Images"></a>Bridging the Data Gap: Spatially Conditioned Diffusion Model for Anomaly Generation in Photovoltaic Electroluminescence Images</h2><p><strong>Authors:Shiva Hanifi, Sasan Jafarnejad, Marc KÃ¶ntges, Andrej Wentnagel, Andreas Kokkas, Raphael Frank</strong></p>
<p>Reliable anomaly detection in photovoltaic (PV) modules is critical for maintaining solar energy efficiency. However, developing robust computer vision models for PV inspection is constrained by the scarcity of large-scale, diverse, and balanced datasets. This study introduces PV-DDPM, a spatially conditioned denoising diffusion probabilistic model that generates anomalous electroluminescence (EL) images across four PV cell types: multi-crystalline silicon (multi-c-Si), mono-crystalline silicon (mono-c-Si), half-cut multi-c-Si, and interdigitated back contact (IBC) with dogbone interconnect. PV-DDPM enables controlled synthesis of single-defect and multi-defect scenarios by conditioning on binary masks representing structural features and defect positions. To the best of our knowledge, this is the first framework that jointly models multiple PV cell types while supporting simultaneous generation of diverse anomaly types. We also introduce E-SCDD, an enhanced version of the SCDD dataset, comprising 1,000 pixel-wise annotated EL images spanning 30 semantic classes, and 1,768 unlabeled synthetic samples. Quantitative evaluation shows our generated images achieve a FrÃ©chet Inception Distance (FID) of 4.10 and Kernel Inception Distance (KID) of 0.0023 $\pm$ 0.0007 across all categories. Training the visionâ€“language anomaly detection model AA-CLIP on E-SCDD, compared to the SCDD dataset, improves pixel-level AUC and average precision by 1.70 and 8.34 points, respectively.</p>
<blockquote>
<p>åœ¨å…‰ä¼ï¼ˆPVï¼‰æ¨¡å—ä¸­è¿›è¡Œå¯é å¼‚å¸¸æ£€æµ‹å¯¹äºç»´æŒå¤ªé˜³èƒ½æ•ˆç‡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¸ºPVæ£€æµ‹å¼€å‘ç¨³å¥çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹å—åˆ°å¤§è§„æ¨¡ã€å¤šæ ·ä¸”å¹³è¡¡æ•°æ®é›†ç¨€ç¼ºçš„é™åˆ¶ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†PV-DDPMï¼Œè¿™æ˜¯ä¸€ç§ç©ºé—´æ¡ä»¶å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼Œå¯ç”Ÿæˆå››ç§å…‰ä¼ç”µæ± ç±»å‹çš„å¼‚å¸¸ç”µè‡´å‘å…‰ï¼ˆELï¼‰å›¾åƒï¼šå¤šæ™¶ç¡…ï¼ˆmulti-c-Siï¼‰ã€å•æ™¶ç¡…ï¼ˆmono-c-Siï¼‰ã€åŠåˆ‡å‰²å¤šæ™¶ç¡…å’Œå…·æœ‰dogboneäº’è¿çš„äº¤å‰èƒŒæ¥è§¦ï¼ˆIBCï¼‰ã€‚PV-DDPMé€šè¿‡ä»¥ä»£è¡¨ç»“æ„ç‰¹å¾å’Œç¼ºé™·ä½ç½®çš„äºŒè¿›åˆ¶è’™ç‰ˆä¸ºæ¡ä»¶ï¼Œå®ç°äº†å•ç¼ºé™·å’Œå¤šç¼ºé™·åœºæ™¯çš„å—æ§åˆæˆã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ä¸€ä¸ªé¦–æ¬¡è”åˆå»ºæ¨¡å¤šç§å…‰ä¼ç”µæ± ç±»å‹çš„åŒæ—¶æ”¯æŒå¤šç§å¼‚å¸¸ç±»å‹ç”Ÿæˆçš„æ¡†æ¶ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†E-SCDDï¼Œè¿™æ˜¯SCDDæ•°æ®é›†çš„å¢å¼ºç‰ˆï¼ŒåŒ…å«1000å¼ åƒç´ çº§æ³¨é‡Šçš„ELå›¾åƒï¼Œè·¨è¶Š30ä¸ªè¯­ä¹‰ç±»åˆ«ï¼Œä»¥åŠ1768ä¸ªæœªæ ‡è®°çš„åˆæˆæ ·æœ¬ã€‚å®šé‡è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬ç”Ÿæˆçš„å›¾åƒåœ¨æ‰€æœ‰ç±»åˆ«ä¸­çš„FrÃ©chet Inception Distanceï¼ˆFIDï¼‰è¾¾åˆ°4.10ï¼ŒKernel Inception Distanceï¼ˆKIDï¼‰ä¸º0.0023 Â± 0.0007ã€‚åœ¨E-SCDDä¸Šè®­ç»ƒè§†è§‰è¯­è¨€å¼‚å¸¸æ£€æµ‹æ¨¡å‹AA-CLIPä¸SCDDæ•°æ®é›†ç›¸æ¯”ï¼Œåƒç´ çº§AUCå’Œå¹³å‡ç²¾åº¦åˆ†åˆ«æé«˜äº†1.7ç‚¹å’Œ8.34ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09604v1">PDF</a> 8 pages, 4 figures</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶åˆ©ç”¨æ‰©æ•£æ¦‚ç‡æ¨¡å‹PV-DDPMï¼Œé’ˆå¯¹å››ç§å…‰ä¼ç”µæ± ç±»å‹ç”Ÿæˆå¼‚å¸¸ç”µè‡´å‘å…‰å›¾åƒã€‚é€šè¿‡æ¡ä»¶åˆæˆå•ä¸€ç¼ºé™·å’Œå¤šé‡ç¼ºé™·åœºæ™¯ï¼Œå®ç°å¯¹å…‰ä¼æ¨¡å—å¯é å¼‚å¸¸æ£€æµ‹çš„æ•°æ®å¢å¼ºã€‚ç ”ç©¶è¿˜æ¨å‡ºäº†å¢å¼ºç‰ˆSCDDæ•°æ®é›†E-SCDDï¼ŒåŒ…å«åƒç´ çº§æ ‡æ³¨çš„ELå›¾åƒå’Œåˆæˆæ ·æœ¬ã€‚é‡‡ç”¨è¯¥æ•°æ®é›†è®­ç»ƒè§†è§‰-è¯­è¨€å¼‚å¸¸æ£€æµ‹æ¨¡å‹AA-CLIPï¼Œæé«˜äº†å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¼•å…¥äº†PV-DDPMæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆå¤šç§å…‰ä¼ç”µæ± ç±»å‹çš„å¼‚å¸¸ç”µè‡´å‘å…‰å›¾åƒã€‚</li>
<li>PV-DDPMå¯é€šè¿‡æ¡ä»¶åˆæˆæ–¹å¼ï¼Œæ¨¡æ‹Ÿå•ä¸€ç¼ºé™·å’Œå¤šé‡ç¼ºé™·åœºæ™¯ï¼Œå¢å¼ºå…‰ä¼æ¨¡å—å¼‚å¸¸æ£€æµ‹çš„æ•°æ®ã€‚</li>
<li>ç ”ç©¶æ¨å‡ºäº†å¢å¼ºç‰ˆçš„SCDDæ•°æ®é›†E-SCDDï¼ŒåŒ…å«æ›´ä¸°å¯Œçš„åƒç´ çº§æ ‡æ³¨ELå›¾åƒå’Œåˆæˆæ ·æœ¬ã€‚</li>
<li>E-SCDDæ•°æ®é›†çš„æ¨å‡ºä¸ºå¼‚å¸¸æ£€æµ‹æä¾›äº†æ›´å¤šè®­ç»ƒæ•°æ®ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨E-SCDDæ•°æ®é›†è®­ç»ƒçš„AA-CLIPæ¨¡å‹ï¼Œç›¸æ¯”SCDDæ•°æ®é›†ï¼Œåœ¨åƒç´ çº§AUCå’Œå¹³å‡ç²¾åº¦ä¸Šæœ‰æ‰€æå‡ã€‚</li>
<li>æœ¬ç ”ç©¶å®ç°äº†å¯¹å¤šç§å…‰ä¼ç”µæ± ç±»å‹çš„åŒæ—¶å»ºæ¨¡ï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„å¼‚å¸¸ç±»å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6fd3e1f6192460f65d3f076a3b4c6066" align="middle">
<img src="https://picx.zhimg.com/v2-9a0e6ccead739d2e61ed5cb4b27deaa4" align="middle">
<img src="https://picx.zhimg.com/v2-78be9d29ee5d64ce3b0b6b95e241ffc0" align="middle">
<img src="https://picx.zhimg.com/v2-d9a280c1b5334525bbe6bfa39cc5b8de" align="middle">
<img src="https://picx.zhimg.com/v2-2a46a694b798ac3e866a86699545e46b" align="middle">
<img src="https://picx.zhimg.com/v2-9f0507c8f38a0ebaf3f829d6df1c76db" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="WDT-MD-Wavelet-Diffusion-Transformers-for-Microaneurysm-Detection-in-Fundus-Images"><a href="#WDT-MD-Wavelet-Diffusion-Transformers-for-Microaneurysm-Detection-in-Fundus-Images" class="headerlink" title="WDT-MD: Wavelet Diffusion Transformers for Microaneurysm Detection in Fundus Images"></a>WDT-MD: Wavelet Diffusion Transformers for Microaneurysm Detection in Fundus Images</h2><p><strong>Authors:Yifei Sun, Yuzhi He, Junhao Jia, Jinhong Wang, Ruiquan Ge, Changmiao Wang, Hongxia Xu</strong></p>
<p>Microaneurysms (MAs), the earliest pathognomonic signs of Diabetic Retinopathy (DR), present as sub-60 $Î¼m$ lesions in fundus images with highly variable photometric and morphological characteristics, rendering manual screening not only labor-intensive but inherently error-prone. While diffusion-based anomaly detection has emerged as a promising approach for automated MA screening, its clinical application is hindered by three fundamental limitations. First, these models often fall prey to â€œidentity mappingâ€, where they inadvertently replicate the input image. Second, they struggle to distinguish MAs from other anomalies, leading to high false positives. Third, their suboptimal reconstruction of normal features hampers overall performance. To address these challenges, we propose a Wavelet Diffusion Transformer framework for MA Detection (WDT-MD), which features three key innovations: a noise-encoded image conditioning mechanism to avoid â€œidentity mappingâ€ by perturbing image conditions during training; pseudo-normal pattern synthesis via inpainting to introduce pixel-level supervision, enabling discrimination between MAs and other anomalies; and a wavelet diffusion Transformer architecture that combines the global modeling capability of diffusion Transformers with multi-scale wavelet analysis to enhance reconstruction of normal retinal features. Comprehensive experiments on the IDRiD and e-ophtha MA datasets demonstrate that WDT-MD outperforms state-of-the-art methods in both pixel-level and image-level MA detection. This advancement holds significant promise for improving early DR screening.</p>
<blockquote>
<p>å¾®åŠ¨è„‰ç˜¤ï¼ˆMAsï¼‰æ˜¯ç³–å°¿ç—…è§†ç½‘è†œç—…å˜ï¼ˆDRï¼‰çš„æœ€æ—©ç—…ç†ç‰¹å¾æ ‡å¿—ï¼Œåœ¨çœ¼åº•å›¾åƒä¸­è¡¨ç°ä¸ºå°äº60å¾®ç±³çš„ç—…å˜ï¼Œå…·æœ‰å¤šå˜çš„å…‰åº¦å­¦å’Œå½¢æ€å­¦ç‰¹å¾ï¼Œè¿™ä½¿å¾—æ‰‹åŠ¨ç­›æŸ¥ä¸ä»…åŠ³åŠ¨å¼ºåº¦å¤§ï¼Œè€Œä¸”å®¹æ˜“å‡ºé”™ã€‚è™½ç„¶åŸºäºæ‰©æ•£çš„å¼‚å¸¸æ£€æµ‹å·²æˆä¸ºè‡ªåŠ¨åŒ–MAç­›æŸ¥çš„æœ‰å‰é€”çš„æ–¹æ³•ï¼Œä½†å…¶ä¸´åºŠåº”ç”¨å—åˆ°ä¸‰ä¸ªåŸºæœ¬å±€é™çš„é˜»ç¢ã€‚é¦–å…ˆï¼Œè¿™äº›æ¨¡å‹ç»å¸¸é™·å…¥â€œèº«ä»½æ˜ å°„â€ï¼Œå³å®ƒä»¬æ— æ„ä¸­å¤åˆ¶äº†è¾“å…¥å›¾åƒã€‚å…¶æ¬¡ï¼Œå®ƒä»¬éš¾ä»¥åŒºåˆ†MAä¸å…¶ä»–å¼‚å¸¸ï¼Œå¯¼è‡´å‡ºç°å¤§é‡è¯¯æŠ¥ã€‚ç¬¬ä¸‰ï¼Œå®ƒä»¬å¯¹æ­£å¸¸ç‰¹å¾çš„é‡å»ºä¸å¤Ÿç†æƒ³ï¼Œå½±å“äº†æ•´ä½“æ€§èƒ½ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºMAæ£€æµ‹çš„Wavelet Diffusion Transformerï¼ˆWDT-MDï¼‰æ¡†æ¶ï¼Œå®ƒå…·æœ‰ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šå™ªå£°ç¼–ç å›¾åƒè°ƒèŠ‚æœºåˆ¶ï¼Œé€šè¿‡è®­ç»ƒæœŸé—´æ‰°åŠ¨å›¾åƒæ¡ä»¶æ¥é¿å…â€œèº«ä»½æ˜ å°„â€ï¼›é€šè¿‡inpaintingè¿›è¡Œä¼ªæ­£å¸¸æ¨¡å¼åˆæˆï¼Œå¼•å…¥åƒç´ çº§ç›‘ç£ï¼Œèƒ½å¤ŸåŒºåˆ†MAå’Œå…¶ä»–å¼‚å¸¸ï¼›ä»¥åŠå°æ³¢æ‰©æ•£Transformeræ¶æ„ï¼Œå®ƒå°†æ‰©æ•£Transformerçš„å…¨å±€å»ºæ¨¡èƒ½åŠ›ä¸å¤šå°ºåº¦å°æ³¢åˆ†æç›¸ç»“åˆï¼Œä»¥æ”¹å–„æ­£å¸¸è§†ç½‘è†œç‰¹å¾çš„é‡å»ºã€‚åœ¨IDRiDå’Œe-ophtha MAæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒWDT-MDåœ¨åƒç´ çº§å’Œå›¾åƒçº§MAæ£€æµ‹æ–¹é¢éƒ½ä¼˜äºç°æœ‰æœ€æ–°æ–¹æ³•ã€‚è¿™ä¸€è¿›å±•ä¸ºæ”¹å–„æ—©æœŸDRç­›æŸ¥å…·æœ‰é‡å¤§å‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08987v1">PDF</a> 9 pages, 6 figures, 8 tables, accepted by AAAI 2026</p>
<p><strong>æ‘˜è¦</strong><br>    é’ˆå¯¹ç³–å°¿ç—…è§†ç½‘è†œç—…å˜æ—©æœŸç—…ç†æ ‡å¿—â€”â€”å¾®åŠ¨è„‰ç˜¤çš„è‡ªåŠ¨æ£€æµ‹ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå¼‚å¸¸æ£€æµ‹ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œå…¶ä¸´åºŠåº”ç”¨é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šèº«ä»½æ˜ å°„é—®é¢˜ã€éš¾ä»¥åŒºåˆ†å¾®åŠ¨è„‰ç˜¤ä¸å…¶ä»–å¼‚å¸¸ã€ä»¥åŠæ­£å¸¸ç‰¹å¾é‡å»ºä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæå‡ºåŸºäºå°æ³¢æ‰©æ•£Transformerçš„å¾®åŠ¨è„‰ç˜¤æ£€æµ‹æ¡†æ¶ï¼ˆWDT-MDï¼‰ï¼ŒåŒ…æ‹¬å™ªå£°ç¼–ç å›¾åƒè°ƒèŠ‚æœºåˆ¶ã€ä¼ªæ­£å¸¸æ¨¡å¼åˆæˆå’Œç»“åˆæ‰©æ•£Transformerå…¨å±€å»ºæ¨¡å’Œå¤šå°ºåº¦å°æ³¢åˆ†æçš„åˆ›æ–°æ¶æ„ã€‚åœ¨IDRiDå’Œe-ophthaæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒWDT-MDåœ¨åƒç´ çº§å’Œå›¾åƒçº§æ£€æµ‹ä¸­å‡è¡¨ç°ä¼˜å¼‚ï¼Œæœ‰æœ›æ”¹å–„æ—©æœŸç³–å°¿ç—…è§†ç½‘è†œç—…å˜ç­›æŸ¥ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¾®åŠ¨è„‰ç˜¤æ˜¯ç³–å°¿ç—…è§†ç½‘è†œç—…å˜çš„æ—©æœŸç—…ç†æ ‡å¿—ï¼Œå…¶æ‰‹åŠ¨ç­›æŸ¥æ—¢è€—æ—¶åˆæ˜“å‡ºé”™ï¼Œå› æ­¤è‡ªåŠ¨æ£€æµ‹è‡³å…³é‡è¦ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå¼‚å¸¸æ£€æµ‹ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†é¢ä¸´èº«ä»½æ˜ å°„ã€å¼‚å¸¸åŒºåˆ†å’Œæ­£å¸¸ç‰¹å¾é‡å»ºç­‰ä¸‰å¤§æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„WDT-MDæ¡†æ¶é€šè¿‡å™ªå£°ç¼–ç å›¾åƒè°ƒèŠ‚æœºåˆ¶é¿å…èº«ä»½æ˜ å°„ã€‚</li>
<li>ä¼ªæ­£å¸¸æ¨¡å¼åˆæˆæŠ€æœ¯å¼•å…¥åƒç´ çº§ç›‘ç£ï¼Œæé«˜å¾®åŠ¨è„‰ç˜¤ä¸å…¶ä»–å¼‚å¸¸çš„è¾¨åˆ«èƒ½åŠ›ã€‚</li>
<li>ç»“åˆæ‰©æ•£Transformerå…¨å±€å»ºæ¨¡å’Œå¤šå°ºåº¦å°æ³¢åˆ†æï¼Œä¼˜åŒ–æ­£å¸¸è§†ç½‘è†œç‰¹å¾çš„é‡å»ºã€‚</li>
<li>åœ¨IDRiDå’Œe-ophthaæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†WDT-MDåœ¨å¾®åŠ¨è„‰ç˜¤æ£€æµ‹ä¸­çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>WDT-MDæ¡†æ¶çš„æå‡ºæœ‰æœ›æ”¹å–„ç³–å°¿ç—…è§†ç½‘è†œç—…å˜çš„æ—©æœŸç­›æŸ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08987">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ccf695f7962fd25488eac774cc55c5b9" align="middle">
<img src="https://picx.zhimg.com/v2-57def212bbd55ab179c65cf5688dc32e" align="middle">
<img src="https://picx.zhimg.com/v2-10561b351c55b3e881555b68d09613c6" align="middle">
<img src="https://picx.zhimg.com/v2-b0f94b3197a6a1dd4ee67b3bf6b6af72" align="middle">
<img src="https://picx.zhimg.com/v2-60fb30738f653ba2fd0529c97e87def5" align="middle">
<img src="https://picx.zhimg.com/v2-42596d8f3f5143b65aa38a1a033f6e9a" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Improving-Conditional-VAE-with-approximation-using-Normalizing-Flows"><a href="#Improving-Conditional-VAE-with-approximation-using-Normalizing-Flows" class="headerlink" title="Improving Conditional VAE with approximation using Normalizing Flows"></a>Improving Conditional VAE with approximation using Normalizing Flows</h2><p><strong>Authors:Tuhin Subhra De</strong></p>
<p>Variational Autoencoders and Generative Adversarial Networks remained the state-of-the-art (SOTA) generative models until 2022. Now they are superseded by diffusion based models. Efforts to improve traditional models have stagnated as a result. In old-school fashion, we explore image generation with conditional Variational Autoencoders (CVAE) to incorporate desired attributes within the images. VAEs are known to produce blurry images with less diversity, we refer a method that solve this issue by leveraging the variance of the gaussian decoder as a learnable parameter during training. Previous works on CVAEs assumed that the conditional distribution of the latent space given the labels is equal to the prior distribution, which is not the case in reality. We show that estimating it using normalizing flows results in better image generation than existing methods by reducing the FID by 5% and increasing log likelihood by 7.7% than the previous case.</p>
<blockquote>
<p>å˜åˆ†è‡ªç¼–ç å™¨å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œåœ¨2022å¹´ä¹‹å‰ä¸€ç›´æ˜¯å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹ã€‚å¦‚ä»Šå®ƒä»¬å·²è¢«åŸºäºæ‰©æ•£çš„æ¨¡å‹æ‰€å–ä»£ã€‚å› æ­¤ï¼Œæ”¹è¿›ä¼ ç»Ÿæ¨¡å‹çš„åŠªåŠ›é™·å…¥äº†åœæ»ã€‚æˆ‘ä»¬ä»¥ä¼ ç»Ÿçš„æ–¹å¼æ¢ç´¢äº†ä½¿ç”¨æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆCVAEï¼‰è¿›è¡Œå›¾åƒç”Ÿæˆï¼Œä»¥åœ¨å›¾åƒä¸­èå…¥æ‰€éœ€çš„å±æ€§ã€‚å·²çŸ¥VAEä¼šäº§ç”Ÿæ¨¡ç³Šä¸”å¤šæ ·æ€§è¾ƒå°‘çš„å›¾åƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨é«˜æ–¯è§£ç å™¨çš„æ–¹å·®ä½œä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„å¯å­¦ä¹ å‚æ•°æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å…³äºCVAEçš„å…ˆå‰å·¥ä½œå‡è®¾ç»™å®šæ ‡ç­¾çš„æ½œåœ¨ç©ºé—´æ¡ä»¶åˆ†å¸ƒç­‰äºå…ˆéªŒåˆ†å¸ƒï¼Œä½†ç°å®ä¸­å¹¶éå¦‚æ­¤ã€‚æˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡ä½¿ç”¨è§„èŒƒåŒ–æµå¯¹å…¶è¿›è¡Œä¼°ç®—ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å¾—æ›´å¥½ï¼Œå°†FIDé™ä½äº†5%ï¼Œå¯¹æ•°ä¼¼ç„¶å¢åŠ äº†7.7%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08946v2">PDF</a> Independent Work</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹å·²ç»åœ¨ç”Ÿæˆæ¨¡å‹é¢†åŸŸå–ä»£äº†å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ï¼Œæˆä¸ºæ–°çš„ä¸»æµæŠ€æœ¯ã€‚æ–‡ç« æ¢ç´¢äº†ä½¿ç”¨æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆCVAEï¼‰è¿›è¡Œå›¾åƒç”Ÿæˆçš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨é«˜æ–¯è§£ç å™¨çš„æ–¹å·®ä½œä¸ºè®­ç»ƒæ—¶çš„å¯å­¦ä¹ å‚æ•°æ¥è§£å†³ç”Ÿæˆå›¾åƒæ¨¡ç³Šå’Œå¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ã€‚åŒæ—¶æŒ‡å‡ºå…ˆå‰çš„CVAEå·¥ä½œä¸­å…³äºæ ‡ç­¾æ½œåœ¨ç©ºé—´æ¡ä»¶åˆ†å¸ƒç­‰äºå…ˆéªŒåˆ†å¸ƒçš„å‡è®¾å­˜åœ¨é—®é¢˜ï¼Œå¹¶æå‡ºäº†é€šè¿‡å½’ä¸€åŒ–æµæ¥ä¼°ç®—è¿™ä¸€åˆ†å¸ƒï¼Œå¯ä»¥æé«˜å›¾åƒç”Ÿæˆçš„è´¨é‡ã€‚é€šè¿‡è¯¥æ–¹æ³•çš„åº”ç”¨ï¼Œç›¸æ¯”åŸæœ‰æ¨¡å‹å¯ä»¥å‡å°‘FIDåˆ†æ•°ï¼Œå¹¶å¢åŠ å¯¹æ•°ä¼¼åº¦ã€‚è¿™ä¹Ÿæ ‡å¿—ç€æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„ä¼˜åŠ¿è¿›ä¸€æ­¥å¾—åˆ°è¯å®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºç”Ÿæˆæ¨¡å‹é¢†åŸŸçš„æœ€æ–°ä¸»æµæŠ€æœ¯ï¼Œå–ä»£äº†å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€‚</li>
<li>æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆCVAEï¼‰è¢«ç”¨äºå›¾åƒç”Ÿæˆï¼Œä»¥è§£å†³ç”Ÿæˆå›¾åƒæ¨¡ç³Šå’Œå¤šæ ·æ€§ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨é«˜æ–¯è§£ç å™¨çš„æ–¹å·®ä½œä¸ºè®­ç»ƒæ—¶çš„å¯å­¦ä¹ å‚æ•°ï¼Œæ”¹è¿›äº†CVAEçš„æ€§èƒ½ã€‚</li>
<li>ä¹‹å‰çš„CVAEå·¥ä½œå‡è®¾æ ‡ç­¾æ½œåœ¨ç©ºé—´çš„æ¡ä»¶åˆ†å¸ƒç­‰äºå…ˆéªŒåˆ†å¸ƒï¼Œä½†å®é™…ä¸Šå¹¶éå¦‚æ­¤ã€‚</li>
<li>é€šè¿‡å½’ä¸€åŒ–æµä¼°ç®—æ¡ä»¶åˆ†å¸ƒå¯ä»¥æé«˜å›¾åƒç”Ÿæˆçš„è´¨é‡ã€‚ç›¸è¾ƒäºä¹‹å‰çš„æ¨¡å‹ï¼Œè¿™ä¸€æ”¹è¿›æœ‰åŠ©äºå‡å°‘FIDåˆ†æ•°å¹¶å¢åŠ å¯¹æ•°ä¼¼åº¦ã€‚è¿™è¡¨æ˜æ–°æ–¹æ³•å…·æœ‰æ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>æ–‡ç« å±•ç¤ºäº†æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸçš„ä¼˜åŠ¿ï¼Œå¹¶éªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08946">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-88f487115f90e3bdd99ac9db8af69664" align="middle">
<img src="https://picx.zhimg.com/v2-96dd16043b72abc4734bafc92d7f9826" align="middle">
<img src="https://picx.zhimg.com/v2-cda0b87a088e6f772de1c62229c7bce2" align="middle">
<img src="https://picx.zhimg.com/v2-6b339cb59c04f681cf44e193751c17ce" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="From-Structure-to-Detail-Hierarchical-Distillation-for-Efficient-Diffusion-Model"><a href="#From-Structure-to-Detail-Hierarchical-Distillation-for-Efficient-Diffusion-Model" class="headerlink" title="From Structure to Detail: Hierarchical Distillation for Efficient Diffusion Model"></a>From Structure to Detail: Hierarchical Distillation for Efficient Diffusion Model</h2><p><strong>Authors:Hanbo Cheng, Peng Wang, Kaixiang Lei, Qi Li, Zhen Zou, Pengfei Hu, Jun Du</strong></p>
<p>The inference latency of diffusion models remains a critical barrier to their real-time application. While trajectory-based and distribution-based step distillation methods offer solutions, they present a fundamental trade-off. Trajectory-based methods preserve global structure but act as a â€œlossy compressorâ€, sacrificing high-frequency details. Conversely, distribution-based methods can achieve higher fidelity but often suffer from mode collapse and unstable training. This paper recasts them from independent paradigms into synergistic components within our novel Hierarchical Distillation (HD) framework. We leverage trajectory distillation not as a final generator, but to establish a structural &#96;&#96;sketchâ€, providing a near-optimal initialization for the subsequent distribution-based refinement stage. This strategy yields an ideal initial distribution that enhances the ceiling of overall performance. To further improve quality, we introduce and refine the adversarial training process. We find standard discriminator structures are ineffective at refining an already high-quality generator. To overcome this, we introduce the Adaptive Weighted Discriminator (AWD), tailored for the HD pipeline. By dynamically allocating token weights, AWD focuses on local imperfections, enabling efficient detail refinement. Our approach demonstrates state-of-the-art performance across diverse tasks. On ImageNet $256\times256$, our single-step model achieves an FID of 2.26, rivaling its 250-step teacher. It also achieves promising results on the high-resolution text-to-image MJHQ benchmark, proving its generalizability. Our method establishes a robust new paradigm for high-fidelity, single-step diffusion models.</p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„æ¨ç†å»¶è¿Ÿä»ç„¶æ˜¯å…¶å®æ—¶åº”ç”¨çš„å…³é”®éšœç¢ã€‚è™½ç„¶åŸºäºè½¨è¿¹å’ŒåŸºäºåˆ†å¸ƒçš„æ­¥éª¤è’¸é¦æ–¹æ³•æä¾›äº†è§£å†³æ–¹æ¡ˆï¼Œä½†å®ƒä»¬ä¹‹é—´å­˜åœ¨åŸºæœ¬æƒè¡¡ã€‚åŸºäºè½¨è¿¹çš„æ–¹æ³•ä¿ç•™äº†å…¨å±€ç»“æ„ï¼Œä½†å……å½“â€œæœ‰æŸå‹ç¼©å™¨â€ï¼Œç‰ºç‰²äº†é«˜é¢‘ç»†èŠ‚ã€‚ç›¸åï¼ŒåŸºäºåˆ†å¸ƒçš„æ–¹æ³•å¯ä»¥è¾¾åˆ°è¾ƒé«˜ä¿çœŸåº¦ï¼Œä½†ç»å¸¸é­å—æ¨¡å¼å´©æºƒå’Œè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚æœ¬æ–‡å°†å…¶ä»ç‹¬ç«‹èŒƒå¼é‡æ–°æ„å»ºä¸ºæˆ‘ä»¬æ–°å‹å±‚æ¬¡è’¸é¦ï¼ˆHDï¼‰æ¡†æ¶å†…çš„ååŒç»„ä»¶ã€‚æˆ‘ä»¬åˆ©ç”¨è½¨è¿¹è’¸é¦æ³•ä¸æ˜¯ä½œä¸ºæœ€ç»ˆç”Ÿæˆå™¨ï¼Œè€Œæ˜¯å»ºç«‹ä¸€ä¸ªç»“æ„æ€§â€œè‰å›¾â€ï¼Œä¸ºåç»­çš„åŸºäºåˆ†å¸ƒçš„ç»†åŒ–é˜¶æ®µæä¾›æ¥è¿‘æœ€ä¼˜çš„åˆå§‹åŒ–ã€‚è¿™ç§ç­–ç•¥äº§ç”Ÿäº†ç†æƒ³çš„åˆå§‹åˆ†å¸ƒï¼Œæé«˜äº†æ•´ä½“æ€§èƒ½çš„ä¸Šé™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08930v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹çš„æ¨ç†å»¶è¿Ÿæ˜¯å…¶å®æ—¶åº”ç”¨çš„å…³é”®éšœç¢ã€‚è™½ç„¶è½¨è¿¹åŸºç¡€å’Œåˆ†å¸ƒåŸºç¡€çš„è’¸é¦æ–¹æ³•æä¾›äº†è§£å†³æ–¹æ¡ˆï¼Œä½†å®ƒä»¬ä¹‹é—´å­˜åœ¨åŸºæœ¬æƒè¡¡ã€‚è½¨è¿¹åŸºç¡€æ–¹æ³•ä¿ç•™å…¨å±€ç»“æ„ï¼Œä½†ç‰ºç‰²é«˜é¢‘ç»†èŠ‚ï¼Œä½œä¸ºâ€œæœ‰æŸå‹ç¼©å™¨â€ã€‚ç›¸åï¼Œåˆ†å¸ƒåŸºç¡€æ–¹æ³•å¯ä»¥è¾¾åˆ°é«˜ä¿çœŸï¼Œä½†ç»å¸¸é­å—æ¨¡å¼å´©æºƒå’Œè®­ç»ƒä¸ç¨³å®šã€‚æœ¬æ–‡å°†å…¶ä»ç‹¬ç«‹èŒƒå¼è½¬å˜ä¸ºæˆ‘ä»¬æ–°é¢–å±‚æ¬¡è’¸é¦ï¼ˆHDï¼‰æ¡†æ¶å†…çš„ååŒç»„ä»¶ã€‚æˆ‘ä»¬åˆ©ç”¨è½¨è¿¹è’¸é¦å¹¶éä½œä¸ºæœ€ç»ˆç”Ÿæˆå™¨ï¼Œè€Œæ˜¯å»ºç«‹ç»“æ„æ€§â€œè‰å›¾â€ï¼Œä¸ºéšåçš„åˆ†å¸ƒåŸºç¡€ç»†åŒ–é˜¶æ®µæä¾›æ¥è¿‘æœ€ä¼˜çš„åˆå§‹åŒ–ã€‚æ­¤ç­–ç•¥äº§ç”Ÿç†æƒ³çš„åˆå§‹åˆ†å¸ƒï¼Œæé«˜äº†æ•´ä½“æ€§èƒ½çš„ä¸Šé™ã€‚è¿›ä¸€æ­¥æé«˜è´¨é‡ï¼Œæˆ‘ä»¬å¯¹å¯¹æŠ—è®­ç»ƒè¿‡ç¨‹è¿›è¡Œäº†ä»‹ç»å’Œç»†åŒ–ã€‚æˆ‘ä»¬å‘ç°æ ‡å‡†é‰´åˆ«å™¨ç»“æ„åœ¨ç»†åŒ–å·²é«˜è´¨é‡ç”Ÿæˆå™¨æ—¶æ— æ•ˆã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªé€‚åº”åŠ æƒé‰´åˆ«å™¨ï¼ˆAWDï¼‰ï¼Œé€‚ç”¨äºHDç®¡é“ã€‚é€šè¿‡åŠ¨æ€åˆ†é…ä»¤ç‰Œæƒé‡ï¼ŒAWDä¸“æ³¨äºå±€éƒ¨ç¼ºé™·ï¼Œå®ç°æœ‰æ•ˆçš„ç»†èŠ‚ç»†åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§ä»»åŠ¡ä¸Šå±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½ã€‚åœ¨ImageNet 256x256ä¸Šï¼Œæˆ‘ä»¬çš„å•æ­¥æ¨¡å‹å®ç°äº†FIDä¸º2.26ï¼Œä¸å®ƒçš„250æ­¥æ•™å¸ˆç›¸æŠ—è¡¡ã€‚åœ¨é«˜åˆ†è¾¨ç‡æ–‡æœ¬åˆ°å›¾åƒçš„MJHQåŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒä¹Ÿå–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœï¼Œè¯æ˜äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºé«˜ä¿çœŸå•æ­¥æ‰©æ•£æ¨¡å‹å»ºç«‹äº†ç¨³å¥çš„æ–°èŒƒå¼ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹çš„æ¨ç†å»¶è¿Ÿé™åˆ¶äº†å…¶å®æ—¶åº”ç”¨ã€‚</li>
<li>ç°æœ‰è§£å†³æ–¹æ³•çš„æƒè¡¡ï¼šè½¨è¿¹åŸºç¡€æ–¹æ³•ä¿ç•™å…¨å±€ç»“æ„ä½†ç‰ºç‰²é«˜é¢‘ç»†èŠ‚ï¼Œåˆ†å¸ƒåŸºç¡€æ–¹æ³•è¿½æ±‚é«˜ä¿çœŸä½†é¢ä¸´æ¨¡å¼å´©æºƒå’Œè®­ç»ƒä¸ç¨³å®šã€‚</li>
<li>å¼•å…¥å±‚æ¬¡è’¸é¦ï¼ˆHDï¼‰æ¡†æ¶ï¼Œå°†è½¨è¿¹åŸºç¡€å’Œåˆ†å¸ƒåŸºç¡€æ–¹æ³•ç»“åˆï¼Œåˆ©ç”¨è½¨è¿¹è’¸é¦å»ºç«‹ç»“æ„æ€§â€œè‰å›¾â€ï¼Œä¸ºåˆ†å¸ƒåŸºç¡€ç»†åŒ–æä¾›ä¼˜åŒ–åˆå§‹åŒ–ã€‚</li>
<li>æ”¹è¿›å¯¹æŠ—è®­ç»ƒè¿‡ç¨‹ï¼Œå¼•å…¥è‡ªé€‚åº”åŠ æƒé‰´åˆ«å™¨ï¼ˆAWDï¼‰ï¼Œä»¥å…³æ³¨å±€éƒ¨ç¼ºé™·å¹¶ç»†åŒ–ç»†èŠ‚ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼ŒåŒ…æ‹¬ImageNetå’ŒMJHQåŸºå‡†æµ‹è¯•ã€‚</li>
<li>å•æ­¥æ¨¡å‹å®ç°äº†é«˜ä¿çœŸå’Œç«äº‰åŠ›æ€§èƒ½ï¼Œä¸å¤šæ­¥æ¨¡å‹ç›¸æŠ—è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4c6c323933d31b1e8ef48c099410c76e" align="middle">
<img src="https://picx.zhimg.com/v2-964e8d8c01c86395dc412f1302209a9f" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="LayerPeeler-Autoregressive-Peeling-for-Layer-wise-Image-Vectorization"><a href="#LayerPeeler-Autoregressive-Peeling-for-Layer-wise-Image-Vectorization" class="headerlink" title="LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization"></a>LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization</h2><p><strong>Authors:Ronghuan Wu, Wanchao Su, Jing Liao</strong></p>
<p>Image vectorization is a powerful technique that converts raster images into vector graphics, enabling enhanced flexibility and interactivity. However, popular image vectorization tools struggle with occluded regions, producing incomplete or fragmented shapes that hinder editability. While recent advancements have explored optimization-based and learning-based layer-wise image vectorization, these methods face limitations in vectorization quality and flexibility. In this paper, we introduce LayerPeeler, a novel layer-wise image vectorization approach that addresses these challenges through a progressive simplification paradigm. The key to LayerPeelerâ€™s success lies in its autoregressive peeling strategy: by identifying and removing the topmost non-occluded layers while recovering underlying content, we generate vector graphics with complete paths and coherent layer structures. Our method leverages vision-language models to construct a layer graph that captures occlusion relationships among elements, enabling precise detection and description for non-occluded layers. These descriptive captions are used as editing instructions for a finetuned image diffusion model to remove the identified layers. To ensure accurate removal, we employ localized attention control that precisely guides the model to target regions while faithfully preserving the surrounding content. To support this, we contribute a large-scale dataset specifically designed for layer peeling tasks. Extensive quantitative and qualitative experiments demonstrate that LayerPeeler significantly outperforms existing techniques, producing vectorization results with superior path semantics, geometric regularity, and visual fidelity.</p>
<blockquote>
<p>å›¾åƒçŸ¢é‡åŒ–æ˜¯ä¸€ç§å¼ºå¤§çš„æŠ€æœ¯ï¼Œèƒ½å°†ä½å›¾å›¾åƒè½¬æ¢ä¸ºçŸ¢é‡å›¾å½¢ï¼Œæé«˜äº†çµæ´»æ€§å’Œäº¤äº’æ€§ã€‚ç„¶è€Œï¼Œæµè¡Œçš„å›¾åƒçŸ¢é‡åŒ–å·¥å…·åœ¨å¤„ç†é®æŒ¡åŒºåŸŸæ—¶é‡åˆ°å›°éš¾ï¼Œäº§ç”Ÿä¸å®Œæ•´æˆ–æ–­è£‚çš„å½¢çŠ¶ï¼Œå¦¨ç¢äº†ç¼–è¾‘èƒ½åŠ›ã€‚è™½ç„¶æœ€è¿‘çš„è¿›å±•æ¢ç´¢äº†åŸºäºä¼˜åŒ–å’ŒåŸºäºå­¦ä¹ çš„åˆ†å±‚å›¾åƒçŸ¢é‡åŒ–ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨çŸ¢é‡åŒ–è´¨é‡å’Œçµæ´»æ€§æ–¹é¢ä»é¢ä¸´å±€é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LayerPeelerï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åˆ†å±‚å›¾åƒçŸ¢é‡åŒ–æ–¹æ³•ï¼Œé€šè¿‡æ¸è¿›ç®€åŒ–èŒƒå¼æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚LayerPeeleræˆåŠŸçš„å…³é”®åœ¨äºå…¶è‡ªå›å½’å‰¥ç¦»ç­–ç•¥ï¼šé€šè¿‡è¯†åˆ«å’Œç§»é™¤æœ€é¡¶éƒ¨çš„éé®æŒ¡å±‚å¹¶æ¢å¤åº•å±‚å†…å®¹ï¼Œç”Ÿæˆå…·æœ‰å®Œæ•´è·¯å¾„å’Œè¿è´¯å±‚ç»“æ„çš„çŸ¢é‡å›¾å½¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æ„å»ºå±‚å›¾ï¼Œæ•æ‰å…ƒç´ ä¹‹é—´çš„é®æŒ¡å…³ç³»ï¼Œä¸ºéé®æŒ¡å±‚çš„ç²¾ç¡®æ£€æµ‹å’Œæè¿°æä¾›æ”¯æŒã€‚è¿™äº›æè¿°æ€§æ ‡é¢˜ç”¨ä½œå¾®è°ƒå›¾åƒæ‰©æ•£æ¨¡å‹çš„ç¼–è¾‘æŒ‡ä»¤ï¼Œä»¥ç§»é™¤å·²è¯†åˆ«çš„å±‚ã€‚ä¸ºç¡®ä¿å‡†ç¡®çš„ç§»é™¤ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å±€éƒ¨æ³¨æ„åŠ›æ§åˆ¶ï¼Œç²¾ç¡®å¼•å¯¼æ¨¡å‹å®šä½åŒºåŸŸï¼ŒåŒæ—¶å¿ å®ä¿ç•™å‘¨å›´å†…å®¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è´¡çŒ®äº†ä¸€ä¸ªä¸“é—¨ä¸ºå›¾å±‚å‰¥ç¦»ä»»åŠ¡è®¾è®¡çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚å¤§é‡çš„å®šé‡å’Œå®šæ€§å®éªŒè¡¨æ˜ï¼ŒLayerPeeleråœ¨è·¯å¾„è¯­ä¹‰ã€å‡ ä½•è§„åˆ™æ€§å’Œè§†è§‰ä¿çœŸåº¦æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œäº§ç”Ÿäº†å‡ºè‰²çš„çŸ¢é‡åŒ–ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.23740v3">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://layerpeeler.github.io/">https://layerpeeler.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºLayerPeelerçš„æ–°å›¾åƒçŸ¢é‡åŒ–æ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ†å±‚å‰¥ç¦»ç­–ç•¥è§£å†³äº†å›¾åƒçŸ¢é‡åŒ–ä¸­çš„é®æŒ¡åŒºåŸŸé—®é¢˜ã€‚è¯¥æ–¹æ³•é‡‡ç”¨æ¸è¿›ç®€åŒ–èŒƒå¼ï¼Œé€šè¿‡è¯†åˆ«å¹¶ç§»é™¤éé®æŒ¡å±‚æ¥ç”ŸæˆçŸ¢é‡å›¾å½¢ï¼Œå¹¶åœ¨æ­¤è¿‡ç¨‹ä¸­æ¢å¤åº•å±‚å†…å®¹ã€‚LayerPeelerä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æ„å»ºå±‚å›¾ï¼Œæ•è·å…ƒç´ ä¹‹é—´çš„é®æŒ¡å…³ç³»ï¼Œä»è€Œå®ç°ç²¾å‡†æ£€æµ‹å’Œæè¿°éé®æŒ¡å±‚ã€‚è¯¥æ–¹æ³•çš„æè¿°æ€§æ ‡é¢˜ç”¨ä½œè®­ç»ƒè¿‡çš„å›¾åƒæ‰©æ•£æ¨¡å‹çš„ç¼–è¾‘æŒ‡ä»¤æ¥ç§»é™¤è¯†åˆ«çš„å±‚ã€‚LayerPeelerçš„å‡ºè‰²è¡¨ç°å¾—åˆ°äº†å¹¿æ³›çš„å®šé‡å’Œå®šæ€§å®éªŒéªŒè¯ï¼Œå…¶ç”Ÿæˆçš„çŸ¢é‡å›¾å½¢å…·æœ‰å‡ºè‰²çš„è·¯å¾„è¯­ä¹‰ã€å‡ ä½•è§„åˆ™å’Œè§†è§‰ä¿çœŸåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LayerPeeleræ˜¯ä¸€ç§æ–°çš„å›¾åƒçŸ¢é‡åŒ–æ–¹æ³•ï¼Œé€šè¿‡åˆ†å±‚å‰¥ç¦»ç­–ç•¥è§£å†³äº†é®æŒ¡åŒºåŸŸé—®é¢˜ã€‚</li>
<li>LayerPeeleré‡‡ç”¨æ¸è¿›ç®€åŒ–èŒƒå¼ï¼Œç”Ÿæˆå…·æœ‰å®Œæ•´è·¯å¾„å’Œè¿è´¯å±‚ç»“æ„çš„çŸ¢é‡å›¾å½¢ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹æ„å»ºå±‚å›¾ï¼Œä»¥æ•è·å…ƒç´ é—´çš„é®æŒ¡å…³ç³»ã€‚</li>
<li>LayerPeelerçš„æè¿°æ€§æ ‡é¢˜ç”¨ä½œå›¾åƒæ‰©æ•£æ¨¡å‹çš„ç¼–è¾‘æŒ‡ä»¤æ¥ç§»é™¤è¯†åˆ«çš„å±‚ã€‚</li>
<li>LayerPeelerèƒ½ç²¾å‡†æ£€æµ‹å¹¶æè¿°éé®æŒ¡å±‚ï¼Œç¡®ä¿å‡†ç¡®ç§»é™¤ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨å±€éƒ¨æ³¨æ„åŠ›æ§åˆ¶ï¼Œç²¾ç¡®æŒ‡å¯¼æ¨¡å‹é’ˆå¯¹ç›®æ ‡åŒºåŸŸï¼ŒåŒæ—¶å¿ å®ä¿ç•™å‘¨å›´å†…å®¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.23740">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-883389f540fff8e1ee7513cffa0b1490" align="middle">
<img src="https://picx.zhimg.com/v2-b71eb744e93de36c7bc44aaffa535f7e" align="middle">
<img src="https://picx.zhimg.com/v2-56163c63288631f4ba8c575179cc2105" align="middle">
<img src="https://picx.zhimg.com/v2-8b70b2b457064fe55664a6e3e6f20c1f" align="middle">
<img src="https://picx.zhimg.com/v2-808af45a27682d14745152029007a7a4" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Towards-Consistent-and-Efficient-Dataset-Distillation-via-Diffusion-Driven-Selection"><a href="#Towards-Consistent-and-Efficient-Dataset-Distillation-via-Diffusion-Driven-Selection" class="headerlink" title="Towards Consistent and Efficient Dataset Distillation via Diffusion-Driven Selection"></a>Towards Consistent and Efficient Dataset Distillation via Diffusion-Driven Selection</h2><p><strong>Authors:Xinhao Zhong, Shuoyang Sun, Xulin Gu, Zhaoyang Xu, Yaowei Wang, Min Zhang, Bin Chen</strong></p>
<p>Dataset distillation provides an effective approach to reduce memory and computational costs by optimizing a compact dataset that achieves performance comparable to the full original. However, for large-scale datasets and complex deep networks (e.g., ImageNet-1K with ResNet-101), the vast optimization space hinders distillation effectiveness, limiting practical applications. Recent methods leverage pre-trained diffusion models to directly generate informative images, thereby bypassing pixel-level optimization and achieving promising results. Nonetheless, these approaches often suffer from distribution shifts between the pre-trained diffusion prior and target datasets, as well as the need for multiple distillation steps under varying settings. To overcome these challenges, we propose a novel framework that is orthogonal to existing diffusion-based distillation techniques by utilizing the diffusion prior for patch selection rather than generation. Our method predicts noise from the diffusion model conditioned on input images and optional text prompts (with or without label information), and computes the associated loss for each image-patch pair. Based on the loss differences, we identify distinctive regions within the original images. Furthermore, we apply intra-class clustering and ranking on the selected patches to enforce diversity constraints. This streamlined pipeline enables a one-step distillation process. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art methods across various metrics and settings.</p>
<blockquote>
<p>æ•°æ®é›†è’¸é¦æä¾›äº†ä¸€ç§é€šè¿‡ä¼˜åŒ–ç´§å‡‘æ•°æ®é›†æ¥å‡å°‘å†…å­˜å’Œè®¡ç®—æˆæœ¬çš„æœ‰æ•ˆæ–¹æ³•ï¼Œè¯¥æ•°æ®é›†çš„æ€§èƒ½å¯ä¸åŸå§‹å®Œæ•´æ•°æ®é›†ç›¸å½“ã€‚ç„¶è€Œï¼Œå¯¹äºå¤§è§„æ¨¡æ•°æ®é›†å’Œå¤æ‚æ·±åº¦ç½‘ç»œï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ResNet-101çš„ImageNet-1Kï¼‰ï¼Œå·¨å¤§çš„ä¼˜åŒ–ç©ºé—´é˜»ç¢äº†è’¸é¦çš„æœ‰æ•ˆæ€§ï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚æœ€è¿‘çš„æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ç›´æ¥ç”Ÿæˆä¿¡æ¯å›¾åƒï¼Œä»è€Œç»•è¿‡åƒç´ çº§ä¼˜åŒ–ï¼Œå¹¶å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ç»å¸¸é­å—é¢„è®­ç»ƒæ‰©æ•£å…ˆéªŒå’Œç›®æ ‡æ•°æ®é›†ä¹‹é—´çš„åˆ†å¸ƒåç§»é—®é¢˜ï¼Œä»¥åŠåœ¨ä¸åŒè®¾ç½®ä¸‹éœ€è¦å¤šæ¬¡è’¸é¦æ­¥éª¤çš„å›°æ‰°ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸ç°æœ‰çš„åŸºäºæ‰©æ•£çš„è’¸é¦æŠ€æœ¯æ­£äº¤ï¼Œåˆ©ç”¨æ‰©æ•£å…ˆéªŒè¿›è¡Œè¡¥ä¸é€‰æ‹©è€Œä¸æ˜¯ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•æ ¹æ®è¾“å…¥å›¾åƒå’Œå¯é€‰çš„æ–‡æœ¬æç¤ºï¼ˆå¸¦æœ‰æˆ–ä¸å¸¦æ ‡ç­¾ä¿¡æ¯ï¼‰é¢„æµ‹æ‰©æ•£æ¨¡å‹ä¸­çš„å™ªå£°ï¼Œå¹¶è®¡ç®—æ¯ä¸ªå›¾åƒ-è¡¥ä¸å¯¹çš„æŸå¤±ã€‚åŸºäºæŸå¤±å·®å¼‚ï¼Œæˆ‘ä»¬å¯ä»¥è¯†åˆ«åŸå§‹å›¾åƒä¸­çš„ç‹¬ç‰¹åŒºåŸŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹é€‰å®šçš„è¡¥ä¸è¿›è¡Œç±»å†…èšç±»å’Œæ’åï¼Œä»¥å®æ–½å¤šæ ·æ€§çº¦æŸã€‚è¿™ç§ç®€åŒ–çš„ç®¡é“å¯å®ç°ä¸€æ­¥è’¸é¦è¿‡ç¨‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§æŒ‡æ ‡å’Œè®¾ç½®ä¸‹å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09959v4">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åˆ©ç”¨æ•°æ®é›†è’¸é¦æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°é™ä½å†…å­˜å’Œè®¡ç®—æˆæœ¬ï¼Œé€šè¿‡ä¼˜åŒ–ç´§å‡‘æ•°æ®é›†å®ç°ä¸åŸå§‹æ•°æ®é›†ç›¸å½“çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹äºå¤§è§„æ¨¡æ•°æ®é›†å’Œå¤æ‚æ·±åº¦ç½‘ç»œï¼ˆå¦‚ImageNet-1Kä¸ResNet-101ï¼‰ï¼Œå·¨å¤§çš„ä¼˜åŒ–ç©ºé—´é˜»ç¢äº†è’¸é¦æ•ˆæœï¼Œé™åˆ¶äº†å®é™…åº”ç”¨ã€‚æœ€è¿‘çš„æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ç›´æ¥ç”Ÿæˆå›¾åƒï¼Œä»è€Œç»•è¿‡åƒç´ çº§ä¼˜åŒ–ï¼Œå¹¶è·å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¸¸å¸¸å—åˆ°é¢„è®­ç»ƒæ‰©æ•£å…ˆéªŒå’Œç›®æ ‡æ•°æ®é›†ä¹‹é—´åˆ†å¸ƒå˜åŒ–çš„å›°æ‰°ï¼Œä»¥åŠéœ€è¦åœ¨ä¸åŒè®¾ç½®ä¸‹è¿›è¡Œå¤šæ¬¡è’¸é¦æ­¥éª¤ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå®ƒä¸ç°æœ‰çš„åŸºäºæ‰©æ•£çš„è’¸é¦æŠ€æœ¯æ­£äº¤ï¼Œåˆ©ç”¨æ‰©æ•£å…ˆéªŒè¿›è¡Œè¡¥ä¸é€‰æ‹©è€Œä¸æ˜¯ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»æ‰©æ•£æ¨¡å‹ä¸­é¢„æµ‹ç»™å®šè¾“å…¥å›¾åƒå’Œå¯é€‰æ–‡æœ¬æç¤ºçš„å™ªå£°ï¼Œå¹¶è®¡ç®—æ¯ä¸ªå›¾åƒè¡¥ä¸å¯¹çš„æŸå¤±ã€‚åŸºäºæŸå¤±å·®å¼‚ï¼Œæˆ‘ä»¬ç¡®å®šäº†åŸå§‹å›¾åƒä¸­çš„ä¸åŒåŒºåŸŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æ‰€é€‰è¡¥ä¸è¿›è¡Œç±»å†…èšç±»å’Œæ’åï¼Œä»¥å®æ–½å¤šæ ·æ€§çº¦æŸã€‚è¿™ä¸€ç®€åŒ–çš„ç®¡é“å®ç°äº†ä¸€æ¬¡æ€§è’¸é¦è¿‡ç¨‹ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§æŒ‡æ ‡å’Œè®¾ç½®ä¸‹å‡ä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä»‹ç»äº†åˆ©ç”¨æ•°æ®é›†è’¸é¦é™ä½å†…å­˜å’Œè®¡ç®—æˆæœ¬çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§è§„æ¨¡æ•°æ®é›†å’Œå¤æ‚æ·±åº¦ç½‘ç»œçš„åº”ç”¨åœºæ™¯ã€‚</li>
<li>è®¨è®ºäº†å½“å‰æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚é¢„è®­ç»ƒæ‰©æ•£å…ˆéªŒä¸ç›®æ ‡æ•°æ®é›†ä¹‹é—´çš„åˆ†å¸ƒå˜åŒ–ä»¥åŠå¤šæ¬¡è’¸é¦æ­¥éª¤çš„éœ€æ±‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è’¸é¦æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè¡¥ä¸é€‰æ‹©è€Œä¸æ˜¯å›¾åƒç”Ÿæˆã€‚</li>
<li>æè¿°äº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å™ªå£°é¢„æµ‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è€ƒè™‘äº†è¾“å…¥å›¾åƒå’Œå¯é€‰æ–‡æœ¬æç¤ºï¼ˆå¸¦æœ‰æˆ–ä¸å¸¦æœ‰æ ‡ç­¾ä¿¡æ¯ï¼‰ã€‚</li>
<li>é€šè¿‡è®¡ç®—æŸå¤±å·®å¼‚æ¥è¯†åˆ«åŸå§‹å›¾åƒä¸­çš„ç‹¬ç‰¹åŒºåŸŸï¼Œå¹¶åº”ç”¨ç±»å†…èšç±»å’Œæ’åæ¥å®æ–½å¤šæ ·æ€§çº¦æŸã€‚</li>
<li>å®ç°äº†ä¸€æ¬¡æ€§è’¸é¦è¿‡ç¨‹ï¼Œæé«˜äº†æ•ˆç‡å¹¶ä¼˜åŒ–äº†æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5af5118a426e93e5f83d6c01f705b119" align="middle">
<img src="https://picx.zhimg.com/v2-1e1e148b49a13a51bf1bdff4632431b9" align="middle">
<img src="https://picx.zhimg.com/v2-15188d8dfc37943642b90ff5606e1db5" align="middle">
<img src="https://picx.zhimg.com/v2-be640fb38532568545cf537a9ad99aa6" align="middle">
<img src="https://picx.zhimg.com/v2-105b6ac504de7dea679c5e411d8d5c24" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="The-Visual-Counter-Turing-Test-VCT2-A-Benchmark-for-Evaluating-AI-Generated-Image-Detection-and-the-Visual-AI-Index-VAI"><a href="#The-Visual-Counter-Turing-Test-VCT2-A-Benchmark-for-Evaluating-AI-Generated-Image-Detection-and-the-Visual-AI-Index-VAI" class="headerlink" title="The Visual Counter Turing Test (VCT2): A Benchmark for Evaluating AI-Generated Image Detection and the Visual AI Index (VAI)"></a>The Visual Counter Turing Test (VCT2): A Benchmark for Evaluating AI-Generated Image Detection and the Visual AI Index (VAI)</h2><p><strong>Authors:Nasrin Imanpour, Abhilekh Borah, Shashwat Bajpai, Subhankar Ghosh, Sainath Reddy Sankepally, Hasnat Md Abdullah, Nishoak Kosaraju, Shreyas Dixit, Ashhar Aziz, Shwetangshu Biswas, Vinija Jain, Aman Chadha, Song Wang, Amit Sheth, Amitava Das</strong></p>
<p>The rapid progress and widespread availability of text-to-image (T2I) generative models have heightened concerns about the misuse of AI-generated visuals, particularly in the context of misinformation campaigns. Existing AI-generated image detection (AGID) methods often overfit to known generators and falter on outputs from newer or unseen models. We introduce the Visual Counter Turing Test (VCT2), a comprehensive benchmark of 166,000 images, comprising both real and synthetic prompt-image pairs produced by six state-of-the-art T2I systems: Stable Diffusion 2.1, SDXL, SD3 Medium, SD3.5 Large, DALL.E 3, and Midjourney 6. We curate two distinct subsets: COCOAI, featuring structured captions from MS COCO, and TwitterAI, containing narrative-style tweets from The New York Times. Under a unified zero-shot evaluation, we benchmark 17 leading AGID models and observe alarmingly low detection accuracy, 58% on COCOAI and 58.34% on TwitterAI. To transcend binary classification, we propose the Visual AI Index (VAI), an interpretable, prompt-agnostic realism metric based on twelve low-level visual features, enabling us to quantify and rank the perceptual quality of generated outputs with greater nuance. Correlation analysis reveals a moderate inverse relationship between VAI and detection accuracy: Pearson of -0.532 on COCOAI and -0.503 on TwitterAI, suggesting that more visually realistic images tend to be harder to detect, a trend observed consistently across generators. We release COCOAI, TwitterAI, and all codes to catalyze future advances in generalized AGID and perceptual realism assessment.</p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿè¿›æ­¥å’Œå¹¿æ³›å¯ç”¨æ€§åŠ å‰§äº†äººä»¬å¯¹äººå·¥æ™ºèƒ½ç”Ÿæˆå›¾åƒè¯¯ç”¨çš„æ‹…å¿§ï¼Œç‰¹åˆ«æ˜¯åœ¨è™šå‡ä¿¡æ¯æ³›æ»¥çš„èƒŒæ™¯ä¸‹ã€‚ç°æœ‰çš„AIç”Ÿæˆå›¾åƒæ£€æµ‹ï¼ˆAGIDï¼‰æ–¹æ³•é€šå¸¸ä¼šå¯¹å·²çŸ¥çš„ç”Ÿæˆå™¨è¿‡åº¦æ‹Ÿåˆï¼Œè€Œåœ¨é¢å¯¹æ›´æ–°æˆ–æœªè§è¿‡çš„æ¨¡å‹è¾“å‡ºæ—¶åˆ™è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬å¼•å…¥äº†è§†è§‰åå›¾çµæµ‹è¯•ï¼ˆVCT2ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«16.6ä¸‡å¼ å›¾åƒçš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­åŒ…æ‹¬ç”±å…­ç§æœ€å…ˆè¿›çš„T2Iç³»ç»Ÿäº§ç”Ÿçš„çœŸå®å’Œåˆæˆæç¤ºå›¾åƒå¯¹ï¼šStable Diffusion 2.1ã€SDXLã€SD3 Mediumã€SD3.5 Largeã€DALL.E 3å’ŒMidjourney 6ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªç‹¬ç‰¹çš„å­é›†ï¼šCOCOAIï¼Œä»¥MS COCOçš„ç»“æ„åŒ–æ ‡é¢˜ä¸ºç‰¹è‰²ï¼Œä»¥åŠTwitterAIï¼ŒåŒ…å«ã€Šçº½çº¦æ—¶æŠ¥ã€‹çš„å™äº‹å¼æ¨ç‰¹ã€‚åœ¨ç»Ÿä¸€çš„é›¶æ ·æœ¬è¯„ä¼°ä¸‹ï¼Œæˆ‘ä»¬å¯¹é¢†å…ˆçš„17ç§AGIDæ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œä»¤äººè­¦æƒ•çš„æ˜¯ï¼Œæ£€æµ‹å‡†ç¡®ç‡æä½ï¼Œåœ¨COCOAIä¸Šä¸º58%ï¼Œåœ¨TwitterAIä¸Šä¸º58.34%ã€‚ä¸ºäº†è¶…è¶ŠäºŒå…ƒåˆ†ç±»ï¼Œæˆ‘ä»¬æå‡ºäº†è§†è§‰äººå·¥æ™ºèƒ½æŒ‡æ•°ï¼ˆVAIï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºåäºŒä¸ªä½çº§è§†è§‰ç‰¹å¾çš„ã€å¯è§£é‡Šçš„ã€æç¤ºæ— å…³çš„é€¼çœŸåº¦æŒ‡æ ‡ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿæ›´å¾®å¦™åœ°é‡åŒ–å¹¶æ’åˆ—ç”Ÿæˆè¾“å‡ºçš„æ„ŸçŸ¥è´¨é‡ã€‚ç›¸å…³æ€§åˆ†ææ˜¾ç¤ºVAIä¸æ£€æµ‹å‡†ç¡®ç‡ä¹‹é—´å­˜åœ¨é€‚ä¸­çš„é€†å‘å…³ç³»ï¼šåœ¨COCOAIä¸Šçš„Pearsonå€¼ä¸º-0.532ï¼Œåœ¨TwitterAIä¸Šä¸º-0.503ï¼Œè¿™è¡¨æ˜æ›´é€¼çœŸçš„å›¾åƒå¾€å¾€æ›´éš¾æ£€æµ‹ï¼Œè¿™ä¸€è¶‹åŠ¿åœ¨å„å¤§ç”Ÿæˆå™¨ä¸­å‡æŒç»­å­˜åœ¨ã€‚æˆ‘ä»¬å‘å¸ƒCOCOAIã€TwitterAIåŠæ‰€æœ‰ä»£ç ï¼Œä»¥æ¨åŠ¨æœªæ¥åœ¨é€šç”¨AGIDå’Œæ„ŸçŸ¥é€¼çœŸåº¦è¯„ä¼°æ–¹é¢çš„è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16754v2">PDF</a> 13 pages, 9 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡å…³æ³¨æ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ¨¡å‹çš„æ»¥ç”¨é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è™šå‡ä¿¡æ¯ä¼ æ’­èƒŒæ™¯ä¸‹çš„æ‹…å¿§ã€‚ç°æœ‰çš„AIç”Ÿæˆå›¾åƒæ£€æµ‹ï¼ˆAGIDï¼‰æ–¹æ³•ç»å¸¸è¿‡åº¦æ‹Ÿåˆå·²çŸ¥ç”Ÿæˆå™¨ï¼Œè€Œæ— æ³•è¯†åˆ«æ¥è‡ªæ–°å‹æˆ–æœªçŸ¥æ¨¡å‹çš„è¾“å‡ºã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†è§†è§‰åå›¾çµæµ‹è¯•ï¼ˆVCT2ï¼‰ï¼ŒåŒ…å«166,000å¼ å›¾åƒçš„ç»¼åˆåŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…æ‹¬ç”±å…­å¤§é¡¶å°–T2Iç³»ç»Ÿäº§ç”Ÿçš„çœŸå®å’Œåˆæˆæç¤ºå›¾åƒå¯¹ã€‚é€šè¿‡ç»Ÿä¸€é›¶æ ·æœ¬è¯„ä¼°ï¼Œå¯¹é¢†å…ˆçš„AGIDæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå‘ç°æ£€æµ‹å‡†ç¡®ç‡ä»¤äººéœ‡æƒŠåœ°ä½ï¼Œåˆ†åˆ«ä¸ºCOCOAIçš„58%å’ŒTwitterAIçš„58.34%ã€‚ä¸ºè¶…è¶ŠäºŒå…ƒåˆ†ç±»ï¼Œæå‡ºäº†å¯è§†åŒ–AIæŒ‡æ•°ï¼ˆVAIï¼‰ï¼Œä¸€ä¸ªåŸºäºåäºŒç§ä½çº§è§†è§‰ç‰¹å¾çš„ã€æç¤ºæ— å…³çš„é€¼çœŸåº¦æŒ‡æ ‡ï¼Œèƒ½å¤Ÿæ›´ç²¾ç»†åœ°è¡¡é‡å’Œæ’åç”Ÿæˆè¾“å‡ºçš„æ„ŸçŸ¥è´¨é‡ã€‚ç›¸å…³æ€§åˆ†ææ˜¾ç¤ºï¼ŒVAIä¸æ£€æµ‹å‡†ç¡®ç‡ä¹‹é—´å­˜åœ¨ä¸­åº¦è´Ÿç›¸å…³å…³ç³»ï¼Œåœ¨COCOAIå’ŒTwitterAIä¸Šçš„Pearsonç³»æ•°åˆ†åˆ«ä¸º-0.532å’Œ-0.503ï¼Œè¿™è¡¨æ˜æ›´é€¼çœŸçš„å›¾åƒå¾€å¾€æ›´éš¾æ£€æµ‹ã€‚æœ¬æ–‡å…¬å¼€äº†COCOAIã€TwitterAIåŠæ‰€æœ‰ä»£ç ï¼Œä»¥æ¨åŠ¨é€šç”¨AGIDå’Œæ„ŸçŸ¥é€¼çœŸåº¦è¯„ä¼°çš„æœªæ¥å‘å±•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•å¼•å‘å…³äºæ»¥ç”¨å’Œè™šå‡ä¿¡æ¯ä¼ æ’­çš„æ‹…å¿§ã€‚</li>
<li>ç°æœ‰çš„AIç”Ÿæˆå›¾åƒæ£€æµ‹ï¼ˆAGIDï¼‰æ–¹æ³•å¯¹æ–°æ¨¡å‹å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>VCT2ä¸ºT2Iç³»ç»Ÿå’ŒAGIDæ–¹æ³•æä¾›äº†ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•é›†ã€‚</li>
<li>äºŒå…ƒåˆ†ç±»æ³•åœ¨å›¾åƒæ£€æµ‹æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œéœ€æ›´å¤šæ ·åŒ–çš„è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>VAIæŒ‡æ ‡çš„å¼•å…¥æœ‰åŠ©äºæ›´ç²¾ç»†åœ°è¡¡é‡ç”Ÿæˆå›¾åƒçš„æ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>VAIä¸AGIDçš„æ£€æµ‹å‡†ç¡®ç‡ä¹‹é—´å­˜åœ¨ä¸­åº¦è´Ÿç›¸å…³å…³ç³»ï¼Œæ›´é€¼çœŸçš„å›¾åƒæ›´éš¾æ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16754">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-52fbd55f973778cf98af5303fd91cae1" align="middle">
<img src="https://picx.zhimg.com/v2-3d7adf022d04950a83819985b89c3793" align="middle">
<img src="https://picx.zhimg.com/v2-3d3e2c86960ced823f5e8308bd3c8b84" align="middle">
<img src="https://picx.zhimg.com/v2-cedd228f521d4015f22866d81f239825" align="middle">
<img src="https://picx.zhimg.com/v2-add7009b22af697ec2c86c57f2c05f12" align="middle">
<img src="https://picx.zhimg.com/v2-a9776037f408a87fe4a0b415cef18177" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Chat2SVG-Vector-Graphics-Generation-with-Large-Language-Models-and-Image-Diffusion-Models"><a href="#Chat2SVG-Vector-Graphics-Generation-with-Large-Language-Models-and-Image-Diffusion-Models" class="headerlink" title="Chat2SVG: Vector Graphics Generation with Large Language Models and Image Diffusion Models"></a>Chat2SVG: Vector Graphics Generation with Large Language Models and Image Diffusion Models</h2><p><strong>Authors:Ronghuan Wu, Wanchao Su, Jing Liao</strong></p>
<p>Scalable Vector Graphics (SVG) has become the de facto standard for vector graphics in digital design, offering resolution independence and precise control over individual elements. Despite their advantages, creating high-quality SVG content remains challenging, as it demands technical expertise with professional editing software and a considerable time investment to craft complex shapes. Recent text-to-SVG generation methods aim to make vector graphics creation more accessible, but they still encounter limitations in shape regularity, generalization ability, and expressiveness. To address these challenges, we introduce Chat2SVG, a hybrid framework that combines the strengths of Large Language Models (LLMs) and image diffusion models for text-to-SVG generation. Our approach first uses an LLM to generate semantically meaningful SVG templates from basic geometric primitives. Guided by image diffusion models, a dual-stage optimization pipeline refines paths in latent space and adjusts point coordinates to enhance geometric complexity. Extensive experiments show that Chat2SVG outperforms existing methods in visual fidelity, path regularity, and semantic alignment. Additionally, our system enables intuitive editing through natural language instructions, making professional vector graphics creation accessible to all users.</p>
<blockquote>
<p>å¯ç¼©æ”¾çŸ¢é‡å›¾å½¢ï¼ˆSVGï¼‰å·²æˆä¸ºæ•°å­—è®¾è®¡ä¸­çŸ¢é‡å›¾å½¢çš„å®é™…æ ‡å‡†ï¼Œæä¾›åˆ†è¾¨ç‡ç‹¬ç«‹æ€§å’Œå¯¹å„ä¸ªå…ƒç´ çš„ç²¾ç¡®æ§åˆ¶ã€‚å°½ç®¡å…·æœ‰ä¼˜åŠ¿ï¼Œä½†åˆ›å»ºé«˜è´¨é‡çš„SVGå†…å®¹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºéœ€è¦æŒæ¡ä¸“ä¸šç¼–è¾‘è½¯ä»¶çš„æŠ€æœ¯çŸ¥è¯†å’Œå¤§é‡æ—¶é—´æ¥å¡‘é€ å¤æ‚çš„å½¢çŠ¶ã€‚æœ€è¿‘çš„æ–‡æœ¬åˆ°SVGç”Ÿæˆæ–¹æ³•æ—¨åœ¨ä½¿çŸ¢é‡å›¾å½¢çš„åˆ›å»ºæ›´åŠ æ˜“äºè®¿é—®ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´ç€å½¢çŠ¶è§„åˆ™ã€é€šç”¨èƒ½åŠ›å’Œè¡¨ç°åŠ›çš„å±€é™æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Chat2SVGï¼Œè¿™æ˜¯ä¸€ä¸ªæ··åˆæ¡†æ¶ï¼Œç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå›¾åƒæ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°SVGç”Ÿæˆä¸­çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆä½¿ç”¨LLMä»åŸºæœ¬å‡ ä½•åŸè¯­ç”Ÿæˆè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„SVGæ¨¡æ¿ã€‚åœ¨å›¾åƒæ‰©æ•£æ¨¡å‹çš„æŒ‡å¯¼ä¸‹ï¼Œä¸€ä¸ªä¸¤é˜¶æ®µä¼˜åŒ–ç®¡é“åœ¨æ½œåœ¨ç©ºé—´ä¸­ç»†åŒ–è·¯å¾„å¹¶è°ƒæ•´ç‚¹åæ ‡ï¼Œä»¥æé«˜å‡ ä½•å¤æ‚æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒChat2SVGåœ¨è§†è§‰ä¿çœŸåº¦ã€è·¯å¾„è§„åˆ™æ€§å’Œè¯­ä¹‰å¯¹é½æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿé€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤å®ç°äº†ç›´è§‚çš„ç¼–è¾‘ï¼Œä½¿ä¸“ä¸šçŸ¢é‡å›¾å½¢çš„åˆ›å»ºå¯¹æ‰€æœ‰ç”¨æˆ·éƒ½æ˜“äºè®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16602v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://chat2svg.github.io/">https://chat2svg.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>SVGå·²æˆä¸ºæ•°å­—è®¾è®¡çš„çŸ¢é‡å›¾å½¢æ ‡å‡†ï¼Œä½†åœ¨åˆ›å»ºé«˜è´¨é‡SVGå†…å®¹æ—¶ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æŒ‘æˆ˜ï¼Œæå‡ºChat2SVGæ··åˆæ¡†æ¶ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ°SVGçš„ç”Ÿæˆã€‚è¯¥æ–¹æ³•é€šè¿‡LLMç”Ÿæˆæœ‰æ„ä¹‰çš„SVGæ¨¡æ¿ï¼Œå¹¶é€šè¿‡å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œè·¯å¾„ä¼˜åŒ–å’Œç‚¹åæ ‡è°ƒæ•´ï¼Œä»è€Œæé«˜å‡ ä½•å¤æ‚æ€§ã€‚Chat2SVGåœ¨è§†è§‰ä¿çœŸåº¦ã€è·¯å¾„è§„å¾‹æ€§å’Œè¯­ä¹‰å¯¹é½æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶å¯é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤è¿›è¡Œç›´è§‚ç¼–è¾‘ï¼Œä½¿ä¸“ä¸šçŸ¢é‡å›¾å½¢åˆ›ä½œå¯¹æ‰€æœ‰äººå¯è¡Œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SVGä½œä¸ºæ•°å­—è®¾è®¡çŸ¢é‡å›¾å½¢æ ‡å‡†çš„ä¼˜åŠ¿åœ¨äºå…¶åˆ†è¾¨ç‡ç‹¬ç«‹æ€§å’Œå¯¹ä¸ªåˆ«å…ƒç´ çš„ç²¾ç¡®æ§åˆ¶ã€‚</li>
<li>åˆ›å»ºé«˜è´¨é‡çš„SVGå†…å®¹éœ€è¦ä¸“ä¸šæŠ€æœ¯çŸ¥è¯†å’Œå¯¹å¤æ‚å½¢çŠ¶çš„æ—¶é—´æŠ•å…¥ã€‚</li>
<li>ç°æœ‰æ–‡æœ¬åˆ°SVGç”Ÿæˆæ–¹æ³•å­˜åœ¨å½¢çŠ¶è§„åˆ™æ€§ã€æ³›åŒ–èƒ½åŠ›å’Œè¡¨ç°åŠ›æ–¹é¢çš„å±€é™ã€‚</li>
<li>Chat2SVGæ¡†æ¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œå›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œç”¨äºæ–‡æœ¬åˆ°SVGçš„ç”Ÿæˆã€‚</li>
<li>Chat2SVGé€šè¿‡LLMç”Ÿæˆæœ‰æ„ä¹‰çš„SVGæ¨¡æ¿ï¼Œå¹¶é€šè¿‡å›¾åƒæ‰©æ•£æ¨¡å‹è¿›è¡Œç²¾ç»†åŒ–ä¼˜åŒ–ã€‚</li>
<li>Chat2SVGåœ¨è§†è§‰ä¿çœŸåº¦ã€è·¯å¾„è§„å¾‹æ€§å’Œè¯­ä¹‰å¯¹é½æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16602">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-76015b4e736d677e2f5be0919816a81f" align="middle">
<img src="https://picx.zhimg.com/v2-7cb4e49f32c6159cb3d107e8b345162e" align="middle">
<img src="https://picx.zhimg.com/v2-32fcd4f789e8913c122df8f1ebbf4aa1" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CART-Compositional-Auto-Regressive-Transformer-for-Image-Generation"><a href="#CART-Compositional-Auto-Regressive-Transformer-for-Image-Generation" class="headerlink" title="CART: Compositional Auto-Regressive Transformer for Image Generation"></a>CART: Compositional Auto-Regressive Transformer for Image Generation</h2><p><strong>Authors:Siddharth Roheda, Rohit Chowdhury, Aniruddha Bala, Rohan Jaiswal</strong></p>
<p>We propose a novel Auto-Regressive (AR) image generation approach that models images as hierarchical compositions of interpretable visual layers. While AR models have achieved transformative success in language modeling, replicating this success in vision tasks remains challenging due to inherent spatial dependencies in images. Addressing the unique challenges of vision tasks, our method (CART) adds image details iteratively via semantically meaningful decompositions. We demonstrate the flexibility and generality of CART by applying it across three distinct decomposition strategies: (i) Base-Detail Decomposition (Mumford-Shah smoothness), (ii) Intrinsic Decomposition (albedo&#x2F;shading), and (iii) Specularity Decomposition (diffuse&#x2F;specular). This next-detail strategy outperforms traditional next-token and next-scale approaches, improving controllability, semantic interpretability, and resolution scalability. Experiments show CART generates visually compelling results while enabling structured image manipulation, opening new directions for controllable generative modeling via physically or perceptually motivated image factorization.</p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºè‡ªå›å½’ï¼ˆARï¼‰çš„å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†å›¾åƒå»ºæ¨¡ä¸ºå¯è§£é‡Šè§†è§‰å±‚æ¬¡çš„å±‚æ¬¡ç»“æ„ç»„åˆã€‚è™½ç„¶è‡ªå›å½’æ¨¡å‹åœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢å–å¾—äº†é©å‘½æ€§çš„æˆåŠŸï¼Œä½†ç”±äºå›¾åƒå›ºæœ‰çš„ç©ºé—´ä¾èµ–æ€§ï¼Œåœ¨è§†è§‰ä»»åŠ¡ä¸Šå¤åˆ¶è¿™ä¸€æˆåŠŸä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ï¼ˆCARTï¼‰é€šè¿‡è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„åˆ†è§£è¿­ä»£åœ°æ·»åŠ å›¾åƒç»†èŠ‚ï¼Œä»¥åº”å¯¹è§†è§‰ä»»åŠ¡çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ä¸‰ç§ä¸åŒçš„åˆ†è§£ç­–ç•¥ä¸­åº”ç”¨CARTæ¥å±•ç¤ºå…¶çµæ´»æ€§å’Œé€šç”¨æ€§ï¼šï¼ˆiï¼‰åŸºç¡€ç»†èŠ‚åˆ†è§£ï¼ˆMumford-Shahå¹³æ»‘åº¦ï¼‰ã€ï¼ˆiiï¼‰å†…åœ¨åˆ†è§£ï¼ˆäº®åº¦&#x2F;é˜´å½±ï¼‰å’Œï¼ˆiiiï¼‰å…‰æ³½åˆ†è§£ï¼ˆæ¼«åå°„&#x2F;å…‰æ³½ï¼‰ã€‚è¿™ç§ä¸‹ä¸€ä¸ªç»†èŠ‚çš„ç­–ç•¥ä¼˜äºä¼ ç»Ÿçš„ä¸‹ä¸€ä¸ªä»¤ç‰Œå’Œä¸‹ä¸€ä¸ªå°ºåº¦çš„æ–¹æ³•ï¼Œæé«˜äº†å¯æ§æ€§ã€è¯­ä¹‰å¯è§£é‡Šæ€§å’Œåˆ†è¾¨ç‡å¯æ‰©å±•æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒCARTç”Ÿæˆäº†è§†è§‰ä¸Šå¼•äººæ³¨ç›®çš„ç»“æœï¼ŒåŒæ—¶å®ç°äº†ç»“æ„åŒ–å›¾åƒæ“ä½œï¼Œä¸ºé€šè¿‡ç‰©ç†æˆ–æ„ŸçŸ¥é©±åŠ¨çš„å›¾åƒåˆ†è§£è¿›è¡Œå¯æ§ç”Ÿæˆå»ºæ¨¡æ‰“å¼€äº†æ–°çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10180v3">PDF</a> figures compressed to meet arxiv size limit</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„è‡ªå›å½’ï¼ˆARï¼‰å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†å›¾åƒå»ºæ¨¡ä¸ºå¯è§£é‡Šè§†è§‰å±‚çº§çš„å±‚æ¬¡ç»“æ„ç»„åˆã€‚å°½ç®¡ARæ¨¡å‹åœ¨è¯­è¨€å»ºæ¨¡ä¸­å–å¾—äº†çªç ´æ€§æˆåŠŸï¼Œä½†åœ¨è§†è§‰ä»»åŠ¡ä¸­å¤åˆ¶è¿™ä¸€æˆåŠŸä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå›¾åƒä¸­å›ºæœ‰çš„ç©ºé—´ä¾èµ–æ€§ã€‚é€šè¿‡è§£å†³è§†è§‰ä»»åŠ¡çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ï¼ˆCARTï¼‰é€šè¿‡è¯­ä¹‰æœ‰æ„ä¹‰çš„åˆ†è§£è¿­ä»£åœ°æ·»åŠ å›¾åƒç»†èŠ‚ã€‚æˆ‘ä»¬å±•ç¤ºäº†CARTåœ¨ä¸‰ç§ä¸åŒåˆ†è§£ç­–ç•¥ä¸Šçš„çµæ´»æ€§å’Œé€šç”¨æ€§ï¼šï¼ˆiï¼‰åŸºç¡€ç»†èŠ‚åˆ†è§£ï¼ˆMumford-Shahå¹³æ»‘åº¦ï¼‰ï¼Œï¼ˆiiï¼‰å†…åœ¨åˆ†è§£ï¼ˆäº®åº¦&#x2F;é˜´å½±ï¼‰ï¼Œä»¥åŠï¼ˆiiiï¼‰å…‰æ³½åˆ†è§£ï¼ˆæ¼«å°„&#x2F;å…‰æ³½ï¼‰ã€‚è¿™ç§ä¸‹ä¸€ç»†èŠ‚ç­–ç•¥ä¼˜äºä¼ ç»Ÿçš„ä¸‹ä¸€ä»¤ç‰Œå’Œä¸‹ä¸€å°ºåº¦æ–¹æ³•ï¼Œæé«˜äº†å¯æ§æ€§ã€è¯­ä¹‰å¯è§£é‡Šæ€§å’Œåˆ†è¾¨ç‡å¯æ‰©å±•æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒCARTç”Ÿæˆäº†è§†è§‰ä¸Šå¼•äººæ³¨ç›®çš„ç»“æœï¼ŒåŒæ—¶å®ç°äº†ç»“æ„åŒ–å›¾åƒæ“ä½œï¼Œä¸ºé€šè¿‡ç‰©ç†æˆ–æ„ŸçŸ¥åŠ¨æœºçš„å›¾åƒåˆ†è§£è¿›è¡Œå¯æ§ç”Ÿæˆå»ºæ¨¡å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªå›å½’ï¼ˆARï¼‰å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œå°†å›¾åƒè§†ä¸ºå¯è§£é‡Šçš„è§†è§‰å±‚çº§ç»“æ„ã€‚</li>
<li>ARæ¨¡å‹åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„åº”ç”¨é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºå›¾åƒçš„ç©ºé—´ä¾èµ–æ€§ã€‚</li>
<li>CARTæ–¹æ³•é€šè¿‡è¯­ä¹‰æœ‰æ„ä¹‰çš„åˆ†è§£è¿­ä»£åœ°å¢åŠ å›¾åƒç»†èŠ‚ã€‚</li>
<li>CARTåœ¨ä¸‰ç§ä¸åŒçš„åˆ†è§£ç­–ç•¥ä¸Šè¿›è¡Œäº†å±•ç¤ºï¼šåŸºç¡€ç»†èŠ‚åˆ†è§£ã€å†…åœ¨åˆ†è§£å’Œå…‰æ³½åˆ†è§£ã€‚</li>
<li>CARTçš„ä¸‹ä¸€ç»†èŠ‚ç­–ç•¥åœ¨å¯æ§æ€§ã€è¯­ä¹‰å¯è§£é‡Šæ€§å’Œåˆ†è¾¨ç‡å¯æ‰©å±•æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>CARTç”Ÿæˆçš„å›¾åƒå…·æœ‰è§†è§‰å¸å¼•åŠ›ï¼Œå¹¶èƒ½å®ç°ç»“æ„åŒ–å›¾åƒæ“ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10180">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d735aed9f6e5116e3243b449a98d5bed" align="middle">
<img src="https://picx.zhimg.com/v2-2cd7d9e316a461ba1a249fa172d19ecc" align="middle">
<img src="https://picx.zhimg.com/v2-7e2b9a84c0bd326a737759855bf6493a" align="middle">
<img src="https://picx.zhimg.com/v2-d28ccc03356bd87a66fb2e2ebf234374" align="middle">
<img src="https://picx.zhimg.com/v2-65b7f1792fb5fdc7a699efaaa8c179c6" align="middle">
<img src="https://picx.zhimg.com/v2-67cab2920f9b7c98f0f0ba3efc04ea79" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="DICE-Discrete-Inversion-Enabling-Controllable-Editing-for-Multinomial-Diffusion-and-Masked-Generative-Models"><a href="#DICE-Discrete-Inversion-Enabling-Controllable-Editing-for-Multinomial-Diffusion-and-Masked-Generative-Models" class="headerlink" title="DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models"></a>DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models</h2><p><strong>Authors:Xiaoxiao He, Quan Dao, Ligong Han, Song Wen, Minhao Bai, Di Liu, Han Zhang, Martin Renqiang Min, Felix Juefei-Xu, Chaowei Tan, Bo Liu, Kang Li, Hongdong Li, Junzhou Huang, Faez Ahmed, Akash Srivastava, Dimitris Metaxas</strong></p>
<p>Discrete diffusion models have achieved success in tasks like image generation and masked language modeling but face limitations in controlled content editing. We introduce DICE (Discrete Inversion for Controllable Editing), the first approach to enable precise inversion for discrete diffusion models, including multinomial diffusion and masked generative models. By recording noise sequences and masking patterns during the reverse diffusion process, DICE enables accurate reconstruction and flexible editing of discrete data without the need for predefined masks or attention manipulation. We demonstrate the effectiveness of DICE across both image and text domains, evaluating it on models such as VQ-Diffusion, Paella, and RoBERTa. Our results show that DICE preserves high data fidelity while enhancing editing capabilities, offering new opportunities for fine-grained content manipulation in discrete spaces.</p>
<blockquote>
<p>ç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œæ©ç è¯­è¨€å»ºæ¨¡ç­‰ä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨å—æ§å†…å®¹ç¼–è¾‘æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†DICEï¼ˆå¯æ§ç¼–è¾‘çš„ç¦»æ•£åè½¬æ³•ï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ç§ä¸ºç¦»æ•£æ‰©æ•£æ¨¡å‹å®ç°ç²¾ç¡®åè½¬çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å¤šé¡¹å¼æ‰©æ•£å’Œæ©ç ç”Ÿæˆæ¨¡å‹ã€‚é€šè¿‡è®°å½•åå‘æ‰©æ•£è¿‡ç¨‹ä¸­çš„å™ªå£°åºåˆ—å’Œæ©ç æ¨¡å¼ï¼ŒDICEèƒ½å¤Ÿåœ¨ä¸éœ€è¦é¢„å…ˆå®šä¹‰çš„æ©ç æˆ–æ³¨æ„åŠ›æ“çºµçš„æƒ…å†µä¸‹ï¼Œå®ç°å¯¹ç¦»æ•£æ•°æ®çš„ç²¾ç¡®é‡æ„å’Œçµæ´»ç¼–è¾‘ã€‚æˆ‘ä»¬åœ¨å›¾åƒå’Œæ–‡æœ¬é¢†åŸŸéƒ½å±•ç¤ºäº†DICEçš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨VQ-Diffusionã€Paellaå’ŒRoBERTaç­‰æ¨¡å‹ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒDICEåœ¨ä¿æŒé«˜æ•°æ®ä¿çœŸåº¦çš„åŒæ—¶ï¼Œæé«˜äº†ç¼–è¾‘èƒ½åŠ›ï¼Œä¸ºç¦»æ•£ç©ºé—´ä¸­çš„ç»†ç²’åº¦å†…å®¹æ“ä½œæä¾›äº†æ–°çš„æœºä¼šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.08207v3">PDF</a> Project webpage: <a target="_blank" rel="noopener" href="https://hexiaoxiao-cs.github.io/DICE/">https://hexiaoxiao-cs.github.io/DICE/</a>. This paper was accepted to CVPR 2025 but later desk-rejected post camera-ready, due to a withdrawal from ICLR made 14 days before reviewer assignment</p>
<p><strong>Summary</strong>ï¼š<br>ç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œé®è”½è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸Šå–å¾—äº†æˆåŠŸï¼Œä½†åœ¨å¯æ§å†…å®¹ç¼–è¾‘æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æˆ‘ä»¬æ¨å‡ºDICEï¼ˆç¦»æ•£å¯æ§ç¼–è¾‘çš„åå‘æ‰©æ•£æŠ€æœ¯ï¼‰ï¼Œå®ƒæ˜¯ä¸ºç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼ˆåŒ…æ‹¬å¤šé¡¹å¼æ‰©æ•£å’Œé®è”½ç”Ÿæˆæ¨¡å‹ï¼‰æä¾›ç²¾ç¡®åå‘çš„é¦–ä¸ªæ–¹æ³•ã€‚é€šè¿‡è®°å½•åå‘æ‰©æ•£è¿‡ç¨‹ä¸­çš„å™ªå£°åºåˆ—å’Œé®è”½æ¨¡å¼ï¼ŒDICEèƒ½å¤Ÿåœ¨ä¸éœ€è¦é¢„å…ˆå®šä¹‰çš„é®è”½æˆ–æ³¨æ„åŠ›æ“çºµçš„æƒ…å†µä¸‹ï¼Œå®ç°ç¦»æ•£æ•°æ®çš„ç²¾ç¡®é‡å»ºå’Œçµæ´»ç¼–è¾‘ã€‚æˆ‘ä»¬åœ¨å›¾åƒå’Œæ–‡æœ¬é¢†åŸŸéªŒè¯äº†DICEçš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨VQ-Diffusionã€Paellaå’ŒRoBERTaç­‰æ¨¡å‹ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒDICEåœ¨ä¿æŒé«˜æ•°æ®ä¿çœŸåº¦çš„åŒæ—¶ï¼Œæé«˜äº†ç¼–è¾‘èƒ½åŠ›ï¼Œä¸ºç¦»æ•£ç©ºé—´ä¸­çš„ç²¾ç»†å†…å®¹æ“ä½œæä¾›äº†æ–°çš„æœºä¼šã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>DICEï¼ˆDiscrete Inversion for Controllable Editingï¼‰ä¸ºç¦»æ•£æ‰©æ•£æ¨¡å‹æä¾›äº†ç²¾ç¡®åå‘çš„æ–¹æ³•ï¼Œå®ç°äº†å¯æ§å†…å®¹ç¼–è¾‘ã€‚</li>
<li>DICEæ”¯æŒå¤šé¡¹å¼æ‰©æ•£å’Œé®è”½ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>é€šè¿‡è®°å½•å™ªå£°åºåˆ—å’Œé®è”½æ¨¡å¼ï¼ŒDICEèƒ½å®ç°ç¦»æ•£æ•°æ®çš„ç²¾ç¡®é‡å»ºå’Œçµæ´»ç¼–è¾‘ã€‚</li>
<li>DICEæ— éœ€é¢„å…ˆå®šä¹‰çš„é®è”½æˆ–æ³¨æ„åŠ›æ“çºµã€‚</li>
<li>DICEåœ¨å›¾åƒå’Œæ–‡æœ¬é¢†åŸŸå‡æœ‰æ•ˆï¼Œå¹¶åœ¨å¤šç§æ¨¡å‹ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚</li>
<li>DICEåœ¨ä¿æŒé«˜æ•°æ®ä¿çœŸåº¦çš„åŒæ—¶ï¼Œæé«˜äº†ç¼–è¾‘èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.08207">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5dc03a27e1c1a64216b7c6016b5f1f3" align="middle">
<img src="https://picx.zhimg.com/v2-30e3955104ba5d0c4a59cfcd13fdd023" align="middle">
<img src="https://picx.zhimg.com/v2-4009eb3dba5cc5e1e148830078aa61ba" align="middle">
<img src="https://picx.zhimg.com/v2-d313dae859cf525241944aac9d0803ad" align="middle">
<img src="https://picx.zhimg.com/v2-f9fe1ba3e71fae3b165ece96b0c8fdd4" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8732f3f412c53512adc63b28ea5bb7de" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  From 2D to 3D Without Extra Baggage Data-Efficient Cancer Detection in Digital Breast Tomosynthesis
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8dcca5f3a09cad2c41089af9d662173d" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  Mip-NeWRF Enhanced Wireless Radiance Field with Hybrid Encoding for Channel Prediction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
