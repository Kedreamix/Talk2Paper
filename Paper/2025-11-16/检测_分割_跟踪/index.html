<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  DGFusion Dual-guided Fusion for Robust Multi-Modal 3D Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-cc750a82c323d20b480426c96b744d22')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-16-æ›´æ–°"><a href="#2025-11-16-æ›´æ–°" class="headerlink" title="2025-11-16 æ›´æ–°"></a>2025-11-16 æ›´æ–°</h1><h2 id="DGFusion-Dual-guided-Fusion-for-Robust-Multi-Modal-3D-Object-Detection"><a href="#DGFusion-Dual-guided-Fusion-for-Robust-Multi-Modal-3D-Object-Detection" class="headerlink" title="DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection"></a>DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection</h2><p><strong>Authors:Feiyang Jia, Caiyan Jia, Ailin Liu, Shaoqing Xu, Qiming Xia, Lin Liu, Lei Yang, Yan Gong, Ziying Song</strong></p>
<p>As a critical task in autonomous driving perception systems, 3D object detection is used to identify and track key objects, such as vehicles and pedestrians. However, detecting distant, small, or occluded objects (hard instances) remains a challenge, which directly compromises the safety of autonomous driving systems. We observe that existing multi-modal 3D object detection methods often follow a single-guided paradigm, failing to account for the differences in information density of hard instances between modalities. In this work, we propose DGFusion, based on the Dual-guided paradigm, which fully inherits the advantages of the Point-guide-Image paradigm and integrates the Image-guide-Point paradigm to address the limitations of the single paradigms. The core of DGFusion, the Difficulty-aware Instance Pair Matcher (DIPM), performs instance-level feature matching based on difficulty to generate easy and hard instance pairs, while the Dual-guided Modules exploit the advantages of both pair types to enable effective multi-modal feature fusion. Experimental results demonstrate that our DGFusion outperforms the baseline methods, with respective improvements of +1.0% mAP, +0.8% NDS, and +1.3% average recall on nuScenes. Extensive experiments demonstrate consistent robustness gains for hard instance detection across ego-distance, size, visibility, and small-scale training scenarios.</p>
<blockquote>
<p>åœ¨è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿä¸­ï¼Œä½œä¸ºå…³é”®ä»»åŠ¡ä¹‹ä¸€çš„3Dç‰©ä½“æ£€æµ‹ç”¨äºè¯†åˆ«å’Œè·Ÿè¸ªå…³é”®ç‰©ä½“ï¼Œå¦‚è½¦è¾†å’Œè¡Œäººã€‚ç„¶è€Œï¼Œæ£€æµ‹è¿œè·ç¦»ã€å°å‹æˆ–é®æŒ¡çš„ç‰©ä½“ï¼ˆç¡¬å®ä¾‹ï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™ç›´æ¥å½±å“äº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ç°æœ‰çš„å¤šæ¨¡æ€3Dç‰©ä½“æ£€æµ‹æ–¹æ³•é€šå¸¸é‡‡ç”¨å•ä¸€æŒ‡å¯¼èŒƒå¼ï¼Œæœªèƒ½è€ƒè™‘åˆ°ä¸åŒæ¨¡æ€ä¹‹é—´ç¡¬å®ä¾‹ä¿¡æ¯å¯†åº¦çš„å·®å¼‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºåŒé‡æŒ‡å¯¼èŒƒå¼çš„DGFusionï¼Œå®ƒç»§æ‰¿äº†Point-guide-ImageèŒƒå¼çš„ä¼˜åŠ¿ï¼Œå¹¶èåˆäº†Image-guide-PointèŒƒå¼æ¥è§£å†³å•ä¸€èŒƒå¼çš„å±€é™æ€§ã€‚DGFusionçš„æ ¸å¿ƒæ˜¯éš¾åº¦æ„ŸçŸ¥å®ä¾‹é…å¯¹åŒ¹é…å™¨ï¼ˆDIPMï¼‰ï¼Œå®ƒåŸºäºéš¾åº¦è¿›è¡Œå®ä¾‹çº§ç‰¹å¾åŒ¹é…ï¼Œä»¥ç”Ÿæˆå®¹æ˜“å’Œå›°éš¾çš„å®ä¾‹å¯¹ï¼Œè€ŒåŒé‡æŒ‡å¯¼æ¨¡å—åˆ™åˆ©ç”¨è¿™ä¸¤ç§ç±»å‹çš„å¯¹çš„ä¼˜åŠ¿æ¥å®ç°æœ‰æ•ˆçš„å¤šæ¨¡æ€ç‰¹å¾èåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DGFusionä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œåœ¨nuScenesä¸Šçš„mAPåˆ†åˆ«æé«˜äº†+1.0%ï¼ŒNDSæé«˜äº†+0.8%ï¼Œå¹³å‡å¬å›ç‡æé«˜äº†+1.3%ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨è‡ªæˆ‘è·ç¦»ã€å¤§å°ã€å¯è§æ€§å’Œå°è§„æ¨¡è®­ç»ƒåœºæ™¯ä¸‹ï¼Œç¡¬å®ä¾‹æ£€æµ‹çš„ç¨³å¥æ€§éƒ½æœ‰ä¸€è‡´çš„æ”¶ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10035v1">PDF</a> </p>
<p><strong>Summary</strong><br>3Då¯¹è±¡æ£€æµ‹æ˜¯è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿçš„å…³é”®ä»»åŠ¡ä¹‹ä¸€ï¼Œä¸»è¦ç”¨äºè¯†åˆ«å’Œè·Ÿè¸ªå…³é”®å¯¹è±¡ã€‚ç„¶è€Œï¼Œæ£€æµ‹è¿œå¤„ã€å°å‹æˆ–é®æŒ¡çš„ç‰©ä½“ï¼ˆç¡¬å®ä¾‹ï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç›´æ¥å½±å“è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚ç°æœ‰ç ”ç©¶å¤šé‡‡ç”¨å•ä¸€æ¨¡æ€æŒ‡å¯¼æ¨¡å¼çš„å¤šæ¨¡æ€ä¸‰ç»´å¯¹è±¡æ£€æµ‹æ–¹æ³•ï¼Œæ— æ³•å…¼é¡¾ä¸åŒæ¨¡æ€ä¸‹ç¡¬å®ä¾‹ä¿¡æ¯å¯†åº¦çš„å·®å¼‚ã€‚æœ¬ç ”ç©¶æå‡ºäº†åŸºäºåŒå¼•å¯¼æ¨¡å¼çš„DGFusionæ–¹æ³•ï¼Œç»“åˆäº†Point-guide-Imageå’ŒImage-guide-Pointä¸¤ç§æ¨¡å¼çš„ä¼˜ç‚¹ï¼Œå¹¶å¼•å…¥éš¾åº¦æ„ŸçŸ¥å®ä¾‹é…å¯¹åŒ¹é…å™¨ï¼ˆDIPMï¼‰å®ç°æœ‰æ•ˆçš„å¤šæ¨¡æ€ç‰¹å¾èåˆã€‚å®éªŒè¡¨æ˜ï¼ŒDGFusionåœ¨ç¡¬å®ä¾‹æ£€æµ‹æ–¹é¢å–å¾—äº†ä¼˜äºåŸºçº¿æ–¹æ³•çš„ç»“æœï¼Œåœ¨ä¸åŒåœºæ™¯ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§å¢ç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è‡ªåŠ¨é©¾é©¶ä¸­çš„ä¸‰ç»´å¯¹è±¡æ£€æµ‹å¯¹äºè¯†åˆ«å’Œè·Ÿè¸ªå…³é”®å¯¹è±¡è‡³å…³é‡è¦ã€‚</li>
<li>æ£€æµ‹è¿œå¤„ã€å°å‹æˆ–é®æŒ¡çš„ç‰©ä½“ï¼ˆç¡¬å®ä¾‹ï¼‰æ˜¯è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿçš„æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>ç›®å‰çš„å¤šæ¨¡æ€ä¸‰ç»´å¯¹è±¡æ£€æµ‹æ–¹æ³•å¤šé‡‡ç”¨å•ä¸€æ¨¡æ€æŒ‡å¯¼æ¨¡å¼ï¼Œå¿½ç•¥äº†ä¸åŒæ¨¡æ€ä¸‹ç¡¬å®ä¾‹ä¿¡æ¯å¯†åº¦çš„å·®å¼‚ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºçš„DGFusionæ–¹æ³•ç»“åˆäº†Point-guide-Imageå’ŒImage-guide-Pointä¸¤ç§æ¨¡å¼çš„ä¼˜ç‚¹ã€‚</li>
<li>DGFusionå¼•å…¥äº†éš¾åº¦æ„ŸçŸ¥å®ä¾‹é…å¯¹åŒ¹é…å™¨ï¼ˆDIPMï¼‰ï¼Œä»¥è¿›è¡Œå¤šæ¨¡æ€ç‰¹å¾èåˆå’Œæ›´å¥½çš„å®ä¾‹é…å¯¹ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºDGFusionåœ¨ç¡¬å®ä¾‹æ£€æµ‹æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶å±•ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§å¢ç›Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10035">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-81b8e2ae5ad2dbf23e3b58701f326476" align="middle">
<img src="https://picx.zhimg.com/v2-05e0a41f324c8a0c9396bf95585ffe9d" align="middle">
<img src="https://picx.zhimg.com/v2-a2db1409fee7323fc84775bbc5ea8a39" align="middle">
<img src="https://picx.zhimg.com/v2-ba6a9727a4a9508aaa89b74e5d73036d" align="middle">
<img src="https://picx.zhimg.com/v2-397e3620d0bab5efd2d4b95df83cc337" align="middle">
<img src="https://picx.zhimg.com/v2-ed9f7fb8999313df3054a65f5c2f7960" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DBGroup-Dual-Branch-Point-Grouping-for-Weakly-Supervised-3D-Instance-Segmentation"><a href="#DBGroup-Dual-Branch-Point-Grouping-for-Weakly-Supervised-3D-Instance-Segmentation" class="headerlink" title="DBGroup: Dual-Branch Point Grouping for Weakly Supervised 3D Instance Segmentation"></a>DBGroup: Dual-Branch Point Grouping for Weakly Supervised 3D Instance Segmentation</h2><p><strong>Authors:Xuexun Liu, Xiaoxu Xu, Qiudan Zhang, Lin Ma, Xu Wang</strong></p>
<p>Weakly supervised 3D instance segmentation is essential for 3D scene understanding, especially as the growing scale of data and high annotation costs associated with fully supervised approaches. Existing methods primarily rely on two forms of weak supervision: one-thing-one-click annotations and bounding box annotations, both of which aim to reduce labeling efforts. However, these approaches still encounter limitations, including labor-intensive annotation processes, high complexity, and reliance on expert annotators. To address these challenges, we propose \textbf{DBGroup}, a two-stage weakly supervised 3D instance segmentation framework that leverages scene-level annotations as a more efficient and scalable alternative. In the first stage, we introduce a Dual-Branch Point Grouping module to generate pseudo labels guided by semantic and mask cues extracted from multi-view images. To further improve label quality, we develop two refinement strategies: Granularity-Aware Instance Merging and Semantic Selection and Propagation. The second stage involves multi-round self-training on an end-to-end instance segmentation network using the refined pseudo-labels. Additionally, we introduce an Instance Mask Filter strategy to address inconsistencies within the pseudo labels. Extensive experiments demonstrate that DBGroup achieves competitive performance compared to sparse-point-level supervised 3D instance segmentation methods, while surpassing state-of-the-art scene-level supervised 3D semantic segmentation approaches. Code is available at <a target="_blank" rel="noopener" href="https://github.com/liuxuexun/DBGroup">https://github.com/liuxuexun/DBGroup</a>.</p>
<blockquote>
<p>å¼±ç›‘ç£3Då®ä¾‹åˆ†å‰²å¯¹äº3Dåœºæ™¯ç†è§£è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯éšç€æ•°æ®è§„æ¨¡çš„å¢é•¿å’Œå…¨ç›‘ç£æ–¹æ³•çš„é«˜æ ‡æ³¨æˆæœ¬ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºä¸¤ç§å½¢å¼çš„å¼±ç›‘ç£ï¼šä¸€ç‚¹ä¸€å‡»æ ‡æ³¨å’Œè¾¹ç•Œæ¡†æ ‡æ³¨ï¼Œä¸¤è€…çš„ç›®æ ‡éƒ½æ˜¯å‡å°‘æ ‡æ³¨å·¥ä½œé‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»é¢ä¸´å±€é™æ€§ï¼ŒåŒ…æ‹¬æ ‡æ³¨è¿‡ç¨‹åŠ³åŠ¨å¼ºåº¦é«˜ã€å¤æ‚æ€§é«˜ä»¥åŠä¾èµ–ä¸“å®¶æ ‡æ³¨äººå‘˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºDBGroupï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„å¼±ç›‘ç£3Då®ä¾‹åˆ†å‰²æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨åœºæ™¯çº§æ ‡æ³¨ä½œä¸ºæ›´é«˜æ•ˆå’Œå¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥åŒåˆ†æ”¯ç‚¹åˆ†ç»„æ¨¡å—ï¼Œæ ¹æ®ä»å¤šè§†è§’å›¾åƒä¸­æå–çš„è¯­ä¹‰å’Œæ©è†œçº¿ç´¢ç”Ÿæˆä¼ªæ ‡ç­¾ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ ‡ç­¾è´¨é‡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸¤ç§ä¼˜åŒ–ç­–ç•¥ï¼šç²’åº¦æ„ŸçŸ¥å®ä¾‹åˆå¹¶å’Œè¯­ä¹‰é€‰æ‹©ä¸ä¼ æ’­ã€‚ç¬¬äºŒé˜¶æ®µæ¶‰åŠåœ¨ç«¯åˆ°ç«¯å®ä¾‹åˆ†å‰²ç½‘ç»œä¸Šè¿›è¡Œå¤šè½®è‡ªè®­ç»ƒï¼Œä½¿ç”¨ç²¾ç‚¼çš„ä¼ªæ ‡ç­¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†å®ä¾‹æ©è†œè¿‡æ»¤ç­–ç•¥ï¼Œä»¥è§£å†³ä¼ªæ ‡ç­¾å†…éƒ¨çš„ä¸ä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDBGroupä¸ç¨€ç–ç‚¹çº§ç›‘ç£çš„3Då®ä¾‹åˆ†å‰²æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶è¶…è¶Šäº†æœ€å…ˆè¿›çš„åœºæ™¯çº§ç›‘ç£çš„3Dè¯­ä¹‰åˆ†å‰²æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lixuexun/DBGroup%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lixuexun/DBGroupæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10003v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDBGroupçš„ä¸¤é˜¶æ®µå¼±ç›‘ç£3Då®ä¾‹åˆ†å‰²æ¡†æ¶ï¼Œåˆ©ç”¨åœºæ™¯çº§æ³¨é‡Šä½œä¸ºæ›´é«˜æ•ˆå’Œå¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡åŒåˆ†æ”¯ç‚¹åˆ†ç»„æ¨¡å—ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œè¯¥æ¨¡å—ç”±å¤šè§†è§’å›¾åƒæå–çš„è¯­ä¹‰å’Œæ©è†œçº¿ç´¢å¼•å¯¼ã€‚ä¸ºæé«˜æ ‡ç­¾è´¨é‡ï¼Œé‡‡ç”¨ç²’åº¦æ„ŸçŸ¥å®ä¾‹åˆå¹¶å’Œè¯­ä¹‰é€‰æ‹©ä¸ä¼ æ’­ä¸¤ç§ä¼˜åŒ–ç­–ç•¥ã€‚ç¬¬äºŒé˜¶æ®µåˆ©ç”¨ä¼˜åŒ–åçš„ä¼ªæ ‡ç­¾è¿›è¡Œå¤šè½®è‡ªè®­ç»ƒï¼Œå®ç°å¯¹å®ä¾‹åˆ†å‰²ç½‘ç»œçš„ç«¯åˆ°ç«¯è®­ç»ƒã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†å®ä¾‹æ©è†œè¿‡æ»¤ç­–ç•¥æ¥è§£å†³ä¼ªæ ‡ç­¾å†…éƒ¨çš„ä¸ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒDBGroupä¸ç¨€ç–ç‚¹çº§ç›‘ç£çš„3Då®ä¾‹åˆ†å‰²æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶è¶…è¶Šäº†æœ€å…ˆè¿›çš„åœºæ™¯çº§ç›‘ç£çš„3Dè¯­ä¹‰åˆ†å‰²æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DBGroupæ˜¯ä¸€ç§ä¸¤é˜¶æ®µçš„å¼±ç›‘ç£3Då®ä¾‹åˆ†å‰²æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•é¢ä¸´çš„é«˜æ ‡æ³¨æˆæœ¬å’Œå¤æ‚æ€§æŒ‘æˆ˜ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µé€šè¿‡åŒåˆ†æ”¯ç‚¹åˆ†ç»„æ¨¡å—ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œåˆ©ç”¨è¯­ä¹‰å’Œæ©è†œçº¿ç´¢æé«˜æ ‡ç­¾è´¨é‡ã€‚</li>
<li>å¼•å…¥ä¸¤ç§ä¼˜åŒ–ç­–ç•¥â€”â€”ç²’åº¦æ„ŸçŸ¥å®ä¾‹åˆå¹¶å’Œè¯­ä¹‰é€‰æ‹©ä¸ä¼ æ’­â€”â€”ä»¥æé«˜ä¼ªæ ‡ç­¾è´¨é‡ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé‡‡ç”¨å¤šè½®è‡ªè®­ç»ƒï¼Œåˆ©ç”¨ä¼˜åŒ–åçš„ä¼ªæ ‡ç­¾è¿›è¡Œç«¯åˆ°ç«¯çš„å®ä¾‹åˆ†å‰²ç½‘ç»œè®­ç»ƒã€‚</li>
<li>æå‡ºå®ä¾‹æ©è†œè¿‡æ»¤ç­–ç•¥ä»¥è§£å†³ä¼ªæ ‡ç­¾å†…éƒ¨çš„ä¸ä¸€è‡´æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10003">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9101bcb815716f1147162e12456629a8" align="middle">
<img src="https://picx.zhimg.com/v2-8fe22f5c694db91e47e6482c00a41594" align="middle">
<img src="https://picx.zhimg.com/v2-55b6e0a665e17f98ae6d50dc7ffd903a" align="middle">
<img src="https://picx.zhimg.com/v2-e98321555d0a2dc5ac173799e8f41fe1" align="middle">
<img src="https://picx.zhimg.com/v2-6fd38483ec5ce490acf28f9c1251dddc" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Robust-Object-Detection-with-Pseudo-Labels-from-VLMs-using-Per-Object-Co-teaching"><a href="#Robust-Object-Detection-with-Pseudo-Labels-from-VLMs-using-Per-Object-Co-teaching" class="headerlink" title="Robust Object Detection with Pseudo Labels from VLMs using Per-Object Co-teaching"></a>Robust Object Detection with Pseudo Labels from VLMs using Per-Object Co-teaching</h2><p><strong>Authors:Uday Bhaskar, Rishabh Bhattacharya, Avinash Patel, Sarthak Khoche, Praveen Anil Kulkarni, Naresh Manwani</strong></p>
<p>Foundation models, especially vision-language models (VLMs), offer compelling zero-shot object detection for applications like autonomous driving, a domain where manual labelling is prohibitively expensive. However, their detection latency and tendency to hallucinate predictions render them unsuitable for direct deployment. This work introduces a novel pipeline that addresses this challenge by leveraging VLMs to automatically generate pseudo-labels for training efficient, real-time object detectors. Our key innovation is a per-object co-teaching-based training strategy that mitigates the inherent noise in VLM-generated labels. The proposed per-object coteaching approach filters noisy bounding boxes from training instead of filtering the entire image. Specifically, two YOLO models learn collaboratively, filtering out unreliable boxes from each mini-batch based on their peersâ€™ per-object loss values. Overall, our pipeline provides an efficient, robust, and scalable approach to train high-performance object detectors for autonomous driving, significantly reducing reliance on costly human annotation. Experimental results on the KITTI dataset demonstrate that our method outperforms a baseline YOLOv5m model, achieving a significant <a href="mailto:&#109;&#x41;&#x50;&#64;&#x30;&#46;&#53;">&#109;&#x41;&#x50;&#64;&#x30;&#46;&#53;</a> boost ($31.12%$ to $46.61%$) while maintaining real-time detection latency. Furthermore, we show that supplementing our pseudo-labelled data with a small fraction of ground truth labels ($10%$) leads to further performance gains, reaching $57.97%$ <a href="mailto:&#109;&#65;&#x50;&#x40;&#48;&#x2e;&#x35;">&#109;&#65;&#x50;&#x40;&#48;&#x2e;&#x35;</a> on the KITTI dataset. We observe similar performance improvements for the ACDC and BDD100k datasets.</p>
<blockquote>
<p>åŸºç¡€æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶ç­‰åº”ç”¨æä¾›äº†å¼•äººæ³¨ç›®çš„é›¶å°„å‡»ç›®æ ‡æ£€æµ‹åŠŸèƒ½ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‰‹åŠ¨æ ‡ç­¾æˆæœ¬è¿‡é«˜çš„é¢†åŸŸã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ£€æµ‹å»¶è¿Ÿå’Œå€¾å‘äºäº§ç”Ÿå¹»è§‰é¢„æµ‹ä½¿å…¶ä¸é€‚åˆç›´æ¥éƒ¨ç½²ã€‚è¿™é¡¹å·¥ä½œå¼•å…¥äº†ä¸€ç§æ–°ç®¡é“æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œåˆ©ç”¨VLMsè‡ªåŠ¨ç”Ÿæˆä¼ªæ ‡ç­¾æ¥è®­ç»ƒé«˜æ•ˆçš„å®æ—¶ç›®æ ‡æ£€æµ‹å™¨ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºåŸºäºæ¯ä¸ªå¯¹è±¡çš„ååŒæ•™å­¦è®­ç»ƒç­–ç•¥ï¼Œè¿™å‡è½»äº†VLMç”Ÿæˆçš„æ ‡ç­¾ä¸­çš„å›ºæœ‰å™ªå£°ã€‚æ‰€æå‡ºçš„é’ˆå¯¹æ¯ä¸ªå¯¹è±¡çš„ååŒæ•™å­¦æ–¹æ³•ä»è®­ç»ƒä¸­è¿‡æ»¤å‡ºå˜ˆæ‚çš„è¾¹ç•Œæ¡†ï¼Œè€Œä¸æ˜¯è¿‡æ»¤æ•´ä¸ªå›¾åƒã€‚å…·ä½“æ¥è¯´ï¼Œä¸¤ä¸ªYOLOæ¨¡å‹é€šè¿‡åä½œå­¦ä¹ ï¼ŒåŸºäºåŒè¡Œä¹‹é—´çš„æ¯ä¸ªå¯¹è±¡çš„æŸå¤±å€¼ï¼Œä»æ¯ä¸ªå°æ‰¹é‡æ•°æ®ä¸­ç­›é€‰å‡ºä¸å¯é çš„æ¡†ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç®¡é“æä¾›äº†ä¸€ç§é«˜æ•ˆã€ç¨³å¥å’Œå¯æ‰©å±•çš„æ–¹æ³•æ¥è®­ç»ƒç”¨äºè‡ªåŠ¨é©¾é©¶çš„é«˜æ€§èƒ½ç›®æ ‡æ£€æµ‹å™¨ï¼Œå¤§å¤§é™ä½äº†å¯¹æ˜‚è´µçš„äººåŠ›æ ‡æ³¨çš„ä¾èµ–ã€‚åœ¨KITTIæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºåŸºçº¿YOLOv5mæ¨¡å‹ï¼Œåœ¨ä¿æŒå®æ—¶æ£€æµ‹å»¶è¿Ÿçš„åŒæ—¶ï¼Œå®ç°äº†æ˜¾è‘—çš„<a href="mailto:&#x6d;&#x41;&#x50;&#64;&#48;&#x2e;&#x35;">&#x6d;&#x41;&#x50;&#64;&#48;&#x2e;&#x35;</a>æå‡ï¼ˆä»31.12%æé«˜åˆ°46.61%ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œç”¨ä¸€å°éƒ¨åˆ†çœŸå®æ ‡ç­¾ï¼ˆ10%ï¼‰è¡¥å……æˆ‘ä»¬çš„ä¼ªæ ‡ç­¾æ•°æ®å¯ä»¥å¸¦æ¥è¿›ä¸€æ­¥çš„æ€§èƒ½æå‡ï¼Œåœ¨KITTIæ•°æ®é›†ä¸Šè¾¾åˆ°57.97%çš„<a href="mailto:&#x6d;&#x41;&#x50;&#x40;&#x30;&#46;&#53;">&#x6d;&#x41;&#x50;&#x40;&#x30;&#46;&#53;</a>ã€‚æˆ‘ä»¬åœ¨ACDCå’ŒBDD100kæ•°æ®é›†ä¸Šä¹Ÿè§‚å¯Ÿåˆ°äº†ç±»ä¼¼çš„æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09955v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è‡ªåŠ¨ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œè®­ç»ƒé«˜æ•ˆå®æ—¶ç›®æ ‡æ£€æµ‹å™¨çš„æ–°é¢–æµç¨‹ã€‚é€šè¿‡é‡‡ç”¨åŸºäºç›®æ ‡ååŒæ•™å­¦çš„è®­ç»ƒç­–ç•¥ï¼Œå‡è½»VLMç”Ÿæˆæ ‡ç­¾çš„å›ºæœ‰å™ªå£°é—®é¢˜ã€‚è¯¥ç­–ç•¥èƒ½å¤Ÿè¿‡æ»¤æ‰è®­ç»ƒä¸­çš„ä¸å¯é è¾¹ç•Œæ¡†ï¼Œè€Œéè¿‡æ»¤æ•´ä¸ªå›¾åƒã€‚åœ¨KITTIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºåŸºçº¿YOLOv5mæ¨¡å‹æ˜¾è‘—æå‡<a href="mailto:&#109;&#x41;&#80;&#64;&#x30;&#46;&#x35;">&#109;&#x41;&#80;&#64;&#x30;&#46;&#x35;</a>æŒ‡æ ‡ï¼Œä»31.12%æå‡è‡³46.61%ï¼ŒåŒæ—¶ä¿æŒå®æ—¶æ£€æµ‹é€Ÿåº¦ã€‚é€šè¿‡è¡¥å……å°éƒ¨åˆ†çœŸå®æ ‡ç­¾æ•°æ®ï¼ˆä»…å 10%ï¼‰ï¼Œæ€§èƒ½å¯è¿›ä¸€æ­¥æå‡è‡³57.97%ã€‚è¯¥æµç¨‹ä¸ºè‡ªåŠ¨é©¾é©¶é¢†åŸŸé«˜æ•ˆã€ç¨³å¥ã€å¯æ‰©å±•çš„ç›®æ ‡æ£€æµ‹å™¨è®­ç»ƒæä¾›äº†æœ‰æ•ˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è‡ªåŠ¨ç”Ÿæˆä¼ªæ ‡ç­¾ä»¥è®­ç»ƒç›®æ ‡æ£€æµ‹å™¨ã€‚</li>
<li>æå‡ºåŸºäºç›®æ ‡ååŒæ•™å­¦çš„è®­ç»ƒç­–ç•¥ï¼Œè¿‡æ»¤ä¸å¯é çš„è¾¹ç•Œæ¡†ã€‚</li>
<li>åœ¨KITTIæ•°æ®é›†ä¸Šå®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œ<a href="mailto:&#x6d;&#x41;&#80;&#64;&#x30;&#x2e;&#53;">&#x6d;&#x41;&#80;&#64;&#x30;&#x2e;&#53;</a>ä»31.12%æå‡è‡³46.61%ã€‚</li>
<li>é€šè¿‡ç»“åˆå°‘é‡çœŸå®æ ‡ç­¾æ•°æ®ï¼Œæ€§èƒ½è¿›ä¸€æ­¥æå‡è‡³57.97%ã€‚</li>
<li>æ–¹æ³•å…·æœ‰é«˜æ•ˆæ€§ã€ç¨³å¥æ€§å’Œå¯æ‰©å±•æ€§ï¼Œé€‚ç”¨äºè‡ªåŠ¨é©¾é©¶é¢†åŸŸã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4382cfcca46dbcfe2814f7e55a58a4c0" align="middle">
<img src="https://picx.zhimg.com/v2-eebc154b44bff13352c812c766b4c117" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SAM-DAQ-Segment-Anything-Model-with-Depth-guided-Adaptive-Queries-for-RGB-D-Video-Salient-Object-Detection"><a href="#SAM-DAQ-Segment-Anything-Model-with-Depth-guided-Adaptive-Queries-for-RGB-D-Video-Salient-Object-Detection" class="headerlink" title="SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection"></a>SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection</h2><p><strong>Authors:Jia Lin, Xiaofei Zhou, Jiyuan Liu, Runmin Cong, Guodao Zhang, Zhi Liu, Jiyong Zhang</strong></p>
<p>Recently segment anything model (SAM) has attracted widespread concerns, and it is often treated as a vision foundation model for universal segmentation. Some researchers have attempted to directly apply the foundation model to the RGB-D video salient object detection (RGB-D VSOD) task, which often encounters three challenges, including the dependence on manual prompts, the high memory consumption of sequential adapters, and the computational burden of memory attention. To address the limitations, we propose a novel method, namely Segment Anything Model with Depth-guided Adaptive Queries (SAM-DAQ), which adapts SAM2 to pop-out salient objects from videos by seamlessly integrating depth and temporal cues within a unified framework. Firstly, we deploy a parallel adapter-based multi-modal image encoder (PAMIE), which incorporates several depth-guided parallel adapters (DPAs) in a skip-connection way. Remarkably, we fine-tune the frozen SAM encoder under prompt-free conditions, where the DPA utilizes depth cues to facilitate the fusion of multi-modal features. Secondly, we deploy a query-driven temporal memory (QTM) module, which unifies the memory bank and prompt embeddings into a learnable pipeline. Concretely, by leveraging both frame-level queries and video-level queries simultaneously, the QTM module can not only selectively extract temporal consistency features but also iteratively update the temporal representations of the queries. Extensive experiments are conducted on three RGB-D VSOD datasets, and the results show that the proposed SAM-DAQ consistently outperforms state-of-the-art methods in terms of all evaluation metrics.</p>
<blockquote>
<p>æœ€è¿‘ï¼Œä»»ä½•å†…å®¹åˆ†å‰²æ¨¡å‹ï¼ˆSAMï¼‰å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œé€šå¸¸è¢«è§†ä¸ºé€šç”¨åˆ†å‰²çš„è§†è§‰åŸºç¡€æ¨¡å‹ã€‚ä¸€äº›ç ”ç©¶äººå‘˜å°è¯•ç›´æ¥å°†åŸºç¡€æ¨¡å‹åº”ç”¨äºRGB-Dè§†é¢‘æ˜¾è‘—ç›®æ ‡æ£€æµ‹ï¼ˆRGB-D VSODï¼‰ä»»åŠ¡ï¼Œè¿™å¸¸å¸¸é¢ä¸´ä¸‰ä¸ªæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¾èµ–æ‰‹åŠ¨æç¤ºã€é¡ºåºé€‚é…å™¨çš„é«˜å†…å­˜æ¶ˆè€—ï¼Œä»¥åŠå†…å­˜æ³¨æ„åŠ›çš„è®¡ç®—è´Ÿæ‹…ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå³æ·±åº¦å¼•å¯¼è‡ªé€‚åº”æŸ¥è¯¢çš„ä»»æ„å†…å®¹åˆ†å‰²æ¨¡å‹ï¼ˆSAM-DAQï¼‰ï¼Œå®ƒå°†SAM2é€‚åº”äºä»è§†é¢‘ä¸­å¼¹å‡ºæ˜¾è‘—ç›®æ ‡ï¼Œé€šè¿‡åœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶å†…æ— ç¼é›†æˆæ·±åº¦å’Œæ—¶æ€çº¿ç´¢ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éƒ¨ç½²äº†åŸºäºå¹¶è¡Œé€‚é…å™¨çš„å¤šæ¨¡æ€å›¾åƒç¼–ç å™¨ï¼ˆPAMIEï¼‰ï¼Œå®ƒä»¥è·³è¿‡è¿æ¥çš„æ–¹å¼ç»“åˆäº†å¤šä¸ªæ·±åº¦å¼•å¯¼å¹¶è¡Œé€‚é…å™¨ï¼ˆDPAï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨æ— æç¤ºæ¡ä»¶ä¸‹å¯¹å†»ç»“çš„SAMç¼–ç å™¨è¿›è¡Œäº†å¾®è°ƒï¼ŒDPAåˆ©ç”¨æ·±åº¦çº¿ç´¢æ¥ä¿ƒè¿›å¤šæ¨¡æ€ç‰¹å¾çš„èåˆã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬éƒ¨ç½²äº†ä¸€ä¸ªæŸ¥è¯¢é©±åŠ¨çš„æ—¶æ€å†…å­˜ï¼ˆQTMï¼‰æ¨¡å—ï¼Œå®ƒå°†å†…å­˜é“¶è¡Œå’Œæç¤ºåµŒå…¥åˆ°ä¸€ä¸ªå¯å­¦ä¹ çš„æµç¨‹ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡åŒæ—¶åˆ©ç”¨å¸§çº§æŸ¥è¯¢å’Œè§†é¢‘çº§æŸ¥è¯¢ï¼ŒQTMæ¨¡å—ä¸ä»…å¯ä»¥æœ‰é€‰æ‹©åœ°æå–æ—¶æ€ä¸€è‡´æ€§ç‰¹å¾ï¼Œè¿˜å¯ä»¥è¿­ä»£æ›´æ–°æŸ¥è¯¢çš„æ—¶æ€è¡¨ç¤ºã€‚åœ¨ä¸‰ä¸ªRGB-D VSODæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„SAM-DAQåœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09870v1">PDF</a> Accepted to 40th AAAI Conference on Artificial Intelligence (AAAI 2026)</p>
<p><strong>Summary</strong></p>
<p>æœ€è¿‘ï¼Œåˆ†æ®µä»»ä½•äº‹ç‰©æ¨¡å‹ï¼ˆSAMï¼‰å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œè¢«è§†ä¸ºé€šç”¨åˆ†å‰²çš„è§†è§‰åŸºç¡€æ¨¡å‹ã€‚ç ”ç©¶è€…å°è¯•å°†å…¶ç›´æ¥åº”ç”¨äºRGB-Dè§†é¢‘æ˜¾è‘—ç›®æ ‡æ£€æµ‹ä»»åŠ¡ï¼Œä½†é¢ä¸´ä¾èµ–æ‰‹åŠ¨æç¤ºã€åºè´¯é€‚é…å™¨é«˜å†…å­˜æ¶ˆè€—å’Œå†…å­˜æ³¨æ„åŠ›è®¡ç®—è´Ÿæ‹…ç­‰ä¸‰å¤§æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›å±€é™ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§åä¸ºSAM-DAQçš„æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡æ·±åº¦å¼•å¯¼è‡ªé€‚åº”æŸ¥è¯¢å°†SAM2é€‚åº”äºè§†é¢‘ä¸­çš„æ˜¾è‘—ç›®æ ‡æ£€æµ‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éƒ¨ç½²å¹¶è¡Œé€‚é…å™¨å¤šæ¨¡æ€å›¾åƒç¼–ç å™¨ï¼ˆPAMIEï¼‰ï¼Œä»¥è·³è¿‡è¿æ¥çš„æ–¹å¼èå…¥å¤šä¸ªæ·±åº¦å¼•å¯¼å¹¶è¡Œé€‚é…å™¨ï¼ˆDPAsï¼‰ã€‚å€¼å¾—æ³¨æ„æ˜¯ï¼Œæˆ‘ä»¬åœ¨æ— æç¤ºæ¡ä»¶ä¸‹å¾®è°ƒäº†å†»ç»“çš„SAMç¼–ç å™¨ï¼ŒDPAåˆ©ç”¨æ·±åº¦çº¿ç´¢ä¿ƒè¿›å¤šæ¨¡æ€ç‰¹å¾çš„èåˆã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥æŸ¥è¯¢é©±åŠ¨å¼ä¸´æ—¶å†…å­˜æ¨¡å—ï¼ˆQTMï¼‰ï¼Œå°†å†…å­˜é“¶è¡Œå’Œæç¤ºåµŒå…¥åˆ°å¯å­¦ä¹ ç®¡é“ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡åŒæ—¶åˆ©ç”¨å¸§çº§æŸ¥è¯¢å’Œè§†é¢‘çº§æŸ¥è¯¢ï¼ŒQTMæ¨¡å—ä¸ä»…å¯ä»¥é€‰æ‹©æ€§æå–æ—¶é—´ä¸€è‡´æ€§ç‰¹å¾ï¼Œè¿˜å¯ä»¥è¿­ä»£æ›´æ–°æŸ¥è¯¢çš„æ—¶é—´è¡¨ç¤ºã€‚åœ¨ä¸‰ä¸ªRGB-D VSODæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„SAM-DAQåœ¨æ‰€æœ‰è¯„ä»·æŒ‡æ ‡ä¸Šå‡ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAMæ¨¡å‹ä½œä¸ºé€šç”¨åˆ†å‰²çš„è§†è§‰åŸºç¡€æ¨¡å‹å—åˆ°å…³æ³¨ã€‚</li>
<li>ç›´æ¥å°†SAMåº”ç”¨äºRGB-Dè§†é¢‘æ˜¾è‘—ç›®æ ‡æ£€æµ‹ä»»åŠ¡é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºSAM-DAQæ–¹æ³•ï¼Œé€šè¿‡æ·±åº¦å¼•å¯¼è‡ªé€‚åº”æŸ¥è¯¢é€‚åº”SAMæ¨¡å‹ä»¥æ£€æµ‹è§†é¢‘ä¸­çš„æ˜¾è‘—ç›®æ ‡ã€‚</li>
<li>PAMIEå’ŒDPAsçš„å¼•å…¥ä¿ƒè¿›äº†å¤šæ¨¡æ€ç‰¹å¾çš„èåˆã€‚</li>
<li>QTMæ¨¡å—èƒ½å¤Ÿæå–æ—¶é—´ä¸€è‡´æ€§ç‰¹å¾å¹¶æ›´æ–°æŸ¥è¯¢çš„æ—¶é—´è¡¨ç¤ºã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSAM-DAQåœ¨å„é¡¹è¯„ä»·æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09870">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-082fdfb931e26bbdb1cf1aa71adec540" align="middle">
<img src="https://picx.zhimg.com/v2-c9687ca61fd04b3fcbbdb6614e962207" align="middle">
<img src="https://picx.zhimg.com/v2-ce302fe205adad6e36de5b9b0e41094f" align="middle">
<img src="https://picx.zhimg.com/v2-f1e988a0a629f0517b78aeffcf0d999a" align="middle">
<img src="https://picx.zhimg.com/v2-2ba4a85abee54fc4dadc4680066f6fe5" align="middle">
<img src="https://picx.zhimg.com/v2-95f8b44af860493efc12da869855928a" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="High-Quality-Proposal-Encoding-and-Cascade-Denoising-for-Imaginary-Supervised-Object-Detection"><a href="#High-Quality-Proposal-Encoding-and-Cascade-Denoising-for-Imaginary-Supervised-Object-Detection" class="headerlink" title="High-Quality Proposal Encoding and Cascade Denoising for Imaginary Supervised Object Detection"></a>High-Quality Proposal Encoding and Cascade Denoising for Imaginary Supervised Object Detection</h2><p><strong>Authors:Zhiyuan Chen, Yuelin Guo, Zitong Huang, Haoyu He, Renhao Lu, Weizhe Zhang</strong></p>
<p>Object detection models demand large-scale annotated datasets, which are costly and labor-intensive to create. This motivated Imaginary Supervised Object Detection (ISOD), where models train on synthetic images and test on real images. However, existing methods face three limitations: (1) synthetic datasets suffer from simplistic prompts, poor image quality, and weak supervision; (2) DETR-based detectors, due to their random query initialization, struggle with slow convergence and overfitting to synthetic patterns, hindering real-world generalization; (3) uniform denoising pressure promotes model overfitting to pseudo-label noise. We propose Cascade HQP-DETR to address these limitations. First, we introduce a high-quality data pipeline using LLaMA-3, Flux, and Grounding DINO to generate the FluxVOC and FluxCOCO datasets, advancing ISOD from weak to full supervision. Second, our High-Quality Proposal guided query encoding initializes object queries with image-specific priors from SAM-generated proposals and RoI-pooled features, accelerating convergence while steering the model to learn transferable features instead of overfitting to synthetic patterns. Third, our cascade denoising algorithm dynamically adjusts training weights through progressively increasing IoU thresholds across decoder layers, guiding the model to learn robust boundaries from reliable visual cues rather than overfitting to noisy labels. Trained for just 12 epochs solely on FluxVOC, Cascade HQP-DETR achieves a SOTA 61.04% <a href="mailto:&#x6d;&#x41;&#x50;&#x40;&#48;&#x2e;&#x35;">&#x6d;&#x41;&#x50;&#x40;&#48;&#x2e;&#x35;</a> on PASCAL VOC 2007, outperforming strong baselines, with its competitive real-data performance confirming the architectureâ€™s universal applicability.</p>
<blockquote>
<p>å¯¹è±¡æ£€æµ‹æ¨¡å‹éœ€è¦å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†ï¼Œè€Œè¿™äº›æ•°æ®é›†çš„åˆ›å»ºæˆæœ¬é«˜æ˜‚ä¸”åŠ³åŠ¨å¯†é›†å‹ã€‚è¿™æ¿€å‘äº†Imaginary Supervised Object Detectionï¼ˆISODï¼‰çš„å‡ºç°ï¼Œæ¨¡å‹åœ¨åˆæˆå›¾åƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨çœŸå®å›¾åƒä¸Šè¿›è¡Œæµ‹è¯•ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é¢ä¸´ä¸‰ä¸ªå±€é™æ€§ï¼šï¼ˆ1ï¼‰åˆæˆæ•°æ®é›†å­˜åœ¨æç¤ºè¿‡äºç®€å•ã€å›¾åƒè´¨é‡å·®å’Œç›‘ç£å¼±çš„é—®é¢˜ï¼›ï¼ˆ2ï¼‰åŸºäºDETRçš„æ¢æµ‹å™¨ç”±äºå…¶éšæœºæŸ¥è¯¢åˆå§‹åŒ–ï¼Œå­˜åœ¨æ”¶æ•›æ…¢å’Œè¿‡åº¦æ‹Ÿåˆåˆæˆæ¨¡å¼çš„é—®é¢˜ï¼Œé˜»ç¢äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„æ³›åŒ–èƒ½åŠ›ï¼›ï¼ˆ3ï¼‰ç»Ÿä¸€çš„å»å™ªå‹åŠ›å¯¼è‡´æ¨¡å‹å¯¹ä¼ªæ ‡ç­¾å™ªå£°è¿‡åº¦æ‹Ÿåˆã€‚æˆ‘ä»¬æå‡ºCascade HQP-DETRæ¥è§£å†³è¿™äº›å±€é™æ€§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨LLaMA-3ã€Fluxå’ŒGrounding DINOå¼•å…¥é«˜è´¨é‡æ•°æ®ç®¡é“ï¼Œç”ŸæˆFluxVOCå’ŒFluxCOCOæ•°æ®é›†ï¼Œå°†ISODä»å¼±ç›‘ç£æ¨è¿›åˆ°å…¨ç›‘ç£ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬çš„é«˜è´¨é‡ææ¡ˆå¼•å¯¼æŸ¥è¯¢ç¼–ç ä½¿ç”¨SAMç”Ÿæˆçš„ææ¡ˆå’ŒRoIæ± åŒ–ç‰¹å¾å¯¹å›¾åƒç‰¹å®šå…ˆéªŒè¿›è¡Œå¯¹è±¡æŸ¥è¯¢åˆå§‹åŒ–ï¼ŒåŠ é€Ÿæ”¶æ•›ï¼Œå¼•å¯¼æ¨¡å‹å­¦ä¹ å¯è¿ç§»ç‰¹å¾ï¼Œè€Œä¸æ˜¯è¿‡åº¦æ‹Ÿåˆåˆæˆæ¨¡å¼ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬çš„çº§è”å»å™ªç®—æ³•é€šè¿‡é€å±‚å¢åŠ IoUé˜ˆå€¼åŠ¨æ€è°ƒæ•´è®­ç»ƒæƒé‡ï¼Œå¼•å¯¼æ¨¡å‹ä»å¯é è§†è§‰çº¿ç´¢ä¸­å­¦ä¹ ç¨³å¥è¾¹ç•Œï¼Œè€Œä¸æ˜¯è¿‡åº¦ä¾èµ–å˜ˆæ‚æ ‡ç­¾ã€‚ä»…åœ¨FluxVOCä¸Šè®­ç»ƒ12ä¸ªå‘¨æœŸï¼ŒCascade HQP-DETRå°±å®ç°äº†PASCAL VOC 2007ä¸Šçš„61.04ï¼…<a href="mailto:&#x6d;&#65;&#80;&#x40;&#48;&#x2e;&#x35;">&#x6d;&#65;&#80;&#x40;&#48;&#x2e;&#x35;</a>çš„é¢†å…ˆæ°´å¹³ï¼Œè¶…è¶Šäº†å¼ºå¤§çš„åŸºçº¿ï¼Œå…¶æœ‰ç«äº‰åŠ›çš„çœŸå®æ•°æ®æ€§èƒ½è¯å®äº†è¯¥æ¶æ„çš„é€šç”¨é€‚ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08018v1">PDF</a> This work has been submitted to Pattern Recognition for possible publication</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ç°æœ‰å¯¹è±¡æ£€æµ‹æ¨¡å‹åœ¨åˆæˆå›¾åƒè®­ç»ƒæ—¶é¢ä¸´çš„ä¸‰å¤§é—®é¢˜ï¼Œæå‡ºäº†Cascade HQP-DETRæ¨¡å‹ã€‚é€šè¿‡å¼•å…¥é«˜è´¨é‡æ•°æ®ç®¡é“ã€é«˜è´¨é‡æè®®å¼•å¯¼æŸ¥è¯¢ç¼–ç ä»¥åŠçº§è”å»å™ªç®—æ³•ï¼Œè§£å†³äº†åˆæˆå›¾åƒè®­ç»ƒä¸­çš„å¼±ç›‘ç£ã€æ”¶æ•›æ…¢å’Œè¿‡æ‹Ÿåˆç­‰é—®é¢˜ã€‚åœ¨PASCAL VOC 2007æ•°æ®é›†ä¸Šå–å¾—äº†è¾ƒé«˜çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Cascade HQP-DETRè§£å†³äº†ç°æœ‰å¯¹è±¡æ£€æµ‹æ¨¡å‹åœ¨åˆæˆå›¾åƒè®­ç»ƒæ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¼±ç›‘ç£ã€æ”¶æ•›é€Ÿåº¦æ…¢å’Œè¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®ç®¡é“ï¼Œåˆ©ç”¨LLaMA-3ã€Fluxå’ŒGrounding DINOç­‰æŠ€æœ¯ç”ŸæˆFluxVOCå’ŒFluxCOCOæ•°æ®é›†ï¼Œå®ç°ISODä»å¼±ç›‘ç£åˆ°å…¨ç›‘ç£çš„è¿›å±•ã€‚</li>
<li>é€šè¿‡é«˜è´¨é‡æè®®å¼•å¯¼æŸ¥è¯¢ç¼–ç ï¼Œä½¿ç”¨SAMç”Ÿæˆçš„æè®®å’ŒRoIæ± åŒ–ç‰¹å¾æ¥åˆå§‹åŒ–å¯¹è±¡æŸ¥è¯¢ï¼Œæé«˜äº†æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦å¹¶ä¿ƒè¿›äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>çº§è”å»å™ªç®—æ³•å¯ä»¥æ ¹æ®è§£ç å™¨å±‚çš„IoUé˜ˆå€¼åŠ¨æ€è°ƒæ•´è®­ç»ƒæƒé‡ï¼Œå¸®åŠ©æ¨¡å‹ä»å¯é çš„è§†è§‰çº¿ç´¢ä¸­å­¦ä¹ é²æ£’çš„è¾¹ç•Œä¿¡æ¯ï¼Œå‡å°‘äº†å¯¹å™ªå£°æ ‡ç­¾çš„è¿‡æ‹Ÿåˆã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08018">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-062f9761bd080cdad95975e8ebcf211e" align="middle">
<img src="https://picx.zhimg.com/v2-f1337f8eb1a12970d8383411fa604245" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Multi-Modal-Assistance-for-Unsupervised-Domain-Adaptation-on-Point-Cloud-3D-Object-Detection"><a href="#Multi-Modal-Assistance-for-Unsupervised-Domain-Adaptation-on-Point-Cloud-3D-Object-Detection" class="headerlink" title="Multi-Modal Assistance for Unsupervised Domain Adaptation on Point Cloud 3D Object Detection"></a>Multi-Modal Assistance for Unsupervised Domain Adaptation on Point Cloud 3D Object Detection</h2><p><strong>Authors:Shenao Zhao, Pengpeng Liang, Zhoufan Yang</strong></p>
<p>Unsupervised domain adaptation for LiDAR-based 3D object detection (3D UDA) based on the teacher-student architecture with pseudo labels has achieved notable improvements in recent years. Although it is quite popular to collect point clouds and images simultaneously, little attention has been paid to the usefulness of image data in 3D UDA when training the models. In this paper, we propose an approach named MMAssist that improves the performance of 3D UDA with multi-modal assistance. A method is designed to align 3D features between the source domain and the target domain by using image and text features as bridges. More specifically, we project the ground truth labels or pseudo labels to the images to get a set of 2D bounding boxes. For each 2D box, we extract its image feature from a pre-trained vision backbone. A large vision-language model (LVLM) is adopted to extract the boxâ€™s text description, and a pre-trained text encoder is used to obtain its text feature. During the training of the model in the source domain and the student model in the target domain, we align the 3D features of the predicted boxes with their corresponding image and text features, and the 3D features and the aligned features are fused with learned weights for the final prediction. The features between the student branch and the teacher branch in the target domain are aligned as well. To enhance the pseudo labels, we use an off-the-shelf 2D object detector to generate 2D bounding boxes from images and estimate their corresponding 3D boxes with the aid of point cloud, and these 3D boxes are combined with the pseudo labels generated by the teacher model. Experimental results show that our approach achieves promising performance compared with state-of-the-art methods in three domain adaptation tasks on three popular 3D object detection datasets. The code is available at <a target="_blank" rel="noopener" href="https://github.com/liangp/MMAssist">https://github.com/liangp/MMAssist</a>.</p>
<blockquote>
<p>åŸºäºå¸¦æœ‰ä¼ªæ ‡ç­¾çš„æ•™å¸ˆ-å­¦ç”Ÿæ¶æ„çš„æ¿€å…‰é›·è¾¾ï¼ˆLiDARï¼‰çš„åŸºäºåŸŸé€‚åº”çš„3Dç›®æ ‡æ£€æµ‹ï¼ˆä¹Ÿç§°ä¸ºåŸºäºå›¾åƒä¸æ–‡å­—ç‰¹å¾çš„è¾…åŠ©å­¦ä¹ LiDAR 3Dç›®æ ‡æ£€æµ‹ï¼‰åœ¨è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚è™½ç„¶åŒæ—¶æ”¶é›†ç‚¹äº‘å’Œå›¾åƒæ˜¯éå¸¸æµè¡Œçš„åšæ³•ï¼Œä½†åœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œå¯¹äºåˆ©ç”¨å›¾åƒæ•°æ®åœ¨æ— äººç›‘ç£é¢†åŸŸé€‚åº”ä¸­ï¼ˆä¹Ÿå³ä¸‰ç»´UDAï¼‰çš„å®é™…ç”¨å¤„å´æ²¡æœ‰å¼•èµ·è¶³å¤Ÿçš„å…³æ³¨ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMMAssistçš„æ–¹æ³•ï¼Œé€šè¿‡å¤šæ¨¡æ€è¾…åŠ©æé«˜äº†ä¸‰ç»´UDAçš„æ€§èƒ½ã€‚è®¾è®¡äº†ä¸€ç§æ–¹æ³•ï¼Œåˆ©ç”¨å›¾åƒå’Œæ–‡å­—ç‰¹å¾ä½œä¸ºæ¡¥æ¢ï¼Œå¯¹é½æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´çš„ä¸‰ç»´ç‰¹å¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†çœŸå®æ ‡ç­¾æˆ–ä¼ªæ ‡ç­¾æŠ•å°„åˆ°å›¾åƒä¸Šå¾—åˆ°ä¸€ç»„äºŒç»´è¾¹ç•Œæ¡†ã€‚å¯¹äºæ¯ä¸ªäºŒç»´æ¡†ï¼Œæˆ‘ä»¬ä»é¢„è®­ç»ƒçš„è§†è§‰éª¨å¹²ç½‘ç»œä¸­æå–å…¶å›¾åƒç‰¹å¾ã€‚ä½¿ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰æ¥æå–æ¡†çš„æ–‡æœ¬æè¿°ï¼Œå¹¶ä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨è·å–å…¶æ–‡æœ¬ç‰¹å¾ã€‚åœ¨æºåŸŸè®­ç»ƒæ¨¡å‹å’Œè®­ç»ƒç›®æ ‡åŸŸçš„å­¦ç”Ÿæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å°†é¢„æµ‹æ¡†çš„ä¸‰ç»´ç‰¹å¾ä¸å…¶å¯¹åº”çš„å›¾åƒå’Œæ–‡å­—ç‰¹å¾å¯¹é½ï¼Œå¹¶å°†ä¸‰ç»´ç‰¹å¾å’Œå·²å¯¹é½çš„ç‰¹å¾èåˆç”¨äºæœ€ç»ˆé¢„æµ‹ã€‚åŒæ—¶ï¼Œç›®æ ‡åŸŸä¸­çš„å­¦ç”Ÿåˆ†æ”¯å’Œæ•™å¸ˆåˆ†æ”¯ä¹‹é—´çš„ç‰¹å¾ä¹Ÿè¢«å¯¹é½ã€‚ä¸ºäº†å¢å¼ºä¼ªæ ‡ç­¾ï¼Œæˆ‘ä»¬ä½¿ç”¨ç°æˆçš„äºŒç»´ç›®æ ‡æ£€æµ‹å™¨ä»å›¾åƒç”ŸæˆäºŒç»´è¾¹ç•Œæ¡†ï¼Œå¹¶åˆ©ç”¨ç‚¹äº‘ä¼°è®¡å…¶å¯¹åº”çš„ä¸‰ç»´æ¡†ï¼Œå¹¶å°†è¿™äº›ä¸‰ç»´æ¡†ä¸ç”±æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„ä¼ªæ ‡ç­¾ç»“åˆä½¿ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„ç®—æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ä¸ªé¢†åŸŸé€‚åº”ä»»åŠ¡ä¸­çš„ä¸‰ä¸ªæµè¡Œçš„ä¸‰ç»´ç›®æ ‡æ£€æµ‹æ•°æ®é›†ä¸Šå–å¾—äº†æœ‰å‰æ™¯çš„è¡¨ç°ã€‚ç›¸å…³ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/liangp/MMAssist%E8%8E%B7%E5%BE%97">https://github.com/liangp/MMAssistè·å¾—</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07966v1">PDF</a> Accepted to AAAI-26</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMMAssistçš„æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›åŸºäºæ¿€å…‰é›·è¾¾çš„3Dç›®æ ‡æ£€æµ‹çš„åŸŸè‡ªé€‚åº”æ€§èƒ½ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ä½œä¸ºæ¡¥æ¢ï¼Œé€šè¿‡å¤šæ¨¡æ€è¾…åŠ©å®ç°å¯¹æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´çš„3Dç‰¹å¾å¯¹é½ã€‚é€šè¿‡ç»“åˆå›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ï¼Œæé«˜ä¼ªæ ‡ç­¾è´¨é‡ï¼Œå¹¶åœ¨ä¸‰ä¸ªæµè¡Œçš„3Dç›®æ ‡æ£€æµ‹æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒéªŒè¯ï¼Œå–å¾—æ˜¾è‘—æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æå‡ºäº†åä¸ºMMAssistçš„æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›åŸºäºæ¿€å…‰é›·è¾¾çš„3Dç›®æ ‡æ£€æµ‹çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰ã€‚</li>
<li>åˆ©ç”¨å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ä½œä¸ºæ¡¥æ¢ï¼Œå®ç°æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´çš„3Dç‰¹å¾å¯¹é½ã€‚</li>
<li>é€šè¿‡ç»“åˆå›¾åƒå’Œæ–‡æœ¬ç‰¹å¾ï¼Œæé«˜ä¼ªæ ‡ç­¾è´¨é‡ã€‚</li>
<li>é‡‡ç”¨å¤šæ¨¡æ€è¾…åŠ©æé«˜ç›®æ ‡æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>åœ¨ä¸‰ä¸ªæµè¡Œçš„æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒéªŒè¯ï¼ŒåŒ…æ‹¬ä¸åŒé¢†åŸŸé€‚åº”æ€§ä»»åŠ¡ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ç›®å‰ä¸»æµæ–¹æ³•ç›¸æ¯”è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6ccc7e780705e2dba495658985be73f" align="middle">
<img src="https://picx.zhimg.com/v2-cfadf32972778eaf8e0c03f4f79a819a" align="middle">
<img src="https://picx.zhimg.com/v2-fd0aba123f4f03379c2e338231377bbf" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MonoCLUE-Object-Aware-Clustering-Enhances-Monocular-3D-Object-Detection"><a href="#MonoCLUE-Object-Aware-Clustering-Enhances-Monocular-3D-Object-Detection" class="headerlink" title="MonoCLUE : Object-Aware Clustering Enhances Monocular 3D Object Detection"></a>MonoCLUE : Object-Aware Clustering Enhances Monocular 3D Object Detection</h2><p><strong>Authors:Sunghun Yang, Minhyeok Lee, Jungho Lee, Sangyoun Lee</strong></p>
<p>Monocular 3D object detection offers a cost-effective solution for autonomous driving but suffers from ill-posed depth and limited field of view. These constraints cause a lack of geometric cues and reduced accuracy in occluded or truncated scenes. While recent approaches incorporate additional depth information to address geometric ambiguity, they overlook the visual cues crucial for robust recognition. We propose MonoCLUE, which enhances monocular 3D detection by leveraging both local clustering and generalized scene memory of visual features. First, we perform K-means clustering on visual features to capture distinct object-level appearance parts (e.g., bonnet, car roof), improving detection of partially visible objects. The clustered features are propagated across regions to capture objects with similar appearances. Second, we construct a generalized scene memory by aggregating clustered features across images, providing consistent representations that generalize across scenes. This improves object-level feature consistency, enabling stable detection across varying environments. Lastly, we integrate both local cluster features and generalized scene memory into object queries, guiding attention toward informative regions. Exploiting a unified local clustering and generalized scene memory strategy, MonoCLUE enables robust monocular 3D detection under occlusion and limited visibility, achieving state-of-the-art performance on the KITTI benchmark.</p>
<blockquote>
<p>å•ç›®3Dç›®æ ‡æ£€æµ‹ä¸ºè‡ªåŠ¨é©¾é©¶æä¾›äº†ç»æµé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä½†å­˜åœ¨æ·±åº¦ä¸æ˜ç¡®å’Œè§†é‡æœ‰é™çš„ç¼ºç‚¹ã€‚è¿™äº›çº¦æŸå¯¼è‡´äº†ç¼ºä¹å‡ ä½•çº¿ç´¢ï¼Œä»¥åŠåœ¨é®æŒ¡æˆ–æˆªæ–­åœºæ™¯ä¸­çš„ç²¾åº¦é™ä½ã€‚è™½ç„¶æœ€è¿‘çš„æ–¹æ³•é€šè¿‡åŠ å…¥æ·±åº¦ä¿¡æ¯æ¥è§£å†³å‡ ä½•æ­§ä¹‰é—®é¢˜ï¼Œä½†å®ƒä»¬å¿½ç•¥äº†å¯¹äºç¨³å¥è¯†åˆ«è‡³å…³é‡è¦çš„è§†è§‰çº¿ç´¢ã€‚æˆ‘ä»¬æå‡ºMonoCLUEï¼Œå®ƒé€šè¿‡åˆ©ç”¨å±€éƒ¨èšç±»å’Œè§†è§‰ç‰¹å¾çš„å¹¿ä¹‰åœºæ™¯è®°å¿†æ¥å¢å¼ºå•ç›®3Dæ£€æµ‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯¹è§†è§‰ç‰¹å¾æ‰§è¡ŒK-meansèšç±»ï¼Œä»¥æ•è·ç‹¬ç‰¹çš„å¯¹è±¡çº§å¤–è§‚éƒ¨åˆ†ï¼ˆä¾‹å¦‚å¼•æ“ç›–ã€æ±½è½¦é¡¶éƒ¨ï¼‰ï¼Œä»è€Œæé«˜å¯¹éƒ¨åˆ†å¯è§å¯¹è±¡çš„æ£€æµ‹èƒ½åŠ›ã€‚èšç±»ç‰¹å¾è¢«ä¼ æ’­åˆ°å„ä¸ªåŒºåŸŸï¼Œä»¥æ•è·å…·æœ‰ç›¸ä¼¼å¤–è§‚çš„å¯¹è±¡ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡èšé›†å›¾åƒä¸­çš„èšç±»ç‰¹å¾æ¥æ„å»ºå¹¿ä¹‰åœºæ™¯è®°å¿†ï¼Œæä¾›ä¸€è‡´çš„è¡¨ç¤ºå½¢å¼ï¼Œè¿™äº›è¡¨ç¤ºå½¢å¼å¯ä»¥æ¨å¹¿åˆ°å„ä¸ªåœºæ™¯ã€‚è¿™æé«˜äº†å¯¹è±¡çº§ç‰¹å¾çš„ä¸€è‡´æ€§ï¼Œèƒ½å¤Ÿåœ¨å„ç§ç¯å¢ƒä¸­å®ç°ç¨³å®šçš„æ£€æµ‹ã€‚æœ€åï¼Œæˆ‘ä»¬å°†å±€éƒ¨èšç±»ç‰¹å¾å’Œå¹¿ä¹‰åœºæ™¯è®°å¿†æ•´åˆåˆ°å¯¹è±¡æŸ¥è¯¢ä¸­ï¼Œå¼•å¯¼æ³¨æ„åŠ›å…³æ³¨ä¿¡æ¯åŒºåŸŸã€‚é€šè¿‡åˆ©ç”¨ç»Ÿä¸€çš„å±€éƒ¨èšç±»å’Œå¹¿ä¹‰åœºæ™¯è®°å¿†ç­–ç•¥ï¼ŒMonoCLUEåœ¨é®æŒ¡å’Œæœ‰é™å¯è§åº¦çš„æƒ…å†µä¸‹å®ç°äº†ç¨³å¥çš„å•ç›®3Dæ£€æµ‹ï¼Œå¹¶åœ¨KITTIåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07862v1">PDF</a> Accepted to AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå•ç›®3Då¯¹è±¡æ£€æµ‹çš„è‡ªä¸»é©¾é©¶æˆæœ¬æ•ˆç›Šè§£å†³æ–¹æ¡ˆï¼Œå› æ·±åº¦æ¨¡ç³ŠåŠè§†é‡å—é™å­˜åœ¨å‡ ä½•çº¿ç´¢ç¼ºå¤±åŠé®æŒ¡æˆ–æˆªæ–­åœºæ™¯ç²¾åº¦ä¸‹é™é—®é¢˜ã€‚è¿‘æœŸæ–¹æ³•è™½èå…¥æ·±åº¦ä¿¡æ¯è§£å†³å‡ ä½•æ­§ä¹‰ï¼Œä½†å¿½ç•¥äº†å…³é”®è§†è§‰çº¿ç´¢çš„ç¨³å¥è¯†åˆ«ã€‚æœ¬æ–‡æå‡ºMonoCLUEï¼Œé€šè¿‡ç»“åˆå±€éƒ¨èšç±»å’Œå¹¿ä¹‰åœºæ™¯è®°å¿†å¼ºåŒ–å•ç›®3Dæ£€æµ‹ã€‚é¦–å…ˆï¼Œå¯¹è§†è§‰ç‰¹å¾è¿›è¡ŒK-meansèšç±»ï¼Œæ•æ‰ç‹¬ç‰¹çš„å¯¹è±¡çº§å¤–è§‚éƒ¨åˆ†ï¼ˆå¦‚å¼•æ“ç›–ã€è½¦é¡¶ï¼‰ï¼Œæ”¹è¿›éƒ¨åˆ†å¯è§å¯¹è±¡çš„æ£€æµ‹ã€‚èšç±»ç‰¹å¾è·¨åŒºåŸŸä¼ æ’­ä»¥æ•æ‰å…·æœ‰ç›¸ä¼¼å¤–è§‚çš„å¯¹è±¡ã€‚å…¶æ¬¡ï¼Œé€šè¿‡èšé›†å›¾åƒä¸­çš„èšç±»ç‰¹å¾æ„å»ºå¹¿ä¹‰åœºæ™¯è®°å¿†ï¼Œæä¾›è·¨åœºæ™¯çš„é€šç”¨è¡¨ç¤ºã€‚è¿™æé«˜äº†å¯¹è±¡çº§åˆ«çš„ç‰¹å¾ä¸€è‡´æ€§ï¼Œå®ç°äº†ä¸åŒç¯å¢ƒä¸‹çš„ç¨³å®šæ£€æµ‹ã€‚æœ€åï¼Œå°†å±€éƒ¨èšç±»ç‰¹å¾å’Œå¹¿ä¹‰åœºæ™¯è®°å¿†æ•´åˆåˆ°å¯¹è±¡æŸ¥è¯¢ä¸­ï¼Œå¼•å¯¼æ³¨æ„åŠ›å…³æ³¨ä¿¡æ¯åŒºåŸŸã€‚å€ŸåŠ©ç»Ÿä¸€çš„å±€éƒ¨èšç±»å’Œå¹¿ä¹‰åœºæ™¯è®°å¿†ç­–ç•¥ï¼ŒMonoCLUEåœ¨é®æŒ¡å’Œæœ‰é™å¯è§åº¦ä¸‹å®ç°äº†ç¨³å¥çš„å•ç›®3Dæ£€æµ‹ï¼Œåœ¨KITTIåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å•ç›®3Då¯¹è±¡æ£€æµ‹é¢ä¸´æ·±åº¦æ¨¡ç³Šå’Œè§†é‡å—é™çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´å‡ ä½•çº¿ç´¢ç¼ºå¤±å’Œç²¾åº¦ä¸‹é™ã€‚</li>
<li>è¿‘æœŸæ–¹æ³•è™½èå…¥æ·±åº¦ä¿¡æ¯ï¼Œä½†å¿½ç•¥äº†å…³é”®è§†è§‰çº¿ç´¢çš„ç¨³å¥è¯†åˆ«ã€‚</li>
<li>MonoCLUEé€šè¿‡ç»“åˆå±€éƒ¨èšç±»å’Œå¹¿ä¹‰åœºæ™¯è®°å¿†å¼ºåŒ–å•ç›®3Dæ£€æµ‹ã€‚</li>
<li>K-meansèšç±»ç”¨äºæ•æ‰ç‹¬ç‰¹çš„å¯¹è±¡çº§å¤–è§‚éƒ¨åˆ†ï¼Œæ”¹è¿›éƒ¨åˆ†å¯è§å¯¹è±¡çš„æ£€æµ‹ã€‚</li>
<li>èšç±»ç‰¹å¾è·¨åŒºåŸŸä¼ æ’­ä»¥è¯†åˆ«å…·æœ‰ç›¸ä¼¼å¤–è§‚çš„å¯¹è±¡ã€‚</li>
<li>é€šè¿‡æ„å»ºå¹¿ä¹‰åœºæ™¯è®°å¿†æä¾›è·¨åœºæ™¯çš„é€šç”¨è¡¨ç¤ºï¼Œæé«˜å¯¹è±¡çº§åˆ«çš„ç‰¹å¾ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3df4c666be240da574245e73b4d8873a" align="middle">
<img src="https://picx.zhimg.com/v2-2e842a20a8760a3e999206344d8a2e2d" align="middle">
<img src="https://picx.zhimg.com/v2-cc750a82c323d20b480426c96b744d22" align="middle">
<img src="https://picx.zhimg.com/v2-f1022b6aa71ba95fb7b5cb5357660ac9" align="middle">
<img src="https://picx.zhimg.com/v2-7eb91401dd3b7fd5a06284eae06c37cd" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="EIDSeg-A-Pixel-Level-Semantic-Segmentation-Dataset-for-Post-Earthquake-Damage-Assessment-from-Social-Media-Images"><a href="#EIDSeg-A-Pixel-Level-Semantic-Segmentation-Dataset-for-Post-Earthquake-Damage-Assessment-from-Social-Media-Images" class="headerlink" title="EIDSeg: A Pixel-Level Semantic Segmentation Dataset for Post-Earthquake Damage Assessment from Social Media Images"></a>EIDSeg: A Pixel-Level Semantic Segmentation Dataset for Post-Earthquake Damage Assessment from Social Media Images</h2><p><strong>Authors:Huili Huang, Chengeng Liu, Danrong Zhang, Shail Patel, Anastasiya Masalava, Sagar Sadak, Parisa Babolhavaeji, WeiHong Low, Max Mahdi Roozbahani, J. David Frost</strong></p>
<p>Rapid post-earthquake damage assessment is crucial for rescue and resource planning. Still, existing remote sensing methods depend on costly aerial images, expert labeling, and produce only binary damage maps for early-stage evaluation. Although ground-level images from social networks provide a valuable source to fill this gap, a large pixel-level annotated dataset for this task is still unavailable. We introduce EIDSeg, the first large-scale semantic segmentation dataset specifically for post-earthquake social media imagery. The dataset comprises 3,266 images from nine major earthquakes (2008-2023), annotated across five classes of infrastructure damage: Undamaged Building, Damaged Building, Destroyed Building, Undamaged Road, and Damaged Road. We propose a practical three-phase cross-disciplinary annotation protocol with labeling guidelines that enables consistent segmentation by non-expert annotators, achieving over 70% inter-annotator agreement. We benchmark several state-of-the-art segmentation models, identifying Encoder-only Mask Transformer (EoMT) as the top-performing method with a Mean Intersection over Union (mIoU) of 80.8%. By unlocking social networksâ€™ rich ground-level perspective, our work paves the way for a faster, finer-grained damage assessment in the post-earthquake scenario.</p>
<blockquote>
<p>å¿«é€Ÿçš„åœ°éœ‡åç¾å®³è¯„ä¼°å¯¹äºæ•‘æ´å’Œèµ„æºè§„åˆ’è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é¥æ„Ÿæ–¹æ³•ä¾èµ–äºæ˜‚è´µçš„èˆªç©ºå›¾åƒã€ä¸“å®¶æ ‡æ³¨ï¼Œå¹¶ä¸”ä»…æä¾›ç”¨äºæ—©æœŸè¯„ä¼°çš„äºŒè¿›åˆ¶çš„ç¾å®³åœ°å›¾ã€‚è™½ç„¶æ¥è‡ªç¤¾äº¤ç½‘ç»œçš„åœ°é¢å›¾åƒä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½æä¾›äº†å®è´µçš„èµ„æºï¼Œä½†é’ˆå¯¹æ­¤é¡¹ä»»åŠ¡çš„å¤§è§„æ¨¡åƒç´ çº§åˆ«çš„æ ‡æ³¨æ•°æ®é›†ä»ç„¶ä¸å¯ç”¨ã€‚æˆ‘ä»¬ä»‹ç»äº†EIDSegï¼Œè¿™æ˜¯ä¸“é—¨ä¸ºåœ°éœ‡åç¤¾äº¤åª’ä½“å›¾åƒè®¾è®¡çš„å¤§å‹è¯­ä¹‰åˆ†å‰²æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªä¹æ¬¡å¤§åœ°éœ‡ï¼ˆ2008å¹´è‡³2023å¹´ï¼‰çš„3,266å¼ å›¾åƒï¼Œè·¨è¶Šäº”ä¸ªç±»åˆ«çš„è®¾æ–½æŸåè¿›è¡Œæ ‡æ³¨ï¼šæœªæŸåå»ºç­‘ã€æŸåå»ºç­‘ã€æ¯åå»ºç­‘ã€æœªæŸåé“è·¯å’ŒæŸåé“è·¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å®ç”¨çš„è·¨å­¦ç§‘çš„ç¬¬ä¸‰é˜¶æ®µæ ‡æ³¨åè®®ï¼ŒåŒ…å«æ ‡æ³¨æŒ‡å—ï¼Œä½¿å¾—éä¸“ä¸šæ ‡æ³¨äººå‘˜èƒ½å¤Ÿè¿›è¡Œä¸€è‡´çš„åˆ†å‰²ï¼Œè¾¾åˆ°äº†è¶…è¿‡70%çš„æ ‡æ³¨è€…é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å¯¹ä¸€äº›å…ˆè¿›çš„åˆ†å‰²æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå‘ç°Encoder-only Mask Transformerï¼ˆEoMTï¼‰æ˜¯è¡¨ç°æœ€å¥½çš„æ–¹æ³•ï¼Œå…¶å¹³å‡äº¤å¹¶æ¯”ï¼ˆmIoUï¼‰ä¸º80.8%ã€‚é€šè¿‡è§£é”ç¤¾äº¤ç½‘ç»œçš„ä¸°å¯Œåœ°é¢è§†è§’ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ä¸ºåœ°éœ‡åçš„æ›´å¿«ã€æ›´ç²¾ç»†çš„ç¾å®³è¯„ä¼°é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06456v1">PDF</a> Camera-Ready for AAAI-AISI26</p>
<p><strong>Summary</strong>ï¼šå¼•å…¥äº†ä¸€ä¸ªé’ˆå¯¹ç¾åç¤¾äº¤åª’ä½“å½±åƒçš„å¤§å‹è¯­ä¹‰åˆ†å‰²æ•°æ®é›†EIDSegã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªä¹ä¸ªä¸»è¦åœ°éœ‡çš„3,266å¼ å›¾ç‰‡ï¼Œæ¶µç›–äº”ç§åŸºç¡€è®¾æ–½æŸåç±»åˆ«ã€‚æå‡ºäº†ä¸€ç§å®ç”¨çš„è·¨å­¦ç§‘æ ‡æ³¨åè®®ï¼Œèƒ½å¤Ÿä¸€è‡´åœ°è¿›è¡Œéä¸“å®¶æ ‡æ³¨ã€‚é€šè¿‡è§£é”ç¤¾äº¤åª’ä½“ä¸°å¯Œçš„åœ°é¢è§†è§’ï¼Œè¯¥ç ”ç©¶ä¸ºå¿«é€Ÿç²¾ç»†çš„åœ°éœ‡åæŸä¼¤è¯„ä¼°é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç¾åå¿«é€ŸæŸä¼¤è¯„ä¼°å¯¹æ•‘æ´å’Œèµ„æºè§„åˆ’è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰çš„é¥æ„Ÿæ–¹æ³•å­˜åœ¨æˆæœ¬é«˜æ˜‚å’Œåªæä¾›äºŒå…ƒæŸä¼¤åœ°å›¾çš„é—®é¢˜ã€‚</li>
<li>ç¤¾äº¤åª’ä½“çš„åœ°é¢è§†è§’å›¾åƒä¸ºè§£å†³æ­¤é—®é¢˜æä¾›äº†æœ‰ä»·å€¼çš„èµ„æºã€‚</li>
<li>å¼•å…¥äº†åä¸ºEIDSegçš„å¤§å‹è¯­ä¹‰åˆ†å‰²æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºç¾åçš„ç¤¾äº¤åª’ä½“å½±åƒã€‚</li>
<li>æ•°æ®é›†åŒ…å«æ¥è‡ªä¹ä¸ªä¸»è¦åœ°éœ‡çš„3,266å¼ å›¾ç‰‡ï¼Œæ¶µç›–äº”ç§åŸºç¡€è®¾æ–½æŸåç±»åˆ«ï¼ˆæ— æŸå»ºç­‘ã€æŸåå»ºç­‘ã€è¢«æ¯å»ºç­‘ã€æ— æŸé“è·¯å’ŒæŸåé“è·¯ï¼‰ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å®ç”¨çš„è·¨å­¦ç§‘æ ‡æ³¨åè®®ï¼Œå¹¶åˆ¶å®šäº†æ ‡æ³¨æŒ‡å—ï¼Œä½¿éä¸“å®¶æ ‡æ³¨å‘˜ä¹Ÿèƒ½è¿›è¡Œä¸€è‡´çš„åˆ†å‰²ã€‚</li>
<li>Encoder-only Mask Transformerï¼ˆEoMTï¼‰è¡¨ç°æœ€ä½³ï¼Œå…¶Mean Intersection over Unionï¼ˆmIoUï¼‰è¾¾åˆ°80.8%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06456">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cdca0c3b590fa22117315795ab465934" align="middle">
<img src="https://picx.zhimg.com/v2-4c3c1c519f2020649109007254028290" align="middle">
<img src="https://picx.zhimg.com/v2-2ec4b8b53eb0de0dee35da149fb631a0" align="middle">
<img src="https://picx.zhimg.com/v2-1c1efc81c4027516abe13eb88e6f0082" align="middle">
<img src="https://picx.zhimg.com/v2-7492f3a0204c40ef44b703247f2d86ef" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SFFR-Spatial-Frequency-Feature-Reconstruction-for-Multispectral-Aerial-Object-Detection"><a href="#SFFR-Spatial-Frequency-Feature-Reconstruction-for-Multispectral-Aerial-Object-Detection" class="headerlink" title="SFFR: Spatial-Frequency Feature Reconstruction for Multispectral Aerial Object Detection"></a>SFFR: Spatial-Frequency Feature Reconstruction for Multispectral Aerial Object Detection</h2><p><strong>Authors:Xin Zuo, Yuchen Qu, Haibo Zhan, Jifeng Shen, Wankou Yang</strong></p>
<p>Recent multispectral object detection methods have primarily focused on spatial-domain feature fusion based on CNNs or Transformers, while the potential of frequency-domain feature remains underexplored. In this work, we propose a novel Spatial and Frequency Feature Reconstruction method (SFFR) method, which leverages the spatial-frequency feature representation mechanisms of the Kolmogorov-Arnold Network (KAN) to reconstruct complementary representations in both spatial and frequency domains prior to feature fusion. The core components of SFFR are the proposed Frequency Component Exchange KAN (FCEKAN) module and Multi-Scale Gaussian KAN (MSGKAN) module. The FCEKAN introduces an innovative selective frequency component exchange strategy that effectively enhances the complementarity and consistency of cross-modal features based on the frequency feature of RGB and IR images. The MSGKAN module demonstrates excellent nonlinear feature modeling capability in the spatial domain. By leveraging multi-scale Gaussian basis functions, it effectively captures the feature variations caused by scale changes at different UAV flight altitudes, significantly enhancing the modelâ€™s adaptability and robustness to scale variations. It is experimentally validated that our proposed FCEKAN and MSGKAN modules are complementary and can effectively capture the frequency and spatial semantic features respectively for better feature fusion. Extensive experiments on the SeaDroneSee, DroneVehicle and DVTOD datasets demonstrate the superior performance and significant advantages of the proposed method in UAV multispectral object perception task. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/qchenyu1027/SFFR">https://github.com/qchenyu1027/SFFR</a>.</p>
<blockquote>
<p>æœ€è¿‘çš„å¤šå…‰è°±ç›®æ ‡æ£€æµ‹æ–¹æ³•ä¸»è¦å…³æ³¨åŸºäºCNNæˆ–Transformerçš„ç©ºé—´åŸŸç‰¹å¾èåˆï¼Œè€Œé¢‘ç‡åŸŸç‰¹å¾çš„å¯èƒ½æ€§å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç©ºé—´ä¸é¢‘ç‡ç‰¹å¾é‡å»ºæ–¹æ³•ï¼ˆSFFRï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰çš„ç©ºé—´é¢‘ç‡ç‰¹å¾è¡¨ç¤ºæœºåˆ¶ï¼Œåœ¨ç‰¹å¾èåˆä¹‹å‰é‡å»ºç©ºé—´åŸŸå’Œé¢‘ç‡åŸŸä¸­çš„äº’è¡¥è¡¨ç¤ºã€‚SFFRçš„æ ¸å¿ƒç»„ä»¶æ˜¯æå‡ºçš„é¢‘ç‡åˆ†é‡äº¤æ¢KANï¼ˆFCEKANï¼‰æ¨¡å—å’Œå¤šå°ºåº¦é«˜æ–¯KANï¼ˆMSGKANï¼‰æ¨¡å—ã€‚FCEKANå¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„é€‰æ‹©æ€§é¢‘ç‡åˆ†é‡äº¤æ¢ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŸºäºRGBå’ŒIRå›¾åƒçš„é¢‘ç‡ç‰¹å¾æœ‰æ•ˆåœ°å¢å¼ºäº†è·¨æ¨¡æ€ç‰¹å¾çš„äº’è¡¥æ€§å’Œä¸€è‡´æ€§ã€‚MSGKANæ¨¡å—åœ¨ç©ºåŸŸè¡¨ç°å‡ºå“è¶Šçš„éçº¿æ€§ç‰¹å¾å»ºæ¨¡èƒ½åŠ›ã€‚é€šè¿‡åˆ©ç”¨å¤šå°ºåº¦é«˜æ–¯åŸºç¡€å‡½æ•°ï¼Œå®ƒæœ‰æ•ˆåœ°æ•è·äº†ç”±äºä¸åŒæ— äººæœºé£è¡Œé«˜åº¦å¼•èµ·çš„å°ºåº¦å˜åŒ–æ‰€å¯¼è‡´çš„ç‰¹å¾å˜åŒ–ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹å°ºåº¦å˜åŒ–çš„é€‚åº”æ€§å’Œç¨³å¥æ€§ã€‚å®éªŒéªŒè¯äº†æˆ‘ä»¬æå‡ºçš„FCEKANå’ŒMSGKANæ¨¡å—çš„äº’è¡¥æ€§ï¼Œå®ƒä»¬å¯ä»¥æœ‰æ•ˆåœ°æ•æ‰é¢‘ç‡å’Œç©ºé—´è¯­ä¹‰ç‰¹å¾ï¼Œä»¥å®ç°æ›´å¥½çš„ç‰¹å¾èåˆã€‚åœ¨SeaDroneSeeã€DroneVehicleå’ŒDVTODæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ— äººæœºå¤šå…‰è°±ç›®æ ‡æ„ŸçŸ¥ä»»åŠ¡ä¸­å…·æœ‰å“è¶Šçš„æ€§èƒ½å’Œæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/qchenyu1027/SFFR%E4%B8%8A%E6%8F%9B%E4%BA%8C%E3%80%82">https://github.com/qchenyu1027/SFFRä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06298v2">PDF</a> 11 pages,8 figures, accepted by IEEE TGRS</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šå…‰è°±ç›®æ ‡æ£€æµ‹æ–¹æ³•â€”â€”ç©ºé—´ä¸é¢‘ç‡ç‰¹å¾é‡å»ºæ–¹æ³•ï¼ˆSFFRï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰çš„ç©ºé—´é¢‘ç‡ç‰¹å¾è¡¨ç¤ºæœºåˆ¶ï¼Œåœ¨ç©ºé—´å’Œé¢‘ç‡åŸŸè¿›è¡Œç‰¹å¾èåˆå‰çš„ç‰¹å¾é‡å»ºã€‚æ ¸å¿ƒæ¨¡å—åŒ…æ‹¬é¢‘ç‡åˆ†é‡äº¤æ¢KANï¼ˆFCEKANï¼‰å’Œå¤šå°ºåº¦é«˜æ–¯KANï¼ˆMSGKANï¼‰ã€‚FCEKANé€šè¿‡é€‰æ‹©æ€§é¢‘ç‡åˆ†é‡äº¤æ¢ç­–ç•¥å¢å¼ºäº†è·¨æ¨¡æ€ç‰¹å¾çš„äº’è¡¥æ€§å’Œä¸€è‡´æ€§ï¼Œè€ŒMSGKANæ¨¡å—åœ¨ç©ºé—´ä¸­å±•ç°äº†å‡ºè‰²çš„éçº¿æ€§ç‰¹å¾å»ºæ¨¡èƒ½åŠ›ã€‚é€šè¿‡å¤šå°ºåº¦é«˜æ–¯åŸºç¡€å‡½æ•°ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæ•æ‰ä¸åŒæ— äººæœºé£è¡Œé«˜åº¦æ‰€å¼•èµ·çš„å°ºåº¦å˜åŒ–ç‰¹å¾ï¼Œæé«˜äº†æ¨¡å‹å¯¹å°ºåº¦å˜åŒ–çš„é€‚åº”æ€§å’Œç¨³å¥æ€§ã€‚å®éªŒéªŒè¯æ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„FCEKANå’ŒMSGKANæ¨¡å—å¯äº’è¡¥æ•æ‰é¢‘ç‡å’Œç©ºé—´è¯­ä¹‰ç‰¹å¾ï¼Œå®ç°æ›´å¥½çš„ç‰¹å¾èåˆï¼Œåœ¨æ— äººæœºå¤šå…‰è°±ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šå…‰è°±ç›®æ ‡æ£€æµ‹æ–¹æ³•â€”â€”ç©ºé—´ä¸é¢‘ç‡ç‰¹å¾é‡å»ºæ–¹æ³•ï¼ˆSFFRï¼‰ã€‚</li>
<li>åˆ©ç”¨Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰è¿›è¡Œç©ºé—´é¢‘ç‡ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>å¼•å…¥é¢‘ç‡åˆ†é‡äº¤æ¢ç­–ç•¥ä»¥å¢å¼ºè·¨æ¨¡æ€ç‰¹å¾çš„äº’è¡¥æ€§å’Œä¸€è‡´æ€§ã€‚</li>
<li>å¤šå°ºåº¦é«˜æ–¯åŸºç¡€å‡½æ•°ç”¨äºæ•æ‰ä¸åŒé£è¡Œé«˜åº¦ä¸‹çš„å°ºåº¦å˜åŒ–ç‰¹å¾ã€‚</li>
<li>FCEKANå’ŒMSGKANæ¨¡å—å¯äº’è¡¥æ•æ‰é¢‘ç‡å’Œç©ºé—´è¯­ä¹‰ç‰¹å¾ã€‚</li>
<li>æ–¹æ³•åœ¨æ— äººæœºå¤šå…‰è°±ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06298">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-839fe34b8e63f1329272baa6b8b68e3b" align="middle">
<img src="https://picx.zhimg.com/v2-37f64fbfcd3aa72f58bdd2ce2d91233a" align="middle">
<img src="https://picx.zhimg.com/v2-2a5cc7b50aa84a41a01c14ad15eeee96" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="TCSA-UDA-Text-Driven-Cross-Semantic-Alignment-for-Unsupervised-Domain-Adaptation-in-Medical-Image-Segmentation"><a href="#TCSA-UDA-Text-Driven-Cross-Semantic-Alignment-for-Unsupervised-Domain-Adaptation-in-Medical-Image-Segmentation" class="headerlink" title="TCSA-UDA: Text-Driven Cross-Semantic Alignment for Unsupervised Domain Adaptation in Medical Image Segmentation"></a>TCSA-UDA: Text-Driven Cross-Semantic Alignment for Unsupervised Domain Adaptation in Medical Image Segmentation</h2><p><strong>Authors:Lalit Maurya, Honghai Liu, Reyer Zwiggelaar</strong></p>
<p>Unsupervised domain adaptation for medical image segmentation remains a significant challenge due to substantial domain shifts across imaging modalities, such as CT and MRI. While recent vision-language representation learning methods have shown promise, their potential in UDA segmentation tasks remains underexplored. To address this gap, we propose TCSA-UDA, a Text-driven Cross-Semantic Alignment framework that leverages domain-invariant textual class descriptions to guide visual representation learning. Our approach introduces a vision-language covariance cosine loss to directly align image encoder features with inter-class textual semantic relations, encouraging semantically meaningful and modality-invariant feature representations. Additionally, we incorporate a prototype alignment module that aligns class-wise pixel-level feature distributions across domains using high-level semantic prototypes. This mitigates residual category-level discrepancies and enhances cross-modal consistency. Extensive experiments on challenging cross-modality cardiac, abdominal, and brain tumor segmentation benchmarks demonstrate that our TCSA-UDA framework significantly reduces domain shift and consistently outperforms state-of-the-art UDA methods, establishing a new paradigm for integrating language-driven semantics into domain-adaptive medical image analysis.</p>
<blockquote>
<p>é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”ï¼ˆUDAï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œç”±äºæˆåƒæ¨¡å¼ï¼ˆå¦‚CTå’ŒMRIï¼‰ä¹‹é—´å­˜åœ¨å¤§é‡çš„åŸŸåç§»ã€‚è™½ç„¶æœ€è¿‘çš„è§†è§‰è¯­è¨€è¡¨ç¤ºå­¦ä¹ æ–¹æ³•æ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„æ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨UDAåˆ†å‰²ä»»åŠ¡ä¸­çš„æ½œåŠ›ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†TCSA-UDAï¼Œä¸€ä¸ªæ–‡æœ¬é©±åŠ¨è·¨è¯­ä¹‰å¯¹é½æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨åŸŸä¸å˜çš„æ–‡æœ¬ç±»æè¿°æ¥æŒ‡å¯¼è§†è§‰è¡¨ç¤ºå­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§è§†è§‰è¯­è¨€åæ–¹å·®ä½™å¼¦æŸå¤±ï¼Œç›´æ¥å¯¹é½å›¾åƒç¼–ç å™¨ç‰¹å¾ä¸è·¨ç±»æ–‡æœ¬è¯­ä¹‰å…³ç³»ï¼Œé¼“åŠ±å…·æœ‰è¯­ä¹‰æ„ä¹‰å’Œæ¨¡æ€ä¸å˜çš„ç‰¹å¾è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åŠ å…¥äº†ä¸€ä¸ªåŸå‹å¯¹é½æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨é«˜çº§è¯­ä¹‰åŸå‹æ¥å¯¹é½è·¨åŸŸçš„é€ç±»åƒç´ çº§ç‰¹å¾åˆ†å¸ƒã€‚è¿™å‡è½»äº†å‰©ä½™çš„ç±»åˆ«çº§å·®å¼‚å¹¶å¢å¼ºäº†è·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„è·¨æ¨¡æ€å¿ƒè„ã€è…¹éƒ¨å’Œè„‘è‚¿ç˜¤åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„TCSA-UDAæ¡†æ¶æ˜¾è‘—å‡å°‘äº†åŸŸåç§»ï¼Œå¹¶ä¸”ä¸€è‡´åœ°ä¼˜äºæœ€å…ˆè¿›çš„UDAæ–¹æ³•ï¼Œä¸ºå°†è¯­è¨€é©±åŠ¨çš„è¯­ä¹‰é›†æˆåˆ°åŸŸè‡ªé€‚åº”åŒ»å­¦å›¾åƒåˆ†æä¸­å»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05782v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºTCSA-UDAæ¡†æ¶ï¼Œåˆ©ç”¨æ–‡æœ¬é©±åŠ¨çš„è·¨è¯­ä¹‰å¯¹é½æŠ€æœ¯æ¥è§£å†³åŒ»å­¦å½±åƒåˆ†å‰²ä¸­çš„æ— ç›‘ç£åŸŸé€‚åº”é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥è§†è§‰è¯­è¨€åæ–¹å·®ä½™å¼¦æŸå¤±å’ŒåŸå‹å¯¹é½æ¨¡å—ï¼Œåˆ©ç”¨è·¨æ¨¡æ€æ–‡æœ¬è¯­ä¹‰æè¿°æ¥æŒ‡å¯¼è§†è§‰è¡¨ç¤ºå­¦ä¹ ï¼Œå¹¶æœ‰æ•ˆå‡å°‘åŸŸåç§»ï¼Œæé«˜è·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜ï¼ŒTCSA-UDAåœ¨å¿ƒè„ã€è…¹éƒ¨å’Œè„‘éƒ¨è‚¿ç˜¤åˆ†å‰²çš„è·¨æ¨¡æ€ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ— ç›‘ç£åŸŸé€‚åº”æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TCSA-UDAæ¡†æ¶è§£å†³äº†åŒ»å­¦å½±åƒåˆ†å‰²ä¸­çš„æ— ç›‘ç£åŸŸé€‚åº”é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨æ–‡æœ¬é©±åŠ¨çš„è·¨è¯­ä¹‰å¯¹é½æŠ€æœ¯ï¼Œé€šè¿‡å¼•å…¥è§†è§‰è¯­è¨€åæ–¹å·®ä½™å¼¦æŸå¤±æ¥æŒ‡å¯¼è§†è§‰è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>å¼•å…¥åŸå‹å¯¹é½æ¨¡å—ï¼Œé€šè¿‡é«˜å±‚æ¬¡çš„è¯­ä¹‰åŸå‹æ¥å¯¹é½è·¨åŸŸçš„ç±»çº§åƒç´ çº§ç‰¹å¾åˆ†å¸ƒã€‚</li>
<li>æœ‰æ•ˆå‡å°‘åŸŸåç§»å¹¶å¢å¼ºè·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨å¿ƒè„ã€è…¹éƒ¨å’Œè„‘éƒ¨è‚¿ç˜¤åˆ†å‰²çš„è·¨æ¨¡æ€ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒéªŒè¯ã€‚</li>
<li>TCSA-UDAæ¡†æ¶æ˜¾è‘—ä¼˜äºç°æœ‰çš„æ— ç›‘ç£åŸŸé€‚åº”æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1dc94be044a416648cc4878882351004" align="middle">
<img src="https://picx.zhimg.com/v2-78c3a5f444bb04c07c8b57ad3c24f39f" align="middle">
<img src="https://picx.zhimg.com/v2-8b4569c08440c658a68469a2b44d8c76" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DGL-RSIS-Decoupling-Global-Spatial-Context-and-Local-Class-Semantics-for-Training-Free-Remote-Sensing-Image-Segmentation"><a href="#DGL-RSIS-Decoupling-Global-Spatial-Context-and-Local-Class-Semantics-for-Training-Free-Remote-Sensing-Image-Segmentation" class="headerlink" title="DGL-RSIS: Decoupling Global Spatial Context and Local Class Semantics for Training-Free Remote Sensing Image Segmentation"></a>DGL-RSIS: Decoupling Global Spatial Context and Local Class Semantics for Training-Free Remote Sensing Image Segmentation</h2><p><strong>Authors:Boyi Li, Ce Zhang, Richard M. Timmerman, Wenxuan Bao</strong></p>
<p>The emergence of vision language models (VLMs) bridges the gap between vision and language, enabling multimodal understanding beyond traditional visual-only deep learning models. However, transferring VLMs from the natural image domain to remote sensing (RS) segmentation remains challenging due to the large domain gap and the diversity of RS inputs across tasks, particularly in open-vocabulary semantic segmentation (OVSS) and referring expression segmentation (RES). Here, we propose a training-free unified framework, termed DGL-RSIS, which decouples visual and textual representations and performs visual-language alignment at both local semantic and global contextual levels. Specifically, a Global-Local Decoupling (GLD) module decomposes textual inputs into local semantic tokens and global contextual tokens, while image inputs are partitioned into class-agnostic mask proposals. Then, a Local Visual-Textual Alignment (LVTA) module adaptively extracts context-aware visual features from the mask proposals and enriches textual features through knowledge-guided prompt engineering, achieving OVSS from a local perspective. Furthermore, a Global Visual-Textual Alignment (GVTA) module employs a global-enhanced Grad-CAM mechanism to capture contextual cues for referring expressions, followed by a mask selection module that integrates pixel-level activations into mask-level segmentation outputs, thereby achieving RES from a global perspective. Experiments on the iSAID (OVSS) and RRSIS-D (RES) benchmarks demonstrate that DGL-RSIS outperforms existing training-free approaches. Ablation studies further validate the effectiveness of each module. To the best of our knowledge, this is the first unified training-free framework for RS image segmentation, which effectively transfers the semantic capability of VLMs trained on natural images to the RS domain without additional training.</p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å‡ºç°ï¼Œç¼©å°äº†è§†è§‰ä¸è¯­è¨€ä¹‹é—´çš„å·®è·ï¼Œå®ç°äº†è¶…è¶Šä¼ ç»Ÿä»…è§†è§‰æ·±åº¦å­¦ä¹ çš„å¤šæ¨¡æ€ç†è§£ã€‚ç„¶è€Œï¼Œå°†VLMsä»è‡ªç„¶å›¾åƒé¢†åŸŸè¿ç§»åˆ°é¥æ„Ÿï¼ˆRSï¼‰åˆ†å‰²ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºé¢†åŸŸå·®è·å¤§ä»¥åŠé¥æ„Ÿè¾“å…¥ä»»åŠ¡çš„å¤šæ ·æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰å’Œå¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²ï¼ˆRESï¼‰ä¸­ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„ç»Ÿä¸€æ¡†æ¶ï¼Œåä¸ºDGL-RSISï¼Œè¯¥æ¡†æ¶å¯ä»¥è§£è€¦è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºï¼Œå¹¶åœ¨å±€éƒ¨è¯­ä¹‰å’Œå…¨å±€ä¸Šä¸‹æ–‡çº§åˆ«è¿›è¡Œè§†è§‰è¯­è¨€å¯¹é½ã€‚å…·ä½“è€Œè¨€ï¼Œå…¨å±€å±€éƒ¨è§£è€¦ï¼ˆGLDï¼‰æ¨¡å—å°†æ–‡æœ¬è¾“å…¥åˆ†è§£ä¸ºå±€éƒ¨è¯­ä¹‰æ ‡è®°å’Œå…¨å±€ä¸Šä¸‹æ–‡æ ‡è®°ï¼Œè€Œå›¾åƒè¾“å…¥åˆ™è¢«åˆ’åˆ†ä¸ºç±»æ— å…³æ©è†œææ¡ˆã€‚ç„¶åï¼Œå±€éƒ¨è§†è§‰æ–‡æœ¬å¯¹é½ï¼ˆLVTAï¼‰æ¨¡å—è‡ªé€‚åº”åœ°ä»æ©è†œææ¡ˆä¸­æå–ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è§†è§‰ç‰¹å¾ï¼Œå¹¶é€šè¿‡çŸ¥è¯†å¼•å¯¼æç¤ºå·¥ç¨‹ä¸°å¯Œæ–‡æœ¬ç‰¹å¾ï¼Œä»è€Œå®ç°ä»å±€éƒ¨è§’åº¦çš„OVSSã€‚æ­¤å¤–ï¼Œå…¨å±€è§†è§‰æ–‡æœ¬å¯¹é½ï¼ˆGVTAï¼‰æ¨¡å—é‡‡ç”¨å…¨å±€å¢å¼ºGrad-CAMæœºåˆ¶æ¥æ•è·å¼•ç”¨è¡¨è¾¾å¼çš„ä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œéšåæ˜¯æ©è†œé€‰æ‹©æ¨¡å—ï¼Œè¯¥æ¨¡å—å°†åƒç´ çº§æ¿€æ´»æ•´åˆåˆ°æ©è†œçº§åˆ†å‰²è¾“å‡ºä¸­ï¼Œä»è€Œå®ç°ä»å…¨å±€è§’åº¦çš„RESã€‚åœ¨iSAIDï¼ˆOVSSï¼‰å’ŒRRSIS-Dï¼ˆRESï¼‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDGL-RSISä¼˜äºç°æœ‰çš„æ— éœ€è®­ç»ƒçš„æ–¹æ³•ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æ¯ä¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„ç»Ÿä¸€é¥æ„Ÿå›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å°†åŸºäºè‡ªç„¶å›¾åƒè®­ç»ƒçš„VLMsçš„è¯­ä¹‰èƒ½åŠ›è½¬ç§»åˆ°é¥æ„Ÿé¢†åŸŸè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00598v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å¤šæ¨¡æ€ç†è§£åœ¨é¥æ„Ÿï¼ˆRSï¼‰å›¾åƒåˆ†å‰²é¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºé¢†åŸŸå·®è·å¤§åŠé¥æ„Ÿè¾“å…¥ä»»åŠ¡çš„å¤šæ ·æ€§ï¼ŒVLMsåœ¨è‡ªç„¶å›¾åƒé¢†åŸŸçš„åº”ç”¨éš¾ä»¥ç›´æ¥è¿ç§»åˆ°RSåˆ†å‰²ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„ç»Ÿä¸€æ¡†æ¶DGL-RSISï¼Œè¯¥æ¡†æ¶å¯åœ¨å±€éƒ¨è¯­ä¹‰å’Œå…¨å±€è¯­å¢ƒå±‚é¢è¿›è¡Œè§†è§‰ä¸è¯­è¨€å¯¹é½ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨iSAIDå’ŒRRSIS-DåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰èƒ½å¤Ÿå¼¥åˆè§†è§‰ä¸è¯­è¨€ä¹‹é—´çš„é¸¿æ²Ÿï¼Œå®ç°å¤šæ¨¡æ€ç†è§£ã€‚</li>
<li>å°†VLMsä»è‡ªç„¶å›¾åƒé¢†åŸŸè¿ç§»åˆ°é¥æ„Ÿï¼ˆRSï¼‰åˆ†å‰²å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºé¢†åŸŸå·®è·å¤§å’Œä»»åŠ¡å¤šæ ·æ€§ã€‚</li>
<li>æå‡ºçš„DGL-RSISæ¡†æ¶æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„ç»Ÿä¸€æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å±€éƒ¨è¯­ä¹‰å’Œå…¨å±€è¯­å¢ƒå±‚é¢è¿›è¡Œè§†è§‰ä¸è¯­è¨€å¯¹é½ã€‚</li>
<li>DGL-RSISæ¡†æ¶åŒ…æ‹¬å…¨å±€-å±€éƒ¨è§£è€¦ï¼ˆGLDï¼‰æ¨¡å—ã€å±€éƒ¨è§†è§‰-æ–‡æœ¬å¯¹é½ï¼ˆLVTAï¼‰æ¨¡å—å’Œå…¨å±€è§†è§‰-æ–‡æœ¬å¯¹é½ï¼ˆGVTAï¼‰æ¨¡å—ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒDGL-RSISæ¡†æ¶åœ¨iSAIDå’ŒRRSIS-DåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ— éœ€è®­ç»ƒçš„æ–¹æ³•ã€‚</li>
<li>DGL-RSISæ¡†æ¶æ˜¯é¦–ä¸ªå°†VLMsåœ¨è‡ªç„¶å›¾åƒé¢†åŸŸçš„è¯­ä¹‰èƒ½åŠ›æœ‰æ•ˆè¿ç§»åˆ°é¥æ„Ÿé¢†åŸŸçš„æ— éœ€è®­ç»ƒçš„ç»Ÿä¸€æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00598">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-214dac5d69ee874e35e792ca9397ca9c" align="middle">
<img src="https://picx.zhimg.com/v2-b57de51f9f68cd42bfee2cf857a43c3b" align="middle">
<img src="https://picx.zhimg.com/v2-f5046e74e4dda7e814f33d6008bdbf05" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="An-Instance-Aware-Prompting-Framework-for-Training-free-Camouflaged-Object-Segmentation"><a href="#An-Instance-Aware-Prompting-Framework-for-Training-free-Camouflaged-Object-Segmentation" class="headerlink" title="An Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation"></a>An Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation</h2><p><strong>Authors:Chao Yin, Jide Li, Hang Yao, Xiaoqiang Li</strong></p>
<p>Training-free Camouflaged Object Segmentation (COS) seeks to segment camouflaged objects without task-specific training, by automatically generating visual prompts to guide the Segment Anything Model (SAM). However, existing pipelines mostly yield semantic-level prompts, which drive SAM to coarse semantic masks and struggle to handle multiple discrete camouflaged instances effectively. To address this critical limitation, we propose an \textbf{I}nstance-\textbf{A}ware \textbf{P}rompting \textbf{F}ramework (IAPF) tailored for the first training-free COS that upgrades prompt granularity from semantic to instance-level while keeping all components frozen. The centerpiece is an Instance Mask Generator that (i) leverages a detector-agnostic enumerator to produce precise instance-level box prompts for the foreground tag, and (ii) introduces the Single-Foreground Multi-Background Prompting (SFMBP) strategy to sample region-constrained point prompts within each box prompt, enabling SAM to output instance masks. The pipeline is supported by a simple text prompt generator that produces image-specific tags and a self-consistency vote across synonymous task-generic prompts to stabilize inference. Extensive evaluations on three COS benchmarks, two CIS benchmarks, and two downstream datasets demonstrate state-of-the-art performance among training-free methods. Code will be released upon acceptance.</p>
<blockquote>
<p>æ— è®­ç»ƒä¼ªè£…ç›®æ ‡åˆ†å‰²ï¼ˆCOSï¼‰æ—¨åœ¨æ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒï¼Œé€šè¿‡è‡ªåŠ¨ç”Ÿæˆè§†è§‰æç¤ºæ¥å¼•å¯¼ä»»ä½•ç›®æ ‡åˆ†å‰²æ¨¡å‹ï¼ˆSAMï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰æµç¨‹å¤§å¤šç”Ÿæˆè¯­ä¹‰çº§åˆ«çš„æç¤ºï¼Œè¿™å¯¼è‡´SAMåªèƒ½è·å¾—ç²—ç•¥çš„è¯­ä¹‰æ©è†œï¼Œå¹¶ä¸”éš¾ä»¥æœ‰æ•ˆå¤„ç†å¤šä¸ªç¦»æ•£ä¼ªè£…å®ä¾‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å…³é”®é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹é¦–æ¬¡æ— è®­ç»ƒCOSçš„å®ä¾‹æ„ŸçŸ¥æç¤ºæ¡†æ¶ï¼ˆIAPFï¼‰ï¼Œè¯¥æ¡†æ¶å°†æç¤ºç²’åº¦ä»è¯­ä¹‰å‡çº§åˆ°å®ä¾‹çº§åˆ«ï¼ŒåŒæ—¶ä¿æŒæ‰€æœ‰ç»„ä»¶å†»ç»“ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªå®ä¾‹æ©è†œç”Ÿæˆå™¨ï¼Œå®ƒï¼ˆiï¼‰åˆ©ç”¨ä¸æ£€æµ‹å™¨æ— å…³çš„æšä¸¾å™¨äº§ç”Ÿç²¾ç¡®åˆ°å®ä¾‹çº§åˆ«çš„æ¡†æç¤ºä½œä¸ºå‰æ™¯æ ‡ç­¾ï¼Œï¼ˆiiï¼‰å¼•å…¥å•å‰æ™¯å¤šèƒŒæ™¯æç¤ºï¼ˆSFMBPï¼‰ç­–ç•¥ï¼Œåœ¨æ¯ä¸ªæ¡†æç¤ºå†…é‡‡æ ·å—åŒºåŸŸçº¦æŸçš„ç‚¹æç¤ºï¼Œä½¿SAMèƒ½å¤Ÿè¾“å‡ºå®ä¾‹æ©è†œã€‚è¯¥æµç¨‹ç”±ç®€å•çš„æ–‡æœ¬æç¤ºç”Ÿæˆå™¨æ”¯æŒï¼Œç”Ÿæˆç‰¹å®šå›¾åƒçš„æ ‡ç­¾ï¼Œå¹¶é€šè¿‡åŒä¹‰è¯é€šç”¨ä»»åŠ¡æç¤ºçš„è‡ªæˆ‘ä¸€è‡´æ€§æŠ•ç¥¨æ¥ç¨³å®šæ¨æ–­ã€‚åœ¨ä¸‰ä¸ªCOSåŸºå‡†æµ‹è¯•ã€ä¸¤ä¸ªCISåŸºå‡†æµ‹è¯•å’Œä¸¤ä¸ªä¸‹æ¸¸æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œå…¶åœ¨æ— è®­ç»ƒæ–¹æ³•ä¸­å¤„äºé¢†å…ˆæ°´å¹³ã€‚ä»£ç å°†åœ¨æ¥å—åå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06904v3">PDF</a> under review</p>
<p><strong>Summary</strong></p>
<p>æ— è®­ç»ƒCOSï¼ˆCamouflaged Object Segmentationï¼‰å¯¹è±¡åˆ†å‰²æ–¹æ³•é€šè¿‡è‡ªåŠ¨ç”Ÿæˆè§†è§‰æç¤ºæ¥å¼•å¯¼åˆ†å‰²ä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒã€‚ä½†ç°æœ‰æµç¨‹ä¸»è¦äº§ç”Ÿè¯­ä¹‰çº§åˆ«çš„æç¤ºï¼Œå¯¼è‡´SAMäº§ç”Ÿç²—ç³™çš„è¯­ä¹‰æ©è†œï¼Œéš¾ä»¥æœ‰æ•ˆå¤„ç†å¤šä¸ªç¦»æ•£éšè”½å®ä¾‹ã€‚ä¸ºè§£å†³æ­¤é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºé¢å‘é¦–ä¸ªæ— è®­ç»ƒCOSçš„å®ä¾‹æ„ŸçŸ¥æç¤ºæ¡†æ¶ï¼ˆIAPFï¼‰ï¼Œå°†æç¤ºç²’åº¦ä»è¯­ä¹‰å‡çº§åˆ°å®ä¾‹çº§åˆ«ï¼ŒåŒæ—¶ä¿æŒæ‰€æœ‰ç»„ä»¶å†»ç»“ã€‚é€šè¿‡å®ä¾‹æ©è†œç”Ÿæˆå™¨ï¼ˆInstance Mask Generatorï¼‰äº§ç”Ÿç²¾ç¡®å®ä¾‹çº§æ¡†æç¤ºå’Œå•å‰æ™¯å¤šèƒŒæ™¯æç¤ºç­–ç•¥ï¼ˆSFMBPï¼‰ï¼Œä½¿SAMè¾“å‡ºå®ä¾‹æ©è†œã€‚è¯¥æµç¨‹ç”±æ–‡æœ¬æç¤ºç”Ÿæˆå™¨æ”¯æŒï¼Œäº§ç”Ÿå›¾åƒç‰¹å®šæ ‡ç­¾ï¼Œå¹¶é€šè¿‡åŒä¹‰è¯é€šç”¨ä»»åŠ¡æç¤ºçš„è‡ªæˆ‘ä¸€è‡´æ€§æŠ•ç¥¨æ¥ç¨³å®šæ¨æ–­ã€‚åœ¨ä¸‰ä¸ªCOSåŸºå‡†æµ‹è¯•ã€ä¸¤ä¸ªCISåŸºå‡†æµ‹è¯•å’Œä¸¤ä¸ªä¸‹æ¸¸æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ— è®­ç»ƒæ–¹æ³•ä¸­è¡¨ç°æœ€ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®­ç»ƒ-free Camouflaged Object Segmentation (COS)æ–¹æ³•é€šè¿‡è‡ªåŠ¨ç”Ÿæˆè§†è§‰æç¤ºå¼•å¯¼Segment Anything Model (SAM)ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒã€‚</li>
<li>ç°æœ‰æµç¨‹ä¸»è¦äº§ç”Ÿè¯­ä¹‰çº§åˆ«çš„æç¤ºï¼Œéš¾ä»¥å¤„ç†å¤šä¸ªç¦»æ•£éšè”½å®ä¾‹ã€‚</li>
<li>æå‡ºé¢å‘æ— è®­ç»ƒCOSçš„å®ä¾‹æ„ŸçŸ¥æç¤ºæ¡†æ¶ï¼ˆIAPFï¼‰ï¼Œå°†æç¤ºç²’åº¦ä»è¯­ä¹‰å‡çº§åˆ°å®ä¾‹çº§åˆ«ã€‚</li>
<li>IAPFåŒ…æ‹¬ä¸€ä¸ªå®ä¾‹æ©è†œç”Ÿæˆå™¨ï¼Œäº§ç”Ÿç²¾ç¡®å®ä¾‹çº§æ¡†æç¤ºå’Œå•å‰æ™¯å¤šèƒŒæ™¯æç¤ºç­–ç•¥ã€‚</li>
<li>æ–‡æœ¬æç¤ºç”Ÿæˆå™¨æ”¯æŒè¯¥æµç¨‹ï¼Œäº§ç”Ÿå›¾åƒç‰¹å®šæ ‡ç­¾ï¼Œé€šè¿‡è‡ªæˆ‘ä¸€è‡´æ€§æŠ•ç¥¨ç¨³å®šæ¨æ–­ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œä¸‹æ¸¸æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ— è®­ç»ƒæ–¹æ³•ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06904">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1854b8540b1e6c76c2593bcd3f7bfda6" align="middle">
<img src="https://picx.zhimg.com/v2-2aa0d33cf26474716c6d409849d705ad" align="middle">
<img src="https://picx.zhimg.com/v2-0249b20f4341d209e87b86fa21e1552c" align="middle">
<img src="https://picx.zhimg.com/v2-bc52ed58ee516da67c66fe99b4b1841c" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="RaGS-Unleashing-3D-Gaussian-Splatting-from-4D-Radar-and-Monocular-Cues-for-3D-Object-Detection"><a href="#RaGS-Unleashing-3D-Gaussian-Splatting-from-4D-Radar-and-Monocular-Cues-for-3D-Object-Detection" class="headerlink" title="RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues for 3D Object Detection"></a>RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues for 3D Object Detection</h2><p><strong>Authors:Xiaokai Bai, Chenxu Zhou, Lianqing Zheng, Si-Yuan Cao, Jianan Liu, Xiaohan Zhang, Yiming Li, Zhengzhuang Zhang, Hui-liang Shen</strong></p>
<p>4D millimeter-wave radar is a promising sensing modality for autonomous driving, yet effective 3D object detection from 4D radar and monocular images remains challenging. Existing fusion approaches either rely on instance proposals lacking global context or dense BEV grids constrained by rigid structures, lacking a flexible and adaptive representation for diverse scenes. To address this, we propose RaGS, the first framework that leverages 3D Gaussian Splatting (GS) to fuse 4D radar and monocular cues for 3D object detection. 3D GS models the scene as a continuous field of Gaussians, enabling dynamic resource allocation to foreground objects while maintaining flexibility and efficiency. Moreover, the velocity dimension of 4D radar provides motion cues that help anchor and refine the spatial distribution of Gaussians. Specifically, RaGS adopts a cascaded pipeline to construct and progressively refine the Gaussian field. It begins with Frustum-based Localization Initiation (FLI), which unprojects foreground pixels to initialize coarse Gaussian centers. Then, Iterative Multimodal Aggregation (IMA) explicitly exploits image semantics and implicitly integrates 4D radar velocity geometry to refine the Gaussians within regions of interest. Finally, Multi-level Gaussian Fusion (MGF) renders the Gaussian field into hierarchical BEV features for 3D object detection. By dynamically focusing on sparse and informative regions, RaGS achieves object-centric precision and comprehensive scene perception. Extensive experiments on View-of-Delft, TJ4DRadSet, and OmniHD-Scenes demonstrate its robustness and SOTA performance. Code will be released.</p>
<blockquote>
<p>4Dæ¯«ç±³æ³¢é›·è¾¾å¯¹äºè‡ªåŠ¨é©¾é©¶æ¥è¯´æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ„ŸçŸ¥æ–¹å¼ï¼Œä½†ä»4Dé›·è¾¾å’Œå•ç›®å›¾åƒè¿›è¡Œæœ‰æ•ˆçš„3Då¯¹è±¡æ£€æµ‹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„èåˆæ–¹æ³•è¦ä¹ˆä¾èµ–äºç¼ºä¹å…¨å±€ä¸Šä¸‹æ–‡çš„å®ä¾‹ææ¡ˆï¼Œè¦ä¹ˆå—é™äºåˆšæ€§çš„å¯†é›†BEVç½‘æ ¼ï¼Œç¼ºä¹çµæ´»å’Œè‡ªé€‚åº”çš„è¡¨ç¤ºæ¥å¤„ç†å„ç§åœºæ™¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RaGSï¼Œè¿™æ˜¯ä¸€ä¸ªé¦–æ¬¡åˆ©ç”¨3Dé«˜æ–¯æ¶‚æ–‘ï¼ˆGSï¼‰èåˆ4Dé›·è¾¾å’Œå•ç›®å›¾åƒæç¤ºè¿›è¡Œ3Då¯¹è±¡æ£€æµ‹çš„æ¡†æ¶ã€‚3D GSå°†åœºæ™¯å»ºæ¨¡ä¸ºé«˜æ–¯è¿ç»­åœºï¼Œå®ç°åŠ¨æ€èµ„æºåˆ†é…ç»™å‰æ™¯å¯¹è±¡ï¼ŒåŒæ—¶ä¿æŒçµæ´»æ€§å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œ4Dé›·è¾¾çš„é€Ÿåº¦ç»´åº¦æä¾›äº†è¿åŠ¨æç¤ºï¼Œæœ‰åŠ©äºé”šå®šå¹¶ä¼˜åŒ–é«˜æ–¯çš„ç©ºé—´åˆ†å¸ƒã€‚å…·ä½“æ¥è¯´ï¼ŒRaGSé‡‡ç”¨çº§è”ç®¡é“æ„å»ºå¹¶é€æ­¥å®Œå–„é«˜æ–¯åœºã€‚å®ƒå§‹äºåŸºäºFrustumçš„å®šä½åˆå§‹åŒ–ï¼ˆFLIï¼‰ï¼Œå°†å‰æ™¯åƒç´ åæŠ•å½±ä»¥åˆå§‹åŒ–ç²—ç•¥çš„é«˜æ–¯ä¸­å¿ƒã€‚ç„¶åï¼Œè¿­ä»£å¤šæ¨¡å¼èšåˆï¼ˆIMAï¼‰æ˜¾å¼åˆ©ç”¨å›¾åƒè¯­ä¹‰å¹¶éšå¼é›†æˆ4Dé›·è¾¾é€Ÿåº¦å‡ ä½•æ¥å®Œå–„æ„Ÿå…´è¶£åŒºåŸŸå†…çš„é«˜æ–¯ã€‚æœ€åï¼Œå¤šçº§é«˜æ–¯èåˆï¼ˆMGFï¼‰å°†é«˜æ–¯åœºå‘ˆç°ä¸ºåˆ†å±‚BEVç‰¹å¾ï¼Œç”¨äº3Då¯¹è±¡æ£€æµ‹ã€‚RaGSé€šè¿‡åŠ¨æ€å…³æ³¨ç¨€ç–ä¸”ä¿¡æ¯ä¸°å¯Œçš„åŒºåŸŸï¼Œå®ç°äº†ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„ç²¾åº¦å’Œå…¨é¢çš„åœºæ™¯æ„ŸçŸ¥ã€‚åœ¨View-of-Delftã€TJ4DRadSetå’ŒOmniHD-Scenesä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†å…¶ç¨³å¥æ€§å’ŒSOTAæ€§èƒ½ã€‚ä»£ç å°†è¢«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19856v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†4Dæ¯«ç±³æ³¢é›·è¾¾åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨å‰æ™¯åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä¸ºè§£å†³ç°æœ‰çš„ä¸‰ç»´ç›®æ ‡æ£€æµ‹åœ¨èåˆè¿‡ç¨‹ä¸­çš„å±€é™æ€§é—®é¢˜ï¼Œæå‡ºäº†å…¨æ–°çš„RaGSæ¡†æ¶ï¼Œç»“åˆé«˜æ–¯æ¨¡æ¿æ³•å’Œå•ç›®å›¾åƒæŠ€æœ¯å®ç°æ›´ç²¾ç¡®çš„ç›®æ ‡æ£€æµ‹ã€‚è¯¥æ¡†æ¶å¯åŠ¨æ€é…ç½®èµ„æºäºå‰æ™¯ç›®æ ‡ï¼ŒåŒæ—¶é€šè¿‡åˆ©ç”¨é›·è¾¾çš„å››ä¸ªç»´åº¦æ•°æ®æä¾›åŠ¨æ€ä¿¡æ¯æ¥é”šå®šå¹¶ä¼˜åŒ–é«˜æ–¯åˆ†å¸ƒã€‚é€šè¿‡ä¸€ç³»åˆ—çš„æ­¥éª¤æ„å»ºå’Œä¼˜åŒ–é«˜æ–¯åœºå¹¶å®ç°å±‚æ¬¡åŒ–æ£€æµ‹ç»“æœï¼Œä½¿é›·è¾¾èƒ½å¤Ÿé«˜æ•ˆå‡†ç¡®æ„ŸçŸ¥è½¦è¾†å‘¨å›´çš„çœŸå®ä¸–ç•Œã€‚ç ”ç©¶è¡¨æ˜RaGSåœ¨å¤šåœºæ™¯å®éªŒä¸­å‡è¡¨ç°ä¼˜è¶Šã€‚å³å°†å…¬å¼€æºä»£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä¸€ã€ç°æœ‰è‡ªåŠ¨é©¾é©¶ä¼ æ„Ÿå™¨é¢ä¸´æŒ‘æˆ˜</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19856">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-364b475b096b4476d60218e269bb6b3e" align="middle">
<img src="https://picx.zhimg.com/v2-f8576dd5954be48e07b533aea27d3fd4" align="middle">
<img src="https://picx.zhimg.com/v2-f176312e5264a949b42d40952e07db91" align="middle">
<img src="https://picx.zhimg.com/v2-951a5e50ce0cd139eabe8a5b0349e736" align="middle">
<img src="https://picx.zhimg.com/v2-0798127e0842b97ad42b6e83464f770c" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Seg2Any-Open-set-Segmentation-Mask-to-Image-Generation-with-Precise-Shape-and-Semantic-Control"><a href="#Seg2Any-Open-set-Segmentation-Mask-to-Image-Generation-with-Precise-Shape-and-Semantic-Control" class="headerlink" title="Seg2Any: Open-set Segmentation-Mask-to-Image Generation with Precise Shape and Semantic Control"></a>Seg2Any: Open-set Segmentation-Mask-to-Image Generation with Precise Shape and Semantic Control</h2><p><strong>Authors:Danfeng Li, Hui Zhang, Sheng Wang, Jiacheng Li, Zuxuan Wu</strong></p>
<p>Despite recent advances in diffusion models, top-tier text-to-image (T2I) models still struggle to achieve precise spatial layout control, i.e. accurately generating entities with specified attributes and locations. Segmentation-mask-to-image (S2I) generation has emerged as a promising solution by incorporating pixel-level spatial guidance and regional text prompts. However, existing S2I methods fail to simultaneously ensure semantic consistency and shape consistency. To address these challenges, we propose Seg2Any, a novel S2I framework built upon advanced multimodal diffusion transformers (e.g. FLUX). First, to achieve both semantic and shape consistency, we decouple segmentation mask conditions into regional semantic and high-frequency shape components. The regional semantic condition is introduced by a Semantic Alignment Attention Mask, ensuring that generated entities adhere to their assigned text prompts. The high-frequency shape condition, representing entity boundaries, is encoded as an Entity Contour Map and then introduced as an additional modality via multi-modal attention to guide image spatial structure. Second, to prevent attribute leakage across entities in multi-entity scenarios, we introduce an Attribute Isolation Attention Mask mechanism, which constrains each entityâ€™s image tokens to attend exclusively to themselves during image self-attention. To support open-set S2I generation, we construct SACap-1M, a large-scale dataset containing 1 million images with 5.9 million segmented entities and detailed regional captions, along with a SACap-Eval benchmark for comprehensive S2I evaluation. Extensive experiments demonstrate that Seg2Any achieves state-of-the-art performance on both open-set and closed-set S2I benchmarks, particularly in fine-grained spatial and attribute control of entities.</p>
<blockquote>
<p>å°½ç®¡æ‰©æ•£æ¨¡å‹æœ€è¿‘æœ‰è¿›å±•ï¼Œä½†é¡¶å°–çš„æ–‡ç”Ÿå›¾ï¼ˆT2Iï¼‰æ¨¡å‹ä»ç„¶éš¾ä»¥å®ç°å¯¹ç²¾ç¡®ç©ºé—´å¸ƒå±€çš„æ§åˆ¶ï¼Œå³å‡†ç¡®ç”Ÿæˆå…·æœ‰æŒ‡å®šå±æ€§å’Œä½ç½®çš„å®ä½“ã€‚æ©è†œåˆ°å›¾åƒï¼ˆS2Iï¼‰çš„ç”Ÿæˆæ–¹æ³•é€šè¿‡èå…¥åƒç´ çº§ç©ºé—´æŒ‡å¯¼å’ŒåŒºåŸŸæ–‡æœ¬æç¤ºï¼Œå·²æˆä¸ºä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„S2Iæ–¹æ³•æ— æ³•åŒæ—¶ç¡®ä¿è¯­ä¹‰ä¸€è‡´æ€§å’Œå½¢çŠ¶ä¸€è‡´æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Seg2Anyï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹S2Iæ¡†æ¶ï¼Œå»ºç«‹åœ¨å…ˆè¿›çš„å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨ï¼ˆå¦‚FLUXï¼‰ä¹‹ä¸Šã€‚é¦–å…ˆï¼Œä¸ºäº†å®ç°è¯­ä¹‰å’Œå½¢çŠ¶çš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å°†åˆ†å‰²æ©è†œæ¡ä»¶åˆ†è§£ä¸ºåŒºåŸŸè¯­ä¹‰å’Œé«˜é¢‘å½¢çŠ¶ç»„ä»¶ã€‚é€šè¿‡è¯­ä¹‰å¯¹é½æ³¨æ„åŠ›æ©è†œå¼•å…¥åŒºåŸŸè¯­ä¹‰æ¡ä»¶ï¼Œç¡®ä¿ç”Ÿæˆçš„å®ä½“ç¬¦åˆå…¶åˆ†é…åˆ°çš„æ–‡æœ¬æç¤ºã€‚ä»£è¡¨å®ä½“è¾¹ç•Œçš„é«˜é¢‘å½¢çŠ¶æ¡ä»¶è¢«ç¼–ç ä¸ºå®ä½“è½®å»“å›¾ï¼Œç„¶åä½œä¸ºå¤šæ¨¡æ€æ³¨æ„åŠ›çš„ä¸€ç§é¢å¤–æ¨¡æ€å¼•å…¥ï¼Œä»¥æŒ‡å¯¼å›¾åƒçš„ç©ºé—´ç»“æ„ã€‚å…¶æ¬¡ï¼Œä¸ºäº†é˜²æ­¢å¤šå®ä½“åœºæ™¯ä¸­å±æ€§åœ¨å®ä½“ä¹‹é—´çš„æ³„éœ²ï¼Œæˆ‘ä»¬å¼•å…¥äº†å±æ€§éš”ç¦»æ³¨æ„åŠ›æ©è†œæœºåˆ¶ï¼Œè¯¥æœºåˆ¶çº¦æŸæ¯ä¸ªå®ä½“çš„å›¾åƒä»¤ç‰Œåœ¨å›¾åƒè‡ªæ³¨æ„åŠ›æœŸé—´åªä¸“æ³¨äºè‡ªèº«ã€‚ä¸ºäº†æ”¯æŒå¼€æ”¾é›†S2Iç”Ÿæˆï¼Œæˆ‘ä»¬æ„å»ºäº†SACap-1Mï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«100ä¸‡å¼ å›¾åƒã€590ä¸‡ä¸ªåˆ†å‰²å®ä½“å’Œè¯¦ç»†åŒºåŸŸæè¿°çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä»¥åŠç”¨äºå…¨é¢S2Iè¯„ä»·çš„SACap-EvalåŸºå‡†æµ‹è¯•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSeg2Anyåœ¨å¼€æ”¾é›†å’Œå°é—­é›†çš„S2IåŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å®ä½“çš„ç²¾ç»†ç²’åº¦ç©ºé—´å’Œå±æ€§æ§åˆ¶æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.00596v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å…´çš„S2Iæ¡†æ¶Seg2Anyï¼Œæ—¨åœ¨è§£å†³é¡¶çº§æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹åœ¨ç©ºé—´å¸ƒå±€æ§åˆ¶ä¸Šçš„ä¸è¶³ã€‚å®ƒé€šè¿‡å¼•å…¥å¤šé‡æœºåˆ¶å¦‚è¯­ä¹‰å¯¹é½æ³¨æ„åŠ›æ©ç ã€å®ä½“è½®å»“åœ°å›¾å’Œå¤šæ¨¡æ€æ³¨æ„åŠ›ï¼Œå®ç°äº†è¯­ä¹‰å’Œå½¢çŠ¶çš„ä¸€è‡´æ€§ã€‚åŒæ—¶ï¼Œä¸ºé˜²èŒƒå¤šå®ä½“åœºæ™¯ä¸­å±æ€§æ³„éœ²ï¼Œå¼•å…¥å±æ€§éš”ç¦»æ³¨æ„åŠ›æ©ç æœºåˆ¶ã€‚Seg2Anyåœ¨å¼€æ”¾å’Œå°é—­é›†åˆçš„S2IåŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œå°¤å…¶åœ¨å®ä½“çš„ç²¾ç»†ç©ºé—´æ§åˆ¶æ–¹é¢è¡¨ç°çªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Seg2Anyæ¡†æ¶æ—¨åœ¨è§£å†³é¡¶çº§æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹åœ¨ç©ºé—´å¸ƒå±€æ§åˆ¶ä¸Šçš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡å¼•å…¥å¤šé‡æœºåˆ¶å¦‚è¯­ä¹‰å¯¹é½æ³¨æ„åŠ›æ©ç å’Œå®ä½“è½®å»“åœ°å›¾ï¼Œå®ç°è¯­ä¹‰å’Œå½¢çŠ¶çš„ä¸€è‡´æ€§ã€‚</li>
<li>å¼•å…¥å±æ€§éš”ç¦»æ³¨æ„åŠ›æ©ç æœºåˆ¶ï¼Œé˜²æ­¢å¤šå®ä½“åœºæ™¯ä¸­å±æ€§æ³„éœ²ã€‚</li>
<li>æ„å»ºå¤§å‹æ•°æ®é›†SACap-1Må’ŒSACap-EvalåŸºå‡†æµ‹è¯•ï¼Œä»¥æ”¯æŒå¼€æ”¾é›†S2Iç”Ÿæˆå’Œå…¨é¢S2Iè¯„ä¼°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.00596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-95e2208ed79d517d3abc5144763bb511" align="middle">
<img src="https://picx.zhimg.com/v2-425cd00ada271bf2518d97c330d8b2c4" align="middle">
<img src="https://picx.zhimg.com/v2-7480047e8dd0565825522e8ffc48ec0c" align="middle">
<img src="https://picx.zhimg.com/v2-1f41217c3e38a43c1d413405394be04f" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Test-Time-Adaptation-of-Vision-Language-Models-for-Open-Vocabulary-Semantic-Segmentation"><a href="#Test-Time-Adaptation-of-Vision-Language-Models-for-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation"></a>Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation</h2><p><strong>Authors:Mehrdad Noori, David Osowiechi, Gustavo Adolfo Vargas Hakim, Ali Bahri, Moslem Yazdanpanah, Sahar Dastani, Farzad Beizaee, Ismail Ben Ayed, Christian Desrosiers</strong></p>
<p>Recently, test-time adaptation has attracted wide interest in the context of vision-language models for image classification. However, to the best of our knowledge, the problem is completely overlooked in dense prediction tasks such as Open-Vocabulary Semantic Segmentation (OVSS). In response, we propose a novel TTA method tailored to adapting VLMs for segmentation during test time. Unlike TTA methods for image classification, our Multi-Level and Multi-Prompt (MLMP) entropy minimization integrates features from intermediate vision-encoder layers and is performed with different text-prompt templates at both the global CLS token and local pixel-wise levels. Our approach could be used as plug-and-play for any segmentation network, does not require additional training data or labels, and remains effective even with a single test sample. Furthermore, we introduce a comprehensive OVSS TTA benchmark suite, which integrates a rigorous evaluation protocol, nine segmentation datasets, 15 common synthetic corruptions, and additional real and rendered domain shifts, \textbf{with a total of 87 distinct test scenarios}, establishing a standardized and comprehensive testbed for future TTA research in open-vocabulary segmentation. Our experiments on this suite demonstrate that our segmentation-tailored method consistently delivers significant gains over direct adoption of TTA classification baselines. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/dosowiechi/MLMP">https://github.com/dosowiechi/MLMP</a>.</p>
<blockquote>
<p>è¿‘æœŸï¼Œæµ‹è¯•æ—¶é€‚åº”ï¼ˆTest-Time Adaptationï¼ŒTTAï¼‰åœ¨å›¾åƒåˆ†ç±»çš„è§†è§‰è¯­è¨€æ¨¡å‹èƒŒæ™¯ä¸‹å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¯¥é—®é¢˜åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ï¼ˆå¦‚å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰ï¼‰ä¸­å®Œå…¨è¢«å¿½ç•¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„TTAæ–¹æ³•ï¼Œä¸“é—¨ç”¨äºåœ¨æµ‹è¯•æ—¶é€‚åº”ç”¨äºåˆ†å‰²çš„VLMsã€‚ä¸ç”¨äºå›¾åƒåˆ†ç±»çš„TTAæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„å¤šå±‚æ¬¡å¤šæç¤ºï¼ˆMulti-Level and Multi-Promptï¼ŒMLMPï¼‰ç†µæœ€å°åŒ–ç»“åˆäº†ä¸­é—´è§†è§‰ç¼–ç å™¨å±‚çš„ç‰¹å¾ï¼Œå¹¶åœ¨å…¨å±€CLSæ ‡è®°å’Œå±€éƒ¨åƒç´ çº§ä½¿ç”¨ä¸åŒçš„æ–‡æœ¬æç¤ºæ¨¡æ¿è¿›è¡Œæ“ä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä½œä¸ºä»»ä½•åˆ†å‰²ç½‘ç»œçš„å³æ’å³ç”¨å·¥å…·ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæ•°æ®æˆ–æ ‡ç­¾ï¼Œå³ä½¿åœ¨å•ä¸ªæµ‹è¯•æ ·æœ¬ä¸Šä¹Ÿèƒ½ä¿æŒæœ‰æ•ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„OVSS TTAåŸºå‡†å¥—ä»¶ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸¥æ ¼çš„è¯„ä¼°åè®®ã€ä¹ä¸ªåˆ†å‰²æ•°æ®é›†ã€15ç§å¸¸è§çš„åˆæˆè…èš€ï¼Œä»¥åŠé¢å¤–çš„çœŸå®å’Œæ¸²æŸ“åŸŸåç§»ï¼Œ\textbf{æ€»å…±æœ‰87ç§ä¸åŒçš„æµ‹è¯•åœºæ™¯}ï¼Œä¸ºæœªæ¥å¼€æ”¾è¯æ±‡åˆ†å‰²çš„TTAç ”ç©¶å»ºç«‹äº†æ ‡å‡†åŒ–å’Œå…¨é¢çš„æµ‹è¯•å¹³å°ã€‚åœ¨æ­¤å¥—ä»¶ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„é’ˆå¯¹åˆ†å‰²çš„æ–¹æ³•åœ¨ç›´æ¥é‡‡ç”¨TTAåˆ†ç±»åŸºçº¿æ—¶å§‹ç»ˆå®ç°äº†æ˜¾è‘—çš„æ”¶ç›Šã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/dosowiechi/MLMP%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/dosowiechi/MLMPä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.21844v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰ä»»åŠ¡ä¸­çš„æµ‹è¯•æ—¶é—´è‡ªé€‚åº”é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæ–°å‹çš„å¤šå±‚æ¬¡å¤šæç¤ºï¼ˆMLMPï¼‰ç†µæœ€å°åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ä¸­é—´è§†è§‰ç¼–ç å™¨å±‚çš„ç‰¹å¾ï¼Œå¹¶åœ¨å…¨å±€CLSæ ‡è®°å’Œå±€éƒ¨åƒç´ çº§ä½¿ç”¨ä¸åŒçš„æ–‡æœ¬æç¤ºæ¨¡æ¿è¿›è¡Œã€‚æ­¤æ–¹æ³•å¯ä½œä¸ºä»»ä½•åˆ†å‰²ç½‘ç»œçš„å³æ’å³ç”¨æ¨¡å—ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæ•°æ®æˆ–æ ‡ç­¾ï¼Œç”šè‡³åœ¨å•ä¸ªæµ‹è¯•æ ·æœ¬ä¸Šä¹Ÿèƒ½ä¿æŒæœ‰æ•ˆã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„OVSS TTAåŸºå‡†å¥—ä»¶ï¼Œä¸ºæœªæ¥TTAç ”ç©¶æä¾›äº†æ ‡å‡†åŒ–å’Œå…¨é¢çš„æµ‹è¯•å¹³å°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åˆ†å‰²ä»»åŠ¡ä¸Šç›¸å¯¹äºç›´æ¥é‡‡ç”¨TTAåˆ†ç±»åŸºå‡†çº¿å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶é—´è‡ªé€‚åº”ï¼ˆTTAï¼‰åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å›¾åƒåˆ†ç±»ä¸­å·²å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡å¦‚å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰ä¸­å´è¢«å¿½è§†ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„TTAæ–¹æ³•ï¼Œåä¸ºå¤šå±‚æ¬¡å¤šæç¤ºï¼ˆMLMPï¼‰ç†µæœ€å°åŒ–ï¼Œä¸“é—¨ç”¨äºæµ‹è¯•æ—¶åœ¨è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­é€‚åº”è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</li>
<li>MLMPæ–¹æ³•ç»“åˆäº†ä¸­é—´è§†è§‰ç¼–ç å™¨å±‚çš„ç‰¹å¾ï¼Œå¹¶åœ¨å…¨å±€å’Œå±€éƒ¨çº§åˆ«ä½¿ç”¨ä¸åŒçš„æ–‡æœ¬æç¤ºæ¨¡æ¿è¿›è¡Œã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä½œä¸ºä»»ä½•åˆ†å‰²ç½‘ç»œçš„å³æ’å³ç”¨æ¨¡å—ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒæ•°æ®æˆ–æ ‡ç­¾ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªå…¨é¢çš„OVSS TTAåŸºå‡†å¥—ä»¶ï¼ŒåŒ…å«ä¸¥æ ¼çš„è¯„ä¼°åè®®ã€ä¹ä¸ªåˆ†å‰²æ•°æ®é›†ã€15ç§å¸¸è§çš„åˆæˆè…èš€ä»¥åŠé¢å¤–çš„çœŸå®å’Œæ¸²æŸ“åŸŸåç§»ï¼Œæ€»å…±æœ‰87ç§ä¸åŒçš„æµ‹è¯•åœºæ™¯ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œå¯¹äºæ‰€æå‡ºçš„åˆ†å‰²ä»»åŠ¡ï¼Œè¯¥æ–¹æ³•ç›¸å¯¹äºç›´æ¥é‡‡ç”¨TTAåˆ†ç±»åŸºå‡†çº¿å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.21844">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-09323b8970d30b0c3c21a435dd6e6adf" align="middle">
<img src="https://picx.zhimg.com/v2-914294a061c5ef2a12563bad6b5cb0ae" align="middle">
<img src="https://picx.zhimg.com/v2-d82b2dc3f0877abc496c411cd74c0574" align="middle">
<img src="https://picx.zhimg.com/v2-3d701f4239ce812be86f17558359ec79" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="RAFT-â€“-A-Domain-Adaptation-Framework-for-RGB-LiDAR-Semantic-Segmentation"><a href="#RAFT-â€“-A-Domain-Adaptation-Framework-for-RGB-LiDAR-Semantic-Segmentation" class="headerlink" title="RAFT â€“ A Domain Adaptation Framework for RGB &amp; LiDAR Semantic Segmentation"></a>RAFT â€“ A Domain Adaptation Framework for RGB &amp; LiDAR Semantic Segmentation</h2><p><strong>Authors:Edward Humes, Xiaomin Lin, Boxun Hu, Rithvik Jonna, Tinoosh Mohsenin</strong></p>
<p>Image segmentation is a powerful computer vision technique for scene understanding. However, real-world deployment is stymied by the need for high-quality, meticulously labeled datasets. Synthetic data provides high-quality labels while reducing the need for manual data collection and annotation. However, deep neural networks trained on synthetic data often face the Syn2Real problem, leading to poor performance in real-world deployments.   To mitigate the aforementioned gap in image segmentation, we propose RAFT, a novel framework for adapting image segmentation models using minimal labeled real-world data through data and feature augmentations, as well as active learning. To validate RAFT, we perform experiments on the synthetic-to-real â€œSYNTHIA-&gt;Cityscapesâ€ and â€œGTAV-&gt;Cityscapesâ€ benchmarks. We managed to surpass the previous state of the art, HALO. SYNTHIA-&gt;Cityscapes experiences an improvement in mIoU* upon domain adaptation of 2.1%&#x2F;79.9%, and GTAV-&gt;Cityscapes experiences a 0.4%&#x2F;78.2% improvement in mIoU. Furthermore, we test our approach on the real-to-real benchmark of â€œCityscapes-&gt;ACDCâ€, and again surpass HALO, with a gain in mIoU upon adaptation of 1.3%&#x2F;73.2%. Finally, we examine the effect of the allocated annotation budget and various components of RAFT upon the final transfer mIoU.</p>
<blockquote>
<p>å›¾åƒåˆ†å‰²æ˜¯ä¸€ç§ç”¨äºåœºæ™¯ç†è§£çš„å¼ºå¤§è®¡ç®—æœºè§†è§‰æŠ€æœ¯ã€‚ç„¶è€Œï¼Œå®é™…éƒ¨ç½²å—åˆ°éœ€è¦é«˜è´¨é‡ã€ç²¾å¿ƒæ ‡æ³¨çš„æ•°æ®é›†çš„é˜»ç¢ã€‚åˆæˆæ•°æ®æä¾›äº†é«˜è´¨é‡æ ‡ç­¾ï¼ŒåŒæ—¶å‡å°‘äº†æ‰‹åŠ¨æ•°æ®æ”¶é›†å’Œæ ‡æ³¨çš„éœ€æ±‚ã€‚ç„¶è€Œï¼Œåœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„æ·±åº¦ç¥ç»ç½‘ç»œå¸¸å¸¸é¢ä¸´Syn2Realé—®é¢˜ï¼Œå¯¼è‡´åœ¨å®é™…éƒ¨ç½²ä¸­çš„æ€§èƒ½ä¸ä½³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.04529v4">PDF</a> Submitted to RA-L</p>
<p><strong>Summary</strong><br>     å›¾åƒåˆ†å‰²æŠ€æœ¯æ˜¯ä¸€ç§å¼ºå¤§çš„è®¡ç®—æœºè§†è§‰åœºæ™¯ç†è§£æŠ€æœ¯ï¼Œä½†å…¶åœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²å—é™äºé«˜è´¨é‡ã€ç²¾ç»†æ ‡æ³¨çš„æ•°æ®é›†çš„éœ€æ±‚ã€‚åˆæˆæ•°æ®è™½ç„¶å¯ä»¥æä¾›é«˜è´¨é‡æ ‡ç­¾ï¼Œå‡å°‘æ‰‹åŠ¨æ•°æ®æ”¶é›†å’Œæ ‡æ³¨çš„éœ€è¦ï¼Œä½†æ·±åº¦ç¥ç»ç½‘ç»œåœ¨åˆæˆæ•°æ®ä¸Šå¸¸å¸¸é¢ä¸´åˆæˆåˆ°ç°å®ï¼ˆSyn2Realï¼‰é—®é¢˜ï¼Œå¯¼è‡´åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²æ—¶è¡¨ç°ä¸ä½³ã€‚ä¸ºç¼“è§£å›¾åƒåˆ†å‰²ä¸­çš„ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RAFTæ¡†æ¶ï¼Œé€šè¿‡æ•°æ®å’Œç‰¹å¾å¢å¼ºä»¥åŠä¸»åŠ¨å­¦ä¹ ï¼Œä½¿ç”¨æœ€å°‘çš„æœ‰æ ‡ç­¾ç°å®ä¸–ç•Œæ•°æ®æ¥é€‚åº”å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒRAFTåœ¨åˆæˆåˆ°ç°å®çš„â€œSYNTHIAâ†’Cityscapesâ€å’Œâ€œGTAVâ†’Cityscapesâ€åŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„æœ€ä½³æ°´å¹³ï¼Œå¹¶è¿›è¡Œäº†è¯¦å°½çš„æ€§èƒ½åˆ†æã€‚æ­¤å¤–ï¼Œåœ¨å®é™…éƒ¨ç½²ä¸­ä¹Ÿè¡¨ç°å‡ºäº†ä¼˜è¶Šæ€§ã€‚è¯¥æ¡†æ¶ä¸ºå›¾åƒåˆ†å‰²æŠ€æœ¯çš„å®é™…åº”ç”¨æä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒåˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„å…³é”®æŠ€æœ¯ï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²å—é™äºæ•°æ®é›†çš„æ ‡æ³¨è´¨é‡ã€‚</li>
<li>åˆæˆæ•°æ®èƒ½å¤Ÿæä¾›é«˜è´¨é‡æ ‡ç­¾ï¼Œå‡å°‘æ‰‹åŠ¨æ•°æ®æ”¶é›†å’Œæ ‡æ³¨çš„éœ€æ±‚ã€‚</li>
<li>æ·±åº¦ç¥ç»ç½‘ç»œåœ¨åˆæˆæ•°æ®ä¸Šå¸¸é¢ä¸´åˆæˆåˆ°ç°å®é—®é¢˜ï¼ˆSyn2Realï¼‰ï¼Œå¯¼è‡´åœ¨ç°å®ä¸–ç•Œè¡¨ç°ä¸ä½³ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºäº†RAFTæ¡†æ¶ï¼Œé€šè¿‡æ•°æ®å’Œç‰¹å¾å¢å¼ºä»¥åŠä¸»åŠ¨å­¦ä¹ é€‚åº”å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚</li>
<li>RAFTåœ¨åˆæˆåˆ°ç°å®çš„åŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„æœ€ä½³æ°´å¹³ï¼Œå¦‚â€œSYNTHIAâ†’Cityscapesâ€å’Œâ€œGTAVâ†’Cityscapesâ€ã€‚</li>
<li>RAFTåœ¨å®é™…éƒ¨ç½²ä¸­ä¹Ÿè¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œä¾‹å¦‚åœ¨â€œCityscapesâ†’ACDCâ€åŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†HALOã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.04529">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-21f3bba7719e093be329a6e850e8eee9" align="middle">
<img src="https://picx.zhimg.com/v2-6c501aa515fcaf836a1149c8fdf87914" align="middle">
<img src="https://picx.zhimg.com/v2-b1ba7f52152a6a5e2d37ec356ecc577f" align="middle">
<img src="https://picx.zhimg.com/v2-27b9dd8ebf56edd683a2c3b41fcee90f" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SM3Det-A-Unified-Model-for-Multi-Modal-Remote-Sensing-Object-Detection"><a href="#SM3Det-A-Unified-Model-for-Multi-Modal-Remote-Sensing-Object-Detection" class="headerlink" title="SM3Det: A Unified Model for Multi-Modal Remote Sensing Object Detection"></a>SM3Det: A Unified Model for Multi-Modal Remote Sensing Object Detection</h2><p><strong>Authors:Yuxuan Li, Xiang Li, Yunheng Li, Yicheng Zhang, Yimian Dai, Qibin Hou, Ming-Ming Cheng, Jian Yang</strong></p>
<p>With the rapid advancement of remote sensing technology, high-resolution multi-modal imagery is now more widely accessible. Conventional Object detection models are trained on a single dataset, often restricted to a specific imaging modality and annotation format. However, such an approach overlooks the valuable shared knowledge across multi-modalities and limits the modelâ€™s applicability in more versatile scenarios. This paper introduces a new task called Multi-Modal Datasets and Multi-Task Object Detection (M2Det) for remote sensing, designed to accurately detect horizontal or oriented objects from any sensor modality. This task poses challenges due to 1) the trade-offs involved in managing multi-modal modelling and 2) the complexities of multi-task optimization. To address these, we establish a benchmark dataset and propose a unified model, SM3Det (Single Model for Multi-Modal datasets and Multi-Task object Detection). SM3Det leverages a grid-level sparse MoE backbone to enable joint knowledge learning while preserving distinct feature representations for different modalities. Furthermore, it integrates a consistency and synchronization optimization strategy using dynamic learning rate adjustment, allowing it to effectively handle varying levels of learning difficulty across modalities and tasks. Extensive experiments demonstrate SM3Detâ€™s effectiveness and generalizability, consistently outperforming specialized models on individual datasets. The code is available at <a target="_blank" rel="noopener" href="https://github.com/zcablii/SM3Det">https://github.com/zcablii/SM3Det</a>.</p>
<blockquote>
<p>éšç€é¥æ„ŸæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œé«˜åˆ†è¾¨ç‡å¤šæ¨¡æ€å›¾åƒç°åœ¨æ›´åŠ æ˜“äºè·å–ã€‚ä¼ ç»Ÿçš„ç›®æ ‡æ£€æµ‹æ¨¡å‹æ˜¯åœ¨å•ä¸€æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œé€šå¸¸å±€é™äºç‰¹å®šçš„æˆåƒæ¨¡å¼å’Œæ³¨é‡Šæ ¼å¼ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¿½ç•¥äº†å¤šæ¨¡æ€ä¹‹é—´å®è´µçš„çŸ¥è¯†å…±äº«ï¼Œå¹¶é™åˆ¶äº†æ¨¡å‹åœ¨æ›´é€šç”¨åœºæ™¯ä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªåä¸ºé¥æ„Ÿå¤šæ¨¡æ€æ•°æ®é›†å’Œå¤šä»»åŠ¡ç›®æ ‡æ£€æµ‹ï¼ˆM2Detï¼‰çš„æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨ä»ä»»ä½•ä¼ æ„Ÿå™¨æ¨¡æ€å‡†ç¡®æ£€æµ‹æ°´å¹³æˆ–å®šå‘ç›®æ ‡ã€‚è¿™é¡¹ä»»åŠ¡ç”±äº1ï¼‰æ¶‰åŠå¤šæ¨¡æ€å»ºæ¨¡çš„æƒè¡¡å’Œ2ï¼‰å¤šä»»åŠ¡ä¼˜åŒ–çš„å¤æ‚æ€§è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªåŸºå‡†æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¨¡å‹SM3Detï¼ˆç”¨äºå¤šæ¨¡æ€æ•°æ®é›†å’Œå¤šä»»åŠ¡ç›®æ ‡æ£€æµ‹çš„å•æ¨¡å‹ï¼‰ã€‚SM3Detåˆ©ç”¨ç½‘æ ¼çº§ç¨€ç–MoEéª¨å¹²ç½‘ï¼Œå®ç°è”åˆçŸ¥è¯†å­¦ä¹ ï¼ŒåŒæ—¶ä¿ç•™ä¸åŒæ¨¡æ€çš„ç‹¬ç‰¹ç‰¹å¾è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œå®ƒé‡‡ç”¨ä¸€è‡´æ€§åŒæ­¥ä¼˜åŒ–ç­–ç•¥ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†ä¸åŒæ¨¡æ€å’Œä»»åŠ¡ä¹‹é—´ä¸åŒç¨‹åº¦çš„å­¦ä¹ éš¾åº¦ã€‚å¤§é‡å®éªŒè¯æ˜äº†SM3Detçš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼Œåœ¨å•ä¸ªæ•°æ®é›†ä¸Šå§‹ç»ˆä¼˜äºä¸“ä¸šæ¨¡å‹ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/zcablii/SM3Det%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/zcablii/SM3Detè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.20665v2">PDF</a> Accepted as Oral in AAAI 2026</p>
<p><strong>Summary</strong><br>     éšç€é¥æ„ŸæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œé«˜åˆ†è¾¨ç‡å¤šæ¨¡æ€å½±åƒç°åœ¨æ›´åŠ æ™®åŠã€‚æœ¬æ–‡æå‡ºäº†é¥æ„Ÿä¸­çš„å¤šæ¨¡æ€æ•°æ®é›†å¤šä»»åŠ¡ç›®æ ‡æ£€æµ‹ï¼ˆM2Detï¼‰æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨å‡†ç¡®æ£€æµ‹ä»»ä½•ä¼ æ„Ÿå™¨æ¨¡æ€çš„æ°´å¹³æˆ–å®šå‘ç›®æ ‡ã€‚ä¸ºåº”å¯¹å¤šæ¨¡æ€å»ºæ¨¡å’Œå¤šä»»åŠ¡ä¼˜åŒ–çš„æŒ‘æˆ˜ï¼Œå»ºç«‹äº†åŸºå‡†æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†ç»Ÿä¸€æ¨¡å‹SM3Detã€‚SM3Detåˆ©ç”¨ç½‘æ ¼çº§ç¨€ç–MoEéª¨å¹²ç½‘å®ç°è”åˆçŸ¥è¯†å­¦ä¹ ï¼ŒåŒæ—¶ä¿ç•™ä¸åŒæ¨¡æ€çš„ç‰¹å®šç‰¹å¾è¡¨ç¤ºã€‚é€šè¿‡åŠ¨æ€è°ƒæ•´å­¦ä¹ é€Ÿç‡ï¼Œé‡‡ç”¨ä¸€è‡´æ€§åŒæ­¥ä¼˜åŒ–ç­–ç•¥ï¼Œæœ‰æ•ˆå¤„ç†ä¸åŒæ¨¡æ€å’Œä»»åŠ¡çš„å­¦ä¹ éš¾åº¦å·®å¼‚ã€‚å®éªŒè¯æ˜SM3Detçš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå‡ä¼˜äºä¸“é¡¹æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜åˆ†è¾¨ç‡å¤šæ¨¡æ€å½±åƒçš„æ™®åŠæ¨åŠ¨äº†é¥æ„Ÿé¢†åŸŸçš„æ–°ä»»åŠ¡â€”â€”å¤šæ¨¡æ€æ•°æ®é›†å¤šä»»åŠ¡ç›®æ ‡æ£€æµ‹ï¼ˆM2Detï¼‰ã€‚</li>
<li>M2Detæ—¨åœ¨å‡†ç¡®æ£€æµ‹ä»»ä½•ä¼ æ„Ÿå™¨æ¨¡æ€çš„æ°´å¹³æˆ–å®šå‘ç›®æ ‡ã€‚</li>
<li>åœ¨å¤„ç†å¤šæ¨¡æ€å»ºæ¨¡å’Œå¤šä»»åŠ¡ä¼˜åŒ–æ—¶é¢ä¸´çš„æŒ‘æˆ˜é€šè¿‡æå‡ºSM3Detæ¨¡å‹å¾—åˆ°è§£å†³ã€‚</li>
<li>SM3Detåˆ©ç”¨ç½‘æ ¼çº§ç¨€ç–MoEéª¨å¹²ç½‘å®ç°è”åˆçŸ¥è¯†å­¦ä¹ ï¼ŒåŒæ—¶è€ƒè™‘ä¸åŒæ¨¡æ€çš„ç‰¹å®šç‰¹å¾ã€‚</li>
<li>SM3Deté€šè¿‡åŠ¨æ€è°ƒæ•´å­¦ä¹ é€Ÿç‡ï¼Œé‡‡ç”¨ä¸€è‡´æ€§åŒæ­¥ä¼˜åŒ–ç­–ç•¥ï¼Œä»¥å¤„ç†ä¸åŒæ¨¡æ€å’Œä»»åŠ¡çš„å­¦ä¹ éš¾åº¦å·®å¼‚ã€‚</li>
<li>å®éªŒè¯æ˜SM3Detåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¸“é¡¹æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.20665">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-051bda8eb6419f586e0f478cb4f4800e" align="middle">
<img src="https://picx.zhimg.com/v2-a7c5129ea608e87b631266492466b9f7" align="middle">
<img src="https://picx.zhimg.com/v2-003855d94e8c653d4cc67b41ec7e129c" align="middle">
<img src="https://picx.zhimg.com/v2-d2f77cd529051c398309e19fc7304feb" align="middle">
<img src="https://picx.zhimg.com/v2-abe2fbf2ec909568455176216a8a6b2e" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MMCL-Correcting-Content-Query-Distributions-for-Improved-Anti-Overlapping-X-Ray-Object-Detection"><a href="#MMCL-Correcting-Content-Query-Distributions-for-Improved-Anti-Overlapping-X-Ray-Object-Detection" class="headerlink" title="MMCL: Correcting Content Query Distributions for Improved Anti-Overlapping X-Ray Object Detection"></a>MMCL: Correcting Content Query Distributions for Improved Anti-Overlapping X-Ray Object Detection</h2><p><strong>Authors:Mingyuan Li, Tong Jia, Hui Lu, Hao Wang, Bowen Ma, Shiyi Guo, Shuyang Lin, Dongyue Chen, Haoran Wang, Baosheng Yu</strong></p>
<p>Unlike natural images with occlusion-based overlap, X-ray images exhibit depth-induced superimposition and semi-transparent appearances, where objects at different depths overlap and their features blend together. These characteristics demand specialized mechanisms to disentangle mixed representations between target objects (e.g., prohibited items) and irrelevant backgrounds. While recent studies have explored adapting detection transformers (DETR) for anti-overlapping object detection, the importance of well-distributed content queries that represent object hypotheses remains underexplored. In this paper, we introduce a multi-class min-margin contrastive learning (MMCL) framework to correct the distribution of content queries, achieving balanced intra-class diversity and inter-class separability. The framework first groups content queries by object category and then applies two proposed complementary loss components: a multi-class exclusion loss to enhance inter-class separability, and a min-margin clustering loss to encourage intra-class diversity. We evaluate the proposed method on three widely used X-ray prohibited-item detection datasets, PIXray, OPIXray, and PIDray, using two backbone networks and four DETR variants. Experimental results demonstrate that MMCL effectively enhances anti-overlapping object detection and achieves state-of-the-art performance on both datasets. Code will be made publicly available on GitHub.</p>
<blockquote>
<p>ä¸åŒäºåŸºäºé®æŒ¡é‡å çš„è‡ªç„¶å›¾åƒï¼ŒXå°„çº¿å›¾åƒè¡¨ç°å‡ºæ·±åº¦å¼•èµ·çš„å åŠ å’ŒåŠé€æ˜å¤–è§‚ï¼Œä¸åŒæ·±åº¦çš„ç‰©ä½“å½¼æ­¤é‡å ï¼Œå…¶ç‰¹å¾èåˆåœ¨ä¸€èµ·ã€‚è¿™äº›ç‰¹ç‚¹éœ€è¦ä¸“é—¨çš„æœºåˆ¶æ¥è§£å¼€ç›®æ ‡å¯¹è±¡ï¼ˆå¦‚è¿ç¦ç‰©å“ï¼‰å’Œæ— å…³èƒŒæ™¯ä¹‹é—´çš„æ··åˆè¡¨ç¤ºã€‚è™½ç„¶è¿‘æœŸç ”ç©¶å·²ç»æ¢ç´¢äº†æ£€æµ‹å˜å‹å™¨ï¼ˆDETRï¼‰åœ¨é˜²é‡å å¯¹è±¡æ£€æµ‹ä¸­çš„åº”ç”¨ï¼Œä½†ä»£è¡¨å¯¹è±¡å‡è®¾çš„åˆ†å¸ƒå¼å†…å®¹æŸ¥è¯¢çš„é‡è¦æ€§ä»è¢«å¿½è§†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šç±»æœ€å°é—´éš”å¯¹æ¯”å­¦ä¹ ï¼ˆMMCLï¼‰æ¡†æ¶ï¼Œä»¥æ ¡æ­£å†…å®¹æŸ¥è¯¢çš„åˆ†å¸ƒï¼Œå®ç°ç±»å†…å¤šæ ·æ€§å¹³è¡¡å’Œç±»é—´å¯åˆ†æ€§ã€‚è¯¥æ¡†æ¶é¦–å…ˆæŒ‰å¯¹è±¡ç±»åˆ«å¯¹å†…å®¹æŸ¥è¯¢è¿›è¡Œåˆ†ç»„ï¼Œç„¶ååº”ç”¨ä¸¤ä¸ªæå‡ºçš„äº’è¡¥æŸå¤±ç»„ä»¶ï¼šå¤šç±»æ’é™¤æŸå¤±ï¼Œä»¥å¢å¼ºç±»é—´å¯åˆ†æ€§ï¼Œå’Œæœ€å°é—´éš”èšç±»æŸå¤±ï¼Œä»¥ä¿ƒè¿›ç±»å†…å¤šæ ·æ€§ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„Xå°„çº¿è¿ç¦ç‰©å“æ£€æµ‹æ•°æ®é›†PIXrayã€OPIXrayå’ŒPIDrayä¸Šè¯„ä¼°äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼Œä½¿ç”¨äº†ä¸¤ä¸ªä¸»å¹²ç½‘ç»œå’Œå››ä¸ªDETRå˜ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMMCLæœ‰æ•ˆåœ°æé«˜äº†é˜²é‡å å¯¹è±¡æ£€æµ‹æ€§èƒ½ï¼Œå¹¶åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å°†åœ¨GitHubä¸Šå…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.03176v2">PDF</a> 16 pages,8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šç±»æœ€å°è¾¹è·å¯¹æ¯”å­¦ä¹ ï¼ˆMMCLï¼‰çš„æ¡†æ¶ï¼Œç”¨äºæ”¹å–„å†…å®¹æŸ¥è¯¢çš„åˆ†å¸ƒï¼Œå®ç°ç±»å†…å¤šæ ·æ€§å’Œç±»é—´å¯åˆ†æ€§çš„å¹³è¡¡ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†ç»„å†…å®¹æŸ¥è¯¢å’Œé‡‡ç”¨ä¸¤ç§äº’è¡¥çš„æŸå¤±ç»„ä»¶â€”â€”å¤šç±»æ’é™¤æŸå¤±å’Œæœ€å°è¾¹è·èšç±»æŸå¤±ï¼Œæ¥æå‡åé‡å ç‰©ä½“çš„æ£€æµ‹æ€§èƒ½ã€‚åœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„Xå…‰è¿ç¦ç‰©å“æ£€æµ‹æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMMCLæœ‰æ•ˆåœ°å¢å¼ºäº†åé‡å ç‰©ä½“æ£€æµ‹æ€§èƒ½ï¼Œè¾¾åˆ°äº†å…ˆè¿›çš„è¡¨ç°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Xå…‰å›¾åƒå…·æœ‰æ·±åº¦å¼•èµ·çš„å åŠ å’ŒåŠé€æ˜å¤–è§‚ï¼Œéœ€è¦ç‰¹æ®Šæœºåˆ¶æ¥è§£å¼€ç›®æ ‡ç‰©ä½“ä¸æ— å…³èƒŒæ™¯ä¹‹é—´çš„æ··åˆè¡¨ç¤ºã€‚</li>
<li>è¿‘æœŸç ”ç©¶å·²æ¢ç´¢äº†ä½¿ç”¨æ£€æµ‹å˜å‹å™¨ï¼ˆDETRï¼‰è¿›è¡Œåé‡å ç‰©ä½“æ£€æµ‹ï¼Œä½†å†…å®¹æŸ¥è¯¢çš„åˆ†å¸ƒé‡è¦æ€§ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>æœ¬æ–‡å¼•å…¥äº†å¤šç±»æœ€å°è¾¹è·å¯¹æ¯”å­¦ä¹ ï¼ˆMMCLï¼‰æ¡†æ¶ï¼Œä»¥çº æ­£å†…å®¹æŸ¥è¯¢çš„åˆ†å¸ƒï¼Œå®ç°ç±»å†…å¤šæ ·æ€§å’Œç±»é—´å¯åˆ†æ€§çš„å¹³è¡¡ã€‚</li>
<li>MMCLæ¡†æ¶é€šè¿‡åˆ†ç»„å†…å®¹æŸ¥è¯¢å¹¶é‡‡ç”¨å¤šç±»æ’é™¤æŸå¤±å’Œæœ€å°è¾¹è·èšç±»æŸå¤±æ¥æé«˜åé‡å ç‰©ä½“æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMMCLåœ¨Xå…‰è¿ç¦ç‰©å“æ£€æµ‹æ–¹é¢è¾¾åˆ°äº†å…ˆè¿›çš„è¡¨ç°æ°´å¹³ï¼Œé€‚ç”¨äºå¤šç§æ•°æ®é›†å’Œbackboneç½‘ç»œä»¥åŠDETRå˜ä½“ã€‚</li>
<li>MMCLæ¡†æ¶å°†åœ¨GitHubä¸Šå…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.03176">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf3cfb5b2278323dd495ff44acecb7e3" align="middle">
<img src="https://picx.zhimg.com/v2-d7d2445fbe3212ec250a5225fc820871" align="middle">
<img src="https://picx.zhimg.com/v2-30eab183784635ced69888e9787a116a" align="middle">
<img src="https://picx.zhimg.com/v2-176be2b16cbb0d854bf7204566b669ee" align="middle">
<img src="https://picx.zhimg.com/v2-6fb1c7cb6e50e959db419df91e3f0de1" align="middle">
<img src="https://picx.zhimg.com/v2-fc81bb0e16cf0d933d46363b61f2e459" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1d66bf749c3489cdcfcc5046a33ee5ca" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  Hybrid second-order gradient histogram based global low-rank sparse regression for robust face recognition
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c56eaef7f8ea845a77f30e0137d04b73" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  LampQ Towards Accurate Layer-wise Mixed Precision Quantization for Vision Transformers
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32714.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
