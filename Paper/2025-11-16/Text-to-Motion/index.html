<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Text-to-Motion">
    <meta name="description" content="Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  Time-to-Move Training-Free Motion Controlled Video Generation via Dual-Clock Denoising">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Text-to-Motion | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-3c65115714e83519b9c6ee5a8950d323')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Text-to-Motion</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Text-to-Motion/">
                                <span class="chip bg-color">Text-to-Motion</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                Text-to-Motion
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    72 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-16-æ›´æ–°"><a href="#2025-11-16-æ›´æ–°" class="headerlink" title="2025-11-16 æ›´æ–°"></a>2025-11-16 æ›´æ–°</h1><h2 id="Time-to-Move-Training-Free-Motion-Controlled-Video-Generation-via-Dual-Clock-Denoising"><a href="#Time-to-Move-Training-Free-Motion-Controlled-Video-Generation-via-Dual-Clock-Denoising" class="headerlink" title="Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising"></a>Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising</h2><p><strong>Authors:Assaf Singer, Noam Rotstein, Amir Mann, Ron Kimmel, Or Litany</strong></p>
<p>Diffusion-based video generation can create realistic videos, yet existing image- and text-based conditioning fails to offer precise motion control. Prior methods for motion-conditioned synthesis typically require model-specific fine-tuning, which is computationally expensive and restrictive. We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models. Our key insight is to use crude reference animations obtained through user-friendly manipulations such as cut-and-drag or depth-based reprojection. Motivated by SDEditâ€™s use of coarse layout cues for image editing, we treat the crude animations as coarse motion cues and adapt the mechanism to the video domain. We preserve appearance with image conditioning and introduce dual-clock denoising, a region-dependent strategy that enforces strong alignment in motion-specified regions while allowing flexibility elsewhere, balancing fidelity to user intent with natural dynamics. This lightweight modification of the sampling process incurs no additional training or runtime cost and is compatible with any backbone. Extensive experiments on object and camera motion benchmarks show that TTM matches or exceeds existing training-based baselines in realism and motion control. Beyond this, TTM introduces a unique capability: precise appearance control through pixel-level conditioning, exceeding the limits of text-only prompting. Visit our project page for video examples and code: <a target="_blank" rel="noopener" href="https://time-to-move.github.io/">https://time-to-move.github.io/</a>.</p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„è§†é¢‘ç”Ÿæˆå¯ä»¥åˆ›å»ºé€¼çœŸçš„è§†é¢‘ï¼Œä½†ç°æœ‰çš„å›¾åƒå’Œæ–‡å­—æ¡ä»¶æ— æ³•æä¾›ç²¾ç¡®çš„è¿åŠ¨æ§åˆ¶ã€‚ä»¥å‰çš„æ–¹æ³•å¯¹äºè¿åŠ¨æ¡ä»¶ä¸‹çš„åˆæˆé€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¿™æ—¢è€—è´¹è®¡ç®—èµ„æºåˆå¾ˆå—é™ã€‚æˆ‘ä»¬å¼•å…¥äº†Time-to-Moveï¼ˆTTMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„å³æ’å³ç”¨æ¡†æ¶ï¼Œä½¿ç”¨å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰æ‰©æ•£æ¨¡å‹è¿›è¡Œè¿åŠ¨å’Œå¤–è§‚æ§åˆ¶çš„è§†é¢‘ç”Ÿæˆã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ä½¿ç”¨é€šè¿‡ç”¨æˆ·å‹å¥½çš„æ“ä½œï¼ˆå¦‚å‰ªåˆ‡å’Œæ‹–åŠ¨æˆ–åŸºäºæ·±åº¦çš„é‡æ–°æŠ•å½±ï¼‰è·å¾—çš„ç²—ç•¥å‚è€ƒåŠ¨ç”»ã€‚å—SDEditä½¿ç”¨ç²—ç•¥å¸ƒå±€çº¿ç´¢è¿›è¡Œå›¾åƒç¼–è¾‘çš„å¯å‘ï¼Œæˆ‘ä»¬å°†ç²—ç•¥åŠ¨ç”»è§†ä¸ºç²—ç•¥çš„è¿åŠ¨çº¿ç´¢å¹¶å°†å…¶é€‚åº”è§†é¢‘é¢†åŸŸã€‚æˆ‘ä»¬ä¿ç•™å›¾åƒæ¡ä»¶çš„å¤–è²Œï¼Œå¹¶å¼•å…¥åŒæ—¶é’Ÿå»å™ªï¼Œè¿™æ˜¯ä¸€ç§åŒºåŸŸä¾èµ–ç­–ç•¥ï¼Œåœ¨æŒ‡å®šçš„è¿åŠ¨åŒºåŸŸå¼ºåˆ¶ä¸¥æ ¼å¯¹é½ï¼Œè€Œåœ¨å…¶ä»–åœ°æ–¹åˆ™å…è®¸çµæ´»æ€§ï¼Œå¹³è¡¡äº†ç¬¦åˆç”¨æˆ·æ„å›¾çš„è‡ªç„¶åŠ¨æ€ã€‚é‡‡æ ·è¿‡ç¨‹çš„è¿™ç§è½»é‡çº§ä¿®æ”¹ä¸ä¼šå¸¦æ¥é¢å¤–çš„è®­ç»ƒæˆ–è¿è¡Œæˆæœ¬ï¼Œå¹¶ä¸”ä¸ä»»ä½•ä¸»å¹²æ¶æ„éƒ½å…¼å®¹ã€‚åœ¨ç‰©ä½“å’Œç›¸æœºè¿åŠ¨åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTTMåœ¨çœŸå®æ„Ÿå’Œè¿åŠ¨æ§åˆ¶æ–¹é¢ä¸ç°æœ‰çš„åŸºäºè®­ç»ƒçš„æ–¹æ³•ç›¸åŒ¹æ•Œæˆ–æ›´å¥½ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒTTMè¿˜å¼•å…¥äº†ä¸€é¡¹ç‹¬ç‰¹çš„èƒ½åŠ›ï¼šé€šè¿‡åƒç´ çº§æ¡ä»¶è¿›è¡Œç²¾ç¡®çš„å¤–è§‚æ§åˆ¶ï¼Œçªç ´äº†ä»…æ–‡æœ¬æç¤ºçš„å±€é™æ€§ã€‚æœ‰å…³è§†é¢‘ç¤ºä¾‹å’Œä»£ç ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://time-to-move.github.io/%E3%80%82">https://time-to-move.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08633v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£çš„è§†é¢‘ç”ŸæˆæŠ€æœ¯èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„è§†é¢‘ï¼Œä½†ç°æœ‰çš„å›¾åƒå’Œæ–‡å­—æ¡ä»¶æ— æ³•æä¾›ç²¾ç¡®çš„è¿åŠ¨æ§åˆ¶ã€‚è¿‡å»çš„æ–¹æ³•éœ€è¦é’ˆå¯¹æ¨¡å‹è¿›è¡Œç‰¹å®šçš„å¾®è°ƒï¼Œè¿™æ—¢è®¡ç®—é‡å¤§åˆæœ‰é™åˆ¶ã€‚æˆ‘ä»¬æ¨å‡ºTime-to-Moveï¼ˆTTMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒã€å³æ’å³ç”¨çš„æ¡†æ¶ï¼Œç”¨äºè¿åŠ¨æ§åˆ¶å’Œå¤–è§‚æ§åˆ¶çš„è§†é¢‘ç”Ÿæˆï¼Œç»“åˆå›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ä½¿ç”¨ç”¨æˆ·å‹å¥½çš„æ“ä½œï¼ˆå¦‚å‰ªåˆ‡å’Œæ‹–åŠ¨æˆ–æ·±åº¦é‡æ–°æŠ•å½±ï¼‰è·å¾—çš„ç²—ç•¥å‚è€ƒåŠ¨ç”»ã€‚å—SDEditä½¿ç”¨ç²—ç•¥å¸ƒå±€çº¿ç´¢è¿›è¡Œå›¾åƒç¼–è¾‘çš„å¯å‘ï¼Œæˆ‘ä»¬å°†ç²—ç•¥åŠ¨ç”»è§†ä¸ºç²—ç•¥è¿åŠ¨çº¿ç´¢å¹¶é€‚åº”è§†é¢‘é¢†åŸŸã€‚æˆ‘ä»¬ä¿ç•™å›¾åƒæ¡ä»¶ä»¥ä¿æŒå¤–è§‚ï¼Œå¹¶å¼•å…¥åŒæ—¶é’Ÿå»å™ªï¼Œè¿™æ˜¯ä¸€ç§åŒºåŸŸä¾èµ–ç­–ç•¥ï¼Œå¼ºåˆ¶è¿åŠ¨æŒ‡å®šåŒºåŸŸä¸­çš„å¼ºå¯¹é½ï¼ŒåŒæ—¶å…è®¸å…¶ä»–åŒºåŸŸçš„çµæ´»æ€§ï¼Œå¹³è¡¡äº†ç¬¦åˆç”¨æˆ·æ„å›¾çš„è‡ªç„¶åŠ¨æ€ã€‚è¿™ç§é‡‡æ ·è¿‡ç¨‹çš„è½»é‡çº§ä¿®æ”¹æ²¡æœ‰äº§ç”Ÿé¢å¤–çš„è®­ç»ƒæˆ–è¿è¡Œæˆæœ¬ï¼Œå¹¶ä¸”ä¸ä»»ä½•ä¸»å¹²æ¶æ„éƒ½å…¼å®¹ã€‚åœ¨ç‰©ä½“å’Œç›¸æœºè¿åŠ¨åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTTMåœ¨çœŸå®æ„Ÿå’Œè¿åŠ¨æ§åˆ¶æ–¹é¢åŒ¹é…æˆ–è¶…è¿‡ç°æœ‰çš„åŸºäºè®­ç»ƒçš„å¯¹æ ‡ç‰©ã€‚æ­¤å¤–ï¼ŒTTMè¿˜å¼•å…¥äº†ç‹¬ç‰¹çš„ç²¾ç¡®å¤–è§‚æ§åˆ¶èƒ½åŠ›ï¼Œé€šè¿‡åƒç´ çº§çš„æ¡ä»¶è¶…è¶Šæ–‡æœ¬æç¤ºçš„å±€é™æ€§ã€‚æ›´å¤šè§†é¢‘ç¤ºä¾‹å’Œä»£ç è¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š[<a target="_blank" rel="noopener" href="https://time-to-move.github.io/]%E2%80%9D%E3%80%82">https://time-to-move.github.io/]â€ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTMæ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œç”¨äºè¿åŠ¨æ§åˆ¶å’Œå¤–è§‚æ§åˆ¶çš„è§†é¢‘ç”Ÿæˆã€‚</li>
<li>ä½¿ç”¨ç”¨æˆ·å‹å¥½çš„æ“ä½œï¼ˆå¦‚å‰ªåˆ‡å’Œæ‹–åŠ¨ï¼‰è·å¾—çš„ç²—ç•¥å‚è€ƒåŠ¨ç”»ä½œä¸ºè¿åŠ¨çº¿ç´¢ã€‚</li>
<li>ç»“åˆå›¾åƒåˆ°è§†é¢‘çš„æ‰©æ•£æ¨¡å‹ï¼Œå®ç°ç²¾ç¡®çš„è¿åŠ¨æ§åˆ¶ã€‚</li>
<li>é€šè¿‡å›¾åƒæ¡ä»¶ä¿æŒå¤–è§‚ä¸€è‡´æ€§ã€‚</li>
<li>å¼•å…¥åŒæ—¶é’Ÿå»å™ªç­–ç•¥ï¼Œå¹³è¡¡ç”¨æˆ·æ„å›¾çš„è‡ªç„¶åŠ¨æ€ã€‚</li>
<li>é‡‡æ ·è¿‡ç¨‹çš„ä¿®æ”¹æ— éœ€é¢å¤–çš„è®­ç»ƒæˆ–è¿è¡Œæˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5bb797ba48d8833e5ed1396fd0dd4513" align="middle">
<img src="https://picx.zhimg.com/v2-31410977e51dab00aaecf45541224100" align="middle">
<img src="https://picx.zhimg.com/v2-f2ebd79950aee5498274f4f870f4a626" align="middle">
<img src="https://picx.zhimg.com/v2-0e2f5a7518f54169543cf5cee8071c24" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Decomate-Leveraging-Generative-Models-for-Co-Creative-SVG-Animation"><a href="#Decomate-Leveraging-Generative-Models-for-Co-Creative-SVG-Animation" class="headerlink" title="Decomate: Leveraging Generative Models for Co-Creative SVG Animation"></a>Decomate: Leveraging Generative Models for Co-Creative SVG Animation</h2><p><strong>Authors:Jihyeon Park, Jiyoon Myung, Seone Shin, Jungki Son, Joohyung Han</strong></p>
<p>Designers often encounter friction when animating static SVG graphics, especially when the visual structure does not match the desired level of motion detail. Existing tools typically depend on predefined groupings or require technical expertise, which limits designersâ€™ ability to experiment and iterate independently. We present Decomate, a system that enables intuitive SVG animation through natural language. Decomate leverages a multimodal large language model to restructure raw SVGs into semantically meaningful, animation-ready components. Designers can then specify motions for each component via text prompts, after which the system generates corresponding HTML&#x2F;CSS&#x2F;JS animations. By supporting iterative refinement through natural language interaction, Decomate integrates generative AI into creative workflows, allowing animation outcomes to be directly shaped by user intent.</p>
<blockquote>
<p>è®¾è®¡å¸ˆåœ¨åŠ¨ç”»é™æ€SVGå›¾å½¢æ—¶ç»å¸¸ä¼šé‡åˆ°æ‘©æ“¦ï¼Œç‰¹åˆ«æ˜¯å½“è§†è§‰ç»“æ„æ— æ³•è¾¾åˆ°æœŸæœ›çš„è¿åŠ¨ç»†èŠ‚æ°´å¹³æ—¶ã€‚ç°æœ‰å·¥å…·é€šå¸¸ä¾èµ–äºé¢„å…ˆå®šä¹‰çš„åˆ†ç»„æˆ–éœ€è¦ä¸“ä¸šæŠ€æœ¯çŸ¥è¯†ï¼Œè¿™é™åˆ¶äº†è®¾è®¡å¸ˆç‹¬ç«‹è¿›è¡Œå®éªŒå’Œè¿­ä»£çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æ¨å‡ºDecomateç³»ç»Ÿï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€å®ç°ç›´è§‚çš„SVGåŠ¨ç”»ã€‚Decomateåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å°†åŸå§‹SVGé‡ç»„ä¸ºè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰ã€é€‚åˆåŠ¨ç”»çš„ç»„ä»¶ã€‚è®¾è®¡å¸ˆéšåå¯ä»¥é€šè¿‡æ–‡æœ¬æç¤ºä¸ºæ¯ä¸ªç»„ä»¶æŒ‡å®šè¿åŠ¨ï¼Œç³»ç»Ÿç”Ÿæˆç›¸åº”çš„HTML&#x2F;CSS&#x2F;JSåŠ¨ç”»ã€‚é€šè¿‡æ”¯æŒé€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’è¿›è¡Œè¿­ä»£ç»†åŒ–ï¼ŒDecomateå°†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½é›†æˆåˆ°åˆ›æ„å·¥ä½œæµç¨‹ä¸­ï¼Œå…è®¸åŠ¨ç”»ç»“æœç›´æ¥æ ¹æ®ç”¨æˆ·æ„å›¾è¿›è¡Œå¡‘é€ ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06297v1">PDF</a> Accepted at the 1st Workshop on Generative and Protective AI for Content Creation (NeurIPS 2025)</p>
<p><strong>Summary</strong></p>
<p>Decomateç³»ç»Ÿé€šè¿‡è‡ªç„¶è¯­è¨€ç›´è§‚åœ°è¿›è¡ŒSVGåŠ¨ç”»è®¾è®¡ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å°†åŸå§‹SVGé‡ç»„ä¸ºå…·æœ‰è¯­ä¹‰æ„ä¹‰çš„åŠ¨ç”»ç»„ä»¶ã€‚è®¾è®¡å¸ˆå¯é€šè¿‡æ–‡æœ¬æç¤ºä¸ºå„ç»„ä»¶æŒ‡å®šåŠ¨ä½œï¼Œç³»ç»Ÿåˆ™ç”Ÿæˆç›¸åº”çš„HTML&#x2F;CSS&#x2F;JSåŠ¨ç”»ã€‚æ­¤ç³»ç»Ÿæ”¯æŒé€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œå°†ç”Ÿæˆå¼AIèå…¥åˆ›æ„å·¥ä½œæµç¨‹ï¼Œä½¿åŠ¨ç”»æ•ˆæœèƒ½ç›´æ¥ä½“ç°ç”¨æˆ·æ„å›¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Decomateè§£å†³è®¾è®¡å¸ˆåœ¨åŠ¨ç”»åŒ–é™æ€SVGå›¾å½¢æ—¶é‡åˆ°çš„é—®é¢˜ï¼Œå°¤å…¶å½“è§†è§‰ç»“æ„ä¸åŒ¹é…æ‰€éœ€è¿åŠ¨ç»†èŠ‚æ—¶ã€‚</li>
<li>ç°æœ‰å·¥å…·é€šå¸¸ä¾èµ–äºé¢„å®šä¹‰åˆ†ç»„æˆ–éœ€è¦ä¸“ä¸šæŠ€æœ¯ï¼Œé™åˆ¶äº†è®¾è®¡å¸ˆçš„ç‹¬ç«‹å®éªŒå’Œè¿­ä»£èƒ½åŠ›ã€‚</li>
<li>Decomateç³»ç»Ÿé€šè¿‡è‡ªç„¶è¯­è¨€ç›´è§‚åœ°è¿›è¡ŒSVGåŠ¨ç”»è®¾è®¡ã€‚</li>
<li>åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é‡ç»„SVGï¼Œå½¢æˆåŠ¨ç”»å°±ç»ªçš„è¯­ä¹‰ç»„ä»¶ã€‚</li>
<li>è®¾è®¡å¸ˆå¯ä»¥ä¸ºæ¯ä¸ªç»„ä»¶é€šè¿‡æ–‡æœ¬æç¤ºæŒ‡å®šåŠ¨ä½œã€‚</li>
<li>ç³»ç»Ÿæ”¯æŒé€šè¿‡è‡ªç„¶è¯­è¨€äº¤äº’è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06297">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3b617e052ef4b305b725a331277c8a9e" align="middle">
<img src="https://picx.zhimg.com/v2-f1520f1a0a9ff3ab01eee5ee488e692a" align="middle">
<img src="https://picx.zhimg.com/v2-30d4c1df3b2d486e8ae8982d3c75783d" align="middle">
<img src="https://picx.zhimg.com/v2-76a72b7f48d1a5cb8a2c5d164f6a752f" align="middle">
<img src="https://picx.zhimg.com/v2-b35df9532a83d7cfa4b02833447f6699" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Shared-Latent-Representation-for-Joint-Text-to-Audio-Visual-Synthesis"><a href="#Shared-Latent-Representation-for-Joint-Text-to-Audio-Visual-Synthesis" class="headerlink" title="Shared Latent Representation for Joint Text-to-Audio-Visual Synthesis"></a>Shared Latent Representation for Joint Text-to-Audio-Visual Synthesis</h2><p><strong>Authors:Dogucan Yaman, Seymanur Akti, Fevziye Irem Eyiokur, Alexander Waibel</strong></p>
<p>We propose a text-to-talking-face synthesis framework leveraging latent speech representations from HierSpeech++. A Text-to-Vec module generates Wav2Vec2 embeddings from text, which jointly condition speech and face generation. To handle distribution shifts between clean and TTS-predicted features, we adopt a two-stage training: pretraining on Wav2Vec2 embeddings and finetuning on TTS outputs. This enables tight audio-visual alignment, preserves speaker identity, and produces natural, expressive speech and synchronized facial motion without ground-truth audio at inference. Experiments show that conditioning on TTS-predicted latent features outperforms cascaded pipelines, improving both lip-sync and visual realism.</p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨ HierSpeech++ çš„æ½œåœ¨è¯­éŸ³è¡¨å¾çš„æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæ¡†æ¶ã€‚Text-to-Vec æ¨¡å—ä»æ–‡æœ¬ç”Ÿæˆ Wav2Vec2 åµŒå…¥ï¼Œè”åˆæ¡ä»¶è¯­éŸ³å’Œé¢éƒ¨ç”Ÿæˆã€‚ä¸ºäº†å¤„ç†å¹²å‡€å’Œ TTS é¢„æµ‹ç‰¹å¾ä¹‹é—´çš„åˆ†å¸ƒåç§»ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼šé¦–å…ˆåœ¨ Wav2Vec2 åµŒå…¥ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨ TTS è¾“å‡ºä¸Šè¿›è¡Œå¾®è°ƒã€‚è¿™å®ç°äº†éŸ³é¢‘å’Œè§†é¢‘çš„ç´§å¯†å¯¹é½ï¼Œä¿ç•™äº†è¯´è¯è€…çš„èº«ä»½ï¼Œå¹¶åœ¨æ¨ç†æ—¶æ— éœ€çœŸå®éŸ³é¢‘å³å¯ç”Ÿæˆè‡ªç„¶ã€å¯Œæœ‰è¡¨ç°åŠ›çš„è¯­éŸ³å’ŒåŒæ­¥çš„é¢éƒ¨åŠ¨ä½œã€‚å®éªŒè¡¨æ˜ï¼Œä»¥ TTS é¢„æµ‹çš„æ½œåœ¨ç‰¹å¾ä¸ºæ¡ä»¶ä¼˜äºçº§è”ç®¡é“ï¼Œæé«˜äº†å”‡åŒæ­¥å’Œè§†è§‰é€¼çœŸåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05432v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ‘˜è¦åˆ©ç”¨åŸºäº HierSpeech++ çš„æ½œåœ¨è¯­éŸ³è¡¨ç¤ºæå‡ºä¸€ä¸ªæ–‡æœ¬åˆ°å¯¹è¯äººè„¸åˆæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶ä½¿ç”¨æ–‡æœ¬ç”Ÿæˆ Wav2Vec2 åµŒå…¥çš„ Text-to-Vec æ¨¡å—ï¼Œè”åˆæ¡ä»¶è¯­éŸ³å’Œé¢éƒ¨ç”Ÿæˆã€‚ä¸ºè§£å†³å¹²å‡€ç‰¹å¾å’Œ TTS é¢„æµ‹ç‰¹å¾ä¹‹é—´çš„åˆ†å¸ƒè½¬ç§»é—®é¢˜ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼šå…ˆåœ¨ Wav2Vec2 åµŒå…¥ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå†åœ¨ TTS è¾“å‡ºä¸Šè¿›è¡Œå¾®è°ƒã€‚è¿™å®ç°äº†éŸ³é¢‘ä¸è§†è§‰çš„ç´§å¯†å¯¹é½ï¼Œä¿æŒäº†è¯´è¯äººçš„èº«ä»½ï¼Œå¹¶åœ¨æ¨æ–­æ—¶äº§ç”Ÿäº†è‡ªç„¶ã€æœ‰è¡¨ç°åŠ›çš„è¯­éŸ³å’ŒåŒæ­¥çš„é¢éƒ¨åŠ¨ä½œï¼Œæ— éœ€çœŸå®éŸ³é¢‘ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨åŸºäº TTS é¢„æµ‹çš„æ½œåœ¨ç‰¹å¾ä¸Šè¿›è¡Œæ¡ä»¶å¤„ç†ä¼˜äºçº§è”ç®¡é“ï¼Œæé«˜äº†å”‡åŒæ­¥å’Œè§†è§‰é€¼çœŸåº¦ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºåŸºäº HierSpeech++ çš„æ–‡æœ¬åˆ°å¯¹è¯äººè„¸åˆæˆæ¡†æ¶ã€‚</li>
<li>ä½¿ç”¨ Text-to-Vec æ¨¡å—ç”Ÿæˆ Wav2Vec2 åµŒå…¥ï¼Œè”åˆæ¡ä»¶è¯­éŸ³å’Œé¢éƒ¨ç”Ÿæˆã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒå¤„ç†åˆ†å¸ƒè½¬ç§»é—®é¢˜ï¼Œå®ç°äº†éŸ³é¢‘ä¸è§†è§‰çš„ç´§å¯†å¯¹é½ã€‚</li>
<li>æ¡†æ¶èƒ½åœ¨æ¨æ–­æ—¶äº§ç”Ÿè‡ªç„¶ã€æœ‰è¡¨ç°åŠ›çš„è¯­éŸ³å’ŒåŒæ­¥çš„é¢éƒ¨åŠ¨ä½œï¼Œæ— éœ€çœŸå®éŸ³é¢‘ã€‚</li>
<li>ç›¸æ¯”çº§è”ç®¡é“ï¼Œè¯¥æ¡†æ¶åœ¨åŸºäº TTS é¢„æµ‹çš„æ½œåœ¨ç‰¹å¾ä¸Šè¿›è¡Œæ¡ä»¶å¤„ç†è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ¡†æ¶æé«˜äº†å”‡åŒæ­¥å’Œè§†è§‰é€¼çœŸåº¦ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿä¿æŒè¯´è¯äººçš„èº«ä»½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05432">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c65115714e83519b9c6ee5a8950d323" align="middle">
<img src="https://picx.zhimg.com/v2-f982568e9a83572794bc25100699780d" align="middle">
<img src="https://picx.zhimg.com/v2-602a6d16fb84fdd5ea5fe19865ab5950" align="middle">
<img src="https://picx.zhimg.com/v2-f425b469ee440094016269aa003f0966" align="middle">
<img src="https://picx.zhimg.com/v2-e4fe1a1610a500b6f17779546d42e481" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Pressure2Motion-Hierarchical-Motion-Synthesis-from-Ground-Pressure-with-Text-Guidance"><a href="#Pressure2Motion-Hierarchical-Motion-Synthesis-from-Ground-Pressure-with-Text-Guidance" class="headerlink" title="Pressure2Motion: Hierarchical Motion Synthesis from Ground Pressure with Text Guidance"></a>Pressure2Motion: Hierarchical Motion Synthesis from Ground Pressure with Text Guidance</h2><p><strong>Authors:Zhengxuan Li, Qinhui Yang, Yiyu Zhuang, Chuan Guo, Xinxin Zuo, Xiaoxiao Long, Yao Yao, Xun Cao, Qiu Shen, Hao Zhu</strong></p>
<p>We present Pressure2Motion, a novel motion capture algorithm that synthesizes human motion from a ground pressure sequence and text prompt. It eliminates the need for specialized lighting setups, cameras, or wearable devices, making it suitable for privacy-preserving, low-light, and low-cost motion capture scenarios. Such a task is severely ill-posed due to the indeterminate nature of the pressure signals to full-body motion. To address this issue, we introduce Pressure2Motion, a generative model that leverages pressure features as input and utilizes a text prompt as a high-level guiding constraint. Specifically, our model utilizes a dual-level feature extractor that accurately interprets pressure data, followed by a hierarchical diffusion model that discerns broad-scale movement trajectories and subtle posture adjustments. Both the physical cues gained from the pressure sequence and the semantic guidance derived from descriptive texts are leveraged to guide the motion generation with precision. To the best of our knowledge, Pressure2Motion is a pioneering work in leveraging both pressure data and linguistic priors for motion generation, and the established MPL benchmark is the first benchmark for this task. Experiments show our method generates high-fidelity, physically plausible motions, establishing a new state-of-the-art for this task. The codes and benchmarks will be publicly released upon publication.</p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Pressure2Motionï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŠ¨ä½œæ•æ‰ç®—æ³•ï¼Œå®ƒå¯ä»¥ä»åœ°é¢å‹åŠ›åºåˆ—å’Œæ–‡æœ¬æç¤ºä¸­åˆæˆäººç±»åŠ¨ä½œã€‚å®ƒä¸éœ€è¦ç‰¹æ®Šçš„ç…§æ˜è®¾ç½®ã€ç›¸æœºæˆ–å¯ç©¿æˆ´è®¾å¤‡ï¼Œå› æ­¤éå¸¸é€‚åˆéšç§ä¿æŠ¤ã€ä½å…‰ç…§å’Œä½æˆæœ¬çš„åŠ¨æ•åœºæ™¯ã€‚ç”±äºå‹åŠ›ä¿¡å·å¯¹å…¨èº«è¿åŠ¨çš„ä¸ç¡®å®šæ€§ï¼Œæ­¤ä»»åŠ¡è®¾å®šæä¸ºä¸æ˜ç¡®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Pressure2Motionï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒä»¥å‹åŠ›ç‰¹å¾ä¸ºè¾“å…¥ï¼Œå¹¶åˆ©ç”¨æ–‡æœ¬æç¤ºä½œä¸ºé«˜çº§æŒ‡å¯¼çº¦æŸã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é‡‡ç”¨åŒçº§ç‰¹å¾æå–å™¨æ¥å‡†ç¡®è§£é‡Šå‹åŠ›æ•°æ®ï¼Œéšåæ˜¯åˆ†å±‚æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºè¾¨åˆ«å¤§ä½“è¿åŠ¨è½¨è¿¹å’Œç»†å¾®å§¿åŠ¿è°ƒæ•´ã€‚æˆ‘ä»¬ä»å‹åŠ›åºåˆ—ä¸­è·å¾—çš„ç‰©ç†çº¿ç´¢å’Œä»æè¿°æ€§æ–‡æœ¬ä¸­å¾—å‡ºçš„è¯­ä¹‰æŒ‡å¯¼éƒ½è¢«ç”¨æ¥ç²¾ç¡®æŒ‡å¯¼åŠ¨ä½œç”Ÿæˆã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒPressure2Motionæ˜¯ç‡å…ˆåˆ©ç”¨å‹åŠ›æ•°æ®å’Œè¯­è¨€å…ˆéªŒå€¼è¿›è¡ŒåŠ¨ä½œç”Ÿæˆçš„å·¥ä½œï¼Œè€Œå»ºç«‹çš„MPLåŸºå‡†åˆ™æ˜¯è¯¥ä»»åŠ¡çš„é¦–ä¸ªåŸºå‡†ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆäº†é«˜ä¿çœŸã€ç‰©ç†ä¸Šåˆç†çš„åŠ¨ä½œï¼Œä¸ºè¯¥ä»»åŠ¡å»ºç«‹äº†æ–°çš„æŠ€æœ¯é¡¶å°–æ°´å¹³ã€‚ä»£ç å’ŒåŸºå‡†å°†åœ¨å‘è¡¨æ—¶å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05038v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æå‡ºä¸€ç§åä¸ºPressure2Motionçš„æ–°å‹è¿åŠ¨æ•æ‰ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥ä»åœ°é¢å‹åŠ›åºåˆ—å’Œæ–‡æœ¬æç¤ºä¸­åˆæˆäººç±»è¿åŠ¨ï¼Œæ— éœ€ç‰¹æ®Šç…§æ˜è®¾ç½®ã€ç›¸æœºæˆ–å¯ç©¿æˆ´è®¾å¤‡ã€‚é€šè¿‡å‹åŠ›ç‰¹å¾å’Œæ–‡æœ¬æç¤ºçš„é«˜çº§çº¦æŸï¼Œè¯¥ç®—æ³•è§£å†³äº†å‹åŠ›ä¿¡å·ä¸å…¨èº«è¿åŠ¨ä¹‹é—´ä¸ç¡®å®šæ€§çš„é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆçš„è¿åŠ¨å…·æœ‰é«˜ä¿çœŸåº¦å’Œç‰©ç†å¯è¡Œæ€§ï¼Œä¸ºè¯¥ä»»åŠ¡å»ºç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Pressure2Motionæ˜¯ä¸€ç§æ–°å‹è¿åŠ¨æ•æ‰ç®—æ³•ï¼Œå¯ä»åœ°é¢å‹åŠ›åºåˆ—å’Œæ–‡æœ¬æç¤ºä¸­åˆæˆäººç±»è¿åŠ¨ã€‚</li>
<li>è¯¥ç®—æ³•é€‚ç”¨äºéšç§ä¿æŠ¤ã€ä½å…‰ç…§å’Œä½æˆæœ¬çš„åœºæ™¯ï¼Œæ— éœ€ç‰¹æ®Šç…§æ˜è®¾ç½®ã€ç›¸æœºæˆ–å¯ç©¿æˆ´è®¾å¤‡ã€‚</li>
<li>Pressure2Motioné€šè¿‡å‹åŠ›ç‰¹å¾å’Œæ–‡æœ¬æç¤ºçš„é«˜çº§çº¦æŸæ¥è§£å†³å‹åŠ›ä¿¡å·ä¸å…¨èº«è¿åŠ¨ä¹‹é—´çš„ä¸ç¡®å®šæ€§é—®é¢˜ã€‚</li>
<li>è¯¥ç®—æ³•é‡‡ç”¨åŒçº§ç‰¹å¾æå–å™¨å’Œåˆ†å±‚æ‰©æ•£æ¨¡å‹ï¼Œåˆ†åˆ«è§£æå‹åŠ›æ•°æ®å’Œè¾¨åˆ«å¤§è§„æ¨¡è¿åŠ¨è½¨è¿¹ä»¥åŠç»†å¾®å§¿åŠ¿è°ƒæ•´ã€‚</li>
<li>Pressure2Motionç»“åˆäº†å‹åŠ›åºåˆ—çš„ç‰©ç†çº¿ç´¢å’Œæè¿°æ€§æ–‡æœ¬çš„è¯­ä¹‰æŒ‡å¯¼ï¼Œä»¥ç²¾å‡†æŒ‡å¯¼è¿åŠ¨ç”Ÿæˆã€‚</li>
<li>æ®æ‚‰ï¼ŒPressure2Motionæ˜¯é¦–æ¬¡åˆ©ç”¨å‹åŠ›æ•°æ®å’Œè¯­è¨€å…ˆéªŒè¿›è¡Œè¿åŠ¨ç”Ÿæˆçš„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05038">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a6c89f5ea76fcf3a625600a298d86e2d" align="middle">
<img src="https://picx.zhimg.com/v2-45d3dc4b8b097aa7bb70817421332ba2" align="middle">
<img src="https://picx.zhimg.com/v2-0717b6a980db1788d6f34779be90c313" align="middle">
<img src="https://picx.zhimg.com/v2-32310c7a5800e98b69ce74f921a6da65" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PhysCorr-Dual-Reward-DPO-for-Physics-Constrained-Text-to-Video-Generation-with-Automated-Preference-Selection"><a href="#PhysCorr-Dual-Reward-DPO-for-Physics-Constrained-Text-to-Video-Generation-with-Automated-Preference-Selection" class="headerlink" title="PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection"></a>PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection</h2><p><strong>Authors:Peiyao Wang, Weining Wang, Qi Li</strong></p>
<p>Recent advances in text-to-video generation have achieved impressive perceptual quality, yet generated content often violates fundamental principles of physical plausibility - manifesting as implausible object dynamics, incoherent interactions, and unrealistic motion patterns. Such failures hinder the deployment of video generation models in embodied AI, robotics, and simulation-intensive domains. To bridge this gap, we propose PhysCorr, a unified framework for modeling, evaluating, and optimizing physical consistency in video generation. Specifically, we introduce PhysicsRM, the first dual-dimensional reward model that quantifies both intra-object stability and inter-object interactions. On this foundation, we develop PhyDPO, a novel direct preference optimization pipeline that leverages contrastive feedback and physics-aware reweighting to guide generation toward physically coherent outputs. Our approach is model-agnostic and scalable, enabling seamless integration into a wide range of video diffusion and transformer-based backbones. Extensive experiments across multiple benchmarks demonstrate that PhysCorr achieves significant improvements in physical realism while preserving visual fidelity and semantic alignment. This work takes a critical step toward physically grounded and trustworthy video generation.</p>
<blockquote>
<p>æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„æœ€æ–°è¿›å±•å·²ç»å®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ„ŸçŸ¥è´¨é‡ï¼Œç„¶è€Œç”Ÿæˆçš„å†…å®¹å¾€å¾€è¿åäº†ç‰©ç†å¯è¡Œæ€§çš„åŸºæœ¬åŸåˆ™â€”â€”è¡¨ç°ä¸ºç‰©ä½“åŠ¨åŠ›å­¦ä¸å¯ä¿¡ã€äº¤äº’ä¸ä¸€è‡´å’ŒåŠ¨ä½œæ¨¡å¼ä¸çœŸå®ã€‚è¿™ç§å¤±è´¥é˜»ç¢äº†è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨åµŒå…¥å¼äººå·¥æ™ºèƒ½ã€æœºå™¨äººæŠ€æœ¯å’Œæ¨¡æ‹Ÿå¯†é›†å‹é¢†åŸŸçš„åº”ç”¨ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†PhysCorrï¼Œä¸€ä¸ªç”¨äºå»ºæ¨¡ã€è¯„ä¼°å’Œä¼˜åŒ–è§†é¢‘ç”Ÿæˆä¸­ç‰©ç†ä¸€è‡´æ€§çš„ç»Ÿä¸€æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»‹ç»äº†PhysicsRMï¼Œç¬¬ä¸€ä¸ªåŒé‡ç»´åº¦çš„å¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ—¢è¡¡é‡ç‰©ä½“å†…éƒ¨çš„ç¨³å®šæ€§ï¼Œåˆè¡¡é‡ç‰©ä½“ä¹‹é—´çš„äº¤äº’ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼€å‘äº†PhyDPOï¼Œä¸€ç§æ–°å‹çš„ç›´æ¥åå¥½ä¼˜åŒ–ç®¡é“ï¼Œå®ƒåˆ©ç”¨å¯¹æ¯”åé¦ˆå’Œç‰©ç†æ„ŸçŸ¥é‡æƒæ¥å¼•å¯¼ç”Ÿæˆç‰©ç†è¿è´¯çš„è¾“å‡ºã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯æ¨¡å‹æ— å…³å’Œå¯æ‰©å±•çš„ï¼Œèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°å¹¿æ³›çš„è§†é¢‘æ‰©æ•£å’ŒåŸºäºå˜å‹å™¨çš„éª¨å¹²ç½‘ä¸­ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPhysCorråœ¨ç‰©ç†ç°å®æ„Ÿæ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼ŒåŒæ—¶ä¿æŒäº†è§†è§‰ä¿çœŸåº¦å’Œè¯­ä¹‰å¯¹é½ã€‚è¿™é¡¹å·¥ä½œä¸ºå®ç°åŸºäºç‰©ç†åŸç†å’Œå¯ä¿¡èµ–çš„è§†é¢‘ç”Ÿæˆè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03997v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºPhysCorrçš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºå»ºæ¨¡ã€è¯„ä¼°å’Œä¼˜åŒ–è§†é¢‘ç”Ÿæˆä¸­çš„ç‰©ç†ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†PhysicsRMåŒç»´å¥–åŠ±æ¨¡å‹ï¼Œå¯¹ç‰©ä½“å†…éƒ¨ç¨³å®šæ€§å’Œç‰©ä½“é—´äº¤äº’è¿›è¡Œé‡åŒ–ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œå¼€å‘äº†ä¸€ç§åä¸ºPhyDPOçš„æ–°å‹ç›´æ¥åå¥½ä¼˜åŒ–ç®¡é“ï¼Œé€šè¿‡å¯¹æ¯”åé¦ˆå’Œç‰©ç†æ„ŸçŸ¥é‡æƒæ¥å¼•å¯¼ç”Ÿæˆæ›´ç‰©ç†ä¸Šè¿è´¯çš„è¾“å‡ºã€‚è¯¥æ–¹æ³•å…·æœ‰æ¨¡å‹æ— å…³æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå¯æ— ç¼é›†æˆåˆ°å„ç§è§†é¢‘æ‰©æ•£å’ŒåŸºäºTransformerçš„æ¶æ„ä¸­ã€‚å®éªŒè¯æ˜ï¼ŒPhysCorråœ¨æå‡ç‰©ç†çœŸå®æ„Ÿçš„åŒæ—¶ï¼Œä¿æŒè§†è§‰ä¿çœŸåº¦å’Œè¯­ä¹‰å¯¹é½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PhysCorræ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºæé«˜è§†é¢‘ç”Ÿæˆä¸­çš„ç‰©ç†ä¸€è‡´æ€§ã€‚</li>
<li>å¼•å…¥äº†PhysicsRMåŒç»´å¥–åŠ±æ¨¡å‹ï¼Œé‡åŒ–ç‰©ä½“ç¨³å®šæ€§å’Œäº¤äº’æ€§ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§åä¸ºPhyDPOçš„ç›´æ¥åå¥½ä¼˜åŒ–ç®¡é“ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨å¯¹æ¯”åé¦ˆå’Œç‰©ç†æ„ŸçŸ¥é‡æƒæ¥å¼•å¯¼ç”Ÿæˆæ›´ç‰©ç†ä¸Šè¿è´¯çš„è¾“å‡ºã€‚</li>
<li>PhysCorré€‚ç”¨äºå„ç§è§†é¢‘æ‰©æ•£å’ŒåŸºäºTransformerçš„æ¶æ„ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒPhysCorråœ¨æå‡ç‰©ç†çœŸå®æ„Ÿçš„åŒæ—¶ä¿æŒè§†è§‰ä¿çœŸåº¦å’Œè¯­ä¹‰å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03997">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ea6b4fc2879e6964ffaae09a9cec8bce" align="middle">
<img src="https://picx.zhimg.com/v2-0ea63bfb0c596caed15d5706f4d9347b" align="middle">
<img src="https://picx.zhimg.com/v2-1722905367cadcb9e76d04ed90fa8a66" align="middle">
<img src="https://picx.zhimg.com/v2-bd26d0776c3ee2e04deec52f8b49f880" align="middle">
<img src="https://picx.zhimg.com/v2-3be1d750fa279215c4689a83bdb9f2f6" align="middle">
<img src="https://picx.zhimg.com/v2-50dc9557f1dd91349dd0ba924edd508a" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SurgViVQA-Temporally-Grounded-Video-Question-Answering-for-Surgical-Scene-Understanding"><a href="#SurgViVQA-Temporally-Grounded-Video-Question-Answering-for-Surgical-Scene-Understanding" class="headerlink" title="SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding"></a>SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding</h2><p><strong>Authors:Mauro Orazio Drago, Luca Carlini, Pelinsu Celebi Balyemez, Dennis Pierantozzi, Chiara Lena, Cesare Hassan, Danail Stoyanov, Elena De Momi, Sophia Bano, Mobarak I. Hoque</strong></p>
<p>Video Question Answering (VideoQA) in the surgical domain aims to enhance intraoperative understanding by enabling AI models to reason over temporally coherent events rather than isolated frames. Current approaches are limited to static image features, and available datasets often lack temporal annotations, ignoring the dynamics critical for accurate procedural interpretation. We propose SurgViVQA, a surgical VideoQA model that extends visual reasoning from static images to dynamic surgical scenes. It uses a Masked Videoâ€“Text Encoder to fuse video and question features, capturing temporal cues such as motion and toolâ€“tissue interactions, which a fine-tuned large language model (LLM) then decodes into coherent answers. To evaluate its performance, we curated REAL-Colon-VQA, a colonoscopic video dataset that includes motion-related questions and diagnostic attributes, as well as out-of-template questions with rephrased or semantically altered formulations to assess model robustness. Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset shows that SurgViVQA outperforms existing image-based VQA benchmark models, particularly in keyword accuracy, improving over PitVQA by +11% on REAL-Colon-VQA and +9% on EndoVis18-VQA. A perturbation study on the questions further confirms improved generalizability and robustness to variations in question phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework for temporally-aware understanding in surgical VideoQA, enabling AI models to interpret dynamic procedural contexts more effectively. Code and dataset available at <a target="_blank" rel="noopener" href="https://github.com/madratak/SurgViVQA">https://github.com/madratak/SurgViVQA</a>.</p>
<blockquote>
<p>æ‰‹æœ¯é¢†åŸŸçš„è§†é¢‘é—®ç­”ï¼ˆVideoQAï¼‰æ—¨åœ¨é€šè¿‡ä½¿AIæ¨¡å‹èƒ½å¤Ÿå¯¹æ—¶é—´ä¸Šè¿è´¯çš„äº‹ä»¶è¿›è¡Œæ¨ç†ï¼Œè€Œä¸æ˜¯å­¤ç«‹çš„å¸§ï¼Œä»è€Œå¢å¼ºæ‰‹æœ¯è¿‡ç¨‹ä¸­çš„ç†è§£ã€‚å½“å‰çš„æ–¹æ³•ä»…é™äºé™æ€å›¾åƒç‰¹å¾ï¼Œè€Œå¯ç”¨çš„æ•°æ®é›†é€šå¸¸ç¼ºä¹æ—¶é—´æ³¨é‡Šï¼Œå¿½ç•¥äº†å¯¹å‡†ç¡®ç¨‹åºè§£é‡Šè‡³å…³é‡è¦çš„åŠ¨æ€ã€‚æˆ‘ä»¬æå‡ºäº†SurgViVQAï¼Œä¸€ç§æ‰‹æœ¯è§†é¢‘é—®ç­”æ¨¡å‹ï¼Œå®ƒå°†è§†è§‰æ¨ç†ä»é™æ€å›¾åƒæ‰©å±•åˆ°åŠ¨æ€æ‰‹æœ¯åœºæ™¯ã€‚å®ƒä½¿ç”¨é®ç½©è§†é¢‘-æ–‡æœ¬ç¼–ç å™¨èåˆè§†é¢‘å’Œé—®é¢˜ç‰¹å¾ï¼Œæ•æ‰æ—¶é—´çº¿ç´¢ï¼Œå¦‚è¿åŠ¨å’Œå·¥å…·-ç»„ç»‡äº¤äº’ï¼Œç„¶åç»è¿‡å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†å…¶è§£ç ä¸ºè¿è´¯çš„ç­”æ¡ˆã€‚ä¸ºäº†è¯„ä¼°å…¶æ€§èƒ½ï¼Œæˆ‘ä»¬åˆ¶ä½œäº†REAL-Colon-VQAï¼Œè¿™æ˜¯ä¸€ä¸ªç»“è‚ é•œè§†é¢‘æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸è¿åŠ¨ç›¸å…³çš„é—®é¢˜å’Œè¯Šæ–­å±æ€§ï¼Œä»¥åŠé‡æ–°è¡¨è¿°æˆ–è¯­ä¹‰æ›´æ”¹çš„æ¨¡æ¿å¤–é—®é¢˜ï¼Œä»¥è¯„ä¼°æ¨¡å‹çš„ç¨³å¥æ€§ã€‚åœ¨REAL-Colon-VQAå’Œå…¬å…±EndoVis18-VQAæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯è¡¨æ˜ï¼ŒSurgViVQAä¼˜äºç°æœ‰çš„åŸºäºå›¾åƒçš„VQAåŸºå‡†æ¨¡å‹ï¼Œå°¤å…¶åœ¨å…³é”®è¯å‡†ç¡®ç‡æ–¹é¢ï¼Œåœ¨REAL-Colon-VQAä¸Šæ¯”PitVQAé«˜å‡º11%ï¼Œåœ¨EndoVis18-VQAä¸Šé«˜å‡º9%ã€‚å¯¹é—®é¢˜çš„æ‰°åŠ¨ç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†å…¶åœ¨é—®é¢˜è¡¨è¿°å˜åŒ–æ–¹é¢çš„é€šç”¨æ€§å’Œç¨³å¥æ€§ã€‚SurgViVQAå’ŒREAL-Colon-VQAæ•°æ®é›†ä¸ºæ‰‹æœ¯è§†é¢‘é—®ç­”ä¸­çš„æ—¶é—´æ„ŸçŸ¥ç†è§£æä¾›äº†æ¡†æ¶ï¼Œä½¿AIæ¨¡å‹èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è§£é‡ŠåŠ¨æ€ç¨‹åºä¸Šä¸‹æ–‡ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/madratak/SurgViVQA%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/madratak/SurgViVQAè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.03325v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ‰‹æœ¯é¢†åŸŸçš„è§†é¢‘é—®ç­”ï¼ˆVideoQAï¼‰æŠ€æœ¯ã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦åŸºäºé™æ€å›¾åƒç‰¹å¾ï¼Œå¿½è§†äº†æ‰‹æœ¯è¿‡ç¨‹ä¸­çš„åŠ¨æ€å˜åŒ–ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†SurgViVQAæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†è§†è§‰æ¨ç†ä»é™æ€å›¾åƒæ‰©å±•åˆ°åŠ¨æ€æ‰‹æœ¯åœºæ™¯ã€‚é€šè¿‡èåˆè§†é¢‘å’Œé—®é¢˜ç‰¹å¾ï¼Œæ•æ‰åŠ¨ä½œå’Œå·¥å…·-ç»„ç»‡äº¤äº’ç­‰æ—¶é—´çº¿ç´¢ï¼Œå¹¶ä½¿ç”¨å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£ç å‡ºè¿è´¯çš„ç­”æ¡ˆã€‚ä¸ºè¯„ä¼°æ€§èƒ½ï¼Œå¼€å‘äº†REAL-Colon-VQAæ•°æ®é›†ï¼ŒåŒ…æ‹¬è¿åŠ¨ç›¸å…³é—®é¢˜ã€è¯Šæ–­å±æ€§å’Œå‡ºæ¨¡æ¿çš„é—®é¢˜ã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼ŒSurgViVQAåœ¨å…³é”®è¯å‡†ç¡®ç‡ä¸Šä¼˜äºç°æœ‰å›¾åƒåŸºå‡†VQAæ¨¡å‹ï¼Œå¦‚æ¯”PitVQAåœ¨REAL-Colon-VQAä¸Šæé«˜äº†11%ï¼Œåœ¨EndoVis18-VQAä¸Šæé«˜äº†9%ã€‚æ­¤å¤–ï¼Œé—®é¢˜çš„æ‰°åŠ¨ç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†å…¶å¯¹äºé—®é¢˜è¡¨è¿°å˜åŒ–çš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VideoQAåœ¨æ‰‹æœ¯é¢†åŸŸæ—¨åœ¨é€šè¿‡AIæ¨¡å‹å¯¹æ—¶åºè¿è´¯äº‹ä»¶çš„ç†è§£æ¥å¢å¼ºæœ¯ä¸­ç†è§£ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦åŸºäºé™æ€å›¾åƒç‰¹å¾ï¼Œå¿½ç•¥äº†æ‰‹æœ¯çš„åŠ¨æ€æ€§ã€‚</li>
<li>SurgViVQAæ¨¡å‹å°†è§†è§‰æ¨ç†ä»é™æ€å›¾åƒæ‰©å±•åˆ°åŠ¨æ€æ‰‹æœ¯åœºæ™¯ã€‚</li>
<li>SurgViVQAä½¿ç”¨Masked Video-Text Encoderèåˆè§†é¢‘å’Œé—®é¢˜ç‰¹å¾ï¼Œæ•æ‰æ—¶é—´çº¿ç´¢ã€‚</li>
<li>REAL-Colon-VQAæ•°æ®é›†çš„å»ºç«‹ï¼ŒåŒ…æ‹¬è¿åŠ¨ç›¸å…³é—®é¢˜ã€è¯Šæ–­å±æ€§å’Œå‡ºæ¨¡æ¿çš„é—®é¢˜ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>SurgViVQAåœ¨å…³é”®è¯å‡†ç¡®ç‡ä¸Šä¼˜äºç°æœ‰å›¾åƒåŸºå‡†VQAæ¨¡å‹ã€‚</li>
<li>SurgViVQAå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œå¯¹é—®é¢˜è¡¨è¿°å˜åŒ–çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.03325">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-397ba56d83d35d0f8a6c089dcf138554" align="middle">
<img src="https://picx.zhimg.com/v2-15e5eac438d6c52fcd246c0dc94d8b68" align="middle">
<img src="https://picx.zhimg.com/v2-30b491b146ee6944c2ddecd7f914c555" align="middle">
<img src="https://picx.zhimg.com/v2-baffec2e7c2b47ff3365ec32e2b5e97e" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MotionStream-Real-Time-Video-Generation-with-Interactive-Motion-Controls"><a href="#MotionStream-Real-Time-Video-Generation-with-Interactive-Motion-Controls" class="headerlink" title="MotionStream: Real-Time Video Generation with Interactive Motion Controls"></a>MotionStream: Real-Time Video Generation with Interactive Motion Controls</h2><p><strong>Authors:Joonghyuk Shin, Zhengqi Li, Richard Zhang, Jun-Yan Zhu, Jaesik Park, Eli Schechtman, Xun Huang</strong></p>
<p>Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons: (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience.</p>
<blockquote>
<p>å½“å‰çš„è¿åŠ¨æ¡ä»¶è§†é¢‘ç”Ÿæˆæ–¹æ³•å­˜åœ¨å»¶è¿Ÿè¿‡å¤§ï¼ˆæ¯åˆ†é’Ÿè¾“å‡ºä¸€ä¸ªè§†é¢‘ï¼‰å’Œéå› æœå¤„ç†çš„é—®é¢˜ï¼Œè¿™é˜»æ­¢äº†å®æ—¶äº¤äº’ã€‚æˆ‘ä»¬æå‡ºäº†MotionStreamï¼Œä½¿ç”¨å•ä¸ªGPUå®ç°äº†å­ç§’çº§å»¶è¿Ÿå’Œé«˜è¾¾29å¸§&#x2F;ç§’çš„æµå¼ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé€šè¿‡å¢å¼ºæ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„è¿åŠ¨æ§åˆ¶åŠŸèƒ½æ¥ç”Ÿæˆé«˜è´¨é‡è§†é¢‘ï¼Œè¿™äº›è§†é¢‘éµå¾ªå…¨å±€æ–‡æœ¬æç¤ºå’Œå±€éƒ¨è¿åŠ¨æŒ‡å¯¼ï¼Œä½†ä¸è¿›è¡Œå³æ—¶æ¨ç†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨è‡ªæˆ‘å¼ºè¿«å’Œåˆ†å¸ƒåŒ¹é…è’¸é¦å°†åŒå‘æ•™å¸ˆæç‚¼ä¸ºå› æœå­¦ç”Ÿï¼Œä»è€Œå®ç°å®æ—¶æµå¼æ¨ç†ã€‚åœ¨ç”Ÿæˆé•¿æ—¶é—´ç”šè‡³å¯èƒ½æ— é™æ—¶é—´èŒƒå›´çš„è§†é¢‘æ—¶ï¼Œä¼šå‡ºç°å‡ ä¸ªå…³é”®æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰å¼¥åˆåœ¨æœ‰é™é•¿åº¦ä¸Šè¿›è¡Œè®­ç»ƒå’Œåœ¨æ— é™èŒƒå›´ä¸Šè¿›è¡Œæ¨æ–­ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼›ï¼ˆ2ï¼‰é€šè¿‡é˜²æ­¢è¯¯å·®ç´¯ç§¯æ¥ç»´æŒé«˜è´¨é‡ï¼›ï¼ˆ3ï¼‰åœ¨ä¸æ–­å¢åŠ çš„ä¸Šä¸‹æ–‡çª—å£å¯¼è‡´çš„è®¡ç®—æˆæœ¬ä¸å¢åŠ çš„æƒ…å†µä¸‹ï¼Œä¿æŒå¿«é€Ÿæ¨ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„å…³é”®åœ¨äºå¼•å…¥ç²¾å¿ƒè®¾è®¡æ»‘åŠ¨çª—å£å› æœæ³¨æ„åŠ›ï¼Œå¹¶ç»“åˆæ³¨æ„åŠ›æ±‡ç‚¹ã€‚é€šè¿‡ç»“åˆè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ³¨æ„åŠ›æ±‡ç‚¹å’ŒKVç¼“å­˜æ»šåŠ¨ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨å›ºå®šä¸Šä¸‹æ–‡çª—å£å†…é€‚å½“åœ°æ¨¡æ‹Ÿæ¨ç†æ—¶é—´å¤–æ¨ï¼Œä»è€Œå®ç°ä»»æ„é•¿åº¦è§†é¢‘çš„æ’å®šé€Ÿåº¦ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨è¿åŠ¨è·Ÿéšå’Œè§†é¢‘è´¨é‡æ–¹é¢è¾¾åˆ°äº†ä¸šç•Œæœ€ä½³æ°´å¹³ï¼ŒåŒæ—¶é€Ÿåº¦æé«˜äº†ä¸¤ä¸ªæ•°é‡çº§ï¼Œèƒ½å¤Ÿå®ç°ç‹¬ä¸€æ— äºŒçš„æ— é™åˆ¶é•¿åº¦æµå¼ä¼ è¾“ã€‚ä½¿ç”¨MotionStreamï¼Œç”¨æˆ·å¯ä»¥åœ¨å®æ—¶æ¶‚é¸¦è½¨è¿¹ã€æ§åˆ¶æ‘„åƒå¤´æˆ–è½¬ç§»è¿åŠ¨ï¼Œå¹¶å®æ—¶çœ‹åˆ°ç»“æœå‘ˆç°ï¼Œæä¾›çœŸæ­£çš„äº¤äº’å¼ä½“éªŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01266v1">PDF</a> Project webpage: <a target="_blank" rel="noopener" href="https://joonghyuk.com/motionstream-web/">https://joonghyuk.com/motionstream-web/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>å½“å‰çš„è¿åŠ¨æ¡ä»¶è§†é¢‘ç”Ÿæˆæ–¹æ³•å­˜åœ¨å»¶è¿Ÿè¿‡é•¿ï¼ˆæ¯åˆ†é’Ÿç”Ÿæˆä¸€ä¸ªè§†é¢‘ï¼‰å’Œéå› æœå¤„ç†çš„é—®é¢˜ï¼Œè¿™é˜»ç¢äº†å®æ—¶äº¤äº’ã€‚æˆ‘ä»¬æå‡ºäº†MotionStreamï¼Œå®ç°äº†æ¯ç§’ç”Ÿæˆé«˜è¾¾29å¸§çš„è§†é¢‘ï¼Œå¹¶åœ¨å•ä¸ªGPUä¸Šè¿›è¡Œæµå¼ç”Ÿæˆï¼Œå»¶è¿Ÿç¼©çŸ­è‡³ä¸åˆ°ä¸€ç§’ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé€šè¿‡å¢å¼ºæ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„è¿åŠ¨æ§åˆ¶åŠŸèƒ½æ¥ç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ï¼Œè¿™äº›è§†é¢‘éµå¾ªå…¨å±€æ–‡æœ¬æç¤ºå’Œå±€éƒ¨è¿åŠ¨æŒ‡å¯¼ï¼Œä½†å¹¶ä¸æ”¯æŒå³æ—¶æ¨ç†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šè¿‡åˆ†å¸ƒåŒ¹é…è’¸é¦çš„è‡ªæˆ‘å¼ºåˆ¶å°†è¿™ç§åŒå‘æ•™å¸ˆè’¸é¦ä¸ºå› æœå­¦ç”Ÿï¼Œä»è€Œå®ç°å®æ—¶æµå¼æ¨ç†ã€‚åœ¨ç”Ÿæˆé•¿æ—¶é—´ç”šè‡³æ— é™æ—¶é•¿è§†é¢‘æ—¶ï¼Œé¢ä¸´ä»¥ä¸‹å…³é”®æŒ‘æˆ˜ï¼š1ï¼‰ä»æœ‰é™é•¿åº¦è®­ç»ƒæ‰©å±•åˆ°æ— é™æ—¶é•¿è§†é¢‘é¢†åŸŸçš„å·®è·ï¼›2ï¼‰é€šè¿‡é˜²æ­¢é”™è¯¯ç´¯ç§¯æ¥ç»´æŒé«˜è´¨é‡ï¼›3ï¼‰åœ¨ä¸æ–­å¢åŠ çš„ä¸Šä¸‹æ–‡çª—å£çš„æƒ…å†µä¸‹ï¼Œä¿æŒå¿«é€Ÿæ¨ç†ï¼Œé¿å…è®¡ç®—æˆæœ¬çš„å¢åŠ ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„å…³é”®åœ¨äºå¼•å…¥ç²¾å¿ƒè®¾è®¡æ»‘åŠ¨çª—å£å› æœæ³¨æ„åŠ›ä»¥åŠæ³¨æ„åŠ›æ± ã€‚é€šè¿‡ç»“åˆè®­ç»ƒæ—¶çš„æ³¨æ„åŠ›æ± å’Œè‡ªæˆ‘æ»šåŠ¨KVç¼“å­˜æ»šåŠ¨ï¼Œæˆ‘ä»¬èƒ½å¤Ÿç”¨å›ºå®šçš„ä¸Šä¸‹æ–‡çª—å£æ¨¡æ‹Ÿæ¨ç†æ—¶é—´çš„å¤–æ¨ï¼Œå®ç°äº†ä»»æ„é•¿åº¦è§†é¢‘çš„æ’å®šé€Ÿåº¦ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨è¿åŠ¨è·Ÿè¸ªå’Œè§†é¢‘è´¨é‡æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ŒåŒæ—¶é€Ÿåº¦æé«˜äº†ä¸¤ä¸ªæ•°é‡çº§ï¼Œèƒ½å¤Ÿå”¯ä¸€å®ç°æ— é™é•¿åº¦æµå¼ä¼ è¾“ã€‚ä½¿ç”¨MotionStreamï¼Œç”¨æˆ·å¯ä»¥åœ¨å®æ—¶ä¸­ç»˜åˆ¶è½¨è¿¹ã€æ§åˆ¶ç›¸æœºæˆ–è½¬æ¢è¿åŠ¨ï¼Œå¹¶çœ‹åˆ°ç»“æœå±•å¼€ï¼Œå®ç°çœŸæ­£çš„äº¤äº’å¼ä½“éªŒã€‚</p>
<p><strong>è¦ç‚¹æ¦‚æ‹¬</strong></p>
<ol>
<li>MotionStreamèƒ½å¤Ÿå®ç°å­ç§’å»¶è¿Ÿçš„è§†é¢‘ç”Ÿæˆï¼Œé«˜è¾¾æ¯ç§’29å¸§çš„æµå¼ç”Ÿæˆé€Ÿåº¦ã€‚</li>
<li>é€šè¿‡å¢å¼ºæ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹çš„è¿åŠ¨æ§åˆ¶åŠŸèƒ½ç”Ÿæˆé«˜è´¨é‡çš„è§†é¢‘ã€‚</li>
<li>é‡‡ç”¨è‡ªæˆ‘å¼ºåˆ¶å’Œåˆ†å¸ƒåŒ¹é…è’¸é¦æ³•å°†åŒå‘æ•™å¸ˆæ¨¡å‹è½¬åŒ–ä¸ºå› æœå­¦ç”Ÿæ¨¡å‹ï¼Œå®ç°å®æ—¶æ¨ç†ã€‚</li>
<li>é¢ä¸´ä»æœ‰é™é•¿åº¦è®­ç»ƒåˆ°æ— é™æ—¶é•¿è§†é¢‘çš„å¤–æ¨æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é¢†åŸŸå·®è·ã€è´¨é‡ç»´æŒå’Œå¿«é€Ÿæ¨ç†çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡æ»‘åŠ¨çª—å£å› æœæ³¨æ„åŠ›å’Œæ³¨æ„åŠ›æ± çš„è®¾è®¡è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œé‡‡ç”¨å›ºå®šä¸Šä¸‹æ–‡çª—å£æ¨¡æ‹Ÿæ¨ç†æ—¶é—´çš„å¤–æ¨ã€‚</li>
<li>æ¨¡å‹åœ¨è¿åŠ¨è·Ÿè¸ªå’Œè§†é¢‘è´¨é‡æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼ŒåŒæ—¶æé«˜ç”Ÿæˆé€Ÿåº¦ä¸¤ä¸ªæ•°é‡çº§ï¼Œæ”¯æŒæ— é™é•¿åº¦æµå¼ä¼ è¾“ã€‚</li>
<li>MotionStreamæä¾›çœŸæ­£çš„äº¤äº’å¼ä½“éªŒï¼Œç”¨æˆ·å¯å®æ—¶ç»˜åˆ¶è½¨è¿¹ã€æ§åˆ¶ç›¸æœºæˆ–è½¬æ¢è¿åŠ¨å¹¶å³æ—¶çœ‹åˆ°ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01266">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c851d2548a65eb1c6fd641103a412e46" align="middle">
<img src="https://picx.zhimg.com/v2-7f2248b5322a5ab1d0c7e60e524ddef9" align="middle">
<img src="https://picx.zhimg.com/v2-73432225be47b698e30cb6c6517eafae" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MoSa-Motion-Generation-with-Scalable-Autoregressive-Modeling"><a href="#MoSa-Motion-Generation-with-Scalable-Autoregressive-Modeling" class="headerlink" title="MoSa: Motion Generation with Scalable Autoregressive Modeling"></a>MoSa: Motion Generation with Scalable Autoregressive Modeling</h2><p><strong>Authors:Mengyuan Liu, Sheng Yan, Yong Wang, Yingjie Li, Gui-Bin Bian, Hong Liu</strong></p>
<p>We introduce MoSa, a novel hierarchical motion generation framework for text-driven 3D human motion generation that enhances the Vector Quantization-guided Generative Transformers (VQ-GT) paradigm through a coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale Token Preservation Strategy (MTPS) integrated into a hierarchical residual vector quantization variational autoencoder (RQ-VAE). MTPS employs interpolation at each hierarchical quantization to effectively retain coarse-to-fine multi-scale tokens. With this, the generative transformer supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens, unlike traditional methods that predict only one token at each step. Consequently, MoSa requires only 10 inference steps, matching the number of RQ-VAE quantization layers. To address potential reconstruction degradation from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and incorporates attention mechanisms to better capture global dependencies. Extensive experiments show that MoSa achieves state-of-the-art generation quality and efficiency, outperforming prior methods in both fidelity and speed. On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMaskâ€™s 0.20) while reducing inference time by 27 percent. Moreover, MoSa generalizes well to downstream tasks such as motion editing, requiring no additional fine-tuning. The code is available at <a target="_blank" rel="noopener" href="https://mosa-web.github.io/MoSa-web">https://mosa-web.github.io/MoSa-web</a></p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†MoSaï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ–‡æœ¬é©±åŠ¨çš„ä¸‰ç»´äººä½“è¿åŠ¨ç”Ÿæˆçš„æ–°å‹åˆ†å±‚è¿åŠ¨ç”Ÿæˆæ¡†æ¶ï¼Œå®ƒé€šè¿‡ç²—ç»†å¯ä¼¸ç¼©çš„ç”Ÿæˆè¿‡ç¨‹å¢å¼ºäº†åŸºäºå‘é‡é‡åŒ–çš„ç”Ÿæˆå¼è½¬æ¢å™¨ï¼ˆVQ-GTï¼‰èŒƒå¼ã€‚åœ¨MoSaä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šå°ºåº¦ç¬¦å·ä¿ç•™ç­–ç•¥ï¼ˆMTPSï¼‰ï¼Œè¯¥ç­–ç•¥è¢«é›†æˆåˆ°åˆ†å±‚å‰©ä½™å‘é‡é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆRQ-VAEï¼‰ä¸­ã€‚MTPSåœ¨æ¯ä¸ªåˆ†å±‚é‡åŒ–ä¸Šé‡‡ç”¨æ’å€¼ï¼Œä»¥æœ‰æ•ˆåœ°ä¿ç•™ç²—ç»†å¤šå°ºåº¦ç¬¦å·ã€‚å› æ­¤ï¼Œç”Ÿæˆå¼è½¬æ¢å™¨æ”¯æŒå¯ä¼¸ç¼©è‡ªå›å½’ï¼ˆSARï¼‰å»ºæ¨¡ï¼Œè¯¥å»ºæ¨¡å¯ä»¥é¢„æµ‹å°ºåº¦ç¬¦å·ï¼Œä¸åŒäºä¼ ç»Ÿæ–¹æ³•åœ¨æ¯ä¸ªæ­¥éª¤ä¸­åªé¢„æµ‹ä¸€ä¸ªç¬¦å·ã€‚å› æ­¤ï¼ŒMoSaä»…éœ€è¦10ä¸ªæ¨ç†æ­¥éª¤ï¼Œä¸RQ-VAEé‡åŒ–å±‚æ•°ç›¸åŒ¹é…ã€‚ä¸ºäº†è§£å†³é¢‘ç¹æ’å€¼å¯èƒ½å¯¼è‡´çš„é‡å»ºè´¨é‡ä¸‹é™é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CAQ-VAEï¼Œè¿™æ˜¯ä¸€ä¸ªè½»ä¾¿è€Œè¡¨ç°åŠ›å¼ºæ··åˆå·ç§¯æ³¨æ„åŠ›VQ-VAEã€‚CAQ-VAEå¢å¼ºäº†å‰©ä½™å—è®¾è®¡å¹¶èå…¥äº†æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æ›´å¥½åœ°æ•æ‰å…¨å±€ä¾èµ–æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMoSaåœ¨ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œåœ¨ä¿çœŸåº¦å’Œé€Ÿåº¦æ–¹é¢éƒ½è¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ã€‚åœ¨Motion-Xæ•°æ®é›†ä¸Šï¼ŒMoSaçš„FIDä¸º0.06ï¼ˆç›¸æ¯”ä¹‹ä¸‹MoMaskä¸º0.20ï¼‰ï¼ŒåŒæ—¶æ¨ç†æ—¶é—´å‡å°‘äº†27%ã€‚æ­¤å¤–ï¼ŒMoSaå¯¹äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚è¿åŠ¨ç¼–è¾‘ï¼‰å…·æœ‰å¾ˆå¥½çš„é€šç”¨æ€§ï¼Œæ— éœ€è¿›è¡Œä»»ä½•é¢å¤–çš„å¾®è°ƒã€‚ç›¸å…³ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://mosa-web.github.io/MoSa-web%E3%80%82">https://mosa-web.github.io/MoSa-webã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.01200v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MoSaï¼Œä¸€ç§ç”¨äºæ–‡æœ¬é©±åŠ¨çš„ä¸‰ç»´äººä½“è¿åŠ¨ç”Ÿæˆçš„æ–°å‹åˆ†å±‚è¿åŠ¨ç”Ÿæˆæ¡†æ¶ã€‚MoSaæ”¹è¿›äº†åŸºäºå‘é‡é‡åŒ–çš„ç”Ÿæˆæ€§è½¬æ¢å™¨ï¼ˆVQ-GTï¼‰èŒƒå¼ï¼Œé€šè¿‡ç²—ç»†ç»“åˆçš„åˆ†å±‚ç”Ÿæˆè¿‡ç¨‹ï¼Œå®ç°äº†ä¼˜ç§€çš„è¿åŠ¨ç”Ÿæˆæ•ˆæœã€‚MoSaé‡‡ç”¨å¤šå°ºåº¦ä»¤ç‰Œä¿ç•™ç­–ç•¥ï¼ˆMTPSï¼‰ï¼Œé›†æˆåˆ°åˆ†å±‚æ®‹å·®å‘é‡é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆRQ-VAEï¼‰ä¸­ã€‚MTPSé€šè¿‡åœ¨æ¯ä¸ªåˆ†å±‚é‡åŒ–ä¸­è¿›è¡Œæ’å€¼ï¼Œæœ‰æ•ˆåœ°ä¿ç•™äº†ç²—åˆ°ç»†çš„å¤šå…ƒä»¤ç‰Œã€‚å› æ­¤ï¼Œç”Ÿæˆæ€§è½¬æ¢å™¨æ”¯æŒå¯ä¼¸ç¼©çš„è‡ªå›å½’ï¼ˆSARï¼‰å»ºæ¨¡ï¼Œå¯é¢„æµ‹ä¸åŒå°ºåº¦çš„ä»¤ç‰Œï¼Œä¸åŒäºä¼ ç»Ÿæ–¹æ³•åœ¨æ¯ä¸ªæ­¥éª¤ä¸­åªé¢„æµ‹ä¸€ä¸ªä»¤ç‰Œã€‚MoSaä»…éœ€è¦10ä¸ªæ¨ç†æ­¥éª¤ï¼Œä¸RQ-VAEçš„é‡åŒ–å±‚æ•°ç›¸åŒ¹é…ã€‚ä¸ºè§£å†³é¢‘ç¹æ’å€¼å¯èƒ½å¯¼è‡´çš„é‡å»ºè´¨é‡ä¸‹é™é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CAQ-VAEï¼Œä¸€ç§è½»ä¾¿è€Œè¡¨ç°åŠ›å¼ºçš„å·ç§¯æ³¨æ„åŠ›æ··åˆVQ-VAEã€‚CAQ-VAEä¼˜åŒ–äº†æ®‹å·®å—è®¾è®¡ï¼Œå¹¶èå…¥äº†æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æ›´å¥½åœ°æ•æ‰å…¨å±€ä¾èµ–æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒMoSaåœ¨ç”Ÿæˆè´¨é‡å’Œæ•ˆç‡æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œåœ¨Motion-Xæ•°æ®é›†ä¸Šçš„FIDå¾—åˆ†ä¸º0.06ï¼ˆå¯¹æ¯”MoMaskçš„0.20ï¼‰ï¼ŒåŒæ—¶å‡å°‘äº†27%çš„æ¨ç†æ—¶é—´ã€‚æ­¤å¤–ï¼ŒMoSaåœ¨ä¸‹æ¸¸ä»»åŠ¡å¦‚è¿åŠ¨ç¼–è¾‘ä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–å¾®è°ƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>MoSaæ˜¯ä¸€ç§æ–°é¢–çš„æ–‡æœ¬é©±åŠ¨çš„ä¸‰ç»´äººä½“è¿åŠ¨ç”Ÿæˆåˆ†å±‚è¿åŠ¨ç”Ÿæˆæ¡†æ¶ï¼ŒåŸºäºVQ-GTèŒƒå¼æ”¹è¿›ã€‚</li>
<li>MoSaé‡‡ç”¨å¤šå°ºåº¦ä»¤ç‰Œä¿ç•™ç­–ç•¥ï¼ˆMTPSï¼‰ï¼Œé›†æˆåˆ°RQ-VAEä¸­ï¼Œæœ‰æ•ˆä¿ç•™å¤šå°ºåº¦ä»¤ç‰Œã€‚</li>
<li>MoSaæ”¯æŒå¯ä¼¸ç¼©çš„è‡ªå›å½’ï¼ˆSARï¼‰å»ºæ¨¡ï¼Œèƒ½å¤Ÿé¢„æµ‹ä¸åŒå°ºåº¦çš„ä»¤ç‰Œã€‚</li>
<li>MoSaé€šè¿‡ä¼˜åŒ–æ¨ç†æ­¥éª¤å’Œæé«˜æ•ˆç‡ï¼Œå®ç°äº†é«˜è´¨é‡çš„è¿åŠ¨ç”Ÿæˆã€‚</li>
<li>CAQ-VAEçš„æå‡ºè§£å†³äº†é¢‘ç¹æ’å€¼å¯¼è‡´çš„é‡å»ºè´¨é‡ä¸‹é™é—®é¢˜ï¼Œæå‡äº†è¿åŠ¨ç”Ÿæˆçš„æ€§èƒ½ã€‚</li>
<li>MoSaåœ¨Motion-Xæ•°æ®é›†ä¸Šçš„FIDå¾—åˆ†ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œæ˜¾ç¤ºå…¶åœ¨ç”Ÿæˆè´¨é‡æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>MoSaå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯åº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡å¦‚è¿åŠ¨ç¼–è¾‘ï¼Œä¸”æ— éœ€é¢å¤–å¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.01200">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7dc6f9ad613f45af8540155c1bfbc451" align="middle">
<img src="https://picx.zhimg.com/v2-1512032d09a2fe18b7c2d71e5f723c90" align="middle">
<img src="https://picx.zhimg.com/v2-770dd1c8e575b6a01e0da180e4acc824" align="middle">
<img src="https://picx.zhimg.com/v2-7337aaa434a930fe416c958698ace8ba" align="middle">
<img src="https://picx.zhimg.com/v2-f05e874e08540f8764db121d2d7af70a" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="RealDPO-Real-or-Not-Real-that-is-the-Preference"><a href="#RealDPO-Real-or-Not-Real-that-is-the-Preference" class="headerlink" title="RealDPO: Real or Not Real, that is the Preference"></a>RealDPO: Real or Not Real, that is the Preference</h2><p><strong>Authors:Guo Cheng, Danni Yang, Ziqi Huang, Jianlou Si, Chenyang Si, Ziwei Liu</strong></p>
<p>Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques.</p>
<blockquote>
<p>è§†é¢‘ç”Ÿæˆæ¨¡å‹æœ€è¿‘åœ¨åˆæˆè´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç”Ÿæˆå¤æ‚è¿åŠ¨ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå› ä¸ºç°æœ‰æ¨¡å‹å¾€å¾€éš¾ä»¥äº§ç”Ÿè‡ªç„¶ã€æµç•…å’Œä¸Šä¸‹æ–‡ä¸€è‡´çš„è¿åŠ¨ã€‚ç”Ÿæˆçš„è¿åŠ¨å’ŒçœŸå®ä¸–ç•Œè¿åŠ¨ä¹‹é—´çš„å·®è·é™åˆ¶äº†å®ƒä»¬çš„å®é™…åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RealDPOï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¯¹é½èŒƒå¼ï¼Œå®ƒåˆ©ç”¨çœŸå®ä¸–ç•Œæ•°æ®ä½œä¸ºæ­£é¢æ ·æœ¬è¿›è¡Œåå¥½å­¦ä¹ ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„è¿åŠ¨åˆæˆã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸åŒï¼Œåè€…åªæä¾›æœ‰é™çš„æ ¡æ­£åé¦ˆï¼ŒRealDPOé‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œå®šåˆ¶çš„æŸå¤±å‡½æ•°æ¥æé«˜è¿åŠ¨çœŸå®æ€§ã€‚é€šè¿‡å¯¹æ¯”çœŸå®ä¸–ç•Œè§†é¢‘å’Œé”™è¯¯çš„æ¨¡å‹è¾“å‡ºï¼ŒRealDPOèƒ½å¤Ÿå®ç°è¿­ä»£è‡ªæˆ‘æ ¡æ­£ï¼Œé€æ­¥æ”¹è¿›è¿åŠ¨è´¨é‡ã€‚ä¸ºäº†æ”¯æŒå¤æ‚è¿åŠ¨åˆæˆçš„åè®­ç»ƒï¼Œæˆ‘ä»¬æå‡ºäº†RealAction-5Kï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„è§†é¢‘æ•°æ®é›†ï¼Œæ•æ‰äººç±»æ—¥å¸¸æ´»åŠ¨ï¼ŒåŒ…å«ä¸°å¯Œè€Œç²¾ç¡®çš„è¿åŠ¨ç»†èŠ‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„æ¨¡å‹å’Œç°æœ‰çš„åå¥½ä¼˜åŒ–æŠ€æœ¯ç›¸æ¯”ï¼ŒRealDPOæ˜¾è‘—æé«˜äº†è§†é¢‘è´¨é‡ã€æ–‡æœ¬å¯¹é½å’Œè¿åŠ¨çœŸå®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14955v2">PDF</a> Code:<a target="_blank" rel="noopener" href="https://github.com/Vchitect/RealDPO">https://github.com/Vchitect/RealDPO</a> Project Page:<a target="_blank" rel="noopener" href="https://vchitect.github.io/RealDPO-Project/">https://vchitect.github.io/RealDPO-Project/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨åˆæˆè´¨é‡ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç”Ÿæˆå¤æ‚è¿åŠ¨ä»æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰æ¨¡å‹éš¾ä»¥äº§ç”Ÿè‡ªç„¶ã€å¹³æ»‘ã€ä¸Šä¸‹æ–‡ä¸€è‡´çš„è¿åŠ¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†RealDPOï¼Œä¸€ç§åˆ©ç”¨çœŸå®ä¸–ç•Œæ•°æ®ä½œä¸ºæ­£æ ·æœ¬è¿›è¡Œåå¥½å­¦ä¹ çš„æ–°å‹å¯¹é½èŒƒå¼ï¼Œä»¥å®ç°æ›´ç²¾ç¡®çš„è¿åŠ¨åˆæˆã€‚ä¸åŒäºæä¾›æœ‰é™çŸ«æ­£åé¦ˆçš„ä¼ ç»Ÿç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼ŒRealDPOé‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œå®šåˆ¶çš„æŸå¤±å‡½æ•°ï¼Œé€šè¿‡å¯¹æ¯”çœŸå®ä¸–ç•Œè§†é¢‘å’Œé”™è¯¯çš„æ¨¡å‹è¾“å‡ºï¼Œå®ç°è¿­ä»£è‡ªæˆ‘ä¿®æ­£ï¼Œé€æ­¥æ”¹è¿›è¿åŠ¨è´¨é‡ã€‚ä¸ºæ”¯æŒå¤æ‚è¿åŠ¨åˆæˆçš„åè®­ç»ƒï¼Œæœ¬æ–‡æå‡ºäº†RealAction-5Kæ•°æ®é›†ï¼ŒåŒ…å«é«˜è´¨é‡çš„äººç±»æ—¥å¸¸æ´»åŠ¨è§†é¢‘ï¼Œå…·æœ‰ä¸°å¯Œå’Œç²¾ç¡®çš„è¿åŠ¨ç»†èŠ‚ã€‚å®éªŒè¡¨æ˜ï¼ŒRealDPOåœ¨è§†é¢‘è´¨é‡ã€æ–‡æœ¬å¯¹é½å’Œè¿åŠ¨ç°å®æ„Ÿæ–¹é¢æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ¨¡å‹å’Œç°æœ‰çš„åå¥½ä¼˜åŒ–æŠ€æœ¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨åˆæˆè´¨é‡ä¸Šå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ç”Ÿæˆå¤æ‚è¿åŠ¨ä»å…·æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ¨¡å‹éš¾ä»¥äº§ç”Ÿè‡ªç„¶ã€å¹³æ»‘ã€ä¸Šä¸‹æ–‡ä¸€è‡´çš„è¿åŠ¨ã€‚</li>
<li>RealDPOåˆ©ç”¨çœŸå®ä¸–ç•Œæ•°æ®ä½œä¸ºæ­£æ ·æœ¬è¿›è¡Œåå¥½å­¦ä¹ ï¼Œå®ç°æ›´ç²¾ç¡®çš„è¿åŠ¨åˆæˆã€‚</li>
<li>RealDPOé‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œå®šåˆ¶æŸå¤±å‡½æ•°ï¼Œæä¾›è¿­ä»£è‡ªæˆ‘ä¿®æ­£ï¼Œæ”¹è¿›è¿åŠ¨è´¨é‡ã€‚</li>
<li>ä¸ºæ”¯æŒå¤æ‚è¿åŠ¨åˆæˆçš„åè®­ç»ƒï¼Œæå‡ºäº†RealAction-5Kæ•°æ®é›†ã€‚</li>
<li>RealDPOåœ¨è§†é¢‘è´¨é‡ã€æ–‡æœ¬å¯¹é½å’Œè¿åŠ¨ç°å®æ„Ÿæ–¹é¢æ˜¾è‘—ä¼˜äºå…¶ä»–æŠ€æœ¯ã€‚</li>
<li>RealDPOçš„å¼•å…¥æœ‰æœ›è§£å†³ç”Ÿæˆæ¨¡å‹ä¸çœŸå®ä¸–ç•Œè¿åŠ¨ä¹‹é—´çš„å·®è·ï¼Œæé«˜æ¨¡å‹çš„å®ç”¨æ€§å’Œé€‚ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55abdd7bd05ba5328a978319156cc5de" align="middle">
<img src="https://picx.zhimg.com/v2-9726f4b14239c03695374fb339ae6cda" align="middle">
<img src="https://picx.zhimg.com/v2-759d8db5a27dd4edbcfee61b07b5eb4d" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="PhysCtrl-Generative-Physics-for-Controllable-and-Physics-Grounded-Video-Generation"><a href="#PhysCtrl-Generative-Physics-for-Controllable-and-Physics-Grounded-Video-Generation" class="headerlink" title="PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation"></a>PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation</h2><p><strong>Authors:Chen Wang, Chuhao Chen, Yiming Huang, Zhiyang Dou, Yuan Liu, Jiatao Gu, Lingjie Liu</strong></p>
<p>Existing video generation models excel at producing photo-realistic videos from text or images, but often lack physical plausibility and 3D controllability. To overcome these limitations, we introduce PhysCtrl, a novel framework for physics-grounded image-to-video generation with physical parameters and force control. At its core is a generative physics network that learns the distribution of physical dynamics across four materials (elastic, sand, plasticine, and rigid) via a diffusion model conditioned on physics parameters and applied forces. We represent physical dynamics as 3D point trajectories and train on a large-scale synthetic dataset of 550K animations generated by physics simulators. We enhance the diffusion model with a novel spatiotemporal attention block that emulates particle interactions and incorporates physics-based constraints during training to enforce physical plausibility. Experiments show that PhysCtrl generates realistic, physics-grounded motion trajectories which, when used to drive image-to-video models, yield high-fidelity, controllable videos that outperform existing methods in both visual quality and physical plausibility. Project Page: <a target="_blank" rel="noopener" href="https://cwchenwang.github.io/physctrl">https://cwchenwang.github.io/physctrl</a></p>
<blockquote>
<p>ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹æ“…é•¿ä»æ–‡æœ¬æˆ–å›¾åƒç”Ÿæˆé€¼çœŸçš„è§†é¢‘ï¼Œä½†å¾€å¾€ç¼ºä¹ç‰©ç†åˆç†æ€§å’Œ3Då¯æ§æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†PhysCtrlï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºç‰©ç†å‚æ•°å’ŒåŠ›æ§åˆ¶çš„æ–°å‹å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªç”Ÿæˆç‰©ç†ç½‘ç»œï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹å­¦ä¹ å››ç§ææ–™ï¼ˆå¼¹æ€§ã€æ²™å­ã€å¯å¡‘æ€§ææ–™å’Œåˆšæ€§ï¼‰çš„ç‰©ç†åŠ¨åŠ›å­¦åˆ†å¸ƒï¼Œè¯¥æ¨¡å‹ä»¥ç‰©ç†å‚æ•°å’Œåº”ç”¨åŠ›ä¸ºæ¡ä»¶ã€‚æˆ‘ä»¬å°†ç‰©ç†åŠ¨åŠ›å­¦è¡¨ç¤ºä¸º3Dç‚¹è½¨è¿¹ï¼Œå¹¶åœ¨ç”±ç‰©ç†æ¨¡æ‹Ÿå™¨ç”Ÿæˆçš„å¤§è§„æ¨¡åˆæˆæ•°æ®é›†ï¼ˆ55ä¸‡ä¸ªåŠ¨ç”»ï¼‰ä¸Šè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬å¢å¼ºæ‰©æ•£æ¨¡å‹ï¼ŒåŠ å…¥æ–°å‹æ—¶ç©ºæ³¨æ„åŠ›æ¨¡å—ï¼Œæ¨¡æ‹Ÿç²’å­äº¤äº’å¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥åŸºäºç‰©ç†çš„çº¦æŸï¼Œä»¥å¼ºåˆ¶æ‰§è¡Œç‰©ç†åˆç†æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒPhysCtrlç”ŸæˆçœŸå®çš„ã€åŸºäºç‰©ç†çš„è¿åŠ¨è½¨è¿¹ï¼Œå½“ç”¨äºé©±åŠ¨å›¾åƒåˆ°è§†é¢‘æ¨¡å‹æ—¶ï¼Œå¯ç”Ÿæˆé«˜è´¨é‡ã€å¯æ§çš„è§†é¢‘ï¼Œåœ¨è§†è§‰è´¨é‡å’Œç‰©ç†åˆç†æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://cwchenwang.github.io/physctrl">https://cwchenwang.github.io/physctrl</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.20358v2">PDF</a> NeurIPS 2025 Camera Ready Version</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºPhysCtrlçš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å®ç°äº†åŸºäºç‰©ç†å‚æ•°çš„å›¾åƒåˆ°è§†é¢‘çš„ç”Ÿæˆã€‚å®ƒé€šè¿‡å¼•å…¥ç‰©ç†å‚æ•°å’ŒåŠ›æ§åˆ¶ï¼Œå…‹æœäº†ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨ç‰©ç†é€¼çœŸåº¦å’Œ3Då¯æ§æ€§æ–¹é¢çš„å±€é™ã€‚é‡‡ç”¨æ‰©æ•£æ¨¡å‹å­¦ä¹ å››ç§ææ–™çš„ç‰©ç†åŠ¨åŠ›å­¦åˆ†å¸ƒï¼Œå¹¶åœ¨å¤§å‹åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒPhysCtrlèƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„ã€åŸºäºç‰©ç†çš„è¿åŠ¨è½¨è¿¹ï¼Œå½“ç”¨äºé©±åŠ¨å›¾åƒåˆ°è§†é¢‘æ¨¡å‹æ—¶ï¼Œå¯ä»¥äº§ç”Ÿé«˜è´¨é‡ã€å¯æ§çš„è§†é¢‘ï¼Œåœ¨è§†è§‰è´¨é‡å’Œç‰©ç†é€¼çœŸåº¦æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PhysCtrlæ˜¯ä¸€ä¸ªåŸºäºç‰©ç†å‚æ•°çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆçš„æ–°å‹æ¡†æ¶ã€‚</li>
<li>å¼•å…¥ç‰©ç†å‚æ•°å’ŒåŠ›æ§åˆ¶ï¼Œä»¥æé«˜è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ç‰©ç†é€¼çœŸåº¦å’Œ3Då¯æ§æ€§ã€‚</li>
<li>é‡‡ç”¨æ‰©æ•£æ¨¡å‹å­¦ä¹ å››ç§ææ–™çš„ç‰©ç†åŠ¨åŠ›å­¦åˆ†å¸ƒã€‚</li>
<li>åœ¨å¤§å‹åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…å«550Kä¸ªåŠ¨ç”»ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°å‹çš„æ—¶ç©ºæ³¨æ„åŠ›å—ï¼Œæ¨¡æ‹Ÿç²’å­äº¤äº’ï¼Œå¹¶åœ¨è®­ç»ƒæœŸé—´åŠ å…¥ç‰©ç†çº¦æŸä»¥åŠ å¼ºç‰©ç†é€¼çœŸåº¦ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒPhysCtrlèƒ½ç”Ÿæˆé€¼çœŸçš„ã€åŸºäºç‰©ç†çš„è¿åŠ¨è½¨è¿¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.20358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-544f4b0c0f9f096a1c8ec00bd5a6542e" align="middle">
<img src="https://picx.zhimg.com/v2-26f2acb616a86ee3e6e243e1671887ef" align="middle">
<img src="https://picx.zhimg.com/v2-8f06e2a8821b18fa89b467d11418403b" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Light-Future-Multimodal-Action-Frame-Prediction-via-InstructPix2Pix"><a href="#Light-Future-Multimodal-Action-Frame-Prediction-via-InstructPix2Pix" class="headerlink" title="Light Future: Multimodal Action Frame Prediction via InstructPix2Pix"></a>Light Future: Multimodal Action Frame Prediction via InstructPix2Pix</h2><p><strong>Authors:Zesen Zhong, Duomin Zhang, Yijia Li</strong></p>
<p>Predicting future motion trajectories is a critical capability across domains such as robotics, autonomous systems, and human activity forecasting, enabling safer and more intelligent decision-making. This paper proposes a novel, efficient, and lightweight approach for robot action prediction, offering significantly reduced computational cost and inference latency compared to conventional video prediction models. Importantly, it pioneers the adaptation of the InstructPix2Pix model for forecasting future visual frames in robotic tasks, extending its utility beyond static image editing. We implement a deep learning-based visual prediction framework that forecasts what a robot will observe 100 frames (10 seconds) into the future, given a current image and a textual instruction. We repurpose and fine-tune the InstructPix2Pix model to accept both visual and textual inputs, enabling multimodal future frame prediction. Experiments on the RoboTWin dataset (generated based on real-world scenarios) demonstrate that our method achieves superior SSIM and PSNR compared to state-of-the-art baselines in robot action prediction tasks. Unlike conventional video prediction models that require multiple input frames, heavy computation, and slow inference latency, our approach only needs a single image and a text prompt as input. This lightweight design enables faster inference, reduced GPU demands, and flexible multimodal control, particularly valuable for applications like robotics and sports motion trajectory analytics, where motion trajectory precision is prioritized over visual fidelity.</p>
<blockquote>
<p>é¢„æµ‹æœªæ¥è¿åŠ¨è½¨è¿¹æ˜¯æœºå™¨äººæŠ€æœ¯ã€è‡ªä¸»ç³»ç»Ÿå’Œäººç±»æ´»åŠ¨é¢„æµ‹ç­‰é¢†åŸŸä¸­çš„ä¸€é¡¹å…³é”®èƒ½åŠ›ï¼Œèƒ½å¤Ÿä¸ºæ›´å®‰å…¨ã€æ›´æ™ºèƒ½çš„å†³ç­–æä¾›æ”¯æ’‘ã€‚æœ¬æ–‡é’ˆå¯¹æœºå™¨äººåŠ¨ä½œé¢„æµ‹æå‡ºäº†ä¸€ç§æ–°é¢–ã€é«˜æ•ˆä¸”è½»é‡çº§çš„æ–¹æ³•ï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„è§†é¢‘é¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ–¹æ³•å¤§å¤§å‡å°‘äº†è®¡ç®—æˆæœ¬å’Œæ¨ç†å»¶è¿Ÿã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒç‡å…ˆå°†InstructPix2Pixæ¨¡å‹åº”ç”¨äºæœºå™¨äººä»»åŠ¡çš„æœªæ¥è§†è§‰å¸§é¢„æµ‹ï¼Œæ‰©å±•äº†å…¶è¶…è¶Šé™æ€å›¾åƒç¼–è¾‘çš„å®ç”¨æ€§ã€‚æˆ‘ä»¬å®ç°äº†ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„è§†è§‰é¢„æµ‹æ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®å½“å‰å›¾åƒå’Œæ–‡æœ¬æŒ‡ä»¤é¢„æµ‹æœºå™¨äººæœªæ¥100å¸§ï¼ˆ10ç§’ï¼‰çš„è§‚æµ‹æƒ…å†µã€‚æˆ‘ä»¬å¯¹InstructPix2Pixæ¨¡å‹è¿›è¡Œæ”¹é€ å’Œå¾®è°ƒï¼Œä»¥æ¥å—è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼Œä»è€Œå®ç°å¤šæ¨¡æ€æœªæ¥å¸§é¢„æµ‹ã€‚åœ¨RoboTWinæ•°æ®é›†ï¼ˆåŸºäºçœŸå®åœºæ™¯ç”Ÿæˆï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æœºå™¨äººåŠ¨ä½œé¢„æµ‹ä»»åŠ¡ä¸­å®ç°äº†ä¼˜äºå…ˆè¿›åŸºå‡†çº¿çš„SSIMå’ŒPSNRæŒ‡æ ‡ã€‚ä¸ä¼ ç»Ÿçš„è§†é¢‘é¢„æµ‹æ¨¡å‹ä¸åŒï¼Œè¿™äº›æ¨¡å‹éœ€è¦å¤šå¸§è¾“å…¥ã€å¤§é‡è®¡ç®—å’Œç¼“æ…¢çš„æ¨ç†å»¶è¿Ÿï¼Œè€Œæˆ‘ä»¬çš„æ–¹æ³•åªéœ€è¦å•ä¸ªå›¾åƒå’Œæ–‡æœ¬æç¤ºä½œä¸ºè¾“å…¥ã€‚è¿™ç§è½»é‡çº§çš„è®¾è®¡å®ç°äº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€é™ä½äº†GPUéœ€æ±‚ï¼Œå¹¶å®ç°äº†çµæ´»çš„å¤šæ¨¡æ€æ§åˆ¶ï¼Œå¯¹äºæœºå™¨äººæŠ€æœ¯å’Œè¿åŠ¨è½¨è¿¹åˆ†æç­‰é¢†åŸŸçš„åº”ç”¨ç‰¹åˆ«æœ‰ä»·å€¼ï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œè¿åŠ¨è½¨è¿¹çš„ç²¾ç¡®åº¦æ¯”è§†è§‰ä¿çœŸåº¦æ›´é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14809v2">PDF</a> 9 pages including appendix, 4 tables, 8 figures, to be submitted to WACV 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºæœºå™¨äººåŠ¨ä½œé¢„æµ‹çš„æ–°å‹é«˜æ•ˆã€è½»é‡çº§æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨æ·±åº¦å­¦ä¹ è§†è§‰é¢„æµ‹æ¡†æ¶ï¼Œæ ¹æ®å½“å‰å›¾åƒå’Œæ–‡æœ¬æŒ‡ä»¤é¢„æµ‹æœºå™¨äººæœªæ¥10ç§’çš„è¿åŠ¨è½¨è¿¹ã€‚é€šè¿‡é‡æ–°è®­ç»ƒå’Œæ”¹è¿›InstructPix2Pixæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼Œå®ç°äº†å¤šæ¨¡æ€æœªæ¥å¸§é¢„æµ‹ã€‚ç›¸è¾ƒäºå…¶ä»–æœºå™¨äººåŠ¨ä½œé¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ›´ä½çš„è®¡ç®—æˆæœ¬å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚åœ¨RoboTWinæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœºå™¨äººåŠ¨ä½œé¢„æµ‹ä»»åŠ¡ä¸­å®ç°äº†è¾ƒé«˜çš„SSIMå’ŒPSNRæŒ‡æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹æœºå™¨äººåŠ¨ä½œé¢„æµ‹çš„è½»é‡çº§é¢„æµ‹æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•åŸºäºæ·±åº¦å­¦ä¹ è§†è§‰é¢„æµ‹æ¡†æ¶ï¼Œåˆ©ç”¨å½“å‰å›¾åƒå’Œæ–‡æœ¬æŒ‡ä»¤è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>é‡‡ç”¨InstructPix2Pixæ¨¡å‹è¿›è¡Œæ”¹è¿›ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼Œå®ç°å¤šæ¨¡æ€é¢„æµ‹ã€‚</li>
<li>è¯¥æ–¹æ³•ç›¸è¾ƒäºå…¶ä»–æœºå™¨äººåŠ¨ä½œé¢„æµ‹æ¨¡å‹å…·æœ‰æ›´ä½çš„è®¡ç®—æˆæœ¬å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</li>
<li>æ–¹æ³•åœ¨RoboTWinæ•°æ®é›†ä¸Šå®ç°äº†è¾ƒé«˜çš„SSIMå’ŒPSNRæŒ‡æ ‡ã€‚</li>
<li>æ–¹æ³•å¯¹äºè¿åŠ¨è½¨è¿¹çš„ç²¾åº¦è¾ƒé«˜ï¼Œå¯¹äºæœºå™¨äººå­¦å’Œè¿åŠ¨è½¨è¿¹åˆ†æåº”ç”¨æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b20183947985f60996e8b7ba5fb33581" align="middle">
<img src="https://picx.zhimg.com/v2-c209e98157ca91bb463b9f0fc3d21057" align="middle">
<img src="https://picx.zhimg.com/v2-61cea6380a6bf47f789d1402e24d7010" align="middle">
<img src="https://picx.zhimg.com/v2-6f2fdb1013fdc7e1466da14df8395252" align="middle">
<img src="https://picx.zhimg.com/v2-9aa0002dea48d977893ff40969541ff6" align="middle">
<img src="https://picx.zhimg.com/v2-ad160a0338b41caa176dfdb493a301fd" align="middle">
<img src="https://picx.zhimg.com/v2-d9a7f80e7f50b7be591d54cba7fc4a56" align="middle">
<img src="https://picx.zhimg.com/v2-ec0c67a52169ffee11028fa359b5b512" align="middle">
<img src="https://picx.zhimg.com/v2-14092300883ec75f851717fcdf3919be" align="middle">
<img src="https://picx.zhimg.com/v2-6251711d55275d527dc37937cd745072" align="middle">
<img src="https://picx.zhimg.com/v2-0e960335b0fc3cb82efcf224facaf40c" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="T-GVC-Trajectory-Guided-Generative-Video-Coding-at-Ultra-Low-Bitrates"><a href="#T-GVC-Trajectory-Guided-Generative-Video-Coding-at-Ultra-Low-Bitrates" class="headerlink" title="T-GVC: Trajectory-Guided Generative Video Coding at Ultra-Low Bitrates"></a>T-GVC: Trajectory-Guided Generative Video Coding at Ultra-Low Bitrates</h2><p><strong>Authors:Zhitao Wang, Hengyu Man, Wenrui Li, Xingtao Wang, Xiaopeng Fan, Debin Zhao</strong></p>
<p>Recent advances in video generation techniques have given rise to an emerging paradigm of generative video coding for Ultra-Low Bitrate (ULB) scenarios by leveraging powerful generative priors. However, most existing methods are limited by domain specificity (e.g., facial or human videos) or excessive dependence on high-level text guidance, which tend to inadequately capture fine-grained motion details, leading to unrealistic or incoherent reconstructions. To address these challenges, we propose Trajectory-Guided Generative Video Coding (dubbed T-GVC), a novel framework that bridges low-level motion tracking with high-level semantic understanding. T-GVC features a semantic-aware sparse motion sampling pipeline that extracts pixel-wise motion as sparse trajectory points based on their semantic importance, significantly reducing the bitrate while preserving critical temporal semantic information. In addition, by integrating trajectory-aligned loss constraints into diffusion processes, we introduce a training-free guidance mechanism in latent space to ensure physically plausible motion patterns without sacrificing the inherent capabilities of generative models. Experimental results demonstrate that T-GVC outperforms both traditional and neural video codecs under ULB conditions. Furthermore, additional experiments confirm that our framework achieves more precise motion control than existing text-guided methods, paving the way for a novel direction of generative video coding guided by geometric motion modeling.</p>
<blockquote>
<p>è¿‘æœŸè§†é¢‘ç”ŸæˆæŠ€æœ¯çš„è¿›å±•ä¸ºè¶…ä½æ¯”ç‰¹ç‡ï¼ˆULBï¼‰åœºæ™¯çš„ç”Ÿæˆå¼è§†é¢‘ç¼–ç æä¾›äº†æ–°çš„èŒƒä¾‹ï¼Œå®ƒå€ŸåŠ©å¼ºå¤§çš„ç”Ÿæˆå…ˆéªŒä¿¡æ¯ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•å—åˆ°é¢†åŸŸç‰¹å®šæ€§ï¼ˆå¦‚é¢éƒ¨æˆ–äººä½“è§†é¢‘ï¼‰çš„é™åˆ¶ï¼Œæˆ–è¿‡äºä¾èµ–é«˜çº§æ–‡æœ¬æŒ‡å¯¼ï¼Œå¾€å¾€æ— æ³•å……åˆ†æ•æ‰ç²¾ç»†çš„è¿åŠ¨ç»†èŠ‚ï¼Œå¯¼è‡´é‡å»ºç»“æœä¸çœŸå®æˆ–ä¸ä¸€è‡´ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†è½¨è¿¹å¼•å¯¼ç”Ÿæˆå¼è§†é¢‘ç¼–ç ï¼ˆç®€ç§°T-GVCï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå°†ä½çº§åˆ«çš„è¿åŠ¨è·Ÿè¸ªä¸é«˜çº§åˆ«çš„è¯­ä¹‰ç†è§£ç›¸ç»“åˆã€‚T-GVCé‡‡ç”¨è¯­ä¹‰æ„ŸçŸ¥ç¨€ç–è¿åŠ¨é‡‡æ ·ç®¡é“ï¼Œæ ¹æ®è¯­ä¹‰é‡è¦æ€§æå–åƒç´ çº§çš„è¿åŠ¨ä½œä¸ºç¨€ç–è½¨è¿¹ç‚¹ï¼Œåœ¨å¤§å¹…é™ä½æ¯”ç‰¹ç‡çš„åŒæ—¶ä¿æŒå…³é”®çš„æ—¶é—´è¯­ä¹‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†è½¨è¿¹å¯¹é½æŸå¤±çº¦æŸé›†æˆåˆ°æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åœ¨æ½œåœ¨ç©ºé—´ä¸­å¼•å…¥äº†ä¸€ç§æ— è®­ç»ƒæŒ‡å¯¼æœºåˆ¶ï¼Œç¡®ä¿ç‰©ç†ä¸Šå¯è¡Œçš„è¿åŠ¨æ¨¡å¼ï¼ŒåŒæ—¶ä¸ç‰ºç‰²ç”Ÿæˆæ¨¡å‹çš„å†…ç”Ÿèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ULBæ¡ä»¶ä¸‹ï¼ŒT-GVCçš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿå’Œç¥ç»ç½‘ç»œè§†é¢‘ç¼–ç å™¨ã€‚è¿›ä¸€æ­¥çš„å®éªŒè¯å®ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ¯”ç°æœ‰çš„æ–‡æœ¬å¼•å¯¼æ–¹æ³•å®ç°äº†æ›´ç²¾ç¡®çš„è¿åŠ¨æ§åˆ¶ï¼Œä¸ºç”Ÿæˆå¼è§†é¢‘ç¼–ç çš„å‡ ä½•è¿åŠ¨å»ºæ¨¡æŒ‡å¯¼æ–¹å‘å¼€åˆ›äº†æ–°çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07633v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†è½¨è¿¹å¼•å¯¼ç”Ÿæˆå¼è§†é¢‘ç¼–ç ï¼ˆT-GVCï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆä½çº§åˆ«çš„è¿åŠ¨è·Ÿè¸ªä¸é«˜çº§åˆ«çš„è¯­ä¹‰ç†è§£ã€‚å®ƒé€šè¿‡è¯­ä¹‰æ„ŸçŸ¥çš„ç¨€ç–è¿åŠ¨é‡‡æ ·ç®¡é“ï¼Œæ ¹æ®åƒç´ çš„è¯­ä¹‰é‡è¦æ€§æå–ç¨€ç–è½¨è¿¹ç‚¹ï¼Œä»è€Œæ˜¾è‘—é™ä½æ¯”ç‰¹ç‡åŒæ—¶ä¿ç•™å…³é”®çš„æ—¶é—´è¯­ä¹‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œé€šè¿‡åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­å¼•å…¥è½¨è¿¹å¯¹é½çš„æŸå¤±çº¦æŸï¼ŒT-GVCåœ¨æ½œåœ¨ç©ºé—´ä¸­å¼•å…¥äº†ä¸€ç§æ— éœ€è®­ç»ƒçš„å¼•å¯¼æœºåˆ¶ï¼Œç¡®ä¿ç‰©ç†ä¸Šåˆç†çš„è¿åŠ¨æ¨¡å¼ï¼ŒåŒæ—¶ä¸ç‰ºç‰²ç”Ÿæˆæ¨¡å‹çš„å†…ç”Ÿèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒT-GVCåœ¨è¶…ä½æ¯”ç‰¹ç‡æ¡ä»¶ä¸‹ä¼˜äºä¼ ç»Ÿå’Œç¥ç»ç½‘ç»œè§†é¢‘ç¼–ç æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T-GVCæ¡†æ¶ç»“åˆäº†ä½çº§åˆ«è¿åŠ¨è·Ÿè¸ªå’Œé«˜çº§åˆ«è¯­ä¹‰ç†è§£ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>è¯­ä¹‰æ„ŸçŸ¥çš„ç¨€ç–è¿åŠ¨é‡‡æ ·ç®¡é“èƒ½æå–é‡è¦åƒç´ çš„ç¨€ç–è½¨è¿¹ç‚¹ï¼Œé™ä½æ¯”ç‰¹ç‡åŒæ—¶ä¿ç•™æ—¶é—´è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­å¼•å…¥è½¨è¿¹å¯¹é½æŸå¤±çº¦æŸï¼ŒT-GVCç¡®ä¿ç‰©ç†ä¸Šåˆç†çš„è¿åŠ¨æ¨¡å¼ã€‚</li>
<li>T-GVCåœ¨è¶…ä½æ¯”ç‰¹ç‡æ¡ä»¶ä¸‹æ€§èƒ½ä¼˜äºä¼ ç»Ÿå’Œç¥ç»ç½‘ç»œè§†é¢‘ç¼–ç æ–¹æ³•ã€‚</li>
<li>ä¸ç°æœ‰æ–‡æœ¬å¼•å¯¼çš„æ–¹æ³•ç›¸æ¯”ï¼ŒT-GVCå®ç°äº†æ›´ç²¾ç¡®çš„è¿åŠ¨æ§åˆ¶ã€‚</li>
<li>T-GVCä¸ºç”Ÿæˆå¼è§†é¢‘ç¼–ç æä¾›äº†æ–°çš„æ–¹å‘ï¼Œå³é€šè¿‡å‡ ä½•è¿åŠ¨å»ºæ¨¡è¿›è¡Œå¼•å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6dd014eff7e3b1dc01de178ff6a20232" align="middle">
<img src="https://picx.zhimg.com/v2-e0859bc7aa7217bc6bb45b90d81cc082" align="middle">
<img src="https://picx.zhimg.com/v2-ae22dc9d52e426cb77cde9f0d29e71ce" align="middle">
<img src="https://picx.zhimg.com/v2-64f3aa79014a718ab8828350962e621d" align="middle">
<img src="https://picx.zhimg.com/v2-b06f7e52539b6328f83df5ae993c569c" align="middle">
<img src="https://picx.zhimg.com/v2-fbcace2ebb494a9066bf98e624457d12" align="middle">
<img src="https://picx.zhimg.com/v2-c5b914eef62373e69fea9abd0ce6fe60" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Generating-Attribute-Aware-Human-Motions-from-Textual-Prompt"><a href="#Generating-Attribute-Aware-Human-Motions-from-Textual-Prompt" class="headerlink" title="Generating Attribute-Aware Human Motions from Textual Prompt"></a>Generating Attribute-Aware Human Motions from Textual Prompt</h2><p><strong>Authors:Xinghan Wang, Kun Xu, Fei Li, Cao Sheng, Jiazhong Yu, Yadong Mu</strong></p>
<p>Text-driven human motion generation has recently attracted considerable attention, allowing models to generate human motions based on textual descriptions. However, current methods neglect the influence of human attributes-such as age, gender, weight, and height-which are key factors shaping human motion patterns. This work represents a pilot exploration for bridging this gap. We conceptualize each motion as comprising both attribute information and action semantics, where textual descriptions align exclusively with action semantics. To achieve this, a new framework inspired by Structural Causal Models is proposed to decouple action semantics from human attributes, enabling text-to-semantics prediction and attribute-controlled generation. The resulting model is capable of generating attribute-aware motion aligned with the userâ€™s text and attribute inputs. For evaluation, we introduce a comprehensive dataset containing attribute annotations for text-motion pairs, setting the first benchmark for attribute-aware motion generation. Extensive experiments validate our modelâ€™s effectiveness.</p>
<blockquote>
<p>æ–‡æœ¬é©±åŠ¨çš„äººç±»è¿åŠ¨ç”Ÿæˆè¿‘æœŸå¼•èµ·äº†å¹¿æ³›çš„å…³æ³¨ï¼Œå®ƒå…è®¸æ¨¡å‹åŸºäºæ–‡æœ¬æè¿°ç”Ÿæˆäººç±»è¿åŠ¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•å¿½è§†äº†äººç±»å±æ€§ï¼ˆå¦‚å¹´é¾„ã€æ€§åˆ«ã€ä½“é‡å’Œèº«é«˜ï¼‰çš„å½±å“ï¼Œè¿™äº›å±æ€§æ˜¯å¡‘é€ äººç±»è¿åŠ¨æ¨¡å¼çš„å…³é”®å› ç´ ã€‚è¿™é¡¹å·¥ä½œæ˜¯å¯¹ç¼©å°è¿™ä¸€å·®è·çš„åˆæ­¥æ¢ç´¢ã€‚æˆ‘ä»¬å°†æ¯ç§è¿åŠ¨æ¦‚å¿µåŒ–ä¸ºå±æ€§ä¿¡æ¯å’ŒåŠ¨ä½œè¯­ä¹‰çš„ç»„åˆï¼Œæ–‡æœ¬æè¿°åªä¸åŠ¨ä½œè¯­ä¹‰ç›¸å¯¹åº”ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œå—ç»“æ„å› æœæ¨¡å‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå°†åŠ¨ä½œè¯­ä¹‰ä¸äººç±»å±æ€§è§£è€¦ï¼Œå®ç°æ–‡æœ¬åˆ°è¯­ä¹‰çš„é¢„æµ‹å’Œå±æ€§æ§åˆ¶ç”Ÿæˆã€‚æ‰€å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®ç”¨æˆ·çš„æ–‡æœ¬å’Œå±æ€§è¾“å…¥ï¼Œç”Ÿæˆä¸ä¹‹ç›¸ç¬¦çš„å±æ€§æ„ŸçŸ¥è¿åŠ¨ã€‚ä¸ºäº†è¯„ä¼°ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç»¼åˆæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«æ–‡æœ¬ä¸è¿åŠ¨å¯¹å±æ€§çš„æ³¨è§£ï¼Œä¸ºå±æ€§æ„ŸçŸ¥è¿åŠ¨ç”Ÿæˆè®¾ç½®äº†ç¬¬ä¸€ä¸ªåŸºå‡†ã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21912v2">PDF</a> Accepted by AAAI 2026</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ–‡æœ¬é©±åŠ¨çš„äººç±»è¿åŠ¨ç”Ÿæˆå·²å¼•èµ·å¹¿æ³›å…³æ³¨ï¼Œå…è®¸æ¨¡å‹åŸºäºæ–‡æœ¬æè¿°ç”Ÿæˆäººç±»è¿åŠ¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•å¿½è§†äº†äººç±»å±æ€§ï¼ˆå¦‚å¹´é¾„ã€æ€§åˆ«ã€ä½“é‡å’Œèº«é«˜ï¼‰çš„å½±å“ï¼Œè¿™äº›å±æ€§æ˜¯å¡‘é€ äººç±»è¿åŠ¨æ¨¡å¼çš„å…³é”®å› ç´ ã€‚è¿™é¡¹å·¥ä½œæ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚æˆ‘ä»¬æ¦‚å¿µåŒ–æ¯ä¸ªåŠ¨ä½œæ—¢åŒ…å«å±æ€§ä¿¡æ¯åˆåŒ…å«åŠ¨ä½œè¯­ä¹‰ï¼Œæ–‡æœ¬æè¿°åªä¸åŠ¨ä½œè¯­ä¹‰å¯¹é½ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œå—ç»“æ„å› æœæ¨¡å‹çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ä¸ªæ–°æ¡†æ¶ï¼Œå°†åŠ¨ä½œè¯­ä¹‰ä¸äººç±»å±æ€§è§£è€¦ï¼Œå®ç°æ–‡æœ¬åˆ°è¯­ä¹‰çš„é¢„æµ‹å’Œå±æ€§æ§åˆ¶ç”Ÿæˆã€‚æ‰€å¾—æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆä¸ç”¨æˆ·æ–‡æœ¬å’Œå±æ€§è¾“å…¥å¯¹é½çš„å±æ€§æ„ŸçŸ¥è¿åŠ¨ã€‚ä¸ºäº†è¯„ä¼°ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«æ–‡æœ¬è¿åŠ¨å¯¹å±æ€§æ³¨é‡Šçš„ç»¼åˆæ•°æ®é›†ï¼Œä¸ºå±æ€§æ„ŸçŸ¥è¿åŠ¨ç”Ÿæˆè®¾ç«‹äº†é¦–ä¸ªåŸºå‡†æµ‹è¯•ã€‚å¤§é‡å®éªŒéªŒè¯äº†è¯¥æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰æ–‡æœ¬é©±åŠ¨çš„äººç±»è¿åŠ¨ç”Ÿæˆæ–¹æ³•å¿½è§†äº†äººç±»å±æ€§çš„å½±å“ï¼Œå¦‚å¹´é¾„ã€æ€§åˆ«ã€ä½“é‡å’Œèº«é«˜ã€‚</li>
<li>æ­¤ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºç»“æ„å› æœæ¨¡å‹çš„æ–°æ¡†æ¶æ¥ç”Ÿæˆä¸äººç±»å±æ€§å’Œæ–‡æœ¬è¾“å…¥å¯¹é½çš„è¿åŠ¨ã€‚</li>
<li>è¯¥æ¡†æ¶å®ç°äº†åŠ¨ä½œè¯­ä¹‰å’Œäººç±»å±æ€§çš„è§£è€¦ï¼Œä½¿æ–‡æœ¬ä¸åŠ¨ä½œè¯­ä¹‰é¢„æµ‹å’Œå±æ€§æ§åˆ¶ç”Ÿæˆæˆä¸ºå¯èƒ½ã€‚</li>
<li>ä¸ºäº†è¯„ä¼°æ¨¡å‹æ•ˆæœï¼Œå¼•å…¥äº†ä¸€ä¸ªåŒ…å«æ–‡æœ¬è¿åŠ¨å¯¹å±æ€§æ³¨é‡Šçš„ç»¼åˆæ•°æ®é›†ã€‚</li>
<li>é€šè¿‡å¤§é‡å®éªŒéªŒè¯äº†æ¨¡å‹åœ¨ç”Ÿæˆä¸ç”¨æˆ·æ–‡æœ¬å’Œå±æ€§è¾“å…¥å¯¹é½çš„å±æ€§æ„ŸçŸ¥è¿åŠ¨æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¿™ç§æ–°æ–¹æ³•ä¸ºæ–‡æœ¬é©±åŠ¨çš„äººç±»è¿åŠ¨ç”Ÿæˆé¢†åŸŸå¼€è¾Ÿäº†æ–°çš„æ–¹å‘ï¼Œç‰¹åˆ«æ˜¯åœ¨è€ƒè™‘äººç±»å±æ€§çš„å½±å“æ–¹é¢ã€‚</li>
<li>éšç€è¿™ä¸€æŠ€æœ¯çš„è¿›ä¸€æ­¥å‘å±•ï¼Œæˆ‘ä»¬å¯ä»¥æœŸå¾…æ›´çœŸå®ã€æ›´ä¸ªæ€§åŒ–çš„è¿åŠ¨ç”Ÿæˆï¼Œä»è€Œåœ¨å„ç§åº”ç”¨ä¸­å®ç°æ›´å¹¿æ³›çš„ç”¨é€”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21912">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-82c3b1a3d22ade879e01268fb31f730f" align="middle">
<img src="https://picx.zhimg.com/v2-03789e06ac7e760d4b996a8cf7a500fe" align="middle">
<img src="https://picx.zhimg.com/v2-9dd72bf62918183d82316fd2adebf5f4" align="middle">
<img src="https://picx.zhimg.com/v2-a30c4c278b4a3e71f3d1396be4416916" align="middle">
<img src="https://picx.zhimg.com/v2-81f58f185143932514936871e6389eb7" align="middle">
<img src="https://picx.zhimg.com/v2-0eb30ba25255fcdf9e32c6ea32df6390" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Advanced-Sign-Language-Video-Generation-with-Compressed-and-Quantized-Multi-Condition-Tokenization"><a href="#Advanced-Sign-Language-Video-Generation-with-Compressed-and-Quantized-Multi-Condition-Tokenization" class="headerlink" title="Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization"></a>Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization</h2><p><strong>Authors:Cong Wang, Zexuan Deng, Zhiwei Jiang, Yafeng Yin, Fei Shen, Zifeng Cheng, Shiping Ge, Shiwei Gan, Qing Gu</strong></p>
<p>Sign Language Video Generation (SLVG) seeks to generate identity-preserving sign language videos from spoken language texts. Existing methods primarily rely on the single coarse condition (\eg, skeleton sequences) as the intermediary to bridge the translation model and the video generation model, which limits both the naturalness and expressiveness of the generated videos. To overcome these limitations, we propose SignViP, a novel SLVG framework that incorporates multiple fine-grained conditions for improved generation fidelity. Rather than directly translating error-prone high-dimensional conditions, SignViP adopts a discrete tokenization paradigm to integrate and represent fine-grained conditions (\ie, fine-grained poses and 3D hands). SignViP contains three core components. (1) Sign Video Diffusion Model is jointly trained with a multi-condition encoder to learn continuous embeddings that encapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization (FSQ) Autoencoder is further trained to compress and quantize these embeddings into discrete tokens for compact representation of the conditions. (3) Multi-Condition Token Translator is trained to translate spoken language text to discrete multi-condition tokens. During inference, Multi-Condition Token Translator first translates the spoken language text into discrete multi-condition tokens. These tokens are then decoded to continuous embeddings by FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion Model to guide video generation. Experimental results show that SignViP achieves state-of-the-art performance across metrics, including video quality, temporal coherence, and semantic fidelity. The code is available at <a target="_blank" rel="noopener" href="https://github.com/umnooob/signvip/">https://github.com/umnooob/signvip/</a>.</p>
<blockquote>
<p>æ‰‹åŠ¿è¯­è¨€è§†é¢‘ç”Ÿæˆï¼ˆSLVGï¼‰æ—¨åœ¨ä»å£è¯­æ–‡æœ¬ç”Ÿæˆä¿æŒèº«ä»½çš„æ‰‹åŠ¿è¯­è¨€è§†é¢‘ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºå•ä¸€ç²—ç•¥æ¡ä»¶ï¼ˆä¾‹å¦‚éª¨æ¶åºåˆ—ï¼‰ä½œä¸ºç¿»è¯‘æ¨¡å‹å’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¹‹é—´çš„æ¡¥æ¢ï¼Œè¿™é™åˆ¶äº†ç”Ÿæˆè§†é¢‘çš„è‡ªç„¶åº¦å’Œè¡¨ç°åŠ›ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†SignViPï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„SLVGæ¡†æ¶ï¼Œå®ƒç»“åˆäº†å¤šç§ç²¾ç»†æ¡ä»¶ï¼Œä»¥æé«˜ç”Ÿæˆçš„ä¿çœŸåº¦ã€‚SignViPæ²¡æœ‰ç›´æ¥ç¿»è¯‘å®¹æ˜“å‡ºé”™çš„é«˜ç»´æ¡ä»¶ï¼Œè€Œæ˜¯é‡‡ç”¨ç¦»æ•£æ ‡è®°åŒ–èŒƒå¼æ¥é›†æˆå’Œè¡¨ç¤ºç²¾ç»†æ¡ä»¶ï¼ˆå³ç²¾ç»†å§¿åŠ¿å’Œ3Dæ‰‹ï¼‰ã€‚SignViPåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚ï¼ˆ1ï¼‰æ‰‹åŠ¿è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸å¤šæ¡ä»¶ç¼–ç å™¨å…±åŒè®­ç»ƒï¼Œå­¦ä¹ è¿ç»­çš„åµŒå…¥ï¼Œè¿™äº›åµŒå…¥åŒ…å«ç²¾ç»†è¿åŠ¨å’Œå¤–è§‚ã€‚ ï¼ˆ2ï¼‰æœ‰é™æ ‡é‡é‡åŒ–ï¼ˆFSQï¼‰è‡ªåŠ¨ç¼–ç å™¨è¿›ä¸€æ­¥è®­ç»ƒè¿™äº›åµŒå…¥ï¼Œå°†å…¶å‹ç¼©å¹¶é‡åŒ–ä¸ºç¦»æ•£æ ‡è®°ï¼Œä»¥ç´§å‡‘è¡¨ç¤ºæ¡ä»¶ã€‚ï¼ˆ3ï¼‰å¤šæ¡ä»¶æ ‡è®°ç¿»è¯‘å™¨è¢«è®­ç»ƒç”¨äºå°†å£è¯­æ–‡æœ¬ç¿»è¯‘æˆç¦»æ•£çš„å¤šæ¡ä»¶æ ‡è®°ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¤šæ¡ä»¶æ ‡è®°ç¿»è¯‘å™¨é¦–å…ˆå°†å£è¯­æ–‡æœ¬ç¿»è¯‘æˆç¦»æ•£çš„å¤šæ¡ä»¶æ ‡è®°ã€‚è¿™äº›æ ‡è®°éšåè¢«FSQè‡ªåŠ¨ç¼–ç å™¨è§£ç ä¸ºè¿ç»­çš„åµŒå…¥ï¼Œç„¶åæ³¨å…¥æ‰‹åŠ¿è§†é¢‘æ‰©æ•£æ¨¡å‹ä»¥æŒ‡å¯¼è§†é¢‘ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSignViPåœ¨è§†é¢‘è´¨é‡ã€æ—¶é—´è¿è´¯æ€§å’Œè¯­ä¹‰ä¿çœŸåº¦ç­‰å„é¡¹æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ä»£ç å¯ç”¨<a target="_blank" rel="noopener" href="https://github.com/umnooob/signvip/%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/umnooob/signvip/è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.15980v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Sign Language Video Generationï¼ˆSLVGï¼‰çš„æŒ‘æˆ˜åŠç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚ä¸ºæ”¹è¿›ç”Ÿæˆè§†é¢‘çš„é€¼çœŸåº¦å’Œè¡¨ç°åŠ›ï¼Œæå‡ºäº†SignViPæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥å¤šç§ç²¾ç»†æ¡ä»¶æ¥æé«˜ç”Ÿæˆç²¾åº¦ã€‚SignViPåŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šSign Video Diffusion Modelã€Finite Scalar Quantization (FSQ) Autoencoderå’Œå¤šæ¡ä»¶ç¿»è¯‘å™¨ã€‚è¯¥æ¡†æ¶é€šè¿‡ç¦»æ•£æ ‡è®°æ³•æ•´åˆå’Œè¡¨è¾¾ç²¾ç»†æ¡ä»¶ï¼Œå¦‚ç²¾ç»†å§¿æ€å’Œä¸‰ç»´æ‰‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSignViPåœ¨è§†é¢‘è´¨é‡ã€æ—¶é—´è¿è´¯æ€§å’Œè¯­ä¹‰ä¿çœŸåº¦ç­‰æ–¹é¢è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SLVGæ—¨åœ¨ä»å£è¯­æ–‡æœ¬ç”Ÿæˆèº«ä»½ä¿ç•™çš„è‚¢ä½“è¯­è¨€è§†é¢‘ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å•ä¸€ç²—ç•¥æ¡ä»¶ä½œä¸ºç¿»è¯‘æ¨¡å‹å’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¹‹é—´çš„æ¡¥æ¢ï¼Œé™åˆ¶äº†ç”Ÿæˆè§†é¢‘çš„è‡ªç„¶æ€§å’Œè¡¨ç°åŠ›ã€‚</li>
<li>SignViPæ¡†æ¶é€šè¿‡å¼•å…¥å¤šç§ç²¾ç»†æ¡ä»¶æé«˜ç”Ÿæˆç²¾åº¦ï¼Œå¦‚ç²¾ç»†å§¿æ€å’Œä¸‰ç»´æ‰‹ã€‚</li>
<li>SignViPåŒ…æ‹¬ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šSign Video Diffusion Modelã€FSQ Autoencoderå’Œå¤šæ¡ä»¶ç¿»è¯‘å™¨ã€‚</li>
<li>SignViPé‡‡ç”¨ç¦»æ•£æ ‡è®°æ³•æ•´åˆå’Œè¡¨è¾¾ç²¾ç»†æ¡ä»¶ï¼Œæé«˜ç”Ÿæˆè§†é¢‘çš„é€¼çœŸåº¦ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSignViPåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼ŒåŒ…æ‹¬è§†é¢‘è´¨é‡ã€æ—¶é—´è¿è´¯æ€§å’Œè¯­ä¹‰ä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.15980">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1c5a6db6bc09f397775d6a29cc548689" align="middle">
<img src="https://picx.zhimg.com/v2-6f7eac2593b8fb60d88893fa32ecd9cf" align="middle">
<img src="https://picx.zhimg.com/v2-f30f4794c289661318804f0e1fc7285e" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Free-T2M-Robust-Text-to-Motion-Generation-for-Humanoid-Robots-via-Frequency-Domain"><a href="#Free-T2M-Robust-Text-to-Motion-Generation-for-Humanoid-Robots-via-Frequency-Domain" class="headerlink" title="Free-T2M: Robust Text-to-Motion Generation for Humanoid Robots via Frequency-Domain"></a>Free-T2M: Robust Text-to-Motion Generation for Humanoid Robots via Frequency-Domain</h2><p><strong>Authors:Wenshuo Chen, Haozhe Jia, Songning Lai, Lei Wang, Yuqi Lin, Hongru Xiao, Lijie Hu, Yutao Yue</strong></p>
<p>Enabling humanoid robots to synthesize complex, physically coherent motions from natural language commands is a cornerstone of autonomous robotics and human-robot interaction. While diffusion models have shown promise in this text-to-motion (T2M) task, they often generate semantically flawed or unstable motions, limiting their applicability to real-world robots. This paper reframes the T2M problem from a frequency-domain perspective, revealing that the generative process mirrors a hierarchical control paradigm. We identify two critical phases: a semantic planning stage, where low-frequency components establish the global motion trajectory, and a fine-grained execution stage, where high-frequency details refine the movement. To address the distinct challenges of each phase, we introduce Frequency enhanced text-to-motion (Free-T2M), a framework incorporating stage-specific frequency-domain consistency alignment. We design a frequency-domain temporal-adaptive module to modulate the alignment effects of different frequency bands. These designs enforce robustness in the foundational semantic plan and enhance the accuracy of detailed execution. Extensive experiments show our method dramatically improves motion quality and semantic correctness. Notably, when applied to the StableMoFusion baseline, Free-T2M reduces the FID from 0.152 to 0.060, establishing a new state-of-the-art within diffusion architectures. These findings underscore the critical role of frequency-domain insights for generating robust and reliable motions, paving the way for more intuitive natural language control of robots.</p>
<blockquote>
<p>å®ç°äººå½¢æœºå™¨äººä»è‡ªç„¶è¯­è¨€å‘½ä»¤ä¸­åˆæˆå¤æ‚ä¸”ç‰©ç†è¿è´¯çš„åŠ¨ä½œï¼Œæ˜¯è‡ªä¸»æœºå™¨äººæŠ€æœ¯å’Œäººæœºäº¤äº’çš„æ ¸å¿ƒéƒ¨åˆ†ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°åŠ¨ä½œï¼ˆT2Mï¼‰ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸ä¼šäº§ç”Ÿè¯­ä¹‰ä¸Šæœ‰ç¼ºé™·æˆ–ä¸ç¨³å®šçš„æ´»åŠ¨ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨çœŸå®ä¸–ç•Œæœºå™¨äººä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡ä»é¢‘åŸŸè§’åº¦é‡æ–°å®šä¹‰äº†T2Mé—®é¢˜ï¼Œå‘ç°ç”Ÿæˆè¿‡ç¨‹åæ˜ äº†ä¸€ç§åˆ†å±‚æ§åˆ¶èŒƒå¼ã€‚æˆ‘ä»¬ç¡®å®šäº†ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼šè¯­ä¹‰è§„åˆ’é˜¶æ®µï¼Œä½é¢‘ç»„ä»¶ç¡®å®šå…¨å±€è¿åŠ¨è½¨è¿¹ï¼›ç²¾ç»†æ‰§è¡Œé˜¶æ®µï¼Œé«˜é¢‘ç»†èŠ‚å®Œå–„è¿åŠ¨ã€‚ä¸ºäº†è§£å†³æ¯ä¸ªé˜¶æ®µçš„ç‰¹å®šæŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢‘ç‡å¢å¼ºæ–‡æœ¬åˆ°åŠ¨ä½œï¼ˆFree-T2Mï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†é˜¶æ®µç‰¹å®šçš„é¢‘åŸŸä¸€è‡´æ€§å¯¹é½ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé¢‘åŸŸæ—¶åºè‡ªé€‚åº”æ¨¡å—ï¼Œä»¥è°ƒèŠ‚ä¸åŒé¢‘å¸¦çš„å¯¹é½æ•ˆæœã€‚è¿™äº›è®¾è®¡å¢å¼ºäº†åŸºç¡€è¯­ä¹‰è®¡åˆ’çš„ç¨³å¥æ€§ï¼Œæé«˜äº†è¯¦ç»†æ‰§è¡Œçš„å‡†ç¡®æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†è¿åŠ¨è´¨é‡å’Œè¯­ä¹‰æ­£ç¡®æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“åº”ç”¨äºStableMoFusionåŸºçº¿æ—¶ï¼ŒFree-T2Må°†FIDä»0.152é™ä½åˆ°0.060ï¼Œåœ¨æ‰©æ•£æ¶æ„ä¸­å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ°´å¹³ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†é¢‘åŸŸæ´å¯ŸåŠ›å¯¹äºç”Ÿæˆç¨³å¥å’Œå¯é è¿åŠ¨çš„å…³é”®ä½œç”¨ï¼Œä¸ºäººå½¢æœºå™¨äººæ›´ç›´è§‚çš„è‡ªç„¶è¯­è¨€æ§åˆ¶é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18232v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç±»äººæœºå™¨äººèƒ½å¤Ÿä»è‡ªç„¶è¯­è¨€å‘½ä»¤ä¸­åˆæˆå¤æ‚ã€ç‰©ç†è¿è´¯çš„è¿åŠ¨çš„é‡è¦æ€§ã€‚é’ˆå¯¹æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°è¿åŠ¨ï¼ˆT2Mï¼‰ä»»åŠ¡ä¸­ç”Ÿæˆè¯­ä¹‰ç¼ºé™·æˆ–ä¸ç¨³å®šè¿åŠ¨çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä»é¢‘ç‡åŸŸè§’åº¦é‡æ–°è€ƒè™‘T2Mé—®é¢˜ï¼Œå¹¶å¼•å…¥é¢‘ç‡å¢å¼ºæ–‡æœ¬åˆ°è¿åŠ¨ï¼ˆFree-T2Mï¼‰æ¡†æ¶ï¼Œé€šè¿‡é˜¶æ®µç‰¹å®šçš„é¢‘ç‡åŸŸä¸€è‡´æ€§å¯¹é½æ¥è§£å†³ä¸¤ä¸ªå…³é”®é˜¶æ®µâ€”â€”è¯­ä¹‰è§„åˆ’é˜¶æ®µå’Œç²¾ç»†æ‰§è¡Œé˜¶æ®µçš„æŒ‘æˆ˜ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜è¿åŠ¨è´¨é‡å’Œè¯­ä¹‰å‡†ç¡®æ€§ï¼Œå½“åº”ç”¨äºStableMoFusionåŸºçº¿æ—¶ï¼ŒFIDä»0.152é™ä½åˆ°0.060ï¼Œå»ºç«‹äº†ä¸€ä¸ªæ–°çš„æ‰©æ•£æ¶æ„å†…çš„æœ€ä½³æ°´å¹³ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†é¢‘ç‡åŸŸæ´å¯Ÿå¯¹äºç”Ÿæˆç¨³å¥å’Œå¯é è¿åŠ¨çš„å…³é”®ä½œç”¨ï¼Œä¸ºæ›´ç›´è§‚çš„æœºå™¨äººè‡ªç„¶è¯­è¨€æ§åˆ¶é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç±»äººæœºå™¨äººä»è‡ªç„¶è¯­è¨€å‘½ä»¤ä¸­åˆæˆå¤æ‚ã€ç‰©ç†è¿è´¯çš„è¿åŠ¨æ˜¯è‡ªä¸»æœºå™¨äººå’Œäººç±»-æœºå™¨äººäº¤äº’çš„æ ¸å¿ƒä»»åŠ¡ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°è¿åŠ¨ï¼ˆT2Mï¼‰ä»»åŠ¡ä¸­å¸¸ç”Ÿæˆè¯­ä¹‰ç¼ºé™·æˆ–ä¸ç¨³å®šè¿åŠ¨ã€‚</li>
<li>æœ¬æ–‡ä»é¢‘ç‡åŸŸè§’åº¦é‡æ–°è€ƒè™‘T2Mé—®é¢˜ï¼Œå¹¶æå‡ºé¢‘ç‡å¢å¼ºæ–‡æœ¬åˆ°è¿åŠ¨ï¼ˆFree-T2Mï¼‰æ¡†æ¶ã€‚</li>
<li>Free-T2Mæ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼šè¯­ä¹‰è§„åˆ’é˜¶æ®µå’Œç²¾ç»†æ‰§è¡Œé˜¶æ®µã€‚</li>
<li>Free-T2Mé€šè¿‡é˜¶æ®µç‰¹å®šçš„é¢‘ç‡åŸŸä¸€è‡´æ€§å¯¹é½æ¥è§£å†³è¿™ä¸¤ä¸ªé˜¶æ®µçš„æŒ‘æˆ˜ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºFree-T2Mæ˜¾è‘—æé«˜äº†è¿åŠ¨è´¨é‡å’Œè¯­ä¹‰å‡†ç¡®æ€§ï¼Œå¹¶å»ºç«‹äº†æ–°çš„æ‰©æ•£æ¶æ„å†…çš„æœ€ä½³æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18232">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a531ef01239fc85eb14fc2ededab86b" align="middle">
<img src="https://picx.zhimg.com/v2-e77310e8914868c0387b220ef9571426" align="middle">
<img src="https://picx.zhimg.com/v2-4633283b3ebf8cd9ac75d743abced625" align="middle">
<img src="https://picx.zhimg.com/v2-367e637e914e00276b4d15e60cbfbcc4" align="middle">
<img src="https://picx.zhimg.com/v2-2a5b1ab201074b6fd0a01bc9decdc7e8" align="middle">
<img src="https://picx.zhimg.com/v2-f36374caf274a841939f6950c7834c52" align="middle">
<img src="https://picx.zhimg.com/v2-98428513d573f09079c740dabf7c8b11" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LangPose-Language-Aligned-Motion-for-Robust-3D-Human-Pose-Estimation"><a href="#LangPose-Language-Aligned-Motion-for-Robust-3D-Human-Pose-Estimation" class="headerlink" title="LangPose: Language-Aligned Motion for Robust 3D Human Pose Estimation"></a>LangPose: Language-Aligned Motion for Robust 3D Human Pose Estimation</h2><p><strong>Authors:Longyun Liao, Rong Zheng</strong></p>
<p>2D-to-3D human pose lifting is an ill-posed problem due to depth ambiguity and occlusion. Existing methods relying on spatial and temporal consistency alone are insufficient to resolve these problems especially in the presence of significant occlusions or high dynamic actions. Semantic information, however, offers a complementary signal that can help disambiguate such cases. To this end, we propose LangPose, a framework that leverages action knowledge by aligning motion embeddings with text embeddings of fine-grained action labels. LangPose operates in two stages: pretraining and fine-tuning. In the pretraining stage, the model simultaneously learns to recognize actions and reconstruct 3D poses from masked and noisy 2D poses. During the fine-tuning stage, the model is further refined using real-world 3D human pose estimation datasets without action labels. Additionally, our framework incorporates masked body parts and masked time windows in motion modeling, encouraging the model to leverage semantic information when spatial and temporal consistency is unreliable. Experiments demonstrate the effectiveness of LangPose, achieving SOTA level performance in 3D pose estimation on public datasets, including Human3.6M and MPI-INF-3DHP. Specifically, LangPose achieves an MPJPE of 36.7mm on Human3.6M with detected 2D poses as input and 15.5mm on MPI-INF-3DHP with ground-truth 2D poses as input.</p>
<blockquote>
<p>äºŒç»´åˆ°ä¸‰ç»´çš„äººä½“å§¿æ€æå‡æ˜¯ä¸€ä¸ªç—…æ€é—®é¢˜ï¼ŒåŸå› åœ¨äºæ·±åº¦æ¨¡ç³Šå’Œé®æŒ¡ã€‚ç°æœ‰ä»…ä¾èµ–ç©ºé—´å’Œæ—¶é—´ä¸€è‡´æ€§çš„æ–¹æ³•ä¸è¶³ä»¥è§£å†³è¿™äº›é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨æ˜¾è‘—é®æŒ¡æˆ–é«˜åŠ¨æ€åŠ¨ä½œçš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œè¯­ä¹‰ä¿¡æ¯æä¾›äº†ä¸€ç§è¡¥å……ä¿¡å·ï¼Œå¯ä»¥å¸®åŠ©è§£å†³è¿™äº›é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LangPoseæ¡†æ¶ï¼Œå®ƒé€šè¿‡å°†å¯¹è¿åŠ¨åµŒå…¥ä¸ç²¾ç»†åŠ¨ä½œæ ‡ç­¾çš„æ–‡æœ¬åµŒå…¥å¯¹é½ï¼Œåˆ©ç”¨åŠ¨ä½œçŸ¥è¯†ã€‚LangPoseåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¢„è®­ç»ƒå’Œå¾®è°ƒã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹åŒæ—¶å­¦ä¹ è¯†åˆ«åŠ¨ä½œå¹¶ä»é®æŒ¡å’Œå˜ˆæ‚çš„äºŒç»´å§¿æ€é‡å»ºä¸‰ç»´å§¿æ€ã€‚åœ¨å¾®è°ƒé˜¶æ®µï¼Œæ¨¡å‹ä½¿ç”¨æ²¡æœ‰åŠ¨ä½œæ ‡ç­¾çš„çœŸå®ä¸–ç•Œä¸‰ç»´äººä½“å§¿æ€ä¼°è®¡æ•°æ®é›†è¿›è¡Œè¿›ä¸€æ­¥å¾®è°ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨è¿åŠ¨å»ºæ¨¡ä¸­èå…¥äº†é®æŒ¡çš„èº«ä½“éƒ¨ä½å’Œé®æŒ¡çš„æ—¶é—´çª—å£ï¼Œé¼“åŠ±æ¨¡å‹åœ¨ç©ºé—´å’Œæ—¶é—´çš„å¯é æ€§è¾ƒä½æ—¶åˆ©ç”¨è¯­ä¹‰ä¿¡æ¯ã€‚å®éªŒè¯æ˜äº†LangPoseçš„æœ‰æ•ˆæ€§ï¼Œåœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„ä¸‰ç»´å§¿æ€ä¼°è®¡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼ŒåŒ…æ‹¬Human3.6Må’ŒMPI-INF-3DHPæ•°æ®é›†ã€‚å…·ä½“æ¥è¯´ï¼ŒLangPoseåœ¨Human3.6Mæ•°æ®é›†ä¸Šè¾“å…¥çš„æ£€æµ‹åˆ°çš„äººä½“äºŒç»´å§¿æ€ä¸‹å®ç°äº†å¹³å‡å…³èŠ‚ä½ç½®è¯¯å·®ï¼ˆMPJPEï¼‰ä¸º36.7æ¯«ç±³çš„å‡†ç¡®åº¦ï¼Œå¹¶åœ¨MPI-INF-3DHPæ•°æ®é›†ä¸Šè¾“å…¥çš„åœ°é¢çœŸå®äºŒç»´å§¿æ€ä¸‹å®ç°äº†MPJPEä¸º15.5æ¯«ç±³çš„å‡†ç¡®åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.00449v2">PDF</a> Accepted by WACV2026. Please find the supplementary material under the â€œAncillary filesâ€</p>
<p><strong>æ‘˜è¦</strong><br>    æ–‡æœ¬æå‡ºä¸€ä¸ªåä¸ºLangPoseçš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç”±äºæ·±åº¦æ­§ä¹‰å’Œé®æŒ¡å¯¼è‡´çš„äºŒç»´åˆ°ä¸‰ç»´äººä½“å§¿æ€æå‡çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŠ¨ä½œçŸ¥è¯†ï¼Œé€šè¿‡å¯¹è¿åŠ¨åµŒå…¥å’Œç²¾ç»†åŠ¨ä½œæ ‡ç­¾æ–‡æœ¬åµŒå…¥çš„å¯¹é½æ¥è¾…åŠ©è§£å†³è¿™äº›é—®é¢˜ã€‚LangPoseåˆ†ä¸ºé¢„è®­ç»ƒå’Œå¾®è°ƒä¸¤ä¸ªé˜¶æ®µï¼Œå¹¶åœ¨å…¬å…±æ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›æ°´å¹³çš„ä¸‰ç»´å§¿æ€ä¼°è®¡æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>2D-to-3Däººä½“å§¿æ€æå‡æ˜¯ä¸€ä¸ªå› æ·±åº¦æ­§ä¹‰å’Œé®æŒ¡è€Œå‡ºç°çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä»…ä¾èµ–ç©ºé—´å’Œæ—¶é—´ä¸€è‡´æ€§æ— æ³•æœ‰æ•ˆè§£å†³è¿™äº›é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨æ˜¾è‘—é®æŒ¡æˆ–é«˜åŠ¨æ€åŠ¨ä½œæ—¶ã€‚</li>
<li>LangPoseæ¡†æ¶é€šè¿‡ç»“åˆåŠ¨ä½œçŸ¥è¯†ï¼Œåˆ©ç”¨è¿åŠ¨åµŒå…¥ä¸ç²¾ç»†åŠ¨ä½œæ ‡ç­¾æ–‡æœ¬åµŒå…¥çš„å¯¹é½æ¥è¾…åŠ©è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>LangPoseåˆ†ä¸ºé¢„è®­ç»ƒå’Œå¾®è°ƒä¸¤ä¸ªé˜¶æ®µï¼Œé¢„è®­ç»ƒé˜¶æ®µä¸­å­¦ä¹ è¯†åˆ«åŠ¨ä½œå¹¶ä»é®æŒ¡å’Œå™ªå£°çš„äºŒç»´å§¿æ€é‡å»ºä¸‰ç»´å§¿æ€ï¼Œå¾®è°ƒé˜¶æ®µåˆ™ä½¿ç”¨çœŸå®ä¸–ç•Œçš„ä¸‰ç»´äººä½“å§¿æ€ä¼°è®¡æ•°æ®é›†è¿›ä¸€æ­¥ç²¾ç‚¼æ¨¡å‹ã€‚</li>
<li>LangPoseæ¡†æ¶åœ¨è¿åŠ¨ä¸­èå…¥äº†æ©è†œèº«ä½“éƒ¨ä½å’Œæ©è†œæ—¶é—´çª—å£ï¼Œé¼“åŠ±æ¨¡å‹åœ¨ç©ºé—´å’Œæ—¶é—´çš„å¯é æ€§ä¸å¯é æ—¶åˆ©ç”¨è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLangPoseåœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„ä¸‰ç»´å§¿æ€ä¼°è®¡æ€§èƒ½è¾¾åˆ°å…ˆè¿›æ°´å¹³ï¼Œå¦‚Human3.6Må’ŒMPI-INF-3DHPæ•°æ®é›†ã€‚</li>
<li>LangPoseåœ¨Human3.6Mæ•°æ®é›†ä¸Šå®ç°çš„MPJPEä¸º36.7mmï¼ˆä½¿ç”¨æ£€æµ‹åˆ°çš„äºŒç»´å§¿æ€ä½œä¸ºè¾“å…¥ï¼‰ï¼Œåœ¨MPI-INF-3DHPæ•°æ®é›†ä¸Šä¸º15.5mmï¼ˆä½¿ç”¨åœ°é¢çœŸå®çš„äºŒç»´å§¿æ€ä½œä¸ºè¾“å…¥ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.00449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3b7663c5771e4624e8c9df2a41c4f92b" align="middle">
<img src="https://picx.zhimg.com/v2-8af2675d1638d3e11307df99ae44c0de" align="middle">
<img src="https://picx.zhimg.com/v2-0b99e4876008ad4f1a133a6843fa8125" align="middle">
<img src="https://picx.zhimg.com/v2-209aad73cee3c43346ac2884e53073fa" align="middle">
<img src="https://picx.zhimg.com/v2-9fa2822a198ea8fb7910be0b23d6fe16" align="middle">
<img src="https://picx.zhimg.com/v2-596f34716851d2a6b4d723118b304cba" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="A-multi-phase-thermo-mechanical-model-for-rock-ice-avalanche"><a href="#A-multi-phase-thermo-mechanical-model-for-rock-ice-avalanche" class="headerlink" title="A multi-phase thermo-mechanical model for rock-ice avalanche"></a>A multi-phase thermo-mechanical model for rock-ice avalanche</h2><p><strong>Authors:Shiva P. Pudasaini</strong></p>
<p>We propose a novel multi-phase thermo-mechanical rock-ice avalanche model. It considers rock, ice and fluid; includes rigorously derived ice melt rate, melting efficiency dependent fluid production rate and a general temperature equation. It explains advection-diffusion of heat including heat exchange across the avalanche, basal heat conduction, production and loss of heat due to frictional shearing and changing temperature, and temperature enhancement due to entrainment. Temperature equation couples rates of thermal conductivity and temperature. Ice melt intensity determines these rates as mixture conductivity evolves, characterizing thermo-mechanical processes. The model includes interfacial mass and momentum exchanges and mass and momentum productions due to entrainment. The latter significantly changes the state of temperature; yet, the former characterizes the rock-ice avalanche. Phase mass and momentum balances and temperature are coupled. New model offers the first-ever complete dynamical solution for rock-ice avalanche with changing temperature and ice melting. We develop an advection-diffusion-decay-source model and its analytical solutions providing novel understanding of temperature evolution. The 2021 Chamoli event simulations with r$.$avaflow (<a target="_blank" rel="noopener" href="https://www.landslidemodels.org/r.avaflow/">https://www.landslidemodels.org/r.avaflow/</a>) illustrate the functionality of thermo-mechanical rock-ice avalanche model. Four scenarios are considered: variations in ice-melt-efficiency; fraction of ice; ice and rock frictions; governing the process of melting, flow transformation, spreading and mobility. Ice melting designates the motion and explains the rock-ice avalanche mobility: a phenomenal thermo-mechanical play. Essentially different controls of ice and rock frictions on the state of flow mobility are revealed, explaining complex thermo-mechanical processes. This provides a useful method for practitioners and engineers in solving problems associated with rock-ice avalanches.</p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šé˜¶æ®µçƒ­æœºæ¢°å²©å†°å´©æ¨¡å‹ã€‚è¯¥æ¨¡å‹è€ƒè™‘äº†å²©çŸ³ã€å†°å’Œæµä½“ï¼ŒåŒ…æ‹¬ä¸¥æ ¼æ¨å¯¼çš„å†°èåŒ–é€Ÿç‡ã€ä¾èµ–äºèåŒ–æ•ˆç‡çš„æµä½“äº§ç”Ÿé€Ÿç‡å’Œä¸€èˆ¬çš„æ¸©åº¦æ–¹ç¨‹ã€‚å®ƒè§£é‡Šäº†çƒ­çš„å¯¹æµ-æ‰©æ•£ï¼ŒåŒ…æ‹¬å´©æ»‘ä¸­çš„çƒ­äº¤æ¢ã€åº•éƒ¨çƒ­ä¼ å¯¼ã€å› æ‘©æ“¦å‰ªåˆ‡å’Œæ¸©åº¦å˜åŒ–äº§ç”Ÿçš„çƒ­é‡ä»¥åŠå› å¤¹å¸¦é€ æˆçš„æ¸©åº¦å¢å¼ºã€‚æ¸©åº¦æ–¹ç¨‹ç»“åˆäº†çƒ­å¯¼ç‡å’Œæ¸©åº¦çš„å˜åŒ–ç‡ã€‚å†°èåŒ–çš„å¼ºåº¦å†³å®šäº†è¿™äº›é€Ÿç‡ï¼Œéšç€æ··åˆç‰©çš„çƒ­å¯¼ç‡çš„å˜åŒ–è€Œå˜åŒ–ï¼Œè¡¨å¾äº†çƒ­æœºæ¢°è¿‡ç¨‹ã€‚è¯¥æ¨¡å‹åŒ…æ‹¬ç•Œé¢è´¨é‡å’ŒåŠ¨é‡äº¤æ¢ä»¥åŠå› å¤¹å¸¦è€Œäº§ç”Ÿçš„è´¨é‡å’ŒåŠ¨é‡ç”Ÿæˆã€‚åè€…å¯¹æ¸©åº¦çŠ¶æ€æœ‰æ˜¾è‘—å½±å“ï¼›ç„¶è€Œï¼Œå‰è€…åˆ™ä»£è¡¨äº†å²©å†°å´©çš„ç‰¹ç‚¹ã€‚é˜¶æ®µè´¨é‡ã€åŠ¨é‡å¹³è¡¡å’Œæ¸©åº¦æ˜¯ç›¸äº’å…³è”çš„ã€‚æ–°æ¨¡å‹ä¸ºå…·æœ‰å¯å˜æ¸©åº¦çš„å²©å†°å´©æä¾›äº†é¦–ä¸ªå®Œæ•´çš„åŠ¨åŠ›å­¦è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå¯¹æµ-æ‰©æ•£-è¡°å‡-æºæ¨¡å‹åŠå…¶è§£æè§£ï¼Œä¸ºæ¸©åº¦æ¼”å˜æä¾›äº†æ–°çš„ç†è§£ã€‚ä½¿ç”¨r.avaflowï¼ˆ<a target="_blank" rel="noopener" href="https://www.landslidemodels.org/r.avaflow/%EF%BC%89%E5%AF%B92021%E5%B9%B4%E6%9F%A5%E8%8E%AB%E5%88%A9%E4%BA%8B%E4%BB%B6%E7%9A%84%E6%A8%A1%E6%8B%9F%E8%AF%B4%E6%98%8E%E4%BA%86%E7%83%AD%E6%9C%BA%E6%A2%B0%E5%B2%A9%E5%86%B0%E5%B4%A9%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8A%9F%E8%83%BD%E3%80%82%E8%80%83%E8%99%91%E4%BA%86%E5%9B%9B%E7%A7%8D%E6%83%85%E6%99%AF%EF%BC%9A%E5%86%B0%E8%9E%8D%E5%8C%96%E6%95%88%E7%8E%87%E7%9A%84%E5%8F%98%E5%8C%96%E3%80%81%E5%86%B0%E7%9A%84%E6%AF%94%E4%BE%8B%E3%80%81%E5%86%B0%E5%92%8C%E5%B2%A9%E7%9F%B3%E7%9A%84%E6%91%A9%E6%93%A6%E5%8A%9B%E4%BB%A5%E5%8F%8A%E6%8E%A7%E5%88%B6%E8%9E%8D%E5%8C%96%E3%80%81%E6%B5%81%E5%8A%A8%E8%BD%AC%E5%8C%96%E3%80%81%E6%89%A9%E6%95%A3%E5%92%8C%E7%A7%BB%E5%8A%A8%E7%9A%84%E8%BF%87%E7%A8%8B%E3%80%82%E5%86%B0%E7%9A%84%E8%9E%8D%E5%8C%96%E6%A0%87%E5%BF%97%E7%9D%80%E8%BF%90%E5%8A%A8%EF%BC%8C%E5%B9%B6%E8%A7%A3%E9%87%8A%E4%BA%86%E5%B2%A9%E5%86%B0%E5%B4%A9%E7%9A%84%E7%A7%BB%E5%8A%A8%E8%83%BD%E5%8A%9B%EF%BC%9A%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E7%8E%B0%E8%B1%A1%E7%BA%A7%E7%9A%84%E7%83%AD%E6%9C%BA%E6%A2%B0%E8%BF%87%E7%A8%8B%E3%80%82%E6%8F%AD%E7%A4%BA%E4%BA%86%E5%86%B0%E5%92%8C%E5%B2%A9%E7%9F%B3%E6%91%A9%E6%93%A6%E5%8A%9B%E5%AF%B9%E6%B5%81%E5%8A%A8%E7%8A%B6%E6%80%81%E7%9A%84%E4%B8%8D%E5%90%8C%E6%8E%A7%E5%88%B6%EF%BC%8C%E8%A7%A3%E9%87%8A%E4%BA%86%E5%A4%8D%E6%9D%82%E7%9A%84%E7%83%AD%E6%9C%BA%E6%A2%B0%E8%BF%87%E7%A8%8B%E3%80%82%E8%BF%99%E4%B8%BA%E4%BB%8E%E4%B8%9A%E4%BA%BA%E5%91%98%E5%92%8C%E5%B7%A5%E7%A8%8B%E5%B8%88%E8%A7%A3%E5%86%B3%E4%B8%8E%E5%B2%A9%E5%86%B0%E5%B4%A9%E7%9B%B8%E5%85%B3%E7%9A%84%E9%97%AE%E9%A2%98%E6%8F%90%E4%BE%9B%E4%BA%86%E4%B8%80%E7%A7%8D%E6%9C%89%E7%94%A8%E7%9A%84%E6%96%B9%E6%B3%95%E3%80%82">https://www.landslidemodels.org/r.avaflow/ï¼‰å¯¹2021å¹´æŸ¥è«åˆ©äº‹ä»¶çš„æ¨¡æ‹Ÿè¯´æ˜äº†çƒ­æœºæ¢°å²©å†°å´©æ¨¡å‹çš„åŠŸèƒ½ã€‚è€ƒè™‘äº†å››ç§æƒ…æ™¯ï¼šå†°èåŒ–æ•ˆç‡çš„å˜åŒ–ã€å†°çš„æ¯”ä¾‹ã€å†°å’Œå²©çŸ³çš„æ‘©æ“¦åŠ›ä»¥åŠæ§åˆ¶èåŒ–ã€æµåŠ¨è½¬åŒ–ã€æ‰©æ•£å’Œç§»åŠ¨çš„è¿‡ç¨‹ã€‚å†°çš„èåŒ–æ ‡å¿—ç€è¿åŠ¨ï¼Œå¹¶è§£é‡Šäº†å²©å†°å´©çš„ç§»åŠ¨èƒ½åŠ›ï¼šè¿™æ˜¯ä¸€ä¸ªç°è±¡çº§çš„çƒ­æœºæ¢°è¿‡ç¨‹ã€‚æ­ç¤ºäº†å†°å’Œå²©çŸ³æ‘©æ“¦åŠ›å¯¹æµåŠ¨çŠ¶æ€çš„ä¸åŒæ§åˆ¶ï¼Œè§£é‡Šäº†å¤æ‚çš„çƒ­æœºæ¢°è¿‡ç¨‹ã€‚è¿™ä¸ºä»ä¸šäººå‘˜å’Œå·¥ç¨‹å¸ˆè§£å†³ä¸å²©å†°å´©ç›¸å…³çš„é—®é¢˜æä¾›äº†ä¸€ç§æœ‰ç”¨çš„æ–¹æ³•ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.06130v4">PDF</a> Section 4 added on application, text enhanced accordingly</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹å¤šé˜¶æ®µçƒ­æœºæ¢°å²©çŸ³å†°å´©æ¨¡å‹ï¼Œè€ƒè™‘äº†å²©çŸ³ã€å†°å’Œæµä½“ï¼ŒåŒ…æ‹¬ä¸¥æ ¼æ¨å¯¼çš„å†°èåŒ–é€Ÿç‡ã€ä¾èµ–äºèåŒ–æ•ˆç‡çš„æµä½“äº§ç”Ÿé€Ÿç‡å’Œä¸€èˆ¬æ¸©åº¦æ–¹ç¨‹ã€‚è¯¥æ¨¡å‹è§£é‡Šäº†çƒ­å¯¹æµæ‰©æ•£ï¼ŒåŒ…æ‹¬å†°å´©ä¸­çš„çƒ­äº¤æ¢ã€åº•éƒ¨çƒ­ä¼ å¯¼ã€å› æ‘©æ“¦å‰ªåˆ‡å’Œæ¸©åº¦å˜åŒ–äº§ç”Ÿçš„çƒ­é‡æŸå¤±ä»¥åŠå› å¤¹å¸¦é€ æˆçš„æ¸©åº¦å¢å¼ºã€‚æ–°å‹æ¨¡å‹ä¸ºå²©çŸ³å†°å´©æä¾›äº†é¦–ä¸ªå®Œæ•´çš„åŠ¨æ€è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰æ¸©åº¦å˜åŒ–åŠå†°èåŒ–çš„ç‰¹ç‚¹ã€‚æ¨¡å‹è¿˜å»ºç«‹äº†ç•Œé¢è´¨é‡å’ŒåŠ¨é‡äº¤æ¢ä»¥åŠå› å¤¹å¸¦å¼•èµ·çš„è´¨é‡å’ŒåŠ¨é‡äº§ç”Ÿã€‚é€šè¿‡æ¨¡æ‹Ÿ2021å¹´çš„Chamoliäº‹ä»¶ï¼Œå±•ç¤ºäº†è¯¥çƒ­æœºæ¢°å²©çŸ³å†°å´©æ¨¡å‹çš„åŠŸèƒ½æ€§ã€‚è¯¥æ¨¡å‹æ­ç¤ºäº†ä¸åŒå†°èåŒ–æ•ˆç‡å’Œæ‘©æ“¦ç³»æ•°ä¸‹çš„å†°å´©è¿‡ç¨‹ä¸­çš„å››ä¸ªåœºæ™¯ï¼Œçªæ˜¾äº†å†°èåŒ–å¯¹å²©çŸ³å†°å´©è¿åŠ¨çŠ¶æ€çš„å…³é”®ä½œç”¨ï¼Œæ­ç¤ºäº†å¤æ‚çš„çƒ­æœºæ¢°è¿‡ç¨‹æ§åˆ¶æœºåˆ¶ã€‚è¿™ä¸ºå·¥ç¨‹å¸ˆå’Œä»ä¸šè€…è§£å†³ä¸å²©çŸ³å†°å´©ç›¸å…³çš„é—®é¢˜æä¾›äº†å®ç”¨æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°å‹å¤šé˜¶æ®µçƒ­æœºæ¢°å²©çŸ³å†°å´©æ¨¡å‹ï¼Œæ¶‰åŠå²©çŸ³ã€å†°å’Œæµä½“çš„ç»¼åˆè€ƒé‡ã€‚</li>
<li>æ¨¡å‹åŒ…å«ä¸¥æ ¼çš„å†°èåŒ–é€Ÿç‡æ¨å¯¼ï¼Œä»¥åŠåŸºäºèåŒ–æ•ˆç‡çš„æµä½“äº§ç”Ÿç‡ã€‚</li>
<li>æ¨¡å‹è§£é‡Šäº†çƒ­å¯¹æµæ‰©æ•£ï¼Œæ¶µç›–å†°å´©ä¸­çš„çƒ­äº¤æ¢ã€åº•éƒ¨çƒ­ä¼ å¯¼ç­‰å¤æ‚è¿‡ç¨‹ã€‚</li>
<li>æ¨¡å‹é¦–æ¬¡ä¸ºå²©çŸ³å†°å´©æä¾›äº†å®Œæ•´çš„åŠ¨æ€è§£å†³æ–¹æ¡ˆï¼Œè€ƒè™‘æ¸©åº¦å˜åŒ–å’Œå†°èåŒ–ã€‚</li>
<li>ç•Œé¢è´¨é‡å’ŒåŠ¨é‡äº¤æ¢ä»¥åŠå¤¹å¸¦å¼•èµ·çš„è´¨é‡åŠ¨é‡äº§ç”Ÿè¢«çº³å…¥æ¨¡å‹çš„è€ƒè™‘èŒƒç•´ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹ŸChamoliäº‹ä»¶ï¼Œå±•ç¤ºäº†æ¨¡å‹åœ¨è§£å†³å®é™…é—®é¢˜ä¸­çš„åº”ç”¨æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.06130">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-99dbc2c9cdaaa4a3f54a152f75b1ea84" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/Text-to-Motion/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/Text-to-Motion/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Text-to-Motion/">
                                    <span class="chip bg-color">Text-to-Motion</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8e3adb44bf8ed190c0ffe932bf3bbb6d" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  Reinforcing Trustworthiness in Multimodal Emotional Support Systems
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c5ed64d6b004d1e6854d030fe95a638b" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  Persona-Aware Alignment Framework for Personalized Dialogue Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32714.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
