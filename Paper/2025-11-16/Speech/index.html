<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  Towards Leveraging Sequential Structure in Animal Vocalizations">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-94f420ebbca02daacc80ff228a134f9e')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-16-æ›´æ–°"><a href="#2025-11-16-æ›´æ–°" class="headerlink" title="2025-11-16 æ›´æ–°"></a>2025-11-16 æ›´æ–°</h1><h2 id="Towards-Leveraging-Sequential-Structure-in-Animal-Vocalizations"><a href="#Towards-Leveraging-Sequential-Structure-in-Animal-Vocalizations" class="headerlink" title="Towards Leveraging Sequential Structure in Animal Vocalizations"></a>Towards Leveraging Sequential Structure in Animal Vocalizations</h2><p><strong>Authors:Eklavya Sarkar, Mathew Magimai. -Doss</strong></p>
<p>Animal vocalizations contain sequential structures that carry important communicative information, yet most computational bioacoustics studies average the extracted frame-level features across the temporal axis, discarding the order of the sub-units within a vocalization. This paper investigates whether discrete acoustic token sequences, derived through vector quantization and gumbel-softmax vector quantization of extracted self-supervised speech model representations can effectively capture and leverage temporal information. To that end, pairwise distance analysis of token sequences generated from HuBERT embeddings shows that they can discriminate call-types and callers across four bioacoustics datasets. Sequence classification experiments using $k$-Nearest Neighbour with Levenshtein distance show that the vector-quantized token sequences yield reasonable call-type and caller classification performances, and hold promise as alternative feature representations towards leveraging sequential information in animal vocalizations.</p>
<blockquote>
<p>åŠ¨ç‰©å‘å‡ºçš„å£°éŸ³åŒ…å«è¿ç»­çš„æºå¸¦é‡è¦é€šä¿¡ä¿¡æ¯çš„ç»“æ„ï¼Œç„¶è€Œå¤§å¤šæ•°è®¡ç®—ç”Ÿç‰©å£°å­¦ç ”ç©¶çš„å¹³å‡æå–å¸§çº§åˆ«çš„ç‰¹å¾ä¼šæ¨ªè·¨æ—¶é—´è½´ï¼Œä»è€Œå¿½ç•¥äº†åœ¨æŸä¸€å‘å£°å†…éƒ¨å­å•å…ƒçš„æ¬¡åºã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç©¶é€šè¿‡çŸ¢é‡é‡åŒ–ä»æå–çš„åŸºäºè‡ªç›‘ç£çš„è¯­éŸ³æ¨¡å‹è¡¨ç¤ºä¸­çš„åºåˆ—ä¸­æå–ç¦»æ•£å£°å­¦æ ‡è®°åºåˆ—æ˜¯å¦æœ‰æ•ˆåœ°æ•æ‰å’Œåˆ©ç”¨æ—¶åºä¿¡æ¯ï¼Œå¹¶ç”¨ä¸€ç³»åˆ—å·§å¦™çš„æ¿€æ´»å™¨æå–è®­ç»ƒå­¦ä¹ å‡ºæ¥æå–å¾—åˆ°çš„è¯­éŸ³å•å…ƒèƒ½å¦è¡¨è¾¾ç±»ä¼¼çš„é¢„æµ‹å‡½æ•°ç›®æ ‡æ¥è¿›è¡ŒåŒºåˆ†åˆ†ææ—¶å®ƒä»¬èƒ½è¯†åˆ«å››ç§ç”Ÿç‰©å£°å­¦æ•°æ®é›†ä¸Šçš„å‘¼å«ç±»å‹å’Œå‘¼å«è€…ã€‚åˆ©ç”¨HuBERTåµŒå…¥ç”Ÿæˆçš„æ ‡è®°åºåˆ—çš„æˆå¯¹è·ç¦»åˆ†æè¡¨æ˜å®ƒä»¬èƒ½å¤ŸåŒºåˆ†å‘¼å«ç±»å‹å’Œå‘¼å«è€…ã€‚ä½¿ç”¨Levenshteinè·ç¦»çš„kæœ€è¿‘é‚»åºåˆ—åˆ†ç±»å®éªŒæ˜¾ç¤ºï¼ŒçŸ¢é‡é‡åŒ–çš„æ ‡è®°åºåˆ—å¯ä»¥å®ç°åˆç†çš„å‘¼å«ç±»å‹å’Œå‘¼å«è€…åˆ†ç±»æ€§èƒ½ï¼Œå¹¶åœ¨åŠ¨ç‰©å«å£°ä¸­çš„é¡ºåºä¿¡æ¯ç‰¹å¾è¡¨è¾¾ä¸Šæœ‰è‰¯å¥½çš„æ½œåŠ›æˆä¸ºåŠ¨ç‰©å‘å£°åˆ†æçš„å¦ä¸€ç§ç‰¹å¾è¡¨ç¤ºæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10190v1">PDF</a> Accepted at NeurIPS workshop (AI for Non-Human Animal Communication)</p>
<p><strong>Summary</strong>ï¼š<br>åŠ¨ç‰©å‘å‡ºçš„å£°éŸ³åŒ…å«é‡è¦çš„æ²Ÿé€šä¿¡æ¯ï¼Œå…¶å£°éŸ³åºåˆ—ç»“æ„æ˜¯å…³é”®éƒ¨åˆ†ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è®¡ç®—ç”Ÿç‰©å£°å­¦çš„ç ”ç©¶éƒ½æ˜¯å¹³å‡æå–çš„å¸§çº§ç‰¹å¾ï¼Œå¿½ç•¥äº†å£°éŸ³å­å•å…ƒçš„æ—¶é—´é¡ºåºã€‚æœ¬æ–‡æ¢è®¨äº†é€šè¿‡å‘é‡é‡åŒ–å’Œgumbel-softmaxå‘é‡é‡åŒ–æå–çš„è‡ªç›‘ç£è¯­éŸ³æ¨¡å‹è¡¨ç¤ºä¸­çš„ç¦»æ•£å£°å­¦ä»¤ç‰Œåºåˆ—æ˜¯å¦èƒ½æœ‰æ•ˆæ•è·å’Œåˆ©ç”¨æ—¶é—´ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œä½¿ç”¨HuBERTåµŒå…¥ç”Ÿæˆçš„ä»¤ç‰Œåºåˆ—çš„æˆå¯¹è·ç¦»åˆ†æè¡¨æ˜ï¼Œå®ƒä»¬å¯ä»¥åœ¨å››ä¸ªç”Ÿç‰©å£°å­¦æ•°æ®é›†ä¸­åŒºåˆ†å‘¼å«ç±»å‹å’Œå‘¼å«è€…ã€‚ä½¿ç”¨Levenshteinè·ç¦»çš„kè¿‘é‚»åˆ†ç±»å®éªŒè¡¨æ˜ï¼Œå‘é‡é‡åŒ–çš„ä»¤ç‰Œåºåˆ—åœ¨å‘¼å«ç±»å‹å’Œå‘¼å«è€…åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œå¹¶æœ‰æœ›ä½œä¸ºåˆ©ç”¨åŠ¨ç‰©å«å£°ä¸­çš„åºåˆ—ä¿¡æ¯çš„æ›¿ä»£ç‰¹å¾è¡¨ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>åŠ¨ç‰©å‘å‡ºçš„å£°éŸ³åŒ…å«é‡è¦çš„æ²Ÿé€šä¿¡æ¯ï¼Œä¸”å…¶é¡ºåºç»“æ„æ˜¯å…³é”®éƒ¨åˆ†ã€‚</li>
<li>ç°æœ‰ç ”ç©¶åœ¨ç”Ÿç‰©å£°å­¦é¢†åŸŸå¸¸å¸¸å¿½ç•¥å£°éŸ³å­å•å…ƒçš„æ—¶é—´é¡ºåºï¼Œä»…å¯¹å¸§çº§ç‰¹å¾è¿›è¡Œå¹³å‡å¤„ç†ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡å‘é‡é‡åŒ–å’Œgumbel-softmaxå‘é‡é‡åŒ–æ¢ç©¶äº†ç¦»æ•£å£°å­¦ä»¤ç‰Œåºåˆ—çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä»¤ç‰Œåºåˆ—èƒ½å¤ŸåŒºåˆ†ä¸åŒçš„å‘¼å«ç±»å‹å’Œå‘¼å«è€…ï¼Œè¿™åœ¨å››ä¸ªç”Ÿç‰©å£°å­¦æ•°æ®é›†ä¸­å¾—åˆ°äº†éªŒè¯ã€‚</li>
<li>ä½¿ç”¨Levenshteinè·ç¦»çš„kè¿‘é‚»åˆ†ç±»å®éªŒè¡¨æ˜ä»¤ç‰Œåºåˆ—åœ¨å‘¼å«åˆ†ç±»æ–¹é¢çš„è‰¯å¥½æ€§èƒ½ã€‚</li>
<li>å‘é‡é‡åŒ–çš„ä»¤ç‰Œåºåˆ—ä½œä¸ºä¸€ç§æ›¿ä»£ç‰¹å¾è¡¨ç¤ºæ–¹æ³•ï¼Œåœ¨åˆ©ç”¨åŠ¨ç‰©å«å£°ä¸­çš„åºåˆ—ä¿¡æ¯æ–¹é¢å±•ç°å‡ºæ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7deb72de2f6a1f9dab17005aae76befb" align="middle">
<img src="https://picx.zhimg.com/v2-dbf130b4d232c85ddc9cb0d2c4b90c1c" align="middle">
<img src="https://picx.zhimg.com/v2-fc0ad57e3b50e1a726f5c9450a510219" align="middle">
<img src="https://picx.zhimg.com/v2-de6afc20dc47cdfbf0358bf2efcf4e22" align="middle">
<img src="https://picx.zhimg.com/v2-5d3c968f55109bdfde22d301bd318283" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FabasedVC-Enhancing-Voice-Conversion-with-Text-Modality-Fusion-and-Phoneme-Level-SSL-Features"><a href="#FabasedVC-Enhancing-Voice-Conversion-with-Text-Modality-Fusion-and-Phoneme-Level-SSL-Features" class="headerlink" title="FabasedVC: Enhancing Voice Conversion with Text Modality Fusion and Phoneme-Level SSL Features"></a>FabasedVC: Enhancing Voice Conversion with Text Modality Fusion and Phoneme-Level SSL Features</h2><p><strong>Authors:Wenyu Wang, Zhetao Hu, Yiquan Zhou, Jiacheng Xu, Zhiyu Wu, Chen Li, Shihao Li</strong></p>
<p>In voice conversion (VC), it is crucial to preserve complete semantic information while accurately modeling the target speakerâ€™s timbre and prosody. This paper proposes FabasedVC to achieve VC with enhanced similarity in timbre, prosody, and duration to the target speaker, as well as improved content integrity. It is an end-to-end VITS-based VC system that integrates relevant textual modality information, phoneme-level self-supervised learning (SSL) features, and a duration predictor. Specifically, we employ a text feature encoder to encode attributes such as text, phonemes, tones and BERT features. We then process the frame-level SSL features into phoneme-level features using two methods: average pooling and attention mechanism based on each phonemeâ€™s duration. Moreover, a duration predictor is incorporated to better align the speech rate and prosody of the target speaker. Experimental results demonstrate that our method outperforms competing systems in terms of naturalness, similarity, and content integrity.</p>
<blockquote>
<p>åœ¨è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰ä¸­ï¼Œä¿ç•™å®Œæ•´çš„è¯­ä¹‰ä¿¡æ¯ï¼ŒåŒæ—¶å‡†ç¡®åœ°å¯¹ç›®æ ‡è¯´è¯è€…çš„éŸ³è‰²å’Œè¯­è°ƒè¿›è¡Œå»ºæ¨¡æ˜¯è‡³å…³é‡è¦çš„ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºç‰¹å¾çš„FabasedVCï¼Œå®ç°äº†è¯­éŸ³è½¬æ¢çš„ç›®æ ‡ï¼Œå¹¶æé«˜äº†ä¸ç›®æ ‡è¯´è¯è€…åœ¨éŸ³è‰²ã€è¯­è°ƒå’ŒæŒç»­æ—¶é—´æ–¹é¢çš„ç›¸ä¼¼æ€§ï¼Œä»¥åŠå†…å®¹çš„å®Œæ•´æ€§ã€‚è¿™æ˜¯ä¸€ä¸ªåŸºäºVITSç«¯åˆ°ç«¯çš„è¯­éŸ³è½¬æ¢ç³»ç»Ÿï¼Œå®ƒèåˆäº†ç›¸å…³çš„æ–‡æœ¬æ¨¡å¼ä¿¡æ¯ã€éŸ³ç´ çº§åˆ«çš„è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ç‰¹å¾ä»¥åŠæŒç»­æ—¶é—´é¢„æµ‹å™¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨æ–‡æœ¬ç‰¹å¾ç¼–ç å™¨å¯¹æ–‡æœ¬ã€éŸ³ç´ ã€éŸ³è°ƒä»¥åŠBERTç‰¹å¾ç­‰å±æ€§è¿›è¡Œç¼–ç ã€‚ç„¶åæˆ‘ä»¬é€šè¿‡ä¸¤ç§æ–¹æ³•å°†å¸§çº§åˆ«çš„SSLç‰¹å¾å¤„ç†ä¸ºéŸ³ç´ çº§åˆ«çš„ç‰¹å¾ï¼šä¸€ç§æ˜¯å¹³å‡æ± åŒ–æ–¹æ³•ï¼Œå¦ä¸€ç§æ˜¯åŸºäºæ¯ä¸ªéŸ³ç´ çš„æŒç»­æ—¶é—´çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†æŒç»­æ—¶é—´é¢„æµ‹å™¨ç›¸ç»“åˆï¼Œä»¥æ›´å¥½åœ°åŒ¹é…ç›®æ ‡è¯´è¯è€…çš„è¯­é€Ÿå’Œè¯­è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è‡ªç„¶åº¦ã€ç›¸ä¼¼åº¦å’Œå†…å®¹å®Œæ•´æ€§æ–¹é¢ä¼˜äºå…¶ä»–ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10112v1">PDF</a> Accepted by ACMMM-Asia 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºFaboçš„è¯­éŸ³è½¬æ¢ç³»ç»Ÿï¼ˆFabasedVCï¼‰ï¼Œè¯¥ç³»ç»Ÿèƒ½ä¿ç•™å®Œæ•´çš„è¯­ä¹‰ä¿¡æ¯ï¼ŒåŒæ—¶å‡†ç¡®æ¨¡æ‹Ÿç›®æ ‡è¯´è¯è€…çš„éŸ³è‰²å’Œè¯­è°ƒã€‚è¯¥ç³»ç»Ÿæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„VITSè¯­éŸ³è½¬æ¢ç³»ç»Ÿï¼Œèåˆäº†æ–‡æœ¬æ¨¡æ€ä¿¡æ¯ã€åŸºäºéŸ³ç´ çº§åˆ«çš„è‡ªç›‘ç£å­¦ä¹ ç‰¹å¾ä»¥åŠæ—¶é•¿é¢„æµ‹å™¨ã€‚é€šè¿‡é‡‡ç”¨æ–‡æœ¬ç‰¹å¾ç¼–ç å™¨ï¼Œç³»ç»Ÿèƒ½å¤„ç†æ–‡æœ¬ã€éŸ³ç´ ã€è¯­è°ƒç­‰å±æ€§ï¼Œå¹¶é€šè¿‡å¹³å‡æ± åŒ–å’ŒåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ—¶é•¿é¢„æµ‹æ–¹æ³•å°†å¸§çº§åˆ«çš„è‡ªç›‘ç£ç‰¹å¾è½¬åŒ–ä¸ºéŸ³ç´ çº§åˆ«ç‰¹å¾ã€‚è¯¥ç³»ç»Ÿèƒ½æœ‰æ•ˆæå‡ç›®æ ‡è¯´è¯è€…çš„éŸ³è‰²ã€è¯­è°ƒåŠå†…å®¹çš„å®Œæ•´æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‡ªç„¶åº¦ã€ç›¸ä¼¼åº¦å’Œå†…å®¹å®Œæ•´æ€§æ–¹é¢ä¼˜äºå…¶ä»–ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FabasedVCæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„è¯­éŸ³è½¬æ¢ç³»ç»Ÿï¼Œæ—¨åœ¨ä¿ç•™è¯­ä¹‰ä¿¡æ¯çš„åŒæ—¶æ¨¡æ‹Ÿç›®æ ‡è¯´è¯è€…çš„éŸ³è‰²å’Œè¯­è°ƒã€‚</li>
<li>ç³»ç»Ÿèåˆäº†æ–‡æœ¬æ¨¡æ€ä¿¡æ¯ã€éŸ³ç´ çº§åˆ«çš„è‡ªç›‘ç£å­¦ä¹ ç‰¹å¾å’Œæ—¶é•¿é¢„æµ‹å™¨ã€‚</li>
<li>é‡‡ç”¨æ–‡æœ¬ç‰¹å¾ç¼–ç å™¨å¤„ç†æ–‡æœ¬ã€éŸ³ç´ ã€è¯­è°ƒç­‰å±æ€§ã€‚</li>
<li>é€šè¿‡å¹³å‡æ± åŒ–å’ŒåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ—¶é•¿é¢„æµ‹æ–¹æ³•å°†å¸§çº§åˆ«çš„è‡ªç›‘ç£ç‰¹å¾è½¬åŒ–ä¸ºéŸ³ç´ çº§åˆ«ç‰¹å¾ã€‚</li>
<li>ç³»ç»Ÿæå‡äº†ç›®æ ‡è¯´è¯è€…çš„éŸ³è‰²ã€è¯­è°ƒåŠæ—¶é•¿é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒFabasedVCåœ¨è‡ªç„¶åº¦ã€ç›¸ä¼¼åº¦å’Œå†…å®¹å®Œæ•´æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10112">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c2de3d24aef4720261c7b71542544755" align="middle">
<img src="https://picx.zhimg.com/v2-b77858c17ddefa043e67aa70af0323ff" align="middle">
<img src="https://picx.zhimg.com/v2-3aeb3fd1d8da0ac8e817dbf06b99edb9" align="middle">
<img src="https://picx.zhimg.com/v2-e8f3ee5b07a0395f1e77fffcfb36c8f4" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ELYADATA-LIA-at-NADI-2025-ASR-and-ADI-Subtasks"><a href="#ELYADATA-LIA-at-NADI-2025-ASR-and-ADI-Subtasks" class="headerlink" title="ELYADATA &amp; LIA at NADI 2025: ASR and ADI Subtasks"></a>ELYADATA &amp; LIA at NADI 2025: ASR and ADI Subtasks</h2><p><strong>Authors:Haroun Elleuch, Youssef Saidi, Salima Mdhaffar, Yannick EstÃ¨ve, Fethi Bougares</strong></p>
<p>This paper describes Elyadata &amp; LIAâ€™s joint submission to the NADI multi-dialectal Arabic Speech Processing 2025. We participated in the Spoken Arabic Dialect Identification (ADI) and multi-dialectal Arabic ASR subtasks. Our submission ranked first for the ADI subtask and second for the multi-dialectal Arabic ASR subtask among all participants. Our ADI system is a fine-tuned Whisper-large-v3 encoder with data augmentation. This system obtained the highest ADI accuracy score of \textbf{79.83%} on the official test set. For multi-dialectal Arabic ASR, we fine-tuned SeamlessM4T-v2 Large (Egyptian variant) separately for each of the eight considered dialects. Overall, we obtained an average WER and CER of \textbf{38.54%} and \textbf{14.53%}, respectively, on the test set. Our results demonstrate the effectiveness of large pre-trained speech models with targeted fine-tuning for Arabic speech processing.</p>
<blockquote>
<p>æœ¬æ–‡æè¿°äº†Elyadataå’ŒLIAå¯¹NADIå¤šæ–¹è¨€é˜¿æ‹‰ä¼¯è¯­è¯­éŸ³å¤„ç†2025çš„è”åˆæäº¤ã€‚æˆ‘ä»¬å‚åŠ äº†å£å¤´é˜¿æ‹‰ä¼¯è¯­æ–¹è¨€è¯†åˆ«ï¼ˆADIï¼‰å’Œå¤šæ–¹è¨€é˜¿æ‹‰ä¼¯è¯­ASRå­ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æäº¤åœ¨ADIå­ä»»åŠ¡ä¸­æ’åç¬¬ä¸€ï¼Œåœ¨å¤šæ–¹è¨€é˜¿æ‹‰ä¼¯è¯­ASRå­ä»»åŠ¡ä¸­æ’åç¬¬äºŒã€‚æˆ‘ä»¬çš„ADIç³»ç»Ÿæ˜¯å¯¹Whisper-large-v3ç¼–ç å™¨çš„å¾®è°ƒï¼Œå¹¶è¿›è¡Œäº†æ•°æ®å¢å¼ºã€‚è¯¥ç³»ç»Ÿçš„ADIå‡†ç¡®ç‡å¾—åˆ†æœ€é«˜ï¼Œä¸º79.83%ï¼Œåœ¨å®˜æ–¹æµ‹è¯•é›†ä¸Šã€‚å¯¹äºå¤šæ–¹è¨€é˜¿æ‹‰ä¼¯è¯­ASRï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªè€ƒè™‘çš„å…«ç§æ–¹è¨€åˆ†åˆ«å¾®è°ƒäº†æ— ç¼M4T-v2 Largeï¼ˆåŸƒåŠå˜ç§ï¼‰ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬åœ¨æµ‹è¯•é›†ä¸Šè·å¾—äº†å¹³å‡çš„WERå’ŒCERåˆ†åˆ«ä¸º38.54%å’Œ14.53%ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†å¤§å‹é¢„è®­ç»ƒè¯­éŸ³æ¨¡å‹é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒå¯¹äºé˜¿æ‹‰ä¼¯è¯­è¯­éŸ³å¤„ç†çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10090v1">PDF</a> Published in Proceedings of the ArabicNLP 2025 Workshop (co-located with EMNLP 2025), Association for Computational Linguistics, 2025</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡ä»‹ç»äº†Elyadataä¸LIAåœ¨NADIå¤šæ–¹è¨€é˜¿æ‹‰ä¼¯è¯­è¯­éŸ³å¤„ç†2025é¡¹ç›®ä¸­çš„è”åˆæäº¤æˆæœã€‚åœ¨å£å¤´é˜¿æ‹‰ä¼¯è¯­æ–¹è¨€è¯†åˆ«ï¼ˆADIï¼‰å’Œå¤šæ–¹è¨€é˜¿æ‹‰ä¼¯è¯­è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„å­ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æäº¤ä½œå“åœ¨æ‰€æœ‰å‚ä¸è€…ä¸­åˆ†åˆ«æ’åç¬¬ä¸€å’Œç¬¬äºŒã€‚æˆ‘ä»¬çš„ADIç³»ç»Ÿä½¿ç”¨å¾®è°ƒè¿‡çš„Whisper-large-v3ç¼–ç å™¨ä¸æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œåœ¨å®˜æ–¹æµ‹è¯•é›†ä¸Šè·å¾—æœ€é«˜çš„ADIå‡†ç¡®ç‡79.83%ã€‚åœ¨å¤šæ–¹è¨€é˜¿æ‹‰ä¼¯è¯­çš„ASRä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹æ¯ç§è€ƒè™‘çš„å…«ç§æ–¹è¨€åˆ†åˆ«å¾®è°ƒäº†SeamlessM4T-v2 Largeï¼ˆåŸƒåŠå˜ä½“ï¼‰ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬åœ¨æµ‹è¯•é›†ä¸Šè·å¾—äº†å¹³å‡çš„å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’Œå­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰åˆ†åˆ«ä¸º38.54%å’Œ14.53%ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­è¯­éŸ³å¤„ç†çš„å¤§å‹é¢„è®­ç»ƒè¯­éŸ³æ¨¡å‹é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒæ˜¯æœ‰æ•ˆçš„ã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ol>
<li>Elyadataå’ŒLIAåœ¨å¤šæ–¹è¨€é˜¿æ‹‰ä¼¯è¯­è¯­éŸ³å¤„ç†é¢†åŸŸçš„æ°å‡ºè´¡çŒ®ã€‚</li>
<li>åœ¨å£å¤´é˜¿æ‹‰ä¼¯è¯­æ–¹è¨€è¯†åˆ«ï¼ˆADIï¼‰ä»»åŠ¡ä¸­å–å¾—ç¬¬ä¸€åçš„å¥½æˆç»©ã€‚</li>
<li>å¤šæ–¹è¨€é˜¿æ‹‰ä¼¯è¯­çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿé€šè¿‡é’ˆå¯¹æ¯ç§æ–¹è¨€çš„å¾®è°ƒå–å¾—è‰¯å¥½æ€§èƒ½ã€‚</li>
<li>ADIç³»ç»Ÿçš„æœ€é«˜å‡†ç¡®ç‡ä¸º79.83%ï¼Œåº”ç”¨äº†æ•°æ®å¢å¼ºæŠ€æœ¯å’ŒWhisper-large-v3ç¼–ç å™¨ã€‚</li>
<li>ASRç³»ç»Ÿåœ¨æµ‹è¯•é›†ä¸Šçš„å¹³å‡å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’Œå­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰åˆ†åˆ«ä¸º38.54%å’Œ14.53%ã€‚</li>
<li>ç ”ç©¶ç»“æœè¯æ˜äº†å¤§å‹é¢„è®­ç»ƒè¯­éŸ³æ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯è¯­è¯­éŸ³å¤„ç†ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10090">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d0e6940ada7305d00d399c0fea81dc7b" align="middle">
<img src="https://picx.zhimg.com/v2-18d885f6fb2bb147b3ad95d1a36031f9" align="middle">
<img src="https://picx.zhimg.com/v2-ef443eb366d8b737edd153547b335461" align="middle">
<img src="https://picx.zhimg.com/v2-778e8a718562257109e1cd9a4d19f5f4" align="middle">
<img src="https://picx.zhimg.com/v2-76a6106e3f2d0019458f9eec858c59ac" align="middle">
<img src="https://picx.zhimg.com/v2-26b304661764659979fe1233ecedff7a" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Time-Layer-Adaptive-Alignment-for-Speaker-Similarity-in-Flow-Matching-Based-Zero-Shot-TTS"><a href="#Time-Layer-Adaptive-Alignment-for-Speaker-Similarity-in-Flow-Matching-Based-Zero-Shot-TTS" class="headerlink" title="Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS"></a>Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS</h2><p><strong>Authors:Haoyu Li, Mingyang Han, Yu Xi, Dongxiao Wang, Hankun Wang, Haoxiang Shi, Boyu Li, Jun Song, Bo Zheng, Shuai Wang</strong></p>
<p>Flow-Matching (FM)-based zero-shot text-to-speech (TTS) systems exhibit high-quality speech synthesis and robust generalization capabilities. However, the speaker representation ability of such systems remains underexplored, primarily due to the lack of explicit speaker-specific supervision in the FM framework. To this end, we conduct an empirical analysis of speaker information distribution and reveal its non-uniform allocation across time steps and network layers, underscoring the need for adaptive speaker alignment. Accordingly, we propose Time-Layer Adaptive Speaker Alignment (TLA-SA), a loss that enhances speaker consistency by jointly leveraging temporal and hierarchical variations in speaker information. Experimental results show that TLA-SA significantly improves speaker similarity compared to baseline systems on both research- and industrial-scale datasets and generalizes effectively across diverse model architectures, including decoder-only language models (LM) and FM-based TTS systems free of LM.</p>
<blockquote>
<p>åŸºäºæµåŒ¹é…ï¼ˆFMï¼‰çš„é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿå±•ç°å‡ºé«˜è´¨é‡çš„è¯­éŸ³åˆæˆå’Œç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ­¤ç±»ç³»ç»Ÿçš„è¯´è¯äººè¡¨å¾èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºFMæ¡†æ¶ä¸­ç¼ºä¹æ˜ç¡®çš„è¯´è¯äººç‰¹å®šç›‘ç£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¹è¯´è¯äººä¿¡æ¯åˆ†å¸ƒè¿›è¡Œäº†å®è¯åˆ†æï¼Œå¹¶æ­ç¤ºäº†å…¶åœ¨æ—¶é—´æ­¥å’Œç½‘ç»œå±‚ä¹‹é—´çš„éå‡åŒ€åˆ†é…ï¼Œè¿™å¼ºè°ƒäº†è‡ªé€‚åº”è¯´è¯äººå¯¹é½çš„éœ€æ±‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ—¶é—´å±‚è‡ªé€‚åº”è¯´è¯äººå¯¹é½ï¼ˆTLA-SAï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æŸå¤±å‡½æ•°ï¼Œé€šè¿‡è”åˆåˆ©ç”¨è¯´è¯äººä¿¡æ¯çš„æ—¶æ€å’Œå±‚æ¬¡å˜åŒ–ï¼Œæé«˜è¯´è¯äººä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºå‡†ç³»ç»Ÿç›¸æ¯”ï¼ŒTLA-SAåœ¨ç ”ç©¶å’Œå·¥ä¸šè§„æ¨¡çš„æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†è¯´è¯äººç›¸ä¼¼æ€§ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„æ¨¡å‹æ¶æ„ä¸­éƒ½èƒ½æœ‰æ•ˆæ³›åŒ–ï¼ŒåŒ…æ‹¬ä»…è§£ç å™¨è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰å’ŒåŸºäºFMçš„TTSç³»ç»Ÿï¼ˆæ— éœ€LMï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09995v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>æ€»ç»“</strong></p>
<p>åŸºäºæµåŒ¹é…ï¼ˆFMï¼‰çš„é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿå±•ç°å‡ºé«˜è´¨é‡çš„è¯­éŸ³åˆæˆå’Œç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚ä½†è¯¥ç³»ç»Ÿåœ¨å‘è¨€äººè¡¨ç¤ºèƒ½åŠ›æ–¹é¢ä»å­˜åœ¨ä¸è¶³ï¼Œä¸»è¦ç”±äºFMæ¡†æ¶ä¸­ç¼ºä¹æ˜ç¡®çš„å‘è¨€äººç‰¹å®šç›‘ç£ã€‚æœ¬æ–‡é€šè¿‡å®è¯åˆ†ææ­ç¤ºäº†å‘è¨€äººä¿¡æ¯åˆ†å¸ƒçš„éå‡åŒ€æ€§ï¼Œå¹¶æŒ‡å‡ºéœ€è¦è‡ªé€‚åº”çš„å‘è¨€äººå¯¹é½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ—¶é—´å±‚è‡ªé€‚åº”å‘è¨€äººå¯¹é½ï¼ˆTLA-SAï¼‰çš„æŸå¤±å‡½æ•°ï¼Œé€šè¿‡è”åˆåˆ©ç”¨å‘è¨€äººåœ¨æ—¶é—´å’Œå±‚æ¬¡ç»“æ„ä¸Šçš„å˜åŒ–ï¼Œæé«˜å‘è¨€äººçš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿ç³»ç»Ÿç›¸æ¯”ï¼ŒTLA-SAåœ¨ç ”ç©¶å’Œå·¥ä¸šè§„æ¨¡çš„æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜å‘è¨€äººç›¸ä¼¼æ€§ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„æ¨¡å‹æ¶æ„ä¸­è¡¨ç°æœ‰æ•ˆï¼ŒåŒ…æ‹¬æ— è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰çš„è§£ç å™¨å”¯ä¸€LMå’ŒåŸºäºFMçš„TTSç³»ç»Ÿã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>åŸºäºæµåŒ¹é…çš„é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ç³»ç»Ÿè™½å…·æœ‰é«˜è´¨é‡è¯­éŸ³åˆæˆå’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½†å‘è¨€äººè¡¨ç¤ºèƒ½åŠ›æœ‰å¾…æå‡ã€‚</li>
<li>å‘è¨€äººä¿¡æ¯åœ¨æ—¶é—´å’Œç½‘ç»œå±‚é¢ä¸Šçš„åˆ†å¸ƒæ˜¯éå‡åŒ€çš„ã€‚</li>
<li>éœ€è¦è‡ªé€‚åº”çš„å‘è¨€äººå¯¹é½æ¥å¢å¼ºå‘è¨€äººçš„ä¸€è‡´æ€§ã€‚</li>
<li>æå‡ºäº†æ—¶é—´å±‚è‡ªé€‚åº”å‘è¨€äººå¯¹é½ï¼ˆTLA-SAï¼‰çš„æŸå¤±å‡½æ•°ã€‚</li>
<li>TLA-SAé€šè¿‡è”åˆåˆ©ç”¨å‘è¨€äººåœ¨æ—¶é—´å’Œå±‚æ¬¡ç»“æ„ä¸Šçš„å˜åŒ–ï¼Œæœ‰æ•ˆæé«˜å‘è¨€äººä¸€è‡´æ€§ã€‚</li>
<li>åœ¨ç ”ç©¶å’Œå·¥ä¸šè§„æ¨¡æ•°æ®é›†ä¸Šï¼ŒTLA-SAæ˜¾è‘—æé«˜å‘è¨€äººç›¸ä¼¼æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96c72907b7fe2d0e0b0c4c91ff1f316f" align="middle">
<img src="https://picx.zhimg.com/v2-86b0a5cfa55d9cff9b610d5fa8a7358b" align="middle">
<img src="https://picx.zhimg.com/v2-e60de9e2763a108fd4ccb095d805266f" align="middle">
<img src="https://picx.zhimg.com/v2-bdbd3363956bc0cb456d9dc48f640a8b" align="middle">
<img src="https://picx.zhimg.com/v2-63f925c0a59753cc4792d7f163b7c478" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Omnilingual-ASR-Open-Source-Multilingual-Speech-Recognition-for-1600-Languages"><a href="#Omnilingual-ASR-Open-Source-Multilingual-Speech-Recognition-for-1600-Languages" class="headerlink" title="Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages"></a>Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages</h2><p><strong>Authors: Omnilingual ASR team, Gil Keren, Artyom Kozhevnikov, Yen Meng, Christophe Ropers, Matthew Setzler, Skyler Wang, Ife Adebara, Michael Auli, Can Balioglu, Kevin Chan, Chierh Cheng, Joe Chuang, Caley Droof, Mark Duppenthaler, Paul-Ambroise Duquenne, Alexander Erben, Cynthia Gao, Gabriel Mejia Gonzalez, Kehan Lyu, Sagar Miglani, Vineel Pratap, Kaushik Ram Sadagopan, Safiyyah Saleem, Arina Turkatenko, Albert Ventayol-Boada, Zheng-Xin Yong, Yu-An Chung, Jean Maillard, Rashel Moritz, Alexandre Mourachko, Mary Williamson, Shireen Yates</strong></p>
<p>Automatic speech recognition (ASR) has advanced in high-resource languages, but most of the worldâ€™s 7,000+ languages remain unsupported, leaving thousands of long-tail languages behind. Expanding ASR coverage has been costly and limited by architectures that restrict language support, making extension inaccessible to mostâ€“all while entangled with ethical concerns when pursued without community collaboration. To transcend these limitations, we introduce Omnilingual ASR, the first large-scale ASR system designed for extensibility. Omnilingual ASR enables communities to introduce unserved languages with only a handful of data samples. It scales self-supervised pre-training to 7B parameters to learn robust speech representations and introduces an encoder-decoder architecture designed for zero-shot generalization, leveraging a LLM-inspired decoder. This capability is grounded in a massive and diverse training corpus; by combining breadth of coverage with linguistic variety, the model learns representations robust enough to adapt to unseen languages. Incorporating public resources with community-sourced recordings gathered through compensated local partnerships, Omnilingual ASR expands coverage to over 1,600 languages, the largest such effort to dateâ€“including over 500 never before served by ASR. Automatic evaluations show substantial gains over prior systems, especially in low-resource conditions, and strong generalization. We release Omnilingual ASR as a family of models, from 300M variants for low-power devices to 7B for maximum accuracy. We reflect on the ethical considerations shaping this design and conclude by discussing its societal impact. In particular, we highlight how open-sourcing models and tools can lower barriers for researchers and communities, inviting new forms of participation. Open-source artifacts are available at <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/omnilingual-asr">https://github.com/facebookresearch/omnilingual-asr</a>.</p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨é«˜èµ„æºè¯­è¨€ä¸­å·²ç»å–å¾—äº†è¿›å±•ï¼Œä½†ä¸–ç•Œä¸Š7000å¤šç§è¯­è¨€ä¸­ï¼Œå¤§éƒ¨åˆ†ä»ä¸å—æ”¯æŒï¼Œç•™ä¸‹äº†è®¸å¤šé•¿å°¾è¯­è¨€æœªè¢«å¼€å‘ã€‚æ‰©å¤§ASRçš„è¦†ç›–èŒƒå›´æˆæœ¬é«˜æ˜‚ï¼Œä¸”å—åˆ°è¯­è¨€æ”¯æŒæ¶æ„çš„é™åˆ¶ï¼Œä½¿å¾—å¤§å¤šæ•°æ‰©å±•æ— æ³•å®æ–½ï¼Œå¹¶ä¸”åœ¨æ²¡æœ‰ç¤¾åŒºåˆä½œçš„æƒ…å†µä¸‹è¿½æ±‚è¿™ä¸€ç›®æ ‡ä¼šä¼´éšç€é“å¾·ä¸Šçš„æ‹…å¿§ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Omnilingual ASRï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸ºå¯æ‰©å±•æ€§è®¾è®¡çš„å¤§å‹ASRç³»ç»Ÿã€‚Omnilingual ASRè®©ç¤¾åŒºåªéœ€å°‘é‡æ•°æ®æ ·æœ¬å°±èƒ½å¼•å…¥æœªæœåŠ¡è¿‡çš„è¯­è¨€ã€‚å®ƒé€šè¿‡è‡ªæˆ‘ç›‘ç£çš„é¢„è®­ç»ƒæ‰©å±•åˆ°7äº¿ä¸ªå‚æ•°ï¼Œå­¦ä¹ é²æ£’çš„è¯­éŸ³è¡¨ç¤ºï¼Œå¹¶å¼•å…¥ä¸€ç§è®¾è®¡ç”¨äºé›¶å¯åŠ¨æ³›åŒ–çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œè¯¥æ¶æ„é‡‡ç”¨LLMé©±åŠ¨çš„è§£ç å™¨ã€‚è¿™ç§èƒ½åŠ›å»ºç«‹åœ¨åºå¤§è€Œå¤šæ ·çš„è®­ç»ƒè¯­æ–™åº“ä¸Šï¼›é€šè¿‡è¦†ç›–å¹¿æ³›ä¸è¯­è¨€å¤šæ ·æ€§ç›¸ç»“åˆï¼Œæ¨¡å‹å­¦ä¹ åˆ°çš„è¡¨ç¤ºè¶³ä»¥é€‚åº”æœªè§è¿‡çš„è¯­è¨€ã€‚Omnilingual ASRç»“åˆäº†å…¬å…±èµ„æºä¸é€šè¿‡æœ‰å¿æœ¬åœ°åˆä½œä¼™ä¼´å…³ç³»æ”¶é›†çš„ç¤¾åŒºæ¥æºå½•éŸ³ï¼Œå°†è¦†ç›–èŒƒå›´æ‰©å¤§åˆ°è¶…è¿‡1600ç§è¯­è¨€ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢è§„æ¨¡æœ€å¤§çš„æ­¤ç±»åŠªåŠ›â€”â€”å…¶ä¸­åŒ…æ‹¬è¶…è¿‡500ç§æ­¤å‰ä»æœªè¢«ASRæœåŠ¡è¿‡çš„è¯­è¨€ã€‚è‡ªåŠ¨è¯„ä¼°è¡¨æ˜ï¼Œä¸ä¹‹å‰çš„ç³»ç»Ÿç›¸æ¯”ï¼Œå®ƒå–å¾—äº†é‡å¤§è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºåŒ®ä¹çš„æ¡ä»¶ä¸‹ä»¥åŠå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬å‘å¸ƒäº†Omnilingual ASRç³»åˆ—æ¨¡å‹ï¼Œä»é€‚ç”¨äºä½åŠŸè€—è®¾å¤‡çš„3äº¿å˜ç§åˆ°ä¸ºæœ€å¤§å‡†ç¡®æ€§è€Œè®¾çš„7äº¿æ¨¡å‹ã€‚æˆ‘ä»¬åæ€äº†å¡‘é€ è¿™é¡¹è®¾è®¡çš„é“å¾·è€ƒé‡ï¼Œå¹¶é€šè¿‡è®¨è®ºå…¶ç¤¾ä¼šå½±å“æ¥æ€»ç»“ã€‚ç‰¹åˆ«æ˜¯æˆ‘ä»¬å¼ºè°ƒäº†å¼€æºæ¨¡å‹å’Œå·¥å…·å¦‚ä½•é™ä½ç ”ç©¶è€…å’Œç¤¾åŒºçš„å£å’ï¼Œé‚€è¯·æ–°çš„å‚ä¸å½¢å¼ã€‚å¼€æºæ–‡ç‰©å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/omnilingual-asr%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/facebookresearch/omnilingual-asrä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09690v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨é«˜èµ„æºè¯­è¨€ä¸­çš„å‘å±•ï¼Œå¹¶æŒ‡å‡ºä»æœ‰æ•°åƒç§é•¿å°¾è¯­è¨€æœªå¾—åˆ°æ”¯æŒã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶å¹¶æ‰©å¤§ASRçš„è¦†ç›–èŒƒå›´ï¼Œæå‡ºäº†ä¸€ç§åä¸ºOmnilingual ASRçš„æ–°å‹å¤§è§„æ¨¡ASRç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¹¶èƒ½ä½¿ç¤¾åŒºå¼•å…¥ä»…å°‘æ•°æ•°æ®æ ·æœ¬çš„æœªæœåŠ¡è¯­è¨€ã€‚Omnilingual ASRé€šè¿‡è‡ªæˆ‘ç›‘ç£çš„é¢„è®­ç»ƒã€ç¼–ç å™¨-è§£ç å™¨æ¶æ„ä»¥åŠå¤§è§„æ¨¡å¤šæ ·åŒ–çš„è®­ç»ƒè¯­æ–™åº“ï¼Œå®ç°äº†å¯¹æœªè§è¯­è¨€çš„é€‚åº”ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä¸ç¤¾åŒºåˆä½œæ”¶é›†èµ„æºï¼ŒOmnilingual ASRæˆåŠŸæ‰©å±•äº†å¯¹è¶…è¿‡1600ç§è¯­è¨€çš„æ”¯æŒï¼ŒåŒ…æ‹¬æ­¤å‰ä»æœªæœ‰è¿‡ASRæ”¯æŒçš„500å¤šç§è¯­è¨€ã€‚è‡ªåŠ¨è¯„ä¼°æ˜¾ç¤ºï¼Œä¸ä¹‹å‰çš„ç³»ç»Ÿç›¸æ¯”ï¼ŒOmnilingual ASRæœ‰æ˜¾è‘—çš„æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ã€‚åŒæ—¶ï¼Œæœ¬æ–‡ä¹Ÿè®¨è®ºäº†å…¶è®¾è®¡è¿‡ç¨‹ä¸­çš„ä¼¦ç†è€ƒé‡å’Œç¤¾ä¼šå½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨é«˜èµ„æºè¯­è¨€ä¸­å·²ç»å–å¾—è¿›å±•ï¼Œä½†è®¸å¤šé•¿å°¾è¯­è¨€ä»æœªå¾—åˆ°æ”¯æŒã€‚</li>
<li>Omnilingual ASRæ˜¯ç¬¬ä¸€ä¸ªä¸ºå¯æ‰©å±•æ€§è®¾è®¡çš„å¤§å‹ASRç³»ç»Ÿï¼Œå¯ä»¥è½»æ¾åœ°å¼•å…¥æœªæœåŠ¡çš„è¯­è¨€ï¼Œåªéœ€è¦å°‘é‡æ•°æ®æ ·æœ¬ã€‚</li>
<li>Omnilingual ASRé€šè¿‡è‡ªæˆ‘ç›‘ç£çš„é¢„è®­ç»ƒã€ç¼–ç å™¨-è§£ç å™¨æ¶æ„ä»¥åŠå¤§è§„æ¨¡å¤šæ ·åŒ–çš„è®­ç»ƒè¯­æ–™åº“ï¼Œå®ç°äº†å¯¹æœªè§è¯­è¨€çš„é€‚åº”ã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡ç»“åˆå¹¿æ³›çš„è¦†ç›–èŒƒå›´å’Œè¯­è¨€å¤šæ ·æ€§ï¼Œå­¦ä¹ äº†å¼ºå¤§çš„è¯­éŸ³è¡¨ç¤ºã€‚</li>
<li>Omnilingual ASRæˆåŠŸæ‰©å±•äº†å¯¹è¶…è¿‡1600ç§è¯­è¨€çš„æ”¯æŒï¼ŒåŒ…æ‹¬ä¹‹å‰æœªå—æ”¯æŒçš„500å¤šç§è¯­è¨€ã€‚</li>
<li>è‡ªåŠ¨è¯„ä¼°æ˜¾ç¤ºï¼ŒOmnilingual ASRåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œä¸ä¹‹å‰çš„ç³»ç»Ÿç›¸æ¯”æœ‰æ˜¾è‘—çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09690">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3f53339f60ee059907f3841d6fdabd72" align="middle">
<img src="https://picx.zhimg.com/v2-dccf22b967536f9979f4cb8f070c8ece" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="End-to-end-Contrastive-Language-Speech-Pretraining-Model-For-Long-form-Spoken-Question-Answering"><a href="#End-to-end-Contrastive-Language-Speech-Pretraining-Model-For-Long-form-Spoken-Question-Answering" class="headerlink" title="End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering"></a>End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering</h2><p><strong>Authors:Jiliang Hu, Zuchao Li, Baoyuan Qi, Liu Guoming, Ping Wang</strong></p>
<p>Significant progress has been made in spoken question answering (SQA) in recent years. However, many existing methods, including large audio language models, struggle with processing long audio. Follow the success of retrieval augmented generation, a speech-related retriever shows promising in help preprocessing long-form speech. But the performance of existing speech-related retrievers is lacking. To address this challenge, we propose CLSR, an end-to-end contrastive language-speech retriever that efficiently extracts question-relevant segments from long audio recordings for downstream SQA task. Unlike conventional speech-text contrastive models, CLSR incorporates an intermediate step that converts acoustic features into text-like representations prior to alignment, thereby more effectively bridging the gap between modalities. Experimental results across four cross-modal retrieval datasets demonstrate that CLSR surpasses both end-to-end speech related retrievers and pipeline approaches combining speech recognition with text retrieval, providing a robust foundation for advancing practical long-form SQA applications.</p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œè¯­éŸ³é—®ç­”ï¼ˆSQAï¼‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼ŒåŒ…æ‹¬å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨å†…çš„è®¸å¤šç°æœ‰æ–¹æ³•åœ¨å¤„ç†é•¿éŸ³é¢‘æ—¶éƒ½é¢ä¸´å›°éš¾ã€‚éšç€æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•çš„æˆåŠŸï¼Œè¯­éŸ³ç›¸å…³æ£€ç´¢å™¨åœ¨å¸®åŠ©é¢„å¤„ç†é•¿è¯­éŸ³æ–¹é¢æ˜¾ç¤ºå‡ºå¹¿é˜”å‰æ™¯ã€‚ä½†æ˜¯ç°æœ‰è¯­éŸ³ç›¸å…³æ£€ç´¢å™¨çš„æ€§èƒ½ä»ç„¶ä¸è¶³ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CLSRï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„å¯¹æ¯”è¯­è¨€è¯­éŸ³æ£€ç´¢å™¨ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°ä»é•¿éŸ³é¢‘ä¸­æå–ä¸é—®é¢˜ç›¸å…³çš„ç‰‡æ®µï¼Œç”¨äºä¸‹æ¸¸çš„è¯­éŸ³é—®ç­”ä»»åŠ¡ã€‚ä¸ä¼ ç»Ÿçš„è¯­éŸ³æ–‡æœ¬å¯¹æ¯”æ¨¡å‹ä¸åŒï¼ŒCLSRåŠ å…¥äº†ä¸€ä¸ªä¸­é—´æ­¥éª¤ï¼Œå°†å£°å­¦ç‰¹å¾è½¬æ¢ä¸ºæ–‡æœ¬è¡¨ç¤ºå½¢å¼ï¼Œç„¶åå†è¿›è¡Œå¯¹é½ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°å¼¥åˆäº†ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚åœ¨å››ä¸ªè·¨æ¨¡æ€æ£€ç´¢æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCLSRè¶…è¶Šäº†ç«¯åˆ°ç«¯çš„è¯­éŸ³ç›¸å…³æ£€ç´¢å™¨å’Œç»“åˆè¯­éŸ³è¯†åˆ«ä¸æ–‡æœ¬æ£€ç´¢çš„ç®¡é“æ–¹æ³•ï¼Œä¸ºæ¨è¿›å®é™…çš„é•¿è¯­éŸ³é—®ç­”åº”ç”¨æä¾›äº†åšå®çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09282v1">PDF</a> 12 pages, 7 figures, accepted by AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸé—®ç­”ç³»ç»Ÿï¼ˆSQAï¼‰åœ¨å£è¯­å›ç­”æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å¤„ç†é•¿éŸ³é¢‘ä»æ˜¯éš¾é¢˜ã€‚ç°æœ‰è¯­éŸ³ç›¸å…³æ£€ç´¢å™¨æ€§èƒ½æ¬ ä½³ã€‚ä¸ºæ­¤ï¼Œæå‡ºCLSRâ€”â€”ä¸€ç§ç«¯åˆ°ç«¯çš„å¯¹æ¯”è¯­è¨€è¯­éŸ³æ£€ç´¢å™¨ï¼Œå®ƒèƒ½ä»é•¿éŸ³é¢‘ä¸­æœ‰æ•ˆæå–ä¸é—®é¢˜ç›¸å…³çš„ç‰‡æ®µï¼Œç”¨äºä¸‹æ¸¸é—®ç­”ä»»åŠ¡ã€‚CLSRä¸åŒäºä¼ ç»Ÿè¯­éŸ³æ–‡æœ¬å¯¹æ¯”æ¨¡å‹ï¼Œå®ƒåœ¨æ¯”å¯¹ä¹‹å‰å¢åŠ äº†å°†å£°éŸ³ç‰¹å¾è½¬åŒ–ä¸ºæ–‡æœ¬è¡¨ç¤ºçš„æ­¥éª¤ï¼Œæ›´æœ‰æ•ˆåœ°ç¼©å°äº†ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚åœ¨å››ä¸ªè·¨æ¨¡æ€æ£€ç´¢æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCLSRçš„è¡¨ç°è¶…è¿‡äº†ç«¯åˆ°ç«¯çš„è¯­éŸ³ç›¸å…³æ£€ç´¢å™¨å’Œç»“åˆè¯­éŸ³è¯†åˆ«ä¸æ–‡æœ¬æ£€ç´¢çš„ç®¡é“æ–¹æ³•ï¼Œä¸ºæ¨è¿›å®é™…é•¿å½¢å¼é—®ç­”åº”ç”¨æä¾›äº†åšå®åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å£è¯­é—®ç­”ç³»ç»Ÿï¼ˆSQAï¼‰åœ¨å¤„ç†é•¿éŸ³é¢‘æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰è¯­éŸ³ç›¸å…³æ£€ç´¢å™¨æ€§èƒ½ä¸è¶³ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>CLSRæ˜¯ä¸€ç§æ–°å‹çš„ç«¯åˆ°ç«¯å¯¹æ¯”è¯­è¨€è¯­éŸ³æ£€ç´¢å™¨ï¼Œç”¨äºä»é•¿éŸ³é¢‘ä¸­æå–ä¸é—®é¢˜ç›¸å…³çš„ç‰‡æ®µã€‚</li>
<li>CLSRåœ¨å¯¹æ¯”ä¹‹å‰å°†å£°éŸ³ç‰¹å¾è½¬åŒ–ä¸ºæ–‡æœ¬è¡¨ç¤ºï¼Œç¼©å°äº†ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚</li>
<li>CLSRåœ¨å››ä¸ªè·¨æ¨¡æ€æ£€ç´¢æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡äº†å…¶ä»–æ–¹æ³•ã€‚</li>
<li>CLSRä¸ºæ¨è¿›å®é™…é•¿å½¢å¼é—®ç­”åº”ç”¨æä¾›äº†åšå®åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad4b1c2613e98f8458337cbfa47f19bc" align="middle">
<img src="https://picx.zhimg.com/v2-7d989da61fa409f90334e91fee93a6f1" align="middle">
<img src="https://picx.zhimg.com/v2-cd8a14e477b26795a28f97532f3b690a" align="middle">
<img src="https://picx.zhimg.com/v2-8897a9f614b7ff9244cb53a87b5e933b" align="middle">
<img src="https://picx.zhimg.com/v2-e9fa8a83d0ea3b3db435986ba2aa53f1" align="middle">
<img src="https://picx.zhimg.com/v2-e0007181cc5a2271ac70af7bc174b82d" align="middle">
<img src="https://picx.zhimg.com/v2-a6d7120f6ab9256b66e5c69799c7aab9" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Unifying-Model-and-Layer-Fusion-for-Speech-Foundation-Models"><a href="#Unifying-Model-and-Layer-Fusion-for-Speech-Foundation-Models" class="headerlink" title="Unifying Model and Layer Fusion for Speech Foundation Models"></a>Unifying Model and Layer Fusion for Speech Foundation Models</h2><p><strong>Authors:Yi-Jen Shih, David Harwath</strong></p>
<p>Speech Foundation Models have gained significant attention recently. Prior works have shown that the fusion of representations from multiple layers of the same model or the fusion of multiple models can improve performance on downstream tasks. We unify these two fusion strategies by proposing an interface module that enables fusion across multiple upstream speech models while integrating information across their layers. We conduct extensive experiments on different self-supervised and supervised models across various speech tasks, including ASR and paralinguistic analysis, and demonstrate that our method outperforms prior fusion approaches. We further analyze its scalability concerning model size and count, highlighting the importance of selecting appropriate upstream models. Our results show that the proposed interface provides an additional performance boost when given a suitable upstream model selection, making it a promising approach for utilizing Speech Foundation Models.</p>
<blockquote>
<p>è¯­éŸ³åŸºç¡€æ¨¡å‹è¿‘æœŸå—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚æ—©æœŸçš„ç ”ç©¶å·¥ä½œå·²ç»è¡¨æ˜ï¼ŒåŒä¸€æ¨¡å‹å¤šå±‚è¡¨ç¤ºçš„èåˆæˆ–å¤šæ¨¡å‹çš„èåˆå¯ä»¥æé«˜ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡æå‡ºä¸€ä¸ªæ¥å£æ¨¡å—ï¼Œç»Ÿä¸€äº†è¿™ä¸¤ç§èåˆç­–ç•¥ï¼Œè¯¥æ¥å£æ¨¡å—èƒ½å¤Ÿåœ¨å¤šä¸ªä¸Šæ¸¸è¯­éŸ³æ¨¡å‹ä¹‹é—´è¿›è¡Œèåˆï¼ŒåŒæ—¶æ•´åˆå…¶å„å±‚çš„ä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„è‡ªç›‘ç£æ¨¡å‹å’Œç›‘ç£æ¨¡å‹ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œæ¶µç›–äº†å„ç§è¯­éŸ³ä»»åŠ¡ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œå‰¯è¯­è¨€åˆ†æï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºä¹‹å‰çš„èåˆæ–¹æ³•ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†å…¶å…³äºæ¨¡å‹å¤§å°å’Œæ•°é‡çš„å¯æ‰©å±•æ€§ï¼Œå¼ºè°ƒäº†é€‰æ‹©é€‚å½“çš„ä¸Šæ¸¸æ¨¡å‹çš„é‡è¦æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå½“ç»™å®šåˆé€‚çš„ä¸Šæ¸¸æ¨¡å‹é€‰æ‹©æ—¶ï¼Œæ‰€æå‡ºçš„æ¥å£æä¾›äº†é¢å¤–çš„æ€§èƒ½æå‡ï¼Œä½¿å…¶æˆä¸ºåˆ©ç”¨è¯­éŸ³åŸºç¡€æ¨¡å‹çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08389v1">PDF</a> Accepted by IEEE ASRU 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªè·¨å¤šä¸ªä¸Šæ¸¸è¯­éŸ³æ¨¡å‹çš„èåˆæ¥å£æ¨¡å—ï¼Œè¯¥æ¨¡å—èƒ½å¤ŸèåˆåŒä¸€æ¨¡å‹çš„å¤šå±‚è¡¨ç¤ºæˆ–ä¸åŒæ¨¡å‹çš„ä¿¡æ¯ï¼Œè¿›è€Œæ”¹å–„ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œè¯¥æ¥å£åœ¨å¤šç§è‡ªç›‘ç£å’Œæœ‰ç›‘ç£çš„è¯­éŸ³æ¨¡å‹ä¸Šï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«å’Œå‰¯è¯­è¨€åˆ†æä»»åŠ¡ï¼Œå±•ç°å‡ºä¼˜äºå…ˆå‰èåˆæ–¹æ³•çš„æ•ˆæœã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜åˆ†æäº†å…¶å…³äºæ¨¡å‹å¤§å°å’Œæ•°é‡çš„å¯æ‰©å±•æ€§ï¼Œå¹¶å¼ºè°ƒäº†é€‰æ‹©é€‚å½“ä¸Šæ¸¸æ¨¡å‹çš„é‡è¦æ€§ã€‚æ­¤æ¥å£ä¸ºé€‰æ‹©åˆé€‚çš„ä¸Šæ¸¸æ¨¡å‹æä¾›äº†é¢å¤–çš„æ€§èƒ½æå‡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨åº”ç”¨è¯­éŸ³åŸºç¡€æ¨¡å‹ä¸­çš„å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ä¸ªèåˆæ¥å£æ¨¡å—ï¼Œå®ç°è·¨å¤šä¸ªä¸Šæ¸¸è¯­éŸ³æ¨¡å‹çš„èåˆã€‚</li>
<li>èåˆç­–ç•¥åŒ…æ‹¬åŒä¸€æ¨¡å‹çš„å¤šå±‚è¡¨ç¤ºèåˆå’Œå¤šä¸ªæ¨¡å‹çš„èåˆã€‚</li>
<li>é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œè¯¥æ¥å£åœ¨å¤šç§è¯­éŸ³ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>åˆ†æäº†æ‰€ææ–¹æ³•åœ¨ä¸åŒæ¨¡å‹å¤§å°å’Œæ•°é‡ä¸Šçš„å¯æ‰©å±•æ€§ã€‚</li>
<li>å¼ºè°ƒäº†é€‰æ‹©é€‚å½“ä¸Šæ¸¸æ¨¡å‹çš„é‡è¦æ€§ã€‚</li>
<li>è¯¥æ¥å£ä¸ºé€‰æ‹©åˆé€‚çš„ä¸Šæ¸¸æ¨¡å‹æä¾›äº†é¢å¤–çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08389">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-860f969a47a795dce89474d36c263575" align="middle">
<img src="https://picx.zhimg.com/v2-8645391a589b062f267dda05cdba46aa" align="middle">
<img src="https://picx.zhimg.com/v2-f35859d4a0d1eeb730c91618c1d6da43" align="middle">
<img src="https://picx.zhimg.com/v2-927ac46be61df6e5a66b282abd4af5d2" align="middle">
<img src="https://picx.zhimg.com/v2-e6f4992600936f3d8b0894cc354f6387" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VocalBench-zh-Decomposing-and-Benchmarking-the-Speech-Conversational-Abilities-in-Mandarin-Context"><a href="#VocalBench-zh-Decomposing-and-Benchmarking-the-Speech-Conversational-Abilities-in-Mandarin-Context" class="headerlink" title="VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context"></a>VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context</h2><p><strong>Authors:Heyang Liu, Ziyang Cheng, Yuhao Wang, Hongcheng Liu, Yiqi Li, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang</strong></p>
<p>The development of multi-modal large language models (LLMs) leads to intelligent approaches capable of speech interactions. As one of the most widely spoken languages globally, Mandarin is supported by most models to enhance their applicability and reach. However, the scarcity of comprehensive speech-to-speech (S2S) benchmarks in Mandarin contexts impedes systematic evaluation for developers and hinders fair model comparison for users. In this work, we propose VocalBench-zh, an ability-level divided evaluation suite adapted to Mandarin context consisting of 10 well-crafted subsets and over 10K high-quality instances, covering 12 user-oriented characters. The evaluation experiment on 14 mainstream models reveals the common challenges for current routes, and highlights the need for new insights into next-generation speech interactive systems. The evaluation codes and datasets will be available at <a target="_blank" rel="noopener" href="https://github.com/SJTU-OmniAgent/VocalBench-zh">https://github.com/SJTU-OmniAgent/VocalBench-zh</a>.</p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•æ¨åŠ¨äº†èƒ½å¤Ÿè¿›è¡Œè¯­éŸ³äº¤äº’çš„æ™ºèƒ½æ–¹æ³•ã€‚ä½œä¸ºä¸–ç•Œä¸Šä½¿ç”¨æœ€å¹¿æ³›çš„è¯­ç§ä¹‹ä¸€ï¼Œæ™®é€šè¯å—åˆ°å¤§å¤šæ•°æ¨¡å‹çš„æ”¯æŒï¼Œå¢å¼ºäº†å…¶é€‚ç”¨æ€§å’Œè¦†ç›–èŒƒå›´ã€‚ç„¶è€Œï¼Œæ™®é€šè¯è¯­å¢ƒä¸­å…¨é¢çš„è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆS2Sï¼‰åŸºå‡†çš„ç¼ºä¹é˜»ç¢äº†å¼€å‘äººå‘˜çš„ç³»ç»Ÿè¯„ä¼°å’Œç”¨æˆ·ä¹‹é—´çš„å…¬å¹³æ¨¡å‹æ¯”è¾ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘æ™®é€šè¯è¯­å¢ƒçš„VocalBench-zhè¯„ä¼°å¥—ä»¶ï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥èƒ½åŠ›æ°´å¹³è¿›è¡Œåˆ’åˆ†çš„èƒ½åŠ›å¥—ä»¶ï¼ŒåŒ…æ‹¬ç²¾å¿ƒåˆ¶ä½œçš„10ä¸ªå­é›†å’Œè¶…è¿‡1ä¸‡ä¸ªé«˜è´¨é‡å®ä¾‹ï¼Œæ¶µç›–é¢å‘ç”¨æˆ·çš„12ä¸ªå­—ç¬¦ã€‚åœ¨ä¸»æµæ¨¡å‹ä¸Šè¿›è¡Œçš„è¯„ä¼°å®éªŒè¡¨æ˜å½“å‰è·¯çº¿å­˜åœ¨çš„æ™®éæŒ‘æˆ˜ï¼Œå¹¶çªæ˜¾äº†æ–°ä¸€ä»£è¯­éŸ³äº¤äº’ç³»ç»Ÿçš„æ–°è§†è§’çš„å¿…è¦æ€§ã€‚è¯„ä¼°ä»£ç å’Œæ•°æ®é›†å°†å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å¾—ï¼š<a target="_blank" rel="noopener" href="https://github.com/SJTU-OmniAgent/VocalBench-zh%E3%80%82">https://github.com/SJTU-OmniAgent/VocalBench-zhã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08230v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•åŠå…¶åœ¨è¯­éŸ³äº¤äº’æ–¹é¢çš„æ™ºèƒ½åº”ç”¨ã€‚ç”±äºæ±‰è¯­æ˜¯å…¨çƒä½¿ç”¨æœ€å¹¿æ³›çš„è¯­ç§ä¹‹ä¸€ï¼Œå¤§å¤šæ•°æ¨¡å‹éƒ½æ”¯æŒæ±‰è¯­ä»¥å¢å¼ºå…¶é€‚ç”¨æ€§å’Œè¦†ç›–èŒƒå›´ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å…¨é¢çš„æ±‰è¯­è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆS2Sï¼‰åŸºå‡†æµ‹è¯•ï¼Œé˜»ç¢äº†å¼€å‘è€…çš„ç³»ç»Ÿè¯„ä¼°å’Œç”¨æˆ·ä¹‹é—´çš„å…¬å¹³æ¨¡å‹æ¯”è¾ƒã€‚æœ¬æ–‡æå‡ºäº†é¢å‘æ±‰è¯­çš„è¯„ä¼°å¥—ä»¶VocalBench-zhï¼ŒåŒ…å«10ä¸ªç²¾å¿ƒè®¾è®¡çš„å­é›†å’Œè¶…è¿‡1ä¸‡é«˜è´¨é‡å®ä¾‹ï¼Œæ¶µç›–12ä¸ªç”¨æˆ·å¯¼å‘è§’è‰²ã€‚å¯¹ä¸»æµæ¨¡å‹çš„è¯„ä¼°å®éªŒæ­ç¤ºäº†å½“å‰è·¯çº¿çš„å…±åŒæŒ‘æˆ˜ï¼Œå¹¶å¼ºè°ƒäº†ä¸‹ä¸€ä»£è¯­éŸ³äº¤äº’ç³»ç»Ÿçš„æ–°è§è§£çš„éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²å‘å±•åˆ°æ”¯æŒè¯­éŸ³äº¤äº’çš„æ™ºèƒ½æ–¹æ³•ã€‚</li>
<li>æ±‰è¯­æ˜¯å¤§å¤šæ•°è¯­è¨€æ¨¡å‹æ”¯æŒçš„è¯­ç§ï¼Œä»¥å¢å¼ºå…¶é€‚ç”¨æ€§å’Œè¦†ç›–èŒƒå›´ã€‚</li>
<li>ç¼ºä¹å…¨é¢çš„æ±‰è¯­è¯­éŸ³åˆ°è¯­éŸ³ï¼ˆS2Sï¼‰åŸºå‡†æµ‹è¯•ï¼Œé˜»ç¢äº†æ¨¡å‹çš„ç³»ç»Ÿè¯„ä¼°å’Œå…¬å¹³æ¯”è¾ƒã€‚</li>
<li>æå‡ºäº†é¢å‘æ±‰è¯­çš„è¯„ä¼°å¥—ä»¶VocalBench-zhï¼ŒåŒ…å«10ä¸ªå­é›†å’Œè¶…è¿‡1ä¸‡é«˜è´¨é‡å®ä¾‹ã€‚</li>
<li>è¯„ä¼°å®éªŒæ­ç¤ºäº†å½“å‰è·¯çº¿ä¸Šçš„æŒ‘æˆ˜ï¼Œéœ€è¦ä¸‹ä¸€ä»£è¯­éŸ³äº¤äº’ç³»ç»Ÿçš„æ–°è§è§£ã€‚</li>
<li>VocalBench-zhåŒ…æ‹¬å¤šç§ç”¨æˆ·å¯¼å‘è§’è‰²ï¼Œèƒ½å¤Ÿå…¨é¢è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08230">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-df1274aebf62f49fd64ca2cedcba6792" align="middle">
<img src="https://picx.zhimg.com/v2-5e35906bed44cfaca98718eaabcf630c" align="middle">
<img src="https://picx.zhimg.com/v2-77c3531897a2c40aef171e7226e11ed9" align="middle">
<img src="https://picx.zhimg.com/v2-7ed95890535dbb13bf7c56e00c9108e0" align="middle">
<img src="https://picx.zhimg.com/v2-2b2d4761cd7a0dd97c07bdd6e93bafcc" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Quantizing-Whisper-small-How-design-choices-affect-ASR-performance"><a href="#Quantizing-Whisper-small-How-design-choices-affect-ASR-performance" class="headerlink" title="Quantizing Whisper-small: How design choices affect ASR performance"></a>Quantizing Whisper-small: How design choices affect ASR performance</h2><p><strong>Authors:Arthur SÃ¶hler, Julian Irigoyen, Andreas SÃ¸eborg Kirkedal</strong></p>
<p>Large speech recognition models like Whisper-small achieve high accuracy but are difficult to deploy on edge devices due to their high computational demand. To this end, we present a unified, cross-library evaluation of post-training quantization (PTQ) on Whisper-small that disentangles the impact of quantization scheme, method, granularity, and bit-width. Our study is based on four libraries: PyTorch, Optimum-Quanto, HQQ, and bitsandbytes. Experiments on LibriSpeech test-clean and test-other show that dynamic int8 quantization with Quanto offers the best trade-off, reducing model size by 57% while improving on the baselineâ€™s word error rate. Static quantization performed worse, likely due to Whisperâ€™s Transformer architecture, while more aggressive formats (e.g., nf4, int3) achieved up to 71% compression at the cost of accuracy in noisy conditions. Overall, our results demonstrate that carefully chosen PTQ methods can substantially reduce model size and inference cost without retraining, enabling efficient deployment of Whisper-small on constrained hardware.</p>
<blockquote>
<p>åƒwhisper-smallè¿™æ ·çš„å¤§è¯­éŸ³è¯†åˆ«æ¨¡å‹è™½ç„¶å…·æœ‰å¾ˆé«˜çš„å‡†ç¡®æ€§ï¼Œä½†ç”±äºå…¶è®¡ç®—éœ€æ±‚é«˜ï¼Œéš¾ä»¥åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¹whisper-smallçš„è·¨åº“è®­ç»ƒåé‡åŒ–ï¼ˆPTQï¼‰è¿›è¡Œäº†ç»Ÿä¸€çš„è¯„ä¼°ï¼Œä»¥åˆ†æé‡åŒ–æ–¹æ¡ˆã€æ–¹æ³•ã€ç²’åº¦ä»¥åŠä½å®½çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶åŸºäºå››ä¸ªåº“ï¼šPyTorchã€Optimum-Quantoã€HQQå’Œbitsandbytesã€‚åœ¨LibriSpeechæµ‹è¯•é›†ï¼ˆcleanå’Œotherï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨Quantoçš„åŠ¨æ€int8é‡åŒ–æä¾›äº†æœ€ä½³çš„æƒè¡¡æ–¹æ¡ˆï¼Œæ¨¡å‹å¤§å°å‡å°‘äº†57%ï¼ŒåŒæ—¶æé«˜äº†åŸºçº¿æ¨¡å‹çš„è¯é”™è¯¯ç‡ã€‚é™æ€é‡åŒ–è¡¨ç°è¾ƒå·®ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºwhisperçš„Transformeræ¶æ„æ‰€è‡´ï¼Œè€Œæ›´æ¿€è¿›çš„å½¢å¼ï¼ˆå¦‚nf4ã€int3ï¼‰åœ¨å™ªå£°æ¡ä»¶ä¸‹è™½ç„¶èƒ½è¾¾åˆ°é«˜è¾¾71%çš„å‹ç¼©ç‡ï¼Œä½†å‡†ç¡®æ€§æœ‰æ‰€æŸå¤±ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç»è¿‡ç²¾å¿ƒé€‰æ‹©çš„PTQæ–¹æ³•å¯ä»¥åœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å¤§å¹…é™ä½æ¨¡å‹å¤§å°å’Œæ¨ç†æˆæœ¬ï¼Œä»è€Œä½¿whisper-smallèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¡¬ä»¶ä¸Šå®ç°é«˜æ•ˆéƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08093v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­éŸ³è¯†åˆ«æ¨¡å‹å¦‚Whisper-smallå…·æœ‰é«˜å‡†ç¡®æ€§ï¼Œä½†ç”±äºè®¡ç®—éœ€æ±‚é«˜ï¼Œéš¾ä»¥åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¯¹Whisper-smallè¿›è¡Œäº†è·¨åº“çš„åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰è¯„ä¼°ï¼Œç ”ç©¶äº†é‡åŒ–æ–¹æ¡ˆã€æ–¹æ³•ã€ç²’åº¦å’Œä½å®½çš„å½±å“ã€‚åœ¨PyTorchã€Optimum-Quantoã€HQQå’Œbitsandbyteså››ä¸ªåº“çš„åŸºç¡€ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨Quantoçš„åŠ¨æ€int8é‡åŒ–åœ¨å‡å°æ¨¡å‹å¤§å°57%çš„åŒæ—¶ï¼Œæé«˜äº†åŸºçº¿è¯çš„é”™è¯¯ç‡ã€‚é™æ€é‡åŒ–æ•ˆæœè¾ƒå·®ï¼Œå¯èƒ½æ˜¯ç”±äºWhisperçš„Transformeræ¶æ„æ‰€è‡´ï¼Œè€Œæ›´æ¿€è¿›æ ¼å¼ï¼ˆå¦‚nf4ã€int3ï¼‰åœ¨å™ªå£°æ¡ä»¶ä¸‹è™½ç„¶èƒ½è¾¾åˆ°71%çš„å‹ç¼©ç‡ï¼Œä½†å‡†ç¡®æ€§æœ‰æ‰€ä¸‹é™ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç²¾å¿ƒé€‰æ‹©çš„PTQæ–¹æ³•å¯ä»¥åœ¨ä¸é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå¤§å¹…å‡å°æ¨¡å‹å¤§å°å¹¶é™ä½æ¨ç†æˆæœ¬ï¼Œä»è€Œå®ç°Whisper-smallåœ¨å—é™ç¡¬ä»¶ä¸Šçš„æœ‰æ•ˆéƒ¨ç½²ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­éŸ³è¯†åˆ«æ¨¡å‹å¦‚Whisper-smallé¢ä¸´åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šçš„éƒ¨ç½²æŒ‘æˆ˜ï¼Œéœ€è¦é«˜è®¡ç®—èµ„æºå’Œå­˜å‚¨ç©ºé—´ã€‚</li>
<li>åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥å‡å°æ¨¡å‹å¤§å°å¹¶é™ä½æ¨ç†æˆæœ¬ã€‚</li>
<li>åœ¨å¯¹Whisper-smallè¿›è¡ŒPTQè¯„ä¼°æ—¶ï¼Œè€ƒè™‘äº†é‡åŒ–æ–¹æ¡ˆã€æ–¹æ³•ã€ç²’åº¦å’Œä½å®½çš„å½±å“ã€‚</li>
<li>åœ¨å¤šä¸ªåº“ï¼ˆPyTorchã€Optimum-Quantoã€HQQå’Œbitsandbytesï¼‰çš„å®éªŒä¸­ï¼ŒåŠ¨æ€int8é‡åŒ–è¡¨ç°å‡ºæœ€ä½³çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿåœ¨å‡å°æ¨¡å‹å¤§å°çš„åŒæ—¶æé«˜å‡†ç¡®æ€§ã€‚</li>
<li>é™æ€é‡åŒ–æ•ˆæœè¾ƒå·®ï¼Œå¯èƒ½æ˜¯ç”±äºæ¨¡å‹çš„Transformeræ¶æ„ç‰¹æ€§æ‰€è‡´ã€‚</li>
<li>æ›´æ¿€è¿›çš„é‡åŒ–æ ¼å¼ï¼ˆå¦‚nf4å’Œint3ï¼‰å¯ä»¥å®ç°æ›´é«˜çš„å‹ç¼©ç‡ï¼Œä½†åœ¨å™ªå£°æ¡ä»¶ä¸‹å¯èƒ½ä¼šæŸå¤±å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5b5614d89aa070d08c586cec385fa00a" align="middle">
<img src="https://picx.zhimg.com/v2-a45b22b53bf787e4be383a3944f3ebfe" align="middle">
<img src="https://picx.zhimg.com/v2-aeead9b8aa7cbdcd31205ef3b64ff483" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SpikCommander-A-High-performance-Spiking-Transformer-with-Multi-view-Learning-for-Efficient-Speech-Command-Recognition"><a href="#SpikCommander-A-High-performance-Spiking-Transformer-with-Multi-view-Learning-for-Efficient-Speech-Command-Recognition" class="headerlink" title="SpikCommander: A High-performance Spiking Transformer with Multi-view Learning for Efficient Speech Command Recognition"></a>SpikCommander: A High-performance Spiking Transformer with Multi-view Learning for Efficient Speech Command Recognition</h2><p><strong>Authors:Jiaqi Wang, Liutao Yu, Xiongri Shen, Sihang Guo, Chenlin Zhou, Leilei Zhao, Yi Zhong, Zhiguo Zhang, Zhengyu Ma</strong></p>
<p>Spiking neural networks (SNNs) offer a promising path toward energy-efficient speech command recognition (SCR) by leveraging their event-driven processing paradigm. However, existing SNN-based SCR methods often struggle to capture rich temporal dependencies and contextual information from speech due to limited temporal modeling and binary spike-based representations. To address these challenges, we first introduce the multi-view spiking temporal-aware self-attention (MSTASA) module, which combines effective spiking temporal-aware attention with a multi-view learning framework to model complementary temporal dependencies in speech commands. Building on MSTASA, we further propose SpikCommander, a fully spike-driven transformer architecture that integrates MSTASA with a spiking contextual refinement channel MLP (SCR-MLP) to jointly enhance temporal context modeling and channel-wise feature integration. We evaluate our method on three benchmark datasets: the Spiking Heidelberg Dataset (SHD), the Spiking Speech Commands (SSC), and the Google Speech Commands V2 (GSC). Extensive experiments demonstrate that SpikCommander consistently outperforms state-of-the-art (SOTA) SNN approaches with fewer parameters under comparable time steps, highlighting its effectiveness and efficiency for robust speech command recognition.</p>
<blockquote>
<p>è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSpiking Neural Networksï¼Œç®€ç§°SNNsï¼‰é€šè¿‡åˆ©ç”¨å…¶äº‹ä»¶é©±åŠ¨å¤„ç†èŒƒå¼ï¼Œä¸ºå®ç°èƒ½æºé«˜æ•ˆçš„è¯­éŸ³æŒ‡ä»¤è¯†åˆ«ï¼ˆSpeech Command Recognitionï¼Œç®€ç§°SCRï¼‰æä¾›äº†å‰æ™¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºSNNçš„SCRæ–¹æ³•å¾€å¾€å› æœ‰é™çš„æ—¶åºå»ºæ¨¡å’ŒåŸºäºäºŒè¿›åˆ¶çš„è„‰å†²è¡¨ç¤ºè€Œéš¾ä»¥ä»è¯­éŸ³ä¸­æ•è·ä¸°å¯Œçš„æ—¶åºä¾èµ–å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†å¤šè§†è§’è„‰å†²æ—¶åºæ„ŸçŸ¥è‡ªæ³¨æ„åŠ›ï¼ˆMulti-View Spiking Temporal-Aware Self-Attentionï¼Œç®€ç§°MSTASAï¼‰æ¨¡å—ï¼Œå®ƒå°†æœ‰æ•ˆçš„è„‰å†²æ—¶åºæ„ŸçŸ¥æ³¨æ„åŠ›ä¸å¤šè§†è§’å­¦ä¹ æ¡†æ¶ç›¸ç»“åˆï¼Œå¯¹è¯­éŸ³å‘½ä»¤ä¸­çš„äº’è¡¥æ—¶åºä¾èµ–è¿›è¡Œå»ºæ¨¡ã€‚åŸºäºMSTASAï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†SpikCommanderï¼Œè¿™æ˜¯ä¸€ç§å®Œå…¨ç”±è„‰å†²é©±åŠ¨çš„å˜å‹å™¨æ¶æ„ï¼Œå®ƒå°†MSTASAä¸è„‰å†²ä¸Šä¸‹æ–‡ç»†åŒ–é€šé“MLPï¼ˆSCR-MLPï¼‰ç›¸ç»“åˆï¼Œä»¥å…±åŒå¢å¼ºæ—¶åºä¸Šä¸‹æ–‡å»ºæ¨¡å’Œé€šé“çº§ç‰¹å¾é›†æˆã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼šSpiking Heidelbergæ•°æ®é›†ï¼ˆSHDï¼‰ã€Spiking Speech Commandsï¼ˆSSCï¼‰å’ŒGoogle Speech Commands V2ï¼ˆGSCï¼‰ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSpikCommanderåœ¨å‚æ•°è¾ƒå°‘ã€æ—¶é—´æ­¥é•¿ç›¸å½“çš„æƒ…å†µä¸‹ï¼Œå§‹ç»ˆä¼˜äºæœ€å…ˆè¿›ï¼ˆState-of-the-Artï¼Œç®€ç§°SOTAï¼‰çš„SNNæ–¹æ³•ï¼Œå‡¸æ˜¾å…¶åœ¨ç¨³å¥çš„è¯­éŸ³æŒ‡ä»¤è¯†åˆ«æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07883v2">PDF</a> Accepted by The Fortieth AAAI Conference on Artificial Intelligence (AAAI 2026)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰çš„äº‹ä»¶é©±åŠ¨å¤„ç†èŒƒå¼ï¼Œå…¶ä¸ºå®ç°èƒ½æºé«˜æ•ˆçš„è¯­éŸ³æŒ‡ä»¤è¯†åˆ«ï¼ˆSCRï¼‰æä¾›äº†å‰æ™¯ã€‚é’ˆå¯¹ç°æœ‰SNN-based SCRæ–¹æ³•åœ¨æ•æ‰è¯­éŸ³çš„ä¸°å¯Œæ—¶é—´ä¾èµ–æ€§å’Œä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢å­˜åœ¨çš„å±€é™æ€§ï¼Œç ”ç©¶å¼•å…¥äº†å¤šè§†è§’è„‰å†²æ—¶é—´æ„ŸçŸ¥è‡ªæ³¨æ„åŠ›ï¼ˆMSTASAï¼‰æ¨¡å—ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥æå‡ºäº†å®Œå…¨è„‰å†²é©±åŠ¨çš„SpikCommanderè½¬æ¢å™¨æ¶æ„ï¼Œç»“åˆMSTASAä¸è„‰å†²ä¸Šä¸‹æ–‡ç»†åŒ–é€šé“MLPï¼ˆSCR-MLPï¼‰ï¼Œå…±åŒå¢å¼ºæ—¶é—´ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œé€šé“ç‰¹å¾é›†æˆã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSpikCommanderåœ¨å‚æ•°è¾ƒå°‘çš„æƒ…å†µä¸‹ï¼Œåœ¨æ—¶é—´æ­¥é•¿ç›¸å½“çš„æƒ…å†µä¸‹å§‹ç»ˆä¼˜äºæœ€æ–°çš„SNNæ–¹æ³•ï¼Œå±•ç°å‡ºå…¶åœ¨ç¨³å¥è¯­éŸ³æŒ‡ä»¤è¯†åˆ«æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰ä¸ºå®ç°èƒ½æºé«˜æ•ˆçš„è¯­éŸ³æŒ‡ä»¤è¯†åˆ«æä¾›äº†æ½œåŠ›ã€‚</li>
<li>ç°æœ‰SNNæ–¹æ³•åœ¨æ•æ‰è¯­éŸ³çš„æ—¶é—´ä¾èµ–æ€§å’Œä¸Šä¸‹æ–‡ä¿¡æ¯æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥çš„å¤šè§†è§’è„‰å†²æ—¶é—´æ„ŸçŸ¥è‡ªæ³¨æ„åŠ›ï¼ˆMSTASAï¼‰æ¨¡å—ï¼Œèƒ½æœ‰æ•ˆå»ºæ¨¡è¯­éŸ³å‘½ä»¤ä¸­çš„æ—¶é—´ä¾èµ–æ€§ã€‚</li>
<li>SpikCommanderæ¶æ„ç»“åˆäº†MSTASAä¸è„‰å†²ä¸Šä¸‹æ–‡ç»†åŒ–é€šé“MLPï¼ˆSCR-MLPï¼‰ï¼Œå¢å¼ºäº†æ—¶é—´ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œé€šé“ç‰¹å¾é›†æˆã€‚</li>
<li>SpikCommanderåœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºå…¶ä»–æœ€æ–°çš„SNNæ–¹æ³•ã€‚</li>
<li>SpikCommanderåœ¨å‚æ•°è¾ƒå°‘å’Œæ—¶é—´æ­¥é•¿ç›¸å½“çš„æƒ…å†µä¸‹è¡¨ç°å‡ºå…¶æœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-63872aacd6d2e0ddbd6a5815818f20e4" align="middle">
<img src="https://picx.zhimg.com/v2-0976e4820e6f416d1c2ce573f5ad7f61" align="middle">
<img src="https://picx.zhimg.com/v2-798bf4f22b0837fb5ede846f4a4a56aa" align="middle">
<img src="https://picx.zhimg.com/v2-523b738b64c7afc5623d61134e2b5ea3" align="middle">
<img src="https://picx.zhimg.com/v2-96612df7e723143d3fba33392d74bf93" align="middle">
<img src="https://picx.zhimg.com/v2-fabf6eaa28e6987f6a2743a98133791e" align="middle">
<img src="https://picx.zhimg.com/v2-a378219636170b7b1a2587fe36124342" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SynTTS-Commands-A-Public-Dataset-for-On-Device-KWS-via-TTS-Synthesized-Multilingual-Speech"><a href="#SynTTS-Commands-A-Public-Dataset-for-On-Device-KWS-via-TTS-Synthesized-Multilingual-Speech" class="headerlink" title="SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech"></a>SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech</h2><p><strong>Authors:Lu Gan, Xi Li</strong></p>
<p>The development of high-performance, on-device keyword spotting (KWS) systems for ultra-low-power hardware is critically constrained by the scarcity of specialized, multi-command training datasets. Traditional data collection through human recording is costly, slow, and lacks scalability. This paper introduces SYNTTS-COMMANDS, a novel, multilingual voice command dataset entirely generated using state-of-the-art Text-to-Speech (TTS) synthesis. By leveraging the CosyVoice 2 model and speaker embeddings from public corpora, we created a scalable collection of English and Chinese commands. Extensive benchmarking across a range of efficient acoustic models demonstrates that our synthetic dataset enables exceptional accuracy, achieving up to 99.5% on English and 98% on Chinese command recognition. These results robustly validate that synthetic speech can effectively replace human-recorded audio for training KWS classifiers. Our work directly addresses the data bottleneck in TinyML, providing a practical, scalable foundation for building private, low-latency, and energy-efficient voice interfaces on resource-constrained edge devices.</p>
<blockquote>
<p>å¼€å‘é’ˆå¯¹è¶…ä½åŠŸè€—ç¡¬ä»¶çš„é«˜æ€§èƒ½ã€è®¾å¤‡å†…å…³é”®è¯è¯†åˆ«ï¼ˆKWSï¼‰ç³»ç»Ÿå—åˆ°äº†ä¸“ç”¨å¤šå‘½ä»¤è®­ç»ƒæ•°æ®é›†ç¨€ç¼ºæ€§çš„ä¸¥æ ¼é™åˆ¶ã€‚é€šè¿‡äººå·¥å½•åˆ¶æ”¶é›†æ•°æ®çš„æ–¹å¼æˆæœ¬é«˜æ˜‚ã€é€Ÿåº¦æ…¢ä¸”ç¼ºä¹å¯æ‰©å±•æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†SYNTTS-COMMANDSï¼Œä¸€ç§å…¨æ–°å¤šè¯­è¨€è¯­éŸ³å‘½ä»¤æ•°æ®é›†ï¼Œå®Œå…¨ä½¿ç”¨æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆç”Ÿæˆã€‚æˆ‘ä»¬é€šè¿‡åˆ©ç”¨CosyVoice 2æ¨¡å‹å’Œå…¬å…±è¯­æ–™åº“ä¸­çš„è¯´è¯äººåµŒå…¥æŠ€æœ¯ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„è‹±è¯­å’Œä¸­æ–‡å‘½ä»¤é›†åˆã€‚åœ¨ä¸€ç³»åˆ—é«˜æ•ˆçš„å£°å­¦æ¨¡å‹ä¸Šè¿›è¡Œå¹¿æ³›åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œæˆ‘ä»¬çš„åˆæˆæ•°æ®é›†èƒ½å¤Ÿå®ç°å‡ºè‰²çš„å‡†ç¡®æ€§ï¼Œè‹±è¯­å‘½ä»¤è¯†åˆ«ç‡é«˜è¾¾99.5%ï¼Œä¸­æ–‡å‘½ä»¤è¯†åˆ«ç‡è¾¾98%ã€‚è¿™äº›ç»“æœç¨³å¥åœ°éªŒè¯äº†åˆæˆè¯­éŸ³å¯ä»¥æœ‰æ•ˆåœ°æ›¿ä»£äººç±»å½•åˆ¶çš„éŸ³é¢‘ï¼Œç”¨äºè®­ç»ƒKWSåˆ†ç±»å™¨ã€‚æˆ‘ä»¬çš„å·¥ä½œç›´æ¥è§£å†³äº†TinyMLä¸­çš„æ•°æ®ç“¶é¢ˆé—®é¢˜ï¼Œä¸ºåœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šæ„å»ºç§å¯†ã€ä½å»¶è¿Ÿå’ŒèŠ‚èƒ½çš„è¯­éŸ³æ¥å£æä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07821v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§å…¨æ–°çš„å¤šè¯­ç§è¯­éŸ³æŒ‡ä»¤æ•°æ®é›†SYNTTS-COMMANDSï¼Œè¯¥æ•°æ®é›†å®Œå…¨é€šè¿‡å…ˆè¿›çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆæŠ€æœ¯ç”Ÿæˆã€‚åˆ©ç”¨CosyVoice 2æ¨¡å‹å’Œå…¬å¼€è¯­æ–™åº“çš„è¯´è¯äººåµŒå…¥æŠ€æœ¯ï¼Œæˆ‘ä»¬åˆ›å»ºäº†åŒ…å«è‹±è¯­å’Œä¸­æ–‡æŒ‡ä»¤çš„å¯æ‰©å±•æ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥åˆæˆæ•°æ®é›†åœ¨å£°å­¦æ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œè‹±è¯­å‘½ä»¤è¯†åˆ«ç‡é«˜è¾¾99.5%ï¼Œä¸­æ–‡è¾¾98%ã€‚ç»“æœè¯æ˜ï¼Œåˆæˆè¯­éŸ³å¯æœ‰æ•ˆæ›¿ä»£äººç±»å½•éŸ³ï¼Œç”¨äºè®­ç»ƒå…³é”®è¯è¯†åˆ«åˆ†ç±»å™¨ã€‚è¯¥ç ”ç©¶ç›´æ¥è§£å†³äº†TinyMLé¢†åŸŸçš„æ•°æ®ç“¶é¢ˆé—®é¢˜ï¼Œä¸ºæ„å»ºç§æœ‰ã€ä½å»¶è¿Ÿã€èŠ‚èƒ½çš„è¯­éŸ³æ¥å£æä¾›äº†å®ç”¨ã€å¯æ‰©å±•çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SYNTTS-COMMANDSæ˜¯ä¸€ä¸ªåŸºäºæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆçš„å¤šè¯­ç§è¯­éŸ³æŒ‡ä»¤æ•°æ®é›†ã€‚</li>
<li>è¯¥æ•°æ®é›†é€šè¿‡åˆ©ç”¨CosyVoice 2æ¨¡å‹å’Œå…¬å¼€è¯­æ–™åº“çš„è¯´è¯äººåµŒå…¥æŠ€æœ¯åˆ›å»ºï¼ŒåŒ…å«è‹±è¯­å’Œä¸­æ–‡æŒ‡ä»¤ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œè¯¥åˆæˆæ•°æ®é›†åœ¨å£°å­¦æ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜ç§€ï¼Œè‹±è¯­å‘½ä»¤è¯†åˆ«ç‡é«˜ã€‚</li>
<li>åˆæˆè¯­éŸ³å¯ä»¥æœ‰æ•ˆæ›¿ä»£äººç±»å½•éŸ³ï¼Œç”¨äºè®­ç»ƒå…³é”®è¯è¯†åˆ«åˆ†ç±»å™¨ã€‚</li>
<li>è¯¥ç ”ç©¶è§£å†³äº†TinyMLé¢†åŸŸçš„æ•°æ®ç“¶é¢ˆé—®é¢˜ã€‚</li>
<li>ç ”ç©¶ä¸ºæ„å»ºç§æœ‰ã€ä½å»¶è¿Ÿã€èŠ‚èƒ½çš„è¯­éŸ³æ¥å£æä¾›äº†å®ç”¨ã€å¯æ‰©å±•çš„åŸºç¡€ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c610255d59a5c99eec6b4950dbb8808e" align="middle">
<img src="https://picx.zhimg.com/v2-e6cb74fd28250359262d951c1fbf5e5b" align="middle">
<img src="https://picx.zhimg.com/v2-63515865902d8d6feaf6ae190e1159e6" align="middle">
<img src="https://picx.zhimg.com/v2-c2d2f2a8521148f1d855b67e039dea40" align="middle">
<img src="https://picx.zhimg.com/v2-4d1d9f4b0ba60c590c4c0c3e0f542ff7" align="middle">
<img src="https://picx.zhimg.com/v2-df876bc8469f2062747aa47e39d05c93" align="middle">
<img src="https://picx.zhimg.com/v2-82c7ae51985b45c94840cbac85a28065" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Omni-AVSR-Towards-Unified-Multimodal-Speech-Recognition-with-Large-Language-Models"><a href="#Omni-AVSR-Towards-Unified-Multimodal-Speech-Recognition-with-Large-Language-Models" class="headerlink" title="Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large Language Models"></a>Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large Language Models</h2><p><strong>Authors:Umberto Cappellazzo, Xubo Liu, Pingchuan Ma, Stavros Petridis, Maja Pantic</strong></p>
<p>Large language models (LLMs) have recently achieved impressive results in speech recognition across multiple modalities, including Auditory Speech Recognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual Speech Recognition (AVSR). Despite this progress, current LLM-based approaches typically address each task independently, training separate models that raise computational and deployment resource use while missing potential cross-task synergies. They also rely on fixed-rate token compression, which restricts flexibility in balancing accuracy with efficiency. These limitations highlight the need for a unified framework that can support ASR, VSR, and AVSR while enabling elastic inference. To this end, we present Omni-AVSR, a unified audio-visual LLM that combines efficient multi-granularity training with parameter-efficient adaptation. Specifically, we adapt the matryoshka representation learning paradigm to efficiently train across multiple audio and visual granularities, reducing its inherent training resource use. Furthermore, we explore three LoRA-based strategies for adapting the backbone LLM, balancing shared and task-specific specialization. Experiments on LRS2 and LRS3 show that Omni-AVSR achieves comparable or superior accuracy to state-of-the-art baselines while training a single model at substantially lower training and deployment resource use. The model also remains robust under acoustic noise, and we analyze its scaling behavior as LLM size increases, providing insights into the trade-off between performance and efficiency.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘åœ¨å¤šæ¨¡æ€è¯­éŸ³è¯†åˆ«çš„å¤šä¸ªé¢†åŸŸå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼ŒåŒ…æ‹¬å¬è§‰è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰å’Œè§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ã€‚å°½ç®¡å–å¾—äº†è¿›å±•ï¼Œä½†å½“å‰çš„LLMæ–¹æ³•é€šå¸¸ç‹¬ç«‹å¤„ç†æ¯ä¸ªä»»åŠ¡ï¼Œè®­ç»ƒå•ç‹¬çš„æ¨¡å‹ï¼Œè¿™å¢åŠ äº†è®¡ç®—å’Œéƒ¨ç½²èµ„æºçš„ä½¿ç”¨ï¼ŒåŒæ—¶å¿½ç•¥äº†æ½œåœ¨çš„è·¨ä»»åŠ¡ååŒä½œç”¨ã€‚å®ƒä»¬è¿˜ä¾èµ–äºå›ºå®šé€Ÿç‡ä»¤ç‰Œå‹ç¼©ï¼Œè¿™é™åˆ¶äº†å¹³è¡¡å‡†ç¡®æ€§å’Œæ•ˆç‡æ—¶çš„çµæ´»æ€§ã€‚è¿™äº›å±€é™æ€§çªæ˜¾äº†éœ€è¦ä¸€ç§èƒ½å¤Ÿæ”¯æŒASRã€VSRå’ŒAVSRçš„ç»Ÿä¸€æ¡†æ¶ï¼ŒåŒæ—¶å®ç°å¼¹æ€§æ¨ç†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Omni-AVSRï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§†å¬LLMï¼Œå®ƒç»“åˆäº†é«˜æ•ˆçš„å¤šç²’åº¦è®­ç»ƒå’Œå‚æ•°é«˜æ•ˆçš„é€‚åº”æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¿„ç½—æ–¯å¥—å¨ƒè¡¨ç¤ºå­¦ä¹ èŒƒå¼ï¼Œåœ¨å¤šä¸ªéŸ³é¢‘å’Œè§†è§‰ç²’åº¦ä¸Šè¿›è¡Œé«˜æ•ˆè®­ç»ƒï¼Œé™ä½äº†å…¶å›ºæœ‰çš„è®­ç»ƒèµ„æºä½¿ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸‰ç§åŸºäºLoRAçš„é€‚åº”ä¸»å¹²LLMçš„ç­–ç•¥ï¼Œå¹³è¡¡å…±äº«å’Œç‰¹å®šä»»åŠ¡çš„ä¸“é•¿ã€‚åœ¨LRS2å’ŒLRS3ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒOmni-AVSRè¾¾åˆ°äº†æˆ–è¶…è¿‡äº†æœ€å…ˆè¿›çš„åŸºå‡†æµ‹è¯•ç²¾åº¦ï¼ŒåŒæ—¶ä½¿ç”¨å•ä¸€æ¨¡å‹åœ¨è®­ç»ƒå’Œéƒ¨ç½²èµ„æºæ–¹é¢å¤§å¤§é™ä½ã€‚è¯¥æ¨¡å‹åœ¨å£°å­¦å™ªå£°ä¸‹ä¿æŒç¨³å¥ï¼Œæˆ‘ä»¬åˆ†æäº†å…¶éšç€LLMè§„æ¨¡å¢åŠ è€Œæ‰©å±•çš„è¡Œä¸ºï¼Œä¸ºæ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´çš„æƒè¡¡æä¾›äº†è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07253v1">PDF</a> Project website: <a target="_blank" rel="noopener" href="https://umbertocappellazzo.github.io/Omni-AVSR/">https://umbertocappellazzo.github.io/Omni-AVSR/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯­éŸ³è¯†åˆ«å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆæœï¼ŒåŒ…æ‹¬å¬è§‰è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰å’Œè§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ã€‚ç„¶è€Œï¼Œå½“å‰LLMæ–¹æ³•é€šå¸¸ç‹¬ç«‹å¤„ç†æ¯é¡¹ä»»åŠ¡ï¼Œè®­ç»ƒå•ç‹¬æ¨¡å‹ï¼Œå¯¼è‡´è®¡ç®—ä¸éƒ¨ç½²èµ„æºæ¶ˆè€—è¾ƒå¤§ï¼Œä¸”æ— æ³•å……åˆ†åˆ©ç”¨è·¨ä»»åŠ¡ååŒä½œç”¨ã€‚æ­¤å¤–ï¼Œå®ƒä»¬ä¾èµ–äºå›ºå®šé€Ÿç‡ä»¤ç‰Œå‹ç¼©ï¼Œéš¾ä»¥åœ¨å‡†ç¡®æ€§ä¸æ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†Omni-AVSRç»Ÿä¸€éŸ³é¢‘è§†è§‰LLMæ¡†æ¶ï¼Œç»“åˆé«˜æ•ˆå¤šç²’åº¦è®­ç»ƒå’Œå‚æ•°é«˜æ•ˆé€‚åº”æŠ€æœ¯ã€‚é€šè¿‡é€‚åº”matryoshkaè¡¨ç¤ºå­¦ä¹ èŒƒå¼ï¼Œå¯åœ¨å¤šä¸ªéŸ³é¢‘å’Œè§†è§‰ç²’åº¦ä¸Šå®ç°é«˜æ•ˆè®­ç»ƒï¼Œé™ä½å›ºæœ‰èµ„æºæ¶ˆè€—ã€‚å®éªŒè¡¨æ˜ï¼ŒOmni-AVSRåœ¨LRS2å’ŒLRS3ä¸Šå®ç°äº†ä¸æœ€ä½³åŸºå‡†ç›¸å½“çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶å¤§å¹…é™ä½äº†è®­ç»ƒå’Œéƒ¨ç½²çš„èµ„æºæ¶ˆè€—ã€‚æ¨¡å‹åœ¨å™ªå£°ç¯å¢ƒä¸‹ä¿æŒç¨³å¥æ€§ï¼Œå¹¶åˆ†æäº†éšç€LLMè§„æ¨¡å¢åŠ ï¼Œæ€§èƒ½ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨è¯­éŸ³è¯†åˆ«å¤šä¸ªé¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬ASRã€VSRå’ŒAVSRã€‚</li>
<li>å½“å‰LLMæ–¹æ³•ç‹¬ç«‹å¤„ç†ä»»åŠ¡ï¼Œå¯¼è‡´èµ„æºæ¶ˆè€—å¤§ï¼Œä¸”ç¼ºä¹è·¨ä»»åŠ¡ååŒã€‚</li>
<li>Omni-AVSRæ¡†æ¶æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œå®ç°ç»Ÿä¸€éŸ³é¢‘è§†è§‰LLMã€‚</li>
<li>Omni-AVSRé‡‡ç”¨é«˜æ•ˆå¤šç²’åº¦è®­ç»ƒå’Œå‚æ•°é«˜æ•ˆé€‚åº”æŠ€æœ¯ã€‚</li>
<li>é€šè¿‡matryoshkaè¡¨ç¤ºå­¦ä¹ èŒƒå¼å®ç°éŸ³é¢‘å’Œè§†è§‰ç²’åº¦ä¸Šçš„é«˜æ•ˆè®­ç»ƒã€‚</li>
<li>å®éªŒè¯æ˜Omni-AVSRåœ¨LRS2å’ŒLRS3ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œèµ„æºæ¶ˆè€—é™ä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b57eb0678a89bd56f282e775e6652c7" align="middle">
<img src="https://picx.zhimg.com/v2-f6f6953effb8f98293a3b861443fa025" align="middle">
<img src="https://picx.zhimg.com/v2-166dd91cb26737f8d4aa7e1da0f73023" align="middle">
<img src="https://picx.zhimg.com/v2-57618d2cbe313972e86e805d456ba43e" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="E2E-VGuard-Adversarial-Prevention-for-Production-LLM-based-End-To-End-Speech-Synthesis"><a href="#E2E-VGuard-Adversarial-Prevention-for-Production-LLM-based-End-To-End-Speech-Synthesis" class="headerlink" title="E2E-VGuard: Adversarial Prevention for Production LLM-based End-To-End Speech Synthesis"></a>E2E-VGuard: Adversarial Prevention for Production LLM-based End-To-End Speech Synthesis</h2><p><strong>Authors:Zhisheng Zhang, Derui Wang, Yifan Mi, Zhiyong Wu, Jie Gao, Yuxin Cao, Kai Ye, Minhui Xue, Jie Hao</strong></p>
<p>Recent advancements in speech synthesis technology have enriched our daily lives, with high-quality and human-like audio widely adopted across real-world applications. However, malicious exploitation like voice-cloning fraud poses severe security risks. Existing defense techniques struggle to address the production large language model (LLM)-based speech synthesis. While previous studies have considered the protection for fine-tuning synthesizers, they assume manually annotated transcripts. Given the labor intensity of manual annotation, end-to-end (E2E) systems leveraging automatic speech recognition (ASR) to generate transcripts are becoming increasingly prevalent, e.g., voice cloning via commercial APIs. Therefore, this E2E speech synthesis also requires new security mechanisms. To tackle these challenges, we propose E2E-VGuard, a proactive defense framework for two emerging threats: (1) production LLM-based speech synthesis, and (2) the novel attack arising from ASR-driven E2E scenarios. Specifically, we employ the encoder ensemble with a feature extractor to protect timbre, while ASR-targeted adversarial examples disrupt pronunciation. Moreover, we incorporate the psychoacoustic model to ensure perturbative imperceptibility. For a comprehensive evaluation, we test 16 open-source synthesizers and 3 commercial APIs across Chinese and English datasets, confirming E2E-VGuardâ€™s effectiveness in timbre and pronunciation protection. Real-world deployment validation is also conducted. Our code and demo page are available at <a target="_blank" rel="noopener" href="https://wxzyd123.github.io/e2e-vguard/">https://wxzyd123.github.io/e2e-vguard/</a>.</p>
<blockquote>
<p>è¿‘æœŸè¯­éŸ³åˆæˆæŠ€æœ¯çš„è¿›å±•ä¸°å¯Œäº†æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ï¼Œé«˜è´¨é‡ã€æ‹ŸäººåŒ–çš„éŸ³é¢‘åœ¨çœŸå®ä¸–ç•Œçš„åº”ç”¨ä¸­å¾—åˆ°äº†å¹¿æ³›é‡‡ç”¨ã€‚ç„¶è€Œï¼Œåƒè¯­éŸ³å…‹éš†æ¬ºè¯ˆè¿™æ ·çš„æ¶æ„åˆ©ç”¨å´å¸¦æ¥äº†ä¸¥é‡çš„å®‰å…¨é£é™©ã€‚ç°æœ‰çš„é˜²å¾¡æŠ€æœ¯å¾ˆéš¾åº”å¯¹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­éŸ³åˆæˆã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶å·²ç»è€ƒè™‘äº†åˆæˆå™¨å¾®è°ƒçš„ä¿æŠ¤ï¼Œä½†å®ƒä»¬å‡è®¾äº†æ‰‹åŠ¨æ ‡æ³¨çš„æ–‡æœ¬ã€‚è€ƒè™‘åˆ°æ‰‹åŠ¨æ ‡æ³¨çš„åŠ³åŠ¨å¯†é›†åº¦ï¼Œåˆ©ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç”Ÿæˆæ–‡æœ¬çš„ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰ç³»ç»Ÿæ­£å˜å¾—è¶Šæ¥è¶Šæ™®éï¼Œä¾‹å¦‚é€šè¿‡å•†ä¸šAPIè¿›è¡Œè¯­éŸ³å…‹éš†ã€‚å› æ­¤ï¼Œè¿™ç§E2Eè¯­éŸ³åˆæˆä¹Ÿéœ€è¦æ–°çš„å®‰å…¨æœºåˆ¶ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†E2E-VGuardï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸¤ç§æ–°å…´å¨èƒçš„ä¸»åŠ¨é˜²å¾¡æ¡†æ¶ï¼šï¼ˆ1ï¼‰åŸºäºç”Ÿäº§LLMçš„è¯­éŸ³åˆæˆï¼Œï¼ˆ2ï¼‰ç”±ASRé©±åŠ¨çš„æ–°å‹æ”»å‡»åœºæ™¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨ç¼–ç å™¨é›†åˆä¸ç‰¹å¾æå–å™¨æ¥ä¿æŠ¤éŸ³è‰²ï¼ŒåŒæ—¶é’ˆå¯¹ASRçš„å¯¹æŠ—æ€§ç¤ºä¾‹ä¼šç ´åå‘éŸ³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆäº†å¿ƒç†å£°å­¦æ¨¡å‹æ¥ç¡®ä¿æ‰°åŠ¨çš„ä¸æ˜“å¯Ÿè§‰æ€§ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬æµ‹è¯•äº†16ä¸ªå¼€æºåˆæˆå™¨å’Œè·¨è¶Šä¸­æ–‡å’Œè‹±æ–‡æ•°æ®é›†çš„3ä¸ªå•†ä¸šAPIï¼Œè¯å®äº†E2E-VGuardåœ¨éŸ³è‰²å’Œå‘éŸ³ä¿æŠ¤æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿˜è¿›è¡Œäº†ç°å®ä¸–ç•Œéƒ¨ç½²éªŒè¯ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¼”ç¤ºé¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://wxzyd123.github.io/e2e-vguard/%E8%AE%BF%E9%97%AE%E3%80%82">https://wxzyd123.github.io/e2e-vguard/è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07099v1">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸè¯­éŸ³åˆæˆæŠ€æœ¯çš„è¿›å±•ä¸ºæ—¥å¸¸ç”Ÿæ´»å¸¦æ¥äº†ä¾¿æ·ï¼Œé«˜è´¨é‡ã€äººæ€§åŒ–çš„éŸ³é¢‘åœ¨å®é™…åº”ç”¨ä¸­è¢«å¹¿æ³›é‡‡ç”¨ã€‚ç„¶è€Œï¼Œè¯­éŸ³å…‹éš†ç­‰æ¶æ„è¡Œä¸ºå¸¦æ¥çš„æ¬ºè¯ˆç°è±¡ç»™å®‰å…¨å¸¦æ¥äº†ä¸¥é‡å¨èƒã€‚ç°æœ‰é˜²å¾¡æŠ€æœ¯éš¾ä»¥åº”å¯¹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­éŸ³åˆæˆã€‚è™½ç„¶ä»¥å‰çš„ç ”ç©¶å·²ç»è€ƒè™‘äº†åˆæˆå™¨çš„ç²¾ç»†è°ƒæ•´ä¿æŠ¤ï¼Œä½†å®ƒä»¬ä¾èµ–äºæ‰‹åŠ¨æ³¨é‡Šçš„æ–‡æœ¬ã€‚è€ƒè™‘åˆ°æ‰‹åŠ¨æ³¨é‡Šçš„åŠ³åŠ¨å¯†é›†æ€§ï¼Œåˆ©ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç”Ÿæˆæ–‡æœ¬çš„ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰ç³»ç»Ÿè¶Šæ¥è¶Šæ™®éï¼Œä¾‹å¦‚é€šè¿‡å•†ä¸šAPIè¿›è¡Œè¯­éŸ³å…‹éš†ã€‚å› æ­¤ï¼Œè¿™ç§E2Eè¯­éŸ³åˆæˆä¹Ÿéœ€è¦æ–°çš„å®‰å…¨æœºåˆ¶ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†E2E-VGuardï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸¤é¡¹æ–°å…´å¨èƒçš„ç§¯æé˜²å¾¡æ¡†æ¶ï¼šä¸€æ˜¯åŸºäºç”Ÿäº§çš„LLMè¯­éŸ³åˆæˆï¼ŒäºŒæ˜¯æ¥è‡ªASRé©±åŠ¨E2Eåœºæ™¯çš„æ–°æ”»å‡»ã€‚æˆ‘ä»¬é€šè¿‡ç¼–ç å™¨ç»„åˆå’Œç‰¹å¾æå–å™¨æ¥ä¿æŠ¤éŸ³è‰²ï¼ŒåŒæ—¶ä½¿ç”¨é’ˆå¯¹ASRçš„å¯¹æŠ—æ€§ä¾‹å­æ¥å¹²æ‰°å‘éŸ³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆå¿ƒç†å£°å­¦æ¨¡å‹æ¥ç¡®ä¿æ‰°åŠ¨çš„ä¸æ˜“å¯Ÿè§‰æ€§ã€‚æˆ‘ä»¬å¯¹16ä¸ªå¼€æºåˆæˆå™¨å’Œ3ä¸ªå•†ä¸šAPIè¿›è¡Œäº†ä¸­è‹±æ–‡æ•°æ®é›†çš„ç»¼åˆè¯„ä¼°ï¼Œè¯å®äº†E2E-VGuardåœ¨éŸ³è‰²å’Œå‘éŸ³ä¿æŠ¤æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿˜è¿›è¡Œäº†å®é™…éƒ¨ç½²éªŒè¯ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¼”ç¤ºé¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://wxzyd123.github.io/e2e-vguard/%E8%AE%BF%E9%97%AE%E3%80%82">https://wxzyd123.github.io/e2e-vguard/è®¿é—®ã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>è¯­éŸ³åˆæˆæŠ€æœ¯çš„æœ€æ–°è¿›å±•åŠå…¶åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å¹¿æ³›åº”ç”¨ã€‚</li>
<li>è¯­éŸ³å…‹éš†æ¬ºè¯ˆç°è±¡çš„å‡ºç°åŠå…¶å¸¦æ¥çš„å®‰å…¨é£é™©ã€‚</li>
<li>ç°æœ‰é˜²å¾¡æŠ€æœ¯åœ¨åº”å¯¹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­éŸ³åˆæˆæ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>æè®®çš„E2E-VGuardæ¡†æ¶æ—¨åœ¨ä¿æŠ¤éŸ³è‰²å’Œå‘éŸ³ï¼Œå¯¹æŠ—æ–°å‹å¨èƒã€‚</li>
<li>E2E-VGuardé‡‡ç”¨ç¼–ç å™¨ç»„åˆã€ç‰¹å¾æå–å’Œå¿ƒç†å£°å­¦æ¨¡å‹ç­‰æŠ€æœ¯æ¥ä¿æŠ¤è¯­éŸ³ã€‚</li>
<li>å¯¹å¤šç§å¼€æºåˆæˆå™¨å’Œå•†ä¸šAPIçš„è¯„ä¼°è¯å®äº†E2E-VGuardçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07099">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1df851e3f56855ab509ba5b887f5dc87" align="middle">
<img src="https://picx.zhimg.com/v2-2d402c2a82596a186957255426ab5c62" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Aligning-Attention-with-Human-Rationales-for-Self-Explaining-Hate-Speech-Detection"><a href="#Aligning-Attention-with-Human-Rationales-for-Self-Explaining-Hate-Speech-Detection" class="headerlink" title="Aligning Attention with Human Rationales for Self-Explaining Hate Speech Detection"></a>Aligning Attention with Human Rationales for Self-Explaining Hate Speech Detection</h2><p><strong>Authors:Brage Eilertsen, RÃ¸skva BjÃ¸rgfinsdÃ³ttir, Francielle Vargas, Ali Ramezani-Kebrya</strong></p>
<p>The opaque nature of deep learning models presents significant challenges for the ethical deployment of hate speech detection systems. To address this limitation, we introduce Supervised Rational Attention (SRA), a framework that explicitly aligns model attention with human rationales, improving both interpretability and fairness in hate speech classification. SRA integrates a supervised attention mechanism into transformer-based classifiers, optimizing a joint objective that combines standard classification loss with an alignment loss term that minimizes the discrepancy between attention weights and human-annotated rationales. We evaluated SRA on hate speech benchmarks in English (HateXplain) and Portuguese (HateBRXplain) with rationale annotations. Empirically, SRA achieves 2.4x better explainability compared to current baselines, and produces token-level explanations that are more faithful and human-aligned. In terms of fairness, SRA achieves competitive fairness across all measures, with second-best performance in detecting toxic posts targeting identity groups, while maintaining comparable results on other metrics. These findings demonstrate that incorporating human rationales into attention mechanisms can enhance interpretability and faithfulness without compromising fairness.</p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ¨¡ç³Šæ€§ä¸ºä»‡æ¨è¨€è®ºæ£€æµ‹ç³»ç»Ÿçš„é“å¾·éƒ¨ç½²å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç›‘ç£ç†æ€§æ³¨æ„åŠ›ï¼ˆSRAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ˜¾å¼åœ°å°†æ¨¡å‹æ³¨æ„åŠ›ä¸äººç±»ç†æ€§å¯¹é½ï¼Œä»è€Œæé«˜äº†ä»‡æ¨è¨€è®ºåˆ†ç±»ä¸­çš„å¯è§£é‡Šæ€§å’Œå…¬å¹³æ€§ã€‚SRAå°†ç›‘ç£æ³¨æ„åŠ›æœºåˆ¶é›†æˆåˆ°åŸºäºå˜å‹å™¨çš„åˆ†ç±»å™¨ä¸­ï¼Œä¼˜åŒ–äº†ä¸€ä¸ªè”åˆç›®æ ‡ï¼Œè¯¥ç›®æ ‡ç»“åˆäº†æ ‡å‡†åˆ†ç±»æŸå¤±ä¸å¯¹é½æŸå¤±é¡¹ï¼Œä»¥æœ€å°åŒ–æ³¨æ„åŠ›æƒé‡ä¸äººç±»æ³¨é‡Šç†ç”±ä¹‹é—´çš„å·®å¼‚ã€‚æˆ‘ä»¬åœ¨è‹±è¯­ï¼ˆHateXplainï¼‰å’Œè‘¡è„ç‰™è¯­ï¼ˆHateBRXplainï¼‰çš„ä»‡æ¨è¨€è®ºåŸºå‡†æµ‹è¯•ä¸Šå¯¹SRAè¿›è¡Œäº†è¯„ä¼°ï¼Œè¿™äº›åŸºå‡†æµ‹è¯•éƒ½æœ‰ç†ç”±æ³¨é‡Šã€‚ä»å®è¯ç»“æœæ¥çœ‹ï¼ŒSRAä¸å½“å‰åŸºçº¿ç›¸æ¯”ï¼Œå¯å®ç°2.4å€çš„å¯è§£é‡Šæ€§æå‡ï¼Œå¹¶äº§ç”Ÿæ›´å¿ å®ã€æ›´ç¬¦åˆäººç±»è§†è§’çš„æ ‡è®°çº§è§£é‡Šã€‚åœ¨å…¬å¹³æ€§æ–¹é¢ï¼ŒSRAåœ¨æ‰€æœ‰è¡¡é‡æ ‡å‡†ä¸Šéƒ½è¡¨ç°å‡ºæœ‰ç«äº‰åŠ›çš„å…¬å¹³æ€§ï¼Œåœ¨æ£€æµ‹é’ˆå¯¹èº«ä»½ç¾¤ä½“çš„æœ‰æ¯’å¸–å­æ—¶è¡¨ç°å±…æ¬¡å¸­ï¼ŒåŒæ—¶åœ¨å…¶ä»–æŒ‡æ ‡ä¸Šä¿æŒç›¸å½“çš„ç»“æœã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå°†äººç±»ç†æ€§çº³å…¥æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥æé«˜å¯è§£é‡Šæ€§å’Œå¿ å®æ€§ï¼Œè€Œä¸æŸå®³å…¬å¹³æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07065v1">PDF</a> Accepted at the Annual AAAI Conference on Artificial Intelligence (AAAI26)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦é’ˆå¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ä»‡æ¨è¨€è®ºæ£€æµ‹ç³»ç»Ÿä¸­çš„ä¸é€æ˜æ€§æ‰€å¸¦æ¥çš„ä¼¦ç†æŒ‘æˆ˜ï¼Œå¼•å…¥äº†ä¸€ç§åä¸ºSupervised Rational Attentionï¼ˆSRAï¼‰çš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ˜ç¡®å°†æ¨¡å‹æ³¨æ„åŠ›ä¸äººç±»ç†æ€§å¯¹é½ï¼Œæé«˜äº†ä»‡æ¨è¨€è®ºåˆ†ç±»ä¸­çš„å¯è§£é‡Šæ€§å’Œå…¬å¹³æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSRAåœ¨è‹±æ–‡ï¼ˆHateXplainï¼‰å’Œè‘¡è„ç‰™è¯­ï¼ˆHateBRXplainï¼‰çš„ä»‡æ¨è¨€è®ºåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†è‰¯å¥½çš„è¡¨ç°ï¼Œå®ç°äº†æ¯”ç°æœ‰åŸºçº¿æ›´é«˜çš„è§£é‡Šæ€§ï¼Œå¹¶äº§ç”Ÿäº†æ›´å¿ å®äºäººç±»ç†è§£çš„è¯çº§è§£é‡Šã€‚åŒæ—¶ï¼ŒSRAåœ¨å…¬å¹³æ€§æ–¹é¢ä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œåœ¨æ£€æµ‹é’ˆå¯¹èº«ä»½ç¾¤ä½“çš„æœ‰æ¯’å¸–å­æ—¶è¡¨ç°ç¬¬äºŒä¼˜ç§€ï¼ŒåŒæ—¶åœ¨å…¶ä»–æŒ‡æ ‡ä¸Šä¿æŒå¯æ¯”æ€§ç»“æœã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå°†äººç±»ç†æ€§èå…¥æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥åœ¨ä¸æŸå®³å…¬å¹³æ€§çš„æƒ…å†µä¸‹æé«˜å¯è§£é‡Šæ€§å’Œå¿ å®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ä¸é€æ˜æ€§åœ¨ä»‡æ¨è¨€è®ºæ£€æµ‹ç³»ç»Ÿçš„ä¼¦ç†éƒ¨ç½²ä¸­å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>SRAæ¡†æ¶é€šè¿‡æ˜ç¡®å¯¹é½æ¨¡å‹æ³¨æ„åŠ›ä¸äººç±»ç†æ€§ï¼Œæé«˜äº†ä»‡æ¨è¨€è®ºåˆ†ç±»ä¸­çš„å¯è§£é‡Šæ€§å’Œå…¬å¹³æ€§ã€‚</li>
<li>SRAæ¡†æ¶é›†æˆäº†ç›‘ç£å¼æ³¨æ„åŠ›æœºåˆ¶åˆ°åŸºäºå˜å‹å™¨çš„åˆ†ç±»å™¨ã€‚</li>
<li>SRAæ¡†æ¶ä¼˜åŒ–äº†ä¸€ä¸ªè”åˆç›®æ ‡ï¼Œè¯¥ç›®æ ‡ç»“åˆäº†æ ‡å‡†åˆ†ç±»æŸå¤±å’Œå¯¹é½æŸå¤±é¡¹ï¼Œä»¥æœ€å°åŒ–æ³¨æ„åŠ›æƒé‡ä¸äººç±»æ³¨é‡Šç†ç”±ä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>åœ¨è‹±æ–‡å’Œè‘¡è„ç‰™è¯­çš„ä»‡æ¨è¨€è®ºåŸºå‡†æµ‹è¯•ä¸Šï¼ŒSRAå®ç°äº†æ¯”ç°æœ‰æ–¹æ³•æ›´å¥½çš„è§£é‡Šæ€§ã€‚</li>
<li>SRAäº§ç”Ÿçš„è¯çº§è§£é‡Šæ›´å¿ å®äºäººç±»ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07065">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d2a50eea7e64cae0ded4eb9ee9597ae3" align="middle">
<img src="https://picx.zhimg.com/v2-bcbaf9b2d6469454357362fe3b189583" align="middle">
<img src="https://picx.zhimg.com/v2-ab2b95b9abf26761538e2179b241e6fd" align="middle">
<img src="https://picx.zhimg.com/v2-8f02c787066af2504a6fcfb7ad9708f7" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="HCFSLN-Adaptive-Hyperbolic-Few-Shot-Learning-for-Multimodal-Anxiety-Detection"><a href="#HCFSLN-Adaptive-Hyperbolic-Few-Shot-Learning-for-Multimodal-Anxiety-Detection" class="headerlink" title="HCFSLN: Adaptive Hyperbolic Few-Shot Learning for Multimodal Anxiety Detection"></a>HCFSLN: Adaptive Hyperbolic Few-Shot Learning for Multimodal Anxiety Detection</h2><p><strong>Authors:Aditya Sneh, Nilesh Kumar Sahu, Anushka Sanjay Shelke, Arya Adyasha, Haroon R. Lone</strong></p>
<p>Anxiety disorders impact millions globally, yet traditional diagnosis relies on clinical interviews, while machine learning models struggle with overfitting due to limited data. Large-scale data collection remains costly and time-consuming, restricting accessibility. To address this, we introduce the Hyperbolic Curvature Few-Shot Learning Network (HCFSLN), a novel Few-Shot Learning (FSL) framework for multimodal anxiety detection, integrating speech, physiological signals, and video data. HCFSLN enhances feature separability through hyperbolic embeddings, cross-modal attention, and an adaptive gating network, enabling robust classification with minimal data. We collected a multimodal anxiety dataset from 108 participants and benchmarked HCFSLN against six FSL baselines, achieving 88% accuracy, outperforming the best baseline by 14%. These results highlight the effectiveness of hyperbolic space for modeling anxiety-related speech patterns and demonstrate FSLâ€™s potential for anxiety classification.</p>
<blockquote>
<p>ç„¦è™‘éšœç¢å½±å“äº†å…¨çƒæ•°ç™¾ä¸‡äººï¼Œç„¶è€Œä¼ ç»Ÿè¯Šæ–­ä¾èµ–äºä¸´åºŠè®¿è°ˆï¼Œè€Œæœºå™¨å­¦ä¹ æ¨¡å‹ç”±äºæ•°æ®æœ‰é™è€Œé¢ä¸´è¿‡æ‹Ÿåˆçš„é—®é¢˜ã€‚å¤§è§„æ¨¡æ•°æ®é‡‡é›†ä»ç„¶æˆæœ¬é«˜æ˜‚ã€è€—æ—¶é•¿ï¼Œé™åˆ¶äº†å¯è®¿é—®æ€§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŒæ›²æ›²é¢å°‘é‡æ ·æœ¬å­¦ä¹ ç½‘ç»œï¼ˆHyperbolic Curvature Few-Shot Learning Networkï¼Œç®€ç§°HCFSLNï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šæ¨¡å¼ç„¦è™‘æ£€æµ‹çš„æ–°å‹å°‘é‡æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰æ¡†æ¶ï¼Œèåˆäº†è¯­éŸ³ã€ç”Ÿç†ä¿¡å·å’Œè§†é¢‘æ•°æ®ã€‚HCFSLNé€šè¿‡åŒæ›²åµŒå…¥ã€è·¨æ¨¡å¼æ³¨æ„åŠ›å’Œè‡ªé€‚åº”é—¨æ§ç½‘ç»œå¢å¼ºç‰¹å¾å¯åˆ†æ€§ï¼Œç”¨æœ€å°‘çš„æ•°æ®å®ç°ç¨³å¥çš„åˆ†ç±»ã€‚æˆ‘ä»¬ä»108åå‚ä¸è€…ä¸­æ”¶é›†äº†å¤šæ¨¡å¼ç„¦è™‘æ•°æ®é›†ï¼Œå¹¶å°†HCFSLNä¸å…­ç§FSLåŸºå‡†è¿›è¡Œäº†è¯„ä¼°ï¼Œå‡†ç¡®ç‡è¾¾åˆ°88%ï¼Œæ¯”æœ€ä½³åŸºå‡†é«˜å‡º14%ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åŒæ›²ç©ºé—´åœ¨æ¨¡æ‹Ÿç„¦è™‘ç›¸å…³è¯­éŸ³æ¨¡å¼æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†FSLåœ¨ç„¦è™‘åˆ†ç±»æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06988v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºHyperbolic Curvature Few-Shot Learning Networkï¼ˆHCFSLNï¼‰çš„æ–°å‹Few-Shot Learningï¼ˆFSLï¼‰æ¡†æ¶ï¼Œç”¨äºå¤šæ¨¡æ€ç„¦è™‘æ£€æµ‹ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è¯­éŸ³ã€ç”Ÿç†ä¿¡å·å’Œè§†é¢‘æ•°æ®ï¼Œé€šè¿‡è¶…æ›²é¢åµŒå…¥ã€è·¨æ¨¡æ€æ³¨æ„åŠ›å’Œè‡ªé€‚åº”é—¨æ§ç½‘ç»œç­‰æŠ€æœ¯æé«˜ç‰¹å¾å¯åˆ†æ€§ï¼Œåœ¨æœ‰é™æ•°æ®ä¸‹å®ç°ç¨³å¥åˆ†ç±»ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHCFSLNåœ¨ç„¦è™‘åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†88%ï¼Œå¹¶è¾ƒæœ€ä½³åŸºçº¿æé«˜äº†14%ã€‚è¿™è¡¨æ˜è¶…æ›²é¢ç©ºé—´åœ¨å»ºæ¨¡ç„¦è™‘ç›¸å…³è¯­éŸ³æ¨¡å¼æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠFSLåœ¨ç„¦è™‘åˆ†ç±»æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç„¦è™‘éšœç¢å½±å“å…¨çƒæ•°ç™¾ä¸‡äººï¼Œä½†ä¼ ç»Ÿè¯Šæ–­æ–¹æ³•ä¾èµ–äºä¸´åºŠè®¿è°ˆï¼Œå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨ç„¦è™‘éšœç¢è¯Šæ–­æ–¹é¢å­˜åœ¨è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œä¸”å¤§è§„æ¨¡æ•°æ®é‡‡é›†æˆæœ¬é«˜æ˜‚ã€è€—æ—¶ã€‚</li>
<li>HCFSLNæ˜¯ä¸€ç§æ–°å‹çš„FSLæ¡†æ¶ï¼Œç”¨äºå¤šæ¨¡æ€ç„¦è™‘æ£€æµ‹ï¼Œé›†æˆäº†è¯­éŸ³ã€ç”Ÿç†ä¿¡å·å’Œè§†é¢‘æ•°æ®ã€‚</li>
<li>HCFSLNé€šè¿‡è¶…æ›²é¢åµŒå…¥ã€è·¨æ¨¡æ€æ³¨æ„åŠ›å’Œè‡ªé€‚åº”é—¨æ§ç½‘ç»œç­‰æŠ€æœ¯æé«˜ç‰¹å¾å¯åˆ†æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒHCFSLNåœ¨ç„¦è™‘åˆ†ç±»ä»»åŠ¡ä¸­å‡†ç¡®ç‡è¾¾åˆ°äº†88%ï¼Œè¾ƒæœ€ä½³åŸºçº¿æé«˜äº†14%ã€‚</li>
<li>è¶…æ›²é¢ç©ºé—´åœ¨å»ºæ¨¡ç„¦è™‘ç›¸å…³è¯­éŸ³æ¨¡å¼æ–¹é¢è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-367505c6f38ab0e02d84e73fa621bf53" align="middle">
<img src="https://picx.zhimg.com/v2-2f5438c19baa77fc270b7e3ca910dba8" align="middle">
<img src="https://picx.zhimg.com/v2-c0f686ccf84b7d34ed0073948b16015a" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MedVoiceBias-A-Controlled-Study-of-Audio-LLM-Behavior-in-Clinical-Decision-Making"><a href="#MedVoiceBias-A-Controlled-Study-of-Audio-LLM-Behavior-in-Clinical-Decision-Making" class="headerlink" title="MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making"></a>MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making</h2><p><strong>Authors:Zhi Rui Tam, Yun-Nung Chen</strong></p>
<p>As large language models transition from text-based interfaces to audio interactions in clinical settings, they might introduce new vulnerabilities through paralinguistic cues in audio. We evaluated these models on 170 clinical cases, each synthesized into speech from 36 distinct voice profiles spanning variations in age, gender, and emotion. Our findings reveal a severe modality bias: surgical recommendations for audio inputs varied by as much as 35% compared to identical text-based inputs, with one model providing 80% fewer recommendations. Further analysis uncovered age disparities of up to 12% between young and elderly voices, which persisted in most models despite chain-of-thought prompting. While explicit reasoning successfully eliminated gender bias, the impact of emotion was not detected due to poor recognition performance. These results demonstrate that audio LLMs are susceptible to making clinical decisions based on a patientâ€™s voice characteristics rather than medical evidence, a flaw that risks perpetuating healthcare disparities. We conclude that bias-aware architectures are essential and urgently needed before the clinical deployment of these models.</p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ä»åŸºäºæ–‡æœ¬çš„ç”¨æˆ·ç•Œé¢è¿‡æ¸¡åˆ°ä¸´åºŠç¯å¢ƒä¸­çš„è¯­éŸ³äº¤äº’ï¼Œå®ƒä»¬å¯èƒ½ä¼šé€šè¿‡è¯­éŸ³ä¸­çš„å‰¯è¯­è¨€çº¿ç´¢å¼•å…¥æ–°çš„æ¼æ´ã€‚æˆ‘ä»¬åœ¨170ä¸ªä¸´åºŠç—…ä¾‹ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œæ¯ä¸ªç—…ä¾‹éƒ½ä»æ¶µç›–å¹´é¾„ã€æ€§åˆ«å’Œæƒ…æ„Ÿå˜åŒ–çš„36ä¸ªä¸åŒçš„è¯­éŸ³ç‰¹å¾ä¸­åˆæˆè¯­éŸ³ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°äº†ä¸€ä¸ªä¸¥é‡çš„æ¨¡å¼åè§ï¼šä¸åŸºäºæ–‡æœ¬çš„è¾“å…¥ç›¸æ¯”ï¼ŒéŸ³é¢‘è¾“å…¥çš„æ‰‹æœ¯å»ºè®®å·®å¼‚é«˜è¾¾35%ï¼Œå…¶ä¸­ä¸€ä¸ªæ¨¡å‹æä¾›çš„å»ºè®®å‡å°‘äº†80%ã€‚è¿›ä¸€æ­¥çš„åˆ†æå‘ç°ï¼Œå¹´è½»å’Œè€å¹´å£°éŸ³ä¹‹é—´çš„å·®å¼‚é«˜è¾¾12%ï¼Œè¿™åœ¨å¤§å¤šæ•°æ¨¡å‹ä¸­ä»ç„¶å­˜åœ¨ï¼Œå°½ç®¡é‡‡ç”¨äº†é“¾å¼æ€ç»´æç¤ºã€‚è™½ç„¶æ˜ç¡®çš„æ¨ç†æˆåŠŸåœ°æ¶ˆé™¤äº†æ€§åˆ«åè§ï¼Œä½†ç”±äºè¯†åˆ«æ€§èƒ½ä¸ä½³ï¼Œæƒ…ç»ªçš„å½±å“å¹¶æœªè¢«æ£€æµ‹åˆ°ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒéŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹å®¹æ˜“æ ¹æ®æ‚£è€…çš„è¯­éŸ³ç‰¹å¾è€Œä¸æ˜¯åŒ»å­¦è¯æ®åšå‡ºä¸´åºŠå†³ç­–ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯èƒ½ä½¿åŒ»ç–—ä¿å¥å·®å¼‚æŒç»­å­˜åœ¨çš„ç¼ºé™·ã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œåœ¨ä¸´åºŠéƒ¨ç½²è¿™äº›æ¨¡å‹ä¹‹å‰ï¼Œé‡‡ç”¨åè§æ„ŸçŸ¥æ¶æ„æ˜¯è‡³å…³é‡è¦ä¸”ååˆ†å¿…è¦çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06592v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯­éŸ³äº¤äº’åœ¨ä¸´åºŠç¯å¢ƒä¸­å¼•å…¥æ–°çš„å®‰å…¨éšæ‚£ã€‚å¯¹è¯­éŸ³æ¨¡å‹è¿›è¡Œä¸´åºŠè¯•éªŒè¯„ä¼°ï¼Œå‘ç°ä¸åŒå£°éŸ³æ¨¡æ€å¯èƒ½å½±å“åŒ»ç–—å†³ç­–å»ºè®®ã€‚å¹´é¾„ã€æ€§åˆ«å·®å¼‚åœ¨æ¨¡å‹ä¸­å°¤ä¸ºæ˜¾è‘—ï¼Œæ€§åˆ«å½±å“å°¤ä¸ºæ˜æ˜¾ã€‚æ¨¡å‹æ ¹æ®æ‚£è€…å£°éŸ³ç‰¹æ€§è€ŒéåŒ»å­¦è¯æ®åšå‡ºå†³ç­–çš„é£é™©å¯èƒ½å¯¼è‡´åŒ»ç–—ä¸å¹³ç­‰ã€‚äºŸéœ€å¼€å‘å…·æœ‰åè§æ„è¯†çš„æ¶æ„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd221133846182128b8cac8c3136b3f5" align="middle">
<img src="https://picx.zhimg.com/v2-92b6a70905bf3721516175b0bf31574f" align="middle">
<img src="https://picx.zhimg.com/v2-aed14b5e5e571a3aa78f022be73a1a05" align="middle">
<img src="https://picx.zhimg.com/v2-f3630edcfeaa637ccb0b6fd691a5f5fe" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MT-HuBERT-Self-Supervised-Mix-Training-for-Few-Shot-Keyword-Spotting-in-Mixed-Speech"><a href="#MT-HuBERT-Self-Supervised-Mix-Training-for-Few-Shot-Keyword-Spotting-in-Mixed-Speech" class="headerlink" title="MT-HuBERT: Self-Supervised Mix-Training for Few-Shot Keyword Spotting in Mixed Speech"></a>MT-HuBERT: Self-Supervised Mix-Training for Few-Shot Keyword Spotting in Mixed Speech</h2><p><strong>Authors:Junming Yuan, Ying Shi, Dong Wang, Lantian Li, Askar Hamdulla</strong></p>
<p>Few-shot keyword spotting aims to detect previously unseen keywords with very limited labeled samples. A pre-training and adaptation paradigm is typically adopted for this task. While effective in clean conditions, most existing approaches struggle with mixed keyword spottingâ€“detecting multiple overlapping keywords within a single utteranceâ€“a capability essential for real-world applications. We have previously proposed a pre-training approach based on Mix-Training (MT) to tackle the mixed keyword detection problem and demonstrated its efficiency. However, this approach is fully supervised, unable to utilize vast unlabeled data. To this end, we propose Mix-Training HuBERT (MT-HuBERT), a self-supervised learning (SSL) pre-training framework that implements the MT criterion during pre-training. MT-HuBERT predicts, in a self-supervised manner, the clean acoustic units of each constituent signal from contextual cues, in contrast to predicting compositional patterns of mixed speech. Experiments conducted on the Google Speech Commands (GSC v2) corpus demonstrate that our proposed MT-HuBERT consistently outperforms several state-of-the-art baselines in few-shot KWS tasks under both mixed and clean conditions.</p>
<blockquote>
<p>å°‘æ•°å…³é”®è¯è¯†åˆ«æ—¨åœ¨ä½¿ç”¨éå¸¸æœ‰é™çš„æ ‡è®°æ ·æœ¬æ£€æµ‹å…ˆå‰æœªè§çš„å…³é”®è¯ã€‚é€šå¸¸ä¸ºæ­¤ä»»åŠ¡é‡‡ç”¨é¢„è®­ç»ƒå’Œé€‚åº”èŒƒå¼ã€‚å°½ç®¡åœ¨æ¸…æ´æ¡ä»¶ä¸‹æœ‰æ•ˆï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•åœ¨å¤„ç†æ··åˆå…³é”®è¯è¯†åˆ«æ–¹é¢å­˜åœ¨å›°éš¾â€”â€”åœ¨åŒä¸€å¥è¯å†…æ£€æµ‹å¤šä¸ªé‡å å…³é”®è¯â€”â€”è¿™æ˜¯ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„åŸºæœ¬èƒ½åŠ›ã€‚æˆ‘ä»¬ä¹‹å‰æå‡ºäº†åŸºäºæ··åˆè®­ç»ƒï¼ˆMTï¼‰çš„é¢„è®­ç»ƒæ–¹æ³•æ¥è§£å†³æ··åˆå…³é”®è¯æ£€æµ‹é—®é¢˜ï¼Œå¹¶è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•æ˜¯å®Œå…¨ç›‘ç£çš„ï¼Œæ— æ³•åˆ©ç”¨å¤§é‡æœªæ ‡è®°æ•°æ®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ··åˆè®­ç»ƒHuBERTï¼ˆMT-HuBERTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰é¢„è®­ç»ƒæ¡†æ¶ï¼Œåœ¨é¢„è®­ç»ƒæœŸé—´å®ç°äº†MTæ ‡å‡†ã€‚MT-HuBERTä»¥è‡ªç›‘ç£çš„æ–¹å¼é¢„æµ‹æ¯ä¸ªç»„æˆä¿¡å·çš„æ¸…æ´å£°å­¦å•å…ƒï¼Œè€Œä¸æ˜¯é¢„æµ‹æ··åˆè¯­éŸ³çš„ç»„åˆæ¨¡å¼ã€‚åœ¨Googleè¯­éŸ³å‘½ä»¤ï¼ˆGSC v2ï¼‰è¯­æ–™åº“ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„MT-HuBERTåœ¨æ··åˆå’Œæ¸…æ´æ¡ä»¶ä¸‹å‡ä¼˜äºå°‘æ•°å…³é”®è¯è¯†åˆ«ä»»åŠ¡ä¸­çš„å‡ ç§æœ€æ–°åŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06296v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹å°‘æ ·æœ¬å…³é”®è¯è¯†åˆ«ä»»åŠ¡çš„æ··åˆè®­ç»ƒHuBERTï¼ˆMT-HuBERTï¼‰æ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰é¢„è®­ç»ƒæ¡†æ¶ï¼Œå®ç°äº†æ··åˆè®­ç»ƒå‡†åˆ™ä»¥å¤„ç†æ··åˆå…³é”®è¯æ£€æµ‹é—®é¢˜ã€‚MT-HuBERTé€šè¿‡ä¸Šä¸‹æ–‡çº¿ç´¢ä»¥è‡ªç›‘ç£çš„æ–¹å¼é¢„æµ‹å„ä¸ªç»„æˆéƒ¨åˆ†ä¿¡å·çš„çº¯å‡€å£°å­¦å•å…ƒï¼Œè€Œéé¢„æµ‹æ··åˆè¯­éŸ³çš„ç»„åˆæ¨¡å¼ã€‚å®éªŒè¯æ˜ï¼Œåœ¨Googleè¯­éŸ³å‘½ä»¤ï¼ˆGSC v2ï¼‰è¯­æ–™åº“ä¸Šï¼ŒMT-HuBERTåœ¨æ··åˆå’Œæ¸…æ´æ¡ä»¶ä¸‹å‡ä¼˜äºå¤šä¸ªå‰æ²¿åŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ ·æœ¬å…³é”®è¯è¯†åˆ«æ—¨åœ¨æ£€æµ‹å…ˆå‰æœªè§è¿‡çš„å…³é”®è¯ï¼Œä¸”åªæœ‰éå¸¸æœ‰é™çš„æ ‡è®°æ ·æœ¬å¯ç”¨ã€‚</li>
<li>é¢„è®­ç»ƒå’Œé€‚åº”èŒƒå¼é€šå¸¸ç”¨äºæ­¤ä»»åŠ¡ï¼Œåœ¨æ¸…æ´æ¡ä»¶ä¸‹æœ‰æ•ˆï¼Œä½†åœ¨æ··åˆå…³é”®è¯æ£€æµ‹æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æ··åˆè®­ç»ƒï¼ˆMTï¼‰æ–¹æ³•è¢«ç”¨æ¥è§£å†³æ··åˆå…³é”®è¯æ£€æµ‹é—®é¢˜ï¼Œä½†å®ƒæ˜¯å…¨ç›‘ç£çš„ï¼Œæ— æ³•åˆ©ç”¨å¤§é‡æœªæ ‡è®°æ•°æ®ã€‚</li>
<li>æå‡ºäº†æ··åˆè®­ç»ƒHuBERTï¼ˆMT-HuBERTï¼‰æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œå®ç°äº†MTå‡†åˆ™ã€‚</li>
<li>MT-HuBERTä»¥è‡ªç›‘ç£çš„æ–¹å¼é¢„æµ‹å„ä¸ªç»„æˆéƒ¨åˆ†ä¿¡å·çš„çº¯å‡€å£°å­¦å•å…ƒã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œåœ¨Googleè¯­éŸ³å‘½ä»¤ï¼ˆGSC v2ï¼‰è¯­æ–™åº“ä¸Šï¼ŒMT-HuBERTåœ¨æ··åˆå’Œæ¸…æ´æ¡ä»¶ä¸‹å‡ä¼˜äºå¤šä¸ªæœ€æ–°æŠ€æœ¯æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06296">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-83df34a513e6188526186b83750465e9" align="middle">
<img src="https://picx.zhimg.com/v2-6c6f6a22b00cdcb6f75a4620da062b0e" align="middle">
<img src="https://picx.zhimg.com/v2-63f6eaebbbbcfa5a38a5035b7feeda4e" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="IDMap-A-Pseudo-Speaker-Generator-Framework-Based-on-Speaker-Identity-Index-to-Vector-Mapping"><a href="#IDMap-A-Pseudo-Speaker-Generator-Framework-Based-on-Speaker-Identity-Index-to-Vector-Mapping" class="headerlink" title="IDMap: A Pseudo-Speaker Generator Framework Based on Speaker Identity Index to Vector Mapping"></a>IDMap: A Pseudo-Speaker Generator Framework Based on Speaker Identity Index to Vector Mapping</h2><p><strong>Authors:Zeyan Liu, Liping Chen, Kong Aik Lee, Zhenhua Ling</strong></p>
<p>Facilitated by the speech generation framework that disentangles speech into content, speaker, and prosody, voice anonymization is accomplished by substituting the original speaker embedding vector with that of a pseudo-speaker. In this framework, the pseudo-speaker generation forms a fundamental challenge. Current pseudo-speaker generation methods demonstrate limitations in the uniqueness of pseudo-speakers, consequently restricting their effectiveness in voice privacy protection. Besides, existing model-based methods suffer from heavy computation costs. Especially, in the large-scale scenario where a huge number of pseudo-speakers are generated, the limitations of uniqueness and computational inefficiency become more significant. To this end, this paper proposes a framework for pseudo-speaker generation, which establishes a mapping from speaker identity index to speaker vector in the feedforward architecture, termed IDMap. Specifically, the framework is specified into two models: IDMap-MLP and IDMap-Diff. Experiments were conducted on both small- and large-scale evaluation datasets. Small-scale evaluations on the LibriSpeech dataset validated the effectiveness of the proposed IDMap framework in enhancing the uniqueness of pseudo-speakers, thereby improving voice privacy protection, while at a reduced computational cost. Large-scale evaluations on the MLS and Common Voice datasets further justified the superiority of the IDMap framework regarding the stability of the voice privacy protection capability as the number of pseudo-speakers increased. Audio samples and open-source code can be found in <a target="_blank" rel="noopener" href="https://github.com/VoicePrivacy/IDMap">https://github.com/VoicePrivacy/IDMap</a>.</p>
<blockquote>
<p>å€ŸåŠ©å°†è¯­éŸ³åˆ†è§£ä¸ºå†…å®¹ã€è¯´è¯è€…å’ŒéŸµå¾‹çš„è¯­éŸ³ç”Ÿæˆæ¡†æ¶ï¼Œè¯­éŸ³åŒ¿ååŒ–æ˜¯é€šè¿‡ç”¨ä¼ªè¯´è¯è€…çš„åµŒå…¥å‘é‡æ›¿æ¢åŸå§‹è¯´è¯è€…çš„åµŒå…¥å‘é‡æ¥å®ç°çš„ã€‚åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼Œä¼ªè¯´è¯è€…çš„ç”Ÿæˆæ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ã€‚ç°æœ‰çš„ä¼ªè¯´è¯è€…ç”Ÿæˆæ–¹æ³•åœ¨ä¼ªè¯´è¯è€…çš„å”¯ä¸€æ€§æ–¹é¢æ˜¾ç¤ºå‡ºå±€é™æ€§ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬åœ¨ä¿æŠ¤è¯­éŸ³éšç§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼ŒåŸºäºç°æœ‰æ¨¡å‹çš„æ–¹æ³•è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚å°¤å…¶åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸‹ç”Ÿæˆå¤§é‡ä¼ªè¯´è¯è€…æ—¶ï¼Œå”¯ä¸€æ€§å’Œè®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜å˜å¾—æ›´åŠ æ˜¾è‘—ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä¼ªè¯´è¯è€…ç”Ÿæˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨å‰é¦ˆæ¶æ„ä¸­å»ºç«‹äº†ä»è¯´è¯äººèº«ä»½æŒ‡æ•°åˆ°è¯´è¯äººå‘é‡çš„æ˜ å°„ï¼Œç§°ä¸ºIDMapã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªæ¨¡å‹ï¼šIDMap-MLPå’ŒIDMap-Diffã€‚å®éªŒæ˜¯åœ¨å°å‹å’Œå¤§å‹è¯„ä¼°æ•°æ®é›†ä¸Šè¿›è¡Œçš„ã€‚åœ¨LibriSpeechæ•°æ®é›†ä¸Šçš„å°å‹è§„æ¨¡è¯„ä¼°éªŒè¯äº†æ‰€æIDMapæ¡†æ¶åœ¨æé«˜ä¼ªè¯´è¯è€…çš„å”¯ä¸€æ€§ã€æé«˜è¯­éŸ³éšç§ä¿æŠ¤èƒ½åŠ›çš„åŒæ—¶ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬çš„æœ‰æ•ˆæ€§ã€‚åœ¨MLSå’ŒCommon Voiceæ•°æ®é›†ä¸Šçš„å¤§è§„æ¨¡è¯„ä¼°è¿›ä¸€æ­¥è¯æ˜äº†IDMapæ¡†æ¶åœ¨éšç€ä¼ªè¯´è¯è€…æ•°é‡å¢åŠ æ—¶ï¼Œå…¶è¯­éŸ³éšç§ä¿æŠ¤èƒ½åŠ›çš„ç¨³å®šæ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚éŸ³é¢‘æ ·æœ¬å’Œå¼€æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/VoicePrivacy/IDMap%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/VoicePrivacy/IDMapæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06246v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¯­éŸ³åŒ¿ååŒ–é€šè¿‡æ›¿ä»£åŸè¯´è¯è€…çš„åµŒå…¥å‘é‡ä»¥å®ç°ã€‚è¯¥æ¡†æ¶æ¶‰åŠä¸€ä¸ªé‡å¤§çš„æŒ‘æˆ˜â€”â€”ä¼ªè¯´è¯è€…ç”Ÿæˆï¼Œå®ƒå¯¹å¤§è§„æ¨¡åœºæ™¯ä¸­æ•°ä»¥ä¸‡è®¡çš„ä¼ªè¯´è¯è€…ç”Ÿæˆè¡¨ç°å‡ºç‹¬ç‰¹æ€§ä¸Šçš„å±€é™å’Œè®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†IDMapæ¡†æ¶ï¼Œå»ºç«‹è¯´è¯è€…èº«ä»½ç´¢å¼•åˆ°è¯´è¯è€…å‘é‡çš„æ˜ å°„å…³ç³»ã€‚å®éªŒè¯æ˜ï¼ŒIDMapæ¡†æ¶åœ¨å°è§„æ¨¡æ•°æ®é›†ä¸Šæé«˜äº†ä¼ªè¯´è¯è€…çš„ç‹¬ç‰¹æ€§ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶è¯­éŸ³éšç§ä¿æŠ¤èƒ½åŠ›çš„ç¨³å®šæ€§ã€‚å…·ä½“è¯¦æƒ…å¯å‚è§ç›¸å…³å¼€æºä»£ç åº“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯­éŸ³åŒ¿ååŒ–æ˜¯é€šè¿‡æ›¿ä»£åŸè¯´è¯è€…çš„åµŒå…¥å‘é‡å®ç°ã€‚</li>
<li>ä¼ªè¯´è¯è€…ç”Ÿæˆæ˜¯è¯­éŸ³åŒ¿ååŒ–çš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨å±€é™æ€§å’Œè®¡ç®—æˆæœ¬é—®é¢˜ã€‚</li>
<li>IDMapæ¡†æ¶é€šè¿‡å»ºç«‹è¯´è¯è€…èº«ä»½ç´¢å¼•ä¸è¯´è¯è€…å‘é‡çš„æ˜ å°„å…³ç³»æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>IDMapæ¡†æ¶åˆ†ä¸ºIDMap-MLPå’ŒIDMap-Diffä¸¤ä¸ªæ¨¡å‹ã€‚</li>
<li>å°è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†IDMapæ¡†æ¶åœ¨æé«˜ä¼ªè¯´è¯è€…çš„ç‹¬ç‰¹æ€§å’Œé™ä½è®¡ç®—æˆæœ¬æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06246">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9728e51a88fcd7047cdee68057004471" align="middle">
<img src="https://picx.zhimg.com/v2-5111599c400a4a529e0b0a972397af72" align="middle">
<img src="https://picx.zhimg.com/v2-b763a6bdc722c523621bb57104a9eb0e" align="middle">
<img src="https://picx.zhimg.com/v2-94f420ebbca02daacc80ff228a134f9e" align="middle">
<img src="https://picx.zhimg.com/v2-31abeecbb447ea539d20980e8d426d81" align="middle">
<img src="https://picx.zhimg.com/v2-f7ed537e40542f19f5dfc019c1db01f5" align="middle">
<img src="https://picx.zhimg.com/v2-bfc18a5333ae69ed767cc6273384b4d9" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Augmenting-Open-Vocabulary-Dysarthric-Speech-Assessment-with-Human-Perceptual-Supervision"><a href="#Augmenting-Open-Vocabulary-Dysarthric-Speech-Assessment-with-Human-Perceptual-Supervision" class="headerlink" title="Augmenting Open-Vocabulary Dysarthric Speech Assessment with Human Perceptual Supervision"></a>Augmenting Open-Vocabulary Dysarthric Speech Assessment with Human Perceptual Supervision</h2><p><strong>Authors:Kaimeng Jia, Minzhu Tu, Zengrui Jin, Siyin Wang, Chao Zhang</strong></p>
<p>Dysarthria is a speech disorder characterized by impaired intelligibility and reduced communicative effectiveness. Automatic dysarthria assessment provides a scalable, cost-effective approach for supporting the diagnosis and treatment of neurological conditions such as Parkinsonâ€™s disease, Alzheimerâ€™s disease, and stroke. This study investigates leveraging human perceptual annotations from speech synthesis assessment as reliable out-of-domain knowledge for dysarthric speech assessment. Experimental results suggest that such supervision can yield consistent and substantial performance improvements in self-supervised learning pre-trained models. These findings suggest that perceptual ratings aligned with human judgments from speech synthesis evaluations represent valuable resources for dysarthric speech modeling, enabling effective cross-domain knowledge transfer.</p>
<blockquote>
<p>æ„éŸ³éšœç¢æ˜¯ä¸€ç§è¡¨ç°ä¸ºå‘éŸ³ä¸æ¸…å’Œæ²Ÿé€šæ•ˆç‡é™ä½çš„è¨€è¯­éšœç¢ã€‚è‡ªåŠ¨æ„éŸ³éšœç¢è¯„ä¼°ä¸ºå¸•é‡‘æ£®ç—…ã€é˜¿å°”èŒ¨æµ·é»˜ç—‡å’Œä¸­é£ç­‰ç¥ç»æ€§ç–¾ç—…çš„è¯Šæ–­å’Œæ²»ç–—æä¾›äº†ä¸€ç§å¯æ‰©å±•ã€ç»æµé«˜æ•ˆçš„è¾…åŠ©æ–¹æ³•ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨è¯­éŸ³åˆæˆè¯„ä¼°ä¸­çš„äººæ„ŸçŸ¥æ³¨é‡Šä½œä¸ºå¯é çš„é¢†åŸŸå¤–çŸ¥è¯†ï¼Œç”¨äºæ„éŸ³éšœç¢è¯­éŸ³è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§ç›‘ç£å¯ä»¥ä¸ºè‡ªç›‘ç£å­¦ä¹ é¢„è®­ç»ƒæ¨¡å‹å¸¦æ¥ä¸€è‡´ä¸”æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œä¸è¯­éŸ³åˆæˆè¯„ä¼°ä¸­äººç±»åˆ¤æ–­ç›¸ä¸€è‡´çš„æ„ŸçŸ¥è¯„åˆ†ä»£è¡¨ç€æ„éŸ³éšœç¢è¯­éŸ³å»ºæ¨¡çš„å®è´µèµ„æºï¼Œèƒ½å¤Ÿå®ç°æœ‰æ•ˆçš„è·¨åŸŸçŸ¥è¯†è¿ç§»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.02270v1">PDF</a> Submission of IEEE ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•åˆ©ç”¨è¯­éŸ³åˆæˆè¯„ä¼°ä¸­çš„äººæ„ŸçŸ¥æ³¨é‡Šä½œä¸ºå¯é çš„è·¨åŸŸçŸ¥è¯†ï¼Œä»¥æ”¯æŒå¯¹å‘éŸ³éšœç¢çš„è‡ªåŠ¨è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§ç›‘ç£å¯ä»¥ä¸ºé¢„è®­ç»ƒæ¨¡å‹å¸¦æ¥ä¸€è‡´ä¸”æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å› æ­¤ï¼Œä¸å‘éŸ³éšœç¢è¯­éŸ³å»ºæ¨¡ç›¸åŒ¹é…çš„æ„ŸçŸ¥è¯„åˆ†ä»£è¡¨äº†å®è´µçš„èµ„æºï¼Œå¯å®ç°æœ‰æ•ˆçš„è·¨åŸŸçŸ¥è¯†è¿ç§»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‘éŸ³éšœç¢æ˜¯ä¸€ç§è¡¨ç°ä¸ºè¯­éŸ³æ¸…æ™°åº¦å—æŸå’Œæ²Ÿé€šæ•ˆæœé™ä½çš„è¨€è¯­éšœç¢ã€‚</li>
<li>è‡ªåŠ¨å‘éŸ³éšœç¢è¯„ä¼°æ˜¯æ”¯æŒå¸•é‡‘æ£®ç—…ã€é˜¿å°”èŒ¨æµ·é»˜ç—‡å’Œè„‘å’ä¸­ç­‰ç¥ç»æ€§ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—çš„ä¸€ç§å¯æ‰©å±•ä¸”ç»æµé«˜æ•ˆçš„æ–¹æ³•ã€‚</li>
<li>åˆ©ç”¨è¯­éŸ³åˆæˆè¯„ä¼°ä¸­çš„äººæ„ŸçŸ¥æ³¨é‡Šå¯ä½œä¸ºå¯é çš„è·¨åŸŸçŸ¥è¯†ï¼Œç”¨äºæ”¯æŒå‘éŸ³éšœç¢çš„è¯­éŸ³è¯„ä¼°ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œç›‘ç£å­¦ä¹ å¯ä»¥æé«˜é¢„è®­ç»ƒæ¨¡å‹åœ¨å‘éŸ³éšœç¢è¯„ä¼°ä¸­çš„æ€§èƒ½ã€‚</li>
<li>æ„ŸçŸ¥è¯„åˆ†ä¸è¯­éŸ³åˆæˆè¯„ä¼°ä¸­çš„äººç±»åˆ¤æ–­ç›¸ç¬¦ï¼Œä¸ºå‘éŸ³éšœç¢è¯­éŸ³å»ºæ¨¡æä¾›äº†æœ‰ä»·å€¼çš„èµ„æºã€‚</li>
<li>é€šè¿‡åˆ©ç”¨è¿™äº›èµ„æºï¼Œå¯ä»¥å®ç°æœ‰æ•ˆçš„è·¨åŸŸçŸ¥è¯†è¿ç§»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02270">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-beba6e7af41e6ca688456edbe22c9c7f" align="middle">
<img src="https://picx.zhimg.com/v2-ca8562f17cd04e9ebda18f1cfdaaad9f" align="middle">
<img src="https://picx.zhimg.com/v2-b55a577ae182281962a291efdd316611" align="middle">
<img src="https://picx.zhimg.com/v2-e84e07b29d3968b5c85d75650ae9790d" align="middle">
<img src="https://picx.zhimg.com/v2-bf59c6a32b9e5aad9e7512553f1035bc" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="SPEAR-A-Unified-SSL-Framework-for-Learning-Speech-and-Audio-Representations"><a href="#SPEAR-A-Unified-SSL-Framework-for-Learning-Speech-and-Audio-Representations" class="headerlink" title="SPEAR: A Unified SSL Framework for Learning Speech and Audio Representations"></a>SPEAR: A Unified SSL Framework for Learning Speech and Audio Representations</h2><p><strong>Authors:Xiaoyu Yang, Yifan Yang, Zengrui Jin, Ziyun Cui, Wen Wu, Baoxiang Li, Chao Zhang, Phil Woodland</strong></p>
<p>Self-Supervised Learning (SSL) excels at learning generic representations of acoustic signals, yet prevailing methods remain domain-specific, tailored to either speech or general audio, hindering the development of a unified representation model with a comprehensive capability over both domains. To address this, we present SPEAR (SPEech and Audio Representations), the first SSL framework to successfully learn unified speech and audio representations from a mixture of speech and audio data. SPEAR proposes a unified pre-training objective based on masked prediction of fine-grained discrete tokens for both speech and general audio. These tokens are derived from continuous speech and audio representations using a Multi-codebook Vector Quantisation (MVQ) method, retaining rich acoustic detail essential for modelling both speech and complex audio events. SPEAR is applied to pre-train both single-domain and unified speech-and-audio SSL models. Our speech-domain model establishes a new state-of-the-art on the SUPERB benchmark, a speech processing benchmark for SSL models, matching or surpassing the highly competitive WavLM Large on 12 out of 15 tasks with the same pre-training corpora and a similar model size. Crucially, our unified model learns complementary features and demonstrates comprehensive capabilities across two major benchmarks, SUPERB and HEAR, for evaluating audio representations. By further scaling up the model size and pre-training data, we present a unified model with 600M parameters that excels in both domains, establishing it as one of the most powerful and versatile open-source SSL models for auditory understanding. The inference code and pre-trained models will be made publicly available.</p>
<blockquote>
<p>è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨å£°å­¦ä¿¡å·çš„é€šç”¨è¡¨ç¤ºå­¦ä¹ ä¸Šè¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä»ç„¶æ˜¯é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„ï¼Œä¸“é—¨ä¸ºè¯­éŸ³æˆ–é€šç”¨éŸ³é¢‘å®šåˆ¶ï¼Œè¿™é˜»ç¢äº†åœ¨ä¸¤è€…é¢†åŸŸä¸Šéƒ½å…·æœ‰ç»¼åˆèƒ½åŠ›çš„ç»Ÿä¸€è¡¨ç¤ºæ¨¡å‹çš„å‘å±•ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†SPEARï¼ˆè¯­éŸ³å’ŒéŸ³é¢‘è¡¨ç¤ºï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæˆåŠŸåœ°ä»è¯­éŸ³å’ŒéŸ³é¢‘æ•°æ®çš„æ··åˆä¸­å­¦ä¹ ç»Ÿä¸€è¯­éŸ³å’ŒéŸ³é¢‘è¡¨ç¤ºçš„SSLæ¡†æ¶ã€‚SPEARæå‡ºäº†ä¸€ä¸ªåŸºäºç²¾ç»†ç²’åº¦ç¦»æ•£æ ‡è®°çš„æ©ç é¢„æµ‹çš„ç»Ÿä¸€é¢„è®­ç»ƒç›®æ ‡ï¼Œé€‚ç”¨äºè¯­éŸ³å’Œé€šç”¨éŸ³é¢‘ã€‚è¿™äº›æ ‡è®°æ˜¯ä»è¿ç»­çš„è¯­éŸ³å’ŒéŸ³é¢‘è¡¨ç¤ºä¸­ä½¿ç”¨å¤šç æœ¬çŸ¢é‡é‡åŒ–ï¼ˆMVQï¼‰æ–¹æ³•å¾—å‡ºçš„ï¼Œä¿ç•™äº†ä¸°å¯Œçš„å£°å­¦ç»†èŠ‚ï¼Œå¯¹äºå»ºæ¨¡è¯­éŸ³å’Œå¤æ‚éŸ³é¢‘äº‹ä»¶éƒ½è‡³å…³é‡è¦ã€‚SPEARè¢«åº”ç”¨äºé¢„è®­ç»ƒå•åŸŸå’Œç»Ÿä¸€çš„è¯­éŸ³ä¸éŸ³é¢‘SSLæ¨¡å‹ã€‚æˆ‘ä»¬çš„è¯­éŸ³é¢†åŸŸæ¨¡å‹åœ¨SUPERBåŸºå‡†æµ‹è¯•ä¸Šå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºSSLæ¨¡å‹çš„è¯­éŸ³å¤„ç†åŸºå‡†æµ‹è¯•ã€‚åœ¨ç›¸åŒçš„é¢„è®­ç»ƒè¯­æ–™åº“å’Œç›¸ä¼¼çš„æ¨¡å‹å¤§å°ä¸‹ï¼Œæˆ‘ä»¬åœ¨15é¡¹ä»»åŠ¡ä¸­çš„12é¡¹ä¸ŠåŒ¹é…æˆ–è¶…è¶Šäº†æå…·ç«äº‰åŠ›çš„WavLM Largeã€‚å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç»Ÿä¸€æ¨¡å‹å­¦ä¹ äº†äº’è¡¥ç‰¹å¾ï¼Œå¹¶åœ¨ä¸¤ä¸ªä¸»è¦åŸºå‡†æµ‹è¯•SUPERBå’ŒHEARä¸Šå±•ç¤ºäº†ç»¼åˆçš„èƒ½åŠ›ï¼Œç”¨äºè¯„ä¼°éŸ³é¢‘è¡¨ç¤ºã€‚é€šè¿‡è¿›ä¸€æ­¥æ‰©å±•æ¨¡å‹å¤§å°å’Œé¢„è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸€ä¸ªå…·æœ‰6äº¿å‚æ•°çš„ç»Ÿä¸€æ¨¡å‹ï¼Œåœ¨ä¸¤ä¸ªé¢†åŸŸéƒ½è¡¨ç°å‡ºè‰²ï¼Œæˆä¸ºæœ€å¼ºå¤§å’Œæœ€é€šç”¨çš„å¼€æºSSLæ¨¡å‹ä¹‹ä¸€ï¼Œç”¨äºå¬è§‰ç†è§£ã€‚æ¨ç†ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.25955v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSPEARçš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ä»è¯­éŸ³å’ŒéŸ³é¢‘æ•°æ®çš„æ··åˆä¸­å­¦ä¹ ç»Ÿä¸€è¡¨ç¤ºã€‚SPEARåŸºäºç²¾ç»†ç¦»æ•£æ ‡è®°çš„æ©ç é¢„æµ‹ï¼Œæå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„é¢„è®­ç»ƒç›®æ ‡ï¼Œé€‚ç”¨äºè¯­éŸ³å’Œé€šç”¨éŸ³é¢‘ã€‚é€šè¿‡å¤šç¼–ç ç°¿çŸ¢é‡é‡åŒ–æ–¹æ³•ä»è¿ç»­è¯­éŸ³å’ŒéŸ³é¢‘è¡¨ç¤ºä¸­å¯¼å‡ºè¿™äº›æ ‡è®°ï¼Œä¿ç•™äº†å»ºæ¨¡è¯­éŸ³å’Œå¤æ‚éŸ³é¢‘äº‹ä»¶æ‰€éœ€çš„ä¸°å¯Œå£°å­¦ç»†èŠ‚ã€‚SPEARåº”ç”¨äºé¢„è®­ç»ƒå•åŸŸå’Œç»Ÿä¸€è¯­éŸ³ä¸éŸ³é¢‘è‡ªç›‘ç£å­¦ä¹ æ¨¡å‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå…¶è¯­éŸ³åŸŸæ¨¡å‹åœ¨SUPERBåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œä¸é«˜åº¦ç«äº‰çš„WavLM Largeæ¨¡å‹åœ¨ç›¸åŒé¢„è®­ç»ƒè¯­æ–™åº“å’Œç›¸ä¼¼æ¨¡å‹å¤§å°çš„æƒ…å†µä¸‹ï¼Œåœ¨15é¡¹ä»»åŠ¡ä¸­çš„12é¡¹ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡å…¶æ€§èƒ½ã€‚å…¶ç»Ÿä¸€æ¨¡å‹å­¦ä¹ äº’è¡¥ç‰¹å¾ï¼Œå¹¶åœ¨ä¸¤å¤§åŸºå‡†æµ‹è¯•SUPERBå’ŒHEARä¸Šè¡¨ç°å‡ºå…¨é¢çš„èƒ½åŠ›ã€‚é€šè¿‡æ‰©å¤§æ¨¡å‹è§„æ¨¡å’Œé¢„è®­ç»ƒæ•°æ®ï¼Œä»–ä»¬æå‡ºäº†ä¸€ç§å…·æœ‰6äº¿å‚æ•°çš„ç»Ÿä¸€æ¨¡å‹ï¼Œåœ¨å„ä¸ªé¢†åŸŸéƒ½è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæˆä¸ºå¬è§‰ç†è§£é¢†åŸŸæœ€å¼ºå¤§å’Œæœ€é€šç”¨çš„å¼€æºè‡ªç›‘ç£å­¦ä¹ æ¨¡å‹ä¹‹ä¸€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPEARæ˜¯é¦–ä¸ªæˆåŠŸå­¦ä¹ ç»Ÿä¸€è¯­éŸ³å’ŒéŸ³é¢‘è¡¨ç¤ºçš„è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ã€‚</li>
<li>SPEARé€šè¿‡æ©ç é¢„æµ‹ç²¾ç»†ç¦»æ•£æ ‡è®°çš„æ–¹æ³•å®ç°é¢„è®­ç»ƒç›®æ ‡ï¼Œé€‚ç”¨äºè¯­éŸ³å’Œé€šç”¨éŸ³é¢‘ã€‚</li>
<li>å¤šç¼–ç ç°¿çŸ¢é‡é‡åŒ–æ–¹æ³•ç”¨äºä»è¿ç»­è¡¨ç¤ºä¸­å¯¼å‡ºæ ‡è®°ï¼Œä¿ç•™ä¸°å¯Œçš„å£°å­¦ç»†èŠ‚ã€‚</li>
<li>SPEARçš„è¯­éŸ³åŸŸæ¨¡å‹åœ¨SUPERBåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸WavLM Largeç›¸æ¯”åœ¨å¤šæ•°ä»»åŠ¡ä¸Šæ›´èƒœä¸€ç­¹ã€‚</li>
<li>ç»Ÿä¸€æ¨¡å‹å­¦ä¹ äº’è¡¥ç‰¹å¾ï¼Œåœ¨ä¸¤å¤§åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå…¨é¢çš„èƒ½åŠ›ã€‚</li>
<li>æ‰©å¤§æ¨¡å‹è§„æ¨¡å’Œé¢„è®­ç»ƒæ•°æ®è¿›ä¸€æ­¥æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.25955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b96844c8ced5167ad8ccdb0152fdb9b6" align="middle">
<img src="https://picx.zhimg.com/v2-4f80873a9f4df42496fc0af2a021a636" align="middle">
<img src="https://picx.zhimg.com/v2-93b886ff92985491cbff1bfbd75800c6" align="middle">
<img src="https://picx.zhimg.com/v2-0b66f628eba1c19981aaf68fd4b5da34" align="middle">
<img src="https://picx.zhimg.com/v2-38f2f157427e9936a4814a48bfeca528" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c016906f7b4edc9abf4e14fd98c2ef35" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  DiffSwap++ 3D Latent-Controlled Diffusion for Identity-Preserving Face Swapping
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a28b29948c60f73fa27cf18feefa0063" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  A Dual-Mode ViT-Conditioned Diffusion Framework with an Adaptive Conditioning Bridge for Breast Cancer Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
