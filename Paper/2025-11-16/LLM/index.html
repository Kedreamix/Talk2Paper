<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e5770523b02b4aa1671994558f7c0f47')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-16-æ›´æ–°"><a href="#2025-11-16-æ›´æ–°" class="headerlink" title="2025-11-16 æ›´æ–°"></a>2025-11-16 æ›´æ–°</h1><h2 id="Enhancing-the-Outcome-Reward-based-RL-Training-of-MLLMs-with-Self-Consistency-Sampling"><a href="#Enhancing-the-Outcome-Reward-based-RL-Training-of-MLLMs-with-Self-Consistency-Sampling" class="headerlink" title="Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling"></a>Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling</h2><p><strong>Authors:Jiahao Wang, Weiye Xu, Aijun Yang, Wengang Zhou, Lewei Lu, Houqiang Li, Xiaohua Wang, Jinguo Zhu</strong></p>
<p>Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.</p>
<blockquote>
<p>ç»“æœå¯¼å‘çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯ä¼˜åŒ–å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„é€æ­¥æ¨ç†çš„ä¸€ç§å¸¸è§ä¸”æ—¥ç›Šé‡è¦çš„æ–¹æ³•ã€‚åœ¨å¤šé¡¹é€‰æ‹©çš„ç¯å¢ƒä¸­â€”â€”å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•çš„ä¸»å¯¼å½¢å¼â€”â€”è¿™ç§æ–¹æ³•é¢ä¸´ä¸€ä¸ªé‡å¤§ä½†å¸¸è¢«å¿½è§†çš„é—®é¢˜ï¼šä¸çœŸå®çš„è½¨è¿¹åœ¨é”™è¯¯çš„æ€ç»´é“¾æ¡åçŒœæµ‹å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œä¸çœŸå®æ¨ç†è·å¾—ç›¸åŒçš„å¥–åŠ±ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸å®¹å¿½è§†çš„ç¼ºé™·ã€‚æˆ‘ä»¬æå‡ºè‡ªæˆ‘ä¸€è‡´æ€§é‡‡æ ·ï¼ˆSCSï¼‰æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å¯¹äºæ¯ä¸ªé—®é¢˜ï¼ŒSCSï¼ˆiï¼‰å¼•å…¥å¾®å°çš„è§†è§‰æ‰°åŠ¨ï¼Œå¹¶ï¼ˆiiï¼‰å¯¹åˆå§‹è½¨è¿¹è¿›è¡Œé‡å¤æˆªæ–­å’Œé‡æ–°é‡‡æ ·ï¼›ç»“æœè½¨è¿¹ä¹‹é—´çš„ä¸€è‡´æ€§äº§ç”Ÿäº†ä¸€ä¸ªå¯åŒºåˆ†çš„ä¸€è‡´æ€§åˆ†æ•°ï¼Œåœ¨ç­–ç•¥æ›´æ–°æ—¶é™ä½ä¸å¯é è½¨è¿¹çš„æƒé‡ã€‚åŸºäºQwen2.5-VL-7B-Instructï¼Œå°†SCSæ’å…¥RLOOã€GRPOå’ŒREINFORCE++ç³»åˆ—ï¼Œåœ¨å…­ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾7.7ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”é¢å¤–çš„è®¡ç®—å‡ ä¹å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚SCSä¹Ÿåœ¨Qwen2.5-VL-3B-Instructå’ŒInternVL3-8Bä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¶ç›Šï¼Œä¸ºMLLMsä¸­çš„ç»“æœå¯¼å‘RLæä¾›äº†ç®€å•ã€é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10648v1">PDF</a> Accepted to NeurIPS 2025 (The Thirty-Ninth Annual Conference on Neural Information Processing Systems)</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å†³ç­–è¿‡ç¨‹ä¸­é‡‡ç”¨ç»“æœå¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼Œä½†åœ¨å¤šé€‰é¢˜ç¯å¢ƒä¸­å­˜åœ¨ä¸å¿ å®è½¨è¿¹é—®é¢˜ï¼Œå³é”™è¯¯æ¨ç†è·¯å¾„å³ä½¿çŒœæµ‹å‡ºæ­£ç¡®ç­”æ¡ˆä¹Ÿèƒ½è·å¾—å¥–åŠ±ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºè‡ªæˆ‘ä¸€è‡´æ€§é‡‡æ ·ï¼ˆSCSï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å¾®å°è§†è§‰æ‰°åŠ¨å’Œé‡å¤æˆªæ–­é‡é‡‡æ ·åˆå§‹è½¨è¿¹ï¼Œè®¡ç®—ä¸€è‡´æ€§å¾—åˆ†ï¼Œé™ä½ä¸å¯é è½¨è¿¹åœ¨ç­–ç•¥æ›´æ–°æ—¶çš„æƒé‡ã€‚å®éªŒè¡¨æ˜ï¼ŒSCSèƒ½æ˜¾è‘—æé«˜å…­ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡ï¼Œæœ€é«˜æå‡7.7ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”è®¡ç®—æˆæœ¬å¢åŠ å¾®ä¹å…¶å¾®ã€‚åŒæ—¶ï¼ŒSCSåœ¨Qwen2.5-VLå’ŒInternVLä¸Šä¹Ÿæœ‰æ‰€æ”¹è¿›ï¼Œä¸ºMLLMsä¸­çš„ç»“æœå¥–åŠ±RLæä¾›äº†ä¸€ç§ç®€å•é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»“æœå¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­å¹¿æ³›åº”ç”¨ï¼Œç”¨ä»¥æ”¹è¿›é€æ­¥æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>åœ¨å¤šé€‰é¢˜ç¯å¢ƒä¸‹ï¼ŒRLé¢ä¸´ä¸€ä¸ªé‡è¦é—®é¢˜ï¼šä¸å¿ å®è½¨è¿¹ï¼Œå³é”™è¯¯æ¨ç†è·¯å¾„ä¹Ÿèƒ½è·å¾—å¥–åŠ±ã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºè‡ªæˆ‘ä¸€è‡´æ€§é‡‡æ ·ï¼ˆSCSï¼‰æ–¹æ³•ã€‚</li>
<li>SCSé€šè¿‡å¼•å…¥è§†è§‰æ‰°åŠ¨å’Œè½¨è¿¹é‡å¤æˆªæ–­é‡é‡‡æ ·ï¼Œè®¡ç®—ä¸€è‡´æ€§å¾—åˆ†ã€‚</li>
<li>SCSèƒ½æ˜¾è‘—æé«˜å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡ï¼Œæœ€é«˜æå‡7.7ä¸ªç™¾åˆ†ç‚¹ã€‚</li>
<li>SCSæ–¹æ³•å…·æœ‰è¾ƒä½çš„è®¡ç®—æˆæœ¬å¢åŠ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10648">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5770523b02b4aa1671994558f7c0f47" align="middle">
<img src="https://picx.zhimg.com/v2-a9bfcfdb6da5397857cf44f59944ffa1" align="middle">
<img src="https://picx.zhimg.com/v2-c159d43af98429c21148f370f8710bcc" align="middle">
<img src="https://picx.zhimg.com/v2-043764230e493279155e2e230efc8960" align="middle">
<img src="https://picx.zhimg.com/v2-fecbbd5237f577a314cf5a74b4fd674d" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Instella-Fully-Open-Language-Models-with-Stellar-Performance"><a href="#Instella-Fully-Open-Language-Models-with-Stellar-Performance" class="headerlink" title="Instella: Fully Open Language Models with Stellar Performance"></a>Instella: Fully Open Language Models with Stellar Performance</h2><p><strong>Authors:Jiang Liu, Jialian Wu, Xiaodong Yu, Yusheng Su, Prakamya Mishra, Gowtham Ramesh, Sudhanshu Ranjan, Chaitanya Manem, Ximeng Sun, Ze Wang, Pratik Prabhanjan Brahma, Zicheng Liu, Emad Barsoum</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.</p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç„¶è€Œï¼Œå¤§å¤šæ•°é«˜æ€§èƒ½æ¨¡å‹ä»ç„¶æ˜¯é—­æºçš„æˆ–éƒ¨åˆ†å¼€æºçš„ï¼Œè¿™é™åˆ¶äº†é€æ˜åº¦å’Œå¯é‡å¤æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Instellaï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºçš„ã€åŸºäºå…¬å¼€æ•°æ®å’Œä»£ç åº“è®­ç»ƒçš„ã€æ‹¥æœ‰ä¸‰åäº¿å‚æ•°çš„è¯­è¨€æ¨¡å‹å®¶æ—ã€‚Instellaç”±AMD Instinct MI300X GPUé©±åŠ¨ï¼Œé€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒã€é€šç”¨æŒ‡ä»¤è°ƒæ•´å’Œä¸äººç±»åå¥½å¯¹é½çš„æ–¹å¼å¼€å‘ã€‚å°½ç®¡ä½¿ç”¨çš„é¢„è®­ç»ƒä»¤ç‰Œè¿œå°‘äºè®¸å¤šåŒé¾„äººï¼Œä½†Instellaåœ¨å®Œå…¨å¼€æºæ¨¡å‹ä¸­çš„è¡¨ç°è¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ï¼Œå¹¶ä¸åŒè§„æ¨¡çš„å¼€æºæƒé‡é¢†å…ˆæ¨¡å‹å…·æœ‰ç«äº‰åŠ›ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸¤ä¸ªä¸“ä¸šç‰ˆæœ¬ï¼šèƒ½å¤Ÿå¤„ç†é•¿è¾¾128Kä»¤ç‰Œä¸Šä¸‹æ–‡é•¿åº¦çš„Instella-Longï¼Œä»¥åŠé€šè¿‡ç›‘ç£å¾®è°ƒå¼ºåŒ–å­¦ä¹ è¿›è¡Œæ•°å­¦ä»»åŠ¡æ¨ç†çš„Instella-Mathã€‚æ€»ä¹‹ï¼Œè¿™äº›è´¡çŒ®ä½¿Instellaæˆä¸ºç¤¾åŒºä¸­é€æ˜ã€é«˜æ€§èƒ½å’Œé€šç”¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†å¼€æºå’Œå¯é‡å¤çš„è¯­è¨€å»ºæ¨¡ç ”ç©¶ç›®æ ‡çš„å®ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10628v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å…¬å¼€ã€å¯å¤ç°çš„å¤§å‹è¯­è¨€æ¨¡å‹ç ”ç©¶å–å¾—è¿›å±•ã€‚æœ¬ç ”ç©¶æ¨å‡ºInstellaç³»åˆ—æ¨¡å‹ï¼Œé‡‡ç”¨å…¬å¼€æ•°æ®å’Œä»£ç åº“è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨AMD Instinct MI300X GPUåŠ é€Ÿã€‚Instellaæ¨¡å‹é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒã€é€šç”¨æŒ‡ä»¤è°ƒæ•´å’Œä¸äººç±»åå¥½å¯¹é½ï¼Œå®ç°äº†å…¨å¼€æºçš„3äº¿å‚æ•°è¯­è¨€æ¨¡å‹ã€‚ä¸å…¶ä»–å…¨å¼€æºæ¨¡å‹ç›¸æ¯”ï¼Œå°½ç®¡ä½¿ç”¨çš„é¢„è®­ç»ƒä»¤ç‰Œæ•°é‡å¤§å¤§å‡å°‘ï¼Œä½†Instellaä»å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œå¹¶å¯ä¸ç›¸å½“è§„æ¨¡çš„å¼€æºæƒé‡æ¨¡å‹ç›¸ç«äº‰ã€‚æ­¤å¤–ï¼Œè¿˜å‘å¸ƒäº†ä¸¤ä¸ªä¸“é—¨ç‰ˆæœ¬ï¼šå¤„ç†ä¸Šä¸‹æ–‡é•¿åº¦å¯è¾¾128Kä»¤ç‰Œçš„Instella-Longå’Œä¸“æ³¨äºæ¨ç†çš„Instella-Mathï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒå¼ºåŒ–å­¦ä¹ åœ¨æ•°å­¦ä»»åŠ¡ä¸Šè¿›è¡Œå¢å¼ºã€‚è¿™äº›è´¡çŒ®ä½¿Instellaæˆä¸ºç¤¾åŒºä¸­é€æ˜ã€é«˜æ€§èƒ½å’Œé€šç”¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ¨åŠ¨å¼€æ”¾å’Œå¯å¤ç°çš„è¯­è¨€å»ºæ¨¡ç ”ç©¶å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Instellaæ˜¯é¦–ä¸ªå®Œå…¨å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ç³»åˆ—ï¼Œä½¿ç”¨å…¬å¼€æ•°æ®å’Œä»£ç åº“è¿›è¡Œè®­ç»ƒã€‚</li>
<li>Instellaæ¨¡å‹é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒã€é€šç”¨æŒ‡ä»¤è°ƒæ•´ä¸äººç±»åå¥½å¯¹é½å®ç°é«˜æ€§èƒ½ã€‚</li>
<li>ä¸è®¸å¤šå½“ä»£æ¨¡å‹ç›¸æ¯”ï¼ŒInstellaåœ¨ä½¿ç”¨çš„é¢„è®­ç»ƒä»¤ç‰Œæ•°é‡ä¸Šæœ‰æ‰€å‡å°‘ã€‚</li>
<li>Instellaåœ¨å®Œå…¨å¼€æºæ¨¡å‹ä¸­çš„è¡¨ç°è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå¹¶ä¸åŒç±»è§„æ¨¡çš„å¼€æºæƒé‡æ¨¡å‹ç«äº‰åŠ›ç›¸å½“ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿå‘å¸ƒäº†ä¸¤ä¸ªä¸“é—¨ç‰ˆæœ¬çš„Instellaæ¨¡å‹ï¼šå¤„ç†é•¿æ–‡æœ¬çš„Instella-Longå’Œä¸“æ³¨äºæ•°å­¦æ¨ç†çš„Instella-Mathã€‚</li>
<li>Instella-Mathé€šè¿‡ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ åœ¨æ•°å­¦ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¢å¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0b50544bfc46f77d0a24e192f86b13da" align="middle">
<img src="https://picx.zhimg.com/v2-a8fab08c1708160e25109b396472203a" align="middle">
<img src="https://picx.zhimg.com/v2-0d12dbb0c23252412ca1b14a472fb1cb" align="middle">
<img src="https://picx.zhimg.com/v2-52234f6b2d92afdc850b28d7dd191cd0" align="middle">
<img src="https://picx.zhimg.com/v2-897d803ec990f59bb3cc02d9c3e21d42" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Querying-Labeled-Time-Series-Data-with-Scenario-Programs"><a href="#Querying-Labeled-Time-Series-Data-with-Scenario-Programs" class="headerlink" title="Querying Labeled Time Series Data with Scenario Programs"></a>Querying Labeled Time Series Data with Scenario Programs</h2><p><strong>Authors:Edward Kim, Devan Shanker, Varun Bharadwaj, Hongbeen Park, Jinkyu Kim, Hazem Torfah, Daniel J Fremont, Sanjit A Seshia</strong></p>
<p>Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.</p>
<blockquote>
<p>åŸºäºæ¨¡æ‹Ÿçš„æµ‹è¯•å·²æˆä¸ºå¯¹ç‰©ç†ç½‘ç»œç³»ç»Ÿï¼ˆCPSï¼‰è¿›è¡Œé“è·¯æµ‹è¯•çš„é‡è¦è¡¥å……ï¼Œä»è€Œç¡®ä¿ç½‘ç»œå®‰å…¨ã€‚å› æ­¤ï¼Œå¤§é‡çš„ç ”ç©¶å·¥ä½œè‡´åŠ›äºåœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è¯†åˆ«å¤±è´¥åœºæ™¯ã€‚ç„¶è€Œï¼Œä»ç„¶æœ‰ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šåœ¨æ¨¡æ‹Ÿä¸­å‘ç°çš„è‡ªåŠ¨é©¾é©¶å¤±è´¥åœºæ™¯èƒ½å¦åœ¨ç°å®ä¸–ç•Œä¸­å®é™…ç³»ç»Ÿçš„é‡ç°ï¼Ÿç”±äºæ¨¡æ‹Ÿæ•°æ®å’ŒçœŸå®ä¼ æ„Ÿå™¨æ•°æ®ä¹‹é—´çš„å·®å¼‚ï¼Œå¯¼è‡´æ¨¡æ‹Ÿä¸çœŸå®ä¹‹é—´çš„å·®è·ï¼Œè¿™æ„å‘³ç€åœ¨æ¨¡æ‹Ÿä¸­ç¡®å®šçš„å¤±è´¥åœºæ™¯å¯èƒ½æ˜¯åˆæˆä¼ æ„Ÿå™¨æ•°æ®çš„äº§ç‰©ï¼Œä¹Ÿå¯èƒ½æ˜¯çœŸå®ä¼ æ„Ÿå™¨æ•°æ®ä¹Ÿä¼šå‡ºç°çš„å®é™…é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒéªŒè¯æ¨¡æ‹Ÿå¤±è´¥åœºæ™¯çš„æœ‰æ•ˆæ–¹æ³•æ˜¯å®šä½ç°å®ä¸–ç•Œæ•°æ®é›†ä¸­çš„è¿™äº›åœºæ™¯å‘ç”Ÿæƒ…å†µï¼Œå¹¶éªŒè¯å¤±è´¥æ˜¯å¦æŒç»­å­˜åœ¨äºè¿™äº›æ•°æ®é›†ä¸­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ­£å¼å®šä¹‰äº†å¦‚ä½•ä½¿æ ‡è®°çš„æ—¶é—´åºåˆ—ä¼ æ„Ÿå™¨æ•°æ®ä¸æŠ½è±¡åœºæ™¯åŒ¹é…ï¼Œè¯¥åœºæ™¯ä½¿ç”¨Scenicæ¦‚ç‡ç¼–ç¨‹è¯­è¨€è¡¨ç¤ºä¸ºåœºæ™¯ç¨‹åºã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æŸ¥è¯¢ç®—æ³•ï¼Œç»™å®šåœºæ™¯ç¨‹åºå’Œæ ‡è®°æ•°æ®é›†ï¼Œè¯¥ç®—æ³•å¯ä»¥è¯†åˆ«åŒ¹é…æŒ‡å®šåœºæ™¯çš„æ•°æ®å­é›†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®—æ³•åœ¨æŸ¥è¯¢åœºæ™¯æ–¹é¢æ¯”æœ€æ–°çš„å•†ä¸šè§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹æ›´å‡†ç¡®ã€é€Ÿåº¦å¿«å¾—å¤šï¼Œå¹¶ä¸”å¯ä»¥éšç€æŸ¥è¯¢æ—¶é—´åºåˆ—æ•°æ®çš„æŒç»­æ—¶é—´è€Œæ‰©å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10627v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ¨¡æ‹Ÿæµ‹è¯•å·²æˆä¸ºç¡®ä¿ç½‘ç»œç‰©ç†ç³»ç»Ÿå®‰å…¨çš„é‡è¦è¡¥å……æ‰‹æ®µã€‚å½“å‰ç ”ç©¶é›†ä¸­åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è¯†åˆ«å¤±è´¥åœºæ™¯ï¼Œä½†æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„å¤±è´¥åœºæ™¯æ˜¯å¦èƒ½åœ¨å®é™…ç³»ç»Ÿä¸­é‡ç°ä»å­˜åœ¨ç–‘é—®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§éªŒè¯æ¨¡æ‹Ÿå¤±è´¥åœºæ™¯çš„æ–¹æ³•ï¼Œé€šè¿‡å®šä½çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸­çš„è¿™äº›åœºæ™¯å¹¶éªŒè¯å…¶æ˜¯å¦å­˜åœ¨å¤±è´¥æƒ…å†µæ¥è§£å†³æ¨¡æ‹Ÿä¸çœŸå®ä¹‹é—´çš„å·®è·é—®é¢˜ã€‚æœ¬æ–‡è¿˜å®šä¹‰äº†ä¸€ç§æ–¹æ³•ï¼Œä½¿ç”¨Scenicæ¦‚ç‡ç¼–ç¨‹è¯­è¨€å°†æ ‡ç­¾æ—¶é—´åºåˆ—ä¼ æ„Ÿå™¨æ•°æ®ä¸æŠ½è±¡åœºæ™¯è¿›è¡ŒåŒ¹é…ï¼Œå¹¶ç»™å‡ºäº†æŸ¥è¯¢ç®—æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç®—æ³•æ¯”ç°æœ‰çš„å•†ä¸šè§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹æ›´å‡†ç¡®ä¸”é€Ÿåº¦å¿«å¾—å¤šï¼Œå¹¶èƒ½éšç€æŸ¥è¯¢æ—¶é—´åºåˆ—æ•°æ®çš„æŒç»­æ—¶é—´è¿›è¡Œæ‰©å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡æ‹Ÿæµ‹è¯•æ˜¯ç½‘ç»œç‰©ç†ç³»ç»Ÿå®‰å…¨æ€§çš„é‡è¦è¡¥å……æ‰‹æ®µã€‚</li>
<li>æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„å¤±è´¥åœºæ™¯åœ¨å®é™…ç³»ç»Ÿä¸­çš„é‡ç°æ€§ä»å­˜åœ¨ç–‘é—®ã€‚</li>
<li>æ¨¡æ‹Ÿä¸çœŸå®ä¹‹é—´çš„å·®è·é—®é¢˜å¯é€šè¿‡éªŒè¯æ¨¡æ‹Ÿå¤±è´¥åœºæ™¯æ¥è§£å†³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ä½¿ç”¨Scenicæ¦‚ç‡ç¼–ç¨‹è¯­è¨€å°†æ ‡ç­¾æ—¶é—´åºåˆ—ä¼ æ„Ÿå™¨æ•°æ®ä¸æŠ½è±¡åœºæ™¯åŒ¹é…çš„æ–¹æ³•ã€‚</li>
<li>ç»™å‡ºäº†æŸ¥è¯¢ç®—æ³•ï¼Œå¯å®šä½çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸­çš„ç‰¹å®šå¤±è´¥åœºæ™¯ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æŸ¥è¯¢ç®—æ³•æ¯”ç°æœ‰å•†ä¸šè§†è§‰å¤§å‹è¯­è¨€æ¨¡å‹æ›´å‡†ç¡®ä¸”é€Ÿåº¦æ›´å¿«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-478e4a9f6d041e6caff92a1a827368b5" align="middle">
<img src="https://picx.zhimg.com/v2-cc161d35470af1449fdfd19405ec6817" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SSR-Socratic-Self-Refine-for-Large-Language-Model-Reasoning"><a href="#SSR-Socratic-Self-Refine-for-Large-Language-Model-Reasoning" class="headerlink" title="SSR: Socratic Self-Refine for Large Language Model Reasoning"></a>SSR: Socratic Self-Refine for Large Language Model Reasoning</h2><p><strong>Authors:Haizhou Shi, Ye Liu, Bo Pang, Zeyu Leo Liu, Hao Wang, Silvio Savarese, Caiming Xiong, Yingbo Zhou, Semih Yavuz</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at <a target="_blank" rel="noopener" href="https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning">https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning</a>.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å±•ç°å‡ºæƒŠäººçš„æ¨ç†èƒ½åŠ›ï¼Œç„¶è€Œç°æœ‰çš„æµ‹è¯•æ—¶é—´æ¡†æ¶é€šå¸¸ä¾èµ–äºç²—ç•¥çš„è‡ªæˆ‘éªŒè¯å’Œè‡ªæˆ‘æ ¡æ­£ï¼Œè¿™åœ¨å¤æ‚ä»»åŠ¡ä¸Šé™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è‹æ ¼æ‹‰åº•è‡ªæˆ‘ä¿®æ­£ï¼ˆSSRï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œç”¨äºå¯¹LLMæ¨ç†è¿›è¡Œç²¾ç»†çš„è¯„ä¼°å’Œç²¾ç¡®çš„ä¼˜åŒ–ã€‚æˆ‘ä»¬æå‡ºçš„SSRå°†æ¨¡å‹å“åº”åˆ†è§£ä¸ºå¯éªŒè¯çš„ï¼ˆå­é—®é¢˜ã€å­ç­”æ¡ˆï¼‰å¯¹ï¼Œé€šè¿‡å—æ§çš„é‡æ–°æ±‚è§£å’Œè‡ªæˆ‘ä¸€è‡´æ€§æ£€æŸ¥ï¼Œå®ç°æ­¥éª¤çº§åˆ«çš„ç½®ä¿¡åº¦ä¼°è®¡ã€‚é€šè¿‡å®šä½ä¸å¯é çš„æ­¥éª¤å¹¶è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼ŒSSRç”Ÿæˆæ›´å‡†ç¡®ä¸”å¯è§£é‡Šçš„æ¨ç†é“¾ã€‚åœ¨äº”ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•å’Œä¸‰ä¸ªLLMä¸Šçš„ç»éªŒç»“æœè¡¨æ˜ï¼ŒSSRå§‹ç»ˆä¼˜äºæœ€æ–°çš„è¿­ä»£è‡ªæˆ‘ä¿®æ­£åŸºçº¿ã€‚é™¤äº†æ€§èƒ½æå‡å¤–ï¼ŒSSRè¿˜æä¾›äº†ä¸€ç§æœ‰åŸåˆ™çš„é»‘ç›’æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°å’Œäº†è§£LLMçš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning">https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10621v1">PDF</a> Preprint; work in progress</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†ç°æœ‰æµ‹è¯•æ¡†æ¶çš„è‡ªæˆ‘éªŒè¯å’Œæ ¡æ­£è¾ƒä¸ºç²—ç•¥ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚ä»»åŠ¡ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºâ€œè‹æ ¼æ‹‰åº•è‡ªæˆ‘ç²¾ç‚¼â€ï¼ˆSSRï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºå¯¹LLMæ¨ç†è¿›è¡Œç²¾ç»†è¯„ä¼°ä¸ç²¾ç¡®ä¼˜åŒ–ã€‚SSRå°†æ¨¡å‹å“åº”åˆ†è§£ä¸ºå¯éªŒè¯çš„ï¼ˆå­é—®é¢˜ã€å­ç­”æ¡ˆï¼‰å¯¹ï¼Œé€šè¿‡å—æ§é‡æ–°æ±‚è§£å’Œè‡ªæˆ‘ä¸€è‡´æ€§æ£€æŸ¥è¿›è¡Œæ­¥éª¤çº§ç½®ä¿¡åº¦ä¼°è®¡ã€‚é€šè¿‡å®šä½ä¸å¯é çš„æ­¥éª¤å¹¶è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼ŒSSRç”Ÿæˆæ›´å‡†ç¡®ä¸”å¯è§£é‡Šæ€§å¼ºçš„æ¨ç†é“¾ã€‚åœ¨äº”ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•å’Œä¸‰ä¸ªLLMä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒSSRæŒç»­ä¼˜äºæœ€æ–°çš„è¿­ä»£è‡ªæˆ‘ä¼˜åŒ–åŸºçº¿ã€‚é™¤äº†æ€§èƒ½æå‡å¤–ï¼ŒSSRè¿˜ä¸ºè¯„ä¼°å’Œç†è§£LLMçš„å†…éƒ¨æ¨ç†è¿‡ç¨‹æä¾›äº†ä¸€ç§åŸåˆ™æ€§çš„é»‘ç›’æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†ç°æœ‰æµ‹è¯•æ¡†æ¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>è‹æ ¼æ‹‰åº•è‡ªæˆ‘ç²¾ç‚¼ï¼ˆSSRï¼‰æ¡†æ¶ç”¨äºç²¾ç»†è¯„ä¼°LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>SSRé€šè¿‡åˆ†è§£æ¨¡å‹å“åº”ä¸ºå¯éªŒè¯çš„å­é—®é¢˜ã€å­ç­”æ¡ˆå¯¹ï¼Œè¿›è¡Œæ­¥éª¤çº§ç½®ä¿¡åº¦ä¼°è®¡ã€‚</li>
<li>SSRé€šè¿‡å®šä½å¹¶ä¼˜åŒ–ä¸å¯é çš„æ­¥éª¤ï¼Œç”Ÿæˆæ›´å‡†ç¡®ä¸”å¯è§£é‡Šçš„æ¨ç†é“¾ã€‚</li>
<li>SSRåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’ŒLLMä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>SSRæä¾›è¯„ä¼°å’Œç†è§£LLMå†…éƒ¨æ¨ç†è¿‡ç¨‹çš„é»‘ç›’æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a13627c835266701336b1a5f413f336d" align="middle">
<img src="https://picx.zhimg.com/v2-338c8dafacec3031df700905d70933ce" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Impact-of-Layer-Norm-on-Memorization-and-Generalization-in-Transformers"><a href="#Impact-of-Layer-Norm-on-Memorization-and-Generalization-in-Transformers" class="headerlink" title="Impact of Layer Norm on Memorization and Generalization in Transformers"></a>Impact of Layer Norm on Memorization and Generalization in Transformers</h2><p><strong>Authors:Rishi Singhal, Jung-Eun Kim</strong></p>
<p>Layer Normalization (LayerNorm) is one of the fundamental components in transformers that stabilizes training and improves optimization. In recent times, Pre-LayerNorm transformers have become the preferred choice over Post-LayerNorm transformers due to their stable gradient flow. However, the impact of LayerNorm on learning and memorization across these architectures remains unclear. In this work, we investigate how LayerNorm influences memorization and learning for Pre- and Post-LayerNorm transformers. We identify that LayerNorm serves as a key factor for stable learning in Pre-LayerNorm transformers, while in Post-LayerNorm transformers, it impacts memorization. Our analysis reveals that eliminating LayerNorm parameters in Pre-LayerNorm models exacerbates memorization and destabilizes learning, while in Post-LayerNorm models, it effectively mitigates memorization by restoring genuine labels. We further precisely identify that early layers LayerNorm are the most critical over middle&#x2F;later layers and their influence varies across Pre and Post LayerNorm models. We have validated it through 13 models across 6 Vision and Language datasets. These insights shed new light on the role of LayerNorm in shaping memorization and learning in transformers.</p>
<blockquote>
<p>å±‚å½’ä¸€åŒ–ï¼ˆLayerNormï¼‰æ˜¯ç¨³å®šè®­ç»ƒå’Œä¼˜åŒ–è½¬æ¢å™¨æ¨¡å‹çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†ä¹‹ä¸€ã€‚ç”±äºå…·æœ‰ç¨³å®šçš„æ¢¯åº¦æµï¼ŒPre-LayerNormè½¬æ¢å™¨å·²æˆä¸ºè¾ƒPost-LayerNormè½¬æ¢å™¨çš„é¦–é€‰ã€‚ç„¶è€Œï¼ŒLayerNormåœ¨è¿™äº›æ¶æ„ä¸­å¯¹å­¦ä¹ å’Œè®°å¿†çš„å½±å“å°šä¸æ¸…æ¥šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†LayerNormå¯¹Pre-å’ŒPost-LayerNormè½¬æ¢å™¨çš„è®°å¿†å’Œå­¦ä¹ çš„å½±å“ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨Pre-LayerNormè½¬æ¢å™¨ä¸­ï¼ŒLayerNormæ˜¯ç¨³å®šå­¦ä¹ çš„å…³é”®å› ç´ ï¼Œè€Œåœ¨Post-LayerNormè½¬æ¢å™¨ä¸­ï¼Œå®ƒå½±å“è®°å¿†ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œæ¶ˆé™¤Pre-LayerNormæ¨¡å‹ä¸­çš„LayerNormå‚æ•°ä¼šåŠ å‰§è®°å¿†é—®é¢˜å¹¶å¯¼è‡´å­¦ä¹ ä¸ç¨³å®šï¼Œè€Œåœ¨Post-LayerNormæ¨¡å‹ä¸­ï¼Œå®ƒé€šè¿‡æ¢å¤çœŸå®æ ‡ç­¾æœ‰æ•ˆåœ°å‡è½»äº†è®°å¿†è´Ÿæ‹…ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ç²¾ç¡®åœ°ç¡®å®šï¼Œæ—©æœŸå±‚çš„LayerNormè¾ƒä¸­å±‚æˆ–åå±‚æ›´ä¸ºé‡è¦ï¼Œå¹¶ä¸”åœ¨Preå’ŒPost LayerNormæ¨¡å‹ä¸­çš„å½±å“å„ä¸ç›¸åŒã€‚æˆ‘ä»¬é€šè¿‡6ä¸ªè§†è§‰å’Œè¯­è¨€æ•°æ®é›†çš„13ä¸ªæ¨¡å‹éªŒè¯äº†è¿™ä¸€ç‚¹ã€‚è¿™äº›è§è§£ä¸ºç†è§£LayerNormåœ¨å¡‘é€ è½¬æ¢å™¨ä¸­çš„è®°å¿†å’Œå­¦ä¹ æ–¹é¢æä¾›äº†æ–°çš„è§†è§’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10566v1">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong><br>     å±‚å½’ä¸€åŒ–ï¼ˆLayerNormï¼‰æ˜¯ç¨³å®šè®­ç»ƒå’Œä¼˜åŒ–è½¬æ¢å™¨æ¨¡å‹çš„å…³é”®ç»„ä»¶ä¹‹ä¸€ã€‚æœ¬æ–‡ç ”ç©¶äº†LayerNormå¯¹é¢„å±‚å½’ä¸€åŒ–å’Œåå±‚å½’ä¸€åŒ–è½¬æ¢å™¨çš„è®°å¿†å’Œå­¦ä¹ çš„å½±å“ï¼Œå‘ç°é¢„å±‚å½’ä¸€åŒ–è½¬æ¢å™¨ä¸­çš„LayerNormæ˜¯å®ç°ç¨³å®šå­¦ä¹ çš„å…³é”®å› ç´ ï¼Œè€Œåå±‚å½’ä¸€åŒ–è½¬æ¢å™¨ä¸­çš„LayerNormåˆ™å½±å“è®°å¿†ã€‚ç§»é™¤é¢„å±‚å½’ä¸€åŒ–æ¨¡å‹ä¸­çš„LayerNormå‚æ•°ä¼šåŠ å‰§è®°å¿†é—®é¢˜å¹¶ç ´åå­¦ä¹ ç¨³å®šæ€§ï¼Œè€Œåœ¨åå±‚å½’ä¸€åŒ–æ¨¡å‹ä¸­ï¼Œç§»é™¤LayerNormå¯æœ‰æ•ˆå‡è½»è®°å¿†é—®é¢˜ã€‚æ—©æœŸå±‚çš„LayerNormç›¸è¾ƒäºä¸­é—´æˆ–åæœŸå±‚æ›´ä¸ºé‡è¦ï¼Œå…¶åœ¨é¢„å±‚å’Œåå±‚å½’ä¸€åŒ–æ¨¡å‹ä¸­çš„å½±å“å„ä¸ç›¸åŒã€‚è¿™äº›è§è§£ä¸ºç†è§£LayerNormåœ¨å¡‘é€ è½¬æ¢å™¨ä¸­çš„è®°å¿†å’Œå­¦ä¹ æä¾›äº†æ–°çš„è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LayerNormæ˜¯è½¬æ¢å™¨æ¨¡å‹ä¸­çš„é‡è¦ç»„ä»¶ï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒå’Œä¼˜åŒ–ã€‚</li>
<li>é¢„å±‚å½’ä¸€åŒ–è½¬æ¢å™¨ä¸­çš„LayerNormå¯¹ç¨³å®šå­¦ä¹ èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>åå±‚å½’ä¸€åŒ–è½¬æ¢å™¨ä¸­çš„LayerNormå½±å“æ¨¡å‹çš„è®°å¿†èƒ½åŠ›ã€‚</li>
<li>ç§»é™¤é¢„å±‚å½’ä¸€åŒ–æ¨¡å‹ä¸­çš„LayerNormå‚æ•°ä¼šåŠ å‰§è®°å¿†é—®é¢˜å¹¶ç ´åå­¦ä¹ ç¨³å®šæ€§ã€‚</li>
<li>åœ¨åå±‚å½’ä¸€åŒ–æ¨¡å‹ä¸­ï¼Œç§»é™¤LayerNormæœ‰åŠ©äºå‡è½»è®°å¿†é—®é¢˜ã€‚</li>
<li>æ—©æœŸå±‚çš„LayerNormå¯¹é¢„å±‚å’Œåå±‚å½’ä¸€åŒ–æ¨¡å‹çš„å½±å“æœ€ä¸ºæ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e70aa7d4cfa1cbe57b4f1ee11476c885" align="middle">
<img src="https://picx.zhimg.com/v2-d6a31e20c263d5d7f8e17d5b61ed9ad5" align="middle">
<img src="https://picx.zhimg.com/v2-9c0e70c6f9406a3161052bf91c0c34ff" align="middle">
<img src="https://picx.zhimg.com/v2-90f88291049803ecc830fd237c74d996" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="URaG-Unified-Retrieval-and-Generation-in-Multimodal-LLMs-for-Efficient-Long-Document-Understanding"><a href="#URaG-Unified-Retrieval-and-Generation-in-Multimodal-LLMs-for-Efficient-Long-Document-Understanding" class="headerlink" title="URaG: Unified Retrieval and Generation in Multimodal LLMs for Efficient Long Document Understanding"></a>URaG: Unified Retrieval and Generation in Multimodal LLMs for Efficient Long Document Understanding</h2><p><strong>Authors:Yongxin Shi, Jiapeng Wang, Zeyu Shan, Dezhi Peng, Zening Lin, Lianwen Jin</strong></p>
<p>Recent multimodal large language models (MLLMs) still struggle with long document understanding due to two fundamental challenges: information interference from abundant irrelevant content, and the quadratic computational cost of Transformer-based architectures. Existing approaches primarily fall into two categories: token compression, which sacrifices fine-grained details; and introducing external retrievers, which increase system complexity and prevent end-to-end optimization. To address these issues, we conduct an in-depth analysis and observe that MLLMs exhibit a human-like coarse-to-fine reasoning pattern: early Transformer layers attend broadly across the document, while deeper layers focus on relevant evidence pages. Motivated by this insight, we posit that the inherent evidence localization capabilities of MLLMs can be explicitly leveraged to perform retrieval during the reasoning process, facilitating efficient long document understanding. To this end, we propose URaG, a simple-yet-effective framework that Unifies Retrieval and Generation within a single MLLM. URaG introduces a lightweight cross-modal retrieval module that converts the early Transformer layers into an efficient evidence selector, identifying and preserving the most relevant pages while discarding irrelevant content. This design enables the deeper layers to concentrate computational resources on pertinent information, improving both accuracy and efficiency. Extensive experiments demonstrate that URaG achieves state-of-the-art performance while reducing computational overhead by 44-56%. The code is available at <a target="_blank" rel="noopener" href="https://github.com/shi-yx/URaG">https://github.com/shi-yx/URaG</a>.</p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†é•¿æ–‡æ¡£ç†è§£æ—¶ä»é¢ä¸´ä¸¤å¤§åŸºæœ¬æŒ‘æˆ˜ï¼šä¸€æ˜¯æ¥è‡ªå¤§é‡æ— å…³å†…å®¹çš„ä¿¡æ¯å¹²æ‰°ï¼ŒäºŒæ˜¯åŸºäºTransformerçš„æ¶æ„çš„äºŒæ¬¡è®¡ç®—æˆæœ¬ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼šä¸€æ˜¯ç‰ºç‰²ç²¾ç»†ç»†èŠ‚çš„è¯å…ƒå‹ç¼©ï¼›äºŒæ˜¯å¼•å…¥å¤–éƒ¨æ£€ç´¢å™¨ï¼Œå¢åŠ äº†ç³»ç»Ÿå¤æ‚æ€§å¹¶é˜»æ­¢äº†ç«¯åˆ°ç«¯çš„ä¼˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¹¶è§‚å¯Ÿåˆ°MLLMsè¡¨ç°å‡ºäººç±»ä»ç²—åˆ°ç»†çš„æ¨ç†æ¨¡å¼ï¼šæ—©æœŸçš„Transformerå±‚ä¼šå¹¿æ³›åœ°å…³æ³¨æ•´ä¸ªæ–‡æ¡£ï¼Œè€Œè¾ƒæ·±çš„å±‚åˆ™ä¸“æ³¨äºç›¸å…³çš„è¯æ®é¡µé¢ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬è®¤ä¸ºå¯ä»¥åˆ©ç”¨MLLMsçš„å›ºæœ‰è¯æ®å®šä½èƒ½åŠ›åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œæ£€ç´¢ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„é•¿æ–‡æ¡£ç†è§£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†URaGï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œå°†æ£€ç´¢å’Œç”Ÿæˆç»Ÿä¸€åˆ°ä¸€ä¸ªå•ä¸€çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ã€‚URaGå¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„è·¨æ¨¡æ€æ£€ç´¢æ¨¡å—ï¼Œè¯¥æ¨¡å—å°†æ—©æœŸçš„Transformerå±‚è½¬åŒ–ä¸ºé«˜æ•ˆçš„è¯æ®é€‰æ‹©å™¨ï¼Œè¯†åˆ«å’Œä¿ç•™æœ€ç›¸å…³çš„é¡µé¢ï¼ŒåŒæ—¶ä¸¢å¼ƒä¸ç›¸å…³å†…å®¹ã€‚è¿™ç§è®¾è®¡ä½¿è¾ƒæ·±çš„å±‚èƒ½å¤Ÿé›†ä¸­è®¡ç®—èµ„æºåœ¨é‡è¦ä¿¡æ¯ä¸Šï¼Œæé«˜äº†å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒURaGè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘äº†44-56%çš„è®¡ç®—å¼€é”€ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/shi-yx/URaG">https://github.com/shi-yx/URaG</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10552v1">PDF</a> Accepted by AAAI 2026 (Oral)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†é•¿æ–‡æ¡£ç†è§£æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶URaGæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚URaGå°†æ£€ç´¢å’Œç”Ÿæˆæ•´åˆåœ¨ä¸€ä¸ªå•ä¸€çš„MLLMä¸­ï¼Œé€šè¿‡å¼•å…¥ä¸€ä¸ªè½»é‡çº§çš„è·¨æ¨¡æ€æ£€ç´¢æ¨¡å—ï¼Œåˆ©ç”¨æ—©æœŸTransformerå±‚çš„è¯æ®å®šä½èƒ½åŠ›è¿›è¡Œæ£€ç´¢ã€‚è¿™å‡å°‘äº†è®¡ç®—èµ„æºçš„æµªè´¹ï¼Œæé«˜äº†å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½å¹¶å‡å°‘äº†è®¡ç®—å¼€é”€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨å¤„ç†é•¿æ–‡æ¡£ç†è§£æ—¶é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¿¡æ¯å¹²æ‰°å’Œè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦åŒ…æ‹¬ç‰ºç‰²ç»†èŠ‚ä¿¡æ¯çš„ä»¤ç‰Œå‹ç¼©å’Œå¢åŠ ç³»ç»Ÿå¤æ‚åº¦çš„å¼•å…¥å¤–éƒ¨æ£€ç´¢å™¨ã€‚</li>
<li>MLLMså±•ç°å‡ºäººç±»ç²—åˆ°ç»†çš„æ¨ç†æ¨¡å¼ï¼Œæ—©æœŸTransformerå±‚å¹¿æ³›å…³æ³¨æ–‡æ¡£ï¼Œè€Œæ·±å±‚åˆ™èšç„¦äºç›¸å…³è¯æ®é¡µé¢ã€‚</li>
<li>URaGæ¡†æ¶åˆ©ç”¨MLLMsçš„å›ºæœ‰è¯æ®å®šä½èƒ½åŠ›ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ‰§è¡Œæ£€ç´¢ã€‚</li>
<li>URaGå¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„è·¨æ¨¡æ€æ£€ç´¢æ¨¡å—ï¼Œå°†æ—©æœŸTransformerå±‚è½¬åŒ–ä¸ºé«˜æ•ˆçš„è¯æ®é€‰æ‹©å™¨ï¼Œè¯†åˆ«å¹¶ä¿ç•™æœ€ç›¸å…³çš„é¡µé¢ï¼ŒåŒæ—¶ä¸¢å¼ƒä¸ç›¸å…³çš„ä¿¡æ¯ã€‚</li>
<li>è¿™ç§è®¾è®¡ä½¿æ·±å±‚ä¸“æ³¨äºé‡è¦ä¿¡æ¯ï¼Œæé«˜äº†å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>URaGè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å‡å°‘è®¡ç®—å¼€é”€æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10552">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0fab4afb9f27601b5df3da23af3ddfa4" align="middle">
<img src="https://picx.zhimg.com/v2-69641c5d485e9bdcf4534aa0df8371fb" align="middle">
<img src="https://picx.zhimg.com/v2-c9b74aac15a4395234778e31cb60754f" align="middle">
<img src="https://picx.zhimg.com/v2-a91f9004096b1aaf4e3b0e08e8e16731" align="middle">
<img src="https://picx.zhimg.com/v2-0b5a3a3fc153fd8a9f920ed08750ef7a" align="middle">
<img src="https://picx.zhimg.com/v2-2f93f7e074049ab1c3b3ed512ae9d2d5" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LocalBench-Benchmarking-LLMs-on-County-Level-Local-Knowledge-and-Reasoning"><a href="#LocalBench-Benchmarking-LLMs-on-County-Level-Local-Knowledge-and-Reasoning" class="headerlink" title="LocalBench: Benchmarking LLMs on County-Level Local Knowledge and Reasoning"></a>LocalBench: Benchmarking LLMs on County-Level Local Knowledge and Reasoning</h2><p><strong>Authors:Zihan Gao, Yifei Xu, Jacob Thebault-Spieker</strong></p>
<p>Large language models (LLMs) have been widely evaluated on macro-scale geographic tasks, such as global factual recall, event summarization, and regional reasoning. Yet, their ability to handle hyper-local knowledge remains poorly understood. This gap is increasingly consequential as real-world applications, from civic platforms to community journalism, demand AI systems that can reason about neighborhood-specific dynamics, cultural narratives, and local governance. Existing benchmarks fall short in capturing this complexity, often relying on coarse-grained data or isolated references. We present LocalBench, the first benchmark designed to systematically evaluate LLMs on county-level local knowledge across the United States. Grounded in the Localness Conceptual Framework, LocalBench includes 14,782 validated question-answer pairs across 526 U.S. counties in 49 states, integrating diverse sources such as Census statistics, local subreddit discourse, and regional news. It spans physical, cognitive, and relational dimensions of locality. Using LocalBench, we evaluate 13 state-of-the-art LLMs under both closed-book and web-augmented settings. Our findings reveal critical limitations: even the best-performing models reach only 56.8% accuracy on narrative-style questions and perform below 15.5% on numerical reasoning. Moreover, larger model size and web augmentation do not guarantee better performance, for example, search improves Geminiâ€™s accuracy by +13.6%, but reduces GPT-series performance by -11.4%. These results underscore the urgent need for language models that can support equitable, place-aware AI systems: capable of engaging with the diverse, fine-grained realities of local communities across geographic and cultural contexts.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®è§‚åœ°ç†ä»»åŠ¡æ–¹é¢å·²å¾—åˆ°å¹¿æ³›è¯„ä¼°ï¼Œå¦‚å…¨çƒäº‹å®å›å¿†ã€äº‹ä»¶æ‘˜è¦å’ŒåŒºåŸŸæ¨ç†ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¤„ç†è¶…æœ¬åœ°çŸ¥è¯†çš„èƒ½åŠ›ä»ç¼ºä¹å……åˆ†äº†è§£ã€‚éšç€ç°å®ä¸–ç•Œåº”ç”¨ï¼ˆä»å…¬æ°‘å¹³å°åˆ°ç¤¾åŒºæ–°é—»ï¼‰çš„éœ€æ±‚ä¸æ–­å¢é•¿ï¼Œè¦æ±‚äººå·¥æ™ºèƒ½ç³»ç»Ÿèƒ½å¤Ÿæ¨ç†å…³äºç‰¹å®šé‚»åŸŸçš„åŠ¨åŠ›ã€æ–‡åŒ–å™äº‹å’Œå½“åœ°æ²»ç†ç­‰äº‹åŠ¡ï¼Œè¿™ä¸€å·®è·å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•åœ¨æ•æ‰è¿™ç§å¤æ‚æ€§æ–¹é¢è¡¨ç°ä¸è¶³ï¼Œé€šå¸¸ä¾èµ–äºç²—ç•¥çš„æ•°æ®æˆ–å­¤ç«‹çš„å‚è€ƒã€‚æˆ‘ä»¬æ¨å‡ºLocalBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°ç¾å›½å¿çº§æœ¬åœ°çŸ¥è¯†çš„LLMçš„åŸºå‡†æµ‹è¯•ã€‚LocalBenchåŸºäºæœ¬åœ°æ¦‚å¿µæ¡†æ¶ï¼ŒåŒ…å«ç¾å›½49ä¸ªå·çš„526ä¸ªå¿å¸‚çš„14,782ä¸ªç»è¿‡éªŒè¯çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œèåˆäº†è¯¸å¦‚äººå£æ™®æŸ¥ç»Ÿè®¡ã€æœ¬åœ°Redditè®ºå›è®¨è®ºå’ŒåŒºåŸŸæ–°é—»ç­‰å„ç§æ¥æºã€‚å®ƒæ¶µç›–äº†æœ¬åœ°çš„ç‰©ç†ã€è®¤çŸ¥å’Œå…³ç³»ç»´åº¦ã€‚ä½¿ç”¨LocalBenchï¼Œæˆ‘ä»¬åœ¨å°é—­ä¹¦ç±å’Œç½‘ç»œå¢å¼ºä¸¤ç§ç¯å¢ƒä¸‹è¯„ä¼°äº†æœ€å…ˆè¿›çš„13ä¸ªLLMã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ˜¾ç¤ºå‡ºé‡å¤§å±€é™æ€§ï¼šå³ä½¿åœ¨å™è¿°æ€§é—®é¢˜ä¸Šï¼Œè¡¨ç°æœ€ä½³çš„æ¨¡å‹ä¹Ÿåªæœ‰56.8%çš„å‡†ç¡®ç‡ï¼Œè€Œåœ¨æ•°å€¼æ¨ç†æ–¹é¢çš„å‡†ç¡®ç‡ä½äº15.5%ã€‚æ­¤å¤–ï¼Œæ›´å¤§çš„æ¨¡å‹å°ºå¯¸å’Œç½‘ç»œå¢å¼ºå¹¶ä¸ä¸€å®šèƒ½ä¿è¯æ›´å¥½çš„æ€§èƒ½ï¼Œä¾‹å¦‚æœç´¢åŠŸèƒ½æé«˜äº†Geminiçš„å‡†ç¡®ç‡+13.6%ï¼Œä½†é™ä½äº†GPTç³»åˆ—çš„æ€§èƒ½-11.4%ã€‚è¿™äº›ç»“æœçªæ˜¾äº†å¯¹æ”¯æŒå…¬å¹³ã€æ³¨é‡åœºæ‰€çš„AIç³»ç»Ÿçš„è¯­è¨€æ¨¡å‹çš„è¿«åˆ‡éœ€æ±‚ï¼šèƒ½å¤Ÿå‚ä¸å¤„ç†è·¨åœ°ç†å’Œæ–‡åŒ–èƒŒæ™¯çš„ä¸åŒåœ°åŒºçš„ç»†å¾®ç°å®ç¤¾åŒºæƒ…å†µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10459v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®è§‚åœ°ç†ä»»åŠ¡ä¸Šçš„è¯„ä¼°å·²ç»ç›¸å½“å¹¿æ³›ï¼Œä½†åœ¨å¤„ç†è¶…æœ¬åœ°çŸ¥è¯†æ–¹é¢çš„èƒ½åŠ›å°šå¾…æ·±å…¥äº†è§£ã€‚éšç€ä»å…¬æ°‘å¹³å°åˆ°ç¤¾åŒºæ–°é—»ç­‰ç°å®åº”ç”¨éœ€æ±‚æ—¥ç›Šå¢é•¿ï¼Œè¦æ±‚äººå·¥æ™ºèƒ½ç³»ç»Ÿèƒ½å¤Ÿæ¨ç†å‡ºé‚»é‡ŒåŠ¨æ€ã€æ–‡åŒ–å™äº‹å’Œå½“åœ°æ²»ç†ç­‰æ–¹é¢çš„æœ¬åœ°ç‰¹å®šçŸ¥è¯†ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•æ•æ‰è¿™ç§å¤æ‚æ€§ï¼Œå¾€å¾€ä¾èµ–äºç²—ç•¥çš„æ•°æ®æˆ–å­¤ç«‹çš„å¼•ç”¨ã€‚æœ¬æ–‡æå‡ºäº†LocalBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°LLMåœ¨ç¾å›½å¿çº§æœ¬åœ°çŸ¥è¯†æ–¹é¢çš„åŸºå‡†æµ‹è¯•ã€‚LocalBenchåŸºäºæœ¬åœ°æ¦‚å¿µæ¡†æ¶ï¼ŒåŒ…å«ç¾å›½526ä¸ªå¿ã€49å·çš„14,782ä¸ªç»è¿‡éªŒè¯çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œèåˆäº†äººå£æ™®æŸ¥ç»Ÿè®¡ã€æœ¬åœ°Redditè®ºå›è®¨è®ºå’ŒåŒºåŸŸæ–°é—»ç­‰å¤šå…ƒæ¥æºã€‚å®ƒæ¶µç›–äº†æœ¬åœ°çš„ç‰©ç†ã€è®¤çŸ¥å’Œå…³ç³»ç»´åº¦ã€‚ä½¿ç”¨LocalBenchï¼Œæˆ‘ä»¬å¯¹13æ¬¾æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å°é—­ä¹¦ç±å’Œç½‘ç»œå¢å¼ºä¸¤ç§ç¯å¢ƒä¸‹çš„è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯æœ€å¥½çš„æ¨¡å‹åœ¨å™äº‹å¼é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡ä¹Ÿåªæœ‰56.8%ï¼Œæ•°å€¼æ¨ç†å‡†ç¡®ç‡ä½äº15.5%ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è§„æ¨¡çš„å¤§å°å’Œç½‘ç»œå¢å¼ºå¹¶ä¸ä¿è¯æ€§èƒ½æå‡ï¼Œæœç´¢åŠŸèƒ½æé«˜äº†Geminiæ¨¡å‹çš„å‡†ç¡®ç‡+13.6%ï¼Œä½†é™ä½äº†GPTç³»åˆ—æ€§èƒ½-11.4%ã€‚è¿™å‡¸æ˜¾äº†å¼€å‘æ”¯æŒå…¬å¹³ã€ç«‹è¶³å½“åœ°çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„è¯­è¨€æ¨¡å‹çš„ç´§è¿«éœ€æ±‚ï¼Œè¦æ±‚è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåº”å¯¹ä¸åŒåœ°ç†å’Œæ–‡åŒ–èƒŒæ™¯ä¸‹çš„æœ¬åœ°ç¤¾åŒºå¤æ‚ç°å®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¶…æœ¬åœ°çŸ¥è¯†æ–¹é¢çš„èƒ½åŠ›å°šå¾…æ·±å…¥äº†è§£ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•å…¨é¢è¯„ä¼°LLMåœ¨å¿çº§æœ¬åœ°çŸ¥è¯†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>LocalBenchæ˜¯é¦–ä¸ªæ—¨åœ¨è¯„ä¼°LLMåœ¨ç¾å›½å¿çº§æœ¬åœ°çŸ¥è¯†æ–¹é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†ç‰©ç†ã€è®¤çŸ¥å’Œå…³ç³»ç»´åº¦ã€‚</li>
<li>åœ¨å™äº‹å¼é—®é¢˜å’Œæ•°å€¼æ¨ç†æ–¹é¢ï¼Œç°æœ‰æ¨¡å‹çš„å‡†ç¡®ç‡è¾ƒä½ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡å’Œç½‘ç»œå¢å¼ºå¹¶ä¸ä¿è¯æ€§èƒ½æå‡ã€‚</li>
<li>æœç´¢åŠŸèƒ½å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“å› æ¨¡å‹è€Œå¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10459">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33a74e55b8e61d3eb97de1b84f90f1d7" align="middle">
<img src="https://picx.zhimg.com/v2-bcc3f1f4be90be359c62fc25e23b38f5" align="middle">
<img src="https://picx.zhimg.com/v2-f03020005919a185eb94ae200bba48f0" align="middle">
<img src="https://picx.zhimg.com/v2-fe00abe640c5d314a4d1a712f429d6bf" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Facial-R1-Aligning-Reasoning-and-Recognition-for-Facial-Emotion-Analysis"><a href="#Facial-R1-Aligning-Reasoning-and-Recognition-for-Facial-Emotion-Analysis" class="headerlink" title="Facial-R1: Aligning Reasoning and Recognition for Facial Emotion Analysis"></a>Facial-R1: Aligning Reasoning and Recognition for Facial Emotion Analysis</h2><p><strong>Authors:Jiulong Wu, Yucheng Shen, Lingyong Yan, Haixin Sun, Deguo Xia, Jizhou Huang, Min Cao</strong></p>
<p>Facial Emotion Analysis (FEA) extends traditional facial emotion recognition by incorporating explainable, fine-grained reasoning. The task integrates three subtasks: emotion recognition, facial Action Unit (AU) recognition, and AU-based emotion reasoning to model affective states jointly. While recent approaches leverage Vision-Language Models (VLMs) and achieve promising results, they face two critical limitations: (1) hallucinated reasoning, where VLMs generate plausible but inaccurate explanations due to insufficient emotion-specific knowledge; and (2) misalignment between emotion reasoning and recognition, caused by fragmented connections between observed facial features and final labels. We propose Facial-R1, a three-stage alignment framework that effectively addresses both challenges with minimal supervision. First, we employ instruction fine-tuning to establish basic emotional reasoning capability. Second, we introduce reinforcement training guided by emotion and AU labels as reward signals, which explicitly aligns the generated reasoning process with the predicted emotion. Third, we design a data synthesis pipeline that iteratively leverages the prior stages to expand the training dataset, enabling scalable self-improvement of the model. Built upon this framework, we introduce FEA-20K, a benchmark dataset comprising 17,737 training and 1,688 test samples with fine-grained emotion analysis annotations. Extensive experiments across eight standard benchmarks demonstrate that Facial-R1 achieves state-of-the-art performance in FEA, with strong generalization and robust interpretability.</p>
<blockquote>
<p>é¢éƒ¨æƒ…ç»ªåˆ†æï¼ˆFEAï¼‰é€šè¿‡èå…¥å¯è§£é‡Šã€ç²¾ç»†çš„æ¨ç†ï¼Œæ‰©å±•äº†ä¼ ç»Ÿçš„é¢éƒ¨æƒ…ç»ªè¯†åˆ«ã€‚è¯¥ä»»åŠ¡èåˆäº†ä¸‰ä¸ªå­ä»»åŠ¡ï¼šæƒ…ç»ªè¯†åˆ«ã€é¢éƒ¨åŠ¨ä½œå•å…ƒï¼ˆAUï¼‰è¯†åˆ«ï¼Œä»¥åŠåŸºäºAUçš„æƒ…ç»ªæ¨ç†ï¼Œä»¥è”åˆå»ºæ¨¡æƒ…æ„ŸçŠ¶æ€ã€‚å°½ç®¡æœ€è¿‘çš„æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¹¶å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†å®ƒä»¬é¢ä¸´ä¸¤ä¸ªå…³é”®å±€é™æ€§ï¼šï¼ˆ1ï¼‰è™šå¹»æ¨ç†ï¼Œç”±äºç¼ºä¹ç‰¹å®šçš„æƒ…æ„ŸçŸ¥è¯†ï¼ŒVLMsä¼šäº§ç”Ÿåˆç†çš„ä½†ä¸å‡†ç¡®çš„è§£é‡Šï¼›ï¼ˆ2ï¼‰æƒ…ç»ªæ¨ç†å’Œè¯†åˆ«ä¹‹é—´çš„ä¸åŒ¹é…ï¼Œè¿™æ˜¯ç”±äºè§‚å¯Ÿåˆ°çš„é¢éƒ¨ç‰¹å¾å’Œæœ€ç»ˆæ ‡ç­¾ä¹‹é—´ç¼ºä¹è¿è´¯çš„è”ç³»ã€‚æˆ‘ä»¬æå‡ºäº†Facial-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µå¯¹é½æ¡†æ¶ï¼Œä»¥æœ€å°çš„ç›‘ç£æœ‰æ•ˆåœ°è§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡æŒ‡ä»¤å¾®è°ƒå»ºç«‹åŸºæœ¬çš„æƒ…ç»ªæ¨ç†èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¼ºåŒ–è®­ç»ƒï¼Œä»¥æƒ…æ„Ÿå’ŒAUæ ‡ç­¾ä½œä¸ºå¥–åŠ±ä¿¡å·è¿›è¡ŒæŒ‡å¯¼ï¼Œæ˜¾å¼åœ°å°†ç”Ÿæˆçš„æ¨ç†è¿‡ç¨‹ä¸é¢„æµ‹çš„æƒ…ç»ªå¯¹é½ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ•°æ®åˆæˆç®¡é“ï¼Œè¯¥ç®¡é“é€šè¿‡è¿­ä»£åˆ©ç”¨å…ˆå‰çš„é˜¶æ®µæ¥æ‰©å±•è®­ç»ƒæ•°æ®é›†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå®ç°å¯æ‰©å±•çš„è‡ªæˆ‘æ”¹è¿›ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†FEA-20Kæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«17,737ä¸ªè®­ç»ƒæ ·æœ¬å’Œ1,688ä¸ªæµ‹è¯•æ ·æœ¬çš„åŸºå‡†æ•°æ®é›†ï¼Œå…·æœ‰ç²¾ç»†çš„æƒ…ç»ªåˆ†ææ³¨é‡Šã€‚åœ¨å…«ä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFacial-R1åœ¨é¢éƒ¨æƒ…ç»ªåˆ†ææ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥çš„å¯è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10254v1">PDF</a> This paper has been accepted by AAAI 2026. 16 pages, 3 figures, 10 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¢éƒ¨æƒ…ç»ªåˆ†æï¼ˆFEAï¼‰å¦‚ä½•é€šè¿‡ç»“åˆè§£é‡Šæ€§ã€ç²¾ç»†çš„æ¨ç†æ¥æ‰©å±•ä¼ ç»Ÿçš„é¢éƒ¨æƒ…ç»ªè¯†åˆ«ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µçš„é¢éƒ¨æƒ…ç»ªåˆ†ææ¡†æ¶â€”â€”Facial-R1ï¼Œè¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é€šè¿‡æƒ…æ„ŸæŒ‡ä»¤å¾®è°ƒå»ºç«‹åŸºæœ¬æƒ…æ„Ÿæ¨ç†èƒ½åŠ›ï¼Œå¼•å…¥å¼ºåŒ–è®­ç»ƒæ˜ç¡®å¯¹é½ç”Ÿæˆæ¨ç†è¿‡ç¨‹å’Œé¢„æµ‹æƒ…æ„Ÿï¼Œä»¥åŠè®¾è®¡æ•°æ®åˆæˆç®¡é“å®ç°æ¨¡å‹çš„è‡ªæˆ‘æå‡ã€‚æ–‡ç« è¿˜ä»‹ç»äº†æ–°çš„FEAæ•°æ®é›†FEA-20Kçš„åº”ç”¨å’Œå¹¿æ³›å®éªŒã€‚ç»“æœæ˜¾ç¤ºï¼ŒFacial-R1åœ¨FEAé¢†åŸŸå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥çš„å¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FEAç»“åˆäº†è§£é‡Šæ€§ã€ç²¾ç»†çš„æ¨ç†æ¥æ‰©å±•ä¼ ç»Ÿçš„é¢éƒ¨æƒ…ç»ªè¯†åˆ«ã€‚</li>
<li>Facial-R1æ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µçš„é¢éƒ¨æƒ…ç»ªåˆ†ææ¡†æ¶ï¼Œç”¨äºè§£å†³æƒ…æ„Ÿåˆ†æçš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>Facial-R1ä½¿ç”¨æƒ…æ„ŸæŒ‡ä»¤å¾®è°ƒã€å¼ºåŒ–è®­ç»ƒå’Œæ•°æ®åˆæˆç®¡é“ç­‰æŠ€æœ¯æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>FEA-20Kæ˜¯ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œç”¨äºé¢éƒ¨æƒ…ç»ªåˆ†æä»»åŠ¡ï¼ŒåŒ…å«ç²¾ç»†çš„æƒ…ç»ªåˆ†ææ³¨é‡Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e91f06ac4452786ba200d35837bbb88a" align="middle">
<img src="https://picx.zhimg.com/v2-2d8d6d4362dc3f359af5b9cc4b5f4c12" align="middle">
<img src="https://picx.zhimg.com/v2-840c2e6c85def69928e4f0e6322abb9f" align="middle">
<img src="https://picx.zhimg.com/v2-b32c7470217e9ac2a6eba93366d4f731" align="middle">
<img src="https://picx.zhimg.com/v2-0cb074295197638f7d6be8f0b3edcda7" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LangGPS-Language-Separability-Guided-Data-Pre-Selection-for-Joint-Multilingual-Instruction-Tuning"><a href="#LangGPS-Language-Separability-Guided-Data-Pre-Selection-for-Joint-Multilingual-Instruction-Tuning" class="headerlink" title="LangGPS: Language Separability Guided Data Pre-Selection for Joint Multilingual Instruction Tuning"></a>LangGPS: Language Separability Guided Data Pre-Selection for Joint Multilingual Instruction Tuning</h2><p><strong>Authors:Yangfan Ye, Xiaocheng Feng, Xiachong Feng, Lei Huang, Weitao Ma, Qichen Hong, Yunfei Lu, Duyu Tang, Dandan Tu, Bing Qin</strong></p>
<p>Joint multilingual instruction tuning is a widely adopted approach to improve the multilingual instruction-following ability and downstream performance of large language models (LLMs), but the resulting multilingual capability remains highly sensitive to the composition and selection of the training data. Existing selection methods, often based on features like text quality, diversity, or task relevance, typically overlook the intrinsic linguistic structure of multilingual data. In this paper, we propose LangGPS, a lightweight two-stage pre-selection framework guided by language separability which quantifies how well samples in different languages can be distinguished in the modelâ€™s representation space. LangGPS first filters training data based on separability scores and then refines the subset using existing selection methods. Extensive experiments across six benchmarks and 22 languages demonstrate that applying LangGPS on top of existing selection methods improves their effectiveness and generalizability in multilingual training, especially for understanding tasks and low-resource languages. Further analysis reveals that highly separable samples facilitate the formation of clearer language boundaries and support faster adaptation, while low-separability samples tend to function as bridges for cross-lingual alignment. Besides, we also find that language separability can serve as an effective signal for multilingual curriculum learning, where interleaving samples with diverse separability levels yields stable and generalizable gains. Together, we hope our work offers a new perspective on data utility in multilingual contexts and support the development of more linguistically informed LLMs.</p>
<blockquote>
<p>è”åˆå¤šè¯­ç§æŒ‡ä»¤å¾®è°ƒæ˜¯æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šè¯­ç§æŒ‡ä»¤éµå¾ªèƒ½åŠ›å’Œä¸‹æ¸¸æ€§èƒ½çš„ä¸€ç§å¹¿æ³›é‡‡ç”¨çš„æ–¹æ³•ï¼Œä½†æ‰€å¾—çš„å¤šè¯­ç§èƒ½åŠ›ä»ç„¶é«˜åº¦ä¾èµ–äºè®­ç»ƒæ•°æ®çš„ç»„æˆå’Œé€‰æ‹©ã€‚ç°æœ‰çš„é€‰æ‹©æ–¹æ³•é€šå¸¸åŸºäºæ–‡æœ¬è´¨é‡ã€å¤šæ ·æ€§æˆ–ä»»åŠ¡ç›¸å…³æ€§ç­‰ç‰¹å¾ï¼Œå¾€å¾€å¿½ç•¥äº†å¤šè¯­ç§æ•°æ®å†…åœ¨çš„è¯­è¨€ç»“æ„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†LangGPSï¼Œè¿™æ˜¯ä¸€ä¸ªç”±è¯­è¨€å¯åˆ†ç¦»æ€§å¼•å¯¼çš„è½»é‡çº§ä¸¤é˜¶æ®µé¢„é€‰æ‹©æ¡†æ¶ï¼Œå®ƒé‡åŒ–ä¸åŒè¯­è¨€ä¸­çš„æ ·æœ¬åœ¨æ¨¡å‹è¡¨ç¤ºç©ºé—´ä¸­èƒ½åŒºåˆ†å¼€æ¥çš„ç¨‹åº¦ã€‚LangGPSé¦–å…ˆæ ¹æ®å¯åˆ†ç¦»æ€§åˆ†æ•°è¿‡æ»¤è®­ç»ƒæ•°æ®ï¼Œç„¶åä½¿ç”¨ç°æœ‰çš„é€‰æ‹©æ–¹æ³•è¿›è¡Œç»†åŒ–ã€‚åœ¨å…­ä¸ªåŸºå‡†æµ‹è¯•å’Œ22ç§è¯­è¨€çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œåœ¨ç°æœ‰é€‰æ‹©æ–¹æ³•ä¹‹ä¸Šåº”ç”¨LangGPSæé«˜äº†å…¶åœ¨å¤šè¯­ç§è®­ç»ƒä¸­çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£ä»»åŠ¡å’Œä½èµ„æºè¯­è¨€æ–¹é¢ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œé«˜åº¦å¯åˆ†ç¦»çš„æ ·æœ¬æœ‰åŠ©äºå½¢æˆæ›´æ¸…æ™°çš„è¯­è¨€è¾¹ç•Œå¹¶æ”¯æŒæ›´å¿«çš„é€‚åº”ï¼Œè€Œä½å¯åˆ†ç¦»æ€§çš„æ ·æœ¬å¾€å¾€ä½œä¸ºè·¨è¯­è¨€å¯¹é½çš„æ¡¥æ¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°è¯­è¨€å¯åˆ†ç¦»æ€§å¯ä»¥ä½œä¸ºå¤šè¯­ç§è¯¾ç¨‹å­¦ä¹ çš„æœ‰æ•ˆä¿¡å·ï¼Œå…¶ä¸­äº¤é”™ä¸åŒå¯åˆ†ç¦»æ€§æ°´å¹³çš„æ ·æœ¬ä¼šäº§ç”Ÿç¨³å®šå’Œå¯æ¨å¹¿çš„æ”¶ç›Šã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå¤šè¯­ç§ç¯å¢ƒä¸‹çš„æ•°æ®æ•ˆç”¨æä¾›æ–°çš„è§†è§’ï¼Œå¹¶æ”¯æŒæ›´å…·è¯­è¨€æ„è¯†çš„LLMçš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10229v1">PDF</a> AAAI2026 Main Track Accepted</p>
<p><strong>Summary</strong></p>
<p>è”åˆå¤šè¯­ç§æ•™å­¦è°ƒèŠ‚èƒ½æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„è·¨è¯­ç§æŒ‡ä»¤ç†è§£å’Œä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œä½†åœ¨è®­ç»ƒæ•°æ®çš„ç»„åˆå’Œç­›é€‰æ–¹é¢éå¸¸æ•æ„Ÿã€‚æœ¬æ–‡æå‡ºäº†åŸºäºè¯­è¨€å¯åˆ†æ€§çš„LangGPSæ¡†æ¶ï¼Œä»¥é‡åŒ–ä¸åŒè¯­è¨€æ ·æœ¬åœ¨æ¨¡å‹è¡¨å¾ç©ºé—´ä¸­çš„åŒºåˆ†åº¦ã€‚å®éªŒè¯æ˜ï¼Œåœ¨ç°æœ‰ç­›é€‰æ–¹æ³•çš„åŸºç¡€ä¸Šåº”ç”¨LangGPSèƒ½æé«˜å…¶æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£å’Œä½èµ„æºè¯­è¨€æ–¹é¢ã€‚æ­¤å¤–ï¼Œè¿˜å‘ç°é«˜åº¦å¯åˆ†çš„æ ·æœ¬æœ‰åŠ©äºå½¢æˆæ›´æ¸…æ™°çš„è¯­è¨€è¾¹ç•Œå’Œæ”¯æŒå¿«é€Ÿé€‚åº”ï¼Œè€Œä½å¯åˆ†åº¦çš„æ ·æœ¬åˆ™å€¾å‘äºä½œä¸ºè·¨è¯­ç§å¯¹é½çš„æ¡¥æ¢ã€‚è¿™ä¸ºå¤šè¯­ç§ç¯å¢ƒä¸‹çš„æ•°æ®æ•ˆç”¨æä¾›äº†æ–°çš„è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è”åˆå¤šè¯­ç§æ•™å­¦è°ƒèŠ‚å¹¿æ³›åº”ç”¨äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„è·¨è¯­ç§æŒ‡ä»¤è·Ÿéšèƒ½åŠ›å’Œä¸‹æ¸¸æ€§èƒ½ã€‚</li>
<li>è®­ç»ƒæ•°æ®çš„ç»„åˆå’Œç­›é€‰å¯¹å¤šè¯­ç§èƒ½åŠ›çš„å½±å“è‡³å…³é‡è¦ã€‚</li>
<li>LangGPSæ¡†æ¶åŸºäºè¯­è¨€å¯åˆ†æ€§è¿›è¡Œè®­ç»ƒæ•°æ®é¢„ç­›é€‰ï¼Œä»¥æé«˜æ¨¡å‹çš„åŒºåˆ†èƒ½åŠ›ã€‚</li>
<li>LangGPSèƒ½æé«˜ç°æœ‰ç­›é€‰æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç†è§£å’Œä½èµ„æºè¯­è¨€æ–¹é¢ã€‚</li>
<li>é«˜åº¦å¯åˆ†çš„æ ·æœ¬æœ‰åŠ©äºå½¢æˆæ›´æ¸…æ™°çš„è¯­è¨€è¾¹ç•Œå’Œæ”¯æŒå¿«é€Ÿé€‚åº”æ¨¡å‹ã€‚</li>
<li>ä½å¯åˆ†åº¦çš„æ ·æœ¬åœ¨å¤šè¯­ç§å¯¹é½ä¸­èµ·åˆ°æ¡¥æ¢ä½œç”¨ã€‚</li>
<li>è¯­è¨€å¯åˆ†æ€§å¯ä»¥ä½œä¸ºå¤šè¯­ç§è¯¾ç¨‹å­¦ä¹ çš„æœ‰æ•ˆä¿¡å·ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10229">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-28210da68fdce116049a4835ee8434a1" align="middle">
<img src="https://picx.zhimg.com/v2-b66020b2960414ab34623f088369cb94" align="middle">
<img src="https://picx.zhimg.com/v2-397d4802540ade8009566b4b5b55a19d" align="middle">
<img src="https://picx.zhimg.com/v2-84c0b95f01d785bfe3d0e686ca041393" align="middle">
<img src="https://picx.zhimg.com/v2-54b6d4aa4a56f3a445fdd4ff5ba2c7ad" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MTAttack-Multi-Target-Backdoor-Attacks-against-Large-Vision-Language-Models"><a href="#MTAttack-Multi-Target-Backdoor-Attacks-against-Large-Vision-Language-Models" class="headerlink" title="MTAttack: Multi-Target Backdoor Attacks against Large Vision-Language Models"></a>MTAttack: Multi-Target Backdoor Attacks against Large Vision-Language Models</h2><p><strong>Authors:Zihan Wang, Guansong Pang, Wenjun Miao, Jin Zheng, Xiao Bai</strong></p>
<p>Recent advances in Large Visual Language Models (LVLMs) have demonstrated impressive performance across various vision-language tasks by leveraging large-scale image-text pretraining and instruction tuning. However, the security vulnerabilities of LVLMs have become increasingly concerning, particularly their susceptibility to backdoor attacks. Existing backdoor attacks focus on single-target attacks, i.e., targeting a single malicious output associated with a specific trigger. In this work, we uncover multi-target backdoor attacks, where multiple independent triggers corresponding to different attack targets are added in a single pass of training, posing a greater threat to LVLMs in real-world applications. Executing such attacks in LVLMs is challenging since there can be many incorrect trigger-target mappings due to severe feature interference among different triggers. To address this challenge, we propose MTAttack, the first multi-target backdoor attack framework for enforcing accurate multiple trigger-target mappings in LVLMs. The core of MTAttack is a novel optimization method with two constraints, namely Proxy Space Partitioning constraint and Trigger Prototype Anchoring constraint. It jointly optimizes multiple triggers in the latent space, with each trigger independently mapping clean images to a unique proxy class while at the same time guaranteeing their separability. Experiments on popular benchmarks demonstrate a high success rate of MTAttack for multi-target attacks, substantially outperforming existing attack methods. Furthermore, our attack exhibits strong generalizability across datasets and robustness against backdoor defense strategies. These findings highlight the vulnerability of LVLMs to multi-target backdoor attacks and underscore the urgent need for mitigating such threats. Code is available at <a target="_blank" rel="noopener" href="https://github.com/mala-lab/MTAttack">https://github.com/mala-lab/MTAttack</a>.</p>
<blockquote>
<p>æœ€è¿‘çš„å¤§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰è¿›å±•é€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡å›¾åƒæ–‡æœ¬é¢„è®­ç»ƒä»¥åŠæŒ‡ä»¤å¾®è°ƒï¼Œåœ¨å„ç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼ŒLVLMsçš„å®‰å…¨æ¼æ´è¶Šæ¥è¶Šä»¤äººæ‹…å¿§ï¼Œå°¤å…¶æ˜¯å®ƒä»¬å®¹æ˜“å—åˆ°åé—¨æ”»å‡»ã€‚ç°æœ‰çš„åé—¨æ”»å‡»ä¸»è¦é›†ä¸­åœ¨å•ç›®æ ‡æ”»å‡»ä¸Šï¼Œå³é’ˆå¯¹ç‰¹å®šè§¦å‘å› ç´ å…³è”çš„å•ä¸ªæ¶æ„è¾“å‡ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†å¤šç›®æ ‡åé—¨æ”»å‡»ï¼Œå³åœ¨å•æ¬¡è®­ç»ƒä¸­æ·»åŠ ä¸ä¸åŒæ”»å‡»ç›®æ ‡å¯¹åº”çš„å¤šä¸ªç‹¬ç«‹è§¦å‘å™¨ï¼Œå¯¹ç°å®ä¸–ç•Œåº”ç”¨çš„LVLMsæ„æˆäº†æ›´å¤§å¨èƒã€‚åœ¨LVLMsä¸­æ‰§è¡Œæ­¤ç±»æ”»å‡»å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºä¸åŒè§¦å‘å™¨ä¹‹é—´çš„ç‰¹å¾å¹²æ‰°ä¸¥é‡ï¼Œå¯èƒ½ä¼šæœ‰è®¸å¤šé”™è¯¯çš„è§¦å‘-ç›®æ ‡æ˜ å°„ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†MTAttackï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºåœ¨LVLMsä¸­å®ç°å‡†ç¡®çš„å¤šè§¦å‘-ç›®æ ‡æ˜ å°„çš„å¤šç›®æ ‡åé—¨æ”»å‡»æ¡†æ¶ã€‚MTAttackçš„æ ¸å¿ƒæ˜¯ä¸€ç§æ–°å‹ä¼˜åŒ–æ–¹æ³•ï¼Œå…·æœ‰ä¸¤ä¸ªçº¦æŸæ¡ä»¶ï¼Œå³ä»£ç†ç©ºé—´åˆ’åˆ†çº¦æŸå’Œè§¦å‘åŸå‹é”šå®šçº¦æŸã€‚å®ƒåœ¨æ½œåœ¨ç©ºé—´ä¸­è”åˆä¼˜åŒ–å¤šä¸ªè§¦å‘å™¨ï¼Œæ¯ä¸ªè§¦å‘å™¨å°†å¹²å‡€å›¾åƒç‹¬ç«‹æ˜ å°„åˆ°å”¯ä¸€çš„ä»£ç†ç±»åˆ«ï¼ŒåŒæ—¶ä¿è¯å®ƒä»¬çš„å¯åˆ†ç¦»æ€§ã€‚åœ¨æµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMTAttackå¯¹å¤šç›®æ ‡æ”»å‡»çš„æˆåŠŸç‡å¾ˆé«˜ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ”»å‡»æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ”»å‡»åœ¨æ•°æ®é›†ä¹‹é—´å…·æœ‰å¾ˆå¼ºçš„é€šç”¨æ€§ï¼Œå¹¶å¯¹æŠ—åé—¨é˜²å¾¡ç­–ç•¥è¡¨ç°å‡ºç¨³å¥æ€§ã€‚è¿™äº›å‘ç°çªæ˜¾äº†LVLMså¯¹å¤šç›®æ ‡åé—¨æ”»å‡»çš„è„†å¼±æ€§ï¼Œå¹¶å¼ºè°ƒäº†ç¼“è§£æ­¤ç±»å¨èƒçš„ç´§è¿«éœ€æ±‚ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mala-lab/MTAttack%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/mala-lab/MTAttackè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10098v1">PDF</a> AAAI2026, with supplementary material</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è·¨å„ç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸»è¦å¾—ç›Šäºå¤§è§„æ¨¡å›¾åƒæ–‡æœ¬é¢„è®­ç»ƒä¸æŒ‡ä»¤å¾®è°ƒã€‚ä½†LVLMsçš„å®‰å…¨æ¼æ´æ—¥ç›Šå¼•äººå…³æ³¨ï¼Œç‰¹åˆ«æ˜¯æ˜“å—åé—¨æ”»å‡»çš„å½±å“ã€‚ç°æœ‰åé—¨æ”»å‡»ä¸»è¦å…³æ³¨å•ç›®æ ‡æ”»å‡»ï¼Œå³é’ˆå¯¹ç‰¹å®šè§¦å‘å› ç´ å…³è”å•ä¸ªæ¶æ„è¾“å‡ºçš„æ”»å‡»ã€‚æœ¬ç ”ç©¶å‘ç°äº†å¤šç›®æ ‡åé—¨æ”»å‡»ï¼Œå³åœ¨å•æ¬¡è®­ç»ƒä¸­åŠ å…¥å¤šä¸ªç‹¬ç«‹è§¦å‘å› ç´ ï¼Œå¯¹åº”ä¸åŒæ”»å‡»ç›®æ ‡ï¼Œå¯¹LVLMsæ„æˆæ›´å¤§å¨èƒã€‚æ‰§è¡Œæ­¤ç±»æ”»å‡»åœ¨LVLMsä¸­é¢‡å…·æŒ‘æˆ˜ï¼Œå› ä¸ºä¸åŒè§¦å‘å› ç´ é—´ç‰¹å¾å¹²æ‰°ä¸¥é‡ï¼Œå¯¼è‡´è®¸å¤šé”™è¯¯çš„è§¦å‘-ç›®æ ‡æ˜ å°„ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºMTAttackï¼Œé¦–ä¸ªç”¨äºLVLMsçš„å¤šç›®æ ‡åé—¨æ”»å‡»æ¡†æ¶ï¼Œé€šè¿‡æ–°å‹ä¼˜åŒ–æ–¹æ³•å®ç°å‡†ç¡®çš„å¤šè§¦å‘-ç›®æ ‡æ˜ å°„ã€‚MTAttackçš„æ ¸å¿ƒåœ¨äºä¸¤ç§çº¦æŸçš„è”åˆä¼˜åŒ–ï¼šä»£ç†ç©ºé—´åˆ’åˆ†çº¦æŸå’Œè§¦å‘åŸå‹é”šå®šçº¦æŸã€‚å®ƒåœ¨æ½œåœ¨ç©ºé—´ä¸­è”åˆä¼˜åŒ–å¤šä¸ªè§¦å‘å™¨ï¼Œæ¯ä¸ªè§¦å‘å™¨å°†å¹²å‡€å›¾åƒç‹¬ç«‹æ˜ å°„åˆ°å”¯ä¸€ä»£ç†ç±»åˆ«ï¼ŒåŒæ—¶ä¿è¯å®ƒä»¬ä¹‹é—´çš„å¯åˆ†æ€§ã€‚åœ¨æµè¡ŒåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMTAttackåœ¨å¤šç›®æ ‡æ”»å‡»æ–¹é¢æˆåŠŸç‡æé«˜ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ”»å‡»æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ”»å‡»åœ¨æ•°æ®é›†é—´è¡¨ç°å‡ºå¼ºå¤§çš„é€šç”¨æ€§å’Œå¯¹åé—¨é˜²å¾¡ç­–ç•¥çš„ç¨³å¥æ€§ã€‚è¿™äº›å‘ç°çªæ˜¾äº†LVLMsé¢ä¸´å¤šç›®æ ‡åé—¨æ”»å‡»æ—¶çš„è„†å¼±æ€§ï¼Œå¹¶è¿«åˆ‡éœ€è¦å¯¹è¿™ç±»å¨èƒè¿›è¡Œç¼“è§£ã€‚ç›¸å…³ä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/mala-lab/MTAttack">é“¾æ¥</a>ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰è™½ç„¶æ€§èƒ½å“è¶Šï¼Œä½†å­˜åœ¨å®‰å…¨æ¼æ´ï¼Œæ˜“å—åé—¨æ”»å‡»å½±å“ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹å¤šç›®æ ‡åé—¨æ”»å‡»æ–¹æ³•â€”â€”MTAttackï¼Œèƒ½å¤Ÿåœ¨å•æ¬¡è®­ç»ƒä¸­åŠ å…¥å¤šä¸ªç‹¬ç«‹è§¦å‘å› ç´ ã€‚</li>
<li>MTAttacké€šè¿‡è”åˆä¼˜åŒ–å¤šä¸ªè§¦å‘å™¨åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„ä½ç½®ï¼Œå®ç°å‡†ç¡®çš„å¤šè§¦å‘-ç›®æ ‡æ˜ å°„ã€‚</li>
<li>MTAttacké‡‡ç”¨ä¸¤ç§çº¦æŸï¼šä»£ç†ç©ºé—´åˆ’åˆ†çº¦æŸå’Œè§¦å‘åŸå‹é”šå®šçº¦æŸã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMTAttackåœ¨å¤šç›®æ ‡æ”»å‡»æ–¹é¢è¡¨ç°å‡ºé«˜æˆåŠŸç‡ã€å¼ºé€šç”¨æ€§å’Œå¯¹åé—¨é˜²å¾¡ç­–ç•¥çš„ç¨³å¥æ€§ã€‚</li>
<li>å½“å‰ç ”ç©¶çªæ˜¾äº†LVLMsé¢ä¸´å¤šç›®æ ‡åé—¨æ”»å‡»çš„è„†å¼±æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10098">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-591b74243cc4bfd377ca89d86a4598a0" align="middle">
<img src="https://picx.zhimg.com/v2-7523fcbb694c1da58a33bfa09ea4b1d2" align="middle">
<img src="https://picx.zhimg.com/v2-b16f83c499bbab627c950c7041ba8b91" align="middle">
<img src="https://picx.zhimg.com/v2-321b1aae6726e5a6a09f090a4b564d37" align="middle">
<img src="https://picx.zhimg.com/v2-3b1f1af94dd030bbd0fad7e2c2d26fb6" align="middle">
<img src="https://picx.zhimg.com/v2-2741f2294472be7f6d7e8adf3c16fd44" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GraphIF-Enhancing-Multi-Turn-Instruction-Following-for-Large-Language-Models-with-Relation-Graph-Prompt"><a href="#GraphIF-Enhancing-Multi-Turn-Instruction-Following-for-Large-Language-Models-with-Relation-Graph-Prompt" class="headerlink" title="GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt"></a>GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt</h2><p><strong>Authors:Zhenhe Li, Can Lin, Ling Zheng, Wen-Da Wei, Junli Liang, Qi Song</strong></p>
<p>Multi-turn instruction following is essential for building intelligent conversational systems that can consistently adhere to instructions across dialogue turns. However, existing approaches to enhancing multi-turn instruction following primarily rely on collecting or generating large-scale multi-turn dialogue datasets to fine-tune large language models (LLMs), which treat each response generation as an isolated task and fail to explicitly incorporate multi-turn instruction following into the optimization objectives. As a result, instruction-tuned LLMs often struggle with complex long-distance constraints. In multi-turn dialogues, relational constraints across turns can be naturally modeled as labeled directed edges, making graph structures particularly suitable for modeling multi-turn instruction following. Despite this potential, leveraging graph structures to enhance the multi-turn instruction following capabilities of LLMs remains unexplored. To bridge this gap, we propose GraphIF, a plug-and-play framework that models multi-turn dialogues as directed relation graphs and leverages graph prompts to enhance the instruction following capabilities of LLMs. GraphIF comprises three key components: (1) an agent-based relation extraction module that captures inter-turn semantic relations via action-triggered mechanisms to construct structured graphs; (2) a relation graph prompt generation module that converts structured graph information into natural language prompts; and (3) a response rewriting module that refines initial LLM outputs using the generated graph prompts. Extensive experiments on two long multi-turn dialogue datasets demonstrate that GraphIF can be seamlessly integrated into instruction-tuned LLMs and leads to significant improvements across all four multi-turn instruction-following evaluation metrics.</p>
<blockquote>
<p>å¤šè½®æŒ‡ä»¤è·Ÿéšå¯¹äºæ„å»ºèƒ½å¤Ÿè·¨å¯¹è¯è½®æ¬¡æŒç»­éµå¾ªæŒ‡ä»¤çš„æ™ºèƒ½å¯¹è¯ç³»ç»Ÿè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¢å¼ºå¤šè½®æŒ‡ä»¤è·Ÿéšçš„æ–¹æ³•ä¸»è¦ä¾èµ–äºæ”¶é›†æˆ–ç”Ÿæˆå¤§è§„æ¨¡çš„å¤šè½®å¯¹è¯æ•°æ®é›†æ¥å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚è¿™äº›æ–¹æ³•å°†æ¯ä¸ªå“åº”ç”Ÿæˆè§†ä¸ºä¸€é¡¹ç‹¬ç«‹ä»»åŠ¡ï¼Œæœªèƒ½æ˜¾å¼åœ°å°†å¤šè½®æŒ‡ä»¤è·Ÿéšçº³å…¥ä¼˜åŒ–ç›®æ ‡ã€‚å› æ­¤ï¼ŒæŒ‡ä»¤è°ƒæ•´çš„LLMå¾€å¾€é¢ä¸´å¤æ‚çš„è¿œè·ç¦»çº¦æŸé—®é¢˜ã€‚åœ¨å¤šè½®å¯¹è¯ä¸­ï¼Œå„è½®ä¹‹é—´çš„å…³ç³»çº¦æŸå¯ä»¥è‡ªç„¶åœ°å»ºæ¨¡ä¸ºå¸¦æ ‡ç­¾çš„æœ‰å‘è¾¹ï¼Œä½¿å¾—å›¾ç»“æ„ç‰¹åˆ«é€‚åˆå»ºæ¨¡å¤šè½®æŒ‡ä»¤è·Ÿéšã€‚å°½ç®¡å­˜åœ¨è¿™ç§æ½œåŠ›ï¼Œä½†åˆ©ç”¨å›¾ç»“æ„å¢å¼ºLLMçš„å¤šè½®æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ä»å°šæœªè¢«æ¢ç´¢ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†GraphIFï¼Œè¿™æ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„æ¡†æ¶ï¼Œå®ƒå°†å¤šè½®å¯¹è¯å»ºæ¨¡ä¸ºæœ‰å‘å…³ç³»å›¾ï¼Œå¹¶åˆ©ç”¨å›¾æç¤ºå¢å¼ºLLMçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚GraphIFåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰åŸºäºä»£ç†çš„å…³ç³»æå–æ¨¡å—ï¼Œå®ƒé€šè¿‡åŠ¨ä½œè§¦å‘æœºåˆ¶æ•è·è½®æ¬¡ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œä»¥æ„å»ºç»“æ„åŒ–å›¾ï¼›ï¼ˆ2ï¼‰å…³ç³»å›¾æç¤ºç”Ÿæˆæ¨¡å—ï¼Œå°†ç»“æ„åŒ–å›¾ä¿¡æ¯è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æç¤ºï¼›ï¼ˆ3ï¼‰å“åº”é‡å†™æ¨¡å—ï¼Œä½¿ç”¨ç”Ÿæˆçš„å›¾æç¤ºå¯¹åˆå§‹LLMè¾“å‡ºè¿›è¡Œç²¾ç‚¼ã€‚åœ¨ä¸¤ä¸ªé•¿å¤šè½®å¯¹è¯æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGraphIFå¯ä»¥æ— ç¼åœ°é›†æˆåˆ°æŒ‡ä»¤è°ƒæ•´çš„LLMä¸­ï¼Œå¹¶åœ¨æ‰€æœ‰å››ä¸ªå¤šè½®æŒ‡ä»¤è·Ÿéšè¯„ä¼°æŒ‡æ ‡ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10051v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†åœ¨æ„å»ºæ™ºèƒ½å¯¹è¯ç³»ç»Ÿæ—¶ï¼Œå¤šè½®æŒ‡ä»¤è·Ÿéšçš„é‡è¦æ€§åŠå…¶ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ”¶é›†æˆ–ç”Ÿæˆå¤§è§„æ¨¡å¤šè½®å¯¹è¯æ•°æ®é›†æ¥å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä½†è¿™ç§æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„é•¿è·ç¦»çº¦æŸæ—¶è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†GraphIFæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å¤šè½®å¯¹è¯å»ºæ¨¡ä¸ºæœ‰å‘å…³ç³»å›¾ï¼Œå¹¶åˆ©ç”¨å›¾æç¤ºå¢å¼ºLLMsçš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚GraphIFåŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼Œèƒ½å¤Ÿå®ç°ç»“æ„å›¾çš„æ„å»ºã€å…³ç³»å›¾æç¤ºçš„ç”Ÿæˆä»¥åŠå“åº”çš„é‡å†™ã€‚å®éªŒè¯æ˜ï¼ŒGraphIFå¯ä»¥æ— ç¼é›†æˆåˆ°æŒ‡ä»¤è°ƒæ•´å‹LLMsä¸­ï¼Œå¹¶åœ¨æ‰€æœ‰å››é¡¹å¤šè½®æŒ‡ä»¤è·Ÿéšè¯„ä¼°æŒ‡æ ‡ä¸Šå®ç°æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè½®æŒ‡ä»¤è·Ÿéšå¯¹äºæ„å»ºæ™ºèƒ½å¯¹è¯ç³»ç»Ÿè‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿç¡®ä¿ç³»ç»Ÿåœ¨å¯¹è¯è¿‡ç¨‹ä¸­å§‹ç»ˆéµå¾ªæŒ‡ä»¤ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å¤§è§„æ¨¡æ•°æ®é›†å¾®è°ƒLLMsæ¥å¤„ç†å¤šè½®æŒ‡ä»¤ï¼Œä½†è¿™ç§æ–¹æ³•åœ¨å¤„ç†å¤æ‚é•¿è·ç¦»çº¦æŸæ—¶è¡¨ç°ä¸è¶³ã€‚</li>
<li>GraphIFæ¡†æ¶é¦–æ¬¡å°†å¤šè½®å¯¹è¯å»ºæ¨¡ä¸ºæœ‰å‘å…³ç³»å›¾ï¼Œä»¥æ›´å¥½åœ°å¤„ç†å¤æ‚çš„æŒ‡ä»¤è·Ÿéšä»»åŠ¡ã€‚</li>
<li>GraphIFåŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šå…³ç³»æå–æ¨¡å—ã€å…³ç³»å›¾æç¤ºç”Ÿæˆæ¨¡å—å’Œå“åº”é‡å†™æ¨¡å—ã€‚</li>
<li>å…³ç³»æå–æ¨¡å—é€šè¿‡åŠ¨ä½œè§¦å‘æœºåˆ¶æ•è·è½®æ¬¡é—´çš„è¯­ä¹‰å…³ç³»ï¼Œæ„å»ºç»“æ„å›¾ã€‚</li>
<li>å…³ç³»å›¾æç¤ºç”Ÿæˆæ¨¡å—å°†ç»“æ„å›¾ä¿¡æ¯è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æç¤ºã€‚</li>
<li>å“åº”é‡å†™æ¨¡å—ä½¿ç”¨ç”Ÿæˆçš„å›¾æç¤ºå¯¹åˆå§‹LLMè¾“å‡ºè¿›è¡Œç²¾ç»†åŒ–å¤„ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10051">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-221dc36f1fdaa208180b25c02e1e2502" align="middle">
<img src="https://picx.zhimg.com/v2-3b908dbce967c5b39229f9432fa0f80f" align="middle">
<img src="https://picx.zhimg.com/v2-bb3f5213991eaba559a4045aa2568f6a" align="middle">
<img src="https://picx.zhimg.com/v2-4f1aa7c2bc88e11e7c1e929980b40bc9" align="middle">
<img src="https://picx.zhimg.com/v2-a0a1fd3be81afdbed17d6d842d2c13da" align="middle">
<img src="https://picx.zhimg.com/v2-ac71a046d29c0c65184e64f9b64059a7" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="How-Small-Can-You-Go-Compact-Language-Models-for-On-Device-Critical-Error-Detection-in-Machine-Translation"><a href="#How-Small-Can-You-Go-Compact-Language-Models-for-On-Device-Critical-Error-Detection-in-Machine-Translation" class="headerlink" title="How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation"></a>How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation</h2><p><strong>Authors:Muskaan Chopra, Lorenz Sparrenberg, Sarthak Khanna, Rafet Sifa</strong></p>
<p>Large Language Models (LLMs) excel at evaluating machine translation (MT), but their scale and cost hinder deployment on edge devices and in privacy-sensitive workflows. We ask: how small can you get while still detecting meaning-altering translation errors? Focusing on English-&gt;German Critical Error Detection (CED), we benchmark sub-2B models (LFM2-350M, Qwen-3-0.6B&#x2F;1.7B, Llama-3.2-1B-Instruct, Gemma-3-1B) across WMT21, WMT22, and SynCED-EnDe-2025. Our framework standardizes prompts, applies lightweight logit-bias calibration and majority voting, and reports both semantic quality (MCC, F1-ERR&#x2F;F1-NOT) and compute metrics (VRAM, latency, throughput). Results reveal a clear sweet spot around one billion parameters: Gemma-3-1B provides the best quality-efficiency trade-off, reaching MCC&#x3D;0.77 with F1-ERR&#x3D;0.98 on SynCED-EnDe-2025 after merged-weights fine-tuning, while maintaining 400 ms single-sample latency on a MacBook Pro M4 Pro (24 GB). At larger scale, Qwen-3-1.7B attains the highest absolute MCC (+0.11 over Gemma) but with higher compute cost. In contrast, ultra-small models (0.6B) remain usable with few-shot calibration yet under-detect entity and number errors. Overall, compact, instruction-tuned LLMs augmented with lightweight calibration and small-sample supervision can deliver trustworthy, on-device CED for MT, enabling private, low-cost error screening in real-world translation pipelines. All datasets, prompts, and scripts are publicly available at our GitHub repository.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ“…é•¿è¯„ä¼°æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰çš„æ•ˆæœï¼Œä½†å®ƒä»¬çš„è§„æ¨¡å’Œæˆæœ¬é˜»ç¢äº†åœ¨è¾¹ç¼˜è®¾å¤‡å’Œéšç§æ•æ„Ÿå·¥ä½œæµç¨‹ä¸­çš„éƒ¨ç½²ã€‚æˆ‘ä»¬çš„é—®é¢˜æ˜¯ï¼šåœ¨ä»èƒ½æ£€æµ‹æ„ä¹‰æ”¹å˜çš„ç¿»è¯‘é”™è¯¯çš„æƒ…å†µä¸‹ï¼Œä½ èƒ½æŠŠæ¨¡å‹ç¼©å°åˆ°ä»€ä¹ˆç¨‹åº¦ï¼Ÿæˆ‘ä»¬ä¸“æ³¨äºè‹±è¯­åˆ°å¾·è¯­çš„ä¸´ç•Œé”™è¯¯æ£€æµ‹ï¼ˆCEDï¼‰ï¼Œå¯¹æ¬¡2Bæ¨¡å‹ï¼ˆLFM2-350Mã€Qwen-3-0.6B&#x2F;1.7Bã€Llama-3.2-1B-Instructã€Gemma-3-1Bï¼‰åœ¨WMT21ã€WMT22å’ŒSynCED-EnDe-2025ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ ‡å‡†åŒ–äº†æç¤ºï¼Œåº”ç”¨äº†è½»é‡çº§çš„logit-biasæ ¡å‡†å’Œå¤šæ•°æŠ•ç¥¨ï¼Œå¹¶æŠ¥å‘Šäº†è¯­ä¹‰è´¨é‡ï¼ˆMCCã€F1-ERR&#x2F;F1-NOTï¼‰å’Œè®¡ç®—æŒ‡æ ‡ï¼ˆVRAMã€å»¶è¿Ÿã€ååé‡ï¼‰ã€‚ç»“æœæ­ç¤ºäº†ä¸€ä¸ªæ˜ç¡®çš„æœ€ä½³å¹³è¡¡ç‚¹ï¼Œå¤§çº¦åäº¿ä¸ªå‚æ•°ï¼šGemma-3-1Båœ¨åˆå¹¶æƒé‡å¾®è°ƒåï¼Œåœ¨SynCED-EnDe-2025ä¸Šè¾¾åˆ°MCC&#x3D;0.77ä¸”F1-ERR&#x3D;0.98çš„æœ€ä½³è´¨é‡æ•ˆç‡æƒè¡¡ï¼ŒåŒæ—¶åœ¨MacBook Pro M4 Proï¼ˆ24GBï¼‰ä¸Šä¿æŒ400æ¯«ç§’çš„å•æ ·æœ¬å»¶è¿Ÿã€‚åœ¨æ›´å¤§è§„æ¨¡ä¸Šï¼ŒQwen-3-1.7Bè¾¾åˆ°äº†æœ€é«˜çš„ç»å¯¹MCCï¼ˆæ¯”Gemmaé«˜å‡º0.11ï¼‰ï¼Œä½†è®¡ç®—æˆæœ¬æ›´é«˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè¶…å°å‹æ¨¡å‹ï¼ˆ0.6Bï¼‰åœ¨å°‘é‡æ ·æœ¬æ ¡å‡†åä»ç„¶å¯ç”¨ï¼Œä½†åœ¨æ£€æµ‹å®ä½“å’Œæ•°å­—é”™è¯¯æ–¹é¢ä»æœ‰ä¸è¶³ã€‚æ€»ä½“è€Œè¨€ï¼Œç´§å‡‘ã€æŒ‡ä»¤è°ƒä¼˜çš„LLMsè¾…ä»¥è½»é‡çº§æ ¡å‡†å’Œå°æ ·æœ¬ç›‘ç£ï¼Œå¯ä»¥åœ¨æœºå™¨ç¿»è¯‘ä¸­å®ç°å¯ä¿¡çš„åœ¨çº¿ä¸´ç•Œé”™è¯¯æ£€æµ‹ï¼Œä»è€Œåœ¨ç°å®ä¸–ç•Œç¿»è¯‘ç®¡é“ä¸­å®ç°ç§äººã€ä½æˆæœ¬çš„é”™è¯¯ç­›æŸ¥ã€‚æ‰€æœ‰æ•°æ®é›†ã€æç¤ºå’Œè„šæœ¬å‡å¯åœ¨æˆ‘ä»¬çš„GitHubä»“åº“ä¸­å…¬å¼€è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09748v1">PDF</a> Accepted in IEEE BigData 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯„ä¼°æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶è§„æ¨¡å’Œæˆæœ¬é˜»ç¢äº†å…¶åœ¨è¾¹ç¼˜è®¾å¤‡å’Œéšç§æ•æ„Ÿå·¥ä½œæµç¨‹ä¸­çš„éƒ¨ç½²ã€‚æœ¬ç ”ç©¶èšç„¦äºè‹±è¯­åˆ°å¾·è¯­çš„ç¿»è¯‘é”™è¯¯æ£€æµ‹ï¼ˆCEDï¼‰ï¼Œå¯¹æ¯”è¯„ä¼°äº†å¤šä¸ªå°å‹LLMæ¨¡å‹åœ¨WMT21ã€WMT22å’ŒSynCED-EnDe-2025ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶é‡‡ç”¨æ ‡å‡†åŒ–æç¤ºã€è½»é‡çº§logit-biasæ ¡å‡†å’Œå¤šæ•°æŠ•ç¥¨æœºåˆ¶ï¼Œå¹¶æŠ¥å‘Šäº†è¯­ä¹‰è´¨é‡ï¼ˆMCCã€F1-ERR&#x2F;F1-NOTï¼‰å’Œè®¡ç®—æŒ‡æ ‡ï¼ˆVRAMã€å»¶è¿Ÿã€ååé‡ï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨çº¦åäº¿å‚æ•°èŒƒå›´å†…å­˜åœ¨ä¸€ä¸ªæ˜æ˜¾çš„æœ€ä½³ç‚¹ï¼šGemma-3-1Båœ¨åˆå¹¶æƒé‡å¾®è°ƒåï¼Œåœ¨SynCED-EnDe-2025ä¸Šè¾¾åˆ°MCC&#x3D;0.77ä¸”F1-ERR&#x3D;0.98çš„é«˜è´¨é‡æ•ˆç‡å¹³è¡¡ï¼ŒåŒæ—¶åœ¨MacBook Pro M4 Proï¼ˆ24GBï¼‰ä¸Šä¿æŒ400msçš„å•æ ·æœ¬å»¶è¿Ÿã€‚è€Œæ›´å¤§è§„æ¨¡çš„Qwen-3-1.7Bè™½ç„¶è·å¾—æœ€é«˜çš„ç»å¯¹MCCå€¼ï¼Œä½†è®¡ç®—æˆæœ¬æ›´é«˜ã€‚è¶…å°å‹æ¨¡å‹ï¼ˆ0.6Bï¼‰åœ¨å°‘æ•°æ ·æœ¬æ ¡å‡†ä¸‹ä»ç„¶å¯ç”¨ï¼Œä½†åœ¨æ£€æµ‹å®ä½“å’Œæ•°å­—é”™è¯¯æ–¹é¢è¡¨ç°ä¸è¶³ã€‚æ€»ä½“è€Œè¨€ï¼Œç»“åˆè½»é‡çº§æ ¡å‡†å’Œå°æ ·æœ¬ç›‘ç£çš„ç´§å‡‘ã€æŒ‡ä»¤è°ƒä¼˜çš„LLMå¯åœ¨MTä¸­å®ç°å¯ä¿¡çš„åœ¨çº¿CEDï¼Œä¸ºçœŸå®ç¿»è¯‘ç®¡é“ä¸­çš„ç§æœ‰ã€ä½æˆæœ¬é”™è¯¯ç­›æŸ¥æä¾›å¯èƒ½ã€‚ç›¸å…³æ•°æ®é›†ã€æç¤ºå’Œè„šæœ¬å·²å…¬å¼€åœ¨æˆ‘ä»¬çš„GitHubä»“åº“ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æœºå™¨ç¿»è¯‘è¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†éƒ¨ç½²åœ¨è¾¹ç¼˜è®¾å¤‡å’Œéšç§æ•æ„Ÿç¯å¢ƒä¸­çš„æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>ç ”ç©¶èšç„¦äºè‹±è¯­åˆ°å¾·è¯­çš„ç¿»è¯‘é”™è¯¯æ£€æµ‹ï¼ˆCEDï¼‰ã€‚</li>
<li>è¯„ä¼°äº†å¤šä¸ªå°å‹LLMæ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚</li>
<li>é‡‡ç”¨æ ‡å‡†åŒ–æç¤ºã€è½»é‡çº§æ ¡å‡†å’Œå¤šæ•°æŠ•ç¥¨æœºåˆ¶æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å‘ç°ä¸€ä¸ªçº¦åäº¿å‚æ•°çš„æ¨¡å‹ï¼ˆGemma-3-1Bï¼‰åœ¨è´¨é‡æ•ˆç‡æ–¹é¢è¾¾åˆ°å¹³è¡¡ï¼Œå®ç°è‰¯å¥½çš„ç¿»è¯‘é”™è¯¯æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>Qwen-3-1.7Bæ¨¡å‹è™½ç„¶è¾¾åˆ°è¾ƒé«˜çš„MCCå€¼ï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09748">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ba89b6d757b4c32e3216c12c9bc044ff" align="middle">
<img src="https://picx.zhimg.com/v2-957e097193495b0a84082cb00e985aac" align="middle">
<img src="https://picx.zhimg.com/v2-89f2e9b87752cb1fb13007bc6055cba1" align="middle">
<img src="https://picx.zhimg.com/v2-469c415a9abf90e7899fe2ab860d2b07" align="middle">
<img src="https://picx.zhimg.com/v2-44b88e438cc0cc8a51965417454a7109" align="middle">
<img src="https://picx.zhimg.com/v2-a35cac53bad461a8768c8a00d112127e" align="middle">
<img src="https://picx.zhimg.com/v2-788decf0feada96c4e6066c829e88602" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="SliderEdit-Continuous-Image-Editing-with-Fine-Grained-Instruction-Control"><a href="#SliderEdit-Continuous-Image-Editing-with-Fine-Grained-Instruction-Control" class="headerlink" title="SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control"></a>SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control</h2><p><strong>Authors:Arman Zarei, Samyadeep Basu, Mobina Pournemat, Sayan Nag, Ryan Rossi, Soheil Feizi</strong></p>
<p>Instruction-based image editing models have recently achieved impressive performance, enabling complex edits to an input image from a multi-instruction prompt. However, these models apply each instruction in the prompt with a fixed strength, limiting the userâ€™s ability to precisely and continuously control the intensity of individual edits. We introduce SliderEdit, a framework for continuous image editing with fine-grained, interpretable instruction control. Given a multi-part edit instruction, SliderEdit disentangles the individual instructions and exposes each as a globally trained slider, allowing smooth adjustment of its strength. Unlike prior works that introduced slider-based attribute controls in text-to-image generation, typically requiring separate training or fine-tuning for each attribute or concept, our method learns a single set of low-rank adaptation matrices that generalize across diverse edits, attributes, and compositional instructions. This enables continuous interpolation along individual edit dimensions while preserving both spatial locality and global semantic consistency. We apply SliderEdit to state-of-the-art image editing models, including FLUX-Kontext and Qwen-Image-Edit, and observe substantial improvements in edit controllability, visual consistency, and user steerability. To the best of our knowledge, we are the first to explore and propose a framework for continuous, fine-grained instruction control in instruction-based image editing models. Our results pave the way for interactive, instruction-driven image manipulation with continuous and compositional control.</p>
<blockquote>
<p>åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æ¨¡å‹æœ€è¿‘å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½è¡¨ç°ï¼Œèƒ½å¤Ÿä»å¤šæŒ‡ä»¤æç¤ºä¸­å¯¹è¾“å…¥å›¾åƒè¿›è¡Œå¤æ‚ç¼–è¾‘ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»¥å›ºå®šçš„å¼ºåº¦åº”ç”¨æç¤ºä¸­çš„æ¯ä¸ªæŒ‡ä»¤ï¼Œé™åˆ¶äº†ç”¨æˆ·ç²¾ç¡®å’Œè¿ç»­æ§åˆ¶å•ä¸ªç¼–è¾‘å¼ºåº¦çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†SliderEditï¼Œä¸€ä¸ªå…·æœ‰ç²¾ç»†ç²’åº¦ã€å¯è§£é‡Šçš„æŒ‡ä»¤æ§åˆ¶çš„è¿ç»­å›¾åƒç¼–è¾‘æ¡†æ¶ã€‚å¯¹äºå¤šéƒ¨åˆ†ç¼–è¾‘æŒ‡ä»¤ï¼ŒSliderEditæ‹†è§£äº†å„ä¸ªæŒ‡ä»¤ï¼Œå¹¶å°†å…¶æš´éœ²ä¸ºå…¨å±€è®­ç»ƒçš„æ»‘å—ï¼Œå…è®¸å¹³æ»‘åœ°è°ƒæ•´å…¶å¼ºåº¦ã€‚ä¸ä¹‹å‰å¼•å…¥æ»‘å—åŸºäºå±æ€§æ§åˆ¶æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å·¥ä½œä¸åŒï¼Œé€šå¸¸éœ€è¦å¯¹æ¯ä¸ªå±æ€§æˆ–æ¦‚å¿µè¿›è¡Œå•ç‹¬çš„è®­ç»ƒæˆ–å¾®è°ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å­¦ä¹ äº†ä¸€ç»„ä½ç§©é€‚åº”çŸ©é˜µï¼Œå¯ä»¥åœ¨å„ç§ç¼–è¾‘ã€å±æ€§å’Œç»„åˆæŒ‡ä»¤ä¸­é€šç”¨ã€‚è¿™å®ç°äº†åœ¨å•ä¸ªç¼–è¾‘ç»´åº¦ä¸Šçš„è¿ç»­æ’å€¼ï¼ŒåŒæ—¶ä¿ç•™ç©ºé—´å±€éƒ¨æ€§å’Œå…¨å±€è¯­ä¹‰ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å°†SliderEditåº”ç”¨äºæœ€å…ˆè¿›çš„å›¾åƒç¼–è¾‘æ¨¡å‹ï¼ŒåŒ…æ‹¬FLUX-Kontextå’ŒQwen-Image-Editï¼Œå¹¶è§‚å¯Ÿåˆ°åœ¨ç¼–è¾‘æ§åˆ¶ã€è§†è§‰ä¸€è‡´æ€§å’Œç”¨æˆ·å¯æ§æ€§æ–¹é¢çš„æ˜¾è‘—æ”¹å–„ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªæ¢ç´¢å¹¶æå‡ºåŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æ¨¡å‹ä¸­çš„è¿ç»­ã€ç²¾ç»†ç²’åº¦æŒ‡ä»¤æ§åˆ¶æ¡†æ¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå…·æœ‰è¿ç»­æ€§å’Œç»„åˆæ§åˆ¶çš„äº¤äº’å¼æŒ‡ä»¤é©±åŠ¨å›¾åƒæ“çºµé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09715v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¯¥æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æ¡†æ¶SliderEditï¼Œå®ƒå®ç°äº†å¯¹å›¾åƒè¿›è¡Œç²¾ç»†åŒ–çš„è¿ç»­ç¼–è¾‘ã€‚è¯¥æ¡†æ¶å°†æ¯ä¸ªç¼–è¾‘æŒ‡ä»¤åˆ†è§£ä¸ºå¯ç‹¬ç«‹è°ƒæ•´çš„æ»‘å—ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿå¹³æ»‘åœ°è°ƒæ•´æ¯ä¸ªæŒ‡ä»¤çš„å¼ºåº¦ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒSliderEdité€šè¿‡å•ä¸€çš„ä½ç§©é€‚åº”çŸ©é˜µå­¦ä¹ å®ç°è·¨ä¸åŒç¼–è¾‘ã€å±æ€§å’Œç»„åˆæŒ‡ä»¤çš„é€šç”¨æ€§ã€‚è¿™ä½¿å¾—åœ¨å•ä¸ªç¼–è¾‘ç»´åº¦ä¸Šè¿›è¡Œè¿ç»­æ’å€¼ï¼ŒåŒæ—¶ä¿ç•™ç©ºé—´å±€éƒ¨æ€§å’Œå…¨å±€è¯­ä¹‰ä¸€è‡´æ€§ã€‚åº”ç”¨SliderEditåˆ°å…ˆè¿›çš„å›¾åƒç¼–è¾‘æ¨¡å‹ï¼Œå¦‚FLUX-Kontextå’ŒQwen-Image-Editï¼Œæ˜¾è‘—æé«˜äº†ç¼–è¾‘çš„æ§åˆ¶æ€§ã€è§†è§‰ä¸€è‡´æ€§å’Œç”¨æˆ·å¯æ“ä½œæ€§ã€‚è¿™æ˜¯é¦–æ¬¡æ¢ç´¢å¹¶æå‡ºåœ¨åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æ¨¡å‹ä¸­è¿›è¡Œè¿ç»­ç²¾ç»†åŒ–æŒ‡ä»¤æ§åˆ¶çš„æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SliderEditæ¡†æ¶å®ç°äº†åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘çš„ç²¾ç»†åŒ–è¿ç»­æ§åˆ¶ã€‚</li>
<li>é€šè¿‡å°†æ¯ä¸ªç¼–è¾‘æŒ‡ä»¤åˆ†è§£ä¸ºç‹¬ç«‹æ»‘å—ï¼Œç”¨æˆ·å¯å¹³æ»‘è°ƒæ•´æŒ‡ä»¤å¼ºåº¦ã€‚</li>
<li>é‡‡ç”¨å•ä¸€çš„ä½ç§©é€‚åº”çŸ©é˜µå­¦ä¹ ï¼Œå®ç°è·¨ä¸åŒç¼–è¾‘ã€å±æ€§å’Œç»„åˆæŒ‡ä»¤çš„é€šç”¨æ€§ã€‚</li>
<li>SliderEditä¿ç•™äº†ç©ºé—´å±€éƒ¨æ€§å’Œå…¨å±€è¯­ä¹‰ä¸€è‡´æ€§ï¼Œå®ç°å•ä¸ªç¼–è¾‘ç»´åº¦ä¸Šçš„è¿ç»­æ’å€¼ã€‚</li>
<li>è¯¥æ¡†æ¶åº”ç”¨äºå¤šä¸ªå…ˆè¿›çš„å›¾åƒç¼–è¾‘æ¨¡å‹ï¼Œå¦‚FLUX-Kontextå’ŒQwen-Image-Editã€‚</li>
<li>åº”ç”¨SliderEditåï¼Œç¼–è¾‘çš„æ§åˆ¶æ€§ã€è§†è§‰ä¸€è‡´æ€§å’Œç”¨æˆ·å¯æ“ä½œæ€§å¾—åˆ°æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09715">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15872d941a7a76845ca18f693aeb6087" align="middle">
<img src="https://picx.zhimg.com/v2-a798d566a84be49248e785013899a0b6" align="middle">
<img src="https://picx.zhimg.com/v2-738f5af8491924a9cec6b423490ee044" align="middle">
<img src="https://picx.zhimg.com/v2-74226ac823e8de55b8c6c70e05a11f8c" align="middle">
<img src="https://picx.zhimg.com/v2-243821a815e2f113ab9feb5fa571be55" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="General-Intelligence-based-Fragmentation-GIF-A-framework-for-peak-labeled-spectra-simulation"><a href="#General-Intelligence-based-Fragmentation-GIF-A-framework-for-peak-labeled-spectra-simulation" class="headerlink" title="General Intelligence-based Fragmentation (GIF): A framework for peak-labeled spectra simulation"></a>General Intelligence-based Fragmentation (GIF): A framework for peak-labeled spectra simulation</h2><p><strong>Authors:Margaret R. Martin, Soha Hassoun</strong></p>
<p>Despite growing reference libraries and advanced computational tools, progress in the field of metabolomics remains constrained by low rates of annotating measured spectra. The recent developments of large language models (LLMs) have led to strong performance across a wide range of generation and reasoning tasks, spurring increased interest in LLMsâ€™ application to domain-specific scientific challenges, such as mass spectra annotation. Here, we present a novel framework, General Intelligence-based Fragmentation (GIF), that guides pretrained LLMs through spectra simulation using structured prompting and reasoning. GIF utilizes tagging, structured inputs&#x2F;outputs, system prompts, instruction-based prompts, and iterative refinement. Indeed, GIF offers a structured alternative to ad hoc prompting, underscoring the need for systematic guidance of LLMs on complex scientific tasks. Using GIF, we evaluate current generalist LLMsâ€™ ability to use reasoning towards fragmentation and to perform intensity prediction after fine-tuning. We benchmark performance on a novel QA dataset, the MassSpecGym QA-sim dataset, that we derive from the MassSpecGym dataset. Through these implementations of GIF, we find that GPT-4o and GPT-4o-mini achieve a cosine similarity of 0.36 and 0.35 between the simulated and true spectra, respectively, outperforming other pretrained models including GPT-5, Llama-3.1, and ChemDFM, despite GPT-5â€™s recency and ChemDFMâ€™s domain specialization. GIF outperforms several deep learning baselines. Our evaluation of GIF highlights the value of using LLMs not only for spectra simulation but for enabling human-in-the-loop workflows and structured, explainable reasoning in molecular fragmentation.</p>
<blockquote>
<p>å°½ç®¡å‚è€ƒåº“ä¸æ–­å¢é•¿å’Œè®¡ç®—å·¥å…·æ—¥ç›Šå…ˆè¿›ï¼Œä½†ä»£è°¢ç»„å­¦é¢†åŸŸçš„è¿›å±•ä»å—åˆ°æ³¨é‡Šæµ‹é‡å…‰è°±é€Ÿç‡ä½çš„é™åˆ¶ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€è¿‘å‘å±•åœ¨å„ç§ç”Ÿæˆå’Œæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä»è€Œå¢åŠ äº†å¯¹LLMåœ¨ç‰¹å®šé¢†åŸŸç§‘å­¦æŒ‘æˆ˜ï¼ˆå¦‚è´¨è°±æ³¨é‡Šï¼‰ä¸­çš„åº”ç”¨çš„å…´è¶£ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé€šç”¨æ™ºèƒ½çš„ç¢ç‰‡åŒ–ï¼ˆGIFï¼‰æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç»“æ„åŒ–æç¤ºå’Œæ¨ç†å¼•å¯¼é¢„è®­ç»ƒçš„LLMè¿›è¡Œå…‰è°±æ¨¡æ‹Ÿã€‚GIFåˆ©ç”¨æ ‡ç­¾ã€ç»“æ„åŒ–è¾“å…¥&#x2F;è¾“å‡ºã€ç³»ç»Ÿæç¤ºã€æŒ‡ä»¤æç¤ºå’Œè¿­ä»£ä¼˜åŒ–ã€‚å®é™…ä¸Šï¼ŒGIFä¸ºå³å…´æç¤ºæä¾›äº†ç»“æ„åŒ–æ›¿ä»£æ–¹æ¡ˆï¼Œå¼ºè°ƒäº†ç³»ç»ŸæŒ‡å¯¼LLMå®Œæˆå¤æ‚ç§‘å­¦ä»»åŠ¡çš„å¿…è¦æ€§ã€‚ä½¿ç”¨GIFï¼Œæˆ‘ä»¬è¯„ä¼°äº†å½“å‰é€šç”¨LLMé€šè¿‡æ¨ç†è¿›è¡Œç¢ç‰‡åŒ–å’Œå¾®è°ƒåè¿›è¡Œå¼ºåº¦é¢„æµ‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä»MassSpecGymæ•°æ®é›†æ´¾ç”Ÿçš„æ–°å‹é—®ç­”æ•°æ®é›†MassSpecGym QA-simæ•°æ®é›†ä¸Šè¯„ä¼°æ€§èƒ½ã€‚é€šè¿‡è¿™äº›GIFçš„å®ç°ï¼Œæˆ‘ä»¬å‘ç°GPT-4oå’ŒGPT-4o-miniåœ¨æ¨¡æ‹Ÿå…‰è°±å’ŒçœŸå®å…‰è°±ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦åˆ†åˆ«è¾¾åˆ°0.36å’Œ0.35ï¼Œä¼˜äºå…¶ä»–é¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…æ‹¬GPT-5ã€Llama-3.1å’ŒChemDFMï¼Œå°½ç®¡GPT-5æ˜¯è¾ƒæ–°çš„æ¨¡å‹ä¸”ChemDFMå…·æœ‰é¢†åŸŸä¸“ä¸šæ€§ã€‚GIFçš„è¡¨ç°ä¼˜äºå‡ ä¸ªæ·±åº¦å­¦ä¹ åŸºçº¿ã€‚æˆ‘ä»¬å¯¹GIFçš„è¯„ä¼°å¼ºè°ƒäº†ä½¿ç”¨LLMä¸ä»…ç”¨äºå…‰è°±æ¨¡æ‹Ÿçš„ä»·å€¼ï¼Œè€Œä¸”åœ¨äºå®ç°äººæœºäº¤äº’å·¥ä½œæµå’Œç»“æ„åŒ–ã€å¯è§£é‡Šçš„åˆ†å­ç¢ç‰‡åŒ–æ¨ç†ä¸­çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09571v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åŸºäºé€šç”¨æ™ºèƒ½çš„æ–°æ¡†æ¶â€”â€”General Intelligence-based Fragmentationï¼ˆGIFï¼‰ï¼Œç”¨äºæŒ‡å¯¼é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå…‰è°±æ¨¡æ‹Ÿã€‚GIFé€šè¿‡ç»“æ„åŒ–æç¤ºå’Œæ¨ç†ï¼Œåˆ©ç”¨æ ‡ç­¾ã€ç»“æ„åŒ–è¾“å…¥&#x2F;è¾“å‡ºã€ç³»ç»Ÿæç¤ºã€æŒ‡ä»¤æç¤ºå’Œè¿­ä»£ä¼˜åŒ–ç­‰æ‰‹æ®µï¼Œå®ç°å¯¹LLMçš„æœ‰åºå¼•å¯¼ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒGPT-4oå’ŒGPT-4o-miniåœ¨æ¨¡æ‹Ÿå…‰è°±å’ŒçœŸå®å…‰è°±ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºå…¶ä»–é¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…æ‹¬GPT-5å’ŒChemDFMç­‰ã€‚è¿™ä¸€æ–°æ¡†æ¶ä¸ä»…ç”¨äºå…‰è°±æ¨¡æ‹Ÿï¼Œè¿˜å¯åº”ç”¨äºäººç±»å‚ä¸çš„æµç¨‹å’Œç»“æ„åŒ–ã€å¯è§£é‡Šçš„åˆ†å­è£‚è§£æ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æåˆ°äº†åœ¨ä»£è°¢ç»„å­¦é¢†åŸŸï¼Œå°½ç®¡æœ‰å¤§é‡çš„å‚è€ƒæ–‡çŒ®å’Œå…ˆè¿›çš„è®¡ç®—å·¥å…·ï¼Œä½†ç”±äºæ³¨é‡Šæµ‹é‡çš„å…‰è°±é€Ÿç‡è¾ƒä½ï¼Œè¿›å±•ä»ç„¶å—åˆ°é™åˆ¶ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ç”Ÿæˆå’Œæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå¼•å‘äº†å¯¹å…¶åœ¨ç‰¹å®šé¢†åŸŸç§‘å­¦æŒ‘æˆ˜ï¼Œå¦‚è´¨è°±æ³¨é‡Šä¸­çš„åº”ç”¨å…´è¶£ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”åŸºäºé€šç”¨æ™ºèƒ½çš„ç¢ç‰‡åŒ–ï¼ˆGIFï¼‰ï¼Œç”¨äºæŒ‡å¯¼é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå…‰è°±æ¨¡æ‹Ÿï¼Œå®ç°ç»“æ„åŒ–æç¤ºå’Œæ¨ç†ã€‚</li>
<li>GIFé€šè¿‡æ ‡ç­¾ã€ç»“æ„åŒ–è¾“å…¥&#x2F;è¾“å‡ºã€ç³»ç»Ÿæç¤ºã€æŒ‡ä»¤æç¤ºå’Œè¿­ä»£ä¼˜åŒ–ç­‰æ‰‹æ®µå®ç°å¯¹LLMçš„æœ‰åºå¼•å¯¼ã€‚</li>
<li>åœ¨æ–°å‹é—®ç­”æ•°æ®é›†MassSpecGym QA-simä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒGPT-4oå’ŒGPT-4o-miniåœ¨æ¨¡æ‹Ÿå…‰è°±å’ŒçœŸå®å…‰è°±ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦æ–¹é¢è¡¨ç°çªå‡ºã€‚</li>
<li>ä¸å…¶ä»–é¢„è®­ç»ƒæ¨¡å‹å¦‚GPT-5å’ŒChemDFMç›¸æ¯”ï¼ŒGPT-4oå’ŒGPT-4o-miniå…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09571">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b7eb851973f97a180b9c88de2a2a22df" align="middle">
<img src="https://picx.zhimg.com/v2-b98ad0c8b911f60c195970d897662199" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="LLM-Inference-Beyond-a-Single-Node-From-Bottlenecks-to-Mitigations-with-Fast-All-Reduce-Communication"><a href="#LLM-Inference-Beyond-a-Single-Node-From-Bottlenecks-to-Mitigations-with-Fast-All-Reduce-Communication" class="headerlink" title="LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations with Fast All-Reduce Communication"></a>LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations with Fast All-Reduce Communication</h2><p><strong>Authors:Prajwal Singhania, Siddharth Singh, Lannie Dalton Hough, Akarsh Srivastava, Harshitha Menon, Charles Fredrick Jekel, Abhinav Bhatele</strong></p>
<p>As large language models (LLMs) continue to grow in size, distributed inference has become increasingly important. Model-parallel strategies must now efficiently scale not only across multiple GPUs but also across multiple nodes. In this work, we present a detailed performance study of multi-node distributed inference using LLMs on GPU-based supercomputers. We conduct experiments with several state-of-the-art inference engines alongside YALIS, a research-oriented prototype engine designed for controlled experimentation. We analyze the strong-scaling behavior of different model-parallel schemes and identify key bottlenecks. Since all-reduce operations are a common performance bottleneck, we develop NVRAR, a hierarchical all-reduce algorithm based on recursive doubling with NVSHMEM. NVRAR achieves up to 1.9x-3.6x lower latency than NCCL for message sizes between 128 KB and 2 MB on HPE Slingshot and InfiniBand interconnects. Integrated into YALIS, NVRAR achieves up to a 1.72x reduction in end-to-end batch latency for the Llama 3.1 405B model in multi-node decode-heavy workloads using tensor parallelism.</p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è§„æ¨¡ä¸æ–­å¢é•¿ï¼Œåˆ†å¸ƒå¼æ¨æ–­å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æ¨¡å‹å¹¶è¡Œç­–ç•¥ç°åœ¨å¿…é¡»æœ‰æ•ˆåœ°æ‰©å±•åˆ°å¤šä¸ªGPUï¼Œå¹¶ä¸”è¿˜è¦æ‰©å±•åˆ°å¤šä¸ªèŠ‚ç‚¹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹åŸºäºGPUçš„è¶…çº§è®¡ç®—æœºä¸Šä½¿ç”¨LLMè¿›è¡Œå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼æ¨æ–­è¿›è¡Œäº†è¯¦ç»†çš„æ€§èƒ½ç ”ç©¶ã€‚æˆ‘ä»¬é™¤äº†ä½¿ç”¨å‡ ä¸ªæœ€å…ˆè¿›çš„æ¨ç†å¼•æ“è¿›è¡Œå®éªŒä¹‹å¤–ï¼Œè¿˜ç»“åˆäº†é¢å‘ç ”ç©¶çš„åŸå‹å¼•æ“YALISè¿›è¡Œè®¾è®¡å®éªŒã€‚æˆ‘ä»¬åˆ†æäº†ä¸åŒæ¨¡å‹å¹¶è¡Œæ–¹æ¡ˆçš„å¯æ‰©å±•è¡Œä¸ºå¹¶ç¡®å®šäº†å…³é”®ç“¶é¢ˆã€‚ç”±äºæ‰€æœ‰å½’çº¦æ“ä½œæ˜¯ä¸€ç§å¸¸è§çš„æ€§èƒ½ç“¶é¢ˆï¼Œå› æ­¤æˆ‘ä»¬å¼€å‘äº†NVRARï¼Œè¿™æ˜¯ä¸€ç§åŸºäºé€’å½’åŠ å€å’ŒNVSHMEMçš„åˆ†å±‚å½’çº¦ç®—æ³•ã€‚NVRARä¸HPE Slingshotå’ŒInfiniBandäº’è”ç›¸æ¯”ï¼Œåœ¨æ¶ˆæ¯å¤§å°ä¸º128KBè‡³2MBçš„æƒ…å†µä¸‹ï¼Œå¯å®ç°é«˜è¾¾1.9å€è‡³3.6å€çš„å»¶è¿Ÿé™ä½ã€‚é›†æˆåˆ°YALISä¸­åï¼ŒNVRARåœ¨é‡‡ç”¨å¼ é‡å¹¶è¡Œæ€§çš„å¤šèŠ‚ç‚¹è§£ç å¯†é›†å‹å·¥ä½œè´Ÿè½½ä¸­ï¼Œå¯¹Llama 3.1 405Bæ¨¡å‹çš„ç«¯åˆ°ç«¯æ‰¹å¤„ç†å»¶è¿Ÿå®ç°äº†é«˜è¾¾1.72å€çš„é™ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09557v2">PDF</a> 12 Figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åˆ†å¸ƒå¼æ¨ç†ä¸­çš„å¤šèŠ‚ç‚¹æ•ˆç‡ç ”ç©¶ã€‚æœ¬ç ”ç©¶è¯¦ç»†æ¢è®¨äº†ä½¿ç”¨GPUè¶…çº§è®¡ç®—æœºè¿›è¡Œå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼æ¨ç†çš„æ¨¡å‹å¹¶è¡Œç­–ç•¥æ€§èƒ½ã€‚ç ”ç©¶åŒ…æ‹¬ä¸YALISç­‰å…ˆè¿›æ¨ç†å¼•æ“çš„å®éªŒï¼Œå¹¶åˆ†æäº†ä¸åŒæ¨¡å‹å¹¶è¡Œæ–¹æ¡ˆçš„å¼ºæ‰©å±•è¡Œä¸ºã€‚ä¸ºè§£å†³å¸¸è§æ€§èƒ½ç“¶é¢ˆâ€”â€”å…¨å‡æ“ä½œï¼Œå¼€å‘äº†åŸºäºNVSHMEMçš„NVRARåˆ†å±‚å…¨å‡ç®—æ³•ï¼Œå…¶é™ä½å»¶è¿Ÿè¾¾1.9xè‡³3.6xã€‚é›†æˆåˆ°YALISåï¼Œå¯¹äºå¤šèŠ‚ç‚¹è§£ç å¯†é›†å‹å·¥ä½œè´Ÿè½½ï¼ŒNVRARå®ç°äº†å¯¹Llama 3.1 405Bæ¨¡å‹çš„ç«¯åˆ°ç«¯æ‰¹å¤„ç†å»¶è¿Ÿçš„æ˜¾è‘—å‡å°‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åˆ†å¸ƒå¼æ¨ç†ä¸­ï¼Œå¤šèŠ‚ç‚¹æ•ˆç‡å˜å¾—è‡³å…³é‡è¦ã€‚</li>
<li>éœ€è¦å¯¹æ¨¡å‹å¹¶è¡Œç­–ç•¥è¿›è¡Œè¯¦ç»†çš„æ€§èƒ½ç ”ç©¶ï¼Œä¸ä»…åœ¨å¤šä¸ªGPUä¸Šï¼Œè€Œä¸”åœ¨å¤šä¸ªèŠ‚ç‚¹ä¸Šã€‚</li>
<li>ç ”ç©¶æ¶‰åŠå¤šç§å…ˆè¿›æ¨ç†å¼•æ“ï¼ŒåŒ…æ‹¬YALISã€‚</li>
<li>å…¨å‡æ“ä½œæ˜¯ä¸€ä¸ªå¸¸è§çš„æ€§èƒ½ç“¶é¢ˆã€‚</li>
<li>å¼€å‘äº†ä¸€ç§æ–°çš„åˆ†å±‚å…¨å‡ç®—æ³•NVRARï¼ŒåŸºäºNVSHMEMï¼Œå¯¹äºæ¶ˆæ¯å¤§å°ä¸º128KBè‡³2MBçš„æƒ…å†µï¼Œå…¶å»¶è¿Ÿä½äºNCCLã€‚</li>
<li>é›†æˆåˆ°YALISåï¼ŒNVRARæ˜¾è‘—é™ä½äº†å¤šèŠ‚ç‚¹è§£ç å¯†é›†å‹å·¥ä½œè´Ÿè½½çš„ç«¯åˆ°ç«¯æ‰¹å¤„ç†å»¶è¿Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09557">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9de844f9603813eefbfa5b0264a6ac5f" align="middle">
<img src="https://picx.zhimg.com/v2-fc77d92cb133515edc8bd27a462206f0" align="middle">
<img src="https://picx.zhimg.com/v2-3ad83e2afaa84ff332f87aed79599177" align="middle">
<img src="https://picx.zhimg.com/v2-b39a495f161b3791060f6224611edea7" align="middle">
<img src="https://picx.zhimg.com/v2-4c691f0daad91d9db2f5f79cfb07a921" align="middle">
<img src="https://picx.zhimg.com/v2-5da184c99a46a4d435726f060b736550" align="middle">
<img src="https://picx.zhimg.com/v2-975b1f828af2cc2af8eab16b7110baad" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="RF-DETR-Neural-Architecture-Search-for-Real-Time-Detection-Transformers"><a href="#RF-DETR-Neural-Architecture-Search-for-Real-Time-Detection-Transformers" class="headerlink" title="RF-DETR: Neural Architecture Search for Real-Time Detection Transformers"></a>RF-DETR: Neural Architecture Search for Real-Time Detection Transformers</h2><p><strong>Authors:Isaac Robinson, Peter Robicheaux, Matvei Popov, Deva Ramanan, Neehar Peri</strong></p>
<p>Open-vocabulary detectors achieve impressive performance on COCO, but often fail to generalize to real-world datasets with out-of-distribution classes not typically found in their pre-training. Rather than simply fine-tuning a heavy-weight vision-language model (VLM) for new domains, we introduce RF-DETR, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS). Our approach fine-tunes a pre-trained base network on a target dataset and evaluates thousands of network configurations with different accuracy-latency tradeoffs without re-training. Further, we revisit the â€œtunable knobsâ€ for NAS to improve the transferability of DETRs to diverse target domains. Notably, RF-DETR significantly improves on prior state-of-the-art real-time methods on COCO and Roboflow100-VL. RF-DETR (nano) achieves 48.0 AP on COCO, beating D-FINE (nano) by 5.3 AP at similar latency, and RF-DETR (2x-large) outperforms GroundingDINO (tiny) by 1.2 AP on Roboflow100-VL while running 20x as fast. To the best of our knowledge, RF-DETR (2x-large) is the first real-time detector to surpass 60 AP on COCO. Our code is at <a target="_blank" rel="noopener" href="https://github.com/roboflow/rf-detr">https://github.com/roboflow/rf-detr</a></p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡æ£€æµ‹å™¨åœ¨COCOä¸Šçš„è¡¨ç°ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†åœ¨é¢å¯¹é¢„è®­ç»ƒé˜¶æ®µæœªæ›¾é‡åˆ°è¿‡çš„éåˆ†å¸ƒç±»ç°å®æ•°æ®é›†æ—¶ï¼Œé€šå¸¸æ— æ³•æ¨å¹¿åˆ°å®é™…åº”ç”¨ã€‚æˆ‘ä»¬å¹¶æœªç®€å•åœ°å¯¹é‡é‡çº§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡Œæ–°åŸŸå¾®è°ƒï¼Œè€Œæ˜¯å¼•å…¥äº†RF-DETRï¼Œè¿™æ˜¯ä¸€æ¬¾è½»é‡çº§çš„ä¸“ä¸šæ£€æµ‹è½¬æ¢å™¨ã€‚å®ƒé€šè¿‡æƒé‡å…±äº«ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰ä¸ºä»»ä½•ç›®æ ‡æ•°æ®é›†æ‰¾åˆ°ç²¾åº¦-å»¶è¿Ÿå¸•ç´¯æ‰˜æ›²çº¿ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯åœ¨ç›®æ ‡æ•°æ®é›†ä¸Šå¾®è°ƒé¢„è®­ç»ƒåŸºç¡€ç½‘ç»œï¼Œå¹¶è¯„ä¼°æ•°åƒç§å…·æœ‰ä¸åŒç²¾åº¦-å»¶è¿Ÿæƒè¡¡çš„ç½‘ç»œé…ç½®ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†NASçš„â€œå¯è°ƒæ—‹é’®â€ï¼Œä»¥æé«˜DETRåœ¨ä¸åŒç›®æ ‡é¢†åŸŸçš„è¿ç§»èƒ½åŠ›.å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒRF-DETRåœ¨COCOå’ŒRoboflow100-VLä¸Šçš„å®æ—¶æ–¹æ³•ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚RF-DETRï¼ˆçº³ç±³ç‰ˆï¼‰åœ¨COCOä¸Šè¾¾åˆ°äº†48.0 APï¼Œåœ¨ç›¸ä¼¼å»¶è¿Ÿä¸‹æ¯”D-FINEï¼ˆçº³ç±³ç‰ˆï¼‰é«˜å‡º5.3 APï¼›RF-DETRï¼ˆ2å€å¤§å‹ï¼‰åœ¨Roboflow100-VLä¸Šçš„æ€§èƒ½ä¼˜äºGroundingDINOï¼ˆå¾®å°ç‰ˆï¼‰ï¼Œè™½ç„¶è¿è¡Œé€Ÿåº¦å¿«20å€ï¼Œä½†åªé«˜å‡º1.2 APã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒRF-DETRï¼ˆ2å€å¤§å‹ï¼‰æ˜¯é¦–ä¸ªåœ¨COCOä¸Šå®æ—¶æ£€æµ‹è¶…è¿‡60 APçš„æ£€æµ‹å™¨ã€‚æˆ‘ä»¬çš„ä»£ç ä½äºï¼š<a target="_blank" rel="noopener" href="https://github.com/roboflow/rf-detr">https://github.com/roboflow/rf-detr</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09554v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://rfdetr.roboflow.com/">https://rfdetr.roboflow.com/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„è½»é‡çº§ä¸“ä¸šæ£€æµ‹å™¨RF-DETRï¼Œå®ƒé€šè¿‡é‡‡ç”¨ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰æŠ€æœ¯ï¼Œå®ç°äº†å¯¹å„ç§ç›®æ ‡æ•°æ®é›†çš„ç²¾åº¦å’Œå»¶è¿Ÿçš„å¸•ç´¯æ‰˜ä¼˜åŒ–ã€‚RF-DETRèƒ½å¤Ÿåœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå¯¹é¢„è®­ç»ƒåŸºç¡€ç½‘ç»œè¿›è¡Œå¾®è°ƒï¼Œå¹¶è¯„ä¼°æ•°åƒç§ä¸åŒç²¾åº¦å’Œå»¶è¿Ÿæƒè¡¡çš„ç½‘ç»œé…ç½®ã€‚æ­¤å¤–ï¼ŒRF-DETRè¿˜é‡æ–°ç ”ç©¶äº†NASçš„å¯è°ƒèŠ‚å‚æ•°ï¼Œä»¥æé«˜DETRåœ¨ä¸åŒç›®æ ‡åŸŸçš„å¯è¿ç§»æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRF-DETRåœ¨COCOå’ŒRoboflow100-VLç­‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–å®æ—¶æ£€æµ‹æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RF-DETRæ˜¯ä¸€ç§æ–°å‹çš„è½»é‡çº§ä¸“ä¸šæ£€æµ‹å™¨ï¼Œé‡‡ç”¨ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰æŠ€æœ¯è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>RF-DETRèƒ½å¤Ÿåœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå¯¹é¢„è®­ç»ƒåŸºç¡€ç½‘ç»œè¿›è¡Œå¾®è°ƒï¼Œå¹¶è¯„ä¼°ä¸åŒç²¾åº¦å’Œå»¶è¿Ÿçš„ç½‘ç»œé…ç½®ã€‚</li>
<li>RF-DETRé€šè¿‡é‡æ–°ç ”ç©¶NASçš„å¯è°ƒèŠ‚å‚æ•°ï¼Œæé«˜äº†DETRåœ¨ä¸åŒç›®æ ‡åŸŸçš„å¯è¿ç§»æ€§ã€‚</li>
<li>RF-DETRåœ¨COCOæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ä¼˜å¼‚ï¼Œå…¶ä¸­RF-DETRï¼ˆnanoï¼‰æ¨¡å‹åœ¨ç›¸ä¼¼å»¶è¿Ÿä¸‹æ¯”D-FINEï¼ˆnanoï¼‰é«˜å‡º5.3 APã€‚</li>
<li>RF-DETRï¼ˆ2x-largeï¼‰åœ¨Roboflow100-VLæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºGroundingDINOï¼ˆtinyï¼‰ï¼Œå¹¶ä¸”è¿è¡Œé€Ÿåº¦æ›´å¿«ã€‚</li>
<li>RF-DETRï¼ˆ2x-largeï¼‰æ˜¯é¦–ä¸ªåœ¨COCOæ•°æ®é›†ä¸Šå®æ—¶æ£€æµ‹æ€§èƒ½è¶…è¿‡60 APçš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09554">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f4691ea40ebcbcf037c38059703b998" align="middle">
<img src="https://picx.zhimg.com/v2-cbe076e72c3310adfd761e14a0707e88" align="middle">
<img src="https://picx.zhimg.com/v2-0b120f06925deeaef9d9f8907c98385d" align="middle">
<img src="https://picx.zhimg.com/v2-3c0ad6f1dccb473cdf3ec803bda20684" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="vMFCoOp-Towards-Equilibrium-on-a-Unified-Hyperspherical-Manifold-for-Prompting-Biomedical-VLMs"><a href="#vMFCoOp-Towards-Equilibrium-on-a-Unified-Hyperspherical-Manifold-for-Prompting-Biomedical-VLMs" class="headerlink" title="vMFCoOp: Towards Equilibrium on a Unified Hyperspherical Manifold for Prompting Biomedical VLMs"></a>vMFCoOp: Towards Equilibrium on a Unified Hyperspherical Manifold for Prompting Biomedical VLMs</h2><p><strong>Authors:Minye Shao, Sihan Guo, Xinrun Li, Xingyu Miao, Haoran Duan, Yang Long</strong></p>
<p>Recent advances in context optimization (CoOp) guided by large language model (LLM)-distilled medical semantic priors offer a scalable alternative to manual prompt engineering and full fine-tuning for adapting biomedical CLIP-based vision-language models (VLMs). However, prompt learning in this context is challenged by semantic misalignment between LLMs and CLIP variants due to divergent training corpora and model architectures; it further lacks scalability across continuously evolving families of foundation models. More critically, pairwise multimodal alignment via conventional Euclidean-space optimization lacks the capacity to model unified representations or apply localized geometric constraints, which tends to amplify modality gaps in complex biomedical imaging and destabilize few-shot adaptation. In this work, we propose vMFCoOp, a framework that inversely estimates von Mises-Fisher (vMF) distributions on a shared Hyperspherical Manifold, aligning semantic biases between arbitrary LLMs and CLIP backbones via Unified Semantic Anchors to achieve robust biomedical prompting and superior few-shot classification. Grounded in three complementary constraints, vMFCoOp demonstrates consistent improvements across 14 medical datasets, 12 medical imaging modalities, and 13 anatomical regions, outperforming state-of-the-art methods in accuracy, generalization, and clinical applicability. This work aims to continuously expand to encompass more downstream applications, and the corresponding resources are intended to be shared through <a target="_blank" rel="noopener" href="https://github.com/VinyehShaw/UniEqui">https://github.com/VinyehShaw/UniEqui</a>.</p>
<blockquote>
<p>æœ€è¿‘ï¼Œå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æç‚¼å‡ºçš„åŒ»å­¦è¯­ä¹‰ä¼˜å…ˆçº§ï¼Œä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼ˆCoOpï¼‰çš„è¿›å±•ä¸ºåŸºäºCLIPçš„ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰æä¾›äº†æ‰‹åŠ¨æç¤ºå·¥ç¨‹å’Œå…¨å¾®è°ƒçš„å¯æ‰©å±•æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”±äºè®­ç»ƒè¯­æ–™åº“å’Œæ¨¡å‹æ¶æ„çš„å·®å¼‚ï¼ŒLLMå’ŒCLIPå˜ä½“ä¹‹é—´çš„è¯­ä¹‰ä¸ä¸€è‡´å¯¹æç¤ºå­¦ä¹ æ„æˆäº†æŒ‘æˆ˜ï¼›å®ƒç¼ºä¹è·¨ä¸æ–­æ¼”å˜çš„åŸºç¡€æ¨¡å‹çš„æ‰©å±•æ€§ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œé€šè¿‡ä¼ ç»Ÿçš„æ¬§å‡ é‡Œå¾—ç©ºé—´ä¼˜åŒ–è¿›è¡Œçš„ä¸€å¯¹å¤šæ¨¡æ€å¯¹é½ï¼Œç¼ºä¹å»ºæ¨¡ç»Ÿä¸€è¡¨ç¤ºæˆ–åº”ç”¨å±€éƒ¨å‡ ä½•çº¦æŸçš„èƒ½åŠ›ï¼Œè¿™å¾€å¾€ä¼šæ”¾å¤§å¤æ‚ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­çš„æ¨¡æ€å·®è·ï¼Œå¹¶ç ´åå°‘é‡é•œå¤´é€‚åº”æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†vMFCoOpæ¡†æ¶ï¼Œå®ƒé€šè¿‡å…±äº«è¶…çƒé¢æµå½¢ä¸Šåå‘ä¼°è®¡å†¯Â·ç±³å¡æ–¯-è´¹å¸Œå°”ï¼ˆvMFï¼‰åˆ†å¸ƒï¼Œé€šè¿‡ç»Ÿä¸€è¯­ä¹‰é”šå¯¹é½ä»»æ„LLMå’ŒCLIPä¸»å¹²ä¹‹é—´çš„è¯­ä¹‰åå·®ï¼Œå®ç°ç¨³å¥çš„ç”Ÿç‰©åŒ»å­¦æç¤ºå’Œä¼˜è¶Šçš„å°æ ·æœ¬åˆ†ç±»ã€‚åŸºäºä¸‰ä¸ªäº’è¡¥çº¦æŸï¼ŒvMFCoOpåœ¨14ä¸ªåŒ»ç–—æ•°æ®é›†ã€12ç§åŒ»ç–—æˆåƒæ–¹å¼å’Œ1 3ä¸ªè§£å‰–åŒºåŸŸä¸Šæ˜¾ç¤ºå‡ºæŒç»­ä¸€è‡´çš„æ”¹è¿›ï¼Œåœ¨å‡†ç¡®æ€§ã€é€šç”¨æ€§å’Œä¸´åºŠé€‚ç”¨æ€§æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯æ–¹æ³•ã€‚æœ¬å·¥ä½œçš„ç›®æ ‡æ˜¯ä¸æ–­æ‰©å¤§ä»¥æ¶µç›–æ›´å¤šçš„ä¸‹æ¸¸åº”ç”¨ï¼Œç›¸å…³èµ„æºå°†é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/VinyehShaw/UniEqui%E8%BF%9B%E8%A1%8C%E5%85%B1%E4%BA%AB%E3%80%82">https://github.com/VinyehShaw/UniEquiè¿›è¡Œå…±äº«ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09540v2">PDF</a> Accepted as an Oral Presentation at AAAI 2026 Main Technical Track (this version is not peer-reviewed; it is the extended version)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è’¸é¦åŒ»å­¦è¯­ä¹‰å…ˆéªŒçš„ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼ˆCoOpï¼‰ä¸ºæ‰‹åŠ¨æç¤ºå·¥ç¨‹ä»¥åŠç”Ÿç‰©åŒ»å­¦CLIPåŸºç¡€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é€‚åº”æä¾›äº†å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œè¿™ä¸€é¢†åŸŸçš„æç¤ºå­¦ä¹ é¢ä¸´è¯­ä¹‰ä¸ä¸€è‡´å’Œæ¨¡å‹æ¶æ„ä¸åŒå¯¼è‡´çš„æŒ‘æˆ˜ï¼Œå¹¶ä¸”åœ¨ä¸æ–­å‘å±•çš„å®¶æ—åŸºç¡€æ¨¡å‹ä¸­ä¹Ÿç¼ºä¹å¯æ‰©å±•æ€§ã€‚æœ¬å·¥ä½œæå‡ºäº†vMFCoOpæ¡†æ¶ï¼Œé€šè¿‡å…±äº«è¶…çƒé¢æµå½¢ä¸Šçš„é€†å‘ä¼°è®¡å†¯ç±³å¡æ–¯è´¹èˆå°”åˆ†å¸ƒå’Œå¯¹ç»Ÿä¸€è¯­ä¹‰é”šç‚¹çš„ä½¿ç”¨ï¼Œå®ç°é²æ£’çš„ç”Ÿç‰©åŒ»å­¦æç¤ºå’Œå“è¶Šçš„å°‘é‡æ ·æœ¬åˆ†ç±»ã€‚åŸºäºä¸‰é¡¹äº’è¡¥çº¦æŸï¼ŒvMFCoOpåœ¨åŒ»ç–—æ•°æ®é›†ã€æˆåƒæ¨¡æ€å’Œè§£å‰–åŒºåŸŸä¸Šå±•ç°å‡ºä¸€è‡´çš„æ”¹è¿›æ•ˆæœã€‚æœªæ¥ï¼Œè¯¥å·¥ä½œå°†ä¸æ–­æ‰©å±•åˆ°æ›´å¤šä¸‹æ¸¸åº”ç”¨ï¼Œå¹¶é€šè¿‡é“¾æ¥å…±äº«èµ„æºï¼š<a target="_blank" rel="noopener" href="https://github.com/VinyehShaw/UniEqui">https://github.com/VinyehShaw/UniEqui</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMè’¸é¦çš„åŒ»å­¦è¯­ä¹‰å…ˆéªŒçŸ¥è¯†å¯¹ä¸Šä¸‹æ–‡ä¼˜åŒ–æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>è¯­ä¹‰ä¸ä¸€è‡´å’Œæ¨¡å‹æ¶æ„å·®å¼‚åœ¨LLMå’ŒCLIPå˜ä½“ä¹‹é—´æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>vMFCoOpæ¡†æ¶èƒ½æœ‰æ•ˆåº”å¯¹è¿™ç§æŒ‘æˆ˜ï¼Œé€šè¿‡åœ¨å…±äº«è¶…çƒé¢æµå½¢ä¸Šé€†å‘ä¼°è®¡å†¯ç±³å¡æ–¯è´¹èˆå°”åˆ†å¸ƒï¼Œå¢å¼ºè¯­ä¹‰å¯¹é½ã€‚</li>
<li>vMFCoOpä½¿ç”¨ç»Ÿä¸€è¯­ä¹‰é”šç‚¹å®ç°é²æ£’ç”Ÿç‰©åŒ»å­¦æç¤ºå’Œå°‘é‡æ ·æœ¬åˆ†ç±»ã€‚</li>
<li>vMFCoOpåœ¨å¤šç§åŒ»ç–—æ•°æ®é›†ã€æˆåƒæ¨¡æ€å’Œè§£å‰–åŒºåŸŸä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>vMFCoOpæ¡†æ¶æ—¨åœ¨ä¸æ–­æ‰©å±•è‡³æ›´å¤šä¸‹æ¸¸åº”ç”¨ï¼Œå¹¶å…±äº«èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09540">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a12f86f9e44d6d45af5fc33a08ce354e" align="middle">
<img src="https://picx.zhimg.com/v2-d1c51fb95f443e62bf65fe82ae6cb003" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Lumine-An-Open-Recipe-for-Building-Generalist-Agents-in-3D-Open-Worlds"><a href="#Lumine-An-Open-Recipe-for-Building-Generalist-Agents-in-3D-Open-Worlds" class="headerlink" title="Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds"></a>Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds</h2><p><strong>Authors:Weihao Tan, Xiangyang Li, Yunhao Fang, Heyuan Yao, Shi Yan, Hao Luo, Tenglong Ao, Huihui Li, Hongbin Ren, Bairen Yi, Yujia Qin, Bo An, Libin Liu, Guang Shi</strong></p>
<p>We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumineâ€™s effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.</p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºLumineï¼Œè¿™æ˜¯é¦–ä¸ªå¼€æ”¾é…æ–¹ï¼Œç”¨äºå¼€å‘èƒ½å¤Ÿåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„3Då¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å®æ—¶å®Œæˆæ•°å°æ—¶å¤æ‚ä»»åŠ¡çš„é€šç”¨æ™ºèƒ½ä½“ã€‚Lumineé‡‡ç”¨ç±»ä¼¼äººç±»çš„äº¤äº’èŒƒå¼ï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼ç»Ÿä¸€æ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨ï¼Œç”±è§†è§‰è¯­è¨€æ¨¡å‹é©±åŠ¨ã€‚å®ƒæ¯ç§’å¤„ç†5å¸§åŸå§‹åƒç´ ï¼Œäº§ç”Ÿç²¾ç¡®çš„30Hzé”®ç›˜é¼ æ ‡åŠ¨ä½œï¼Œå¹¶åœ¨å¿…è¦æ—¶è‡ªé€‚åº”åœ°è°ƒç”¨æ¨ç†åŠŸèƒ½ã€‚ç»è¿‡åœ¨ã€ŠåŸç¥ã€‹ä¸­çš„è®­ç»ƒï¼ŒLumineæˆåŠŸå®Œæˆäº†é•¿è¾¾äº”ä¸ªå°æ—¶çš„è’™å¾·ä¸»è¦æ•…äº‹æƒ…èŠ‚ï¼Œæ•ˆç‡å ªæ¯”äººç±»ï¼Œå¹¶éµå¾ªè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œåœ¨3Då¼€æ”¾ä¸–ç•Œæ¢ç´¢å’Œ2Då›¾å½¢ç”¨æˆ·ç•Œé¢æ“ä½œæ–¹é¢æ‰§è¡Œå¹¿æ³›çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ”¶é›†ã€æˆ˜æ–—ã€è§£è°œå’ŒNPCäº¤äº’ã€‚é™¤äº†é¢†åŸŸå†…çš„è¡¨ç°å¤–ï¼ŒLumineè¿˜å±•ç¤ºäº†å¼ºå¤§çš„é›¶å¯åŠ¨è·¨æ¸¸æˆæ³›åŒ–èƒ½åŠ›ã€‚æ— éœ€å¾®è°ƒï¼Œå®ƒå¯ä»¥åœ¨ã€Šé£æµªæ€’å¼ã€‹ä¸­å®Œæˆ100åˆ†é’Ÿçš„ä½¿å‘½ï¼Œä»¥åŠã€Šå´©åï¼šæ˜Ÿç©¹é“é“ã€‹çš„é¦–ç« å…¨é•¿äº”å°æ—¶çš„ä»»åŠ¡ã€‚è¿™äº›ä»¤äººé¼“èˆçš„ç»“æœå‡¸æ˜¾äº†Lumineåœ¨ä¸åŒä¸–ç•Œå’Œäº¤äº’åŠ¨æ€ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæœç€å¼€æ”¾ç¯å¢ƒä¸­é€šç”¨æ™ºèƒ½ä½“çš„æ–¹å‘å‘å±•è¿ˆå‡ºäº†åšå®çš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08892v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Lumineæ˜¯ä¸€ç§é€šç”¨æ™ºèƒ½ä½“å¼€å‘çš„æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å®æ—¶3Då¼€æ”¾ä¸–ç•Œç¯å¢ƒä¸­å®Œæˆé•¿è¾¾æ•°å°æ—¶çš„å¤æ‚ä»»åŠ¡ã€‚Lumineé‡‡ç”¨ç±»ä¼¼äººç±»çš„äº¤äº’æ¨¡å¼ï¼Œé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ç»Ÿä¸€æ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨ï¼Œèƒ½å¤Ÿå¤„ç†åŸå§‹åƒç´ å¹¶è‡ªé€‚åº”è°ƒç”¨æ¨ç†ã€‚åœ¨ã€ŠåŸç¥ã€‹ä¸­è®­ç»ƒåï¼ŒLumineä»¥ä¸äººç±»ç›¸å½“çš„æ•ˆç‡å®Œæˆäº†äº”å°æ—¶çš„è’™å¾·ä¸»è¦å‰§æƒ…ï¼Œè¿˜èƒ½åœ¨è™šæ‹Ÿ3Dä¸–ç•Œæ¢ç´¢å’Œç°å®ä¸–ç•Œäº¤äº’åœºæ™¯ä¸‹å®Œæˆå¤šæ ·åŒ–çš„ä»»åŠ¡ã€‚é™¤äº†å…·æœ‰é¢†åŸŸå†…çš„é«˜æ€§èƒ½è¡¨ç°ï¼ŒLumineè¿˜å…·æœ‰å¼ºå¤§çš„è·¨æ¸¸æˆæ³›åŒ–èƒ½åŠ›ï¼Œæœªç»å¾®è°ƒå³æˆåŠŸå®Œæˆã€Šé£äº‘ä¹±ä¸–ã€‹çš„100åˆ†é’Ÿä»»åŠ¡å’Œã€Šå´©åï¼šæ˜Ÿé™…é“è·¯ã€‹çš„ç¬¬ä¸€ç« èŠ‚çš„äº”å°æ—¶æ¸¸æˆã€‚æ­¤é¡¹ç›®çš„æˆæœæ˜¾ç¤ºäº†é€šç”¨æ™ºèƒ½ä½“åœ¨å¼€æ”¾ç¯å¢ƒä¸‹çš„åº”ç”¨æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Lumineæ˜¯é¦–ä¸ªèƒ½å¤Ÿå®Œæˆåœ¨å¤æ‚ã€å¼€æ”¾çš„3Dç¯å¢ƒä¸­é•¿è¾¾æ•°å°æ—¶çš„å®æ—¶ä»»åŠ¡çš„é€šç”¨æ™ºèƒ½ä½“å¼€å‘é£Ÿè°±ã€‚</li>
<li>Lumineé‡‡ç”¨ç±»ä¼¼äººç±»çš„äº¤äº’æ¨¡å¼ï¼Œé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹ç»Ÿä¸€æ„ŸçŸ¥ã€æ¨ç†å’Œè¡ŒåŠ¨ã€‚</li>
<li>Lumineå¯ä»¥åœ¨å¤„ç†åŸå§‹åƒç´ çš„åŒæ—¶è‡ªé€‚åº”è°ƒç”¨æ¨ç†ï¼Œç¡®ä¿æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>åœ¨ã€ŠåŸç¥ã€‹çš„è®­ç»ƒä¸‹ï¼ŒLumineæˆåŠŸåœ°å®Œæˆäº†ç›¸å½“äºäººç±»æ•ˆç‡çš„äº”å°æ—¶è’™å¾·ä¸»è¦å‰§æƒ…ã€‚</li>
<li>Lumineèƒ½å¤Ÿåœ¨è™šæ‹Ÿçš„3Dä¸–ç•Œæ¢ç´¢å’Œç°å®ä¸–ç•Œçš„äº¤äº’åœºæ™¯ä¸‹å®Œæˆå¤šæ ·åŒ–çš„ä»»åŠ¡ã€‚</li>
<li>Lumineä¸ä»…åœ¨è®­ç»ƒæ¸¸æˆé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œè¿˜å±•ç¤ºäº†å¼ºå¤§çš„è·¨æ¸¸æˆæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08892">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec5f9dbf38453c08b87eec6cccaaa0af" align="middle">
<img src="https://picx.zhimg.com/v2-751e50418ec4b23a01676ff80c8732c2" align="middle">
<img src="https://picx.zhimg.com/v2-5113eb787174867fa85c9a9bd2effe30" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Toward-Automated-Cognitive-Assessment-in-Parkinsonâ€™s-Disease-Using-Pretrained-Language-Models"><a href="#Toward-Automated-Cognitive-Assessment-in-Parkinsonâ€™s-Disease-Using-Pretrained-Language-Models" class="headerlink" title="Toward Automated Cognitive Assessment in Parkinsonâ€™s Disease Using Pretrained Language Models"></a>Toward Automated Cognitive Assessment in Parkinsonâ€™s Disease Using Pretrained Language Models</h2><p><strong>Authors:Varada Khanna, Nilay Bhatt, Ikgyu Shin, Sule Tinaz, Yang Ren, Hua Xu, Vipina K. Keloth</strong></p>
<p>Understanding how individuals with Parkinsonâ€™s disease (PD) describe cognitive experiences in their daily lives can offer valuable insights into disease-related cognitive and emotional changes. However, extracting such information from unstructured patient narratives is challenging due to the subtle, overlapping nature of cognitive constructs. This study developed and evaluated natural language processing (NLP) models to automatically identify categories that reflect various cognitive processes from de-identified first-person narratives. Three model families, a Bio_ClinicalBERT-based span categorization model for nested entity recognition, a fine-tuned Meta-Llama-3-8B-Instruct model using QLoRA for instruction following, and GPT-4o mini evaluated under zero- and few-shot settings, were compared on their performance on extracting seven categories. Our findings indicated that model performance varied substantially across categories and model families. The fine-tuned Meta-Llama-3-8B-Instruct achieved the highest overall F1-scores (0.74 micro-average and 0.59 macro-average), particularly excelling in context-dependent categories such as thought and social interaction. Bio_ClinicalBERT exhibited high precision but low recall and performed comparable to Llama for some category types such as location and time but failed on other categories such as thought, emotion and social interaction. Compared to conventional information extraction tasks, this task presents a greater challenge due to the abstract and overlapping nature of narrative accounts of complex cognitive processes. Nonetheless, with continued refinement, these NLP systems hold promise for enabling low-burden, longitudinal monitoring of cognitive function and serving as a valuable complement to formal neuropsychological assessments in PD.</p>
<blockquote>
<p>ç†è§£å¸•é‡‘æ£®ç—…æ‚£è€…å¦‚ä½•æè¿°æ—¥å¸¸ç”Ÿæ´»ä¸­çš„è®¤çŸ¥ä½“éªŒï¼Œå¯ä»¥ä¸ºä¸ç–¾ç—…ç›¸å…³çš„è®¤çŸ¥å’Œæƒ…ç»ªå˜åŒ–æä¾›å®è´µçš„è§è§£ã€‚ç„¶è€Œï¼Œä»éç»“æ„åŒ–çš„æ‚£è€…å™è¿°ä¸­æå–æ­¤ç±»ä¿¡æ¯æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºè®¤çŸ¥æ¦‚å¿µå…·æœ‰ç»†å¾®ä¸”é‡å çš„æ€§è´¨ã€‚æœ¬ç ”ç©¶å¼€å‘å’Œè¯„ä¼°äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨ä»éèº«ä»½è¯†åˆ«çš„ç¬¬ä¸€äººç§°å™è¿°ä¸­è¯†åˆ«åæ˜ å„ç§è®¤çŸ¥è¿‡ç¨‹çš„ç±»åˆ«ã€‚å¯¹åŸºäºBio_ClinicalBERTçš„è·¨åº¦åˆ†ç±»æ¨¡å‹ï¼ˆç”¨äºåµŒå¥—å®ä½“è¯†åˆ«ï¼‰ã€ä½¿ç”¨QLoRAè¿›è¡ŒæŒ‡ä»¤è·Ÿè¸ªçš„ç²¾ç»†è°ƒæ•´çš„Meta-Llama-3-8B-Instructæ¨¡å‹ä»¥åŠåœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬è®¾ç½®ä¸‹è¯„ä¼°çš„GPT-4o miniè¿™ä¸‰ç§æ¨¡å‹å®¶æ—çš„å¯¹æ¯”å‘ç°ï¼Œå®ƒä»¬åœ¨æå–ä¸ƒä¸ªç±»åˆ«æ—¶çš„æ€§èƒ½å„ä¸ç›¸åŒã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç²¾ç»†è°ƒæ•´çš„Meta-Llama-3-8B-Instructæ¨¡å‹åœ¨æ€»ä½“F1åˆ†æ•°ä¸Šè¡¨ç°æœ€ä½³ï¼ˆå¾®å¹³å‡å€¼ä¸º0.74ï¼Œå®å¹³å‡å€¼ä¸º0.59ï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸Šä¸‹æ–‡ç›¸å…³çš„ç±»åˆ«ä¸­ï¼Œå¦‚æ€ç»´å’Œç¤¾äº¤äº’åŠ¨æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚Bio_ClinicalBERTç²¾åº¦é«˜ä½†å¬å›ç‡ä½ï¼Œåœ¨æŸäº›ç±»åˆ«ç±»å‹ï¼ˆå¦‚åœ°ç‚¹å’Œæ—¶é—´ï¼‰ä¸Šä¸Llamaè¡¨ç°ç›¸å½“ï¼Œä½†åœ¨å…¶ä»–ç±»åˆ«ï¼ˆå¦‚æ€ç»´ã€æƒ…æ„Ÿå’Œç¤¾äº¤äº’åŠ¨ï¼‰ä¸Šè¡¨ç°ä¸ä½³ã€‚ä¸ä¼ ç»Ÿçš„ä¿¡æ¯æå–ä»»åŠ¡ç›¸æ¯”ï¼Œç”±äºå¤æ‚è®¤çŸ¥è¿‡ç¨‹çš„å™è¿°å…·æœ‰æŠ½è±¡æ€§å’Œé‡å æ€§ï¼Œæ­¤ä»»åŠ¡å‘ˆç°å‡ºæ›´å¤§çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œéšç€ä¸æ–­çš„æ”¹è¿›ï¼Œè¿™äº›NLPç³»ç»Ÿåœ¨å®ç°è½»æ¾çš„é•¿æœŸè®¤çŸ¥åŠŸèƒ½ç›‘æµ‹æ–¹é¢å¤§æœ‰æ½œåŠ›ï¼Œå¹¶å°†ä½œä¸ºæ­£å¼ç¥ç»å¿ƒç†å­¦è¯„ä¼°åœ¨å¸•é‡‘æ£®ç—…ä¸­çš„å®è´µè¡¥å……ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08806v1">PDF</a> 15 pages, 4 figures, 1 table. Varada Khanna and Nilay Bhatt are co-first authors. Sule Tinaz and Hua Xu are co-senior authors. Corresponding author: Vipina K. Keloth (<a href="mailto:&#118;&#105;&#112;&#105;&#x6e;&#x61;&#x2e;&#x6b;&#x75;&#x74;&#x74;&#x69;&#x63;&#x68;&#x69;&#x6b;&#101;&#108;&#111;&#x74;&#104;&#64;&#121;&#x61;&#108;&#101;&#x2e;&#x65;&#x64;&#117;">&#118;&#105;&#112;&#105;&#x6e;&#x61;&#x2e;&#x6b;&#x75;&#x74;&#x74;&#x69;&#x63;&#x68;&#x69;&#x6b;&#101;&#108;&#111;&#x74;&#104;&#64;&#121;&#x61;&#108;&#101;&#x2e;&#x65;&#x64;&#117;</a>)</p>
<p><strong>æ‘˜è¦</strong><br>å¸•é‡‘æ£®ç—…æ‚£è€…çš„è®¤çŸ¥ä½“éªŒæè¿°å¯¹äºäº†è§£ç–¾ç—…ç›¸å…³çš„è®¤çŸ¥å’Œæƒ…ç»ªå˜åŒ–å…·æœ‰å®è´µä»·å€¼ã€‚æœ¬ç ”ç©¶å¼€å‘å¹¶è¯„ä¼°äº†è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ï¼Œç”¨äºä»å»æ ‡è¯†åŒ–çš„ç¬¬ä¸€äººç§°å™è¿°ä¸­è‡ªåŠ¨è¯†åˆ«åæ˜ å„ç§è®¤çŸ¥è¿‡ç¨‹çš„ç±»åˆ«ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹æ€§èƒ½åœ¨ä¸åŒç±»åˆ«å’Œæ¨¡å‹å®¶æ—ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚å¾®è°ƒåçš„Meta-Llama-3-8B-Instructæ¨¡å‹åœ¨å¾®å¹³å‡å’Œå®å¹³å‡çš„F1å¾—åˆ†ä¸Šè¡¨ç°æœ€ä½³ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸Šä¸‹æ–‡ç›¸å…³çš„ç±»åˆ«ä¸­å¦‚æ€ç»´å’Œç¤¾äº¤äº’åŠ¨ã€‚Bio_ClinicalBERTè¡¨ç°å‡ºè¾ƒé«˜çš„ç²¾åº¦ä½†å¬å›ç‡è¾ƒä½ï¼Œåœ¨æŸäº›ç±»åˆ«å¦‚åœ°ç‚¹å’Œæ—¶é—´ä¸Šä¸Llamaè¡¨ç°ç›¸å½“ï¼Œä½†åœ¨å…¶ä»–ç±»åˆ«å¦‚æ€ç»´ã€æƒ…æ„Ÿå’Œç¤¾äº¤äº’åŠ¨æ–¹é¢è¡¨ç°ä¸ä½³ã€‚è¿™äº›NLPç³»ç»Ÿåœ¨ç»§ç»­æ”¹è¿›çš„æƒ…å†µä¸‹ï¼Œæœ‰æœ›å®ç°å¯¹è®¤çŸ¥åŠŸèƒ½çš„ä½è´Ÿæ‹…é•¿æœŸç›‘æµ‹ï¼Œå¹¶æˆä¸ºå¸•é‡‘æ£®ç—…æ­£å¼ç¥ç»å¿ƒç†å­¦è¯„ä¼°çš„æœ‰ç›Šè¡¥å……ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>PDæ‚£è€…çš„æ—¥å¸¸è®¤çŸ¥ä½“éªŒæè¿°å¯¹äºç†è§£ç–¾ç—…ç›¸å…³çš„è®¤çŸ¥å’Œæƒ…ç»ªå˜åŒ–è‡³å…³é‡è¦ã€‚</li>
<li>è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹èƒ½å¤Ÿä»æ‚£è€…çš„éç»“æ„åŒ–å™è¿°ä¸­è‡ªåŠ¨æå–è®¤çŸ¥è¿‡ç¨‹ç±»åˆ«ã€‚</li>
<li>ä¸åŒæ¨¡å‹å®¶æ—åœ¨å¤„ç†ä¸åŒè®¤çŸ¥ç±»åˆ«æ—¶çš„è¡¨ç°å­˜åœ¨å·®å¼‚ã€‚</li>
<li>Meta-Llama-3-8B-Instructæ¨¡å‹åœ¨æ€»ä½“æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ï¼Œå°¤å…¶åœ¨å¤„ç†ä¸Šä¸‹æ–‡ç›¸å…³çš„è®¤çŸ¥ç±»åˆ«ä¸Šå¦‚æ€ç»´å’Œç¤¾äº¤äº’åŠ¨ã€‚</li>
<li>Bio_ClinicalBERTæ¨¡å‹åœ¨ç²¾ç¡®æ€§æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¬å›ç‡ä¸Šæœ‰æ‰€ä¸è¶³ï¼Œå¯¹æŸäº›ç±»åˆ«å¦‚åœ°ç‚¹å’Œæ—¶é—´çš„è¯†åˆ«è¾ƒä¸ºå‡†ç¡®ï¼Œä½†åœ¨æ€ç»´ã€æƒ…æ„Ÿå’Œç¤¾äº¤äº’åŠ¨ç­‰ç±»åˆ«ä¸Šçš„è¡¨ç°æœ‰å¾…æé«˜ã€‚</li>
<li>ä¸ä¼ ç»Ÿä¿¡æ¯æå–ä»»åŠ¡ç›¸æ¯”ï¼Œä»å™è¿°ä¸­æå–å¤æ‚è®¤çŸ¥è¿‡ç¨‹çš„ä»»åŠ¡æ›´å…·æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå™è¿°å…·æœ‰æŠ½è±¡æ€§å’Œäº¤å‰æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08806">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a70f6f84c3ebd6fe2370ba83c7654cf6" align="middle">
<img src="https://picx.zhimg.com/v2-b29dc4b6672580d626701ca05cd810c0" align="middle">
<img src="https://picx.zhimg.com/v2-41ceea66ef7e96e7ba1b6a86050b4c3f" align="middle">
<img src="https://picx.zhimg.com/v2-8af4966d2a86395f2f06b5cc5db77f1e" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="RESTL-Reinforcement-Learning-Guided-by-Multi-Aspect-Rewards-for-Signal-Temporal-Logic-Transformation"><a href="#RESTL-Reinforcement-Learning-Guided-by-Multi-Aspect-Rewards-for-Signal-Temporal-Logic-Transformation" class="headerlink" title="RESTL: Reinforcement Learning Guided by Multi-Aspect Rewards for Signal Temporal Logic Transformation"></a>RESTL: Reinforcement Learning Guided by Multi-Aspect Rewards for Signal Temporal Logic Transformation</h2><p><strong>Authors:Yue Fang, Jin Zhi, Jie An, Hongshen Chen, Xiaohong Chen, Naijun Zhan</strong></p>
<p>Signal Temporal Logic (STL) is a powerful formal language for specifying real-time specifications of Cyber-Physical Systems (CPS). Transforming specifications written in natural language into STL formulas automatically has attracted increasing attention. Existing rule-based methods depend heavily on rigid pattern matching and domain-specific knowledge, limiting their generalizability and scalability. Recently, Supervised Fine-Tuning (SFT) of large language models (LLMs) has been successfully applied to transform natural language into STL. However, the lack of fine-grained supervision on atomic proposition correctness, semantic fidelity, and formula readability often leads SFT-based methods to produce formulas misaligned with the intended meaning. To address these issues, we propose RESTL, a reinforcement learning (RL)-based framework for the transformation from natural language to STL. RESTL introduces multiple independently trained reward models that provide fine-grained, multi-faceted feedback from four perspectives, i.e., atomic proposition consistency, semantic alignment, formula succinctness, and symbol matching. These reward models are trained with a curriculum learning strategy to improve their feedback accuracy, and their outputs are aggregated into a unified signal that guides the optimization of the STL generator via Proximal Policy Optimization (PPO). Experimental results demonstrate that RESTL significantly outperforms state-of-the-art methods in both automatic metrics and human evaluations.</p>
<blockquote>
<p>ä¿¡å·æ—¶åºé€»è¾‘ï¼ˆSTLï¼‰æ˜¯æè¿°ç½‘ç»œç‰©ç†ç³»ç»Ÿï¼ˆCPSï¼‰å®æ—¶è§„èŒƒçš„ä¸€ç§å¼ºå¤§å½¢å¼è¯­è¨€ã€‚å°†è‡ªç„¶è¯­è¨€ç¼–å†™çš„è§„èŒƒè‡ªåŠ¨è½¬æ¢ä¸ºSTLå…¬å¼å·²ç»å¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç°æœ‰çš„åŸºäºè§„åˆ™çš„æ–¹æ³•ä¸¥é‡ä¾èµ–äºåƒµåŒ–çš„æ¨¡å¼åŒ¹é…å’Œé¢†åŸŸç‰¹å®šçŸ¥è¯†ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å·²æˆåŠŸåº”ç”¨äºå°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºSTLã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å…³äºåŸå­å‘½é¢˜æ­£ç¡®æ€§ã€è¯­ä¹‰ä¿çœŸå’Œå…¬å¼å¯è¯»æ€§çš„ç²¾ç»†ç›‘ç£ï¼ŒåŸºäºSFTçš„æ–¹æ³•å¾€å¾€ä¼šäº§ç”Ÿä¸æ„å›¾ä¸ç¬¦çš„å…¬å¼ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RESTLï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è‡ªç„¶è¯­è¨€åˆ°STLè½¬æ¢æ¡†æ¶ã€‚RESTLå¼•å…¥äº†å¤šä¸ªç‹¬ç«‹è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼Œä»å››ä¸ªè§’åº¦æä¾›ç²¾ç»†ã€å¤šæ–¹é¢çš„åé¦ˆï¼Œå³åŸå­å‘½é¢˜ä¸€è‡´æ€§ã€è¯­ä¹‰å¯¹é½ã€å…¬å¼ç®€æ´æ€§å’Œç¬¦å·åŒ¹é…ã€‚è¿™äº›å¥–åŠ±æ¨¡å‹é‡‡ç”¨è¯¾ç¨‹å­¦ä¹ ç­–ç•¥è¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜å…¶åé¦ˆå‡†ç¡®æ€§ï¼Œå®ƒä»¬çš„è¾“å‡ºè¢«èšåˆä¸ºä¸€ä¸ªç»Ÿä¸€ä¿¡å·ï¼Œé€šè¿‡è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰æŒ‡å¯¼STLç”Ÿæˆå™¨çš„ä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRESTLåœ¨è‡ªåŠ¨åº¦é‡æŒ‡æ ‡å’Œäººç±»è¯„ä¼°æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08555v1">PDF</a> 12 pages, 4 figures</p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†Signal Temporal Logicï¼ˆSTLï¼‰ä½œä¸ºä¸€ç§å¼ºå¤§çš„å½¢å¼è¯­è¨€åœ¨æŒ‡å®šç½‘ç»œç‰©ç†ç³»ç»Ÿå®æ—¶è§„èŒƒæ–¹é¢çš„åº”ç”¨ã€‚æ–‡ç« æŒ‡å‡ºå°†è‡ªç„¶è¯­è¨€è§„èŒƒè½¬æ¢ä¸ºSTLå…¬å¼çš„æ–¹æ³•å·²ç»å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å·²æˆåŠŸåº”ç”¨äºæ­¤è½¬æ¢ï¼Œä½†ç¼ºä¹åŸå­å‘½é¢˜æ­£ç¡®æ€§ã€è¯­ä¹‰ä¿çœŸå’Œå…¬å¼å¯è¯»æ€§çš„ç²¾ç»†ç›‘ç£å¸¸å¸¸å¯¼è‡´äº§ç”Ÿçš„å…¬å¼ä¸é¢„æœŸæ„ä¹‰ä¸ç¬¦ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„è½¬æ¢æ¡†æ¶RESTLï¼Œé€šè¿‡å¤šé‡ç‹¬ç«‹è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ä»å››ä¸ªç»´åº¦æä¾›ç²¾ç»†ã€å¤šç»´çš„åé¦ˆï¼Œå¹¶é€šè¿‡è¯¾ç¨‹å­¦ä¹ ç­–ç•¥æé«˜åé¦ˆç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRESTLåœ¨è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å’Œäººç±»è¯„ä»·æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>STLæ˜¯ä¸€ç§ç”¨äºæè¿°ç½‘ç»œç‰©ç†ç³»ç»Ÿå®æ—¶è§„èŒƒçš„å½¢å¼è¯­è¨€ã€‚</li>
<li>å°†è‡ªç„¶è¯­è¨€è§„èŒƒè½¬æ¢ä¸ºSTLå…¬å¼å·²å—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¦‚ä¾èµ–æ¨¡å¼åŒ¹é…å’Œé¢†åŸŸç‰¹å®šçŸ¥è¯†ï¼Œå½±å“å…¶é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>SFTè™½æˆåŠŸåº”ç”¨äºè¯­è¨€æ¨¡å‹ï¼Œä½†ç¼ºä¹ç²¾ç»†ç›‘ç£å¯èƒ½å¯¼è‡´å…¬å¼ä¸é¢„æœŸæ„ä¹‰ä¸ç¬¦ã€‚</li>
<li>RESTLæ¡†æ¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œé€šè¿‡å¤šé‡å¥–åŠ±æ¨¡å‹æä¾›ç²¾ç»†åé¦ˆã€‚</li>
<li>å¥–åŠ±æ¨¡å‹ä»å››ä¸ªç»´åº¦è¿›è¡Œè¯„ä»·ï¼šåŸå­å‘½é¢˜ä¸€è‡´æ€§ã€è¯­ä¹‰å¯¹é½ã€å…¬å¼ç®€æ´æ€§å’Œç¬¦å·åŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08555">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e069e5fd783b45dd906c6a6d2fcee69" align="middle">
<img src="https://picx.zhimg.com/v2-5a004d372f307c10545bed68a94bac1a" align="middle">
<img src="https://picx.zhimg.com/v2-98da20a4c2b4a14d0109f127c0eb62e8" align="middle">
<img src="https://picx.zhimg.com/v2-ac159cea42286654daa0709938703fba" align="middle">
<img src="https://picx.zhimg.com/v2-6fe666ef45df8edb4708798a7b0d04ac" align="middle">
<img src="https://picx.zhimg.com/v2-0b6b2674705e81bbe509064ec912a4db" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0a848a00a420f03031281421c1fd90d6" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  MVU-Eval Towards Multi-Video Understanding Evaluation for Multimodal LLMs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-897d803ec990f59bb3cc02d9c3e21d42" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
