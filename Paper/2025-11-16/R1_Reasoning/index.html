<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-897d803ec990f59bb3cc02d9c3e21d42')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-16-æ›´æ–°"><a href="#2025-11-16-æ›´æ–°" class="headerlink" title="2025-11-16 æ›´æ–°"></a>2025-11-16 æ›´æ–°</h1><h2 id="Enhancing-the-Outcome-Reward-based-RL-Training-of-MLLMs-with-Self-Consistency-Sampling"><a href="#Enhancing-the-Outcome-Reward-based-RL-Training-of-MLLMs-with-Self-Consistency-Sampling" class="headerlink" title="Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling"></a>Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling</h2><p><strong>Authors:Jiahao Wang, Weiye Xu, Aijun Yang, Wengang Zhou, Lewei Lu, Houqiang Li, Xiaohua Wang, Jinguo Zhu</strong></p>
<p>Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.</p>
<blockquote>
<p>ç»“æœå¯¼å‘çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯ä¼˜åŒ–å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„é€æ­¥æ¨ç†çš„ä¸€ç§å¸¸è§ä¸”æ—¥ç›Šé‡è¦çš„æ–¹æ³•ã€‚åœ¨å¤šé€‰ç¯å¢ƒä¸­â€”â€”å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•çš„ä¸»å¯¼å½¢å¼â€”â€”è¯¥æ–¹æ³•é¢ä¸´ä¸€ä¸ªé‡å¤§ä½†ç»å¸¸è¢«å¿½è§†çš„é—®é¢˜ï¼šä¸å¿ å®çš„è½¨è¿¹åœ¨é”™è¯¯çš„æ€ç»´é“¾æ¡åçŒœæµ‹æ­£ç¡®é€‰é¡¹ï¼Œä¸çœŸå®æ¨ç†è·å¾—ç›¸åŒå¥–åŠ±ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸å®¹å¿½è§†çš„ç¼ºé™·ã€‚æˆ‘ä»¬æå‡ºè‡ªæˆ‘ä¸€è‡´æ€§é‡‡æ ·ï¼ˆSCSï¼‰æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å¯¹äºæ¯ä¸ªé—®é¢˜ï¼ŒSCSï¼ˆiï¼‰å¼•å…¥å¾®å°çš„è§†è§‰æ‰°åŠ¨ï¼Œå¹¶ï¼ˆiiï¼‰å¯¹åˆå§‹è½¨è¿¹è¿›è¡Œé‡å¤æˆªæ–­å’Œé‡æ–°é‡‡æ ·ï¼›ç»“æœè½¨è¿¹ä¹‹é—´çš„ä¸€è‡´æ€§èƒ½äº§ç”Ÿå¯åŒºåˆ†çš„ä¸€è‡´æ€§å¾—åˆ†ï¼Œè¿™åœ¨ç­–ç•¥æ›´æ–°æ—¶ä¼šé™ä½ä¸å¯é è½¨è¿¹çš„æƒé‡ã€‚åŸºäºQwen2.5-VL-7B-Instructï¼Œå°†SCSæ’å…¥RLOOã€GRPOå’ŒREINFORCE++ç³»åˆ—ï¼Œåœ¨å…­ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾7.7ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”é¢å¤–è®¡ç®—é‡å¾®ä¹å…¶å¾®ã€‚SCSåœ¨Qwen2.5-VL-3B-Instructå’ŒInternVL3-8Bä¸Šä¹Ÿå–å¾—äº†æ˜¾è‘—çš„æ”¶ç›Šï¼Œä¸ºMLLMsä¸­çš„ç»“æœå¯¼å‘RLæä¾›äº†ç®€å•ã€é€šç”¨çš„è¡¥æ•‘æªæ–½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10648v1">PDF</a> Accepted to NeurIPS 2025 (The Thirty-Ninth Annual Conference on Neural Information Processing Systems)</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€æ­¥æ¨ç†ä¸­ï¼Œç»“æœå¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯ä¸€ç§å¸¸è§ä¸”æ—¥ç›Šé‡è¦çš„æ–¹æ³•ã€‚åœ¨å¤šé€‰æ¨¡å¼ä¸‹ï¼Œè¯¥æ–¹æ³•é¢ä¸´ä¸€ä¸ªé‡å¤§ä½†è¢«å¿½è§†çš„é—®é¢˜ï¼šä¸å¿ å®çš„è½¨è¿¹ã€‚å³ä½¿çŒœæµ‹æ­£ç¡®é€‰é¡¹çš„æ¨ç†è¿‡ç¨‹å­˜åœ¨é”™è¯¯ï¼Œä¹Ÿä¼šè·å¾—ä¸çœŸå®æ¨ç†ç›¸åŒçš„å¥–åŠ±ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªæˆ‘ä¸€è‡´æ€§é‡‡æ ·ï¼ˆSCSï¼‰æ–¹æ³•ã€‚SCSé€šè¿‡ï¼ˆiï¼‰å¼•å…¥å¾®å°è§†è§‰æ‰°åŠ¨å’Œï¼ˆiiï¼‰å¯¹åˆå§‹è½¨è¿¹è¿›è¡Œé‡å¤æˆªæ–­å’Œé‡æ–°é‡‡æ ·ï¼Œä¸ºæ¯ä¸ªé—®é¢˜ç”Ÿæˆä¸€è‡´æ€§åˆ†æ•°ã€‚åœ¨ç­–ç•¥æ›´æ–°è¿‡ç¨‹ä¸­ï¼Œä¸å¯é çš„è½¨è¿¹ä¼šè¢«é™ä½æƒé‡ã€‚åŸºäºQwen2.5-VL-7B-Instructï¼Œå°†SCSåº”ç”¨äºRLOOã€GRPOå’ŒREINFORCE++ç³»åˆ—ï¼Œåœ¨å…­ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡æé«˜äº†é«˜è¾¾7.7ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”é¢å¤–è®¡ç®—é‡å¾®ä¹å…¶å¾®ã€‚SCSåœ¨Qwen2.5-VL-3B-Instructå’ŒInternVL3-8Bä¸Šä¹Ÿå–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œä¸ºMLLMsä¸­çš„ç»“æœå¥–åŠ±RLæä¾›äº†ç®€å•é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»“æœå¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„é€æ­¥æ¨ç†ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>å¤šé€‰æ¨¡å¼ä¸‹ï¼ŒRLé¢ä¸´ä¸€ä¸ªè¢«å¿½è§†çš„é—®é¢˜ï¼šé”™è¯¯æ¨ç†è·¯å¾„ä¹Ÿå¯èƒ½è·å¾—æ­£ç¡®ç­”æ¡ˆçš„å¥–åŠ±ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºäº†è‡ªæˆ‘ä¸€è‡´æ€§é‡‡æ ·ï¼ˆSCSï¼‰æ–¹æ³•ã€‚</li>
<li>SCSé€šè¿‡å¼•å…¥è§†è§‰æ‰°åŠ¨å’Œé‡å¤è½¨è¿¹é‡‡æ ·ï¼Œè®¡ç®—ä¸€è‡´æ€§åˆ†æ•°æ¥åŒºåˆ†å¯é ä¸ä¸å¯é çš„è½¨è¿¹ã€‚</li>
<li>SCSåœ¨å¤šä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡ï¼Œä¸”é¢å¤–è®¡ç®—æˆæœ¬è¾ƒä½ã€‚</li>
<li>SCSåœ¨å¤šç§MLLMsä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„é€‚ç”¨æ€§ï¼Œæä¾›äº†ä¸€ä¸ªé€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10648">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5770523b02b4aa1671994558f7c0f47" align="middle">
<img src="https://picx.zhimg.com/v2-a9bfcfdb6da5397857cf44f59944ffa1" align="middle">
<img src="https://picx.zhimg.com/v2-c159d43af98429c21148f370f8710bcc" align="middle">
<img src="https://picx.zhimg.com/v2-043764230e493279155e2e230efc8960" align="middle">
<img src="https://picx.zhimg.com/v2-fecbbd5237f577a314cf5a74b4fd674d" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Flexible-Simulation-Based-Inference-for-Galaxy-Photometric-Fitting-with-Synthesizer"><a href="#Flexible-Simulation-Based-Inference-for-Galaxy-Photometric-Fitting-with-Synthesizer" class="headerlink" title="Flexible Simulation Based Inference for Galaxy Photometric Fitting with Synthesizer"></a>Flexible Simulation Based Inference for Galaxy Photometric Fitting with Synthesizer</h2><p><strong>Authors:Thomas Harvey, Christopher C. Lovell, Sophie Newman, Christopher J. Conselice, Duncan Austin, Aswin P. Vijayan, Stephen M. Wilkins, Vadim Rusakov, Qiong Li, Nathan Adams, Kai Magdwick, Matthew Ho</strong></p>
<p>We introduce Synference, a new, flexible Python framework for galaxy SED fitting using simulation-based inference (SBI). Synference leverages the Synthesizer package for flexible forward-modelling of galaxy SEDs and integrates the LtU-ILI package to ensure best practices in model training and validation. In this work we demonstrate Synference by training a neural posterior estimator on $10^6$ simulated galaxies, based on a flexible 8-parameter physical model, to infer galaxy properties from 14-band HST and JWST photometry. We validate this model, demonstrating excellent parameter recovery (e.g. R$^2&gt;$0.99 for M$_\star$) and accurate posterior calibration against nested sampling results. We apply our trained model to 3,088 spectroscopically-confirmed galaxies in the JADES GOODS-South field. The amortized inference is exceptionally fast, having nearly fixed cost per posterior evaluation and processing the entire sample in $\sim$3 minutes on a single CPU (18 galaxies&#x2F;CPU&#x2F;sec), a $\sim$1700$\times$ speedup over traditional nested sampling or MCMC techniques. We demonstrate Synferenceâ€™s ability to simultaneously infer photometric redshifts and physical parameters, and highlight its utility for rapid Bayesian model comparison by demonstrating systematic stellar mass differences between two commonly used stellar population synthesis models. Synference is a powerful, scalable tool poised to maximise the scientific return of next-generation galaxy surveys.</p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Synferenceï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„çµæ´»Pythonæ¡†æ¶ï¼Œåˆ©ç”¨åŸºäºæ¨¡æ‹Ÿçš„æ¨æ–­ï¼ˆSBIï¼‰è¿›è¡Œæ˜Ÿç³»SEDæ‹Ÿåˆã€‚Synferenceåˆ©ç”¨Synthesizerè½¯ä»¶åŒ…å¯¹æ˜Ÿç³»SEDè¿›è¡Œçµæ´»çš„å‰å‘å»ºæ¨¡ï¼Œå¹¶é›†æˆLtU-ILIè½¯ä»¶åŒ…ä»¥ç¡®ä¿æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯çš„æœ€ä½³å®è·µã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çµæ´»çš„8å‚æ•°ç‰©ç†æ¨¡å‹å¯¹æ¨¡æ‹Ÿçš„$ 10^6 $ä¸ªæ˜Ÿç³»è¿›è¡Œè®­ç»ƒç¥ç»ç½‘ç»œåéªŒä¼°è®¡å™¨ï¼ŒåŸºäº14æ³¢æ®µHSTå’ŒJWSTçš„å…‰åº¦æµ‹é‡æ¨æ–­æ˜Ÿç³»å±æ€§ã€‚æˆ‘ä»¬å¯¹æ¨¡å‹è¿›è¡Œäº†éªŒè¯ï¼Œå±•ç¤ºäº†å‡ºè‰²çš„å‚æ•°æ¢å¤èƒ½åŠ›ï¼ˆä¾‹å¦‚ï¼Œæ’æ˜Ÿè´¨é‡M_çš„RÂ²&gt; 0.99ï¼‰ï¼Œå¹¶ä¸”ç›¸å¯¹äºåµŒå¥—é‡‡æ ·ç»“æœçš„åéªŒæ ¡å‡†æ˜¯å‡†ç¡®çš„ã€‚æˆ‘ä»¬å°†è®­ç»ƒå¥½çš„æ¨¡å‹åº”ç”¨äºJADES GOODS-Southå­—æ®µä¸­çš„3,088ä¸ªå…‰è°±ç¡®è®¤çš„æ˜Ÿç³»ã€‚æ‘Šé”€æ¨æ–­éå¸¸å¿«ï¼Œå‡ ä¹æ¯æ¬¡åéªŒè¯„ä¼°éƒ½æœ‰å›ºå®šçš„æˆæœ¬ï¼Œåœ¨å•ä¸ªCPUä¸Šå¤„ç†æ•´ä¸ªæ ·æœ¬å¤§çº¦éœ€è¦3åˆ†é’Ÿï¼ˆæ¯ç§’å¤„ç†18ä¸ªæ˜Ÿç³»&#x2F; CPUï¼‰ï¼Œæ¯”ä¼ ç»Ÿçš„åµŒå¥—é‡‡æ ·æˆ–MCMCæŠ€æœ¯å¤§çº¦å¿«çº¦1700å€ã€‚æˆ‘ä»¬å±•ç¤ºäº†SynferenceåŒæ—¶æ¨æ–­å…‰åº¦çº¢ç§»å’Œç‰©ç†å‚æ•°çš„èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å±•ç¤ºä¸¤ä¸ªå¸¸ç”¨çš„æ’æ˜Ÿäººå£åˆæˆæ¨¡å‹ä¹‹é—´çš„ç³»ç»Ÿæ€§æ’æ˜Ÿè´¨é‡å·®å¼‚æ¥çªå‡ºå…¶åœ¨å¿«é€Ÿè´å¶æ–¯æ¨¡å‹æ¯”è¾ƒä¸­çš„å®ç”¨æ€§ã€‚Synferenceæ˜¯ä¸€ä¸ªå¼ºå¤§ä¸”å¯æ‰©å±•çš„å·¥å…·ï¼Œæ—¨åœ¨æœ€å¤§é™åº¦åœ°æé«˜ä¸‹ä¸€ä»£æ˜Ÿç³»è°ƒæŸ¥çš„ç§‘ç ”äº§å‡ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10640v1">PDF</a> 23 pages, 12 figures. Submitted to MNRAS. The Synference package is available at <a target="_blank" rel="noopener" href="https://github.com/synthesizer-project/synference/">https://github.com/synthesizer-project/synference/</a></p>
<p><strong>Summary</strong><br>    Synferenceæ˜¯ä¸€ä¸ªç”¨äºæ˜Ÿç³»SEDæ‹Ÿåˆçš„æ–°é¢–ã€çµæ´»çš„Pythonæ¡†æ¶ï¼Œé‡‡ç”¨æ¨¡æ‹Ÿæ¨ç†ï¼ˆSBIï¼‰ã€‚å®ƒåˆ©ç”¨SynthesizeråŒ…è¿›è¡Œçµæ´»çš„æ˜Ÿç³»SEDæ­£å‘å»ºæ¨¡ï¼Œå¹¶ä¸LtU-ILIåŒ…é›†æˆä»¥ç¡®ä¿æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯çš„æœ€ä½³å®è·µã€‚é€šè¿‡å¯¹æ¨¡æ‹Ÿæ˜Ÿç³»æ•°æ®çš„è®­ç»ƒï¼ŒéªŒè¯äº†Synferenceæ¨¡å‹åœ¨å‚æ•°æ¢å¤ä¸Šçš„ä¼˜ç§€è¡¨ç°åŠå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨å¤„ç†å¤§é‡æ˜Ÿç³»æ•°æ®æ—¶å…·æœ‰é«˜æ•ˆç‡ï¼Œèƒ½å¤Ÿå®ç°å¿«é€Ÿçš„åéªŒè¯„ä¼°ï¼Œå¤§å¹…æé€Ÿäºä¼ ç»Ÿçš„åµŒå¥—é‡‡æ ·æˆ–MCMCæŠ€æœ¯ã€‚åŒæ—¶ï¼ŒSynferenceå¯ä»¥åŒæ—¶æ¨æ–­å…‰åº¦å’Œç‰©ç†å‚æ•°ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨ä¸åŒæ’æ˜Ÿäººå£åˆæˆæ¨¡å‹ä¹‹é—´çš„ç³»ç»Ÿæ’æ˜Ÿè´¨é‡å·®å¼‚æ¯”è¾ƒæ–¹é¢çš„å®ç”¨æ€§ã€‚å¯¹äºä¸‹ä¸€ä»£æ˜Ÿç³»è°ƒæŸ¥ï¼ŒSynferenceæ˜¯ä¸€ä¸ªå¼ºå¤§ä¸”å¯æ‰©å±•çš„å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Synferenceæ˜¯ä¸€ä¸ªåŸºäºæ¨¡æ‹Ÿæ¨ç†ï¼ˆSBIï¼‰çš„Pythonæ¡†æ¶ï¼Œç”¨äºçµæ´»çš„æ˜Ÿç³»SEDæ‹Ÿåˆã€‚</li>
<li>åˆ©ç”¨SynthesizeråŒ…è¿›è¡Œæ­£å‘å»ºæ¨¡ï¼Œå¹¶ä¸LtU-ILIé›†æˆç¡®ä¿æœ€ä½³å®è·µã€‚</li>
<li>é€šè¿‡è®­ç»ƒç¥ç»ç½‘ç»œåéªŒä¼°è®¡å™¨åœ¨æ¨¡æ‹Ÿæ•°æ®ä¸ŠéªŒè¯äº†æ¨¡å‹çš„ä¼˜ç§€æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹å±•ç°å‡ºè‰¯å¥½çš„å‚æ•°æ¢å¤èƒ½åŠ›ï¼ˆå¦‚Mstarçš„RÂ²&gt; 0.99ï¼‰ã€‚</li>
<li>åéªŒè¯„ä¼°å…·æœ‰é«˜æ•ˆç‡ï¼Œå¤„ç†å¤§é‡æ ·æœ¬æ—¶è¡¨ç°å‡ºæ˜¾è‘—çš„é€Ÿåº¦ä¼˜åŠ¿ã€‚</li>
<li>åŒæ—¶å…·å¤‡æ¨æ–­å…‰åº¦å’Œç‰©ç†å‚æ•°çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10640">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20455bfa02e931576416fc71c6a1686a" align="middle">
<img src="https://picx.zhimg.com/v2-a0b457fb06c5125c2804bd6365b7a87f" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Instella-Fully-Open-Language-Models-with-Stellar-Performance"><a href="#Instella-Fully-Open-Language-Models-with-Stellar-Performance" class="headerlink" title="Instella: Fully Open Language Models with Stellar Performance"></a>Instella: Fully Open Language Models with Stellar Performance</h2><p><strong>Authors:Jiang Liu, Jialian Wu, Xiaodong Yu, Yusheng Su, Prakamya Mishra, Gowtham Ramesh, Sudhanshu Ranjan, Chaitanya Manem, Ximeng Sun, Ze Wang, Pratik Prabhanjan Brahma, Zicheng Liu, Emad Barsoum</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.</p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç„¶è€Œå¤§å¤šæ•°é«˜æ€§èƒ½æ¨¡å‹ä»ç„¶æ˜¯é—­æºçš„æˆ–éƒ¨åˆ†å¼€æºçš„ï¼Œè¿™é™åˆ¶äº†é€æ˜åº¦å’Œå¯é‡å¤æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Instellaï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æºçš„ä¸‰äº¿å‚æ•°è¯­è¨€æ¨¡å‹å®¶æ—ï¼Œè¯¥æ¨¡å‹å®Œå…¨åŸºäºå…¬å¼€å¯ç”¨æ•°æ®å’Œä»£ç åº“è¿›è¡Œè®­ç»ƒã€‚å€ŸåŠ©AMD Instinct MI300X GPUï¼ŒInstellaé€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒã€é€šç”¨æŒ‡ä»¤è°ƒæ•´å’Œä¸äººç±»åå¥½å¯¹é½çš„æ–¹å¼å¼€å‘ã€‚å°½ç®¡ä½¿ç”¨çš„é¢„è®­ç»ƒä»¤ç‰Œæ•°é‡æ¯”è®¸å¤šåŒé¾„äººå°‘å¾—å¤šï¼Œä½†Instellaåœ¨å®Œå…¨å¼€æºçš„æ¨¡å‹ä¸­å®ç°äº†æœ€æ–°æŠ€æœ¯æˆæœï¼Œå¹¶ä¸”ä¸åŒç­‰è§„æ¨¡çš„é¢†å…ˆå¼€æºæƒé‡æ¨¡å‹ç›¸ç«äº‰ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸¤ä¸ªä¸“ä¸šç‰ˆæœ¬ï¼šInstella-Longï¼Œèƒ½å¤Ÿå¤„ç†é•¿è¾¾128Kä»¤ç‰Œçš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä»¥åŠä¸“æ³¨äºæ¨ç†çš„Instella-Mathï¼Œè¯¥æ¨¡å‹é€šè¿‡æ•°å­¦ä»»åŠ¡çš„ç›‘ç£å¾®è°ƒå¼ºåŒ–å­¦ä¹ è¿›è¡Œå¢å¼ºã€‚æ€»ä¹‹ï¼Œè¿™äº›è´¡çŒ®ä½¿Instellaæˆä¸ºç¤¾åŒºä¸­é€æ˜ã€é«˜æ€§èƒ½å’Œé€šç”¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†å¼€æºå’Œå¯é‡å¤çš„è¯­è¨€å»ºæ¨¡ç ”ç©¶ç›®æ ‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10628v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹å®¶æ—Instellaçš„å‡ºç°æ˜¯ä¸€ä¸ªé‡è¦çªç ´ã€‚è¯¥æ¨¡å‹å®Œå…¨åŸºäºå…¬å¼€æ•°æ®å’Œä»£ç åº“å¼€å‘ï¼Œé‡‡ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒã€é€šç”¨æŒ‡ä»¤è°ƒæ•´å’Œä¸äººç±»åå¥½å¯¹é½çš„æŠ€æœ¯ã€‚å°½ç®¡é¢„è®­ç»ƒä»¤ç‰Œæ•°é‡è¾ƒè®¸å¤šå½“ä»£æ¨¡å‹å¤§å¤§å‡å°‘ï¼ŒInstellaåœ¨å®Œå…¨å¼€æºæ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æˆæœï¼Œå¹¶ä¸ç›¸å½“è§„æ¨¡çš„å¼€æºæƒé‡æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†å¤„ç†ä¸Šä¸‹æ–‡é•¿åº¦è¾¾128Kä»¤ç‰Œçš„Instella-Longå’Œé’ˆå¯¹æ•°å­¦ä»»åŠ¡è¿›è¡Œå¼ºåŒ–å­¦ä¹ å’Œç›‘ç£å¾®è°ƒåçš„æ¨ç†èšç„¦æ¨¡å‹Instella-Mathä¸¤ä¸ªä¸“ä¸šå˜ç§ã€‚è¿™äº›è´¡çŒ®ä½¿Instellaæˆä¸ºç¤¾åŒºä¸­é€æ˜ã€é«˜æ€§èƒ½å’Œé€šç”¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†å¼€æ”¾å’Œå¯é‡å¤çš„è¯­è¨€å»ºæ¨¡ç ”ç©¶ç›®æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Instellaæ˜¯é¦–ä¸ªå®Œå…¨åŸºäºå…¬å¼€æ•°æ®å’Œä»£ç åº“å¼€å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹å®¶æ—ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæŠ€æœ¯ï¼Œå¹¶æ³¨é‡é€šç”¨æŒ‡ä»¤è°ƒæ•´å’Œä¸äººç±»åå¥½å¯¹é½ã€‚</li>
<li>å°½ç®¡ä½¿ç”¨äº†è¾ƒå°‘çš„é¢„è®­ç»ƒä»¤ç‰Œï¼ŒInstellaåœ¨å¼€æºæ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æˆæœã€‚</li>
<li>Instellaä¸åŒç­‰è§„æ¨¡çš„å¼€æºæƒé‡æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>æ¨å‡ºäº†å¤„ç†é•¿æ–‡æœ¬çš„ä¸“ä¸šæ¨¡å‹Instella-Longã€‚</li>
<li>è¿˜æ¨å‡ºäº†é’ˆå¯¹æ•°å­¦ä»»åŠ¡çš„æ¨ç†èšç„¦æ¨¡å‹Instella-Mathã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0b50544bfc46f77d0a24e192f86b13da" align="middle">
<img src="https://picx.zhimg.com/v2-a8fab08c1708160e25109b396472203a" align="middle">
<img src="https://picx.zhimg.com/v2-0d12dbb0c23252412ca1b14a472fb1cb" align="middle">
<img src="https://picx.zhimg.com/v2-52234f6b2d92afdc850b28d7dd191cd0" align="middle">
<img src="https://picx.zhimg.com/v2-897d803ec990f59bb3cc02d9c3e21d42" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SSR-Socratic-Self-Refine-for-Large-Language-Model-Reasoning"><a href="#SSR-Socratic-Self-Refine-for-Large-Language-Model-Reasoning" class="headerlink" title="SSR: Socratic Self-Refine for Large Language Model Reasoning"></a>SSR: Socratic Self-Refine for Large Language Model Reasoning</h2><p><strong>Authors:Haizhou Shi, Ye Liu, Bo Pang, Zeyu Leo Liu, Hao Wang, Silvio Savarese, Caiming Xiong, Yingbo Zhou, Semih Yavuz</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at <a target="_blank" rel="noopener" href="https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning">https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning</a>.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å±•ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œç„¶è€Œç°æœ‰çš„æµ‹è¯•æ—¶é—´æ¡†æ¶é€šå¸¸ä¾èµ–äºç²—ç•¥çš„è‡ªæˆ‘éªŒè¯å’Œè‡ªæˆ‘æ ¡æ­£ï¼Œè¿™åœ¨å¤æ‚ä»»åŠ¡ä¸Šé™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è‹æ ¼æ‹‰åº•è‡ªæˆ‘ç²¾ç‚¼ï¼ˆSSRï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œç”¨äºå¯¹LLMæ¨ç†è¿›è¡Œç²¾ç»†ç²’åº¦çš„è¯„ä¼°å’Œç²¾ç¡®ä¼˜åŒ–ã€‚æˆ‘ä»¬æå‡ºçš„SSRå°†æ¨¡å‹å“åº”åˆ†è§£ä¸ºå¯éªŒè¯çš„ï¼ˆå­é—®é¢˜ã€å­ç­”æ¡ˆï¼‰å¯¹ï¼Œé€šè¿‡å—æ§çš„é‡æ–°æ±‚è§£å’Œè‡ªæˆ‘ä¸€è‡´æ€§æ£€æŸ¥ï¼Œå®ç°æ­¥éª¤çº§åˆ«çš„ç½®ä¿¡åº¦ä¼°è®¡ã€‚é€šè¿‡å®šä½ä¸å¯é çš„æ­¥éª¤å¹¶å¯¹å…¶è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼ŒSSRç”Ÿæˆæ›´å‡†ç¡®ä¸”å¯è§£é‡Šçš„æ¨ç†é“¾ã€‚åœ¨äº”ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•å’Œä¸‰ä¸ªLLMä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒSSRå§‹ç»ˆä¼˜äºæœ€æ–°çš„è¿­ä»£è‡ªæˆ‘ä¼˜åŒ–åŸºçº¿ã€‚é™¤äº†æ€§èƒ½æå‡å¤–ï¼ŒSSRè¿˜æä¾›äº†ä¸€ç§æœ‰åŸåˆ™çš„é»‘ç›’æ–¹æ³•æ¥è¯„ä¼°å’Œäº†è§£LLMçš„å†…éƒ¨æ¨ç†è¿‡ç¨‹ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning">https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10621v1">PDF</a> Preprint; work in progress</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œä½†ç°æœ‰çš„æµ‹è¯•æ¡†æ¶å¾€å¾€ä¾èµ–äºç²—ç³™çš„è‡ªæˆ‘éªŒè¯å’Œä¿®æ­£ï¼Œå¯¹äºå¤æ‚ä»»åŠ¡æ•ˆæœä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºåä¸ºè‹æ ¼æ‹‰åº•è‡ªæˆ‘ç²¾è¿›ï¼ˆSSRï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨ç²¾ç»†åœ°è¯„ä¼°å¹¶ç²¾è¿›LLMçš„æ¨ç†èƒ½åŠ›ã€‚SSRé€šè¿‡å°†æ¨¡å‹å“åº”åˆ†è§£æˆå¯éªŒè¯çš„ï¼ˆå­é—®é¢˜ã€å­ç­”æ¡ˆï¼‰å¯¹ï¼Œé€šè¿‡å—æ§çš„é‡æ–°æ±‚è§£å’Œè‡ªæˆ‘ä¸€è‡´æ€§æ£€æŸ¥ï¼Œå®ç°æ­¥éª¤çº§åˆ«çš„ç½®ä¿¡åº¦ä¼°è®¡ã€‚SSRèƒ½å¤Ÿç²¾å‡†å®šä½ä¸å¯é çš„æ­¥éª¤å¹¶è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®ä¸”å¯è§£é‡Šæ€§å¼ºçš„æ¨ç†é“¾ã€‚åœ¨äº”ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•å’Œä¸‰ç§LLMä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒSSRæŒç»­ä¼˜äºæœ€æ–°çš„è¿­ä»£è‡ªæˆ‘ç²¾è¿›åŸºçº¿ã€‚æ­¤å¤–ï¼ŒSSRè¿˜ä¸ºè¯„ä¼°å’Œç†è§£LLMçš„å†…éƒ¨æ¨ç†è¿‡ç¨‹æä¾›äº†ä¸€ä¸ªæœ‰åŸåˆ™çš„é»‘ç›’æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†ç°æœ‰æµ‹è¯•æ¡†æ¶åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ•ˆæœæœ‰é™ã€‚</li>
<li>è‹æ ¼æ‹‰åº•è‡ªæˆ‘ç²¾è¿›ï¼ˆSSRï¼‰æ¡†æ¶æ—¨åœ¨ç²¾ç»†è¯„ä¼°å¹¶ç²¾è¿›LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>SSRé€šè¿‡åˆ†è§£æ¨¡å‹å“åº”ä¸ºå¯éªŒè¯çš„ï¼ˆå­é—®é¢˜ã€å­ç­”æ¡ˆï¼‰å¯¹ï¼Œå®ç°æ­¥éª¤çº§åˆ«çš„ç½®ä¿¡åº¦ä¼°è®¡ã€‚</li>
<li>SSRèƒ½å¤Ÿå®šä½å¹¶ä¼˜åŒ–ä¸å¯é çš„æ¨ç†æ­¥éª¤ï¼Œç”Ÿæˆæ›´å‡†ç¡®ä¸”å¯è§£é‡Šçš„æ¨ç†é“¾ã€‚</li>
<li>SSRåœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºç°æœ‰çš„è¿­ä»£è‡ªæˆ‘ç²¾è¿›æ–¹æ³•ã€‚</li>
<li>SSRæä¾›äº†ä¸€ä¸ªè¯„ä¼°å’Œç†è§£LLMå†…éƒ¨æ¨ç†è¿‡ç¨‹çš„é»‘ç›’æ–¹æ³•ã€‚</li>
<li>SSRæ¡†æ¶çš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a13627c835266701336b1a5f413f336d" align="middle">
<img src="https://picx.zhimg.com/v2-338c8dafacec3031df700905d70933ce" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Evaluating-Prompting-Strategies-with-MedGemma-for-Medical-Order-Extraction"><a href="#Evaluating-Prompting-Strategies-with-MedGemma-for-Medical-Order-Extraction" class="headerlink" title="Evaluating Prompting Strategies with MedGemma for Medical Order Extraction"></a>Evaluating Prompting Strategies with MedGemma for Medical Order Extraction</h2><p><strong>Authors:Abhinand Balachandran, Bavana Durgapraveen, Gowsikkan Sikkan Sudhagar, Vidhya Varshany J S, Sriram Rajkumar</strong></p>
<p>The accurate extraction of medical orders from doctor-patient conversations is a critical task for reducing clinical documentation burdens and ensuring patient safety. This paper details our team submission to the MEDIQA-OE-2025 Shared Task. We investigate the performance of MedGemma, a new domain-specific open-source language model, for structured order extraction. We systematically evaluate three distinct prompting paradigms: a straightforward one-Shot approach, a reasoning-focused ReAct framework, and a multi-step agentic workflow. Our experiments reveal that while more complex frameworks like ReAct and agentic flows are powerful, the simpler one-shot prompting method achieved the highest performance on the official validation set. We posit that on manually annotated transcripts, complex reasoning chains can lead to â€œoverthinkingâ€ and introduce noise, making a direct approach more robust and efficient. Our work provides valuable insights into selecting appropriate prompting strategies for clinical information extraction in varied data conditions.</p>
<blockquote>
<p>ä»åŒ»ç”Ÿä¸æ‚£è€…å¯¹è¯ä¸­å‡†ç¡®æå–åŒ»ç–—æŒ‡ä»¤æ˜¯å‡è½»ä¸´åºŠæ–‡æ¡£è´Ÿæ‹…å’Œç¡®ä¿æ‚£è€…å®‰å…¨çš„å…³é”®ä»»åŠ¡ã€‚æœ¬æ–‡è¯¦ç»†ä»‹ç»äº†æˆ‘ä»¬å›¢é˜Ÿå¯¹MEDIQA-OE-2025å…±äº«ä»»åŠ¡çš„æäº¤å†…å®¹ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†MedGemmaï¼ˆä¸€ç§æ–°çš„ç‰¹å®šé¢†åŸŸçš„å¼€æºè¯­è¨€æ¨¡å‹ï¼‰åœ¨ç»“æ„åŒ–æŒ‡ä»¤æå–æ–¹é¢çš„æ€§èƒ½ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸‰ç§ä¸åŒçš„æç¤ºèŒƒå¼ï¼šç›´æ¥çš„One-Shotæ–¹æ³•ã€æ³¨é‡æ¨ç†çš„ReActæ¡†æ¶å’Œå¤šæ­¥ä»£ç†å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè™½ç„¶åƒReActå’Œä»£ç†æµç¨‹è¿™æ ·çš„æ›´å¤æ‚æ¡†æ¶å¾ˆå¼ºå¤§ï¼Œä½†æ›´ç®€å•çš„ä¸€æ¬¡æ€§æç¤ºæ–¹æ³•å´åœ¨å®˜æ–¹éªŒè¯é›†ä¸Šå–å¾—äº†æœ€é«˜æ€§èƒ½ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œåœ¨æ‰‹åŠ¨æ³¨é‡Šçš„æ–‡æœ¬ä¸­ï¼Œå¤æ‚çš„æ¨ç†é“¾å¯èƒ½ä¼šå¯¼è‡´â€œè¿‡åº¦æ€è€ƒâ€å¹¶å¼•å…¥å™ªéŸ³ï¼Œä½¿ç›´æ¥æ–¹æ³•æ›´åŠ ç¨³å¥å’Œé«˜æ•ˆã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†åœ¨å¤šç§æ•°æ®æ¡ä»¶ä¸‹é€‰æ‹©é€‚å½“æç¤ºç­–ç•¥è¿›è¡Œä¸´åºŠä¿¡æ¯æå–çš„å®è´µè§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10583v1">PDF</a> 2 figures 7 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å›¢é˜Ÿåœ¨MEDIQA-OE-2025å…±äº«ä»»åŠ¡ä¸­çš„æäº¤å†…å®¹ã€‚å›¢é˜Ÿæ¢ç©¶äº†MedGemmaè¿™ä¸€æ–°çš„é¢†åŸŸç‰¹å®šå¼€æºè¯­è¨€æ¨¡å‹åœ¨ç»“æ„åŒ–è®¢å•æå–ä¸­çš„è¡¨ç°ï¼Œå¹¶ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸‰ç§ä¸åŒçš„æç¤ºèŒƒå¼ï¼šç›´æ¥çš„ä¸€ç«™å¼æ–¹æ³•ã€ä¾§é‡äºæ¨ç†çš„ReActæ¡†æ¶ä»¥åŠå¤šæ­¥éª¤ä»£ç†å·¥ä½œæµç¨‹ã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡å¤æ‚çš„æ¡†æ¶å¦‚ReActå’Œä»£ç†å·¥ä½œæµç¨‹åŠŸèƒ½å¼ºå¤§ï¼Œä½†ç®€å•çš„ä¸€ç«™å¼æç¤ºæ–¹æ³•å´åœ¨å®˜æ–¹éªŒè¯é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚ä½œè€…è®¤ä¸ºï¼Œåœ¨æ‰‹åŠ¨æ³¨é‡Šçš„è½¬å½•ä¸­ï¼Œå¤æ‚çš„æ¨ç†é“¾å¯èƒ½å¯¼è‡´è¿‡åº¦æ€è€ƒå¹¶å¼•å…¥å™ªéŸ³ï¼Œä½¿ç›´æ¥æ–¹æ³•æ›´å…·ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚è¿™é¡¹å·¥ä½œä¸ºåœ¨ä¸åŒæ•°æ®æ¡ä»¶ä¸‹è¿›è¡Œä¸´åºŠä¿¡æ¯æå–æ—¶é€‰æ‹©åˆé€‚æç¤ºç­–ç•¥æä¾›äº†å®è´µè§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®æå–åŒ»ç–—è®¢å•å¯¹äºå‡è½»ä¸´åºŠè®°å½•è´Ÿæ‹…å’Œç¡®ä¿æ‚£è€…å®‰å…¨è‡³å…³é‡è¦ã€‚</li>
<li>MedGemmaæ˜¯ä¸€ç§æ–°çš„é¢†åŸŸç‰¹å®šå¼€æºè¯­è¨€æ¨¡å‹ï¼Œè¢«ç”¨äºç»“æ„åŒ–è®¢å•æå–ã€‚</li>
<li>å›¢é˜Ÿæ¢ç©¶äº†ä¸‰ç§ä¸åŒçš„æç¤ºèŒƒå¼ï¼šä¸€ç«™å¼æ–¹æ³•ã€ReActæ¡†æ¶å’Œå¤šæ­¥éª¤ä»£ç†å·¥ä½œæµç¨‹ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œç®€å•çš„ä¸€ç«™å¼æç¤ºæ–¹æ³•åœ¨å®˜æ–¹éªŒè¯é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</li>
<li>å¤æ‚çš„æ¨ç†é“¾å¯èƒ½å¯¼è‡´è¿‡åº¦æ€è€ƒå¹¶å¼•å…¥å™ªéŸ³ï¼Œå½±å“æå–çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç›´æ¥æ–¹æ³•å…·æœ‰æ›´é«˜çš„ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10583">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b451574c4678367a029c41fd77687dcc" align="middle">
<img src="https://picx.zhimg.com/v2-b45c3a563e8c13930b58e1d430d3395b" align="middle">
<img src="https://picx.zhimg.com/v2-c7ca31b4dec10cfbe12fefc8f08579ce" align="middle">
<img src="https://picx.zhimg.com/v2-bfda0cd19f82035ca8f844f5b5ba58ea" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LOCA-R-Near-Perfect-Performance-on-the-Chinese-Physics-Olympiad-2025"><a href="#LOCA-R-Near-Perfect-Performance-on-the-Chinese-Physics-Olympiad-2025" class="headerlink" title="LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025"></a>LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025</h2><p><strong>Authors:Dong-Shan Jian, Xiang Li, Chen-Xu Yan, Hui-Wen Zheng, Zhi-Zhang Bian, You-Le Fang, Sheng-Qi Zhang, Bing-Rui Gong, Ren-Xi He, Jing-Tian Zhang, Ce Meng, Yan-Qing Ma</strong></p>
<p>Olympiad-level physics problem-solving presents a significant challenge for both humans and artificial intelligence (AI), as it requires a sophisticated integration of precise calculation, abstract reasoning, and a fundamental grasp of physical principles. The Chinese Physics Olympiad (CPhO), renowned for its complexity and depth, serves as an ideal and rigorous testbed for these advanced capabilities. In this paper, we introduce LOCA-R (LOgical Chain Augmentation for Reasoning), an improved version of the LOCA framework adapted for complex reasoning, and apply it to the CPhO 2025 theory examination. LOCA-R achieves a near-perfect score of 313 out of 320 points, solidly surpassing the highest-scoring human competitor and significantly outperforming all baseline methods.</p>
<blockquote>
<p>å¥¥æ—åŒ¹å…‹çº§åˆ«çš„ç‰©ç†é—®é¢˜è§£å†³å¯¹äºäººç±»å’Œäººå·¥æ™ºèƒ½éƒ½æ„æˆäº†é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒè¦æ±‚ç²¾ç¡®è®¡ç®—çš„å¤æ‚é›†æˆã€æŠ½è±¡æ¨ç†ä»¥åŠå¯¹ç‰©ç†åŸç†çš„åŸºæœ¬æŒæ¡ã€‚ä»¥å¤æ‚å’Œæ·±åˆ»è‘—ç§°çš„ä¸­å›½ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›æ˜¯æµ‹è¯•è¿™äº›é«˜çº§èƒ½åŠ›çš„ç†æƒ³ä¸”ä¸¥æ ¼çš„æµ‹è¯•å¹³å°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LOCA-Rï¼ˆç”¨äºæ¨ç†çš„é€»è¾‘é“¾å¢å¼ºï¼‰ï¼Œè¿™æ˜¯LOCAæ¡†æ¶çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œç”¨äºé€‚åº”å¤æ‚æ¨ç†ï¼Œå¹¶åº”ç”¨äºCPhO 2025ç†è®ºè€ƒè¯•ã€‚LOCA-Rè·å¾—äº†æ¥è¿‘å®Œç¾çš„åˆ†æ•°ï¼Œå¾—åˆ†ä¸º313åˆ†ä¸­çš„è¿‘æ»¡åˆ†ï¼ˆæœ€é«˜åˆ†è¢«ç«èµ›æ—¶è¶…è¶Šå¹¶åˆ—ä½äºç¬¬ä¸‰ä½ï¼Œä½†è¢«ç•¥å¾®æ‰©å¤§é—´è·é¢†å…ˆè¶…è¿‡äº†ä»¥å‰æ–¹æ³•çš„ç¡®æˆå°±æ»¡æ»¡ä¸ä¸€ç›´è¶…è¿‡æœªè°ƒæ•´çš„æ£€æµ‹ç»“æœä»¥åŠåœ¨ç°å®çš„èƒœå‡ºç»“åˆå¼€åˆ›çš„æ—¶ä»£æœ€é«˜å³°ï¼‰ã€‚å®ƒç‰¢å›ºåœ°è¶…è¶Šäº†æœ€é«˜åˆ†çš„äººç±»ç«äº‰å¯¹æ‰‹ï¼Œå¹¶æ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10515v1">PDF</a> 19 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡æœ¬ä¸»è¦ä»‹ç»äº†åœ¨ç‰©ç†å­¦å¥¥æ—åŒ¹å…‹ç«èµ›çº§åˆ«çš„è§£é¢˜èƒ½åŠ›å¯¹äºäººå·¥æ™ºèƒ½å’Œäººç±»æ¥è¯´æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œè¦æ±‚ç²¾ç¡®è®¡ç®—ã€æŠ½è±¡æ¨ç†ä»¥åŠå¯¹ç‰©ç†åŸç†çš„åŸºæœ¬æŒæ¡ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œä¸­å›½ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›æˆä¸ºäº†æ£€éªŒè¿™äº›é«˜çº§èƒ½åŠ›çš„ç†æƒ³åœºæ‰€ã€‚æ–‡ä¸­å¼•å…¥äº†ä¸€ç§åä¸ºLOCA-Rçš„é€»è¾‘é“¾å¢å¼ºæ¨ç†æ–¹æ³•ï¼Œå®ƒåœ¨CPhO 2025ç†è®ºè€ƒè¯•ä¸­å–å¾—äº†è¿‘ä¹å®Œç¾çš„æˆç»©ï¼Œè¿œè¶…äººç±»é¡¶å°–é€‰æ‰‹å’Œæ‰€æœ‰åŸºå‡†æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç‰©ç†å­¦å¥¥æ—åŒ¹å…‹ç«èµ›çº§åˆ«çš„è§£é¢˜èƒ½åŠ›å¯¹äºäººå·¥æ™ºèƒ½å’Œäººç±»éƒ½æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œéœ€è¦ç²¾ç¡®è®¡ç®—ã€æŠ½è±¡æ¨ç†ä»¥åŠå¯¹ç‰©ç†åŸç†çš„æ·±å…¥ç†è§£ã€‚</li>
<li>ä¸­å›½ç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›æ˜¯æ£€éªŒé«˜çº§èƒ½åŠ›çš„ç†æƒ³åœºæ‰€ã€‚</li>
<li>LOCA-Ræ–¹æ³•æ˜¯ä¸€ç§æ”¹è¿›çš„æ¨ç†æ–¹æ³•ï¼Œé€‚ç”¨äºå¤æ‚æ¨ç†ã€‚</li>
<li>LOCA-Råœ¨CPhO 2025ç†è®ºè€ƒè¯•ä¸­å–å¾—äº†è¿‘ä¹å®Œç¾çš„æˆç»©ã€‚</li>
<li>LOCA-Rçš„è¡¨ç°è¿œè¶…äººç±»é¡¶å°–é€‰æ‰‹ã€‚</li>
<li>LOCA-Rç›¸æ¯”å…¶ä»–åŸºå‡†æ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10515">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb671299168d6dc6308e2ef3912b6e7a" align="middle">
<img src="https://picx.zhimg.com/v2-c5bd67329570b5149adf7ee426b362f0" align="middle">
<img src="https://picx.zhimg.com/v2-99f37322185208566169aa167842af39" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Bayesian-model-comparison-and-validation-with-Gaussian-Process-Regression-for-interferometric-21-cm-signal-recovery"><a href="#Bayesian-model-comparison-and-validation-with-Gaussian-Process-Regression-for-interferometric-21-cm-signal-recovery" class="headerlink" title="Bayesian model comparison and validation with Gaussian Process Regression for interferometric 21-cm signal recovery"></a>Bayesian model comparison and validation with Gaussian Process Regression for interferometric 21-cm signal recovery</h2><p><strong>Authors:Yuchen Liu, Eloy de Lera Acedo, Peter Sims</strong></p>
<p>The 21-cm signal from neutral hydrogen is anticipated to reveal critical insights into the formation of early cosmic structures during the Cosmic Dawn and the subsequent Epoch of Reionization. However, the intrinsic faintness of the signal, as opposed to astrophysical foregrounds, poses a formidable challenge for its detection. Motivated by the recent success of machine learning based Gaussian Process Regression (GPR) methods in LOFAR and NenuFAR observations, we perform a Bayesian comparison among five GPR models to account for the simulated 4-hour tracking observations with the SKA-Low telescope. The simulated sky is convolved with the instrumental beam response and includes realistic radio sources and thermal noise from 122 to 134 MHz. A Bayesian model evaluation framework is applied to five GPR models to discern the most effective modelling strategy and determine the optimal model parameters. The GPR model with wedge parametrization ($\textit{Wedge}$) and its extension ($Î±\textit{Noise}$) with noise scaling achieve the highest Bayesian evidence of the observed data and the least biased 21-cm power spectrum recovery. The $Î±\textit{Noise}$ and $\textit{Wedge}$ models also forecast the best local power-spectrum recovery, demonstrating fractional differences of $-0.14%$ and $0.47%$ respectively, compared to the injected 21-cm power at $k &#x3D; 0.32\ \mathrm{h\ cMpc}^{-1}$. We additionally perform Bayesian null tests to validate the five models, finding that the two optimal models also pass with the remaining three models yielding spurious detections in data containing no 21-cm signal.</p>
<blockquote>
<p>é¢„è®¡ä¸­æ€§æ°¢çš„21å˜ç±³ä¿¡å·å°†æ­ç¤ºå®‡å®™é»æ˜å’Œéšåçš„å†ç”µç¦»æ—¶ä»£æ—©æœŸå®‡å®™ç»“æ„å½¢æˆçš„å…³é”®è§è§£ã€‚ç„¶è€Œï¼Œä¸å¤©æ–‡å‰æ™¯ç›¸æ¯”ï¼Œä¿¡å·æœ¬èº«çš„å¾®å¼±æ€§ç»™å…¶æ£€æµ‹å¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚å—LOFARå’ŒNenuFARè§‚æµ‹ä¸­åŸºäºæœºå™¨å­¦ä¹ çš„é«˜æ–¯è¿‡ç¨‹å›å½’ï¼ˆGPRï¼‰æ–¹æ³•è¿‘æœŸæˆåŠŸçš„æ¿€åŠ±ï¼Œæˆ‘ä»¬å¯¹äº”ç§GPRæ¨¡å‹è¿›è¡Œäº†è´å¶æ–¯æ¯”è¾ƒï¼Œä»¥è§£é‡Šä½¿ç”¨SKA-Lowæœ›è¿œé•œè¿›è¡Œçš„æ¨¡æ‹Ÿ4å°æ—¶è·Ÿè¸ªè§‚æµ‹ã€‚æ¨¡æ‹Ÿçš„å¤©ç©ºä¸ä»ªå™¨å“åº”å·ç§¯åœ¨ä¸€èµ·ï¼Œå¹¶åŒ…æ‹¬æ¥è‡ª122è‡³134å…†èµ«çš„çœŸå®å°„ç”µæºå’Œçƒ­å™ªå£°ã€‚åº”ç”¨è´å¶æ–¯æ¨¡å‹è¯„ä¼°æ¡†æ¶å¯¹äº”ä¸ªGPRæ¨¡å‹è¿›è¡Œé‰´åˆ«ï¼Œä»¥ç¡®å®šæœ€æœ‰æ•ˆçš„å»ºæ¨¡ç­–ç•¥å¹¶ç¡®å®šæœ€ä½³æ¨¡å‹å‚æ•°ã€‚å…·æœ‰æ¥”å½¢å‚æ•°åŒ–ï¼ˆWedgedï¼‰çš„GPRæ¨¡å‹åŠå…¶å¸¦å™ªå£°ç¼©æ”¾çš„æ‰©å±•ï¼ˆÎ±Noiseï¼‰è·å¾—äº†æœ€é«˜è´å¶æ–¯è¯æ®å’Œæ¢å¤æœ€å°‘çš„21å˜ç±³åŠŸç‡è°±ã€‚Î±Noiseå’ŒWedgedæ¨¡å‹è¿˜é¢„æµ‹äº†æœ€ä½³å±€éƒ¨åŠŸç‡è°±æ¢å¤ï¼Œåœ¨k &#x3D; 0.32 h cMpc^-1çš„æ³¨å…¥çš„21å˜ç±³åŠŸç‡ä¸‹ï¼Œåˆ†åˆ«è¡¨ç°å‡º-0.14%å’Œ0.47%çš„å·®å¼‚ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†è´å¶æ–¯é›¶æ•ˆæ£€éªŒæ¥éªŒè¯è¿™äº”ç§æ¨¡å‹ï¼Œå‘ç°ä¸¤ä¸ªæœ€ä½³æ¨¡å‹åŒæ ·æœ‰æ•ˆï¼Œå…¶ä½™ä¸‰ä¸ªæ¨¡å‹åœ¨æ²¡æœ‰åŒ…å«çœŸå®çš„ 21å˜ç±³ä¿¡å·çš„æ£€æµ‹æ•°æ®ä¼šäº§ç”Ÿé”™è¯¯çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10499v1">PDF</a> 25 pages, 17 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä¸»è¦ä»‹ç»äº†åˆ©ç”¨æœºå™¨å­¦ä¹ ä¸­çš„é«˜æ–¯è¿‡ç¨‹å›å½’ï¼ˆGPRï¼‰æ¨¡å‹æ£€æµ‹æ—©æœŸå®‡å®™ç»“æ„å½¢æˆçš„é‡è¦ä¿¡å·â€”â€”ä¸­æ€§æ°¢çš„21å˜ç±³ä¿¡å·ã€‚é¢å¯¹ä¿¡å·æœ¬èº«çš„å¾®å¼±ä»¥åŠä¸å¤©ä½“ç‰©ç†å‰æ™¯çš„å¯¹æ¯”æŒ‘æˆ˜ï¼Œé€šè¿‡æ¨¡æ‹ŸSKA-Lowæœ›è¿œé•œçš„4å°æ—¶è§‚æµ‹æ•°æ®ï¼Œå¯¹äº”ç§GPRæ¨¡å‹è¿›è¡Œè´å¶æ–¯æ¯”è¾ƒè¯„ä¼°ï¼Œæœ€ç»ˆå‘ç°å¸¦æœ‰wedgeå‚æ•°åŒ–çš„GPRæ¨¡å‹åŠå…¶å™ªå£°æ‰©å±•æ¨¡å‹åœ¨æ•°æ®è§‚æµ‹å’Œ21å˜ç±³åŠŸç‡è°±æ¢å¤ä¸Šè¡¨ç°æœ€ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸­æ€§æ°¢çš„21å˜ç±³ä¿¡å·å¯¹äºäº†è§£å®‡å®™é»æ˜å’Œå†ç”µç¦»æ—¶ä»£æ—©æœŸå®‡å®™ç»“æ„çš„å½¢æˆè‡³å…³é‡è¦ã€‚</li>
<li>ä¿¡å·æœ¬èº«çš„å¾®å¼±æ€§ä½¿å¾—æ£€æµ‹æˆä¸ºä¸€å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤©ä½“ç‰©ç†å‰æ™¯çš„å¯¹æ¯”ä¸‹ã€‚</li>
<li>åˆ©ç”¨æœºå™¨å­¦ä¹ ä¸­çš„é«˜æ–¯è¿‡ç¨‹å›å½’ï¼ˆGPRï¼‰æ¨¡å‹è¿›è¡Œæ¨¡æ‹Ÿè§‚æµ‹æ•°æ®çš„åˆ†æã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿçš„SKA-Lowæœ›è¿œé•œè§‚æµ‹æ•°æ®ä¸­ï¼Œè¿›è¡Œäº†äº”ç§GPRæ¨¡å‹çš„è´å¶æ–¯æ¯”è¾ƒè¯„ä¼°ã€‚</li>
<li>å¸¦wedgeå‚æ•°åŒ–çš„GPRæ¨¡å‹åŠå…¶å™ªå£°æ‰©å±•æ¨¡å‹åœ¨æ•°æ®è§‚æµ‹å’Œ21å˜ç±³åŠŸç‡è°±æ¢å¤ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>æœ€ä½³æ¨¡å‹çš„æœ¬åœ°åŠŸç‡è°±æ¢å¤ä¸æ³¨å…¥çš„21å˜ç±³åŠŸç‡ç›¸æ¯”ï¼Œå·®å¼‚æå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10499">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e90498a5916ad563e715a31bd7f85826" align="middle">
<img src="https://picx.zhimg.com/v2-88cde02802925520cc8e9ae510d488cf" align="middle">
<img src="https://picx.zhimg.com/v2-b4af2dd384f08b7a474e5d5dc9b873bc" align="middle">
<img src="https://picx.zhimg.com/v2-53eb501fd3d3b66ac52107b53f5ddccc" align="middle">
<img src="https://picx.zhimg.com/v2-61f824b4a52d21083ab55c6ee58c48aa" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SPOT-Sparsification-with-Attention-Dynamics-via-Token-Relevance-in-Vision-Transformers"><a href="#SPOT-Sparsification-with-Attention-Dynamics-via-Token-Relevance-in-Vision-Transformers" class="headerlink" title="SPOT: Sparsification with Attention Dynamics via Token Relevance in Vision Transformers"></a>SPOT: Sparsification with Attention Dynamics via Token Relevance in Vision Transformers</h2><p><strong>Authors:Oded Schlesinger, Amirhossein Farzam, J. Matias Di Martino, Guillermo Sapiro</strong></p>
<p>While Vision Transformers (ViT) have demonstrated remarkable performance across diverse tasks, their computational demands are substantial, scaling quadratically with the number of processed tokens. Compact attention representations, reflecting token interaction distributions, can guide early detection and reduction of less salient tokens prior to attention computation. Motivated by this, we present SParsification with attentiOn dynamics via Token relevance (SPOT), a framework for early detection of redundant tokens within ViTs that leverages token embeddings, interactions, and attention dynamics across layers to infer token importance, resulting in a more context-aware and interpretable relevance detection process. SPOT informs token sparsification and facilitates the elimination of such tokens, improving computational efficiency without sacrificing performance. SPOT employs computationally lightweight predictors that can be plugged into various ViT architectures and learn to derive effective input-specific token prioritization across layers. Its versatile design supports a range of performance levels adaptable to varying resource constraints. Empirical evaluations demonstrate significant efficiency gains of up to 40% compared to standard ViTs, while maintaining or even improving accuracy. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/odedsc/SPOT">https://github.com/odedsc/SPOT</a> .</p>
<blockquote>
<p>å°½ç®¡Vision Transformersï¼ˆViTï¼‰åœ¨ä¸åŒä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å…¶è®¡ç®—éœ€æ±‚å¾ˆå¤§ï¼Œå¹¶ä¸”ä¸å¤„ç†ä»¤ç‰Œçš„æ•°é‡çš„äºŒæ¬¡æ–¹æˆæ­£æ¯”ã€‚ç´§å‡‘çš„æ³¨æ„åŠ›è¡¨ç¤ºåæ˜ äº†ä»¤ç‰Œäº¤äº’åˆ†å¸ƒï¼Œå¯ä»¥åœ¨è®¡ç®—æ³¨æ„åŠ›ä¹‹å‰å¼•å¯¼æ—©æœŸæ£€æµ‹å’Œå‡å°‘ä¸å¤ªçªå‡ºçš„ä»¤ç‰Œã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºä»¤ç‰Œç›¸å…³æ€§çš„æ³¨æ„åŠ›åŠ¨æ€ç¨€ç–åŒ–ï¼ˆSPOTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—©æœŸæ£€æµ‹ViTä¸­å†—ä½™ä»¤ç‰Œçš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ä»¤ç‰ŒåµŒå…¥ã€äº¤äº’å’Œè·¨å±‚çš„æ³¨æ„åŠ›åŠ¨æ€æ¥æ¨æ–­ä»¤ç‰Œé‡è¦æ€§ï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªæ›´å…·ä¸Šä¸‹æ–‡æ„è¯†å’Œå¯è§£é‡Šæ€§çš„ç›¸å…³æ€§æ£€æµ‹è¿‡ç¨‹ã€‚SPOTä¸ºä»¤ç‰Œç¨€ç–åŒ–æä¾›äº†ä¿¡æ¯å¹¶ä¿ƒè¿›äº†ä»¤ç‰Œçš„æ¶ˆé™¤ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶ä¸ç‰ºç‰²æ€§èƒ½ã€‚SPOTé‡‡ç”¨è®¡ç®—é‡è½»çš„å¯é¢„æµ‹å› å­ï¼Œå¯ä»¥æ’å…¥å„ç§ViTæ¶æ„ä¸­ï¼Œå¹¶å­¦ä¹ åœ¨è·¨å±‚æ¨å¯¼æœ‰æ•ˆçš„è¾“å…¥ç‰¹å®šä»¤ç‰Œä¼˜å…ˆçº§ã€‚å…¶é€šç”¨è®¾è®¡æ”¯æŒé€‚åº”ä¸åŒèµ„æºçº¦æŸçš„å¤šç§æ€§èƒ½çº§åˆ«ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸æ ‡å‡†ViTç›¸æ¯”ï¼Œå…¶è¾¾åˆ°äº†é«˜è¾¾40%çš„æ˜¾è‘—æ•ˆç‡æå‡ï¼ŒåŒæ—¶ç»´æŒç”šè‡³æé«˜äº†å‡†ç¡®æ€§ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/odedsc/SPOT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/odedsc/SPOTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10488v1">PDF</a> Project repository: <a target="_blank" rel="noopener" href="https://github.com/odedsc/SPOT">https://github.com/odedsc/SPOT</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ³¨æ„åŠ›åŠ¨æ€å’Œä»¤ç‰Œç›¸å…³æ€§çš„ä»¤ç‰Œç¨€ç–åŒ–æ¡†æ¶SPOTã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨æ—©æœŸæ£€æµ‹å†—ä½™ä»¤ç‰Œï¼Œé€šè¿‡åˆ©ç”¨ä»¤ç‰ŒåµŒå…¥ã€äº¤äº’å’Œè·¨å±‚çš„æ³¨æ„åŠ›åŠ¨æ€æ¥æ¨æ–­ä»¤ç‰Œé‡è¦æ€§ï¼Œä»è€Œæé«˜è®¡ç®—æ•ˆç‡è€Œä¸æŸå¤±æ€§èƒ½ã€‚SPOTå¯ä»¥æ’å…¥å„ç§ViTæ¶æ„ä¸­ï¼Œå­¦ä¹ åœ¨è·¨å±‚ä¸­è·å¾—æœ‰æ•ˆçš„è¾“å…¥ç‰¹å®šä»¤ç‰Œä¼˜å…ˆçº§ã€‚å…¶çµæ´»çš„è®¾è®¡å¯é€‚åº”ä¸åŒçš„èµ„æºçº¦æŸï¼Œå¹¶åœ¨å®è¯è¯„ä¼°ä¸­å®ç°äº†ä¸æ ‡å‡†ViTç›¸æ¯”é«˜è¾¾40%çš„æ˜¾è‘—æ•ˆç‡æå‡ï¼ŒåŒæ—¶ä¿æŒç”šè‡³æé«˜å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViT)çš„è®¡ç®—éœ€æ±‚å¤§ï¼Œå¤„ç†ä»¤ç‰Œæ—¶å‘ˆäºŒæ¬¡æ–¹å¢é•¿ã€‚</li>
<li>SPOTæ¡†æ¶é€šè¿‡æ—©æœŸæ£€æµ‹å†—ä½™ä»¤ç‰Œæ¥æé«˜ViTçš„è®¡ç®—æ•ˆç‡ã€‚</li>
<li>SPOTåˆ©ç”¨ä»¤ç‰ŒåµŒå…¥ã€äº¤äº’å’Œè·¨å±‚æ³¨æ„åŠ›åŠ¨æ€æ¥æ¨æ–­ä»¤ç‰Œé‡è¦æ€§ã€‚</li>
<li>SPOTèƒ½å¤Ÿæ’å…¥å„ç§ViTæ¶æ„ä¸­ï¼Œå¹¶å…·æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œå¯è§£é‡Šçš„ä»¤ç‰Œæ£€æµ‹è¿‡ç¨‹ã€‚</li>
<li>SPOTé€šè¿‡å®ç°è¾“å…¥ç‰¹å®šä»¤ç‰Œä¼˜å…ˆçº§æ¥æé«˜è®¡ç®—æ•ˆç‡ã€‚</li>
<li>SPOTæ¡†æ¶çš„è®¾è®¡çµæ´»ï¼Œå¯é€‚åº”ä¸åŒçš„èµ„æºçº¦æŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10488">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-28be61f86349063bc7d735ce15df7a61" align="middle">
<img src="https://picx.zhimg.com/v2-cffe05ed24b9192e107716aa5fdaf000" align="middle">
<img src="https://picx.zhimg.com/v2-a47705684f8b2db141dbe00945636efb" align="middle">
<img src="https://picx.zhimg.com/v2-191569f649a718b08a2e95482beb8bbd" align="middle">
<img src="https://picx.zhimg.com/v2-169fbc248475de1d49a13eddf99f70de" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Beyond-Elicitation-Provision-based-Prompt-Optimization-for-Knowledge-Intensive-Tasks"><a href="#Beyond-Elicitation-Provision-based-Prompt-Optimization-for-Knowledge-Intensive-Tasks" class="headerlink" title="Beyond Elicitation: Provision-based Prompt Optimization for Knowledge-Intensive Tasks"></a>Beyond Elicitation: Provision-based Prompt Optimization for Knowledge-Intensive Tasks</h2><p><strong>Authors:Yunzhe Xu, Zhuosheng Zhang, Zhe Liu</strong></p>
<p>While prompt optimization has emerged as a critical technique for enhancing language model performance, existing approaches primarily focus on elicitation-based strategies that search for optimal prompts to activate modelsâ€™ capabilities. These methods exhibit fundamental limitations when addressing knowledge-intensive tasks, as they operate within fixed parametric boundaries rather than providing the factual knowledge, terminology precision, and reasoning patterns required in specialized domains. To address these limitations, we propose Knowledge-Provision-based Prompt Optimization (KPPO), a framework that reformulates prompt optimization as systematic knowledge integration rather than potential elicitation. KPPO introduces three key innovations: 1) a knowledge gap filling mechanism for knowledge gap identification and targeted remediation; 2) a batch-wise candidate evaluation approach that considers both performance improvement and distributional stability; 3) an adaptive knowledge pruning strategy that balances performance and token efficiency, reducing up to 29% token usage. Extensive evaluation on 15 knowledge-intensive benchmarks from various domains demonstrates KPPOâ€™s superiority over elicitation-based methods, with an average performance improvement of ~6% over the strongest baseline while achieving comparable or lower token consumption. Code at: <a target="_blank" rel="noopener" href="https://github.com/xyz9911/KPPO">https://github.com/xyz9911/KPPO</a>.</p>
<blockquote>
<p>è™½ç„¶æç¤ºä¼˜åŒ–å·²æˆä¸ºæé«˜è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å…³é”®æŠ€æœ¯ï¼Œä½†ç°æœ‰çš„æ–¹æ³•ä¸»è¦ä¾§é‡äºåŸºäºæ¿€å‘çš„ç­–ç•¥ï¼Œæœç´¢æœ€ä½³æç¤ºä»¥æ¿€æ´»æ¨¡å‹çš„èƒ½åŠ›ã€‚è¿™äº›æ–¹æ³•åœ¨åº”å¯¹çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡æ—¶å­˜åœ¨åŸºæœ¬å±€é™ï¼Œå› ä¸ºå®ƒä»¬æ˜¯åœ¨å›ºå®šçš„å‚æ•°è¾¹ç•Œå†…è¿è¡Œï¼Œè€Œä¸æ˜¯æä¾›ä¸“ä¸šçŸ¥è¯†ã€æœ¯è¯­ç²¾åº¦å’Œç‰¹å®šé¢†åŸŸæ‰€éœ€çš„æ¨ç†æ¨¡å¼ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºçŸ¥è¯†æä¾›çš„æç¤ºä¼˜åŒ–ï¼ˆKPPOï¼‰æ¡†æ¶ï¼Œå°†æç¤ºä¼˜åŒ–é‡æ–°å®šä¹‰ä¸ºç³»ç»ŸçŸ¥è¯†é›†æˆï¼Œè€Œéæ½œåœ¨æ¿€å‘ã€‚KPPOå¼•å…¥äº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼š1ï¼‰çŸ¥è¯†å·®è·å¡«å……æœºåˆ¶ï¼Œç”¨äºè¯†åˆ«çŸ¥è¯†å·®è·å¹¶é’ˆå¯¹æ€§åœ°è¿›è¡Œè¡¥æ•‘ï¼›2ï¼‰æ‰¹é‡å€™é€‰è¯„ä¼°æ–¹æ³•ï¼ŒåŒæ—¶è€ƒè™‘æ€§èƒ½æ”¹è¿›å’Œåˆ†å¸ƒç¨³å®šæ€§ï¼›3ï¼‰è‡ªé€‚åº”çŸ¥è¯†ä¿®å‰ªç­–ç•¥ï¼Œå¹³è¡¡æ€§èƒ½å’Œä»¤ç‰Œæ•ˆç‡ï¼Œå‡å°‘é«˜è¾¾29%çš„ä»¤ç‰Œä½¿ç”¨é‡ã€‚åœ¨15ä¸ªæ¥è‡ªä¸åŒé¢†åŸŸçš„çŸ¥è¯†å¯†é›†å‹åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒKPPOä¼˜äºåŸºäºæ¿€å‘çš„æ–¹æ³•ï¼Œåœ¨æœ€å¼ºåŸºçº¿çš„åŸºç¡€ä¸Šå¹³å‡æ€§èƒ½æé«˜çº¦6%ï¼ŒåŒæ—¶å®ç°ç›¸å½“æˆ–æ›´ä½çš„ä»¤ç‰Œæ¶ˆè€—ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/xyz9911/KPPO%E3%80%82">https://github.com/xyz9911/KPPOã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10465v1">PDF</a> 16 pages, 19 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è¯­è¨€æ¨¡å‹æ€§èƒ½ä¼˜åŒ–çš„çŸ¥è¯†æä¾›å‹æç¤ºä¼˜åŒ–æ¡†æ¶ï¼ˆKPPOï¼‰ã€‚è¯¥æ¡†æ¶çªç ´äº†ç°æœ‰æç¤ºä¼˜åŒ–æ–¹æ³•çš„å±€é™ï¼Œå°†çŸ¥è¯†æ•´åˆçº³å…¥å…¶ä¸­ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°åº”å¯¹çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ã€‚é€šè¿‡å¼•å…¥çŸ¥è¯†ç¼ºå£å¡«å……æœºåˆ¶ã€æ‰¹é‡å€™é€‰è¯„ä¼°æ–¹æ³•å’Œè‡ªé€‚åº”çŸ¥è¯†ä¿®å‰ªç­–ç•¥ï¼ŒKPPOåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹³å‡æ€§èƒ½æå‡çº¦6%ï¼ŒåŒæ—¶é™ä½äº†é«˜è¾¾29%çš„ä»¤ç‰Œä½¿ç”¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†æä¾›å‹æç¤ºä¼˜åŒ–æ¡†æ¶ï¼ˆKPPOï¼‰å°†çŸ¥è¯†æ•´åˆçº³å…¥è¯­è¨€æ¨¡å‹æ€§èƒ½ä¼˜åŒ–ä¸­ï¼Œçªç ´ç°æœ‰æç¤ºä¼˜åŒ–æ–¹æ³•çš„å±€é™ã€‚</li>
<li>KPPOå¼•å…¥çŸ¥è¯†ç¼ºå£å¡«å……æœºåˆ¶ï¼Œç”¨äºè¯†åˆ«å¹¶é’ˆå¯¹æ€§è§£å†³çŸ¥è¯†ç¼ºå£é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨æ‰¹é‡å€™é€‰è¯„ä¼°æ–¹æ³•ï¼ŒåŒæ—¶è€ƒè™‘æ€§èƒ½æå‡å’Œåˆ†å¸ƒç¨³å®šæ€§ã€‚</li>
<li>KPPOé‡‡ç”¨è‡ªé€‚åº”çŸ¥è¯†ä¿®å‰ªç­–ç•¥ï¼Œåœ¨æ€§èƒ½å’Œä»¤ç‰Œæ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚</li>
<li>KPPOåœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹³å‡æ€§èƒ½æå‡çº¦6%ã€‚</li>
<li>ä¸å…¶ä»–åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒKPPOå®ç°äº†æ›´ä½çš„ä»¤ç‰Œä½¿ç”¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10465">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-899c12b13fe0005d2df451c8debf0bc9" align="middle">
<img src="https://picx.zhimg.com/v2-88eb149ad7cf016074061754bd24f75e" align="middle">
<img src="https://picx.zhimg.com/v2-6c668b0ee84bd72cfd4b1e6e51a37f63" align="middle">
<img src="https://picx.zhimg.com/v2-878342177da068c67210b4ec0b608994" align="middle">
<img src="https://picx.zhimg.com/v2-7a74ed15d165f5fbf2b1b0704ebefe44" align="middle">
<img src="https://picx.zhimg.com/v2-2ba46e8e0fd7f28094788770bf4b453c" align="middle">
<img src="https://picx.zhimg.com/v2-22dc501c4d8587bc311ad0370e400bea" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LocalBench-Benchmarking-LLMs-on-County-Level-Local-Knowledge-and-Reasoning"><a href="#LocalBench-Benchmarking-LLMs-on-County-Level-Local-Knowledge-and-Reasoning" class="headerlink" title="LocalBench: Benchmarking LLMs on County-Level Local Knowledge and Reasoning"></a>LocalBench: Benchmarking LLMs on County-Level Local Knowledge and Reasoning</h2><p><strong>Authors:Zihan Gao, Yifei Xu, Jacob Thebault-Spieker</strong></p>
<p>Large language models (LLMs) have been widely evaluated on macro-scale geographic tasks, such as global factual recall, event summarization, and regional reasoning. Yet, their ability to handle hyper-local knowledge remains poorly understood. This gap is increasingly consequential as real-world applications, from civic platforms to community journalism, demand AI systems that can reason about neighborhood-specific dynamics, cultural narratives, and local governance. Existing benchmarks fall short in capturing this complexity, often relying on coarse-grained data or isolated references. We present LocalBench, the first benchmark designed to systematically evaluate LLMs on county-level local knowledge across the United States. Grounded in the Localness Conceptual Framework, LocalBench includes 14,782 validated question-answer pairs across 526 U.S. counties in 49 states, integrating diverse sources such as Census statistics, local subreddit discourse, and regional news. It spans physical, cognitive, and relational dimensions of locality. Using LocalBench, we evaluate 13 state-of-the-art LLMs under both closed-book and web-augmented settings. Our findings reveal critical limitations: even the best-performing models reach only 56.8% accuracy on narrative-style questions and perform below 15.5% on numerical reasoning. Moreover, larger model size and web augmentation do not guarantee better performance, for example, search improves Geminiâ€™s accuracy by +13.6%, but reduces GPT-series performance by -11.4%. These results underscore the urgent need for language models that can support equitable, place-aware AI systems: capable of engaging with the diverse, fine-grained realities of local communities across geographic and cultural contexts.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²åœ¨å®è§‚åœ°ç†ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œä¾‹å¦‚å…¨çƒäº‹å®å›å¿†ã€äº‹ä»¶æ‘˜è¦å’ŒåŒºåŸŸæ¨ç†ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¤„ç†è¶…æœ¬åœ°çŸ¥è¯†çš„èƒ½åŠ›ä»é²œä¸ºäººçŸ¥ã€‚éšç€ä»å…¬æ°‘å¹³å°åˆ°ç¤¾åŒºæ–°é—»ç­‰ç°å®åº”ç”¨çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ï¼Œè¦æ±‚äººå·¥æ™ºèƒ½ç³»ç»Ÿèƒ½å¤Ÿæ¨ç†é‚»é‡Œç‰¹å®šçš„åŠ¨æ€ã€æ–‡åŒ–å™äº‹å’Œå½“åœ°æ²»ç†ï¼Œè¿™ä¸€å·®è·å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨æ•æ‰è¿™ç§å¤æ‚æ€§æ–¹é¢è¡¨ç°ä¸è¶³ï¼Œé€šå¸¸ä¾èµ–äºç²—ç²’åº¦æ•°æ®æˆ–å­¤ç«‹çš„å¼•ç”¨ã€‚æˆ‘ä»¬æ¨å‡ºäº†LocalBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨ç³»ç»Ÿè¯„ä¼°ç¾å›½å¿çº§æœ¬åœ°çŸ¥è¯†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºå‡†æµ‹è¯•ã€‚LocalBenchåŸºäºæœ¬åœ°æ¦‚å¿µæ¡†æ¶ï¼ŒåŒ…å«ç¾å›½49ä¸ªå·çš„526ä¸ªå¿å¸‚çš„14782ä¸ªç»è¿‡éªŒè¯çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œèåˆäº†è¯¸å¦‚äººå£æ™®æŸ¥ç»Ÿè®¡ã€æœ¬åœ°Redditè®ºå›è®¨è®ºå’ŒåŒºåŸŸæ–°é—»ç­‰å„ç§æ¥æºçš„æ•°æ®ã€‚å®ƒæ¶µç›–äº†ç‰©ç†ã€è®¤çŸ¥å’Œå…³ç³»çš„æœ¬åœ°ç»´åº¦ã€‚ä½¿ç”¨LocalBenchï¼Œæˆ‘ä»¬åœ¨å°é—­ä¹¦ç±å’Œç½‘ç»œå¢å¼ºä¸¤ç§ç¯å¢ƒä¸‹è¯„ä¼°äº†13é¡¹æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå­˜åœ¨å…³é”®å±€é™æ€§ï¼šå³ä½¿åœ¨å™äº‹é£æ ¼çš„é—®é¢˜ä¸Šï¼Œè¡¨ç°æœ€ä½³çš„æ¨¡å‹å‡†ç¡®ç‡ä¹Ÿåªæœ‰56.8%ï¼Œåœ¨æ•°å€¼æ¨ç†æ–¹é¢çš„è¡¨ç°æ›´æ˜¯ä½äº15.5%ã€‚æ­¤å¤–ï¼Œæ›´å¤§çš„æ¨¡å‹è§„æ¨¡å’Œç½‘ç»œå¢å¼ºå¹¶ä¸ä¸€å®šèƒ½ä¿è¯æ›´å¥½çš„æ€§èƒ½â€”â€”ä¾‹å¦‚ï¼Œæœç´¢åŠŸèƒ½æé«˜äº†Geminiçš„å‡†ç¡®ç‡+13.6%ï¼Œä½†é™ä½äº†GPTç³»åˆ—çš„æ€§èƒ½-11.4%ã€‚è¿™äº›ç»“æœçªæ˜¾äº†å¯¹èƒ½å¤Ÿæ”¯æŒå…¬å¹³ã€åœ°ç†æ„è¯†çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„è¯­è¨€æ¨¡å‹çš„è¿«åˆ‡éœ€æ±‚ï¼šèƒ½å¤Ÿåœ¨åœ°ç†å’Œæ–‡åŒ–èƒŒæ™¯ä¸‹çš„æœ¬åœ°ç¤¾åŒºçš„å¤šæ ·åŒ–å’Œç²¾ç»†åŒ–çš„ç°å®ä¸­å‘æŒ¥ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10459v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®è§‚åœ°ç†ä»»åŠ¡ä¸Šçš„è¯„ä¼°å·²ç»ç›¸å½“å¹¿æ³›ï¼Œå¦‚å…¨çƒäº‹å®å›å¿†ã€äº‹ä»¶æ‘˜è¦å’ŒåŒºåŸŸæ¨ç†ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¤„ç†è¶…æœ¬åœ°çŸ¥è¯†çš„èƒ½åŠ›å°šæœªè¢«å……åˆ†äº†è§£ã€‚éšç€ä»å…¬æ°‘å¹³å°åˆ°ç¤¾åŒºæ–°é—»ç­‰ç°å®åº”ç”¨éœ€æ±‚ä¸æ–­å¢é•¿ï¼Œè¦æ±‚äººå·¥æ™ºèƒ½ç³»ç»Ÿèƒ½å¤Ÿæ¨ç†é‚»é‡ŒåŠ¨æ€ã€æ–‡åŒ–å™äº‹å’Œå½“åœ°æ²»ç†ç­‰åœ°æ–¹ç‰¹å®šäº‹é¡¹ï¼Œè¿™ä¸€å·®è·æ„ˆå‘é‡è¦ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•åœ¨æ•æ‰è¿™ç§å¤æ‚æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œé€šå¸¸ä¾èµ–äºç²—ç•¥çš„æ•°æ®æˆ–å­¤ç«‹çš„å¼•ç”¨ã€‚æˆ‘ä»¬æ¨å‡ºäº†LocalBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°ç¾å›½å¿çº§æœ¬åœ°çŸ¥è¯†çš„LLMçš„åŸºå‡†æµ‹è¯•ã€‚LocalBenchåŸºäºæœ¬åœ°æ¦‚å¿µæ¡†æ¶ï¼ŒåŒ…å«ç¾å›½49ä¸ªå·çš„526ä¸ªå¿å¸‚çš„14782ä¸ªç»è¿‡éªŒè¯çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œèåˆäº†è¯¸å¦‚äººå£æ™®æŸ¥ç»Ÿè®¡ã€æœ¬åœ°Redditè®ºå›è®¨è®ºå’ŒåŒºåŸŸæ–°é—»ç­‰å¤šå…ƒæ¥æºã€‚å®ƒæ¶µç›–äº†å±€éƒ¨çš„ç‰©ç†ã€è®¤çŸ¥å’Œå…³ç³»ç»´åº¦ã€‚ä½¿ç”¨LocalBenchï¼Œæˆ‘ä»¬å¯¹13æ¬¾æœ€æ–°LLMè¿›è¡Œäº†å°é—­å¼å’Œç½‘é¡µå¢å¼ºæ¨¡å¼ä¸‹çš„è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯è¡¨ç°æœ€ä½³çš„æ¨¡å‹åœ¨å™äº‹é£æ ¼é—®é¢˜ä¸Šçš„å‡†ç¡®ç‡ä¹Ÿåªæœ‰56.8%ï¼Œåœ¨æ•°å€¼æ¨ç†ä¸Šçš„è¡¨ç°ä½äº15.5%ã€‚æ­¤å¤–ï¼Œæ›´å¤§çš„æ¨¡å‹è§„æ¨¡å’Œç½‘é¡µå¢å¼ºå¹¶ä¸ä¸€å®šèƒ½ä¿è¯æ›´å¥½çš„è¡¨ç°ï¼Œä¾‹å¦‚æœç´¢åŠŸèƒ½æé«˜äº†Geminiçš„å‡†ç¡®ç‡+13.6%ï¼Œä½†é™ä½äº†GPTç³»åˆ—çš„æ€§èƒ½-11.4%ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†è¯­è¨€æ¨¡å‹éœ€è¦æ”¯æŒå…¬å¹³ã€åœ°ç†ä½ç½®æ„ŸçŸ¥çš„AIç³»ç»Ÿï¼šèƒ½å¤Ÿä¸è·¨è¶Šåœ°ç†å’Œæ–‡åŒ–èƒŒæ™¯çš„æœ¬åœ°ç¤¾åŒºçš„å¤šæ ·åŒ–å’Œç²¾ç»†ç°å®è¿›è¡Œäº’åŠ¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¶…æœ¬åœ°çŸ¥è¯†æ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†ç†è§£ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨è¯„ä¼°LLMçš„å¿çº§æœ¬åœ°çŸ¥è¯†æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>LocalBenchæ˜¯é¦–ä¸ªæ—¨åœ¨è¯„ä¼°ç¾å›½å¿çº§æœ¬åœ°çŸ¥è¯†çš„LLMçš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†ç‰©ç†ã€è®¤çŸ¥å’Œå…³ç³»ç»´åº¦ã€‚</li>
<li>LLMåœ¨å™äº‹é£æ ¼å’Œæ•°å€¼æ¨ç†æ–¹é¢çš„è¡¨ç°å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚</li>
<li>æ¨¡å‹è§„æ¨¡å’Œç½‘é¡µå¢å¼ºå¹¶ä¸æ€»èƒ½ä¿è¯æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>è¯­è¨€æ¨¡å‹éœ€è¦æ”¯æŒå…¬å¹³ã€åœ°ç†ä½ç½®æ„ŸçŸ¥çš„AIç³»ç»Ÿï¼Œä»¥é€‚åº”åœ°æ–¹ç¤¾åŒºçš„å¤šæ ·åŒ–å’Œç²¾ç»†ç°å®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10459">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33a74e55b8e61d3eb97de1b84f90f1d7" align="middle">
<img src="https://picx.zhimg.com/v2-bcc3f1f4be90be359c62fc25e23b38f5" align="middle">
<img src="https://picx.zhimg.com/v2-f03020005919a185eb94ae200bba48f0" align="middle">
<img src="https://picx.zhimg.com/v2-fe00abe640c5d314a4d1a712f429d6bf" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LongComp-Long-Tail-Compositional-Zero-Shot-Generalization-for-Robust-Trajectory-Prediction"><a href="#LongComp-Long-Tail-Compositional-Zero-Shot-Generalization-for-Robust-Trajectory-Prediction" class="headerlink" title="LongComp: Long-Tail Compositional Zero-Shot Generalization for Robust Trajectory Prediction"></a>LongComp: Long-Tail Compositional Zero-Shot Generalization for Robust Trajectory Prediction</h2><p><strong>Authors:Benjamin Stoler, Jonathan Francis, Jean Oh</strong></p>
<p>Methods for trajectory prediction in Autonomous Driving must contend with rare, safety-critical scenarios that make reliance on real-world data collection alone infeasible. To assess robustness under such conditions, we propose new long-tail evaluation settings that repartition datasets to create challenging out-of-distribution (OOD) test sets. We first introduce a safety-informed scenario factorization framework, which disentangles scenarios into discrete ego and social contexts. Building on analogies to compositional zero-shot image-labeling in Computer Vision, we then hold out novel context combinations to construct challenging closed-world and open-world settings. This process induces OOD performance gaps in future motion prediction of 5.0% and 14.7% in closed-world and open-world settings, respectively, relative to in-distribution performance for a state-of-the-art baseline. To improve generalization, we extend task-modular gating networks to operate within trajectory prediction models, and develop an auxiliary, difficulty-prediction head to refine internal representations. Our strategies jointly reduce the OOD performance gaps to 2.8% and 11.5% in the two settings, respectively, while still improving in-distribution performance.</p>
<blockquote>
<p>åœ¨è‡ªåŠ¨é©¾é©¶çš„è½¨è¿¹é¢„æµ‹æ–¹æ³•ä¸­ï¼Œå¿…é¡»åº”å¯¹ç½•è§ä¸”å¯¹å®‰å…¨è‡³å…³é‡è¦çš„åœºæ™¯ï¼Œè¿™ä½¿å¾—ä»…ä¾èµ–çœŸå®ä¸–ç•Œçš„æ•°æ®æ”¶é›†å˜å¾—ä¸å¯è¡Œã€‚ä¸ºäº†è¯„ä¼°åœ¨è¿™ç§æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°çš„é•¿å°¾è¯„ä¼°è®¾ç½®ï¼Œè¯¥è®¾ç½®é‡æ–°åˆ’åˆ†æ•°æ®é›†ä»¥åˆ›å»ºå…·æœ‰æŒ‘æˆ˜æ€§çš„è¶…å‡ºåˆ†å¸ƒèŒƒå›´çš„ï¼ˆOODï¼‰æµ‹è¯•é›†ã€‚æˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ä¸ªä»¥å®‰å…¨ä¸ºè€ƒè™‘çš„åœºæ™¯åˆ†è§£æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†åœºæ™¯åˆ†è§£ä¸ºç¦»æ•£çš„è‡ªé©¾å’Œç¤¾ä¼šç¯å¢ƒä¸Šä¸‹æ–‡ã€‚åŸºäºè®¡ç®—æœºè§†è§‰ä¸­ç»„åˆé›¶æ ·æœ¬å›¾åƒæ ‡ç­¾çš„ç±»æ¯”ï¼Œæˆ‘ä»¬é€šè¿‡ä¿ç•™æ–°é¢–ä¸Šä¸‹æ–‡ç»„åˆæ¥æ„å»ºå…·æœ‰æŒ‘æˆ˜æ€§çš„å°é—­ä¸–ç•Œå’Œå¼€æ”¾ä¸–ç•Œè®¾ç½®ã€‚è¿™ä¸€è¿‡ç¨‹å¯¼è‡´æœªæ¥è¿åŠ¨é¢„æµ‹çš„OODæ€§èƒ½å·®è·åˆ†åˆ«ä¸ºå°é—­ä¸–ç•Œå’Œå¼€æ”¾ä¸–ç•Œè®¾ç½®ä¸­çš„5.0%å’Œ14.7%ï¼Œç›¸å¯¹äºæœ€æ–°åŸºçº¿åœ¨åˆ†å¸ƒå†…çš„æ€§èƒ½ã€‚ä¸ºäº†æ”¹å–„æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬æ‰©å±•äº†ä»»åŠ¡æ¨¡å—åŒ–é—¨æ§ç½‘ç»œä½¿å…¶åœ¨è½¨è¿¹é¢„æµ‹æ¨¡å‹å†…è¿è¡Œï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªè¾…åŠ©çš„éš¾æ˜“åº¦é¢„æµ‹å¤´æ¥ä¼˜åŒ–å†…éƒ¨è¡¨ç¤ºã€‚æˆ‘ä»¬çš„ç­–ç•¥åœ¨è¿™ä¸¤ç§è®¾ç½®ä¸­åˆ†åˆ«å°†OODæ€§èƒ½å·®è·é™ä½åˆ°2.8%å’Œ11.5%ï¼ŒåŒæ—¶åœ¨åˆ†å¸ƒå†…çš„æ€§èƒ½ä»ç„¶æœ‰æ‰€æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10411v1">PDF</a> 8 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºåœ¨è‡ªåŠ¨é©¾é©¶è½¨è¿¹é¢„æµ‹ä¸­ï¼Œéœ€è¦åº”å¯¹ç½•è§ä¸”å¯¹å®‰å…¨æ€§è‡³å…³é‡è¦çš„åœºæ™¯ï¼Œè¿™äº›åœºæ™¯ä»…ä¾èµ–çœŸå®ä¸–ç•Œçš„æ•°æ®æ”¶é›†æ˜¯ä¸å¯è¡Œçš„ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†æ–°å‹çš„é•¿å°¾è¯„ä¼°è®¾ç½®ï¼Œé€šè¿‡é‡æ–°åˆ’åˆ†æ•°æ®é›†ä»¥åˆ›å»ºå…·æœ‰æŒ‘æˆ˜æ€§çš„æœªçŸ¥åˆ†å¸ƒæµ‹è¯•é›†ã€‚æ–‡ç« ä»‹ç»äº†å®‰å…¨çŸ¥è¯†æƒ…å¢ƒåˆ†è§£æ¡†æ¶ï¼Œå°†æƒ…å¢ƒåˆ†è§£ä¸ºç¦»æ•£çš„è‡ªé©¾ä¸ç¤¾ä¼šæƒ…å¢ƒã€‚å€ŸåŠ©è®¡ç®—æœºè§†è§‰ä¸­çš„ç»„åˆé›¶æ ·æœ¬å›¾åƒæ ‡ç­¾çš„ç±»æ¯”ï¼Œä½œè€…é€šè¿‡ä¿æŒæ–°é¢–æƒ…å¢ƒç»„åˆæ¥æ„å»ºå°é—­ä¸–ç•Œå’Œå¼€æ”¾ä¸–ç•Œè®¾ç½®ã€‚è¿™ç§æ–¹æ³•å¯¼è‡´æœªæ¥è¿åŠ¨é¢„æµ‹åœ¨å°é—­ä¸–ç•Œå’Œå¼€æ”¾ä¸–ç•Œè®¾ç½®ä¸­çš„æ€§èƒ½å·®è·åˆ†åˆ«ä¸º5.0%å’Œç›¸å¯¹äºå…ˆè¿›åŸºçº¿æ–¹æ³•çš„åŸºçº¿æ€§èƒ½çš„14.7%ã€‚ä¸ºäº†æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œä½œè€…æ‰©å±•äº†ä»»åŠ¡æ¨¡å—åŒ–é—¨æ§ç½‘ç»œåœ¨è½¨è¿¹é¢„æµ‹æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªè¾…åŠ©éš¾åº¦é¢„æµ‹å¤´æ¥ä¼˜åŒ–å†…éƒ¨è¡¨ç¤ºã€‚è¿™äº›ç­–ç•¥åˆ†åˆ«å°†ä¸¤ä¸ªè®¾ç½®ä¸­çš„æ€§èƒ½å·®è·ç¼©å°åˆ°2.8%å’Œ11.5%ï¼ŒåŒæ—¶æé«˜äº†åˆ†å¸ƒå†…çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶è½¨è¿¹é¢„æµ‹éœ€åº”å¯¹ç½•è§ä¸”å¯¹å®‰å…¨æ€§è‡³å…³é‡è¦çš„åœºæ™¯ï¼Œä¸èƒ½ä»…ä¾èµ–çœŸå®ä¸–ç•Œæ•°æ®æ”¶é›†ã€‚</li>
<li>æå‡ºæ–°å‹é•¿å°¾è¯„ä¼°è®¾ç½®ï¼Œé€šè¿‡é‡æ–°åˆ’åˆ†æ•°æ®é›†ä»¥åˆ›å»ºæŒ‘æˆ˜æ€§æœªçŸ¥åˆ†å¸ƒæµ‹è¯•é›†ã€‚</li>
<li>ä»‹ç»å®‰å…¨çŸ¥è¯†æƒ…å¢ƒåˆ†è§£æ¡†æ¶ï¼Œå°†æƒ…å¢ƒåˆ†è§£ä¸ºç¦»æ•£çš„è‡ªé©¾ä¸ç¤¾ä¼šæƒ…å¢ƒã€‚</li>
<li>åˆ©ç”¨è®¡ç®—æœºè§†è§‰ä¸­çš„ç»„åˆé›¶æ ·æœ¬å›¾åƒæ ‡ç­¾ç±»æ¯”ï¼Œæ„å»ºå°é—­ä¸–ç•Œå’Œå¼€æ”¾ä¸–ç•Œè®¾ç½®ã€‚</li>
<li>æœªæ¥è¿åŠ¨é¢„æµ‹åœ¨å°é—­ä¸–ç•Œå’Œå¼€æ”¾ä¸–ç•Œè®¾ç½®ä¸­å­˜åœ¨æ€§èƒ½å·®è·ã€‚</li>
<li>æ‰©å±•ä»»åŠ¡æ¨¡å—åŒ–é—¨æ§ç½‘ç»œåœ¨è½¨è¿¹é¢„æµ‹æ¨¡å‹ä¸­çš„åº”ç”¨ä»¥æé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10411">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0db217c82e8b99b4df5b00c45d6fef20" align="middle">
<img src="https://picx.zhimg.com/v2-e2c8185e38e47c0f54e0d4b6723dd7fb" align="middle">
<img src="https://picx.zhimg.com/v2-191a48359070835a31def2f8cec63c2a" align="middle">
<img src="https://picx.zhimg.com/v2-fcac5272711a2503ec9e13bafe43b3f3" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GrounDiff-Diffusion-Based-Ground-Surface-Generation-from-Digital-Surface-Models"><a href="#GrounDiff-Diffusion-Based-Ground-Surface-Generation-from-Digital-Surface-Models" class="headerlink" title="GrounDiff: Diffusion-Based Ground Surface Generation from Digital Surface Models"></a>GrounDiff: Diffusion-Based Ground Surface Generation from Digital Surface Models</h2><p><strong>Authors:Oussema Dhaouadi, Johannes Meier, Jacques Kaiser, Daniel Cremers</strong></p>
<p>Digital Terrain Models (DTMs) represent the bare-earth elevation and are important in numerous geospatial applications. Such data models cannot be directly measured by sensors and are typically generated from Digital Surface Models (DSMs) derived from LiDAR or photogrammetry. Traditional filtering approaches rely on manually tuned parameters, while learning-based methods require well-designed architectures, often combined with post-processing. To address these challenges, we introduce Ground Diffusion (GrounDiff), the first diffusion-based framework that iteratively removes non-ground structures by formulating the problem as a denoising task. We incorporate a gated design with confidence-guided generation that enables selective filtering. To increase scalability, we further propose Prior-Guided Stitching (PrioStitch), which employs a downsampled global prior automatically generated using GrounDiff to guide local high-resolution predictions. We evaluate our method on the DSM-to-DTM translation task across diverse datasets, showing that GrounDiff consistently outperforms deep learning-based state-of-the-art methods, reducing RMSE by up to 93% on ALS2DTM and up to 47% on USGS benchmarks. In the task of road reconstruction, which requires both high precision and smoothness, our method achieves up to 81% lower distance error compared to specialized techniques on the GeRoD benchmark, while maintaining competitive surface smoothness using only DSM inputs, without task-specific optimization. Our variant for road reconstruction, GrounDiff+, is specifically designed to produce even smoother surfaces, further surpassing state-of-the-art methods. The project page is available at <a target="_blank" rel="noopener" href="https://deepscenario.github.io/GrounDiff/">https://deepscenario.github.io/GrounDiff/</a>.</p>
<blockquote>
<p>æ•°å­—åœ°å½¢æ¨¡å‹ï¼ˆDTMï¼‰ä»£è¡¨åœ°é¢é«˜ç¨‹ï¼Œåœ¨ä¼—å¤šçš„åœ°ç†ç©ºé—´åº”ç”¨ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚æ­¤ç±»æ•°æ®æ¨¡å‹æ— æ³•ç”±ä¼ æ„Ÿå™¨ç›´æ¥æµ‹é‡ï¼Œé€šå¸¸æ˜¯ç”±æ¿€å…‰é›·è¾¾æˆ–æ‘„å½±æµ‹é‡çš„æ•°å­—è¡¨é¢æ¨¡å‹ï¼ˆDSMï¼‰æ´¾ç”Ÿè€Œæ¥ã€‚ä¼ ç»Ÿçš„è¿‡æ»¤æ–¹æ³•ä¾èµ–äºæ‰‹åŠ¨è°ƒæ•´çš„å‚æ•°ï¼Œè€ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•åˆ™éœ€è¦ç²¾å¿ƒè®¾è®¡æ¶æ„ï¼Œé€šå¸¸ç»“åˆåæœŸå¤„ç†ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Ground Diffusionï¼ˆGrounDiffï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œé€šè¿‡å°†å…¶åˆ¶å®šä¸ºå»å™ªä»»åŠ¡æ¥è¿­ä»£åœ°å»é™¤éåœ°é¢ç»“æ„ã€‚æˆ‘ä»¬é‡‡ç”¨å¸¦æœ‰ç½®ä¿¡å¼•å¯¼ç”Ÿæˆçš„é—¨æ§è®¾è®¡ï¼Œä»¥å®ç°é€‰æ‹©æ€§è¿‡æ»¤ã€‚ä¸ºäº†æé«˜å¯æ‰©å±•æ€§ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†Prior-Guided Stitchingï¼ˆPrioStitchï¼‰ï¼Œå®ƒé‡‡ç”¨ä½¿ç”¨GrounDiffè‡ªåŠ¨ç”Ÿæˆçš„é™é‡‡æ ·å…¨å±€å…ˆéªŒæ¥æŒ‡å¯¼å±€éƒ¨é«˜åˆ†è¾¨ç‡é¢„æµ‹ã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬åœ¨DSMåˆ°DTMè½¬æ¢ä»»åŠ¡ä¸Šçš„æ–¹æ³•ï¼Œç»“æœè¡¨æ˜GrounDiffå§‹ç»ˆä¼˜äºåŸºäºæ·±åº¦å­¦ä¹ çš„å‰æ²¿æ–¹æ³•ï¼Œåœ¨ALS2DTMä¸Šå‡å°‘äº†é«˜è¾¾93%çš„RMSEï¼Œåœ¨USGSåŸºå‡†æµ‹è¯•ä¸­å‡å°‘äº†é«˜è¾¾47%çš„RMSEã€‚åœ¨é“è·¯é‡å»ºä»»åŠ¡ä¸­ï¼Œæ—¢éœ€è¦é«˜ç²¾åº¦ä¹Ÿéœ€è¦å¹³æ»‘åº¦ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸GeRoDåŸºå‡†æµ‹è¯•ä¸Šçš„ä¸“ç”¨æŠ€æœ¯ç›¸æ¯”ï¼Œè·ç¦»è¯¯å·®é™ä½äº†é«˜è¾¾81%ï¼ŒåŒæ—¶ä½¿ç”¨ä»…ä½¿ç”¨DSMè¾“å…¥çš„ç«å¹³å°é¢å¹³æ»‘åº¦ï¼Œæ— éœ€é’ˆå¯¹ä»»åŠ¡è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬ä¸ºé“è·¯é‡å»ºè®¾è®¡çš„å˜ä½“GrounDiff+èƒ½å¤Ÿäº§ç”Ÿæ›´å¹³æ»‘çš„è¡¨é¢ï¼Œè¿›ä¸€æ­¥è¶…è¶Šäº†å‰æ²¿æ–¹æ³•ã€‚é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://deepscenario.github.io/GrounDiff/%E6%8E%A2%E8%AE%BF%E3%80%82">https://deepscenario.github.io/GrounDiff/è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10391v1">PDF</a> Accepted at WACV 2026</p>
<p><strong>Summary</strong></p>
<p>æ•°å­—åœ°å½¢æ¨¡å‹ï¼ˆDTMï¼‰ä»£è¡¨åœ°é¢é«˜ç¨‹ï¼Œåœ¨è¯¸å¤šåœ°ç†ç©ºé—´åº”ç”¨ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚æœ¬æ–‡ä»‹ç»äº†åŸºäºæ‰©æ•£çš„æ–¹æ³•Ground Diffusionï¼ˆGrounDiffï¼‰ï¼Œè¯¥æ–¹æ³•å°†DSMåˆ°DTMçš„ç¿»è¯‘é—®é¢˜è¡¨è¿°ä¸ºå»å™ªä»»åŠ¡ï¼Œé€šè¿‡è¿­ä»£å»é™¤éåœ°é¢ç»“æ„ã€‚ä¸ºæé«˜å¯æ‰©å±•æ€§ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†Prior-Guided Stitchingï¼ˆPrioStitchï¼‰ï¼Œåˆ©ç”¨ä¸‹é‡‡æ ·å…¨å±€å…ˆéªŒè‡ªåŠ¨ç”Ÿæˆçš„GrounDiffæ¥æŒ‡å¯¼å±€éƒ¨é«˜åˆ†è¾¨ç‡é¢„æµ‹ã€‚åœ¨DSMåˆ°DTMçš„ç¿»è¯‘ä»»åŠ¡å’Œé“è·¯é‡å»ºä»»åŠ¡ä¸­ï¼ŒGrounDiffå‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œé™ä½äº†RMSEè¯¯å·®ï¼Œå¹¶åœ¨GeRoDåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†è¾ƒä½çš„è·ç¦»è¯¯å·®ã€‚ç›¸å…³é¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://deepscenario.github.io/GrounDiff/">é“¾æ¥</a>æŸ¥çœ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ground Diffusionï¼ˆGrounDiffï¼‰æ˜¯é¦–ä¸ªåŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œç”¨äºä»æ•°å­—è¡¨é¢æ¨¡å‹ï¼ˆDSMï¼‰ç”Ÿæˆæ•°å­—åœ°å½¢æ¨¡å‹ï¼ˆDTMï¼‰ã€‚</li>
<li>GrounDiffé€šè¿‡å°†é—®é¢˜è¡¨è¿°ä¸ºå»å™ªä»»åŠ¡ï¼Œèƒ½å¤Ÿè¿­ä»£å»é™¤éåœ°é¢ç»“æ„ã€‚</li>
<li>ä¸ºæé«˜å¯æ‰©å±•æ€§ï¼Œæå‡ºäº†Prior-Guided Stitchingï¼ˆPrioStitchï¼‰ï¼Œåˆ©ç”¨å…¨å±€å…ˆéªŒæŒ‡å¯¼å±€éƒ¨é¢„æµ‹ã€‚</li>
<li>GrounDiffåœ¨å¤šç§æ•°æ®é›†ä¸Šçš„DSMåˆ°DTMç¿»è¯‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œé™ä½äº†RMSEè¯¯å·®ã€‚</li>
<li>åœ¨éœ€è¦é«˜ç²¾åº¦å’Œå…‰æ»‘åº¦çš„é“è·¯é‡å»ºä»»åŠ¡ä¸­ï¼ŒGrounDiffå®ç°äº†è¾ƒä½çš„è·ç¦»è¯¯å·®ï¼Œå¹¶ä¿æŒäº†è¡¨é¢å…‰æ»‘åº¦ã€‚</li>
<li>GrounDiff+æ˜¯ä¸“ä¸ºé“è·¯é‡å»ºè®¾è®¡çš„å˜ä½“ï¼Œèƒ½å¤Ÿäº§ç”Ÿæ›´å…‰æ»‘çš„è¡¨é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10391">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-88a99efeafcc792761f5aeba24cc3598" align="middle">
<img src="https://picx.zhimg.com/v2-c272cfeba66ff7ae4074950083446d48" align="middle">
<img src="https://picx.zhimg.com/v2-30eacd9c2666333f5012614301a9e5ec" align="middle">
<img src="https://picx.zhimg.com/v2-32e849d430336c8f242075588a4cfe73" align="middle">
<img src="https://picx.zhimg.com/v2-5def1bf64632b11e17b7ffc12349b64c" align="middle">
<img src="https://picx.zhimg.com/v2-0f26f75b28e092100e9532a4a8157a52" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MonkeyOCR-v1-5-Technical-Report-Unlocking-Robust-Document-Parsing-for-Complex-Patterns"><a href="#MonkeyOCR-v1-5-Technical-Report-Unlocking-Robust-Document-Parsing-for-Complex-Patterns" class="headerlink" title="MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns"></a>MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns</h2><p><strong>Authors:Jiarui Zhang, Yuliang Liu, Zijun Wu, Guosheng Pang, Zhili Ye, Yupei Zhong, Junteng Ma, Tao Wei, Haiyang Xu, Weikai Chen, Zeen Wang, Qiangjun Ji, Fanxi Zhou, Qi Zhang, Yuanrui Hu, Jiahao Liu, Zhang Li, Ziyang Zhang, Qiang Liu, Xiang Bai</strong></p>
<p>Document parsing is a core task in document intelligence, supporting applications such as information extraction, retrieval-augmented generation, and automated document analysis. However, real-world documents often feature complex layouts with multi-level tables, embedded images or formulas, and cross-page structures, which remain challenging for existing OCR systems. We introduce MonkeyOCR v1.5, a unified vision-language framework that enhances both layout understanding and content recognition through a two-stage parsing pipeline. The first stage employs a large multimodal model to jointly predict document layout and reading order, leveraging visual information to ensure structural and sequential consistency. The second stage performs localized recognition of text, formulas, and tables within detected regions, maintaining high visual fidelity while reducing error propagation. To address complex table structures, we propose a visual consistency-based reinforcement learning scheme that evaluates recognition quality via render-and-compare alignment, improving structural accuracy without manual annotations. Additionally, two specialized modules, Image-Decoupled Table Parsing and Type-Guided Table Merging, are introduced to enable reliable parsing of tables containing embedded images and reconstruction of tables crossing pages or columns. Comprehensive experiments on OmniDocBench v1.5 demonstrate that MonkeyOCR v1.5 achieves state-of-the-art performance, outperforming PPOCR-VL and MinerU 2.5 while showing exceptional robustness in visually complex document scenarios.</p>
<blockquote>
<p>æ–‡æ¡£è§£ææ˜¯æ–‡æ¡£æ™ºèƒ½çš„æ ¸å¿ƒä»»åŠ¡ï¼Œæ”¯æŒä¿¡æ¯æå–ã€æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œè‡ªåŠ¨åŒ–æ–‡æ¡£åˆ†æç­‰åº”ç”¨ç¨‹åºã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„æ–‡æ¡£é€šå¸¸å…·æœ‰å¤æ‚çš„å¸ƒå±€ï¼ŒåŒ…å«å¤šçº§è¡¨æ ¼ã€åµŒå…¥çš„å›¾åƒæˆ–å…¬å¼ä»¥åŠè·¨é¡µç»“æ„ï¼Œè¿™å¯¹ç°æœ‰OCRç³»ç»Ÿä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æ¨å‡ºäº†MonkeyOCR v1.5ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰è¯­è¨€æ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè§£æç®¡é“å¢å¼ºå¸ƒå±€ç†è§£å’Œå†…å®¹è¯†åˆ«ã€‚ç¬¬ä¸€é˜¶æ®µé‡‡ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è”åˆé¢„æµ‹æ–‡æ¡£å¸ƒå±€å’Œé˜…è¯»é¡ºåºï¼Œåˆ©ç”¨è§†è§‰ä¿¡æ¯ç¡®ä¿ç»“æ„å’Œé¡ºåºä¸€è‡´æ€§ã€‚ç¬¬äºŒé˜¶æ®µåœ¨æ£€æµ‹åˆ°çš„åŒºåŸŸå†…æ‰§è¡Œæ–‡æœ¬ã€å…¬å¼å’Œè¡¨æ ¼çš„å±€éƒ¨è¯†åˆ«ï¼Œä¿æŒé«˜è§†è§‰ä¿çœŸåº¦çš„åŒæ—¶å‡å°‘é”™è¯¯ä¼ æ’­ã€‚ä¸ºäº†è§£å†³å¤æ‚çš„è¡¨æ ¼ç»“æ„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰ä¸€è‡´æ€§å¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆï¼Œé€šè¿‡å‘ˆç°å’Œæ¯”è¾ƒå¯¹é½æ¥è¯„ä¼°è¯†åˆ«è´¨é‡ï¼Œæé«˜ç»“æ„å‡†ç¡®æ€§è€Œæ— éœ€æ‰‹åŠ¨æ³¨é‡Šã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸¤ä¸ªä¸“ä¸šæ¨¡å—ï¼Œå³å›¾åƒè§£è€¦è¡¨æ ¼è§£æå’Œç±»å‹æŒ‡å¯¼è¡¨æ ¼åˆå¹¶ï¼Œä»¥å®ç°å¯¹åŒ…å«åµŒå…¥å›¾åƒçš„è¡¨æ ¼çš„å¯é è§£æä»¥åŠå¯¹è·¨é¡µæˆ–åˆ—çš„è¡¨æ ¼çš„é‡å»ºã€‚åœ¨OmniDocBench v1.5ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒMonkeyOCR v1.5è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œä¼˜äºPPOCR-VLå’ŒMinerU 2.5ï¼Œåœ¨è§†è§‰å¤æ‚çš„æ–‡æ¡£åœºæ™¯ä¸­è¡¨ç°å‡ºæƒŠäººçš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10390v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ–‡æ¡£è§£æåœ¨æ–‡æ¡£æ™ºèƒ½ä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œä»¥åŠé’ˆå¯¹ç°å®ä¸–ç•Œä¸­å¤æ‚æ–‡æ¡£å¸ƒå±€çš„OCRç³»ç»Ÿé¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« å¼•å…¥äº†MonkeyOCR v1.5ï¼Œä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰è¯­è¨€æ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè§£æç®¡é“å¢å¼ºå¸ƒå±€ç†è§£å’Œå†…å®¹è¯†åˆ«ã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è”åˆé¢„æµ‹æ–‡æ¡£å¸ƒå±€å’Œé˜…è¯»é¡ºåºï¼Œåˆ©ç”¨è§†è§‰ä¿¡æ¯ç¡®ä¿ç»“æ„å’Œé¡ºåºä¸€è‡´æ€§ã€‚ç¬¬äºŒé˜¶æ®µå¯¹æ£€æµ‹åˆ°çš„åŒºåŸŸè¿›è¡Œå±€éƒ¨æ–‡æœ¬ã€å…¬å¼å’Œè¡¨æ ¼è¯†åˆ«ï¼Œä¿æŒé«˜è§†è§‰ä¿çœŸåº¦ï¼Œå‡å°‘é”™è¯¯ä¼ æ’­ã€‚é’ˆå¯¹å¤æ‚è¡¨æ ¼ç»“æ„ï¼Œæ–‡ç« æå‡ºäº†åŸºäºè§†è§‰ä¸€è‡´æ€§çš„å¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆï¼Œé€šè¿‡æ¸²æŸ“å’Œæ¯”è¾ƒå¯¹é½è¯„ä¼°è¯†åˆ«è´¨é‡ï¼Œæé«˜ç»“æ„å‡†ç¡®æ€§ï¼Œæ— éœ€æ‰‹åŠ¨æ³¨é‡Šã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†ä¸¤ä¸ªä¸“é—¨æ¨¡å—â€”â€”å›¾åƒè§£è€¦è¡¨æ ¼è§£æå’Œç±»å‹å¼•å¯¼è¡¨æ ¼åˆå¹¶ï¼Œä»¥å®ç°å¯¹åŒ…å«åµŒå…¥å¼å›¾åƒçš„è¡¨æ ¼çš„å¯é è§£æå’Œè·¨é¡µæˆ–è·¨åˆ—çš„è¡¨æ ¼çš„é‡å»ºã€‚åœ¨OmniDocBench v1.5ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒMonkeyOCR v1.5å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¼˜äºPPOCR-VLå’ŒMinerU 2.5ï¼Œåœ¨è§†è§‰å¤æ‚æ–‡æ¡£åœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šçš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æ¡£è§£ææ˜¯æ–‡æ¡£æ™ºèƒ½ä¸­çš„æ ¸å¿ƒä»»åŠ¡ï¼Œåº”ç”¨äºä¿¡æ¯æå–ã€æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œè‡ªåŠ¨åŒ–æ–‡æ¡£åˆ†æç­‰ã€‚</li>
<li>ç°å®ä¸–ç•Œçš„æ–‡æ¡£å¸ƒå±€å¤æ‚ï¼ŒåŒ…å«å¤šå±‚çº§è¡¨æ ¼ã€åµŒå…¥å¼å›¾åƒå’Œå…¬å¼ã€è·¨é¡µç»“æ„ç­‰ï¼Œå¯¹ç°æœ‰çš„OCRç³»ç»Ÿæ„æˆæŒ‘æˆ˜ã€‚</li>
<li>MonkeyOCR v1.5æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰è¯­è¨€æ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè§£æç®¡é“æé«˜å¸ƒå±€ç†è§£å’Œå†…å®¹è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨å¤šæ¨¡æ€æ¨¡å‹é¢„æµ‹æ–‡æ¡£å¸ƒå±€å’Œé˜…è¯»é¡ºåºï¼Œç¡®ä¿ç»“æ„å’Œé¡ºåºçš„ä¸€è‡´æ€§ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µè¿›è¡Œå±€éƒ¨æ–‡æœ¬ã€å…¬å¼å’Œè¡¨æ ¼è¯†åˆ«ï¼Œä¿æŒé«˜è§†è§‰ä¿çœŸåº¦ï¼Œå¹¶å‡å°‘é”™è¯¯ä¼ æ’­ã€‚</li>
<li>é’ˆå¯¹å¤æ‚è¡¨æ ¼ç»“æ„ï¼Œæ–‡ç« æå‡ºäº†åŸºäºè§†è§‰ä¸€è‡´æ€§çš„å¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆï¼Œé€šè¿‡æ¸²æŸ“å’Œæ¯”è¾ƒå¯¹é½æé«˜ç»“æ„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-513d8904afc0ca454a756a48b39f2438" align="middle">
<img src="https://picx.zhimg.com/v2-4579dd5e8c5b17f5883fe2b6ea3179af" align="middle">
<img src="https://picx.zhimg.com/v2-4a3f73ceef74737acbcfb9452eb808ec" align="middle">
<img src="https://picx.zhimg.com/v2-813cfa5d10b37774c0834ef05565ada1" align="middle">
<img src="https://picx.zhimg.com/v2-d4447368ba54286b1718e65d18bcae04" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Rectify-Evaluation-Preference-Improving-LLMsâ€™-Critique-on-Math-Reasoning-via-Perplexity-aware-Reinforcement-Learning"><a href="#Rectify-Evaluation-Preference-Improving-LLMsâ€™-Critique-on-Math-Reasoning-via-Perplexity-aware-Reinforcement-Learning" class="headerlink" title="Rectify Evaluation Preference: Improving LLMsâ€™ Critique on Math Reasoning via Perplexity-aware Reinforcement Learning"></a>Rectify Evaluation Preference: Improving LLMsâ€™ Critique on Math Reasoning via Perplexity-aware Reinforcement Learning</h2><p><strong>Authors:Changyuan Tian, Zhicong Lu, Shuang Qian, Nayu Liu, Peiguang Li, Li Jin, Leiyi Hu, Zhizhao Zeng, Sirui Wang, Ke Zeng, Zhi Guo</strong></p>
<p>To improve Multi-step Mathematical Reasoning (MsMR) of Large Language Models (LLMs), it is crucial to obtain scalable supervision from the corpus by automatically critiquing mistakes in the reasoning process of MsMR and rendering a final verdict of the problem-solution. Most existing methods rely on crafting high-quality supervised fine-tuning demonstrations for critiquing capability enhancement and pay little attention to delving into the underlying reason for the poor critiquing performance of LLMs. In this paper, we orthogonally quantify and investigate the potential reason â€“ imbalanced evaluation preference, and conduct a statistical preference analysis. Motivated by the analysis of the reason, a novel perplexity-aware reinforcement learning algorithm is proposed to rectify the evaluation preference, elevating the critiquing capability. Specifically, to probe into LLMsâ€™ critiquing characteristics, a One-to-many Problem-Solution (OPS) benchmark is meticulously constructed to quantify the behavior difference of LLMs when evaluating the problem solutions generated by itself and others. Then, to investigate the behavior difference in depth, we conduct a statistical preference analysis oriented on perplexity and find an intriguing phenomenon â€“ &#96;&#96;LLMs incline to judge solutions with lower perplexity as correctâ€™â€™, which is dubbed as \textit{imbalanced evaluation preference}. To rectify this preference, we regard perplexity as the baton in the algorithm of Group Relative Policy Optimization, supporting the LLMs to explore trajectories that judge lower perplexity as wrong and higher perplexity as correct. Extensive experimental results on our built OPS and existing available critic benchmarks demonstrate the validity of our method.</p>
<blockquote>
<p>ä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ­¥æ•°å­¦æ¨ç†ï¼ˆMsMRï¼‰èƒ½åŠ›ï¼Œä»è¯­æ–™åº“ä¸­è·å–å¯è§„æ¨¡åŒ–ç›‘ç£è‡³å…³é‡è¦ï¼Œè¿™éœ€è¦è‡ªåŠ¨æ‰¹åˆ¤MsMRæ¨ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œå¹¶å¯¹é—®é¢˜è§£å†³æ–¹æ¡ˆç»™å‡ºæœ€ç»ˆåˆ¤æ–­ã€‚ç°æœ‰å¤§å¤šæ•°æ–¹æ³•éƒ½ä¾èµ–äºåˆ¶ä½œé«˜è´¨é‡çš„ç›‘ç£å¾®è°ƒæ¼”ç¤ºæ¥å¢å¼ºæ‰¹åˆ¤èƒ½åŠ›ï¼Œå´å¿½è§†äº†æ·±å…¥æŒ–æ˜LLMæ‰¹åˆ¤æ€§èƒ½ä¸ä½³çš„æ½œåœ¨åŸå› ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ­£äº¤é‡åŒ–å’Œç ”ç©¶äº†æ½œåœ¨åŸå› â€”â€”è¯„ä¼°åå¥½ä¸å‡è¡¡ï¼Œå¹¶è¿›è¡Œäº†ç»Ÿè®¡åå¥½åˆ†æã€‚å—åˆ†æç»“æœå¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„å›°æƒ‘åº¦æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»¥çº æ­£è¯„ä¼°åå¥½ï¼Œæå‡æ‰¹åˆ¤èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†æ¢ç©¶LLMçš„æ‰¹åˆ¤ç‰¹æ€§ï¼Œæˆ‘ä»¬ç²¾å¿ƒæ„å»ºäº†ä¸€ä¸ªä¸€å¯¹ä¸€è‡³å¤šé—®é¢˜è§£å†³æ–¹æ¡ˆï¼ˆOPSï¼‰åŸºå‡†æµ‹è¯•ï¼Œä»¥é‡åŒ–LLMåœ¨è¯„ä¼°è‡ªèº«ç”Ÿæˆçš„é—®é¢˜è§£å†³æ–¹æ¡ˆå’Œä»–äººè§£å†³æ–¹æ¡ˆæ—¶çš„è¡Œä¸ºå·®å¼‚ã€‚ç„¶åï¼Œä¸ºäº†æ·±å…¥ç ”ç©¶è¡Œä¸ºå·®å¼‚ï¼Œæˆ‘ä»¬ä»å›°æƒ‘åº¦å‡ºå‘è¿›è¡Œäº†ç»Ÿè®¡åå¥½åˆ†æï¼Œå‘ç°äº†ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡â€”â€”â€œLLMå€¾å‘äºåˆ¤æ–­ä½å›°æƒ‘åº¦çš„è§£å†³æ–¹æ¡ˆä¸ºæ­£ç¡®â€ï¼Œè¿™è¢«ç§°ä¸ºâ€œè¯„ä¼°åå¥½ä¸å‡è¡¡â€ã€‚ä¸ºäº†çº æ­£è¿™ä¸€åå¥½ï¼Œæˆ‘ä»¬å°†å›°æƒ‘åº¦è§†ä¸ºç®—æ³•ä¸­çš„æŒ‡æŒ¥æ£’ï¼Œåœ¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ä¸­æ”¯æŒLLMæ¢ç´¢åˆ¤æ–­ä½å›°æƒ‘åº¦é”™è¯¯ã€é«˜å›°æƒ‘åº¦æ­£ç¡®çš„è½¨è¿¹ã€‚åœ¨æˆ‘ä»¬æ„å»ºçš„OPSå’Œç°æœ‰çš„æ‰¹åˆ¤åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10303v1">PDF</a> Accepted by AAAI2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ­¥æ•°å­¦æ¨ç†ä¸­çš„è¯„ä¼°åå¥½é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å›°æƒ‘åº¦æ„ŸçŸ¥çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•æ¥çº æ­£ä¸å¹³è¡¡çš„è¯„ä»·åå¥½ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ‰¹åˆ¤èƒ½åŠ›ã€‚é€šè¿‡æ„å»ºä¸€å¯¹ä¸€å¤šé—®é¢˜è§£å†³æ–¹æ¡ˆåŸºå‡†æµ‹è¯•ï¼Œæ·±å…¥ç ”ç©¶è¯­è¨€æ¨¡å‹çš„æ‰¹åˆ¤ç‰¹æ€§ï¼Œå¹¶å‘ç°è¯­è¨€æ¨¡å‹å€¾å‘äºå°†è¾ƒä½å›°æƒ‘åº¦çš„è§£å†³æ–¹æ¡ˆåˆ¤æ–­ä¸ºæ­£ç¡®ï¼Œå³å­˜åœ¨è¯„ä»·åå¥½å¤±è¡¡çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œé‡‡ç”¨é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œä»¥å›°æƒ‘åº¦ä¸ºæŒ‡å¼•ï¼Œä¿ƒä½¿è¯­è¨€æ¨¡å‹æ¢ç´¢ä¸åŒçš„åˆ¤æ–­è½¨è¿¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ­¥æ•°å­¦æ¨ç†çš„è¯„ä¼°ä¸­å­˜åœ¨è¯„ä»·åå¥½å¤±è¡¡çš„é—®é¢˜ï¼Œå³å€¾å‘äºåˆ¤æ–­ä½å›°æƒ‘åº¦è§£å†³æ–¹æ¡ˆä¸ºæ­£ç¡®ã€‚</li>
<li>ä¸ºäº†ç ”ç©¶è¿™ä¸€é—®é¢˜ï¼Œæ„å»ºäº†ä¸€å¯¹ä¸€ä¸ªå¤šé—®é¢˜è§£å†³æ–¹æ¡ˆåŸºå‡†æµ‹è¯•ï¼Œç”¨äºé‡åŒ–è¯­è¨€æ¨¡å‹åœ¨è¯„ä»·è‡ªèº«å’Œä»–äººè§£å†³æ–¹æ¡ˆæ—¶çš„è¡Œä¸ºå·®å¼‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å›°æƒ‘åº¦æ„ŸçŸ¥çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»¥çº æ­£è¯­è¨€æ¨¡å‹çš„è¯„ä»·åå¥½å¤±è¡¡é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œä»¥å›°æƒ‘åº¦ä½œä¸ºæŒ‡å¼•ï¼Œä¿ƒä½¿è¯­è¨€æ¨¡å‹æ¢ç´¢ä¸åŒçš„åˆ¤æ–­è½¨è¿¹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‡ªå»ºçš„OPSå’Œç°æœ‰æ‰¹åˆ¤åŸºå‡†æµ‹è¯•ä¸Šå‡æœ‰æ•ˆã€‚</li>
<li>æ­¤ç ”ç©¶å¯¹äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ‰¹åˆ¤èƒ½åŠ›å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10303">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-867279bb3498984242a365401f02abec" align="middle">
<img src="https://picx.zhimg.com/v2-a8068e8bc18daa7be9af70590b96880e" align="middle">
<img src="https://picx.zhimg.com/v2-2db0d1e41011fb77db5fc685a42a21a9" align="middle">
<img src="https://picx.zhimg.com/v2-d41b02a3d01551d893090620b130ae99" align="middle">
<img src="https://picx.zhimg.com/v2-705a7f27ce0c6ad7c501c30ee02e4c63" align="middle">
<img src="https://picx.zhimg.com/v2-a670d049f6e7c1ca312bef363d811c92" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Music-Flamingo-Scaling-Music-Understanding-in-Audio-Language-Models"><a href="#Music-Flamingo-Scaling-Music-Understanding-in-Audio-Language-Models" class="headerlink" title="Music Flamingo: Scaling Music Understanding in Audio Language Models"></a>Music Flamingo: Scaling Music Understanding in Audio Language Models</h2><p><strong>Authors:Sreyan Ghosh, Arushi Goel, Lasha Koroshinadze, Sang-gil Lee, Zhifeng Kong, Joao Felipe Santos, Ramani Duraiswami, Dinesh Manocha, Wei Ping, Mohammad Shoeybi, Bryan Catanzaro</strong></p>
<p>We introduce Music Flamingo, a novel large audio-language model designed to advance music (including song) understanding in foundational audio models. While audio-language research has progressed rapidly, music remains challenging due to its dynamic, layered, and information-dense nature. Progress has been further limited by the difficulty of scaling open audio understanding models, primarily because of the scarcity of high-quality music data and annotations. As a result, prior models are restricted to producing short, high-level captions, answering only surface-level questions, and showing limited generalization across diverse musical cultures. To address these challenges, we curate MF-Skills, a large-scale dataset labeled through a multi-stage pipeline that yields rich captions and question-answer pairs covering harmony, structure, timbre, lyrics, and cultural context. We fine-tune an enhanced Audio Flamingo 3 backbone on MF-Skills and further strengthen multiple skills relevant to music understanding. To improve the modelâ€™s reasoning abilities, we introduce a post-training recipe: we first cold-start with MF-Think, a novel chain-of-thought dataset grounded in music theory, followed by GRPO-based reinforcement learning with custom rewards. Music Flamingo achieves state-of-the-art results across 10+ benchmarks for music understanding and reasoning, establishing itself as a generalist and musically intelligent audio-language model. Beyond strong empirical results, Music Flamingo sets a new standard for advanced music understanding by demonstrating how models can move from surface-level recognition toward layered, human-like perception of songs. We believe this work provides both a benchmark and a foundation for the community to build the next generation of models that engage with music as meaningfully as humans do.</p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†éŸ³ä¹ç«çƒˆé¸Ÿï¼ˆMusic Flamingoï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æ¨åŠ¨åŸºç¡€éŸ³é¢‘æ¨¡å‹ä¸­çš„éŸ³ä¹ï¼ˆåŒ…æ‹¬æ­Œæ›²ï¼‰ç†è§£ã€‚å°½ç®¡éŸ³é¢‘è¯­è¨€ç ”ç©¶å·²ç»è¿…é€Ÿå‘å±•ï¼Œä½†ç”±äºéŸ³ä¹çš„åŠ¨æ€ã€åˆ†å±‚å’Œä¿¡æ¯å¯†é›†çš„ç‰¹æ€§ï¼ŒéŸ³ä¹ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç”±äºé«˜è´¨é‡éŸ³ä¹æ•°æ®å’Œæ³¨é‡Šçš„ç¨€ç¼ºï¼Œæ‰©å±•å¼€æ”¾éŸ³é¢‘ç†è§£æ¨¡å‹çš„éš¾åº¦è¿›ä¸€æ­¥åŠ å¤§ã€‚å› æ­¤ï¼Œå…ˆå‰æ¨¡å‹ä»…é™äºç”Ÿæˆç®€çŸ­çš„é«˜çº§æ ‡é¢˜ï¼Œåªèƒ½å›ç­”è¡¨é¢é—®é¢˜ï¼Œå¹¶ä¸”åœ¨è·¨è¶Šä¸åŒéŸ³ä¹æ–‡åŒ–çš„æ³›åŒ–æ–¹é¢è¡¨ç°å‡ºå±€é™æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ•´ç†äº†MF-Skillsæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†é€šè¿‡å¤šé˜¶æ®µç®¡é“è¿›è¡Œæ ‡è®°ï¼Œç”Ÿæˆä¸°å¯Œçš„æ ‡é¢˜å’Œé—®ç­”å¯¹ï¼Œæ¶µç›–å’Œå£°ã€ç»“æ„ã€éŸ³è‰²ã€æ­Œè¯å’Œæ–‡åŒ–èƒŒæ™¯ã€‚æˆ‘ä»¬åœ¨MF-Skillsä¸Šå¯¹å¢å¼ºçš„Audio Flamingo 3ä¸»å¹²è¿›è¡Œå¾®è°ƒï¼Œå¹¶è¿›ä¸€æ­¥åŠ å¼ºå¯¹éŸ³ä¹ç†è§£ç›¸å…³çš„å¤šç§æŠ€èƒ½ã€‚ä¸ºäº†æ”¹å–„æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åè®­ç»ƒé…æ–¹ï¼šé¦–å…ˆä½¿ç”¨MF-Thinkè¿›è¡Œå†·å¯åŠ¨ï¼ŒMF-Thinkæ˜¯ä¸€ç§åŸºäºéŸ³ä¹ç†è®ºçš„æ–°å‹æ€ç»´é“¾æ•°æ®é›†ï¼Œæ¥ç€ä½¿ç”¨åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ è¿›è¡Œè‡ªå®šä¹‰å¥–åŠ±ã€‚éŸ³ä¹ç«çƒˆé¸Ÿåœ¨10å¤šä¸ªéŸ³ä¹ç†è§£å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€ä½³ç»“æœï¼Œæˆä¸ºäº†ä¸€ç§å¤šæ‰å¤šè‰ºä¸”éŸ³ä¹æ™ºèƒ½çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚é™¤äº†å¼ºå¤§çš„å®è¯ç»“æœå¤–ï¼ŒéŸ³ä¹ç«çƒˆé¸Ÿè¿˜ä¸ºé«˜çº§éŸ³ä¹ç†è§£è®¾å®šäº†æ–°çš„æ ‡å‡†ï¼Œå±•ç¤ºäº†æ¨¡å‹å¦‚ä½•ä»è¡¨é¢è¯†åˆ«èµ°å‘æ­Œæ›²çš„åˆ†å±‚ã€äººç±»æ„ŸçŸ¥ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™é¡¹å·¥ä½œæ—¢ä¸ºç¤¾åŒºæä¾›äº†åŸºå‡†æµ‹è¯•ï¼Œä¹Ÿä¸ºä¸‹ä¸€ä»£æ¨¡å‹å¥ å®šäº†åŸºç¡€ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿä»¥äººç±»çš„æ–¹å¼ä¸éŸ³ä¹è¿›è¡Œæœ‰æ„ä¹‰çš„äº¤äº’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10289v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/adlr/MF/">https://research.nvidia.com/labs/adlr/MF/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Music Flamingoï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æ¨è¿›éŸ³ä¹ï¼ˆåŒ…æ‹¬æ­Œæ›²ï¼‰åœ¨åŸºç¡€éŸ³é¢‘æ¨¡å‹ä¸­çš„ç†è§£ã€‚æ–‡ç« è§£å†³äº†éŸ³ä¹ç†è§£ä¸­çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®ç¨€ç¼ºå’Œæ ‡æ³¨å›°éš¾ç­‰é—®é¢˜ï¼Œé€šè¿‡æ„å»ºMF-Skillsæ•°æ®é›†å’Œå¤šé˜¶æ®µç®¡é“æ ‡æ³¨æŠ€æœ¯ï¼Œæé«˜äº†æ¨¡å‹å¯¹å’Œè°ã€ç»“æ„ã€éŸ³è‰²ã€æ­Œè¯å’Œæ–‡åŒ–èƒŒæ™¯çš„ä¸°å¯Œæ ‡æ³¨å’Œé—®ç­”å¯¹çš„èƒ½åŠ›ã€‚é€šè¿‡å¢å¼ºAudio Flamingo 3çš„åå¤‡å¹¶åœ¨MF-Thinkä¸Šè¿›è¡Œåè®­ç»ƒï¼ŒMusic Flamingoæé«˜äº†éŸ³ä¹ç†è§£çš„èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨å¤šä¸ªéŸ³ä¹ç†è§£å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—æœ€ä½³ç»“æœï¼Œå¹¶å±•ç°å‡ºä»è¡¨é¢çº§è¯†åˆ«å‘åˆ†å±‚ã€äººç±»èˆ¬çš„æ­Œæ›²æ„ŸçŸ¥çš„è½¬å˜ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æä¾›äº†å¼ºå¤§çš„å®è¯ç»“æœï¼Œè¿˜ä¸ºç¤¾åŒºå»ºç«‹äº†æ–°çš„æ ‡å‡†ï¼Œä¸ºä¸‹ä¸€ä»£æ¨¡å‹çš„å»ºè®¾å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Music Flamingoæ˜¯ä¸€ä¸ªç”¨äºæ¨è¿›éŸ³ä¹ç†è§£çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ã€‚</li>
<li>éŸ³ä¹ç†è§£çš„æŒ‘æˆ˜åŒ…æ‹¬æ•°æ®çš„åŠ¨æ€ã€åˆ†å±‚å’Œä¿¡æ¯å¯†é›†æ€§è´¨ï¼Œä»¥åŠé«˜è´¨é‡éŸ³ä¹æ•°æ®å’Œæ³¨è§£çš„ç¨€ç¼ºæ€§ã€‚</li>
<li>MF-Skillsæ•°æ®é›†çš„æ„å»ºï¼Œé€šè¿‡å¤šé˜¶æ®µç®¡é“æ ‡æ³¨æŠ€æœ¯ï¼Œæä¾›äº†ä¸°å¯Œçš„æ ‡æ³¨å’Œé—®ç­”å¯¹ï¼Œæ¶µç›–å’Œè°ã€ç»“æ„ã€éŸ³è‰²ã€æ­Œè¯å’Œæ–‡åŒ–èƒŒæ™¯ã€‚</li>
<li>Music Flamingoåœ¨å¤šä¸ªéŸ³ä¹ç†è§£å’Œæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—æœ€ä½³ç»“æœã€‚</li>
<li>Music Flamingoèƒ½å¤Ÿä»è¡¨é¢çº§è¯†åˆ«å‘åˆ†å±‚ã€äººç±»èˆ¬çš„æ­Œæ›²æ„ŸçŸ¥è½¬å˜ã€‚</li>
<li>è¯¥å·¥ä½œæä¾›äº†å¼ºå¤§çš„å®è¯ç»“æœï¼Œä¸ºç¤¾åŒºå»ºç«‹äº†æ–°çš„æ ‡å‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10289">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-094d81e2621adddf46652c6f94b131e6" align="middle">
<img src="https://picx.zhimg.com/v2-7e9c9e893d9a74c449a64f82667240e5" align="middle">
<img src="https://picx.zhimg.com/v2-786abeac15f6374d810f10558827c0e8" align="middle">
<img src="https://picx.zhimg.com/v2-d30d9eb0fe6979099f4a9ebd160e2026" align="middle">
<img src="https://picx.zhimg.com/v2-b7a41cd462b83da6ea883ba87c832385" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Facial-R1-Aligning-Reasoning-and-Recognition-for-Facial-Emotion-Analysis"><a href="#Facial-R1-Aligning-Reasoning-and-Recognition-for-Facial-Emotion-Analysis" class="headerlink" title="Facial-R1: Aligning Reasoning and Recognition for Facial Emotion Analysis"></a>Facial-R1: Aligning Reasoning and Recognition for Facial Emotion Analysis</h2><p><strong>Authors:Jiulong Wu, Yucheng Shen, Lingyong Yan, Haixin Sun, Deguo Xia, Jizhou Huang, Min Cao</strong></p>
<p>Facial Emotion Analysis (FEA) extends traditional facial emotion recognition by incorporating explainable, fine-grained reasoning. The task integrates three subtasks: emotion recognition, facial Action Unit (AU) recognition, and AU-based emotion reasoning to model affective states jointly. While recent approaches leverage Vision-Language Models (VLMs) and achieve promising results, they face two critical limitations: (1) hallucinated reasoning, where VLMs generate plausible but inaccurate explanations due to insufficient emotion-specific knowledge; and (2) misalignment between emotion reasoning and recognition, caused by fragmented connections between observed facial features and final labels. We propose Facial-R1, a three-stage alignment framework that effectively addresses both challenges with minimal supervision. First, we employ instruction fine-tuning to establish basic emotional reasoning capability. Second, we introduce reinforcement training guided by emotion and AU labels as reward signals, which explicitly aligns the generated reasoning process with the predicted emotion. Third, we design a data synthesis pipeline that iteratively leverages the prior stages to expand the training dataset, enabling scalable self-improvement of the model. Built upon this framework, we introduce FEA-20K, a benchmark dataset comprising 17,737 training and 1,688 test samples with fine-grained emotion analysis annotations. Extensive experiments across eight standard benchmarks demonstrate that Facial-R1 achieves state-of-the-art performance in FEA, with strong generalization and robust interpretability.</p>
<blockquote>
<p>é¢éƒ¨æƒ…ç»ªåˆ†æï¼ˆFEAï¼‰é€šè¿‡èå…¥å¯è§£é‡Šçš„ã€ç²¾ç»†çš„æ¨ç†ï¼Œæ‰©å±•äº†ä¼ ç»Ÿçš„é¢éƒ¨æƒ…ç»ªè¯†åˆ«ã€‚è¯¥ä»»åŠ¡ç»“åˆäº†ä¸‰ä¸ªå­ä»»åŠ¡ï¼šæƒ…ç»ªè¯†åˆ«ã€é¢éƒ¨åŠ¨ä½œå•å…ƒï¼ˆAUï¼‰è¯†åˆ«ä»¥åŠåŸºäºAUçš„æƒ…ç»ªæ¨ç†ï¼Œä»¥å…±åŒæ¨¡æ‹Ÿæƒ…æ„ŸçŠ¶æ€ã€‚å°½ç®¡æœ€è¿‘çš„æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¹¶å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†å®ƒä»¬é¢ä¸´ä¸¤ä¸ªå…³é”®å±€é™ï¼šï¼ˆ1ï¼‰è™šå¹»æ¨ç†ï¼Œå³VLMç”±äºç¼ºä¹ç‰¹å®šçš„æƒ…ç»ªçŸ¥è¯†è€Œäº§ç”Ÿä¼¼ä¹åˆç†ä½†ä¸å‡†ç¡®çš„è§£é‡Šï¼›ï¼ˆ2ï¼‰æƒ…ç»ªæ¨ç†ä¸è¯†åˆ«ä¹‹é—´çš„ä¸åŒ¹é…ï¼Œè¿™æ˜¯ç”±äºè§‚å¯Ÿåˆ°çš„é¢éƒ¨ç‰¹å¾ä¸æœ€ç»ˆæ ‡ç­¾ä¹‹é—´çš„è”ç³»ç‰‡æ®µåŒ–æ‰€å¯¼è‡´çš„ã€‚æˆ‘ä»¬æå‡ºFacial-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µå¯¹é½æ¡†æ¶ï¼Œä»¥æœ€å°çš„ç›‘ç£æœ‰æ•ˆåœ°è§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é‡‡ç”¨æŒ‡ä»¤å¾®è°ƒæ¥å»ºç«‹åŸºæœ¬çš„æƒ…ç»ªæ¨ç†èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥å¼ºåŒ–è®­ç»ƒï¼Œä»¥æƒ…æ„Ÿå’ŒAUæ ‡ç­¾ä½œä¸ºå¥–åŠ±ä¿¡å·è¿›è¡ŒæŒ‡å¯¼ï¼Œæ˜¾å¼åœ°å°†ç”Ÿæˆçš„æ¨ç†è¿‡ç¨‹ä¸é¢„æµ‹çš„æƒ…ç»ªå¯¹é½ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ•°æ®åˆæˆç®¡é“ï¼Œè¯¥ç®¡é“é€šè¿‡è¿­ä»£åˆ©ç”¨å…ˆå‰çš„é˜¶æ®µæ¥æ‰©å±•è®­ç»ƒæ•°æ®é›†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå®ç°å¯æ‰©å±•çš„è‡ªæˆ‘æ”¹è¿›ã€‚åŸºäºæ­¤æ¡†æ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢éƒ¨æƒ…ç»ªåˆ†æ-20Kï¼ˆFEA-20Kï¼‰åŸºå‡†æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å¸¦æœ‰ç²¾ç»†æƒ…ç»ªåˆ†ææ³¨é‡Šçš„17737ä¸ªè®­ç»ƒæ ·æœ¬å’Œ1688ä¸ªæµ‹è¯•æ ·æœ¬ã€‚åœ¨å…«ä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFacial-R1åœ¨é¢éƒ¨æƒ…ç»ªåˆ†ææ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½æ°´å¹³ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥çš„å¯è§£é‡Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10254v1">PDF</a> This paper has been accepted by AAAI 2026. 16 pages, 3 figures, 10 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¢éƒ¨æƒ…ç»ªåˆ†æï¼ˆFEAï¼‰çš„æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡ç»“åˆå¯è§£é‡Šæ€§ã€ç²¾ç»†ç²’åº¦çš„æ¨ç†æ¥æ‰©å±•ä¼ ç»Ÿçš„é¢éƒ¨æƒ…ç»ªè¯†åˆ«ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µå¯¹é½æ¡†æ¶â€”â€”Facial-R1ï¼Œæ—¨åœ¨è§£å†³å½“å‰æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚å¹»è§‰æ¨ç†å’Œæƒ…ç»ªæ¨ç†ä¸è¯†åˆ«ä¹‹é—´çš„ä¸ä¸€è‡´ã€‚è¯¥æ¡†æ¶é€šè¿‡æŒ‡ä»¤å¾®è°ƒå»ºç«‹åŸºæœ¬æƒ…ç»ªæ¨ç†èƒ½åŠ›ï¼Œå¼•å…¥å¼ºåŒ–è®­ç»ƒä»¥æƒ…æ„Ÿä¸AUæ ‡ç­¾ä½œä¸ºå¥–åŠ±ä¿¡å·æ˜ç¡®å¯¹é½ç”Ÿæˆæ¨ç†è¿‡ç¨‹ä¸é¢„æµ‹æƒ…æ„Ÿï¼Œå¹¶è®¾è®¡æ•°æ®åˆæˆç®¡é“ä»¥æ‰©å¤§è®­ç»ƒæ•°æ®é›†ï¼Œå®ç°æ¨¡å‹çš„è‡ªæˆ‘æ”¹è¿›ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†åŒ…å«ç²¾ç»†ç²’åº¦æƒ…ç»ªåˆ†ææ³¨é‡Šçš„FEA-20KåŸºå‡†æ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼ŒFacial-R1åœ¨é¢éƒ¨æƒ…ç»ªåˆ†æé¢†åŸŸå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢éƒ¨æƒ…ç»ªåˆ†æï¼ˆFEAï¼‰ç»“åˆäº†æƒ…ç»ªè¯†åˆ«ã€é¢éƒ¨åŠ¨ä½œå•å…ƒï¼ˆAUï¼‰è¯†åˆ«å’ŒåŸºäºAUçš„æƒ…ç»ªæ¨ç†ä¸‰ä¸ªå­ä»»åŠ¡ã€‚</li>
<li>å½“å‰æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å–å¾—äº†ä¸€å®šæˆæœï¼Œä½†å­˜åœ¨å¹»è§‰æ¨ç†å’Œæƒ…ç»ªæ¨ç†ä¸è¯†åˆ«ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>Facial-R1æ¡†æ¶é€šè¿‡ä¸‰ä¸ªé˜¶æ®µè§£å†³ä¸Šè¿°é—®é¢˜ï¼šæŒ‡ä»¤å¾®è°ƒã€å¼ºåŒ–è®­ç»ƒå’ŒåŸºäºæƒ…æ„Ÿä¸AUæ ‡ç­¾çš„æ•°æ®åˆæˆç®¡é“è®¾è®¡ã€‚</li>
<li>Facial-R1å®ç°äº†æ¨¡å‹çš„æœ‰æ•ˆè‡ªæˆ‘æ”¹è¿›ï¼Œå¹¶é€šè¿‡æ‰©å±•è®­ç»ƒæ•°æ®é›†æå‡å…¶æ€§èƒ½ã€‚</li>
<li>FEA-20Kæ•°æ®é›†åŒ…å«ç²¾ç»†ç²’åº¦çš„æƒ…ç»ªåˆ†ææ³¨é‡Šï¼Œæœ‰åŠ©äºæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ã€‚</li>
<li>å®éªŒè¯æ˜Facial-R1åœ¨é¢éƒ¨æƒ…ç»ªåˆ†æé¢†åŸŸå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ã€æ³›åŒ–èƒ½åŠ›å’Œå¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e91f06ac4452786ba200d35837bbb88a" align="middle">
<img src="https://picx.zhimg.com/v2-2d8d6d4362dc3f359af5b9cc4b5f4c12" align="middle">
<img src="https://picx.zhimg.com/v2-840c2e6c85def69928e4f0e6322abb9f" align="middle">
<img src="https://picx.zhimg.com/v2-b32c7470217e9ac2a6eba93366d4f731" align="middle">
<img src="https://picx.zhimg.com/v2-0cb074295197638f7d6be8f0b3edcda7" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ProgRAG-Hallucination-Resistant-Progressive-Retrieval-and-Reasoning-over-Knowledge-Graphs"><a href="#ProgRAG-Hallucination-Resistant-Progressive-Retrieval-and-Reasoning-over-Knowledge-Graphs" class="headerlink" title="ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs"></a>ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs</h2><p><strong>Authors:Minbae Park, Hyemin Yang, Jeonghyun Kim, Kunsoo Park, Hyunjoon Kim</strong></p>
<p>Large Language Models (LLMs) demonstrate strong reasoning capabilities but struggle with hallucinations and limited transparency. Recently, KG-enhanced LLMs that integrate knowledge graphs (KGs) have been shown to improve reasoning performance, particularly for complex, knowledge-intensive tasks. However, these methods still face significant challenges, including inaccurate retrieval and reasoning failures, often exacerbated by long input contexts that obscure relevant information or by context constructions that struggle to capture the richer logical directions required by different question types. Furthermore, many of these approaches rely on LLMs to directly retrieve evidence from KGs, and to self-assess the sufficiency of this evidence, which often results in premature or incorrect reasoning. To address the retrieval and reasoning failures, we propose ProgRAG, a multi-hop knowledge graph question answering (KGQA) framework that decomposes complex questions into sub-questions, and progressively extends partial reasoning paths by answering each sub-question. At each step, external retrievers gather candidate evidence, which is then refined through uncertainty-aware pruning by the LLM. Finally, the context for LLM reasoning is optimized by organizing and rearranging the partial reasoning paths obtained from the sub-question answers. Experiments on three well-known datasets demonstrate that ProgRAG outperforms existing baselines in multi-hop KGQA, offering improved reliability and reasoning quality.</p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å¹»è§‰å’Œé€æ˜åº¦æ–¹é¢å­˜åœ¨å±€é™ã€‚æœ€è¿‘ï¼Œé›†æˆçŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰çš„KGå¢å¼ºLLMå·²è¢«è¯æ˜å¯ä»¥æ”¹å–„æ¨ç†æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤æ‚ã€çŸ¥è¯†å¯†é›†å‹çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ£€ç´¢ä¸å‡†ç¡®å’Œæ¨ç†å¤±è´¥ï¼Œè¿™äº›æŒ‘æˆ˜å¾€å¾€å› é•¿è¾“å…¥è¯­å¢ƒè€ŒåŠ å‰§ï¼Œç›¸å…³ä¿¡æ¯çš„è¯­å¢ƒé®è”½æˆ–ä¸åŒé—®é¢˜ç±»å‹æ‰€éœ€çš„æ›´ä¸°å¯Œé€»è¾‘æ–¹å‘éš¾ä»¥æ•æ‰ã€‚æ­¤å¤–ï¼Œè®¸å¤šè¿™äº›æ–¹æ³•ä¾èµ–äºLLMç›´æ¥ä»KGsä¸­æ£€ç´¢è¯æ®ï¼Œå¹¶è‡ªæˆ‘è¯„ä¼°è¯æ®çš„å……åˆ†æ€§ï¼Œè¿™å¾€å¾€å¯¼è‡´è¿‡æ—©æˆ–é”™è¯¯çš„æ¨ç†ã€‚ä¸ºäº†è§£å†³æ£€ç´¢å’Œæ¨ç†å¤±è´¥çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ProgRAGï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè·³çŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰æ¡†æ¶ï¼Œå®ƒå°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºå­é—®é¢˜ï¼Œå¹¶é€šè¿‡å›ç­”æ¯ä¸ªå­é—®é¢˜æ¥é€æ­¥æ‰©å±•éƒ¨åˆ†æ¨ç†è·¯å¾„ã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œå¤–éƒ¨æ£€ç´¢å™¨æ”¶é›†å€™é€‰è¯æ®ï¼Œç„¶åé€šè¿‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥ä¿®å‰ªç”±LLMè¿›è¡Œç²¾ç‚¼ã€‚æœ€åï¼Œé€šè¿‡å¯¹ä»å­é—®é¢˜ç­”æ¡ˆä¸­è·å¾—çš„å±€éƒ¨æ¨ç†è·¯å¾„è¿›è¡Œç»„ç»‡å’Œé‡æ–°æ’åˆ—ï¼Œä¼˜åŒ–LLMæ¨ç†çš„ä¸Šä¸‹æ–‡ã€‚åœ¨ä¸‰ä¸ªçŸ¥åæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒProgRAGåœ¨å¤šè·³KGQAæ–¹é¢çš„æ€§èƒ½è¶…è¿‡äº†ç°æœ‰åŸºçº¿ï¼Œæé«˜äº†å¯é æ€§å’Œæ¨ç†è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10240v1">PDF</a> </p>
<p><strong>Summary</strong>:<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å¹»è§‰å’Œé€æ˜åº¦æ–¹é¢å­˜åœ¨å±€é™ã€‚èåˆçŸ¥è¯†å›¾è°±ï¼ˆKGsï¼‰çš„KGå¢å¼ºLLMsåœ¨å¤æ‚ã€çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­æ”¹å–„äº†æ¨ç†æ€§èƒ½ï¼Œä½†ä»é¢ä¸´ä¸å‡†ç¡®æ£€ç´¢å’Œæ¨ç†å¤±è´¥ç­‰æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºProgRAGå¤šè·³çŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰æ¡†æ¶ï¼Œé€šè¿‡åˆ†è§£å¤æ‚é—®é¢˜ä¸ºå­é—®é¢˜ï¼Œé€æ­¥æ‰©å±•éƒ¨åˆ†æ¨ç†è·¯å¾„ã€‚å®éªŒè¯æ˜ï¼ŒProgRAGåœ¨å¤šè·³KGQAä¸­ä¼˜äºç°æœ‰åŸºçº¿ï¼Œæé«˜äº†å¯é æ€§å’Œæ¨ç†è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>LLMså…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨å¹»è§‰å’Œé€æ˜åº¦é—®é¢˜ã€‚</li>
<li>KGå¢å¼ºLLMsèåˆçŸ¥è¯†å›¾è°±æ”¹å–„äº†åœ¨å¤æ‚ã€çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­çš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>KGå¢å¼ºLLMsé¢ä¸´ä¸å‡†ç¡®æ£€ç´¢å’Œæ¨ç†å¤±è´¥çš„æŒ‘æˆ˜ã€‚</li>
<li>ProgRAGæ¡†æ¶é€šè¿‡åˆ†è§£å¤æ‚é—®é¢˜ä¸ºå­é—®é¢˜ï¼Œé€æ­¥æ‰©å±•éƒ¨åˆ†æ¨ç†è·¯å¾„æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>ProgRAGåˆ©ç”¨å¤–éƒ¨æ£€ç´¢å™¨æ”¶é›†å€™é€‰è¯æ®ï¼Œå¹¶é€šè¿‡ä¸ç¡®å®šæ€§æ„ŸçŸ¥ä¿®å‰ªè¿›è¡Œç²¾ç‚¼ã€‚</li>
<li>LLMä¼˜åŒ–ä¸Šä¸‹æ–‡æ˜¯é€šè¿‡ç»„ç»‡å’Œé‡æ–°æ’åˆ—ä»å­é—®é¢˜ç­”æ¡ˆä¸­è·å¾—çš„å±€éƒ¨æ¨ç†è·¯å¾„æ¥å®ç°çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10240">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8d2f871e47232d7206c62ec42324e3d9" align="middle">
<img src="https://picx.zhimg.com/v2-46b9f7eb5360426cd1a441e91aa50a7f" align="middle">
<img src="https://picx.zhimg.com/v2-736d0ceb834caa45856e5271c4725b2e" align="middle">
<img src="https://picx.zhimg.com/v2-4c3bc3d04598ba9c3e5daff599a10b10" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Lost-in-Serialization-Invariance-and-Generalization-of-LLM-Graph-Reasoners"><a href="#Lost-in-Serialization-Invariance-and-Generalization-of-LLM-Graph-Reasoners" class="headerlink" title="Lost in Serialization: Invariance and Generalization of LLM Graph Reasoners"></a>Lost in Serialization: Invariance and Generalization of LLM Graph Reasoners</h2><p><strong>Authors:Daniel Herbst, Lea Karbeska, Divyanshu Kumar, Akanksha Ahuja, Fatemeh Gholamzadeh Nasrabadi, Fabrizio Frasca</strong></p>
<p>While promising, graph reasoners based on Large Language Models (LLMs) lack built-in invariance to symmetries in graph representations. Operating on sequential graph serializations, LLMs can produce different outputs under node reindexing, edge reordering, or formatting changes, raising robustness concerns. We systematically analyze these effects, studying how fine-tuning impacts encoding sensitivity as well generalization on unseen tasks. We propose a principled decomposition of graph serializations into node labeling, edge encoding, and syntax, and evaluate LLM robustness to variations of each of these factors on a comprehensive benchmarking suite. We also contribute a novel set of spectral tasks to further assess generalization abilities of fine-tuned reasoners. Results show that larger (non-fine-tuned) models are more robust. Fine-tuning reduces sensitivity to node relabeling but may increase it to variations in structure and format, while it does not consistently improve performance on unseen tasks.</p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å›¾æ¨ç†å™¨è™½ç„¶å‰æ™¯å¹¿é˜”ï¼Œä½†åœ¨å¤„ç†å›¾è¡¨ç¤ºä¸­çš„å¯¹ç§°æ€§æ—¶ç¼ºä¹å†…ç½®çš„ä¸å˜æ€§ã€‚ç”±äºæ“ä½œçš„æ˜¯åºåˆ—åŒ–çš„å›¾ï¼Œè¿™äº›æ¨¡å‹åœ¨èŠ‚ç‚¹é‡æ–°ç´¢å¼•ã€è¾¹é‡æ–°æ’åºæˆ–æ ¼å¼æ›´æ”¹æ—¶ä¼šäº§ç”Ÿä¸åŒçš„è¾“å‡ºï¼Œè¿™å¼•å‘äº†å…³äºå…¶ç¨³å¥æ€§çš„æ‹…å¿§ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°åˆ†æäº†è¿™äº›å½±å“ï¼Œç ”ç©¶äº†å¾®è°ƒå¯¹ç¼–ç æ•æ„Ÿæ€§çš„å½±å“ä»¥åŠå¯¹æœªè§ä»»åŠ¡æ³›åŒ–çš„å½±å“ã€‚æˆ‘ä»¬å¯¹å›¾åºåˆ—åŒ–è¿›è¡Œæœ‰åŸåˆ™çš„åˆ†è§£ï¼ŒåŒ…æ‹¬èŠ‚ç‚¹æ ‡è®°ã€è¾¹ç¼˜ç¼–ç å’Œè¯­æ³•ï¼Œå¹¶åœ¨ä¸€å¥—å…¨é¢çš„åŸºå‡†æµ‹è¯•ä¸­å¯¹å¤§å‹è¯­è¨€æ¨¡å‹å¯¹è¿™äº›å› ç´ å˜åŒ–çš„ç¨³å¥æ€§è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬è¿˜è´¡çŒ®äº†ä¸€ç³»åˆ—æ–°çš„å…‰è°±ä»»åŠ¡ï¼Œä»¥è¿›ä¸€æ­¥è¯„ä¼°ç»è¿‡å¾®è°ƒåçš„æ¨ç†å™¨çš„æ³›åŒ–èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œè¾ƒå¤§çš„ï¼ˆæœªç»è¿‡å¾®è°ƒï¼‰æ¨¡å‹æ›´ç¨³å¥ã€‚å¾®è°ƒé™ä½äº†å¯¹èŠ‚ç‚¹é‡æ–°æ ‡è®°çš„æ•æ„Ÿæ€§ï¼Œä½†å¯èƒ½ä¼šå¢åŠ å¯¹ç»“æ„å’Œæ ¼å¼å˜åŒ–çš„æ•æ„Ÿæ€§ï¼Œè€Œä¸”å®ƒå¹¶ä¸æ€»èƒ½æé«˜æœªè§ä»»åŠ¡çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10234v1">PDF</a> AAAI 2026 Workshop on Graphs and more Complex Structures For Learning and Reasoning (GCLR)</p>
<p><strong>Summary</strong>:<br>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å›¾æ¨ç†å™¨è™½å…·æ½œåŠ›ï¼Œä½†åœ¨å¤„ç†å›¾å½¢è¡¨ç¤ºä¸­çš„å¯¹ç§°æ€§æ—¶ç¼ºä¹å†…ç½®ä¸å˜æ€§ã€‚é’ˆå¯¹èŠ‚ç‚¹é‡æ–°ç´¢å¼•ã€è¾¹é‡æ–°æ’åºæˆ–æ ¼å¼æ›´æ”¹ç­‰æ“ä½œï¼ŒLLMsä¼šäº§ç”Ÿä¸åŒçš„è¾“å‡ºï¼Œè¿™å¼•å‘äº†ç¨³å¥æ€§çš„æ‹…å¿§ã€‚æœ¬æ–‡é€šè¿‡ç³»ç»Ÿåœ°åˆ†æè¿™äº›å› ç´ ï¼Œç ”ç©¶å¾®è°ƒå¯¹ç¼–ç æ•æ„Ÿæ€§çš„å½±å“ä»¥åŠåœ¨æœªè§ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡å°†å›¾åºåˆ—åŒ–åˆ†è§£ä¸ºèŠ‚ç‚¹æ ‡ç­¾ã€è¾¹ç¼–ç å’Œè¯­æ³•ï¼Œå¹¶è¯„ä¼°LLMå¯¹ä¸åŒå› ç´ çš„ç¨³å¥æ€§ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è´¡çŒ®äº†ä¸€ç³»åˆ—å…‰è°±ä»»åŠ¡æ¥è¿›ä¸€æ­¥è¯„ä¼°å¾®è°ƒåçš„æ¨ç†èƒ½åŠ›çš„æ³›åŒ–èƒ½åŠ›ã€‚ç»“æœè¡¨æ˜ï¼Œè¾ƒå¤§çš„ï¼ˆæœªå¾®è°ƒï¼‰æ¨¡å‹æ›´ç¨³å¥ï¼Œå¾®è°ƒå¯ä»¥å‡å°‘å¯¹èŠ‚ç‚¹é‡æ–°æ ‡è®°çš„æ•æ„Ÿæ€§ï¼Œä½†å¯èƒ½ä¼šå¢åŠ å¯¹ç»“æ„å’Œæ ¼å¼å˜åŒ–çš„æ•æ„Ÿæ€§ï¼Œå¹¶ä¸”åœ¨æœªè§ä»»åŠ¡ä¸Šå¹¶ä¸æ€»æ˜¯èƒ½æé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å›¾æ¨ç†å™¨å­˜åœ¨ç¨³å¥æ€§é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬åœ¨å¤„ç†å›¾å½¢è¡¨ç¤ºä¸­çš„å¯¹ç§°æ€§æ—¶ç¼ºä¹å†…ç½®ä¸å˜æ€§ã€‚</li>
<li>èŠ‚ç‚¹é‡æ–°ç´¢å¼•ã€è¾¹é‡æ–°æ’åºå’Œæ ¼å¼æ›´æ”¹ä¼šå¯¼è‡´LLMsäº§ç”Ÿä¸åŒçš„è¾“å‡ºã€‚</li>
<li>é€šè¿‡ç³»ç»Ÿåœ°åˆ†æï¼Œç ”ç©¶å‘ç°å¾®è°ƒå½±å“ç¼–ç æ•æ„Ÿæ€§å’Œåœ¨æœªè§ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºå°†å›¾åºåˆ—åŒ–åˆ†è§£ä¸ºèŠ‚ç‚¹æ ‡ç­¾ã€è¾¹ç¼–ç å’Œè¯­æ³•çš„æ–¹æ³•ï¼Œä»¥è¯„ä¼°LLMå¯¹ä¸åŒå› ç´ çš„ç¨³å¥æ€§ã€‚</li>
<li>è´¡çŒ®äº†ä¸€ç³»åˆ—å…‰è°±ä»»åŠ¡æ¥è¯„ä¼°æ¨ç†èƒ½åŠ›çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¾ƒå¤§çš„æ¨¡å‹ï¼ˆæœªç»å¾®è°ƒï¼‰è¡¨ç°å‡ºæ›´é«˜çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10234">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-78370b9b4bb524bc92c999c5d5757de3" align="middle">
<img src="https://picx.zhimg.com/v2-d4981fb3fd242848fceb6a34e1eaf1f2" align="middle">
<img src="https://picx.zhimg.com/v2-c87fbf8797735865c2878397ee8c97b2" align="middle">
<img src="https://picx.zhimg.com/v2-23bf85e447e5904ba52cad3a809f6e33" align="middle">
<img src="https://picx.zhimg.com/v2-bb3a485ff83192bc21db5e58e0bb1372" align="middle">
<img src="https://picx.zhimg.com/v2-c27bf0b1548d3c83c46819c6c80a8343" align="middle">
<img src="https://picx.zhimg.com/v2-4f6d2e6774dc4ed193f1ab81f34b3ba2" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Text2SQL-Flow-A-Robust-SQL-Aware-Data-Augmentation-Framework-for-Text-to-SQL"><a href="#Text2SQL-Flow-A-Robust-SQL-Aware-Data-Augmentation-Framework-for-Text-to-SQL" class="headerlink" title="Text2SQL-Flow: A Robust SQL-Aware Data Augmentation Framework for Text-to-SQL"></a>Text2SQL-Flow: A Robust SQL-Aware Data Augmentation Framework for Text-to-SQL</h2><p><strong>Authors:Qifeng Cai, Hao Liang, Chang Xu, Tao Xie, Wentao Zhang, Bin Cui</strong></p>
<p>The data-centric paradigm has become pivotal in AI, especially for Text-to-SQL, where performance is limited by scarce, simplistic, and low-diversity datasets. To address this, we propose Text2SQL-Flow, a SQL-aware data augmentation framework that generates large-scale, semantically valid, and structurally diverse Text-to-SQL pairs from minimal seed data. It operates across six augmentation dimensions and integrates an end-to-end pipeline featuring SQL execution verification, natural language question generation, chain-of-thought reasoning traces, and data classification. A modular Database Manager ensures cross-database compatibility and scalability. Using this framework, we build SQLFlow, a high-quality dataset of 89,544 annotated examples. We evaluate SQLFlow in two settings: (1) For open-source LLMs, fine-tuning on SQLFlow consistently improves performance across benchmarks under the same data budget. (2) For closed-source LLMs, we introduce a masked alignment retrieval method that treats SQLFlow as both knowledge base and training data for the retriever. This enables structure-aware example matching by modeling fine-grained alignments between questions and SQL queries. Experiments show our retrieval strategy outperforms existing methods, underscoring the value of SQLFlowâ€™s high-fidelity data and our novel technique. Our work establishes a scalable, data-centric foundation for advancing Text-to-SQL systems and highlights the critical role of high-quality structured data in modern AI.</p>
<blockquote>
<p>æ•°æ®ä¸ºä¸­å¿ƒçš„èŒƒå¼åœ¨äººå·¥æ™ºèƒ½ä¸­å·²æˆä¸ºå…³é”®ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ°SQLçš„è½¬æ¢ä¸­ï¼Œå…¶æ€§èƒ½å—é™äºç¨€ç¼ºã€ç®€å•å’Œä½å¤šæ ·æ€§çš„æ•°æ®é›†ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Text2SQL-Flowï¼Œè¿™æ˜¯ä¸€ä¸ªSQLæ„ŸçŸ¥çš„æ•°æ®å¢å¼ºæ¡†æ¶ï¼Œå¯ä»å°‘é‡ç§å­æ•°æ®ä¸­ç”Ÿæˆå¤§è§„æ¨¡ã€è¯­ä¹‰æœ‰æ•ˆä¸”ç»“æ„å¤šæ ·çš„æ–‡æœ¬åˆ°SQLå¯¹ã€‚å®ƒåœ¨å…­ä¸ªå¢å¼ºç»´åº¦ä¸Šè¿è¡Œï¼Œå¹¶é›†æˆäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç®¡é“ï¼Œå…·æœ‰SQLæ‰§è¡ŒéªŒè¯ã€è‡ªç„¶è¯­è¨€é—®é¢˜ç”Ÿæˆã€æ€ç»´é“¾æ¨ç†è·Ÿè¸ªå’Œæ•°æ®åˆ†ç±»ç­‰åŠŸèƒ½ã€‚æ¨¡å—åŒ–æ•°æ®åº“ç®¡ç†å™¨ç¡®ä¿è·¨æ•°æ®åº“å…¼å®¹æ€§å’Œå¯æ‰©å±•æ€§ã€‚ä½¿ç”¨æ­¤æ¡†æ¶ï¼Œæˆ‘ä»¬æ„å»ºäº†SQLFlowæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«89,544ä¸ªæ³¨é‡Šç¤ºä¾‹ã€‚æˆ‘ä»¬åœ¨ä¸¤ç§ç¯å¢ƒä¸‹å¯¹SQLFlowè¿›è¡Œäº†è¯„ä¼°ï¼šï¼ˆ1ï¼‰å¯¹äºå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨ç›¸åŒçš„æ•°æ®é¢„ç®—ä¸‹ï¼Œå¯¹SQLFlowè¿›è¡Œå¾®è°ƒå¯ä»¥å§‹ç»ˆæ”¹å–„è·¨åŸºå‡†æµ‹è¯•çš„æ€§èƒ½ã€‚ï¼ˆ2ï¼‰å¯¹äºå°é—­æºä»£ç çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ©ç å¯¹é½æ£€ç´¢æ–¹æ³•ï¼Œå°†SQLFlowæ—¢è§†ä¸ºçŸ¥è¯†åº“åˆä½œä¸ºæ£€ç´¢å™¨çš„è®­ç»ƒæ•°æ®ã€‚è¿™é€šè¿‡å»ºæ¨¡é—®é¢˜å’ŒSQLæŸ¥è¯¢ä¹‹é—´çš„ç²¾ç»†å¯¹é½ï¼Œå®ç°äº†ç»“æ„æ„ŸçŸ¥çš„ç¤ºä¾‹åŒ¹é…ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ£€ç´¢ç­–ç•¥ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œçªæ˜¾äº†SQLFlowé«˜ä¿çœŸæ•°æ®çš„ä»·å€¼å’Œæˆ‘ä»¬çš„æ–°æŠ€æœ¯ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºæ¨è¿›æ–‡æœ¬åˆ°SQLç³»ç»Ÿçš„æŠ€æœ¯å»ºç«‹äº†å¯æ‰©å±•çš„ã€ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„åŸºç¡€ï¼Œå¹¶å¼ºè°ƒäº†é«˜è´¨é‡ç»“æ„åŒ–æ•°æ®åœ¨ç°ä»£äººå·¥æ™ºèƒ½ä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10192v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ•°æ®é©±åŠ¨çš„æ–¹æ³•åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ°SQLè½¬æ¢ä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹æ–‡æœ¬åˆ°SQLè½¬æ¢ä¸­æ•°æ®é›†ç¼ºä¹ã€ç®€å•å’Œä½å¤šæ ·æ€§çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºText2SQL-Flowçš„SQLæ„ŸçŸ¥æ•°æ®å¢å¼ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å…­ä¸ªå¢å¼ºç»´åº¦ç”Ÿæˆå¤§è§„æ¨¡ã€è¯­ä¹‰æœ‰æ•ˆå’Œç»“æ„å¤šæ ·çš„æ–‡æœ¬åˆ°SQLå¯¹ï¼Œå¹¶ä½¿ç”¨SQLæ‰§è¡ŒéªŒè¯ã€è‡ªç„¶è¯­è¨€é—®é¢˜ç”Ÿæˆã€æ€ç»´é“¾è·Ÿè¸ªå’Œæ•°æ®åˆ†ç±»ç­‰æŠ€æœ¯æ„å»ºä¸€ä¸ªå®Œæ•´æµç¨‹ã€‚é€šè¿‡ä½¿ç”¨è¯¥æ¡†æ¶åˆ›å»ºäº†åä¸ºSQLFlowçš„é«˜è´¨é‡æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œè¿˜æ¢è®¨äº†ä½¿ç”¨å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹å’Œé—­æºå¤§å‹è¯­è¨€æ¨¡å‹çš„æƒ…å†µä¸‹æ•°æ®é›†çš„åº”ç”¨æ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ£€ç´¢ç­–ç•¥ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜äº†å…¶ä»·å€¼ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶ä¸ºæ–‡æœ¬åˆ°SQLç³»ç»Ÿçš„è¿›æ­¥å»ºç«‹äº†å¯æ‰©å±•çš„æ•°æ®é©±åŠ¨åŸºç¡€ï¼Œå¹¶å¼ºè°ƒäº†é«˜è´¨é‡ç»“æ„åŒ–æ•°æ®åœ¨ç°ä»£äººå·¥æ™ºèƒ½ä¸­çš„å…³é”®ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®é©±åŠ¨æ–¹æ³•åœ¨AIé¢†åŸŸçš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ°SQLè½¬æ¢ä¸­çš„åº”ç”¨ã€‚</li>
<li>Text2SQL-Flowæ¡†æ¶ç”¨äºè§£å†³æ–‡æœ¬åˆ°SQLè½¬æ¢ä¸­æ•°æ®é›†ç¼ºä¹å¤šæ ·æ€§çš„é—®é¢˜ã€‚</li>
<li>Text2SQL-Flowæ¡†æ¶ç”Ÿæˆå¤§è§„æ¨¡ã€è¯­ä¹‰æœ‰æ•ˆå’Œç»“æ„å¤šæ ·çš„æ–‡æœ¬åˆ°SQLå¯¹æ•°æ®ã€‚</li>
<li>æ¡†æ¶åŒ…å«SQLæ‰§è¡ŒéªŒè¯ã€è‡ªç„¶è¯­è¨€é—®é¢˜ç”Ÿæˆã€æ€ç»´é“¾è·Ÿè¸ªå’Œæ•°æ®åˆ†ç±»ç­‰æŠ€æœ¯ã€‚</li>
<li>ä½¿ç”¨Text2SQL-Flowæ¡†æ¶åˆ›å»ºäº†åä¸ºSQLFlowçš„é«˜è´¨é‡æ•°æ®é›†ã€‚</li>
<li>æ¢è®¨äº†ä½¿ç”¨å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹å’Œé—­æºå¤§å‹è¯­è¨€æ¨¡å‹çš„æƒ…å†µä¸‹æ•°æ®é›†çš„åº”ç”¨æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10192">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf19dcbdb24d76d40f20d4167fda8c61" align="middle">
<img src="https://picx.zhimg.com/v2-adff3d310463a4a7aa85d8dfbf1e5780" align="middle">
<img src="https://picx.zhimg.com/v2-ae30f6ccdde243b725eca771e87d26f6" align="middle">
<img src="https://picx.zhimg.com/v2-af8eff8fd95f7fd7608f11b681ff5314" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e5770523b02b4aa1671994558f7c0f47" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-15/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e9fa8a83d0ea3b3db435986ba2aa53f1" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-15  End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32714.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
