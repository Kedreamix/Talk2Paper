<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  From 2D to 3D Without Extra Baggage Data-Efficient Cancer Detection in Digital Breast Tomosynthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-8732f3f412c53512adc63b28ea5bb7de')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-16-æ›´æ–°"><a href="#2025-11-16-æ›´æ–°" class="headerlink" title="2025-11-16 æ›´æ–°"></a>2025-11-16 æ›´æ–°</h1><h2 id="From-2D-to-3D-Without-Extra-Baggage-Data-Efficient-Cancer-Detection-in-Digital-Breast-Tomosynthesis"><a href="#From-2D-to-3D-Without-Extra-Baggage-Data-Efficient-Cancer-Detection-in-Digital-Breast-Tomosynthesis" class="headerlink" title="From 2D to 3D Without Extra Baggage: Data-Efficient Cancer Detection in Digital Breast Tomosynthesis"></a>From 2D to 3D Without Extra Baggage: Data-Efficient Cancer Detection in Digital Breast Tomosynthesis</h2><p><strong>Authors:Yen Nhi Truong Vu, Dan Guo, Sripad Joshi, Harshit Kumar, Jason Su, Thomas Paul Matthews</strong></p>
<p>Digital Breast Tomosynthesis (DBT) enhances finding visibility for breast cancer detection by providing volumetric information that reduces the impact of overlapping tissues; however, limited annotated data has constrained the development of deep learning models for DBT. To address data scarcity, existing methods attempt to reuse 2D full-field digital mammography (FFDM) models by either flattening DBT volumes or processing slices individually, thus discarding volumetric information. Alternatively, 3D reasoning approaches introduce complex architectures that require more DBT training data. Tackling these drawbacks, we propose M&amp;M-3D, an architecture that enables learnable 3D reasoning while remaining parameter-free relative to its FFDM counterpart, M&amp;M. M&amp;M-3D constructs malignancy-guided 3D features, and 3D reasoning is learned through repeatedly mixing these 3D features with slice-level information. This is achieved by modifying operations in M&amp;M without adding parameters, thus enabling direct weight transfer from FFDM. Extensive experiments show that M&amp;M-3D surpasses 2D projection and 3D slice-based methods by 11-54% for localization and 3-10% for classification. Additionally, M&amp;M-3D outperforms complex 3D reasoning variants by 20-47% for localization and 2-10% for classification in the low-data regime, while matching their performance in high-data regime. On the popular BCS-DBT benchmark, M&amp;M-3D outperforms previous top baseline by 4% for classification and 10% for localization.</p>
<blockquote>
<p>æ•°å­—ä¹³è…ºæ–­å±‚åˆæˆæŠ€æœ¯ï¼ˆDBTï¼‰é€šè¿‡æä¾›ä½“ç§¯ä¿¡æ¯å‡å°‘äº†é‡å ç»„ç»‡çš„å½±å“ï¼Œæé«˜äº†ä¹³è…ºç™Œæ£€æµ‹çš„å¯è§æ€§ã€‚ç„¶è€Œï¼Œæ ‡æ³¨æ•°æ®çš„æœ‰é™é™åˆ¶äº†DBTæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å‘å±•ã€‚ä¸ºäº†åº”å¯¹æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡é‡æ–°ä½¿ç”¨äºŒç»´å…¨åœºæ•°å­—ä¹³è…ºæ‘„å½±ï¼ˆFFDMï¼‰æ¨¡å‹æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¿™äº›æ–¹æ³•è¦ä¹ˆé€šè¿‡å‹å¹³DBTä½“ç§¯è¦ä¹ˆå•ç‹¬å¤„ç†åˆ‡ç‰‡æ¥ä¸¢å¼ƒä½“ç§¯ä¿¡æ¯ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸‰ç»´æ¨ç†æ–¹æ³•å¼•å…¥äº†å¤æ‚çš„æ¶æ„ï¼Œéœ€è¦æ›´å¤šçš„DBTè®­ç»ƒæ•°æ®ã€‚ä¸ºäº†è§£å†³è¿™äº›ç¼ºç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†M&amp;M-3Dæ¶æ„ï¼Œå®ƒèƒ½å¤Ÿåœ¨ä¸‰ç»´æ¨ç†ä¸­å­¦ä¹ ï¼ŒåŒæ—¶ç›¸å¯¹äºå…¶FFDMå¯¹åº”ç‰©M&amp;Mä¿æŒæ— å‚æ•°çŠ¶æ€ã€‚M&amp;M-3Dæ„å»ºäº†ä»¥æ¶æ€§ç—…å˜ä¸ºå¯¼å‘çš„ä¸‰ç»´ç‰¹å¾ï¼Œé€šè¿‡åå¤æ··åˆè¿™äº›ä¸‰ç»´ç‰¹å¾ä¸åˆ‡ç‰‡çº§åˆ«çš„ä¿¡æ¯æ¥å­¦ä¹ ä¸‰ç»´æ¨ç†ã€‚è¿™æ˜¯é€šè¿‡åœ¨M&amp;Mä¸­ä¿®æ”¹æ“ä½œè€Œæ— éœ€æ·»åŠ å‚æ•°æ¥å®ç°çš„ï¼Œä»è€Œå®ç°ä»FFDMçš„ç›´æ¥æƒé‡è½¬ç§»ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨å®šä½å’Œåˆ†ç±»æ–¹é¢ï¼ŒM&amp;M-3Dç›¸è¾ƒäºäºŒç»´æŠ•å½±å’ŒåŸºäºä¸‰ç»´åˆ‡ç‰‡çš„æ–¹æ³•æé«˜äº†11%-54%å’Œ3%-10%ã€‚æ­¤å¤–ï¼Œåœ¨ä½æ•°æ®æƒ…å†µä¸‹ï¼ŒM&amp;M-3Dåœ¨å®šä½å’Œåˆ†ç±»æ–¹é¢åˆ†åˆ«è¶…è¶Šäº†å¤æ‚çš„ä¸‰ç»´æ¨ç†å˜ä½“20%-47%å’Œ2%-10%ï¼Œè€Œåœ¨é«˜æ•°æ®æƒ…å†µä¸‹ä¸ä¹‹è¡¨ç°ç›¸åŒ¹é…ã€‚åœ¨æµè¡Œçš„BCS-DBTåŸºå‡†æµ‹è¯•ä¸­ï¼ŒM&amp;M-3Dåœ¨åˆ†ç±»æ–¹é¢ä¼˜äºä¹‹å‰çš„æœ€ä½³åŸºçº¿æ¨¡å‹4%ï¼Œåœ¨å®šä½æ–¹é¢æé«˜äº†10%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10597v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    DBTæŠ€æœ¯èƒ½æé«˜ä¹³è…ºç™Œæ£€æµ‹ä¸­ç—…ç¶çš„å¯è§æ€§ï¼Œä½†å…¶æœ‰é™çš„æ ‡æ³¨æ•°æ®é™åˆ¶äº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å‘å±•ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºM&amp;M-3Dæ¶æ„ï¼Œå¯åœ¨æ— éœ€é¢å¤–å‚æ•°çš„æƒ…å†µä¸‹å®ç°å¯å­¦ä¹ çš„ä¸‰ç»´æ¨ç†ã€‚å®ƒé€šè¿‡æ„å»ºæ¶æ€§ç—…å˜å¼•å¯¼çš„ä¸‰ç»´ç‰¹å¾ï¼Œå¹¶é€šè¿‡åå¤æ··åˆè¿™äº›ç‰¹å¾ä¸åˆ‡ç‰‡çº§ä¿¡æ¯æ¥å­¦ä¹ ä¸‰ç»´æ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼ŒM&amp;M-3Dåœ¨å®šä½å’Œåˆ†ç±»æ–¹é¢è¶…è¶Šäº†äºŒç»´æŠ•å½±å’Œä¸‰ç»´åˆ‡ç‰‡æ–¹æ³•ï¼Œä¸”åœ¨ä½æ•°æ®åœºæ™¯ä¸‹è¡¨ç°å‡ºæ›´å‡ºè‰²çš„æ€§èƒ½ã€‚åœ¨BCS-DBTåŸºå‡†æµ‹è¯•ä¸­ï¼ŒM&amp;M-3Dçš„åˆ†ç±»å’Œå®šä½æ€§èƒ½å‡ä¼˜äºå…ˆå‰æœ€ä½³æ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>DBTæŠ€æœ¯é€šè¿‡æä¾›ä½“ç§¯ä¿¡æ¯æé«˜ä¹³è…ºç™Œæ£€æµ‹çš„ç—…ç¶å¯è§æ€§ï¼Œä½†æ ‡æ³¨æ•°æ®çš„æœ‰é™æ€§é™åˆ¶äº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å‘å±•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡å¹³å¦åŒ–DBTä½“ç§¯æˆ–å•ç‹¬å¤„ç†åˆ‡ç‰‡æ¥å†åˆ©ç”¨äºŒç»´å…¨åœºæ•°å­—ä¹³è…ºæ‘„å½±ï¼ˆFFDMï¼‰æ¨¡å‹ï¼Œä»è€Œå¿½ç•¥äº†ä½“ç§¯ä¿¡æ¯ã€‚</li>
<li>å¤æ‚çš„ä¸‰ç»´æ¨ç†æ–¹æ³•éœ€è¦å¤§é‡DBTè®­ç»ƒæ•°æ®ã€‚</li>
<li>M&amp;M-3Dæ¶æ„èƒ½åœ¨æ— éœ€é¢å¤–å‚æ•°çš„æƒ…å†µä¸‹å®ç°å¯å­¦ä¹ çš„ä¸‰ç»´æ¨ç†ï¼Œé€šè¿‡æ„å»ºæ¶æ€§ç—…å˜å¼•å¯¼çš„ä¸‰ç»´ç‰¹å¾ï¼Œå¹¶ä¸åˆ‡ç‰‡çº§ä¿¡æ¯æ··åˆæ¥å­¦ä¹ ä¸‰ç»´æ¨ç†ã€‚</li>
<li>M&amp;M-3Dåœ¨å®šä½å’Œåˆ†ç±»æ–¹é¢è¶…è¶Šäº†äºŒç»´æŠ•å½±å’Œä¸‰ç»´åˆ‡ç‰‡æ–¹æ³•ï¼Œæ€§èƒ½æå‡èŒƒå›´ä¸º11-54%ã€‚</li>
<li>åœ¨ä½æ•°æ®åœºæ™¯ä¸‹ï¼ŒM&amp;M-3Dåœ¨å®šä½å’Œåˆ†ç±»æ–¹é¢è¡¨ç°å‡ºä¼˜äºå¤æ‚ä¸‰ç»´æ¨ç†æ–¹æ³•çš„æ€§èƒ½ï¼Œæå‡èŒƒå›´åˆ†åˆ«ä¸º20-47%å’Œ2-10%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-97b8c73981863feeeb179b3b248b3356" align="middle">
<img src="https://picx.zhimg.com/v2-de10952de19c806c96c07850f887a62a" align="middle">
<img src="https://picx.zhimg.com/v2-9ba232ac7ed3d1251ae82c72b3670b81" align="middle">
<img src="https://picx.zhimg.com/v2-29a024020e29765c9caa56f3ffe3de4e" align="middle">
<img src="https://picx.zhimg.com/v2-2bca68b65d97a370b3ab92c2fb2209e5" align="middle">
<img src="https://picx.zhimg.com/v2-e943d1fe3641b9507715cfde9379f6a6" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Learnable-Total-Variation-with-Lambda-Mapping-for-Low-Dose-CT-Denoising"><a href="#Learnable-Total-Variation-with-Lambda-Mapping-for-Low-Dose-CT-Denoising" class="headerlink" title="Learnable Total Variation with Lambda Mapping for Low-Dose CT Denoising"></a>Learnable Total Variation with Lambda Mapping for Low-Dose CT Denoising</h2><p><strong>Authors:Yusuf Talha Basak, Mehmet Ozan Unal, Metin Ertas, Isa Yildirim</strong></p>
<p>Although Total Variation (TV) performs well in noise reduction and edge preservation on images, its dependence on the lambda parameter limits its efficiency and makes it difficult to use effectively. In this study, we present a Learnable Total Variation (LTV) framework that couples an unrolled TV solver with a data-driven Lambda Mapping Network (LambdaNet) predicting a per-pixel regularization map. The pipeline is trained end-to-end so that reconstruction and regularization are optimized jointly, yielding spatially adaptive smoothing: strong in homogeneous regions, relaxed near anatomical boundaries. Experiments on the DeepLesion dataset, using a realistic noise model adapted from the LoDoPaB-CT methodology, show consistent gains over classical TV and FBP+U-Net: +2.9 dB PSNR and +6% SSIM on average. LTV provides an interpretable alternative to black-box CNNs and a basis for 3D and data-consistency-driven reconstruction. Our codes are available at: <a target="_blank" rel="noopener" href="https://github.com/itu-biai/deep_tv_for_ldct">https://github.com/itu-biai/deep_tv_for_ldct</a></p>
<blockquote>
<p>å°½ç®¡æ€»å˜å·®ï¼ˆTVï¼‰åœ¨å›¾åƒé™å™ªå’Œè¾¹ç¼˜ä¿æŒæ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†å…¶å¯¹lambdaå‚æ•°çš„ä¾èµ–é™åˆ¶äº†å…¶æ•ˆç‡ï¼Œå¹¶ä¸”éš¾ä»¥æœ‰æ•ˆåœ°ä½¿ç”¨ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯å­¦ä¹ çš„æ€»å˜å·®ï¼ˆLTVï¼‰æ¡†æ¶ï¼Œå®ƒå°†å±•å¼€çš„TVæ±‚è§£å™¨ä¸æ•°æ®é©±åŠ¨çš„Lambdaæ˜ å°„ç½‘ç»œï¼ˆLambdaNetï¼‰ç›¸ç»“åˆï¼Œé¢„æµ‹æ¯ä¸ªåƒç´ çš„æ­£åˆ™åŒ–å›¾ã€‚è¯¥ç®¡é“æ˜¯ç«¯åˆ°ç«¯è¿›è¡Œè®­ç»ƒçš„ï¼Œä½¿é‡å»ºå’Œæ­£åˆ™åŒ–èƒ½å¤Ÿè”åˆä¼˜åŒ–ï¼Œäº§ç”Ÿç©ºé—´è‡ªé€‚åº”å¹³æ»‘ï¼šåœ¨å‡åŒ€åŒºåŸŸä¸­å¼ºçƒˆï¼Œåœ¨è§£å‰–è¾¹ç•Œé™„è¿‘æ”¾æ¾ã€‚åœ¨é‡‡ç”¨æ¥è‡ªLoDoPaB-CTæ–¹æ³•è®ºçš„ç°å®å™ªå£°æ¨¡å‹çš„DeepLesionæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ç»å…¸TVå’ŒFBP+U-Netç›¸æ¯”ï¼ŒLTVå…·æœ‰ä¸€è‡´çš„ä¼˜åŠ¿ï¼Œå¹³å‡PSNRæé«˜2.9 dBï¼ŒSSIMæé«˜6%ã€‚LTVä¸ºé»‘ç®±å·ç§¯ç¥ç»ç½‘ç»œæä¾›äº†ä¸€ç§å¯è§£é‡Šçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶ä¸º3Då’Œæ•°æ®ä¸€è‡´æ€§é©±åŠ¨çš„é‡å»ºæä¾›äº†åŸºç¡€ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/itu-biai/deep_tv_for_ldct">https://github.com/itu-biai/deep_tv_for_ldct</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10500v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å­¦ä¹ æ€§æ€»å˜åˆ†ï¼ˆLTVï¼‰æ¡†æ¶ç»“åˆæ— å·ç§¯æ€»å˜åˆ†è§£ç®—å™¨å’Œæ•°æ®é©±åŠ¨Lambdaæ˜ å°„ç½‘ç»œï¼ˆLambdaNetï¼‰ï¼Œä¼˜åŒ–å›¾åƒé‡å»ºå’Œæ­£åˆ™åŒ–ï¼Œæé«˜å›¾åƒå»å™ªå’Œè¾¹ç¼˜ä¿æŒæ€§èƒ½ã€‚ä¸ç»å…¸æ€»å˜åˆ†å’ŒFBP+U-Netç›¸æ¯”ï¼ŒLTVåœ¨DeepLesionæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶å¹³å‡å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰å’Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰åˆ†åˆ«æé«˜äº†2.9 dBå’Œ6%ã€‚LTVæä¾›äº†ä¸€ä¸ªå¯è§£é‡Šçš„CNNæ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶ä¸º3Då’Œæ•°æ®ä¸€è‡´æ€§é©±åŠ¨çš„é‡å»ºæä¾›äº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LTVæ¡†æ¶ç»“åˆäº†æ— å·ç§¯çš„æ€»å˜åˆ†è§£ç®—å™¨å’ŒLambdaæ˜ å°„ç½‘ç»œï¼ˆLambdaNetï¼‰ï¼Œä»¥ä¼˜åŒ–å›¾åƒé‡å»ºå’Œæ­£åˆ™åŒ–è¿‡ç¨‹ã€‚</li>
<li>LTVé€šè¿‡æ•°æ®é©±åŠ¨çš„æ–¹å¼é¢„æµ‹åƒç´ çº§çš„æ­£åˆ™åŒ–æ˜ å°„ï¼Œæé«˜äº†å›¾åƒå»å™ªå’Œè¾¹ç¼˜ä¿æŒæ€§èƒ½ã€‚</li>
<li>LTVæ¡†æ¶å®ç°äº†ç©ºé—´è‡ªé€‚åº”å¹³æ»‘ï¼Œåœ¨å‡åŒ€åŒºåŸŸå®ç°å¼ºå»å™ªï¼Œåœ¨æ¥è¿‘è§£å‰–è¾¹ç•Œå¤„å®ç°æ”¾æ¾ã€‚</li>
<li>åœ¨DeepLesionæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸ç»å…¸çš„æ€»å˜åˆ†å’ŒFBP+U-Netç›¸æ¯”ï¼ŒLTVæä¾›äº†æ›´å¥½çš„å›¾åƒé‡å»ºç»“æœï¼ŒPSNRå’ŒSSIMåˆ†åˆ«æé«˜äº†2.9 dBå’Œ6%ã€‚</li>
<li>LTVæä¾›äº†ä¸€ä¸ªå¯è§£é‡Šçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆCNNï¼‰çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå…·æœ‰æ›´é«˜çš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>LTVæ¡†æ¶å…·æœ‰æ½œåŠ›åº”ç”¨äº3Då›¾åƒé‡å»ºå’Œæ•°æ®ä¸€è‡´æ€§é©±åŠ¨çš„é‡å»ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10500">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-199c6b958b9ff9ba856b55cc9ca471af" align="middle">
<img src="https://picx.zhimg.com/v2-19add22067efcee2c089168315e548c5" align="middle">
<img src="https://picx.zhimg.com/v2-8bc43818ed3f7dd9d34192c437247cc3" align="middle">
<img src="https://picx.zhimg.com/v2-5c2ca518e0cf5688f69636cce03a71ec" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Utility-of-Pancreas-Surface-Lobularity-as-a-CT-Biomarker-for-Opportunistic-Screening-of-Type-2-Diabetes"><a href="#Utility-of-Pancreas-Surface-Lobularity-as-a-CT-Biomarker-for-Opportunistic-Screening-of-Type-2-Diabetes" class="headerlink" title="Utility of Pancreas Surface Lobularity as a CT Biomarker for Opportunistic Screening of Type 2 Diabetes"></a>Utility of Pancreas Surface Lobularity as a CT Biomarker for Opportunistic Screening of Type 2 Diabetes</h2><p><strong>Authors:Tejas Sudharshan Mathai, Anisa V. Prasad, Xinya Wang, Praveen T. S. Balamuralikrishna, Yan Zhuang, Abhinav Suri, Jianfei Liu, Perry J. Pickhardt, Ronald M. Summers</strong></p>
<p>Type 2 Diabetes Mellitus (T2DM) is a chronic metabolic disease that affects millions of people worldwide. Early detection is crucial as it can alter pancreas function through morphological changes and increased deposition of ectopic fat, eventually leading to organ damage. While studies have shown an association between T2DM and pancreas volume and fat content, the role of increased pancreatic surface lobularity (PSL) in patients with T2DM has not been fully investigated. In this pilot work, we propose a fully automated approach to delineate the pancreas and other abdominal structures, derive CT imaging biomarkers, and opportunistically screen for T2DM. Four deep learning-based models were used to segment the pancreas in an internal dataset of 584 patients (297 males, 437 non-diabetic, age: 45$\pm$15 years). PSL was automatically detected and it was higher for diabetic patients (p&#x3D;0.01) at 4.26 $\pm$ 8.32 compared to 3.19 $\pm$ 3.62 for non-diabetic patients. The PancAP model achieved the highest Dice score of 0.79 $\pm$ 0.17 and lowest ASSD error of 1.94 $\pm$ 2.63 mm (p$&lt;$0.05). For predicting T2DM, a multivariate model trained with CT biomarkers attained 0.90 AUC, 66.7% sensitivity, and 91.9% specificity. Our results suggest that PSL is useful for T2DM screening and could potentially help predict the early onset of T2DM.</p>
<blockquote>
<p>ç³–å°¿ç—…ç±»å‹â…¡å‹ï¼ˆT2DMï¼‰æ˜¯ä¸€ç§æ…¢æ€§ä»£è°¢æ€§ç–¾ç—…ï¼Œå…¨çƒèŒƒå›´å†…å½±å“äº†æ•°ä»¥ç™¾ä¸‡è®¡çš„äººç¾¤ã€‚æ—©æœŸæ£€æµ‹å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºå®ƒèƒ½é€šè¿‡å½¢æ€å˜åŒ–å’Œå¼‚ä½è„‚è‚ªæ²‰ç§¯æ”¹å˜èƒ°è…ºåŠŸèƒ½ï¼Œæœ€ç»ˆå¯¼è‡´å™¨å®˜æŸä¼¤ã€‚è™½ç„¶å·²æœ‰ç ”ç©¶è¡¨æ˜T2DMä¸èƒ°è…ºä½“ç§¯å’Œè„‚è‚ªå«é‡ä¹‹é—´å­˜åœ¨å…³è”ï¼Œä½†T2DMæ‚£è€…èƒ°è…ºè¡¨é¢å°å¶å¢å¤šï¼ˆPSLï¼‰çš„ä½œç”¨å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚åœ¨è¿™é¡¹è¯•ç‚¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨è‡ªåŠ¨çš„æ–¹æ³•ï¼Œç”¨äºæç»˜èƒ°è…ºå’Œå…¶ä»–è…¹éƒ¨ç»“æ„ï¼Œè·å–CTæˆåƒç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œå¹¶æœ‰æœºä¼šè¿›è¡ŒT2DMç­›æŸ¥ã€‚ä½¿ç”¨å››ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ¨¡å‹å¯¹å†…éƒ¨æ•°æ®é›†ï¼ˆåŒ…å«584åæ‚£è€…ï¼Œå…¶ä¸­297åç”·æ€§ï¼Œ437åéç³–å°¿ç—…æ‚£è€…ï¼Œå¹´é¾„45Â±15å²ï¼‰è¿›è¡Œèƒ°è…ºåˆ†æ®µã€‚è‡ªåŠ¨æ£€æµ‹åˆ°PSLï¼Œç³–å°¿ç—…æ‚£è€…çš„PSLè¾ƒé«˜ï¼ˆp&#x3D;0.01ï¼‰ï¼Œä¸º4.26Â±8.32ï¼Œéç³–å°¿ç—…æ‚£è€…ä¸º3.19Â±3.62ã€‚PancAPæ¨¡å‹è·å¾—æœ€é«˜çš„Diceå¾—åˆ†ï¼ˆ0.79Â±0.17ï¼‰å’Œæœ€ä½çš„ASSDè¯¯å·®ï¼ˆ1.94Â±2.63æ¯«ç±³ï¼‰ï¼ˆp&lt;0.05ï¼‰ã€‚ä½¿ç”¨CTæ ‡å¿—ç‰©è®­ç»ƒçš„å¤šå…ƒæ¨¡å‹é¢„æµ‹T2DMè¾¾åˆ°äº†AUC 0.90ã€æ•æ„Ÿæ€§66.7%å’Œç‰¹å¼‚æ€§91.9%ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒPSLå¯¹T2DMç­›æŸ¥æœ‰ç”¨ï¼Œå¯èƒ½æœ‰åŠ©äºé¢„æµ‹T2DMçš„æ—©æœŸå‘ç”Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10484v1">PDF</a> Submitted to IEEE ISBI 2026</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ç« ç ”ç©¶äº†â…¡å‹ç³–å°¿ç—…ä¸èƒ°è…ºè¡¨é¢å¶çŠ¶ç»“æ„ï¼ˆPSLï¼‰ä¹‹é—´çš„å…³ç³»ã€‚é€šè¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹èƒ°è…ºè¿›è¡Œè‡ªåŠ¨åˆ†å‰²ï¼Œå‘ç°ç³–å°¿ç—…æ‚£è€…å…·æœ‰æ›´é«˜çš„èƒ°è…ºè¡¨é¢å¶çŠ¶ç»“æ„å€¼ã€‚è¯¥ç ”ç©¶è¡¨æ˜ï¼ŒPSLå¯èƒ½æ˜¯æ—©æœŸé¢„æµ‹â…¡å‹ç³–å°¿ç—…çš„é‡è¦æŒ‡æ ‡ä¹‹ä¸€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2DMæ˜¯ä¸€ç§å¸¸è§çš„æ…¢æ€§ä»£è°¢æ€§ç–¾ç—…ï¼Œæ—©æœŸæ£€æµ‹è‡³å…³é‡è¦ã€‚</li>
<li>èƒ°è…ºä½“ç§¯å’Œè„‚è‚ªå«é‡ä¸T2DMæœ‰å…³è”ã€‚</li>
<li>èƒ°è…ºè¡¨é¢å¶çŠ¶ç»“æ„ï¼ˆPSLï¼‰åœ¨T2DMæ‚£è€…ä¸­çš„ç ”ç©¶å°šæœªå……åˆ†è°ƒæŸ¥ã€‚</li>
<li>ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è‡ªåŠ¨æ£€æµ‹èƒ°è…ºè¡¨é¢å¶çŠ¶ç»“æ„ï¼Œå‘ç°ç³–å°¿ç—…æ‚£è€…å€¼æ›´é«˜ã€‚</li>
<li>PancAPæ¨¡å‹è¡¨ç°å‡ºæœ€ä½³çš„æ€§èƒ½ï¼Œç”¨äºè‡ªåŠ¨åˆ†å‰²èƒ°è…ºã€‚</li>
<li>åˆ©ç”¨CTå½±åƒç”Ÿç‰©æ ‡å¿—ç‰©é¢„æµ‹T2DMçš„å¤šå…ƒæ¨¡å‹è¡¨ç°å‡ºè‰¯å¥½çš„é¢„æµ‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10484">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8732f3f412c53512adc63b28ea5bb7de" align="middle">
<img src="https://picx.zhimg.com/v2-0e5fd4b2f151a9a90a724d81c93d46cb" align="middle">
<img src="https://picx.zhimg.com/v2-5bce7cfb97c3b6ec8502c228e20a2891" align="middle">
<img src="https://picx.zhimg.com/v2-96d7c7359fae86d214f42bc3981fee2d" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Domain-Adaptation-for-Camera-Specific-Image-Characteristics-using-Shallow-Discriminators"><a href="#Domain-Adaptation-for-Camera-Specific-Image-Characteristics-using-Shallow-Discriminators" class="headerlink" title="Domain Adaptation for Camera-Specific Image Characteristics using Shallow Discriminators"></a>Domain Adaptation for Camera-Specific Image Characteristics using Shallow Discriminators</h2><p><strong>Authors:Maximiliane Gruber, JÃ¼rgen Seiler, AndrÃ© Kaup</strong></p>
<p>Each image acquisition setup leads to its own camera-specific image characteristics degrading the image quality. In learning-based perception algorithms, characteristics occurring during the application phase, but absent in the training data, lead to a domain gap impeding the performance. Previously, pixel-level domain adaptation through unpaired learning of the pristine-to-distorted mapping function has been proposed. In this work, we propose shallow discriminator architectures to address limitations of these approaches. We show that a smaller receptive field size improves learning of unknown image distortions by more accurately reproducing local distortion characteristics at a low network complexity. In a domain adaptation setup for instance segmentation, we achieve mean average precision increases over previous methods of up to 0.15 for individual distortions and up to 0.16 for camera-specific image characteristics in a simplified camera model. In terms of number of parameters, our approach matches the complexity of one state of the art method while reducing complexity by a factor of 20 compared to another, demonstrating superior efficiency without compromising performance.</p>
<blockquote>
<p>æ¯ä¸ªå›¾åƒé‡‡é›†è®¾ç½®éƒ½ä¼šå¯¼è‡´å…¶ç‰¹æœ‰çš„ç›¸æœºç‰¹å®šå›¾åƒç‰¹å¾ï¼Œä»è€Œé™ä½äº†å›¾åƒè´¨é‡ã€‚åœ¨åŸºäºå­¦ä¹ çš„æ„ŸçŸ¥ç®—æ³•ä¸­ï¼Œåº”ç”¨é˜¶æ®µå‡ºç°ä½†åœ¨è®­ç»ƒæ•°æ®ä¸­ä¸å­˜åœ¨çš„ç‰¹å¾ä¼šå¯¼è‡´åŸŸå·®è·ï¼Œä»è€Œå½±å“æ€§èƒ½ã€‚å…ˆå‰å·²ç»æå‡ºäº†é€šè¿‡åŸå§‹åˆ°å¤±çœŸçš„æ˜ å°„å‡½æ•°çš„éé…å¯¹å­¦ä¹ æ¥è¿›è¡Œåƒç´ çº§åŸŸé€‚åº”ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æµ…åˆ¤åˆ«å™¨æ¶æ„æ¥è§£å†³è¿™äº›æ–¹æ³•çš„å±€é™æ€§ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¾ƒå°çš„æ„Ÿå—é‡å°ºå¯¸é€šè¿‡æ›´å‡†ç¡®åœ°å†ç°å±€éƒ¨å¤±çœŸç‰¹å¾ï¼Œåœ¨ä½ç½‘ç»œå¤æ‚åº¦ä¸‹æ”¹è¿›äº†å¯¹æœªçŸ¥å›¾åƒå¤±çœŸçš„å­¦ä¹ ã€‚åœ¨å®ä¾‹åˆ†å‰²çš„åŸŸé€‚åº”è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†å¹³å‡ç²¾åº¦æé«˜ï¼Œä¸ä»¥å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œé’ˆå¯¹å•ä¸ªå¤±çœŸçš„æé«˜å¹…åº¦é«˜è¾¾0.15ï¼Œé’ˆå¯¹ç®€åŒ–ç›¸æœºæ¨¡å‹ä¸­çš„ç›¸æœºç‰¹å®šå›¾åƒç‰¹æ€§çš„æé«˜å¹…åº¦é«˜è¾¾0.16ã€‚å°±å‚æ•°æ•°é‡è€Œè¨€ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸æœ€æ–°æŠ€æœ¯çŠ¶æ€çš„å¤æ‚åº¦ç›¸åŒ¹é…ï¼Œä½†ä¸å¦ä¸€ç§æ–¹æ³•ç›¸æ¯”å‡å°‘äº†20å€çš„å¤æ‚åº¦ï¼Œåœ¨ä¸å½±å“æ€§èƒ½çš„æƒ…å†µä¸‹å±•ç¤ºäº†æ›´é«˜çš„æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10424v1">PDF</a> 5 pages, 7 figures, accepted for International Conference on Visual Communications and Image Processing (VCIP) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºåˆ©ç”¨æµ…åˆ¤åˆ«å™¨æ¶æ„æ¥è§£å†³å›¾åƒé‡‡é›†è®¾ç½®ä¸­å¯¼è‡´çš„é¢†åŸŸå·®å¼‚é—®é¢˜ï¼Œä»¥æé«˜å­¦ä¹ æ„ŸçŸ¥ç®—æ³•çš„æ€§èƒ½ã€‚é€šè¿‡å‡å°æ„Ÿå—é‡å¤§å°ï¼Œæ›´å‡†ç¡®æ¨¡æ‹Ÿå±€éƒ¨å¤±çœŸç‰¹æ€§ï¼Œå¹¶åœ¨ç®€åŒ–ç›¸æœºæ¨¡å‹çš„å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­å®ç°å¹³å‡ç²¾åº¦æå‡ã€‚è¯¥æ–¹æ³•åœ¨å‚æ•°æ•°é‡ä¸ŠåŒ¹é…ä¸€ç§å…ˆè¿›æŠ€æœ¯ï¼Œç›¸è¾ƒäºå¦ä¸€ç§æŠ€æœ¯å¤æ‚åº¦é™ä½20å€ï¼Œå±•ç°å‡ºå“è¶Šçš„æ•ˆç‡ä¸”ä¸å½±å“æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒé‡‡é›†è®¾ç½®å¯¼è‡´ç›¸æœºç‰¹å®šçš„å›¾åƒç‰¹æ€§å½±å“å›¾åƒè´¨é‡ã€‚</li>
<li>å­¦ä¹ æ„ŸçŸ¥ç®—æ³•ä¸­çš„é¢†åŸŸå·®å¼‚é˜»ç¢äº†æ€§èƒ½ã€‚</li>
<li>æµ…åˆ¤åˆ«å™¨æ¶æ„è¢«æå‡ºæ¥è§£å†³å…ˆå‰åƒç´ çº§é¢†åŸŸé€‚åº”æ–¹æ³•çš„é—®é¢˜ã€‚</li>
<li>å‡å°æ„Ÿå—é‡å¤§å°èƒ½æ›´å‡†ç¡®åœ°æ¨¡æ‹Ÿå±€éƒ¨å¤±çœŸç‰¹æ€§ã€‚</li>
<li>åœ¨å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­å®ç°äº†å¹³å‡ç²¾åº¦æå‡ï¼Œè¶…è¿‡ä¹‹å‰çš„æ–¹æ³•æœ€å¤šè¾¾0.15çš„ä¸ªä½“å¤±çœŸå’Œæœ€å¤šè¾¾0.16çš„ç›¸æœºç‰¹å®šå›¾åƒç‰¹æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å‚æ•°æ•°é‡ä¸Šè¡¨ç°å‡ºé«˜æ•ˆæ€§ï¼ŒåŒ¹é…ä¸€ç§å…ˆè¿›æŠ€æœ¯å¹¶ç›¸è¾ƒäºå¦ä¸€ç§é™ä½å¤æ‚åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10424">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e6152115152af6c067393dd95bd48de6" align="middle">
<img src="https://picx.zhimg.com/v2-74cecc2c81b2817df1780eafc47fa297" align="middle">
<img src="https://picx.zhimg.com/v2-652eb5c6be12506fb3380a5e34ddaee6" align="middle">
<img src="https://picx.zhimg.com/v2-13ed5f0db05b77c5a25e23dfd8c4118a" align="middle">
<img src="https://picx.zhimg.com/v2-13885e497bcc50103b3a70ba82e1f60a" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="3DFETUS-Standardizing-Fetal-Facial-Planes-in-3D-Ultrasound"><a href="#3DFETUS-Standardizing-Fetal-Facial-Planes-in-3D-Ultrasound" class="headerlink" title="3DFETUS: Standardizing Fetal Facial Planes in 3D Ultrasound"></a>3DFETUS: Standardizing Fetal Facial Planes in 3D Ultrasound</h2><p><strong>Authors:Alomar Antonia, Rubio Ricardo, Albaiges Gerard, Salort-Benejam Laura, Caminal Julia, Prat Maria, Rueda Carolina, Cortes Berta, Piella Gemma, Sukno Federico</strong></p>
<p>Acquiring standard facial planes during routine fetal ultrasound (US) examinations is often challenging due to fetal movement, variability in orientation, and operator-dependent expertise. These factors contribute to inconsistencies, increased examination time, and potential diagnostic bias.   To address these challenges in the context of facial assessment, we present: 1) GT++, a robust algorithm that estimates standard facial planes from 3D US volumes using annotated anatomical landmarks; and 2) 3DFETUS, a deep learning model that automates and standardizes their localization in 3D fetal US volumes.   We evaluated our methods both qualitatively, through expert clinical review, and quantitatively. The proposed approach achieved a mean translation error of 4.13 mm and a mean rotation error of 7.93 degrees per plane, outperforming other state-of-the-art methods on 3D US volumes. Clinical assessments further confirmed the effectiveness of both GT++ and 3DFETUS, demonstrating statistically significant improvements in plane estimation accuracy.</p>
<blockquote>
<p>åœ¨å¸¸è§„èƒå„¿è¶…å£°æ£€æŸ¥è¿‡ç¨‹ä¸­è·å–æ ‡å‡†é¢éƒ¨å¹³é¢å¸¸å¸¸å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºèƒå„¿ä¼šç§»åŠ¨ï¼Œæ–¹å‘å…·æœ‰å¤šå˜æ€§å’Œå–å†³äºæ“ä½œå‘˜çš„ä¸“é•¿ã€‚è¿™äº›å› ç´ ä¼šå¯¼è‡´ç»“æœä¸ä¸€è‡´æ€§ã€æ£€æŸ¥æ—¶é—´å»¶é•¿ä»¥åŠæ½œåœ¨çš„è¯Šæ–­åè§ã€‚ä¸ºäº†è§£å†³é¢éƒ¨è¯„ä¼°ä¸­é‡åˆ°çš„è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºï¼š1ï¼‰GT++ç®—æ³•ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿä»ä¸‰ç»´è¶…å£°æ•°æ®ä¸­ä¼°è®¡å‡ºæ ‡å‡†é¢éƒ¨å¹³é¢å¹¶åˆ©ç”¨è§£å‰–æ ‡è®°ç‚¹æ³¨é‡Šã€‚è¿™æ˜¯é²æ£’æ€§çš„ç®—æ³•ï¼›2ï¼‰é€šè¿‡æ·±åº¦å­¦ä¹ æŠ€æœ¯è‡ªåŠ¨åŒ–çš„è‡ªåŠ¨åŒ–æŠ€æœ¯å¹¶ç»“åˆæ ‡å‡†è¿›è¡Œå…¶åœ¨ä¸‰ç»´èƒå„¿è¶…å£°ä½“ç§¯ä¸­çš„å®šä½ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º3DFETUSã€‚æˆ‘ä»¬é€šè¿‡ä¸“å®¶ä¸´åºŠå®¡æŸ¥å’Œå®šé‡è¯„ä¼°å¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ã€‚æ‰€æå‡ºçš„æ–¹æ³•è¾¾åˆ°äº†å¹³å‡å¹³ç§»è¯¯å·®ä¸º4.13æ¯«ç±³å’Œå¹³å‡æ—‹è½¬è¯¯å·®ä¸ºæ¯å¹³é¢7.93åº¦çš„æ°´å¹³ï¼Œç›¸è¾ƒäºå…¶ä»–é’ˆå¯¹ä¸‰ç»´è¶…å£°ä½“ç§¯çš„æœ€å…ˆè¿›æ–¹æ³•è¡¨ç°å¾—æ›´ä¸ºä¼˜è¶Šã€‚ä¸´åºŠè¯„ä¼°è¿›ä¸€æ­¥è¯å®äº†GT++å’Œ3DFETUSçš„æœ‰æ•ˆæ€§ï¼Œåœ¨å¹³é¢ä¼°è®¡å‡†ç¡®æ€§æ–¹é¢æ˜¾ç¤ºå‡ºç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—æ”¹å–„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10412v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºä¸‰ç»´èƒå„¿è¶…å£°æˆåƒä¸­çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚èƒå„¿çš„é¢‘ç¹ç§»åŠ¨å’Œæ“ä½œçš„éš¾åº¦ï¼Œæ–‡ç« æå‡ºä¸¤ç§é’ˆå¯¹é¢éƒ¨è¯„ä¼°çš„æ–¹æ³•ï¼šGT++ç®—æ³•å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹3DFETUSã€‚GT++ç®—æ³•é€šè¿‡æ ‡æ³¨è§£å‰–æ ‡å¿—ç‚¹æ¥ä¼°è®¡æ ‡å‡†é¢éƒ¨å¹³é¢ï¼Œè€Œæ·±åº¦å­¦ä¹ æ¨¡å‹åˆ™è‡ªåŠ¨åŒ–å®šä½è¿™äº›å¹³é¢ã€‚ç»è¿‡å®šé‡å’Œå®šæ€§è¯„ä¼°ï¼Œè¯¥æ–¹æ³•çš„å¹³å‡å¹³ç§»è¯¯å·®ä¸º4.13æ¯«ç±³ï¼Œå¹³å‡æ—‹è½¬è¯¯å·®ä¸ºæ¯å¹³é¢7.93åº¦ï¼Œç›¸è¾ƒäºå…¶ä»–æ–¹æ³•åœ¨ä¸‰ç»´è¶…å£°æˆåƒä¸­çš„ä½¿ç”¨å…·æœ‰æ›´é«˜çš„æ€§èƒ½ã€‚ä¸´åºŠè¯„ä¼°è¿›ä¸€æ­¥è¯å®äº†GT++å’Œ3DFETUSåœ¨å¹³é¢ä¼°è®¡å‡†ç¡®æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åœ¨å¸¸è§„èƒå„¿è¶…å£°æ£€æŸ¥ä¸­ï¼Œè·å–æ ‡å‡†é¢éƒ¨å¹³é¢å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºèƒå„¿çš„ç§»åŠ¨å’Œæ“ä½œç»éªŒçš„å·®å¼‚ä¼šå½±å“æˆåƒçš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>æ–‡ç« ä»‹ç»äº†GT++ç®—æ³•å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹3DFETUSï¼Œä»¥æ”¹è¿›è¿™ä¸€è¿‡ç¨‹ã€‚GT++ç®—æ³•åˆ©ç”¨è§£å‰–æ ‡å¿—ç‚¹ä¼°è®¡æ ‡å‡†é¢éƒ¨å¹³é¢ï¼Œè€Œæ·±åº¦å­¦ä¹ æ¨¡å‹åˆ™ç”¨äºè‡ªåŠ¨åŒ–å®šä½è¿™äº›å¹³é¢ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å®šé‡è¯„ä¼°å’Œä¸“å®¶ä¸´åºŠå®¡æŸ¥éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚ç›¸è¾ƒäºå…¶ä»–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•çš„å¹³å‡å¹³ç§»è¯¯å·®å’Œæ—‹è½¬è¯¯å·®è¾ƒå°ã€‚</li>
<li>ä¸´åºŠè¯„ä¼°æ˜¾ç¤ºGT++å’Œ3DFETUSåœ¨å¹³é¢ä¼°è®¡å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—æé«˜ã€‚</li>
<li>è¿™äº›æŠ€æœ¯å¯ä»¥å¸®åŠ©å‡å°‘å› èƒå„¿ç§»åŠ¨å’Œæ“ä½œç»éªŒå·®å¼‚å¯¼è‡´çš„è¯Šæ–­åå·®å’Œä¸ä¸€è‡´æ€§ã€‚</li>
<li>è¿™äº›æŠ€æœ¯çš„å®æ–½å¯èƒ½æœ‰åŠ©äºå‡å°‘æ£€æŸ¥æ—¶é—´å¹¶æé«˜è¯Šæ–­æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10412">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a4ff07e3aaa3b26325367097f7baf068" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MonkeyOCR-v1-5-Technical-Report-Unlocking-Robust-Document-Parsing-for-Complex-Patterns"><a href="#MonkeyOCR-v1-5-Technical-Report-Unlocking-Robust-Document-Parsing-for-Complex-Patterns" class="headerlink" title="MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns"></a>MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns</h2><p><strong>Authors:Jiarui Zhang, Yuliang Liu, Zijun Wu, Guosheng Pang, Zhili Ye, Yupei Zhong, Junteng Ma, Tao Wei, Haiyang Xu, Weikai Chen, Zeen Wang, Qiangjun Ji, Fanxi Zhou, Qi Zhang, Yuanrui Hu, Jiahao Liu, Zhang Li, Ziyang Zhang, Qiang Liu, Xiang Bai</strong></p>
<p>Document parsing is a core task in document intelligence, supporting applications such as information extraction, retrieval-augmented generation, and automated document analysis. However, real-world documents often feature complex layouts with multi-level tables, embedded images or formulas, and cross-page structures, which remain challenging for existing OCR systems. We introduce MonkeyOCR v1.5, a unified vision-language framework that enhances both layout understanding and content recognition through a two-stage parsing pipeline. The first stage employs a large multimodal model to jointly predict document layout and reading order, leveraging visual information to ensure structural and sequential consistency. The second stage performs localized recognition of text, formulas, and tables within detected regions, maintaining high visual fidelity while reducing error propagation. To address complex table structures, we propose a visual consistency-based reinforcement learning scheme that evaluates recognition quality via render-and-compare alignment, improving structural accuracy without manual annotations. Additionally, two specialized modules, Image-Decoupled Table Parsing and Type-Guided Table Merging, are introduced to enable reliable parsing of tables containing embedded images and reconstruction of tables crossing pages or columns. Comprehensive experiments on OmniDocBench v1.5 demonstrate that MonkeyOCR v1.5 achieves state-of-the-art performance, outperforming PPOCR-VL and MinerU 2.5 while showing exceptional robustness in visually complex document scenarios.</p>
<blockquote>
<p>æ–‡æ¡£è§£ææ˜¯æ–‡æ¡£æ™ºèƒ½çš„æ ¸å¿ƒä»»åŠ¡ä¹‹ä¸€ï¼Œæ”¯æŒä¿¡æ¯æå–ã€æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œè‡ªåŠ¨åŒ–æ–‡æ¡£åˆ†æç­‰å¤šç§åº”ç”¨ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„æ–‡æ¡£é€šå¸¸å…·æœ‰å¤æ‚çš„å¸ƒå±€ï¼ŒåŒ…å«å¤šå±‚æ¬¡è¡¨æ ¼ã€åµŒå…¥çš„å›¾åƒæˆ–å…¬å¼ä»¥åŠè·¨é¡µç»“æ„ï¼Œè¿™å¯¹ç°æœ‰OCRç³»ç»Ÿä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æ¨å‡ºäº†MonkeyOCR v1.5ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è§†è§‰è¯­è¨€æ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè§£æç®¡é“å¢å¼ºå¸ƒå±€ç†è§£å’Œå†…å®¹è¯†åˆ«ã€‚ç¬¬ä¸€é˜¶æ®µé‡‡ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è”åˆé¢„æµ‹æ–‡æ¡£å¸ƒå±€å’Œé˜…è¯»é¡ºåºï¼Œåˆ©ç”¨è§†è§‰ä¿¡æ¯ç¡®ä¿ç»“æ„å’Œé¡ºåºä¸€è‡´æ€§ã€‚ç¬¬äºŒé˜¶æ®µå¯¹æ£€æµ‹åˆ°çš„åŒºåŸŸè¿›è¡Œæ–‡æœ¬ã€å…¬å¼å’Œè¡¨æ ¼çš„å±€éƒ¨è¯†åˆ«ï¼Œä¿æŒé«˜è§†è§‰ä¿çœŸåº¦ï¼ŒåŒæ—¶å‡å°‘é”™è¯¯ä¼ æ’­ã€‚é’ˆå¯¹å¤æ‚çš„è¡¨æ ¼ç»“æ„ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè§†è§‰ä¸€è‡´æ€§å¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆï¼Œé€šè¿‡æ¸²æŸ“å’Œæ¯”è¾ƒå¯¹é½æ¥è¯„ä¼°è¯†åˆ«è´¨é‡ï¼Œæé«˜ç»“æ„å‡†ç¡®æ€§ï¼Œæ— éœ€æ‰‹åŠ¨æ³¨é‡Šã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†Image-Decoupled Table Parsingå’ŒType-Guided Table Mergingä¸¤ä¸ªä¸“ç”¨æ¨¡å—ï¼Œèƒ½å¤Ÿå®ç°åŒ…å«åµŒå…¥å›¾åƒçš„è¡¨æ ¼å¯é è§£æä»¥åŠè·¨é¡µæˆ–åˆ—çš„è¡¨æ ¼é‡å»ºã€‚åœ¨OmniDocBench v1.5ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒMonkeyOCR v1.5è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œä¼˜äºPPOCR-VLå’ŒMinerU 2.5ï¼Œåœ¨è§†è§‰å¤æ‚çš„æ–‡æ¡£åœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šçš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10390v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MonkeyOCR v1.5æ˜¯ä¸€æ¬¾ç»Ÿä¸€çš„è§†è§‰è¯­è¨€æ¡†æ¶ï¼Œç”¨äºå¢å¼ºæ–‡æ¡£å¸ƒå±€ç†è§£å’Œå†…å®¹è¯†åˆ«ã€‚å®ƒé€šè¿‡ä¸¤é˜¶æ®µè§£æç®¡é“å®ç°æ–‡æ¡£è§£æï¼Œæé«˜å¤æ‚æ–‡æ¡£åœºæ™¯ä¸‹çš„ç¨³å¥æ€§å’Œæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MonkeyOCR v1.5æ˜¯ä¸€ä¸ªè§†è§‰è¯­è¨€æ¡†æ¶ï¼Œç”¨äºæ–‡æ¡£æ™ºèƒ½ä¸­çš„æ–‡æ¡£è§£æä»»åŠ¡ã€‚</li>
<li>å®ƒæ”¯æŒä¿¡æ¯æå–ã€æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œè‡ªåŠ¨åŒ–æ–‡æ¡£åˆ†æåº”ç”¨ç¨‹åºã€‚</li>
<li>å¼•å…¥äº†ä¸¤é˜¶æ®µè§£æç®¡é“ï¼Œç¬¬ä¸€é˜¶æ®µé¢„æµ‹æ–‡æ¡£å¸ƒå±€å’Œé˜…è¯»é¡ºåºï¼Œç¬¬äºŒé˜¶æ®µè¿›è¡Œå±€éƒ¨æ–‡æœ¬ã€å…¬å¼å’Œè¡¨æ ¼è¯†åˆ«ã€‚</li>
<li>æå‡ºäº†åŸºäºè§†è§‰ä¸€è‡´æ€§çš„å¼ºåŒ–å­¦ä¹ æ–¹æ¡ˆï¼Œé€šè¿‡æ¸²æŸ“å’Œæ¯”è¾ƒå¯¹é½æ¥è¯„ä¼°è¯†åˆ«è´¨é‡ï¼Œæé«˜ç»“æ„å‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥äº†Image-Decoupled Table Parsingå’ŒType-Guided Table Mergingä¸¤ä¸ªä¸“ç”¨æ¨¡å—ï¼Œä»¥å¤„ç†åŒ…å«åµŒå…¥å¼å›¾åƒçš„è¡¨æ ¼è§£æå’Œè·¨é¡µæˆ–åˆ—çš„è¡¨æ ¼é‡å»ºã€‚</li>
<li>åœ¨OmniDocBench v1.5ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMonkeyOCR v1.5è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œä¼˜äºPPOCR-VLå’ŒMinerU 2.5ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-513d8904afc0ca454a756a48b39f2438" align="middle">
<img src="https://picx.zhimg.com/v2-4579dd5e8c5b17f5883fe2b6ea3179af" align="middle">
<img src="https://picx.zhimg.com/v2-4a3f73ceef74737acbcfb9452eb808ec" align="middle">
<img src="https://picx.zhimg.com/v2-813cfa5d10b37774c0834ef05565ada1" align="middle">
<img src="https://picx.zhimg.com/v2-d4447368ba54286b1718e65d18bcae04" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Depth-Consistent-3D-Gaussian-Splatting-via-Physical-Defocus-Modeling-and-Multi-View-Geometric-Supervision"><a href="#Depth-Consistent-3D-Gaussian-Splatting-via-Physical-Defocus-Modeling-and-Multi-View-Geometric-Supervision" class="headerlink" title="Depth-Consistent 3D Gaussian Splatting via Physical Defocus Modeling and Multi-View Geometric Supervision"></a>Depth-Consistent 3D Gaussian Splatting via Physical Defocus Modeling and Multi-View Geometric Supervision</h2><p><strong>Authors:Yu Deng, Baozhu Zhao, Junyan Su, Xiaohan Zhang, Qi Liu</strong></p>
<p>Three-dimensional reconstruction in scenes with extreme depth variations remains challenging due to inconsistent supervisory signals between near-field and far-field regions. Existing methods fail to simultaneously address inaccurate depth estimation in distant areas and structural degradation in close-range regions. This paper proposes a novel computational framework that integrates depth-of-field supervision and multi-view consistency supervision to advance 3D Gaussian Splatting. Our approach comprises two core components: (1) Depth-of-field Supervision employs a scale-recovered monocular depth estimator (e.g., Metric3D) to generate depth priors, leverages defocus convolution to synthesize physically accurate defocused images, and enforces geometric consistency through a novel depth-of-field loss, thereby enhancing depth fidelity in both far-field and near-field regions; (2) Multi-View Consistency Supervision employing LoFTR-based semi-dense feature matching to minimize cross-view geometric errors and enforce depth consistency via least squares optimization of reliable matched points. By unifying defocus physics with multi-view geometric constraints, our method achieves superior depth fidelity, demonstrating a 0.8 dB PSNR improvement over the state-of-the-art method on the Waymo Open Dataset. This framework bridges physical imaging principles and learning-based depth regularization, offering a scalable solution for complex depth stratification in urban environments.</p>
<blockquote>
<p>åœ¨åœºæ™¯æ·±åº¦å˜åŒ–æç«¯çš„ä¸‰ç»´é‡å»ºä¸­ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè¿‘åœºå’Œè¿œåœºåŒºåŸŸä¹‹é—´çš„ç›‘ç£ä¿¡å·ä¸ä¸€è‡´ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½åŒæ—¶è§£å†³è¿œè·ç¦»åŒºåŸŸçš„æ·±åº¦ä¼°è®¡ä¸å‡†ç¡®å’Œè¿‘è·ç¦»åŒºåŸŸçš„ç»“æ„é€€åŒ–é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹è®¡ç®—æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æ™¯æ·±ç›‘ç£å’Œå¤šè§†è§’ä¸€è‡´æ€§ç›‘ç£ï¼Œä»¥æ¨åŠ¨3Dé«˜æ–¯å±•å¸ƒæŠ€æœ¯çš„å‘å±•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šï¼ˆ1ï¼‰æ™¯æ·±ç›‘ç£é‡‡ç”¨å°ºåº¦æ¢å¤çš„å•ç›®æ·±åº¦ä¼°è®¡å™¨ï¼ˆä¾‹å¦‚Metric3Dï¼‰æ¥ç”Ÿæˆæ·±åº¦å…ˆéªŒï¼Œåˆ©ç”¨æ•£ç„¦å·ç§¯åˆæˆç‰©ç†å‡†ç¡®çš„æ•£ç„¦å›¾åƒï¼Œå¹¶é€šè¿‡æ–°å‹çš„æ™¯æ·±æŸå¤±æ¥å¼ºåˆ¶æ‰§è¡Œå‡ ä½•ä¸€è‡´æ€§ï¼Œä»è€Œæé«˜è¿œåœºå’Œè¿‘åœºåŒºåŸŸçš„æ·±åº¦ä¿çœŸåº¦ï¼›ï¼ˆ2ï¼‰å¤šè§†è§’ä¸€è‡´æ€§ç›‘ç£é‡‡ç”¨åŸºäºLoFTRçš„åŠå¯†é›†ç‰¹å¾åŒ¹é…æ¥æœ€å°åŒ–è·¨è§†è§’çš„å‡ ä½•è¯¯å·®ï¼Œå¹¶é€šè¿‡å¯é çš„åŒ¹é…ç‚¹çš„æœ€å°äºŒä¹˜ä¼˜åŒ–æ¥å¼ºåˆ¶æ‰§è¡Œæ·±åº¦ä¸€è‡´æ€§ã€‚é€šè¿‡å°†æ•£ç„¦ç‰©ç†ä¸å¤šè§†è§’å‡ ä½•çº¦æŸç›¸ç»“åˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å“è¶Šçš„æ·±åº¦ä¿çœŸåº¦ï¼Œåœ¨Waymo Open Datasetä¸Šæ¯”æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº†0.8dBçš„PSNRã€‚è¯¥æ¡†æ¶æ¶èµ·äº†ç‰©ç†æˆåƒåŸç†å’ŒåŸºäºå­¦ä¹ çš„æ·±åº¦æ­£åˆ™åŒ–ä¹‹é—´çš„æ¡¥æ¢ï¼Œä¸ºåŸå¸‚ç¯å¢ƒä¸­å¤æ‚çš„æ·±åº¦åˆ†å±‚æä¾›äº†å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10316v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆæ™¯æ·±ç›‘ç£å’Œå¤šè§†è§’ä¸€è‡´æ€§ç›‘ç£çš„æ–°å‹è®¡ç®—æ¡†æ¶ï¼Œç”¨äºæ”¹è¿›3Dé«˜æ–¯å–·ç»˜æŠ€æœ¯åœ¨æç«¯æ·±åº¦å˜åŒ–åœºæ™¯ä¸­çš„åº”ç”¨ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šæ™¯æ·±ç›‘ç£å’Œå¤šè§†è§’ä¸€è‡´æ€§ç›‘ç£ï¼Œèƒ½å¤Ÿæé«˜è¿œè¿‘åœºæ™¯çš„æ·±åº¦ä¼°è®¡å‡†ç¡®æ€§ï¼Œå¹¶å¢å¼ºç»“æ„é€€åŒ–åŒºåŸŸçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æç«¯æ·±åº¦å˜åŒ–åœºæ™¯ä¸­çš„ä¸‰ç»´é‡å»ºå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè¿‘åœºå’Œè¿œåœºåŒºåŸŸä¹‹é—´çš„ç›‘ç£ä¿¡å·ä¸ä¸€è‡´ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸èƒ½åŒæ—¶è§£å†³è¿œè·ç¦»æ·±åº¦ä¼°è®¡ä¸å‡†ç¡®å’Œè¿‘ç¨‹åŒºåŸŸç»“æ„é€€åŒ–çš„é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹è®¡ç®—æ¡†æ¶ï¼Œé›†æˆäº†æ™¯æ·±ç›‘ç£å’Œå¤šè§†è§’ä¸€è‡´æ€§ç›‘ç£ï¼Œä»¥æ”¹è¿›3Dé«˜æ–¯å–·ç»˜æŠ€æœ¯ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šæ™¯æ·±ç›‘ç£é€šè¿‡å°ºåº¦æ¢å¤çš„å•ç›®æ·±åº¦ä¼°è®¡å™¨ç”Ÿæˆæ·±åº¦å…ˆéªŒï¼Œåˆ©ç”¨æ•£ç„¦å·ç§¯åˆæˆç‰©ç†å‡†ç¡®çš„æ•£ç„¦å›¾åƒï¼Œå¹¶é€šè¿‡æ–°çš„æ™¯æ·±æŸå¤±å¼ºåˆ¶æ‰§è¡Œå‡ ä½•ä¸€è‡´æ€§ã€‚</li>
<li>å¤šè§†è§’ä¸€è‡´æ€§ç›‘ç£é‡‡ç”¨LoFTRçš„åŠå¯†é›†ç‰¹å¾åŒ¹é…æ¥æœ€å°åŒ–è·¨è§†å›¾çš„å‡ ä½•è¯¯å·®ï¼Œå¹¶é€šè¿‡å¯é çš„åŒ¹é…ç‚¹è¿›è¡Œæœ€å°äºŒä¹˜ä¼˜åŒ–æ¥å¼ºåˆ¶æ‰§è¡Œæ·±åº¦ä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å°†æ•£ç„¦ç‰©ç†ä¸å¤šè§†è§’å‡ ä½•çº¦æŸç›¸ç»“åˆï¼Œå®ç°äº†å“è¶Šçš„æ·±åº¦ä¿çœŸåº¦ï¼Œåœ¨Waymo Openæ•°æ®é›†ä¸Šæ¯”ç°æœ‰æŠ€æœ¯æé«˜äº†0.8 dBçš„PSNRã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10316">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4131229af3e8fd91c594da2779f1fb7" align="middle">
<img src="https://picx.zhimg.com/v2-a51511c94b54cfce3bd2aea83074dd0a" align="middle">
<img src="https://picx.zhimg.com/v2-926ef861d98cfcd1048d49a85f4c703d" align="middle">
<img src="https://picx.zhimg.com/v2-26addc5abba58c0dba6a70653e120f8e" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Revisiting-Evaluation-of-Deep-Neural-Networks-for-Pedestrian-Detection"><a href="#Revisiting-Evaluation-of-Deep-Neural-Networks-for-Pedestrian-Detection" class="headerlink" title="Revisiting Evaluation of Deep Neural Networks for Pedestrian Detection"></a>Revisiting Evaluation of Deep Neural Networks for Pedestrian Detection</h2><p><strong>Authors:Patrick Feifel, Benedikt Franke, Frank Bonarens, Frank KÃ¶ster, Arne Raulf, Friedhelm Schwenker</strong></p>
<p>Reliable pedestrian detection represents a crucial step towards automated driving systems. However, the current performance benchmarks exhibit weaknesses. The currently applied metrics for various subsets of a validation dataset prohibit a realistic performance evaluation of a DNN for pedestrian detection. As image segmentation supplies fine-grained information about a street scene, it can serve as a starting point to automatically distinguish between different types of errors during the evaluation of a pedestrian detector. In this work, eight different error categories for pedestrian detection are proposed and new metrics are proposed for performance comparison along these error categories. We use the new metrics to compare various backbones for a simplified version of the APD, and show a more fine-grained and robust way to compare models with each other especially in terms of safety-critical performance. We achieve SOTA on CityPersons-reasonable (without extra training data) by using a rather simple architecture.</p>
<blockquote>
<p>åœ¨è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­ï¼Œå¯é çš„è¡Œäººæ£€æµ‹æ˜¯è‡³å…³é‡è¦çš„ä¸€æ­¥ã€‚ç„¶è€Œï¼Œå½“å‰æ€§èƒ½åŸºå‡†å±•ç°å‡ºäº†å¼±ç‚¹ã€‚å¯¹äºéªŒè¯æ•°æ®é›†ä¸­çš„ä¸åŒå­é›†ç›®å‰æ‰€åº”ç”¨çš„è¯„ä»·æŒ‡æ ‡ï¼Œæ— æ³•çœŸå®è¯„ä¼°æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨è¡Œäººæ£€æµ‹æ–¹é¢çš„æ€§èƒ½ã€‚å›¾åƒåˆ†å‰²ä¸ºè¡—é“åœºæ™¯æä¾›äº†ç²¾ç»†ä¿¡æ¯ï¼Œå¯ä»¥ä½œä¸ºè‡ªåŠ¨åŒºåˆ†è¡Œäººæ£€æµ‹ä¸­ä¸åŒç±»å‹é”™è¯¯çš„èµ·ç‚¹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæå‡ºäº†8ç§ä¸åŒçš„è¡Œäººæ£€æµ‹é”™è¯¯ç±»åˆ«ï¼Œå¹¶é’ˆå¯¹è¿™äº›é”™è¯¯ç±»åˆ«æå‡ºäº†æ–°çš„æ€§èƒ½æ¯”è¾ƒæŒ‡æ ‡ã€‚æˆ‘ä»¬ä½¿ç”¨æ–°æŒ‡æ ‡æ¥æ¯”è¾ƒAPDç®€åŒ–ç‰ˆçš„å„ç§éª¨å¹²ç½‘ç»œï¼Œå¹¶å±•ç¤ºäº†ä¸€ç§æ›´ç²¾ç»†ä¸”ç¨³å¥çš„æ–¹å¼æ¥å¯¹æ¯”æ¨¡å‹å½¼æ­¤ä¹‹é—´çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨æ€§èƒ½ä¸Šã€‚é€šè¿‡ä½¿ç”¨ç›¸å¯¹ç®€å•çš„æ¶æ„ï¼Œæˆ‘ä»¬åœ¨CityPersons-reasonableï¼ˆæ— éœ€é¢å¤–è®­ç»ƒæ•°æ®ï¼‰ä¸Šå®ç°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10308v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­è¡Œäººæ£€æµ‹çš„å¯é æ€§é—®é¢˜ã€‚ç°æœ‰è¯„ä¼°æŒ‡æ ‡æ— æ³•çœŸå®åæ˜ æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨è¡Œäººæ£€æµ‹æ–¹é¢çš„æ€§èƒ½ï¼Œå› æ­¤æå‡ºå…«ç§è¡Œäººæ£€æµ‹é”™è¯¯ç±»å‹åŠç›¸åº”çš„æ–°è¯„ä¼°æŒ‡æ ‡ã€‚é€šè¿‡æ–°æŒ‡æ ‡å¯¹ä¸åŒéª¨å¹²ç½‘ç»œè¿›è¡Œæ¯”è¾ƒï¼Œå±•ç¤ºäº†ä¸€ç§æ›´ç²¾ç»†ä¸”ç¨³å¥çš„æ¨¡å‹å¯¹æ¯”æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨æ€§èƒ½æ–¹é¢çš„å¯¹æ¯”ã€‚é‡‡ç”¨ç›¸å¯¹ç®€å•çš„æ¶æ„åœ¨åŸå¸‚äººç‰©æ•°æ®é›†ä¸Šè¾¾åˆ°äº†å…ˆè¿›æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­çš„è¡Œäººæ£€æµ‹æ˜¯é‡è¦æ­¥éª¤ï¼Œä½†ç°æœ‰æ€§èƒ½è¯„ä¼°æŒ‡æ ‡å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å›¾åƒåˆ†å‰²æŠ€æœ¯ä¸ºè¡—é“åœºæ™¯æä¾›äº†ç²¾ç»†ä¿¡æ¯ï¼Œæœ‰åŠ©äºè‡ªåŠ¨åŒºåˆ†ä¸åŒç±»å‹çš„é”™è¯¯ã€‚</li>
<li>æå‡ºäº†å…«ç§è¡Œäººæ£€æµ‹çš„è¯¯å·®ç±»åˆ«åŠç›¸åº”çš„æ–°è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºæ›´ç²¾ç»†çš„æ€§èƒ½æ¯”è¾ƒã€‚</li>
<li>é€šè¿‡æ–°æŒ‡æ ‡æ¯”è¾ƒäº†ä¸åŒéª¨å¹²ç½‘ç»œåœ¨è¡Œäººæ£€æµ‹æ¨¡å‹ä¸­çš„è¡¨ç°ã€‚</li>
<li>å±•ç¤ºäº†ä¸€ç§ç®€å•æ¶æ„åœ¨åŸå¸‚äººç‰©æ•°æ®é›†ä¸Šçš„å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>æ–°æ–¹æ³•æä¾›äº†æ›´ç²¾ç»†ä¸”ç¨³å¥çš„æ¨¡å‹å¯¹æ¯”æ–¹å¼ï¼Œç‰¹åˆ«æ˜¯åœ¨å®‰å…¨æ€§èƒ½æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10308">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fc178c55aee1fd82cbf5e215b74b9717" align="middle">
<img src="https://picx.zhimg.com/v2-38d80d7e9a2c323774ffa1d327b6628b" align="middle">
<img src="https://picx.zhimg.com/v2-03ee34da1a12a796b491bc3264ab2a81" align="middle">
<img src="https://picx.zhimg.com/v2-c64c9b9d788b122347e64b402245a423" align="middle">
<img src="https://picx.zhimg.com/v2-8ebfd0191a60b962eaf2277442f00198" align="middle">
<img src="https://picx.zhimg.com/v2-6b1e7d60062de833e547d0ddcdac04c3" align="middle">
<img src="https://picx.zhimg.com/v2-6198dd37cbcdb5c8549e62dbc7e446dc" align="middle">
<img src="https://picx.zhimg.com/v2-06cb5cd5b4e6d31ffe9bec80dcc14f76" align="middle">
<img src="https://picx.zhimg.com/v2-caace1b8daf72e5dbbc5bf45f9d250e7" align="middle">
<img src="https://picx.zhimg.com/v2-45b632d527b0d95060fa8f449acdb0fb" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Out-of-Context-Misinformation-Detection-via-Variational-Domain-Invariant-Learning-with-Test-Time-Training"><a href="#Out-of-Context-Misinformation-Detection-via-Variational-Domain-Invariant-Learning-with-Test-Time-Training" class="headerlink" title="Out-of-Context Misinformation Detection via Variational Domain-Invariant Learning with Test-Time Training"></a>Out-of-Context Misinformation Detection via Variational Domain-Invariant Learning with Test-Time Training</h2><p><strong>Authors:Xi Yang, Han Zhang, Zhijian Lin, Yibiao Hu, Hong Han</strong></p>
<p>Out-of-context misinformation (OOC) is a low-cost form of misinformation in news reports, which refers to place authentic images into out-of-context or fabricated image-text pairings. This problem has attracted significant attention from researchers in recent years. Current methods focus on assessing image-text consistency or generating explanations. However, these approaches assume that the training and test data are drawn from the same distribution. When encountering novel news domains, models tend to perform poorly due to the lack of prior knowledge. To address this challenge, we propose \textbf{VDT} to enhance the domain adaptation capability for OOC misinformation detection by learning domain-invariant features and test-time training mechanisms. Domain-Invariant Variational Align module is employed to jointly encodes source and target domain data to learn a separable distributional space domain-invariant features. For preserving semantic integrity, we utilize domain consistency constraint module to reconstruct the source and target domain latent distribution. During testing phase, we adopt the test-time training strategy and confidence-variance filtering module to dynamically updating the VAE encoder and classifier, facilitating the modelâ€™s adaptation to the target domain distribution. Extensive experiments conducted on the benchmark dataset NewsCLIPpings demonstrate that our method outperforms state-of-the-art baselines under most domain adaptation settings.</p>
<blockquote>
<p>è„±ç¦»ä¸Šä¸‹æ–‡çš„ä¿¡æ¯è¯¯å¯¼ï¼ˆOOCï¼‰æ˜¯æ–°é—»æŠ¥é“ä¸­ä½æˆæœ¬çš„ä¿¡æ¯è¯¯å¯¼å½¢å¼ï¼Œå®ƒå°†çœŸå®çš„å›¾ç‰‡æ”¾å…¥è„±ç¦»ä¸Šä¸‹æ–‡çš„æˆ–è™šæ„çš„å›¾åƒæ–‡æœ¬é…å¯¹ä¸­ã€‚è¿™ä¸ªé—®é¢˜è¿‘å¹´æ¥å¼•èµ·äº†ç ”ç©¶äººå‘˜çš„å¹¿æ³›å…³æ³¨ã€‚å½“å‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è¯„ä¼°å›¾åƒæ–‡æœ¬çš„ä¸€è‡´æ€§æˆ–ç”Ÿæˆè§£é‡Šã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å‡è®¾è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®æ¥è‡ªåŒä¸€åˆ†å¸ƒã€‚å½“é‡åˆ°æ–°çš„æ–°é—»é¢†åŸŸæ—¶ï¼Œç”±äºç¼ºä¹å…ˆéªŒçŸ¥è¯†ï¼Œæ¨¡å‹å¾€å¾€è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡º<strong>VDT</strong>æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ åŸŸä¸å˜ç‰¹å¾å’Œæµ‹è¯•æ—¶é—´è®­ç»ƒæœºåˆ¶ï¼Œå¢å¼ºOOCä¿¡æ¯è¯¯å¯¼æ£€æµ‹çš„åŸŸé€‚åº”èƒ½åŠ›ã€‚é‡‡ç”¨åŸŸä¸å˜å˜åˆ†å¯¹é½æ¨¡å—ï¼Œå¯¹æºåŸŸå’Œç›®æ ‡åŸŸæ•°æ®è¿›è¡Œè”åˆç¼–ç ï¼Œå­¦ä¹ å¯åˆ†ç¦»çš„åˆ†å¸ƒç©ºé—´åŸŸä¸å˜ç‰¹å¾ã€‚ä¸ºäº†ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼Œæˆ‘ä»¬åˆ©ç”¨åŸŸä¸€è‡´æ€§çº¦æŸæ¨¡å—é‡å»ºæºåŸŸå’Œç›®æ ‡åŸŸçš„æ½œåœ¨åˆ†å¸ƒã€‚åœ¨æµ‹è¯•é˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨æµ‹è¯•æ—¶é—´è®­ç»ƒç­–ç•¥å’Œä¿¡å¿ƒæ–¹å·®è¿‡æ»¤æ¨¡å—ï¼ŒåŠ¨æ€æ›´æ–°VAEç¼–ç å™¨å’Œåˆ†ç±»å™¨ï¼Œä¿ƒè¿›æ¨¡å‹å¯¹ç›®æ ‡åŸŸåˆ†å¸ƒçš„é€‚åº”ã€‚åœ¨NewsCLIPpingsåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤§å¤šæ•°åŸŸé€‚åº”è®¾ç½®ä¸‹ä¼˜äºæœ€æ–°åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10213v1">PDF</a> accepted by the AAAI Conference on Artificial Intelligence (AAAI) 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å‡ºå›¾è¯­å¢ƒä¿¡æ¯é”™è¯¯ï¼ˆOOCï¼‰çš„é—®é¢˜ï¼Œä¸€ç§æ–°é—»æŠ¥å‘Šä¸­çš„ä½æˆæœ¬é”™è¯¯ä¿¡æ¯ã€‚è¯¥é—®é¢˜å·²å¼•èµ·ç ”ç©¶è€…å…³æ³¨ã€‚å½“å‰æ–¹æ³•ä¸»è¦è¯„ä¼°å›¾åƒå’Œæ–‡å­—çš„ä¸€è‡´æ€§æˆ–ç”Ÿæˆè§£é‡Šï¼Œä½†å‡è®¾è®­ç»ƒå’Œæµ‹è¯•æ•°æ®æ¥è‡ªåŒä¸€åˆ†å¸ƒã€‚é¢å¯¹æ–°é¢†åŸŸæ–°é—»æ—¶ï¼Œç”±äºç¼ºä¹å…ˆéªŒçŸ¥è¯†ï¼Œæ¨¡å‹è¡¨ç°ä¸ä½³ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºé€šè¿‡å­¦ä¹ åŸŸä¸å˜ç‰¹å¾å’Œæµ‹è¯•æ—¶é—´è®­ç»ƒæœºåˆ¶æ¥å¢å¼ºæ¨¡å‹å¯¹OOCè¯¯ä¿¡æ¯çš„åŸŸé€‚åº”èƒ½åŠ›ã€‚é€šè¿‡é‡‡ç”¨åŸŸä¸å˜å˜åˆ†å¯¹é½æ¨¡å—å’ŒåŸŸä¸€è‡´æ€§çº¦æŸæ¨¡å—ï¼Œè¯¥æ–¹æ³•å¯ä»¥è”åˆç¼–ç æºå’Œç›®æ ‡åŸŸæ•°æ®ï¼Œå­¦ä¹ å¯åˆ†ç¦»çš„åŸŸä¸å˜ç‰¹å¾ç©ºé—´å¹¶ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ã€‚æµ‹è¯•é˜¶æ®µé‡‡ç”¨æµ‹è¯•æ—¶é—´è®­ç»ƒç­–ç•¥å’Œä¿¡å¿ƒæ–¹å·®è¿‡æ»¤æ¨¡å—ï¼Œå¯åŠ¨æ€æ›´æ–°VAEç¼–ç å™¨å’Œåˆ†ç±»å™¨ï¼Œä¿ƒè¿›æ¨¡å‹é€‚åº”ç›®æ ‡åŸŸåˆ†å¸ƒã€‚åœ¨NewsCLIPpingsåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§å¤šæ•°åŸŸé€‚åº”è®¾ç½®ä¸‹ä¼˜äºæœ€æ–°åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OOCï¼ˆå‡ºå›¾è¯­å¢ƒä¿¡æ¯é”™è¯¯ï¼‰æ˜¯æ–°é—»æŠ¥å‘Šä¸­ä¸€ç§ä½æˆæœ¬çš„ä¿¡æ¯é”™è¯¯å½¢å¼ï¼Œæ¶‰åŠå°†çœŸå®å›¾åƒæ”¾å…¥ä¸åˆé€‚çš„ä¸Šä¸‹æ–‡æˆ–ä¼ªé€ å›¾åƒæ–‡æœ¬é…å¯¹ä¸­ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦è¯„ä¼°å›¾åƒå’Œæ–‡æœ¬çš„ä¸€è‡´æ€§æˆ–ç”Ÿæˆè§£é‡Šï¼Œä½†å®ƒä»¬åœ¨é¢å¯¹æ–°é¢†åŸŸæ–°é—»æ—¶è¡¨ç°ä¸ä½³ï¼Œå› ä¸ºç¼ºä¹å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†VDTæ–¹æ³•ï¼Œé€šè¿‡è”åˆç¼–ç æºå’Œç›®æ ‡åŸŸæ•°æ®æ¥å¢å¼ºåŸŸé€‚åº”èƒ½åŠ›ï¼Œå­¦ä¹ å¯åˆ†ç¦»çš„åŸŸä¸å˜ç‰¹å¾ç©ºé—´ã€‚</li>
<li>é‡‡ç”¨åŸŸä¸å˜å˜åˆ†å¯¹é½æ¨¡å—å’ŒåŸŸä¸€è‡´æ€§çº¦æŸæ¨¡å—æ¥ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ã€‚</li>
<li>æµ‹è¯•é˜¶æ®µé‡‡ç”¨æµ‹è¯•æ—¶é—´è®­ç»ƒç­–ç•¥å’Œä¿¡å¿ƒæ–¹å·®è¿‡æ»¤æ¨¡å—ï¼Œä¿ƒè¿›æ¨¡å‹é€‚åº”ç›®æ ‡åŸŸåˆ†å¸ƒã€‚</li>
<li>åœ¨NewsCLIPpingsæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVDTæ–¹æ³•åœ¨å¤§å¤šæ•°åŸŸé€‚åº”è®¾ç½®ä¸‹ä¼˜äºå…¶ä»–æœ€æ–°æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10213">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-17e48cfb7f9e5d55e7ada206cb2a0a87" align="middle">
<img src="https://picx.zhimg.com/v2-2694b2675dbe8f297578c3610edbc387" align="middle">
<img src="https://picx.zhimg.com/v2-cf00cf59e216c8132f9c5359c3a73d37" align="middle">
<img src="https://picx.zhimg.com/v2-90e983bd1474949d6c1d593042e3a301" align="middle">
<img src="https://picx.zhimg.com/v2-cc84e5ba952c320edcb2be7c43f9a557" align="middle">
<img src="https://picx.zhimg.com/v2-eaa9432953dc4fd3cfeb947ad773b7a0" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CephRes-MHNet-A-Multi-Head-Residual-Network-for-Accurate-and-Robust-Cephalometric-Landmark-Detection"><a href="#CephRes-MHNet-A-Multi-Head-Residual-Network-for-Accurate-and-Robust-Cephalometric-Landmark-Detection" class="headerlink" title="CephRes-MHNet: A Multi-Head Residual Network for Accurate and Robust Cephalometric Landmark Detection"></a>CephRes-MHNet: A Multi-Head Residual Network for Accurate and Robust Cephalometric Landmark Detection</h2><p><strong>Authors:Ahmed Jaheen, Islam Hassan, Mohanad Abouserie, Abdelaty Rehab, Adham Elasfar, Knzy Elmasry, Mostafa El-Dawlatly, Seif Eldawlatly</strong></p>
<p>Accurate localization of cephalometric landmarks from 2D lateral skull X-rays is vital for orthodontic diagnosis and treatment. Manual annotation is time-consuming and error-prone, whereas automated approaches often struggle with low contrast and anatomical complexity. This paper introduces CephRes-MHNet, a multi-head residual convolutional network for robust and efficient cephalometric landmark detection. The architecture integrates residual encoding, dual-attention mechanisms, and multi-head decoders to enhance contextual reasoning and anatomical precision. Trained on the Aariz Cephalometric dataset of 1,000 radiographs, CephRes-MHNet achieved a mean radial error (MRE) of 1.23 mm and a success detection rate (SDR) @ 2.0 mm of 85.5%, outperforming all evaluated models. In particular, it exceeded the strongest baseline, the attention-driven AFPF-Net (MRE &#x3D; 1.25 mm, SDR @ 2.0 mm &#x3D; 84.1%), while using less than 25% of its parameters. These results demonstrate that CephRes-MHNet attains state-of-the-art accuracy through architectural efficiency, providing a practical solution for real-world orthodontic analysis.</p>
<blockquote>
<p>ä»äºŒç»´ä¾§é¢é¢…éª¨Xå°„çº¿ç‰‡ä¸­å‡†ç¡®å®šä½å¤´å½±æµ‹é‡æ ‡å¿—ç‚¹å¯¹äºæ­£ç•¸è¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚æ‰‹åŠ¨æ ‡æ³¨æ—¢è€—æ—¶åˆå®¹æ˜“å‡ºé”™ï¼Œè€Œè‡ªåŠ¨æ–¹æ³•åˆ™å¸¸å¸¸å› å¯¹æ¯”åº¦ä½å’Œè§£å‰–ç»“æ„å¤æ‚è€Œéš¾ä»¥åº”å¯¹ã€‚æœ¬æ–‡ä»‹ç»äº†CephRes-MHNetï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç¨³å¥é«˜æ•ˆå¤´å½±æµ‹é‡æ ‡å¿—ç‚¹æ£€æµ‹çš„å¤šå¤´æ®‹å·®å·ç§¯ç½‘ç»œã€‚è¯¥æ¶æ„èåˆäº†æ®‹å·®ç¼–ç ã€åŒé‡æ³¨æ„åŠ›æœºåˆ¶å’Œå¤šä¸ªè§£ç å™¨ï¼Œä»¥å¢å¼ºä¸Šä¸‹æ–‡æ¨ç†å’Œè§£å‰–ç²¾åº¦ã€‚åœ¨Aarizå¤´å½±æµ‹é‡æ•°æ®é›†ï¼ˆåŒ…å«1000å¼ æ”¾å°„å›¾åƒï¼‰ä¸Šè¿›è¡Œè®­ç»ƒçš„CephRes-MHNetè¾¾åˆ°äº†å¹³å‡å¾„å‘è¯¯å·®ï¼ˆMREï¼‰ä¸º1.23æ¯«ç±³ï¼Œåœ¨2.0æ¯«ç±³å¤„çš„æˆåŠŸæ£€æµ‹ç‡ï¼ˆSDRï¼‰ä¸º85.5%ï¼Œè¶…è¿‡äº†æ‰€æœ‰è¯„ä¼°çš„æ¨¡å‹ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå®ƒåœ¨å‚æ•°ä½¿ç”¨é‡ä¸åˆ°å››åˆ†ä¹‹ä¸€çš„æƒ…å†µä¸‹ï¼Œè¶…è¶Šäº†è¡¨ç°æœ€å¼ºçš„åŸºçº¿æ¨¡å‹â€”â€”AFPF-Netï¼ˆMRE&#x3D;1.25æ¯«ç±³ï¼ŒSDR @ 2.0æ¯«ç±³&#x3D;84.1%ï¼‰ã€‚è¿™äº›ç»“æœè¯æ˜äº†CephRes-MHNeté€šè¿‡æ¶æ„æ•ˆç‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç²¾ç¡®åº¦ï¼Œä¸ºç°å®ä¸–ç•Œçš„æ­£ç•¸åˆ†ææä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10173v1">PDF</a> 5 Pages, Under Review at The IEEE International Symposium on Biomedical Imaging (ISBI 2026)</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºå¤´é¢…æµ‹é‡æ ‡å¿—ç‚¹æ£€æµ‹çš„ç¨³å¥è€Œé«˜æ•ˆçš„æ–¹æ³•â€”â€”CephRes-MHNetã€‚å®ƒé€šè¿‡é›†æˆæ®‹å·®ç¼–ç ã€åŒé‡æ³¨æ„åŠ›æœºåˆ¶å’Œå¤šä¸ªè§£ç å™¨æ¥æé«˜ä¸Šä¸‹æ–‡æ¨ç†å’Œè§£å‰–ç²¾åº¦ã€‚åœ¨Aarizå¤´é¢…æµ‹é‡æ•°æ®é›†ä¸Šè®­ç»ƒçš„CephRes-MHNetå–å¾—äº†å¹³å‡å¾„å‘è¯¯å·®ï¼ˆMREï¼‰ä¸º1.23æ¯«ç±³å’ŒæˆåŠŸæ£€æµ‹ç‡ï¼ˆSDRï¼‰@ 2æ¯«ç±³ä¸º85.5%çš„ç»“æœï¼Œä¼˜äºæ‰€æœ‰è¯„ä¼°çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬å‚æ•°è¾ƒå°‘çš„æœ€ä½³åŸºçº¿æ¨¡å‹AFPF-Netã€‚è¿™è¡¨æ˜CephRes-MHNeté€šè¿‡ç»“æ„æ•ˆç‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œä¸ºå®é™…çš„æ­£ç•¸åˆ†ææä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CephRes-MHNetæ˜¯ä¸€ç§ç”¨äºå¤´é¢…æµ‹é‡æ ‡å¿—ç‚¹æ£€æµ‹çš„å¤šå¤´æ®‹å·®å·ç§¯ç½‘ç»œï¼Œæ—¨åœ¨å®ç°ç¨³å¥é«˜æ•ˆçš„æ£€æµ‹ã€‚</li>
<li>ç½‘ç»œæ¶æ„èåˆäº†æ®‹å·®ç¼–ç ã€åŒé‡æ³¨æ„åŠ›æœºåˆ¶å’Œå¤šä¸ªè§£ç å™¨ï¼Œå¢å¼ºäº†ä¸Šä¸‹æ–‡æ¨ç†å’Œè§£å‰–ç²¾åº¦ã€‚</li>
<li>CephRes-MHNetåœ¨Aarizå¤´é¢…æµ‹é‡æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œå®ç°äº†å¹³å‡å¾„å‘è¯¯å·®ï¼ˆMREï¼‰ä¸º1.23æ¯«ç±³çš„ä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>ä¸å…¶ä»–æ¨¡å‹ç›¸æ¯”ï¼ŒåŒ…æ‹¬å‚æ•°è¾ƒå°‘çš„æœ€ä½³åŸºçº¿æ¨¡å‹AFPF-Netï¼ŒCephRes-MHNetåœ¨æˆåŠŸæ£€æµ‹ç‡ï¼ˆSDRï¼‰æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>CephRes-MHNetè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œé€šè¿‡ç»“æ„æ•ˆç‡å®ç°äº†é«˜æ•ˆæ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºæ­£ç•¸è¯Šæ–­æä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªåŠ¨åŒ–å¤„ç†å¤´é¢…æµ‹é‡æ ‡å¿—ç‚¹æ£€æµ‹æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10173">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a2c94cd84aa20cf0cacc9bf963cbca42" align="middle">
<img src="https://picx.zhimg.com/v2-684d5eb9a659d65213d028a014b84f4e" align="middle">
<img src="https://picx.zhimg.com/v2-5cd21842740928c36282e4ffe9549e24" align="middle">
<img src="https://picx.zhimg.com/v2-5ee4e0798b0f37cfc755e3f89ca089cd" align="middle">
<img src="https://picx.zhimg.com/v2-15389eea7674a5b853c3393ef8e09dc4" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Split-Layer-Enhancing-Implicit-Neural-Representation-by-Maximizing-the-Dimensionality-of-Feature-Space"><a href="#Split-Layer-Enhancing-Implicit-Neural-Representation-by-Maximizing-the-Dimensionality-of-Feature-Space" class="headerlink" title="Split-Layer: Enhancing Implicit Neural Representation by Maximizing the Dimensionality of Feature Space"></a>Split-Layer: Enhancing Implicit Neural Representation by Maximizing the Dimensionality of Feature Space</h2><p><strong>Authors:Zhicheng Cai, Hao Zhu, Linsen Chen, Qiu Shen, Xun Cao</strong></p>
<p>Implicit neural representation (INR) models signals as continuous functions using neural networks, offering efficient and differentiable optimization for inverse problems across diverse disciplines. However, the representational capacity of INR defined by the range of functions the neural network can characterize, is inherently limited by the low-dimensional feature space in conventional multilayer perceptron (MLP) architectures. While widening the MLP can linearly increase feature space dimensionality, it also leads to a quadratic growth in computational and memory costs. To address this limitation, we propose the split-layer, a novel reformulation of MLP construction. The split-layer divides each layer into multiple parallel branches and integrates their outputs via Hadamard product, effectively constructing a high-degree polynomial space. This approach significantly enhances INRâ€™s representational capacity by expanding the feature space dimensionality without incurring prohibitive computational overhead. Extensive experiments demonstrate that the split-layer substantially improves INR performance, surpassing existing methods across multiple tasks, including 2D image fitting, 2D CT reconstruction, 3D shape representation, and 5D novel view synthesis.</p>
<blockquote>
<p>éšå¼ç¥ç»ç½‘ç»œè¡¨ç¤ºï¼ˆINRï¼‰ä½¿ç”¨ç¥ç»ç½‘ç»œå°†ä¿¡å·è¡¨ç¤ºä¸ºè¿ç»­å‡½æ•°ï¼Œä¸ºå„å­¦ç§‘ä¸­çš„åé—®é¢˜æä¾›äº†é«˜æ•ˆä¸”å¯å¾®åˆ†çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚ç„¶è€Œï¼ŒINRçš„è¡¨ç¤ºèƒ½åŠ›å—é™äºç¥ç»ç½‘ç»œèƒ½å¤Ÿè¡¨å¾çš„å‡½æ•°èŒƒå›´ï¼Œè¿™æœ¬è´¨ä¸Šæ˜¯ç”±ä¼ ç»Ÿå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰æ¶æ„ä¸­çš„ä½ç»´ç‰¹å¾ç©ºé—´æ‰€å¯¼è‡´çš„ã€‚è™½ç„¶æ‹“å®½MLPå¯ä»¥çº¿æ€§å¢åŠ ç‰¹å¾ç©ºé—´çš„ç»´åº¦ï¼Œä½†è¿™ä¹Ÿä¼šå¯¼è‡´è®¡ç®—å’Œå†…å­˜æˆæœ¬çš„äºŒæ¬¡å¢é•¿ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å‰²å±‚ï¼ˆsplit-layerï¼‰çš„æ¦‚å¿µï¼Œè¿™æ˜¯å¯¹MLPæ„å»ºçš„ä¸€ç§æ–°å‹é‡æ–°è¡¨è¿°ã€‚åˆ†å‰²å±‚å°†æ¯ä¸€å±‚åˆ†å‰²æˆå¤šä¸ªå¹¶è¡Œåˆ†æ”¯ï¼Œå¹¶é€šè¿‡å“ˆè¾¾ç›ç§¯ï¼ˆHadamard productï¼‰æ•´åˆå®ƒä»¬çš„è¾“å‡ºï¼Œä»è€Œæœ‰æ•ˆåœ°æ„å»ºäº†ä¸€ä¸ªé«˜æ¬¡å¤šé¡¹å¼ç©ºé—´ã€‚è¿™ç§æ–¹æ³•é€šè¿‡æ‰©å±•ç‰¹å¾ç©ºé—´çš„ç»´åº¦ï¼Œæ˜¾è‘—æé«˜äº†INRçš„è¡¨ç¤ºèƒ½åŠ›ï¼ŒåŒæ—¶ä¸ä¼šäº§ç”Ÿè¿‡é«˜çš„è®¡ç®—å¼€é”€ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåˆ†å‰²å±‚èƒ½æ˜¾è‘—æé«˜INRçš„æ€§èƒ½ï¼Œåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬2Då›¾åƒæ‹Ÿåˆã€2D CTé‡å»ºã€3Då½¢çŠ¶è¡¨ç¤ºå’Œ5Dæ–°é¢–è§†å›¾åˆæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10142v1">PDF</a> AAAI 2026</p>
<p><strong>æ€»ç»“</strong><br>    éšå¼ç¥ç»ç½‘ç»œï¼ˆINRï¼‰å°†ä¿¡å·è§†ä¸ºè¿ç»­å‡½æ•°å¹¶è¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œä¼˜åŒ–ï¼Œä¸ºè§£å†³è·¨å­¦ç§‘çš„é€†é—®é¢˜æä¾›äº†é«˜æ•ˆä¸”å¯åŒºåˆ†çš„æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç”±äºå¸¸è§„å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰æ¶æ„çš„ç‰¹å¾ç©ºé—´ç»´åº¦è¾ƒä½ï¼ŒINRçš„è¡¨å¾èƒ½åŠ›å—åˆ°é™åˆ¶ã€‚ä¸ºæ‰©å¤§ç‰¹å¾ç©ºé—´ç»´åº¦è€Œå¢åŠ MLPå±‚å®½ä¼šå¯¼è‡´è®¡ç®—å’Œå†…å­˜æˆæœ¬äºŒæ¬¡å¢é•¿ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†è£‚å±‚è¿™ä¸€æ–°å‹MLPæ„å»ºæ–¹æ³•ã€‚åˆ†è£‚å±‚å°†æ¯å±‚åˆ†ä¸ºå¤šä¸ªå¹¶è¡Œåˆ†æ”¯ï¼Œé€šè¿‡Hadamardç§¯æ•´åˆè¾“å‡ºï¼Œæ„å»ºå‡ºé«˜æ•ˆçš„é«˜é˜¶å¤šé¡¹å¼ç©ºé—´ã€‚æ­¤æ–¹æ³•åœ¨æ‰©å¤§ç‰¹å¾ç©ºé—´ç»´åº¦çš„åŒæ—¶ï¼Œä¸ä¼šé€ æˆè¿‡å¤§çš„è®¡ç®—è´Ÿæ‹…ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåˆ†è£‚å±‚æŠ€æœ¯å¤§å¹…æå‡äº†INRçš„æ€§èƒ½ï¼Œåœ¨å¤šé¡¹ä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬2Då›¾åƒæ‹Ÿåˆã€2D CTé‡å»ºã€3Då½¢çŠ¶è¡¨ç¤ºå’Œ5Dæ–°é¢–è§†å›¾åˆæˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>éšå¼ç¥ç»ç½‘ç»œï¼ˆINRï¼‰åˆ©ç”¨ç¥ç»ç½‘ç»œå°†ä¿¡å·è§†ä¸ºè¿ç»­å‡½æ•°ï¼Œé€‚ç”¨äºå¤šç§å­¦ç§‘çš„é€†é—®é¢˜ã€‚</li>
<li>INRçš„è¡¨å¾èƒ½åŠ›å—é™äºå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰çš„ç‰¹å¾ç©ºé—´ç»´åº¦ã€‚</li>
<li>å•çº¯å¢åŠ MLPå±‚å®½è™½èƒ½æå‡ç‰¹å¾ç©ºé—´ç»´åº¦ï¼Œä½†ä¼šå¯¼è‡´è®¡ç®—å’Œå†…å­˜æˆæœ¬æ˜¾è‘—å¢åŠ ã€‚</li>
<li>æå‡ºçš„åˆ†è£‚å±‚æ–¹æ³•é€šè¿‡å¹¶è¡Œåˆ†æ”¯å’ŒHadamardç§¯æœ‰æ•ˆæ‰©å¤§äº†ç‰¹å¾ç©ºé—´ï¼Œæå‡äº†INRçš„è¡¨å¾èƒ½åŠ›ã€‚</li>
<li>åˆ†è£‚å±‚æ–¹æ³•åœ¨å¤šé¡¹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒæ‹Ÿåˆã€CTé‡å»ºã€å½¢çŠ¶è¡¨ç¤ºå’Œè§†å›¾åˆæˆã€‚</li>
<li>åˆ†è£‚å±‚æ–¹æ³•åœ¨ä¿è¯è®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œæœ‰æ•ˆæå‡äº†INRçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10142">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c0054aa38eb22b663ffd8cf99ad20ab" align="middle">
<img src="https://picx.zhimg.com/v2-414b74769db81172df2fc2fb1f01a561" align="middle">
<img src="https://picx.zhimg.com/v2-a44f2c210c294146666e601bd0e76410" align="middle">
<img src="https://picx.zhimg.com/v2-5d737b1c886777854c72742aaa3b27b1" align="middle">
<img src="https://picx.zhimg.com/v2-3c19d63452fcea838ecbaa53e7fe3bab" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Radiology-Workflow-Guided-Hierarchical-Reinforcement-Fine-Tuning-for-Medical-Report-Generation"><a href="#Radiology-Workflow-Guided-Hierarchical-Reinforcement-Fine-Tuning-for-Medical-Report-Generation" class="headerlink" title="Radiology Workflow-Guided Hierarchical Reinforcement Fine-Tuning for Medical Report Generation"></a>Radiology Workflow-Guided Hierarchical Reinforcement Fine-Tuning for Medical Report Generation</h2><p><strong>Authors:Bodong Du, Honglong Yang, Xiaomeng Li</strong></p>
<p>Radiologists compose diagnostic reports through a structured workflow: they describe visual findings, summarize them into impressions, and carefully refine statements in clinically critical cases. However, most existing medical report generation (MRG) systems treat reports as flat sequences, overlooking this hierarchical organization and leading to inconsistencies between descriptive and diagnostic content. To align model behavior with real-world reporting practices, we propose RadFlow, a hierarchical workflow-guided reinforcement optimization framework that explicitly models the structured nature of clinical reporting. RadFlow introduces a clinically grounded reward hierarchy that mirrors the organization of radiological reports. At the global level, the reward integrates linguistic fluency, medical-domain correctness, and cross-sectional consistency between Finding and Impression, promoting coherent and clinically faithful narratives. At the local level, a section-specific reward emphasizes Impression quality, reflecting its central role in diagnostic accuracy. Furthermore, a critical-aware policy optimization mechanism adaptively regularizes learning for high-risk or clinically sensitive cases, emulating the cautious refinement behavior of radiologists when documenting critical findings. Together, these components translate the structured reporting paradigm into the reinforcement fine-tuning process, enabling the model to generate reports that are both linguistically consistent and clinically aligned. Experiments on chest X-ray and carotid ultrasound datasets demonstrate that RadFlow consistently improves diagnostic coherence and overall report quality compared with state-of-the-art baselines.</p>
<blockquote>
<p>æ”¾å°„ç§‘åŒ»ç”Ÿé€šè¿‡ç»“æ„åŒ–å·¥ä½œæµç¨‹æ’°å†™è¯Šæ–­æŠ¥å‘Šï¼šä»–ä»¬æè¿°æ‰€è§ï¼Œå°†å…¶æ€»ç»“ä¸ºå°è±¡ï¼Œå¹¶åœ¨ä¸´åºŠå…³é”®ç—…ä¾‹ä¸­ä»”ç»†ç²¾ç‚¼é™ˆè¿°ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„åŒ»å­¦æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿï¼ˆMRGï¼‰å°†æŠ¥å‘Šè§†ä¸ºå¹³å¦çš„åºåˆ—ï¼Œå¿½ç•¥äº†è¿™ç§åˆ†å±‚ç»„ç»‡ï¼Œå¯¼è‡´æè¿°æ€§å’Œè¯Šæ–­å†…å®¹ä¹‹é—´å‡ºç°ä¸ä¸€è‡´ã€‚ä¸ºäº†å°†æ¨¡å‹è¡Œä¸ºä¸çœŸå®ä¸–ç•Œçš„æŠ¥å‘Šå®è·µä¿æŒä¸€è‡´ï¼Œæˆ‘ä»¬æå‡ºäº†RadFlowï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ†å±‚çš„å·¥ä½œæµé©±åŠ¨å¼ºåŒ–ä¼˜åŒ–æ¡†æ¶ï¼Œå®ƒæ˜¾å¼åœ°æ¨¡æ‹Ÿä¸´åºŠæŠ¥å‘Šçš„ç»“æ„åŒ–ç‰¹æ€§ã€‚RadFlowå¼•å…¥äº†ä¸€ä¸ªåŸºäºä¸´åºŠçš„å¥–åŠ±å±‚æ¬¡ç»“æ„ï¼Œè¯¥ç»“æ„åæ˜ äº†æ”¾å°„æŠ¥å‘Šçš„ç»„ç»‡æ–¹å¼ã€‚å…¨å±€å±‚é¢ä¸Šï¼Œå¥–åŠ±èåˆäº†è¯­è¨€æµç•…æ€§ã€åŒ»å­¦é¢†åŸŸæ­£ç¡®æ€§ï¼Œä»¥åŠå‘ç°ä¸å°è±¡ä¹‹é—´çš„æ¨ªæˆªé¢ä¸€è‡´æ€§ï¼Œä¿ƒè¿›è¿è´¯ä¸”ç¬¦åˆä¸´åºŠå®é™…çš„æ•…äº‹å™è¿°ã€‚å±€éƒ¨å±‚é¢ä¸Šï¼Œéƒ¨åˆ†ç‰¹å®šçš„å¥–åŠ±å¼ºè°ƒå°è±¡è´¨é‡ï¼Œåæ˜ äº†å…¶åœ¨è¯Šæ–­å‡†ç¡®æ€§ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚æ­¤å¤–ï¼Œä¸€ç§å…³é”®æ„è¯†æ”¿ç­–ä¼˜åŒ–æœºåˆ¶è‡ªé€‚åº”åœ°è°ƒæ•´é«˜é£é™©æˆ–ä¸´åºŠæ•æ„Ÿç—…ä¾‹çš„å­¦ä¹ è§„åˆ™ï¼Œæ¨¡æ‹Ÿæ”¾å°„ç§‘åŒ»ç”Ÿåœ¨è®°å½•å…³é”®å‘ç°æ—¶çš„ç²¾ç»†ä¿®æ­£è¡Œä¸ºã€‚è¿™äº›ç»„ä»¶å…±åŒå°†ç»“æ„åŒ–æŠ¥å‘ŠèŒƒå¼è½¬åŒ–ä¸ºå¼ºåŒ–å¾®è°ƒè¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ—¢ç¬¦åˆè¯­è¨€ä¸€è‡´æ€§åˆç¬¦åˆä¸´åºŠå®é™…çš„æŠ¥å‘Šã€‚åœ¨èƒ¸éƒ¨Xå°„çº¿å’Œé¢ˆåŠ¨è„‰è¶…å£°æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°åŸºçº¿ç›¸æ¯”ï¼ŒRadFlowæŒç»­æé«˜äº†è¯Šæ–­è¿è´¯æ€§å’Œæ€»ä½“æŠ¥å‘Šè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10065v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºRadFlowçš„åŒ»å­¦æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé‡‡ç”¨åˆ†å±‚å·¥ä½œæµå¼•å¯¼çš„å¼ºåŒ–ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿæ”¾å°„ç§‘åŒ»ç”Ÿæ’°å†™æŠ¥å‘Šçš„ç»“æ„åŒ–è¿‡ç¨‹ã€‚RadFlowé€šè¿‡æ„å»ºä¸´åºŠæŠ¥å‘Šçš„ç»“æ„åŒ–æ¨¡å‹ã€å¼•å…¥å¥–åŠ±å±‚æ¬¡ç»“æ„ä»¥åŠé’ˆå¯¹å…³é”®ç—…ä¾‹çš„æ”¿ç­–ä¼˜åŒ–æœºåˆ¶ï¼Œæé«˜äº†æŠ¥å‘Šçš„è¿è´¯æ€§å’Œä¸´åºŠå‡†ç¡®æ€§ã€‚å®éªŒè¯æ˜ï¼ŒRadFlowç›¸è¾ƒäºå…¶ä»–ç³»ç»Ÿï¼Œèƒ½ç”Ÿæˆæ›´é«˜è´¨é‡çš„åŒ»å­¦æŠ¥å‘Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æŠ¥å‘Šçš„ç”Ÿæˆé€šå¸¸é‡‡ç”¨ç»“æ„åŒ–å·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬æè¿°è§†è§‰å‘ç°ã€æ€»ç»“å°è±¡ä»¥åŠç²¾ç»†æè¿°ä¸´åºŠå…³é”®ç—…ä¾‹ã€‚</li>
<li>ç°æœ‰åŒ»å­¦æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿå¤šå°†æŠ¥å‘Šè§†ä¸ºå¹³é¢åºåˆ—ï¼Œå¿½è§†äº†å…¶å±‚æ¬¡ç»“æ„ï¼Œå¯¼è‡´æè¿°ä¸è¯Šæ–­å†…å®¹çš„ä¸ä¸€è‡´ã€‚</li>
<li>RadFlowæ˜¯ä¸€ä¸ªåˆ†å±‚å·¥ä½œæµå¼•å¯¼çš„å¼ºåŒ–ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„æŠ¥å‘Šå®è·µï¼Œå¹¶æ˜ç¡®å»ºæ¨¡ä¸´åºŠæŠ¥å‘Šçš„ç»“æ„åŒ–ç‰¹æ€§ã€‚</li>
<li>RadFlowå¼•å…¥äº†ä¸€ä¸ªä¸´åºŠåŸºç¡€çš„å¥–åŠ±å±‚æ¬¡ç»“æ„ï¼Œè¯¥ç»“æ„åæ˜ äº†æ”¾å°„å­¦æŠ¥å‘Šçš„ç»„ç»‡ç»“æ„ã€‚</li>
<li>åœ¨å…¨å±€å±‚é¢ï¼Œå¥–åŠ±ç»“åˆäº†è¯­è¨€æµç•…æ€§ã€åŒ»å­¦é¢†åŸŸæ­£ç¡®æ€§å’Œè·¨èŠ‚ä¸€è‡´æ€§ï¼Œä»¥ä¿ƒè¿›è¿è´¯ä¸”ç¬¦åˆä¸´åºŠå®é™…çš„å™è¿°ã€‚</li>
<li>åœ¨å±€éƒ¨å±‚é¢ï¼Œé’ˆå¯¹å°è±¡éƒ¨åˆ†çš„å¥–åŠ±å¼ºè°ƒäº†å…¶è´¨é‡ï¼Œåæ˜ äº†å°è±¡åœ¨è¯Šæ–­å‡†ç¡®æ€§ä¸­çš„æ ¸å¿ƒä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10065">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a00eb7d287cca9de8c5f8b422f8a314" align="middle">
<img src="https://picx.zhimg.com/v2-87eb2d2907b833acff43c96f64267dd4" align="middle">
<img src="https://picx.zhimg.com/v2-bc4073f5b4092c1afcf13757bf5e77e2" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MuSc-V2-Zero-Shot-Multimodal-Industrial-Anomaly-Classification-and-Segmentation-with-Mutual-Scoring-of-Unlabeled-Samples"><a href="#MuSc-V2-Zero-Shot-Multimodal-Industrial-Anomaly-Classification-and-Segmentation-with-Mutual-Scoring-of-Unlabeled-Samples" class="headerlink" title="MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples"></a>MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples</h2><p><strong>Authors:Xurui Li, Feng Xue, Yu Zhou</strong></p>
<p>Zero-shot anomaly classification (AC) and segmentation (AS) methods aim to identify and outline defects without using any labeled samples. In this paper, we reveal a key property that is overlooked by existing methods: normal image patches across industrial products typically find many other similar patches, not only in 2D appearance but also in 3D shapes, while anomalies remain diverse and isolated. To explicitly leverage this discriminative property, we propose a Mutual Scoring framework (MuSc-V2) for zero-shot AC&#x2F;AS, which flexibly supports single 2D&#x2F;3D or multimodality. Specifically, our method begins by improving 3D representation through Iterative Point Grouping (IPG), which reduces false positives from discontinuous surfaces. Then we use Similarity Neighborhood Aggregation with Multi-Degrees (SNAMD) to fuse 2D&#x2F;3D neighborhood cues into more discriminative multi-scale patch features for mutual scoring. The core comprises a Mutual Scoring Mechanism (MSM) that lets samples within each modality to assign score to each other, and Cross-modal Anomaly Enhancement (CAE) that fuses 2D and 3D scores to recover modality-specific missing anomalies. Finally, Re-scoring with Constrained Neighborhood (RsCon) suppresses false classification based on similarity to more representative samples. Our framework flexibly works on both the full dataset and smaller subsets with consistently robust performance, ensuring seamless adaptability across diverse product lines. In aid of the novel framework, MuSc-V2 achieves significant performance improvements: a $\textbf{+23.7%}$ AP gain on the MVTec 3D-AD dataset and a $\textbf{+19.3%}$ boost on the Eyecandies dataset, surpassing previous zero-shot benchmarks and even outperforming most few-shot methods. The code will be available at The code will be available at \href{<a target="_blank" rel="noopener" href="https://github.com/HUST-SLOW/MuSc-V2%7D%7Bhttps://github.com/HUST-SLOW/MuSc-V2%7D">https://github.com/HUST-SLOW/MuSc-V2}{https://github.com/HUST-SLOW/MuSc-V2}</a>.</p>
<blockquote>
<p>é›¶æ ·æœ¬å¼‚å¸¸åˆ†ç±»ï¼ˆACï¼‰å’Œåˆ†å‰²ï¼ˆASï¼‰æ–¹æ³•æ—¨åœ¨ä¸ä½¿ç”¨ä»»ä½•æ ‡è®°æ ·æœ¬æ¥è¯†åˆ«å’Œæè¿°ç¼ºé™·ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ­ç¤ºäº†ä¸€ä¸ªè¢«ç°æœ‰æ–¹æ³•æ‰€å¿½è§†çš„å…³é”®å±æ€§ï¼šå·¥ä¸šäº§å“ä¸­çš„æ­£å¸¸å›¾åƒè¡¥ä¸é€šå¸¸ä¼šæ‰¾åˆ°è®¸å¤šå…¶ä»–ç›¸ä¼¼çš„è¡¥ä¸ï¼Œä¸ä»…åœ¨2Då¤–è§‚æ–¹é¢ï¼Œè€Œä¸”åœ¨3Då½¢çŠ¶æ–¹é¢ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œè€Œå¼‚å¸¸å€¼ä»ç„¶å¤šæ ·ä¸”å­¤ç«‹ã€‚ä¸ºäº†æ˜ç¡®åˆ©ç”¨è¿™ç§åˆ¤åˆ«å±æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºé›¶æ ·æœ¬AC&#x2F;ASçš„Mutual Scoringæ¡†æ¶ï¼ˆMuSc-V2ï¼‰ï¼Œå®ƒçµæ´»æ”¯æŒå•2D&#x2F;3Dæˆ–å¤šæ¨¡å¼ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé€šè¿‡è¿­ä»£ç‚¹åˆ†ç»„ï¼ˆIPGï¼‰æ”¹è¿›3Dè¡¨ç¤ºï¼Œä»è€Œå‡å°‘æ¥è‡ªä¸è¿ç»­è¡¨é¢çš„è¯¯æŠ¥ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨å…·æœ‰å¤šåº¦çš„ç›¸ä¼¼æ€§é‚»åŸŸèšåˆï¼ˆSNAMDï¼‰æ¥å°†2D&#x2F;3Dé‚»åŸŸçº¿ç´¢èåˆä¸ºæ›´å…·åˆ¤åˆ«åŠ›çš„å¤šå°ºåº¦è¡¥ä¸ç‰¹å¾ï¼Œä»¥è¿›è¡Œç›¸äº’è¯„åˆ†ã€‚æ ¸å¿ƒéƒ¨åˆ†åŒ…æ‹¬è®©æ¯ç§æ¨¡æ€å†…çš„æ ·æœ¬ç›¸äº’è¯„åˆ†çš„Mutual Scoring Mechanismï¼ˆMSMï¼‰ï¼Œä»¥åŠèåˆ2Då’Œ3Dè¯„åˆ†çš„Cross-modal Anomaly Enhancementï¼ˆCAEï¼‰ï¼Œä»¥æ¢å¤ç‰¹å®šäºæ¨¡æ€çš„ç¼ºå¤±å¼‚å¸¸å€¼ã€‚æœ€åï¼ŒåŸºäºä¸æ›´å…·ä»£è¡¨æ€§æ ·æœ¬çš„ç›¸ä¼¼æ€§ï¼Œé€šè¿‡å—çº¦æŸçš„é‚»åŸŸé‡æ–°è¯„åˆ†ï¼ˆRsConï¼‰æŠ‘åˆ¶è¯¯åˆ†ç±»ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ—¢é€‚ç”¨äºå…¨æ•°æ®é›†ï¼Œä¹Ÿé€‚ç”¨äºè¾ƒå°çš„å­é›†ï¼Œå¹¶ä¸”å…·æœ‰ä¸€è‡´ç¨³å®šçš„æ€§èƒ½ï¼Œå¯ç¡®ä¿åœ¨ä¸åŒäº§å“çº¿ä¹‹é—´è¿›è¡Œæ— ç¼é€‚åº”ã€‚å€ŸåŠ©æ–°å‹æ¡†æ¶MuSc-V2ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼šåœ¨MVTec 3D-ADæ•°æ®é›†ä¸Šæé«˜äº†+23.7ï¼…çš„APå€¼ï¼Œåœ¨Eyecandiesæ•°æ®é›†ä¸Šæé«˜äº†+19.3ï¼…çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„é›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ï¼Œç”šè‡³è¶…è¶Šäº†å¤§å¤šæ•°å°æ ·æ–¹æ³•ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/HUST-SLOW/MuSc-V2">https://github.com/HUST-SLOW/MuSc-V2</a>æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10047v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„é›¶æ ·æœ¬å¼‚å¸¸åˆ†ç±»ä¸åˆ†å‰²æ–¹æ³•ï¼Œå³MuSc-V2æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨æ­£å¸¸å›¾åƒè¡¥ä¸åœ¨å·¥ä¸šå“ä¸­çš„ç›¸ä¼¼æ€§å’Œå·®å¼‚æ€§ï¼Œæé«˜äº†å¼‚å¸¸æ£€æµ‹å’Œåˆ†å‰²çš„æ€§èƒ½ã€‚é€šè¿‡æ”¹è¿›3Dè¡¨ç¤ºã€èåˆå¤šå°ºåº¦è¡¥ä¸ç‰¹å¾ã€å»ºç«‹ç›¸äº’è¯„åˆ†æœºåˆ¶å’Œè·¨æ¨¡æ€å¼‚å¸¸å¢å¼ºï¼ŒMuSc-V2æ¡†æ¶åœ¨MVTec 3D-ADå’ŒEyecandiesæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é›¶æ ·æœ¬å¼‚å¸¸åˆ†ç±»å’Œåˆ†å‰²æ–¹æ³•æ—¨åœ¨æ— éœ€æ ‡æ³¨æ ·æœ¬è¿›è¡Œç¼ºé™·è¯†åˆ«å’Œæ ‡æ³¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½ç•¥äº†æ­£å¸¸å›¾åƒè¡¥ä¸åœ¨å·¥ä¸šå“ä¸­çš„ç›¸ä¼¼æ€§å’Œå·®å¼‚æ€§è¿™ä¸€å…³é”®å±æ€§ã€‚</li>
<li>MuSc-V2æ¡†æ¶é€šè¿‡åˆ©ç”¨è¿™ä¸€å±æ€§ï¼Œæé«˜äº†å¼‚å¸¸æ£€æµ‹å’Œåˆ†å‰²çš„æ€§èƒ½ã€‚</li>
<li>MuSc-V2æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼šæ”¹è¿›3Dè¡¨ç¤ºã€èåˆå¤šå°ºåº¦è¡¥ä¸ç‰¹å¾çš„ç›¸äº’è¯„åˆ†æœºåˆ¶å’Œè·¨æ¨¡æ€å¼‚å¸¸å¢å¼ºã€‚</li>
<li>MuSc-V2æ¡†æ¶å…·æœ‰çµæ´»æ€§å’Œå¼ºå¤§çš„æ€§èƒ½ï¼Œå¯é€‚åº”ä¸åŒçš„äº§å“çº¿å’Œæ•°æ®é›†å¤§å°ã€‚</li>
<li>MuSc-V2æ¡†æ¶åœ¨MVTec 3D-ADå’ŒEyecandiesæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºä¹‹å‰çš„é›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ï¼Œç”šè‡³è¶…è¿‡äº†å¤§å¤šæ•°å°‘æ ·æœ¬æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10047">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e0d72a6bad08efacf16adb64ed406dd" align="middle">
<img src="https://picx.zhimg.com/v2-26ed1aed70b801eb31422ff2b241964d" align="middle">
<img src="https://picx.zhimg.com/v2-64bdea85898a2a23c36adae03167e3d3" align="middle">
<img src="https://picx.zhimg.com/v2-8a9012993a2481f8ef1cb0bab65f4558" align="middle">
<img src="https://picx.zhimg.com/v2-3fe52fc29768f5e885d3f39128a7bf24" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MIRNet-Integrating-Constrained-Graph-Based-Reasoning-with-Pre-training-for-Diagnostic-Medical-Imaging"><a href="#MIRNet-Integrating-Constrained-Graph-Based-Reasoning-with-Pre-training-for-Diagnostic-Medical-Imaging" class="headerlink" title="MIRNet: Integrating Constrained Graph-Based Reasoning with Pre-training for Diagnostic Medical Imaging"></a>MIRNet: Integrating Constrained Graph-Based Reasoning with Pre-training for Diagnostic Medical Imaging</h2><p><strong>Authors:Shufeng Kong, Zijie Wang, Nuan Cui, Hao Tang, Yihan Meng, Yuanyuan Wei, Feifan Chen, Yingheng Wang, Zhuo Cai, Yaonan Wang, Yulong Zhang, Yuzheng Li, Zibin Zheng, Caihua Liu</strong></p>
<p>Automated interpretation of medical images demands robust modeling of complex visual-semantic relationships while addressing annotation scarcity, label imbalance, and clinical plausibility constraints. We introduce MIRNet (Medical Image Reasoner Network), a novel framework that integrates self-supervised pre-training with constrained graph-based reasoning. Tongue image diagnosis is a particularly challenging domain that requires fine-grained visual and semantic understanding. Our approach leverages self-supervised masked autoencoder (MAE) to learn transferable visual representations from unlabeled data; employs graph attention networks (GAT) to model label correlations through expert-defined structured graphs; enforces clinical priors via constraint-aware optimization using KL divergence and regularization losses; and mitigates imbalance using asymmetric loss (ASL) and boosting ensembles. To address annotation scarcity, we also introduce TongueAtlas-4K, a comprehensive expert-curated benchmark comprising 4,000 images annotated with 22 diagnostic labelsâ€“representing the largest public dataset in tongue analysis. Validation shows our method achieves state-of-the-art performance. While optimized for tongue diagnosis, the framework readily generalizes to broader diagnostic medical imaging tasks.</p>
<blockquote>
<p>åŒ»å­¦å›¾åƒè‡ªåŠ¨è§£è¯»è¦æ±‚å¯¹å¤æ‚çš„è§†è§‰è¯­ä¹‰å…³ç³»è¿›è¡Œç¨³å¥å»ºæ¨¡ï¼ŒåŒæ—¶è§£å†³æ ‡æ³¨ç¨€ç¼ºã€æ ‡ç­¾ä¸å¹³è¡¡å’Œä¸´åºŠå¯è¡Œæ€§çº¦æŸç­‰é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†MIRNetï¼ˆåŒ»å­¦å›¾åƒæ¨ç†ç½‘ç»œï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ƒå°†è‡ªç›‘ç£é¢„è®­ç»ƒä¸åŸºäºçº¦æŸçš„å›¾å½¢æ¨ç†ç›¸ç»“åˆã€‚èˆŒè‹”å›¾åƒè¯Šæ–­æ˜¯ä¸€ä¸ªç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§çš„é¢†åŸŸï¼Œéœ€è¦ç²¾ç»†çš„è§†è§‰å’Œè¯­ä¹‰ç†è§£ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨è‡ªç›‘ç£æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEï¼‰ä»éæ ‡è®°æ•°æ®ä¸­å­¦ä¹ å¯è¿ç§»çš„è§†è§‰è¡¨ç¤ºï¼›é‡‡ç”¨å›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆGATï¼‰é€šè¿‡ä¸“å®¶å®šä¹‰çš„ç»“æ„åŒ–å›¾å½¢å¯¹æ ‡ç­¾ç›¸å…³æ€§è¿›è¡Œå»ºæ¨¡ï¼›é€šè¿‡åˆ©ç”¨KLæ•£åº¦å’Œæ­£åˆ™åŒ–æŸå¤±çš„çº¦æŸæ„ŸçŸ¥ä¼˜åŒ–æ¥å®æ–½ä¸´åºŠå…ˆéªŒï¼›å¹¶ä½¿ç”¨ä¸å¯¹ç§°æŸå¤±ï¼ˆASLï¼‰å’Œå¢å¼ºé›†æˆæ¥ç¼“è§£ä¸å¹³è¡¡é—®é¢˜ã€‚ä¸ºè§£å†³æ ‡æ³¨ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†TongueAtlas-4Kï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«4000å¼ å›¾åƒå’Œ22ä¸ªè¯Šæ–­æ ‡ç­¾çš„ç»¼åˆæ€§ä¸“å®¶ç²¾é€‰åŸºå‡†æ•°æ®é›†â€”â€”ä»£è¡¨äº†èˆŒè‹”åˆ†æé¢†åŸŸæœ€å¤§çš„å…¬å¼€æ•°æ®é›†ã€‚éªŒè¯è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚è™½ç„¶è¯¥æ–¹æ³•é’ˆå¯¹èˆŒè‹”è¯Šæ–­è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä½†å®ƒå¾ˆå®¹æ˜“æ¨å¹¿åˆ°æ›´å¹¿æ³›çš„è¯Šæ–­åŒ»å­¦å½±åƒä»»åŠ¡ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10013v1">PDF</a> To appear at AAAI-26</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒ»ç–—å›¾åƒè‡ªåŠ¨è§£è¯»ä¸­çš„MIRNetæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†è‡ªç›‘ç£é¢„è®­ç»ƒå’ŒåŸºäºçº¦æŸå›¾çš„æ¨ç†ã€‚é’ˆå¯¹èˆŒå›¾åƒè¯Šæ–­è¿™ä¸€å…·æœ‰æŒ‘æˆ˜çš„é¢†åŸŸï¼ŒMIRNetåˆ©ç”¨è‡ªç›‘ç£æ©ç è‡ªåŠ¨ç¼–ç å™¨å­¦ä¹ å¯è¿ç§»çš„è§†è§‰è¡¨ç¤ºï¼Œä½¿ç”¨å›¾æ³¨æ„åŠ›ç½‘ç»œå¯¹ä¸“å®¶å®šä¹‰çš„ç»“æ„å›¾è¿›è¡Œæ ‡ç­¾å…³è”å»ºæ¨¡ï¼Œé€šè¿‡KLæ•£åº¦å’Œæ­£åˆ™åŒ–æŸå¤±å®æ–½ä¸´åºŠå…ˆéªŒçº¦æŸæ„ŸçŸ¥ä¼˜åŒ–ï¼Œå¹¶é‡‡ç”¨ä¸å¯¹ç§°æŸå¤±å’Œå¢å¼ºé›†æˆæ¥ç¼“è§£ä¸å¹³è¡¡é—®é¢˜ã€‚ä¸ºè§£å†³æ ‡æ³¨ç¨€ç¼ºé—®é¢˜ï¼Œè¿˜æ¨å‡ºäº†TongueAtlas-4Kæ•°æ®é›†ã€‚éªŒè¯ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œå¹¶æ˜“äºæ¨å¹¿è‡³æ›´å¹¿æ³›çš„åŒ»ç–—å›¾åƒè¯Šæ–­ä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MIRNetæ¡†æ¶ç»“åˆäº†è‡ªç›‘ç£é¢„è®­ç»ƒå’ŒåŸºäºçº¦æŸå›¾çš„æ¨ç†ï¼Œç”¨äºåŒ»ç–—å›¾åƒè‡ªåŠ¨è§£è¯»ã€‚</li>
<li>èˆŒå›¾åƒè¯Šæ–­æ˜¯ä¸€ä¸ªéœ€è¦ç²¾ç»†è§†è§‰å’Œè¯­ä¹‰ç†è§£çš„æŒ‘æˆ˜é¢†åŸŸã€‚</li>
<li>MIRNetåˆ©ç”¨è‡ªç›‘ç£æ©ç è‡ªåŠ¨ç¼–ç å™¨å­¦ä¹ å¯è¿ç§»çš„è§†è§‰è¡¨ç¤ºï¼Œä»éæ ‡è®°æ•°æ®ä¸­è·å–ä¿¡æ¯ã€‚</li>
<li>å›¾æ³¨æ„åŠ›ç½‘ç»œè¢«ç”¨äºé€šè¿‡ä¸“å®¶å®šä¹‰çš„ç»“æ„å›¾è¿›è¡Œæ ‡ç­¾å…³è”å»ºæ¨¡ã€‚</li>
<li>å®æ–½ä¸´åºŠå…ˆéªŒçº¦æŸæ„ŸçŸ¥ä¼˜åŒ–ï¼Œé€šè¿‡KLæ•£åº¦å’Œæ­£åˆ™åŒ–æŸå¤±æ¥å¤„ç†ä¸´åºŠå¯è¡Œæ€§çº¦æŸã€‚</li>
<li>é‡‡ç”¨ä¸å¯¹ç§°æŸå¤±å’Œå¢å¼ºé›†æˆæ¥ç¼“è§£æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>æ¨å‡ºäº†TongueAtlas-4Kæ•°æ®é›†ï¼Œè¿™æ˜¯èˆŒåˆ†æé¢†åŸŸæœ€å¤§çš„å…¬å¼€æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10013">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f1d4b43c19b7191b0f9a1d59fb2cde8d" align="middle">
<img src="https://picx.zhimg.com/v2-b21a619d44a10c38ced8ec26eadc8243" align="middle">
<img src="https://picx.zhimg.com/v2-e39091647df2f381e18e57155b5da961" align="middle">
<img src="https://picx.zhimg.com/v2-a623dffa883fcbc3fd467c3eb2b536d3" align="middle">
<img src="https://picx.zhimg.com/v2-86781c7aef46ef2178d6180426adcdaf" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DBGroup-Dual-Branch-Point-Grouping-for-Weakly-Supervised-3D-Instance-Segmentation"><a href="#DBGroup-Dual-Branch-Point-Grouping-for-Weakly-Supervised-3D-Instance-Segmentation" class="headerlink" title="DBGroup: Dual-Branch Point Grouping for Weakly Supervised 3D Instance Segmentation"></a>DBGroup: Dual-Branch Point Grouping for Weakly Supervised 3D Instance Segmentation</h2><p><strong>Authors:Xuexun Liu, Xiaoxu Xu, Qiudan Zhang, Lin Ma, Xu Wang</strong></p>
<p>Weakly supervised 3D instance segmentation is essential for 3D scene understanding, especially as the growing scale of data and high annotation costs associated with fully supervised approaches. Existing methods primarily rely on two forms of weak supervision: one-thing-one-click annotations and bounding box annotations, both of which aim to reduce labeling efforts. However, these approaches still encounter limitations, including labor-intensive annotation processes, high complexity, and reliance on expert annotators. To address these challenges, we propose \textbf{DBGroup}, a two-stage weakly supervised 3D instance segmentation framework that leverages scene-level annotations as a more efficient and scalable alternative. In the first stage, we introduce a Dual-Branch Point Grouping module to generate pseudo labels guided by semantic and mask cues extracted from multi-view images. To further improve label quality, we develop two refinement strategies: Granularity-Aware Instance Merging and Semantic Selection and Propagation. The second stage involves multi-round self-training on an end-to-end instance segmentation network using the refined pseudo-labels. Additionally, we introduce an Instance Mask Filter strategy to address inconsistencies within the pseudo labels. Extensive experiments demonstrate that DBGroup achieves competitive performance compared to sparse-point-level supervised 3D instance segmentation methods, while surpassing state-of-the-art scene-level supervised 3D semantic segmentation approaches. Code is available at <a target="_blank" rel="noopener" href="https://github.com/liuxuexun/DBGroup">https://github.com/liuxuexun/DBGroup</a>.</p>
<blockquote>
<p>å¼±ç›‘ç£3Då®ä¾‹åˆ†å‰²å¯¹äº3Dåœºæ™¯ç†è§£è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯éšç€æ•°æ®è§„æ¨¡çš„å¢é•¿å’Œå…¨ç›‘ç£æ–¹æ³•ç›¸å…³çš„é«˜æ ‡æ³¨æˆæœ¬ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºä¸¤ç§å½¢å¼çš„å¼±ç›‘ç£ï¼šä¸€ç‚¹ä¸€å‡»æ ‡æ³¨å’Œè¾¹ç•Œæ¡†æ ‡æ³¨ï¼Œä¸¤è€…çš„ç›®æ ‡éƒ½æ˜¯å‡å°‘æ ‡æ³¨å·¥ä½œé‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»é¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ ‡æ³¨è¿‡ç¨‹åŠ³åŠ¨å¼ºåº¦é«˜ã€å¤æ‚æ€§é«˜ä»¥åŠä¾èµ–ä¸“å®¶æ ‡æ³¨è€…ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>DBGroup</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„å¼±ç›‘ç£3Då®ä¾‹åˆ†å‰²æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨åœºæ™¯çº§æ ‡æ³¨ä½œä¸ºæ›´é«˜æ•ˆå’Œå¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒåˆ†æ”¯ç‚¹åˆ†ç»„æ¨¡å—ï¼Œè¯¥æ¨¡å—ä»¥å¤šè§†è§’å›¾åƒä¸­æå–çš„è¯­ä¹‰å’Œæ©è†œçº¿ç´¢ä¸ºæŒ‡å¯¼ç”Ÿæˆä¼ªæ ‡ç­¾ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ ‡ç­¾è´¨é‡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸¤ç§ç»†åŒ–ç­–ç•¥ï¼šç²’åº¦æ„ŸçŸ¥å®ä¾‹åˆå¹¶å’Œè¯­ä¹‰é€‰æ‹©å’Œä¼ æ’­ã€‚ç¬¬äºŒé˜¶æ®µæ¶‰åŠåœ¨ç«¯åˆ°ç«¯å®ä¾‹åˆ†å‰²ç½‘ç»œä¸Šè¿›è¡Œå¤šè½®è‡ªè®­ç»ƒï¼Œä½¿ç”¨ç²¾ç‚¼çš„ä¼ªæ ‡ç­¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå®ä¾‹æ©è†œè¿‡æ»¤ç­–ç•¥ï¼Œä»¥è§£å†³ä¼ªæ ‡ç­¾å†…éƒ¨çš„ä¸ä¸€è‡´æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDBGroupä¸ç¨€ç–ç‚¹çº§ç›‘ç£çš„3Då®ä¾‹åˆ†å‰²æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶è¶…è¶Šäº†æœ€å…ˆè¿›çš„åœºæ™¯çº§ç›‘ç£çš„3Dè¯­ä¹‰åˆ†å‰²æ–¹æ³•ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/liuxuexun/DBGroup%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/liuxuexun/DBGroupè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10003v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥ç ”ç©¶é’ˆå¯¹ä¸‰ç»´åœºæ™¯ç†è§£æå‡ºäº†ä¸€ç§å¼±ç›‘ç£ä¸‹çš„ä¸‰ç»´å®ä¾‹åˆ†å‰²æ–¹æ³•ï¼Œå‘½åä¸ºDBGroupã€‚å®ƒé‡‡ç”¨åœºæ™¯çº§åˆ«çš„æ ‡æ³¨ä½œä¸ºé«˜æ•ˆå¯ä¼¸ç¼©çš„æ›¿ä»£æ–¹æ¡ˆï¼Œåˆ©ç”¨åŒåˆ†æ”¯ç‚¹åˆ†ç»„æ¨¡å—ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œè¯¥ä¼ªæ ‡ç­¾ç”±å¤šè§†è§’å›¾åƒæå–çš„è¯­ä¹‰å’Œæ©è†œçº¿ç´¢å¼•å¯¼ã€‚é€šè¿‡ç²’åº¦æ„ŸçŸ¥å®ä¾‹åˆå¹¶å’Œè¯­ä¹‰é€‰æ‹©å’Œä¼ æ’­ä¸¤ç§ç­–ç•¥è¿›ä¸€æ­¥ä¼˜åŒ–æ ‡ç­¾è´¨é‡ã€‚åœ¨ä¼ªæ ‡ç­¾çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡å¤šè½®ç«¯åˆ°ç«¯å®ä¾‹åˆ†å‰²ç½‘ç»œçš„è‡ªè®­ç»ƒï¼Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDBGroupç›¸è¾ƒäºç¨€ç–ç‚¹çº§ç›‘ç£çš„ä¸‰ç»´å®ä¾‹åˆ†å‰²æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶è¶…è¶Šäº†åœºæ™¯çº§ç›‘ç£çš„ä¸‰ç»´è¯­ä¹‰åˆ†å‰²æ–¹æ³•ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>DBGroupæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„å¼±ç›‘ç£ä¸‰ç»´å®ä¾‹åˆ†å‰²æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡æ•°æ®ä¸‹çš„æ ‡æ³¨æˆæœ¬é«˜é—®é¢˜ã€‚</li>
<li>åœ¨ç¬¬ä¸€é˜¶æ®µä¸­å¼•å…¥äº†åŒåˆ†æ”¯ç‚¹åˆ†ç»„æ¨¡å—ï¼Œä½¿ç”¨åœºæ™¯çº§åˆ«æ ‡æ³¨ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œç»“åˆè¯­ä¹‰å’Œæ©è†œçº¿ç´¢æŒ‡å¯¼ä¼ªæ ‡ç­¾çš„ç”Ÿæˆã€‚</li>
<li>ä¸ºæé«˜æ ‡ç­¾è´¨é‡ï¼Œæå‡ºç²’åº¦æ„ŸçŸ¥å®ä¾‹åˆå¹¶å’Œè¯­ä¹‰é€‰æ‹©å’Œä¼ æ’­ä¸¤ç§ç»†åŒ–ç­–ç•¥ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé€šè¿‡å¤šè½®è‡ªè®­ç»ƒåœ¨ç«¯åˆ°ç«¯çš„å®ä¾‹åˆ†å‰²ç½‘ç»œä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œå¹¶é‡‡ç”¨å®ä¾‹æ©è†œè¿‡æ»¤ç­–ç•¥è§£å†³ä¼ªæ ‡ç­¾çš„ä¸ä¸€è‡´æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜DBGroupåœ¨å¼±ç›‘ç£ç¯å¢ƒä¸‹çš„æ€§èƒ½è¡¨ç°ä¼˜å¼‚ï¼Œä¸å…ˆè¿›çš„åœºæ™¯çº§ç›‘ç£æ–¹æ³•ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>è¯¥æ–¹æ³•çš„ä»£ç å·²ç»å…¬å¼€å¯ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10003">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9101bcb815716f1147162e12456629a8" align="middle">
<img src="https://picx.zhimg.com/v2-8fe22f5c694db91e47e6482c00a41594" align="middle">
<img src="https://picx.zhimg.com/v2-55b6e0a665e17f98ae6d50dc7ffd903a" align="middle">
<img src="https://picx.zhimg.com/v2-e98321555d0a2dc5ac173799e8f41fe1" align="middle">
<img src="https://picx.zhimg.com/v2-6fd38483ec5ce490acf28f9c1251dddc" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="GPDM-Generation-Prior-Diffusion-Model-for-Accelerated-Direct-Attenuation-and-Scatter-Correction-of-Whole-body-18F-FDG-PET"><a href="#GPDM-Generation-Prior-Diffusion-Model-for-Accelerated-Direct-Attenuation-and-Scatter-Correction-of-Whole-body-18F-FDG-PET" class="headerlink" title="GPDM: Generation-Prior Diffusion Model for Accelerated Direct Attenuation and Scatter Correction of Whole-body 18F-FDG PET"></a>GPDM: Generation-Prior Diffusion Model for Accelerated Direct Attenuation and Scatter Correction of Whole-body 18F-FDG PET</h2><p><strong>Authors:Min Jeong Cho, Hyeong Seok Shim, Sungyu Kim, Jae Sung Lee</strong></p>
<p>Accurate attenuation and scatter corrections are crucial in positron emission tomography (PET) imaging for accurate visual interpretation and quantitative analysis. Traditional methods relying on computed tomography (CT) or magnetic resonance imaging (MRI) have limitations in accuracy, radiation exposure, and applicability. Deep neural networks provide potential approaches to estimating attenuation and scatter-corrected (ASC) PET from non-attenuation and non-scatter-corrected (NASC) PET images based on VAE or CycleGAN. However, the limitations inherent to conventional GAN-based methods, such as unstable training and mode collapse, need further advancements. To address these limitations and achieve more accurate attenuation and scatter corrections, we propose a novel framework for generating high-quality ASC PET images from NASC PET images: Generation-Prior Diffusion Model (GPDM). Our GPDM framework is based on the Denoising Diffusion Probabilistic Model (DDPM), but instead of starting sampling from an entirely different image distribution, it begins from a distribution similar to the target images we aim to generate. This similar distribution is referred to as the Generation-Prior. By leveraging this Generation-Prior, the GPDM framework effectively reduces the number of sampling steps and generates more refined ASC PET images. Our experimental results demonstrate that GPDM outperforms existing methods in generating ASC PET images, achieving superior accuracy while significantly reducing sampling time. These findings highlight the potential of GPDM to address the limitations of conventional methods and establish a new standard for efficient and accurate attenuation and scatter correction in PET imaging.</p>
<blockquote>
<p>å‡†ç¡®çš„è¡°å‡å’Œæ•£å°„æ ¡æ­£åœ¨æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰æˆåƒä¸­å¯¹äºå‡†ç¡®çš„è§†è§‰è§£è¯»å’Œå®šé‡åˆ†æè‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ä¾èµ–äºè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æˆ–ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ï¼Œä½†åœ¨å‡†ç¡®æ€§ã€è¾å°„æš´éœ²å’Œé€‚ç”¨æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æ·±åº¦ç¥ç»ç½‘ç»œæä¾›äº†åŸºäºå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰æˆ–CycleGANä»éè¡°å‡å’Œéæ•£å°„æ ¡æ­£ï¼ˆNASCï¼‰PETå›¾åƒä¼°è®¡è¡°å‡å’Œæ•£å°„æ ¡æ­£ï¼ˆASCï¼‰PETçš„æ½œåœ¨æ–¹æ³•ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ–¹æ³•å›ºæœ‰çš„å±€é™æ€§ï¼Œå¦‚è®­ç»ƒä¸ç¨³å®šå’Œæ¨¡å¼å´©æºƒï¼Œéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œå®ç°æ›´å‡†ç¡®çš„è¡°å‡å’Œæ•£å°„æ ¡æ­£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä»NASC PETå›¾åƒç”Ÿæˆé«˜è´¨é‡ASC PETå›¾åƒçš„æ–°æ¡†æ¶ï¼šç”Ÿæˆä¼˜å…ˆæ‰©æ•£æ¨¡å‹ï¼ˆGPDMï¼‰ã€‚æˆ‘ä»¬çš„GPDMæ¡†æ¶åŸºäºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ï¼Œä½†ä¸ä»ä¸€ä¸ªå®Œå…¨ä¸åŒçš„å›¾åƒåˆ†å¸ƒå¼€å§‹é‡‡æ ·ä¸åŒï¼Œå®ƒä»ä¸æˆ‘ä»¬æ—¨åœ¨ç”Ÿæˆçš„ç›®æ ‡å›¾åƒç›¸ä¼¼çš„åˆ†å¸ƒå¼€å§‹ã€‚è¿™ä¸ªç›¸ä¼¼çš„åˆ†å¸ƒè¢«ç§°ä¸ºç”Ÿæˆä¼˜å…ˆã€‚é€šè¿‡åˆ©ç”¨è¿™ç§ç”Ÿæˆä¼˜å…ˆï¼ŒGPDMæ¡†æ¶æœ‰æ•ˆåœ°å‡å°‘äº†é‡‡æ ·æ­¥éª¤çš„æ•°é‡ï¼Œå¹¶ç”Ÿæˆäº†æ›´ç²¾ç»†çš„ASC PETå›¾åƒã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGPDMåœ¨ç”ŸæˆASC PETå›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†é‡‡æ ·æ—¶é—´ã€‚è¿™äº›å‘ç°çªå‡ºäº†GPDMè§£å†³ä¼ ç»Ÿæ–¹æ³•å±€é™æ€§çš„æ½œåŠ›ï¼Œå¹¶ä¸ºPETæˆåƒä¸­çš„é«˜æ•ˆå’Œå‡†ç¡®è¡°å‡å’Œæ•£å°„æ ¡æ­£å»ºç«‹äº†æ–°æ ‡å‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09941v1">PDF</a> 25 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„åŒ»å­¦å›¾åƒé‡å»ºæŠ€æœ¯å·²æˆä¸ºå½“ä»Šç ”ç©¶çš„çƒ­ç‚¹ï¼Œç‰¹åˆ«æ˜¯å¯¹äºPETæˆåƒä¸­çš„è¡°å‡å’Œæ•£å°„æ ¡æ­£é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ç”Ÿæˆå…ˆéªŒæ‰©æ•£æ¨¡å‹ï¼ˆGPDMï¼‰ï¼Œé‡‡ç”¨æ‰©æ•£æ¦‚ç‡æ¨¡å‹ä½œä¸ºåŸºç¡€ï¼Œå€ŸåŠ©ç”Ÿæˆå…ˆéªŒï¼ˆGeneration-Priorï¼‰æ¥ç”Ÿæˆé«˜è´¨é‡çš„è¡°å‡æ•£å°„æ ¡æ­£PETå›¾åƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGPDMåœ¨ç”ŸæˆASC PETå›¾åƒæ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®åº¦é«˜ä¸”é‡‡æ ·æ—¶é—´çŸ­ï¼Œæœ‰æœ›è§£å†³ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§å¹¶ç¡®ç«‹æ–°çš„æ ‡å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡°å‡å’Œæ•£å°„æ ¡æ­£åœ¨PETæˆåƒä¸­è‡³å…³é‡è¦ï¼Œå½±å“è§†è§‰è§£è¯»å’Œå®šé‡åˆ†æã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å¦‚ä¾èµ–CTæˆ–MRIå­˜åœ¨å‡†ç¡®æ€§ã€è¾å°„æš´éœ²å’Œé€‚ç”¨æ€§çš„å±€é™æ€§ã€‚</li>
<li>æ·±åº¦ç¥ç»ç½‘ç»œä¸ºä¼°è®¡è¡°å‡æ•£å°„æ ¡æ­£PETå›¾åƒæä¾›äº†æ½œåœ¨æ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åŸºäºç”Ÿæˆå…ˆéªŒæ‰©æ•£æ¨¡å‹ï¼ˆGPDMï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¦‚ç‡æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„ASC PETå›¾åƒã€‚</li>
<li>GPDMå€ŸåŠ©ç”Ÿæˆå…ˆéªŒæ¥å‡å°‘é‡‡æ ·æ­¥éª¤å¹¶ç”Ÿæˆæ›´ç²¾ç»†çš„ASC PETå›¾åƒã€‚</li>
<li>å®éªŒè¯æ˜GPDMåœ¨ç”ŸæˆASC PETå›¾åƒæ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œå‡†ç¡®åº¦é«˜ä¸”é‡‡æ ·æ—¶é—´çŸ­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09941">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f1497ac36e0b2c01d64b9bd50044c4bb" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Regional-Attention-Enhanced-Swin-Transformer-for-Clinically-Relevant-Medical-Image-Captioning"><a href="#Regional-Attention-Enhanced-Swin-Transformer-for-Clinically-Relevant-Medical-Image-Captioning" class="headerlink" title="Regional Attention-Enhanced Swin Transformer for Clinically Relevant Medical Image Captioning"></a>Regional Attention-Enhanced Swin Transformer for Clinically Relevant Medical Image Captioning</h2><p><strong>Authors:Zubia Naz, Farhan Asghar, Muhammad Ishfaq Hussain, Yahya Hadadi, Muhammad Aasim Rafique, Wookjin Choi, Moongu Jeon</strong></p>
<p>Automated medical image captioning translates complex radiological images into diagnostic narratives that can support reporting workflows. We present a Swin-BART encoder-decoder system with a lightweight regional attention module that amplifies diagnostically salient regions before cross-attention. Trained and evaluated on ROCO, our model achieves state-of-the-art semantic fidelity while remaining compact and interpretable. We report results as mean$\pm$std over three seeds and include $95%$ confidence intervals. Compared with baselines, our approach improves ROUGE (proposed 0.603, ResNet-CNN 0.356, BLIP2-OPT 0.255) and BERTScore (proposed 0.807, BLIP2-OPT 0.645, ResNet-CNN 0.623), with competitive BLEU, CIDEr, and METEOR. We further provide ablations (regional attention on&#x2F;off and token-count sweep), per-modality analysis (CT&#x2F;MRI&#x2F;X-ray), paired significance tests, and qualitative heatmaps that visualize the regions driving each description. Decoding uses beam search (beam size $&#x3D;4$), length penalty $&#x3D;1.1$, $no_repeat_ngram_size$ $&#x3D;3$, and max length $&#x3D;128$. The proposed design yields accurate, clinically phrased captions and transparent regional attributions, supporting safe research use with a human in the loop.</p>
<blockquote>
<p>è‡ªåŠ¨åŒ»å­¦å›¾åƒæ³¨é‡Šå°†å¤æ‚çš„æ”¾å°„å­¦å›¾åƒè½¬åŒ–ä¸ºè¯Šæ–­æ€§å™è¿°ï¼Œæ”¯æŒæŠ¥å‘Šå·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºSwin-BARTç¼–ç å™¨-è§£ç å™¨çš„ç³»ç»Ÿï¼Œå¹¶å¸¦æœ‰è½»é‡çº§åŒºåŸŸæ³¨æ„æ¨¡å—ï¼Œåœ¨äº¤å‰æ³¨æ„ä¹‹å‰æ”¾å¤§è¯Šæ–­æ˜¾è‘—çš„åŒºåŸŸã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨ROCOæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„è¯­ä¹‰ä¿çœŸåº¦ï¼ŒåŒæ—¶ä¿æŒç´§å‡‘å’Œå¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬æŠ¥å‘Šçš„ç»“æœä¸ºä¸‰ä¸ªç§å­çš„å¹³å‡å€¼Â±æ ‡å‡†å·®ï¼Œå¹¶åŒ…æ‹¬95%çš„ç½®ä¿¡åŒºé—´ã€‚ä¸åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ROUGEï¼ˆæå‡ºè€…å¾—åˆ†ä¸º0.603ï¼ŒResNet-CNNå¾—åˆ†ä¸º0.356ï¼ŒBLIP2-OPTå¾—åˆ†ä¸º0.255ï¼‰å’ŒBERTScoreï¼ˆæå‡ºè€…å¾—åˆ†ä¸º0.807ï¼ŒBLIP2-OPTå¾—åˆ†ä¸º0.645ï¼ŒResNet-CNNå¾—åˆ†ä¸º0.623ï¼‰ä¸Šè¡¨ç°æ›´å¥½ï¼ŒåŒæ—¶BLEUã€CIDErå’ŒMETEORä¹Ÿå…·æœ‰ç«äº‰åŠ›ã€‚æˆ‘ä»¬è¿˜æä¾›äº†æ¶ˆèç ”ç©¶ï¼ˆåŒºåŸŸæ³¨æ„åŠ›å¼€å…³å’Œä»¤ç‰Œè®¡æ•°æ‰«æï¼‰ã€æ¨¡æ€åˆ†æï¼ˆCT&#x2F;MRI&#x2F;Xå°„çº¿ï¼‰ã€é…å¯¹æ˜¾è‘—æ€§æµ‹è¯•ä»¥åŠå¯è§†åŒ–é©±åŠ¨æè¿°çš„åŒºåŸŸçš„å®šæ€§çƒ­å›¾ã€‚è§£ç ä½¿ç”¨é›†æŸæœç´¢ï¼ˆé›†æŸå¤§å°&#x3D;4ï¼‰ï¼Œé•¿åº¦æƒ©ç½š&#x3D;1.1ï¼Œæ— é‡å¤ngramå¤§å°&#x3D;3ï¼Œæœ€å¤§é•¿åº¦&#x3D;128ã€‚æ‰€æå‡ºçš„è®¾è®¡äº§ç”Ÿäº†å‡†ç¡®ã€ä¸´åºŠæ€§æªè¾çš„æ³¨é‡Šå’Œé€æ˜çš„åŒºåŸŸå½’å±ï¼Œæ”¯æŒæœ‰äººå·¥å‚ä¸çš„å®‰å…¨ç ”ç©¶ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09893v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºSwin-BARTç¼–ç å™¨è§£ç å™¨çš„è‡ªåŠ¨åŒ–åŒ»å­¦å›¾åƒæè¿°ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé…å¤‡äº†è½»é‡çº§åŒºåŸŸæ³¨æ„åŠ›æ¨¡å—ï¼Œèƒ½å¤Ÿæ”¾å¤§è¯Šæ–­å…³é”®åŒºåŸŸã€‚åœ¨ROCOæ•°æ®é›†ä¸Šè®­ç»ƒä¸è¯„ä¼°ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒç´§å‡‘å’Œå¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œå®ç°äº†å…ˆè¿›çš„è¯­ä¹‰ä¿çœŸåº¦ã€‚é€šè¿‡ä¸€ç³»åˆ—å®éªŒå’Œè¯„ä¼°æŒ‡æ ‡ï¼Œè¯æ˜äº†è¯¥æ¨¡å‹ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•ï¼Œåœ¨ROUGEã€BERTScoreç­‰è¯„ä¼°æŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ã€‚åŒæ—¶ï¼Œæä¾›äº†å¯è§†åŒ–çƒ­å›¾æ¥å±•ç¤ºé©±åŠ¨æè¿°çš„å…³é”®åŒºåŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†åŸºäºSwin-BARTç¼–ç å™¨è§£ç å™¨çš„åŒ»å­¦å›¾åƒæè¿°ç³»ç»Ÿã€‚</li>
<li>ç³»ç»ŸåŒ…å«ä¸€ä¸ªè½»é‡çº§åŒºåŸŸæ³¨æ„åŠ›æ¨¡å—ï¼Œç”¨äºæ”¾å¤§è¯Šæ–­å…³é”®åŒºåŸŸã€‚</li>
<li>æ¨¡å‹åœ¨ROCOæ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒä¸è¯„ä¼°ï¼Œå®ç°é«˜è¯­ä¹‰ä¿çœŸåº¦ã€‚</li>
<li>æ¨¡å‹ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•åœ¨ROUGEå’ŒBERTScoreç­‰è¯„ä¼°æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>æä¾›å¯è§†åŒ–çƒ­å›¾æ¥å±•ç¤ºæè¿°çš„å…³é”®åŒºåŸŸã€‚</li>
<li>æ¨¡å‹çš„è§£ç è¿‡ç¨‹é‡‡ç”¨äº†ç‰¹å®šçš„è®¾ç½®ï¼ŒåŒ…æ‹¬beamæœç´¢ã€é•¿åº¦æƒ©ç½šã€æ— é‡å¤ngramå¤§å°ä»¥åŠæœ€å¤§é•¿åº¦ç­‰å‚æ•°ã€‚</li>
<li>è¯¥ç³»ç»Ÿè®¾è®¡æ—¨åœ¨ä¸ºåŒ»å­¦å›¾åƒç”Ÿæˆå‡†ç¡®ã€ä¸´åºŠæœ¯è¯­çš„æè¿°ï¼Œå¹¶æä¾›é€æ˜çš„åŒºåŸŸå½’å±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09893">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-334bdc7c2844d67f94afbc84588a15c8" align="middle">
<img src="https://picx.zhimg.com/v2-cb120bae131505db3e0d6f7490ff0155" align="middle">
<img src="https://picx.zhimg.com/v2-8551dea490c852d9b7ea906317bd5f24" align="middle">
<img src="https://picx.zhimg.com/v2-94aecc2352f4831f66dc0ab0ccdf4dae" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="TomoGraphView-3D-Medical-Image-Classification-with-Omnidirectional-Slice-Representations-and-Graph-Neural-Networks"><a href="#TomoGraphView-3D-Medical-Image-Classification-with-Omnidirectional-Slice-Representations-and-Graph-Neural-Networks" class="headerlink" title="TomoGraphView: 3D Medical Image Classification with Omnidirectional Slice Representations and Graph Neural Networks"></a>TomoGraphView: 3D Medical Image Classification with Omnidirectional Slice Representations and Graph Neural Networks</h2><p><strong>Authors:Johannes Kiechle, Stefan M. Fischer, Daniel M. Lang, Cosmin I. Bercea, Matthew J. Nyflot, Lina Felsner, Julia A. Schnabel, Jan C. Peeken</strong></p>
<p>The growing number of medical tomography examinations has necessitated the development of automated methods capable of extracting comprehensive imaging features to facilitate downstream tasks such as tumor characterization, while assisting physicians in managing their growing workload. However, 3D medical image classification remains a challenging task due to the complex spatial relationships and long-range dependencies inherent in volumetric data. Training models from scratch suffers from low data regimes, and the absence of 3D large-scale multimodal datasets has limited the development of 3D medical imaging foundation models. Recent studies, however, have highlighted the potential of 2D vision foundation models, originally trained on natural images, as powerful feature extractors for medical image analysis. Despite these advances, existing approaches that apply 2D models to 3D volumes via slice-based decomposition remain suboptimal. Conventional volume slicing strategies, which rely on canonical planes such as axial, sagittal, or coronal, may inadequately capture the spatial extent of target structures when these are misaligned with standardized viewing planes. Furthermore, existing slice-wise aggregation strategies rarely account for preserving the volumetric structure, resulting in a loss of spatial coherence across slices. To overcome these limitations, we propose TomoGraphView, a novel framework that integrates omnidirectional volume slicing with spherical graph-based feature aggregation. We publicly share our accessible code base at <a target="_blank" rel="noopener" href="http://github.com/compai-lab/2025-MedIA-kiechle">http://github.com/compai-lab/2025-MedIA-kiechle</a> and provide a user-friendly library for omnidirectional volume slicing at <a target="_blank" rel="noopener" href="https://pypi.org/project/OmniSlicer">https://pypi.org/project/OmniSlicer</a>.</p>
<blockquote>
<p>éšç€åŒ»å­¦æ–­å±‚æ‰«ææ£€æŸ¥æ•°é‡çš„ä¸æ–­å¢åŠ ï¼Œå¿…é¡»å¼€å‘èƒ½å¤Ÿæå–å…¨é¢æˆåƒç‰¹å¾çš„è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œä»¥ä¿ƒè¿›è‚¿ç˜¤ç‰¹å¾åŒ–ç­‰ä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒæ—¶å¸®åŠ©åŒ»ç”Ÿç®¡ç†æ—¥ç›Šå¢é•¿çš„å·¥ä½œé‡ã€‚ç„¶è€Œï¼Œç”±äºä½“ç§¯æ•°æ®ä¸­çš„å¤æ‚ç©ºé—´å…³ç³»å’Œé•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œ3DåŒ»å­¦å›¾åƒåˆ†ç±»ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ä¼šå—åˆ°æ•°æ®ä¸è¶³çš„é™åˆ¶ï¼Œç¼ºä¹å¤§è§„æ¨¡çš„3Då¤šæ¨¡å¼æ•°æ®é›†é™åˆ¶äº†3DåŒ»å­¦æˆåƒåŸºç¡€æ¨¡å‹çš„å‘å±•ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶çªå‡ºäº†åŸæœ¬åœ¨è‡ªç„¶å›¾åƒä¸Šè®­ç»ƒçš„2Dè§†è§‰åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å¼ºå¤§ç‰¹å¾æå–æ½œåŠ›ã€‚å°½ç®¡å–å¾—äº†è¿™äº›è¿›å±•ï¼Œä½†ç°æœ‰çš„å°†2Dæ¨¡å‹åº”ç”¨äº3Dä½“ç§¯æ•°æ®çš„åˆ‡ç‰‡åˆ†è§£æ–¹æ³•ä»ä¸ç†æƒ³ã€‚ä¼ ç»Ÿçš„ä½“ç§¯åˆ‡ç‰‡ç­–ç•¥ä¾èµ–äºæ ‡å‡†å¹³é¢ï¼ˆå¦‚è½´å‘ã€çŸ¢çŠ¶é¢æˆ–å† çŠ¶é¢ï¼‰ï¼Œå½“ç›®æ ‡ç»“æ„ä¸æ ‡å‡†åŒ–è§‚çœ‹å¹³é¢ä¸å¯¹é½æ—¶ï¼Œå¯èƒ½æ— æ³•å……åˆ†æ•æ‰å…¶ç©ºé—´èŒƒå›´ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åˆ‡ç‰‡çº§èšåˆç­–ç•¥å¾ˆå°‘è€ƒè™‘ä¿æŒä½“ç§¯ç»“æ„ï¼Œå¯¼è‡´åˆ‡ç‰‡é—´ç©ºé—´è¿è´¯æ€§çš„ä¸§å¤±ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†TomoGraphViewè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒç»“åˆäº†å…¨å‘ä½“ç§¯åˆ‡ç‰‡å’ŒåŸºäºçƒå½¢å›¾çš„ç‰¹å¾èšåˆã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="http://github.com/compai-lab/2025-MedIA-kiechle%E5%85%AC%E5%BC%80%E5%88%86%E4%BA%AB%E4%BA%86%E6%88%91%E4%BB%AC%E5%8F%AF%E8%AE%BF%E9%97%AE%E7%9A%84%E4%BB%A3%E7%A0%81%E5%BA%93%EF%BC%8C%E5%B9%B6%E5%9C%A8https://pypi.org/project/OmniSlicer%E6%8F%90%E4%BE%9B%E4%BA%86%E7%94%A8%E6%88%B7%E5%8F%8B%E5%A5%BD%E7%9A%84%E5%85%A8%E5%90%91%E4%BD%93%E7%A7%AF%E5%88%87%E7%89%87%E5%BA%93%E3%80%82">http://github.com/compai-lab/2025-MedIA-kiechleå…¬å¼€åˆ†äº«äº†æˆ‘ä»¬å¯è®¿é—®çš„ä»£ç åº“ï¼Œå¹¶åœ¨https://pypi.org/project/OmniSliceræä¾›äº†ç”¨æˆ·å‹å¥½çš„å…¨å‘ä½“ç§¯åˆ‡ç‰‡åº“ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09605v1">PDF</a> Preprint submitted to Medical Image Analysis (MedIA)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŒ»å­¦æ–­å±‚æ‰«ææ£€æŸ¥æ•°é‡çš„å¢é•¿å¸¦æ¥çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬éœ€è¦ä»åŒ»å­¦å›¾åƒä¸­æå–å…¨é¢ç‰¹å¾ä»¥æ”¯æŒè‚¿ç˜¤è¡¨å¾ç­‰ä¸‹æ¸¸ä»»åŠ¡ï¼Œä»¥åŠååŠ©åŒ»ç”Ÿåº”å¯¹æ—¥ç›Šå¢åŠ çš„å·¥ä½œé‡ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå°½ç®¡å­˜åœ¨äºŒç»´è§†è§‰åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„æ½œåŠ›ï¼Œä½†ç”±äºå¯¹ä¸‰ç»´å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†çš„ç¼ºä¹ï¼Œä»¥åŠä¼ ç»Ÿçš„åŸºäºåˆ‡ç‰‡çš„äºŒç»´æ¨¡å‹åœ¨ä¸‰ç»´åŒ»å­¦å›¾åƒåº”ç”¨ä¸­çš„å±€é™æ€§ï¼Œå¦‚åˆ‡ç‰‡è¿‡ç¨‹ä¸­ç›®æ ‡ç»“æ„ç©ºé—´èŒƒå›´æ•æ‰ä¸è¶³å’Œåˆ‡ç‰‡é—´ç©ºé—´è¿è´¯æ€§çš„ä¸§å¤±ç­‰é—®é¢˜ï¼Œç›®å‰ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†ç±»ä»æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶TomoGraphViewï¼Œç»“åˆäº†å…¨æ–¹å‘ä½“ç§¯åˆ‡ç‰‡å’Œçƒå½¢å›¾ç‰¹å¾èšåˆæŠ€æœ¯ï¼Œå¹¶å…¬å¼€åˆ†äº«äº†å…¶ä»£ç åº“å’Œç”¨æˆ·å‹å¥½çš„å…¨æ–¹å‘ä½“ç§¯åˆ‡ç‰‡åº“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>éšç€åŒ»å­¦æ–­å±‚æ‰«ææ£€æŸ¥æ•°é‡çš„å¢é•¿ï¼Œéœ€è¦å¼€å‘è‡ªåŠ¨åŒ–æ–¹æ³•ä»åŒ»å­¦å›¾åƒä¸­æå–å…¨é¢çš„ç‰¹å¾ä»¥æ”¯æŒä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†ç±»é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºæ•°æ®å¤æ‚æ€§ã€è®­ç»ƒæ¨¡å‹çš„å±€é™æ€§ä»¥åŠç¼ºä¹å¤§è§„æ¨¡ä¸‰ç»´å¤šæ¨¡æ€æ•°æ®é›†ã€‚</li>
<li>äºŒç»´è§†è§‰åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†ä¼ ç»Ÿçš„åŸºäºåˆ‡ç‰‡çš„äºŒç»´æ¨¡å‹åº”ç”¨äºä¸‰ç»´åŒ»å­¦å›¾åƒå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å…¨æ–¹å‘ä½“ç§¯åˆ‡ç‰‡èƒ½å¤Ÿæ•æ‰æ›´å¤šä¿¡æ¯ï¼Œè€Œç°æœ‰çš„åˆ‡ç‰‡æ–¹æ³•å¯èƒ½æ— æ³•å……åˆ†æ•æ‰ç›®æ ‡çš„ç©ºé—´èŒƒå›´ã€‚</li>
<li>TomoGraphViewæ¡†æ¶ç»“åˆäº†å…¨æ–¹å‘ä½“ç§¯åˆ‡ç‰‡å’Œçƒå½¢å›¾ç‰¹å¾èšåˆæŠ€æœ¯æ¥å…‹æœç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09605">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-74384f0455610ae8955efe0e13141f17" align="middle">
<img src="https://picx.zhimg.com/v2-827dbddd3bf11f999eda89a8d75e930a" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Diffusion-Based-Quality-Control-of-Medical-Image-Segmentations-across-Organs"><a href="#Diffusion-Based-Quality-Control-of-Medical-Image-Segmentations-across-Organs" class="headerlink" title="Diffusion-Based Quality Control of Medical Image Segmentations across Organs"></a>Diffusion-Based Quality Control of Medical Image Segmentations across Organs</h2><p><strong>Authors:Vincenzo MarcianÃ², Hava Chaptoukaev, Virginia Fernandez, M. Jorge Cardoso, SÃ©bastien Ourselin, Michela Antonelli, Maria A. Zuluaga</strong></p>
<p>Medical image segmentation using deep learning (DL) has enabled the development of automated analysis pipelines for large-scale population studies. However, state-of-the-art DL methods are prone to hallucinations, which can result in anatomically implausible segmentations. With manual correction impractical at scale, automated quality control (QC) techniques have to address the challenge. While promising, existing QC methods are organ-specific, limiting their generalizability and usability beyond their original intended task. To overcome this limitation, we propose no-new Quality Control (nnQC), a robust QC framework based on a diffusion-generative paradigm that self-adapts to any input organ dataset. Central to nnQC is a novel Team of Experts (ToE) architecture, where two specialized experts independently encode 3D spatial awareness, represented by the relative spatial position of an axial slice, and anatomical information derived from visual features from the original image. A weighted conditional module dynamically combines the pair of independent embeddings, or opinions to condition the sampling mechanism within a diffusion process, enabling the generation of a spatially aware pseudo-ground truth for predicting QC scores. Within its framework, nnQC integrates fingerprint adaptation to ensure adaptability across organs, datasets, and imaging modalities. We evaluated nnQC on seven organs using twelve publicly available datasets. Our results demonstrate that nnQC consistently outperforms state-of-the-art methods across all experiments, including cases where segmentation masks are highly degraded or completely missing, confirming its versatility and effectiveness across different organs.</p>
<blockquote>
<p>ä½¿ç”¨æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰è¿›è¡ŒåŒ»å­¦å›¾åƒåˆ†å‰²å·²ç»èƒ½å¤Ÿæ¨åŠ¨å¤§è§„æ¨¡äººç¾¤ç ”ç©¶çš„è‡ªåŠ¨åŒ–åˆ†ææµç¨‹çš„å¼€å‘ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„DLæ–¹æ³•å®¹æ˜“å‡ºç°â€œå¹»è§†â€ç°è±¡ï¼Œè¿™å¯èƒ½å¯¼è‡´è§£å‰–ä¸Šä¸åˆç†çš„åˆ†å‰²ç»“æœã€‚ç”±äºæ‰‹åŠ¨ä¿®æ­£åœ¨å¤§è§„æ¨¡æƒ…å†µä¸‹ä¸åˆ‡å®é™…ï¼Œå› æ­¤å¿…é¡»é‡‡ç”¨è‡ªåŠ¨åŒ–è´¨é‡æ§åˆ¶ï¼ˆQCï¼‰æŠ€æœ¯æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚å°½ç®¡å‰æ™¯å¹¿é˜”ï¼Œä½†ç°æœ‰çš„QCæ–¹æ³•æ˜¯é’ˆå¯¹ç‰¹å®šå™¨å®˜çš„ï¼Œé™åˆ¶äº†å…¶åœ¨åŸå§‹ä»»åŠ¡ä¹‹å¤–çš„é€šç”¨æ€§å’Œå¯ç”¨æ€§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æ— æ–°è´¨é‡æ§åˆ¶ï¼ˆnnQCï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£ç”ŸæˆèŒƒå¼çš„ç¨³å¥QCæ¡†æ¶ï¼Œå¯è‡ªé€‚åº”äºä»»ä½•è¾“å…¥å™¨å®˜æ•°æ®é›†ã€‚nnQCçš„æ ¸å¿ƒæ˜¯ä¸€ç§æ–°å‹ä¸“å®¶å›¢é˜Ÿï¼ˆToEï¼‰æ¶æ„ï¼Œå…¶ä¸­ä¸¤ä½ä¸“ä¸šä¸“å®¶ç‹¬ç«‹ç¼–ç 3Dç©ºé—´æ„ŸçŸ¥ï¼Œç”±è½´å‘åˆ‡ç‰‡çš„ç›¸å¯¹ç©ºé—´ä½ç½®è¡¨ç¤ºï¼Œå¹¶ä»åŸå§‹å›¾åƒä¸­æå–çš„è§†è§‰ç‰¹å¾ä¸­æ´¾ç”Ÿå‡ºçš„è§£å‰–ä¿¡æ¯ã€‚åŠ æƒæ¡ä»¶æ¨¡å—åŠ¨æ€ç»“åˆäº†è¿™å¯¹ç‹¬ç«‹çš„åµŒå…¥æˆ–æ„è§ï¼Œä»¥åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­è°ƒèŠ‚é‡‡æ ·æœºåˆ¶ï¼Œä»è€Œç”Ÿæˆç©ºé—´æ„ŸçŸ¥ä¼ªçœŸå®åœ°é¢æ•°æ®ï¼Œç”¨äºé¢„æµ‹QCåˆ†æ•°ã€‚åœ¨å…¶æ¡†æ¶ä¸‹ï¼ŒnnQCé›†æˆäº†æŒ‡çº¹è‡ªé€‚åº”æŠ€æœ¯ï¼Œä»¥ç¡®ä¿åœ¨ä¸åŒå™¨å®˜ã€æ•°æ®é›†å’Œæˆåƒæ¨¡å¼ä¹‹é—´çš„é€‚åº”æ€§ã€‚æˆ‘ä»¬åœ¨ä¸ƒä¸ªå™¨å®˜å’ŒåäºŒä¸ªå…¬å¼€æ•°æ®é›†ä¸Šè¯„ä¼°äº†nnQCã€‚ç»“æœè¡¨æ˜ï¼ŒnnQCåœ¨æ‰€æœ‰å®éªŒä¸­å‡è¡¨ç°å‡ºä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬åˆ†å‰²æ©è†œé«˜åº¦é€€åŒ–æˆ–å®Œå…¨ç¼ºå¤±çš„æƒ…å†µï¼Œè¿™è¯å®äº†å…¶åœ¨ä¸åŒå™¨å®˜ä¸­çš„é€šç”¨æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09588v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºæ·±åº¦å­¦ä¹ çš„åŒ»å­¦å›¾åƒåˆ†å‰²æŠ€æœ¯ä¸ºå¤§è§„æ¨¡äººç¾¤ç ”ç©¶æä¾›äº†è‡ªåŠ¨åŒ–åˆ†æç®¡é“ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ–¹æ³•å®¹æ˜“å‡ºç°å¹»è§‰ï¼Œå¯¼è‡´è§£å‰–ç»“æ„ä¸åˆç†çš„åˆ†å‰²ç»“æœã€‚é’ˆå¯¹æ‰‹åŠ¨æ ¡æ­£åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸­ä¸å®ç”¨çš„æƒ…å†µï¼Œå¿…é¡»é‡‡ç”¨è‡ªåŠ¨åŒ–è´¨é‡æ§åˆ¶æŠ€æœ¯æ¥è§£å†³æŒ‘æˆ˜ã€‚å°½ç®¡ç°æœ‰è´¨é‡æ§åˆ¶æ–¹æ³•å‰æ™¯å¹¿é˜”ï¼Œä½†å®ƒä»¬å…·æœ‰å™¨å®˜ç‰¹å¼‚æ€§ï¼Œé™åˆ¶äº†å…¶åœ¨åŸå§‹ä»»åŠ¡ä¹‹å¤–çš„ä¸€èˆ¬åŒ–å’Œå¯ç”¨æ€§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ‰©æ•£ç”ŸæˆèŒƒå¼çš„æ–°æ— æ–°è´¨é‡æ§åˆ¶ï¼ˆnnQCï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯è‡ªé€‚åº”äºä»»ä½•è¾“å…¥å™¨å®˜æ•°æ®é›†ã€‚nnQCçš„æ ¸å¿ƒæ˜¯ä¸€ç§æ–°å‹ä¸“å®¶å›¢é˜Ÿï¼ˆToEï¼‰æ¶æ„ï¼Œå…¶ä¸­ä¸¤ä¸ªä¸“ä¸šä¸“å®¶ç‹¬ç«‹ç¼–ç 3Dç©ºé—´æ„ŸçŸ¥å’Œä»åŸå§‹å›¾åƒæ´¾ç”Ÿçš„è§£å‰–å­¦ä¿¡æ¯ã€‚åŠ æƒæ¡ä»¶æ¨¡å—åŠ¨æ€ç»“åˆäº†è¿™ä¸¤ä¸ªç‹¬ç«‹çš„åµŒå…¥æˆ–æ„è§ï¼Œä»¥è°ƒèŠ‚é‡‡æ ·æœºåˆ¶ä¸­çš„æ‰©æ•£è¿‡ç¨‹ï¼Œç”Ÿæˆç©ºé—´æ„ŸçŸ¥ä¼ªçœŸå®ç”¨äºé¢„æµ‹è´¨é‡æ§åˆ¶åˆ†æ•°ã€‚åœ¨æ¡†æ¶å†…ï¼ŒnnQCé€šè¿‡æŒ‡çº¹é€‚åº”æ€§ç¡®ä¿äº†è·¨å™¨å®˜ã€æ•°æ®é›†å’Œæˆåƒæ¨¡å¼çš„æ•°æ®é€‚åº”æ€§ã€‚<strong>å…³é”®è§è§£å¦‚ä¸‹</strong>ï¼š</p>
<pre><code>**Key Takeaways**
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09588">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-43f1271f8bd90490e7bce654f87e19b1" align="middle">
<img src="https://picx.zhimg.com/v2-a5cf173c6726beabc4e46be7674c0b9a" align="middle">
<img src="https://picx.zhimg.com/v2-c4844d9d1b78f95d0e234e0548ffb0e4" align="middle">
<img src="https://picx.zhimg.com/v2-b8b2ec322ad2ffb924b93405f7dad31c" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="DualFete-Revisiting-Teacher-Student-Interactions-from-a-Feedback-Perspective-for-Semi-supervised-Medical-Image-Segmentation"><a href="#DualFete-Revisiting-Teacher-Student-Interactions-from-a-Feedback-Perspective-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="DualFete: Revisiting Teacher-Student Interactions from a Feedback Perspective for Semi-supervised Medical Image Segmentation"></a>DualFete: Revisiting Teacher-Student Interactions from a Feedback Perspective for Semi-supervised Medical Image Segmentation</h2><p><strong>Authors:Le Yi, Wei Huang, Lei Zhang, Kefu Zhao, Yan Wang, Zizhou Wang</strong></p>
<p>The teacher-student paradigm has emerged as a canonical framework in semi-supervised learning. When applied to medical image segmentation, the paradigm faces challenges due to inherent image ambiguities, making it particularly vulnerable to erroneous supervision. Crucially, the studentâ€™s iterative reconfirmation of these errors leads to self-reinforcing bias. While some studies attempt to mitigate this bias, they often rely on external modifications to the conventional teacher-student framework, overlooking its intrinsic potential for error correction. In response, this work introduces a feedback mechanism into the teacher-student framework to counteract error reconfirmations. Here, the student provides feedback on the changes induced by the teacherâ€™s pseudo-labels, enabling the teacher to refine these labels accordingly. We specify that this interaction hinges on two key components: the feedback attributor, which designates pseudo-labels triggering the studentâ€™s update, and the feedback receiver, which determines where to apply this feedback. Building on this, a dual-teacher feedback model is further proposed, which allows more dynamics in the feedback loop and fosters more gains by resolving disagreements through cross-teacher supervision while avoiding consistent errors. Comprehensive evaluations on three medical image benchmarks demonstrate the methodâ€™s effectiveness in addressing error propagation in semi-supervised medical image segmentation.</p>
<blockquote>
<p>å¸ˆå¾’èŒƒå¼å·²æˆä¸ºåŠç›‘ç£å­¦ä¹ ä¸­çš„ç»å…¸æ¡†æ¶ã€‚å½“åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²æ—¶ï¼Œç”±äºå›¾åƒå›ºæœ‰çš„æ¨¡ç³Šæ€§ï¼Œè¯¥èŒƒå¼é¢ä¸´æŒ‘æˆ˜ï¼Œä½¿å…¶ç‰¹åˆ«å®¹æ˜“å—åˆ°é”™è¯¯çš„ç›‘ç£ã€‚å…³é”®çš„æ˜¯ï¼Œå­¦ç”Ÿå¯¹è¿™äº›é”™è¯¯çš„è¿­ä»£ç¡®è®¤ä¼šå¯¼è‡´è‡ªæˆ‘åŠ å¼ºçš„åè§ã€‚è™½ç„¶ä¸€äº›ç ”ç©¶è¯•å›¾å‡è½»è¿™ç§åè§ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºå¯¹ä¼ ç»Ÿå¸ˆå¾’æ¡†æ¶çš„å¤–éƒ¨ä¿®æ”¹ï¼Œå¿½è§†äº†å…¶å†…åœ¨çš„é”™è¯¯çº æ­£æ½œåŠ›ã€‚ä¸ºæ­¤ï¼Œè¿™é¡¹å·¥ä½œå°†åé¦ˆæœºåˆ¶å¼•å…¥å¸ˆå¾’æ¡†æ¶ä»¥å¯¹æŠ—é”™è¯¯ç¡®è®¤ã€‚åœ¨è¿™é‡Œï¼Œå­¦ç”Ÿæä¾›æœ‰å…³æ•™å¸ˆä¼ªæ ‡ç­¾å¼•èµ·çš„å˜åŒ–çš„åé¦ˆï¼Œä½¿æ•™å¸ˆèƒ½å¤Ÿç›¸åº”åœ°æ”¹è¿›è¿™äº›æ ‡ç­¾ã€‚æˆ‘ä»¬æŒ‡å®šè¿™ç§äº’åŠ¨ä¾èµ–äºä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šåé¦ˆå±æ€§æŒ‡å®šè§¦å‘å­¦ç”Ÿæ›´æ–°çš„ä¼ªæ ‡ç­¾ï¼Œåé¦ˆæ¥æ”¶å™¨ç¡®å®šåº”ç”¨åé¦ˆçš„åœ°æ–¹ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥æå‡ºäº†åŒæ•™å¸ˆåé¦ˆæ¨¡å‹ï¼Œä½¿åé¦ˆå›è·¯æ›´åŠ åŠ¨æ€ï¼Œé€šè¿‡è·¨æ•™å¸ˆç›‘ç£è§£å†³åˆ†æ­§ï¼Œé¿å…æŒç»­é”™è¯¯ï¼Œä»è€Œå®ç°æ›´å¤šæ”¶ç›Šã€‚åœ¨ä¸‰ä¸ªåŒ»å­¦å›¾åƒåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆè¯„ä¼°è¯æ˜äº†è¯¥æ–¹æ³•åœ¨åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è§£å†³é”™è¯¯ä¼ æ’­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09319v1">PDF</a> Accepted by Proceedings of the AAAI Conference on Artificial Intelligence 40 (AAAI-26)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æ•™å¸ˆ-å­¦ç”Ÿæ¨¡å¼çš„æŒ‘æˆ˜åŠå…¶è§£å†³æ–¹æ¡ˆã€‚ç”±äºå›¾åƒå›ºæœ‰çš„æ¨¡ç³Šæ€§ï¼Œè¯¥æ¨¡å¼å®¹æ˜“å—åˆ°é”™è¯¯çš„ç›‘ç£å½±å“ã€‚å­¦ç”Ÿåå¤ç¡®è®¤è¿™äº›é”™è¯¯ä¼šå¯¼è‡´è‡ªæˆ‘å¼ºåŒ–åè§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥åé¦ˆæœºåˆ¶ï¼Œè®©å­¦ç”Ÿæä¾›å…³äºæ•™å¸ˆä¼ªæ ‡ç­¾å˜åŒ–çš„åé¦ˆï¼Œä½¿æ•™å¸ˆèƒ½å¤Ÿç›¸åº”è°ƒæ•´æ ‡ç­¾ã€‚é€šè¿‡åŒæ•™å¸ˆåé¦ˆæ¨¡å‹çš„æ„å»ºï¼Œä¸ºåé¦ˆå¾ªç¯æä¾›æ›´åŠ¨æ€çš„æ–¹å¼ï¼Œé€šè¿‡è·¨æ•™å¸ˆç›‘ç£è§£å†³åˆ†æ­§ï¼Œé¿å…æŒç»­é”™è¯¯ï¼Œä»è€Œè·å¾—æ›´å¤šæ”¶ç›Šã€‚åœ¨ä¸‰ä¸ªåŒ»å­¦å›¾åƒåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆè¯„ä¼°è¯æ˜äº†è¯¥æ–¹æ³•åœ¨è§£å†³åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„é”™è¯¯ä¼ æ’­æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•™å¸ˆ-å­¦ç”Ÿæ¨¡å¼åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå› å›¾åƒæ¨¡ç³Šæ€§æ˜“å¯¼è‡´é”™è¯¯ç›‘ç£ã€‚</li>
<li>å­¦ç”Ÿåå¤ç¡®è®¤é”™è¯¯ä¼šå¯¼è‡´è‡ªæˆ‘å¼ºåŒ–åè§ï¼Œéœ€è¦æœ‰æ•ˆæœºåˆ¶æ¥çº æ­£ã€‚</li>
<li>å¼•å…¥åé¦ˆæœºåˆ¶ï¼Œå­¦ç”Ÿæä¾›å…³äºæ•™å¸ˆä¼ªæ ‡ç­¾å˜åŒ–çš„åé¦ˆï¼Œä¿ƒè¿›æ ‡ç­¾è°ƒæ•´ã€‚</li>
<li>åé¦ˆæœºåˆ¶åŒ…æ‹¬åé¦ˆèµ‹äºˆå™¨å’Œåé¦ˆæ¥æ”¶å™¨ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>æå‡ºåŒæ•™å¸ˆåé¦ˆæ¨¡å‹ï¼Œæä¾›æ›´å¤šåŠ¨æ€åé¦ˆæ–¹å¼ï¼Œè§£å†³åˆ†æ­§å¹¶é¿å…æŒç»­é”™è¯¯ã€‚</li>
<li>è·¨æ•™å¸ˆç›‘ç£æœ‰åŠ©äºå¢å¼ºæ¨¡å‹çš„å‡†ç¡®æ€§åŠé”™è¯¯çº æ­£èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09319">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-002fe607a6248e8d245b4cf6cc577976" align="middle">
<img src="https://picx.zhimg.com/v2-b929d968669e2b149f33a6274912f3b5" align="middle">
<img src="https://picx.zhimg.com/v2-1dc32c5362436ac7c2b379c190761738" align="middle">
<img src="https://picx.zhimg.com/v2-70db45b84367e154e4855dbb14ebd866" align="middle">
<img src="https://picx.zhimg.com/v2-eae2feeb0f72817e5aeec64800d50f63" align="middle">
<img src="https://picx.zhimg.com/v2-e1915574be6e9ebf87469a9148d4bc7e" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f56b9e6ef23d90258166f86d0b6688f1" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-78be9d29ee5d64ce3b0b6b95e241ffc0" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  VLF-MSC Vision-Language Feature-Based Multimodal Semantic Communication System
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33446.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
