<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-04-24  CAPO Cost-Aware Prompt Optimization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-5978f89427404591c207ba4f2ff32435.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-04-27
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    29 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-24-更新"><a href="#2025-04-24-更新" class="headerlink" title="2025-04-24 更新"></a>2025-04-24 更新</h1><h2 id="CAPO-Cost-Aware-Prompt-Optimization"><a href="#CAPO-Cost-Aware-Prompt-Optimization" class="headerlink" title="CAPO: Cost-Aware Prompt Optimization"></a>CAPO: Cost-Aware Prompt Optimization</h2><p><strong>Authors:Tom Zehle, Moritz Schlager, Timo Heiß, Matthias Feurer</strong></p>
<p>Large language models (LLMs) have revolutionized natural language processing by solving a wide range of tasks simply guided by a prompt. Yet their performance is highly sensitive to prompt formulation. While automated prompt optimization addresses this challenge by finding optimal prompts, current methods require a substantial number of LLM calls and input tokens, making prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt Optimization), an algorithm that enhances prompt optimization efficiency by integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as operators, incorporating racing to save evaluations and multi-objective optimization to balance performance with prompt length. It jointly optimizes instructions and few-shot examples while leveraging task descriptions for improved robustness. Our extensive experiments across diverse datasets and LLMs demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization methods in 11&#x2F;15 cases with improvements up to 21%p. Our algorithm achieves better performances already with smaller budgets, saves evaluations through racing, and decreases average prompt length via a length penalty, making it both cost-efficient and cost-aware. Even without few-shot examples, CAPO outperforms its competitors and generally remains robust to initial prompts. CAPO represents an important step toward making prompt optimization more powerful and accessible by improving cost-efficiency. </p>
<blockquote>
<p>大型语言模型（LLM）通过简单的提示指导解决了多种任务，从而彻底改变了自然语言处理的格局。然而，它们的性能对提示的制定非常敏感。虽然自动提示优化可以通过找到最佳提示来解决这一挑战，但当前的方法需要大量的LLM调用和输入令牌，这使得提示优化成本高昂。我们引入了CAPO（成本感知提示优化），这是一种通过集成AutoML技术提高提示优化效率的算法。CAPO是一种进化方法，以LLM作为操作员，结合了比赛以节省评估和多目标优化来平衡性能与提示长度。它联合优化指令和少量示例，并利用任务描述来提高稳健性。我们在多个数据集和LLM上进行的广泛实验表明，在15种情况下，CAPO在11种情况下优于最先进的离散提示优化方法，性能提高了高达21%。我们的算法在较小的预算下即可实现更好的性能，通过比赛节省评估，并通过长度惩罚减少平均提示长度，使其既经济又注重成本。即使没有少量示例，CAPO也能超越竞争对手，并且对初始提示保持稳健。CAPO是朝着使提示优化更加强大和可访问性的重要一步，通过提高成本效益来改善。</p>
</blockquote>
<p><strong>简化解释</strong></p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16005v1">PDF</a> Submitted to AutoML 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）通过提示引导解决了一系列任务，但其性能对提示制定高度敏感。为解决此挑战，虽存在自动化提示优化方法，但它们需要大量的LLM调用和输入令牌，导致提示优化成本高昂。我们引入CAPO（成本感知提示优化），通过集成AutoML技术提高提示优化效率。CAPO采用进化方法，以LLM作为操作员，结合竞赛以节省评估和多重目标优化来平衡性能和提示长度。它联合优化指令和少量示例，并利用任务描述提高稳健性。实验表明，CAPO在多种数据集和LLM上的表现优于最新离散提示优化方法，在15次中有11次表现更佳，提升幅度达21%。CAPO算法在较小的预算下实现更好的性能，通过竞赛节省评估，并通过长度惩罚减少平均提示长度，既节约成本又提高效率。即使在没有少量示例的情况下，CAPO也能超越竞争对手，并对初始提示保持稳健。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）通过提示引导解决多种任务，但提示制定对其性能有重要影响。</li>
<li>自动化提示优化方法虽能解决挑战，但需要大量LLM调用和输入令牌，导致成本高昂。</li>
<li>引入CAPO算法，结合AutoML技术提高提示优化效率。</li>
<li>CAPO采用进化方法，以LLM作为操作员，结合竞赛机制节省评估，实现成本效益。</li>
<li>CAPO通过联合优化指令和少量示例，提高性能并增强稳健性。</li>
<li>实验表明，CAPO在多种数据集和LLM上的表现优于其他方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16005">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f90ee0a4dafd8331bae268d1b3aa19dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b116fd3254b73260dad8b5aba0aa636b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fba975ea5ea648e831c5bf75967937cd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Few-shot-Hate-Speech-Detection-Based-on-the-MindSpore-Framework"><a href="#Few-shot-Hate-Speech-Detection-Based-on-the-MindSpore-Framework" class="headerlink" title="Few-shot Hate Speech Detection Based on the MindSpore Framework"></a>Few-shot Hate Speech Detection Based on the MindSpore Framework</h2><p><strong>Authors:Zhenkai Qin, Dongze Wu, Yuxin Liu, Guifang Yang</strong></p>
<p>The proliferation of hate speech on social media poses a significant threat to online communities, requiring effective detection systems. While deep learning models have shown promise, their performance often deteriorates in few-shot or low-resource settings due to reliance on large annotated corpora. To address this, we propose MS-FSLHate, a prompt-enhanced neural framework for few-shot hate speech detection implemented on the MindSpore deep learning platform. The model integrates learnable prompt embeddings, a CNN-BiLSTM backbone with attention pooling, and synonym-based adversarial data augmentation to improve generalization. Experimental results on two benchmark datasets-HateXplain and HSOL-demonstrate that our approach outperforms competitive baselines in precision, recall, and F1-score. Additionally, the framework shows high efficiency and scalability, suggesting its suitability for deployment in resource-constrained environments. These findings highlight the potential of combining prompt-based learning with adversarial augmentation for robust and adaptable hate speech detection in few-shot scenarios. </p>
<blockquote>
<p>社交媒体上仇恨言论的泛滥对在线社区构成了重大威胁，需要有效的检测系统。深度学习模型虽展现出了一定的潜力，但在小样本或资源匮乏的环境中，由于其依赖大量标注语料，性能往往会下降。为解决这一问题，我们提出了基于MindSpore深度学习平台的MS-FSLHate，这是一个用于小样本仇恨言论检测的提示增强神经网络框架。该模型集成了可学习的提示嵌入、带有注意力池化的CNN-BiLSTM主干和基于同义词的对抗数据增强，以提高泛化能力。在HateXplain和HSOL两个基准数据集上的实验结果表明，我们的方法在精确度、召回率和F1分数方面优于竞争基线。此外，该框架显示出高效性和可扩展性，表明它适合在资源受限的环境中部署。这些发现强调了将基于提示的学习与对抗性增强相结合，在少量样本情况下实现稳健和适应性的仇恨言论检测的巨大潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15987v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>社交媒体上仇恨言论的泛滥对在线社区构成重大威胁，需要有效的检测系统进行应对。针对深度学习模型在少样本或资源匮乏环境下的性能下降问题，提出了基于MindSpore深度学习平台的MS-FSLHate框架。该框架结合了可学习的提示嵌入、带有注意力池化的CNN-BiLSTM主干和基于同义词的对抗数据增强技术，以提高模型的泛化能力。在HateXplain和HSOL两个基准数据集上的实验结果表明，该方法在精度、召回率和F1分数方面优于竞争对手的基础模型。此外，该框架还表现出高效能和可扩展性，适合在资源受限的环境中部署。这些发现突显了将提示学习和对抗性增强相结合，在少样本场景下实现稳健和可适应的仇恨言论检测的巨大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>社交媒体上的仇恨言论对在线社区构成威胁，需要有效的检测手段。</li>
<li>当前深度学习模型在少样本或资源受限环境下性能下降。</li>
<li>提出了基于MindSpore平台的MS-FSLHate框架，用于少样本仇恨言论检测。</li>
<li>MS-FSLHate框架结合了提示嵌入、CNN-BiLSTM和注意力池化技术。</li>
<li>通过同义词对抗数据增强技术提高模型泛化能力。</li>
<li>在两个基准数据集上的实验结果表明，该方法性能优于其他模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15987">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1f1f78f0c72ab36dcc4aa168ec69fc0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7978599e130f339c2a83dc1b7b93f1b9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DINOv2-powered-Few-Shot-Semantic-Segmentation-A-Unified-Framework-via-Cross-Model-Distillation-and-4D-Correlation-Mining"><a href="#DINOv2-powered-Few-Shot-Semantic-Segmentation-A-Unified-Framework-via-Cross-Model-Distillation-and-4D-Correlation-Mining" class="headerlink" title="DINOv2-powered Few-Shot Semantic Segmentation: A Unified Framework via   Cross-Model Distillation and 4D Correlation Mining"></a>DINOv2-powered Few-Shot Semantic Segmentation: A Unified Framework via   Cross-Model Distillation and 4D Correlation Mining</h2><p><strong>Authors:Wei Zhuo, Zhiyue Tang, Wufeng Xue, Hao Ding, Linlin Shen</strong></p>
<p>Few-shot semantic segmentation has gained increasing interest due to its generalization capability, i.e., segmenting pixels of novel classes requiring only a few annotated images. Prior work has focused on meta-learning for support-query matching, with extensive development in both prototype-based and aggregation-based methods. To address data scarcity, recent approaches have turned to foundation models to enhance representation transferability for novel class segmentation. Among them, a hybrid dual-modal framework including both DINOv2 and SAM has garnered attention due to their complementary capabilities. We wonder “can we build a unified model with knowledge from both foundation models?” To this end, we propose FS-DINO, with only DINOv2’s encoder and a lightweight segmenter. The segmenter features a bottleneck adapter, a meta-visual prompt generator based on dense similarities and semantic embeddings, and a decoder. Through coarse-to-fine cross-model distillation, we effectively integrate SAM’s knowledge into our lightweight segmenter, which can be further enhanced by 4D correlation mining on support-query pairs. Extensive experiments on COCO-20i, PASCAL-5i, and FSS-1000 demonstrate the effectiveness and superiority of our method. </p>
<blockquote>
<p>少量样本语义分割由于其泛化能力而日益受到关注，即只需要少量标注图像就能对新型类别的像素进行分割。前期的研究工作主要集中在基于支持向量机与查询匹配的元学习，并在基于原型和基于聚合的方法中均有大量开发。为了解决数据稀缺问题，近期的方法已转向基础模型，以增强新型类别分割的表示迁移性。其中，包括DINOv2和SAM的混合双模态框架因其互补能力而受到关注。我们想知道“我们能否建立一个统一模型，融合两种基础模型的知识？”为此，我们提出了FS-DINO，它仅采用DINOv2的编码器和轻量级分割器。分割器具有瓶颈适配器、基于密集相似性和语义嵌入的元视觉提示生成器以及解码器。通过粗到细的跨模型蒸馏，我们有效地将SAM的知识融入我们的轻量级分割器中，通过支持查询对上的4D关联挖掘可以进一步增强其功能。在COCO-20i、PASCAL-5i和FSS-1000上的大量实验证明了我们方法的有效性和优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15669v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了基于基础模型的少样本语义分割方法。通过引入DINOv2和SAM两种基础模型的混合双模态框架，结合一个轻量级分割器构成FS-DINO模型。模型使用了一种新颖的粗到细的跨模型蒸馏技术，并融入了SAM的知识，同时采用支持查询对的4D相关性挖掘进行增强。实验结果表明，该方法在COCO-20i、PASCAL-5i和FSS-1000数据集上表现出优越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>少样本语义分割（Few-shot Semantic Segmentation）具有广阔的应用前景和重要的研究价值。</li>
<li>基于基础模型的代表迁移性技术成为了解决数据稀缺问题的关键。</li>
<li>DINOv2和SAM两种基础模型的混合双模态框架具有互补优势，被广泛应用于少样本语义分割任务。</li>
<li>FS-DINO模型结合了DINOv2的编码器和轻量级分割器，实现了高效的知识整合。</li>
<li>模型引入了粗到细的跨模型蒸馏技术，有效地提升了分割性能。</li>
<li>通过结合SAM的知识和采用支持查询对的4D相关性挖掘技术，模型性能得到了进一步提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15669">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-270dac8f19d24cf891747ae7412093d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1b3d4db6d74d97dc00881c9c258be31.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c62ddbd4ca531111a14c1fc0cab979b2.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Vision-Language-Action-Incremental-Policy-Learning"><a href="#Few-Shot-Vision-Language-Action-Incremental-Policy-Learning" class="headerlink" title="Few-Shot Vision-Language Action-Incremental Policy Learning"></a>Few-Shot Vision-Language Action-Incremental Policy Learning</h2><p><strong>Authors:Mingchen Song, Xiang Deng, Guoqiang Zhong, Qi Lv, Jia Wan, Yinchuan Li, Jianye Hao, Weili Guan</strong></p>
<p>Recently, Transformer-based robotic manipulation methods utilize multi-view spatial representations and language instructions to learn robot motion trajectories by leveraging numerous robot demonstrations. However, the collection of robot data is extremely challenging, and existing methods lack the capability for continuous learning on new tasks with only a few demonstrations. In this paper, we formulate these challenges as the Few-Shot Action-Incremental Learning (FSAIL) task, and accordingly design a Task-prOmpt graPh evolutIon poliCy (TOPIC) to address these issues. Specifically, to address the data scarcity issue in robotic imitation learning, TOPIC learns Task-Specific Prompts (TSP) through the deep interaction of multi-modal information within few-shot demonstrations, thereby effectively extracting the task-specific discriminative information. On the other hand, to enhance the capability for continual learning on new tasks and mitigate the issue of catastrophic forgetting, TOPIC adopts a Continuous Evolution Strategy (CES). CES leverages the intrinsic relationships between tasks to construct a task relation graph, which effectively facilitates the adaptation of new tasks by reusing skills learned from previous tasks. TOPIC pioneers few-shot continual learning in the robotic manipulation task, and extensive experimental results demonstrate that TOPIC outperforms state-of-the-art baselines by over 26$%$ in success rate, significantly enhancing the continual learning capabilities of existing Transformer-based policies. </p>
<blockquote>
<p>最近，基于Transformer的机器人操作方法利用多视角空间表示和语言指令，通过大量的机器人演示来学习机器人的运动轨迹。然而，机器人数据的收集极具挑战性，现有方法缺乏仅通过少数演示就在新任务上进行持续学习的能力。在本文中，我们将这些挑战制定为Few-Shot Action-Incremental Learning（FSAIL）任务，并相应地设计了一种Task-prOmpt graPh evolutIon poliCy（TOPIC）来解决这些问题。具体来说，为了解决机器人模仿学习中的数据稀缺问题，TOPIC通过少数演示中的多模态信息的深度交互学习特定任务提示（TSP），从而有效地提取特定任务的判别信息。另一方面，为了提高对新任务的持续学习能力并缓解灾难性遗忘的问题，TOPIC采用了一种持续进化策略（CES）。CES利用任务之间的内在关系构建任务关系图，这有效地促进了新任务的适应，并通过对以前任务中学到的技能进行再利用来实现这一点。TOPIC在机器人操作任务中的小样本持续学习方面具有开创性，大量的实验结果证明，TOPIC在成功率方面优于最新基线26%以上，显著提高了现有基于Transformer的策略的持续学习能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15517v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于Transformer的机器人操控方法利用多视角空间表征和语言指令，通过大量机器人演示学习机器人运动轨迹。然而，机器人数据的收集极具挑战性，现有方法缺乏在新任务上仅通过几次演示进行持续学习的能力。本文提出将这些挑战表述为少量动作增量学习任务（FSAIL），并设计了任务提示图演化策略（TOPIC）来解决这些问题。TOPIC通过解决机器人模仿学习中的数据稀缺问题，并通过少量演示中的多模态信息的深度交互来学习特定任务提示（TSP），从而有效地提取特定任务的判别信息。另一方面，为了增强对新任务的持续学习能力并缓解灾难性遗忘问题，TOPIC采用了一种连续进化策略（CES）。CES利用任务之间的内在关系构建任务关系图，有效地促进了新任务的适应，并通过对以前任务中学到的技能的再利用来适应新任务。TOPIC在机器人操控任务中开创了少量持续学习的先河，广泛的实验结果证明，TOPIC在成功率上超过了最先进的基线模型超过26%，显著提高了现有基于Transformer的策略的持续学习能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer-based方法利用多视角空间表征和语言指令学习机器人运动轨迹。</li>
<li>现有方法面临数据收集挑战，缺乏在新任务上的持续学习能力。</li>
<li>本文提出FSAIL任务来解决这些挑战。</li>
<li>TOPIC通过深度交互学习特定任务提示（TSP）来解决数据稀缺问题。</li>
<li>TOPIC采用连续进化策略（CES）以增强对新任务的持续学习能力并缓解灾难性遗忘。</li>
<li>TOPIC通过构建任务关系图来适应新任务，利用以前任务中学到的技能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15517">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f6a19d30dc48e2434317cc97a136b3ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a46c39aaee6e9787a6f12008427f8d8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f375454b4a966f602bf94aa71ad0878.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5978f89427404591c207ba4f2ff32435.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Manifold-Induced-Biases-for-Zero-shot-and-Few-shot-Detection-of-Generated-Images"><a href="#Manifold-Induced-Biases-for-Zero-shot-and-Few-shot-Detection-of-Generated-Images" class="headerlink" title="Manifold Induced Biases for Zero-shot and Few-shot Detection of   Generated Images"></a>Manifold Induced Biases for Zero-shot and Few-shot Detection of   Generated Images</h2><p><strong>Authors:Jonathan Brokman, Amit Giloni, Omer Hofman, Roman Vainshtein, Hisashi Kojima, Guy Gilboa</strong></p>
<p>Distinguishing between real and AI-generated images, commonly referred to as ‘image detection’, presents a timely and significant challenge. Despite extensive research in the (semi-)supervised regime, zero-shot and few-shot solutions have only recently emerged as promising alternatives. Their main advantage is in alleviating the ongoing data maintenance, which quickly becomes outdated due to advances in generative technologies. We identify two main gaps: (1) a lack of theoretical grounding for the methods, and (2) significant room for performance improvements in zero-shot and few-shot regimes. Our approach is founded on understanding and quantifying the biases inherent in generated content, where we use these quantities as criteria for characterizing generated images. Specifically, we explore the biases of the implicit probability manifold, captured by a pre-trained diffusion model. Through score-function analysis, we approximate the curvature, gradient, and bias towards points on the probability manifold, establishing criteria for detection in the zero-shot regime. We further extend our contribution to the few-shot setting by employing a mixture-of-experts methodology. Empirical results across 20 generative models demonstrate that our method outperforms current approaches in both zero-shot and few-shot settings. This work advances the theoretical understanding and practical usage of generated content biases through the lens of manifold analysis. </p>
<blockquote>
<p>区分真实图像和人工智能生成的图像，通常被称为“图像检测”，这是一个及时且重要的挑战。尽管在（半）监督体制下进行了大量研究，但零样本和少样本解决方案最近才作为有前途的替代方案出现。它们的主要优势在于缓解了正在进行的数据维护，由于生成技术的不断进步，这些数据很快会过时。我们确定了两个主要差距：（1）这些方法缺乏理论支撑，（2）零样本和少样本体制下的性能改进空间很大。我们的方法建立在理解和量化生成内容所固有的偏见上，我们使用这些量作为表征生成图像的标准。具体来说，我们探索了由预训练扩散模型捕获的隐概率流形偏见。通过评分函数分析，我们近似概率流形上的点处的曲率、梯度和偏见，建立零样本状态下的检测标准。我们通过采用混合专家方法进一步将我们的贡献扩展到少样本设置。在20个生成模型上的经验结果表明，我们的方法在零样本和少样本环境下均优于当前方法。这项工作通过流形分析的角度，推进了生成内容偏见的理论理解和实践应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15470v1">PDF</a> Accepted to ICLR 2025 (The International Conference on Learning   Representations)</p>
<p><strong>Summary</strong></p>
<p>该文本探讨了区分真实图像和人工智能生成图像的挑战，特别是零样本和少样本解决方案的兴起。研究团队利用生成内容中的固有偏见，通过探索预训练扩散模型的隐概率流形偏见，建立零样本体制下的检测标准。此外，该研究还将贡献扩展到少样本设置，采用专家混合方法。经验结果表明，该方法在零样本和少样本设置中都优于当前方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>区分真实和AI生成的图像（即图像检测）是一个及时且重要的挑战。</li>
<li>零样本和少样本解决方案作为对监督学习的有前途的替代方案已经兴起。</li>
<li>生成内容的偏见被用作检测生成图像的标准。</li>
<li>研究团队利用预训练扩散模型的隐概率流形的偏见进行了探索。</li>
<li>通过分数函数分析，研究团队建立了零样本体制下的检测标准。</li>
<li>研究将贡献扩展到少样本设置，采用专家混合方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15470">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d9ff5a9175d553f77043c4f719180c52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8e18012d3e7c1ca7ee3736223aa0c26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea90bd09638f3dc67cc37da40227441b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27a019bc4cefc9d08a846418b46078e4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Bayesian-Cross-Modal-Alignment-Learning-for-Few-Shot-Out-of-Distribution-Generalization"><a href="#Bayesian-Cross-Modal-Alignment-Learning-for-Few-Shot-Out-of-Distribution-Generalization" class="headerlink" title="Bayesian Cross-Modal Alignment Learning for Few-Shot Out-of-Distribution   Generalization"></a>Bayesian Cross-Modal Alignment Learning for Few-Shot Out-of-Distribution   Generalization</h2><p><strong>Authors:Lin Zhu, Xinbing Wang, Chenghu Zhou, Nanyang Ye</strong></p>
<p>Recent advances in large pre-trained models showed promising results in few-shot learning. However, their generalization ability on two-dimensional Out-of-Distribution (OoD) data, i.e., correlation shift and diversity shift, has not been thoroughly investigated. Researches have shown that even with a significant amount of training data, few methods can achieve better performance than the standard empirical risk minimization method (ERM) in OoD generalization. This few-shot OoD generalization dilemma emerges as a challenging direction in deep neural network generalization research, where the performance suffers from overfitting on few-shot examples and OoD generalization errors. In this paper, leveraging a broader supervision source, we explore a novel Bayesian cross-modal image-text alignment learning method (Bayes-CAL) to address this issue. Specifically, the model is designed as only text representations are fine-tuned via a Bayesian modelling approach with gradient orthogonalization loss and invariant risk minimization (IRM) loss. The Bayesian approach is essentially introduced to avoid overfitting the base classes observed during training and improve generalization to broader unseen classes. The dedicated loss is introduced to achieve better image-text alignment by disentangling the causal and non-casual parts of image features. Numerical experiments demonstrate that Bayes-CAL achieved state-of-the-art OoD generalization performances on two-dimensional distribution shifts. Moreover, compared with CLIP-like models, Bayes-CAL yields more stable generalization performances on unseen classes. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/LinLLLL/BayesCAL">https://github.com/LinLLLL/BayesCAL</a>. </p>
<blockquote>
<p>近期大型预训练模型在少量样本学习方面展现出令人瞩目的成果。然而，它们在二维域外数据（即关联转移和多样性转移）上的泛化能力尚未得到充分研究。研究表明，即使使用大量的训练数据，也几乎没有方法能在域外泛化方面取得优于标准经验风险最小化方法（经验风险最小化，简称 ERM）的表现。这种小样本的域外泛化困境成为了深度神经网络泛化研究中的一个具有挑战性的方向，该方向的问题在于从少数样本例子中过拟合以及域外泛化错误。在本文中，我们借助更广泛的监督源，探索了一种新型的贝叶斯跨模态图像文本对齐学习方法（贝叶斯交叉对齐学习法，简称Bayes-CAL），以解决这一问题。具体来说，该模型设计仅通过文本表示进行微调，采用贝叶斯建模方法，结合梯度正交损失和不变风险最小化（IRM）损失。引入贝叶斯方法主要是为了避免对训练时观察到的基本类别的过拟合，并提高对更广泛未见类别的泛化能力。通过引入专用损失来实现更好的图像文本对齐，通过分离图像特征的有因果部分和无因果部分。数值实验表明，Bayes-CAL在二维分布转移上实现了最先进的域外泛化性能。此外，与CLIP类似模型相比，Bayes-CAL在未见类别上表现出更稳定的泛化性能。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/LinLLLL/BayesCAL">https://github.com/LinLLLL/BayesCAL</a>找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09448v2">PDF</a> Accepted by AAAI2023</p>
<p><strong>Summary</strong></p>
<p>大型预训练模型在少样本学习领域展现出令人瞩目的成果，但在二维域外数据（Out-of-Distribution，OoD）上的泛化能力，尤其是面对关联转移和多样性转移时，尚待深入研究。即使使用大量训练数据，目前只有少数方法能在OoD泛化上超越传统的经验风险最小化方法（Empirical Risk Minimization，ERM）。本文提出了一种新颖的贝叶斯跨模态图像文本对齐学习方法（Bayes-CAL），以应对这一难题。模型设计仅通过文本表示进行微调，采用贝叶斯建模方法，引入梯度正交化和不变风险最小化（Invariant Risk Minimization，IRM）损失来避免对训练时观察到的基本类别的过度拟合，并改善对更广泛未见类别的泛化。此外，通过解耦图像特征的因果和非因果部分，实现了更好的图像文本对齐。数值实验表明，Bayes-CAL在二维分布转移上实现了最先进的OoD泛化性能，并且在未见类别上相较于CLIP类模型表现出更稳定的泛化性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型预训练模型在少样本学习上表现出优异结果，但在面对二维域外数据的泛化能力方面存在不足。</li>
<li>当前仅有少数方法能超越传统的经验风险最小化方法在OoD泛化方面的表现。</li>
<li>研究提出了一种贝叶斯跨模态图像文本对齐学习方法（Bayes-CAL）来解决这一问题。</li>
<li>Bayes-CAL模型仅通过文本表示进行微调设计，并采用贝叶斯建模方法和特定的损失函数来改善泛化能力并避免过度拟合。</li>
<li>Bayes-CAL实现了更好的图像文本对齐，通过解耦图像特征的因果和非因果部分。</li>
<li>数值实验证明，Bayes-CAL在二维分布转移上达到了最先进的OoD泛化性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09448">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f6b76455b56f4b55b147801c8a377011.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f1d6c0d5930d0fb819c3021ea70a9bf4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b891a3a2161a59bffb4c75048b9266a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4612dd2d42ff89f02d93d348d3e71b8d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Tratto-A-Neuro-Symbolic-Approach-to-Deriving-Axiomatic-Test-Oracles"><a href="#Tratto-A-Neuro-Symbolic-Approach-to-Deriving-Axiomatic-Test-Oracles" class="headerlink" title="Tratto: A Neuro-Symbolic Approach to Deriving Axiomatic Test Oracles"></a>Tratto: A Neuro-Symbolic Approach to Deriving Axiomatic Test Oracles</h2><p><strong>Authors:Davide Molinelli, Alberto Martin-Lopez, Elliott Zackrone, Beyza Eken, Michael D. Ernst, Mauro Pezzè</strong></p>
<p>This paper presents Tratto, a neuro-symbolic approach that generates assertions (boolean expressions) that can serve as axiomatic oracles, from source code and documentation. The symbolic module of Tratto takes advantage of the grammar of the programming language, the unit under test, and the context of the unit (its class and available APIs) to restrict the search space of the tokens that can be successfully used to generate valid oracles. The neural module of Tratto uses transformers fine-tuned for both deciding whether to output an oracle or not and selecting the next lexical token to incrementally build the oracle from the set of tokens returned by the symbolic module. Our experiments show that Tratto outperforms the state-of-the-art axiomatic oracle generation approaches, with 73% accuracy, 72% precision, and 61% F1-score, largely higher than the best results of the symbolic and neural approaches considered in our study (61%, 62%, and 37%, respectively). Tratto can generate three times more axiomatic oracles than current symbolic approaches, while generating 10 times less false positives than GPT4 complemented with few-shot learning and Chain-of-Thought prompting. </p>
<blockquote>
<p>本文介绍了Tratto，这是一种神经符号方法，能够从源代码和文档生成断言（布尔表达式），这些断言可以作为公理神谕。Tratto的符号模块利用编程语言的语法、测试单元以及单元上下文（其类和可用的API）来限制成功生成有效神谕的标记搜索空间。Tratto的神经网络模块使用微调过的转换器来决定是否输出神谕，并从符号模块返回的标记集中逐步构建神谕，选择下一个词汇标记。我们的实验表明，Tratto在准确率（73%）、精确率（72%）和F1分数（61%）方面超越了最新的公理神谕生成方法，大大高于我们研究中考虑的最佳符号方法和神经方法的相应指标（分别为61%、62%和37%）。Tratto能够生成比当前符号方法多三倍的公理神谕，同时产生的误报数量比结合了少量学习和思维链提示的GPT4少十倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04251v2">PDF</a> Accepted for publication at ISSTA 2025</p>
<p><strong>Summary</strong></p>
<p>Tratto是一种神经符号方法，可从源代码和文档生成断言（布尔表达式），作为公理判定器。它包含符号模块和神经模块，前者利用编程语言语法、测试单元以及上下文（类别和可用API）来限制可成功用于生成有效判定器的令牌搜索空间；后者使用微调过的转换器来决定是否输出判定器，并从符号模块返回的令牌集中逐步构建判定器。实验表明，Tratto在准确性、精确度和F1分数方面优于最新最先进的公理判定器生成方法，生成的三倍于当前符号方法的公理判定器，同时产生的误报数量是GPT4结合少样本学习和思维链提示的十分之一。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Tratto是一种神经符号方法，可以从源代码和文档生成断言（布尔表达式）。</li>
<li>Tratto包含符号模块和神经模块，分别利用编程语言和上下文信息来生成有效的断言。</li>
<li>实验结果显示Tratto在公理判定器生成方面优于其他方法，具有更高的准确性、精确度和F1分数。</li>
<li>Tratto可以生成三倍于当前符号方法的公理判定器数量。</li>
<li>Tratto产生的误报数量远低于GPT4结合少样本学习和思维链提示的方法。</li>
<li>Tratto的符号模块利用编程语言的语法、测试单元以及上下文来限制令牌的搜索空间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04251">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a6d1d0d4c31bef59d35d876e0deaf5e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94f2b00c5b5f8959b56d7a91ba92c79a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-24/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-24/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-24/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f5c9edf5ea8efbf4dcd4fa2bb26d13bb.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-04-24  Rotational ultrasound and photoacoustic tomography of the human body
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-24/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-79ae2874afc5327f8afa097f761ba08a.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-04-24  Parallel Corpora for Machine Translation in Low-resource Indic   Languages A Comprehensive Review
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">16668k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
