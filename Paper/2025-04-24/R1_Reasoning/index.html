<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-24  TTRL Test-Time Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-2d83a63f87db38a77ea2777480d0ec25.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-24-æ›´æ–°"><a href="#2025-04-24-æ›´æ–°" class="headerlink" title="2025-04-24 æ›´æ–°"></a>2025-04-24 æ›´æ–°</h1><h2 id="TTRL-Test-Time-Reinforcement-Learning"><a href="#TTRL-Test-Time-Reinforcement-Learning" class="headerlink" title="TTRL: Test-Time Reinforcement Learning"></a>TTRL: Test-Time Reinforcement Learning</h2><p><strong>Authors:Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, Bowen Zhou</strong></p>
<p>This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks, and highlight TTRLâ€™s potential for broader tasks and domains. GitHub: <a target="_blank" rel="noopener" href="https://github.com/PRIME-RL/TTRL">https://github.com/PRIME-RL/TTRL</a> </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†ä»»åŠ¡ä¸­ï¼Œåœ¨æ— æ˜ç¡®æ ‡ç­¾æ•°æ®ä¸Šçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åº”ç”¨ã€‚è¯¥é—®é¢˜çš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºåœ¨æ— æ³•è·å–çœŸå®ä¿¡æ¯çš„æƒ…å†µä¸‹è¿›è¡Œæ¨ç†æ—¶çš„å¥–åŠ±ä¼°ç®—ã€‚å°½ç®¡è¿™ä¸ªè®¾å®šçœ‹ä¼¼éš¾ä»¥æ‰æ‘¸ï¼Œä½†æˆ‘ä»¬å‘ç°æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰çš„å¸¸è§åšæ³•ï¼Œå¦‚å¤šæ•°æŠ•ç¥¨ï¼Œå¯ä»¥äº§ç”Ÿä»¤äººæƒŠè®¶çš„æœ‰æ•ˆå¥–åŠ±ï¼Œé€‚åˆé©±åŠ¨RLè®­ç»ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æµ‹è¯•æ—¶é—´å¼ºåŒ–å­¦ä¹ ï¼ˆTTRLï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šè®­ç»ƒLLMçš„æ–°å‹æ–¹æ³•ã€‚TTRLåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„å…ˆéªŒçŸ¥è¯†ï¼Œå®ç°LLMçš„è‡ªæˆ‘è¿›åŒ–ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒTTRLåœ¨å„ç§ä»»åŠ¡å’Œæ¨¡å‹ä¸Šçš„æ€§èƒ½æŒç»­æé«˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨AIME 2024ä¸Šï¼ŒTTRLå°†Qwen-2.5-Math-7Bçš„pass@1æ€§èƒ½æé«˜äº†çº¦159%ï¼Œä»…ä½¿ç”¨æ— æ ‡ç­¾çš„æµ‹è¯•æ•°æ®ã€‚æ­¤å¤–ï¼Œå°½ç®¡TTRLä»…å—åˆ°Maj@NæŒ‡æ ‡çš„ç›‘ç£ï¼Œä½†å…¶è¡¨ç°ä¸€ç›´è¶…è¿‡åˆå§‹æ¨¡å‹çš„ä¸Šé™ï¼Œå¹¶æ¥è¿‘ç›´æ¥åœ¨æµ‹è¯•æ•°æ®ä¸Šä½¿ç”¨çœŸå®æ ‡ç­¾è®­ç»ƒçš„æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜TTRLåœ¨å¤šç§ä»»åŠ¡ä¸­çš„æ™®éæœ‰æ•ˆæ€§ï¼Œå¹¶çªå‡ºäº†å…¶åœ¨æ›´å¹¿æ³›çš„ä»»åŠ¡å’Œé¢†åŸŸä¸­çš„æ½œåŠ›ã€‚GitHubï¼š<a target="_blank" rel="noopener" href="https://github.com/PRIME-RL/TTRL">https://github.com/PRIME-RL/TTRL</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16084v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä»»åŠ¡ç ”ç©¶ã€‚æœ¬è®ºæ–‡è§£å†³çš„æ ¸å¿ƒé—®é¢˜æ˜¯æ¨ç†è¿‡ç¨‹ä¸­å¥–åŠ±ä¼°è®¡çš„éš¾é¢˜ï¼Œå³åœ¨ç¼ºä¹çœŸå®ä¿¡æ¯çš„æƒ…å†µä¸‹è¿›è¡Œæ¨æ–­ã€‚ç ”ç©¶å‘ç°ï¼Œæµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰çš„å¸¸è§åšæ³•ï¼Œå¦‚å¤šæ•°æŠ•ç¥¨ï¼Œèƒ½äº§ç”Ÿæœ‰æ•ˆçš„å¥–åŠ±ï¼Œé€‚ç”¨äºé©±åŠ¨RLè®­ç»ƒã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°å‹è®­ç»ƒLLMçš„æ–¹æ³•â€”â€”æµ‹è¯•æ—¶é—´å¼ºåŒ–å­¦ä¹ ï¼ˆTTRLï¼‰ï¼Œè¯¥æ–¹æ³•åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šè¿›è¡Œè‡ªæˆ‘è¿›åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒTTRLåœ¨å„ç§ä»»åŠ¡å’Œæ¨¡å‹ä¸Šçš„è¡¨ç°å‡æœ‰æ‰€æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°æå‡æ˜¾è‘—ã€‚æ½œåœ¨çš„åº”ç”¨é¢†åŸŸå¹¿æ³›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶è§£å†³äº†å¼ºåŒ–å­¦ä¹ åœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šçš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†ä»»åŠ¡çš„å¥–åŠ±ä¼°è®¡éš¾é¢˜ã€‚</li>
<li>æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰çš„å¸¸è§æ–¹æ³•ï¼Œå¦‚å¤šæ•°æŠ•ç¥¨ï¼Œèƒ½æœ‰æ•ˆç”Ÿæˆå¥–åŠ±ä¿¡å·ç”¨äºRLè®­ç»ƒã€‚</li>
<li>ç ”ç©¶æå‡ºäº†æµ‹è¯•æ—¶é—´å¼ºåŒ–å­¦ä¹ ï¼ˆTTRLï¼‰è¿™ä¸€æ–°å‹LLMè®­ç»ƒæ–¹æ³•ã€‚</li>
<li>TTRLåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†å®ç°è‡ªæˆ‘è¿›åŒ–ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒTTRLåœ¨å„ç§ä»»åŠ¡å’Œæ¨¡å‹ä¸Šçš„è¡¨ç°å‡æœ‰æ‰€æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æå‡æ˜¾è‘—ã€‚</li>
<li>TTRLçš„æ½œåŠ›ä¸ä»…é™äºç‰¹å®šä»»åŠ¡ï¼Œå¯åº”ç”¨äºæ›´å¹¿æ³›çš„é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16084">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-713d31245df3d20743ffdff1446f4e5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4102386456ea92688350b766d5f061d4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e35354102e822a30d5786a5d54dc315e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="PHYBench-Holistic-Evaluation-of-Physical-Perception-and-Reasoning-in-Large-Language-Models"><a href="#PHYBench-Holistic-Evaluation-of-Physical-Perception-and-Reasoning-in-Large-Language-Models" class="headerlink" title="PHYBench: Holistic Evaluation of Physical Perception and Reasoning in   Large Language Models"></a>PHYBench: Holistic Evaluation of Physical Perception and Reasoning in   Large Language Models</h2><p><strong>Authors:Shi Qiu, Shaoyang Guo, Zhuo-Yang Song, Yunbo Sun, Zeyu Cai, Jiashen Wei, Tianyu Luo, Yixuan Yin, Haoxu Zhang, Yi Hu, Chenyang Wang, Chencheng Tang, Haoling Chang, Qi Liu, Ziheng Zhou, Tianyu Zhang, Jingtian Zhang, Zhangyi Liu, Minghao Li, Yuku Zhang, Boxuan Jing, Xianqi Yin, Yutong Ren, Zizhuo Fu, Weike Wang, Xudong Tian, Anqi Lv, Laifu Man, Jianxiang Li, Feiyu Tao, Qihua Sun, Zhou Liang, Yushu Mu, Zhongxuan Li, Jing-Jun Zhang, Shutao Zhang, Xiaotian Li, Xingqi Xia, Jiawei Lin, Zheyu Shen, Jiahang Chen, Qiuhao Xiong, Binran Wang, Fengyuan Wang, Ziyang Ni, Bohan Zhang, Fan Cui, Changkun Shao, Qing-Hong Cao, Ming-xing Luo, Muhan Zhang, Hua Xing Zhu</strong></p>
<p>We introduce PHYBench, a novel, high-quality benchmark designed for evaluating reasoning capabilities of large language models (LLMs) in physical contexts. PHYBench consists of 500 meticulously curated physics problems based on real-world physical scenarios, designed to assess the ability of models to understand and reason about realistic physical processes. Covering mechanics, electromagnetism, thermodynamics, optics, modern physics, and advanced physics, the benchmark spans difficulty levels from high school exercises to undergraduate problems and Physics Olympiad challenges. Additionally, we propose the Expression Edit Distance (EED) Score, a novel evaluation metric based on the edit distance between mathematical expressions, which effectively captures differences in model reasoning processes and results beyond traditional binary scoring methods. We evaluate various LLMs on PHYBench and compare their performance with human experts. Our results reveal that even state-of-the-art reasoning models significantly lag behind human experts, highlighting their limitations and the need for improvement in complex physical reasoning scenarios. Our benchmark results and dataset are publicly available at <a target="_blank" rel="noopener" href="https://phybench-official.github.io/phybench-demo/">https://phybench-official.github.io/phybench-demo/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†PHYBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„é«˜è´¨é‡åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç‰©ç†ç¯å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›ã€‚PHYBenchåŒ…å«500ä¸ªç²¾å¿ƒç­–åˆ’çš„ç‰©ç†é—®é¢˜ï¼ŒåŸºäºç°å®ä¸–ç•Œçš„ç‰©ç†åœºæ™¯è®¾è®¡ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹ç†è§£å’Œæ¨ç†ç°å®ç‰©ç†è¿‡ç¨‹çš„èƒ½åŠ›ã€‚è¿™äº›é—®é¢˜æ¶µç›–äº†åŠ›å­¦ã€ç”µç£å­¦ã€çƒ­åŠ›å­¦ã€å…‰å­¦ã€ç°ä»£ç‰©ç†å­¦å’Œé«˜çº§ç‰©ç†å­¦ï¼Œéš¾åº¦ä»é«˜ä¸­ç»ƒä¹ åˆ°å¤§å­¦é—®é¢˜å’Œç‰©ç†å¥¥æ—åŒ¹å…‹æŒ‘æˆ˜ä¸ç­‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†è¡¨è¾¾å¼ç¼–è¾‘è·ç¦»ï¼ˆEEDï¼‰åˆ†æ•°ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ•°å­¦è¡¨è¾¾å¼ä¹‹é—´ç¼–è¾‘è·ç¦»çš„æ–°å‹è¯„ä¼°æŒ‡æ ‡ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ•è·æ¨¡å‹æ¨ç†è¿‡ç¨‹å’Œç»“æœä¹‹é—´çš„å·®å¼‚ï¼Œè€Œä¼ ç»Ÿçš„äºŒå…ƒè¯„åˆ†æ–¹æ³•åˆ™æ— æ³•åšåˆ°ã€‚æˆ‘ä»¬åœ¨PHYBenchä¸Šè¯„ä¼°äº†å„ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸äººç±»ä¸“å®¶çš„è¡¨ç°è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€æ–°çš„äººå·¥æ™ºèƒ½æ¨ç†æ¨¡å‹ä¹Ÿè¿œè¿œè½åäºäººç±»ä¸“å®¶ï¼Œè¿™çªæ˜¾äº†å®ƒä»¬åœ¨å¤æ‚ç‰©ç†æ¨ç†åœºæ™¯ä¸­çš„å±€é™æ€§ä»¥åŠæ”¹è¿›çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ç»“æœå’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://phybench-official.github.io/phybench-demo/%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://phybench-official.github.io/phybench-demo/å…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16074v1">PDF</a> 21 pages ,8 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>PHYBenchæ˜¯ä¸€ä¸ªé’ˆå¯¹ç‰©ç†åœºæ™¯çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›è¯„ä¼°çš„æ–°å‹é«˜è´¨é‡åŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…å«500ä¸ªç²¾å¿ƒç­–åˆ’çš„ç‰©ç†é—®é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹ç†è§£å’Œæ¨ç†ç°å®ç‰©ç†è¿‡ç¨‹çš„èƒ½åŠ›ï¼Œæ¶µç›–ä»é«˜ä¸­ä¹ é¢˜åˆ°å¤§å­¦ç‰©ç†é—®é¢˜å’Œç‰©ç†å¥¥æ—åŒ¹å…‹ç«èµ›çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†åŸºäºæ•°å­¦è¡¨è¾¾å¼ç¼–è¾‘è·ç¦»çš„æ–°å‹è¯„ä¼°æŒ‡æ ‡â€”â€”è¡¨è¾¾ç¼–è¾‘è·ç¦»ï¼ˆEEDï¼‰åˆ†æ•°ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåœ°æ•æ‰äº†æ¨¡å‹æ¨ç†è¿‡ç¨‹å’Œç»“æœçš„å·®å¼‚ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„äºŒå…ƒè¯„åˆ†æ–¹æ³•ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹ï¼Œå…¶è¡¨ç°ä¹Ÿæ˜¾è‘—è½åäºäººç±»ä¸“å®¶ï¼Œè¿™çªæ˜¾äº†å¤æ‚ç‰©ç†æ¨ç†åœºæ™¯ä¸­çš„å±€é™æ€§å’Œæ”¹è¿›çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PHYBenchæ˜¯ä¸€ä¸ªæ–°å‹çš„åŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç‰©ç†åœºæ™¯ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®ƒåŒ…å«500ä¸ªåŸºäºçœŸå®ä¸–ç•Œç‰©ç†åœºæ™¯çš„é—®é¢˜ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹ç†è§£å’Œæ¨ç†ç°å®ç‰©ç†è¿‡ç¨‹çš„èƒ½åŠ›ã€‚</li>
<li>æ¶µç›–å¤šä¸ªç‰©ç†é¢†åŸŸï¼ŒåŒ…æ‹¬åŠ›å­¦ã€ç”µç£å­¦ã€çƒ­åŠ›å­¦ã€å…‰å­¦ã€ç°ä»£ç‰©ç†å­¦å’Œé«˜çº§ç‰©ç†å­¦ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æŒ‡æ ‡â€”â€”è¡¨è¾¾ç¼–è¾‘è·ç¦»ï¼ˆEEDï¼‰åˆ†æ•°ï¼Œç”¨äºæ›´å‡†ç¡®åœ°è¯„ä¼°æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡PHYBenchè¯„ä¼°çš„LLMè¡¨ç°æ™®éè½åäºäººç±»ä¸“å®¶ï¼Œè¿™çªæ˜¾äº†å¤æ‚ç‰©ç†æ¨ç†é¢†åŸŸçš„æŒ‘æˆ˜å’Œæ¨¡å‹å±€é™æ€§ã€‚</li>
<li>è¿™ä¸ªåŸºå‡†æµ‹è¯•å’Œç›¸å…³çš„æ•°æ®é›†æ˜¯å…¬å¼€å¯ç”¨çš„ï¼Œä»¥ä¾¿ç ”ç©¶äººå‘˜è¿›è¡Œè¿›ä¸€æ­¥çš„è¯„ä¼°å’Œæ¯”è¾ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bc1014d144ec4cac59bcf7a0e4406715.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-834480df6042d19a39c7bec930fdb536.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0d86daddc550a226bb7fee443c9939c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-369136749d935196905d3daaad9073bf.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Vision-Language-Models-Are-Not-Pragmatically-Competent-in-Referring-Expression-Generation"><a href="#Vision-Language-Models-Are-Not-Pragmatically-Competent-in-Referring-Expression-Generation" class="headerlink" title="Vision-Language Models Are Not Pragmatically Competent in Referring   Expression Generation"></a>Vision-Language Models Are Not Pragmatically Competent in Referring   Expression Generation</h2><p><strong>Authors:Ziqiao Ma, Jing Ding, Xuejun Zhang, Dezhi Luo, Jiahe Ding, Sihan Xu, Yuchen Huang, Run Peng, Joyce Chai</strong></p>
<p>Referring Expression Generation (REG) is a core task for evaluating the pragmatic competence of vision-language systems, requiring not only accurate semantic grounding but also adherence to principles of cooperative communication (Grice, 1975). However, current evaluations of vision-language models (VLMs) often overlook the pragmatic dimension, reducing REG to a region-based captioning task and neglecting Gricean maxims. In this work, we revisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of 1.5k images annotated with both written and spoken referring expressions. Through a systematic evaluation of state-of-the-art VLMs, we identify three key failures of pragmatic competence: (1) failure to uniquely identify the referent, (2) inclusion of excessive or irrelevant information, and (3) misalignment with human pragmatic preference, such as the underuse of minimal spatial cues. We also show that standard automatic evaluations fail to capture these pragmatic violations, reinforcing superficial cues rather than genuine referential success. Our findings call for a renewed focus on pragmatically informed models and evaluation frameworks that align with real human communication. </p>
<blockquote>
<p>æŒ‡ä»£è¡¨è¾¾å¼ç”Ÿæˆï¼ˆREGï¼‰æ˜¯è¯„ä¼°è§†è§‰è¯­è¨€ç³»ç»Ÿè¯­ç”¨èƒ½åŠ›çš„æ ¸å¿ƒä»»åŠ¡ï¼Œè¿™ä¸ä»…è¦æ±‚å‡†ç¡®çš„è¯­ä¹‰å®šä½ï¼Œè¿˜è¦æ±‚éµå¾ªåˆä½œæ²Ÿé€šçš„åŸåˆ™ï¼ˆGriceï¼Œ1975ï¼‰ã€‚ç„¶è€Œï¼Œå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å½“å‰è¯„ä¼°å¾€å¾€å¿½ç•¥äº†è¯­ç”¨ç»´åº¦ï¼Œå°†REGç®€åŒ–ä¸ºåŸºäºåŒºåŸŸçš„æè¿°ä»»åŠ¡ï¼Œå¹¶å¿½ç•¥äº†Griceançš„åŸåˆ™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»è¯­ç”¨å­¦çš„è§’åº¦é‡æ–°å®¡è§†REGï¼Œä»‹ç»äº†ä¸€ä¸ªåŒ…å«1500å¼ å›¾åƒçš„æ–°æ•°æ®é›†ï¼ˆRefOIï¼‰ï¼Œè¿™äº›å›¾åƒéƒ½æ³¨æœ‰ä¹¦é¢å’Œå£è¯­æŒ‡ä»£è¡¨è¾¾å¼ã€‚é€šè¿‡å¯¹æœ€å…ˆè¿›çš„VLMçš„ç³»ç»Ÿè¯„ä¼°ï¼Œæˆ‘ä»¬ç¡®å®šäº†ä¸‰ç§å…³é”®çš„è¯­ç”¨èƒ½åŠ›ç¼ºå¤±ï¼šï¼ˆ1ï¼‰æ— æ³•å”¯ä¸€åœ°è¯†åˆ«å‚ç…§ç‰©ï¼Œï¼ˆ2ï¼‰åŒ…å«è¿‡å¤šæˆ–æ— å…³çš„ä¿¡æ¯ï¼Œä»¥åŠï¼ˆ3ï¼‰ä¸äººç±»è¯­ç”¨åå¥½ä¸ä¸€è‡´ï¼Œå¦‚è¿‡åº¦ä½¿ç”¨æœ€å°‘çš„ç©ºé—´çº¿ç´¢ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæ ‡å‡†çš„è‡ªåŠ¨è¯„ä¼°æ— æ³•æ•æ‰åˆ°è¿™äº›è¯­ç”¨è¿è§„è¡Œä¸ºï¼Œåè€Œä¼šå¼ºåŒ–è¡¨å±‚çº¿ç´¢ï¼Œè€Œä¸æ˜¯çœŸæ­£çš„æŒ‡ä»£æˆåŠŸã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå‘¼åäººä»¬é‡æ–°å…³æ³¨è¯­ç”¨ä¿¡æ¯ä¸°å¯Œçš„æ¨¡å‹å’Œè¯„ä¼°æ¡†æ¶ï¼Œä½¿å…¶ä¸çœŸå®çš„äººç±»æ²Ÿé€šç›¸ä¸€è‡´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16060v1">PDF</a> Homepage: <a target="_blank" rel="noopener" href="https://vlm-reg.github.io/">https://vlm-reg.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨è§†è§‰è¯­è¨€ç³»ç»Ÿçš„æ ¸å¿ƒä»»åŠ¡â€”â€”æŒ‡ä»£è¡¨è¾¾ç”Ÿæˆï¼ˆREGï¼‰ï¼ŒæŒ‡å‡ºå½“å‰çš„è¯„ä»·ä½“ç³»å¿½è§†äº†è¯­è¨€çš„å®é™…è¿ç”¨åœºæ™¯å’Œåˆä½œæ²Ÿé€šåŸåˆ™ã€‚ä½œè€…ä»å®ç”¨è§’åº¦é‡æ–°å®¡è§†REGï¼Œå¹¶å¼•å…¥æ–°çš„æ•°æ®é›†RefOIï¼ŒåŒ…å«1.5kå¼ å›¾åƒæ ‡æ³¨çš„ä¹¦é¢å’Œå£è¯­æŒ‡ä»£è¡¨è¾¾ã€‚é€šè¿‡å¯¹ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹çš„å…¨é¢è¯„ä¼°ï¼Œå‘ç°ä¸‰å¤§ç¼ºä¹å®ç”¨èƒ½åŠ›çš„è¡¨ç°ï¼šæ— æ³•å‡†ç¡®è¯†åˆ«æŒ‡ä»£å¯¹è±¡ã€åŒ…å«è¿‡å¤šæˆ–æ— å…³ä¿¡æ¯ä»¥åŠä¸äººç±»å®ç”¨åå¥½ä¸ç¬¦ã€‚åŒæ—¶æŒ‡å‡ºæ ‡å‡†è‡ªåŠ¨è¯„ä¼°éš¾ä»¥æ•æ‰è¿™äº›å®ç”¨è¿åæƒ…å†µï¼Œå¼ºè°ƒéœ€è¦é‡æ–°å…³æ³¨å®ç”¨è§’åº¦çš„æ¨¡å‹å’Œè¯„ä¼°æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è§†è§‰è¯­è¨€ç³»ç»Ÿï¼ˆVLMï¼‰çš„è¯„ä»·ä½“ç³»å¿½è§†äº†è¯­è¨€çš„å®é™…è¿ç”¨åœºæ™¯å’Œåˆä½œæ²Ÿé€šåŸåˆ™ã€‚</li>
<li>ä»£æŒ‡è¡¨è¾¾ç”Ÿæˆï¼ˆREGï¼‰æ˜¯è¯„ä»·è§†è§‰è¯­è¨€ç³»ç»Ÿçš„é‡è¦ä»»åŠ¡ä¹‹ä¸€ã€‚</li>
<li>ä»‹ç»äº†æ–°çš„æ•°æ®é›†RefOIï¼Œç”¨äºç ”ç©¶å®ç”¨è§†è§’ä¸‹çš„REGä»»åŠ¡ã€‚</li>
<li>ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨REGä»»åŠ¡ä¸­å­˜åœ¨ä¸‰å¤§é—®é¢˜ï¼šæ— æ³•å‡†ç¡®è¯†åˆ«æŒ‡ä»£å¯¹è±¡ã€åŒ…å«è¿‡å¤šæˆ–æ— å…³ä¿¡æ¯ä»¥åŠä¸äººç±»å®ç”¨åå¥½ä¸ç¬¦ã€‚</li>
<li>æ ‡å‡†è‡ªåŠ¨è¯„ä¼°æ–¹æ³•éš¾ä»¥æ•æ‰æ¨¡å‹åœ¨å®ç”¨æ–¹é¢çš„ç¼ºé™·ã€‚</li>
<li>æ¨¡å‹å’Œè¯„ä¼°æ¡†æ¶éœ€è¦é‡æ–°å…³æ³¨å®ç”¨è§’åº¦ï¼Œä»¥æ›´å¥½åœ°ä¸äººç±»æ²Ÿé€šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16060">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3503a0f3a52079b68abfcd8d715c4fbf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec658b1660843bf61cd35cc845ea8d64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa493bcc08fffdeb30dba69a9324fe41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfd9ad1fb0d94ba6a79c8c2e2fc6c83f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6c8ee1690694322295a9b91cc9801c3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Benchmarking-LLM-for-Code-Smells-Detection-OpenAI-GPT-4-0-vs-DeepSeek-V3"><a href="#Benchmarking-LLM-for-Code-Smells-Detection-OpenAI-GPT-4-0-vs-DeepSeek-V3" class="headerlink" title="Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs   DeepSeek-V3"></a>Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs   DeepSeek-V3</h2><p><strong>Authors:Ahmed R. Sadik, Siddhata Govind</strong></p>
<p>Determining the most effective Large Language Model for code smell detection presents a complex challenge. This study introduces a structured methodology and evaluation matrix to tackle this issue, leveraging a curated dataset of code samples consistently annotated with known smells. The dataset spans four prominent programming languages Java, Python, JavaScript, and C++; allowing for cross language comparison. We benchmark two state of the art LLMs, OpenAI GPT 4.0 and DeepSeek-V3, using precision, recall, and F1 score as evaluation metrics. Our analysis covers three levels of detail: overall performance, category level performance, and individual code smell type performance. Additionally, we explore cost effectiveness by comparing the token based detection approach of GPT 4.0 with the pattern-matching techniques employed by DeepSeek V3. The study also includes a cost analysis relative to traditional static analysis tools such as SonarQube. The findings offer valuable guidance for practitioners in selecting an efficient, cost effective solution for automated code smell detection </p>
<blockquote>
<p>ç¡®å®šç”¨äºä»£ç å¼‚å‘³æ£€æµ‹çš„æœ€æœ‰æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯ä¸€ä¸ªå¤æ‚çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§ç»“æ„åŒ–çš„æ–¹æ³•å’Œè¯„ä¼°çŸ©é˜µæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåˆ©ç”¨ä¸€ç»„ç»è¿‡ç­›é€‰çš„ä»£ç æ ·æœ¬æ•°æ®é›†ï¼Œè¿™äº›ä»£ç æ ·æœ¬ä¸å·²çŸ¥å¼‚å‘³æŒç»­æ ‡æ³¨ã€‚æ•°æ®é›†æ¶µç›–äº†å››ç§æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ï¼ŒåŒ…æ‹¬Javaã€Pythonã€JavaScriptå’ŒC++ï¼Œå¯å®ç°è·¨è¯­è¨€æ¯”è¾ƒã€‚æˆ‘ä»¬ä½¿ç”¨ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°ä½œä¸ºè¯„ä»·æŒ‡æ ‡ï¼Œå¯¹ä¸¤ç§æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹OpenAI GPT 4.0å’ŒDeepSeek-V3è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„åˆ†ææ¶µç›–äº†ä¸‰ä¸ªå±‚æ¬¡çš„ç»†èŠ‚ï¼šæ€»ä½“æ€§èƒ½ã€ç±»åˆ«çº§åˆ«çš„æ€§èƒ½å’Œå•ä¸ªä»£ç å¼‚å‘³ç±»å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡æ¯”è¾ƒGPT 4.0çš„åŸºäºä»¤ç‰Œæ£€æµ‹æ–¹æ³•ä¸DeepSeek V3æ‰€é‡‡ç”¨çš„æ¨¡å¼åŒ¹é…æŠ€æœ¯ï¼Œæ¢è®¨äº†æˆæœ¬æ•ˆç›Šã€‚è¯¥ç ”ç©¶è¿˜åŒ…æ‹¬ç›¸å¯¹äºä¼ ç»Ÿé™æ€åˆ†æå·¥å…·ï¼ˆå¦‚SonarQubeï¼‰çš„æˆæœ¬åˆ†æã€‚ç ”ç©¶ç»“æœä¸ºä»ä¸šè€…é€‰æ‹©é«˜æ•ˆã€ç»æµçš„è‡ªåŠ¨åŒ–ä»£ç å¼‚å‘³æ£€æµ‹è§£å†³æ–¹æ¡ˆæä¾›äº†å®è´µçš„æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16027v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§è¯­è¨€æ¨¡å‹åœ¨ä»£ç å¼‚å‘³æ£€æµ‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¼•å…¥äº†ä¸€ç§ç»“æ„åŒ–çš„æ–¹æ³•å’Œè¯„ä¼°çŸ©é˜µæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§ç»è¿‡æ•´ç†çš„ä»£ç æ ·æœ¬æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†å·²ç”¨å·²çŸ¥çš„å¼‚å‘³è¿›è¡Œæ ‡æ³¨ï¼Œæ¶µç›–äº†å››ç§æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ï¼šJavaã€Pythonã€JavaScriptå’ŒC++ã€‚ç ”ç©¶åˆ†æäº†ä¸¤æ¬¾å…ˆè¿›çš„LLMæ¨¡å‹â€”â€”OpenAI GPT 4.0å’ŒDeepSeek-V3çš„æ€§èƒ½ï¼Œé‡‡ç”¨ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°ä½œä¸ºè¯„ä»·æŒ‡æ ‡ã€‚ç ”ç©¶è¿˜æ¶µç›–äº†æˆæœ¬æ•ˆç›Šåˆ†æï¼Œå°†GPT 4.0çš„åŸºäºä»¤ç‰Œæ£€æµ‹æ–¹æ³•ä¸DeepSeek V3çš„æ¨¡å¼åŒ¹é…æŠ€æœ¯è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶å°†ç»“æœä¸ä¼ ç»Ÿçš„é™æ€åˆ†æå·¥å…·å¦‚SonarQubeè¿›è¡Œäº†å¯¹æ¯”ã€‚æœ¬ç ”ç©¶çš„å‘ç°å¯ä¸ºä»ä¸šäººå‘˜åœ¨é€‰æ‹©é«˜æ•ˆã€ç»æµçš„è‡ªåŠ¨åŒ–ä»£ç å¼‚å‘³æ£€æµ‹è§£å†³æ–¹æ¡ˆæ—¶æä¾›å®è´µæŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„æ–¹æ³•å’Œè¯„ä¼°çŸ©é˜µæ¥ç¡®å®šæœ€æœ‰æ•ˆçš„è¯­è¨€æ¨¡å‹è¿›è¡Œä»£ç å¼‚å‘³æ£€æµ‹ã€‚</li>
<li>ä½¿ç”¨çš„æ•°æ®é›†åŒ…å«å››ç§ä¸»è¦ç¼–ç¨‹è¯­è¨€çš„ä»£ç æ ·æœ¬ï¼Œå®ç°äº†è·¨è¯­è¨€çš„æ¯”è¾ƒã€‚</li>
<li>GPT 4.0å’ŒDeepSeek-V3ä¸¤æ¬¾LLMæ¨¡å‹è¢«é€‰ä¸ºåŸºå‡†æµ‹è¯•ï¼Œé‡‡ç”¨ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°ä½œä¸ºè¯„ä»·æŒ‡æ ‡ã€‚</li>
<li>ç ”ç©¶åˆ†æäº†è¿™ä¸¤æ¬¾æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€åˆ†ç±»çº§åˆ«æ€§èƒ½å’Œå•ä¸ªä»£ç å¼‚å‘³ç±»å‹çš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶æ¯”è¾ƒäº†GPT 4.0çš„åŸºäºä»¤ç‰Œæ£€æµ‹æ–¹æ³•å’ŒDeepSeek V3çš„æ¨¡å¼åŒ¹é…æŠ€æœ¯ï¼Œå¹¶è¿›è¡Œäº†æˆæœ¬æ•ˆç›Šåˆ†æã€‚</li>
<li>ç ”ç©¶è¿˜å°†LLMæ¨¡å‹ä¸ä¼ ç»Ÿé™æ€åˆ†æå·¥å…·å¦‚SonarQubeè¿›è¡Œäº†æˆæœ¬åˆ†æå¯¹æ¯”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16027">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fdf5e1a103b7e289dc9d6445cf2026cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81fd649bcf3459ddcf58165430d3a497.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1da54363ca7ee7c4bc2f8912e805a299.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2eb48ec9ee0c781e1d0f88fecbd0494c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-661dc5058b00d59a9c19bf7f8ede853f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83710ec1f2f17b794fbd554b7f74e817.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Navigating-the-State-of-Cognitive-Flow-Context-Aware-AI-Interventions-for-Effective-Reasoning-Support"><a href="#Navigating-the-State-of-Cognitive-Flow-Context-Aware-AI-Interventions-for-Effective-Reasoning-Support" class="headerlink" title="Navigating the State of Cognitive Flow: Context-Aware AI Interventions   for Effective Reasoning Support"></a>Navigating the State of Cognitive Flow: Context-Aware AI Interventions   for Effective Reasoning Support</h2><p><strong>Authors:Dinithi Dissanayake, Suranga Nanayakkara</strong></p>
<p>Flow theory describes an optimal cognitive state where individuals experience deep focus and intrinsic motivation when a taskâ€™s difficulty aligns with their skill level. In AI-augmented reasoning, interventions that disrupt the state of cognitive flow can hinder rather than enhance decision-making. This paper proposes a context-aware cognitive augmentation framework that adapts interventions based on three key contextual factors: type, timing, and scale. By leveraging multimodal behavioral cues (e.g., gaze behavior, typing hesitation, interaction speed), AI can dynamically adjust cognitive support to maintain or restore flow. We introduce the concept of cognitive flow, an extension of flow theory in AI-augmented reasoning, where interventions are personalized, adaptive, and minimally intrusive. By shifting from static interventions to context-aware augmentation, our approach ensures that AI systems support deep engagement in complex decision-making and reasoning without disrupting cognitive immersion. </p>
<blockquote>
<p>æµåŠ¨ç†è®ºæè¿°äº†ä¸€ç§æœ€ä½³è®¤çŸ¥çŠ¶æ€ï¼Œå½“ä»»åŠ¡çš„éš¾åº¦ä¸ä¸ªäººçš„æŠ€èƒ½æ°´å¹³ç›¸åŒ¹é…æ—¶ï¼Œä¸ªäººä¼šä½“éªŒåˆ°æ·±åº¦é›†ä¸­å’Œå†…åœ¨åŠ¨æœºã€‚åœ¨äººå·¥æ™ºèƒ½è¾…åŠ©æ¨ç†ä¸­ï¼Œç ´åè®¤çŸ¥æµåŠ¨çŠ¶æ€çš„å¹²é¢„æªæ–½å¯èƒ½ä¼šé˜»ç¢è€Œéä¿ƒè¿›å†³ç­–åˆ¶å®šã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è®¤çŸ¥å¢å¼ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ ¹æ®ä¸‰ä¸ªå…³é”®ä¸Šä¸‹æ–‡å› ç´ è°ƒæ•´å¹²é¢„æªæ–½ï¼šç±»å‹ã€æ—¶é—´å’Œè§„æ¨¡ã€‚é€šè¿‡åˆ©ç”¨å¤šæ¨¡å¼è¡Œä¸ºçº¿ç´¢ï¼ˆå¦‚ç›®å…‰è¡Œä¸ºã€æ‰“å­—çŠ¹è±«ã€äº¤äº’é€Ÿåº¦ï¼‰ï¼Œäººå·¥æ™ºèƒ½å¯ä»¥åŠ¨æ€è°ƒæ•´è®¤çŸ¥æ”¯æŒï¼Œä»¥ç»´æŒæˆ–æ¢å¤æµåŠ¨çŠ¶æ€ã€‚æˆ‘ä»¬ä»‹ç»äº†è®¤çŸ¥æµåŠ¨çš„æ¦‚å¿µï¼Œè¿™æ˜¯æµåŠ¨ç†è®ºåœ¨äººå·¥æ™ºèƒ½è¾…åŠ©æ¨ç†ä¸­çš„æ‰©å±•ï¼Œå…¶ä¸­å¹²é¢„æªæ–½å…·æœ‰ä¸ªæ€§åŒ–ã€è‡ªé€‚åº”å’Œæœ€å°‘å¹²æ‰°çš„ç‰¹ç‚¹ã€‚é€šè¿‡ä»é™æ€å¹²é¢„è½¬å‘åŸºäºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¢å¼ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨æ”¯æŒå¤æ‚å†³ç­–å’Œæ¨ç†çš„æ·±å…¥å‚ä¸æ—¶ï¼Œä¸ä¼šç ´åè®¤çŸ¥æ²‰æµ¸ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16021v1">PDF</a> Presented at the 2025 ACM Workshop on Human-AI Interaction for   Augmented Reasoning, Report Number: CHI25-WS-AUGMENTED-REASONING</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†è®¤çŸ¥æµçš„æœ€ä½³çŠ¶æ€ï¼Œå³ä¸ªä½“åœ¨ä»»åŠ¡éš¾åº¦ä¸æŠ€èƒ½æ°´å¹³ç›¸åŒ¹é…æ—¶ä½“éªŒåˆ°æ·±åº¦ä¸“æ³¨å’Œå†…åœ¨åŠ¨æœºçš„çŠ¶æ€ã€‚åœ¨AIè¾…åŠ©æ¨ç†ä¸­ï¼Œå¹²é¢„æªæ–½å¦‚æ‰“æ–­è®¤çŸ¥æµå¯èƒ½ä¼šé˜»ç¢è€Œéä¿ƒè¿›å†³ç­–åˆ¶å®šã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è®¤çŸ¥å¢å¼ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ ¹æ®ç±»å‹ã€æ—¶é—´å’Œè§„æ¨¡ä¸‰ä¸ªå…³é”®ä¸Šä¸‹æ–‡å› ç´ è°ƒæ•´å¹²é¢„æªæ–½ã€‚å€ŸåŠ©å¤šæ¨¡å¼è¡Œä¸ºçº¿ç´¢ï¼ˆå¦‚ç›®å…‰è¡Œä¸ºã€æ‰“å­—çŠ¹è±«ã€äº¤äº’é€Ÿåº¦ï¼‰ï¼ŒAIå¯ä»¥åŠ¨æ€è°ƒæ•´è®¤çŸ¥æ”¯æŒä»¥ç»´æŒæˆ–æ¢å¤è®¤çŸ¥æµã€‚æˆ‘ä»¬ä»‹ç»äº†è®¤çŸ¥æµçš„æ¦‚å¿µï¼Œå®ƒæ˜¯AIè¾…åŠ©æ¨ç†ä¸­æµç†è®ºçš„æ‰©å±•ï¼Œå…¶ä¸­å¹²é¢„æªæ–½å…·æœ‰ä¸ªæ€§åŒ–ã€è‡ªé€‚åº”å’Œæœ€å°‘å¹²æ‰°çš„ç‰¹ç‚¹ã€‚é€šè¿‡ä»é™æ€å¹²é¢„è½¬å‘åŸºäºä¸Šä¸‹æ–‡çš„å¢å¼ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç¡®ä¿AIç³»ç»Ÿåœ¨æ”¯æŒå¤æ‚å†³ç­–å’Œæ¨ç†çš„æ·±åº¦å‚ä¸æ—¶ä¸ä¼šç ´åè®¤çŸ¥æ²‰æµ¸ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµç†è®ºæè¿°äº†ä¸€ç§æœ€ä½³è®¤çŸ¥çŠ¶æ€ï¼Œå³ä¸ªä½“åœ¨ä»»åŠ¡éš¾åº¦ä¸æŠ€èƒ½æ°´å¹³ç›¸åŒ¹é…æ—¶ä½“éªŒæ·±åº¦ä¸“æ³¨å’Œå†…åœ¨åŠ¨æœºã€‚</li>
<li>åœ¨AIè¾…åŠ©æ¨ç†ä¸­ï¼Œæ‰“ç ´è®¤çŸ¥æµçŠ¶æ€çš„å¹²é¢„å¯èƒ½ä¼šé˜»ç¢å†³ç­–åˆ¶å®šã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåŸºäºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è®¤çŸ¥å¢å¼ºæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ ¹æ®ç±»å‹ã€æ—¶é—´å’Œè§„æ¨¡ä¸‰ä¸ªå…³é”®ä¸Šä¸‹æ–‡å› ç´ è°ƒæ•´å¹²é¢„ã€‚</li>
<li>AIå¯ä»¥åˆ©ç”¨å¤šæ¨¡å¼è¡Œä¸ºçº¿ç´¢æ¥åŠ¨æ€è°ƒæ•´è®¤çŸ¥æ”¯æŒï¼Œä»¥ç»´æŒæˆ–æ¢å¤è®¤çŸ¥æµã€‚</li>
<li>è®¤çŸ¥æµæ˜¯AIè¾…åŠ©æ¨ç†ä¸­æµç†è®ºçš„æ‰©å±•ï¼Œå¼ºè°ƒå¹²é¢„æªæ–½çš„ä¸ªæ€§åŒ–ã€è‡ªé€‚åº”å’Œæœ€å°‘å¹²æ‰°ç‰¹ç‚¹ã€‚</li>
<li>é™æ€å¹²é¢„è½¬å‘åŸºäºä¸Šä¸‹æ–‡çš„å¢å¼ºï¼Œç¡®ä¿AIåœ¨æ”¯æŒå¤æ‚å†³ç­–å’Œæ¨ç†æ—¶ä¸ä¼šç ´åè®¤çŸ¥æ²‰æµ¸ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16021">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3729f129eef7e5bc0a7506fbe1e46fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9be64e6d0b39a6b36ecd0038b5c2a37.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="StreamRL-Scalable-Heterogeneous-and-Elastic-RL-for-LLMs-with-Disaggregated-Stream-Generation"><a href="#StreamRL-Scalable-Heterogeneous-and-Elastic-RL-for-LLMs-with-Disaggregated-Stream-Generation" class="headerlink" title="StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with   Disaggregated Stream Generation"></a>StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with   Disaggregated Stream Generation</h2><p><strong>Authors:Yinmin Zhong, Zili Zhang, Xiaoniu Song, Hanpeng Hu, Chao Jin, Bingyang Wu, Nuo Chen, Yukun Chen, Yu Zhou, Changyi Wan, Hongyu Zhou, Yimin Jiang, Yibo Zhu, Daxin Jiang</strong></p>
<p>Reinforcement learning (RL) has become the core post-training technique for large language models (LLMs). RL for LLMs involves two stages: generation and training. The LLM first generates samples online, which are then used to derive rewards for training. The conventional view holds that the colocated architecture, where the two stages share resources via temporal multiplexing, outperforms the disaggregated architecture, in which dedicated resources are assigned to each stage. However, in real-world deployments, we observe that the colocated architecture suffers from resource coupling, where the two stages are constrained to use the same resources. This coupling compromises the scalability and cost-efficiency of colocated RL in large-scale training. In contrast, the disaggregated architecture allows for flexible resource allocation, supports heterogeneous training setups, and facilitates cross-datacenter deployment.   StreamRL is designed with disaggregation from first principles and fully unlocks its potential by addressing two types of performance bottlenecks in existing disaggregated RL frameworks: pipeline bubbles, caused by stage dependencies, and skewness bubbles, resulting from long-tail output length distributions. To address pipeline bubbles, StreamRL breaks the traditional stage boundary in synchronous RL algorithms through stream generation and achieves full overlapping in asynchronous RL. To address skewness bubbles, StreamRL employs an output-length ranker model to identify long-tail samples and reduces generation time via skewness-aware dispatching and scheduling. Experiments show that StreamRL improves throughput by up to 2.66x compared to existing state-of-the-art systems, and improves cost-effectiveness by up to 1.33x in a heterogeneous, cross-datacenter setting. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ¸å¿ƒåè®­ç»ƒæŠ€æœ¯ã€‚LLMçš„RLæ¶‰åŠä¸¤ä¸ªé˜¶æ®µï¼šç”Ÿæˆå’Œè®­ç»ƒã€‚LLMé¦–å…ˆåœ¨çº¿ç”Ÿæˆæ ·æœ¬ï¼Œç„¶åç”¨äºæ¨å¯¼è®­ç»ƒå¥–åŠ±ã€‚ä¼ ç»Ÿè§‚ç‚¹è®¤ä¸ºï¼Œå…±ç½®æ¶æ„ï¼ˆä¸¤ä¸ªé˜¶æ®µé€šè¿‡æ—¶é—´å¤ç”¨å…±äº«èµ„æºï¼‰ä¼˜äºåˆ†æ•£æ¶æ„ï¼ˆä¸ºæ¯ä¸ªé˜¶æ®µåˆ†é…ä¸“ç”¨èµ„æºï¼‰ã€‚ä½†åœ¨å®é™…éƒ¨ç½²ä¸­ï¼Œæˆ‘ä»¬å‘ç°å…±ç½®æ¶æ„å­˜åœ¨èµ„æºè€¦åˆçš„é—®é¢˜ï¼Œä¸¤ä¸ªé˜¶æ®µè¢«çº¦æŸä¸ºä½¿ç”¨ç›¸åŒèµ„æºã€‚è¿™ç§è€¦åˆé™ä½äº†å…±ç½®RLåœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­çš„å¯æ‰©å±•æ€§å’Œæˆæœ¬æ•ˆç›Šã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåˆ†æ•£æ¶æ„å…è®¸çµæ´»çš„èµ„æºåˆ†é…ï¼Œæ”¯æŒå¼‚æ„è®­ç»ƒè®¾ç½®ï¼Œå¹¶ä¾¿äºè·¨æ•°æ®ä¸­å¿ƒéƒ¨ç½²ã€‚StreamRLä»ç¬¬ä¸€æ€§åŸåˆ™å‡ºå‘è¿›è¡Œåˆ†æ•£è®¾è®¡ï¼Œå¹¶é€šè¿‡è§£å†³ç°æœ‰åˆ†æ•£RLæ¡†æ¶ä¸­çš„ä¸¤ç§æ€§èƒ½ç“¶é¢ˆæ¥å……åˆ†å‘æŒ¥å…¶æ½œåŠ›ï¼šç”±é˜¶æ®µä¾èµ–æ€§å¼•èµ·çš„ç®¡é“æ°”æ³¡å’Œç”±é•¿å°¾è¾“å‡ºé•¿åº¦åˆ†å¸ƒå¯¼è‡´çš„åæ€æ°”æ³¡ã€‚ä¸ºè§£å†³ç®¡é“æ°”æ³¡é—®é¢˜ï¼ŒStreamRLæ‰“ç ´äº†åŒæ­¥RLç®—æ³•ä¸­çš„ä¼ ç»Ÿé˜¶æ®µç•Œé™ï¼Œé€šè¿‡æµç”Ÿæˆå®ç°å¼‚æ­¥RLçš„å®Œå…¨é‡å ã€‚ä¸ºè§£å†³åæ€æ°”æ³¡é—®é¢˜ï¼ŒStreamRLé‡‡ç”¨è¾“å‡ºé•¿åº¦æ’åæ¨¡å‹æ¥è¯†åˆ«é•¿å°¾å·´æ ·æœ¬ï¼Œå¹¶é€šè¿‡åæ€æ„ŸçŸ¥çš„åˆ†æ´¾å’Œè°ƒåº¦å‡å°‘ç”Ÿæˆæ—¶é—´ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„ç³»ç»Ÿç›¸æ¯”ï¼ŒStreamRLçš„ååé‡æé«˜äº†é«˜è¾¾2.66å€ï¼Œåœ¨å¼‚æ„ã€è·¨æ•°æ®ä¸­å¿ƒè®¾ç½®ä¸­ï¼Œæˆæœ¬æ•ˆç›Šæé«˜äº†é«˜è¾¾1.33å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15930v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åè®­ç»ƒä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œä»‹ç»äº†RL for LLMçš„ç”Ÿæˆå’Œè®­ç»ƒä¸¤ä¸ªé˜¶æ®µã€‚æ–‡ç« æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„å…±ç½®æ¶æ„åœ¨èµ„æºåˆ†é…ä¸Šå­˜åœ¨è€¦åˆé—®é¢˜ï¼Œå½±å“å¯æ‰©å±•æ€§å’Œæˆæœ¬æ•ˆç›Šã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåˆ†æ•£å¼æ¶æ„å…è®¸çµæ´»çš„èµ„æºåˆ†é…ï¼Œå¹¶æ”¯æŒè·¨æ•°æ®ä¸­å¿ƒéƒ¨ç½²ã€‚StreamRLä»åŸºæœ¬åŸåˆ™ä¸Šé‡‡ç”¨åˆ†æ•£å¼è®¾è®¡ï¼Œè§£å†³äº†ç°æœ‰åˆ†æ•£å¼RLæ¡†æ¶ä¸­çš„ä¸¤ç§æ€§èƒ½ç“¶é¢ˆé—®é¢˜ï¼ŒåŒ…æ‹¬ç®¡é“æ°”æ³¡å’Œåæ–œæ°”æ³¡ã€‚StreamRLé€šè¿‡æµç”Ÿæˆæ‰“ç ´äº†ä¼ ç»Ÿé˜¶æ®µçš„è¾¹ç•Œï¼Œå¹¶é‡‡ç”¨äº†è¾“å‡ºé•¿åº¦æ’åæ¨¡å‹æ¥è¯†åˆ«é•¿å°¾æ ·æœ¬ï¼Œä»è€Œå‡å°‘ç”Ÿæˆæ—¶é—´ã€‚å®éªŒè¡¨æ˜ï¼ŒStreamRLåœ¨ååé‡æ–¹é¢æé«˜äº†æœ€å¤š2.66å€ï¼Œåœ¨å¼‚æ„ã€è·¨æ•°æ®ä¸­å¿ƒç¯å¢ƒä¸­æˆæœ¬æ•ˆç›Šæé«˜äº†æœ€å¤š1.33å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å·²æˆä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒåè®­ç»ƒæŠ€æœ¯ã€‚</li>
<li>ä¼ ç»Ÿå…±ç½®æ¶æ„åœ¨èµ„æºåˆ†é…ä¸Šå­˜åœ¨è€¦åˆé—®é¢˜ï¼Œå½±å“å¯æ‰©å±•æ€§å’Œæˆæœ¬æ•ˆç›Šã€‚</li>
<li>åˆ†æ•£å¼æ¶æ„å…è®¸çµæ´»çš„èµ„æºåˆ†é…ï¼Œå¹¶æ”¯æŒè·¨æ•°æ®ä¸­å¿ƒéƒ¨ç½²ã€‚</li>
<li>StreamRLè§£å†³äº†ç°æœ‰åˆ†æ•£å¼RLæ¡†æ¶ä¸­çš„ç®¡é“æ°”æ³¡å’Œåæ–œæ°”æ³¡é—®é¢˜ã€‚</li>
<li>StreamRLé€šè¿‡æµç”Ÿæˆæ‰“ç ´äº†ä¼ ç»Ÿé˜¶æ®µçš„è¾¹ç•Œï¼Œæé«˜äº†æ€§èƒ½ã€‚</li>
<li>StreamRLé‡‡ç”¨è¾“å‡ºé•¿åº¦æ’åæ¨¡å‹æ¥è¯†åˆ«é•¿å°¾æ ·æœ¬ï¼Œä¼˜åŒ–ç”Ÿæˆæ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4b2d6cb656ce6c14b846ab0d87bc6dca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b17b450c0c293890148ee1d88b449c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-560430be7ff9506725807578149e07fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bc3b303ff1ec74201ae4336d8b33ee5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10ee40462255895fc9969ee62fac69aa.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Towards-Test-Generation-from-Task-Description-for-Mobile-Testing-with-Multi-modal-Reasoning"><a href="#Towards-Test-Generation-from-Task-Description-for-Mobile-Testing-with-Multi-modal-Reasoning" class="headerlink" title="Towards Test Generation from Task Description for Mobile Testing with   Multi-modal Reasoning"></a>Towards Test Generation from Task Description for Mobile Testing with   Multi-modal Reasoning</h2><p><strong>Authors:Hieu Huynh, Hai Phung, Hao Pham, Tien N. Nguyen, Vu Nguyen</strong></p>
<p>In Android GUI testing, generating an action sequence for a task that can be replayed as a test script is common. Generating sequences of actions and respective test scripts from task goals described in natural language can eliminate the need for manually writing test scripts. However, existing approaches based on large language models (LLM) often struggle with identifying the final action, and either end prematurely or continue past the final screen. In this paper, we introduce VisiDroid, a multi-modal, LLM-based, multi-agent framework that iteratively determines the next action and leverages visual images of screens to detect the taskâ€™s completeness. The multi-modal approach enhances our model in two significant ways. First, this approach enables it to avoid prematurely terminating a task when textual content alone provides misleading indications of task completion. Additionally, visual input helps the tool avoid errors when changes in the GUI do not directly affect functionality toward task completion, such as adjustments to font sizes or colors. Second, the multi-modal approach also ensures the tool not progress beyond the final screen, which might lack explicit textual indicators of task completion but could display a visual element indicating task completion, which is common in GUI apps. Our evaluation shows that VisiDroid achieves an accuracy of 87.3%, outperforming the best baseline relatively by 23.5%. We also demonstrate that our multi-modal framework with images and texts enables the LLM to better determine when a task is completed. </p>
<blockquote>
<p>åœ¨Android GUIæµ‹è¯•ä¸­ï¼Œä¸ºä»»åŠ¡ç”Ÿæˆå¯é‡æ’­ä¸ºæµ‹è¯•è„šæœ¬çš„åŠ¨ä½œåºåˆ—æ˜¯å¾ˆå¸¸è§çš„ã€‚ä»è‡ªç„¶è¯­è¨€æè¿°çš„ä»»åŠ¡ç›®æ ‡ä¸­ç”ŸæˆåŠ¨ä½œåºåˆ—å’Œç›¸åº”çš„æµ‹è¯•è„šæœ¬ï¼Œå¯ä»¥æ¶ˆé™¤æ‰‹åŠ¨ç¼–å†™æµ‹è¯•è„šæœ¬çš„éœ€æ±‚ã€‚ç„¶è€Œï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥è¯†åˆ«æœ€ç»ˆåŠ¨ä½œï¼Œå¹¶ä¸”è¦ä¹ˆè¿‡æ—©ç»“æŸï¼Œè¦ä¹ˆç»§ç»­è¶…è¿‡æœ€ç»ˆå±å¹•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†VisiDroidï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMçš„å¤šæ¨¡å¼ã€å¤šä»£ç†æ¡†æ¶ï¼Œå®ƒè¿­ä»£åœ°ç¡®å®šä¸‹ä¸€ä¸ªåŠ¨ä½œå¹¶åˆ©ç”¨å±å¹•è§†è§‰å›¾åƒæ¥æ£€æµ‹ä»»åŠ¡çš„å®Œæˆç¨‹åº¦ã€‚å¤šæ¨¡å¼æ–¹æ³•åœ¨ä¸¤ä¸ªé‡è¦æ–¹é¢å¢å¼ºäº†æˆ‘ä»¬çš„æ¨¡å‹ã€‚é¦–å…ˆï¼Œè¿™ç§æ–¹æ³•ä½¿å…¶èƒ½å¤Ÿé¿å…ä»…å‡­æ–‡æœ¬å†…å®¹æä¾›è¯¯å¯¼ä»»åŠ¡å®Œæˆçš„è¿¹è±¡æ—¶è¿‡æ—©ç»ˆæ­¢ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè§†è§‰è¾“å…¥æœ‰åŠ©äºå·¥å…·åœ¨GUIæ›´æ”¹ä¸ç›´æ¥å½±å“ä»»åŠ¡å®Œæˆçš„åŠŸèƒ½æ—¶é¿å…é”™è¯¯ï¼Œä¾‹å¦‚è°ƒæ•´å­—ä½“å¤§å°æˆ–é¢œè‰²ã€‚å…¶æ¬¡ï¼Œå¤šæ¨¡å¼æ–¹æ³•è¿˜ç¡®ä¿å·¥å…·ä¸ä¼šè¶…è¶Šæœ€ç»ˆå±å¹•ï¼Œè¿™å¯èƒ½ç¼ºå°‘æ˜ç¡®çš„æ–‡æœ¬æŒ‡ç¤ºä»»åŠ¡å®Œæˆï¼Œä½†å¯èƒ½ä¼šæ˜¾ç¤ºè¡¨ç¤ºä»»åŠ¡å®Œæˆçš„è§†è§‰å…ƒç´ ï¼Œè¿™åœ¨GUIåº”ç”¨ç¨‹åºä¸­å¾ˆå¸¸è§ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒVisiDroidçš„å‡†ç¡®åº¦è¾¾åˆ°87.3%ï¼Œç›¸å¯¹äºæœ€ä½³åŸºçº¿æé«˜äº†23.5%ã€‚æˆ‘ä»¬è¿˜è¯æ˜ï¼Œæˆ‘ä»¬çš„å…·æœ‰å›¾åƒå’Œæ–‡æœ¬çš„å¤šæ¨¡å¼æ¡†æ¶ä½¿LLMèƒ½å¤Ÿæ›´å¥½åœ°ç¡®å®šä»»åŠ¡ä½•æ—¶å®Œæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15917v1">PDF</a> Under review for a conference</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†VisiDroidï¼Œä¸€ç§åŸºäºå¤šæ¨¡æ€å’Œå¤šä»£ç†çš„æ¡†æ¶ï¼Œç”¨äºåœ¨Android GUIæµ‹è¯•ä¸­ç”Ÿæˆå¯é‡æ’­çš„æµ‹è¯•è„šæœ¬ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿­ä»£ç¡®å®šä¸‹ä¸€æ­¥æ“ä½œï¼Œå¹¶åˆ©ç”¨å±å¹•è§†è§‰å›¾åƒæ£€æµ‹ä»»åŠ¡çš„å®Œæˆæƒ…å†µã€‚å…¶å¤šæ¨¡æ€æ–¹æ³•å¢å¼ºäº†æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œé¿å…æå‰ç»ˆæ­¢ä»»åŠ¡å¹¶é¿å…åœ¨GUIä¸å½±å“ä»»åŠ¡å®Œæˆæ—¶çš„åŠŸèƒ½å˜åŒ–æ—¶çš„é”™è¯¯ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥ç¡®ä¿ä¸ä¼šè¿›å±•åˆ°æœ€åç¼ºå°‘ä»»åŠ¡å®Œæˆæ–‡æœ¬æŒ‡ç¤ºçš„å±å¹•ï¼Œä½†å¯ä»¥æ˜¾ç¤ºè¡¨ç¤ºä»»åŠ¡å®Œæˆçš„è§†è§‰å…ƒç´ ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒVisiDroidçš„å‡†ç¡®åº¦è¾¾åˆ°87.3%ï¼Œç›¸å¯¹äºæœ€ä½³åŸºçº¿æé«˜äº†23.5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<pre><code> 1. VisiDroidæ˜¯ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å’Œå¤šä»£ç†çš„æ¡†æ¶ï¼Œç”¨äºåœ¨Android GUIæµ‹è¯•ä¸­ç”Ÿæˆå¯é‡æ’­çš„æµ‹è¯•è„šæœ¬ã€‚
 2. è¯¥æ¡†æ¶é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¿­ä»£ç¡®å®šä¸‹ä¸€æ­¥æ“ä½œï¼Œå¹¶ç»“åˆå±å¹•è§†è§‰å›¾åƒæ£€æµ‹ä»»åŠ¡çš„å®Œæˆæƒ…å†µã€‚
 3. å¤šæ¨¡æ€æ–¹æ³•å¢å¼ºäº†æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œé¿å…æå‰ç»ˆæ­¢ä»»åŠ¡æˆ–é”™è¯¯åœ°åˆ¤æ–­ä»»åŠ¡å®Œæˆæƒ…å†µã€‚
 4. è§†è§‰è¾“å…¥æœ‰åŠ©äºå·¥å…·é¿å…åœ¨GUIå˜åŒ–ä¸å½±å“ä»»åŠ¡å®Œæˆæ—¶äº§ç”Ÿé”™è¯¯åˆ¤æ–­ï¼Œå¦‚å­—ä½“å¤§å°æˆ–é¢œè‰²çš„è°ƒæ•´ã€‚
 5. è¯¥æ¡†æ¶èƒ½å¤Ÿè¯†åˆ«ä»»åŠ¡çš„æœ€ç»ˆåŠ¨ä½œå¹¶ç¡®ä¿ä¸ä¼šè¿›å±•åˆ°æœ€åå±å¹•ï¼Œå³ä½¿ç¼ºå°‘æ–‡æœ¬æŒ‡ç¤ºä¹Ÿèƒ½é€šè¿‡è§†è§‰å…ƒç´ åˆ¤æ–­ä»»åŠ¡æ˜¯å¦å®Œæˆã€‚
 6. è¯„ä¼°æ˜¾ç¤ºVisiDroidçš„å‡†ç¡®åº¦è¾¾åˆ°87.3%ï¼Œç›¸å¯¹äºæœ€ä½³åŸºçº¿æé«˜äº†23.5%ã€‚è¿™ä¸€å‡†ç¡®æ€§åæ˜ å‡ºäº†å…¶åœ¨å¤„ç†Android GUIæµ‹è¯•æ—¶çš„ä¼˜å¼‚è¡¨ç°ã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15917">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33aa8b3e1fcd8cd2d35c241ef1db2259.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79383a1bf87d7c25bd4df932c67bcef5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2acd52735e9746561fce61293155a423.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9aee6926514be5fec0af0a4aec9f079b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f4bcbcdf58014bfd7952c0f8b254e74.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SARI-Structured-Audio-Reasoning-via-Curriculum-Guided-Reinforcement-Learning"><a href="#SARI-Structured-Audio-Reasoning-via-Curriculum-Guided-Reinforcement-Learning" class="headerlink" title="SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement   Learning"></a>SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement   Learning</h2><p><strong>Authors:Cheng Wen, Tingwei Guo, Shuaijiang Zhao, Wei Zou, Xiangang Li</strong></p>
<p>Recent work shows that reinforcement learning(RL) can markedly sharpen the reasoning ability of large language models (LLMs) by prompting them to â€œthink before answering.â€ Yet whether and how these gains transfer to audio-language reasoning remains largely unexplored. We extend the Group-Relative Policy Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model (LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage regimen supervised fine-tuning on structured and unstructured chains-of-thought, followed by curriculum-guided GRPO, we systematically compare implicit vs. explicit, and structured vs. free form reasoning under identical architectures. Our structured audio reasoning model, SARI (Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a 16.35% improvement in average accuracy over the base model Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark. Ablation experiments show that on the base model we use: (i) SFT warm-up is important for stable RL training, (ii) structured chains yield more robust generalization than unstructured ones, and (iii) easy-to-hard curricula accelerate convergence and improve final performance. These findings demonstrate that explicit, structured reasoning and curriculum learning substantially enhances audio-language understanding. </p>
<blockquote>
<p>æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡æç¤ºâ€œå…ˆæ€è€ƒå†å›ç­”â€ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æ˜¾è‘—æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ”¶ç›Šæ˜¯å¦ä»¥åŠå¦‚ä½•è½¬ç§»åˆ°éŸ³é¢‘è¯­è¨€æ¨ç†é¢†åŸŸä»é²œæœ‰æ¢ç´¢ã€‚æˆ‘ä»¬å°†DeepSeek-R1çš„Group-Relative Policy Optimizationï¼ˆGRPOï¼‰æ¡†æ¶æ‰©å±•åˆ°ä¸€ä¸ªå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«32kæ ·æœ¬çš„é€‰æ‹©é¢˜è¯­æ–™åº“ã€‚é€šè¿‡ç»“æ„åŒ–ä¸éç»“æ„åŒ–æ€ç»´é“¾çš„ä¸¤é˜¶æ®µç›‘ç®¡ç›‘ç£å¾®è°ƒï¼Œéšåè¿›è¡Œè¯¾ç¨‹æŒ‡å¯¼çš„GRPOï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æ¯”è¾ƒäº†éšå¼å’Œæ˜¾å¼çš„ç»“æ„åŒ–ä¸éç»“æ„åŒ–æ¨ç†åœ¨ç›¸åŒæ¶æ„ä¸‹çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç»“æ„åŒ–éŸ³é¢‘æ¨ç†æ¨¡å‹SARIï¼ˆé€šè¿‡è¯¾ç¨‹æŒ‡å¯¼å¼ºåŒ–å­¦ä¹ çš„ç»“æ„åŒ–éŸ³é¢‘æ¨ç†ï¼‰åœ¨åŸºå‡†æ¨¡å‹Qwen2-Audio-7B-Instructçš„åŸºç¡€ä¸Šå®ç°äº†å¹³å‡ç²¾åº¦16.35%çš„æå‡ã€‚æ­¤å¤–ï¼ŒåŸºäºQwen2.5-Omniçš„å˜ä½“åœ¨MMAUæµ‹è¯•å°å‹åŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä¸º67.08%ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼Œåœ¨æˆ‘ä»¬ä½¿ç”¨çš„åŸºå‡†æ¨¡å‹ä¸Šï¼šï¼ˆiï¼‰SFTé¢„çƒ­å¯¹äºç¨³å®šçš„RLè®­ç»ƒå¾ˆé‡è¦ï¼Œï¼ˆiiï¼‰ç»“æ„åŒ–é“¾æ¯”éç»“æ„åŒ–é“¾äº§ç”Ÿæ›´ç¨³å¥çš„æ³›åŒ–æ€§èƒ½ï¼Œï¼ˆiiiï¼‰ä»æ˜“åˆ°éš¾çš„è¯¾ç¨‹åŠ é€Ÿæ”¶æ•›å¹¶æé«˜äº†æœ€ç»ˆæ€§èƒ½ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæ˜ç¡®çš„ã€ç»“æ„åŒ–çš„æ¨ç†å’Œè¯¾ç¨‹å­¦ä¹ æ˜¾è‘—å¢å¼ºäº†éŸ³é¢‘è¯­è¨€ç†è§£èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15900v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶åœ¨éŸ³é¢‘è¯­è¨€æ¨ç†ä¸­çš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶å°†Group-Relative Policy Optimizationï¼ˆGRPOï¼‰æ¡†æ¶æ‰©å±•åˆ°å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰ï¼Œæ„å»ºäº†åŒ…å«å¤šç§é€‰æ‹©çš„éŸ³é¢‘æ•°æ®é›†ã€‚é€šè¿‡ç»“æ„åŒ–ä¸éç»“æ„åŒ–æ€ç»´é“¾çš„ç²¾ç»†è°ƒæ•´ï¼Œä»¥åŠè¯¾ç¨‹æŒ‡å¯¼çš„GRPOï¼Œæœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¯”è¾ƒäº†éšå¼å’Œæ˜¾å¼ã€ç»“æ„åŒ–ä¸éç»“æ„åŒ–æ¨ç†ã€‚ç»“æœæ˜¾ç¤ºï¼Œç»“æ„åŒ–éŸ³é¢‘æ¨ç†æ¨¡å‹SARIåœ¨åŸºå‡†æ¨¡å‹Qwen2-Audio-7B-Instructçš„åŸºç¡€ä¸Šå¹³å‡å‡†ç¡®ç‡æé«˜äº†16.35%ã€‚æ­¤å¤–ï¼ŒåŸºäºQwen2.5-Omniçš„ç‰ˆæœ¬åœ¨MMAUæµ‹è¯•å°æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚å®éªŒè¡¨æ˜ï¼šï¼ˆiï¼‰SFTé¢„çƒ­å¯¹äºç¨³å®šçš„RLè®­ç»ƒè‡³å…³é‡è¦ï¼›ï¼ˆiiï¼‰ç»“æ„åŒ–æ€ç»´é“¾æ¯”éç»“æ„åŒ–æ€ç»´é“¾æ›´å…·ç¨³å¥æ€§ï¼›ï¼ˆiiiï¼‰ä»æ˜“åˆ°éš¾çš„è¯¾ç¨‹èƒ½åŠ é€Ÿæ”¶æ•›å¹¶æé«˜æœ€ç»ˆæ€§èƒ½ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæ˜¾å¼ã€ç»“æ„åŒ–çš„æ¨ç†å’Œè¯¾ç¨‹å­¦ä¹ èƒ½æ˜¾è‘—æé«˜éŸ³é¢‘è¯­è¨€ç†è§£çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨éŸ³é¢‘è¯­è¨€æ¨ç†ä¸­çš„åº”ç”¨å°šæœªå¹¿æ³›ç ”ç©¶ã€‚</li>
<li>ç ”ç©¶äººå‘˜å°†GRPOæ¡†æ¶æ‰©å±•åˆ°å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«å¤šç§é€‰æ‹©çš„éŸ³é¢‘æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡ç²¾ç»†åŒ–è°ƒæ•´ï¼Œç»“æ„åŒ–æ€ç»´é“¾çš„æ¨ç†æ¨¡å‹è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>SARIæ¨¡å‹åœ¨åŸºå‡†æ¨¡å‹ä¸Šçš„å¹³å‡å‡†ç¡®ç‡æé«˜äº†16.35%ã€‚</li>
<li>Qwen2.5-Omniç‰ˆæœ¬åœ¨MMAUæµ‹è¯•å°æ•°æ®é›†ä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSFTé¢„çƒ­å¯¹ç¨³å®šRLè®­ç»ƒè‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9907d84410969cae5a0d9ef1edf48de7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9a4c222bf685456a159399fc337e9db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52a85868ad6ee7128cfd9498f7ebc10b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Insights-from-Verification-Training-a-Verilog-Generation-LLM-with-Reinforcement-Learning-with-Testbench-Feedback"><a href="#Insights-from-Verification-Training-a-Verilog-Generation-LLM-with-Reinforcement-Learning-with-Testbench-Feedback" class="headerlink" title="Insights from Verification: Training a Verilog Generation LLM with   Reinforcement Learning with Testbench Feedback"></a>Insights from Verification: Training a Verilog Generation LLM with   Reinforcement Learning with Testbench Feedback</h2><p><strong>Authors:Ning Wang, Bingkun Yao, Jie Zhou, Yuchen Hu, Xi Wang, Nan Guan, Zhe Jiang</strong></p>
<p>Large language models (LLMs) have shown strong performance in Verilog generation from natural language description. However, ensuring the functional correctness of the generated code remains a significant challenge. This paper introduces a method that integrates verification insights from testbench into the training of Verilog generation LLMs, aligning the training with the fundamental goal of hardware design: functional correctness. The main obstacle in using LLMs for Verilog code generation is the lack of sufficient functional verification data, particularly testbenches paired with design specifications and code. To address this problem, we introduce an automatic testbench generation pipeline that decomposes the process and uses feedback from the Verilog compiler simulator (VCS) to reduce hallucination and ensure correctness. We then use the testbench to evaluate the generated codes and collect them for further training, where verification insights are introduced. Our method applies reinforcement learning (RL), specifically direct preference optimization (DPO), to align Verilog code generation with functional correctness by training preference pairs based on testbench outcomes. In evaluations on VerilogEval-Machine, VerilogEval-Human, RTLLM v1.1, RTLLM v2, and VerilogEval v2, our approach consistently outperforms state-of-the-art baselines in generating functionally correct Verilog code. We open source all training code, data, and models at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/VeriPrefer-E88B">https://anonymous.4open.science/r/VeriPrefer-E88B</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€æè¿°ç”ŸæˆVerilogä»£ç æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç¡®ä¿ç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†æµ‹è¯•å¹³å°ä¸­çš„éªŒè¯è§è§£é›†æˆåˆ°Verilogç”ŸæˆLLMçš„è®­ç»ƒä¸­ï¼Œä½¿è®­ç»ƒä¸ç¡¬ä»¶è®¾è®¡çš„æ ¹æœ¬ç›®æ ‡â€”â€”åŠŸèƒ½æ­£ç¡®æ€§â€”â€”ä¿æŒä¸€è‡´ã€‚åœ¨ä½¿ç”¨LLMè¿›è¡ŒVerilogä»£ç ç”Ÿæˆçš„ä¸»è¦éšœç¢æ˜¯ç¼ºä¹è¶³å¤Ÿçš„åŠŸèƒ½éªŒè¯æ•°æ®ï¼Œç‰¹åˆ«æ˜¯ä¸è®¾è®¡è§„èŒƒå’Œæ•°æ®ç›¸åŒ¹é…çš„æµ‹è¯•å¹³å°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè‡ªåŠ¨æµ‹è¯•å¹³å°ç”Ÿæˆæµç¨‹ï¼Œè¯¥æµç¨‹åˆ†è§£äº†è¿‡ç¨‹å¹¶ä½¿ç”¨Verilogç¼–è¯‘å™¨æ¨¡æ‹Ÿå™¨ï¼ˆVCSï¼‰çš„åé¦ˆæ¥å‡å°‘å¹»è§‰å¹¶ç¡®ä¿æ­£ç¡®æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æµ‹è¯•å¹³å°æ¥è¯„ä¼°ç”Ÿæˆçš„ä»£ç å¹¶å°†å…¶æ”¶é›†ç”¨äºè¿›ä¸€æ­¥çš„è®­ç»ƒï¼Œå…¶ä¸­å¼•å…¥äº†éªŒè¯è§è§£ã€‚æˆ‘ä»¬çš„æ–¹æ³•åº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œç‰¹åˆ«æ˜¯ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œé€šè¿‡åŸºäºæµ‹è¯•å¹³å°ç»“æœçš„åå¥½å¯¹æ¥ä½¿Verilogä»£ç ç”Ÿæˆä¸åŠŸèƒ½æ­£ç¡®æ€§ä¿æŒä¸€è‡´ã€‚åœ¨VerilogEval-Machineã€VerilogEval-Humanã€RTLLMv1.1ã€RTLLMv2å’ŒVerilogEval v2ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”ŸæˆåŠŸèƒ½æ­£ç¡®çš„Verilogä»£ç æ–¹é¢å§‹ç»ˆä¼˜äºæœ€æ–°åŸºçº¿ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/VeriPrefer-E88B">https://anonymous.4open.science/r/VeriPrefer-E88B</a>å…¬å¼€æ‰€æœ‰è®­ç»ƒä»£ç ã€æ•°æ®å’Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15804v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°ç”ŸæˆVerilogä»£ç æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†ä¿è¯ç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†æµ‹è¯•å¹³å°ä¸­çš„éªŒè¯è§è§£é›†æˆåˆ°Verilogç”Ÿæˆå¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒä¸­ï¼Œä½¿è®­ç»ƒç¬¦åˆç¡¬ä»¶è®¾è®¡çš„æ ¹æœ¬ç›®æ ‡ï¼šåŠŸèƒ½æ­£ç¡®æ€§ã€‚æˆ‘ä»¬é¢ä¸´çš„ä¸»è¦é—®é¢˜æ˜¯ç¼ºä¹è¶³å¤Ÿçš„åŠŸèƒ½éªŒè¯æ•°æ®ï¼Œç‰¹åˆ«æ˜¯ä¸è®¾è®¡è§„èŒƒå’Œä»£ç é…å¥—çš„æµ‹è¯•å¹³å°ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªåŠ¨æµ‹è¯•å¹³å°ç”Ÿæˆæµç¨‹ï¼Œè¯¥æµç¨‹åˆ†è§£è¿‡ç¨‹å¹¶ä½¿ç”¨Verilogç¼–è¯‘å™¨æ¨¡æ‹Ÿå™¨ï¼ˆVCSï¼‰çš„åé¦ˆæ¥å‡å°‘å¹»è§‰å¹¶ç¡®ä¿æ­£ç¡®æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æµ‹è¯•å¹³å°è¯„ä¼°ç”Ÿæˆçš„ä»£ç å¹¶æ”¶é›†å®ƒä»¬ä»¥è¿›è¡Œè¿›ä¸€æ­¥çš„è®­ç»ƒï¼ŒåŒæ—¶å¼•å…¥éªŒè¯è§è§£ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œç‰¹åˆ«æ˜¯åŸºäºæµ‹è¯•å¹³å°ç»“æœçš„åå¥½å¯¹ï¼ˆpreference pairsï¼‰æ¥ä¼˜åŒ–Verilogä»£ç ç”Ÿæˆï¼Œä½¿å…¶ç¬¦åˆåŠŸèƒ½æ­£ç¡®æ€§ã€‚åœ¨VerilogEval-Machineã€VerilogEval-Humanã€RTLLMv1.1ã€RTLLMv2å’ŒVerilogEvalv2ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”ŸæˆåŠŸèƒ½æ­£ç¡®çš„Verilogä»£ç æ–¹é¢å§‹ç»ˆä¼˜äºæœ€æ–°åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨Verilogä»£ç ç”Ÿæˆä¸­è¡¨ç°å‡ºå¼ºå¤§æ€§èƒ½ï¼Œä½†åŠŸèƒ½æ­£ç¡®æ€§æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>ç¼ºä¹è¶³å¤Ÿçš„åŠŸèƒ½éªŒè¯æ•°æ®ï¼Œç‰¹åˆ«æ˜¯ä¸è®¾è®¡è§„èŒƒå’Œä»£ç é…å¥—çš„æµ‹è¯•å¹³å°ã€‚</li>
<li>å¼•å…¥è‡ªåŠ¨æµ‹è¯•å¹³å°ç”Ÿæˆæµç¨‹ï¼Œä½¿ç”¨Verilogç¼–è¯‘å™¨æ¨¡æ‹Ÿå™¨çš„åé¦ˆç¡®ä¿ä»£ç æ­£ç¡®æ€§ã€‚</li>
<li>ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–Verilogä»£ç ç”Ÿæˆï¼Œä½¿å…¶ç¬¦åˆåŠŸèƒ½æ­£ç¡®æ€§ã€‚</li>
<li>æ–¹æ³•é€šè¿‡åŸºäºæµ‹è¯•å¹³å°ç»“æœçš„åå¥½å¯¹è¿›è¡Œè®­ç»ƒã€‚</li>
<li>åœ¨å¤šä¸ªè¯„ä¼°ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨ç”ŸæˆåŠŸèƒ½æ­£ç¡®çš„Verilogä»£ç æ–¹é¢ä¼˜äºæœ€æ–°åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2227f92383106061da18745358c89023.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5d528d13dc93de4bcbcd80d92f19e85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e87c4061440afd90e361459bfe197e89.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f206ef58e138d5071a6797f74f0ae562.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2596d8c9f67de27282912359e65a1a26.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35be808042bc39f1979b9d3932a4f7bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0f6aceeff4152775f409f667be8f9a0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="WALL-E-2-0-World-Alignment-by-NeuroSymbolic-Learning-improves-World-Model-based-LLM-Agents"><a href="#WALL-E-2-0-World-Alignment-by-NeuroSymbolic-Learning-improves-World-Model-based-LLM-Agents" class="headerlink" title="WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World   Model-based LLM Agents"></a>WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World   Model-based LLM Agents</h2><p><strong>Authors:Siyu Zhou, Tianyi Zhou, Yijun Yang, Guodong Long, Deheng Ye, Jing Jiang, Chengqi Zhang</strong></p>
<p>Can we build accurate world models out of large language models (LLMs)? How can world models benefit LLM agents? The gap between the prior knowledge of LLMs and the specified environmentâ€™s dynamics usually bottlenecks LLMsâ€™ performance as world models. To bridge the gap, we propose a training-free â€œworld alignmentâ€ that learns an environmentâ€™s symbolic knowledge complementary to LLMs. The symbolic knowledge covers action rules, knowledge graphs, and scene graphs, which are extracted by LLMs from exploration trajectories and encoded into executable codes to regulate LLM agentsâ€™ policies. We further propose an RL-free, model-based agent â€œWALL-E 2.0â€ through the model-predictive control (MPC) framework. Unlike classical MPC requiring costly optimization on the fly, we adopt an LLM agent as an efficient look-ahead optimizer of future stepsâ€™ actions by interacting with the neurosymbolic world model. While the LLM agentâ€™s strong heuristics make it an efficient planner in MPC, the quality of its planned actions is also secured by the accurate predictions of the aligned world model. They together considerably improve learning efficiency in a new environment. On open-world challenges in Mars (Minecraft like) and ALFWorld (embodied indoor environments), WALL-E 2.0 significantly outperforms existing methods, e.g., surpassing baselines in Mars by 16.1%-51.6% of success rate and by at least 61.7% in score. In ALFWorld, it achieves a new record 98% success rate after only 4 iterations. </p>
<blockquote>
<p>æˆ‘ä»¬èƒ½å¦åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ„å»ºç²¾ç¡®çš„ä¸–ç•Œæ¨¡å‹ï¼Ÿä¸–ç•Œæ¨¡å‹å¦‚ä½•å¯¹LLMä»£ç†äº§ç”Ÿç›Šå¤„ï¼Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…ˆéªŒçŸ¥è¯†ä¸ç‰¹å®šç¯å¢ƒåŠ¨æ€ä¹‹é—´çš„å·®è·é€šå¸¸ä¼šé™åˆ¶LLMä½œä¸ºä¸–ç•Œæ¨¡å‹çš„è¡¨ç°ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„â€œä¸–ç•Œå¯¹é½â€æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥å­¦ä¹ ä¸LLMäº’è¡¥çš„ç¯å¢ƒç¬¦å·çŸ¥è¯†ã€‚ç¬¦å·çŸ¥è¯†åŒ…æ‹¬è¡ŒåŠ¨è§„åˆ™ã€çŸ¥è¯†å›¾è°±å’Œåœºæ™¯å›¾ï¼Œè¿™äº›æ˜¯ç”±LLMä»æ¢ç´¢è½¨è¿¹ä¸­æå–å¹¶ç¼–ç æˆå¯æ‰§è¡Œä»£ç ï¼Œä»¥è°ƒèŠ‚LLMä»£ç†çš„ç­–ç•¥ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªåŸºäºæ¨¡å‹çš„ä»£ç†â€œWALL-E 2.0â€ï¼Œè¯¥ä»£ç†åŸºäºæ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰æ¡†æ¶ï¼Œå¹¶ä¸”æ— éœ€å¼ºåŒ–å­¦ä¹ ã€‚ä¸ä¼ ç»Ÿçš„MPCéœ€è¦åœ¨é£è¡Œä¸­è¿›è¡Œæ˜‚è´µçš„ä¼˜åŒ–ä¸åŒï¼Œæˆ‘ä»¬é‡‡ç”¨LLMä»£ç†ä½œä¸ºé«˜æ•ˆçš„å‘å‰é¢„æµ‹ä¼˜åŒ–å™¨ï¼Œé€šè¿‡ä¸ç¥ç»ç¬¦å·ä¸–ç•Œæ¨¡å‹çš„äº¤äº’ï¼Œå¯¹æœªæ¥æ­¥éª¤çš„è¡ŒåŠ¨è¿›è¡Œé¢„æµ‹ã€‚LLMä»£ç†çš„å¼ºå¤§å¯å‘å¼ä½¿å…¶æˆä¸ºMPCä¸­çš„é«˜æ•ˆè§„åˆ’å™¨ï¼Œè€Œå…¶è®¡åˆ’çš„è¡ŒåŠ¨è´¨é‡ä¹Ÿå¾—åˆ°äº†å¯¹é½çš„ä¸–ç•Œæ¨¡å‹çš„å‡†ç¡®é¢„æµ‹ä¿éšœã€‚å®ƒä»¬å…±åŒä½œç”¨ï¼Œå¤§å¤§æé«˜äº†æ–°ç¯å¢ƒä¸‹çš„å­¦ä¹ æ•ˆç‡ã€‚åœ¨ç±»ä¼¼äºç«æ˜Ÿï¼ˆMinecraftï¼‰çš„å¼€æ”¾ä¸–ç•ŒæŒ‘æˆ˜å’Œå®¤å†…ç¯å¢ƒä½“åŒ–çš„ALFWorldä¸­ï¼ŒWALL-E 2.0æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¾‹å¦‚åœ¨ç«æ˜Ÿä¸Šçš„æˆåŠŸç‡è¶…è¿‡åŸºçº¿16.1%~51.6%ï¼Œå¾—åˆ†è‡³å°‘æé«˜61.7%ã€‚åœ¨ALFWorldä¸­ï¼Œå®ƒä»…ç»è¿‡4æ¬¡è¿­ä»£å°±è¾¾åˆ°äº†98%çš„æ–°çºªå½•æˆåŠŸç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15785v1">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/elated-sawyer/WALL-E">https://github.com/elated-sawyer/WALL-E</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•æ„å»ºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸–ç•Œæ¨¡å‹ï¼Œå¹¶è§£å†³äº†LLMä¸ç‰¹å®šç¯å¢ƒåŠ¨æ€ä¹‹é—´çš„çŸ¥è¯†å·®è·é—®é¢˜ã€‚æå‡ºäº†æ— éœ€è®­ç»ƒçš„â€œä¸–ç•Œå¯¹é½â€æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥å­¦ä¹ ç¯å¢ƒçš„ç¬¦å·çŸ¥è¯†å¹¶ä¸LLMäº’è¡¥ã€‚é€šè¿‡æå–åŠ¨ä½œè§„åˆ™ã€çŸ¥è¯†å›¾å’Œåœºæ™¯å›¾ç­‰ä¿¡æ¯ï¼Œå¹¶å°†å…¶ç¼–ç ä¸ºå¯æ‰§è¡Œä»£ç ï¼Œä»¥è°ƒèŠ‚LLMä»£ç†çš„ç­–ç•¥ã€‚æ­¤å¤–ï¼Œä»‹ç»äº†æ— éœ€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åŸºäºæ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰æ¡†æ¶çš„â€œWALL-E 2.0â€ä»£ç†ã€‚ä¸éœ€è¦å³æ—¶ä¼˜åŒ–çš„ç»å…¸MPCä¸åŒï¼Œæˆ‘ä»¬é‡‡ç”¨LLMä»£ç†ä½œä¸ºæœªæ¥æ­¥éª¤åŠ¨ä½œçš„é«˜æ•ˆå‰ç»ä¼˜åŒ–å™¨ï¼Œé€šè¿‡ä¸ç¥ç»ç¬¦å·ä¸–ç•Œæ¨¡å‹çš„äº¤äº’æ¥å®ç°ã€‚åœ¨ç«æ˜Ÿï¼ˆMinecrafté£æ ¼ï¼‰å’ŒALFWorldï¼ˆå®¤å†…ç¯å¢ƒï¼‰çš„å¼€æ”¾ä¸–ç•ŒæŒ‘æˆ˜ä¸­ï¼ŒWALL-E 2.0æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¦‚åœ¨ç«æ˜Ÿä¸Šçš„æˆåŠŸç‡è¶…è¿‡åŸºçº¿16.1%~51.6%ï¼Œå¾—åˆ†è‡³å°‘æé«˜61.7%ã€‚åœ¨ALFWorldä¸­ï¼Œä»…ç»è¿‡4æ¬¡è¿­ä»£å°±è¾¾åˆ°äº†98%çš„æˆåŠŸç‡ï¼Œåˆ›é€ äº†æ–°çºªå½•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ„å»ºä¸–ç•Œæ¨¡å‹æ—¶å­˜åœ¨ä¸ç‰¹å®šç¯å¢ƒåŠ¨æ€çš„çŸ¥è¯†å·®è·é—®é¢˜ã€‚</li>
<li>æå‡ºäº†æ— éœ€è®­ç»ƒçš„â€œä¸–ç•Œå¯¹é½â€æ–¹æ³•æ¥è§£å†³è¿™ä¸€çŸ¥è¯†å·®è·ï¼Œå­¦ä¹ ç¯å¢ƒçš„ç¬¦å·çŸ¥è¯†ä¸LLMsäº’è¡¥ã€‚</li>
<li>ç¬¦å·çŸ¥è¯†åŒ…æ‹¬åŠ¨ä½œè§„åˆ™ã€çŸ¥è¯†å›¾å’Œåœºæ™¯å›¾ï¼Œç”±LLMsä»æ¢ç´¢è½¨è¿¹ä¸­æå–å¹¶ç¼–ç ä¸ºå¯æ‰§è¡Œä»£ç ï¼Œä»¥è°ƒèŠ‚LLMä»£ç†çš„ç­–ç•¥ã€‚</li>
<li>ä»‹ç»äº†åŸºäºæ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰æ¡†æ¶çš„â€œWALL-E 2.0â€ä»£ç†ï¼Œæ— éœ€å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚</li>
<li>LLMä»£ç†ä½œä¸ºæœªæ¥æ­¥éª¤åŠ¨ä½œçš„é«˜æ•ˆå‰ç»ä¼˜åŒ–å™¨ï¼Œé€šè¿‡ä¸ç¥ç»ç¬¦å·ä¸–ç•Œæ¨¡å‹çš„äº¤äº’æ¥æé«˜å­¦ä¹ æ•ˆç‡ã€‚</li>
<li>åœ¨å¼€æ”¾ä¸–ç•ŒæŒ‘æˆ˜ä¸­ï¼Œå¦‚ç«æ˜Ÿï¼ˆMinecrafté£æ ¼ï¼‰å’ŒALFWorldï¼ˆå®¤å†…ç¯å¢ƒï¼‰ï¼ŒWALL-E 2.0æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒæˆåŠŸç‡å’Œå¾—åˆ†å‡æœ‰æ˜æ˜¾æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15785">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-23688b313dcc2d82d2ee9dd9bba5a2df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3244ff369769fcc7848ddbb995c6a608.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cae94112807d9d991833caba09438e8.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Tina-Tiny-Reasoning-Models-via-LoRA"><a href="#Tina-Tiny-Reasoning-Models-via-LoRA" class="headerlink" title="Tina: Tiny Reasoning Models via LoRA"></a>Tina: Tiny Reasoning Models via LoRA</h2><p><strong>Authors:Shangshang Wang, Julian Asilis, Ã–mer Faruk AkgÃ¼l, Enes Burak Bilgin, Ollie Liu, Willie Neiswanger</strong></p>
<p>How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, we present Tina, a family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using only minimal resources, by applying parameter-efficient updates during reinforcement learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B parameter base model. This minimalist approach produces models that achieve reasoning performance which is competitive with, and sometimes surpasses, SOTA RL reasoning models built upon the same base model. Crucially, this is achieved at a tiny fraction of the computational post-training cost employed by existing SOTA models. In fact, the best Tina model achieves a &gt;20% reasoning performance increase and 43.33% Pass@1 accuracy on AIME24, at only $9 USD post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our work reveals the surprising effectiveness of efficient RL reasoning via LoRA. We validate this across multiple open-source reasoning datasets and various ablation settings starting with a single, fixed set of hyperparameters. Furthermore, we hypothesize that this effectiveness and efficiency stem from LoRA rapidly adapting the model to the structural format of reasoning rewarded by RL, while largely preserving the base modelâ€™s underlying knowledge. In service of accessibility and open research, we fully open-source all code, training logs, and model weights &amp; checkpoints. </p>
<blockquote>
<p>å¦‚ä½•å®ç°è¯­è¨€æ¨¡å‹ä¸­çš„é«˜æ•ˆæ¨ç†èƒ½åŠ›ï¼Ÿåœ¨è¿™ä¸ªåŸºæœ¬é—®é¢˜çš„é©±åŠ¨ä¸‹ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Tinaï¼Œè¿™æ˜¯ä¸€æ¬¾å…·æœ‰é«˜æˆæœ¬æ•ˆç›Šçš„å°å‹æ¨ç†æ¨¡å‹ç³»åˆ—ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒTinaå±•ç¤ºäº†åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿‡ç¨‹ä¸­åº”ç”¨å‚æ•°é«˜æ•ˆæ›´æ–°å’Œä½é˜¶é€‚åº”ï¼ˆLoRAï¼‰æ—¶ï¼Œä»…ä½¿ç”¨æå°‘çš„èµ„æºå°±èƒ½å®ç°æ˜¾è‘—çš„æ¨ç†æ€§èƒ½ã€‚è¿™ä¸€æç®€çš„æ–¹æ³•äº§ç”Ÿçš„æ¨¡å‹ï¼Œå…¶æ¨ç†æ€§èƒ½ä¸åŸºäºåŒä¸€åŸºç¡€æ¨¡å‹çš„æœ€å…ˆè¿›çš„RLæ¨ç†æ¨¡å‹ç›¸ç«äº‰ï¼Œæœ‰æ—¶ç”šè‡³æ›´èƒœä¸€ç­¹ã€‚å…³é”®çš„æ˜¯ï¼Œè¿™ä¸€åˆ‡éƒ½æ˜¯åœ¨ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹æ‰€ä½¿ç”¨çš„ä¸€å°éƒ¨åˆ†è®­ç»ƒåè®¡ç®—æˆæœ¬ä¸‹å®ç°çš„ã€‚äº‹å®ä¸Šï¼Œæœ€å¥½çš„Tinaæ¨¡å‹åœ¨AIME24ä¸Šå®ç°äº†è¶…è¿‡20%çš„æ¨ç†æ€§èƒ½æå‡å’Œ43.33%çš„Pass@1å‡†ç¡®ç‡ï¼Œè€Œè®­ç»ƒåè¯„ä¼°å’Œæˆæœ¬ä»…ä¸º9ç¾å…ƒï¼ˆå³ä¼°è®¡çš„æˆæœ¬é™ä½äº†çº¦26å€ï¼‰ã€‚æˆ‘ä»¬çš„å·¥ä½œæ­ç¤ºäº†é€šè¿‡LoRAå®ç°é«˜æ•ˆRLæ¨ç†çš„æƒŠäººæ•ˆæœã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå¼€æºæ¨ç†æ•°æ®é›†å’Œå„ç§æ¶ˆèè®¾ç½®ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œèµ·å§‹äºä¸€ç»„å›ºå®šçš„è¶…å‚æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‡è®¾è¿™ç§æœ‰æ•ˆæ€§å’Œæ•ˆç‡æºäºLoRAèƒ½å¤Ÿè¿…é€Ÿé€‚åº”RLæ‰€å¥–åŠ±çš„æ¨ç†çš„ç»“æ„åŒ–æ ¼å¼ï¼ŒåŒæ—¶ä¿ç•™åŸºç¡€æ¨¡å‹çš„åº•å±‚çŸ¥è¯†ã€‚ä¸ºäº†å¯è®¿é—®æ€§å’Œå¼€æ”¾ç ”ç©¶ï¼Œæˆ‘ä»¬å®Œå…¨å¼€æºæ‰€æœ‰ä»£ç ã€è®­ç»ƒæ—¥å¿—å’Œæ¨¡å‹æƒé‡åŠæ£€æŸ¥ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15777v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Tinaç³»åˆ—å¾®å°æ¨ç†æ¨¡å‹çš„é«˜æˆæœ¬æ•ˆç›Šå®ç°æ–¹å¼ã€‚é€šè¿‡åº”ç”¨å‚æ•°æœ‰æ•ˆçš„æ›´æ–°å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»¥åŠä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯ï¼Œåœ¨ä»…æœ‰æå°èµ„æºçš„æƒ…å†µä¸‹ï¼ŒTinaå±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚ç›¸è¾ƒäºå½“å‰æœ€ä½³å®è·µï¼ˆSOTAï¼‰æ¨¡å‹ï¼ŒTinaä¸ä»…å®ç°äº†ç«äº‰æ€§çš„æ¨ç†æ€§èƒ½ï¼Œæœ‰æ—¶ç”šè‡³èƒ½è¶…è¶Šå®ƒä»¬ï¼ŒåŒæ—¶å¤§å¹…é™ä½äº†è®­ç»ƒåçš„è®¡ç®—æˆæœ¬ã€‚æœ€ä½³Tinaæ¨¡å‹åœ¨AIME24ä¸Šå®ç°äº†è¶…è¿‡20%çš„æ¨ç†æ€§èƒ½æå‡å’Œ43.33%çš„Pass@1å‡†ç¡®ç‡ï¼Œè€Œè®­ç»ƒååŠè¯„ä¼°æˆæœ¬ä»…ä¸º9ç¾å…ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†é€šè¿‡LoRAæŠ€æœ¯å®ç°é«˜æ•ˆRLæ¨ç†çš„æƒŠäººæ•ˆæœã€‚æˆ‘ä»¬è·¨å¤šä¸ªå¼€æºæ¨ç†æ•°æ®é›†å’Œä¸åŒè®¾ç½®è¿›è¡Œäº†éªŒè¯ï¼Œå¹¶å‡è®¾è¿™ç§æ•ˆæœå’Œæ•ˆç‡æ¥æºäºLoRAèƒ½å¿«é€Ÿé€‚åº”RLå¥–åŠ±çš„ç»“æ„åŒ–æ ¼å¼ï¼ŒåŒæ—¶ä¿ç•™åŸºç¡€æ¨¡å‹çš„åº•å±‚çŸ¥è¯†ã€‚æˆ‘ä»¬å…¬å¼€äº†æ‰€æœ‰ä»£ç ã€è®­ç»ƒæ—¥å¿—å’Œæ¨¡å‹æƒé‡åŠæ£€æŸ¥ç‚¹ï¼Œä»¥ä¿ƒè¿›ç ”ç©¶çš„å¼€æ”¾æ€§å’Œå¯è®¿é—®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Tinaæ˜¯å¾®å°æ¨ç†æ¨¡å‹ï¼Œä»¥é«˜æˆæœ¬æ•ˆç›Šå®ç°å¼ºå¤§æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å‚æ•°æœ‰æ•ˆçš„æ›´æ–°å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»¥åŠä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯ï¼ŒTinaåœ¨å°‘é‡èµ„æºä¸‹å±•ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>Tinaåœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†å½“å‰æœ€ä½³å®è·µï¼ˆSOTAï¼‰æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>æœ€ä½³Tinaæ¨¡å‹åœ¨AIME24ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡å’Œå‡†ç¡®ç‡ã€‚</li>
<li>LoRAæŠ€æœ¯è¢«è®¤ä¸ºæ˜¯å®ç°é«˜æ•ˆRLæ¨ç†çš„å…³é”®ã€‚</li>
<li>LoRAèƒ½å¿«é€Ÿé€‚åº”RLå¥–åŠ±çš„ç»“æ„åŒ–æ ¼å¼ï¼ŒåŒæ—¶ä¿ç•™åŸºç¡€æ¨¡å‹çš„åº•å±‚çŸ¥è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2d83a63f87db38a77ea2777480d0ec25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9ba20e393c530d11b33116e388a5256.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1abc08ca2ba3f249bc6bf9b238b19a80.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a3f199b24c01dae5664d82d094e63fd2.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="IV-Bench-A-Benchmark-for-Image-Grounded-Video-Perception-and-Reasoning-in-Multimodal-LLMs"><a href="#IV-Bench-A-Benchmark-for-Image-Grounded-Video-Perception-and-Reasoning-in-Multimodal-LLMs" class="headerlink" title="IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning   in Multimodal LLMs"></a>IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning   in Multimodal LLMs</h2><p><strong>Authors:David Ma, Yuanxing Zhang, Jincheng Ren, Jarvis Guo, Yifan Yao, Zhenlin Wei, Zhenzhu Yang, Zhongyuan Peng, Boyu Feng, Jun Ma, Xiao Gu, Zhoufutu Wen, King Zhu, Yancheng He, Meng Cao, Shiwen Ni, Jiaheng Liu, Wenhao Huang, Ge Zhang, Xiaojie Jin</strong></p>
<p>Existing evaluation frameworks for Multimodal Large Language Models (MLLMs) primarily focus on image reasoning or general video understanding tasks, largely overlooking the significant role of image context in video comprehension. To bridge this gap, we propose IV-Bench, the first comprehensive benchmark for evaluating Image-Grounded Video Perception and Reasoning. IV-Bench consists of 967 videos paired with 2,585 meticulously annotated image-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5 representative categories. Extensive evaluations of state-of-the-art open-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o, Gemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models substantially underperform in image-grounded video Perception and Reasoning, merely achieving at most 28.9% accuracy. Further analysis reveals key factors influencing model performance on IV-Bench, including inference pattern, frame number, and resolution. Additionally, through a simple data synthesis approach, we demonstratethe challenges of IV- Bench extend beyond merely aligning the data format in the training proecss. These findings collectively provide valuable insights for future research. Our codes and data are released in <a target="_blank" rel="noopener" href="https://github.com/multimodal-art-projection/IV-Bench">https://github.com/multimodal-art-projection/IV-Bench</a>. </p>
<blockquote>
<p>ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¯„ä¼°æ¡†æ¶ä¸»è¦å…³æ³¨å›¾åƒæ¨ç†æˆ–é€šç”¨è§†é¢‘ç†è§£ä»»åŠ¡ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†å›¾åƒä¸Šä¸‹æ–‡åœ¨è§†é¢‘ç†è§£ä¸­çš„é‡è¦ä½œç”¨ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†IV-Benchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè¯„ä¼°å›¾åƒåŸºåœ°è§†é¢‘æ„ŸçŸ¥å’Œæ¨ç†çš„ç»¼åˆæ€§åŸºå‡†æµ‹è¯•ã€‚IV-BenchåŒ…å«967ä¸ªè§†é¢‘å’Œä¸ä¹‹é…å¯¹çš„2585ä¸ªç²¾å¿ƒæ ‡æ³¨çš„å›¾åƒæ–‡æœ¬æŸ¥è¯¢ï¼Œæ¶µç›–13ä¸ªä»»åŠ¡ï¼ˆ7ä¸ªæ„ŸçŸ¥ä»»åŠ¡å’Œ6ä¸ªæ¨ç†ä»»åŠ¡ï¼‰ï¼Œä»¥åŠ5ä¸ªä»£è¡¨æ€§ç±»åˆ«ã€‚å¯¹æœ€å…ˆè¿›çš„å¼€æºï¼ˆä¾‹å¦‚InternVL2.5ã€Qwen2.5-VLï¼‰å’Œé—­æºï¼ˆä¾‹å¦‚GPT-4oã€Gemini2-Flashå’ŒGemini2-Proï¼‰MLLMçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨å›¾åƒåŸºåœ°è§†é¢‘æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œæœ€å¤šä»…è¾¾åˆ°28.9%çš„å‡†ç¡®ç‡ã€‚è¿›ä¸€æ­¥çš„åˆ†ææ­ç¤ºäº†å½±å“IV-Benchä¸Šæ¨¡å‹æ€§èƒ½çš„å…³é”®å› ç´ ï¼ŒåŒ…æ‹¬æ¨ç†æ¨¡å¼ã€å¸§æ•°ä»¥åŠåˆ†è¾¨ç‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç®€å•çš„æ•°æ®åˆæˆæ–¹æ³•ï¼Œæˆ‘ä»¬è¯æ˜äº†IV-Benchçš„æŒ‘æˆ˜ä¸ä»…ä»…åœ¨äºè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ•°æ®æ ¼å¼å¯¹é½ã€‚è¿™äº›å‘ç°å…±åŒä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å®è´µçš„è§è§£ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/multimodal-art-projection/IV-Bench%E3%80%82">https://github.com/multimodal-art-projection/IV-Benchã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15415v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€é¡¹åä¸ºIV-Benchçš„æ–°è¯„ä¼°åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å›¾æ–‡ç»“åˆçš„å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å›¾åƒåŸºç¡€è§†é¢‘æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚å®ƒåŒ…å«å¤§é‡è§†é¢‘å’Œé…å¯¹å›¾åƒæ–‡æœ¬æŸ¥è¯¢ï¼Œæ¶‰åŠå¤šç§ä»»åŠ¡å’Œç±»åˆ«ã€‚å¯¹ç°æœ‰æ¨¡å‹çš„è¯„ä¼°è¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨æ­¤ç±»ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸¥é‡ä¸è¶³ï¼Œè€Œä¸€äº›å…³é”®å› ç´ å¦‚æ¨ç†æ¨¡å¼ã€å¸§æ•°ã€åˆ†è¾¨ç‡ç­‰å½±å“äº†æ¨¡å‹åœ¨IV-Benchä¸Šçš„æ€§èƒ½ã€‚é€šè¿‡æ•°æ®åˆæˆæ–¹æ³•ï¼Œä½œè€…å±•ç¤ºäº†IV-Benchçš„æŒ‘æˆ˜ä¸ä»…åœ¨äºæ•°æ®æ ¼å¼çš„å¯¹é½ï¼Œè€Œä¸”å…·æœ‰ä¸ºæœªæ¥ç ”ç©¶æä¾›æœ‰ä»·å€¼è§è§£çš„æ½œåŠ›ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å·²å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IV-Benchæ˜¯é¦–ä¸ªé’ˆå¯¹å›¾åƒåŸºç¡€è§†é¢‘æ„ŸçŸ¥å’Œæ¨ç†çš„è¯„ä¼°åŸºå‡†ã€‚</li>
<li>å®ƒåŒ…å«967ä¸ªè§†é¢‘å’Œ2,585ä¸ªç²¾å¿ƒæ ‡æ³¨çš„å›¾åƒæ–‡æœ¬æŸ¥è¯¢ã€‚</li>
<li>IV-Benchæ¶µç›–13ä¸ªä»»åŠ¡ï¼Œåˆ†ä¸ºæ„ŸçŸ¥å’Œæ¨ç†ä»»åŠ¡ä¸¤å¤§ç±»ï¼Œä»¥åŠäº”ä¸ªä»£è¡¨æ€§ç±»åˆ«ã€‚</li>
<li>å½“å‰å¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹åœ¨IV-Benchä¸Šçš„è¡¨ç°ä»¤äººå¤±æœ›ï¼Œæœ€é«˜å‡†ç¡®ç‡ä»…ä¸º28.9%ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½å—åˆ°æ¨ç†æ¨¡å¼ã€å¸§æ•°ã€åˆ†è¾¨ç‡ç­‰å› ç´ çš„å½±å“ã€‚</li>
<li>é€šè¿‡æ•°æ®åˆæˆæ–¹æ³•ï¼Œå±•ç¤ºäº†IV-Benchçš„æŒ‘æˆ˜ä¸ä»…é™äºè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ•°æ®æ ¼å¼å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15415">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3b58c4d5ae5f4d5b063216f8150eaeb6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-79d6839deb5f606a3fbc48ed15861d7e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="PolicyEvol-Agent-Evolving-Policy-via-Environment-Perception-and-Self-Awareness-with-Theory-of-Mind"><a href="#PolicyEvol-Agent-Evolving-Policy-via-Environment-Perception-and-Self-Awareness-with-Theory-of-Mind" class="headerlink" title="PolicyEvol-Agent: Evolving Policy via Environment Perception and   Self-Awareness with Theory of Mind"></a>PolicyEvol-Agent: Evolving Policy via Environment Perception and   Self-Awareness with Theory of Mind</h2><p><strong>Authors:Yajie Yu, Yue Feng</strong></p>
<p>Multi-agents has exhibited significant intelligence in real-word simulations with Large language models (LLMs) due to the capabilities of social cognition and knowledge retrieval. However, existing research on agents equipped with effective cognition chains including reasoning, planning, decision-making and reflecting remains limited, especially in the dynamically interactive scenarios. In addition, unlike human, prompt-based responses face challenges in psychological state perception and empirical calibration during uncertain gaming process, which can inevitably lead to cognition bias. In light of above, we introduce PolicyEvol-Agent, a comprehensive LLM-empowered framework characterized by systematically acquiring intentions of others and adaptively optimizing irrational strategies for continual enhancement. Specifically, PolicyEvol-Agent first obtains reflective expertise patterns and then integrates a range of cognitive operations with Theory of Mind alongside internal and external perspectives. Simulation results, outperforming RL-based models and agent-based methods, demonstrate the superiority of PolicyEvol-Agent for final gaming victory. Moreover, the policy evolution mechanism reveals the effectiveness of dynamic guideline adjustments in both automatic and human evaluation. </p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“åœ¨ç°å®ä¸–ç•Œçš„æ¨¡æ‹Ÿä¸­å±•ç°å‡ºäº†æ˜¾è‘—æ™ºèƒ½ï¼Œè¿™å½’åŠŸäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç¤¾ä¼šè®¤çŸ¥å’ŒçŸ¥è¯†æ£€ç´¢èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…³äºé…å¤‡æœ‰æ•ˆè®¤çŸ¥é“¾çš„æ™ºèƒ½ä½“çš„ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œè¿™äº›è®¤çŸ¥é“¾åŒ…æ‹¬æ¨ç†ã€è§„åˆ’ã€å†³ç­–å’Œåæ€ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€äº¤äº’åœºæ™¯ä¸­ã€‚æ­¤å¤–ï¼Œä¸äººç±»ä¸åŒï¼ŒåŸºäºæç¤ºçš„å›åº”åœ¨æ¸¸æˆè¿‡ç¨‹ä¸­çš„å¿ƒç†çŠ¶æ€æ„ŸçŸ¥å’Œç»éªŒæ ¡å‡†æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´ä¸å¯é¿å…çš„è®¤çŸ¥åè§ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†PolicyEvol-Agentï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„LLMèµ‹èƒ½æ¡†æ¶ï¼Œå…¶ç‰¹ç‚¹æ˜¯ç³»ç»Ÿåœ°è·å–ä»–äººçš„æ„å›¾å¹¶è‡ªé€‚åº”ä¼˜åŒ–éç†æ€§ç­–ç•¥ä»¥å®ç°æŒç»­å¢å¼ºã€‚å…·ä½“æ¥è¯´ï¼ŒPolicyEvol-Agenté¦–å…ˆè·å¾—åæ€ä¸“å®¶æ¨¡å¼ï¼Œç„¶åå°†ä¸€ç³»åˆ—è®¤çŸ¥æ“ä½œä¸å¿ƒæ™ºç†è®ºä»¥åŠå†…å¤–è§†è§’ç›¸ç»“åˆã€‚ä»¿çœŸç»“æœè¯æ˜PolicyEvol-Agentä¼˜äºåŸºäºRLçš„æ¨¡å‹å’ŒåŸºäºä»£ç†çš„æ–¹æ³•ï¼Œä¸ºæœ€ç»ˆçš„æ¸¸æˆèƒœåˆ©å¸¦æ¥äº†ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œæ”¿ç­–è¿›åŒ–æœºåˆ¶è¡¨æ˜åŠ¨æ€æŒ‡å—è°ƒæ•´åœ¨è‡ªåŠ¨å’Œäººç±»è¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15313v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ™ºèƒ½ä½“åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠ æŒä¸‹ï¼Œåœ¨æ¨¡æ‹Ÿåœºæ™¯ä¸­å±•ç°äº†ç¤¾ä¼šè®¤çŸ¥å’ŒçŸ¥è¯†æ£€ç´¢çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›®å‰å¯¹äºåŒ…å«æ¨ç†ã€è§„åˆ’ã€å†³ç­–å’Œåæ€çš„è®¤çŸ¥é“¾ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œå°¤å…¶åœ¨åŠ¨æ€äº¤äº’åœºæ™¯ä¸­ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºPolicyEvol-Agentæ¡†æ¶ï¼Œå…·å¤‡è·å–ä»–äººæ„å›¾å’Œç³»ç»Ÿä¼˜åŒ–éç†æ€§ç­–ç•¥çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¿ƒæ™ºç†è®ºã€å†…éƒ¨å’Œå¤–éƒ¨è§†è§’ï¼Œé€šè¿‡æ¨¡æ‹ŸéªŒè¯å…¶ä¼˜è¶Šæ€§ï¼Œè¶…è¶ŠåŸºäºå¼ºåŒ–å­¦ä¹ æ¨¡å‹å’Œæ™ºèƒ½ä½“çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç­–ç•¥è¿›åŒ–æœºåˆ¶æ˜¾ç¤ºå‡ºåŠ¨æ€æŒ‡å¯¼è°ƒæ•´çš„æœ‰æ•ˆæ€§ã€‚åœ¨ä¸ç¡®å®šçš„æ¸¸æˆè¿‡ç¨‹ä¸­é¢ä¸´ç€å¿ƒç†çŠ¶æ€æ„ŸçŸ¥å’Œç»éªŒæ ¡å‡†çš„æŒ‘æˆ˜ï¼Œå¯èƒ½å¯¼è‡´è®¤çŸ¥åè§ã€‚PolicyEvol-Agentèƒ½å¤Ÿè·å–åæ€ä¸“å®¶æ¨¡å¼ï¼Œé€‚åº”æ€§åœ°ä¼˜åŒ–éç†æ€§ç­–ç•¥ä»¥å®ç°æŒç»­æ”¹è¿›ã€‚æ€»ä½“è€Œè¨€ï¼ŒPolicyEvol-Agentæ¡†æ¶æœ‰åŠ©äºæå‡æ™ºèƒ½ä½“çš„è®¤çŸ¥èƒ½åŠ›å’Œæ¸¸æˆèƒœåˆ©æ¦‚ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“åœ¨æ¨¡æ‹Ÿåœºæ™¯ä¸­å±•ç°å‡ºç¤¾ä¼šè®¤çŸ¥å’ŒçŸ¥è¯†æ£€ç´¢èƒ½åŠ›ã€‚</li>
<li>ç›®å‰å¯¹äºæ™ºèƒ½ä½“çš„è®¤çŸ¥é“¾ç ”ç©¶æœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨æ€äº¤äº’åœºæ™¯ä¸­ã€‚</li>
<li>PolicyEvol-Agentæ¡†æ¶å…·å¤‡è·å–ä»–äººæ„å›¾å’Œç³»ç»Ÿä¼˜åŒ–éç†æ€§ç­–ç•¥çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†å¿ƒæ™ºç†è®ºã€å†…éƒ¨å’Œå¤–éƒ¨è§†è§’ã€‚</li>
<li>æ¨¡æ‹ŸéªŒè¯PolicyEvol-Agentæ¡†æ¶çš„ä¼˜è¶Šæ€§ï¼Œè¶…è¶ŠåŸºäºå¼ºåŒ–å­¦ä¹ æ¨¡å‹å’Œæ™ºèƒ½ä½“çš„æ–¹æ³•ã€‚</li>
<li>ç­–ç•¥è¿›åŒ–æœºåˆ¶æ˜¾ç¤ºåŠ¨æ€æŒ‡å¯¼è°ƒæ•´çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a48b9b7e68e71d128d96181f0d26788b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-011d8c6652455516c246424176e277b1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="LogicTree-Structured-Proof-Exploration-for-Coherent-and-Rigorous-Logical-Reasoning-with-Large-Language-Models"><a href="#LogicTree-Structured-Proof-Exploration-for-Coherent-and-Rigorous-Logical-Reasoning-with-Large-Language-Models" class="headerlink" title="LogicTree: Structured Proof Exploration for Coherent and Rigorous   Logical Reasoning with Large Language Models"></a>LogicTree: Structured Proof Exploration for Coherent and Rigorous   Logical Reasoning with Large Language Models</h2><p><strong>Authors:Kang He, Kaushik Roy</strong></p>
<p>Large language models (LLMs) have achieved remarkable multi-step reasoning capabilities across various domains. However, LLMs still face distinct challenges in complex logical reasoning, as (1) proof-finding requires systematic exploration and the maintenance of logical coherence and (2) searching the right combination of premises at each reasoning step is inherently challenging in tasks with large premise space. To address this, we propose LogicTree, an inference-time modular framework employing algorithm-guided search to automate structured proof exploration and ensure logical coherence. Advancing beyond tree-of-thought (ToT), we incorporate caching mechanism into LogicTree to enable effective utilization of historical knowledge, preventing reasoning stagnation and minimizing redundancy. Furthermore, we address the combinatorial complexity of premise search by decomposing it into a linear process. The refined premise selection restricts subsequent inference to at most one derivation per step, enhancing reasoning granularity and enforcing strict step-by-step reasoning. Additionally, we introduce two LLM-free heuristics for premise prioritization, enabling strategic proof search. Experimental results on five datasets demonstrate that LogicTree optimally scales inference-time computation to achieve higher proof accuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6% and 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o outperforms o3-mini by 7.6% on average. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ä¸ªé¢†åŸŸéƒ½å–å¾—äº†æ˜¾è‘—çš„å¤šæ­¥æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒLLMsåœ¨å¤æ‚é€»è¾‘æ¨ç†æ–¹é¢ä»é¢ä¸´ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œå› ä¸ºï¼ˆ1ï¼‰è¯æ˜å¯»æ‰¾éœ€è¦ç³»ç»Ÿçš„æ¢ç´¢å’Œç»´æŒé€»è¾‘è¿è´¯æ€§ï¼›ï¼ˆ2ï¼‰åœ¨å…·æœ‰å¤§é‡å‰æç©ºé—´çš„ä»»åŠ¡ä¸­ï¼Œæœç´¢æ¯ä¸€æ­¥æ¨ç†çš„æ­£ç¡®å‰æç»„åˆæœ¬è´¨ä¸Šæ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LogicTreeï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨ç†æ—¶é—´æ¨¡å—åŒ–æ¡†æ¶ï¼Œé‡‡ç”¨ç®—æ³•æŒ‡å¯¼çš„æœç´¢æ¥è‡ªåŠ¨ç»“æ„åŒ–è¯æ˜æ¢ç´¢å¹¶ç¡®ä¿é€»è¾‘è¿è´¯æ€§ã€‚è¶…è¶Šæ€ç»´æ ‘ï¼ˆToTï¼‰ï¼Œæˆ‘ä»¬åœ¨LogicTreeä¸­å¼•å…¥äº†ç¼“å­˜æœºåˆ¶ï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨å†å²çŸ¥è¯†ï¼Œé˜²æ­¢æ¨ç†åœæ»å¹¶å°½é‡å‡å°‘å†—ä½™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†å‰ææœç´¢çš„ç»„åˆå¤æ‚æ€§åˆ†è§£ä¸ºçº¿æ€§è¿‡ç¨‹æ¥è§£å†³ã€‚ç²¾ç‚¼çš„å‰æé€‰æ‹©å°†éšåçš„æ¨ç†é™åˆ¶åœ¨æ¯ä¸€æ­¥æœ€å¤šåªæœ‰ä¸€ä¸ªæ¨å¯¼ï¼Œæé«˜äº†æ¨ç†çš„ç²’åº¦å¹¶å®æ–½äº†ä¸¥æ ¼çš„é€æ­¥æ¨ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§ç”¨äºå‰æä¼˜å…ˆçº§çš„LLMè‡ªç”±å¯å‘å¼æ–¹æ³•ï¼Œä»¥å®ç°æˆ˜ç•¥æ€§çš„è¯æ˜æœç´¢ã€‚åœ¨äº”ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLogicTreeæœ€ä¼˜åœ°æ‰©å±•äº†æ¨ç†æ—¶é—´çš„è®¡ç®—ï¼Œå®ç°äº†æ›´é«˜çš„è¯æ˜ç²¾åº¦ï¼Œè¶…è¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰å’ŒToTçš„å¹³å‡å¢ç›Šåˆ†åˆ«ä¸º23.6%å’Œ12.5%ï¼Œåœ¨GPT-4oä¸Šè¡¨ç°å°¤å…¶æ˜æ˜¾ã€‚è€Œä¸”ï¼Œåœ¨LogicTreeå†…éƒ¨ï¼ŒGPT-4oå¹³å‡æ¯”o3-minié«˜å‡º7.6%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14089v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ­¥æ¨ç†é¢†åŸŸå±•ç°å‡ºæ˜¾è‘—èƒ½åŠ›ï¼Œä½†åœ¨å¤æ‚é€»è¾‘æ¨ç†æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†LogicTreeæ¡†æ¶ï¼Œé‡‡ç”¨ç®—æ³•å¼•å¯¼æœç´¢ï¼Œå®ç°ç»“æ„åŒ–è¯æ˜æ¢ç´¢å¹¶ç¡®ä¿é€»è¾‘è¿è´¯æ€§ã€‚LogicTreeèå…¥ç¼“å­˜æœºåˆ¶ï¼Œæœ‰æ•ˆåˆ©ç”¨å†å²çŸ¥è¯†ï¼Œé˜²æ­¢æ¨ç†åœæ»å¹¶å‡å°‘å†—ä½™ã€‚æ­¤å¤–ï¼Œå®ƒé€šè¿‡åˆ†è§£å‰ææœç´¢ä¸ºçº¿æ€§è¿‡ç¨‹ï¼Œè§£å†³ç»„åˆå¤æ‚æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLogicTreeåœ¨æ¨ç†æ—¶é—´è®¡ç®—ä¸Šå®ç°ä¼˜åŒ–è§„æ¨¡ï¼Œæé«˜è¯æ˜å‡†ç¡®æ€§ï¼Œä¼˜äºé“¾å¼æ€ç»´ï¼ˆCoTï¼‰å’Œæ ‘çŠ¶æ€ç»´ï¼ˆToTï¼‰ï¼Œå¹¶åœ¨GPT-4oä¸Šå¹³å‡æå‡23.6%ã€‚åŒæ—¶ï¼Œåœ¨LogicTreeå†…éƒ¨ï¼ŒGPT-4oè¾ƒo3-miniå¹³å‡é«˜å‡º7.6%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šæ­¥æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚é€»è¾‘æ¨ç†ä¸­ä»å­˜æŒ‘æˆ˜ã€‚</li>
<li>LogicTreeæ¡†æ¶é‡‡ç”¨ç®—æ³•å¼•å¯¼æœç´¢ï¼Œå®ç°ç»“æ„åŒ–è¯æ˜æ¢ç´¢ï¼Œç¡®ä¿é€»è¾‘è¿è´¯æ€§ã€‚</li>
<li>LogicTreeèå…¥ç¼“å­˜æœºåˆ¶ï¼Œæœ‰æ•ˆåˆ©ç”¨å†å²çŸ¥è¯†ï¼Œé˜²æ­¢æ¨ç†åœæ»ã€‚</li>
<li>é€šè¿‡åˆ†è§£å‰ææœç´¢ä¸ºçº¿æ€§è¿‡ç¨‹ï¼Œè§£å†³ç»„åˆå¤æ‚æ€§ã€‚</li>
<li>LogicTreeä¼˜åŒ–æ¨ç†æ—¶é—´è®¡ç®—ï¼Œæé«˜è¯æ˜å‡†ç¡®æ€§ã€‚</li>
<li>LogicTreeè¾ƒé“¾å¼æ€ç»´ï¼ˆCoTï¼‰å’Œæ ‘çŠ¶æ€ç»´ï¼ˆToTï¼‰è¡¨ç°æ›´ä¼˜ï¼Œåœ¨GPT-4oä¸Šå¹³å‡æå‡23.6%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14089">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5765b9c6e1436a44cfb95fa3f0bd908a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7701b7e3aff702910f54a2d630c0d9e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7be7e4a7c410b6808cfe30855059d84.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DoomArena-A-framework-for-Testing-AI-Agents-Against-Evolving-Security-Threats"><a href="#DoomArena-A-framework-for-Testing-AI-Agents-Against-Evolving-Security-Threats" class="headerlink" title="DoomArena: A framework for Testing AI Agents Against Evolving Security   Threats"></a>DoomArena: A framework for Testing AI Agents Against Evolving Security   Threats</h2><p><strong>Authors:Leo Boisvert, Mihir Bansal, Chandra Kiran Reddy Evuru, Gabriel Huang, Abhay Puri, Avinandan Bose, Maryam Fazel, Quentin Cappart, Jason Stanley, Alexandre Lacoste, Alexandre Drouin, Krishnamurthy Dvijotham</strong></p>
<p>We present DoomArena, a security evaluation framework for AI agents. DoomArena is designed on three principles: 1) It is a plug-in framework and integrates easily into realistic agentic frameworks like BrowserGym (for web agents) and $\tau$-bench (for tool calling agents); 2) It is configurable and allows for detailed threat modeling, allowing configuration of specific components of the agentic framework being attackable, and specifying targets for the attacker; and 3) It is modular and decouples the development of attacks from details of the environment in which the agent is deployed, allowing for the same attacks to be applied across multiple environments. We illustrate several advantages of our framework, including the ability to adapt to new threat models and environments easily, the ability to easily combine several previously published attacks to enable comprehensive and fine-grained security testing, and the ability to analyze trade-offs between various vulnerabilities and performance. We apply DoomArena to state-of-the-art (SOTA) web and tool-calling agents and find a number of surprising results: 1) SOTA agents have varying levels of vulnerability to different threat models (malicious user vs malicious environment), and there is no Pareto dominant agent across all threat models; 2) When multiple attacks are applied to an agent, they often combine constructively; 3) Guardrail model-based defenses seem to fail, while defenses based on powerful SOTA LLMs work better. DoomArena is available at <a target="_blank" rel="noopener" href="https://github.com/ServiceNow/DoomArena">https://github.com/ServiceNow/DoomArena</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºDoomArenaï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹AIä»£ç†çš„å®‰å…¨è¯„ä¼°æ¡†æ¶ã€‚DoomArenaåŸºäºä¸‰ä¸ªåŸåˆ™è®¾è®¡ï¼š1ï¼‰å®ƒæ˜¯ä¸€ä¸ªæ’ä»¶æ¡†æ¶ï¼Œå¯ä»¥è½»æ¾åœ°é›†æˆåˆ°å®é™…çš„ä»£ç†æ¡†æ¶ä¸­ï¼Œä¾‹å¦‚BrowserGymï¼ˆç”¨äºç½‘ç»œä»£ç†ï¼‰å’ŒÏ„-benchï¼ˆç”¨äºå·¥å…·è°ƒç”¨ä»£ç†ï¼‰ï¼›2ï¼‰å®ƒæ˜¯å¯é…ç½®çš„ï¼Œå…è®¸è¿›è¡Œè¯¦ç»†å¨èƒå»ºæ¨¡ï¼Œå…è®¸é…ç½®ä»£ç†æ¡†æ¶çš„ç‰¹å®šç»„ä»¶å—åˆ°æ”»å‡»ï¼Œå¹¶ä¸ºæ”»å‡»è€…æŒ‡å®šç›®æ ‡ï¼›3ï¼‰å®ƒæ˜¯æ¨¡å—åŒ–çš„ï¼Œå°†æ”»å‡»çš„å‘å±•ä¸ä»£ç†éƒ¨ç½²çš„ç¯å¢ƒç»†èŠ‚åˆ†å¼€ï¼Œä½¿å¾—ç›¸åŒçš„æ”»å‡»å¯ä»¥åº”ç”¨äºå¤šä¸ªç¯å¢ƒã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬æ¡†æ¶çš„å‡ ä¸ªä¼˜ç‚¹ï¼ŒåŒ…æ‹¬èƒ½å¤Ÿè½»æ¾é€‚åº”æ–°çš„å¨èƒæ¨¡å‹å’Œç¯å¢ƒã€èƒ½å¤Ÿè½»æ¾ç»„åˆä¹‹å‰å‘å¸ƒçš„å¤šä¸ªæ”»å‡»ä»¥å®ç°å…¨é¢å’Œç²¾ç»†çš„å®‰å…¨æµ‹è¯•ï¼Œä»¥åŠåˆ†æå„ç§æ¼æ´å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬å°†DoomArenaåº”ç”¨äºæœ€å…ˆè¿›çš„ç½‘ç»œå’Œå·¥å…·è°ƒç”¨ä»£ç†ï¼Œå¹¶å‘ç°äº†ä¸€äº›æ„æƒ³ä¸åˆ°çš„ç»“æœï¼š1ï¼‰æœ€å…ˆè¿›çš„ä»£ç†å¯¹ä¸åŒå¨èƒæ¨¡å‹ï¼ˆæ¶æ„ç”¨æˆ·ä¸æ¶æ„ç¯å¢ƒï¼‰çš„è„†å¼±æ€§ç¨‹åº¦ä¸åŒï¼Œæ²¡æœ‰åœ¨æ‰€æœ‰å¨èƒæ¨¡å‹ä¸­å æ®ç»å¯¹ä¼˜åŠ¿çš„ä»£ç†ï¼›2ï¼‰å½“å¤šä¸ªæ”»å‡»åŒæ—¶æ”»å‡»ä¸€ä¸ªä»£ç†æ—¶ï¼Œå®ƒä»¬å¾€å¾€ä¼šç»“åˆå»ºè®¾æ€§åœ°è¿›è¡Œæ”»å‡»ï¼›3ï¼‰åŸºäºæŠ¤æ æ¨¡å‹çš„é˜²å¾¡ä¼¼ä¹å¤±è´¥ï¼Œè€ŒåŸºäºå…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„é˜²å¾¡æ•ˆæœæ›´å¥½ã€‚ä½ å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/ServiceNow/DoomArena%E8%8E%B7%E5%8F%96DoomArena%E3%80%82">https://github.com/ServiceNow/DoomArenaè·å–DoomArenaã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14064v2">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨ä»‹ç»äº†ä¸€é¡¹åä¸ºDoomArenaçš„é’ˆå¯¹äººå·¥æ™ºèƒ½çš„å®‰å…¨è¯„ä¼°æ¡†æ¶ï¼Œå…¶ç‰¹ç‚¹æ˜¯åŸºäºä¸‰ä¸ªè®¾è®¡åŸåˆ™ï¼šæ’ä»¶åŒ–ã€å¯é…ç½®å’Œæ¨¡å—åŒ–ã€‚è¯¥æ¡†æ¶å¯ä»¥è½»æ¾åœ°é›†æˆåˆ°çœŸå®çš„ä»£ç†æ¡†æ¶ä¸­ï¼Œå…è®¸è¯¦ç»†çš„å¨èƒå»ºæ¨¡ï¼Œæ¨¡å—åŒ–è®¾è®¡ä½¿å¾—æ”»å‡»çš„å¼€å‘ç‹¬ç«‹äºä»£ç†éƒ¨ç½²çš„ç¯å¢ƒç»†èŠ‚ã€‚æ–‡ç« è¿˜ä»‹ç»äº†è¯¥æ¡†æ¶çš„å‡ ä¸ªä¼˜åŠ¿ï¼Œå¦‚æ˜“äºé€‚åº”æ–°çš„å¨èƒæ¨¡å‹å’Œå¤šç§ç¯å¢ƒï¼Œèƒ½å¤Ÿç»„åˆå¤šä¸ªå…ˆå‰å‘å¸ƒçš„æ”»å‡»è¿›è¡Œç»†è‡´å…¨é¢çš„å®‰å…¨æµ‹è¯•ï¼Œå¹¶åˆ†æäº†å„ç§æ¼æ´å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡ã€‚é€šè¿‡åº”ç”¨è¯¥æ¡†æ¶äºå½“å‰å…ˆè¿›çš„Webå’Œå·¥å…·è°ƒç”¨ä»£ç†ï¼Œå‘ç°äº†ä¸€äº›æ„å¤–ç»“æœã€‚å¯¹äºä¸åŒçš„å¨èƒæ¨¡å‹ï¼ˆæ¶æ„ç”¨æˆ·å’Œæ¶æ„ç¯å¢ƒï¼‰ï¼Œå…ˆè¿›ä»£ç†çš„è„†å¼±æ€§ç¨‹åº¦å„ä¸ç›¸åŒï¼Œå¹¶ä¸”ä¸å­˜åœ¨å¯¹æ‰€æœ‰å¨èƒæ¨¡å‹éƒ½è¡¨ç°æœ€å¥½çš„ä»£ç†ï¼›å½“å¯¹ä»£ç†å‘èµ·å¤šé‡æ”»å‡»æ—¶ï¼Œå®ƒä»¬å¾€å¾€ä¼šååŒä½œç”¨ï¼›åŸºäºGuardrailæ¨¡å‹çš„é˜²å¾¡ä¼¼ä¹å¤±è´¥ï¼Œè€ŒåŸºäºå…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„é˜²å¾¡åˆ™è¡¨ç°æ›´å¥½ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·è®¿é—®å…¶GitHubé¡µé¢ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DoomArenaæ˜¯ä¸€ä¸ªé’ˆå¯¹AIçš„å®‰å…¨è¯„ä¼°æ¡†æ¶ï¼Œå…·æœ‰æ’ä»¶åŒ–ã€å¯é…ç½®å’Œæ¨¡å—åŒ–ä¸‰å¤§ç‰¹ç‚¹ã€‚</li>
<li>å¯ä»¥è½»æ¾é›†æˆåˆ°çœŸå®çš„ä»£ç†æ¡†æ¶ä¸­ï¼Œå…è®¸è¯¦ç»†çš„å¨èƒå»ºæ¨¡ã€‚</li>
<li>è¯¥æ¡†æ¶å…·æœ‰é€‚åº”æ–°å¨èƒæ¨¡å‹å’Œå¤šç§ç¯å¢ƒçš„èƒ½åŠ›ã€‚</li>
<li>èƒ½å¤Ÿç»„åˆå¤šç§æ”»å‡»è¿›è¡Œå®‰å…¨æµ‹è¯•ï¼Œåˆ†æä¸åŒæ¼æ´å’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14064">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31126b96cd4229911e855f6d47bb069a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbd10cc46e604e41316b88cfa0dbb134.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-923c2dc444a5e0708b68bbf8b3b786c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fa598b951b3a08fa023e0d74617feea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab3005fa8351bb32ef33fdc773410355.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ddb819fdc6f1d17d31a3f60ad226a59.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Think-Deep-Think-Fast-Investigating-Efficiency-of-Verifier-free-Inference-time-scaling-Methods"><a href="#Think-Deep-Think-Fast-Investigating-Efficiency-of-Verifier-free-Inference-time-scaling-Methods" class="headerlink" title="Think Deep, Think Fast: Investigating Efficiency of Verifier-free   Inference-time-scaling Methods"></a>Think Deep, Think Fast: Investigating Efficiency of Verifier-free   Inference-time-scaling Methods</h2><p><strong>Authors:Junlin Wang, Shang Zhu, Jon Saad-Falcon, Ben Athiwaratkun, Qingyang Wu, Jue Wang, Shuaiwen Leon Song, Ce Zhang, Bhuwan Dhingra, James Zou</strong></p>
<p>There is intense interest in investigating how inference time compute (ITC) (e.g. repeated sampling, refinements, etc) can improve large language model (LLM) capabilities. At the same time, recent breakthroughs in reasoning models, such as Deepseek-R1, unlock the opportunity for reinforcement learning to improve LLM reasoning skills. An in-depth understanding of how ITC interacts with reasoning across different models could provide important guidance on how to further advance the LLM frontier. This work conducts a comprehensive analysis of inference-time scaling methods for both reasoning and non-reasoning models on challenging reasoning tasks. Specifically, we focus our research on verifier-free inference time-scaling methods due to its generalizability without needing a reward model. We construct the Pareto frontier of quality and efficiency. We find that non-reasoning models, even with an extremely high inference budget, still fall substantially behind reasoning models. For reasoning models, majority voting proves to be a robust inference strategy, generally competitive or outperforming other more sophisticated ITC methods like best-of-N and sequential revisions, while the additional inference compute offers minimal improvements. We further perform in-depth analyses of the association of key response features (length and linguistic markers) with response quality, with which we can improve the existing ITC methods. We find that correct responses from reasoning models are typically shorter and have fewer hedging and thinking markers (but more discourse markers) than the incorrect responses. </p>
<blockquote>
<p>å…³äºå¦‚ä½•è¿ç”¨æ¨ç†æ—¶é—´è®¡ç®—ï¼ˆITCï¼‰ï¼ˆä¾‹å¦‚é‡å¤é‡‡æ ·ã€ç²¾ç»†åŒ–ç­‰ï¼‰æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ï¼Œäººä»¬è¡¨ç°å‡ºæµ“åšçš„å…´è¶£ã€‚åŒæ—¶ï¼ŒåƒDeepseek-R1è¿™æ ·çš„æ¨ç†æ¨¡å‹çš„çªç ´ï¼Œå¼€å¯äº†å¼ºåŒ–å­¦ä¹ æå‡LLMæ¨ç†èƒ½åŠ›çš„å¯èƒ½æ€§ã€‚æ·±å…¥äº†è§£ITCåœ¨ä¸åŒæ¨¡å‹ä¸­ä¸æ¨ç†çš„äº¤äº’ä½œç”¨ï¼Œå¯ä»¥ä¸ºè¿›ä¸€æ­¥æ¨åŠ¨LLMå‰æ²¿æä¾›é‡è¦æŒ‡å¯¼ã€‚æœ¬ç ”ç©¶å¯¹æ¨ç†å’Œéæ¨ç†æ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨ç†ä»»åŠ¡ä¸Šçš„æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•è¿›è¡Œäº†ç»¼åˆåˆ†æã€‚æˆ‘ä»¬ç‰¹åˆ«å…³æ³¨æ— éœ€éªŒè¯å™¨ï¼ˆverifierï¼‰çš„æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼Œå› ä¸ºå®ƒå…·æœ‰ä¸éœ€è¦å¥–åŠ±æ¨¡å‹çš„é€šç”¨æ€§ã€‚æˆ‘ä»¬æ„å»ºäº†è´¨é‡ä¸æ•ˆç‡ä¹‹é—´çš„å¸•ç´¯æ‰˜å‰æ²¿ã€‚æˆ‘ä»¬å‘ç°ï¼Œå³ä½¿æ‹¥æœ‰æé«˜çš„æ¨ç†é¢„ç®—ï¼Œéæ¨ç†æ¨¡å‹ä»ç„¶è¿œè¿œè½åäºæ¨ç†æ¨¡å‹ã€‚å¯¹äºæ¨ç†æ¨¡å‹æ¥è¯´ï¼Œå¤šæ•°æŠ•ç¥¨è¯æ˜æ˜¯ä¸€ç§ç¨³å¥çš„æ¨ç†ç­–ç•¥ï¼Œé€šå¸¸ä¸å…¶ä»–æ›´å¤æ‚çš„ITCæ–¹æ³•ç«äº‰æˆ–è¡¨ç°æ›´å¥½ï¼Œå¦‚é€‰æ‹©æœ€ä¼˜å’Œé¡ºåºä¿®è®¢ç­‰ï¼Œè€Œé¢å¤–çš„æ¨ç†è®¡ç®—æä¾›çš„æ•ˆæœå¾®ä¹å…¶å¾®ã€‚æˆ‘ä»¬è¿˜æ·±å…¥åˆ†æäº†å…³é”®å“åº”ç‰¹å¾ï¼ˆé•¿åº¦å’Œè¯­è¨€æ ‡è®°ï¼‰ä¸å“åº”è´¨é‡çš„å…³è”ï¼Œä»¥æ­¤æ”¹è¿›ç°æœ‰çš„ITCæ–¹æ³•ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ¨ç†æ¨¡å‹çš„æ­£ç¡®å“åº”é€šå¸¸æ›´çŸ­ï¼Œå«æœ‰è¾ƒå°‘çš„æ¨¡ç³Šå’Œæ€ç»´æ ‡è®°ï¼ˆä½†å«æœ‰æ›´å¤šçš„ç¯‡ç« æ ‡è®°ï¼‰æ¯”é”™è¯¯å“åº”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14047v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ¨ç†æ¨¡å‹å’Œéæ¨ç†æ¨¡å‹åœ¨æŒ‘æˆ˜æ€§æ¨ç†ä»»åŠ¡ä¸Šçš„æ¨ç†æ—¶é—´è®¡ç®—ï¼ˆITCï¼‰ç¼©æ”¾æ–¹æ³•ã€‚ç ”ç©¶é‡ç‚¹å…³æ³¨æ— éœ€å¥–åŠ±æ¨¡å‹çš„é€šç”¨æ€§å¼ºçš„æ— éªŒè¯å™¨æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œéæ¨ç†æ¨¡å‹å³ä½¿åœ¨æé«˜çš„æ¨ç†é¢„ç®—ä¸‹ï¼Œä¸æ¨ç†æ¨¡å‹çš„æ€§èƒ½å·®è·ä»ç„¶æ˜¾è‘—ã€‚å¯¹äºæ¨ç†æ¨¡å‹ï¼Œå¤šæ•°æŠ•ç¥¨è¯æ˜æ˜¯ä¸€ç§ç¨³å¥çš„æ¨ç†ç­–ç•¥ï¼Œé€šå¸¸ä¸å…¶ä»–æ›´å¤æ‚çš„ITCæ–¹æ³•ç«äº‰æˆ–è¡¨ç°æ›´å¥½ï¼Œè€Œé¢å¤–çš„æ¨ç†è®¡ç®—åªæä¾›å¾®å°çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼Œè¿˜å¯¹å…³é”®å“åº”ç‰¹å¾ä¸å“åº”è´¨é‡çš„å…³è”è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œä»¥æœŸæ”¹è¿›ç°æœ‰çš„ITCæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†æ—¶é—´è®¡ç®—ï¼ˆITCï¼‰å¯¹æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›æœ‰æµ“åšå…´è¶£ï¼Œä¸æ¨¡å‹æ¨ç†èƒ½åŠ›æœ‰å…³ã€‚</li>
<li>Deepseek-R1ç­‰çªç ´æ€§çš„æ¨ç†æ¨¡å‹ä¸ºå¼ºåŒ–å­¦ä¹ æå‡LLMæ¨ç†æŠ€èƒ½åˆ›é€ äº†æœºä¼šã€‚</li>
<li>åœ¨æŒ‘æˆ˜æ€§æ¨ç†ä»»åŠ¡ä¸Šï¼Œå¯¹æ¨ç†å’Œéæ¨ç†æ¨¡å‹çš„æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚</li>
<li>ç ”ç©¶é‡ç‚¹æ˜¯æ— éªŒè¯å™¨æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼Œå› å…¶å…·æœ‰æ™®éæ€§ï¼Œæ— éœ€å¥–åŠ±æ¨¡å‹ã€‚</li>
<li>éæ¨ç†æ¨¡å‹åœ¨æé«˜æ¨ç†é¢„ç®—ä¸‹ä»æ˜¾è‘—è½åäºæ¨ç†æ¨¡å‹ã€‚</li>
<li>å¤šæ•°æŠ•ç¥¨æ˜¯ä¸€ç§ç¨³å¥çš„æ¨ç†ç­–ç•¥ï¼Œé€šå¸¸æ¯”å…¶ä»–æ›´å¤æ‚çš„ITCæ–¹æ³•æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14047">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0bf3c50429ca70627ad892240bef46c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a06c6b5cfff79e9d44ba695dd16d64d.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Open-Medical-R1-How-to-Choose-Data-for-RLVR-Training-at-Medicine-Domain"><a href="#Open-Medical-R1-How-to-Choose-Data-for-RLVR-Training-at-Medicine-Domain" class="headerlink" title="Open-Medical-R1: How to Choose Data for RLVR Training at Medicine Domain"></a>Open-Medical-R1: How to Choose Data for RLVR Training at Medicine Domain</h2><p><strong>Authors:Zhongxi Qiu, Zhang Zhang, Yan Hu, Heng Li, Jiang Liu</strong></p>
<p>This paper explores optimal data selection strategies for Reinforcement Learning with Verified Rewards (RLVR) training in the medical domain. While RLVR has shown exceptional potential for enhancing reasoning capabilities in large language models, most prior implementations have focused on mathematics and logical puzzles, with limited exploration of domain-specific applications like medicine. We investigate four distinct data sampling strategies from MedQA-USMLE: random sampling (baseline), and filtering using Phi-4, Gemma-3-27b-it, and Gemma-3-12b-it models. Using Gemma-3-12b-it as our base model and implementing Group Relative Policy Optimization (GRPO), we evaluate performance across multiple benchmarks including MMLU, GSM8K, MMLU-Pro, and CMMLU. Our findings demonstrate that models trained on filtered data generally outperform those trained on randomly selected samples. Notably, training on self-filtered samples (using Gemma-3-12b-it for filtering) achieved superior performance in medical domains but showed reduced robustness across different benchmarks, while filtering with larger models from the same series yielded better overall robustness. These results provide valuable insights into effective data organization strategies for RLVR in specialized domains and highlight the importance of thoughtful data selection in achieving optimal performance. You can access our repository (<a target="_blank" rel="noopener" href="https://github.com/Qsingle/open-medical-r1">https://github.com/Qsingle/open-medical-r1</a>) to get the codes. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†é’ˆå¯¹åŒ»å­¦é¢†åŸŸå¼ºåŒ–å­¦ä¹ éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è®­ç»ƒçš„æœ€ä¼˜æ•°æ®é€‰æ‹©ç­–ç•¥ã€‚å°½ç®¡RLVRåœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å¤§å¤šæ•°å…ˆå‰çš„ç ”ç©¶éƒ½é›†ä¸­åœ¨æ•°å­¦å’Œé€»è¾‘è°œé¢˜ä¸Šï¼Œå¯¹ç‰¹å®šé¢†åŸŸçš„åº”ç”¨ç¨‹åºï¼ˆå¦‚åŒ»å­¦ï¼‰çš„æ¢ç´¢æœ‰é™ã€‚æˆ‘ä»¬ä»MedQA-USMLEè°ƒæŸ¥äº†å››ç§ä¸åŒçš„æ•°æ®é‡‡æ ·ç­–ç•¥ï¼šéšæœºé‡‡æ ·ï¼ˆåŸºçº¿ï¼‰ï¼Œä»¥åŠä½¿ç”¨Phi-4ã€Gemma-3-27b-itå’ŒGemma-3-12b-itæ¨¡å‹çš„è¿‡æ»¤ã€‚æˆ‘ä»¬ä»¥Gemma-3-12b-itä¸ºåŸºç¡€æ¨¡å‹ï¼Œé‡‡ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬MMLUã€GSM8Kã€MMLU-Proå’ŒCMMLUï¼‰ä¸Šè¯„ä¼°æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»è¿‡è¿‡æ»¤çš„æ•°æ®è®­ç»ƒçš„æ¨¡å‹é€šå¸¸æ¯”ç»è¿‡éšæœºé€‰æ‹©æ ·æœ¬è®­ç»ƒçš„æ¨¡å‹è¡¨ç°æ›´å¥½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨è‡ªæˆ‘è¿‡æ»¤æ ·æœ¬ï¼ˆä½¿ç”¨Gemma-3-12b-itè¿›è¡Œè¿‡æ»¤ï¼‰åœ¨åŒ»å­¦é¢†åŸŸå–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼Œä½†åœ¨ä¸åŒåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºè¾ƒä½çš„ç¨³å¥æ€§ï¼Œè€Œä½¿ç”¨åŒä¸€ç³»åˆ—ä¸­æ›´å¤§çš„æ¨¡å‹è¿›è¡Œè¿‡æ»¤åˆ™äº§ç”Ÿäº†æ›´å¥½çš„æ€»ä½“ç¨³å¥æ€§ã€‚è¿™äº›ç»“æœæä¾›äº†å…³äºRLVRåœ¨ç‰¹å®šé¢†åŸŸè¿›è¡Œæœ‰æ•ˆæ•°æ®ç»„ç»‡ç­–ç•¥çš„å®è´µè§è§£ï¼Œå¹¶å¼ºè°ƒäº†æ·±æ€ç†Ÿè™‘çš„æ•°æ®é€‰æ‹©åœ¨å®ç°æœ€ä½³æ€§èƒ½æ–¹é¢çš„é‡è¦æ€§ã€‚æ‚¨å¯ä»¥è®¿é—®æˆ‘ä»¬çš„ä»“åº“ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/Qsingle/open-medical-r1%EF%BC%89%E8%8E%B7%E5%8F%96%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/Qsingle/open-medical-r1ï¼‰è·å–ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13950v1">PDF</a> 15 figures</p>
<p><strong>Summary</strong><br>æ•°æ®é€‰æ‹©ç­–ç•¥å¯¹äºå¼ºåŒ–å­¦ä¹ åœ¨åŒ»ç–—é¢†åŸŸçš„è¡¨ç°è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å››ç§ä¸åŒçš„æ•°æ®é‡‡æ ·ç­–ç•¥ï¼Œå‘ç°ç»è¿‡è¿‡æ»¤çš„æ•°æ®è®­ç»ƒçš„æ¨¡å‹é€šå¸¸ä¼˜äºéšæœºé‡‡æ ·æ•°æ®è®­ç»ƒçš„æ¨¡å‹ã€‚ä½¿ç”¨Gemma-3-12b-itæ¨¡å‹è¿›è¡Œè¿‡æ»¤çš„è‡ªè¿‡æ»¤æ ·æœ¬åœ¨åŒ»ç–—é¢†åŸŸè¡¨ç°ä¼˜è¶Šï¼Œä½†è·¨ä¸åŒåŸºå‡†æµ‹è¯•çš„ç¨³å¥æ€§è¾ƒä½ã€‚æ¥è‡ªåŒä¸€ç³»åˆ—çš„å¤§å‹æ¨¡å‹çš„è¿‡æ»¤å¯å¸¦æ¥æ›´å¥½çš„æ•´ä½“ç¨³å¥æ€§ã€‚æ­¤ç ”ç©¶å¯¹å¼ºåŒ–å­¦ä¹ åœ¨ç‰¹å®šé¢†åŸŸçš„æ•°æ®ç»„ç»‡ç­–ç•¥æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨åŒ»ç–—é¢†åŸŸçš„æ•°æ®é€‰æ‹©ç­–ç•¥å¯¹æ¨¡å‹æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>ç ”ç©¶äº†å››ç§æ•°æ®é‡‡æ ·ç­–ç•¥ï¼ŒåŒ…æ‹¬éšæœºé‡‡æ ·å’Œä¸‰ç§è¿‡æ»¤æ–¹æ³•ã€‚</li>
<li>è¿‡æ»¤æ•°æ®è®­ç»ƒçš„æ¨¡å‹é€šå¸¸ä¼˜äºéšæœºé‡‡æ ·æ•°æ®è®­ç»ƒçš„æ¨¡å‹ã€‚</li>
<li>è‡ªè¿‡æ»¤æ ·æœ¬åœ¨åŒ»ç–—é¢†åŸŸè¡¨ç°ä¼˜è¶Šï¼Œä½†ç¨³å¥æ€§è¾ƒä½ã€‚</li>
<li>ä½¿ç”¨å¤§å‹æ¨¡å‹è¿›è¡Œè¿‡æ»¤å¯æé«˜æ¨¡å‹çš„æ€»ä½“ç¨³å¥æ€§ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯¹å¼ºåŒ–å­¦ä¹ åœ¨åŒ»ç–—é¢†åŸŸçš„æœ‰æ•ˆæ•°æ®ç»„ç»‡ç­–ç•¥æä¾›äº†å®è´µè§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13950">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a98189c3c0bd84550be9e223f36640e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80dc8053cdc95594eb0eb3b66ef98c8f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1003b972ea60957662f9002211c78a60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1fc1a178550ae880f406b256e0c881c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dabab6d9ae05891ae23584a57bdb5b4.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Evaluating-Menu-OCR-and-Translation-A-Benchmark-for-Aligning-Human-and-Automated-Evaluations-in-Large-Vision-Language-Models"><a href="#Evaluating-Menu-OCR-and-Translation-A-Benchmark-for-Aligning-Human-and-Automated-Evaluations-in-Large-Vision-Language-Models" class="headerlink" title="Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and   Automated Evaluations in Large Vision-Language Models"></a>Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and   Automated Evaluations in Large Vision-Language Models</h2><p><strong>Authors:Zhanglin Wu, Tengfei Song, Ning Xie, Weidong Zhang, Mengli Zhu, Shuang Wu, Shiliang Sun, Hao Yang</strong></p>
<p>The rapid advancement of large vision-language models (LVLMs) has significantly propelled applications in document understanding, particularly in optical character recognition (OCR) and multilingual translation. However, current evaluations of LVLMs, like the widely used OCRBench, mainly focus on verifying the correctness of their short-text responses and long-text responses with simple layout, while the evaluation of their ability to understand long texts with complex layout design is highly significant but largely overlooked. In this paper, we propose Menu OCR and Translation Benchmark (MOTBench), a specialized evaluation framework emphasizing the pivotal role of menu translation in cross-cultural communication. MOTBench requires LVLMs to accurately recognize and translate each dish, along with its price and unit items on a menu, providing a comprehensive assessment of their visual understanding and language processing capabilities. Our benchmark is comprised of a collection of Chinese and English menus, characterized by intricate layouts, a variety of fonts, and culturally specific elements across different languages, along with precise human annotations. Experiments show that our automatic evaluation results are highly consistent with professional human evaluation. We evaluate a range of publicly available state-of-the-art LVLMs, and through analyzing their output to identify the strengths and weaknesses in their performance, offering valuable insights to guide future advancements in LVLM development. MOTBench is available at <a target="_blank" rel="noopener" href="https://github.com/gitwzl/MOTBench">https://github.com/gitwzl/MOTBench</a>. </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„å¿«é€Ÿå‘å±•æå¤§åœ°æ¨åŠ¨äº†æ–‡æ¡£ç†è§£åº”ç”¨ç¨‹åºçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰å’Œå¤šè¯­ç§ç¿»è¯‘é¢†åŸŸã€‚ç„¶è€Œï¼Œå½“å‰å¯¹LVLMsçš„è¯„ä¼°ï¼Œå¦‚å¹¿æ³›ä½¿ç”¨çš„OCRBenchï¼Œä¸»è¦ä¾§é‡äºéªŒè¯å…¶ç®€çŸ­æ–‡æœ¬å›åº”å’Œç®€å•å¸ƒå±€é•¿æ–‡æœ¬å›åº”çš„æ­£ç¡®æ€§ï¼Œè€Œå¯¹äºä»–ä»¬ç†è§£å…·æœ‰å¤æ‚å¸ƒå±€è®¾è®¡çš„é•¿æ–‡æœ¬çš„è¯„ä¼°è‡³å…³é‡è¦ï¼Œå´è¢«å¿½è§†äº†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Menu OCR and Translation Benchmarkï¼ˆMOTBenchï¼‰è¿™ä¸€ä¸“é—¨çš„è¯„ä¼°æ¡†æ¶ï¼Œå¼ºè°ƒèœå•ç¿»è¯‘åœ¨è·¨æ–‡åŒ–äº¤æµä¸­çš„å…³é”®ä½œç”¨ã€‚MOTBenchè¦æ±‚LVLMså‡†ç¡®è¯†åˆ«å¹¶ç¿»è¯‘èœå•ä¸Šçš„æ¯ä¸€é“èœåŠå…¶ä»·æ ¼å’Œå•ä½é¡¹ï¼Œä»è€Œå…¨é¢è¯„ä¼°å…¶è§†è§‰ç†è§£å’Œè¯­è¨€å¤„ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•åŒ…å«ä¸­è‹±æ–‡èœå•çš„é›†åˆï¼Œè¿™äº›èœå•å…·æœ‰å¤æ‚çš„å¸ƒå±€ã€å¤šç§å­—ä½“ä»¥åŠä¸åŒè¯­è¨€çš„ç‰¹å®šæ–‡åŒ–å…ƒç´ ï¼ŒåŒæ—¶é…å¤‡ç²¾ç¡®çš„äººå·¥æ³¨é‡Šã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è‡ªåŠ¨è¯„ä¼°ç»“æœä¸ä¸“ä¸šçš„äººå·¥è¯„ä¼°ç»“æœé«˜åº¦ä¸€è‡´ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸€ç³»åˆ—å…¬å¼€çš„å…ˆè¿›LVLMsï¼Œé€šè¿‡åˆ†æä»–ä»¬çš„è¾“å‡ºæ¥ç¡®å®šå…¶æ€§èƒ½çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ï¼Œä¸ºæœªæ¥çš„LVLMå‘å±•æä¾›äº†å®è´µçš„è§è§£ã€‚MOTBenchå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/gitwzl/MOTBench%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/gitwzl/MOTBenchè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13945v2">PDF</a> 12 pages, 5 figures, 5 Tables</p>
<p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶MOTBenchï¼Œç”¨äºè¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨èœå•ç¿»è¯‘æ–¹é¢çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶å¼ºè°ƒèœå•ç¿»è¯‘åœ¨è·¨æ–‡åŒ–äº¤æµä¸­çš„é‡è¦æ€§ï¼Œè¦æ±‚LVLMså‡†ç¡®è¯†åˆ«å¹¶ç¿»è¯‘èœå•ä¸Šçš„èœå“ã€ä»·æ ¼åŠå•ä½ï¼Œå…¨é¢è¯„ä¼°å…¶è§†è§‰ç†è§£å’Œè¯­è¨€å¤„ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LVLMsåœ¨æ–‡æ¡£ç†è§£åº”ç”¨ï¼Œå¦‚å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰å’Œå¤šè¯­ç§ç¿»è¯‘æ–¹é¢å–å¾—å¿«é€Ÿè¿›å±•ã€‚</li>
<li>å½“å‰å¯¹LVLMsçš„è¯„ä¼°ä¸»è¦å…³æ³¨ç®€å•æ–‡æœ¬å¸ƒå±€ï¼Œä½†å¯¹å…¶å¤„ç†å¤æ‚æ–‡æœ¬å¸ƒå±€çš„èƒ½åŠ›è¯„ä¼°ä¸è¶³ã€‚</li>
<li>æ–‡æœ¬æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶MOTBenchï¼Œä¸“æ³¨äºèœå•ç¿»è¯‘ï¼Œå¼ºè°ƒè·¨æ–‡åŒ–äº¤æµä¸­çš„é‡è¦æ€§ã€‚</li>
<li>MOTBenchè¦æ±‚LVLMså‡†ç¡®è¯†åˆ«å¹¶ç¿»è¯‘èœå•ä¸Šçš„è¯¦ç»†ä¿¡æ¯ï¼Œå…¨é¢è¯„ä¼°å…¶è§†è§‰ç†è§£å’Œè¯­è¨€å¤„ç†èƒ½åŠ›ã€‚</li>
<li>MOTBenchåŒ…å«ä¸­è‹±æ–‡èœå•ï¼Œå…·æœ‰å¤æ‚å¸ƒå±€ã€å¤šç§å­—ä½“å’Œè·¨æ–‡åŒ–å…ƒç´ ï¼Œä¸”æœ‰äººç±»ç²¾ç¡®æ³¨é‡Šã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œè‡ªåŠ¨è¯„ä¼°ç»“æœä¸ä¸“ä¸šäººå·¥è¯„ä¼°é«˜åº¦ä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13945">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-19c29afb433d483f65e9e98de9d6666b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-829c697fef70f51df49f75b03a16332a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-feb1a14896ca65d0e3721a0fb76e2a25.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a038a47b8b58911626c3b2a8c080a5f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65ddcdb91972372f442096eb0830d400.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bb69a11481c6fb054eb3298ae2c4f0a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="SolarZip-An-Efficient-and-Adaptive-Compression-Framework-for-Solar-EUV-Imaging-Data"><a href="#SolarZip-An-Efficient-and-Adaptive-Compression-Framework-for-Solar-EUV-Imaging-Data" class="headerlink" title="SolarZip: An Efficient and Adaptive Compression Framework for Solar EUV   Imaging Data"></a>SolarZip: An Efficient and Adaptive Compression Framework for Solar EUV   Imaging Data</h2><p><strong>Authors:Zedong Liu, Song Tan, Alexander Warmuth, FrÃ©dÃ©ric Schuller, Yun Hong, Wenjing Huang, Yida Gu, Bojing Zhu, Guangming Tan, Dingwen Tao</strong></p>
<p>Context: With the advancement of solar physics research, next-generation solar space missions and ground-based telescopes face significant challenges in efficiently transmitting and&#x2F;or storing large-scale observational data. Aims: We develop an efficient compression and evaluation framework for solar EUV data, specifically optimized for Solar Orbiter Extreme Ultraviolet Imager (EUI) data, significantly reducing data volume while preserving scientific usability. Methods: We systematically evaluated four error-bounded lossy compressors across two EUI datasets. However, the existing methods cannot perfectly handle the EUI datasets (with continuously changing distance and significant resolution differences). Motivated by this, we develop an adaptive hybrid compression strategy with optimized interpolation predictors. Moreover, we designed a two-stage evaluation framework integrating distortion analysis with downstream scientific workflows, ensuring that observational analysis is not affected at high compression ratios. Results: Our framework SolarZip achieved up to 800x reduction for Full Sun Imager (FSI) data and 500x for High Resolution Imager (HRI$_{\text{EUV}}$) data. It significantly outperformed both traditional and advanced algorithms, achieving 3-50x higher compression ratios than traditional algorithms, surpassing the second-best algorithm by up to 30%. Simulation experiments verified that SolarZip can reduce data transmission time by up to 270x while ensuring the preservation of scientific usability. Conclusions: The SolarZip framework significantly enhances solar observational data compression efficiency while preserving scientific usability by dynamically selecting optimal compression methods based on observational scenarios and user requirements. This provides a promising data management solution for deep space missions like Solar Orbiter. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šéšç€å¤ªé˜³ç‰©ç†å­¦ç ”ç©¶çš„è¿›æ­¥ï¼Œä¸‹ä¸€ä»£å¤ªé˜³ç©ºé—´ä»»åŠ¡å’Œåœ°é¢æœ›è¿œé•œåœ¨æœ‰æ•ˆåœ°ä¼ è¾“å’Œ&#x2F;æˆ–å­˜å‚¨å¤§è§„æ¨¡è§‚æµ‹æ•°æ®æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç›®æ ‡ï¼šæˆ‘ä»¬ä¸ºå¤ªé˜³è§‚æµ‹ä»ªæç´«å¤–æˆåƒä»ªï¼ˆEUIï¼‰æ•°æ®å¼€å‘äº†ä¸€ä¸ªé«˜æ•ˆçš„å‹ç¼©å’Œè¯„ä¼°æ¡†æ¶ï¼Œèƒ½åœ¨ä¿æŒç§‘å­¦ä½¿ç”¨æ€§çš„åŒæ—¶æ˜¾è‘—å‡å°‘æ•°æ®é‡ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä»·äº†å››ä¸ªè¯¯å·®æœ‰ç•Œçš„æœ‰æŸå‹ç¼©æœºåœ¨ä¸¤å¥—EUIæ•°æ®é›†ä¸Šçš„åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä¸èƒ½å®Œå…¨å¤„ç†EUIæ•°æ®é›†ï¼ˆå…·æœ‰è¿ç»­å˜åŒ–çš„è·ç¦»å’Œæ˜¾è‘—çš„åˆ†è¾¨ç‡å·®å¼‚ï¼‰ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¸¦æœ‰ä¼˜åŒ–æ’å€¼é¢„æµ‹å™¨çš„è‡ªé€‚åº”æ··åˆå‹ç¼©ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„è¯„ä¼°æ¡†æ¶ï¼Œå°†å¤±çœŸåˆ†æä¸ä¸‹æ¸¸ç§‘å­¦å·¥ä½œæµç¨‹ç›¸ç»“åˆï¼Œç¡®ä¿åœ¨é«˜å‹ç¼©æ¯”ä¸‹è§‚æµ‹åˆ†æä¸å—å½±å“ã€‚ç»“æœï¼šæˆ‘ä»¬çš„SolarZipæ¡†æ¶å¯¹å…¨æ—¥æˆåƒä»ªï¼ˆFSIï¼‰æ•°æ®å®ç°äº†é«˜è¾¾800å€çš„å‹ç¼©æ¯”ï¼Œå¯¹é«˜åˆ†è¾¨ç‡æˆåƒä»ªï¼ˆHRI$_{EUV}$ï¼‰æ•°æ®å®ç°äº†é«˜è¾¾500å€çš„å‹ç¼©æ¯”ã€‚ä¸ä¼ ç»Ÿçš„å…ˆè¿›ç®—æ³•ç›¸æ¯”ï¼ŒSolarZipçš„è¡¨ç°æ›´åŠ å‡ºè‰²ï¼Œè¾¾åˆ°äº†ä¼ ç»Ÿç®—æ³•çš„3è‡³50å€é«˜å‹ç¼©æ¯”ï¼Œè¶…è¿‡äº†ç¬¬äºŒä¼˜ç§€çš„ç®—æ³•æœ€å¤šé«˜è¾¾30%ã€‚æ¨¡æ‹Ÿå®éªŒè¯å®ï¼ŒSolarZipèƒ½åœ¨ç¡®ä¿ç§‘å­¦ä½¿ç”¨æ€§çš„åŒæ—¶ï¼Œå‡å°‘æ•°æ®ä¼ è¾“æ—¶é—´é«˜è¾¾270å€ã€‚ç»“è®ºï¼šSolarZipæ¡†æ¶é€šè¿‡åŠ¨æ€é€‰æ‹©æœ€ä½³å‹ç¼©æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†å¤ªé˜³è§‚æµ‹æ•°æ®å‹ç¼©æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒç§‘å­¦ä½¿ç”¨æ€§ã€‚è¿™ä¸ºåƒå¤ªé˜³è§‚æµ‹å™¨è¿™æ ·çš„æ·±ç©ºä»»åŠ¡æä¾›äº†æœ‰å‰æ™¯çš„æ•°æ®ç®¡ç†è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13504v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>éšç€å¤ªé˜³èƒ½ç‰©ç†å­¦ç ”ç©¶çš„è¿›æ­¥ï¼Œä¸‹ä¸€ä»£å¤ªé˜³ç©ºé—´ä»»åŠ¡å’Œåœ°é¢æœ›è¿œé•œåœ¨æœ‰æ•ˆä¼ è¾“å’Œ&#x2F;æˆ–å­˜å‚¨å¤§è§„æ¨¡è§‚æµ‹æ•°æ®æ–¹é¢é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªé’ˆå¯¹å¤ªé˜³è§‚æµ‹å™¨æç´«å¤–æˆåƒä»ªï¼ˆEUIï¼‰æ•°æ®çš„å‹ç¼©å’Œè¯„ä¼°æ¡†æ¶ï¼Œå¯åœ¨ä¿æŒç§‘å­¦ä½¿ç”¨æ€§çš„åŒæ—¶å¤§å¹…å‡å°‘æ•°æ®é‡ã€‚é€šè¿‡ç³»ç»Ÿè¯„ä¼°å››ç§è¯¯å·®æœ‰ç•Œæœ‰æŸå‹ç¼©æœºï¼Œæˆ‘ä»¬å‘ç°ç°æœ‰æ–¹æ³•æ— æ³•å®Œç¾å¤„ç†EUIæ•°æ®é›†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§è‡ªé€‚åº”æ··åˆå‹ç¼©ç­–ç•¥ï¼Œé‡‡ç”¨ä¼˜åŒ–åçš„æ’å€¼é¢„æµ‹å™¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µè¯„ä¼°æ¡†æ¶ï¼Œç»“åˆå¤±çœŸåˆ†æä¸ä¸‹æ¸¸ç§‘å­¦å·¥ä½œæµç¨‹ï¼Œç¡®ä¿é«˜å‹ç¼©æ¯”ä¸‹è§‚æµ‹åˆ†æä¸å—å½±å“ã€‚æˆ‘ä»¬çš„æ¡†æ¶SolarZipå¯¹å…¨å¤ªé˜³æˆåƒä»ªï¼ˆFSIï¼‰æ•°æ®å®ç°äº†é«˜è¾¾800å€çš„å‹ç¼©ï¼Œå¯¹é«˜åˆ†è¾¨ç‡æˆåƒä»ªï¼ˆHRIï¼‰æ•°æ®å®ç°äº†é«˜è¾¾500å€çš„å‹ç¼©ã€‚ä¸ä¼ ç»Ÿå’Œå…ˆè¿›ç®—æ³•ç›¸æ¯”ï¼ŒSolarZipçš„å‹ç¼©æ¯”é«˜å‡º3-50å€ï¼Œæœ€é«˜å¯è¾¾ç¬¬äºŒå…ˆè¿›ç®—æ³•çš„3å€ã€‚æ¨¡æ‹Ÿå®éªŒè¡¨æ˜ï¼ŒSolarZipèƒ½å‡å°‘æ•°æ®ä¼ è¾“æ—¶é—´é«˜è¾¾27å€ï¼ŒåŒæ—¶ç¡®ä¿ç§‘å­¦ä½¿ç”¨æ€§çš„ä¿æŒã€‚ç»“è®ºï¼šSolarZipæ¡†æ¶é€šè¿‡åŠ¨æ€é€‰æ‹©æœ€ä½³å‹ç¼©æ–¹æ³•ï¼Œæ ¹æ®è§‚æµ‹åœºæ™¯å’Œç”¨æˆ·è¦æ±‚å¤§å¤§æé«˜å¤ªé˜³èƒ½è§‚æµ‹æ•°æ®å‹ç¼©æ•ˆç‡å¹¶ä¿æŒäº†ç§‘å­¦çš„å¯ç”¨æ€§ã€‚è¿™ä¸ºæ·±ç©ºä»»åŠ¡å¦‚å¤ªé˜³èƒ½è½¨é“ä»»åŠ¡æä¾›äº†æœ‰å‰é€”çš„æ•°æ®ç®¡ç†è§£å†³æ–¹æ¡ˆã€‚ </p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>SolarZipæ¡†æ¶é’ˆå¯¹å¤ªé˜³è§‚æµ‹å™¨æç´«å¤–æˆåƒä»ªï¼ˆEUIï¼‰æ•°æ®å¼€å‘ï¼Œæ—¨åœ¨æé«˜å‹ç¼©æ•ˆç‡å¹¶å‡å°‘æ•°æ®å­˜å‚¨éœ€æ±‚ã€‚</li>
<li>SolarZipæ¡†æ¶åœ¨ä¿æŒç§‘å­¦ä½¿ç”¨æ€§çš„åŒæ—¶å®ç°äº†æ˜¾è‘—çš„æ•°æ®å‹ç¼©æ•ˆæœï¼Œå¯¹å…¨å¤ªé˜³æˆåƒä»ªï¼ˆFSIï¼‰æ•°æ®å®ç°é«˜è¾¾800å€å‹ç¼©ï¼Œå¯¹é«˜åˆ†è¾¨ç‡æˆåƒä»ªï¼ˆHRIï¼‰æ•°æ®å®ç°é«˜è¾¾500å€å‹ç¼©ã€‚</li>
<li>SolarZipæ¡†æ¶æ˜¾è‘—ä¼˜äºä¼ ç»Ÿå’Œå…ˆè¿›ç®—æ³•ï¼Œæœ€é«˜å‹ç¼©æ¯”é«˜å‡ºç¬¬äºŒå…ˆè¿›ç®—æ³•è¾¾3å€ã€‚</li>
<li>SolarZipæ¡†æ¶é€šè¿‡å‡å°‘æ•°æ®ä¼ è¾“æ—¶é—´æ¥æé«˜æ•°æ®ç®¡ç†çš„æ•ˆç‡ï¼Œæ¨¡æ‹Ÿå®éªŒè¡¨æ˜å…¶èƒ½å‡å°‘æ•°æ®ä¼ è¾“æ—¶é—´é«˜è¾¾27å€ã€‚</li>
<li>SolarZipæ¡†æ¶é‡‡ç”¨è‡ªé€‚åº”æ··åˆå‹ç¼©ç­–ç•¥å’Œä¼˜åŒ–æ’å€¼é¢„æµ‹å™¨æ¥é€‚åº”ä¸åŒçš„è§‚æµ‹æ•°æ®å’Œç”¨æˆ·éœ€æ±‚ã€‚ </li>
<li>SolarZipæ¡†æ¶çš„åŠ¨æ€å‹ç¼©ç­–ç•¥å¯¹ä¸åŒçš„è§‚æµ‹åœºæ™¯å’Œç”¨æˆ·è¦æ±‚èƒ½å¤Ÿä¿æŒè¾ƒé«˜çš„é€‚ç”¨æ€§ï¼Œä¸ºåç»­çš„å¤ªé˜³ç‰©ç†ç ”ç©¶æä¾›äº†é‡è¦çš„æ•°æ®æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13504">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0f9f1a9e1e54bc06f5926f3cebb0fa22.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8edf7280ad144aced663ea72e86a8406.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94b3d16131fb4de6b6ee96725ee15e6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bcc25d336440cc72758051a95cfd885.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f389cf336e2b6b4d2ca743b938d7c94.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="SkyReels-V2-Infinite-length-Film-Generative-Model"><a href="#SkyReels-V2-Infinite-length-Film-Generative-Model" class="headerlink" title="SkyReels-V2: Infinite-length Film Generative Model"></a>SkyReels-V2: Infinite-length Film Generative Model</h2><p><strong>Authors:Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, Weiming Xiong, Wei Wang, Nuo Pang, Kang Kang, Zhiheng Xu, Yuzhe Jin, Yupeng Liang, Yubing Song, Peng Zhao, Boyuan Xu, Di Qiu, Debang Li, Zhengcong Fei, Yang Li, Yahui Zhou</strong></p>
<p>Recent advances in video generation have been driven by diffusion models and autoregressive frameworks, yet critical challenges persist in harmonizing prompt adherence, visual quality, motion dynamics, and duration: compromises in motion dynamics to enhance temporal visual quality, constrained video duration (5-10 seconds) to prioritize resolution, and inadequate shot-aware generation stemming from general-purpose MLLMsâ€™ inability to interpret cinematic grammar, such as shot composition, actor expressions, and camera motions. These intertwined limitations hinder realistic long-form synthesis and professional film-style generation. To address these limitations, we propose SkyReels-V2, an Infinite-length Film Generative Model, that synergizes Multi-modal Large Language Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and Diffusion Forcing Framework. Firstly, we design a comprehensive structural representation of video that combines the general descriptions by the Multi-modal LLM and the detailed shot language by sub-expert models. Aided with human annotation, we then train a unified Video Captioner, named SkyCaptioner-V1, to efficiently label the video data. Secondly, we establish progressive-resolution pretraining for the fundamental video generation, followed by a four-stage post-training enhancement: Initial concept-balanced Supervised Fine-Tuning (SFT) improves baseline quality; Motion-specific Reinforcement Learning (RL) training with human-annotated and synthetic distortion data addresses dynamic artifacts; Our diffusion forcing framework with non-decreasing noise schedules enables long-video synthesis in an efficient search space; Final high-quality SFT refines visual fidelity. All the code and models are available at <a target="_blank" rel="noopener" href="https://github.com/SkyworkAI/SkyReels-V2">https://github.com/SkyworkAI/SkyReels-V2</a>. </p>
<blockquote>
<p>è¿‘æœŸè§†é¢‘ç”ŸæˆæŠ€æœ¯çš„è¿›å±•ä¸»è¦å¾—ç›Šäºæ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¡†æ¶çš„æ¨åŠ¨ï¼Œç„¶è€Œï¼Œåœ¨æç¤ºéµå¾ªã€è§†è§‰è´¨é‡ã€è¿åŠ¨åŠ¨æ€å’ŒæŒç»­æ—¶é—´ç­‰æ–¹é¢ä»å­˜åœ¨å…³é”®æŒ‘æˆ˜ï¼šä¸ºæå‡æ—¶é—´è§†è§‰è´¨é‡è€Œå¦¥åè¿åŠ¨åŠ¨æ€ã€ä¸ºä¼˜å…ˆä¿è¯åˆ†è¾¨ç‡è€Œé™åˆ¶è§†é¢‘æ—¶é•¿ï¼ˆ5-10ç§’ï¼‰ã€ä»¥åŠç”±é€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ— æ³•è§£é‡Šç”µå½±è¯­æ³•ï¼ˆå¦‚é•œå¤´æ„å›¾ã€æ¼”å‘˜è¡¨è¾¾å’Œæ‘„å½±æœºè¿åŠ¨ï¼‰å¯¼è‡´çš„é•œå¤´æ„ŸçŸ¥ç”Ÿæˆä¸è¶³ã€‚è¿™äº›äº¤ç»‡çš„é™åˆ¶é˜»ç¢äº†çœŸå®çš„é•¿å½¢å¼åˆæˆå’Œä¸“ä¸šçš„ç”µå½±é£æ ¼ç”Ÿæˆã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™ï¼Œæˆ‘ä»¬æå‡ºäº†SkyReels-V2ï¼Œä¸€ç§æ— é™é•¿ç”µå½±ç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ååŒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€å¤šé˜¶æ®µé¢„è®­ç»ƒã€å¼ºåŒ–å­¦ä¹ å’Œæ‰©æ•£å¼ºåˆ¶æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å…¨é¢çš„è§†é¢‘ç»“æ„è¡¨ç¤ºï¼Œç»“åˆå¤šæ¨¡æ€LLMçš„ä¸€èˆ¬æè¿°å’Œå­ä¸“å®¶æ¨¡å‹çš„è¯¦ç»†é•œå¤´è¯­è¨€ã€‚å€ŸåŠ©äººå·¥æ ‡æ³¨ï¼Œæˆ‘ä»¬éšåè®­ç»ƒäº†ä¸€ä¸ªç»Ÿä¸€çš„è§†é¢‘å­—å¹•å™¨ï¼Œåä¸ºSkyCaptioner-V1ï¼Œä»¥æœ‰æ•ˆåœ°æ ‡æ³¨è§†é¢‘æ•°æ®ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä¸ºåŸºæœ¬çš„è§†é¢‘ç”Ÿæˆå»ºç«‹äº†æ¸è¿›å¼åˆ†è¾¨ç‡é¢„è®­ç»ƒï¼Œéšåæ˜¯å››ä¸ªé˜¶æ®µçš„åè®­ç»ƒå¢å¼ºï¼šåˆå§‹æ¦‚å¿µå¹³è¡¡çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æé«˜åŸºçº¿è´¨é‡ï¼›ä½¿ç”¨äººå·¥æ³¨é‡Šå’Œåˆæˆå¤±çœŸæ•°æ®çš„ç‰¹å®šè¿åŠ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒè§£å†³åŠ¨æ€ä¼ªå½±é—®é¢˜ï¼›æˆ‘ä»¬çš„æ‰©æ•£å¼ºåˆ¶æ¡†æ¶å’Œéé€’å‡å™ªå£°æ—¶é—´è¡¨ä½¿é•¿è§†é¢‘åˆæˆåœ¨æœ‰æ•ˆçš„æœç´¢ç©ºé—´ä¸­è¿›è¡Œï¼›æœ€åçš„é«˜è´¨é‡SFTç»†åŒ–è§†è§‰ä¿çœŸåº¦ã€‚æ‰€æœ‰ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SkyworkAI/SkyReels-V2%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SkyworkAI/SkyReels-V2æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13074v3">PDF</a> 31 pages,10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†é¢‘ç”Ÿæˆé¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚æç¤ºéµå¾ªã€è§†è§‰è´¨é‡ã€è¿åŠ¨åŠ¨åŠ›å­¦å’ŒæŒç»­æ—¶é—´çš„å¹³è¡¡é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†SkyReels-V2ï¼Œä¸€ä¸ªæ— é™é•¿åº¦çš„ç”µå½±ç”Ÿæˆæ¨¡å‹ï¼Œèåˆäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€å¤šé˜¶æ®µé¢„è®­ç»ƒã€å¼ºåŒ–å­¦ä¹ å’Œæ‰©æ•£å¼ºåˆ¶æ¡†æ¶ã€‚é€šè¿‡è®¾è®¡å…¨é¢çš„è§†é¢‘ç»“æ„è¡¨ç¤ºã€è®­ç»ƒç»Ÿä¸€è§†é¢‘æ ‡æ³¨å™¨ã€å»ºç«‹æ¸è¿›å¼åˆ†è¾¨ç‡é¢„è®­ç»ƒç­‰å…³é”®æ–¹æ³•ï¼Œå®ç°äº†é•¿è§†é¢‘åˆæˆå’Œç”µå½±é£æ ¼ç”Ÿæˆçš„ä¸“ä¸šæ€§æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è§†é¢‘ç”Ÿæˆé¢†åŸŸå­˜åœ¨æç¤ºéµå¾ªã€è§†è§‰è´¨é‡ã€è¿åŠ¨åŠ¨åŠ›å­¦å’ŒæŒç»­æ—¶é—´å¹³è¡¡çš„æŒ‘æˆ˜ã€‚</li>
<li>SkyReels-V2æ¨¡å‹èåˆäº†MLLMã€å¤šé˜¶æ®µé¢„è®­ç»ƒã€å¼ºåŒ–å­¦ä¹ å’Œæ‰©æ•£å¼ºåˆ¶æ¡†æ¶ã€‚</li>
<li>è®¾è®¡äº†å…¨é¢çš„è§†é¢‘ç»“æ„è¡¨ç¤ºï¼Œç»“åˆå¤šæ¨¡æ€LLMå’Œè¯¦ç»†é•œå¤´è¯­è¨€çš„å­ä¸“å®¶æ¨¡å‹ã€‚</li>
<li>è®­ç»ƒäº†ç»Ÿä¸€è§†é¢‘æ ‡æ³¨å™¨SkyCaptioner-V1ï¼Œä»¥é«˜æ•ˆæ ‡æ³¨è§†é¢‘æ•°æ®ã€‚</li>
<li>å»ºç«‹æ¸è¿›å¼åˆ†è¾¨ç‡é¢„è®­ç»ƒï¼Œé‡‡ç”¨å››é˜¶æ®µåè®­ç»ƒå¢å¼ºï¼ŒåŒ…æ‹¬åˆå§‹æ¦‚å¿µå¹³è¡¡çš„ç›‘ç£å¾®è°ƒã€è¿åŠ¨ç‰¹å®šçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€æ‰©æ•£å¼ºåˆ¶æ¡†æ¶å’Œéé€’å‡å™ªå£°è°ƒåº¦ä»¥åŠæœ€ç»ˆé«˜è´¨é‡ç›‘ç£å¾®è°ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7374afea9675835bab42517c0b12eff6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-135bbc853f17d529787e36191b59350b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9fbf10fae977ac87c8ae1741923de30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5302e464edde26d5c78c6efa43188ed7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-32f40e05213ea0ac9ecd71eeecc2b78b.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-24/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-24/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-24/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e4a38315911c4a91d92aa1ad4a03a35d.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-24  TTRL Test-Time Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-23/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-742838256fa83ed5eb961747dbd4757a.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-23  SOLIDO A Robust Watermarking Method for Speech Synthesis via Low-Rank   Adaptation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
