<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-24  TTRL Test-Time Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-e4a38315911c4a91d92aa1ad4a03a35d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-24-æ›´æ–°"><a href="#2025-04-24-æ›´æ–°" class="headerlink" title="2025-04-24 æ›´æ–°"></a>2025-04-24 æ›´æ–°</h1><h2 id="TTRL-Test-Time-Reinforcement-Learning"><a href="#TTRL-Test-Time-Reinforcement-Learning" class="headerlink" title="TTRL: Test-Time Reinforcement Learning"></a>TTRL: Test-Time Reinforcement Learning</h2><p><strong>Authors:Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, Bowen Zhou</strong></p>
<p>This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks, and highlight TTRLâ€™s potential for broader tasks and domains. GitHub: <a target="_blank" rel="noopener" href="https://github.com/PRIME-RL/TTRL">https://github.com/PRIME-RL/TTRL</a> </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†ä»»åŠ¡ä¸­ï¼Œåœ¨æ— æ˜ç¡®æ ‡ç­¾æ•°æ®ä¸Šåº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æƒ…å†µã€‚è¯¥é—®é¢˜çš„æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºåœ¨æ— æ³•è·å–çœŸå®ä¿¡æ¯çš„æƒ…å†µä¸‹è¿›è¡Œæ¨ç†æ—¶çš„å¥–åŠ±ä¼°è®¡ã€‚å°½ç®¡è¿™ä¸ªè®¾å®šä¼¼ä¹ä»¤äººå›°æƒ‘ï¼Œä½†æˆ‘ä»¬å‘ç°æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆTTSï¼‰ä¸­çš„å¸¸ç”¨æ–¹æ³•ï¼Œä¾‹å¦‚å¤šæ•°æŠ•ç¥¨ï¼Œå¯ä»¥äº§ç”Ÿå‡ºäººæ„æ–™çš„ã€é€‚åˆé©±åŠ¨RLè®­ç»ƒçš„æœ‰æ•ˆå¥–åŠ±ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æµ‹è¯•æ—¶é—´å¼ºåŒ–å­¦ä¹ ï¼ˆTTRLï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä½¿ç”¨æ— æ ‡ç­¾æ•°æ®å¯¹LLMè¿›è¡ŒRLè®­ç»ƒçš„æ–°æ–¹æ³•ã€‚TTRLé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„å…ˆéªŒçŸ¥è¯†ï¼Œå®ç°äº†LLMçš„è‡ªæˆ‘è¿›åŒ–ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒTTRLåœ¨å„ç§ä»»åŠ¡å’Œæ¨¡å‹ä¸Šçš„è¡¨ç°æŒç»­æé«˜ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨AIME 2024æ¯”èµ›ä¸­ï¼ŒTTRLå°†Qwen-2.5-Math-7Bçš„pass@1æ€§èƒ½æé«˜äº†çº¦159%ï¼Œè€Œè¿™ä¸€åˆ‡ä»…ä½¿ç”¨äº†æ— æ ‡ç­¾çš„æµ‹è¯•æ•°æ®ã€‚æ­¤å¤–ï¼Œå°½ç®¡TTRLåªå—åˆ°Maj@NæŒ‡æ ‡çš„ç›‘ç£ï¼Œä½†å…¶è¡¨ç°ä¸€ç›´è¶…è¿‡åˆå§‹æ¨¡å‹çš„ä¸Šé™ï¼Œå¹¶æ¥è¿‘ç›´æ¥åœ¨æµ‹è¯•æ•°æ®ä¸Šä½¿ç”¨çœŸå®æ ‡ç­¾è®­ç»ƒçš„æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜TTRLåœ¨å¤šç§ä»»åŠ¡ä¸­çš„æ™®éæœ‰æ•ˆæ€§ï¼Œå¹¶çªå‡ºäº†å…¶åœ¨æ›´å¹¿æ³›çš„ä»»åŠ¡å’Œé¢†åŸŸä¸­çš„æ½œåŠ›ã€‚GitHubï¼š<a target="_blank" rel="noopener" href="https://github.com/PRIME-RL/TTRL">https://github.com/PRIME-RL/TTRL</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16084v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨æ— æ ‡ç­¾æ•°æ®çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„è‡ªæˆ‘è¿›åŒ–ç ”ç©¶ï¼Œåˆ©ç”¨å…ˆéªŒä¿¡æ¯æ”¹å–„æ€§èƒ½ï¼Œå°¤å…¶å¯¹äºæµ‹è¯•æ—¶è‡ªé€‚åº”è¿›åŒ–è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œå¹¶å¯èƒ½åœ¨ä¸åŒä»»åŠ¡é¢†åŸŸä¸­å‘æŒ¥æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¢«ç”¨äºåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¤„ç†æ— æ˜ç¡®æ ‡ç­¾çš„æ•°æ®ä»¥æ‰§è¡Œæ¨ç†ä»»åŠ¡ã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºåœ¨æ²¡æœ‰çœŸå®ä¿¡æ¯çš„æƒ…å†µä¸‹è¿›è¡Œå¥–åŠ±ä¼°è®¡ã€‚</li>
<li>æµ‹è¯•æ—¶é—´å°ºåº¦ï¼ˆTTSï¼‰çš„å¸¸è§å®è·µï¼Œå¦‚å¤šæ•°æŠ•ç¥¨ï¼Œå¯ä»¥ç”Ÿæˆç”¨äºé©±åŠ¨RLè®­ç»ƒçš„æœ‰æ•ˆå¥–åŠ±ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼šæµ‹è¯•æ—¶é—´å¼ºåŒ–å­¦ä¹ ï¼ˆTTRLï¼‰ï¼Œç”¨äºåœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸­ä½¿ç”¨RLå¯¹LLMè¿›è¡Œè®­ç»ƒã€‚æ­¤æ–¹æ³•ä½¿å¾—LLMèƒ½å¤Ÿè¿›è¡Œè‡ªæˆ‘è¿›åŒ–ã€‚</li>
<li>TTRLåœ¨å„ç§ä»»åŠ¡ä¸Šçš„è¡¨ç°æŒç»­æé«˜ï¼Œå¹¶æ˜¾è‘—æå‡äº†ç‰¹å®šæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>TTRLçš„æ€§èƒ½è¶…è¶Šäº†åˆå§‹æ¨¡å‹çš„æ€§èƒ½ä¸Šé™ï¼Œå¹¶æ¥è¿‘åœ¨æµ‹è¯•æ•°æ®ä¸Šç›´æ¥è®­ç»ƒçš„æ¨¡å‹çš„æ€§èƒ½ï¼Œå³ä½¿è¿™äº›æ¨¡å‹ä½¿ç”¨äº†çœŸå®æ ‡ç­¾ä½œä¸ºç›‘ç£ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†TTRLçš„ä¸€èˆ¬æœ‰æ•ˆæ€§ï¼Œå¹¶è¡¨æ˜äº†å…¶åœ¨ä¸åŒä»»åŠ¡é¢†åŸŸçš„æ½œåœ¨åº”ç”¨å‰æ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16084">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-713d31245df3d20743ffdff1446f4e5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4102386456ea92688350b766d5f061d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e35354102e822a30d5786a5d54dc315e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="From-Reflection-to-Perfection-Scaling-Inference-Time-Optimization-for-Text-to-Image-Diffusion-Models-via-Reflection-Tuning"><a href="#From-Reflection-to-Perfection-Scaling-Inference-Time-Optimization-for-Text-to-Image-Diffusion-Models-via-Reflection-Tuning" class="headerlink" title="From Reflection to Perfection: Scaling Inference-Time Optimization for   Text-to-Image Diffusion Models via Reflection Tuning"></a>From Reflection to Perfection: Scaling Inference-Time Optimization for   Text-to-Image Diffusion Models via Reflection Tuning</h2><p><strong>Authors:Le Zhuo, Liangbing Zhao, Sayak Paul, Yue Liao, Renrui Zhang, Yi Xin, Peng Gao, Mohamed Elhoseiny, Hongsheng Li</strong></p>
<p>Recent text-to-image diffusion models achieve impressive visual quality through extensive scaling of training data and model parameters, yet they often struggle with complex scenes and fine-grained details. Inspired by the self-reflection capabilities emergent in large language models, we propose ReflectionFlow, an inference-time framework enabling diffusion models to iteratively reflect upon and refine their outputs. ReflectionFlow introduces three complementary inference-time scaling axes: (1) noise-level scaling to optimize latent initialization; (2) prompt-level scaling for precise semantic guidance; and most notably, (3) reflection-level scaling, which explicitly provides actionable reflections to iteratively assess and correct previous generations. To facilitate reflection-level scaling, we construct GenRef, a large-scale dataset comprising 1 million triplets, each containing a reflection, a flawed image, and an enhanced image. Leveraging this dataset, we efficiently perform reflection tuning on state-of-the-art diffusion transformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified framework. Experimental results show that ReflectionFlow significantly outperforms naive noise-level scaling methods, offering a scalable and compute-efficient solution toward higher-quality image synthesis on challenging tasks. </p>
<blockquote>
<p>æœ€è¿‘å‡ºç°çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹é€šè¿‡å¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®å’Œæ¨¡å‹å‚æ•°æ‰©å±•ï¼Œå®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„è§†è§‰è´¨é‡ï¼Œä½†å®ƒä»¬é€šå¸¸å¯¹äºå¤æ‚åœºæ™¯å’Œç²¾ç»†ç»†èŠ‚çš„å¤„ç†èƒ½åŠ›æœ‰é™ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è‡ªæˆ‘åæ€èƒ½åŠ›å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ReflectionFlowï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨ç†æ—¶é—´æ¡†æ¶ï¼Œèƒ½å¤Ÿä½¿æ‰©æ•£æ¨¡å‹å¯¹å…¶è¾“å‡ºè¿›è¡Œè¿­ä»£åæ€å’Œæ”¹è¿›ã€‚ReflectionFlowå¼•å…¥äº†ä¸‰ä¸ªäº’è¡¥çš„æ¨ç†æ—¶é—´å°ºåº¦è½´ï¼šï¼ˆ1ï¼‰å™ªå£°çº§åˆ«ç¼©æ”¾ä»¥ä¼˜åŒ–æ½œåœ¨åˆå§‹åŒ–ï¼›ï¼ˆ2ï¼‰æç¤ºçº§åˆ«ç¼©æ”¾ä»¥å®ç°ç²¾ç¡®è¯­ä¹‰æŒ‡å¯¼ï¼›æœ€é‡è¦çš„æ˜¯ï¼ˆ3ï¼‰åæ€çº§åˆ«ç¼©æ”¾ï¼Œå®ƒæ˜ç¡®æä¾›äº†å¯æ“ä½œçš„åæ€æ¥è¿­ä»£è¯„ä¼°å’Œçº æ­£ä¹‹å‰çš„ç”Ÿæˆã€‚ä¸ºäº†ä¿ƒè¿›åæ€çº§åˆ«ç¼©æ”¾ï¼Œæˆ‘ä»¬æ„å»ºäº†GenRefæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«ç™¾ä¸‡ä¸ªä¸‰å…ƒç»„çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œæ¯ä¸ªä¸‰å…ƒç»„éƒ½åŒ…å«ä¸€æ¬¡åæ€ã€ä¸€ä¸ªç¼ºé™·å›¾åƒå’Œä¸€ä¸ªå¢å¼ºå›¾åƒã€‚åˆ©ç”¨æ­¤æ•°æ®é›†ï¼Œæˆ‘ä»¬åœ¨ç»Ÿä¸€æ¡†æ¶å†…å¯¹æœ€å…ˆè¿›çš„æ‰©æ•£å˜å‹å™¨FLUX.1-devè¿›è¡Œå¤šæ¨¡å¼è¾“å…¥çš„è”åˆå»ºæ¨¡ï¼Œæœ‰æ•ˆåœ°æ‰§è¡Œäº†åå°„è°ƒä¼˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReflectionFlowæ˜¾è‘—ä¼˜äºç®€å•çš„å™ªå£°çº§åˆ«ç¼©æ”¾æ–¹æ³•ï¼Œä¸ºæŒ‘æˆ˜æ€§ä»»åŠ¡çš„é«˜è´¨é‡å›¾åƒåˆæˆæä¾›äº†å¯æ‰©å±•å’Œè®¡ç®—é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16080v1">PDF</a> All code, checkpoints, and datasets are available at   \url{<a target="_blank" rel="noopener" href="https://diffusion-cot.github.io/reflection2perfection%7D">https://diffusion-cot.github.io/reflection2perfection}</a></p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹å·²å¹¿æ³›è¿ç”¨äºæ–‡æœ¬åˆ°å›¾åƒçš„è½¬æ¢ä¸­ï¼Œå°½ç®¡å…¶åœ¨è®­ç»ƒæ•°æ®å’Œæ¨¡å‹å‚æ•°ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„è§†è§‰æ•ˆæœï¼Œä½†å¯¹äºå¤æ‚åœºæ™¯å’Œç²¾ç»†ç»†èŠ‚ä»æœ‰å±€é™ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ReflectionFlowæ¡†æ¶ï¼Œæ—¨åœ¨èµ‹äºˆæ‰©æ•£æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„è‡ªæˆ‘åæ€èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥ä¸‰æ¡äº’è¡¥çš„æ¨ç†å°ºåº¦ï¼Œå¦‚å™ªå£°æ°´å¹³ã€æç¤ºæ°´å¹³å’Œåæ€æ°´å¹³ç¼©æ”¾ï¼Œå¹¶ç»“åˆå¤§å‹æ•°æ®é›†GenRefè¿›è¡Œè®­ç»ƒå’Œä¼˜åŒ–ï¼ŒReflectionFlowæ˜¾è‘—æé«˜äº†å›¾åƒåˆæˆçš„è´¨é‡ã€‚è¯¥æ¡†æ¶åœ¨æŒ‘æˆ˜ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè¶…è¶Šä¼ ç»Ÿå™ªå£°æ°´å¹³ç¼©æ”¾æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œä¸ºé«˜è´¨é‡å›¾åƒåˆæˆæä¾›äº†å¯æ‰©å±•ä¸”è®¡ç®—é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å¹¿æ³›åº”ç”¨äºæ–‡æœ¬åˆ°å›¾åƒçš„è½¬æ¢ä¸­ï¼Œä½†å¤„ç†å¤æ‚åœºæ™¯å’Œç²¾ç»†ç»†èŠ‚æ—¶å­˜åœ¨å±€é™ã€‚</li>
<li>ReflectionFlowæ¡†æ¶æ—¨åœ¨èµ‹äºˆæ‰©æ•£æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­çš„è‡ªæˆ‘åæ€èƒ½åŠ›ã€‚</li>
<li>ReflectionFlowå¼•å…¥ä¸‰æ¡äº’è¡¥çš„æ¨ç†å°ºåº¦ï¼šå™ªå£°æ°´å¹³ç¼©æ”¾ã€æç¤ºæ°´å¹³ç¼©æ”¾å’Œåæ€æ°´å¹³ç¼©æ”¾ã€‚</li>
<li>GenRefæ•°æ®é›†ç”¨äºä¿ƒè¿›åæ€æ°´å¹³çš„è®­ç»ƒå’Œä¼˜åŒ–ã€‚</li>
<li>ReflectionFlowç»“åˆå¤§å‹æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜å›¾åƒåˆæˆçš„è´¨é‡ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨æŒ‘æˆ˜ä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¶Šä¼ ç»Ÿå™ªå£°æ°´å¹³ç¼©æ”¾æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-dfd6c6395e34273a2ed17fc72955362c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-531502b431a822dc1b7da3db6eb89543.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-552efa358ba9c027393f3b69a6329e6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdd67822a9fd95116aa5102a85ca682d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PHYBench-Holistic-Evaluation-of-Physical-Perception-and-Reasoning-in-Large-Language-Models"><a href="#PHYBench-Holistic-Evaluation-of-Physical-Perception-and-Reasoning-in-Large-Language-Models" class="headerlink" title="PHYBench: Holistic Evaluation of Physical Perception and Reasoning in   Large Language Models"></a>PHYBench: Holistic Evaluation of Physical Perception and Reasoning in   Large Language Models</h2><p><strong>Authors:Shi Qiu, Shaoyang Guo, Zhuo-Yang Song, Yunbo Sun, Zeyu Cai, Jiashen Wei, Tianyu Luo, Yixuan Yin, Haoxu Zhang, Yi Hu, Chenyang Wang, Chencheng Tang, Haoling Chang, Qi Liu, Ziheng Zhou, Tianyu Zhang, Jingtian Zhang, Zhangyi Liu, Minghao Li, Yuku Zhang, Boxuan Jing, Xianqi Yin, Yutong Ren, Zizhuo Fu, Weike Wang, Xudong Tian, Anqi Lv, Laifu Man, Jianxiang Li, Feiyu Tao, Qihua Sun, Zhou Liang, Yushu Mu, Zhongxuan Li, Jing-Jun Zhang, Shutao Zhang, Xiaotian Li, Xingqi Xia, Jiawei Lin, Zheyu Shen, Jiahang Chen, Qiuhao Xiong, Binran Wang, Fengyuan Wang, Ziyang Ni, Bohan Zhang, Fan Cui, Changkun Shao, Qing-Hong Cao, Ming-xing Luo, Muhan Zhang, Hua Xing Zhu</strong></p>
<p>We introduce PHYBench, a novel, high-quality benchmark designed for evaluating reasoning capabilities of large language models (LLMs) in physical contexts. PHYBench consists of 500 meticulously curated physics problems based on real-world physical scenarios, designed to assess the ability of models to understand and reason about realistic physical processes. Covering mechanics, electromagnetism, thermodynamics, optics, modern physics, and advanced physics, the benchmark spans difficulty levels from high school exercises to undergraduate problems and Physics Olympiad challenges. Additionally, we propose the Expression Edit Distance (EED) Score, a novel evaluation metric based on the edit distance between mathematical expressions, which effectively captures differences in model reasoning processes and results beyond traditional binary scoring methods. We evaluate various LLMs on PHYBench and compare their performance with human experts. Our results reveal that even state-of-the-art reasoning models significantly lag behind human experts, highlighting their limitations and the need for improvement in complex physical reasoning scenarios. Our benchmark results and dataset are publicly available at <a target="_blank" rel="noopener" href="https://phybench-official.github.io/phybench-demo/">https://phybench-official.github.io/phybench-demo/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†PHYBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç‰©ç†ä¸Šä¸‹æ–‡ä¸­çš„æ¨ç†èƒ½åŠ›è€Œè®¾è®¡çš„æ–°å‹é«˜è´¨é‡åŸºå‡†æµ‹è¯•ã€‚PHYBenchç”±500ä¸ªç²¾å¿ƒç­–åˆ’çš„ç‰©ç†é—®é¢˜ç»„æˆï¼Œè¿™äº›é—®é¢˜åŸºäºçœŸå®ä¸–ç•Œçš„ç‰©ç†åœºæ™¯è®¾è®¡ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹ç†è§£å’Œæ¨ç†ç°å®ç‰©ç†è¿‡ç¨‹çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬åŠ›å­¦ã€ç”µç£å­¦ã€çƒ­åŠ›å­¦ã€å…‰å­¦ã€ç°ä»£ç‰©ç†å­¦å’Œé«˜çº§ç‰©ç†å­¦ï¼Œéš¾åº¦å±‚æ¬¡ä»é«˜ä¸­ç»ƒä¹ åˆ°å¤§å­¦é—®é¢˜å’Œç‰©ç†å¥¥æ—åŒ¹å…‹æŒ‘æˆ˜éƒ½æœ‰æ‰€æ¶µç›–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†è¡¨è¾¾å¼ç¼–è¾‘è·ç¦»ï¼ˆEEDï¼‰åˆ†æ•°ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ•°å­¦è¡¨è¾¾å¼ä¹‹é—´ç¼–è¾‘è·ç¦»çš„æ–°å‹è¯„ä¼°æŒ‡æ ‡ï¼Œå®ƒèƒ½æœ‰æ•ˆåœ°æ•æ‰æ¨¡å‹æ¨ç†è¿‡ç¨‹å’Œç»“æœä¹‹é—´çš„å·®å¼‚ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„äºŒå…ƒè¯„åˆ†æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨PHYBenchä¸Šè¯„ä¼°äº†å„ç§LLMï¼Œå¹¶å°†ä»–ä»¬çš„æ€§èƒ½ä¸äººç±»ä¸“å®¶è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹ä¹Ÿä¸äººç±»ä¸“å®¶å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œè¿™å‡¸æ˜¾äº†å®ƒä»¬åœ¨å¤æ‚ç‰©ç†æ¨ç†åœºæ™¯ä¸­çš„å±€é™æ€§ä»¥åŠæ”¹è¿›çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ç»“æœå’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://phybench-official.github.io/phybench-demo/%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://phybench-official.github.io/phybench-demo/å…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16074v1">PDF</a> 21 pages ,8 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>PHYBenchæ˜¯ä¸€ä¸ªä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç‰©ç†æƒ…å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›è€Œè®¾è®¡çš„é«˜è´¨é‡åŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…å«500ä¸ªç²¾å¿ƒæŒ‘é€‰çš„ç‰©ç†é—®é¢˜ï¼Œæ¶µç›–ä»é«˜ä¸­åˆ°å¤§å­¦æ°´å¹³çš„ç‰©ç†å†…å®¹ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹å¯¹çœŸå®ç‰©ç†è¿‡ç¨‹çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†åŸºäºæ•°å­¦è¡¨è¾¾å¼ç¼–è¾‘è·ç¦»çš„æ–°å‹è¯„ä¼°æŒ‡æ ‡â€”â€”Expression Edit Distance (EED) Scoreï¼Œä»¥æ›´æœ‰æ•ˆåœ°æ•æ‰æ¨¡å‹æ¨ç†è¿‡ç¨‹å’Œç»“æœçš„å·®å¼‚ã€‚è¯„ä¼°ç»“æœæ­ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹ï¼Œåœ¨å¤æ‚ç‰©ç†æ¨ç†åœºæ™¯ä¸­ä»æ˜¾è‘—è½åäºäººç±»ä¸“å®¶ï¼Œè¿™å¼ºè°ƒäº†æ¨¡å‹çš„å±€é™æ€§åŠæ”¹è¿›çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PHYBenchæ˜¯ä¸€ä¸ªä¸ºè¯„ä¼°LLMåœ¨ç‰©ç†æƒ…å¢ƒä¸­çš„æ¨ç†èƒ½åŠ›è€Œè®¾è®¡çš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚</li>
<li>å®ƒåŒ…å«500ä¸ªè¦†ç›–å¤šä¸ªç‰©ç†é¢†åŸŸçš„ç²¾å¿ƒæŒ‘é€‰çš„é—®é¢˜ã€‚</li>
<li>PHYBenchæ—¨åœ¨è¯„ä¼°æ¨¡å‹å¯¹ä»é«˜ä¸­åˆ°å¤§å­¦æ°´å¹³çš„çœŸå®ç‰©ç†è¿‡ç¨‹çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†åŸºäºæ•°å­¦è¡¨è¾¾å¼ç¼–è¾‘è·ç¦»çš„EED Scoreè¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>EED Scoreèƒ½æœ‰æ•ˆæ•æ‰æ¨¡å‹æ¨ç†è¿‡ç¨‹å’Œç»“æœçš„å·®å¼‚ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯å…ˆè¿›çš„LLMåœ¨å¤æ‚ç‰©ç†æ¨ç†åœºæ™¯ä¸­ä»æ˜¾è‘—è½åäºäººç±»ä¸“å®¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc1014d144ec4cac59bcf7a0e4406715.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-834480df6042d19a39c7bec930fdb536.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0d86daddc550a226bb7fee443c9939c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-369136749d935196905d3daaad9073bf.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Honey-I-Shrunk-the-Language-Model-Impact-of-Knowledge-Distillation-Methods-on-Performance-and-Explainability"><a href="#Honey-I-Shrunk-the-Language-Model-Impact-of-Knowledge-Distillation-Methods-on-Performance-and-Explainability" class="headerlink" title="Honey, I Shrunk the Language Model: Impact of Knowledge Distillation   Methods on Performance and Explainability"></a>Honey, I Shrunk the Language Model: Impact of Knowledge Distillation   Methods on Performance and Explainability</h2><p><strong>Authors:Daniel Hendriks, Philipp Spitzer, Niklas KÃ¼hl, Gerhard Satzger</strong></p>
<p>Artificial Intelligence (AI) has increasingly influenced modern society, recently in particular through significant advancements in Large Language Models (LLMs). However, high computational and storage demands of LLMs still limit their deployment in resource-constrained environments. Knowledge distillation addresses this challenge by training a small student model from a larger teacher model. Previous research has introduced several distillation methods for both generating training data and for training the student model. Despite their relevance, the effects of state-of-the-art distillation methods on model performance and explainability have not been thoroughly investigated and compared. In this work, we enlarge the set of available methods by applying critique-revision prompting to distillation for data generation and by synthesizing existing methods for training. For these methods, we provide a systematic comparison based on the widely used Commonsense Question-Answering (CQA) dataset. While we measure performance via student model accuracy, we employ a human-grounded study to evaluate explainability. We contribute new distillation methods and their comparison in terms of both performance and explainability. This should further advance the distillation of small language models and, thus, contribute to broader applicability and faster diffusion of LLM technology. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å¯¹ç°ä»£ç¤¾ä¼šçš„å½±å“è¶Šæ¥è¶Šå¤§ï¼Œæœ€è¿‘ç‰¹åˆ«æ˜¯é€šè¿‡è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é‡å¤§è¿›æ­¥ã€‚ç„¶è€Œï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„é«˜è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚ä»ç„¶é™åˆ¶äº†å®ƒä»¬åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„éƒ¨ç½²ã€‚çŸ¥è¯†è’¸é¦é€šè¿‡ä»ä¸€ä¸ªè¾ƒå¤§çš„æ•™å¸ˆæ¨¡å‹è®­ç»ƒä¸€ä¸ªå°çš„å­¦ç”Ÿæ¨¡å‹æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚ä¹‹å‰çš„ç ”ç©¶å·²ç»ä»‹ç»äº†ç”Ÿæˆè®­ç»ƒæ•°æ®å’Œè®­ç»ƒå­¦ç”Ÿæ¨¡å‹çš„å‡ ç§è’¸é¦æ–¹æ³•ã€‚å°½ç®¡å®ƒä»¬å¾ˆé‡è¦ï¼Œä½†æœ€å…ˆè¿›çš„è’¸é¦æ–¹æ³•å¯¹æ¨¡å‹æ€§èƒ½å’Œå¯è§£é‡Šæ€§çš„å½±å“å°šæœªå¾—åˆ°å…¨é¢å’Œå½»åº•çš„è°ƒæŸ¥ä¸æ¯”è¾ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†æ‰¹åˆ¤ä¿®è®¢æç¤ºåº”ç”¨äºæ•°æ®ç”Ÿæˆä¸­çš„è’¸é¦ï¼Œå¹¶é€šè¿‡åˆæˆç°æœ‰æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œæ‰©å¤§äº†å¯ç”¨æ–¹æ³•çš„èŒƒå›´ã€‚å¯¹äºè¿™äº›æ–¹æ³•ï¼Œæˆ‘ä»¬åŸºäºå¹¿æ³›ä½¿ç”¨çš„å¸¸è¯†é—®ç­”ï¼ˆCQAï¼‰æ•°æ®é›†è¿›è¡Œäº†ç³»ç»Ÿçš„æ¯”è¾ƒã€‚è™½ç„¶æˆ‘ä»¬é€šè¿‡å­¦ç”Ÿæ¨¡å‹çš„å‡†ç¡®æ€§æ¥è¡¡é‡æ€§èƒ½ï¼Œä½†æˆ‘ä»¬é‡‡ç”¨äº†åŸºäºäººç±»çš„ç ”ç©¶æ¥è¯„ä¼°å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬ä¸ºæ–°çš„è’¸é¦æ–¹æ³•åŠå…¶æ€§èƒ½å’Œå¯è§£é‡Šæ€§çš„æ¯”è¾ƒåšå‡ºäº†è´¡çŒ®ã€‚è¿™å°†è¿›ä¸€æ­¥æ¨åŠ¨å°å‹è¯­è¨€æ¨¡å‹çš„è’¸é¦æŠ€æœ¯çš„å‘å±•ï¼Œä»è€Œä¸ºLLMæŠ€æœ¯çš„æ›´å¹¿æ³›åº”ç”¨å’Œæ›´å¿«ä¼ æ’­åšå‡ºè´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16056v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å¯¹ç°ä»£ç¤¾ä¼šçš„å½±å“æ—¥ç›Šæ˜¾è‘—ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼ŒLLMçš„é«˜è®¡ç®—éœ€æ±‚å’Œå­˜å‚¨éœ€æ±‚ä»ç„¶é™åˆ¶äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„éƒ¨ç½²ã€‚çŸ¥è¯†è’¸é¦æŠ€æœ¯é€šè¿‡è®­ç»ƒå°å‹çš„å­¦ç”Ÿæ¨¡å‹æ¥æ¨¡ä»¿å¤§å‹æ•™å¸ˆæ¨¡å‹çš„æ€§èƒ½ï¼Œè§£å†³äº†è¿™ä¸€æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†è¯„è®ºä¿®è®¢æç¤ºç­‰æ–¹æ³•è¿›è¡Œæ•°æ®ç”Ÿæˆå’Œè®­ç»ƒå­¦ç”Ÿæ¨¡å‹çš„è’¸é¦è¿‡ç¨‹ã€‚æˆ‘ä»¬åœ¨å¹¿æ³›ä½¿ç”¨çš„å¸¸è¯†é—®ç­”ï¼ˆCQAï¼‰æ•°æ®é›†ä¸Šå¯¹è¿™äº›æ–¹æ³•è¿›è¡Œäº†ç³»ç»Ÿæ¯”è¾ƒã€‚é€šè¿‡å­¦ç”Ÿæ¨¡å‹çš„å‡†ç¡®æ€§è¡¡é‡æ€§èƒ½ï¼Œå¹¶é€šè¿‡åŸºäºäººç±»çš„ç ”ç©¶æ¥è¯„ä¼°å¯è§£é‡Šæ€§ã€‚æœ¬ç ”ç©¶ä¸ºå°å‹è¯­è¨€æ¨¡å‹æä¾›äº†æ–°çš„è’¸é¦æ–¹æ³•å’Œæ¯”è¾ƒç»“æœï¼Œè¿›ä¸€æ­¥æ¨åŠ¨äº†å…¶åœ¨æ€§èƒ½å’Œå¯è§£é‡Šæ€§æ–¹é¢çš„åº”ç”¨ï¼Œä¸ºLLMæŠ€æœ¯çš„å¹¿æ³›åº”ç”¨å’Œå¿«é€Ÿæ‰©æ•£åšå‡ºè´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹ç°ä»£ç¤¾ä¼šäº§ç”Ÿäº†æ·±è¿œå½±å“ï¼Œä½†èµ„æºå—é™ç¯å¢ƒçš„éƒ¨ç½²ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>çŸ¥è¯†è’¸é¦æŠ€æœ¯æ˜¯ä¸€ç§è§£å†³ç­–ç•¥ï¼Œé€šè¿‡è®­ç»ƒå°å‹å­¦ç”Ÿæ¨¡å‹æ¥æ¨¡ä»¿å¤§å‹æ•™å¸ˆæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æœ¬ç ”ç©¶å¼•å…¥äº†è¯„è®ºä¿®è®¢æç¤ºç­‰æ–¹æ³•è¿›è¡Œæ•°æ®ç”Ÿæˆå’Œè®­ç»ƒå­¦ç”Ÿæ¨¡å‹çš„è’¸é¦è¿‡ç¨‹ã€‚</li>
<li>åœ¨å¸¸è¯†é—®ç­”ï¼ˆCQAï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œäº†ç³»ç»Ÿæ¯”è¾ƒï¼Œé€šè¿‡å­¦ç”Ÿæ¨¡å‹çš„å‡†ç¡®æ€§è¡¡é‡æ€§èƒ½ã€‚</li>
<li>é™¤äº†æ€§èƒ½è¯„ä¼°ï¼Œè¿˜é€šè¿‡åŸºäºäººç±»çš„ç ”ç©¶æ¥è¯„ä¼°æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>æœ¬ç ”ç©¶æä¾›äº†æ–°çš„è’¸é¦æ–¹æ³•å’Œæ¯”è¾ƒç»“æœï¼Œæœ‰åŠ©äºæ¨åŠ¨å°å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨å’Œå‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16056">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-41c0914944849839a67dd2fe7eda171a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1edadde9af8277e0ff4f03335a5169ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e344d8414ac548ea2c5ef7ea5e833781.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a182cf3475a7e8b024ac8a284a2328ff.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LiveCC-Learning-Video-LLM-with-Streaming-Speech-Transcription-at-Scale"><a href="#LiveCC-Learning-Video-LLM-with-Streaming-Speech-Transcription-at-Scale" class="headerlink" title="LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale"></a>LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale</h2><p><strong>Authors:Joya Chen, Ziyun Zeng, Yiqi Lin, Wei Li, Zejun Ma, Mike Zheng Shou</strong></p>
<p>Recent video large language models (Video LLMs) often depend on costly human annotations or proprietary model APIs (e.g., GPT-4o) to produce training data, which limits their training at scale. In this paper, we explore large-scale training for Video LLM with cheap automatic speech recognition (ASR) transcripts. Specifically, we propose a novel streaming training approach that densely interleaves the ASR words and video frames according to their timestamps. Compared to previous studies in vision-language representation with ASR, our method naturally fits the streaming characteristics of ASR, thus enabling the model to learn temporally-aligned, fine-grained vision-language modeling. To support the training algorithm, we introduce a data production pipeline to process YouTube videos and their closed captions (CC, same as ASR), resulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset for high-quality supervised fine-tuning (SFT). Remarkably, even without SFT, the ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general video QA performance and exhibits a new capability in real-time video commentary. To evaluate this, we carefully design a new LiveSports-3K benchmark, using LLM-as-a-judge to measure the free-form commentary. Experiments show our final LiveCC-7B-Instruct model can surpass advanced 72B models (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even working in a real-time mode. Meanwhile, it achieves state-of-the-art results at the 7B&#x2F;8B scale on popular video QA benchmarks such as VideoMME and OVOBench, demonstrating the broad generalizability of our approach. All resources of this paper have been released at <a target="_blank" rel="noopener" href="https://showlab.github.io/livecc">https://showlab.github.io/livecc</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰é€šå¸¸ä¾èµ–äºæ˜‚è´µçš„äººå·¥æ ‡æ³¨æˆ–ä¸“æœ‰æ¨¡å‹APIï¼ˆä¾‹å¦‚GPT-4oï¼‰æ¥ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œè¿™é™åˆ¶äº†å…¶å¤§è§„æ¨¡è®­ç»ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢ä½¿ç”¨å»‰ä»·çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•æ•°æ®å¯¹Video LLMè¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æµå¼è®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ ¹æ®æ—¶é—´æˆ³ç´§å¯†åœ°äº¤ç»‡ASRå•è¯å’Œè§†é¢‘å¸§ã€‚ä¸ä»¥å¾€ä½¿ç”¨ASRçš„è§†å¬è¯­è¨€è¡¨ç¤ºç ”ç©¶ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è‡ªç„¶åœ°é€‚åº”äº†ASRçš„æµå¼ç‰¹æ€§ï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ—¶é—´å¯¹é½çš„ã€ç²¾ç»†çš„è§†å¬è¯­è¨€å»ºæ¨¡ã€‚ä¸ºäº†æ”¯æŒè®­ç»ƒç®—æ³•ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ•°æ®ç”Ÿäº§ç®¡é“æ¥å¤„ç†YouTubeè§†é¢‘åŠå…¶å…³é—­å­—å¹•ï¼ˆCCï¼Œä¸ASRç›¸åŒï¼‰ï¼Œä»è€Œäº§ç”Ÿäº†ç”¨äºé¢„è®­ç»ƒçš„Live-CC-5Mæ•°æ®é›†å’Œç”¨äºé«˜è´¨é‡ç›‘ç£ç²¾ç»†è°ƒæ•´ï¼ˆSFTï¼‰çš„Live-Whisperx-526Kæ•°æ®é›†ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿æ²¡æœ‰SFTï¼Œä»…ä½¿ç”¨ASRé¢„è®­ç»ƒçš„LiveCC-7B-Baseæ¨¡å‹ä¹Ÿè¡¨ç°å‡ºæœ‰ç«äº‰åŠ›çš„é€šç”¨è§†é¢‘é—®ç­”æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºå®æ—¶è§†é¢‘è¯„è®ºçš„æ–°èƒ½åŠ›ã€‚ä¸ºäº†è¯„ä¼°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ç²¾å¿ƒè®¾è®¡äº†æ–°çš„LiveSports-3KåŸºå‡†æµ‹è¯•ï¼Œåˆ©ç”¨LLM-as-a-judgeæ¥è¡¡é‡è‡ªç”±å½¢å¼çš„è¯„è®ºã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æœ€ç»ˆçš„LiveCC-7B-Instructæ¨¡å‹å³ä½¿åœ¨å®æ—¶æ¨¡å¼ä¸‹ä¹Ÿèƒ½åœ¨è¯„è®ºè´¨é‡ä¸Šè¶…è¶Šå…ˆè¿›çš„72Bæ¨¡å‹ï¼ˆQwen2.5-VL-72B-Instructï¼ŒLLaVA-Video-72Bï¼‰ã€‚åŒæ—¶ï¼Œå®ƒåœ¨æµè¡Œçš„è§†é¢‘QAåŸºå‡†æµ‹è¯•ï¼ˆå¦‚VideoMMEå’ŒOVOBenchï¼‰ä¸Šè¾¾åˆ°äº†7B&#x2F;8Bè§„æ¨¡çš„æœ€ä½³ç»“æœï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰å¹¿æ³›çš„é€šç”¨æ€§ã€‚æœ¬è®ºæ–‡çš„æ‰€æœ‰èµ„æºå·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://showlab.github.io/livecc%E3%80%82">https://showlab.github.io/liveccã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16030v1">PDF</a> CVPR 2025. If any references are missing, please contact   <a href="mailto:&#x6a;&#x6f;&#x79;&#97;&#x63;&#x68;&#x65;&#110;&#x40;&#117;&#46;&#x6e;&#117;&#x73;&#46;&#101;&#100;&#117;">&#x6a;&#x6f;&#x79;&#97;&#x63;&#x68;&#x65;&#110;&#x40;&#117;&#46;&#x6e;&#117;&#x73;&#46;&#101;&#100;&#117;</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åˆ©ç”¨ä½æˆæœ¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è½¬å½•æ•°æ®å¯¹è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMï¼‰è¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒçš„æ–¹æ³•ã€‚æå‡ºäº†ä¸€ç§æ–°çš„æµå¼è®­ç»ƒæ–¹å¼ï¼Œç´§å¯†åœ°å°†ASRè¯è¯­å’Œè§†é¢‘å¸§æŒ‰ç…§æ—¶é—´æˆ³äº¤ç»‡åœ¨ä¸€èµ·ã€‚ç›¸æ¯”ä»¥å¾€çš„ç ”ç©¶ï¼Œè¯¥æ–¹æ³•è‡ªç„¶é€‚åˆASRçš„æµå¼ç‰¹æ€§ï¼Œä½¿æ¨¡å‹èƒ½å­¦ä¹ æ—¶é—´å¯¹é½çš„ã€ç²¾ç»†çš„è§†è¯­è¨€å»ºæ¨¡ã€‚ä¸ºæ”¯æŒè®­ç»ƒç®—æ³•ï¼Œä»‹ç»äº†å¤„ç†YouTubeè§†é¢‘åŠå…¶å°é—­å­—å¹•ï¼ˆCCï¼Œä¸ASRç›¸åŒï¼‰çš„æ•°æ®ç”Ÿäº§ç®¡é“ï¼Œå½¢æˆäº†ç”¨äºé¢„è®­ç»ƒçš„Live-CC-5Mæ•°æ®é›†å’Œç”¨äºé«˜è´¨é‡ç›‘ç£ç²¾ç»†è°ƒæ•´çš„Live-Whisperx-526Kæ•°æ®é›†ã€‚ä»…ä½¿ç”¨ASRé¢„è®­ç»ƒçš„LiveCC-7B-Baseæ¨¡å‹ï¼Œåœ¨æ— éœ€ç²¾ç»†è°ƒæ•´çš„æƒ…å†µä¸‹ï¼Œå°±å±•ç°å‡ºæœ‰ç«äº‰åŠ›çš„é€šç”¨è§†é¢‘é—®ç­”æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºå®æ—¶è§†é¢‘è¯„è®ºçš„æ–°èƒ½åŠ›ã€‚é€šè¿‡æ–°è®¾è®¡çš„LiveSports-3KåŸºå‡†æµ‹è¯•ï¼Œä»¥LLMä½œä¸ºæ³•å®˜æ¥è¡¡é‡è‡ªç”±å½¢å¼çš„è¯„è®ºï¼Œå®éªŒæ˜¾ç¤ºï¼Œæœ€åçš„LiveCC-7B-Instructæ¨¡å‹å³ä½¿åœ¨å®æ—¶æ¨¡å¼ä¸‹ï¼Œä¹Ÿèƒ½åœ¨è¯„è®ºè´¨é‡ä¸Šè¶…è¶Šå…ˆè¿›çš„72Bæ¨¡å‹ï¼ˆQwen2.5-VL-72B-Instructï¼ŒLLaVA-Video-72Bï¼‰ï¼ŒåŒæ—¶åœ¨æµè¡Œçš„è§†é¢‘QAåŸºå‡†æµ‹è¯•ï¼ˆå¦‚VideoMMEå’ŒOVOBenchï¼‰ä¸Šè¾¾åˆ°æœ€ä½³ç»“æœï¼Œæ˜¾ç¤ºå‡ºæ–¹æ³•çš„å¹¿æ³›é€šç”¨æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Video LLMè®­ç»ƒå—é™äºæˆæœ¬é«˜æ˜‚çš„äººç±»æ³¨é‡Šå’Œä¸“æœ‰æ¨¡å‹APIã€‚</li>
<li>æå‡ºä½¿ç”¨ä½æˆæœ¬ASRè½¬å½•æ•°æ®å¯¹Video LLMè¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒçš„æ–¹æ³•ã€‚</li>
<li>æ–°å‹æµå¼è®­ç»ƒæ–¹å¼èƒ½å­¦ä¹ æ—¶é—´å¯¹é½çš„ã€ç²¾ç»†çš„è§†è¯­è¨€å»ºæ¨¡ã€‚</li>
<li>å¼•å…¥æ•°æ®å¤„ç†ç®¡é“ï¼Œå½¢æˆç”¨äºé¢„è®­ç»ƒå’Œç²¾ç»†è°ƒæ•´çš„ç‰¹å®šæ•°æ®é›†ã€‚</li>
<li>ä»…ä½¿ç”¨ASRé¢„è®­ç»ƒçš„æ¨¡å‹å±•ç°å‡ºç«äº‰åŠ›å¼ºçš„è§†é¢‘é—®ç­”å’Œå®æ—¶è§†é¢‘è¯„è®ºèƒ½åŠ›ã€‚</li>
<li>åœ¨å®æ—¶æ¨¡å¼ä¸‹ï¼Œè¯„è®ºè´¨é‡è¶…è¶Šå…ˆè¿›çš„å¤§å‹æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16030">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc07589db3dd3fd6981d002ebda1e260.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9029e419c550ea4ef0212445c4484b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-07d238cb3311b3f221e2e32d1fe575c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9e395f8855cc2d5da190209aa7f9334.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07e38f900e7b023a057337f7796bb2ea.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Benchmarking-LLM-for-Code-Smells-Detection-OpenAI-GPT-4-0-vs-DeepSeek-V3"><a href="#Benchmarking-LLM-for-Code-Smells-Detection-OpenAI-GPT-4-0-vs-DeepSeek-V3" class="headerlink" title="Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs   DeepSeek-V3"></a>Benchmarking LLM for Code Smells Detection: OpenAI GPT-4.0 vs   DeepSeek-V3</h2><p><strong>Authors:Ahmed R. Sadik, Siddhata Govind</strong></p>
<p>Determining the most effective Large Language Model for code smell detection presents a complex challenge. This study introduces a structured methodology and evaluation matrix to tackle this issue, leveraging a curated dataset of code samples consistently annotated with known smells. The dataset spans four prominent programming languages Java, Python, JavaScript, and C++; allowing for cross language comparison. We benchmark two state of the art LLMs, OpenAI GPT 4.0 and DeepSeek-V3, using precision, recall, and F1 score as evaluation metrics. Our analysis covers three levels of detail: overall performance, category level performance, and individual code smell type performance. Additionally, we explore cost effectiveness by comparing the token based detection approach of GPT 4.0 with the pattern-matching techniques employed by DeepSeek V3. The study also includes a cost analysis relative to traditional static analysis tools such as SonarQube. The findings offer valuable guidance for practitioners in selecting an efficient, cost effective solution for automated code smell detection </p>
<blockquote>
<p>ç¡®å®šç”¨äºä»£ç å¼‚å‘³æ£€æµ‹çš„æœ€æœ‰æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯ä¸€ä¸ªå¤æ‚çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§ç»“æ„åŒ–çš„æ–¹æ³•å’Œè¯„ä¼°çŸ©é˜µæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåˆ©ç”¨äº†ä¸€ç»„ç»è¿‡æ•´ç†çš„ä»£ç æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬å§‹ç»ˆä¸å·²çŸ¥çš„å¼‚å‘³è¿›è¡Œæ ‡æ³¨ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†å››ç§æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ï¼šJavaã€Pythonã€JavaScriptå’ŒC++ï¼Œå¯å®ç°è·¨è¯­è¨€æ¯”è¾ƒã€‚æˆ‘ä»¬ä½¿ç”¨ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°ä½œä¸ºè¯„ä»·æŒ‡æ ‡ï¼Œå¯¹ä¸¤ç§æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹OpenAI GPT 4.0å’ŒDeepSeek-V3è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„åˆ†ææ¶µç›–äº†ä¸‰ä¸ªå±‚æ¬¡çš„ç»†èŠ‚ï¼šæ•´ä½“æ€§èƒ½ã€ç±»åˆ«æ€§èƒ½ä»¥åŠå•ä¸ªä»£ç å¼‚å‘³ç±»å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡æ¯”è¾ƒGPT 4.0çš„åŸºäºä»¤ç‰Œæ£€æµ‹æ–¹æ³•ä¸DeepSeek V3é‡‡ç”¨çš„æ¨¡å¼åŒ¹é…æŠ€æœ¯ï¼Œæ¢è®¨äº†æˆæœ¬æ•ˆç›Šã€‚è¯¥ç ”ç©¶è¿˜åŒ…æ‹¬ç›¸å¯¹äºä¼ ç»Ÿé™æ€åˆ†æå·¥å…·ï¼ˆå¦‚SonarQubeï¼‰çš„æˆæœ¬åˆ†æã€‚ç ”ç©¶ç»“æœä¸ºä»ä¸šè€…åœ¨é€‰æ‹©é«˜æ•ˆã€ç»æµçš„è‡ªåŠ¨åŒ–ä»£ç å¼‚å‘³æ£€æµ‹è§£å†³æ–¹æ¡ˆæ–¹é¢æä¾›äº†å®è´µçš„æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16027v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬ç ”ç©¶é’ˆå¯¹ä»£ç å¼‚å‘³æ£€æµ‹çš„æœ€æœ‰æ•ˆå¤§å‹è¯­è¨€æ¨¡å‹é€‰æ‹©é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“æ„åŒ–æ–¹æ³•å’Œè¯„ä¼°çŸ©é˜µã€‚ç ”ç©¶ä½¿ç”¨äº†ä¸€ç§ç»è¿‡æ•´ç†çš„åŒ…å«å·²çŸ¥å¼‚å‘³æ³¨é‡Šçš„ä»£ç æ ·æœ¬æ•°æ®é›†ï¼Œæ¶µç›–äº†Javaã€Pythonã€JavaScriptå’ŒC++å››ç§ä¸»æµç¼–ç¨‹è¯­è¨€ï¼Œä¾¿äºè·¨è¯­è¨€æ¯”è¾ƒã€‚é€šè¿‡å¯¹OpenAI GPT 4.0å’ŒDeepSeek-V3ä¸¤ç§å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œä»¥ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼Œæœ¬ç ”ç©¶å…¨é¢åˆ†æäº†ä¸¤ç§æ¨¡å‹åœ¨æ€»ä½“æ€§èƒ½ã€åˆ†ç±»æ€§èƒ½ä»¥åŠå•ä¸ªä»£ç å¼‚å‘³ç±»å‹æ€§èƒ½æ–¹é¢çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¯”è¾ƒäº†GPT 4.0çš„åŸºäºä»¤ç‰Œæ£€æµ‹æ–¹æ³•ä¸DeepSeek V3çš„æ¨¡å¼åŒ¹é…æŠ€æœ¯ï¼Œå¹¶è¿›è¡Œäº†ä¸ä¼ ç»Ÿé™æ€åˆ†æå·¥å…·å¦‚SonarQubeçš„æˆæœ¬åˆ†æã€‚ç ”ç©¶ä¸ºä»ä¸šè€…é€‰æ‹©é«˜æ•ˆä¸”ç»æµçš„è‡ªåŠ¨åŒ–ä»£ç å¼‚å‘³æ£€æµ‹è§£å†³æ–¹æ¡ˆæä¾›äº†å®è´µæŒ‡å¯¼ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç å¼‚å‘³æ£€æµ‹æ–¹é¢æ€§èƒ½è¯„ä¼°çš„ç»“æ„åŒ–æ–¹æ³•å’Œè¯„ä¼°çŸ©é˜µã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†åŒ…å«å¤šç§ç¼–ç¨‹è¯­è¨€çš„ä»£ç æ ·æœ¬æ•°æ®é›†ï¼Œå¹¶è¿›è¡Œäº†è·¨è¯­è¨€æ¯”è¾ƒã€‚</li>
<li>ç ”ç©¶å¯¹OpenAI GPT 4.0å’ŒDeepSeek-V3ä¸¤ç§å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†æ€»ä½“ã€åˆ†ç±»å’Œå•ä¸ªä»£ç å¼‚å‘³ç±»å‹ä¸‰ä¸ªå±‚é¢çš„æ€§èƒ½åˆ†æã€‚</li>
<li>ç ”ç©¶æ¯”è¾ƒäº†GPT 4.0çš„åŸºäºä»¤ç‰Œæ£€æµ‹æ–¹æ³•ä¸DeepSeek V3çš„æ¨¡å¼åŒ¹é…æŠ€æœ¯çš„æˆæœ¬æ•ˆç›Šã€‚</li>
<li>ç ”ç©¶è¿˜è¿›è¡Œäº†ä¸ä¼ ç»Ÿé™æ€åˆ†æå·¥å…·çš„æˆæœ¬åˆ†æå¯¹æ¯”ã€‚</li>
<li>ç ”ç©¶ç»“æœæä¾›äº†å®è·µè€…å¦‚ä½•é€‰æ‹©æœ‰æ•ˆä¸”ç»æµçš„è‡ªåŠ¨åŒ–ä»£ç å¼‚å‘³æ£€æµ‹è§£å†³æ–¹æ¡ˆçš„æŒ‡å¯¼æ€§å»ºè®®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16027">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fdf5e1a103b7e289dc9d6445cf2026cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81fd649bcf3459ddcf58165430d3a497.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1da54363ca7ee7c4bc2f8912e805a299.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2eb48ec9ee0c781e1d0f88fecbd0494c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-661dc5058b00d59a9c19bf7f8ede853f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83710ec1f2f17b794fbd554b7f74e817.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CAPO-Cost-Aware-Prompt-Optimization"><a href="#CAPO-Cost-Aware-Prompt-Optimization" class="headerlink" title="CAPO: Cost-Aware Prompt Optimization"></a>CAPO: Cost-Aware Prompt Optimization</h2><p><strong>Authors:Tom Zehle, Moritz Schlager, Timo HeiÃŸ, Matthias Feurer</strong></p>
<p>Large language models (LLMs) have revolutionized natural language processing by solving a wide range of tasks simply guided by a prompt. Yet their performance is highly sensitive to prompt formulation. While automated prompt optimization addresses this challenge by finding optimal prompts, current methods require a substantial number of LLM calls and input tokens, making prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt Optimization), an algorithm that enhances prompt optimization efficiency by integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as operators, incorporating racing to save evaluations and multi-objective optimization to balance performance with prompt length. It jointly optimizes instructions and few-shot examples while leveraging task descriptions for improved robustness. Our extensive experiments across diverse datasets and LLMs demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization methods in 11&#x2F;15 cases with improvements up to 21%p. Our algorithm achieves better performances already with smaller budgets, saves evaluations through racing, and decreases average prompt length via a length penalty, making it both cost-efficient and cost-aware. Even without few-shot examples, CAPO outperforms its competitors and generally remains robust to initial prompts. CAPO represents an important step toward making prompt optimization more powerful and accessible by improving cost-efficiency. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç®€å•çš„æç¤ºæŒ‡å¯¼è§£å†³äº†å¹¿æ³›çš„ä»»åŠ¡ï¼Œä»è€Œé©æ–°äº†è‡ªç„¶è¯­è¨€å¤„ç†ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ€§èƒ½å¯¹æç¤ºçš„æ„æ€éå¸¸æ•æ„Ÿã€‚è™½ç„¶è‡ªåŠ¨æç¤ºä¼˜åŒ–å¯ä»¥é€šè¿‡æ‰¾åˆ°æœ€ä½³æç¤ºæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œä½†å½“å‰çš„æ–¹æ³•éœ€è¦å¤§é‡çš„LLMè°ƒç”¨å’Œè¾“å…¥ä»¤ç‰Œï¼Œä½¿å¾—æç¤ºä¼˜åŒ–æˆæœ¬é«˜æ˜‚ã€‚æˆ‘ä»¬å¼•å…¥äº†CAPOï¼ˆåŸºäºæˆæœ¬çš„æç¤ºä¼˜åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡é›†æˆAutoMLæŠ€æœ¯æé«˜æç¤ºä¼˜åŒ–æ•ˆç‡çš„ç®—æ³•ã€‚CAPOæ˜¯ä¸€ç§è¿›åŒ–æ–¹æ³•ï¼Œä»¥LLMä½œä¸ºæ“ä½œå‘˜ï¼Œç»“åˆäº†ç«èµ›ä»¥èŠ‚çœè¯„ä¼°å’ŒåŸºäºæ€§èƒ½çš„å¤šç›®æ ‡ä¼˜åŒ–æ¥å¹³è¡¡æç¤ºé•¿åº¦ã€‚å®ƒåŒæ—¶ä¼˜åŒ–æŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹ï¼Œå¹¶åˆ©ç”¨ä»»åŠ¡æè¿°æ¥æé«˜ç¨³å¥æ€§ã€‚æˆ‘ä»¬åœ¨å„ç§æ•°æ®é›†å’ŒLLMä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œåœ¨15ä¸ªæ¡ˆä¾‹ä¸­ï¼ŒCAPOåœ¨11ä¸ªæ¡ˆä¾‹ä¸­çš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„ç¦»æ•£æç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œæ”¹è¿›å¹…åº¦é«˜è¾¾21%ã€‚æˆ‘ä»¬çš„ç®—æ³•åœ¨è¾ƒå°çš„é¢„ç®—ä¸‹å°±å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ï¼Œé€šè¿‡ç«èµ›èŠ‚çœäº†è¯„ä¼°å·¥ä½œï¼Œå¹¶é€šè¿‡é•¿åº¦æƒ©ç½šå‡å°‘äº†å¹³å‡æç¤ºé•¿åº¦ï¼Œä½¿å…¶æ—¢ç»æµåˆæ³¨é‡æˆæœ¬ã€‚å³ä½¿æ²¡æœ‰å°‘é‡ç¤ºä¾‹ï¼ŒCAPOä¹Ÿèƒ½è¶…è¶Šç«äº‰å¯¹æ‰‹ï¼Œå¹¶ä¸”å¯¹åˆå§‹æç¤ºä¿æŒç¨³å¥ã€‚CAPOæœç€æé«˜æç¤ºä¼˜åŒ–çš„æˆæœ¬æ•ˆç›Šè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œä½¿å…¶æ›´å¼ºå¤§ã€æ›´æ˜“è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16005v1">PDF</a> Submitted to AutoML 2025</p>
<p><strong>Summary</strong></p>
<p>LLMçš„æç¤ºä¼˜åŒ–é—®é¢˜å¯ä»¥é€šè¿‡é›†æˆè‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ æŠ€æœ¯è§£å†³ï¼Œæé«˜æ•ˆç‡å’Œé™ä½æˆæœ¬ã€‚ä»‹ç»äº†ä¸€ç§åä¸ºCAPOçš„ç®—æ³•ï¼Œå®ƒé€šè¿‡è¿›åŒ–æ–¹æ³•è”åˆä¼˜åŒ–æŒ‡ä»¤å’Œå°‘é‡çš„ä¾‹å­ï¼Œåœ¨ä»»åŠ¡æè¿°æ–¹é¢å–å¾—äº†ç¨³å¥çš„æ”¹è¿›ã€‚åœ¨å¤šæ ·åŒ–çš„æ•°æ®é›†å’ŒLLMæ–¹é¢çš„å®éªŒè¡¨æ˜ï¼ŒCAPOåœ¨å¤§å¤šæ•°åœºæ™¯ä¸‹ä¼˜äºç°æœ‰çš„ç¦»æ•£æç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œå¹¶åœ¨é¢„ç®—è¾ƒå°çš„æƒ…å†µä¸‹å®ç°äº†æ›´å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMçš„æ€§èƒ½å¯¹æç¤ºåˆ¶å®šé«˜åº¦æ•æ„Ÿã€‚</li>
<li>å½“å‰æç¤ºä¼˜åŒ–æ–¹æ³•éœ€è¦å¤§é‡çš„LLMè°ƒç”¨å’Œè¾“å…¥ä»¤ç‰Œï¼Œæˆæœ¬é«˜æ˜‚ã€‚</li>
<li>CAPOç®—æ³•é€šè¿‡é›†æˆAutoMLæŠ€æœ¯æé«˜äº†æç¤ºä¼˜åŒ–çš„æ•ˆç‡ã€‚</li>
<li>CAPOé‡‡ç”¨è¿›åŒ–æ–¹æ³•ï¼Œè”åˆä¼˜åŒ–æŒ‡ä»¤å’Œå°‘é‡ç¤ºä¾‹ï¼ŒåŒæ—¶åˆ©ç”¨ä»»åŠ¡æè¿°æ¥æé«˜ç¨³å¥æ€§ã€‚</li>
<li>CAPOåœ¨å¤šæ ·åŒ–çš„æ•°æ®é›†å’ŒLLMæ–¹é¢çš„å®éªŒè¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºç°æœ‰çš„ç¦»æ•£æç¤ºä¼˜åŒ–æ–¹æ³•ã€‚</li>
<li>CAPOåœ¨è¾ƒå°çš„é¢„ç®—ä¸‹å®ç°äº†æ›´å¥½çš„æ€§èƒ½ï¼Œé€šè¿‡æ¯”èµ›èŠ‚çœè¯„ä¼°æ¬¡æ•°ï¼Œå¹¶é€šè¿‡é•¿åº¦æƒ©ç½šå‡å°‘å¹³å‡æç¤ºé•¿åº¦ã€‚</li>
<li>CAPOç®—æ³•ä½¿å¾—æç¤ºä¼˜åŒ–æ›´å…·æˆæœ¬æ•ˆç›Šå’Œå¯è®¿é—®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16005">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f90ee0a4dafd8331bae268d1b3aa19dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b116fd3254b73260dad8b5aba0aa636b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fba975ea5ea648e831c5bf75967937cd.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="StreamRL-Scalable-Heterogeneous-and-Elastic-RL-for-LLMs-with-Disaggregated-Stream-Generation"><a href="#StreamRL-Scalable-Heterogeneous-and-Elastic-RL-for-LLMs-with-Disaggregated-Stream-Generation" class="headerlink" title="StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with   Disaggregated Stream Generation"></a>StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with   Disaggregated Stream Generation</h2><p><strong>Authors:Yinmin Zhong, Zili Zhang, Xiaoniu Song, Hanpeng Hu, Chao Jin, Bingyang Wu, Nuo Chen, Yukun Chen, Yu Zhou, Changyi Wan, Hongyu Zhou, Yimin Jiang, Yibo Zhu, Daxin Jiang</strong></p>
<p>Reinforcement learning (RL) has become the core post-training technique for large language models (LLMs). RL for LLMs involves two stages: generation and training. The LLM first generates samples online, which are then used to derive rewards for training. The conventional view holds that the colocated architecture, where the two stages share resources via temporal multiplexing, outperforms the disaggregated architecture, in which dedicated resources are assigned to each stage. However, in real-world deployments, we observe that the colocated architecture suffers from resource coupling, where the two stages are constrained to use the same resources. This coupling compromises the scalability and cost-efficiency of colocated RL in large-scale training. In contrast, the disaggregated architecture allows for flexible resource allocation, supports heterogeneous training setups, and facilitates cross-datacenter deployment.   StreamRL is designed with disaggregation from first principles and fully unlocks its potential by addressing two types of performance bottlenecks in existing disaggregated RL frameworks: pipeline bubbles, caused by stage dependencies, and skewness bubbles, resulting from long-tail output length distributions. To address pipeline bubbles, StreamRL breaks the traditional stage boundary in synchronous RL algorithms through stream generation and achieves full overlapping in asynchronous RL. To address skewness bubbles, StreamRL employs an output-length ranker model to identify long-tail samples and reduces generation time via skewness-aware dispatching and scheduling. Experiments show that StreamRL improves throughput by up to 2.66x compared to existing state-of-the-art systems, and improves cost-effectiveness by up to 1.33x in a heterogeneous, cross-datacenter setting. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ¸å¿ƒåè®­ç»ƒæŠ€æœ¯ã€‚LLMçš„RLæ¶‰åŠä¸¤ä¸ªé˜¶æ®µï¼šç”Ÿæˆå’Œè®­ç»ƒã€‚LLMé¦–å…ˆåœ¨çº¿ç”Ÿæˆæ ·æœ¬ï¼Œç„¶åç”¨äºæ¨å¯¼è®­ç»ƒçš„å¥–åŠ±ã€‚ä¼ ç»Ÿè§‚ç‚¹è®¤ä¸ºï¼Œå…±ç½®æ¶æ„ï¼ˆä¸¤ä¸ªé˜¶æ®µé€šè¿‡æ—¶é—´å¤ç”¨å…±äº«èµ„æºï¼‰ä¼˜äºåˆ†æ•£æ¶æ„ï¼ˆä¸ºæ¯ä¸ªé˜¶æ®µåˆ†é…ä¸“ç”¨èµ„æºï¼‰ã€‚ç„¶è€Œï¼Œåœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨éƒ¨ç½²ä¸­ï¼Œæˆ‘ä»¬å‘ç°å…±ç½®æ¶æ„å­˜åœ¨èµ„æºè€¦åˆçš„é—®é¢˜ï¼Œä¸¤ä¸ªé˜¶æ®µçš„èµ„æºä½¿ç”¨å—åˆ°é™åˆ¶ã€‚è¿™ç§è€¦åˆå½±å“äº†å…±ç½®RLåœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­çš„å¯æ‰©å±•æ€§å’Œæˆæœ¬æ•ˆç›Šã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåˆ†æ•£æ¶æ„å…è®¸çµæ´»çš„èµ„æºåˆ†é…ï¼Œæ”¯æŒå¼‚æ„è®­ç»ƒè®¾ç½®ï¼Œå¹¶æœ‰åˆ©äºè·¨æ•°æ®ä¸­å¿ƒéƒ¨ç½²ã€‚StreamRLä»ç¬¬ä¸€æ€§åŸåˆ™å‡ºå‘è¿›è¡Œåˆ†æ•£è®¾è®¡ï¼Œå¹¶é€šè¿‡è§£å†³ç°æœ‰åˆ†æ•£RLæ¡†æ¶ä¸­çš„ä¸¤ç§æ€§èƒ½ç“¶é¢ˆæ¥å……åˆ†å‘æŒ¥å…¶æ½œåŠ›ï¼šç”±é˜¶æ®µä¾èµ–æ€§å¼•èµ·çš„ç®¡é“æ°”æ³¡å’Œç”±é•¿å°¾è¾“å‡ºé•¿åº¦åˆ†å¸ƒå¼•èµ·çš„åæ–œæ°”æ³¡ã€‚ä¸ºè§£å†³ç®¡é“æ°”æ³¡é—®é¢˜ï¼ŒStreamRLæ‰“ç ´äº†åŒæ­¥RLç®—æ³•ä¸­çš„ä¼ ç»Ÿé˜¶æ®µè¾¹ç•Œï¼Œé€šè¿‡æµç”Ÿæˆå®ç°å¼‚æ­¥RLçš„å®Œå…¨é‡å ã€‚ä¸ºè§£å†³åæ–œæ°”æ³¡é—®é¢˜ï¼ŒStreamRLé‡‡ç”¨è¾“å‡ºé•¿åº¦æ’åæ¨¡å‹æ¥è¯†åˆ«é•¿å°¾å·´æ ·æœ¬ï¼Œå¹¶é€šè¿‡åæ–œæ„ŸçŸ¥çš„åˆ†æ´¾å’Œè°ƒåº¦ç¼©çŸ­ç”Ÿæˆæ—¶é—´ã€‚å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„ç³»ç»Ÿç›¸æ¯”ï¼ŒStreamRLçš„ååé‡æé«˜äº†é«˜è¾¾2.66å€ï¼Œåœ¨å¼‚æ„ã€è·¨æ•°æ®ä¸­å¿ƒè®¾ç½®ä¸­ï¼Œæˆæœ¬æ•ˆç›Šæé«˜äº†é«˜è¾¾1.33å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15930v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ¸å¿ƒè®­ç»ƒæŠ€æœ¯åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šç”Ÿæˆä¸è®­ç»ƒã€‚ä¼ ç»Ÿè§‚ç‚¹è®¤ä¸ºé›†æˆæ¶æ„ä¼˜äºåˆ†æ•£æ¶æ„ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé›†æˆæ¶æ„å­˜åœ¨èµ„æºè€¦åˆé—®é¢˜ï¼Œå½±å“å¯æ‰©å±•æ€§å’Œæˆæœ¬æ•ˆç›Šã€‚ç›¸åï¼Œåˆ†æ•£æ¶æ„å…è®¸çµæ´»èµ„æºåˆ†é…å¹¶æ”¯æŒè·¨æ•°æ®ä¸­å¿ƒéƒ¨ç½²ã€‚StreamRLä»åŸºæœ¬åŸåˆ™ä¸Šè¿›è¡Œåˆ†æ•£è®¾è®¡ï¼Œè§£å†³äº†ç°æœ‰åˆ†æ•£RLæ¡†æ¶ä¸­çš„ä¸¤ç§æ€§èƒ½ç“¶é¢ˆé—®é¢˜ï¼ŒåŒ…æ‹¬ç®¡é“æ³¡å’Œåæ–œæ³¡ã€‚å®ƒé€šè¿‡æ‰“ç ´åŒæ­¥RLç®—æ³•çš„ä¼ ç»Ÿé˜¶æ®µè¾¹ç•Œå’Œé‡‡ç”¨è¾“å‡ºé•¿åº¦æ’åæ¨¡å‹æ¥è¯†åˆ«é•¿å°¾æ ·æœ¬ï¼Œæé«˜äº†ååé‡å’Œæˆæœ¬æ•ˆç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼ºåŒ–å­¦ä¹ æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…³é”®è®­ç»ƒæŠ€æœ¯ã€‚</li>
<li>LLMçš„å¼ºåŒ–å­¦ä¹ æ¶‰åŠç”Ÿæˆå’Œè®­ç»ƒä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>ä¼ ç»Ÿä¸Šï¼Œé›†æˆæ¶æ„è¢«è®¤ä¸ºæ˜¯ä¼˜äºåˆ†æ•£æ¶æ„çš„ï¼Œä½†åœ¨å®è·µä¸­å­˜åœ¨èµ„æºè€¦åˆé—®é¢˜ã€‚</li>
<li>åˆ†æ•£æ¶æ„æœ‰åˆ©äºçµæ´»èµ„æºåˆ†é…ã€æ”¯æŒä¸åŒè®­ç»ƒè®¾ç½®å’Œè·¨æ•°æ®ä¸­å¿ƒéƒ¨ç½²ã€‚</li>
<li>StreamRLè§£å†³äº†ç°æœ‰åˆ†æ•£RLæ¡†æ¶ä¸­çš„ç®¡é“æ³¡å’Œåæ–œæ³¡é—®é¢˜ã€‚</li>
<li>StreamRLé€šè¿‡æ‰“ç ´åŒæ­¥RLç®—æ³•çš„ä¼ ç»Ÿé˜¶æ®µè¾¹ç•Œå’Œé‡‡ç”¨è¾“å‡ºé•¿åº¦æ’åæ¨¡å‹ï¼Œæé«˜äº†æ€§èƒ½å’Œæˆæœ¬æ•ˆç›Šã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4b2d6cb656ce6c14b846ab0d87bc6dca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b17b450c0c293890148ee1d88b449c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-560430be7ff9506725807578149e07fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5bc3b303ff1ec74201ae4336d8b33ee5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10ee40462255895fc9969ee62fac69aa.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Towards-Test-Generation-from-Task-Description-for-Mobile-Testing-with-Multi-modal-Reasoning"><a href="#Towards-Test-Generation-from-Task-Description-for-Mobile-Testing-with-Multi-modal-Reasoning" class="headerlink" title="Towards Test Generation from Task Description for Mobile Testing with   Multi-modal Reasoning"></a>Towards Test Generation from Task Description for Mobile Testing with   Multi-modal Reasoning</h2><p><strong>Authors:Hieu Huynh, Hai Phung, Hao Pham, Tien N. Nguyen, Vu Nguyen</strong></p>
<p>In Android GUI testing, generating an action sequence for a task that can be replayed as a test script is common. Generating sequences of actions and respective test scripts from task goals described in natural language can eliminate the need for manually writing test scripts. However, existing approaches based on large language models (LLM) often struggle with identifying the final action, and either end prematurely or continue past the final screen. In this paper, we introduce VisiDroid, a multi-modal, LLM-based, multi-agent framework that iteratively determines the next action and leverages visual images of screens to detect the taskâ€™s completeness. The multi-modal approach enhances our model in two significant ways. First, this approach enables it to avoid prematurely terminating a task when textual content alone provides misleading indications of task completion. Additionally, visual input helps the tool avoid errors when changes in the GUI do not directly affect functionality toward task completion, such as adjustments to font sizes or colors. Second, the multi-modal approach also ensures the tool not progress beyond the final screen, which might lack explicit textual indicators of task completion but could display a visual element indicating task completion, which is common in GUI apps. Our evaluation shows that VisiDroid achieves an accuracy of 87.3%, outperforming the best baseline relatively by 23.5%. We also demonstrate that our multi-modal framework with images and texts enables the LLM to better determine when a task is completed. </p>
<blockquote>
<p>åœ¨Android GUIæµ‹è¯•ä¸­ï¼Œä¸ºä»»åŠ¡ç”Ÿæˆå¯é‡æ’­ä¸ºæµ‹è¯•è„šæœ¬çš„åŠ¨ä½œåºåˆ—æ˜¯å¾ˆå¸¸è§çš„ã€‚ä»è‡ªç„¶è¯­è¨€æè¿°çš„ä»»åŠ¡ç›®æ ‡ä¸­ç”ŸæˆåŠ¨ä½œåºåˆ—å’Œç›¸åº”çš„æµ‹è¯•è„šæœ¬ï¼Œå¯ä»¥æ¶ˆé™¤æ‰‹åŠ¨ç¼–å†™æµ‹è¯•è„šæœ¬çš„éœ€æ±‚ã€‚ç„¶è€Œï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç°æœ‰æ–¹æ³•å¾€å¾€éš¾ä»¥è¯†åˆ«æœ€ç»ˆåŠ¨ä½œï¼Œå¹¶ä¸”è¦ä¹ˆè¿‡æ—©ç»“æŸï¼Œè¦ä¹ˆè¶…è¿‡æœ€ç»ˆå±å¹•ç»§ç»­æ‰§è¡Œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†VisiDroidï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMçš„å¤šæ¨¡å¼ã€å¤šä»£ç†æ¡†æ¶ï¼Œå¯ä»¥è¿­ä»£ç¡®å®šä¸‹ä¸€ä¸ªåŠ¨ä½œï¼Œå¹¶åˆ©ç”¨å±å¹•è§†è§‰å›¾åƒæ¥æ£€æµ‹ä»»åŠ¡çš„å®Œæˆç¨‹åº¦ã€‚å¤šæ¨¡å¼æ–¹æ³•åœ¨ä¸¤ä¸ªé‡è¦æ–¹é¢å¢å¼ºäº†æˆ‘ä»¬çš„æ¨¡å‹ã€‚é¦–å…ˆï¼Œè¿™ç§æ–¹æ³•ä½¿å…¶èƒ½å¤Ÿé¿å…ä»…å‡­æ–‡æœ¬å†…å®¹æä¾›è¯¯å¯¼ä»»åŠ¡å®Œæˆçš„è¿¹è±¡è€Œæå‰ç»ˆæ­¢ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè§†è§‰è¾“å…¥æœ‰åŠ©äºå·¥å…·åœ¨GUIæ›´æ”¹ä¸ç›´æ¥å½±å“ä»»åŠ¡å®Œæˆçš„åŠŸèƒ½æ—¶é¿å…é”™è¯¯ï¼Œä¾‹å¦‚è°ƒæ•´å­—ä½“å¤§å°æˆ–é¢œè‰²ã€‚å…¶æ¬¡ï¼Œå¤šæ¨¡å¼æ–¹æ³•è¿˜ç¡®ä¿å·¥å…·ä¸ä¼šè¶…å‡ºæœ€ç»ˆå±å¹•çš„èŒƒå›´ï¼Œæœ€ç»ˆå±å¹•ä¸Šå¯èƒ½æ²¡æœ‰æ˜ç¡®çš„æ–‡æœ¬æŒ‡ç¤ºä»»åŠ¡å®Œæˆï¼Œä½†å¯èƒ½ä¼šæ˜¾ç¤ºè¡¨ç¤ºä»»åŠ¡å®Œæˆçš„è§†è§‰å…ƒç´ ï¼Œè¿™åœ¨GUIåº”ç”¨ç¨‹åºä¸­æ˜¯å¾ˆå¸¸è§çš„ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼ŒVisiDroidçš„å‡†ç¡®ç‡è¾¾åˆ°äº†87.3%ï¼Œç›¸å¯¹äºæœ€ä½³åŸºå‡†å€¼æé«˜äº†23.5%ã€‚æˆ‘ä»¬è¿˜è¯æ˜ï¼Œæˆ‘ä»¬çš„å¸¦æœ‰å›¾åƒå’Œæ–‡æœ¬çš„å¤šæ¨¡å¼æ¡†æ¶èƒ½å¤Ÿä½¿LLMæ›´å¥½åœ°ç¡®å®šä»»åŠ¡ä½•æ—¶å®Œæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15917v1">PDF</a> Under review for a conference</p>
<p><strong>Summary</strong><br>åœ¨Android GUIæµ‹è¯•ä¸­ï¼Œç”Ÿæˆå¯é‡æ’­çš„æµ‹è¯•è„šæœ¬çš„åŠ¨ä½œåºåˆ—æ˜¯å¸¸è§çš„ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€ã€å¤šä»£ç†æ¡†æ¶VisiDroidï¼Œå®ƒé€šè¿‡è¿­ä»£ç¡®å®šä¸‹ä¸€æ­¥åŠ¨ä½œå¹¶åˆ©ç”¨å±å¹•è§†è§‰å›¾åƒæ£€æµ‹ä»»åŠ¡çš„å®Œæˆåº¦ã€‚æ­¤æ–¹æ³•èƒ½é¿å…åŸºäºæ–‡æœ¬å†…å®¹æå‰ç»ˆæ­¢ä»»åŠ¡ï¼Œå¹¶å¸®åŠ©å·¥å…·é¿å…åœ¨GUIæ›´æ”¹ä¸å½±å“ä»»åŠ¡å®Œæˆæ—¶å‡ºé”™ï¼Œå¦‚å­—ä½“å¤§å°æˆ–é¢œè‰²çš„è°ƒæ•´ã€‚åŒæ—¶ï¼Œè¯¥å¤šæ¨¡æ€æ–¹æ³•è¿˜èƒ½ç¡®ä¿å·¥å…·ä¸ä¼šè¶…å‡ºæœ€ç»ˆå±å¹•è¿›åº¦ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„æµ‹è¯•ã€‚è¯„ä¼°æ˜¾ç¤ºï¼ŒVisiDroidçš„å‡†ç¡®åº¦è¾¾åˆ°87.3%ï¼Œç›¸è¾ƒäºæœ€ä½³åŸºçº¿æœ‰23.5%çš„ç›¸å¯¹ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¯é‡æ’­çš„æµ‹è¯•è„šæœ¬çš„åŠ¨ä½œåºåˆ—åœ¨Android GUIæµ‹è¯•ä¸­å¾ˆå¸¸è§ã€‚</li>
<li>VisiDroidæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€ã€å¤šä»£ç†æ¡†æ¶ï¼Œç”¨äºAndroid GUIæµ‹è¯•ã€‚</li>
<li>VisiDroidé€šè¿‡ç»“åˆè§†è§‰å›¾åƒå’Œæ–‡æœ¬ï¼Œè¿­ä»£ç¡®å®šä¸‹ä¸€æ­¥åŠ¨ä½œå¹¶æ£€æµ‹ä»»åŠ¡çš„å®Œæˆåº¦ã€‚</li>
<li>å¤šæ¨¡æ€æ–¹æ³•å¯ä»¥é¿å…åŸºäºæ–‡æœ¬å†…å®¹æå‰ç»ˆæ­¢ä»»åŠ¡ï¼Œå¹¶å¸®åŠ©å·¥å…·æ›´å¥½åœ°é€‚åº”GUIæ›´æ”¹ä¸å½±å“ä»»åŠ¡å®Œæˆçš„æƒ…å†µã€‚</li>
<li>å¤šæ¨¡æ€æ–¹æ³•è¿˜èƒ½ç¡®ä¿æµ‹è¯•å·¥å…·ä¸ä¼šè¶…å‡ºæœ€ç»ˆå±å¹•è¿›åº¦ã€‚</li>
<li>VisiDroidçš„å‡†ç¡®åº¦è¾¾åˆ°87.3%ï¼Œç›¸è¾ƒäºæœ€ä½³åŸºçº¿æœ‰æ˜¾è‘—çš„ç›¸å¯¹ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15917">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33aa8b3e1fcd8cd2d35c241ef1db2259.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79383a1bf87d7c25bd4df932c67bcef5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2acd52735e9746561fce61293155a423.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9aee6926514be5fec0af0a4aec9f079b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f4bcbcdf58014bfd7952c0f8b254e74.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Automated-Bug-Report-Prioritization-in-Large-Open-Source-Projects"><a href="#Automated-Bug-Report-Prioritization-in-Large-Open-Source-Projects" class="headerlink" title="Automated Bug Report Prioritization in Large Open-Source Projects"></a>Automated Bug Report Prioritization in Large Open-Source Projects</h2><p><strong>Authors:Riley Pierson, Armin Moin</strong></p>
<p>Large open-source projects receive a large number of issues (known as bugs), including software defect (i.e., bug) reports and new feature requests from their user and developer communities at a fast rate. The often limited project resources do not allow them to deal with all issues. Instead, they have to prioritize them according to the projectâ€™s priorities and the issuesâ€™ severities. In this paper, we propose a novel approach to automated bug prioritization based on the natural language text of the bug reports that are stored in the open bug repositories of the issue-tracking systems. We conduct topic modeling using a variant of LDA called TopicMiner-MTM and text classification with the BERT large language model to achieve a higher performance level compared to the state-of-the-art. Experimental results using an existing reference dataset containing 85,156 bug reports of the Eclipse Platform project indicate that we outperform existing approaches in terms of Accuracy, Precision, Recall, and F1-measure of the bug report priority prediction. </p>
<blockquote>
<p>å¤§å‹å¼€æºé¡¹ç›®ä¼šå¿«é€Ÿæ¥æ”¶åˆ°å¤§é‡é—®é¢˜ï¼ˆä¹Ÿç§°ä¸ºé”™è¯¯ï¼‰ï¼ŒåŒ…æ‹¬æ¥è‡ªå…¶ç”¨æˆ·å’Œå¼€å‘ç¤¾åŒºçš„è½¯ä»¶ç¼ºé™·ï¼ˆå³é”™è¯¯ï¼‰æŠ¥å‘Šå’Œæ–°åŠŸèƒ½è¯·æ±‚ã€‚ç”±äºé¡¹ç›®èµ„æºé€šå¸¸æœ‰é™ï¼Œä»–ä»¬æ— æ³•å¤„ç†æ‰€æœ‰é—®é¢˜ã€‚ç›¸åï¼Œä»–ä»¬å¿…é¡»æ ¹æ®é¡¹ç›®çš„ä¼˜å…ˆçº§å’Œé—®é¢˜çš„ä¸¥é‡æ€§è¿›è¡Œä¼˜å…ˆæ’åºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå­˜å‚¨åœ¨é—®é¢˜è·Ÿè¸ªç³»ç»Ÿçš„å¼€æ”¾é”™è¯¯å­˜å‚¨åº“ä¸­çš„é”™è¯¯æŠ¥å‘Šçš„è‡ªç„¶è¯­è¨€æ–‡æœ¬è¿›è¡Œè‡ªåŠ¨é”™è¯¯ä¼˜å…ˆçº§æ’åºçš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ç§åä¸ºTopicMiner-MTMçš„LDAå˜ä½“è¿›è¡Œä¸»é¢˜å»ºæ¨¡ï¼Œå¹¶ä½¿ç”¨BERTå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ†ç±»ï¼Œä»¥å®ç°ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”çš„æ›´é«˜æ€§èƒ½æ°´å¹³ã€‚ä½¿ç”¨åŒ…å«Eclipseå¹³å°é¡¹ç›®85,156ä»½é”™è¯¯æŠ¥å‘Šç°æœ‰å‚è€ƒæ•°æ®é›†è¿›è¡Œçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬åœ¨å‡†ç¡®ç‡ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’Œé”™è¯¯æŠ¥å‘Šä¼˜å…ˆçº§é¢„æµ‹çš„F1å¾—åˆ†æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15912v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼€æºå¤§å‹é¡¹ç›®é¢ä¸´å¤§é‡é—®é¢˜æŠ¥å‘Šå’Œæ–°åŠŸèƒ½è¯·æ±‚ï¼Œæœ‰é™çš„èµ„æºä½¿å…¶æ— æ³•å¤„ç†æ‰€æœ‰é—®é¢˜ï¼Œéœ€ä¼˜å…ˆå¤„ç†é‡è¦é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè‡ªç„¶è¯­è¨€æ–‡æœ¬çš„è‡ªåŠ¨åŒ–bugä¼˜å…ˆçº§åˆ¤å®šæ–°æ–¹æ³•ï¼Œåˆ©ç”¨ä¸»é¢˜å»ºæ¨¡å’Œæ–‡æœ¬åˆ†ç±»æŠ€æœ¯å®ç°é«˜æ€§èƒ½é¢„æµ‹ã€‚åœ¨Eclipse Platformé¡¹ç›®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡†ç¡®æ€§ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹å¼€æºé¡¹ç›®é¢ä¸´å¤„ç†å¤§é‡é—®é¢˜çš„æŒ‘æˆ˜ã€‚</li>
<li>é¡¹ç›®éœ€æ ¹æ®ä¼˜å…ˆçº§å’Œé—®é¢˜çš„ä¸¥é‡æ€§è¿›è¡Œèµ„æºåˆ†é…ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè‡ªç„¶è¯­è¨€æ–‡æœ¬çš„è‡ªåŠ¨åŒ–bugä¼˜å…ˆçº§åˆ¤å®šæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿ç”¨ä¸»é¢˜å»ºæ¨¡å’Œæ–‡æœ¬åˆ†ç±»æŠ€æœ¯ã€‚</li>
<li>ä¸»é¢˜å»ºæ¨¡ä½¿ç”¨äº†LDAçš„å˜ç§â€”â€”TopicMiner-MTMã€‚</li>
<li>æ–‡æœ¬åˆ†ç±»ä½¿ç”¨äº†BERTå¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15912">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa4672c2ae9ac9f919d0b1261e934d6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bee04b125a439fdf91de3bdbca1d3c42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-944e8a6ea2e411ee8b9ca55b0843ab95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40adea847ccffdcb2b1d7b23a236cbf0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be4c37b8472a96440d156f32cfc9d5c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fea9389b6ffbe83ac78fa52b628edd2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SARI-Structured-Audio-Reasoning-via-Curriculum-Guided-Reinforcement-Learning"><a href="#SARI-Structured-Audio-Reasoning-via-Curriculum-Guided-Reinforcement-Learning" class="headerlink" title="SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement   Learning"></a>SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement   Learning</h2><p><strong>Authors:Cheng Wen, Tingwei Guo, Shuaijiang Zhao, Wei Zou, Xiangang Li</strong></p>
<p>Recent work shows that reinforcement learning(RL) can markedly sharpen the reasoning ability of large language models (LLMs) by prompting them to â€œthink before answering.â€ Yet whether and how these gains transfer to audio-language reasoning remains largely unexplored. We extend the Group-Relative Policy Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model (LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage regimen supervised fine-tuning on structured and unstructured chains-of-thought, followed by curriculum-guided GRPO, we systematically compare implicit vs. explicit, and structured vs. free form reasoning under identical architectures. Our structured audio reasoning model, SARI (Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a 16.35% improvement in average accuracy over the base model Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark. Ablation experiments show that on the base model we use: (i) SFT warm-up is important for stable RL training, (ii) structured chains yield more robust generalization than unstructured ones, and (iii) easy-to-hard curricula accelerate convergence and improve final performance. These findings demonstrate that explicit, structured reasoning and curriculum learning substantially enhances audio-language understanding. </p>
<blockquote>
<p>æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡æç¤ºâ€œå…ˆæ€è€ƒå†å›ç­”â€ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ”¶ç›Šæ˜¯å¦ä»¥åŠå¦‚ä½•è½¬ç§»åˆ°éŸ³é¢‘è¯­è¨€æ¨ç†ä¸Šä»åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚æˆ‘ä»¬å°†DeepSeek-R1ä¸­çš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¡†æ¶æ‰©å±•åˆ°ä¸€ä¸ªå¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«32kæ ·æœ¬çš„å¤šé¡¹é€‰æ‹©è¯­æ–™åº“ã€‚é€šè¿‡ç»“æ„åŒ–ä¸éç»“æ„åŒ–æ€ç»´é“¾çš„ä¸¤é˜¶æ®µç›‘ç®¡ç›‘ç£å¾®è°ƒï¼Œä»¥åŠè¯¾ç¨‹æŒ‡å¯¼çš„GRPOï¼Œæˆ‘ä»¬åœ¨ç›¸åŒæ¶æ„ä¸‹ç³»ç»Ÿåœ°æ¯”è¾ƒäº†éšå¼å’Œæ˜¾å¼ä»¥åŠç»“æ„åŒ–ä¸éç»“æ„åŒ–æ¨ç†ã€‚æˆ‘ä»¬çš„ç»“æ„åŒ–éŸ³é¢‘æ¨ç†æ¨¡å‹SARIï¼ˆé€šè¿‡è¯¾ç¨‹æŒ‡å¯¼å¼ºåŒ–å­¦ä¹ çš„ç»“æ„åŒ–éŸ³é¢‘æ¨ç†ï¼‰åœ¨åŸºå‡†æ¨¡å‹Qwen2-Audio-7B-Instructçš„åŸºç¡€ä¸Šå®ç°äº†å¹³å‡å‡†ç¡®ç‡æé«˜16.35%ã€‚æ­¤å¤–ï¼ŒåŸºäºQwen2.5-Omniçš„å˜ä½“åœ¨MMAUæµ‹è¯•å°å‹åŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä¸º67.08%ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼Œåœ¨æˆ‘ä»¬ä½¿ç”¨çš„åŸºå‡†æ¨¡å‹ä¸­ï¼šï¼ˆiï¼‰SFTé¢„çƒ­å¯¹äºç¨³å®šçš„RLè®­ç»ƒå¾ˆé‡è¦ï¼Œï¼ˆiiï¼‰ç»“æ„åŒ–é“¾æ¡ç›¸æ¯”äºéç»“æ„åŒ–é“¾æ¡èƒ½å¸¦æ¥æ›´ç¨³å¥çš„æ³›åŒ–æ€§èƒ½ï¼Œï¼ˆiiiï¼‰ä»æ˜“åˆ°éš¾çš„è¯¾ç¨‹èƒ½åŠ é€Ÿæ”¶æ•›å¹¶æé«˜æœ€ç»ˆæ€§èƒ½ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæ˜ç¡®çš„ã€ç»“æ„åŒ–çš„æ¨ç†å’Œè¯¾ç¨‹å­¦ä¹ æå¤§åœ°å¢å¼ºäº†éŸ³é¢‘è¯­è¨€ç†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15900v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½å¤Ÿæ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡æç¤ºå®ƒä»¬â€œåœ¨å›ç­”ä¹‹å‰æ€è€ƒâ€ã€‚ç„¶è€Œï¼Œè¿™äº›å¢ç›Šæ˜¯å¦ä»¥åŠå¦‚ä½•è½¬ç§»åˆ°éŸ³é¢‘è¯­è¨€æ¨ç†é¢†åŸŸä»é²œæœ‰ç ”ç©¶ã€‚æœ¬ç ”ç©¶å°†Group-Relative Policy Optimizationï¼ˆGRPOï¼‰æ¡†æ¶æ‰©å±•åˆ°å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«32kæ ·æœ¬çš„å¤šé¡¹é€‰æ‹©é¢˜è¯­æ–™åº“ã€‚é€šè¿‡ç»“æ„åŒ–ä¸éç»“æ„åŒ–æ€ç»´é“¾çš„ä¸¤é˜¶æ®µç›‘ç®¡å¾®è°ƒï¼Œä»¥åŠè¯¾ç¨‹æŒ‡å¯¼çš„GRPOï¼Œæœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¯”è¾ƒäº†éšå¼å’Œæ˜¾å¼ã€ç»“æ„åŒ–ä¸éç»“æ„åŒ–æ¨ç†åœ¨ç›¸åŒæ¶æ„ä¸‹çš„è¡¨ç°ã€‚ç»“æ„åŒ–éŸ³é¢‘æ¨ç†æ¨¡å‹SARIï¼ˆé€šè¿‡è¯¾ç¨‹æŒ‡å¯¼å¼ºåŒ–å­¦ä¹ çš„ç»“æ„åŒ–éŸ³é¢‘æ¨ç†ï¼‰åœ¨åŸºå‡†æ¨¡å‹Qwen2-Audio-7B-Instructçš„åŸºç¡€ä¸Šå®ç°äº†å¹³å‡ç²¾åº¦16.35%çš„æå‡ã€‚æ­¤å¤–ï¼ŒåŸºäºQwen2.5-Omniçš„å˜ä½“åœ¨MMAUæµ‹è¯•è¿·ä½ åŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä¸º67.08%ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æ‰€ä½¿ç”¨çš„åŸºå‡†æ¨¡å‹ï¼šï¼ˆiï¼‰SFTé¢„çƒ­å¯¹äºç¨³å®šçš„RLè®­ç»ƒå¾ˆé‡è¦ï¼Œï¼ˆiiï¼‰ç»“æ„åŒ–é“¾æ¡ç›¸æ¯”éç»“æ„åŒ–é“¾æ¡èƒ½å¸¦æ¥æ›´ç¨³å¥çš„æ³›åŒ–æ€§èƒ½ï¼Œï¼ˆiiiï¼‰ä»æ˜“åˆ°éš¾çš„è¯¾ç¨‹èƒ½åŠ é€Ÿæ”¶æ•›å¹¶æå‡æœ€ç»ˆæ€§èƒ½ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œæ˜¾å¼ã€ç»“æ„åŒ–çš„æ¨ç†å’Œè¯¾ç¨‹å­¦ä¹ æ˜¾è‘—æé«˜äº†éŸ³é¢‘è¯­è¨€ç†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶å°†GRPOæ¡†æ¶æ‰©å±•åˆ°éŸ³é¢‘è¯­è¨€æ¨¡å‹é¢†åŸŸã€‚</li>
<li>é€šè¿‡ç»“æ„åŒ–ä¸éç»“æ„åŒ–æ€ç»´é“¾çš„ä¸¤é˜¶æ®µç›‘ç®¡å¾®è°ƒä»¥åŠè¯¾ç¨‹æŒ‡å¯¼çš„GRPOè¿›è¡Œç³»ç»Ÿæ¯”è¾ƒã€‚</li>
<li>ç»“æ„åŒ–éŸ³é¢‘æ¨ç†æ¨¡å‹SARIå®ç°äº†ç›¸å¯¹äºåŸºå‡†æ¨¡å‹çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>åŸºäºQwen2.5-Omniçš„æ¨¡å‹åœ¨MMAUæµ‹è¯•ä¸Šè¾¾åˆ°å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>SFTé¢„çƒ­å¯¹ç¨³å®šRLè®­ç»ƒè‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9907d84410969cae5a0d9ef1edf48de7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9a4c222bf685456a159399fc337e9db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52a85868ad6ee7128cfd9498f7ebc10b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="A-LoRA-Based-Approach-to-Fine-Tuning-LLMs-for-Educational-Guidance-in-Resource-Constrained-Settings"><a href="#A-LoRA-Based-Approach-to-Fine-Tuning-LLMs-for-Educational-Guidance-in-Resource-Constrained-Settings" class="headerlink" title="A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in   Resource-Constrained Settings"></a>A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in   Resource-Constrained Settings</h2><p><strong>Authors:Md Millat, Md Motiur</strong></p>
<p>The current study describes a cost-effective method for adapting large language models (LLMs) for academic advising with study-abroad contexts in mind and for application in low-resource methods for acculturation. With the Mistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and a 4-bit quantization method, the model underwent training in two distinct stages related to this studyâ€™s purpose to enhance domain specificity while maintaining computational efficiency. In Phase 1, the model was conditioned with a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained with manually curated datasets from the StudyAbroadGPT project to achieve enhanced, contextualized responses. Technical innovations entailed memory-efficient quantization, parameter-efficient adaptation, and continuous training analytics via Weights &amp; Biases. After training, this study demonstrated a reduction in training loss by 52.7%, 92% accuracy in domain-specific recommendations, achieved 95% markdown-based formatting support, and a median run-rate of 100 samples per second on off-the-shelf GPU equipment. These findings support the effective application of instruction-tuned LLMs within educational advisers, especially in low-resource institutional scenarios. Limitations included decreased generalizability and the application of a synthetically generated dataset, but this framework is scalable for adding new multilingual-augmented and real-time academic advising processes. Future directions may include plans for the integration of retrieval-augmented generation, applying dynamic quantization routines, and connecting to real-time academic databases to increase adaptability and accuracy. </p>
<blockquote>
<p>å½“å‰ç ”ç©¶æè¿°äº†ä¸€ç§å…·æœ‰æˆæœ¬æ•ˆç›Šçš„æ–¹æ³•ï¼Œç”¨äºé€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä»¥å­¦æœ¯å’¨è¯¢ä¸ºå†…å®¹ï¼Œå¹¶è€ƒè™‘åˆ°å‡ºå›½ç•™å­¦èƒŒæ™¯ï¼Œä»¥åŠåº”ç”¨äºä½èµ„æºæ–¹æ³•çš„é€‚åº”æ–‡åŒ–é€‚åº”ã€‚é€šè¿‡åº”ç”¨Mistral-7B-Instructæ¨¡å‹å’ŒLow-Rank Adaptationï¼ˆLoRAï¼‰æ–¹æ³•ä¸4ä½é‡åŒ–æ–¹æ³•ï¼Œè¯¥æ¨¡å‹ç»è¿‡ä¸¤ä¸ªé˜¶æ®µçš„ç›¸å…³è®­ç»ƒï¼Œæ—¨åœ¨æé«˜é¢†åŸŸç‰¹å¼‚æ€§åŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œè¯¥æ¨¡å‹é€šè¿‡Gemini Pro APIä»¥åˆæˆæ•°æ®é›†è¿›è¡Œæ¡ä»¶å¤„ç†ï¼›åœ¨ç¬¬äºŒé˜¶æ®µï¼Œä½¿ç”¨StudyAbroadGPTé¡¹ç›®çš„æ‰‹åŠ¨æ•´ç†æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä»¥å®ç°å¢å¼ºæƒ…å¢ƒåŒ–çš„å“åº”ã€‚æŠ€æœ¯åˆ›æ–°åŒ…æ‹¬å†…å­˜é«˜æ•ˆçš„é‡åŒ–ã€å‚æ•°é«˜æ•ˆçš„é€‚åº”ä»¥åŠé€šè¿‡Weights &amp; Biasesçš„æŒç»­è®­ç»ƒåˆ†æã€‚è®­ç»ƒåï¼Œè¯¥ç ”ç©¶è¯æ˜äº†è®­ç»ƒæŸå¤±å‡å°‘äº†52.7%ï¼Œé¢†åŸŸç‰¹å®šå»ºè®®çš„å‡†ç¡®æ€§è¾¾åˆ°92%ï¼Œå®ç°äº†åŸºäºæ ‡è®°çš„æ ¼å¼åŒ–æ”¯æŒ95%ï¼Œå¹¶åœ¨ç°æˆçš„GPUè®¾å¤‡ä¸Šä»¥æ¯ç§’100ä¸ªæ ·æœ¬çš„ä¸­å€¼è¿è¡Œé€Ÿç‡ã€‚è¿™äº›å‘ç°æ”¯æŒåœ¨æ•™è‚²é¡¾é—®ä¸­æœ‰æ•ˆåº”ç”¨æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºçš„æœºæ„åœºæ™¯ä¸­ã€‚å±€é™æ€§åŒ…æ‹¬é€šç”¨æ€§é™ä½å’Œåº”ç”¨åˆæˆç”Ÿæˆçš„æ•°æ®é›†ï¼Œä½†è¿™ä¸ªæ¡†æ¶å¯ä»¥æ‰©å±•ï¼Œç”¨äºæ·»åŠ æ–°çš„å¤šè¯­è¨€å¢å¼ºå’Œå®æ—¶å­¦æœ¯å’¨è¯¢æµç¨‹ã€‚æœªæ¥æ–¹å‘å¯èƒ½åŒ…æ‹¬é›†æˆæ£€ç´¢å¢å¼ºç”Ÿæˆã€åº”ç”¨åŠ¨æ€é‡åŒ–ä¾‹è¡Œç¨‹åºä»¥åŠä¸å®æ—¶å­¦æœ¯æ•°æ®åº“è¿æ¥ï¼Œä»¥æé«˜é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15610v1">PDF</a> 18 pages, 6 figures (3 graphs + 3 flowchart&#x2F;architecture diagrams),   submitted as a preprint for review consideration in AI for Education or   Machine Learning applications in low-resource settings. Includes detailed   experiments with LoRA and quantization methods for efficient LLM fine-tuning</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æè¿°äº†ä¸€ç§å…·æœ‰æˆæœ¬æ•ˆç›Šçš„æ–¹æ³•ï¼Œç”¨äºé’ˆå¯¹ç•™å­¦è¾…å¯¼é¢†åŸŸè°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚é€šè¿‡åº”ç”¨Mistral-7B-Instructæ¨¡å‹å’ŒLoRAæ–¹æ³•ä»¥åŠ4ä½é‡åŒ–æ–¹æ³•ï¼Œæ¨¡å‹ç»å†äº†ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒï¼Œä»¥å¢å¼ºé¢†åŸŸç‰¹å¼‚æ€§åŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨åˆæˆæ•°æ®é›†é€šè¿‡Gemini Pro APIè¿›è¡Œæ¡ä»¶è®­ç»ƒï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨StudyAbroadGPTé¡¹ç›®çš„æ‰‹åŠ¨æ•´ç†æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä»¥å®ç°å¢å¼ºå’Œä¸Šä¸‹æ–‡åŒ–çš„å“åº”ã€‚æŠ€æœ¯åˆ›æ–°åŒ…æ‹¬å†…å­˜é«˜æ•ˆçš„é‡åŒ–ã€å‚æ•°æœ‰æ•ˆçš„é€‚åº”å’Œè¿ç»­è®­ç»ƒåˆ†æã€‚è®­ç»ƒåï¼Œè¯¥ç ”ç©¶è¡¨ç°å‡ºè®­ç»ƒæŸå¤±é™ä½52.7%ï¼Œé¢†åŸŸç‰¹å®šå»ºè®®å‡†ç¡®æ€§è¾¾92%ï¼Œæ”¯æŒ95%çš„markdownæ ¼å¼ï¼Œå¹¶åœ¨ç°è´§GPUè®¾å¤‡ä¸Šæ¯ç§’å¤„ç†100ä¸ªæ ·æœ¬ã€‚è¿™äº›å‘ç°æ”¯æŒåœ¨æ•™è‚²é¡¾é—®ä¸­æœ‰æ•ˆåº”ç”¨æŒ‡ä»¤è°ƒæ•´LLMï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºæœºæ„åœºæ™¯ä¸­ã€‚å°½ç®¡å­˜åœ¨é€šç”¨æ€§å’Œåˆæˆæ•°æ®é›†åº”ç”¨çš„å±€é™æ€§ï¼Œä½†è¯¥æ¡†æ¶å¯æ‰©å±•ï¼Œå¯åŠ å…¥æ–°çš„å¤šè¯­è¨€å¢å¼ºå’Œå®æ—¶å­¦æœ¯å’¨è¯¢æµç¨‹ã€‚æœªæ¥æ–¹å‘å¯èƒ½åŒ…æ‹¬é›†æˆæ£€ç´¢å¢å¼ºç”Ÿæˆã€åº”ç”¨åŠ¨æ€é‡åŒ–ç¨‹åºä»¥åŠä¸å®æ—¶å­¦æœ¯æ•°æ®åº“çš„è¿æ¥ï¼Œä»¥æé«˜é€‚åº”æ€§å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æè¿°äº†ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜çš„æ–¹æ³•ï¼Œç”¨äºé’ˆå¯¹ç•™å­¦è¾…å¯¼é¢†åŸŸè°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>é€šè¿‡ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒæ¨¡å‹ï¼Œå¢å¼ºé¢†åŸŸç‰¹å¼‚æ€§åŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚</li>
<li>ä½¿ç”¨åˆæˆæ•°æ®é›†å’Œæ‰‹åŠ¨æ•´ç†æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå®ç°å¢å¼ºå’Œä¸Šä¸‹æ–‡åŒ–çš„å“åº”ã€‚</li>
<li>æŠ€æœ¯åˆ›æ–°åŒ…æ‹¬å†…å­˜é«˜æ•ˆçš„é‡åŒ–ã€å‚æ•°æœ‰æ•ˆçš„é€‚åº”å’Œè¿ç»­è®­ç»ƒåˆ†æã€‚</li>
<li>è®­ç»ƒåè¡¨ç°å‡ºé«˜å‡†ç¡®ç‡å’Œæ€§èƒ½ï¼Œæ”¯æŒåœ¨æ•™è‚²é¡¾é—®ä¸­æœ‰æ•ˆåº”ç”¨æŒ‡ä»¤è°ƒæ•´LLMã€‚</li>
<li>æ¡†æ¶å…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¯åŠ å…¥æ–°çš„å¤šè¯­è¨€å¢å¼ºå’Œå®æ—¶å­¦æœ¯å’¨è¯¢æµç¨‹ã€‚</li>
<li>æœªæ¥å‘å±•æ–¹å‘åŒ…æ‹¬é›†æˆæ£€ç´¢å¢å¼ºç”Ÿæˆã€åº”ç”¨åŠ¨æ€é‡åŒ–ç¨‹åºä»¥åŠä¸å®æ—¶å­¦æœ¯æ•°æ®åº“çš„è¿æ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15610">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4726be7e0dbc7567128426abdd178691.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c243e013c216ef58bcd9c62d924be9d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e296a99c7d2ea16d3d2899461f6de4dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-accc151f344aa2ff467b89529383bfc7.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Instruction-Tuning-Data-Synthesis-from-Scratch-via-Web-Reconstruction"><a href="#Instruction-Tuning-Data-Synthesis-from-Scratch-via-Web-Reconstruction" class="headerlink" title="Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction"></a>Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction</h2><p><strong>Authors:Yuxin Jiang, Yufei Wang, Chuhan Wu, Xinyi Dai, Yan Xu, Weinan Gan, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, Wei Wang</strong></p>
<p>The improvement of LLMsâ€™ instruction-following capabilities depends critically on the availability of high-quality instruction-response pairs. While existing automatic data synthetic methods alleviate the burden of manual curation, they often rely heavily on either the quality of seed data or strong assumptions about the structure and content of web documents. To tackle these challenges, we propose Web Reconstruction (WebR), a fully automated framework for synthesizing high-quality instruction-tuning (IT) data directly from raw web documents with minimal assumptions. Leveraging the inherent diversity of raw web content, we conceptualize web reconstruction as an instruction-tuning data synthesis task via a novel dual-perspective paradigmâ€“Web as Instruction and Web as Responseâ€“where each web document is designated as either an instruction or a response to trigger the reconstruction process. Comprehensive experiments show that datasets generated by WebR outperform state-of-the-art baselines by up to 16.65% across four instruction-following benchmarks. Notably, WebR demonstrates superior compatibility, data efficiency, and scalability, enabling enhanced domain adaptation with minimal effort. The data and code are publicly available at <a target="_blank" rel="noopener" href="https://github.com/YJiangcm/WebR">https://github.com/YJiangcm/WebR</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ‰§è¡ŒæŒ‡ä»¤èƒ½åŠ›çš„æ”¹è¿›å…³é”®åœ¨äºé«˜è´¨é‡æŒ‡ä»¤å“åº”å¯¹çš„å¯ç”¨æ€§ã€‚è™½ç„¶ç°æœ‰çš„è‡ªåŠ¨æ•°æ®åˆæˆæ–¹æ³•å‡è½»äº†æ‰‹åŠ¨æ•´ç†æ•°æ®çš„è´Ÿæ‹…ï¼Œä½†å®ƒä»¬å¾€å¾€ä¸¥é‡ä¾èµ–äºç§å­æ•°æ®çš„è´¨é‡æˆ–å¯¹ç½‘é¡µæ–‡æ¡£ç»“æ„å’Œå†…å®¹çš„å‡è®¾ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Webé‡å»ºï¼ˆWebRï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿä»åŸå§‹ç½‘é¡µæ–‡æ¡£ä¸­ç›´æ¥åˆæˆé«˜è´¨é‡æŒ‡ä»¤å¾®è°ƒï¼ˆITï¼‰æ•°æ®ï¼Œå‡ ä¹æ— éœ€åšå‡ºå‡è®¾ã€‚åˆ©ç”¨åŸå§‹ç½‘é¡µå†…å®¹çš„å›ºæœ‰å¤šæ ·æ€§ï¼Œæˆ‘ä»¬å°†ç½‘é¡µé‡å»ºæ¦‚å¿µåŒ–ä¸ºæŒ‡ä»¤å¾®è°ƒæ•°æ®åˆæˆä»»åŠ¡ï¼Œé€šè¿‡æ–°é¢–çš„åŒè§†è§’æ¨¡å¼â€”â€”å°†ç½‘ç»œè§†ä¸ºæŒ‡ä»¤å’Œå°†ç½‘ç»œè§†ä¸ºå“åº”â€”â€”æ¯ä¸ªç½‘é¡µæ–‡æ¡£éƒ½è¢«æŒ‡å®šä¸ºæŒ‡ä»¤æˆ–å“åº”ä»¥è§¦å‘é‡å»ºè¿‡ç¨‹ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒWebRç”Ÿæˆçš„æ•°æ®é›†åœ¨å››é¡¹æ‰§è¡ŒæŒ‡ä»¤åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°åŸºçº¿ï¼Œæœ€é«˜æå‡äº†16.65%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒWebRå±•ç°å‡ºå“è¶Šçš„å…¼å®¹æ€§ã€æ•°æ®æ•ˆç‡å’Œå¯æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿå®ç°ä½æˆæœ¬çš„é¢†åŸŸè‡ªé€‚åº”ã€‚æ•°æ®å’Œä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/YJiangcm/WebR%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/YJiangcm/WebRä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15573v1">PDF</a> 15 pages, 11 figures, 9 tables</p>
<p><strong>Summary</strong></p>
<p>LLMsçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›æ”¹è¿›çš„å…³é”®åœ¨äºé«˜è´¨é‡æŒ‡ä»¤å“åº”å¯¹çš„å¯ç”¨æ€§ã€‚ç°æœ‰è‡ªåŠ¨æ•°æ®åˆæˆæ–¹æ³•è™½ç„¶å‡è½»äº†æ‰‹åŠ¨æ•´ç†è´Ÿæ‹…ï¼Œä½†ä»ä¾èµ–äºç§å­æ•°æ®è´¨é‡æˆ–å¯¹ç½‘é¡µæ–‡æ¡£ç»“æ„å’Œå†…å®¹çš„å¼ºçƒˆå‡è®¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºWebRï¼Œä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–çš„æ¡†æ¶ï¼Œç›´æ¥ä»åŸå§‹ç½‘é¡µæ–‡æ¡£ä¸­åˆæˆé«˜è´¨é‡çš„æŒ‡ä»¤è°ƒæ•´ï¼ˆITï¼‰æ•°æ®ï¼Œå‡è®¾æœ€å°‘ã€‚å€ŸåŠ©åŸå§‹ç½‘é¡µå†…å®¹çš„å†…åœ¨å¤šæ ·æ€§ï¼Œæˆ‘ä»¬å°†ç½‘é¡µé‡å»ºæ¦‚å¿µåŒ–ä¸ºæŒ‡ä»¤è°ƒæ•´æ•°æ®åˆæˆä»»åŠ¡ï¼Œé€šè¿‡æ–°é¢–çš„åŒè§†è§’æ¨¡å¼â€”â€”ç½‘é¡µä½œä¸ºæŒ‡ä»¤å’Œç½‘é¡µä½œä¸ºå“åº”â€”â€”æ¯ä¸ªç½‘é¡µæ–‡æ¡£éƒ½è¢«æŒ‡å®šä¸ºæŒ‡ä»¤æˆ–å“åº”ä»¥è§¦å‘é‡å»ºè¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒWebRç”Ÿæˆçš„æ•°æ®é›†åœ¨å››ä¸ªæŒ‡ä»¤éµå¾ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºæœ€ä½³åŸºçº¿ï¼Œé«˜å‡º16.65%ã€‚WebRå±•ç°å‡ºå“è¶Šçš„ä¸€è‡´æ€§ã€æ•°æ®æ•ˆç‡å’Œå¯æ‰©å±•æ€§ï¼Œèƒ½è½»æ¾åœ°å®ç°æœ€å°çš„åŠªåŠ›ä¸‹çš„é¢†åŸŸé€‚åº”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›æ”¹è¿›ä¾èµ–äºé«˜è´¨é‡æŒ‡ä»¤å“åº”å¯¹çš„å¯ç”¨æ€§ã€‚</li>
<li>ç°æœ‰è‡ªåŠ¨æ•°æ®åˆæˆæ–¹æ³•å­˜åœ¨å¯¹ç§å­æ•°æ®è´¨é‡å’Œç½‘é¡µæ–‡æ¡£ç»“æ„å†…å®¹çš„å‡è®¾ä¾èµ–é—®é¢˜ã€‚</li>
<li>WebRæ˜¯ä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–çš„æ¡†æ¶ï¼Œèƒ½ä»åŸå§‹ç½‘é¡µæ–‡æ¡£ä¸­åˆæˆé«˜è´¨é‡çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®ï¼Œå‡è®¾æœ€å°‘ã€‚</li>
<li>WebRåˆ©ç”¨åŒè§†è§’æ¨¡å¼å¤„ç†ç½‘é¡µæ–‡æ¡£ï¼Œåˆ†åˆ«ä½œä¸ºæŒ‡ä»¤å’Œå“åº”ã€‚</li>
<li>WebRç”Ÿæˆçš„æ•°æ®é›†åœ¨å››ä¸ªæŒ‡ä»¤éµå¾ªåŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºæœ€ä½³åŸºçº¿ã€‚</li>
<li>WebRå…·æœ‰å“è¶Šçš„ä¸€è‡´æ€§ã€æ•°æ®æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15573">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ce709269a839ccbad07580acff9675a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32c7e3f1818793be1552b8586766c76d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-717b518733059218efd7ba560fbb6e7e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-330cc88266b2f6728a350daa25b9534b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b242c5a86175517cab8d0ec3a9d69461.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Optimizing-SLO-oriented-LLM-Serving-with-PD-Multiplexing"><a href="#Optimizing-SLO-oriented-LLM-Serving-with-PD-Multiplexing" class="headerlink" title="Optimizing SLO-oriented LLM Serving with PD-Multiplexing"></a>Optimizing SLO-oriented LLM Serving with PD-Multiplexing</h2><p><strong>Authors:Weihao Cui, Yukang Chen, Han Zhao, Ziyi Xu, Quan Chen, Xusheng Chen, Yangjie Zhou, Shixuan Sun, Minyi Guo</strong></p>
<p>Modern LLM services demand high throughput and stringent SLO guarantees across two distinct inference phases-prefill and decode-and complex multi-turn workflows. However, current systems face a fundamental tradeoff: out-of-place compute partition enables per-phase SLO attainment, while in-place memory sharing maximizes throughput via KV cache reuse. Moreover, existing in-place compute partition also encounters low utilization and high overhead due to phase-coupling design. We present Drift, a new LLM serving framework that resolves this tension via PD multiplexing, enabling in-place and phase-decoupled compute partition. Drift leverages low-level GPU partitioning techniques to multiplex prefill and decode phases spatially and adaptively on shared GPUs, while preserving in-place memory sharing. To fully leverage the multiplexing capability, Drift introduces an adaptive gang scheduling mechanism, a contention-free modeling method, and a SLO-aware dispatching policy. Evaluation shows that Drift achieves an average $5.1\times$ throughput improvement (up to $17.5\times$) over state-of-the-art baselines, while consistently meeting SLO targets under complex LLM workloads. </p>
<blockquote>
<p>ç°ä»£LLMæœåŠ¡è¦æ±‚åœ¨ä¸¤ä¸ªä¸åŒçš„æ¨ç†é˜¶æ®µï¼ˆé¢„å¡«å……å’Œè§£ç ï¼‰ä»¥åŠå¤æ‚çš„å¤šè½®å·¥ä½œæµç¨‹ä¸­å®ç°é«˜ååé‡å’Œä¸¥æ ¼çš„SLOä¿è¯ã€‚ç„¶è€Œï¼Œå½“å‰ç³»ç»Ÿé¢ä¸´ä¸€ä¸ªåŸºæœ¬æƒè¡¡ï¼šç¦»ä½è®¡ç®—åˆ†åŒºå¯å®ç°æ¯ä¸ªé˜¶æ®µçš„SLOè¾¾æˆï¼Œè€ŒåŸä½å†…å­˜å…±äº«åˆ™é€šè¿‡KVç¼“å­˜é‡ç”¨æœ€å¤§åŒ–ååé‡ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„åŸä½è®¡ç®—åˆ†åŒºè¿˜å› ä¸ºé˜¶æ®µè€¦åˆè®¾è®¡è€Œé¢ä¸´åˆ©ç”¨ç‡ä½å’Œå¼€é”€é«˜çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†Driftï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„LLMæœåŠ¡æ¡†æ¶ï¼Œå®ƒé€šè¿‡PDå¤šè·¯å¤ç”¨è§£å†³è¿™ä¸€ç´§å¼ é—®é¢˜ï¼Œå®ç°åŸä½å’Œé˜¶æ®µè§£è€¦çš„è®¡ç®—åˆ†åŒºã€‚Driftåˆ©ç”¨ä½çº§GPUåˆ†åŒºæŠ€æœ¯ï¼Œåœ¨å…±äº«GPUä¸Šç©ºé—´å’Œè‡ªé€‚åº”åœ°å¤šè·¯å¤ç”¨é¢„å¡«å……å’Œè§£ç é˜¶æ®µï¼ŒåŒæ—¶ä¿ç•™åŸä½å†…å­˜å…±äº«ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨å¤šè·¯å¤ç”¨åŠŸèƒ½ï¼ŒDriftå¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”å°ç»„è°ƒåº¦æœºåˆ¶ã€ä¸€ç§æ— äº‰ç”¨å»ºæ¨¡æ–¹æ³•å’Œä¸€ç§SLOæ„ŸçŸ¥è°ƒåº¦ç­–ç•¥ã€‚è¯„ä¼°è¡¨æ˜ï¼Œä¸æœ€æ–°åŸºçº¿ç›¸æ¯”ï¼ŒDriftå¹³å‡æé«˜äº†5.1å€ååé‡ï¼ˆæœ€é«˜è¾¾17.5å€ï¼‰ï¼ŒåŒæ—¶åœ¨å¤æ‚LLMå·¥ä½œè´Ÿè½½ä¸‹å§‹ç»ˆè¾¾åˆ°SLOç›®æ ‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14489v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä¸»è¦æè¿°äº†ä¸€ç§åä¸ºDriftçš„æ–°LLMæœåŠ¡æ¡†æ¶ï¼Œå®ƒè§£å†³äº†ç°æœ‰ç³»ç»Ÿé¢ä¸´çš„è®¡ç®—å’Œå†…å­˜åˆ©ç”¨ç‡çš„æƒè¡¡é—®é¢˜ã€‚é€šè¿‡PDå¤ç”¨æŠ€æœ¯ï¼ŒDriftèƒ½åœ¨å…±äº«GPUä¸Šå®ç°åŸä½å’Œé˜¶æ®µè§£è€¦çš„è®¡ç®—åˆ†åŒºï¼ŒåŒæ—¶ä¿æŒåŸä½å†…å­˜å…±äº«ã€‚Driftè¿˜å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”çš„è°ƒåº¦æœºåˆ¶ã€æ— äº‰ç”¨çš„å»ºæ¨¡æ–¹æ³•å’ŒSLOæ„ŸçŸ¥è°ƒåº¦ç­–ç•¥ï¼Œä»¥å……åˆ†åˆ©ç”¨å…¶å¤ç”¨èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒDriftç›¸è¾ƒäºç°æœ‰æŠ€æœ¯å¹³å‡æé«˜äº†5.1å€çš„ååé‡ï¼ˆæœ€é«˜å¯è¾¾17.5å€ï¼‰ï¼ŒåŒæ—¶åœ¨å¤æ‚çš„LLMå·¥ä½œè´Ÿè½½ä¸‹å§‹ç»ˆæ»¡è¶³SLOç›®æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£LLMæœåŠ¡éœ€è¦é«˜ååé‡å’Œä¸¥æ ¼çš„SLOä¿è¯ï¼Œè·¨è¶Šä¸¤ä¸ªç‹¬ç‰¹çš„æ¨ç†é˜¶æ®µâ€”â€”é¢„å¡«å……å’Œè§£ç ï¼Œä»¥åŠå¤æ‚çš„å¤šè½®å¯¹è¯å·¥ä½œæµç¨‹ã€‚</li>
<li>å½“å‰ç³»ç»Ÿé¢ä¸´è®¡ç®—å’Œå†…å­˜åˆ©ç”¨ç‡çš„æƒè¡¡é—®é¢˜ï¼šå‡ºä½è®¡ç®—åˆ†åŒºå¯å®ç°æ¯é˜¶æ®µSLOè¾¾æˆï¼Œè€ŒåŸä½å†…å­˜å…±äº«åˆ™é€šè¿‡KVç¼“å­˜é‡ç”¨æœ€å¤§åŒ–ååé‡ã€‚</li>
<li>Driftæ˜¯ä¸€ä¸ªæ–°çš„LLMæœåŠ¡æ¡†æ¶ï¼Œé€šè¿‡PDå¤ç”¨æŠ€æœ¯è§£å†³è¿™ä¸€æƒè¡¡é—®é¢˜ï¼Œèƒ½åœ¨å…±äº«GPUä¸Šå®ç°åŸä½å’Œé˜¶æ®µè§£è€¦çš„è®¡ç®—åˆ†åŒºã€‚</li>
<li>Driftä¿æŒåŸä½å†…å­˜å…±äº«ï¼ŒåŒæ—¶å¼•å…¥è‡ªé€‚åº”çš„è°ƒåº¦æœºåˆ¶ã€æ— äº‰ç”¨çš„å»ºæ¨¡æ–¹æ³•å’ŒSLOæ„ŸçŸ¥è°ƒåº¦ç­–ç•¥ã€‚</li>
<li>Driftå®ç°äº†å¯¹å…ˆè¿›åŸºçº¿æŠ€æœ¯çš„å¹³å‡5.1å€ååé‡æ”¹è¿›ï¼ˆæœ€é«˜å¯è¾¾17.5å€ï¼‰ã€‚</li>
<li>Driftèƒ½åœ¨å¤æ‚çš„LLMå·¥ä½œè´Ÿè½½ä¸‹å§‹ç»ˆæ»¡è¶³SLOç›®æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14489">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a91c41266761b99f6dc1f9d046f06059.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26726fef9362860e9e7d03a8edbb37d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a01c131b55f0cf9473c972dc1b28385.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af34ff4e150f1ab1f1ca9ddaa1afb50c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-621039a84a02cc7681ea68bb6d01dd9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ead1aa444666e86d612f0650cd86717e.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ASIDE-Architectural-Separation-of-Instructions-and-Data-in-Language-Models"><a href="#ASIDE-Architectural-Separation-of-Instructions-and-Data-in-Language-Models" class="headerlink" title="ASIDE: Architectural Separation of Instructions and Data in Language   Models"></a>ASIDE: Architectural Separation of Instructions and Data in Language   Models</h2><p><strong>Authors:Egor Zverev, Evgenii Kortukov, Alexander Panfilov, Alexandra Volkova, Soroush Tabesh, Sebastian Lapuschkin, Wojciech Samek, Christoph H. Lampert</strong></p>
<p>Despite their remarkable performance, large language models lack elementary safety features, and this makes them susceptible to numerous malicious attacks. In particular, previous work has identified the absence of an intrinsic separation between instructions and data as a root cause for the success of prompt injection attacks. In this work, we propose a method, ASIDE, that allows the model to clearly separate between instructions and data on the level of embeddings. ASIDE applies a fixed orthogonal rotation to the embeddings of data tokens, thus creating distinct representations of instructions and data tokens without introducing any additional parameters. We demonstrate the effectiveness of our method by instruct-tuning LLMs with ASIDE and showing (1) highly increased instruction-data separation scores without a loss in model capabilities and (2) competitive results on prompt injection benchmarks, even without dedicated safety training. Additionally, we study the working mechanism behind our method through an analysis of model representations. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ç¼ºä¹åŸºæœ¬çš„å®‰å…¨åŠŸèƒ½ï¼Œè¿™ä½¿å¾—å®ƒä»¬å®¹æ˜“å—åˆ°ä¼—å¤šæ¶æ„æ”»å‡»çš„å½±å“ã€‚ç‰¹åˆ«æ˜¯ï¼Œå…ˆå‰çš„å·¥ä½œå·²ç»ç¡®å®šæŒ‡ä»¤å’Œæ•°æ®ä¹‹é—´ç¼ºä¹å†…åœ¨åˆ†ç¦»æ˜¯æç¤ºæ³¨å…¥æ”»å‡»æˆåŠŸçš„æ ¹æœ¬åŸå› ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ASIDEï¼Œå®ƒå…è®¸æ¨¡å‹åœ¨åµŒå…¥çº§åˆ«ä¸Šæ¸…æ™°åœ°åˆ†ç¦»æŒ‡ä»¤å’Œæ•°æ®ã€‚ASIDEé€šè¿‡å¯¹æ•°æ®æ ‡è®°çš„åµŒå…¥è¿›è¡Œå›ºå®šçš„æ­£äº¤æ—‹è½¬ï¼Œä»è€Œåˆ›å»ºæŒ‡ä»¤å’Œæ•°æ®æ ‡è®°çš„æ˜ç¡®è¡¨ç¤ºï¼Œè€Œæ— éœ€å¼•å…¥ä»»ä½•é¢å¤–çš„å‚æ•°ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ASIDEå¯¹LLMè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œå¹¶å±•ç¤ºï¼ˆ1ï¼‰åœ¨æ¨¡å‹èƒ½åŠ›æ²¡æœ‰æŸå¤±çš„æƒ…å†µä¸‹é«˜åº¦æé«˜æŒ‡ä»¤ä¸æ•°æ®çš„åˆ†ç¦»åº¦å¾—åˆ†ï¼›ï¼ˆ2ï¼‰å³ä½¿åœ¨æœªè¿›è¡Œä¸“é—¨çš„å®‰å…¨è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œåœ¨æç¤ºæ³¨å…¥åŸºå‡†æµ‹è¯•ä¸­ä¹Ÿå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡åˆ†ææ¨¡å‹è¡¨ç¤ºæ¥ç ”ç©¶æˆ‘ä»¬æ–¹æ³•çš„å·¥ä½œåŸç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10566v2">PDF</a> ICLR 2025 Workshop on Building Trust in Language Models and   Applications</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹è™½ç„¶è¡¨ç°ä¼˜å¼‚ï¼Œä½†ç¼ºä¹åŸºæœ¬çš„å®‰å…¨ç‰¹æ€§ï¼Œå®¹æ˜“é­å—æ¶æ„æ”»å‡»ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–¹æ³•â€”â€”ASIDEï¼Œé€šè¿‡åœ¨åµŒå…¥å±‚é¢å®ç°æŒ‡ä»¤ä¸æ•°æ®çš„æ¸…æ™°åˆ†ç¦»ï¼Œå¢å¼ºæ¨¡å‹çš„å®‰å…¨æ€§ã€‚ASIDEé€šè¿‡å¯¹æ•°æ®æ ‡è®°çš„åµŒå…¥è¿›è¡Œå›ºå®šæ­£äº¤æ—‹è½¬ï¼Œä¸ºæŒ‡ä»¤å’Œæ•°æ®æ ‡è®°åˆ›å»ºä¸åŒçš„è¡¨ç¤ºå½¢å¼ï¼Œä¸”æ— éœ€å¼•å…¥ä»»ä½•é¢å¤–å‚æ•°ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ASIDEè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¿æŒæ¨¡å‹èƒ½åŠ›çš„åŒæ—¶ï¼Œæé«˜äº†æŒ‡ä»¤ä¸æ•°æ®çš„åˆ†ç¦»åº¦ï¼Œå¹¶åœ¨æç¤ºæ³¨å…¥åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œå³ä½¿æœªè¿›è¡Œä¸“é—¨çš„å®‰å…¨è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ç¼ºä¹åŸºæœ¬å®‰å…¨ç‰¹æ€§ï¼Œæ˜“å—åˆ°æ¶æ„æ”»å‡»ã€‚</li>
<li>æŒ‡ä»¤ä¸æ•°æ®æ··æ·†æ˜¯è¯­è¨€æ¨¡å‹æ˜“å—æ”»å‡»çš„åŸå› ä¹‹ä¸€ã€‚</li>
<li>ASIDEæ–¹æ³•é€šè¿‡åµŒå…¥å±‚é¢çš„æ“ä½œå®ç°æŒ‡ä»¤ä¸æ•°æ®çš„æ¸…æ™°åˆ†ç¦»ã€‚</li>
<li>ASIDEæ–¹æ³•é€šè¿‡å›ºå®šæ­£äº¤æ—‹è½¬æ•°æ®æ ‡è®°çš„åµŒå…¥æ¥åˆ›å»ºä¸åŒçš„æŒ‡ä»¤å’Œæ•°æ®è¡¨ç¤ºã€‚</li>
<li>ä½¿ç”¨ASIDEè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¿æŒæ¨¡å‹èƒ½åŠ›çš„åŒæ—¶æé«˜äº†æŒ‡ä»¤ä¸æ•°æ®çš„åˆ†ç¦»åº¦ã€‚</li>
<li>åœ¨æœªè¿›è¡Œä¸“é—¨çš„å®‰å…¨è®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒASIDEæ–¹æ³•åœ¨æç¤ºæ³¨å…¥åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d0aaf3ea7730ebc63d8a97b69680e4ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6f45b560e638aeef486cd28889f6623.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89bb12951c50e92f9b22a916afc35730.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Plan-and-Act-Improving-Planning-of-Agents-for-Long-Horizon-Tasks"><a href="#Plan-and-Act-Improving-Planning-of-Agents-for-Long-Horizon-Tasks" class="headerlink" title="Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks"></a>Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks</h2><p><strong>Authors:Lutfi Eren Erdogan, Nicholas Lee, Sehoon Kim, Suhong Moon, Hiroki Furuta, Gopala Anumanchipalli, Kurt Keutzer, Amir Gholami</strong></p>
<p>Large language models (LLMs) have shown remarkable advancements in enabling language agents to tackle simple tasks. However, applying them for complex, multi-step, long-horizon tasks remains a challenge. Recent work have found success by separating high-level planning from low-level execution, which enables the model to effectively balance high-level planning objectives and low-level execution details. However, generating accurate plans remains difficult since LLMs are not inherently trained for this task. To address this, we propose Plan-and-Act, a novel framework that incorporates explicit planning into LLM-based agents and introduces a scalable method to enhance plan generation through a novel synthetic data generation method. Plan-and-Act consists of a Planner model which generates structured, high-level plans to achieve user goals, and an Executor model that translates these plans into environment-specific actions. To train the Planner effectively, we introduce a synthetic data generation method that annotates ground-truth trajectories with feasible plans, augmented with diverse and extensive examples to enhance generalization. We evaluate Plan-and-Act using web navigation as a representative long-horizon planning environment, demonstrating a state-of-the-art 57.58% success rate on the WebArena-Lite benchmark as well as a text-only state-of-the-art 81.36% success rate on WebVoyager. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯­è¨€æ™ºèƒ½å¤„ç†ç®€å•ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå°†å…¶åº”ç”¨äºå¤æ‚ã€å¤šæ­¥éª¤ã€é•¿æœŸçš„ä»»åŠ¡ä»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è¿‘æœŸçš„ç ”ç©¶æˆåŠŸå°†é«˜çº§è§„åˆ’ä¸ä½çº§æ‰§è¡Œåˆ†ç¦»ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å¹³è¡¡é«˜çº§è§„åˆ’ç›®æ ‡å’Œä½çº§æ‰§è¡Œç»†èŠ‚ã€‚ç„¶è€Œï¼Œç”Ÿæˆå‡†ç¡®çš„è®¡åˆ’ä»ç„¶å¾ˆå›°éš¾ï¼Œå› ä¸ºLLMå¹¶æ²¡æœ‰å¤©ç”Ÿä¸ºæ­¤ä»»åŠ¡è€Œè®­ç»ƒã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œè®¡åˆ’ä¸æ‰§è¡Œâ€è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†æ˜ç¡®çš„è®¡åˆ’çº³å…¥åŸºäºLLMçš„ä»£ç†ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§å¯ä¼¸ç¼©çš„æ–¹æ³•ï¼Œé€šè¿‡ä¸€ç§æ–°çš„åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•æ¥å¢å¼ºè®¡åˆ’ç”Ÿæˆã€‚â€œè®¡åˆ’ä¸æ‰§è¡Œâ€åŒ…æ‹¬ä¸€ä¸ªè§„åˆ’å™¨æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆç»“æ„åŒ–ã€é«˜çº§çš„è®¡åˆ’æ¥å®ç°ç”¨æˆ·ç›®æ ‡ï¼Œä»¥åŠä¸€ä¸ªæ‰§è¡Œå™¨æ¨¡å‹ï¼Œç”¨äºå°†è¿™äº›è®¡åˆ’ç¿»è¯‘æˆç¯å¢ƒç‰¹å®šçš„è¡ŒåŠ¨ã€‚ä¸ºäº†æœ‰æ•ˆåœ°è®­ç»ƒè§„åˆ’å™¨ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç”¨å¯è¡Œçš„è®¡åˆ’æ ‡æ³¨äº†çœŸå®è½¨è¿¹çš„åœ°é¢å®å†µï¼Œå¹¶è¾…ä»¥å¤šæ ·åŒ–å’Œå¹¿æ³›çš„ä¾‹å­æ¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨ç½‘é¡µå¯¼èˆªä½œä¸ºä»£è¡¨é•¿æœŸè§„åˆ’ç¯å¢ƒçš„è¯„ä¼°å¯¹è±¡ï¼Œåœ¨WebArena-LiteåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„57.58%æˆåŠŸç‡ï¼Œä»¥åŠåœ¨WebVoyagerä¸Šå–å¾—äº†çº¯æ–‡æœ¬çš„æœ€å…ˆè¿›81.36%æˆåŠŸç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09572v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†ç®€å•ä»»åŠ¡æ—¶è¡¨ç°å‡ºæ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é¢å¯¹å¤æ‚ã€å¤šæ­¥éª¤ã€é•¿æœŸè§„åˆ’çš„ä»»åŠ¡æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…é‡‡ç”¨åˆ†ç¦»é«˜çº§è§„åˆ’ä¸ä½çº§æ‰§è¡Œçš„æ–¹æ³•ï¼Œå¹¶åœ¨LLMæ¨¡å‹ä¸­å¼•å…¥æ˜ç¡®çš„è§„åˆ’ã€‚ç„¶è€Œï¼Œç”Ÿæˆå‡†ç¡®è§„åˆ’ä»ç„¶å›°éš¾ï¼Œå› ä¸ºLLMå¹¶æœªå¤©ç”Ÿä¸ºæ­¤ä»»åŠ¡è€Œè®­ç»ƒã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºPlan-and-Actæ¡†æ¶ï¼Œé€šè¿‡æ–°å‹åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•å¼ºåŒ–è§„åˆ’ç”Ÿæˆã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ç”Ÿæˆç»“æ„åŒ–é«˜çº§è§„åˆ’çš„Planneræ¨¡å‹å’Œå°†è§„åˆ’è½¬åŒ–ä¸ºç¯å¢ƒç‰¹å®šè¡ŒåŠ¨çš„Executoræ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒPlan-and-Actåœ¨Webå¯¼èˆªç­‰é•¿æœŸè§„åˆ’ç¯å¢ƒä¸­è¡¨ç°å“è¶Šï¼Œåœ¨WebArena-LiteåŸºå‡†æµ‹è¯•ä¸Šå–å¾—é¢†å…ˆçš„æˆåŠŸç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤„ç†å¤æ‚ã€å¤šæ­¥éª¤ã€é•¿æœŸè§„åˆ’çš„ä»»åŠ¡æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>åˆ†ç¦»é«˜çº§è§„åˆ’ä¸ä½çº§æ‰§è¡Œæœ‰åŠ©äºLLMæ¨¡å‹æœ‰æ•ˆå¹³è¡¡è§„åˆ’ä¸æ‰§è¡Œã€‚</li>
<li>ç”Ÿæˆå‡†ç¡®è§„åˆ’æ˜¯LLMé¢ä¸´çš„ä¸€ä¸ªéš¾é¢˜ï¼Œéœ€è¦æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>Plan-and-Actæ¡†æ¶å¼•å…¥æ˜ç¡®çš„è§„åˆ’ï¼ŒåŒ…æ‹¬Plannerå’ŒExecutorä¸¤ä¸ªæ¨¡å‹ã€‚</li>
<li>Plan-and-Acté€šè¿‡æ–°å‹åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•å¼ºåŒ–è®¡åˆ’ç”Ÿæˆã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒPlan-and-Actåœ¨Webå¯¼èˆªç­‰é•¿æœŸè§„åˆ’ç¯å¢ƒä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09572">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab4a19c78bae25a983a4005ef3a66d7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d00488a3407e3735f346a7db13e9dc3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4048614979f12343768c9bea9c9d86c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Digital-Twin-Buildings-3D-Modeling-GIS-Integration-and-Visual-Descriptions-Using-Gaussian-Splatting-ChatGPT-Deepseek-and-Google-Maps-Platform"><a href="#Digital-Twin-Buildings-3D-Modeling-GIS-Integration-and-Visual-Descriptions-Using-Gaussian-Splatting-ChatGPT-Deepseek-and-Google-Maps-Platform" class="headerlink" title="Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual   Descriptions Using Gaussian Splatting, ChatGPT&#x2F;Deepseek, and Google Maps   Platform"></a>Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual   Descriptions Using Gaussian Splatting, ChatGPT&#x2F;Deepseek, and Google Maps   Platform</h2><p><strong>Authors:Kyle Gao, Dening Lu, Liangzhi Li, Nan Chen, Hongjie He, Linlin Xu, Jonathan Li</strong></p>
<p>Urban digital twins are virtual replicas of cities that use multi-source data and data analytics to optimize urban planning, infrastructure management, and decision-making. Towards this, we propose a framework focused on the single-building scale. By connecting to cloud mapping platforms such as Google Map Platforms APIs, by leveraging state-of-the-art multi-agent Large Language Models data analysis using ChatGPT(4o) and Deepseek-V3&#x2F;R1, and by using our Gaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings framework can retrieve a buildingâ€™s 3D model, visual descriptions, and achieve cloud-based mapping integration with large language model-based data analytics using a buildingâ€™s address, postal code, or geographic coordinates. </p>
<blockquote>
<p>åŸå¸‚æ•°å­—åŒèƒèƒæ˜¯åˆ©ç”¨å¤šæºæ•°æ®å’Œæ•°æ®åˆ†æä¼˜åŒ–åŸå¸‚è§„åˆ’ã€åŸºç¡€è®¾æ–½ç®¡ç†å’Œå†³ç­–åˆ¶å®šçš„åŸå¸‚è™šæ‹Ÿå‰¯æœ¬ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä»¥å•æ ‹å»ºç­‘è§„æ¨¡ä¸ºé‡ç‚¹çš„æ¡†æ¶ã€‚é€šè¿‡è¿æ¥åˆ°è°·æ­Œåœ°å›¾å¹³å°APIç­‰äº‘åœ°å›¾å¹³å°ï¼Œåˆ©ç”¨æœ€å…ˆè¿›çš„åŸºäºå¤šæ™ºèƒ½ä½“çš„è¯­è¨€æ¨¡å‹ChatGPTï¼ˆç¬¬4ç‰ˆï¼‰å’ŒDeepseek-V3&#x2F;R1è¿›è¡Œæ•°æ®åˆ†æï¼Œä»¥åŠä½¿ç”¨åŸºäºé«˜æ–¯æº…ç‚¹æŠ€æœ¯çš„ç½‘æ ¼æå–ç®¡é“ï¼Œæˆ‘ä»¬çš„æ•°å­—åŒèƒèƒå»ºç­‘æ¡†æ¶å¯ä»¥æ£€ç´¢å»ºç­‘çš„3Dæ¨¡å‹ã€è§†è§‰æè¿°ä¿¡æ¯ï¼Œå¹¶é€šè¿‡å»ºç­‘åœ°å€ã€é‚®æ”¿ç¼–ç æˆ–åœ°ç†åæ ‡å®ç°åŸºäºäº‘çš„åœ°å›¾é›†æˆä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°æ®åˆ†æã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05769v3">PDF</a> -Fixed minor typo</p>
<p><strong>Summary</strong><br>åŸå¸‚æ•°å­—åŒèƒèƒæ˜¯åŸå¸‚çš„è™šæ‹Ÿå‰¯æœ¬ï¼Œåˆ©ç”¨å¤šæºæ•°æ®å’Œæ•°æ®åˆ†æä¼˜åŒ–åŸå¸‚è§„åˆ’ã€åŸºç¡€è®¾æ–½ç®¡ç†å’Œå†³ç­–åˆ¶å®šã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªä»¥å•æ ‹å»ºç­‘ä¸ºå°ºåº¦çš„æ¡†æ¶ï¼Œé€šè¿‡è¿æ¥äº‘å¹³å°ã€åˆ©ç”¨æœ€æ–°çš„å¤šæ™ºèƒ½ä½“å¤§æ•°æ®æ¨¡å‹å’Œç®—æ³•ï¼Œå®ç°åŸå¸‚æ•°å­—åŒèƒèƒçš„æ„å»ºï¼Œå¯è·å–å»ºç­‘çš„3Dæ¨¡å‹ã€è§†è§‰æè¿°ï¼Œå¹¶å®ç°åŸºäºäº‘çš„æ˜ å°„é›†æˆä¸å¤§æ•°æ®æ¨¡å‹åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸå¸‚æ•°å­—åŒèƒèƒæ˜¯åŸå¸‚çš„è™šæ‹Ÿå‰¯æœ¬ï¼Œæ—¨åœ¨ä¼˜åŒ–åŸå¸‚è§„åˆ’ã€åŸºç¡€è®¾æ–½ç®¡ç†å’Œå†³ç­–åˆ¶å®šã€‚</li>
<li>æ¡†æ¶ä»¥å•æ ‹å»ºç­‘ä¸ºå°ºåº¦ï¼Œå®ç°æ›´ç²¾ç»†åŒ–çš„åŸå¸‚æ•°å­—åŒèƒèƒæ„å»ºã€‚</li>
<li>é€šè¿‡è¿æ¥äº‘å¹³å°å¦‚Google Map Platforms APIsï¼Œè·å–å»ºç­‘çš„å¤šæºæ•°æ®ã€‚</li>
<li>åˆ©ç”¨æœ€æ–°çš„å¤šæ™ºèƒ½ä½“å¤§æ•°æ®æ¨¡å‹å’Œç®—æ³•ï¼Œå¦‚ChatGPTå’ŒDeepseek-V3&#x2F;R1ï¼Œè¿›è¡Œæ•°æ®åˆ†æã€‚</li>
<li>é‡‡ç”¨Gaussian Splatting-based meshæå–ç®¡é“æŠ€æœ¯ï¼Œè·å–å»ºç­‘çš„3Dæ¨¡å‹å’Œè§†è§‰æè¿°ã€‚</li>
<li>å®ç°åŸºäºäº‘çš„æ˜ å°„é›†æˆï¼Œå¯é€šè¿‡å»ºç­‘åœ°å€ã€é‚®æ”¿ç¼–ç æˆ–åœ°ç†åæ ‡è¿›è¡Œæ•°æ®åˆ†æå’Œè®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05769">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-62dacfbdeefb0f9fc0dd3a79766f2a5a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5ecc6427fbe8bec581b6686f010decd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-12b2f95e8e829eb8767ae6fb471e782f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3d2b3b35ffaaa212c18e54dab7e23cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b6a47cec1f153dfa2ff4d795c8b069c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d5743d9c232a5d49f6fef2e78990f76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6763660a6a78dec49ec6480d4cd9dd1f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="LongProc-Benchmarking-Long-Context-Language-Models-on-Long-Procedural-Generation"><a href="#LongProc-Benchmarking-Long-Context-Language-Models-on-Long-Procedural-Generation" class="headerlink" title="LongProc: Benchmarking Long-Context Language Models on Long Procedural   Generation"></a>LongProc: Benchmarking Long-Context Language Models on Long Procedural   Generation</h2><p><strong>Authors:Xi Ye, Fangcong Yin, Yinghui He, Joie Zhang, Howard Yen, Tianyu Gao, Greg Durrett, Danqi Chen</strong></p>
<p>Existing benchmarks for evaluating long-context language models (LCLMs) primarily focus on long-context recall, requiring models to produce short responses based on a few critical snippets while processing thousands of irrelevant tokens. We introduce LongProc (Long Procedural Generation), a new benchmark that requires both the integration of highly dispersed information and long-form generation. LongProc consists of six diverse procedural generation tasks, such as extracting structured information from HTML pages into a TSV format and executing complex search procedures to create travel plans. These tasks challenge LCLMs by testing their ability to follow detailed procedural instructions, synthesize and reason over dispersed information, and generate structured, long-form outputs (up to 8K tokens). Furthermore, as these tasks adhere to deterministic procedures and yield structured outputs, they enable reliable rule-based evaluation. We evaluated 23 LCLMs, including instruction-tuned models and recent reasoning models, on LongProc at three difficulty levels, with the maximum number of output tokens set at 500, 2K, and 8K. Notably, while all tested models claim a context window size above 32K tokens, open-weight models typically falter on 2K-token tasks, and closed-source models like GPT-4o show significant degradation on 8K-token tasks. Reasoning models achieve stronger overall performance in long-form generation, benefiting from long CoT training. Further analysis reveals that LCLMs struggle to maintain long-range coherence in long-form generations. These findings highlight critical limitations in current LCLMs and suggest substantial room for improvement. Data and code available at: <a target="_blank" rel="noopener" href="https://princeton-pli.github.io/LongProc">https://princeton-pli.github.io/LongProc</a>. </p>
<blockquote>
<p>ç›®å‰è¯„ä¼°é•¿è¯­å¢ƒè¯­è¨€æ¨¡å‹ï¼ˆLCLMï¼‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦ä¾§é‡äºé•¿è¯­å¢ƒå›å¿†ï¼Œè¦æ±‚æ¨¡å‹åœ¨å¤„ç†æˆåƒä¸Šä¸‡çš„æ— å…³æ ‡è®°æ—¶ï¼ŒåŸºäºå‡ ä¸ªå…³é”®ç‰‡æ®µäº§ç”ŸçŸ­å›å¤ã€‚æˆ‘ä»¬å¼•å…¥äº†LongProcï¼ˆé•¿ç¨‹åºç”Ÿæˆï¼‰è¿™ä¸€æ–°åŸºå‡†æµ‹è¯•ï¼Œå®ƒè¦æ±‚æ—¢æ•´åˆé«˜åº¦åˆ†æ•£çš„ä¿¡æ¯ï¼Œåˆè¿›è¡Œé•¿æ–‡æœ¬ç”Ÿæˆã€‚LongProcåŒ…å«å…­ä¸ªä¸åŒçš„ç¨‹åºç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚ä»HTMLé¡µé¢æå–ç»“æ„åŒ–ä¿¡æ¯å¹¶è½¬æ¢ä¸ºTSVæ ¼å¼ï¼Œä»¥åŠæ‰§è¡Œå¤æ‚çš„æœç´¢ç¨‹åºä»¥åˆ›å»ºæ—…è¡Œè®¡åˆ’ã€‚è¿™äº›ä»»åŠ¡é€šè¿‡æµ‹è¯•LCLMéµå¾ªè¯¦ç»†ç¨‹åºæŒ‡ä»¤çš„èƒ½åŠ›ã€åˆæˆå’Œæ¨ç†åˆ†æ•£ä¿¡æ¯çš„èƒ½åŠ›ï¼Œä»¥åŠç”Ÿæˆç»“æ„åŒ–é•¿æ–‡æœ¬è¾“å‡ºï¼ˆæœ€å¤šè¾¾8Kæ ‡è®°ï¼‰çš„èƒ½åŠ›æ¥æŒ‘æˆ˜LCLMã€‚æ­¤å¤–ï¼Œç”±äºè¿™äº›ä»»åŠ¡éµå¾ªç¡®å®šæ€§ç¨‹åºå¹¶äº§ç”Ÿç»“æ„åŒ–è¾“å‡ºï¼Œå› æ­¤å®ƒä»¬èƒ½å¤Ÿè¿›è¡Œå¯é çš„åŸºäºè§„åˆ™çš„è¯„ä»·ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªéš¾åº¦çº§åˆ«ä¸Šå¯¹23ä¸ªLCLMè¿›è¡Œäº†LongProcè¯„ä¼°ï¼ŒåŒ…æ‹¬æŒ‡ä»¤è°ƒæ•´æ¨¡å‹å’Œæœ€æ–°çš„æ¨ç†æ¨¡å‹ï¼Œæœ€å¤§è¾“å‡ºæ ‡è®°æ•°è®¾ä¸º500ã€2Kå’Œ8Kã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡æ‰€æœ‰æµ‹è¯•æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°éƒ½è¶…è¿‡32Kæ ‡è®°ï¼Œä½†å¼€æ”¾æƒé‡æ¨¡å‹é€šå¸¸åœ¨2Kæ ‡è®°ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œè€ŒåƒGPT-4oè¿™æ ·çš„é—­æºæ¨¡å‹åœ¨8Kæ ‡è®°ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—é€€åŒ–ã€‚æ¨ç†æ¨¡å‹åœ¨é•¿æ–‡æœ¬ç”Ÿæˆæ–¹é¢æ•´ä½“è¡¨ç°æ›´å¼ºï¼Œå¾—ç›Šäºé•¿æœŸä¸Šä¸‹æ–‡è®­ç»ƒã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒLCLMåœ¨ç»´æŒé•¿æ–‡æœ¬ç”Ÿæˆä¸­çš„é•¿æœŸè¿è´¯æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚è¿™äº›å‘ç°çªå‡ºäº†å½“å‰LCLMçš„å…³é”®å±€é™æ€§ï¼Œå¹¶è¡¨æ˜å­˜åœ¨å¤§é‡æ”¹è¿›ç©ºé—´ã€‚æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://princeton-pli.github.io/LongProc%E8%AE%BF%E9%97%AE%E3%80%82">https://princeton-pli.github.io/LongProcè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05414v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹é•¿è¯­å¢ƒè¯­è¨€æ¨¡å‹ï¼ˆLCLMï¼‰çš„æ–°åŸºå‡†æµ‹è¯•LongProcã€‚è¯¥æµ‹è¯•åŒ…å«ä¸€ç³»åˆ—ç¨‹åºç”Ÿæˆä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹åœ¨æ•´åˆé«˜åº¦åˆ†æ•£ä¿¡æ¯çš„åŒæ—¶è¿›è¡Œé•¿æ–‡æœ¬ç”Ÿæˆã€‚ä»»åŠ¡æŒ‘æˆ˜åœ¨äºæµ‹è¯•æ¨¡å‹éµå¾ªè¯¦ç»†ç¨‹åºæŒ‡ä»¤ã€åˆæˆå’Œæ¨ç†åˆ†æ•£ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå¹¶ç”Ÿæˆç»“æ„åŒ–ã€é•¿å½¢å¼çš„è¾“å‡ºã€‚å¯¹ç°æœ‰æ¨¡å‹çš„è¯„ä¼°å‘ç°ï¼Œå°½ç®¡æ¨¡å‹å£°ç§°èƒ½å¤Ÿå¤„ç†è¶…è¿‡32Kæ ‡è®°çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œä½†åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ä»å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LongProcæ˜¯ä¸€ä¸ªæ–°çš„é•¿è¯­å¢ƒè¯­è¨€æ¨¡å‹ï¼ˆLCLMï¼‰åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬ä¸€ç³»åˆ—ç¨‹åºç”Ÿæˆä»»åŠ¡ï¼Œå¼ºè°ƒé•¿æ–‡æœ¬ç”Ÿæˆçš„èƒ½åŠ›ã€‚</li>
<li>LCLMsè¢«è¦æ±‚åœ¨æ•´åˆé«˜åº¦åˆ†æ•£ä¿¡æ¯çš„åŒæ—¶è¿›è¡Œç”Ÿæˆï¼Œè¿™è¦æ±‚æ¨¡å‹å…·æœ‰è¾ƒå¼ºçš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>æµ‹è¯•çš„ä»»åŠ¡éµå¾ªç¡®å®šæ€§ç¨‹åºå¹¶äº§ç”Ÿç»“æ„åŒ–è¾“å‡ºï¼Œä»è€Œå¯ä»¥è¿›è¡Œå¯é çš„è§„åˆ™åŸºç¡€è¯„ä¼°ã€‚</li>
<li>åœ¨LongProcæµ‹è¯•ä¸­è¯„ä¼°äº†23ä¸ªLCLMæ¨¡å‹ï¼Œå‘ç°å³ä½¿åœ¨å¤„ç†å¤§é‡ä¸Šä¸‹æ–‡ä¿¡æ¯æ—¶ï¼Œè¿™äº›æ¨¡å‹åœ¨é•¿æ–‡æœ¬ç”Ÿæˆæ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¼€æ”¾æƒé‡æ¨¡å‹é€šå¸¸åœ¨2Kæ ‡è®°ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œè€Œå°é—­æºæ¨¡å‹å¦‚GPT-4oåœ¨8Kæ ‡è®°ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—é€€åŒ–ã€‚</li>
<li>æ¨ç†æ¨¡å‹åœ¨é•¿æ–‡æœ¬ç”Ÿæˆæ–¹é¢è¡¨ç°è¾ƒå¥½ï¼Œå¾—ç›Šäºå…¶é•¿æœŸè®­ç»ƒçš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05414">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4ce28306b47a1dbffbb2bbf4146a3eb1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4a38315911c4a91d92aa1ad4a03a35d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2d898e6f0c0ac5cc49aa0982bd4b3ac7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6440a9afa5c04fdaf8703baefdc84a0d.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="AI-Predicts-AGI-Leveraging-AGI-Forecasting-and-Peer-Review-to-Explore-LLMsâ€™-Complex-Reasoning-Capabilities"><a href="#AI-Predicts-AGI-Leveraging-AGI-Forecasting-and-Peer-Review-to-Explore-LLMsâ€™-Complex-Reasoning-Capabilities" class="headerlink" title="AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore   LLMsâ€™ Complex Reasoning Capabilities"></a>AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore   LLMsâ€™ Complex Reasoning Capabilities</h2><p><strong>Authors:Fabrizio Davide, Pietro Torre, Leonardo Ercolani, Andrea Gaggioli</strong></p>
<p>We tasked 16 state-of-the-art large language models (LLMs) with estimating the likelihood of Artificial General Intelligence (AGI) emerging by 2030. To assess the quality of these forecasts, we implemented an automated peer review process (LLM-PR). The LLMsâ€™ estimates varied widely, ranging from 3% (Reka- Core) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align with a recent expert survey that projected a 10% likelihood of AGI by 2027, underscoring the relevance of LLMs in forecasting complex, speculative scenarios. The LLM-PR process demonstrated strong reliability, evidenced by a high Intraclass Correlation Coefficient (ICC &#x3D; 0.79), reflecting notable consistency in scoring across the models. Among the models, Pplx-70b-online emerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A cross-comparison with external benchmarks, such as LMSYS Chatbot Arena, revealed that LLM rankings remained consistent across different evaluation methods, suggesting that existing benchmarks may not encapsulate some of the skills relevant for AGI prediction. We further explored the use of weighting schemes based on external benchmarks, optimizing the alignment of LLMsâ€™ predictions with human expert forecasts. This analysis led to the development of a new, â€˜AGI benchmarkâ€™ designed to highlight performance differences in AGI-related tasks. Our findings offer insights into LLMsâ€™ capabilities in speculative, interdisciplinary forecasting tasks and emphasize the growing need for innovative evaluation frameworks for assessing AI performance in complex, uncertain real-world scenarios. </p>
<blockquote>
<p>æˆ‘ä»¬è®©16ä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢„æµ‹åˆ°2030å¹´å‡ºç°é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„å¯èƒ½æ€§ã€‚ä¸ºäº†è¯„ä¼°è¿™äº›é¢„æµ‹çš„è´¨é‡ï¼Œæˆ‘ä»¬å®æ–½äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„åŒè¡Œè¯„å®¡è¿‡ç¨‹ï¼ˆLLM-PRï¼‰ã€‚è¿™äº›å¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„æµ‹ç»“æœå·®å¼‚å¾ˆå¤§ï¼Œä»Reka-Coreé¢„æµ‹çš„3%åˆ°GPT-4oé¢„æµ‹çš„47.6%ï¼Œä¸­ä½æ•°ä¸º12.5%ã€‚è¿™äº›é¢„æµ‹ç»“æœä¸æœ€è¿‘çš„ä¸€é¡¹ä¸“å®¶è°ƒæŸ¥ç›¸å»åˆï¼Œè¯¥è°ƒæŸ¥é¢„æµ‹åˆ°2027å¹´AGIçš„å‡ºç°æ¦‚ç‡ä¸º10%ï¼Œè¿™å¼ºè°ƒäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢„æµ‹å¤æ‚å’ŒæŠ•æœºåœºæ™¯ä¸­çš„é‡è¦æ€§ã€‚LLM-PRè¿‡ç¨‹è¡¨ç°å‡ºå¾ˆå¼ºçš„å¯é æ€§ï¼ŒåŒè´¨æ€§ç›¸å…³ç³»æ•°ï¼ˆICC&#x3D;0.79ï¼‰å¾ˆé«˜ï¼Œè¿™è¡¨æ˜å„æ¨¡å‹ä¹‹é—´çš„è¯„åˆ†å­˜åœ¨æ˜¾è‘—çš„ä¸€è‡´æ€§ã€‚åœ¨æ¨¡å‹ä¸­ï¼ŒPplx-70b-onlineè¡¨ç°æœ€å¥½ï¼Œè€ŒGemini-1.5-pro-apiæ’åæœ€ä½ã€‚ä¸å¤–éƒ¨åŸºå‡†æµ‹è¯•ï¼ˆå¦‚LMSYSèŠå¤©æœºå™¨äººç«æŠ€åœºï¼‰çš„äº¤å‰å¯¹æ¯”æ˜¾ç¤ºï¼Œä¸åŒè¯„ä¼°æ–¹æ³•ä¸‹çš„å¤§å‹è¯­è¨€æ¨¡å‹æ’åä¿æŒä¸€è‡´ï¼Œè¿™è¡¨æ˜ç°æœ‰åŸºå‡†æµ‹è¯•å¯èƒ½æ²¡æœ‰æ¶µç›–ä¸AGIé¢„æµ‹ç›¸å…³çš„ä¸€äº›æŠ€èƒ½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†åŸºäºå¤–éƒ¨åŸºå‡†æµ‹è¯•ä½¿ç”¨æƒé‡æ–¹æ¡ˆçš„æ–¹æ³•ï¼Œä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹é¢„æµ‹ä¸äººç±»ä¸“å®¶é¢„æµ‹çš„å¯¹é½ã€‚è¿™ä¸€åˆ†æå‚¬ç”Ÿäº†ä¸€ä¸ªæ–°çš„â€œAGIåŸºå‡†æµ‹è¯•â€ï¼Œæ—¨åœ¨çªå‡ºAGIç›¸å…³ä»»åŠ¡ä¸­çš„æ€§èƒ½å·®å¼‚ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ´å¯Ÿäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æŠ•æœºæ€§ã€è·¨å­¦ç§‘é¢„æµ‹ä»»åŠ¡ä¸­çš„èƒ½åŠ›ï¼Œå¹¶å¼ºè°ƒäº†è¯„ä¼°äººå·¥æ™ºèƒ½åœ¨å¤æ‚ã€ä¸ç¡®å®šçš„ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„æ€§èƒ½æ—¶ï¼Œå¯¹åˆ›æ–°è¯„ä¼°æ¡†æ¶çš„æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09385v2">PDF</a> 47 pages, 8 figures, 17 tables, appendix with data and code</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡è¯„ä¼°äº†16æ¬¾å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹2030å¹´å‰å‡ºç°é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰å¯èƒ½æ€§çš„é¢„æµ‹èƒ½åŠ›ã€‚é€šè¿‡å®æ–½è‡ªåŠ¨åŒ–åŒè¡Œè¯„å®¡æµç¨‹ï¼ˆLLM-PRï¼‰ï¼Œå‘ç°å„æ¨¡å‹çš„é¢„æµ‹ä¼°è®¡å·®å¼‚è¾ƒå¤§ï¼Œä»Reka-Coreçš„3%åˆ°GPT-4oçš„47.6%ï¼Œä¸­ä½æ•°ä¸º12.5%ã€‚è¿™äº›é¢„æµ‹ä¸æœ€è¿‘ä¸“å®¶å¯¹2027å¹´å‰AGIå‡ºç°å¯èƒ½æ€§çš„10%é¢„æµ‹ç›¸å»åˆï¼Œçªæ˜¾äº†LLMåœ¨é¢„æµ‹å¤æ‚å‡è®¾æƒ…å¢ƒæ–¹é¢çš„ä»·å€¼ã€‚LLM-PRæµç¨‹å¯é æ€§é«˜ï¼Œè¯„åˆ†ä¸€è‡´æ€§æ˜¾è‘—ï¼Œå…¶ä¸­Pplx-70b-onlineè¡¨ç°æœ€ä½³ï¼ŒGemini-1.5-pro-apiè¡¨ç°æœ€å·®ã€‚ä¸å¤–éƒ¨åŸºå‡†æµ‹è¯•ï¼ˆå¦‚LMSYSèŠå¤©æœºå™¨äººç«æŠ€åœºï¼‰çš„äº¤å‰æ¯”è¾ƒæ˜¾ç¤ºï¼ŒLLMæ’ååœ¨ä¸åŒè¯„ä¼°æ–¹æ³•ä¸‹ä¿æŒä¸€è‡´ï¼Œè¡¨æ˜ç°æœ‰åŸºå‡†æµ‹è¯•å¯èƒ½æ— æ³•æ¶µç›–ä¸AGIé¢„æµ‹ç›¸å…³çš„éƒ¨åˆ†æŠ€èƒ½ã€‚æœ¬æ–‡è¿˜æ¢è®¨äº†åŸºäºå¤–éƒ¨åŸºå‡†çš„æƒé‡æ–¹æ¡ˆï¼Œä¼˜åŒ–LLMsé¢„æµ‹ä¸äººç±»ä¸“å®¶é¢„æµ‹çš„å¯¹é½ã€‚åˆ†æåå¼€å‘äº†ä¸€ä¸ªå…¨æ–°çš„â€œAGIåŸºå‡†æµ‹è¯•â€ï¼Œæ—¨åœ¨çªæ˜¾åœ¨AGIç›¸å…³ä»»åŠ¡ä¸­çš„æ€§èƒ½å·®å¼‚ã€‚æœ¬ç ”ç©¶ç»“æœæ·±å…¥äº†è§£LLMsåœ¨å‡è®¾æ€§è·¨å­¦ç§‘é¢„æµ‹ä»»åŠ¡ä¸­çš„èƒ½åŠ›ï¼Œå¹¶å¼ºè°ƒäº†åœ¨å¤æ‚ä¸ç¡®å®šçš„ç°å®ä¸–ç•Œåœºæ™¯ä¸­è¯„ä¼°äººå·¥æ™ºèƒ½æ€§èƒ½æ—¶ï¼Œå¯¹åˆ›æ–°è¯„ä¼°æ¡†æ¶çš„æ—¥ç›Šéœ€æ±‚ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>16æ¬¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹äººå·¥é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰åˆ°2030å¹´çš„å‡ºç°æ—¶é—´è¿›è¡Œäº†é¢„æµ‹ã€‚</li>
<li>LLMsçš„é¢„æµ‹ä¼°è®¡èŒƒå›´ä»3%åˆ°47.6%ï¼Œä¸­ä½æ•°ä¸º12.5%ï¼Œæ˜¾ç¤ºå‡ºäº†é¢„æµ‹ä¹‹é—´çš„å·¨å¤§å·®å¼‚ã€‚</li>
<li>LLMsçš„é¢„æµ‹ä¸ä¸“å®¶å¯¹AGIåœ¨ä¸ä¹…çš„å°†æ¥å‡ºç°çš„é¢„æµ‹ç›¸ç¬¦ï¼Œçªå‡ºäº†å®ƒä»¬åœ¨é¢„æµ‹å¤æ‚å‡è®¾æƒ…å¢ƒæ–¹é¢çš„ä»·å€¼ã€‚</li>
<li>LLM-PRæµç¨‹æ˜¾ç¤ºå‡ºå¼ºå¤§çš„å¯é æ€§ï¼Œè¯„åˆ†ä¸€è‡´æ€§é«˜ã€‚</li>
<li>åœ¨æ‰€æœ‰è¯„ä¼°çš„LLMsä¸­ï¼ŒPplx-70b-onlineè¡¨ç°æœ€ä½³ï¼Œè€ŒGemini-1.5-pro-apiè¡¨ç°æœ€å·®ã€‚</li>
<li>ä¸å¤–éƒ¨åŸºå‡†æµ‹è¯•çš„æ¯”è¾ƒè¡¨æ˜ï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•å¯èƒ½æ— æ³•å…¨é¢è¯„ä¼°ä¸AGIé¢„æµ‹ç›¸å…³çš„æŠ€èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c72517dc4bf07d3fba4677f0e696184.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a8fa4380fa3e8a49ecf06582ae71e1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7ab0607b5862ea794f905214b0b1b31.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Towards-Unifying-Evaluation-of-Counterfactual-Explanations-Leveraging-Large-Language-Models-for-Human-Centric-Assessments"><a href="#Towards-Unifying-Evaluation-of-Counterfactual-Explanations-Leveraging-Large-Language-Models-for-Human-Centric-Assessments" class="headerlink" title="Towards Unifying Evaluation of Counterfactual Explanations: Leveraging   Large Language Models for Human-Centric Assessments"></a>Towards Unifying Evaluation of Counterfactual Explanations: Leveraging   Large Language Models for Human-Centric Assessments</h2><p><strong>Authors:Marharyta Domnich, Julius VÃ¤lja, Rasmus Moorits Veski, Giacomo Magnifico, Kadi Tulver, Eduard Barbu, Raul Vicente</strong></p>
<p>As machine learning models evolve, maintaining transparency demands more human-centric explainable AI techniques. Counterfactual explanations, with roots in human reasoning, identify the minimal input changes needed to obtain a given output and, hence, are crucial for supporting decision-making. Despite their importance, the evaluation of these explanations often lacks grounding in user studies and remains fragmented, with existing metrics not fully capturing human perspectives. To address this challenge, we developed a diverse set of 30 counterfactual scenarios and collected ratings across 8 evaluation metrics from 206 respondents. Subsequently, we fine-tuned different Large Language Models (LLMs) to predict average or individual human judgment across these metrics. Our methodology allowed LLMs to achieve an accuracy of up to 63% in zero-shot evaluations and 85% (over a 3-classes prediction) with fine-tuning across all metrics. The fine-tuned models predicting human ratings offer better comparability and scalability in evaluating different counterfactual explanation frameworks. </p>
<blockquote>
<p>éšç€æœºå™¨å­¦ä¹ æ¨¡å‹çš„å‘å±•ï¼Œä¿æŒé€æ˜åº¦éœ€è¦æ›´å¤šä»¥äººä¸ºä¸­å¿ƒçš„å¯è§£é‡Šäººå·¥æ™ºèƒ½æŠ€æœ¯ã€‚å½’å› äºäººç±»æ¨ç†çš„å› æœè§£é‡Šï¼Œèƒ½å¤Ÿè¯†åˆ«è·å¾—ç»™å®šè¾“å‡ºæ‰€éœ€çš„æœ€å°è¾“å…¥å˜åŒ–ï¼Œå› æ­¤ï¼Œå¯¹äºæ”¯æŒå†³ç­–åˆ¶å®šè‡³å…³é‡è¦ã€‚å°½ç®¡å› æœè§£é‡Šå¾ˆé‡è¦ï¼Œä½†å…¶è¯„ä¼°å¾€å¾€ç¼ºä¹ç”¨æˆ·ç ”ç©¶çš„ä¾æ®ï¼Œå¹¶ä¸”ä»ç„¶å‘ˆç°ç¢ç‰‡åŒ–ï¼Œç°æœ‰æŒ‡æ ‡æœªèƒ½å®Œå…¨æ•æ‰äººç±»è§†è§’ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€å¥—30ç§ä¸åŒçš„å› æœåœºæ™¯ï¼Œå¹¶ä»206åå—è®¿è€…ä¸­æ”¶é›†äº†8é¡¹è¯„ä¼°æŒ‡æ ‡çš„è¯„åˆ†ã€‚éšåï¼Œæˆ‘ä»¬å¯¹ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œäº†å¾®è°ƒï¼Œä»¥é¢„æµ‹è¿™äº›æŒ‡æ ‡çš„å¹³å‡å€¼æˆ–ä¸ªäººåˆ¤æ–­ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬è¯„ä¼°ä¸­è¾¾åˆ°é«˜è¾¾63%çš„å‡†ç¡®ç‡ï¼Œåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šè¿›è¡Œå¾®è°ƒåè¾¾åˆ°85%ï¼ˆé’ˆå¯¹ä¸‰ç±»é¢„æµ‹ï¼‰ã€‚ç»è¿‡å¾®è°ƒå¯é¢„æµ‹äººç±»è¯„åˆ†çš„æ¨¡å‹åœ¨è¯„ä¼°ä¸åŒçš„å› æœè§£é‡Šæ¡†æ¶æ—¶æä¾›äº†æ›´å¥½çš„å¯æ¯”æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.21131v3">PDF</a> This paper extends the AAAI-2025 version by including the Appendix</p>
<p><strong>Summary</strong></p>
<p>éšç€æœºå™¨å­¦ä¹ æ¨¡å‹çš„å‘å±•ï¼Œä¿æŒé€æ˜åº¦éœ€è¦æ›´å¤šä»¥äººä¸ºä¸­å¿ƒçš„è§£é‡Šæ€§äººå·¥æ™ºèƒ½æŠ€æœ¯ã€‚æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨æ ¹æ¤äºäººç±»æ¨ç†ä¸­çš„åäº‹å®è§£é‡Šï¼ˆcounterfactual explanationsï¼‰çš„é‡è¦æ€§ã€‚å°½ç®¡è¿™äº›è§£é‡Šå¾ˆå…³é”®ï¼Œä½†è¯„ä¼°æ—¶å´ç»å¸¸ç¼ºä¹ç”¨æˆ·ç ”ç©¶çš„ä¾æ®ï¼Œå¹¶ä¸”è¯„ä¼°æ–¹æ³•ç¢ç‰‡åŒ–ä¸¥é‡ï¼Œç°æœ‰æŒ‡æ ‡æœªèƒ½å……åˆ†æ•æ‰äººç±»è§†è§’ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€å¥—å¤šå…ƒåŒ–çš„åäº‹å®åœºæ™¯ï¼Œæ”¶é›†äº†å¯¹å…«ä¸ªè¯„ä»·æŒ‡æ ‡çš„è¯„çº§æ•°æ®ï¼Œå¹¶å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¾®è°ƒä»¥é¢„æµ‹äººç±»å¯¹æ­¤çš„å¹³å‡è¯„ä»·æˆ–ä¸ªåˆ«è¯„ä»·ã€‚é€šè¿‡æ­¤ç ”ç©¶æ–¹æ³•ï¼ŒLLMåœ¨é›¶æ ·æœ¬è¯„ä¼°ä¸­çš„å‡†ç¡®ç‡è¾¾åˆ°äº†æœ€é«˜ä¸º63%ï¼Œåœ¨æ‰€æœ‰è¯„ä»·æŒ‡æ ‡ä¸­ç»è¿‡å¾®è°ƒåè¾¾åˆ°æœ€é«˜ä¸º85%ï¼ˆä¸‰ç±»é¢„æµ‹ï¼‰ã€‚è¿™äº›ç»è¿‡è®­ç»ƒçš„æ¨¡å‹é¢„æµ‹äººç±»è¯„çº§æä¾›äº†æ›´å¥½çš„å¯æ¯”æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä»¥è¯„ä¼°ä¸åŒçš„åäº‹å®è§£é‡Šæ¡†æ¶ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬è®ºæ–‡æ­ç¤ºäº†è¯„ä¼°åäº‹å®è§£é‡Šçš„æ–°æ–¹æ³•åŠå…¶åœ¨æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„åº”ç”¨å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšç€æœºå™¨å­¦ä¹ æ¨¡å‹çš„å‘å±•ï¼Œä¿æŒé€æ˜åº¦å˜å¾—è‡³å…³é‡è¦ï¼Œéœ€è¦æ›´ä»¥äººä¸ºä¸­å¿ƒçš„è§£é‡Šæ€§äººå·¥æ™ºèƒ½æŠ€æœ¯çš„æ”¯æŒã€‚</li>
<li>åäº‹å®è§£é‡Šåœ¨æœºå™¨å­¦ä¹ æ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œèƒ½å¤Ÿè¯†åˆ«è·å¾—ç‰¹å®šè¾“å‡ºæ‰€éœ€çš„æœ€å°è¾“å…¥å˜åŒ–ã€‚</li>
<li>å½“å‰åäº‹å®è§£é‡Šçš„è¯„ä¼°æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œç¼ºä¹ç”¨æˆ·ç ”ç©¶çš„ä¾æ®ï¼Œç°æœ‰æŒ‡æ ‡æ— æ³•å…¨é¢åæ˜ äººç±»è§†è§’å’Œéœ€æ±‚ã€‚</li>
<li>é€šè¿‡å¼€å‘å¤šå…ƒåŒ–çš„åäº‹å®åœºæ™¯å’Œæ”¶é›†è¯„ä»·æ•°æ®ï¼Œå¯ä»¥æ›´å¥½åœ°è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿé¢„æµ‹äººç±»å¯¹è¯„ä»·æŒ‡æ ‡çš„è¯„ä¼°ç»“æœï¼Œæé«˜äº†è¯„ä¼°çš„å¯æ¯”æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>ç»è¿‡è®­ç»ƒçš„æ¨¡å‹åœ¨é›¶æ ·æœ¬è¯„ä¼°å’Œå¾®è°ƒåçš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº†æœ€é«˜ä¸º63%å’Œæœ€é«˜ä¸º85%ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.21131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be1604f0eb9fa924fc454c230cd819b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abf40e97a6c7a2d9f911aef4664db5fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-614d92ee680d63fff1f6348262f22d10.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2a4bb6d93f6483dcf38081a4a3b67e63.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-107222e5daf6239cf52e5229dcd81541.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0fa7678b7d0886f7fdff612bf1348cdf.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-24/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-24/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-24/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-795fb9c949a52454ad3d3d61b7f1af90.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-24  Guiding VLM Agents with Process Rewards at Inference Time for GUI   Navigation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-24/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2d83a63f87db38a77ea2777480d0ec25.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-24  TTRL Test-Time Reinforcement Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
