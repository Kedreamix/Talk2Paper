<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-04-24  From Reflection to Perfection Scaling Inference-Time Optimization for   Text-to-Image Diffusion Models via Reflection Tuning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-5bab06f9fc547a0b2c40652a0533ac66.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-24
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    34 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-24-更新"><a href="#2025-04-24-更新" class="headerlink" title="2025-04-24 更新"></a>2025-04-24 更新</h1><h2 id="From-Reflection-to-Perfection-Scaling-Inference-Time-Optimization-for-Text-to-Image-Diffusion-Models-via-Reflection-Tuning"><a href="#From-Reflection-to-Perfection-Scaling-Inference-Time-Optimization-for-Text-to-Image-Diffusion-Models-via-Reflection-Tuning" class="headerlink" title="From Reflection to Perfection: Scaling Inference-Time Optimization for   Text-to-Image Diffusion Models via Reflection Tuning"></a>From Reflection to Perfection: Scaling Inference-Time Optimization for   Text-to-Image Diffusion Models via Reflection Tuning</h2><p><strong>Authors:Le Zhuo, Liangbing Zhao, Sayak Paul, Yue Liao, Renrui Zhang, Yi Xin, Peng Gao, Mohamed Elhoseiny, Hongsheng Li</strong></p>
<p>Recent text-to-image diffusion models achieve impressive visual quality through extensive scaling of training data and model parameters, yet they often struggle with complex scenes and fine-grained details. Inspired by the self-reflection capabilities emergent in large language models, we propose ReflectionFlow, an inference-time framework enabling diffusion models to iteratively reflect upon and refine their outputs. ReflectionFlow introduces three complementary inference-time scaling axes: (1) noise-level scaling to optimize latent initialization; (2) prompt-level scaling for precise semantic guidance; and most notably, (3) reflection-level scaling, which explicitly provides actionable reflections to iteratively assess and correct previous generations. To facilitate reflection-level scaling, we construct GenRef, a large-scale dataset comprising 1 million triplets, each containing a reflection, a flawed image, and an enhanced image. Leveraging this dataset, we efficiently perform reflection tuning on state-of-the-art diffusion transformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified framework. Experimental results show that ReflectionFlow significantly outperforms naive noise-level scaling methods, offering a scalable and compute-efficient solution toward higher-quality image synthesis on challenging tasks. </p>
<blockquote>
<p>最近出现的文本到图像的扩散模型通过大规模扩展训练数据和模型参数，实现了令人印象深刻的视觉质量，但它们往往在处理复杂场景和精细细节方面遇到困难。受大型语言模型中出现的自我反思能力的启发，我们提出了ReflectionFlow，这是一个推理时间框架，能够使扩散模型迭代地反思和完善其输出。ReflectionFlow引入了三种互补的推理时间尺度轴：（1）噪声水平尺度优化潜在初始化；（2）提示水平尺度提供精确语义指导；以及最值得注意的是（3）反思水平尺度，它明确提供了可操作的反思来迭代评估和纠正之前的生成。为了促进反思水平尺度的发展，我们构建了GenRef数据集，该数据集包含100万组三元组，每组包含一次反思、一个缺陷图像和一个增强的图像。利用此数据集，我们在统一框架下联合建模多模式输入，对最先进的扩散变压器FLUX.1-dev进行高效反射调优。实验结果表明，ReflectionFlow显著优于简单的噪声水平尺度方法，为具有挑战性的任务提供了可伸缩和计算高效的解决方案，以实现更高质量的图像合成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16080v1">PDF</a> All code, checkpoints, and datasets are available at   \url{<a target="_blank" rel="noopener" href="https://diffusion-cot.github.io/reflection2perfection%7D">https://diffusion-cot.github.io/reflection2perfection}</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于文本到图像扩散模型的改进方案ReflectionFlow，它通过引入噪声级别缩放、提示级别缩放和反射级别缩放三种互补的推理时间缩放轴，提升了扩散模型的性能。其中，反射级别缩放能够显式地提供反馈来迭代评估和修正之前的生成图像。实验结果表明，ReflectionFlow在具有挑战性的任务上显著优于简单的噪声级别缩放方法，为高质量图像合成提供了可扩展和计算高效的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型虽能通过大规模的训练数据和模型参数实现高质量的图像生成，但在复杂场景和精细细节方面存在挑战。</li>
<li>ReflectionFlow是一个推理时间框架，启用了扩散模型的迭代反思和细化输出能力。</li>
<li>ReflectionFlow引入了三种互补的推理时间缩放轴：噪声级别缩放、提示级别缩放和反射级别缩放。</li>
<li>反射级别缩放能够明确提供反馈，以迭代评估和修正之前的生成图像。</li>
<li>为了促进反射级别缩放，构建了GenRef数据集，包含百万个三元组，每个包含反思、有缺陷的图像和增强的图像。</li>
<li>利用GenRef数据集，在最新扩散变压器FLUX上进行了高效反射调整。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16080">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dfd6c6395e34273a2ed17fc72955362c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-531502b431a822dc1b7da3db6eb89543.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-552efa358ba9c027393f3b69a6329e6a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cdd67822a9fd95116aa5102a85ca682d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Satellite-to-GroundScape-–-Large-scale-Consistent-Ground-View-Generation-from-Satellite-Views"><a href="#Satellite-to-GroundScape-–-Large-scale-Consistent-Ground-View-Generation-from-Satellite-Views" class="headerlink" title="Satellite to GroundScape – Large-scale Consistent Ground View   Generation from Satellite Views"></a>Satellite to GroundScape – Large-scale Consistent Ground View   Generation from Satellite Views</h2><p><strong>Authors:Ningli Xu, Rongjun Qin</strong></p>
<p>Generating consistent ground-view images from satellite imagery is challenging, primarily due to the large discrepancies in viewing angles and resolution between satellite and ground-level domains. Previous efforts mainly concentrated on single-view generation, often resulting in inconsistencies across neighboring ground views. In this work, we propose a novel cross-view synthesis approach designed to overcome these challenges by ensuring consistency across ground-view images generated from satellite views. Our method, based on a fixed latent diffusion model, introduces two conditioning modules: satellite-guided denoising, which extracts high-level scene layout to guide the denoising process, and satellite-temporal denoising, which captures camera motion to maintain consistency across multiple generated views. We further contribute a large-scale satellite-ground dataset containing over 100,000 perspective pairs to facilitate extensive ground scene or video generation. Experimental results demonstrate that our approach outperforms existing methods on perceptual and temporal metrics, achieving high photorealism and consistency in multi-view outputs. </p>
<blockquote>
<p>从卫星图像生成连贯的地面视图图像是一项挑战，主要是由于卫星和地面领域之间观看角度和分辨率存在很大差异。之前的研究主要集中在单视图生成上，往往导致相邻地面视图之间出现不一致。在这项工作中，我们提出了一种新的跨视图合成方法，旨在通过确保从卫星视图生成的地面视图图像之间的一致性来克服这些挑战。我们的方法基于固定的潜在扩散模型，引入了两个条件模块：卫星引导去噪，用于提取高级场景布局以引导去噪过程；卫星时间去噪，用于捕获摄像机运动以维持多个生成视图之间的一致性。我们还贡献了一个大规模卫星地面数据集，包含超过10万个透视对，以促进广泛的地面场景或视频生成。实验结果表明，我们的方法在感知和时间度量上优于现有方法，实现了多视图输出的高逼真度和一致性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15786v1">PDF</a> 8 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于固定潜在扩散模型的新型跨视图合成方法，旨在克服从卫星图像生成地面视图图像的挑战。该方法通过引入卫星引导去噪和卫星时间去噪两个条件模块，确保从不同卫星视图生成的地面视图图像之间的一致性。同时，贡献了一个大规模卫星-地面数据集，包含超过10万对视角数据，以促进地面场景或视频生成的研究。实验结果证明，该方法在感知和时间度量上优于现有方法，实现了多视图输出的高逼真度和一致性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>跨视图合成方法被提出，旨在从卫星图像生成一致的地面视图图像。</li>
<li>方法基于固定潜在扩散模型，确保多视图之间的一致性。</li>
<li>引入两个条件模块：卫星引导去噪和卫星时间去噪，以优化生成过程。</li>
<li>贡献了一个大规模卫星-地面数据集，包含超过10万对视角数据。</li>
<li>方法在感知和时间度量上表现出优越性。</li>
<li>生成的结果具有高逼真度和多视图的一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15786">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-535c4b2eeb07862a15f848901ed4c294.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-074fd0a0d06375ba4b8043d26773776a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2e38c312338988d53e003c53a06bc15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43a28653b0125f17a5eec1a230958fbd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cbdb57f497a29a3bf459bed42df706c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="InstaRevive-One-Step-Image-Enhancement-via-Dynamic-Score-Matching"><a href="#InstaRevive-One-Step-Image-Enhancement-via-Dynamic-Score-Matching" class="headerlink" title="InstaRevive: One-Step Image Enhancement via Dynamic Score Matching"></a>InstaRevive: One-Step Image Enhancement via Dynamic Score Matching</h2><p><strong>Authors:Yixuan Zhu, Haolin Wang, Ao Li, Wenliang Zhao, Yansong Tang, Jingxuan Niu, Lei Chen, Jie Zhou, Jiwen Lu</strong></p>
<p>Image enhancement finds wide-ranging applications in real-world scenarios due to complex environments and the inherent limitations of imaging devices. Recent diffusion-based methods yield promising outcomes but necessitate prolonged and computationally intensive iterative sampling. In response, we propose InstaRevive, a straightforward yet powerful image enhancement framework that employs score-based diffusion distillation to harness potent generative capability and minimize the sampling steps. To fully exploit the potential of the pre-trained diffusion model, we devise a practical and effective diffusion distillation pipeline using dynamic control to address inaccuracies in updating direction during score matching. Our control strategy enables a dynamic diffusing scope, facilitating precise learning of denoising trajectories within the diffusion model and ensuring accurate distribution matching gradients during training. Additionally, to enrich guidance for the generative power, we incorporate textual prompts via image captioning as auxiliary conditions, fostering further exploration of the diffusion model. Extensive experiments substantiate the efficacy of our framework across a diverse array of challenging tasks and datasets, unveiling the compelling efficacy and efficiency of InstaRevive in delivering high-quality and visually appealing results. Code is available at <a target="_blank" rel="noopener" href="https://github.com/EternalEvan/InstaRevive">https://github.com/EternalEvan/InstaRevive</a>. </p>
<blockquote>
<p>图像增强在现实场景中具有广泛的应用，这归功于复杂的环境和成像设备固有的局限性。虽然最近的扩散方法产生了有前途的结果，但它们需要进行长期和计算密集型的迭代采样。针对这一问题，我们提出了InstaRevive，这是一个简单而强大的图像增强框架，它采用基于分数的扩散蒸馏来利用强大的生成能力并减少采样步骤。为了充分利用预训练扩散模型的潜力，我们设计了一个实用有效的扩散蒸馏管道，使用动态控制来解决分数匹配过程中更新方向的不准确问题。我们的控制策略能够实现动态扩散范围，便于在扩散模型中精确学习去噪轨迹，并在训练过程中确保准确的分布匹配梯度。此外，为了丰富生成能力的指导，我们通过图像描述作为辅助条件融入文本提示，促进扩散模型的进一步探索。大量实验证明，我们的框架在多种具有挑战性的任务和数据集上效果显著，揭示了InstaRevive在提供高质量和视觉上吸引人的结果方面的令人信服的有效性和效率。代码可通过以下网址获取：<a target="_blank" rel="noopener" href="https://github.com/EternalEvan/InstaRevive">https://github.com/EternalEvan/InstaRevive</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15513v1">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong><br>     针对图像增强在真实场景中的广泛应用及成像设备的固有局限性，提出了一种基于扩散蒸馏的即时增强框架InstaRevive。通过得分为基础的扩散蒸馏技术实现强大的生成能力和减少采样步骤。使用动态控制策略实现预训练扩散模型的潜力最大化，确保精确学习去噪轨迹和分布匹配梯度。结合图像描述文本提示作为辅助条件，进一步探索扩散模型的潜力。实验证明，该框架在多种挑战任务和数据集上效果显著。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>InstaRevive是一个基于扩散蒸馏的图像增强框架，旨在解决复杂环境和成像设备限制下的图像问题。</li>
<li>该框架采用得分基础的扩散蒸馏技术，以强大的生成能力和减少采样步骤为特点。</li>
<li>通过动态控制策略实现预训练扩散模型的潜力最大化，确保精确的去噪轨迹学习和分布匹配梯度的准确性。</li>
<li>InstaRevive结合图像描述文本提示，作为辅助条件来丰富生成力量的指导。</li>
<li>框架在多种挑战任务和数据集上进行了广泛实验，证明了其有效性和高效率。</li>
<li>该框架可提供高质量和视觉吸引力的结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15513">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-14eb16871de689e89029d4d0e9d85aaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28c3df2c41a96e059fb51f1d2323e4f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-212568bb9023b67ce4acdcc616050708.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16fc418fabcc249ffd6f44c3fd7a1cdc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Manifold-Induced-Biases-for-Zero-shot-and-Few-shot-Detection-of-Generated-Images"><a href="#Manifold-Induced-Biases-for-Zero-shot-and-Few-shot-Detection-of-Generated-Images" class="headerlink" title="Manifold Induced Biases for Zero-shot and Few-shot Detection of   Generated Images"></a>Manifold Induced Biases for Zero-shot and Few-shot Detection of   Generated Images</h2><p><strong>Authors:Jonathan Brokman, Amit Giloni, Omer Hofman, Roman Vainshtein, Hisashi Kojima, Guy Gilboa</strong></p>
<p>Distinguishing between real and AI-generated images, commonly referred to as ‘image detection’, presents a timely and significant challenge. Despite extensive research in the (semi-)supervised regime, zero-shot and few-shot solutions have only recently emerged as promising alternatives. Their main advantage is in alleviating the ongoing data maintenance, which quickly becomes outdated due to advances in generative technologies. We identify two main gaps: (1) a lack of theoretical grounding for the methods, and (2) significant room for performance improvements in zero-shot and few-shot regimes. Our approach is founded on understanding and quantifying the biases inherent in generated content, where we use these quantities as criteria for characterizing generated images. Specifically, we explore the biases of the implicit probability manifold, captured by a pre-trained diffusion model. Through score-function analysis, we approximate the curvature, gradient, and bias towards points on the probability manifold, establishing criteria for detection in the zero-shot regime. We further extend our contribution to the few-shot setting by employing a mixture-of-experts methodology. Empirical results across 20 generative models demonstrate that our method outperforms current approaches in both zero-shot and few-shot settings. This work advances the theoretical understanding and practical usage of generated content biases through the lens of manifold analysis. </p>
<blockquote>
<p>区分真实图像和人工智能生成的图像，通常被称为“图像检测”，这是一个及时且重大的挑战。尽管在（半）监督制度方面进行了大量研究，但零样本和少样本解决方案最近才作为有前途的替代方案出现。它们的主要优势在于缓解了正在进行的数据维护问题，由于生成技术的不断进步，这些数据很快会过时。我们发现了两个主要空白：（1）这些方法缺乏理论支撑，（2）在零样本和少样本制度下，性能仍有很大提升空间。我们的方法建立在理解和量化生成内容所固有的偏见上，我们使用这些量作为表征生成图像的标准。具体来说，我们探索了由预训练扩散模型捕获的隐概率流形偏见。通过得分函数分析，我们近似概率流形上的点处的曲率、梯度和偏见，为无样本制度下建立检测标准。我们进一步通过将混合专家方法应用于少样本设置来扩展我们的贡献。在跨越20个生成模型的实证结果表明，我们的方法在零样本和少样本设置中都优于当前方法。这项工作通过流形分析的角度，推动了生成内容偏见的理论理解的实际应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15470v1">PDF</a> Accepted to ICLR 2025 (The International Conference on Learning   Representations)</p>
<p><strong>Summary</strong></p>
<p>本文探讨了区分真实图像和AI生成图像的挑战，重点介绍了基于生成内容偏见的检测方法和理论。通过理解并量化生成内容中的偏见，使用预训练的扩散模型捕获隐式概率流形，通过评分函数分析近似曲率、梯度和概率流形上的点偏向，建立零样本下的检测标准。同时，采用混合专家方法解决小样本问题，并在20个生成模型上进行实证测试，证明该方法在零样本和小样本环境下均优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>区分真实和AI生成的图像是一个及时且重要的挑战。</li>
<li>零样本和少样本解决方案是新兴的有前途的替代方法，可以缓解数据维护的问题。</li>
<li>当前方法缺乏理论支撑和性能改进的空间。</li>
<li>本文通过理解并量化生成内容中的偏见来建立检测标准。</li>
<li>使用预训练的扩散模型捕获隐式概率流形，通过评分函数分析进行图像检测。</li>
<li>在零样本和小样本环境下，该方法均表现出优异的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15470">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d9ff5a9175d553f77043c4f719180c52.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a8e18012d3e7c1ca7ee3736223aa0c26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea90bd09638f3dc67cc37da40227441b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27a019bc4cefc9d08a846418b46078e4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MirrorVerse-Pushing-Diffusion-Models-to-Realistically-Reflect-the-World"><a href="#MirrorVerse-Pushing-Diffusion-Models-to-Realistically-Reflect-the-World" class="headerlink" title="MirrorVerse: Pushing Diffusion Models to Realistically Reflect the World"></a>MirrorVerse: Pushing Diffusion Models to Realistically Reflect the World</h2><p><strong>Authors:Ankit Dhiman, Manan Shah, R Venkatesh Babu</strong></p>
<p>Diffusion models have become central to various image editing tasks, yet they often fail to fully adhere to physical laws, particularly with effects like shadows, reflections, and occlusions. In this work, we address the challenge of generating photorealistic mirror reflections using diffusion-based generative models. Despite extensive training data, existing diffusion models frequently overlook the nuanced details crucial to authentic mirror reflections. Recent approaches have attempted to resolve this by creating synhetic datasets and framing reflection generation as an inpainting task; however, they struggle to generalize across different object orientations and positions relative to the mirror. Our method overcomes these limitations by introducing key augmentations into the synthetic data pipeline: (1) random object positioning, (2) randomized rotations, and (3) grounding of objects, significantly enhancing generalization across poses and placements. To further address spatial relationships and occlusions in scenes with multiple objects, we implement a strategy to pair objects during dataset generation, resulting in a dataset robust enough to handle these complex scenarios. Achieving generalization to real-world scenes remains a challenge, so we introduce a three-stage training curriculum to develop the MirrorFusion 2.0 model to improve real-world performance. We provide extensive qualitative and quantitative evaluations to support our approach. The project page is available at: <a target="_blank" rel="noopener" href="https://mirror-verse.github.io/">https://mirror-verse.github.io/</a>. </p>
<blockquote>
<p>扩散模型在各种图像编辑任务中发挥着核心作用，但它们往往不能完全遵守物理定律，特别是在阴影、反射和遮挡等效果方面。在这项工作中，我们致力于利用基于扩散的生成模型生成逼真的镜面反射图像。尽管有大量的训练数据，但现有的扩散模型往往会忽略对真实镜面反射至关重要的细微细节。最近的方法试图通过创建合成数据集并将反射生成视为填充任务来解决这个问题；然而，它们在镜子不同方向和位置的泛化方面存在困难。我们的方法通过向合成数据管道中添加关键增强来克服这些限制：（1）随机对象定位，（2）随机旋转，（3）对象接地，显著提高了姿势和放置的泛化能力。为了进一步解决具有多个对象的场景中的空间关系和遮挡问题，我们在数据集生成过程中实现了对象配对策略，从而得到一个足以应对这些复杂场景的稳健数据集。实现向真实世界场景的泛化仍然是一个挑战，因此我们引入了一个三阶段训练课程来开发MirrorFusion 2.0模型，以提高其在现实世界中的性能。我们提供了广泛的质量和数量评估来支持我们的方法。项目页面可通过以下网址访问：<a target="_blank" rel="noopener" href="https://mirror-verse.github.io/%E3%80%82">https://mirror-verse.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15397v1">PDF</a> Accepted to CVPR 2025. Project Page: <a target="_blank" rel="noopener" href="https://mirror-verse.github.io/">https://mirror-verse.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文解决了扩散模型在生成真实镜面反射图像时面临的挑战，通过引入关键增强手段，如随机物体定位、随机旋转和物体定位，提高了模型在不同姿态和放置场景下的泛化能力。为解决多物体场景中空间关系和遮挡问题，项目实施了配对物体生成数据集策略。为提升现实场景的泛化性能，还推出了三阶段训练课程来完善MirrorFusion 2.0模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像编辑任务中扮演核心角色，但在处理镜面反射等物理效应时面临挑战。</li>
<li>现有扩散模型难以捕捉镜面反射的细微细节。</li>
<li>现有方法通过创建合成数据集和将反射生成视为补全任务来解决此问题，但难以泛化不同物体相对于镜子的方向和位置。</li>
<li>本文通过引入随机物体定位、随机旋转和物体定位关键增强手段，提高了模型的泛化能力。</li>
<li>为处理多物体场景中的空间关系和遮挡，项目实施了配对物体生成数据集策略。</li>
<li>为提升在现实场景的泛化性能，采用了三阶段训练课程完善模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15397">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-242dc16bb7312369d203405363d8069c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aaddafa2e08e2c8b8034ae85038a2916.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a82cb68350be8d310ac2b3e1d7516602.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b1457aa11fdeb4afc3516d197806a9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da8b7429411347cf928870d8ace7012f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-723b7fe671ac0bcea665bdf63276adf6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="VistaDepth-Frequency-Modulation-With-Bias-Reweighting-For-Enhanced-Long-Range-Depth-Estimation"><a href="#VistaDepth-Frequency-Modulation-With-Bias-Reweighting-For-Enhanced-Long-Range-Depth-Estimation" class="headerlink" title="VistaDepth: Frequency Modulation With Bias Reweighting For Enhanced   Long-Range Depth Estimation"></a>VistaDepth: Frequency Modulation With Bias Reweighting For Enhanced   Long-Range Depth Estimation</h2><p><strong>Authors:Mingxia Zhan, Li Zhang, Xiaomeng Chu, Beibei Wang</strong></p>
<p>Monocular depth estimation (MDE) aims to predict per-pixel depth values from a single RGB image. Recent advancements have positioned diffusion models as effective MDE tools by framing the challenge as a conditional image generation task. Despite their progress, these methods often struggle with accurately reconstructing distant depths, due largely to the imbalanced distribution of depth values and an over-reliance on spatial-domain features. To overcome these limitations, we introduce VistaDepth, a novel framework that integrates adaptive frequency-domain feature enhancements with an adaptive weight-balancing mechanism into the diffusion process. Central to our approach is the Latent Frequency Modulation (LFM) module, which dynamically refines spectral responses in the latent feature space, thereby improving the preservation of structural details and reducing noisy artifacts. Furthermore, we implement an adaptive weighting strategy that modulates the diffusion loss in real-time, enhancing the model’s sensitivity towards distant depth reconstruction. These innovations collectively result in superior depth perception performance across both distance and detail. Experimental evaluations confirm that VistaDepth achieves state-of-the-art performance among diffusion-based MDE techniques, particularly excelling in the accurate reconstruction of distant regions. </p>
<blockquote>
<p>单眼深度估计（MDE）旨在从单个RGB图像预测每个像素的深度值。最近的发展使扩散模型成为有效的MDE工具，将挑战定位为条件图像生成任务。尽管取得了进展，这些方法在准确重建远距离深度方面仍面临困难，这主要是由于深度值分布不平衡以及过于依赖空间域特征。为了克服这些限制，我们引入了VistaDepth，这是一个新型框架，将自适应频域特征增强和自适应权重平衡机制集成到扩散过程中。我们的方法的核心是潜在频率调制（LFM）模块，它动态地优化潜在特征空间中的光谱响应，从而提高结构细节的保留，减少噪声伪影。此外，我们实现了实时调整扩散损失的自适应加权策略，提高模型对远距离深度重建的敏感性。这些创新共同带来了在距离和细节方面的卓越深度感知性能。实验评估证实，VistaDepth在基于扩散的MDE技术中实现了最先进的性能，特别是在准确重建远距离区域方面表现出色。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15095v2">PDF</a> 8 pages, 6 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>扩散模型在单目深度估计（MDE）中展现出良好的性能，但存在对远距离深度重建的不准确问题。为此，VistaDepth框架引入自适应频域特征增强和自适应权重平衡机制。其核心模块——潜在频率调制（LFM）能动态优化潜在特征空间的频谱响应，提高结构细节保留并减少噪声。此外，实施自适应权重策略实时调整扩散损失，提升模型对远距离深度的敏感度。这些创新使VistaDepth在距离和细节上的深度感知性能均达到领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在单目深度估计中表现出良好的性能。</li>
<li>现有方法存在对远距离深度重建的不准确问题。</li>
<li>VistaDepth框架引入潜在频率调制（LFM）模块，提高结构细节保留并减少噪声。</li>
<li>VistaDepth使用自适应频域特征增强和自适应权重平衡机制来改善扩散模型的性能。</li>
<li>潜在频率调制能够动态优化潜在特征空间的频谱响应。</li>
<li>自适应权重策略可实时调整扩散损失，提高对远距离深度的敏感度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15095">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6e9541719030f61d80cf36ea1e700f67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72cda878b092ffe90a7ac0e1850b497a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e61bd7439e03175edb293be32c720b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aada7c6e3a3d4d7c7ab217b83811009b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d38f9f6803a7a066c6cf2b5a5a508534.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Gungnir-Exploiting-Stylistic-Features-in-Images-for-Backdoor-Attacks-on-Diffusion-Models"><a href="#Gungnir-Exploiting-Stylistic-Features-in-Images-for-Backdoor-Attacks-on-Diffusion-Models" class="headerlink" title="Gungnir: Exploiting Stylistic Features in Images for Backdoor Attacks on   Diffusion Models"></a>Gungnir: Exploiting Stylistic Features in Images for Backdoor Attacks on   Diffusion Models</h2><p><strong>Authors:Yu Pan, Bingrong Dai, Jiahao Chen, Lin Wang, Yi Du, Jiao Liu</strong></p>
<p>In recent years, Diffusion Models (DMs) have demonstrated significant advances in the field of image generation. However, according to current research, DMs are vulnerable to backdoor attacks, which allow attackers to control the model’s output by inputting data containing covert triggers, such as a specific visual patch or phrase. Existing defense strategies are well equipped to thwart such attacks through backdoor detection and trigger inversion because previous attack methods are constrained by limited input spaces and low-dimensional triggers. For example, visual triggers are easily observed by defenders, text-based or attention-based triggers are more susceptible to neural network detection. To explore more possibilities of backdoor attack in DMs, we propose Gungnir, a novel method that enables attackers to activate the backdoor in DMs through style triggers within input images. Our approach proposes using stylistic features as triggers for the first time and implements backdoor attacks successfully in image-to-image tasks by introducing Reconstructing-Adversarial Noise (RAN) and Short-Term Timesteps-Retention (STTR). Our technique generates trigger-embedded images that are perceptually indistinguishable from clean images, thus bypassing both manual inspection and automated detection neural networks. Experiments demonstrate that Gungnir can easily bypass existing defense methods. Among existing DM defense frameworks, our approach achieves a 0 backdoor detection rate (BDR). Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/paoche11/Gungnir">https://github.com/paoche11/Gungnir</a>. </p>
<blockquote>
<p>近年来，扩散模型（DMs）在图生成领域取得了显著进展。然而，根据目前的研究，DMs容易受到后门攻击的影响，攻击者可以通过输入包含隐蔽触发器的数据来控制模型的输出，例如特定的视觉斑块或短语。现有的防御策略能够通过后门检测和触发反转来有效地阻止此类攻击，因为以前的攻击方法受到输入空间有限和低维触发的限制。例如，视觉触发器很容易被防御者观察到，而基于文本或基于注意力的触发器更容易受到神经网络检测。为了探索DM中后门攻击的可能性，我们提出了Gungnir这一新方法，它能够让攻击者通过输入图像内的风格触发器在DMs中激活后门。我们的方法首次提出使用风格特征作为触发器，并通过引入重建对抗噪声（RAN）和短期时间步保留（STTR）成功地在图像到图像任务中实现后门攻击。我们的技术生成了嵌入触发器的图像，这些图像在感知上与干净图像无法区分，从而绕过了手动检查和自动检测神经网络。实验表明，Gungnir可以轻松绕过现有防御方法。在现有的DM防御框架中，我们的方法实现了0后门检测率（BDR）。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/paoche11/Gungnir%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/paoche11/Gungnir找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20650v2">PDF</a> </p>
<p><strong>Summary</strong><br>     扩散模型（DMs）在图像生成领域取得了显著进展，但近期研究发现其存在后门攻击漏洞。攻击者可通过输入含有隐蔽触发器的数据来控制模型输出。现有防御策略可通过后门检测和触发逆转来阻止此类攻击。为探索扩散模型中后门攻击的新可能性，提出一种新方法Gungnir，该方法利用风格触发器在输入图像中实现后门激活。Gungnir成功在图像到图像的任务中实施后门攻击，通过引入重建对抗噪声（RAN）和短期时间步保留（STTR），生成含有触发器的图像，这些图像与干净图像在感知上无法区分，从而绕过手动检查和自动检测神经网络。实验表明，Gungnir可以轻松绕过现有防御方法，实现零后门检测率（BDR）。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型（DMs）在图像生成领域表现优异，但存在后门攻击漏洞。</li>
<li>现有防御策略能通过后门检测和触发逆转来阻止攻击。</li>
<li>Gungnir是一种新的攻击方法，通过风格触发器在输入图像中实现后门激活。</li>
<li>Gungnir成功在图像到图像的任务中实施后门攻击，生成触发嵌入图像，这些图像与干净图像难以区分。</li>
<li>Gungnir方法绕过现有防御方法，实现零后门检测率（BDR）。</li>
<li>Gungnir代码已公开可用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20650">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fff9ed6673e4b713df3f3206b7d2b529.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc1a2d8ccf2256a2a2857c9ba590e20d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bab06f9fc547a0b2c40652a0533ac66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a81fca22a2eff8388dc126313d3c966d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aff4cb75641dfb8a32e6796489f219df.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="On-the-Guidance-of-Flow-Matching"><a href="#On-the-Guidance-of-Flow-Matching" class="headerlink" title="On the Guidance of Flow Matching"></a>On the Guidance of Flow Matching</h2><p><strong>Authors:Ruiqi Feng, Tailin Wu, Chenglei Yu, Wenhao Deng, Peiyan Hu</strong></p>
<p>Flow matching has shown state-of-the-art performance in various generative tasks, ranging from image generation to decision-making, where guided generation is pivotal. However, the guidance of flow matching is more general than and thus substantially different from that of its predecessor, diffusion models. Therefore, the challenge in guidance for general flow matching remains largely underexplored. In this paper, we propose the first framework of general guidance for flow matching. From this framework, we derive a family of guidance techniques that can be applied to general flow matching. These include a new training-free asymptotically exact guidance, novel training losses for training-based guidance, and two classes of approximate guidance that cover classical gradient guidance methods as special cases. We theoretically investigate these different methods to give a practical guideline for choosing suitable methods in different scenarios. Experiments on synthetic datasets, image inverse problems, and offline reinforcement learning demonstrate the effectiveness of our proposed guidance methods and verify the correctness of our flow matching guidance framework. Code to reproduce the experiments can be found at <a target="_blank" rel="noopener" href="https://github.com/AI4Science-WestlakeU/flow_guidance">https://github.com/AI4Science-WestlakeU/flow_guidance</a>. </p>
<blockquote>
<p>流匹配在各种生成任务中表现出了最先进的性能，从图像生成到决策制定，其中引导生成是关键。然而，流匹配的指导更加通用，因此与其前身扩散模型有着显著差异。因此，通用流匹配的指导挑战仍然被大大忽视。在本文中，我们提出了流匹配通用指导框架。在此框架的基础上，我们推导出了一系列可应用于通用流匹配的指导技术。这包括一种新的无需训练即可渐近精确指导、用于基于训练指导的新训练损失，以及两类涵盖经典梯度指导方法为特殊情况的近似指导。我们从理论上探讨了这些方法，为不同场景选择适当的方法提供了实用指南。在合成数据集、图像反问题和离线强化学习上的实验证明了我们提出的指导方法的有效性，并验证了我们的流匹配指导框架的正确性。可在<a target="_blank" rel="noopener" href="https://github.com/AI4Science-WestlakeU/flow_guidance%E6%89%BE%E5%88%B0%E9%87%8D%E7%8E%B0%E5%AE%9E%E9%AA%8C%E7%9A%84%E6%BA%90%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/AI4Science-WestlakeU/flow_guidance找到重现实验的源代码。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02150v2">PDF</a> 35 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出了针对流匹配技术的通用指导框架，涵盖无训练指导方法、基于训练的新损失函数指导方法以及包含传统梯度指导方法的近似指导方法两类。通过对不同方法的理论分析，给出了在实际场景中选取合适方法的实用指南。实验证明，该指导框架能有效应用于合成数据集、图像逆问题和离线强化学习等领域。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>流匹配技术在生成任务中展现出卓越性能，其应用范围从图像生成到决策制定不等。</li>
<li>流匹配的指导理念与其前身扩散模型有显著区别，但后者缺乏通用指导框架。</li>
<li>提出首个针对流匹配的通用指导框架，涵盖多种指导技术。</li>
<li>引入无训练指导方法，包括新的渐进精确指导技术。</li>
<li>提出新的基于训练损失函数的方法作为另一种指导技术。</li>
<li>指导框架包括近似方法，包含传统的梯度指导方法作为特例。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02150">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-eeec81ef3c2019669004f143eaeb4b1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01c5e75ae464f67c34055a16db596472.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8a3e41427cae22bb2efb6e00eeeb8eb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="An-Undetectable-Watermark-for-Generative-Image-Models"><a href="#An-Undetectable-Watermark-for-Generative-Image-Models" class="headerlink" title="An Undetectable Watermark for Generative Image Models"></a>An Undetectable Watermark for Generative Image Models</h2><p><strong>Authors:Sam Gunn, Xuandong Zhao, Dawn Song</strong></p>
<p>We present the first undetectable watermarking scheme for generative image models. Undetectability ensures that no efficient adversary can distinguish between watermarked and un-watermarked images, even after making many adaptive queries. In particular, an undetectable watermark does not degrade image quality under any efficiently computable metric. Our scheme works by selecting the initial latents of a diffusion model using a pseudorandom error-correcting code (Christ and Gunn, 2024), a strategy which guarantees undetectability and robustness. We experimentally demonstrate that our watermarks are quality-preserving and robust using Stable Diffusion 2.1. Our experiments verify that, in contrast to every prior scheme we tested, our watermark does not degrade image quality. Our experiments also demonstrate robustness: existing watermark removal attacks fail to remove our watermark from images without significantly degrading the quality of the images. Finally, we find that we can robustly encode 512 bits in our watermark, and up to 2500 bits when the images are not subjected to watermark removal attacks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/XuandongZhao/PRC-Watermark">https://github.com/XuandongZhao/PRC-Watermark</a>. </p>
<blockquote>
<p>我们为生成图像模型提出了第一个不可检测的水印方案。不可检测性确保高效的对手即使在执行多次自适应查询后，也无法区分带水印和不带水印的图像。特别是，不可检测的水印在任何可高效计算的指标下都不会降低图像质量。我们的方案通过选择扩散模型的初始潜在变量来实现，方法是使用伪随机纠错代码（Christ和Gunn，2024），这一策略可以保证不可检测性和稳健性。我们通过Stable Diffusion 2.1进行实验，证明我们的水印质量保持不变且稳健。我们的实验验证了我们所测试的每个先前方案相反，我们的水印不会降低图像质量。我们的实验还证明了其稳健性：现有的水印移除攻击无法在我们的水印从图像中移除而不显著降低图像质量。最后，我们发现我们可以稳健地在水印中编码512位信息，并且在图像未受到水印移除攻击时，甚至可以编码高达2500位信息。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/XuandongZhao/PRC-Watermark">https://github.com/XuandongZhao/PRC-Watermark</a>处获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07369v4">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong><br>     本文提出首个针对生成图像模型的不可检测水印方案。该方案保证了不可检测性，即使进行多次自适应查询，高效对手也无法区分带水印和不带水印的图像。水印通过在扩散模型的初始潜在表示中使用伪随机纠错码实现，确保了不可检测性和稳健性。实验证明该水印质量无损且稳健，使用Stable Diffusion 2.1验证。对比其他测试方案，该水印不会降低图像质量，并且具有强大的稳健性。同时，可在水印中稳健编码512位信息，若图像未遭受水印移除攻击，则可编码高达2500位信息。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出首个针对生成图像模型的不可检测水印方案。</li>
<li>不可检测水印保证了高效对手无法区分带水印和不带水印的图像。</li>
<li>使用伪随机纠错码实现水印，确保不可检测性和稳健性。</li>
<li>实验证明水印质量无损，不会降低图像质量。</li>
<li>水印具有强大的稳健性，现有水印移除攻击无法去除该水印而不显著破坏图像质量。</li>
<li>可稳健地在水印中编码512位信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.07369">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-30720469f3974817785ba522061ffe72.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-24/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-24/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-24/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-df60ede95705bc7bb12f8536a0c30ccb.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-04-24  Rotational ultrasound and photoacoustic tomography of the human body
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-24
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-24/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0e7789b6d465b30f994c1840f140c433.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-04-24  Pose Optimization for Autonomous Driving Datasets using Neural Rendering   Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">24595.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
