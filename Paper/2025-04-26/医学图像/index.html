<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-26  Self-Supervised Noise Adaptive MRI Denoising via Repetition to   Repetition (Rep2Rep) Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d4ff568aadca0897bbf49396281e914e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    63 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-26-æ›´æ–°"><a href="#2025-04-26-æ›´æ–°" class="headerlink" title="2025-04-26 æ›´æ–°"></a>2025-04-26 æ›´æ–°</h1><h2 id="Self-Supervised-Noise-Adaptive-MRI-Denoising-via-Repetition-to-Repetition-Rep2Rep-Learning"><a href="#Self-Supervised-Noise-Adaptive-MRI-Denoising-via-Repetition-to-Repetition-Rep2Rep-Learning" class="headerlink" title="Self-Supervised Noise Adaptive MRI Denoising via Repetition to   Repetition (Rep2Rep) Learning"></a>Self-Supervised Noise Adaptive MRI Denoising via Repetition to   Repetition (Rep2Rep) Learning</h2><p><strong>Authors:Nikola JanjuÅ¡eviÄ‡, Jingjia Chen, Luke Ginocchio, Mary Bruno, Yuhui Huang, Yao Wang, Hersh Chandarana, Li Feng</strong></p>
<p>Purpose: This work proposes a novel self-supervised noise-adaptive image denoising framework, called Repetition to Repetition (Rep2Rep) learning, for low-field (&lt;1T) MRI applications. Methods: Rep2Rep learning extends the Noise2Noise framework by training a neural network on two repeated MRI acquisitions, using one repetition as input and another as target, without requiring ground-truth data. It incorporates noise-adaptive training, enabling denoising generalization across varying noise levels and flexible inference with any number of repetitions. Performance was evaluated on both synthetic noisy brain MRI and 0.55T prostate MRI data, and compared against supervised learning and Monte Carlo Steinâ€™s Unbiased Risk Estimator (MC-SURE). Results: Rep2Rep learning outperforms MC-SURE on both synthetic and 0.55T MRI datasets. On synthetic brain data, it achieved denoising quality comparable to supervised learning and surpassed MC-SURE, particularly in preserving structural details and reducing residual noise. On the 0.55T prostate MRI dataset, a reader study showed radiologists preferred Rep2Rep-denoised 2-average images over 8-average noisy images. Rep2Rep demonstrated robustness to noise-level discrepancies between training and inference, supporting its practical implementation. Conclusion: Rep2Rep learning offers an effective self-supervised denoising for low-field MRI by leveraging routinely acquired multi-repetition data. Its noise-adaptivity enables generalization to different SNR regimes without clean reference images. This makes Rep2Rep learning a promising tool for improving image quality and scan efficiency in low-field MRI. </p>
<blockquote>
<p>ç›®çš„ï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„è‡ªæˆ‘ç›‘ç£å™ªå£°è‡ªé€‚åº”å›¾åƒå»å™ªæ¡†æ¶ï¼Œç§°ä¸ºé‡å¤åˆ°é‡å¤ï¼ˆRep2Repï¼‰å­¦ä¹ ï¼Œç”¨äºä½åœºï¼ˆ&lt;1Tï¼‰MRIåº”ç”¨ã€‚æ–¹æ³•ï¼šRep2Repå­¦ä¹ é€šè¿‡æ‰©å±•Noise2Noiseæ¡†æ¶ï¼Œå¯¹ä¸¤æ¬¡é‡å¤çš„MRIé‡‡é›†è¿›è¡Œç¥ç»ç½‘ç»œè®­ç»ƒï¼Œä»¥ä¸€æ¬¡é‡å¤ä½œä¸ºè¾“å…¥ï¼Œå¦ä¸€æ¬¡ä½œä¸ºç›®æ ‡ï¼Œæ— éœ€çœŸå®æ•°æ®ã€‚å®ƒèå…¥äº†å™ªå£°è‡ªé€‚åº”è®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„å™ªå£°æ°´å¹³ä¸Šè¿›è¡Œå»å™ªæ³›åŒ–ï¼Œå¹¶ä¸”å…·æœ‰ä»»æ„æ¬¡é‡å¤çš„çµæ´»æ¨ç†ã€‚æ€§èƒ½è¯„ä¼°æ˜¯åœ¨åˆæˆå™ªå£°è„‘MRIå’Œ0.55Tå‰åˆ—è…ºMRIæ•°æ®ä¸Šè¿›è¡Œçš„ï¼Œå¹¶ä¸ç›‘ç£å­¦ä¹ å’Œè’™ç‰¹å¡æ´›æ–¯å¦æ— åé£é™©ä¼°è®¡ï¼ˆMC-SUREï¼‰è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœï¼šRep2Repå­¦ä¹ åœ¨åˆæˆå’Œ0.55T MRIæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºMC-SUREã€‚åœ¨åˆæˆè„‘æ•°æ®ä¸Šï¼Œå…¶å»å™ªè´¨é‡å¯ä¸ç›‘ç£å­¦ä¹ ç›¸åª²ç¾ï¼Œå¹¶ä¸”è¶…è¿‡MC-SUREï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿ç•™ç»“æ„ç»†èŠ‚å’Œå‡å°‘æ®‹ç•™å™ªå£°æ–¹é¢ã€‚åœ¨0.55Tå‰åˆ—è…ºMRIæ•°æ®é›†ä¸Šï¼Œè¯»è€…ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œæ”¾å°„ç§‘åŒ»ç”Ÿæ›´å–œæ¬¢ä½¿ç”¨Rep2Repå»å™ªçš„2æ¬¡å¹³å‡å›¾åƒè€Œä¸æ˜¯8æ¬¡å¹³å‡çš„å™ªå£°å›¾åƒã€‚Rep2Repæ˜¾ç¤ºå‡ºå¯¹è®­ç»ƒå’Œæ¨ç†æœŸé—´å™ªå£°æ°´å¹³å·®å¼‚çš„ç¨³å¥æ€§ï¼Œæ”¯æŒå…¶å®è·µå®æ–½ã€‚ç»“è®ºï¼šRep2Repå­¦ä¹ é€šè¿‡åˆ©ç”¨å¸¸è§„è·å–çš„å¤šé‡å¤æ•°æ®ï¼Œä¸ºä½åœºMRIæä¾›äº†æœ‰æ•ˆçš„è‡ªæˆ‘ç›‘ç£å»å™ªã€‚å…¶å™ªå£°é€‚åº”æ€§ä½¿å¾—èƒ½å¤Ÿåœ¨ä¸åŒçš„ä¿¡å™ªæ¯”åˆ¶åº¦ä¸‹å®ç°æ³›åŒ–ï¼Œè€Œæ— éœ€æ¸…æ´çš„å‚è€ƒå›¾åƒã€‚è¿™ä½¿å¾—Rep2Repå­¦ä¹ æˆä¸ºæé«˜ä½åœºMRIå›¾åƒè´¨é‡å’Œæ‰«ææ•ˆç‡çš„æœ‰å‰é€”çš„å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17698v1">PDF</a> 13 pages, 9 figures, 1 table, supplementary information at end of   document</p>
<p><strong>Summary</strong><br>    æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºè‡ªç›‘ç£å™ªå£°è‡ªé€‚åº”çš„å›¾åƒå»å™ªæ¡†æ¶ï¼Œåä¸ºé‡å¤å­¦ä¹ ï¼ˆRep2Repï¼‰ï¼Œé€‚ç”¨äºä½åœºMRIåº”ç”¨ã€‚æ–¹æ³•é€šè¿‡åœ¨ä¸¤ä¸ªé‡å¤çš„MRIé‡‡é›†ä¸Šè®­ç»ƒç¥ç»ç½‘ç»œï¼Œä»¥ä¸€æ¬¡é‡å¤ä½œä¸ºè¾“å…¥ï¼Œå¦ä¸€æ¬¡ä½œä¸ºç›®æ ‡ï¼Œæ— éœ€çœŸå®æ•°æ®ã€‚å®ƒå®ç°äº†å™ªå£°è‡ªé€‚åº”è®­ç»ƒï¼Œå¯åœ¨ä¸åŒçš„å™ªå£°æ°´å¹³ä¸Šå®ç°å»å™ªæ³›åŒ–å¹¶çµæ´»æ¨ç†ã€‚Rep2Repåœ¨åˆæˆå™ªå£°å¤§è„‘MRIå’Œ0.55Tå‰åˆ—è…ºMRIæ•°æ®ä¸Šçš„è¡¨ç°å‡ä¼˜äºMC-SUREã€‚è¯»è€…ç ”ç©¶è¡¨æ˜ï¼Œæ”¾å°„ç§‘åŒ»ç”Ÿæ›´å€¾å‘äºé€‰æ‹©ç»è¿‡Rep2Repå¤„ç†åçš„ä¸¤æ¬¡å¹³å‡å›¾åƒè€Œä¸æ˜¯å…«æ¬¡å¹³å‡å™ªå£°å›¾åƒã€‚ç»“è®ºæŒ‡å‡ºï¼ŒRep2Repå­¦ä¹ ä¸ºä½åœºMRIæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„è‡ªç›‘ç£å»å™ªæ–¹æ³•ï¼Œåˆ©ç”¨å¸¸è§„è·å–çš„å¤šé‡å¤æ•°æ®ã€‚å…¶å™ªå£°é€‚åº”æ€§ä½¿å¾—èƒ½å¤Ÿåœ¨æ²¡æœ‰æ¸…æ´å‚è€ƒå›¾åƒçš„æƒ…å†µä¸‹é€‚åº”ä¸åŒçš„ä¿¡å™ªæ¯”ç¯å¢ƒã€‚è¿™ä½¿å…¶æˆä¸ºæé«˜ä½åœºMRIå›¾åƒè´¨é‡å’Œæ‰«ææ•ˆç‡çš„æ½œåŠ›å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Rep2Repå­¦ä¹ æ˜¯ä¸€ç§ç”¨äºä½åœºMRIçš„è‡ªç›‘ç£å™ªå£°è‡ªé€‚åº”å›¾åƒå»å™ªæ¡†æ¶ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡é‡å¤MRIé‡‡é›†è®­ç»ƒç¥ç»ç½‘ç»œï¼Œæ— éœ€çœŸå®æ•°æ®ä½œä¸ºå‚ç…§ã€‚</li>
<li>Rep2Repå®ç°äº†åœ¨ä¸åŒå™ªå£°æ°´å¹³ä¸Šçš„å»å™ªæ³›åŒ–å¹¶å…è®¸çµæ´»çš„æ¨ç†ã€‚</li>
<li>åœ¨åˆæˆå¤§è„‘MRIå’Œ0.55Tå‰åˆ—è…ºMRIæ•°æ®ä¸Šï¼ŒRep2Repè¡¨ç°ä¼˜äºMC-SUREå’Œå…¶ä»–æ–¹æ³•ã€‚</li>
<li>è¯»è€…ç ”ç©¶æ”¯æŒäº†Rep2Repåœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>Rep2Repèƒ½å¤Ÿé€‚åº”ä¸åŒçš„ä¿¡å™ªæ¯”ç¯å¢ƒï¼Œæé«˜äº†å…¶åœ¨ä¸åŒç¯å¢ƒä¸‹çš„å®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d4ff568aadca0897bbf49396281e914e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8850a5679830dff771f82fdf42174d64.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Quantifying-jet-interstellar-medium-interactions-in-Cyg-X-1-Insights-from-dual-frequency-bow-shock-detection-with-MeerKAT"><a href="#Quantifying-jet-interstellar-medium-interactions-in-Cyg-X-1-Insights-from-dual-frequency-bow-shock-detection-with-MeerKAT" class="headerlink" title="Quantifying jet-interstellar medium interactions in Cyg X-1: Insights   from dual-frequency bow shock detection with MeerKAT"></a>Quantifying jet-interstellar medium interactions in Cyg X-1: Insights   from dual-frequency bow shock detection with MeerKAT</h2><p><strong>Authors:P. Atri, S. E. Motta, Jakob van den Eijnden, James H. Matthews, James C. A. Miller-Jones, Rob Fender, David Williams-Baldwin, Ian Heywood, Patrick Woudt</strong></p>
<p>Accretion and outflows are astrophysical phenomena observed across a wide range of objects, from white dwarfs to supermassive black holes. Developing a complete picture of these processes requires complementary studies across this full spectrum of jet-launching sources. Jet-interstellar medium (ISM) interaction sites near black hole X-ray binaries provide unique laboratories to study jet energetics. This work aims to detect and characterise the bow shock near one black hole X-ray binary, Cyg X-1, and then use this bow shock structure to parametrise the properties of the jet launched by Cyg X-1 over its lifetime. We used the MeerKAT radio telescope to investigate the bow shock structure formed by the interaction between the jets of Cyg X-1 and the ISM. We successfully detect the bow shock north of Cyg X-1 in the L and S bands and report its size and brightness. We present the spectral index distribution across the bow shock, which is in the range -0.9 to 0.4, with an error distribution (0.6 to 1.5) that peaks at unity. We determine that the unshocked ISM density is 6-7 cm^-3 for a temperature range of 10^4 to 3<em>10^6 K. This temperature range suggests that the velocity of the bow shock is 21 km&#x2F;s to 364 km&#x2F;s. The age of the Cyg X-1 jet responsible for the bow shock is 0.04 to 0.3 Myr, and the power of the jet is constrained to 2</em>10^31 ergs&#x2F;s to 10^35 ergs&#x2F;s. We also detect new morphological features of the bow shock in the S-band image. The comparison of archival H_alpha maps with the new radio observations hints at different regions of emission, different temperature ranges, and different ISM densities. The spectral index suggests a consistent emission origin across the structure. The ISM density around Cyg X-1 is on the higher end for Galactic environments, and our results indicate a lower jet energy transport rate than prior estimates. </p>
<blockquote>
<p>ç§¯äº‘å’Œæµå‡ºæ˜¯åœ¨ç™½çŸ®æ˜Ÿåˆ°è¶…å¤§è´¨é‡é»‘æ´ç­‰å„ç§å¤©ä½“ä¸­è§‚å¯Ÿåˆ°çš„å¤©æ–‡ç°è±¡ã€‚è¦å…¨é¢ç†è§£è¿™äº›è¿‡ç¨‹ï¼Œéœ€è¦å¯¹è¿™ä¸€ç³»åˆ—å–·å°„æºè¿›è¡Œå…¨é¢çš„ç ”ç©¶ã€‚é»‘æ´Xå°„çº¿åŒæ˜Ÿé™„è¿‘çš„å–·å°„æµä¸æ˜Ÿé™…ä»‹è´¨ï¼ˆISMï¼‰ç›¸äº’ä½œç”¨åŒºåŸŸä¸ºç ”ç©¶å–·å°„æµèƒ½é‡æä¾›äº†ç‹¬ç‰¹çš„å®éªŒå®¤ã€‚è¿™é¡¹å·¥ä½œæ—¨åœ¨æ£€æµ‹é è¿‘é»‘æ´Xå°„çº¿åŒæ˜Ÿä¹‹ä¸€çš„èµ›æ ¼é©¬X-1ï¼ˆCyg X-1ï¼‰çš„å¼“å½¢å†²å‡»æ³¢ï¼Œå¹¶åˆ©ç”¨è¿™ä¸€å¼“å½¢å†²å‡»æ³¢ç»“æ„æ¥å‚æ•°åŒ–Cyg X-1åœ¨å…¶ç”Ÿå‘½å‘¨æœŸå†…å–·å°„æµçš„ç‰¹æ€§ã€‚æˆ‘ä»¬ä½¿ç”¨MeerKATå°„ç”µæœ›è¿œé•œæ¥ç ”ç©¶ç”±Cyg X-1å–·å°„æµä¸æ˜Ÿé™…ä»‹è´¨çš„ç›¸äº’ä½œç”¨æ‰€å½¢æˆçš„å¼“å½¢å†²å‡»æ³¢ç»“æ„ã€‚æˆ‘ä»¬æˆåŠŸåœ°åœ¨Læ³¢æ®µå’ŒSæ³¢æ®µæ£€æµ‹åˆ°Cyg X-1åŒ—éƒ¨çš„å¼“å½¢å†²å‡»æ³¢ï¼Œå¹¶æŠ¥å‘Šäº†å…¶å¤§å°å’Œäº®åº¦ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¼“å½¢å†²å‡»æ³¢çš„å…‰è°±æŒ‡æ•°åˆ†å¸ƒèŒƒå›´åœ¨-0.9åˆ°0.4ä¹‹é—´ï¼Œè¯¯å·®åˆ†å¸ƒï¼ˆ0.6åˆ°1.5ï¼‰ä»¥å•ä½å€¼ä¸ºä¸­å¿ƒã€‚æˆ‘ä»¬ç¡®å®šäº†åœ¨æ¸©åº¦èŒƒå›´ä¸º10^4åˆ°3<em>10^6 Kçš„æƒ…å†µä¸‹ï¼Œæœªå—å†²å‡»çš„æ˜Ÿé™…ä»‹è´¨å¯†åº¦æ˜¯6-7 cm^-3ã€‚è¿™ä¸ªæ¸©åº¦èŒƒå›´è¡¨æ˜å¼“å½¢å†²å‡»æ³¢çš„é€Ÿåº¦æ˜¯21 km&#x2F;såˆ°364 km&#x2F;sã€‚é€ æˆå¼“å½¢å†²å‡»æ³¢çš„Cyg X-1å–·å°„æµçš„å¹´é¾„ä¸º0.04åˆ°0.3Myrï¼Œå–·å°„åŠŸç‡è¢«é™åˆ¶åœ¨2</em>10^31 ergs&#x2F;såˆ°10^35 ergs&#x2F;sä¹‹é—´ã€‚æˆ‘ä»¬è¿˜å‘ç°äº†Sæ³¢æ®µå›¾åƒä¸­å¼“å½¢å†²å‡»æ³¢çš„æ–°å½¢æ€ç‰¹å¾ã€‚æ¡£æ¡ˆä¸­çš„H_alphaåœ°å›¾ä¸æ–°å°„ç”µè§‚æµ‹ç»“æœçš„æ¯”è¾ƒæš—ç¤ºäº†ä¸åŒçš„å‘å°„åŒºåŸŸã€ä¸åŒçš„æ¸©åº¦èŒƒå›´å’Œä¸åŒçš„æ˜Ÿé™…ä»‹è´¨å¯†åº¦ã€‚å…‰è°±æŒ‡æ•°è¡¨æ˜æ•´ä¸ªç»“æ„çš„å‘å°„æ¥æºæ˜¯ä¸€è‡´çš„ã€‚Cyg X-1å‘¨å›´çš„æ˜Ÿé™…ä»‹è´¨å¯†åº¦å¤„äºé“¶æ²³ç¯å¢ƒçš„é«˜ç«¯ï¼Œæˆ‘ä»¬çš„ç»“æœæŒ‡ç¤ºå–·å°„æµèƒ½é‡ä¼ è¾“ç‡ä½äºå…ˆå‰çš„ä¼°è®¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17635v1">PDF</a> 14 pages, 7 figures, Published in A&amp;A</p>
<p><strong>Summary</strong><br>     è§‚æµ‹Cyg X-1é»‘æ´Xå°„çº¿åŒæ˜Ÿé™„è¿‘å°„æµä¸æ˜Ÿé™…ä»‹è´¨ç›¸äº’ä½œç”¨å½¢æˆçš„å¼“å½¢å†²å‡»æ³¢ï¼Œä»¥ç ”ç©¶å°„æµèƒ½é‡å­¦ã€‚ä½¿ç”¨MeerKATæœ›è¿œé•œæˆåŠŸæ£€æµ‹åˆ°Cyg X-1åŒ—éƒ¨çš„å¼“å½¢å†²å‡»æ³¢ï¼Œå¹¶æŠ¥å‘Šå…¶å¤§å°å’Œäº®åº¦ã€‚å¼“å½¢å†²å‡»æ³¢çš„é¢‘è°±æŒ‡æ•°åˆ†å¸ƒèŒƒå›´åœ¨-0.9è‡³0.4ä¹‹é—´ï¼Œå¹¶ç¡®å®šæœªå—æƒŠæ‰°çš„æ˜Ÿé™…ä»‹è´¨å¯†åº¦å’Œæ¸©åº¦èŒƒå›´ï¼Œè¿›ä¸€æ­¥ä¼°ç®—å‡ºå¼“å½¢å†²å‡»çš„é€Ÿåº¦ã€å°„æµè´Ÿè´£æ—¶é—´å’ŒåŠŸç‡ã€‚æ¯”è¾ƒå­˜æ¡£çš„H_alphaåœ°å›¾ä¸æ–°çš„æ— çº¿ç”µè§‚æµ‹ç»“æœï¼Œæš—ç¤ºå‘å°„åŒºåŸŸã€æ¸©åº¦èŒƒå›´å’Œæ˜Ÿé™…ä»‹è´¨å¯†åº¦çš„å·®å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§‚æµ‹åˆ°äº†Cyg X-1é»‘æ´Xå°„çº¿åŒæ˜Ÿé™„è¿‘çš„å¼“å½¢å†²å‡»æ³¢ç»“æ„ï¼Œæ­¤ä¸ºç ”ç©¶å°„æµèƒ½é‡å­¦çš„é‡è¦å®éªŒã€‚</li>
<li>ä½¿ç”¨MeerKATæœ›è¿œé•œæˆåŠŸæ£€æµ‹åˆ°å¼“å½¢å†²å‡»æ³¢åœ¨Læ³¢æ®µå’ŒSæ³¢æ®µçš„å­˜åœ¨ï¼Œå¹¶æŠ¥å‘Šå…¶å°ºå¯¸å’Œäº®åº¦ä¿¡æ¯ã€‚</li>
<li>å¼“å½¢å†²å‡»æ³¢çš„é¢‘è°±æŒ‡æ•°åˆ†å¸ƒèŒƒå›´åŠå…¶è¯¯å·®åˆ†å¸ƒå·²ç¡®å®šã€‚</li>
<li>é€šè¿‡æœªå—æƒŠæ‰°çš„æ˜Ÿé™…ä»‹è´¨å¯†åº¦ä¼°è®¡äº†å¼“å½¢å†²å‡»çš„é€Ÿåº¦èŒƒå›´ä¸º21 km&#x2F;sè‡³364 km&#x2F;sã€‚</li>
<li>Cyg X-1å°„æµçš„å¹´é¾„å’ŒåŠŸç‡å·²ç»ä¼°ç®—å‡ºæ¥ï¼Œç»™å‡ºäº†å…·ä½“æ•°å€¼èŒƒå›´ã€‚</li>
<li>æ–°è§‚å¯Ÿåˆ°çš„å¼“å½¢å†²å‡»æ³¢å½¢æ€ç‰¹å¾æ­ç¤ºäº†æ›´å¤šå…³äºå°„æµä¸æ˜Ÿé™…ä»‹è´¨ç›¸äº’ä½œç”¨çš„ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a1d44f082fe8d7937af3596764b68cfa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fd6dbff82630de4c102f4db590e1f72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebf51a422cbc559c5b40603de3e7e5d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4d4a54ef03e179a2c3adceaaa6c82ae.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Occlusion-Aware-Self-Supervised-Monocular-Depth-Estimation-for-Weak-Texture-Endoscopic-Images"><a href="#Occlusion-Aware-Self-Supervised-Monocular-Depth-Estimation-for-Weak-Texture-Endoscopic-Images" class="headerlink" title="Occlusion-Aware Self-Supervised Monocular Depth Estimation for   Weak-Texture Endoscopic Images"></a>Occlusion-Aware Self-Supervised Monocular Depth Estimation for   Weak-Texture Endoscopic Images</h2><p><strong>Authors:Zebo Huang, Yinghui Wang</strong></p>
<p>We propose a self-supervised monocular depth estimation network tailored for endoscopic scenes, aiming to infer depth within the gastrointestinal tract from monocular images. Existing methods, though accurate, typically assume consistent illumination, which is often violated due to dynamic lighting and occlusions caused by GI motility. These variations lead to incorrect geometric interpretations and unreliable self-supervised signals, degrading depth reconstruction quality. To address this, we introduce an occlusion-aware self-supervised framework. First, we incorporate an occlusion mask for data augmentation, generating pseudo-labels by simulating viewpoint-dependent occlusion scenarios. This enhances the modelâ€™s ability to learn robust depth features under partial visibility. Second, we leverage semantic segmentation guided by non-negative matrix factorization, clustering convolutional activations to generate pseudo-labels in texture-deprived regions, thereby improving segmentation accuracy and mitigating information loss from lighting changes. Experimental results on the SCARED dataset show that our method achieves state-of-the-art performance in self-supervised depth estimation. Additionally, evaluations on the Endo-SLAM and SERV-CT datasets demonstrate strong generalization across diverse endoscopic environments. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹å†…çª¥é•œåœºæ™¯çš„è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡ç½‘ç»œï¼Œæ—¨åœ¨ä»å•ç›®å›¾åƒä¸­æ¨æ–­èƒƒè‚ é“å†…çš„æ·±åº¦ã€‚ç°æœ‰æ–¹æ³•è™½ç„¶å‡†ç¡®ï¼Œä½†é€šå¸¸å‡è®¾å…‰ç…§ä¸€è‡´ï¼Œè€Œç”±äºèƒƒè‚ é“çš„åŠ¨æ€ç…§æ˜å’Œç”±GIè¿åŠ¨å¼•èµ·çš„é®æŒ¡ï¼Œè¿™ä¸€å‡è®¾ç»å¸¸è¢«è¿åã€‚è¿™äº›å˜åŒ–å¯¼è‡´å‡ ä½•è§£é‡Šé”™è¯¯å’Œè‡ªç›‘ç£ä¿¡å·ä¸å¯é ï¼Œä»è€Œé™ä½äº†æ·±åº¦é‡å»ºè´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é®æŒ¡æ„ŸçŸ¥è‡ªç›‘ç£æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é‡‡ç”¨é®æŒ¡æ©è†œè¿›è¡Œæ•°æ®å¢å¼ºï¼Œé€šè¿‡æ¨¡æ‹Ÿè§†ç‚¹ç›¸å…³çš„é®æŒ¡åœºæ™¯ç”Ÿæˆä¼ªæ ‡ç­¾ã€‚è¿™å¢å¼ºäº†æ¨¡å‹åœ¨éƒ¨åˆ†å¯è§æƒ…å†µä¸‹å­¦ä¹ ç¨³å¥æ·±åº¦ç‰¹å¾çš„èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ©ç”¨éè´ŸçŸ©é˜µåˆ†è§£å¼•å¯¼è¯­ä¹‰åˆ†å‰²ï¼Œèšç±»å·ç§¯æ¿€æ´»ä»¥åœ¨çº¹ç†ç¼ºå¤±åŒºåŸŸç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œä»è€Œæé«˜åˆ†å‰²å‡†ç¡®æ€§å¹¶ç¼“è§£ç”±å…‰ç…§å˜åŒ–å¼•èµ·çš„ä¿¡æ¯æŸå¤±ã€‚åœ¨SCAREDæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è‡ªç›‘ç£æ·±åº¦ä¼°è®¡æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨Endo-SLAMå’ŒSERV-CTæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†å…¶åœ¨å¤šç§å†…çª¥é•œç¯å¢ƒä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17582v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å†…çª¥é•œåœºæ™¯çš„è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡ç½‘ç»œï¼Œæ—¨åœ¨ä»å•ç›®å›¾åƒä¸­æ¨æ–­èƒƒè‚ é“å†…çš„æ·±åº¦ã€‚ä¸ºè§£å†³å› å…‰ç…§åŠ¨æ€å˜åŒ–å’Œèƒƒè‚ é“è¿åŠ¨å¯¼è‡´çš„é®æŒ¡é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ç§é®æŒ¡æ„ŸçŸ¥è‡ªç›‘ç£æ¡†æ¶ã€‚é€šè¿‡æ•°æ®å¢å¼ºç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œæé«˜æ¨¡å‹åœ¨éƒ¨åˆ†å¯è§æƒ…å†µä¸‹çš„æ·±åº¦ç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚åŒæ—¶ï¼Œç»“åˆéè´ŸçŸ©é˜µåˆ†è§£å¼•å¯¼è¯­ä¹‰åˆ†å‰²ï¼Œæ”¹å–„çº¹ç†ç¼ºå¤±åŒºåŸŸçš„ä¼ªæ ‡ç­¾ç”Ÿæˆï¼Œæé«˜åˆ†å‰²ç²¾åº¦å¹¶å‡è½»å…‰ç…§å˜åŒ–çš„ä¿¡æ¯æŸå¤±ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‡ªæˆ‘ç›‘ç£æ·±åº¦ä¼°è®¡æ–¹é¢è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶åœ¨ä¸åŒå†…çª¥é•œç¯å¢ƒä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§è‡ªç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡ç½‘ç»œï¼Œä¸“é—¨ç”¨äºå†…çª¥é•œåœºæ™¯ã€‚</li>
<li>æ—¨åœ¨è§£å†³å› åŠ¨æ€å…‰ç…§å’Œèƒƒè‚ é“è¿åŠ¨å¯¼è‡´çš„é®æŒ¡é—®é¢˜ã€‚</li>
<li>å¼•å…¥é®æŒ¡æ„ŸçŸ¥è‡ªç›‘ç£æ¡†æ¶ï¼Œé€šè¿‡æ•°æ®å¢å¼ºç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œæé«˜æ¨¡å‹åœ¨éƒ¨åˆ†å¯è§æƒ…å†µä¸‹çš„å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>ç»“åˆéè´ŸçŸ©é˜µåˆ†è§£å¼•å¯¼è¯­ä¹‰åˆ†å‰²ï¼Œæ”¹å–„çº¹ç†ç¼ºå¤±åŒºåŸŸçš„ä¼ªæ ‡ç­¾è´¨é‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‡ªæˆ‘ç›‘ç£æ·±åº¦ä¼°è®¡æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>åœ¨ä¸åŒå†…çª¥é•œç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›å¼ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17582">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-867e9b46844ed31e9b2ed97e56b5c14e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31e31ac0327f6b8fa43dd574d98d48b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f47cbb4fe62507d6fcf42cdf00f68e1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Mamba-Sea-A-Mamba-based-Framework-with-Global-to-Local-Sequence-Augmentation-for-Generalizable-Medical-Image-Segmentation"><a href="#Mamba-Sea-A-Mamba-based-Framework-with-Global-to-Local-Sequence-Augmentation-for-Generalizable-Medical-Image-Segmentation" class="headerlink" title="Mamba-Sea: A Mamba-based Framework with Global-to-Local Sequence   Augmentation for Generalizable Medical Image Segmentation"></a>Mamba-Sea: A Mamba-based Framework with Global-to-Local Sequence   Augmentation for Generalizable Medical Image Segmentation</h2><p><strong>Authors:Zihan Cheng, Jintao Guo, Jian Zhang, Lei Qi, Luping Zhou, Yinghuan Shi, Yang Gao</strong></p>
<p>To segment medical images with distribution shifts, domain generalization (DG) has emerged as a promising setting to train models on source domains that can generalize to unseen target domains. Existing DG methods are mainly based on CNN or ViT architectures. Recently, advanced state space models, represented by Mamba, have shown promising results in various supervised medical image segmentation. The success of Mamba is primarily owing to its ability to capture long-range dependencies while keeping linear complexity with input sequence length, making it a promising alternative to CNNs and ViTs. Inspired by the success, in the paper, we explore the potential of the Mamba architecture to address distribution shifts in DG for medical image segmentation. Specifically, we propose a novel Mamba-based framework, Mamba-Sea, incorporating global-to-local sequence augmentation to improve the modelâ€™s generalizability under domain shift issues. Our Mamba-Sea introduces a global augmentation mechanism designed to simulate potential variations in appearance across different sites, aiming to suppress the modelâ€™s learning of domain-specific information. At the local level, we propose a sequence-wise augmentation along input sequences, which perturbs the style of tokens within random continuous sub-sequences by modeling and resampling style statistics associated with domain shifts. To our best knowledge, Mamba-Sea is the first work to explore the generalization of Mamba for medical image segmentation, providing an advanced and promising Mamba-based architecture with strong robustness to domain shifts. Remarkably, our proposed method is the first to surpass a Dice coefficient of 90% on the Prostate dataset, which exceeds previous SOTA of 88.61%. The code is available at <a target="_blank" rel="noopener" href="https://github.com/orange-czh/Mamba-Sea">https://github.com/orange-czh/Mamba-Sea</a>. </p>
<blockquote>
<p>é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„åˆ†å¸ƒåç§»é—®é¢˜ï¼Œé¢†åŸŸæ³›åŒ–ï¼ˆDGï¼‰ä½œä¸ºä¸€ç§åœ¨æºé¢†åŸŸè®­ç»ƒæ¨¡å‹å¹¶æ¨å¹¿è‡³æœªè§ç›®æ ‡é¢†åŸŸçš„æ–¹æ³•ï¼Œå·²æ˜¾ç¤ºå‡ºå…¶å·¨å¤§æ½œåŠ›ã€‚ç°æœ‰çš„DGæ–¹æ³•ä¸»è¦åŸºäºCNNæˆ–ViTæ¶æ„ã€‚æœ€è¿‘ï¼Œä»¥Mambaä¸ºä»£è¡¨çš„é«˜çº§çŠ¶æ€ç©ºé—´æ¨¡å‹åœ¨å¤šç§ç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚Mambaçš„æˆåŠŸä¸»è¦å½’åŠŸäºå…¶èƒ½å¤Ÿæ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»çš„åŒæ—¶ä¿æŒçº¿æ€§è®¡ç®—å¤æ‚åº¦ï¼Œä½¿å…¶æˆä¸ºCNNå’ŒViTçš„æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆã€‚å—æˆåŠŸçš„å¯å‘ï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†Mambaæ¶æ„åœ¨è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²çš„DGä¸­çš„åˆ†å¸ƒåç§»é—®é¢˜çš„æ½œåŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºMambaçš„æ–°æ¡†æ¶Mamba-Seaï¼Œå®ƒç»“åˆäº†å…¨å±€åˆ°å±€éƒ¨çš„åºåˆ—å¢å¼ºæŠ€æœ¯ï¼Œä»¥æé«˜æ¨¡å‹åœ¨é¢†åŸŸåç§»é—®é¢˜ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„Mamba-Seaå¼•å…¥äº†ä¸€ç§å…¨å±€å¢å¼ºæœºåˆ¶ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿä¸åŒç«™ç‚¹å¤–è§‚çš„æ½œåœ¨å˜åŒ–ï¼Œæ—¨åœ¨æŠ‘åˆ¶æ¨¡å‹å¯¹é¢†åŸŸç‰¹å®šä¿¡æ¯çš„å­¦ä¹ ã€‚åœ¨å±€éƒ¨å±‚é¢ï¼Œæˆ‘ä»¬æå‡ºäº†æ²¿è¾“å…¥åºåˆ—çš„åºåˆ—çº§å¢å¼ºï¼Œé€šè¿‡å»ºæ¨¡å’Œé‡æ–°é‡‡æ ·ä¸é¢†åŸŸåç§»ç›¸å…³çš„é£æ ¼ç»Ÿè®¡ä¿¡æ¯ï¼Œæ‰°åŠ¨éšæœºè¿ç»­å­åºåˆ—å†…æ ‡è®°çš„é£æ ¼ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒMamba-Seaæ˜¯é¦–æ¬¡æ¢ç´¢åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­Mambaæ³›åŒ–çš„å·¥ä½œï¼Œæä¾›äº†ä¸€ç§å…ˆè¿›ä¸”å¯¹é¢†åŸŸåç§»å…·æœ‰å¼ºå¤§ç¨³å¥æ€§çš„Mambaæ¶æ„ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å‰åˆ—è…ºæ•°æ®é›†ä¸Šçš„Diceç³»æ•°é¦–æ¬¡è¶…è¿‡äº†90ï¼…ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„æœ€ä½³çºªå½•88.61ï¼…ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/orange-czh/mamba-sea%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/orange-czh/Mamba-Seaæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17515v1">PDF</a> Accepted by IEEE TMI 2025. The code is available at   <a target="_blank" rel="noopener" href="https://github.com/orange-czh/Mamba-Sea">https://github.com/orange-czh/Mamba-Sea</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åŸºäºMambaæ¶æ„åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œä»¥åº”å¯¹åˆ†å¸ƒåç§»é—®é¢˜ã€‚æå‡ºä¸€ç§æ–°å‹Mamba-Seaæ¡†æ¶ï¼Œç»“åˆå…¨å±€åˆ°å±€éƒ¨åºåˆ—å¢å¼ºï¼Œæé«˜æ¨¡å‹åœ¨åŸŸåç§»é—®é¢˜ä¸‹çš„æ³›åŒ–æ€§èƒ½ã€‚é€šè¿‡å…¨å±€å¢å¼ºæœºåˆ¶æ¨¡æ‹Ÿä¸åŒç«™ç‚¹å¤–è§‚çš„æ½œåœ¨å˜åŒ–ï¼ŒæŠ‘åˆ¶æ¨¡å‹å¯¹åŸŸç‰¹å®šä¿¡æ¯çš„å­¦ä¹ ã€‚åœ¨å±€éƒ¨å±‚é¢ï¼Œé€šè¿‡å»ºæ¨¡å’Œé‡é‡‡æ ·ä¸åŸŸåç§»ç›¸å…³çš„é£æ ¼ç»Ÿè®¡ï¼Œå¯¹è¾“å…¥åºåˆ—è¿›è¡Œåºåˆ—å¢å¼ºã€‚Mamba-Seaåœ¨å‰åˆ—è…ºæ•°æ®é›†ä¸Šçš„Diceç³»æ•°è¶…è¿‡90%ï¼Œè¶…è¶Šä¹‹å‰çš„æœ€ä¼˜è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mambaæ¶æ„å› èƒ½æ•æ‰é•¿è·ç¦»ä¾èµ–å¹¶ä¿æŒçº¿æ€§å¤æ‚åº¦ï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>ç°æœ‰åŸŸæ³›åŒ–ï¼ˆDGï¼‰æ–¹æ³•ä¸»è¦åŸºäºCNNæˆ–ViTæ¶æ„ï¼Œè€ŒMamba-Seaæ¡†æ¶æ¢ç´¢äº†Mambaæ¶æ„åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„åŸŸæ³›åŒ–æ½œåŠ›ã€‚</li>
<li>Mamba-Seaé€šè¿‡å…¨å±€åˆ°å±€éƒ¨åºåˆ—å¢å¼ºæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼Œæ¨¡æ‹Ÿä¸åŒç«™ç‚¹çš„å¤–è§‚å˜åŒ–å¹¶æŠ‘åˆ¶åŸŸç‰¹å®šä¿¡æ¯çš„å­¦ä¹ ã€‚</li>
<li>Mamba-Seaé¦–æ¬¡è¶…è¿‡å‰åˆ—è…ºæ•°æ®é›†ä¸Šçš„Diceç³»æ•°90%ï¼Œè¡¨ç°è¶…è¶Šå…ˆå‰æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚</li>
<li>Mamba-Seaæ¡†æ¶æ˜¯é¦–ä¸ªæ¢ç´¢Mambaåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æ³›åŒ–çš„å·¥ä½œï¼Œæä¾›å¼ºå¤§ä¸”å¯¹åŸŸåç§»å…·æœ‰é²æ£’æ€§çš„MambaåŸºç¡€æ¶æ„ã€‚</li>
<li>æå‡ºçš„å…¨å±€å’Œå±€éƒ¨å¢å¼ºæœºåˆ¶æœ‰åŠ©äºæ¨¡å‹é€‚åº”ä¸åŒçš„åŸŸåç§»æƒ…å†µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17515">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b1c4b964026f29dece36c00018622bb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca3e394c2601a6df09ec17fc1bfd5459.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f407de78a2c81fd4b30711f2b572e5b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-36c2caa6846f50f675b91a2d7489cd7d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Physiological-neural-representation-for-personalised-tracer-kinetic-parameter-estimation-from-dynamic-PET"><a href="#Physiological-neural-representation-for-personalised-tracer-kinetic-parameter-estimation-from-dynamic-PET" class="headerlink" title="Physiological neural representation for personalised tracer kinetic   parameter estimation from dynamic PET"></a>Physiological neural representation for personalised tracer kinetic   parameter estimation from dynamic PET</h2><p><strong>Authors:Kartikay Tehlan, Thomas Wendler</strong></p>
<p>Dynamic positron emission tomography (PET) with [$^{18}$F]FDG enables non-invasive quantification of glucose metabolism through kinetic analysis, often modelled by the two-tissue compartment model (TCKM). However, voxel-wise kinetic parameter estimation using conventional methods is computationally intensive and limited by spatial resolution. Deep neural networks (DNNs) offer an alternative but require large training datasets and significant computational resources. To address these limitations, we propose a physiological neural representation based on implicit neural representations (INRs) for personalized kinetic parameter estimation. INRs, which learn continuous functions, allow for efficient, high-resolution parametric imaging with reduced data requirements. Our method also integrates anatomical priors from a 3D CT foundation model to enhance robustness and precision in kinetic modelling. We evaluate our approach on an [$^{18}$F]FDG dynamic PET&#x2F;CT dataset and compare it to state-of-the-art DNNs. Results demonstrate superior spatial resolution, lower mean-squared error, and improved anatomical consistency, particularly in tumour and highly vascularized regions. Our findings highlight the potential of INRs for personalized, data-efficient tracer kinetic modelling, enabling applications in tumour characterization, segmentation, and prognostic assessment. </p>
<blockquote>
<p>åŠ¨æ€æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰é€šè¿‡[$^{18}$F]FDGå®ç°äº†éä¾µå…¥æ€§çš„è‘¡è„ç³–ä»£è°¢å®šé‡è¯„ä¼°ï¼Œé€šå¸¸é€šè¿‡åŠ¨åŠ›å­¦åˆ†æè¿›è¡Œå»ºæ¨¡ï¼ŒåŠ¨åŠ›å­¦åˆ†æå¸¸å¸¸ä»¥ä¸¤ç»„ç»‡é—´éš”æ¨¡å‹ï¼ˆTCKMï¼‰ä¸ºæ¨¡å‹ã€‚ç„¶è€Œï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œä½“ç´ çº§çš„åŠ¨åŠ›å­¦å‚æ•°ä¼°è®¡åœ¨è®¡ç®—ä¸Šå¾ˆå¯†é›†ä¸”å—é™äºç©ºé—´åˆ†è¾¨ç‡ã€‚æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰æä¾›äº†å¦ä¸€ç§é€‰æ‹©ï¼Œä½†å®ƒä»¬éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®é›†å’Œé‡è¦çš„è®¡ç®—èµ„æºã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºéšç¥ç»è¡¨ç¤ºæ³•ï¼ˆINRï¼‰çš„ç”Ÿç†ç¥ç»è¡¨ç¤ºæ³•ç”¨äºä¸ªæ€§åŒ–åŠ¨åŠ›å­¦å‚æ•°ä¼°è®¡ã€‚INRå­¦ä¹ è¿ç»­å‡½æ•°ï¼Œå…è®¸é«˜æ•ˆçš„é«˜åˆ†è¾¨ç‡å‚æ•°æˆåƒå¹¶å‡å°‘æ•°æ®éœ€æ±‚ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜ç»“åˆäº†æ¥è‡ªä¸‰ç»´CTåŸºç¡€æ¨¡å‹çš„è§£å‰–å­¦å…ˆéªŒçŸ¥è¯†ï¼Œä»¥æé«˜åŠ¨åŠ›å­¦å»ºæ¨¡çš„ç¨³å¥æ€§å’Œç²¾ç¡®åº¦ã€‚æˆ‘ä»¬åœ¨ä¸€ç»„[$^{18}$F]FDGåŠ¨æ€PET&#x2F;CTæ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•å¹¶ä¸æœ€å…ˆè¿›çš„DNNsè¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰å“è¶Šçš„ç©ºé—´åˆ†è¾¨ç‡ã€æ›´ä½çš„å‡æ–¹è¯¯å·®ä»¥åŠæ›´å¥½çš„è§£å‰–å­¦ä¸€è‡´æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è‚¿ç˜¤å’Œé«˜åº¦è¡€ç®¡åŒ–çš„åŒºåŸŸä¸­ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜äº†éšç¥ç»è¡¨ç¤ºæ³•åœ¨ä¸ªæ€§åŒ–ã€é«˜æ•ˆç‡çš„ç¤ºè¸ªå‰‚åŠ¨åŠ›å­¦å»ºæ¨¡ä¸­çš„æ½œåŠ›ï¼Œå¯ä¸ºè‚¿ç˜¤ç‰¹å¾æè¿°ã€åˆ†å‰²å’Œé¢„åè¯„ä¼°ç­‰åº”ç”¨æä¾›æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17122v1">PDF</a> The code is available at: <a target="_blank" rel="noopener" href="https://github.com/tkartikay/PhysNRPET">https://github.com/tkartikay/PhysNRPET</a></p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåŠ¨æ€æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å¯é€šè¿‡$^{18}$Fæ ‡è®°çš„è„±æ°§è‘¡è„ç³–ï¼ˆFDGï¼‰è¿›è¡Œè‘¡è„ç³–ä»£è°¢çš„éä¾µå…¥æ€§å®šé‡åˆ†æã€‚å¸¸è§„æ–¹æ³•åœ¨è®¡ç®—ä¸Šè¾ƒä¸ºå¯†é›†ä¸”å—ç©ºé—´åˆ†è¾¨ç‡é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºéšç¥ç»è¡¨å¾çš„æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå®ç°ä¸ªæ€§åŒ–å‚æ•°ä¼°è®¡å’Œé«˜æ•ˆé«˜åˆ†è¾¨å‚æ•°æˆåƒï¼Œä¸”å‡å°‘äº†æ•°æ®éœ€æ±‚ï¼Œè¿˜èåˆäº†æ¥è‡ªä¸‰ç»´CTåŸºç¡€æ¨¡å‹çš„è§£å‰–å­¦å…ˆéªŒä¿¡æ¯ä»¥æå‡å…¶åœ¨åŠ¨åŠ›å­¦å»ºæ¨¡ä¸­çš„ç¨³å¥æ€§å’Œç²¾ç¡®æ€§ã€‚è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨$^{18}$Fæ ‡è®°çš„è„±æ°§è‘¡è„ç³–åŠ¨æ€PET&#x2F;CTæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„ç©ºé—´åˆ†è¾¨ç‡ã€æ›´ä½çš„å‡æ–¹è¯¯å·®å’Œæ›´å¥½çš„è§£å‰–å­¦ä¸€è‡´æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨è‚¿ç˜¤å’Œé«˜åº¦è¡€ç®¡åŒ–åŒºåŸŸã€‚è¿™ä¸ºè‚¿ç˜¤ç‰¹å¾åŒ–ã€åˆ†å‰²å’Œé¢„åè¯„ä¼°æä¾›äº†æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŠ¨æ€PETç»“åˆ$^{18}$Fæ ‡è®°çš„è„±æ°§è‘¡è„ç³–å¯ç”¨äºéä¾µå…¥æ€§åœ°è¯„ä¼°è‘¡è„ç³–ä»£è°¢ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•åœ¨å‚æ•°ä¼°è®¡ä¸Šè®¡ç®—å¯†é›†ä¸”å—ç©ºé—´åˆ†è¾¨ç‡é™åˆ¶ã€‚</li>
<li>éšç¥ç»è¡¨å¾æ–¹æ³•ç”¨äºä¸ªæ€§åŒ–å‚æ•°ä¼°è®¡ï¼Œå®ç°é«˜æ•ˆé«˜åˆ†è¾¨æˆåƒï¼Œå‡å°‘æ•°æ®éœ€æ±‚ã€‚</li>
<li>ç»“åˆä¸‰ç»´CTåŸºç¡€æ¨¡å‹çš„è§£å‰–å­¦å…ˆéªŒä¿¡æ¯å¢å¼ºåŠ¨åŠ›å­¦å»ºæ¨¡çš„ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨è‚¿ç˜¤å’Œé«˜åº¦è¡€ç®¡åŒ–åŒºåŸŸã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f9567e7420c2354604e593ac979a0ce8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e807dbe62749c56e35f90ae58f7194da.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Meta-Entity-Driven-Triplet-Mining-for-Aligning-Medical-Vision-Language-Models"><a href="#Meta-Entity-Driven-Triplet-Mining-for-Aligning-Medical-Vision-Language-Models" class="headerlink" title="Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language   Models"></a>Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language   Models</h2><p><strong>Authors:Saban Ozturk, Melih B. Yilmaz, Muti Kara, M. Talat Yavuz, Aykut KoÃ§, Tolga Ã‡ukur</strong></p>
<p>Diagnostic imaging relies on interpreting both images and radiology reports, but the growing data volumes place significant pressure on medical experts, yielding increased errors and workflow backlogs. Medical vision-language models (med-VLMs) have emerged as a powerful framework to efficiently process multimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit their performance hinges on how well image and text representations are aligned. Existing alignment methods, predominantly based on contrastive learning, prioritize separation between disease classes over segregation of fine-grained pathology attributes like location, size or severity, leading to suboptimal representations. Here, we propose MedTrim (Meta-entity-driven Triplet mining), a novel method that enhances image-text alignment through multimodal triplet learning synergistically guided by disease class as well as adjectival and directional pathology descriptors. Unlike common alignment methods that separate broad disease classes, MedTrim leverages structured meta-entity information to preserve subtle but clinically significant intra-class variations. For this purpose, we first introduce an ontology-based entity recognition module that extracts pathology-specific meta-entities from CXR reports, as annotations on pathology attributes are rare in public datasets. For refined sample selection in triplet mining, we then introduce a novel score function that captures an aggregate measure of inter-sample similarity based on disease classes and adjectival&#x2F;directional descriptors. Lastly, we introduce a multimodal triplet alignment objective for explicit within- and cross-modal alignment between samples sharing detailed pathology characteristics. Our demonstrations indicate that MedTrim improves performance in downstream retrieval and classification tasks compared to state-of-the-art alignment methods. </p>
<blockquote>
<p>è¯Šæ–­æˆåƒä¾èµ–äºå¯¹å›¾åƒå’Œæ”¾å°„å­¦æŠ¥å‘Šçš„è§£é‡Šï¼Œä½†æ—¥ç›Šå¢é•¿çš„æ•°æ®é‡ç»™åŒ»å­¦ä¸“å®¶å¸¦æ¥äº†å·¨å¤§çš„å‹åŠ›ï¼Œå¯¼è‡´äº†è¯¯å·®å¢åŠ å’Œå·¥ä½œæµç¨‹ç§¯å‹ã€‚åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆmed-VLMï¼‰ä½œä¸ºä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å¤šæ¨¡æ€æˆåƒæ•°æ®ï¼Œç‰¹åˆ«æ˜¯åœ¨èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰è¯„ä¼°ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå…¶æ€§èƒ½çš„å¥½åå–å†³äºå›¾åƒå’Œæ–‡æœ¬è¡¨ç¤ºçš„å¯¹é½ç¨‹åº¦ã€‚ç°æœ‰çš„å¯¹é½æ–¹æ³•ä¸»è¦åŸºäºå¯¹æ¯”å­¦ä¹ ï¼Œæ›´ä¾§é‡äºç–¾ç—…ç±»åˆ«ä¹‹é—´çš„åŒºåˆ†ï¼Œè€Œéç»†å¾®çš„ç—…ç†å±æ€§ï¼ˆå¦‚ä½ç½®ã€å¤§å°æˆ–ä¸¥é‡ç¨‹åº¦ï¼‰çš„åˆ†ç¦»ï¼Œå¯¼è‡´è¡¨ç¤ºä¸ä½³ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†MedTrimï¼ˆåŸºäºå…ƒå®ä½“é©±åŠ¨çš„ä¸‰å…ƒç»„æŒ–æ˜ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç–¾ç—…ç±»åˆ«ä»¥åŠå½¢å®¹è¯å’Œæ–¹å‘æ€§ç—…ç†æè¿°ç¬¦ååŒå¼•å¯¼çš„å¤šæ¨¡æ€ä¸‰å…ƒç»„å­¦ä¹ çš„æ–°æ–¹æ³•ï¼Œç”¨äºå¢å¼ºå›¾åƒæ–‡æœ¬å¯¹é½ã€‚ä¸å¸¸è§çš„ä»…æ ¹æ®ç–¾ç—…ç±»åˆ«è¿›è¡Œåˆ†ç¦»çš„å¯¹é½æ–¹æ³•ä¸åŒï¼ŒMedTrimåˆ©ç”¨ç»“æ„åŒ–çš„å…ƒå®ä½“ä¿¡æ¯æ¥ä¿ç•™å¾®å¦™çš„ä½†ä¸´åºŠä¸Šé‡è¦çš„ç±»å†…å˜åŒ–ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ä¸ªåŸºäºæœ¬ä½“è®ºçš„å®ä½“è¯†åˆ«æ¨¡å—ï¼Œä»CXRæŠ¥å‘Šä¸­æå–ç—…ç†ç‰¹å®šçš„å…ƒå®ä½“ï¼Œå› ä¸ºå…¬å…±æ•°æ®é›†ä¸­å…³äºç—…ç†å±æ€§çš„æ³¨é‡Šå¾ˆå°‘è§ã€‚ä¸ºäº†è¿›è¡Œç²¾ç»†çš„æ ·æœ¬é€‰æ‹©è¿›è¡Œä¸‰å…ƒç»„æŒ–æ˜ï¼Œç„¶åæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è¯„åˆ†å‡½æ•°ï¼Œè¯¥å‡½æ•°åŸºäºç–¾ç—…ç±»åˆ«å’Œå½¢å®¹è¯&#x2F;æ–¹å‘æ€§æè¿°ç¬¦æ¥æ•è·æ ·æœ¬é—´ç›¸ä¼¼æ€§çš„ç»¼åˆåº¦é‡ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šæ¨¡æ€ä¸‰å…ƒç»„å¯¹é½ç›®æ ‡ï¼Œç”¨äºåœ¨å…·æœ‰è¯¦ç»†ç—…ç†ç‰¹å¾çš„æ ·æœ¬ä¹‹é—´è¿›è¡Œæ˜ç¡®çš„å†…éƒ¨å’Œè·¨æ¨¡æ€å¯¹é½ã€‚æˆ‘ä»¬çš„æ¼”ç¤ºè¡¨æ˜ï¼Œä¸æœ€æ–°çš„å¯¹é½æ–¹æ³•ç›¸æ¯”ï¼ŒMedTrimåœ¨ä¸‹æ¸¸æ£€ç´¢å’Œåˆ†ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½æœ‰æ‰€æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15929v2">PDF</a> 18 pages, 7 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŒ»ç–—å½±åƒè¯Šæ–­ä¸­é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®é‡å¤§å¯¼è‡´çš„ä¸“å®¶å‹åŠ›å¢å¤§ã€é”™è¯¯å¢å¤šå’Œå·¥ä½œæµç¨‹ç§¯å‹é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºMedTrimçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å¤šæ¨¡æ€ä¸‰å…ƒç»„å­¦ä¹ ï¼Œä»¥ç–¾ç—…ç±»åˆ«åŠå½¢å®¹è¯å’Œæ–¹å‘æ€§ç—…ç†æè¿°ç¬¦ä¸ºå¼•å¯¼ï¼Œæé«˜å›¾åƒä¸æ–‡æœ¬çš„å¯¹é½æ€§èƒ½ã€‚MedTrimé‡‡ç”¨åŸºäºæœ¬ä½“çš„å®ä½“è¯†åˆ«æ¨¡å—ï¼Œä»èƒ¸è…”Xå°„çº¿æŠ¥å‘Šä¸­æå–ç—…ç†ç‰¹å¼‚æ€§å…ƒå®ä½“ï¼Œå¹¶å¼•å…¥æ–°çš„è¯„åˆ†å‡½æ•°å’Œè·¨æ¨¡æ€ä¸‰å…ƒç»„å¯¹é½ç›®æ ‡ï¼Œä»¥æ”¹è¿›æ ·æœ¬é€‰æ‹©å’Œç»†åŒ–ç—…ç†ç‰¹å¾çš„å…±äº«ã€‚æ€»ä½“è€Œè¨€ï¼ŒMedTrimæé«˜äº†ä¸‹æ¸¸æ£€ç´¢å’Œåˆ†ç±»ä»»åŠ¡çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å½±åƒè¯Šæ–­é¢ä¸´æ•°æ®é‡å¤§å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œéœ€è¦æ›´é«˜æ•ˆçš„å¤šæ¨¡æ€æ•°æ®å¤„ç†æ–¹æ³•ã€‚</li>
<li>åŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆmed-VLMsï¼‰åœ¨å¤„ç†å¤šæ¨¡æ€æˆåƒæ•°æ®æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§æ½œåŠ›ã€‚</li>
<li>ç°æœ‰å›¾åƒä¸æ–‡æœ¬å¯¹é½æ–¹æ³•ä¸»è¦å…³æ³¨ç–¾ç—…ç±»åˆ«çš„åˆ†ç¦»ï¼Œä½†å¿½ç•¥äº†ç»†ç²’åº¦ç—…ç†å±æ€§çš„è¡¨å¾ï¼Œå¯¼è‡´è¡¨ç¤ºä¸ä½³ã€‚</li>
<li>MedTrimé€šè¿‡å¤šæ¨¡æ€ä¸‰å…ƒç»„å­¦ä¹ æé«˜å›¾åƒä¸æ–‡æœ¬çš„å¯¹é½ç²¾åº¦ï¼ŒåŒæ—¶è€ƒè™‘ç–¾ç—…ç±»åˆ«å’Œå½¢å®¹è¯ã€æ–¹å‘æ€§ç—…ç†æè¿°ç¬¦ã€‚</li>
<li>MedTrimé‡‡ç”¨åŸºäºæœ¬ä½“çš„å®ä½“è¯†åˆ«æ¨¡å—ï¼Œä»èƒ¸è…”Xå°„çº¿æŠ¥å‘Šä¸­æå–ç—…ç†ç‰¹å¼‚æ€§å…ƒå®ä½“ï¼Œä»¥ä¿ç•™å…¬å…±æ•°æ®é›†ä¸­ç½•è§çš„ç—…ç†å±æ€§æ³¨é‡Šã€‚</li>
<li>å¼•å…¥æ–°çš„è¯„åˆ†å‡½æ•°ï¼ŒåŸºäºç–¾ç—…ç±»åˆ«å’Œå½¢å®¹è¯&#x2F;æ–¹å‘æ€§æè¿°ç¬¦æ¥è¡¡é‡æ ·æœ¬é—´çš„ç›¸ä¼¼åº¦ï¼Œä»¥ä¼˜åŒ–æ ·æœ¬é€‰æ‹©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15929">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-077321fcf7a12406024830d72c6455ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2f84989aa64125a6a8ae84fab8dab60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ca291eeefc0ffdef1d6c755ed0f8c2c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Static-linear-density-response-from-X-ray-Thomson-scattering-measurements-a-case-study-of-warm-dense-beryllium"><a href="#Static-linear-density-response-from-X-ray-Thomson-scattering-measurements-a-case-study-of-warm-dense-beryllium" class="headerlink" title="Static linear density response from X-ray Thomson scattering   measurements: a case study of warm dense beryllium"></a>Static linear density response from X-ray Thomson scattering   measurements: a case study of warm dense beryllium</h2><p><strong>Authors:Sebastian Schwalbe, Hannah Bellenbaum, Tilo DÃ¶ppner, Maximilian BÃ¶hme, Thomas Gawne, Dominik Kraus, Michael J. MacDonald, Zhandos Moldabekov, Panagiotis Tolias, Jan Vorberger, Tobias Dornheim</strong></p>
<p>Linear response theory is ubiquitous throughout physics and plays a central role in the theoretical description of warm dense matter â€“ an extreme state that occurs within compact astrophysical objects and that is traversed on the compression path of a fuel capsule in inertial confinement fusion applications. Here we show how one can relate the static linear density response function to X-ray Thomson scattering (XRTS) measurements, which opens up new possibilities for the diagnostics of extreme states of matter, and for the rigorous assessment and verification of theoretical models and approximations. As a practical example, we consider an XRTS data set of warm dense beryllium taken at the National Ignition Facility [T.<del>D&quot;oppner \emph{et al.}, \textit{Nature} \textbf{618}, 270-275 (2023)]. The comparison with state-of-the-art \emph{ab initio} path integral Monte Carlo (PIMC) simulations [T.</del>Dornheim \emph{et al.}, \textit{Nature Commun.}~(in print), arXiv:2402.19113] gives us a best estimate of the mass density of $\rho&#x3D;18\pm6,$g&#x2F;cc, which is consistent with previous PIMC and density functional theory based studies, but rules out the original estimate of $\rho&#x3D;34\pm4,$g&#x2F;cc based on a Chihara model fit. </p>
<blockquote>
<p>çº¿æ€§å“åº”ç†è®ºåœ¨ç‰©ç†å­¦ä¸­æ™®éå­˜åœ¨ï¼Œå¹¶åœ¨æè¿°æ¸©æš–è‡´å¯†ç‰©è´¨çš„ç†è®ºä¸­èµ·ç€æ ¸å¿ƒä½œç”¨ã€‚è¿™æ˜¯ä¸€ç§æç«¯çš„ç‰©è´¨çŠ¶æ€ï¼Œå‡ºç°åœ¨è‡´å¯†çš„å¤©ä½“ç‰©ç†å¯¹è±¡ä¸­ï¼Œä¹Ÿå‡ºç°åœ¨æƒ¯æ€§çº¦æŸèšå˜åº”ç”¨çš„ç‡ƒæ–™èƒ¶å›Šå‹ç¼©è·¯å¾„ä¸Šã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†é™æ€çº¿æ€§å¯†åº¦å“åº”å‡½æ•°ä¸Xå°„çº¿æ±¤å§†æ£®æ•£å°„ï¼ˆXRTSï¼‰æµ‹é‡ç›¸å…³è”ï¼Œè¿™ä¸ºæç«¯ç‰©è´¨çŠ¶æ€çš„è¯Šæ–­ã€ç†è®ºæ¨¡å‹å’Œè¿‘ä¼¼çš„ä¸¥æ ¼è¯„ä¼°å’ŒéªŒè¯æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚ä½œä¸ºä¸€ä¸ªå®é™…ä¾‹å­ï¼Œæˆ‘ä»¬è€ƒè™‘äº†åœ¨å›½å®¶ç‚¹ç«è®¾æ–½ï¼ˆT. Doppnerç­‰äººï¼Œã€Šè‡ªç„¶ã€‹æ‚å¿—ï¼Œç¬¬618æœŸï¼Œç¬¬270-275é¡µï¼ˆ2023å¹´ï¼‰ï¼‰è·å¾—çš„æ¸©æš–è‡´å¯†é“çš„XRTSæ•°æ®é›†ã€‚ä¸æœ€æ–°æœ€å…ˆè¿›çš„ä»å¤´å¼€å§‹è·¯å¾„ç§¯åˆ†è’™ç‰¹å¡ç½—ï¼ˆPIMCï¼‰æ¨¡æ‹Ÿï¼ˆT. Dornheimç­‰äººï¼Œã€Šè‡ªç„¶é€šè®¯ã€‹ï¼ˆå°åˆ·ä¸­ï¼‰ï¼ŒarXivï¼š2402.19113ï¼‰çš„æ¯”è¾ƒï¼Œä¸ºæˆ‘ä»¬æä¾›äº†æœ€ä½³è´¨é‡å¯†åº¦ä¼°è®¡å€¼ä¸ºÏ&#x3D;18Â±6 g&#x2F;ccã€‚è¿™ä¸ä¹‹å‰çš„PIMCå’ŒåŸºäºå¯†åº¦æ³›å‡½ç†è®ºçš„ç ”ç©¶ç›¸ä¸€è‡´ï¼Œä½†æ’é™¤äº†åŸºäºChiharaæ¨¡å‹æ‹Ÿåˆå¾—åˆ°çš„åŸå§‹ä¼°è®¡å€¼Ï&#x3D;34Â±4 g&#x2F;ccã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13611v2">PDF</a> </p>
<p><strong>Summary</strong><br>     çº¿æ€§å“åº”ç†è®ºåœ¨ç‰©ç†å­¦ä¸­æ™®éå­˜åœ¨ï¼Œå¯¹äºæè¿°çƒ­å¯†ç‰©è´¨çš„ç†è®ºèµ·åˆ°äº†æ ¸å¿ƒä½œç”¨ã€‚æœ¬æ–‡é€šè¿‡å±•ç¤ºé™æ€çº¿æ€§å¯†åº¦å“åº”å‡½æ•°ä¸Xå°„çº¿æ±¤å§†æ£®æ•£å°„æµ‹é‡çš„å…³ç³»ï¼Œä¸ºæç«¯æ€ç‰©è´¨çš„è¯Šæ–­ä»¥åŠç†è®ºæ¨¡å‹å’Œè¿‘ä¼¼å€¼çš„ä¸¥æ ¼è¯„ä¼°å’ŒéªŒè¯æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚ä»¥ç¾å›½å›½å®¶ç‚¹ç«è£…ç½®å¯¹çƒ­å¯†æ€é“ç‰©è´¨çš„ç ”ç©¶ä¸ºä¾‹ï¼Œä¸æœ€æ–°çš„ä»å¤´ç®—è·¯å¾„ç§¯åˆ†è’™ç‰¹å¡ç½—æ¨¡æ‹Ÿç›¸æ¯”è¾ƒï¼Œå¾—åˆ°æœ€ä½³ä¼°è®¡ç‰©è´¨å¯†åº¦ä¸º$\rho&#x3D;18\pm6g&#x2F;cc$ï¼Œä¸å‰äººç ”ç©¶ç»“æœä¸€è‡´ï¼Œæ’é™¤äº†åŸºäºå¥‡å“ˆæ‹‰æ¨¡å‹æ‹Ÿåˆçš„åŸå§‹ä¼°è®¡$\rho&#x3D;34\pm4g&#x2F;cc$ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çº¿æ€§å“åº”ç†è®ºåœ¨æè¿°çƒ­å¯†ç‰©è´¨çš„ç†è®ºä¸­æ‰®æ¼”æ ¸å¿ƒè§’è‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰©ç†å­¦é¢†åŸŸã€‚</li>
<li>é€šè¿‡å°†é™æ€çº¿æ€§å¯†åº¦å“åº”å‡½æ•°ä¸Xå°„çº¿æ±¤å§†æ£®æ•£å°„æµ‹é‡ç›¸ç»“åˆï¼Œä¸ºæç«¯çŠ¶æ€ç‰©è´¨çš„è¯Šæ–­å’Œç†è®ºæ¨¡å‹çš„è¯„ä¼°æä¾›äº†æ–°çš„æ–¹æ³•ã€‚</li>
<li>æ–‡ç« ä»¥ç¾å›½å›½å®¶ç‚¹ç«è£…ç½®å¯¹çƒ­å¯†æ€é“ç‰©è´¨çš„ç ”ç©¶ä¸ºä¾‹ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•çš„åº”ç”¨ã€‚</li>
<li>ä¸æœ€æ–°çš„ä»å¤´ç®—è·¯å¾„ç§¯åˆ†è’™ç‰¹å¡ç½—æ¨¡æ‹Ÿç›¸æ¯”è¾ƒï¼Œå¾—åˆ°äº†ç‰©è´¨å¯†åº¦çš„æœ€ä½³ä¼°è®¡å€¼ã€‚</li>
<li>æœ€ä½³ä¼°è®¡ç‰©è´¨å¯†åº¦ä¸º$\rho&#x3D;18\pm6g&#x2F;cc$ï¼Œä¸å‰äººç ”ç©¶ç»“æœä¸€è‡´ã€‚</li>
<li>æ’é™¤åŸºäºå¥‡å“ˆæ‹‰æ¨¡å‹æ‹Ÿåˆçš„åŸå§‹ä¼°è®¡$\rho&#x3D;34\pm4g&#x2F;cc$ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13611">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-05b1e8bce7611bee1074e7b3343565b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc221e242d3621eb574c061234735577.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fd360b6f56fe56da9d12d1074df1263.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="OmniMamba4D-Spatio-temporal-Mamba-for-longitudinal-CT-lesion-segmentation"><a href="#OmniMamba4D-Spatio-temporal-Mamba-for-longitudinal-CT-lesion-segmentation" class="headerlink" title="OmniMamba4D: Spatio-temporal Mamba for longitudinal CT lesion   segmentation"></a>OmniMamba4D: Spatio-temporal Mamba for longitudinal CT lesion   segmentation</h2><p><strong>Authors:Justin Namuk Kim, Yiqiao Liu, Rajath Soans, Keith Persson, Sarah Halek, Michal Tomaszewski, Jianda Yuan, Gregory Goldmacher, Antong Chen</strong></p>
<p>Accurate segmentation of longitudinal CT scans is important for monitoring tumor progression and evaluating treatment responses. However, existing 3D segmentation models solely focus on spatial information. To address this gap, we propose OmniMamba4D, a novel segmentation model designed for 4D medical images (3D images over time). OmniMamba4D utilizes a spatio-temporal tetra-orientated Mamba block to effectively capture both spatial and temporal features. Unlike traditional 3D models, which analyze single-time points, OmniMamba4D processes 4D CT data, providing comprehensive spatio-temporal information on lesion progression. Evaluated on an internal dataset comprising of 3,252 CT scans, OmniMamba4D achieves a competitive Dice score of 0.682, comparable to state-of-the-arts (SOTA) models, while maintaining computational efficiency and better detecting disappeared lesions. This work demonstrates a new framework to leverage spatio-temporal information for longitudinal CT lesion segmentation. </p>
<blockquote>
<p>å¯¹çºµå‘CTæ‰«æè¿›è¡Œç²¾ç¡®åˆ†å‰²å¯¹äºç›‘æµ‹è‚¿ç˜¤è¿›å±•å’Œè¯„ä¼°æ²»ç–—æ•ˆæœéå¸¸é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„3Dåˆ†å‰²æ¨¡å‹åªå…³æ³¨ç©ºé—´ä¿¡æ¯ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†OmniMamba4Dï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸º4DåŒ»å­¦å›¾åƒï¼ˆéšæ—¶é—´å˜åŒ–çš„3Då›¾åƒï¼‰è®¾è®¡çš„å…¨æ–°åˆ†å‰²æ¨¡å‹ã€‚OmniMamba4Dåˆ©ç”¨æ—¶ç©ºå››é¢ä½“å®šå‘çš„Mambaå—ï¼Œæœ‰æ•ˆåœ°æ•æ‰ç©ºé—´å’Œæ—¶é—´ç‰¹å¾ã€‚ä¸ä¼ ç»Ÿçš„ä»…åˆ†æå•ä¸€æ—¶é—´ç‚¹çš„3Dæ¨¡å‹ä¸åŒï¼ŒOmniMamba4Då¤„ç†4D CTæ•°æ®ï¼Œæä¾›å…³äºç—…ç¶è¿›å±•çš„å…¨é¢æ—¶ç©ºä¿¡æ¯ã€‚åœ¨åŒ…å«3252æ¬¡CTæ‰«æçš„å†…éƒ¨æ•°æ®é›†ä¸Šè¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼ŒOmniMamba4Dçš„Diceåˆ†æ•°è¾¾åˆ°äº†å…·æœ‰ç«äº‰åŠ›çš„0.682ï¼Œä¸æœ€å…ˆè¿›ï¼ˆSOTAï¼‰æ¨¡å‹ç›¸æ¯”ä¸ç›¸ä¸Šä¸‹ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ï¼Œå¹¶æ›´å¥½åœ°æ£€æµ‹åˆ°äº†æ¶ˆå¤±çš„ç—…å˜ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†ä¸€ä¸ªåˆ©ç”¨æ—¶ç©ºä¿¡æ¯è¿›è¡Œçºµå‘CTç—…ç¶åˆ†å‰²çš„æ–°æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09655v2">PDF</a> Accepted at IEEE International Symposium on Biomedical Imaging (ISBI)   2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒå‡†ç¡®åˆ†å‰²å¯¹ç›‘æµ‹è‚¿ç˜¤è¿›å±•å’Œè¯„ä¼°æ²»ç–—æ•ˆæœè‡³å…³é‡è¦ã€‚ç°æœ‰ä¸‰ç»´åˆ†å‰²æ¨¡å‹ä¸»è¦å…³æ³¨ç©ºé—´ä¿¡æ¯ï¼Œä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºOmniMamba4Dæ¨¡å‹ï¼Œä¸“ä¸ºå››ç»´åŒ»å­¦å›¾åƒè®¾è®¡ï¼Œå¯æ•æ‰æ—¶ç©ºç‰¹å¾ã€‚è¯¥æ¨¡å‹åœ¨åŒ…å«3252æ¬¡CTæ‰«æçš„å†…éƒ¨æ•°æ®é›†ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„Diceåˆ†æ•°ï¼ˆ0.682ï¼‰ï¼Œå¯ä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸ï¿½ï¿½eæ¯”è¾ƒï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡å’Œæ›´å¥½çš„æ¶ˆå¤±ç—…å˜æ£€æµ‹èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒå‡†ç¡®åˆ†å‰²å¯¹è‚¿ç˜¤è¿›å±•ç›‘æµ‹å’Œæ²»ç–—ååº”è¯„ä¼°è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ä¸‰ç»´åˆ†å‰²æ¨¡å‹ä¸»è¦å…³æ³¨ç©ºé—´ä¿¡æ¯ï¼Œå¿½ç•¥äº†æ—¶é—´ç»´åº¦ã€‚</li>
<li>OmniMamba4Dæ¨¡å‹ä¸“ä¸ºå››ç»´åŒ»å­¦å›¾åƒè®¾è®¡ï¼Œç»“åˆäº†æ—¶ç©ºç‰¹å¾ã€‚</li>
<li>OmniMamba4Dæ¨¡å‹ä½¿ç”¨æ—¶ç©ºå››æ–¹å‘Mambaå—è¿›è¡Œæœ‰æ•ˆç‰¹å¾æ•æ‰ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å†…éƒ¨æ•°æ®é›†ä¸Šå–å¾—äº†ç«äº‰åŠ›çš„Diceåˆ†æ•°ï¼Œä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸å½“ã€‚</li>
<li>OmniMamba4Dæ¨¡å‹åœ¨è®¡ç®—æ•ˆç‡å’Œæ¶ˆå¤±ç—…å˜æ£€æµ‹æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cce4b9cecf147d33cfee9c7bdd53102c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3999d6e301fe7f96cbbd74fcf496f918.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8706fe0cc12d7a6040dc3699bf8972ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adc8986214f5a0ac4cd75e33e7e711a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22678d39febc678d85bfad7f4a458267.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Q-ball-mechanism-of-electron-transport-properties-of-high-T-c-superconductors"><a href="#Q-ball-mechanism-of-electron-transport-properties-of-high-T-c-superconductors" class="headerlink" title="Q-ball mechanism of electron transport properties of high-T$_c$   superconductors"></a>Q-ball mechanism of electron transport properties of high-T$_c$   superconductors</h2><p><strong>Authors:S. I. Mukhin</strong></p>
<p>Proposed recently by the author Q-ball mechanism of the pseudogap state and high-Tc superconductivity in cuprates (2022) was supported by micro X-ray diffraction data in HgBa$<em>2$CuO$</em>{4+y}$ (2023). In the present paper it is demonstrated that T-linear temperature dependence of electrical resistivity arises naturally in the Q-ball gas phase, that may explain corresponding experimental data in the â€œstrange metalâ€ phase of high-T$_c$ cuprates, as reviewed by Barisic et al. (2013). In the present theory it arises due to scattering of electrons on the Q-balls gas of condensed charge&#x2F;spin fluctuations. Close to the lowest temperature boundary of the â€œstrange metalâ€ phase, at which Q-ball radius diverges, electrical resistivity caused by a slide of the Q-balls as a whole is calculated using fluctuation paraconductivity calculation method by Alex Abrikosov (1987). The diamagnetic response of Q-balls gas is calculated as well and shows good accord with experimental data by L.Li et al. (2010) in the â€œstrange metalâ€ phase. In total, obtained results demonstrate different properties of the correlated electrons systems that arise due to formation of Q-balls possessing internal bosonic frequency $\Omega&#x3D;2\pi nT$ in Matsubara time and, thus, forming the quantum thermodynamic time polycrystals. Presented theory may give a clue concerning a possible mechanism of the experimentally measured properties of high-T$_c$ cuprates in the â€œstrange metalâ€ phase of their phase diagram. We believe , these results provide support to the quantum thermodynamic time crystal model of the Euclidean Q-balls considered in the present paper. </p>
<blockquote>
<p>ä½œè€…è¿‘æœŸæå‡ºçš„å…³äºä¼ªé—´éš™æ€å’Œé«˜æ¸©è¶…å¯¼æ€§çš„Qçƒæœºåˆ¶ï¼ˆ2022å¹´ï¼‰å¾—åˆ°äº†æ±åŸºé“œé…¸ç›ï¼ˆHgBa$<em>2$CuO$</em>{4+y}$ï¼‰ä¸­çš„å¾®Xå°„çº¿è¡å°„æ•°æ®æ”¯æŒï¼ˆ2023å¹´ï¼‰ã€‚æœ¬æ–‡å±•ç¤ºäº†çº¿æ€§æ¸©åº¦ä¾èµ–çš„ç”µé˜»ç‡è‡ªç„¶äº§ç”ŸäºQçƒæ°”æ€ç›¸ä¸­ï¼Œè¿™å¯ä»¥è§£é‡Šé«˜æ¸©é“œé…¸ç›çš„â€œå¥‡å¼‚é‡‘å±â€ç›¸ä¸­ç›¸åº”çš„å®éªŒæ•°æ®ï¼Œå¦‚å·´é‡Œæ–¯ç­‰äººï¼ˆBarisic et al.ï¼Œ2013ï¼‰æ‰€è¿°ã€‚æ ¹æ®å½“å‰ç†è®ºï¼Œè¿™æ˜¯ç”±äºç”µå­åœ¨å‡èšçš„ç”µè·&#x2F;è‡ªæ—‹æ³¢åŠ¨å½¢æˆçš„Qçƒæ°”ä½“ä¸Šçš„æ•£å°„æ‰€è‡´ã€‚åœ¨â€œå¥‡å¼‚é‡‘å±â€ç›¸çš„æœ€ä½æ¸©åº¦è¾¹ç•Œé™„è¿‘ï¼ŒQçƒåŠå¾„å‘æ•£ï¼Œæ•´ä¸ªQçƒçš„æ»‘åŠ¨å¼•èµ·çš„ç”µé˜»ç‡æ˜¯é€šè¿‡äºšå†å…‹æ–¯Â·é˜¿å¸ƒé‡Œç§‘ç´¢å¤«çš„æ¶¨è½è¶…å¯¼è®¡ç®—æ³•ï¼ˆAlex Abrikosovï¼Œ1987ï¼‰è®¡ç®—çš„ã€‚åŒæ—¶ï¼Œè¿˜è®¡ç®—äº†Qçƒæ°”ä½“çš„æŠ—ç£æ€§å“åº”ï¼Œè¿™ä¸ææ–‡ç­‰äººåœ¨â€œå¥‡å¼‚é‡‘å±â€ç›¸ä¸­çš„å®éªŒç»“æœï¼ˆL.Li et al.ï¼Œ2010ï¼‰å»åˆè‰¯å¥½ã€‚æ€»ä½“è€Œè¨€ï¼Œè·å¾—çš„ç»“æœå±•ç¤ºäº†ç”±äºå½¢æˆå…·æœ‰å†…éƒ¨ç»è‰²é¢‘ç‡$\Omega&#x3D;2\pi nT$çš„é©¬ä¿®å·´æ—¶é—´ä¸­çš„Qçƒè€Œäº§ç”Ÿçš„å…³è”ç”µå­ç³»ç»Ÿçš„ä¸åŒç‰¹æ€§ï¼Œä»è€Œå½¢æˆäº†é‡å­çƒ­åŠ›å­¦æ—¶é—´å¤šæ™¶ã€‚æœ¬æ–‡æå‡ºçš„ç†è®ºå¯èƒ½ä¸ºé«˜æ¸©é“œé…¸ç›çš„â€œå¥‡å¼‚é‡‘å±â€ç›¸ä¸­å®éªŒæµ‹é‡çš„æ€§è´¨æä¾›å¯èƒ½çš„æœºåˆ¶çº¿ç´¢ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œè¿™äº›ç»“æœä¸ºæœ¬æ–‡ä¸­è€ƒè™‘çš„æ¬§å‡ é‡Œå¾—Qçƒçš„é‡å­çƒ­åŠ›å­¦æ—¶é—´æ™¶ä½“æ¨¡å‹æä¾›äº†æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09610v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸä½œè€…æå‡ºçš„ä¼ªé—´éš™æ€çš„Qçƒæœºåˆ¶å’Œé«˜æ¸©è¶…å¯¼åœ¨é“œé…¸ç›ä¸­çš„è¡¨ç°å¾—åˆ°äº†å¾®Xå°„çº¿è¡å°„æ•°æ®çš„æ”¯æŒã€‚æœ¬æ–‡å±•ç¤ºäº†çº¿æ€§æ¸©åº¦ä¾èµ–æ€§çš„ç”µé˜»ç‡è‡ªç„¶äº§ç”ŸäºQçƒæ°”æ€ï¼Œè¿™è§£é‡Šäº†é«˜æ¸©é“œé…¸ç›çš„â€œå¥‡å¼‚é‡‘å±â€ç›¸ä¸­çš„å®éªŒæ•°æ®ã€‚åœ¨è¯¥ç†è®ºä¸­ï¼Œå®ƒæºäºç”µå­åœ¨å‡èšçš„ç”µè·&#x2F;è‡ªæ—‹æ³¢åŠ¨çš„Qçƒæ°”ä½“ä¸Šçš„æ•£å°„ã€‚åœ¨â€œå¥‡å¼‚é‡‘å±â€ç›¸çš„æœ€ä½æ¸©åº¦è¾¹ç•Œé™„è¿‘ï¼ŒQçƒçš„åŠå¾„å‘æ•£ï¼Œç”µå­é€šè¿‡æ•´ä½“çš„Qçƒè¿åŠ¨é€ æˆçš„ç”µé˜»ç‡çš„è®¡ç®—ä½¿ç”¨é˜¿åˆ—å…‹æ–¯Â·é˜¿å¸ƒç‘ç§‘å¤«æ–¯çš„æ¶¨è½å‡†å¯¼è®¡ç®—æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è®¡ç®—äº†Qçƒæ°”ä½“çš„é¡ºç£æ€§å“åº”ï¼Œå¹¶ä¸å®éªŒæ•°æ®è¡¨ç°ç›¸ç¬¦ã€‚ç»¼ä¸Šï¼Œè·å¾—çš„ç»“æœè¡¨æ˜äº†å› åœ¨Matsubaraæ—¶é—´ä¸­äº§ç”Ÿå†…éƒ¨æ³¢è‰²é¢‘ç‡è€Œå½¢æˆçš„Qçƒå±•ç°å‡ºä¸åŒçš„ç”µå­ç³»ç»Ÿç‰¹æ€§ã€‚æ­¤ç†è®ºä¸ºæˆ‘ä»¬æä¾›äº†é«˜æ¸©é“œé…¸ç›çš„â€œå¥‡å¼‚é‡‘å±â€ç›¸åœ¨å®éªŒæµ‹é‡æ€§èƒ½æ–¹é¢çš„çº¿ç´¢ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œè¿™äº›ç»“æœä¸ºé‡å­çƒ­åŠ›å­¦æ—¶é—´æ™¶ä½“æ¨¡å‹æä¾›äº†æ”¯æŒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Qçƒæœºåˆ¶ç†è®ºå¾—åˆ°å¾®Xå°„çº¿è¡å°„æ•°æ®çš„æ”¯æŒï¼Œè¯å®å­˜åœ¨äºHgBa$<em>2$CuO$</em>{4+y}$ä¸­ã€‚</li>
<li>Qçƒæ°”æ€è‡ªç„¶äº§ç”ŸTçº¿æ€§æ¸©åº¦ä¾èµ–æ€§çš„ç”µé˜»ç‡ï¼Œä¸é«˜æ¸©é“œé…¸ç›çš„â€œå¥‡å¼‚é‡‘å±â€ç›¸å®éªŒæ•°æ®ç›¸ç¬¦ã€‚</li>
<li>ç”µå­åœ¨å‡èšçš„ç”µè·&#x2F;è‡ªæ—‹æ³¢åŠ¨çš„Qçƒæ°”ä½“ä¸Šçš„æ•£å°„å¯¼è‡´ç”µé˜»ç‡äº§ç”Ÿã€‚</li>
<li>åœ¨â€œå¥‡å¼‚é‡‘å±â€ç›¸çš„æœ€ä½æ¸©åº¦è¾¹ç•Œé™„è¿‘ï¼ŒQçƒçš„åŠå¾„å‘æ•£ï¼Œç”µé˜»ç‡é€šè¿‡æ•´ä½“çš„Qçƒè¿åŠ¨è®¡ç®—å¾—å‡ºã€‚</li>
<li>Qçƒæ°”ä½“çš„é¡ºç£æ€§å“åº”ä¸å®éªŒæ•°æ®ç›¸ç¬¦ã€‚</li>
<li>Qçƒå…·æœ‰å†…éƒ¨æ³¢è‰²é¢‘ç‡ï¼Œå±•ç¤ºäº†ç”µå­ç³»ç»Ÿçš„ä¸åŒç‰¹æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09610">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1e6c261e1afbda90808f41b84b22aeb2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DiffKillR-Killing-and-Recreating-Diffeomorphisms-for-Cell-Annotation-in-Dense-Microscopy-Images"><a href="#DiffKillR-Killing-and-Recreating-Diffeomorphisms-for-Cell-Annotation-in-Dense-Microscopy-Images" class="headerlink" title="DiffKillR: Killing and Recreating Diffeomorphisms for Cell Annotation in   Dense Microscopy Images"></a>DiffKillR: Killing and Recreating Diffeomorphisms for Cell Annotation in   Dense Microscopy Images</h2><p><strong>Authors:Chen Liu, Danqi Liao, Alejandro Parada-Mayorga, Alejandro Ribeiro, Marcello DiStasio, Smita Krishnaswamy</strong></p>
<p>The proliferation of digital microscopy images, driven by advances in automated whole slide scanning, presents significant opportunities for biomedical research and clinical diagnostics. However, accurately annotating densely packed information in these images remains a major challenge. To address this, we introduce DiffKillR, a novel framework that reframes cell annotation as the combination of archetype matching and image registration tasks. DiffKillR employs two complementary neural networks: one that learns a diffeomorphism-invariant feature space for robust cell matching and another that computes the precise warping field between cells for annotation mapping. Using a small set of annotated archetypes, DiffKillR efficiently propagates annotations across large microscopy images, reducing the need for extensive manual labeling. More importantly, it is suitable for any type of pixel-level annotation. We will discuss the theoretical properties of DiffKillR and validate it on three microscopy tasks, demonstrating its advantages over existing supervised, semi-supervised, and unsupervised methods. The code is available at <a target="_blank" rel="noopener" href="https://github.com/KrishnaswamyLab/DiffKillR">https://github.com/KrishnaswamyLab/DiffKillR</a>. </p>
<blockquote>
<p>éšç€è‡ªåŠ¨å…¨åˆ‡ç‰‡æ‰«ææŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ï¼Œæ•°å­—æ˜¾å¾®é•œå›¾åƒçš„æ¿€å¢ä¸ºç”Ÿç‰©åŒ»å­¦ç ”ç©¶å’Œä¸´åºŠè¯Šæ–­æä¾›äº†é‡è¦çš„æœºä¼šã€‚ç„¶è€Œï¼Œåœ¨è¿™äº›å›¾åƒä¸­å‡†ç¡®æ ‡æ³¨å¯†é›†çš„ä¿¡æ¯ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DiffKillRï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå®ƒå°†ç»†èƒæ³¨é‡Šé‡æ–°æ„å»ºä¸ºåŸå‹åŒ¹é…å’Œå›¾åƒé…å‡†ä»»åŠ¡çš„ç»„åˆã€‚DiffKillRé‡‡ç”¨ä¸¤ä¸ªäº’è¡¥çš„ç¥ç»ç½‘ç»œï¼šä¸€ä¸ªå­¦ä¹ å¾®åˆ†åŒèƒšä¸å˜çš„ç‰¹å¾ç©ºé—´ä»¥å®ç°ç¨³å¥çš„ç»†èƒåŒ¹é…ï¼Œå¦ä¸€ä¸ªè®¡ç®—ç»†èƒä¹‹é—´çš„ç²¾ç¡®æ‰­æ›²åœºä»¥å®ç°æ³¨é‡Šæ˜ å°„ã€‚ä½¿ç”¨ä¸€å°éƒ¨åˆ†å·²æ³¨é‡Šçš„åŸå‹ï¼ŒDiffKillRå¯ä»¥æœ‰æ•ˆåœ°åœ¨å¤§æ˜¾å¾®é•œå›¾åƒä¸Šä¼ æ’­æ³¨é‡Šï¼Œå‡å°‘äº†å¤§é‡æ‰‹åŠ¨æ ‡è®°çš„éœ€æ±‚ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒé€‚ç”¨äºä»»ä½•ç±»å‹çš„åƒç´ çº§æ³¨é‡Šã€‚æˆ‘ä»¬å°†è®¨è®ºDiffKillRçš„ç†è®ºå±æ€§ï¼Œå¹¶åœ¨ä¸‰ä¸ªæ˜¾å¾®é•œä»»åŠ¡ä¸Šå¯¹å…¶è¿›è¡ŒéªŒè¯ï¼Œå±•ç¤ºå…¶ä¸ç°æœ‰çš„ç›‘ç£ã€åŠç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•ç›¸æ¯”çš„ä¼˜åŠ¿ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/KrishnaswamyLab/DiffKillR">https://github.com/KrishnaswamyLab/DiffKillR</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03058v2">PDF</a> ICASSP 2025, Oral Presentation</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒæ•°å­—åŒ–è¿›å±•è¿…é€Ÿï¼Œå…¨è‡ªåŠ¨æ‰«ææŠ€æœ¯æ¨åŠ¨å¤§é‡æ•°å­—æ˜¾å¾®é•œå›¾åƒç”Ÿæˆï¼Œä¸ºç”Ÿç‰©åŒ»å­¦ç ”ç©¶å’Œä¸´åºŠè¯Šç–—æä¾›é‡è¦æœºé‡ã€‚ç„¶è€Œï¼Œå‡†ç¡®æ ‡æ³¨å¯†é›†å›¾åƒä¿¡æ¯æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œå¼•å…¥DiffKillRæ¡†æ¶ï¼Œå°†ç»†èƒæ ‡æ³¨è½¬åŒ–ä¸ºåŸå‹åŒ¹é…å’Œå›¾åƒæ³¨å†Œä»»åŠ¡ç»„åˆã€‚DiffKillRé‡‡ç”¨ä¸¤ä¸ªäº’è¡¥ç¥ç»ç½‘ç»œï¼Œä¸€ä¸ªå­¦ä¹ ä¿å½¢ä¸å˜çš„ç‰¹æ€§ç©ºé—´ä»¥å®ç°ç¨³å¥çš„ç»†èƒåŒ¹é…ï¼Œå¦ä¸€ä¸ªè®¡ç®—ç»†èƒé—´çš„ç²¾ç¡®å˜å½¢åœºä»¥å®ç°æ ‡æ³¨æ˜ å°„ã€‚ä»…éœ€å°‘é‡å·²æ ‡æ³¨åŸå‹ï¼ŒDiffKillRå³å¯åœ¨å¤§æ˜¾å¾®é•œå›¾åƒä¸Šæœ‰æ•ˆä¼ æ’­æ ‡æ³¨ï¼Œå‡å°‘å¤§é‡æ‰‹åŠ¨æ ‡æ³¨éœ€æ±‚ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒé€‚ç”¨äºä»»ä½•åƒç´ çº§åˆ«çš„æ ‡æ³¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°å­—æ˜¾å¾®é•œå›¾åƒçš„æ™®åŠå¯¹ç”Ÿç‰©åŒ»å­¦ç ”ç©¶å’Œä¸´åºŠè¯Šç–—æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>å‡†ç¡®æ ‡æ³¨å¯†é›†å›¾åƒä¿¡æ¯æ˜¯å½“å‰çš„æŒ‘æˆ˜ã€‚</li>
<li>DiffKillRæ¡†æ¶é€šè¿‡å°†ç»†èƒæ ‡æ³¨è½¬åŒ–ä¸ºåŸå‹åŒ¹é…å’Œå›¾åƒæ³¨å†Œä»»åŠ¡ç»„åˆæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>DiffKillRä½¿ç”¨ç¥ç»ç½‘ç»œå­¦ä¹ ä¿å½¢ä¸å˜çš„ç‰¹æ€§ç©ºé—´å’Œè®¡ç®—ç²¾ç¡®å˜å½¢åœºã€‚</li>
<li>DiffKillRèƒ½å¤Ÿåˆ©ç”¨å°‘é‡æ ‡æ³¨çš„åŸå‹æœ‰æ•ˆä¼ æ’­æ ‡æ³¨ï¼Œé™ä½æ‰‹åŠ¨æ ‡æ³¨çš„éœ€æ±‚ã€‚</li>
<li>DiffKillRé€‚ç”¨äºä»»ä½•ç±»å‹çš„åƒç´ çº§åˆ«æ ‡æ³¨ã€‚</li>
<li>DiffKillRåœ¨ä¸‰ä¸ªæ˜¾å¾®é•œä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„ç›‘ç£ã€åŠç›‘ç£å’Œæ— ç›‘ç£æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03058">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-583fe28f8945c6605b10ecede3dee3cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d135ced10c94ee5dafdbfd1a620c7cf2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-635d4b517761115a591d351a909b70f1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DDU-Net-A-Domain-Decomposition-Based-CNN-for-High-Resolution-Image-Segmentation-on-Multiple-GPUs"><a href="#DDU-Net-A-Domain-Decomposition-Based-CNN-for-High-Resolution-Image-Segmentation-on-Multiple-GPUs" class="headerlink" title="DDU-Net: A Domain Decomposition-Based CNN for High-Resolution Image   Segmentation on Multiple GPUs"></a>DDU-Net: A Domain Decomposition-Based CNN for High-Resolution Image   Segmentation on Multiple GPUs</h2><p><strong>Authors:CornÃ© Verburg, Alexander Heinlein, Eric C. Cyr</strong></p>
<p>The segmentation of ultra-high resolution images poses challenges such as loss of spatial information or computational inefficiency. In this work, a novel approach that combines encoder-decoder architectures with domain decomposition strategies to address these challenges is proposed. Specifically, a domain decomposition-based U-Net (DDU-Net) architecture is introduced, which partitions input images into non-overlapping patches that can be processed independently on separate devices. A communication network is added to facilitate inter-patch information exchange to enhance the understanding of spatial context. Experimental validation is performed on a synthetic dataset that is designed to measure the effectiveness of the communication network. Then, the performance is tested on the DeepGlobe land cover classification dataset as a real-world benchmark data set. The results demonstrate that the approach, which includes inter-patch communication for images divided into $16\times16$ non-overlapping subimages, achieves a $2-3,%$ higher intersection over union (IoU) score compared to the same network without inter-patch communication. The performance of the network which includes communication is equivalent to that of a baseline U-Net trained on the full image, showing that our model provides an effective solution for segmenting ultra-high-resolution images while preserving spatial context. The code is available at <a target="_blank" rel="noopener" href="https://github.com/corne00/DDU-Net">https://github.com/corne00/DDU-Net</a>. </p>
<blockquote>
<p>è¶…é«˜åˆ†è¾¨ç‡å›¾åƒçš„åˆ†å‰²é¢ä¸´ç€æŸå¤±ç©ºé—´ä¿¡æ¯æˆ–è®¡ç®—æ•ˆç‡ä½ä¸‹ç­‰æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆç¼–ç å™¨-è§£ç å™¨æ¶æ„å’ŒåŸŸåˆ†è§£ç­–ç•¥çš„æ–°æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œå¼•å…¥äº†ä¸€ç§åŸºäºåŸŸåˆ†è§£çš„U-Netï¼ˆDDU-Netï¼‰æ¶æ„ï¼Œè¯¥æ¶æ„å°†è¾“å…¥å›¾åƒåˆ†å‰²æˆå¯ä»¥åœ¨ä¸åŒè®¾å¤‡ä¸Šç‹¬ç«‹å¤„ç†çš„éé‡å æ–‘å—ã€‚æ·»åŠ é€šä¿¡ç½‘ç»œä»¥ä¿ƒè¿›æ–‘å—é—´çš„ä¿¡æ¯äº¤æ¢ï¼Œä»¥å¢å¼ºå¯¹ç©ºé—´ä¸Šä¸‹æ–‡çš„ç†è§£ã€‚åœ¨ä¸“ä¸ºæµ‹é‡é€šä¿¡ç½‘ç»œæœ‰æ•ˆæ€§è€Œè®¾è®¡çš„åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒéªŒè¯ã€‚ç„¶åï¼Œåœ¨DeepGlobeåœŸåœ°è¦†ç›–åˆ†ç±»æ•°æ®é›†ä¸Šæµ‹è¯•æ€§èƒ½ï¼Œä½œä¸ºç°å®ä¸–ç•ŒåŸºå‡†æ•°æ®é›†ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†åˆ†å‰²ä¸º$ 16\times16$éé‡å å­å›¾åƒçš„å›¾åƒæ—¶ï¼ŒåŒ…æ‹¬æ–‘å—é—´é€šä¿¡ï¼Œä¸æ²¡æœ‰æ–‘å—é—´é€šä¿¡çš„ç›¸åŒç½‘ç»œç›¸æ¯”ï¼Œå…¶äº¤é›†(IoU)å¾—åˆ†æé«˜äº†$ 2-3ï¼…$ã€‚åŒ…å«é€šä¿¡çš„ç½‘ç»œæ€§èƒ½ç›¸å½“äºåœ¨å®Œæ•´å›¾åƒä¸Šè®­ç»ƒçš„åŸºçº¿U-Netçš„æ€§èƒ½ï¼Œè¿™è¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹ä¸ºåˆ†å‰²è¶…é«˜åˆ†è¾¨ç‡å›¾åƒåŒæ—¶ä¿ç•™ç©ºé—´ä¸Šä¸‹æ–‡æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/corne00/DDU-Net">https://github.com/corne00/DDU-Net</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.21266v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æå‡ºä¸€ç§ç»“åˆç¼–ç å™¨-è§£ç å™¨æ¶æ„ä¸åŸŸåˆ†è§£ç­–ç•¥çš„æ–°æ–¹æ³•ï¼Œè§£å†³è¶…é«˜åˆ†è¾¨ç‡å›¾åƒåˆ†å‰²é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚ç©ºé—´ä¿¡æ¯ä¸¢å¤±æˆ–è®¡ç®—æ•ˆç‡ä½ä¸‹ã€‚å¼•å…¥åŸºäºåŸŸåˆ†è§£çš„U-Netï¼ˆDDU-Netï¼‰æ¶æ„ï¼Œå°†è¾“å…¥å›¾åƒåˆ†æˆå¯ç‹¬ç«‹å¤„ç†çš„æ— é‡å å—ï¼Œé€šè¿‡é€šä¿¡ç½‘ç»œè¿›è¡Œå—é—´ä¿¡æ¯äº¤æ¢ï¼Œå¢å¼ºç©ºé—´ä¸Šä¸‹æ–‡ç†è§£ã€‚åœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œçš„DeepGlobeåœŸåœ°è¦†ç›–åˆ†ç±»æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒéªŒè¯ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å›¾åƒåˆ†å‰²ä¸º$16\times16$éé‡å å­å›¾åƒæ—¶åŠ å…¥å—é—´é€šä¿¡ï¼Œä¸æ²¡æœ‰å—é—´é€šä¿¡çš„ç½‘ç»œç›¸æ¯”ï¼Œæé«˜äº†$2-3%$çš„äº¤é›†ï¼ˆIoUï¼‰å¾—åˆ†ã€‚åŒ…å«é€šä¿¡çš„ç½‘ç»œæ€§èƒ½ç›¸å½“äºåœ¨å®Œæ•´å›¾åƒä¸Šè®­ç»ƒçš„åŸºçº¿U-Netçš„æ€§èƒ½ï¼Œè¡¨æ˜è¯¥æ¨¡å‹åœ¨åˆ†å‰²è¶…é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶ä¿ç•™äº†ç©ºé—´ä¸Šä¸‹æ–‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¶…é«˜åˆ†è¾¨ç‡å›¾åƒåˆ†å‰²é¢ä¸´ç©ºé—´ä¿¡æ¯ä¸¢å¤±å’Œè®¡ç®—æ•ˆç‡ä½çš„é—®é¢˜ã€‚</li>
<li>æå‡ºç»“åˆç¼–ç å™¨-è§£ç å™¨æ¶æ„ä¸åŸŸåˆ†è§£ç­–ç•¥çš„æ–°æ–¹æ³•ï¼Œè§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>å¼•å…¥åŸºäºåŸŸåˆ†è§£çš„U-Netï¼ˆDDU-Netï¼‰æ¶æ„ï¼Œå°†å›¾åƒåˆ†æˆæ— é‡å å—è¿›è¡Œç‹¬ç«‹å¤„ç†ã€‚</li>
<li>æ·»åŠ é€šä¿¡ç½‘ç»œè¿›è¡Œå—é—´ä¿¡æ¯äº¤æ¢ï¼Œå¢å¼ºç©ºé—´ä¸Šä¸‹æ–‡ç†è§£ã€‚</li>
<li>åœ¨åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒéªŒè¯ï¼Œæµ‹è¯•é€šä¿¡ç½‘ç»œçš„çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>åœ¨çœŸå®ä¸–ç•Œçš„DeepGlobeåœŸåœ°è¦†ç›–åˆ†ç±»æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºæ— å—é—´é€šä¿¡çš„ç½‘ç»œæé«˜äº†$2-3%$çš„IoUå¾—åˆ†ã€‚</li>
<li>åŒ…å«é€šä¿¡çš„ç½‘ç»œæ€§èƒ½ä¸åœ¨å®Œæ•´å›¾åƒä¸Šè®­ç»ƒçš„åŸºçº¿U-Netç›¸å½“ï¼Œè¡¨æ˜è¯¥æ¨¡å‹åœ¨åˆ†å‰²è¶…é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶æœ‰æ•ˆå¹¶ä¿ç•™ç©ºé—´ä¸Šä¸‹æ–‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.21266">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b5415143f9580e01893c88c7b13f2b8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c26c1209c2954b38382fac4baf04799.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8512922c8cf69aee9d003a088f3e0430.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbd3318307973127d2cd6fdf1ebafa45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-203bc6810a63699760659a73304dfeeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a67d5917097145d6b86b39e1f7b90bfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8b96ddf8a703b756f4f04f1ef2c7e31.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ImageFlowNet-Forecasting-Multiscale-Image-Level-Trajectories-of-Disease-Progression-with-Irregularly-Sampled-Longitudinal-Medical-Images"><a href="#ImageFlowNet-Forecasting-Multiscale-Image-Level-Trajectories-of-Disease-Progression-with-Irregularly-Sampled-Longitudinal-Medical-Images" class="headerlink" title="ImageFlowNet: Forecasting Multiscale Image-Level Trajectories of Disease   Progression with Irregularly-Sampled Longitudinal Medical Images"></a>ImageFlowNet: Forecasting Multiscale Image-Level Trajectories of Disease   Progression with Irregularly-Sampled Longitudinal Medical Images</h2><p><strong>Authors:Chen Liu, Ke Xu, Liangbo L. Shen, Guillaume Huguet, Zilong Wang, Alexander Tong, Danilo Bzdok, Jay Stewart, Jay C. Wang, Lucian V. Del Priore, Smita Krishnaswamy</strong></p>
<p>Advances in medical imaging technologies have enabled the collection of longitudinal images, which involve repeated scanning of the same patients over time, to monitor disease progression. However, predictive modeling of such data remains challenging due to high dimensionality, irregular sampling, and data sparsity. To address these issues, we propose ImageFlowNet, a novel model designed to forecast disease trajectories from initial images while preserving spatial details. ImageFlowNet first learns multiscale joint representation spaces across patients and time points, then optimizes deterministic or stochastic flow fields within these spaces using a position-parameterized neural ODE&#x2F;SDE framework. The model leverages a UNet architecture to create robust multiscale representations and mitigates data scarcity by combining knowledge from all patients. We provide theoretical insights that support our formulation of ODEs, and motivate our regularizations involving high-level visual features, latent space organization, and trajectory smoothness. We validate ImageFlowNet on three longitudinal medical image datasets depicting progression in geographic atrophy, multiple sclerosis, and glioblastoma, demonstrating its ability to effectively forecast disease progression and outperform existing methods. Our contributions include the development of ImageFlowNet, its theoretical underpinnings, and empirical validation on real-world datasets. The official implementation is available at <a target="_blank" rel="noopener" href="https://github.com/KrishnaswamyLab/ImageFlowNet">https://github.com/KrishnaswamyLab/ImageFlowNet</a>. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒæŠ€æœ¯çš„è¿›æ­¥ä½¿å¾—èƒ½å¤Ÿæ”¶é›†çºµå‘å›¾åƒï¼Œè¿™äº›å›¾åƒæ¶‰åŠéšç€æ—¶é—´çš„æ¨ç§»å¯¹åŒä¸€æ‚£è€…çš„åå¤æ‰«æï¼Œä»¥ç›‘æµ‹ç–¾ç—…çš„è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®çš„é«˜ç»´æ€§ã€ä¸è§„åˆ™é‡‡æ ·å’Œæ•°æ®ç¨€ç–æ€§ï¼Œæ­¤ç±»æ•°æ®çš„é¢„æµ‹å»ºæ¨¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ImageFlowNetè¿™ä¸€æ–°å‹æ¨¡å‹ï¼Œæ—¨åœ¨ä»åˆå§‹å›¾åƒé¢„æµ‹ç–¾ç—…è½¨è¿¹çš„åŒæ—¶ä¿ç•™ç©ºé—´ç»†èŠ‚ã€‚ImageFlowNeté¦–å…ˆå­¦ä¹ è·¨æ‚£è€…å’Œæ—¶é—´ç‚¹çš„å¤šå°ºåº¦è”åˆè¡¨ç¤ºç©ºé—´ï¼Œç„¶ååˆ©ç”¨ä½ç½®å‚æ•°åŒ–çš„ç¥ç»ODE&#x2F;SDEæ¡†æ¶ä¼˜åŒ–è¿™äº›ç©ºé—´å†…çš„ç¡®å®šæ€§æˆ–éšæœºæµåœºã€‚è¯¥æ¨¡å‹åˆ©ç”¨UNetæ¶æ„åˆ›å»ºç¨³å¥çš„å¤šå°ºåº¦è¡¨ç¤ºï¼Œå¹¶é€šè¿‡æ•´åˆæ‰€æœ‰æ‚£è€…çš„çŸ¥è¯†æ¥ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚æˆ‘ä»¬æä¾›äº†æ”¯æŒæˆ‘ä»¬å»ºç«‹ODEsçš„ç†è®ºè§è§£ï¼Œå¹¶æ¿€å‘äº†æˆ‘ä»¬æ¶‰åŠé«˜çº§è§†è§‰ç‰¹å¾ã€æ½œåœ¨ç©ºé—´ç»„ç»‡å’Œè½¨è¿¹å¹³æ»‘æ€§çš„æ­£åˆ™åŒ–åŠ¨æœºã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæç»˜åœ°ç†èç¼©ã€å¤šå‘æ€§ç¡¬åŒ–ç—‡å’Œèƒ¶è´¨æ¯ç»†èƒç˜¤è¿›å±•çš„çºµå‘åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸ŠéªŒè¯äº†ImageFlowNetï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆé¢„æµ‹ç–¾ç—…è¿›å±•çš„èƒ½åŠ›ï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬ImageFlowNetçš„å¼€å‘ã€å…¶ç†è®ºåŸºç¡€ä»¥åŠåœ¨çœŸå®æ•°æ®é›†ä¸Šçš„ç»éªŒéªŒè¯ã€‚å®˜æ–¹å®ç°å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/KrishnaswamyLab/ImageFlowNet%E3%80%82">https://github.com/KrishnaswamyLab/ImageFlowNetã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14794v6">PDF</a> ICASSP 2025, Oral Presentation</p>
<p><strong>Summary</strong></p>
<p>åŸºäºåŒ»ç–—æˆåƒæŠ€æœ¯çš„è¿›æ­¥ï¼Œé•¿æœŸå›¾åƒæ”¶é›†è¢«å¹¿æ³›åº”ç”¨äºç›‘æµ‹ç–¾ç—…è¿›å±•ã€‚é’ˆå¯¹æ­¤ç±»æ•°æ®é¢„æµ‹å­˜åœ¨çš„æŒ‘æˆ˜ï¼Œå¦‚é«˜ç»´åº¦ã€ä¸è§„åˆ™é‡‡æ ·å’Œæ•°æ®ç¨€ç–æ€§é—®é¢˜ï¼Œæå‡ºäº†ImageFlowNetæ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆå§‹å›¾åƒé¢„æµ‹ç–¾ç—…è½¨è¿¹ï¼ŒåŒæ—¶ä¿ç•™ç©ºé—´ç»†èŠ‚ã€‚å…¶é‡‡ç”¨å¤šå°ºåº¦è”åˆè¡¨ç¤ºç©ºé—´å­¦ä¹ ï¼Œå¹¶åˆ©ç”¨ç¥ç»ODE&#x2F;SDEæ¡†æ¶ä¼˜åŒ–ç¡®å®šæ€§æˆ–éšæœºæµåœºã€‚é€šè¿‡UNetæ¶æ„åˆ›å»ºç¨³å¥çš„å¤šå°ºåº¦è¡¨ç¤ºï¼Œå¹¶ç»“åˆæ‰€æœ‰æ‚£è€…çš„çŸ¥è¯†ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚åœ¨ä¸‰ä¸ªçºµå‘åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸ŠéªŒè¯äº†ImageFlowNetçš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†å…¶é¢„æµ‹ç–¾ç—…è¿›å±•çš„èƒ½åŠ›å¹¶è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—æˆåƒæŠ€æœ¯ç°åœ¨èƒ½å¤Ÿæ”¶é›†é•¿æœŸå›¾åƒï¼Œç”¨äºç›‘æµ‹ç–¾ç—…è¿›å±•ã€‚</li>
<li>ImageFlowNetæ¨¡å‹ç”¨äºä»åˆå§‹å›¾åƒé¢„æµ‹ç–¾ç—…è½¨è¿¹ï¼Œä¿ç•™ç©ºé—´ç»†èŠ‚ã€‚</li>
<li>ImageFlowNeté€šè¿‡å¤šå°ºåº¦è”åˆè¡¨ç¤ºç©ºé—´å­¦ä¹ æ¥å¤„ç†é«˜ç»´åº¦æ•°æ®ã€‚</li>
<li>ç¥ç»ODE&#x2F;SDEæ¡†æ¶è¢«ç”¨äºä¼˜åŒ–ç¡®å®šæ€§æˆ–éšæœºæµåœºã€‚</li>
<li>UNetæ¶æ„ç”¨äºåˆ›å»ºç¨³å¥çš„å¤šå°ºåº¦è¡¨ç¤ºã€‚</li>
<li>ImageFlowNeté€šè¿‡ç»“åˆæ‰€æœ‰æ‚£è€…çš„çŸ¥è¯†æ¥ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14794">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4d346bd860819d1d9f95706ce8069093.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95bb82a74bb2746a824659bcebb35fd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-668419b156643de27c9f64266d7041c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a44925c256226d39af72e3757ab975c0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CADS-A-Systematic-Literature-Review-on-the-Challenges-of-Abstractive-Dialogue-Summarization"><a href="#CADS-A-Systematic-Literature-Review-on-the-Challenges-of-Abstractive-Dialogue-Summarization" class="headerlink" title="CADS: A Systematic Literature Review on the Challenges of Abstractive   Dialogue Summarization"></a>CADS: A Systematic Literature Review on the Challenges of Abstractive   Dialogue Summarization</h2><p><strong>Authors:Frederic Kirstein, Jan Philip Wahle, Bela Gipp, Terry Ruas</strong></p>
<p>Abstractive dialogue summarization is the task of distilling conversations into informative and concise summaries. Although reviews have been conducted on this topic, there is a lack of comprehensive work detailing the challenges of dialogue summarization, unifying the differing understanding of the task, and aligning proposed techniques, datasets, and evaluation metrics with the challenges. This article summarizes the research on Transformer-based abstractive summarization for English dialogues by systematically reviewing 1262 unique research papers published between 2019 and 2024, relying on the Semantic Scholar and DBLP databases. We cover the main challenges present in dialog summarization (i.e., language, structure, comprehension, speaker, salience, and factuality) and link them to corresponding techniques such as graph-based approaches, additional training tasks, and planning strategies, which typically overly rely on BART-based encoder-decoder models. We find that while some challenges, like language, have seen considerable progress, mainly due to training methods, others, such as comprehension, factuality, and salience, remain difficult and hold significant research opportunities. We investigate how these approaches are typically assessed, covering the datasets for the subdomains of dialogue (e.g., meeting, medical), the established automatic metrics and human evaluation approaches for assessing scores and annotator agreement. We observe that only a few datasets span across all subdomains. The ROUGE metric is the most used, while human evaluation is frequently reported without sufficient detail on inner-annotator agreement and annotation guidelines. Additionally, we discuss the possible implications of the recently explored large language models and conclude that despite a potential shift in relevance and difficulty, our described challenge taxonomy remains relevant. </p>
<blockquote>
<p>æ‘˜è¦æ€§å¯¹è¯æ€»ç»“æ˜¯å°†å¯¹è¯æç‚¼æˆä¿¡æ¯ä¸°å¯Œä¸”ç®€æ´çš„æ‘˜è¦çš„ä»»åŠ¡ã€‚å°½ç®¡å·²ç»å¯¹æ­¤ä¸»é¢˜è¿›è¡Œäº†è¯„è®ºï¼Œä½†ä»ç¼ºä¹ç»¼åˆæ€§çš„å·¥ä½œæ¥è¯¦ç»†é˜è¿°å¯¹è¯æ€»ç»“çš„æŒ‘æˆ˜ï¼Œç»Ÿä¸€å¯¹ä»»åŠ¡çš„ä¸åŒç†è§£ï¼Œä»¥åŠä½¿æ‰€æå‡ºçš„æŠ€æœ¯ã€æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ä¸è¿™äº›æŒ‘æˆ˜ç›¸åŒ¹é…ã€‚æœ¬æ–‡é€šè¿‡ç³»ç»Ÿå›é¡¾2019å¹´è‡³2024å¹´é—´å‘è¡¨çš„1262ç¯‡ç‹¬ç‰¹ç ”ç©¶è®ºæ–‡ï¼Œæ€»ç»“äº†åŸºäºTransformerçš„è‹±è¯­å¯¹è¯æ‘˜è¦ç ”ç©¶ï¼Œè¿™äº›è®ºæ–‡æ¥è‡ªSemantic Scholarå’ŒDBLPæ•°æ®åº“ã€‚æˆ‘ä»¬ä»‹ç»äº†å¯¹è¯æ‘˜è¦ä¸­å­˜åœ¨çš„ä¸»è¦æŒ‘æˆ˜ï¼ˆå³è¯­è¨€ã€ç»“æ„ã€ç†è§£ã€è¯´è¯è€…ã€çªå‡ºæ€§å’Œäº‹å®æ€§ï¼‰ï¼Œå¹¶å°†å®ƒä»¬ä¸ç›¸åº”çš„æŠ€æœ¯ï¼ˆå¦‚åŸºäºå›¾çš„æ–¹æ³•ã€é¢å¤–çš„è®­ç»ƒä»»åŠ¡å’Œè§„åˆ’ç­–ç•¥ï¼‰è”ç³»èµ·æ¥ï¼Œè¿™äº›æŠ€æœ¯é€šå¸¸è¿‡äºä¾èµ–åŸºäºBARTçš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ã€‚æˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶ä¸€äº›æŒ‘æˆ˜ï¼ˆå¦‚è¯­è¨€ï¼‰å·²ç»å–å¾—äº†ç›¸å½“å¤§çš„è¿›å±•ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºè®­ç»ƒæ–¹æ³•ï¼Œä½†å…¶ä»–æŒ‘æˆ˜ï¼ˆå¦‚ç†è§£ã€äº‹å®å’Œçªå‡ºæ€§ï¼‰ä»ç„¶å›°éš¾ï¼Œå¹¶å­˜åœ¨é‡è¦çš„ç ”ç©¶æœºä¼šã€‚æˆ‘ä»¬è°ƒæŸ¥äº†è¿™äº›æ–¹æ³•çš„å…¸å‹è¯„ä¼°æ–¹å¼ï¼Œæ¶µç›–äº†å¯¹è¯å­é¢†åŸŸçš„æ•°æ®é›†ï¼ˆä¾‹å¦‚ä¼šè®®ã€åŒ»ç–—ï¼‰ï¼Œä»¥åŠç”¨äºè¯„ä¼°åˆ†æ•°å’Œæ³¨é‡Šè€…ä¸€è‡´æ€§çš„æ—¢å®šè‡ªåŠ¨æŒ‡æ ‡å’Œäººå·¥è¯„ä¼°æ–¹æ³•ã€‚æˆ‘ä»¬å‘ç°åªæœ‰å°‘æ•°æ•°æ®é›†æ¶µç›–æ‰€æœ‰å­åŸŸã€‚ROUGEæŒ‡æ ‡æ˜¯æœ€å¸¸ç”¨çš„æŒ‡æ ‡ï¼Œè€Œäººå·¥è¯„ä¼°é€šå¸¸æ²¡æœ‰è¶³å¤Ÿè¯¦ç»†çš„å†…éƒ¨è¯„ä¼°è€…ä¸€è‡´æ€§å’Œæ³¨é‡ŠæŒ‡å—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†æœ€è¿‘æ¢ç´¢çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯èƒ½å½±å“ï¼Œå¹¶å¾—å‡ºç»“è®ºï¼šå°½ç®¡ç›¸å…³æ€§å’Œéš¾åº¦å¯èƒ½å­˜åœ¨æ½œåœ¨çš„è½¬å˜ï¼Œæˆ‘ä»¬æè¿°çš„æŒ‘æˆ˜åˆ†ç±»ä»ç„¶å¾ˆé‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.07494v3">PDF</a> Published in the Journal of Artificial Intelligence Research (JAIR)   (<a target="_blank" rel="noopener" href="https://www.jair.org/index.php/jair/article/view/16674">https://www.jair.org/index.php/jair/article/view/16674</a>)</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç»¼è¿°äº†åŸºäºTransformerçš„è‹±è¯­å¯¹è¯æ‘˜è¦ç ”ç©¶ï¼Œé€šè¿‡ç³»ç»Ÿåœ°å›é¡¾äº†2019å¹´è‡³2024å¹´é—´å‘è¡¨çš„1262ç¯‡ç‹¬ç‰¹çš„ç ”ç©¶è®ºæ–‡ï¼Œä¸»è¦æ¢è®¨äº†å¯¹è¯æ‘˜è¦é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ï¼Œå¦‚è¯­è¨€ã€ç»“æ„ã€ç†è§£ã€è¯´è¯è€…ã€æ˜¾è‘—æ€§äº‹å®å’ŒçœŸå®æ€§ç­‰ã€‚æœ¬æ–‡é“¾æ¥äº†è¿™äº›æŒ‘æˆ˜ä¸ç›¸åº”çš„æŠ€æœ¯ï¼Œå¦‚åŸºäºå›¾çš„æ–¹æ³•ã€é¢å¤–çš„è®­ç»ƒä»»åŠ¡å’Œè§„åˆ’ç­–ç•¥ç­‰ï¼Œè¿™äº›æŠ€æœ¯é€šå¸¸è¿‡åº¦ä¾èµ–äºBARTç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶è¯­è¨€ç­‰æŒ‘æˆ˜å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç†è§£ã€çœŸå®æ€§å’Œæ˜¾è‘—æ€§ç­‰æŒ‘æˆ˜ä»ç„¶å›°éš¾é‡é‡ï¼Œå­˜åœ¨å¤§é‡çš„ç ”ç©¶æœºä¼šã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†è¿™äº›æ–¹æ³•çš„è¯„ä¼°æ–¹å¼ï¼Œæ¶µç›–äº†å¯¹è¯å­åŸŸçš„æ•°æ®é›†ï¼ˆå¦‚ä¼šè®®ã€åŒ»ç–—ç­‰ï¼‰ã€ç°æœ‰çš„è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å’Œäººå·¥è¯„ä¼°æ–¹æ³•ï¼Œå‘ç°åªæœ‰å°‘æ•°æ•°æ®é›†æ¶µç›–æ‰€æœ‰å­åŸŸï¼ŒROUGEæŒ‡æ ‡æ˜¯æœ€å¸¸ç”¨çš„ï¼Œè€Œäººå·¥è¯„ä¼°å¾€å¾€ç¼ºä¹å†…éƒ¨è¯„ä¼°è€…ä¸€è‡´æ€§å’Œè¯„ä¼°æŒ‡å—çš„è¶³å¤Ÿç»†èŠ‚ã€‚æœ€åï¼Œæœ¬æ–‡è®¨è®ºäº†æœ€è¿‘æ¢ç´¢çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯èƒ½å½±å“ï¼Œå¹¶å¾—å‡ºç»“è®ºï¼Œå°½ç®¡ç›¸å…³æ€§å’Œéš¾åº¦å¯èƒ½å­˜åœ¨æ½œåœ¨çš„è½¬å˜ï¼Œä½†æœ¬æ–‡æè¿°çš„æŒ‘æˆ˜åˆ†ç±»ä»ç„¶å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¯¹è¯æ‘˜è¦çš„ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬è¯­è¨€ã€ç»“æ„ã€ç†è§£ã€è¯´è¯è€…ã€æ˜¾è‘—æ€§äº‹å®å’ŒçœŸå®æ€§ç­‰ã€‚</li>
<li>æŠ€æœ¯è¿›æ­¥å¦‚åŸºäºå›¾çš„æ–¹æ³•ã€é¢å¤–è®­ç»ƒä»»åŠ¡å’Œè§„åˆ’ç­–ç•¥ç­‰é€šå¸¸ä¾èµ–äºBARTç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ã€‚</li>
<li>è¯­è¨€æŒ‘æˆ˜å·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ç†è§£ã€çœŸå®æ€§å’Œæ˜¾è‘—æ€§ç­‰ä»ç„¶é¢ä¸´å›°éš¾ã€‚</li>
<li>å¯¹è¯æ‘˜è¦çš„è¯„ä¼°æ¶‰åŠå¤šä¸ªæ•°æ®é›†ã€è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å’Œäººå·¥è¯„ä¼°æ–¹æ³•ã€‚ä½†æ•°æ®é›†è¦†ç›–é¢æœ‰é™ï¼Œä¸”ç¼ºä¹å†…éƒ¨è¯„ä¼°è€…ä¸€è‡´æ€§å’Œè¯„ä¼°æŒ‡å—çš„è¶³å¤Ÿç»†èŠ‚ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„å‡ºç°å¯èƒ½æ”¹å˜ç›¸å…³æ€§å’Œéš¾åº¦çš„æ ¼å±€ï¼Œä½†æŒ‘æˆ˜åˆ†ç±»ä¾ç„¶é‡è¦ã€‚</li>
<li>ç›®å‰ç ”ç©¶è™½ç„¶ä¸°å¯Œä½†å­˜åœ¨ä¸è¶³ï¼Œä»éœ€è¦æ›´å¤šå…¨é¢çš„ç ”ç©¶å’Œæ·±å…¥æ¢ç´¢æ–°çš„æ–¹æ³•å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.07494">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-05a37057c11c5c16fc516453a9539f17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b23525e372b68c711510f882e9f22a2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-008069fb70fe5f4c37971db02f9a2735.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AI-in-Lung-Health-Benchmarking-Detection-and-Diagnostic-Models-Across-Multiple-CT-Scan-Datasets"><a href="#AI-in-Lung-Health-Benchmarking-Detection-and-Diagnostic-Models-Across-Multiple-CT-Scan-Datasets" class="headerlink" title="AI in Lung Health: Benchmarking Detection and Diagnostic Models Across   Multiple CT Scan Datasets"></a>AI in Lung Health: Benchmarking Detection and Diagnostic Models Across   Multiple CT Scan Datasets</h2><p><strong>Authors:Fakrul Islam Tushar, Avivah Wang, Lavsen Dahal, Michael R. Harowicz, Kyle J. Lafata, Tina D. Tailor, Joseph Y. Lo</strong></p>
<p>Lung cancer remains the leading cause of cancer-related mortality worldwide, and early detection through low-dose computed tomography (LDCT) has shown significant promise in reducing death rates. With the growing integration of artificial intelligence (AI) into medical imaging, the development and evaluation of robust AI models require access to large, well-annotated datasets. In this study, we introduce the utility of Duke Lung Cancer Screening (DLCS) Dataset, the largest open-access LDCT dataset with over 2,000 scans and 3,000 expert-verified nodules. We benchmark deep learning models for both 3D nodule detection and lung cancer classification across internal and external datasets including LUNA16, LUNA25, and NLST-3D+. For detection, we develop two MONAI-based RetinaNet models (DLCSDmD and LUNA16-mD), evaluated using the Competition Performance Metric (CPM). For classification, we compare five models, including state-of-the-art pretrained models (Models Genesis, Med3D), a selfsupervised foundation model (FMCB), a randomly initialized ResNet50, and proposed a novel Strategic Warm-Start++ (SWS++) model. SWS++ uses curated candidate patches to pretrain a classification backbone within the same detection pipeline, enabling task-relevant feature learning. Our models demonstrated strong generalizability, with SWS++ achieving comparable or superior performance to existing foundational models across multiple datasets (AUC: 0.71 to 0.90). All code, models, and data are publicly released to promote reproducibility and collaboration. This work establishes a standardized benchmarking resource for lung cancer AI research, supporting future efforts in model development, validation, and clinical translation. </p>
<blockquote>
<p>è‚ºç™Œä»ç„¶æ˜¯å…¨çƒç™Œç—‡ç›¸å…³æ­»äº¡çš„ä¸»è¦åŸå› ï¼Œé€šè¿‡ä½å‰‚é‡è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆLDCTï¼‰è¿›è¡Œæ—©æœŸæ£€æµ‹åœ¨é™ä½æ­»äº¡ç‡æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚éšç€äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨åŒ»å­¦æˆåƒä¸­çš„æ—¥ç›Šèåˆï¼Œå¼€å‘å’Œè¯„ä¼°ç¨³å¥çš„AIæ¨¡å‹éœ€è¦è®¿é—®å¤§é‡ã€ç»è¿‡è‰¯å¥½æ³¨é‡Šçš„æ•°æ®é›†ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Dukeè‚ºç™Œç­›æŸ¥ï¼ˆDLCSï¼‰æ•°æ®é›†çš„å®ç”¨æ€§ï¼Œè¿™æ˜¯æœ€å¤§çš„å¼€æ”¾è®¿é—®LDCTæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡2000æ¬¡æ‰«æå’Œ3000ä¸ªä¸“å®¶éªŒè¯çš„ç»“èŠ‚ã€‚æˆ‘ä»¬å¯¹å†…éƒ¨å’Œå¤–éƒ¨æ•°æ®é›†ï¼ˆåŒ…æ‹¬LUNA16ã€LUNA25å’ŒNLST-3D+ï¼‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°å…¶å¯¹3Dç»“èŠ‚æ£€æµ‹å’Œè‚ºç™Œåˆ†ç±»çš„æ•ˆæœã€‚å¯¹äºæ£€æµ‹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸¤ä¸ªåŸºäºMONAIçš„RetinaNetæ¨¡å‹ï¼ˆDLCSDmDå’ŒLUNA16-mDï¼‰ï¼Œä½¿ç”¨ç«èµ›æ€§èƒ½æŒ‡æ ‡ï¼ˆCPMï¼‰è¿›è¡Œè¯„ä¼°ã€‚å¯¹äºåˆ†ç±»ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†äº”ç§æ¨¡å‹ï¼ŒåŒ…æ‹¬æœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆModels Genesisã€Med3Dï¼‰ã€ä¸€ä¸ªè‡ªç›‘ç£åŸºç¡€æ¨¡å‹ï¼ˆFMCBï¼‰ã€ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„ResNet50ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æˆ˜ç•¥æ¸©å¯åŠ¨ï¼ˆSWS++ï¼‰æ¨¡å‹ã€‚SWS++ä½¿ç”¨ç²¾é€‰çš„å€™é€‰è¡¥ä¸åœ¨ç›¸åŒçš„æ£€æµ‹ç®¡é“ä¸­é¢„è®­ç»ƒåˆ†ç±»ä¸»å¹²ï¼Œä»è€Œå®ç°ä»»åŠ¡ç›¸å…³ç‰¹å¾å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°å‡ºå¾ˆå¼ºçš„é€šç”¨æ€§ï¼Œå…¶ä¸­SWS++åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸ç°æœ‰åŸºç¡€æ¨¡å‹ç›¸å½“æˆ–æ›´ä¼˜ï¼ˆAUCï¼š0.71è‡³0.90ï¼‰ã€‚ä¸ºäº†ä¿ƒè¿›å¯é‡å¤æ€§å’Œåä½œï¼Œæˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†æ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œæ•°æ®ã€‚è¿™é¡¹å·¥ä½œä¸ºè‚ºç™ŒAIç ”ç©¶å»ºç«‹äº†æ ‡å‡†åŒ–çš„åŸºå‡†èµ„æºï¼Œæ”¯æŒæœªæ¥çš„æ¨¡å‹å¼€å‘ã€éªŒè¯å’Œä¸´åºŠç¿»è¯‘å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.04605v3">PDF</a> 2 tables, 6 figures</p>
<p><strong>æ‘˜è¦</strong><br>è‚ºç™Œä»æ˜¯å…¨çƒç™Œç—‡æ­»äº¡çš„ä¸»è¦åŸå› ï¼Œä½å‰‚é‡è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆLDCTï¼‰çš„æ—©æœŸæ£€æµ‹åœ¨é™ä½æ­»äº¡ç‡æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚éšç€äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨åŒ»å­¦æˆåƒä¸­çš„æ—¥ç›Šèåˆï¼Œå¼€å‘å’Œè¯„ä¼°ç¨³å¥çš„AIæ¨¡å‹éœ€è¦è®¿é—®å¤§é‡ã€ç»è¿‡è‰¯å¥½æ³¨é‡Šçš„æ•°æ®é›†ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†Duke Lung Cancer Screeningï¼ˆDLCSï¼‰æ•°æ®é›†çš„å®ç”¨æ€§ï¼Œè¿™æ˜¯æœ€å¤§çš„å…¬å¼€è®¿é—®LDCTæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡2000æ¬¡æ‰«æå’Œ3000ä¸ªä¸“å®¶éªŒè¯çš„ç»“èŠ‚ã€‚æˆ‘ä»¬å¯¹å†…éƒ¨å’Œå¤–éƒ¨æ•°æ®é›†ï¼ˆåŒ…æ‹¬LUNA16ã€LUNA25å’ŒNLST-3D+ï¼‰è¿›è¡Œäº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äº3Dç»“èŠ‚æ£€æµ‹å’Œè‚ºç™Œåˆ†ç±»ã€‚å¯¹äºæ£€æµ‹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸¤ä¸ªåŸºäºMONAIçš„RetinaNetæ¨¡å‹ï¼ˆDLCSDmDå’ŒLUNA16-mDï¼‰ï¼Œä½¿ç”¨ç«èµ›æ€§èƒ½æŒ‡æ ‡ï¼ˆCPMï¼‰è¿›è¡Œè¯„ä¼°ã€‚å¯¹äºåˆ†ç±»ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†äº”ç§æ¨¡å‹ï¼ŒåŒ…æ‹¬æœ€æ–°é¢„è®­ç»ƒæ¨¡å‹ï¼ˆModels Genesisã€Med3Dï¼‰ã€è‡ªç›‘ç£åŸºç¡€æ¨¡å‹ï¼ˆFMCBï¼‰ã€éšæœºåˆå§‹åŒ–çš„ResNet50ï¼Œä»¥åŠæå‡ºçš„æ–°å‹æˆ˜ç•¥æ¸©å¯ï¼ˆSWS++ï¼‰æ¨¡å‹ã€‚SWS++ä½¿ç”¨ç²¾é€‰çš„å€™é€‰è¡¥ä¸åœ¨åŒä¸€æ£€æµ‹ç®¡é“ä¸­é¢„è®­ç»ƒåˆ†ç±»ä¸»å¹²ï¼Œå®ç°ä»»åŠ¡ç›¸å…³ç‰¹å¾å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒSWS++åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¸ç°æœ‰åŸºç¡€æ¨¡å‹ç›¸å½“æˆ–æ›´ä¼˜ï¼ˆAUCï¼š0.71è‡³0.90ï¼‰ã€‚æ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å‡å…¬å¼€å‘å¸ƒï¼Œä»¥ä¿ƒè¿›å¯é‡å¤æ€§å’Œåä½œã€‚è¿™é¡¹å·¥ä½œä¸ºè‚ºç™ŒAIç ”ç©¶å»ºç«‹äº†æ ‡å‡†åŒ–çš„åŸºå‡†æµ‹è¯•èµ„æºï¼Œæ”¯æŒæœªæ¥çš„æ¨¡å‹å¼€å‘ã€éªŒè¯å’Œä¸´åºŠè½¬åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚ºç™Œä»æ˜¯å…¨çƒä¸»è¦çš„ç™Œç—‡æ­»äº¡åŸå› ï¼ŒLDCTåœ¨æ—©æœŸæ£€æµ‹ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>AIåœ¨åŒ»å­¦æˆåƒä¸­çš„é›†æˆå¯¹è‚ºç™Œè¯Šæ–­æœ‰ç§¯æå½±å“ã€‚</li>
<li>ä»‹ç»äº†DLCSæ•°æ®é›†ï¼Œè¿™æ˜¯æœ€å¤§çš„å…¬å¼€è®¿é—®LDCTæ•°æ®é›†ï¼ŒåŒ…å«ç»è¿‡ä¸“å®¶éªŒè¯çš„ç»“èŠ‚ã€‚</li>
<li>åŸºå‡†æµ‹è¯•äº†æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œ3Dç»“èŠ‚æ£€æµ‹å’Œè‚ºç™Œåˆ†ç±»ã€‚</li>
<li>å¼€å‘å¹¶è¯„ä¼°äº†åŸºäºMONAIçš„RetinaNetæ¨¡å‹å’Œå…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>SWS++æ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ€§èƒ½ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.04605">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af08055818ef3346cc126d882f887cd6.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-26/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-26/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-26/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-24963cfbdfcde706aa6653a7af336d1c.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-26  Graph covers and semi-covers Who is stronger?
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-26/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f4b84ec53e433cda86e5a98e5b439053.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-26  Beyond Labels Zero-Shot Diabetic Foot Ulcer Wound Segmentation with   Self-attention Diffusion Models and the Potential for Text-Guided   Customization
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19778.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
