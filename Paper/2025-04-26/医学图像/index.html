<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-04-26  Self-Supervised Noise Adaptive MRI Denoising via Repetition to   Repetition (Rep2Rep) Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-d4ff568aadca0897bbf49396281e914e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    15.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    63 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-26-更新"><a href="#2025-04-26-更新" class="headerlink" title="2025-04-26 更新"></a>2025-04-26 更新</h1><h2 id="Self-Supervised-Noise-Adaptive-MRI-Denoising-via-Repetition-to-Repetition-Rep2Rep-Learning"><a href="#Self-Supervised-Noise-Adaptive-MRI-Denoising-via-Repetition-to-Repetition-Rep2Rep-Learning" class="headerlink" title="Self-Supervised Noise Adaptive MRI Denoising via Repetition to   Repetition (Rep2Rep) Learning"></a>Self-Supervised Noise Adaptive MRI Denoising via Repetition to   Repetition (Rep2Rep) Learning</h2><p><strong>Authors:Nikola Janjušević, Jingjia Chen, Luke Ginocchio, Mary Bruno, Yuhui Huang, Yao Wang, Hersh Chandarana, Li Feng</strong></p>
<p>Purpose: This work proposes a novel self-supervised noise-adaptive image denoising framework, called Repetition to Repetition (Rep2Rep) learning, for low-field (&lt;1T) MRI applications. Methods: Rep2Rep learning extends the Noise2Noise framework by training a neural network on two repeated MRI acquisitions, using one repetition as input and another as target, without requiring ground-truth data. It incorporates noise-adaptive training, enabling denoising generalization across varying noise levels and flexible inference with any number of repetitions. Performance was evaluated on both synthetic noisy brain MRI and 0.55T prostate MRI data, and compared against supervised learning and Monte Carlo Stein’s Unbiased Risk Estimator (MC-SURE). Results: Rep2Rep learning outperforms MC-SURE on both synthetic and 0.55T MRI datasets. On synthetic brain data, it achieved denoising quality comparable to supervised learning and surpassed MC-SURE, particularly in preserving structural details and reducing residual noise. On the 0.55T prostate MRI dataset, a reader study showed radiologists preferred Rep2Rep-denoised 2-average images over 8-average noisy images. Rep2Rep demonstrated robustness to noise-level discrepancies between training and inference, supporting its practical implementation. Conclusion: Rep2Rep learning offers an effective self-supervised denoising for low-field MRI by leveraging routinely acquired multi-repetition data. Its noise-adaptivity enables generalization to different SNR regimes without clean reference images. This makes Rep2Rep learning a promising tool for improving image quality and scan efficiency in low-field MRI. </p>
<blockquote>
<p>目的：本研究提出了一种新型的自我监督噪声自适应图像去噪框架，称为重复到重复（Rep2Rep）学习，用于低场（&lt;1T）MRI应用。方法：Rep2Rep学习通过扩展Noise2Noise框架，对两次重复的MRI采集进行神经网络训练，以一次重复作为输入，另一次作为目标，无需真实数据。它融入了噪声自适应训练，能够在不同的噪声水平上进行去噪泛化，并且具有任意次重复的灵活推理。性能评估是在合成噪声脑MRI和0.55T前列腺MRI数据上进行的，并与监督学习和蒙特卡洛斯坦无偏风险估计（MC-SURE）进行了比较。结果：Rep2Rep学习在合成和0.55T MRI数据集上的表现均优于MC-SURE。在合成脑数据上，其去噪质量可与监督学习相媲美，并且超过MC-SURE，特别是在保留结构细节和减少残留噪声方面。在0.55T前列腺MRI数据集上，读者研究结果显示，放射科医生更喜欢使用Rep2Rep去噪的2次平均图像而不是8次平均的噪声图像。Rep2Rep显示出对训练和推理期间噪声水平差异的稳健性，支持其实践实施。结论：Rep2Rep学习通过利用常规获取的多重复数据，为低场MRI提供了有效的自我监督去噪。其噪声适应性使得能够在不同的信噪比制度下实现泛化，而无需清洁的参考图像。这使得Rep2Rep学习成为提高低场MRI图像质量和扫描效率的有前途的工具。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17698v1">PDF</a> 13 pages, 9 figures, 1 table, supplementary information at end of   document</p>
<p><strong>Summary</strong><br>    本研究提出了一种新型的基于自监督噪声自适应的图像去噪框架，名为重复学习（Rep2Rep），适用于低场MRI应用。方法通过在两个重复的MRI采集上训练神经网络，以一次重复作为输入，另一次作为目标，无需真实数据。它实现了噪声自适应训练，可在不同的噪声水平上实现去噪泛化并灵活推理。Rep2Rep在合成噪声大脑MRI和0.55T前列腺MRI数据上的表现均优于MC-SURE。读者研究表明，放射科医生更倾向于选择经过Rep2Rep处理后的两次平均图像而不是八次平均噪声图像。结论指出，Rep2Rep学习为低场MRI提供了一种有效的自监督去噪方法，利用常规获取的多重复数据。其噪声适应性使得能够在没有清洁参考图像的情况下适应不同的信噪比环境。这使其成为提高低场MRI图像质量和扫描效率的潜力工具。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Rep2Rep学习是一种用于低场MRI的自监督噪声自适应图像去噪框架。</li>
<li>该方法通过重复MRI采集训练神经网络，无需真实数据作为参照。</li>
<li>Rep2Rep实现了在不同噪声水平上的去噪泛化并允许灵活的推理。</li>
<li>在合成大脑MRI和0.55T前列腺MRI数据上，Rep2Rep表现优于MC-SURE和其他方法。</li>
<li>读者研究支持了Rep2Rep在实际应用中的优越性。</li>
<li>Rep2Rep能够适应不同的信噪比环境，提高了其在不同环境下的实用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17698">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d4ff568aadca0897bbf49396281e914e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8850a5679830dff771f82fdf42174d64.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Quantifying-jet-interstellar-medium-interactions-in-Cyg-X-1-Insights-from-dual-frequency-bow-shock-detection-with-MeerKAT"><a href="#Quantifying-jet-interstellar-medium-interactions-in-Cyg-X-1-Insights-from-dual-frequency-bow-shock-detection-with-MeerKAT" class="headerlink" title="Quantifying jet-interstellar medium interactions in Cyg X-1: Insights   from dual-frequency bow shock detection with MeerKAT"></a>Quantifying jet-interstellar medium interactions in Cyg X-1: Insights   from dual-frequency bow shock detection with MeerKAT</h2><p><strong>Authors:P. Atri, S. E. Motta, Jakob van den Eijnden, James H. Matthews, James C. A. Miller-Jones, Rob Fender, David Williams-Baldwin, Ian Heywood, Patrick Woudt</strong></p>
<p>Accretion and outflows are astrophysical phenomena observed across a wide range of objects, from white dwarfs to supermassive black holes. Developing a complete picture of these processes requires complementary studies across this full spectrum of jet-launching sources. Jet-interstellar medium (ISM) interaction sites near black hole X-ray binaries provide unique laboratories to study jet energetics. This work aims to detect and characterise the bow shock near one black hole X-ray binary, Cyg X-1, and then use this bow shock structure to parametrise the properties of the jet launched by Cyg X-1 over its lifetime. We used the MeerKAT radio telescope to investigate the bow shock structure formed by the interaction between the jets of Cyg X-1 and the ISM. We successfully detect the bow shock north of Cyg X-1 in the L and S bands and report its size and brightness. We present the spectral index distribution across the bow shock, which is in the range -0.9 to 0.4, with an error distribution (0.6 to 1.5) that peaks at unity. We determine that the unshocked ISM density is 6-7 cm^-3 for a temperature range of 10^4 to 3<em>10^6 K. This temperature range suggests that the velocity of the bow shock is 21 km&#x2F;s to 364 km&#x2F;s. The age of the Cyg X-1 jet responsible for the bow shock is 0.04 to 0.3 Myr, and the power of the jet is constrained to 2</em>10^31 ergs&#x2F;s to 10^35 ergs&#x2F;s. We also detect new morphological features of the bow shock in the S-band image. The comparison of archival H_alpha maps with the new radio observations hints at different regions of emission, different temperature ranges, and different ISM densities. The spectral index suggests a consistent emission origin across the structure. The ISM density around Cyg X-1 is on the higher end for Galactic environments, and our results indicate a lower jet energy transport rate than prior estimates. </p>
<blockquote>
<p>积云和流出是在白矮星到超大质量黑洞等各种天体中观察到的天文现象。要全面理解这些过程，需要对这一系列喷射源进行全面的研究。黑洞X射线双星附近的喷射流与星际介质（ISM）相互作用区域为研究喷射流能量提供了独特的实验室。这项工作旨在检测靠近黑洞X射线双星之一的赛格马X-1（Cyg X-1）的弓形冲击波，并利用这一弓形冲击波结构来参数化Cyg X-1在其生命周期内喷射流的特性。我们使用MeerKAT射电望远镜来研究由Cyg X-1喷射流与星际介质的相互作用所形成的弓形冲击波结构。我们成功地在L波段和S波段检测到Cyg X-1北部的弓形冲击波，并报告了其大小和亮度。我们展示了弓形冲击波的光谱指数分布范围在-0.9到0.4之间，误差分布（0.6到1.5）以单位值为中心。我们确定了在温度范围为10^4到3<em>10^6 K的情况下，未受冲击的星际介质密度是6-7 cm^-3。这个温度范围表明弓形冲击波的速度是21 km&#x2F;s到364 km&#x2F;s。造成弓形冲击波的Cyg X-1喷射流的年龄为0.04到0.3Myr，喷射功率被限制在2</em>10^31 ergs&#x2F;s到10^35 ergs&#x2F;s之间。我们还发现了S波段图像中弓形冲击波的新形态特征。档案中的H_alpha地图与新射电观测结果的比较暗示了不同的发射区域、不同的温度范围和不同的星际介质密度。光谱指数表明整个结构的发射来源是一致的。Cyg X-1周围的星际介质密度处于银河环境的高端，我们的结果指示喷射流能量传输率低于先前的估计。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17635v1">PDF</a> 14 pages, 7 figures, Published in A&amp;A</p>
<p><strong>Summary</strong><br>     观测Cyg X-1黑洞X射线双星附近射流与星际介质相互作用形成的弓形冲击波，以研究射流能量学。使用MeerKAT望远镜成功检测到Cyg X-1北部的弓形冲击波，并报告其大小和亮度。弓形冲击波的频谱指数分布范围在-0.9至0.4之间，并确定未受惊扰的星际介质密度和温度范围，进一步估算出弓形冲击的速度、射流负责时间和功率。比较存档的H_alpha地图与新的无线电观测结果，暗示发射区域、温度范围和星际介质密度的差异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>观测到了Cyg X-1黑洞X射线双星附近的弓形冲击波结构，此为研究射流能量学的重要实验。</li>
<li>使用MeerKAT望远镜成功检测到弓形冲击波在L波段和S波段的存在，并报告其尺寸和亮度信息。</li>
<li>弓形冲击波的频谱指数分布范围及其误差分布已确定。</li>
<li>通过未受惊扰的星际介质密度估计了弓形冲击的速度范围为21 km&#x2F;s至364 km&#x2F;s。</li>
<li>Cyg X-1射流的年龄和功率已经估算出来，给出了具体数值范围。</li>
<li>新观察到的弓形冲击波形态特征揭示了更多关于射流与星际介质相互作用的信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17635">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a1d44f082fe8d7937af3596764b68cfa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fd6dbff82630de4c102f4db590e1f72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebf51a422cbc559c5b40603de3e7e5d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4d4a54ef03e179a2c3adceaaa6c82ae.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Occlusion-Aware-Self-Supervised-Monocular-Depth-Estimation-for-Weak-Texture-Endoscopic-Images"><a href="#Occlusion-Aware-Self-Supervised-Monocular-Depth-Estimation-for-Weak-Texture-Endoscopic-Images" class="headerlink" title="Occlusion-Aware Self-Supervised Monocular Depth Estimation for   Weak-Texture Endoscopic Images"></a>Occlusion-Aware Self-Supervised Monocular Depth Estimation for   Weak-Texture Endoscopic Images</h2><p><strong>Authors:Zebo Huang, Yinghui Wang</strong></p>
<p>We propose a self-supervised monocular depth estimation network tailored for endoscopic scenes, aiming to infer depth within the gastrointestinal tract from monocular images. Existing methods, though accurate, typically assume consistent illumination, which is often violated due to dynamic lighting and occlusions caused by GI motility. These variations lead to incorrect geometric interpretations and unreliable self-supervised signals, degrading depth reconstruction quality. To address this, we introduce an occlusion-aware self-supervised framework. First, we incorporate an occlusion mask for data augmentation, generating pseudo-labels by simulating viewpoint-dependent occlusion scenarios. This enhances the model’s ability to learn robust depth features under partial visibility. Second, we leverage semantic segmentation guided by non-negative matrix factorization, clustering convolutional activations to generate pseudo-labels in texture-deprived regions, thereby improving segmentation accuracy and mitigating information loss from lighting changes. Experimental results on the SCARED dataset show that our method achieves state-of-the-art performance in self-supervised depth estimation. Additionally, evaluations on the Endo-SLAM and SERV-CT datasets demonstrate strong generalization across diverse endoscopic environments. </p>
<blockquote>
<p>我们提出了一种针对内窥镜场景的自监督单目深度估计网络，旨在从单目图像中推断胃肠道内的深度。现有方法虽然准确，但通常假设光照一致，而由于胃肠道的动态照明和由GI运动引起的遮挡，这一假设经常被违反。这些变化导致几何解释错误和自监督信号不可靠，从而降低了深度重建质量。为了解决这一问题，我们引入了一种遮挡感知自监督框架。首先，我们采用遮挡掩膜进行数据增强，通过模拟视点相关的遮挡场景生成伪标签。这增强了模型在部分可见情况下学习稳健深度特征的能力。其次，我们利用非负矩阵分解引导语义分割，聚类卷积激活以在纹理缺失区域生成伪标签，从而提高分割准确性并缓解由光照变化引起的信息损失。在SCARED数据集上的实验结果表明，我们的方法在自监督深度估计方面达到了最新性能。此外，在Endo-SLAM和SERV-CT数据集上的评估证明了其在多种内窥镜环境中的强大泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17582v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种针对内窥镜场景的自监督单目深度估计网络，旨在从单目图像中推断胃肠道内的深度。为解决因光照动态变化和胃肠道运动导致的遮挡问题，引入了一种遮挡感知自监督框架。通过数据增强生成伪标签，提高模型在部分可见情况下的深度特征学习能力。同时，结合非负矩阵分解引导语义分割，改善纹理缺失区域的伪标签生成，提高分割精度并减轻光照变化的信息损失。实验结果表明，该方法在自我监督深度估计方面达到最新技术水平，并在不同内窥镜环境中表现出强大的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种自监督单目深度估计网络，专门用于内窥镜场景。</li>
<li>旨在解决因动态光照和胃肠道运动导致的遮挡问题。</li>
<li>引入遮挡感知自监督框架，通过数据增强生成伪标签，提高模型在部分可见情况下的学习能力。</li>
<li>结合非负矩阵分解引导语义分割，改善纹理缺失区域的伪标签质量。</li>
<li>实验结果表明，该方法在自我监督深度估计方面表现优异。</li>
<li>在不同内窥镜环境中的泛化能力强。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17582">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-867e9b46844ed31e9b2ed97e56b5c14e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31e31ac0327f6b8fa43dd574d98d48b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f47cbb4fe62507d6fcf42cdf00f68e1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Mamba-Sea-A-Mamba-based-Framework-with-Global-to-Local-Sequence-Augmentation-for-Generalizable-Medical-Image-Segmentation"><a href="#Mamba-Sea-A-Mamba-based-Framework-with-Global-to-Local-Sequence-Augmentation-for-Generalizable-Medical-Image-Segmentation" class="headerlink" title="Mamba-Sea: A Mamba-based Framework with Global-to-Local Sequence   Augmentation for Generalizable Medical Image Segmentation"></a>Mamba-Sea: A Mamba-based Framework with Global-to-Local Sequence   Augmentation for Generalizable Medical Image Segmentation</h2><p><strong>Authors:Zihan Cheng, Jintao Guo, Jian Zhang, Lei Qi, Luping Zhou, Yinghuan Shi, Yang Gao</strong></p>
<p>To segment medical images with distribution shifts, domain generalization (DG) has emerged as a promising setting to train models on source domains that can generalize to unseen target domains. Existing DG methods are mainly based on CNN or ViT architectures. Recently, advanced state space models, represented by Mamba, have shown promising results in various supervised medical image segmentation. The success of Mamba is primarily owing to its ability to capture long-range dependencies while keeping linear complexity with input sequence length, making it a promising alternative to CNNs and ViTs. Inspired by the success, in the paper, we explore the potential of the Mamba architecture to address distribution shifts in DG for medical image segmentation. Specifically, we propose a novel Mamba-based framework, Mamba-Sea, incorporating global-to-local sequence augmentation to improve the model’s generalizability under domain shift issues. Our Mamba-Sea introduces a global augmentation mechanism designed to simulate potential variations in appearance across different sites, aiming to suppress the model’s learning of domain-specific information. At the local level, we propose a sequence-wise augmentation along input sequences, which perturbs the style of tokens within random continuous sub-sequences by modeling and resampling style statistics associated with domain shifts. To our best knowledge, Mamba-Sea is the first work to explore the generalization of Mamba for medical image segmentation, providing an advanced and promising Mamba-based architecture with strong robustness to domain shifts. Remarkably, our proposed method is the first to surpass a Dice coefficient of 90% on the Prostate dataset, which exceeds previous SOTA of 88.61%. The code is available at <a target="_blank" rel="noopener" href="https://github.com/orange-czh/Mamba-Sea">https://github.com/orange-czh/Mamba-Sea</a>. </p>
<blockquote>
<p>针对医学图像分割中的分布偏移问题，领域泛化（DG）作为一种在源领域训练模型并推广至未见目标领域的方法，已显示出其巨大潜力。现有的DG方法主要基于CNN或ViT架构。最近，以Mamba为代表的高级状态空间模型在多种监督医学图像分割中取得了令人鼓舞的结果。Mamba的成功主要归功于其能够捕捉长距离依赖关系的同时保持线性计算复杂度，使其成为CNN和ViT的有前途的替代方案。受成功的启发，在本文中，我们探索了Mamba架构在解决医学图像分割的DG中的分布偏移问题的潜力。具体来说，我们提出了一种基于Mamba的新框架Mamba-Sea，它结合了全局到局部的序列增强技术，以提高模型在领域偏移问题下的泛化能力。我们的Mamba-Sea引入了一种全局增强机制，旨在模拟不同站点外观的潜在变化，旨在抑制模型对领域特定信息的学习。在局部层面，我们提出了沿输入序列的序列级增强，通过建模和重新采样与领域偏移相关的风格统计信息，扰动随机连续子序列内标记的风格。据我们所知，Mamba-Sea是首次探索医学图像分割中Mamba泛化的工作，提供了一种先进且对领域偏移具有强大稳健性的Mamba架构。值得注意的是，我们提出的方法在前列腺数据集上的Dice系数首次超过了90％，超过了之前的最佳纪录88.61％。代码可在<a target="_blank" rel="noopener" href="https://github.com/orange-czh/mamba-sea%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/orange-czh/Mamba-Sea找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17515v1">PDF</a> Accepted by IEEE TMI 2025. The code is available at   <a target="_blank" rel="noopener" href="https://github.com/orange-czh/Mamba-Sea">https://github.com/orange-czh/Mamba-Sea</a></p>
<p><strong>Summary</strong></p>
<p>本文探索了基于Mamba架构在医学图像分割中的域泛化能力，以应对分布偏移问题。提出一种新型Mamba-Sea框架，结合全局到局部序列增强，提高模型在域偏移问题下的泛化性能。通过全局增强机制模拟不同站点外观的潜在变化，抑制模型对域特定信息的学习。在局部层面，通过建模和重采样与域偏移相关的风格统计，对输入序列进行序列增强。Mamba-Sea在前列腺数据集上的Dice系数超过90%，超越之前的最优解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mamba架构因能捕捉长距离依赖并保持线性复杂度，在医学图像分割中展现出潜力。</li>
<li>现有域泛化（DG）方法主要基于CNN或ViT架构，而Mamba-Sea框架探索了Mamba架构在医学图像分割中的域泛化潜力。</li>
<li>Mamba-Sea通过全局到局部序列增强提高模型泛化能力，模拟不同站点的外观变化并抑制域特定信息的学习。</li>
<li>Mamba-Sea首次超过前列腺数据集上的Dice系数90%，表现超越先前最佳解决方案。</li>
<li>Mamba-Sea框架是首个探索Mamba在医学图像分割中泛化的工作，提供强大且对域偏移具有鲁棒性的Mamba基础架构。</li>
<li>提出的全局和局部增强机制有助于模型适应不同的域偏移情况。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17515">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b1c4b964026f29dece36c00018622bb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca3e394c2601a6df09ec17fc1bfd5459.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f407de78a2c81fd4b30711f2b572e5b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-36c2caa6846f50f675b91a2d7489cd7d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Physiological-neural-representation-for-personalised-tracer-kinetic-parameter-estimation-from-dynamic-PET"><a href="#Physiological-neural-representation-for-personalised-tracer-kinetic-parameter-estimation-from-dynamic-PET" class="headerlink" title="Physiological neural representation for personalised tracer kinetic   parameter estimation from dynamic PET"></a>Physiological neural representation for personalised tracer kinetic   parameter estimation from dynamic PET</h2><p><strong>Authors:Kartikay Tehlan, Thomas Wendler</strong></p>
<p>Dynamic positron emission tomography (PET) with [$^{18}$F]FDG enables non-invasive quantification of glucose metabolism through kinetic analysis, often modelled by the two-tissue compartment model (TCKM). However, voxel-wise kinetic parameter estimation using conventional methods is computationally intensive and limited by spatial resolution. Deep neural networks (DNNs) offer an alternative but require large training datasets and significant computational resources. To address these limitations, we propose a physiological neural representation based on implicit neural representations (INRs) for personalized kinetic parameter estimation. INRs, which learn continuous functions, allow for efficient, high-resolution parametric imaging with reduced data requirements. Our method also integrates anatomical priors from a 3D CT foundation model to enhance robustness and precision in kinetic modelling. We evaluate our approach on an [$^{18}$F]FDG dynamic PET&#x2F;CT dataset and compare it to state-of-the-art DNNs. Results demonstrate superior spatial resolution, lower mean-squared error, and improved anatomical consistency, particularly in tumour and highly vascularized regions. Our findings highlight the potential of INRs for personalized, data-efficient tracer kinetic modelling, enabling applications in tumour characterization, segmentation, and prognostic assessment. </p>
<blockquote>
<p>动态正电子发射断层扫描（PET）通过[$^{18}$F]FDG实现了非侵入性的葡萄糖代谢定量评估，通常通过动力学分析进行建模，动力学分析常常以两组织间隔模型（TCKM）为模型。然而，使用传统方法进行体素级的动力学参数估计在计算上很密集且受限于空间分辨率。深度神经网络（DNNs）提供了另一种选择，但它们需要大量训练数据集和重要的计算资源。为了解决这些限制，我们提出了一种基于隐神经表示法（INR）的生理神经表示法用于个性化动力学参数估计。INR学习连续函数，允许高效的高分辨率参数成像并减少数据需求。我们的方法还结合了来自三维CT基础模型的解剖学先验知识，以提高动力学建模的稳健性和精确度。我们在一组[$^{18}$F]FDG动态PET&#x2F;CT数据集上评估了我们的方法并与最先进的DNNs进行了比较。结果表明，我们的方法具有卓越的空间分辨率、更低的均方误差以及更好的解剖学一致性，特别是在肿瘤和高度血管化的区域中。我们的研究结果表明了隐神经表示法在个性化、高效率的示踪剂动力学建模中的潜力，可为肿瘤特征描述、分割和预后评估等应用提供支持。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17122v1">PDF</a> The code is available at: <a target="_blank" rel="noopener" href="https://github.com/tkartikay/PhysNRPET">https://github.com/tkartikay/PhysNRPET</a></p>
<p><strong>Summary</strong><br>医学图像动态正电子发射断层扫描（PET）可通过$^{18}$F标记的脱氧葡萄糖（FDG）进行葡萄糖代谢的非侵入性定量分析。常规方法在计算上较为密集且受空间分辨率限制。本文提出了一种基于隐神经表征的方法来解决这些问题，实现个性化参数估计和高效高分辨参数成像，且减少了数据需求，还融合了来自三维CT基础模型的解剖学先验信息以提升其在动力学建模中的稳健性和精确性。评估显示，该方法在$^{18}$F标记的脱氧葡萄糖动态PET&#x2F;CT数据集上表现出更好的空间分辨率、更低的均方误差和更好的解剖学一致性，特别是在肿瘤和高度血管化区域。这为肿瘤特征化、分割和预后评估提供了潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>动态PET结合$^{18}$F标记的脱氧葡萄糖可用于非侵入性地评估葡萄糖代谢。</li>
<li>传统方法在参数估计上计算密集且受空间分辨率限制。</li>
<li>隐神经表征方法用于个性化参数估计，实现高效高分辨成像，减少数据需求。</li>
<li>结合三维CT基础模型的解剖学先验信息增强动力学建模的稳健性和准确性。</li>
<li>评估结果显示该方法在特定数据集上表现优越，特别是在肿瘤和高度血管化区域。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17122">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f9567e7420c2354604e593ac979a0ce8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e807dbe62749c56e35f90ae58f7194da.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Meta-Entity-Driven-Triplet-Mining-for-Aligning-Medical-Vision-Language-Models"><a href="#Meta-Entity-Driven-Triplet-Mining-for-Aligning-Medical-Vision-Language-Models" class="headerlink" title="Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language   Models"></a>Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language   Models</h2><p><strong>Authors:Saban Ozturk, Melih B. Yilmaz, Muti Kara, M. Talat Yavuz, Aykut Koç, Tolga Çukur</strong></p>
<p>Diagnostic imaging relies on interpreting both images and radiology reports, but the growing data volumes place significant pressure on medical experts, yielding increased errors and workflow backlogs. Medical vision-language models (med-VLMs) have emerged as a powerful framework to efficiently process multimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit their performance hinges on how well image and text representations are aligned. Existing alignment methods, predominantly based on contrastive learning, prioritize separation between disease classes over segregation of fine-grained pathology attributes like location, size or severity, leading to suboptimal representations. Here, we propose MedTrim (Meta-entity-driven Triplet mining), a novel method that enhances image-text alignment through multimodal triplet learning synergistically guided by disease class as well as adjectival and directional pathology descriptors. Unlike common alignment methods that separate broad disease classes, MedTrim leverages structured meta-entity information to preserve subtle but clinically significant intra-class variations. For this purpose, we first introduce an ontology-based entity recognition module that extracts pathology-specific meta-entities from CXR reports, as annotations on pathology attributes are rare in public datasets. For refined sample selection in triplet mining, we then introduce a novel score function that captures an aggregate measure of inter-sample similarity based on disease classes and adjectival&#x2F;directional descriptors. Lastly, we introduce a multimodal triplet alignment objective for explicit within- and cross-modal alignment between samples sharing detailed pathology characteristics. Our demonstrations indicate that MedTrim improves performance in downstream retrieval and classification tasks compared to state-of-the-art alignment methods. </p>
<blockquote>
<p>诊断成像依赖于对图像和放射学报告的解释，但日益增长的数据量给医学专家带来了巨大的压力，导致了误差增加和工作流程积压。医疗视觉语言模型（med-VLM）作为一个强大的框架，能够有效地处理多模态成像数据，特别是在胸部X射线（CXR）评估中表现出色。然而，其性能的好坏取决于图像和文本表示的对齐程度。现有的对齐方法主要基于对比学习，更侧重于疾病类别之间的区分，而非细微的病理属性（如位置、大小或严重程度）的分离，导致表示不佳。在这里，我们提出了MedTrim（基于元实体驱动的三元组挖掘），这是一种通过疾病类别以及形容词和方向性病理描述符协同引导的多模态三元组学习的新方法，用于增强图像文本对齐。与常见的仅根据疾病类别进行分离的对齐方法不同，MedTrim利用结构化的元实体信息来保留微妙的但临床上重要的类内变化。为此，我们首先引入了一个基于本体论的实体识别模块，从CXR报告中提取病理特定的元实体，因为公共数据集中关于病理属性的注释很少见。为了进行精细的样本选择进行三元组挖掘，然后我们引入了一个新的评分函数，该函数基于疾病类别和形容词&#x2F;方向性描述符来捕获样本间相似性的综合度量。最后，我们引入了多模态三元组对齐目标，用于在具有详细病理特征的样本之间进行明确的内部和跨模态对齐。我们的演示表明，与最新的对齐方法相比，MedTrim在下游检索和分类任务中的性能有所提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15929v2">PDF</a> 18 pages, 7 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>本文探讨了医疗影像诊断中面临的挑战，包括数据量大导致的专家压力增大、错误增多和工作流程积压问题。为解决这些问题，文章提出了一种名为MedTrim的新方法，通过多模态三元组学习，以疾病类别及形容词和方向性病理描述符为引导，提高图像与文本的对齐性能。MedTrim采用基于本体的实体识别模块，从胸腔X射线报告中提取病理特异性元实体，并引入新的评分函数和跨模态三元组对齐目标，以改进样本选择和细化病理特征的共享。总体而言，MedTrim提高了下游检索和分类任务的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学影像诊断面临数据量大带来的挑战，需要更高效的多模态数据处理方法。</li>
<li>医疗视觉语言模型（med-VLMs）在处理多模态成像数据方面表现出强大潜力。</li>
<li>现有图像与文本对齐方法主要关注疾病类别的分离，但忽略了细粒度病理属性的表征，导致表示不佳。</li>
<li>MedTrim通过多模态三元组学习提高图像与文本的对齐精度，同时考虑疾病类别和形容词、方向性病理描述符。</li>
<li>MedTrim采用基于本体的实体识别模块，从胸腔X射线报告中提取病理特异性元实体，以保留公共数据集中罕见的病理属性注释。</li>
<li>引入新的评分函数，基于疾病类别和形容词&#x2F;方向性描述符来衡量样本间的相似度，以优化样本选择。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15929">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-077321fcf7a12406024830d72c6455ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2f84989aa64125a6a8ae84fab8dab60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ca291eeefc0ffdef1d6c755ed0f8c2c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Static-linear-density-response-from-X-ray-Thomson-scattering-measurements-a-case-study-of-warm-dense-beryllium"><a href="#Static-linear-density-response-from-X-ray-Thomson-scattering-measurements-a-case-study-of-warm-dense-beryllium" class="headerlink" title="Static linear density response from X-ray Thomson scattering   measurements: a case study of warm dense beryllium"></a>Static linear density response from X-ray Thomson scattering   measurements: a case study of warm dense beryllium</h2><p><strong>Authors:Sebastian Schwalbe, Hannah Bellenbaum, Tilo Döppner, Maximilian Böhme, Thomas Gawne, Dominik Kraus, Michael J. MacDonald, Zhandos Moldabekov, Panagiotis Tolias, Jan Vorberger, Tobias Dornheim</strong></p>
<p>Linear response theory is ubiquitous throughout physics and plays a central role in the theoretical description of warm dense matter – an extreme state that occurs within compact astrophysical objects and that is traversed on the compression path of a fuel capsule in inertial confinement fusion applications. Here we show how one can relate the static linear density response function to X-ray Thomson scattering (XRTS) measurements, which opens up new possibilities for the diagnostics of extreme states of matter, and for the rigorous assessment and verification of theoretical models and approximations. As a practical example, we consider an XRTS data set of warm dense beryllium taken at the National Ignition Facility [T.<del>D&quot;oppner \emph{et al.}, \textit{Nature} \textbf{618}, 270-275 (2023)]. The comparison with state-of-the-art \emph{ab initio} path integral Monte Carlo (PIMC) simulations [T.</del>Dornheim \emph{et al.}, \textit{Nature Commun.}~(in print), arXiv:2402.19113] gives us a best estimate of the mass density of $\rho&#x3D;18\pm6,$g&#x2F;cc, which is consistent with previous PIMC and density functional theory based studies, but rules out the original estimate of $\rho&#x3D;34\pm4,$g&#x2F;cc based on a Chihara model fit. </p>
<blockquote>
<p>线性响应理论在物理学中普遍存在，并在描述温暖致密物质的理论中起着核心作用。这是一种极端的物质状态，出现在致密的天体物理对象中，也出现在惯性约束聚变应用的燃料胶囊压缩路径上。在这里，我们展示了如何将静态线性密度响应函数与X射线汤姆森散射（XRTS）测量相关联，这为极端物质状态的诊断、理论模型和近似的严格评估和验证提供了新的可能性。作为一个实际例子，我们考虑了在国家点火设施（T. Doppner等人，《自然》杂志，第618期，第270-275页（2023年））获得的温暖致密铍的XRTS数据集。与最新最先进的从头开始路径积分蒙特卡罗（PIMC）模拟（T. Dornheim等人，《自然通讯》（印刷中），arXiv：2402.19113）的比较，为我们提供了最佳质量密度估计值为ρ&#x3D;18±6 g&#x2F;cc。这与之前的PIMC和基于密度泛函理论的研究相一致，但排除了基于Chihara模型拟合得到的原始估计值ρ&#x3D;34±4 g&#x2F;cc。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13611v2">PDF</a> </p>
<p><strong>Summary</strong><br>     线性响应理论在物理学中普遍存在，对于描述热密物质的理论起到了核心作用。本文通过展示静态线性密度响应函数与X射线汤姆森散射测量的关系，为极端态物质的诊断以及理论模型和近似值的严格评估和验证提供了新的可能性。以美国国家点火装置对热密态铍物质的研究为例，与最新的从头算路径积分蒙特卡罗模拟相比较，得到最佳估计物质密度为$\rho&#x3D;18\pm6g&#x2F;cc$，与前人研究结果一致，排除了基于奇哈拉模型拟合的原始估计$\rho&#x3D;34\pm4g&#x2F;cc$。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>线性响应理论在描述热密物质的理论中扮演核心角色，特别是在物理学领域。</li>
<li>通过将静态线性密度响应函数与X射线汤姆森散射测量相结合，为极端状态物质的诊断和理论模型的评估提供了新的方法。</li>
<li>文章以美国国家点火装置对热密态铍物质的研究为例，展示了该方法的应用。</li>
<li>与最新的从头算路径积分蒙特卡罗模拟相比较，得到了物质密度的最佳估计值。</li>
<li>最佳估计物质密度为$\rho&#x3D;18\pm6g&#x2F;cc$，与前人研究结果一致。</li>
<li>排除基于奇哈拉模型拟合的原始估计$\rho&#x3D;34\pm4g&#x2F;cc$。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13611">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-05b1e8bce7611bee1074e7b3343565b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc221e242d3621eb574c061234735577.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fd360b6f56fe56da9d12d1074df1263.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="OmniMamba4D-Spatio-temporal-Mamba-for-longitudinal-CT-lesion-segmentation"><a href="#OmniMamba4D-Spatio-temporal-Mamba-for-longitudinal-CT-lesion-segmentation" class="headerlink" title="OmniMamba4D: Spatio-temporal Mamba for longitudinal CT lesion   segmentation"></a>OmniMamba4D: Spatio-temporal Mamba for longitudinal CT lesion   segmentation</h2><p><strong>Authors:Justin Namuk Kim, Yiqiao Liu, Rajath Soans, Keith Persson, Sarah Halek, Michal Tomaszewski, Jianda Yuan, Gregory Goldmacher, Antong Chen</strong></p>
<p>Accurate segmentation of longitudinal CT scans is important for monitoring tumor progression and evaluating treatment responses. However, existing 3D segmentation models solely focus on spatial information. To address this gap, we propose OmniMamba4D, a novel segmentation model designed for 4D medical images (3D images over time). OmniMamba4D utilizes a spatio-temporal tetra-orientated Mamba block to effectively capture both spatial and temporal features. Unlike traditional 3D models, which analyze single-time points, OmniMamba4D processes 4D CT data, providing comprehensive spatio-temporal information on lesion progression. Evaluated on an internal dataset comprising of 3,252 CT scans, OmniMamba4D achieves a competitive Dice score of 0.682, comparable to state-of-the-arts (SOTA) models, while maintaining computational efficiency and better detecting disappeared lesions. This work demonstrates a new framework to leverage spatio-temporal information for longitudinal CT lesion segmentation. </p>
<blockquote>
<p>对纵向CT扫描进行精确分割对于监测肿瘤进展和评估治疗效果非常重要。然而，现有的3D分割模型只关注空间信息。为了弥补这一空白，我们提出了OmniMamba4D，这是一种专为4D医学图像（随时间变化的3D图像）设计的全新分割模型。OmniMamba4D利用时空四面体定向的Mamba块，有效地捕捉空间和时间特征。与传统的仅分析单一时间点的3D模型不同，OmniMamba4D处理4D CT数据，提供关于病灶进展的全面时空信息。在包含3252次CT扫描的内部数据集上进行的评估表明，OmniMamba4D的Dice分数达到了具有竞争力的0.682，与最先进（SOTA）模型相比不相上下，同时保持了计算效率，并更好地检测到了消失的病变。这项工作展示了一个利用时空信息进行纵向CT病灶分割的新框架。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09655v2">PDF</a> Accepted at IEEE International Symposium on Biomedical Imaging (ISBI)   2025</p>
<p><strong>Summary</strong><br>医学图像准确分割对监测肿瘤进展和评估治疗效果至关重要。现有三维分割模型主要关注空间信息，为此，我们提出OmniMamba4D模型，专为四维医学图像设计，可捕捉时空特征。该模型在包含3252次CT扫描的内部数据集上取得了具有竞争力的Dice分数（0.682），可与最先进的模型相��e比较，同时保持计算效率和更好的消失病变检测能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像准确分割对肿瘤进展监测和治疗反应评估至关重要。</li>
<li>现有三维分割模型主要关注空间信息，忽略了时间维度。</li>
<li>OmniMamba4D模型专为四维医学图像设计，结合了时空特征。</li>
<li>OmniMamba4D模型使用时空四方向Mamba块进行有效特征捕捉。</li>
<li>该模型在内部数据集上取得了竞争力的Dice分数，与最先进的模型相当。</li>
<li>OmniMamba4D模型在计算效率和消失病变检测方面表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09655">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cce4b9cecf147d33cfee9c7bdd53102c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3999d6e301fe7f96cbbd74fcf496f918.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8706fe0cc12d7a6040dc3699bf8972ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adc8986214f5a0ac4cd75e33e7e711a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22678d39febc678d85bfad7f4a458267.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Q-ball-mechanism-of-electron-transport-properties-of-high-T-c-superconductors"><a href="#Q-ball-mechanism-of-electron-transport-properties-of-high-T-c-superconductors" class="headerlink" title="Q-ball mechanism of electron transport properties of high-T$_c$   superconductors"></a>Q-ball mechanism of electron transport properties of high-T$_c$   superconductors</h2><p><strong>Authors:S. I. Mukhin</strong></p>
<p>Proposed recently by the author Q-ball mechanism of the pseudogap state and high-Tc superconductivity in cuprates (2022) was supported by micro X-ray diffraction data in HgBa$<em>2$CuO$</em>{4+y}$ (2023). In the present paper it is demonstrated that T-linear temperature dependence of electrical resistivity arises naturally in the Q-ball gas phase, that may explain corresponding experimental data in the “strange metal” phase of high-T$_c$ cuprates, as reviewed by Barisic et al. (2013). In the present theory it arises due to scattering of electrons on the Q-balls gas of condensed charge&#x2F;spin fluctuations. Close to the lowest temperature boundary of the “strange metal” phase, at which Q-ball radius diverges, electrical resistivity caused by a slide of the Q-balls as a whole is calculated using fluctuation paraconductivity calculation method by Alex Abrikosov (1987). The diamagnetic response of Q-balls gas is calculated as well and shows good accord with experimental data by L.Li et al. (2010) in the “strange metal” phase. In total, obtained results demonstrate different properties of the correlated electrons systems that arise due to formation of Q-balls possessing internal bosonic frequency $\Omega&#x3D;2\pi nT$ in Matsubara time and, thus, forming the quantum thermodynamic time polycrystals. Presented theory may give a clue concerning a possible mechanism of the experimentally measured properties of high-T$_c$ cuprates in the “strange metal” phase of their phase diagram. We believe , these results provide support to the quantum thermodynamic time crystal model of the Euclidean Q-balls considered in the present paper. </p>
<blockquote>
<p>作者近期提出的关于伪间隙态和高温超导性的Q球机制（2022年）得到了汞基铜酸盐（HgBa$<em>2$CuO$</em>{4+y}$）中的微X射线衍射数据支持（2023年）。本文展示了线性温度依赖的电阻率自然产生于Q球气态相中，这可以解释高温铜酸盐的“奇异金属”相中相应的实验数据，如巴里斯等人（Barisic et al.，2013）所述。根据当前理论，这是由于电子在凝聚的电荷&#x2F;自旋波动形成的Q球气体上的散射所致。在“奇异金属”相的最低温度边界附近，Q球半径发散，整个Q球的滑动引起的电阻率是通过亚历克斯·阿布里科索夫的涨落超导计算法（Alex Abrikosov，1987）计算的。同时，还计算了Q球气体的抗磁性响应，这与李文等人在“奇异金属”相中的实验结果（L.Li et al.，2010）吻合良好。总体而言，获得的结果展示了由于形成具有内部玻色频率$\Omega&#x3D;2\pi nT$的马修巴时间中的Q球而产生的关联电子系统的不同特性，从而形成了量子热力学时间多晶。本文提出的理论可能为高温铜酸盐的“奇异金属”相中实验测量的性质提供可能的机制线索。我们相信，这些结果为本文中考虑的欧几里得Q球的量子热力学时间晶体模型提供了支持。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09610v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>近期作者提出的伪间隙态的Q球机制和高温超导在铜酸盐中的表现得到了微X射线衍射数据的支持。本文展示了线性温度依赖性的电阻率自然产生于Q球气态，这解释了高温铜酸盐的“奇异金属”相中的实验数据。在该理论中，它源于电子在凝聚的电荷&#x2F;自旋波动的Q球气体上的散射。在“奇异金属”相的最低温度边界附近，Q球的半径发散，电子通过整体的Q球运动造成的电阻率的计算使用阿列克斯·阿布瑞科夫斯的涨落准导计算法。此外，本文计算了Q球气体的顺磁性响应，并与实验数据表现相符。综上，获得的结果表明了因在Matsubara时间中产生内部波色频率而形成的Q球展现出不同的电子系统特性。此理论为我们提供了高温铜酸盐的“奇异金属”相在实验测量性能方面的线索。本文认为，这些结果为量子热力学时间晶体模型提供了支持。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>Q球机制理论得到微X射线衍射数据的支持，证实存在于HgBa$<em>2$CuO$</em>{4+y}$中。</li>
<li>Q球气态自然产生T线性温度依赖性的电阻率，与高温铜酸盐的“奇异金属”相实验数据相符。</li>
<li>电子在凝聚的电荷&#x2F;自旋波动的Q球气体上的散射导致电阻率产生。</li>
<li>在“奇异金属”相的最低温度边界附近，Q球的半径发散，电阻率通过整体的Q球运动计算得出。</li>
<li>Q球气体的顺磁性响应与实验数据相符。</li>
<li>Q球具有内部波色频率，展示了电子系统的不同特性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09610">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1e6c261e1afbda90808f41b84b22aeb2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DiffKillR-Killing-and-Recreating-Diffeomorphisms-for-Cell-Annotation-in-Dense-Microscopy-Images"><a href="#DiffKillR-Killing-and-Recreating-Diffeomorphisms-for-Cell-Annotation-in-Dense-Microscopy-Images" class="headerlink" title="DiffKillR: Killing and Recreating Diffeomorphisms for Cell Annotation in   Dense Microscopy Images"></a>DiffKillR: Killing and Recreating Diffeomorphisms for Cell Annotation in   Dense Microscopy Images</h2><p><strong>Authors:Chen Liu, Danqi Liao, Alejandro Parada-Mayorga, Alejandro Ribeiro, Marcello DiStasio, Smita Krishnaswamy</strong></p>
<p>The proliferation of digital microscopy images, driven by advances in automated whole slide scanning, presents significant opportunities for biomedical research and clinical diagnostics. However, accurately annotating densely packed information in these images remains a major challenge. To address this, we introduce DiffKillR, a novel framework that reframes cell annotation as the combination of archetype matching and image registration tasks. DiffKillR employs two complementary neural networks: one that learns a diffeomorphism-invariant feature space for robust cell matching and another that computes the precise warping field between cells for annotation mapping. Using a small set of annotated archetypes, DiffKillR efficiently propagates annotations across large microscopy images, reducing the need for extensive manual labeling. More importantly, it is suitable for any type of pixel-level annotation. We will discuss the theoretical properties of DiffKillR and validate it on three microscopy tasks, demonstrating its advantages over existing supervised, semi-supervised, and unsupervised methods. The code is available at <a target="_blank" rel="noopener" href="https://github.com/KrishnaswamyLab/DiffKillR">https://github.com/KrishnaswamyLab/DiffKillR</a>. </p>
<blockquote>
<p>随着自动全切片扫描技术的不断进步，数字显微镜图像的激增为生物医学研究和临床诊断提供了重要的机会。然而，在这些图像中准确标注密集的信息仍然是一个主要挑战。为了解决这一问题，我们引入了DiffKillR，这是一种新的框架，它将细胞注释重新构建为原型匹配和图像配准任务的组合。DiffKillR采用两个互补的神经网络：一个学习微分同胚不变的特征空间以实现稳健的细胞匹配，另一个计算细胞之间的精确扭曲场以实现注释映射。使用一小部分已注释的原型，DiffKillR可以有效地在大显微镜图像上传播注释，减少了大量手动标记的需求。更重要的是，它适用于任何类型的像素级注释。我们将讨论DiffKillR的理论属性，并在三个显微镜任务上对其进行验证，展示其与现有的监督、半监督和无监督方法相比的优势。代码可在<a target="_blank" rel="noopener" href="https://github.com/KrishnaswamyLab/DiffKillR">https://github.com/KrishnaswamyLab/DiffKillR</a>获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03058v2">PDF</a> ICASSP 2025, Oral Presentation</p>
<p><strong>Summary</strong><br>医学图像数字化进展迅速，全自动扫描技术推动大量数字显微镜图像生成，为生物医学研究和临床诊疗提供重要机遇。然而，准确标注密集图像信息是一大挑战。为解决此问题，引入DiffKillR框架，将细胞标注转化为原型匹配和图像注册任务组合。DiffKillR采用两个互补神经网络，一个学习保形不变的特性空间以实现稳健的细胞匹配，另一个计算细胞间的精确变形场以实现标注映射。仅需少量已标注原型，DiffKillR即可在大显微镜图像上有效传播标注，减少大量手动标注需求。更重要的是，它适用于任何像素级别的标注。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数字显微镜图像的普及对生物医学研究和临床诊疗有重要意义。</li>
<li>准确标注密集图像信息是当前的挑战。</li>
<li>DiffKillR框架通过将细胞标注转化为原型匹配和图像注册任务组合来解决这一问题。</li>
<li>DiffKillR使用神经网络学习保形不变的特性空间和计算精确变形场。</li>
<li>DiffKillR能够利用少量标注的原型有效传播标注，降低手动标注的需求。</li>
<li>DiffKillR适用于任何类型的像素级别标注。</li>
<li>DiffKillR在三个显微镜任务上的表现优于现有的监督、半监督和无监督方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03058">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-583fe28f8945c6605b10ecede3dee3cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d135ced10c94ee5dafdbfd1a620c7cf2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-635d4b517761115a591d351a909b70f1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DDU-Net-A-Domain-Decomposition-Based-CNN-for-High-Resolution-Image-Segmentation-on-Multiple-GPUs"><a href="#DDU-Net-A-Domain-Decomposition-Based-CNN-for-High-Resolution-Image-Segmentation-on-Multiple-GPUs" class="headerlink" title="DDU-Net: A Domain Decomposition-Based CNN for High-Resolution Image   Segmentation on Multiple GPUs"></a>DDU-Net: A Domain Decomposition-Based CNN for High-Resolution Image   Segmentation on Multiple GPUs</h2><p><strong>Authors:Corné Verburg, Alexander Heinlein, Eric C. Cyr</strong></p>
<p>The segmentation of ultra-high resolution images poses challenges such as loss of spatial information or computational inefficiency. In this work, a novel approach that combines encoder-decoder architectures with domain decomposition strategies to address these challenges is proposed. Specifically, a domain decomposition-based U-Net (DDU-Net) architecture is introduced, which partitions input images into non-overlapping patches that can be processed independently on separate devices. A communication network is added to facilitate inter-patch information exchange to enhance the understanding of spatial context. Experimental validation is performed on a synthetic dataset that is designed to measure the effectiveness of the communication network. Then, the performance is tested on the DeepGlobe land cover classification dataset as a real-world benchmark data set. The results demonstrate that the approach, which includes inter-patch communication for images divided into $16\times16$ non-overlapping subimages, achieves a $2-3,%$ higher intersection over union (IoU) score compared to the same network without inter-patch communication. The performance of the network which includes communication is equivalent to that of a baseline U-Net trained on the full image, showing that our model provides an effective solution for segmenting ultra-high-resolution images while preserving spatial context. The code is available at <a target="_blank" rel="noopener" href="https://github.com/corne00/DDU-Net">https://github.com/corne00/DDU-Net</a>. </p>
<blockquote>
<p>超高分辨率图像的分割面临着损失空间信息或计算效率低下等挑战。针对这些挑战，本文提出了一种结合编码器-解码器架构和域分解策略的新方法。具体来说，引入了一种基于域分解的U-Net（DDU-Net）架构，该架构将输入图像分割成可以在不同设备上独立处理的非重叠斑块。添加通信网络以促进斑块间的信息交换，以增强对空间上下文的理解。在专为测量通信网络有效性而设计的合成数据集上进行实验验证。然后，在DeepGlobe土地覆盖分类数据集上测试性能，作为现实世界基准数据集。结果表明，该方法在处理分割为$ 16\times16$非重叠子图像的图像时，包括斑块间通信，与没有斑块间通信的相同网络相比，其交集(IoU)得分提高了$ 2-3％$。包含通信的网络性能相当于在完整图像上训练的基线U-Net的性能，这表明我们的模型为分割超高分辨率图像同时保留空间上下文提供了有效的解决方案。代码可在 <a target="_blank" rel="noopener" href="https://github.com/corne00/DDU-Net">https://github.com/corne00/DDU-Net</a> 获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.21266v3">PDF</a> </p>
<p><strong>摘要</strong><br>    提出一种结合编码器-解码器架构与域分解策略的新方法，解决超高分辨率图像分割面临的挑战，如空间信息丢失或计算效率低下。引入基于域分解的U-Net（DDU-Net）架构，将输入图像分成可独立处理的无重叠块，通过通信网络进行块间信息交换，增强空间上下文理解。在合成数据集和真实世界的DeepGlobe土地覆盖分类数据集上进行实验验证，结果显示该方法在图像分割为$16\times16$非重叠子图像时加入块间通信，与没有块间通信的网络相比，提高了$2-3%$的交集（IoU）得分。包含通信的网络性能相当于在完整图像上训练的基线U-Net的性能，表明该模型在分割超高分辨率图像时提供了有效的解决方案，同时保留了空间上下文。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>超高分辨率图像分割面临空间信息丢失和计算效率低的问题。</li>
<li>提出结合编码器-解码器架构与域分解策略的新方法，解决这些问题。</li>
<li>引入基于域分解的U-Net（DDU-Net）架构，将图像分成无重叠块进行独立处理。</li>
<li>添加通信网络进行块间信息交换，增强空间上下文理解。</li>
<li>在合成数据集上进行实验验证，测试通信网络的的有效性。</li>
<li>在真实世界的DeepGlobe土地覆盖分类数据集上进行测试，该方法相较于无块间通信的网络提高了$2-3%$的IoU得分。</li>
<li>包含通信的网络性能与在完整图像上训练的基线U-Net相当，表明该模型在分割超高分辨率图像时有效并保留空间上下文。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.21266">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b5415143f9580e01893c88c7b13f2b8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c26c1209c2954b38382fac4baf04799.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8512922c8cf69aee9d003a088f3e0430.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbd3318307973127d2cd6fdf1ebafa45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-203bc6810a63699760659a73304dfeeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a67d5917097145d6b86b39e1f7b90bfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8b96ddf8a703b756f4f04f1ef2c7e31.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ImageFlowNet-Forecasting-Multiscale-Image-Level-Trajectories-of-Disease-Progression-with-Irregularly-Sampled-Longitudinal-Medical-Images"><a href="#ImageFlowNet-Forecasting-Multiscale-Image-Level-Trajectories-of-Disease-Progression-with-Irregularly-Sampled-Longitudinal-Medical-Images" class="headerlink" title="ImageFlowNet: Forecasting Multiscale Image-Level Trajectories of Disease   Progression with Irregularly-Sampled Longitudinal Medical Images"></a>ImageFlowNet: Forecasting Multiscale Image-Level Trajectories of Disease   Progression with Irregularly-Sampled Longitudinal Medical Images</h2><p><strong>Authors:Chen Liu, Ke Xu, Liangbo L. Shen, Guillaume Huguet, Zilong Wang, Alexander Tong, Danilo Bzdok, Jay Stewart, Jay C. Wang, Lucian V. Del Priore, Smita Krishnaswamy</strong></p>
<p>Advances in medical imaging technologies have enabled the collection of longitudinal images, which involve repeated scanning of the same patients over time, to monitor disease progression. However, predictive modeling of such data remains challenging due to high dimensionality, irregular sampling, and data sparsity. To address these issues, we propose ImageFlowNet, a novel model designed to forecast disease trajectories from initial images while preserving spatial details. ImageFlowNet first learns multiscale joint representation spaces across patients and time points, then optimizes deterministic or stochastic flow fields within these spaces using a position-parameterized neural ODE&#x2F;SDE framework. The model leverages a UNet architecture to create robust multiscale representations and mitigates data scarcity by combining knowledge from all patients. We provide theoretical insights that support our formulation of ODEs, and motivate our regularizations involving high-level visual features, latent space organization, and trajectory smoothness. We validate ImageFlowNet on three longitudinal medical image datasets depicting progression in geographic atrophy, multiple sclerosis, and glioblastoma, demonstrating its ability to effectively forecast disease progression and outperform existing methods. Our contributions include the development of ImageFlowNet, its theoretical underpinnings, and empirical validation on real-world datasets. The official implementation is available at <a target="_blank" rel="noopener" href="https://github.com/KrishnaswamyLab/ImageFlowNet">https://github.com/KrishnaswamyLab/ImageFlowNet</a>. </p>
<blockquote>
<p>医学影像技术的进步使得能够收集纵向图像，这些图像涉及随着时间的推移对同一患者的反复扫描，以监测疾病的进展。然而，由于数据的高维性、不规则采样和数据稀疏性，此类数据的预测建模仍然具有挑战性。为了解决这些问题，我们提出了ImageFlowNet这一新型模型，旨在从初始图像预测疾病轨迹的同时保留空间细节。ImageFlowNet首先学习跨患者和时间点的多尺度联合表示空间，然后利用位置参数化的神经ODE&#x2F;SDE框架优化这些空间内的确定性或随机流场。该模型利用UNet架构创建稳健的多尺度表示，并通过整合所有患者的知识来缓解数据稀缺问题。我们提供了支持我们建立ODEs的理论见解，并激发了我们涉及高级视觉特征、潜在空间组织和轨迹平滑性的正则化动机。我们在三个描绘地理萎缩、多发性硬化症和胶质母细胞瘤进展的纵向医学图像数据集上验证了ImageFlowNet，证明了其有效预测疾病进展的能力，并超越了现有方法。我们的贡献包括ImageFlowNet的开发、其理论基础以及在真实数据集上的经验验证。官方实现可访问：<a target="_blank" rel="noopener" href="https://github.com/KrishnaswamyLab/ImageFlowNet%E3%80%82">https://github.com/KrishnaswamyLab/ImageFlowNet。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14794v6">PDF</a> ICASSP 2025, Oral Presentation</p>
<p><strong>Summary</strong></p>
<p>基于医疗成像技术的进步，长期图像收集被广泛应用于监测疾病进展。针对此类数据预测存在的挑战，如高维度、不规则采样和数据稀疏性问题，提出了ImageFlowNet模型。该模型通过初始图像预测疾病轨迹，同时保留空间细节。其采用多尺度联合表示空间学习，并利用神经ODE&#x2F;SDE框架优化确定性或随机流场。通过UNet架构创建稳健的多尺度表示，并结合所有患者的知识缓解数据稀缺问题。在三个纵向医学图像数据集上验证了ImageFlowNet的有效性，展示了其预测疾病进展的能力并超越了现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医疗成像技术现在能够收集长期图像，用于监测疾病进展。</li>
<li>ImageFlowNet模型用于从初始图像预测疾病轨迹，保留空间细节。</li>
<li>ImageFlowNet通过多尺度联合表示空间学习来处理高维度数据。</li>
<li>神经ODE&#x2F;SDE框架被用于优化确定性或随机流场。</li>
<li>UNet架构用于创建稳健的多尺度表示。</li>
<li>ImageFlowNet通过结合所有患者的知识来缓解数据稀缺问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14794">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4d346bd860819d1d9f95706ce8069093.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95bb82a74bb2746a824659bcebb35fd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-668419b156643de27c9f64266d7041c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a44925c256226d39af72e3757ab975c0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CADS-A-Systematic-Literature-Review-on-the-Challenges-of-Abstractive-Dialogue-Summarization"><a href="#CADS-A-Systematic-Literature-Review-on-the-Challenges-of-Abstractive-Dialogue-Summarization" class="headerlink" title="CADS: A Systematic Literature Review on the Challenges of Abstractive   Dialogue Summarization"></a>CADS: A Systematic Literature Review on the Challenges of Abstractive   Dialogue Summarization</h2><p><strong>Authors:Frederic Kirstein, Jan Philip Wahle, Bela Gipp, Terry Ruas</strong></p>
<p>Abstractive dialogue summarization is the task of distilling conversations into informative and concise summaries. Although reviews have been conducted on this topic, there is a lack of comprehensive work detailing the challenges of dialogue summarization, unifying the differing understanding of the task, and aligning proposed techniques, datasets, and evaluation metrics with the challenges. This article summarizes the research on Transformer-based abstractive summarization for English dialogues by systematically reviewing 1262 unique research papers published between 2019 and 2024, relying on the Semantic Scholar and DBLP databases. We cover the main challenges present in dialog summarization (i.e., language, structure, comprehension, speaker, salience, and factuality) and link them to corresponding techniques such as graph-based approaches, additional training tasks, and planning strategies, which typically overly rely on BART-based encoder-decoder models. We find that while some challenges, like language, have seen considerable progress, mainly due to training methods, others, such as comprehension, factuality, and salience, remain difficult and hold significant research opportunities. We investigate how these approaches are typically assessed, covering the datasets for the subdomains of dialogue (e.g., meeting, medical), the established automatic metrics and human evaluation approaches for assessing scores and annotator agreement. We observe that only a few datasets span across all subdomains. The ROUGE metric is the most used, while human evaluation is frequently reported without sufficient detail on inner-annotator agreement and annotation guidelines. Additionally, we discuss the possible implications of the recently explored large language models and conclude that despite a potential shift in relevance and difficulty, our described challenge taxonomy remains relevant. </p>
<blockquote>
<p>摘要性对话总结是将对话提炼成信息丰富且简洁的摘要的任务。尽管已经对此主题进行了评论，但仍缺乏综合性的工作来详细阐述对话总结的挑战，统一对任务的不同理解，以及使所提出的技术、数据集和评估指标与这些挑战相匹配。本文通过系统回顾2019年至2024年间发表的1262篇独特研究论文，总结了基于Transformer的英语对话摘要研究，这些论文来自Semantic Scholar和DBLP数据库。我们介绍了对话摘要中存在的主要挑战（即语言、结构、理解、说话者、突出性和事实性），并将它们与相应的技术（如基于图的方法、额外的训练任务和规划策略）联系起来，这些技术通常过于依赖基于BART的编码器-解码器模型。我们发现，虽然一些挑战（如语言）已经取得了相当大的进展，这主要是因为训练方法，但其他挑战（如理解、事实和突出性）仍然困难，并存在重要的研究机会。我们调查了这些方法的典型评估方式，涵盖了对话子领域的数据集（例如会议、医疗），以及用于评估分数和注释者一致性的既定自动指标和人工评估方法。我们发现只有少数数据集涵盖所有子域。ROUGE指标是最常用的指标，而人工评估通常没有足够详细的内部评估者一致性和注释指南。此外，我们还讨论了最近探索的大型语言模型的可能影响，并得出结论：尽管相关性和难度可能存在潜在的转变，我们描述的挑战分类仍然很重要。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.07494v3">PDF</a> Published in the Journal of Artificial Intelligence Research (JAIR)   (<a target="_blank" rel="noopener" href="https://www.jair.org/index.php/jair/article/view/16674">https://www.jair.org/index.php/jair/article/view/16674</a>)</p>
<p><strong>摘要</strong></p>
<p>本文综述了基于Transformer的英语对话摘要研究，通过系统地回顾了2019年至2024年间发表的1262篇独特的研究论文，主要探讨了对话摘要面临的主要挑战，如语言、结构、理解、说话者、显著性事实和真实性等。本文链接了这些挑战与相应的技术，如基于图的方法、额外的训练任务和规划策略等，这些技术通常过度依赖于BART编码器-解码器模型。研究发现，虽然语言等挑战取得了显著进展，但理解、真实性和显著性等挑战仍然困难重重，存在大量的研究机会。此外，本文还探讨了这些方法的评估方式，涵盖了对话子域的数据集（如会议、医疗等）、现有的自动评估指标和人工评估方法，发现只有少数数据集涵盖所有子域，ROUGE指标是最常用的，而人工评估往往缺乏内部评估者一致性和评估指南的足够细节。最后，本文讨论了最近探索的大型语言模型的可能影响，并得出结论，尽管相关性和难度可能存在潜在的转变，但本文描述的挑战分类仍然具有重要意义。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>对话摘要的主要挑战包括语言、结构、理解、说话者、显著性事实和真实性等。</li>
<li>技术进步如基于图的方法、额外训练任务和规划策略等通常依赖于BART编码器-解码器模型。</li>
<li>语言挑战已取得显著进展，但理解、真实性和显著性等仍然面临困难。</li>
<li>对话摘要的评估涉及多个数据集、自动评估指标和人工评估方法。但数据集覆盖面有限，且缺乏内部评估者一致性和评估指南的足够细节。</li>
<li>大型语言模型的出现可能改变相关性和难度的格局，但挑战分类依然重要。</li>
<li>目前研究虽然丰富但存在不足，仍需要更多全面的研究和深入探索新的方法和应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.07494">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-05a37057c11c5c16fc516453a9539f17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b23525e372b68c711510f882e9f22a2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-008069fb70fe5f4c37971db02f9a2735.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AI-in-Lung-Health-Benchmarking-Detection-and-Diagnostic-Models-Across-Multiple-CT-Scan-Datasets"><a href="#AI-in-Lung-Health-Benchmarking-Detection-and-Diagnostic-Models-Across-Multiple-CT-Scan-Datasets" class="headerlink" title="AI in Lung Health: Benchmarking Detection and Diagnostic Models Across   Multiple CT Scan Datasets"></a>AI in Lung Health: Benchmarking Detection and Diagnostic Models Across   Multiple CT Scan Datasets</h2><p><strong>Authors:Fakrul Islam Tushar, Avivah Wang, Lavsen Dahal, Michael R. Harowicz, Kyle J. Lafata, Tina D. Tailor, Joseph Y. Lo</strong></p>
<p>Lung cancer remains the leading cause of cancer-related mortality worldwide, and early detection through low-dose computed tomography (LDCT) has shown significant promise in reducing death rates. With the growing integration of artificial intelligence (AI) into medical imaging, the development and evaluation of robust AI models require access to large, well-annotated datasets. In this study, we introduce the utility of Duke Lung Cancer Screening (DLCS) Dataset, the largest open-access LDCT dataset with over 2,000 scans and 3,000 expert-verified nodules. We benchmark deep learning models for both 3D nodule detection and lung cancer classification across internal and external datasets including LUNA16, LUNA25, and NLST-3D+. For detection, we develop two MONAI-based RetinaNet models (DLCSDmD and LUNA16-mD), evaluated using the Competition Performance Metric (CPM). For classification, we compare five models, including state-of-the-art pretrained models (Models Genesis, Med3D), a selfsupervised foundation model (FMCB), a randomly initialized ResNet50, and proposed a novel Strategic Warm-Start++ (SWS++) model. SWS++ uses curated candidate patches to pretrain a classification backbone within the same detection pipeline, enabling task-relevant feature learning. Our models demonstrated strong generalizability, with SWS++ achieving comparable or superior performance to existing foundational models across multiple datasets (AUC: 0.71 to 0.90). All code, models, and data are publicly released to promote reproducibility and collaboration. This work establishes a standardized benchmarking resource for lung cancer AI research, supporting future efforts in model development, validation, and clinical translation. </p>
<blockquote>
<p>肺癌仍然是全球癌症相关死亡的主要原因，通过低剂量计算机断层扫描（LDCT）进行早期检测在降低死亡率方面显示出巨大潜力。随着人工智能（AI）在医学成像中的日益融合，开发和评估稳健的AI模型需要访问大量、经过良好注释的数据集。在本研究中，我们介绍了Duke肺癌筛查（DLCS）数据集的实用性，这是最大的开放访问LDCT数据集，包含超过2000次扫描和3000个专家验证的结节。我们对内部和外部数据集（包括LUNA16、LUNA25和NLST-3D+）的深度学习模型进行了基准测试，以评估其对3D结节检测和肺癌分类的效果。对于检测，我们开发了两个基于MONAI的RetinaNet模型（DLCSDmD和LUNA16-mD），使用竞赛性能指标（CPM）进行评估。对于分类，我们比较了五种模型，包括最先进的预训练模型（Models Genesis、Med3D）、一个自监督基础模型（FMCB）、一个随机初始化的ResNet50，并提出了一种新的战略温启动（SWS++）模型。SWS++使用精选的候选补丁在相同的检测管道中预训练分类主干，从而实现任务相关特征学习。我们的模型表现出很强的通用性，其中SWS++在多个数据集上的性能与现有基础模型相当或更优（AUC：0.71至0.90）。为了促进可重复性和协作，我们公开发布了所有代码、模型和数据。这项工作为肺癌AI研究建立了标准化的基准资源，支持未来的模型开发、验证和临床翻译工作。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.04605v3">PDF</a> 2 tables, 6 figures</p>
<p><strong>摘要</strong><br>肺癌仍是全球癌症死亡的主要原因，低剂量计算机断层扫描（LDCT）的早期检测在降低死亡率方面显示出巨大潜力。随着人工智能（AI）在医学成像中的日益融合，开发和评估稳健的AI模型需要访问大量、经过良好注释的数据集。本研究介绍了Duke Lung Cancer Screening（DLCS）数据集的实用性，这是最大的公开访问LDCT数据集，包含超过2000次扫描和3000个专家验证的结节。我们对内部和外部数据集（包括LUNA16、LUNA25和NLST-3D+）进行了深度学习模型的基准测试，用于3D结节检测和肺癌分类。对于检测，我们开发了两个基于MONAI的RetinaNet模型（DLCSDmD和LUNA16-mD），使用竞赛性能指标（CPM）进行评估。对于分类，我们比较了五种模型，包括最新预训练模型（Models Genesis、Med3D）、自监督基础模型（FMCB）、随机初始化的ResNet50，以及提出的新型战略温启（SWS++）模型。SWS++使用精选的候选补丁在同一检测管道中预训练分类主干，实现任务相关特征学习。我们的模型表现出强大的泛化能力，SWS++在多个数据集上的性能与现有基础模型相当或更优（AUC：0.71至0.90）。所有代码、模型和数据均公开发布，以促进可重复性和协作。这项工作为肺癌AI研究建立了标准化的基准测试资源，支持未来的模型开发、验证和临床转化。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>肺癌仍是全球主要的癌症死亡原因，LDCT在早期检测中扮演重要角色。</li>
<li>AI在医学成像中的集成对肺癌诊断有积极影响。</li>
<li>介绍了DLCS数据集，这是最大的公开访问LDCT数据集，包含经过专家验证的结节。</li>
<li>基准测试了深度学习模型进行3D结节检测和肺癌分类。</li>
<li>开发并评估了基于MONAI的RetinaNet模型和其他深度学习模型。</li>
<li>SWS++模型表现出强大的泛化能力，在多个数据集上性能优越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.04605">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-af08055818ef3346cc126d882f887cd6.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-26/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-26/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-26/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-24963cfbdfcde706aa6653a7af336d1c.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-04-26  Graph covers and semi-covers Who is stronger?
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-26/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-f4b84ec53e433cda86e5a98e5b439053.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-04-26  Beyond Labels Zero-Shot Diabetic Foot Ulcer Wound Segmentation with   Self-attention Diffusion Models and the Potential for Text-Guided   Customization
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">19778.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
