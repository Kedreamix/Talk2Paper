<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-04-26  Beyond Labels Zero-Shot Diabetic Foot Ulcer Wound Segmentation with   Self-attention Diffusion Models and the Potential for Text-Guided   Customization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f4b84ec53e433cda86e5a98e5b439053.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    43 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-26-更新"><a href="#2025-04-26-更新" class="headerlink" title="2025-04-26 更新"></a>2025-04-26 更新</h1><h2 id="Beyond-Labels-Zero-Shot-Diabetic-Foot-Ulcer-Wound-Segmentation-with-Self-attention-Diffusion-Models-and-the-Potential-for-Text-Guided-Customization"><a href="#Beyond-Labels-Zero-Shot-Diabetic-Foot-Ulcer-Wound-Segmentation-with-Self-attention-Diffusion-Models-and-the-Potential-for-Text-Guided-Customization" class="headerlink" title="Beyond Labels: Zero-Shot Diabetic Foot Ulcer Wound Segmentation with   Self-attention Diffusion Models and the Potential for Text-Guided   Customization"></a>Beyond Labels: Zero-Shot Diabetic Foot Ulcer Wound Segmentation with   Self-attention Diffusion Models and the Potential for Text-Guided   Customization</h2><p><strong>Authors:Abderrachid Hamrani, Daniela Leizaola, Renato Sousa, Jose P. Ponce, Stanley Mathis, David G. Armstrong, Anuradha Godavarty</strong></p>
<p>Diabetic foot ulcers (DFUs) pose a significant challenge in healthcare, requiring precise and efficient wound assessment to enhance patient outcomes. This study introduces the Attention Diffusion Zero-shot Unsupervised System (ADZUS), a novel text-guided diffusion model that performs wound segmentation without relying on labeled training data. Unlike conventional deep learning models, which require extensive annotation, ADZUS leverages zero-shot learning to dynamically adapt segmentation based on descriptive prompts, offering enhanced flexibility and adaptability in clinical applications. Experimental evaluations demonstrate that ADZUS surpasses traditional and state-of-the-art segmentation models, achieving an IoU of 86.68% and the highest precision of 94.69% on the chronic wound dataset, outperforming supervised approaches such as FUSegNet. Further validation on a custom-curated DFU dataset reinforces its robustness, with ADZUS achieving a median DSC of 75%, significantly surpassing FUSegNet’s 45%. The model’s text-guided segmentation capability enables real-time customization of segmentation outputs, allowing targeted analysis of wound characteristics based on clinical descriptions. Despite its competitive performance, the computational cost of diffusion-based inference and the need for potential fine-tuning remain areas for future improvement. ADZUS represents a transformative step in wound segmentation, providing a scalable, efficient, and adaptable AI-driven solution for medical imaging. </p>
<blockquote>
<p>糖尿病足溃疡（DFUs）在医疗保健中构成重大挑战，需要进行精确高效的伤口评估以提高患者治疗效果。本研究介绍了注意力扩散零样本无监督系统（ADZUS），这是一种新型文本引导扩散模型，可在无需标注训练数据的情况下进行伤口分割。不同于需要大量标注的传统深度学习模型，ADZUS采用零样本学习，根据描述性提示动态适应分割，在临床应用中提供了更高的灵活性和适应性。实验评估表明，ADZUS超越了传统和最先进的分割模型，在慢性伤口数据集上达到86.68%的交并比（IoU）和最高94.69%的精确度，超越了如FUSegNet等监督方法。在自定义的DFU数据集上进一步验证，ADZUS的中位DSC达到75%，显著超越了FUSegNet的45%。该模型的文本引导分割能力能够实时定制分割输出，允许根据临床描述进行有针对性的伤口特征分析。尽管其性能具有竞争力，但扩散推理的计算成本和潜在精细调整的需求仍是未来改进的领域。ADZUS是伤口分割中的一项变革性进步，提供可规模化、高效、适应性强的AI驱动解决方案，用于医学成像。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17628v1">PDF</a> 12 pages, 8 figures, journal article</p>
<p><strong>摘要</strong></p>
<p>本研究引入了一种名为Attention Diffusion Zero-shot Unsupervised System（ADZUS）的新型文本引导扩散模型，用于糖尿病足溃疡（DFU）的伤口分割，且无需标注训练数据。ADZUS采用零样本学习方式，可根据描述性提示动态适应分割，提高了在临床应用中的灵活性和适应性。实验评估表明，ADZUS在慢性伤口数据集上的表现超越了传统和最新分割模型，交并比（IoU）达到86.68%，精度最高达94.69%，甚至超越了FUSegNet等监督方法。在定制的DFU数据集上的验证也证明了其稳健性，ADZUS的中位DSC达到75%，远超FUSegNet的45%。该模型的文本引导分割能力可实现分割输出的实时定制，根据临床描述进行有针对性的伤口特性分析。虽然扩散推理的计算成本较高且可能需要进行微调，但ADZUS仍为伤口分割提供了可规模化、高效和适应性强的AI驱动解决方案。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>ADZUS是一种新型的文本引导扩散模型，用于糖尿病足溃疡的伤口分割，无需标注训练数据。</li>
<li>ADZUS采用零样本学习，可动态适应分割，提高了临床应用的灵活性和适应性。</li>
<li>实验评估显示，ADZUS在慢性伤口数据集上的表现超越了传统和最新的分割模型。</li>
<li>ADZUS在定制的DFU数据集上表现出稳健性，中位DSC达到75%。</li>
<li>ADZUS具备文本引导分割能力，可实现分割输出的实时定制。</li>
<li>扩散推理的计算成本较高，可能需要进行微调。</li>
<li>ADZUS为伤口分割提供了可规模化、高效和适应性强的AI驱动解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17628">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5eeccc0e0482d280482199e8d8da5e36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe2706fbcb29824851a5131b691f71ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a1583e48a79eae71ee492d5e4fb7484.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eba839dddf9240e449f25c86fe25c945.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90d655b07ab5a0ef07712ea204cf2a89.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DRC-Enhancing-Personalized-Image-Generation-via-Disentangled-Representation-Composition"><a href="#DRC-Enhancing-Personalized-Image-Generation-via-Disentangled-Representation-Composition" class="headerlink" title="DRC: Enhancing Personalized Image Generation via Disentangled   Representation Composition"></a>DRC: Enhancing Personalized Image Generation via Disentangled   Representation Composition</h2><p><strong>Authors:Yiyan Xu, Wuqiang Zheng, Wenjie Wang, Fengbin Zhu, Xinting Hu, Yang Zhang, Fuli Feng, Tat-Seng Chua</strong></p>
<p>Personalized image generation has emerged as a promising direction in multimodal content creation. It aims to synthesize images tailored to individual style preferences (e.g., color schemes, character appearances, layout) and semantic intentions (e.g., emotion, action, scene contexts) by leveraging user-interacted history images and multimodal instructions. Despite notable progress, existing methods – whether based on diffusion models, large language models, or Large Multimodal Models (LMMs) – struggle to accurately capture and fuse user style preferences and semantic intentions. In particular, the state-of-the-art LMM-based method suffers from the entanglement of visual features, leading to Guidance Collapse, where the generated images fail to preserve user-preferred styles or reflect the specified semantics.   To address these limitations, we introduce DRC, a novel personalized image generation framework that enhances LMMs through Disentangled Representation Composition. DRC explicitly extracts user style preferences and semantic intentions from history images and the reference image, respectively, to form user-specific latent instructions that guide image generation within LMMs. Specifically, it involves two critical learning stages: 1) Disentanglement learning, which employs a dual-tower disentangler to explicitly separate style and semantic features, optimized via a reconstruction-driven paradigm with difficulty-aware importance sampling; and 2) Personalized modeling, which applies semantic-preserving augmentations to effectively adapt the disentangled representations for robust personalized generation. Extensive experiments on two benchmarks demonstrate that DRC shows competitive performance while effectively mitigating the guidance collapse issue, underscoring the importance of disentangled representation learning for controllable and effective personalized image generation. </p>
<blockquote>
<p>个性化图像生成已成为多媒体内容创建的一个具有前景的方向。它旨在通过利用用户交互的历史图像和多模态指令，合成符合个人风格偏好（如色彩方案、角色外观、布局）和语义意图（如情绪、动作、场景上下文）的图像。尽管取得了显著的进展，但现有方法——无论是基于扩散模型、大型语言模型还是大型多媒体模型（LMM）——在准确捕捉和融合用户风格偏好和语义意图方面都存在困难。特别是最先进的基于LMM的方法受到视觉特征纠缠的影响，导致出现“指导崩溃”的问题，生成的图像无法保持用户偏好的风格或反映指定的语义。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17349v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>个性化图像生成是一个充满前景的多模态内容创建方向。它旨在通过利用用户交互的历史图像和多模态指令，合成符合个人风格偏好和语义意图的图像。尽管已有显著进展，但现有方法，无论是基于扩散模型、大型语言模型还是大型多模态模型（LMMs），在准确捕捉和融合用户风格偏好和语义意图方面仍存在困难。针对当前先进LMMs的缺陷，我们引入了DRC框架，通过解耦表示组合增强LMMs。DRC从用户历史图像中提取用户风格偏好和语义意图，形成用户特定的潜在指令，在LMMs内部指导图像生成。实验证明，DRC在竞争性能的同时有效缓解了指导崩溃问题，凸显了解耦表示学习对于可控、高效个性化图像生成的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>个人化图像生成已成为多模态内容创建的一个前景方向，旨在根据用户的风格偏好和语义意图合成图像。</li>
<li>当前方法（基于扩散模型、大型语言模型或大型多模态模型）在捕捉和融合用户风格偏好和语义意图方面存在困难。</li>
<li>引入DRC框架以增强大型多模态模型（LMMs），通过解耦表示组合解决现有问题。</li>
<li>DRC从用户历史图像中提取用户风格偏好和语义意图，形成特定的潜在指令来指导图像生成。</li>
<li>DRC包含两个关键学习阶段：解耦学习和个性化建模。</li>
<li>解耦学习阶段采用双塔分离器来明确区分风格和语义特征，通过重建驱动方法和难度感知重要性采样进行优化。</li>
<li>实验证明，DRC在竞争性能的同时有效缓解了指导崩溃问题，凸显解耦表示学习的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17349">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f4b84ec53e433cda86e5a98e5b439053.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70af2ee72af8b7081e6414c3620f2cad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97edd6cb76e8b2daaaf1bb4f6e7a3f0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88fe1da4009369164f605ff615968988.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4485d51bfc10b1c2853e6e9e833b12a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Towards-Generalized-and-Training-Free-Text-Guided-Semantic-Manipulation"><a href="#Towards-Generalized-and-Training-Free-Text-Guided-Semantic-Manipulation" class="headerlink" title="Towards Generalized and Training-Free Text-Guided Semantic Manipulation"></a>Towards Generalized and Training-Free Text-Guided Semantic Manipulation</h2><p><strong>Authors:Yu Hong, Xiao Cai, Pengpeng Zeng, Shuai Zhang, Jingkuan Song, Lianli Gao, Heng Tao Shen</strong></p>
<p>Text-guided semantic manipulation refers to semantically editing an image generated from a source prompt to match a target prompt, enabling the desired semantic changes (e.g., addition, removal, and style transfer) while preserving irrelevant contents. With the powerful generative capabilities of the diffusion model, the task has shown the potential to generate high-fidelity visual content. Nevertheless, existing methods either typically require time-consuming fine-tuning (inefficient), fail to accomplish multiple semantic manipulations (poorly extensible), and&#x2F;or lack support for different modality tasks (limited generalizability). Upon further investigation, we find that the geometric properties of noises in the diffusion model are strongly correlated with the semantic changes. Motivated by this, we propose a novel $\textit{GTF}$ for text-guided semantic manipulation, which has the following attractive capabilities: 1) $\textbf{Generalized}$: our $\textit{GTF}$ supports multiple semantic manipulations (e.g., addition, removal, and style transfer) and can be seamlessly integrated into all diffusion-based methods (i.e., Plug-and-play) across different modalities (i.e., modality-agnostic); and 2) $\textbf{Training-free}$: $\textit{GTF}$ produces high-fidelity results via simply controlling the geometric relationship between noises without tuning or optimization. Our extensive experiments demonstrate the efficacy of our approach, highlighting its potential to advance the state-of-the-art in semantics manipulation. </p>
<blockquote>
<p>文本引导语义操纵是指对由源提示生成的图像进行语义编辑，使其与目标提示相匹配，从而实现期望的语义变化（例如添加、删除和风格转换），同时保留无关内容。扩散模型的强大生成能力使该任务有潜力生成高保真视觉内容。然而，现有方法通常需要耗时的微调（效率低下）、无法完成多次语义操纵（扩展性差），并且&#x2F;或缺乏对不同模态任务的支持（泛化能力有限）。通过进一步调查，我们发现扩散模型中的噪声几何属性与语义变化具有很强的相关性。受此启发，我们提出了一种用于文本引导语义操纵的新型GTF方法，它具有以下吸引人的能力：1）通用性：我们的GTF支持多种语义操纵（例如添加、删除和风格转换），可以无缝集成到所有基于扩散的方法中（即插即用），适用于不同模态（即模态无关）；2）无训练：GTF通过简单控制噪声之间的几何关系即可产生高保真结果，无需调整或优化。我们的大量实验证明了我们的方法的有效性，突出了其在语义操纵方面领先技术的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17269v1">PDF</a> </p>
<p><strong>Summary</strong><br>文本引导语义操纵指的是对由源提示生成的图像进行语义编辑，使其匹配目标提示，从而实现期望的语义变化（例如添加、删除和风格转换），同时保留无关内容。借助扩散模型的强大生成能力，该任务有潜力生成高保真视觉内容。然而，现有方法通常需要耗时且效率低下的微调，难以完成多种语义操纵，且缺乏对不同模态任务的支持。我们发现扩散模型中的噪声几何属性与语义变化之间存在强烈相关性。因此，我们提出了一种新型的GTF文本引导语义操纵方法，具有支持多种语义操纵、可无缝集成到所有扩散基方法、无需训练即可产生高保真结果等吸引力人的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本引导语义操纵能够编辑图像以匹配目标提示，实现语义变化，同时保留无关内容。</li>
<li>扩散模型具有强大的生成能力，在语义操纵任务中表现出潜力。</li>
<li>现有方法存在效率低下、难以完成多种语义操纵、缺乏对不同模态任务支持的问题。</li>
<li>噪声的几何属性与语义变化之间存在强烈相关性。</li>
<li>提出的GTF方法支持多种语义操纵，并能无缝集成到所有扩散基方法中。</li>
<li>GTF方法无需训练即可产生高保真结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17269">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e2904c3907150b5c0a8cf931237c6d28.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a4e2755939fcde4bbc745cfec835222.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ed0b404ab53b298ab302e30b67c606a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a10f27a104ee9725c9f933a57bf18e61.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DIVE-Inverting-Conditional-Diffusion-Models-for-Discriminative-Tasks"><a href="#DIVE-Inverting-Conditional-Diffusion-Models-for-Discriminative-Tasks" class="headerlink" title="DIVE: Inverting Conditional Diffusion Models for Discriminative Tasks"></a>DIVE: Inverting Conditional Diffusion Models for Discriminative Tasks</h2><p><strong>Authors:Yinqi Li, Hong Chang, Ruibing Hou, Shiguang Shan, Xilin Chen</strong></p>
<p>Diffusion models have shown remarkable progress in various generative tasks such as image and video generation. This paper studies the problem of leveraging pretrained diffusion models for performing discriminative tasks. Specifically, we extend the discriminative capability of pretrained frozen generative diffusion models from the classification task to the more complex object detection task, by “inverting” a pretrained layout-to-image diffusion model. To this end, a gradient-based discrete optimization approach for replacing the heavy prediction enumeration process, and a prior distribution model for making more accurate use of the Bayes’ rule, are proposed respectively. Empirical results show that this method is on par with basic discriminative object detection baselines on COCO dataset. In addition, our method can greatly speed up the previous diffusion-based method for classification without sacrificing accuracy. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/LiYinqi/DIVE">https://github.com/LiYinqi/DIVE</a> . </p>
<blockquote>
<p>扩散模型在图像和视频生成等各种生成任务中取得了显著的进步。本文研究了利用预训练的扩散模型执行判别任务的问题。具体来说，我们通过“反转”预训练的布局到图像的扩散模型，将预冻结生成扩散模型的判别能力从分类任务扩展到更复杂的对象检测任务。为此，分别提出了基于梯度的离散优化方法，以替代繁重的预测枚举过程，以及先验分布模型，以更准确地应用贝叶斯法则。经验结果表明，该方法在COCO数据集上的表现与基本的判别对象检测基线相当。此外，我们的方法可以大大提高之前用于分类的扩散方法的速度，而不会牺牲准确性。代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/LiYinqi/DIVE%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LiYinqi/DIVE中找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17253v1">PDF</a> Accepted by IEEE Transactions on Multimedia</p>
<p><strong>Summary</strong></p>
<p>本文研究了如何利用预训练的扩散模型执行判别任务，特别是将预训练的冷冻生成扩散模型的判别能力从分类任务扩展到更复杂的对象检测任务。通过采用梯度基础上的离散优化方法和先验分布模型，实现了对预训练布局到图像扩散模型的“反转”。在COCO数据集上，该方法与基本的判别对象检测基线表现相当，并且能显著加快之前的扩散分类方法的速度，同时不损失准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文探索了预训练扩散模型在判别任务中的应用，特别是将分类任务的判别能力扩展到对象检测任务。</li>
<li>通过“反转”预训练的布局到图像扩散模型，实现了这一扩展。</li>
<li>采用了梯度基础上的离散优化方法，以替代繁重的预测枚举过程。</li>
<li>引入了先验分布模型，更准确地应用贝叶斯规则。</li>
<li>在COCO数据集上，该方法与判别对象检测的基线方法表现相当。</li>
<li>与之前的扩散分类方法相比，该方法显著加速了过程，同时保持了准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17253">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-18495e567652a39c5f9f94042ef80e37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-937575dabbd806032a0ba53bdd79ba51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86a10b0ffbe50f70ef395b7fb683b197.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df21a56626a1795813cfe1792d9b15f3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AUTHENTICATION-Identifying-Rare-Failure-Modes-in-Autonomous-Vehicle-Perception-Systems-using-Adversarially-Guided-Diffusion-Models"><a href="#AUTHENTICATION-Identifying-Rare-Failure-Modes-in-Autonomous-Vehicle-Perception-Systems-using-Adversarially-Guided-Diffusion-Models" class="headerlink" title="AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle   Perception Systems using Adversarially Guided Diffusion Models"></a>AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle   Perception Systems using Adversarially Guided Diffusion Models</h2><p><strong>Authors:Mohammad Zarei, Melanie A Jutras, Eliana Evans, Mike Tan, Omid Aaramoon</strong></p>
<p>Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately detect objects and interpret their surroundings. However, even when trained using millions of miles of real-world data, AVs are often unable to detect rare failure modes (RFMs). The problem of RFMs is commonly referred to as the “long-tail challenge”, due to the distribution of data including many instances that are very rarely seen. In this paper, we present a novel approach that utilizes advanced generative and explainable AI techniques to aid in understanding RFMs. Our methods can be used to enhance the robustness and reliability of AVs when combined with both downstream model training and testing. We extract segmentation masks for objects of interest (e.g., cars) and invert them to create environmental masks. These masks, combined with carefully crafted text prompts, are fed into a custom diffusion model. We leverage the Stable Diffusion inpainting model guided by adversarial noise optimization to generate images containing diverse environments designed to evade object detection models and expose vulnerabilities in AI systems. Finally, we produce natural language descriptions of the generated RFMs that can guide developers and policymakers to improve the safety and reliability of AV systems. </p>
<blockquote>
<p>自动驾驶车辆（AVs）依赖于人工智能（AI）来准确检测物体并解读周围环境。然而，即使使用数百万英里的真实世界数据进行训练，AVs通常也无法检测到罕见的失败模式（RFMs）。由于数据分布包含许多很少见到的实例，RFM的问题通常被称为“长尾挑战”。在本文中，我们提出了一种利用先进的生成式和可解释的人工智能技术来帮助理解RFMs的新方法。当与下游模型训练和测试相结合时，我们的方法可用于提高AVs的鲁棒性和可靠性。我们提取感兴趣对象（例如汽车）的分割掩膜并将其反转以创建环境掩膜。这些掩膜与精心设计的文本提示相结合，输入到自定义的扩散模型中。我们利用由对抗性噪声优化引导的Stable Diffusion补全模型，生成包含各种环境的图像，这些图像旨在避开物体检测模型并暴露AI系统的漏洞。最后，我们生成对生成的RFMs的自然语言描述，可以指导开发者和政策制定者提高AV系统的安全性和可靠性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17179v1">PDF</a> 8 pages, 10 figures. Accepted to IEEE Conference on Artificial   Intelligence (CAI), 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种利用先进的生成式和可解释的人工智能技术来理解自动驾驶车辆的罕见故障模式（RFMs）的新方法。通过结合下游模型训练和测试，增强自动驾驶车辆的稳健性和可靠性。通过提取感兴趣对象的分割掩膜并反转创建环境掩膜，结合精心设计的文本提示，输入到自定义的扩散模型中。利用对抗性噪声优化引导的Stable Diffusion inpainting模型生成包含多样环境的图像，以避开物体检测模型并暴露AI系统的漏洞。最后，生成对RFMs的自然语言描述，可指导开发者和政策制定者提高自动驾驶系统的安全性和可靠性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动驾驶车辆（AVs）依赖人工智能（AI）来检测物体和解读周围环境，但面临罕见故障模式（RFMs）的挑战。</li>
<li>RFMs问题被称为“长尾挑战”，因为数据分布包含许多很少见到的实例。</li>
<li>本文提出一种利用生成式和可解释的人工智能技术理解RFMs的新方法。</li>
<li>通过提取感兴趣对象的分割掩膜并创建环境掩膜，结合文本提示输入到自定义扩散模型中。</li>
<li>利用Stable Diffusion inpainting模型生成多样环境图像，以检测物体并暴露AI系统的漏洞。</li>
<li>生成的自然语言描述可以帮助开发者和政策制定者了解RFMs，提高自动驾驶系统的安全性和可靠性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17179">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b38dd75b0e8d7ae74f5aee44c36242b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-864ffea2418abc67222a64cac38237cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7db5a7e3018bc6f8f31806915c76b7f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b424344865eb68843b119e35d6f76fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bdf2b59aa2628953cd8ae495eab7b900.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f45f3ddd3b593836295a8a0a43cb1cc.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Physics-guided-and-fabrication-aware-inverse-design-of-photonic-devices-using-diffusion-models"><a href="#Physics-guided-and-fabrication-aware-inverse-design-of-photonic-devices-using-diffusion-models" class="headerlink" title="Physics-guided and fabrication-aware inverse design of photonic devices   using diffusion models"></a>Physics-guided and fabrication-aware inverse design of photonic devices   using diffusion models</h2><p><strong>Authors:Dongjin Seo, Soobin Um, Sangbin Lee, Jong Chul Ye, Haejun Chung</strong></p>
<p>Designing free-form photonic devices is fundamentally challenging due to the vast number of possible geometries and the complex requirements of fabrication constraints. Traditional inverse-design approaches–whether driven by human intuition, global optimization, or adjoint-based gradient methods–often involve intricate binarization and filtering steps, while recent deep learning strategies demand prohibitively large numbers of simulations (10^5 to 10^6). To overcome these limitations, we present AdjointDiffusion, a physics-guided framework that integrates adjoint sensitivity gradients into the sampling process of diffusion models. AdjointDiffusion begins by training a diffusion network on a synthetic, fabrication-aware dataset of binary masks. During inference, we compute the adjoint gradient of a candidate structure and inject this physics-based guidance at each denoising step, steering the generative process toward high figure-of-merit (FoM) solutions without additional post-processing. We demonstrate our method on two canonical photonic design problems–a bent waveguide and a CMOS image sensor color router–and show that our method consistently outperforms state-of-the-art nonlinear optimizers (such as MMA and SLSQP) in both efficiency and manufacturability, while using orders of magnitude fewer simulations (approximately 2 x 10^2) than pure deep learning approaches (approximately 10^5 to 10^6). By eliminating complex binarization schedules and minimizing simulation overhead, AdjointDiffusion offers a streamlined, simulation-efficient, and fabrication-aware pipeline for next-generation photonic device design. Our open-source implementation is available at <a target="_blank" rel="noopener" href="https://github.com/dongjin-seo2020/AdjointDiffusion">https://github.com/dongjin-seo2020/AdjointDiffusion</a>. </p>
<blockquote>
<p>设计自由形式的光子器件具有根本挑战性，原因在于存在大量可能的几何结构以及复杂的制造约束要求。传统的逆向设计方法——无论是基于人类直觉、全局优化还是基于伴随的梯度方法——通常涉及复杂的二值化和过滤步骤，而最近的深度学习策略则需要进行大量模拟（从10万到一百万）。为了克服这些局限性，我们提出了AdjointDiffusion，这是一个物理引导框架，它将伴随敏感性梯度整合到扩散模型的采样过程中。AdjointDiffusion首先在一个合成且了解制造过程的二进制掩膜数据集上训练扩散网络。在推理过程中，我们计算候选结构的伴随梯度，并在每个去噪步骤中注入基于物理的指导，引导生成过程朝着高品质因数（FoM）解决方案发展，无需额外的后处理。我们在两个典型的光子设计问题上展示了我们的方法——弯曲波导和CMOS图像传感器颜色路由器——并证明我们的方法在效率和制造性方面始终优于最先进的非线性优化器（如MMA和SLSQP），同时使用的模拟次数比纯深度学习方法少几个数量级（大约为2 x 10² vs. 大约介于一百万以上）。通过消除复杂的二值化时间表并最大限度地减少模拟开销，AdjointDiffusion为下一代光子器件设计提供了简化、模拟高效且了解制造过程的管道。我们的开源实现可在<a target="_blank" rel="noopener" href="https://github.com/dongjin-seo2020/AdjointDiffusion%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/dongjin-seo2020/AdjointDiffusion找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17077v1">PDF</a> 25 pages, 7 Figures</p>
<p><strong>Summary</strong><br>光子器件设计面临巨大的几何可能性和复杂的制造约束挑战。传统逆向设计方法和最新的深度学习策略都存在局限性。AdjointDiffusion框架结合逆向敏感性梯度与扩散模型的采样过程，以物理指导生成设计。该框架在合成、具有制造意识的二进制掩膜数据集上训练扩散网络，并在推理过程中计算候选结构的逆向梯度，在每个去噪步骤中注入物理指导，引导生成过程向高图梅尔性能解决方案发展。该方法在光子设计问题上表现优异，使用较少的模拟次数，提高了效率和制造性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>光子器件设计面临巨大的几何和制造约束挑战。</li>
<li>传统逆向设计方法和深度学习策略都有局限性。</li>
<li>AdjointDiffusion框架通过结合逆向敏感性梯度和扩散模型的采样过程来进行物理指导生成设计。</li>
<li>该框架使用合成、具有制造意识的二进制掩膜数据集进行训练。</li>
<li>在推理过程中，计算候选结构的逆向梯度并注入物理指导。</li>
<li>AdjointDiffusion在光子设计问题上表现优异，提高了效率和制造性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17077">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-992a9100ea7aa09abffe5c0f76b8febb.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Deep-Generative-Model-Based-Generation-of-Synthetic-Individual-Specific-Brain-MRI-Segmentations"><a href="#Deep-Generative-Model-Based-Generation-of-Synthetic-Individual-Specific-Brain-MRI-Segmentations" class="headerlink" title="Deep Generative Model-Based Generation of Synthetic Individual-Specific   Brain MRI Segmentations"></a>Deep Generative Model-Based Generation of Synthetic Individual-Specific   Brain MRI Segmentations</h2><p><strong>Authors:Ruijie Wang, Luca Rossetto, Susan Mérillat, Christina Röcke, Mike Martin, Abraham Bernstein</strong></p>
<p>To the best of our knowledge, all existing methods that can generate synthetic brain magnetic resonance imaging (MRI) scans for a specific individual require detailed structural or volumetric information about the individual’s brain. However, such brain information is often scarce, expensive, and difficult to obtain. In this paper, we propose the first approach capable of generating synthetic brain MRI segmentations – specifically, 3D white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF) segmentations – for individuals using their easily obtainable and often readily available demographic, interview, and cognitive test information. Our approach features a novel deep generative model, CSegSynth, which outperforms existing prominent generative models, including conditional variational autoencoder (C-VAE), conditional generative adversarial network (C-GAN), and conditional latent diffusion model (C-LDM). We demonstrate the high quality of our synthetic segmentations through extensive evaluations. Also, in assessing the effectiveness of the individual-specific generation, we achieve superior volume prediction, with mean absolute errors of only 36.44mL, 29.20mL, and 35.51mL between the ground-truth WM, GM, and CSF volumes of test individuals and those volumes predicted based on generated individual-specific segmentations, respectively. </p>
<blockquote>
<p>据我们所知，所有目前能够针对特定个人生成合成的大脑磁共振成像（MRI）扫描方法都需要关于个人大脑的详细结构或体积信息。然而，此类大脑信息通常稀缺、昂贵且难以获取。在本文中，我们提出了第一种能够生成合成大脑MRI分割的方法——具体来说，是生成三维白质（WM）、灰质（GM）和脑脊液（CSF）的分割——使用个人易于获取且经常容易获得的人口统计、面试和认知测试信息。我们的方法采用了一种新型深度生成模型CSegSynth，它在现有突出的生成模型中表现出色，包括条件变分自动编码器（C-VAE）、条件生成对抗网络（C-GAN）和条件潜在扩散模型（C-LDM）。我们通过广泛评估证明了合成分割的高质量。同时，在评估特定个人的生成效果时，我们实现了出色的体积预测，测试个体真实WM、GM和CSF体积与基于生成个体特定分割所预测的相应体积之间的平均绝对误差分别为仅36.44mL、29.20mL和35.51mL。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12352v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种全新的方法，通过个体易于获取且经常可得的诸如人口统计信息、访谈和认知测试信息，生成合成的大脑MRI扫描图像分割结果。该方法采用创新的深度生成模型CSegSynth，相较于现有的主流生成模型如条件变分自编码器（C-VAE）、条件生成对抗网络（C-GAN）和条件潜在扩散模型（C-LDM）有更好的表现。通过广泛的评估和体积预测，验证了该方法的优越性和准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有生成大脑MRI扫描图像的方法需要详细的个体结构或体积信息，这些信息通常稀缺、昂贵且难以获取。</li>
<li>本文提出了使用个体易于获取的信息（如人口统计信息、访谈和认知测试信息）生成合成大脑MRI分割结果的方法。</li>
<li>采用创新的深度生成模型CSegSynth，能生成特定个体的3D白质、灰质和脑脊液分割图像。</li>
<li>CSegSynth模型在性能上超越了现有的主流生成模型，如条件变分自编码器、条件生成对抗网络和条件潜在扩散模型。</li>
<li>本文通过广泛的评估验证了合成分割图像的高质量。</li>
<li>在评估个体特定生成的有效性方面，该方法实现了优异的体积预测，与真实MRI体积相比，预测误差非常小。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12352">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ca49803a4c7bc1471151bfaf23e398c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-064a0cd210275ae7840ecedd9e19bd84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5474f6add41bfacb071d56e014bd3ddc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a1eb487f7cab1e2d7e61dfd4e3720f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdfc7da1db8c4eefbb77c36f9a7bef8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3dd61af50bad981f4dcaf1f959451d8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Improved-implicit-diffusion-model-with-knowledge-distillation-to-estimate-the-spatial-distribution-density-of-carbon-stock-in-remote-sensing-imagery"><a href="#Improved-implicit-diffusion-model-with-knowledge-distillation-to-estimate-the-spatial-distribution-density-of-carbon-stock-in-remote-sensing-imagery" class="headerlink" title="Improved implicit diffusion model with knowledge distillation to   estimate the spatial distribution density of carbon stock in remote sensing   imagery"></a>Improved implicit diffusion model with knowledge distillation to   estimate the spatial distribution density of carbon stock in remote sensing   imagery</h2><p><strong>Authors:Zhenyu Yu, Jinnian Wang, Mohd Yamani Idna Idris</strong></p>
<p>The forest serves as the most significant terrestrial carbon stock mechanism, effectively reducing atmospheric CO2 concentrations and mitigating climate change. Remote sensing provides high data accuracy and enables large-scale observations. Optical images facilitate long-term monitoring, which is crucial for future carbon stock estimation studies. This study focuses on Huize County, Qujing City, Yunnan Province, China, utilizing GF-1 WFV satellite imagery. The KD-VGG and KD-UNet modules were introduced for initial feature extraction, and the improved implicit diffusion model (IIDM) was proposed. The results showed: (1) The VGG module improved initial feature extraction, improving accuracy, and reducing inference time with optimized model parameters. (2) The Cross-attention + MLPs module enabled effective feature fusion, establishing critical relationships between global and local features, achieving high-accuracy estimation. (3) The IIDM model, a novel contribution, demonstrated the highest estimation accuracy with an RMSE of 12.17%, significantly improving by 41.69% to 42.33% compared to the regression model. In carbon stock estimation, the generative model excelled in extracting deeper features, significantly outperforming other models, demonstrating the feasibility of AI-generated content in quantitative remote sensing. The 16-meter resolution estimates provide a robust basis for tailoring forest carbon sink regulations, enhancing regional carbon stock management. </p>
<blockquote>
<p>森林作为最重要的陆地碳储存机制，有效地降低了大气中二氧化碳的浓度，缓解了气候变化。遥感技术提供了高精度的数据，并实现了大规模观测。光学图像有助于长期监测，对未来碳储量估算研究至关重要。本研究以位于中国云南省曲靖市会泽县为例，利用GF-1卫星WFV影像数据，引入了KD-VGG和KD-UNet模块进行初步特征提取，并提出了改进型隐式扩散模型（IIDM）。研究结果表明：（1）VGG模块改进了初始特征提取，提高了精度，并优化了模型参数，减少了推理时间。（2）Cross-attention+MLPs模块实现了有效的特征融合，建立了全局和局部特征之间的关键关系，实现了高精度估算。（3）新型贡献的IIDM模型表现出最高的估算精度，均方根误差为12.17%，与回归模型相比，改进了41.69%至42.33%。在碳储量估算方面，生成模型在提取深层特征方面表现出色，显著优于其他模型，证明了人工智能生成内容在定量遥感中的可行性。16米分辨率的估计为定制森林碳汇法规、增强区域碳储量管理提供了坚实的基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17973v2">PDF</a> </p>
<p><strong>Summary</strong><br>     该研究利用光学图像和遥感技术，对云南省曲靖市会泽县的森林碳储量进行了估算。研究引入了KD-VGG和KD-UNet模块进行初步特征提取，并提出了改进型隐式扩散模型（IIDM）。结果显示，该模型在森林碳储量估算中具有较高准确性，为制定森林碳汇政策提供了坚实基础。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>森林是减少大气中二氧化碳浓度和缓解气候变化的重要机制。</li>
<li>遥感技术为大规模观察提供了高度准确的数据。</li>
<li>光学图像对于长期森林碳储量监测至关重要。</li>
<li>KD-VGG和KD-UNet模块的引入改进了初步特征提取，提高了估算精度并缩短了推理时间。</li>
<li>Cross-attention + MLPs模块实现了有效的特征融合，建立了全局和局部特征之间的关键关系。</li>
<li>改进型隐式扩散模型（IIDM）在森林碳储量估算中表现出最高精度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17973">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a48f7b6fcc67efef55b75dfc310d1017.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Convergence-of-Diffusion-Models-Under-the-Manifold-Hypothesis-in-High-Dimensions"><a href="#Convergence-of-Diffusion-Models-Under-the-Manifold-Hypothesis-in-High-Dimensions" class="headerlink" title="Convergence of Diffusion Models Under the Manifold Hypothesis in   High-Dimensions"></a>Convergence of Diffusion Models Under the Manifold Hypothesis in   High-Dimensions</h2><p><strong>Authors:Iskander Azangulov, George Deligiannidis, Judith Rousseau</strong></p>
<p>Denoising Diffusion Probabilistic Models (DDPM) are powerful state-of-the-art methods used to generate synthetic data from high-dimensional data distributions and are widely used for image, audio, and video generation as well as many more applications in science and beyond. The \textit{manifold hypothesis} states that high-dimensional data often lie on lower-dimensional manifolds within the ambient space, and is widely believed to hold in provided examples. While recent results have provided invaluable insight into how diffusion models adapt to the manifold hypothesis, they do not capture the great empirical success of these models, making this a very fruitful research direction.   In this work, we study DDPMs under the manifold hypothesis and prove that they achieve rates independent of the ambient dimension in terms of score learning. In terms of sampling complexity, we obtain rates independent of the ambient dimension w.r.t. the Kullback-Leibler divergence, and $O(\sqrt{D})$ w.r.t. the Wasserstein distance. We do this by developing a new framework connecting diffusion models to the well-studied theory of extrema of Gaussian Processes. </p>
<blockquote>
<p>去噪扩散概率模型（DDPM）是目前用于从高维数据分布生成合成数据的最先进方法之一，广泛用于图像、音频和视频生成以及科学和其他更多领域。<em>流形假设</em>指出，高维数据通常位于环境空间中的低维流形上，并且在提供的示例中普遍认为存在。虽然最近的研究结果提供了关于扩散模型如何适应流形假设的宝贵见解，但它们并没有捕捉到这些模型的巨大经验成就，这使得这成为一个非常有成果的研究方向。在本研究中，我们在流形假设下研究DDPM，并证明其在得分学习方面实现了独立于环境维度的速率。在采样复杂度方面，我们获得了独立于环境维度的速率，关于Kullback-Leibler散度以及关于Wasserstein距离的$O(\sqrt{D})$。我们通过开发将扩散模型与广受欢迎的Gaussian Processes极值理论相联系的新框架来实现这一点。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18804v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于去噪扩散概率模型（DDPM）的强大能力，它已成为生成高维数据分布合成数据的最先进方法之一，广泛应用于图像、音频和视频生成以及科学和其他领域。本研究在流形假设下研究DDPM，证明其在评分学习方面实现了与环境维度无关的比率。在采样复杂性方面，我们获得了与环境维度无关的关于Kullback-Leibler散度的比率，以及关于Wasserstein距离的O(√D)比率。我们通过开发将扩散模型与高斯过程极值理论研究相连的新框架来实现这一点。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DDPM是生成高维数据分布合成数据的先进方法之一，广泛应用于多个领域。</li>
<li>流形假设指出高维数据经常位于环境空间中的低维流形上，DDPM在此假设下进行了研究。</li>
<li>DDPM在评分学习方面实现了与环境维度无关的比率。</li>
<li>在采样复杂性方面，DDPM获得了与环境维度无关的关于Kullback-Leibler散度的比率。</li>
<li>关于Wasserstein距离，DDPM的比率是O(√D)。</li>
<li>本研究通过开发新的框架将扩散模型与高斯过程极值理论相连。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18804">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ebebf4e18250e65e1fa8b3bb24c270e2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Contrastive-Learning-with-Synthetic-Positives"><a href="#Contrastive-Learning-with-Synthetic-Positives" class="headerlink" title="Contrastive Learning with Synthetic Positives"></a>Contrastive Learning with Synthetic Positives</h2><p><strong>Authors:Dewen Zeng, Yawen Wu, Xinrong Hu, Xiaowei Xu, Yiyu Shi</strong></p>
<p>Contrastive learning with the nearest neighbor has proved to be one of the most efficient self-supervised learning (SSL) techniques by utilizing the similarity of multiple instances within the same class. However, its efficacy is constrained as the nearest neighbor algorithm primarily identifies “easy” positive pairs, where the representations are already closely located in the embedding space. In this paper, we introduce a novel approach called Contrastive Learning with Synthetic Positives (CLSP) that utilizes synthetic images, generated by an unconditional diffusion model, as the additional positives to help the model learn from diverse positives. Through feature interpolation in the diffusion model sampling process, we generate images with distinct backgrounds yet similar semantic content to the anchor image. These images are considered “hard” positives for the anchor image, and when included as supplementary positives in the contrastive loss, they contribute to a performance improvement of over 2% and 1% in linear evaluation compared to the previous NNCLR and All4One methods across multiple benchmark datasets such as CIFAR10, achieving state-of-the-art methods. On transfer learning benchmarks, CLSP outperforms existing SSL frameworks on 6 out of 8 downstream datasets. We believe CLSP establishes a valuable baseline for future SSL studies incorporating synthetic data in the training process. </p>
<blockquote>
<p>对比学习结合最近邻已被证明是一种非常有效的自监督学习（SSL）技术，它利用同一类别内多个实例的相似性。然而，其效果受到限制，因为最近邻算法主要识别的是“容易”的正对，这些正对的表示在嵌入空间中已经位于相近位置。在本文中，我们介绍了一种称为合成阳性对比学习（CLSP）的新方法，它利用无条件扩散模型生成的合成图像作为额外的正样本，帮助模型从多样化的正样本中学习。在扩散模型采样过程中的特征插值中，我们生成了具有不同背景但语义内容与锚图像相似的图像。这些图像被认为是锚图像的“硬”正样本，当它们作为对比损失中的附加正样本时，与之前的NNCLR和All4One方法相比，在CIFAR10等多个基准数据集上实现了超过2%和1%的线性评估性能提升，达到了最先进的水平。在迁移学习基准测试中，CLSP在8个下游数据集中的6个上超越了现有的SSL框架。我们相信CLSP为未来的SSL研究在训练过程中融入合成数据建立了有价值的基准线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16965v2">PDF</a> 8 pages, conference</p>
<p><strong>Summary</strong></p>
<p>对比学习通过最近邻方法在自我监督学习中展现了出色的效率，其主要利用同一类别实例间的相似性。然而，由于最近邻算法主要识别“容易”的正样本对，其有效性受到限制。本文提出了一种名为“对比学习与合成正样本（CLSP）”的新方法，利用由无条件扩散模型生成的合成图像作为额外的正样本，帮助模型从多样化的正样本中学习。通过扩散模型采样过程中的特征插值，我们生成了具有不同背景但语义内容与锚图像相似的图像。这些图像被视为锚图像的“硬”正样本，并作为对比损失中的补充正样本包含在内，从而提高了性能。相较于先前的NNCLR和All4One方法，CLSP在多个基准数据集（如CIFAR10）上的线性评估性能提高了超过2%和1%，并且在迁移学习基准测试中，CLSP在六个下游数据集中的表现优于现有SSL框架。我们相信CLSP为未来的自我监督学习中融入合成数据的研究建立了有价值的基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>对比学习通过最近邻方法是一种有效的自我监督学习方法，但受限于识别“容易”正样本的能力。</li>
<li>本文提出CLSP方法，利用合成图像作为额外的正样本，以增强模型的泛化能力。</li>
<li>通过扩散模型的特征插值生成合成图像，这些图像具有不同的背景但相似的语义内容。</li>
<li>合成图像被视为“硬”正样本，有助于模型在对比学习中更有效地学习。</li>
<li>CLSP在多个基准数据集上的性能超过之前的NNCLR和All4One方法。</li>
<li>在迁移学习测试中，CLSP在多个下游数据集上的表现优于现有的SSL框架。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.16965">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f6254ca7c208559f3fdb699a40ba13f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29c769a4df288296c66d68d1fc49c453.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Lumina-mGPT-Illuminate-Flexible-Photorealistic-Text-to-Image-Generation-with-Multimodal-Generative-Pretraining"><a href="#Lumina-mGPT-Illuminate-Flexible-Photorealistic-Text-to-Image-Generation-with-Multimodal-Generative-Pretraining" class="headerlink" title="Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation   with Multimodal Generative Pretraining"></a>Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation   with Multimodal Generative Pretraining</h2><p><strong>Authors:Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yi Xin, Xinyue Li, Qi Qin, Yu Qiao, Hongsheng Li, Peng Gao</strong></p>
<p>We present Lumina-mGPT, a family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions. By initializing from multimodal Generative PreTraining (mGPT), we demonstrate that decoder-only Autoregressive (AR) model can achieve image generation performance comparable to modern diffusion models with high efficiency through Flexible Progressive Supervised Fine-tuning (FP-SFT). Equipped with our proposed Unambiguous image Representation (UniRep), Lumina-mGPT can flexibly generate high-quality images of varying aspect ratios. Building on the strong image generation capabilities, we further explore Ominiponent Supervised Fine-tuning (Omni-SFT), an initial attempt to elevate Lumina-mGPT into a unified multi-modal generalist. The resulting model demonstrates versatile multimodal capabilities, including visual generation tasks like text-to-image&#x2F;multiview generation and controllable generation, visual recognition tasks like segmentation and depth estimation, and vision-language tasks like multi-turn visual question answering, showing the rosy potential of the technical direction. Codes and checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/Alpha-VLLM/Lumina-mGPT">https://github.com/Alpha-VLLM/Lumina-mGPT</a>. </p>
<blockquote>
<p>我们推出了Lumina-mGPT，这是一系列多模态自回归模型，能够执行各种视觉和语言任务，特别是在根据文本描述生成灵活的光栅图像方面表现出色。通过采用多模态生成预训练（mGPT）进行初始化，我们证明了解码器仅有的自回归（AR）模型可以通过灵活的渐进式监督微调（FP-SFT）实现与现代扩散模型相当的高效图像生成性能。配备了我们提出的明确图像表示（UniRep），Lumina-mGPT可以灵活地生成各种纵横比的高质量图像。在强大的图像生成能力的基础上，我们进一步探索了全能监督微调（Omni-SFT），这是首次尝试将Lumina-mGPT提升为统一的多模态通才。结果证明该模型具有多功能的多模态能力，包括视觉生成任务（如文本到图像&#x2F;多视图生成和可控生成）、视觉识别任务（如分割和深度估计）以及视觉语言任务（如多轮视觉问答），展示了该技术方向的广阔潜力。相关代码和检查点可访问<a target="_blank" rel="noopener" href="https://github.com/Alpha-VLLM/Lumina-mGPT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Alpha-VLLM/Lumina-mGPT获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.02657v3">PDF</a> Code available at: <a target="_blank" rel="noopener" href="https://github.com/Alpha-VLLM/Lumina-mGPT">https://github.com/Alpha-VLLM/Lumina-mGPT</a></p>
<p><strong>Summary</strong></p>
<p>Lumina-mGPT是一个多模态自回归模型家族，擅长根据文本描述生成灵活逼真的图像。通过多模态生成预训练（mGPT）初始化，并借助灵活渐进监督微调（FP-SFT）和高效率的无歧义图像表示（UniRep），Lumina-mGPT能够灵活生成高质量、不同比例尺的图像。该模型还能执行多种视觉和语言任务，包括文本到图像&#x2F;多视图生成、可控生成、分割、深度估计和多轮视觉问答等。相关代码和检查点已发布在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Lumina-mGPT是一个多模态自回归模型，能处理多种视觉和语言任务。</li>
<li>模型能从文本描述中生成灵活的逼真图像。</li>
<li>通过多模态生成预训练（mGPT）初始化，实现了高效的图像生成性能。</li>
<li>借助灵活渐进监督微调（FP-SFT），模型的性能与现代扩散模型相当。</li>
<li>无歧义图像表示（UniRep）使模型能够生成高质量、不同比例尺的图像。</li>
<li>模型具备多种视觉生成任务能力，如文本到图像生成、可控生成等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.02657">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d696d56990814d409ebb405b3ff0667f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a36eeccc0ac22139d093e977d319ad0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d99b6daa18c59136b99c2cd515997c27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-318ef0e021430f0d8624dc5990ef9b27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3439256919e1cea1f7203273b38fbdce.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-26/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-26/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-26/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d4ff568aadca0897bbf49396281e914e.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-04-26  Self-Supervised Noise Adaptive MRI Denoising via Repetition to   Repetition (Rep2Rep) Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-26/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e05c1ff695a317fab1e81c4116828eb7.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-04-26  CasualHDRSplat Robust High Dynamic Range 3D Gaussian Splatting from   Casually Captured Videos
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17862.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
