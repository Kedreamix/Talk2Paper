<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-26  Beyond Labels Zero-Shot Diabetic Foot Ulcer Wound Segmentation with   Self-attention Diffusion Models and the Potential for Text-Guided   Customization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f4b84ec53e433cda86e5a98e5b439053.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    43 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-26-æ›´æ–°"><a href="#2025-04-26-æ›´æ–°" class="headerlink" title="2025-04-26 æ›´æ–°"></a>2025-04-26 æ›´æ–°</h1><h2 id="Beyond-Labels-Zero-Shot-Diabetic-Foot-Ulcer-Wound-Segmentation-with-Self-attention-Diffusion-Models-and-the-Potential-for-Text-Guided-Customization"><a href="#Beyond-Labels-Zero-Shot-Diabetic-Foot-Ulcer-Wound-Segmentation-with-Self-attention-Diffusion-Models-and-the-Potential-for-Text-Guided-Customization" class="headerlink" title="Beyond Labels: Zero-Shot Diabetic Foot Ulcer Wound Segmentation with   Self-attention Diffusion Models and the Potential for Text-Guided   Customization"></a>Beyond Labels: Zero-Shot Diabetic Foot Ulcer Wound Segmentation with   Self-attention Diffusion Models and the Potential for Text-Guided   Customization</h2><p><strong>Authors:Abderrachid Hamrani, Daniela Leizaola, Renato Sousa, Jose P. Ponce, Stanley Mathis, David G. Armstrong, Anuradha Godavarty</strong></p>
<p>Diabetic foot ulcers (DFUs) pose a significant challenge in healthcare, requiring precise and efficient wound assessment to enhance patient outcomes. This study introduces the Attention Diffusion Zero-shot Unsupervised System (ADZUS), a novel text-guided diffusion model that performs wound segmentation without relying on labeled training data. Unlike conventional deep learning models, which require extensive annotation, ADZUS leverages zero-shot learning to dynamically adapt segmentation based on descriptive prompts, offering enhanced flexibility and adaptability in clinical applications. Experimental evaluations demonstrate that ADZUS surpasses traditional and state-of-the-art segmentation models, achieving an IoU of 86.68% and the highest precision of 94.69% on the chronic wound dataset, outperforming supervised approaches such as FUSegNet. Further validation on a custom-curated DFU dataset reinforces its robustness, with ADZUS achieving a median DSC of 75%, significantly surpassing FUSegNetâ€™s 45%. The modelâ€™s text-guided segmentation capability enables real-time customization of segmentation outputs, allowing targeted analysis of wound characteristics based on clinical descriptions. Despite its competitive performance, the computational cost of diffusion-based inference and the need for potential fine-tuning remain areas for future improvement. ADZUS represents a transformative step in wound segmentation, providing a scalable, efficient, and adaptable AI-driven solution for medical imaging. </p>
<blockquote>
<p>ç³–å°¿ç—…è¶³æºƒç–¡ï¼ˆDFUsï¼‰åœ¨åŒ»ç–—ä¿å¥ä¸­æ„æˆé‡å¤§æŒ‘æˆ˜ï¼Œéœ€è¦è¿›è¡Œç²¾ç¡®é«˜æ•ˆçš„ä¼¤å£è¯„ä¼°ä»¥æé«˜æ‚£è€…æ²»ç–—æ•ˆæœã€‚æœ¬ç ”ç©¶ä»‹ç»äº†æ³¨æ„åŠ›æ‰©æ•£é›¶æ ·æœ¬æ— ç›‘ç£ç³»ç»Ÿï¼ˆADZUSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ–‡æœ¬å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼Œå¯åœ¨æ— éœ€æ ‡æ³¨è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œä¼¤å£åˆ†å‰²ã€‚ä¸åŒäºéœ€è¦å¤§é‡æ ‡æ³¨çš„ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒADZUSé‡‡ç”¨é›¶æ ·æœ¬å­¦ä¹ ï¼Œæ ¹æ®æè¿°æ€§æç¤ºåŠ¨æ€é€‚åº”åˆ†å‰²ï¼Œåœ¨ä¸´åºŠåº”ç”¨ä¸­æä¾›äº†æ›´é«˜çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒADZUSè¶…è¶Šäº†ä¼ ç»Ÿå’Œæœ€å…ˆè¿›çš„åˆ†å‰²æ¨¡å‹ï¼Œåœ¨æ…¢æ€§ä¼¤å£æ•°æ®é›†ä¸Šè¾¾åˆ°86.68%çš„äº¤å¹¶æ¯”ï¼ˆIoUï¼‰å’Œæœ€é«˜94.69%çš„ç²¾ç¡®åº¦ï¼Œè¶…è¶Šäº†å¦‚FUSegNetç­‰ç›‘ç£æ–¹æ³•ã€‚åœ¨è‡ªå®šä¹‰çš„DFUæ•°æ®é›†ä¸Šè¿›ä¸€æ­¥éªŒè¯ï¼ŒADZUSçš„ä¸­ä½DSCè¾¾åˆ°75%ï¼Œæ˜¾è‘—è¶…è¶Šäº†FUSegNetçš„45%ã€‚è¯¥æ¨¡å‹çš„æ–‡æœ¬å¼•å¯¼åˆ†å‰²èƒ½åŠ›èƒ½å¤Ÿå®æ—¶å®šåˆ¶åˆ†å‰²è¾“å‡ºï¼Œå…è®¸æ ¹æ®ä¸´åºŠæè¿°è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„ä¼¤å£ç‰¹å¾åˆ†æã€‚å°½ç®¡å…¶æ€§èƒ½å…·æœ‰ç«äº‰åŠ›ï¼Œä½†æ‰©æ•£æ¨ç†çš„è®¡ç®—æˆæœ¬å’Œæ½œåœ¨ç²¾ç»†è°ƒæ•´çš„éœ€æ±‚ä»æ˜¯æœªæ¥æ”¹è¿›çš„é¢†åŸŸã€‚ADZUSæ˜¯ä¼¤å£åˆ†å‰²ä¸­çš„ä¸€é¡¹å˜é©æ€§è¿›æ­¥ï¼Œæä¾›å¯è§„æ¨¡åŒ–ã€é«˜æ•ˆã€é€‚åº”æ€§å¼ºçš„AIé©±åŠ¨è§£å†³æ–¹æ¡ˆï¼Œç”¨äºåŒ»å­¦æˆåƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17628v1">PDF</a> 12 pages, 8 figures, journal article</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§åä¸ºAttention Diffusion Zero-shot Unsupervised Systemï¼ˆADZUSï¼‰çš„æ–°å‹æ–‡æœ¬å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºç³–å°¿ç—…è¶³æºƒç–¡ï¼ˆDFUï¼‰çš„ä¼¤å£åˆ†å‰²ï¼Œä¸”æ— éœ€æ ‡æ³¨è®­ç»ƒæ•°æ®ã€‚ADZUSé‡‡ç”¨é›¶æ ·æœ¬å­¦ä¹ æ–¹å¼ï¼Œå¯æ ¹æ®æè¿°æ€§æç¤ºåŠ¨æ€é€‚åº”åˆ†å‰²ï¼Œæé«˜äº†åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒADZUSåœ¨æ…¢æ€§ä¼¤å£æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ä¼ ç»Ÿå’Œæœ€æ–°åˆ†å‰²æ¨¡å‹ï¼Œäº¤å¹¶æ¯”ï¼ˆIoUï¼‰è¾¾åˆ°86.68%ï¼Œç²¾åº¦æœ€é«˜è¾¾94.69%ï¼Œç”šè‡³è¶…è¶Šäº†FUSegNetç­‰ç›‘ç£æ–¹æ³•ã€‚åœ¨å®šåˆ¶çš„DFUæ•°æ®é›†ä¸Šçš„éªŒè¯ä¹Ÿè¯æ˜äº†å…¶ç¨³å¥æ€§ï¼ŒADZUSçš„ä¸­ä½DSCè¾¾åˆ°75%ï¼Œè¿œè¶…FUSegNetçš„45%ã€‚è¯¥æ¨¡å‹çš„æ–‡æœ¬å¼•å¯¼åˆ†å‰²èƒ½åŠ›å¯å®ç°åˆ†å‰²è¾“å‡ºçš„å®æ—¶å®šåˆ¶ï¼Œæ ¹æ®ä¸´åºŠæè¿°è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„ä¼¤å£ç‰¹æ€§åˆ†æã€‚è™½ç„¶æ‰©æ•£æ¨ç†çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ä¸”å¯èƒ½éœ€è¦è¿›è¡Œå¾®è°ƒï¼Œä½†ADZUSä»ä¸ºä¼¤å£åˆ†å‰²æä¾›äº†å¯è§„æ¨¡åŒ–ã€é«˜æ•ˆå’Œé€‚åº”æ€§å¼ºçš„AIé©±åŠ¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ADZUSæ˜¯ä¸€ç§æ–°å‹çš„æ–‡æœ¬å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºç³–å°¿ç—…è¶³æºƒç–¡çš„ä¼¤å£åˆ†å‰²ï¼Œæ— éœ€æ ‡æ³¨è®­ç»ƒæ•°æ®ã€‚</li>
<li>ADZUSé‡‡ç”¨é›¶æ ·æœ¬å­¦ä¹ ï¼Œå¯åŠ¨æ€é€‚åº”åˆ†å‰²ï¼Œæé«˜äº†ä¸´åºŠåº”ç”¨çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚</li>
<li>å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒADZUSåœ¨æ…¢æ€§ä¼¤å£æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ä¼ ç»Ÿå’Œæœ€æ–°çš„åˆ†å‰²æ¨¡å‹ã€‚</li>
<li>ADZUSåœ¨å®šåˆ¶çš„DFUæ•°æ®é›†ä¸Šè¡¨ç°å‡ºç¨³å¥æ€§ï¼Œä¸­ä½DSCè¾¾åˆ°75%ã€‚</li>
<li>ADZUSå…·å¤‡æ–‡æœ¬å¼•å¯¼åˆ†å‰²èƒ½åŠ›ï¼Œå¯å®ç°åˆ†å‰²è¾“å‡ºçš„å®æ—¶å®šåˆ¶ã€‚</li>
<li>æ‰©æ•£æ¨ç†çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œå¯èƒ½éœ€è¦è¿›è¡Œå¾®è°ƒã€‚</li>
<li>ADZUSä¸ºä¼¤å£åˆ†å‰²æä¾›äº†å¯è§„æ¨¡åŒ–ã€é«˜æ•ˆå’Œé€‚åº”æ€§å¼ºçš„AIé©±åŠ¨è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17628">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5eeccc0e0482d280482199e8d8da5e36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe2706fbcb29824851a5131b691f71ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a1583e48a79eae71ee492d5e4fb7484.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eba839dddf9240e449f25c86fe25c945.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90d655b07ab5a0ef07712ea204cf2a89.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DRC-Enhancing-Personalized-Image-Generation-via-Disentangled-Representation-Composition"><a href="#DRC-Enhancing-Personalized-Image-Generation-via-Disentangled-Representation-Composition" class="headerlink" title="DRC: Enhancing Personalized Image Generation via Disentangled   Representation Composition"></a>DRC: Enhancing Personalized Image Generation via Disentangled   Representation Composition</h2><p><strong>Authors:Yiyan Xu, Wuqiang Zheng, Wenjie Wang, Fengbin Zhu, Xinting Hu, Yang Zhang, Fuli Feng, Tat-Seng Chua</strong></p>
<p>Personalized image generation has emerged as a promising direction in multimodal content creation. It aims to synthesize images tailored to individual style preferences (e.g., color schemes, character appearances, layout) and semantic intentions (e.g., emotion, action, scene contexts) by leveraging user-interacted history images and multimodal instructions. Despite notable progress, existing methods â€“ whether based on diffusion models, large language models, or Large Multimodal Models (LMMs) â€“ struggle to accurately capture and fuse user style preferences and semantic intentions. In particular, the state-of-the-art LMM-based method suffers from the entanglement of visual features, leading to Guidance Collapse, where the generated images fail to preserve user-preferred styles or reflect the specified semantics.   To address these limitations, we introduce DRC, a novel personalized image generation framework that enhances LMMs through Disentangled Representation Composition. DRC explicitly extracts user style preferences and semantic intentions from history images and the reference image, respectively, to form user-specific latent instructions that guide image generation within LMMs. Specifically, it involves two critical learning stages: 1) Disentanglement learning, which employs a dual-tower disentangler to explicitly separate style and semantic features, optimized via a reconstruction-driven paradigm with difficulty-aware importance sampling; and 2) Personalized modeling, which applies semantic-preserving augmentations to effectively adapt the disentangled representations for robust personalized generation. Extensive experiments on two benchmarks demonstrate that DRC shows competitive performance while effectively mitigating the guidance collapse issue, underscoring the importance of disentangled representation learning for controllable and effective personalized image generation. </p>
<blockquote>
<p>ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆå·²æˆä¸ºå¤šåª’ä½“å†…å®¹åˆ›å»ºçš„ä¸€ä¸ªå…·æœ‰å‰æ™¯çš„æ–¹å‘ã€‚å®ƒæ—¨åœ¨é€šè¿‡åˆ©ç”¨ç”¨æˆ·äº¤äº’çš„å†å²å›¾åƒå’Œå¤šæ¨¡æ€æŒ‡ä»¤ï¼Œåˆæˆç¬¦åˆä¸ªäººé£æ ¼åå¥½ï¼ˆå¦‚è‰²å½©æ–¹æ¡ˆã€è§’è‰²å¤–è§‚ã€å¸ƒå±€ï¼‰å’Œè¯­ä¹‰æ„å›¾ï¼ˆå¦‚æƒ…ç»ªã€åŠ¨ä½œã€åœºæ™¯ä¸Šä¸‹æ–‡ï¼‰çš„å›¾åƒã€‚å°½ç®¡å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•â€”â€”æ— è®ºæ˜¯åŸºäºæ‰©æ•£æ¨¡å‹ã€å¤§å‹è¯­è¨€æ¨¡å‹è¿˜æ˜¯å¤§å‹å¤šåª’ä½“æ¨¡å‹ï¼ˆLMMï¼‰â€”â€”åœ¨å‡†ç¡®æ•æ‰å’Œèåˆç”¨æˆ·é£æ ¼åå¥½å’Œè¯­ä¹‰æ„å›¾æ–¹é¢éƒ½å­˜åœ¨å›°éš¾ã€‚ç‰¹åˆ«æ˜¯æœ€å…ˆè¿›çš„åŸºäºLMMçš„æ–¹æ³•å—åˆ°è§†è§‰ç‰¹å¾çº ç¼ çš„å½±å“ï¼Œå¯¼è‡´å‡ºç°â€œæŒ‡å¯¼å´©æºƒâ€çš„é—®é¢˜ï¼Œç”Ÿæˆçš„å›¾åƒæ— æ³•ä¿æŒç”¨æˆ·åå¥½çš„é£æ ¼æˆ–åæ˜ æŒ‡å®šçš„è¯­ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17349v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆæ˜¯ä¸€ä¸ªå……æ»¡å‰æ™¯çš„å¤šæ¨¡æ€å†…å®¹åˆ›å»ºæ–¹å‘ã€‚å®ƒæ—¨åœ¨é€šè¿‡åˆ©ç”¨ç”¨æˆ·äº¤äº’çš„å†å²å›¾åƒå’Œå¤šæ¨¡æ€æŒ‡ä»¤ï¼Œåˆæˆç¬¦åˆä¸ªäººé£æ ¼åå¥½å’Œè¯­ä¹‰æ„å›¾çš„å›¾åƒã€‚å°½ç®¡å·²æœ‰æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•ï¼Œæ— è®ºæ˜¯åŸºäºæ‰©æ•£æ¨¡å‹ã€å¤§å‹è¯­è¨€æ¨¡å‹è¿˜æ˜¯å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ï¼Œåœ¨å‡†ç¡®æ•æ‰å’Œèåˆç”¨æˆ·é£æ ¼åå¥½å’Œè¯­ä¹‰æ„å›¾æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚é’ˆå¯¹å½“å‰å…ˆè¿›LMMsçš„ç¼ºé™·ï¼Œæˆ‘ä»¬å¼•å…¥äº†DRCæ¡†æ¶ï¼Œé€šè¿‡è§£è€¦è¡¨ç¤ºç»„åˆå¢å¼ºLMMsã€‚DRCä»ç”¨æˆ·å†å²å›¾åƒä¸­æå–ç”¨æˆ·é£æ ¼åå¥½å’Œè¯­ä¹‰æ„å›¾ï¼Œå½¢æˆç”¨æˆ·ç‰¹å®šçš„æ½œåœ¨æŒ‡ä»¤ï¼Œåœ¨LMMså†…éƒ¨æŒ‡å¯¼å›¾åƒç”Ÿæˆã€‚å®éªŒè¯æ˜ï¼ŒDRCåœ¨ç«äº‰æ€§èƒ½çš„åŒæ—¶æœ‰æ•ˆç¼“è§£äº†æŒ‡å¯¼å´©æºƒé—®é¢˜ï¼Œå‡¸æ˜¾äº†è§£è€¦è¡¨ç¤ºå­¦ä¹ å¯¹äºå¯æ§ã€é«˜æ•ˆä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆçš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸ªäººåŒ–å›¾åƒç”Ÿæˆå·²æˆä¸ºå¤šæ¨¡æ€å†…å®¹åˆ›å»ºçš„ä¸€ä¸ªå‰æ™¯æ–¹å‘ï¼Œæ—¨åœ¨æ ¹æ®ç”¨æˆ·çš„é£æ ¼åå¥½å’Œè¯­ä¹‰æ„å›¾åˆæˆå›¾åƒã€‚</li>
<li>å½“å‰æ–¹æ³•ï¼ˆåŸºäºæ‰©æ•£æ¨¡å‹ã€å¤§å‹è¯­è¨€æ¨¡å‹æˆ–å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼‰åœ¨æ•æ‰å’Œèåˆç”¨æˆ·é£æ ¼åå¥½å’Œè¯­ä¹‰æ„å›¾æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>å¼•å…¥DRCæ¡†æ¶ä»¥å¢å¼ºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ï¼Œé€šè¿‡è§£è€¦è¡¨ç¤ºç»„åˆè§£å†³ç°æœ‰é—®é¢˜ã€‚</li>
<li>DRCä»ç”¨æˆ·å†å²å›¾åƒä¸­æå–ç”¨æˆ·é£æ ¼åå¥½å’Œè¯­ä¹‰æ„å›¾ï¼Œå½¢æˆç‰¹å®šçš„æ½œåœ¨æŒ‡ä»¤æ¥æŒ‡å¯¼å›¾åƒç”Ÿæˆã€‚</li>
<li>DRCåŒ…å«ä¸¤ä¸ªå…³é”®å­¦ä¹ é˜¶æ®µï¼šè§£è€¦å­¦ä¹ å’Œä¸ªæ€§åŒ–å»ºæ¨¡ã€‚</li>
<li>è§£è€¦å­¦ä¹ é˜¶æ®µé‡‡ç”¨åŒå¡”åˆ†ç¦»å™¨æ¥æ˜ç¡®åŒºåˆ†é£æ ¼å’Œè¯­ä¹‰ç‰¹å¾ï¼Œé€šè¿‡é‡å»ºé©±åŠ¨æ–¹æ³•å’Œéš¾åº¦æ„ŸçŸ¥é‡è¦æ€§é‡‡æ ·è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒDRCåœ¨ç«äº‰æ€§èƒ½çš„åŒæ—¶æœ‰æ•ˆç¼“è§£äº†æŒ‡å¯¼å´©æºƒé—®é¢˜ï¼Œå‡¸æ˜¾è§£è€¦è¡¨ç¤ºå­¦ä¹ çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17349">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f4b84ec53e433cda86e5a98e5b439053.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-70af2ee72af8b7081e6414c3620f2cad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97edd6cb76e8b2daaaf1bb4f6e7a3f0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88fe1da4009369164f605ff615968988.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4485d51bfc10b1c2853e6e9e833b12a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Towards-Generalized-and-Training-Free-Text-Guided-Semantic-Manipulation"><a href="#Towards-Generalized-and-Training-Free-Text-Guided-Semantic-Manipulation" class="headerlink" title="Towards Generalized and Training-Free Text-Guided Semantic Manipulation"></a>Towards Generalized and Training-Free Text-Guided Semantic Manipulation</h2><p><strong>Authors:Yu Hong, Xiao Cai, Pengpeng Zeng, Shuai Zhang, Jingkuan Song, Lianli Gao, Heng Tao Shen</strong></p>
<p>Text-guided semantic manipulation refers to semantically editing an image generated from a source prompt to match a target prompt, enabling the desired semantic changes (e.g., addition, removal, and style transfer) while preserving irrelevant contents. With the powerful generative capabilities of the diffusion model, the task has shown the potential to generate high-fidelity visual content. Nevertheless, existing methods either typically require time-consuming fine-tuning (inefficient), fail to accomplish multiple semantic manipulations (poorly extensible), and&#x2F;or lack support for different modality tasks (limited generalizability). Upon further investigation, we find that the geometric properties of noises in the diffusion model are strongly correlated with the semantic changes. Motivated by this, we propose a novel $\textit{GTF}$ for text-guided semantic manipulation, which has the following attractive capabilities: 1) $\textbf{Generalized}$: our $\textit{GTF}$ supports multiple semantic manipulations (e.g., addition, removal, and style transfer) and can be seamlessly integrated into all diffusion-based methods (i.e., Plug-and-play) across different modalities (i.e., modality-agnostic); and 2) $\textbf{Training-free}$: $\textit{GTF}$ produces high-fidelity results via simply controlling the geometric relationship between noises without tuning or optimization. Our extensive experiments demonstrate the efficacy of our approach, highlighting its potential to advance the state-of-the-art in semantics manipulation. </p>
<blockquote>
<p>æ–‡æœ¬å¼•å¯¼è¯­ä¹‰æ“çºµæ˜¯æŒ‡å¯¹ç”±æºæç¤ºç”Ÿæˆçš„å›¾åƒè¿›è¡Œè¯­ä¹‰ç¼–è¾‘ï¼Œä½¿å…¶ä¸ç›®æ ‡æç¤ºç›¸åŒ¹é…ï¼Œä»è€Œå®ç°æœŸæœ›çš„è¯­ä¹‰å˜åŒ–ï¼ˆä¾‹å¦‚æ·»åŠ ã€åˆ é™¤å’Œé£æ ¼è½¬æ¢ï¼‰ï¼ŒåŒæ—¶ä¿ç•™æ— å…³å†…å®¹ã€‚æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ä½¿è¯¥ä»»åŠ¡æœ‰æ½œåŠ›ç”Ÿæˆé«˜ä¿çœŸè§†è§‰å†…å®¹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦è€—æ—¶çš„å¾®è°ƒï¼ˆæ•ˆç‡ä½ä¸‹ï¼‰ã€æ— æ³•å®Œæˆå¤šæ¬¡è¯­ä¹‰æ“çºµï¼ˆæ‰©å±•æ€§å·®ï¼‰ï¼Œå¹¶ä¸”&#x2F;æˆ–ç¼ºä¹å¯¹ä¸åŒæ¨¡æ€ä»»åŠ¡çš„æ”¯æŒï¼ˆæ³›åŒ–èƒ½åŠ›æœ‰é™ï¼‰ã€‚é€šè¿‡è¿›ä¸€æ­¥è°ƒæŸ¥ï¼Œæˆ‘ä»¬å‘ç°æ‰©æ•£æ¨¡å‹ä¸­çš„å™ªå£°å‡ ä½•å±æ€§ä¸è¯­ä¹‰å˜åŒ–å…·æœ‰å¾ˆå¼ºçš„ç›¸å…³æ€§ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæ–‡æœ¬å¼•å¯¼è¯­ä¹‰æ“çºµçš„æ–°å‹GTFæ–¹æ³•ï¼Œå®ƒå…·æœ‰ä»¥ä¸‹å¸å¼•äººçš„èƒ½åŠ›ï¼š1ï¼‰é€šç”¨æ€§ï¼šæˆ‘ä»¬çš„GTFæ”¯æŒå¤šç§è¯­ä¹‰æ“çºµï¼ˆä¾‹å¦‚æ·»åŠ ã€åˆ é™¤å’Œé£æ ¼è½¬æ¢ï¼‰ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°æ‰€æœ‰åŸºäºæ‰©æ•£çš„æ–¹æ³•ä¸­ï¼ˆå³æ’å³ç”¨ï¼‰ï¼Œé€‚ç”¨äºä¸åŒæ¨¡æ€ï¼ˆå³æ¨¡æ€æ— å…³ï¼‰ï¼›2ï¼‰æ— è®­ç»ƒï¼šGTFé€šè¿‡ç®€å•æ§åˆ¶å™ªå£°ä¹‹é—´çš„å‡ ä½•å…³ç³»å³å¯äº§ç”Ÿé«˜ä¿çœŸç»“æœï¼Œæ— éœ€è°ƒæ•´æˆ–ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œçªå‡ºäº†å…¶åœ¨è¯­ä¹‰æ“çºµæ–¹é¢é¢†å…ˆæŠ€æœ¯çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17269v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬å¼•å¯¼è¯­ä¹‰æ“çºµæŒ‡çš„æ˜¯å¯¹ç”±æºæç¤ºç”Ÿæˆçš„å›¾åƒè¿›è¡Œè¯­ä¹‰ç¼–è¾‘ï¼Œä½¿å…¶åŒ¹é…ç›®æ ‡æç¤ºï¼Œä»è€Œå®ç°æœŸæœ›çš„è¯­ä¹‰å˜åŒ–ï¼ˆä¾‹å¦‚æ·»åŠ ã€åˆ é™¤å’Œé£æ ¼è½¬æ¢ï¼‰ï¼ŒåŒæ—¶ä¿ç•™æ— å…³å†…å®¹ã€‚å€ŸåŠ©æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ï¼Œè¯¥ä»»åŠ¡æœ‰æ½œåŠ›ç”Ÿæˆé«˜ä¿çœŸè§†è§‰å†…å®¹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦è€—æ—¶ä¸”æ•ˆç‡ä½ä¸‹çš„å¾®è°ƒï¼Œéš¾ä»¥å®Œæˆå¤šç§è¯­ä¹‰æ“çºµï¼Œä¸”ç¼ºä¹å¯¹ä¸åŒæ¨¡æ€ä»»åŠ¡çš„æ”¯æŒã€‚æˆ‘ä»¬å‘ç°æ‰©æ•£æ¨¡å‹ä¸­çš„å™ªå£°å‡ ä½•å±æ€§ä¸è¯­ä¹‰å˜åŒ–ä¹‹é—´å­˜åœ¨å¼ºçƒˆç›¸å…³æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„GTFæ–‡æœ¬å¼•å¯¼è¯­ä¹‰æ“çºµæ–¹æ³•ï¼Œå…·æœ‰æ”¯æŒå¤šç§è¯­ä¹‰æ“çºµã€å¯æ— ç¼é›†æˆåˆ°æ‰€æœ‰æ‰©æ•£åŸºæ–¹æ³•ã€æ— éœ€è®­ç»ƒå³å¯äº§ç”Ÿé«˜ä¿çœŸç»“æœç­‰å¸å¼•åŠ›äººçš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬å¼•å¯¼è¯­ä¹‰æ“çºµèƒ½å¤Ÿç¼–è¾‘å›¾åƒä»¥åŒ¹é…ç›®æ ‡æç¤ºï¼Œå®ç°è¯­ä¹‰å˜åŒ–ï¼ŒåŒæ—¶ä¿ç•™æ— å…³å†…å®¹ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å…·æœ‰å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œåœ¨è¯­ä¹‰æ“çºµä»»åŠ¡ä¸­è¡¨ç°å‡ºæ½œåŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨æ•ˆç‡ä½ä¸‹ã€éš¾ä»¥å®Œæˆå¤šç§è¯­ä¹‰æ“çºµã€ç¼ºä¹å¯¹ä¸åŒæ¨¡æ€ä»»åŠ¡æ”¯æŒçš„é—®é¢˜ã€‚</li>
<li>å™ªå£°çš„å‡ ä½•å±æ€§ä¸è¯­ä¹‰å˜åŒ–ä¹‹é—´å­˜åœ¨å¼ºçƒˆç›¸å…³æ€§ã€‚</li>
<li>æå‡ºçš„GTFæ–¹æ³•æ”¯æŒå¤šç§è¯­ä¹‰æ“çºµï¼Œå¹¶èƒ½æ— ç¼é›†æˆåˆ°æ‰€æœ‰æ‰©æ•£åŸºæ–¹æ³•ä¸­ã€‚</li>
<li>GTFæ–¹æ³•æ— éœ€è®­ç»ƒå³å¯äº§ç”Ÿé«˜ä¿çœŸç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17269">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e2904c3907150b5c0a8cf931237c6d28.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a4e2755939fcde4bbc745cfec835222.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ed0b404ab53b298ab302e30b67c606a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a10f27a104ee9725c9f933a57bf18e61.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DIVE-Inverting-Conditional-Diffusion-Models-for-Discriminative-Tasks"><a href="#DIVE-Inverting-Conditional-Diffusion-Models-for-Discriminative-Tasks" class="headerlink" title="DIVE: Inverting Conditional Diffusion Models for Discriminative Tasks"></a>DIVE: Inverting Conditional Diffusion Models for Discriminative Tasks</h2><p><strong>Authors:Yinqi Li, Hong Chang, Ruibing Hou, Shiguang Shan, Xilin Chen</strong></p>
<p>Diffusion models have shown remarkable progress in various generative tasks such as image and video generation. This paper studies the problem of leveraging pretrained diffusion models for performing discriminative tasks. Specifically, we extend the discriminative capability of pretrained frozen generative diffusion models from the classification task to the more complex object detection task, by â€œinvertingâ€ a pretrained layout-to-image diffusion model. To this end, a gradient-based discrete optimization approach for replacing the heavy prediction enumeration process, and a prior distribution model for making more accurate use of the Bayesâ€™ rule, are proposed respectively. Empirical results show that this method is on par with basic discriminative object detection baselines on COCO dataset. In addition, our method can greatly speed up the previous diffusion-based method for classification without sacrificing accuracy. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/LiYinqi/DIVE">https://github.com/LiYinqi/DIVE</a> . </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆç­‰å„ç§ç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚æœ¬æ–‡ç ”ç©¶äº†åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ‰§è¡Œåˆ¤åˆ«ä»»åŠ¡çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡â€œåè½¬â€é¢„è®­ç»ƒçš„å¸ƒå±€åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼Œå°†é¢„å†»ç»“ç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›ä»åˆ†ç±»ä»»åŠ¡æ‰©å±•åˆ°æ›´å¤æ‚çš„å¯¹è±¡æ£€æµ‹ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œåˆ†åˆ«æå‡ºäº†åŸºäºæ¢¯åº¦çš„ç¦»æ•£ä¼˜åŒ–æ–¹æ³•ï¼Œä»¥æ›¿ä»£ç¹é‡çš„é¢„æµ‹æšä¸¾è¿‡ç¨‹ï¼Œä»¥åŠå…ˆéªŒåˆ†å¸ƒæ¨¡å‹ï¼Œä»¥æ›´å‡†ç¡®åœ°åº”ç”¨è´å¶æ–¯æ³•åˆ™ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨COCOæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¸åŸºæœ¬çš„åˆ¤åˆ«å¯¹è±¡æ£€æµ‹åŸºçº¿ç›¸å½“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å¤§å¤§æé«˜ä¹‹å‰ç”¨äºåˆ†ç±»çš„æ‰©æ•£æ–¹æ³•çš„é€Ÿåº¦ï¼Œè€Œä¸ä¼šç‰ºç‰²å‡†ç¡®æ€§ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LiYinqi/DIVE%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LiYinqi/DIVEä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17253v1">PDF</a> Accepted by IEEE Transactions on Multimedia</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ‰§è¡Œåˆ¤åˆ«ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯å°†é¢„è®­ç»ƒçš„å†·å†»ç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›ä»åˆ†ç±»ä»»åŠ¡æ‰©å±•åˆ°æ›´å¤æ‚çš„å¯¹è±¡æ£€æµ‹ä»»åŠ¡ã€‚é€šè¿‡é‡‡ç”¨æ¢¯åº¦åŸºç¡€ä¸Šçš„ç¦»æ•£ä¼˜åŒ–æ–¹æ³•å’Œå…ˆéªŒåˆ†å¸ƒæ¨¡å‹ï¼Œå®ç°äº†å¯¹é¢„è®­ç»ƒå¸ƒå±€åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„â€œåè½¬â€ã€‚åœ¨COCOæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•ä¸åŸºæœ¬çš„åˆ¤åˆ«å¯¹è±¡æ£€æµ‹åŸºçº¿è¡¨ç°ç›¸å½“ï¼Œå¹¶ä¸”èƒ½æ˜¾è‘—åŠ å¿«ä¹‹å‰çš„æ‰©æ•£åˆ†ç±»æ–¹æ³•çš„é€Ÿåº¦ï¼ŒåŒæ—¶ä¸æŸå¤±å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æ¢ç´¢äº†é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹åœ¨åˆ¤åˆ«ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å°†åˆ†ç±»ä»»åŠ¡çš„åˆ¤åˆ«èƒ½åŠ›æ‰©å±•åˆ°å¯¹è±¡æ£€æµ‹ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡â€œåè½¬â€é¢„è®­ç»ƒçš„å¸ƒå±€åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†è¿™ä¸€æ‰©å±•ã€‚</li>
<li>é‡‡ç”¨äº†æ¢¯åº¦åŸºç¡€ä¸Šçš„ç¦»æ•£ä¼˜åŒ–æ–¹æ³•ï¼Œä»¥æ›¿ä»£ç¹é‡çš„é¢„æµ‹æšä¸¾è¿‡ç¨‹ã€‚</li>
<li>å¼•å…¥äº†å…ˆéªŒåˆ†å¸ƒæ¨¡å‹ï¼Œæ›´å‡†ç¡®åœ°åº”ç”¨è´å¶æ–¯è§„åˆ™ã€‚</li>
<li>åœ¨COCOæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•ä¸åˆ¤åˆ«å¯¹è±¡æ£€æµ‹çš„åŸºçº¿æ–¹æ³•è¡¨ç°ç›¸å½“ã€‚</li>
<li>ä¸ä¹‹å‰çš„æ‰©æ•£åˆ†ç±»æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—åŠ é€Ÿäº†è¿‡ç¨‹ï¼ŒåŒæ—¶ä¿æŒäº†å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-18495e567652a39c5f9f94042ef80e37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-937575dabbd806032a0ba53bdd79ba51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86a10b0ffbe50f70ef395b7fb683b197.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df21a56626a1795813cfe1792d9b15f3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AUTHENTICATION-Identifying-Rare-Failure-Modes-in-Autonomous-Vehicle-Perception-Systems-using-Adversarially-Guided-Diffusion-Models"><a href="#AUTHENTICATION-Identifying-Rare-Failure-Modes-in-Autonomous-Vehicle-Perception-Systems-using-Adversarially-Guided-Diffusion-Models" class="headerlink" title="AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle   Perception Systems using Adversarially Guided Diffusion Models"></a>AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle   Perception Systems using Adversarially Guided Diffusion Models</h2><p><strong>Authors:Mohammad Zarei, Melanie A Jutras, Eliana Evans, Mike Tan, Omid Aaramoon</strong></p>
<p>Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately detect objects and interpret their surroundings. However, even when trained using millions of miles of real-world data, AVs are often unable to detect rare failure modes (RFMs). The problem of RFMs is commonly referred to as the â€œlong-tail challengeâ€, due to the distribution of data including many instances that are very rarely seen. In this paper, we present a novel approach that utilizes advanced generative and explainable AI techniques to aid in understanding RFMs. Our methods can be used to enhance the robustness and reliability of AVs when combined with both downstream model training and testing. We extract segmentation masks for objects of interest (e.g., cars) and invert them to create environmental masks. These masks, combined with carefully crafted text prompts, are fed into a custom diffusion model. We leverage the Stable Diffusion inpainting model guided by adversarial noise optimization to generate images containing diverse environments designed to evade object detection models and expose vulnerabilities in AI systems. Finally, we produce natural language descriptions of the generated RFMs that can guide developers and policymakers to improve the safety and reliability of AV systems. </p>
<blockquote>
<p>è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆAVsï¼‰ä¾èµ–äºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ¥å‡†ç¡®æ£€æµ‹ç‰©ä½“å¹¶è§£è¯»å‘¨å›´ç¯å¢ƒã€‚ç„¶è€Œï¼Œå³ä½¿ä½¿ç”¨æ•°ç™¾ä¸‡è‹±é‡Œçš„çœŸå®ä¸–ç•Œæ•°æ®è¿›è¡Œè®­ç»ƒï¼ŒAVsé€šå¸¸ä¹Ÿæ— æ³•æ£€æµ‹åˆ°ç½•è§çš„å¤±è´¥æ¨¡å¼ï¼ˆRFMsï¼‰ã€‚ç”±äºæ•°æ®åˆ†å¸ƒåŒ…å«è®¸å¤šå¾ˆå°‘è§åˆ°çš„å®ä¾‹ï¼ŒRFMçš„é—®é¢˜é€šå¸¸è¢«ç§°ä¸ºâ€œé•¿å°¾æŒ‘æˆ˜â€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å…ˆè¿›çš„ç”Ÿæˆå¼å’Œå¯è§£é‡Šçš„äººå·¥æ™ºèƒ½æŠ€æœ¯æ¥å¸®åŠ©ç†è§£RFMsçš„æ–°æ–¹æ³•ã€‚å½“ä¸ä¸‹æ¸¸æ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•ç›¸ç»“åˆæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ç”¨äºæé«˜AVsçš„é²æ£’æ€§å’Œå¯é æ€§ã€‚æˆ‘ä»¬æå–æ„Ÿå…´è¶£å¯¹è±¡ï¼ˆä¾‹å¦‚æ±½è½¦ï¼‰çš„åˆ†å‰²æ©è†œå¹¶å°†å…¶åè½¬ä»¥åˆ›å»ºç¯å¢ƒæ©è†œã€‚è¿™äº›æ©è†œä¸ç²¾å¿ƒè®¾è®¡çš„æ–‡æœ¬æç¤ºç›¸ç»“åˆï¼Œè¾“å…¥åˆ°è‡ªå®šä¹‰çš„æ‰©æ•£æ¨¡å‹ä¸­ã€‚æˆ‘ä»¬åˆ©ç”¨ç”±å¯¹æŠ—æ€§å™ªå£°ä¼˜åŒ–å¼•å¯¼çš„Stable Diffusionè¡¥å…¨æ¨¡å‹ï¼Œç”ŸæˆåŒ…å«å„ç§ç¯å¢ƒçš„å›¾åƒï¼Œè¿™äº›å›¾åƒæ—¨åœ¨é¿å¼€ç‰©ä½“æ£€æµ‹æ¨¡å‹å¹¶æš´éœ²AIç³»ç»Ÿçš„æ¼æ´ã€‚æœ€åï¼Œæˆ‘ä»¬ç”Ÿæˆå¯¹ç”Ÿæˆçš„RFMsçš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œå¯ä»¥æŒ‡å¯¼å¼€å‘è€…å’Œæ”¿ç­–åˆ¶å®šè€…æé«˜AVç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17179v1">PDF</a> 8 pages, 10 figures. Accepted to IEEE Conference on Artificial   Intelligence (CAI), 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨å…ˆè¿›çš„ç”Ÿæˆå¼å’Œå¯è§£é‡Šçš„äººå·¥æ™ºèƒ½æŠ€æœ¯æ¥ç†è§£è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„ç½•è§æ•…éšœæ¨¡å¼ï¼ˆRFMsï¼‰çš„æ–°æ–¹æ³•ã€‚é€šè¿‡ç»“åˆä¸‹æ¸¸æ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•ï¼Œå¢å¼ºè‡ªåŠ¨é©¾é©¶è½¦è¾†çš„ç¨³å¥æ€§å’Œå¯é æ€§ã€‚é€šè¿‡æå–æ„Ÿå…´è¶£å¯¹è±¡çš„åˆ†å‰²æ©è†œå¹¶åè½¬åˆ›å»ºç¯å¢ƒæ©è†œï¼Œç»“åˆç²¾å¿ƒè®¾è®¡çš„æ–‡æœ¬æç¤ºï¼Œè¾“å…¥åˆ°è‡ªå®šä¹‰çš„æ‰©æ•£æ¨¡å‹ä¸­ã€‚åˆ©ç”¨å¯¹æŠ—æ€§å™ªå£°ä¼˜åŒ–å¼•å¯¼çš„Stable Diffusion inpaintingæ¨¡å‹ç”ŸæˆåŒ…å«å¤šæ ·ç¯å¢ƒçš„å›¾åƒï¼Œä»¥é¿å¼€ç‰©ä½“æ£€æµ‹æ¨¡å‹å¹¶æš´éœ²AIç³»ç»Ÿçš„æ¼æ´ã€‚æœ€åï¼Œç”Ÿæˆå¯¹RFMsçš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œå¯æŒ‡å¯¼å¼€å‘è€…å’Œæ”¿ç­–åˆ¶å®šè€…æé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆAVsï¼‰ä¾èµ–äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ¥æ£€æµ‹ç‰©ä½“å’Œè§£è¯»å‘¨å›´ç¯å¢ƒï¼Œä½†é¢ä¸´ç½•è§æ•…éšœæ¨¡å¼ï¼ˆRFMsï¼‰çš„æŒ‘æˆ˜ã€‚</li>
<li>RFMsé—®é¢˜è¢«ç§°ä¸ºâ€œé•¿å°¾æŒ‘æˆ˜â€ï¼Œå› ä¸ºæ•°æ®åˆ†å¸ƒåŒ…å«è®¸å¤šå¾ˆå°‘è§åˆ°çš„å®ä¾‹ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨ç”Ÿæˆå¼å’Œå¯è§£é‡Šçš„äººå·¥æ™ºèƒ½æŠ€æœ¯ç†è§£RFMsçš„æ–°æ–¹æ³•ã€‚</li>
<li>é€šè¿‡æå–æ„Ÿå…´è¶£å¯¹è±¡çš„åˆ†å‰²æ©è†œå¹¶åˆ›å»ºç¯å¢ƒæ©è†œï¼Œç»“åˆæ–‡æœ¬æç¤ºè¾“å…¥åˆ°è‡ªå®šä¹‰æ‰©æ•£æ¨¡å‹ä¸­ã€‚</li>
<li>åˆ©ç”¨Stable Diffusion inpaintingæ¨¡å‹ç”Ÿæˆå¤šæ ·ç¯å¢ƒå›¾åƒï¼Œä»¥æ£€æµ‹ç‰©ä½“å¹¶æš´éœ²AIç³»ç»Ÿçš„æ¼æ´ã€‚</li>
<li>ç”Ÿæˆçš„è‡ªç„¶è¯­è¨€æè¿°å¯ä»¥å¸®åŠ©å¼€å‘è€…å’Œæ”¿ç­–åˆ¶å®šè€…äº†è§£RFMsï¼Œæé«˜è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17179">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b38dd75b0e8d7ae74f5aee44c36242b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-864ffea2418abc67222a64cac38237cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7db5a7e3018bc6f8f31806915c76b7f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b424344865eb68843b119e35d6f76fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bdf2b59aa2628953cd8ae495eab7b900.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f45f3ddd3b593836295a8a0a43cb1cc.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Physics-guided-and-fabrication-aware-inverse-design-of-photonic-devices-using-diffusion-models"><a href="#Physics-guided-and-fabrication-aware-inverse-design-of-photonic-devices-using-diffusion-models" class="headerlink" title="Physics-guided and fabrication-aware inverse design of photonic devices   using diffusion models"></a>Physics-guided and fabrication-aware inverse design of photonic devices   using diffusion models</h2><p><strong>Authors:Dongjin Seo, Soobin Um, Sangbin Lee, Jong Chul Ye, Haejun Chung</strong></p>
<p>Designing free-form photonic devices is fundamentally challenging due to the vast number of possible geometries and the complex requirements of fabrication constraints. Traditional inverse-design approachesâ€“whether driven by human intuition, global optimization, or adjoint-based gradient methodsâ€“often involve intricate binarization and filtering steps, while recent deep learning strategies demand prohibitively large numbers of simulations (10^5 to 10^6). To overcome these limitations, we present AdjointDiffusion, a physics-guided framework that integrates adjoint sensitivity gradients into the sampling process of diffusion models. AdjointDiffusion begins by training a diffusion network on a synthetic, fabrication-aware dataset of binary masks. During inference, we compute the adjoint gradient of a candidate structure and inject this physics-based guidance at each denoising step, steering the generative process toward high figure-of-merit (FoM) solutions without additional post-processing. We demonstrate our method on two canonical photonic design problemsâ€“a bent waveguide and a CMOS image sensor color routerâ€“and show that our method consistently outperforms state-of-the-art nonlinear optimizers (such as MMA and SLSQP) in both efficiency and manufacturability, while using orders of magnitude fewer simulations (approximately 2 x 10^2) than pure deep learning approaches (approximately 10^5 to 10^6). By eliminating complex binarization schedules and minimizing simulation overhead, AdjointDiffusion offers a streamlined, simulation-efficient, and fabrication-aware pipeline for next-generation photonic device design. Our open-source implementation is available at <a target="_blank" rel="noopener" href="https://github.com/dongjin-seo2020/AdjointDiffusion">https://github.com/dongjin-seo2020/AdjointDiffusion</a>. </p>
<blockquote>
<p>è®¾è®¡è‡ªç”±å½¢å¼çš„å…‰å­å™¨ä»¶å…·æœ‰æ ¹æœ¬æŒ‘æˆ˜æ€§ï¼ŒåŸå› åœ¨äºå­˜åœ¨å¤§é‡å¯èƒ½çš„å‡ ä½•ç»“æ„ä»¥åŠå¤æ‚çš„åˆ¶é€ çº¦æŸè¦æ±‚ã€‚ä¼ ç»Ÿçš„é€†å‘è®¾è®¡æ–¹æ³•â€”â€”æ— è®ºæ˜¯åŸºäºäººç±»ç›´è§‰ã€å…¨å±€ä¼˜åŒ–è¿˜æ˜¯åŸºäºä¼´éšçš„æ¢¯åº¦æ–¹æ³•â€”â€”é€šå¸¸æ¶‰åŠå¤æ‚çš„äºŒå€¼åŒ–å’Œè¿‡æ»¤æ­¥éª¤ï¼Œè€Œæœ€è¿‘çš„æ·±åº¦å­¦ä¹ ç­–ç•¥åˆ™éœ€è¦è¿›è¡Œå¤§é‡æ¨¡æ‹Ÿï¼ˆä»10ä¸‡åˆ°ä¸€ç™¾ä¸‡ï¼‰ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†AdjointDiffusionï¼Œè¿™æ˜¯ä¸€ä¸ªç‰©ç†å¼•å¯¼æ¡†æ¶ï¼Œå®ƒå°†ä¼´éšæ•æ„Ÿæ€§æ¢¯åº¦æ•´åˆåˆ°æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹ä¸­ã€‚AdjointDiffusioné¦–å…ˆåœ¨ä¸€ä¸ªåˆæˆä¸”äº†è§£åˆ¶é€ è¿‡ç¨‹çš„äºŒè¿›åˆ¶æ©è†œæ•°æ®é›†ä¸Šè®­ç»ƒæ‰©æ•£ç½‘ç»œã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—å€™é€‰ç»“æ„çš„ä¼´éšæ¢¯åº¦ï¼Œå¹¶åœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­æ³¨å…¥åŸºäºç‰©ç†çš„æŒ‡å¯¼ï¼Œå¼•å¯¼ç”Ÿæˆè¿‡ç¨‹æœç€é«˜å“è´¨å› æ•°ï¼ˆFoMï¼‰è§£å†³æ–¹æ¡ˆå‘å±•ï¼Œæ— éœ€é¢å¤–çš„åå¤„ç†ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…¸å‹çš„å…‰å­è®¾è®¡é—®é¢˜ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•â€”â€”å¼¯æ›²æ³¢å¯¼å’ŒCMOSå›¾åƒä¼ æ„Ÿå™¨é¢œè‰²è·¯ç”±å™¨â€”â€”å¹¶è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•ˆç‡å’Œåˆ¶é€ æ€§æ–¹é¢å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„éçº¿æ€§ä¼˜åŒ–å™¨ï¼ˆå¦‚MMAå’ŒSLSQPï¼‰ï¼ŒåŒæ—¶ä½¿ç”¨çš„æ¨¡æ‹Ÿæ¬¡æ•°æ¯”çº¯æ·±åº¦å­¦ä¹ æ–¹æ³•å°‘å‡ ä¸ªæ•°é‡çº§ï¼ˆå¤§çº¦ä¸º2 x 10Â² vs. å¤§çº¦ä»‹äºä¸€ç™¾ä¸‡ä»¥ä¸Šï¼‰ã€‚é€šè¿‡æ¶ˆé™¤å¤æ‚çš„äºŒå€¼åŒ–æ—¶é—´è¡¨å¹¶æœ€å¤§é™åº¦åœ°å‡å°‘æ¨¡æ‹Ÿå¼€é”€ï¼ŒAdjointDiffusionä¸ºä¸‹ä¸€ä»£å…‰å­å™¨ä»¶è®¾è®¡æä¾›äº†ç®€åŒ–ã€æ¨¡æ‹Ÿé«˜æ•ˆä¸”äº†è§£åˆ¶é€ è¿‡ç¨‹çš„ç®¡é“ã€‚æˆ‘ä»¬çš„å¼€æºå®ç°å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/dongjin-seo2020/AdjointDiffusion%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/dongjin-seo2020/AdjointDiffusionæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17077v1">PDF</a> 25 pages, 7 Figures</p>
<p><strong>Summary</strong><br>å…‰å­å™¨ä»¶è®¾è®¡é¢ä¸´å·¨å¤§çš„å‡ ä½•å¯èƒ½æ€§å’Œå¤æ‚çš„åˆ¶é€ çº¦æŸæŒ‘æˆ˜ã€‚ä¼ ç»Ÿé€†å‘è®¾è®¡æ–¹æ³•å’Œæœ€æ–°çš„æ·±åº¦å­¦ä¹ ç­–ç•¥éƒ½å­˜åœ¨å±€é™æ€§ã€‚AdjointDiffusionæ¡†æ¶ç»“åˆé€†å‘æ•æ„Ÿæ€§æ¢¯åº¦ä¸æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹ï¼Œä»¥ç‰©ç†æŒ‡å¯¼ç”Ÿæˆè®¾è®¡ã€‚è¯¥æ¡†æ¶åœ¨åˆæˆã€å…·æœ‰åˆ¶é€ æ„è¯†çš„äºŒè¿›åˆ¶æ©è†œæ•°æ®é›†ä¸Šè®­ç»ƒæ‰©æ•£ç½‘ç»œï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­è®¡ç®—å€™é€‰ç»“æ„çš„é€†å‘æ¢¯åº¦ï¼Œåœ¨æ¯ä¸ªå»å™ªæ­¥éª¤ä¸­æ³¨å…¥ç‰©ç†æŒ‡å¯¼ï¼Œå¼•å¯¼ç”Ÿæˆè¿‡ç¨‹å‘é«˜å›¾æ¢…å°”æ€§èƒ½è§£å†³æ–¹æ¡ˆå‘å±•ã€‚è¯¥æ–¹æ³•åœ¨å…‰å­è®¾è®¡é—®é¢˜ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½¿ç”¨è¾ƒå°‘çš„æ¨¡æ‹Ÿæ¬¡æ•°ï¼Œæé«˜äº†æ•ˆç‡å’Œåˆ¶é€ æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…‰å­å™¨ä»¶è®¾è®¡é¢ä¸´å·¨å¤§çš„å‡ ä½•å’Œåˆ¶é€ çº¦æŸæŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿé€†å‘è®¾è®¡æ–¹æ³•å’Œæ·±åº¦å­¦ä¹ ç­–ç•¥éƒ½æœ‰å±€é™æ€§ã€‚</li>
<li>AdjointDiffusionæ¡†æ¶é€šè¿‡ç»“åˆé€†å‘æ•æ„Ÿæ€§æ¢¯åº¦å’Œæ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹æ¥è¿›è¡Œç‰©ç†æŒ‡å¯¼ç”Ÿæˆè®¾è®¡ã€‚</li>
<li>è¯¥æ¡†æ¶ä½¿ç”¨åˆæˆã€å…·æœ‰åˆ¶é€ æ„è¯†çš„äºŒè¿›åˆ¶æ©è†œæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚</li>
<li>åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œè®¡ç®—å€™é€‰ç»“æ„çš„é€†å‘æ¢¯åº¦å¹¶æ³¨å…¥ç‰©ç†æŒ‡å¯¼ã€‚</li>
<li>AdjointDiffusionåœ¨å…‰å­è®¾è®¡é—®é¢˜ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæé«˜äº†æ•ˆç‡å’Œåˆ¶é€ æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17077">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-992a9100ea7aa09abffe5c0f76b8febb.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Deep-Generative-Model-Based-Generation-of-Synthetic-Individual-Specific-Brain-MRI-Segmentations"><a href="#Deep-Generative-Model-Based-Generation-of-Synthetic-Individual-Specific-Brain-MRI-Segmentations" class="headerlink" title="Deep Generative Model-Based Generation of Synthetic Individual-Specific   Brain MRI Segmentations"></a>Deep Generative Model-Based Generation of Synthetic Individual-Specific   Brain MRI Segmentations</h2><p><strong>Authors:Ruijie Wang, Luca Rossetto, Susan MÃ©rillat, Christina RÃ¶cke, Mike Martin, Abraham Bernstein</strong></p>
<p>To the best of our knowledge, all existing methods that can generate synthetic brain magnetic resonance imaging (MRI) scans for a specific individual require detailed structural or volumetric information about the individualâ€™s brain. However, such brain information is often scarce, expensive, and difficult to obtain. In this paper, we propose the first approach capable of generating synthetic brain MRI segmentations â€“ specifically, 3D white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF) segmentations â€“ for individuals using their easily obtainable and often readily available demographic, interview, and cognitive test information. Our approach features a novel deep generative model, CSegSynth, which outperforms existing prominent generative models, including conditional variational autoencoder (C-VAE), conditional generative adversarial network (C-GAN), and conditional latent diffusion model (C-LDM). We demonstrate the high quality of our synthetic segmentations through extensive evaluations. Also, in assessing the effectiveness of the individual-specific generation, we achieve superior volume prediction, with mean absolute errors of only 36.44mL, 29.20mL, and 35.51mL between the ground-truth WM, GM, and CSF volumes of test individuals and those volumes predicted based on generated individual-specific segmentations, respectively. </p>
<blockquote>
<p>æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæ‰€æœ‰ç›®å‰èƒ½å¤Ÿé’ˆå¯¹ç‰¹å®šä¸ªäººç”Ÿæˆåˆæˆçš„å¤§è„‘ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ‰«ææ–¹æ³•éƒ½éœ€è¦å…³äºä¸ªäººå¤§è„‘çš„è¯¦ç»†ç»“æ„æˆ–ä½“ç§¯ä¿¡æ¯ã€‚ç„¶è€Œï¼Œæ­¤ç±»å¤§è„‘ä¿¡æ¯é€šå¸¸ç¨€ç¼ºã€æ˜‚è´µä¸”éš¾ä»¥è·å–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ç§èƒ½å¤Ÿç”Ÿæˆåˆæˆå¤§è„‘MRIåˆ†å‰²çš„æ–¹æ³•â€”â€”å…·ä½“æ¥è¯´ï¼Œæ˜¯ç”Ÿæˆä¸‰ç»´ç™½è´¨ï¼ˆWMï¼‰ã€ç°è´¨ï¼ˆGMï¼‰å’Œè„‘è„Šæ¶²ï¼ˆCSFï¼‰çš„åˆ†å‰²â€”â€”ä½¿ç”¨ä¸ªäººæ˜“äºè·å–ä¸”ç»å¸¸å®¹æ˜“è·å¾—çš„äººå£ç»Ÿè®¡ã€é¢è¯•å’Œè®¤çŸ¥æµ‹è¯•ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨äº†ä¸€ç§æ–°å‹æ·±åº¦ç”Ÿæˆæ¨¡å‹CSegSynthï¼Œå®ƒåœ¨ç°æœ‰çªå‡ºçš„ç”Ÿæˆæ¨¡å‹ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬æ¡ä»¶å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆC-VAEï¼‰ã€æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆC-GANï¼‰å’Œæ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆC-LDMï¼‰ã€‚æˆ‘ä»¬é€šè¿‡å¹¿æ³›è¯„ä¼°è¯æ˜äº†åˆæˆåˆ†å‰²çš„é«˜è´¨é‡ã€‚åŒæ—¶ï¼Œåœ¨è¯„ä¼°ç‰¹å®šä¸ªäººçš„ç”Ÿæˆæ•ˆæœæ—¶ï¼Œæˆ‘ä»¬å®ç°äº†å‡ºè‰²çš„ä½“ç§¯é¢„æµ‹ï¼Œæµ‹è¯•ä¸ªä½“çœŸå®WMã€GMå’ŒCSFä½“ç§¯ä¸åŸºäºç”Ÿæˆä¸ªä½“ç‰¹å®šåˆ†å‰²æ‰€é¢„æµ‹çš„ç›¸åº”ä½“ç§¯ä¹‹é—´çš„å¹³å‡ç»å¯¹è¯¯å·®åˆ†åˆ«ä¸ºä»…36.44mLã€29.20mLå’Œ35.51mLã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12352v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡ä¸ªä½“æ˜“äºè·å–ä¸”ç»å¸¸å¯å¾—çš„è¯¸å¦‚äººå£ç»Ÿè®¡ä¿¡æ¯ã€è®¿è°ˆå’Œè®¤çŸ¥æµ‹è¯•ä¿¡æ¯ï¼Œç”Ÿæˆåˆæˆçš„å¤§è„‘MRIæ‰«æå›¾åƒåˆ†å‰²ç»“æœã€‚è¯¥æ–¹æ³•é‡‡ç”¨åˆ›æ–°çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹CSegSynthï¼Œç›¸è¾ƒäºç°æœ‰çš„ä¸»æµç”Ÿæˆæ¨¡å‹å¦‚æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆC-VAEï¼‰ã€æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆC-GANï¼‰å’Œæ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆC-LDMï¼‰æœ‰æ›´å¥½çš„è¡¨ç°ã€‚é€šè¿‡å¹¿æ³›çš„è¯„ä¼°å’Œä½“ç§¯é¢„æµ‹ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§å’Œå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰ç”Ÿæˆå¤§è„‘MRIæ‰«æå›¾åƒçš„æ–¹æ³•éœ€è¦è¯¦ç»†çš„ä¸ªä½“ç»“æ„æˆ–ä½“ç§¯ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯é€šå¸¸ç¨€ç¼ºã€æ˜‚è´µä¸”éš¾ä»¥è·å–ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä½¿ç”¨ä¸ªä½“æ˜“äºè·å–çš„ä¿¡æ¯ï¼ˆå¦‚äººå£ç»Ÿè®¡ä¿¡æ¯ã€è®¿è°ˆå’Œè®¤çŸ¥æµ‹è¯•ä¿¡æ¯ï¼‰ç”Ÿæˆåˆæˆå¤§è„‘MRIåˆ†å‰²ç»“æœçš„æ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨åˆ›æ–°çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹CSegSynthï¼Œèƒ½ç”Ÿæˆç‰¹å®šä¸ªä½“çš„3Dç™½è´¨ã€ç°è´¨å’Œè„‘è„Šæ¶²åˆ†å‰²å›¾åƒã€‚</li>
<li>CSegSynthæ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„ä¸»æµç”Ÿæˆæ¨¡å‹ï¼Œå¦‚æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ã€æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå’Œæ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡å¹¿æ³›çš„è¯„ä¼°éªŒè¯äº†åˆæˆåˆ†å‰²å›¾åƒçš„é«˜è´¨é‡ã€‚</li>
<li>åœ¨è¯„ä¼°ä¸ªä½“ç‰¹å®šç”Ÿæˆçš„æœ‰æ•ˆæ€§æ–¹é¢ï¼Œè¯¥æ–¹æ³•å®ç°äº†ä¼˜å¼‚çš„ä½“ç§¯é¢„æµ‹ï¼Œä¸çœŸå®MRIä½“ç§¯ç›¸æ¯”ï¼Œé¢„æµ‹è¯¯å·®éå¸¸å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12352">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca49803a4c7bc1471151bfaf23e398c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-064a0cd210275ae7840ecedd9e19bd84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5474f6add41bfacb071d56e014bd3ddc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a1eb487f7cab1e2d7e61dfd4e3720f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdfc7da1db8c4eefbb77c36f9a7bef8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3dd61af50bad981f4dcaf1f959451d8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Improved-implicit-diffusion-model-with-knowledge-distillation-to-estimate-the-spatial-distribution-density-of-carbon-stock-in-remote-sensing-imagery"><a href="#Improved-implicit-diffusion-model-with-knowledge-distillation-to-estimate-the-spatial-distribution-density-of-carbon-stock-in-remote-sensing-imagery" class="headerlink" title="Improved implicit diffusion model with knowledge distillation to   estimate the spatial distribution density of carbon stock in remote sensing   imagery"></a>Improved implicit diffusion model with knowledge distillation to   estimate the spatial distribution density of carbon stock in remote sensing   imagery</h2><p><strong>Authors:Zhenyu Yu, Jinnian Wang, Mohd Yamani Idna Idris</strong></p>
<p>The forest serves as the most significant terrestrial carbon stock mechanism, effectively reducing atmospheric CO2 concentrations and mitigating climate change. Remote sensing provides high data accuracy and enables large-scale observations. Optical images facilitate long-term monitoring, which is crucial for future carbon stock estimation studies. This study focuses on Huize County, Qujing City, Yunnan Province, China, utilizing GF-1 WFV satellite imagery. The KD-VGG and KD-UNet modules were introduced for initial feature extraction, and the improved implicit diffusion model (IIDM) was proposed. The results showed: (1) The VGG module improved initial feature extraction, improving accuracy, and reducing inference time with optimized model parameters. (2) The Cross-attention + MLPs module enabled effective feature fusion, establishing critical relationships between global and local features, achieving high-accuracy estimation. (3) The IIDM model, a novel contribution, demonstrated the highest estimation accuracy with an RMSE of 12.17%, significantly improving by 41.69% to 42.33% compared to the regression model. In carbon stock estimation, the generative model excelled in extracting deeper features, significantly outperforming other models, demonstrating the feasibility of AI-generated content in quantitative remote sensing. The 16-meter resolution estimates provide a robust basis for tailoring forest carbon sink regulations, enhancing regional carbon stock management. </p>
<blockquote>
<p>æ£®æ—ä½œä¸ºæœ€é‡è¦çš„é™†åœ°ç¢³å‚¨å­˜æœºåˆ¶ï¼Œæœ‰æ•ˆåœ°é™ä½äº†å¤§æ°”ä¸­äºŒæ°§åŒ–ç¢³çš„æµ“åº¦ï¼Œç¼“è§£äº†æ°”å€™å˜åŒ–ã€‚é¥æ„ŸæŠ€æœ¯æä¾›äº†é«˜ç²¾åº¦çš„æ•°æ®ï¼Œå¹¶å®ç°äº†å¤§è§„æ¨¡è§‚æµ‹ã€‚å…‰å­¦å›¾åƒæœ‰åŠ©äºé•¿æœŸç›‘æµ‹ï¼Œå¯¹æœªæ¥ç¢³å‚¨é‡ä¼°ç®—ç ”ç©¶è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶ä»¥ä½äºä¸­å›½äº‘å—çœæ›²é–å¸‚ä¼šæ³½å¿ä¸ºä¾‹ï¼Œåˆ©ç”¨GF-1å«æ˜ŸWFVå½±åƒæ•°æ®ï¼Œå¼•å…¥äº†KD-VGGå’ŒKD-UNetæ¨¡å—è¿›è¡Œåˆæ­¥ç‰¹å¾æå–ï¼Œå¹¶æå‡ºäº†æ”¹è¿›å‹éšå¼æ‰©æ•£æ¨¡å‹ï¼ˆIIDMï¼‰ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼šï¼ˆ1ï¼‰VGGæ¨¡å—æ”¹è¿›äº†åˆå§‹ç‰¹å¾æå–ï¼Œæé«˜äº†ç²¾åº¦ï¼Œå¹¶ä¼˜åŒ–äº†æ¨¡å‹å‚æ•°ï¼Œå‡å°‘äº†æ¨ç†æ—¶é—´ã€‚ï¼ˆ2ï¼‰Cross-attention+MLPsæ¨¡å—å®ç°äº†æœ‰æ•ˆçš„ç‰¹å¾èåˆï¼Œå»ºç«‹äº†å…¨å±€å’Œå±€éƒ¨ç‰¹å¾ä¹‹é—´çš„å…³é”®å…³ç³»ï¼Œå®ç°äº†é«˜ç²¾åº¦ä¼°ç®—ã€‚ï¼ˆ3ï¼‰æ–°å‹è´¡çŒ®çš„IIDMæ¨¡å‹è¡¨ç°å‡ºæœ€é«˜çš„ä¼°ç®—ç²¾åº¦ï¼Œå‡æ–¹æ ¹è¯¯å·®ä¸º12.17%ï¼Œä¸å›å½’æ¨¡å‹ç›¸æ¯”ï¼Œæ”¹è¿›äº†41.69%è‡³42.33%ã€‚åœ¨ç¢³å‚¨é‡ä¼°ç®—æ–¹é¢ï¼Œç”Ÿæˆæ¨¡å‹åœ¨æå–æ·±å±‚ç‰¹å¾æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œè¯æ˜äº†äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹åœ¨å®šé‡é¥æ„Ÿä¸­çš„å¯è¡Œæ€§ã€‚16ç±³åˆ†è¾¨ç‡çš„ä¼°è®¡ä¸ºå®šåˆ¶æ£®æ—ç¢³æ±‡æ³•è§„ã€å¢å¼ºåŒºåŸŸç¢³å‚¨é‡ç®¡ç†æä¾›äº†åšå®çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17973v2">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶åˆ©ç”¨å…‰å­¦å›¾åƒå’Œé¥æ„ŸæŠ€æœ¯ï¼Œå¯¹äº‘å—çœæ›²é–å¸‚ä¼šæ³½å¿çš„æ£®æ—ç¢³å‚¨é‡è¿›è¡Œäº†ä¼°ç®—ã€‚ç ”ç©¶å¼•å…¥äº†KD-VGGå’ŒKD-UNetæ¨¡å—è¿›è¡Œåˆæ­¥ç‰¹å¾æå–ï¼Œå¹¶æå‡ºäº†æ”¹è¿›å‹éšå¼æ‰©æ•£æ¨¡å‹ï¼ˆIIDMï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨æ£®æ—ç¢³å‚¨é‡ä¼°ç®—ä¸­å…·æœ‰è¾ƒé«˜å‡†ç¡®æ€§ï¼Œä¸ºåˆ¶å®šæ£®æ—ç¢³æ±‡æ”¿ç­–æä¾›äº†åšå®åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ£®æ—æ˜¯å‡å°‘å¤§æ°”ä¸­äºŒæ°§åŒ–ç¢³æµ“åº¦å’Œç¼“è§£æ°”å€™å˜åŒ–çš„é‡è¦æœºåˆ¶ã€‚</li>
<li>é¥æ„ŸæŠ€æœ¯ä¸ºå¤§è§„æ¨¡è§‚å¯Ÿæä¾›äº†é«˜åº¦å‡†ç¡®çš„æ•°æ®ã€‚</li>
<li>å…‰å­¦å›¾åƒå¯¹äºé•¿æœŸæ£®æ—ç¢³å‚¨é‡ç›‘æµ‹è‡³å…³é‡è¦ã€‚</li>
<li>KD-VGGå’ŒKD-UNetæ¨¡å—çš„å¼•å…¥æ”¹è¿›äº†åˆæ­¥ç‰¹å¾æå–ï¼Œæé«˜äº†ä¼°ç®—ç²¾åº¦å¹¶ç¼©çŸ­äº†æ¨ç†æ—¶é—´ã€‚</li>
<li>Cross-attention + MLPsæ¨¡å—å®ç°äº†æœ‰æ•ˆçš„ç‰¹å¾èåˆï¼Œå»ºç«‹äº†å…¨å±€å’Œå±€éƒ¨ç‰¹å¾ä¹‹é—´çš„å…³é”®å…³ç³»ã€‚</li>
<li>æ”¹è¿›å‹éšå¼æ‰©æ•£æ¨¡å‹ï¼ˆIIDMï¼‰åœ¨æ£®æ—ç¢³å‚¨é‡ä¼°ç®—ä¸­è¡¨ç°å‡ºæœ€é«˜ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17973">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a48f7b6fcc67efef55b75dfc310d1017.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Convergence-of-Diffusion-Models-Under-the-Manifold-Hypothesis-in-High-Dimensions"><a href="#Convergence-of-Diffusion-Models-Under-the-Manifold-Hypothesis-in-High-Dimensions" class="headerlink" title="Convergence of Diffusion Models Under the Manifold Hypothesis in   High-Dimensions"></a>Convergence of Diffusion Models Under the Manifold Hypothesis in   High-Dimensions</h2><p><strong>Authors:Iskander Azangulov, George Deligiannidis, Judith Rousseau</strong></p>
<p>Denoising Diffusion Probabilistic Models (DDPM) are powerful state-of-the-art methods used to generate synthetic data from high-dimensional data distributions and are widely used for image, audio, and video generation as well as many more applications in science and beyond. The \textit{manifold hypothesis} states that high-dimensional data often lie on lower-dimensional manifolds within the ambient space, and is widely believed to hold in provided examples. While recent results have provided invaluable insight into how diffusion models adapt to the manifold hypothesis, they do not capture the great empirical success of these models, making this a very fruitful research direction.   In this work, we study DDPMs under the manifold hypothesis and prove that they achieve rates independent of the ambient dimension in terms of score learning. In terms of sampling complexity, we obtain rates independent of the ambient dimension w.r.t. the Kullback-Leibler divergence, and $O(\sqrt{D})$ w.r.t. the Wasserstein distance. We do this by developing a new framework connecting diffusion models to the well-studied theory of extrema of Gaussian Processes. </p>
<blockquote>
<p>å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰æ˜¯ç›®å‰ç”¨äºä»é«˜ç»´æ•°æ®åˆ†å¸ƒç”Ÿæˆåˆæˆæ•°æ®çš„æœ€å…ˆè¿›æ–¹æ³•ä¹‹ä¸€ï¼Œå¹¿æ³›ç”¨äºå›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç”Ÿæˆä»¥åŠç§‘å­¦å’Œå…¶ä»–æ›´å¤šé¢†åŸŸã€‚<em>æµå½¢å‡è®¾</em>æŒ‡å‡ºï¼Œé«˜ç»´æ•°æ®é€šå¸¸ä½äºç¯å¢ƒç©ºé—´ä¸­çš„ä½ç»´æµå½¢ä¸Šï¼Œå¹¶ä¸”åœ¨æä¾›çš„ç¤ºä¾‹ä¸­æ™®éè®¤ä¸ºå­˜åœ¨ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶ç»“æœæä¾›äº†å…³äºæ‰©æ•£æ¨¡å‹å¦‚ä½•é€‚åº”æµå½¢å‡è®¾çš„å®è´µè§è§£ï¼Œä½†å®ƒä»¬å¹¶æ²¡æœ‰æ•æ‰åˆ°è¿™äº›æ¨¡å‹çš„å·¨å¤§ç»éªŒæˆå°±ï¼Œè¿™ä½¿å¾—è¿™æˆä¸ºä¸€ä¸ªéå¸¸æœ‰æˆæœçš„ç ”ç©¶æ–¹å‘ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åœ¨æµå½¢å‡è®¾ä¸‹ç ”ç©¶DDPMï¼Œå¹¶è¯æ˜å…¶åœ¨å¾—åˆ†å­¦ä¹ æ–¹é¢å®ç°äº†ç‹¬ç«‹äºç¯å¢ƒç»´åº¦çš„é€Ÿç‡ã€‚åœ¨é‡‡æ ·å¤æ‚åº¦æ–¹é¢ï¼Œæˆ‘ä»¬è·å¾—äº†ç‹¬ç«‹äºç¯å¢ƒç»´åº¦çš„é€Ÿç‡ï¼Œå…³äºKullback-Leibleræ•£åº¦ä»¥åŠå…³äºWassersteinè·ç¦»çš„$O(\sqrt{D})$ã€‚æˆ‘ä»¬é€šè¿‡å¼€å‘å°†æ‰©æ•£æ¨¡å‹ä¸å¹¿å—æ¬¢è¿çš„Gaussian Processesæå€¼ç†è®ºç›¸è”ç³»çš„æ–°æ¡†æ¶æ¥å®ç°è¿™ä¸€ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18804v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰çš„å¼ºå¤§èƒ½åŠ›ï¼Œå®ƒå·²æˆä¸ºç”Ÿæˆé«˜ç»´æ•°æ®åˆ†å¸ƒåˆæˆæ•°æ®çš„æœ€å…ˆè¿›æ–¹æ³•ä¹‹ä¸€ï¼Œå¹¿æ³›åº”ç”¨äºå›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç”Ÿæˆä»¥åŠç§‘å­¦å’Œå…¶ä»–é¢†åŸŸã€‚æœ¬ç ”ç©¶åœ¨æµå½¢å‡è®¾ä¸‹ç ”ç©¶DDPMï¼Œè¯æ˜å…¶åœ¨è¯„åˆ†å­¦ä¹ æ–¹é¢å®ç°äº†ä¸ç¯å¢ƒç»´åº¦æ— å…³çš„æ¯”ç‡ã€‚åœ¨é‡‡æ ·å¤æ‚æ€§æ–¹é¢ï¼Œæˆ‘ä»¬è·å¾—äº†ä¸ç¯å¢ƒç»´åº¦æ— å…³çš„å…³äºKullback-Leibleræ•£åº¦çš„æ¯”ç‡ï¼Œä»¥åŠå…³äºWassersteinè·ç¦»çš„O(âˆšD)æ¯”ç‡ã€‚æˆ‘ä»¬é€šè¿‡å¼€å‘å°†æ‰©æ•£æ¨¡å‹ä¸é«˜æ–¯è¿‡ç¨‹æå€¼ç†è®ºç ”ç©¶ç›¸è¿çš„æ–°æ¡†æ¶æ¥å®ç°è¿™ä¸€ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DDPMæ˜¯ç”Ÿæˆé«˜ç»´æ•°æ®åˆ†å¸ƒåˆæˆæ•°æ®çš„å…ˆè¿›æ–¹æ³•ä¹‹ä¸€ï¼Œå¹¿æ³›åº”ç”¨äºå¤šä¸ªé¢†åŸŸã€‚</li>
<li>æµå½¢å‡è®¾æŒ‡å‡ºé«˜ç»´æ•°æ®ç»å¸¸ä½äºç¯å¢ƒç©ºé—´ä¸­çš„ä½ç»´æµå½¢ä¸Šï¼ŒDDPMåœ¨æ­¤å‡è®¾ä¸‹è¿›è¡Œäº†ç ”ç©¶ã€‚</li>
<li>DDPMåœ¨è¯„åˆ†å­¦ä¹ æ–¹é¢å®ç°äº†ä¸ç¯å¢ƒç»´åº¦æ— å…³çš„æ¯”ç‡ã€‚</li>
<li>åœ¨é‡‡æ ·å¤æ‚æ€§æ–¹é¢ï¼ŒDDPMè·å¾—äº†ä¸ç¯å¢ƒç»´åº¦æ— å…³çš„å…³äºKullback-Leibleræ•£åº¦çš„æ¯”ç‡ã€‚</li>
<li>å…³äºWassersteinè·ç¦»ï¼ŒDDPMçš„æ¯”ç‡æ˜¯O(âˆšD)ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡å¼€å‘æ–°çš„æ¡†æ¶å°†æ‰©æ•£æ¨¡å‹ä¸é«˜æ–¯è¿‡ç¨‹æå€¼ç†è®ºç›¸è¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ebebf4e18250e65e1fa8b3bb24c270e2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Contrastive-Learning-with-Synthetic-Positives"><a href="#Contrastive-Learning-with-Synthetic-Positives" class="headerlink" title="Contrastive Learning with Synthetic Positives"></a>Contrastive Learning with Synthetic Positives</h2><p><strong>Authors:Dewen Zeng, Yawen Wu, Xinrong Hu, Xiaowei Xu, Yiyu Shi</strong></p>
<p>Contrastive learning with the nearest neighbor has proved to be one of the most efficient self-supervised learning (SSL) techniques by utilizing the similarity of multiple instances within the same class. However, its efficacy is constrained as the nearest neighbor algorithm primarily identifies â€œeasyâ€ positive pairs, where the representations are already closely located in the embedding space. In this paper, we introduce a novel approach called Contrastive Learning with Synthetic Positives (CLSP) that utilizes synthetic images, generated by an unconditional diffusion model, as the additional positives to help the model learn from diverse positives. Through feature interpolation in the diffusion model sampling process, we generate images with distinct backgrounds yet similar semantic content to the anchor image. These images are considered â€œhardâ€ positives for the anchor image, and when included as supplementary positives in the contrastive loss, they contribute to a performance improvement of over 2% and 1% in linear evaluation compared to the previous NNCLR and All4One methods across multiple benchmark datasets such as CIFAR10, achieving state-of-the-art methods. On transfer learning benchmarks, CLSP outperforms existing SSL frameworks on 6 out of 8 downstream datasets. We believe CLSP establishes a valuable baseline for future SSL studies incorporating synthetic data in the training process. </p>
<blockquote>
<p>å¯¹æ¯”å­¦ä¹ ç»“åˆæœ€è¿‘é‚»å·²è¢«è¯æ˜æ˜¯ä¸€ç§éå¸¸æœ‰æ•ˆçš„è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æŠ€æœ¯ï¼Œå®ƒåˆ©ç”¨åŒä¸€ç±»åˆ«å†…å¤šä¸ªå®ä¾‹çš„ç›¸ä¼¼æ€§ã€‚ç„¶è€Œï¼Œå…¶æ•ˆæœå—åˆ°é™åˆ¶ï¼Œå› ä¸ºæœ€è¿‘é‚»ç®—æ³•ä¸»è¦è¯†åˆ«çš„æ˜¯â€œå®¹æ˜“â€çš„æ­£å¯¹ï¼Œè¿™äº›æ­£å¯¹çš„è¡¨ç¤ºåœ¨åµŒå…¥ç©ºé—´ä¸­å·²ç»ä½äºç›¸è¿‘ä½ç½®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ç§°ä¸ºåˆæˆé˜³æ€§å¯¹æ¯”å­¦ä¹ ï¼ˆCLSPï¼‰çš„æ–°æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åˆæˆå›¾åƒä½œä¸ºé¢å¤–çš„æ­£æ ·æœ¬ï¼Œå¸®åŠ©æ¨¡å‹ä»å¤šæ ·åŒ–çš„æ­£æ ·æœ¬ä¸­å­¦ä¹ ã€‚åœ¨æ‰©æ•£æ¨¡å‹é‡‡æ ·è¿‡ç¨‹ä¸­çš„ç‰¹å¾æ’å€¼ä¸­ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†å…·æœ‰ä¸åŒèƒŒæ™¯ä½†è¯­ä¹‰å†…å®¹ä¸é”šå›¾åƒç›¸ä¼¼çš„å›¾åƒã€‚è¿™äº›å›¾åƒè¢«è®¤ä¸ºæ˜¯é”šå›¾åƒçš„â€œç¡¬â€æ­£æ ·æœ¬ï¼Œå½“å®ƒä»¬ä½œä¸ºå¯¹æ¯”æŸå¤±ä¸­çš„é™„åŠ æ­£æ ·æœ¬æ—¶ï¼Œä¸ä¹‹å‰çš„NNCLRå’ŒAll4Oneæ–¹æ³•ç›¸æ¯”ï¼Œåœ¨CIFAR10ç­‰å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†è¶…è¿‡2%å’Œ1%çš„çº¿æ€§è¯„ä¼°æ€§èƒ½æå‡ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚åœ¨è¿ç§»å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCLSPåœ¨8ä¸ªä¸‹æ¸¸æ•°æ®é›†ä¸­çš„6ä¸ªä¸Šè¶…è¶Šäº†ç°æœ‰çš„SSLæ¡†æ¶ã€‚æˆ‘ä»¬ç›¸ä¿¡CLSPä¸ºæœªæ¥çš„SSLç ”ç©¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èå…¥åˆæˆæ•°æ®å»ºç«‹äº†æœ‰ä»·å€¼çš„åŸºå‡†çº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16965v2">PDF</a> 8 pages, conference</p>
<p><strong>Summary</strong></p>
<p>å¯¹æ¯”å­¦ä¹ é€šè¿‡æœ€è¿‘é‚»æ–¹æ³•åœ¨è‡ªæˆ‘ç›‘ç£å­¦ä¹ ä¸­å±•ç°äº†å‡ºè‰²çš„æ•ˆç‡ï¼Œå…¶ä¸»è¦åˆ©ç”¨åŒä¸€ç±»åˆ«å®ä¾‹é—´çš„ç›¸ä¼¼æ€§ã€‚ç„¶è€Œï¼Œç”±äºæœ€è¿‘é‚»ç®—æ³•ä¸»è¦è¯†åˆ«â€œå®¹æ˜“â€çš„æ­£æ ·æœ¬å¯¹ï¼Œå…¶æœ‰æ•ˆæ€§å—åˆ°é™åˆ¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œå¯¹æ¯”å­¦ä¹ ä¸åˆæˆæ­£æ ·æœ¬ï¼ˆCLSPï¼‰â€çš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨ç”±æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åˆæˆå›¾åƒä½œä¸ºé¢å¤–çš„æ­£æ ·æœ¬ï¼Œå¸®åŠ©æ¨¡å‹ä»å¤šæ ·åŒ–çš„æ­£æ ·æœ¬ä¸­å­¦ä¹ ã€‚é€šè¿‡æ‰©æ•£æ¨¡å‹é‡‡æ ·è¿‡ç¨‹ä¸­çš„ç‰¹å¾æ’å€¼ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†å…·æœ‰ä¸åŒèƒŒæ™¯ä½†è¯­ä¹‰å†…å®¹ä¸é”šå›¾åƒç›¸ä¼¼çš„å›¾åƒã€‚è¿™äº›å›¾åƒè¢«è§†ä¸ºé”šå›¾åƒçš„â€œç¡¬â€æ­£æ ·æœ¬ï¼Œå¹¶ä½œä¸ºå¯¹æ¯”æŸå¤±ä¸­çš„è¡¥å……æ­£æ ·æœ¬åŒ…å«åœ¨å†…ï¼Œä»è€Œæé«˜äº†æ€§èƒ½ã€‚ç›¸è¾ƒäºå…ˆå‰çš„NNCLRå’ŒAll4Oneæ–¹æ³•ï¼ŒCLSPåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ï¼ˆå¦‚CIFAR10ï¼‰ä¸Šçš„çº¿æ€§è¯„ä¼°æ€§èƒ½æé«˜äº†è¶…è¿‡2%å’Œ1%ï¼Œå¹¶ä¸”åœ¨è¿ç§»å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCLSPåœ¨å…­ä¸ªä¸‹æ¸¸æ•°æ®é›†ä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰SSLæ¡†æ¶ã€‚æˆ‘ä»¬ç›¸ä¿¡CLSPä¸ºæœªæ¥çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ ä¸­èå…¥åˆæˆæ•°æ®çš„ç ”ç©¶å»ºç«‹äº†æœ‰ä»·å€¼çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”å­¦ä¹ é€šè¿‡æœ€è¿‘é‚»æ–¹æ³•æ˜¯ä¸€ç§æœ‰æ•ˆçš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œä½†å—é™äºè¯†åˆ«â€œå®¹æ˜“â€æ­£æ ·æœ¬çš„èƒ½åŠ›ã€‚</li>
<li>æœ¬æ–‡æå‡ºCLSPæ–¹æ³•ï¼Œåˆ©ç”¨åˆæˆå›¾åƒä½œä¸ºé¢å¤–çš„æ­£æ ·æœ¬ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡æ‰©æ•£æ¨¡å‹çš„ç‰¹å¾æ’å€¼ç”Ÿæˆåˆæˆå›¾åƒï¼Œè¿™äº›å›¾åƒå…·æœ‰ä¸åŒçš„èƒŒæ™¯ä½†ç›¸ä¼¼çš„è¯­ä¹‰å†…å®¹ã€‚</li>
<li>åˆæˆå›¾åƒè¢«è§†ä¸ºâ€œç¡¬â€æ­£æ ·æœ¬ï¼Œæœ‰åŠ©äºæ¨¡å‹åœ¨å¯¹æ¯”å­¦ä¹ ä¸­æ›´æœ‰æ•ˆåœ°å­¦ä¹ ã€‚</li>
<li>CLSPåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¶…è¿‡ä¹‹å‰çš„NNCLRå’ŒAll4Oneæ–¹æ³•ã€‚</li>
<li>åœ¨è¿ç§»å­¦ä¹ æµ‹è¯•ä¸­ï¼ŒCLSPåœ¨å¤šä¸ªä¸‹æ¸¸æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„SSLæ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.16965">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f6254ca7c208559f3fdb699a40ba13f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29c769a4df288296c66d68d1fc49c453.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Lumina-mGPT-Illuminate-Flexible-Photorealistic-Text-to-Image-Generation-with-Multimodal-Generative-Pretraining"><a href="#Lumina-mGPT-Illuminate-Flexible-Photorealistic-Text-to-Image-Generation-with-Multimodal-Generative-Pretraining" class="headerlink" title="Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation   with Multimodal Generative Pretraining"></a>Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation   with Multimodal Generative Pretraining</h2><p><strong>Authors:Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yi Xin, Xinyue Li, Qi Qin, Yu Qiao, Hongsheng Li, Peng Gao</strong></p>
<p>We present Lumina-mGPT, a family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions. By initializing from multimodal Generative PreTraining (mGPT), we demonstrate that decoder-only Autoregressive (AR) model can achieve image generation performance comparable to modern diffusion models with high efficiency through Flexible Progressive Supervised Fine-tuning (FP-SFT). Equipped with our proposed Unambiguous image Representation (UniRep), Lumina-mGPT can flexibly generate high-quality images of varying aspect ratios. Building on the strong image generation capabilities, we further explore Ominiponent Supervised Fine-tuning (Omni-SFT), an initial attempt to elevate Lumina-mGPT into a unified multi-modal generalist. The resulting model demonstrates versatile multimodal capabilities, including visual generation tasks like text-to-image&#x2F;multiview generation and controllable generation, visual recognition tasks like segmentation and depth estimation, and vision-language tasks like multi-turn visual question answering, showing the rosy potential of the technical direction. Codes and checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/Alpha-VLLM/Lumina-mGPT">https://github.com/Alpha-VLLM/Lumina-mGPT</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Lumina-mGPTï¼Œè¿™æ˜¯ä¸€ç³»åˆ—å¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹ï¼Œèƒ½å¤Ÿæ‰§è¡Œå„ç§è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆçµæ´»çš„å…‰æ …å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚é€šè¿‡é‡‡ç”¨å¤šæ¨¡æ€ç”Ÿæˆé¢„è®­ç»ƒï¼ˆmGPTï¼‰è¿›è¡Œåˆå§‹åŒ–ï¼Œæˆ‘ä»¬è¯æ˜äº†è§£ç å™¨ä»…æœ‰çš„è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹å¯ä»¥é€šè¿‡çµæ´»çš„æ¸è¿›å¼ç›‘ç£å¾®è°ƒï¼ˆFP-SFTï¼‰å®ç°ä¸ç°ä»£æ‰©æ•£æ¨¡å‹ç›¸å½“çš„é«˜æ•ˆå›¾åƒç”Ÿæˆæ€§èƒ½ã€‚é…å¤‡äº†æˆ‘ä»¬æå‡ºçš„æ˜ç¡®å›¾åƒè¡¨ç¤ºï¼ˆUniRepï¼‰ï¼ŒLumina-mGPTå¯ä»¥çµæ´»åœ°ç”Ÿæˆå„ç§çºµæ¨ªæ¯”çš„é«˜è´¨é‡å›¾åƒã€‚åœ¨å¼ºå¤§çš„å›¾åƒç”Ÿæˆèƒ½åŠ›çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†å…¨èƒ½ç›‘ç£å¾®è°ƒï¼ˆOmni-SFTï¼‰ï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•å°†Lumina-mGPTæå‡ä¸ºç»Ÿä¸€çš„å¤šæ¨¡æ€é€šæ‰ã€‚ç»“æœè¯æ˜è¯¥æ¨¡å‹å…·æœ‰å¤šåŠŸèƒ½çš„å¤šæ¨¡æ€èƒ½åŠ›ï¼ŒåŒ…æ‹¬è§†è§‰ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ°å›¾åƒ&#x2F;å¤šè§†å›¾ç”Ÿæˆå’Œå¯æ§ç”Ÿæˆï¼‰ã€è§†è§‰è¯†åˆ«ä»»åŠ¡ï¼ˆå¦‚åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ï¼‰ä»¥åŠè§†è§‰è¯­è¨€ä»»åŠ¡ï¼ˆå¦‚å¤šè½®è§†è§‰é—®ç­”ï¼‰ï¼Œå±•ç¤ºäº†è¯¥æŠ€æœ¯æ–¹å‘çš„å¹¿é˜”æ½œåŠ›ã€‚ç›¸å…³ä»£ç å’Œæ£€æŸ¥ç‚¹å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/Alpha-VLLM/Lumina-mGPT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Alpha-VLLM/Lumina-mGPTè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.02657v3">PDF</a> Code available at: <a target="_blank" rel="noopener" href="https://github.com/Alpha-VLLM/Lumina-mGPT">https://github.com/Alpha-VLLM/Lumina-mGPT</a></p>
<p><strong>Summary</strong></p>
<p>Lumina-mGPTæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹å®¶æ—ï¼Œæ“…é•¿æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆçµæ´»é€¼çœŸçš„å›¾åƒã€‚é€šè¿‡å¤šæ¨¡æ€ç”Ÿæˆé¢„è®­ç»ƒï¼ˆmGPTï¼‰åˆå§‹åŒ–ï¼Œå¹¶å€ŸåŠ©çµæ´»æ¸è¿›ç›‘ç£å¾®è°ƒï¼ˆFP-SFTï¼‰å’Œé«˜æ•ˆç‡çš„æ— æ­§ä¹‰å›¾åƒè¡¨ç¤ºï¼ˆUniRepï¼‰ï¼ŒLumina-mGPTèƒ½å¤Ÿçµæ´»ç”Ÿæˆé«˜è´¨é‡ã€ä¸åŒæ¯”ä¾‹å°ºçš„å›¾åƒã€‚è¯¥æ¨¡å‹è¿˜èƒ½æ‰§è¡Œå¤šç§è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°å›¾åƒ&#x2F;å¤šè§†å›¾ç”Ÿæˆã€å¯æ§ç”Ÿæˆã€åˆ†å‰²ã€æ·±åº¦ä¼°è®¡å’Œå¤šè½®è§†è§‰é—®ç­”ç­‰ã€‚ç›¸å…³ä»£ç å’Œæ£€æŸ¥ç‚¹å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Lumina-mGPTæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹ï¼Œèƒ½å¤„ç†å¤šç§è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ã€‚</li>
<li>æ¨¡å‹èƒ½ä»æ–‡æœ¬æè¿°ä¸­ç”Ÿæˆçµæ´»çš„é€¼çœŸå›¾åƒã€‚</li>
<li>é€šè¿‡å¤šæ¨¡æ€ç”Ÿæˆé¢„è®­ç»ƒï¼ˆmGPTï¼‰åˆå§‹åŒ–ï¼Œå®ç°äº†é«˜æ•ˆçš„å›¾åƒç”Ÿæˆæ€§èƒ½ã€‚</li>
<li>å€ŸåŠ©çµæ´»æ¸è¿›ç›‘ç£å¾®è°ƒï¼ˆFP-SFTï¼‰ï¼Œæ¨¡å‹çš„æ€§èƒ½ä¸ç°ä»£æ‰©æ•£æ¨¡å‹ç›¸å½“ã€‚</li>
<li>æ— æ­§ä¹‰å›¾åƒè¡¨ç¤ºï¼ˆUniRepï¼‰ä½¿æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€ä¸åŒæ¯”ä¾‹å°ºçš„å›¾åƒã€‚</li>
<li>æ¨¡å‹å…·å¤‡å¤šç§è§†è§‰ç”Ÿæˆä»»åŠ¡èƒ½åŠ›ï¼Œå¦‚æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€å¯æ§ç”Ÿæˆç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.02657">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d696d56990814d409ebb405b3ff0667f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a36eeccc0ac22139d093e977d319ad0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d99b6daa18c59136b99c2cd515997c27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-318ef0e021430f0d8624dc5990ef9b27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3439256919e1cea1f7203273b38fbdce.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-26/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-26/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-26/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d4ff568aadca0897bbf49396281e914e.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-26  Self-Supervised Noise Adaptive MRI Denoising via Repetition to   Repetition (Rep2Rep) Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-26/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e05c1ff695a317fab1e81c4116828eb7.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-26  CasualHDRSplat Robust High Dynamic Range 3D Gaussian Splatting from   Casually Captured Videos
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17862.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
