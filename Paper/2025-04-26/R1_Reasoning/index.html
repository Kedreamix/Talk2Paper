<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-04-26  DeepDistill Enhancing LLM Reasoning Capabilities via Large-Scale   Difficulty-Graded Data Training">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1935b781154349fb0918b181e49aa7d7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-04-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    32 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-04-26-更新"><a href="#2025-04-26-更新" class="headerlink" title="2025-04-26 更新"></a>2025-04-26 更新</h1><h2 id="DeepDistill-Enhancing-LLM-Reasoning-Capabilities-via-Large-Scale-Difficulty-Graded-Data-Training"><a href="#DeepDistill-Enhancing-LLM-Reasoning-Capabilities-via-Large-Scale-Difficulty-Graded-Data-Training" class="headerlink" title="DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale   Difficulty-Graded Data Training"></a>DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale   Difficulty-Graded Data Training</h2><p><strong>Authors:Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Yunjie Ji, Han Zhao, Xiangang Li</strong></p>
<p>Although large language models (LLMs) have recently achieved remarkable performance on various complex reasoning benchmarks, the academic community still lacks an in-depth understanding of base model training processes and data quality. To address this, we construct a large-scale, difficulty-graded reasoning dataset containing approximately 3.34 million unique queries of varying difficulty levels and about 40 million distilled responses generated by multiple models over several passes. Leveraging pass rate and Coefficient of Variation (CV), we precisely select the most valuable training data to enhance reasoning capability. Notably, we observe a training pattern shift, indicating that reasoning-focused training based on base models requires higher learning rates for effective training. Using this carefully selected data, we significantly improve the reasoning capabilities of the base model, achieving a pass rate of 79.2% on the AIME2024 mathematical reasoning benchmark. This result surpasses most current distilled models and closely approaches state-of-the-art performance. We provide detailed descriptions of our data processing, difficulty assessment, and training methodology, and have publicly released all datasets and methods to promote rapid progress in open-source long-reasoning LLMs. The dataset is available at: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M">https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M</a> </p>
<blockquote>
<p>尽管大型语言模型（LLM）最近在各种复杂的推理基准测试中取得了显著的性能，但学术界仍然缺乏对基础模型训练过程和数据质量的深入了解。为了解决这个问题，我们构建了一个大规模、难度分级的推理数据集，包含约334万个独特查询和大约40万个蒸馏响应，这些响应是由多个模型经过多次迭代生成的。我们利用通过率和变异系数（CV）精确选择最有价值的训练数据，以提高推理能力。值得注意的是，我们观察到训练模式的转变，这表明基于基础模型的推理导向训练需要更高的学习率才能进行有效训练。使用这些精心挑选的数据，我们显著提高了基础模型的推理能力，在AIME2024数学推理基准测试上的通过率达到79.2%。这一结果超过了大多数当前的蒸馏模型，并接近最新技术性能。我们提供了关于数据处理、难度评估和培训方法的详细描述，并已公开发布所有数据集和方法，以促进开源长推理LLM的快速发展。数据集可在：<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M">https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M</a> 获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17565v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在复杂推理基准测试上表现出卓越性能，但仍缺乏对基础模型训练过程和数据质量深入的理解。为此，我们构建了一个大规模、难度分级的推理数据集，包含约33万独特查询和大约数千万次精炼后的回复。通过利用通过率与变异系数（CV），我们精确筛选出最有价值的训练数据以提升推理能力。我们发现训练模式发生转变，基于基础模型的推理训练需要更高的学习率才能有效训练。使用这些数据集，我们显著提升了基础模型的推理能力，在AIME 2024数学推理基准测试中达到79.2%的通过率，超越大多数现有蒸馏模型并接近最新技术水平。我们公开了所有数据集和方法，以促进开源长推理LLMs的快速发展。数据集可通过：<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M">链接地址</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>构建了一个大规模、难度分级的推理数据集，旨在深入理解大型语言模型的训练过程和数据质量对推理能力的影响。</li>
<li>通过率和变异系数被用来评估和筛选训练数据的有效性，用于提升模型的推理能力。</li>
<li>研究发现，与基础模型的推理训练相比，需要更高的学习率才能有效训练模型。</li>
<li>使用此数据集显著提升了基础模型的推理能力，在AIME 2024数学推理基准测试中取得了较高的通过率。</li>
<li>此研究成果超越大多数现有蒸馏模型并接近最新技术水平。</li>
<li>所有数据集和方法已公开，以促进该领域的快速发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17565">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-440dee1fa035bcf8da74a5fc55aba2d3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="RefVNLI-Towards-Scalable-Evaluation-of-Subject-driven-Text-to-image-Generation"><a href="#RefVNLI-Towards-Scalable-Evaluation-of-Subject-driven-Text-to-image-Generation" class="headerlink" title="RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image   Generation"></a>RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image   Generation</h2><p><strong>Authors:Aviv Slobodkin, Hagai Taitelbaum, Yonatan Bitton, Brian Gordon, Michal Sokolik, Nitzan Bitton Guetta, Almog Gueta, Royi Rassin, Itay Laish, Dani Lischinski, Idan Szpektor</strong></p>
<p>Subject-driven text-to-image (T2I) generation aims to produce images that align with a given textual description, while preserving the visual identity from a referenced subject image. Despite its broad downstream applicability – ranging from enhanced personalization in image generation to consistent character representation in video rendering – progress in this field is limited by the lack of reliable automatic evaluation. Existing methods either assess only one aspect of the task (i.e., textual alignment or subject preservation), misalign with human judgments, or rely on costly API-based evaluation. To address this, we introduce RefVNLI, a cost-effective metric that evaluates both textual alignment and subject preservation in a single prediction. Trained on a large-scale dataset derived from video-reasoning benchmarks and image perturbations, RefVNLI outperforms or matches existing baselines across multiple benchmarks and subject categories (e.g., \emph{Animal}, \emph{Object}), achieving up to 6.4-point gains in textual alignment and 8.5-point gains in subject consistency. It also excels with lesser-known concepts, aligning with human preferences at over 87% accuracy. </p>
<blockquote>
<p>主题驱动的文本到图像（T2I）生成旨在根据给定的文本描述生成图像，同时保留参考主题图像的可视化标识。尽管其在下游应用的广泛性从增强图像生成的个性化到视频渲染中的一致字符表示都适用，但此领域的进展却受限于可靠的自动评估的缺乏。现有方法只评估任务的某一方面（如文本对齐或主题保留），与人类判断不符，或依赖于成本高昂的基于API的评估。为解决此问题，我们引入了RefVNLI，这是一种经济高效的指标，可以在一次预测中对文本对齐和主题保留进行评估。RefVNLI经过视频推理基准测试和图像扰动衍生的大规模数据集的训练，在多个基准测试和主题类别（如“动物”、“物体”）中的表现优于或匹配现有基线，在文本对齐方面实现了高达6.4点的增益，在主题一致性方面实现了高达8.5点的增益。此外，在处理较为陌生的概念时，它以超过87%的准确率与人类偏好保持一致。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17502v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本到图像生成的任务旨在根据给定的文本描述生成图像，同时保留参考主体图像的可视身份。尽管该技术在个性化图像生成、视频渲染中的角色一致性等方面有广泛的应用前景，但其进展受限于缺乏可靠的自动评估方法。为解决这一问题，我们推出了RefVNLI，这是一种经济高效的评估指标，可以在一次预测中对文本对齐和主题保留进行评估。该指标经过大规模数据集训练，来源于视频推理基准测试和图像扰动，在多个基准测试和主题类别中表现优异，实现了文本对齐高达6.4点的增益和主题一致性高达8.5点的增益。它还能很好地处理较为陌生的概念，与人类偏好对齐的准确率超过87%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到图像生成任务旨在根据文本描述生成图像，同时保留参考图像的主体身份。</li>
<li>该领域的进展受限于缺乏可靠的自动评估方法。</li>
<li>现有评估方法往往只评估任务的一个方面，与人类判断不一致，且依赖于昂贵的API评估。</li>
<li>RefVNLI是一种经济高效的评估指标，可以同时评估文本对齐和主题保留。</li>
<li>RefVNLI在多个基准测试和主题类别中表现优异，实现了文本对齐和主题一致性的显著增益。</li>
<li>RefVNLI处理较为陌生的概念时表现良好，与人类偏好对齐的准确率超过87%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17502">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9b8c489d7bcc9b86e6e962dbb53b11bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc47404e4035a901e18a9dac548c0966.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1935b781154349fb0918b181e49aa7d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc6656c5a1c1e86a6b2848de11e00b0f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-59f4e66fa5033ef25a17f4a3480dad5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ec355885488a101d2fc35160973703c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a515a4c16d910545106e629932cdfa1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7175d069acd24b22a53acccb826afe7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Perspective-Aware-Reasoning-in-Vision-Language-Models-via-Mental-Imagery-Simulation"><a href="#Perspective-Aware-Reasoning-in-Vision-Language-Models-via-Mental-Imagery-Simulation" class="headerlink" title="Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery   Simulation"></a>Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery   Simulation</h2><p><strong>Authors:Phillip Y. Lee, Jihyeon Je, Chanho Park, Mikaela Angelina Uy, Leonidas Guibas, Minhyuk Sung</strong></p>
<p>We present a framework for perspective-aware reasoning in vision-language models (VLMs) through mental imagery simulation. Perspective-taking, the ability to perceive an environment or situation from an alternative viewpoint, is a key benchmark for human-level visual understanding, essential for environmental interaction and collaboration with autonomous agents. Despite advancements in spatial reasoning within VLMs, recent research has shown that modern VLMs significantly lack perspective-aware reasoning capabilities and exhibit a strong bias toward egocentric interpretations. To bridge the gap between VLMs and human perception, we focus on the role of mental imagery, where humans perceive the world through abstracted representations that facilitate perspective shifts. Motivated by this, we propose a framework for perspective-aware reasoning, named Abstract Perspective Change (APC), that effectively leverages vision foundation models, such as object detection, segmentation, and orientation estimation, to construct scene abstractions and enable perspective transformations. Our experiments on synthetic and real-image benchmarks, compared with various VLMs, demonstrate significant improvements in perspective-aware reasoning with our framework, further outperforming fine-tuned spatial reasoning models and novel-view-synthesis-based approaches. </p>
<blockquote>
<p>我们提出了一种通过心理图像模拟在视觉语言模型（VLM）中进行视角感知推理的框架。视角感知能力是从不同角度感知环境或情境的能力，是人类视觉理解的关键基准，对于环境交互以及与自主实体的协作至关重要。尽管VLM在视觉推理方面有所进步，但最新研究表明，现代VLM严重缺乏视角感知推理能力，并表现出强烈的以自我为中心的解读偏见。为了弥补VLM与人类感知之间的差距，我们关注心理图像的作用，人类通过抽象的表示来感知世界，这有助于视角变化。基于此，我们提出了一个名为抽象视角变化（APC）的视角感知推理框架，它有效地利用视觉基础模型，如目标检测、分割和方位估计来构建场景抽象，实现视角转换。我们在合成和真实图像基准测试上的实验与各种VLM相比，证明了我们的框架在视角感知推理方面的显著改进，进一步超越了微调的空间推理模型和基于新视图合成的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17207v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://apc-vlm.github.io/">https://apc-vlm.github.io/</a></p>
<p><strong>Summary</strong><br>基于人类心智模拟，我们提出了一种面向视觉语言模型（VLMs）的视角感知推理框架。该框架强调视角转换的重要性，能够模拟人类从不同角度感知环境和情境的能力。通过利用先进的视觉基础模型如目标检测、分割和方位估计等，构建场景抽象并启用视角转换，该框架显著提高了VLMs在视角感知推理方面的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>介绍了视角感知推理在视觉语言模型中的重要性。</li>
<li>强调了模拟人类心智的重要性，以从不同角度理解环境和情境。</li>
<li>利用先进的视觉基础模型构建场景抽象。</li>
<li>提出的Abstract Perspective Change（APC）框架能有效提高VLMs在视角感知推理方面的能力。</li>
<li>在合成和真实图像基准测试上的实验证明APC框架的优越性。</li>
<li>APC框架相比其他空间推理模型和基于新视角合成的方法表现出更好的性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17207">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b915cbfb0ec6eb19fdddf0781f9da142.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9475581e23339f09045698d9f237e75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76ab0850ad6c5980209bf5ccf4fd95c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37c5115ecc6ed9fdc7b17793402bd77a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7aefc1a8f7cb5bd8d150904e2d09a0b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c80b80153f6fc6f68810763e0953be1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Nemotron-CrossThink-Scaling-Self-Learning-beyond-Math-Reasoning"><a href="#Nemotron-CrossThink-Scaling-Self-Learning-beyond-Math-Reasoning" class="headerlink" title="Nemotron-CrossThink: Scaling Self-Learning beyond Math Reasoning"></a>Nemotron-CrossThink: Scaling Self-Learning beyond Math Reasoning</h2><p><strong>Authors:Syeda Nahida Akter, Shrimai Prabhumoye, Matvei Novikov, Seungju Han, Ying Lin, Evelina Bakhturina, Eric Nyberg, Yejin Choi, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro</strong></p>
<p>Large Language Models (LLMs) have shown strong reasoning capabilities, particularly when enhanced through Reinforcement Learning (RL). While prior work has successfully applied RL to mathematical reasoning – where rules and correctness are well-defined – generalizing these methods to broader reasoning domains remains challenging due to limited data, the lack of verifiable reward structures, and diverse task requirements. In this work, we propose NEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain corpora, including both synthetic and real-world question-answer pairs, into RL training to improve generalization across diverse reasoning tasks. NEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from varied sources spanning STEM, humanities, social sciences, etc.; (2) applying structured templates (e.g., multiple-choice and open-ended) to control answer-space complexity; (3) filtering for verifiable answers; and (4) optimizing data blending strategies that utilizes data from multiple sources effectively. Our approach enables scalable and verifiable reward modeling beyond mathematics and demonstrates improved accuracies on both math (MATH-500: +30.1%, AMC23:+27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%, GPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover, NEMOTRON-CROSSTHINK exhibits significantly improved response efficiency – using 28% fewer tokens for correct answers – highlighting more focused and effective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that integrating multi-domain, multi-format data in RL leads to more accurate, efficient, and generalizable LLMs. </p>
<blockquote>
<p>大型语言模型（LLM）展现出强大的推理能力，特别是在通过强化学习（RL）增强的情况下。尽管先前的工作已成功将RL应用于数学推理——规则和正确性定义明确——但由于数据有限、可验证的奖励结构缺失以及多样化的任务要求，将这些方法推广到更广泛的推理领域仍然具有挑战性。在这项工作中，我们提出了NEMOTRON-CROSSTHINK框架，该框架系统地结合了多领域语料库，包括合成和现实世界的问题答案对，用于RL训练，以提高在不同推理任务上的泛化能力。NEMOTRON-CROSSTHINK通过以下关键挑战：（1）融入涵盖STEM、人文、社会科学等领域的多元数据来源；（2）应用结构化模板（如选择题和开放式问题）来控制答案空间的复杂性；（3）筛选可验证的答案；（4）优化数据混合策略，有效利用多源数据。我们的方法能够在数学之外实现可扩展和可验证的奖励建模，并在数学（MATH-500：+30.1%，AMC23：+27.5%）和非数学推理基准测试（MMLU-PRO：+12.8%，GPQA-DIAMOND：+11.3%，AGIEVAL：+15.1%，SUPERGPQA：+3.8%）上显示出更高的准确性。此外，NEMOTRON-CROSSTHINK的响应效率显著提高——正确答案使用的令牌减少了28%，突显出更集中、更有效的推理。通过NEMOTRON-CROSSTHINK，我们证明了在RL中整合多领域、多格式数据会导致更准确、高效和可推广的LLM。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13941v2">PDF</a> 18 pages, 7 figures</p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLMs）通过强化学习（RL）展现出强大的推理能力。尽管先前的工作已成功将RL应用于数学推理，但将这些方法推广到更广泛的推理领域仍然具有挑战性，因为存在数据有限、可验证的奖励结构缺乏以及任务要求多样化等问题。本研究提出NEMOTRON-CROSSTHINK框架，通过融入多领域语料库，包括合成和现实世界的问题答案对，来改善RL训练在多样化推理任务中的泛化能力。NEMOTRON-CROSSTHINK通过以下方式解决关键挑战：（1）融入涵盖STEM、人文、社会科学等领域的多元数据来源；（2）应用结构化的模板（如选择题和开放性问题）来控制答案空间的复杂性；（3）筛选可验证的答案；（4）优化数据混合策略，有效利用多来源数据。我们的方法使奖励模型能够在数学之外进行扩展和验证，并在数学（MATH-500：+30.1%，AMC23：+27.5%）和非数学推理基准测试（MMLU-PRO：+12.8%，GPQA-DIAMOND：+11.3%，AGIEVAL：+15.1%，SUPERGPQA：+3.8%）上显示出更高的准确性。此外，NEMOTRON-CROSSTHINK的响应效率显著提高，正确答案使用的令牌减少了28%，显示出更加集中和有效的推理能力。通过NEMOTRON-CROSSTHINK，我们证明了在RL中整合多领域、多格式数据会导致更准确、高效和可泛化的LLMs。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型（LLMs）通过强化学习（RL）展现出强大的推理能力，特别是在多领域推理方面。</li>
<li>NEMOTRON-CROSSTHINK框架通过融入多领域语料库，包括合成和现实世界的问题答案对，来改善RL训练在多样化推理任务中的泛化能力。</li>
<li>NEMOTRON-CROSSTHINK解决了将强化学习应用于更广泛推理领域的关键挑战，包括数据有限、可验证奖励结构的缺乏以及任务要求的多样化。</li>
<li>NEMOTRON-CROSSTHINK通过融入多元数据来源、应用结构化模板、筛选可验证答案以及优化数据混合策略等方法来提高LLMs的准确性和效率。</li>
<li>NEMOTRON-CROSSTHINK在多个数学和非数学推理基准测试上表现出更高的准确性，并且在响应效率上显著提高。</li>
<li>NEMOTRON-CROSSTHINK方法使得奖励模型能够扩展到数学领域之外，并且能够对非数学推理任务进行验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13941">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0c15b0028bf70f31228c98f3d66f2d39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3fe186f528135c2a02915541714230a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7728e11fcdcfd2d0ea86564ef83fd0da.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c3fb5f2d740bd181cd509b9eb7cc8968.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CheatAgent-Attacking-LLM-Empowered-Recommender-Systems-via-LLM-Agent"><a href="#CheatAgent-Attacking-LLM-Empowered-Recommender-Systems-via-LLM-Agent" class="headerlink" title="CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent"></a>CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent</h2><p><strong>Authors:Liang-bo Ning, Shijie Wang, Wenqi Fan, Qing Li, Xin Xu, Hao Chen, Feiran Huang</strong></p>
<p>Recently, Large Language Model (LLM)-empowered recommender systems (RecSys) have brought significant advances in personalized user experience and have attracted considerable attention. Despite the impressive progress, the research question regarding the safety vulnerability of LLM-empowered RecSys still remains largely under-investigated. Given the security and privacy concerns, it is more practical to focus on attacking the black-box RecSys, where attackers can only observe the system’s inputs and outputs. However, traditional attack approaches employing reinforcement learning (RL) agents are not effective for attacking LLM-empowered RecSys due to the limited capabilities in processing complex textual inputs, planning, and reasoning. On the other hand, LLMs provide unprecedented opportunities to serve as attack agents to attack RecSys because of their impressive capability in simulating human-like decision-making processes. Therefore, in this paper, we propose a novel attack framework called CheatAgent by harnessing the human-like capabilities of LLMs, where an LLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our method first identifies the insertion position for maximum impact with minimal input modification. After that, the LLM agent is designed to generate adversarial perturbations to insert at target positions. To further improve the quality of generated perturbations, we utilize the prompt tuning technique to improve attacking strategies via feedback from the victim RecSys iteratively. Extensive experiments across three real-world datasets demonstrate the effectiveness of our proposed attacking method. </p>
<blockquote>
<p>最近，大型语言模型（LLM）赋能的推荐系统（RecSys）在个性化用户体验方面取得了显著进展，并引起了广泛关注。尽管取得了令人印象深刻的进步，但关于LLM赋能的RecSys的安全漏洞的研究问题仍然在很大程度上被忽视。考虑到安全和隐私的担忧，更实际的是关注攻击黑盒RecSys，攻击者只能观察系统的输入和输出。然而，采用强化学习（RL）代理的传统攻击方法由于处理复杂文本输入、规划和推理的能力有限，因此攻击LLM赋能的RecSys并不有效。另一方面，LLM由于在模拟人类决策过程方面表现出色，提供了作为攻击RecSys代理的空前机会。因此，本文提出了一种新的攻击框架，名为CheatAgent，该框架利用LLM的人类化能力，开发了一个基于LLM的代理来攻击LLM赋能的RecSys。具体来说，我们的方法首先确定插入位置，以在最小输入修改的情况下实现最大影响。然后，LLM代理被设计成在目标位置插入对抗性扰动。为了进一步提高生成扰动的质量，我们利用提示调整技术，通过来自受害者RecSys的反馈来改进攻击策略。在三个真实数据集上的大量实验表明了我们提出的攻击方法的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13192v2">PDF</a> Accepted by KDD 2024;</p>
<p><strong>Summary</strong>：<br>最近，大型语言模型（LLM）赋能的推荐系统（RecSys）在个性化用户体验方面取得了显著进展，并引起了广泛关注。然而，关于LLM赋能的RecSys的安全漏洞的研究问题仍然被大大忽视。考虑到安全和隐私问题，更实际的是关注攻击黑箱RecSys，攻击者只能观察系统的输入和输出。本文提出了一种新的攻击框架，名为CheatAgent，利用LLM的人类化能力来攻击LLM赋能的RecSys。通过识别最大影响的最小输入修改位置，LLM代理生成对抗性扰动并插入目标位置。利用提示调整技术，通过受害RecSys的反馈迭代改进攻击策略。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>LLM赋能的推荐系统（RecSys）在个性化体验方面取得显著进展，但安全漏洞问题亟待研究。</li>
<li>传统采用强化学习（RL）的攻击方法对于LLM赋能的RecSys效果不佳。</li>
<li>LLM具有模拟人类决策过程的强大能力，可服务于攻击RecSys。</li>
<li>提出了一种新的攻击框架CheatAgent，利用LLM代理攻击LLM赋能的RecSys。</li>
<li>CheatAgent通过识别最大影响的最小输入修改位置来实施攻击。</li>
<li>利用提示调整技术通过受害RecSys的反馈改进攻击策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13192">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-775288518611024853cab145a144f22b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d51821e0080fffa8a4f5d94567282240.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c6ed2d611c426ca35592c79d4696ec3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="GeoSense-Evaluating-Identification-and-Application-of-Geometric-Principles-in-Multimodal-Reasoning"><a href="#GeoSense-Evaluating-Identification-and-Application-of-Geometric-Principles-in-Multimodal-Reasoning" class="headerlink" title="GeoSense: Evaluating Identification and Application of Geometric   Principles in Multimodal Reasoning"></a>GeoSense: Evaluating Identification and Application of Geometric   Principles in Multimodal Reasoning</h2><p><strong>Authors:Liangyu Xu, Yingxiu Zhao, Jingyun Wang, Yingyao Wang, Bu Pi, Chen Wang, Mingliang Zhang, Jihao Gu, Xiang Li, Xiaoyong Zhu, Jun Song, Bo Zheng</strong></p>
<p>Geometry problem-solving (GPS), a challenging task requiring both visual comprehension and symbolic reasoning, effectively measures the reasoning capabilities of multimodal large language models (MLLMs). Humans exhibit strong reasoning ability in this task through accurate identification and adaptive application of geometric principles within visual contexts. However, existing benchmarks fail to jointly assess both dimensions of the human-like geometric reasoning mechanism in MLLMs, remaining a critical gap in assessing their ability to tackle GPS. To this end, we introduce GeoSense, the first comprehensive bilingual benchmark designed to systematically evaluate the geometric reasoning abilities of MLLMs through the lens of geometric principles. GeoSense features a five-level hierarchical framework of geometric principles spanning plane and solid geometry, an intricately annotated dataset of 1,789 problems, and an innovative evaluation strategy. Through extensive experiments on GeoSense with various open-source and closed-source MLLMs, we observe that Gemini-2.0-pro-flash performs best, achieving an overall score of $65.3$. Our in-depth analysis reveals that the identification and application of geometric principles remain a bottleneck for leading MLLMs, jointly hindering their reasoning abilities. These findings underscore GeoSense’s potential to guide future advancements in MLLMs’ geometric reasoning capabilities, paving the way for more robust and human-like reasoning in artificial intelligence. </p>
<blockquote>
<p>几何问题解决（GPS）是一项具有挑战性的任务，要求视觉理解和符号推理，能有效衡量多模态大型语言模型（MLLMs）的推理能力。人类在此任务中展现出强大的推理能力，能在视觉环境中准确识别并灵活应用几何原理。然而，现有的基准测试未能同时评估人类类似的几何推理机制在MLLMs中的两个维度，这在评估MLLMs处理GPS的能力时存在关键差距。为此，我们引入了GeoSense，这是首个全面的双语基准测试，旨在通过几何原理的视角，系统地评估MLLMs的几何推理能力。GeoSense具有涵盖平面和立体几何的五级分层几何原理框架、经过精心注释的1789个问题集和创新的评估策略。通过在GeoSense上与各种开源和闭源的MLLMs进行广泛实验，我们发现Gemini-2.0-pro-flash表现最佳，总体得分为65.3分。我们的深入分析表明，几何原理的识别和应用仍然是领先MLLMs的瓶颈，共同制约了它们的推理能力。这些发现突出了GeoSense在指导未来MLLMs几何推理能力进步方面的潜力，为人工智能中更稳健、更符合人类推理方式的开发奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12597v2">PDF</a> 10 pages, 8 figures</p>
<p><strong>Summary</strong>：引入了一个综合的双语基准测试GeoSense，旨在系统地评估多模态大型语言模型（MLLMs）的几何推理能力。通过广泛的实验，观察到Gemini-2.0-pro-flash表现最佳，达到总体得分65.3。分析表明，几何原则的识别和应用仍是MLLMs的瓶颈。GeoSense有望指导MLLMs在几何推理能力方面的未来发展，为人工智能带来更稳健和人性化的推理。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>几何问题解决（GPS）是评估推理能力的重要任务，涉及视觉理解和符号推理。</li>
<li>人类在GPS任务中通过准确识别和适应性地应用几何原则展示强大的推理能力。</li>
<li>现有基准测试未能联合评估MLLMs的人类式几何推理机制的两个方面。</li>
<li>GeoSense是首个全面的双语基准测试，旨在评估MLLMs的几何推理能力。</li>
<li>GeoSense具有涵盖平面和立体几何的五个层次的原则框架、精心注释的问题集和创新评估策略。</li>
<li>在GeoSense上进行的实验表明，Gemini-2.0-pro-flash表现最佳，总体得分为65.3。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12597">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-62264de97f2e94ba8434ea2ba284aaa0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1248f789a0525ec8f4a0da82c7b3ce0f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dc2cb82d62d4f765bd5f967cd7ed1646.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be16d49e97b7b4b82cab328c2f4189a9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a209b682d46f01bfbd7b332c7049eaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fa74d74ca62fb103cf841e68d0a1906.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Teaching-Large-Language-Models-to-Reason-through-Learning-and-Forgetting"><a href="#Teaching-Large-Language-Models-to-Reason-through-Learning-and-Forgetting" class="headerlink" title="Teaching Large Language Models to Reason through Learning and Forgetting"></a>Teaching Large Language Models to Reason through Learning and Forgetting</h2><p><strong>Authors:Tianwei Ni, Allen Nie, Sapana Chaudhary, Yao Liu, Huzefa Rangwala, Rasool Fakoor</strong></p>
<p>Leveraging inference-time search in large language models has proven effective in further enhancing a trained model’s capability to solve complex mathematical and reasoning problems. However, this approach significantly increases computational costs and inference time, as the model must generate and evaluate multiple candidate solutions to identify a viable reasoning path. To address this, we propose an effective approach that integrates search capabilities directly into the model by fine-tuning it using both successful (learning) and failed reasoning paths (forgetting) derived from diverse search methods. While fine-tuning the model with these data might seem straightforward, we identify a critical issue: the model’s search capability tends to degrade rapidly if fine-tuning is performed naively. We show that this degradation can be substantially mitigated by employing a smaller learning rate. Extensive experiments on the challenging Game-of-24 and Countdown mathematical reasoning benchmarks show that our approach not only outperforms both standard fine-tuning and inference-time search baselines but also significantly reduces inference time by 180$\times$. </p>
<blockquote>
<p>利用大型语言模型中的推理时间搜索，已被证明可以进一步增强训练模型解决复杂数学和推理问题的能力。然而，这种方法显著增加了计算成本和推理时间，因为模型必须生成并评估多个候选解决方案来识别可行的推理路径。为了解决这个问题，我们提出了一种有效的方法，通过微调模型直接集成搜索能力，使用来自不同搜索方法的成功（学习）和失败推理路径（遗忘）。虽然用这些数据微调模型看似简单，但我们发现了一个关键问题：如果盲目进行微调，模型的搜索能力往往会迅速下降。我们表明，通过采用较小的学习率，可以大大缓解这种退化。在具有挑战性的24点游戏和倒计时数学推理基准测试的大量实验表明，我们的方法不仅优于标准微调方法和推理时间搜索基准测试，而且通过减少推理时间高达180倍，实现了显著的性能提升。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11364v2">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/twni2016/llm-reasoning-uft">https://github.com/twni2016/llm-reasoning-uft</a></p>
<p><strong>Summary</strong></p>
<p>大型语言模型中引入推理时间搜索可增强模型解决复杂数学和推理问题的能力，但计算成本和推理时间显著增加。为解决这个问题，本文提出了一种将搜索能力直接集成到模型中的方法，通过精细调整模型，使用成功和失败的推理路径数据。虽然看似简单的模型调整却存在关键问题，即如果进行简单的调整则模型的搜索能力会迅速下降。本文展示了一种解决方案，即通过减小学习率可以大幅缓解性能下降问题。实验证明，该方法不仅优于标准精细调整和推理时间搜索基线，还能显著减少推理时间。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入推理时间搜索可以增强大型语言模型的解决复杂数学和推理问题的能力。</li>
<li>这种方法会增加计算成本和推理时间。</li>
<li>通过精细调整模型并集成搜索能力，可以利用成功和失败的推理路径数据。</li>
<li>简单的模型精细调整可能导致模型的搜索能力迅速下降。</li>
<li>通过减小学习率可以显著缓解模型性能的下降。</li>
<li>实验证明该方法在Game-of-24和Countdown数学推理基准测试中表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11364">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-680b4394d313ea443864aea7a3431b62.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c501d76cb21910ad64c9e86f93fe7ace.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-41adb84ed68e74a63a8994dc0aace7f8.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Digital-Twin-Buildings-3D-Modeling-GIS-Integration-and-Visual-Descriptions-Using-Gaussian-Splatting-ChatGPT-Deepseek-and-Google-Maps-Platform"><a href="#Digital-Twin-Buildings-3D-Modeling-GIS-Integration-and-Visual-Descriptions-Using-Gaussian-Splatting-ChatGPT-Deepseek-and-Google-Maps-Platform" class="headerlink" title="Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual   Descriptions Using Gaussian Splatting, ChatGPT&#x2F;Deepseek, and Google Maps   Platform"></a>Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual   Descriptions Using Gaussian Splatting, ChatGPT&#x2F;Deepseek, and Google Maps   Platform</h2><p><strong>Authors:Kyle Gao, Dening Lu, Liangzhi Li, Nan Chen, Hongjie He, Linlin Xu, Jonathan Li</strong></p>
<p>Urban digital twins are virtual replicas of cities that use multi-source data and data analytics to optimize urban planning, infrastructure management, and decision-making. Towards this, we propose a framework focused on the single-building scale. By connecting to cloud mapping platforms such as Google Map Platforms APIs, by leveraging state-of-the-art multi-agent Large Language Models data analysis using ChatGPT(4o) and Deepseek-V3&#x2F;R1, and by using our Gaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings framework can retrieve a building’s 3D model, visual descriptions, and achieve cloud-based mapping integration with large language model-based data analytics using a building’s address, postal code, or geographic coordinates. </p>
<blockquote>
<p>城市数字双胞胎是利用多源数据和数据分析优化城市规划、基础设施管理和决策制定的城市虚拟副本。为此，我们提出了以单体建筑为尺度的框架。通过连接到谷歌地图平台API等云地图平台，利用最新的多智能体大型语言模型（使用ChatGPT（第4版）和Deepseek-V3&#x2F;R1进行数据分析），以及基于高斯溅渍的网格提取管道，我们的数字双胞胎建筑框架可以检索建筑的3D模型和视觉描述，实现基于云的地图集成与以建筑地址、邮政编码或地理坐标的大型语言模型数据分析。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05769v3">PDF</a> -Fixed minor typo</p>
<p><strong>Summary</strong><br>城市数字双胞胎是城市的虚拟副本，利用多源数据和数据分析优化城市规划、基础设施管理和决策制定。我们提出一个专注于单体建筑尺度的框架，通过连接云计算平台、利用先进的多智能体大型语言模型数据分析技术，实现建筑的三维模型获取、视觉描述以及与基于大型语言模型的云计算集成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>城市数字双胞胎是城市的虚拟副本，用于优化城市规划、基础设施管理和决策制定。</li>
<li>提出的框架专注于单体建筑尺度。</li>
<li>通过连接云计算平台，如Google Map Platforms APIs，获取建筑数据。</li>
<li>利用多智能体大型语言模型数据分析技术，如ChatGPT(4o)和Deepseek-V3&#x2F;R1，进行分析。</li>
<li>采用高斯插值法基于网格提取管道技术，实现建筑的三维模型获取和视觉描述。</li>
<li>框架能够实现与基于大型语言模型的云计算集成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05769">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-62dacfbdeefb0f9fc0dd3a79766f2a5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5ecc6427fbe8bec581b6686f010decd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-12b2f95e8e829eb8767ae6fb471e782f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3d2b3b35ffaaa212c18e54dab7e23cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b6a47cec1f153dfa2ff4d795c8b069c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d5743d9c232a5d49f6fef2e78990f76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6763660a6a78dec49ec6480d4cd9dd1f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-26/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-26/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-26/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ac18c8327b85eb1266e60f8882f07ace.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-04-26  The Sparse Frontier Sparse Attention Trade-offs in Transformer LLMs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-25/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ad4fd38429316b081ae30de4107501c9.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-04-25  FREAK Frequency-modulated High-fidelity and Real-time Audio-driven   Talking Portrait Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23154.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
