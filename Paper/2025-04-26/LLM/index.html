<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-26  The Sparse Frontier Sparse Attention Trade-offs in Transformer LLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ac18c8327b85eb1266e60f8882f07ace.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-04-26
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-04-26-æ›´æ–°"><a href="#2025-04-26-æ›´æ–°" class="headerlink" title="2025-04-26 æ›´æ–°"></a>2025-04-26 æ›´æ–°</h1><h2 id="The-Sparse-Frontier-Sparse-Attention-Trade-offs-in-Transformer-LLMs"><a href="#The-Sparse-Frontier-Sparse-Attention-Trade-offs-in-Transformer-LLMs" class="headerlink" title="The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs"></a>The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs</h2><p><strong>Authors:Piotr Nawrot, Robert Li, Renjie Huang, Sebastian Ruder, Kelly Marchisio, Edoardo M. Ponti</strong></p>
<p>Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, we perform a careful comparison of training-free sparse attention methods at varying model scales, sequence lengths, and sparsity levels on a diverse collection of long-sequence tasks-including novel ones that rely on natural language while remaining controllable and easy to evaluate. Based on our experiments, we report a series of key findings: 1) an isoFLOPS analysis reveals that for very long sequences, larger and highly sparse models are preferable to smaller and dense ones. 2) The level of sparsity attainable while statistically guaranteeing accuracy preservation is higher during decoding than prefilling, and correlates with model size in the former. 3) There is no clear strategy that performs best across tasks and phases, with different units of sparsification or budget adaptivity needed for different scenarios. Even moderate sparsity levels often result in significant performance degradation on at least one task, highlighting that sparse attention is not a universal solution. 4) We introduce and validate novel scaling laws specifically tailored for sparse attention, providing evidence that our findings are likely to hold true beyond our range of experiments. Through these insights, we demonstrate that sparse attention is a key tool to enhance the capabilities of Transformer LLMs for processing longer sequences, but requires careful evaluation of trade-offs for performance-sensitive applications. </p>
<blockquote>
<p>ç¨€ç–æ³¨æ„åŠ›ä¸ºæ‰©å±•Transformerå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›æä¾›äº†æœ‰å‰æ™¯çš„ç­–ç•¥ï¼Œä½†å…¶å¯è¡Œæ€§ã€æ•ˆç‡ä¸å‡†ç¡®åº¦çš„æƒè¡¡ä»¥åŠç³»ç»Ÿæ‰©å±•ç ”ç©¶ä»æœªè¢«æ¢ç´¢ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬åœ¨ä¸€ç³»åˆ—é•¿åºåˆ—ä»»åŠ¡ä¸Šï¼Œå¯¹ä¸åŒæ¨¡å‹è§„æ¨¡ã€åºåˆ—é•¿åº¦å’Œç¨€ç–æ°´å¹³çš„è®­ç»ƒæ— å…³ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•è¿›è¡Œäº†ä»”ç»†æ¯”è¾ƒã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬ä¾èµ–è‡ªç„¶è¯­è¨€çš„æ–°å‹ä»»åŠ¡ï¼ŒåŒæ—¶ä¿æŒå¯æ§æ€§å’Œæ˜“äºè¯„ä¼°çš„ç‰¹ç‚¹ã€‚åŸºäºæˆ‘ä»¬çš„å®éªŒï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†ä¸€ç³»åˆ—é‡è¦å‘ç°ï¼š</p>
</blockquote>
<p>1ï¼‰isoFLOPSåˆ†æè¡¨æ˜ï¼Œå¯¹äºéå¸¸é•¿çš„åºåˆ—ï¼Œæ›´å¤§ä¸”é«˜åº¦ç¨€ç–çš„æ¨¡å‹ä¼˜äºè¾ƒå°ä¸”å¯†é›†çš„æ¨¡å‹ã€‚</p>
<p>2ï¼‰åœ¨è§£ç è¿‡ç¨‹ä¸­ï¼Œåœ¨ä¿è¯å‡†ç¡®æ€§ä¿ç•™çš„ç»Ÿè®¡æ°´å¹³ä¸Šï¼Œå¯è¾¾åˆ°çš„ç¨€ç–æ°´å¹³é«˜äºé¢„å¡«å……é˜¶æ®µï¼Œå¹¶ä¸”ä¸å‰è€…çš„æ¨¡å‹è§„æ¨¡ç›¸å…³ã€‚</p>
<p>3ï¼‰æ²¡æœ‰ä¸€ç§ç­–ç•¥åœ¨æ‰€æœ‰ä»»åŠ¡å’Œé˜¶æ®µä¸­è¡¨ç°æœ€ä½³ï¼Œä¸åŒçš„ç¨€ç–å•ä½æˆ–é¢„ç®—é€‚åº”æ€§ç­–ç•¥é€‚ç”¨äºä¸åŒåœºæ™¯ã€‚ç”šè‡³é€‚ä¸­çš„ç¨€ç–æ°´å¹³ä¹Ÿå¾€å¾€è‡³å°‘åœ¨ä¸€ä¸ªä»»åŠ¡ä¸Šå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè¿™è¡¨æ˜ç¨€ç–æ³¨æ„åŠ›å¹¶éä¸‡èƒ½è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17768v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶æ˜¯æ‰©å±•Transformerå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›çš„ä¸€ç§æœ‰å‰é€”çš„ç­–ç•¥ï¼Œä½†å…¶å¯è¡Œæ€§ã€æ•ˆç‡ä¸å‡†ç¡®åº¦çš„æƒè¡¡ä»¥åŠç³»ç»Ÿæ€§æ‰©å±•ç ”ç©¶ä»å¾…æ¢ç´¢ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹è®­ç»ƒæ— å…³çš„ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼ŒåŒ…æ‹¬åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ã€åºåˆ—é•¿åº¦å’Œç¨€ç–å±‚æ¬¡ä¸Šçš„å¤šæ ·é•¿åºåˆ—ä»»åŠ¡ï¼ˆåŒ…æ‹¬ä¾èµ–è‡ªç„¶è¯­è¨€çš„æ–°ä»»åŠ¡ï¼‰ã€‚åŸºäºå®éªŒï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†ä¸€ç³»åˆ—å…³é”®å‘ç°ï¼š1ï¼‰å¯¹äºæé•¿åºåˆ—ï¼Œæ›´å¤§ä¸”é«˜åº¦ç¨€ç–çš„æ¨¡å‹æ¯”å°è€Œå¯†é›†çš„æ¨¡å‹æ›´å¯å–ã€‚2ï¼‰åœ¨è§£ç æ—¶å¯è¾¾åˆ°çš„ä¿è¯å‡†ç¡®åº¦ä¿ç•™çš„ç¨€ç–æ°´å¹³é«˜äºé¢„å¡«å……æ—¶çš„æ°´å¹³ï¼Œå¹¶ä¸”åœ¨å‰è€…ä¸­ä¸æ¨¡å‹å¤§å°ç›¸å…³ã€‚3ï¼‰æ²¡æœ‰ä¸€ç§ç­–ç•¥èƒ½åœ¨æ‰€æœ‰ä»»åŠ¡å’Œé˜¶æ®µä¸­è¡¨ç°æœ€ä½³ï¼Œä¸åŒçš„ç¨€ç–åŒ–å•ä½æˆ–é¢„ç®—é€‚åº”æ€§é’ˆå¯¹ä¸åŒåœºæ™¯æ˜¯å¿…è¦çš„ã€‚å³ä½¿é€‚åº¦çš„ç¨€ç–æ°´å¹³ä¹Ÿå¾€å¾€å¯¼è‡´è‡³å°‘ä¸€é¡¹ä»»åŠ¡æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè¿™è¡¨æ˜ç¨€ç–æ³¨æ„åŠ›å¹¶ä¸æ˜¯ä¸‡èƒ½è§£å†³æ–¹æ¡ˆã€‚4ï¼‰æˆ‘ä»¬å¼•å…¥å¹¶éªŒè¯äº†ä¸“é—¨é’ˆå¯¹ç¨€ç–æ³¨æ„åŠ›çš„æ–°æ‰©å±•å®šå¾‹ï¼Œè¯æ˜æˆ‘ä»¬çš„å‘ç°å¯èƒ½åœ¨å®éªŒèŒƒå›´ä¹‹å¤–ä¹Ÿæˆç«‹ã€‚é€šè¿‡è¿™äº›è§è§£ï¼Œæˆ‘ä»¬è¯æ˜äº†ç¨€ç–æ³¨æ„åŠ›æ˜¯æé«˜Transformer LLMå¤„ç†é•¿åºåˆ—èƒ½åŠ›çš„é‡è¦å·¥å…·ï¼Œä½†å¯¹äºæ€§èƒ½æ•æ„Ÿçš„åº”ç”¨ï¼Œéœ€è¦ä»”ç»†è¯„ä¼°æƒè¡¡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¯¹äºå¤„ç†æé•¿åºåˆ—ï¼Œè€ƒè™‘æ¨¡å‹è§„æ¨¡å’Œç¨€ç–åº¦æ˜¯å…³é”®çš„ã€‚è¾ƒå¤§çš„ç¨€ç–æ¨¡å‹è¡¨ç°æ›´ä½³ã€‚</li>
<li>è§£ç æ—¶çš„ç¨€ç–æ°´å¹³å¯ä»¥æ›´é«˜ï¼Œå¹¶ä¸”ä¸æ¨¡å‹è§„æ¨¡æœ‰å…³ã€‚</li>
<li>æ²¡æœ‰ä¸€ç§æ˜ç¡®çš„æœ€ä½³ç­–ç•¥é€‚ç”¨äºæ‰€æœ‰ä»»åŠ¡å’Œé˜¶æ®µï¼Œéœ€è¦é’ˆå¯¹ç‰¹å®šåœºæ™¯è°ƒæ•´ç¨€ç–åŒ–ç­–ç•¥ã€‚</li>
<li>é€‚åº¦çš„ç¨€ç–æ°´å¹³å¯èƒ½ä¼šå¯¼è‡´æŸäº›ä»»åŠ¡æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå› æ­¤éœ€è°¨æ…ä½¿ç”¨ã€‚</li>
<li>ç¨€ç–æ³¨æ„åŠ›å¹¶éä¸‡èƒ½è§£å†³æ–¹æ¡ˆï¼Œéœ€è¦ç»“åˆå…¶ä»–æ–¹æ³•ä½¿ç”¨ã€‚</li>
<li>é’ˆå¯¹ç¨€ç–æ³¨æ„åŠ›çš„æ–°æ‰©å±•å®šå¾‹ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17768">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-402bd75c3669a9c9357257f36e5c30bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f69201b9ad78653b6d0dda256e7ab920.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-303bdb9023132d83f3d4741eec6025f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b276a2908ebc16fb4a2557cf56f8e29.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="L3-DIMM-PIM-Integrated-Architecture-and-Coordination-for-Scalable-Long-Context-LLM-Inference"><a href="#L3-DIMM-PIM-Integrated-Architecture-and-Coordination-for-Scalable-Long-Context-LLM-Inference" class="headerlink" title="L3: DIMM-PIM Integrated Architecture and Coordination for Scalable   Long-Context LLM Inference"></a>L3: DIMM-PIM Integrated Architecture and Coordination for Scalable   Long-Context LLM Inference</h2><p><strong>Authors:Qingyuan Liu, Liyan Chen, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen</strong></p>
<p>Large Language Models (LLMs) increasingly require processing long text sequences, but GPU memory limitations force difficult trade-offs between memory capacity and bandwidth. While HBM-based acceleration offers high bandwidth, its capacity remains constrained. Offloading data to host-side DIMMs improves capacity but introduces costly data swapping overhead. We identify that the critical memory bottleneck lies in the decoding phase of multi-head attention (MHA) exclusively, which demands substantial capacity for storing KV caches and high bandwidth for attention computation. Our key insight reveals this operation uniquely aligns with modern DIMM-based processing-in-memory (PIM) architectures, which offers scalability of both capacity and bandwidth.   Based on this observation and insight, we propose L3, a hardware-software co-designed system integrating DIMM-PIM and GPU devices. L3 introduces three innovations: First, hardware redesigns resolve data layout mismatches and computational element mismatches in DIMM-PIM, enhancing LLM inference utilization. Second, communication optimization enables hiding the data transfer overhead with the computation. Third, an adaptive scheduler coordinates GPU-DIMM-PIM operations to maximize parallelism between devices. Evaluations using real-world traces show L3 achieves up to 6.1$\times$ speedup over state-of-the-art HBM-PIM solutions while significantly improving batch sizes. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¶Šæ¥è¶Šéœ€è¦å¤„ç†é•¿æ–‡æœ¬åºåˆ—ï¼Œä½†GPUå†…å­˜é™åˆ¶ä½¿å¾—åœ¨å†…å­˜å®¹é‡å’Œå¸¦å®½ä¹‹é—´åšå‡ºè‰°éš¾çš„æƒè¡¡ã€‚è™½ç„¶åŸºäºHBMçš„åŠ é€Ÿæä¾›äº†é«˜å¸¦å®½ï¼Œä½†å…¶å®¹é‡ä»ç„¶å—é™ã€‚å°†æ•°æ®è¿ç§»åˆ°ä¸»æœºä¾§DIMMså¯ä»¥æé«˜å®¹é‡ï¼Œä½†å¼•å…¥äº†æ˜‚è´µçš„æ•°æ®äº¤æ¢å¼€é”€ã€‚æˆ‘ä»¬ç¡®å®šå…³é”®çš„å†…å­˜ç“¶é¢ˆä»…å­˜åœ¨äºå¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰çš„è§£ç é˜¶æ®µï¼Œè¯¥é˜¶æ®µéœ€è¦å¤§é‡å­˜å‚¨ç©ºé—´æ¥å­˜å‚¨KVç¼“å­˜å’Œç”¨äºæ³¨æ„åŠ›è®¡ç®—çš„é«˜å¸¦å®½ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œè¿™ä¸€æ“ä½œä¸ç°ä»£DIMMsåŸºäºå†…å­˜å¤„ç†ï¼ˆPIMï¼‰çš„æ¶æ„å®Œå…¨å¥‘åˆï¼Œè¯¥æ¶æ„åœ¨å®¹é‡å’Œå¸¦å®½æ–¹é¢æä¾›äº†å¯æ‰©å±•æ€§ã€‚åŸºäºè¿™ä¸€è§‚å¯ŸåŠ›å’Œæ´å¯ŸåŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†L3ç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†DIMM-PIMå’ŒGPUè®¾å¤‡çš„è½¯ç¡¬ä»¶ååŒè®¾è®¡ç³»ç»Ÿã€‚L3å¼•å…¥äº†ä¸‰é¡¹åˆ›æ–°ï¼šé¦–å…ˆï¼Œç¡¬ä»¶é‡æ–°è®¾è®¡è§£å†³äº†DIMM-PIMä¸­çš„æ•°æ®å¸ƒå±€ä¸åŒ¹é…å’Œè®¡ç®—å…ƒç´ ä¸åŒ¹é…çš„é—®é¢˜ï¼Œæé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†çš„åˆ©ç”¨ç‡ï¼›å…¶æ¬¡ï¼Œé€šä¿¡ä¼˜åŒ–èƒ½å¤Ÿéšè—æ•°æ®ä¼ è¾“è¿‡ç¨‹ä¸­çš„è®¡ç®—å¼€é”€ï¼›ç¬¬ä¸‰ï¼Œè‡ªé€‚åº”è°ƒåº¦å™¨åè°ƒGPU-DIMM-PIMæ“ä½œï¼Œä»¥æœ€å¤§é™åº¦åœ°æé«˜è®¾å¤‡ä¹‹é—´çš„å¹¶è¡Œæ€§ã€‚ä½¿ç”¨çœŸå®ä¸–ç•Œæ•°æ®çš„è¯„ä¼°è¡¨æ˜ï¼ŒL3ä¸å½“å‰å…ˆè¿›çš„HBM-PIMè§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼Œé€Ÿåº¦æé«˜äº†é«˜è¾¾6.1å€ï¼Œå¹¶ä¸”æ˜¾è‘—æé«˜äº†æ‰¹å¤„ç†è§„æ¨¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17584v1">PDF</a> 16 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤„ç†é•¿æ–‡æœ¬åºåˆ—æ—¶ï¼Œå­˜åœ¨GPUå†…å­˜é™åˆ¶çš„é—®é¢˜ï¼Œè¿™éœ€è¦åœ¨å†…å­˜å®¹é‡å’Œå¸¦å®½ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚æ–‡ç« æŒ‡å‡ºå…³é”®ç“¶é¢ˆåœ¨äºå¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰çš„è§£ç é˜¶æ®µï¼Œéœ€è¦å¤§é‡å†…å­˜ç¼“å­˜å’Œé«˜å¸¦å®½è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚æ–‡ç« ç»“åˆç°ä»£DIMMå†…å­˜ä¸­çš„å¤„ç†ï¼ˆPIMï¼‰æ¶æ„ï¼Œæå‡ºäº†ä¸€ä¸ªç¡¬ä»¶å’Œè½¯ä»¶ååŒè®¾è®¡çš„ç³»ç»ŸL3ï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†DIMM-PIMå’ŒGPUè®¾å¤‡ã€‚L3é€šè¿‡ç¡¬ä»¶é‡æ–°è®¾è®¡ã€é€šä¿¡ä¼˜åŒ–å’Œè‡ªé€‚åº”è°ƒåº¦å™¨ä¸‰ä¸ªåˆ›æ–°ç‚¹ï¼Œå®ç°äº†å¯¹LLMæ¨ç†çš„é«˜æ•ˆåˆ©ç”¨ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„HBM-PIMè§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼ŒL3çš„é€Ÿåº¦æé«˜äº†é«˜è¾¾6.1å€ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†æ‰¹æ¬¡å¤§å°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤„ç†é•¿æ–‡æœ¬åºåˆ—æ—¶é¢ä¸´GPUå†…å­˜é™åˆ¶çš„æŒ‘æˆ˜ã€‚</li>
<li>å…³é”®ç“¶é¢ˆåœ¨äºå¤šå¤´æ³¨æ„åŠ›ï¼ˆMHAï¼‰çš„è§£ç é˜¶æ®µï¼Œéœ€è¦å¤§é‡å†…å­˜ç¼“å­˜å’Œé«˜å¸¦å®½ã€‚</li>
<li>ç°ä»£DIMM-PIMæ¶æ„ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜æä¾›äº†å¯èƒ½æ€§ï¼Œç»“åˆäº†å®¹é‡å’Œå¸¦å®½çš„å¯æ‰©å±•æ€§ã€‚</li>
<li>L3ç³»ç»Ÿç»“åˆäº†DIMM-PIMå’ŒGPUè®¾å¤‡ï¼Œé€šè¿‡ç¡¬ä»¶å’Œè½¯ä»¶ååŒè®¾è®¡æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>L3é€šè¿‡ç¡¬ä»¶é‡æ–°è®¾è®¡ã€é€šä¿¡ä¼˜åŒ–å’Œè‡ªé€‚åº”è°ƒåº¦å™¨ä¸‰ä¸ªåˆ›æ–°ç‚¹æ¥ä¼˜åŒ–æ€§èƒ½ã€‚</li>
<li>L3å®ç°äº†å¯¹LLMæ¨ç†çš„é«˜æ•ˆåˆ©ç”¨ï¼Œè¯„ä¼°ç»“æœè¡¨æ˜å…¶é€Ÿåº¦æ¯”ç°æœ‰è§£å†³æ–¹æ¡ˆæ›´å¿«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17584">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-498cfe3ffea219d74be472db47edde1d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-87c07283314a3ffaa1200d3f8663392f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3af370daaffea65094618873274fc96c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de51d25fa6ec71c7e93c3fd93152c0c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfcbe77e037356a3dfc81e5ef87b6db0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2fa93a31c378a90f4a3e16e7b2c6a8fd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DeepDistill-Enhancing-LLM-Reasoning-Capabilities-via-Large-Scale-Difficulty-Graded-Data-Training"><a href="#DeepDistill-Enhancing-LLM-Reasoning-Capabilities-via-Large-Scale-Difficulty-Graded-Data-Training" class="headerlink" title="DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale   Difficulty-Graded Data Training"></a>DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale   Difficulty-Graded Data Training</h2><p><strong>Authors:Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Yunjie Ji, Han Zhao, Xiangang Li</strong></p>
<p>Although large language models (LLMs) have recently achieved remarkable performance on various complex reasoning benchmarks, the academic community still lacks an in-depth understanding of base model training processes and data quality. To address this, we construct a large-scale, difficulty-graded reasoning dataset containing approximately 3.34 million unique queries of varying difficulty levels and about 40 million distilled responses generated by multiple models over several passes. Leveraging pass rate and Coefficient of Variation (CV), we precisely select the most valuable training data to enhance reasoning capability. Notably, we observe a training pattern shift, indicating that reasoning-focused training based on base models requires higher learning rates for effective training. Using this carefully selected data, we significantly improve the reasoning capabilities of the base model, achieving a pass rate of 79.2% on the AIME2024 mathematical reasoning benchmark. This result surpasses most current distilled models and closely approaches state-of-the-art performance. We provide detailed descriptions of our data processing, difficulty assessment, and training methodology, and have publicly released all datasets and methods to promote rapid progress in open-source long-reasoning LLMs. The dataset is available at: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M">https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M</a> </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ€è¿‘åœ¨å„ç§å¤æ‚çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†å­¦æœ¯ç•Œä»ç„¶ç¼ºä¹å¯¹åŸºç¡€æ¨¡å‹è®­ç»ƒè¿‡ç¨‹å’Œæ•°æ®è´¨é‡çš„æ·±å…¥äº†è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€éš¾åº¦åˆ†çº§çš„æ¨ç†æ•°æ®é›†ï¼ŒåŒ…å«å¤§çº¦334ä¸‡ä¸ªä¸åŒéš¾åº¦çº§åˆ«çš„å”¯ä¸€æŸ¥è¯¢å’Œçº¦ç”±å¤šä¸ªæ¨¡å‹å¤šæ¬¡ä¼ é€’ç”Ÿæˆçš„4åƒä¸‡ä¸ªè’¸é¦å“åº”ã€‚æˆ‘ä»¬åˆ©ç”¨é€šè¿‡ç‡å’Œå˜å¼‚ç³»æ•°ï¼ˆCVï¼‰ç²¾ç¡®é€‰æ‹©æœ€æœ‰ä»·å€¼çš„è®­ç»ƒæ•°æ®ï¼Œä»¥æé«˜æ¨ç†èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è®­ç»ƒæ¨¡å¼çš„å˜åŒ–ï¼Œè¿™è¡¨æ˜åŸºäºåŸºç¡€æ¨¡å‹çš„æ¨ç†è®­ç»ƒéœ€è¦æ›´é«˜çš„å­¦ä¹ ç‡æ‰èƒ½è¿›è¡Œæœ‰æ•ˆè®­ç»ƒã€‚ä½¿ç”¨ç»è¿‡ç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®ï¼Œæˆ‘ä»¬æ˜¾è‘—æé«˜äº†åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨AIME2024æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„é€šè¿‡ç‡è¾¾åˆ°79.2%ã€‚è¿™ä¸€ç»“æœè¶…è¶Šäº†å¤§å¤šæ•°å½“å‰çš„è’¸é¦æ¨¡å‹ï¼Œå¹¶æ¥è¿‘äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬æä¾›äº†å…³äºæ•°æ®å¤„ç†ã€éš¾åº¦è¯„ä¼°å’ŒåŸ¹è®­æ–¹æ³•çš„è¯¦ç»†æè¿°ï¼Œå¹¶å·²å…¬å¼€å‘å¸ƒæ‰€æœ‰æ•°æ®é›†å’Œæ–¹æ³•ï¼Œä»¥ä¿ƒè¿›å¼€æºé•¿æ¨ç†LLMçš„å¿«é€Ÿå‘å±•ã€‚æ•°æ®é›†å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M">https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17565v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†ä»ç¼ºä¹å¯¹åŸºç¡€æ¨¡å‹è®­ç»ƒè¿‡ç¨‹å’Œæ•°æ®è´¨é‡çš„æ·±å…¥äº†è§£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€éš¾åº¦åˆ†çº§çš„æ¨ç†æ•°æ®é›†ï¼ŒåŒ…å«çº¦334ä¸‡æ¡å”¯ä¸€æŸ¥è¯¢å’Œçº¦4åƒä¸‡æ¡è’¸é¦å“åº”ã€‚é€šè¿‡åˆ©ç”¨é€šè¿‡ç‡ï¼ˆpass rateï¼‰å’Œå˜å¼‚ç³»æ•°ï¼ˆCoefficient of Variationï¼ŒCVï¼‰ï¼Œæˆ‘ä»¬ç²¾ç¡®é€‰æ‹©äº†æœ€æœ‰ä»·å€¼çš„è®­ç»ƒæ•°æ®ä»¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè§‚å¯Ÿåˆ°è®­ç»ƒæ¨¡å¼è½¬å˜ï¼Œè¡¨æ˜åŸºäºåŸºç¡€æ¨¡å‹çš„æ¨ç†è®­ç»ƒéœ€è¦æ›´é«˜çš„å­¦ä¹ ç‡è¿›è¡Œæœ‰æ•ˆè®­ç»ƒã€‚ä½¿ç”¨è¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬æ˜¾è‘—æé«˜äº†åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨AIME2024æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†79.2%çš„é€šè¿‡ç‡ï¼Œè¶…è¶Šå¤§å¤šæ•°ç°æœ‰è’¸é¦æ¨¡å‹ï¼Œæ¥è¿‘æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æˆ‘ä»¬æä¾›äº†è¯¦ç»†çš„æ•°æ®å¤„ç†ã€éš¾åº¦è¯„ä¼°å’Œè®­ç»ƒæ–¹æ³•æè¿°ï¼Œå¹¶å·²å…¬å¼€å‘å¸ƒæ‰€æœ‰æ•°æ®é›†å’Œæ–¹æ³•ï¼Œä»¥ä¿ƒè¿›å¼€æºé•¿æ¨ç†LLMçš„å¿«é€Ÿå‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡éš¾åº¦åˆ†çº§çš„æ¨ç†æ•°æ®é›†ï¼ŒåŒ…å«æ•°ç™¾ä¸‡æ¡æŸ¥è¯¢å’Œå“åº”ã€‚</li>
<li>åˆ©ç”¨é€šè¿‡ç‡ï¼ˆpass rateï¼‰å’Œå˜å¼‚ç³»æ•°ï¼ˆCoefficient of Variation, CVï¼‰é€‰æ‹©è®­ç»ƒæ•°æ®ä»¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚</li>
<li>è§‚å¯Ÿåˆ°äº†è®­ç»ƒæ¨¡å¼çš„è½¬å˜ï¼ŒæŒ‡å‡ºåŸºäºåŸºç¡€æ¨¡å‹çš„æ¨ç†è®­ç»ƒéœ€è¦æ›´é«˜çš„å­¦ä¹ ç‡ã€‚</li>
<li>é€šè¿‡ç²¾å¿ƒé€‰æ‹©æ•°æ®å’Œè°ƒæ•´è®­ç»ƒç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åœ¨AIME2024æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†é«˜é€šè¿‡ç‡ï¼Œè¡¨ç°è¶…è¶Šå¤šæ•°ç°æœ‰æ¨¡å‹å¹¶æ¥è¿‘æœ€æ–°æŠ€æœ¯ã€‚</li>
<li>æä¾›äº†è¯¦ç»†çš„æ•°æ®å¤„ç†ã€éš¾åº¦è¯„ä¼°åŠè®­ç»ƒæ–¹æ³•çš„æè¿°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17565">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-440dee1fa035bcf8da74a5fc55aba2d3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Driven-Concolic-Execution-for-Highly-Structured-Test-Input-Generation"><a href="#Large-Language-Model-Driven-Concolic-Execution-for-Highly-Structured-Test-Input-Generation" class="headerlink" title="Large Language Model-Driven Concolic Execution for Highly Structured   Test Input Generation"></a>Large Language Model-Driven Concolic Execution for Highly Structured   Test Input Generation</h2><p><strong>Authors:Haoxin Tu, Seongmin Lee, Yuxian Li, Peng Chen, Lingxiao Jiang, Marcel BÃ¶hme</strong></p>
<p>How can we perform concolic execution to generate highly structured test inputs for systematically testing parsing programs? Existing concolic execution engines are significantly restricted by (1) input structure-agnostic path constraint selection, leading to the waste of testing effort or missing coverage; (2) limited constraint-solving capability, yielding many syntactically invalid test inputs; (3) reliance on manual acquisition of highly structured seed inputs, resulting in non-continuous testing.   This paper proposes Cottontail, a new Large Language Model (LLM)-driven concolic execution engine, to mitigate the above limitations. A more complete program path representation, named Expressive Structural Coverage Tree (ESCT), is first constructed to select structure-aware path constraints. Later, an LLM-driven constraint solver based on a Solve-Complete paradigm is designed to solve the path constraints smartly to get test inputs that are not only satisfiable to the constraints but also valid to the input syntax. Finally, a history-guided seed acquisition is employed to obtain new highly structured test inputs either before testing starts or after testing is saturated.   We implemented Cottontail on top of SymCC and evaluated eight extensively tested open-source libraries across four different formats (XML, SQL, JavaScript, and JSON). The experimental result is promising: it shows that Cottontail outperforms state-of-the-art approaches (SymCC and Marco) by 14.15% and 14.31% in terms of line coverage. Besides, Cottontail found 6 previously unknown vulnerabilities (six new CVEs have been assigned). We have reported these issues to developers, and 4 out of them have been fixed so far. </p>
<blockquote>
<p>å¦‚ä½•æ‰§è¡Œç¬¦å·æ‰§è¡Œä¸æ¨¡æ‹Ÿæ‰§è¡Œç»“åˆçš„æµ‹è¯•æ–¹æ³•ï¼ˆConcolic Executionï¼‰ï¼Œä»¥ç”Ÿæˆé«˜åº¦ç»“æ„åŒ–çš„æµ‹è¯•è¾“å…¥ï¼Œå¯¹è§£æç¨‹åºè¿›è¡Œç³»ç»Ÿæµ‹è¯•ï¼Ÿç°æœ‰çš„ç¬¦å·æ‰§è¡Œä¸æ¨¡æ‹Ÿæ‰§è¡Œç»“åˆæµ‹è¯•å¼•æ“å—åˆ°ä»¥ä¸‹é™åˆ¶ï¼šï¼ˆ1ï¼‰è¾“å…¥ç»“æ„æ— å…³çš„è·¯å¾„çº¦æŸé€‰æ‹©ï¼Œå¯¼è‡´æµ‹è¯•å·¥ä½œæµªè´¹æˆ–è¦†ç›–ä¸è¶³ï¼›ï¼ˆ2ï¼‰æœ‰é™çš„çº¦æŸæ±‚è§£èƒ½åŠ›ï¼Œäº§ç”Ÿè®¸å¤šè¯­æ³•ä¸Šæ— æ•ˆçš„æµ‹è¯•è¾“å…¥ï¼›ï¼ˆ3ï¼‰é«˜åº¦ç»“æ„åŒ–ç§å­è¾“å…¥çš„ä¾èµ–æ‰‹åŠ¨è·å–ï¼Œå¯¼è‡´éè¿ç»­æµ‹è¯•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„ç¬¦å·æ‰§è¡Œä¸æ¨¡æ‹Ÿæ‰§è¡Œç»“åˆæµ‹è¯•å¼•æ“â€”â€”Cottontailï¼Œä»¥ç¼“è§£ä¸Šè¿°é™åˆ¶ã€‚é¦–å…ˆï¼Œæ„å»ºäº†ä¸€ä¸ªåä¸ºè¡¨è¾¾æ€§ç»“æ„è¦†ç›–æ ‘ï¼ˆESCTï¼‰çš„æ›´å®Œæ•´çš„ç¨‹åºè·¯å¾„è¡¨ç¤ºï¼Œä»¥é€‰æ‹©ç»“æ„æ„ŸçŸ¥çš„è·¯å¾„çº¦æŸã€‚ç„¶åï¼Œè®¾è®¡äº†ä¸€ç§åŸºäºæ±‚è§£å®Œæ•´èŒƒå¼çš„LLMé©±åŠ¨çº¦æŸæ±‚è§£å™¨ï¼Œä»¥æ™ºèƒ½åœ°è§£å†³è·¯å¾„çº¦æŸï¼Œä»è€Œè·å¾—ä¸ä»…æ»¡è¶³çº¦æŸè€Œä¸”ç¬¦åˆè¾“å…¥è¯­æ³•çš„æµ‹è¯•è¾“å…¥ã€‚æœ€åï¼Œé‡‡ç”¨å†å²å¼•å¯¼çš„ç§å­è·å–æ–¹å¼ï¼Œåœ¨æµ‹è¯•å¼€å§‹ä¹‹å‰æˆ–æµ‹è¯•é¥±å’Œä¹‹åè·å¾—æ–°çš„é«˜åº¦ç»“æ„åŒ–çš„æµ‹è¯•è¾“å…¥ã€‚æˆ‘ä»¬åœ¨SymCCä¹‹ä¸Šå®ç°äº†Cottontailï¼Œå¹¶è¯„ä¼°äº†å››ä¸ªä¸åŒæ ¼å¼ï¼ˆXMLã€SQLã€JavaScriptå’ŒJSONï¼‰çš„å…«ä¸ªå¹¿æ³›æµ‹è¯•çš„å¼€æºåº“ã€‚å®éªŒç»“æœå…·æœ‰å‰æ™¯ï¼šå®ƒè¡¨æ˜ï¼Œåœ¨è¡Œè¦†ç›–ç‡æ–¹é¢ï¼ŒCottontailæ¯”æœ€æ–°æ–¹æ³•ï¼ˆSymCCå’ŒMarcoï¼‰é«˜å‡º14.15%å’Œ14.31%ã€‚æ­¤å¤–ï¼ŒCottontailå‘ç°äº†6ä¸ªä»¥å‰æœªçŸ¥çš„æ¼æ´ï¼ˆå·²åˆ†é…äº†å…­ä¸ªæ–°çš„CVEï¼‰ã€‚æˆ‘ä»¬å·²ç»å‘å¼€å‘äººå‘˜æŠ¥å‘Šäº†è¿™äº›é—®é¢˜ï¼Œç›®å‰å·²æœ‰å››ä¸ªå¾—åˆ°ä¿®å¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17542v1">PDF</a> 18 pages (including Appendix)</p>
<p><strong>Summary</strong><br>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„Cottontailæ‰§è¡Œå¼•æ“æå‡ºè§£å†³concolicæ‰§è¡Œç”Ÿæˆé«˜åº¦ç»“æ„åŒ–æµ‹è¯•è¾“å…¥çš„é—®é¢˜ã€‚å®ƒè§£å†³äº†ç°æœ‰concolicæ‰§è¡Œå¼•æ“çš„é™åˆ¶ï¼Œå¦‚è·¯å¾„çº¦æŸé€‰æ‹©çš„ç»“æ„æ— å…³æ€§ã€çº¦æŸæ±‚è§£èƒ½åŠ›æœ‰é™ä»¥åŠä¾èµ–æ‰‹åŠ¨è·å–é«˜åº¦ç»“æ„åŒ–ç§å­è¾“å…¥çš„é—®é¢˜ã€‚é€šè¿‡æ„å»ºè¡¨è¾¾æ€§ç»“æ„è¦†ç›–æ ‘ï¼ˆESCTï¼‰è¿›è¡Œæ›´å®Œæ•´çš„ç¨‹åºè·¯å¾„è¡¨ç¤ºï¼Œæ™ºèƒ½åœ°è§£å†³è·¯å¾„çº¦æŸæ¥ç”Ÿæˆæ»¡è¶³çº¦æŸä¸”ç¬¦åˆè¾“å…¥è¯­æ³•çš„æµ‹è¯•è¾“å…¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCottontailåœ¨è¡Œè¦†ç›–ç‡æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹¶å‘ç°äº†å…­ä¸ªä»¥å‰æœªçŸ¥çš„æ¼æ´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Cottontailæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„concolicæ‰§è¡Œå¼•æ“ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰concolicæ‰§è¡Œå¼•æ“åœ¨ç”Ÿæˆé«˜åº¦ç»“æ„åŒ–æµ‹è¯•è¾“å…¥æ–¹é¢çš„é™åˆ¶ã€‚</li>
<li>é€šè¿‡æ„å»ºè¡¨è¾¾æ€§ç»“æ„è¦†ç›–æ ‘ï¼ˆESCTï¼‰æ¥è¿›è¡Œæ›´å®Œæ•´çš„ç¨‹åºè·¯å¾„è¡¨ç¤ºï¼Œä»¥é€‰æ‹©ç»“æ„æ„ŸçŸ¥çš„è·¯å¾„çº¦æŸã€‚</li>
<li>é‡‡ç”¨åŸºäºSolve-CompleteèŒƒå¼çš„LLMé©±åŠ¨çº¦æŸæ±‚è§£å™¨ï¼Œæ™ºèƒ½åœ°è§£å†³è·¯å¾„çº¦æŸï¼Œç”Ÿæˆæ—¢æ»¡è¶³çº¦æŸåˆç¬¦åˆè¾“å…¥è¯­æ³•çš„æµ‹è¯•è¾“å…¥ã€‚</li>
<li>Cottontailå®ç°äº†å†å²å¼•å¯¼çš„ç§å­è·å–ï¼Œå¯åœ¨æµ‹è¯•å¼€å§‹å‰æˆ–æµ‹è¯•é¥±å’Œåè·å–æ–°çš„é«˜åº¦ç»“æ„åŒ–æµ‹è¯•è¾“å…¥ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17542">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bc615a5521876b80f4a3e17d4dddfdf8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d60a7643e34dcc7a83ac9279dad10c37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3eadf208509a160534a710a76fe9c66c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cd9856774029c26c5173fb0f8b4b6a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca92e6e80857be84b8db812839cc3003.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Novel-Graph-Transformer-Framework-for-Gene-Regulatory-Network-Inference"><a href="#A-Novel-Graph-Transformer-Framework-for-Gene-Regulatory-Network-Inference" class="headerlink" title="A Novel Graph Transformer Framework for Gene Regulatory Network   Inference"></a>A Novel Graph Transformer Framework for Gene Regulatory Network   Inference</h2><p><strong>Authors:Binon Teji, Swarup Roy</strong></p>
<p>The inference of gene regulatory networks (GRNs) is a foundational stride towards deciphering the fundamentals of complex biological systems. Inferring a possible regulatory link between two genes can be formulated as a link prediction problem. Inference of GRNs via gene coexpression profiling data may not always reflect true biological interactions, as its susceptibility to noise and misrepresenting true biological regulatory relationships. Most GRN inference methods face several challenges in the network reconstruction phase. Therefore, it is important to encode gene expression values, leverege the prior knowledge gained from the available inferred network structures and positional informations of the input network nodes towards inferring a better and more confident GRN network reconstruction. In this paper, we explore the integration of multiple inferred networks to enhance the inference of Gene Regulatory Networks (GRNs). Primarily, we employ autoencoder embeddings to capture gene expression patterns directly from raw data, preserving intricate biological signals. Then, we embed the prior knowledge from GRN structures transforming them into a text-like representation using random walks, which are then encoded with a masked language model, BERT, to generate global embeddings for each gene across all networks. Additionally, we embed the positional encodings of the input gene networks to better identify the position of each unique gene within the graph. These embeddings are integrated into graph transformer-based model, termed GT-GRN, for GRN inference. The GT-GRN model effectively utilizes the topological structure of the ground truth network while incorporating the enriched encoded information. Experimental results demonstrate that GT-GRN significantly outperforms existing GRN inference methods, achieving superior accuracy and highlighting the robustness of our approach. </p>
<blockquote>
<p>åŸºå› è°ƒæ§ç½‘ç»œï¼ˆGRNsï¼‰çš„æ¨æ–­æ˜¯è§£å¼€å¤æ‚ç”Ÿç‰©ç³»ç»ŸåŸºæœ¬åŸç†çš„é‡è¦æ­¥éª¤ã€‚æ¨æ–­ä¸¤ä¸ªåŸºå› ä¹‹é—´å¯èƒ½çš„è°ƒæ§å…³ç³»å¯ä»¥åˆ¶å®šä¸ºé“¾æ¥é¢„æµ‹é—®é¢˜ã€‚é€šè¿‡åŸºå› å…±è¡¨è¾¾è°±æ•°æ®æ¨æ–­GRNså¯èƒ½å¹¶ä¸æ€»æ˜¯åæ˜ çœŸå®çš„ç”Ÿç‰©ç›¸äº’ä½œç”¨ï¼Œå› ä¸ºå…¶å®¹æ˜“å—åˆ°å™ªå£°å’Œè¯¯æŠ¥çœŸå®ç”Ÿç‰©è°ƒæ§å…³ç³»çš„å½±å“ã€‚å¤§å¤šæ•°GRNæ¨æ–­æ–¹æ³•åœ¨ç½‘ç»œé‡å»ºé˜¶æ®µé¢ä¸´å‡ ä¸ªæŒ‘æˆ˜ã€‚å› æ­¤ï¼Œç¼–ç åŸºå› è¡¨è¾¾å€¼ã€åˆ©ç”¨ä»ç°æœ‰æ¨æ–­ç½‘ç»œç»“æ„å’Œè¾“å…¥èŠ‚ç‚¹ä½ç½®ä¿¡æ¯ä¸­è·å¾—çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¯¹äºæ¨æ–­æ›´å¥½ã€æ›´å¯é çš„GRNç½‘ç»œé‡å»ºéå¸¸é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†æ•´åˆå¤šä¸ªæ¨æ–­ç½‘ç»œä»¥å¢å¼ºåŸºå› è°ƒæ§ç½‘ç»œï¼ˆGRNsï¼‰çš„æ¨æ–­ã€‚æˆ‘ä»¬ä¸»è¦ä½¿ç”¨è‡ªåŠ¨ç¼–ç å™¨åµŒå…¥ç›´æ¥ä»åŸå§‹æ•°æ®ä¸­æ•è·åŸºå› è¡¨è¾¾æ¨¡å¼ï¼Œä¿ç•™å¤æ‚çš„ç”Ÿç‰©ä¿¡å·ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨éšæœºæ¸¸èµ°å°†GRNç»“æ„çš„å…ˆéªŒçŸ¥è¯†è½¬åŒ–ä¸ºç±»ä¼¼æ–‡æœ¬çš„è¡¨ç¤ºï¼Œç„¶åä½¿ç”¨æ©ç è¯­è¨€æ¨¡å‹BERTç”Ÿæˆæ‰€æœ‰ç½‘ç»œæ¯ä¸ªåŸºå› çš„å…¨å±€åµŒå…¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹è¾“å…¥åŸºå› ç½‘ç»œçš„ä½ç½®ç¼–ç è¿›è¡ŒåµŒå…¥ï¼Œä»¥æ›´å¥½åœ°è¯†åˆ«å›¾ä¸­æ¯ä¸ªå”¯ä¸€åŸºå› çš„ä½ç½®ã€‚è¿™äº›åµŒå…¥è¢«é›†æˆåˆ°åŸºäºå›¾å˜å‹å™¨çš„æ¨¡å‹ä¸­ï¼Œç”¨äºGRNsæ¨æ–­ï¼Œåä¸ºGT-GRNsã€‚GT-GRNsæ¨¡å‹æœ‰æ•ˆåœ°åˆ©ç”¨çœŸå®ç½‘ç»œçš„æ‹“æ‰‘ç»“æ„ï¼ŒåŒæ—¶èå…¥ä¸°å¯Œçš„ç¼–ç ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGT-GRNsæ˜¾è‘—ä¼˜äºç°æœ‰çš„GRNæ¨æ–­æ–¹æ³•ï¼Œå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16961v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºå› è°ƒæ§ç½‘ç»œï¼ˆGRNsï¼‰çš„æ¨æ–­æ˜¯è§£å¼€å¤æ‚ç”Ÿç‰©ç³»ç»ŸåŸºæœ¬å¥¥ç§˜çš„é‡è¦æ­¥éª¤ã€‚é€šè¿‡åŸºå› å…±è¡¨è¾¾è°±æ•°æ®æ¨æ–­GRNså¯èƒ½æ— æ³•çœŸå®åæ˜ ç”Ÿç‰©ç›¸äº’ä½œç”¨ï¼Œå› ä¸ºè¿™ç§æ–¹æ³•å®¹æ˜“å—åˆ°å™ªå£°å¹²æ‰°å¹¶ä¸”å¯èƒ½è¯¯æŠ¥çœŸå®çš„ç”Ÿç‰©è°ƒæ§å…³ç³»ã€‚å¤§å¤šæ•°GRNæ¨æ–­æ–¹æ³•åœ¨ç½‘ç»œé‡å»ºé˜¶æ®µé¢ä¸´æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œç¼–ç åŸºå› è¡¨è¾¾å€¼ã€åˆ©ç”¨ç°æœ‰æ¨æ–­ç½‘ç»œç»“æ„çš„å…ˆéªŒçŸ¥è¯†ä»¥åŠè¾“å…¥ç½‘ç»œèŠ‚ç‚¹çš„ä½ç½®ä¿¡æ¯å¯¹äºæ¨æ–­æ›´å¥½ã€æ›´å¯é çš„GRNsç½‘ç»œé‡å»ºè‡³å…³é‡è¦ã€‚æœ¬æ–‡æ¢ç´¢äº†æ•´åˆå¤šä¸ªæ¨æ–­ç½‘ç»œä»¥å¢å¼ºåŸºå› è°ƒæ§ç½‘ç»œï¼ˆGRNsï¼‰çš„æ¨æ–­ã€‚æˆ‘ä»¬ä¸»è¦ä½¿ç”¨è‡ªåŠ¨ç¼–ç å™¨åµŒå…¥ç›´æ¥ä»åŸå§‹æ•°æ®ä¸­æ•è·åŸºå› è¡¨è¾¾æ¨¡å¼ï¼Œä¿ç•™å¤æ‚çš„ç”Ÿç‰©ä¿¡å·ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨éšæœºæ¸¸èµ°å°†GRNsç»“æ„çš„å…ˆéªŒçŸ¥è¯†è½¬åŒ–ä¸ºæ–‡æœ¬å¼è¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨æ©ç è¯­è¨€æ¨¡å‹BERTç”Ÿæˆæ‰€æœ‰ç½‘ç»œæ¯ä¸ªåŸºå› çš„å…¨å±€åµŒå…¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹è¾“å…¥åŸºå› ç½‘ç»œçš„ä½ç½®ç¼–ç è¿›è¡ŒåµŒå…¥ï¼Œä»¥æ›´å¥½åœ°è¯†åˆ«å›¾ä¸­æ¯ä¸ªç‹¬ç‰¹åŸºå› çš„ä½ç½®ã€‚è¿™äº›åµŒå…¥è¢«é›†æˆåˆ°åŸºäºå›¾å˜å‹å™¨çš„æ¨¡å‹ä¸­ï¼Œç§°ä¸ºGT-GRNï¼Œç”¨äºGRNsæ¨æ–­ã€‚GT-GRNæ¨¡å‹æœ‰æ•ˆåœ°åˆ©ç”¨åœ°é¢çœŸå®ç½‘ç»œçš„æ‹“æ‰‘ç»“æ„ï¼ŒåŒæ—¶èå…¥ä¸°å¯Œçš„ç¼–ç ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGT-GRNæ˜¾è‘—ä¼˜äºç°æœ‰çš„GRNæ¨æ–­æ–¹æ³•ï¼Œå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŸºå› è°ƒæ§ç½‘ç»œï¼ˆGRNsï¼‰çš„æ¨æ–­æ˜¯ç†è§£å¤æ‚ç”Ÿç‰©ç³»ç»Ÿçš„åŸºç¡€ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡åŸºå› å…±è¡¨è¾¾è°±æ•°æ®æ¨æ–­GRNså¯èƒ½å­˜åœ¨è¯¯å·®ï¼Œæ˜“å—å™ªå£°å¹²æ‰°å¹¶å¯èƒ½è¯¯æŠ¥çœŸå®ç”Ÿç‰©è°ƒæ§å…³ç³»ã€‚</li>
<li>æ•´åˆå¤šä¸ªæ¨æ–­ç½‘ç»œèƒ½å¢å¼ºGRNsçš„æ¨æ–­ã€‚</li>
<li>ä½¿ç”¨è‡ªåŠ¨ç¼–ç å™¨åµŒå…¥ç›´æ¥ä»åŸå§‹æ•°æ®ä¸­æ•è·åŸºå› è¡¨è¾¾æ¨¡å¼ï¼Œå¹¶ä¿ç•™å¤æ‚çš„ç”Ÿç‰©ä¿¡å·ã€‚</li>
<li>åˆ©ç”¨éšæœºæ¸¸èµ°å’ŒBERTæ¨¡å‹å°†å…ˆéªŒçŸ¥è¯†è½¬åŒ–ä¸ºæ–‡æœ¬è¡¨ç¤ºï¼Œå¹¶ç”Ÿæˆæ¯ä¸ªåŸºå› çš„å…¨å±€åµŒå…¥ã€‚</li>
<li>æ•´åˆä½ç½®ç¼–ç ä»¥è¯†åˆ«å›¾ä¸­æ¯ä¸ªåŸºå› çš„ç‹¬ç‰¹ä½ç½®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16961">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-376a0b2274651ea4f67363395934fca0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4a62e82bf24ea1dd6ca0b8a544c9369.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47abe17365cd15fd377a58be119a8d9b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Exploring-How-LLMs-Capture-and-Represent-Domain-Specific-Knowledge"><a href="#Exploring-How-LLMs-Capture-and-Represent-Domain-Specific-Knowledge" class="headerlink" title="Exploring How LLMs Capture and Represent Domain-Specific Knowledge"></a>Exploring How LLMs Capture and Represent Domain-Specific Knowledge</h2><p><strong>Authors:Mirian Hipolito Garcia, Camille Couturier, Daniel Madrigal Diaz, Ankur Mallick, Anastasios Kyrillidis, Robert Sim, Victor Ruhle, Saravan Rajmohan</strong></p>
<p>We study whether Large Language Models (LLMs) inherently capture domain-specific nuances in natural language. Our experiments probe the domain sensitivity of LLMs by examining their ability to distinguish queries from different domains using hidden states generated during the prefill phase. We reveal latent domain-related trajectories that indicate the modelâ€™s internal recognition of query domains. We also study the robustness of these domain representations to variations in prompt styles and sources. Our approach leverages these representations for model selection, mapping the LLM that best matches the domain trace of the input query (i.e., the model with the highest performance on similar traces). Our findings show that LLMs can differentiate queries for related domains, and that the fine-tuned model is not always the most accurate. Unlike previous work, our interpretations apply to both closed and open-ended generative tasks </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦å¤©ç„¶åœ°æ•è·è‡ªç„¶è¯­è¨€ä¸­çš„ç‰¹å®šé¢†åŸŸç»†å¾®å·®åˆ«ã€‚æˆ‘ä»¬é€šè¿‡è€ƒå¯Ÿé¢„å¡«å……é˜¶æ®µç”Ÿæˆçš„éšè—çŠ¶æ€æ¥æ¢ç©¶LLMå¯¹ä¸åŒé¢†åŸŸçš„æŸ¥è¯¢çš„åŒºåˆ†èƒ½åŠ›ï¼Œä»è€Œæ¢ç©¶å…¶å¯¹é¢†åŸŸçš„æ•æ„Ÿæ€§ã€‚æˆ‘ä»¬æ­ç¤ºäº†ä¸é¢†åŸŸç›¸å…³çš„æ½œåœ¨è½¨è¿¹ï¼Œè¡¨æ˜æ¨¡å‹å¯¹æŸ¥è¯¢é¢†åŸŸçš„å†…éƒ¨è¯†åˆ«ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†è¿™äº›é¢†åŸŸè¡¨ç¤ºå¯¹æç¤ºé£æ ¼å’Œæ¥æºå˜åŒ–çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨è¿™äº›è¡¨ç¤ºä¸ºæ¨¡å‹é€‰æ‹©æä¾›æ”¯æŒï¼Œå°†æœ€ä½³åŒ¹é…è¾“å…¥æŸ¥è¯¢é¢†åŸŸè½¨è¿¹çš„LLMï¼ˆå³åœ¨ç±»ä¼¼è½¨è¿¹ä¸Šè¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼‰æ˜ å°„å‡ºæ¥ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMèƒ½å¤ŸåŒºåˆ†ç›¸å…³é¢†åŸŸçš„æŸ¥è¯¢ï¼Œè€Œä¸”ç²¾ç»†è°ƒæ•´çš„æ¨¡å‹å¹¶ä¸æ€»æ˜¯æœ€å‡†ç¡®çš„ã€‚ä¸ä»¥å‰çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬çš„è§£é‡Šé€‚ç”¨äºå°é—­ä»»åŠ¡å’Œå¼€æ”¾å¼ç”Ÿæˆä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16871v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¦æ•æ‰è‡ªç„¶è¯­è¨€ä¸­çš„ç‰¹å®šé¢†åŸŸç»†å¾®å·®åˆ«è¿›è¡Œäº†ç ”ç©¶ã€‚é€šè¿‡å®éªŒæ¢ç©¶äº†LLMsåœ¨é¢„å¡«å……é˜¶æ®µç”Ÿæˆçš„éšè—çŠ¶æ€å¯¹åŒºåˆ†ä¸åŒé¢†åŸŸæŸ¥è¯¢çš„èƒ½åŠ›ï¼Œæ­ç¤ºäº†æ¨¡å‹å¯¹æŸ¥è¯¢é¢†åŸŸçš„å†…éƒ¨è¯†åˆ«è½¨è¿¹ã€‚åŒæ—¶ç ”ç©¶äº†è¿™äº›é¢†åŸŸè¡¨å¾å¯¹å„ç§æç¤ºé£æ ¼å’Œæ¥æºçš„ç¨³å¥æ€§ã€‚é€šè¿‡åˆ©ç”¨è¿™äº›è¡¨å¾æ¥è¿›è¡Œæ¨¡å‹é€‰æ‹©ï¼Œæ‰¾åˆ°ä¸è¾“å…¥æŸ¥è¯¢çš„åŸŸè½¨è¿¹æœ€åŒ¹é…çš„LLMï¼ˆå³åœ¨ç±»ä¼¼è½¨è¿¹ä¸Šè¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼‰ã€‚ç ”ç©¶å‘ç°LLMèƒ½å¤ŸåŒºåˆ†ç›¸å…³é¢†åŸŸçš„æŸ¥è¯¢ï¼Œå¹¶ä¸”å¾®è°ƒæ¨¡å‹å¹¶ä¸æ€»æ˜¯æœ€å‡†ç¡®çš„ã€‚ä¸ä»¥å‰çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬çš„è§£é‡Šé€‚ç”¨äºå°é—­å’Œå¼€æ”¾å¼çš„ç”Ÿæˆä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsèƒ½å¤Ÿæ•æ‰è‡ªç„¶è¯­è¨€ä¸­çš„ç‰¹å®šé¢†åŸŸç»†å¾®å·®åˆ«ã€‚</li>
<li>é€šè¿‡æ¢ç©¶LLMsåœ¨é¢„å¡«å……é˜¶æ®µç”Ÿæˆçš„éšè—çŠ¶æ€ï¼Œæ­ç¤ºäº†æ¨¡å‹å¯¹æŸ¥è¯¢é¢†åŸŸçš„å†…éƒ¨è¯†åˆ«è½¨è¿¹ã€‚</li>
<li>LLMsèƒ½å¤ŸåŒºåˆ†ç›¸å…³é¢†åŸŸçš„æŸ¥è¯¢ã€‚</li>
<li>æ¨¡å‹é€‰æ‹©å¯ä»¥é€šè¿‡åˆ©ç”¨è¿™äº›é¢†åŸŸè¡¨å¾æ¥å®ç°ï¼Œæ‰¾åˆ°ä¸è¾“å…¥æŸ¥è¯¢çš„åŸŸè½¨è¿¹æœ€åŒ¹é…çš„LLMã€‚</li>
<li>å³ä½¿åœ¨ç±»ä¼¼ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½çš„å¾®è°ƒæ¨¡å‹å¹¶ä¸æ€»æ˜¯æœ€å‡†ç¡®çš„ã€‚è¿™ä¸€å‘ç°å¯¹æ¨¡å‹é€‰æ‹©å’Œä»»åŠ¡ä¼˜åŒ–æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>æ­¤ç ”ç©¶æä¾›çš„è§è§£é€‚ç”¨äºä¸åŒç±»å‹çš„ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬å°é—­å¼å’Œå¼€æ”¾å¼çš„ä»»åŠ¡ã€‚è¿™å¯¹äºåœ¨å„ç§ä»»åŠ¡åœºæ™¯ä¸‹åˆ©ç”¨LLMå…·æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16871">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-05b10c011d3594159aa852a50026353e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5b9022b5c9c212f88a312e966fbfc77.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7f2e6e15b6691708ac19d9748b540dc0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Can-Large-Language-Models-Help-Multimodal-Language-Analysis-MMLA-A-Comprehensive-Benchmark"><a href="#Can-Large-Language-Models-Help-Multimodal-Language-Analysis-MMLA-A-Comprehensive-Benchmark" class="headerlink" title="Can Large Language Models Help Multimodal Language Analysis? MMLA: A   Comprehensive Benchmark"></a>Can Large Language Models Help Multimodal Language Analysis? MMLA: A   Comprehensive Benchmark</h2><p><strong>Authors:Hanlei Zhang, Zhuohang Li, Yeshuang Zhu, Hua Xu, Peiwu Wang, Haige Zhu, Jie Zhou, Jinchao Zhang</strong></p>
<p>Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA comprises over 61K multimodal utterances drawn from both staged and real-world scenarios, covering six core dimensions of multimodal semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. We evaluate eight mainstream branches of LLMs and MLLMs using three methods: zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive experiments reveal that even fine-tuned models achieve only about 60%~70% accuracy, underscoring the limitations of current MLLMs in understanding complex human language. We believe that MMLA will serve as a solid foundation for exploring the potential of large language models in multimodal language analysis and provide valuable resources to advance this field. The datasets and code are open-sourced at <a target="_blank" rel="noopener" href="https://github.com/thuiar/MMLA">https://github.com/thuiar/MMLA</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€è¯­è¨€åˆ†ææ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼Œå®ƒåˆ©ç”¨å¤šç§æ¨¡æ€å¢å¼ºå¯¹äººç±»ä¼šè¯è¡¨è¿°ä¸­é«˜çº§è¯­ä¹‰çš„ç†è§£ã€‚å°½ç®¡è¿™ä¸€é¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½†å…³äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç†è§£è®¤çŸ¥çº§è¯­ä¹‰æ–¹é¢çš„èƒ½åŠ›çš„ç ”ç©¶å´å¾ˆå°‘ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MMLAï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè§£å†³è¿™ä¸€å·®è·çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚MMLAåŒ…å«æ¥è‡ªèˆå°å’ŒçœŸå®ä¸–ç•Œåœºæ™¯çš„è¶…è¿‡6.1ä¸‡æ¡å¤šæ¨¡æ€è¡¨è¿°ï¼Œæ¶µç›–å¤šæ¨¡æ€è¯­ä¹‰çš„å…­ä¸ªæ ¸å¿ƒç»´åº¦ï¼šæ„å›¾ã€æƒ…æ„Ÿã€å¯¹è¯è¡Œä¸ºã€æƒ…æ„Ÿå€¾å‘ã€è¯´è¯é£æ ¼å’Œæ²Ÿé€šè¡Œä¸ºã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰ç§æ–¹æ³•ï¼šé›¶æ ·æœ¬æ¨ç†ã€ç›‘ç£å¾®è°ƒã€æŒ‡ä»¤å¾®è°ƒï¼Œè¯„ä¼°äº†å…«ä¸ªä¸»æµçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åˆ†æ”¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œå³ä½¿ç»è¿‡ç²¾ç»†è®­ç»ƒçš„æ¨¡å‹ä¹Ÿåªèƒ½è¾¾åˆ°çº¦60%~70%çš„å‡†ç¡®ç‡ï¼Œçªæ˜¾å‡ºå½“å‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç†è§£å¤æ‚äººç±»è¯­è¨€æ–¹é¢çš„å±€é™æ€§ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼ŒMMLAå°†ä½œä¸ºæ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€è¯­è¨€åˆ†ææ½œåŠ›çš„é‡è¦åŸºç¡€ï¼Œå¹¶ä¸ºæ¨åŠ¨è¿™ä¸€é¢†åŸŸçš„å‘å±•æä¾›å®è´µçš„èµ„æºã€‚æ•°æ®é›†å’Œä»£ç å·²å¼€æºåœ¨<a target="_blank" rel="noopener" href="https://github.com/thuiar/MMLA%E3%80%82">https://github.com/thuiar/MMLAã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16427v2">PDF</a> 23 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€è¯­è¨€åˆ†æé¢†åŸŸçš„ä¸€ä¸ªç»¼åˆæ€§åŸºå‡†æµ‹è¯•MMLAï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¯¹è®¤çŸ¥çº§è¯­ä¹‰çš„ç†è§£èƒ½åŠ›ã€‚MMLAåŒ…å«è¶…è¿‡61Kä¸ªæ¥è‡ªèˆå°å’ŒçœŸå®åœºæ™¯çš„å¤šæ¨¡æ€è¯è¯­ï¼Œæ¶µç›–æ„å›¾ã€æƒ…æ„Ÿã€å¯¹è¯è¡Œä¸ºç­‰å…­ä¸ªæ ¸å¿ƒç»´åº¦çš„å¤šæ¨¡æ€è¯­ä¹‰ã€‚å®éªŒè¡¨æ˜ï¼Œå½“å‰MLLMsåœ¨ç†è§£å¤æ‚äººç±»è¯­è¨€æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå³ä½¿ç»è¿‡ç²¾ç»†è®­ç»ƒçš„æ¨¡å‹ä¹Ÿåªèƒ½è¾¾åˆ°çº¦60%~70%çš„å‡†ç¡®ç‡ã€‚æœ¬æ–‡è®¤ä¸ºMMLAå°†ä¸ºæ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€è¯­è¨€åˆ†æä¸­çš„æ½œåŠ›æä¾›åšå®åŸºç¡€ï¼Œå¹¶ä¸ºæ¨åŠ¨è¯¥é¢†åŸŸå‘å±•æä¾›å®è´µèµ„æºã€‚æ•°æ®é›†å’Œä»£ç å·²å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤šæ¨¡æ€è¯­è¨€åˆ†ææ˜¯ä¸€ä¸ªåˆ©ç”¨å¤šç§æ¨¡å¼å¢å¼ºå¯¹äººç±»ä¼šè¯ä¸­é«˜å±‚è¯­ä¹‰ç†è§£çš„æ–°å…´é¢†åŸŸã€‚</li>
<li>MMLAåŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¯¹è®¤çŸ¥çº§è¯­ä¹‰çš„ç†è§£èƒ½åŠ›ã€‚</li>
<li>MMLAåŒ…å«æ¥è‡ªèˆå°å’ŒçœŸå®åœºæ™¯çš„å¤šæ¨¡æ€è¯è¯­ï¼Œæ¶µç›–å¤šä¸ªæ ¸å¿ƒç»´åº¦çš„å¤šæ¨¡æ€è¯­ä¹‰ã€‚</li>
<li>å®éªŒè¡¨æ˜å½“å‰MLLMsåœ¨ç†è§£å¤æ‚äººç±»è¯­è¨€æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>MMLAä¸ºæ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€è¯­è¨€åˆ†æä¸­çš„æ½œåŠ›æä¾›äº†åšå®åŸºç¡€ã€‚</li>
<li>æ•°æ®é›†å’Œä»£ç å·²å¼€æºï¼Œæœ‰åŠ©äºæ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16427">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5dd65c1216b18ad2db30917bc8c90bf3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3292eb7453076a3df09ae79787826dca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd028fc0228a4ef56110959c9f7f4004.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a2dd48f933e7a28feefd44bbe390597.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="EditLord-Learning-Code-Transformation-Rules-for-Code-Editing"><a href="#EditLord-Learning-Code-Transformation-Rules-for-Code-Editing" class="headerlink" title="EditLord: Learning Code Transformation Rules for Code Editing"></a>EditLord: Learning Code Transformation Rules for Code Editing</h2><p><strong>Authors:Weichen Li, Albert Jan, Baishakhi Ray, Chengzhi Mao, Junfeng Yang, Kexin Pei</strong></p>
<p>Code editing is a foundational task in software development, where its effectiveness depends on whether it introduces desired code property changes without changing the original codeâ€™s intended functionality. Existing approaches often formulate code editing as an implicit end-to-end task, omitting the fact that code-editing procedures inherently consist of discrete and explicit steps. Thus, they suffer from suboptimal performance and lack of robustness and generalization. We introduce EditLord, a code editing framework that makes the code transformation steps explicit. Our key insight is to employ a language model (LM) as an inductive learner to extract code editing rules from the training code pairs as concise meta-rule sets. Such rule sets will be manifested for each training sample to augment them for finetuning or assist in prompting- and iterative-based code editing. EditLordoutperforms the state-of-the-art by an average of 22.7% in editing performance and 58.1% in robustness while achieving 20.2% higher functional correctness across critical software engineering and security applications, LM models, and editing modes. </p>
<blockquote>
<p>ä»£ç ç¼–è¾‘æ˜¯è½¯ä»¶å¼€å‘ä¸­çš„åŸºç¡€ä»»åŠ¡ï¼Œå…¶æœ‰æ•ˆæ€§å–å†³äºæ˜¯å¦åœ¨ä¸æ”¹å˜åŸå§‹ä»£ç é¢„æœŸåŠŸèƒ½çš„æƒ…å†µä¸‹å¼•å…¥äº†æ‰€éœ€çš„ä»£ç å±æ€§æ›´æ”¹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†ä»£ç ç¼–è¾‘åˆ¶å®šä¸ºéšå¼çš„ç«¯åˆ°ç«¯ä»»åŠ¡ï¼Œå¿½ç•¥äº†ä»£ç ç¼–è¾‘è¿‡ç¨‹æœ¬è´¨ä¸ŠåŒ…å«ç¦»æ•£å’Œæ˜ç¡®æ­¥éª¤è¿™ä¸€äº‹å®ã€‚å› æ­¤ï¼Œå®ƒä»¬é¢ä¸´æ€§èƒ½ä¸ä½³ã€ç¼ºä¹ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†EditLordï¼Œè¿™æ˜¯ä¸€ä¸ªä½¿ä»£ç è½¬æ¢æ­¥éª¤æ˜ç¡®çš„ä»£ç ç¼–è¾‘æ¡†æ¶ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œé‡‡ç”¨è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ä½œä¸ºå½’çº³å­¦ä¹ è€…ï¼Œä»è®­ç»ƒä»£ç å¯¹ä¸­æå–ç®€æ´çš„å…ƒè§„åˆ™é›†ä½œä¸ºä»£ç ç¼–è¾‘è§„åˆ™ã€‚é’ˆå¯¹æ¯ä¸ªè®­ç»ƒæ ·æœ¬æ˜¾ç¤ºæ­¤ç±»è§„åˆ™é›†ï¼Œä»¥å¢å¼ºå®ƒä»¬è¿›è¡Œå¾®è°ƒæˆ–è¾…åŠ©åŸºäºæç¤ºå’Œè¿­ä»£çš„ä»£ç ç¼–è¾‘ã€‚EditLordåœ¨ç¼–è¾‘æ€§èƒ½ä¸Šå¹³å‡æ¯”ç°æœ‰æŠ€æœ¯é«˜å‡º22.7%ï¼Œåœ¨ç¨³å¥æ€§ä¸Šé«˜å‡º58.1%ï¼ŒåŒæ—¶åœ¨å…³é”®è½¯ä»¶å·¥ç¨‹å’Œå®‰å…¨åº”ç”¨ç¨‹åºã€è¯­è¨€æ¨¡å‹ä»¥åŠç¼–è¾‘æ¨¡å¼æ–¹é¢å®ç°äº†20.2%æ›´é«˜çš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15284v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>ä»£ç ç¼–è¾‘æ˜¯è½¯ä»¶å¼€å‘ä¸­çš„åŸºç¡€ä»»åŠ¡ï¼Œå…¶æœ‰æ•ˆæ€§å–å†³äºæ˜¯å¦åœ¨æ”¹å˜åŸå§‹ä»£ç æ„å›¾åŠŸèƒ½çš„åŒæ—¶å¼•å…¥æ‰€éœ€çš„ä»£ç å±æ€§æ›´æ”¹ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸å°†ä»£ç ç¼–è¾‘åˆ¶å®šä¸ºéšå¼çš„ç«¯åˆ°ç«¯ä»»åŠ¡ï¼Œå¿½ç•¥äº†ä»£ç ç¼–è¾‘è¿‡ç¨‹æœ¬è´¨ä¸ŠåŒ…å«ç¦»æ•£å’Œæ˜ç¡®çš„æ­¥éª¤ã€‚å› æ­¤ï¼Œå®ƒä»¬å­˜åœ¨æ€§èƒ½ä¸ä½³ã€ç¼ºä¹ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥äº†EditLordä»£ç ç¼–è¾‘æ¡†æ¶ï¼Œä½¿ä»£ç è½¬æ¢æ­¥éª¤æ˜ç¡®ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œé‡‡ç”¨è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ä½œä¸ºå½’çº³å­¦ä¹ è€…ï¼Œä»è®­ç»ƒä»£ç å¯¹ä¸­æå–ä»£ç ç¼–è¾‘è§„åˆ™ï¼Œå½¢æˆç®€æ´çš„å…ƒè§„åˆ™é›†ã€‚è¿™äº›è§„åˆ™é›†å°†ä¸ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬æä¾›è¾…åŠ©å¾®è°ƒæˆ–è¾…åŠ©æç¤ºå’Œè¿­ä»£å¼ä»£ç ç¼–è¾‘ã€‚EditLordåœ¨ç¼–è¾‘æ€§èƒ½ã€ç¨³å¥æ€§å’ŒåŠŸèƒ½æ­£ç¡®æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œåœ¨å…³é”®è½¯ä»¶å·¥ç¨‹å’Œå®‰å…¨åº”ç”¨ç¨‹åºã€è¯­è¨€æ¨¡å‹å’Œç¼–è¾‘æ¨¡å¼æ–¹é¢å¹³å‡æé«˜äº†22.7%ã€58.1%å’Œ20.2%ã€‚</p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ol>
<li>ä»£ç ç¼–è¾‘æ˜¯è½¯ä»¶å¼€å‘ä¸­çš„æ ¸å¿ƒä»»åŠ¡ï¼Œè¦æ±‚åœ¨ä¸æ”¹å˜åŸå§‹ä»£ç æ„å›¾åŠŸèƒ½çš„å‰æä¸‹å¼•å…¥æ‰€éœ€çš„ä»£ç å±æ€§æ›´æ”¹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å°†ä»£ç ç¼–è¾‘è§†ä¸ºéšå¼çš„ç«¯åˆ°ç«¯ä»»åŠ¡ï¼Œå¿½ç•¥äº†å…¶å›ºæœ‰çš„ç¦»æ•£å’Œæ˜ç¡®æ­¥éª¤ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³å’Œç¼ºä¹ç¨³å¥æ€§ã€‚</li>
<li>EditLordæ¡†æ¶æ—¨åœ¨ä½¿ä»£ç è½¬æ¢æ­¥éª¤æ˜ç¡®ï¼Œé€šè¿‡é‡‡ç”¨è¯­è¨€æ¨¡å‹æå–ä»£ç ç¼–è¾‘è§„åˆ™æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>è¯­è¨€æ¨¡å‹ä½œä¸ºå½’çº³å­¦ä¹ è€…ï¼Œä»è®­ç»ƒä»£ç å¯¹ä¸­æå–ç®€æ´çš„å…ƒè§„åˆ™é›†ï¼Œä¸ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬æä¾›è¾…åŠ©ã€‚</li>
<li>EditLordé€šè¿‡æ˜ç¡®çš„ç¼–è¾‘æ­¥éª¤å’Œå…ƒè§„åˆ™é›†çš„åº”ç”¨ï¼Œæ˜¾è‘—æé«˜äº†ç¼–è¾‘æ€§èƒ½ã€ç¨³å¥æ€§å’ŒåŠŸèƒ½æ­£ç¡®æ€§ã€‚</li>
<li>åœ¨å…³é”®è½¯ä»¶å·¥ç¨‹å’Œå®‰å…¨åº”ç”¨ç¨‹åºæ–¹é¢ï¼ŒEditLordç›¸è¾ƒäºç°æœ‰æŠ€æœ¯å¹³å‡æå‡äº†22.7%çš„ç¼–è¾‘æ€§èƒ½ã€58.1%çš„ç¨³å¥æ€§å’Œ20.2%çš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15284">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1d45e7658a332273c9e21c2053143d91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-321c80802027cd6817fceaa7b9e9771b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d1776333dc183ad23f0a680e43b947f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0362f3b985a433adc7c232457466f55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e981d381a327640cc0cd0650b81cd42.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Transferable-text-data-distillation-by-trajectory-matching"><a href="#Transferable-text-data-distillation-by-trajectory-matching" class="headerlink" title="Transferable text data distillation by trajectory matching"></a>Transferable text data distillation by trajectory matching</h2><p><strong>Authors:Rong Yao, Hailin Hu, Yifei Fu, Hanting Chen, Wenyi Fang, Fanyi Du, Kai Han, Yunhe Wang</strong></p>
<p>In the realm of large language model (LLM), as the size of large models increases, it also brings higher training costs. There is a urgent need to minimize the data size in LLM training. Compared with data selection method, the data distillation method aims to synthesize a small number of data samples to achieve the training effect of the full data set and has better flexibility. Despite its successes in computer vision, the discreteness of text data has hitherto stymied its exploration in natural language processing (NLP). In this work, we proposed a method that involves learning pseudo prompt data based on trajectory matching and finding its nearest neighbor ID to achieve cross-architecture transfer. During the distillation process, we introduce a regularization loss to improve the robustness of our distilled data. To our best knowledge, this is the first data distillation work suitable for text generation tasks such as instruction tuning. Evaluations on two benchmarks, including ARC-Easy and MMLU instruction tuning datasets, established the superiority of our distillation approach over the SOTA data selection method LESS. Furthermore, our method demonstrates a good transferability over LLM structures (i.e., OPT to Llama). </p>
<blockquote>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢†åŸŸï¼Œéšç€å¤§å‹æ¨¡å‹è§„æ¨¡çš„å¢åŠ ï¼Œå…¶è®­ç»ƒæˆæœ¬ä¹Ÿéšä¹‹æé«˜ã€‚å› æ­¤ï¼Œè¿«åˆ‡éœ€è¦å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒæ‰€éœ€çš„æ•°æ®é‡è¿›è¡Œç¼©å‡ã€‚ç›¸è¾ƒäºæ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œæ•°æ®è’¸é¦æ–¹æ³•æ—¨åœ¨åˆæˆå°‘é‡æ•°æ®æ ·æœ¬ä»¥å®ç°å…¨æ•°æ®é›†çš„åŸ¹è®­æ•ˆæœï¼Œå¹¶ä¸”å…·æœ‰æ›´å¥½çš„çµæ´»æ€§ã€‚å°½ç®¡å…¶åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå·²ç»å–å¾—äº†æˆåŠŸï¼Œä½†æ–‡æœ¬æ•°æ®çš„ç¦»æ•£æ€§è¿„ä»Šä¸ºæ­¢é˜»ç¢äº†å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„æ¢ç´¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.09818v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒæˆæœ¬éšç€æ¨¡å‹è§„æ¨¡çš„å¢å¤§è€Œå¢åŠ ï¼Œè¿«åˆ‡éœ€è¦å‡å°è®­ç»ƒæ•°æ®çš„å¤§å°ã€‚ç›¸è¾ƒäºæ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œæ•°æ®è’¸é¦æ–¹æ³•å¯ä»¥åˆæˆå°‘é‡æ•°æ®æ ·æœ¬ä»¥å®ç°å…¨æ•°æ®é›†çš„åŸ¹è®­æ•ˆæœï¼Œå…·æœ‰æ›´å¥½çš„çµæ´»æ€§ã€‚å°½ç®¡åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå–å¾—äº†æˆåŠŸï¼Œæ–‡æœ¬æ•°æ®çš„ç¦»æ•£æ€§é˜»ç¢äº†å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„åº”ç”¨æ¢ç´¢ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè½¨è¿¹åŒ¹é…å­¦ä¹ ä¼ªæç¤ºæ•°æ®çš„æ–¹æ³•ï¼Œé€šè¿‡å¯»æ‰¾æœ€è¿‘é‚»IDå®ç°è·¨æ¶æ„è¿ç§»ã€‚åœ¨è’¸é¦è¿‡ç¨‹ä¸­ï¼Œå¼•å…¥æ­£åˆ™åŒ–æŸå¤±ä»¥æé«˜è’¸é¦æ•°æ®çš„ç¨³å¥æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–ä¸ªé€‚ç”¨äºæŒ‡ä»¤è°ƒæ•´ç­‰æ–‡æœ¬ç”Ÿæˆä»»åŠ¡çš„æ•°æ®è’¸é¦å·¥ä½œã€‚åœ¨ARC-Easyå’ŒMMLUæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„è’¸é¦æ–¹æ³•ä¼˜äºç°æœ‰æœ€ä½³çš„æ•°æ®é€‰æ‹©æ–¹æ³•LESSï¼Œå¹¶ä¸”åœ¨LLMç»“æ„ä¹‹é—´å…·æœ‰è‰¯å¥½çš„å¯è¿ç§»æ€§ï¼ˆä¾‹å¦‚ï¼Œä»OPTåˆ°Llamaï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œå­˜åœ¨å¯¹å‡å°è®­ç»ƒæ•°æ®å¤§å°çš„éœ€æ±‚ã€‚</li>
<li>æ•°æ®è’¸é¦æ˜¯ä¸€ç§å¯ä»¥åˆæˆå°‘é‡æ•°æ®æ ·æœ¬ä»¥å®ç°å…¨æ•°æ®é›†åŸ¹è®­æ•ˆæœçš„æ–¹æ³•ï¼Œå…·æœ‰çµæ´»æ€§ã€‚</li>
<li>å°½ç®¡æ•°æ®è’¸é¦åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸæœ‰æ‰€æˆå°±ï¼Œä½†åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„åº”ç”¨ä»é¢ä¸´æ–‡æœ¬æ•°æ®ç¦»æ•£æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡è½¨è¿¹åŒ¹é…å­¦ä¹ ä¼ªæç¤ºæ•°æ®çš„æ–¹æ³•ï¼Œå®ç°è·¨æ¶æ„è¿ç§»ã€‚</li>
<li>åœ¨è’¸é¦è¿‡ç¨‹ä¸­å¼•å…¥æ­£åˆ™åŒ–æŸå¤±ä»¥æé«˜æ•°æ®çš„ç¨³å¥æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶æå‡ºçš„è’¸é¦æ–¹æ³•ä¼˜äºç°æœ‰çš„æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œå¦‚åœ¨ARC-Easyå’ŒMMLUæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.09818">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8526010b6331b76eac25d507ca5a5fc3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45f2698b6d386f101ce28becbb208788.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65a96e8cc1431bd8e6503ae6470e62cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1609a3c7a1dd89423b6c7a8285befa1c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AudioX-Diffusion-Transformer-for-Anything-to-Audio-Generation"><a href="#AudioX-Diffusion-Transformer-for-Anything-to-Audio-Generation" class="headerlink" title="AudioX: Diffusion Transformer for Anything-to-Audio Generation"></a>AudioX: Diffusion Transformer for Anything-to-Audio Generation</h2><p><strong>Authors:Zeyue Tian, Yizhu Jin, Zhaoyang Liu, Ruibin Yuan, Xu Tan, Qifeng Chen, Wei Xue, Yike Guo</strong></p>
<p>Audio and music generation have emerged as crucial tasks in many applications, yet existing approaches face significant limitations: they operate in isolation without unified capabilities across modalities, suffer from scarce high-quality, multi-modal training data, and struggle to effectively integrate diverse inputs. In this work, we propose AudioX, a unified Diffusion Transformer model for Anything-to-Audio and Music Generation. Unlike previous domain-specific models, AudioX can generate both general audio and music with high quality, while offering flexible natural language control and seamless processing of various modalities including text, video, image, music, and audio. Its key innovation is a multi-modal masked training strategy that masks inputs across modalities and forces the model to learn from masked inputs, yielding robust and unified cross-modal representations. To address data scarcity, we curate two comprehensive datasets: vggsound-caps with 190K audio captions based on the VGGSound dataset, and V2M-caps with 6 million music captions derived from the V2M dataset. Extensive experiments demonstrate that AudioX not only matches or outperforms state-of-the-art specialized models, but also offers remarkable versatility in handling diverse input modalities and generation tasks within a unified architecture. The code and datasets will be available at <a target="_blank" rel="noopener" href="https://zeyuet.github.io/AudioX/">https://zeyuet.github.io/AudioX/</a> </p>
<blockquote>
<p>éŸ³é¢‘å’ŒéŸ³ä¹ç”Ÿæˆåœ¨è®¸å¤šåº”ç”¨ä¸­å·²æˆä¸ºè‡³å…³é‡è¦çš„ä»»åŠ¡ï¼Œç„¶è€Œç°æœ‰æ–¹æ³•é¢ä¸´é‡å¤§å±€é™ï¼šå®ƒä»¬åœ¨å­¤ç«‹ç¯å¢ƒä¸­è¿è¡Œï¼Œæ— æ³•è·¨æ¨¡æ€ç»Ÿä¸€èƒ½åŠ›ï¼Œç¼ºä¹é«˜è´¨é‡çš„å¤šæ¨¡æ€è®­ç»ƒæ•°æ®ï¼Œå¹¶ä¸”éš¾ä»¥æœ‰æ•ˆæ•´åˆå„ç§è¾“å…¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AudioXï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ‰©æ•£è½¬æ¢å™¨æ¨¡å‹ï¼Œç”¨äºä»»ä½•å†…å®¹è½¬éŸ³é¢‘å’ŒéŸ³ä¹ç”Ÿæˆã€‚ä¸åŒäºä»¥å‰çš„ç‰¹å®šé¢†åŸŸæ¨¡å‹ï¼ŒAudioXå¯ä»¥é«˜è´¨é‡åœ°ç”Ÿæˆé€šç”¨éŸ³é¢‘å’ŒéŸ³ä¹ï¼ŒåŒæ—¶æä¾›çµæ´»çš„è‡ªç„¶è¯­è¨€æ§åˆ¶ï¼Œæ— ç¼å¤„ç†å„ç§æ¨¡æ€ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è§†é¢‘ã€å›¾åƒã€éŸ³ä¹å’ŒéŸ³é¢‘ã€‚å…¶å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºå¤šæ¨¡æ€æ©è†œè®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä¼šå±è”½è·¨æ¨¡æ€çš„è¾“å…¥ï¼Œå¹¶è¿«ä½¿æ¨¡å‹ä»è¢«å±è”½çš„è¾“å…¥ä¸­å­¦ä¹ ï¼Œä»è€Œäº§ç”Ÿç¨³å¥å’Œç»Ÿä¸€çš„è·¨æ¨¡æ€è¡¨ç¤ºã€‚ä¸ºäº†è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬ç­›é€‰äº†ä¸¤ä¸ªç»¼åˆæ•°æ®é›†ï¼šåŸºäºVGGSoundæ•°æ®é›†çš„19ä¸‡æ¡éŸ³é¢‘å­—å¹•çš„vggsound-capsï¼Œä»¥åŠä»V2Mæ•°æ®é›†ä¸­æ´¾ç”Ÿçš„600ä¸‡æ¡éŸ³ä¹å­—å¹•çš„V2M-capsã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAudioXä¸ä»…ä¸æœ€å…ˆè¿›çš„ä¸“ç”¨æ¨¡å‹ç›¸åŒ¹é…æˆ–è¡¨ç°æ›´ä½³ï¼Œè€Œä¸”åœ¨å¤„ç†å„ç§è¾“å…¥æ¨¡æ€å’Œç”Ÿæˆä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„é€šç”¨æ€§ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨[<a target="_blank" rel="noopener" href="https://zeyuet.github.io/AudioX/]%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://zeyuet.github.io/AudioX/]ä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10522v2">PDF</a> The code and datasets will be available at   <a target="_blank" rel="noopener" href="https://zeyuet.github.io/AudioX/">https://zeyuet.github.io/AudioX/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬å·¥ä½œæå‡ºäº†AudioXï¼Œä¸€ä¸ªç»Ÿä¸€çš„æ‰©æ•£Transformeræ¨¡å‹ï¼Œç”¨äºä»»ä½•å†…å®¹ç”ŸæˆéŸ³é¢‘å’ŒéŸ³ä¹ã€‚è¯¥æ¨¡å‹å¯é«˜è´¨é‡ç”Ÿæˆé€šç”¨éŸ³é¢‘å’ŒéŸ³ä¹ï¼Œæ”¯æŒçµæ´»çš„è‡ªç„¶è¯­è¨€æ§åˆ¶ï¼Œå¹¶èƒ½æ— ç¼å¤„ç†å¤šç§æ¨¡æ€ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è§†é¢‘ã€å›¾åƒã€éŸ³ä¹å’ŒéŸ³é¢‘ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºå¤šæ¨¡æ€æ©è†œè®­ç»ƒç­–ç•¥ï¼Œé€šè¿‡è·¨æ¨¡æ€è¾“å…¥æ©è†œï¼Œè¿«ä½¿æ¨¡å‹ä»æ©è†œè¾“å…¥ä¸­å­¦ä¹ ï¼Œäº§ç”Ÿç¨³å¥ç»Ÿä¸€çš„è·¨æ¨¡æ€è¡¨ç¤ºã€‚ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªå¤§å‹æ•°æ®é›†ï¼šåŸºäºVGGSoundæ•°æ®é›†çš„19ä¸‡éŸ³é¢‘å­—å¹•çš„vggsound-capså’Œä»V2Mæ•°æ®é›†ä¸­æ´¾ç”Ÿçš„6ç™¾ä¸‡éŸ³ä¹å­—å¹•çš„V2M-capsã€‚å®éªŒè¡¨æ˜ï¼ŒAudioXä¸ä»…ä¸æœ€å…ˆè¿›çš„ç‰¹å®šæ¨¡å‹ç›¸åŒ¹é…æˆ–è¡¨ç°æ›´ä½³ï¼Œè€Œä¸”åœ¨ä¸€ä¸ªç»Ÿä¸€æ¶æ„ä¸­å¤„ç†å„ç§è¾“å…¥æ¨¡æ€å’Œç”Ÿæˆä»»åŠ¡æ—¶è¡¨ç°å‡ºå“è¶Šçš„å¤šåŠŸèƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AudioXæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ‰©æ•£Transformeræ¨¡å‹ï¼Œç”¨äºä»»ä½•å†…å®¹ç”ŸæˆéŸ³é¢‘å’ŒéŸ³ä¹ã€‚</li>
<li>å¯ç”Ÿæˆé«˜è´¨é‡é€šç”¨éŸ³é¢‘å’ŒéŸ³ä¹ï¼Œå¹¶æ”¯æŒçµæ´»çš„è‡ªç„¶è¯­è¨€æ§åˆ¶ã€‚</li>
<li>æ— ç¼å¤„ç†å¤šç§æ¨¡æ€ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è§†é¢‘ã€å›¾åƒã€éŸ³ä¹å’ŒéŸ³é¢‘ã€‚</li>
<li>æ ¸å¿ƒåˆ›æ–°åœ¨äºå¤šæ¨¡æ€æ©è†œè®­ç»ƒç­–ç•¥ï¼Œäº§ç”Ÿç¨³å¥ç»Ÿä¸€çš„è·¨æ¨¡æ€è¡¨ç¤ºã€‚</li>
<li>è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œåˆ›å»ºäº†ä¸¤ä¸ªå¤§å‹æ•°æ®é›†ï¼švggsound-capså’ŒV2M-capsã€‚</li>
<li>å®éªŒè¡¨æ˜AudioXæ€§èƒ½ä¼˜è¶Šï¼Œä¸æœ€å…ˆè¿›çš„ç‰¹å®šæ¨¡å‹ç›¸åŒ¹é…æˆ–è¡¨ç°æ›´ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10522">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-994e45ffd7d70f44b376dcc2d0e4396d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fda29bb30da1f0a10d892593bcfec63a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a30d06630359804ef2a6acab6319ed08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f22b662cff6ecbc633c7e71c9e7d7cf3.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="HierarQ-Task-Aware-Hierarchical-Q-Former-for-Enhanced-Video-Understanding"><a href="#HierarQ-Task-Aware-Hierarchical-Q-Former-for-Enhanced-Video-Understanding" class="headerlink" title="HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video   Understanding"></a>HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video   Understanding</h2><p><strong>Authors:Shehreen Azad, Vibhav Vineet, Yogesh Singh Rawat</strong></p>
<p>Despite advancements in multimodal large language models (MLLMs), current approaches struggle in medium-to-long video understanding due to frame and context length limitations. As a result, these models often depend on frame sampling, which risks missing key information over time and lacks task-specific relevance. To address these challenges, we introduce HierarQ, a task-aware hierarchical Q-Former based framework that sequentially processes frames to bypass the need for frame sampling, while avoiding LLMâ€™s context length limitations. We introduce a lightweight two-stream language-guided feature modulator to incorporate task awareness in video understanding, with the entity stream capturing frame-level object information within a short context and the scene stream identifying their broader interactions over longer period of time. Each stream is supported by dedicated memory banks which enables our proposed Hierachical Querying transformer (HierarQ) to effectively capture short and long-term context. Extensive evaluations on 10 video benchmarks across video understanding, question answering, and captioning tasks demonstrate HierarQâ€™s state-of-the-art performance across most datasets, proving its robustness and efficiency for comprehensive video analysis. </p>
<blockquote>
<p>å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å–å¾—äº†è¿›å±•ï¼Œä½†å½“å‰çš„æ–¹æ³•åœ¨ä¸­é•¿è§†é¢‘ç†è§£æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå­˜åœ¨å¸§å’Œä¸Šä¸‹æ–‡é•¿åº¦çš„é™åˆ¶ã€‚å› æ­¤ï¼Œè¿™äº›æ¨¡å‹é€šå¸¸ä¾èµ–äºå¸§é‡‡æ ·ï¼Œè¿™æœ‰å¯èƒ½åœ¨é•¿æ—¶é—´å†…é—æ¼å…³é”®ä¿¡æ¯å¹¶ç¼ºä¹ç‰¹å®šä»»åŠ¡çš„å…³è”æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº† HierarQï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä»»åŠ¡æ„ŸçŸ¥çš„åˆ†å±‚ Q-Former æ¡†æ¶ï¼Œå®ƒèƒ½å¤ŸæŒ‰é¡ºåºå¤„ç†å¸§ï¼Œä»è€Œç»•è¿‡å¸§é‡‡æ ·çš„éœ€è¦ï¼ŒåŒæ—¶é¿å… LLM çš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„åŒæµè¯­è¨€å¼•å¯¼ç‰¹å¾è°ƒåˆ¶å™¨ï¼Œä»¥åœ¨è§†é¢‘ç†è§£ä¸­èå…¥ä»»åŠ¡æ„ŸçŸ¥èƒ½åŠ›ï¼Œå…¶ä¸­å®ä½“æµåœ¨çŸ­è¯­å¢ƒå†…æ•è·å¸§çº§å¯¹è±¡ä¿¡æ¯ï¼Œåœºæ™¯æµåˆ™è¯†åˆ«é•¿æ—¶é—´å†…çš„æ›´å¹¿æ³›çš„äº¤äº’ã€‚æ¯ä¸ªæµéƒ½å—åˆ°ä¸“ç”¨å†…å­˜åº“çš„æ”¯æŒï¼Œè¿™ä½¿å¾—æˆ‘ä»¬æå‡ºçš„åˆ†å±‚æŸ¥è¯¢è½¬æ¢å™¨ï¼ˆHierarQï¼‰å¯ä»¥æœ‰æ•ˆåœ°æ•è·çŸ­æœŸå’Œé•¿æœŸçš„ä¸Šä¸‹æ–‡ã€‚åœ¨è§†é¢‘ç†è§£ã€é—®ç­”å’Œå­—å¹•ä»»åŠ¡ç­‰åä¸ªè§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒHierarQ åœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šçš„æ€§èƒ½éƒ½å¤„äºæœ€æ–°æ°´å¹³ï¼Œè¯æ˜äº†å…¶åœ¨ç»¼åˆè§†é¢‘åˆ†æä¸­çš„ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08585v2">PDF</a> Accepted in CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä¸­ç­‰è‡³é•¿è§†é¢‘æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå­˜åœ¨å¸§å’Œä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ã€‚å› æ­¤ï¼Œè¿™äº›æ¨¡å‹é€šå¸¸ä¾èµ–äºå¸§é‡‡æ ·ï¼Œè¿™å¯èƒ½ä¼šé”™è¿‡å…³é”®ä¿¡æ¯å¹¶ç¼ºä¹ç‰¹å®šä»»åŠ¡çš„å…³è”æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†HierarQæ¡†æ¶ï¼Œå®ƒé€šè¿‡ä»»åŠ¡æ„ŸçŸ¥çš„å±‚æ¬¡å‹Q-Formerè¿›è¡Œå¸§çš„åºåˆ—å¤„ç†ï¼Œç»•è¿‡å¸§é‡‡æ ·çš„éœ€è¦ï¼Œé¿å…LLMçš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„åŒæµè¯­è¨€å¼•å¯¼ç‰¹å¾è°ƒåˆ¶å™¨ï¼Œå°†ä»»åŠ¡æ„ŸçŸ¥èå…¥è§†é¢‘ç†è§£ä¸­ï¼Œå®ä½“æµæ•æ‰çŸ­ä¸Šä¸‹æ–‡å†…çš„å¸§çº§å¯¹è±¡ä¿¡æ¯ï¼Œåœºæ™¯æµè¯†åˆ«é•¿æ—¶é—´å†…çš„æ›´å¹¿æ³›çš„äº¤äº’ã€‚æ¯ä¸ªæµéƒ½å—åˆ°ä¸“ç”¨å†…å­˜åº“çš„æ”¯æŒï¼Œè¿™ä½¿å¾—æˆ‘ä»¬æå‡ºçš„å±‚æ¬¡æŸ¥è¯¢è½¬æ¢å™¨ï¼ˆHierarQï¼‰èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰çŸ­æœŸå’Œé•¿æœŸçš„ä¸Šä¸‹æ–‡ã€‚åœ¨è·¨è¶Šè§†é¢‘ç†è§£ã€é—®ç­”å’Œå­—å¹•ä»»åŠ¡çš„åä¸ªè§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒHierarQåœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨å…¨é¢çš„è§†é¢‘åˆ†æä¸­çš„ç¨³å¥æ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä¸­ç­‰è‡³é•¿è§†é¢‘æ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸»è¦å› ä¸ºå¸§å’Œä¸Šä¸‹æ–‡é•¿åº¦çš„é™åˆ¶ã€‚</li>
<li>ç°æœ‰æ¨¡å‹å¸¸ä¾èµ–å¸§é‡‡æ ·ï¼Œè¿™å¯èƒ½é”™è¿‡å…³é”®ä¿¡æ¯å¹¶ç¼ºä¹ç‰¹å®šä»»åŠ¡çš„å…³è”æ€§ã€‚</li>
<li>HierarQæ¡†æ¶é€šè¿‡ä»»åŠ¡æ„ŸçŸ¥çš„å±‚æ¬¡å‹Q-Formerè¿›è¡Œå¸§çš„åºåˆ—å¤„ç†ï¼Œç»•è¿‡å¸§é‡‡æ ·çš„é™åˆ¶ã€‚</li>
<li>HierarQå¼•å…¥äº†è¯­è¨€å¼•å¯¼çš„ç‰¹å¾è°ƒåˆ¶å™¨ï¼ŒåŒ…å«å®ä½“æµå’Œåœºæ™¯æµä»¥åˆ†åˆ«æ•æ‰å¸§çº§å’Œåœºæ™¯çº§çš„ä¿¡æ¯ã€‚</li>
<li>å®ä½“æµå’Œåœºæ™¯æµéƒ½å—åˆ°ä¸“ç”¨å†…å­˜åº“çš„æ”¯æŒï¼Œä»¥æœ‰æ•ˆåœ°æ•æ‰çŸ­æœŸå’Œé•¿æœŸçš„ä¸Šä¸‹æ–‡ã€‚</li>
<li>HierarQåœ¨å¤šä¸ªè§†é¢‘ç†è§£ã€é—®ç­”å’Œå­—å¹•ä»»åŠ¡çš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08585">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4cc7b3e3ec03af59ce925e35b9a3b77e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b39db4d159f3ecd80ed2300cdf0799e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-694c96fdb21142f0449281a55bba6a3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc300cf9d458b1b9bd2ae68dc6b3c099.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CallNavi-A-Challenge-and-Empirical-Study-on-LLM-Function-Calling-and-Routing"><a href="#CallNavi-A-Challenge-and-Empirical-Study-on-LLM-Function-Calling-and-Routing" class="headerlink" title="CallNavi, A Challenge and Empirical Study on LLM Function Calling and   Routing"></a>CallNavi, A Challenge and Empirical Study on LLM Function Calling and   Routing</h2><p><strong>Authors:Yewei Song, Xunzhu Tang, Cedric Lothritz, Saad Ezzini, Jacques Klein, TegawendÃ© F. BissyandÃ©, Andrey Boytsov, Ulrick Ble, Anne Goujon</strong></p>
<p>API-driven chatbot systems are increasingly integral to software engineering applications, yet their effectiveness hinges on accurately generating and executing API calls. This is particularly challenging in scenarios requiring multi-step interactions with complex parameterization and nested API dependencies. Addressing these challenges, this work contributes to the evaluation and assessment of AI-based software development through three key advancements: (1) the introduction of a novel dataset specifically designed for benchmarking API function selection, parameter generation, and nested API execution; (2) an empirical evaluation of state-of-the-art language models, analyzing their performance across varying task complexities in API function generation and parameter accuracy; and (3) a hybrid approach to API routing, combining general-purpose large language models for API selection with fine-tuned models and prompt engineering for parameter generation. These innovations significantly improve API execution in chatbot systems, offering practical methodologies for enhancing software design, testing, and operational workflows in real-world software engineering contexts. </p>
<blockquote>
<p>APIé©±åŠ¨çš„èŠå¤©æœºå™¨äººç³»ç»Ÿè¶Šæ¥è¶Šæˆä¸ºè½¯ä»¶å·¥ç¨‹æŠ€æœ¯åº”ç”¨çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä½†å…¶æœ‰æ•ˆæ€§å–å†³äºAPIè°ƒç”¨çš„å‡†ç¡®ç”Ÿæˆå’Œæ‰§è¡Œã€‚åœ¨éœ€è¦å¤šæ­¥äº¤äº’ã€å¤æ‚å‚æ•°åŒ–å’ŒåµŒå¥—APIä¾èµ–çš„åœºæ™¯ä¸‹ï¼Œè¿™å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶é€šè¿‡ä¸‰ä¸ªå…³é”®è¿›å±•ä¸ºåŸºäºäººå·¥æ™ºèƒ½çš„è½¯ä»¶å¼€å‘è¯„ä¼°åšå‡ºäº†è´¡çŒ®ï¼šï¼ˆ1ï¼‰å¼•å…¥ä¸“é—¨ç”¨äºåŸºå‡†æµ‹è¯•APIå‡½æ•°é€‰æ‹©ã€å‚æ•°ç”Ÿæˆå’ŒåµŒå¥—APIæ‰§è¡Œçš„æ–°å‹æ•°æ®é›†ï¼›ï¼ˆ2ï¼‰å¯¹æœ€å…ˆè¿›è¯­è¨€æ¨¡å‹çš„å®è¯è¯„ä¼°ï¼Œåˆ†æå…¶åœ¨ä¸åŒä»»åŠ¡å¤æ‚æ€§å’ŒAPIå‡½æ•°ç”Ÿæˆçš„å‚æ•°å‡†ç¡®æ€§æ–¹é¢çš„æ€§èƒ½ï¼›ï¼ˆ3ï¼‰ä¸€ç§APIè·¯ç”±çš„æ··åˆæ–¹æ³•ï¼Œç»“åˆç”¨äºAPIé€‰æ‹©çš„é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸é’ˆå¯¹å‚æ•°ç”Ÿæˆçš„ç»è¿‡å¾®è°ƒæ¨¡å‹å’Œæç¤ºå·¥ç¨‹ã€‚è¿™äº›åˆ›æ–°æ˜¾è‘—æé«˜äº†èŠå¤©æœºå™¨äººç³»ç»Ÿä¸­çš„APIæ‰§è¡Œèƒ½åŠ›ï¼Œä¸ºå¢å¼ºç°å®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹æŠ€æœ¯ç¯å¢ƒä¸­çš„è½¯ä»¶è®¾è®¡ã€æµ‹è¯•å’Œæ“ä½œæµç¨‹æä¾›äº†å®ç”¨æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05255v2">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨è½¯ä»¶å·¥ç¨‹åº”ç”¨ä¸­ï¼ŒAPIé©±åŠ¨çš„èŠå¤©æœºå™¨äººç³»ç»Ÿè¶Šæ¥è¶Šé‡è¦ï¼Œä½†å…¶æœ‰æ•ˆæ€§å–å†³äºå‡†ç¡®ç”Ÿæˆå’Œæ‰§è¡ŒAPIè°ƒç”¨ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡ä¸‰ä¸ªå…³é”®è¿›å±•å¯¹AIåœ¨è½¯ä»¶å¼€å‘ä¸­çš„åº”ç”¨è¿›è¡Œäº†è¯„ä¼°ï¼šå¼•å…¥ä¸“é—¨ç”¨äºåŸºå‡†æµ‹è¯•APIåŠŸèƒ½é€‰æ‹©ã€å‚æ•°ç”Ÿæˆå’ŒåµŒå¥—APIæ‰§è¡Œçš„æ–°æ•°æ®é›†ï¼›å¯¹å…ˆè¿›è¯­è¨€æ¨¡å‹çš„å®è¯è¯„ä¼°ï¼›ä»¥åŠAPIè·¯ç”±çš„æ··åˆæ–¹æ³•ã€‚è¿™äº›åˆ›æ–°å¤§å¤§æé«˜äº†APIåœ¨èŠå¤©æœºå™¨äººç³»ç»Ÿä¸­çš„æ‰§è¡Œèƒ½åŠ›ï¼Œä¸ºå¢å¼ºè½¯ä»¶è®¾è®¡ã€æµ‹è¯•å’Œæ“ä½œæµç¨‹æä¾›äº†å®ç”¨æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>APIé©±åŠ¨çš„èŠå¤©æœºå™¨äººç³»ç»Ÿåœ¨è½¯ä»¶å·¥ç¨‹ä¸­è¶Šæ¥è¶Šé‡è¦ã€‚</li>
<li>å‡†ç¡®ç”Ÿæˆå’Œæ‰§è¡ŒAPIè°ƒç”¨æ˜¯ç¡®ä¿ç³»ç»Ÿæœ‰æ•ˆæ€§çš„å…³é”®ã€‚</li>
<li>å·¥ä½œè´¡çŒ®ä¸€ï¼šå¼•å…¥ä¸“é—¨ç”¨äºAPIåŠŸèƒ½é€‰æ‹©ã€å‚æ•°ç”Ÿæˆå’ŒåµŒå¥—APIæ‰§è¡ŒåŸºå‡†æµ‹è¯•çš„æ–°æ•°æ®é›†ã€‚</li>
<li>å·¥ä½œè´¡çŒ®äºŒï¼šå¯¹å…ˆè¿›è¯­è¨€æ¨¡å‹çš„æ€§èƒ½è¿›è¡Œäº†å®è¯è¯„ä¼°ï¼Œåˆ†æå…¶åœ¨ä¸åŒä»»åŠ¡å¤æ‚åº¦å’Œå‚æ•°å‡†ç¡®æ€§æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ··åˆAPIè·¯ç”±æ–¹æ³•ï¼Œç»“åˆé€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒAPIé€‰æ‹©å’Œç²¾ç»†è°ƒæ•´æ¨¡å‹ä»¥åŠæç¤ºå·¥ç¨‹è¿›è¡Œå‚æ•°ç”Ÿæˆã€‚</li>
<li>è¿™äº›åˆ›æ–°å¤§å¤§æé«˜äº†èŠå¤©æœºå™¨äººç³»ç»Ÿä¸­APIçš„æ‰§è¡Œèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-93e6a28fd3e8e24400096804db0cd9e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82f4e24af93d4598a89bdc94e8d2f25e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d0ce738a8e65e4c94becacc5fdd38db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5241d54f849a1a8d5fcabf37d2412fa0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a3a6e91321eb61471c20fa6f64f4e3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ccf4ac8a9dfdf19a062679a999d8797b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21ecfd436630c51722bf88975d4e7c3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63b7036cbbd340ca450859f44c86e7a0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="3D-LLaVA-Towards-Generalist-3D-LMMs-with-Omni-Superpoint-Transformer"><a href="#3D-LLaVA-Towards-Generalist-3D-LMMs-with-Omni-Superpoint-Transformer" class="headerlink" title="3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer"></a>3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer</h2><p><strong>Authors:Jiajun Deng, Tianyu He, Li Jiang, Tianyu Wang, Feras Dayoub, Ian Reid</strong></p>
<p>Current 3D Large Multimodal Models (3D LMMs) have shown tremendous potential in 3D-vision-based dialogue and reasoning. However, how to further enhance 3D LMMs to achieve fine-grained scene understanding and facilitate flexible human-agent interaction remains a challenging problem. In this work, we introduce 3D-LLaVA, a simple yet highly powerful 3D LMM designed to act as an intelligent assistant in comprehending, reasoning, and interacting with the 3D world. Unlike existing top-performing methods that rely on complicated pipelines-such as offline multi-view feature extraction or additional task-specific heads-3D-LLaVA adopts a minimalist design with integrated architecture and only takes point clouds as input. At the core of 3D-LLaVA is a new Omni Superpoint Transformer (OST), which integrates three functionalities: (1) a visual feature selector that converts and selects visual tokens, (2) a visual prompt encoder that embeds interactive visual prompts into the visual token space, and (3) a referring mask decoder that produces 3D masks based on text description. This versatile OST is empowered by the hybrid pretraining to obtain perception priors and leveraged as the visual connector that bridges the 3D data to the LLM. After performing unified instruction tuning, our 3D-LLaVA reports impressive results on various benchmarks. </p>
<blockquote>
<p>å½“å‰ï¼Œ3Då¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆ3D LMMsï¼‰åœ¨åŸºäº3Dè§†è§‰çš„å¯¹è¯å’Œæ¨ç†æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¦‚ä½•è¿›ä¸€æ­¥å¢å¼º3D LMMsä»¥å®ç°ç²¾ç»†åœºæ™¯ç†è§£å’Œä¿ƒè¿›çµæ´»çš„äººæœºäº¤äº’ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†åä¸ºâ€œä¸‰ç»´LLaVAâ€çš„ç®€å•è€Œå¼ºå¤§çš„ä¸‰ç»´æ¨¡å‹ã€‚å®ƒæ—¨åœ¨å……å½“æ™ºèƒ½åŠ©ç†ï¼Œç”¨äºç†è§£ã€æ¨ç†å’Œä¸ä¸‰ç»´ä¸–ç•Œäº¤äº’ã€‚ä¸åŒäºç°æœ‰é«˜æ€§èƒ½çš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºå¤æ‚çš„ç®¡é“ï¼Œå¦‚ç¦»çº¿å¤šè§†å›¾ç‰¹å¾æå–æˆ–é¢å¤–çš„ç‰¹å®šä»»åŠ¡å¤´ï¼Œä¸‰ç»´LLaVAé‡‡ç”¨æç®€è®¾è®¡ï¼Œå…·æœ‰é›†æˆæ¶æ„ï¼Œä»…é‡‡ç”¨ç‚¹äº‘ä½œä¸ºè¾“å…¥ã€‚ä¸‰ç»´LLaVAçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå…¨æ–°çš„Omni Superpoint Transformerï¼ˆOSTï¼‰ï¼Œå®ƒé›†æˆäº†ä¸‰ç§åŠŸèƒ½ï¼šï¼ˆ1ï¼‰è§†è§‰ç‰¹å¾é€‰æ‹©å™¨ï¼Œç”¨äºè½¬æ¢å’Œé€‰æ‹©è§†è§‰ä»¤ç‰Œï¼›ï¼ˆ2ï¼‰è§†è§‰æç¤ºç¼–ç å™¨å°†äº¤äº’å¼è§†è§‰æç¤ºåµŒå…¥è§†è§‰ä»¤ç‰Œç©ºé—´ï¼›ï¼ˆ3ï¼‰å‚ç…§æ©ç è§£ç å™¨æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆä¸‰ç»´æ©ç ã€‚è¿™ç§é€šç”¨OSTé€šè¿‡æ··åˆé¢„è®­ç»ƒè·å¾—æ„ŸçŸ¥å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶è¢«ç”¨ä½œè¿æ¥ä¸‰ç»´æ•°æ®å’ŒLLMçš„è§†è§‰è¿æ¥å™¨ã€‚ç»è¿‡ç»Ÿä¸€çš„æŒ‡ä»¤å¾®è°ƒåï¼Œæˆ‘ä»¬çš„ä¸‰ç»´LLaVAåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01163v2">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ä¸‰ç»´ä¸–ç•Œç†è§£ã€æ¨ç†å’Œäº¤äº’çš„æ™ºèƒ½åŠ©æ‰‹â€”â€”åä¸ºâ€œLLaVAâ€çš„å¤§å‹ä¸‰ç»´å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLarge Multimodal Modelï¼‰ã€‚å®ƒé‡‡ç”¨æç®€è®¾è®¡ï¼Œä»…ä½¿ç”¨ç‚¹äº‘ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡æ··åˆé¢„è®­ç»ƒå¼ºåŒ–æ„ŸçŸ¥å…ˆéªŒã€‚å…¶æ ¸å¿ƒä¸ºå…¨æ–°çš„Omni Superpoint Transformerï¼ˆOSTï¼‰ï¼Œå®ƒèåˆäº†è§†è§‰ç‰¹å¾é€‰æ‹©ã€äº¤äº’å¼è§†è§‰æç¤ºåµŒå…¥å’ŒåŸºäºæ–‡æœ¬æè¿°ç”Ÿæˆä¸‰ç»´æ©ç çš„åŠŸèƒ½ã€‚è¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¤§å‹ä¸‰ç»´å¤šæ¨¡æ€æ¨¡å‹ï¼ˆ3D LMMsï¼‰åœ¨åŸºäºä¸‰ç»´è§†è§‰çš„å¯¹è¯å’Œæ¨ç†æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºâ€œLLaVAâ€çš„æ–°å‹æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°åœºæ™¯ç²¾ç»†åŒ–ç†è§£åŠçµæ´»çš„äººæœºäº¤äº’ã€‚</li>
<li>LLaVAæ¨¡å‹è®¾è®¡æç®€ï¼Œä»¥ç‚¹äº‘ä¸ºè¾“å…¥ï¼Œé€šè¿‡é›†æˆæ¶æ„è¿›è¡Œä¿¡æ¯å¤„ç†ã€‚</li>
<li>æ¨¡å‹æ ¸å¿ƒä¸ºOmni Superpoint Transformerï¼ˆOSTï¼‰ï¼Œèåˆäº†è§†è§‰ç‰¹å¾é€‰æ‹©ã€è§†è§‰æç¤ºåµŒå…¥å’ŒåŸºäºæ–‡æœ¬æè¿°ç”Ÿæˆä¸‰ç»´æ©ç åŠŸèƒ½ã€‚</li>
<li>LLaVAé‡‡ç”¨æ··åˆé¢„è®­ç»ƒï¼Œé€šè¿‡æ„ŸçŸ¥å…ˆéªŒå¢å¼ºæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹å®ç°äº†ç»Ÿä¸€çš„æŒ‡ä»¤è°ƒæ•´ä¼˜åŒ–æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01163">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a8632e02d3d0161f085427aac5099562.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-962c1e5ea2fadb993a0833faf7ec89fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac18c8327b85eb1266e60f8882f07ace.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2e05bd2bda7bab9045ddbfccfbc3841.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-251c5a8b9a6ea83c7fa7385d9bdef9a7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="QUART-Online-Latency-Free-Large-Multimodal-Language-Model-for-Quadruped-Robot-Learning"><a href="#QUART-Online-Latency-Free-Large-Multimodal-Language-Model-for-Quadruped-Robot-Learning" class="headerlink" title="QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped   Robot Learning"></a>QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped   Robot Learning</h2><p><strong>Authors:Xinyang Tong, Pengxiang Ding, Yiguo Fan, Donglin Wang, Wenjie Zhang, Can Cui, Mingyang Sun, Han Zhao, Hongyin Zhang, Yonghao Dang, Siteng Huang, Shangke Lyu</strong></p>
<p>This paper addresses the inherent inference latency challenges associated with deploying multimodal large language models (MLLM) in quadruped vision-language-action (QUAR-VLA) tasks. Our investigation reveals that conventional parameter reduction techniques ultimately impair the performance of the language foundation model during the action instruction tuning phase, making them unsuitable for this purpose. We introduce a novel latency-free quadruped MLLM model, dubbed QUART-Online, designed to enhance inference efficiency without degrading the performance of the language foundation model. By incorporating Action Chunk Discretization (ACD), we compress the original action representation space, mapping continuous action values onto a smaller set of discrete representative vectors while preserving critical information. Subsequently, we fine-tune the MLLM to integrate vision, language, and compressed actions into a unified semantic space. Experimental results demonstrate that QUART-Online operates in tandem with the existing MLLM system, achieving real-time inference in sync with the underlying controller frequency, significantly boosting the success rate across various tasks by 65%. Our project page is <a target="_blank" rel="noopener" href="https://quart-online.github.io/">https://quart-online.github.io</a>. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å››è¶³è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆQUAR-VLAï¼‰ä»»åŠ¡ä¸­éƒ¨ç½²å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ‰€é¢ä¸´çš„å›ºæœ‰æ¨ç†å»¶è¿ŸæŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥å‘ç°ï¼Œä¼ ç»Ÿçš„å‚æ•°å‡å°‘æŠ€æœ¯æœ€ç»ˆä¼šæŸå®³è¯­è¨€åŸºç¡€æ¨¡å‹åœ¨åŠ¨ä½œæŒ‡ä»¤è°ƒæ•´é˜¶æ®µçš„æ€§èƒ½ï¼Œä½¿å…¶ä¸é€‚åˆæ­¤ç›®çš„ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ— å»¶è¿Ÿå››è¶³MLLMæ¨¡å‹ï¼Œåä¸ºQUART-Onlineï¼Œæ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¸é™ä½è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥åŠ¨ä½œç‰‡æ®µç¦»æ•£åŒ–ï¼ˆACDï¼‰ï¼Œæˆ‘ä»¬å‹ç¼©äº†åŸå§‹åŠ¨ä½œè¡¨ç¤ºç©ºé—´ï¼Œå°†è¿ç»­åŠ¨ä½œå€¼æ˜ å°„åˆ°ä¸€ç»„è¾ƒå°çš„ç¦»æ•£ä»£è¡¨å‘é‡ä¸Šï¼ŒåŒæ—¶ä¿ç•™å…³é”®ä¿¡æ¯ã€‚éšåï¼Œæˆ‘ä»¬å¯¹MLLMè¿›è¡Œå¾®è°ƒï¼Œä»¥å°†è§†è§‰ã€è¯­è¨€å’Œå‹ç¼©åŠ¨ä½œé›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQUART-Onlineä¸ç°æœ‰MLLMç³»ç»ŸååŒå·¥ä½œï¼Œå®ç°ä¸åº•å±‚æ§åˆ¶å™¨é¢‘ç‡åŒæ­¥çš„å®æ—¶æ¨ç†ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­çš„æˆåŠŸç‡æé«˜äº†65%ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://quart-online.github.io./">https://quart-online.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15576v4">PDF</a> Accepted to ICRA 2025; Github page: <a target="_blank" rel="noopener" href="https://quart-online.github.io/">https://quart-online.github.io</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†éƒ¨ç½²å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å››è¶³è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆQUAR-VLAï¼‰ä»»åŠ¡æ—¶é¢ä¸´çš„å›ºæœ‰æ¨ç†å»¶è¿ŸæŒ‘æˆ˜ã€‚ç ”ç©¶å‘ç°ï¼Œä¼ ç»Ÿçš„å‚æ•°å‡å°‘æŠ€æœ¯æœ€ç»ˆä¼šæŸå®³è¯­è¨€åŸºç¡€æ¨¡å‹åœ¨åŠ¨ä½œæŒ‡ä»¤è°ƒæ•´é˜¶æ®µçš„æ€§èƒ½ï¼Œä½¿å…¶ä¸é€‚åˆæ­¤ç”¨é€”ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— å»¶è¿Ÿå››è¶³MLLMæ¨¡å‹ï¼Œåä¸ºQUART-Onlineï¼Œæ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¸é™ä½è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥åŠ¨ä½œå—ç¦»æ•£åŒ–ï¼ˆACDï¼‰æŠ€æœ¯ï¼Œè¯¥æ¨¡å‹å‹ç¼©äº†åŸå§‹åŠ¨ä½œè¡¨ç¤ºç©ºé—´ï¼Œå°†è¿ç»­åŠ¨ä½œå€¼æ˜ å°„åˆ°ä¸€ç»„è¾ƒå°çš„ç¦»æ•£ä»£è¡¨å‘é‡ä¸Šï¼ŒåŒæ—¶ä¿ç•™äº†å…³é”®ä¿¡æ¯ã€‚éšåï¼Œå¯¹MLLMè¿›è¡Œå¾®è°ƒï¼Œä»¥å°†è§†è§‰ã€è¯­è¨€å’Œå‹ç¼©åŠ¨ä½œé›†æˆåˆ°ç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQUART-Onlineä¸ç°æœ‰MLLMç³»ç»ŸååŒå·¥ä½œï¼Œå®ç°äº†ä¸åº•å±‚æ§åˆ¶å™¨é¢‘ç‡åŒæ­¥çš„å®æ—¶æ¨ç†ï¼ŒæˆåŠŸæé«˜äº†å„é¡¹ä»»åŠ¡çš„æˆåŠŸç‡è¾¾65%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å››è¶³è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆQUAR-VLAï¼‰ä»»åŠ¡ä¸­çš„æ¨ç†å»¶è¿Ÿé—®é¢˜ã€‚</li>
<li>ä¼ ç»Ÿçš„å‚æ•°å‡å°‘æŠ€æœ¯ä¼šæŸå®³è¯­è¨€åŸºç¡€æ¨¡å‹åœ¨åŠ¨ä½œæŒ‡ä»¤è°ƒæ•´é˜¶æ®µçš„æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†æ–°å‹çš„æ— å»¶è¿Ÿå››è¶³MLLMæ¨¡å‹â€”â€”QUART-Onlineï¼Œæ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡ä¸”ä¸å½±å“è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>QUART-Onlineé€šè¿‡å¼•å…¥åŠ¨ä½œå—ç¦»æ•£åŒ–ï¼ˆACDï¼‰æŠ€æœ¯ï¼Œå‹ç¼©äº†åŠ¨ä½œè¡¨ç¤ºç©ºé—´ã€‚</li>
<li>QUART-Onlineå®ç°äº†ä¸ç°æœ‰MLLMç³»ç»Ÿçš„ååŒå·¥ä½œï¼Œå®ç°äº†å®æ—¶æ¨ç†ã€‚</li>
<li>QUART-Onlineæ˜¾è‘—æé«˜äº†å„é¡¹ä»»åŠ¡çš„æˆåŠŸç‡ï¼Œè¾¾åˆ°65%ã€‚</li>
<li>é¡¹ç›®çš„åœ¨çº¿èµ„æºå¯é€šè¿‡<a target="_blank" rel="noopener" href="https://quart-online.github.ioè¿½è®¿./">https://quart-online.github.ioè®¿é—®ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-490c06ef9bd76d213eda9831b7004aa1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d30d5b758629aee84aab79c5afdf7dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ce948ac2f4b9ff0aaec327e5ab8e5c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ff60e5751fe1c420784b8f1a03c44d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ec980b6d0eb7246a89afda6c4045dfc.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MedMax-Mixed-Modal-Instruction-Tuning-for-Training-Biomedical-Assistants"><a href="#MedMax-Mixed-Modal-Instruction-Tuning-for-Training-Biomedical-Assistants" class="headerlink" title="MedMax: Mixed-Modal Instruction Tuning for Training Biomedical   Assistants"></a>MedMax: Mixed-Modal Instruction Tuning for Training Biomedical   Assistants</h2><p><strong>Authors:Hritik Bansal, Daniel Israel, Siyan Zhao, Shufan Li, Tung Nguyen, Aditya Grover</strong></p>
<p>Recent advancements in mixed-modal generative have opened new avenues for developing unified biomedical assistants capable of analyzing biomedical images, answering complex questions about them, and generating multimodal patient reports. However, existing datasets face challenges such as small sizes, limited coverage of biomedical tasks and domains, and a reliance on narrow sources. To address these gaps, we present MedMax, a large-scale multimodal biomedical instruction-tuning dataset for mixed-modal foundation models. With 1.47 million instances, MedMax encompasses a diverse range of tasks, including interleaved image-text generation, biomedical image captioning and generation, visual chat, and report understanding. These tasks span knowledge across diverse biomedical domains, including radiology and histopathology, grounded in medical papers and YouTube videos. Subsequently, we fine-tune a mixed-modal foundation model on the MedMax dataset, achieving significant performance improvements: a 26% gain over the Chameleon model and an 18.3% improvement over GPT-4o across 12 downstream biomedical visual question-answering tasks. Finally, we introduce a unified evaluation suite for biomedical tasks to guide the development of mixed-modal biomedical AI assistants. The data, model, and code is available at <a target="_blank" rel="noopener" href="https://mint-medmax.github.io/">https://mint-medmax.github.io/</a>. </p>
<blockquote>
<p>è¿‘æœŸæ··åˆæ¨¡æ€ç”ŸæˆæŠ€æœ¯çš„è¿›å±•ä¸ºå¼€å‘èƒ½å¤Ÿåˆ†æç”Ÿç‰©åŒ»å­¦å›¾åƒã€å›ç­”å…³äºå®ƒä»¬çš„å¤æ‚é—®é¢˜å¹¶ç”Ÿæˆå¤šæ¨¡æ€æ‚£è€…æŠ¥å‘Šçš„ç»Ÿä¸€ç”Ÿç‰©åŒ»å­¦åŠ©ç†å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚ç„¶è€Œï¼Œç°æœ‰æ•°æ®é›†é¢ä¸´è§„æ¨¡è¾ƒå°ã€ç”Ÿç‰©åŒ»å­¦ä»»åŠ¡å’Œé¢†åŸŸè¦†ç›–æœ‰é™ä»¥åŠä¾èµ–ç‹­çª„æ¥æºç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MedMaxï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ··åˆæ¨¡æ€åŸºç¡€æ¨¡å‹çš„å¤§å‹å¤šæ¨¡æ€ç”Ÿç‰©åŒ»å­¦æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚MedMaxåŒ…å«147ä¸‡ä¸ªå®ä¾‹ï¼Œæ¶µç›–å„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬äº¤æ›¿å›¾åƒæ–‡æœ¬ç”Ÿæˆã€ç”Ÿç‰©åŒ»å­¦å›¾åƒæè¿°å’Œç”Ÿæˆã€è§†è§‰èŠå¤©å’ŒæŠ¥å‘Šç†è§£ã€‚è¿™äº›ä»»åŠ¡æ¶µç›–äº†æ”¾å°„å­¦å’Œç—…ç†ç”Ÿç†å­¦ç­‰å¤šæ ·åŒ–çš„ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçŸ¥è¯†ï¼ŒåŸºäºåŒ»å­¦è®ºæ–‡å’ŒYouTubeè§†é¢‘ã€‚éšåï¼Œæˆ‘ä»¬åœ¨MedMaxæ•°æ®é›†ä¸Šå¯¹æ··åˆæ¨¡æ€åŸºç¡€æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼šåœ¨12ä¸ªä¸‹æ¸¸ç”Ÿç‰©åŒ»å­¦è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šï¼Œç›¸å¯¹äºå˜è‰²é¾™æ¨¡å‹æé«˜äº†26%ï¼Œç›¸å¯¹äºGPT-4oæé«˜äº†18.3%ã€‚æœ€åï¼Œæˆ‘ä»¬ä¸ºç”Ÿç‰©åŒ»å­¦ä»»åŠ¡å¼•å…¥äº†ä¸€å¥—ç»Ÿä¸€çš„è¯„ä¼°å¥—ä»¶ï¼Œä»¥æŒ‡å¯¼æ··åˆæ¨¡æ€ç”Ÿç‰©åŒ»å­¦äººå·¥æ™ºèƒ½åŠ©ç†çš„å¼€å‘ã€‚æ•°æ®ã€æ¨¡å‹å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://mint-medmax.github.io/%E8%8E%B7%E5%8F%96%E3%80%82">https://mint-medmax.github.io/è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12661v2">PDF</a> 29 pages</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸæ··åˆæ¨¡æ€ç”ŸæˆæŠ€æœ¯çš„è¿›å±•ä¸ºå¼€å‘èƒ½å¤Ÿåˆ†æç”Ÿç‰©åŒ»å­¦å›¾åƒã€å›ç­”ç›¸å…³é—®é¢˜å¹¶ç”Ÿæˆå¤šæ¨¡æ€æ‚£è€…æŠ¥å‘Šçš„ç»Ÿä¸€ç”Ÿç‰©åŒ»å­¦åŠ©ç†æä¾›äº†æ–°çš„é€”å¾„ã€‚é’ˆå¯¹ç°æœ‰æ•°æ®é›†é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚è§„æ¨¡å°ã€ç”Ÿç‰©åŒ»å­¦ä»»åŠ¡å’Œé¢†åŸŸè¦†ç›–æœ‰é™ä»¥åŠå¯¹ç‹­çª„æ¥æºçš„ä¾èµ–ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MedMaxï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡æ€ç”Ÿç‰©åŒ»å­¦æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œç”¨äºæ··åˆæ¨¡æ€åŸºç¡€æ¨¡å‹ã€‚MedMaxåŒ…å«147ä¸‡ä¸ªå®ä¾‹ï¼Œæ¶µç›–å¤šç§ä»»åŠ¡ï¼Œå¦‚äº¤é”™å›¾åƒæ–‡æœ¬ç”Ÿæˆã€ç”Ÿç‰©åŒ»å­¦å›¾åƒæè¿°å’Œç”Ÿæˆã€è§†è§‰èŠå¤©å’ŒæŠ¥å‘Šç†è§£ç­‰ï¼Œè·¨è¶Šä¸åŒçš„ç”Ÿç‰©åŒ»å­¦é¢†åŸŸï¼Œå¦‚æ”¾å°„å­¦å’Œç—…ç†å­¦ã€‚åœ¨MedMaxæ•°æ®é›†ä¸Šå¾®è°ƒæ··åˆæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨12ä¸ªä¸‹æ¸¸ç”Ÿç‰©åŒ»å­¦è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šåˆ†åˆ«è¾ƒChameleonæ¨¡å‹å’ŒGPT-4oæé«˜äº†26%å’Œ18.3%ã€‚æœ€åï¼Œæˆ‘ä»¬ä¸ºç”Ÿç‰©åŒ»å­¦ä»»åŠ¡å¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°å¥—ä»¶ï¼Œä»¥æŒ‡å¯¼æ··åˆæ¨¡æ€ç”Ÿç‰©åŒ»å­¦äººå·¥æ™ºèƒ½åŠ©ç†çš„å¼€å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ··åˆæ¨¡æ€ç”ŸæˆæŠ€æœ¯çš„æœ€æ–°è¿›å±•ä¸ºç”Ÿç‰©åŒ»å­¦åŠ©ç†çš„å¼€å‘æä¾›äº†æ–°çš„æœºä¼šã€‚</li>
<li>MedMaxæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡æ€ç”Ÿç‰©åŒ»å­¦æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼Œç”¨äºæ··åˆæ¨¡æ€åŸºç¡€æ¨¡å‹ã€‚</li>
<li>MedMaxåŒ…å«å¤šæ ·çš„ä»»åŠ¡ï¼Œå¦‚å›¾åƒæ–‡æœ¬ç”Ÿæˆã€ç”Ÿç‰©åŒ»å­¦å›¾åƒæè¿°å’Œç”Ÿæˆã€è§†è§‰èŠå¤©å’ŒæŠ¥å‘Šç†è§£ç­‰ã€‚</li>
<li>MedMaxæ•°æ®é›†æ¶µç›–äº†å¹¿æ³›çš„ç”Ÿç‰©åŒ»å­¦é¢†åŸŸï¼ŒåŒ…æ‹¬æ”¾å°„å­¦å’Œç—…ç†å­¦ã€‚</li>
<li>åœ¨MedMaxä¸Šå¾®è°ƒçš„æ··åˆæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨å¤šä¸ªç”Ÿç‰©åŒ»å­¦è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°å¥—ä»¶ï¼Œä»¥æŒ‡å¯¼ç”Ÿç‰©åŒ»å­¦ä»»åŠ¡çš„æ··åˆæ¨¡æ€äººå·¥æ™ºèƒ½åŠ©ç†çš„å¼€å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12661">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c10ae0af9efb66faf9cb2ecc9c41f075.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-39f6f737e5774c12ae7b2fa2f330586a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-307b9c87f6fd992c762d562d9eb8c5df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a310d17c7411c9e408fbd000f0ec9184.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Fine-Grained-and-Multi-Dimensional-Metrics-for-Document-Level-Machine-Translation"><a href="#Fine-Grained-and-Multi-Dimensional-Metrics-for-Document-Level-Machine-Translation" class="headerlink" title="Fine-Grained and Multi-Dimensional Metrics for Document-Level Machine   Translation"></a>Fine-Grained and Multi-Dimensional Metrics for Document-Level Machine   Translation</h2><p><strong>Authors:Yirong Sun, Dawei Zhu, Yanjun Chen, Erjia Xiao, Xinghao Chen, Xiaoyu Shen</strong></p>
<p>Large language models (LLMs) have excelled in various NLP tasks, including machine translation (MT), yet most studies focus on sentence-level translation. This work investigates the inherent capability of instruction-tuned LLMs for document-level translation (docMT). Unlike prior approaches that require specialized techniques, we evaluate LLMs by directly prompting them to translate entire documents in a single pass. Our results show that this method improves translation quality compared to translating sentences separately, even without document-level fine-tuning. However, this advantage is not reflected in BLEU scores, which often favor sentence-based translations. We propose using the LLM-as-a-judge paradigm for evaluation, where GPT-4 is used to assess document coherence, accuracy, and fluency in a more nuanced way than n-gram-based metrics. Overall, our work demonstrates that instruction-tuned LLMs can effectively leverage document context for translation. However, we caution against using BLEU scores for evaluating docMT, as they often provide misleading outcomes, failing to capture the quality of document-level translation. Code and the outputs from GPT4-as-a-judge are available at <a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/BLEUless_DocMT">https://github.com/EIT-NLP/BLEUless_DocMT</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç ”ç©¶éƒ½é›†ä¸­åœ¨å¥å­çº§åˆ«çš„ç¿»è¯‘ä¸Šã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶æŒ‡ä»¤ä¼˜åŒ–åçš„LLMåœ¨æ–‡æ¡£çº§åˆ«ç¿»è¯‘ï¼ˆdocMTï¼‰æ–¹é¢çš„å›ºæœ‰èƒ½åŠ›ã€‚ä¸åŒäºéœ€è¦ä¸“é—¨æŠ€æœ¯çš„å…ˆå‰æ–¹æ³•ï¼Œæˆ‘ä»¬é€šè¿‡ç›´æ¥æç¤ºLLMä¸€æ¬¡æ€§ç¿»è¯‘æ•´ä¸ªæ–‡æ¡£æ¥è¯„ä¼°å…¶æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨æé«˜ç¿»è¯‘è´¨é‡æ–¹é¢ä¼˜äºåˆ†åˆ«ç¿»è¯‘å¥å­ï¼Œå³ä½¿åœ¨æœªç»æ–‡æ¡£çº§åˆ«å¾®è°ƒçš„æƒ…å†µä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ç„¶è€Œï¼Œè¿™ä¸€ä¼˜åŠ¿å¹¶æ²¡æœ‰ä½“ç°åœ¨BLEUåˆ†æ•°ä¸Šï¼ŒBLEUåˆ†æ•°é€šå¸¸æ›´å€¾å‘äºåŸºäºå¥å­çš„ç¿»è¯‘ã€‚æˆ‘ä»¬æå‡ºäº†ä½¿ç”¨LLMä½œä¸ºè¯„åˆ¤è€…çš„è¯„ä¼°èŒƒå¼ï¼Œåˆ©ç”¨GPT-4ä»¥æ›´ä¸ºç»†è‡´çš„æ–¹å¼è¯„ä¼°æ–‡æ¡£çš„è¿è´¯æ€§ã€å‡†ç¡®æ€§å’Œæµç•…æ€§ï¼Œè€ŒéåŸºäºnå…ƒè¯­æ³•çš„æŒ‡æ ‡ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„å·¥ä½œè¡¨æ˜ï¼ŒæŒ‡ä»¤ä¼˜åŒ–åçš„LLMå¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨æ–‡æ¡£ä¸Šä¸‹æ–‡è¿›è¡Œç¿»è¯‘ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬è­¦å‘Šè¯´ï¼Œä¸åº”ä½¿ç”¨BLEUåˆ†æ•°æ¥è¯„ä¼°docMTï¼Œå› ä¸ºå®ƒä»¬åœ¨è¯„ä¼°æ–‡æ¡£çº§åˆ«ç¿»è¯‘çš„è´¨é‡æ—¶å¸¸å¸¸æä¾›è¯¯å¯¼æ€§çš„ç»“æœã€‚æœ‰å…³GPT4ä½œä¸ºè¯„åˆ¤è€…çš„ä»£ç å’Œè¾“å‡ºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/BLEUless_DocMT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/EIT-NLP/BLEUless_DocMTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20941v4">PDF</a> Accepted at NAACL 2025 Student Research Workshop</p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ…æ‹¬æœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰åœ¨å†…çš„å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æœ¬ç ”ç©¶å…³æ³¨æŒ‡ä»¤è®­ç»ƒLLMåœ¨æ–‡æ¡£çº§åˆ«ç¿»è¯‘ï¼ˆdocMTï¼‰ä¸Šçš„å†…åœ¨èƒ½åŠ›ã€‚é€šè¿‡ç›´æ¥æç¤ºLLMä¸€æ¬¡æ€§ç¿»è¯‘æ•´ä¸ªæ–‡æ¡£ï¼Œå‘ç°è¯¥æ–¹æ³•ç›¸è¾ƒäºå•ç‹¬ç¿»è¯‘å¥å­èƒ½æé«˜ç¿»è¯‘è´¨é‡ï¼Œä¸”æ— éœ€å¯¹æ–‡æ¡£çº§åˆ«è¿›è¡Œå¾®è°ƒã€‚ç„¶è€Œï¼Œè¿™ä¸€ä¼˜åŠ¿å¹¶æœªä½“ç°åœ¨BLEUåˆ†æ•°ä¸Šï¼Œå› æ­¤æå‡ºä½¿ç”¨LLMä½œä¸ºæ³•å®˜æ¨¡å¼è¿›è¡Œè¯„ä¼°ï¼Œåˆ©ç”¨GPT-4è¯„ä¼°æ–‡æ¡£è¿è´¯æ€§ã€å‡†ç¡®æ€§å’Œæµç•…æ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒæŒ‡ä»¤è®­ç»ƒLLMèƒ½æœ‰æ•ˆåˆ©ç”¨æ–‡æ¡£ä¸Šä¸‹æ–‡è¿›è¡Œç¿»è¯‘ã€‚ä½†è­¦å‘Šç§°ï¼Œå¯¹äºdocMTçš„è¯„ä¼°ï¼ŒBLEUåˆ†æ•°å¸¸ç»™å‡ºè¯¯å¯¼æ€§ç»“æœï¼Œæ— æ³•å‡†ç¡®åæ˜ æ–‡æ¡£çº§åˆ«ç¿»è¯‘çš„è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMåœ¨æ–‡æ¡£çº§åˆ«ç¿»è¯‘ï¼ˆdocMTï¼‰ä¸Šå…·æœ‰å†…åœ¨èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç›´æ¥æç¤ºLLMç¿»è¯‘æ•´ä¸ªæ–‡æ¡£ï¼Œå¯æé«˜ç¿»è¯‘è´¨é‡ã€‚</li>
<li>ç›¸è¾ƒäºå¥å­çº§ç¿»è¯‘ï¼ŒLLMåœ¨æ–‡æ¡£çº§ç¿»è¯‘ä¸­æ— éœ€é¢å¤–å¾®è°ƒã€‚</li>
<li>BLEUåˆ†æ•°æ— æ³•å…¨é¢åæ˜ æ–‡æ¡£çº§åˆ«ç¿»è¯‘çš„è´¨é‡ã€‚</li>
<li>æå‡ºä½¿ç”¨LLMä½œä¸ºæ³•å®˜æ¨¡å¼ï¼Œåˆ©ç”¨GPT-4è¯„ä¼°æ–‡æ¡£ç¿»è¯‘çš„è¿è´¯æ€§ã€å‡†ç¡®æ€§å’Œæµç•…æ€§ã€‚</li>
<li>LLMèƒ½æœ‰æ•ˆåˆ©ç”¨æ–‡æ¡£ä¸Šä¸‹æ–‡è¿›è¡Œç¿»è¯‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.20941">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1ec305a3049820702c7cce10ef8b3bcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed9775791d08d331ec5b908bb7927fd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51fffe02a87fc95b397caca7e3865bea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-65976aaea99f2ad70ac94616e3b436e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d030a972e9de43d835b87242cac61256.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a87e770c8109c3de44ced6835527167.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Context-Parametric-Inversion-Why-Instruction-Finetuning-Can-Worsen-Context-Reliance"><a href="#Context-Parametric-Inversion-Why-Instruction-Finetuning-Can-Worsen-Context-Reliance" class="headerlink" title="Context-Parametric Inversion: Why Instruction Finetuning Can Worsen   Context Reliance"></a>Context-Parametric Inversion: Why Instruction Finetuning Can Worsen   Context Reliance</h2><p><strong>Authors:Sachin Goyal, Christina Baek, J. Zico Kolter, Aditi Raghunathan</strong></p>
<p>A standard practice when using large language models is for users to supplement their instruction with an input context containing new information for the model to process. However, models struggle to reliably follow the input context, especially when it conflicts with their parametric knowledge from pretraining. In-principle, one would expect models to adapt to the user context better after instruction finetuning, particularly when handling knowledge conflicts. However, we observe a surprising failure mode: during instruction tuning, the context reliance under knowledge conflicts initially increases as expected, but then gradually decreases as instruction finetuning progresses. This happens while the performance on standard benchmarks keeps on increasing far after this drop. We call this phenomenon context-parametric inversion and observe it across multiple general purpose instruction tuning datasets such as TULU, Alpaca and Ultrachat, across different model families like Llama, Mistral, and Pythia. We perform various controlled studies and theoretical analysis to show that context-parametric inversion occurs due to examples in the instruction finetuning data where the input context provides information that aligns with modelâ€™s parametric knowledge. Our analysis suggests some natural mitigation strategies with limited but insightful gains, and serves as a useful starting point in addressing this deficiency in instruction finetuning. </p>
<blockquote>
<p>åœ¨ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œç”¨æˆ·çš„æ ‡å‡†åšæ³•æ˜¯é€šè¿‡åŒ…å«æ–°ä¿¡æ¯çš„è¾“å…¥ä¸Šä¸‹æ–‡æ¥è¡¥å……æŒ‡ä»¤ï¼Œä»¥ä¾›æ¨¡å‹å¤„ç†ã€‚ç„¶è€Œï¼Œæ¨¡å‹åœ¨å¯é åœ°éµå¾ªè¾“å…¥ä¸Šä¸‹æ–‡æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå°¤å…¶å½“å®ƒä¸é¢„è®­ç»ƒä¸­çš„å‚æ•°çŸ¥è¯†å‘ç”Ÿå†²çªæ—¶ã€‚åŸåˆ™ä¸Šï¼Œäººä»¬ä¼šæœŸæœ›æ¨¡å‹åœ¨æŒ‡ä»¤å¾®è°ƒåèƒ½æ›´å¥½åœ°é€‚åº”ç”¨æˆ·ä¸Šä¸‹æ–‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†çŸ¥è¯†å†²çªæ—¶ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°äº†ä¸€ç§ä»¤äººæƒŠè®¶çš„å¤±è´¥æ¨¡å¼ï¼šåœ¨æŒ‡ä»¤è°ƒæ•´è¿‡ç¨‹ä¸­ï¼ŒçŸ¥è¯†å†²çªä¸‹çš„ä¸Šä¸‹æ–‡ä¾èµ–æœ€åˆå¦‚é¢„æœŸå¢åŠ ï¼Œä½†éšç€æŒ‡ä»¤å¾®è°ƒçš„è¿›è¡Œï¼Œå®ƒé€æ¸å‡å°‘ã€‚è¿™ç§ç°è±¡å‘ç”Ÿåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•çš„æ€§èƒ½æŒç»­æé«˜ä¹‹åã€‚æˆ‘ä»¬å°†è¿™ç§ç°è±¡ç§°ä¸ºâ€œä¸Šä¸‹æ–‡å‚æ•°åè½¬â€ï¼Œå¹¶åœ¨å¤šä¸ªé€šç”¨æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼ˆå¦‚TULUã€Alpacaå’ŒUltrachatï¼‰ä»¥åŠä¸åŒæ¨¡å‹å®¶æ—ï¼ˆå¦‚Llamaã€Mistralå’ŒPythiaï¼‰ä¸­è§‚å¯Ÿåˆ°å®ƒã€‚æˆ‘ä»¬è¿›è¡Œäº†å„ç§å¯¹ç…§ç ”ç©¶å’Œç†è®ºåˆ†æï¼Œä»¥è¡¨æ˜ä¸Šä¸‹æ–‡å‚æ•°åè½¬æ˜¯ç”±äºæŒ‡ä»¤å¾®è°ƒæ•°æ®ä¸­çš„ç¤ºä¾‹å¯¼è‡´çš„ï¼Œå…¶ä¸­è¾“å…¥ä¸Šä¸‹æ–‡æä¾›äº†ä¸æ¨¡å‹å‚æ•°çŸ¥è¯†ç›¸ç¬¦çš„ä¿¡æ¯ã€‚æˆ‘ä»¬çš„åˆ†ææå‡ºäº†ä¸€äº›è‡ªç„¶ç¼“è§£ç­–ç•¥ï¼Œè™½ç„¶æ”¶æ•ˆæœ‰é™ä½†é¢‡æœ‰è§è¯†ï¼Œå¹¶ä¸ºè§£å†³æŒ‡ä»¤å¾®è°ƒä¸­çš„è¿™ä¸€ç¼ºé™·æä¾›äº†æœ‰ç”¨çš„èµ·ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10796v3">PDF</a> Published at ICLR 2025 (Oral)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½¿ç”¨æ—¶ï¼Œç”¨æˆ·é€šå¸¸ä¼šè¡¥å……è¾“å…¥ä¸Šä¸‹æ–‡ä»¥æä¾›æ–°ä¿¡æ¯ä¾›æ¨¡å‹å¤„ç†ã€‚ç„¶è€Œï¼Œå½“ä¸Šä¸‹æ–‡ä¸æ¨¡å‹çš„é¢„è®­ç»ƒå‚æ•°çŸ¥è¯†å‘ç”Ÿå†²çªæ—¶ï¼Œæ¨¡å‹åœ¨å¯é åœ°éµå¾ªä¸Šä¸‹æ–‡æ–¹é¢ä¼šé‡åˆ°å›°éš¾ã€‚åŸåˆ™ä¸Šï¼Œäººä»¬æœŸæœ›æ¨¡å‹åœ¨æŒ‡ä»¤å¾®è°ƒåèƒ½æ›´å¥½åœ°é€‚åº”ç”¨æˆ·ä¸Šä¸‹æ–‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†çŸ¥è¯†å†²çªæ—¶ã€‚ä½†è§‚å¯Ÿåˆ°ä¸€ä¸ªæ„å¤–æƒ…å†µï¼šåœ¨æŒ‡ä»¤è°ƒæ•´è¿‡ç¨‹ä¸­ï¼Œè™½ç„¶åˆå§‹é˜¶æ®µä¸Šä¸‹æ–‡ä¾èµ–ç¨‹åº¦å¦‚é¢„æœŸçš„é‚£æ ·å¢åŠ ï¼Œä½†éšç€æŒ‡ä»¤å¾®è°ƒçš„è¿›è¡Œé€æ¸å‡å°‘ã€‚è¿™ç§æƒ…å†µå‘ç”Ÿåœ¨æ€§èƒ½åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸ŠæŒç»­å¢åŠ ä¹‹åã€‚æˆ‘ä»¬å°†è¿™ç§ç°è±¡ç§°ä¸ºâ€œä¸Šä¸‹æ–‡å‚æ•°åè½¬â€ï¼Œå¹¶åœ¨å¤šä¸ªé€šç”¨æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ï¼ˆå¦‚TULUã€Alpacaå’ŒUltrachatï¼‰ä»¥åŠä¸åŒæ¨¡å‹å®¶æ—ï¼ˆå¦‚Llamaã€Mistralå’ŒPythiaï¼‰ä¸­è§‚å¯Ÿåˆ°å®ƒã€‚æˆ‘ä»¬è¿›è¡Œå„ç§å—æ§ç ”ç©¶å’Œç†è®ºåˆ†æï¼Œæ˜¾ç¤ºä¸Šä¸‹æ–‡å‚æ•°åè½¬æ˜¯ç”±äºæŒ‡ä»¤å¾®è°ƒæ•°æ®ä¸­çš„ç¤ºä¾‹å¯¼è‡´çš„ï¼Œå…¶ä¸­è¾“å…¥ä¸Šä¸‹æ–‡æä¾›äº†ä¸æ¨¡å‹å‚æ•°çŸ¥è¯†ç›¸ç¬¦çš„ä¿¡æ¯ã€‚æˆ‘ä»¬çš„åˆ†ææå‡ºäº†ä¸€äº›è‡ªç„¶ç¼“è§£ç­–ç•¥ï¼Œè™½ç„¶æ•ˆæœæœ‰é™ä½†é¢‡å…·å¯å‘æ€§ï¼Œå¹¶ä¸ºè§£å†³æŒ‡ä»¤å¾®è°ƒä¸­çš„è¿™ä¸€ç¼ºé™·æä¾›äº†ä¸€ä¸ªæœ‰ç”¨çš„èµ·ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä¸ç”¨æˆ·æŒ‡ä»¤ç›¸å…³çš„ä¸Šä¸‹æ–‡æ—¶å¯èƒ½é‡åˆ°å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨çŸ¥è¯†å†²çªçš„æƒ…å†µä¸‹ã€‚</li>
<li>åœ¨æŒ‡ä»¤å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å¯¹ä¸Šä¸‹æ–‡çš„ä¾èµ–ç¨‹åº¦å‘ˆç°å…ˆå¢åå‡çš„è¶‹åŠ¿ã€‚</li>
<li>è¿™ç§è¶‹åŠ¿å˜åŒ–å‘ç”Ÿåœ¨æ¨¡å‹åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½æŒç»­æé«˜ä¹‹åã€‚</li>
<li>ä¸Šä¸‹æ–‡å‚æ•°åè½¬ç°è±¡åœ¨å¤šä¸ªé€šç”¨æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†å’Œä¸åŒæ¨¡å‹å®¶æ—ä¸­å‡è¢«è§‚å¯Ÿåˆ°ã€‚</li>
<li>ä¸Šä¸‹æ–‡å‚æ•°åè½¬æ˜¯ç”±äºæŒ‡ä»¤å¾®è°ƒæ•°æ®ä¸­çš„ç¤ºä¾‹å¯¼è‡´çš„ï¼Œå…¶ä¸­è¾“å…¥ä¸Šä¸‹æ–‡ä¸æ¨¡å‹çš„å‚æ•°çŸ¥è¯†ç›¸ç¬¦ã€‚</li>
<li>æˆ‘ä»¬çš„åˆ†ææå‡ºäº†è‡ªç„¶ç¼“è§£ç­–ç•¥ï¼Œè™½ç„¶æ•ˆæœæœ‰é™ä½†å…·æœ‰å¯å‘æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10796">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-19f8ef3abeed20a71a2f7ba4106639cb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e40a0612e4380de3dd2edc2992f450a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c85819695ee318d4e4eb1380b5c9ff2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df23b3a352cee333c510ac7d49b5266b.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Selective-Attention-Improves-Transformer"><a href="#Selective-Attention-Improves-Transformer" class="headerlink" title="Selective Attention Improves Transformer"></a>Selective Attention Improves Transformer</h2><p><strong>Authors:Yaniv Leviathan, Matan Kalman, Yossi Matias</strong></p>
<p>Unneeded elements in the attentionâ€™s context degrade performance. We introduce Selective Attention, a simple parameter-free change to the standard attention mechanism which reduces attention to unneeded elements. Selective attention consistently improves language modeling and downstream task performance in a variety of model sizes and context lengths. For example, transformers trained with the language modeling objective on C4 with selective attention perform language modeling equivalently to standard transformers with ~2X more heads and parameters in their attention modules. Selective attention also allows decreasing the size of the attentionâ€™s context buffer, leading to meaningful reductions in the memory and compute requirements during inference. For example, transformers trained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and 47X less memory for their attention module, respectively, when equipped with selective attention, as those without selective attention, with the same validation perplexity. </p>
<blockquote>
<p>æ³¨æ„åŠ›ä¸Šä¸‹æ–‡ä¸­çš„å¤šä½™å…ƒç´ ä¼šé™ä½æ€§èƒ½ã€‚æˆ‘ä»¬å¼•å…¥äº†é€‰æ‹©æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¿™æ˜¯å¯¹æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶çš„ç®€å•æ— å‚æ•°æ”¹è¿›ï¼Œå¯ä»¥å‡å°‘å¯¹ä¸éœ€è¦å…ƒç´ çš„å…³æ³¨ã€‚é€‰æ‹©æ€§æ³¨æ„åŠ›åœ¨å„ç§æ¨¡å‹å¤§å°å’Œä¸Šä¸‹æ–‡é•¿åº¦ä¸­éƒ½èƒ½æé«˜è¯­è¨€å»ºæ¨¡å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨C4ä¸Šä½¿ç”¨è¯­è¨€å»ºæ¨¡ç›®æ ‡è®­ç»ƒçš„å˜å‹å™¨ï¼Œé€šè¿‡é€‰æ‹©æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶è¯­è¨€å»ºæ¨¡èƒ½åŠ›ä¸æ ‡å‡†å˜å‹å™¨ç›¸å½“ï¼Œä½†æ³¨æ„åŠ›æ¨¡å—ä¸­çš„å¤´æ•°å’Œå‚æ•°å¢åŠ äº†çº¦ä¸¤å€ã€‚é€‰æ‹©æ€§æ³¨æ„åŠ›è¿˜å…è®¸å‡å°æ³¨æ„åŠ›ä¸Šä¸‹æ–‡ç¼“å†²åŒºçš„å¤§å°ï¼Œä»è€Œå‡å°‘äº†æ¨ç†è¿‡ç¨‹ä¸­çš„å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚ä¾‹å¦‚ï¼Œåœ¨C4ä¸Šè®­ç»ƒçš„ä¸Šä¸‹æ–‡å¤§å°ä¸º512ã€1024å’Œ2048çš„å˜å‹å™¨ï¼Œåœ¨ä½¿ç”¨é€‰æ‹©æ€§æ³¨æ„åŠ›åï¼Œå…¶æ³¨æ„åŠ›æ¨¡å—çš„æ‰€éœ€å†…å­˜åˆ†åˆ«å‡å°‘äº†æ— éœ€é€‰æ‹©æ€§æ³¨æ„åŠ›çš„æ¨¡å‹çš„16å€ã€25å€å’Œ47å€ï¼ŒåŒæ—¶ä¿æŒäº†ç›¸åŒçš„éªŒè¯å›°æƒ‘åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02703v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong>ï¼šé€‰æ‹©æ€§æ³¨æ„åŠ›æœºåˆ¶èƒ½æœ‰æ•ˆå‡å°‘ä¸å¿…è¦çš„å…ƒç´ å…³æ³¨åº¦ï¼Œä»è€Œæå‡è¯­è¨€æ¨¡å‹å’Œä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½è¡¨ç°ã€‚ç›¸è¾ƒäºä¼ ç»Ÿæ³¨æ„åŠ›æœºåˆ¶ï¼Œé€‰æ‹©æ€§æ³¨æ„åŠ›å¯åœ¨å‡å°‘å‚æ•°çš„åŒæ—¶ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€‰æ‹©æ€§æ³¨æ„åŠ›è¿˜èƒ½é™ä½æ³¨æ„åŠ›ä¸Šä¸‹æ–‡ç¼“å†²åŒºçš„è§„æ¨¡ï¼Œä»è€Œå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é€‰æ‹©æ€§æ³¨æ„åŠ›æœºåˆ¶æœ‰åŠ©äºå‡å°‘ä¸å¿…è¦çš„å…ƒç´ å…³æ³¨åº¦ã€‚</li>
<li>é€‰æ‹©æ€§æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥æé«˜è¯­è¨€æ¨¡å‹å’Œä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>é€‰æ‹©æ€§æ³¨æ„åŠ›æœºåˆ¶åœ¨å‡å°‘å‚æ•°çš„åŒæ—¶æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é€‰æ‹©æ€§æ³¨æ„åŠ›æœºåˆ¶å¯é™ä½æ³¨æ„åŠ›ä¸Šä¸‹æ–‡ç¼“å†²åŒºè§„æ¨¡ã€‚</li>
<li>é€‰æ‹©æ€§æ³¨æ„åŠ›æœ‰åŠ©äºå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„å†…å­˜éœ€æ±‚ã€‚</li>
<li>é€‰æ‹©æ€§æ³¨æ„åŠ›æœ‰åŠ©äºå‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02703">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ed98009c39c2870fe9640c75b4f05ec6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0f275015990993612c40d08f40aeaed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c610c44ab9de6121505059c99668d59.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9cab8690aa9ecac814ca6a507892ee6b.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Measuring-and-Enhancing-Trustworthiness-of-LLMs-in-RAG-through-Grounded-Attributions-and-Learning-to-Refuse"><a href="#Measuring-and-Enhancing-Trustworthiness-of-LLMs-in-RAG-through-Grounded-Attributions-and-Learning-to-Refuse" class="headerlink" title="Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded   Attributions and Learning to Refuse"></a>Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded   Attributions and Learning to Refuse</h2><p><strong>Authors:Maojia Song, Shang Hong Sim, Rishabh Bhardwaj, Hai Leong Chieu, Navonil Majumder, Soujanya Poria</strong></p>
<p>LLMs are an integral component of retrieval-augmented generation (RAG) systems. While many studies focus on evaluating the overall quality of end-to-end RAG systems, there is a gap in understanding the appropriateness of LLMs for the RAG task. To address this, we introduce Trust-Score, a holistic metric that evaluates the trustworthiness of LLMs within the RAG framework. Our results show that various prompting methods, such as in-context learning, fail to effectively adapt LLMs to the RAG task as measured by Trust-Score. Consequently, we propose Trust-Align, a method to align LLMs for improved Trust-Score performance. 26 out of 27 models aligned using Trust-Align substantially outperform competitive baselines on ASQA, QAMPARI, and ELI5. Specifically, in LLaMA-3-8b, Trust-Align outperforms FRONT on ASQA (up 12.56), QAMPARI (up 36.04), and ELI5 (up 17.69). Trust-Align also significantly enhances modelsâ€™ ability to correctly refuse and provide quality citations. We also demonstrate the effectiveness of Trust-Align across different open-weight models, including the LLaMA series (1b to 8b), Qwen-2.5 series (0.5b to 7b), and Phi3.5 (3.8b). We release our code at <a target="_blank" rel="noopener" href="https://github.com/declare-lab/trust-align">https://github.com/declare-lab/trust-align</a>. </p>
<blockquote>
<p>LLMæ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚å°½ç®¡è®¸å¤šç ”ç©¶ä¸“æ³¨äºè¯„ä¼°ç«¯åˆ°ç«¯RAGç³»ç»Ÿçš„æ•´ä½“è´¨é‡ï¼Œä½†å¯¹äºLLMåœ¨RAGä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ä»å­˜åœ¨ç†è§£ä¸Šçš„ç©ºç™½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Trust-Scoreï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢è¯„ä¼°LLMåœ¨RAGæ¡†æ¶å†…å¯ä¿¡åº¦çš„æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåƒä¸Šä¸‹æ–‡å­¦ä¹ è¿™æ ·çš„å„ç§æç¤ºæ–¹æ³•ï¼Œæ— æ³•æœ‰æ•ˆåœ°ä½¿LLMé€‚åº”ç”±Trust-Scoreè¡¡é‡çš„RAGä»»åŠ¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Trust-Alignæ–¹æ³•ï¼Œä»¥å¯¹LLMè¿›è¡Œå¯¹é½ï¼Œä»¥æé«˜Trust-Scoreçš„æ€§èƒ½ã€‚åœ¨ASQAã€QAMPARIå’ŒELI5ä¸Šï¼Œä½¿ç”¨Trust-Alignå¯¹é½çš„26ä¸ªæ¨¡å‹ä¸­çš„å¤§å¤šæ•°éƒ½æ˜¾è‘—ä¼˜äºç«äº‰åŸºçº¿ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨LLaMA-3-8bä¸­ï¼ŒTrust-Alignåœ¨ASQAï¼ˆæé«˜12.56ï¼‰ã€QAMPARIï¼ˆæé«˜36.04ï¼‰å’ŒELI5ï¼ˆæé«˜17.69ï¼‰ä¸Šçš„è¡¨ç°å‡ä¼˜äºFRONTã€‚Trust-Alignè¿˜æ˜¾è‘—æé«˜äº†æ¨¡å‹æ­£ç¡®æ‹’ç»å’Œæä¾›é«˜è´¨é‡å¼•ç”¨çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†Trust-Alignåœ¨ä¸åŒå¼€æ”¾æƒé‡æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬LLaMAç³»åˆ—ï¼ˆ1båˆ°8bï¼‰ã€Qwen-2.5ç³»åˆ—ï¼ˆ0.5båˆ°7bï¼‰å’ŒPhi3.5ï¼ˆ3.8bï¼‰ã€‚æˆ‘ä»¬å·²å°†ä»£ç å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/declare-lab/trust-align%E3%80%82">https://github.com/declare-lab/trust-alignã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11242v4">PDF</a> Published at ICLR 2025 (Oral)</p>
<p><strong>Summary</strong></p>
<p>LLMsåœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚é’ˆå¯¹LLMsåœ¨RAGä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ï¼Œå¼•å…¥Trust-Scoreè¿™ä¸€å…¨é¢è¯„ä¼°æŒ‡æ ‡ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä¸Šä¸‹æ–‡å­¦ä¹ ç­‰æç¤ºæ–¹æ³•æ— æ³•æœ‰æ•ˆé€‚åº”RAGä»»åŠ¡ï¼Œè€ŒTrust-Alignæ–¹æ³•åˆ™å¯æé«˜LLMsçš„Trust-Scoreè¡¨ç°ã€‚é‡‡ç”¨Trust-Alignå¯¹é½çš„26ä¸ªæ¨¡å‹ä¸­ï¼Œå¤§éƒ¨åˆ†åœ¨ASQAã€QAMPARIå’ŒELI5ç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚Trust-Alignå¯æ˜¾è‘—æé«˜æ¨¡å‹æ‹’ç»ä¸å½“å¼•ç”¨å’Œæä¾›é«˜è´¨é‡å¼•ç”¨çš„èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºå¤šç§è§„æ¨¡ä¸åŒçš„å¼€æºæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsæ˜¯RAGç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å­˜åœ¨å¯¹LLMsåœ¨RAGä»»åŠ¡ä¸­é€‚ç”¨æ€§çš„ç†è§£å·®è·ã€‚</li>
<li>Trust-Scoreè¢«å¼•å…¥ä½œä¸ºè¯„ä¼°LLMsåœ¨RAGæ¡†æ¶ä¸­å¯ä¿¡åº¦çš„å…¨é¢æŒ‡æ ‡ã€‚</li>
<li>æç¤ºæ–¹æ³•ï¼ˆå¦‚ä¸Šä¸‹æ–‡å­¦ä¹ ï¼‰æœªèƒ½æœ‰æ•ˆé€‚åº”RAGä»»åŠ¡ï¼Œè¡¨ç°ä¸ä½³ã€‚</li>
<li>Trust-Alignæ–¹æ³•èƒ½æé«˜LLMsçš„Trust-Scoreè¡¨ç°ã€‚</li>
<li>é‡‡ç”¨Trust-Alignå¯¹é½çš„æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11242">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-120beb373e66cae04cc388739cfc1241.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b42955d1d78ffe07b7732f5d37b86cf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f3fa4eac82250931be832a95b6019b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea7f685b7fb95250ef53c9bf7a778e9f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-for-Verilog-Generation-with-Code-Structure-Guided-Reinforcement-Learning"><a href="#Large-Language-Model-for-Verilog-Generation-with-Code-Structure-Guided-Reinforcement-Learning" class="headerlink" title="Large Language Model for Verilog Generation with Code-Structure-Guided   Reinforcement Learning"></a>Large Language Model for Verilog Generation with Code-Structure-Guided   Reinforcement Learning</h2><p><strong>Authors:Ning Wang, Bingkun Yao, Jie Zhou, Xi Wang, Zhe Jiang, Nan Guan</strong></p>
<p>Recent advancements in large language models (LLMs) have sparked significant interest in the automatic generation of Register Transfer Level (RTL) designs, particularly using Verilog. Current research on this topic primarily focuses on pre-training and instruction tuning, but the effectiveness of these methods is constrained by the limited availability of training data, as public Verilog code is far less abundant than software code. In particular, these methods struggle to effectively capture Verilog parallel code structures, which fundamentally differ from the imperative, sequential control flow typical in most software programming languages. This paper introduces VeriSeek, an LLM enhanced by reinforcement learning using a limited amount of high-quality training data to achieve high Verilog code generation performance. Our reinforcement learning approach employs code structure information as feedback signals to refine the pre-trained model, enabling it to effectively learn important patterns from Verilog code with parallel structures. Experiments show that VeriSeek outperforms state-of-the-art methods across multiple benchmarks. </p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•å¼•å‘äº†äººä»¬å¯¹è‡ªåŠ¨ç”Ÿæˆå¯„å­˜å™¨ä¼ è¾“çº§åˆ«ï¼ˆRTLï¼‰è®¾è®¡çš„æµ“åšå…´è¶£ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨Verilogè®¾è®¡ã€‚å½“å‰å…³äºè¿™ä¸€ä¸»é¢˜çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´ä¸Šï¼Œä½†è¿™äº›æ–¹æ³•çš„æœ‰æ•ˆæ€§å—åˆ°è®­ç»ƒæ•°æ®æœ‰é™æ€§çš„åˆ¶çº¦ï¼Œå› ä¸ºå…¬å…±Verilogä»£ç æ¯”è½¯ä»¶ä»£ç è¦å°‘å¾—å¤šã€‚å°¤å…¶è¿™äº›æ–¹æ³•åœ¨æ•è·Verilogå¹¶è¡Œä»£ç ç»“æ„æ—¶é‡åˆ°å›°éš¾ï¼Œè¿™äº›ç»“æ„ä¸å¤§å¤šæ•°è½¯ä»¶ç¼–ç¨‹è¯­è¨€ä¸­çš„å‘½ä»¤å¼ã€é¡ºåºæ§åˆ¶æµæ ¹æœ¬ä¸åŒã€‚æœ¬æ–‡ä»‹ç»äº†VeriSeekï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨æœ‰é™çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®å®ç°é«˜æ€§èƒ½çš„Verilogä»£ç ç”Ÿæˆã€‚æˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é‡‡ç”¨ä»£ç ç»“æ„ä¿¡æ¯ä½œä¸ºåé¦ˆä¿¡å·æ¥ä¼˜åŒ–é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°ä»å…·æœ‰å¹¶è¡Œç»“æ„çš„Verilogä»£ç ä¸­å­¦ä¹ é‡è¦æ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼ŒVeriSeekåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šä¼˜äºæœ€æ–°æŠ€æœ¯çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.18271v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨ç”Ÿæˆå¯„å­˜å™¨ä¼ è¾“çº§ï¼ˆRTLï¼‰è®¾è®¡ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨Verilogè¯­è¨€æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ã€‚ç ”ç©¶ä¸»è¦é›†ä¸­äºé¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä½†å—é™äºå…¬å…±Verilogä»£ç è¾ƒè½¯ä»¶ä»£ç ç¨€ç¼ºï¼Œè¿™äº›æ–¹æ³•çš„æ•ˆæœæœ‰é™ã€‚æœ¬æ–‡æå‡ºVeriSeekï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¢å¼ºLLMï¼Œåˆ©ç”¨æœ‰é™çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®å®ç°é«˜æ€§èƒ½çš„Verilogä»£ç ç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒVeriSeekåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨ç”Ÿæˆå¯„å­˜å™¨ä¼ è¾“çº§ï¼ˆRTLï¼‰è®¾è®¡æ–¹é¢å–å¾—è¿›å±•ã€‚</li>
<li>ç ”ç©¶é›†ä¸­åœ¨é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒæ•´ä¸Šï¼Œä½†å—é™äºVerilogä»£ç çš„ç¨€ç¼ºæ€§ã€‚</li>
<li>Verilogå¹¶è¡Œä»£ç ç»“æ„ä¸å¤§å¤šæ•°è½¯ä»¶ç¼–ç¨‹è¯­è¨€ä¸­çš„å‘½ä»¤å¼ã€é¡ºåºæ§åˆ¶æµå­˜åœ¨æ ¹æœ¬å·®å¼‚ã€‚</li>
<li>æå‡ºçš„VeriSeeké‡‡ç”¨å¼ºåŒ–å­¦ä¹ å¢å¼ºLLMï¼Œæœ‰æ•ˆåˆ©ç”¨æœ‰é™çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®ã€‚</li>
<li>VeriSeeké€šè¿‡åˆ©ç”¨ä»£ç ç»“æ„ä¿¡æ¯ä½œä¸ºåé¦ˆä¿¡å·æ¥ä¼˜åŒ–é¢„è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>VeriSeekèƒ½æœ‰æ•ˆå­¦ä¹ Verilogä»£ç ä¸­çš„é‡è¦æ¨¡å¼ï¼Œå°¤å…¶æ˜¯å¹¶è¡Œç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.18271">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ebe0775bc99a9fab9c6c09fad8abfd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a3332c518db3d81b7f43a5f00a3a43f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6006497bbe2fdb41168e7f8b98a08b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44453d36af81f3d3041814118e0b58bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ff767edc31c861d3d8c593d4f6961b38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5536bf373bc269100f741f362c86e6f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d48b489d6b42577d2695b77a35f1176d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-898a8651e3e02f77ef9889fe7a7d86c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b20207a3e4ae07f7042acdd980e27ff.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-04-26/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-04-26/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-26/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1d3231822b18b5ffc11752e8d6afeae5.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-26  MCAF Efficient Agent-based Video Understanding Framework through   Multimodal Coarse-to-Fine Attention Focusing
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-04-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-04-26/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1935b781154349fb0918b181e49aa7d7.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-04-26  DeepDistill Enhancing LLM Reasoning Capabilities via Large-Scale   Difficulty-Graded Data Training
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-04-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25219.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
