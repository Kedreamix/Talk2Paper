<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-05-28  DiffCalib Reformulating Monocular Camera Calibration as Diffusion-Based   Dense Incident Map Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-36a0effe69414b2ffa084f4cd6a69d06.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-05-28
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    25 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-05-28-更新"><a href="#2024-05-28-更新" class="headerlink" title="2024-05-28 更新"></a>2024-05-28 更新</h1><h2 id="DiffCalib-Reformulating-Monocular-Camera-Calibration-as-Diffusion-Based-Dense-Incident-Map-Generation"><a href="#DiffCalib-Reformulating-Monocular-Camera-Calibration-as-Diffusion-Based-Dense-Incident-Map-Generation" class="headerlink" title="DiffCalib: Reformulating Monocular Camera Calibration as Diffusion-Based   Dense Incident Map Generation"></a>DiffCalib: Reformulating Monocular Camera Calibration as Diffusion-Based   Dense Incident Map Generation</h2><p><strong>Authors:Xiankang He, Guangkai Xu, Bo Zhang, Hao Chen, Ying Cui, Dongyan Guo</strong></p>
<p>Monocular camera calibration is a key precondition for numerous 3D vision applications. Despite considerable advancements, existing methods often hinge on specific assumptions and struggle to generalize across varied real-world scenarios, and the performance is limited by insufficient training data. Recently, diffusion models trained on expansive datasets have been confirmed to maintain the capability to generate diverse, high-quality images. This success suggests a strong potential of the models to effectively understand varied visual information. In this work, we leverage the comprehensive visual knowledge embedded in pre-trained diffusion models to enable more robust and accurate monocular camera intrinsic estimation. Specifically, we reformulate the problem of estimating the four degrees of freedom (4-DoF) of camera intrinsic parameters as a dense incident map generation task. The map details the angle of incidence for each pixel in the RGB image, and its format aligns well with the paradigm of diffusion models. The camera intrinsic then can be derived from the incident map with a simple non-learning RANSAC algorithm during inference. Moreover, to further enhance the performance, we jointly estimate a depth map to provide extra geometric information for the incident map estimation. Extensive experiments on multiple testing datasets demonstrate that our model achieves state-of-the-art performance, gaining up to a 40% reduction in prediction errors. Besides, the experiments also show that the precise camera intrinsic and depth maps estimated by our pipeline can greatly benefit practical applications such as 3D reconstruction from a single in-the-wild image. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15619v1">PDF</a> </p>
<p><strong>Summary</strong><br>单目相机校准是众多3D视觉应用的关键先决条件。最近，基于大规模数据集训练的扩散模型被证实能够生成多样且高质量的图像，为单目相机内在估计提供更强大和准确的支持。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>单目相机校准对于多种3D视觉应用至关重要</li>
<li>扩散模型能够生成多样且高质量的图像</li>
<li>通过利用扩散模型中的视觉知识，能够实现更稳健和准确的单目相机内在估计</li>
<li>通过将估计相机内在参数的问题重新构建为密集入射图生成任务，能够实现更简单的推断过程</li>
<li>联合估计深度图能够进一步提升性能</li>
<li>实验证明该模型达到了最先进的性能，预测误差降低了40%</li>
<li>精确的相机内在和深度图能够极大地促进从单张野外图像进行的3D重建等实际应用</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>
<p>Title: DiffCalib: Reformulating Monocular Camera Calibration as Diffusion-Based Dense Incident Map Generation (DiffCalib：将单目摄像机校准重构为基于扩散的密集入射图生成)</p>
</li>
<li>
<p>Authors: Xiankang He, Guangkai Xu, Bo Zhang, Hao Chen, Ying Cui, Dongyan Guo</p>
</li>
<li>
<p>Affiliation: 浙江工业大学</p>
</li>
<li>
<p>Keywords: monocular camera calibration, diffusion models, incident map generation</p>
</li>
<li>
<p>Urls: arXiv:2405.15619v1, Github:None</p>
</li>
<li>
<p>Summary:</p>
</li>
<li>
<p>(1):本文的研究背景是单目摄像机校准，这是许多三维视觉应用的关键前提条件。</p>
</li>
<li>
<p>(2):过去的方法存在一些假设和限制，无法在不同的真实世界场景中泛化，并且受限于训练数据的不足。最近，扩散模型在生成高质量图像方面取得了成功，这启发了我们使用扩散模型来实现更鲁棒和准确的单目摄像机校准。</p>
</li>
<li>
<p>(3):本文提出的研究方法是将单目摄像机校准问题重构为基于扩散的密集入射图生成任务，使用预训练的扩散模型生成入射图，然后使用RANSAC算法推断摄像机参。</p>
</li>
<li>
<p>(4):本文的方法在单目摄像机校准任务上取得了很好的性能，证明了扩散模型在理解视觉信息方面的潜力，并且可以用于在野三维重建任务中。</p>
</li>
<li>
<p>方法：</p>
</li>
<li>
<p>(1)：将单目摄像机校准问题重构为基于扩散的密集入射图生成任务，以便能够利用预训练的扩散模型生成入射图。</p>
</li>
<li>
<p>(2)：使用Stable Diffusion v2.1模型对入射图进行编码和解码，生成噪声后的入射图latent codes，并训练U-Net模型来预测噪声。</p>
</li>
<li>
<p>(3)：将深度图和入射图联合学习，以提高入射图生成的准确性和鲁棒性。</p>
</li>
<li>
<p>(4)：使用RANSAC算法从生成的入射图中恢复摄像机的内参数矩阵K。</p>
</li>
<li>
<p>(5)：使用ensemble方法来提高入射图生成的准确性和稳定性。</p>
</li>
<li>
<p>(6)：使用恢复的摄像机内参数矩阵K来进行单目摄像机校准。</p>
</li>
<li>
<p>Conclusion: </p>
</li>
<li>
<p>(1): 这篇文章的意义在于提出了对于[领域]的新思路，为该领域的研究和发展带来了新的启发和方向；</p>
</li>
<li>(2): Innovation point: 该文章的创新点在于提出了一种全新的[创新点]，突破了传统的[创新点]方式； Performance: 该文章在实验表现方面展现出了较高的准确性和稳定性，但仍有待进一步提升； Workload: 该文章的工作量较大，需要更多的实验数据和分析来支撑其结论。</li>
</ol>



<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-02a306a749ab4f7167af1ae9e9bd38f3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-3354b1c0f182b11d7a2fe0d1f53745ed.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-a3bcd389775a3247ad6697fadd1fd9cd.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8a6244aa42d8f424a5319ca260b17f35.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-36a0effe69414b2ffa084f4cd6a69d06.jpg" align="middle">
</details>




<h2 id="Defensive-Unlearning-with-Adversarial-Training-for-Robust-Concept-Erasure-in-Diffusion-Models"><a href="#Defensive-Unlearning-with-Adversarial-Training-for-Robust-Concept-Erasure-in-Diffusion-Models" class="headerlink" title="Defensive Unlearning with Adversarial Training for Robust Concept   Erasure in Diffusion Models"></a>Defensive Unlearning with Adversarial Training for Robust Concept   Erasure in Diffusion Models</h2><p><strong>Authors:Yimeng Zhang, Xin Chen, Jinghan Jia, Yihua Zhang, Chongyu Fan, Jiancheng Liu, Mingyi Hong, Ke Ding, Sijia Liu</strong></p>
<p>Diffusion models (DMs) have achieved remarkable success in text-to-image generation, but they also pose safety risks, such as the potential generation of harmful content and copyright violations. The techniques of machine unlearning, also known as concept erasing, have been developed to address these risks. However, these techniques remain vulnerable to adversarial prompt attacks, which can prompt DMs post-unlearning to regenerate undesired images containing concepts (such as nudity) meant to be erased. This work aims to enhance the robustness of concept erasing by integrating the principle of adversarial training (AT) into machine unlearning, resulting in the robust unlearning framework referred to as AdvUnlearn. However, achieving this effectively and efficiently is highly nontrivial. First, we find that a straightforward implementation of AT compromises DMs’ image generation quality post-unlearning. To address this, we develop a utility-retaining regularization on an additional retain set, optimizing the trade-off between concept erasure robustness and model utility in AdvUnlearn. Moreover, we identify the text encoder as a more suitable module for robustification compared to UNet, ensuring unlearning effectiveness. And the acquired text encoder can serve as a plug-and-play robust unlearner for various DM types. Empirically, we perform extensive experiments to demonstrate the robustness advantage of AdvUnlearn across various DM unlearning scenarios, including the erasure of nudity, objects, and style concepts. In addition to robustness, AdvUnlearn also achieves a balanced tradeoff with model utility. To our knowledge, this is the first work to systematically explore robust DM unlearning through AT, setting it apart from existing methods that overlook robustness in concept erasing. Codes are available at: <a target="_blank" rel="noopener" href="https://github.com/OPTML-Group/AdvUnlearn">https://github.com/OPTML-Group/AdvUnlearn</a> </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15234v1">PDF</a> Codes are available at <a target="_blank" rel="noopener" href="https://github.com/OPTML-Group/AdvUnlearn">https://github.com/OPTML-Group/AdvUnlearn</a></p>
<p><strong>Summary</strong><br>基于对抗训练增强机器unlearning，提出AdvUnlearn框架，以提高概念擦除的鲁棒性。</p>
<p><strong>Key Takeaways</strong><br>•  Diffusion模型在文本到图像生成中取得了显著成功，但也存在安全风险，如生成有害内容和版权违规。<br>•  机器unlearning技术可以解决这些风险，但易受到对抗prompt攻击。<br>•  本工作提出AdvUnlearn框架，通过将对抗训练原则集成到机器unlearning中，以提高概念擦除的鲁棒性。<br>• AdvUnlearn框架使用utility-retaining regularization来平衡概念擦除鲁棒性和模型实用性。<br>•  文本编码器是实现机器unlearning的更适合模块。<br>•  AdvUnlearn框架可以在各种Diffusion模型unlearning场景下实现鲁棒的概念擦除。<br>•  本工作是首次系统地探索通过对抗训练实现鲁棒的Diffusion模型unlearning。</p>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>
<p>Title: AdvUnlearn: Robust Unlearning for Diffusion Models (Diffusion模型的鲁棒unlearning)</p>
</li>
<li>
<p>Authors: (no authors listed)</p>
</li>
<li>
<p>Affiliation: 无</p>
</li>
<li>
<p>Keywords: Diffusion Models, Machine Unlearning, Adversarial Training, Text-to-Image Generation</p>
</li>
<li>
<p>Urls: https://github.com/OPTML-Group/AdvUnlearn</p>
</li>
<li>
<p>Summary:</p>
<ul>
<li>
<p>(1):随着Diffusion模型在文本到图像生成任务中的成功，它们也带来了安全风险，如生成有害内容和版权违反。为解决这些风险，机器unlearning技术被开发出来，但是这些技术仍易受对抗性prompt攻击的影响。</p>
</li>
<li>
<p>(2):过去的方法，如ScissorHands和EraseDiff，虽然可以实现高的unlearning robustness，但是它们图像生成质量下降明显。这些方法的motivation不足，无法解决机器unlearning中的安全风险。</p>
</li>
<li>
<p>(3):本文提出了AdvUnlearn框架，结合对抗性训练来增强机器unlearning的robustness。该框架使用utility-retaining regularization来平衡概念擦除的robustness和模实用性，并将文本编码器作为robustification的模块。</p>
</li>
<li>
<p>(4):本文在多个Diffusion模型unlearning场景中进行了实验，包括裸体、对象和风格概念的擦除。结果表明，AdvUnlearn框架可以实现robust的机器unlearning，同时保持模型的实用性。</p>
</li>
<li>方法：</li>
</ul>
</li>
<li>
<p>(1):提出AdvUnlearn框架，结合对抗性训练来增强机器unlearning的robustness，使用utility-retaining regularization来平衡概念擦除的robustness和模实用，并将文本编码器作为robustification的模块。</p>
</li>
<li>
<p>(2):使用large language model (LLM)作为judge来筛选保留prompt，排除与目标概念擦除相关的prompt，从而确保图像生成质量不受损害。</p>
</li>
<li>
<p>(3):定义utility-retaining regularization损失函数ℓESD，penalizes图像生成质量的下降，使用当前Diffusion模型θ与原始θo下的保留概念˜c来计算。</p>
</li>
<li>
<p>(4):使用fast attack generation方法来简化AdvUnlearn的lower-level优化，使用fast gradient sign method (FGSM)来解决quadratic program，并生成对抗性prompt。</p>
</li>
<li>
<p>(5):将AdvUnlearn应用于不同的Diffusion模型unlearning场景，包括裸体、对象和风格概念的擦除，并评估其robustness和图像生成质量。</p>
</li>
<li>
<p>(6):比较AdvUnlearn与其方法（如ESD和AT-ESD）的性能，证明AdvUnlearn可以实现robust的机器unlearning，同时保持模型的实用性</p>
</li>
<li>
<p>(7):探索AdvUnlearn的模块化设计，讨论将文本编码器作为plug-in unlearner的可能性，以提高机器unlearning的效率和普适性。</p>
</li>
<li>
<p>Conclusion:</p>
</li>
<li>
<p>(1):本文提出的AdvUnlearn框架对Diffusion模型的机器unlearning领域具有重要意义，因为它可以增强机器unlearning的robustness，同时保持模型的实用性。</p>
</li>
<li>
<p>(2):Innovation point: 本文提出了一种新的机器unlearning方法，结合对抗性训练和utility-retaining regularization来增强机器unlearning的robustness；Performance: AdvUnlearn框架在多个Diffusion模型unlearning场景中表现出色，实现了robust的机器unlearning，同时保持模型的实用性；Workload: 本文的实验设计和实现相对复杂，需要大量的计算资源和时间。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-12bc7afe95c87708c06799dd505c46da.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-c3f86497a08db26b9953f1bc30dad1c3.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-7ef67ded1db4d01263a65cdacd20797a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-202a39b4f890f5df5c6e0f34c4f7a6a7.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-89575cd27c93753bf34b1aebf5ce8aef.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-005e6d2cd8b93a64b356e1bd2dd224c9.jpg" align="middle">
</details>




<h2 id="DEEM-Diffusion-Models-Serve-as-the-Eyes-of-Large-Language-Models-for-Image-Perception"><a href="#DEEM-Diffusion-Models-Serve-as-the-Eyes-of-Large-Language-Models-for-Image-Perception" class="headerlink" title="DEEM: Diffusion Models Serve as the Eyes of Large Language Models for   Image Perception"></a>DEEM: Diffusion Models Serve as the Eyes of Large Language Models for   Image Perception</h2><p><strong>Authors:Run Luo, Yunshui Li, Longze Chen, Wanwei He, Ting-En Lin, Ziqiang Liu, Lei Zhang, Zikai Song, Xiaobo Xia, Tongliang Liu, Min Yang, Binyuan Hui</strong></p>
<p>The development of large language models (LLMs) has significantly advanced the emergence of large multimodal models (LMMs). While LMMs have achieved tremendous success by promoting the synergy between multimodal comprehension and creation, they often face challenges when confronted with out-of-distribution data. This is primarily due to their reliance on image encoders trained to encode images into task-relevant features, which may lead them to disregard irrelevant details. Delving into the modeling capabilities of diffusion models for images naturally prompts the question: Can diffusion models serve as the eyes of large language models for image perception? In this paper, we propose DEEM, a simple and effective approach that utilizes the generative feedback of diffusion models to align the semantic distributions of the image encoder. This addresses the drawbacks of previous methods that solely relied on image encoders like ViT, thereby enhancing the model’s resilience against out-of-distribution samples and reducing visual hallucinations. Importantly, this is achieved without requiring additional training modules and with fewer training parameters. We extensively evaluated DEEM on both our newly constructed RobustVQA benchmark and another well-known benchmark, POPE, for object hallucination. Compared to the state-of-the-art interleaved content generation models, DEEM exhibits enhanced robustness and a superior capacity to alleviate model hallucinations while utilizing fewer trainable parameters, less pre-training data (10%), and a smaller base model size. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15232v1">PDF</a> 25 pages</p>
<p><strong>Summary</strong><br>通过使用扩散模型，本文提出了一种名为DEEM的简单而有效的方法，利用扩散模型的生成反馈来调整图像编码器的语义分布，从而增强了模型对于超出分布数据的鲁棒性，减少了视觉幻觉，同时无需额外的训练模块和更少的训练参数。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型（LLMs）的发展推动了大型多模态模型（LMMs）的出现；</li>
<li>LMMs在促进多模态理解和创作方面取得了巨大成功，但在处理超出分布数据时面临挑战；</li>
<li>DEEM利用扩散模型的生成反馈来调整图像编码器的语义分布，解决了以往仅依赖于图像编码器的方法的缺陷；</li>
<li>DEEM在RobustVQA基准和POPE基准上得到了广泛评估，表现出卓越的鲁棒性和减少模型幻觉的能力；</li>
<li>DEEM相较于最先进的交替内容生成模型，展现出更强的鲁棒性，并利用更少的可训练参数、更少的预训练数据（10%）和更小的基础模型尺寸。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<h2>Paper:1</h2>
<ol>
<li>
<p>Title: DEEM：使用扩散模型对大型多模态模型的图像感知进行增强 (DEEM: Enhancing Image Perception of Large Multimodal Models with Diffusion Models)</p>
</li>
<li>
<p>Authors: (no author names provided)</p>
</li>
<li>
<p>Affiliation: 无 (no affiliation provided)</p>
</li>
<li>
<p>Keywords: large language models, large multimodal models, diffusion models, image perception, robustness, hallucination</p>
</li>
<li>
<p>Urls: arXiv:2405.15232v1, Github: None</p>
</li>
<li>
<p>Summary:</p>
<ul>
<li>
<p>(1):该论文的研究背景是大型语言模型（LLMs）和大型多模态模型（LMMs）的发展，后者通过简单的映射模块将LLMs与图像编码器连接起来，实现多模态理解任务。</p>
</li>
<li>
<p>(2):过去的方法主要依赖图像编码器来将图像编码为任务相关特征，可能忽视无关细节，从而导致模型对外分布数据的robustness和hallucination问题。</p>
</li>
<li>
<p>(3):本文提出的方法是DEEM，它使用扩散模型的生成反馈来对齐图像编码器的语义分布，提高模型对外分布数据的robustness和减少hallucination。</p>
</li>
<li>
<p>(4):该方法在RobustVQA和POPE两个基准测试数据集上进行了评估，结果表明DEEM相比于当前最先进的模型具有更好的robustness和减少hallucination能力，同时还可以在多模态任务如视觉问答、图像字幕生成和文本条件图像合成等方面取得竞争性的结果。</p>
</li>
<li>方法：</li>
</ul>
</li>
<li>
<p>(1)：首先，使用大型语言模型（LLM）作为文本编码器，生成图像相关的文本特征，以便与图像编码器进行对齐。</p>
</li>
<li>
<p>(2)：然后，使用扩散模型（Diffusion Model）对图像编码器的输出进行生成反馈，以调整图像编码器语义分布，提高模型对外分布数据的robustness。</p>
</li>
<li>
<p>(3)：在生成反馈过程中，使用对抗训练（Adversarial Training）来鼓励图像编码器生成更加robust的特征，减少hallucination的可能性。</p>
</li>
<li>
<p>(4)：接着，对DEEM模型进行多模态任务的fine-tuning，例如视觉问答、图像字幕生成和文本条件图像合成等，以提高模型在多模态任务上的性能。</p>
</li>
<li>
<p>(5)：最后，在RobustVQA和POPE两个基准测试数据集上进行评估，评估DEEM模型的robustness和hallucination能力，並与当前最先进的模型进行比较。</p>
</li>
<li>
<p>Conclusion: </p>
</li>
<li>
<p>(1): 本研究的意义在于提出了一种新的方法（DEEM），通过使用扩散模型对大型多模态模型进行图像感知增强，有效提高了模型的鲁棒性和减少了虚假感知，为多模态任务的性能提升提供了新的思路。</p>
</li>
<li>
<p>(2): 创新点：DEEM方法利用扩散模型对图像编码器的语义分布进行调整，在提高模型鲁棒性和减少虚假感知方面取得显著进展。性能：DEEM在RobustVQA和POPE两个基准测试数据集上相比当前最先进模型具有更好的鲁棒性和减少虚假感知能力，并在多模态任务上取得了竞争性的结果。工作量：论文所提出的DEEM方法需要进一步实验和验证，以确保其在不同领域的泛化性能，这可能需要更多的工作量来支持。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c0b6103bc7ef9889b013616a33153dac.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-5911a832e2f068efcd4f1c57fb6c0989.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-2f388f04ad9850dd89191f6903b1cf64.jpg" align="middle">
</details>




<h2 id="NIVeL-Neural-Implicit-Vector-Layers-for-Text-to-Vector-Generation"><a href="#NIVeL-Neural-Implicit-Vector-Layers-for-Text-to-Vector-Generation" class="headerlink" title="NIVeL: Neural Implicit Vector Layers for Text-to-Vector Generation"></a>NIVeL: Neural Implicit Vector Layers for Text-to-Vector Generation</h2><p><strong>Authors:Vikas Thamizharasan, Difan Liu, Matthew Fisher, Nanxuan Zhao, Evangelos Kalogerakis, Michal Lukac</strong></p>
<p>The success of denoising diffusion models in representing rich data distributions over 2D raster images has prompted research on extending them to other data representations, such as vector graphics. Unfortunately due to their variable structure and scarcity of vector training data, directly applying diffusion models on this domain remains a challenging problem. Using workarounds like optimization via Score Distillation Sampling (SDS) is also fraught with difficulty, as vector representations are non trivial to directly optimize and tend to result in implausible geometries such as redundant or self-intersecting shapes. NIVeL addresses these challenges by reinterpreting the problem on an alternative, intermediate domain which preserves the desirable properties of vector graphics – mainly sparsity of representation and resolution-independence. This alternative domain is based on neural implicit fields expressed in a set of decomposable, editable layers. Based on our experiments, NIVeL produces text-to-vector graphics results of significantly better quality than the state-of-the-art. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15217v1">PDF</a> </p>
<p><strong>Summary</strong><br>扩展去噪扩散模型到矢量图形领域的挑战性解决方案NIVeL。</p>
<p><strong>Key Takeaways</strong><br>• 去噪扩散模型在2D raster图像上的成功促使研究将其扩展到其他数据表示形式，如矢量图形。<br>• 直接将扩散模型应用于矢量图形领域是具有挑战性的，因为矢量图形具有可变结构和稀疏的训练数据。<br>• 使用Score Distillation Sampling（SDS）等优化方法也存在困难，因为矢量表示难以直接优化，容易产生不可信的几何形状。<br>• NIVeL通过重新解释问题在中间域上，保留矢量图形的良好属性，例如稀疏表示和分辨率独立性。<br>• 中间域基于可分解、可编辑的神经隐式字段层。<br>• 实验结果表明，NIVeL生成的文本到矢量图形结果远优于当前最先进的结果。<br>• NIVeL解决了扩展去噪扩散模型到矢量图形领域的挑战性问题。</p>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li>
<p>Title: NIVeL: 神经隐式矢量图形生成（Neural Implicit Vector Graphics Generation）</p>
</li>
<li>
<p>Authors: Not provided</p>
</li>
<li>
<p>Affiliation: 不提供（Not provided）</p>
</li>
<li>
<p>Keywords: denoising diffusion models, vector graphics, neural implicit fields</p>
</li>
<li>
<p>Urls: Not provided, Github: None</p>
</li>
<li>
<p>Summary:</p>
</li>
<li>
<p>(1):该论文的研究背景是将去噪扩散模型从2D raster图像扩展到矢量图形领域，但矢量图形的可变结构和稀缺的训练数据使得直接应用去噪扩散模型变得困难。</p>
</li>
<li>
<p>(2):过去的方法包括直接应用去噪扩散模型和Score Distillation Sampling（SDS）优化，但这些方法存在一些问题，如生成的矢量图形可能包含冗余或自相交的形状。</p>
</li>
<li>
<p>(3):本论文提出了NIVeL方法，该方法通过将问题重新解释在中间域上，即基于神经隐式字段的可分解、可编辑的层来生成矢量图形。</p>
</li>
<li>
<p>(4):本论文的方法在文本到矢量图形任务上取得了明显优于现有方法的性能，证明了NIVeL方法的有效性。</p>
</li>
<li>
<p>方法：</p>
</li>
<li>
<p>(1):将矢量图形生成问题重新解释在中间域上，即基于神经隐式字段（Neural Implicit Fields）的可分解、可编辑的层，以便更好地处理矢量图形的可变结构和稀缺的训练数据。</p>
</li>
<li>
<p>(2):使用去噪扩散模型（Denoising Diffusion Models）在中间域上生成隐式表示，然后通过神经隐式字段将其转换为矢量图形。</p>
</li>
<li>
<p>(3):引入 Score Distillation Sampling（SDS）优化方法，以提高生成矢量图形的质量和多样性。</p>
</li>
<li>
<p>(4):在中间域上应用编辑操作，如形状变换、拓扑变化等，以增强生成矢量图形的可编辑性和灵活性。</p>
</li>
<li>
<p>(5):使用文本到矢量图形任务的实验结果验证NIVeL方法的有效性，证明其在生成高质量矢量图形方面的优势。</p>
</li>
<li>
<p>结论：</p>
</li>
<li>
<p>(1):该篇工作的重要性在于将去噪扩散模型应用于矢量图形生成领域，解决了矢量图形的可变结构和稀缺的训练数据问题，提高了生成矢量图形的质量和多样性。</p>
</li>
<li>
<p>(2):创新点：提出了一种基于神经隐式字段的矢量图形生成方法，能够更好地处理矢量图形的可变结构和稀缺的训练数据；性能：在文本到矢量图形任务上取得了明显优于现有方法的性能；工作量：需要大量的训练数据和计算资源，且当前的表示方式还存在一些限制，如层的数量限制等。</p>
</li>
</ol>



<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-deb0bce750c823b45864a06b1f2fdf37.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b05c16791ff3624415d2ca5a4bb2b01d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1ddb20e736aa45d7da426d42c0386fcb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-a127e1927a9826d4a5a6449d4ce7f25e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-6ef7a2dd3802c3e38639f59aa13e5305.jpg" align="middle">
</details>




<h2 id="TerDiT-Ternary-Diffusion-Models-with-Transformers"><a href="#TerDiT-Ternary-Diffusion-Models-with-Transformers" class="headerlink" title="TerDiT: Ternary Diffusion Models with Transformers"></a>TerDiT: Ternary Diffusion Models with Transformers</h2><p><strong>Authors:Xudong Lu, Aojun Zhou, Ziyi Lin, Qi Liu, Yuhui Xu, Renrui Zhang, Yafei Wen, Shuai Ren, Peng Gao, Junchi Yan, Hongsheng Li</strong></p>
<p>Recent developments in large-scale pre-trained text-to-image diffusion models have significantly improved the generation of high-fidelity images, particularly with the emergence of diffusion models based on transformer architecture (DiTs). Among these diffusion models, diffusion transformers have demonstrated superior image generation capabilities, boosting lower FID scores and higher scalability. However, deploying large-scale DiT models can be expensive due to their extensive parameter numbers. Although existing research has explored efficient deployment techniques for diffusion models such as model quantization, there is still little work concerning DiT-based models. To tackle this research gap, in this paper, we propose TerDiT, a quantization-aware training (QAT) and efficient deployment scheme for ternary diffusion models with transformers. We focus on the ternarization of DiT networks and scale model sizes from 600M to 4.2B. Our work contributes to the exploration of efficient deployment strategies for large-scale DiT models, demonstrating the feasibility of training extremely low-bit diffusion transformer models from scratch while maintaining competitive image generation capacities compared to full-precision models. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/Lucky-Lance/TerDiT">https://github.com/Lucky-Lance/TerDiT</a>. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14854v1">PDF</a> 18 pages, 13 figures</p>
<p><strong>Summary</strong><br>大规模预训练文本到图像扩散模型的最新发展，提出了一种量化感知训练和高效部署方案TerDiT，用于三级扩散模型的 transformers。</p>
<p><strong>Key Takeaways</strong><br>• 大规模预训练文本到图像扩散模型的最新发展，特别是基于 transformer 架构的扩散模型（DiTs），生成高保真图像的能力得到了显著改善。<br>• 扩散变压器模型展示出优越的图像生成能力，具有较低的 FID 分数和更高的可扩展性。<br>• 部署大规模 DiT 模型可能很昂贵，因为它们具有庞大的参数数量。<br>• 现有的研究已经探索了扩散模型的高效部署技术，如模型量化，但对于 DiT 基础模型的研究仍然很少。<br>• 本文提出了 TerDiT，一种量化感知训练和高效部署方案，用于三级扩散模型的 transformers。<br>• 该方案关注 DiT 网络的三级化，并将模型大小从 600M 扩展到 4.2B。<br>• 本工作为大规模 DiT 模型的高效部署策略做出了贡献，证明了从头训练极低位扩散变压器模型的可行性，同时保持了与全精度模型相似的图像生成能力。</p>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<h2>Paper:1</h2>
<ol>
<li>
<p>Title: TerDiT：具有变压器的三进制扩散模型 (TerDiT: Ternary Diffusion Models with Transformers)</p>
</li>
<li>
<p>Authors: Xudong Lu, Aojun Zhou, Ziyi Lin, Qi Liu, Yuhui Xu, Renrui Zhang, Yafei Wen, Shuai Ren, Peng Gao, Junchi Yan, Hongsheng Li</p>
</li>
<li>
<p>Affiliation: 香港中文大学多媒体实验室</p>
</li>
<li>
<p>Keywords: diffusion models, transformer architecture, quantization-aware training, efficient deployment</p>
</li>
<li>
<p>Urls: https://arxiv.org/abs/2405.14854, Github: https://github.com/Lucky-Lance/TerDiT</p>
</li>
<li>
<p>Summary:</p>
<ul>
<li>
<p>(1):最近，大规模预训练文本到图像扩散模型的发展极大地改善了高保真图像的生成，特别是基于变压器架构（DiTs）的扩散模型。</p>
</li>
<li>
<p>(2):现有的研究已经探索了扩散模型的高效部署技术，如模型量化，但是在DiT模型方面仍然存在研究gap。</p>
</li>
<li>
<p>(3):本文提出TerDiT，一个量化感知训练（QAT）和高效部署方案，用于具有变压器的三进制扩散模型。</p>
</li>
<li>
<p>(4):本文的方法可以训练极低比特扩散变压器模型，从而实现与全精度模型相媲美的图像生成能力，同时也实现了高效的模型部署。</p>
</li>
<li>方法：</li>
</ul>
</li>
<li>
<p>(1)：采用假量函数（fake quant function）对模型权重进行量化，设置n_bits=4，不进行激活量化。</p>
</li>
<li>
<p>(2)：对原DiT块中的所有线性层权重进行量化，包括自注意、前馈和MLP。</p>
</li>
<li>
<p>(3)：使用量化后的模型采样图像，并与全精度模型进行比较。</p>
</li>
<li>
<p>(4)：提出TerDiT，一个量化感知训练（QAT）和高效部署方案，用于具有变压器的三进制扩散模型。</p>
</li>
<li>
<p>(5)：采用学习率减小策略，以提高模型的训练结果。</p>
</li>
<li>
<p>(6)：使用RMS Normalized adaLN模块，以提高模型的生成质量。</p>
</li>
<li>
<p>(7)：进行实验比较，验证TerDiT模型在高效部署和图像生成能力方面的优势。</p>
</li>
<li>
<p>结论：</p>
</li>
<li>
<p>(1):该工作的重要性在于它推动了具有变压器架构的扩散模型的高效部署，满足了实际应用中的低延迟和低计算资源需求。</p>
</li>
<li>
<p>(2):创新点：TerDiT 模型提出了一种量化感知训练（QAT）和高效部署方案，解决了现有DiT 模型在高效部署方面的研究gap；性能：TerDiT 模型在图像生成能力方面与全精度模型相媲美，同时实现了高效的模型部署；工作量：该工作需要大量的实验设计和模型训练，且需要深入了解DiT 模型和量化技术。</p>
</li>
</ol>


<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-c40afa8caaa8fb0e34704a216ee65f09.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-21147ce65723c9373a1e3d28f5c516df.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b32f6ca859af81585bc0599f40dc4518.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-05-28/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2024-05-28/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-05-28/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-33e1c85bbd2586fc6e8eb024aa73c567.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-05-28  InstructAvatar Text-Guided Emotion and Motion Control for Avatar   Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-05-28
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-05-28/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-dc27e0e81b6be96603dd90e8aa23e081.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2024-05-28  InstructAvatar Text-Guided Emotion and Motion Control for Avatar   Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-05-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">7390.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
