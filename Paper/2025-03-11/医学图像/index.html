<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-11  Task-oriented Uncertainty Collaborative Learning for Label-Efficient   Brain Tumor Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0e7804ec03209bb5563c5f11aa2c3130.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-11-æ›´æ–°"><a href="#2025-03-11-æ›´æ–°" class="headerlink" title="2025-03-11 æ›´æ–°"></a>2025-03-11 æ›´æ–°</h1><h2 id="Task-oriented-Uncertainty-Collaborative-Learning-for-Label-Efficient-Brain-Tumor-Segmentation"><a href="#Task-oriented-Uncertainty-Collaborative-Learning-for-Label-Efficient-Brain-Tumor-Segmentation" class="headerlink" title="Task-oriented Uncertainty Collaborative Learning for Label-Efficient   Brain Tumor Segmentation"></a>Task-oriented Uncertainty Collaborative Learning for Label-Efficient   Brain Tumor Segmentation</h2><p><strong>Authors:Zhenxuan Zhang, Hongjie Wu, Jiahao Huang, Baihong Xie, Zhifan Gao, Junxian Du, Pete Lally, Guang Yang</strong></p>
<p>Multi-contrast magnetic resonance imaging (MRI) plays a vital role in brain tumor segmentation and diagnosis by leveraging complementary information from different contrasts. Each contrast highlights specific tumor characteristics, enabling a comprehensive understanding of tumor morphology, edema, and pathological heterogeneity. However, existing methods still face the challenges of multi-level specificity perception across different contrasts, especially with limited annotations. These challenges include data heterogeneity, granularity differences, and interference from redundant information. To address these limitations, we propose a Task-oriented Uncertainty Collaborative Learning (TUCL) framework for multi-contrast MRI segmentation. TUCL introduces a task-oriented prompt attention (TPA) module with intra-prompt and cross-prompt attention mechanisms to dynamically model feature interactions across contrasts and tasks. Additionally, a cyclic process is designed to map the predictions back to the prompt to ensure that the prompts are effectively utilized. In the decoding stage, the TUCL framework proposes a dual-path uncertainty refinement (DUR) strategy which ensures robust segmentation by refining predictions iteratively. Extensive experimental results on limited labeled data demonstrate that TUCL significantly improves segmentation accuracy (88.2% in Dice and 10.853 mm in HD95). It shows that TUCL has the potential to extract multi-contrast information and reduce the reliance on extensive annotations. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/Zhenxuan-Zhang/TUCL_BrainSeg">https://github.com/Zhenxuan-Zhang/TUCL_BrainSeg</a>. </p>
<blockquote>
<p>å¤šå¯¹æ¯”åº¦ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨è„‘è‚¿ç˜¤åˆ†å‰²å’Œè¯Šæ–­ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œå®ƒé€šè¿‡åˆ©ç”¨æ¥è‡ªä¸åŒå¯¹æ¯”åº¦çš„è¡¥å……ä¿¡æ¯æ¥å‘æŒ¥ä½œç”¨ã€‚æ¯ä¸ªå¯¹æ¯”åº¦éƒ½çªå‡ºäº†ç‰¹å®šçš„è‚¿ç˜¤ç‰¹å¾ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿå…¨é¢äº†è§£è‚¿ç˜¤çš„å½¢æ€ã€æ°´è‚¿å’Œç—…ç†å¼‚è´¨æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä»ç„¶é¢ä¸´ç€ä¸åŒå¯¹æ¯”åº¦ä¹‹é—´çš„å¤šå±‚æ¬¡ç‰¹å¼‚æ€§æ„ŸçŸ¥æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯æ ‡æ³¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚è¿™äº›æŒ‘æˆ˜åŒ…æ‹¬æ•°æ®å¼‚è´¨æ€§ã€ç²’åº¦å·®å¼‚ä»¥åŠæ¥è‡ªå†—ä½™ä¿¡æ¯çš„å¹²æ‰°ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘ä»»åŠ¡çš„ååŒä¸ç¡®å®šæ€§å­¦ä¹ ï¼ˆTUCLï¼‰æ¡†æ¶ï¼Œç”¨äºå¤šå¯¹æ¯”åº¦MRIåˆ†å‰²ã€‚TUCLå¼•å…¥äº†ä¸€ä¸ªé¢å‘ä»»åŠ¡çš„æç¤ºæ³¨æ„åŠ›ï¼ˆTPAï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å…·æœ‰å†…éƒ¨æç¤ºå’Œè·¨æç¤ºæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯ä»¥åŠ¨æ€åœ°å»ºæ¨¡è·¨å¯¹æ¯”åº¦å’Œä»»åŠ¡çš„ç‰¹å¾äº¤äº’ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªå¾ªç¯è¿‡ç¨‹ï¼Œå°†é¢„æµ‹æ˜ å°„å›æç¤ºï¼Œä»¥ç¡®ä¿æœ‰æ•ˆåœ°åˆ©ç”¨æç¤ºã€‚åœ¨è§£ç é˜¶æ®µï¼ŒTUCLæ¡†æ¶æå‡ºäº†ä¸€ç§åŒè·¯å¾„ä¸ç¡®å®šæ€§ä¼˜åŒ–ï¼ˆDURï¼‰ç­–ç•¥ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–é¢„æµ‹ï¼Œç¡®ä¿ç¨³å¥çš„åˆ†å‰²æ•ˆæœã€‚åœ¨æœ‰é™æ ‡æ³¨æ•°æ®ä¸Šçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼ŒTUCLæ˜¾è‘—æé«˜äº†åˆ†å‰²ç²¾åº¦ï¼ˆDiceç³»æ•°ä¸º88.2%ï¼ŒHD95ä¸º10.853mmï¼‰ã€‚è¿™è¡¨æ˜TUCLå…·æœ‰æå–å¤šå¯¹æ¯”åº¦ä¿¡æ¯å¹¶å‡å°‘å¯¹é¢æ ‡æ³¨ä¾èµ–çš„æ½œåŠ›ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/Zhenxuan-Zhang/TUCL_BrainSeg%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Zhenxuan-Zhang/TUCL_BrainSegè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05682v1">PDF</a> </p>
<p><strong>Summary</strong><br>    å¤šå¯¹æ¯”ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨è„‘è‚¿ç˜¤åˆ†å‰²å’Œè¯Šæ–­ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œé€šè¿‡ä¸åŒå¯¹æ¯”ä¿¡æ¯æ­ç¤ºè‚¿ç˜¤ç‰¹å¾ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´è·¨å¯¹æ¯”å¤šå±‚æ¬¡ç‰¹å¼‚æ€§æ„ŸçŸ¥çš„æŒ‘æˆ˜ï¼Œå¦‚æ•°æ®å¼‚è´¨æ€§ã€ç²’åº¦å·®å¼‚å’Œå†—ä½™ä¿¡æ¯å¹²æ‰°ã€‚ä¸ºæ­¤ï¼Œæå‡ºä»»åŠ¡å¯¼å‘ä¸ç¡®å®šæ€§ååŒå­¦ä¹ ï¼ˆTUCLï¼‰æ¡†æ¶ï¼ŒåŒ…å«ä»»åŠ¡å¯¼å‘æç¤ºæ³¨æ„åŠ›ï¼ˆTPAï¼‰æ¨¡å—å’Œå¾ªç¯è¿‡ç¨‹ï¼Œå®ç°è·¨å¯¹æ¯”å’Œä»»åŠ¡ç‰¹å¾äº¤äº’åŠ¨æ€å»ºæ¨¡ã€‚è§£ç é˜¶æ®µé‡‡ç”¨åŒè·¯å¾„ä¸ç¡®å®šæ€§ä¼˜åŒ–ï¼ˆDURï¼‰ç­–ç•¥ï¼Œç¡®ä¿åœ¨æœ‰é™æ ‡æ³¨æ•°æ®ä¸‹ç²¾å‡†åˆ†å‰²ã€‚TUCLæ¡†æ¶æ˜¾è‘—æé«˜åˆ†å‰²ç²¾åº¦ï¼ˆDice 88.2%ï¼ŒHD95ä¸º10.853mmï¼‰ï¼Œè¯æ˜å…¶æŒ–æ˜å¤šå¯¹æ¯”ä¿¡æ¯åŠå‡å°‘å¯¹å¤§é‡æ ‡æ³¨çš„ä¾èµ–æ½œåŠ›ã€‚ç›¸å…³ä»£ç å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/Zhenxuan-Zhang/TUCL_BrainSeg">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤šå¯¹æ¯”ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨è„‘è‚¿ç˜¤è¯Šæ–­ä¸åˆ†å‰²ä¸­ä½œç”¨å…³é”®ï¼Œé€šè¿‡ä¸åŒå¯¹æ¯”ä¿¡æ¯å…¨é¢æ­ç¤ºè‚¿ç˜¤ç‰¹æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´è·¨å¯¹æ¯”å¤šå±‚æ¬¡ç‰¹å¼‚æ€§æ„ŸçŸ¥çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®å¼‚è´¨æ€§ã€ç²’åº¦å·®å¼‚å’Œå†—ä½™ä¿¡æ¯å¹²æ‰°ã€‚</li>
<li>æå‡ºçš„ä»»åŠ¡å¯¼å‘ä¸ç¡®å®šæ€§ååŒå­¦ä¹ ï¼ˆTUCLï¼‰æ¡†æ¶åŒ…æ‹¬TPAæ¨¡å—å’Œå¾ªç¯è¿‡ç¨‹ï¼Œå®ç°ç‰¹å¾äº¤äº’çš„åŠ¨æ€å»ºæ¨¡ã€‚</li>
<li>TUCLæ¡†æ¶é‡‡ç”¨åŒè·¯å¾„ä¸ç¡®å®šæ€§ä¼˜åŒ–ï¼ˆDURï¼‰ç­–ç•¥ï¼Œç¡®ä¿ç²¾å‡†åˆ†å‰²ï¼Œå³ä½¿åœ¨æœ‰é™æ ‡æ³¨æ•°æ®ä¸‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05682">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0e7804ec03209bb5563c5f11aa2c3130.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3d9677e59799fda26959a0625fac8ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-176a97e524006cef941b6ed5b7be5a37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e338a1ce43e6d294411f56bae13ec652.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25fb92ea04cd120bdc94ba1c105501f2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CACTUS-An-Open-Dataset-and-Framework-for-Automated-Cardiac-Assessment-and-Classification-of-Ultrasound-Images-Using-Deep-Transfer-Learning"><a href="#CACTUS-An-Open-Dataset-and-Framework-for-Automated-Cardiac-Assessment-and-Classification-of-Ultrasound-Images-Using-Deep-Transfer-Learning" class="headerlink" title="CACTUS: An Open Dataset and Framework for Automated Cardiac Assessment   and Classification of Ultrasound Images Using Deep Transfer Learning"></a>CACTUS: An Open Dataset and Framework for Automated Cardiac Assessment   and Classification of Ultrasound Images Using Deep Transfer Learning</h2><p><strong>Authors:Hanae Elmekki, Ahmed Alagha, Hani Sami, Amanda Spilkin, Antonela Mariel Zanuttini, Ehsan Zakeri, Jamal Bentahar, Lyes Kadem, Wen-Fang Xie, Philippe Pibarot, Rabeb Mizouni, Hadi Otrok, Shakti Singh, Azzam Mourad</strong></p>
<p>Cardiac ultrasound (US) scanning is a commonly used techniques in cardiology to diagnose the health of the heart and its proper functioning. Therefore, it is necessary to consider ways to automate these tasks and assist medical professionals in classifying and assessing cardiac US images. Machine learning (ML) techniques are regarded as a prominent solution due to their success in numerous applications aimed at enhancing the medical field, including addressing the shortage of echography technicians. However, the limited availability of medical data presents a significant barrier to applying ML in cardiology, particularly regarding US images of the heart. This paper addresses this challenge by introducing the first open graded dataset for Cardiac Assessment and ClassificaTion of UltraSound (CACTUS), which is available online. This dataset contains images obtained from scanning a CAE Blue Phantom and representing various heart views and different quality levels, exceeding the conventional cardiac views typically found in the literature. Additionally, the paper introduces a Deep Learning (DL) framework consisting of two main components. The first component classifies cardiac US images based on the heart view using a Convolutional Neural Network (CNN). The second component uses Transfer Learning (TL) to fine-tune the knowledge from the first component and create a model for grading and assessing cardiac images. The framework demonstrates high performance in both classification and grading, achieving up to 99.43% accuracy and as low as 0.3067 error, respectively. To showcase its robustness, the framework is further fine-tuned using new images representing additional cardiac views and compared to several other state-of-the-art architectures. The frameworkâ€™s outcomes and performance in handling real-time scans were also assessed using a questionnaire answered by cardiac experts. </p>
<blockquote>
<p>å¿ƒè„è¶…å£°ï¼ˆUSï¼‰æ‰«ææ˜¯å¿ƒè„ç—…å­¦ä¸­å¸¸ç”¨çš„æŠ€æœ¯ï¼Œç”¨äºè¯Šæ–­å¿ƒè„çš„å¥åº·çŠ¶å†µå’Œæ­£å¸¸åŠŸèƒ½ã€‚å› æ­¤ï¼Œå¿…é¡»è€ƒè™‘è‡ªåŠ¨åŒ–è¿™äº›ä»»åŠ¡ï¼Œå¹¶å¸®åŠ©åŒ»ç–—ä¸“ä¸šäººå£«å¯¹å¿ƒè„è¶…å£°å›¾åƒè¿›è¡Œåˆ†ç±»å’Œè¯„ä¼°ã€‚æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æŠ€æœ¯è¢«è§†ä¸ºä¸€ç§çªå‡ºçš„è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºå®ƒä»¬åœ¨è®¸å¤šæ—¨åœ¨æ”¹å–„åŒ»å­¦é¢†åŸŸçš„åº”ç”¨ä¸­å–å¾—äº†æˆåŠŸï¼ŒåŒ…æ‹¬è§£å†³è¶…å£°æŠ€æœ¯äººå‘˜çŸ­ç¼ºçš„é—®é¢˜ã€‚ç„¶è€Œï¼ŒåŒ»ç–—æ•°æ®çš„æœ‰é™å¯ç”¨æ€§æ˜¯å¿ƒè„ç—…å­¦ä¸­åº”ç”¨æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡å¤§éšœç¢ï¼Œå°¤å…¶æ˜¯å…³äºå¿ƒè„çš„è¶…å£°å›¾åƒã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥å¿ƒè„è¯„ä¼°å’Œåˆ†ç±»è¶…å£°ï¼ˆCACTUSï¼‰çš„ç¬¬ä¸€ä¸ªå…¬å¼€åˆ†çº§æ•°æ®é›†æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¯¥æ•°æ®é›†å¯åœ¨ç½‘ä¸Šè·å¾—ã€‚è¯¥æ•°æ®é›†åŒ…å«ä»æ‰«æCAE Blue Phantomè·å¾—çš„å›¾åƒï¼Œä»£è¡¨ä¸åŒçš„å¿ƒè„è§†å›¾å’Œä¸åŒè´¨é‡çº§åˆ«ï¼Œè¶…è¿‡äº†æ–‡çŒ®ä¸­é€šå¸¸å‘ç°çš„ä¼ ç»Ÿå¿ƒè„è§†å›¾ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ã€‚ç¬¬ä¸€ä¸ªç»„ä»¶ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ ¹æ®å¿ƒè„è§†å›¾å¯¹å¿ƒè„è¶…å£°å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚ç¬¬äºŒä¸ªç»„ä»¶ä½¿ç”¨è¿ç§»å­¦ä¹ ï¼ˆTLï¼‰å¯¹ç¬¬ä¸€ä¸ªç»„ä»¶çš„çŸ¥è¯†è¿›è¡Œå¾®è°ƒï¼Œå¹¶åˆ›å»ºä¸€ä¸ªç”¨äºåˆ†çº§å’Œè¯„ä¼°å¿ƒè„å›¾åƒçš„æ¨¡å‹ã€‚è¯¥æ¡†æ¶åœ¨åˆ†ç±»å’Œåˆ†çº§æ–¹é¢éƒ½è¡¨ç°å‡ºå‡ºè‰²çš„æ€§èƒ½ï¼Œåˆ†åˆ«è¾¾åˆ°äº†é«˜è¾¾99.43ï¼…çš„å‡†ç¡®ç‡å’Œä½è‡³0.3067çš„é”™è¯¯ç‡ã€‚ä¸ºäº†å±•ç¤ºå…¶ç¨³å¥æ€§ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨ä»£è¡¨å…¶ä»–å¿ƒè„è§†å›¾çš„å›¾åƒè¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶ä¸å…¶ä»–ä¸€äº›æœ€å…ˆè¿›çš„æ¶æ„è¿›è¡Œäº†æ¯”è¾ƒã€‚æ¡†æ¶å¤„ç†å®æ—¶æ‰«æçš„ç»“æœå’Œæ€§èƒ½ä¹Ÿé€šè¿‡å¿ƒè„ç—…ä¸“å®¶å›ç­”çš„é—®å·è¿›è¡Œäº†è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05604v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§è§£å†³å¿ƒè„è¶…å£°æ‰«æè‡ªåŠ¨åŒ–åˆ†ç±»å’Œè¯„ä¼°é—®é¢˜çš„æ–¹æ³•ã€‚é’ˆå¯¹åŒ»å­¦æ•°æ®æœ‰é™è¿™ä¸€æŒ‘æˆ˜ï¼Œæ–‡ç« å¼•å…¥äº†é¦–ä¸ªå…¬å¼€åˆ†çº§çš„å¿ƒè„è¶…å£°å›¾åƒè¯„ä¼°ä¸åˆ†ç±»æ•°æ®é›†CACTUSï¼ŒåŒ…å«ä»ä¸åŒè§†è§’å’Œä¸åŒè´¨é‡æ°´å¹³æ‰«æå¾—åˆ°çš„å›¾åƒã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸€ç§æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬åŸºäºå·ç§¯ç¥ç»ç½‘ç»œçš„å¿ƒè¶…å›¾åƒè§†è§’åˆ†ç±»å’ŒåŸºäºè¿ç§»å­¦ä¹ çš„å¿ƒè„å›¾åƒåˆ†çº§è¯„ä¼°æ¨¡å‹ã€‚è¯¥æ¡†æ¶åœ¨åˆ†ç±»å’Œåˆ†çº§æ–¹é¢è¡¨ç°å‡ºé«˜æ€§èƒ½ï¼Œå‡†ç¡®ç‡é«˜è¾¾99.43%ï¼Œè¯¯å·®ä½è‡³0.3067ã€‚å¹¶é€šè¿‡ä¸“å®¶é—®å·è¯„ä¼°äº†å…¶åœ¨å¤„ç†å®æ—¶æ‰«æä¸­çš„ç¨³å¥æ€§å’Œæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¿ƒè„è¶…å£°æ‰«ææ˜¯è¯Šæ–­å¿ƒè„å¥åº·å’Œå…¶åŠŸèƒ½çš„é‡è¦æŠ€æœ¯ï¼Œè‡ªåŠ¨åŒ–åˆ†ç±»å’Œè¯„ä¼°è¿™äº›å›¾åƒå¯¹åŒ»ç–—ä¸“ä¸šäººå‘˜çš„å¸®åŠ©å¾ˆå¤§ã€‚</li>
<li>æœºå™¨å­¦ä¹ æŠ€æœ¯åœ¨åŒ»ç–—é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†åŒ»å­¦æ•°æ®çš„æœ‰é™æ€§æ˜¯åº”ç”¨æœºå™¨å­¦ä¹ äºå¿ƒè„ç—…å­¦çš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡å¼•å…¥äº†CACTUSæ•°æ®é›†ï¼ŒåŒ…å«ä¸åŒå¿ƒè„è§†è§’å’Œä¸åŒè´¨é‡æ°´å¹³çš„å›¾åƒï¼Œè§£å†³äº†åŒ»å­¦æ•°æ®æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„æ·±åº¦å­¦ä¹ æ¡†æ¶åŒ…æ‹¬åŸºäºCNNçš„å¿ƒè¶…å›¾åƒè§†è§’åˆ†ç±»å’ŒåŸºäºè¿ç§»å­¦ä¹ çš„å¿ƒè„å›¾åƒåˆ†çº§è¯„ä¼°æ¨¡å‹ã€‚</li>
<li>æ¡†æ¶åœ¨åˆ†ç±»å’Œåˆ†çº§æ–¹é¢è¡¨ç°å‡ºé«˜æ€§èƒ½ï¼Œå‡†ç¡®ç‡é«˜è¾¾99.43%ï¼Œè¯¯å·®ä½ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ä¸“å®¶é—®å·è¯„ä¼°äº†å…¶åœ¨å¤„ç†å®æ—¶æ‰«æä¸­çš„ç¨³å¥æ€§å’Œæ€§èƒ½ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå¿ƒè„è¶…å£°æ‰«æçš„è‡ªåŠ¨åŒ–å’Œæ™ºèƒ½åŒ–æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4846966a427b9e3d5224de751598f138.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a548f43dcd2fda512cd7a9e19da0fbd3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Disconnect-to-Connect-A-Data-Augmentation-Method-for-Improving-Topology-Accuracy-in-Image-Segmentation"><a href="#Disconnect-to-Connect-A-Data-Augmentation-Method-for-Improving-Topology-Accuracy-in-Image-Segmentation" class="headerlink" title="Disconnect to Connect: A Data Augmentation Method for Improving Topology   Accuracy in Image Segmentation"></a>Disconnect to Connect: A Data Augmentation Method for Improving Topology   Accuracy in Image Segmentation</h2><p><strong>Authors:Juan Miguel Valverde, Maja Ã˜stergaard, Adrian Rodriguez-Palomo, Peter Alling Strange Vibe, Nina KÃ¸lln Wittig, Henrik Birkedal, Anders Bjorholm Dahl</strong></p>
<p>Accurate segmentation of thin, tubular structures (e.g., blood vessels) is challenging for deep neural networks. These networks classify individual pixels, and even minor misclassifications can break the thin connections within these structures. Existing methods for improving topology accuracy, such as topology loss functions, rely on very precise, topologically-accurate training labels, which are difficult to obtain. This is because annotating images, especially 3D images, is extremely laborious and time-consuming. Low image resolution and contrast further complicates the annotation by causing tubular structures to appear disconnected. We present CoLeTra, a data augmentation strategy that integrates to the models the prior knowledge that structures that appear broken are actually connected. This is achieved by creating images with the appearance of disconnected structures while maintaining the original labels. Our extensive experiments, involving different architectures, loss functions, and datasets, demonstrate that CoLeTra leads to segmentations topologically more accurate while often improving the Dice coefficient and Hausdorff distance. CoLeTraâ€™s hyper-parameters are intuitive to tune, and our sensitivity analysis shows that CoLeTra is robust to changes in these hyper-parameters. We also release a dataset specifically suited for image segmentation methods with a focus on topology accuracy. CoLetraâ€™s code can be found at <a target="_blank" rel="noopener" href="https://github.com/jmlipman/CoLeTra">https://github.com/jmlipman/CoLeTra</a>. </p>
<blockquote>
<p>ç²¾ç¡®åˆ†å‰²è–„ç®¡çŠ¶ç»“æ„ï¼ˆä¾‹å¦‚è¡€ç®¡ï¼‰å¯¹äºæ·±åº¦ç¥ç»ç½‘ç»œæ¥è¯´æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚è¿™äº›ç½‘ç»œå¯¹å•ä¸ªåƒç´ è¿›è¡Œåˆ†ç±»ï¼Œç”šè‡³è½»å¾®çš„è¯¯åˆ†ç±»ä¹Ÿå¯èƒ½ä¼šç ´åè¿™äº›ç»“æ„ä¸­çš„è–„è¿æ¥ã€‚ç°æœ‰çš„æé«˜æ‹“æ‰‘ç²¾åº¦çš„æ–¹æ³•ï¼Œå¦‚æ‹“æ‰‘æŸå¤±å‡½æ•°ï¼Œä¾èµ–äºéå¸¸ç²¾ç¡®ä¸”æ‹“æ‰‘å‡†ç¡®çš„è®­ç»ƒæ ‡ç­¾ï¼Œè€Œè¿™äº›æ ‡ç­¾çš„è·å–éå¸¸å›°éš¾ã€‚è¿™æ˜¯å› ä¸ºæ ‡æ³¨å›¾åƒï¼Œå°¤å…¶æ˜¯3Då›¾åƒï¼Œæ˜¯ä¸€é¡¹æå…¶ç¹çå’Œè€—æ—¶çš„ä»»åŠ¡ã€‚ä½å›¾åƒåˆ†è¾¨ç‡å’Œå¯¹æ¯”åº¦ä¼šä½¿ç®¡çŠ¶ç»“æ„å‡ºç°æ–­è£‚çš„å¤–è§‚ï¼Œä»è€Œè¿›ä¸€æ­¥ä½¿æ ‡æ³¨å¤æ‚åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†CoLeTraï¼Œè¿™æ˜¯ä¸€ç§æ•°æ®å¢å¼ºç­–ç•¥ï¼Œå®ƒå°†å…ˆéªŒçŸ¥è¯†æ•´åˆåˆ°æ¨¡å‹ä¸­ï¼Œå³å¤–è§‚æ–­è£‚çš„ç»“æ„å®é™…ä¸Šæ˜¯ç›¸è¿çš„ã€‚è¿™æ˜¯é€šè¿‡åˆ›å»ºå…·æœ‰æ–­è£‚ç»“æ„å¤–è§‚çš„å›¾åƒåŒæ—¶ä¿æŒåŸå§‹æ ‡ç­¾æ¥å®ç°çš„ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œæ¶‰åŠä¸åŒçš„æ¶æ„ã€æŸå¤±å‡½æ•°å’Œæ•°æ®é›†ï¼Œè¯æ˜CoLeTraåœ¨æ‹“æ‰‘ä¸Šå®ç°äº†æ›´ç²¾ç¡®çš„åˆ†å‰²ï¼ŒåŒæ—¶ç»å¸¸æé«˜Diceç³»æ•°å’ŒHausdorffè·ç¦»ã€‚CoLeTraçš„è¶…å‚æ•°æ˜“äºè°ƒæ•´ï¼Œæˆ‘ä»¬çš„æ•æ„Ÿæ€§åˆ†æè¡¨æ˜CoLeTraå¯¹è¿™äº›è¶…å‚æ•°çš„å˜åŠ¨å…·æœ‰ç¨³å¥æ€§ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªä¸“é—¨ç”¨äºå›¾åƒåˆ†å‰²æ–¹æ³•çš„æ•°æ®é›†ï¼Œé‡ç‚¹å…³æ³¨æ‹“æ‰‘ç²¾åº¦ã€‚CoLeTraçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/jmlipman/CoLeTra">https://github.com/jmlipman/CoLeTra</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05541v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCoLeTraçš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œç”¨äºæ”¹è¿›æ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹è–„ç®¡çŠ¶ç»“æ„ï¼ˆå¦‚è¡€ç®¡ï¼‰çš„åˆ†å‰²å‡†ç¡®æ€§ã€‚è¯¥ç­–ç•¥é€šè¿‡ç”Ÿæˆçœ‹ä¼¼æ–­è£‚çš„ç»“æ„å›¾åƒåŒæ—¶ä¿ç•™åŸå§‹æ ‡ç­¾ï¼Œä½¿æ¨¡å‹å­¦ä¹ åˆ°å®é™…è¿æ¥çš„å…ˆéªŒçŸ¥è¯†ã€‚å®éªŒè¡¨æ˜ï¼ŒCoLeTraèƒ½æé«˜æ‹“æ‰‘å‡†ç¡®æ€§ï¼ŒåŒæ—¶æ”¹å–„Diceç³»æ•°å’ŒHausdorffè·ç¦»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨åˆ†å‰²è–„ç®¡çŠ¶ç»“æ„ï¼ˆå¦‚è¡€ç®¡ï¼‰æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› å¾®å°è¯¯åˆ†ç±»å¯èƒ½å¯¼è‡´ç»“æ„æ–­è£‚ã€‚</li>
<li>ç°æœ‰æé«˜æ‹“æ‰‘å‡†ç¡®æ€§çš„æ–¹æ³•ä¾èµ–ç²¾ç¡®çš„è®­ç»ƒæ ‡ç­¾ï¼Œä½†è·å–è¿™äº›æ ‡ç­¾æä¸ºè€—æ—¶ã€‚</li>
<li>CoLeTraæ˜¯ä¸€ç§æ•°æ®å¢å¼ºç­–ç•¥ï¼Œä½¿æ¨¡å‹å­¦ä¹ åˆ°æ–­è£‚ç»“æ„å®é™…è¿æ¥çš„å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>CoLeTraé€šè¿‡ç”Ÿæˆçœ‹ä¼¼æ–­è£‚çš„ç»“æ„å›¾åƒåŒæ—¶ä¿ç•™åŸå§‹æ ‡ç­¾æ¥å®ç°è¿™ä¸€ç›®çš„ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCoLeTraèƒ½æé«˜æ‹“æ‰‘å‡†ç¡®æ€§ï¼ŒåŒæ—¶æ”¹å–„è¯„ä¼°åˆ†å‰²è´¨é‡çš„Diceç³»æ•°å’ŒHausdorffè·ç¦»ã€‚</li>
<li>CoLeTraçš„è¶…å‚æ•°æ˜“äºè°ƒæ•´ï¼Œä¸”å¯¹å‚æ•°å˜åŒ–å…·æœ‰é²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05541">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f3ca547dd517089fc626fe9e1821c04b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97b92e9760ffea96a5fba51f29423b25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a24ff9db781fd71d79f0b006e289cf3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bd6c37bd6f6e66b8e863e9328227ca7.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="S4M-Segment-Anything-with-4-Extreme-Points"><a href="#S4M-Segment-Anything-with-4-Extreme-Points" class="headerlink" title="S4M: Segment Anything with 4 Extreme Points"></a>S4M: Segment Anything with 4 Extreme Points</h2><p><strong>Authors:Adrien Meyer, Lorenzo Arboit, Giuseppe Massimiani, Francesco Brucchi, Luca Emanuele Amodio, Didier Mutter, Nicolas Padoy</strong></p>
<p>The Segment Anything Model (SAM) has revolutionized open-set interactive image segmentation, inspiring numerous adapters for the medical domain. However, SAM primarily relies on sparse prompts such as point or bounding box, which may be suboptimal for fine-grained instance segmentation, particularly in endoscopic imagery, where precise localization is critical and existing prompts struggle to capture object boundaries effectively. To address this, we introduce S4M (Segment Anything with 4 Extreme Points), which augments SAM by leveraging extreme points â€“ the top-, bottom-, left-, and right-most points of an instance â€“ prompts. These points are intuitive to identify and provide a faster, structured alternative to box prompts. However, a na&quot;ive use of extreme points degrades performance, due to SAMâ€™s inability to interpret their semantic roles. To resolve this, we introduce dedicated learnable embeddings, enabling the model to distinguish extreme points from generic free-form points and better reason about their spatial relationships. We further propose an auxiliary training task through the Canvas module, which operates solely on prompts â€“ without vision input â€“ to predict a coarse instance mask. This encourages the model to internalize the relationship between extreme points and mask distributions, leading to more robust segmentation. S4M outperforms other SAM-based approaches on three endoscopic surgical datasets, demonstrating its effectiveness in complex scenarios. Finally, we validate our approach through a human annotation study on surgical endoscopic videos, confirming that extreme points are faster to acquire than bounding boxes. </p>
<blockquote>
<p>Segment Anything Modelï¼ˆSAMï¼‰å·²ç»å½»åº•æ”¹å˜äº†å¼€æ”¾é›†äº¤äº’å¼å›¾åƒåˆ†å‰²ï¼Œå¹¶ä¸ºåŒ»å­¦é¢†åŸŸæ¿€å‘äº†ä¼—å¤šé€‚é…å™¨ã€‚ç„¶è€Œï¼ŒSAMä¸»è¦ä¾èµ–äºç¨€ç–æç¤ºï¼Œå¦‚ç‚¹æˆ–è¾¹ç•Œæ¡†ï¼Œè¿™å¯èƒ½å¯¹äºç²¾ç»†ç²’åº¦çš„å®ä¾‹åˆ†å‰²æ¥è¯´å¹¶ä¸ç†æƒ³ï¼Œç‰¹åˆ«æ˜¯åœ¨å†…çª¥é•œå½±åƒä¸­ï¼Œç²¾ç¡®çš„å®šä½è‡³å…³é‡è¦ï¼Œè€Œç°æœ‰çš„æç¤ºå¾ˆéš¾æœ‰æ•ˆåœ°æ•æ‰å¯¹è±¡è¾¹ç•Œã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†S4Mï¼ˆä½¿ç”¨å››ä¸ªæç‚¹è¿›è¡Œä»»ä½•åˆ†å‰²ï¼‰ï¼Œå®ƒé€šè¿‡åˆ©ç”¨å®ä¾‹çš„æç‚¹â€”â€”æœ€é¡¶éƒ¨ã€åº•éƒ¨ã€å·¦ä¾§å’Œå³ä¾§çš„ç‚¹â€”â€”æ¥å¢å¼ºSAMçš„æç¤ºã€‚è¿™äº›ç‚¹æ˜“äºè¯†åˆ«ï¼Œå¹¶æä¾›äº†ä¸€ç§æ›´å¿«ã€æ›´ç»“æ„åŒ–çš„æ›¿ä»£æ¡†æç¤ºã€‚ç„¶è€Œï¼Œæç«¯ç‚¹çš„ç›´æ¥ä½¿ç”¨ä¼šé™ä½æ€§èƒ½ï¼Œå› ä¸ºSAMæ— æ³•è§£é‡Šå®ƒä»¬çš„è¯­ä¹‰è§’è‰²ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸“ç”¨çš„å¯å­¦ä¹ åµŒå…¥ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŒºåˆ†æç«¯ç‚¹å’Œä¸€èˆ¬çš„è‡ªç”±å½¢å¼ç‚¹ï¼Œå¹¶æ›´å¥½åœ°æ¨ç†å®ƒä»¬ä¹‹é—´çš„ç©ºé—´å…³ç³»ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªè¾…åŠ©è®­ç»ƒä»»åŠ¡ï¼Œé€šè¿‡Canvasæ¨¡å—ï¼Œè¯¥æ¨¡å—ä»…é€šè¿‡æç¤ºè¿›è¡Œæ“ä½œâ€”â€”æ²¡æœ‰è§†è§‰è¾“å…¥â€”â€”æ¥é¢„æµ‹ç²—ç•¥çš„å®ä¾‹æ©ç ã€‚è¿™é¼“åŠ±æ¨¡å‹å†…åŒ–æç‚¹å’Œæ©ç åˆ†å¸ƒä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œå®ç°æ›´ç¨³å¥çš„åˆ†å‰²ã€‚S4Måœ¨ä¸‰ä¸ªå†…çª¥é•œæ‰‹æœ¯æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–SAMæ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨å¤æ‚åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡å…³äºæ‰‹æœ¯å†…çª¥é•œè§†é¢‘çš„äººç±»æ³¨é‡Šç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç¡®è®¤æç‚¹æ¯”è¾¹ç•Œæ¡†æ›´å¿«è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05534v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SAMæ¨¡å‹åœ¨å¼€æ”¾é›†äº¤äº’å¼å›¾åƒåˆ†å‰²é¢†åŸŸå…·æœ‰é©å‘½æ€§å½±å“ï¼Œå¹¶ä¸ºåŒ»å­¦é¢†åŸŸæä¾›äº†è®¸å¤šé€‚é…å™¨ã€‚ç„¶è€Œï¼ŒSAMä¸»è¦ä¾èµ–äºç¨€ç–æç¤ºï¼Œå¦‚ç‚¹æˆ–è¾¹ç•Œæ¡†ï¼Œå¯¹äºç²¾ç»†çš„å®ä¾‹åˆ†å‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨å†…çª¥é•œå½±åƒä¸­ï¼Œå¯èƒ½ä¸å¤Ÿç†æƒ³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†S4Mï¼ˆé€šè¿‡å››ç‚¹è¿›è¡Œä»»ä½•åˆ†å‰²ï¼‰ï¼Œå®ƒé€šè¿‡åˆ©ç”¨å®ä¾‹çš„æœ€ä¸Šã€æœ€ä¸‹ã€æœ€å·¦å’Œæœ€å³å››ç‚¹æç¤ºæ¥å¢å¼ºSAMã€‚è¿™äº›ç‚¹æ˜“äºè¯†åˆ«ï¼Œæä¾›äº†æ›´å¿«çš„ç»“æ„åŒ–æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå•çº¯ä½¿ç”¨æç‚¹ä¼šé™ä½æ€§èƒ½ï¼Œå› ä¸ºSAMæ— æ³•è§£é‡Šå®ƒä»¬çš„è¯­ä¹‰è§’è‰²ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸“ç”¨å¯å­¦ä¹ åµŒå…¥ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŒºåˆ†æç‚¹å’Œé€šç”¨è‡ªç”±å½¢å¼ç‚¹ï¼Œå¹¶æ›´å¥½åœ°æ¨ç†å®ƒä»¬ä¹‹é—´çš„ç©ºé—´å…³ç³»ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ä¸ªå†…çª¥é•œæ‰‹æœ¯æ•°æ®é›†ä¸Šä¼˜äºå…¶ä»–SAMæ–¹æ³•ï¼ŒéªŒè¯äº†å…¶åœ¨å¤æ‚åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬é€šè¿‡å…³äºæ‰‹æœ¯å†…çª¥é•œè§†é¢‘çš„äººç±»æ³¨é‡Šç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç¡®è®¤æç‚¹æ¯”è¾¹ç•Œæ¡†æ›´å®¹æ˜“è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SAMæ¨¡å‹åœ¨å¼€æ”¾é›†äº¤äº’å¼å›¾åƒåˆ†å‰²ä¸­çš„é‡è¦ä½œç”¨ï¼Œå¹¶æè¿°äº†å…¶å¯¹åŒ»å­¦é¢†åŸŸçš„å½±å“ã€‚</li>
<li>SAMä¸»è¦ä¾èµ–äºç¨€ç–æç¤ºï¼ˆå¦‚ç‚¹å’Œè¾¹ç•Œæ¡†ï¼‰è¿›è¡Œå›¾åƒåˆ†å‰²ï¼Œè¿™åœ¨ç²¾ç»†çš„å®ä¾‹åˆ†å‰²ä¸­å¯èƒ½å­˜åœ¨é—®é¢˜ã€‚</li>
<li>S4Mé€šè¿‡åˆ©ç”¨å®ä¾‹çš„å››ä¸ªæç‚¹ï¼ˆæœ€ä¸Šã€æœ€ä¸‹ã€æœ€å·¦ã€æœ€å³ï¼‰æ¥å¢å¼ºSAMï¼Œè¿™äº›ç‚¹æä¾›äº†ä¸€ç§æ›´å¿«é€Ÿå’Œç»“æ„åŒ–çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>å•çº¯ä½¿ç”¨æç‚¹ä¼šé™ä½æ€§èƒ½ï¼Œå› ä¸ºSAMæ— æ³•è§£é‡Šå®ƒä»¬çš„è¯­ä¹‰è§’è‰²ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œå¼•å…¥äº†ä¸“ç”¨å¯å­¦ä¹ åµŒå…¥å’Œè¾…åŠ©è®­ç»ƒä»»åŠ¡ã€‚</li>
<li>S4Måœ¨ä¸‰ä¸ªå†…çª¥é•œæ‰‹æœ¯æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–SAMæ–¹æ³•ï¼Œæ˜¾ç¤ºäº†å…¶åœ¨å¤æ‚åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05534">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-136e1bb2d0cf7eaf4e739585724f55ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3fde665c70456a5e4edabefdcc19f68a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-351b8e5f2e93cae617b528a45f377302.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20f62765021dc838d7b309a215562b24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3bc6d76d45e2f491ea773484b18748c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2683da772bb5031f0ccab39d1069e789.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="State-of-the-Art-Stroke-Lesion-Segmentation-at-1-1000th-of-Parameters"><a href="#State-of-the-Art-Stroke-Lesion-Segmentation-at-1-1000th-of-Parameters" class="headerlink" title="State-of-the-Art Stroke Lesion Segmentation at 1&#x2F;1000th of Parameters"></a>State-of-the-Art Stroke Lesion Segmentation at 1&#x2F;1000th of Parameters</h2><p><strong>Authors:Alex Fedorov, Yutong Bu, Xiao Hu, Chris Rorden, Sergey Plis</strong></p>
<p>Efficient and accurate whole-brain lesion segmentation remains a challenge in medical image analysis. In this work, we revisit MeshNet, a parameter-efficient segmentation model, and introduce a novel multi-scale dilation pattern with an encoder-decoder structure. This innovation enables capturing broad contextual information and fine-grained details without traditional downsampling, upsampling, or skip-connections. Unlike previous approaches processing subvolumes or slices, we operate directly on whole-brain $256^3$ MRI volumes. Evaluations on the Aphasia Recovery Cohort (ARC) dataset demonstrate that MeshNet achieves superior or comparable DICE scores to state-of-the-art architectures such as MedNeXt and U-MAMBA at 1&#x2F;1000th of parameters. Our results validate MeshNetâ€™s strong balance of efficiency and performance, making it particularly suitable for resource-limited environments such as web-based applications and opening new possibilities for the widespread deployment of advanced medical image analysis tools. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼Œé«˜æ•ˆä¸”å‡†ç¡®çš„å…¨è„‘ç—…å˜åˆ†å‰²ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°ç ”ç©¶äº†MeshNetï¼ˆä¸€ç§å‚æ•°é«˜æ•ˆçš„åˆ†å‰²æ¨¡å‹ï¼‰ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„å¤šå°ºåº¦è†¨èƒ€æ¨¡å¼ï¼Œè¯¥æ¨¡å¼å…·æœ‰ç¼–ç å™¨-è§£ç å™¨ç»“æ„ã€‚è¿™ç§åˆ›æ–°ä½¿å¾—èƒ½å¤Ÿåœ¨ä¸é‡‡ç”¨ä¼ ç»Ÿä¸‹é‡‡æ ·ã€ä¸Šé‡‡æ ·æˆ–è·³è¿‡è¿æ¥çš„æƒ…å†µä¸‹æ•è·å¹¿æ³›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œç²¾ç»†ç»†èŠ‚ã€‚ä¸ä»¥å¾€å¤„ç†å­ä½“ç§¯æˆ–åˆ‡ç‰‡çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬ç›´æ¥åœ¨å…¨è„‘çš„$ 256^3 $ MRIä½“ç§¯ä¸Šè¿›è¡Œæ“ä½œã€‚åœ¨é˜¿å¼—è¥¿äºšæ¢å¤é˜Ÿåˆ—ï¼ˆARCï¼‰æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMeshNetçš„DICEå¾—åˆ†ä¸æœ€æ–°æ¶æ„MedNeXtå’ŒU-MAMBAç›¸æ¯”è¡¨ç°ä¼˜è¶Šæˆ–ç›¸å½“ï¼Œè€Œå…¶å‚æ•°ä»…ä¸ºåè€…çš„åƒåˆ†ä¹‹ä¸€ã€‚æˆ‘ä»¬çš„ç»“æœéªŒè¯äº†MeshNetåœ¨æ•ˆç‡å’Œæ€§èƒ½æ–¹é¢çš„å¼ºå¤§å¹³è¡¡æ€§ï¼Œä½¿å…¶ç‰¹åˆ«é€‚ç”¨äºèµ„æºå—é™çš„ç¯å¢ƒï¼Œå¦‚åŸºäºWebçš„åº”ç”¨ç¨‹åºï¼Œå¹¶ä¸ºé«˜çº§åŒ»å­¦å›¾åƒåˆ†æå·¥å…·çš„å¹¿æ³›éƒ¨ç½²å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05531v1">PDF</a> International Symposium on Biomedical Imaging, April 14-17, 2025</p>
<p><strong>Summary</strong><br>é«˜æ•ˆä¸”ç²¾å‡†çš„å…¨è„‘ç—…å˜åˆ†å‰²åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­ä»æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶é‡æ–°è€ƒå¯Ÿäº†MeshNetè¿™ä¸€å‚æ•°é«˜æ•ˆçš„åˆ†å‰²æ¨¡å‹ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„å¤šå°ºåº¦è†¨èƒ€æ¨¡å¼ä¸ç¼–ç å™¨-è§£ç å™¨ç»“æ„ã€‚æ­¤åˆ›æ–°èƒ½åœ¨ä¸é‡‡ç”¨ä¼ ç»Ÿä¸‹é‡‡æ ·ã€ä¸Šé‡‡æ ·æˆ–è·³è¿‡è¿æ¥çš„æƒ…å†µä¸‹ï¼Œæ•æ‰å¹¿æ³›ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œç²¾ç»†ç»†èŠ‚ã€‚ä¸åŒäºå¤„ç†å­ä½“ç§¯æˆ–åˆ‡ç‰‡çš„å…ˆå‰æ–¹æ³•ï¼Œæˆ‘ä»¬ç›´æ¥åœ¨æ•´è„‘$256^3$çš„MRIä½“ç§¯ä¸Šè¿›è¡Œæ“ä½œã€‚åœ¨é˜¿å¼—è¥¿äºšæ¢å¤é˜Ÿåˆ—ï¼ˆARCï¼‰æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMeshNetçš„DICEå¾—åˆ†ä¸æœ€æ–°æ¶æ„MedNeXtå’ŒU-MAMBAç›¸æ¯”å…·æœ‰ä¼˜è¶Šæ€§æˆ–å¯æ¯”æ€§ï¼Œè€Œæ‰€éœ€çš„å‚æ•°åªæœ‰åè€…çš„åƒåˆ†ä¹‹ä¸€ã€‚æˆ‘ä»¬çš„ç»“æœéªŒè¯äº†MeshNetåœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´çš„å‡ºè‰²å¹³è¡¡ï¼Œä½¿å…¶æˆä¸ºèµ„æºå—é™ç¯å¢ƒï¼ˆå¦‚åŸºäºç½‘ç»œçš„åº”ç”¨ç¨‹åºï¼‰çš„ç†æƒ³é€‰æ‹©ï¼Œå¹¶ä¸ºå…ˆè¿›çš„åŒ»å­¦å›¾åƒåˆ†æå·¥å…·çš„å¹¿æ³›éƒ¨ç½²æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MeshNetæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­å…·æœ‰é«˜æ•ˆå’Œå‡†ç¡®çš„åˆ†å‰²èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥æ–°çš„å¤šå°ºåº¦è†¨èƒ€æ¨¡å¼ä¸ç¼–ç å™¨-è§£ç å™¨ç»“æ„ï¼Œæ•æ‰å¹¿æ³›ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œç²¾ç»†ç»†èŠ‚ã€‚</li>
<li>ç›´æ¥åœ¨æ•´è„‘MRIä½“ç§¯ä¸Šè¿›è¡Œæ“ä½œï¼Œæ— éœ€ä¼ ç»Ÿä¸‹é‡‡æ ·ã€ä¸Šé‡‡æ ·æˆ–è·³è¿‡è¿æ¥ã€‚</li>
<li>åœ¨ARCæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒMeshNetçš„DICEå¾—åˆ†ä¸æœ€æ–°æ¶æ„ç›¸æ¯”å…·æœ‰ä¼˜è¶Šæ€§æˆ–ç›¸å½“ã€‚</li>
<li>MeshNetæ¨¡å‹å‚æ•°æ•ˆç‡æé«˜ï¼Œä»…ä½¿ç”¨æœ€æ–°æ¶æ„åƒåˆ†ä¹‹ä¸€çš„å‚æ•°ã€‚</li>
<li>MeshNetåœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´è¾¾åˆ°è‰¯å¥½å¹³è¡¡ï¼Œé€‚åˆèµ„æºå—é™ç¯å¢ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05531">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-683eedfc3db7df2a3e34a1cd3eb9a350.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2610642602ac0ce7b1b8edc2653dce67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d598704673f81789bcbd703e5addf8ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56162c11a0ad7ae36c08385e7cc0f4ca.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Quantitative-Determination-of-Spatial-Resolution-and-Linearity-of-Position-Sensitive-LG-SiPMs-at-Sub-Millimeter-Scale-via-Ricean-Distribution-Fitting"><a href="#Quantitative-Determination-of-Spatial-Resolution-and-Linearity-of-Position-Sensitive-LG-SiPMs-at-Sub-Millimeter-Scale-via-Ricean-Distribution-Fitting" class="headerlink" title="Quantitative Determination of Spatial Resolution and Linearity of   Position-Sensitive LG-SiPMs at Sub-Millimeter Scale via Ricean Distribution   Fitting"></a>Quantitative Determination of Spatial Resolution and Linearity of   Position-Sensitive LG-SiPMs at Sub-Millimeter Scale via Ricean Distribution   Fitting</h2><p><strong>Authors:Aramis Raiola, Fabio Acerbi, Cyril Alispach, Hossein Arabi, Domenico della Volpe, Alberto Gola, Habib Zaidi</strong></p>
<p>Position-sensitive SiPMs are useful in all light detection applications requiring a small number of readout channels while preserving the information about the incoming lightâ€™s interaction position. Focusing on a 2x2 array of LG-SiPMs covering an area of $\sim 15.5 \times 15.5<del>\rm{mm}$ with just 6 readout channels, we proposed a quantitative method to evaluate image reconstruction performance. The method is based on a statistical approach to assess the deviceâ€™s precision (spatial resolution) and accuracy (linearity) in reconstructing the light spot center of gravity. This evaluation is achieved through a Rice probability distribution function fitting. We obtained an average sensor spatial resolutionâ€™s best value of $81 \pm 3</del>\rm{\mu m}$ (standard deviation), which is achieved by reconstructing each position with the amplitude of the channelsâ€™ output signals. The corresponding accuracy is $231 \pm 4~\rm{\mu m}$. </p>
<blockquote>
<p>ä½ç½®æ•æ„Ÿçš„SiPMåœ¨æ‰€æœ‰éœ€è¦å°‘é‡è¯»å‡ºé€šé“åŒæ—¶ä¿ç•™å…³äºå…¥å°„å…‰äº¤äº’ä½ç½®ä¿¡æ¯çš„å…‰æ£€æµ‹åº”ç”¨ä¸­éƒ½æœ‰ç”¨ã€‚æˆ‘ä»¬é‡ç‚¹å…³æ³¨ä¸€ä¸ªç”±LG-SiPMç»„æˆçš„2x2é˜µåˆ—ï¼Œè¦†ç›–çº¦$ 15.5 \times 15.5æ¯«ç±³$çš„åŒºåŸŸï¼Œä»…ä½¿ç”¨6ä¸ªè¯»å‡ºé€šé“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å®šé‡è¯„ä¼°å›¾åƒé‡å»ºæ€§èƒ½çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åŸºäºç»Ÿè®¡æ–¹æ³•è¯„ä¼°è®¾å¤‡åœ¨é‡å»ºå…‰æ–‘é‡å¿ƒæ—¶çš„ç²¾åº¦ï¼ˆç©ºé—´åˆ†è¾¨ç‡ï¼‰å’Œå‡†ç¡®åº¦ï¼ˆçº¿æ€§åº¦ï¼‰ã€‚è¿™ç§è¯„ä¼°æ˜¯é€šè¿‡æ‹ŸåˆRiceæ¦‚ç‡åˆ†å¸ƒå‡½æ•°æ¥å®ç°çš„ã€‚æˆ‘ä»¬è·å¾—äº†ä¼ æ„Ÿå™¨å¹³å‡ç©ºé—´åˆ†è¾¨ç‡çš„æœ€ä½³å€¼ï¼Œä¸º$ 81 \pm 3å¾®ç±³$ï¼ˆæ ‡å‡†åå·®ï¼‰ï¼Œè¿™æ˜¯é€šè¿‡æ ¹æ®å„é€šé“çš„è¾“å‡ºæ¥é‡å»ºæ¯ä¸ªä½ç½®è€Œå®ç°çš„ã€‚ç›¸åº”çš„å‡†ç¡®åº¦ä¸º$ 231 \pm 4å¾®ç±³$ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05450v1">PDF</a> 17 pages, 9 figures, 2 tables</p>
<p><strong>Summary</strong><br>     ä½ç½®æ•æ„Ÿçš„SiPMåœ¨éœ€è¦å°‘é‡è¯»å‡ºé€šé“çš„åŒæ—¶ä¿ç•™å…³äºå…¥å°„å…‰äº¤äº’ä½ç½®çš„ä¿¡æ¯ï¼Œé€‚ç”¨äºæ‰€æœ‰å…‰æ£€æµ‹åº”ç”¨ã€‚é’ˆå¯¹ä¸€ä¸ªè¦†ç›–çº¦$ 15.5 \times 15.5 \rm{mm}$åŒºåŸŸï¼Œä»…æœ‰6ä¸ªè¯»å‡ºé€šé“çš„LG-SiPMçš„2x2é˜µåˆ—ï¼Œæå‡ºäº†ä¸€ç§å®šé‡è¯„ä¼°å›¾åƒé‡å»ºæ€§èƒ½çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•åŸºäºç»Ÿè®¡æ–¹æ³•è¯„ä¼°è®¾å¤‡çš„ç²¾åº¦ï¼ˆç©ºé—´åˆ†è¾¨ç‡ï¼‰å’Œå‡†ç¡®æ€§ï¼ˆçº¿æ€§åº¦ï¼‰ï¼Œé€šè¿‡æ‹ŸåˆRiceæ¦‚ç‡åˆ†å¸ƒå‡½æ•°å®ç°å…‰æ–‘é‡å¿ƒé‡å»ºçš„è¯„ä¼°ã€‚æœ€ä½³ä¼ æ„Ÿå™¨ç©ºé—´åˆ†è¾¨ç‡ä¸º$ 81 \pm 3 \rm{\mu m}$ï¼ˆæ ‡å‡†åå·®ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½ç½®æ•æ„Ÿçš„SiPMé€‚ç”¨äºå…‰æ£€æµ‹åº”ç”¨ï¼Œå°¤å…¶æ˜¯éœ€è¦ä¿ç•™å…‰äº¤äº’ä½ç½®ä¿¡æ¯çš„æƒ…å†µã€‚</li>
<li>å¯¹äºLG-SiPMçš„2x2é˜µåˆ—ï¼Œå³ä½¿ä½¿ç”¨æœ‰é™çš„è¯»å‡ºé€šé“ï¼Œä¹Ÿèƒ½å®ç°å›¾åƒé‡å»ºæ€§èƒ½çš„è¯„ä»·ã€‚</li>
<li>é‡‡ç”¨ç»Ÿè®¡æ–¹æ³•è¯„ä¼°è®¾å¤‡çš„ç©ºé—´åˆ†è¾¨ç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡æ‹ŸåˆRiceæ¦‚ç‡åˆ†å¸ƒå‡½æ•°æ¥è¯„ä¼°å…‰æ–‘é‡å¿ƒçš„é‡å»ºæ•ˆæœã€‚</li>
<li>æœ€ä½³ä¼ æ„Ÿå™¨ç©ºé—´åˆ†è¾¨ç‡è¾¾åˆ°$ 81 \pm 3 \rm{\mu m}$ã€‚</li>
<li>è®¾å¤‡åœ¨å…‰æ–‘é‡å»ºæ–¹é¢çš„å‡†ç¡®æ€§ä¸º$ 231 \pm 4 \rm{\mu m}$ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05450">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-79dfa9d50a95c12e11a01dcfbf92a3f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a30865787c81f0186c8d7a54470a71d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f1a518500cc4d8c6ea446b81c3c7ca7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="GEMA-Score-Granular-Explainable-Multi-Agent-Score-for-Radiology-Report-Evaluation"><a href="#GEMA-Score-Granular-Explainable-Multi-Agent-Score-for-Radiology-Report-Evaluation" class="headerlink" title="GEMA-Score: Granular Explainable Multi-Agent Score for Radiology Report   Evaluation"></a>GEMA-Score: Granular Explainable Multi-Agent Score for Radiology Report   Evaluation</h2><p><strong>Authors:Zhenxuan Zhang, Kinhei Lee, Weihang Deng, Huichi Zhou, Zihao Jin, Jiahao Huang, Zhifan Gao, Dominic C Marshall, Yingying Fang, Guang Yang</strong></p>
<p>Automatic medical report generation supports clinical diagnosis, reduces the workload of radiologists, and holds the promise of improving diagnosis consistency. However, existing evaluation metrics primarily assess the accuracy of key medical information coverage in generated reports compared to human-written reports, while overlooking crucial details such as the location and certainty of reported abnormalities. These limitations hinder the comprehensive assessment of the reliability of generated reports and pose risks in their selection for clinical use. Therefore, we propose a Granular Explainable Multi-Agent Score (GEMA-Score) in this paper, which conducts both objective quantification and subjective evaluation through a large language model-based multi-agent workflow. Our GEMA-Score parses structured reports and employs NER-F1 calculations through interactive exchanges of information among agents to assess disease diagnosis, location, severity, and uncertainty. Additionally, an LLM-based scoring agent evaluates completeness, readability, and clinical terminology while providing explanatory feedback. Extensive experiments validate that GEMA-Score achieves the highest correlation with human expert evaluations on a public dataset, demonstrating its effectiveness in clinical scoring (Kendall coefficient &#x3D; 0.70 for Rexval dataset and Kendall coefficient &#x3D; 0.54 for RadEvalX dataset). The anonymous project demo is available at: <a target="_blank" rel="noopener" href="https://github.com/Zhenxuan-Zhang/GEMA_score">https://github.com/Zhenxuan-Zhang/GEMA_score</a>. </p>
<blockquote>
<p>è‡ªåŠ¨ç”Ÿæˆçš„åŒ»å­¦æŠ¥å‘Šæ”¯æŒä¸´åºŠè¯Šæ–­ï¼Œå‡è½»äº†æ”¾å°„ç§‘åŒ»å¸ˆçš„å·¥ä½œé‡ï¼Œå¹¶æœ‰æœ›æé«˜è¯Šæ–­çš„ä¸€è‡´æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡ä¸»è¦è¯„ä¼°è‡ªåŠ¨ç”ŸæˆæŠ¥å‘Šä¸äººå·¥ç¼–å†™æŠ¥å‘Šç›¸æ¯”å…³é”®åŒ»ç–—ä¿¡æ¯è¦†ç›–çš„å‡†ç¡®æ€§ï¼Œå´å¿½è§†äº†æŠ¥å‘Šå¼‚å¸¸çš„ä½ç½®å’Œç¡®å®šæ€§ç­‰é‡è¦ç»†èŠ‚ã€‚è¿™äº›å±€é™æ€§é˜»ç¢äº†ç”ŸæˆæŠ¥å‘Šçš„å¯é æ€§çš„å…¨é¢è¯„ä¼°ï¼Œå¹¶ç»™å…¶ä¸´åºŠä½¿ç”¨é€‰æ‹©å¸¦æ¥äº†é£é™©ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»†ç²’åº¦å¯è§£é‡Šå¤šæ™ºèƒ½ä½“è¯„åˆ†ï¼ˆGEMA-Scoreï¼‰ï¼Œå®ƒé‡‡ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šæ™ºèƒ½ä½“å·¥ä½œæµç¨‹è¿›è¡Œå®¢è§‚é‡åŒ–å’Œä¸»è§‚è¯„ä»·ã€‚æˆ‘ä»¬çš„GEMA-Scoreè§£æç»“æ„åŒ–æŠ¥å‘Šï¼Œé€šè¿‡æ™ºèƒ½ä½“ä¹‹é—´çš„ä¿¡æ¯äº¤äº’äº¤æµï¼Œåˆ©ç”¨NER-F1è®¡ç®—æ¥è¯„ä¼°ç–¾ç—…è¯Šæ–­ã€ä½ç½®ã€ä¸¥é‡ç¨‹åº¦å’Œä¸ç¡®å®šæ€§ã€‚æ­¤å¤–ï¼ŒåŸºäºLLMçš„è¯„åˆ†æ™ºèƒ½ä½“è¯„ä¼°å®Œæ•´æ€§ã€å¯è¯»æ€§å’Œä¸´åºŠæœ¯è¯­ï¼ŒåŒæ—¶æä¾›è§£é‡Šæ€§åé¦ˆã€‚å¤§é‡å®éªŒéªŒè¯ï¼ŒGEMA-Scoreåœ¨å…¬å…±æ•°æ®é›†ä¸Šä¸äººç±»ä¸“å®¶è¯„ä»·çš„ç›¸å…³æ€§æœ€é«˜ï¼Œè¯æ˜äº†å…¶åœ¨ä¸´åºŠè¯„åˆ†ä¸­çš„æœ‰æ•ˆæ€§ï¼ˆRexvalæ•°æ®é›†çš„Kendallç³»æ•°ä¸º0.70ï¼ŒRadEvalXæ•°æ®é›†çš„Kendallç³»æ•°ä¸º0.54ï¼‰ã€‚åŒ¿åé¡¹ç›®æ¼”ç¤ºåœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/Zhenxuan-Zhang/GEMA_score%E3%80%82">https://github.com/Zhenxuan-Zhang/GEMA_scoreã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05347v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦æŠ¥å‘Šè‡ªåŠ¨ç”ŸæˆæŠ€æœ¯æœ‰åŠ©äºä¸´åºŠè¯Šæ–­ï¼Œå‡è½»åŒ»ç”Ÿå·¥ä½œé‡ï¼Œæé«˜è¯Šæ–­ä¸€è‡´æ€§ã€‚ä½†ç°æœ‰è¯„ä¼°æŒ‡æ ‡ä¸»è¦å…³æ³¨ç”Ÿæˆçš„æŠ¥å‘Šæ˜¯å¦è¦†ç›–å…³é”®åŒ»å­¦ä¿¡æ¯ï¼Œè€Œå¿½è§†äº†å¼‚å¸¸éƒ¨ä½çš„å®šä½å’Œç¡®å®šæ€§ç­‰é‡è¦ç»†èŠ‚ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºGranular Explainable Multi-Agent Scoreï¼ˆGEMA-Scoreï¼‰çš„è¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“å·¥ä½œæµç¨‹è¿›è¡Œå®¢è§‚é‡åŒ–å’Œä¸»è§‚è¯„ä»·ã€‚è¯¥æ–¹æ³•èƒ½è¯„ä¼°ç–¾ç—…è¯Šæ–­ã€éƒ¨ä½ã€ä¸¥é‡ç¨‹åº¦å’Œä¸ç¡®å®šæ€§ç­‰ï¼ŒåŒæ—¶è¯„ä»·æŠ¥å‘Šçš„å®Œæ•´æ€§ã€å¯è¯»æ€§å’Œä¸´åºŠæœ¯è¯­ä½¿ç”¨ã€‚å®éªŒè¯æ˜ï¼ŒGEMA-Scoreåœ¨å…¬å…±æ•°æ®é›†ä¸Šä¸äººä¸“å®¶è¯„ä»·çš„å…³è”åº¦æœ€é«˜ï¼Œå¯æœ‰æ•ˆç”¨äºä¸´åºŠè¯„åˆ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æŠ¥å‘Šè‡ªåŠ¨ç”ŸæˆæŠ€æœ¯æœ‰åŠ©äºæå‡ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—æ•ˆç‡ã€‚</li>
<li>å½“å‰è¯„ä¼°æ–¹æ³•ä¸»è¦å…³æ³¨åŒ»å­¦ä¿¡æ¯çš„è¦†ç›–ï¼Œä½†å¿½è§†äº†å¼‚å¸¸éƒ¨ä½å’Œç¡®å®šæ€§ç­‰å…³é”®ç»†èŠ‚ã€‚</li>
<li>æå‡ºçš„GEMA-Scoreæ–¹æ³•ç»“åˆäº†å®¢è§‚é‡åŒ–å’Œä¸»è§‚è¯„ä»·ã€‚</li>
<li>GEMA-Scoreèƒ½å¤Ÿè¯„ä¼°ç–¾ç—…è¯Šæ–­ã€éƒ¨ä½ã€ä¸¥é‡ç¨‹åº¦å’Œä¸ç¡®å®šæ€§ã€‚</li>
<li>é™¤äº†è¯„ä¼°åŠŸèƒ½ï¼ŒGEMA-Scoreè¿˜èƒ½è¯„ä»·æŠ¥å‘Šçš„å®Œæ•´æ€§ã€å¯è¯»æ€§å’Œä¸´åºŠæœ¯è¯­çš„ä½¿ç”¨ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGEMA-Scoreä¸äººçš„ä¸“å®¶è¯„ä»·çš„å…³è”åº¦æœ€é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05347">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-546ab013daa58445c66f8ea50eb94509.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eb855f88ea5c7583bb12916b6aa428e7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6cc2af9dbb365ca5ddf759eab54d0a42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d6b9e2cc4e6a13c1029f2be71a39484.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-362a06da224c7baa33bb1c9dcbdaae1a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e602b91e0a460fe349815c5a350805d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Pretext-Task-Adversarial-Learning-for-Unpaired-Low-field-to-Ultra-High-field-MRI-Synthesis"><a href="#Pretext-Task-Adversarial-Learning-for-Unpaired-Low-field-to-Ultra-High-field-MRI-Synthesis" class="headerlink" title="Pretext Task Adversarial Learning for Unpaired Low-field to Ultra   High-field MRI Synthesis"></a>Pretext Task Adversarial Learning for Unpaired Low-field to Ultra   High-field MRI Synthesis</h2><p><strong>Authors:Zhenxuan Zhang, Peiyuan Jing, Coraline Beitone, Jiahao Huang, Zhifan Gao, Guang Yang, Pete Lally</strong></p>
<p>Given the scarcity and cost of high-field MRI, the synthesis of high-field MRI from low-field MRI holds significant potential when there is limited data for training downstream tasks (e.g. segmentation). Low-field MRI often suffers from a reduced signal-to-noise ratio (SNR) and spatial resolution compared to high-field MRI. However, synthesizing high-field MRI data presents challenges. These involve aligning image features across domains while preserving anatomical accuracy and enhancing fine details. To address these challenges, we propose a Pretext Task Adversarial (PTA) learning framework for high-field MRI synthesis from low-field MRI data. The framework comprises three processes: (1) The slice-wise gap perception (SGP) network aligns the slice inconsistencies of low-field and high-field datasets based on contrastive learning. (2) The local structure correction (LSC) network extracts local structures by restoring the locally rotated and masked images. (3) The pretext task-guided adversarial training process introduces additional supervision and incorporates a discriminator to improve image realism. Extensive experiments on low-field to ultra high-field task demonstrate the effectiveness of our method, achieving state-of-the-art performance (16.892 in FID, 1.933 in IS, and 0.324 in MS-SSIM). This enables the generation of high-quality high-field-like MRI data from low-field MRI data to augment training datasets for downstream tasks. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/Zhenxuan-Zhang/PTA4Unpaired_HF_MRI_SYN">https://github.com/Zhenxuan-Zhang/PTA4Unpaired_HF_MRI_SYN</a>. </p>
<blockquote>
<p>é‰´äºé«˜åœºMRIçš„ç¨€ç¼ºæ€§å’Œæˆæœ¬ï¼Œå½“è®­ç»ƒä¸‹æ¸¸ä»»åŠ¡ï¼ˆä¾‹å¦‚åˆ†å‰²ï¼‰çš„æ•°æ®æœ‰é™æ—¶ï¼Œä»ä½åœºMRIåˆæˆé«˜åœºMRIå…·æœ‰é‡è¦çš„æ½œåŠ›ã€‚ä¸é«˜åœºMRIç›¸æ¯”ï¼Œä½åœºMRIé€šå¸¸å…·æœ‰è¾ƒä½çš„ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰å’Œç©ºé—´åˆ†è¾¨ç‡ã€‚ç„¶è€Œï¼Œåˆæˆé«˜åœºMRIæ•°æ®å­˜åœ¨æŒ‘æˆ˜ï¼Œè¿™æ¶‰åŠåˆ°è·¨åŸŸå¯¹é½å›¾åƒç‰¹å¾ï¼ŒåŒæ—¶ä¿ç•™è§£å‰–ç²¾åº¦å¹¶å¢å¼ºç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä»ä½åœºMRIæ•°æ®åˆæˆé«˜åœºMRIçš„Pretext Task Adversarialï¼ˆPTAï¼‰å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªè¿‡ç¨‹ï¼šï¼ˆ1ï¼‰åˆ‡ç‰‡çº§é—´éš™æ„ŸçŸ¥ï¼ˆSGPï¼‰ç½‘ç»œåŸºäºå¯¹æ¯”å­¦ä¹ å¯¹é½ä½åœºå’Œé«˜åœºæ•°æ®é›†çš„ä¸ä¸€è‡´æ€§åˆ‡ç‰‡ã€‚ï¼ˆ2ï¼‰å±€éƒ¨ç»“æ„æ ¡æ­£ï¼ˆLSCï¼‰ç½‘ç»œé€šè¿‡æ¢å¤å±€éƒ¨æ—‹è½¬å’Œé®æŒ¡çš„å›¾åƒæ¥æå–å±€éƒ¨ç»“æ„ã€‚ï¼ˆ3ï¼‰é¢„è®­ç»ƒä»»åŠ¡å¼•å¯¼å¯¹æŠ—è®­ç»ƒè¿‡ç¨‹å¼•å…¥äº†é¢å¤–çš„ç›‘ç£å¹¶åˆå¹¶äº†ä¸€ä¸ªé‰´åˆ«å™¨ï¼Œä»¥æé«˜å›¾åƒçš„çœŸå®æ€§ã€‚åœ¨ä½åœºåˆ°è¶…é«˜åœºçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•éå¸¸æœ‰æ•ˆï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆFIDä¸º16.892ï¼ŒISä¸º1.933ï¼ŒMS-SSIMä¸º0.324ï¼‰ã€‚è¿™èƒ½å¤Ÿä»ä½åœºMRIæ•°æ®ç”Ÿæˆé«˜è´¨é‡çš„é«˜åœºMRIæ•°æ®ï¼Œä»¥æ‰©å……ä¸‹æ¸¸ä»»åŠ¡çš„è®­ç»ƒæ•°æ®é›†ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/Zhenxuan-Zhang/PTA4Unpaired_HF_MRI_SYN">https://github.com/Zhenxuan-Zhang/PTA4Unpaired_HF_MRI_SYN</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05339v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨ä½åœºMRIæ•°æ®åˆæˆé«˜åœºMRIæ•°æ®çš„æ–¹æ³•ï¼Œä»¥è§£å†³é«˜åœºMRIç¨€ç¼ºå’Œæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚é’ˆå¯¹ä½åœºMRIçš„ä¿¡å·å™ªå£°æ¯”å’Œç©ºé—´åˆ†è¾¨ç‡è¾ƒä½çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºå¯¹æŠ—å­¦ä¹ ï¼ˆPTAï¼‰çš„é«˜åœºMRIåˆæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬åˆ‡ç‰‡ä¸ä¸€è‡´æ€§å¯¹é½ã€å±€éƒ¨ç»“æ„æ ¡æ­£å’Œå¯¹æŠ—è®­ç»ƒä¸‰ä¸ªè¿‡ç¨‹ï¼Œå®ç°äº†ä»ä½åœºåˆ°é«˜åœºçš„MRIæ•°æ®åˆæˆï¼Œå¹¶è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ã€‚æ­¤æŠ€æœ¯æœ‰åŠ©äºæ‰©å……è®­ç»ƒæ•°æ®é›†ï¼Œä¸ºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚åˆ†å‰²ï¼‰æä¾›é«˜è´¨é‡çš„é«˜åœºMRIæ•°æ®ã€‚ç›¸å…³ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆ©ç”¨ä½åœºMRIæ•°æ®åˆæˆé«˜åœºMRIæ•°æ®ï¼Œä»¥è§£å†³é«˜åœºMRIèµ„æºä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>ä»‹ç»äº†PTAå­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬åˆ‡ç‰‡ä¸ä¸€è‡´æ€§å¯¹é½ã€å±€éƒ¨ç»“æ„æ ¡æ­£å’Œå¯¹æŠ—è®­ç»ƒä¸‰ä¸ªå…³é”®è¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡å¹¿æ³›å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼Œå¹¶æä¾›äº†å…·ä½“è¯„ä»·æŒ‡æ ‡ï¼ˆFID&#x3D;16.892, IS&#x3D;1.933, MS-SSIM&#x3D;0.324ï¼‰ã€‚</li>
<li>è¯¥æŠ€æœ¯æœ‰åŠ©äºæé«˜è®­ç»ƒæ•°æ®é›†çš„è´¨é‡ï¼Œä¸ºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚åŒ»å­¦å›¾åƒåˆ†å‰²ï¼‰æä¾›æœ‰ç›Šæ”¯æŒã€‚</li>
<li>å…¬å¼€äº†ç›¸å…³ä»£ç ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
<li>æ­¤æŠ€æœ¯å¯¹äºæé«˜åŒ»å­¦å›¾åƒåˆ†æçš„å‡†ç¡®æ€§å’Œæ•ˆç‡å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05339">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c9e5b0700cd849339d7b4ef5ef834dde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1221d34acdb32d40c66ede724ba49859.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-770862aabb00dc92801af5834c0b6878.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30710b972163a1947cc16185f47f777e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e78227335b235fd3f27c15f530417837.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-63ce9e89de0f1c09a91dcf3271c63d16.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Radio-pulse-search-from-Aql-X-1"><a href="#Radio-pulse-search-from-Aql-X-1" class="headerlink" title="Radio pulse search from Aql X-1"></a>Radio pulse search from Aql X-1</h2><p><strong>Authors:Long Peng, Zhaosheng Li, Yuanyue Pan, Shanshan Weng, Wengming Yan, Na Wang, Bojun Wang, Shuangqiang Wang</strong></p>
<p>We present 12 observations of the accreting millisecond X-ray pulsar Aql X-1, taken from August 2022 to October 2023 using the Five-hundred-meter Aperture Spherical Radio Telescope at 1250 MHz. These observations covered both the quiescence and X-ray outburst states, as determined by analyzing the X-ray data from the Neutron Star Interior Composition Explorer and the Monitor of All-sky X-ray Image. Periodicity and single-pulse searches were conducted for each observation, but no pulsed signals were detected. The obtained upper limit flux densities are in the range of 2.86-5.73 uJy, which provide the lowest limits to date. We discuss several mechanisms that may prevent detection, suggesting that Aql X-1 may be in the radio-ejection state during quiescence, where the radio pulsed emissions are absorbed by the matter surrounding the system. </p>
<blockquote>
<p>æˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨äº”ç™¾å¹´å£å¾„çƒé¢å°„ç”µæœ›è¿œé•œåœ¨1250 MHzé¢‘ç‡ä¸‹ä»2022å¹´8æœˆåˆ°2023å¹´10æœˆå¯¹å¢äº®å‹æ¯«ç§’Xå°„çº¿è„‰å†²æ˜ŸAql X-1è¿›è¡Œçš„12æ¬¡è§‚æµ‹ç»“æœã€‚è¿™äº›è§‚æµ‹æ¶µç›–äº†é™æ¯æ€å’ŒXå°„çº¿çˆ†å‘æ€ï¼Œè¿™æ˜¯é€šè¿‡åˆ†ææ¥è‡ªä¸­å­æ˜Ÿå†…éƒ¨ç»“æ„æ¢æµ‹å™¨å’Œå…¨å¤©ç©ºXå°„çº¿å›¾åƒç›‘æµ‹å™¨çš„Xå°„çº¿æ•°æ®æ¥ç¡®å®šçš„ã€‚æˆ‘ä»¬å¯¹æ¯æ¬¡è§‚æµ‹è¿›è¡Œäº†å‘¨æœŸæ€§å’Œå•è„‰å†²æœç´¢ï¼Œä½†æ²¡æœ‰æ£€æµ‹åˆ°è„‰å†²ä¿¡å·ã€‚æ‰€è·å¾—çš„æµé‡å¯†åº¦ä¸Šé™èŒƒå›´ä¸º2.86-5.73å¾®å‰ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢çš„æœ€ä½å€¼ã€‚æˆ‘ä»¬è®¨è®ºäº†å‡ ç§å¯èƒ½å¯¼è‡´æ— æ³•æ£€æµ‹åˆ°çš„æœºåˆ¶ï¼Œå¹¶æå‡ºAql X-1å¯èƒ½åœ¨é™æ¯æ€æ—¶å¤„äºå°„ç”µå–·å°„çŠ¶æ€ï¼Œæ­¤æ—¶å°„ç”µè„‰å†²è¾å°„è¢«ç³»ç»Ÿå‘¨å›´çš„ç‰©è´¨å¸æ”¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05237v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ä½¿ç”¨å°„ç”µæœ›è¿œé•œè§‚æµ‹äº†å°„ç”µæ¯«ç§’è„‰å†²æ˜ŸAql X-1çš„åäºŒæ¬¡è§‚æµ‹æ•°æ®ï¼Œè¦†ç›–å…¶å¹³é™æœŸä¸Xå°„çº¿çˆ†å‘çŠ¶æ€ã€‚å‘¨æœŸæ€§åŠå•ä¸€è„‰å†²æœç´¢å‡æœªå‘ç°è„‰å†²ä¿¡å·ï¼Œè·å¾—çš„ä¸Šé™æµé‡å¯†åº¦èŒƒå›´åœ¨2.86-5.73 uJyä¹‹é—´ï¼Œä¸ºç›®å‰æœ€ä½å€¼ã€‚è®¨è®ºäº†å¯èƒ½é˜»ç¢æ¢æµ‹çš„æœºåˆ¶ï¼Œæå‡ºAql X-1åœ¨å¹³é™æœŸå¯èƒ½å¤„äºå°„ç”µå–·å°„çŠ¶æ€ï¼Œå°„ç”µè„‰å†²æ’æ”¾è¢«ç³»ç»Ÿå‘¨å›´ç‰©è´¨å¸æ”¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨å°„ç”µæœ›è¿œé•œè§‚æµ‹äº†Aql X-1çš„åäºŒæ¬¡æ•°æ®ã€‚</li>
<li>è§‚æµ‹è¦†ç›–äº†Aql X-1çš„å¹³é™æœŸå’ŒXå°„çº¿çˆ†å‘çŠ¶æ€ã€‚</li>
<li>æœç´¢è¿‡ç¨‹ä¸­æœªå‘ç°å‘¨æœŸæ€§æˆ–å•ä¸€è„‰å†²ä¿¡å·ã€‚</li>
<li>è·å¾—çš„ä¸Šé™æµé‡å¯†åº¦æ˜¯è¿„ä»Šä¸ºæ­¢çš„æœ€ä½å€¼ï¼ŒèŒƒå›´åœ¨2.86-5.73 uJyä¹‹é—´ã€‚</li>
<li>è®¨è®ºäº†å¯èƒ½é˜»ç¢å°„ç”µä¿¡å·æ¢æµ‹çš„æœºåˆ¶ã€‚</li>
<li>æå‡ºAql X-1åœ¨å¹³é™æœŸå¯èƒ½å¤„äºå°„ç”µå–·å°„çŠ¶æ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05237">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-50bd0e63b1ab1c2264291571013ec068.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f1fef92b6ed9bfc8f1463db473d3b08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df5c010441fc781ca17091cf06b1e802.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-978f6c217429f2bc1228d4c0a349c147.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d43e72e02dda205e74486fdf4bc66d5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Gaussian-Random-Fields-as-an-Abstract-Representation-of-Patient-Metadata-for-Multimodal-Medical-Image-Segmentation"><a href="#Gaussian-Random-Fields-as-an-Abstract-Representation-of-Patient-Metadata-for-Multimodal-Medical-Image-Segmentation" class="headerlink" title="Gaussian Random Fields as an Abstract Representation of Patient Metadata   for Multimodal Medical Image Segmentation"></a>Gaussian Random Fields as an Abstract Representation of Patient Metadata   for Multimodal Medical Image Segmentation</h2><p><strong>Authors:Bill Cassidy, Christian McBride, Connah Kendrick, Neil D. Reeves, Joseph M. Pappachan, Shaghayegh Raad, Moi Hoon Yap</strong></p>
<p>The growing rate of chronic wound occurrence, especially in patients with diabetes, has become a concerning trend in recent years. Chronic wounds are difficult and costly to treat, and have become a serious burden on health care systems worldwide. Chronic wounds can have devastating consequences for the patient, with infection often leading to reduced quality of life and increased mortality risk. Innovative deep learning methods for the detection and monitoring of such wounds have the potential to reduce the impact to both patient and clinician. We present a novel multimodal segmentation method which allows for the introduction of patient metadata into the training workflow whereby the patient data are expressed as Gaussian random fields. Our results indicate that the proposed method improved performance when utilising multiple models, each trained on different metadata categories. Using the Diabetic Foot Ulcer Challenge 2022 test set, when compared to the baseline results (intersection over union &#x3D; 0.4670, Dice similarity coefficient &#x3D; 0.5908) we demonstrate improvements of +0.0220 and +0.0229 for intersection over union and Dice similarity coefficient respectively. This paper presents the first study to focus on integrating patient data into a chronic wound segmentation workflow. Our results show significant performance gains when training individual models using specific metadata categories, followed by average merging of prediction masks using distance transforms. All source code for this study is available at: <a target="_blank" rel="noopener" href="https://github.com/mmu-dermatology-research/multimodal-grf">https://github.com/mmu-dermatology-research/multimodal-grf</a> </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ…¢æ€§ä¼¤å£çš„å‘ç”Ÿç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç³–å°¿ç—…æ‚£è€…ä¸­ï¼Œå‘ˆç°å‡ºä»¤äººæ‹…å¿§çš„å¢é•¿è¶‹åŠ¿ã€‚æ…¢æ€§ä¼¤å£çš„æ²»ç–—æ—¢å›°éš¾åˆæ˜‚è´µï¼Œå·²æˆä¸ºå…¨çƒåŒ»ç–—ç³»ç»Ÿé¢ä¸´çš„ä¸€é¡¹ä¸¥é‡è´Ÿæ‹…ã€‚å¯¹äºæ‚£è€…è€Œè¨€ï¼Œæ…¢æ€§ä¼¤å£å¯èƒ½äº§ç”Ÿç¾éš¾æ€§çš„åæœï¼Œæ„ŸæŸ“å¸¸å¸¸å¯¼è‡´ç”Ÿæ´»è´¨é‡ä¸‹é™å’Œæ­»äº¡é£é™©å¢åŠ ã€‚é’ˆå¯¹æ­¤ç±»ä¼¤å£çš„æ£€æµ‹å’Œç›‘æµ‹ï¼Œåˆ›æ–°çš„æ·±åº¦å­¦ä¹ æ–¹æ³•å…·æœ‰å‡è½»æ‚£è€…å’Œä¸´åºŠåŒ»ç”Ÿè´Ÿæ‹…çš„æ½œåŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€åˆ†å‰²æ–¹æ³•ï¼Œå…è®¸å°†æ‚£è€…å…ƒæ•°æ®å¼•å…¥è®­ç»ƒæµç¨‹ä¸­ï¼Œæ‚£è€…æ•°æ®ä»¥é«˜æ–¯éšæœºåœºçš„å½¢å¼è¡¨è¾¾ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨åˆ©ç”¨å¤šä¸ªæ¨¡å‹æ—¶ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æé«˜äº†æ€§èƒ½ï¼Œæ¯ä¸ªæ¨¡å‹éƒ½åœ¨ä¸åŒçš„å…ƒæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚åœ¨Diabetic Foot Ulcer Challenge 2022æµ‹è¯•é›†ä¸Šï¼Œä¸åŸºçº¿ç»“æœç›¸æ¯”ï¼ˆäº¤å¹¶æ¯”&#x3D;0.4670ï¼Œç‹„æ°ç›¸ä¼¼ç³»æ•°&#x3D;0.5908ï¼‰ï¼Œæˆ‘ä»¬çš„äº¤å¹¶æ¯”å’Œç‹„æ°ç›¸ä¼¼ç³»æ•°åˆ†åˆ«æé«˜äº†+0.0220å’Œ+0.0229ã€‚æœ¬æ–‡æ˜¯é¦–æ¬¡ä¸“æ³¨äºå°†æ‚£è€…æ•°æ®é›†æˆåˆ°æ…¢æ€§ä¼¤å£åˆ†å‰²æµç¨‹ä¸­çš„ç ”ç©¶ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå½“ä½¿ç”¨ç‰¹å®šçš„å…ƒæ•°æ®é›†å¯¹å•ä¸ªæ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œç„¶åé‡‡ç”¨è·ç¦»å˜æ¢å¯¹é¢„æµ‹æ©è†œè¿›è¡Œå¹³å‡åˆå¹¶æ—¶ï¼Œå¯ä»¥è·å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶çš„æ‰€æœ‰æºä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/mmu-dermatology-research/multimodal-grf">https://github.com/mmu-dermatology-research/multimodal-grf</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05214v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨æ…¢æ€§ä¼¤å£ï¼Œç‰¹åˆ«æ˜¯ç³–å°¿ç—…æ‚£è€…æ…¢æ€§ä¼¤å£çš„æ—¥ç›Šå¢é•¿è¶‹åŠ¿ï¼Œæå‡ºä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€åˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†æ‚£è€…å…ƒæ•°æ®å¼•å…¥è®­ç»ƒæµç¨‹ï¼Œå¹¶ä»¥é«˜æ–¯éšæœºåœºçš„å½¢å¼è¡¨è¾¾ã€‚åœ¨ç³–å°¿ç—…è¶³æºƒç–¡æŒ‘æˆ˜2022æµ‹è¯•é›†ä¸Šçš„ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºåŸºçº¿ç»“æœæœ‰æ‰€æé«˜ã€‚æœ¬æ–‡é›†æˆäº†æ‚£è€…æ•°æ®åˆ°æ…¢æ€§ä¼¤å£åˆ†å‰²æµç¨‹ä¸­ï¼Œå¹¶é€šè¿‡ç‰¹å®šå…ƒæ•°æ®é›†è®­ç»ƒæ¨¡å‹å†å¹³å‡åˆå¹¶é¢„æµ‹æ©è†œçš„æ–¹å¼ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ…¢æ€§ä¼¤å£ï¼Œå°¤å…¶æ˜¯ç³–å°¿ç—…æ‚£è€…çš„æ…¢æ€§ä¼¤å£ï¼Œå…¶å‘ç”Ÿç‡å‘ˆå¢é•¿è¶‹åŠ¿ï¼Œæˆä¸ºå…¨çƒåŒ»ç–—ç³»ç»Ÿçš„ä¸¥é‡è´Ÿæ‹…ã€‚</li>
<li>æ…¢æ€§ä¼¤å£çš„æ²»ç–—å›°éš¾ä¸”è´¹ç”¨é«˜æ˜‚ï¼Œå¯¹æ‚£è€…çš„ç”Ÿæ´»è´¨é‡å’Œç”Ÿå‘½å¥åº·äº§ç”Ÿä¸¥é‡å½±å“ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨æ£€æµ‹ä¸ç›‘æ§æ…¢æ€§ä¼¤å£ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>æå‡ºä¸€ç§æ–°é¢–çš„å¤šæ¨¡æ€åˆ†å‰²æ–¹æ³•ï¼Œå°†æ‚£è€…å…ƒæ•°æ®å¼•å…¥è®­ç»ƒæµç¨‹ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡é«˜æ–¯éšæœºåœºè¡¨è¾¾æ‚£è€…æ•°æ®ï¼Œå¹¶åœ¨å¤šä¸ªæ¨¡å‹ä¸Šå–å¾—è‰¯å¥½æ€§èƒ½ã€‚</li>
<li>åœ¨ç³–å°¿ç—…è¶³æºƒç–¡æŒ‘æˆ˜2022æµ‹è¯•é›†ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºåŸºçº¿ç»“æœæœ‰æ‰€æå‡ï¼Œæ˜¾ç¤ºå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05214">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e40e0d16c02adfb7ae1078c2c04e4b92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7aba887ddab754bacff4ecae3bede785.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-744a3b3545fe4afe7f2c2f3b48b4aafd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17aa016994b786ad75b125799b58372d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb130ca4f1bb9fbadebd266e0fe10bae.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Partially-Supervised-Unpaired-Multi-Modal-Learning-for-Label-Efficient-Medical-Image-Segmentation"><a href="#Partially-Supervised-Unpaired-Multi-Modal-Learning-for-Label-Efficient-Medical-Image-Segmentation" class="headerlink" title="Partially Supervised Unpaired Multi-Modal Learning for Label-Efficient   Medical Image Segmentation"></a>Partially Supervised Unpaired Multi-Modal Learning for Label-Efficient   Medical Image Segmentation</h2><p><strong>Authors:Lei Zhu, Yanyu Xu, Huazhu Fu, Xinxing Xu, Rick Siow Mong Goh, Yong Liu</strong></p>
<p>Unpaired Multi-Modal Learning (UMML) which leverages unpaired multi-modal data to boost model performance on each individual modality has attracted a lot of research interests in medical image analysis. However, existing UMML methods require multi-modal datasets to be fully labeled, which incurs tremendous annotation cost. In this paper, we investigate the use of partially labeled data for label-efficient unpaired multi-modal learning, which can reduce the annotation cost by up to one half. We term the new learning paradigm as Partially Supervised Unpaired Multi-Modal Learning (PSUMML) and propose a novel Decomposed partial class adaptation with snapshot Ensembled Self-Training (DEST) framework for it. Specifically, our framework consists of a compact segmentation network with modality specific normalization layers for learning with partially labeled unpaired multi-modal data. The key challenge in PSUMML lies in the complex partial class distribution discrepancy due to partial class annotation, which hinders effective knowledge transfer across modalities. We theoretically analyze this phenomenon with a decomposition theorem and propose a decomposed partial class adaptation technique to precisely align the partially labeled classes across modalities to reduce the distribution discrepancy. We further propose a snapshot ensembled self-training technique to leverage the valuable snapshot models during training to assign pseudo-labels to partially labeled pixels for self-training to boost model performance. We perform extensive experiments under different scenarios of PSUMML for two medical image segmentation tasks, namely cardiac substructure segmentation and abdominal multi-organ segmentation. Our framework outperforms existing methods significantly. </p>
<blockquote>
<p>æ— é…å¯¹å¤šæ¨¡æ€å­¦ä¹ ï¼ˆUMMLï¼‰åˆ©ç”¨æ— é…å¯¹çš„å¤šæ¨¡æ€æ•°æ®æ¥æå‡æ¯ä¸ªç‹¬ç«‹æ¨¡æ€çš„æ¨¡å‹æ€§èƒ½ï¼Œåœ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå¼•èµ·äº†å¤§é‡çš„ç ”ç©¶å…´è¶£ã€‚ç„¶è€Œï¼Œç°æœ‰çš„UMMLæ–¹æ³•è¦æ±‚å¤šæ¨¡æ€æ•°æ®é›†å®Œå…¨æ ‡æ³¨ï¼Œè¿™äº§ç”Ÿäº†å·¨å¤§çš„æ ‡æ³¨æˆæœ¬ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åˆ©ç”¨éƒ¨åˆ†æ ‡æ³¨æ•°æ®è¿›è¡Œæ ‡ç­¾é«˜æ•ˆçš„æ— é…å¯¹å¤šæ¨¡æ€å­¦ä¹ ï¼Œè¿™å¯ä»¥å°†æ ‡æ³¨æˆæœ¬é™ä½ä¸€åŠã€‚æˆ‘ä»¬å°†è¿™ç§æ–°çš„å­¦ä¹ èŒƒå¼ç§°ä¸ºéƒ¨åˆ†ç›‘ç£æ— é…å¯¹å¤šæ¨¡æ€å­¦ä¹ ï¼ˆPSUMMLï¼‰ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåä¸ºåˆ†è§£éƒ¨åˆ†ç±»é€‚åº”ä¸å¿«ç…§é›†æˆè‡ªè®­ç»ƒï¼ˆDESTï¼‰çš„æ–°æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ç”±ä¸€ä¸ªç´§å‡‘çš„åˆ†å‰²ç½‘ç»œç»„æˆï¼Œè¯¥ç½‘ç»œå…·æœ‰é’ˆå¯¹éƒ¨åˆ†æ ‡æ³¨æ— é…å¯¹å¤šæ¨¡æ€æ•°æ®å­¦ä¹ çš„æ¨¡æ€ç‰¹å®šå½’ä¸€åŒ–å±‚ã€‚PSUMMLçš„å…³é”®æŒ‘æˆ˜åœ¨äºéƒ¨åˆ†ç±»åˆ†å¸ƒå·®å¼‚å¤æ‚ï¼Œè¿™æ˜¯ç”±äºéƒ¨åˆ†ç±»æ ‡æ³¨é€ æˆçš„ï¼Œé˜»ç¢äº†è·¨æ¨¡æ€çš„æœ‰æ•ˆçŸ¥è¯†è½¬ç§»ã€‚æˆ‘ä»¬é€šè¿‡åˆ†è§£å®šç†å¯¹å…¶è¿›è¡Œç†è®ºåˆ†æï¼Œå¹¶æå‡ºäº†ä¸€ç§åˆ†è§£çš„éƒ¨åˆ†ç±»é€‚åº”æŠ€æœ¯ï¼Œä»¥ç²¾ç¡®å¯¹é½éƒ¨åˆ†æ ‡æ³¨çš„è·¨æ¨¡æ€ç±»åˆ«ï¼Œå‡å°‘åˆ†å¸ƒå·®å¼‚ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§å¿«ç…§é›†æˆè‡ªè®­ç»ƒæŠ€æœ¯ï¼Œä»¥åˆ©ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ‰ä»·å€¼çš„å¿«ç…§æ¨¡å‹ï¼Œå¯¹éƒ¨åˆ†æ ‡æ³¨çš„åƒç´ è¿›è¡Œè‡ªè®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸åŒçš„PSUMMLåœºæ™¯ä¸‹è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œæ¶‰åŠä¸¤ä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ï¼Œå³å¿ƒè„å­ç»“æ„åˆ†å‰²å’Œè…¹éƒ¨å¤šå™¨å®˜åˆ†å‰²ã€‚æˆ‘ä»¬çš„æ¡†æ¶æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05190v1">PDF</a> Accepted to MLMI 2024</p>
<p><strong>æ‘˜è¦</strong><br>     è¯¥ç ”ç©¶æ¢ç´¢äº†åˆ©ç”¨éƒ¨åˆ†æ ‡è®°æ•°æ®å®ç°æ ‡ç­¾æ•ˆç‡ä½çš„éé…å¯¹å¤šæ¨¡æ€å­¦ä¹ ï¼ˆPSUMMLï¼‰ï¼Œå¯èŠ‚çœä¸€åŠçš„æ ‡æ³¨æˆæœ¬ã€‚ç ”ç©¶æå‡ºäº†åä¸ºåˆ†è§£éƒ¨åˆ†ç±»åˆ«è‡ªé€‚åº”å¿«ç…§é›†æˆè‡ªè®­ç»ƒï¼ˆDESTï¼‰çš„æ–°å‹æ¡†æ¶ã€‚æ¡†æ¶å…·æœ‰æ¨¡æ€ç‰¹å®šå½’ä¸€åŒ–å±‚çš„ç´§å‡‘åˆ†å‰²ç½‘ç»œï¼Œç”¨äºå¤„ç†éƒ¨åˆ†æ ‡è®°çš„éé…å¯¹å¤šæ¨¡æ€æ•°æ®ã€‚PSUMMLçš„ä¸»è¦æŒ‘æˆ˜åœ¨äºç”±äºéƒ¨åˆ†ç±»åˆ«æ ‡æ³¨é€ æˆçš„å¤æ‚éƒ¨åˆ†ç±»åˆ«åˆ†å¸ƒå·®å¼‚ï¼Œé˜»ç¢äº†è·¨æ¨¡æ€çš„æœ‰æ•ˆçŸ¥è¯†è½¬ç§»ã€‚ç ”ç©¶é€šè¿‡åˆ†è§£å®šç†è¿›è¡Œç†è®ºåˆ†æï¼Œå¹¶æå‡ºäº†ç²¾ç¡®å¯¹é½è·¨æ¨¡æ€éƒ¨åˆ†æ ‡è®°ç±»åˆ«çš„åˆ†è§£éƒ¨åˆ†ç±»åˆ«è‡ªé€‚åº”æŠ€æœ¯ï¼Œä»¥å‡å°‘åˆ†å¸ƒå·®å¼‚ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†å¿«ç…§é›†æˆè‡ªè®­ç»ƒæŠ€æœ¯ï¼Œåˆ©ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¿«ç…§æ¨¡å‹å¯¹éƒ¨åˆ†æ ‡è®°åƒç´ åˆ†é…ä¼ªæ ‡ç­¾è¿›è¡Œè‡ªè®­ç»ƒï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚åœ¨ä¸åŒPSUMMLåœºæ™¯ä¸‹çš„ä¸¤ä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ï¼ˆå¿ƒè„å­ç»“æ„åˆ†å‰²å’Œè…¹éƒ¨å¤šå™¨å®˜åˆ†å‰²ï¼‰çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<ol>
<li>ç ”ç©¶å¼•å…¥äº†éƒ¨åˆ†ç›‘ç£çš„éé…å¯¹å¤šæ¨¡æ€å­¦ä¹ ï¼ˆPSUMMLï¼‰ï¼Œå…è®¸ä½¿ç”¨éƒ¨åˆ†æ ‡è®°çš„æ•°æ®æ¥å‡å°‘æ ‡æ³¨æˆæœ¬ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºDESTçš„æ–°å‹æ¡†æ¶ï¼Œåˆ©ç”¨ç´§å‡‘çš„åˆ†å‰²ç½‘ç»œå’Œæ¨¡æ€ç‰¹å®šå½’ä¸€åŒ–å±‚å¤„ç†PSUMMLä¸­çš„æ•°æ®ã€‚</li>
<li>PSUMMLé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯éƒ¨åˆ†ç±»åˆ«æ ‡æ³¨å¼•èµ·çš„å¤æ‚åˆ†å¸ƒå·®å¼‚ã€‚</li>
<li>é€šè¿‡åˆ†è§£å®šç†åˆ†æè¿™ä¸€é—®é¢˜ï¼Œå¹¶æå‡ºåˆ†è§£éƒ¨åˆ†ç±»åˆ«è‡ªé€‚åº”æŠ€æœ¯æ¥è§£å†³è·¨æ¨¡æ€çš„éƒ¨åˆ†æ ‡è®°ç±»åˆ«å¯¹é½é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨å¿«ç…§é›†æˆè‡ªè®­ç»ƒæŠ€æœ¯ï¼Œåˆ©ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¿«ç…§æ¨¡å‹è¿›è¡Œè‡ªè®­ç»ƒï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒéªŒè¯DESTæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-edb8833a4b617f4633b6ff23c0bc0294.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e586231cf15061b0cb116bca0056cb5.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GaussianCAD-Robust-Self-Supervised-CAD-Reconstruction-from-Three-Orthographic-Views-Using-3D-Gaussian-Splatting"><a href="#GaussianCAD-Robust-Self-Supervised-CAD-Reconstruction-from-Three-Orthographic-Views-Using-3D-Gaussian-Splatting" class="headerlink" title="GaussianCAD: Robust Self-Supervised CAD Reconstruction from Three   Orthographic Views Using 3D Gaussian Splatting"></a>GaussianCAD: Robust Self-Supervised CAD Reconstruction from Three   Orthographic Views Using 3D Gaussian Splatting</h2><p><strong>Authors:Zheng Zhou, Zhe Li, Bo Yu, Lina Hu, Liang Dong, Zijian Yang, Xiaoli Liu, Ning Xu, Ziwei Wang, Yonghao Dang, Jianqin Yin</strong></p>
<p>The automatic reconstruction of 3D computer-aided design (CAD) models from CAD sketches has recently gained significant attention in the computer vision community. Most existing methods, however, rely on vector CAD sketches and 3D ground truth for supervision, which are often difficult to be obtained in industrial applications and are sensitive to noise inputs. We propose viewing CAD reconstruction as a specific instance of sparse-view 3D reconstruction to overcome these limitations. While this reformulation offers a promising perspective, existing 3D reconstruction methods typically require natural images and corresponding camera poses as inputs, which introduces two major significant challenges: (1) modality discrepancy between CAD sketches and natural images, and (2) difficulty of accurate camera pose estimation for CAD sketches. To solve these issues, we first transform the CAD sketches into representations resembling natural images and extract corresponding masks. Next, we manually calculate the camera poses for the orthographic views to ensure accurate alignment within the 3D coordinate system. Finally, we employ a customized sparse-view 3D reconstruction method to achieve high-quality reconstructions from aligned orthographic views. By leveraging raster CAD sketches for self-supervision, our approach eliminates the reliance on vector CAD sketches and 3D ground truth. Experiments on the Sub-Fusion360 dataset demonstrate that our proposed method significantly outperforms previous approaches in CAD reconstruction performance and exhibits strong robustness to noisy inputs. </p>
<blockquote>
<p>ä»è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰è‰å›¾è‡ªåŠ¨é‡å»º3Dè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹æœ€è¿‘åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•éƒ½ä¾èµ–äºçŸ¢é‡CADè‰å›¾å’Œ3DçœŸå®å€¼è¿›è¡Œç›‘ç®¡ï¼Œè¿™åœ¨å·¥ä¸šåº”ç”¨ä¸­å¾€å¾€éš¾ä»¥è·å¾—ï¼Œå¹¶ä¸”å¯¹å™ªå£°è¾“å…¥å¾ˆæ•æ„Ÿã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºå°†CADé‡å»ºè§†ä¸ºç¨€ç–è§†å›¾3Dé‡å»ºçš„ä¸€ä¸ªç‰¹å®šå®ä¾‹ã€‚è™½ç„¶è¿™ç§é‡æ–°è¡¨è¿°æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„è§†è§’ï¼Œä½†ç°æœ‰çš„3Dé‡å»ºæ–¹æ³•é€šå¸¸éœ€è¦è‡ªç„¶å›¾åƒå’Œç›¸åº”çš„ç›¸æœºå§¿æ€ä½œä¸ºè¾“å…¥ï¼Œè¿™å¸¦æ¥äº†ä¸¤å¤§æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰CADè‰å›¾ä¸è‡ªç„¶å›¾åƒä¹‹é—´çš„æ¨¡æ€å·®å¼‚ï¼Œä»¥åŠï¼ˆ2ï¼‰ä¸ºCADè‰å›¾å‡†ç¡®ä¼°è®¡ç›¸æœºå§¿æ€çš„å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆå°†CADè‰å›¾è½¬æ¢ä¸ºç±»ä¼¼äºè‡ªç„¶å›¾åƒçš„è¡¨ç¤ºå½¢å¼ï¼Œå¹¶æå–ç›¸åº”çš„æ©æ¨¡ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ‰‹åŠ¨è®¡ç®—æ­£å°„è§†å›¾çš„ç›¸æœºå§¿æ€ï¼Œä»¥ç¡®ä¿åœ¨3Dåæ ‡ç³»å†…çš„å‡†ç¡®å¯¹é½ã€‚æœ€åï¼Œæˆ‘ä»¬é‡‡ç”¨å®šåˆ¶çš„ç¨€ç–è§†å›¾3Dé‡å»ºæ–¹æ³•ï¼Œä»å¯¹é½çš„æ­£å°„è§†å›¾ä¸­å®ç°é«˜è´¨é‡é‡å»ºã€‚é€šè¿‡åˆ©ç”¨æ …æ ¼CADè‰å›¾è¿›è¡Œè‡ªç›‘ç£ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¶ˆé™¤äº†å¯¹çŸ¢é‡CADè‰å›¾å’Œ3DçœŸå®å€¼çš„ä¾èµ–ã€‚åœ¨Sub-Fusion360æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨CADé‡å»ºæ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œå¹¶å¯¹å˜ˆæ‚è¾“å…¥è¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05161v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶å…³æ³¨äºå°†è®¡ç®—æœºè§†è§‰åº”ç”¨äºè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰æ¨¡å‹çš„è‡ªåŠ¨é‡å»ºé—®é¢˜ï¼Œè§£å†³äº†ä»CADè‰å›¾é‡æ„ä¸‰ç»´æ¨¡å‹æ‰€é¢ä¸´çš„å›°éš¾ã€‚è¯¥æ–¹æ³•å€Ÿé‰´äº†ç¨€ç–è§†å›¾çš„ä¸‰ç»´é‡å»ºæ€æƒ³ï¼Œåˆ©ç”¨çŸ¢é‡CADè‰å›¾ä»¥åŠç›¸æœºå§¿æ€æ•°æ®è§£å†³é‡æ„éš¾é¢˜ï¼Œæœ‰æ•ˆæ¶ˆé™¤äº†å™ªå£°è¾“å…¥å¹²æ‰°ï¼Œå¹¶å°†æ¨¡å‹æ³›åŒ–åº”ç”¨äºæ–°çš„è‰å›¾è¾“å…¥ã€‚æœ€ç»ˆé€šè¿‡Sub-Fusion360æ•°æ®é›†çš„å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•çš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰æ›´å¼ºçš„é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶é’ˆå¯¹CADè‰å›¾è‡ªåŠ¨é‡å»ºä¸‰ç»´æ¨¡å‹çš„é—®é¢˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç¨€ç–è§†å›¾çš„ä¸‰ç»´é‡å»ºæ–¹æ³•æ¥é‡å»ºCADæ¨¡å‹ï¼Œé¿å…äº†ç°æœ‰æ–¹æ³•ä¾èµ–äºçŸ¢é‡CADè‰å›¾ä»¥åŠçœŸå®ä¸‰ç»´æ¨¡å‹çš„å›°éš¾ã€‚</li>
<li>ç ”ç©¶è§£å†³äº†CADè‰å›¾ä¸çœŸå®å›¾åƒä¹‹é—´çš„æ¨¡æ€å·®å¼‚é—®é¢˜ä»¥åŠç›¸æœºå§¿æ€ä¼°è®¡çš„éš¾é¢˜ã€‚é€šè¿‡å°†CADè‰å›¾è½¬åŒ–ä¸ºä¸è‡ªç„¶å›¾åƒç›¸ä¼¼çš„è¡¨ç°å½¢å¼æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>æ–¹æ³•çš„æ ¸å¿ƒæ­¥éª¤åŒ…æ‹¬ï¼šè‰å›¾åˆ°è‡ªç„¶å›¾åƒå½¢å¼çš„è½¬æ¢ã€æå–å¯¹åº”æ©è†œã€æ‰‹åŠ¨è®¡ç®—ç›¸æœºå§¿æ€ä»¥ç¡®ä¿åœ¨ä¸‰ç»´åæ ‡ç³»ä¸­çš„å‡†ç¡®å¯¹é½ä»¥åŠä½¿ç”¨å®šåˆ¶çš„ç¨€ç–è§†å›¾ä¸‰ç»´é‡å»ºæ–¹æ³•å®ç°é«˜è´¨é‡é‡å»ºã€‚ </li>
<li>æ–¹æ³•å€ŸåŠ©è‡ªç›‘ç£æŠ€æœ¯ä½¿ç”¨å…‰æ …åŒ–CADè‰å›¾æ¥è®­ç»ƒæ¨¡å‹ï¼Œå¤§å¤§æå‡äº†æ¨¡å‹å¯¹äºå™ªå£°çš„é²æ£’æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05161">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b80d5cda8202c148c0d445e7e65cce92.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5f0e9ed6653df810387619db3fc38dde.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cd37a4a3e1391bebd41e73bca64d71f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3bdcfc9f6b640741a0c2564593d7a3a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa8fb2fa7ceb8387b7eea520eb2f1744.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5852cd55f1f0fec5e9620b2ffb9f8a1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="We-Care-Each-Pixel-Calibrating-on-Medical-Segmentation-Model"><a href="#We-Care-Each-Pixel-Calibrating-on-Medical-Segmentation-Model" class="headerlink" title="We Care Each Pixel: Calibrating on Medical Segmentation Model"></a>We Care Each Pixel: Calibrating on Medical Segmentation Model</h2><p><strong>Authors:Wenhao Liang, Wei Zhang, Yue Lin, Miao Xu, Olaf Maennel, Weitong Chen</strong></p>
<p>Medical image segmentation is fundamental for computer-aided diagnostics, providing accurate delineation of anatomical structures and pathological regions. While common metrics such as Accuracy, DSC, IoU, and HD primarily quantify spatial agreement between predictions and ground-truth labels, they do not assess the calibration quality of segmentation models, which is crucial for clinical reliability. To address this limitation, we propose pixel-wise Expected Calibration Error (pECE), a novel metric that explicitly measures miscalibration at the pixel level, thereby ensuring both spatial precision and confidence reliability. We further introduce a morphological adaptation strategy that applies morphological operations to ground-truth masks before computing calibration losses, particularly benefiting margin-based losses such as Margin SVLS and NACL. Additionally, we present the Signed Distance Calibration Loss (SDC), which aligns boundary geometry with calibration objectives by penalizing discrepancies between predicted and ground-truth signed distance functions (SDFs). Extensive experiments demonstrate that our method not only enhances segmentation performance but also improves calibration quality, yielding more trustworthy confidence estimates. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/EagleAdelaide/SDC-Loss">https://github.com/EagleAdelaide/SDC-Loss</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹äºè®¡ç®—æœºè¾…åŠ©è¯Šæ–­è‡³å…³é‡è¦ï¼Œå®ƒæä¾›äº†å‡†ç¡®çš„è§£å‰–ç»“æ„å’Œç—…ç†åŒºåŸŸçš„æç»˜ã€‚è™½ç„¶å‡†ç¡®åº¦ã€DSCã€IoUå’ŒHDç­‰å¸¸è§æŒ‡æ ‡ä¸»è¦é‡åŒ–é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾ä¹‹é—´çš„ç©ºé—´ä¸€è‡´æ€§ï¼Œä½†å®ƒä»¬å¹¶æ²¡æœ‰è¯„ä¼°åˆ†å‰²æ¨¡å‹çš„æ ¡å‡†è´¨é‡ï¼Œè¿™å¯¹äºä¸´åºŠå¯é æ€§è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åƒç´ çº§æœŸæœ›æ ¡å‡†è¯¯å·®ï¼ˆpECEï¼‰è¿™ä¸€æ–°æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡æ˜ç¡®æµ‹é‡åƒç´ çº§åˆ«çš„æ ¡å‡†è¯¯å·®ï¼Œä»è€Œç¡®ä¿ç©ºé—´ç²¾åº¦å’Œç½®ä¿¡åº¦å¯é æ€§ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§å½¢æ€å­¦é€‚åº”ç­–ç•¥ï¼Œå³å¯¹çœŸå®æ ‡ç­¾æ©è†œè¿›è¡Œå½¢æ€å­¦æ“ä½œï¼Œç„¶åå†è®¡ç®—æ ¡å‡†æŸå¤±ï¼Œè¿™å¯¹åŸºäºè¾¹è·çš„æŸå¤±ï¼ˆå¦‚Margin SVLSå’ŒNACLï¼‰ç‰¹åˆ«æœ‰ç›Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¦å·è·ç¦»æ ¡å‡†æŸå¤±ï¼ˆSDCï¼‰ï¼Œå®ƒé€šè¿‡æƒ©ç½šé¢„æµ‹å’ŒçœŸå®æ ‡ç­¾ç¬¦å·è·ç¦»å‡½æ•°ï¼ˆSDFsï¼‰ä¹‹é—´çš„å·®å¼‚ï¼Œå°†è¾¹ç•Œå‡ ä½•ä¸æ ¡å‡†ç›®æ ‡å¯¹é½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æé«˜äº†åˆ†å‰²æ€§èƒ½ï¼Œè¿˜æé«˜äº†æ ¡å‡†è´¨é‡ï¼Œäº§ç”Ÿäº†æ›´å¯é çš„ç½®ä¿¡åº¦ä¼°è®¡ã€‚ä»£ç å¯åœ¨ï¼š[<a target="_blank" rel="noopener" href="https://github.com/EagleAdelaide/SDC-Loss%E8%8E%B7%E5%8F%96%E3%80%82]">https://github.com/EagleAdelaide/SDC-Lossè·å–ã€‚]</a>(<a target="_blank" rel="noopener" href="https://github.com/EagleAdelaide/SDC-Loss%E8%8E%B7%E5">https://github.com/EagleAdelaide/SDC-Loss%E8%8E%B7%E5</a> %8F%96%E3%80%82)</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05107v1">PDF</a> Under Reviewing</p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹äºè®¡ç®—æœºè¾…åŠ©è¯Šæ–­è‡³å…³é‡è¦ï¼Œå®ƒä¸ºè§£å‰–ç»“æ„å’Œç—…ç†åŒºåŸŸçš„å‡†ç¡®ç•Œå®šæä¾›äº†ä¾æ®ã€‚é’ˆå¯¹ç°æœ‰è¯„ä¼°æŒ‡æ ‡æ— æ³•å…¨é¢è¯„ä¼°æ¨¡å‹æ ¡å‡†è´¨é‡çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åƒç´ çº§æœŸæœ›æ ¡å‡†è¯¯å·®ï¼ˆpECEï¼‰è¿™ä¸€æ–°æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡èƒ½å¤Ÿæ˜ç¡®æµ‹é‡åƒç´ çº§åˆ«çš„è¯¯æ ¡å‡†æƒ…å†µï¼Œä»è€Œç¡®ä¿ç©ºé—´ç²¾åº¦å’Œç½®ä¿¡åº¦å¯é æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†å½¢æ€å­¦é€‚åº”ç­–ç•¥ï¼Œé€šè¿‡å½¢æ€å­¦æ“ä½œå¯¹çœŸå®æ ‡ç­¾æ©è†œè¿›è¡Œå¤„ç†ä»¥è®¡ç®—æ ¡å‡†æŸå¤±ï¼Œå¯¹åŸºäºè¾¹è·çš„æŸå¤±å¦‚Margin SVLSå’ŒNACLç‰¹åˆ«æœ‰ç›Šã€‚åŒæ—¶ï¼Œæœ¬æ–‡æå‡ºäº†ç¬¦å·è·ç¦»æ ¡å‡†æŸå¤±ï¼ˆSDCï¼‰ï¼Œå®ƒé€šè¿‡æƒ©ç½šé¢„æµ‹ä¸çœŸå®æ ‡ç­¾çš„ç¬¦å·è·ç¦»å‡½æ•°ä¹‹é—´çš„å·®å¼‚æ¥å¯¹é½è¾¹ç•Œå‡ ä½•ä¸æ ¡å‡†ç›®æ ‡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…æé«˜äº†åˆ†å‰²æ€§èƒ½ï¼Œè¿˜æé«˜äº†æ ¡å‡†è´¨é‡ï¼Œäº§ç”Ÿäº†æ›´å¯é çš„ç½®ä¿¡åº¦ä¼°è®¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹è®¡ç®—æœºè¾…åŠ©è¯Šæ–­è‡³å…³é‡è¦ï¼Œç”¨äºå‡†ç¡®ç•Œå®šè§£å‰–ç»“æ„å’Œç—…ç†åŒºåŸŸã€‚</li>
<li>ç°æœ‰è¯„ä¼°æŒ‡æ ‡å¦‚Accuracyã€DSCã€IoUå’ŒHDä¸»è¦å…³æ³¨é¢„æµ‹ä¸çœŸå®æ ‡ç­¾çš„ç©ºé—´ä¸€è‡´æ€§ï¼Œä½†æ— æ³•è¯„ä¼°æ¨¡å‹çš„æ ¡å‡†è´¨é‡ï¼Œè¿™æ˜¯ä¸´åºŠå¯é æ€§çš„å…³é”®ã€‚</li>
<li>åƒç´ çº§æœŸæœ›æ ¡å‡†è¯¯å·®ï¼ˆpECEï¼‰æ˜¯ä¸€ç§æ–°æŒ‡æ ‡ï¼Œç”¨äºæ˜ç¡®æµ‹é‡åƒç´ çº§åˆ«çš„è¯¯æ ¡å‡†æƒ…å†µï¼Œç¡®ä¿ç©ºé—´ç²¾åº¦å’Œç½®ä¿¡åº¦å¯é æ€§ã€‚</li>
<li>å½¢æ€å­¦é€‚åº”ç­–ç•¥é€šè¿‡å½¢æ€å­¦æ“ä½œå¤„ç†çœŸå®æ ‡ç­¾æ©è†œæ¥è®¡ç®—æ ¡å‡†æŸå¤±ï¼Œç‰¹åˆ«é€‚ç”¨äºåŸºäºè¾¹è·çš„æŸå¤±ã€‚</li>
<li>ç¬¦å·è·ç¦»æ ¡å‡†æŸå¤±ï¼ˆSDCï¼‰é€šè¿‡æƒ©ç½šé¢„æµ‹ä¸çœŸå®æ ‡ç­¾çš„ç¬¦å·è·ç¦»å‡½æ•°å·®å¼‚æ¥å¢å¼ºè¾¹ç•Œå‡ ä½•ä¸æ ¡å‡†ç›®æ ‡çš„å¯¹é½ã€‚</li>
<li>æœ¬æ–‡æ–¹æ³•ä¸ä»…æé«˜äº†åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ€§èƒ½ï¼Œè¿˜æé«˜äº†æ¨¡å‹çš„æ ¡å‡†è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05107">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a571e3be3ae36c50314ec37dc5801ef9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbc77f38220450278a050143f7268ad4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2476221a4bbb083de66d693dbf4aff01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3aef861b8218c480a2e21485991422d7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="HyDA-Hypernetworks-for-Test-Time-Domain-Adaptation-in-Medical-Imaging-Analysis"><a href="#HyDA-Hypernetworks-for-Test-Time-Domain-Adaptation-in-Medical-Imaging-Analysis" class="headerlink" title="HyDA: Hypernetworks for Test Time Domain Adaptation in Medical Imaging   Analysis"></a>HyDA: Hypernetworks for Test Time Domain Adaptation in Medical Imaging   Analysis</h2><p><strong>Authors:Doron Serebro, Tammy Riklin-Raviv</strong></p>
<p>Medical imaging datasets often vary due to differences in acquisition protocols, patient demographics, and imaging devices. These variations in data distribution, known as domain shift, present a significant challenge in adapting imaging analysis models for practical healthcare applications.   Most current domain adaptation (DA) approaches aim either to align the distributions between the source and target domains or to learn an invariant feature space that generalizes well across all domains. However, both strategies require access to a sufficient number of examples, though not necessarily annotated, from the test domain during training. This limitation hinders the widespread deployment of models in clinical settings, where target domain data may only be accessible in real time.   In this work, we introduce HyDA, a novel hypernetwork framework that leverages domain characteristics rather than suppressing them, enabling dynamic adaptation at inference time. Specifically, HyDA learns implicit domain representations and uses them to adjust model parameters on-the-fly, effectively interpolating to unseen domains. We validate HyDA on two clinically relevant applications - MRI brain age prediction and chest X-ray pathology classification - demonstrating its ability to generalize across tasks and modalities. Our code is available at TBD. </p>
<blockquote>
<p>åŒ»å­¦æˆåƒæ•°æ®é›†å¾€å¾€ç”±äºé‡‡é›†åè®®ã€æ‚£è€…ç‰¹å¾å’Œæˆåƒè®¾å¤‡ç­‰æ–¹é¢çš„å·®å¼‚è€Œæœ‰æ‰€ä¸åŒã€‚æ•°æ®åˆ†å¸ƒçš„è¿™äº›å˜åŒ–è¢«ç§°ä¸ºåŸŸåç§»ï¼Œåœ¨ä¸ºå®é™…åŒ»ç–—ä¿å¥åº”ç”¨é€‚åº”æˆåƒåˆ†ææ¨¡å‹æ—¶ï¼Œè¿™æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚å½“å‰å¤§å¤šæ•°é¢†åŸŸé€‚åº”ï¼ˆDAï¼‰æ–¹æ³•æ—¨åœ¨å¯¹é½æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´çš„åˆ†å¸ƒï¼Œæˆ–å­¦ä¹ ä¸€ä¸ªåœ¨æ‰€æœ‰é¢†åŸŸä¸­éƒ½è¡¨ç°è‰¯å¥½çš„ä¸å˜ç‰¹å¾ç©ºé—´ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§ç­–ç•¥éƒ½éœ€è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¿é—®æµ‹è¯•åŸŸçš„è¶³å¤Ÿæ•°é‡çš„æ ·æœ¬ï¼Œå°½ç®¡ä¸ä¸€å®šéœ€è¦æ³¨é‡Šã€‚è¿™ä¸€é™åˆ¶é˜»ç¢äº†æ¨¡å‹åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„å¹¿æ³›éƒ¨ç½²ï¼Œå› ä¸ºç›®æ ‡åŸŸæ•°æ®å¯èƒ½åªèƒ½åœ¨å®æ—¶ç¯å¢ƒä¸­è®¿é—®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†HyDAï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è¶…ç½‘ç»œæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨é¢†åŸŸç‰¹æ€§è€Œä¸æ˜¯æŠ‘åˆ¶å®ƒä»¬ï¼Œä»è€Œåœ¨æ¨ç†æ—¶é—´å®ç°åŠ¨æ€é€‚åº”ã€‚å…·ä½“æ¥è¯´ï¼ŒHyDAå­¦ä¹ éšå¼é¢†åŸŸè¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨å®ƒä»¬åœ¨è¿è¡Œæ—¶å®æ—¶è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œæœ‰æ•ˆåœ°æ’å€¼åˆ°æœªè§è¿‡çš„é¢†åŸŸã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸´åºŠç›¸å…³çš„åº”ç”¨ä¸ŠéªŒè¯äº†HyDAçš„æœ‰æ•ˆæ€§ï¼Œåˆ†åˆ«æ˜¯MRIè„‘é¾„é¢„æµ‹å’Œèƒ¸éƒ¨Xå°„çº¿ç—…ç†åˆ†ç±»ï¼Œå±•ç¤ºäº†å®ƒåœ¨è·¨ä»»åŠ¡å’Œè·¨æ¨¡æ€æ–¹é¢çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨ï¼ˆå¾…å®šï¼‰å¤„æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04979v1">PDF</a> submitted to MICCAI 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦æˆåƒæ•°æ®é›†ç”±äºé‡‡é›†åè®®ã€æ‚£è€…ç‰¹å¾å’Œæˆåƒè®¾å¤‡å·®å¼‚è€Œå¸¸æœ‰å˜åŒ–ã€‚è¿™äº›è¢«ç§°ä¸ºåŸŸåç§»çš„æ•°æ®åˆ†å¸ƒå˜åŒ–ï¼Œä¸ºå°†æˆåƒåˆ†ææ¨¡å‹é€‚åº”äºå®é™…åŒ»ç–—åº”ç”¨å¸¦æ¥äº†æŒ‘æˆ˜ã€‚å½“å‰å¤§å¤šæ•°åŸŸé€‚åº”æ–¹æ³•æ—¨åœ¨å¯¹é½æºåŸŸå’Œç›®æ ‡åŸŸåˆ†å¸ƒï¼Œæˆ–å­¦ä¹ ä¸€ä¸ªåœ¨æ‰€æœ‰åŸŸä¸­è¡¨ç°è‰¯å¥½çš„ä¸å˜ç‰¹å¾ç©ºé—´ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§ç­–ç•¥éƒ½éœ€è¦åœ¨è®­ç»ƒæœŸé—´è®¿é—®æµ‹è¯•åŸŸçš„è¶³å¤Ÿæ ·æœ¬ï¼Œå³ä½¿ä¸å¿…è¿›è¡Œæ ‡æ³¨ã€‚è¿™ä¸€å±€é™æ€§é˜»ç¢äº†æ¨¡å‹åœ¨ä¸´åºŠç¯å¢ƒä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå› ä¸ºåœ¨ç°å®æ—¶é—´ä¸­å¯èƒ½ä»…å¯è®¿é—®ç›®æ ‡åŸŸæ•°æ®ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†HyDAï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨åŸŸç‰¹å¾è€ŒéæŠ‘åˆ¶å®ƒä»¬çš„è¶…ç½‘ç»œæ¡†æ¶ï¼Œå¯åœ¨æ¨æ–­æ—¶è¿›è¡ŒåŠ¨æ€é€‚åº”ã€‚å…·ä½“è€Œè¨€ï¼ŒHyDAå­¦ä¹ éšå¼åŸŸè¡¨ç¤ºå¹¶ä½¿ç”¨å®ƒä»¬å³æ—¶è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œæœ‰æ•ˆåœ°æ¨å¹¿åˆ°æœªè§è¿‡çš„åŸŸã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸´åºŠåº”ç”¨ä¸ŠéªŒè¯äº†HyDAçš„æœ‰æ•ˆæ€§ï¼Œåˆ†åˆ«æ˜¯MRIè„‘éƒ¨å¹´é¾„é¢„æµ‹å’Œèƒ¸éƒ¨Xå°„çº¿ç—…ç†åˆ†ç±»ï¼Œè¯æ˜äº†å…¶è·¨ä»»åŠ¡å’Œæ¨¡æ€çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒæ•°æ®é›†å­˜åœ¨ç”±äºé‡‡é›†åè®®ã€æ‚£è€…ç‰¹å¾å’Œæˆåƒè®¾å¤‡å·®å¼‚å¯¼è‡´çš„åŸŸåç§»é—®é¢˜ã€‚</li>
<li>å½“å‰åŸŸé€‚åº”æ–¹æ³•ä¸»è¦é€šè¿‡å¯¹é½åŸŸåˆ†å¸ƒæˆ–å­¦ä¹ ä¸å˜ç‰¹å¾ç©ºé—´æ¥é€‚åº”æ¨¡å‹ï¼Œä½†éœ€è®¿é—®æµ‹è¯•åŸŸçš„æ ·æœ¬ï¼Œè¿™åœ¨ä¸´åºŠç¯å¢ƒä¸­å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>HyDAæ˜¯ä¸€ç§æ–°å‹è¶…ç½‘ç»œæ¡†æ¶ï¼Œåˆ©ç”¨éšå¼åŸŸè¡¨ç¤ºè¿›è¡ŒåŠ¨æ€é€‚åº”ï¼Œæ— éœ€åœ¨è®­ç»ƒæœŸé—´è®¿é—®ç›®æ ‡åŸŸæ ·æœ¬ã€‚</li>
<li>HyDAåœ¨MRIè„‘éƒ¨å¹´é¾„é¢„æµ‹å’Œèƒ¸éƒ¨Xå°„çº¿ç—…ç†åˆ†ç±»ç­‰ä¸´åºŠåº”ç”¨ä¸­å¾—åˆ°äº†éªŒè¯ã€‚</li>
<li>HyDAå…·æœ‰è·¨ä»»åŠ¡å’Œæ¨¡æ€çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†åŠ¨æ€å‚æ•°è°ƒæ•´ä»¥é€‚åº”ä¸åŒåŸŸçš„æ ·æœ¬ï¼Œå±•ç°å‡ºå¯¹æœªè§åŸŸçš„æ³›åŒ–æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-29eabc968e47e93aedcb05832f9e2aed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84a30891e046fd5ec19ca169037165aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87d2edf5f227379b2f917d2d05816363.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0792852476e2199d59cc38756e08b4fe.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Rethinking-Few-Shot-Medical-Image-Segmentation-by-SAM2-A-Training-Free-Framework-with-Augmentative-Prompting-and-Dynamic-Matching"><a href="#Rethinking-Few-Shot-Medical-Image-Segmentation-by-SAM2-A-Training-Free-Framework-with-Augmentative-Prompting-and-Dynamic-Matching" class="headerlink" title="Rethinking Few-Shot Medical Image Segmentation by SAM2: A Training-Free   Framework with Augmentative Prompting and Dynamic Matching"></a>Rethinking Few-Shot Medical Image Segmentation by SAM2: A Training-Free   Framework with Augmentative Prompting and Dynamic Matching</h2><p><strong>Authors:Haiyue Zu, Jun Ge, Heting Xiao, Jile Xie, Zhangzhe Zhou, Yifan Meng, Jiayi Ni, Junjie Niu, Linlin Zhang, Li Ni, Huilin Yang</strong></p>
<p>The reliance on large labeled datasets presents a significant challenge in medical image segmentation. Few-shot learning offers a potential solution, but existing methods often still require substantial training data. This paper proposes a novel approach that leverages the Segment Anything Model 2 (SAM2), a vision foundation model with strong video segmentation capabilities. We conceptualize 3D medical image volumes as video sequences, departing from the traditional slice-by-slice paradigm. Our core innovation is a support-query matching strategy: we perform extensive data augmentation on a single labeled support image and, for each frame in the query volume, algorithmically select the most analogous augmented support image. This selected image, along with its corresponding mask, is used as a mask prompt, driving SAM2â€™s video segmentation. This approach entirely avoids model retraining or parameter updates. We demonstrate state-of-the-art performance on benchmark few-shot medical image segmentation datasets, achieving significant improvements in accuracy and annotation efficiency. This plug-and-play method offers a powerful and generalizable solution for 3D medical image segmentation. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œå¯¹å¤§æ ‡ç­¾æ•°æ®é›†çš„ä¾èµ–æ„æˆäº†ä¸€å¤§æŒ‘æˆ˜ã€‚å°æ ·æœ¬å­¦ä¹ æä¾›äº†ä¸€ä¸ªæ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€ä»éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨Segment Anything Model 2ï¼ˆSAM2ï¼‰çš„æ–°æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰å¼ºå¤§è§†é¢‘åˆ†å‰²èƒ½åŠ›çš„è§†è§‰åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬å°†3DåŒ»å­¦å›¾åƒä½“ç§¯æ¦‚å¿µåŒ–ä¸ºè§†é¢‘åºåˆ—ï¼Œæ‘’å¼ƒäº†ä¼ ç»Ÿçš„é€ç‰‡å¤„ç†æ¨¡å¼ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒåˆ›æ–°ä¹‹å¤„åœ¨äºæ”¯æŒæŸ¥è¯¢åŒ¹é…ç­–ç•¥ï¼šæˆ‘ä»¬å¯¹å•ä¸ªæ ‡è®°çš„æ”¯æŒå›¾åƒè¿›è¡Œå¤§é‡æ•°æ®å¢å¼ºï¼Œå¹¶é’ˆå¯¹æŸ¥è¯¢ä½“ç§¯ä¸­çš„æ¯ä¸€å¸§ï¼Œç®—æ³•é€‰æ‹©æœ€ç±»ä¼¼çš„å¢å¼ºæ”¯æŒå›¾åƒã€‚æ‰€é€‰å›¾åƒåŠå…¶ç›¸åº”çš„æ©è†œè¢«ç”¨ä½œæ©è†œæç¤ºï¼Œé©±åŠ¨SAM2çš„è§†é¢‘åˆ†å‰²ã€‚æ­¤æ–¹æ³•å®Œå…¨é¿å…äº†æ¨¡å‹é‡æ–°è®­ç»ƒæˆ–å‚æ•°æ›´æ–°ã€‚æˆ‘ä»¬åœ¨åŸºå‡†å°æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å‡†ç¡®æ€§å’Œæ³¨é‡Šæ•ˆç‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚è¿™ç§å³æ’å³ç”¨æ–¹æ³•ä¸º3DåŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†å¼ºå¤§ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04826v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œåˆ©ç”¨Segment Anything Model 2ï¼ˆSAM2ï¼‰æ¨¡å‹å¼ºå¤§çš„è§†é¢‘åˆ†å‰²èƒ½åŠ›ã€‚é€šè¿‡å°†3DåŒ»å­¦å›¾åƒä½“ç§¯æ¦‚å¿µåŒ–ä¸ºè§†é¢‘åºåˆ—ï¼Œé‡‡ç”¨æ”¯æŒæŸ¥è¯¢åŒ¹é…ç­–ç•¥ï¼Œå¯¹å•ä¸ªæ ‡è®°çš„æ”¯æŒå›¾åƒè¿›è¡Œå¤§é‡æ•°æ®å¢å¼ºï¼Œå¹¶ç®—æ³•é€‰æ‹©æœ€ç›¸ä¼¼çš„å¢å¼ºæ”¯æŒå›¾åƒä¸ºæŸ¥è¯¢ä½“ç§¯çš„æ¯ä¸€å¸§è¿›è¡Œåˆ†å‰²ã€‚æ­¤æ–¹æ³•é¿å…äº†æ¨¡å‹é‡æ–°è®­ç»ƒæˆ–å‚æ•°æ›´æ–°ï¼Œæé«˜äº†å‡†ç¡®æ€§å’Œæ ‡æ³¨æ•ˆç‡ï¼Œä¸ºä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†å¼ºå¤§ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ä¾èµ–å¤§é‡æ ‡è®°æ•°æ®é›†æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>Few-shotå­¦ä¹ æä¾›äº†ä¸€ç§æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç°æœ‰æ–¹æ³•ä»éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®ã€‚</li>
<li>æœ¬è®ºæ–‡æå‡ºä¸€ç§æ–°å‹æ–¹æ³•ï¼Œåˆ©ç”¨Segment Anything Model 2ï¼ˆSAM2ï¼‰æ¨¡å‹è¿›è¡ŒåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>å°†3DåŒ»å­¦å›¾åƒä½“ç§¯æ¦‚å¿µåŒ–ä¸ºè§†é¢‘åºåˆ—ï¼Œé‡‡ç”¨æ”¯æŒæŸ¥è¯¢åŒ¹é…ç­–ç•¥è¿›è¡Œæ•°æ®å¢å¼ºã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡é€‰æ‹©æœ€ç›¸ä¼¼çš„å¢å¼ºæ”¯æŒå›¾åƒä¸ºæŸ¥è¯¢ä½“ç§¯çš„æ¯ä¸€å¸§è¿›è¡Œåˆ†å‰²ï¼Œé¿å…äº†æ¨¡å‹é‡æ–°è®­ç»ƒæˆ–å‚æ•°æ›´æ–°ã€‚</li>
<li>åœ¨åŸºå‡†å°‘æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’Œæ ‡æ³¨æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04826">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-419cd1ae8355968af7b09762edb4669e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39a938d457bc457bec9f634b911c8e89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2cd0daa69baaef5b4804bd728e675ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f0da8f7ea6c174497756b87d14f56db7.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Spatial-regularisation-for-improved-accuracy-and-interpretability-in-keypoint-based-registration"><a href="#Spatial-regularisation-for-improved-accuracy-and-interpretability-in-keypoint-based-registration" class="headerlink" title="Spatial regularisation for improved accuracy and interpretability in   keypoint-based registration"></a>Spatial regularisation for improved accuracy and interpretability in   keypoint-based registration</h2><p><strong>Authors:Benjamin Billot, Ramya Muthukrishnan, Esra Abaci-Turk, P. Ellen Grant, Nicholas Ayache, HervÃ© Delingette, Polina Golland</strong></p>
<p>Unsupervised registration strategies bypass requirements in ground truth transforms or segmentations by optimising similarity metrics between fixed and moved volumes. Among these methods, a recent subclass of approaches based on unsupervised keypoint detection stand out as very promising for interpretability. Specifically, these methods train a network to predict feature maps for fixed and moving images, from which explainable centres of mass are computed to obtain point clouds, that are then aligned in closed-form. However, the features returned by the network often yield spatially diffuse patterns that are hard to interpret, thus undermining the purpose of keypoint-based registration. Here, we propose a three-fold loss to regularise the spatial distribution of the features. First, we use the KL divergence to model features as point spread functions that we interpret as probabilistic keypoints. Then, we sharpen the spatial distributions of these features to increase the precision of the detected landmarks. Finally, we introduce a new repulsive loss across keypoints to encourage spatial diversity. Overall, our loss considerably improves the interpretability of the features, which now correspond to precise and anatomically meaningful landmarks. We demonstrate our three-fold loss in foetal rigid motion tracking and brain MRI affine registration tasks, where it not only outperforms state-of-the-art unsupervised strategies, but also bridges the gap with state-of-the-art supervised methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/BenBillot/spatial_regularisation">https://github.com/BenBillot/spatial_regularisation</a>. </p>
<blockquote>
<p>æ— ç›‘ç£çš„æ³¨å†Œç­–ç•¥é€šè¿‡ä¼˜åŒ–å›ºå®šä½“ç§¯å’Œç§»åŠ¨ä½“ç§¯ä¹‹é—´çš„ç›¸ä¼¼åº¦æŒ‡æ ‡ï¼Œä»è€Œç»•è¿‡äº†å¯¹çœŸå®å˜æ¢æˆ–åˆ†å‰²çš„è¦æ±‚ã€‚åœ¨è¿™äº›æ–¹æ³•ä¸­ï¼Œæœ€è¿‘å‡ºç°çš„ä¸€ç§åŸºäºæ— ç›‘ç£å…³é”®ç‚¹æ£€æµ‹çš„æ–¹æ³•ä½œä¸ºè§£é‡Šæ€§å¾ˆå¼ºçš„å­ç±»è„±é¢–è€Œå‡ºã€‚å…·ä½“æ¥è¯´ï¼Œè¿™äº›æ–¹æ³•è®­ç»ƒç½‘ç»œå¯¹å›ºå®šå›¾åƒå’Œç§»åŠ¨å›¾åƒè¿›è¡Œç‰¹å¾æ˜ å°„é¢„æµ‹ï¼Œä»ä¸­è®¡ç®—å‡ºå¯è§£é‡Šçš„è´¨é‡ä¸­å¿ƒï¼Œä»¥è·å¾—ç‚¹äº‘ï¼Œç„¶åè¿›è¡Œé—­å¼å¯¹é½ã€‚ç„¶è€Œï¼Œç½‘ç»œè¿”å›çš„ç‰¹å¾é€šå¸¸ä¼šäº§ç”Ÿéš¾ä»¥è§£é‡Šçš„ç©ºé—´æ‰©æ•£æ¨¡å¼ï¼Œä»è€Œç ´åäº†åŸºäºå…³é”®ç‚¹çš„æ³¨å†Œç›®çš„ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸‰é‡æŸå¤±æ¥è§„èŒƒç‰¹å¾çš„ç©ºé—´åˆ†å¸ƒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨KLæ•£åº¦å°†ç‰¹å¾å»ºæ¨¡ä¸ºç‚¹æ‰©æ•£å‡½æ•°ï¼Œæˆ‘ä»¬å°†å…¶è§£é‡Šä¸ºæ¦‚ç‡å…³é”®ç‚¹ã€‚ç„¶åï¼Œæˆ‘ä»¬é”åŒ–è¿™äº›ç‰¹å¾çš„ç©ºé—´åˆ†å¸ƒï¼Œä»¥æé«˜æ£€æµ‹åˆ°çš„åœ°æ ‡çš„ç²¾åº¦ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨å…³é”®ç‚¹ä¹‹é—´å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ’æ–¥æŸå¤±æ¥é¼“åŠ±ç©ºé—´å¤šæ ·æ€§ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„æŸå¤±å¤§å¤§æé«˜äº†ç‰¹å¾çš„è§£é‡Šæ€§ï¼Œè¿™äº›ç‰¹å¾ç°åœ¨å¯¹åº”äºç²¾ç¡®ä¸”è§£å‰–ä¸Šæ„ä¹‰é‡å¤§çš„åœ°æ ‡ã€‚æˆ‘ä»¬åœ¨èƒå„¿åˆšä½“è¿åŠ¨è·Ÿè¸ªå’Œè„‘éƒ¨MRIä»¿å°„æ³¨å†Œä»»åŠ¡ä¸­å±•ç¤ºäº†æˆ‘ä»¬çš„ä¸‰é‡æŸå¤±ï¼Œå®ƒä¸ä»…è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ— ç›‘ç£ç­–ç•¥ï¼Œè€Œä¸”ç¼©å°äº†ä¸æœ€å…ˆè¿›çš„ç›‘ç£æ–¹æ³•çš„å·®è·ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BenBillot/spatial_regularisation%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/BenBillot/spatial_regularisationè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04499v2">PDF</a> under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ— ç›‘ç£å…³é”®ç‚¹æ£€æµ‹çš„æ–¹æ³•ï¼Œç”¨äºåŒ»å­¦å›¾åƒé…å‡†ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–å›ºå®šå›¾åƒå’Œç§»åŠ¨å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§åº¦é‡æ¥å®ç°æ— ç›‘ç£é…å‡†ã€‚é’ˆå¯¹ç½‘ç»œç‰¹å¾è¿”å›çš„ç©ºé—´æ‰©æ•£æ¨¡å¼éš¾ä»¥è§£é‡Šçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸‰å€æŸå¤±æ¥è§„èŒƒç‰¹å¾çš„ç©ºé—´åˆ†å¸ƒã€‚é€šè¿‡KLæ•£åº¦å»ºæ¨¡ç‰¹å¾ä¸ºç‚¹æ‰©æ•£å‡½æ•°ï¼Œå°–é”åŒ–ç‰¹å¾çš„ç©ºé—´åˆ†å¸ƒï¼Œå¹¶å¼•å…¥æ’æ–¥æŸå¤±é¼“åŠ±ç©ºé—´å¤šæ ·æ€§ã€‚æ”¹è¿›åçš„æ–¹æ³•åœ¨èƒå„¿åˆšæ€§è¿åŠ¨è·Ÿè¸ªå’ŒMRIè„‘éƒ¨å›¾åƒä»¿å°„é…å‡†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…¶æ€§èƒ½è¶…è¶Šäº†ç°æœ‰æ— ç›‘ç£ç­–ç•¥ï¼Œå¹¶æ¥è¿‘äº†ç›‘ç£æ–¹æ³•çš„æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ— ç›‘ç£é…å‡†ç­–ç•¥é€šè¿‡ä¼˜åŒ–å›ºå®šå’Œç§»åŠ¨å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§åº¦é‡ï¼Œç»•è¿‡å¯¹çœŸå®æ ‡ç­¾å˜æ¢æˆ–åˆ†å‰²çš„è¦æ±‚ã€‚</li>
<li>åŸºäºæ— ç›‘ç£å…³é”®ç‚¹æ£€æµ‹çš„æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒé…å‡†ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>ç½‘ç»œç‰¹å¾è¿”å›çš„ç©ºé—´æ‰©æ•£æ¨¡å¼éš¾ä»¥è§£é‡Šï¼Œå½±å“å…³é”®ç‚¹é…å‡†çš„æ•ˆæœã€‚</li>
<li>æå‡ºäº†ä¸€ç§ä¸‰å€æŸå¤±æ¥è§„èŒƒç‰¹å¾çš„ç©ºé—´åˆ†å¸ƒï¼ŒåŒ…æ‹¬ä½¿ç”¨KLæ•£åº¦å»ºæ¨¡ç‰¹å¾ã€å°–é”åŒ–ç‰¹å¾ç©ºé—´åˆ†å¸ƒä»¥åŠå¼•å…¥æ’æ–¥æŸå¤±ã€‚</li>
<li>æ”¹è¿›åçš„æ–¹æ³•åœ¨èƒå„¿åˆšæ€§è¿åŠ¨è·Ÿè¸ªå’ŒMRIè„‘éƒ¨å›¾åƒä»¿å°„é…å‡†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…è¶…è¶Šäº†ç°æœ‰æ— ç›‘ç£ç­–ç•¥ï¼Œè¿˜ç¼©å°äº†ä¸ç°æœ‰ç›‘ç£æ–¹æ³•çš„å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04499">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-26d1823aefbb956e95dd43ef0e6d0f92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc71c82a2932ed132b853769d83c9338.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61f4271b1952ce09e7b0ed0f08304acb.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GBT-SAM-A-Parameter-Efficient-Depth-Aware-Model-for-Generalizable-Brain-tumour-Segmentation-on-mp-MRI"><a href="#GBT-SAM-A-Parameter-Efficient-Depth-Aware-Model-for-Generalizable-Brain-tumour-Segmentation-on-mp-MRI" class="headerlink" title="GBT-SAM: A Parameter-Efficient Depth-Aware Model for Generalizable Brain   tumour Segmentation on mp-MRI"></a>GBT-SAM: A Parameter-Efficient Depth-Aware Model for Generalizable Brain   tumour Segmentation on mp-MRI</h2><p><strong>Authors:Cecilia Diana-Albelda, Roberto Alcover-Couso, Ãlvaro GarcÃ­a-MartÃ­n, Jesus Bescos, Marcos Escudero-ViÃ±olo</strong></p>
<p>Gliomas are brain tumours that stand out for their highly lethal and aggressive nature, which demands a precise approach in their diagnosis. Medical image segmentation plays a crucial role in the evaluation and follow-up of these tumours, allowing specialists to analyse their morphology. However, existing methods for automatic glioma segmentation often lack generalization capability across other brain tumour domains, require extensive computational resources, or fail to fully utilize the multi-parametric MRI (mp-MRI) data used to delineate them. In this work, we introduce GBT-SAM, a novel Generalizable Brain Tumour (GBT) framework that extends the Segment Anything Model (SAM) to brain tumour segmentation tasks. Our method employs a two-step training protocol: first, fine-tuning the patch embedding layer to process the entire mp-MRI modalities, and second, incorporating parameter-efficient LoRA blocks and a Depth-Condition block into the Vision Transformer (ViT) to capture inter-slice correlations. GBT-SAM achieves state-of-the-art performance on the Adult Glioma dataset (Dice Score of $93.54$) while demonstrating robust generalization across Meningioma, Pediatric Glioma, and Sub-Saharan Glioma datasets. Furthermore, GBT-SAM uses less than 6.5M trainable parameters, thus offering an efficient solution for brain tumour segmentation. \ Our code and models are available at <a target="_blank" rel="noopener" href="https://github.com/vpulab/med-sam-brain">https://github.com/vpulab/med-sam-brain</a> . </p>
<blockquote>
<p>èƒ¶è´¨ç˜¤æ˜¯çªå‡ºçš„è„‘è‚¿ç˜¤ï¼Œå…·æœ‰æé«˜çš„è‡´æ­»æ€§å’Œä¾µè¢­æ€§ï¼Œè¿™è¦æ±‚åœ¨è¯Šæ–­æ—¶é‡‡å–ç²¾ç¡®çš„æ–¹æ³•ã€‚åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨è¯„ä¼°å’Œè·Ÿè¸ªè¿™äº›è‚¿ç˜¤ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå…è®¸ä¸“å®¶åˆ†æå®ƒä»¬çš„å½¢æ€ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è‡ªåŠ¨èƒ¶è´¨ç˜¤åˆ†å‰²æ–¹æ³•å¾€å¾€ç¼ºä¹åœ¨å…¶ä»–è„‘è‚¿ç˜¤é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œæˆ–è€…æœªèƒ½å……åˆ†åˆ©ç”¨ç”¨äºæç»˜å®ƒä»¬çš„å¤šå‚æ•°MRIï¼ˆmp-MRIï¼‰æ•°æ®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†GBT-SAMï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„é€šç”¨è„‘è‚¿ç˜¤ï¼ˆGBTï¼‰æ¡†æ¶ï¼Œå®ƒå°†Segment Anything Modelï¼ˆSAMï¼‰æ‰©å±•åˆ°è„‘è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä¸¤æ­¥è®­ç»ƒåè®®ï¼šé¦–å…ˆï¼Œå¾®è°ƒè¡¥ä¸åµŒå…¥å±‚ä»¥å¤„ç†æ•´ä¸ªmp-MRIæ¨¡å¼ï¼›å…¶æ¬¡ï¼Œåœ¨è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ä¸­èå…¥å‚æ•°é«˜æ•ˆçš„LoRAå—å’Œæ·±åº¦æ¡ä»¶å—ï¼Œä»¥æ•æ‰è·¨åˆ‡ç‰‡çš„ç›¸å…³æ€§ã€‚GBT-SAMåœ¨æˆäººèƒ¶è´¨ç˜¤æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆDiceå¾—åˆ†ä¸º93.54%ï¼‰ï¼ŒåŒæ—¶æ˜¾ç¤ºå‡ºåœ¨è„‘è†œç˜¤ã€å„¿ç«¥èƒ¶è´¨ç˜¤å’Œæ’’å“ˆæ‹‰ä»¥å—èƒ¶è´¨ç˜¤æ•°æ®é›†ä¸Šçš„ç¨³å¥æ³›åŒ–ã€‚æ­¤å¤–ï¼ŒGBT-SAMä½¿ç”¨çš„å¯è®­ç»ƒå‚æ•°å°‘äº650ä¸‡ï¼Œå› æ­¤ä¸ºè„‘è‚¿ç˜¤åˆ†å‰²æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/vpulab/med-sam-brain%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/vpulab/med-sam-brainä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04325v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹é€šç”¨è„‘è‚¿ç˜¤ï¼ˆGBTï¼‰åˆ†å‰²æ¡†æ¶GBT-SAMï¼Œå®ƒæ‰©å±•äº†Segment Anything Modelï¼ˆSAMï¼‰ä»¥è¿›è¡Œè„‘è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤æ­¥è®­ç»ƒåè®®ï¼Œé¦–å…ˆå¾®è°ƒè¡¥ä¸åµŒå…¥å±‚ä»¥å¤„ç†æ•´ä¸ªå¤šå‚æ•°MRIï¼ˆmp-MRIï¼‰æ¨¡å¼ï¼Œå…¶æ¬¡åœ¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ä¸­èå…¥å‚æ•°é«˜æ•ˆçš„LoRAå—å’Œæ·±åº¦æ¡ä»¶å—ä»¥æ•è·åˆ‡ç‰‡é—´çš„ç›¸å…³æ€§ã€‚GBT-SAMåœ¨æˆäººèƒ¶è´¨ç˜¤æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆDiceå¾—åˆ†ä¸º93.54%ï¼‰ï¼Œå¹¶åœ¨è„‘è†œç˜¤ã€å„¿ç«¥èƒ¶è´¨ç˜¤å’Œæ’’å“ˆæ‹‰ä»¥å—èƒ¶è´¨ç˜¤æ•°æ®é›†ä¸Šè¡¨ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒGBT-SAMä½¿ç”¨çš„å¯è®­ç»ƒå‚æ•°å°‘äº650ä¸‡ï¼Œä¸ºè„‘è‚¿ç˜¤åˆ†å‰²æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GBT-SAMæ˜¯ä¸€ç§é’ˆå¯¹è„‘è‚¿ç˜¤åˆ†å‰²çš„æ–°å‹æ¡†æ¶ï¼ŒåŸºäºSegment Anything Modelï¼ˆSAMï¼‰ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤æ­¥è®­ç»ƒåè®®ï¼ŒåŒ…æ‹¬å¾®è°ƒè¡¥ä¸åµŒå…¥å±‚å’Œèå…¥å‚æ•°é«˜æ•ˆçš„LoRAå—åŠæ·±åº¦æ¡ä»¶å—ã€‚</li>
<li>GBT-SAMåœ¨æˆäººèƒ¶è´¨ç˜¤æ•°æ®é›†ä¸Šå–å¾—é«˜Diceå¾—åˆ†ï¼Œå¹¶å±•ç¤ºäº†å¯¹ä¸åŒç±»å‹è„‘è‚¿ç˜¤çš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>GBT-SAMä½¿ç”¨çš„å¯è®­ç»ƒå‚æ•°è¾ƒå°‘ï¼Œä¸ºè„‘è‚¿ç˜¤åˆ†å‰²æä¾›äº†é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚</li>
<li>GBT-SAMæ–¹æ³•å……åˆ†åˆ©ç”¨äº†å¤šå‚æ•°MRIæ•°æ®ï¼Œæé«˜äº†è„‘è‚¿ç˜¤åˆ†å‰²çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04325">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-885a149fbc4607f5038c307daf347ff7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-750f89e2323cf1001dcd372ff31c1a77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5966b058c28bc67abfd53731528eb601.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfe813f87d2ab035ad259e574c045a07.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Generating-Novel-Brain-Morphology-by-Deforming-Learned-Templates"><a href="#Generating-Novel-Brain-Morphology-by-Deforming-Learned-Templates" class="headerlink" title="Generating Novel Brain Morphology by Deforming Learned Templates"></a>Generating Novel Brain Morphology by Deforming Learned Templates</h2><p><strong>Authors:Alan Q. Wang, Fangrui Huang, Bailey Trang, Wei Peng, Mohammad Abbasi, Kilian Pohl, Mert Sabuncu, Ehsan Adeli</strong></p>
<p>Designing generative models for 3D structural brain MRI that synthesize morphologically-plausible and attribute-specific (e.g., age, sex, disease state) samples is an active area of research. Existing approaches based on frameworks like GANs or diffusion models synthesize the image directly, which may limit their ability to capture intricate morphological details. In this work, we propose a 3D brain MRI generation method based on state-of-the-art latent diffusion models (LDMs), called MorphLDM, that generates novel images by applying synthesized deformation fields to a learned template. Instead of using a reconstruction-based autoencoder (as in a typical LDM), our encoder outputs a latent embedding derived from both an image and a learned template that is itself the output of a template decoder; this latent is passed to a deformation field decoder, whose output is applied to the learned template. A registration loss is minimized between the original image and the deformed template with respect to the encoder and both decoders. Empirically, our approach outperforms generative baselines on metrics spanning image diversity, adherence with respect to input conditions, and voxel-based morphometry. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/alanqrwang/morphldm">https://github.com/alanqrwang/morphldm</a>. </p>
<blockquote>
<p>è®¾è®¡ç”¨äºåˆæˆå½¢æ€ä¸Šåˆç†ä¸”å…·å¤‡ç‰¹å®šå±æ€§ï¼ˆä¾‹å¦‚å¹´é¾„ã€æ€§åˆ«ã€ç–¾ç—…çŠ¶æ€ï¼‰æ ·æœ¬çš„3Dç»“æ„è„‘MRIç”Ÿæˆæ¨¡å‹æ˜¯ä¸€ä¸ªçƒ­é—¨ç ”ç©¶é¢†åŸŸã€‚ç°æœ‰åŸºäºGANæˆ–æ‰©æ•£æ¨¡å‹ç­‰æ¡†æ¶çš„æ–¹æ³•ç›´æ¥åˆæˆå›¾åƒï¼Œè¿™å¯èƒ½é™åˆ¶äº†å®ƒä»¬åœ¨æ•æ‰å¤æ‚å½¢æ€ç»†èŠ‚æ–¹é¢çš„èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæœ€æ–°æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„3Dè„‘MRIç”Ÿæˆæ–¹æ³•ï¼Œç§°ä¸ºMorphLDMã€‚è¯¥æ–¹æ³•é€šè¿‡åº”ç”¨åˆæˆå˜å½¢åœºäºå·²å­¦ä¹ æ¨¡æ¿æ¥ç”Ÿæˆæ–°å‹å›¾åƒã€‚ä¸å…¶ä»–ä½¿ç”¨åŸºäºé‡å»ºçš„è‡ªç¼–ç å™¨ï¼ˆå¦‚å…¸å‹LDMä¸­çš„ï¼‰ä¸åŒï¼Œæˆ‘ä»¬çš„ç¼–ç å™¨è¾“å‡ºä¸€ä¸ªæ½œåœ¨åµŒå…¥ï¼Œè¯¥åµŒå…¥æ¥æºäºå›¾åƒå’Œå·²å­¦ä¹ æ¨¡æ¿ï¼Œè€Œè¯¥æ¨¡æ¿æœ¬èº«ä¹Ÿæ˜¯æ¨¡æ¿è§£ç å™¨çš„è¾“å‡ºï¼›æ­¤æ½œåœ¨åµŒå…¥è¢«ä¼ é€’ç»™å˜å½¢åœºè§£ç å™¨ï¼Œå…¶è¾“å‡ºåº”ç”¨äºå·²å­¦ä¹ æ¨¡æ¿ã€‚é€šè¿‡æœ€å°åŒ–åŸå§‹å›¾åƒå’Œå˜å½¢æ¨¡æ¿ä¹‹é—´çš„æ³¨å†ŒæŸå¤±ï¼Œå…³äºç¼–ç å™¨å’Œä¸¤ä¸ªè§£ç å™¨çš„æŸå¤±éƒ½ä¼šè¢«ä¼˜åŒ–ã€‚ä»å®è¯ç»“æœæ¥çœ‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒå¤šæ ·æ€§ã€ç¬¦åˆè¾“å…¥æ¡ä»¶ä»¥åŠåŸºäºä½“ç´ çš„å½¢æ€æµ‹é‡ç­‰æŒ‡æ ‡ä¸Šçš„è¡¨ç°å‡è¶…è¿‡äº†åŸºå‡†ç”Ÿæˆæ¨¡å‹ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/alanqrwang/morphldm%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/alanqrwang/morphldmæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03778v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœ€æ–°æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„3Dè„‘MRIç”Ÿæˆæ–¹æ³•MorphLDMï¼Œé€šè¿‡åº”ç”¨åˆæˆå˜å½¢åœºäºå­¦ä¹ æ¨¡æ¿ä»¥ç”Ÿæˆæ–°å›¾åƒã€‚ç›¸è¾ƒäºä¼ ç»ŸLDMä½¿ç”¨é‡å»ºè‡ªç¼–ç å™¨ï¼ŒMorphLDMçš„ç¼–ç å™¨è¾“å‡ºä¸€ä¸ªç”±å›¾åƒå’Œå­¦ä¹ æ¨¡æ¿å…±åŒå¾—å‡ºçš„æ½œåœ¨åµŒå…¥ï¼Œç„¶åä¼ é€’ç»™å˜å½¢åœºè§£ç å™¨ï¼Œå…¶è¾“å‡ºåº”ç”¨äºå­¦ä¹ æ¨¡æ¿ã€‚é€šè¿‡æœ€å°åŒ–åŸå§‹å›¾åƒä¸å˜å½¢æ¨¡æ¿ä¹‹é—´çš„æ³¨å†ŒæŸå¤±ï¼Œä»¥æé«˜å›¾åƒå¤šæ ·æ€§ã€ç¬¦åˆè¾“å…¥æ¡ä»¶ä»¥åŠåŸºäºä½“ç´ çš„å½¢æ€æµ‹é‡ç­‰æŒ‡æ ‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç ”ç©¶é¢†åŸŸï¼šè¯¥ç ”ç©¶å…³æ³¨äºè®¾è®¡ç”¨äº3Dç»“æ„è„‘MRIçš„ç”Ÿæˆæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿåˆæˆå½¢æ€ä¸Šåˆç†ä¸”å…·å¤‡ç‰¹å®šå±æ€§ï¼ˆå¦‚å¹´é¾„ã€æ€§åˆ«ã€ç–¾ç—…çŠ¶æ€ï¼‰çš„æ ·æœ¬ã€‚</li>
<li>æ–¹æ³•åˆ›æ–°ï¼šæå‡ºäº†ä¸€ç§åŸºäºæœ€æ–°æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„3Dè„‘MRIç”Ÿæˆæ–¹æ³•MorphLDMã€‚ä¸åŒäºä¼ ç»Ÿç›´æ¥åˆæˆå›¾åƒçš„æ–¹æ³•ï¼ŒMorphLDMé€šè¿‡åº”ç”¨åˆæˆå˜å½¢åœºåˆ°å­¦ä¹ æ¨¡æ¿æ¥ç”Ÿæˆæ–°å›¾åƒã€‚</li>
<li>ç¼–ç å™¨çš„ç‰¹ç‚¹ï¼šMorphLDMçš„ç¼–ç å™¨è¾“å‡ºä¸€ä¸ªç”±å›¾åƒå’Œå­¦ä¹ æ¨¡æ¿å…±åŒäº§ç”Ÿçš„æ½œåœ¨åµŒå…¥ã€‚å­¦ä¹ æ¨¡æ¿æœ¬èº«æ˜¯æ¨¡æ¿è§£ç å™¨çš„è¾“å‡ºã€‚</li>
<li>å˜å½¢åœºå’Œæ³¨å†ŒæŸå¤±ï¼šé€šè¿‡æœ€å°åŒ–åŸå§‹å›¾åƒå’Œå˜å½¢æ¨¡æ¿ä¹‹é—´çš„æ³¨å†ŒæŸå¤±ï¼Œä»¥æé«˜å›¾åƒåˆæˆçš„è´¨é‡ã€‚è¿™ä¸€æŸå¤±æ¶‰åŠåˆ°ç¼–ç å™¨å’Œä¸¤ä¸ªè§£ç å™¨ã€‚</li>
<li>æ€§èƒ½è¯„ä¼°ï¼šè¯¥æ–¹æ³•çš„æ€§èƒ½é€šè¿‡ä¸€ç³»åˆ—æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬å›¾åƒå¤šæ ·æ€§ã€ç¬¦åˆè¾“å…¥æ¡ä»¶çš„èƒ½åŠ›ä»¥åŠåŸºäºä½“ç´ çš„å½¢æ€æµ‹é‡ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>ä»£ç å…±äº«ï¼šç ”ç©¶å›¢é˜Ÿå·²å°†ç›¸å…³ä»£ç ä¸Šä¼ è‡³GitHubï¼ˆé“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/alanqrwang/morphldm%EF%BC%89%E3%80%82">https://github.com/alanqrwang/morphldmï¼‰ã€‚</a></li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03778">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e75b20c191401cf5d0bfb5fe79cf755d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5592fa3279ad2dff6283f3ba0f8e793.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f1fb47401ef9a0efd5831d0c1822cc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c30907be9e88147d3a507b7df4033fca.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Multi-Knowledge-oriented-Nighttime-Haze-Imaging-Enhancer-for-Vision-driven-Intelligent-Systems"><a href="#Multi-Knowledge-oriented-Nighttime-Haze-Imaging-Enhancer-for-Vision-driven-Intelligent-Systems" class="headerlink" title="Multi-Knowledge-oriented Nighttime Haze Imaging Enhancer for   Vision-driven Intelligent Systems"></a>Multi-Knowledge-oriented Nighttime Haze Imaging Enhancer for   Vision-driven Intelligent Systems</h2><p><strong>Authors:Ai Chen, Yuxu Lu, Dong Yang, Junlin Zhou, Yan Fu, Duanbing Chen</strong></p>
<p>Salient object detection (SOD) plays a critical role in vision-driven measurement systems (VMS), facilitating the detection and segmentation of key visual elements in an image. However, adverse imaging conditions such as haze during the day, low light, and haze at night severely degrade image quality, and complicating the SOD process. To address these challenges, we propose a multi-task-oriented nighttime haze imaging enhancer (MToIE), which integrates three tasks: daytime dehazing, low-light enhancement, and nighttime dehazing. The MToIE incorporates two key innovative components: First, the network employs a task-oriented node learning mechanism to handle three specific degradation types: day-time haze, low light, and night-time haze conditions, with an embedded self-attention module enhancing its performance in nighttime imaging. In addition, multi-receptive field enhancement module that efficiently extracts multi-scale features through three parallel depthwise separable convolution branches with different dilation rates, capturing comprehensive spatial information with minimal computational overhead. To ensure optimal image reconstruction quality and visual characteristics, we suggest a hybrid loss function. Extensive experiments on different types of weather&#x2F;imaging conditions illustrate that MToIE surpasses existing methods, significantly enhancing the accuracy and reliability of vision systems across diverse imaging scenarios. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Ai-Chen-Lab/MKoIE">https://github.com/Ai-Chen-Lab/MKoIE</a>. </p>
<blockquote>
<p>æ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹ï¼ˆSODï¼‰åœ¨è§†è§‰é©±åŠ¨æµ‹é‡ç³»ç»Ÿï¼ˆVMSï¼‰ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå®ƒä¿ƒè¿›äº†å›¾åƒä¸­å…³é”®è§†è§‰å…ƒç´ çš„æ£€æµ‹å’Œåˆ†å‰²ã€‚ç„¶è€Œï¼Œä¸åˆ©çš„æˆåƒæ¡ä»¶ï¼Œå¦‚ç™½å¤©çš„é›¾éœ¾ã€ä½å…‰ç…§å’Œå¤œæ™šçš„é›¾éœ¾ï¼Œä¸¥é‡é™ä½äº†å›¾åƒè´¨é‡ï¼Œå¹¶ä½¿å¾—SODè¿‡ç¨‹å¤æ‚åŒ–ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡çš„å¤œé—´é›¾éœ¾æˆåƒå¢å¼ºå™¨ï¼ˆMToIEï¼‰ï¼Œå®ƒç»“åˆäº†ä¸‰é¡¹ä»»åŠ¡ï¼šç™½å¤©å»é›¾ã€ä½å…‰å¢å¼ºå’Œå¤œé—´å»é›¾ã€‚MToIEåŒ…å«ä¸¤ä¸ªå…³é”®çš„åˆ›æ–°ç»„ä»¶ï¼šé¦–å…ˆï¼Œç½‘ç»œé‡‡ç”¨ä»»åŠ¡å¯¼å‘çš„èŠ‚ç‚¹å­¦ä¹ æœºåˆ¶æ¥å¤„ç†ä¸‰ç§ç‰¹å®šçš„é€€åŒ–ç±»å‹ï¼šç™½å¤©é›¾éœ¾ã€ä½å…‰å’Œå¤œé—´é›¾éœ¾æ¡ä»¶ï¼Œå…¶ä¸­åµŒå…¥çš„è‡ªæ³¨æ„æ¨¡å—å¢å¼ºäº†å…¶åœ¨å¤œé—´æˆåƒçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¤šæ„Ÿå—é‡å¢å¼ºæ¨¡å—é€šè¿‡ä¸‰ä¸ªå¹¶è¡Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯åˆ†æ”¯æœ‰æ•ˆåœ°æå–å¤šå°ºåº¦ç‰¹å¾ï¼Œè¿™äº›åˆ†æ”¯å…·æœ‰ä¸åŒçš„è†¨èƒ€ç‡ï¼Œä»¥æœ€å°çš„è®¡ç®—å¼€é”€æ•è·å…¨é¢çš„ç©ºé—´ä¿¡æ¯ã€‚ä¸ºäº†ç¡®ä¿æœ€ä½³çš„å›¾åƒé‡å»ºè´¨é‡å’Œè§†è§‰ç‰¹æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ··åˆæŸå¤±å‡½æ•°ã€‚åœ¨ä¸åŒå¤©æ°”&#x2F;æˆåƒæ¡ä»¶ä¸‹çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMToIEè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†ä¸åŒæˆåƒåœºæ™¯ä¸­è§†è§‰ç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Ai-Chen-Lab/MKoIE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Ai-Chen-Lab/MKoIEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07351v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹æ—¥é—´é›¾éœ¾ã€ä½å…‰ç…§å’Œå¤œé—´é›¾éœ¾ç­‰æ¶åŠ£æˆåƒæ¡ä»¶å¯¹è§†è§‰ç³»ç»Ÿçš„å½±å“ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡å¯¼å‘çš„å¤œé—´é›¾éœ¾æˆåƒå¢å¼ºå™¨ï¼ˆMToIEï¼‰ã€‚å®ƒé€šè¿‡é›†æˆæ—¥é—´å»é›¾ã€ä½å…‰å¢å¼ºå’Œå¤œé—´å»é›¾ä¸‰é¡¹ä»»åŠ¡ï¼Œé‡‡ç”¨ä»»åŠ¡å¯¼å‘èŠ‚ç‚¹å­¦ä¹ æœºåˆ¶å’Œè‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œå¹¶ç»“åˆå¤šæ„Ÿå—é‡å¢å¼ºæ¨¡å—å’Œæ··åˆæŸå¤±å‡½æ•°ï¼Œæé«˜äº†æˆåƒè´¨é‡ï¼Œæ˜¾è‘—æå‡äº†è§†è§‰ç³»ç»Ÿåœ¨å¤šç§æˆåƒåœºæ™¯ä¸‹çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Salient object detection (SOD)åœ¨æ¶åŠ£æˆåƒæ¡ä»¶ä¸‹ï¼ˆå¦‚æ—¥é—´é›¾éœ¾ã€ä½å…‰ç…§å’Œå¤œé—´é›¾éœ¾ï¼‰é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡å¯¼å‘çš„å¤œé—´é›¾éœ¾æˆåƒå¢å¼ºå™¨ï¼ˆMToIEï¼‰ï¼Œé›†æˆäº†ä¸‰é¡¹ä»»åŠ¡ï¼šæ—¥é—´å»é›¾ã€ä½å…‰å¢å¼ºå’Œå¤œé—´å»é›¾ã€‚</li>
<li>MToIEé‡‡ç”¨ä»»åŠ¡å¯¼å‘èŠ‚ç‚¹å­¦ä¹ æœºåˆ¶å’Œè‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥å¤„ç†ä¸åŒç±»å‹çš„æ¶åŠ£æˆåƒæ¡ä»¶å¹¶å¢å¼ºå¤œé—´æˆåƒæ€§èƒ½ã€‚</li>
<li>MToIEå¼•å…¥äº†å¤šæ„Ÿå—é‡å¢å¼ºæ¨¡å—ï¼Œé€šè¿‡ä¸‰ä¸ªå…·æœ‰ä¸åŒè†¨èƒ€ç‡çš„å¹¶è¡Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯åˆ†æ”¯æ¥æœ‰æ•ˆåœ°æå–å¤šå°ºåº¦ç‰¹å¾ï¼Œä»è€Œæ•è·å…¨é¢çš„ç©ºé—´ä¿¡æ¯å¹¶ä¿æŒè¾ƒä½çš„è®¡ç®—å¼€é”€ã€‚</li>
<li>ä¸ºäº†ç¡®ä¿å›¾åƒé‡å»ºè´¨é‡å’Œè§†è§‰ç‰¹æ€§ï¼Œå»ºè®®ä½¿ç”¨æ··åˆæŸå¤±å‡½æ•°ã€‚</li>
<li>å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒMToIEåœ¨å¤šç§å¤©æ°”&#x2F;æˆåƒæ¡ä»¶ä¸‹è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†è§†è§‰ç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
<li>ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/Ai-Chen-Lab/MKoIE%EF%BC%89%E3%80%82">https://github.com/Ai-Chen-Lab/MKoIEï¼‰ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7b856d7dd6668ea6d70f98f2f9a0b793.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b6871fd4c13259e51b7c446c98f8a0a.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Quantum-Down-Sampling-Filter-for-Variational-Auto-encoder"><a href="#Quantum-Down-Sampling-Filter-for-Variational-Auto-encoder" class="headerlink" title="Quantum Down Sampling Filter for Variational Auto-encoder"></a>Quantum Down Sampling Filter for Variational Auto-encoder</h2><p><strong>Authors:Farina Riaz, Fakhar Zaman, Hajime Suzuki, Sharif Abuadbba, David Nguyen</strong></p>
<p>Variational autoencoders (VAEs) are fundamental for generative modeling and image reconstruction, yet their performance often struggles to maintain high fidelity in reconstructions. This study introduces a hybrid model, quantum variational autoencoder (Q-VAE), which integrates quantum encoding within the encoder while utilizing fully connected layers to extract meaningful representations. The decoder uses transposed convolution layers for up-sampling. The Q-VAE is evaluated against the classical VAE and the classical direct-passing VAE, which utilizes windowed pooling filters. Results on the MNIST and USPS datasets demonstrate that Q-VAE consistently outperforms classical approaches, achieving lower Fr&#39;echet inception distance scores, thereby indicating superior image fidelity and enhanced reconstruction quality. These findings highlight the potential of Q-VAE for high-quality synthetic data generation and improved image reconstruction in generative models. </p>
<blockquote>
<p>å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰åœ¨ç”Ÿæˆå»ºæ¨¡å’Œå›¾åƒé‡å»ºä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œä½†å…¶æ€§èƒ½åœ¨ä¿æŒé«˜ä¿çœŸé‡å»ºæ–¹é¢å¾€å¾€å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ··åˆæ¨¡å‹ï¼Œå³é‡å­å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆQ-VAEï¼‰ï¼Œå®ƒåœ¨ç¼–ç å™¨å†…é›†æˆäº†é‡å­ç¼–ç ï¼ŒåŒæ—¶ä½¿ç”¨å…¨è¿æ¥å±‚æå–æœ‰æ„ä¹‰çš„è¡¨ç¤ºã€‚è§£ç å™¨ä½¿ç”¨è½¬ç½®å·ç§¯å±‚è¿›è¡Œä¸Šé‡‡æ ·ã€‚Q-VAEä¸ç»å…¸VAEå’Œç»å…¸ç›´é€šVAEï¼ˆä½¿ç”¨çª—å£æ± åŒ–æ»¤æ³¢å™¨ï¼‰è¿›è¡Œäº†è¯„ä¼°æ¯”è¾ƒã€‚åœ¨MNISTå’ŒUSPSæ•°æ®é›†ä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒQ-VAEå§‹ç»ˆä¼˜äºç»å…¸æ–¹æ³•ï¼Œå®ç°äº†æ›´ä½çš„FrÃ©chet inceptionè·ç¦»å¾—åˆ†ï¼Œä»è€Œè¡¨æ˜å…¶å›¾åƒä¿çœŸåº¦æ›´é«˜ï¼Œé‡å»ºè´¨é‡å¢å¼ºã€‚è¿™äº›å‘ç°çªå‡ºäº†Q-VAEåœ¨é«˜è´¨é‡åˆæˆæ•°æ®ç”Ÿæˆå’Œç”Ÿæˆæ¨¡å‹ä¸­æ”¹è¿›å›¾åƒé‡å»ºçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.06259v3">PDF</a> 18 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é‡å­å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆQ-VAEï¼‰çš„ç ”ç©¶ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†é‡å­ç¼–ç æŠ€æœ¯çš„æ··åˆæ¨¡å‹ã€‚ç›¸è¾ƒäºä¼ ç»Ÿçš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å’Œç›´æ¥ä¼ é€’çš„VAEï¼ŒQ-VAEåœ¨æå–æœ‰æ„ä¹‰è¡¨ç¤ºæ–¹é¢è¡¨ç°æ›´ä½³ï¼Œä½¿ç”¨äº†è½¬ç½®å·ç§¯å±‚è¿›è¡Œä¸Šé‡‡æ ·ã€‚åœ¨MNISTå’ŒUSPSæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒQ-VAEå…·æœ‰æ›´ä½çš„FrÃ©chet inceptionè·ç¦»å¾—åˆ†ï¼Œè¡¨æ˜å…¶å›¾åƒä¿çœŸåº¦å’Œé‡å»ºè´¨é‡æ›´é«˜ï¼Œå±•ç°å‡ºå…¶åœ¨é«˜è´¨é‡åˆæˆæ•°æ®ç”Ÿæˆå’Œæ”¹è¿›ç”Ÿæˆæ¨¡å‹ä¸­çš„å›¾åƒé‡å»ºæ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡å­å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆQ-VAEï¼‰ç»“åˆäº†é‡å­ç¼–ç æŠ€æœ¯ä¸å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ã€‚</li>
<li>Q-VAEä½¿ç”¨å…¨è¿æ¥å±‚æå–æœ‰æ„ä¹‰çš„è¡¨ç¤ºã€‚</li>
<li>è§£ç å™¨é‡‡ç”¨è½¬ç½®å·ç§¯å±‚è¿›è¡Œä¸Šé‡‡æ ·ã€‚</li>
<li>åœ¨MNISTå’ŒUSPSæ•°æ®é›†ä¸Šï¼ŒQ-VAEç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>Q-VAEå…·æœ‰æ›´ä½çš„FrÃ©chet inceptionè·ç¦»å¾—åˆ†ï¼Œè¡¨æ˜å…¶å›¾åƒé‡å»ºè´¨é‡æ›´é«˜ã€‚</li>
<li>Q-VAEå…·æœ‰æ½œåŠ›ç”¨äºé«˜è´¨é‡åˆæˆæ•°æ®ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.06259">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2b078b3f6b6766276e35aaea0a4ce5b6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9a24d52cc1d3966ff9a678733468e83c.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-11/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-11/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-11/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ab19c577adcf2b40424c51e20a8b8ca6.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-11  DiVISe Direct Visual-Input Speech Synthesis Preserving Speaker   Characteristics And Intelligibility
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-11/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-63e680a30df0d6460edaab10aa63b1e9.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-11  AIM-Fair Advancing Algorithmic Fairness via Selectively Fine-Tuning   Biased Models with Contextual Synthetic Data
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24231k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
