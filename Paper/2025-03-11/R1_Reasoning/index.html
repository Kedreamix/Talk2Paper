<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-11  Symbolic Mixture-of-Experts Adaptive Skill-based Routing for   Heterogeneous Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-94055dde32c30802ccdd214b8a84c796.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    75 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-11-æ›´æ–°"><a href="#2025-03-11-æ›´æ–°" class="headerlink" title="2025-03-11 æ›´æ–°"></a>2025-03-11 æ›´æ–°</h1><h2 id="Symbolic-Mixture-of-Experts-Adaptive-Skill-based-Routing-for-Heterogeneous-Reasoning"><a href="#Symbolic-Mixture-of-Experts-Adaptive-Skill-based-Routing-for-Heterogeneous-Reasoning" class="headerlink" title="Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for   Heterogeneous Reasoning"></a>Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for   Heterogeneous Reasoning</h2><p><strong>Authors:Justin Chih-Yao Chen, Sukwon Yun, Elias Stengel-Eskin, Tianlong Chen, Mohit Bansal</strong></p>
<p>Combining existing pre-trained expert LLMs is a promising avenue for scalably tackling large-scale and diverse tasks. However, selecting experts at the task level is often too coarse-grained, as heterogeneous tasks may require different expertise for each instance. To enable adaptive instance-level mixing of pre-trained LLM experts, we propose Symbolic-MoE, a symbolic, text-based, and gradient-free Mixture-of-Experts framework. Symbolic-MoE takes a fine-grained approach to selection by emphasizing skills, e.g., algebra in math or molecular biology in biomedical reasoning. We propose a skill-based recruiting strategy that dynamically selects the most relevant set of expert LLMs for diverse reasoning tasks based on their strengths. Each selected expert then generates its own reasoning, resulting in k outputs from k experts, which are then synthesized into a final high-quality response by an aggregator chosen based on its ability to integrate diverse reasoning outputs. We show that Symbolic-MoEâ€™s instance-level expert selection improves performance by a large margin but â€“ when implemented naively â€“ can introduce a high computational overhead due to the need for constant model loading and offloading. To address this, we implement a batch inference strategy that groups instances based on their assigned experts, loading each model only once. This allows us to integrate 16 expert models on 1 GPU with a time cost comparable to or better than prior multi-agent baselines using 4 GPUs. Through extensive evaluations on diverse benchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we demonstrate that Symbolic-MoE outperforms strong LLMs like GPT4o-mini, as well as multi-agent approaches, with an absolute average improvement of 8.15% over the best multi-agent baseline. Moreover, Symbolic-MoE removes the need for expensive multi-round discussions, outperforming discussion baselines with less computation. </p>
<blockquote>
<p>ç»“åˆç°æœ‰çš„é¢„è®­ç»ƒä¸“å®¶LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰ä¸ºå¤§è§„æ¨¡å’Œå¤šæ ·åŒ–ä»»åŠ¡çš„å¯æ‰©å±•å¤„ç†å¸¦æ¥äº†å¸Œæœ›ã€‚ç„¶è€Œï¼Œåœ¨ä»»åŠ¡å±‚é¢é€‰æ‹©ä¸“å®¶é€šå¸¸å¤ªè¿‡ç²—ç³™ï¼Œå› ä¸ºä¸åŒçš„ä»»åŠ¡å®ä¾‹å¯èƒ½éœ€è¦ä¸åŒçš„ä¸“ä¸šçŸ¥è¯†ã€‚ä¸ºäº†å®ç°å¯¹é¢„è®­ç»ƒLLMä¸“å®¶çš„è‡ªé€‚åº”å®ä¾‹çº§æ··åˆï¼Œæˆ‘ä»¬æå‡ºäº†Symbolic-MoEï¼Œä¸€ä¸ªåŸºäºç¬¦å·ã€æ–‡æœ¬å’Œæ— æ¢¯åº¦çš„æ··åˆä¸“å®¶æ¡†æ¶ã€‚Symbolic-MoEé€šè¿‡å¼ºè°ƒæŠ€èƒ½æ¥é‡‡å–ç²¾ç»†ç²’åº¦çš„é€‰æ‹©æ–¹æ³•ï¼Œä¾‹å¦‚æ•°å­¦ä¸­çš„ä»£æ•°æˆ–ç”Ÿç‰©åŒ»å­¦æ¨ç†ä¸­çš„åˆ†å­ç”Ÿç‰©å­¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæŠ€èƒ½çš„æ‹›è˜ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æ ¹æ®ä¸“å®¶çš„ä¼˜åŠ¿åŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„ä¸€ç»„ä¸“å®¶LLMæ¥å¤„ç†å¤šæ ·åŒ–çš„æ¨ç†ä»»åŠ¡ã€‚æ¯ä¸ªé€‰ä¸­çš„ä¸“å®¶ç„¶åç”Ÿæˆè‡ªå·±çš„æ¨ç†ç»“æœï¼Œäº§ç”Ÿæ¥è‡ªkä¸ªä¸“å®¶çš„kä¸ªè¾“å‡ºï¼Œç„¶åé€šè¿‡ä¸€ä¸ªèšåˆå™¨å°†å…¶ç»¼åˆæˆæœ€ç»ˆçš„é«˜è´¨é‡å“åº”ï¼Œèšåˆå™¨çš„é€‰æ‹©åŸºäºå…¶æ•´åˆå¤šç§æ¨ç†è¾“å‡ºçš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒSymbolic-MoEçš„å®ä¾‹çº§ä¸“å®¶é€‰æ‹©å¤§å¹…åº¦æé«˜äº†æ€§èƒ½ï¼Œä½†å¦‚æœå®æ–½å¾—è¿‡äºç®€å•ï¼Œç”±äºéœ€è¦ä¸æ–­çš„æ¨¡å‹åŠ è½½å’Œå¸è½½ï¼Œå¯èƒ½ä¼šå¼•å…¥å¾ˆé«˜çš„è®¡ç®—å¼€é”€ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ç§æ‰¹é‡æ¨ç†ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æ ¹æ®åˆ†é…çš„ä¸“å®¶å¯¹å®ä¾‹è¿›è¡Œåˆ†ç»„ï¼Œæ¯ä¸ªæ¨¡å‹åªåŠ è½½ä¸€æ¬¡ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨å•ä¸ªGPUä¸Šé›†æˆ16ä¸ªä¸“å®¶æ¨¡å‹ï¼Œæ—¶é—´æˆæœ¬å¯ä¸ä½¿ç”¨4ä¸ªGPUçš„å…ˆå‰å¤šä»£ç†åŸºçº¿ç›¸æ¯”ï¼Œç”šè‡³æ›´å¥½ã€‚é€šè¿‡åœ¨ä¸åŒåŸºå‡†æµ‹è¯•ï¼ˆMMLU-Proã€GPQAã€AIMEå’ŒMedMCQAï¼‰ä¸Šçš„å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜Symbolic-MoEåœ¨å¼ºå¤§çš„LLMï¼ˆå¦‚GPT4o miniï¼‰å’Œå¤šä»£ç†æ–¹æ³•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç»å¯¹å¹³å‡æé«˜äº†8.15%ï¼Œè¶…è¿‡äº†æœ€ä½³å¤šä»£ç†åŸºçº¿ã€‚æ­¤å¤–ï¼ŒSymbolic-MoEä¸éœ€è¦æ˜‚è´µçš„å¤šè½®è®¨è®ºï¼Œå…¶è¡¨ç°ä¼˜äºè®¡ç®—è¾ƒå°‘çš„è®¨è®ºåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05641v1">PDF</a> The first three authors contributed equally. Project Page:   <a target="_blank" rel="noopener" href="https://symbolic_moe.github.io/">https://symbolic_moe.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†ç»“åˆé¢„è®­ç»ƒä¸“å®¶LLMsçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡Symbolic-MoEæ¡†æ¶å®ç°è‡ªé€‚åº”å®ä¾‹çº§åˆ«çš„æ··åˆã€‚è¯¥æ–¹æ³•é‡‡ç”¨åŸºäºæŠ€èƒ½çš„ç²¾ç»†é€‰æ‹©ç­–ç•¥ï¼Œé’ˆå¯¹ä¸åŒç±»å‹çš„ä»»åŠ¡åŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„ä¸“å®¶LLMã€‚å®éªŒè¡¨æ˜ï¼ŒSymbolic-MoEçš„å®ä¾‹çº§åˆ«ä¸“å®¶é€‰æ‹©èƒ½å¤§å¹…æé«˜æ€§èƒ½ï¼Œä¸”é€šè¿‡æ‰¹é‡æ¨ç†ç­–ç•¥ä¼˜åŒ–è®¡ç®—å¼€é”€ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒSymbolic-MoEè¡¨ç°å‡ºä¼˜äºGPT4o-miniç­‰å¤šæ¨¡å‹æ–¹æ³•çš„æ•ˆæœï¼Œç»å¯¹å¹³å‡æå‡è¾¾8.15%ã€‚æ­¤å¤–ï¼ŒSymbolic-MoEæ— éœ€æ˜‚è´µçš„å¤šè½®è®¨è®ºï¼Œè®¡ç®—æ•ˆç‡æ›´é«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»“åˆé¢„è®­ç»ƒä¸“å®¶LLMsæ˜¯è§£å†³å¤§è§„æ¨¡å¤šæ ·ä»»åŠ¡çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>Symbolic-MoEæ¡†æ¶å®ç°è‡ªé€‚åº”å®ä¾‹çº§åˆ«çš„æ··åˆï¼Œé‡‡ç”¨åŸºäºæŠ€èƒ½çš„ç²¾ç»†é€‰æ‹©ç­–ç•¥ã€‚</li>
<li>åŸºäºæŠ€èƒ½çš„ç­–ç•¥èƒ½åŠ¨æ€é€‰æ‹©æœ€ç›¸å…³çš„ä¸“å®¶LLMä»¥åº”å¯¹ä¸åŒç±»å‹çš„ä»»åŠ¡ã€‚</li>
<li>Symbolic-MoEé€šè¿‡æ‰¹é‡æ¨ç†ç­–ç•¥ä¼˜åŒ–è®¡ç®—å¼€é”€ï¼Œèƒ½åœ¨å•GPUä¸Šé›†æˆå¤šä¸ªä¸“å®¶æ¨¡å‹ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒSymbolic-MoEè¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œä¼˜äºGPT4o-miniç­‰å¤šæ¨¡å‹æ–¹æ³•ã€‚</li>
<li>Symbolic-MoEçš„ç»å¯¹å¹³å‡æå‡è¾¾8.15%ï¼Œå±•ç°äº†å¼ºå¤§çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05641">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3aaf451a10b9fb61e5c5b6af2edd980.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c39293a7a96c5b27e6ec1cf7381cbb38.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f78ae42266ee002f5a13cb978c615778.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="R1-Searcher-Incentivizing-the-Search-Capability-in-LLMs-via-Reinforcement-Learning"><a href="#R1-Searcher-Incentivizing-the-Search-Capability-in-LLMs-via-Reinforcement-Learning" class="headerlink" title="R1-Searcher: Incentivizing the Search Capability in LLMs via   Reinforcement Learning"></a>R1-Searcher: Incentivizing the Search Capability in LLMs via   Reinforcement Learning</h2><p><strong>Authors:Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Ji-Rong Wen</strong></p>
<p>Existing Large Reasoning Models (LRMs) have shown the potential of reinforcement learning (RL) to enhance the complex reasoning capabilities of Large Language Models~(LLMs). While they achieve remarkable performance on challenging tasks such as mathematics and coding, they often rely on their internal knowledge to solve problems, which can be inadequate for time-sensitive or knowledge-intensive questions, leading to inaccuracies and hallucinations. To address this, we propose \textbf{R1-Searcher}, a novel two-stage outcome-based RL approach designed to enhance the search capabilities of LLMs. This method allows LLMs to autonomously invoke external search systems to access additional knowledge during the reasoning process. Our framework relies exclusively on RL, without requiring process rewards or distillation for a cold start. % effectively generalizing to out-of-domain datasets and supporting both Base and Instruct models. Our experiments demonstrate that our method significantly outperforms previous strong RAG methods, even when compared to the closed-source GPT-4o-mini. </p>
<blockquote>
<p>ç°æœ‰çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰å·²ç»æ˜¾ç¤ºå‡ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚è™½ç„¶ä»–ä»¬åœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆç»©ï¼Œä½†ä»–ä»¬å¾€å¾€ä¾èµ–äºå†…éƒ¨çŸ¥è¯†æ¥è§£å†³é—®é¢˜ï¼Œè¿™å¯¹äºæ—¶é—´æ•æ„Ÿæˆ–çŸ¥è¯†å¯†é›†çš„é—®é¢˜å¯èƒ½ä¸è¶³ä»¥è§£å†³é—®é¢˜ï¼Œå¯¼è‡´å‡†ç¡®æ€§å’Œå¹»æƒ³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>R1-Searcher</strong>ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µåŸºäºç»“æœçš„RLæ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºLLMçš„æœç´¢èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•å…è®¸LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»è°ƒç”¨å¤–éƒ¨æœç´¢ç³»ç»Ÿæ¥è®¿é—®é¢å¤–çš„çŸ¥è¯†ã€‚æˆ‘ä»¬çš„æ¡†æ¶å®Œå…¨ä¾èµ–äºRLï¼Œè€Œä¸éœ€è¦è¿‡ç¨‹å¥–åŠ±æˆ–å†·å¯åŠ¨æ—¶çš„è’¸é¦ã€‚%æœ‰æ•ˆåœ°æ³›åŒ–åˆ°åŸŸå¤–æ•°æ®é›†å¹¶æ”¯æŒBaseå’ŒInstructæ¨¡å‹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºä¹‹å‰çš„å¼ºå¤§RAGæ–¹æ³•ï¼Œå³ä½¿ä¸é—­æºçš„GPT-4o-miniç›¸æ¯”ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
</blockquote>
<p><strong>ç®€ä½“ä¸­æ–‡ç¿»è¯‘</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05592v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚è™½ç„¶ç°æœ‰æ¨¡å‹åœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å®ƒä»¬å¾€å¾€ä¾èµ–å†…éƒ¨çŸ¥è¯†è§£å†³é—®é¢˜ï¼Œå¯¹äºæ—¶é—´æ•æ„Ÿæˆ–çŸ¥è¯†å¯†é›†å‹é—®é¢˜å¯èƒ½ä¸è¶³å¤Ÿå‡†ç¡®ï¼Œç”šè‡³äº§ç”Ÿå¹»è§‰ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†R1-Searcherï¼Œä¸€ç§æ–°å‹ä¸¤é˜¶æ®µç»“æœå¯¼å‘çš„RLæ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºLLMçš„æœç´¢èƒ½åŠ›ã€‚è¯¥æ–¹æ³•å…è®¸LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»è°ƒç”¨å¤–éƒ¨æœç´¢ç³»ç»Ÿè·å–é¢å¤–çŸ¥è¯†ã€‚æˆ‘ä»¬çš„æ¡†æ¶å®Œå…¨ä¾èµ–äºRLï¼Œæ— éœ€è¿‡ç¨‹å¥–åŠ±æˆ–å†·å¯åŠ¨æ—¶çš„è’¸é¦ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºå…ˆå‰çš„å¼ºå¤§RAGæ–¹æ³•ï¼Œå³ä½¿ä¸é—­æºçš„GPT-4o-miniç›¸æ¯”ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>LRMsåœ¨è§£å†³æ•°å­¦å’Œç¼–ç¨‹ç­‰æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨æ—¶é—´æ•æ„Ÿæˆ–çŸ¥è¯†å¯†é›†å‹é—®é¢˜ä¸Šå¯èƒ½ä¸å‡†ç¡®ã€‚</li>
<li>R1-Searcheræ˜¯ä¸€ç§æ–°å‹ä¸¤é˜¶æ®µç»“æœå¯¼å‘çš„RLæ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºLLMçš„æœç´¢èƒ½åŠ›ã€‚</li>
<li>R1-Searcherå…è®¸LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»è°ƒç”¨å¤–éƒ¨æœç´¢ç³»ç»Ÿè·å–é¢å¤–çŸ¥è¯†ã€‚</li>
<li>R1-Searcheræ¡†æ¶å®Œå…¨ä¾èµ–äºå¼ºåŒ–å­¦ä¹ ï¼Œæ— éœ€è¿‡ç¨‹å¥–åŠ±æˆ–å†·å¯åŠ¨æ—¶çš„çŸ¥è¯†è’¸é¦ã€‚</li>
<li>å®éªŒè¯æ˜R1-Searcheræ–¹æ³•æ˜¾è‘—ä¼˜äºå…ˆå‰çš„RAGæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-aa0cce0eef49a56440a233b97ab14c00.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f962a90cf16a20b5690ab8dd46334a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-603d2edf70630ae009a368b957e8eadc.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="An-Empirical-Study-of-Conformal-Prediction-in-LLM-with-ASP-Scaffolds-for-Robust-Reasoning"><a href="#An-Empirical-Study-of-Conformal-Prediction-in-LLM-with-ASP-Scaffolds-for-Robust-Reasoning" class="headerlink" title="An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for   Robust Reasoning"></a>An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for   Robust Reasoning</h2><p><strong>Authors:Navdeep Kaur, Lachlan McPheat, Alessandra Russo, Anthony G Cohn, Pranava Madhyastha</strong></p>
<p>In this paper, we examine the use of Conformal Language Modelling (CLM) alongside Answer Set Programming (ASP) to enhance the performance of standard open-weight LLMs on complex multi-step reasoning tasks. Using the StepGame dataset, which requires spatial reasoning, we apply CLM to generate sets of ASP programs from an LLM, providing statistical guarantees on the correctness of the outputs. Experimental results show that CLM significantly outperforms baseline models that use standard sampling methods, achieving substantial accuracy improvements across different levels of reasoning complexity. Additionally, the LLM-as-Judge metric enhances CLMâ€™s performance, especially in assessing structurally and logically correct ASP outputs. However, calibrating CLM with diverse calibration sets did not improve generalizability for tasks requiring much longer reasoning steps, indicating limitations in handling more complex tasks. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å°†Conformal Language Modellingï¼ˆCLMï¼‰ä¸Answer Set Programmingï¼ˆASPï¼‰ç»“åˆä½¿ç”¨ï¼Œä»¥æé«˜æ ‡å‡†å¼€æ”¾å¼æƒé‡çš„å¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬åˆ©ç”¨éœ€è¦ç©ºé—´æ¨ç†çš„StepGameæ•°æ®é›†ï¼Œåº”ç”¨CLMä»LLMç”ŸæˆASPç¨‹åºé›†ï¼Œä¸ºè¾“å‡ºç»“æœçš„æ­£ç¡®æ€§æä¾›ç»Ÿè®¡ä¿è¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸é‡‡ç”¨æ ‡å‡†é‡‡æ ·æ–¹æ³•çš„åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒCLMè¡¨ç°æ›´ä¼˜å¼‚ï¼Œå¹¶ä¸”åœ¨ä¸åŒçº§åˆ«çš„æ¨ç†å¤æ‚åº¦ä¸Šå®ç°äº†å®è´¨æ€§çš„å‡†ç¡®æ€§æå‡ã€‚æ­¤å¤–ï¼ŒLLM-as-JudgeæŒ‡æ ‡æé«˜äº†CLMçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯„ä¼°ç»“æ„ä¸Šå’Œé€»è¾‘ä¸Šæ­£ç¡®çš„ASPè¾“å‡ºæ–¹é¢ã€‚ç„¶è€Œï¼Œä½¿ç”¨å„ç§æ ¡å‡†é›†æ ¡å‡†CLMå¹¶æ²¡æœ‰æé«˜å¯¹éœ€è¦æ›´å¤šæ¨ç†æ­¥éª¤çš„ä»»åŠ¡çš„é€šç”¨æ€§ï¼Œè¿™è¡¨æ˜åœ¨å¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡æ—¶å­˜åœ¨å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05439v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å°†ç¬¦åˆè¯­è¨€å»ºæ¨¡ï¼ˆCLMï¼‰ä¸ç­”æ¡ˆé›†ç¼–ç¨‹ï¼ˆASPï¼‰ç›¸ç»“åˆï¼Œä»¥æé«˜æ ‡å‡†å¼€æ”¾æƒé‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚é€šè¿‡åº”ç”¨CLMç”ŸæˆASPç¨‹åºé›†ï¼Œå¯¹ç©ºé—´æ¨ç†è¦æ±‚çš„StepGameæ•°æ®é›†è¿›è¡Œç»Ÿè®¡ä¿è¯è¾“å‡ºçš„æ­£ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLMæ˜¾è‘—ä¼˜äºä½¿ç”¨æ ‡å‡†é‡‡æ ·æ–¹æ³•çš„åŸºçº¿æ¨¡å‹ï¼Œåœ¨ä¸åŒå±‚æ¬¡çš„æ¨ç†å¤æ‚åº¦ä¸Šå®ç°äº†å®è´¨æ€§çš„å‡†ç¡®æ€§æé«˜ã€‚ç„¶è€Œï¼Œå¯¹äºéœ€è¦æ›´é•¿æ—¶é—´æ¨ç†çš„æ­¥éª¤ï¼Œä½¿ç”¨å¤šç§æ ¡å‡†é›†æ ¡å‡†CLMå¹¶æ²¡æœ‰æé«˜å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„é€šç”¨æ€§ï¼Œè¡¨æ˜å…¶åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ–¹é¢çš„å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æœ¬æ–‡ç»“åˆConformal Language Modelling (CLM)ä¸Answer Set Programming (ASP)ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨StepGameæ•°æ®é›†å±•ç¤ºCLMç”ŸæˆASPç¨‹åºçš„èƒ½åŠ›ï¼Œå¹¶æä¾›äº†ç»Ÿè®¡ä¿è¯çš„æ­£ç¡®æ€§è¾“å‡ºã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCLMç›¸è¾ƒäºåŸºçº¿æ¨¡å‹åœ¨å‡†ç¡®æ€§ä¸Šæœ‰æ˜¾è‘—æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŒå±‚æ¬¡çš„æ¨ç†å¤æ‚åº¦ä¸Šã€‚</li>
<li>LLM-as-Judgeè¯„ä¼°æ–¹æ³•èƒ½æœ‰æ•ˆæå‡CLMçš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨è¯„ä¼°ASPè¾“å‡ºçš„ç»“æ„å’Œé€»è¾‘æ­£ç¡®æ€§æ–¹é¢ã€‚</li>
<li>å¯¹äºéœ€è¦æ›´é•¿æ¨ç†æ­¥éª¤çš„ä»»åŠ¡ï¼ŒCLMçš„é€šç”¨æ€§å­˜åœ¨å±€é™æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c20077726464f924923c42c27852f698.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Speculative-Decoding-for-Multi-Sample-Inference"><a href="#Speculative-Decoding-for-Multi-Sample-Inference" class="headerlink" title="Speculative Decoding for Multi-Sample Inference"></a>Speculative Decoding for Multi-Sample Inference</h2><p><strong>Authors:Yiwei Li, Jiayi Shi, Shaoxiong Feng, Peiwen Yuan, Xinglin Wang, Yueqi Zhang, Ji Zhang, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li</strong></p>
<p>We propose a novel speculative decoding method tailored for multi-sample reasoning scenarios, such as self-consistency and Best-of-N sampling. Our method exploits the intrinsic consensus of parallel generation paths to synthesize high-quality draft tokens without requiring auxiliary models or external databases. By dynamically analyzing structural patterns across parallel reasoning paths through a probabilistic aggregation mechanism, it identifies consensus token sequences that align with the decoding distribution. Evaluations on mathematical reasoning benchmarks demonstrate a substantial improvement in draft acceptance rates over baselines, while reducing the latency in draft token construction. This work establishes a paradigm shift for efficient multi-sample inference, enabling seamless integration of speculative decoding with sampling-based reasoning techniques. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤šæ ·æœ¬æ¨ç†åœºæ™¯ï¼ˆå¦‚è‡ªæ´½æ€§å’Œæœ€ä½³Né‡‡æ ·ï¼‰é‡èº«å®šåˆ¶çš„æ–°å‹æŠ•æœºè§£ç æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¹¶è¡Œç”Ÿæˆè·¯å¾„çš„å†…åœ¨ä¸€è‡´æ€§ï¼Œåˆæˆé«˜è´¨é‡è‰ç¨¿æ ‡è®°ï¼Œæ— éœ€è¾…åŠ©æ¨¡å‹æˆ–å¤–éƒ¨æ•°æ®åº“ã€‚å®ƒé€šè¿‡æ¦‚ç‡èšåˆæœºåˆ¶åŠ¨æ€åˆ†æå¹¶è¡Œæ¨ç†è·¯å¾„çš„ç»“æ„æ¨¡å¼ï¼Œè¯†åˆ«ä¸è§£ç åˆ†å¸ƒä¸€è‡´çš„å…±è¯†æ ‡è®°åºåˆ—ã€‚åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œè‰æ¡ˆæ¥å—ç‡æœ‰æ˜¾è‘—æé«˜ï¼ŒåŒæ—¶é™ä½äº†è‰ç¨¿æ ‡è®°æ„å»ºçš„å»¶è¿Ÿã€‚è¿™é¡¹å·¥ä½œå®ç°äº†é«˜æ•ˆå¤šæ ·æœ¬æ¨æ–­çš„æ¨¡å¼è½¬å˜ï¼Œå®ç°äº†æŠ•æœºè§£ç ä¸åŸºäºé‡‡æ ·çš„æ¨ç†æŠ€æœ¯çš„æ— ç¼é›†æˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05330v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤šæ ·æœ¬æ¨ç†åœºæ™¯ï¼ˆå¦‚è‡ªæ´½å’Œæœ€ä½³Né‡‡æ ·ï¼‰çš„å®šåˆ¶æ–°å‹æŠ•æœºè§£ç æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¹¶è¡Œç”Ÿæˆè·¯å¾„çš„å†…åœ¨å…±è¯†ï¼Œé€šè¿‡æ¦‚ç‡èšåˆæœºåˆ¶åŠ¨æ€åˆ†æå¹¶è¡Œæ¨ç†è·¯å¾„çš„ç»“æ„æ¨¡å¼ï¼Œåˆæˆé«˜è´¨é‡è‰ç¨¿æ ‡è®°ï¼Œæ— éœ€è¾…åŠ©æ¨¡å‹æˆ–å¤–éƒ¨æ•°æ®åº“ã€‚å®ƒåœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿ç›¸æ¯”ï¼Œè‰æ¡ˆæ¥å—ç‡æœ‰æ˜¾è‘—æé«˜ï¼ŒåŒæ—¶é™ä½äº†è‰æ¡ˆæ ‡è®°æ„å»ºçš„å»¶è¿Ÿã€‚è¿™é¡¹å·¥ä½œä¸ºé«˜æ•ˆçš„å¤šæ ·æœ¬æ¨æ–­å¸¦æ¥äº†èŒƒå¼è½¬å˜ï¼Œå®ç°äº†æŠ•æœºè§£ç ä¸åŸºäºé‡‡æ ·çš„æ¨ç†æŠ€æœ¯çš„æ— ç¼é›†æˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤šæ ·æœ¬æ¨ç†åœºæ™¯çš„æ–°å‹æŠ•æœºè§£ç æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨å¹¶è¡Œç”Ÿæˆè·¯å¾„çš„å†…åœ¨å…±è¯†æ¥åˆæˆé«˜è´¨é‡è‰ç¨¿æ ‡è®°ã€‚</li>
<li>é€šè¿‡æ¦‚ç‡èšåˆæœºåˆ¶åŠ¨æ€åˆ†æç»“æ„æ¨¡å¼æ¥è¯†åˆ«ä¸è§£ç åˆ†å¸ƒä¸€è‡´çš„å…±è¯†æ ‡è®°åºåˆ—ã€‚</li>
<li>åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœï¼Œæ˜¾ç¤ºå‡ºå¯¹è‰æ¡ˆæ¥å—ç‡çš„æ˜¾è‘—æé«˜å’Œé™ä½æ ‡è®°æ„å»ºçš„å»¶è¿Ÿã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€è¾…åŠ©æ¨¡å‹æˆ–å¤–éƒ¨æ•°æ®åº“ã€‚</li>
<li>å®ç°äº†æŠ•æœºè§£ç ä¸åŸºäºé‡‡æ ·çš„æ¨ç†æŠ€æœ¯çš„æ— ç¼é›†æˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05330">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-efe4ecc935a0b40326b49f669a748d19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4156bfdcd85bf9c3af478304fde7033d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5972bd1b4dadf252b0636f6ca4da3330.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd1373b8243f5638b8016a6f69877f06.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="WritingBench-A-Comprehensive-Benchmark-for-Generative-Writing"><a href="#WritingBench-A-Comprehensive-Benchmark-for-Generative-Writing" class="headerlink" title="WritingBench: A Comprehensive Benchmark for Generative Writing"></a>WritingBench: A Comprehensive Benchmark for Generative Writing</h2><p><strong>Authors:Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, SHaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, Fei Huang</strong></p>
<p>Recent advancements in large language models (LLMs) have significantly enhanced text generation capabilities, yet evaluating their performance in generative writing remains a challenge. Existing benchmarks primarily focus on generic text generation or limited in writing tasks, failing to capture the diverse requirements of high-quality written contents across various domains. To bridge this gap, we present WritingBench, a comprehensive benchmark designed to evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing creative, persuasive, informative, and technical writing. We further propose a query-dependent evaluation framework that empowers LLMs to dynamically generate instance-specific assessment criteria. This framework is complemented by a fine-tuned critic model for criteria-aware scoring, enabling evaluations in style, format and length. The frameworkâ€™s validity is further demonstrated by its data curation capability, which enables 7B-parameter models to approach state-of-the-art (SOTA) performance. We open-source the benchmark, along with evaluation tools and modular framework components, to advance the development of LLMs in writing. </p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•æ˜¾è‘—å¢å¼ºäº†æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œä½†è¯„ä¼°å…¶åœ¨ç”Ÿæˆå†™ä½œä¸­çš„æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨é€šç”¨æ–‡æœ¬ç”Ÿæˆæˆ–æœ‰é™çš„å†™ä½œä»»åŠ¡ï¼Œæ— æ³•æ•æ‰è·¨ä¸åŒé¢†åŸŸé«˜è´¨é‡å†…å®¹çš„å¤šæ ·åŒ–è¦æ±‚ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†WritingBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMåœ¨6ä¸ªæ ¸å¿ƒå†™ä½œåŸŸå’Œ100ä¸ªå­åŸŸä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬åˆ›é€ æ€§ã€è¯´æœåŠ›ã€ä¿¡æ¯æ€§å’ŒæŠ€æœ¯æ€§å†™ä½œã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªæŸ¥è¯¢ä¾èµ–è¯„ä¼°æ¡†æ¶ï¼Œä½¿LLMèƒ½å¤ŸåŠ¨æ€ç”Ÿæˆç‰¹å®šå®ä¾‹çš„è¯„ä¼°æ ‡å‡†ã€‚è¯¥æ¡†æ¶é€šè¿‡å¾®è°ƒè¯„è®ºå®¶æ¨¡å‹è¿›è¡Œæ ‡å‡†æ„ŸçŸ¥è¯„åˆ†ï¼Œä»¥é£æ ¼ã€æ ¼å¼å’Œé•¿åº¦ä¸ºç‰¹è‰²è¿›è¡Œè¯„ä»·ã€‚æ¡†æ¶çš„æœ‰æ•ˆæ€§è¿›ä¸€æ­¥é€šè¿‡å…¶æ•°æ®æ•´åˆèƒ½åŠ›å¾—åˆ°è¯æ˜ï¼Œè¿™ä½¿å¾—7Bå‚æ•°æ¨¡å‹èƒ½å¤Ÿæ¥è¿‘æœ€æ–°æŠ€æœ¯ï¼ˆSOTAï¼‰çš„æ€§èƒ½æ°´å¹³ã€‚æˆ‘ä»¬å¼€æºåŸºå‡†æµ‹è¯•ã€è¯„ä¼°å·¥å…·å’Œæ¨¡å—åŒ–æ¡†æ¶ç»„ä»¶ï¼Œä»¥ä¿ƒè¿›å†™ä½œé¢†åŸŸçš„LLMå‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05244v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿‘æœŸè¿›å±•æå¤§æå‡äº†æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œä½†åœ¨è¯„ä¼°å…¶ç”Ÿæˆå†™ä½œæ€§èƒ½ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚å½“å‰è¯„ä¼°åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨é€šç”¨æ–‡æœ¬ç”Ÿæˆæˆ–æœ‰é™å†™ä½œä»»åŠ¡ä¸Šï¼Œæ— æ³•æ•æ‰ä¸åŒé¢†åŸŸé«˜è´¨é‡å†…å®¹çš„å¤šæ ·éœ€æ±‚ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºWritingBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°LLMåœ¨å…­å¤§æ ¸å¿ƒå†™ä½œé¢†åŸŸåŠä¸€ç™¾ä¸ªå­é¢†åŸŸçš„å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–åˆ›æ„ã€è¯´æœã€ä¿¡æ¯å’ŒæŠ€æœ¯å†™ä½œã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æŸ¥è¯¢ä¾èµ–è¯„ä¼°æ¡†æ¶ï¼Œä½¿LLMèƒ½å¤ŸåŠ¨æ€ç”Ÿæˆç‰¹å®šå®ä¾‹çš„è¯„ä¼°æ ‡å‡†ã€‚è¯¥æ¡†æ¶é€šè¿‡ç²¾ç»†è°ƒæ•´çš„è¯„è®ºå®¶æ¨¡å‹è¿›è¡Œæ ‡å‡†æ„ŸçŸ¥è¯„åˆ†ï¼Œä»¥é£æ ¼ã€æ ¼å¼å’Œé•¿åº¦è¿›è¡Œè¯„ä¼°ã€‚æ¡†æ¶çš„æœ‰æ•ˆæ€§é€šè¿‡å…¶æ•°æ®æ•´åˆèƒ½åŠ›è¿›ä¸€æ­¥å¾—åˆ°è¯æ˜ï¼Œä½¿å¾—7Bå‚æ•°æ¨¡å‹èƒ½å¤Ÿæ¥è¿‘æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æˆ‘ä»¬å¼€æºåŸºå‡†æµ‹è¯•ä»¥åŠä¸è¯„ä¼°å·¥å…·å’Œæ¨¡å—åŒ–æ¡†æ¶ç»„ä»¶ç›¸å…³çš„å·¥å…·ï¼Œä»¥ä¿ƒè¿›å†™ä½œé¢†åŸŸLLMçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è¯„ä¼°å…¶ç”Ÿæˆå†™ä½œæ€§èƒ½çš„åŸºå‡†æµ‹è¯•ä»ä¸è¶³ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨é€šç”¨æ–‡æœ¬ç”Ÿæˆå’Œæœ‰é™å†™ä½œä»»åŠ¡ä¸Šï¼Œå¿½ç•¥äº†ä¸åŒå†™ä½œé¢†åŸŸçš„å¤šæ ·æ€§ã€‚</li>
<li>WritingBenchæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMåœ¨å…­å¤§æ ¸å¿ƒå†™ä½œé¢†åŸŸåŠå¤šä¸ªå­é¢†åŸŸçš„è¡¨ç°ã€‚</li>
<li>æå‡ºæŸ¥è¯¢ä¾èµ–è¯„ä¼°æ¡†æ¶ï¼Œä½¿LLMèƒ½åŠ¨æ€ç”Ÿæˆç‰¹å®šå®ä¾‹çš„è¯„ä¼°æ ‡å‡†ã€‚</li>
<li>é€šè¿‡ç²¾ç»†è°ƒæ•´çš„è¯„è®ºå®¶æ¨¡å‹è¿›è¡Œæ ‡å‡†æ„ŸçŸ¥è¯„åˆ†ï¼Œæ¶µç›–é£æ ¼ã€æ ¼å¼å’Œé•¿åº¦ç­‰æ–¹é¢ã€‚</li>
<li>æ¡†æ¶çš„æœ‰æ•ˆæ€§é€šè¿‡å…¶æ•°æ®æ•´åˆèƒ½åŠ›å¾—åˆ°è¯æ˜ï¼Œ7Bå‚æ•°æ¨¡å‹è¡¨ç°æ¥è¿‘æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05244">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94055dde32c30802ccdd214b8a84c796.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9d2679ce0909222c8237a98f10f37f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b383dbe5b18325eac07de6ef521b4099.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23cd85fb8bb759282c32fb7b4c8ffc19.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e186f4c47e00c084054c23e80bde1c4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73f18a576e39f854e1b087442cab49d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc3d1baf1d6bda364f0136fad377c7d6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Memory-augmented-Query-Reconstruction-for-LLM-based-Knowledge-Graph-Reasoning"><a href="#Memory-augmented-Query-Reconstruction-for-LLM-based-Knowledge-Graph-Reasoning" class="headerlink" title="Memory-augmented Query Reconstruction for LLM-based Knowledge Graph   Reasoning"></a>Memory-augmented Query Reconstruction for LLM-based Knowledge Graph   Reasoning</h2><p><strong>Authors:Mufan Xu, Gewen Liang, Kehai Chen, Wei Wang, Xun Zhou, Muyun Yang, Tiejun Zhao, Min Zhang</strong></p>
<p>Large language models (LLMs) have achieved remarkable performance on knowledge graph question answering (KGQA) tasks by planning and interacting with knowledge graphs. However, existing methods often confuse tool utilization with knowledge reasoning, harming readability of model outputs and giving rise to hallucinatory tool invocations, which hinder the advancement of KGQA. To address this issue, we propose Memory-augmented Query Reconstruction for LLM-based Knowledge Graph Reasoning (MemQ) to decouple LLM from tool invocation tasks using LLM-built query memory. By establishing a memory module with explicit descriptions of query statements, the proposed MemQ facilitates the KGQA process with natural language reasoning and memory-augmented query reconstruction. Meanwhile, we design an effective and readable reasoning to enhance the LLMâ€™s reasoning capability in KGQA. Experimental results that MemQ achieves state-of-the-art performance on widely used benchmarks WebQSP and CWQ. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡è§„åˆ’ä¸çŸ¥è¯†å›¾è°±è¿›è¡Œäº¤äº’ï¼Œåœ¨çŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å°†å·¥å…·åˆ©ç”¨ä¸çŸ¥è¯†æ¨ç†æ··æ·†ï¼ŒæŸå®³äº†æ¨¡å‹è¾“å‡ºçš„å¯è¯»æ€§ï¼Œå¹¶å‡ºç°äº†å¹»è§‰å·¥å…·è°ƒç”¨ï¼Œé˜»ç¢äº†KGQAçš„å‘å±•ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºLLMçŸ¥è¯†å›¾è°±æ¨ç†çš„è®°å¿†å¢å¼ºæŸ¥è¯¢é‡å»ºï¼ˆMemQï¼‰æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨LLMæ„å»ºçš„æŸ¥è¯¢è®°å¿†ï¼Œå°†LLMä¸å·¥å…·è°ƒç”¨ä»»åŠ¡è§£è€¦ã€‚é€šè¿‡å»ºç«‹åŒ…å«æŸ¥è¯¢è¯­å¥æ˜ç¡®æè¿°çš„å†…å­˜æ¨¡å—ï¼Œæ‰€æå‡ºçš„MemQé€šè¿‡è‡ªç„¶è¯­è¨€æ¨ç†å’Œè®°å¿†å¢å¼ºæŸ¥è¯¢é‡å»ºæ¥ä¿ƒè¿›KGQAè¿‡ç¨‹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæœ‰æ•ˆä¸”æ˜“äºç†è§£çš„æ¨ç†ï¼Œä»¥æé«˜LLMåœ¨KGQAä¸­çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMemQåœ¨å¹¿æ³›ä½¿ç”¨çš„WebQSPå’ŒCWQåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05193v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨çŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKGQAï¼‰ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨æ··æ·†å·¥å…·ä½¿ç”¨ä¸çŸ¥è¯†æ¨ç†çš„é—®é¢˜ï¼Œå½±å“æ¨¡å‹è¾“å‡ºçš„å¯è¯»æ€§å¹¶å¯¼è‡´å‡ºç°å¹»è§‰å·¥å…·è°ƒç”¨ï¼Œé˜»ç¢äº†KGQAçš„è¿›æ­¥ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨LLMæ„å»ºçš„æŸ¥è¯¢è®°å¿†æ¥è§£è€¦LLMä¸å·¥å…·è°ƒç”¨ä»»åŠ¡çš„Memory-augmented Query Reconstructionæ–¹æ³•ï¼ˆMemQï¼‰ã€‚é€šè¿‡å»ºç«‹åŒ…å«æ˜ç¡®æŸ¥è¯¢æè¿°çš„å­˜å‚¨æ¨¡å—ï¼ŒMemQé€šè¿‡è‡ªç„¶è¯­è¨€æ¨ç†å’Œè®°å¿†å¢å¼ºæŸ¥è¯¢é‡æ„ä¿ƒè¿›KGQAè¿‡ç¨‹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æœ‰æ•ˆä¸”å¯ç†è§£çš„æ¨ç†æ–¹æ³•ï¼Œä»¥æé«˜LLMåœ¨KGQAä¸­çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMemQåœ¨å¹¿æ³›ä½¿ç”¨çš„WebQSPå’ŒCWQåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMåœ¨KGQAä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œä½†å­˜åœ¨å·¥å…·ä½¿ç”¨ä¸çŸ¥è¯†æ¨ç†æ··æ·†çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰é—®é¢˜å¯¼è‡´æ¨¡å‹è¾“å‡ºå¯è¯»æ€§ä¸‹é™å’Œå¹»è§‰å·¥å…·è°ƒç”¨ã€‚</li>
<li>MemQé€šè¿‡LLMæ„å»ºçš„æŸ¥è¯¢è®°å¿†è§£è€¦LLMä¸å·¥å…·è°ƒç”¨ä»»åŠ¡ã€‚</li>
<li>MemQå»ºç«‹åŒ…å«æ˜ç¡®æŸ¥è¯¢æè¿°çš„å­˜å‚¨æ¨¡å—ã€‚</li>
<li>MemQä¿ƒè¿›KGQAè¿‡ç¨‹é€šè¿‡è‡ªç„¶è¯­è¨€æ¨ç†å’Œè®°å¿†å¢å¼ºæŸ¥è¯¢é‡æ„ã€‚</li>
<li>è®¾è®¡æœ‰æ•ˆä¸”å¯ç†è§£çš„æ¨ç†æ–¹æ³•æé«˜LLMåœ¨KGQAä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05193">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8e37c1e5bcc898258d83247b213dc99d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-129e3789229d70f937e3b31f37969a9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cf5bb5ba7702003b7cf1e3b1fc94cd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8db73abce5252960b6596f463a418c2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-654932101b2cbffc9a9d2155ed04b5b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7f217873f300f78812d8fe728676c05.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Sketch-of-Thought-Efficient-LLM-Reasoning-with-Adaptive-Cognitive-Inspired-Sketching"><a href="#Sketch-of-Thought-Efficient-LLM-Reasoning-with-Adaptive-Cognitive-Inspired-Sketching" class="headerlink" title="Sketch-of-Thought: Efficient LLM Reasoning with Adaptive   Cognitive-Inspired Sketching"></a>Sketch-of-Thought: Efficient LLM Reasoning with Adaptive   Cognitive-Inspired Sketching</h2><p><strong>Authors:Simon A. Aytes, Jinheon Baek, Sung Ju Hwang</strong></p>
<p>Recent advances in large language models have demonstrated remarkable reasoning capabilities through Chain of Thought (CoT) prompting, but often at the cost of excessive verbosity in their intermediate outputs, which increases computational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting framework that combines cognitive-inspired reasoning paradigms with linguistic constraints to minimize token usage while preserving reasoning accuracy. SoT is designed as a flexible framework that can incorporate any custom reasoning paradigms based on cognitive science, and we instantiate it with three such paradigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each tailored to different reasoning tasks and selected dynamically via a lightweight routing model. Through comprehensive evaluation across 15 reasoning datasets with multiple languages and multimodal scenarios, we demonstrate that SoT achieves token reductions of 76% with negligible accuracy impact. In certain domains like mathematical and multi-hop reasoning, it even improves accuracy while using significantly fewer tokens. Our code is publicly available: <a target="_blank" rel="noopener" href="https://www.github.com/SimonAytes/SoT">https://www.github.com/SimonAytes/SoT</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€šè¿‡æ€ç»´é“¾ï¼ˆChain of Thoughtï¼Œç®€ç§°CoTï¼‰æç¤ºå±•ç°å‡ºæ˜¾è‘—æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œå…¶äº§ç”Ÿçš„ä¸­é—´è¾“å‡ºç‰©å¾€å¾€è¿‡äºå†—é•¿ï¼Œå¢åŠ äº†è®¡ç®—å¼€é”€ã€‚æˆ‘ä»¬å¼•å…¥äº†æ€ç»´è‰å›¾ï¼ˆSketch-of-Thoughtï¼Œç®€ç§°SoTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æç¤ºæ¡†æ¶ï¼Œç»“åˆäº†è®¤çŸ¥å¯å‘å¼çš„æ¨ç†æ¨¡å¼å’Œè¯­è¨€çº¦æŸï¼Œæ—¨åœ¨æœ€å°åŒ–æ ‡è®°ä½¿ç”¨çš„åŒæ—¶ä¿æŒæ¨ç†å‡†ç¡®æ€§ã€‚SoTè¢«è®¾è®¡ä¸ºä¸€ä¸ªçµæ´»çš„æ¡†æ¶ï¼Œå¯ä»¥æ ¹æ®è®¤çŸ¥ç§‘å­¦èå…¥ä»»ä½•è‡ªå®šä¹‰æ¨ç†æ¨¡å¼ï¼Œæˆ‘ä»¬ä¸ºå…¶æä¾›äº†ä¸‰ç§æ¨¡å¼å®ä¾‹â€”â€”æ¦‚å¿µé“¾ã€åˆ†å—ç¬¦å·å’Œä¸“å®¶è¯æ±‡é›†â€”â€”æ¯ç§æ¨¡å¼éƒ½é’ˆå¯¹ç‰¹å®šçš„æ¨ç†ä»»åŠ¡å®šåˆ¶ï¼Œå¹¶é€šè¿‡è½»é‡çº§è·¯ç”±æ¨¡å‹åŠ¨æ€é€‰æ‹©ã€‚é€šè¿‡æ¶µç›–å¤šç§è¯­è¨€å’Œè·¨æ¨¡æ€åœºæ™¯çš„15ä¸ªæ¨ç†æ•°æ®é›†çš„ç»¼åˆè¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†SoTåœ¨å‡å°‘76%æ ‡è®°çš„åŒæ—¶å‡ ä¹ä¸å½±å“å‡†ç¡®æ€§ã€‚åœ¨æŸäº›é¢†åŸŸå¦‚æ•°å­¦å’Œå¤šè·³æ¨ç†ä¸­ï¼Œå³ä½¿åœ¨ä½¿ç”¨æ›´å°‘çš„æ ‡è®°çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æé«˜å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://www.github.com/SimonAytes/SoT%E3%80%82">https://www.github.com/SimonAytes/SoTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05179v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•é€šè¿‡æ€ç»´é“¾ï¼ˆChain of Thoughtï¼ŒCoTï¼‰æç¤ºå±•ç°å‡ºæƒŠäººçš„æ¨ç†èƒ½åŠ›ï¼Œä½†å¾€å¾€ä¼´éšç€ä¸­é—´è¾“å‡ºè¿‡äºå†—é•¿çš„é—®é¢˜ï¼Œå¢åŠ äº†è®¡ç®—å¼€é”€ã€‚æœ¬æ–‡æå‡ºSketch-of-Thoughtï¼ˆSoTï¼‰è¿™ä¸€æ–°å‹æç¤ºæ¡†æ¶ï¼Œç»“åˆè®¤çŸ¥å¯å‘å¼çš„æ¨ç†èŒƒå¼å’Œè¯­è¨€å­¦çº¦æŸï¼Œæ—¨åœ¨å‡å°‘æ ‡è®°ä½¿ç”¨åŒæ—¶ä¿æŒæ¨ç†å‡†ç¡®æ€§ã€‚SoTè¢«è®¾è®¡æˆå¯çµæ´»èå…¥è®¤çŸ¥ç§‘å­¦ä¸ºåŸºç¡€çš„å„ç§è‡ªå®šä¹‰æ¨ç†èŒƒå¼çš„æ¡†æ¶ï¼Œå¹¶å®ä¾‹åŒ–äº†ä¸‰ç§èŒƒå¼ï¼šæ¦‚å¿µé“¾ã€åˆ†å—ç¬¦å·å’Œä¸“å®¶è¯æ±‡ã€‚é€šè¿‡è·¨è¶Š15ä¸ªæ¨ç†æ•°æ®é›†çš„å…¨é¢è¯„ä¼°ï¼Œå±•ç¤ºSoTåœ¨å‡å°‘76%æ ‡è®°ä½¿ç”¨çš„åŒæ—¶ï¼Œå¯¹å‡†ç¡®ç‡å½±å“ç”šå¾®ã€‚åœ¨æŸäº›é¢†åŸŸå¦‚æ•°å­¦å’Œå¤šæ­¥æ¨ç†ä¸­ï¼Œç”šè‡³åœ¨ä½¿ç”¨æ›´å°‘æ ‡è®°çš„æƒ…å†µä¸‹æé«˜äº†å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Sketch-of-Thoughtï¼ˆSoTï¼‰æ¡†æ¶ç»“åˆäº†è®¤çŸ¥å¯å‘å¼çš„æ¨ç†èŒƒå¼å’Œè¯­è¨€å­¦çº¦æŸï¼Œæ—¨åœ¨å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ ‡è®°ä½¿ç”¨ã€‚</li>
<li>SoTæ¡†æ¶æ—¨åœ¨åœ¨ä¿æŒæ¨ç†å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œé™ä½è®¡ç®—å¼€é”€ã€‚</li>
<li>SoTæ¡†æ¶å…·æœ‰çµæ´»æ€§ï¼Œå¯èå…¥å¤šç§è®¤çŸ¥ç§‘å­¦ä¸ºåŸºç¡€çš„è‡ªå®šæ¨ç†èŒƒå¼ã€‚</li>
<li>SoTå®ä¾‹åŒ–äº†ä¸‰ç§ç‰¹å®šæ¨ç†èŒƒå¼ï¼šæ¦‚å¿µé“¾ã€åˆ†å—ç¬¦å·å’Œä¸“å®¶è¯æ±‡ã€‚</li>
<li>SoTåœ¨å¤šä¸ªè¯­è¨€å’Œè·¨æ¨¡æ€åœºæ™¯çš„15ä¸ªæ¨ç†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚</li>
<li>SoTå®ç°äº†76%çš„æ ‡è®°å‡å°‘è€Œå¯¹å‡†ç¡®ç‡å½±å“ç”šå¾®ï¼Œå¹¶ä¸”åœ¨æŸäº›é¢†åŸŸå¦‚æ•°å­¦å’Œå¤šæ­¥æ¨ç†ä¸­æé«˜äº†å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05179">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-26d0a0568202f66addcc6dc5e2e4801b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e14c12c7342df4536b9cd58c8d4759fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b29b6bb29112037613c7db2b11254f73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38845ec973a701783dd202432ff598fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24cd29c069c710f04566f76bbd48fcce.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="R1-Zeroâ€™s-â€œAha-Momentâ€-in-Visual-Reasoning-on-a-2B-Non-SFT-Model"><a href="#R1-Zeroâ€™s-â€œAha-Momentâ€-in-Visual-Reasoning-on-a-2B-Non-SFT-Model" class="headerlink" title="R1-Zeroâ€™s â€œAha Momentâ€ in Visual Reasoning on a 2B Non-SFT Model"></a>R1-Zeroâ€™s â€œAha Momentâ€ in Visual Reasoning on a 2B Non-SFT Model</h2><p><strong>Authors:Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, Cho-Jui Hsieh</strong></p>
<p>Recently DeepSeek R1 demonstrated how reinforcement learning with simple rule-based incentives can enable autonomous development of complex reasoning in large language models, characterized by the â€œaha momentâ€, in which the model manifest self-reflection and increased response length during training. However, attempts to extend this success to multimodal reasoning often failed to reproduce these key characteristics. In this report, we present the first successful replication of these emergent characteristics for multimodal reasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying reinforcement learning directly on the SAT dataset, our model achieves 59.47% accuracy on CVBench, outperforming the base model by approximately ~30% and exceeding both SFT setting by ~2%. In addition, we share our failed attempts and insights in attempting to achieve R1-like reasoning using RL with instruct models. aiming to shed light on the challenges involved. Our key observations include: (1) applying RL on instruct model often results in trivial reasoning trajectories, and (2) naive length reward are ineffective in eliciting reasoning capabilities. The project code is available at <a target="_blank" rel="noopener" href="https://github.com/turningpoint-ai/VisualThinker-R1-Zero">https://github.com/turningpoint-ai/VisualThinker-R1-Zero</a> </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒDeepSeek R1å±•ç¤ºäº†å¦‚ä½•é€šè¿‡åŸºäºç®€å•è§„åˆ™çš„æ¿€åŠ±æ¥åº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œä»è€Œåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è‡ªä¸»å‘å±•å‡ºå¤æ‚çš„æ¨ç†èƒ½åŠ›ï¼Œå…¶ç‰¹ç‚¹æ˜¯è¡¨ç°ä¸ºâ€œå•Šå“ˆæ—¶åˆ»â€ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¡¨ç°å‡ºè‡ªæˆ‘åæ€å’Œå“åº”é•¿åº¦çš„å¢åŠ ã€‚ç„¶è€Œï¼Œå°è¯•å°†è¿™ç§æˆåŠŸæ‰©å±•åˆ°å¤šæ¨¡æ€æ¨ç†æ—¶ï¼Œå¾€å¾€æ— æ³•é‡ç°è¿™äº›å…³é”®ç‰¹å¾ã€‚åœ¨æœ¬æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æˆåŠŸåœ°åœ¨ä»…ä½¿ç”¨éSFT 2Bæ¨¡å‹çš„æ¡ä»¶ä¸‹ï¼Œå®ç°äº†å¤šæ¨¡æ€æ¨ç†çš„è¿™äº›æ–°å…´ç‰¹å¾ã€‚æˆ‘ä»¬ä»¥Qwen2-VL-2Bä¸ºèµ·ç‚¹ï¼Œç›´æ¥åœ¨SATæ•°æ®é›†ä¸Šåº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨CVBenchä¸Šè¾¾åˆ°äº†59.47%çš„å‡†ç¡®ç‡ï¼Œæ¯”åŸºç¡€æ¨¡å‹é«˜å‡ºçº¦30%ï¼Œå¹¶ä¸”æ¯”SFTè®¾ç½®çš„å‡†ç¡®ç‡é«˜å‡ºçº¦2%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ†äº«äº†åœ¨ä½¿ç”¨RLè¿›è¡ŒæŒ‡ä»¤æ¨¡å‹ä»¥å®ç°R1å¼æ¨ç†çš„å°è¯•ä¸­çš„å¤±è´¥ç»éªŒå’Œè§è§£ï¼Œä»¥æœŸé˜æ˜æ‰€æ¶‰åŠçš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ä¸»è¦è§‚å¯Ÿç»“æœåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰åœ¨æŒ‡ä»¤æ¨¡å‹ä¸Šåº”ç”¨RLé€šå¸¸ä¼šå¯¼è‡´å¹³å‡¡çš„æ¨ç†è½¨è¿¹ï¼Œï¼ˆ2ï¼‰ç®€å•çš„é•¿åº¦å¥–åŠ±åœ¨æ¿€å‘æ¨ç†èƒ½åŠ›æ–¹é¢æ— æ•ˆã€‚é¡¹ç›®ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/turningpoint-ai/VisualThinker-R1-Zero%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/turningpoint-ai/VisualThinker-R1-Zeroæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05132v1">PDF</a> 10 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æŠ¥å‘Šä»‹ç»äº†å…³äºDeepSeek R1åˆ©ç”¨å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªä¸»å‘å±•å¤æ‚æ¨ç†æŠ€æœ¯çš„æ–°è¿›å±•ã€‚ç ”ç©¶äººå‘˜æˆåŠŸåœ¨éSFT 2Bæ¨¡å‹ä¸Šå®ç°äº†å¤šæ¨¡æ€æ¨ç†çš„â€œå•Šå“ˆæ—¶åˆ»â€ï¼Œè¡¨ç°å‡ºè‡ªæˆ‘åæ€å’Œå“åº”é•¿åº¦å¢åŠ çš„ç‰¹æ€§ã€‚é€šè¿‡ç›´æ¥åœ¨SATæ•°æ®é›†ä¸Šåº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œæ¨¡å‹åœ¨CVBenchä¸Šè¾¾åˆ°äº†59.47%çš„å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºåŸºç¡€æ¨¡å‹æå‡äº†çº¦30%ï¼Œå¹¶ä¸”ç›¸è¾ƒäºSFTè®¾ç½®æå‡äº†çº¦2%ã€‚æ­¤å¤–ï¼ŒæŠ¥å‘Šè¿˜åˆ†äº«äº†ä½¿ç”¨RLå¯¹æŒ‡ä»¤æ¨¡å‹è¿›è¡Œå°è¯•æ—¶çš„å¤±è´¥ç»éªŒå’Œè§‚å¯Ÿç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿé©±åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªä¸»å‘å±•å¤æ‚æ¨ç†æŠ€æœ¯ã€‚</li>
<li>åœ¨éSFT 2Bæ¨¡å‹ä¸ŠæˆåŠŸå®ç°å¤šæ¨¡æ€æ¨ç†çš„â€œå•Šå“ˆæ—¶åˆ»â€ã€‚</li>
<li>æ¨¡å‹å±•ç°å‡ºè‡ªæˆ‘åæ€å’Œå“åº”é•¿åº¦å¢åŠ çš„ç‰¹æ€§ã€‚</li>
<li>åœ¨SATæ•°æ®é›†ä¸Šåº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œæ¨¡å‹åœ¨CVBenchä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†59.47%ã€‚</li>
<li>å¯¹æ¯”åŸºç¡€æ¨¡å‹ï¼Œæ–°æ¨¡å‹çš„å‡†ç¡®ç‡æå‡äº†çº¦30%ã€‚</li>
<li>å¯¹æ¯”SFTè®¾ç½®ï¼Œæ–°æ¨¡å‹çš„å‡†ç¡®ç‡æå‡äº†çº¦2%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05132">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cd01b082111c4954a1cdc8c9ddde849a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-64e9adc43c94b0c60b4028d6b478fc1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cda586f7225b0cb352485121403c731.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-980c035b50957dd0952d1196d8e74510.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f85e8dae55de22d7b569369554e6ccf7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18bdff6214bf9f0305b6c69151a64fac.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Beyond-RAG-Task-Aware-KV-Cache-Compression-for-Comprehensive-Knowledge-Reasoning"><a href="#Beyond-RAG-Task-Aware-KV-Cache-Compression-for-Comprehensive-Knowledge-Reasoning" class="headerlink" title="Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge   Reasoning"></a>Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge   Reasoning</h2><p><strong>Authors:Giulio Corallo, Orion Weller, Fabio Petroni, Paolo Papotti</strong></p>
<p>Incorporating external knowledge in large language models (LLMs) enhances their utility across diverse applications, but existing methods have trade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via similarity search, but key information may fall outside top ranked results. Long-context models can process multiple documents but are computationally expensive and limited by context window size. Inspired by students condensing study material for open-book exams, we propose task-aware key-value (KV) cache compression, which compresses external knowledge in a zero- or few-shot setup. This enables LLMs to reason efficiently over a compacted representation of all relevant information. Experiments show our approach outperforms both RAG and task-agnostic compression methods. On LongBench v2, it improves accuracy by up to 7 absolute points over RAG with a 30x compression rate, while reducing inference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG performs well when sparse evidence suffices, whereas task-aware compression is superior for broad knowledge tasks. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¤–éƒ¨çŸ¥è¯†ç›¸ç»“åˆï¼Œæé«˜äº†å…¶åœ¨å„ç§åº”ç”¨ä¸­çš„å®ç”¨æ€§ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨æƒè¡¡ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡ç›¸ä¼¼æ€§æœç´¢è·å–è¯æ®ï¼Œä½†å…³é”®ä¿¡æ¯å¯èƒ½ä¸åœ¨æ’åé å‰çš„ç»“æœä¸­ã€‚é•¿ä¸Šä¸‹æ–‡æ¨¡å‹å¯ä»¥å¤„ç†å¤šä¸ªæ–‡æ¡£ï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œå¹¶å—é™äºä¸Šä¸‹æ–‡çª—å£å¤§å°ã€‚é€šè¿‡å€Ÿé‰´å­¦ç”Ÿä¸ºå¼€å·è€ƒè¯•ç²¾ç®€å­¦ä¹ ææ–™çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†ä»»åŠ¡æ„ŸçŸ¥é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜å‹ç¼©æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯ä»¥åœ¨é›¶æ¬¡æˆ–å°‘æ•°å‡ æ¬¡å°„å‡»ä¸­å‹ç¼©å¤–éƒ¨çŸ¥è¯†ã€‚è¿™ä½¿å¾—LLMèƒ½å¤Ÿåœ¨å‹ç¼©åè¡¨ç¤ºçš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ä¸Šè¿›è¡Œé«˜æ•ˆæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºRAGå’Œä»»åŠ¡æ— å…³çš„å‹ç¼©æ–¹æ³•ã€‚åœ¨LongBench v2ä¸Šï¼Œä¸RAGç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡†ç¡®ç‡ä¸Šæé«˜äº†é«˜è¾¾7ä¸ªç»å¯¹ç‚¹ï¼Œå‹ç¼©ç‡é«˜è¾¾30å€ï¼ŒåŒæ—¶å°†æ¨ç†å»¶è¿Ÿä»0.43ç§’å‡å°‘åˆ°0.16ç§’ã€‚åˆæˆæ•°æ®é›†çš„é‡ç‚¹æ˜¯ï¼Œå½“ç¨€ç–è¯æ®è¶³å¤Ÿæ—¶ï¼ŒRAGè¡¨ç°è‰¯å¥½ï¼Œè€Œå¯¹äºå¹¿æ³›çš„çŸ¥è¯†ä»»åŠ¡ï¼Œä»»åŠ¡æ„ŸçŸ¥å‹ç¼©è¡¨ç°æ›´ä¼˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04973v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­èå…¥å¤–éƒ¨çŸ¥è¯†å¯æé«˜å…¶è·¨å¤šç§åº”ç”¨çš„å®ç”¨æ€§ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨æƒè¡¡ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆæ³•é€šè¿‡ç›¸ä¼¼æ€§æœç´¢è·å–è¯æ®ï¼Œä½†å…³é”®ä¿¡æ¯å¯èƒ½å‡ºç°åœ¨æ’åé åçš„ç»“æœä¸­ã€‚é•¿æ–‡æœ¬æ¨¡å‹å¯å¤„ç†å¤šä¸ªæ–‡æ¡£ï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”å—é™äºä¸Šä¸‹æ–‡çª—å£å¤§å°ã€‚å—å­¦ç”Ÿä¸ºå¼€å·è€ƒè¯•æµ“ç¼©å­¦ä¹ ææ–™å¯å‘ï¼Œæˆ‘ä»¬æå‡ºä»»åŠ¡æ„ŸçŸ¥é”®å€¼ç¼“å­˜å‹ç¼©æŠ€æœ¯ï¼Œå¯åœ¨é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬è®¾ç½®ä¸­å‹ç¼©å¤–éƒ¨çŸ¥è¯†ã€‚è¿™ä½¿å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨ç´§å‡‘è¡¨ç¤ºæ‰€æœ‰ç›¸å…³ä¿¡æ¯çš„åŸºç¡€ä¸Šé«˜æ•ˆæ¨ç†ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæ£€ç´¢å¢å¼ºç”Ÿæˆæ³•å’Œä»»åŠ¡æ— å…³å‹ç¼©æ–¹æ³•ã€‚åœ¨LongBench v2ä¸Šï¼Œä¸RAGç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†é«˜è¾¾7ä¸ªç»å¯¹ç‚¹çš„å‡†ç¡®ç‡ï¼Œå‹ç¼©ç‡é«˜è¾¾30å€ï¼ŒåŒæ—¶å°†æ¨ç†å»¶è¿Ÿä»0.43ç§’å‡å°‘åˆ°0.16ç§’ã€‚åˆæˆæ•°æ®é›†æ˜¾ç¤ºï¼Œå½“ç¨€ç–è¯æ®å……è¶³æ—¶ï¼ŒRAGè¡¨ç°è‰¯å¥½ï¼Œè€Œå¯¹äºå¹¿æ³›çŸ¥è¯†ä»»åŠ¡ï¼Œä»»åŠ¡æ„ŸçŸ¥å‹ç¼©è¡¨ç°æ›´ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹èå…¥å¤–éƒ¨çŸ¥è¯†å¯æå‡è·¨åº”ç”¨å®ç”¨æ€§ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆæ³•å¯èƒ½å¿½ç•¥å…³é”®ä¿¡æ¯ï¼Œå› ä¸ºå…³é”®ä¿¡æ¯å¯èƒ½ä¸åœ¨é«˜æ’åç»“æœä¸­ã€‚</li>
<li>é•¿æ–‡æœ¬æ¨¡å‹å¤„ç†å¤šä¸ªæ–‡æ¡£çš„èƒ½åŠ›å—é™ï¼Œä¸”è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>æå‡ºä»»åŠ¡æ„ŸçŸ¥é”®å€¼ç¼“å­˜å‹ç¼©æŠ€æœ¯ï¼Œå¯åœ¨é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬è®¾ç½®ä¸‹é«˜æ•ˆå‹ç¼©å¤–éƒ¨çŸ¥è¯†ã€‚</li>
<li>ä¸RAGç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜å‡†ç¡®ç‡ã€å‹ç¼©ç‡å’Œæ¨ç†é€Ÿåº¦ã€‚</li>
<li>åœ¨ä¸åŒåœºæ™¯ä¸‹ï¼ˆç¨€ç–è¯æ®å……è¶³å’Œå¹¿æ³›çŸ¥è¯†ä»»åŠ¡ï¼‰ï¼Œè¯¥æ–¹æ³•è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04973">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-332f524e6c36ad65d614cdb0df8763c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cf9b4bae8727ffe10ecb1b87639834d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa73b6dd7abd271f3066cf3cbfd0fa5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e2f97ed2de0ce138d2ffd5783eae6c6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1341fb45e84af530e3384b827d2a9fb6.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DB-Explore-Automated-Database-Exploration-and-Instruction-Synthesis-for-Text-to-SQL"><a href="#DB-Explore-Automated-Database-Exploration-and-Instruction-Synthesis-for-Text-to-SQL" class="headerlink" title="DB-Explore: Automated Database Exploration and Instruction Synthesis for   Text-to-SQL"></a>DB-Explore: Automated Database Exploration and Instruction Synthesis for   Text-to-SQL</h2><p><strong>Authors:Haoyuan Ma, Yongliang Shen, Hengwei Liu, Wenqi Zhang, Haolei Xu, Qiuying Peng, Jun Wang, Weiming Lu</strong></p>
<p>Recent text-to-SQL systems powered by large language models (LLMs) have demonstrated remarkable performance in translating natural language queries into SQL. However, these systems often struggle with complex database structures and domain-specific queries, as they primarily focus on enhancing logical reasoning and SQL syntax while overlooking the critical need for comprehensive database understanding. To address this limitation, we propose DB-Explore, a novel framework that systematically aligns LLMs with database knowledge through automated exploration and instruction synthesis. DB-Explore constructs database graphs to capture complex relational schemas, leverages GPT-4 to systematically mine structural patterns and semantic knowledge, and synthesizes instructions to distill this knowledge for efficient fine-tuning of LLMs. Our framework enables comprehensive database understanding through diverse sampling strategies and automated instruction generation, bridging the gap between database structures and language models. Experiments conducted on the SPIDER and BIRD benchmarks validate the effectiveness of DB-Explore, achieving an execution accuracy of 52.1% on BIRD and 84.0% on SPIDER. Notably, our open-source implementation, based on the Qwen2.5-coder-7B model, outperforms multiple GPT-4-driven text-to-SQL systems in comparative evaluations, and achieves near state-of-the-art performance with minimal computational cost. </p>
<blockquote>
<p>æœ€è¿‘ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„æ–‡æœ¬åˆ°SQLç³»ç»Ÿåœ¨è‡ªç„¶è¯­è¨€æŸ¥è¯¢åˆ°SQLçš„è½¬æ¢ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿåœ¨å¤„ç†å¤æ‚çš„æ•°æ®åº“ç»“æ„å’Œç‰¹å®šé¢†åŸŸçš„æŸ¥è¯¢æ—¶ç»å¸¸é‡åˆ°å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬ä¸»è¦å…³æ³¨äºæé«˜é€»è¾‘æ¨ç†è§£é¢˜èƒ½åŠ›å’ŒSQLè¯­æ³•ï¼Œè€Œå¿½è§†äº†å…¨é¢ç†è§£æ•°æ®åº“çš„å…³é”®éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†DB-Exploreè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡è‡ªåŠ¨åŒ–æ¢ç´¢å’ŒæŒ‡ä»¤åˆæˆï¼Œç³»ç»Ÿåœ°ä½¿LLMä¸æ•°æ®åº“çŸ¥è¯†ç›¸ç»“åˆã€‚DB-Exploreæ„å»ºæ•°æ®åº“å›¾æ¥æ•æ‰å¤æ‚çš„å…³è”æ¨¡å¼ï¼Œåˆ©ç”¨GPT-4ç³»ç»Ÿåœ°æŒ–æ˜ç»“æ„æ¨¡å¼å’Œè¯­ä¹‰çŸ¥è¯†ï¼Œå¹¶åˆæˆæŒ‡ä»¤æ¥æç‚¼è¿™äº›çŸ¥è¯†ï¼Œä»¥ä¾¿æœ‰æ•ˆåœ°å¾®è°ƒLLMã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡å¤šæ ·åŒ–çš„é‡‡æ ·ç­–ç•¥å’Œè‡ªåŠ¨åŒ–çš„æŒ‡ä»¤ç”Ÿæˆï¼Œå®ç°äº†å…¨é¢çš„æ•°æ®åº“ç†è§£ï¼Œç¼©å°äº†æ•°æ®åº“ç»“æ„å’Œè¯­è¨€æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚åœ¨SPIDERå’ŒBIRDåŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œçš„å®éªŒéªŒè¯äº†DB-Exploreçš„æœ‰æ•ˆæ€§ï¼Œåœ¨BIRDä¸Šå®ç°äº†æ‰§è¡Œå‡†ç¡®ç‡52.1%ï¼Œåœ¨SPIDERä¸Šå®ç°äº†æ‰§è¡Œå‡†ç¡®ç‡84.0%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åŸºäºQwen2.5-coder-7Bæ¨¡å‹çš„å¼€æºå®ç°ï¼Œåœ¨æ¯”è¾ƒè¯„ä¼°ä¸­è¶…è¶Šäº†å¤šä¸ªGPT-4é©±åŠ¨çš„æ–‡æœ¬åˆ°SQLç³»ç»Ÿï¼Œå¹¶ä»¥æä½çš„è®¡ç®—æˆæœ¬è¾¾åˆ°äº†è¿‘ä¹æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04959v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMé©±åŠ¨çš„æ–‡æœ¬åˆ°SQLç³»ç»Ÿè™½åœ¨ç¿»è¯‘è‡ªç„¶è¯­è¨€æŸ¥è¯¢ä¸ºSQLæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†å¤æ‚æ•°æ®åº“ç»“æ„å’Œç‰¹å®šé¢†åŸŸæŸ¥è¯¢æ—¶å­˜åœ¨å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæå‡ºDB-Exploreæ¡†æ¶ï¼Œé€šè¿‡è‡ªåŠ¨æ¢ç´¢å’ŒæŒ‡ä»¤åˆæˆï¼Œä½¿LLMä¸æ•°æ®åº“çŸ¥è¯†ç›¸ç»“åˆã€‚è¯¥æ¡†æ¶æ„å»ºæ•°æ®åº“å›¾æ•æ‰å¤æ‚å…³ç³»æ¨¡å¼ï¼Œåˆ©ç”¨GPT-4æŒ–æ˜ç»“æ„å’Œè¯­ä¹‰çŸ¥è¯†ï¼Œå¹¶åˆæˆæŒ‡ä»¤ä»¥æç‚¼çŸ¥è¯†ï¼Œæœ‰æ•ˆå¾®è°ƒLLMã€‚å®éªŒéªŒè¯DB-Exploreçš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨SPIDERå’ŒBIRDåŸºå‡†æµ‹è¯•ä¸­å®ç°è¾ƒé«˜æ‰§è¡Œå‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°SQLç³»ç»Ÿé¢ä¸´å¤„ç†å¤æ‚æ•°æ®åº“ç»“æ„å’Œç‰¹å®šé¢†åŸŸæŸ¥è¯¢çš„æŒ‘æˆ˜ã€‚</li>
<li>DB-Exploreæ¡†æ¶æå‡ºï¼Œé€šè¿‡ç»“åˆLLMå’Œæ•°æ®åº“çŸ¥è¯†è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>DB-Exploreæ„å»ºæ•°æ®åº“å›¾ä»¥æ•æ‰å¤æ‚å…³ç³»æ¨¡å¼ã€‚</li>
<li>GPT-4è¢«ç”¨æ¥æŒ–æ˜ç»“æ„å’Œè¯­ä¹‰çŸ¥è¯†ã€‚</li>
<li>DB-Exploreé€šè¿‡åˆæˆæŒ‡ä»¤æç‚¼çŸ¥è¯†ï¼Œæœ‰æ•ˆå¾®è°ƒLLMã€‚</li>
<li>å®éªŒéªŒè¯DB-Exploreçš„æœ‰æ•ˆæ€§ï¼Œåœ¨SPIDERå’ŒBIRDåŸºå‡†æµ‹è¯•ä¸­æœ‰é«˜æ‰§è¡Œå‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4a2f43a170a78e1e2d3c86738e5f9d7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1baa9ca4fa22f60620ce7e81c5c7bb86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bba96dfdf627fbb47ed171c0f5ade56.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e46abf01e537b284b67d59560afb795a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FirePlace-Geometric-Refinements-of-LLM-Common-Sense-Reasoning-for-3D-Object-Placement"><a href="#FirePlace-Geometric-Refinements-of-LLM-Common-Sense-Reasoning-for-3D-Object-Placement" class="headerlink" title="FirePlace: Geometric Refinements of LLM Common Sense Reasoning for 3D   Object Placement"></a>FirePlace: Geometric Refinements of LLM Common Sense Reasoning for 3D   Object Placement</h2><p><strong>Authors:Ian Huang, Yanan Bao, Karen Truong, Howard Zhou, Cordelia Schmid, Leonidas Guibas, Alireza Fathi</strong></p>
<p>Scene generation with 3D assets presents a complex challenge, requiring both high-level semantic understanding and low-level geometric reasoning. While Multimodal Large Language Models (MLLMs) excel at semantic tasks, their application to 3D scene generation is hindered by their limited grounding on 3D geometry. In this paper, we investigate how to best work with MLLMs in an object placement task. Towards this goal, we introduce a novel framework, FirePlace, that applies existing MLLMs in (1) 3D geometric reasoning and the extraction of relevant geometric details from the 3D scene, (2) constructing and solving geometric constraints on the extracted low-level geometry, and (3) pruning for final placements that conform to common sense. By combining geometric reasoning with real-world understanding of MLLMs, our method can propose object placements that satisfy both geometric constraints as well as high-level semantic common-sense considerations. Our experiments show that these capabilities allow our method to place objects more effectively in complex scenes with intricate geometry, surpassing the quality of prior work. </p>
<blockquote>
<p>ä½¿ç”¨3Dèµ„äº§è¿›è¡Œåœºæ™¯ç”Ÿæˆæ˜¯ä¸€ä¸ªå¤æ‚çš„æŒ‘æˆ˜ï¼Œéœ€è¦é«˜çº§è¯­ä¹‰ç†è§£å’Œä½çº§å‡ ä½•æ¨ç†ã€‚è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¯­ä¹‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨3Dåœºæ™¯ç”Ÿæˆä¸­çš„åº”ç”¨å—åˆ°å…¶å¯¹3Då‡ ä½•çŸ¥è¯†æŒæ¡æœ‰é™çš„é™åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¦‚ä½•åœ¨å¯¹è±¡æ”¾ç½®ä»»åŠ¡ä¸­ä¸MLLMsæœ€ä½³ååŒå·¥ä½œã€‚ä¸ºæ­¤ç›®æ ‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°å‹æ¡†æ¶FirePlaceï¼Œè¯¥æ¡†æ¶å°†ç°æœ‰MLLMsåº”ç”¨äºï¼ˆ1ï¼‰3Då‡ ä½•æ¨ç†å’Œä»3Dåœºæ™¯ä¸­æå–ç›¸å…³å‡ ä½•ç»†èŠ‚ï¼Œï¼ˆ2ï¼‰æ„å»ºå’Œè§£å†³æå–çš„ä½çº§å‡ ä½•çš„å‡ ä½•çº¦æŸï¼Œä»¥åŠï¼ˆ3ï¼‰ä¿®å‰ªæœ€ç»ˆæ”¾ç½®ä»¥ç¬¦åˆå¸¸è¯†ã€‚é€šè¿‡å°†å‡ ä½•æ¨ç†ä¸MLLMså¯¹ç°å®ä¸–ç•Œçš„ç†è§£ç›¸ç»“åˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæå‡ºå¯¹è±¡æ”¾ç½®æ–¹æ¡ˆï¼Œè¿™äº›æ–¹æ¡ˆæ—¢æ»¡è¶³å‡ ä½•çº¦æŸï¼Œåˆç¬¦åˆé«˜çº§è¯­ä¹‰å¸¸è¯†çš„è€ƒé‡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™äº›åŠŸèƒ½ä½¿æˆ‘ä»¬çš„æ–¹æ³•åœ¨å…·æœ‰å¤æ‚å‡ ä½•ç»“æ„çš„åœºæ™¯ä¸­æ”¾ç½®ç‰©ä½“æ—¶æ›´åŠ æœ‰æ•ˆï¼Œè¶…è¶Šäº†ä»¥å‰å·¥ä½œçš„è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04919v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¯¹è±¡æ”¾ç½®ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ä¸æœºé‡ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºFirePlaceçš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†MLLMsåœ¨è¯­ä¹‰ç†è§£æ–¹é¢çš„ä¼˜åŠ¿ä¸å¯¹ä¸‰ç»´å‡ ä½•çš„ç†è§£ï¼ŒåŒ…æ‹¬ä»ä¸‰ç»´åœºæ™¯ä¸­æå–å‡ ä½•ç»†èŠ‚ã€æ„å»ºå¹¶è§£å†³å‡ ä½•çº¦æŸä»¥åŠç¬¦åˆå¸¸è¯†çš„æœ€ç»ˆæ”¾ç½®ä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½åœ¨å¤æ‚åœºæ™¯ä¸­æ›´æœ‰æ•ˆåœ°æ”¾ç½®ç‰©ä½“ï¼Œè¾¾åˆ°è¶…è¶Šå…ˆå‰å·¥ä½œçš„è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¯­ä¹‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸‰ç»´åœºæ™¯ç”Ÿæˆæ–¹é¢å­˜åœ¨å¯¹ä¸‰ç»´å‡ ä½•ç†è§£çš„å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºFirePlaceçš„æ–°æ¡†æ¶ï¼Œç”¨äºå¤„ç†MLLMsåœ¨å¯¹è±¡æ”¾ç½®ä»»åŠ¡ä¸­çš„é™åˆ¶é—®é¢˜ã€‚</li>
<li>FirePlaceç»“åˆäº†ä¸‰ç»´å‡ ä½•æ¨ç†å’Œç°å®ä¸–ç•Œç†è§£ï¼Œå°†ç°æœ‰çš„MLLMsç”¨äºä»åœºæ™¯ä¸­æå–ç›¸å…³å‡ ä½•ç»†èŠ‚å’Œè¿›è¡Œæ¨ç†å¤„ç†ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½è§£å†³æå–çš„ä½çº§å‡ ä½•ä¸Šçš„å‡ ä½•çº¦æŸé—®é¢˜ã€‚</li>
<li>é€šè¿‡ä¿®å‰ªè¿‡ç¨‹ï¼Œç¡®ä¿æœ€ç»ˆæ”¾ç½®ç¬¦åˆå¸¸è¯†å’Œå‡ ä½•çº¦æŸã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸­æ”¾ç½®ç‰©ä½“æ—¶è¡¨ç°å‡ºæ›´é«˜çš„æ•ˆèƒ½å’Œè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04919">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c1bb88e4cd830eeb9b6a2fe2b0d114f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81a4fb9041046546d3cd3691328cc333.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0029abadffb5e4136c88c1d0a1ea1d5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e88ac0986d71bc13ebdaeaec8f6ca3c.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Learning-from-Failures-in-Multi-Attempt-Reinforcement-Learning"><a href="#Learning-from-Failures-in-Multi-Attempt-Reinforcement-Learning" class="headerlink" title="Learning from Failures in Multi-Attempt Reinforcement Learning"></a>Learning from Failures in Multi-Attempt Reinforcement Learning</h2><p><strong>Authors:Stephen Chung, Wenyu Du, Jie Fu</strong></p>
<p>Recent advancements in reinforcement learning (RL) for large language models (LLMs), exemplified by DeepSeek R1, have shown that even a simple question-answering task can substantially improve an LLMâ€™s reasoning capabilities. In this work, we extend this approach by modifying the task into a multi-attempt setting. Instead of generating a single response per question, the model is given multiple attempts, with feedback provided after incorrect responses. The multi-attempt task encourages the model to refine its previous attempts and improve search efficiency. Experimental results show that even a small LLM trained on a multi-attempt task achieves significantly higher accuracy when evaluated with more attempts, improving from 45.6% with 1 attempt to 52.5% with 2 attempts on the math benchmark. In contrast, the same LLM trained on a standard single-turn task exhibits only a marginal improvement, increasing from 42.3% to 43.2% when given more attempts during evaluation. The results indicate that, compared to the standard single-turn task, an LLM trained on a multi-attempt task achieves slightly better performance on math benchmarks while also learning to refine its responses more effectively based on user feedback. Full code is available at <a target="_blank" rel="noopener" href="https://github.com/DualityRL/multi-attempt">https://github.com/DualityRL/multi-attempt</a> </p>
<blockquote>
<p>è¿‘æœŸï¼Œä»¥DeepSeek R1ä¸ºä¾‹ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„åº”ç”¨å–å¾—äº†è¿›å±•ï¼Œè¡¨æ˜å³ä½¿æ˜¯ç®€å•çš„é—®ç­”ä»»åŠ¡ä¹Ÿèƒ½æ˜¾è‘—æé«˜LLMçš„æ¨ç†èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†ä»»åŠ¡æ”¹ä¸ºå¤šæ¬¡å°è¯•è®¾ç½®æ¥æ‰©å±•è¿™ç§æ–¹æ³•ã€‚ä¸ä¸ºæ¯ä¸ªé—®é¢˜ç”Ÿæˆä¸€æ¬¡å›åº”ä¸åŒï¼Œæ¨¡å‹è¢«ç»™äºˆå¤šæ¬¡å°è¯•çš„æœºä¼šï¼Œå¹¶åœ¨ä¸æ­£ç¡®çš„å›åº”ä¹‹åå¾—åˆ°åé¦ˆã€‚å¤šæ¬¡å°è¯•çš„ä»»åŠ¡é¼“åŠ±æ¨¡å‹æ”¹è¿›å…¶ä¹‹å‰çš„å°è¯•å¹¶æé«˜æœç´¢æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å°è¯•äº†å¤šæ¬¡ï¼Œç»è¿‡å¤šæ¬¡å°è¯•ä»»åŠ¡è®­ç»ƒçš„å°å‹LLMä¹Ÿå®ç°äº†æ˜¾è‘—æ›´é«˜çš„å‡†ç¡®æ€§ï¼Œä»å•æ¬¡å°è¯•çš„45.6%æé«˜åˆ°ä¸¤æ¬¡å°è¯•çš„52.5%ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨æ ‡å‡†å•å›åˆä»»åŠ¡ä¸Šè®­ç»ƒçš„åŒä¸€LLMåœ¨è¯„ä¼°æ—¶å³ä½¿å°è¯•æ›´å¤šæ¬¡ä¹Ÿåªèƒ½å®ç°å¾®å°çš„æ”¹è¿›ï¼Œä»42.3%æé«˜åˆ°43.2%ã€‚ç»“æœè¡¨æ˜ï¼Œä¸æ ‡å‡†çš„å•å›åˆä»»åŠ¡ç›¸æ¯”ï¼Œç»è¿‡å¤šæ¬¡å°è¯•ä»»åŠ¡è®­ç»ƒçš„LLMåœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ç¨å¥½ï¼ŒåŒæ—¶å­¦ä¹ æ ¹æ®ç”¨æˆ·åé¦ˆæ›´æœ‰æ•ˆåœ°æ”¹è¿›å…¶å›åº”ã€‚å®Œæ•´ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/DualityRL/multi-attempt">https://github.com/DualityRL/multi-attempt</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04808v1">PDF</a> preprint</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨æœ‰äº†æ–°çš„è¿›å±•ï¼Œé€šè¿‡å¤šå°è¯•ä»»åŠ¡è®­ç»ƒï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ç”¨æˆ·åé¦ˆçš„åŸºç¡€ä¸Šä¸æ–­ä¼˜åŒ–ç­”æ¡ˆå¹¶æå‡æœç´¢æ•ˆç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤šå°è¯•ä»»åŠ¡ä¸‹ï¼Œå°å‹è¯­è¨€æ¨¡å‹çš„å‡†ç¡®ç‡æœ‰æ˜æ˜¾æå‡ï¼Œä»å•æ¬¡å°è¯•çš„45.6%æå‡åˆ°ä¸¤æ¬¡å°è¯•çš„52.5%ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨æ ‡å‡†å•å›åˆä»»åŠ¡ä¸‹è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œå³ä¾¿å¢åŠ å°è¯•æ¬¡æ•°ï¼Œæ€§èƒ½ä¹Ÿåªå¾—åˆ°æœ‰é™æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿæ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¤šå°è¯•ä»»åŠ¡è®­ç»ƒï¼Œè¯­è¨€æ¨¡å‹èƒ½å¤Ÿåœ¨ç”¨æˆ·åé¦ˆçš„åŸºç¡€ä¸Šä¸æ–­ä¼˜åŒ–ç­”æ¡ˆã€‚</li>
<li>å¤šå°è¯•ä»»åŠ¡è®­ç»ƒçš„è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºæ ‡å‡†å•å›åˆä»»åŠ¡è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ã€‚</li>
<li>å¤šå°è¯•ä»»åŠ¡èƒ½å¤Ÿæé«˜è¯­è¨€æ¨¡å‹çš„æœç´¢æ•ˆç‡ã€‚</li>
<li>ä¸å•å›åˆä»»åŠ¡ç›¸æ¯”ï¼Œå¤šå°è¯•ä»»åŠ¡è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å±•ç°å‡ºæ›´å¼ºçš„æ€§èƒ½æå‡æ½œåŠ›ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯ï¼Œå¤šå°è¯•ä»»åŠ¡è®­ç»ƒå¯¹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½æœ‰æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04808">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-38e7d3b15e6dd95d37a8ee5d98509810.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b6dbc7b7e3f9c8eb223ec9be72fe5e74.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f7f7fb2ff1986b8e9a74b64d951055f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4a0a0bfdeea97de5953936ae43b1174.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0ec9246cb49e9899f57c9468643ac36f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74931fbbe675e6f72d5250edee853e0b.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="START-Self-taught-Reasoner-with-Tools"><a href="#START-Self-taught-Reasoner-with-Tools" class="headerlink" title="START: Self-taught Reasoner with Tools"></a>START: Self-taught Reasoner with Tools</h2><p><strong>Authors:Chengpeng Li, Mingfeng Xue, Zhenru Zhang, Jiaxi Yang, Beichen Zhang, Xiang Wang, Bowen Yu, Binyuan Hui, Junyang Lin, Dayiheng Liu</strong></p>
<p>Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-of-thought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes. In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing the limitations of LRMs. The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., &#96;&#96;Wait, maybe using Python here is a good idea.â€™â€™) during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by fine-tuning the LRM. Through this framework, we have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å¦‚OpenAI-o1å’ŒDeepSeek-R1ï¼Œå·²ç»å±•ç°å‡ºåœ¨å¤æ‚æ¨ç†ä»»åŠ¡æ–¹é¢çš„å“è¶Šèƒ½åŠ›ï¼Œé€šè¿‡è¿ç”¨é•¿çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¸¸å¸¸å› è¿‡åˆ†ä¾èµ–å†…éƒ¨æ¨ç†è¿‡ç¨‹è€Œé™·å…¥å¹»æƒ³å’Œæ•ˆç‡ä¸é«˜çš„å›°å¢ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†STARTï¼ˆå¸¦å·¥å…·çš„è‡ªæ•™æ¨ç†å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å·¥å…·é›†æˆé•¿CoTæ¨ç†LLMï¼Œå®ƒé€šè¿‡åˆ©ç”¨å¤–éƒ¨å·¥å…·æ¥æ˜¾è‘—å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ä»£ç æ‰§è¡Œï¼ŒSTARTèƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„è®¡ç®—ã€è‡ªæˆ‘æ£€æŸ¥ã€æ¢ç´¢å¤šç§æ–¹æ³•ä»¥åŠè‡ªæˆ‘è°ƒè¯•ï¼Œä»è€Œè§£å†³LRMçš„å±€é™æ€§ã€‚STARTçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶è‡ªå­¦ä¹ æ¡†æ¶ï¼Œå®ƒåŒ…å«ä¸¤ä¸ªå…³é”®æŠ€æœ¯ï¼š1ï¼‰æç¤ºæ¨æ–­ï¼šæˆ‘ä»¬è¯æ˜åœ¨LRMçš„æ¨ç†è¿‡ç¨‹ä¸­æ’å…¥äººå·¥è®¾è®¡çš„æç¤ºï¼ˆä¾‹å¦‚ï¼Œâ€œç­‰ç­‰ï¼Œä¹Ÿè®¸åœ¨è¿™é‡Œä½¿ç”¨Pythonæ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚â€ï¼‰èƒ½æœ‰æ•ˆåœ°åˆºæ¿€å…¶åˆ©ç”¨å¤–éƒ¨å·¥å…·çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€ä»»ä½•æ¼”ç¤ºæ•°æ®ã€‚æç¤ºæ¨æ–­è¿˜å¯ä»¥ä½œä¸ºä¸€ç§ç®€å•æœ‰æ•ˆçš„åºåˆ—æµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼›2ï¼‰æç¤ºæ‹’ç»é‡‡æ ·å¾®è°ƒï¼ˆHint-RFTï¼‰ï¼šHint-RFTç»“åˆäº†æç¤ºæ¨æ–­å’ŒRFTï¼Œé€šè¿‡å¯¹ç”±LRMé€šè¿‡æç¤ºæ¨æ–­äº§ç”Ÿçš„å¸¦æœ‰å·¥å…·è°ƒç”¨çš„æ¨ç†è½¨è¿¹è¿›è¡Œæ‰“åˆ†ã€è¿‡æ»¤å’Œä¿®æ”¹ï¼Œç„¶åå¯¹LRMè¿›è¡Œå¾®è°ƒã€‚é€šè¿‡è¿™ä¸ªæ¡†æ¶ï¼Œæˆ‘ä»¬å·²ç»å¾®è°ƒäº†QwQ-32Bæ¨¡å‹æ¥å®ç°STARTã€‚åœ¨åšå£«çº§ç§‘å­¦é—®ç­”ï¼ˆGPQAï¼‰ã€ç«èµ›çº§æ•°å­¦åŸºå‡†æµ‹è¯•ï¼ˆAMC23ã€AIME24ã€AIME25ï¼‰å’Œç«èµ›çº§ä»£ç åŸºå‡†æµ‹è¯•ï¼ˆLiveCodeBenchï¼‰ä¸­ï¼ŒSTARTçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º63.6%ã€95.0%ã€66.7%ã€47.1%å’Œ47.3%ã€‚å®ƒæ˜¾è‘—ä¼˜äºåŸºç¡€QwQ-32Bæ¨¡å‹ï¼Œå¹¶å®ç°äº†ä¸æœ€æ–°å¼€æºæ¨¡å‹R1-Distill-Qwen-32Bå’Œä¸“æœ‰æ¨¡å‹o1-Previewç›¸å½“çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>Translation into Simplified Chinese</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04625v2">PDF</a> 38 pages, 5 figures and 6 tables</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å¦‚OpenAI-o1å’ŒDeepSeek-R1ï¼Œåœ¨é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰çš„åº”ç”¨ä¸­å±•ç°å‡ºäº†å¼ºå¤§çš„å¤æ‚æ¨ç†èƒ½åŠ›ï¼Œä½†åŒæ—¶ä¹Ÿå­˜åœ¨æ˜“äº§ç”Ÿå¹»è§‰å’Œæ•ˆç‡ä¸é«˜çš„ç¼ºç‚¹ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†é›†æˆå·¥å…·çš„é•¿æœŸæ¨ç†å¤§æ¨¡å‹STARTã€‚STARTé€šè¿‡æ‰§è¡Œä»£ç å®ç°äº†å¤æ‚è®¡ç®—ã€è‡ªæˆ‘æ ¡éªŒã€æ¢ç´¢å¤šå…ƒæ–¹æ³•ä»¥åŠè‡ªæˆ‘è°ƒè¯•çš„åŠŸèƒ½ã€‚å…¶åˆ›æ–°ä¹‹å¤„åœ¨äºè‡ªæˆ‘å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®æŠ€å·§ï¼šHint-inferå’ŒHint Rejection Sampling Fine-Tuning (Hint-RFT)ã€‚STARTåœ¨æŸäº›ç‰¹å®šé¢†åŸŸç«èµ›ä¸Šçš„å‡†ç¡®ç‡é«˜äºåŸºç¡€æ¨¡å‹QwQ-32Bå¹¶è¾¾åˆ°è¡Œä¸šé¢†å…ˆæ¨¡å‹çš„æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å­˜åœ¨å¹»è§‰å’Œæ•ˆç‡é—®é¢˜ã€‚</li>
<li>STARTæ˜¯ä¸€ç§æ–°å‹å·¥å…·é›†æˆé•¿é“¾æ€ç»´æ¨ç†æ¨¡å‹ï¼Œèƒ½æ‰§è¡Œå¤æ‚è®¡ç®—ã€è‡ªæˆ‘æ ¡éªŒç­‰ï¼Œè§£å†³LRMsçš„å±€é™æ€§ã€‚</li>
<li>STARTçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶è‡ªæˆ‘å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬Hint-inferå’ŒHint-RFTä¸¤ä¸ªå…³é”®æŠ€å·§ã€‚</li>
<li>Hint-inferæŠ€å·§é€šè¿‡æ’å…¥äººå·¥æç¤ºæ¥åˆºæ¿€æ¨¡å‹åˆ©ç”¨å¤–éƒ¨å·¥å…·çš„èƒ½åŠ›ï¼Œæ— éœ€ç¤ºèŒƒæ•°æ®ã€‚</li>
<li>é€šè¿‡Hint-RFTæŠ€å·§ï¼Œå¯¹é€šè¿‡Hint-inferç”Ÿæˆçš„æ¨ç†è½¨è¿¹è¿›è¡Œè¯„åˆ†ã€è¿‡æ»¤å’Œä¿®æ”¹ï¼Œéšåå¾®è°ƒæ¨¡å‹ã€‚</li>
<li>STARTåœ¨æŸäº›ç‰¹å®šé¢†åŸŸç«èµ›ä¸Šçš„å‡†ç¡®ç‡é«˜äºåŸºç¡€æ¨¡å‹å¹¶è¾¾åˆ°è¡Œä¸šé¢†å…ˆæ°´å¹³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04625">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-09b4cfd7a2235ef490d54cea13d0bcb2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d8e1008273a0fae3d95f0f15ce7d3b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1231242d76bda54dfcf39dc57d7eeb65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc26dd16ba266590b8d800940822d12d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb51b72dc1ba94fcdbf1b5da0cf9b0ed.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ReSo-A-Reward-driven-Self-organizing-LLM-based-Multi-Agent-System-for-Reasoning-Tasks"><a href="#ReSo-A-Reward-driven-Self-organizing-LLM-based-Multi-Agent-System-for-Reasoning-Tasks" class="headerlink" title="ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for   Reasoning Tasks"></a>ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for   Reasoning Tasks</h2><p><strong>Authors:Heng Zhou, Hejia Geng, Xiangyuan Xue, Zhenfei Yin, Lei Bai</strong></p>
<p>Multi-agent systems have emerged as a promising approach for enhancing the reasoning capabilities of large language models in complex problem-solving. However, current MAS frameworks are limited by poor flexibility and scalability, with underdeveloped optimization strategies. To address these challenges, we propose ReSo, which integrates task graph generation with a reward-driven two-stage agent selection process. The core of ReSo is the proposed Collaborative Reward Model, which can provide fine-grained reward signals for MAS cooperation for optimization. We also introduce an automated data synthesis framework for generating MAS benchmarks, without human annotations. Experimentally, ReSo matches or outperforms existing methods. ReSo achieves \textbf{33.7%} and \textbf{32.3%} accuracy on Math-MAS and SciBench-MAS SciBench, while other methods completely fail. Code is available at: \href{<a target="_blank" rel="noopener" href="https://github.com/hengzzzhou/ReSo%7D%7BReSo%7D">https://github.com/hengzzzhou/ReSo}{ReSo}</a> </p>
<blockquote>
<p>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œåœ¨å¤æ‚é—®é¢˜è§£å†³çš„é¢†åŸŸé‡Œï¼Œç”¨äºæå‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¡†æ¶å—é™äºè¾ƒå·®çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä¼˜åŒ–ç­–ç•¥å°šä¸æˆç†Ÿã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReSoï¼Œå®ƒç»“åˆäº†ä»»åŠ¡å›¾ç”Ÿæˆå’Œå¥–åŠ±é©±åŠ¨çš„ä¸¤é˜¶æ®µæ™ºèƒ½ä½“é€‰æ‹©è¿‡ç¨‹ã€‚ReSoçš„æ ¸å¿ƒæ˜¯æå‡ºçš„ååŒå¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥ä¸ºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åˆä½œæä¾›ç²¾ç»†ç²’åº¦çš„å¥–åŠ±ä¿¡å·ä»¥å®ç°ä¼˜åŒ–ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æ•°æ®åˆæˆæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆæ— éœ€äººå·¥æ³¨è§£çš„å¤šæ™ºèƒ½ä½“ç³»ç»ŸåŸºå‡†æµ‹è¯•ã€‚é€šè¿‡å®éªŒï¼ŒReSoä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”è¡¨ç°æŒå¹³æˆ–æ›´èƒœä¸€ç­¹ã€‚åœ¨Math-MASå’ŒSciBench-MAS SciBenchä¸Šï¼ŒReSoåˆ†åˆ«è¾¾åˆ°äº†**33.7%<strong>å’Œ</strong>32.3%**çš„å‡†ç¡®ç‡ï¼Œè€Œå…¶ä»–æ–¹æ³•åˆ™å®Œå…¨å¤±è´¥ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hengzzzhou/ReSo">https://github.com/hengzzzhou/ReSo</a>å¤„è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02390v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿç”¨äºå¢å¼ºå¤§è¯­è¨€æ¨¡å‹åœ¨å¤æ‚é—®é¢˜ä¸­çš„æ¨ç†èƒ½åŠ›å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰MASæ¡†æ¶å—é™äºçµæ´»æ€§å’Œå¯æ‰©å±•æ€§ä¸è¶³ï¼Œä¼˜åŒ–ç­–ç•¥å°šä¸æˆç†Ÿã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºReSoæ–¹æ³•ï¼Œé›†æˆä»»åŠ¡å›¾ç”Ÿæˆä¸å¥–åŠ±é©±åŠ¨çš„ä¸¤é˜¶æ®µæ™ºèƒ½ä½“é€‰æ‹©æµç¨‹ã€‚æ ¸å¿ƒåœ¨äºæå‡ºåä½œå¥–åŠ±æ¨¡å‹ï¼Œä¸ºMASåˆä½œæä¾›ç²¾ç»†å¥–åŠ±ä¿¡å·ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚åŒæ—¶å¼•å…¥è‡ªåŠ¨åŒ–æ•°æ®åˆæˆæ¡†æ¶ï¼Œæ— éœ€äººå·¥æ ‡æ³¨å³å¯ç”ŸæˆMASåŸºå‡†æµ‹è¯•ã€‚å®éªŒæ˜¾ç¤ºï¼ŒReSoåœ¨Math-MASå’ŒSciBench-MASä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº†33.7%å’Œ32.3%ï¼Œè€Œå…¶ä»–æ–¹æ³•åˆ™å®Œå…¨å¤±æ•ˆã€‚ä»£ç å¯åœ¨ReSoï¼ˆé“¾æ¥ï¼‰å¤„ä¸‹è½½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿé€šè¿‡å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå±•ç°å‡ºåœ¨å¤æ‚é—®é¢˜å¤„ç†ä¸Šçš„æ½œåŠ›ã€‚</li>
<li>å½“å‰MASæ¡†æ¶é¢ä¸´çµæ´»æ€§å’Œå¯æ‰©å±•æ€§é—®é¢˜ï¼Œä¼˜åŒ–ç­–ç•¥äºŸå¾…æ”¹è¿›ã€‚</li>
<li>ReSoæ–¹æ³•é›†æˆäº†ä»»åŠ¡å›¾ç”Ÿæˆå’Œå¥–åŠ±é©±åŠ¨çš„ä¸¤é˜¶æ®µæ™ºèƒ½ä½“é€‰æ‹©æµç¨‹ï¼Œä»¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>åä½œå¥–åŠ±æ¨¡å‹æ˜¯ReSoçš„æ ¸å¿ƒï¼Œä¸ºMASåˆä½œæä¾›ç²¾ç»†å¥–åŠ±ä¿¡å·ä»¥å®ç°ä¼˜åŒ–ã€‚</li>
<li>ReSoå¼•å…¥äº†è‡ªåŠ¨åŒ–æ•°æ®åˆæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿç”ŸæˆMASåŸºå‡†æµ‹è¯•ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒReSoåœ¨Math-MASå’ŒSciBench-MASä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>ReSoæ–¹æ³•çš„å‡†ç¡®ç‡ä¸ºMath-MASçš„33.7%å’ŒSciBench-MASçš„32.3%ï¼Œè€Œå…¶ä»–æ–¹æ³•æœªèƒ½è¾¾åˆ°é¢„æœŸæ•ˆæœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-aee38a0299ff739fb2106c9649d1d63c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6b7ce059c252e0e00369fd17aab3a5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2342a6a6ee798a4c06bb51f1dd4d61dd.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="M3HF-Multi-agent-Reinforcement-Learning-from-Multi-phase-Human-Feedback-of-Mixed-Quality"><a href="#M3HF-Multi-agent-Reinforcement-Learning-from-Multi-phase-Human-Feedback-of-Mixed-Quality" class="headerlink" title="M3HF: Multi-agent Reinforcement Learning from Multi-phase Human Feedback   of Mixed Quality"></a>M3HF: Multi-agent Reinforcement Learning from Multi-phase Human Feedback   of Mixed Quality</h2><p><strong>Authors:Ziyan Wang, Zhicheng Zhang, Fei Fang, Yali Du</strong></p>
<p>Designing effective reward functions in multi-agent reinforcement learning (MARL) is a significant challenge, often leading to suboptimal or misaligned behaviors in complex, coordinated environments. We introduce Multi-agent Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality (M3HF), a novel framework that integrates multi-phase human feedback of mixed quality into the MARL training process. By involving humans with diverse expertise levels to provide iterative guidance, M3HF leverages both expert and non-expert feedback to continuously refine agentsâ€™ policies. During training, we strategically pause agent learning for human evaluation, parse feedback using large language models to assign it appropriately and update reward functions through predefined templates and adaptive weight by using weight decay and performance-based adjustments. Our approach enables the integration of nuanced human insights across various levels of quality, enhancing the interpretability and robustness of multi-agent cooperation. Empirical results in challenging environments demonstrate that M3HF significantly outperforms state-of-the-art methods, effectively addressing the complexities of reward design in MARL and enabling broader human participation in the training process. </p>
<blockquote>
<p>åœ¨å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä¸­è®¾è®¡æœ‰æ•ˆçš„å¥–åŠ±å‡½æ•°æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œé€šå¸¸ä¼šå¯¼è‡´åœ¨å¤æ‚ã€åè°ƒçš„ç¯å¢ƒä¸­è¡¨ç°å‡ºæ¬¡ä¼˜æˆ–é”™ä½çš„è¡Œä¸ºã€‚æˆ‘ä»¬å¼•å…¥äº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„æ··åˆè´¨é‡å¤šé˜¶æ®µäººç±»åé¦ˆï¼ˆM3HFï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†æ··åˆè´¨é‡çš„å¤šé˜¶æ®µäººç±»åé¦ˆæ•´åˆåˆ°MARLè®­ç»ƒè¿‡ç¨‹ä¸­ã€‚M3HFé€šè¿‡æ¶‰åŠå…·æœ‰ä¸åŒä¸“ä¸šæ°´å¹³çš„äººç±»æä¾›è¿­ä»£æŒ‡å¯¼ï¼Œåˆ©ç”¨ä¸“å®¶å’Œéä¸“å®¶çš„åé¦ˆæ¥æŒç»­æ”¹è¿›æ™ºèƒ½ä½“çš„ç­–ç•¥ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æˆ˜ç•¥æ€§åœ°åœ¨äººç±»è¯„ä¼°æ—¶æš‚åœæ™ºèƒ½ä½“çš„å­¦ä¹ ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è§£æåé¦ˆå¹¶é€‚å½“åœ°åˆ†é…ä»»åŠ¡ï¼Œå¹¶é€šè¿‡ä½¿ç”¨æƒé‡è¡°å‡å’ŒåŸºäºæ€§èƒ½çš„è°ƒæ•´æ¥æ›´æ–°å¥–åŠ±å‡½æ•°çš„é¢„è®¾æ¨¡æ¿å’Œè‡ªé€‚åº”æƒé‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ•´åˆå„ç§è´¨é‡æ°´å¹³ä¸Šçš„å¾®å¦™äººç±»è§è§£ï¼Œæé«˜å¤šæ™ºèƒ½ä½“åˆä½œçš„è§£é‡Šæ€§å’Œç¨³å¥æ€§ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­çš„ç»éªŒç»“æœè¡¨æ˜ï¼ŒM3HFæ˜¾è‘—ä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†MARLä¸­å¥–åŠ±è®¾è®¡å¤æ‚æ€§é—®é¢˜çš„åŒæ—¶ï¼Œä¹Ÿå®ç°äº†äººç±»æ›´å¹¿æ³›åœ°å‚ä¸åˆ°è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02077v2">PDF</a> Seventeen pages, four figures</p>
<p><strong>Summary</strong></p>
<p>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ä¸­è®¾è®¡æœ‰æ•ˆçš„å¥–åŠ±å‡½æ•°æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶åœ¨å¤æ‚åè°ƒç¯å¢ƒä¸­å¯èƒ½å¯¼è‡´æ¬¡ä¼˜æˆ–è¡Œä¸ºå¤±å‡†ã€‚æˆ‘ä»¬æå‡ºäº†å¤šé˜¶æ®µæ··åˆè´¨é‡åé¦ˆçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆM3HFï¼‰æ–°æ¡†æ¶ï¼Œå®ƒå°†å¤šé˜¶æ®µçš„äººæœºæ··åˆè´¨é‡åé¦ˆèå…¥åˆ°MARLè®­ç»ƒè¿‡ç¨‹ä¸­ã€‚M3HFé€šè¿‡æ¶‰åŠä¸åŒä¸“ä¸šç¨‹åº¦çš„äººç±»æä¾›è¿­ä»£æŒ‡å¯¼ï¼Œåˆ©ç”¨ä¸“å®¶å’Œæ–°æ‰‹åé¦ˆæ¥ä¸æ–­å¾®è°ƒæ™ºèƒ½ä½“çš„ç­–ç•¥ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æˆ˜ç•¥æ€§åœ°æš‚åœæ™ºèƒ½ä½“çš„å­¦ä¹ ä»¥ä¾›äººç±»è¯„ä¼°ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è§£æåé¦ˆå¹¶é€‚å½“åˆ†é…ä»»åŠ¡ï¼Œé€šè¿‡é¢„è®¾æ¨¡æ¿å’Œè‡ªé€‚åº”æƒé‡æ›´æ–°å¥–åŠ±å‡½æ•°ï¼Œé‡‡ç”¨æƒé‡è¡°å‡å’ŒåŸºäºæ€§èƒ½çš„è°ƒæ•´ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ•´åˆå„ç§è´¨é‡å±‚æ¬¡çš„äººç±»è§è§£ï¼Œæé«˜å¤šæ™ºèƒ½ä½“åˆä½œçš„è§£é‡Šæ€§å’Œç¨³å¥æ€§ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­çš„å®è¯ç»“æœè¡¨æ˜ï¼ŒM3HFæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæœ‰æ•ˆè§£å†³MARLä¸­å¥–åŠ±è®¾è®¡çš„å¤æ‚æ€§ï¼Œä½¿æ›´å¹¿æ³›çš„äººç±»å‚ä¸è®­ç»ƒè¿‡ç¨‹æˆä¸ºå¯èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰åœ¨è®¾è®¡å¥–åŠ±å‡½æ•°æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¯èƒ½å¯¼è‡´æ¬¡ä¼˜æˆ–è¡Œä¸ºå¤±å‡†ã€‚</li>
<li>M3HFæ¡†æ¶å°†å¤šé˜¶æ®µäººæœºæ··åˆè´¨é‡åé¦ˆèå…¥MARLè®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>M3HFåˆ©ç”¨ä¸åŒä¸“ä¸šç¨‹åº¦çš„äººç±»åé¦ˆæ¥å¾®è°ƒæ™ºèƒ½ä½“çš„ç­–ç•¥ã€‚</li>
<li>åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒM3HFæˆ˜ç•¥æ€§åœ°æš‚åœæ™ºèƒ½ä½“å­¦ä¹ ä»¥ä¾›äººç±»è¯„ä¼°ã€‚</li>
<li>M3HFä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è§£æäººç±»åé¦ˆå¹¶æ›´æ–°å¥–åŠ±å‡½æ•°ã€‚</li>
<li>M3HFé€šè¿‡é¢„è®¾æ¨¡æ¿å’Œè‡ªé€‚åº”æƒé‡ã€æƒé‡è¡°å‡å’ŒåŸºäºæ€§èƒ½è°ƒæ•´çš„æ–¹æ³•æ›´æ–°å¥–åŠ±å‡½æ•°ã€‚</li>
<li>M3HFæé«˜äº†å¤šæ™ºèƒ½ä½“åˆä½œçš„è§£é‡Šæ€§å’Œç¨³å¥æ€§ï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02077">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-461d46390e7699d84ee74b6233a3d4f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0891fa4edc9c9d837892084c104c620.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a0728f06061055baf04ed8e5a7f0dfc.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Why-Is-Spatial-Reasoning-Hard-for-VLMs-An-Attention-Mechanism-Perspective-on-Focus-Areas"><a href="#Why-Is-Spatial-Reasoning-Hard-for-VLMs-An-Attention-Mechanism-Perspective-on-Focus-Areas" class="headerlink" title="Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism   Perspective on Focus Areas"></a>Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism   Perspective on Focus Areas</h2><p><strong>Authors:Shiqi Chen, Tongyao Zhu, Ruochen Zhou, Jinghan Zhang, Siyang Gao, Juan Carlos Niebles, Mor Geva, Junxian He, Jiajun Wu, Manling Li</strong></p>
<p>Large Vision Language Models (VLMs) have long struggled with spatial reasoning tasks. Surprisingly, even simple spatial reasoning tasks, such as recognizing â€œunderâ€ or â€œbehindâ€ relationships between only two objects, pose significant challenges for current VLMs. In this work, we study the spatial reasoning challenge from the lens of mechanistic interpretability, diving into the modelâ€™s internal states to examine the interactions between image and text tokens. By tracing attention distribution over the image through out intermediate layers, we observe that successful spatial reasoning correlates strongly with the modelâ€™s ability to align its attention distribution with actual object locations, particularly differing between familiar and unfamiliar spatial relationships. Motivated by these findings, we propose ADAPTVIS based on inference-time confidence scores to sharpen the attention on highly relevant regions when confident, while smoothing and broadening the attention window to consider a wider context when confidence is lower. This training-free decoding method shows significant improvement (e.g., up to a 50 absolute point improvement) on spatial reasoning benchmarks such as WhatsUp and VSR with negligible cost. We make code and data publicly available for research purposes at <a target="_blank" rel="noopener" href="https://github.com/shiqichen17/AdaptVis">https://github.com/shiqichen17/AdaptVis</a>. </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é•¿æœŸä»¥æ¥ä¸€ç›´é¢ä¸´ç©ºé—´æ¨ç†ä»»åŠ¡çš„æŒ‘æˆ˜ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå³ä½¿æ˜¯ç®€å•çš„ç©ºé—´æ¨ç†ä»»åŠ¡ï¼Œå¦‚è¯†åˆ«ä¸¤ä¸ªå¯¹è±¡ä¹‹é—´çš„â€œåœ¨â€¦â€¦ä¸‹é¢â€æˆ–â€œåœ¨â€¦â€¦åé¢â€çš„å…³ç³»ï¼Œä¹Ÿå¯¹å½“å‰çš„VLMsæ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»æœºæ¢°å¯è§£é‡Šæ€§çš„è§’åº¦ç ”ç©¶ç©ºé—´æ¨ç†æŒ‘æˆ˜ï¼Œæ·±å…¥æ¨¡å‹çš„å†…éƒ¨çŠ¶æ€ï¼Œç ”ç©¶å›¾åƒå’Œæ–‡æœ¬æ ‡è®°ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚é€šè¿‡è¿½è¸ªå›¾åƒä¸Šå„å±‚çš„æ³¨æ„åŠ›åˆ†å¸ƒï¼Œæˆ‘ä»¬å‘ç°æˆåŠŸçš„ç©ºé—´æ¨ç†ä¸æ¨¡å‹å°†æ³¨æ„åŠ›åˆ†å¸ƒä¸å®é™…ç‰©ä½“ä½ç½®å¯¹é½çš„èƒ½åŠ›å¯†åˆ‡ç›¸å…³ï¼Œç‰¹åˆ«æ˜¯é¢å¯¹ç†Ÿæ‚‰å’Œé™Œç”Ÿçš„ç©ºé—´å…³ç³»æ—¶ã€‚å—è¿™äº›å‘ç°çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ¨ç†æ—¶é—´ç½®ä¿¡åº¦åˆ†æ•°çš„ADAPTVISæ–¹æ³•ï¼Œåœ¨ç½®ä¿¡åº¦è¾ƒé«˜æ—¶ï¼Œå°†æ³¨æ„åŠ›é›†ä¸­åœ¨é«˜åº¦ç›¸å…³çš„åŒºåŸŸä¸Šï¼Œè€Œåœ¨ç½®ä¿¡åº¦è¾ƒä½æ—¶ï¼Œå¹³æ»‘å¹¶æ‰©å¤§æ³¨æ„åŠ›çª—å£ä»¥è€ƒè™‘æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ã€‚è¿™ç§æ— éœ€è®­ç»ƒçš„è§£ç æ–¹æ³•åœ¨ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå¦‚WhatsUpå’ŒVSRï¼‰ä¸Šæ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ï¼ˆä¾‹å¦‚ï¼Œç»å¯¹æ”¹è¿›ç‚¹é«˜è¾¾50ç‚¹ï¼‰ï¼Œä¸”æˆæœ¬å¾®ä¹å…¶å¾®ã€‚æˆ‘ä»¬ä¸ºäº†ç ”ç©¶ç›®çš„å…¬å¼€äº†ä»£ç å’Œæ•°æ®ï¼š<a target="_blank" rel="noopener" href="https://github.com/shiqichen17/AdaptVis%E3%80%82">https://github.com/shiqichen17/AdaptVisã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01773v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†ç©ºé—´æ¨ç†ä»»åŠ¡æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡é€šè¿‡æœºåˆ¶æ€§è§£é‡Šæ–¹æ³•ï¼Œæ·±å…¥ç ”ç©¶äº†æ¨¡å‹å†…éƒ¨çŠ¶æ€ï¼Œè§‚å¯Ÿå›¾åƒå’Œæ–‡æœ¬ç¬¦å·ä¹‹é—´çš„äº¤äº’ä½œç”¨ã€‚ç ”ç©¶å‘ç°æˆåŠŸçš„ç©ºé—´æ¨ç†ä¸æ¨¡å‹çš„æ³¨æ„åŠ›åˆ†å¸ƒä¸å®é™…ç‰©ä½“ä½ç½®çš„åŒ¹é…èƒ½åŠ›å¯†åˆ‡ç›¸å…³ï¼Œç‰¹åˆ«æ˜¯é¢å¯¹ç†Ÿæ‚‰å’Œé™Œç”Ÿçš„ç©ºé—´å…³ç³»æ—¶ã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºæ¨ç†æ—¶é—´ç½®ä¿¡åº¦å¾—åˆ†çš„ADAPTVISæ–¹æ³•ï¼Œåœ¨ç½®ä¿¡åº¦é«˜æ—¶é”åŒ–å¯¹é‡è¦åŒºåŸŸçš„æ³¨æ„åŠ›ï¼Œåœ¨ç½®ä¿¡åº¦ä½æ—¶å¹³æ»‘å¹¶æ‰©å¤§æ³¨æ„åŠ›èŒƒå›´ä»¥è€ƒè™‘æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ã€‚è¿™ç§æ— éœ€è®­ç»ƒçš„è§£ç æ–¹æ³•åœ¨ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œå¦‚WhatsUpå’ŒVSRï¼Œæ”¹è¿›å¹…åº¦é«˜è¾¾50ä¸ªç»å¯¹ç‚¹ï¼Œä¸”æˆæœ¬è¾ƒä½ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å·²å…¬å¼€å‘å¸ƒï¼Œä¾›ç ”ç©¶ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLMsåœ¨å¤„ç†ç®€å•ç©ºé—´æ¨ç†ä»»åŠ¡æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æˆåŠŸçš„ç©ºé—´æ¨ç†ä¸æ¨¡å‹æ³¨æ„åŠ›åˆ†å¸ƒä¸ç‰©ä½“ä½ç½®çš„åŒ¹é…èƒ½åŠ›æœ‰å…³ã€‚</li>
<li>ç†Ÿæ‚‰ä¸é™Œç”Ÿçš„ç©ºé—´å…³ç³»å¤„ç†åœ¨æ¨¡å‹å†…éƒ¨æœ‰ä¸åŒçš„è¡¨ç°ã€‚</li>
<li>ADAPTVISæ–¹æ³•åŸºäºæ¨ç†æ—¶é—´ç½®ä¿¡åº¦å¾—åˆ†æ¥è°ƒæ•´æ³¨æ„åŠ›åˆ†å¸ƒã€‚</li>
<li>ADAPTVISåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—æ”¹è¿›ï¼Œå¦‚WhatsUpå’ŒVSRã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•æ— éœ€é¢å¤–è®­ç»ƒï¼Œä¸”å…·æœ‰è¾ƒä½çš„æˆæœ¬ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01773">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-894aec5e6476397db43fc319d4e1630a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-921f70332f7976e39082bc7496a086b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9af0c2fa3bbb617831282dee3b668456.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d564834cc3e200105e3a00d55251a27c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7807762bc1fa9d00d8d28f0145acb0a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b536b7908dfd5986fdbc3713f9088fbd.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="HarmonySet-A-Comprehensive-Dataset-for-Understanding-Video-Music-Semantic-Alignment-and-Temporal-Synchronization"><a href="#HarmonySet-A-Comprehensive-Dataset-for-Understanding-Video-Music-Semantic-Alignment-and-Temporal-Synchronization" class="headerlink" title="HarmonySet: A Comprehensive Dataset for Understanding Video-Music   Semantic Alignment and Temporal Synchronization"></a>HarmonySet: A Comprehensive Dataset for Understanding Video-Music   Semantic Alignment and Temporal Synchronization</h2><p><strong>Authors:Zitang Zhou, Ke Mei, Yu Lu, Tianyi Wang, Fengyun Rao</strong></p>
<p>This paper introduces HarmonySet, a comprehensive dataset designed to advance video-music understanding. HarmonySet consists of 48,328 diverse video-music pairs, annotated with detailed information on rhythmic synchronization, emotional alignment, thematic coherence, and cultural relevance. We propose a multi-step human-machine collaborative framework for efficient annotation, combining human insights with machine-generated descriptions to identify key transitions and assess alignment across multiple dimensions. Additionally, we introduce a novel evaluation framework with tasks and metrics to assess the multi-dimensional alignment of video and music, including rhythm, emotion, theme, and cultural context. Our extensive experiments demonstrate that HarmonySet, along with the proposed evaluation framework, significantly improves the ability of multimodal models to capture and analyze the intricate relationships between video and music. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†HarmonySetï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºäº†æ¨è¿›è§†é¢‘éŸ³ä¹ç†è§£è€Œè®¾è®¡çš„ç»¼åˆæ•°æ®é›†ã€‚HarmonySetåŒ…å«48,328ä¸ªå¤šæ ·åŒ–çš„è§†é¢‘éŸ³ä¹å¯¹ï¼Œå¹¶æ³¨æ˜äº†å…³äºèŠ‚å¥åŒæ­¥ã€æƒ…æ„Ÿå¯¹é½ã€ä¸»é¢˜è¿è´¯æ€§å’Œæ–‡åŒ–ç›¸å…³æ€§çš„è¯¦ç»†ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ­¥éª¤çš„äººæœºåä½œæ¡†æ¶ï¼Œç”¨äºæœ‰æ•ˆæ ‡æ³¨ï¼Œç»“åˆäººç±»è§è§£å’Œæœºå™¨ç”Ÿæˆçš„æè¿°æ¥è¯†åˆ«å…³é”®è¿‡æ¸¡å¹¶è¯„ä¼°å¤šä¸ªç»´åº¦ä¸Šçš„å¯¹é½æƒ…å†µã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬ä»»åŠ¡å’ŒæŒ‡æ ‡ï¼Œä»¥è¯„ä¼°è§†é¢‘å’ŒéŸ³ä¹çš„å¤šç»´å¯¹é½ï¼ŒåŒ…æ‹¬èŠ‚å¥ã€æƒ…æ„Ÿã€ä¸»é¢˜å’Œæ–‡åŒ–èƒŒæ™¯ã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHarmonySetä»¥åŠæå‡ºçš„è¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜å¤šæ¨¡æ€æ¨¡å‹æ•æ‰å’Œåˆ†æè§†é¢‘å’ŒéŸ³ä¹ä¹‹é—´å¤æ‚å…³ç³»çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01725v2">PDF</a> Accepted at CVPR 2025. Project page: <a target="_blank" rel="noopener" href="https://harmonyset.github.io/">https://harmonyset.github.io/</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†HarmonySetæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ—¨åœ¨ä¿ƒè¿›è§†é¢‘éŸ³ä¹ç†è§£çš„å‘å±•ã€‚HarmonySetåŒ…å«48ï¼Œ328ä¸ªå¤šæ ·åŒ–çš„è§†é¢‘éŸ³ä¹å¯¹ï¼Œå¹¶è¯¦ç»†æ ‡æ³¨äº†èŠ‚å¥åŒæ­¥ã€æƒ…æ„Ÿå¯¹é½ã€ä¸»é¢˜è¿è´¯æ€§å’Œæ–‡åŒ–ç›¸å…³æ€§ç­‰ä¿¡æ¯ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§äººæœºåä½œçš„æ ‡æ³¨æ¡†æ¶ï¼Œç»“åˆäººç±»æ´å¯ŸåŠ›å’Œæœºå™¨ç”Ÿæˆçš„æè¿°æ¥è¯†åˆ«å…³é”®è¿‡æ¸¡å¹¶è¯„ä¼°å¤šä¸ªç»´åº¦çš„å¯¹é½æƒ…å†µã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬ä»»åŠ¡å’ŒæŒ‡æ ‡ï¼Œä»¥è¯„ä¼°è§†é¢‘å’ŒéŸ³ä¹çš„å¤šç»´å¯¹é½ï¼Œå¦‚èŠ‚å¥ã€æƒ…æ„Ÿã€ä¸»é¢˜å’Œæ–‡åŒ–èƒŒæ™¯ã€‚å®éªŒè¡¨æ˜ï¼ŒHarmonySetåŠè¯„ä¼°æ¡†æ¶æ˜¾è‘—æé«˜äº†å¤šæ¨¡å¼æ¨¡å‹æ•æ‰å’Œåˆ†æè§†é¢‘ä¸éŸ³ä¹ä¹‹é—´å¤æ‚å…³ç³»çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>HarmonySetæ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„æ•°æ®é›†ï¼Œç”¨äºæ¨è¿›è§†é¢‘éŸ³ä¹ç†è§£çš„ç ”ç©¶ã€‚</li>
<li>æ•°æ®é›†åŒ…å«48ï¼Œ328ä¸ªå¤šæ ·åŒ–è§†é¢‘éŸ³ä¹å¯¹ï¼Œè¯¦ç»†æ ‡æ³¨äº†èŠ‚å¥åŒæ­¥ç­‰ä¿¡æ¯ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªäººæœºåä½œçš„æ ‡æ³¨æ¡†æ¶ï¼Œèƒ½é«˜æ•ˆè¯†åˆ«å…³é”®è¿‡æ¸¡å¹¶è¯„ä¼°å¤šä¸ªç»´åº¦çš„å¯¹é½æƒ…å†µã€‚</li>
<li>å¼•å…¥äº†æ–°çš„è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°è§†é¢‘å’ŒéŸ³ä¹çš„å¤šç»´å¯¹é½ã€‚</li>
<li>è¯¥æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶æœ‰åŠ©äºæé«˜æ¨¡å‹æ•æ‰è§†é¢‘ä¸éŸ³ä¹ä¹‹é—´å¤æ‚å…³ç³»çš„èƒ½åŠ›ã€‚</li>
<li>æ•°æ®é›†æ³¨é‡æ–‡åŒ–ç›¸å…³æ€§ï¼Œæœ‰åŠ©äºç†è§£å’Œåˆ†æä¸åŒæ–‡åŒ–èƒŒæ™¯ä¸‹çš„è§†é¢‘éŸ³ä¹äº¤äº’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01725">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4cf39f58004f6fd073d3a94c65f81a67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6fc744b2ed06235866916e47fe0c2d9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbc20f499c866f12b62790a7b7b6969e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9c3d7142781ecca54acf45c021f3aa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a410a057cecc89989f81bb8ea93cd51.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Cancer-Type-Stage-and-Prognosis-Assessment-from-Pathology-Reports-using-LLMs"><a href="#Cancer-Type-Stage-and-Prognosis-Assessment-from-Pathology-Reports-using-LLMs" class="headerlink" title="Cancer Type, Stage and Prognosis Assessment from Pathology Reports using   LLMs"></a>Cancer Type, Stage and Prognosis Assessment from Pathology Reports using   LLMs</h2><p><strong>Authors:Rachit Saluja, Jacob Rosenthal, Yoav Artzi, David J. Pisapia, Benjamin L. Liechty, Mert R. Sabuncu</strong></p>
<p>Large Language Models (LLMs) have shown significant promise across various natural language processing tasks. However, their application in the field of pathology, particularly for extracting meaningful insights from unstructured medical texts such as pathology reports, remains underexplored and not well quantified. In this project, we leverage state-of-the-art language models, including the GPT family, Mistral models, and the open-source Llama models, to evaluate their performance in comprehensively analyzing pathology reports. Specifically, we assess their performance in cancer type identification, AJCC stage determination, and prognosis assessment, encompassing both information extraction and higher-order reasoning tasks. Based on a detailed analysis of their performance metrics in a zero-shot setting, we developed two instruction-tuned models: Path-llama3.1-8B and Path-GPT-4o-mini-FT. These models demonstrated superior performance in zero-shot cancer type identification, staging, and prognosis assessment compared to the other models evaluated. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç—…ç†å­¦é¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»ç—…ç†æŠ¥å‘Šç­‰æ— ç»“æ„åŒ»å­¦æ–‡æœ¬ä¸­æå–æœ‰æ„ä¹‰è§è§£æ–¹é¢ï¼Œä»è¢«æ¢ç´¢å¾—ä¸å¤Ÿæ·±å…¥ä¸”æœªå¾—åˆ°å¾ˆå¥½çš„é‡åŒ–ã€‚åœ¨æœ¬é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬GPTç³»åˆ—ã€Mistralæ¨¡å‹å’Œå¼€æºçš„Llamaæ¨¡å‹ï¼Œæ¥è¯„ä¼°å®ƒä»¬åœ¨ç»¼åˆåˆ†æç—…ç†æŠ¥å‘Šæ–¹é¢çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å®ƒä»¬åœ¨ç™Œç—‡ç±»å‹è¯†åˆ«ã€AJCCåˆ†æœŸç¡®å®šå’Œé¢„åè¯„ä¼°æ–¹é¢çš„æ€§èƒ½ï¼Œè¿™äº›è¯„ä¼°æ¶µç›–äº†ä¿¡æ¯æå–å’Œé«˜çº§æ¨ç†ä»»åŠ¡ã€‚åŸºäºå¯¹é›¶æ ·æœ¬è®¾ç½®ä¸‹æ€§èƒ½æŒ‡æ ‡çš„è¯¦ç»†åˆ†æï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸¤ä¸ªæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ï¼šPath-llama3.1-8Bå’ŒPath-GPT-4o-mini-FTã€‚è¿™äº›æ¨¡å‹åœ¨é›¶æ ·æœ¬ç™Œç—‡ç±»å‹è¯†åˆ«ã€åˆ†æœŸå’Œé¢„åè¯„ä¼°æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¸å…¶ä»–è¯„ä¼°çš„æ¨¡å‹ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01194v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨ç—…ç†å­¦é¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»ç—…ç†æŠ¥å‘Šç­‰æ— ç»“æ„åŒ»å­¦æ–‡æœ¬ä¸­æå–æœ‰æ„ä¹‰ä¿¡æ¯æ–¹é¢ï¼Œå…¶åº”ç”¨å°šå¾…è¿›ä¸€æ­¥æ¢ç´¢å’Œç ”ç©¶ã€‚æœ¬é¡¹ç›®åˆ©ç”¨æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬GPTç³»åˆ—ã€Mistralæ¨¡å‹å’Œå¼€æºLlamaæ¨¡å‹ï¼Œè¯„ä¼°å®ƒä»¬åœ¨ç»¼åˆåˆ†æç—…ç†æŠ¥å‘Šæ–¹é¢çš„æ€§èƒ½ã€‚é€šè¿‡é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„æ€§èƒ½åˆ†æï¼Œå¼€å‘äº†ä¸¤ä¸ªæŒ‡ä»¤è°ƒä¼˜æ¨¡å‹Path-llama3.1-8Bå’ŒPath-GPT-4o-mini-FTï¼Œåœ¨é›¶æ ·æœ¬ç™Œç—‡ç±»å‹è¯†åˆ«ã€åˆ†æœŸå’Œé¢„åè¯„ä¼°æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>åœ¨ç—…ç†å­¦é¢†åŸŸï¼ŒLLMsçš„åº”ç”¨ç‰¹åˆ«æ˜¯åœ¨ä»ç—…ç†æŠ¥å‘Šæå–ä¿¡æ¯æ–¹é¢å°šå¾…æ¢ç´¢å’Œç ”ç©¶ã€‚</li>
<li>æœ¬é¡¹ç›®åˆ©ç”¨å¤šç§æœ€å…ˆè¿›è¯­è¨€æ¨¡å‹è¯„ä¼°åœ¨ç—…ç†æŠ¥å‘Šåˆ†æä¸­çš„æ€§èƒ½ã€‚</li>
<li>è¯„ä¼°å†…å®¹åŒ…æ‹¬ç™Œç—‡ç±»å‹è¯†åˆ«ã€AJCCåˆ†æœŸå’Œé¢„åè¯„ä¼°ã€‚</li>
<li>é€šè¿‡é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„æ€§èƒ½åˆ†æï¼Œå¼€å‘äº†ä¸¤ä¸ªæŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ã€‚</li>
<li>è¿™ä¸¤ä¸ªæ¨¡å‹åœ¨ç™Œç—‡ç±»å‹è¯†åˆ«ã€åˆ†æœŸå’Œé¢„åè¯„ä¼°æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01194">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f9b477ec0e225601a11bd14523441e7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a00f2d514a91775b1343adee35e44eb0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6198bf5d8c1936749922a1a3c2334ec5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c73dea0ae000f5919209434f8ecfe944.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="An-evaluation-of-DeepSeek-Models-in-Biomedical-Natural-Language-Processing"><a href="#An-evaluation-of-DeepSeek-Models-in-Biomedical-Natural-Language-Processing" class="headerlink" title="An evaluation of DeepSeek Models in Biomedical Natural Language   Processing"></a>An evaluation of DeepSeek Models in Biomedical Natural Language   Processing</h2><p><strong>Authors:Zaifu Zhan, Shuang Zhou, Huixue Zhou, Jiawen Deng, Yu Hou, Jeremy Yeung, Rui Zhang</strong></p>
<p>The advancement of Large Language Models (LLMs) has significantly impacted biomedical Natural Language Processing (NLP), enhancing tasks such as named entity recognition, relation extraction, event extraction, and text classification. In this context, the DeepSeek series of models have shown promising potential in general NLP tasks, yet their capabilities in the biomedical domain remain underexplored. This study evaluates multiple DeepSeek models (Distilled-DeepSeek-R1 series and Deepseek-LLMs) across four key biomedical NLP tasks using 12 datasets, benchmarking them against state-of-the-art alternatives (Llama3-8B, Qwen2.5-7B, Mistral-7B, Phi-4-14B, Gemma-2-9B). Our results reveal that while DeepSeek models perform competitively in named entity recognition and text classification, challenges persist in event and relation extraction due to precision-recall trade-offs. We provide task-specific model recommendations and highlight future research directions. This evaluation underscores the strengths and limitations of DeepSeek models in biomedical NLP, guiding their future deployment and optimization. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥å¯¹ç”Ÿç‰©åŒ»å­¦è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰äº§ç”Ÿäº†é‡å¤§å½±å“ï¼Œå¢å¼ºäº†å‘½åå®ä½“è¯†åˆ«ã€å…³ç³»æŠ½å–ã€äº‹ä»¶æŠ½å–å’Œæ–‡æœ¬åˆ†ç±»ç­‰ä»»åŠ¡ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼ŒDeepSeekç³»åˆ—æ¨¡å‹åœ¨ä¸€èˆ¬NLPä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„èƒ½åŠ›ä»ç„¶è¢«ä½ä¼°ã€‚æœ¬ç ”ç©¶ä½¿ç”¨12ä¸ªæ•°æ®é›†ï¼Œåœ¨å››ä¸ªå…³é”®çš„ç”Ÿç‰©åŒ»å­¦NLPä»»åŠ¡ä¸Šè¯„ä¼°äº†å¤šä¸ªDeepSeekæ¨¡å‹ï¼ˆDistilled-DeepSeek-R1ç³»åˆ—å’ŒDeepseek-LLMsï¼‰ï¼Œå¹¶å°†å®ƒä»¬ä¸æœ€æ–°çš„æ›¿ä»£æ–¹æ¡ˆï¼ˆLlama3-8Bã€Qwen2.5-7Bã€Mistral-7Bã€Phi-4-14Bã€Gemma-2-9Bï¼‰è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒDeepSeekæ¨¡å‹åœ¨å‘½åå®ä½“è¯†åˆ«å’Œæ–‡æœ¬åˆ†ç±»æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†ç”±äºç²¾ç¡®ç‡å’Œå¬å›ç‡ä¹‹é—´çš„æƒè¡¡ï¼Œäº‹ä»¶å’Œå…³ç³»æŠ½å–ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬æä¾›äº†é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ¨¡å‹æ¨èå¹¶å¼ºè°ƒäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚è¿™ä¸€è¯„ä¼°å¼ºè°ƒäº†DeepSeekæ¨¡å‹åœ¨ç”Ÿç‰©åŒ»å­¦NLPä¸­çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œä¸ºæœªæ¥çš„éƒ¨ç½²å’Œä¼˜åŒ–æä¾›äº†æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00624v1">PDF</a> Plan to submit to AMIA 2025 Annual Symposium. 10 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘å±•å¯¹ç”Ÿç‰©åŒ»å­¦è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰äº§ç”Ÿäº†é‡å¤§å½±å“ï¼Œæé«˜äº†å®ä½“å‘½åè¯†åˆ«ã€å…³ç³»æŠ½å–ã€äº‹ä»¶æŠ½å–å’Œæ–‡æœ¬åˆ†ç±»ç­‰ä»»åŠ¡çš„æ•ˆæœã€‚DeepSeekç³»åˆ—æ¨¡å‹åœ¨é€šç”¨NLPä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„åº”ç”¨å°šå¾…æ¢ç´¢ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†DeepSeekæ¨¡å‹åœ¨å››ä¸ªå…³é”®ç”Ÿç‰©åŒ»å­¦NLPä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œä¸å‰æ²¿çš„LLMsè¿›è¡Œå¯¹æ¯”ã€‚ç»“æœæ˜¾ç¤ºï¼ŒDeepSeekæ¨¡å‹åœ¨å®ä½“å‘½åè¯†åˆ«å’Œæ–‡æœ¬åˆ†ç±»æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨äº‹ä»¶å’Œå…³ç³»æŠ½å–æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶ä¸ºç‰¹å®šä»»åŠ¡çš„æ¨¡å‹é€‰æ‹©å’Œæœªæ¥ç ”ç©¶æ–¹å‘æä¾›äº†æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹ç”Ÿç‰©åŒ»å­¦è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰äº§ç”Ÿäº†é‡è¦å½±å“ã€‚</li>
<li>DeepSeekç³»åˆ—æ¨¡å‹åœ¨é€šç”¨NLPä»»åŠ¡ä¸­è¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„åº”ç”¨å°šå¾…å……åˆ†æ¢ç´¢ã€‚</li>
<li>æœ¬ç ”ç©¶è¯„ä¼°äº†DeepSeekæ¨¡å‹åœ¨å››ä¸ªå…³é”®ç”Ÿç‰©åŒ»å­¦NLPä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>DeepSeekæ¨¡å‹åœ¨å®ä½“å‘½åè¯†åˆ«å’Œæ–‡æœ¬åˆ†ç±»æ–¹é¢è¡¨ç°è‰¯å¥½ã€‚</li>
<li>åœ¨äº‹ä»¶å’Œå…³ç³»æŠ½å–ä»»åŠ¡ä¸­ï¼ŒDeepSeekæ¨¡å‹é¢ä¸´æŒ‘æˆ˜ï¼Œå­˜åœ¨ç²¾åº¦å’Œå¬å›ç‡çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>ç ”ç©¶æä¾›äº†é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ¨¡å‹æ¨èã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00624">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7523545fc9d7d90cd796e22ecf4c2e85.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-086d77a803893aa9d6626be173b2e032.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d162bb260f08f034456d352e0bbbea1.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Inst3D-LMM-Instance-Aware-3D-Scene-Understanding-with-Multi-modal-Instruction-Tuning"><a href="#Inst3D-LMM-Instance-Aware-3D-Scene-Understanding-with-Multi-modal-Instruction-Tuning" class="headerlink" title="Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal   Instruction Tuning"></a>Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal   Instruction Tuning</h2><p><strong>Authors:Hanxun Yu, Wentong Li, Song Wang, Junbo Chen, Jianke Zhu</strong></p>
<p>Despite encouraging progress in 3D scene understanding, it remains challenging to develop an effective Large Multi-modal Model (LMM) that is capable of understanding and reasoning in complex 3D environments. Most previous methods typically encode 3D point and 2D image features separately, neglecting interactions between 2D semantics and 3D object properties, as well as the spatial relationships within the 3D environment. This limitation not only hinders comprehensive representations of 3D scene, but also compromises training and inference efficiency. To address these challenges, we propose a unified Instance-aware 3D Large Multi-modal Model (Inst3D-LMM) to deal with multiple 3D scene understanding tasks simultaneously. To obtain the fine-grained instance-level visual tokens, we first introduce a novel Multi-view Cross-Modal Fusion (MCMF) module to inject the multi-view 2D semantics into their corresponding 3D geometric features. For scene-level relation-aware tokens, we further present a 3D Instance Spatial Relation (3D-ISR) module to capture the intricate pairwise spatial relationships among objects. Additionally, we perform end-to-end multi-task instruction tuning simultaneously without the subsequent task-specific fine-tuning. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods across 3D scene understanding, reasoning and grounding tasks. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/hanxunyu/Inst3D-LMM">https://github.com/hanxunyu/Inst3D-LMM</a> </p>
<blockquote>
<p>å°½ç®¡åœ¨ä¸‰ç»´åœºæ™¯ç†è§£æ–¹é¢å–å¾—äº†ä»¤äººé¼“èˆçš„è¿›å±•ï¼Œä½†å¼€å‘èƒ½å¤Ÿåœ¨å¤æ‚ä¸‰ç»´ç¯å¢ƒä¸­è¿›è¡Œç†è§£å’Œæ¨ç†çš„æœ‰æ•ˆå¤§å‹å¤šæ¨¡å¼æ¨¡å‹ï¼ˆLMMï¼‰ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å¤§å¤šæ•°ä¹‹å‰çš„æ–¹æ³•é€šå¸¸åˆ†åˆ«ç¼–ç ä¸‰ç»´ç‚¹äº‘å’ŒäºŒç»´å›¾åƒç‰¹å¾ï¼Œå¿½ç•¥äº†äºŒç»´è¯­ä¹‰å’Œä¸‰ç»´å¯¹è±¡å±æ€§ä¹‹é—´çš„äº¤äº’ï¼Œä»¥åŠä¸‰ç»´ç¯å¢ƒå†…çš„ç©ºé—´å…³ç³»ã€‚è¿™ç§å±€é™æ€§ä¸ä»…é˜»ç¢äº†ä¸‰ç»´åœºæ™¯çš„ç»¼åˆè¡¨ç¤ºï¼Œè¿˜å½±å“äº†è®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å®ä¾‹æ„ŸçŸ¥ä¸‰ç»´å¤§å‹å¤šæ¨¡å¼æ¨¡å‹ï¼ˆInst3D-LMMï¼‰ï¼Œä»¥åŒæ—¶å¤„ç†å¤šä¸ªä¸‰ç»´åœºæ™¯ç†è§£ä»»åŠ¡ã€‚ä¸ºäº†è·å¾—ç²¾ç»†çš„å®ä¾‹çº§è§†è§‰æ ‡è®°ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ç§æ–°å‹çš„å¤šè§†è§’è·¨æ¨¡å¼èåˆï¼ˆMCMFï¼‰æ¨¡å—ï¼Œå°†å¤šè§†è§’çš„äºŒç»´è¯­ä¹‰æ³¨å…¥åˆ°å…¶å¯¹åº”çš„ä¸‰ç»´å‡ ä½•ç‰¹å¾ä¸­ã€‚å¯¹äºåœºæ™¯çº§çš„å…³ç³»æ„ŸçŸ¥æ ‡è®°ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§ä¸‰ç»´å®ä¾‹ç©ºé—´å…³ç³»ï¼ˆ3D-ISRï¼‰æ¨¡å—ï¼Œä»¥æ•è·å¯¹è±¡ä¹‹é—´å¤æ‚çš„é…å¯¹ç©ºé—´å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ç«¯åˆ°ç«¯çš„å¤šä»»åŠ¡æŒ‡ä»¤è°ƒæ•´ï¼ŒåŒæ—¶è¿›è¡Œäº†å¤šé¡¹å®éªŒï¼Œæ— éœ€åç»­çš„ä»»åŠ¡ç‰¹å®šå¾®è°ƒã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ç»´åœºæ™¯ç†è§£ã€æ¨ç†å’Œå®šä½ä»»åŠ¡ä¸Šä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hanxunyu/Inst3D-LMM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hanxunyu/Inst3D-LMMæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00513v1">PDF</a> CVPR2025, Code Link: <a target="_blank" rel="noopener" href="https://github.com/hanxunyu/Inst3D-LMM">https://github.com/hanxunyu/Inst3D-LMM</a></p>
<p><strong>Summary</strong>ï¼š<br>å°½ç®¡åœ¨ä¸‰ç»´åœºæ™¯ç†è§£æ–¹é¢å–å¾—äº†ä»¤äººé¼“èˆçš„è¿›å±•ï¼Œä½†å¼€å‘ä¸€ç§èƒ½å¤Ÿç†è§£å’Œæ¨ç†å¤æ‚ä¸‰ç»´ç¯å¢ƒçš„æœ‰æ•ˆå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å…ˆå‰çš„å¤§å¤šæ•°æ–¹æ³•é€šå¸¸åˆ†åˆ«ç¼–ç ä¸‰ç»´ç‚¹ç‰¹å¾å’ŒäºŒç»´å›¾åƒç‰¹å¾ï¼Œå¿½ç•¥äº†äºŒç»´è¯­ä¹‰å’Œä¸‰ç»´å¯¹è±¡å±æ€§ä¹‹é—´çš„äº¤äº’ï¼Œä»¥åŠä¸‰ç»´ç¯å¢ƒå†…çš„ç©ºé—´å…³ç³»ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å®ä¾‹æ„ŸçŸ¥ä¸‰ç»´å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆInst3D-LMMï¼‰ï¼Œå¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªä¸‰ç»´åœºæ™¯ç†è§£ä»»åŠ¡ã€‚é€šè¿‡å¼•å…¥å¤šè§†å›¾è·¨æ¨¡æ€èåˆï¼ˆMCMFï¼‰æ¨¡å—å’Œä¸‰ç»´å®ä¾‹ç©ºé—´å…³ç³»ï¼ˆ3D-ISRï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè·å–ç²¾ç»†çš„å®ä¾‹çº§è§†è§‰æ ‡è®°å’Œåœºæ™¯çº§å…³ç³»æ„ŸçŸ¥æ ‡è®°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ç»´åœºæ™¯ç†è§£ã€æ¨ç†å’Œæ¥åœ°ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰åœ¨å¤æ‚ä¸‰ç»´åœºæ™¯ç†è§£å’Œæ¨ç†æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å¤§å¤šæ•°å…ˆå‰çš„æ–¹æ³•å¿½ç•¥äº†äºŒç»´è¯­ä¹‰å’Œä¸‰ç»´å¯¹è±¡å±æ€§ä¹‹é—´çš„äº¤äº’ï¼Œä»¥åŠä¸‰ç»´ç¯å¢ƒå†…çš„ç©ºé—´å…³ç³»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å®ä¾‹æ„ŸçŸ¥ä¸‰ç»´å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆInst3D-LMMï¼‰ï¼Œä»¥åŒæ—¶å¤„ç†å¤šä¸ªä¸‰ç»´åœºæ™¯ç†è§£ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡å¼•å…¥å¤šè§†å›¾è·¨æ¨¡æ€èåˆï¼ˆMCMFï¼‰æ¨¡å—ï¼Œå°†å¤šè§†å›¾äºŒç»´è¯­ä¹‰æ³¨å…¥åˆ°å…¶å¯¹åº”çš„ä¸‰ç»´å‡ ä½•ç‰¹å¾ä¸­ï¼Œè·å¾—ç²¾ç»†çš„å®ä¾‹çº§è§†è§‰æ ‡è®°ã€‚</li>
<li>é€šè¿‡å¼•å…¥ä¸‰ç»´å®ä¾‹ç©ºé—´å…³ç³»ï¼ˆ3D-ISRï¼‰æ¨¡å—ï¼Œæ•æ‰å¯¹è±¡ä¹‹é—´å¤æ‚çš„é…å¯¹ç©ºé—´å…³ç³»ï¼Œè·å¾—åœºæ™¯çº§å…³ç³»æ„ŸçŸ¥æ ‡è®°ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨è¿›è¡Œå¤šä»»åŠ¡æŒ‡ä»¤è°ƒæ•´æ—¶ï¼Œæ— éœ€åç»­ä»»åŠ¡ç‰¹å®šå¾®è°ƒå³å¯è¿›è¡Œç«¯åˆ°ç«¯çš„è°ƒæ•´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00513">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-435a00e774f1b78b5e4b1a35151bc07b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-013e502dbdde407d4c199e6edd2e484b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08b1d0d8ad78774e002c26662a40445e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-020ecc80c8721387c2ad327205b752ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f824df1622d27712f8489f755359a28f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4001a43fec328517b4489305b6e11fd3.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-11/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-11/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-11/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-62e21c85bd0a35d31f4248953d8d42f5.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-11  A Survey of Large Language Model Empowered Agents for Recommendation and   Search Towards Next-Generation Information Retrieval
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c82da61db7ff856715585c9cfa33c208.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-10  FREAK Frequency-modulated High-fidelity and Real-time Audio-driven   Talking Portrait Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17012.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
