<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-03-11  A Survey of Large Language Model Empowered Agents for Recommendation and   Search Towards Next-Generation Information Retrieval">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-62e21c85bd0a35d31f4248953d8d42f5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    79 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-11-更新"><a href="#2025-03-11-更新" class="headerlink" title="2025-03-11 更新"></a>2025-03-11 更新</h1><h2 id="A-Survey-of-Large-Language-Model-Empowered-Agents-for-Recommendation-and-Search-Towards-Next-Generation-Information-Retrieval"><a href="#A-Survey-of-Large-Language-Model-Empowered-Agents-for-Recommendation-and-Search-Towards-Next-Generation-Information-Retrieval" class="headerlink" title="A Survey of Large Language Model Empowered Agents for Recommendation and   Search: Towards Next-Generation Information Retrieval"></a>A Survey of Large Language Model Empowered Agents for Recommendation and   Search: Towards Next-Generation Information Retrieval</h2><p><strong>Authors:Yu Zhang, Shutong Qiao, Jiaqi Zhang, Tzu-Heng Lin, Chen Gao, Yong Li</strong></p>
<p>Information technology has profoundly altered the way humans interact with information. The vast amount of content created, shared, and disseminated online has made it increasingly difficult to access relevant information. Over the past two decades, search and recommendation systems (collectively referred to as information retrieval systems) have evolved significantly to address these challenges. Recent advances in large language models (LLMs) have demonstrated capabilities that surpass human performance in various language-related tasks and exhibit general understanding, reasoning, and decision-making abilities. This paper explores the transformative potential of large language model agents in enhancing search and recommendation systems. We discuss the motivations and roles of LLM agents, and establish a classification framework to elaborate on the existing research. We highlight the immense potential of LLM agents in addressing current challenges in search and recommendation, providing insights into future research directions. This paper is the first to systematically review and classify the research on LLM agents in these domains, offering a novel perspective on leveraging this advanced AI technology for information retrieval. To help understand the existing works, we list the existing papers on agent-based simulation with large language models at this link: <a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search">https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search</a>. </p>
<blockquote>
<p>信息技术已经深刻改变了人类与信息的交互方式。网上创建、共享和传播的大量内容使得获取相关信息的难度越来越大。在过去的二十年中，搜索和推荐系统（统称为信息检索系统）已经发生了显著的变化，以应对这些挑战。大型语言模型（LLM）的最新进展在各种语言相关任务中表现出了超越人类的性能，并展现出了一般理解、推理和决策能力。本文探讨了大型语言模型代理在增强搜索和推荐系统方面的变革潜力。我们讨论了LLM代理的动机和角色，并建立了一个分类框架来详细阐述现有研究。我们强调了LLM代理在解决搜索和推荐方面的当前挑战方面的巨大潜力，为未来的研究方向提供了见解。本文首次系统回顾和分类了这些领域中的LLM代理研究，为如何利用这种先进的AI技术进行信息检索提供了一个新颖的视角。为了帮助理解现有工作，我们在以下链接列出了关于基于代理模拟的大型语言模型的现有论文：<a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search">https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05659v1">PDF</a> </p>
<p><strong>Summary</strong><br>     信息技术深刻改变了人类与信息的交互方式。随着在线创建、共享和传播的内容量激增，访问相关信息变得越来越困难。搜索和推荐系统（统称为信息检索系统）在过去的二十年中已经显著发展，以应对这些挑战。大型语言模型（LLM）的最新进展在各种语言相关任务中展现了超越人类性能的能力，并展现出一般性的理解、推理和决策能力。本文探讨了大型语言模型代理在信息检索中增强搜索和推荐系统的变革潜力。我们讨论了LLM代理的动机和角色，并建立了一个分类框架来阐述现有研究。我们强调了LLM代理在解决搜索和推荐中的当前挑战方面的巨大潜力，并为未来的研究方向提供了见解。本文系统地回顾和分类了在这些领域中关于LLM代理的研究，为如何利用这一先进的AI技术进行信息检索提供了新的视角。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>信息技术改变了人类与信息的交互方式，信息过载问题日益突出。</li>
<li>搜索和推荐系统已显著发展，以应对信息过载挑战。</li>
<li>大型语言模型（LLM）在多种语言任务中表现超越人类，具备理解、推理和决策能力。</li>
<li>LLM在信息检索中具有巨大潜力，可增强搜索和推荐系统的性能。</li>
<li>LLM代理的动机和角色在研究中受到关注，并建立了分类框架来阐述现有研究。</li>
<li>LLM代理能够解决搜索和推荐中的当前挑战，提供未来研究方向的见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05659">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-09728ef6b9800d8ee303f8508009b376.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cc6a1ec69b246209f029e4ed3b7fb080.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="R1-Searcher-Incentivizing-the-Search-Capability-in-LLMs-via-Reinforcement-Learning"><a href="#R1-Searcher-Incentivizing-the-Search-Capability-in-LLMs-via-Reinforcement-Learning" class="headerlink" title="R1-Searcher: Incentivizing the Search Capability in LLMs via   Reinforcement Learning"></a>R1-Searcher: Incentivizing the Search Capability in LLMs via   Reinforcement Learning</h2><p><strong>Authors:Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Ji-Rong Wen</strong></p>
<p>Existing Large Reasoning Models (LRMs) have shown the potential of reinforcement learning (RL) to enhance the complex reasoning capabilities of Large Language Models~(LLMs). While they achieve remarkable performance on challenging tasks such as mathematics and coding, they often rely on their internal knowledge to solve problems, which can be inadequate for time-sensitive or knowledge-intensive questions, leading to inaccuracies and hallucinations. To address this, we propose \textbf{R1-Searcher}, a novel two-stage outcome-based RL approach designed to enhance the search capabilities of LLMs. This method allows LLMs to autonomously invoke external search systems to access additional knowledge during the reasoning process. Our framework relies exclusively on RL, without requiring process rewards or distillation for a cold start. % effectively generalizing to out-of-domain datasets and supporting both Base and Instruct models. Our experiments demonstrate that our method significantly outperforms previous strong RAG methods, even when compared to the closed-source GPT-4o-mini. </p>
<blockquote>
<p>现有的大型推理模型（LRM）已经显示出强化学习（RL）在增强大型语言模型（LLM）的复杂推理能力方面的潜力。虽然它们在数学和编码等具有挑战性的任务上取得了显著的成绩，但它们通常依赖于内部知识来解决问题，这对于时间敏感或知识密集的问题可能不够用，导致不准确和幻觉。为了解决这一问题，我们提出了\textbf{R1-Searcher}，这是一种新型的两阶段成果导向的RL方法，旨在增强LLM的搜索能力。这种方法允许LLM在推理过程中自主调用外部搜索系统来访问额外的知识。我们的框架完全依赖于RL，无需处理奖励或蒸馏冷启动等复杂流程。%能够有效地泛化到域外数据集并支持基础模型和指令模型。我们的实验表明，我们的方法显著优于以前的强大RAG方法，即使与闭源的GPT-4o-mini相比也是如此。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05592v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>强化学习（RL）在提升大型语言模型（LLM）的复杂推理能力方面具有潜力。现有大型推理模型（LRMs）在数学和编程等挑战性任务上表现出卓越性能，但往往依赖内部知识解决问题，对于时间敏感或知识密集型问题可能显得不足，导致不准确和幻觉。为解决这一问题，提出新型两阶段结果导向的RL方法——R1-Searcher，旨在增强LLM的搜索能力，使其能够自主调用外部搜索系统获取额外知识以辅助推理过程。该方法仅依赖RL，无需过程奖励或冷启动时的蒸馏。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习用于增强大型语言模型的推理能力。</li>
<li>LRMs在数学和编程任务上表现优异，但在时间敏感和知识密集型问题上存在不足。</li>
<li>R1-Searcher是一种新型的两阶段结果导向的RL方法，旨在增强LLM的搜索能力。</li>
<li>LLMs可自主调用外部搜索系统获取额外知识以辅助推理。</li>
<li>R1-Searcher方法显著提高之前的RAG方法性能，甚至超越闭源的GPT-4o-mini。</li>
<li>该方法仅依赖强化学习，无需过程奖励或冷启动时的知识蒸馏。</li>
<li>方法具有推广至跨领域数据集的能力，并适用于基础模型和指令模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05592">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-aa0cce0eef49a56440a233b97ab14c00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f962a90cf16a20b5690ab8dd46334a8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-603d2edf70630ae009a368b957e8eadc.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LLM-based-Iterative-Approach-to-Metamodeling-in-Automotive"><a href="#LLM-based-Iterative-Approach-to-Metamodeling-in-Automotive" class="headerlink" title="LLM-based Iterative Approach to Metamodeling in Automotive"></a>LLM-based Iterative Approach to Metamodeling in Automotive</h2><p><strong>Authors:Nenad Petrovic, Fengjunjie Pan, Vahid Zolfaghari, Alois Knoll</strong></p>
<p>In this paper, we introduce an automated approach to domain-specific metamodel construction relying on Large Language Model (LLM). The main focus is adoption in automotive domain. As outcome, a prototype was implemented as web service using Python programming language, while OpenAI’s GPT-4o was used as the underlying LLM. Based on the initial experiments, this approach successfully constructs Ecore metamodel based on set of automotive requirements and visualizes it making use of PlantUML notation, so human experts can provide feedback in order to refine the result. Finally, locally deployable solution is also considered, including the limitations and additional steps required. </p>
<blockquote>
<p>在这篇论文中，我们介绍了一种基于大型语言模型（LLM）的领域特定元模型构建自动化方法。主要关注在汽车行业的应用。我们采用Python编程语言实现了一个原型作为web服务，同时使用了OpenAI的GPT-4o作为基础的大型语言模型。基于初步实验，该方法成功根据汽车需求构建了Ecore元模型，并利用PlantUML符号进行可视化，以便人类专家提供反馈以完善结果。最后，还考虑了可本地部署的解决方案，包括其局限性和所需的额外步骤。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05449v1">PDF</a> </p>
<p><strong>Summary</strong><br>基于大型语言模型（LLM）的自动化领域特定元模型构建方法被介绍。重点研究其在汽车领域的应用。通过使用Python编程语言实现原型作为Web服务，并利用OpenAI的GPT-4o作为底层LLM。初步实验表明，该方法能够根据汽车需求构建Ecore元模型，并利用PlantUML符号进行可视化，使人类专家可以提供反馈以完善结果。最后还考虑了本地可部署的解决方案，包括其局限性和所需额外步骤。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了基于LLM的自动化领域特定元模型构建方法。</li>
<li>研究的重点是在汽车领域的应用。</li>
<li>使用Python实现原型作为Web服务，并依赖OpenAI的GPT-4o作为LLM。</li>
<li>初步实验证明，该方法能够根据汽车需求构建Ecore元模型。</li>
<li>利用PlantUML符号进行可视化，以便人类专家提供反馈以改进结果。</li>
<li>考虑了本地可部署的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05449">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c2dcc90c858f0f1d3c7912780b014bb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c54f635f6646ea837df213ad9f668d16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5840f177b2b0dc7bb24b8bcafe81e7d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31aee169e659eeb3f4e52120c7330f5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2138c17ba4d205312f11c141c74e0749.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2555f1deb7d238d23b7def4462ba8b4a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Are-Your-LLM-based-Text-to-SQL-Models-Secure-Exploring-SQL-Injection-via-Backdoor-Attacks"><a href="#Are-Your-LLM-based-Text-to-SQL-Models-Secure-Exploring-SQL-Injection-via-Backdoor-Attacks" class="headerlink" title="Are Your LLM-based Text-to-SQL Models Secure? Exploring SQL Injection   via Backdoor Attacks"></a>Are Your LLM-based Text-to-SQL Models Secure? Exploring SQL Injection   via Backdoor Attacks</h2><p><strong>Authors:Meiyu Lin, Haichuan Zhang, Jiale Lao, Renyuan Li, Yuanchun Zhou, Carl Yang, Yang Cao, Mingjie Tang</strong></p>
<p>Large language models (LLMs) have shown state-of-the-art results in translating natural language questions into SQL queries (Text-to-SQL), a long-standing challenge within the database community. However, security concerns remain largely unexplored, particularly the threat of backdoor attacks, which can introduce malicious behaviors into models through fine-tuning with poisoned datasets. In this work, we systematically investigate the vulnerabilities of LLM-based Text-to-SQL models and present ToxicSQL, a novel backdoor attack framework. Our approach leverages stealthy {semantic and character-level triggers} to make backdoors difficult to detect and remove, ensuring that malicious behaviors remain covert while maintaining high model accuracy on benign inputs. Furthermore, we propose leveraging SQL injection payloads as backdoor targets, enabling the generation of malicious yet executable SQL queries, which pose severe security and privacy risks in language model-based SQL development. We demonstrate that injecting only 0.44% of poisoned data can result in an attack success rate of 79.41%, posing a significant risk to database security. Additionally, we propose detection and mitigation strategies to enhance model reliability. Our findings highlight the urgent need for security-aware Text-to-SQL development, emphasizing the importance of robust defenses against backdoor threats. </p>
<blockquote>
<p>基于大型语言模型（LLM）的自然语言翻译成为SQL查询（文本到SQL）取得了最先进的成果，这是数据库领域长期以来的一个挑战。然而，安全问题仍未得到充分研究，尤其是后门攻击带来的威胁。后门攻击可以通过中毒数据集对模型进行微调，从而引入恶意行为。在这项工作中，我们系统地研究了基于LLM的文本到SQL模型的漏洞，并推出了ToxicSQL，一种新型的后门攻击框架。我们的方法利用隐蔽的语义和字符级触发器使后门难以检测和移除，确保在良性输入上保持模型准确性的同时，恶意行为保持隐蔽。此外，我们提出利用SQL注入负载作为后门目标，能够生成恶意且可执行的SQL查询，这在基于语言模型的SQL开发中带来了严重的安全和隐私风险。我们证明，仅注入0.44%的中毒数据就可以达到79.41%的攻击成功率，给数据库安全带来显著风险。此外，我们还提出了检测和缓解策略来提高模型可靠性。我们的研究结果强调了安全意识的文本到SQL开发的紧迫需求，并强调了对抗后门威胁的稳健防御的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05445v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLM基于的Text-to-SQL模型存在安全风险，本文提出了针对该模型的ToxicSQL后门攻击框架，利用语义和字符级触发器使后门难以检测和移除，通过注入SQL注入负载生成恶意且可执行的SQL查询，对数据库安全构成严重威胁。同时，本文也提出了检测和缓解策略来提升模型可靠性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM的Text-to-SQL模型面临后门攻击的安全风险。</li>
<li>提出了一种新的后门攻击框架——ToxicSQL。</li>
<li>利用语义和字符级触发器使后门难以检测和移除。</li>
<li>通过注入SQL注入负载生成恶意且可执行的SQL查询。</li>
<li>少量（0.44%）的带毒数据即可导致高达79.41%的攻击成功率。</li>
<li>提出了检测和缓解策略以提升模型可靠性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05445">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0df934928f6d4bd8bd996bc9a5334364.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c76f5eb3d01ffa29ca4df3e07d6cb61.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-390a17a1e4107dac097485c954f554b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04231effcfe5d79158ea2dd02dc272ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd23844e137e3bd173da1e5760cf1a2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c98bbe7d44261d79943106ccb3211198.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MatrixFlow-System-Accelerator-co-design-for-high-performance-transformer-applications"><a href="#MatrixFlow-System-Accelerator-co-design-for-high-performance-transformer-applications" class="headerlink" title="MatrixFlow: System-Accelerator co-design for high-performance   transformer applications"></a>MatrixFlow: System-Accelerator co-design for high-performance   transformer applications</h2><p><strong>Authors:Qunyou Liu, Marina Zapater, David Atienza</strong></p>
<p>Transformers are central to advances in artificial intelligence (AI), excelling in fields ranging from computer vision to natural language processing. Despite their success, their large parameter count and computational demands challenge efficient acceleration. To address these limitations, this paper proposes MatrixFlow, a novel co-designed system-accelerator architecture based on a loosely coupled systolic array including a new software mapping approach for efficient transformer code execution. MatrixFlow is co-optimized via a novel dataflow-based matrix multiplication technique that reduces memory overhead. These innovations significantly improve data throughput, which is critical for handling the extensive computations required by transformers. We validate our approach through full system simulation using gem5 across various BERT and ViT Transformer models featuring different data types, demonstrating significant application-wide speed-ups. Our method achieves up to a 22x improvement compared to a many-core CPU system, and outperforms the closest state-of-the-art loosely-coupled and tightly-coupled accelerators by over 5x and 8x, respectively. </p>
<blockquote>
<p>Transformer是人工智能（AI）进步的核心，其在从计算机视觉到自然语言处理等多个领域表现出卓越的性能。尽管它们取得了成功，但由于参数众多和计算需求巨大，它们面临着有效加速的挑战。为了解决这些局限性，本文提出了MatrixFlow，这是一种新型协同设计的系统加速器架构，基于松散耦合的脉动阵列，包括一种用于高效执行Transformer代码的新软件映射方法。MatrixFlow通过基于数据流的新型矩阵乘法技术进行优化，减少了内存开销。这些创新大大提高了数据吞吐量，对于处理Transformer所需的大量计算而言至关重要。我们通过使用gem5进行全面系统仿真，验证了各种BERT和ViT Transformer模型的方法，这些模型具有不同的数据类型，显示出显著的全应用加速效果。我们的方法与多核CPU系统相比，实现了最高达22倍的改进，并且比最新的松散耦合和紧密耦合加速器分别高出超过5倍和8倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05290v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对人工智能领域中的Transformer模型计算效率低下的问题，提出了一种名为MatrixFlow的新型协同设计系统加速器架构。该架构基于松散耦合的收缩阵列，包括新的软件映射方法和基于数据流矩阵乘法的优化技术。MatrixFlow显著提高了数据吞吐量，这对于处理Transformer所需的大量计算至关重要。通过gem5进行的全系统仿真验证，在多种BERT和ViT Transformer模型上，该方法实现了显著的应用加速，相比于多核CPU系统提升了最高达22倍的速度，并且相较于目前先进的松散耦合和紧密耦合加速器分别有超过5倍和8倍的性能优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformers是人工智能领域的重要突破，但计算效率和内存需求是一大挑战。</li>
<li>MatrixFlow是一种新型系统加速器架构，专为优化Transformer的计算效率而设计。</li>
<li>MatrixFlow基于松散耦合的收缩阵列结构，并采用了创新的软件映射方法和数据流矩阵乘法技术。</li>
<li>MatrixFlow通过提高数据吞吐量来优化Transformer的计算性能。</li>
<li>通过gem5的全系统仿真验证，MatrixFlow在多种Transformer模型上实现了显著的速度提升。</li>
<li>与多核CPU系统相比，MatrixFlow最高实现了22倍的速度提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05290">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-268a98b21dff8b50a57e7c4f4e533530.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40320b65aab371fe2a415c7045b26038.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b85acab7f94b36df42a5c520f5561881.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79851503774726aa4d628c7df07250fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2a685386b5b73a55924be401f2bbacac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28b027063162b1ab2ec197650bf29605.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aec6c2d113efacfb9036dd0ed20ed0b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-41138e47b662facf648a4155d27b1a89.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="“Only-ChatGPT-gets-me”-An-Empirical-Analysis-of-GPT-versus-other-Large-Language-Models-for-Emotion-Detection-in-Text"><a href="#“Only-ChatGPT-gets-me”-An-Empirical-Analysis-of-GPT-versus-other-Large-Language-Models-for-Emotion-Detection-in-Text" class="headerlink" title="“Only ChatGPT gets me”: An Empirical Analysis of GPT versus other Large   Language Models for Emotion Detection in Text"></a>“Only ChatGPT gets me”: An Empirical Analysis of GPT versus other Large   Language Models for Emotion Detection in Text</h2><p><strong>Authors:Florian Lecourt, Madalina Croitoru, Konstantin Todorov</strong></p>
<p>This work investigates the capabilities of large language models (LLMs) in detecting and understanding human emotions through text. Drawing upon emotion models from psychology, we adopt an interdisciplinary perspective that integrates computational and affective sciences insights. The main goal is to assess how accurately they can identify emotions expressed in textual interactions and compare different models on this specific task. This research contributes to broader efforts to enhance human-computer interaction, making artificial intelligence technologies more responsive and sensitive to users’ emotional nuances. By employing a methodology that involves comparisons with a state-of-the-art model on the GoEmotions dataset, we aim to gauge LLMs’ effectiveness as a system for emotional analysis, paving the way for potential applications in various fields that require a nuanced understanding of human language. </p>
<blockquote>
<p>本文旨在研究大型语言模型（LLM）通过文本检测和了解人类情绪的能力。我们借鉴心理学的情感模型，采用跨学科视角，融合计算科学和情感科学的见解。主要目标是评估这些模型在识别文本互动中所表达的情绪方面的准确性，并在这一特定任务上比较不同的模型。这项研究为提升人机交互的广泛努力做出了贡献，使人工智能技术对用户的情绪细微差别更具响应性和敏感性。通过采用与GoEmotions数据集上的最新模型进行对比的方法，我们旨在评估LLM作为情感分析系统的有效性，为需要在人类语言方面具备细微理解能力的各个领域的应用铺平道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04831v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本工作研究大型语言模型（LLM）在检测和理解人类情绪方面的能力。该研究结合心理学中的情感模型，采用跨学科方法，整合计算和情感科学的见解。主要目标是评估LLM准确识别文本互动中表达情感的能力，并在这一特定任务上比较不同的模型。该研究为增强人机交互的广泛努力做出贡献，使人工智能技术对用户的情绪细微差别更加敏感和响应。通过采用与GoEmotions数据集上的最新模型进行对比的方法，本研究旨在评估LLM在情感分析方面的有效性，为可能需要深刻理解人类语言的各个领域的应用铺平道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究探讨了大型语言模型（LLM）在检测和理解人类情绪方面的能力。</li>
<li>研究结合了心理学中的情感模型，并采用了跨学科的方法。</li>
<li>主要目标是评估LLM在识别文本中的情感表达方面的准确性，并比较不同模型在此任务上的表现。</li>
<li>研究对增强人机交互的贡献，使AI技术对用户情绪更敏感和响应。</li>
<li>通过与GoEmotions数据集中的最新模型对比，评估了LLM在情感分析方面的有效性。</li>
<li>该研究为可能需要深入理解人类语言的各个领域（如自然语言处理、社交媒体分析、聊天机器人等）提供了潜在的应用价值。</li>
<li>整体而言，该研究为大型语言模型在情感分析领域的进一步发展提供了有价值的见解和参考。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04831">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d8f1a1b9a61913572c1576bd2000f388.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92bf9e00cb4c2be0e96791d2c2bac131.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45b7576b660d577c27685d443489fbb6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2d247ee585e8554f1660124dedaee28e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-278566f15dda47a686e92902f41ab67e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0ead619ab42cf8137c88c70431bae36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-75e96f4820cdde9e9c450b0d233056fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d41fb8b54b551bfdb19c0e6aa585ee1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Comparative-Analysis-Based-on-DeepSeek-ChatGPT-and-Google-Gemini-Features-Techniques-Performance-Future-Prospects"><a href="#Comparative-Analysis-Based-on-DeepSeek-ChatGPT-and-Google-Gemini-Features-Techniques-Performance-Future-Prospects" class="headerlink" title="Comparative Analysis Based on DeepSeek, ChatGPT, and Google Gemini:   Features, Techniques, Performance, Future Prospects"></a>Comparative Analysis Based on DeepSeek, ChatGPT, and Google Gemini:   Features, Techniques, Performance, Future Prospects</h2><p><strong>Authors:Anichur Rahman, Shahariar Hossain Mahir, Md Tanjum An Tashrif, Airin Afroj Aishi, Md Ahsan Karim, Dipanjali Kundu, Tanoy Debnath, Md. Abul Ala Moududi, MD. Zunead Abedin Eidmum</strong></p>
<p>Nowadays, DeepSeek, ChatGPT, and Google Gemini are the most trending and exciting Large Language Model (LLM) technologies for reasoning, multimodal capabilities, and general linguistic performance worldwide. DeepSeek employs a Mixture-of-Experts (MoE) approach, activating only the parameters most relevant to the task at hand, which makes it especially effective for domain-specific work. On the other hand, ChatGPT relies on a dense transformer model enhanced through reinforcement learning from human feedback (RLHF), and then Google Gemini actually uses a multimodal transformer architecture that integrates text, code, and images into a single framework. However, by using those technologies, people can be able to mine their desired text, code, images, etc, in a cost-effective and domain-specific inference. People may choose those techniques based on the best performance. In this regard, we offer a comparative study based on the DeepSeek, ChatGPT, and Gemini techniques in this research. Initially, we focus on their methods and materials, appropriately including the data selection criteria. Then, we present state-of-the-art features of DeepSeek, ChatGPT, and Gemini based on their applications. Most importantly, we show the technological comparison among them and also cover the dataset analysis for various applications. Finally, we address extensive research areas and future potential guidance regarding LLM-based AI research for the community. </p>
<blockquote>
<p>如今，DeepSeek、ChatGPT和Google Gemini是世界上最流行、最令人兴奋的用于推理、多模态能力和一般语言性能的大型语言模型（LLM）技术。DeepSeek采用混合专家（MoE）方法，只激活与手头任务最相关的参数，使其在特定领域的工作尤其有效。另一方面，ChatGPT依赖于通过强化学习从人类反馈（RLHF）增强的密集变压器模型，然后Google Gemini实际上使用了一种多模态变压器架构，该架构将文本、代码和图像集成到一个单一框架中。然而，通过利用这些技术，人们能够以成本效益和特定领域的推理来挖掘他们所需的文本、代码、图像等。人们可能会根据最佳性能选择这些技术。在这方面，我们在这项研究中基于DeepSeek、ChatGPT和Gemini技术提供了一项比较研究。最初，我们关注它们的方法和材料，适当地包括数据选择标准。然后，我们根据它们的应用展示DeepSeek、ChatGPT和Gemini的最新功能。最重要的是，我们展示了它们之间的技术比较，并涵盖了各种应用的数据集分析。最后，我们针对社区提出了关于基于LLM的人工智能研究的广泛研究领域和未来潜在指导。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04783v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>深度探索（DeepSeek）、ChatGPT和谷歌双子座（Google Gemini）是当前最流行的大型语言模型（LLM）技术，它们分别采用混合专家（MoE）方法、强化学习从人类反馈（RLHF）和多模态转换器架构，用于推理、多模态能力和一般语言性能。这些技术使人们能够以成本效益和特定领域的方式进行文本、代码、图像等的挖掘。本文对这些技术进行了比较研究，重点介绍了它们的方法、材料和数据选择标准，以及它们在应用程序方面的最新特性。同时，对三者进行了技术比较，并对各种应用的数据集进行了分析。最后，探讨了广泛的研究领域和未来关于LLM基于AI研究的潜在指导方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek、ChatGPT和Google Gemini是当前最流行的LLM技术。</li>
<li>DeepSeek采用混合专家（MoE）方法，适用于特定领域的任务。</li>
<li>ChatGPT使用强化学习从人类反馈（RLHF）进行增强。</li>
<li>Google Gemini使用多模态转换器架构，整合文本、代码和图像。</li>
<li>这些技术使文本、代码、图像等的挖掘变得成本效益高且特定领域化。</li>
<li>文章对这些技术进行了比较，包括方法、材料、数据选择标准和应用程序。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04783">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-33e2fb8acf33a58345e831f7542ecab3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-350ab728234c22b4fd154c528140888c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8083d3db6885b5c85ab0a40015166c88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ce877eaae75452f53bacda2708200a1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Universality-of-Layer-Level-Entropy-Weighted-Quantization-Beyond-Model-Architecture-and-Size"><a href="#Universality-of-Layer-Level-Entropy-Weighted-Quantization-Beyond-Model-Architecture-and-Size" class="headerlink" title="Universality of Layer-Level Entropy-Weighted Quantization Beyond Model   Architecture and Size"></a>Universality of Layer-Level Entropy-Weighted Quantization Beyond Model   Architecture and Size</h2><p><strong>Authors:Alireza Behtash, Marijan Fofonjka, Ethan Baird, Tyler Mauer, Hossein Moghimifam, David Stout, Joel Dennison</strong></p>
<p>We present a novel approach to selective model quantization that transcends the limitations of architecture-specific and size-dependent compression methods for Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ). By analyzing the entropy distribution across transformer blocks, EWQ determines which blocks can be safely quantized without causing significant performance degradation, independent of model architecture or size. Our method outperforms uniform quantization approaches, maintaining Massive Multitask Language Understanding (MMLU) accuracy scores within 0.5% of unquantized models while reducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ across multiple architectures – from 1.6B to 70B parameters – and showcase consistent improvements in the quality-compression trade-off regardless of model scale or architectural design. A surprising finding of EWQ is its ability to reduce perplexity compared to unquantized models, suggesting the presence of beneficial regularization through selective precision reduction. This improvement holds across different model families, indicating a fundamental relationship between layer-level entropy and optimal precision requirements. Additionally, we introduce FastEWQ, a rapid method for entropy distribution analysis that eliminates the need for loading model weights. This technique leverages universal characteristics of entropy distribution that persist across various architectures and scales, enabling near-instantaneous quantization decisions while maintaining 80% classification accuracy with full entropy analysis. Our results demonstrate that effective quantization strategies can be developed independently of specific architectural choices or model sizes, opening new possibilities for efficient LLM deployment. </p>
<blockquote>
<p>我们提出了一种新的选择性模型量化方法，通过使用熵加权量化（EWQ）超越了针对大型语言模型（LLM）的特定架构和大小依赖的压缩方法的局限性。通过分析变压器模块之间的熵分布，EWQ能够确定哪些模块可以安全量化，而不会导致性能显著下降，这独立于模型架构或大小。我们的方法在保持大规模多任务语言理解（MMLU）准确度得分与未量化模型相差不到0.5%的同时，减少了高达18%的内存使用，优于均匀量化方法。我们证明了EWQ在多个架构中的有效性，参数从1.6B到70B不等，并且在模型规模或架构设计上实现了持续的改进，在质量压缩权衡方面表现优异。EWQ的一个意外发现是它能够减少困惑度与未量化模型相比，这表明通过选择性精度降低存在有益的正则化。这种改进在不同模型家族中普遍存在，表明层级熵和最佳精度要求之间存在根本关系。此外，我们引入了FastEWQ，这是一种快速分析熵分布的方法，无需加载模型权重。该技术利用熵分布的通用特性，这些特性在各种架构和规模上持续存在，能够在保持80%分类准确度的同时，进行近乎即时的量化决策。我们的结果表明，有效的量化策略可以独立于特定的架构选择或模型大小来开发，为高效部署LLM开辟了新的可能性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04704v2">PDF</a> 29 pages, 7 figures, 14 tables; Fixed some types, added some   clarifications and improvements</p>
<p><strong>Summary</strong><br>     提出一种利用熵加权量化（EWQ）的LLM选择性模型量化新方法，该方法突破了架构特定和大小依赖的压缩方法的局限。通过分析变压器块的熵分布，EWQ可确定哪些块可在不影响性能的情况下安全量化，独立于模型架构和大小。该方法优于均匀量化方法，保持大规模多任务语言理解准确性，在不超过未量化模型0.5%误差的同时减少内存使用高达18%。此方法适用于多种架构，在不同规模或设计的模型中实现一致的改进。此外，EWQ通过选择性精度降低展现出有益的正规化效果，并降低困惑度。我们还介绍了FastEWQ技术，可以快速分析熵分布，无需加载模型权重。这表明，独立于特定架构选择和模型大小的有效量化策略是可行的，为高效部署LLM打开了新的可能性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种名为EWQ的新型LLM模型量化方法，能针对不同的模型和架构进行选择性量化。</li>
<li>EWQ基于熵分布的分析确定可量化的变压器块，避免因量化导致的性能下降。</li>
<li>EWQ相较于均匀量化方法，能维持较高的准确性，同时显著降低内存使用。</li>
<li>EWQ在多个不同规模和设计的模型中表现一致，包括大型语言模型。</li>
<li>EWQ在选择性降低精度的情况下展现出正则化效果，相较于未量化的模型降低了困惑度。</li>
<li>介绍了FastEWQ技术，能快速分析熵分布而无需加载模型权重，提高了量化决策的效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04704">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9ddd04d641a1b3b1d8f29cee6b160cff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6492f6d39d4c08c3948fc8505e6d028b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bc8e6214e3baab03dda9edb9d9c1712.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LiGT-Layout-infused-Generative-Transformer-for-Visual-Question-Answering-on-Vietnamese-Receipts"><a href="#LiGT-Layout-infused-Generative-Transformer-for-Visual-Question-Answering-on-Vietnamese-Receipts" class="headerlink" title="LiGT: Layout-infused Generative Transformer for Visual Question   Answering on Vietnamese Receipts"></a>LiGT: Layout-infused Generative Transformer for Visual Question   Answering on Vietnamese Receipts</h2><p><strong>Authors:Thanh-Phong Le, Trung Le Chi Phan, Nghia Hieu Nguyen, Kiet Van Nguyen</strong></p>
<p>Document Visual Question Answering (Document VQA) challenges multimodal systems to holistically handle textual, layout, and visual modalities to provide appropriate answers. Document VQA has gained popularity in recent years due to the increasing amount of documents and the high demand for digitization. Nonetheless, most of document VQA datasets are developed in high-resource languages such as English. In this paper, we present ReceiptVQA (\textbf{Receipt} \textbf{V}isual \textbf{Q}uestion \textbf{A}nswering), the initial large-scale document VQA dataset in Vietnamese dedicated to receipts, a document kind with high commercial potentials. The dataset encompasses \textbf{9,000+} receipt images and \textbf{60,000+} manually annotated question-answer pairs. In addition to our study, we introduce LiGT (\textbf{L}ayout-\textbf{i}nfused \textbf{G}enerative \textbf{T}ransformer), a layout-aware encoder-decoder architecture designed to leverage embedding layers of language models to operate layout embeddings, minimizing the use of additional neural modules. Experiments on ReceiptVQA show that our architecture yielded promising performance, achieving competitive results compared with outstanding baselines. Furthermore, throughout analyzing experimental results, we found evident patterns that employing encoder-only model architectures has considerable disadvantages in comparison to architectures that can generate answers. We also observed that it is necessary to combine multiple modalities to tackle our dataset, despite the critical role of semantic understanding from language models. We hope that our work will encourage and facilitate future development in Vietnamese document VQA, contributing to a diverse multimodal research community in the Vietnamese language. </p>
<blockquote>
<p>文档视觉问答（Document VQA）挑战多模态系统，以全面处理文本、布局和视觉模式，以提供适当的答案。近年来，由于文档数量的不断增加和数字化需求的不断增长，文档VQA的受欢迎程度日益提高。然而，大部分的文档VQA数据集都是用资源丰富的语言（如英语）开发的。在本文中，我们介绍了ReceiptVQA（收据视觉问答），这是越南语中针对收据的首个大规模文档VQA数据集，收据是一种具有极高商业潜力的文档类型。该数据集包含超过9000张收据图像和超过6万个手动标注的问题答案对。除了我们的研究，我们还引入了LiGT（布局融入生成Transformer），这是一个布局感知的编码器-解码器架构，旨在利用语言模型的嵌入层来操作布局嵌入，尽量减少使用额外的神经网络模块。在ReceiptVQA上的实验表明，我们的架构取得了有希望的性能，与优秀的基线相比取得了具有竞争力的结果。此外，通过分析实验结果，我们发现与能够生成答案的架构相比，使用仅编码器模型架构具有明显的劣势。我们还观察到，尽管语言模型的语义理解起着关键作用，但结合多种模式来解决我们的数据集是必要的。我们希望我们的工作能鼓励和促进越南语文档VQA的未来发展，为越南语的多模态研究社区做出贡献。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19202v2">PDF</a> Accepted at IJDAR</p>
<p><strong>摘要</strong></p>
<p>本文介绍了Document Visual Question Answering（Document VQA）的挑战，并针对文本、布局和视觉模态的综合处理要求提供答案。文中提出了一种新的越南语收据文档VQA数据集ReceiptVQA，包含超过9,000张收据图像和超过6万组手动标注的问题答案对。此外，还引入了LiGT模型，这是一种布局感知的编码器-解码器架构，旨在利用语言模型的嵌入层进行布局嵌入，减少使用额外的神经网络模块。实验表明，该架构在ReceiptVQA上表现出良好的性能，并与基线模型相比具有竞争力。分析实验结果显示，与只能编码的模型架构相比，能够生成答案的架构具有明显的优势。同时观察到，尽管语言模型的语义理解起着关键作用，但结合多种模态对于解决数据集的问题仍是必要的。希望这项工作能推动越南语文档VQA的发展，并为越南语的多模态研究社区做出贡献。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>Document VQA面临处理文本、布局和视觉模态的挑战。</li>
<li>提出了一种新的越南语收据文档VQA数据集ReceiptVQA。</li>
<li>引入了LiGT模型，一种布局感知的编码器-解码器架构。</li>
<li>LiGT模型在ReceiptVQA数据集上表现出良好的性能。</li>
<li>实验分析显示，生成答案的模型架构比仅编码的模型架构具有优势。</li>
<li>结合多种模态对于解决数据集的问题是必要的。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19202">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8b7aaf3ce5f4d096635b436189db57f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-47b89354738d77b07706da892591f68d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Exoplanet-Transit-Candidate-Identification-in-TESS-Full-Frame-Images-via-a-Transformer-Based-Algorithm"><a href="#Exoplanet-Transit-Candidate-Identification-in-TESS-Full-Frame-Images-via-a-Transformer-Based-Algorithm" class="headerlink" title="Exoplanet Transit Candidate Identification in TESS Full-Frame Images via   a Transformer-Based Algorithm"></a>Exoplanet Transit Candidate Identification in TESS Full-Frame Images via   a Transformer-Based Algorithm</h2><p><strong>Authors:Helem Salinas, Rafael Brahm, Greg Olmschenk, Richard K. Barry, Karim Pichara, Stela Ishitani Silva, Vladimir Araujo</strong></p>
<p>The Transiting Exoplanet Survey Satellite (TESS) is surveying a large fraction of the sky, generating a vast database of photometric time series data that requires thorough analysis to identify exoplanetary transit signals. Automated learning approaches have been successfully applied to identify transit signals. However, most existing methods focus on the classification and validation of candidates, while few efforts have explored new techniques for the search of candidates. To search for new exoplanet transit candidates, we propose an approach to identify exoplanet transit signals without the need for phase folding or assuming periodicity in the transit signals, such as those observed in multi-transit light curves. To achieve this, we implement a new neural network inspired by Transformers to directly process Full Frame Image (FFI) light curves to detect exoplanet transits. Transformers, originally developed for natural language processing, have recently demonstrated significant success in capturing long-range dependencies compared to previous approaches focused on sequential data. This ability allows us to employ multi-head self-attention to identify exoplanet transit signals directly from the complete light curves, combined with background and centroid time series, without requiring prior transit parameters. The network is trained to learn characteristics of the transit signal, like the dip shape, which helps distinguish planetary transits from other variability sources. Our model successfully identified 214 new planetary system candidates, including 122 multi-transit light curves, 88 single-transit and 4 multi-planet systems from TESS sectors 1-26 with a radius &gt; 0.27 $R_{\mathrm{Jupiter}}$, demonstrating its ability to detect transits regardless of their periodicity. </p>
<blockquote>
<p>凌日系外行星勘测卫星（TESS）正在对天空进行大规模勘测，生成大量光度时间序列数据，需要进行彻底分析以识别凌日系外行星的信号。自动化学习方法已成功应用于识别凌日信号。然而，大多数现有方法都集中在候选对象的分类和验证上，而对寻找新候选对象的新技术探索却很少。为了寻找新的凌日系外行星候选对象，我们提出了一种无需相位折叠或假设凌日信号周期性的方法，例如多凌日光度曲线所观察到的那样。为了实现这一点，我们受到Transformer启发的神经网络，直接处理全帧图像（FFI）光度曲线来检测凌日系外行星。Transformer最初是为自然语言处理而开发的，最近的研究表明，与以前专注于序列数据的方法相比，它在捕获长期依赖关系方面取得了重大成功。这种能力使我们能够利用多头自注意力从完整的光度曲线中直接识别凌日系外行星的信号，结合背景和质心时间序列，无需事先的凌日参数。网络经过训练，学习凌日信号的特征，如暗形，这有助于区分行星凌日和其他变源。我们的模型成功识别了214个新的行星系统候选对象，包括122个多凌日光度曲线、88个单凌日和来自TESS 1-26扇区的4个多行星系统，其半径大于0.27个木星半径，证明了其检测凌日的能力，无论其周期性如何。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07542v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了利用Transformer神经网络直接处理全帧图像（FFI）光变曲线来检测行星过境的新方法。该方法无需相位折叠或假设过境信号的周期性，可直接从完整的光变曲线中识别出行星过境信号。此方法已成功识别出TESS探测器观测到的214个新的行星系统候选体。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TESS正在对天空进行大规模观测，生成了大量的光变曲线数据。</li>
<li>现有方法主要集中在候选者的分类和验证上，对新候选者的搜索方法探索较少。</li>
<li>提出了一种新的基于Transformer神经网络的方法，用于直接处理全帧图像（FFI）光变曲线来检测行星过境信号。</li>
<li>该方法无需假设过境信号的周期性，可直接从完整的光变曲线中识别信号。</li>
<li>Transformer网络具有捕捉长期依赖关系的能力，有助于识别行星过境信号。</li>
<li>该方法通过学习和识别过境信号的特征，如暗斑形状，来区分行星过境和其他变化源。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07542">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-69f0e2833271f1b8b6a66cdc6c5f94a9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-751a0f349e754c4a8b2280ad20a413b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74c6938214d33c1513b62e4ae30a8d27.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Bias-Unveiled-Investigating-Social-Bias-in-LLM-Generated-Code"><a href="#Bias-Unveiled-Investigating-Social-Bias-in-LLM-Generated-Code" class="headerlink" title="Bias Unveiled: Investigating Social Bias in LLM-Generated Code"></a>Bias Unveiled: Investigating Social Bias in LLM-Generated Code</h2><p><strong>Authors:Lin Ling, Fazle Rabbi, Song Wang, Jinqiu Yang</strong></p>
<p>Large language models (LLMs) have significantly advanced the field of automated code generation. However, a notable research gap exists in evaluating social biases that may be present in the code produced by LLMs. To solve this issue, we propose a novel fairness framework, i.e., Solar, to assess and mitigate the social biases of LLM-generated code. Specifically, Solar can automatically generate test cases for quantitatively uncovering social biases of the auto-generated code by LLMs. To quantify the severity of social biases in generated code, we develop a dataset that covers a diverse set of social problems. We applied Solar and the crafted dataset to four state-of-the-art LLMs for code generation. Our evaluation reveals severe bias in the LLM-generated code from all the subject LLMs. Furthermore, we explore several prompting strategies for mitigating bias, including Chain-of-Thought (CoT) prompting, combining positive role-playing with CoT prompting and dialogue with Solar. Our experiments show that dialogue with Solar can effectively reduce social bias in LLM-generated code by up to 90%. Last, we make the code and data publicly available is highly extensible to evaluate new social problems. </p>
<blockquote>
<p>大型语言模型（LLM）在自动化代码生成领域取得了显著进展。然而，评估LLM生成的代码中可能存在的社会偏见仍存在显著的研究空白。为了解决这一问题，我们提出了一个公平性的新框架，即Solar，来评估和缓解LLM生成代码的社会偏见。具体来说，Solar能够自动生成测试用例，定量发现LLM生成的代码中的社会偏见。为了量化生成代码中社会偏见的严重程度，我们开发了一个涵盖多种社会问题的数据集。我们将Solar和定制数据集应用于四个最先进的代码生成LLM。我们的评估发现所有测试LLM生成的代码都存在严重偏见。此外，我们探索了几种减轻偏见的方法，包括链式思维（CoT）提示、将积极角色扮演与CoT提示和与Solar对话相结合。我们的实验表明，与Solar的对话可以有效地减少LLM生成代码中的社会偏见高达90%。最后，我们公开了代码和数据集，这对于评估新的社会问题具有很强的可扩展性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10351v4">PDF</a> accepted for publication in the Association for the Advancement of   Artificial Intelligence (AAAI), 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在自动化代码生成领域取得了显著进展，但评估LLM生成代码中可能存在的社会偏见的研究空白仍然显著。为解决这一问题，本文提出了一个名为Solar的新型公平性框架，用于评估和缓解LLM生成代码中的社会偏见。Solar能够自动生成测试用例，定量发现LLM生成代码中的社会偏见。为量化生成代码中社会偏见的严重程度，本文开发了一个涵盖多种社会问题的数据集。将Solar和数据集应用于四种先进的LLM代码生成模型，发现所有模型生成的代码都存在严重偏见。此外，本文探讨了多种缓解偏见的方法，包括Chain-of-Thought（CoT）提示法、结合正面角色扮演与CoT提示法以及与Solar对话等。实验表明，与Solar的对话可以有效减少LLM生成代码中的社会偏见，高达90%。最后，公开的代码和数据集可高度扩展，用于评估新的社会问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在自动化代码生成中表现出显著进展。</li>
<li>LLM生成的代码中可能存在社会偏见的问题尚未得到充分研究。</li>
<li>Solar框架被提出用于评估和缓解LLM生成代码中的社会偏见。</li>
<li>Solar能自动生成测试用例以发现LLM生成代码中的社会偏见。</li>
<li>开发了一个涵盖多种社会问题的数据集，用于量化生成代码中社会偏见的严重程度。</li>
<li>对四种先进的LLM代码生成模型进行评估，发现存在严重偏见。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10351">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-62e21c85bd0a35d31f4248953d8d42f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2733e10ea8d115e0ec2a41c89dc13133.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6be8a100582abc67c57003030921274.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4123bd42d872de181dc57f4f838b50ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64379123c25fe8b7c7669a1d2d352644.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7426eb788a641a457e1d50a836288590.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Improving-Data-Efficiency-via-Curating-LLM-Driven-Rating-Systems"><a href="#Improving-Data-Efficiency-via-Curating-LLM-Driven-Rating-Systems" class="headerlink" title="Improving Data Efficiency via Curating LLM-Driven Rating Systems"></a>Improving Data Efficiency via Curating LLM-Driven Rating Systems</h2><p><strong>Authors:Jinlong Pang, Jiaheng Wei, Ankit Parag Shah, Zhaowei Zhu, Yaxuan Wang, Chen Qian, Yang Liu, Yujia Bao, Wei Wei</strong></p>
<p>Instruction tuning is critical for adapting large language models (LLMs) to downstream tasks, and recent studies have demonstrated that small amounts of human-curated data can outperform larger datasets, challenging traditional data scaling laws. While LLM-based data quality rating systems offer a cost-effective alternative to human annotation, they often suffer from inaccuracies and biases, even in powerful models like GPT-4. In this work, we introduce DS2, a Diversity-aware Score curation method for Data Selection. By systematically modeling error patterns through a score transition matrix, DS2 corrects LLM-based scores and promotes diversity in the selected data samples. Our approach shows that a curated subset (just 3.3% of the original dataset) outperforms full-scale datasets (300k samples) across various machine-alignment benchmarks, and matches or surpasses human-aligned datasets such as LIMA with the same sample size (1k samples). These findings challenge conventional data scaling assumptions, highlighting that redundant, low-quality samples can degrade performance and reaffirming that “more can be less.” </p>
<blockquote>
<p>指令调整对于适应大型语言模型（LLM）下游任务至关重要，最近的研究表明，少量人工整理的数据可以超越大规模数据集，挑战传统数据规模定律。虽然基于LLM的数据质量评分系统为人工标注提供了一种成本效益高的替代方案，但它们即使在GPT-4等强大模型中也会存在不准确和偏见的问题。在这项工作中，我们引入了DS2，一种用于数据选择的多样性感知分数整理方法。通过系统地通过评分转换矩阵对错误模式进行建模，DS2校正了基于LLM的评分并促进了所选数据样本的多样性。我们的方法表明，经过整理的子集（仅占原始数据集的3.3%）在各种机器对齐基准测试中优于全面数据集（30万样本），并且与相同样本量（1千个样本）的人类对齐数据集如LIMA相匹配或更胜一筹。这些发现挑战了传统数据规模假设，强调冗余、低质量的样本可能会降低性能并再次证明“更多可能是更少”。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10877v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文强调了指令微调在适应大型语言模型（LLM）到下游任务中的重要性，并指出小量的人工编制数据可能优于大规模数据集，挑战了传统数据规模定律。针对LLM基础数据质量评估系统存在的不准确和偏见问题，本文提出了DS2，一种用于数据选择的多样性感知分数编纂方法。通过系统建模误差模式，DS2校正了LLM基础上的分数并促进了所选数据样本的多样性。实验结果显示，精选的子集（仅占原始数据的3.3%）在各种机器对齐基准测试中表现优于全面数据集（30万样本），并在相同样本量下与人工对齐的数据集（如LIMA）相匹配或表现更好。这些发现挑战了传统数据规模假设，强调冗余、低质量的样本会损害性能，再次证明了“更多不一定更好”。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>指令微调在LLM适应下游任务中起关键作用。</li>
<li>小规模人工编制数据在性能上可能优于大规模数据集，对传统数据规模定律提出挑战。</li>
<li>LLM基础数据质量评估系统存在不准确性和偏见问题。</li>
<li>DS2方法通过系统建模误差模式来校正LLM分数并促进数据多样性。</li>
<li>DS2方法显示精选子集在各种机器对齐基准测试中表现优异。</li>
<li>与相同规模的人工对齐数据集相比，精选子集的表现相匹配或更佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10877">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c3614a0b352885a9e91600561589aede.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f11386ea3313af7c4120c99c1d6cd4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5172b34c7780a7496719bb21678f50bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-711ab48503ba7265d8fe1c580aa6e762.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d79995305895b9e21052be432ea22002.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="AlphaEdit-Null-Space-Constrained-Knowledge-Editing-for-Language-Models"><a href="#AlphaEdit-Null-Space-Constrained-Knowledge-Editing-for-Language-Models" class="headerlink" title="AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models"></a>AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models</h2><p><strong>Authors:Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Shi Jie, Xiang Wang, Xiangnan He, Tat-seng Chua</strong></p>
<p>Large language models (LLMs) often exhibit hallucinations due to incorrect or outdated knowledge. Hence, model editing methods have emerged to enable targeted knowledge updates. To achieve this, a prevailing paradigm is the locating-then-editing approach, which first locates influential parameters and then edits them by introducing a perturbation. While effective, current studies have demonstrated that this perturbation inevitably disrupt the originally preserved knowledge within LLMs, especially in sequential editing scenarios. To address this, we introduce AlphaEdit, a novel solution that projects perturbation onto the null space of the preserved knowledge before applying it to the parameters. We theoretically prove that this projection ensures the output of post-edited LLMs remains unchanged when queried about the preserved knowledge, thereby mitigating the issue of disruption. Extensive experiments on various LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts the performance of most locating-then-editing methods by an average of 36.4% with a single line of additional code for projection solely. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/jianghoucheng/AlphaEdit">https://github.com/jianghoucheng/AlphaEdit</a>. </p>
<blockquote>
<p>大型语言模型（LLM）由于知识不正确或过时，经常会出现幻觉。因此，出现了模型编辑方法，以实现针对性的知识更新。为此，一种普遍的方法是“定位然后编辑”的方法，它首先定位有影响力的参数，然后通过引入扰动来进行编辑。尽管这种方法有效，但当前的研究已经证明，这种扰动不可避免地会破坏LLM中原本保存的知识，特别是在连续编辑场景中。为解决这一问题，我们引入了AlphaEdit，这是一种全新的解决方案，它将扰动投影到保留知识的零空间上，然后再将其应用到参数上。我们从理论上证明了这种投影确保了当被问及保留的知识时，编辑后的LLM的输出保持不变，从而缓解了知识被干扰的问题。在包括LLaMA3、GPT2-XL和GPT-J等各种LLM上的大量实验表明，AlphaEdit通过将投影的单行代码添加到现有的定位然后编辑方法中，平均提高了其性能约36.4%。我们的代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/jianghoucheng/AlphaEdit%E3%80%82">https://github.com/jianghoucheng/AlphaEdit。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02355v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大语言模型（LLM）易出现幻觉，原因在于知识不正确或过时。因此，出现了模型编辑方法对知识库进行有针对性的更新。当前主流方法是定位-编辑模式，即先定位关键参数后进行编辑。然而，这种方式会破坏原有知识，特别是在连续编辑场景中。为解决这一问题，我们推出AlphaEdit，一种将扰动投影到保留知识的零空间再应用于参数的新方法。理论证明，该方法能确保对保留知识的查询得到相同输出，解决了扰动导致的破坏问题。在多种LLM上的实验表明，AlphaEdit能平均提升定位-编辑方法的性能达36.4%，仅需添加一行投影代码。代码地址：<a target="_blank" rel="noopener" href="https://github.com/jianghoucheng/AlphaEdit%E3%80%82">https://github.com/jianghoucheng/AlphaEdit。</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMs易因知识不正确或过时产生幻觉。</li>
<li>现有的模型编辑方法采用定位-编辑模式，但会破坏原有知识。</li>
<li>AlphaEdit通过投影技术将扰动应用于保留知识的零空间，确保对保留知识的查询输出不变。</li>
<li>AlphaEdit能显著提升定位-编辑方法的性能，平均提升达36.4%。</li>
<li>AlphaEdit方法只需在现有代码基础上添加一行投影代码。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02355">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9949cb3f937e23fde60c58b32c83922d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72b287c1115359cba8510b1da8648019.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-213d4c019b0c265a3d553fd026937630.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c522e885475e01b094ee4982193718a9.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SynSUM-–-Synthetic-Benchmark-with-Structured-and-Unstructured-Medical-Records"><a href="#SynSUM-–-Synthetic-Benchmark-with-Structured-and-Unstructured-Medical-Records" class="headerlink" title="SynSUM – Synthetic Benchmark with Structured and Unstructured Medical   Records"></a>SynSUM – Synthetic Benchmark with Structured and Unstructured Medical   Records</h2><p><strong>Authors:Paloma Rabaey, Henri Arno, Stefan Heytens, Thomas Demeester</strong></p>
<p>We present the SynSUM benchmark, a synthetic dataset linking unstructured clinical notes to structured background variables. The dataset consists of 10,000 artificial patient records containing tabular variables (like symptoms, diagnoses and underlying conditions) and related notes describing the fictional patient encounter in the domain of respiratory diseases. The tabular portion of the data is generated through a Bayesian network, where both the causal structure between the variables and the conditional probabilities are proposed by an expert based on domain knowledge. We then prompt a large language model (GPT-4o) to generate a clinical note related to this patient encounter, describing the patient symptoms and additional context. We conduct both an expert evaluation study to assess the quality of the generated notes, as well as running some simple predictor models on both the tabular and text portions of the dataset, forming a baseline for further research. The SynSUM dataset is primarily designed to facilitate research on clinical information extraction in the presence of tabular background variables, which can be linked through domain knowledge to concepts of interest to be extracted from the text - the symptoms, in the case of SynSUM. Secondary uses include research on the automation of clinical reasoning over both tabular data and text, causal effect estimation in the presence of tabular and&#x2F;or textual confounders, and multi-modal synthetic data generation. </p>
<blockquote>
<p>我们推出了SynSUM基准测试集，这是一个合成数据集，它将非结构化的临床笔记与结构化的背景变量联系起来。该数据集包含10,000条人工患者记录，包括表格变量（如症状、诊断和潜在疾病）以及描述呼吸系统疾病领域中虚构患者遭遇的相关笔记。数据的表格部分是通过贝叶斯网络生成的，变量之间的因果结构和条件概率均由专家基于领域知识提出。然后，我们提示大型语言模型（GPT-4o）生成与该患者遭遇相关的临床笔记，描述患者的症状和额外的上下文。我们既进行了专家评估研究以评估生成的笔记的质量，也在数据集的文字和表格部分上运行了一些简单的预测模型，为进一步的研究提供参考基准。SynSUM数据集主要用于促进在有表格背景变量的情况下进行临床信息提取的研究，这些背景变量可以通过领域知识链接到文本中感兴趣的概念——以SynSUM中的症状为例。次要用途包括在表格数据和文本上的自动化临床推理研究、存在表格和&#x2F;或文本混淆因素时的因果效应估计以及多模式合成数据生成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08936v2">PDF</a> The dataset can be downloaded from <a target="_blank" rel="noopener" href="https://github.com/prabaey/synsum">https://github.com/prabaey/synsum</a>.   Presented at the GenAI4Health workshop at AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>本数据集SynSUM为合成数据集，链接了结构化背景变量与非结构化临床笔记。数据集包含一万条虚构的患者记录，涉及呼吸疾病领域的表格变量（如症状、诊断及基础疾病）及相关描述患者遭遇的临床笔记。表格数据通过贝叶斯网络生成，专家基于领域知识提出变量间的因果结构及条件概率。之后利用大型语言模型（GPT-4o）生成与病人遭遇相关的临床笔记，描述病人症状及额外情境。本研究通过专家评估生成的笔记质量，同时在数据集文本和表格部分运行简单的预测模型，为后续研究提供基线。SynSUM数据集主要用于促进临床信息提取研究，尤其在存在表格背景变量的情况下，可通过领域知识链接文本中的概念（如SynSUM中的症状）。次要用途包括自动化临床推理研究，处理表格和文本数据时的因果效应估算和多模态合成数据生成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SynSUM是一个合成数据集，将非结构化临床笔记与结构化背景变量链接起来。</li>
<li>数据集包含一万条虚构的患者记录，涉及呼吸疾病领域的表格数据和临床笔记。</li>
<li>表格数据通过贝叶斯网络生成，反映变量间的因果结构和条件概率。</li>
<li>利用大型语言模型生成临床笔记，描述患者症状及情境。</li>
<li>数据集用于研究临床信息提取、自动化临床推理、因果效应估算和多模态合成数据生成等。</li>
<li>数据集经过专家评估生成的笔记质量，同时提供了预测模型的基线数据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08936">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-66b85e0a2c75b15539228e096a9a9367.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-629003cbd6eb8c5b5484367400a72eea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c22ea4cac9e92aa2eda8cba314931448.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b6c4aa3fe9e550d23b242f3c76ee0e5.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Transformer-Block-Coupling-and-its-Correlation-with-Generalization-in-LLMs"><a href="#Transformer-Block-Coupling-and-its-Correlation-with-Generalization-in-LLMs" class="headerlink" title="Transformer Block Coupling and its Correlation with Generalization in   LLMs"></a>Transformer Block Coupling and its Correlation with Generalization in   LLMs</h2><p><strong>Authors:Murdock Aubry, Haoming Meng, Anton Sugolov, Vardan Papyan</strong></p>
<p>Large Language Models (LLMs) have made significant strides in natural language processing, and a precise understanding of the internal mechanisms driving their success is essential. In this work, we analyze the trajectories of token embeddings as they pass through transformer blocks, linearizing the system along these trajectories through their Jacobian matrices. By examining the relationships between these block Jacobians, we uncover the phenomenon of \textbf{transformer block coupling} in a multitude of LLMs, characterized by the coupling of their top singular vectors across tokens and depth. Our findings reveal that coupling \textit{positively correlates} with model performance, and that this relationship is stronger than with other hyperparameters such as parameter count, model depth, and embedding dimension. We further investigate how these properties emerge during training, observing a progressive development of coupling, increased linearity, and layer-wise exponential growth in token trajectories. Additionally, experiments with Vision Transformers (ViTs) corroborate the emergence of coupling and its relationship with generalization, reinforcing our findings in LLMs. Collectively, these insights offer a novel perspective on token interactions in transformers, opening new directions for studying their mechanisms as well as improving training and generalization. </p>
<blockquote>
<p>大型语言模型（LLM）在自然语言处理方面取得了显著进展，对驱动其成功的内部机制进行精确理解至关重要。在这项工作中，我们分析了令牌嵌入在通过transformer块时的轨迹，通过其雅可比矩阵沿这些轨迹线性化系统。通过检查这些块雅可比之间的关系，我们在多种LLM中发现了“transformer块耦合”现象，其特征在于令牌和深度之间的顶部奇异向量的耦合。我们的研究发现，耦合与模型性能呈正相关，而且这种关系比参数数量、模型深度和嵌入维度等其他超参数更强。我们进一步调查了这些属性在训练过程中的出现情况，观察到耦合、线性增强和逐层指数增长的轨迹的发展过程。此外，对视觉转换器（ViTs）的实验证实了耦合的出现及其与泛化的关系，证实了我们在LLM中的发现。总的来说，这些见解为变压器中的令牌交互提供了新的视角，为学习其机制以及改进训练和泛化能力开辟了新方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.07810v5">PDF</a> Published as a conference paper at the International Conference on   Learning Representations (ICLR 2025)</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）通过变换器块进行标记嵌入轨迹分析，揭示了变换器块耦合现象。该现象表现为标记和深度之间的顶部奇异向量耦合。研究发现，耦合与模型性能正相关，与其他超参数相比，如参数计数、模型深度和嵌入维度等更为紧密。训练过程中，观察到耦合、线性增长和逐层指数增长的标记轨迹的逐步发展。此外，视觉变换器（ViT）实验也证实了耦合的出现与其泛化性的关联，为理解标记在转换器中的交互作用提供了新视角，也为改进训练和泛化指明了方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs通过变换器块进行标记嵌入轨迹分析，揭示了变换器块耦合现象。</li>
<li>变换器块耦合表现为标记和深度之间的顶部奇异向量耦合。</li>
<li>耦合与模型性能呈正相关。</li>
<li>相比其他超参数，模型耦合与模型性能的关系更为紧密。</li>
<li>训练过程中观察到耦合、线性增长和逐层指数增长的标记轨迹的逐步发展。</li>
<li>ViT实验证实了耦合的出现与泛化性之间的关联。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.07810">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7299557f36276ca627b81b5e4a88ec80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d17c87cfff274cebf1d5c847fd46e59f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03dabf3ff5d59bae61a8b625c3759f11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca4bc421c2aed853499760675080ccb0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Efficient-Automated-Circuit-Discovery-in-Transformers-using-Contextual-Decomposition"><a href="#Efficient-Automated-Circuit-Discovery-in-Transformers-using-Contextual-Decomposition" class="headerlink" title="Efficient Automated Circuit Discovery in Transformers using Contextual   Decomposition"></a>Efficient Automated Circuit Discovery in Transformers using Contextual   Decomposition</h2><p><strong>Authors:Aliyah R. Hsu, Georgia Zhou, Yeshwanth Cherapanamjeri, Yaxuan Huang, Anobel Y. Odisho, Peter R. Carroll, Bin Yu</strong></p>
<p>Automated mechanistic interpretation research has attracted great interest due to its potential to scale explanations of neural network internals to large models. Existing automated circuit discovery work relies on activation patching or its approximations to identify subgraphs in models for specific tasks (circuits). They often suffer from slow runtime, approximation errors, and specific requirements of metrics, such as non-zero gradients. In this work, we introduce contextual decomposition for transformers (CD-T) to build interpretable circuits in large language models. CD-T can produce circuits of arbitrary level of abstraction, and is the first able to produce circuits as fine-grained as attention heads at specific sequence positions efficiently. CD-T consists of a set of mathematical equations to isolate contribution of model features. Through recursively computing contribution of all nodes in a computational graph of a model using CD-T followed by pruning, we are able to reduce circuit discovery runtime from hours to seconds compared to state-of-the-art baselines. On three standard circuit evaluation datasets (indirect object identification, greater-than comparisons, and docstring completion), we demonstrate that CD-T outperforms ACDC and EAP by better recovering the manual circuits with an average of 97% ROC AUC under low runtimes. In addition, we provide evidence that faithfulness of CD-T circuits is not due to random chance by showing our circuits are 80% more faithful than random circuits of up to 60% of the original model size. Finally, we show CD-T circuits are able to perfectly replicate original models’ behavior (faithfulness $ &#x3D; 1$) using fewer nodes than the baselines for all tasks. Our results underscore the great promise of CD-T for efficient automated mechanistic interpretability, paving the way for new insights into the workings of large language models. </p>
<blockquote>
<p>自动化机制解释研究因其将神经网络内部解释扩展到大型模型的潜力而备受关注。现有的自动化电路发现工作依赖于激活补丁或其近似值来识别模型中的特定任务子图（电路）。它们常常受到运行速度慢、近似误差和特定指标要求（如非零梯度）的困扰。在这项工作中，我们引入了用于构建大型语言模型中可解释电路的变压器上下文分解（CD-T）。CD-T可以产生任意抽象层次的电路，并且是第一个能够高效地在特定序列位置产生精细到注意力头的电路的方法。CD-T由一组数学方程构成，用于隔离模型特征的贡献。通过递归计算模型计算图中所有节点的贡献，并使用CD-T进行修剪，与最新基线相比，我们将电路发现运行时间从数小时减少到了数秒。在三个标准电路评估数据集（间接对象识别、大于比较和docstring完成）上，我们证明了CD-T优于ACDC和EAP，能够更好地恢复手动电路，在低运行时间下的平均ROC AUC达到97%。此外，我们通过显示我们的电路比随机电路更忠实（忠实度高出80%），而随机电路最多达到原始模型大小的60%，证明了CD-T电路的忠实性并非偶然。最后，我们展示了CD-T电路能够完美复制原始模型的行为（忠实度&#x3D; 1），在所有任务中使用比基线更少的节点。我们的研究结果强调了CD-T在高效自动化机制解释方面的巨大潜力，为深入了解大型语言模型的工作原理开辟了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.00886v3">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了一种针对大型语言模型的解释性电路构建方法——上下文分解法（CD-T）。现有自动化电路发现工作存在运行速度慢、近似误差和特定指标要求等问题，而CD-T能够产生任意层次的抽象电路，并能高效地产生精细到特定序列位置注意力头的电路。通过递归计算模型中所有节点的贡献并进行修剪，CD-T将电路发现的时间从小时减少到秒。在三个标准电路评估数据集上，CD-T表现出优异的性能，优于ACDC和EAP，平均ROC AUC达到97%。此外，CD-T电路的可信性证据显示，其比随机电路更忠实于原始模型，且使用较少的节点就能完全复制原始模型的行为。结果突出了CD-T在高效自动化机械解释方面的巨大潜力，为深入了解大型语言模型的工作机制铺平了道路。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>现有自动化电路发现方法存在运行速度慢、近似误差和特定指标要求等问题。</li>
<li>引入上下文分解法（CD-T）来构建大型语言模型的解释性电路。</li>
<li>CD-T能高效地产生精细到注意力头的电路。</li>
<li>通过递归计算模型中所有节点的贡献并进行修剪，显著减少电路发现时间。</li>
<li>在三个标准电路评估数据集上，CD-T表现出卓越的性能，优于其他方法。</li>
<li>CD-T电路比随机电路更忠实于原始模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.00886">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-23a0597799b2a7746915e7052120be48.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SoK-Membership-Inference-Attacks-on-LLMs-are-Rushing-Nowhere-and-How-to-Fix-It"><a href="#SoK-Membership-Inference-Attacks-on-LLMs-are-Rushing-Nowhere-and-How-to-Fix-It" class="headerlink" title="SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How   to Fix It)"></a>SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How   to Fix It)</h2><p><strong>Authors:Matthieu Meeus, Igor Shilov, Shubham Jain, Manuel Faysse, Marek Rei, Yves-Alexandre de Montjoye</strong></p>
<p>Whether LLMs memorize their training data and what this means, from measuring privacy leakage to detecting copyright violations, has become a rapidly growing area of research. In the last few months, more than 10 new methods have been proposed to perform Membership Inference Attacks (MIAs) against LLMs. Contrary to traditional MIAs which rely on fixed-but randomized-records or models, these methods are mostly trained and tested on datasets collected post-hoc. Sets of members and non-members, used to evaluate the MIA, are constructed using informed guesses after the release of a model. This lack of randomization raises concerns of a distribution shift between members and non-members. In this work, we first extensively review the literature on MIAs against LLMs and show that, while most work focuses on sequence-level MIAs evaluated in post-hoc setups, a range of target models, motivations and units of interest are considered. We then quantify distribution shifts present in 6 datasets used in the literature using a model-less bag of word classifier and show that all datasets constructed post-hoc suffer from strong distribution shifts. These shifts invalidate the claims of LLMs memorizing strongly in real-world scenarios and, potentially, also the methodological contributions of the recent papers based on these datasets. Yet, all hope might not be lost. We introduce important considerations to properly evaluate MIAs against LLMs and discuss, in turn, potential ways forwards: randomized test splits, injections of randomized (unique) sequences, randomized fine-tuning, and several post-hoc control methods. While each option comes with its advantages and limitations, we believe they collectively provide solid grounds to guide MIA development and study LLM memorization. We conclude with an overview of recommended approaches to benchmark sequence-level and document-level MIAs against LLMs. </p>
<blockquote>
<p>关于LLM是否记忆其训练数据以及这意味着什么，从衡量隐私泄露到检测版权侵犯，已成为一个迅速发展的研究领域。在过去的几个月里，已经提出了超过10种针对LLM执行成员推理攻击（MIA）的新方法。与传统依靠固定但随机记录或模型的MIA不同，这些方法大多在收集的后验数据集上进行训练和测试。用于评估MIA的成员和非成员集是在模型发布后通过有根据的猜测构建的。这种缺乏随机性引发了成员与非成员之间分布转移的担忧。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.17975v3">PDF</a> IEEE Conference on Secure and Trustworthy Machine Learning (SaTML   2025)</p>
<p><strong>Summary</strong></p>
<p>LLMs面临成员推理攻击（MIAs）的研究迅速增长，新的攻击方法不断被提出。这些新方法主要在事后收集的数据集上进行训练和测试，存在成员与非成员之间的分布偏移问题。本文对LLMs的MIAs文献进行全面回顾，发现存在一系列目标模型、动机和感兴趣单位。通过模型无关的词汇分类器量化六个数据集的分布偏移，显示所有事后构建的数据集都存在强烈的分布偏移。这些偏移对LLMs在现实场景中的记忆能力提出了质疑，也可能使基于这些数据集的方法论贡献无效。本文讨论了正确评估LLMs的MIAs的重要考虑因素，并提出了前进的潜在方式，包括随机测试分割、注入随机（唯一）序列、随机微调等。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs面临成员推理攻击（MIAs）的研究迅速增长，已提出超过10种新的攻击方法。</li>
<li>这些新方法主要在事后数据集上进行训练和测试，存在分布偏移问题。</li>
<li>大多数研究关注序列级别的MIAs和事后评估设置，但目标模型、动机和感兴趣单位存在多种选择。</li>
<li>通过模型无关的词汇分类器量化六个数据集的分布偏移，显示所有数据集都存在强烈的分布偏移。</li>
<li>分布偏移对LLMs在现实场景中的记忆能力提出质疑，也可能使基于这些数据集的方法论无效。</li>
<li>本文讨论了正确评估LLMs的MIAs的重要因素，并提出了改进方向，包括随机测试分割、注入随机序列等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.17975">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0ba3af5e4abf2c3bbfa860feaaa15f5e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2d6760fdf521774eb95254f2ee44df54.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bfdfed67f2b63d170c55ca7a38f21c47.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Efficient-Evolutionary-Search-Over-Chemical-Space-with-Large-Language-Models"><a href="#Efficient-Evolutionary-Search-Over-Chemical-Space-with-Large-Language-Models" class="headerlink" title="Efficient Evolutionary Search Over Chemical Space with Large Language   Models"></a>Efficient Evolutionary Search Over Chemical Space with Large Language   Models</h2><p><strong>Authors:Haorui Wang, Marta Skreta, Cher-Tian Ser, Wenhao Gao, Lingkai Kong, Felix Strieth-Kalthoff, Chenru Duan, Yuchen Zhuang, Yue Yu, Yanqiao Zhu, Yuanqi Du, Alán Aspuru-Guzik, Kirill Neklyudov, Chao Zhang</strong></p>
<p>Molecular discovery, when formulated as an optimization problem, presents significant computational challenges because optimization objectives can be non-differentiable. Evolutionary Algorithms (EAs), often used to optimize black-box objectives in molecular discovery, traverse chemical space by performing random mutations and crossovers, leading to a large number of expensive objective evaluations. In this work, we ameliorate this shortcoming by incorporating chemistry-aware Large Language Models (LLMs) into EAs. Namely, we redesign crossover and mutation operations in EAs using LLMs trained on large corpora of chemical information. We perform extensive empirical studies on both commercial and open-source models on multiple tasks involving property optimization, molecular rediscovery, and structure-based drug design, demonstrating that the joint usage of LLMs with EAs yields superior performance over all baseline models across single- and multi-objective settings. We demonstrate that our algorithm improves both the quality of the final solution and convergence speed, thereby reducing the number of required objective evaluations. Our code is available at <a target="_blank" rel="noopener" href="http://github.com/zoom-wang112358/MOLLEO">http://github.com/zoom-wang112358/MOLLEO</a> </p>
<blockquote>
<p>分子发现的优化问题带来了巨大的计算挑战，因为优化目标可能是不可微分的。进化算法（EA）常用于优化分子发现中的黑盒目标，它通过执行随机突变和交叉操作来遍历化学空间，从而导致大量的目标评估成本高昂。在这项工作中，我们通过将具有化学感知能力的大型语言模型（LLM）纳入进化算法来缓解这一缺点。具体来说，我们使用在大量化学信息语料库上训练的大型语言模型重新设计进化算法中的交叉和突变操作。我们在涉及属性优化、分子再发现和基于结构的药物设计等多个任务上，对商业和开源模型进行了广泛的实证研究，证明了大型语言模型与进化算法的联合使用在单目标和多目标设置中都优于所有基线模型。我们证明我们的算法提高了最终解决方案的质量和收敛速度，从而减少了所需的目标评估次数。我们的代码可在<a target="_blank" rel="noopener" href="http://github.com/zoom-wang112358/MOLLEO%E6%89%BE%E5%88%B0%E3%80%82">http://github.com/zoom-wang112358/MOLLEO找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.16976v3">PDF</a> Published in ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了将大型语言模型（LLMs）融入进化算法（EAs）来解决分子发现中的优化问题。通过利用LLMs处理化学信息的能力，重新设计了EA中的交叉和突变操作。在多个任务上进行了实证研究，证明联合使用LLMs和EAs的方法在单目标和多目标设置下均优于所有基线模型，提高了最终解决方案的质量和收敛速度，减少了所需的目标评估次数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）被融入进化算法（EAs），以解决分子发现中的优化问题。</li>
<li>LLMs用于重新设计EA中的交叉和突变操作。</li>
<li>实证研究证明了LLMs和EAs联合使用在多个任务上的优越性。</li>
<li>该方法提高了最终解决方案的质量和收敛速度。</li>
<li>该方法减少了所需的目标评估次数。</li>
<li>该算法在单目标和多目标设置下均表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.16976">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6babb78a951a372471e38a9db61ebe31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6edbcc1f355b94155e5bcbcad7211c72.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="VoCo-LLaMA-Towards-Vision-Compression-with-Large-Language-Models"><a href="#VoCo-LLaMA-Towards-Vision-Compression-with-Large-Language-Models" class="headerlink" title="VoCo-LLaMA: Towards Vision Compression with Large Language Models"></a>VoCo-LLaMA: Towards Vision Compression with Large Language Models</h2><p><strong>Authors:Xubing Ye, Yukang Gan, Xiaoke Huang, Yixiao Ge, Yansong Tang</strong></p>
<p>Vision-Language Models (VLMs) have achieved remarkable success in various multi-modal tasks, but they are often bottlenecked by the limited context window and high computational cost of processing high-resolution image inputs and videos. Vision compression can alleviate this problem by reducing the vision token count. Previous approaches compress vision tokens with external modules and force LLMs to understand the compressed ones, leading to visual information loss. However, the LLMs’ understanding paradigm of vision tokens is not fully utilised in the compression learning process. We propose VoCo-LLaMA, the first approach to compress vision tokens using LLMs. By introducing Vision Compression tokens during the vision instruction tuning phase and leveraging attention distillation, our method distill how LLMs comprehend vision tokens into their processing of VoCo tokens. VoCo-LLaMA facilitates effective vision compression and improves the computational efficiency during the inference stage. Specifically, our method achieves minimal performance loss with a compression ratio of 576$\times$, resulting in up to 94.8$%$ fewer FLOPs and 69.6$%$ acceleration in inference time. Furthermore, through continuous training using time-series compressed token sequences of video frames, VoCo-LLaMA demonstrates the ability to understand temporal correlations, outperforming previous methods on popular video question-answering benchmarks. Our approach presents a promising way to unlock the full potential of VLMs’ contextual window, enabling more scalable multi-modal applications. The project page, along with the associated code, can be accessed via <a target="_blank" rel="noopener" href="https://yxxxb.github.io/VoCo-LLaMA-page/">https://yxxxb.github.io/VoCo-LLaMA-page/</a>. </p>
<blockquote>
<p>视觉语言模型（VLMs）在各种多模态任务中取得了显著的成功，但它们常常受到处理高分辨率图像输入和视频时有限上下文窗口和高计算成本的限制。视觉压缩可以通过减少视觉令牌数量来缓解这个问题。以前的方法使用外部模块压缩视觉令牌，并强制LLMs理解压缩后的令牌，导致视觉信息丢失。然而，LLMs对视觉令牌的理解模式在压缩学习过程中没有得到充分利用。我们提出了VoCo-LLaMA，这是使用LLMs压缩视觉令牌的第一种方法。通过在视觉指令调整阶段引入视觉压缩令牌，并利用注意力蒸馏，我们的方法将LLMs如何理解视觉令牌转化为它们对VoCo令牌的处理。VoCo-LLaMA促进了有效的视觉压缩，并提高了推理阶段的计算效率。具体来说，我们的方法以576倍的压缩比实现了最小的性能损失，导致FLOPs减少高达94.8%，推理时间加速69.6%。此外，通过连续使用时间序列压缩令牌序列的视频帧，VoCo-LLaMA展示了理解时间关联的能力，在流行的视频问答基准测试中超越了之前的方法。我们的方法为解锁VLMs上下文窗口的潜力提供了一种有前途的方式，使更多可扩展的多模态应用程序成为可能。项目页面以及相关代码可以通过<a target="_blank" rel="noopener" href="https://yxxxb.github.io/VoCo-LLaMA-page/%E8%BF%BD%E9T%E%%E4%BB%A5%E4%B8%8A%E5%86%85%E5%AE%B9%E6%98%AF%E5%85%B3%E4%BA%8E%E5%AF%B9%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88VLMs%EF%BC%89%E7%9A%84%E4%B8%80%E7%A7%8D%E6%96%B0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94VoCo-LLaMA%E7%9A%84%E4%BB%8B%E7%BB%8D%EF%BC%8C%E8%AF%A5%E6%96%B9%E6%B3%95%E5%88%A9%E7%94%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLMs%EF%BC%89%E6%9D%A5%E8%BF%9B%E8%A1%8C%E8%A7%86%E8%A7%89%E5%8E%8B%E7%BC%A9%E4%BB%A5%E6%8F%90%E5%8D%87%E6%95%88%E7%8E%87%E3%80%81%E5%8E%8B%E7%BC%A9%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E5%86%97%E4%BD%99%E4%B8%8E%E8%80%97%E6%97%B6%E7%9A%84%E6%96%B9%E9%9D%A2%E3%80%81%E5%87%8F%E8%BD%BB%E5%AF%B9%E4%BA%8E%E5%A4%A7%E9%87%8F%E5%8F%82%E6%95%B0%E9%9C%80%E6%B1%82%E4%BA%A7%E7%94%9F%E7%9A%84%E8%AE%A1%E7%AE%97%E8%B4%9F%E6%8B%85%E5%B9%B6%E6%8F%90%E9%AB%98%E5%87%86%E7%A1%AE%E6%80%A7%E7%AD%89%E6%96%B9%E9%9D%A2%E8%8E%B7%E5%BE%97%E9%87%8D%E8%A6%81%E7%AA%81%E7%A0%B4%E3%80%82">https://yxxxb.github.io/VoCo-LLaMA-page/访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.12275v2">PDF</a> 11 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了在视觉语言模型（VLMs）中利用大型语言模型（LLMs）进行视觉压缩的新方法——VoCo-LLaMA。该方法引入视觉压缩令牌，在视觉指令微调阶段利用注意力蒸馏，将LLMs对视觉令牌的理解转化为对VoCo令牌的处理。VoCo-LLaMA实现了有效的视觉压缩，提高了推理阶段的计算效率，实现了高压缩比和低性能损失。此外，通过连续训练使用视频帧的时间序列压缩令牌序列，VoCo-LLaMA展现出理解时间关联的能力，并在流行的视频问答基准测试中表现出超越先前方法的效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMs在处理高分辨率图像和视频时面临上下文窗口有限和计算成本高的问题。</li>
<li>视觉压缩可以缓解这些问题，但先前的压缩方法导致视觉信息损失。</li>
<li>VoCo-LLaMA是首个利用LLMs进行视觉压缩的方法。</li>
<li>VoCo-LLaMA通过引入视觉压缩令牌和注意力蒸馏，将LLMs对视觉令牌的理解转化为处理VoCo令牌。</li>
<li>VoCo-LLaMA实现了高压缩比和低性能损失，推理阶段计算效率显著提高。</li>
<li>通过连续训练使用视频帧的时间序列压缩令牌序列，VoCo-LLaMA展现出理解时间关联的能力。</li>
<li>VoCo-LLaMA在视频问答基准测试中表现优于先前方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.12275">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4d55abe1b0567abfc73570959167faf6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-96422a866cc25660d3e4522501d94afe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc41a0abf033bab665bfe4d81dd7dd13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-823a328fcc819653641f37712b73944f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fc191013c4d17bdae0c39344a786ad4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fa6eae18cccbde8f9eb65b9f98eb9c0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Mixture-of-Subspaces-in-Low-Rank-Adaptation"><a href="#Mixture-of-Subspaces-in-Low-Rank-Adaptation" class="headerlink" title="Mixture-of-Subspaces in Low-Rank Adaptation"></a>Mixture-of-Subspaces in Low-Rank Adaptation</h2><p><strong>Authors:Taiqiang Wu, Jiahao Wang, Zhe Zhao, Ngai Wong</strong></p>
<p>In this paper, we introduce a subspace-inspired Low-Rank Adaptation (LoRA) method, which is computationally efficient, easy to implement, and readily applicable to large language, multimodal, and diffusion models. Initially, we equivalently decompose the weights of LoRA into two subspaces, and find that simply mixing them can enhance performance. To study such a phenomenon, we revisit it through a fine-grained subspace lens, showing that such modification is equivalent to employing a fixed mixer to fuse the subspaces. To be more flexible, we jointly learn the mixer with the original LoRA weights, and term the method Mixture-of-Subspaces LoRA (MoSLoRA). MoSLoRA consistently outperforms LoRA on tasks in different modalities, including commonsense reasoning, visual instruction tuning, and subject-driven text-to-image generation, demonstrating its effectiveness and robustness. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/wutaiqiang/MoSLoRA">https://github.com/wutaiqiang/MoSLoRA</a>. </p>
<blockquote>
<p>本文介绍了一种受子空间启发的低秩适配（LoRA）方法，该方法计算效率高，易于实现，可轻松应用于大型语言、多模态和扩散模型。首先，我们将LoRA的权重等价地分解为两个子空间，并发现简单地混合它们可以增强性能。为了研究这一现象，我们通过精细的子空间透镜重新考察，表明这种修改相当于使用固定的混合器来融合子空间。为了更灵活，我们与原始LoRA权重一起学习混合器，并将该方法称为“子空间混合LoRA（MoSLoRA）”。MoSLoRA在不同模态的任务上均优于LoRA，包括常识推理、视觉指令调整和主题驱动的文字到图像生成，证明了其有效性和稳健性。代码可从<a target="_blank" rel="noopener" href="https://github.com/wutaiqiang/MoSLoRA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/wutaiqiang/MoSLoRA获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.11909v4">PDF</a> EMNLP 2024 Main, Oral</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种受子空间启发的低秩适应（LoRA）方法，该方法计算效率高、易于实现，可广泛应用于大型语言、多模态和扩散模型。通过对LoRA权重进行等效分解，并引入混合技术，提出了一种名为MoSLoRA的混合子空间LoRA方法。MoSLoRA在不同模态的任务上表现出优越性能，包括常识推理、视觉指令调整和主题驱动的文字图像生成，凸显其有效性和稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种新的低秩适应（LoRA）方法，适用于大型语言、多模态和扩散模型。</li>
<li>通过等效分解LoRA权重，发现了混合技术能增强性能。</li>
<li>MoSLoRA方法通过引入子空间混合概念，对LoRA进行了改进。</li>
<li>MoSLoRA在常识推理、视觉指令调整和文本图像生成等任务上表现优越。</li>
<li>MoSLoRA方法具有计算效率高和易于实现的特点。</li>
<li>研究表明，MoSLoRA方法能提高模型的灵活性和性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.11909">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-dfed933bdc8d34b5d8b61ca51ba771ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5f6061723325ed88888410170cd452a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3368db1fb1a0b75869f5852f8c9ad186.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f7af46f790d58bf801e5aa0710f1d23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4b426ae941893be77f84865843862ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57ea34c42224a2c7f987bd00b8d5a7bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2a3f7268d6e635b2b548c6d879aa450.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78e569ff3a94cd3bb0325d0d95cd4ebf.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-11/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-11/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-11/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-818ed1ec7c1943f86c1bb77eea7ec35c.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-03-11  A Survey of Large Language Model Empowered Agents for Recommendation and   Search Towards Next-Generation Information Retrieval
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-11/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-94055dde32c30802ccdd214b8a84c796.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-03-11  Symbolic Mixture-of-Experts Adaptive Skill-based Routing for   Heterogeneous Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26384.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
