<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-11  A Survey of Large Language Model Empowered Agents for Recommendation and   Search Towards Next-Generation Information Retrieval">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-62e21c85bd0a35d31f4248953d8d42f5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-11-æ›´æ–°"><a href="#2025-03-11-æ›´æ–°" class="headerlink" title="2025-03-11 æ›´æ–°"></a>2025-03-11 æ›´æ–°</h1><h2 id="A-Survey-of-Large-Language-Model-Empowered-Agents-for-Recommendation-and-Search-Towards-Next-Generation-Information-Retrieval"><a href="#A-Survey-of-Large-Language-Model-Empowered-Agents-for-Recommendation-and-Search-Towards-Next-Generation-Information-Retrieval" class="headerlink" title="A Survey of Large Language Model Empowered Agents for Recommendation and   Search: Towards Next-Generation Information Retrieval"></a>A Survey of Large Language Model Empowered Agents for Recommendation and   Search: Towards Next-Generation Information Retrieval</h2><p><strong>Authors:Yu Zhang, Shutong Qiao, Jiaqi Zhang, Tzu-Heng Lin, Chen Gao, Yong Li</strong></p>
<p>Information technology has profoundly altered the way humans interact with information. The vast amount of content created, shared, and disseminated online has made it increasingly difficult to access relevant information. Over the past two decades, search and recommendation systems (collectively referred to as information retrieval systems) have evolved significantly to address these challenges. Recent advances in large language models (LLMs) have demonstrated capabilities that surpass human performance in various language-related tasks and exhibit general understanding, reasoning, and decision-making abilities. This paper explores the transformative potential of large language model agents in enhancing search and recommendation systems. We discuss the motivations and roles of LLM agents, and establish a classification framework to elaborate on the existing research. We highlight the immense potential of LLM agents in addressing current challenges in search and recommendation, providing insights into future research directions. This paper is the first to systematically review and classify the research on LLM agents in these domains, offering a novel perspective on leveraging this advanced AI technology for information retrieval. To help understand the existing works, we list the existing papers on agent-based simulation with large language models at this link: <a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search">https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search</a>. </p>
<blockquote>
<p>ä¿¡æ¯æŠ€æœ¯å·²ç»æ·±åˆ»æ”¹å˜äº†äººç±»ä¸ä¿¡æ¯çš„äº¤äº’æ–¹å¼ã€‚ç½‘ä¸Šåˆ›å»ºã€å…±äº«å’Œä¼ æ’­çš„å¤§é‡å†…å®¹ä½¿å¾—è·å–ç›¸å…³ä¿¡æ¯çš„éš¾åº¦è¶Šæ¥è¶Šå¤§ã€‚åœ¨è¿‡å»çš„äºŒåå¹´ä¸­ï¼Œæœç´¢å’Œæ¨èç³»ç»Ÿï¼ˆç»Ÿç§°ä¸ºä¿¡æ¯æ£€ç´¢ç³»ç»Ÿï¼‰å·²ç»å‘ç”Ÿäº†æ˜¾è‘—çš„å˜åŒ–ï¼Œä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•åœ¨å„ç§è¯­è¨€ç›¸å…³ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†è¶…è¶Šäººç±»çš„æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºäº†ä¸€èˆ¬ç†è§£ã€æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†åœ¨å¢å¼ºæœç´¢å’Œæ¨èç³»ç»Ÿæ–¹é¢çš„å˜é©æ½œåŠ›ã€‚æˆ‘ä»¬è®¨è®ºäº†LLMä»£ç†çš„åŠ¨æœºå’Œè§’è‰²ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªåˆ†ç±»æ¡†æ¶æ¥è¯¦ç»†é˜è¿°ç°æœ‰ç ”ç©¶ã€‚æˆ‘ä»¬å¼ºè°ƒäº†LLMä»£ç†åœ¨è§£å†³æœç´¢å’Œæ¨èæ–¹é¢çš„å½“å‰æŒ‘æˆ˜æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†è§è§£ã€‚æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿå›é¡¾å’Œåˆ†ç±»äº†è¿™äº›é¢†åŸŸä¸­çš„LLMä»£ç†ç ”ç©¶ï¼Œä¸ºå¦‚ä½•åˆ©ç”¨è¿™ç§å…ˆè¿›çš„AIæŠ€æœ¯è¿›è¡Œä¿¡æ¯æ£€ç´¢æä¾›äº†ä¸€ä¸ªæ–°é¢–çš„è§†è§’ã€‚ä¸ºäº†å¸®åŠ©ç†è§£ç°æœ‰å·¥ä½œï¼Œæˆ‘ä»¬åœ¨ä»¥ä¸‹é“¾æ¥åˆ—å‡ºäº†å…³äºåŸºäºä»£ç†æ¨¡æ‹Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„ç°æœ‰è®ºæ–‡ï¼š<a target="_blank" rel="noopener" href="https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search">https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05659v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ä¿¡æ¯æŠ€æœ¯æ·±åˆ»æ”¹å˜äº†äººç±»ä¸ä¿¡æ¯çš„äº¤äº’æ–¹å¼ã€‚éšç€åœ¨çº¿åˆ›å»ºã€å…±äº«å’Œä¼ æ’­çš„å†…å®¹é‡æ¿€å¢ï¼Œè®¿é—®ç›¸å…³ä¿¡æ¯å˜å¾—è¶Šæ¥è¶Šå›°éš¾ã€‚æœç´¢å’Œæ¨èç³»ç»Ÿï¼ˆç»Ÿç§°ä¸ºä¿¡æ¯æ£€ç´¢ç³»ç»Ÿï¼‰åœ¨è¿‡å»çš„äºŒåå¹´ä¸­å·²ç»æ˜¾è‘—å‘å±•ï¼Œä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•åœ¨å„ç§è¯­è¨€ç›¸å…³ä»»åŠ¡ä¸­å±•ç°äº†è¶…è¶Šäººç±»æ€§èƒ½çš„èƒ½åŠ›ï¼Œå¹¶å±•ç°å‡ºä¸€èˆ¬æ€§çš„ç†è§£ã€æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ä»£ç†åœ¨ä¿¡æ¯æ£€ç´¢ä¸­å¢å¼ºæœç´¢å’Œæ¨èç³»ç»Ÿçš„å˜é©æ½œåŠ›ã€‚æˆ‘ä»¬è®¨è®ºäº†LLMä»£ç†çš„åŠ¨æœºå’Œè§’è‰²ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªåˆ†ç±»æ¡†æ¶æ¥é˜è¿°ç°æœ‰ç ”ç©¶ã€‚æˆ‘ä»¬å¼ºè°ƒäº†LLMä»£ç†åœ¨è§£å†³æœç´¢å’Œæ¨èä¸­çš„å½“å‰æŒ‘æˆ˜æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†è§è§£ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°å›é¡¾å’Œåˆ†ç±»äº†åœ¨è¿™äº›é¢†åŸŸä¸­å…³äºLLMä»£ç†çš„ç ”ç©¶ï¼Œä¸ºå¦‚ä½•åˆ©ç”¨è¿™ä¸€å…ˆè¿›çš„AIæŠ€æœ¯è¿›è¡Œä¿¡æ¯æ£€ç´¢æä¾›äº†æ–°çš„è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¿¡æ¯æŠ€æœ¯æ”¹å˜äº†äººç±»ä¸ä¿¡æ¯çš„äº¤äº’æ–¹å¼ï¼Œä¿¡æ¯è¿‡è½½é—®é¢˜æ—¥ç›Šçªå‡ºã€‚</li>
<li>æœç´¢å’Œæ¨èç³»ç»Ÿå·²æ˜¾è‘—å‘å±•ï¼Œä»¥åº”å¯¹ä¿¡æ¯è¿‡è½½æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°è¶…è¶Šäººç±»ï¼Œå…·å¤‡ç†è§£ã€æ¨ç†å’Œå†³ç­–èƒ½åŠ›ã€‚</li>
<li>LLMåœ¨ä¿¡æ¯æ£€ç´¢ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¯å¢å¼ºæœç´¢å’Œæ¨èç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>LLMä»£ç†çš„åŠ¨æœºå’Œè§’è‰²åœ¨ç ”ç©¶ä¸­å—åˆ°å…³æ³¨ï¼Œå¹¶å»ºç«‹äº†åˆ†ç±»æ¡†æ¶æ¥é˜è¿°ç°æœ‰ç ”ç©¶ã€‚</li>
<li>LLMä»£ç†èƒ½å¤Ÿè§£å†³æœç´¢å’Œæ¨èä¸­çš„å½“å‰æŒ‘æˆ˜ï¼Œæä¾›æœªæ¥ç ”ç©¶æ–¹å‘çš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05659">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-09728ef6b9800d8ee303f8508009b376.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cc6a1ec69b246209f029e4ed3b7fb080.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="R1-Searcher-Incentivizing-the-Search-Capability-in-LLMs-via-Reinforcement-Learning"><a href="#R1-Searcher-Incentivizing-the-Search-Capability-in-LLMs-via-Reinforcement-Learning" class="headerlink" title="R1-Searcher: Incentivizing the Search Capability in LLMs via   Reinforcement Learning"></a>R1-Searcher: Incentivizing the Search Capability in LLMs via   Reinforcement Learning</h2><p><strong>Authors:Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Ji-Rong Wen</strong></p>
<p>Existing Large Reasoning Models (LRMs) have shown the potential of reinforcement learning (RL) to enhance the complex reasoning capabilities of Large Language Models~(LLMs). While they achieve remarkable performance on challenging tasks such as mathematics and coding, they often rely on their internal knowledge to solve problems, which can be inadequate for time-sensitive or knowledge-intensive questions, leading to inaccuracies and hallucinations. To address this, we propose \textbf{R1-Searcher}, a novel two-stage outcome-based RL approach designed to enhance the search capabilities of LLMs. This method allows LLMs to autonomously invoke external search systems to access additional knowledge during the reasoning process. Our framework relies exclusively on RL, without requiring process rewards or distillation for a cold start. % effectively generalizing to out-of-domain datasets and supporting both Base and Instruct models. Our experiments demonstrate that our method significantly outperforms previous strong RAG methods, even when compared to the closed-source GPT-4o-mini. </p>
<blockquote>
<p>ç°æœ‰çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰å·²ç»æ˜¾ç¤ºå‡ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚è™½ç„¶å®ƒä»¬åœ¨æ•°å­¦å’Œç¼–ç ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆç»©ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºå†…éƒ¨çŸ¥è¯†æ¥è§£å†³é—®é¢˜ï¼Œè¿™å¯¹äºæ—¶é—´æ•æ„Ÿæˆ–çŸ¥è¯†å¯†é›†çš„é—®é¢˜å¯èƒ½ä¸å¤Ÿç”¨ï¼Œå¯¼è‡´ä¸å‡†ç¡®å’Œå¹»è§‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†\textbf{R1-Searcher}ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µæˆæœå¯¼å‘çš„RLæ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºLLMçš„æœç´¢èƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•å…è®¸LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªä¸»è°ƒç”¨å¤–éƒ¨æœç´¢ç³»ç»Ÿæ¥è®¿é—®é¢å¤–çš„çŸ¥è¯†ã€‚æˆ‘ä»¬çš„æ¡†æ¶å®Œå…¨ä¾èµ–äºRLï¼Œæ— éœ€å¤„ç†å¥–åŠ±æˆ–è’¸é¦å†·å¯åŠ¨ç­‰å¤æ‚æµç¨‹ã€‚%èƒ½å¤Ÿæœ‰æ•ˆåœ°æ³›åŒ–åˆ°åŸŸå¤–æ•°æ®é›†å¹¶æ”¯æŒåŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤æ¨¡å‹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºä»¥å‰çš„å¼ºå¤§RAGæ–¹æ³•ï¼Œå³ä½¿ä¸é—­æºçš„GPT-4o-miniç›¸æ¯”ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05592v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤æ‚æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚ç°æœ‰å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨æ•°å­¦å’Œç¼–ç¨‹ç­‰æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†å¾€å¾€ä¾èµ–å†…éƒ¨çŸ¥è¯†è§£å†³é—®é¢˜ï¼Œå¯¹äºæ—¶é—´æ•æ„Ÿæˆ–çŸ¥è¯†å¯†é›†å‹é—®é¢˜å¯èƒ½æ˜¾å¾—ä¸è¶³ï¼Œå¯¼è‡´ä¸å‡†ç¡®å’Œå¹»è§‰ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºæ–°å‹ä¸¤é˜¶æ®µç»“æœå¯¼å‘çš„RLæ–¹æ³•â€”â€”R1-Searcherï¼Œæ—¨åœ¨å¢å¼ºLLMçš„æœç´¢èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªä¸»è°ƒç”¨å¤–éƒ¨æœç´¢ç³»ç»Ÿè·å–é¢å¤–çŸ¥è¯†ä»¥è¾…åŠ©æ¨ç†è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•ä»…ä¾èµ–RLï¼Œæ— éœ€è¿‡ç¨‹å¥–åŠ±æˆ–å†·å¯åŠ¨æ—¶çš„è’¸é¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ç”¨äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>LRMsåœ¨æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨æ—¶é—´æ•æ„Ÿå’ŒçŸ¥è¯†å¯†é›†å‹é—®é¢˜ä¸Šå­˜åœ¨ä¸è¶³ã€‚</li>
<li>R1-Searcheræ˜¯ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µç»“æœå¯¼å‘çš„RLæ–¹æ³•ï¼Œæ—¨åœ¨å¢å¼ºLLMçš„æœç´¢èƒ½åŠ›ã€‚</li>
<li>LLMså¯è‡ªä¸»è°ƒç”¨å¤–éƒ¨æœç´¢ç³»ç»Ÿè·å–é¢å¤–çŸ¥è¯†ä»¥è¾…åŠ©æ¨ç†ã€‚</li>
<li>R1-Searcheræ–¹æ³•æ˜¾è‘—æé«˜ä¹‹å‰çš„RAGæ–¹æ³•æ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šé—­æºçš„GPT-4o-miniã€‚</li>
<li>è¯¥æ–¹æ³•ä»…ä¾èµ–å¼ºåŒ–å­¦ä¹ ï¼Œæ— éœ€è¿‡ç¨‹å¥–åŠ±æˆ–å†·å¯åŠ¨æ—¶çš„çŸ¥è¯†è’¸é¦ã€‚</li>
<li>æ–¹æ³•å…·æœ‰æ¨å¹¿è‡³è·¨é¢†åŸŸæ•°æ®é›†çš„èƒ½åŠ›ï¼Œå¹¶é€‚ç”¨äºåŸºç¡€æ¨¡å‹å’ŒæŒ‡ä»¤æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aa0cce0eef49a56440a233b97ab14c00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f962a90cf16a20b5690ab8dd46334a8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-603d2edf70630ae009a368b957e8eadc.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LLM-based-Iterative-Approach-to-Metamodeling-in-Automotive"><a href="#LLM-based-Iterative-Approach-to-Metamodeling-in-Automotive" class="headerlink" title="LLM-based Iterative Approach to Metamodeling in Automotive"></a>LLM-based Iterative Approach to Metamodeling in Automotive</h2><p><strong>Authors:Nenad Petrovic, Fengjunjie Pan, Vahid Zolfaghari, Alois Knoll</strong></p>
<p>In this paper, we introduce an automated approach to domain-specific metamodel construction relying on Large Language Model (LLM). The main focus is adoption in automotive domain. As outcome, a prototype was implemented as web service using Python programming language, while OpenAIâ€™s GPT-4o was used as the underlying LLM. Based on the initial experiments, this approach successfully constructs Ecore metamodel based on set of automotive requirements and visualizes it making use of PlantUML notation, so human experts can provide feedback in order to refine the result. Finally, locally deployable solution is also considered, including the limitations and additional steps required. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é¢†åŸŸç‰¹å®šå…ƒæ¨¡å‹æ„å»ºè‡ªåŠ¨åŒ–æ–¹æ³•ã€‚ä¸»è¦å…³æ³¨åœ¨æ±½è½¦è¡Œä¸šçš„åº”ç”¨ã€‚æˆ‘ä»¬é‡‡ç”¨Pythonç¼–ç¨‹è¯­è¨€å®ç°äº†ä¸€ä¸ªåŸå‹ä½œä¸ºwebæœåŠ¡ï¼ŒåŒæ—¶ä½¿ç”¨äº†OpenAIçš„GPT-4oä½œä¸ºåŸºç¡€çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚åŸºäºåˆæ­¥å®éªŒï¼Œè¯¥æ–¹æ³•æˆåŠŸæ ¹æ®æ±½è½¦éœ€æ±‚æ„å»ºäº†Ecoreå…ƒæ¨¡å‹ï¼Œå¹¶åˆ©ç”¨PlantUMLç¬¦å·è¿›è¡Œå¯è§†åŒ–ï¼Œä»¥ä¾¿äººç±»ä¸“å®¶æä¾›åé¦ˆä»¥å®Œå–„ç»“æœã€‚æœ€åï¼Œè¿˜è€ƒè™‘äº†å¯æœ¬åœ°éƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬å…¶å±€é™æ€§å’Œæ‰€éœ€çš„é¢å¤–æ­¥éª¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05449v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨åŒ–é¢†åŸŸç‰¹å®šå…ƒæ¨¡å‹æ„å»ºæ–¹æ³•è¢«ä»‹ç»ã€‚é‡ç‚¹ç ”ç©¶å…¶åœ¨æ±½è½¦é¢†åŸŸçš„åº”ç”¨ã€‚é€šè¿‡ä½¿ç”¨Pythonç¼–ç¨‹è¯­è¨€å®ç°åŸå‹ä½œä¸ºWebæœåŠ¡ï¼Œå¹¶åˆ©ç”¨OpenAIçš„GPT-4oä½œä¸ºåº•å±‚LLMã€‚åˆæ­¥å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ ¹æ®æ±½è½¦éœ€æ±‚æ„å»ºEcoreå…ƒæ¨¡å‹ï¼Œå¹¶åˆ©ç”¨PlantUMLç¬¦å·è¿›è¡Œå¯è§†åŒ–ï¼Œä½¿äººç±»ä¸“å®¶å¯ä»¥æä¾›åé¦ˆä»¥å®Œå–„ç»“æœã€‚æœ€åè¿˜è€ƒè™‘äº†æœ¬åœ°å¯éƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬å…¶å±€é™æ€§å’Œæ‰€éœ€é¢å¤–æ­¥éª¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†åŸºäºLLMçš„è‡ªåŠ¨åŒ–é¢†åŸŸç‰¹å®šå…ƒæ¨¡å‹æ„å»ºæ–¹æ³•ã€‚</li>
<li>ç ”ç©¶çš„é‡ç‚¹æ˜¯åœ¨æ±½è½¦é¢†åŸŸçš„åº”ç”¨ã€‚</li>
<li>ä½¿ç”¨Pythonå®ç°åŸå‹ä½œä¸ºWebæœåŠ¡ï¼Œå¹¶ä¾èµ–OpenAIçš„GPT-4oä½œä¸ºLLMã€‚</li>
<li>åˆæ­¥å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ ¹æ®æ±½è½¦éœ€æ±‚æ„å»ºEcoreå…ƒæ¨¡å‹ã€‚</li>
<li>åˆ©ç”¨PlantUMLç¬¦å·è¿›è¡Œå¯è§†åŒ–ï¼Œä»¥ä¾¿äººç±»ä¸“å®¶æä¾›åé¦ˆä»¥æ”¹è¿›ç»“æœã€‚</li>
<li>è€ƒè™‘äº†æœ¬åœ°å¯éƒ¨ç½²çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c2dcc90c858f0f1d3c7912780b014bb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c54f635f6646ea837df213ad9f668d16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5840f177b2b0dc7bb24b8bcafe81e7d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31aee169e659eeb3f4e52120c7330f5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2138c17ba4d205312f11c141c74e0749.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2555f1deb7d238d23b7def4462ba8b4a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Are-Your-LLM-based-Text-to-SQL-Models-Secure-Exploring-SQL-Injection-via-Backdoor-Attacks"><a href="#Are-Your-LLM-based-Text-to-SQL-Models-Secure-Exploring-SQL-Injection-via-Backdoor-Attacks" class="headerlink" title="Are Your LLM-based Text-to-SQL Models Secure? Exploring SQL Injection   via Backdoor Attacks"></a>Are Your LLM-based Text-to-SQL Models Secure? Exploring SQL Injection   via Backdoor Attacks</h2><p><strong>Authors:Meiyu Lin, Haichuan Zhang, Jiale Lao, Renyuan Li, Yuanchun Zhou, Carl Yang, Yang Cao, Mingjie Tang</strong></p>
<p>Large language models (LLMs) have shown state-of-the-art results in translating natural language questions into SQL queries (Text-to-SQL), a long-standing challenge within the database community. However, security concerns remain largely unexplored, particularly the threat of backdoor attacks, which can introduce malicious behaviors into models through fine-tuning with poisoned datasets. In this work, we systematically investigate the vulnerabilities of LLM-based Text-to-SQL models and present ToxicSQL, a novel backdoor attack framework. Our approach leverages stealthy {semantic and character-level triggers} to make backdoors difficult to detect and remove, ensuring that malicious behaviors remain covert while maintaining high model accuracy on benign inputs. Furthermore, we propose leveraging SQL injection payloads as backdoor targets, enabling the generation of malicious yet executable SQL queries, which pose severe security and privacy risks in language model-based SQL development. We demonstrate that injecting only 0.44% of poisoned data can result in an attack success rate of 79.41%, posing a significant risk to database security. Additionally, we propose detection and mitigation strategies to enhance model reliability. Our findings highlight the urgent need for security-aware Text-to-SQL development, emphasizing the importance of robust defenses against backdoor threats. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªç„¶è¯­è¨€ç¿»è¯‘æˆä¸ºSQLæŸ¥è¯¢ï¼ˆæ–‡æœ¬åˆ°SQLï¼‰å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œè¿™æ˜¯æ•°æ®åº“é¢†åŸŸé•¿æœŸä»¥æ¥çš„ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œå®‰å…¨é—®é¢˜ä»æœªå¾—åˆ°å……åˆ†ç ”ç©¶ï¼Œå°¤å…¶æ˜¯åé—¨æ”»å‡»å¸¦æ¥çš„å¨èƒã€‚åé—¨æ”»å‡»å¯ä»¥é€šè¿‡ä¸­æ¯’æ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»è€Œå¼•å…¥æ¶æ„è¡Œä¸ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†åŸºäºLLMçš„æ–‡æœ¬åˆ°SQLæ¨¡å‹çš„æ¼æ´ï¼Œå¹¶æ¨å‡ºäº†ToxicSQLï¼Œä¸€ç§æ–°å‹çš„åé—¨æ”»å‡»æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨éšè”½çš„è¯­ä¹‰å’Œå­—ç¬¦çº§è§¦å‘å™¨ä½¿åé—¨éš¾ä»¥æ£€æµ‹å’Œç§»é™¤ï¼Œç¡®ä¿åœ¨è‰¯æ€§è¾“å…¥ä¸Šä¿æŒæ¨¡å‹å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œæ¶æ„è¡Œä¸ºä¿æŒéšè”½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨SQLæ³¨å…¥è´Ÿè½½ä½œä¸ºåé—¨ç›®æ ‡ï¼Œèƒ½å¤Ÿç”Ÿæˆæ¶æ„ä¸”å¯æ‰§è¡Œçš„SQLæŸ¥è¯¢ï¼Œè¿™åœ¨åŸºäºè¯­è¨€æ¨¡å‹çš„SQLå¼€å‘ä¸­å¸¦æ¥äº†ä¸¥é‡çš„å®‰å…¨å’Œéšç§é£é™©ã€‚æˆ‘ä»¬è¯æ˜ï¼Œä»…æ³¨å…¥0.44%çš„ä¸­æ¯’æ•°æ®å°±å¯ä»¥è¾¾åˆ°79.41%çš„æ”»å‡»æˆåŠŸç‡ï¼Œç»™æ•°æ®åº“å®‰å…¨å¸¦æ¥æ˜¾è‘—é£é™©ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†æ£€æµ‹å’Œç¼“è§£ç­–ç•¥æ¥æé«˜æ¨¡å‹å¯é æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†å®‰å…¨æ„è¯†çš„æ–‡æœ¬åˆ°SQLå¼€å‘çš„ç´§è¿«éœ€æ±‚ï¼Œå¹¶å¼ºè°ƒäº†å¯¹æŠ—åé—¨å¨èƒçš„ç¨³å¥é˜²å¾¡çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05445v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMåŸºäºçš„Text-to-SQLæ¨¡å‹å­˜åœ¨å®‰å…¨é£é™©ï¼Œæœ¬æ–‡æå‡ºäº†é’ˆå¯¹è¯¥æ¨¡å‹çš„ToxicSQLåé—¨æ”»å‡»æ¡†æ¶ï¼Œåˆ©ç”¨è¯­ä¹‰å’Œå­—ç¬¦çº§è§¦å‘å™¨ä½¿åé—¨éš¾ä»¥æ£€æµ‹å’Œç§»é™¤ï¼Œé€šè¿‡æ³¨å…¥SQLæ³¨å…¥è´Ÿè½½ç”Ÿæˆæ¶æ„ä¸”å¯æ‰§è¡Œçš„SQLæŸ¥è¯¢ï¼Œå¯¹æ•°æ®åº“å®‰å…¨æ„æˆä¸¥é‡å¨èƒã€‚åŒæ—¶ï¼Œæœ¬æ–‡ä¹Ÿæå‡ºäº†æ£€æµ‹å’Œç¼“è§£ç­–ç•¥æ¥æå‡æ¨¡å‹å¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„Text-to-SQLæ¨¡å‹é¢ä¸´åé—¨æ”»å‡»çš„å®‰å…¨é£é™©ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åé—¨æ”»å‡»æ¡†æ¶â€”â€”ToxicSQLã€‚</li>
<li>åˆ©ç”¨è¯­ä¹‰å’Œå­—ç¬¦çº§è§¦å‘å™¨ä½¿åé—¨éš¾ä»¥æ£€æµ‹å’Œç§»é™¤ã€‚</li>
<li>é€šè¿‡æ³¨å…¥SQLæ³¨å…¥è´Ÿè½½ç”Ÿæˆæ¶æ„ä¸”å¯æ‰§è¡Œçš„SQLæŸ¥è¯¢ã€‚</li>
<li>å°‘é‡ï¼ˆ0.44%ï¼‰çš„å¸¦æ¯’æ•°æ®å³å¯å¯¼è‡´é«˜è¾¾79.41%çš„æ”»å‡»æˆåŠŸç‡ã€‚</li>
<li>æå‡ºäº†æ£€æµ‹å’Œç¼“è§£ç­–ç•¥ä»¥æå‡æ¨¡å‹å¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05445">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0df934928f6d4bd8bd996bc9a5334364.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c76f5eb3d01ffa29ca4df3e07d6cb61.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-390a17a1e4107dac097485c954f554b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04231effcfe5d79158ea2dd02dc272ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd23844e137e3bd173da1e5760cf1a2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c98bbe7d44261d79943106ccb3211198.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MatrixFlow-System-Accelerator-co-design-for-high-performance-transformer-applications"><a href="#MatrixFlow-System-Accelerator-co-design-for-high-performance-transformer-applications" class="headerlink" title="MatrixFlow: System-Accelerator co-design for high-performance   transformer applications"></a>MatrixFlow: System-Accelerator co-design for high-performance   transformer applications</h2><p><strong>Authors:Qunyou Liu, Marina Zapater, David Atienza</strong></p>
<p>Transformers are central to advances in artificial intelligence (AI), excelling in fields ranging from computer vision to natural language processing. Despite their success, their large parameter count and computational demands challenge efficient acceleration. To address these limitations, this paper proposes MatrixFlow, a novel co-designed system-accelerator architecture based on a loosely coupled systolic array including a new software mapping approach for efficient transformer code execution. MatrixFlow is co-optimized via a novel dataflow-based matrix multiplication technique that reduces memory overhead. These innovations significantly improve data throughput, which is critical for handling the extensive computations required by transformers. We validate our approach through full system simulation using gem5 across various BERT and ViT Transformer models featuring different data types, demonstrating significant application-wide speed-ups. Our method achieves up to a 22x improvement compared to a many-core CPU system, and outperforms the closest state-of-the-art loosely-coupled and tightly-coupled accelerators by over 5x and 8x, respectively. </p>
<blockquote>
<p>Transformeræ˜¯äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰è¿›æ­¥çš„æ ¸å¿ƒï¼Œå…¶åœ¨ä»è®¡ç®—æœºè§†è§‰åˆ°è‡ªç„¶è¯­è¨€å¤„ç†ç­‰å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚å°½ç®¡å®ƒä»¬å–å¾—äº†æˆåŠŸï¼Œä½†ç”±äºå‚æ•°ä¼—å¤šå’Œè®¡ç®—éœ€æ±‚å·¨å¤§ï¼Œå®ƒä»¬é¢ä¸´ç€æœ‰æ•ˆåŠ é€Ÿçš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†MatrixFlowï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ååŒè®¾è®¡çš„ç³»ç»ŸåŠ é€Ÿå™¨æ¶æ„ï¼ŒåŸºäºæ¾æ•£è€¦åˆçš„è„‰åŠ¨é˜µåˆ—ï¼ŒåŒ…æ‹¬ä¸€ç§ç”¨äºé«˜æ•ˆæ‰§è¡ŒTransformerä»£ç çš„æ–°è½¯ä»¶æ˜ å°„æ–¹æ³•ã€‚MatrixFlowé€šè¿‡åŸºäºæ•°æ®æµçš„æ–°å‹çŸ©é˜µä¹˜æ³•æŠ€æœ¯è¿›è¡Œä¼˜åŒ–ï¼Œå‡å°‘äº†å†…å­˜å¼€é”€ã€‚è¿™äº›åˆ›æ–°å¤§å¤§æé«˜äº†æ•°æ®ååé‡ï¼Œå¯¹äºå¤„ç†Transformeræ‰€éœ€çš„å¤§é‡è®¡ç®—è€Œè¨€è‡³å…³é‡è¦ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨gem5è¿›è¡Œå…¨é¢ç³»ç»Ÿä»¿çœŸï¼ŒéªŒè¯äº†å„ç§BERTå’ŒViT Transformeræ¨¡å‹çš„æ–¹æ³•ï¼Œè¿™äº›æ¨¡å‹å…·æœ‰ä¸åŒçš„æ•°æ®ç±»å‹ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„å…¨åº”ç”¨åŠ é€Ÿæ•ˆæœã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸å¤šæ ¸CPUç³»ç»Ÿç›¸æ¯”ï¼Œå®ç°äº†æœ€é«˜è¾¾22å€çš„æ”¹è¿›ï¼Œå¹¶ä¸”æ¯”æœ€æ–°çš„æ¾æ•£è€¦åˆå’Œç´§å¯†è€¦åˆåŠ é€Ÿå™¨åˆ†åˆ«é«˜å‡ºè¶…è¿‡5å€å’Œ8å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05290v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹äººå·¥æ™ºèƒ½é¢†åŸŸä¸­çš„Transformeræ¨¡å‹è®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMatrixFlowçš„æ–°å‹ååŒè®¾è®¡ç³»ç»ŸåŠ é€Ÿå™¨æ¶æ„ã€‚è¯¥æ¶æ„åŸºäºæ¾æ•£è€¦åˆçš„æ”¶ç¼©é˜µåˆ—ï¼ŒåŒ…æ‹¬æ–°çš„è½¯ä»¶æ˜ å°„æ–¹æ³•å’ŒåŸºäºæ•°æ®æµçŸ©é˜µä¹˜æ³•çš„ä¼˜åŒ–æŠ€æœ¯ã€‚MatrixFlowæ˜¾è‘—æé«˜äº†æ•°æ®ååé‡ï¼Œè¿™å¯¹äºå¤„ç†Transformeræ‰€éœ€çš„å¤§é‡è®¡ç®—è‡³å…³é‡è¦ã€‚é€šè¿‡gem5è¿›è¡Œçš„å…¨ç³»ç»Ÿä»¿çœŸéªŒè¯ï¼Œåœ¨å¤šç§BERTå’ŒViT Transformeræ¨¡å‹ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†æ˜¾è‘—çš„åº”ç”¨åŠ é€Ÿï¼Œç›¸æ¯”äºå¤šæ ¸CPUç³»ç»Ÿæå‡äº†æœ€é«˜è¾¾22å€çš„é€Ÿåº¦ï¼Œå¹¶ä¸”ç›¸è¾ƒäºç›®å‰å…ˆè¿›çš„æ¾æ•£è€¦åˆå’Œç´§å¯†è€¦åˆåŠ é€Ÿå™¨åˆ†åˆ«æœ‰è¶…è¿‡5å€å’Œ8å€çš„æ€§èƒ½ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformersæ˜¯äººå·¥æ™ºèƒ½é¢†åŸŸçš„é‡è¦çªç ´ï¼Œä½†è®¡ç®—æ•ˆç‡å’Œå†…å­˜éœ€æ±‚æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>MatrixFlowæ˜¯ä¸€ç§æ–°å‹ç³»ç»ŸåŠ é€Ÿå™¨æ¶æ„ï¼Œä¸“ä¸ºä¼˜åŒ–Transformerçš„è®¡ç®—æ•ˆç‡è€Œè®¾è®¡ã€‚</li>
<li>MatrixFlowåŸºäºæ¾æ•£è€¦åˆçš„æ”¶ç¼©é˜µåˆ—ç»“æ„ï¼Œå¹¶é‡‡ç”¨äº†åˆ›æ–°çš„è½¯ä»¶æ˜ å°„æ–¹æ³•å’Œæ•°æ®æµçŸ©é˜µä¹˜æ³•æŠ€æœ¯ã€‚</li>
<li>MatrixFlowé€šè¿‡æé«˜æ•°æ®ååé‡æ¥ä¼˜åŒ–Transformerçš„è®¡ç®—æ€§èƒ½ã€‚</li>
<li>é€šè¿‡gem5çš„å…¨ç³»ç»Ÿä»¿çœŸéªŒè¯ï¼ŒMatrixFlowåœ¨å¤šç§Transformeræ¨¡å‹ä¸Šå®ç°äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡ã€‚</li>
<li>ä¸å¤šæ ¸CPUç³»ç»Ÿç›¸æ¯”ï¼ŒMatrixFlowæœ€é«˜å®ç°äº†22å€çš„é€Ÿåº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05290">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-268a98b21dff8b50a57e7c4f4e533530.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40320b65aab371fe2a415c7045b26038.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b85acab7f94b36df42a5c520f5561881.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79851503774726aa4d628c7df07250fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2a685386b5b73a55924be401f2bbacac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28b027063162b1ab2ec197650bf29605.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aec6c2d113efacfb9036dd0ed20ed0b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-41138e47b662facf648a4155d27b1a89.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="â€œOnly-ChatGPT-gets-meâ€-An-Empirical-Analysis-of-GPT-versus-other-Large-Language-Models-for-Emotion-Detection-in-Text"><a href="#â€œOnly-ChatGPT-gets-meâ€-An-Empirical-Analysis-of-GPT-versus-other-Large-Language-Models-for-Emotion-Detection-in-Text" class="headerlink" title="â€œOnly ChatGPT gets meâ€: An Empirical Analysis of GPT versus other Large   Language Models for Emotion Detection in Text"></a>â€œOnly ChatGPT gets meâ€: An Empirical Analysis of GPT versus other Large   Language Models for Emotion Detection in Text</h2><p><strong>Authors:Florian Lecourt, Madalina Croitoru, Konstantin Todorov</strong></p>
<p>This work investigates the capabilities of large language models (LLMs) in detecting and understanding human emotions through text. Drawing upon emotion models from psychology, we adopt an interdisciplinary perspective that integrates computational and affective sciences insights. The main goal is to assess how accurately they can identify emotions expressed in textual interactions and compare different models on this specific task. This research contributes to broader efforts to enhance human-computer interaction, making artificial intelligence technologies more responsive and sensitive to usersâ€™ emotional nuances. By employing a methodology that involves comparisons with a state-of-the-art model on the GoEmotions dataset, we aim to gauge LLMsâ€™ effectiveness as a system for emotional analysis, paving the way for potential applications in various fields that require a nuanced understanding of human language. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æ–‡æœ¬æ£€æµ‹å’Œäº†è§£äººç±»æƒ…ç»ªçš„èƒ½åŠ›ã€‚æˆ‘ä»¬å€Ÿé‰´å¿ƒç†å­¦çš„æƒ…æ„Ÿæ¨¡å‹ï¼Œé‡‡ç”¨è·¨å­¦ç§‘è§†è§’ï¼Œèåˆè®¡ç®—ç§‘å­¦å’Œæƒ…æ„Ÿç§‘å­¦çš„è§è§£ã€‚ä¸»è¦ç›®æ ‡æ˜¯è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨è¯†åˆ«æ–‡æœ¬äº’åŠ¨ä¸­æ‰€è¡¨è¾¾çš„æƒ…ç»ªæ–¹é¢çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨è¿™ä¸€ç‰¹å®šä»»åŠ¡ä¸Šæ¯”è¾ƒä¸åŒçš„æ¨¡å‹ã€‚è¿™é¡¹ç ”ç©¶ä¸ºæå‡äººæœºäº¤äº’çš„å¹¿æ³›åŠªåŠ›åšå‡ºäº†è´¡çŒ®ï¼Œä½¿äººå·¥æ™ºèƒ½æŠ€æœ¯å¯¹ç”¨æˆ·çš„æƒ…ç»ªç»†å¾®å·®åˆ«æ›´å…·å“åº”æ€§å’Œæ•æ„Ÿæ€§ã€‚é€šè¿‡é‡‡ç”¨ä¸GoEmotionsæ•°æ®é›†ä¸Šçš„æœ€æ–°æ¨¡å‹è¿›è¡Œå¯¹æ¯”çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æ—¨åœ¨è¯„ä¼°LLMä½œä¸ºæƒ…æ„Ÿåˆ†æç³»ç»Ÿçš„æœ‰æ•ˆæ€§ï¼Œä¸ºéœ€è¦åœ¨äººç±»è¯­è¨€æ–¹é¢å…·å¤‡ç»†å¾®ç†è§£èƒ½åŠ›çš„å„ä¸ªé¢†åŸŸçš„åº”ç”¨é“ºå¹³é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04831v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬å·¥ä½œç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ£€æµ‹å’Œç†è§£äººç±»æƒ…ç»ªæ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ç»“åˆå¿ƒç†å­¦ä¸­çš„æƒ…æ„Ÿæ¨¡å‹ï¼Œé‡‡ç”¨è·¨å­¦ç§‘æ–¹æ³•ï¼Œæ•´åˆè®¡ç®—å’Œæƒ…æ„Ÿç§‘å­¦çš„è§è§£ã€‚ä¸»è¦ç›®æ ‡æ˜¯è¯„ä¼°LLMå‡†ç¡®è¯†åˆ«æ–‡æœ¬äº’åŠ¨ä¸­è¡¨è¾¾æƒ…æ„Ÿçš„èƒ½åŠ›ï¼Œå¹¶åœ¨è¿™ä¸€ç‰¹å®šä»»åŠ¡ä¸Šæ¯”è¾ƒä¸åŒçš„æ¨¡å‹ã€‚è¯¥ç ”ç©¶ä¸ºå¢å¼ºäººæœºäº¤äº’çš„å¹¿æ³›åŠªåŠ›åšå‡ºè´¡çŒ®ï¼Œä½¿äººå·¥æ™ºèƒ½æŠ€æœ¯å¯¹ç”¨æˆ·çš„æƒ…ç»ªç»†å¾®å·®åˆ«æ›´åŠ æ•æ„Ÿå’Œå“åº”ã€‚é€šè¿‡é‡‡ç”¨ä¸GoEmotionsæ•°æ®é›†ä¸Šçš„æœ€æ–°æ¨¡å‹è¿›è¡Œå¯¹æ¯”çš„æ–¹æ³•ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°LLMåœ¨æƒ…æ„Ÿåˆ†ææ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸ºå¯èƒ½éœ€è¦æ·±åˆ»ç†è§£äººç±»è¯­è¨€çš„å„ä¸ªé¢†åŸŸçš„åº”ç”¨é“ºå¹³é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ£€æµ‹å’Œç†è§£äººç±»æƒ…ç»ªæ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶ç»“åˆäº†å¿ƒç†å­¦ä¸­çš„æƒ…æ„Ÿæ¨¡å‹ï¼Œå¹¶é‡‡ç”¨äº†è·¨å­¦ç§‘çš„æ–¹æ³•ã€‚</li>
<li>ä¸»è¦ç›®æ ‡æ˜¯è¯„ä¼°LLMåœ¨è¯†åˆ«æ–‡æœ¬ä¸­çš„æƒ…æ„Ÿè¡¨è¾¾æ–¹é¢çš„å‡†ç¡®æ€§ï¼Œå¹¶æ¯”è¾ƒä¸åŒæ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>ç ”ç©¶å¯¹å¢å¼ºäººæœºäº¤äº’çš„è´¡çŒ®ï¼Œä½¿AIæŠ€æœ¯å¯¹ç”¨æˆ·æƒ…ç»ªæ›´æ•æ„Ÿå’Œå“åº”ã€‚</li>
<li>é€šè¿‡ä¸GoEmotionsæ•°æ®é›†ä¸­çš„æœ€æ–°æ¨¡å‹å¯¹æ¯”ï¼Œè¯„ä¼°äº†LLMåœ¨æƒ…æ„Ÿåˆ†ææ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå¯èƒ½éœ€è¦æ·±å…¥ç†è§£äººç±»è¯­è¨€çš„å„ä¸ªé¢†åŸŸï¼ˆå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€ç¤¾äº¤åª’ä½“åˆ†æã€èŠå¤©æœºå™¨äººç­‰ï¼‰æä¾›äº†æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚</li>
<li>æ•´ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æƒ…æ„Ÿåˆ†æé¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•æä¾›äº†æœ‰ä»·å€¼çš„è§è§£å’Œå‚è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04831">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d8f1a1b9a61913572c1576bd2000f388.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92bf9e00cb4c2be0e96791d2c2bac131.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45b7576b660d577c27685d443489fbb6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2d247ee585e8554f1660124dedaee28e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-278566f15dda47a686e92902f41ab67e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0ead619ab42cf8137c88c70431bae36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-75e96f4820cdde9e9c450b0d233056fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d41fb8b54b551bfdb19c0e6aa585ee1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Comparative-Analysis-Based-on-DeepSeek-ChatGPT-and-Google-Gemini-Features-Techniques-Performance-Future-Prospects"><a href="#Comparative-Analysis-Based-on-DeepSeek-ChatGPT-and-Google-Gemini-Features-Techniques-Performance-Future-Prospects" class="headerlink" title="Comparative Analysis Based on DeepSeek, ChatGPT, and Google Gemini:   Features, Techniques, Performance, Future Prospects"></a>Comparative Analysis Based on DeepSeek, ChatGPT, and Google Gemini:   Features, Techniques, Performance, Future Prospects</h2><p><strong>Authors:Anichur Rahman, Shahariar Hossain Mahir, Md Tanjum An Tashrif, Airin Afroj Aishi, Md Ahsan Karim, Dipanjali Kundu, Tanoy Debnath, Md. Abul Ala Moududi, MD. Zunead Abedin Eidmum</strong></p>
<p>Nowadays, DeepSeek, ChatGPT, and Google Gemini are the most trending and exciting Large Language Model (LLM) technologies for reasoning, multimodal capabilities, and general linguistic performance worldwide. DeepSeek employs a Mixture-of-Experts (MoE) approach, activating only the parameters most relevant to the task at hand, which makes it especially effective for domain-specific work. On the other hand, ChatGPT relies on a dense transformer model enhanced through reinforcement learning from human feedback (RLHF), and then Google Gemini actually uses a multimodal transformer architecture that integrates text, code, and images into a single framework. However, by using those technologies, people can be able to mine their desired text, code, images, etc, in a cost-effective and domain-specific inference. People may choose those techniques based on the best performance. In this regard, we offer a comparative study based on the DeepSeek, ChatGPT, and Gemini techniques in this research. Initially, we focus on their methods and materials, appropriately including the data selection criteria. Then, we present state-of-the-art features of DeepSeek, ChatGPT, and Gemini based on their applications. Most importantly, we show the technological comparison among them and also cover the dataset analysis for various applications. Finally, we address extensive research areas and future potential guidance regarding LLM-based AI research for the community. </p>
<blockquote>
<p>å¦‚ä»Šï¼ŒDeepSeekã€ChatGPTå’ŒGoogle Geminiæ˜¯ä¸–ç•Œä¸Šæœ€æµè¡Œã€æœ€ä»¤äººå…´å¥‹çš„ç”¨äºæ¨ç†ã€å¤šæ¨¡æ€èƒ½åŠ›å’Œä¸€èˆ¬è¯­è¨€æ€§èƒ½çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯ã€‚DeepSeeké‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ–¹æ³•ï¼Œåªæ¿€æ´»ä¸æ‰‹å¤´ä»»åŠ¡æœ€ç›¸å…³çš„å‚æ•°ï¼Œä½¿å…¶åœ¨ç‰¹å®šé¢†åŸŸçš„å·¥ä½œå°¤å…¶æœ‰æ•ˆã€‚å¦ä¸€æ–¹é¢ï¼ŒChatGPTä¾èµ–äºé€šè¿‡å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å¢å¼ºçš„å¯†é›†å˜å‹å™¨æ¨¡å‹ï¼Œç„¶åGoogle Geminiå®é™…ä¸Šä½¿ç”¨äº†ä¸€ç§å¤šæ¨¡æ€å˜å‹å™¨æ¶æ„ï¼Œè¯¥æ¶æ„å°†æ–‡æœ¬ã€ä»£ç å’Œå›¾åƒé›†æˆåˆ°ä¸€ä¸ªå•ä¸€æ¡†æ¶ä¸­ã€‚ç„¶è€Œï¼Œé€šè¿‡åˆ©ç”¨è¿™äº›æŠ€æœ¯ï¼Œäººä»¬èƒ½å¤Ÿä»¥æˆæœ¬æ•ˆç›Šå’Œç‰¹å®šé¢†åŸŸçš„æ¨ç†æ¥æŒ–æ˜ä»–ä»¬æ‰€éœ€çš„æ–‡æœ¬ã€ä»£ç ã€å›¾åƒç­‰ã€‚äººä»¬å¯èƒ½ä¼šæ ¹æ®æœ€ä½³æ€§èƒ½é€‰æ‹©è¿™äº›æŠ€æœ¯ã€‚åœ¨è¿™æ–¹é¢ï¼Œæˆ‘ä»¬åœ¨è¿™é¡¹ç ”ç©¶ä¸­åŸºäºDeepSeekã€ChatGPTå’ŒGeminiæŠ€æœ¯æä¾›äº†ä¸€é¡¹æ¯”è¾ƒç ”ç©¶ã€‚æœ€åˆï¼Œæˆ‘ä»¬å…³æ³¨å®ƒä»¬çš„æ–¹æ³•å’Œææ–™ï¼Œé€‚å½“åœ°åŒ…æ‹¬æ•°æ®é€‰æ‹©æ ‡å‡†ã€‚ç„¶åï¼Œæˆ‘ä»¬æ ¹æ®å®ƒä»¬çš„åº”ç”¨å±•ç¤ºDeepSeekã€ChatGPTå’ŒGeminiçš„æœ€æ–°åŠŸèƒ½ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å®ƒä»¬ä¹‹é—´çš„æŠ€æœ¯æ¯”è¾ƒï¼Œå¹¶æ¶µç›–äº†å„ç§åº”ç”¨çš„æ•°æ®é›†åˆ†æã€‚æœ€åï¼Œæˆ‘ä»¬é’ˆå¯¹ç¤¾åŒºæå‡ºäº†å…³äºåŸºäºLLMçš„äººå·¥æ™ºèƒ½ç ”ç©¶çš„å¹¿æ³›ç ”ç©¶é¢†åŸŸå’Œæœªæ¥æ½œåœ¨æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04783v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦æ¢ç´¢ï¼ˆDeepSeekï¼‰ã€ChatGPTå’Œè°·æ­ŒåŒå­åº§ï¼ˆGoogle Geminiï¼‰æ˜¯å½“å‰æœ€æµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯ï¼Œå®ƒä»¬åˆ†åˆ«é‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ–¹æ³•ã€å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å’Œå¤šæ¨¡æ€è½¬æ¢å™¨æ¶æ„ï¼Œç”¨äºæ¨ç†ã€å¤šæ¨¡æ€èƒ½åŠ›å’Œä¸€èˆ¬è¯­è¨€æ€§èƒ½ã€‚è¿™äº›æŠ€æœ¯ä½¿äººä»¬èƒ½å¤Ÿä»¥æˆæœ¬æ•ˆç›Šå’Œç‰¹å®šé¢†åŸŸçš„æ–¹å¼è¿›è¡Œæ–‡æœ¬ã€ä»£ç ã€å›¾åƒç­‰çš„æŒ–æ˜ã€‚æœ¬æ–‡å¯¹è¿™äº›æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒç ”ç©¶ï¼Œé‡ç‚¹ä»‹ç»äº†å®ƒä»¬çš„æ–¹æ³•ã€ææ–™å’Œæ•°æ®é€‰æ‹©æ ‡å‡†ï¼Œä»¥åŠå®ƒä»¬åœ¨åº”ç”¨ç¨‹åºæ–¹é¢çš„æœ€æ–°ç‰¹æ€§ã€‚åŒæ—¶ï¼Œå¯¹ä¸‰è€…è¿›è¡Œäº†æŠ€æœ¯æ¯”è¾ƒï¼Œå¹¶å¯¹å„ç§åº”ç”¨çš„æ•°æ®é›†è¿›è¡Œäº†åˆ†æã€‚æœ€åï¼Œæ¢è®¨äº†å¹¿æ³›çš„ç ”ç©¶é¢†åŸŸå’Œæœªæ¥å…³äºLLMåŸºäºAIç ”ç©¶çš„æ½œåœ¨æŒ‡å¯¼æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeekã€ChatGPTå’ŒGoogle Geminiæ˜¯å½“å‰æœ€æµè¡Œçš„LLMæŠ€æœ¯ã€‚</li>
<li>DeepSeeké‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ–¹æ³•ï¼Œé€‚ç”¨äºç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ã€‚</li>
<li>ChatGPTä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰è¿›è¡Œå¢å¼ºã€‚</li>
<li>Google Geminiä½¿ç”¨å¤šæ¨¡æ€è½¬æ¢å™¨æ¶æ„ï¼Œæ•´åˆæ–‡æœ¬ã€ä»£ç å’Œå›¾åƒã€‚</li>
<li>è¿™äº›æŠ€æœ¯ä½¿æ–‡æœ¬ã€ä»£ç ã€å›¾åƒç­‰çš„æŒ–æ˜å˜å¾—æˆæœ¬æ•ˆç›Šé«˜ä¸”ç‰¹å®šé¢†åŸŸåŒ–ã€‚</li>
<li>æ–‡ç« å¯¹è¿™äº›æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒï¼ŒåŒ…æ‹¬æ–¹æ³•ã€ææ–™ã€æ•°æ®é€‰æ‹©æ ‡å‡†å’Œåº”ç”¨ç¨‹åºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-33e2fb8acf33a58345e831f7542ecab3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-350ab728234c22b4fd154c528140888c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8083d3db6885b5c85ab0a40015166c88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ce877eaae75452f53bacda2708200a1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Universality-of-Layer-Level-Entropy-Weighted-Quantization-Beyond-Model-Architecture-and-Size"><a href="#Universality-of-Layer-Level-Entropy-Weighted-Quantization-Beyond-Model-Architecture-and-Size" class="headerlink" title="Universality of Layer-Level Entropy-Weighted Quantization Beyond Model   Architecture and Size"></a>Universality of Layer-Level Entropy-Weighted Quantization Beyond Model   Architecture and Size</h2><p><strong>Authors:Alireza Behtash, Marijan Fofonjka, Ethan Baird, Tyler Mauer, Hossein Moghimifam, David Stout, Joel Dennison</strong></p>
<p>We present a novel approach to selective model quantization that transcends the limitations of architecture-specific and size-dependent compression methods for Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ). By analyzing the entropy distribution across transformer blocks, EWQ determines which blocks can be safely quantized without causing significant performance degradation, independent of model architecture or size. Our method outperforms uniform quantization approaches, maintaining Massive Multitask Language Understanding (MMLU) accuracy scores within 0.5% of unquantized models while reducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ across multiple architectures â€“ from 1.6B to 70B parameters â€“ and showcase consistent improvements in the quality-compression trade-off regardless of model scale or architectural design. A surprising finding of EWQ is its ability to reduce perplexity compared to unquantized models, suggesting the presence of beneficial regularization through selective precision reduction. This improvement holds across different model families, indicating a fundamental relationship between layer-level entropy and optimal precision requirements. Additionally, we introduce FastEWQ, a rapid method for entropy distribution analysis that eliminates the need for loading model weights. This technique leverages universal characteristics of entropy distribution that persist across various architectures and scales, enabling near-instantaneous quantization decisions while maintaining 80% classification accuracy with full entropy analysis. Our results demonstrate that effective quantization strategies can be developed independently of specific architectural choices or model sizes, opening new possibilities for efficient LLM deployment. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é€‰æ‹©æ€§æ¨¡å‹é‡åŒ–æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨ç†µåŠ æƒé‡åŒ–ï¼ˆEWQï¼‰è¶…è¶Šäº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç‰¹å®šæ¶æ„å’Œå¤§å°ä¾èµ–çš„å‹ç¼©æ–¹æ³•çš„å±€é™æ€§ã€‚é€šè¿‡åˆ†æå˜å‹å™¨æ¨¡å—ä¹‹é—´çš„ç†µåˆ†å¸ƒï¼ŒEWQèƒ½å¤Ÿç¡®å®šå“ªäº›æ¨¡å—å¯ä»¥å®‰å…¨é‡åŒ–ï¼Œè€Œä¸ä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè¿™ç‹¬ç«‹äºæ¨¡å‹æ¶æ„æˆ–å¤§å°ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒå¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼ˆMMLUï¼‰å‡†ç¡®åº¦å¾—åˆ†ä¸æœªé‡åŒ–æ¨¡å‹ç›¸å·®ä¸åˆ°0.5%çš„åŒæ—¶ï¼Œå‡å°‘äº†é«˜è¾¾18%çš„å†…å­˜ä½¿ç”¨ï¼Œä¼˜äºå‡åŒ€é‡åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬è¯æ˜äº†EWQåœ¨å¤šä¸ªæ¶æ„ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå‚æ•°ä»1.6Båˆ°70Bä¸ç­‰ï¼Œå¹¶ä¸”åœ¨æ¨¡å‹è§„æ¨¡æˆ–æ¶æ„è®¾è®¡ä¸Šå®ç°äº†æŒç»­çš„æ”¹è¿›ï¼Œåœ¨è´¨é‡å‹ç¼©æƒè¡¡æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚EWQçš„ä¸€ä¸ªæ„å¤–å‘ç°æ˜¯å®ƒèƒ½å¤Ÿå‡å°‘å›°æƒ‘åº¦ä¸æœªé‡åŒ–æ¨¡å‹ç›¸æ¯”ï¼Œè¿™è¡¨æ˜é€šè¿‡é€‰æ‹©æ€§ç²¾åº¦é™ä½å­˜åœ¨æœ‰ç›Šçš„æ­£åˆ™åŒ–ã€‚è¿™ç§æ”¹è¿›åœ¨ä¸åŒæ¨¡å‹å®¶æ—ä¸­æ™®éå­˜åœ¨ï¼Œè¡¨æ˜å±‚çº§ç†µå’Œæœ€ä½³ç²¾åº¦è¦æ±‚ä¹‹é—´å­˜åœ¨æ ¹æœ¬å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†FastEWQï¼Œè¿™æ˜¯ä¸€ç§å¿«é€Ÿåˆ†æç†µåˆ†å¸ƒçš„æ–¹æ³•ï¼Œæ— éœ€åŠ è½½æ¨¡å‹æƒé‡ã€‚è¯¥æŠ€æœ¯åˆ©ç”¨ç†µåˆ†å¸ƒçš„é€šç”¨ç‰¹æ€§ï¼Œè¿™äº›ç‰¹æ€§åœ¨å„ç§æ¶æ„å’Œè§„æ¨¡ä¸ŠæŒç»­å­˜åœ¨ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒ80%åˆ†ç±»å‡†ç¡®åº¦çš„åŒæ—¶ï¼Œè¿›è¡Œè¿‘ä¹å³æ—¶çš„é‡åŒ–å†³ç­–ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæœ‰æ•ˆçš„é‡åŒ–ç­–ç•¥å¯ä»¥ç‹¬ç«‹äºç‰¹å®šçš„æ¶æ„é€‰æ‹©æˆ–æ¨¡å‹å¤§å°æ¥å¼€å‘ï¼Œä¸ºé«˜æ•ˆéƒ¨ç½²LLMå¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04704v2">PDF</a> 29 pages, 7 figures, 14 tables; Fixed some types, added some   clarifications and improvements</p>
<p><strong>Summary</strong><br>     æå‡ºä¸€ç§åˆ©ç”¨ç†µåŠ æƒé‡åŒ–ï¼ˆEWQï¼‰çš„LLMé€‰æ‹©æ€§æ¨¡å‹é‡åŒ–æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•çªç ´äº†æ¶æ„ç‰¹å®šå’Œå¤§å°ä¾èµ–çš„å‹ç¼©æ–¹æ³•çš„å±€é™ã€‚é€šè¿‡åˆ†æå˜å‹å™¨å—çš„ç†µåˆ†å¸ƒï¼ŒEWQå¯ç¡®å®šå“ªäº›å—å¯åœ¨ä¸å½±å“æ€§èƒ½çš„æƒ…å†µä¸‹å®‰å…¨é‡åŒ–ï¼Œç‹¬ç«‹äºæ¨¡å‹æ¶æ„å’Œå¤§å°ã€‚è¯¥æ–¹æ³•ä¼˜äºå‡åŒ€é‡åŒ–æ–¹æ³•ï¼Œä¿æŒå¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£å‡†ç¡®æ€§ï¼Œåœ¨ä¸è¶…è¿‡æœªé‡åŒ–æ¨¡å‹0.5%è¯¯å·®çš„åŒæ—¶å‡å°‘å†…å­˜ä½¿ç”¨é«˜è¾¾18%ã€‚æ­¤æ–¹æ³•é€‚ç”¨äºå¤šç§æ¶æ„ï¼Œåœ¨ä¸åŒè§„æ¨¡æˆ–è®¾è®¡çš„æ¨¡å‹ä¸­å®ç°ä¸€è‡´çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒEWQé€šè¿‡é€‰æ‹©æ€§ç²¾åº¦é™ä½å±•ç°å‡ºæœ‰ç›Šçš„æ­£è§„åŒ–æ•ˆæœï¼Œå¹¶é™ä½å›°æƒ‘åº¦ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†FastEWQæŠ€æœ¯ï¼Œå¯ä»¥å¿«é€Ÿåˆ†æç†µåˆ†å¸ƒï¼Œæ— éœ€åŠ è½½æ¨¡å‹æƒé‡ã€‚è¿™è¡¨æ˜ï¼Œç‹¬ç«‹äºç‰¹å®šæ¶æ„é€‰æ‹©å’Œæ¨¡å‹å¤§å°çš„æœ‰æ•ˆé‡åŒ–ç­–ç•¥æ˜¯å¯è¡Œçš„ï¼Œä¸ºé«˜æ•ˆéƒ¨ç½²LLMæ‰“å¼€äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åä¸ºEWQçš„æ–°å‹LLMæ¨¡å‹é‡åŒ–æ–¹æ³•ï¼Œèƒ½é’ˆå¯¹ä¸åŒçš„æ¨¡å‹å’Œæ¶æ„è¿›è¡Œé€‰æ‹©æ€§é‡åŒ–ã€‚</li>
<li>EWQåŸºäºç†µåˆ†å¸ƒçš„åˆ†æç¡®å®šå¯é‡åŒ–çš„å˜å‹å™¨å—ï¼Œé¿å…å› é‡åŒ–å¯¼è‡´çš„æ€§èƒ½ä¸‹é™ã€‚</li>
<li>EWQç›¸è¾ƒäºå‡åŒ€é‡åŒ–æ–¹æ³•ï¼Œèƒ½ç»´æŒè¾ƒé«˜çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½å†…å­˜ä½¿ç”¨ã€‚</li>
<li>EWQåœ¨å¤šä¸ªä¸åŒè§„æ¨¡å’Œè®¾è®¡çš„æ¨¡å‹ä¸­è¡¨ç°ä¸€è‡´ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>EWQåœ¨é€‰æ‹©æ€§é™ä½ç²¾åº¦çš„æƒ…å†µä¸‹å±•ç°å‡ºæ­£åˆ™åŒ–æ•ˆæœï¼Œç›¸è¾ƒäºæœªé‡åŒ–çš„æ¨¡å‹é™ä½äº†å›°æƒ‘åº¦ã€‚</li>
<li>ä»‹ç»äº†FastEWQæŠ€æœ¯ï¼Œèƒ½å¿«é€Ÿåˆ†æç†µåˆ†å¸ƒè€Œæ— éœ€åŠ è½½æ¨¡å‹æƒé‡ï¼Œæé«˜äº†é‡åŒ–å†³ç­–çš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04704">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ddd04d641a1b3b1d8f29cee6b160cff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6492f6d39d4c08c3948fc8505e6d028b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bc8e6214e3baab03dda9edb9d9c1712.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LiGT-Layout-infused-Generative-Transformer-for-Visual-Question-Answering-on-Vietnamese-Receipts"><a href="#LiGT-Layout-infused-Generative-Transformer-for-Visual-Question-Answering-on-Vietnamese-Receipts" class="headerlink" title="LiGT: Layout-infused Generative Transformer for Visual Question   Answering on Vietnamese Receipts"></a>LiGT: Layout-infused Generative Transformer for Visual Question   Answering on Vietnamese Receipts</h2><p><strong>Authors:Thanh-Phong Le, Trung Le Chi Phan, Nghia Hieu Nguyen, Kiet Van Nguyen</strong></p>
<p>Document Visual Question Answering (Document VQA) challenges multimodal systems to holistically handle textual, layout, and visual modalities to provide appropriate answers. Document VQA has gained popularity in recent years due to the increasing amount of documents and the high demand for digitization. Nonetheless, most of document VQA datasets are developed in high-resource languages such as English. In this paper, we present ReceiptVQA (\textbf{Receipt} \textbf{V}isual \textbf{Q}uestion \textbf{A}nswering), the initial large-scale document VQA dataset in Vietnamese dedicated to receipts, a document kind with high commercial potentials. The dataset encompasses \textbf{9,000+} receipt images and \textbf{60,000+} manually annotated question-answer pairs. In addition to our study, we introduce LiGT (\textbf{L}ayout-\textbf{i}nfused \textbf{G}enerative \textbf{T}ransformer), a layout-aware encoder-decoder architecture designed to leverage embedding layers of language models to operate layout embeddings, minimizing the use of additional neural modules. Experiments on ReceiptVQA show that our architecture yielded promising performance, achieving competitive results compared with outstanding baselines. Furthermore, throughout analyzing experimental results, we found evident patterns that employing encoder-only model architectures has considerable disadvantages in comparison to architectures that can generate answers. We also observed that it is necessary to combine multiple modalities to tackle our dataset, despite the critical role of semantic understanding from language models. We hope that our work will encourage and facilitate future development in Vietnamese document VQA, contributing to a diverse multimodal research community in the Vietnamese language. </p>
<blockquote>
<p>æ–‡æ¡£è§†è§‰é—®ç­”ï¼ˆDocument VQAï¼‰æŒ‘æˆ˜å¤šæ¨¡æ€ç³»ç»Ÿï¼Œä»¥å…¨é¢å¤„ç†æ–‡æœ¬ã€å¸ƒå±€å’Œè§†è§‰æ¨¡å¼ï¼Œä»¥æä¾›é€‚å½“çš„ç­”æ¡ˆã€‚è¿‘å¹´æ¥ï¼Œç”±äºæ–‡æ¡£æ•°é‡çš„ä¸æ–­å¢åŠ å’Œæ•°å­—åŒ–éœ€æ±‚çš„ä¸æ–­å¢é•¿ï¼Œæ–‡æ¡£VQAçš„å—æ¬¢è¿ç¨‹åº¦æ—¥ç›Šæé«˜ã€‚ç„¶è€Œï¼Œå¤§éƒ¨åˆ†çš„æ–‡æ¡£VQAæ•°æ®é›†éƒ½æ˜¯ç”¨èµ„æºä¸°å¯Œçš„è¯­è¨€ï¼ˆå¦‚è‹±è¯­ï¼‰å¼€å‘çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ReceiptVQAï¼ˆæ”¶æ®è§†è§‰é—®ç­”ï¼‰ï¼Œè¿™æ˜¯è¶Šå—è¯­ä¸­é’ˆå¯¹æ”¶æ®çš„é¦–ä¸ªå¤§è§„æ¨¡æ–‡æ¡£VQAæ•°æ®é›†ï¼Œæ”¶æ®æ˜¯ä¸€ç§å…·æœ‰æé«˜å•†ä¸šæ½œåŠ›çš„æ–‡æ¡£ç±»å‹ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡9000å¼ æ”¶æ®å›¾åƒå’Œè¶…è¿‡6ä¸‡ä¸ªæ‰‹åŠ¨æ ‡æ³¨çš„é—®é¢˜ç­”æ¡ˆå¯¹ã€‚é™¤äº†æˆ‘ä»¬çš„ç ”ç©¶ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†LiGTï¼ˆå¸ƒå±€èå…¥ç”ŸæˆTransformerï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¸ƒå±€æ„ŸçŸ¥çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œæ—¨åœ¨åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„åµŒå…¥å±‚æ¥æ“ä½œå¸ƒå±€åµŒå…¥ï¼Œå°½é‡å‡å°‘ä½¿ç”¨é¢å¤–çš„ç¥ç»ç½‘ç»œæ¨¡å—ã€‚åœ¨ReceiptVQAä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¶æ„å–å¾—äº†æœ‰å¸Œæœ›çš„æ€§èƒ½ï¼Œä¸ä¼˜ç§€çš„åŸºçº¿ç›¸æ¯”å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ†æå®éªŒç»“æœï¼Œæˆ‘ä»¬å‘ç°ä¸èƒ½å¤Ÿç”Ÿæˆç­”æ¡ˆçš„æ¶æ„ç›¸æ¯”ï¼Œä½¿ç”¨ä»…ç¼–ç å™¨æ¨¡å‹æ¶æ„å…·æœ‰æ˜æ˜¾çš„åŠ£åŠ¿ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ï¼Œå°½ç®¡è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ç†è§£èµ·ç€å…³é”®ä½œç”¨ï¼Œä½†ç»“åˆå¤šç§æ¨¡å¼æ¥è§£å†³æˆ‘ä»¬çš„æ•°æ®é›†æ˜¯å¿…è¦çš„ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œèƒ½é¼“åŠ±å’Œä¿ƒè¿›è¶Šå—è¯­æ–‡æ¡£VQAçš„æœªæ¥å‘å±•ï¼Œä¸ºè¶Šå—è¯­çš„å¤šæ¨¡æ€ç ”ç©¶ç¤¾åŒºåšå‡ºè´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19202v2">PDF</a> Accepted at IJDAR</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Document Visual Question Answeringï¼ˆDocument VQAï¼‰çš„æŒ‘æˆ˜ï¼Œå¹¶é’ˆå¯¹æ–‡æœ¬ã€å¸ƒå±€å’Œè§†è§‰æ¨¡æ€çš„ç»¼åˆå¤„ç†è¦æ±‚æä¾›ç­”æ¡ˆã€‚æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°çš„è¶Šå—è¯­æ”¶æ®æ–‡æ¡£VQAæ•°æ®é›†ReceiptVQAï¼ŒåŒ…å«è¶…è¿‡9,000å¼ æ”¶æ®å›¾åƒå’Œè¶…è¿‡6ä¸‡ç»„æ‰‹åŠ¨æ ‡æ³¨çš„é—®é¢˜ç­”æ¡ˆå¯¹ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†LiGTæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§å¸ƒå±€æ„ŸçŸ¥çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œæ—¨åœ¨åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„åµŒå…¥å±‚è¿›è¡Œå¸ƒå±€åµŒå…¥ï¼Œå‡å°‘ä½¿ç”¨é¢å¤–çš„ç¥ç»ç½‘ç»œæ¨¡å—ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¶æ„åœ¨ReceiptVQAä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œå¹¶ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚åˆ†æå®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸åªèƒ½ç¼–ç çš„æ¨¡å‹æ¶æ„ç›¸æ¯”ï¼Œèƒ½å¤Ÿç”Ÿæˆç­”æ¡ˆçš„æ¶æ„å…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚åŒæ—¶è§‚å¯Ÿåˆ°ï¼Œå°½ç®¡è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ç†è§£èµ·ç€å…³é”®ä½œç”¨ï¼Œä½†ç»“åˆå¤šç§æ¨¡æ€å¯¹äºè§£å†³æ•°æ®é›†çš„é—®é¢˜ä»æ˜¯å¿…è¦çš„ã€‚å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½æ¨åŠ¨è¶Šå—è¯­æ–‡æ¡£VQAçš„å‘å±•ï¼Œå¹¶ä¸ºè¶Šå—è¯­çš„å¤šæ¨¡æ€ç ”ç©¶ç¤¾åŒºåšå‡ºè´¡çŒ®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Document VQAé¢ä¸´å¤„ç†æ–‡æœ¬ã€å¸ƒå±€å’Œè§†è§‰æ¨¡æ€çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è¶Šå—è¯­æ”¶æ®æ–‡æ¡£VQAæ•°æ®é›†ReceiptVQAã€‚</li>
<li>å¼•å…¥äº†LiGTæ¨¡å‹ï¼Œä¸€ç§å¸ƒå±€æ„ŸçŸ¥çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚</li>
<li>LiGTæ¨¡å‹åœ¨ReceiptVQAæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>å®éªŒåˆ†ææ˜¾ç¤ºï¼Œç”Ÿæˆç­”æ¡ˆçš„æ¨¡å‹æ¶æ„æ¯”ä»…ç¼–ç çš„æ¨¡å‹æ¶æ„å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>ç»“åˆå¤šç§æ¨¡æ€å¯¹äºè§£å†³æ•°æ®é›†çš„é—®é¢˜æ˜¯å¿…è¦çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19202">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8b7aaf3ce5f4d096635b436189db57f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-47b89354738d77b07706da892591f68d.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Exoplanet-Transit-Candidate-Identification-in-TESS-Full-Frame-Images-via-a-Transformer-Based-Algorithm"><a href="#Exoplanet-Transit-Candidate-Identification-in-TESS-Full-Frame-Images-via-a-Transformer-Based-Algorithm" class="headerlink" title="Exoplanet Transit Candidate Identification in TESS Full-Frame Images via   a Transformer-Based Algorithm"></a>Exoplanet Transit Candidate Identification in TESS Full-Frame Images via   a Transformer-Based Algorithm</h2><p><strong>Authors:Helem Salinas, Rafael Brahm, Greg Olmschenk, Richard K. Barry, Karim Pichara, Stela Ishitani Silva, Vladimir Araujo</strong></p>
<p>The Transiting Exoplanet Survey Satellite (TESS) is surveying a large fraction of the sky, generating a vast database of photometric time series data that requires thorough analysis to identify exoplanetary transit signals. Automated learning approaches have been successfully applied to identify transit signals. However, most existing methods focus on the classification and validation of candidates, while few efforts have explored new techniques for the search of candidates. To search for new exoplanet transit candidates, we propose an approach to identify exoplanet transit signals without the need for phase folding or assuming periodicity in the transit signals, such as those observed in multi-transit light curves. To achieve this, we implement a new neural network inspired by Transformers to directly process Full Frame Image (FFI) light curves to detect exoplanet transits. Transformers, originally developed for natural language processing, have recently demonstrated significant success in capturing long-range dependencies compared to previous approaches focused on sequential data. This ability allows us to employ multi-head self-attention to identify exoplanet transit signals directly from the complete light curves, combined with background and centroid time series, without requiring prior transit parameters. The network is trained to learn characteristics of the transit signal, like the dip shape, which helps distinguish planetary transits from other variability sources. Our model successfully identified 214 new planetary system candidates, including 122 multi-transit light curves, 88 single-transit and 4 multi-planet systems from TESS sectors 1-26 with a radius &gt; 0.27 $R_{\mathrm{Jupiter}}$, demonstrating its ability to detect transits regardless of their periodicity. </p>
<blockquote>
<p>å‡Œæ—¥ç³»å¤–è¡Œæ˜Ÿå‹˜æµ‹å«æ˜Ÿï¼ˆTESSï¼‰æ­£åœ¨å¯¹å¤©ç©ºè¿›è¡Œå¤§è§„æ¨¡å‹˜æµ‹ï¼Œç”Ÿæˆå¤§é‡å…‰åº¦æ—¶é—´åºåˆ—æ•°æ®ï¼Œéœ€è¦è¿›è¡Œå½»åº•åˆ†æä»¥è¯†åˆ«å‡Œæ—¥ç³»å¤–è¡Œæ˜Ÿçš„ä¿¡å·ã€‚è‡ªåŠ¨åŒ–å­¦ä¹ æ–¹æ³•å·²æˆåŠŸåº”ç”¨äºè¯†åˆ«å‡Œæ—¥ä¿¡å·ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•éƒ½é›†ä¸­åœ¨å€™é€‰å¯¹è±¡çš„åˆ†ç±»å’ŒéªŒè¯ä¸Šï¼Œè€Œå¯¹å¯»æ‰¾æ–°å€™é€‰å¯¹è±¡çš„æ–°æŠ€æœ¯æ¢ç´¢å´å¾ˆå°‘ã€‚ä¸ºäº†å¯»æ‰¾æ–°çš„å‡Œæ—¥ç³»å¤–è¡Œæ˜Ÿå€™é€‰å¯¹è±¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€ç›¸ä½æŠ˜å æˆ–å‡è®¾å‡Œæ—¥ä¿¡å·å‘¨æœŸæ€§çš„æ–¹æ³•ï¼Œä¾‹å¦‚å¤šå‡Œæ—¥å…‰åº¦æ›²çº¿æ‰€è§‚å¯Ÿåˆ°çš„é‚£æ ·ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å—åˆ°Transformerå¯å‘çš„ç¥ç»ç½‘ç»œï¼Œç›´æ¥å¤„ç†å…¨å¸§å›¾åƒï¼ˆFFIï¼‰å…‰åº¦æ›²çº¿æ¥æ£€æµ‹å‡Œæ—¥ç³»å¤–è¡Œæ˜Ÿã€‚Transformeræœ€åˆæ˜¯ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†è€Œå¼€å‘çš„ï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œä¸ä»¥å‰ä¸“æ³¨äºåºåˆ—æ•°æ®çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒåœ¨æ•è·é•¿æœŸä¾èµ–å…³ç³»æ–¹é¢å–å¾—äº†é‡å¤§æˆåŠŸã€‚è¿™ç§èƒ½åŠ›ä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›ä»å®Œæ•´çš„å…‰åº¦æ›²çº¿ä¸­ç›´æ¥è¯†åˆ«å‡Œæ—¥ç³»å¤–è¡Œæ˜Ÿçš„ä¿¡å·ï¼Œç»“åˆèƒŒæ™¯å’Œè´¨å¿ƒæ—¶é—´åºåˆ—ï¼Œæ— éœ€äº‹å…ˆçš„å‡Œæ—¥å‚æ•°ã€‚ç½‘ç»œç»è¿‡è®­ç»ƒï¼Œå­¦ä¹ å‡Œæ—¥ä¿¡å·çš„ç‰¹å¾ï¼Œå¦‚æš—å½¢ï¼Œè¿™æœ‰åŠ©äºåŒºåˆ†è¡Œæ˜Ÿå‡Œæ—¥å’Œå…¶ä»–å˜æºã€‚æˆ‘ä»¬çš„æ¨¡å‹æˆåŠŸè¯†åˆ«äº†214ä¸ªæ–°çš„è¡Œæ˜Ÿç³»ç»Ÿå€™é€‰å¯¹è±¡ï¼ŒåŒ…æ‹¬122ä¸ªå¤šå‡Œæ—¥å…‰åº¦æ›²çº¿ã€88ä¸ªå•å‡Œæ—¥å’Œæ¥è‡ªTESS 1-26æ‰‡åŒºçš„4ä¸ªå¤šè¡Œæ˜Ÿç³»ç»Ÿï¼Œå…¶åŠå¾„å¤§äº0.27ä¸ªæœ¨æ˜ŸåŠå¾„ï¼Œè¯æ˜äº†å…¶æ£€æµ‹å‡Œæ—¥çš„èƒ½åŠ›ï¼Œæ— è®ºå…¶å‘¨æœŸæ€§å¦‚ä½•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.07542v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†åˆ©ç”¨Transformerç¥ç»ç½‘ç»œç›´æ¥å¤„ç†å…¨å¸§å›¾åƒï¼ˆFFIï¼‰å…‰å˜æ›²çº¿æ¥æ£€æµ‹è¡Œæ˜Ÿè¿‡å¢ƒçš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ— éœ€ç›¸ä½æŠ˜å æˆ–å‡è®¾è¿‡å¢ƒä¿¡å·çš„å‘¨æœŸæ€§ï¼Œå¯ç›´æ¥ä»å®Œæ•´çš„å…‰å˜æ›²çº¿ä¸­è¯†åˆ«å‡ºè¡Œæ˜Ÿè¿‡å¢ƒä¿¡å·ã€‚æ­¤æ–¹æ³•å·²æˆåŠŸè¯†åˆ«å‡ºTESSæ¢æµ‹å™¨è§‚æµ‹åˆ°çš„214ä¸ªæ–°çš„è¡Œæ˜Ÿç³»ç»Ÿå€™é€‰ä½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TESSæ­£åœ¨å¯¹å¤©ç©ºè¿›è¡Œå¤§è§„æ¨¡è§‚æµ‹ï¼Œç”Ÿæˆäº†å¤§é‡çš„å…‰å˜æ›²çº¿æ•°æ®ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å€™é€‰è€…çš„åˆ†ç±»å’ŒéªŒè¯ä¸Šï¼Œå¯¹æ–°å€™é€‰è€…çš„æœç´¢æ–¹æ³•æ¢ç´¢è¾ƒå°‘ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºTransformerç¥ç»ç½‘ç»œçš„æ–¹æ³•ï¼Œç”¨äºç›´æ¥å¤„ç†å…¨å¸§å›¾åƒï¼ˆFFIï¼‰å…‰å˜æ›²çº¿æ¥æ£€æµ‹è¡Œæ˜Ÿè¿‡å¢ƒä¿¡å·ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€å‡è®¾è¿‡å¢ƒä¿¡å·çš„å‘¨æœŸæ€§ï¼Œå¯ç›´æ¥ä»å®Œæ•´çš„å…‰å˜æ›²çº¿ä¸­è¯†åˆ«ä¿¡å·ã€‚</li>
<li>Transformerç½‘ç»œå…·æœ‰æ•æ‰é•¿æœŸä¾èµ–å…³ç³»çš„èƒ½åŠ›ï¼Œæœ‰åŠ©äºè¯†åˆ«è¡Œæ˜Ÿè¿‡å¢ƒä¿¡å·ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å­¦ä¹ å’Œè¯†åˆ«è¿‡å¢ƒä¿¡å·çš„ç‰¹å¾ï¼Œå¦‚æš—æ–‘å½¢çŠ¶ï¼Œæ¥åŒºåˆ†è¡Œæ˜Ÿè¿‡å¢ƒå’Œå…¶ä»–å˜åŒ–æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.07542">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69f0e2833271f1b8b6a66cdc6c5f94a9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-751a0f349e754c4a8b2280ad20a413b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74c6938214d33c1513b62e4ae30a8d27.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Bias-Unveiled-Investigating-Social-Bias-in-LLM-Generated-Code"><a href="#Bias-Unveiled-Investigating-Social-Bias-in-LLM-Generated-Code" class="headerlink" title="Bias Unveiled: Investigating Social Bias in LLM-Generated Code"></a>Bias Unveiled: Investigating Social Bias in LLM-Generated Code</h2><p><strong>Authors:Lin Ling, Fazle Rabbi, Song Wang, Jinqiu Yang</strong></p>
<p>Large language models (LLMs) have significantly advanced the field of automated code generation. However, a notable research gap exists in evaluating social biases that may be present in the code produced by LLMs. To solve this issue, we propose a novel fairness framework, i.e., Solar, to assess and mitigate the social biases of LLM-generated code. Specifically, Solar can automatically generate test cases for quantitatively uncovering social biases of the auto-generated code by LLMs. To quantify the severity of social biases in generated code, we develop a dataset that covers a diverse set of social problems. We applied Solar and the crafted dataset to four state-of-the-art LLMs for code generation. Our evaluation reveals severe bias in the LLM-generated code from all the subject LLMs. Furthermore, we explore several prompting strategies for mitigating bias, including Chain-of-Thought (CoT) prompting, combining positive role-playing with CoT prompting and dialogue with Solar. Our experiments show that dialogue with Solar can effectively reduce social bias in LLM-generated code by up to 90%. Last, we make the code and data publicly available is highly extensible to evaluate new social problems. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¯„ä¼°LLMç”Ÿæˆçš„ä»£ç ä¸­å¯èƒ½å­˜åœ¨çš„ç¤¾ä¼šåè§ä»å­˜åœ¨æ˜¾è‘—çš„ç ”ç©¶ç©ºç™½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¬å¹³æ€§çš„æ–°æ¡†æ¶ï¼Œå³Solarï¼Œæ¥è¯„ä¼°å’Œç¼“è§£LLMç”Ÿæˆä»£ç çš„ç¤¾ä¼šåè§ã€‚å…·ä½“æ¥è¯´ï¼ŒSolarèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œå®šé‡å‘ç°LLMç”Ÿæˆçš„ä»£ç ä¸­çš„ç¤¾ä¼šåè§ã€‚ä¸ºäº†é‡åŒ–ç”Ÿæˆä»£ç ä¸­ç¤¾ä¼šåè§çš„ä¸¥é‡ç¨‹åº¦ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ¶µç›–å¤šç§ç¤¾ä¼šé—®é¢˜çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å°†Solarå’Œå®šåˆ¶æ•°æ®é›†åº”ç”¨äºå››ä¸ªæœ€å…ˆè¿›çš„ä»£ç ç”ŸæˆLLMã€‚æˆ‘ä»¬çš„è¯„ä¼°å‘ç°æ‰€æœ‰æµ‹è¯•LLMç”Ÿæˆçš„ä»£ç éƒ½å­˜åœ¨ä¸¥é‡åè§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å‡ ç§å‡è½»åè§çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºã€å°†ç§¯æè§’è‰²æ‰®æ¼”ä¸CoTæç¤ºå’Œä¸Solarå¯¹è¯ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸Solarçš„å¯¹è¯å¯ä»¥æœ‰æ•ˆåœ°å‡å°‘LLMç”Ÿæˆä»£ç ä¸­çš„ç¤¾ä¼šåè§é«˜è¾¾90%ã€‚æœ€åï¼Œæˆ‘ä»¬å…¬å¼€äº†ä»£ç å’Œæ•°æ®é›†ï¼Œè¿™å¯¹äºè¯„ä¼°æ–°çš„ç¤¾ä¼šé—®é¢˜å…·æœ‰å¾ˆå¼ºçš„å¯æ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10351v4">PDF</a> accepted for publication in the Association for the Advancement of   Artificial Intelligence (AAAI), 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¯„ä¼°LLMç”Ÿæˆä»£ç ä¸­å¯èƒ½å­˜åœ¨çš„ç¤¾ä¼šåè§çš„ç ”ç©¶ç©ºç™½ä»ç„¶æ˜¾è‘—ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºSolarçš„æ–°å‹å…¬å¹³æ€§æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å’Œç¼“è§£LLMç”Ÿæˆä»£ç ä¸­çš„ç¤¾ä¼šåè§ã€‚Solarèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ï¼Œå®šé‡å‘ç°LLMç”Ÿæˆä»£ç ä¸­çš„ç¤¾ä¼šåè§ã€‚ä¸ºé‡åŒ–ç”Ÿæˆä»£ç ä¸­ç¤¾ä¼šåè§çš„ä¸¥é‡ç¨‹åº¦ï¼Œæœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªæ¶µç›–å¤šç§ç¤¾ä¼šé—®é¢˜çš„æ•°æ®é›†ã€‚å°†Solarå’Œæ•°æ®é›†åº”ç”¨äºå››ç§å…ˆè¿›çš„LLMä»£ç ç”Ÿæˆæ¨¡å‹ï¼Œå‘ç°æ‰€æœ‰æ¨¡å‹ç”Ÿæˆçš„ä»£ç éƒ½å­˜åœ¨ä¸¥é‡åè§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æ¢è®¨äº†å¤šç§ç¼“è§£åè§çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬Chain-of-Thoughtï¼ˆCoTï¼‰æç¤ºæ³•ã€ç»“åˆæ­£é¢è§’è‰²æ‰®æ¼”ä¸CoTæç¤ºæ³•ä»¥åŠä¸Solarå¯¹è¯ç­‰ã€‚å®éªŒè¡¨æ˜ï¼Œä¸Solarçš„å¯¹è¯å¯ä»¥æœ‰æ•ˆå‡å°‘LLMç”Ÿæˆä»£ç ä¸­çš„ç¤¾ä¼šåè§ï¼Œé«˜è¾¾90%ã€‚æœ€åï¼Œå…¬å¼€çš„ä»£ç å’Œæ•°æ®é›†å¯é«˜åº¦æ‰©å±•ï¼Œç”¨äºè¯„ä¼°æ–°çš„ç¤¾ä¼šé—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆä¸­è¡¨ç°å‡ºæ˜¾è‘—è¿›å±•ã€‚</li>
<li>LLMç”Ÿæˆçš„ä»£ç ä¸­å¯èƒ½å­˜åœ¨ç¤¾ä¼šåè§çš„é—®é¢˜å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚</li>
<li>Solaræ¡†æ¶è¢«æå‡ºç”¨äºè¯„ä¼°å’Œç¼“è§£LLMç”Ÿæˆä»£ç ä¸­çš„ç¤¾ä¼šåè§ã€‚</li>
<li>Solarèƒ½è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ä»¥å‘ç°LLMç”Ÿæˆä»£ç ä¸­çš„ç¤¾ä¼šåè§ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªæ¶µç›–å¤šç§ç¤¾ä¼šé—®é¢˜çš„æ•°æ®é›†ï¼Œç”¨äºé‡åŒ–ç”Ÿæˆä»£ç ä¸­ç¤¾ä¼šåè§çš„ä¸¥é‡ç¨‹åº¦ã€‚</li>
<li>å¯¹å››ç§å…ˆè¿›çš„LLMä»£ç ç”Ÿæˆæ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°å­˜åœ¨ä¸¥é‡åè§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-62e21c85bd0a35d31f4248953d8d42f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2733e10ea8d115e0ec2a41c89dc13133.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6be8a100582abc67c57003030921274.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4123bd42d872de181dc57f4f838b50ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64379123c25fe8b7c7669a1d2d352644.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7426eb788a641a457e1d50a836288590.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Improving-Data-Efficiency-via-Curating-LLM-Driven-Rating-Systems"><a href="#Improving-Data-Efficiency-via-Curating-LLM-Driven-Rating-Systems" class="headerlink" title="Improving Data Efficiency via Curating LLM-Driven Rating Systems"></a>Improving Data Efficiency via Curating LLM-Driven Rating Systems</h2><p><strong>Authors:Jinlong Pang, Jiaheng Wei, Ankit Parag Shah, Zhaowei Zhu, Yaxuan Wang, Chen Qian, Yang Liu, Yujia Bao, Wei Wei</strong></p>
<p>Instruction tuning is critical for adapting large language models (LLMs) to downstream tasks, and recent studies have demonstrated that small amounts of human-curated data can outperform larger datasets, challenging traditional data scaling laws. While LLM-based data quality rating systems offer a cost-effective alternative to human annotation, they often suffer from inaccuracies and biases, even in powerful models like GPT-4. In this work, we introduce DS2, a Diversity-aware Score curation method for Data Selection. By systematically modeling error patterns through a score transition matrix, DS2 corrects LLM-based scores and promotes diversity in the selected data samples. Our approach shows that a curated subset (just 3.3% of the original dataset) outperforms full-scale datasets (300k samples) across various machine-alignment benchmarks, and matches or surpasses human-aligned datasets such as LIMA with the same sample size (1k samples). These findings challenge conventional data scaling assumptions, highlighting that redundant, low-quality samples can degrade performance and reaffirming that â€œmore can be less.â€ </p>
<blockquote>
<p>æŒ‡ä»¤è°ƒæ•´å¯¹äºé€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå°‘é‡äººå·¥æ•´ç†çš„æ•°æ®å¯ä»¥è¶…è¶Šå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒæŒ‘æˆ˜ä¼ ç»Ÿæ•°æ®è§„æ¨¡å®šå¾‹ã€‚è™½ç„¶åŸºäºLLMçš„æ•°æ®è´¨é‡è¯„åˆ†ç³»ç»Ÿä¸ºäººå·¥æ ‡æ³¨æä¾›äº†ä¸€ç§æˆæœ¬æ•ˆç›Šé«˜çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å®ƒä»¬å³ä½¿åœ¨GPT-4ç­‰å¼ºå¤§æ¨¡å‹ä¸­ä¹Ÿä¼šå­˜åœ¨ä¸å‡†ç¡®å’Œåè§çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†DS2ï¼Œä¸€ç§ç”¨äºæ•°æ®é€‰æ‹©çš„å¤šæ ·æ€§æ„ŸçŸ¥åˆ†æ•°æ•´ç†æ–¹æ³•ã€‚é€šè¿‡ç³»ç»Ÿåœ°é€šè¿‡è¯„åˆ†è½¬æ¢çŸ©é˜µå¯¹é”™è¯¯æ¨¡å¼è¿›è¡Œå»ºæ¨¡ï¼ŒDS2æ ¡æ­£äº†åŸºäºLLMçš„è¯„åˆ†å¹¶ä¿ƒè¿›äº†æ‰€é€‰æ•°æ®æ ·æœ¬çš„å¤šæ ·æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¡¨æ˜ï¼Œç»è¿‡æ•´ç†çš„å­é›†ï¼ˆä»…å åŸå§‹æ•°æ®é›†çš„3.3%ï¼‰åœ¨å„ç§æœºå™¨å¯¹é½åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå…¨é¢æ•°æ®é›†ï¼ˆ30ä¸‡æ ·æœ¬ï¼‰ï¼Œå¹¶ä¸”ä¸ç›¸åŒæ ·æœ¬é‡ï¼ˆ1åƒä¸ªæ ·æœ¬ï¼‰çš„äººç±»å¯¹é½æ•°æ®é›†å¦‚LIMAç›¸åŒ¹é…æˆ–æ›´èƒœä¸€ç­¹ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†ä¼ ç»Ÿæ•°æ®è§„æ¨¡å‡è®¾ï¼Œå¼ºè°ƒå†—ä½™ã€ä½è´¨é‡çš„æ ·æœ¬å¯èƒ½ä¼šé™ä½æ€§èƒ½å¹¶å†æ¬¡è¯æ˜â€œæ›´å¤šå¯èƒ½æ˜¯æ›´å°‘â€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.10877v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒäº†æŒ‡ä»¤å¾®è°ƒåœ¨é€‚åº”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ°ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºå°é‡çš„äººå·¥ç¼–åˆ¶æ•°æ®å¯èƒ½ä¼˜äºå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒæŒ‘æˆ˜äº†ä¼ ç»Ÿæ•°æ®è§„æ¨¡å®šå¾‹ã€‚é’ˆå¯¹LLMåŸºç¡€æ•°æ®è´¨é‡è¯„ä¼°ç³»ç»Ÿå­˜åœ¨çš„ä¸å‡†ç¡®å’Œåè§é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†DS2ï¼Œä¸€ç§ç”¨äºæ•°æ®é€‰æ‹©çš„å¤šæ ·æ€§æ„ŸçŸ¥åˆ†æ•°ç¼–çº‚æ–¹æ³•ã€‚é€šè¿‡ç³»ç»Ÿå»ºæ¨¡è¯¯å·®æ¨¡å¼ï¼ŒDS2æ ¡æ­£äº†LLMåŸºç¡€ä¸Šçš„åˆ†æ•°å¹¶ä¿ƒè¿›äº†æ‰€é€‰æ•°æ®æ ·æœ¬çš„å¤šæ ·æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç²¾é€‰çš„å­é›†ï¼ˆä»…å åŸå§‹æ•°æ®çš„3.3%ï¼‰åœ¨å„ç§æœºå™¨å¯¹é½åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå…¨é¢æ•°æ®é›†ï¼ˆ30ä¸‡æ ·æœ¬ï¼‰ï¼Œå¹¶åœ¨ç›¸åŒæ ·æœ¬é‡ä¸‹ä¸äººå·¥å¯¹é½çš„æ•°æ®é›†ï¼ˆå¦‚LIMAï¼‰ç›¸åŒ¹é…æˆ–è¡¨ç°æ›´å¥½ã€‚è¿™äº›å‘ç°æŒ‘æˆ˜äº†ä¼ ç»Ÿæ•°æ®è§„æ¨¡å‡è®¾ï¼Œå¼ºè°ƒå†—ä½™ã€ä½è´¨é‡çš„æ ·æœ¬ä¼šæŸå®³æ€§èƒ½ï¼Œå†æ¬¡è¯æ˜äº†â€œæ›´å¤šä¸ä¸€å®šæ›´å¥½â€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŒ‡ä»¤å¾®è°ƒåœ¨LLMé€‚åº”ä¸‹æ¸¸ä»»åŠ¡ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>å°è§„æ¨¡äººå·¥ç¼–åˆ¶æ•°æ®åœ¨æ€§èƒ½ä¸Šå¯èƒ½ä¼˜äºå¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¯¹ä¼ ç»Ÿæ•°æ®è§„æ¨¡å®šå¾‹æå‡ºæŒ‘æˆ˜ã€‚</li>
<li>LLMåŸºç¡€æ•°æ®è´¨é‡è¯„ä¼°ç³»ç»Ÿå­˜åœ¨ä¸å‡†ç¡®æ€§å’Œåè§é—®é¢˜ã€‚</li>
<li>DS2æ–¹æ³•é€šè¿‡ç³»ç»Ÿå»ºæ¨¡è¯¯å·®æ¨¡å¼æ¥æ ¡æ­£LLMåˆ†æ•°å¹¶ä¿ƒè¿›æ•°æ®å¤šæ ·æ€§ã€‚</li>
<li>DS2æ–¹æ³•æ˜¾ç¤ºç²¾é€‰å­é›†åœ¨å„ç§æœºå™¨å¯¹é½åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä¸ç›¸åŒè§„æ¨¡çš„äººå·¥å¯¹é½æ•°æ®é›†ç›¸æ¯”ï¼Œç²¾é€‰å­é›†çš„è¡¨ç°ç›¸åŒ¹é…æˆ–æ›´ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.10877">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3614a0b352885a9e91600561589aede.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f11386ea3313af7c4120c99c1d6cd4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5172b34c7780a7496719bb21678f50bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-711ab48503ba7265d8fe1c580aa6e762.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d79995305895b9e21052be432ea22002.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="AlphaEdit-Null-Space-Constrained-Knowledge-Editing-for-Language-Models"><a href="#AlphaEdit-Null-Space-Constrained-Knowledge-Editing-for-Language-Models" class="headerlink" title="AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models"></a>AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models</h2><p><strong>Authors:Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Shi Jie, Xiang Wang, Xiangnan He, Tat-seng Chua</strong></p>
<p>Large language models (LLMs) often exhibit hallucinations due to incorrect or outdated knowledge. Hence, model editing methods have emerged to enable targeted knowledge updates. To achieve this, a prevailing paradigm is the locating-then-editing approach, which first locates influential parameters and then edits them by introducing a perturbation. While effective, current studies have demonstrated that this perturbation inevitably disrupt the originally preserved knowledge within LLMs, especially in sequential editing scenarios. To address this, we introduce AlphaEdit, a novel solution that projects perturbation onto the null space of the preserved knowledge before applying it to the parameters. We theoretically prove that this projection ensures the output of post-edited LLMs remains unchanged when queried about the preserved knowledge, thereby mitigating the issue of disruption. Extensive experiments on various LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts the performance of most locating-then-editing methods by an average of 36.4% with a single line of additional code for projection solely. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/jianghoucheng/AlphaEdit">https://github.com/jianghoucheng/AlphaEdit</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”±äºçŸ¥è¯†ä¸æ­£ç¡®æˆ–è¿‡æ—¶ï¼Œç»å¸¸ä¼šå‡ºç°å¹»è§‰ã€‚å› æ­¤ï¼Œå‡ºç°äº†æ¨¡å‹ç¼–è¾‘æ–¹æ³•ï¼Œä»¥å®ç°é’ˆå¯¹æ€§çš„çŸ¥è¯†æ›´æ–°ã€‚ä¸ºæ­¤ï¼Œä¸€ç§æ™®éçš„æ–¹æ³•æ˜¯â€œå®šä½ç„¶åç¼–è¾‘â€çš„æ–¹æ³•ï¼Œå®ƒé¦–å…ˆå®šä½æœ‰å½±å“åŠ›çš„å‚æ•°ï¼Œç„¶åé€šè¿‡å¼•å…¥æ‰°åŠ¨æ¥è¿›è¡Œç¼–è¾‘ã€‚å°½ç®¡è¿™ç§æ–¹æ³•æœ‰æ•ˆï¼Œä½†å½“å‰çš„ç ”ç©¶å·²ç»è¯æ˜ï¼Œè¿™ç§æ‰°åŠ¨ä¸å¯é¿å…åœ°ä¼šç ´åLLMä¸­åŸæœ¬ä¿å­˜çš„çŸ¥è¯†ï¼Œç‰¹åˆ«æ˜¯åœ¨è¿ç»­ç¼–è¾‘åœºæ™¯ä¸­ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AlphaEditï¼Œè¿™æ˜¯ä¸€ç§å…¨æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œå®ƒå°†æ‰°åŠ¨æŠ•å½±åˆ°ä¿ç•™çŸ¥è¯†çš„é›¶ç©ºé—´ä¸Šï¼Œç„¶åå†å°†å…¶åº”ç”¨åˆ°å‚æ•°ä¸Šã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†è¿™ç§æŠ•å½±ç¡®ä¿äº†å½“è¢«é—®åŠä¿ç•™çš„çŸ¥è¯†æ—¶ï¼Œç¼–è¾‘åçš„LLMçš„è¾“å‡ºä¿æŒä¸å˜ï¼Œä»è€Œç¼“è§£äº†çŸ¥è¯†è¢«å¹²æ‰°çš„é—®é¢˜ã€‚åœ¨åŒ…æ‹¬LLaMA3ã€GPT2-XLå’ŒGPT-Jç­‰å„ç§LLMä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAlphaEdité€šè¿‡å°†æŠ•å½±çš„å•è¡Œä»£ç æ·»åŠ åˆ°ç°æœ‰çš„å®šä½ç„¶åç¼–è¾‘æ–¹æ³•ä¸­ï¼Œå¹³å‡æé«˜äº†å…¶æ€§èƒ½çº¦36.4%ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/jianghoucheng/AlphaEdit%E3%80%82">https://github.com/jianghoucheng/AlphaEditã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02355v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜“å‡ºç°å¹»è§‰ï¼ŒåŸå› åœ¨äºçŸ¥è¯†ä¸æ­£ç¡®æˆ–è¿‡æ—¶ã€‚å› æ­¤ï¼Œå‡ºç°äº†æ¨¡å‹ç¼–è¾‘æ–¹æ³•å¯¹çŸ¥è¯†åº“è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„æ›´æ–°ã€‚å½“å‰ä¸»æµæ–¹æ³•æ˜¯å®šä½-ç¼–è¾‘æ¨¡å¼ï¼Œå³å…ˆå®šä½å…³é”®å‚æ•°åè¿›è¡Œç¼–è¾‘ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹å¼ä¼šç ´ååŸæœ‰çŸ¥è¯†ï¼Œç‰¹åˆ«æ˜¯åœ¨è¿ç»­ç¼–è¾‘åœºæ™¯ä¸­ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºAlphaEditï¼Œä¸€ç§å°†æ‰°åŠ¨æŠ•å½±åˆ°ä¿ç•™çŸ¥è¯†çš„é›¶ç©ºé—´å†åº”ç”¨äºå‚æ•°çš„æ–°æ–¹æ³•ã€‚ç†è®ºè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½ç¡®ä¿å¯¹ä¿ç•™çŸ¥è¯†çš„æŸ¥è¯¢å¾—åˆ°ç›¸åŒè¾“å‡ºï¼Œè§£å†³äº†æ‰°åŠ¨å¯¼è‡´çš„ç ´åé—®é¢˜ã€‚åœ¨å¤šç§LLMä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAlphaEditèƒ½å¹³å‡æå‡å®šä½-ç¼–è¾‘æ–¹æ³•çš„æ€§èƒ½è¾¾36.4%ï¼Œä»…éœ€æ·»åŠ ä¸€è¡ŒæŠ•å½±ä»£ç ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/jianghoucheng/AlphaEdit%E3%80%82">https://github.com/jianghoucheng/AlphaEditã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMsæ˜“å› çŸ¥è¯†ä¸æ­£ç¡®æˆ–è¿‡æ—¶äº§ç”Ÿå¹»è§‰ã€‚</li>
<li>ç°æœ‰çš„æ¨¡å‹ç¼–è¾‘æ–¹æ³•é‡‡ç”¨å®šä½-ç¼–è¾‘æ¨¡å¼ï¼Œä½†ä¼šç ´ååŸæœ‰çŸ¥è¯†ã€‚</li>
<li>AlphaEdité€šè¿‡æŠ•å½±æŠ€æœ¯å°†æ‰°åŠ¨åº”ç”¨äºä¿ç•™çŸ¥è¯†çš„é›¶ç©ºé—´ï¼Œç¡®ä¿å¯¹ä¿ç•™çŸ¥è¯†çš„æŸ¥è¯¢è¾“å‡ºä¸å˜ã€‚</li>
<li>AlphaEditèƒ½æ˜¾è‘—æå‡å®šä½-ç¼–è¾‘æ–¹æ³•çš„æ€§èƒ½ï¼Œå¹³å‡æå‡è¾¾36.4%ã€‚</li>
<li>AlphaEditæ–¹æ³•åªéœ€åœ¨ç°æœ‰ä»£ç åŸºç¡€ä¸Šæ·»åŠ ä¸€è¡ŒæŠ•å½±ä»£ç ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02355">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9949cb3f937e23fde60c58b32c83922d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72b287c1115359cba8510b1da8648019.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-213d4c019b0c265a3d553fd026937630.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c522e885475e01b094ee4982193718a9.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SynSUM-â€“-Synthetic-Benchmark-with-Structured-and-Unstructured-Medical-Records"><a href="#SynSUM-â€“-Synthetic-Benchmark-with-Structured-and-Unstructured-Medical-Records" class="headerlink" title="SynSUM â€“ Synthetic Benchmark with Structured and Unstructured Medical   Records"></a>SynSUM â€“ Synthetic Benchmark with Structured and Unstructured Medical   Records</h2><p><strong>Authors:Paloma Rabaey, Henri Arno, Stefan Heytens, Thomas Demeester</strong></p>
<p>We present the SynSUM benchmark, a synthetic dataset linking unstructured clinical notes to structured background variables. The dataset consists of 10,000 artificial patient records containing tabular variables (like symptoms, diagnoses and underlying conditions) and related notes describing the fictional patient encounter in the domain of respiratory diseases. The tabular portion of the data is generated through a Bayesian network, where both the causal structure between the variables and the conditional probabilities are proposed by an expert based on domain knowledge. We then prompt a large language model (GPT-4o) to generate a clinical note related to this patient encounter, describing the patient symptoms and additional context. We conduct both an expert evaluation study to assess the quality of the generated notes, as well as running some simple predictor models on both the tabular and text portions of the dataset, forming a baseline for further research. The SynSUM dataset is primarily designed to facilitate research on clinical information extraction in the presence of tabular background variables, which can be linked through domain knowledge to concepts of interest to be extracted from the text - the symptoms, in the case of SynSUM. Secondary uses include research on the automation of clinical reasoning over both tabular data and text, causal effect estimation in the presence of tabular and&#x2F;or textual confounders, and multi-modal synthetic data generation. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†SynSUMåŸºå‡†æµ‹è¯•é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆæˆæ•°æ®é›†ï¼Œå®ƒå°†éç»“æ„åŒ–çš„ä¸´åºŠç¬”è®°ä¸ç»“æ„åŒ–çš„èƒŒæ™¯å˜é‡è”ç³»èµ·æ¥ã€‚è¯¥æ•°æ®é›†åŒ…å«10,000æ¡äººå·¥æ‚£è€…è®°å½•ï¼ŒåŒ…æ‹¬è¡¨æ ¼å˜é‡ï¼ˆå¦‚ç—‡çŠ¶ã€è¯Šæ–­å’Œæ½œåœ¨ç–¾ç—…ï¼‰ä»¥åŠæè¿°å‘¼å¸ç³»ç»Ÿç–¾ç—…é¢†åŸŸä¸­è™šæ„æ‚£è€…é­é‡çš„ç›¸å…³ç¬”è®°ã€‚æ•°æ®çš„è¡¨æ ¼éƒ¨åˆ†æ˜¯é€šè¿‡è´å¶æ–¯ç½‘ç»œç”Ÿæˆçš„ï¼Œå˜é‡ä¹‹é—´çš„å› æœç»“æ„å’Œæ¡ä»¶æ¦‚ç‡å‡ç”±ä¸“å®¶åŸºäºé¢†åŸŸçŸ¥è¯†æå‡ºã€‚ç„¶åï¼Œæˆ‘ä»¬æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆGPT-4oï¼‰ç”Ÿæˆä¸è¯¥æ‚£è€…é­é‡ç›¸å…³çš„ä¸´åºŠç¬”è®°ï¼Œæè¿°æ‚£è€…çš„ç—‡çŠ¶å’Œé¢å¤–çš„ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬æ—¢è¿›è¡Œäº†ä¸“å®¶è¯„ä¼°ç ”ç©¶ä»¥è¯„ä¼°ç”Ÿæˆçš„ç¬”è®°çš„è´¨é‡ï¼Œä¹Ÿåœ¨æ•°æ®é›†çš„æ–‡å­—å’Œè¡¨æ ¼éƒ¨åˆ†ä¸Šè¿è¡Œäº†ä¸€äº›ç®€å•çš„é¢„æµ‹æ¨¡å‹ï¼Œä¸ºè¿›ä¸€æ­¥çš„ç ”ç©¶æä¾›å‚è€ƒåŸºå‡†ã€‚SynSUMæ•°æ®é›†ä¸»è¦ç”¨äºä¿ƒè¿›åœ¨æœ‰è¡¨æ ¼èƒŒæ™¯å˜é‡çš„æƒ…å†µä¸‹è¿›è¡Œä¸´åºŠä¿¡æ¯æå–çš„ç ”ç©¶ï¼Œè¿™äº›èƒŒæ™¯å˜é‡å¯ä»¥é€šè¿‡é¢†åŸŸçŸ¥è¯†é“¾æ¥åˆ°æ–‡æœ¬ä¸­æ„Ÿå…´è¶£çš„æ¦‚å¿µâ€”â€”ä»¥SynSUMä¸­çš„ç—‡çŠ¶ä¸ºä¾‹ã€‚æ¬¡è¦ç”¨é€”åŒ…æ‹¬åœ¨è¡¨æ ¼æ•°æ®å’Œæ–‡æœ¬ä¸Šçš„è‡ªåŠ¨åŒ–ä¸´åºŠæ¨ç†ç ”ç©¶ã€å­˜åœ¨è¡¨æ ¼å’Œ&#x2F;æˆ–æ–‡æœ¬æ··æ·†å› ç´ æ—¶çš„å› æœæ•ˆåº”ä¼°è®¡ä»¥åŠå¤šæ¨¡å¼åˆæˆæ•°æ®ç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08936v2">PDF</a> The dataset can be downloaded from <a target="_blank" rel="noopener" href="https://github.com/prabaey/synsum">https://github.com/prabaey/synsum</a>.   Presented at the GenAI4Health workshop at AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ•°æ®é›†SynSUMä¸ºåˆæˆæ•°æ®é›†ï¼Œé“¾æ¥äº†ç»“æ„åŒ–èƒŒæ™¯å˜é‡ä¸éç»“æ„åŒ–ä¸´åºŠç¬”è®°ã€‚æ•°æ®é›†åŒ…å«ä¸€ä¸‡æ¡è™šæ„çš„æ‚£è€…è®°å½•ï¼Œæ¶‰åŠå‘¼å¸ç–¾ç—…é¢†åŸŸçš„è¡¨æ ¼å˜é‡ï¼ˆå¦‚ç—‡çŠ¶ã€è¯Šæ–­åŠåŸºç¡€ç–¾ç—…ï¼‰åŠç›¸å…³æè¿°æ‚£è€…é­é‡çš„ä¸´åºŠç¬”è®°ã€‚è¡¨æ ¼æ•°æ®é€šè¿‡è´å¶æ–¯ç½‘ç»œç”Ÿæˆï¼Œä¸“å®¶åŸºäºé¢†åŸŸçŸ¥è¯†æå‡ºå˜é‡é—´çš„å› æœç»“æ„åŠæ¡ä»¶æ¦‚ç‡ã€‚ä¹‹ååˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆGPT-4oï¼‰ç”Ÿæˆä¸ç—…äººé­é‡ç›¸å…³çš„ä¸´åºŠç¬”è®°ï¼Œæè¿°ç—…äººç—‡çŠ¶åŠé¢å¤–æƒ…å¢ƒã€‚æœ¬ç ”ç©¶é€šè¿‡ä¸“å®¶è¯„ä¼°ç”Ÿæˆçš„ç¬”è®°è´¨é‡ï¼ŒåŒæ—¶åœ¨æ•°æ®é›†æ–‡æœ¬å’Œè¡¨æ ¼éƒ¨åˆ†è¿è¡Œç®€å•çš„é¢„æµ‹æ¨¡å‹ï¼Œä¸ºåç»­ç ”ç©¶æä¾›åŸºçº¿ã€‚SynSUMæ•°æ®é›†ä¸»è¦ç”¨äºä¿ƒè¿›ä¸´åºŠä¿¡æ¯æå–ç ”ç©¶ï¼Œå°¤å…¶åœ¨å­˜åœ¨è¡¨æ ¼èƒŒæ™¯å˜é‡çš„æƒ…å†µä¸‹ï¼Œå¯é€šè¿‡é¢†åŸŸçŸ¥è¯†é“¾æ¥æ–‡æœ¬ä¸­çš„æ¦‚å¿µï¼ˆå¦‚SynSUMä¸­çš„ç—‡çŠ¶ï¼‰ã€‚æ¬¡è¦ç”¨é€”åŒ…æ‹¬è‡ªåŠ¨åŒ–ä¸´åºŠæ¨ç†ç ”ç©¶ï¼Œå¤„ç†è¡¨æ ¼å’Œæ–‡æœ¬æ•°æ®æ—¶çš„å› æœæ•ˆåº”ä¼°ç®—å’Œå¤šæ¨¡æ€åˆæˆæ•°æ®ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SynSUMæ˜¯ä¸€ä¸ªåˆæˆæ•°æ®é›†ï¼Œå°†éç»“æ„åŒ–ä¸´åºŠç¬”è®°ä¸ç»“æ„åŒ–èƒŒæ™¯å˜é‡é“¾æ¥èµ·æ¥ã€‚</li>
<li>æ•°æ®é›†åŒ…å«ä¸€ä¸‡æ¡è™šæ„çš„æ‚£è€…è®°å½•ï¼Œæ¶‰åŠå‘¼å¸ç–¾ç—…é¢†åŸŸçš„è¡¨æ ¼æ•°æ®å’Œä¸´åºŠç¬”è®°ã€‚</li>
<li>è¡¨æ ¼æ•°æ®é€šè¿‡è´å¶æ–¯ç½‘ç»œç”Ÿæˆï¼Œåæ˜ å˜é‡é—´çš„å› æœç»“æ„å’Œæ¡ä»¶æ¦‚ç‡ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸´åºŠç¬”è®°ï¼Œæè¿°æ‚£è€…ç—‡çŠ¶åŠæƒ…å¢ƒã€‚</li>
<li>æ•°æ®é›†ç”¨äºç ”ç©¶ä¸´åºŠä¿¡æ¯æå–ã€è‡ªåŠ¨åŒ–ä¸´åºŠæ¨ç†ã€å› æœæ•ˆåº”ä¼°ç®—å’Œå¤šæ¨¡æ€åˆæˆæ•°æ®ç”Ÿæˆç­‰ã€‚</li>
<li>æ•°æ®é›†ç»è¿‡ä¸“å®¶è¯„ä¼°ç”Ÿæˆçš„ç¬”è®°è´¨é‡ï¼ŒåŒæ—¶æä¾›äº†é¢„æµ‹æ¨¡å‹çš„åŸºçº¿æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08936">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-66b85e0a2c75b15539228e096a9a9367.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-629003cbd6eb8c5b5484367400a72eea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c22ea4cac9e92aa2eda8cba314931448.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b6c4aa3fe9e550d23b242f3c76ee0e5.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Transformer-Block-Coupling-and-its-Correlation-with-Generalization-in-LLMs"><a href="#Transformer-Block-Coupling-and-its-Correlation-with-Generalization-in-LLMs" class="headerlink" title="Transformer Block Coupling and its Correlation with Generalization in   LLMs"></a>Transformer Block Coupling and its Correlation with Generalization in   LLMs</h2><p><strong>Authors:Murdock Aubry, Haoming Meng, Anton Sugolov, Vardan Papyan</strong></p>
<p>Large Language Models (LLMs) have made significant strides in natural language processing, and a precise understanding of the internal mechanisms driving their success is essential. In this work, we analyze the trajectories of token embeddings as they pass through transformer blocks, linearizing the system along these trajectories through their Jacobian matrices. By examining the relationships between these block Jacobians, we uncover the phenomenon of \textbf{transformer block coupling} in a multitude of LLMs, characterized by the coupling of their top singular vectors across tokens and depth. Our findings reveal that coupling \textit{positively correlates} with model performance, and that this relationship is stronger than with other hyperparameters such as parameter count, model depth, and embedding dimension. We further investigate how these properties emerge during training, observing a progressive development of coupling, increased linearity, and layer-wise exponential growth in token trajectories. Additionally, experiments with Vision Transformers (ViTs) corroborate the emergence of coupling and its relationship with generalization, reinforcing our findings in LLMs. Collectively, these insights offer a novel perspective on token interactions in transformers, opening new directions for studying their mechanisms as well as improving training and generalization. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå¯¹é©±åŠ¨å…¶æˆåŠŸçš„å†…éƒ¨æœºåˆ¶è¿›è¡Œç²¾ç¡®ç†è§£è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†ä»¤ç‰ŒåµŒå…¥åœ¨é€šè¿‡transformerå—æ—¶çš„è½¨è¿¹ï¼Œé€šè¿‡å…¶é›…å¯æ¯”çŸ©é˜µæ²¿è¿™äº›è½¨è¿¹çº¿æ€§åŒ–ç³»ç»Ÿã€‚é€šè¿‡æ£€æŸ¥è¿™äº›å—é›…å¯æ¯”ä¹‹é—´çš„å…³ç³»ï¼Œæˆ‘ä»¬åœ¨å¤šç§LLMä¸­å‘ç°äº†â€œtransformerå—è€¦åˆâ€ç°è±¡ï¼Œå…¶ç‰¹å¾åœ¨äºä»¤ç‰Œå’Œæ·±åº¦ä¹‹é—´çš„é¡¶éƒ¨å¥‡å¼‚å‘é‡çš„è€¦åˆã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè€¦åˆä¸æ¨¡å‹æ€§èƒ½å‘ˆæ­£ç›¸å…³ï¼Œè€Œä¸”è¿™ç§å…³ç³»æ¯”å‚æ•°æ•°é‡ã€æ¨¡å‹æ·±åº¦å’ŒåµŒå…¥ç»´åº¦ç­‰å…¶ä»–è¶…å‚æ•°æ›´å¼ºã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è°ƒæŸ¥äº†è¿™äº›å±æ€§åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‡ºç°æƒ…å†µï¼Œè§‚å¯Ÿåˆ°è€¦åˆã€çº¿æ€§å¢å¼ºå’Œé€å±‚æŒ‡æ•°å¢é•¿çš„è½¨è¿¹çš„å‘å±•è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œå¯¹è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰çš„å®éªŒè¯å®äº†è€¦åˆçš„å‡ºç°åŠå…¶ä¸æ³›åŒ–çš„å…³ç³»ï¼Œè¯å®äº†æˆ‘ä»¬åœ¨LLMä¸­çš„å‘ç°ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›è§è§£ä¸ºå˜å‹å™¨ä¸­çš„ä»¤ç‰Œäº¤äº’æä¾›äº†æ–°çš„è§†è§’ï¼Œä¸ºå­¦ä¹ å…¶æœºåˆ¶ä»¥åŠæ”¹è¿›è®­ç»ƒå’Œæ³›åŒ–èƒ½åŠ›å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.07810v5">PDF</a> Published as a conference paper at the International Conference on   Learning Representations (ICLR 2025)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å˜æ¢å™¨å—è¿›è¡Œæ ‡è®°åµŒå…¥è½¨è¿¹åˆ†æï¼Œæ­ç¤ºäº†å˜æ¢å™¨å—è€¦åˆç°è±¡ã€‚è¯¥ç°è±¡è¡¨ç°ä¸ºæ ‡è®°å’Œæ·±åº¦ä¹‹é—´çš„é¡¶éƒ¨å¥‡å¼‚å‘é‡è€¦åˆã€‚ç ”ç©¶å‘ç°ï¼Œè€¦åˆä¸æ¨¡å‹æ€§èƒ½æ­£ç›¸å…³ï¼Œä¸å…¶ä»–è¶…å‚æ•°ç›¸æ¯”ï¼Œå¦‚å‚æ•°è®¡æ•°ã€æ¨¡å‹æ·±åº¦å’ŒåµŒå…¥ç»´åº¦ç­‰æ›´ä¸ºç´§å¯†ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè§‚å¯Ÿåˆ°è€¦åˆã€çº¿æ€§å¢é•¿å’Œé€å±‚æŒ‡æ•°å¢é•¿çš„æ ‡è®°è½¨è¿¹çš„é€æ­¥å‘å±•ã€‚æ­¤å¤–ï¼Œè§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰å®éªŒä¹Ÿè¯å®äº†è€¦åˆçš„å‡ºç°ä¸å…¶æ³›åŒ–æ€§çš„å…³è”ï¼Œä¸ºç†è§£æ ‡è®°åœ¨è½¬æ¢å™¨ä¸­çš„äº¤äº’ä½œç”¨æä¾›äº†æ–°è§†è§’ï¼Œä¹Ÿä¸ºæ”¹è¿›è®­ç»ƒå’Œæ³›åŒ–æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsé€šè¿‡å˜æ¢å™¨å—è¿›è¡Œæ ‡è®°åµŒå…¥è½¨è¿¹åˆ†æï¼Œæ­ç¤ºäº†å˜æ¢å™¨å—è€¦åˆç°è±¡ã€‚</li>
<li>å˜æ¢å™¨å—è€¦åˆè¡¨ç°ä¸ºæ ‡è®°å’Œæ·±åº¦ä¹‹é—´çš„é¡¶éƒ¨å¥‡å¼‚å‘é‡è€¦åˆã€‚</li>
<li>è€¦åˆä¸æ¨¡å‹æ€§èƒ½å‘ˆæ­£ç›¸å…³ã€‚</li>
<li>ç›¸æ¯”å…¶ä»–è¶…å‚æ•°ï¼Œæ¨¡å‹è€¦åˆä¸æ¨¡å‹æ€§èƒ½çš„å…³ç³»æ›´ä¸ºç´§å¯†ã€‚</li>
<li>è®­ç»ƒè¿‡ç¨‹ä¸­è§‚å¯Ÿåˆ°è€¦åˆã€çº¿æ€§å¢é•¿å’Œé€å±‚æŒ‡æ•°å¢é•¿çš„æ ‡è®°è½¨è¿¹çš„é€æ­¥å‘å±•ã€‚</li>
<li>ViTå®éªŒè¯å®äº†è€¦åˆçš„å‡ºç°ä¸æ³›åŒ–æ€§ä¹‹é—´çš„å…³è”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.07810">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7299557f36276ca627b81b5e4a88ec80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d17c87cfff274cebf1d5c847fd46e59f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03dabf3ff5d59bae61a8b625c3759f11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca4bc421c2aed853499760675080ccb0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Efficient-Automated-Circuit-Discovery-in-Transformers-using-Contextual-Decomposition"><a href="#Efficient-Automated-Circuit-Discovery-in-Transformers-using-Contextual-Decomposition" class="headerlink" title="Efficient Automated Circuit Discovery in Transformers using Contextual   Decomposition"></a>Efficient Automated Circuit Discovery in Transformers using Contextual   Decomposition</h2><p><strong>Authors:Aliyah R. Hsu, Georgia Zhou, Yeshwanth Cherapanamjeri, Yaxuan Huang, Anobel Y. Odisho, Peter R. Carroll, Bin Yu</strong></p>
<p>Automated mechanistic interpretation research has attracted great interest due to its potential to scale explanations of neural network internals to large models. Existing automated circuit discovery work relies on activation patching or its approximations to identify subgraphs in models for specific tasks (circuits). They often suffer from slow runtime, approximation errors, and specific requirements of metrics, such as non-zero gradients. In this work, we introduce contextual decomposition for transformers (CD-T) to build interpretable circuits in large language models. CD-T can produce circuits of arbitrary level of abstraction, and is the first able to produce circuits as fine-grained as attention heads at specific sequence positions efficiently. CD-T consists of a set of mathematical equations to isolate contribution of model features. Through recursively computing contribution of all nodes in a computational graph of a model using CD-T followed by pruning, we are able to reduce circuit discovery runtime from hours to seconds compared to state-of-the-art baselines. On three standard circuit evaluation datasets (indirect object identification, greater-than comparisons, and docstring completion), we demonstrate that CD-T outperforms ACDC and EAP by better recovering the manual circuits with an average of 97% ROC AUC under low runtimes. In addition, we provide evidence that faithfulness of CD-T circuits is not due to random chance by showing our circuits are 80% more faithful than random circuits of up to 60% of the original model size. Finally, we show CD-T circuits are able to perfectly replicate original modelsâ€™ behavior (faithfulness $ &#x3D; 1$) using fewer nodes than the baselines for all tasks. Our results underscore the great promise of CD-T for efficient automated mechanistic interpretability, paving the way for new insights into the workings of large language models. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–æœºåˆ¶è§£é‡Šç ”ç©¶å› å…¶å°†ç¥ç»ç½‘ç»œå†…éƒ¨è§£é‡Šæ‰©å±•åˆ°å¤§å‹æ¨¡å‹çš„æ½œåŠ›è€Œå¤‡å—å…³æ³¨ã€‚ç°æœ‰çš„è‡ªåŠ¨åŒ–ç”µè·¯å‘ç°å·¥ä½œä¾èµ–äºæ¿€æ´»è¡¥ä¸æˆ–å…¶è¿‘ä¼¼å€¼æ¥è¯†åˆ«æ¨¡å‹ä¸­çš„ç‰¹å®šä»»åŠ¡å­å›¾ï¼ˆç”µè·¯ï¼‰ã€‚å®ƒä»¬å¸¸å¸¸å—åˆ°è¿è¡Œé€Ÿåº¦æ…¢ã€è¿‘ä¼¼è¯¯å·®å’Œç‰¹å®šæŒ‡æ ‡è¦æ±‚ï¼ˆå¦‚éé›¶æ¢¯åº¦ï¼‰çš„å›°æ‰°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç”¨äºæ„å»ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¯è§£é‡Šç”µè·¯çš„å˜å‹å™¨ä¸Šä¸‹æ–‡åˆ†è§£ï¼ˆCD-Tï¼‰ã€‚CD-Tå¯ä»¥äº§ç”Ÿä»»æ„æŠ½è±¡å±‚æ¬¡çš„ç”µè·¯ï¼Œå¹¶ä¸”æ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿé«˜æ•ˆåœ°åœ¨ç‰¹å®šåºåˆ—ä½ç½®äº§ç”Ÿç²¾ç»†åˆ°æ³¨æ„åŠ›å¤´çš„ç”µè·¯çš„æ–¹æ³•ã€‚CD-Tç”±ä¸€ç»„æ•°å­¦æ–¹ç¨‹æ„æˆï¼Œç”¨äºéš”ç¦»æ¨¡å‹ç‰¹å¾çš„è´¡çŒ®ã€‚é€šè¿‡é€’å½’è®¡ç®—æ¨¡å‹è®¡ç®—å›¾ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„è´¡çŒ®ï¼Œå¹¶ä½¿ç”¨CD-Tè¿›è¡Œä¿®å‰ªï¼Œä¸æœ€æ–°åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬å°†ç”µè·¯å‘ç°è¿è¡Œæ—¶é—´ä»æ•°å°æ—¶å‡å°‘åˆ°äº†æ•°ç§’ã€‚åœ¨ä¸‰ä¸ªæ ‡å‡†ç”µè·¯è¯„ä¼°æ•°æ®é›†ï¼ˆé—´æ¥å¯¹è±¡è¯†åˆ«ã€å¤§äºæ¯”è¾ƒå’Œdocstringå®Œæˆï¼‰ä¸Šï¼Œæˆ‘ä»¬è¯æ˜äº†CD-Tä¼˜äºACDCå’ŒEAPï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ¢å¤æ‰‹åŠ¨ç”µè·¯ï¼Œåœ¨ä½è¿è¡Œæ—¶é—´ä¸‹çš„å¹³å‡ROC AUCè¾¾åˆ°97%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æ˜¾ç¤ºæˆ‘ä»¬çš„ç”µè·¯æ¯”éšæœºç”µè·¯æ›´å¿ å®ï¼ˆå¿ å®åº¦é«˜å‡º80%ï¼‰ï¼Œè€Œéšæœºç”µè·¯æœ€å¤šè¾¾åˆ°åŸå§‹æ¨¡å‹å¤§å°çš„60%ï¼Œè¯æ˜äº†CD-Tç”µè·¯çš„å¿ å®æ€§å¹¶éå¶ç„¶ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†CD-Tç”µè·¯èƒ½å¤Ÿå®Œç¾å¤åˆ¶åŸå§‹æ¨¡å‹çš„è¡Œä¸ºï¼ˆå¿ å®åº¦&#x3D; 1ï¼‰ï¼Œåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­ä½¿ç”¨æ¯”åŸºçº¿æ›´å°‘çš„èŠ‚ç‚¹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†CD-Tåœ¨é«˜æ•ˆè‡ªåŠ¨åŒ–æœºåˆ¶è§£é‡Šæ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºæ·±å…¥äº†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„å·¥ä½œåŸç†å¼€è¾Ÿäº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.00886v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„è§£é‡Šæ€§ç”µè·¯æ„å»ºæ–¹æ³•â€”â€”ä¸Šä¸‹æ–‡åˆ†è§£æ³•ï¼ˆCD-Tï¼‰ã€‚ç°æœ‰è‡ªåŠ¨åŒ–ç”µè·¯å‘ç°å·¥ä½œå­˜åœ¨è¿è¡Œé€Ÿåº¦æ…¢ã€è¿‘ä¼¼è¯¯å·®å’Œç‰¹å®šæŒ‡æ ‡è¦æ±‚ç­‰é—®é¢˜ï¼Œè€ŒCD-Tèƒ½å¤Ÿäº§ç”Ÿä»»æ„å±‚æ¬¡çš„æŠ½è±¡ç”µè·¯ï¼Œå¹¶èƒ½é«˜æ•ˆåœ°äº§ç”Ÿç²¾ç»†åˆ°ç‰¹å®šåºåˆ—ä½ç½®æ³¨æ„åŠ›å¤´çš„ç”µè·¯ã€‚é€šè¿‡é€’å½’è®¡ç®—æ¨¡å‹ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„è´¡çŒ®å¹¶è¿›è¡Œä¿®å‰ªï¼ŒCD-Tå°†ç”µè·¯å‘ç°çš„æ—¶é—´ä»å°æ—¶å‡å°‘åˆ°ç§’ã€‚åœ¨ä¸‰ä¸ªæ ‡å‡†ç”µè·¯è¯„ä¼°æ•°æ®é›†ä¸Šï¼ŒCD-Tè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¼˜äºACDCå’ŒEAPï¼Œå¹³å‡ROC AUCè¾¾åˆ°97%ã€‚æ­¤å¤–ï¼ŒCD-Tç”µè·¯çš„å¯ä¿¡æ€§è¯æ®æ˜¾ç¤ºï¼Œå…¶æ¯”éšæœºç”µè·¯æ›´å¿ å®äºåŸå§‹æ¨¡å‹ï¼Œä¸”ä½¿ç”¨è¾ƒå°‘çš„èŠ‚ç‚¹å°±èƒ½å®Œå…¨å¤åˆ¶åŸå§‹æ¨¡å‹çš„è¡Œä¸ºã€‚ç»“æœçªå‡ºäº†CD-Tåœ¨é«˜æ•ˆè‡ªåŠ¨åŒ–æœºæ¢°è§£é‡Šæ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºæ·±å…¥äº†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„å·¥ä½œæœºåˆ¶é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç°æœ‰è‡ªåŠ¨åŒ–ç”µè·¯å‘ç°æ–¹æ³•å­˜åœ¨è¿è¡Œé€Ÿåº¦æ…¢ã€è¿‘ä¼¼è¯¯å·®å’Œç‰¹å®šæŒ‡æ ‡è¦æ±‚ç­‰é—®é¢˜ã€‚</li>
<li>å¼•å…¥ä¸Šä¸‹æ–‡åˆ†è§£æ³•ï¼ˆCD-Tï¼‰æ¥æ„å»ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è§£é‡Šæ€§ç”µè·¯ã€‚</li>
<li>CD-Tèƒ½é«˜æ•ˆåœ°äº§ç”Ÿç²¾ç»†åˆ°æ³¨æ„åŠ›å¤´çš„ç”µè·¯ã€‚</li>
<li>é€šè¿‡é€’å½’è®¡ç®—æ¨¡å‹ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„è´¡çŒ®å¹¶è¿›è¡Œä¿®å‰ªï¼Œæ˜¾è‘—å‡å°‘ç”µè·¯å‘ç°æ—¶é—´ã€‚</li>
<li>åœ¨ä¸‰ä¸ªæ ‡å‡†ç”µè·¯è¯„ä¼°æ•°æ®é›†ä¸Šï¼ŒCD-Tè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>CD-Tç”µè·¯æ¯”éšæœºç”µè·¯æ›´å¿ å®äºåŸå§‹æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.00886">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-23a0597799b2a7746915e7052120be48.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SoK-Membership-Inference-Attacks-on-LLMs-are-Rushing-Nowhere-and-How-to-Fix-It"><a href="#SoK-Membership-Inference-Attacks-on-LLMs-are-Rushing-Nowhere-and-How-to-Fix-It" class="headerlink" title="SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How   to Fix It)"></a>SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How   to Fix It)</h2><p><strong>Authors:Matthieu Meeus, Igor Shilov, Shubham Jain, Manuel Faysse, Marek Rei, Yves-Alexandre de Montjoye</strong></p>
<p>Whether LLMs memorize their training data and what this means, from measuring privacy leakage to detecting copyright violations, has become a rapidly growing area of research. In the last few months, more than 10 new methods have been proposed to perform Membership Inference Attacks (MIAs) against LLMs. Contrary to traditional MIAs which rely on fixed-but randomized-records or models, these methods are mostly trained and tested on datasets collected post-hoc. Sets of members and non-members, used to evaluate the MIA, are constructed using informed guesses after the release of a model. This lack of randomization raises concerns of a distribution shift between members and non-members. In this work, we first extensively review the literature on MIAs against LLMs and show that, while most work focuses on sequence-level MIAs evaluated in post-hoc setups, a range of target models, motivations and units of interest are considered. We then quantify distribution shifts present in 6 datasets used in the literature using a model-less bag of word classifier and show that all datasets constructed post-hoc suffer from strong distribution shifts. These shifts invalidate the claims of LLMs memorizing strongly in real-world scenarios and, potentially, also the methodological contributions of the recent papers based on these datasets. Yet, all hope might not be lost. We introduce important considerations to properly evaluate MIAs against LLMs and discuss, in turn, potential ways forwards: randomized test splits, injections of randomized (unique) sequences, randomized fine-tuning, and several post-hoc control methods. While each option comes with its advantages and limitations, we believe they collectively provide solid grounds to guide MIA development and study LLM memorization. We conclude with an overview of recommended approaches to benchmark sequence-level and document-level MIAs against LLMs. </p>
<blockquote>
<p>å…³äºLLMæ˜¯å¦è®°å¿†å…¶è®­ç»ƒæ•°æ®ä»¥åŠè¿™æ„å‘³ç€ä»€ä¹ˆï¼Œä»è¡¡é‡éšç§æ³„éœ²åˆ°æ£€æµ‹ç‰ˆæƒä¾µçŠ¯ï¼Œå·²æˆä¸ºä¸€ä¸ªè¿…é€Ÿå‘å±•çš„ç ”ç©¶é¢†åŸŸã€‚åœ¨è¿‡å»çš„å‡ ä¸ªæœˆé‡Œï¼Œå·²ç»æå‡ºäº†è¶…è¿‡10ç§é’ˆå¯¹LLMæ‰§è¡Œæˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAï¼‰çš„æ–°æ–¹æ³•ã€‚ä¸ä¼ ç»Ÿä¾é å›ºå®šä½†éšæœºè®°å½•æˆ–æ¨¡å‹çš„MIAä¸åŒï¼Œè¿™äº›æ–¹æ³•å¤§å¤šåœ¨æ”¶é›†çš„åéªŒæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ã€‚ç”¨äºè¯„ä¼°MIAçš„æˆå‘˜å’Œéæˆå‘˜é›†æ˜¯åœ¨æ¨¡å‹å‘å¸ƒåé€šè¿‡æœ‰æ ¹æ®çš„çŒœæµ‹æ„å»ºçš„ã€‚è¿™ç§ç¼ºä¹éšæœºæ€§å¼•å‘äº†æˆå‘˜ä¸éæˆå‘˜ä¹‹é—´åˆ†å¸ƒè½¬ç§»çš„æ‹…å¿§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.17975v3">PDF</a> IEEE Conference on Secure and Trustworthy Machine Learning (SaTML   2025)</p>
<p><strong>Summary</strong></p>
<p>LLMsé¢ä¸´æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAsï¼‰çš„ç ”ç©¶è¿…é€Ÿå¢é•¿ï¼Œæ–°çš„æ”»å‡»æ–¹æ³•ä¸æ–­è¢«æå‡ºã€‚è¿™äº›æ–°æ–¹æ³•ä¸»è¦åœ¨äº‹åæ”¶é›†çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œå­˜åœ¨æˆå‘˜ä¸éæˆå‘˜ä¹‹é—´çš„åˆ†å¸ƒåç§»é—®é¢˜ã€‚æœ¬æ–‡å¯¹LLMsçš„MIAsæ–‡çŒ®è¿›è¡Œå…¨é¢å›é¡¾ï¼Œå‘ç°å­˜åœ¨ä¸€ç³»åˆ—ç›®æ ‡æ¨¡å‹ã€åŠ¨æœºå’Œæ„Ÿå…´è¶£å•ä½ã€‚é€šè¿‡æ¨¡å‹æ— å…³çš„è¯æ±‡åˆ†ç±»å™¨é‡åŒ–å…­ä¸ªæ•°æ®é›†çš„åˆ†å¸ƒåç§»ï¼Œæ˜¾ç¤ºæ‰€æœ‰äº‹åæ„å»ºçš„æ•°æ®é›†éƒ½å­˜åœ¨å¼ºçƒˆçš„åˆ†å¸ƒåç§»ã€‚è¿™äº›åç§»å¯¹LLMsåœ¨ç°å®åœºæ™¯ä¸­çš„è®°å¿†èƒ½åŠ›æå‡ºäº†è´¨ç–‘ï¼Œä¹Ÿå¯èƒ½ä½¿åŸºäºè¿™äº›æ•°æ®é›†çš„æ–¹æ³•è®ºè´¡çŒ®æ— æ•ˆã€‚æœ¬æ–‡è®¨è®ºäº†æ­£ç¡®è¯„ä¼°LLMsçš„MIAsçš„é‡è¦è€ƒè™‘å› ç´ ï¼Œå¹¶æå‡ºäº†å‰è¿›çš„æ½œåœ¨æ–¹å¼ï¼ŒåŒ…æ‹¬éšæœºæµ‹è¯•åˆ†å‰²ã€æ³¨å…¥éšæœºï¼ˆå”¯ä¸€ï¼‰åºåˆ—ã€éšæœºå¾®è°ƒç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsé¢ä¸´æˆå‘˜æ¨ç†æ”»å‡»ï¼ˆMIAsï¼‰çš„ç ”ç©¶è¿…é€Ÿå¢é•¿ï¼Œå·²æå‡ºè¶…è¿‡10ç§æ–°çš„æ”»å‡»æ–¹æ³•ã€‚</li>
<li>è¿™äº›æ–°æ–¹æ³•ä¸»è¦åœ¨äº‹åæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ï¼Œå­˜åœ¨åˆ†å¸ƒåç§»é—®é¢˜ã€‚</li>
<li>å¤§å¤šæ•°ç ”ç©¶å…³æ³¨åºåˆ—çº§åˆ«çš„MIAså’Œäº‹åè¯„ä¼°è®¾ç½®ï¼Œä½†ç›®æ ‡æ¨¡å‹ã€åŠ¨æœºå’Œæ„Ÿå…´è¶£å•ä½å­˜åœ¨å¤šç§é€‰æ‹©ã€‚</li>
<li>é€šè¿‡æ¨¡å‹æ— å…³çš„è¯æ±‡åˆ†ç±»å™¨é‡åŒ–å…­ä¸ªæ•°æ®é›†çš„åˆ†å¸ƒåç§»ï¼Œæ˜¾ç¤ºæ‰€æœ‰æ•°æ®é›†éƒ½å­˜åœ¨å¼ºçƒˆçš„åˆ†å¸ƒåç§»ã€‚</li>
<li>åˆ†å¸ƒåç§»å¯¹LLMsåœ¨ç°å®åœºæ™¯ä¸­çš„è®°å¿†èƒ½åŠ›æå‡ºè´¨ç–‘ï¼Œä¹Ÿå¯èƒ½ä½¿åŸºäºè¿™äº›æ•°æ®é›†çš„æ–¹æ³•è®ºæ— æ•ˆã€‚</li>
<li>æœ¬æ–‡è®¨è®ºäº†æ­£ç¡®è¯„ä¼°LLMsçš„MIAsçš„é‡è¦å› ç´ ï¼Œå¹¶æå‡ºäº†æ”¹è¿›æ–¹å‘ï¼ŒåŒ…æ‹¬éšæœºæµ‹è¯•åˆ†å‰²ã€æ³¨å…¥éšæœºåºåˆ—ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.17975">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0ba3af5e4abf2c3bbfa860feaaa15f5e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2d6760fdf521774eb95254f2ee44df54.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bfdfed67f2b63d170c55ca7a38f21c47.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Efficient-Evolutionary-Search-Over-Chemical-Space-with-Large-Language-Models"><a href="#Efficient-Evolutionary-Search-Over-Chemical-Space-with-Large-Language-Models" class="headerlink" title="Efficient Evolutionary Search Over Chemical Space with Large Language   Models"></a>Efficient Evolutionary Search Over Chemical Space with Large Language   Models</h2><p><strong>Authors:Haorui Wang, Marta Skreta, Cher-Tian Ser, Wenhao Gao, Lingkai Kong, Felix Strieth-Kalthoff, Chenru Duan, Yuchen Zhuang, Yue Yu, Yanqiao Zhu, Yuanqi Du, AlÃ¡n Aspuru-Guzik, Kirill Neklyudov, Chao Zhang</strong></p>
<p>Molecular discovery, when formulated as an optimization problem, presents significant computational challenges because optimization objectives can be non-differentiable. Evolutionary Algorithms (EAs), often used to optimize black-box objectives in molecular discovery, traverse chemical space by performing random mutations and crossovers, leading to a large number of expensive objective evaluations. In this work, we ameliorate this shortcoming by incorporating chemistry-aware Large Language Models (LLMs) into EAs. Namely, we redesign crossover and mutation operations in EAs using LLMs trained on large corpora of chemical information. We perform extensive empirical studies on both commercial and open-source models on multiple tasks involving property optimization, molecular rediscovery, and structure-based drug design, demonstrating that the joint usage of LLMs with EAs yields superior performance over all baseline models across single- and multi-objective settings. We demonstrate that our algorithm improves both the quality of the final solution and convergence speed, thereby reducing the number of required objective evaluations. Our code is available at <a target="_blank" rel="noopener" href="http://github.com/zoom-wang112358/MOLLEO">http://github.com/zoom-wang112358/MOLLEO</a> </p>
<blockquote>
<p>åˆ†å­å‘ç°çš„ä¼˜åŒ–é—®é¢˜å¸¦æ¥äº†å·¨å¤§çš„è®¡ç®—æŒ‘æˆ˜ï¼Œå› ä¸ºä¼˜åŒ–ç›®æ ‡å¯èƒ½æ˜¯ä¸å¯å¾®åˆ†çš„ã€‚è¿›åŒ–ç®—æ³•ï¼ˆEAï¼‰å¸¸ç”¨äºä¼˜åŒ–åˆ†å­å‘ç°ä¸­çš„é»‘ç›’ç›®æ ‡ï¼Œå®ƒé€šè¿‡æ‰§è¡Œéšæœºçªå˜å’Œäº¤å‰æ“ä½œæ¥éå†åŒ–å­¦ç©ºé—´ï¼Œä»è€Œå¯¼è‡´å¤§é‡çš„ç›®æ ‡è¯„ä¼°æˆæœ¬é«˜æ˜‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å°†å…·æœ‰åŒ–å­¦æ„ŸçŸ¥èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çº³å…¥è¿›åŒ–ç®—æ³•æ¥ç¼“è§£è¿™ä¸€ç¼ºç‚¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨åœ¨å¤§é‡åŒ–å­¦ä¿¡æ¯è¯­æ–™åº“ä¸Šè®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹é‡æ–°è®¾è®¡è¿›åŒ–ç®—æ³•ä¸­çš„äº¤å‰å’Œçªå˜æ“ä½œã€‚æˆ‘ä»¬åœ¨æ¶‰åŠå±æ€§ä¼˜åŒ–ã€åˆ†å­å†å‘ç°å’ŒåŸºäºç»“æ„çš„è¯ç‰©è®¾è®¡ç­‰å¤šä¸ªä»»åŠ¡ä¸Šï¼Œå¯¹å•†ä¸šå’Œå¼€æºæ¨¡å‹è¿›è¡Œäº†å¹¿æ³›çš„å®è¯ç ”ç©¶ï¼Œè¯æ˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸è¿›åŒ–ç®—æ³•çš„è”åˆä½¿ç”¨åœ¨å•ç›®æ ‡å’Œå¤šç›®æ ‡è®¾ç½®ä¸­éƒ½ä¼˜äºæ‰€æœ‰åŸºçº¿æ¨¡å‹ã€‚æˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„ç®—æ³•æé«˜äº†æœ€ç»ˆè§£å†³æ–¹æ¡ˆçš„è´¨é‡å’Œæ”¶æ•›é€Ÿåº¦ï¼Œä»è€Œå‡å°‘äº†æ‰€éœ€çš„ç›®æ ‡è¯„ä¼°æ¬¡æ•°ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="http://github.com/zoom-wang112358/MOLLEO%E6%89%BE%E5%88%B0%E3%80%82">http://github.com/zoom-wang112358/MOLLEOæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.16976v3">PDF</a> Published in ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èå…¥è¿›åŒ–ç®—æ³•ï¼ˆEAsï¼‰æ¥è§£å†³åˆ†å­å‘ç°ä¸­çš„ä¼˜åŒ–é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨LLMså¤„ç†åŒ–å­¦ä¿¡æ¯çš„èƒ½åŠ›ï¼Œé‡æ–°è®¾è®¡äº†EAä¸­çš„äº¤å‰å’Œçªå˜æ“ä½œã€‚åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†å®è¯ç ”ç©¶ï¼Œè¯æ˜è”åˆä½¿ç”¨LLMså’ŒEAsçš„æ–¹æ³•åœ¨å•ç›®æ ‡å’Œå¤šç›®æ ‡è®¾ç½®ä¸‹å‡ä¼˜äºæ‰€æœ‰åŸºçº¿æ¨¡å‹ï¼Œæé«˜äº†æœ€ç»ˆè§£å†³æ–¹æ¡ˆçš„è´¨é‡å’Œæ”¶æ•›é€Ÿåº¦ï¼Œå‡å°‘äº†æ‰€éœ€çš„ç›®æ ‡è¯„ä¼°æ¬¡æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¢«èå…¥è¿›åŒ–ç®—æ³•ï¼ˆEAsï¼‰ï¼Œä»¥è§£å†³åˆ†å­å‘ç°ä¸­çš„ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>LLMsç”¨äºé‡æ–°è®¾è®¡EAä¸­çš„äº¤å‰å’Œçªå˜æ“ä½œã€‚</li>
<li>å®è¯ç ”ç©¶è¯æ˜äº†LLMså’ŒEAsè”åˆä½¿ç”¨åœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†æœ€ç»ˆè§£å†³æ–¹æ¡ˆçš„è´¨é‡å’Œæ”¶æ•›é€Ÿåº¦ã€‚</li>
<li>è¯¥æ–¹æ³•å‡å°‘äº†æ‰€éœ€çš„ç›®æ ‡è¯„ä¼°æ¬¡æ•°ã€‚</li>
<li>è¯¥ç®—æ³•åœ¨å•ç›®æ ‡å’Œå¤šç›®æ ‡è®¾ç½®ä¸‹å‡è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.16976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6babb78a951a372471e38a9db61ebe31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6edbcc1f355b94155e5bcbcad7211c72.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="VoCo-LLaMA-Towards-Vision-Compression-with-Large-Language-Models"><a href="#VoCo-LLaMA-Towards-Vision-Compression-with-Large-Language-Models" class="headerlink" title="VoCo-LLaMA: Towards Vision Compression with Large Language Models"></a>VoCo-LLaMA: Towards Vision Compression with Large Language Models</h2><p><strong>Authors:Xubing Ye, Yukang Gan, Xiaoke Huang, Yixiao Ge, Yansong Tang</strong></p>
<p>Vision-Language Models (VLMs) have achieved remarkable success in various multi-modal tasks, but they are often bottlenecked by the limited context window and high computational cost of processing high-resolution image inputs and videos. Vision compression can alleviate this problem by reducing the vision token count. Previous approaches compress vision tokens with external modules and force LLMs to understand the compressed ones, leading to visual information loss. However, the LLMsâ€™ understanding paradigm of vision tokens is not fully utilised in the compression learning process. We propose VoCo-LLaMA, the first approach to compress vision tokens using LLMs. By introducing Vision Compression tokens during the vision instruction tuning phase and leveraging attention distillation, our method distill how LLMs comprehend vision tokens into their processing of VoCo tokens. VoCo-LLaMA facilitates effective vision compression and improves the computational efficiency during the inference stage. Specifically, our method achieves minimal performance loss with a compression ratio of 576$\times$, resulting in up to 94.8$%$ fewer FLOPs and 69.6$%$ acceleration in inference time. Furthermore, through continuous training using time-series compressed token sequences of video frames, VoCo-LLaMA demonstrates the ability to understand temporal correlations, outperforming previous methods on popular video question-answering benchmarks. Our approach presents a promising way to unlock the full potential of VLMsâ€™ contextual window, enabling more scalable multi-modal applications. The project page, along with the associated code, can be accessed via <a target="_blank" rel="noopener" href="https://yxxxb.github.io/VoCo-LLaMA-page/">https://yxxxb.github.io/VoCo-LLaMA-page/</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å„ç§å¤šæ¨¡æ€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å®ƒä»¬å¸¸å¸¸å—åˆ°å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒè¾“å…¥å’Œè§†é¢‘æ—¶æœ‰é™ä¸Šä¸‹æ–‡çª—å£å’Œé«˜è®¡ç®—æˆæœ¬çš„é™åˆ¶ã€‚è§†è§‰å‹ç¼©å¯ä»¥é€šè¿‡å‡å°‘è§†è§‰ä»¤ç‰Œæ•°é‡æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚ä»¥å‰çš„æ–¹æ³•ä½¿ç”¨å¤–éƒ¨æ¨¡å—å‹ç¼©è§†è§‰ä»¤ç‰Œï¼Œå¹¶å¼ºåˆ¶LLMsç†è§£å‹ç¼©åçš„ä»¤ç‰Œï¼Œå¯¼è‡´è§†è§‰ä¿¡æ¯ä¸¢å¤±ã€‚ç„¶è€Œï¼ŒLLMså¯¹è§†è§‰ä»¤ç‰Œçš„ç†è§£æ¨¡å¼åœ¨å‹ç¼©å­¦ä¹ è¿‡ç¨‹ä¸­æ²¡æœ‰å¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†VoCo-LLaMAï¼Œè¿™æ˜¯ä½¿ç”¨LLMså‹ç¼©è§†è§‰ä»¤ç‰Œçš„ç¬¬ä¸€ç§æ–¹æ³•ã€‚é€šè¿‡åœ¨è§†è§‰æŒ‡ä»¤è°ƒæ•´é˜¶æ®µå¼•å…¥è§†è§‰å‹ç¼©ä»¤ç‰Œï¼Œå¹¶åˆ©ç”¨æ³¨æ„åŠ›è’¸é¦ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†LLMså¦‚ä½•ç†è§£è§†è§‰ä»¤ç‰Œè½¬åŒ–ä¸ºå®ƒä»¬å¯¹VoCoä»¤ç‰Œçš„å¤„ç†ã€‚VoCo-LLaMAä¿ƒè¿›äº†æœ‰æ•ˆçš„è§†è§‰å‹ç¼©ï¼Œå¹¶æé«˜äº†æ¨ç†é˜¶æ®µçš„è®¡ç®—æ•ˆç‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»¥576å€çš„å‹ç¼©æ¯”å®ç°äº†æœ€å°çš„æ€§èƒ½æŸå¤±ï¼Œå¯¼è‡´FLOPså‡å°‘é«˜è¾¾94.8%ï¼Œæ¨ç†æ—¶é—´åŠ é€Ÿ69.6%ã€‚æ­¤å¤–ï¼Œé€šè¿‡è¿ç»­ä½¿ç”¨æ—¶é—´åºåˆ—å‹ç¼©ä»¤ç‰Œåºåˆ—çš„è§†é¢‘å¸§ï¼ŒVoCo-LLaMAå±•ç¤ºäº†ç†è§£æ—¶é—´å…³è”çš„èƒ½åŠ›ï¼Œåœ¨æµè¡Œçš„è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºè§£é”VLMsä¸Šä¸‹æ–‡çª—å£çš„æ½œåŠ›æä¾›äº†ä¸€ç§æœ‰å‰é€”çš„æ–¹å¼ï¼Œä½¿æ›´å¤šå¯æ‰©å±•çš„å¤šæ¨¡æ€åº”ç”¨ç¨‹åºæˆä¸ºå¯èƒ½ã€‚é¡¹ç›®é¡µé¢ä»¥åŠç›¸å…³ä»£ç å¯ä»¥é€šè¿‡<a target="_blank" rel="noopener" href="https://yxxxb.github.io/VoCo-LLaMA-page/%E8%BF%BD%E9T%E%%E4%BB%A5%E4%B8%8A%E5%86%85%E5%AE%B9%E6%98%AF%E5%85%B3%E4%BA%8E%E5%AF%B9%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88VLMs%EF%BC%89%E7%9A%84%E4%B8%80%E7%A7%8D%E6%96%B0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94VoCo-LLaMA%E7%9A%84%E4%BB%8B%E7%BB%8D%EF%BC%8C%E8%AF%A5%E6%96%B9%E6%B3%95%E5%88%A9%E7%94%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLMs%EF%BC%89%E6%9D%A5%E8%BF%9B%E8%A1%8C%E8%A7%86%E8%A7%89%E5%8E%8B%E7%BC%A9%E4%BB%A5%E6%8F%90%E5%8D%87%E6%95%88%E7%8E%87%E3%80%81%E5%8E%8B%E7%BC%A9%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E5%86%97%E4%BD%99%E4%B8%8E%E8%80%97%E6%97%B6%E7%9A%84%E6%96%B9%E9%9D%A2%E3%80%81%E5%87%8F%E8%BD%BB%E5%AF%B9%E4%BA%8E%E5%A4%A7%E9%87%8F%E5%8F%82%E6%95%B0%E9%9C%80%E6%B1%82%E4%BA%A7%E7%94%9F%E7%9A%84%E8%AE%A1%E7%AE%97%E8%B4%9F%E6%8B%85%E5%B9%B6%E6%8F%90%E9%AB%98%E5%87%86%E7%A1%AE%E6%80%A7%E7%AD%89%E6%96%B9%E9%9D%A2%E8%8E%B7%E5%BE%97%E9%87%8D%E8%A6%81%E7%AA%81%E7%A0%B4%E3%80%82">https://yxxxb.github.io/VoCo-LLaMA-page/è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.12275v2">PDF</a> 11 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œè§†è§‰å‹ç¼©çš„æ–°æ–¹æ³•â€”â€”VoCo-LLaMAã€‚è¯¥æ–¹æ³•å¼•å…¥è§†è§‰å‹ç¼©ä»¤ç‰Œï¼Œåœ¨è§†è§‰æŒ‡ä»¤å¾®è°ƒé˜¶æ®µåˆ©ç”¨æ³¨æ„åŠ›è’¸é¦ï¼Œå°†LLMså¯¹è§†è§‰ä»¤ç‰Œçš„ç†è§£è½¬åŒ–ä¸ºå¯¹VoCoä»¤ç‰Œçš„å¤„ç†ã€‚VoCo-LLaMAå®ç°äº†æœ‰æ•ˆçš„è§†è§‰å‹ç¼©ï¼Œæé«˜äº†æ¨ç†é˜¶æ®µçš„è®¡ç®—æ•ˆç‡ï¼Œå®ç°äº†é«˜å‹ç¼©æ¯”å’Œä½æ€§èƒ½æŸå¤±ã€‚æ­¤å¤–ï¼Œé€šè¿‡è¿ç»­è®­ç»ƒä½¿ç”¨è§†é¢‘å¸§çš„æ—¶é—´åºåˆ—å‹ç¼©ä»¤ç‰Œåºåˆ—ï¼ŒVoCo-LLaMAå±•ç°å‡ºç†è§£æ—¶é—´å…³è”çš„èƒ½åŠ›ï¼Œå¹¶åœ¨æµè¡Œçš„è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè¶…è¶Šå…ˆå‰æ–¹æ³•çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒå’Œè§†é¢‘æ—¶é¢ä¸´ä¸Šä¸‹æ–‡çª—å£æœ‰é™å’Œè®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚</li>
<li>è§†è§‰å‹ç¼©å¯ä»¥ç¼“è§£è¿™äº›é—®é¢˜ï¼Œä½†å…ˆå‰çš„å‹ç¼©æ–¹æ³•å¯¼è‡´è§†è§‰ä¿¡æ¯æŸå¤±ã€‚</li>
<li>VoCo-LLaMAæ˜¯é¦–ä¸ªåˆ©ç”¨LLMsè¿›è¡Œè§†è§‰å‹ç¼©çš„æ–¹æ³•ã€‚</li>
<li>VoCo-LLaMAé€šè¿‡å¼•å…¥è§†è§‰å‹ç¼©ä»¤ç‰Œå’Œæ³¨æ„åŠ›è’¸é¦ï¼Œå°†LLMså¯¹è§†è§‰ä»¤ç‰Œçš„ç†è§£è½¬åŒ–ä¸ºå¤„ç†VoCoä»¤ç‰Œã€‚</li>
<li>VoCo-LLaMAå®ç°äº†é«˜å‹ç¼©æ¯”å’Œä½æ€§èƒ½æŸå¤±ï¼Œæ¨ç†é˜¶æ®µè®¡ç®—æ•ˆç‡æ˜¾è‘—æé«˜ã€‚</li>
<li>é€šè¿‡è¿ç»­è®­ç»ƒä½¿ç”¨è§†é¢‘å¸§çš„æ—¶é—´åºåˆ—å‹ç¼©ä»¤ç‰Œåºåˆ—ï¼ŒVoCo-LLaMAå±•ç°å‡ºç†è§£æ—¶é—´å…³è”çš„èƒ½åŠ›ã€‚</li>
<li>VoCo-LLaMAåœ¨è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå…ˆå‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.12275">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4d55abe1b0567abfc73570959167faf6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-96422a866cc25660d3e4522501d94afe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc41a0abf033bab665bfe4d81dd7dd13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-823a328fcc819653641f37712b73944f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fc191013c4d17bdae0c39344a786ad4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fa6eae18cccbde8f9eb65b9f98eb9c0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Mixture-of-Subspaces-in-Low-Rank-Adaptation"><a href="#Mixture-of-Subspaces-in-Low-Rank-Adaptation" class="headerlink" title="Mixture-of-Subspaces in Low-Rank Adaptation"></a>Mixture-of-Subspaces in Low-Rank Adaptation</h2><p><strong>Authors:Taiqiang Wu, Jiahao Wang, Zhe Zhao, Ngai Wong</strong></p>
<p>In this paper, we introduce a subspace-inspired Low-Rank Adaptation (LoRA) method, which is computationally efficient, easy to implement, and readily applicable to large language, multimodal, and diffusion models. Initially, we equivalently decompose the weights of LoRA into two subspaces, and find that simply mixing them can enhance performance. To study such a phenomenon, we revisit it through a fine-grained subspace lens, showing that such modification is equivalent to employing a fixed mixer to fuse the subspaces. To be more flexible, we jointly learn the mixer with the original LoRA weights, and term the method Mixture-of-Subspaces LoRA (MoSLoRA). MoSLoRA consistently outperforms LoRA on tasks in different modalities, including commonsense reasoning, visual instruction tuning, and subject-driven text-to-image generation, demonstrating its effectiveness and robustness. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/wutaiqiang/MoSLoRA">https://github.com/wutaiqiang/MoSLoRA</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å—å­ç©ºé—´å¯å‘çš„ä½ç§©é€‚é…ï¼ˆLoRAï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è®¡ç®—æ•ˆç‡é«˜ï¼Œæ˜“äºå®ç°ï¼Œå¯è½»æ¾åº”ç”¨äºå¤§å‹è¯­è¨€ã€å¤šæ¨¡æ€å’Œæ‰©æ•£æ¨¡å‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†LoRAçš„æƒé‡ç­‰ä»·åœ°åˆ†è§£ä¸ºä¸¤ä¸ªå­ç©ºé—´ï¼Œå¹¶å‘ç°ç®€å•åœ°æ··åˆå®ƒä»¬å¯ä»¥å¢å¼ºæ€§èƒ½ã€‚ä¸ºäº†ç ”ç©¶è¿™ä¸€ç°è±¡ï¼Œæˆ‘ä»¬é€šè¿‡ç²¾ç»†çš„å­ç©ºé—´é€é•œé‡æ–°è€ƒå¯Ÿï¼Œè¡¨æ˜è¿™ç§ä¿®æ”¹ç›¸å½“äºä½¿ç”¨å›ºå®šçš„æ··åˆå™¨æ¥èåˆå­ç©ºé—´ã€‚ä¸ºäº†æ›´çµæ´»ï¼Œæˆ‘ä»¬ä¸åŸå§‹LoRAæƒé‡ä¸€èµ·å­¦ä¹ æ··åˆå™¨ï¼Œå¹¶å°†è¯¥æ–¹æ³•ç§°ä¸ºâ€œå­ç©ºé—´æ··åˆLoRAï¼ˆMoSLoRAï¼‰â€ã€‚MoSLoRAåœ¨ä¸åŒæ¨¡æ€çš„ä»»åŠ¡ä¸Šå‡ä¼˜äºLoRAï¼ŒåŒ…æ‹¬å¸¸è¯†æ¨ç†ã€è§†è§‰æŒ‡ä»¤è°ƒæ•´å’Œä¸»é¢˜é©±åŠ¨çš„æ–‡å­—åˆ°å›¾åƒç”Ÿæˆï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/wutaiqiang/MoSLoRA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/wutaiqiang/MoSLoRAè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.11909v4">PDF</a> EMNLP 2024 Main, Oral</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å—å­ç©ºé—´å¯å‘çš„ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è®¡ç®—æ•ˆç‡é«˜ã€æ˜“äºå®ç°ï¼Œå¯å¹¿æ³›åº”ç”¨äºå¤§å‹è¯­è¨€ã€å¤šæ¨¡æ€å’Œæ‰©æ•£æ¨¡å‹ã€‚é€šè¿‡å¯¹LoRAæƒé‡è¿›è¡Œç­‰æ•ˆåˆ†è§£ï¼Œå¹¶å¼•å…¥æ··åˆæŠ€æœ¯ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMoSLoRAçš„æ··åˆå­ç©ºé—´LoRAæ–¹æ³•ã€‚MoSLoRAåœ¨ä¸åŒæ¨¡æ€çš„ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬å¸¸è¯†æ¨ç†ã€è§†è§‰æŒ‡ä»¤è°ƒæ•´å’Œä¸»é¢˜é©±åŠ¨çš„æ–‡å­—å›¾åƒç”Ÿæˆï¼Œå‡¸æ˜¾å…¶æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ–¹æ³•ï¼Œé€‚ç”¨äºå¤§å‹è¯­è¨€ã€å¤šæ¨¡æ€å’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>é€šè¿‡ç­‰æ•ˆåˆ†è§£LoRAæƒé‡ï¼Œå‘ç°äº†æ··åˆæŠ€æœ¯èƒ½å¢å¼ºæ€§èƒ½ã€‚</li>
<li>MoSLoRAæ–¹æ³•é€šè¿‡å¼•å…¥å­ç©ºé—´æ··åˆæ¦‚å¿µï¼Œå¯¹LoRAè¿›è¡Œäº†æ”¹è¿›ã€‚</li>
<li>MoSLoRAåœ¨å¸¸è¯†æ¨ç†ã€è§†è§‰æŒ‡ä»¤è°ƒæ•´å’Œæ–‡æœ¬å›¾åƒç”Ÿæˆç­‰ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>MoSLoRAæ–¹æ³•å…·æœ‰è®¡ç®—æ•ˆç‡é«˜å’Œæ˜“äºå®ç°çš„ç‰¹ç‚¹ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼ŒMoSLoRAæ–¹æ³•èƒ½æé«˜æ¨¡å‹çš„çµæ´»æ€§å’Œæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.11909">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dfed933bdc8d34b5d8b61ca51ba771ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5f6061723325ed88888410170cd452a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3368db1fb1a0b75869f5852f8c9ad186.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f7af46f790d58bf801e5aa0710f1d23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4b426ae941893be77f84865843862ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-57ea34c42224a2c7f987bd00b8d5a7bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2a3f7268d6e635b2b548c6d879aa450.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78e569ff3a94cd3bb0325d0d95cd4ebf.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-11/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-11/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-11/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-818ed1ec7c1943f86c1bb77eea7ec35c.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-11  A Survey of Large Language Model Empowered Agents for Recommendation and   Search Towards Next-Generation Information Retrieval
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-11/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-94055dde32c30802ccdd214b8a84c796.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-11  Symbolic Mixture-of-Experts Adaptive Skill-based Routing for   Heterogeneous Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26384.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
