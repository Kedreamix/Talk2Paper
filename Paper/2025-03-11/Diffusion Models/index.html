<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-03-11  AIM-Fair Advancing Algorithmic Fairness via Selectively Fine-Tuning   Biased Models with Contextual Synthetic Data">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-63e680a30df0d6460edaab10aa63b1e9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    31 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-11-更新"><a href="#2025-03-11-更新" class="headerlink" title="2025-03-11 更新"></a>2025-03-11 更新</h1><h2 id="AIM-Fair-Advancing-Algorithmic-Fairness-via-Selectively-Fine-Tuning-Biased-Models-with-Contextual-Synthetic-Data"><a href="#AIM-Fair-Advancing-Algorithmic-Fairness-via-Selectively-Fine-Tuning-Biased-Models-with-Contextual-Synthetic-Data" class="headerlink" title="AIM-Fair: Advancing Algorithmic Fairness via Selectively Fine-Tuning   Biased Models with Contextual Synthetic Data"></a>AIM-Fair: Advancing Algorithmic Fairness via Selectively Fine-Tuning   Biased Models with Contextual Synthetic Data</h2><p><strong>Authors:Zengqun Zhao, Ziquan Liu, Yu Cao, Shaogang Gong, Ioannis Patras</strong></p>
<p>Recent advances in generative models have sparked research on improving model fairness with AI-generated data. However, existing methods often face limitations in the diversity and quality of synthetic data, leading to compromised fairness and overall model accuracy. Moreover, many approaches rely on the availability of demographic group labels, which are often costly to annotate. This paper proposes AIM-Fair, aiming to overcome these limitations and harness the potential of cutting-edge generative models in promoting algorithmic fairness. We investigate a fine-tuning paradigm starting from a biased model initially trained on real-world data without demographic annotations. This model is then fine-tuned using unbiased synthetic data generated by a state-of-the-art diffusion model to improve its fairness. Two key challenges are identified in this fine-tuning paradigm, 1) the low quality of synthetic data, which can still happen even with advanced generative models, and 2) the domain and bias gap between real and synthetic data. To address the limitation of synthetic data quality, we propose Contextual Synthetic Data Generation (CSDG) to generate data using a text-to-image diffusion model (T2I) with prompts generated by a context-aware LLM, ensuring both data diversity and control of bias in synthetic data. To resolve domain and bias shifts, we introduce a novel selective fine-tuning scheme in which only model parameters more sensitive to bias and less sensitive to domain shift are updated. Experiments on CelebA and UTKFace datasets show that our AIM-Fair improves model fairness while maintaining utility, outperforming both fully and partially fine-tuned approaches to model fairness. </p>
<blockquote>
<p>近期生成模型领域的进展激发了关于使用AI生成数据提高模型公平性的研究。然而，现有方法通常在合成数据的多样性和质量方面存在局限，导致公平性和整体模型准确度受损。此外，许多方法依赖于人口统计群体标签的可用性，这些标签的标注成本往往很高。本文针对这些局限性，提出了一种名为AIM-Fair的方法，旨在利用最前沿生成模型的潜力来促进算法公平性。研究了一种微调模式，从最初在真实世界数据上训练的带有偏见的模型开始，该模型没有人口统计注释。然后，使用由最先进扩散模型生成的公正合成数据对此模型进行微调，以提高其公平性。在此微调模式中确定了两个关键挑战：1）即使使用先进的生成模型，合成数据的质量仍然可能很低；2）真实数据和合成数据之间的领域和偏见差距。为了解决合成数据质量的局限性，我们提出了基于上下文合成数据生成（CSDG）的方法，使用文本到图像扩散模型（T2I）生成数据，并通过上下文感知LLM生成提示，确保合成数据的多样性和偏见的控制。为了解决领域和偏见转移问题，我们引入了一种新型选择性微调方案，其中只更新对偏见更敏感并且对领域转移不太敏感的部分模型参数。在CelebA和UTKFace数据集上的实验表明，我们的AIM-Fair方法在维持效用的同时提高了模型的公平性，并且优于完全和部分微调模型公平性的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05665v1">PDF</a> Accepted at CVPR 2025. Github:   <a target="_blank" rel="noopener" href="https://github.com/zengqunzhao/AIM-Fair">https://github.com/zengqunzhao/AIM-Fair</a>. Project page:   <a target="_blank" rel="noopener" href="https://zengqunzhao.github.io/AIMFair">https://zengqunzhao.github.io/AIMFair</a></p>
<p><strong>Summary</strong></p>
<p>随着生成模型的最新发展，提高模型公平性的研究愈发重要。本文提出了AIM-Fair方法，旨在克服现有方法的局限性并充分利用先进生成模型在促进算法公平性方面的潜力。该方法从一个基于现实数据但带有偏见的初始模型开始，然后使用由最先进的扩散模型生成的无偏见合成数据进行微调。为应对合成数据质量低的挑战，提出了基于文本到图像的扩散模型（T2I）与语境感知的大型语言模型（LLM）相结合生成数据的语境合成数据生成方法（CSDG），保证数据多样性和合成数据的偏见控制。同时采用选择性微调策略，仅更新对偏见敏感但对领域转移不敏感的模型参数来解决领域和偏见转移问题。实验表明，AIM-Fair在保持模型效用的同时提高了模型的公平性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成模型的进步引发了对提高模型公平性的研究关注。</li>
<li>现有方法面临合成数据多样性和质量上的局限性，影响模型的公平性和准确性。</li>
<li>提出AIM-Fair方法，旨在利用先进生成模型促进算法公平性。</li>
<li>使用合成数据微调初始模型以提高公平性。</li>
<li>针对合成数据质量低的问题，采用基于文本到图像的扩散模型和语境感知的大型语言模型的结合生成数据。</li>
<li>采用选择性微调策略来解决领域和偏见转移问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05665">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9b39ed088038284e55ca5dd28c7de98e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-65c2210db808626a9e11bd3b900e199d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-712052902098c6b158b315a910995673.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63e680a30df0d6460edaab10aa63b1e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96d6d0f74fb394c5ebf265e1bb4c2349.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Development-and-Enhancement-of-Text-to-Image-Diffusion-Models"><a href="#Development-and-Enhancement-of-Text-to-Image-Diffusion-Models" class="headerlink" title="Development and Enhancement of Text-to-Image Diffusion Models"></a>Development and Enhancement of Text-to-Image Diffusion Models</h2><p><strong>Authors:Rajdeep Roshan Sahu</strong></p>
<p>This research focuses on the development and enhancement of text-to-image denoising diffusion models, addressing key challenges such as limited sample diversity and training instability. By incorporating Classifier-Free Guidance (CFG) and Exponential Moving Average (EMA) techniques, this study significantly improves image quality, diversity, and stability. Utilizing Hugging Face’s state-of-the-art text-to-image generation model, the proposed enhancements establish new benchmarks in generative AI. This work explores the underlying principles of diffusion models, implements advanced strategies to overcome existing limitations, and presents a comprehensive evaluation of the improvements achieved. Results demonstrate substantial progress in generating stable, diverse, and high-quality images from textual descriptions, advancing the field of generative artificial intelligence and providing new foundations for future applications.   Keywords: Text-to-image, Diffusion model, Classifier-free guidance, Exponential moving average, Image generation. </p>
<blockquote>
<p>本文重点研究文本到图像降噪扩散模型的发展和改进，解决样本多样性有限和训练不稳定等关键挑战。通过融入无分类器引导（CFG）和指数移动平均（EMA）技术，该研究显著提高了图像质量、多样性和稳定性。利用Hugging Face最先进的文本到图像生成模型，所提出的改进在生成式人工智能领域建立了新的基准。这项工作探索了扩散模型的基本原理，实施了先进的策略来克服现有局限性，并全面评估了所取得的改进。结果证明，在从文本来生成稳定、多样且高质量的图像方面取得了实质性进展，为生成人工智能领域提供了新的基础和未来应用的可能性。关键词：文本到图像、扩散模型、无分类器引导、指数移动平均、图像生成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05149v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了文本到图像去噪扩散模型的发展和改进，通过融入Classifier-Free Guidance（CFG）和Exponential Moving Average（EMA）技术，提高了图像质量、多样性和稳定性。该研究利用Hugging Face的先进文本到图像生成模型，建立了生成式人工智能的新基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究聚焦于文本到图像去噪扩散模型的发展和改进。</li>
<li>借助Classifier-Free Guidance（CFG）和Exponential Moving Average（EMA）技术，克服了样本多样性有限和训练不稳定等关键挑战。</li>
<li>利用Hugging Face的先进文本到图像生成模型，实现了生成式人工智能的新突破。<br>4.该研究深入探讨了扩散模型的基本原理，实施了高级策略来克服现有局限性。</li>
<li>实现了对改进的全面评估，证明了在生成稳定、多样、高质量的图像方面的显著进展。<br>6.该研究为文本描述生成图像的技术提供了新基础，推动了生成人工智能领域的发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05149">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-601c70d2a0bad9e23fd0382c88f51def.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e6abb21d166c02469865f28fac48711.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0448854c8457bbfc151b39da988c50b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ac242194085047da87324718e476431.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a235c8094ee7eee4cc4966b24b48314.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Generating-Novel-Brain-Morphology-by-Deforming-Learned-Templates"><a href="#Generating-Novel-Brain-Morphology-by-Deforming-Learned-Templates" class="headerlink" title="Generating Novel Brain Morphology by Deforming Learned Templates"></a>Generating Novel Brain Morphology by Deforming Learned Templates</h2><p><strong>Authors:Alan Q. Wang, Fangrui Huang, Bailey Trang, Wei Peng, Mohammad Abbasi, Kilian Pohl, Mert Sabuncu, Ehsan Adeli</strong></p>
<p>Designing generative models for 3D structural brain MRI that synthesize morphologically-plausible and attribute-specific (e.g., age, sex, disease state) samples is an active area of research. Existing approaches based on frameworks like GANs or diffusion models synthesize the image directly, which may limit their ability to capture intricate morphological details. In this work, we propose a 3D brain MRI generation method based on state-of-the-art latent diffusion models (LDMs), called MorphLDM, that generates novel images by applying synthesized deformation fields to a learned template. Instead of using a reconstruction-based autoencoder (as in a typical LDM), our encoder outputs a latent embedding derived from both an image and a learned template that is itself the output of a template decoder; this latent is passed to a deformation field decoder, whose output is applied to the learned template. A registration loss is minimized between the original image and the deformed template with respect to the encoder and both decoders. Empirically, our approach outperforms generative baselines on metrics spanning image diversity, adherence with respect to input conditions, and voxel-based morphometry. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/alanqrwang/morphldm">https://github.com/alanqrwang/morphldm</a>. </p>
<blockquote>
<p>设计针对3D结构性脑MRI的生成模型，以合成形态上合理且具有特定属性（例如年龄、性别、疾病状态）的样本是一个研究热点。现有的基于GAN或扩散模型等框架的方法直接合成图像，这可能限制了它们捕捉复杂形态细节的能力。在这项工作中，我们提出了一种基于最先进的潜在扩散模型（LDMs）的3D脑MRI生成方法，称为MorphLDM。它通过应用合成变形场到一个学习到的模板来生成新的图像。我们的编码器输出的潜在嵌入来源于图像和学习到的模板（其本身也是模板解码器的输出），这个潜在嵌入被传递给变形场解码器，其输出被应用到学习到的模板上。通过最小化原始图像和变形模板之间的注册损失，同时优化编码器和两个解码器。经验上，我们的方法在图像多样性、符合输入条件的坚持性以及基于体素的形态测量等指标上的表现均超过了生成基线。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/alanqrwang/morphldm%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/alanqrwang/morphldm上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03778v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于潜在扩散模型（LDM）的3D脑MRI生成方法已成为研究热点。MorphLDM作为基于先进LDM的新方法，通过合成变形场对所学模板进行操作生成新的图像。与典型的LDM使用的重建式自编码器不同，MorphLDM的编码器输出的是来自图像和所学模板两者的潜在嵌入，此潜在嵌入被传递给变形场解码器，其输出应用于所学模板。经过最小化原始图像和变形模板之间的注册损失后，其表现超出现在的主要生成模型。代码已公开在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>研究焦点在于设计用于合成形态合理且属性特定的（如年龄、性别、疾病状态）3D结构脑MRI样本的生成模型。</li>
<li>当前方法基于潜在扩散模型（LDM）。</li>
<li>提出一种新方法MorphLDM，它通过合成变形场对所学模板进行操作来生成图像。</li>
<li>与典型LDM不同，MorphLDM的编码器输出结合了图像和模板信息的潜在嵌入。</li>
<li>使用变形场解码器处理潜在嵌入并应用于模板。</li>
<li>通过最小化注册损失来优化模型性能，该损失考虑了原始图像和变形模板之间的差异。</li>
<li>该方法在图像多样性、符合输入条件以及体素形态测量方面超越了现有生成模型的基准测试。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03778">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e75b20c191401cf5d0bfb5fe79cf755d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5592fa3279ad2dff6283f3ba0f8e793.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f1fb47401ef9a0efd5831d0c1822cc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c30907be9e88147d3a507b7df4033fca.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VISION-XL-High-Definition-Video-Inverse-Problem-Solver-using-Latent-Image-Diffusion-Models"><a href="#VISION-XL-High-Definition-Video-Inverse-Problem-Solver-using-Latent-Image-Diffusion-Models" class="headerlink" title="VISION-XL: High Definition Video Inverse Problem Solver using Latent   Image Diffusion Models"></a>VISION-XL: High Definition Video Inverse Problem Solver using Latent   Image Diffusion Models</h2><p><strong>Authors:Taesung Kwon, Jong Chul Ye</strong></p>
<p>In this paper, we propose a novel framework for solving high-definition video inverse problems using latent image diffusion models. Building on recent advancements in spatio-temporal optimization for video inverse problems using image diffusion models, our approach leverages latent-space diffusion models to achieve enhanced video quality and resolution. To address the high computational demands of processing high-resolution frames, we introduce a pseudo-batch consistent sampling strategy, allowing efficient operation on a single GPU. Additionally, to improve temporal consistency, we present pseudo-batch inversion, an initialization technique that incorporates informative latents from the measurement. By integrating with SDXL, our framework achieves state-of-the-art video reconstruction across a wide range of spatio-temporal inverse problems, including complex combinations of frame averaging and various spatial degradations, such as deblurring, super-resolution, and inpainting. Unlike previous methods, our approach supports multiple aspect ratios (landscape, vertical, and square) and delivers HD-resolution reconstructions (exceeding 1280x720) in under 6 seconds per frame on a single NVIDIA 4090 GPU. </p>
<blockquote>
<p>本文提出了一种基于潜在图像扩散模型解决高清视频逆问题的新型框架。我们的方法建立在最近基于图像扩散模型的视频逆问题时空优化的进展之上，利用潜在空间扩散模型实现增强视频质量和分辨率。为了解决处理高分辨率帧的高计算需求，我们引入了一种伪批量一致采样策略，该策略可在单个GPU上进行高效操作。此外，为了提高时间一致性，我们提出了伪批次反演初始化技术，该技术结合了测量中的信息潜在变量。通过与SDXL集成，我们的框架在广泛的时空逆问题中实现了最先进的视频重建效果，包括帧平均和各种空间退化的复杂组合，如去模糊、超分辨率和图像修复等。与之前的方法不同，我们的方法支持多种纵横比（横屏、竖屏和正方形），并在单个NVIDIA 4090 GPU上以每秒超过6帧的速度实现高清分辨率重建（超过1280x720）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00156v4">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://vision-xl.github.io/">https://vision-xl.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于潜在图像扩散模型的高清视频逆问题解决方案框架。该框架利用时空优化技术，通过潜在空间扩散模型增强视频质量和分辨率。为解决高分辨率帧的高计算需求，引入伪批次一致采样策略，可在单个GPU上实现高效操作。通过伪批次反转初始化技术，提高时间一致性。结合SDXL，该框架在广泛的时空逆问题中实现了最先进的视频重建效果，包括帧平均和各种空间退化如去模糊、超分辨率和修复等。与以前的方法不同，该方法支持多种纵横比，并在单个NVIDIA 4090 GPU上实现高清分辨率重建（超过1280x720），每帧处理时间不到6秒。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了基于潜在图像扩散模型的高清视频逆问题解决方案框架。</li>
<li>利用时空优化技术和潜在空间扩散模型增强视频质量和分辨率。</li>
<li>引入伪批次一致采样策略，满足高计算需求，实现高效操作。</li>
<li>通过伪批次反转初始化技术，提高时间一致性。</li>
<li>结合SDXL，在多种时空逆问题中实现最先进的视频重建效果。</li>
<li>支持多种纵横比的视频处理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00156">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-aeaad2b9ea679da0a5106718e83d14f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27773556491a35615cfba955d32f2339.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da7977b88fe9422f90125fcf30d2701f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c22b718224e9a0d69b92fd4ef97831f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0922418f7c44893e19bbe0bd8e3e8d5d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Gaussians-to-Life-Text-Driven-Animation-of-3D-Gaussian-Splatting-Scenes"><a href="#Gaussians-to-Life-Text-Driven-Animation-of-3D-Gaussian-Splatting-Scenes" class="headerlink" title="Gaussians-to-Life: Text-Driven Animation of 3D Gaussian Splatting Scenes"></a>Gaussians-to-Life: Text-Driven Animation of 3D Gaussian Splatting Scenes</h2><p><strong>Authors:Thomas Wimmer, Michael Oechsle, Michael Niemeyer, Federico Tombari</strong></p>
<p>State-of-the-art novel view synthesis methods achieve impressive results for multi-view captures of static 3D scenes. However, the reconstructed scenes still lack “liveliness,” a key component for creating engaging 3D experiences. Recently, novel video diffusion models generate realistic videos with complex motion and enable animations of 2D images, however they cannot naively be used to animate 3D scenes as they lack multi-view consistency. To breathe life into the static world, we propose Gaussians2Life, a method for animating parts of high-quality 3D scenes in a Gaussian Splatting representation. Our key idea is to leverage powerful video diffusion models as the generative component of our model and to combine these with a robust technique to lift 2D videos into meaningful 3D motion. We find that, in contrast to prior work, this enables realistic animations of complex, pre-existing 3D scenes and further enables the animation of a large variety of object classes, while related work is mostly focused on prior-based character animation, or single 3D objects. Our model enables the creation of consistent, immersive 3D experiences for arbitrary scenes. </p>
<blockquote>
<p>最新先进的新型视图合成方法对于静态三维场景的多视图捕获取得了令人印象深刻的结果。然而，重建的场景仍然缺乏“生动性”，这是创建吸引人的三维体验的关键要素。最近，新型视频扩散模型能够生成具有复杂运动的现实视频并为二维图像提供动画效果，但它们不能直接用于动画三维场景，因为缺乏多视图一致性。为了给静态世界注入生命力，我们提出了Gaussians2Life方法，这是一种以高斯拼贴表示法来动画高质量三维场景部分的方法。我们的核心思想是利用强大的视频扩散模型作为我们模型的生产组件，并将其与一种将二维视频提升到有意义的三维运动的技术相结合。我们发现，与之前的工作相比，这能够实现对复杂预存三维场景的逼真动画，并进一步实现对各种对象类别的动画，而相关工作主要集中在基于先验的角色动画或单个三维对象上。我们的模型能够为任意场景创建连贯、沉浸式的三维体验。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19233v2">PDF</a> Project website at <a target="_blank" rel="noopener" href="https://wimmerth.github.io/gaussians2life.html">https://wimmerth.github.io/gaussians2life.html</a>.   Accepted to 3DV 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为Gaussians2Life的方法，该方法利用视频扩散模型为高质量3D场景的部分内容注入活力。通过结合强大的视频扩散模型和将2D视频提升到有意义的3D运动的技术，实现了对复杂预存3D场景的真实动画渲染，并能对各种对象类别进行动画处理，与先前的工作相比，具有更大的灵活性和广泛的应用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前先进的新视图合成方法在静态3D场景的多视图捕捉方面取得了令人印象深刻的结果，但重建的场景缺乏”生动性”，这是创建吸引入的3D体验的关键要素。</li>
<li>新的视频扩散模型可以生成具有复杂运动的真实视频并使2D图像动画化，但它们不能简单地用于动画化3D场景，缺乏多视图一致性。</li>
<li>Gaussians2Life方法利用视频扩散模型为高质量3D场景注入活力，实现复杂预存3D场景的真实动画渲染。</li>
<li>Gaussians2Life方法能将2D视频提升到有意义的3D运动，使得对任意场景的3D体验更加一致和沉浸式。</li>
<li>该方法不仅适用于基于先验的角色动画，还适用于单个3D物体的动画，具有更广泛的应用范围。</li>
<li>与先前的工作相比，Gaussians2Life方法具有更大的灵活性，能够处理更广泛的物体类别和场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.19233">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2e475ccf8fd9504d0538d2590f16e2df.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-723416616b977c377bb0265a1cc72832.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20c7dc4bc514ddfeb79ab05e7c3150cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf27637164dd2d4c936c89cce762b3b6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Reward-Fine-Tuning-Two-Step-Diffusion-Models-via-Learning-Differentiable-Latent-Space-Surrogate-Reward"><a href="#Reward-Fine-Tuning-Two-Step-Diffusion-Models-via-Learning-Differentiable-Latent-Space-Surrogate-Reward" class="headerlink" title="Reward Fine-Tuning Two-Step Diffusion Models via Learning Differentiable   Latent-Space Surrogate Reward"></a>Reward Fine-Tuning Two-Step Diffusion Models via Learning Differentiable   Latent-Space Surrogate Reward</h2><p><strong>Authors:Zhiwei Jia, Yuesong Nan, Huixi Zhao, Gengdai Liu</strong></p>
<p>Recent research has shown that fine-tuning diffusion models (DMs) with arbitrary rewards, including non-differentiable ones, is feasible with reinforcement learning (RL) techniques, enabling flexible model alignment. However, applying existing RL methods to timestep-distilled DMs is challenging for ultra-fast ($\le2$-step) image generation. Our analysis suggests several limitations of policy-based RL methods such as PPO or DPO toward this goal. Based on the insights, we propose fine-tuning DMs with learned differentiable surrogate rewards. Our method, named LaSRO, learns surrogate reward models in the latent space of SDXL to convert arbitrary rewards into differentiable ones for efficient reward gradient guidance. LaSRO leverages pre-trained latent DMs for reward modeling and specifically targets image generation $\le2$ steps for reward optimization, enhancing generalizability and efficiency. LaSRO is effective and stable for improving ultra-fast image generation with different reward objectives, outperforming popular RL methods including PPO and DPO. We further show LaSRO’s connection to value-based RL, providing theoretical insights. See our webpage at <a target="_blank" rel="noopener" href="https://sites.google.com/view/lasro">https://sites.google.com/view/lasro</a>. </p>
<blockquote>
<p>最近的研究表明，利用强化学习（RL）技术对扩散模型（DMs）进行微调，接受任意奖励（包括不可区分的奖励）是可行的，从而实现灵活的模型对齐。然而，将现有强化学习方法应用于时间步蒸馏的扩散模型在超快速（≤2步）图像生成方面存在挑战。我们的分析指出了基于政策的强化学习方法（如PPO或DPO）在此目标上的局限性。基于这些见解，我们提出了利用学习到的可区分替代奖励对扩散模型进行微调的方法。我们的方法名为LaSRO，它在SDXL的潜在空间中学习替代奖励模型，将任意奖励转换为可区分的奖励，以实现有效的奖励梯度指导。LaSRO利用预训练的潜在扩散模型进行奖励建模，并针对奖励优化专门设定图像生成≤2步，从而提高通用性和效率。LaSRO在具有不同奖励目标的超快速图像生成方面非常有效且稳定，优于包括PPO和DPO在内的流行强化学习方法。我们还展示了LaSRO与价值型强化学习之间的联系，提供了理论见解。更多信息请访问我们的网页：<a target="_blank" rel="noopener" href="https://sites.google.com/view/lasro%E3%80%82">https://sites.google.com/view/lasro。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15247v2">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>扩散模型使用强化学习技术进行任意奖励的微调，具有灵活模型对齐的能力。然而，在超快速（≤2步）图像生成方面，应用现有的强化学习方法具有挑战性。为此，我们提出了使用可学习的可分化替代奖励来微调扩散模型的方法——LaSRO。它在SDXL的潜在空间中学习替代奖励模型，将任意奖励转化为可区分的奖励，提供有效的奖励梯度指导。LaSRO利用预训练的潜在扩散模型进行奖励建模，专门针对≤2步的图像生成进行奖励优化，提高了通用性和效率。对于不同的奖励目标，LaSRO在改进超快速图像生成方面表现出色，超越了包括PPO和DPO在内的流行强化学习方法。此外，我们还展示了LaSRO与价值型强化学习之间的联系，提供了理论见解。有关详细信息，请访问我们的网页：<a target="_blank" rel="noopener" href="https://sites.google.com/view/lasro">https://sites.google.com/view/lasro</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型可以通过强化学习技术微调，以灵活对齐模型，即使使用非可分化奖励也可实现。</li>
<li>在超快速图像生成（≤2步）方面，现有强化学习方法存在挑战。</li>
<li>LaSRO方法通过学习在潜在空间中的可分化替代奖励来解决这一问题，将任意奖励转化为可区分的奖励。</li>
<li>LaSRO利用预训练的潜在扩散模型进行奖励建模，优化图像生成的奖励目标。</li>
<li>LaSRO提高了图像生成的通用性和效率，在不同奖励目标下表现优异。</li>
<li>LaSRO超越了流行的强化学习方法，如PPO和DPO。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15247">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fdd0fed6a6a20425119369f2657a52df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2b950333397b41e1e602bcbeaddfcff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41ef8c50f349fb805342fc036cdf815d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0837f5a8f73c50b90868e0ffdb97766d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41a7480a3b723b68a599122339bae019.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Modification-Takes-Courage-Seamless-Image-Stitching-via-Reference-Driven-Inpainting"><a href="#Modification-Takes-Courage-Seamless-Image-Stitching-via-Reference-Driven-Inpainting" class="headerlink" title="Modification Takes Courage: Seamless Image Stitching via   Reference-Driven Inpainting"></a>Modification Takes Courage: Seamless Image Stitching via   Reference-Driven Inpainting</h2><p><strong>Authors:Ziqi Xie, Xiao Lai, Weidong Zhao, Siqi Jiang, Xianhui Liu, Wenlong Hou</strong></p>
<p>Current image stitching methods often produce noticeable seams in challenging scenarios such as uneven hue and large parallax. To tackle this problem, we propose the Reference-Driven Inpainting Stitcher (RDIStitcher), which reformulates the image fusion and rectangling as a reference-based inpainting model, incorporating a larger modification fusion area and stronger modification intensity than previous methods. Furthermore, we introduce a self-supervised model training method, which enables the implementation of RDIStitcher without requiring labeled data by fine-tuning a Text-to-Image (T2I) diffusion model. Recognizing difficulties in assessing the quality of stitched images, we present the Multimodal Large Language Models (MLLMs)-based metrics, offering a new perspective on evaluating stitched image quality. Compared to the state-of-the-art (SOTA) method, extensive experiments demonstrate that our method significantly enhances content coherence and seamless transitions in the stitched images. Especially in the zero-shot experiments, our method exhibits strong generalization capabilities. Code: <a target="_blank" rel="noopener" href="https://github.com/yayoyo66/RDIStitcher">https://github.com/yayoyo66/RDIStitcher</a> </p>
<blockquote>
<p>当前图像拼接方法往往在具有挑战性的场景中产生明显的接缝，如色调不均匀和较大视差。为了解决这个问题，我们提出了基于参考驱动的填充拼接器（RDIStitcher），它将图像融合和矩形框重新定义为基于参考的填充模型，并融入了比以往更大的修改融合区域和更强的修改强度。此外，我们引入了一种自监督模型训练方法，通过微调文本到图像（T2I）扩散模型，使RDIStitcher的实现无需标记数据。由于拼接图像的质量评估难度大，我们提出基于多模态大型语言模型（MLLMs）的度量标准，为评估拼接图像质量提供了新的视角。与最新方法相比，大量实验表明，我们的方法在拼接图像的内容连贯性和无缝过渡方面有了显著的提升。特别是在零样本实验中，我们的方法表现出强大的泛化能力。代码链接：<a target="_blank" rel="noopener" href="https://github.com/yayoyo66/RDIStitcher">https://github.com/yayoyo66/RDIStitcher</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10309v2">PDF</a> 18 pages, 10 figures</p>
<p><strong>摘要</strong><br>     针对现有图像拼接方法在处理不均匀色调和大视差等复杂场景时产生的明显接缝问题，我们提出了基于参考驱动的图像填充拼接器（RDIStitcher）。该方法将图像融合和矩形化重新构建为基于参考的填充模型，并引入更大的修改融合区域和更强的修改强度。此外，我们采用了一种自监督模型训练方法，通过微调文本到图像（T2I）扩散模型，无需标注数据即可实现RDIStitcher。同时，我们认识到评估拼接图像质量存在困难，因此提出了基于多模态大型语言模型（MLLMs）的评估指标，为评估拼接图像质量提供了新的视角。相较于现有最先进的（SOTA）方法，大量实验证明我们的方法显著提高了拼接图像的内容连贯性和无缝过渡效果，尤其在零样本实验中，我们的方法展现出强大的泛化能力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>RDIStitcher方法针对图像拼接中的明显接缝问题进行了优化，特别在复杂场景如不均匀色调和大视差情况下表现更优秀。</li>
<li>RDIStitcher将图像融合和矩形化重构为基于参考的填充模型，扩大了修改融合区域并增强了修改强度。</li>
<li>通过微调文本到图像（T2I）扩散模型，实现了RDIStitcher的自监督模型训练，无需标注数据。</li>
<li>提出了基于多模态大型语言模型（MLLMs）的评估指标，为评估拼接图像质量提供了新视角。</li>
<li>与现有最先进的图像拼接方法相比，RDIStitcher在内容连贯性和无缝过渡方面有明显提升。</li>
<li>RDIStitcher在零样本实验中表现出强大的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10309">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7d21837f4df5bcd140eac0498a81db74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-447e78e123da3e244f8ba39fbca798cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6c7621bd88e3a27d597c1da2bdc0880.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d952721c946bcee1874bf3958b4194c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Vulnerabilities-in-AI-generated-Image-Detection-The-Challenge-of-Adversarial-Attacks"><a href="#Vulnerabilities-in-AI-generated-Image-Detection-The-Challenge-of-Adversarial-Attacks" class="headerlink" title="Vulnerabilities in AI-generated Image Detection: The Challenge of   Adversarial Attacks"></a>Vulnerabilities in AI-generated Image Detection: The Challenge of   Adversarial Attacks</h2><p><strong>Authors:Yunfeng Diao, Naixin Zhai, Changtao Miao, Zitong Yu, Xingxing Wei, Xun Yang, Meng Wang</strong></p>
<p>Recent advancements in image synthesis, particularly with the advent of GAN and Diffusion models, have amplified public concerns regarding the dissemination of disinformation. To address such concerns, numerous AI-generated Image (AIGI) Detectors have been proposed and achieved promising performance in identifying fake images. However, there still lacks a systematic understanding of the adversarial robustness of AIGI detectors. In this paper, we examine the vulnerability of state-of-the-art AIGI detectors against adversarial attack under white-box and black-box settings, which has been rarely investigated so far. To this end, we propose a new method to attack AIGI detectors. First, inspired by the obvious difference between real images and fake images in the frequency domain, we add perturbations under the frequency domain to push the image away from its original frequency distribution. Second, we explore the full posterior distribution of the surrogate model to further narrow this gap between heterogeneous AIGI detectors, e.g. transferring adversarial examples across CNNs and ViTs. This is achieved by introducing a novel post-train Bayesian strategy that turns a single surrogate into a Bayesian one, capable of simulating diverse victim models using one pre-trained surrogate, without the need for re-training. We name our method as Frequency-based Post-train Bayesian Attack, or FPBA. Through FPBA, we show that adversarial attack is truly a real threat to AIGI detectors, because FPBA can deliver successful black-box attacks across models, generators, defense methods, and even evade cross-generator detection, which is a crucial real-world detection scenario. The code will be shared upon acceptance. </p>
<blockquote>
<p>近期图像合成领域的进展，特别是生成对抗网络（GAN）和扩散模型的出现，加剧了公众对传播虚假信息的担忧。为了应对这些担忧，已经提出了许多人工智能生成的图像（AIGI）检测器，并在识别虚假图像方面取得了令人鼓舞的性能。然而，目前对于AIGI检测器的对抗性稳健性仍缺乏系统的理解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.20836v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>近期图像合成技术的进展，特别是GAN和Diffusion模型的出现，加剧了公众对假信息传播的担忧。为应对这些担忧，已经提出了许多AI生成的图像（AIGI）检测器，它们在识别虚假图像方面表现出良好的性能。然而，对于AIGI检测器的对抗性稳健性，仍缺乏系统的理解。本文研究了最先进的AIGI检测器在白盒和黑盒设置下对抗攻击的脆弱性，这一领域的研究迄今为止仍很少见。为此，我们提出了一种新的方法来攻击AIGI检测器。首先，受真实图像和虚假图像在频域上明显差异的启发，我们在频域中添加扰动使图像偏离其原始频率分布。其次，我们探索了代理模型的后验分布，以进一步缩小不同AIGI检测器之间的鸿沟。通过引入一种新的后训练贝叶斯策略，我们将单一代理转变为贝叶斯代理，能够模拟多种受害者模型，使用预训练的单一代理而无需重新训练即可实现这一目标。我们称我们的方法为基于频率的后训练贝叶斯攻击（FPBA）。通过FPBA，我们证明了对抗性攻击确实对AIGI检测器构成了真正的威胁。我们的方法成功实现了跨模型、生成器、防御方法的黑盒攻击，并能逃避跨生成器检测，这是真实世界检测场景中的关键能力。代码将在接受后共享。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>AIGI检测器面临着对抗性攻击的威胁，这对其在实际应用中的稳健性提出了挑战。</li>
<li>提出了一种新的攻击方法FPBA，能够在频域中添加扰动来影响检测器的性能。</li>
<li>FPBA能够模拟多种受害者模型，使用预训练的单一代理模型进行攻击，无需重新训练。</li>
<li>FPBA攻击能够跨模型、生成器、防御方法进行黑盒攻击，显示出其强大的攻击能力。</li>
<li>FPBA能够逃避跨生成器检测，这在真实世界的检测场景中是一个重要的能力。</li>
<li>本文的研究强调了对抗性攻击的严重性，并提醒研究人员需要进一步提高AIGI检测器的稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.20836">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-2a4b1d21fbcd97906832d08975d396c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c82f2fff667db0fb1123e881d358555a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-811f50070869a29f56c31cff905cb7a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac7eb11f615f0ad2559cb7713ffc88d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d850ae0943901a6257d755c9802101c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b71052a1e017ea0332c0511e1c19f6eb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-11/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-11/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-11/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0e7804ec03209bb5563c5f11aa2c3130.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-03-11  Task-oriented Uncertainty Collaborative Learning for Label-Efficient   Brain Tumor Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-11/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-01e294b4790a8a4cb8427f2d5bc95ca3.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-03-11  Metadata-free Georegistration of Ground and Airborne Imagery
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">19380.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
