<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-08-09  MM2CT MR-to-CT translation for multi-modal image fusion with mamba">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-c4f3381c9ca1a8d2d3266d998931c981.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    83 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-09-更新"><a href="#2025-08-09-更新" class="headerlink" title="2025-08-09 更新"></a>2025-08-09 更新</h1><h2 id="MM2CT-MR-to-CT-translation-for-multi-modal-image-fusion-with-mamba"><a href="#MM2CT-MR-to-CT-translation-for-multi-modal-image-fusion-with-mamba" class="headerlink" title="MM2CT: MR-to-CT translation for multi-modal image fusion with mamba"></a>MM2CT: MR-to-CT translation for multi-modal image fusion with mamba</h2><p><strong>Authors:Chaohui Gong, Zhiying Wu, Zisheng Huang, Gaofeng Meng, Zhen Lei, Hongbin Liu</strong></p>
<p>Magnetic resonance (MR)-to-computed tomography (CT) translation offers significant advantages, including the elimination of radiation exposure associated with CT scans and the mitigation of imaging artifacts caused by patient motion. The existing approaches are based on single-modality MR-to-CT translation, with limited research exploring multimodal fusion. To address this limitation, we introduce Multi-modal MR to CT (MM2CT) translation method by leveraging multimodal T1- and T2-weighted MRI data, an innovative Mamba-based framework for multi-modal medical image synthesis. Mamba effectively overcomes the limited local receptive field in CNNs and the high computational complexity issues in Transformers. MM2CT leverages this advantage to maintain long-range dependencies modeling capabilities while achieving multi-modal MR feature integration. Additionally, we incorporate a dynamic local convolution module and a dynamic enhancement module to improve MRI-to-CT synthesis. The experiments on a public pelvis dataset demonstrate that MM2CT achieves state-of-the-art performance in terms of Structural Similarity Index Measure (SSIM) and Peak Signal-to-Noise Ratio (PSNR). Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Gots-ch/MM2CT">https://github.com/Gots-ch/MM2CT</a>. </p>
<blockquote>
<p>磁共振（MR）到计算机断层扫描（CT）的转换具有显著的优势，包括消除与CT扫描相关的辐射暴露以及减轻由患者运动引起的成像伪影。现有方法基于单模态MR-to-CT转换，对多模态融合的研究有限。为了解决这一局限性，我们引入了多模态MR到CT（MM2CT）转换方法，该方法利用T1加权和T2加权MRI数据的多模态信息，以及基于Mamba的多模态医学图像合成创新框架。Mamba有效地克服了卷积神经网络中局部感受野的限制和变压器中的高计算复杂性。MM2CT利用这一优势，在保持长期依赖建模能力的同时，实现了多模态MR特征融合。此外，我们结合了动态局部卷积模块和动态增强模块，以改进MRI到CT的合成。在公共骨盆数据集上的实验表明，MM2CT在结构相似性指数度量（SSIM）和峰值信噪比（PSNR）方面达到了最先进的技术性能。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/Gots-ch/MM2CT%E5%85%AC%E5%BC%80%E6%8F%BD%E5%BE%AE%E3%80%82">https://github.com/Gots-ch/MM2CT公开获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05476v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了多模态MR到CT转换方法的研究。通过引入基于多模态MRI数据的Mamba框架，实现MRI到CT转换的深度学习模型，提高成像质量并克服现有方法的局限性。该方法结合了多模态MRI数据的优势，有效克服了CNN的局部感受野限制和Transformer的高计算复杂性。实验结果表明，该方法在公共骨盆数据集上取得了最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MR到CT转换能够消除CT扫描相关的辐射暴露并减少由患者运动引起的成像伪影。</li>
<li>现有方法主要基于单模态MR到CT转换，研究在探索多模态融合方面有限。</li>
<li>引入多模态MR到CT（MM2CT）转换方法，利用T1和T2加权MRI数据。</li>
<li>MM2CT使用Mamba框架进行多模态医学图像合成，克服了CNN的局部感受野和Transformer的高计算复杂性。</li>
<li>MM2CT结合动态局部卷积模块和动态增强模块，提高了MRI到CT的合成效果。</li>
<li>在公共骨盆数据集上的实验表明，MM2CT在结构相似性指数度量（SSIM）和峰值信噪比（PSNR）方面达到最佳性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05476">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e7653b56dc55ddf55476640f001343e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05b58b965e018b4b2a7c4541d3a8e502.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4f3381c9ca1a8d2d3266d998931c981.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2667ccce90a816c3df8a5a42b3da9f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f42bef444e54bc1aa5d86f6b6ffa95a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Look-Up-Table-Correction-for-Beam-Hardening-Induced-Signal-of-Clinical-Dark-Field-Chest-Radiographs"><a href="#Look-Up-Table-Correction-for-Beam-Hardening-Induced-Signal-of-Clinical-Dark-Field-Chest-Radiographs" class="headerlink" title="Look-Up Table-Correction for Beam Hardening-Induced Signal of Clinical   Dark-Field Chest Radiographs"></a>Look-Up Table-Correction for Beam Hardening-Induced Signal of Clinical   Dark-Field Chest Radiographs</h2><p><strong>Authors:Maximilian E. Lochschmidt, Theresa Urban, Lennard Kaster, Rafael Schick, Thomas Koehler, Daniela Pfeiffer, Franz Pfeiffer</strong></p>
<p>Background: Material structures at the micrometer scale cause ultra-small-angle X-ray scattering, e.g., seen in lung tissue or plastic foams. In grating-based X-ray imaging, this causes a reduction of the fringe visibility, forming a dark-field signal. Polychromatic beam hardening also changes visibility, adding a false dark-field signal due to attenuation, even in homogeneous, non-scattering materials. Purpose: The objective of this study is to develop a fast, simple, and robust method to correct dark-field signals and bony structures present due to beam hardening on dark-field chest radiographs of study participants. Methods: The method is based on calibration measurements and image processing. Beam hardening by bones and soft tissue is modeled by aluminum and water, respectively, which have no microstructure and thus only generate an artificial dark-field signal. Look-up tables were then created for both. By using a weighted mean of these, forming a single look-up table, and using the attenuation images, the artificial dark-field signal and thus the bone structures present are reduced for study participants. Results: It was found that applying a correction using a weighted look-up table leads to a significant reduction of bone structures in the dark-field image. The weighting of the aluminum component has a substantial impact on the degree to which bone structures remain visible in the dark-field image. Furthermore, a large negative bias in the dark-field image, dependent on the aluminum weighting, was successfully corrected. Conclusions: The beam-hardening-induced signal in the dark-field images was successfully reduced using the method described. The choice of aluminum weighting to suppress rib structures, as well as the selection of bias correction, should be evaluated based on the specific clinical question. </p>
<blockquote>
<p>背景：微米尺度的材料结构会导致超小角度X射线散射，例如在肺组织或塑料泡沫中。在基于光栅的X射线成像中，这会导致条纹可见度降低，形成暗场信号。多色光束硬化也会改变可见度，即使在均匀、非散射材料中，由于衰减也会增加虚假的暗场信号。目的：本研究的目标是开发一种快速、简单、稳健的方法来校正由于光束硬化产生的暗场信号以及研究参与者存在的骨结构。方法：该方法基于校准测量和图像处理。骨骼和软组织引起的光束硬化分别通过铝和水进行建模，它们没有微观结构，因此只产生人工暗场信号。之后为这两者创建了查找表。通过计算加权平均值形成单一查找表，并使用衰减图像，可以减少研究参与者的人工暗场信号及其存在的骨结构。结果：发现使用加权查找表进行校正会导致暗场图像中的骨结构显著减少。铝成分的权重对暗场图像中骨结构的可见度有很大影响。此外，成功校正了依赖于铝权重的暗场图像中的大负偏差。结论：使用所描述的方法成功减少了暗场图像中由光束硬化引起的信号。抑制肋骨结构的铝权重选择以及偏差校正的选择应根据具体的临床问题进行评估。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05422v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>微米级材料结构导致超小角度X射线散射，如在肺组织或塑料泡沫中所见。在基于格栅的X射线成像中，这降低了边缘可见度，形成暗场信号。多色光束硬化也会改变可见度，在均质、非散射材料中添加因衰减而产生的虚假暗场信号。本研究旨在开发一种快速、简单、稳健的方法来修正因光束硬化产生的暗场信号和骨质结构，应用于研究参与者的暗场胸部X光摄影。方法基于校准测量和图像处理，通过铝和水（分别模拟骨和软组织的光束硬化）创建查找表，并使用加权平均值形成单一查找表，结合衰减图像，减少研究参与者的骨质结构和人工暗场信号。结果发现使用加权查找表进行校正可显著降低暗场图像中的骨质结构，铝成分的权重对暗场图像中骨结构可见度有很大影响，并成功校正了依赖于铝权重的暗场图像的负偏差。结论：使用所描述的方法成功减少了暗场图像中的光束硬化信号。根据具体的临床问题，应评估用于抑制肋骨结构的铝权重选择和偏差校正的选择。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>微米级材料结构导致超小角度X射线散射，影响X光成像的清晰度。</li>
<li>在基于格栅的X射线成像中，边缘可见度会降低，并生成暗场信号。</li>
<li>多色光束硬化会改变可见度，产生虚假暗场信号。</li>
<li>研究目标：开发一种快速、简单、稳健的方法来修正因光束硬化产生的暗场信号和骨质结构。</li>
<li>方法基于校准测量和图像处理，使用铝和水模拟骨和软组织的光束硬化效应，并创建查找表进行校正。</li>
<li>应用加权查找表进行校正可显著降低暗场图像中的骨质结构。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05422">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c8f6fc0fb367c5253d867367849f9a73.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2fcd3f67aadb271a0b19288a1195b356.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6476955c0db6970e7c60c1304770f5e6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Artificial-Intelligence-Based-Classification-of-Spitz-Tumors"><a href="#Artificial-Intelligence-Based-Classification-of-Spitz-Tumors" class="headerlink" title="Artificial Intelligence-Based Classification of Spitz Tumors"></a>Artificial Intelligence-Based Classification of Spitz Tumors</h2><p><strong>Authors:Ruben T. Lucassen, Marjanna Romers, Chiel F. Ebbelaar, Aia N. Najem, Donal P. Hayes, Antien L. Mooyaart, Sara Roshani, Liliane C. D. Wynaendts, Nikolas Stathonikos, Gerben E. Breimer, Anne M. L. Jansen, Mitko Veta, Willeke A. M. Blokx</strong></p>
<p>Spitz tumors are diagnostically challenging due to overlap in atypical histological features with conventional melanomas. We investigated to what extent AI models, using histological and&#x2F;or clinical features, can: (1) distinguish Spitz tumors from conventional melanomas; (2) predict the underlying genetic aberration of Spitz tumors; and (3) predict the diagnostic category of Spitz tumors. The AI models were developed and validated using a dataset of 393 Spitz tumors and 379 conventional melanomas. Predictive performance was measured using the AUROC and the accuracy. The performance of the AI models was compared with that of four experienced pathologists in a reader study. Moreover, a simulation experiment was conducted to investigate the impact of implementing AI-based recommendations for ancillary diagnostic testing on the workflow of the pathology department. The best AI model based on UNI features reached an AUROC of 0.95 and an accuracy of 0.86 in differentiating Spitz tumors from conventional melanomas. The genetic aberration was predicted with an accuracy of 0.55 compared to 0.25 for randomly guessing. The diagnostic category was predicted with an accuracy of 0.51, where random chance-level accuracy equaled 0.33. On all three tasks, the AI models performed better than the four pathologists, although differences were not statistically significant for most individual comparisons. Based on the simulation experiment, implementing AI-based recommendations for ancillary diagnostic testing could reduce material costs, turnaround times, and examinations. In conclusion, the AI models achieved a strong predictive performance in distinguishing between Spitz tumors and conventional melanomas. On the more challenging tasks of predicting the genetic aberration and the diagnostic category of Spitz tumors, the AI models performed better than random chance. </p>
<blockquote>
<p>斯皮茨瘤由于具有与常规黑色素瘤重叠的非典型组织特征，因此在诊断上颇具挑战性。我们调查了人工智能模型能在多大程度上利用组织学和（或）临床特征：（1）区分斯皮茨瘤和常规黑色素瘤；（2）预测斯皮茨瘤的基本遗传异常；（3）预测斯皮茨瘤的诊断类别。人工智能模型的开发和验证使用了包含393例斯皮茨瘤和379例常规黑色素瘤的数据集。通过AUROC和准确性来衡量预测性能。在读者研究中，将人工智能模型的性能与四位经验丰富的病理医生的性能进行了比较。此外，进行了一项模拟实验，以研究在病理学部门实施基于人工智能的辅助诊断建议对工作流程的影响。基于UNI特征的最佳人工智能模型在区分斯皮茨瘤和常规黑色素瘤方面达到了AUROC为0.95和准确性为0.86。遗传异常的预测准确率为0.55，而随机猜测的准确率为0.25。诊断类别的预测准确率为0.51，其中随机机会水平的准确率等于0.33。在所有三项任务中，人工智能模型的性能都优于四位病理医生，尽管大多数个别比较的差异没有统计学意义。基于模拟实验的结果，实施基于人工智能的辅助诊断建议可以降低材料成本、缩短周转时间和检查次数。总之，人工智能模型在区分斯皮茨瘤和常规黑色素瘤方面表现出强大的预测性能。在预测斯皮茨瘤的遗传异常和诊断类别这些更具挑战性的任务中，人工智能模型的表现优于随机机会水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05391v1">PDF</a> 19 pages, 2 figures, 6 tables, 6 supplementary tables</p>
<p><strong>Summary</strong></p>
<p>基于Spitz肿瘤与常规黑色素瘤在组织形态学特征上的重叠，本研究探讨了使用组织学和&#x2F;或临床特征的AI模型在区分Spitz肿瘤与常规黑色素瘤、预测Spitz肿瘤的遗传异常以及预测其诊断类别方面的能力。研究使用包含393例Spitz肿瘤和379例常规黑色素瘤的数据集进行AI模型的开发和验证。结果显示，最佳模型在区分Spitz肿瘤与常规黑色素瘤方面达到了较高的预测性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI模型在区分Spitz肿瘤与常规黑色素瘤方面表现出强预测性能，最佳模型的AUROC值为0.95，准确率为0.86。</li>
<li>AI模型在预测Spitz肿瘤的遗传异常方面达到了一定的准确性（0.55），相较于随机猜测（0.25）有显著提升。</li>
<li>在预测Spitz肿瘤的诊断类别方面，AI模型的准确率为0.51，尽管相较于随机概率有所提升，但仍为较具挑战性的任务。</li>
<li>AI模型在三项任务中的表现均优于四位病理医生，尽管大多数个别比较差异未达统计学显著水平。</li>
<li>实施AI辅助诊断测试推荐可以降低材料成本、缩短周转时间以及检查次数。</li>
<li>AI模型在区分Spitz肿瘤与常规黑色素瘤方面的表现尤为突出。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05391">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cd41347cb06befb556bbf7034a386c05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea702850cafb2f911763ed1a23e85a59.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Deformable-Attention-Graph-Representation-Learning-for-Histopathology-Whole-Slide-Image-Analysis"><a href="#Deformable-Attention-Graph-Representation-Learning-for-Histopathology-Whole-Slide-Image-Analysis" class="headerlink" title="Deformable Attention Graph Representation Learning for Histopathology   Whole Slide Image Analysis"></a>Deformable Attention Graph Representation Learning for Histopathology   Whole Slide Image Analysis</h2><p><strong>Authors:Mingxi Fu, Xitong Ling, Yuxuan Chen, Jiawen Li, fanglei fu, Huaitian Yuan, Tian Guan, Yonghong He, Lianghui Zhu</strong></p>
<p>Accurate classification of Whole Slide Images (WSIs) and Regions of Interest (ROIs) is a fundamental challenge in computational pathology. While mainstream approaches often adopt Multiple Instance Learning (MIL), they struggle to capture the spatial dependencies among tissue structures. Graph Neural Networks (GNNs) have emerged as a solution to model inter-instance relationships, yet most rely on static graph topologies and overlook the physical spatial positions of tissue patches. Moreover, conventional attention mechanisms lack specificity, limiting their ability to focus on structurally relevant regions. In this work, we propose a novel GNN framework with deformable attention for pathology image analysis. We construct a dynamic weighted directed graph based on patch features, where each node aggregates contextual information from its neighbors via attention-weighted edges. Specifically, we incorporate learnable spatial offsets informed by the real coordinates of each patch, enabling the model to adaptively attend to morphologically relevant regions across the slide. This design significantly enhances the contextual field while preserving spatial specificity. Our framework achieves state-of-the-art performance on four benchmark datasets (TCGA-COAD, BRACS, gastric intestinal metaplasia grading, and intestinal ROI classification), demonstrating the power of deformable attention in capturing complex spatial structures in WSIs and ROIs. </p>
<blockquote>
<p>全切片图像（WSIs）和感兴趣区域（ROIs）的精确分类是计算病理学中的一项基本挑战。虽然主流方法经常采用多实例学习（MIL），但它们很难捕获组织结构之间的空间依赖关系。图神经网络（GNN）已出现为建模实例间关系的一种解决方案，但大多数依赖于静态图拓扑，并忽略了组织斑块的物理空间位置。此外，传统的注意力机制缺乏特异性，限制了它们关注结构相关区域的能力。在这项工作中，我们提出了一种用于病理学图像分析的新型可变形注意力图神经网络框架。我们基于斑块特征构建了一个动态加权有向图，其中每个节点都通过注意力加权的边从其邻居那里聚合上下文信息。具体来说，我们结合了可学习的空间偏移，以了解每个斑块的真实坐标，使模型能够自适应地关注切片上形态相关的区域。这种设计在增强上下文字段的同时保留了空间特异性。我们的框架在四个基准数据集上达到了最新性能（TCGA-COAD、BRACS、胃肠化生分级和肠道ROI分类），证明了可变形注意力在捕获WSIs和ROIs中复杂空间结构的力量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05382v1">PDF</a> </p>
<p><strong>Summary</strong><br>医学图像分类中，全幻灯片图像（WSIs）和感兴趣区域（ROIs）的准确分类是计算病理学中的基本挑战。主流方法多采用多实例学习（MIL），但难以捕捉组织结构的空间依赖性。图神经网络（GNNs）作为解决建模实例间关系的方案而出现，但大多数依赖于静态图拓扑结构而忽略了组织补丁的物理空间位置。此外，传统的注意力机制缺乏特异性，限制了其在结构上相关区域的关注能力。本研究提出了一种具有可变形注意力的新型GNN框架用于病理学图像分析。我们根据补丁特征构建了一个动态加权的有向图，每个节点通过注意力加权的边从其邻居处聚合上下文信息。具体来说，我们引入了可学习的空间偏移量，这些偏移量由每个补丁的真实坐标信息得出，使模型能够自适应地关注幻灯片中形态相关的区域。这种设计在增强上下文字段的同时保留了空间特异性。我们的框架在四个基准数据集上实现了最先进的性能，证明了可变形注意力在捕捉WSIs和ROIs中复杂空间结构的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>计算病理学中对全幻灯片图像（WSIs）和感兴趣区域（ROIs）的准确分类是一个核心挑战。</li>
<li>主流方法如多实例学习（MIL）难以捕捉组织结构的空间依赖性。</li>
<li>图神经网络（GNNs）能够建模实例间的关系，但大多方法忽略了组织补丁的空间位置。</li>
<li>传统的注意力机制在关注结构上相关区域时存在局限性。</li>
<li>本研究提出了一个新型的图神经网络框架，结合可变形注意力用于病理学图像分析。</li>
<li>该框架根据补丁特征构建动态加权的有向图，并考虑每个补丁的空间坐标，以自适应地关注形态相关的区域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05382">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8d51a6e711cce5870a686bc0e7755d5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b12f60026b3b21d0db1bb5c1ce1c98e1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a08c201d84246e109c60f5ae8b00122.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69deea88da923e74318823ec6398f270.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbbc79797b2fe532264bc1bf0faba992.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0455430535db719ec8f55b9e31f32bfb.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CT-GRAPH-Hierarchical-Graph-Attention-Network-for-Anatomy-Guided-CT-Report-Generation"><a href="#CT-GRAPH-Hierarchical-Graph-Attention-Network-for-Anatomy-Guided-CT-Report-Generation" class="headerlink" title="CT-GRAPH: Hierarchical Graph Attention Network for Anatomy-Guided CT   Report Generation"></a>CT-GRAPH: Hierarchical Graph Attention Network for Anatomy-Guided CT   Report Generation</h2><p><strong>Authors:Hamza Kalisch, Fabian Hörst, Jens Kleesiek, Ken Herrmann, Constantin Seibold</strong></p>
<p>As medical imaging is central to diagnostic processes, automating the generation of radiology reports has become increasingly relevant to assist radiologists with their heavy workloads. Most current methods rely solely on global image features, failing to capture fine-grained organ relationships crucial for accurate reporting. To this end, we propose CT-GRAPH, a hierarchical graph attention network that explicitly models radiological knowledge by structuring anatomical regions into a graph, linking fine-grained organ features to coarser anatomical systems and a global patient context. Our method leverages pretrained 3D medical feature encoders to obtain global and organ-level features by utilizing anatomical masks. These features are further refined within the graph and then integrated into a large language model to generate detailed medical reports. We evaluate our approach for the task of report generation on the large-scale chest CT dataset CT-RATE. We provide an in-depth analysis of pretrained feature encoders for CT report generation and show that our method achieves a substantial improvement of absolute 7.9% in F1 score over current state-of-the-art methods. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/hakal104/CT-GRAPH">https://github.com/hakal104/CT-GRAPH</a>. </p>
<blockquote>
<p>医学成像在诊断过程中占据核心地位，因此自动化生成放射学报告对于帮助放射科医生应对大量工作具有越来越重要的意义。当前大多数方法仅依赖于全局图像特征，无法捕获对准确报告至关重要的精细器官关系。为此，我们提出了CT-GRAPH，这是一种分层图注意力网络，它通过构建解剖区域图来显式地建立放射学知识模型，将精细的器官特征链接到较粗的解剖系统和全局患者背景。我们的方法利用预训练的3D医学特征编码器，通过使用解剖掩膜获得全局和器官级别的特征。这些特征在图中得到进一步精炼，然后集成到大型语言模型中，以生成详细的医疗报告。我们在大规模的胸部CT数据集CT-RATE上对生成报告的任务进行了评估。我们深入分析了用于CT报告生成的预训练特征编码器，并证明我们的方法在F1分数上较当前最先进的方法提高了绝对7.9%。代码可在<a target="_blank" rel="noopener" href="https://github.com/hakal104/CT-GRAPH%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hakal104/CT-GRAPH公开获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05375v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>医学成像在诊断过程中占据核心地位，为帮助放射科医生应对繁重的工作负担，自动化生成放射学报告变得至关重要。当前大多数方法仅依赖全局图像特征，忽略了精细器官关系对于准确报告的重要性。为此，本文提出CT-GRAPH方法，该方法利用层次化图注意力网络显式建模放射学知识，通过构建解剖区域图连接精细器官特征与较粗的解剖系统和全局患者背景。实验表明，该方法在大规模胸部CT数据集CT-RATE上的报告生成任务中取得了显著成果，相较于当前最先进的方法，F1分数提高了7.9%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学成像在诊断中起核心作用，自动化生成放射学报告对帮助放射科医生具有重要意义。</li>
<li>当前大多数自动化报告生成方法仅依赖全局图像特征，存在不足。</li>
<li>CT-GRAPH方法通过层次化图注意力网络显式建模放射学知识，结合精细器官关系与全局患者背景。</li>
<li>方法利用预训练的3D医学特征编码器获取全局和器官级别的特征。</li>
<li>实验表明，CT-GRAPH在胸部CT数据集CT-RATE上的报告生成任务表现优异。</li>
<li>与现有方法相比，CT-GRAPH在F1分数上提高了7.9%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05375">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-564b0b6c8ef7707ef5a6f7ab7b08adcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1ebaf932b90f5bcc567838c81865e8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72f69230672f44d42c5434d99484f3b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-030d44bf6217bee14152a252404cf810.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-962616d1b96a34216bd6ae9fedd5ef8b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PriorRG-Prior-Guided-Contrastive-Pre-training-and-Coarse-to-Fine-Decoding-for-Chest-X-ray-Report-Generation"><a href="#PriorRG-Prior-Guided-Contrastive-Pre-training-and-Coarse-to-Fine-Decoding-for-Chest-X-ray-Report-Generation" class="headerlink" title="PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine   Decoding for Chest X-ray Report Generation"></a>PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine   Decoding for Chest X-ray Report Generation</h2><p><strong>Authors:Kang Liu, Zhuoqi Ma, Zikang Fang, Yunan Li, Kun Xie, Qiguang Miao</strong></p>
<p>Chest X-ray report generation aims to reduce radiologists’ workload by automatically producing high-quality preliminary reports. A critical yet underexplored aspect of this task is the effective use of patient-specific prior knowledge – including clinical context (e.g., symptoms, medical history) and the most recent prior image – which radiologists routinely rely on for diagnostic reasoning. Most existing methods generate reports from single images, neglecting this essential prior information and thus failing to capture diagnostic intent or disease progression. To bridge this gap, we propose PriorRG, a novel chest X-ray report generation framework that emulates real-world clinical workflows via a two-stage training pipeline. In Stage 1, we introduce a prior-guided contrastive pre-training scheme that leverages clinical context to guide spatiotemporal feature extraction, allowing the model to align more closely with the intrinsic spatiotemporal semantics in radiology reports. In Stage 2, we present a prior-aware coarse-to-fine decoding for report generation that progressively integrates patient-specific prior knowledge with the vision encoder’s hidden states. This decoding allows the model to align with diagnostic focus and track disease progression, thereby enhancing the clinical accuracy and fluency of the generated reports. Extensive experiments on MIMIC-CXR and MIMIC-ABN datasets demonstrate that PriorRG outperforms state-of-the-art methods, achieving a 3.6% BLEU-4 and 3.8% F1 score improvement on MIMIC-CXR, and a 5.9% BLEU-1 gain on MIMIC-ABN. Code and checkpoints will be released upon acceptance. </p>
<blockquote>
<p>胸部X光报告生成旨在通过自动产生高质量的初步报告来减轻放射科医生的工作量。该任务的一个关键但尚未被充分研究的方面是如何有效地利用患者特定的先验知识，包括临床背景（例如症状、病史）和最近的先前图像，放射科医生在进行诊断推理时通常会依赖这些知识。大多数现有方法都是从单一图像生成报告，忽略了这些关键的先验信息，因此无法捕捉诊断意图或疾病进展。为了弥补这一差距，我们提出了PriorRG，这是一种新的胸部X光报告生成框架，它通过两阶段训练管道模拟现实临床工作流程。在第一阶段，我们引入了基于先验的对比预训练方案，利用临床背景引导时空特征提取，使模型更贴近放射学报告中的内在时空语义。在第二阶段，我们提出了基于先验的粗细到细解码报告生成方法，该方法逐步将患者特定的先验知识与视觉编码器的隐藏状态集成在一起。这种解码方式使模型能够符合诊断重点并跟踪疾病进展，从而提高生成报告的临床准确性和流畅性。在MIMIC-CXR和MIMIC-ABN数据集上的大量实验表明，PriorRG优于最新方法，在MIMIC-CXR上实现了BLEU-4的3.6%和F1得分的3.8%的提升，在MIMIC-ABN上实现了BLEU-1的5.9%的提升。论文接受后将公布代码和检查点。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05353v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文关注通过利用患者特定先验知识（包括临床症状、病史和最新图像等），实现自动产生高质量初步报告，从而减少放射科医生工作量的目标。针对当前多数方法仅关注单一图像报告生成，忽略了重要的先验信息，无法捕捉诊断意图或疾病进展的问题，本文提出了一种新的胸部X光报告生成框架PriorRG。该框架模拟真实临床工作流程，通过两阶段训练管道进行训练。第一阶段引入基于先验知识的对比预训练方案，利用临床语境指导时空特征提取；第二阶段采用先验知识感知的粗细解码方式生成报告，逐步将患者特定先验知识与视觉编码器的隐藏状态整合在一起。通过这种方式，模型能够更好地聚焦于诊断重点并追踪疾病进展，提高了生成报告的临床准确性和流畅性。在MIMIC-CXR和MIMIC-ABN数据集上的实验表明，PriorRG较先进方法实现了显著性能提升，在MIMIC-CXR上BLEU-4和F1得分提高了3.6%和3.8%，在MIMIC-ABN上BLEU-1得分提高了5.9%。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>胸部X光报告生成旨在通过自动产生高质量初步报告减少放射科医生工作量。</li>
<li>当前方法忽略了患者特定的先验信息，包括临床症状、病史和最新图像等，无法捕捉诊断意图和疾病进展。</li>
<li>PriorRG框架通过模拟真实临床工作流程来整合患者特定先验知识，提高报告生成的准确性和流畅性。</li>
<li>PriorRG采用两阶段训练管道：第一阶段利用临床语境进行先验知识指导的对比预训练；第二阶段采用先验知识感知的解码方式生成报告。</li>
<li>实验结果表明，PriorRG在MIMIC-CXR和MIMIC-ABN数据集上较先进方法实现了显著性能提升。</li>
<li>PriorRG模型能更好地聚焦于诊断重点并追踪疾病进展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05353">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9d096105cae123088be71d8934448602.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-887a288e91f1ed595c80fa3a32861364.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3be48e1e784d34cecce759d8b504750d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f00a38dcdfedb2ff89e338f1f30e8dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae2e96d0a2c0d17c039ae19a4a0a30dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a0e2ac850fa80c8b33071e7b1656377.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RegionMed-CLIP-A-Region-Aware-Multimodal-Contrastive-Learning-Pre-trained-Model-for-Medical-Image-Understanding"><a href="#RegionMed-CLIP-A-Region-Aware-Multimodal-Contrastive-Learning-Pre-trained-Model-for-Medical-Image-Understanding" class="headerlink" title="RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning   Pre-trained Model for Medical Image Understanding"></a>RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning   Pre-trained Model for Medical Image Understanding</h2><p><strong>Authors:Tianchen Fang, Guiru Liu</strong></p>
<p>Medical image understanding plays a crucial role in enabling automated diagnosis and data-driven clinical decision support. However, its progress is impeded by two primary challenges: the limited availability of high-quality annotated medical data and an overreliance on global image features, which often miss subtle but clinically significant pathological regions. To address these issues, we introduce RegionMed-CLIP, a region-aware multimodal contrastive learning framework that explicitly incorporates localized pathological signals along with holistic semantic representations. The core of our method is an innovative region-of-interest (ROI) processor that adaptively integrates fine-grained regional features with the global context, supported by a progressive training strategy that enhances hierarchical multimodal alignment. To enable large-scale region-level representation learning, we construct MedRegion-500k, a comprehensive medical image-text corpus that features extensive regional annotations and multilevel clinical descriptions. Extensive experiments on image-text retrieval, zero-shot classification, and visual question answering tasks demonstrate that RegionMed-CLIP consistently exceeds state-of-the-art vision language models by a wide margin. Our results highlight the critical importance of region-aware contrastive pre-training and position RegionMed-CLIP as a robust foundation for advancing multimodal medical image understanding. </p>
<blockquote>
<p>医学图像理解在实现自动化诊断和基于数据的临床决策支持中起着至关重要的作用。然而，其进展受到两个主要挑战的限制：高质量标注医学数据的有限可用性，以及对全局图像特征的过度依赖，这往往导致忽略细微但临床上重要的病理区域。为了解决这些问题，我们引入了RegionMed-CLIP，这是一个区域感知的多模式对比学习框架，它显式地结合了局部病理信号和整体语义表示。我们的方法的核心是一个创新的兴趣区域（ROI）处理器，它自适应地集成精细的局部特征与全局上下文，辅以一种增强层次化多模式对齐的渐进式训练策略。为了实现对大规模区域级别表示的学习，我们构建了MedRegion-500k，这是一个以区域注释和多层次临床描述为特色的医学图像-文本语料库。在图像-文本检索、零样本分类和视觉问答任务上的大量实验表明，RegionMed-CLIP一致且大幅度地超越了最先进的视觉语言模型。我们的研究结果强调了区域感知对比预训练的关键重要性，并将RegionMed-CLIP定位为推进多模式医学图像理解的有力基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05244v1">PDF</a> </p>
<p><strong>Summary</strong><br>     医学图像理解在自动化诊断和基于数据的临床决策支持中发挥着关键作用，但面临着高质量标注医学数据有限和过于依赖全局图像特征两大挑战。为解决这些问题，我们提出了RegionMed-CLIP，这是一个区域感知的多模态对比学习框架，它显式地结合了局部病理信号和整体语义表示。我们的方法核心是自适应集成精细区域特征与全局上下文的感兴趣区域（ROI）处理器，辅以增强层次多模态对齐的渐进训练策略。为实现大规模区域级别表示学习，我们构建了MedRegion-500k，这是一个包含广泛区域注释和多层临床描述的综合医学图像文本语料库。实验表明，RegionMed-CLIP在图像文本检索、零样本分类和视觉问答任务上显著超越了最先进的视觉语言模型。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>医学图像理解在自动化诊断和临床决策支持中起关键作用。</li>
<li>医学图像理解的两大挑战是高质量标注数据有限和过度依赖全局图像特征。</li>
<li>RegionMed-CLIP是一个区域感知的多模态对比学习框架，结合了局部病理信号和整体语义表示。</li>
<li>ROI处理器是RegionMed-CLIP的核心，它自适应地集成精细区域特征与全局上下文。</li>
<li>渐进的训练策略增强了RegionMed-CLIP的层次多模态对齐能力。</li>
<li>MedRegion-500k是一个综合医学图像文本语料库，包含广泛的区域注释和多层临床描述。</li>
<li>RegionMed-CLIP在多项任务上表现优异，显著超越了现有的视觉语言模型。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05244">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4409389a475b3efc3467f6322ecaf107.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0038387d1731557539231913673eb74.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b048fbad3f03f4ac0c39114b067eb10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e98bdf7cef64cadde7f2ee8c5e2dc8cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a93e3776597773699674e91737962a70.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Beyond-Pixels-Medical-Image-Quality-Assessment-with-Implicit-Neural-Representations"><a href="#Beyond-Pixels-Medical-Image-Quality-Assessment-with-Implicit-Neural-Representations" class="headerlink" title="Beyond Pixels: Medical Image Quality Assessment with Implicit Neural   Representations"></a>Beyond Pixels: Medical Image Quality Assessment with Implicit Neural   Representations</h2><p><strong>Authors:Caner Özer, Patryk Rygiel, Bram de Wilde, İlkay Öksüz, Jelmer M. Wolterink</strong></p>
<p>Artifacts pose a significant challenge in medical imaging, impacting diagnostic accuracy and downstream analysis. While image-based approaches for detecting artifacts can be effective, they often rely on preprocessing methods that can lead to information loss and high-memory-demand medical images, thereby limiting the scalability of classification models. In this work, we propose the use of implicit neural representations (INRs) for image quality assessment. INRs provide a compact and continuous representation of medical images, naturally handling variations in resolution and image size while reducing memory overhead. We develop deep weight space networks, graph neural networks, and relational attention transformers that operate on INRs to achieve image quality assessment. Our method is evaluated on the ACDC dataset with synthetically generated artifact patterns, demonstrating its effectiveness in assessing image quality while achieving similar performance with fewer parameters. </p>
<blockquote>
<p>在医学成像中，伪影构成了一大挑战，影响了诊断准确性和后续分析。虽然基于图像的伪影检测方法可以很有效，但它们经常依赖于可能导致信息丢失和高内存需求的医学图像预处理方法，从而限制了分类模型的可扩展性。在这项工作中，我们建议使用隐式神经表示（INR）来进行图像质量评估。INR为医学图像提供了紧凑且连续的代表，可自然处理分辨率和图像大小的差异，同时减少内存开销。我们开发了深度权重空间网络、图神经网络和关系注意力转换器，它们在INR上运行以实现图像质量评估。我们的方法在ACDC数据集上使用合成生成的伪影模式进行了评估，证明了其在评估图像质量时的有效性，同时以更少的参数实现了类似性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05168v1">PDF</a> Accepted in 16th Machine Learning in Medical Imaging (MLMI 2025)   workshop</p>
<p><strong>Summary</strong></p>
<p>本文提出使用隐式神经网络表示（INRs）进行医学图像质量评估的方法。该方法能有效处理医学图像中的伪影问题，通过紧凑且连续的图像表示，自然应对分辨率和图像大小的差异，同时降低内存开销。通过深度权重空间网络、图神经网络和关系注意力转换器对INRs进行操作，实现了图像质量评估。在ACDC数据集上，使用合成伪影模式进行验证，证明该方法在评估图像质量时表现出色，且参数更少。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>伪影对医学成像构成挑战，影响诊断和下游分析。</li>
<li>图像方法虽可有效检测伪影，但依赖的预处理可能导致信息损失及高内存需求。</li>
<li>引入隐式神经网络表示（INRs）进行医学图像质量评估。</li>
<li>INRs提供紧凑且连续的图像表示，适应不同分辨率和图像大小。</li>
<li>使用深度权重空间网络、图神经网络和关系注意力转换器对INRs进行评估操作。</li>
<li>方法在ACDC数据集上验证有效，合成伪影模式下表现良好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05168">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-eab06a6c0c9ff796fe82b9b3d731c6f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b3a28b64649100510671c6669009337.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7bbf2535e930d740726d452a51079f8.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="FedGIN-Federated-Learning-with-Dynamic-Global-Intensity-Non-linear-Augmentation-for-Organ-Segmentation-using-Multi-modal-Images"><a href="#FedGIN-Federated-Learning-with-Dynamic-Global-Intensity-Non-linear-Augmentation-for-Organ-Segmentation-using-Multi-modal-Images" class="headerlink" title="FedGIN: Federated Learning with Dynamic Global Intensity Non-linear   Augmentation for Organ Segmentation using Multi-modal Images"></a>FedGIN: Federated Learning with Dynamic Global Intensity Non-linear   Augmentation for Organ Segmentation using Multi-modal Images</h2><p><strong>Authors:Sachin Dudda Nagaraju, Ashkan Moradi, Bendik Skarre Abrahamsen, Mattijs Elschot</strong></p>
<p>Medical image segmentation plays a crucial role in AI-assisted diagnostics, surgical planning, and treatment monitoring. Accurate and robust segmentation models are essential for enabling reliable, data-driven clinical decision making across diverse imaging modalities. Given the inherent variability in image characteristics across modalities, developing a unified model capable of generalizing effectively to multiple modalities would be highly beneficial. This model could streamline clinical workflows and reduce the need for modality-specific training. However, real-world deployment faces major challenges, including data scarcity, domain shift between modalities (e.g., CT vs. MRI), and privacy restrictions that prevent data sharing. To address these issues, we propose FedGIN, a Federated Learning (FL) framework that enables multimodal organ segmentation without sharing raw patient data. Our method integrates a lightweight Global Intensity Non-linear (GIN) augmentation module that harmonizes modality-specific intensity distributions during local training. We evaluated FedGIN using two types of datasets: an imputed dataset and a complete dataset. In the limited dataset scenario, the model was initially trained using only MRI data, and CT data was added to assess its performance improvements. In the complete dataset scenario, both MRI and CT data were fully utilized for training on all clients. In the limited-data scenario, FedGIN achieved a 12 to 18% improvement in 3D Dice scores on MRI test cases compared to FL without GIN and consistently outperformed local baselines. In the complete dataset scenario, FedGIN demonstrated near-centralized performance, with a 30% Dice score improvement over the MRI-only baseline and a 10% improvement over the CT-only baseline, highlighting its strong cross-modality generalization under privacy constraints. </p>
<blockquote>
<p>医学图像分割在人工智能辅助诊断、手术规划和治疗监测中起着至关重要的作用。准确且稳健的分割模型对于在多种成像模式上实现可靠的数据驱动临床决策至关重要。考虑到不同模态图像特性固有的变化，开发一种能够有效推广到多种模态的统一模型将大有裨益。该模型可以简化临床工作流程，减少针对特定模态的培训需求。然而，在现实世界的部署中，面临着数据稀缺、不同模态之间的领域偏移（例如CT与MRI）以及阻止数据共享的隐私限制等重大挑战。为了解决这些问题，我们提出了FedGIN，这是一个联邦学习（FL）框架，能够在不共享原始患者数据的情况下实现多模态器官分割。我们的方法整合了一个轻量级的全局强度非线性（GIN）增强模块，该模块在本地训练过程中协调特定模态的强度分布。我们使用两种类型的数据集对FedGIN进行了评估：一个填充数据集和一个完整数据集。在有限数据集的情况下，模型最初仅使用MRI数据进行训练，然后加入CT数据以评估其性能改进。在完整数据集的情况下，MRI和CT数据都用于所有客户端的训练。在数据有限的情况下，与没有GIN的FL相比，FedGIN在MRI测试案例中的3D Dice得分提高了12%到18%，并且始终超过了本地基准。在完整数据集的情况下，FedGIN表现出接近集中式的性能，与仅使用MRI的基线相比，Dice得分提高了30%，与仅使用CT的基线相比，提高了10%，这突显了其在隐私约束下强大的跨模态泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05137v1">PDF</a> Paper Accepted at MICCAI 2025 DeCaf Workshop Track</p>
<p><strong>摘要</strong></p>
<p>医学图像分割在人工智能辅助诊断、手术规划和治疗监测中发挥着关键作用。开发准确且稳健的分割模型对于实现可靠的数据驱动临床决策至关重要，特别是在多种成像模态中。为应对不同模态间图像特性固有的差异性，开发能够有效泛化至多种模态的统一模型将大有��d益。该模型能简化临床工作流程，减少针对特定模态的培训需求。然而，现实世界部署面临重大挑战，包括数据稀缺、模态间的领域偏移（如CT与MRI）以及阻止数据共享的隐私限制。为解决这些问题，我们提出FedGIN，一个联邦学习（FL）框架，可在不共享原始患者数据的情况下实现多模态器官分割。我们的方法整合了轻量级的全局强度非线性（GIN）增强模块，在本地训练过程中协调模态特定强度分布。我们使用两种类型的数据集对FedGIN进行了评估：插补数据集和完整数据集。在有限数据集场景中，模型最初仅使用MRI数据进行训练，并加入CT数据以评估其性能改进。在完整数据集场景中，MRI和CT数据均用于所有客户端的训练。在有限数据场景中，与无GIN的FL相比，FedGIN在MRI测试病例中的3D骰子分数提高了12%到18%，并且始终优于本地基线。在完整数据集场景中，FedGIN表现出接近集中化的性能，与MRI-only基线相比，骰子分数提高了30%，与CT-only基线相比，提高了10%，这凸显了其在隐私约束下的跨模态泛化能力。</p>
<p><strong>要点解析</strong></p>
<ol>
<li>医学图像分割在多个医疗应用中的重要性。</li>
<li>不同成像模态之间图像特性的差异性带来的挑战。</li>
<li>开发能泛化至多种模态的统一模型的优势。</li>
<li>现实世界部署面临的三大挑战：数据稀缺、领域偏移和隐私限制。</li>
<li>FedGIN联邦学习框架的提出以及其整合的GIN模块的作用。</li>
<li>FedGIN在有限数据集和完整数据集场景下的性能表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05137">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0306a3a321ea9d3eec149a2e7c7c8e6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0786af1fc754cbe6f20bb203b54bb791.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60ab1ac82b04dc0779aa2e6799b4284a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bde6cf13a33986dabfd4e7ff3a9ea9f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Latent-Expression-Generation-for-Referring-Image-Segmentation-and-Grounding"><a href="#Latent-Expression-Generation-for-Referring-Image-Segmentation-and-Grounding" class="headerlink" title="Latent Expression Generation for Referring Image Segmentation and   Grounding"></a>Latent Expression Generation for Referring Image Segmentation and   Grounding</h2><p><strong>Authors:Seonghoon Yu, Joonbeom Hong, Joonseok Lee, Jeany Son</strong></p>
<p>Visual grounding tasks, such as referring image segmentation (RIS) and referring expression comprehension (REC), aim to localize a target object based on a given textual description. The target object in an image can be described in multiple ways, reflecting diverse attributes such as color, position, and more. However, most existing methods rely on a single textual input, which captures only a fraction of the rich information available in the visual domain. This mismatch between rich visual details and sparse textual cues can lead to the misidentification of similar objects. To address this, we propose a novel visual grounding framework that leverages multiple latent expressions generated from a single textual input by incorporating complementary visual details absent from the original description. Specifically, we introduce subject distributor and visual concept injector modules to embed both shared-subject and distinct-attributes concepts into the latent representations, thereby capturing unique and target-specific visual cues. We also propose a positive-margin contrastive learning strategy to align all latent expressions with the original text while preserving subtle variations. Experimental results show that our method not only outperforms state-of-the-art RIS and REC approaches on multiple benchmarks but also achieves outstanding performance on the generalized referring expression segmentation (GRES) benchmark. </p>
<blockquote>
<p>视觉定位任务，如引用图像分割（RIS）和引用表达式理解（REC），旨在根据给定的文本描述定位目标对象。图像中的目标对象可以用多种方式描述，反映颜色、位置等多种属性。然而，大多数现有方法依赖于单个文本输入，只能捕捉到视觉领域丰富信息的一小部分。丰富视觉细节和稀疏文本线索之间的不匹配可能导致类似对象的误识别。为解决这一问题，我们提出了一种新的视觉定位框架，该框架利用单个文本输入产生的多个潜在表达式，并融入原始描述中缺失的互补视觉细节。具体来说，我们引入了主体分配器和视觉概念注入器模块，将共享主体和独特属性概念嵌入到潜在表示中，从而捕获独特且针对目标对象的视觉线索。我们还提出了一种正边距对比学习策略，将所有潜在表达式与原始文本对齐，同时保留细微变化。实验结果表明，我们的方法不仅在多个基准测试上超越了最先进的RIS和REC方法，而且在广义引用表达式分割（GRES）基准测试上也取得了卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05123v1">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>这是一篇关于视觉定位任务的研究论文，主要介绍了如何通过结合多种潜在表达和视觉细节来解决单一文本输入导致的视觉与文本信息不匹配的问题。论文提出了一种新的视觉定位框架，该框架通过从单一文本输入生成多个潜在表达，并融入缺失的视觉细节，从而提高目标对象的识别准确性。实验结果表明，该方法在多个基准测试上优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉定位任务旨在根据给定的文本描述定位图像中的目标对象。</li>
<li>现有方法主要依赖单一文本输入，无法充分利用视觉域中的丰富信息。</li>
<li>论文提出了一种新的视觉定位框架，通过结合多种潜在表达和视觉细节来解决视觉与文本信息的不匹配问题。</li>
<li>引入主体分布器和视觉概念注入器模块，将共享主体和独特属性概念嵌入到潜在表示中，从而捕捉独特的目标特定视觉线索。</li>
<li>提出了一种正向边距对比学习策略，将所有潜在表达与原始文本对齐，同时保留细微变化。</li>
<li>论文的方法在多个基准测试上表现出优异的性能，特别是在广义引用表达式分割（GRES）基准测试上。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05123">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3bb3559ce40f86bb1210f870eaa24148.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-93230b615fbc5169c6979697497918dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f2cb9397e789fb3011b3a508ef3b593.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MedMambaLite-Hardware-Aware-Mamba-for-Medical-Image-Classification"><a href="#MedMambaLite-Hardware-Aware-Mamba-for-Medical-Image-Classification" class="headerlink" title="MedMambaLite: Hardware-Aware Mamba for Medical Image Classification"></a>MedMambaLite: Hardware-Aware Mamba for Medical Image Classification</h2><p><strong>Authors:Romina Aalishah, Mozhgan Navardi, Tinoosh Mohsenin</strong></p>
<p>AI-powered medical devices have driven the need for real-time, on-device inference such as biomedical image classification. Deployment of deep learning models at the edge is now used for applications such as anomaly detection and classification in medical images. However, achieving this level of performance on edge devices remains challenging due to limitations in model size and computational capacity. To address this, we present MedMambaLite, a hardware-aware Mamba-based model optimized through knowledge distillation for medical image classification. We start with a powerful MedMamba model, integrating a Mamba structure for efficient feature extraction in medical imaging. We make the model lighter and faster in training and inference by modifying and reducing the redundancies in the architecture. We then distill its knowledge into a smaller student model by reducing the embedding dimensions. The optimized model achieves 94.5% overall accuracy on 10 MedMNIST datasets. It also reduces parameters 22.8x compared to MedMamba. Deployment on an NVIDIA Jetson Orin Nano achieves 35.6 GOPS&#x2F;J energy per inference. This outperforms MedMamba by 63% improvement in energy per inference. </p>
<blockquote>
<p>AI驱动的医疗器械推动了实时设备端推断的需求，例如生物医学图像分类。边缘部署深度学习模型现在被用于医学图像中的异常检测和分类等应用。然而，由于模型大小和计算能力的限制，在边缘设备上实现这一级别的性能仍然具有挑战性。为了解决这个问题，我们推出了MedMambaLite，这是一个基于硬件感知的Mamba模型，通过知识蒸馏针对医学图像分类进行了优化。我们从功能强大的MedMamba模型开始，集成了Mamba结构，实现医学成像中的高效特征提取。我们通过修改并减少架构中的冗余部分，使模型在训练和推断上更轻便、更快。然后，我们通过减小嵌入维度将其知识蒸馏到一个较小的学生模型中。优化后的模型在10个MedMNIST数据集上达到了94.5%的总体准确率，同时与MedMamba相比减少了参数高达22.8倍。在NVIDIA Jetson Orin Nano上的部署可实现每推断能耗为35.6 GOPS&#x2F;J。这在每推断能耗方面比MedMamba提高了63%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05049v1">PDF</a> 21st IEEE Biomedical Circuits and Systems Conference (BioCAS) 2025</p>
<p><strong>Summary</strong><br>     基于人工智能的医疗设备需要实时在设备上进行分析推断，如生物医学图像分类。边缘设备上部署深度学习模型用于医学图像中的异常检测和分类等应用。然而，由于模型大小和计算能力的限制，实现这一性能在边缘设备上仍具有挑战性。为解决此问题，我们提出了MedMambaLite模型，它是基于Mamba结构进行优化的硬件感知模型，用于医学图像分类。通过知识蒸馏技术，我们从强大的MedMamba模型出发，集成Mamba结构以高效提取医学图像特征。通过修改和减少架构中的冗余部分，使模型在训练和推断上更加轻便和快速。然后将知识蒸馏到较小的学生模型中，减少嵌入维度。优化后的模型在10个MedMNIST数据集上达到了94.5%的总体准确率，并且参数减少了22.8倍。在NVIDIA Jetson Orin Nano上进行部署时，每次推理的能量达到35.6 GOPS&#x2F;J，相较于MedMamba模型在能量使用效率上提高了63%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI驱动的医疗设备需要实时在设备上进行分析推断，如生物医学图像分类。</li>
<li>在边缘设备上部署深度学习模型面临模型大小和计算能力的挑战。</li>
<li>MedMambaLite模型是基于Mamba结构优化的硬件感知模型，用于医学图像分类。</li>
<li>通过知识蒸馏技术，优化后的模型实现了高效的特征提取和推理速度。</li>
<li>优化后的模型在MedMNIST数据集上达到了94.5%的总体准确率，参数大大减少。</li>
<li>MedMambaLite模型在NVIDIA Jetson Orin Nano上的部署实现了较高的能量使用效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05049">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8b26e14f66480d05a298d175bdd29de4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d9af9c90c8c8d75e8e0d6a0f7842235.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e306260e7f3b54baa489f2111ba2969a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c0fa3fa96b602b04f662d827c5f3b33.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Multimodal-Causal-Driven-Representation-Learning-for-Generalizable-Medical-Image-Segmentation"><a href="#Multimodal-Causal-Driven-Representation-Learning-for-Generalizable-Medical-Image-Segmentation" class="headerlink" title="Multimodal Causal-Driven Representation Learning for Generalizable   Medical Image Segmentation"></a>Multimodal Causal-Driven Representation Learning for Generalizable   Medical Image Segmentation</h2><p><strong>Authors:Xusheng Liang, Lihua Zhou, Nianxin Li, Miao Xu, Ziyang Song, Dong Yi, Jinlin Wu, Hongbin Liu, Jiebo Luo, Zhen Lei</strong></p>
<p>Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable zero-shot capabilities in various computer vision tasks. However, their application to medical imaging remains challenging due to the high variability and complexity of medical data. Specifically, medical images often exhibit significant domain shifts caused by various confounders, including equipment differences, procedure artifacts, and imaging modes, which can lead to poor generalization when models are applied to unseen domains. To address this limitation, we propose Multimodal Causal-Driven Representation Learning (MCDRL), a novel framework that integrates causal inference with the VLM to tackle domain generalization in medical image segmentation. MCDRL is implemented in two steps: first, it leverages CLIP’s cross-modal capabilities to identify candidate lesion regions and construct a confounder dictionary through text prompts, specifically designed to represent domain-specific variations; second, it trains a causal intervention network that utilizes this dictionary to identify and eliminate the influence of these domain-specific variations while preserving the anatomical structural information critical for segmentation tasks. Extensive experiments demonstrate that MCDRL consistently outperforms competing methods, yielding superior segmentation accuracy and exhibiting robust generalizability. </p>
<blockquote>
<p>视觉语言模型（VLMs），如CLIP，在各种计算机视觉任务中表现出了出色的零样本能力。然而，由于其应用于医学成像时面临的高可变性和复杂性，其应用仍然具有挑战性。具体来说，医学图像通常会出现由各种混杂因素引起的显著领域偏移，包括设备差异、程序伪影和成像模式，当模型应用于未见领域时，可能导致较差的泛化性能。为了解决这一局限性，我们提出了多模态因果驱动表示学习（MCDRL）这一新型框架，它将因果推理与VLM相结合，解决医学图像分割中的领域泛化问题。MCDRL分两步实现：首先，它利用CLIP的跨模态能力来识别候选病变区域，并通过文本提示构建混杂因素词典，这些文本提示专门用于表示领域特定的变化；其次，它训练一个因果干预网络，利用这个词典来识别和消除这些领域特定变化的影响，同时保留对分割任务至关重要的解剖结构信息。大量实验表明，MCDRL始终优于其他方法，具有更高的分割精度和稳健的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05008v1">PDF</a> Under Review</p>
<p><strong>Summary</strong><br>医学视觉语言模型（如CLIP）在计算机视觉任务中展现出强大的零样本能力，但在医学成像应用上仍面临挑战。针对医学图像领域的高变性和复杂性，提出多模态因果驱动表示学习（MCDRL）框架，结合因果推理与VLM解决医学图像分割中的领域泛化问题。MCDRL通过CLIP的跨模态能力识别候选病变区域，构建代表领域特定变化的干扰字典，并训练因果干预网络利用该字典消除领域特定变化的影响，同时保留对分割任务至关重要的解剖结构信息。实验证明MCDRL在分割准确性和泛化性方面均表现优越。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMs在计算机视觉任务中表现出强大的零样本能力，但在医学成像应用上仍面临高变性和复杂性的挑战。</li>
<li>MCDRL框架结合因果推理与VLM解决医学图像分割中的领域泛化问题。</li>
<li>MCDRL通过CLIP的跨模态能力识别候选病变区域并构建干扰字典，以代表领域特定的变化。</li>
<li>MCDRL利用构建的干扰字典训练因果干预网络，以消除领域特定变化的影响。</li>
<li>MCDRL在分割准确性方面表现出优越性能，包括在未见领域的泛化能力。</li>
<li>MCDRL能够同时保留对分割任务至关重要的解剖结构信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05008">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-74f25f73a82a947bda03d679c70f7bea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb8cd88ff77e00ce185898cfeb56f95c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a3f5f1d594d7ffc1d0d3fec0b2e8f4b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b2f06b3d3ec69174b4476bbbee23b000.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5e32283f1303fc48a7d588b9d5d91a6.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Policy-to-Assist-Iteratively-Local-Segmentation-Optimising-Modality-and-Location-Selection-for-Prostate-Cancer-Localisation"><a href="#Policy-to-Assist-Iteratively-Local-Segmentation-Optimising-Modality-and-Location-Selection-for-Prostate-Cancer-Localisation" class="headerlink" title="Policy to Assist Iteratively Local Segmentation: Optimising Modality and   Location Selection for Prostate Cancer Localisation"></a>Policy to Assist Iteratively Local Segmentation: Optimising Modality and   Location Selection for Prostate Cancer Localisation</h2><p><strong>Authors:Xiangcen Wu, Shaheer U. Saeed, Yipei Wang, Ester Bonmati Coll, Yipeng Hu</strong></p>
<p>Radiologists often mix medical image reading strategies, including inspection of individual modalities and local image regions, using information at different locations from different images independently as well as concurrently. In this paper, we propose a recommend system to assist machine learning-based segmentation models, by suggesting appropriate image portions along with the best modality, such that prostate cancer segmentation performance can be maximised. Our approach trains a policy network that assists tumor localisation, by recommending both the optimal imaging modality and the specific sections of interest for review. During training, a pre-trained segmentation network mimics radiologist inspection on individual or variable combinations of these imaging modalities and their sections - selected by the policy network. Taking the locally segmented regions as an input for the next step, this dynamic decision making process iterates until all cancers are best localised. We validate our method using a data set of 1325 labelled multiparametric MRI images from prostate cancer patients, demonstrating its potential to improve annotation efficiency and segmentation accuracy, especially when challenging pathology is present. Experimental results show that our approach can surpass standard segmentation networks. Perhaps more interestingly, our trained agent independently developed its own optimal strategy, which may or may not be consistent with current radiologist guidelines such as PI-RADS. This observation also suggests a promising interactive application, in which the proposed policy networks assist human radiologists. </p>
<blockquote>
<p>放射科医生在医学图像阅读策略上通常会混合使用多种方法，包括检查各种单一模态和局部图像区域，独立地同时使用不同位置的不同图像的信息。在本文中，我们提出了一种推荐系统，以协助基于机器学习的分割模型，通过建议适当的图像部分以及最佳模态，最大限度地提高前列腺癌分割的性能。我们的方法训练了一个策略网络，通过推荐最佳的成像模态和特定审查的感兴趣部分，来协助肿瘤定位。在训练过程中，预训练的分割网络模仿放射科医生对单一或这些成像模态及其部分的组合的检查，这些部分由策略网络选择。以局部分割区域作为下一步的输入，这种动态决策过程会不断迭代，直到所有癌症的最佳定位。我们使用包含来自前列腺癌患者的1325个标记的多参数MRI图像数据集验证了我们的方法，证明了其在提高注释效率和分割精度方面的潜力，特别是在存在挑战性病理的情况下。实验结果表明，我们的方法可以超越标准分割网络。更有趣的是，我们的训练代理独立地开发了自己的最佳策略，这可能一致也可能与当前的放射科医生指南（如PI-RADS）不一致。这一观察结果也表明了一个有前景的交互应用，即所提出的策略网络可以辅助人类放射科医生。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03953v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种推荐系统，辅助机器学习分割模型进行前列腺癌分割，通过推荐适当的图像部分及最佳模态来最大化分割性能。该系统训练一个策略网络，辅助肿瘤定位，并推荐最佳的成像模态和特定感兴趣区域进行审查。通过模拟放射科医生对各种成像模态及其区域的检查来选择策略网络推荐的局部分割区域进行训练。此方法可提高标注效率和分割精度，特别是在存在挑战性病理的情况下。实验结果表明，该方法可超越标准分割网络性能，并且独立发展出可能与当前放射科医生指南一致或不一致的最优策略，展现出良好的交互式应用前景。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文中提出了一种新的推荐系统，旨在辅助机器学习模型在医疗图像分析中的性能，特别是在前列腺癌的分割上。</li>
<li>该系统通过训练一个策略网络来推荐最佳的成像模态和图像区域，帮助定位肿瘤。</li>
<li>策略网络是在模拟放射科医生对成像模态及其区域的检查过程中进行训练的。</li>
<li>该方法能提高标注效率和分割精度，特别是在面对具有挑战性的病理情况时。</li>
<li>实验结果显示，该方法的性能超越了标准的分割网络。</li>
<li>训练出的策略网络能够独立发展出最优策略，这些策略可能与现有的放射科医生指南一致或不一致。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03953">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ac23e18f201a738fcb202f84f1b544c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-382fd1860f7ef57805a0b9ab1456de65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a53a5550d8302cfaea703c03f6a72bd.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SAM2-UNeXT-An-Improved-High-Resolution-Baseline-for-Adapting-Foundation-Models-to-Downstream-Segmentation-Tasks"><a href="#SAM2-UNeXT-An-Improved-High-Resolution-Baseline-for-Adapting-Foundation-Models-to-Downstream-Segmentation-Tasks" class="headerlink" title="SAM2-UNeXT: An Improved High-Resolution Baseline for Adapting Foundation   Models to Downstream Segmentation Tasks"></a>SAM2-UNeXT: An Improved High-Resolution Baseline for Adapting Foundation   Models to Downstream Segmentation Tasks</h2><p><strong>Authors:Xinyu Xiong, Zihuang Wu, Lei Zhang, Lei Lu, Ming Li, Guanbin Li</strong></p>
<p>Recent studies have highlighted the potential of adapting the Segment Anything Model (SAM) for various downstream tasks. However, constructing a more powerful and generalizable encoder to further enhance performance remains an open challenge. In this work, we propose SAM2-UNeXT, an advanced framework that builds upon the core principles of SAM2-UNet while extending the representational capacity of SAM2 through the integration of an auxiliary DINOv2 encoder. By incorporating a dual-resolution strategy and a dense glue layer, our approach enables more accurate segmentation with a simple architecture, relaxing the need for complex decoder designs. Extensive experiments conducted on four benchmarks, including dichotomous image segmentation, camouflaged object detection, marine animal segmentation, and remote sensing saliency detection, demonstrate the superior performance of our proposed method. The code is available at <a target="_blank" rel="noopener" href="https://github.com/WZH0120/SAM2-UNeXT">https://github.com/WZH0120/SAM2-UNeXT</a>. </p>
<blockquote>
<p>最近的研究已经强调了适应Segment Anything Model（SAM）对各种下游任务的潜力。然而，构建一个更强大、更具通用性的编码器，以进一步提高性能，仍然是一个开放性的挑战。在这项工作中，我们提出了SAM2-UNeXT，这是一个先进的框架，它基于SAM2-UNet的核心原则，并通过集成辅助DINOv2编码器扩展了SAM2的代表性容量。通过采用双分辨率策略和密集粘合层，我们的方法能够在简单的架构上实现更精确的分割，减轻了复杂解码器设计的必要性。在四项基准测试上的广泛实验包括二值图像分割、伪装目标检测、海洋动物分割和遥感显著性检测，证明了我们提出的方法的优越性。代码可在<a target="_blank" rel="noopener" href="https://github.com/WZH0">https://github.com/WZH0</a> 0 &#x2F;SAM2-UNeXT找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03566v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>SAM2-UNeXT框架的提出，基于SAM的核心原理，结合了SAM2-UNet的优势，并通过引入辅助的DINOv2编码器扩展了代表容量。利用双分辨率策略和稠密粘着层，我们能够实现更精确的分割，简化了架构，减少了复杂解码器设计的需要。在四个基准测试中表现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SAM2-UNeXT是基于Segment Anything Model（SAM）构建的先进框架。</li>
<li>通过引入DINOv2编码器扩展了代表容量。</li>
<li>采用双分辨率策略以提高分割准确性。</li>
<li>利用稠密粘着层简化架构，降低复杂解码器设计的需求。</li>
<li>在四个基准测试中表现出卓越性能，包括二值图像分割、隐蔽目标检测、海洋动物分割和遥感显著性检测。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03566">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fed2642c93a9db8083996637fff45974.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37d73a9add551c507aefbcaf432954ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f4c8b79a852b312e9844a06e914dfbe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90830baf66a1c444f1a5d64df9ff6cff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6410ff683ef8db54f598d500e218f40b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4363cad44151a296f23904cf0a6530a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03339466c803ccfa47ca69e7b5b7ea41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3517b5a5e49487dc0c80ca14d2b73e41.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MAUP-Training-free-Multi-center-Adaptive-Uncertainty-aware-Prompting-for-Cross-domain-Few-shot-Medical-Image-Segmentation"><a href="#MAUP-Training-free-Multi-center-Adaptive-Uncertainty-aware-Prompting-for-Cross-domain-Few-shot-Medical-Image-Segmentation" class="headerlink" title="MAUP: Training-free Multi-center Adaptive Uncertainty-aware Prompting   for Cross-domain Few-shot Medical Image Segmentation"></a>MAUP: Training-free Multi-center Adaptive Uncertainty-aware Prompting   for Cross-domain Few-shot Medical Image Segmentation</h2><p><strong>Authors:Yazhou Zhu, Haofeng Zhang</strong></p>
<p>Cross-domain Few-shot Medical Image Segmentation (CD-FSMIS) is a potential solution for segmenting medical images with limited annotation using knowledge from other domains. The significant performance of current CD-FSMIS models relies on the heavily training procedure over other source medical domains, which degrades the universality and ease of model deployment. With the development of large visual models of natural images, we propose a training-free CD-FSMIS model that introduces the Multi-center Adaptive Uncertainty-aware Prompting (MAUP) strategy for adapting the foundation model Segment Anything Model (SAM), which is trained with natural images, into the CD-FSMIS task. To be specific, MAUP consists of three key innovations: (1) K-means clustering based multi-center prompts generation for comprehensive spatial coverage, (2) uncertainty-aware prompts selection that focuses on the challenging regions, and (3) adaptive prompt optimization that can dynamically adjust according to the target region complexity. With the pre-trained DINOv2 feature encoder, MAUP achieves precise segmentation results across three medical datasets without any additional training compared with several conventional CD-FSMIS models and training-free FSMIS model. The source code is available at: <a target="_blank" rel="noopener" href="https://github.com/YazhouZhu19/MAUP">https://github.com/YazhouZhu19/MAUP</a>. </p>
<blockquote>
<p>跨域小样本医学图像分割（CD-FSMIS）是利用其他领域的知识对医学图像进行有限标注分割的一种潜在解决方案。当前CD-FSMIS模型的出色性能依赖于其他源医学领域的繁重训练过程，这降低了模型的通用性和部署的便捷性。随着自然图像大型视觉模型的发展，我们提出了一种无需训练的CD-FSMIS模型，该模型引入了多中心自适应不确定性感知提示（MAUP）策略，以适应基础模型——用自然图像训练的分割任何模型（SAM），用于CD-FSMIS任务。具体来说，MAUP包括三个关键创新点：（1）基于K-means聚类的多中心提示生成，实现全面的空间覆盖；（2）关注困难区域的不确定性感知提示选择；（3）可根据目标区域复杂性进行动态调整的自适应提示优化。与几个传统的CD-FSMIS模型和无需训练的FSMIS模型相比，使用预训练的DINOv2特征编码器，MAUP在三个医学数据集上实现了精确的分割结果。源代码可在：<a target="_blank" rel="noopener" href="https://github.com/YazhouZhu19/MAUP%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/YazhouZhu19/MAUP获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03511v1">PDF</a> Accepted by MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了跨域小样医疗图像分割（CD-FSMIS）的问题，并提出了一种基于预训练的自然图像模型SAM的无需训练的新方法MAUP。该方法通过K-means聚类生成多中心提示、不确定性感知提示选择和自适应提示优化等技术，实现了对医疗图像的精准分割，无需额外训练即可在不同医疗数据集上取得良好效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CD-FSMIS模型利用其他领域的知识对医疗图像进行分割，但现有模型需要大量源医疗领域的训练数据，影响其通用性和部署的便捷性。</li>
<li>提出了一种基于预训练的自然图像模型SAM的无需训练的CD-FSMIS模型MAUP，解决了上述问题。</li>
<li>MAUP策略包括三个关键创新点：基于K-means聚类的多中心提示生成、不确定性感知提示选择和自适应提示优化。</li>
<li>多中心提示生成实现了全面的空间覆盖；不确定性感知提示选择关注于挑战区域；自适应提示优化能根据目标区域的复杂性进行动态调整。</li>
<li>MAUP利用预训练的DINOv2特征编码器，实现了在三个医疗数据集上的精准分割结果。</li>
<li>与传统的CD-FSMIS模型和无需训练的FSMIS模型相比，MAUP取得了更好的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03511">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c4ce8753932600e79a64a29fef59d5b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88d91333fa6c10dbe48a67f52ca48a5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9d4ca7caf0a1ae93150454786c425bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0dd224fe8792e5aafd4a68b1dc59b640.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Prototype-Enhanced-Confidence-Modeling-for-Cross-Modal-Medical-Image-Report-Retrieval"><a href="#Prototype-Enhanced-Confidence-Modeling-for-Cross-Modal-Medical-Image-Report-Retrieval" class="headerlink" title="Prototype-Enhanced Confidence Modeling for Cross-Modal Medical   Image-Report Retrieval"></a>Prototype-Enhanced Confidence Modeling for Cross-Modal Medical   Image-Report Retrieval</h2><p><strong>Authors:Shreyank N Gowda, Xiaobo Jin, Christian Wagner</strong></p>
<p>In cross-modal retrieval tasks, such as image-to-report and report-to-image retrieval, accurately aligning medical images with relevant text reports is essential but challenging due to the inherent ambiguity and variability in medical data. Existing models often struggle to capture the nuanced, multi-level semantic relationships in radiology data, leading to unreliable retrieval results. To address these issues, we propose the Prototype-Enhanced Confidence Modeling (PECM) framework, which introduces multi-level prototypes for each modality to better capture semantic variability and enhance retrieval robustness. PECM employs a dual-stream confidence estimation that leverages prototype similarity distributions and an adaptive weighting mechanism to control the impact of high-uncertainty data on retrieval rankings. Applied to radiology image-report datasets, our method achieves significant improvements in retrieval precision and consistency, effectively handling data ambiguity and advancing reliability in complex clinical scenarios. We report results on multiple different datasets and tasks including fully supervised and zero-shot retrieval obtaining performance gains of up to 10.17%, establishing in new state-of-the-art. </p>
<blockquote>
<p>在多模态检索任务（如图像到报告和报告到图像的检索）中，准确对齐医学图像与相关的文本报告是至关重要的，但由于医学数据固有的模糊性和变化性，这具有挑战性。现有模型往往难以捕捉放射数据中细微的多级语义关系，导致检索结果不可靠。为了解决这个问题，我们提出了原型增强信心建模（PECM）框架，它引入每个模态的多级原型，以更好地捕捉语义变化，提高检索的稳健性。PECM采用双流信心估计，利用原型相似性分布和自适应加权机制来控制高不确定性数据对检索排名的影响。我们的方法应用于放射学图像报告数据集，在检索精度和一致性方面取得了显著改进，有效地处理了数据模糊性，提高了复杂临床场景中的可靠性。我们在多个不同的数据集和任务上报告了结果，包括全监督和零射击检索，性能提升高达10.17%，创下了新的最先进的记录。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03494v1">PDF</a> </p>
<p><strong>Summary</strong><br>医学跨模态检索任务中，准确对齐医学图像和相关文本报告至关重要但具有挑战性。为应对挑战，提出一种名为PECM（原型增强信心建模）的框架，引入多级别原型以捕获语义变化并增强检索稳健性。通过原型相似性分布和自适应加权机制进行双流信心估计，提高检索精度和一致性，有效处理数据模糊性，并在复杂临床场景中提高可靠性。在放射学图像报告数据集上应用，取得了显著的性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学跨模态检索任务面临准确对齐医学图像和文本报告的挑战。</li>
<li>现有模型难以捕获放射学数据中的多级别语义关系。</li>
<li>PECM框架引入多级别原型以更好地捕获语义变化和增强检索稳健性。</li>
<li>PECM采用双流信心估计，利用原型相似性分布和自适应加权机制。</li>
<li>PECM提高了检索精度和一致性，有效处理数据模糊性。</li>
<li>在复杂临床场景中，PECM提高了可靠性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03494">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-03a5ba29aa4a2269aaccf2081cac129b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e714fd3b48c3dad47d13851151a48ca6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-937708ea01a9695b629387bde201d24f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b98f80462942dae1eb786aa61c1c4305.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MedCAL-Bench-A-Comprehensive-Benchmark-on-Cold-Start-Active-Learning-with-Foundation-Models-for-Medical-Image-Analysis"><a href="#MedCAL-Bench-A-Comprehensive-Benchmark-on-Cold-Start-Active-Learning-with-Foundation-Models-for-Medical-Image-Analysis" class="headerlink" title="MedCAL-Bench: A Comprehensive Benchmark on Cold-Start Active Learning   with Foundation Models for Medical Image Analysis"></a>MedCAL-Bench: A Comprehensive Benchmark on Cold-Start Active Learning   with Foundation Models for Medical Image Analysis</h2><p><strong>Authors:Ning Zhu, Xiaochuan Ma, Shaoting Zhang, Guotai Wang</strong></p>
<p>Cold-Start Active Learning (CSAL) aims to select informative samples for annotation without prior knowledge, which is important for improving annotation efficiency and model performance under a limited annotation budget in medical image analysis. Most existing CSAL methods rely on Self-Supervised Learning (SSL) on the target dataset for feature extraction, which is inefficient and limited by insufficient feature representation. Recently, pre-trained Foundation Models (FMs) have shown powerful feature extraction ability with a potential for better CSAL. However, this paradigm has been rarely investigated, with a lack of benchmarks for comparison of FMs in CSAL tasks. To this end, we propose MedCAL-Bench, the first systematic FM-based CSAL benchmark for medical image analysis. We evaluate 14 FMs and 7 CSAL strategies across 7 datasets under different annotation budgets, covering classification and segmentation tasks from diverse medical modalities. It is also the first CSAL benchmark that evaluates both the feature extraction and sample selection stages. Our experimental results reveal that: 1) Most FMs are effective feature extractors for CSAL, with DINO family performing the best in segmentation; 2) The performance differences of these FMs are large in segmentation tasks, while small for classification; 3) Different sample selection strategies should be considered in CSAL on different datasets, with Active Learning by Processing Surprisal (ALPS) performing the best in segmentation while RepDiv leading for classification. The code is available at <a target="_blank" rel="noopener" href="https://github.com/HiLab-git/MedCAL-Bench">https://github.com/HiLab-git/MedCAL-Bench</a>. </p>
<blockquote>
<p>冷启动主动学习（CSAL）旨在在没有先验知识的情况下选择信息样本进行标注，这对于在医疗图像分析中使用有限的标注预算提高标注效率和模型性能至关重要。大多数现有的CSAL方法依赖于目标数据集上的自监督学习（SSL）进行特征提取，这是低效的，并且受到特征表示不足的限制。最近，预训练的Foundation Models（FMs）显示出强大的特征提取能力，具有更好的CSAL潜力。然而，这一范式很少受到研究，缺乏用于比较FMs在CSAL任务中的基准测试。为此，我们提出了MedCAL-Bench，这是基于FMs的医疗图像分析的首个系统性CSAL基准测试。我们在7个数据集上评估了14个FMs和7种CSAL策略，涉及不同标注预算下的分类和分割任务，涵盖多种医学模态。它也是第一个同时评估特征提取和样本选择阶段的CSAL基准测试。我们的实验结果揭示：1）大多数FMs对于CSAL都是有效的特征提取器，DINO系列在分割方面表现最佳；2）这些FMs在分割任务中的性能差异很大，而在分类任务中则较小；3）CSAL中的不同样本选择策略应针对不同的数据集进行考虑，处理惊喜（ALPS）在分割方面表现最佳，而RepDiv在分类方面领先。代码可在<a target="_blank" rel="noopener" href="https://github.com/HiLab-git/MedCAL-Bench">https://github.com/HiLab-git/MedCAL-Bench</a>获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03441v1">PDF</a> 23 pages, 6 figures, 10 tables</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对医学图像分析的冷启动主动学习方法（CSAL）的挑战和现状。为提高标注效率和模型性能，研究者提出了基于预训练基础模型（FMs）的CSAL新方法，并建立了首个系统性的FM-based CSAL基准测试平台MedCAL-Bench。实验结果显示，大多数基础模型在CSAL中能有效提取特征，不同数据集上的样本选择策略也有差异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>冷启动主动学习方法（CSAL）旨在在没有先验知识的情况下选择信息样本进行标注，以提高医学图像分析的标注效率和模型性能。</li>
<li>现有CSAL方法大多依赖目标数据集的自我监督学习（SSL）进行特征提取，但这种方法效率低下，且特征表示有限。</li>
<li>预训练基础模型（FMs）具有强大的特征提取能力，在CSAL中有潜在优势。</li>
<li>MedCAL-Bench是首个基于FM的CSAL基准测试平台，用于医学图像分析。</li>
<li>实验结果显示，大多数基础模型在CSAL中能有效提取特征，DINO家族在分割任务中表现最佳。</li>
<li>分割任务中基础模型的性能差异较大，分类任务则较小。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03441">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-17de953125d36d379a8a181d2ee46c14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6b0ce134ea92ca2627a93aabfa7126c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2dcfdbb520e75b30590bdee24c770681.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6de8b4cb73d02ce4bb9fe59105ed730f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dda582b066f01f4196a7cc045d968d11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62bb34848ad0d3af3b9af848623a496f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77848ba007867b296f351bc6fc2ba2bb.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="R2GenKG-Hierarchical-Multi-modal-Knowledge-Graph-for-LLM-based-Radiology-Report-Generation"><a href="#R2GenKG-Hierarchical-Multi-modal-Knowledge-Graph-for-LLM-based-Radiology-Report-Generation" class="headerlink" title="R2GenKG: Hierarchical Multi-modal Knowledge Graph for LLM-based   Radiology Report Generation"></a>R2GenKG: Hierarchical Multi-modal Knowledge Graph for LLM-based   Radiology Report Generation</h2><p><strong>Authors:Futian Wang, Yuhan Qiao, Xiao Wang, Fuling Wang, Yuxiang Zhang, Dengdi Sun</strong></p>
<p>X-ray medical report generation is one of the important applications of artificial intelligence in healthcare. With the support of large foundation models, the quality of medical report generation has significantly improved. However, challenges such as hallucination and weak disease diagnostic capability still persist. In this paper, we first construct a large-scale multi-modal medical knowledge graph (termed M3KG) based on the ground truth medical report using the GPT-4o. It contains 2477 entities, 3 kinds of relations, 37424 triples, and 6943 disease-aware vision tokens for the CheXpert Plus dataset. Then, we sample it to obtain multi-granularity semantic graphs and use an R-GCN encoder for feature extraction. For the input X-ray image, we adopt the Swin-Transformer to extract the vision features and interact with the knowledge using cross-attention. The vision tokens are fed into a Q-former and retrieved the disease-aware vision tokens using another cross-attention. Finally, we adopt the large language model to map the semantic knowledge graph, input X-ray image, and disease-aware vision tokens into language descriptions. Extensive experiments on multiple datasets fully validated the effectiveness of our proposed knowledge graph and X-ray report generation framework. The source code of this paper will be released on <a target="_blank" rel="noopener" href="https://github.com/Event-AHU/Medical_Image_Analysis">https://github.com/Event-AHU/Medical_Image_Analysis</a>. </p>
<blockquote>
<p>X射线医学报告生成是人工智能在医疗保健领域的重要应用之一。在大规模基础模型的支持下，医学报告生成的质量得到了显著提升。然而，诸如虚构和弱疾病诊断能力等挑战仍然存在。在本文中，我们首先基于真实医学报告构建了一个大规模的多模式医学知识图谱（称为M3KG），使用GPT-4o针对CheXpert Plus数据集进行操作，它包含2477个实体、3种关系、37424个三元组和6943个疾病感知视觉标记。然后，我们对它进行采样以获得多粒度语义图，并使用R-GCN编码器进行特征提取。对于输入的X射线图像，我们采用Swin-Transformer提取视觉特征，并使用交叉注意力与知识进行交互。视觉标记被输入到Q-former中，并使用另一个交叉注意力检索疾病感知视觉标记。最后，我们采用大型语言模型将语义知识图谱、输入的X射线图像和疾病感知视觉标记映射到语言描述中。在多个数据集上进行的广泛实验充分验证了我们提出的知识图谱和X射线报告生成框架的有效性。本文的源代码将在<a target="_blank" rel="noopener" href="https://github.com/Event-AHU/Medical_Image_Analysis%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Event-AHU/Medical_Image_Analysis上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03426v1">PDF</a> </p>
<p><strong>摘要</strong><br>    基于大型基础模型的支持，利用多模态医学知识图谱（M3KG）和Swin-Transformer等技术，本文提出了一种改进X光报告生成的方法。通过构建大规模医学知识图谱，结合X光图像特征提取和跨注意力交互，实现了高质量的医学报告生成。实验证明该方法的有效性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>X光医学报告生成是人工智能在医疗保健中的重要应用之一。</li>
<li>利用大型基础模型的支持，医学报告生成的质量已显著提高。<br>3.仍存在诸如幻觉和弱疾病诊断能力等挑战。</li>
<li>本文构建了基于真实医学报告的多模态医学知识图谱（M3KG）。</li>
<li>通过采样获得多粒度语义图，并使用R-GCN编码器进行特征提取。</li>
<li>采用Swin-Transformer提取X光图像特征，并通过跨注意力与知识交互。</li>
<li>结合大型语言模型，将语义知识图谱、X光图像和疾病感知视觉标记转化为语言描述。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03426">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-20cabdfe0aedeed2d29b07de38b42f1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffefc3fbe5876c2b1f2dab80a9f39823.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ceeab43741aad5c9c61aa68c989af697.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30bb45e4dd942e1c66956553cc3d169f.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Nexus-INR-Diverse-Knowledge-guided-Arbitrary-Scale-Multimodal-Medical-Image-Super-Resolution"><a href="#Nexus-INR-Diverse-Knowledge-guided-Arbitrary-Scale-Multimodal-Medical-Image-Super-Resolution" class="headerlink" title="Nexus-INR: Diverse Knowledge-guided Arbitrary-Scale Multimodal Medical   Image Super-Resolution"></a>Nexus-INR: Diverse Knowledge-guided Arbitrary-Scale Multimodal Medical   Image Super-Resolution</h2><p><strong>Authors:Bo Zhang, JianFei Huo, Zheng Zhang, Wufan Wang, Hui Gao, Xiangyang Gong, Wendong Wang</strong></p>
<p>Arbitrary-resolution super-resolution (ARSR) provides crucial flexibility for medical image analysis by adapting to diverse spatial resolutions. However, traditional CNN-based methods are inherently ill-suited for ARSR, as they are typically designed for fixed upsampling factors. While INR-based methods overcome this limitation, they still struggle to effectively process and leverage multi-modal images with varying resolutions and details. In this paper, we propose Nexus-INR, a Diverse Knowledge-guided ARSR framework, which employs varied information and downstream tasks to achieve high-quality, adaptive-resolution medical image super-resolution. Specifically, Nexus-INR contains three key components. A dual-branch encoder with an auxiliary classification task to effectively disentangle shared anatomical structures and modality-specific features; a knowledge distillation module using cross-modal attention that guides low-resolution modality reconstruction with high-resolution reference, enhanced by self-supervised consistency loss; an integrated segmentation module that embeds anatomical semantics to improve both reconstruction quality and downstream segmentation performance. Experiments on the BraTS2020 dataset for both super-resolution and downstream segmentation demonstrate that Nexus-INR outperforms state-of-the-art methods across various metrics. </p>
<blockquote>
<p>任意分辨率超分辨率（ARSR）通过适应不同空间分辨率，为医学图像分析提供了关键的灵活性。然而，基于传统CNN的方法本质上不适合ARSR，因为它们通常设计为固定放大倍数。虽然INR方法克服了这一局限性，但它们仍然难以有效处理和利用具有不同分辨率和细节的多模态图像。在本文中，我们提出了Nexus-INR，一个以多样知识引导的ARSR框架，它利用不同的信息和下游任务来实现高质量、自适应分辨率的医学图像超分辨率。具体来说，Nexus-INR包含三个关键组件。一个具有辅助分类任务的双分支编码器，有效地分离共享解剖结构和模态特定特征；一个使用跨模态注意力的知识蒸馏模块，该模块以高分辨率参考引导低分辨率模态的重构，通过自监督一致性损失增强；一个集成分割模块，嵌入解剖语义以提高重建质量和下游分割性能。在BraTS2020数据集上的超分辨率和下游分割实验表明，Nexus-INR在各项指标上均优于最新方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03073v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一个名为Nexus-INR的灵活超分辨率框架，用于实现高质量、自适应分辨率的医疗图像超分辨率。该框架结合了多种技术和策略，包括双分支编码器、知识蒸馏模块和集成分割模块，以实现有效处理不同模态和不同分辨率的医疗图像的目标。其在BraTS2020数据集上的实验结果证明了其超越现有方法的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ARSR为医学图像分析提供了重要的灵活性，能够适应不同的空间分辨率。</li>
<li>传统CNN方法对于ARSR任务来说存在局限性，因为它们通常针对固定的上采样因子设计。</li>
<li>INR方法克服了CNN的局限性，但在处理多模态图像时仍面临挑战。</li>
<li>Nexus-INR框架包含三个关键组件：双分支编码器、知识蒸馏模块和集成分割模块。</li>
<li>双分支编码器通过辅助分类任务有效地分离共享解剖结构和模态特定特征。</li>
<li>知识蒸馏模块利用跨模态注意力来指导低分辨率模态的重构，同时结合高分辨率参考和自我监督的一致性损失。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03073">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b75c9316716618eb252eefd377ee11f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bcc68d914ca73c8270481a50dbfb4846.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25a2c8b50be14aa5a8045973ce32c6f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-96e2522f604378a80512c4c840dea0d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f4437b7e897ad61e0186d98f99bca0f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="SSFMamba-Symmetry-driven-Spatial-Frequency-Feature-Fusion-for-3D-Medical-Image-Segmentation"><a href="#SSFMamba-Symmetry-driven-Spatial-Frequency-Feature-Fusion-for-3D-Medical-Image-Segmentation" class="headerlink" title="SSFMamba: Symmetry-driven Spatial-Frequency Feature Fusion for 3D   Medical Image Segmentation"></a>SSFMamba: Symmetry-driven Spatial-Frequency Feature Fusion for 3D   Medical Image Segmentation</h2><p><strong>Authors:Bo Zhang, Yifan Zhang, Shuo Yan, Yu Bai, Zheng Zhang, Wu Liu, Xiuzhuang Zhou, Wendong Wang</strong></p>
<p>In light of the spatial domain’s limited capacity for modeling global context in 3D medical image segmentation, emerging approaches have begun to incorporate frequency domain representations. However, straightforward feature extraction strategies often overlook the unique properties of frequency domain information, such as conjugate symmetry. They also fail to account for the fundamental differences in data distribution between the spatial and frequency domains, which can ultimately dilute or obscure the complementary strengths that frequency-based representations offer. In this paper, we propose SSFMamba, a Mamba based Symmetry-driven Spatial-Frequency feature fusion network for 3D medical image segmentation. SSFMamba employs a complementary dual-branch architecture that extracts features from both the spatial and frequency domains, and leverages a Mamba block to fuse these heterogeneous features to preserve global context while reinforcing local details. In the frequency domain branch, we harness Mamba’s exceptional capability to extract global contextual information in conjunction with the synergistic effect of frequency domain features to further enhance global modeling. Moreover, we design a 3D multi-directional scanning mechanism to strengthen the fusion of local and global cues. Extensive experiments on the BraTS2020 and BraTS2023 datasets demonstrate that our approach consistently outperforms state-of-the-art methods across various evaluation metrics. </p>
<blockquote>
<p>鉴于三维医学图像分割中空间域对全局上下文建模的有限能力，新兴方法开始融入频率域表示。然而，直接的特征提取策略往往忽略了频率域信息的独特属性，如共轭对称性。它们也未能考虑到空间域和频率域之间数据分布的根本差异，这最终可能会稀释或掩盖基于频率的表示所提供的互补优势。在本文中，我们提出了SSFMamba，这是一种基于Mamba的对称驱动空间-频率特征融合网络，用于三维医学图像分割。SSFMamba采用互补的双分支架构，从空间和频率域提取特征，并利用Mamba块融合这些异构特征，以保留全局上下文并强化局部细节。在频率域分支中，我们利用Mamba提取全局上下文信息的卓越能力，结合频率域特征的协同作用，进一步增强了全局建模。此外，我们设计了一种三维多方向扫描机制，以加强局部和全局线索的融合。在BraTS2020和BraTS2023数据集上的广泛实验表明，我们的方法在各种评估指标上均优于最新技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03069v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于Mamba的对称驱动时空融合网络SSFMamba，用于三维医学图像分割。该网络采用互补双分支架构，从时空域提取特征并使用Mamba块融合这些异构特征，以保留全局上下文并强化局部细节。在频率域分支中，结合Mamba提取全局上下文信息的卓越能力与频率域特征的协同作用，进一步增强全局建模。设计三维多方向扫描机制，加强局部和全局线索的融合。在BraTS2020和BraTS2023数据集上的实验表明，该方法在各项评估指标上均优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分割中，空间域在建模全局上下文时存在局限性，因此新兴方法开始结合频率域表示。</li>
<li>现有特征提取策略忽略了频率域信息的独特性质，如共轭对称性，并且未能考虑时空域间数据分布的基础差异。</li>
<li>SSFMamba网络采用双分支架构，分别从时空域提取特征，并使用Mamba块进行特征融合。</li>
<li>频率域分支利用Mamba提取全局上下文信息，并与频率域特征的协同作用结合，以增强全局建模。</li>
<li>SSFMamba设计了一个三维多方向扫描机制，强化局部和全局信息的融合。</li>
<li>在BraTS2020和BraTS2023数据集上的实验表明，SSFMBama方法性能卓越，优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03069">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-968b0e1d11e3648757ebe34bb0806cc4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84fe3f030495603dc52ae8025ad09fe9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4746e45bb28c62317e836f155130eb40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66b3bc822a8f706ce788c536c2900026.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63663d5dd17b6f008e7aaaf1e0939c90.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-65d1e068eda2867fe18e5d22ad33f55c.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-08-09  A Scalable Pipeline for Enabling Non-Verbal Speech Generation and   Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2d20e35a5c763131af23218456bdb7ac.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-08-09  GAP Gaussianize Any Point Clouds with Text Guidance
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29774.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
