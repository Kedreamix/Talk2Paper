<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-09  MM2CT MR-to-CT translation for multi-modal image fusion with mamba">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-c4f3381c9ca1a8d2d3266d998931c981.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-09-æ›´æ–°"><a href="#2025-08-09-æ›´æ–°" class="headerlink" title="2025-08-09 æ›´æ–°"></a>2025-08-09 æ›´æ–°</h1><h2 id="MM2CT-MR-to-CT-translation-for-multi-modal-image-fusion-with-mamba"><a href="#MM2CT-MR-to-CT-translation-for-multi-modal-image-fusion-with-mamba" class="headerlink" title="MM2CT: MR-to-CT translation for multi-modal image fusion with mamba"></a>MM2CT: MR-to-CT translation for multi-modal image fusion with mamba</h2><p><strong>Authors:Chaohui Gong, Zhiying Wu, Zisheng Huang, Gaofeng Meng, Zhen Lei, Hongbin Liu</strong></p>
<p>Magnetic resonance (MR)-to-computed tomography (CT) translation offers significant advantages, including the elimination of radiation exposure associated with CT scans and the mitigation of imaging artifacts caused by patient motion. The existing approaches are based on single-modality MR-to-CT translation, with limited research exploring multimodal fusion. To address this limitation, we introduce Multi-modal MR to CT (MM2CT) translation method by leveraging multimodal T1- and T2-weighted MRI data, an innovative Mamba-based framework for multi-modal medical image synthesis. Mamba effectively overcomes the limited local receptive field in CNNs and the high computational complexity issues in Transformers. MM2CT leverages this advantage to maintain long-range dependencies modeling capabilities while achieving multi-modal MR feature integration. Additionally, we incorporate a dynamic local convolution module and a dynamic enhancement module to improve MRI-to-CT synthesis. The experiments on a public pelvis dataset demonstrate that MM2CT achieves state-of-the-art performance in terms of Structural Similarity Index Measure (SSIM) and Peak Signal-to-Noise Ratio (PSNR). Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Gots-ch/MM2CT">https://github.com/Gots-ch/MM2CT</a>. </p>
<blockquote>
<p>ç£å…±æŒ¯ï¼ˆMRï¼‰åˆ°è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰çš„è½¬æ¢å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬æ¶ˆé™¤ä¸CTæ‰«æç›¸å…³çš„è¾å°„æš´éœ²ä»¥åŠå‡è½»ç”±æ‚£è€…è¿åŠ¨å¼•èµ·çš„æˆåƒä¼ªå½±ã€‚ç°æœ‰æ–¹æ³•åŸºäºå•æ¨¡æ€MR-to-CTè½¬æ¢ï¼Œå¯¹å¤šæ¨¡æ€èåˆçš„ç ”ç©¶æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šæ¨¡æ€MRåˆ°CTï¼ˆMM2CTï¼‰è½¬æ¢æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨T1åŠ æƒå’ŒT2åŠ æƒMRIæ•°æ®çš„å¤šæ¨¡æ€ä¿¡æ¯ï¼Œä»¥åŠåŸºäºMambaçš„å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆæˆåˆ›æ–°æ¡†æ¶ã€‚Mambaæœ‰æ•ˆåœ°å…‹æœäº†å·ç§¯ç¥ç»ç½‘ç»œä¸­å±€éƒ¨æ„Ÿå—é‡çš„é™åˆ¶å’Œå˜å‹å™¨ä¸­çš„é«˜è®¡ç®—å¤æ‚æ€§ã€‚MM2CTåˆ©ç”¨è¿™ä¸€ä¼˜åŠ¿ï¼Œåœ¨ä¿æŒé•¿æœŸä¾èµ–å»ºæ¨¡èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†å¤šæ¨¡æ€MRç‰¹å¾èåˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆäº†åŠ¨æ€å±€éƒ¨å·ç§¯æ¨¡å—å’ŒåŠ¨æ€å¢å¼ºæ¨¡å—ï¼Œä»¥æ”¹è¿›MRIåˆ°CTçš„åˆæˆã€‚åœ¨å…¬å…±éª¨ç›†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMM2CTåœ¨ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰å’Œå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Gots-ch/MM2CT%E5%85%AC%E5%BC%80%E6%8F%BD%E5%BE%AE%E3%80%82">https://github.com/Gots-ch/MM2CTå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05476v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†å¤šæ¨¡æ€MRåˆ°CTè½¬æ¢æ–¹æ³•çš„ç ”ç©¶ã€‚é€šè¿‡å¼•å…¥åŸºäºå¤šæ¨¡æ€MRIæ•°æ®çš„Mambaæ¡†æ¶ï¼Œå®ç°MRIåˆ°CTè½¬æ¢çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæé«˜æˆåƒè´¨é‡å¹¶å…‹æœç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¤šæ¨¡æ€MRIæ•°æ®çš„ä¼˜åŠ¿ï¼Œæœ‰æ•ˆå…‹æœäº†CNNçš„å±€éƒ¨æ„Ÿå—é‡é™åˆ¶å’ŒTransformerçš„é«˜è®¡ç®—å¤æ‚æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…¬å…±éª¨ç›†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRåˆ°CTè½¬æ¢èƒ½å¤Ÿæ¶ˆé™¤CTæ‰«æç›¸å…³çš„è¾å°„æš´éœ²å¹¶å‡å°‘ç”±æ‚£è€…è¿åŠ¨å¼•èµ·çš„æˆåƒä¼ªå½±ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦åŸºäºå•æ¨¡æ€MRåˆ°CTè½¬æ¢ï¼Œç ”ç©¶åœ¨æ¢ç´¢å¤šæ¨¡æ€èåˆæ–¹é¢æœ‰é™ã€‚</li>
<li>å¼•å…¥å¤šæ¨¡æ€MRåˆ°CTï¼ˆMM2CTï¼‰è½¬æ¢æ–¹æ³•ï¼Œåˆ©ç”¨T1å’ŒT2åŠ æƒMRIæ•°æ®ã€‚</li>
<li>MM2CTä½¿ç”¨Mambaæ¡†æ¶è¿›è¡Œå¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆæˆï¼Œå…‹æœäº†CNNçš„å±€éƒ¨æ„Ÿå—é‡å’ŒTransformerçš„é«˜è®¡ç®—å¤æ‚æ€§ã€‚</li>
<li>MM2CTç»“åˆåŠ¨æ€å±€éƒ¨å·ç§¯æ¨¡å—å’ŒåŠ¨æ€å¢å¼ºæ¨¡å—ï¼Œæé«˜äº†MRIåˆ°CTçš„åˆæˆæ•ˆæœã€‚</li>
<li>åœ¨å…¬å…±éª¨ç›†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMM2CTåœ¨ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ï¼ˆSSIMï¼‰å’Œå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æ–¹é¢è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05476">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e7653b56dc55ddf55476640f001343e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05b58b965e018b4b2a7c4541d3a8e502.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4f3381c9ca1a8d2d3266d998931c981.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2667ccce90a816c3df8a5a42b3da9f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f42bef444e54bc1aa5d86f6b6ffa95a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Look-Up-Table-Correction-for-Beam-Hardening-Induced-Signal-of-Clinical-Dark-Field-Chest-Radiographs"><a href="#Look-Up-Table-Correction-for-Beam-Hardening-Induced-Signal-of-Clinical-Dark-Field-Chest-Radiographs" class="headerlink" title="Look-Up Table-Correction for Beam Hardening-Induced Signal of Clinical   Dark-Field Chest Radiographs"></a>Look-Up Table-Correction for Beam Hardening-Induced Signal of Clinical   Dark-Field Chest Radiographs</h2><p><strong>Authors:Maximilian E. Lochschmidt, Theresa Urban, Lennard Kaster, Rafael Schick, Thomas Koehler, Daniela Pfeiffer, Franz Pfeiffer</strong></p>
<p>Background: Material structures at the micrometer scale cause ultra-small-angle X-ray scattering, e.g., seen in lung tissue or plastic foams. In grating-based X-ray imaging, this causes a reduction of the fringe visibility, forming a dark-field signal. Polychromatic beam hardening also changes visibility, adding a false dark-field signal due to attenuation, even in homogeneous, non-scattering materials. Purpose: The objective of this study is to develop a fast, simple, and robust method to correct dark-field signals and bony structures present due to beam hardening on dark-field chest radiographs of study participants. Methods: The method is based on calibration measurements and image processing. Beam hardening by bones and soft tissue is modeled by aluminum and water, respectively, which have no microstructure and thus only generate an artificial dark-field signal. Look-up tables were then created for both. By using a weighted mean of these, forming a single look-up table, and using the attenuation images, the artificial dark-field signal and thus the bone structures present are reduced for study participants. Results: It was found that applying a correction using a weighted look-up table leads to a significant reduction of bone structures in the dark-field image. The weighting of the aluminum component has a substantial impact on the degree to which bone structures remain visible in the dark-field image. Furthermore, a large negative bias in the dark-field image, dependent on the aluminum weighting, was successfully corrected. Conclusions: The beam-hardening-induced signal in the dark-field images was successfully reduced using the method described. The choice of aluminum weighting to suppress rib structures, as well as the selection of bias correction, should be evaluated based on the specific clinical question. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šå¾®ç±³å°ºåº¦çš„ææ–™ç»“æ„ä¼šå¯¼è‡´è¶…å°è§’åº¦Xå°„çº¿æ•£å°„ï¼Œä¾‹å¦‚åœ¨è‚ºç»„ç»‡æˆ–å¡‘æ–™æ³¡æ²«ä¸­ã€‚åœ¨åŸºäºå…‰æ …çš„Xå°„çº¿æˆåƒä¸­ï¼Œè¿™ä¼šå¯¼è‡´æ¡çº¹å¯è§åº¦é™ä½ï¼Œå½¢æˆæš—åœºä¿¡å·ã€‚å¤šè‰²å…‰æŸç¡¬åŒ–ä¹Ÿä¼šæ”¹å˜å¯è§åº¦ï¼Œå³ä½¿åœ¨å‡åŒ€ã€éæ•£å°„ææ–™ä¸­ï¼Œç”±äºè¡°å‡ä¹Ÿä¼šå¢åŠ è™šå‡çš„æš—åœºä¿¡å·ã€‚ç›®çš„ï¼šæœ¬ç ”ç©¶çš„ç›®æ ‡æ˜¯å¼€å‘ä¸€ç§å¿«é€Ÿã€ç®€å•ã€ç¨³å¥çš„æ–¹æ³•æ¥æ ¡æ­£ç”±äºå…‰æŸç¡¬åŒ–äº§ç”Ÿçš„æš—åœºä¿¡å·ä»¥åŠç ”ç©¶å‚ä¸è€…å­˜åœ¨çš„éª¨ç»“æ„ã€‚æ–¹æ³•ï¼šè¯¥æ–¹æ³•åŸºäºæ ¡å‡†æµ‹é‡å’Œå›¾åƒå¤„ç†ã€‚éª¨éª¼å’Œè½¯ç»„ç»‡å¼•èµ·çš„å…‰æŸç¡¬åŒ–åˆ†åˆ«é€šè¿‡é“å’Œæ°´è¿›è¡Œå»ºæ¨¡ï¼Œå®ƒä»¬æ²¡æœ‰å¾®è§‚ç»“æ„ï¼Œå› æ­¤åªäº§ç”Ÿäººå·¥æš—åœºä¿¡å·ã€‚ä¹‹åä¸ºè¿™ä¸¤è€…åˆ›å»ºäº†æŸ¥æ‰¾è¡¨ã€‚é€šè¿‡è®¡ç®—åŠ æƒå¹³å‡å€¼å½¢æˆå•ä¸€æŸ¥æ‰¾è¡¨ï¼Œå¹¶ä½¿ç”¨è¡°å‡å›¾åƒï¼Œå¯ä»¥å‡å°‘ç ”ç©¶å‚ä¸è€…çš„äººå·¥æš—åœºä¿¡å·åŠå…¶å­˜åœ¨çš„éª¨ç»“æ„ã€‚ç»“æœï¼šå‘ç°ä½¿ç”¨åŠ æƒæŸ¥æ‰¾è¡¨è¿›è¡Œæ ¡æ­£ä¼šå¯¼è‡´æš—åœºå›¾åƒä¸­çš„éª¨ç»“æ„æ˜¾è‘—å‡å°‘ã€‚é“æˆåˆ†çš„æƒé‡å¯¹æš—åœºå›¾åƒä¸­éª¨ç»“æ„çš„å¯è§åº¦æœ‰å¾ˆå¤§å½±å“ã€‚æ­¤å¤–ï¼ŒæˆåŠŸæ ¡æ­£äº†ä¾èµ–äºé“æƒé‡çš„æš—åœºå›¾åƒä¸­çš„å¤§è´Ÿåå·®ã€‚ç»“è®ºï¼šä½¿ç”¨æ‰€æè¿°çš„æ–¹æ³•æˆåŠŸå‡å°‘äº†æš—åœºå›¾åƒä¸­ç”±å…‰æŸç¡¬åŒ–å¼•èµ·çš„ä¿¡å·ã€‚æŠ‘åˆ¶è‚‹éª¨ç»“æ„çš„é“æƒé‡é€‰æ‹©ä»¥åŠåå·®æ ¡æ­£çš„é€‰æ‹©åº”æ ¹æ®å…·ä½“çš„ä¸´åºŠé—®é¢˜è¿›è¡Œè¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05422v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¾®ç±³çº§ææ–™ç»“æ„å¯¼è‡´è¶…å°è§’åº¦Xå°„çº¿æ•£å°„ï¼Œå¦‚åœ¨è‚ºç»„ç»‡æˆ–å¡‘æ–™æ³¡æ²«ä¸­æ‰€è§ã€‚åœ¨åŸºäºæ ¼æ …çš„Xå°„çº¿æˆåƒä¸­ï¼Œè¿™é™ä½äº†è¾¹ç¼˜å¯è§åº¦ï¼Œå½¢æˆæš—åœºä¿¡å·ã€‚å¤šè‰²å…‰æŸç¡¬åŒ–ä¹Ÿä¼šæ”¹å˜å¯è§åº¦ï¼Œåœ¨å‡è´¨ã€éæ•£å°„ææ–™ä¸­æ·»åŠ å› è¡°å‡è€Œäº§ç”Ÿçš„è™šå‡æš—åœºä¿¡å·ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§å¿«é€Ÿã€ç®€å•ã€ç¨³å¥çš„æ–¹æ³•æ¥ä¿®æ­£å› å…‰æŸç¡¬åŒ–äº§ç”Ÿçš„æš—åœºä¿¡å·å’Œéª¨è´¨ç»“æ„ï¼Œåº”ç”¨äºç ”ç©¶å‚ä¸è€…çš„æš—åœºèƒ¸éƒ¨Xå…‰æ‘„å½±ã€‚æ–¹æ³•åŸºäºæ ¡å‡†æµ‹é‡å’Œå›¾åƒå¤„ç†ï¼Œé€šè¿‡é“å’Œæ°´ï¼ˆåˆ†åˆ«æ¨¡æ‹Ÿéª¨å’Œè½¯ç»„ç»‡çš„å…‰æŸç¡¬åŒ–ï¼‰åˆ›å»ºæŸ¥æ‰¾è¡¨ï¼Œå¹¶ä½¿ç”¨åŠ æƒå¹³å‡å€¼å½¢æˆå•ä¸€æŸ¥æ‰¾è¡¨ï¼Œç»“åˆè¡°å‡å›¾åƒï¼Œå‡å°‘ç ”ç©¶å‚ä¸è€…çš„éª¨è´¨ç»“æ„å’Œäººå·¥æš—åœºä¿¡å·ã€‚ç»“æœå‘ç°ä½¿ç”¨åŠ æƒæŸ¥æ‰¾è¡¨è¿›è¡Œæ ¡æ­£å¯æ˜¾è‘—é™ä½æš—åœºå›¾åƒä¸­çš„éª¨è´¨ç»“æ„ï¼Œé“æˆåˆ†çš„æƒé‡å¯¹æš—åœºå›¾åƒä¸­éª¨ç»“æ„å¯è§åº¦æœ‰å¾ˆå¤§å½±å“ï¼Œå¹¶æˆåŠŸæ ¡æ­£äº†ä¾èµ–äºé“æƒé‡çš„æš—åœºå›¾åƒçš„è´Ÿåå·®ã€‚ç»“è®ºï¼šä½¿ç”¨æ‰€æè¿°çš„æ–¹æ³•æˆåŠŸå‡å°‘äº†æš—åœºå›¾åƒä¸­çš„å…‰æŸç¡¬åŒ–ä¿¡å·ã€‚æ ¹æ®å…·ä½“çš„ä¸´åºŠé—®é¢˜ï¼Œåº”è¯„ä¼°ç”¨äºæŠ‘åˆ¶è‚‹éª¨ç»“æ„çš„é“æƒé‡é€‰æ‹©å’Œåå·®æ ¡æ­£çš„é€‰æ‹©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¾®ç±³çº§ææ–™ç»“æ„å¯¼è‡´è¶…å°è§’åº¦Xå°„çº¿æ•£å°„ï¼Œå½±å“Xå…‰æˆåƒçš„æ¸…æ™°åº¦ã€‚</li>
<li>åœ¨åŸºäºæ ¼æ …çš„Xå°„çº¿æˆåƒä¸­ï¼Œè¾¹ç¼˜å¯è§åº¦ä¼šé™ä½ï¼Œå¹¶ç”Ÿæˆæš—åœºä¿¡å·ã€‚</li>
<li>å¤šè‰²å…‰æŸç¡¬åŒ–ä¼šæ”¹å˜å¯è§åº¦ï¼Œäº§ç”Ÿè™šå‡æš—åœºä¿¡å·ã€‚</li>
<li>ç ”ç©¶ç›®æ ‡ï¼šå¼€å‘ä¸€ç§å¿«é€Ÿã€ç®€å•ã€ç¨³å¥çš„æ–¹æ³•æ¥ä¿®æ­£å› å…‰æŸç¡¬åŒ–äº§ç”Ÿçš„æš—åœºä¿¡å·å’Œéª¨è´¨ç»“æ„ã€‚</li>
<li>æ–¹æ³•åŸºäºæ ¡å‡†æµ‹é‡å’Œå›¾åƒå¤„ç†ï¼Œä½¿ç”¨é“å’Œæ°´æ¨¡æ‹Ÿéª¨å’Œè½¯ç»„ç»‡çš„å…‰æŸç¡¬åŒ–æ•ˆåº”ï¼Œå¹¶åˆ›å»ºæŸ¥æ‰¾è¡¨è¿›è¡Œæ ¡æ­£ã€‚</li>
<li>åº”ç”¨åŠ æƒæŸ¥æ‰¾è¡¨è¿›è¡Œæ ¡æ­£å¯æ˜¾è‘—é™ä½æš—åœºå›¾åƒä¸­çš„éª¨è´¨ç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05422">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c8f6fc0fb367c5253d867367849f9a73.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2fcd3f67aadb271a0b19288a1195b356.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6476955c0db6970e7c60c1304770f5e6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Artificial-Intelligence-Based-Classification-of-Spitz-Tumors"><a href="#Artificial-Intelligence-Based-Classification-of-Spitz-Tumors" class="headerlink" title="Artificial Intelligence-Based Classification of Spitz Tumors"></a>Artificial Intelligence-Based Classification of Spitz Tumors</h2><p><strong>Authors:Ruben T. Lucassen, Marjanna Romers, Chiel F. Ebbelaar, Aia N. Najem, Donal P. Hayes, Antien L. Mooyaart, Sara Roshani, Liliane C. D. Wynaendts, Nikolas Stathonikos, Gerben E. Breimer, Anne M. L. Jansen, Mitko Veta, Willeke A. M. Blokx</strong></p>
<p>Spitz tumors are diagnostically challenging due to overlap in atypical histological features with conventional melanomas. We investigated to what extent AI models, using histological and&#x2F;or clinical features, can: (1) distinguish Spitz tumors from conventional melanomas; (2) predict the underlying genetic aberration of Spitz tumors; and (3) predict the diagnostic category of Spitz tumors. The AI models were developed and validated using a dataset of 393 Spitz tumors and 379 conventional melanomas. Predictive performance was measured using the AUROC and the accuracy. The performance of the AI models was compared with that of four experienced pathologists in a reader study. Moreover, a simulation experiment was conducted to investigate the impact of implementing AI-based recommendations for ancillary diagnostic testing on the workflow of the pathology department. The best AI model based on UNI features reached an AUROC of 0.95 and an accuracy of 0.86 in differentiating Spitz tumors from conventional melanomas. The genetic aberration was predicted with an accuracy of 0.55 compared to 0.25 for randomly guessing. The diagnostic category was predicted with an accuracy of 0.51, where random chance-level accuracy equaled 0.33. On all three tasks, the AI models performed better than the four pathologists, although differences were not statistically significant for most individual comparisons. Based on the simulation experiment, implementing AI-based recommendations for ancillary diagnostic testing could reduce material costs, turnaround times, and examinations. In conclusion, the AI models achieved a strong predictive performance in distinguishing between Spitz tumors and conventional melanomas. On the more challenging tasks of predicting the genetic aberration and the diagnostic category of Spitz tumors, the AI models performed better than random chance. </p>
<blockquote>
<p>æ–¯çš®èŒ¨ç˜¤ç”±äºå…·æœ‰ä¸å¸¸è§„é»‘è‰²ç´ ç˜¤é‡å çš„éå…¸å‹ç»„ç»‡ç‰¹å¾ï¼Œå› æ­¤åœ¨è¯Šæ–­ä¸Šé¢‡å…·æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†äººå·¥æ™ºèƒ½æ¨¡å‹èƒ½åœ¨å¤šå¤§ç¨‹åº¦ä¸Šåˆ©ç”¨ç»„ç»‡å­¦å’Œï¼ˆæˆ–ï¼‰ä¸´åºŠç‰¹å¾ï¼šï¼ˆ1ï¼‰åŒºåˆ†æ–¯çš®èŒ¨ç˜¤å’Œå¸¸è§„é»‘è‰²ç´ ç˜¤ï¼›ï¼ˆ2ï¼‰é¢„æµ‹æ–¯çš®èŒ¨ç˜¤çš„åŸºæœ¬é—ä¼ å¼‚å¸¸ï¼›ï¼ˆ3ï¼‰é¢„æµ‹æ–¯çš®èŒ¨ç˜¤çš„è¯Šæ–­ç±»åˆ«ã€‚äººå·¥æ™ºèƒ½æ¨¡å‹çš„å¼€å‘å’ŒéªŒè¯ä½¿ç”¨äº†åŒ…å«393ä¾‹æ–¯çš®èŒ¨ç˜¤å’Œ379ä¾‹å¸¸è§„é»‘è‰²ç´ ç˜¤çš„æ•°æ®é›†ã€‚é€šè¿‡AUROCå’Œå‡†ç¡®æ€§æ¥è¡¡é‡é¢„æµ‹æ€§èƒ½ã€‚åœ¨è¯»è€…ç ”ç©¶ä¸­ï¼Œå°†äººå·¥æ™ºèƒ½æ¨¡å‹çš„æ€§èƒ½ä¸å››ä½ç»éªŒä¸°å¯Œçš„ç—…ç†åŒ»ç”Ÿçš„æ€§èƒ½è¿›è¡Œäº†æ¯”è¾ƒã€‚æ­¤å¤–ï¼Œè¿›è¡Œäº†ä¸€é¡¹æ¨¡æ‹Ÿå®éªŒï¼Œä»¥ç ”ç©¶åœ¨ç—…ç†å­¦éƒ¨é—¨å®æ–½åŸºäºäººå·¥æ™ºèƒ½çš„è¾…åŠ©è¯Šæ–­å»ºè®®å¯¹å·¥ä½œæµç¨‹çš„å½±å“ã€‚åŸºäºUNIç‰¹å¾çš„æœ€ä½³äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨åŒºåˆ†æ–¯çš®èŒ¨ç˜¤å’Œå¸¸è§„é»‘è‰²ç´ ç˜¤æ–¹é¢è¾¾åˆ°äº†AUROCä¸º0.95å’Œå‡†ç¡®æ€§ä¸º0.86ã€‚é—ä¼ å¼‚å¸¸çš„é¢„æµ‹å‡†ç¡®ç‡ä¸º0.55ï¼Œè€ŒéšæœºçŒœæµ‹çš„å‡†ç¡®ç‡ä¸º0.25ã€‚è¯Šæ–­ç±»åˆ«çš„é¢„æµ‹å‡†ç¡®ç‡ä¸º0.51ï¼Œå…¶ä¸­éšæœºæœºä¼šæ°´å¹³çš„å‡†ç¡®ç‡ç­‰äº0.33ã€‚åœ¨æ‰€æœ‰ä¸‰é¡¹ä»»åŠ¡ä¸­ï¼Œäººå·¥æ™ºèƒ½æ¨¡å‹çš„æ€§èƒ½éƒ½ä¼˜äºå››ä½ç—…ç†åŒ»ç”Ÿï¼Œå°½ç®¡å¤§å¤šæ•°ä¸ªåˆ«æ¯”è¾ƒçš„å·®å¼‚æ²¡æœ‰ç»Ÿè®¡å­¦æ„ä¹‰ã€‚åŸºäºæ¨¡æ‹Ÿå®éªŒçš„ç»“æœï¼Œå®æ–½åŸºäºäººå·¥æ™ºèƒ½çš„è¾…åŠ©è¯Šæ–­å»ºè®®å¯ä»¥é™ä½ææ–™æˆæœ¬ã€ç¼©çŸ­å‘¨è½¬æ—¶é—´å’Œæ£€æŸ¥æ¬¡æ•°ã€‚æ€»ä¹‹ï¼Œäººå·¥æ™ºèƒ½æ¨¡å‹åœ¨åŒºåˆ†æ–¯çš®èŒ¨ç˜¤å’Œå¸¸è§„é»‘è‰²ç´ ç˜¤æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„é¢„æµ‹æ€§èƒ½ã€‚åœ¨é¢„æµ‹æ–¯çš®èŒ¨ç˜¤çš„é—ä¼ å¼‚å¸¸å’Œè¯Šæ–­ç±»åˆ«è¿™äº›æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸­ï¼Œäººå·¥æ™ºèƒ½æ¨¡å‹çš„è¡¨ç°ä¼˜äºéšæœºæœºä¼šæ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05391v1">PDF</a> 19 pages, 2 figures, 6 tables, 6 supplementary tables</p>
<p><strong>Summary</strong></p>
<p>åŸºäºSpitzè‚¿ç˜¤ä¸å¸¸è§„é»‘è‰²ç´ ç˜¤åœ¨ç»„ç»‡å½¢æ€å­¦ç‰¹å¾ä¸Šçš„é‡å ï¼Œæœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨ç»„ç»‡å­¦å’Œ&#x2F;æˆ–ä¸´åºŠç‰¹å¾çš„AIæ¨¡å‹åœ¨åŒºåˆ†Spitzè‚¿ç˜¤ä¸å¸¸è§„é»‘è‰²ç´ ç˜¤ã€é¢„æµ‹Spitzè‚¿ç˜¤çš„é—ä¼ å¼‚å¸¸ä»¥åŠé¢„æµ‹å…¶è¯Šæ–­ç±»åˆ«æ–¹é¢çš„èƒ½åŠ›ã€‚ç ”ç©¶ä½¿ç”¨åŒ…å«393ä¾‹Spitzè‚¿ç˜¤å’Œ379ä¾‹å¸¸è§„é»‘è‰²ç´ ç˜¤çš„æ•°æ®é›†è¿›è¡ŒAIæ¨¡å‹çš„å¼€å‘å’ŒéªŒè¯ã€‚ç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³æ¨¡å‹åœ¨åŒºåˆ†Spitzè‚¿ç˜¤ä¸å¸¸è§„é»‘è‰²ç´ ç˜¤æ–¹é¢è¾¾åˆ°äº†è¾ƒé«˜çš„é¢„æµ‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIæ¨¡å‹åœ¨åŒºåˆ†Spitzè‚¿ç˜¤ä¸å¸¸è§„é»‘è‰²ç´ ç˜¤æ–¹é¢è¡¨ç°å‡ºå¼ºé¢„æµ‹æ€§èƒ½ï¼Œæœ€ä½³æ¨¡å‹çš„AUROCå€¼ä¸º0.95ï¼Œå‡†ç¡®ç‡ä¸º0.86ã€‚</li>
<li>AIæ¨¡å‹åœ¨é¢„æµ‹Spitzè‚¿ç˜¤çš„é—ä¼ å¼‚å¸¸æ–¹é¢è¾¾åˆ°äº†ä¸€å®šçš„å‡†ç¡®æ€§ï¼ˆ0.55ï¼‰ï¼Œç›¸è¾ƒäºéšæœºçŒœæµ‹ï¼ˆ0.25ï¼‰æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>åœ¨é¢„æµ‹Spitzè‚¿ç˜¤çš„è¯Šæ–­ç±»åˆ«æ–¹é¢ï¼ŒAIæ¨¡å‹çš„å‡†ç¡®ç‡ä¸º0.51ï¼Œå°½ç®¡ç›¸è¾ƒäºéšæœºæ¦‚ç‡æœ‰æ‰€æå‡ï¼Œä½†ä»ä¸ºè¾ƒå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚</li>
<li>AIæ¨¡å‹åœ¨ä¸‰é¡¹ä»»åŠ¡ä¸­çš„è¡¨ç°å‡ä¼˜äºå››ä½ç—…ç†åŒ»ç”Ÿï¼Œå°½ç®¡å¤§å¤šæ•°ä¸ªåˆ«æ¯”è¾ƒå·®å¼‚æœªè¾¾ç»Ÿè®¡å­¦æ˜¾è‘—æ°´å¹³ã€‚</li>
<li>å®æ–½AIè¾…åŠ©è¯Šæ–­æµ‹è¯•æ¨èå¯ä»¥é™ä½ææ–™æˆæœ¬ã€ç¼©çŸ­å‘¨è½¬æ—¶é—´ä»¥åŠæ£€æŸ¥æ¬¡æ•°ã€‚</li>
<li>AIæ¨¡å‹åœ¨åŒºåˆ†Spitzè‚¿ç˜¤ä¸å¸¸è§„é»‘è‰²ç´ ç˜¤æ–¹é¢çš„è¡¨ç°å°¤ä¸ºçªå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05391">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cd41347cb06befb556bbf7034a386c05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea702850cafb2f911763ed1a23e85a59.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Deformable-Attention-Graph-Representation-Learning-for-Histopathology-Whole-Slide-Image-Analysis"><a href="#Deformable-Attention-Graph-Representation-Learning-for-Histopathology-Whole-Slide-Image-Analysis" class="headerlink" title="Deformable Attention Graph Representation Learning for Histopathology   Whole Slide Image Analysis"></a>Deformable Attention Graph Representation Learning for Histopathology   Whole Slide Image Analysis</h2><p><strong>Authors:Mingxi Fu, Xitong Ling, Yuxuan Chen, Jiawen Li, fanglei fu, Huaitian Yuan, Tian Guan, Yonghong He, Lianghui Zhu</strong></p>
<p>Accurate classification of Whole Slide Images (WSIs) and Regions of Interest (ROIs) is a fundamental challenge in computational pathology. While mainstream approaches often adopt Multiple Instance Learning (MIL), they struggle to capture the spatial dependencies among tissue structures. Graph Neural Networks (GNNs) have emerged as a solution to model inter-instance relationships, yet most rely on static graph topologies and overlook the physical spatial positions of tissue patches. Moreover, conventional attention mechanisms lack specificity, limiting their ability to focus on structurally relevant regions. In this work, we propose a novel GNN framework with deformable attention for pathology image analysis. We construct a dynamic weighted directed graph based on patch features, where each node aggregates contextual information from its neighbors via attention-weighted edges. Specifically, we incorporate learnable spatial offsets informed by the real coordinates of each patch, enabling the model to adaptively attend to morphologically relevant regions across the slide. This design significantly enhances the contextual field while preserving spatial specificity. Our framework achieves state-of-the-art performance on four benchmark datasets (TCGA-COAD, BRACS, gastric intestinal metaplasia grading, and intestinal ROI classification), demonstrating the power of deformable attention in capturing complex spatial structures in WSIs and ROIs. </p>
<blockquote>
<p>å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIsï¼‰å’Œæ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIsï¼‰çš„ç²¾ç¡®åˆ†ç±»æ˜¯è®¡ç®—ç—…ç†å­¦ä¸­çš„ä¸€é¡¹åŸºæœ¬æŒ‘æˆ˜ã€‚è™½ç„¶ä¸»æµæ–¹æ³•ç»å¸¸é‡‡ç”¨å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰ï¼Œä½†å®ƒä»¬å¾ˆéš¾æ•è·ç»„ç»‡ç»“æ„ä¹‹é—´çš„ç©ºé—´ä¾èµ–å…³ç³»ã€‚å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰å·²å‡ºç°ä¸ºå»ºæ¨¡å®ä¾‹é—´å…³ç³»çš„ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œä½†å¤§å¤šæ•°ä¾èµ–äºé™æ€å›¾æ‹“æ‰‘ï¼Œå¹¶å¿½ç•¥äº†ç»„ç»‡æ–‘å—çš„ç‰©ç†ç©ºé—´ä½ç½®ã€‚æ­¤å¤–ï¼Œä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶ç¼ºä¹ç‰¹å¼‚æ€§ï¼Œé™åˆ¶äº†å®ƒä»¬å…³æ³¨ç»“æ„ç›¸å…³åŒºåŸŸçš„èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºç—…ç†å­¦å›¾åƒåˆ†æçš„æ–°å‹å¯å˜å½¢æ³¨æ„åŠ›å›¾ç¥ç»ç½‘ç»œæ¡†æ¶ã€‚æˆ‘ä»¬åŸºäºæ–‘å—ç‰¹å¾æ„å»ºäº†ä¸€ä¸ªåŠ¨æ€åŠ æƒæœ‰å‘å›¾ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹éƒ½é€šè¿‡æ³¨æ„åŠ›åŠ æƒçš„è¾¹ä»å…¶é‚»å±…é‚£é‡Œèšåˆä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç»“åˆäº†å¯å­¦ä¹ çš„ç©ºé—´åç§»ï¼Œä»¥äº†è§£æ¯ä¸ªæ–‘å—çš„çœŸå®åæ ‡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°å…³æ³¨åˆ‡ç‰‡ä¸Šå½¢æ€ç›¸å…³çš„åŒºåŸŸã€‚è¿™ç§è®¾è®¡åœ¨å¢å¼ºä¸Šä¸‹æ–‡å­—æ®µçš„åŒæ—¶ä¿ç•™äº†ç©ºé—´ç‰¹å¼‚æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ï¼ˆTCGA-COADã€BRACSã€èƒƒè‚ åŒ–ç”Ÿåˆ†çº§å’Œè‚ é“ROIåˆ†ç±»ï¼‰ï¼Œè¯æ˜äº†å¯å˜å½¢æ³¨æ„åŠ›åœ¨æ•è·WSIså’ŒROIsä¸­å¤æ‚ç©ºé—´ç»“æ„çš„åŠ›é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05382v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­ï¼Œå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰å’Œæ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIsï¼‰çš„å‡†ç¡®åˆ†ç±»æ˜¯è®¡ç®—ç—…ç†å­¦ä¸­çš„åŸºæœ¬æŒ‘æˆ˜ã€‚ä¸»æµæ–¹æ³•å¤šé‡‡ç”¨å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰ï¼Œä½†éš¾ä»¥æ•æ‰ç»„ç»‡ç»“æ„çš„ç©ºé—´ä¾èµ–æ€§ã€‚å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ä½œä¸ºè§£å†³å»ºæ¨¡å®ä¾‹é—´å…³ç³»çš„æ–¹æ¡ˆè€Œå‡ºç°ï¼Œä½†å¤§å¤šæ•°ä¾èµ–äºé™æ€å›¾æ‹“æ‰‘ç»“æ„è€Œå¿½ç•¥äº†ç»„ç»‡è¡¥ä¸çš„ç‰©ç†ç©ºé—´ä½ç½®ã€‚æ­¤å¤–ï¼Œä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶ç¼ºä¹ç‰¹å¼‚æ€§ï¼Œé™åˆ¶äº†å…¶åœ¨ç»“æ„ä¸Šç›¸å…³åŒºåŸŸçš„å…³æ³¨èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å…·æœ‰å¯å˜å½¢æ³¨æ„åŠ›çš„æ–°å‹GNNæ¡†æ¶ç”¨äºç—…ç†å­¦å›¾åƒåˆ†æã€‚æˆ‘ä»¬æ ¹æ®è¡¥ä¸ç‰¹å¾æ„å»ºäº†ä¸€ä¸ªåŠ¨æ€åŠ æƒçš„æœ‰å‘å›¾ï¼Œæ¯ä¸ªèŠ‚ç‚¹é€šè¿‡æ³¨æ„åŠ›åŠ æƒçš„è¾¹ä»å…¶é‚»å±…å¤„èšåˆä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¯å­¦ä¹ çš„ç©ºé—´åç§»é‡ï¼Œè¿™äº›åç§»é‡ç”±æ¯ä¸ªè¡¥ä¸çš„çœŸå®åæ ‡ä¿¡æ¯å¾—å‡ºï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªé€‚åº”åœ°å…³æ³¨å¹»ç¯ç‰‡ä¸­å½¢æ€ç›¸å…³çš„åŒºåŸŸã€‚è¿™ç§è®¾è®¡åœ¨å¢å¼ºä¸Šä¸‹æ–‡å­—æ®µçš„åŒæ—¶ä¿ç•™äº†ç©ºé—´ç‰¹å¼‚æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å¯å˜å½¢æ³¨æ„åŠ›åœ¨æ•æ‰WSIså’ŒROIsä¸­å¤æ‚ç©ºé—´ç»“æ„çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—ç—…ç†å­¦ä¸­å¯¹å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIsï¼‰å’Œæ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIsï¼‰çš„å‡†ç¡®åˆ†ç±»æ˜¯ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>ä¸»æµæ–¹æ³•å¦‚å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰éš¾ä»¥æ•æ‰ç»„ç»‡ç»“æ„çš„ç©ºé—´ä¾èµ–æ€§ã€‚</li>
<li>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰èƒ½å¤Ÿå»ºæ¨¡å®ä¾‹é—´çš„å…³ç³»ï¼Œä½†å¤§å¤šæ–¹æ³•å¿½ç•¥äº†ç»„ç»‡è¡¥ä¸çš„ç©ºé—´ä½ç½®ã€‚</li>
<li>ä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶åœ¨å…³æ³¨ç»“æ„ä¸Šç›¸å…³åŒºåŸŸæ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ–°å‹çš„å›¾ç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œç»“åˆå¯å˜å½¢æ³¨æ„åŠ›ç”¨äºç—…ç†å­¦å›¾åƒåˆ†æã€‚</li>
<li>è¯¥æ¡†æ¶æ ¹æ®è¡¥ä¸ç‰¹å¾æ„å»ºåŠ¨æ€åŠ æƒçš„æœ‰å‘å›¾ï¼Œå¹¶è€ƒè™‘æ¯ä¸ªè¡¥ä¸çš„ç©ºé—´åæ ‡ï¼Œä»¥è‡ªé€‚åº”åœ°å…³æ³¨å½¢æ€ç›¸å…³çš„åŒºåŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05382">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8d51a6e711cce5870a686bc0e7755d5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b12f60026b3b21d0db1bb5c1ce1c98e1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a08c201d84246e109c60f5ae8b00122.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69deea88da923e74318823ec6398f270.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbbc79797b2fe532264bc1bf0faba992.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0455430535db719ec8f55b9e31f32bfb.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CT-GRAPH-Hierarchical-Graph-Attention-Network-for-Anatomy-Guided-CT-Report-Generation"><a href="#CT-GRAPH-Hierarchical-Graph-Attention-Network-for-Anatomy-Guided-CT-Report-Generation" class="headerlink" title="CT-GRAPH: Hierarchical Graph Attention Network for Anatomy-Guided CT   Report Generation"></a>CT-GRAPH: Hierarchical Graph Attention Network for Anatomy-Guided CT   Report Generation</h2><p><strong>Authors:Hamza Kalisch, Fabian HÃ¶rst, Jens Kleesiek, Ken Herrmann, Constantin Seibold</strong></p>
<p>As medical imaging is central to diagnostic processes, automating the generation of radiology reports has become increasingly relevant to assist radiologists with their heavy workloads. Most current methods rely solely on global image features, failing to capture fine-grained organ relationships crucial for accurate reporting. To this end, we propose CT-GRAPH, a hierarchical graph attention network that explicitly models radiological knowledge by structuring anatomical regions into a graph, linking fine-grained organ features to coarser anatomical systems and a global patient context. Our method leverages pretrained 3D medical feature encoders to obtain global and organ-level features by utilizing anatomical masks. These features are further refined within the graph and then integrated into a large language model to generate detailed medical reports. We evaluate our approach for the task of report generation on the large-scale chest CT dataset CT-RATE. We provide an in-depth analysis of pretrained feature encoders for CT report generation and show that our method achieves a substantial improvement of absolute 7.9% in F1 score over current state-of-the-art methods. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/hakal104/CT-GRAPH">https://github.com/hakal104/CT-GRAPH</a>. </p>
<blockquote>
<p>åŒ»å­¦æˆåƒåœ¨è¯Šæ–­è¿‡ç¨‹ä¸­å æ®æ ¸å¿ƒåœ°ä½ï¼Œå› æ­¤è‡ªåŠ¨åŒ–ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šå¯¹äºå¸®åŠ©æ”¾å°„ç§‘åŒ»ç”Ÿåº”å¯¹å¤§é‡å·¥ä½œå…·æœ‰è¶Šæ¥è¶Šé‡è¦çš„æ„ä¹‰ã€‚å½“å‰å¤§å¤šæ•°æ–¹æ³•ä»…ä¾èµ–äºå…¨å±€å›¾åƒç‰¹å¾ï¼Œæ— æ³•æ•è·å¯¹å‡†ç¡®æŠ¥å‘Šè‡³å…³é‡è¦çš„ç²¾ç»†å™¨å®˜å…³ç³»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†CT-GRAPHï¼Œè¿™æ˜¯ä¸€ç§åˆ†å±‚å›¾æ³¨æ„åŠ›ç½‘ç»œï¼Œå®ƒé€šè¿‡æ„å»ºè§£å‰–åŒºåŸŸå›¾æ¥æ˜¾å¼åœ°å»ºç«‹æ”¾å°„å­¦çŸ¥è¯†æ¨¡å‹ï¼Œå°†ç²¾ç»†çš„å™¨å®˜ç‰¹å¾é“¾æ¥åˆ°è¾ƒç²—çš„è§£å‰–ç³»ç»Ÿå’Œå…¨å±€æ‚£è€…èƒŒæ™¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„3DåŒ»å­¦ç‰¹å¾ç¼–ç å™¨ï¼Œé€šè¿‡ä½¿ç”¨è§£å‰–æ©è†œè·å¾—å…¨å±€å’Œå™¨å®˜çº§åˆ«çš„ç‰¹å¾ã€‚è¿™äº›ç‰¹å¾åœ¨å›¾ä¸­å¾—åˆ°è¿›ä¸€æ­¥ç²¾ç‚¼ï¼Œç„¶åé›†æˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»¥ç”Ÿæˆè¯¦ç»†çš„åŒ»ç–—æŠ¥å‘Šã€‚æˆ‘ä»¬åœ¨å¤§è§„æ¨¡çš„èƒ¸éƒ¨CTæ•°æ®é›†CT-RATEä¸Šå¯¹ç”ŸæˆæŠ¥å‘Šçš„ä»»åŠ¡è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬æ·±å…¥åˆ†æäº†ç”¨äºCTæŠ¥å‘Šç”Ÿæˆçš„é¢„è®­ç»ƒç‰¹å¾ç¼–ç å™¨ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨F1åˆ†æ•°ä¸Šè¾ƒå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•æé«˜äº†ç»å¯¹7.9%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hakal104/CT-GRAPH%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hakal104/CT-GRAPHå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05375v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦æˆåƒåœ¨è¯Šæ–­è¿‡ç¨‹ä¸­å æ®æ ¸å¿ƒåœ°ä½ï¼Œä¸ºå¸®åŠ©æ”¾å°„ç§‘åŒ»ç”Ÿåº”å¯¹ç¹é‡çš„å·¥ä½œè´Ÿæ‹…ï¼Œè‡ªåŠ¨åŒ–ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šå˜å¾—è‡³å…³é‡è¦ã€‚å½“å‰å¤§å¤šæ•°æ–¹æ³•ä»…ä¾èµ–å…¨å±€å›¾åƒç‰¹å¾ï¼Œå¿½ç•¥äº†ç²¾ç»†å™¨å®˜å…³ç³»å¯¹äºå‡†ç¡®æŠ¥å‘Šçš„é‡è¦æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºCT-GRAPHæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å±‚æ¬¡åŒ–å›¾æ³¨æ„åŠ›ç½‘ç»œæ˜¾å¼å»ºæ¨¡æ”¾å°„å­¦çŸ¥è¯†ï¼Œé€šè¿‡æ„å»ºè§£å‰–åŒºåŸŸå›¾è¿æ¥ç²¾ç»†å™¨å®˜ç‰¹å¾ä¸è¾ƒç²—çš„è§£å‰–ç³»ç»Ÿå’Œå…¨å±€æ‚£è€…èƒŒæ™¯ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§è§„æ¨¡èƒ¸éƒ¨CTæ•°æ®é›†CT-RATEä¸Šçš„æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆæœï¼Œç›¸è¾ƒäºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒF1åˆ†æ•°æé«˜äº†7.9%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒåœ¨è¯Šæ–­ä¸­èµ·æ ¸å¿ƒä½œç”¨ï¼Œè‡ªåŠ¨åŒ–ç”Ÿæˆæ”¾å°„å­¦æŠ¥å‘Šå¯¹å¸®åŠ©æ”¾å°„ç§‘åŒ»ç”Ÿå…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>å½“å‰å¤§å¤šæ•°è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆæ–¹æ³•ä»…ä¾èµ–å…¨å±€å›¾åƒç‰¹å¾ï¼Œå­˜åœ¨ä¸è¶³ã€‚</li>
<li>CT-GRAPHæ–¹æ³•é€šè¿‡å±‚æ¬¡åŒ–å›¾æ³¨æ„åŠ›ç½‘ç»œæ˜¾å¼å»ºæ¨¡æ”¾å°„å­¦çŸ¥è¯†ï¼Œç»“åˆç²¾ç»†å™¨å®˜å…³ç³»ä¸å…¨å±€æ‚£è€…èƒŒæ™¯ã€‚</li>
<li>æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„3DåŒ»å­¦ç‰¹å¾ç¼–ç å™¨è·å–å…¨å±€å’Œå™¨å®˜çº§åˆ«çš„ç‰¹å¾ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCT-GRAPHåœ¨èƒ¸éƒ¨CTæ•°æ®é›†CT-RATEä¸Šçš„æŠ¥å‘Šç”Ÿæˆä»»åŠ¡è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒCT-GRAPHåœ¨F1åˆ†æ•°ä¸Šæé«˜äº†7.9%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05375">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-564b0b6c8ef7707ef5a6f7ab7b08adcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1ebaf932b90f5bcc567838c81865e8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72f69230672f44d42c5434d99484f3b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-030d44bf6217bee14152a252404cf810.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-962616d1b96a34216bd6ae9fedd5ef8b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PriorRG-Prior-Guided-Contrastive-Pre-training-and-Coarse-to-Fine-Decoding-for-Chest-X-ray-Report-Generation"><a href="#PriorRG-Prior-Guided-Contrastive-Pre-training-and-Coarse-to-Fine-Decoding-for-Chest-X-ray-Report-Generation" class="headerlink" title="PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine   Decoding for Chest X-ray Report Generation"></a>PriorRG: Prior-Guided Contrastive Pre-training and Coarse-to-Fine   Decoding for Chest X-ray Report Generation</h2><p><strong>Authors:Kang Liu, Zhuoqi Ma, Zikang Fang, Yunan Li, Kun Xie, Qiguang Miao</strong></p>
<p>Chest X-ray report generation aims to reduce radiologistsâ€™ workload by automatically producing high-quality preliminary reports. A critical yet underexplored aspect of this task is the effective use of patient-specific prior knowledge â€“ including clinical context (e.g., symptoms, medical history) and the most recent prior image â€“ which radiologists routinely rely on for diagnostic reasoning. Most existing methods generate reports from single images, neglecting this essential prior information and thus failing to capture diagnostic intent or disease progression. To bridge this gap, we propose PriorRG, a novel chest X-ray report generation framework that emulates real-world clinical workflows via a two-stage training pipeline. In Stage 1, we introduce a prior-guided contrastive pre-training scheme that leverages clinical context to guide spatiotemporal feature extraction, allowing the model to align more closely with the intrinsic spatiotemporal semantics in radiology reports. In Stage 2, we present a prior-aware coarse-to-fine decoding for report generation that progressively integrates patient-specific prior knowledge with the vision encoderâ€™s hidden states. This decoding allows the model to align with diagnostic focus and track disease progression, thereby enhancing the clinical accuracy and fluency of the generated reports. Extensive experiments on MIMIC-CXR and MIMIC-ABN datasets demonstrate that PriorRG outperforms state-of-the-art methods, achieving a 3.6% BLEU-4 and 3.8% F1 score improvement on MIMIC-CXR, and a 5.9% BLEU-1 gain on MIMIC-ABN. Code and checkpoints will be released upon acceptance. </p>
<blockquote>
<p>èƒ¸éƒ¨Xå…‰æŠ¥å‘Šç”Ÿæˆæ—¨åœ¨é€šè¿‡è‡ªåŠ¨äº§ç”Ÿé«˜è´¨é‡çš„åˆæ­¥æŠ¥å‘Šæ¥å‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œé‡ã€‚è¯¥ä»»åŠ¡çš„ä¸€ä¸ªå…³é”®ä½†å°šæœªè¢«å……åˆ†ç ”ç©¶çš„æ–¹é¢æ˜¯å¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨æ‚£è€…ç‰¹å®šçš„å…ˆéªŒçŸ¥è¯†ï¼ŒåŒ…æ‹¬ä¸´åºŠèƒŒæ™¯ï¼ˆä¾‹å¦‚ç—‡çŠ¶ã€ç—…å²ï¼‰å’Œæœ€è¿‘çš„å…ˆå‰å›¾åƒï¼Œæ”¾å°„ç§‘åŒ»ç”Ÿåœ¨è¿›è¡Œè¯Šæ–­æ¨ç†æ—¶é€šå¸¸ä¼šä¾èµ–è¿™äº›çŸ¥è¯†ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•éƒ½æ˜¯ä»å•ä¸€å›¾åƒç”ŸæˆæŠ¥å‘Šï¼Œå¿½ç•¥äº†è¿™äº›å…³é”®çš„å…ˆéªŒä¿¡æ¯ï¼Œå› æ­¤æ— æ³•æ•æ‰è¯Šæ–­æ„å›¾æˆ–ç–¾ç—…è¿›å±•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†PriorRGï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„èƒ¸éƒ¨Xå…‰æŠ¥å‘Šç”Ÿæˆæ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“æ¨¡æ‹Ÿç°å®ä¸´åºŠå·¥ä½œæµç¨‹ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºå…ˆéªŒçš„å¯¹æ¯”é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œåˆ©ç”¨ä¸´åºŠèƒŒæ™¯å¼•å¯¼æ—¶ç©ºç‰¹å¾æå–ï¼Œä½¿æ¨¡å‹æ›´è´´è¿‘æ”¾å°„å­¦æŠ¥å‘Šä¸­çš„å†…åœ¨æ—¶ç©ºè¯­ä¹‰ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå…ˆéªŒçš„ç²—ç»†åˆ°ç»†è§£ç æŠ¥å‘Šç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€æ­¥å°†æ‚£è€…ç‰¹å®šçš„å…ˆéªŒçŸ¥è¯†ä¸è§†è§‰ç¼–ç å™¨çš„éšè—çŠ¶æ€é›†æˆåœ¨ä¸€èµ·ã€‚è¿™ç§è§£ç æ–¹å¼ä½¿æ¨¡å‹èƒ½å¤Ÿç¬¦åˆè¯Šæ–­é‡ç‚¹å¹¶è·Ÿè¸ªç–¾ç—…è¿›å±•ï¼Œä»è€Œæé«˜ç”ŸæˆæŠ¥å‘Šçš„ä¸´åºŠå‡†ç¡®æ€§å’Œæµç•…æ€§ã€‚åœ¨MIMIC-CXRå’ŒMIMIC-ABNæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPriorRGä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œåœ¨MIMIC-CXRä¸Šå®ç°äº†BLEU-4çš„3.6%å’ŒF1å¾—åˆ†çš„3.8%çš„æå‡ï¼Œåœ¨MIMIC-ABNä¸Šå®ç°äº†BLEU-1çš„5.9%çš„æå‡ã€‚è®ºæ–‡æ¥å—åå°†å…¬å¸ƒä»£ç å’Œæ£€æŸ¥ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05353v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡å…³æ³¨é€šè¿‡åˆ©ç”¨æ‚£è€…ç‰¹å®šå…ˆéªŒçŸ¥è¯†ï¼ˆåŒ…æ‹¬ä¸´åºŠç—‡çŠ¶ã€ç—…å²å’Œæœ€æ–°å›¾åƒç­‰ï¼‰ï¼Œå®ç°è‡ªåŠ¨äº§ç”Ÿé«˜è´¨é‡åˆæ­¥æŠ¥å‘Šï¼Œä»è€Œå‡å°‘æ”¾å°„ç§‘åŒ»ç”Ÿå·¥ä½œé‡çš„ç›®æ ‡ã€‚é’ˆå¯¹å½“å‰å¤šæ•°æ–¹æ³•ä»…å…³æ³¨å•ä¸€å›¾åƒæŠ¥å‘Šç”Ÿæˆï¼Œå¿½ç•¥äº†é‡è¦çš„å…ˆéªŒä¿¡æ¯ï¼Œæ— æ³•æ•æ‰è¯Šæ–­æ„å›¾æˆ–ç–¾ç—…è¿›å±•çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„èƒ¸éƒ¨Xå…‰æŠ¥å‘Šç”Ÿæˆæ¡†æ¶PriorRGã€‚è¯¥æ¡†æ¶æ¨¡æ‹ŸçœŸå®ä¸´åºŠå·¥ä½œæµç¨‹ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“è¿›è¡Œè®­ç»ƒã€‚ç¬¬ä¸€é˜¶æ®µå¼•å…¥åŸºäºå…ˆéªŒçŸ¥è¯†çš„å¯¹æ¯”é¢„è®­ç»ƒæ–¹æ¡ˆï¼Œåˆ©ç”¨ä¸´åºŠè¯­å¢ƒæŒ‡å¯¼æ—¶ç©ºç‰¹å¾æå–ï¼›ç¬¬äºŒé˜¶æ®µé‡‡ç”¨å…ˆéªŒçŸ¥è¯†æ„ŸçŸ¥çš„ç²—ç»†è§£ç æ–¹å¼ç”ŸæˆæŠ¥å‘Šï¼Œé€æ­¥å°†æ‚£è€…ç‰¹å®šå…ˆéªŒçŸ¥è¯†ä¸è§†è§‰ç¼–ç å™¨çš„éšè—çŠ¶æ€æ•´åˆåœ¨ä¸€èµ·ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°èšç„¦äºè¯Šæ–­é‡ç‚¹å¹¶è¿½è¸ªç–¾ç—…è¿›å±•ï¼Œæé«˜äº†ç”ŸæˆæŠ¥å‘Šçš„ä¸´åºŠå‡†ç¡®æ€§å’Œæµç•…æ€§ã€‚åœ¨MIMIC-CXRå’ŒMIMIC-ABNæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPriorRGè¾ƒå…ˆè¿›æ–¹æ³•å®ç°äº†æ˜¾è‘—æ€§èƒ½æå‡ï¼Œåœ¨MIMIC-CXRä¸ŠBLEU-4å’ŒF1å¾—åˆ†æé«˜äº†3.6%å’Œ3.8%ï¼Œåœ¨MIMIC-ABNä¸ŠBLEU-1å¾—åˆ†æé«˜äº†5.9%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>èƒ¸éƒ¨Xå…‰æŠ¥å‘Šç”Ÿæˆæ—¨åœ¨é€šè¿‡è‡ªåŠ¨äº§ç”Ÿé«˜è´¨é‡åˆæ­¥æŠ¥å‘Šå‡å°‘æ”¾å°„ç§‘åŒ»ç”Ÿå·¥ä½œé‡ã€‚</li>
<li>å½“å‰æ–¹æ³•å¿½ç•¥äº†æ‚£è€…ç‰¹å®šçš„å…ˆéªŒä¿¡æ¯ï¼ŒåŒ…æ‹¬ä¸´åºŠç—‡çŠ¶ã€ç—…å²å’Œæœ€æ–°å›¾åƒç­‰ï¼Œæ— æ³•æ•æ‰è¯Šæ–­æ„å›¾å’Œç–¾ç—…è¿›å±•ã€‚</li>
<li>PriorRGæ¡†æ¶é€šè¿‡æ¨¡æ‹ŸçœŸå®ä¸´åºŠå·¥ä½œæµç¨‹æ¥æ•´åˆæ‚£è€…ç‰¹å®šå…ˆéªŒçŸ¥è¯†ï¼Œæé«˜æŠ¥å‘Šç”Ÿæˆçš„å‡†ç¡®æ€§å’Œæµç•…æ€§ã€‚</li>
<li>PriorRGé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ï¼šç¬¬ä¸€é˜¶æ®µåˆ©ç”¨ä¸´åºŠè¯­å¢ƒè¿›è¡Œå…ˆéªŒçŸ¥è¯†æŒ‡å¯¼çš„å¯¹æ¯”é¢„è®­ç»ƒï¼›ç¬¬äºŒé˜¶æ®µé‡‡ç”¨å…ˆéªŒçŸ¥è¯†æ„ŸçŸ¥çš„è§£ç æ–¹å¼ç”ŸæˆæŠ¥å‘Šã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPriorRGåœ¨MIMIC-CXRå’ŒMIMIC-ABNæ•°æ®é›†ä¸Šè¾ƒå…ˆè¿›æ–¹æ³•å®ç°äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>PriorRGæ¨¡å‹èƒ½æ›´å¥½åœ°èšç„¦äºè¯Šæ–­é‡ç‚¹å¹¶è¿½è¸ªç–¾ç—…è¿›å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05353">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9d096105cae123088be71d8934448602.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-887a288e91f1ed595c80fa3a32861364.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3be48e1e784d34cecce759d8b504750d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f00a38dcdfedb2ff89e338f1f30e8dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae2e96d0a2c0d17c039ae19a4a0a30dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a0e2ac850fa80c8b33071e7b1656377.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="RegionMed-CLIP-A-Region-Aware-Multimodal-Contrastive-Learning-Pre-trained-Model-for-Medical-Image-Understanding"><a href="#RegionMed-CLIP-A-Region-Aware-Multimodal-Contrastive-Learning-Pre-trained-Model-for-Medical-Image-Understanding" class="headerlink" title="RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning   Pre-trained Model for Medical Image Understanding"></a>RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning   Pre-trained Model for Medical Image Understanding</h2><p><strong>Authors:Tianchen Fang, Guiru Liu</strong></p>
<p>Medical image understanding plays a crucial role in enabling automated diagnosis and data-driven clinical decision support. However, its progress is impeded by two primary challenges: the limited availability of high-quality annotated medical data and an overreliance on global image features, which often miss subtle but clinically significant pathological regions. To address these issues, we introduce RegionMed-CLIP, a region-aware multimodal contrastive learning framework that explicitly incorporates localized pathological signals along with holistic semantic representations. The core of our method is an innovative region-of-interest (ROI) processor that adaptively integrates fine-grained regional features with the global context, supported by a progressive training strategy that enhances hierarchical multimodal alignment. To enable large-scale region-level representation learning, we construct MedRegion-500k, a comprehensive medical image-text corpus that features extensive regional annotations and multilevel clinical descriptions. Extensive experiments on image-text retrieval, zero-shot classification, and visual question answering tasks demonstrate that RegionMed-CLIP consistently exceeds state-of-the-art vision language models by a wide margin. Our results highlight the critical importance of region-aware contrastive pre-training and position RegionMed-CLIP as a robust foundation for advancing multimodal medical image understanding. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒç†è§£åœ¨å®ç°è‡ªåŠ¨åŒ–è¯Šæ–­å’ŒåŸºäºæ•°æ®çš„ä¸´åºŠå†³ç­–æ”¯æŒä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œå…¶è¿›å±•å—åˆ°ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜çš„é™åˆ¶ï¼šé«˜è´¨é‡æ ‡æ³¨åŒ»å­¦æ•°æ®çš„æœ‰é™å¯ç”¨æ€§ï¼Œä»¥åŠå¯¹å…¨å±€å›¾åƒç‰¹å¾çš„è¿‡åº¦ä¾èµ–ï¼Œè¿™å¾€å¾€å¯¼è‡´å¿½ç•¥ç»†å¾®ä½†ä¸´åºŠä¸Šé‡è¦çš„ç—…ç†åŒºåŸŸã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RegionMed-CLIPï¼Œè¿™æ˜¯ä¸€ä¸ªåŒºåŸŸæ„ŸçŸ¥çš„å¤šæ¨¡å¼å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œå®ƒæ˜¾å¼åœ°ç»“åˆäº†å±€éƒ¨ç—…ç†ä¿¡å·å’Œæ•´ä½“è¯­ä¹‰è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªåˆ›æ–°çš„å…´è¶£åŒºåŸŸï¼ˆROIï¼‰å¤„ç†å™¨ï¼Œå®ƒè‡ªé€‚åº”åœ°é›†æˆç²¾ç»†çš„å±€éƒ¨ç‰¹å¾ä¸å…¨å±€ä¸Šä¸‹æ–‡ï¼Œè¾…ä»¥ä¸€ç§å¢å¼ºå±‚æ¬¡åŒ–å¤šæ¨¡å¼å¯¹é½çš„æ¸è¿›å¼è®­ç»ƒç­–ç•¥ã€‚ä¸ºäº†å®ç°å¯¹å¤§è§„æ¨¡åŒºåŸŸçº§åˆ«è¡¨ç¤ºçš„å­¦ä¹ ï¼Œæˆ‘ä»¬æ„å»ºäº†MedRegion-500kï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥åŒºåŸŸæ³¨é‡Šå’Œå¤šå±‚æ¬¡ä¸´åºŠæè¿°ä¸ºç‰¹è‰²çš„åŒ»å­¦å›¾åƒ-æ–‡æœ¬è¯­æ–™åº“ã€‚åœ¨å›¾åƒ-æ–‡æœ¬æ£€ç´¢ã€é›¶æ ·æœ¬åˆ†ç±»å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRegionMed-CLIPä¸€è‡´ä¸”å¤§å¹…åº¦åœ°è¶…è¶Šäº†æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†åŒºåŸŸæ„ŸçŸ¥å¯¹æ¯”é¢„è®­ç»ƒçš„å…³é”®é‡è¦æ€§ï¼Œå¹¶å°†RegionMed-CLIPå®šä½ä¸ºæ¨è¿›å¤šæ¨¡å¼åŒ»å­¦å›¾åƒç†è§£çš„æœ‰åŠ›åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05244v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒç†è§£åœ¨è‡ªåŠ¨åŒ–è¯Šæ–­å’ŒåŸºäºæ•°æ®çš„ä¸´åºŠå†³ç­–æ”¯æŒä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œä½†é¢ä¸´ç€é«˜è´¨é‡æ ‡æ³¨åŒ»å­¦æ•°æ®æœ‰é™å’Œè¿‡äºä¾èµ–å…¨å±€å›¾åƒç‰¹å¾ä¸¤å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RegionMed-CLIPï¼Œè¿™æ˜¯ä¸€ä¸ªåŒºåŸŸæ„ŸçŸ¥çš„å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œå®ƒæ˜¾å¼åœ°ç»“åˆäº†å±€éƒ¨ç—…ç†ä¿¡å·å’Œæ•´ä½“è¯­ä¹‰è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•æ ¸å¿ƒæ˜¯è‡ªé€‚åº”é›†æˆç²¾ç»†åŒºåŸŸç‰¹å¾ä¸å…¨å±€ä¸Šä¸‹æ–‡çš„æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰å¤„ç†å™¨ï¼Œè¾…ä»¥å¢å¼ºå±‚æ¬¡å¤šæ¨¡æ€å¯¹é½çš„æ¸è¿›è®­ç»ƒç­–ç•¥ã€‚ä¸ºå®ç°å¤§è§„æ¨¡åŒºåŸŸçº§åˆ«è¡¨ç¤ºå­¦ä¹ ï¼Œæˆ‘ä»¬æ„å»ºäº†MedRegion-500kï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«å¹¿æ³›åŒºåŸŸæ³¨é‡Šå’Œå¤šå±‚ä¸´åºŠæè¿°çš„ç»¼åˆåŒ»å­¦å›¾åƒæ–‡æœ¬è¯­æ–™åº“ã€‚å®éªŒè¡¨æ˜ï¼ŒRegionMed-CLIPåœ¨å›¾åƒæ–‡æœ¬æ£€ç´¢ã€é›¶æ ·æœ¬åˆ†ç±»å’Œè§†è§‰é—®ç­”ä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šäº†æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åŒ»å­¦å›¾åƒç†è§£åœ¨è‡ªåŠ¨åŒ–è¯Šæ–­å’Œä¸´åºŠå†³ç­–æ”¯æŒä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>åŒ»å­¦å›¾åƒç†è§£çš„ä¸¤å¤§æŒ‘æˆ˜æ˜¯é«˜è´¨é‡æ ‡æ³¨æ•°æ®æœ‰é™å’Œè¿‡åº¦ä¾èµ–å…¨å±€å›¾åƒç‰¹å¾ã€‚</li>
<li>RegionMed-CLIPæ˜¯ä¸€ä¸ªåŒºåŸŸæ„ŸçŸ¥çš„å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†å±€éƒ¨ç—…ç†ä¿¡å·å’Œæ•´ä½“è¯­ä¹‰è¡¨ç¤ºã€‚</li>
<li>ROIå¤„ç†å™¨æ˜¯RegionMed-CLIPçš„æ ¸å¿ƒï¼Œå®ƒè‡ªé€‚åº”åœ°é›†æˆç²¾ç»†åŒºåŸŸç‰¹å¾ä¸å…¨å±€ä¸Šä¸‹æ–‡ã€‚</li>
<li>æ¸è¿›çš„è®­ç»ƒç­–ç•¥å¢å¼ºäº†RegionMed-CLIPçš„å±‚æ¬¡å¤šæ¨¡æ€å¯¹é½èƒ½åŠ›ã€‚</li>
<li>MedRegion-500kæ˜¯ä¸€ä¸ªç»¼åˆåŒ»å­¦å›¾åƒæ–‡æœ¬è¯­æ–™åº“ï¼ŒåŒ…å«å¹¿æ³›çš„åŒºåŸŸæ³¨é‡Šå’Œå¤šå±‚ä¸´åºŠæè¿°ã€‚</li>
<li>RegionMed-CLIPåœ¨å¤šé¡¹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05244">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4409389a475b3efc3467f6322ecaf107.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0038387d1731557539231913673eb74.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b048fbad3f03f4ac0c39114b067eb10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e98bdf7cef64cadde7f2ee8c5e2dc8cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a93e3776597773699674e91737962a70.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Beyond-Pixels-Medical-Image-Quality-Assessment-with-Implicit-Neural-Representations"><a href="#Beyond-Pixels-Medical-Image-Quality-Assessment-with-Implicit-Neural-Representations" class="headerlink" title="Beyond Pixels: Medical Image Quality Assessment with Implicit Neural   Representations"></a>Beyond Pixels: Medical Image Quality Assessment with Implicit Neural   Representations</h2><p><strong>Authors:Caner Ã–zer, Patryk Rygiel, Bram de Wilde, Ä°lkay Ã–ksÃ¼z, Jelmer M. Wolterink</strong></p>
<p>Artifacts pose a significant challenge in medical imaging, impacting diagnostic accuracy and downstream analysis. While image-based approaches for detecting artifacts can be effective, they often rely on preprocessing methods that can lead to information loss and high-memory-demand medical images, thereby limiting the scalability of classification models. In this work, we propose the use of implicit neural representations (INRs) for image quality assessment. INRs provide a compact and continuous representation of medical images, naturally handling variations in resolution and image size while reducing memory overhead. We develop deep weight space networks, graph neural networks, and relational attention transformers that operate on INRs to achieve image quality assessment. Our method is evaluated on the ACDC dataset with synthetically generated artifact patterns, demonstrating its effectiveness in assessing image quality while achieving similar performance with fewer parameters. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒä¸­ï¼Œä¼ªå½±æ„æˆäº†ä¸€å¤§æŒ‘æˆ˜ï¼Œå½±å“äº†è¯Šæ–­å‡†ç¡®æ€§å’Œåç»­åˆ†æã€‚è™½ç„¶åŸºäºå›¾åƒçš„ä¼ªå½±æ£€æµ‹æ–¹æ³•å¯ä»¥å¾ˆæœ‰æ•ˆï¼Œä½†å®ƒä»¬ç»å¸¸ä¾èµ–äºå¯èƒ½å¯¼è‡´ä¿¡æ¯ä¸¢å¤±å’Œé«˜å†…å­˜éœ€æ±‚çš„åŒ»å­¦å›¾åƒé¢„å¤„ç†æ–¹æ³•ï¼Œä»è€Œé™åˆ¶äº†åˆ†ç±»æ¨¡å‹çš„å¯æ‰©å±•æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰æ¥è¿›è¡Œå›¾åƒè´¨é‡è¯„ä¼°ã€‚INRä¸ºåŒ»å­¦å›¾åƒæä¾›äº†ç´§å‡‘ä¸”è¿ç»­çš„ä»£è¡¨ï¼Œå¯è‡ªç„¶å¤„ç†åˆ†è¾¨ç‡å’Œå›¾åƒå¤§å°çš„å·®å¼‚ï¼ŒåŒæ—¶å‡å°‘å†…å­˜å¼€é”€ã€‚æˆ‘ä»¬å¼€å‘äº†æ·±åº¦æƒé‡ç©ºé—´ç½‘ç»œã€å›¾ç¥ç»ç½‘ç»œå’Œå…³ç³»æ³¨æ„åŠ›è½¬æ¢å™¨ï¼Œå®ƒä»¬åœ¨INRä¸Šè¿è¡Œä»¥å®ç°å›¾åƒè´¨é‡è¯„ä¼°ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ACDCæ•°æ®é›†ä¸Šä½¿ç”¨åˆæˆç”Ÿæˆçš„ä¼ªå½±æ¨¡å¼è¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶åœ¨è¯„ä¼°å›¾åƒè´¨é‡æ—¶çš„æœ‰æ•ˆæ€§ï¼ŒåŒæ—¶ä»¥æ›´å°‘çš„å‚æ•°å®ç°äº†ç±»ä¼¼æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05168v1">PDF</a> Accepted in 16th Machine Learning in Medical Imaging (MLMI 2025)   workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä½¿ç”¨éšå¼ç¥ç»ç½‘ç»œè¡¨ç¤ºï¼ˆINRsï¼‰è¿›è¡ŒåŒ»å­¦å›¾åƒè´¨é‡è¯„ä¼°çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆå¤„ç†åŒ»å­¦å›¾åƒä¸­çš„ä¼ªå½±é—®é¢˜ï¼Œé€šè¿‡ç´§å‡‘ä¸”è¿ç»­çš„å›¾åƒè¡¨ç¤ºï¼Œè‡ªç„¶åº”å¯¹åˆ†è¾¨ç‡å’Œå›¾åƒå¤§å°çš„å·®å¼‚ï¼ŒåŒæ—¶é™ä½å†…å­˜å¼€é”€ã€‚é€šè¿‡æ·±åº¦æƒé‡ç©ºé—´ç½‘ç»œã€å›¾ç¥ç»ç½‘ç»œå’Œå…³ç³»æ³¨æ„åŠ›è½¬æ¢å™¨å¯¹INRsè¿›è¡Œæ“ä½œï¼Œå®ç°äº†å›¾åƒè´¨é‡è¯„ä¼°ã€‚åœ¨ACDCæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨åˆæˆä¼ªå½±æ¨¡å¼è¿›è¡ŒéªŒè¯ï¼Œè¯æ˜è¯¥æ–¹æ³•åœ¨è¯„ä¼°å›¾åƒè´¨é‡æ—¶è¡¨ç°å‡ºè‰²ï¼Œä¸”å‚æ•°æ›´å°‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ªå½±å¯¹åŒ»å­¦æˆåƒæ„æˆæŒ‘æˆ˜ï¼Œå½±å“è¯Šæ–­å’Œä¸‹æ¸¸åˆ†æã€‚</li>
<li>å›¾åƒæ–¹æ³•è™½å¯æœ‰æ•ˆæ£€æµ‹ä¼ªå½±ï¼Œä½†ä¾èµ–çš„é¢„å¤„ç†å¯èƒ½å¯¼è‡´ä¿¡æ¯æŸå¤±åŠé«˜å†…å­˜éœ€æ±‚ã€‚</li>
<li>å¼•å…¥éšå¼ç¥ç»ç½‘ç»œè¡¨ç¤ºï¼ˆINRsï¼‰è¿›è¡ŒåŒ»å­¦å›¾åƒè´¨é‡è¯„ä¼°ã€‚</li>
<li>INRsæä¾›ç´§å‡‘ä¸”è¿ç»­çš„å›¾åƒè¡¨ç¤ºï¼Œé€‚åº”ä¸åŒåˆ†è¾¨ç‡å’Œå›¾åƒå¤§å°ã€‚</li>
<li>ä½¿ç”¨æ·±åº¦æƒé‡ç©ºé—´ç½‘ç»œã€å›¾ç¥ç»ç½‘ç»œå’Œå…³ç³»æ³¨æ„åŠ›è½¬æ¢å™¨å¯¹INRsè¿›è¡Œè¯„ä¼°æ“ä½œã€‚</li>
<li>æ–¹æ³•åœ¨ACDCæ•°æ®é›†ä¸ŠéªŒè¯æœ‰æ•ˆï¼Œåˆæˆä¼ªå½±æ¨¡å¼ä¸‹è¡¨ç°è‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05168">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eab06a6c0c9ff796fe82b9b3d731c6f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b3a28b64649100510671c6669009337.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7bbf2535e930d740726d452a51079f8.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="FedGIN-Federated-Learning-with-Dynamic-Global-Intensity-Non-linear-Augmentation-for-Organ-Segmentation-using-Multi-modal-Images"><a href="#FedGIN-Federated-Learning-with-Dynamic-Global-Intensity-Non-linear-Augmentation-for-Organ-Segmentation-using-Multi-modal-Images" class="headerlink" title="FedGIN: Federated Learning with Dynamic Global Intensity Non-linear   Augmentation for Organ Segmentation using Multi-modal Images"></a>FedGIN: Federated Learning with Dynamic Global Intensity Non-linear   Augmentation for Organ Segmentation using Multi-modal Images</h2><p><strong>Authors:Sachin Dudda Nagaraju, Ashkan Moradi, Bendik Skarre Abrahamsen, Mattijs Elschot</strong></p>
<p>Medical image segmentation plays a crucial role in AI-assisted diagnostics, surgical planning, and treatment monitoring. Accurate and robust segmentation models are essential for enabling reliable, data-driven clinical decision making across diverse imaging modalities. Given the inherent variability in image characteristics across modalities, developing a unified model capable of generalizing effectively to multiple modalities would be highly beneficial. This model could streamline clinical workflows and reduce the need for modality-specific training. However, real-world deployment faces major challenges, including data scarcity, domain shift between modalities (e.g., CT vs. MRI), and privacy restrictions that prevent data sharing. To address these issues, we propose FedGIN, a Federated Learning (FL) framework that enables multimodal organ segmentation without sharing raw patient data. Our method integrates a lightweight Global Intensity Non-linear (GIN) augmentation module that harmonizes modality-specific intensity distributions during local training. We evaluated FedGIN using two types of datasets: an imputed dataset and a complete dataset. In the limited dataset scenario, the model was initially trained using only MRI data, and CT data was added to assess its performance improvements. In the complete dataset scenario, both MRI and CT data were fully utilized for training on all clients. In the limited-data scenario, FedGIN achieved a 12 to 18% improvement in 3D Dice scores on MRI test cases compared to FL without GIN and consistently outperformed local baselines. In the complete dataset scenario, FedGIN demonstrated near-centralized performance, with a 30% Dice score improvement over the MRI-only baseline and a 10% improvement over the CT-only baseline, highlighting its strong cross-modality generalization under privacy constraints. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨äººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­ã€æ‰‹æœ¯è§„åˆ’å’Œæ²»ç–—ç›‘æµ‹ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚å‡†ç¡®ä¸”ç¨³å¥çš„åˆ†å‰²æ¨¡å‹å¯¹äºåœ¨å¤šç§æˆåƒæ¨¡å¼ä¸Šå®ç°å¯é çš„æ•°æ®é©±åŠ¨ä¸´åºŠå†³ç­–è‡³å…³é‡è¦ã€‚è€ƒè™‘åˆ°ä¸åŒæ¨¡æ€å›¾åƒç‰¹æ€§å›ºæœ‰çš„å˜åŒ–ï¼Œå¼€å‘ä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆæ¨å¹¿åˆ°å¤šç§æ¨¡æ€çš„ç»Ÿä¸€æ¨¡å‹å°†å¤§æœ‰è£¨ç›Šã€‚è¯¥æ¨¡å‹å¯ä»¥ç®€åŒ–ä¸´åºŠå·¥ä½œæµç¨‹ï¼Œå‡å°‘é’ˆå¯¹ç‰¹å®šæ¨¡æ€çš„åŸ¹è®­éœ€æ±‚ã€‚ç„¶è€Œï¼Œåœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²ä¸­ï¼Œé¢ä¸´ç€æ•°æ®ç¨€ç¼ºã€ä¸åŒæ¨¡æ€ä¹‹é—´çš„é¢†åŸŸåç§»ï¼ˆä¾‹å¦‚CTä¸MRIï¼‰ä»¥åŠé˜»æ­¢æ•°æ®å…±äº«çš„éšç§é™åˆ¶ç­‰é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FedGINï¼Œè¿™æ˜¯ä¸€ä¸ªè”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸å…±äº«åŸå§‹æ‚£è€…æ•°æ®çš„æƒ…å†µä¸‹å®ç°å¤šæ¨¡æ€å™¨å®˜åˆ†å‰²ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ•´åˆäº†ä¸€ä¸ªè½»é‡çº§çš„å…¨å±€å¼ºåº¦éçº¿æ€§ï¼ˆGINï¼‰å¢å¼ºæ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨æœ¬åœ°è®­ç»ƒè¿‡ç¨‹ä¸­åè°ƒç‰¹å®šæ¨¡æ€çš„å¼ºåº¦åˆ†å¸ƒã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ç§ç±»å‹çš„æ•°æ®é›†å¯¹FedGINè¿›è¡Œäº†è¯„ä¼°ï¼šä¸€ä¸ªå¡«å……æ•°æ®é›†å’Œä¸€ä¸ªå®Œæ•´æ•°æ®é›†ã€‚åœ¨æœ‰é™æ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹æœ€åˆä»…ä½¿ç”¨MRIæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œç„¶ååŠ å…¥CTæ•°æ®ä»¥è¯„ä¼°å…¶æ€§èƒ½æ”¹è¿›ã€‚åœ¨å®Œæ•´æ•°æ®é›†çš„æƒ…å†µä¸‹ï¼ŒMRIå’ŒCTæ•°æ®éƒ½ç”¨äºæ‰€æœ‰å®¢æˆ·ç«¯çš„è®­ç»ƒã€‚åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œä¸æ²¡æœ‰GINçš„FLç›¸æ¯”ï¼ŒFedGINåœ¨MRIæµ‹è¯•æ¡ˆä¾‹ä¸­çš„3D Diceå¾—åˆ†æé«˜äº†12%åˆ°18%ï¼Œå¹¶ä¸”å§‹ç»ˆè¶…è¿‡äº†æœ¬åœ°åŸºå‡†ã€‚åœ¨å®Œæ•´æ•°æ®é›†çš„æƒ…å†µä¸‹ï¼ŒFedGINè¡¨ç°å‡ºæ¥è¿‘é›†ä¸­å¼çš„æ€§èƒ½ï¼Œä¸ä»…ä½¿ç”¨MRIçš„åŸºçº¿ç›¸æ¯”ï¼ŒDiceå¾—åˆ†æé«˜äº†30%ï¼Œä¸ä»…ä½¿ç”¨CTçš„åŸºçº¿ç›¸æ¯”ï¼Œæé«˜äº†10%ï¼Œè¿™çªæ˜¾äº†å…¶åœ¨éšç§çº¦æŸä¸‹å¼ºå¤§çš„è·¨æ¨¡æ€æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05137v1">PDF</a> Paper Accepted at MICCAI 2025 DeCaf Workshop Track</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨äººå·¥æ™ºèƒ½è¾…åŠ©è¯Šæ–­ã€æ‰‹æœ¯è§„åˆ’å’Œæ²»ç–—ç›‘æµ‹ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚å¼€å‘å‡†ç¡®ä¸”ç¨³å¥çš„åˆ†å‰²æ¨¡å‹å¯¹äºå®ç°å¯é çš„æ•°æ®é©±åŠ¨ä¸´åºŠå†³ç­–è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šç§æˆåƒæ¨¡æ€ä¸­ã€‚ä¸ºåº”å¯¹ä¸åŒæ¨¡æ€é—´å›¾åƒç‰¹æ€§å›ºæœ‰çš„å·®å¼‚æ€§ï¼Œå¼€å‘èƒ½å¤Ÿæœ‰æ•ˆæ³›åŒ–è‡³å¤šç§æ¨¡æ€çš„ç»Ÿä¸€æ¨¡å‹å°†å¤§æœ‰ï¿½ï¿½dç›Šã€‚è¯¥æ¨¡å‹èƒ½ç®€åŒ–ä¸´åºŠå·¥ä½œæµç¨‹ï¼Œå‡å°‘é’ˆå¯¹ç‰¹å®šæ¨¡æ€çš„åŸ¹è®­éœ€æ±‚ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œéƒ¨ç½²é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®ç¨€ç¼ºã€æ¨¡æ€é—´çš„é¢†åŸŸåç§»ï¼ˆå¦‚CTä¸MRIï¼‰ä»¥åŠé˜»æ­¢æ•°æ®å…±äº«çš„éšç§é™åˆ¶ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºFedGINï¼Œä¸€ä¸ªè”é‚¦å­¦ä¹ ï¼ˆFLï¼‰æ¡†æ¶ï¼Œå¯åœ¨ä¸å…±äº«åŸå§‹æ‚£è€…æ•°æ®çš„æƒ…å†µä¸‹å®ç°å¤šæ¨¡æ€å™¨å®˜åˆ†å‰²ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ•´åˆäº†è½»é‡çº§çš„å…¨å±€å¼ºåº¦éçº¿æ€§ï¼ˆGINï¼‰å¢å¼ºæ¨¡å—ï¼Œåœ¨æœ¬åœ°è®­ç»ƒè¿‡ç¨‹ä¸­åè°ƒæ¨¡æ€ç‰¹å®šå¼ºåº¦åˆ†å¸ƒã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ç§ç±»å‹çš„æ•°æ®é›†å¯¹FedGINè¿›è¡Œäº†è¯„ä¼°ï¼šæ’è¡¥æ•°æ®é›†å’Œå®Œæ•´æ•°æ®é›†ã€‚åœ¨æœ‰é™æ•°æ®é›†åœºæ™¯ä¸­ï¼Œæ¨¡å‹æœ€åˆä»…ä½¿ç”¨MRIæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶åŠ å…¥CTæ•°æ®ä»¥è¯„ä¼°å…¶æ€§èƒ½æ”¹è¿›ã€‚åœ¨å®Œæ•´æ•°æ®é›†åœºæ™¯ä¸­ï¼ŒMRIå’ŒCTæ•°æ®å‡ç”¨äºæ‰€æœ‰å®¢æˆ·ç«¯çš„è®­ç»ƒã€‚åœ¨æœ‰é™æ•°æ®åœºæ™¯ä¸­ï¼Œä¸æ— GINçš„FLç›¸æ¯”ï¼ŒFedGINåœ¨MRIæµ‹è¯•ç—…ä¾‹ä¸­çš„3Déª°å­åˆ†æ•°æé«˜äº†12%åˆ°18%ï¼Œå¹¶ä¸”å§‹ç»ˆä¼˜äºæœ¬åœ°åŸºçº¿ã€‚åœ¨å®Œæ•´æ•°æ®é›†åœºæ™¯ä¸­ï¼ŒFedGINè¡¨ç°å‡ºæ¥è¿‘é›†ä¸­åŒ–çš„æ€§èƒ½ï¼Œä¸MRI-onlyåŸºçº¿ç›¸æ¯”ï¼Œéª°å­åˆ†æ•°æé«˜äº†30%ï¼Œä¸CT-onlyåŸºçº¿ç›¸æ¯”ï¼Œæé«˜äº†10%ï¼Œè¿™å‡¸æ˜¾äº†å…¶åœ¨éšç§çº¦æŸä¸‹çš„è·¨æ¨¡æ€æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>è¦ç‚¹è§£æ</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨å¤šä¸ªåŒ»ç–—åº”ç”¨ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ä¸åŒæˆåƒæ¨¡æ€ä¹‹é—´å›¾åƒç‰¹æ€§çš„å·®å¼‚æ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼€å‘èƒ½æ³›åŒ–è‡³å¤šç§æ¨¡æ€çš„ç»Ÿä¸€æ¨¡å‹çš„ä¼˜åŠ¿ã€‚</li>
<li>ç°å®ä¸–ç•Œéƒ¨ç½²é¢ä¸´çš„ä¸‰å¤§æŒ‘æˆ˜ï¼šæ•°æ®ç¨€ç¼ºã€é¢†åŸŸåç§»å’Œéšç§é™åˆ¶ã€‚</li>
<li>FedGINè”é‚¦å­¦ä¹ æ¡†æ¶çš„æå‡ºä»¥åŠå…¶æ•´åˆçš„GINæ¨¡å—çš„ä½œç”¨ã€‚</li>
<li>FedGINåœ¨æœ‰é™æ•°æ®é›†å’Œå®Œæ•´æ•°æ®é›†åœºæ™¯ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05137">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0306a3a321ea9d3eec149a2e7c7c8e6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0786af1fc754cbe6f20bb203b54bb791.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60ab1ac82b04dc0779aa2e6799b4284a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bde6cf13a33986dabfd4e7ff3a9ea9f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Latent-Expression-Generation-for-Referring-Image-Segmentation-and-Grounding"><a href="#Latent-Expression-Generation-for-Referring-Image-Segmentation-and-Grounding" class="headerlink" title="Latent Expression Generation for Referring Image Segmentation and   Grounding"></a>Latent Expression Generation for Referring Image Segmentation and   Grounding</h2><p><strong>Authors:Seonghoon Yu, Joonbeom Hong, Joonseok Lee, Jeany Son</strong></p>
<p>Visual grounding tasks, such as referring image segmentation (RIS) and referring expression comprehension (REC), aim to localize a target object based on a given textual description. The target object in an image can be described in multiple ways, reflecting diverse attributes such as color, position, and more. However, most existing methods rely on a single textual input, which captures only a fraction of the rich information available in the visual domain. This mismatch between rich visual details and sparse textual cues can lead to the misidentification of similar objects. To address this, we propose a novel visual grounding framework that leverages multiple latent expressions generated from a single textual input by incorporating complementary visual details absent from the original description. Specifically, we introduce subject distributor and visual concept injector modules to embed both shared-subject and distinct-attributes concepts into the latent representations, thereby capturing unique and target-specific visual cues. We also propose a positive-margin contrastive learning strategy to align all latent expressions with the original text while preserving subtle variations. Experimental results show that our method not only outperforms state-of-the-art RIS and REC approaches on multiple benchmarks but also achieves outstanding performance on the generalized referring expression segmentation (GRES) benchmark. </p>
<blockquote>
<p>è§†è§‰å®šä½ä»»åŠ¡ï¼Œå¦‚å¼•ç”¨å›¾åƒåˆ†å‰²ï¼ˆRISï¼‰å’Œå¼•ç”¨è¡¨è¾¾å¼ç†è§£ï¼ˆRECï¼‰ï¼Œæ—¨åœ¨æ ¹æ®ç»™å®šçš„æ–‡æœ¬æè¿°å®šä½ç›®æ ‡å¯¹è±¡ã€‚å›¾åƒä¸­çš„ç›®æ ‡å¯¹è±¡å¯ä»¥ç”¨å¤šç§æ–¹å¼æè¿°ï¼Œåæ˜ é¢œè‰²ã€ä½ç½®ç­‰å¤šç§å±æ€§ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¾èµ–äºå•ä¸ªæ–‡æœ¬è¾“å…¥ï¼Œåªèƒ½æ•æ‰åˆ°è§†è§‰é¢†åŸŸä¸°å¯Œä¿¡æ¯çš„ä¸€å°éƒ¨åˆ†ã€‚ä¸°å¯Œè§†è§‰ç»†èŠ‚å’Œç¨€ç–æ–‡æœ¬çº¿ç´¢ä¹‹é—´çš„ä¸åŒ¹é…å¯èƒ½å¯¼è‡´ç±»ä¼¼å¯¹è±¡çš„è¯¯è¯†åˆ«ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰å®šä½æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å•ä¸ªæ–‡æœ¬è¾“å…¥äº§ç”Ÿçš„å¤šä¸ªæ½œåœ¨è¡¨è¾¾å¼ï¼Œå¹¶èå…¥åŸå§‹æè¿°ä¸­ç¼ºå¤±çš„äº’è¡¥è§†è§‰ç»†èŠ‚ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸»ä½“åˆ†é…å™¨å’Œè§†è§‰æ¦‚å¿µæ³¨å…¥å™¨æ¨¡å—ï¼Œå°†å…±äº«ä¸»ä½“å’Œç‹¬ç‰¹å±æ€§æ¦‚å¿µåµŒå…¥åˆ°æ½œåœ¨è¡¨ç¤ºä¸­ï¼Œä»è€Œæ•è·ç‹¬ç‰¹ä¸”é’ˆå¯¹ç›®æ ‡å¯¹è±¡çš„è§†è§‰çº¿ç´¢ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ­£è¾¹è·å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œå°†æ‰€æœ‰æ½œåœ¨è¡¨è¾¾å¼ä¸åŸå§‹æ–‡æœ¬å¯¹é½ï¼ŒåŒæ—¶ä¿ç•™ç»†å¾®å˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„RISå’ŒRECæ–¹æ³•ï¼Œè€Œä¸”åœ¨å¹¿ä¹‰å¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²ï¼ˆGRESï¼‰åŸºå‡†æµ‹è¯•ä¸Šä¹Ÿå–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05123v1">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ç¯‡å…³äºè§†è§‰å®šä½ä»»åŠ¡çš„ç ”ç©¶è®ºæ–‡ï¼Œä¸»è¦ä»‹ç»äº†å¦‚ä½•é€šè¿‡ç»“åˆå¤šç§æ½œåœ¨è¡¨è¾¾å’Œè§†è§‰ç»†èŠ‚æ¥è§£å†³å•ä¸€æ–‡æœ¬è¾“å…¥å¯¼è‡´çš„è§†è§‰ä¸æ–‡æœ¬ä¿¡æ¯ä¸åŒ¹é…çš„é—®é¢˜ã€‚è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰å®šä½æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ä»å•ä¸€æ–‡æœ¬è¾“å…¥ç”Ÿæˆå¤šä¸ªæ½œåœ¨è¡¨è¾¾ï¼Œå¹¶èå…¥ç¼ºå¤±çš„è§†è§‰ç»†èŠ‚ï¼Œä»è€Œæé«˜ç›®æ ‡å¯¹è±¡çš„è¯†åˆ«å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰å®šä½ä»»åŠ¡æ—¨åœ¨æ ¹æ®ç»™å®šçš„æ–‡æœ¬æè¿°å®šä½å›¾åƒä¸­çš„ç›®æ ‡å¯¹è±¡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–å•ä¸€æ–‡æœ¬è¾“å…¥ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨è§†è§‰åŸŸä¸­çš„ä¸°å¯Œä¿¡æ¯ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰å®šä½æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¤šç§æ½œåœ¨è¡¨è¾¾å’Œè§†è§‰ç»†èŠ‚æ¥è§£å†³è§†è§‰ä¸æ–‡æœ¬ä¿¡æ¯çš„ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>å¼•å…¥ä¸»ä½“åˆ†å¸ƒå™¨å’Œè§†è§‰æ¦‚å¿µæ³¨å…¥å™¨æ¨¡å—ï¼Œå°†å…±äº«ä¸»ä½“å’Œç‹¬ç‰¹å±æ€§æ¦‚å¿µåµŒå…¥åˆ°æ½œåœ¨è¡¨ç¤ºä¸­ï¼Œä»è€Œæ•æ‰ç‹¬ç‰¹çš„ç›®æ ‡ç‰¹å®šè§†è§‰çº¿ç´¢ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ­£å‘è¾¹è·å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œå°†æ‰€æœ‰æ½œåœ¨è¡¨è¾¾ä¸åŸå§‹æ–‡æœ¬å¯¹é½ï¼ŒåŒæ—¶ä¿ç•™ç»†å¾®å˜åŒ–ã€‚</li>
<li>è®ºæ–‡çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¹¿ä¹‰å¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²ï¼ˆGRESï¼‰åŸºå‡†æµ‹è¯•ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05123">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3bb3559ce40f86bb1210f870eaa24148.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-93230b615fbc5169c6979697497918dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f2cb9397e789fb3011b3a508ef3b593.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="MedMambaLite-Hardware-Aware-Mamba-for-Medical-Image-Classification"><a href="#MedMambaLite-Hardware-Aware-Mamba-for-Medical-Image-Classification" class="headerlink" title="MedMambaLite: Hardware-Aware Mamba for Medical Image Classification"></a>MedMambaLite: Hardware-Aware Mamba for Medical Image Classification</h2><p><strong>Authors:Romina Aalishah, Mozhgan Navardi, Tinoosh Mohsenin</strong></p>
<p>AI-powered medical devices have driven the need for real-time, on-device inference such as biomedical image classification. Deployment of deep learning models at the edge is now used for applications such as anomaly detection and classification in medical images. However, achieving this level of performance on edge devices remains challenging due to limitations in model size and computational capacity. To address this, we present MedMambaLite, a hardware-aware Mamba-based model optimized through knowledge distillation for medical image classification. We start with a powerful MedMamba model, integrating a Mamba structure for efficient feature extraction in medical imaging. We make the model lighter and faster in training and inference by modifying and reducing the redundancies in the architecture. We then distill its knowledge into a smaller student model by reducing the embedding dimensions. The optimized model achieves 94.5% overall accuracy on 10 MedMNIST datasets. It also reduces parameters 22.8x compared to MedMamba. Deployment on an NVIDIA Jetson Orin Nano achieves 35.6 GOPS&#x2F;J energy per inference. This outperforms MedMamba by 63% improvement in energy per inference. </p>
<blockquote>
<p>AIé©±åŠ¨çš„åŒ»ç–—å™¨æ¢°æ¨åŠ¨äº†å®æ—¶è®¾å¤‡ç«¯æ¨æ–­çš„éœ€æ±‚ï¼Œä¾‹å¦‚ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»ã€‚è¾¹ç¼˜éƒ¨ç½²æ·±åº¦å­¦ä¹ æ¨¡å‹ç°åœ¨è¢«ç”¨äºåŒ»å­¦å›¾åƒä¸­çš„å¼‚å¸¸æ£€æµ‹å’Œåˆ†ç±»ç­‰åº”ç”¨ã€‚ç„¶è€Œï¼Œç”±äºæ¨¡å‹å¤§å°å’Œè®¡ç®—èƒ½åŠ›çš„é™åˆ¶ï¼Œåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°è¿™ä¸€çº§åˆ«çš„æ€§èƒ½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MedMambaLiteï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºç¡¬ä»¶æ„ŸçŸ¥çš„Mambaæ¨¡å‹ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†ç±»è¿›è¡Œäº†ä¼˜åŒ–ã€‚æˆ‘ä»¬ä»åŠŸèƒ½å¼ºå¤§çš„MedMambaæ¨¡å‹å¼€å§‹ï¼Œé›†æˆäº†Mambaç»“æ„ï¼Œå®ç°åŒ»å­¦æˆåƒä¸­çš„é«˜æ•ˆç‰¹å¾æå–ã€‚æˆ‘ä»¬é€šè¿‡ä¿®æ”¹å¹¶å‡å°‘æ¶æ„ä¸­çš„å†—ä½™éƒ¨åˆ†ï¼Œä½¿æ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨æ–­ä¸Šæ›´è½»ä¾¿ã€æ›´å¿«ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡å‡å°åµŒå…¥ç»´åº¦å°†å…¶çŸ¥è¯†è’¸é¦åˆ°ä¸€ä¸ªè¾ƒå°çš„å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚ä¼˜åŒ–åçš„æ¨¡å‹åœ¨10ä¸ªMedMNISTæ•°æ®é›†ä¸Šè¾¾åˆ°äº†94.5%çš„æ€»ä½“å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä¸MedMambaç›¸æ¯”å‡å°‘äº†å‚æ•°é«˜è¾¾22.8å€ã€‚åœ¨NVIDIA Jetson Orin Nanoä¸Šçš„éƒ¨ç½²å¯å®ç°æ¯æ¨æ–­èƒ½è€—ä¸º35.6 GOPS&#x2F;Jã€‚è¿™åœ¨æ¯æ¨æ–­èƒ½è€—æ–¹é¢æ¯”MedMambaæé«˜äº†63%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05049v1">PDF</a> 21st IEEE Biomedical Circuits and Systems Conference (BioCAS) 2025</p>
<p><strong>Summary</strong><br>     åŸºäºäººå·¥æ™ºèƒ½çš„åŒ»ç–—è®¾å¤‡éœ€è¦å®æ—¶åœ¨è®¾å¤‡ä¸Šè¿›è¡Œåˆ†ææ¨æ–­ï¼Œå¦‚ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»ã€‚è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²æ·±åº¦å­¦ä¹ æ¨¡å‹ç”¨äºåŒ»å­¦å›¾åƒä¸­çš„å¼‚å¸¸æ£€æµ‹å’Œåˆ†ç±»ç­‰åº”ç”¨ã€‚ç„¶è€Œï¼Œç”±äºæ¨¡å‹å¤§å°å’Œè®¡ç®—èƒ½åŠ›çš„é™åˆ¶ï¼Œå®ç°è¿™ä¸€æ€§èƒ½åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šä»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MedMambaLiteæ¨¡å‹ï¼Œå®ƒæ˜¯åŸºäºMambaç»“æ„è¿›è¡Œä¼˜åŒ–çš„ç¡¬ä»¶æ„ŸçŸ¥æ¨¡å‹ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†ç±»ã€‚é€šè¿‡çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œæˆ‘ä»¬ä»å¼ºå¤§çš„MedMambaæ¨¡å‹å‡ºå‘ï¼Œé›†æˆMambaç»“æ„ä»¥é«˜æ•ˆæå–åŒ»å­¦å›¾åƒç‰¹å¾ã€‚é€šè¿‡ä¿®æ”¹å’Œå‡å°‘æ¶æ„ä¸­çš„å†—ä½™éƒ¨åˆ†ï¼Œä½¿æ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨æ–­ä¸Šæ›´åŠ è½»ä¾¿å’Œå¿«é€Ÿã€‚ç„¶åå°†çŸ¥è¯†è’¸é¦åˆ°è¾ƒå°çš„å­¦ç”Ÿæ¨¡å‹ä¸­ï¼Œå‡å°‘åµŒå…¥ç»´åº¦ã€‚ä¼˜åŒ–åçš„æ¨¡å‹åœ¨10ä¸ªMedMNISTæ•°æ®é›†ä¸Šè¾¾åˆ°äº†94.5%çš„æ€»ä½“å‡†ç¡®ç‡ï¼Œå¹¶ä¸”å‚æ•°å‡å°‘äº†22.8å€ã€‚åœ¨NVIDIA Jetson Orin Nanoä¸Šè¿›è¡Œéƒ¨ç½²æ—¶ï¼Œæ¯æ¬¡æ¨ç†çš„èƒ½é‡è¾¾åˆ°35.6 GOPS&#x2F;Jï¼Œç›¸è¾ƒäºMedMambaæ¨¡å‹åœ¨èƒ½é‡ä½¿ç”¨æ•ˆç‡ä¸Šæé«˜äº†63%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIé©±åŠ¨çš„åŒ»ç–—è®¾å¤‡éœ€è¦å®æ—¶åœ¨è®¾å¤‡ä¸Šè¿›è¡Œåˆ†ææ¨æ–­ï¼Œå¦‚ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»ã€‚</li>
<li>åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²æ·±åº¦å­¦ä¹ æ¨¡å‹é¢ä¸´æ¨¡å‹å¤§å°å’Œè®¡ç®—èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚</li>
<li>MedMambaLiteæ¨¡å‹æ˜¯åŸºäºMambaç»“æ„ä¼˜åŒ–çš„ç¡¬ä»¶æ„ŸçŸ¥æ¨¡å‹ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†ç±»ã€‚</li>
<li>é€šè¿‡çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œä¼˜åŒ–åçš„æ¨¡å‹å®ç°äº†é«˜æ•ˆçš„ç‰¹å¾æå–å’Œæ¨ç†é€Ÿåº¦ã€‚</li>
<li>ä¼˜åŒ–åçš„æ¨¡å‹åœ¨MedMNISTæ•°æ®é›†ä¸Šè¾¾åˆ°äº†94.5%çš„æ€»ä½“å‡†ç¡®ç‡ï¼Œå‚æ•°å¤§å¤§å‡å°‘ã€‚</li>
<li>MedMambaLiteæ¨¡å‹åœ¨NVIDIA Jetson Orin Nanoä¸Šçš„éƒ¨ç½²å®ç°äº†è¾ƒé«˜çš„èƒ½é‡ä½¿ç”¨æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05049">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8b26e14f66480d05a298d175bdd29de4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d9af9c90c8c8d75e8e0d6a0f7842235.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e306260e7f3b54baa489f2111ba2969a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c0fa3fa96b602b04f662d827c5f3b33.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Multimodal-Causal-Driven-Representation-Learning-for-Generalizable-Medical-Image-Segmentation"><a href="#Multimodal-Causal-Driven-Representation-Learning-for-Generalizable-Medical-Image-Segmentation" class="headerlink" title="Multimodal Causal-Driven Representation Learning for Generalizable   Medical Image Segmentation"></a>Multimodal Causal-Driven Representation Learning for Generalizable   Medical Image Segmentation</h2><p><strong>Authors:Xusheng Liang, Lihua Zhou, Nianxin Li, Miao Xu, Ziyang Song, Dong Yi, Jinlin Wu, Hongbin Liu, Jiebo Luo, Zhen Lei</strong></p>
<p>Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable zero-shot capabilities in various computer vision tasks. However, their application to medical imaging remains challenging due to the high variability and complexity of medical data. Specifically, medical images often exhibit significant domain shifts caused by various confounders, including equipment differences, procedure artifacts, and imaging modes, which can lead to poor generalization when models are applied to unseen domains. To address this limitation, we propose Multimodal Causal-Driven Representation Learning (MCDRL), a novel framework that integrates causal inference with the VLM to tackle domain generalization in medical image segmentation. MCDRL is implemented in two steps: first, it leverages CLIPâ€™s cross-modal capabilities to identify candidate lesion regions and construct a confounder dictionary through text prompts, specifically designed to represent domain-specific variations; second, it trains a causal intervention network that utilizes this dictionary to identify and eliminate the influence of these domain-specific variations while preserving the anatomical structural information critical for segmentation tasks. Extensive experiments demonstrate that MCDRL consistently outperforms competing methods, yielding superior segmentation accuracy and exhibiting robust generalizability. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ï¼Œå¦‚CLIPï¼Œåœ¨å„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å‡ºè‰²çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºå…¶åº”ç”¨äºåŒ»å­¦æˆåƒæ—¶é¢ä¸´çš„é«˜å¯å˜æ€§å’Œå¤æ‚æ€§ï¼Œå…¶åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å…·ä½“æ¥è¯´ï¼ŒåŒ»å­¦å›¾åƒé€šå¸¸ä¼šå‡ºç°ç”±å„ç§æ··æ‚å› ç´ å¼•èµ·çš„æ˜¾è‘—é¢†åŸŸåç§»ï¼ŒåŒ…æ‹¬è®¾å¤‡å·®å¼‚ã€ç¨‹åºä¼ªå½±å’Œæˆåƒæ¨¡å¼ï¼Œå½“æ¨¡å‹åº”ç”¨äºæœªè§é¢†åŸŸæ—¶ï¼Œå¯èƒ½å¯¼è‡´è¾ƒå·®çš„æ³›åŒ–æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€å› æœé©±åŠ¨è¡¨ç¤ºå­¦ä¹ ï¼ˆMCDRLï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†å› æœæ¨ç†ä¸VLMç›¸ç»“åˆï¼Œè§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„é¢†åŸŸæ³›åŒ–é—®é¢˜ã€‚MCDRLåˆ†ä¸¤æ­¥å®ç°ï¼šé¦–å…ˆï¼Œå®ƒåˆ©ç”¨CLIPçš„è·¨æ¨¡æ€èƒ½åŠ›æ¥è¯†åˆ«å€™é€‰ç—…å˜åŒºåŸŸï¼Œå¹¶é€šè¿‡æ–‡æœ¬æç¤ºæ„å»ºæ··æ‚å› ç´ è¯å…¸ï¼Œè¿™äº›æ–‡æœ¬æç¤ºä¸“é—¨ç”¨äºè¡¨ç¤ºé¢†åŸŸç‰¹å®šçš„å˜åŒ–ï¼›å…¶æ¬¡ï¼Œå®ƒè®­ç»ƒä¸€ä¸ªå› æœå¹²é¢„ç½‘ç»œï¼Œåˆ©ç”¨è¿™ä¸ªè¯å…¸æ¥è¯†åˆ«å’Œæ¶ˆé™¤è¿™äº›é¢†åŸŸç‰¹å®šå˜åŒ–çš„å½±å“ï¼ŒåŒæ—¶ä¿ç•™å¯¹åˆ†å‰²ä»»åŠ¡è‡³å…³é‡è¦çš„è§£å‰–ç»“æ„ä¿¡æ¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMCDRLå§‹ç»ˆä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå…·æœ‰æ›´é«˜çš„åˆ†å‰²ç²¾åº¦å’Œç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05008v1">PDF</a> Under Review</p>
<p><strong>Summary</strong><br>åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œä½†åœ¨åŒ»å­¦æˆåƒåº”ç”¨ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚é’ˆå¯¹åŒ»å­¦å›¾åƒé¢†åŸŸçš„é«˜å˜æ€§å’Œå¤æ‚æ€§ï¼Œæå‡ºå¤šæ¨¡æ€å› æœé©±åŠ¨è¡¨ç¤ºå­¦ä¹ ï¼ˆMCDRLï¼‰æ¡†æ¶ï¼Œç»“åˆå› æœæ¨ç†ä¸VLMè§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„é¢†åŸŸæ³›åŒ–é—®é¢˜ã€‚MCDRLé€šè¿‡CLIPçš„è·¨æ¨¡æ€èƒ½åŠ›è¯†åˆ«å€™é€‰ç—…å˜åŒºåŸŸï¼Œæ„å»ºä»£è¡¨é¢†åŸŸç‰¹å®šå˜åŒ–çš„å¹²æ‰°å­—å…¸ï¼Œå¹¶è®­ç»ƒå› æœå¹²é¢„ç½‘ç»œåˆ©ç”¨è¯¥å­—å…¸æ¶ˆé™¤é¢†åŸŸç‰¹å®šå˜åŒ–çš„å½±å“ï¼ŒåŒæ—¶ä¿ç•™å¯¹åˆ†å‰²ä»»åŠ¡è‡³å…³é‡è¦çš„è§£å‰–ç»“æ„ä¿¡æ¯ã€‚å®éªŒè¯æ˜MCDRLåœ¨åˆ†å‰²å‡†ç¡®æ€§å’Œæ³›åŒ–æ€§æ–¹é¢å‡è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œä½†åœ¨åŒ»å­¦æˆåƒåº”ç”¨ä¸Šä»é¢ä¸´é«˜å˜æ€§å’Œå¤æ‚æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>MCDRLæ¡†æ¶ç»“åˆå› æœæ¨ç†ä¸VLMè§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„é¢†åŸŸæ³›åŒ–é—®é¢˜ã€‚</li>
<li>MCDRLé€šè¿‡CLIPçš„è·¨æ¨¡æ€èƒ½åŠ›è¯†åˆ«å€™é€‰ç—…å˜åŒºåŸŸå¹¶æ„å»ºå¹²æ‰°å­—å…¸ï¼Œä»¥ä»£è¡¨é¢†åŸŸç‰¹å®šçš„å˜åŒ–ã€‚</li>
<li>MCDRLåˆ©ç”¨æ„å»ºçš„å¹²æ‰°å­—å…¸è®­ç»ƒå› æœå¹²é¢„ç½‘ç»œï¼Œä»¥æ¶ˆé™¤é¢†åŸŸç‰¹å®šå˜åŒ–çš„å½±å“ã€‚</li>
<li>MCDRLåœ¨åˆ†å‰²å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬åœ¨æœªè§é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>MCDRLèƒ½å¤ŸåŒæ—¶ä¿ç•™å¯¹åˆ†å‰²ä»»åŠ¡è‡³å…³é‡è¦çš„è§£å‰–ç»“æ„ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05008">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-74f25f73a82a947bda03d679c70f7bea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb8cd88ff77e00ce185898cfeb56f95c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a3f5f1d594d7ffc1d0d3fec0b2e8f4b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b2f06b3d3ec69174b4476bbbee23b000.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5e32283f1303fc48a7d588b9d5d91a6.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Policy-to-Assist-Iteratively-Local-Segmentation-Optimising-Modality-and-Location-Selection-for-Prostate-Cancer-Localisation"><a href="#Policy-to-Assist-Iteratively-Local-Segmentation-Optimising-Modality-and-Location-Selection-for-Prostate-Cancer-Localisation" class="headerlink" title="Policy to Assist Iteratively Local Segmentation: Optimising Modality and   Location Selection for Prostate Cancer Localisation"></a>Policy to Assist Iteratively Local Segmentation: Optimising Modality and   Location Selection for Prostate Cancer Localisation</h2><p><strong>Authors:Xiangcen Wu, Shaheer U. Saeed, Yipei Wang, Ester Bonmati Coll, Yipeng Hu</strong></p>
<p>Radiologists often mix medical image reading strategies, including inspection of individual modalities and local image regions, using information at different locations from different images independently as well as concurrently. In this paper, we propose a recommend system to assist machine learning-based segmentation models, by suggesting appropriate image portions along with the best modality, such that prostate cancer segmentation performance can be maximised. Our approach trains a policy network that assists tumor localisation, by recommending both the optimal imaging modality and the specific sections of interest for review. During training, a pre-trained segmentation network mimics radiologist inspection on individual or variable combinations of these imaging modalities and their sections - selected by the policy network. Taking the locally segmented regions as an input for the next step, this dynamic decision making process iterates until all cancers are best localised. We validate our method using a data set of 1325 labelled multiparametric MRI images from prostate cancer patients, demonstrating its potential to improve annotation efficiency and segmentation accuracy, especially when challenging pathology is present. Experimental results show that our approach can surpass standard segmentation networks. Perhaps more interestingly, our trained agent independently developed its own optimal strategy, which may or may not be consistent with current radiologist guidelines such as PI-RADS. This observation also suggests a promising interactive application, in which the proposed policy networks assist human radiologists. </p>
<blockquote>
<p>æ”¾å°„ç§‘åŒ»ç”Ÿåœ¨åŒ»å­¦å›¾åƒé˜…è¯»ç­–ç•¥ä¸Šé€šå¸¸ä¼šæ··åˆä½¿ç”¨å¤šç§æ–¹æ³•ï¼ŒåŒ…æ‹¬æ£€æŸ¥å„ç§å•ä¸€æ¨¡æ€å’Œå±€éƒ¨å›¾åƒåŒºåŸŸï¼Œç‹¬ç«‹åœ°åŒæ—¶ä½¿ç”¨ä¸åŒä½ç½®çš„ä¸åŒå›¾åƒçš„ä¿¡æ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨èç³»ç»Ÿï¼Œä»¥ååŠ©åŸºäºæœºå™¨å­¦ä¹ çš„åˆ†å‰²æ¨¡å‹ï¼Œé€šè¿‡å»ºè®®é€‚å½“çš„å›¾åƒéƒ¨åˆ†ä»¥åŠæœ€ä½³æ¨¡æ€ï¼Œæœ€å¤§é™åº¦åœ°æé«˜å‰åˆ—è…ºç™Œåˆ†å‰²çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•è®­ç»ƒäº†ä¸€ä¸ªç­–ç•¥ç½‘ç»œï¼Œé€šè¿‡æ¨èæœ€ä½³çš„æˆåƒæ¨¡æ€å’Œç‰¹å®šå®¡æŸ¥çš„æ„Ÿå…´è¶£éƒ¨åˆ†ï¼Œæ¥ååŠ©è‚¿ç˜¤å®šä½ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé¢„è®­ç»ƒçš„åˆ†å‰²ç½‘ç»œæ¨¡ä»¿æ”¾å°„ç§‘åŒ»ç”Ÿå¯¹å•ä¸€æˆ–è¿™äº›æˆåƒæ¨¡æ€åŠå…¶éƒ¨åˆ†çš„ç»„åˆçš„æ£€æŸ¥ï¼Œè¿™äº›éƒ¨åˆ†ç”±ç­–ç•¥ç½‘ç»œé€‰æ‹©ã€‚ä»¥å±€éƒ¨åˆ†å‰²åŒºåŸŸä½œä¸ºä¸‹ä¸€æ­¥çš„è¾“å…¥ï¼Œè¿™ç§åŠ¨æ€å†³ç­–è¿‡ç¨‹ä¼šä¸æ–­è¿­ä»£ï¼Œç›´åˆ°æ‰€æœ‰ç™Œç—‡çš„æœ€ä½³å®šä½ã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…å«æ¥è‡ªå‰åˆ—è…ºç™Œæ‚£è€…çš„1325ä¸ªæ ‡è®°çš„å¤šå‚æ•°MRIå›¾åƒæ•°æ®é›†éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯æ˜äº†å…¶åœ¨æé«˜æ³¨é‡Šæ•ˆç‡å’Œåˆ†å‰²ç²¾åº¦æ–¹é¢çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨æŒ‘æˆ˜æ€§ç—…ç†çš„æƒ…å†µä¸‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è¶…è¶Šæ ‡å‡†åˆ†å‰²ç½‘ç»œã€‚æ›´æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬çš„è®­ç»ƒä»£ç†ç‹¬ç«‹åœ°å¼€å‘äº†è‡ªå·±çš„æœ€ä½³ç­–ç•¥ï¼Œè¿™å¯èƒ½ä¸€è‡´ä¹Ÿå¯èƒ½ä¸å½“å‰çš„æ”¾å°„ç§‘åŒ»ç”ŸæŒ‡å—ï¼ˆå¦‚PI-RADSï¼‰ä¸ä¸€è‡´ã€‚è¿™ä¸€è§‚å¯Ÿç»“æœä¹Ÿè¡¨æ˜äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„äº¤äº’åº”ç”¨ï¼Œå³æ‰€æå‡ºçš„ç­–ç•¥ç½‘ç»œå¯ä»¥è¾…åŠ©äººç±»æ”¾å°„ç§‘åŒ»ç”Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03953v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ¨èç³»ç»Ÿï¼Œè¾…åŠ©æœºå™¨å­¦ä¹ åˆ†å‰²æ¨¡å‹è¿›è¡Œå‰åˆ—è…ºç™Œåˆ†å‰²ï¼Œé€šè¿‡æ¨èé€‚å½“çš„å›¾åƒéƒ¨åˆ†åŠæœ€ä½³æ¨¡æ€æ¥æœ€å¤§åŒ–åˆ†å‰²æ€§èƒ½ã€‚è¯¥ç³»ç»Ÿè®­ç»ƒä¸€ä¸ªç­–ç•¥ç½‘ç»œï¼Œè¾…åŠ©è‚¿ç˜¤å®šä½ï¼Œå¹¶æ¨èæœ€ä½³çš„æˆåƒæ¨¡æ€å’Œç‰¹å®šæ„Ÿå…´è¶£åŒºåŸŸè¿›è¡Œå®¡æŸ¥ã€‚é€šè¿‡æ¨¡æ‹Ÿæ”¾å°„ç§‘åŒ»ç”Ÿå¯¹å„ç§æˆåƒæ¨¡æ€åŠå…¶åŒºåŸŸçš„æ£€æŸ¥æ¥é€‰æ‹©ç­–ç•¥ç½‘ç»œæ¨èçš„å±€éƒ¨åˆ†å‰²åŒºåŸŸè¿›è¡Œè®­ç»ƒã€‚æ­¤æ–¹æ³•å¯æé«˜æ ‡æ³¨æ•ˆç‡å’Œåˆ†å‰²ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨æŒ‘æˆ˜æ€§ç—…ç†çš„æƒ…å†µä¸‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯è¶…è¶Šæ ‡å‡†åˆ†å‰²ç½‘ç»œæ€§èƒ½ï¼Œå¹¶ä¸”ç‹¬ç«‹å‘å±•å‡ºå¯èƒ½ä¸å½“å‰æ”¾å°„ç§‘åŒ»ç”ŸæŒ‡å—ä¸€è‡´æˆ–ä¸ä¸€è‡´çš„æœ€ä¼˜ç­–ç•¥ï¼Œå±•ç°å‡ºè‰¯å¥½çš„äº¤äº’å¼åº”ç”¨å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°çš„æ¨èç³»ç»Ÿï¼Œæ—¨åœ¨è¾…åŠ©æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨åŒ»ç–—å›¾åƒåˆ†æä¸­çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å‰åˆ—è…ºç™Œçš„åˆ†å‰²ä¸Šã€‚</li>
<li>è¯¥ç³»ç»Ÿé€šè¿‡è®­ç»ƒä¸€ä¸ªç­–ç•¥ç½‘ç»œæ¥æ¨èæœ€ä½³çš„æˆåƒæ¨¡æ€å’Œå›¾åƒåŒºåŸŸï¼Œå¸®åŠ©å®šä½è‚¿ç˜¤ã€‚</li>
<li>ç­–ç•¥ç½‘ç»œæ˜¯åœ¨æ¨¡æ‹Ÿæ”¾å°„ç§‘åŒ»ç”Ÿå¯¹æˆåƒæ¨¡æ€åŠå…¶åŒºåŸŸçš„æ£€æŸ¥è¿‡ç¨‹ä¸­è¿›è¡Œè®­ç»ƒçš„ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½æé«˜æ ‡æ³¨æ•ˆç‡å’Œåˆ†å‰²ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢å¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ç—…ç†æƒ…å†µæ—¶ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•çš„æ€§èƒ½è¶…è¶Šäº†æ ‡å‡†çš„åˆ†å‰²ç½‘ç»œã€‚</li>
<li>è®­ç»ƒå‡ºçš„ç­–ç•¥ç½‘ç»œèƒ½å¤Ÿç‹¬ç«‹å‘å±•å‡ºæœ€ä¼˜ç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥å¯èƒ½ä¸ç°æœ‰çš„æ”¾å°„ç§‘åŒ»ç”ŸæŒ‡å—ä¸€è‡´æˆ–ä¸ä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03953">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ac23e18f201a738fcb202f84f1b544c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-382fd1860f7ef57805a0b9ab1456de65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a53a5550d8302cfaea703c03f6a72bd.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SAM2-UNeXT-An-Improved-High-Resolution-Baseline-for-Adapting-Foundation-Models-to-Downstream-Segmentation-Tasks"><a href="#SAM2-UNeXT-An-Improved-High-Resolution-Baseline-for-Adapting-Foundation-Models-to-Downstream-Segmentation-Tasks" class="headerlink" title="SAM2-UNeXT: An Improved High-Resolution Baseline for Adapting Foundation   Models to Downstream Segmentation Tasks"></a>SAM2-UNeXT: An Improved High-Resolution Baseline for Adapting Foundation   Models to Downstream Segmentation Tasks</h2><p><strong>Authors:Xinyu Xiong, Zihuang Wu, Lei Zhang, Lei Lu, Ming Li, Guanbin Li</strong></p>
<p>Recent studies have highlighted the potential of adapting the Segment Anything Model (SAM) for various downstream tasks. However, constructing a more powerful and generalizable encoder to further enhance performance remains an open challenge. In this work, we propose SAM2-UNeXT, an advanced framework that builds upon the core principles of SAM2-UNet while extending the representational capacity of SAM2 through the integration of an auxiliary DINOv2 encoder. By incorporating a dual-resolution strategy and a dense glue layer, our approach enables more accurate segmentation with a simple architecture, relaxing the need for complex decoder designs. Extensive experiments conducted on four benchmarks, including dichotomous image segmentation, camouflaged object detection, marine animal segmentation, and remote sensing saliency detection, demonstrate the superior performance of our proposed method. The code is available at <a target="_blank" rel="noopener" href="https://github.com/WZH0120/SAM2-UNeXT">https://github.com/WZH0120/SAM2-UNeXT</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶å·²ç»å¼ºè°ƒäº†é€‚åº”Segment Anything Modelï¼ˆSAMï¼‰å¯¹å„ç§ä¸‹æ¸¸ä»»åŠ¡çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œæ„å»ºä¸€ä¸ªæ›´å¼ºå¤§ã€æ›´å…·é€šç”¨æ€§çš„ç¼–ç å™¨ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SAM2-UNeXTï¼Œè¿™æ˜¯ä¸€ä¸ªå…ˆè¿›çš„æ¡†æ¶ï¼Œå®ƒåŸºäºSAM2-UNetçš„æ ¸å¿ƒåŸåˆ™ï¼Œå¹¶é€šè¿‡é›†æˆè¾…åŠ©DINOv2ç¼–ç å™¨æ‰©å±•äº†SAM2çš„ä»£è¡¨æ€§å®¹é‡ã€‚é€šè¿‡é‡‡ç”¨åŒåˆ†è¾¨ç‡ç­–ç•¥å’Œå¯†é›†ç²˜åˆå±‚ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ç®€å•çš„æ¶æ„ä¸Šå®ç°æ›´ç²¾ç¡®çš„åˆ†å‰²ï¼Œå‡è½»äº†å¤æ‚è§£ç å™¨è®¾è®¡çš„å¿…è¦æ€§ã€‚åœ¨å››é¡¹åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒåŒ…æ‹¬äºŒå€¼å›¾åƒåˆ†å‰²ã€ä¼ªè£…ç›®æ ‡æ£€æµ‹ã€æµ·æ´‹åŠ¨ç‰©åˆ†å‰²å’Œé¥æ„Ÿæ˜¾è‘—æ€§æ£€æµ‹ï¼Œè¯æ˜äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/WZH0">https://github.com/WZH0</a> 0 &#x2F;SAM2-UNeXTæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03566v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>SAM2-UNeXTæ¡†æ¶çš„æå‡ºï¼ŒåŸºäºSAMçš„æ ¸å¿ƒåŸç†ï¼Œç»“åˆäº†SAM2-UNetçš„ä¼˜åŠ¿ï¼Œå¹¶é€šè¿‡å¼•å…¥è¾…åŠ©çš„DINOv2ç¼–ç å™¨æ‰©å±•äº†ä»£è¡¨å®¹é‡ã€‚åˆ©ç”¨åŒåˆ†è¾¨ç‡ç­–ç•¥å’Œç¨ å¯†ç²˜ç€å±‚ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå®ç°æ›´ç²¾ç¡®çš„åˆ†å‰²ï¼Œç®€åŒ–äº†æ¶æ„ï¼Œå‡å°‘äº†å¤æ‚è§£ç å™¨è®¾è®¡çš„éœ€è¦ã€‚åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SAM2-UNeXTæ˜¯åŸºäºSegment Anything Modelï¼ˆSAMï¼‰æ„å»ºçš„å…ˆè¿›æ¡†æ¶ã€‚</li>
<li>é€šè¿‡å¼•å…¥DINOv2ç¼–ç å™¨æ‰©å±•äº†ä»£è¡¨å®¹é‡ã€‚</li>
<li>é‡‡ç”¨åŒåˆ†è¾¨ç‡ç­–ç•¥ä»¥æé«˜åˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
<li>åˆ©ç”¨ç¨ å¯†ç²˜ç€å±‚ç®€åŒ–æ¶æ„ï¼Œé™ä½å¤æ‚è§£ç å™¨è®¾è®¡çš„éœ€æ±‚ã€‚</li>
<li>åœ¨å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬äºŒå€¼å›¾åƒåˆ†å‰²ã€éšè”½ç›®æ ‡æ£€æµ‹ã€æµ·æ´‹åŠ¨ç‰©åˆ†å‰²å’Œé¥æ„Ÿæ˜¾è‘—æ€§æ£€æµ‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fed2642c93a9db8083996637fff45974.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37d73a9add551c507aefbcaf432954ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f4c8b79a852b312e9844a06e914dfbe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90830baf66a1c444f1a5d64df9ff6cff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6410ff683ef8db54f598d500e218f40b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4363cad44151a296f23904cf0a6530a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03339466c803ccfa47ca69e7b5b7ea41.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3517b5a5e49487dc0c80ca14d2b73e41.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MAUP-Training-free-Multi-center-Adaptive-Uncertainty-aware-Prompting-for-Cross-domain-Few-shot-Medical-Image-Segmentation"><a href="#MAUP-Training-free-Multi-center-Adaptive-Uncertainty-aware-Prompting-for-Cross-domain-Few-shot-Medical-Image-Segmentation" class="headerlink" title="MAUP: Training-free Multi-center Adaptive Uncertainty-aware Prompting   for Cross-domain Few-shot Medical Image Segmentation"></a>MAUP: Training-free Multi-center Adaptive Uncertainty-aware Prompting   for Cross-domain Few-shot Medical Image Segmentation</h2><p><strong>Authors:Yazhou Zhu, Haofeng Zhang</strong></p>
<p>Cross-domain Few-shot Medical Image Segmentation (CD-FSMIS) is a potential solution for segmenting medical images with limited annotation using knowledge from other domains. The significant performance of current CD-FSMIS models relies on the heavily training procedure over other source medical domains, which degrades the universality and ease of model deployment. With the development of large visual models of natural images, we propose a training-free CD-FSMIS model that introduces the Multi-center Adaptive Uncertainty-aware Prompting (MAUP) strategy for adapting the foundation model Segment Anything Model (SAM), which is trained with natural images, into the CD-FSMIS task. To be specific, MAUP consists of three key innovations: (1) K-means clustering based multi-center prompts generation for comprehensive spatial coverage, (2) uncertainty-aware prompts selection that focuses on the challenging regions, and (3) adaptive prompt optimization that can dynamically adjust according to the target region complexity. With the pre-trained DINOv2 feature encoder, MAUP achieves precise segmentation results across three medical datasets without any additional training compared with several conventional CD-FSMIS models and training-free FSMIS model. The source code is available at: <a target="_blank" rel="noopener" href="https://github.com/YazhouZhu19/MAUP">https://github.com/YazhouZhu19/MAUP</a>. </p>
<blockquote>
<p>è·¨åŸŸå°æ ·æœ¬åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆCD-FSMISï¼‰æ˜¯åˆ©ç”¨å…¶ä»–é¢†åŸŸçš„çŸ¥è¯†å¯¹åŒ»å­¦å›¾åƒè¿›è¡Œæœ‰é™æ ‡æ³¨åˆ†å‰²çš„ä¸€ç§æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚å½“å‰CD-FSMISæ¨¡å‹çš„å‡ºè‰²æ€§èƒ½ä¾èµ–äºå…¶ä»–æºåŒ»å­¦é¢†åŸŸçš„ç¹é‡è®­ç»ƒè¿‡ç¨‹ï¼Œè¿™é™ä½äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œéƒ¨ç½²çš„ä¾¿æ·æ€§ã€‚éšç€è‡ªç„¶å›¾åƒå¤§å‹è§†è§‰æ¨¡å‹çš„å‘å±•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„CD-FSMISæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¼•å…¥äº†å¤šä¸­å¿ƒè‡ªé€‚åº”ä¸ç¡®å®šæ€§æ„ŸçŸ¥æç¤ºï¼ˆMAUPï¼‰ç­–ç•¥ï¼Œä»¥é€‚åº”åŸºç¡€æ¨¡å‹â€”â€”ç”¨è‡ªç„¶å›¾åƒè®­ç»ƒçš„åˆ†å‰²ä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰ï¼Œç”¨äºCD-FSMISä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼ŒMAUPåŒ…æ‹¬ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰åŸºäºK-meansèšç±»çš„å¤šä¸­å¿ƒæç¤ºç”Ÿæˆï¼Œå®ç°å…¨é¢çš„ç©ºé—´è¦†ç›–ï¼›ï¼ˆ2ï¼‰å…³æ³¨å›°éš¾åŒºåŸŸçš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥æç¤ºé€‰æ‹©ï¼›ï¼ˆ3ï¼‰å¯æ ¹æ®ç›®æ ‡åŒºåŸŸå¤æ‚æ€§è¿›è¡ŒåŠ¨æ€è°ƒæ•´çš„è‡ªé€‚åº”æç¤ºä¼˜åŒ–ã€‚ä¸å‡ ä¸ªä¼ ç»Ÿçš„CD-FSMISæ¨¡å‹å’Œæ— éœ€è®­ç»ƒçš„FSMISæ¨¡å‹ç›¸æ¯”ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„DINOv2ç‰¹å¾ç¼–ç å™¨ï¼ŒMAUPåœ¨ä¸‰ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šå®ç°äº†ç²¾ç¡®çš„åˆ†å‰²ç»“æœã€‚æºä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/YazhouZhu19/MAUP%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/YazhouZhu19/MAUPè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03511v1">PDF</a> Accepted by MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è·¨åŸŸå°æ ·åŒ»ç–—å›¾åƒåˆ†å‰²ï¼ˆCD-FSMISï¼‰çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒçš„è‡ªç„¶å›¾åƒæ¨¡å‹SAMçš„æ— éœ€è®­ç»ƒçš„æ–°æ–¹æ³•MAUPã€‚è¯¥æ–¹æ³•é€šè¿‡K-meansèšç±»ç”Ÿæˆå¤šä¸­å¿ƒæç¤ºã€ä¸ç¡®å®šæ€§æ„ŸçŸ¥æç¤ºé€‰æ‹©å’Œè‡ªé€‚åº”æç¤ºä¼˜åŒ–ç­‰æŠ€æœ¯ï¼Œå®ç°äº†å¯¹åŒ»ç–—å›¾åƒçš„ç²¾å‡†åˆ†å‰²ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯åœ¨ä¸åŒåŒ»ç–—æ•°æ®é›†ä¸Šå–å¾—è‰¯å¥½æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CD-FSMISæ¨¡å‹åˆ©ç”¨å…¶ä»–é¢†åŸŸçš„çŸ¥è¯†å¯¹åŒ»ç–—å›¾åƒè¿›è¡Œåˆ†å‰²ï¼Œä½†ç°æœ‰æ¨¡å‹éœ€è¦å¤§é‡æºåŒ»ç–—é¢†åŸŸçš„è®­ç»ƒæ•°æ®ï¼Œå½±å“å…¶é€šç”¨æ€§å’Œéƒ¨ç½²çš„ä¾¿æ·æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒçš„è‡ªç„¶å›¾åƒæ¨¡å‹SAMçš„æ— éœ€è®­ç»ƒçš„CD-FSMISæ¨¡å‹MAUPï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>MAUPç­–ç•¥åŒ…æ‹¬ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šåŸºäºK-meansèšç±»çš„å¤šä¸­å¿ƒæç¤ºç”Ÿæˆã€ä¸ç¡®å®šæ€§æ„ŸçŸ¥æç¤ºé€‰æ‹©å’Œè‡ªé€‚åº”æç¤ºä¼˜åŒ–ã€‚</li>
<li>å¤šä¸­å¿ƒæç¤ºç”Ÿæˆå®ç°äº†å…¨é¢çš„ç©ºé—´è¦†ç›–ï¼›ä¸ç¡®å®šæ€§æ„ŸçŸ¥æç¤ºé€‰æ‹©å…³æ³¨äºæŒ‘æˆ˜åŒºåŸŸï¼›è‡ªé€‚åº”æç¤ºä¼˜åŒ–èƒ½æ ¹æ®ç›®æ ‡åŒºåŸŸçš„å¤æ‚æ€§è¿›è¡ŒåŠ¨æ€è°ƒæ•´ã€‚</li>
<li>MAUPåˆ©ç”¨é¢„è®­ç»ƒçš„DINOv2ç‰¹å¾ç¼–ç å™¨ï¼Œå®ç°äº†åœ¨ä¸‰ä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šçš„ç²¾å‡†åˆ†å‰²ç»“æœã€‚</li>
<li>ä¸ä¼ ç»Ÿçš„CD-FSMISæ¨¡å‹å’Œæ— éœ€è®­ç»ƒçš„FSMISæ¨¡å‹ç›¸æ¯”ï¼ŒMAUPå–å¾—äº†æ›´å¥½çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03511">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4ce8753932600e79a64a29fef59d5b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88d91333fa6c10dbe48a67f52ca48a5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9d4ca7caf0a1ae93150454786c425bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0dd224fe8792e5aafd4a68b1dc59b640.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Prototype-Enhanced-Confidence-Modeling-for-Cross-Modal-Medical-Image-Report-Retrieval"><a href="#Prototype-Enhanced-Confidence-Modeling-for-Cross-Modal-Medical-Image-Report-Retrieval" class="headerlink" title="Prototype-Enhanced Confidence Modeling for Cross-Modal Medical   Image-Report Retrieval"></a>Prototype-Enhanced Confidence Modeling for Cross-Modal Medical   Image-Report Retrieval</h2><p><strong>Authors:Shreyank N Gowda, Xiaobo Jin, Christian Wagner</strong></p>
<p>In cross-modal retrieval tasks, such as image-to-report and report-to-image retrieval, accurately aligning medical images with relevant text reports is essential but challenging due to the inherent ambiguity and variability in medical data. Existing models often struggle to capture the nuanced, multi-level semantic relationships in radiology data, leading to unreliable retrieval results. To address these issues, we propose the Prototype-Enhanced Confidence Modeling (PECM) framework, which introduces multi-level prototypes for each modality to better capture semantic variability and enhance retrieval robustness. PECM employs a dual-stream confidence estimation that leverages prototype similarity distributions and an adaptive weighting mechanism to control the impact of high-uncertainty data on retrieval rankings. Applied to radiology image-report datasets, our method achieves significant improvements in retrieval precision and consistency, effectively handling data ambiguity and advancing reliability in complex clinical scenarios. We report results on multiple different datasets and tasks including fully supervised and zero-shot retrieval obtaining performance gains of up to 10.17%, establishing in new state-of-the-art. </p>
<blockquote>
<p>åœ¨å¤šæ¨¡æ€æ£€ç´¢ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ°æŠ¥å‘Šå’ŒæŠ¥å‘Šåˆ°å›¾åƒçš„æ£€ç´¢ï¼‰ä¸­ï¼Œå‡†ç¡®å¯¹é½åŒ»å­¦å›¾åƒä¸ç›¸å…³çš„æ–‡æœ¬æŠ¥å‘Šæ˜¯è‡³å…³é‡è¦çš„ï¼Œä½†ç”±äºåŒ»å­¦æ•°æ®å›ºæœ‰çš„æ¨¡ç³Šæ€§å’Œå˜åŒ–æ€§ï¼Œè¿™å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ¨¡å‹å¾€å¾€éš¾ä»¥æ•æ‰æ”¾å°„æ•°æ®ä¸­ç»†å¾®çš„å¤šçº§è¯­ä¹‰å…³ç³»ï¼Œå¯¼è‡´æ£€ç´¢ç»“æœä¸å¯é ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸå‹å¢å¼ºä¿¡å¿ƒå»ºæ¨¡ï¼ˆPECMï¼‰æ¡†æ¶ï¼Œå®ƒå¼•å…¥æ¯ä¸ªæ¨¡æ€çš„å¤šçº§åŸå‹ï¼Œä»¥æ›´å¥½åœ°æ•æ‰è¯­ä¹‰å˜åŒ–ï¼Œæé«˜æ£€ç´¢çš„ç¨³å¥æ€§ã€‚PECMé‡‡ç”¨åŒæµä¿¡å¿ƒä¼°è®¡ï¼Œåˆ©ç”¨åŸå‹ç›¸ä¼¼æ€§åˆ†å¸ƒå’Œè‡ªé€‚åº”åŠ æƒæœºåˆ¶æ¥æ§åˆ¶é«˜ä¸ç¡®å®šæ€§æ•°æ®å¯¹æ£€ç´¢æ’åçš„å½±å“ã€‚æˆ‘ä»¬çš„æ–¹æ³•åº”ç”¨äºæ”¾å°„å­¦å›¾åƒæŠ¥å‘Šæ•°æ®é›†ï¼Œåœ¨æ£€ç´¢ç²¾åº¦å’Œä¸€è‡´æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œæœ‰æ•ˆåœ°å¤„ç†äº†æ•°æ®æ¨¡ç³Šæ€§ï¼Œæé«˜äº†å¤æ‚ä¸´åºŠåœºæ™¯ä¸­çš„å¯é æ€§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä¸åŒçš„æ•°æ®é›†å’Œä»»åŠ¡ä¸ŠæŠ¥å‘Šäº†ç»“æœï¼ŒåŒ…æ‹¬å…¨ç›‘ç£å’Œé›¶å°„å‡»æ£€ç´¢ï¼Œæ€§èƒ½æå‡é«˜è¾¾10.17%ï¼Œåˆ›ä¸‹äº†æ–°çš„æœ€å…ˆè¿›çš„è®°å½•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03494v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦è·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸­ï¼Œå‡†ç¡®å¯¹é½åŒ»å­¦å›¾åƒå’Œç›¸å…³æ–‡æœ¬æŠ¥å‘Šè‡³å…³é‡è¦ä½†å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºåº”å¯¹æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§åä¸ºPECMï¼ˆåŸå‹å¢å¼ºä¿¡å¿ƒå»ºæ¨¡ï¼‰çš„æ¡†æ¶ï¼Œå¼•å…¥å¤šçº§åˆ«åŸå‹ä»¥æ•è·è¯­ä¹‰å˜åŒ–å¹¶å¢å¼ºæ£€ç´¢ç¨³å¥æ€§ã€‚é€šè¿‡åŸå‹ç›¸ä¼¼æ€§åˆ†å¸ƒå’Œè‡ªé€‚åº”åŠ æƒæœºåˆ¶è¿›è¡ŒåŒæµä¿¡å¿ƒä¼°è®¡ï¼Œæé«˜æ£€ç´¢ç²¾åº¦å’Œä¸€è‡´æ€§ï¼Œæœ‰æ•ˆå¤„ç†æ•°æ®æ¨¡ç³Šæ€§ï¼Œå¹¶åœ¨å¤æ‚ä¸´åºŠåœºæ™¯ä¸­æé«˜å¯é æ€§ã€‚åœ¨æ”¾å°„å­¦å›¾åƒæŠ¥å‘Šæ•°æ®é›†ä¸Šåº”ç”¨ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦è·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡é¢ä¸´å‡†ç¡®å¯¹é½åŒ»å­¦å›¾åƒå’Œæ–‡æœ¬æŠ¥å‘Šçš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ¨¡å‹éš¾ä»¥æ•è·æ”¾å°„å­¦æ•°æ®ä¸­çš„å¤šçº§åˆ«è¯­ä¹‰å…³ç³»ã€‚</li>
<li>PECMæ¡†æ¶å¼•å…¥å¤šçº§åˆ«åŸå‹ä»¥æ›´å¥½åœ°æ•è·è¯­ä¹‰å˜åŒ–å’Œå¢å¼ºæ£€ç´¢ç¨³å¥æ€§ã€‚</li>
<li>PECMé‡‡ç”¨åŒæµä¿¡å¿ƒä¼°è®¡ï¼Œåˆ©ç”¨åŸå‹ç›¸ä¼¼æ€§åˆ†å¸ƒå’Œè‡ªé€‚åº”åŠ æƒæœºåˆ¶ã€‚</li>
<li>PECMæé«˜äº†æ£€ç´¢ç²¾åº¦å’Œä¸€è‡´æ€§ï¼Œæœ‰æ•ˆå¤„ç†æ•°æ®æ¨¡ç³Šæ€§ã€‚</li>
<li>åœ¨å¤æ‚ä¸´åºŠåœºæ™¯ä¸­ï¼ŒPECMæé«˜äº†å¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03494">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-03a5ba29aa4a2269aaccf2081cac129b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e714fd3b48c3dad47d13851151a48ca6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-937708ea01a9695b629387bde201d24f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b98f80462942dae1eb786aa61c1c4305.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MedCAL-Bench-A-Comprehensive-Benchmark-on-Cold-Start-Active-Learning-with-Foundation-Models-for-Medical-Image-Analysis"><a href="#MedCAL-Bench-A-Comprehensive-Benchmark-on-Cold-Start-Active-Learning-with-Foundation-Models-for-Medical-Image-Analysis" class="headerlink" title="MedCAL-Bench: A Comprehensive Benchmark on Cold-Start Active Learning   with Foundation Models for Medical Image Analysis"></a>MedCAL-Bench: A Comprehensive Benchmark on Cold-Start Active Learning   with Foundation Models for Medical Image Analysis</h2><p><strong>Authors:Ning Zhu, Xiaochuan Ma, Shaoting Zhang, Guotai Wang</strong></p>
<p>Cold-Start Active Learning (CSAL) aims to select informative samples for annotation without prior knowledge, which is important for improving annotation efficiency and model performance under a limited annotation budget in medical image analysis. Most existing CSAL methods rely on Self-Supervised Learning (SSL) on the target dataset for feature extraction, which is inefficient and limited by insufficient feature representation. Recently, pre-trained Foundation Models (FMs) have shown powerful feature extraction ability with a potential for better CSAL. However, this paradigm has been rarely investigated, with a lack of benchmarks for comparison of FMs in CSAL tasks. To this end, we propose MedCAL-Bench, the first systematic FM-based CSAL benchmark for medical image analysis. We evaluate 14 FMs and 7 CSAL strategies across 7 datasets under different annotation budgets, covering classification and segmentation tasks from diverse medical modalities. It is also the first CSAL benchmark that evaluates both the feature extraction and sample selection stages. Our experimental results reveal that: 1) Most FMs are effective feature extractors for CSAL, with DINO family performing the best in segmentation; 2) The performance differences of these FMs are large in segmentation tasks, while small for classification; 3) Different sample selection strategies should be considered in CSAL on different datasets, with Active Learning by Processing Surprisal (ALPS) performing the best in segmentation while RepDiv leading for classification. The code is available at <a target="_blank" rel="noopener" href="https://github.com/HiLab-git/MedCAL-Bench">https://github.com/HiLab-git/MedCAL-Bench</a>. </p>
<blockquote>
<p>å†·å¯åŠ¨ä¸»åŠ¨å­¦ä¹ ï¼ˆCSALï¼‰æ—¨åœ¨åœ¨æ²¡æœ‰å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹é€‰æ‹©ä¿¡æ¯æ ·æœ¬è¿›è¡Œæ ‡æ³¨ï¼Œè¿™å¯¹äºåœ¨åŒ»ç–—å›¾åƒåˆ†æä¸­ä½¿ç”¨æœ‰é™çš„æ ‡æ³¨é¢„ç®—æé«˜æ ‡æ³¨æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚å¤§å¤šæ•°ç°æœ‰çš„CSALæ–¹æ³•ä¾èµ–äºç›®æ ‡æ•°æ®é›†ä¸Šçš„è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰è¿›è¡Œç‰¹å¾æå–ï¼Œè¿™æ˜¯ä½æ•ˆçš„ï¼Œå¹¶ä¸”å—åˆ°ç‰¹å¾è¡¨ç¤ºä¸è¶³çš„é™åˆ¶ã€‚æœ€è¿‘ï¼Œé¢„è®­ç»ƒçš„Foundation Modelsï¼ˆFMsï¼‰æ˜¾ç¤ºå‡ºå¼ºå¤§çš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œå…·æœ‰æ›´å¥½çš„CSALæ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿™ä¸€èŒƒå¼å¾ˆå°‘å—åˆ°ç ”ç©¶ï¼Œç¼ºä¹ç”¨äºæ¯”è¾ƒFMsåœ¨CSALä»»åŠ¡ä¸­çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†MedCAL-Benchï¼Œè¿™æ˜¯åŸºäºFMsçš„åŒ»ç–—å›¾åƒåˆ†æçš„é¦–ä¸ªç³»ç»Ÿæ€§CSALåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬åœ¨7ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°äº†14ä¸ªFMså’Œ7ç§CSALç­–ç•¥ï¼Œæ¶‰åŠä¸åŒæ ‡æ³¨é¢„ç®—ä¸‹çš„åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ï¼Œæ¶µç›–å¤šç§åŒ»å­¦æ¨¡æ€ã€‚å®ƒä¹Ÿæ˜¯ç¬¬ä¸€ä¸ªåŒæ—¶è¯„ä¼°ç‰¹å¾æå–å’Œæ ·æœ¬é€‰æ‹©é˜¶æ®µçš„CSALåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæ­ç¤ºï¼š1ï¼‰å¤§å¤šæ•°FMså¯¹äºCSALéƒ½æ˜¯æœ‰æ•ˆçš„ç‰¹å¾æå–å™¨ï¼ŒDINOç³»åˆ—åœ¨åˆ†å‰²æ–¹é¢è¡¨ç°æœ€ä½³ï¼›2ï¼‰è¿™äº›FMsåœ¨åˆ†å‰²ä»»åŠ¡ä¸­çš„æ€§èƒ½å·®å¼‚å¾ˆå¤§ï¼Œè€Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸­åˆ™è¾ƒå°ï¼›3ï¼‰CSALä¸­çš„ä¸åŒæ ·æœ¬é€‰æ‹©ç­–ç•¥åº”é’ˆå¯¹ä¸åŒçš„æ•°æ®é›†è¿›è¡Œè€ƒè™‘ï¼Œå¤„ç†æƒŠå–œï¼ˆALPSï¼‰åœ¨åˆ†å‰²æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè€ŒRepDivåœ¨åˆ†ç±»æ–¹é¢é¢†å…ˆã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HiLab-git/MedCAL-Bench">https://github.com/HiLab-git/MedCAL-Bench</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03441v1">PDF</a> 23 pages, 6 figures, 10 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†æçš„å†·å¯åŠ¨ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ï¼ˆCSALï¼‰çš„æŒ‘æˆ˜å’Œç°çŠ¶ã€‚ä¸ºæé«˜æ ‡æ³¨æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ï¼Œç ”ç©¶è€…æå‡ºäº†åŸºäºé¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰çš„CSALæ–°æ–¹æ³•ï¼Œå¹¶å»ºç«‹äº†é¦–ä¸ªç³»ç»Ÿæ€§çš„FM-based CSALåŸºå‡†æµ‹è¯•å¹³å°MedCAL-Benchã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤§å¤šæ•°åŸºç¡€æ¨¡å‹åœ¨CSALä¸­èƒ½æœ‰æ•ˆæå–ç‰¹å¾ï¼Œä¸åŒæ•°æ®é›†ä¸Šçš„æ ·æœ¬é€‰æ‹©ç­–ç•¥ä¹Ÿæœ‰å·®å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å†·å¯åŠ¨ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ï¼ˆCSALï¼‰æ—¨åœ¨åœ¨æ²¡æœ‰å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹é€‰æ‹©ä¿¡æ¯æ ·æœ¬è¿›è¡Œæ ‡æ³¨ï¼Œä»¥æé«˜åŒ»å­¦å›¾åƒåˆ†æçš„æ ‡æ³¨æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ç°æœ‰CSALæ–¹æ³•å¤§å¤šä¾èµ–ç›®æ ‡æ•°æ®é›†çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰è¿›è¡Œç‰¹å¾æå–ï¼Œä½†è¿™ç§æ–¹æ³•æ•ˆç‡ä½ä¸‹ï¼Œä¸”ç‰¹å¾è¡¨ç¤ºæœ‰é™ã€‚</li>
<li>é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰å…·æœ‰å¼ºå¤§çš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œåœ¨CSALä¸­æœ‰æ½œåœ¨ä¼˜åŠ¿ã€‚</li>
<li>MedCAL-Benchæ˜¯é¦–ä¸ªåŸºäºFMçš„CSALåŸºå‡†æµ‹è¯•å¹³å°ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†æã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤§å¤šæ•°åŸºç¡€æ¨¡å‹åœ¨CSALä¸­èƒ½æœ‰æ•ˆæå–ç‰¹å¾ï¼ŒDINOå®¶æ—åœ¨åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>åˆ†å‰²ä»»åŠ¡ä¸­åŸºç¡€æ¨¡å‹çš„æ€§èƒ½å·®å¼‚è¾ƒå¤§ï¼Œåˆ†ç±»ä»»åŠ¡åˆ™è¾ƒå°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03441">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-17de953125d36d379a8a181d2ee46c14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6b0ce134ea92ca2627a93aabfa7126c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2dcfdbb520e75b30590bdee24c770681.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6de8b4cb73d02ce4bb9fe59105ed730f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dda582b066f01f4196a7cc045d968d11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62bb34848ad0d3af3b9af848623a496f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77848ba007867b296f351bc6fc2ba2bb.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="R2GenKG-Hierarchical-Multi-modal-Knowledge-Graph-for-LLM-based-Radiology-Report-Generation"><a href="#R2GenKG-Hierarchical-Multi-modal-Knowledge-Graph-for-LLM-based-Radiology-Report-Generation" class="headerlink" title="R2GenKG: Hierarchical Multi-modal Knowledge Graph for LLM-based   Radiology Report Generation"></a>R2GenKG: Hierarchical Multi-modal Knowledge Graph for LLM-based   Radiology Report Generation</h2><p><strong>Authors:Futian Wang, Yuhan Qiao, Xiao Wang, Fuling Wang, Yuxiang Zhang, Dengdi Sun</strong></p>
<p>X-ray medical report generation is one of the important applications of artificial intelligence in healthcare. With the support of large foundation models, the quality of medical report generation has significantly improved. However, challenges such as hallucination and weak disease diagnostic capability still persist. In this paper, we first construct a large-scale multi-modal medical knowledge graph (termed M3KG) based on the ground truth medical report using the GPT-4o. It contains 2477 entities, 3 kinds of relations, 37424 triples, and 6943 disease-aware vision tokens for the CheXpert Plus dataset. Then, we sample it to obtain multi-granularity semantic graphs and use an R-GCN encoder for feature extraction. For the input X-ray image, we adopt the Swin-Transformer to extract the vision features and interact with the knowledge using cross-attention. The vision tokens are fed into a Q-former and retrieved the disease-aware vision tokens using another cross-attention. Finally, we adopt the large language model to map the semantic knowledge graph, input X-ray image, and disease-aware vision tokens into language descriptions. Extensive experiments on multiple datasets fully validated the effectiveness of our proposed knowledge graph and X-ray report generation framework. The source code of this paper will be released on <a target="_blank" rel="noopener" href="https://github.com/Event-AHU/Medical_Image_Analysis">https://github.com/Event-AHU/Medical_Image_Analysis</a>. </p>
<blockquote>
<p>Xå°„çº¿åŒ»å­¦æŠ¥å‘Šç”Ÿæˆæ˜¯äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸçš„é‡è¦åº”ç”¨ä¹‹ä¸€ã€‚åœ¨å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹çš„æ”¯æŒä¸‹ï¼ŒåŒ»å­¦æŠ¥å‘Šç”Ÿæˆçš„è´¨é‡å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œè¯¸å¦‚è™šæ„å’Œå¼±ç–¾ç—…è¯Šæ–­èƒ½åŠ›ç­‰æŒ‘æˆ˜ä»ç„¶å­˜åœ¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåŸºäºçœŸå®åŒ»å­¦æŠ¥å‘Šæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡å¼åŒ»å­¦çŸ¥è¯†å›¾è°±ï¼ˆç§°ä¸ºM3KGï¼‰ï¼Œä½¿ç”¨GPT-4oé’ˆå¯¹CheXpert Plusæ•°æ®é›†è¿›è¡Œæ“ä½œï¼Œå®ƒåŒ…å«2477ä¸ªå®ä½“ã€3ç§å…³ç³»ã€37424ä¸ªä¸‰å…ƒç»„å’Œ6943ä¸ªç–¾ç—…æ„ŸçŸ¥è§†è§‰æ ‡è®°ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹å®ƒè¿›è¡Œé‡‡æ ·ä»¥è·å¾—å¤šç²’åº¦è¯­ä¹‰å›¾ï¼Œå¹¶ä½¿ç”¨R-GCNç¼–ç å™¨è¿›è¡Œç‰¹å¾æå–ã€‚å¯¹äºè¾“å…¥çš„Xå°„çº¿å›¾åƒï¼Œæˆ‘ä»¬é‡‡ç”¨Swin-Transformeræå–è§†è§‰ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›ä¸çŸ¥è¯†è¿›è¡Œäº¤äº’ã€‚è§†è§‰æ ‡è®°è¢«è¾“å…¥åˆ°Q-formerä¸­ï¼Œå¹¶ä½¿ç”¨å¦ä¸€ä¸ªäº¤å‰æ³¨æ„åŠ›æ£€ç´¢ç–¾ç—…æ„ŸçŸ¥è§†è§‰æ ‡è®°ã€‚æœ€åï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å°†è¯­ä¹‰çŸ¥è¯†å›¾è°±ã€è¾“å…¥çš„Xå°„çº¿å›¾åƒå’Œç–¾ç—…æ„ŸçŸ¥è§†è§‰æ ‡è®°æ˜ å°„åˆ°è¯­è¨€æè¿°ä¸­ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒå……åˆ†éªŒè¯äº†æˆ‘ä»¬æå‡ºçš„çŸ¥è¯†å›¾è°±å’ŒXå°„çº¿æŠ¥å‘Šç”Ÿæˆæ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚æœ¬æ–‡çš„æºä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Event-AHU/Medical_Image_Analysis%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Event-AHU/Medical_Image_Analysisä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03426v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    åŸºäºå¤§å‹åŸºç¡€æ¨¡å‹çš„æ”¯æŒï¼Œåˆ©ç”¨å¤šæ¨¡æ€åŒ»å­¦çŸ¥è¯†å›¾è°±ï¼ˆM3KGï¼‰å’ŒSwin-Transformerç­‰æŠ€æœ¯ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›Xå…‰æŠ¥å‘Šç”Ÿæˆçš„æ–¹æ³•ã€‚é€šè¿‡æ„å»ºå¤§è§„æ¨¡åŒ»å­¦çŸ¥è¯†å›¾è°±ï¼Œç»“åˆXå…‰å›¾åƒç‰¹å¾æå–å’Œè·¨æ³¨æ„åŠ›äº¤äº’ï¼Œå®ç°äº†é«˜è´¨é‡çš„åŒ»å­¦æŠ¥å‘Šç”Ÿæˆã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Xå…‰åŒ»å­¦æŠ¥å‘Šç”Ÿæˆæ˜¯äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—ä¿å¥ä¸­çš„é‡è¦åº”ç”¨ä¹‹ä¸€ã€‚</li>
<li>åˆ©ç”¨å¤§å‹åŸºç¡€æ¨¡å‹çš„æ”¯æŒï¼ŒåŒ»å­¦æŠ¥å‘Šç”Ÿæˆçš„è´¨é‡å·²æ˜¾è‘—æé«˜ã€‚<br>3.ä»å­˜åœ¨è¯¸å¦‚å¹»è§‰å’Œå¼±ç–¾ç—…è¯Šæ–­èƒ½åŠ›ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æ„å»ºäº†åŸºäºçœŸå®åŒ»å­¦æŠ¥å‘Šçš„å¤šæ¨¡æ€åŒ»å­¦çŸ¥è¯†å›¾è°±ï¼ˆM3KGï¼‰ã€‚</li>
<li>é€šè¿‡é‡‡æ ·è·å¾—å¤šç²’åº¦è¯­ä¹‰å›¾ï¼Œå¹¶ä½¿ç”¨R-GCNç¼–ç å™¨è¿›è¡Œç‰¹å¾æå–ã€‚</li>
<li>é‡‡ç”¨Swin-Transformeræå–Xå…‰å›¾åƒç‰¹å¾ï¼Œå¹¶é€šè¿‡è·¨æ³¨æ„åŠ›ä¸çŸ¥è¯†äº¤äº’ã€‚</li>
<li>ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå°†è¯­ä¹‰çŸ¥è¯†å›¾è°±ã€Xå…‰å›¾åƒå’Œç–¾ç—…æ„ŸçŸ¥è§†è§‰æ ‡è®°è½¬åŒ–ä¸ºè¯­è¨€æè¿°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03426">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-20cabdfe0aedeed2d29b07de38b42f1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffefc3fbe5876c2b1f2dab80a9f39823.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ceeab43741aad5c9c61aa68c989af697.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30bb45e4dd942e1c66956553cc3d169f.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Nexus-INR-Diverse-Knowledge-guided-Arbitrary-Scale-Multimodal-Medical-Image-Super-Resolution"><a href="#Nexus-INR-Diverse-Knowledge-guided-Arbitrary-Scale-Multimodal-Medical-Image-Super-Resolution" class="headerlink" title="Nexus-INR: Diverse Knowledge-guided Arbitrary-Scale Multimodal Medical   Image Super-Resolution"></a>Nexus-INR: Diverse Knowledge-guided Arbitrary-Scale Multimodal Medical   Image Super-Resolution</h2><p><strong>Authors:Bo Zhang, JianFei Huo, Zheng Zhang, Wufan Wang, Hui Gao, Xiangyang Gong, Wendong Wang</strong></p>
<p>Arbitrary-resolution super-resolution (ARSR) provides crucial flexibility for medical image analysis by adapting to diverse spatial resolutions. However, traditional CNN-based methods are inherently ill-suited for ARSR, as they are typically designed for fixed upsampling factors. While INR-based methods overcome this limitation, they still struggle to effectively process and leverage multi-modal images with varying resolutions and details. In this paper, we propose Nexus-INR, a Diverse Knowledge-guided ARSR framework, which employs varied information and downstream tasks to achieve high-quality, adaptive-resolution medical image super-resolution. Specifically, Nexus-INR contains three key components. A dual-branch encoder with an auxiliary classification task to effectively disentangle shared anatomical structures and modality-specific features; a knowledge distillation module using cross-modal attention that guides low-resolution modality reconstruction with high-resolution reference, enhanced by self-supervised consistency loss; an integrated segmentation module that embeds anatomical semantics to improve both reconstruction quality and downstream segmentation performance. Experiments on the BraTS2020 dataset for both super-resolution and downstream segmentation demonstrate that Nexus-INR outperforms state-of-the-art methods across various metrics. </p>
<blockquote>
<p>ä»»æ„åˆ†è¾¨ç‡è¶…åˆ†è¾¨ç‡ï¼ˆARSRï¼‰é€šè¿‡é€‚åº”ä¸åŒç©ºé—´åˆ†è¾¨ç‡ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†ææä¾›äº†å…³é”®çš„çµæ´»æ€§ã€‚ç„¶è€Œï¼ŒåŸºäºä¼ ç»ŸCNNçš„æ–¹æ³•æœ¬è´¨ä¸Šä¸é€‚åˆARSRï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸è®¾è®¡ä¸ºå›ºå®šæ”¾å¤§å€æ•°ã€‚è™½ç„¶INRæ–¹æ³•å…‹æœäº†è¿™ä¸€å±€é™æ€§ï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥æœ‰æ•ˆå¤„ç†å’Œåˆ©ç”¨å…·æœ‰ä¸åŒåˆ†è¾¨ç‡å’Œç»†èŠ‚çš„å¤šæ¨¡æ€å›¾åƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Nexus-INRï¼Œä¸€ä¸ªä»¥å¤šæ ·çŸ¥è¯†å¼•å¯¼çš„ARSRæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ä¸åŒçš„ä¿¡æ¯å’Œä¸‹æ¸¸ä»»åŠ¡æ¥å®ç°é«˜è´¨é‡ã€è‡ªé€‚åº”åˆ†è¾¨ç‡çš„åŒ»å­¦å›¾åƒè¶…åˆ†è¾¨ç‡ã€‚å…·ä½“æ¥è¯´ï¼ŒNexus-INRåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ã€‚ä¸€ä¸ªå…·æœ‰è¾…åŠ©åˆ†ç±»ä»»åŠ¡çš„åŒåˆ†æ”¯ç¼–ç å™¨ï¼Œæœ‰æ•ˆåœ°åˆ†ç¦»å…±äº«è§£å‰–ç»“æ„å’Œæ¨¡æ€ç‰¹å®šç‰¹å¾ï¼›ä¸€ä¸ªä½¿ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›çš„çŸ¥è¯†è’¸é¦æ¨¡å—ï¼Œè¯¥æ¨¡å—ä»¥é«˜åˆ†è¾¨ç‡å‚è€ƒå¼•å¯¼ä½åˆ†è¾¨ç‡æ¨¡æ€çš„é‡æ„ï¼Œé€šè¿‡è‡ªç›‘ç£ä¸€è‡´æ€§æŸå¤±å¢å¼ºï¼›ä¸€ä¸ªé›†æˆåˆ†å‰²æ¨¡å—ï¼ŒåµŒå…¥è§£å‰–è¯­ä¹‰ä»¥æé«˜é‡å»ºè´¨é‡å’Œä¸‹æ¸¸åˆ†å‰²æ€§èƒ½ã€‚åœ¨BraTS2020æ•°æ®é›†ä¸Šçš„è¶…åˆ†è¾¨ç‡å’Œä¸‹æ¸¸åˆ†å‰²å®éªŒè¡¨æ˜ï¼ŒNexus-INRåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03073v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºNexus-INRçš„çµæ´»è¶…åˆ†è¾¨ç‡æ¡†æ¶ï¼Œç”¨äºå®ç°é«˜è´¨é‡ã€è‡ªé€‚åº”åˆ†è¾¨ç‡çš„åŒ»ç–—å›¾åƒè¶…åˆ†è¾¨ç‡ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¤šç§æŠ€æœ¯å’Œç­–ç•¥ï¼ŒåŒ…æ‹¬åŒåˆ†æ”¯ç¼–ç å™¨ã€çŸ¥è¯†è’¸é¦æ¨¡å—å’Œé›†æˆåˆ†å‰²æ¨¡å—ï¼Œä»¥å®ç°æœ‰æ•ˆå¤„ç†ä¸åŒæ¨¡æ€å’Œä¸åŒåˆ†è¾¨ç‡çš„åŒ»ç–—å›¾åƒçš„ç›®æ ‡ã€‚å…¶åœ¨BraTS2020æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†å…¶è¶…è¶Šç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ARSRä¸ºåŒ»å­¦å›¾åƒåˆ†ææä¾›äº†é‡è¦çš„çµæ´»æ€§ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„ç©ºé—´åˆ†è¾¨ç‡ã€‚</li>
<li>ä¼ ç»ŸCNNæ–¹æ³•å¯¹äºARSRä»»åŠ¡æ¥è¯´å­˜åœ¨å±€é™æ€§ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸é’ˆå¯¹å›ºå®šçš„ä¸Šé‡‡æ ·å› å­è®¾è®¡ã€‚</li>
<li>INRæ–¹æ³•å…‹æœäº†CNNçš„å±€é™æ€§ï¼Œä½†åœ¨å¤„ç†å¤šæ¨¡æ€å›¾åƒæ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>Nexus-INRæ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šåŒåˆ†æ”¯ç¼–ç å™¨ã€çŸ¥è¯†è’¸é¦æ¨¡å—å’Œé›†æˆåˆ†å‰²æ¨¡å—ã€‚</li>
<li>åŒåˆ†æ”¯ç¼–ç å™¨é€šè¿‡è¾…åŠ©åˆ†ç±»ä»»åŠ¡æœ‰æ•ˆåœ°åˆ†ç¦»å…±äº«è§£å‰–ç»“æ„å’Œæ¨¡æ€ç‰¹å®šç‰¹å¾ã€‚</li>
<li>çŸ¥è¯†è’¸é¦æ¨¡å—åˆ©ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›æ¥æŒ‡å¯¼ä½åˆ†è¾¨ç‡æ¨¡æ€çš„é‡æ„ï¼ŒåŒæ—¶ç»“åˆé«˜åˆ†è¾¨ç‡å‚è€ƒå’Œè‡ªæˆ‘ç›‘ç£çš„ä¸€è‡´æ€§æŸå¤±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03073">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b75c9316716618eb252eefd377ee11f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bcc68d914ca73c8270481a50dbfb4846.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25a2c8b50be14aa5a8045973ce32c6f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-96e2522f604378a80512c4c840dea0d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f4437b7e897ad61e0186d98f99bca0f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="SSFMamba-Symmetry-driven-Spatial-Frequency-Feature-Fusion-for-3D-Medical-Image-Segmentation"><a href="#SSFMamba-Symmetry-driven-Spatial-Frequency-Feature-Fusion-for-3D-Medical-Image-Segmentation" class="headerlink" title="SSFMamba: Symmetry-driven Spatial-Frequency Feature Fusion for 3D   Medical Image Segmentation"></a>SSFMamba: Symmetry-driven Spatial-Frequency Feature Fusion for 3D   Medical Image Segmentation</h2><p><strong>Authors:Bo Zhang, Yifan Zhang, Shuo Yan, Yu Bai, Zheng Zhang, Wu Liu, Xiuzhuang Zhou, Wendong Wang</strong></p>
<p>In light of the spatial domainâ€™s limited capacity for modeling global context in 3D medical image segmentation, emerging approaches have begun to incorporate frequency domain representations. However, straightforward feature extraction strategies often overlook the unique properties of frequency domain information, such as conjugate symmetry. They also fail to account for the fundamental differences in data distribution between the spatial and frequency domains, which can ultimately dilute or obscure the complementary strengths that frequency-based representations offer. In this paper, we propose SSFMamba, a Mamba based Symmetry-driven Spatial-Frequency feature fusion network for 3D medical image segmentation. SSFMamba employs a complementary dual-branch architecture that extracts features from both the spatial and frequency domains, and leverages a Mamba block to fuse these heterogeneous features to preserve global context while reinforcing local details. In the frequency domain branch, we harness Mambaâ€™s exceptional capability to extract global contextual information in conjunction with the synergistic effect of frequency domain features to further enhance global modeling. Moreover, we design a 3D multi-directional scanning mechanism to strengthen the fusion of local and global cues. Extensive experiments on the BraTS2020 and BraTS2023 datasets demonstrate that our approach consistently outperforms state-of-the-art methods across various evaluation metrics. </p>
<blockquote>
<p>é‰´äºä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ç©ºé—´åŸŸå¯¹å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡çš„æœ‰é™èƒ½åŠ›ï¼Œæ–°å…´æ–¹æ³•å¼€å§‹èå…¥é¢‘ç‡åŸŸè¡¨ç¤ºã€‚ç„¶è€Œï¼Œç›´æ¥çš„ç‰¹å¾æå–ç­–ç•¥å¾€å¾€å¿½ç•¥äº†é¢‘ç‡åŸŸä¿¡æ¯çš„ç‹¬ç‰¹å±æ€§ï¼Œå¦‚å…±è½­å¯¹ç§°æ€§ã€‚å®ƒä»¬ä¹Ÿæœªèƒ½è€ƒè™‘åˆ°ç©ºé—´åŸŸå’Œé¢‘ç‡åŸŸä¹‹é—´æ•°æ®åˆ†å¸ƒçš„æ ¹æœ¬å·®å¼‚ï¼Œè¿™æœ€ç»ˆå¯èƒ½ä¼šç¨€é‡Šæˆ–æ©ç›–åŸºäºé¢‘ç‡çš„è¡¨ç¤ºæ‰€æä¾›çš„äº’è¡¥ä¼˜åŠ¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SSFMambaï¼Œè¿™æ˜¯ä¸€ç§åŸºäºMambaçš„å¯¹ç§°é©±åŠ¨ç©ºé—´-é¢‘ç‡ç‰¹å¾èåˆç½‘ç»œï¼Œç”¨äºä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚SSFMambaé‡‡ç”¨äº’è¡¥çš„åŒåˆ†æ”¯æ¶æ„ï¼Œä»ç©ºé—´å’Œé¢‘ç‡åŸŸæå–ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨Mambaå—èåˆè¿™äº›å¼‚æ„ç‰¹å¾ï¼Œä»¥ä¿ç•™å…¨å±€ä¸Šä¸‹æ–‡å¹¶å¼ºåŒ–å±€éƒ¨ç»†èŠ‚ã€‚åœ¨é¢‘ç‡åŸŸåˆ†æ”¯ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨Mambaæå–å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å“è¶Šèƒ½åŠ›ï¼Œç»“åˆé¢‘ç‡åŸŸç‰¹å¾çš„ååŒä½œç”¨ï¼Œè¿›ä¸€æ­¥å¢å¼ºäº†å…¨å±€å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ä¸‰ç»´å¤šæ–¹å‘æ‰«ææœºåˆ¶ï¼Œä»¥åŠ å¼ºå±€éƒ¨å’Œå…¨å±€çº¿ç´¢çš„èåˆã€‚åœ¨BraTS2020å’ŒBraTS2023æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§è¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03069v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºMambaçš„å¯¹ç§°é©±åŠ¨æ—¶ç©ºèåˆç½‘ç»œSSFMambaï¼Œç”¨äºä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚è¯¥ç½‘ç»œé‡‡ç”¨äº’è¡¥åŒåˆ†æ”¯æ¶æ„ï¼Œä»æ—¶ç©ºåŸŸæå–ç‰¹å¾å¹¶ä½¿ç”¨Mambaå—èåˆè¿™äº›å¼‚æ„ç‰¹å¾ï¼Œä»¥ä¿ç•™å…¨å±€ä¸Šä¸‹æ–‡å¹¶å¼ºåŒ–å±€éƒ¨ç»†èŠ‚ã€‚åœ¨é¢‘ç‡åŸŸåˆ†æ”¯ä¸­ï¼Œç»“åˆMambaæå–å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å“è¶Šèƒ½åŠ›ä¸é¢‘ç‡åŸŸç‰¹å¾çš„ååŒä½œç”¨ï¼Œè¿›ä¸€æ­¥å¢å¼ºå…¨å±€å»ºæ¨¡ã€‚è®¾è®¡ä¸‰ç»´å¤šæ–¹å‘æ‰«ææœºåˆ¶ï¼ŒåŠ å¼ºå±€éƒ¨å’Œå…¨å±€çº¿ç´¢çš„èåˆã€‚åœ¨BraTS2020å’ŒBraTS2023æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„é¡¹è¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œç©ºé—´åŸŸåœ¨å»ºæ¨¡å…¨å±€ä¸Šä¸‹æ–‡æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå› æ­¤æ–°å…´æ–¹æ³•å¼€å§‹ç»“åˆé¢‘ç‡åŸŸè¡¨ç¤ºã€‚</li>
<li>ç°æœ‰ç‰¹å¾æå–ç­–ç•¥å¿½ç•¥äº†é¢‘ç‡åŸŸä¿¡æ¯çš„ç‹¬ç‰¹æ€§è´¨ï¼Œå¦‚å…±è½­å¯¹ç§°æ€§ï¼Œå¹¶ä¸”æœªèƒ½è€ƒè™‘æ—¶ç©ºåŸŸé—´æ•°æ®åˆ†å¸ƒçš„åŸºç¡€å·®å¼‚ã€‚</li>
<li>SSFMambaç½‘ç»œé‡‡ç”¨åŒåˆ†æ”¯æ¶æ„ï¼Œåˆ†åˆ«ä»æ—¶ç©ºåŸŸæå–ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨Mambaå—è¿›è¡Œç‰¹å¾èåˆã€‚</li>
<li>é¢‘ç‡åŸŸåˆ†æ”¯åˆ©ç”¨Mambaæå–å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶ä¸é¢‘ç‡åŸŸç‰¹å¾çš„ååŒä½œç”¨ç»“åˆï¼Œä»¥å¢å¼ºå…¨å±€å»ºæ¨¡ã€‚</li>
<li>SSFMambaè®¾è®¡äº†ä¸€ä¸ªä¸‰ç»´å¤šæ–¹å‘æ‰«ææœºåˆ¶ï¼Œå¼ºåŒ–å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯çš„èåˆã€‚</li>
<li>åœ¨BraTS2020å’ŒBraTS2023æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSSFMBamaæ–¹æ³•æ€§èƒ½å“è¶Šï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03069">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-968b0e1d11e3648757ebe34bb0806cc4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84fe3f030495603dc52ae8025ad09fe9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4746e45bb28c62317e836f155130eb40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66b3bc822a8f706ce788c536c2900026.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63663d5dd17b6f008e7aaaf1e0939c90.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-65d1e068eda2867fe18e5d22ad33f55c.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-09  A Scalable Pipeline for Enabling Non-Verbal Speech Generation and   Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2d20e35a5c763131af23218456bdb7ac.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-09  GAP Gaussianize Any Point Clouds with Text Guidance
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29774.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
