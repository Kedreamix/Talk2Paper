<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-09  On the Generalization of SFT A Reinforcement Learning Perspective with   Reward Rectification">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-41a22450d3db1ced069fec47b3e50b75.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    93 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-09-æ›´æ–°"><a href="#2025-08-09-æ›´æ–°" class="headerlink" title="2025-08-09 æ›´æ–°"></a>2025-08-09 æ›´æ–°</h1><h2 id="On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification"><a href="#On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification" class="headerlink" title="On the Generalization of SFT: A Reinforcement Learning Perspective with   Reward Rectification"></a>On the Generalization of SFT: A Reinforcement Learning Perspective with   Reward Rectification</h2><p><strong>Authors:Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, Ming-Hsuan Yang, Xu Yang</strong></p>
<p>We present a simple yet theoretically motivated improvement to Supervised Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited generalization compared to reinforcement learning (RL). Through mathematical analysis, we reveal that standard SFT gradients implicitly encode a problematic reward structure that may severely restrict the generalization capabilities of model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing gradient updates for each token by dynamically rescaling the objective function with the probability of this token. Remarkably, this single-line code change significantly outperforms standard SFT across multiple challenging benchmarks and base models, demonstrating greatly improved generalization. Additionally, our approach shows competitive results in offline RL settings, offering an effective yet simpler alternative. This work bridges theoretical insight and practical solutions, substantially advancing SFT performance. The code will be available at <a target="_blank" rel="noopener" href="https://github.com/yongliang-wu/DFT">https://github.com/yongliang-wu/DFT</a>. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œäº†ç®€å•ä½†ç†è®ºä¸Šçš„æ”¹è¿›ï¼Œè§£å†³äº†å…¶ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç›¸æ¯”çš„æœ‰é™æ³›åŒ–é—®é¢˜ã€‚é€šè¿‡æ•°å­¦åˆ†æï¼Œæˆ‘ä»¬å‘ç°æ ‡å‡†SFTæ¢¯åº¦éšå«äº†ä¸€ç§é—®é¢˜æ€§çš„å¥–åŠ±ç»“æ„ï¼Œè¿™å¯èƒ½ä¸¥é‡é™åˆ¶æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†çº æ­£è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€å¾®è°ƒï¼ˆDFTï¼‰ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´ç›®æ ‡å‡½æ•°ä¸ä»¤ç‰Œæ¦‚ç‡æ¥ç¨³å®šæ¯ä¸ªä»¤ç‰Œçš„æ¢¯åº¦æ›´æ–°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸€ç®€å•çš„ä»£ç æ›´æ”¹åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•å’ŒåŸºå‡†æ¨¡å‹ä¸Šå¤§å¤§ä¼˜äºæ ‡å‡†SFTï¼Œè¡¨ç°å‡ºæå¤§çš„æ³›åŒ–æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨çº¿ä¸‹å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­ä¹Ÿå–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œæä¾›äº†ä¸€ä¸ªæœ‰æ•ˆä¸”ç®€å•çš„æ›¿ä»£æ–¹æ¡ˆã€‚è¿™é¡¹å·¥ä½œå°†ç†è®ºæ´å¯Ÿä¸å®ç”¨è§£å†³æ–¹æ¡ˆç›¸ç»“åˆï¼Œæå¤§åœ°æé«˜äº†SFTçš„æ€§èƒ½ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/yongliang-wu/DFT%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/yongliang-wu/DFTä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05629v1">PDF</a> 14 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„ä¸€ç§ç®€å•è€Œç†è®ºä¸Šçš„æ”¹è¿›æ–¹æ³•ï¼Œè§£å†³äº†å…¶ç›¸å¯¹äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æœ‰é™æ³›åŒ–èƒ½åŠ›é—®é¢˜ã€‚é€šè¿‡æ•°å­¦åˆ†æï¼Œä½œè€…å‘ç°æ ‡å‡†SFTæ¢¯åº¦éšå«äº†ä¸€ä¸ªå¯èƒ½ä¸¥é‡é™åˆ¶æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„é—®é¢˜å¥–åŠ±ç»“æ„ã€‚ä¸ºäº†çº æ­£è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†åŠ¨æ€å¾®è°ƒï¼ˆDFTï¼‰ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´ç›®æ ‡å‡½æ•°çš„æ¦‚ç‡æ¥ç¨³å®šæ¯ä¸ªæ ‡è®°çš„æ¢¯åº¦æ›´æ–°ã€‚è¿™ä¸€ç®€å•çš„ä»£ç æ”¹åŠ¨åœ¨å¤šä¸ªæŒ‘æˆ˜åŸºå‡†æµ‹è¯•å’ŒåŸºç¡€æ¨¡å‹ä¸Šæ˜¾è‘—ä¼˜äºæ ‡å‡†SFTï¼Œè¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ç¦»çº¿RLè®¾ç½®ä¸­ä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œæä¾›äº†ä¸€ç§æœ‰æ•ˆä¸”ç®€å•çš„æ›¿ä»£æ–¹æ¡ˆã€‚è¯¥ç ”ç©¶åœ¨ç†è®ºä¸Šæœ‰æ‰€çªç ´å¹¶æä¾›äº†å®é™…è§£å†³æ–¹æ¡ˆï¼Œå¤§å¹…æå‡äº†SFTçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« æå‡ºäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„ç†è®ºé—®é¢˜ï¼Œå³å…¶æœ‰é™çš„æ³›åŒ–èƒ½åŠ›ä¸æ½œåœ¨çš„å¥–åŠ±ç»“æ„é—®é¢˜ã€‚</li>
<li>é€šè¿‡æ•°å­¦åˆ†ææ­ç¤ºäº†æ ‡å‡†SFTæ¢¯åº¦çš„é—®é¢˜ï¼Œå¹¶è¿›è¡Œäº†æ·±å…¥çš„è®¨è®ºã€‚</li>
<li>æå‡ºäº†åŠ¨æ€å¾®è°ƒï¼ˆDFTï¼‰ä½œä¸ºè§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´ç›®æ ‡å‡½æ•°æ¦‚ç‡æ¥ç¨³å®šæ¢¯åº¦æ›´æ–°ã€‚</li>
<li>DFTæ˜¾è‘—æé«˜äº†æ ‡å‡†SFTçš„æ€§èƒ½ï¼Œåœ¨å¤šä¸ªæŒ‘æˆ˜åŸºå‡†æµ‹è¯•å’ŒåŸºç¡€æ¨¡å‹ä¸Šè¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>DFTåœ¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ è®¾ç½®ä¸­ä¹Ÿæ˜¾ç¤ºå‡ºç«äº‰åŠ›ï¼Œæä¾›äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ä»…åœ¨ç†è®ºä¸Šæœ‰æ‰€çªç ´ï¼Œè¿˜æä¾›äº†å®ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œæœ‰åŠ©äºæ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9cb25f1c0d97726188f92047e12a433e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f08fd390f6bc1a892f430ec45de7d340.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Shuffle-R1-Efficient-RL-framework-for-Multimodal-Large-Language-Models-via-Data-centric-Dynamic-Shuffle"><a href="#Shuffle-R1-Efficient-RL-framework-for-Multimodal-Large-Language-Models-via-Data-centric-Dynamic-Shuffle" class="headerlink" title="Shuffle-R1: Efficient RL framework for Multimodal Large Language Models   via Data-centric Dynamic Shuffle"></a>Shuffle-R1: Efficient RL framework for Multimodal Large Language Models   via Data-centric Dynamic Shuffle</h2><p><strong>Authors:Linghao Zhu, Yiran Guan, Dingkang Liang, Jianzhong Ju, Zhenbo Luo, Bin Qin, Jian Luan, Yuliang Liu, Xiang Bai</strong></p>
<p>Reinforcement learning (RL) has emerged as an effective post-training paradigm for enhancing the reasoning capabilities of multimodal large language model (MLLM). However, current RL pipelines often suffer from training inefficiencies caused by two underexplored issues: Advantage Collapsing, where most advantages in a batch concentrate near zero, and Rollout Silencing, where the proportion of rollouts contributing non-zero gradients diminishes over time. These issues lead to suboptimal gradient updates and hinder long-term learning efficiency. To address these issues, we propose Shuffle-R1, a simple yet principled framework that improves RL fine-tuning efficiency by dynamically restructuring trajectory sampling and batch composition. It introduces (1) Pairwise Trajectory Sampling, which selects high-contrast trajectories with large advantages to improve gradient signal quality, and (2) Advantage-based Trajectory Shuffle, which increases exposure of valuable rollouts through informed batch reshuffling. Experiments across multiple reasoning benchmarks show that our framework consistently outperforms strong RL baselines with minimal overhead. These results highlight the importance of data-centric adaptations for more efficient RL training in MLLM. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²ç»ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„è®­ç»ƒåèŒƒå¼ï¼Œç”¨äºæé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„RLç®¡é“é€šå¸¸å­˜åœ¨ä¸¤ä¸ªæœªå……åˆ†æ¢ç´¢çš„é—®é¢˜ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆç‡ä½ä¸‹ï¼šä¼˜åŠ¿å´©å¡Œï¼Œå³ä¸€æ‰¹æ¬¡ä¸­çš„å¤§éƒ¨åˆ†ä¼˜åŠ¿é›†ä¸­åœ¨é›¶é™„è¿‘ï¼›ä»¥åŠå›åˆé™é»˜ï¼Œå³æä¾›éé›¶æ¢¯åº¦çš„å›åˆæ¯”ä¾‹éšæ—¶é—´å‡å°‘ã€‚è¿™äº›é—®é¢˜å¯¼è‡´æ¢¯åº¦æ›´æ–°ä¸ä½³ï¼Œå¹¶é˜»ç¢é•¿æœŸå­¦ä¹ æ•ˆç‡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Shuffle-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•è€ŒåŸºäºåŸåˆ™çš„æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€é‡æ„è½¨è¿¹é‡‡æ ·å’Œæ‰¹æ¬¡ç»„æˆæ¥æé«˜RLå¾®è°ƒæ•ˆç‡ã€‚å®ƒå¼•å…¥äº†ï¼ˆ1ï¼‰é…å¯¹è½¨è¿¹é‡‡æ ·ï¼Œé€‰æ‹©é«˜å¯¹æ¯”åº¦çš„è½¨è¿¹å’Œè¾ƒå¤§çš„ä¼˜åŠ¿æ¥æé«˜æ¢¯åº¦ä¿¡å·è´¨é‡ï¼›ï¼ˆ2ï¼‰åŸºäºä¼˜åŠ¿çš„è½¨è¿¹æ´—ç‰Œï¼Œé€šè¿‡ä¿¡æ¯æ‰¹å¤„ç†æ´—ç‰Œå¢åŠ æœ‰ä»·å€¼çš„å›åˆçš„æ›å…‰ã€‚åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å§‹ç»ˆä¼˜äºå¼ºå¤§çš„RLåŸºçº¿ï¼Œä¸”å¼€é”€æœ€å°ã€‚è¿™äº›ç»“æœçªå‡ºäº†æ•°æ®ä¸ºä¸­å¿ƒé€‚åº”åœ¨MLLMä¸­æ›´é«˜æ•ˆRLè®­ç»ƒçš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05612v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½œä¸ºæé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¨ç†èƒ½åŠ›çš„åè®­ç»ƒèŒƒå¼ï¼Œä½†å­˜åœ¨è®­ç»ƒæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚é’ˆå¯¹ä¼˜åŠ¿å´©æºƒå’Œæ»šåŠ¨é™é»˜è¿™ä¸¤ä¸ªæœªå……åˆ†æ¢ç´¢çš„é—®é¢˜ï¼Œæå‡ºäº†Shuffle-R1æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€é‡æ„è½¨è¿¹é‡‡æ ·å’Œæ‰¹æ¬¡ç»„æˆæ¥æé«˜RLå¾®è°ƒæ•ˆç‡ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬æˆå¯¹è½¨è¿¹é‡‡æ ·å’ŒåŸºäºä¼˜åŠ¿çš„è½¨è¿¹æ´—ç‰Œï¼Œä»¥æé«˜æ¢¯åº¦ä¿¡å·è´¨é‡å’Œå¢åŠ æœ‰ä»·å€¼è½¨è¿¹çš„æš´éœ²ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå§‹ç»ˆä¼˜äºå¼ºå¤§çš„RLåŸºå‡†æµ‹è¯•ï¼Œä¸”å‡ ä¹ä¸å¢åŠ å¼€é”€ã€‚è¿™å¼ºè°ƒäº†æ•°æ®ä¸ºä¸­å¿ƒé€‚åº”å¯¹äºæé«˜MLLMä¸­çš„RLè®­ç»ƒæ•ˆç‡çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç”¨äºæé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰RLç®¡é“å­˜åœ¨è®­ç»ƒæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œä¸»è¦è¡¨ç°ä¸ºä¼˜åŠ¿å´©æºƒå’Œæ»šåŠ¨é™é»˜ã€‚</li>
<li>Shuffle-R1æ¡†æ¶é€šè¿‡åŠ¨æ€é‡æ„è½¨è¿¹é‡‡æ ·å’Œæ‰¹æ¬¡ç»„æˆæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>Shuffle-R1åŒ…æ‹¬æˆå¯¹è½¨è¿¹é‡‡æ ·å’ŒåŸºäºä¼˜åŠ¿çš„è½¨è¿¹æ´—ç‰Œï¼Œä»¥æé«˜æ¢¯åº¦ä¿¡å·è´¨é‡å’Œæœ‰ä»·å€¼è½¨è¿¹çš„æš´éœ²ã€‚</li>
<li>å®éªŒè¯æ˜Shuffle-R1åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜äºå¼ºRLåŸºå‡†æµ‹è¯•ã€‚</li>
<li>Shuffle-R1æ¡†æ¶å‡ ä¹ä¸å¢åŠ å¼€é”€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cc1d3a55c02ed2126dde61d9391bd17f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c2cba9eecf11bf9962322028086f1dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-67b94c09311fb76397ad561c6baaba15.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef1e4d871002ce3fa6bd0f2aefc219a6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-94515193d32f5c47b63d72d74bdc87a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64777269f147350dce43e52793a00a66.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Hi3DEval-Advancing-3D-Generation-Evaluation-with-Hierarchical-Validity"><a href="#Hi3DEval-Advancing-3D-Generation-Evaluation-with-Hierarchical-Validity" class="headerlink" title="Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity"></a>Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity</h2><p><strong>Authors:Yuhan Zhang, Long Zhuo, Ziyang Chu, Tong Wu, Zhibing Li, Liang Pan, Dahua Lin, Ziwei Liu</strong></p>
<p>Despite rapid advances in 3D content generation, quality assessment for the generated 3D assets remains challenging. Existing methods mainly rely on image-based metrics and operate solely at the object level, limiting their ability to capture spatial coherence, material authenticity, and high-fidelity local details. 1) To address these challenges, we introduce Hi3DEval, a hierarchical evaluation framework tailored for 3D generative content. It combines both object-level and part-level evaluation, enabling holistic assessments across multiple dimensions as well as fine-grained quality analysis. Additionally, we extend texture evaluation beyond aesthetic appearance by explicitly assessing material realism, focusing on attributes such as albedo, saturation, and metallicness. 2) To support this framework, we construct Hi3DBench, a large-scale dataset comprising diverse 3D assets and high-quality annotations, accompanied by a reliable multi-agent annotation pipeline. We further propose a 3D-aware automated scoring system based on hybrid 3D representations. Specifically, we leverage video-based representations for object-level and material-subject evaluations to enhance modeling of spatio-temporal consistency and employ pretrained 3D features for part-level perception. Extensive experiments demonstrate that our approach outperforms existing image-based metrics in modeling 3D characteristics and achieves superior alignment with human preference, providing a scalable alternative to manual evaluations. The project page is available at <a target="_blank" rel="noopener" href="https://zyh482.github.io/Hi3DEval/">https://zyh482.github.io/Hi3DEval/</a>. </p>
<blockquote>
<p>å°½ç®¡åœ¨ä¸‰ç»´å†…å®¹ç”Ÿæˆæ–¹é¢å–å¾—äº†å¿«é€Ÿå‘å±•ï¼Œä½†å¯¹ç”Ÿæˆçš„ä¸‰ç»´èµ„äº§çš„è´¨é‡è¯„ä¼°ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºå›¾åƒæŒ‡æ ‡ï¼Œä»…åœ¨å¯¹è±¡å±‚é¢è¿›è¡Œæ“ä½œï¼Œè¿™é™åˆ¶äº†å®ƒä»¬æ•æ‰ç©ºé—´è¿è´¯æ€§ã€ææ–™çœŸå®æ€§å’Œé«˜ä¿çœŸå±€éƒ¨ç»†èŠ‚çš„èƒ½åŠ›ã€‚1ï¼‰ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Hi3DEvalï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºä¸‰ç»´ç”Ÿæˆå†…å®¹è®¾è®¡çš„åˆ†å±‚è¯„ä¼°æ¡†æ¶ã€‚å®ƒç»“åˆäº†å¯¹è±¡å±‚é¢å’Œéƒ¨åˆ†å±‚é¢çš„è¯„ä¼°ï¼Œèƒ½å¤Ÿå®ç°å¤šä¸ªç»´åº¦çš„æ•´ä½“è¯„ä¼°ä»¥åŠç²¾ç»†çš„è´¨é‡åˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æ˜ç¡®è¯„ä¼°ææ–™çœŸå®æ€§ï¼Œå°†çº¹ç†è¯„ä¼°æ‰©å±•åˆ°ç¾å­¦å¤–è§‚ä¹‹å¤–ï¼Œé‡ç‚¹å…³æ³¨å¦‚æ¼«åå°„ã€é¥±å’Œåº¦å’Œé‡‘å±æ€§ç­‰å±æ€§ã€‚2ï¼‰ä¸ºäº†æ”¯æŒè¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬æ„å»ºäº†Hi3DBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«å„ç§ä¸‰ç»´èµ„äº§å’Œé«˜è´¨é‡æ³¨é‡Šçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä»¥åŠå¯é çš„å¤šå…ƒæ³¨é‡Šç®¡é“ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†åŸºäºæ··åˆä¸‰ç»´è¡¨ç¤ºçš„3Dæ„ŸçŸ¥è‡ªåŠ¨è¯„åˆ†ç³»ç»Ÿã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨è§†é¢‘è¡¨ç¤ºè¿›è¡Œå¯¹è±¡çº§å’Œææ–™ä¸»é¢˜è¯„ä¼°ï¼Œä»¥å¢å¼ºæ—¶ç©ºä¸€è‡´æ€§çš„å»ºæ¨¡ï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„3Dç‰¹å¾è¿›è¡Œéƒ¨åˆ†å±‚é¢çš„æ„ŸçŸ¥ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å»ºæ¨¡ä¸‰ç»´ç‰¹å¾æ–¹é¢ä¼˜äºç°æœ‰çš„å›¾åƒæŒ‡æ ‡ï¼Œä¸äººç±»åå¥½é«˜åº¦ä¸€è‡´ï¼Œä¸ºæ‰‹åŠ¨è¯„ä¼°æä¾›äº†å¯ä¼¸ç¼©çš„æ›¿ä»£æ–¹æ¡ˆã€‚é¡¹ç›®é¡µé¢å¯è®¿é—®äº <a target="_blank" rel="noopener" href="https://zyh482.github.io/Hi3DEval/%E3%80%82">https://zyh482.github.io/Hi3DEval/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05609v1">PDF</a> Page: <a target="_blank" rel="noopener" href="https://zyh482.github.io/Hi3DEval/">https://zyh482.github.io/Hi3DEval/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹3Dç”Ÿæˆå†…å®¹çš„è´¨é‡è¯„ä¼°çš„æŒ‘æˆ˜ï¼Œå¹¶å¼•å…¥äº†Hi3DEvalè¿™ä¸€å±‚æ¬¡åŒ–çš„è¯„ä¼°æ¡†æ¶ä»¥åŠHi3DBenchæ•°æ®é›†ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¯¹è±¡çº§åˆ«å’Œéƒ¨åˆ†çº§åˆ«çš„è¯„ä¼°ï¼Œèƒ½å¤Ÿè¿›è¡Œè·¨å¤šä¸ªç»´åº¦çš„æ•´ä½“è¯„ä¼°ä»¥åŠç²¾ç»†çš„è´¨é‡åˆ†æã€‚åŒæ—¶ï¼Œæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é›†Hi3DBenchï¼Œå¹¶å¼€å‘äº†åŸºäºæ··åˆä¸‰ç»´è¡¨ç¤ºçš„è‡ªåŠ¨åŒ–è¯„åˆ†ç³»ç»Ÿã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿä¸‰ç»´ç‰¹æ€§æ–¹é¢ä¼˜äºç°æœ‰å›¾åƒåŸºç¡€æŒ‡æ ‡ï¼Œä¸äººç±»åå¥½å¯¹é½æ€§æ›´é«˜ï¼Œä¸ºæ‰‹åŠ¨è¯„ä¼°æä¾›äº†å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰3Dèµ„äº§ç”Ÿæˆè´¨é‡è¯„ä¼°é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦æ›´å…¨é¢çš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>å¼•å…¥Hi3DEvalå±‚æ¬¡åŒ–è¯„ä¼°æ¡†æ¶ï¼Œç»“åˆå¯¹è±¡çº§åˆ«å’Œéƒ¨åˆ†çº§åˆ«çš„è¯„ä¼°ï¼Œèƒ½å¤Ÿæ•æ‰ç©ºé—´è¿è´¯æ€§ã€ææ–™çœŸå®æ€§å’Œé«˜ä¿çœŸå±€éƒ¨ç»†èŠ‚ã€‚</li>
<li>æ„å»ºHi3DBenchå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«å¤šæ ·3Dèµ„äº§å’Œé«˜è´¨é‡æ³¨é‡Šï¼Œæ”¯æŒHi3DEvalæ¡†æ¶ã€‚</li>
<li>å¼€å‘äº†åŸºäºæ··åˆä¸‰ç»´è¡¨ç¤ºçš„è‡ªåŠ¨åŒ–è¯„åˆ†ç³»ç»Ÿï¼Œåˆ©ç”¨è§†é¢‘è¡¨ç¤ºè¿›è¡Œå¯¹è±¡çº§åˆ«å’Œææ–™ä¸»é¢˜è¯„ä¼°ï¼Œå¢å¼ºæ—¶ç©ºä¸€è‡´æ€§å»ºæ¨¡ã€‚</li>
<li>æ¡†æ¶è¶…è¶Šäº†å•çº¯çš„ç¾å­¦å¤–è§‚çº¹ç†è¯„ä¼°ï¼Œæ˜ç¡®è¯„ä¼°ææ–™çœŸå®æ€§ï¼Œå…³æ³¨å¦‚æ˜æš—åº¦ã€é¥±å’Œåº¦å’Œé‡‘å±æ„Ÿç­‰å±æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨¡æ‹Ÿä¸‰ç»´ç‰¹æ€§æ–¹é¢ä¼˜äºç°æœ‰å›¾åƒåŸºç¡€æŒ‡æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05609">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-324882c4195acf485454f4ab1ca58b1a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-272c214b26c0a3e5d77cfed52fe4b3c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e7bc188fe757c95b35970f5286ad986.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Uni-cot-Towards-Unified-Chain-of-Thought-Reasoning-Across-Text-and-Vision"><a href="#Uni-cot-Towards-Unified-Chain-of-Thought-Reasoning-Across-Text-and-Vision" class="headerlink" title="Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and   Vision"></a>Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and   Vision</h2><p><strong>Authors:Luozheng Qin, Jia Gong, Yuqing Sun, Tianjiao Li, Mengping Yang, Xiaomeng Yang, Chao Qu, Zhiyu Tan, Hao Li</strong></p>
<p>Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large Language Models (LLMs) by decomposing complex tasks into simpler, sequential subtasks. However, extending CoT to vision-language reasoning tasks remains challenging, as it often requires interpreting transitions of visual states to support reasoning. Existing methods often struggle with this due to limited capacity of modeling visual state transitions or incoherent visual trajectories caused by fragmented architectures.   To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought framework that enables coherent and grounded multimodal reasoning within a single unified model. The key idea is to leverage a model capable of both image understanding and generation to reason over visual content and model evolving visual states. However, empowering a unified model to achieve that is non-trivial, given the high computational cost and the burden of training. To address this, Uni-CoT introduces a novel two-level reasoning paradigm: A Macro-Level CoT for high-level task planning and A Micro-Level CoT for subtask execution. This design significantly reduces the computational overhead. Furthermore, we introduce a structured training paradigm that combines interleaved image-text supervision for macro-level CoT with multi-task objectives for micro-level CoT. Together, these innovations allow Uni-CoT to perform scalable and coherent multi-modal reasoning. Furthermore, thanks to our design, all experiments can be efficiently completed using only 8 A100 GPUs with 80GB VRAM each. Experimental results on reasoning-driven image generation benchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT demonstrates SOTA performance and strong generalization, establishing Uni-CoT as a promising solution for multi-modal reasoning. Project Page and Code: <a target="_blank" rel="noopener" href="https://sais-fuxi.github.io/projects/uni-cot/">https://sais-fuxi.github.io/projects/uni-cot/</a> </p>
<blockquote>
<p>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†å·²è¢«å¹¿æ³›åº”ç”¨äºé€šè¿‡åˆ†è§£å¤æ‚ä»»åŠ¡ä¸ºæ›´ç®€å•ã€è¿ç»­çš„å­ä»»åŠ¡æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ç„¶è€Œï¼Œå°†CoTæ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨ç†ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒé€šå¸¸éœ€è¦å¯¹è§†è§‰çŠ¶æ€çš„è¿‡æ¸¡è¿›è¡Œè§£é‡Šä»¥æ”¯æŒæ¨ç†ã€‚ç°æœ‰æ–¹æ³•å¸¸å¸¸å› æ­¤è€Œè‹¦è‹¦æŒ£æ‰ï¼Œå› ä¸ºå®ƒä»¬åœ¨å»ºæ¨¡è§†è§‰çŠ¶æ€è¿‡æ¸¡æ–¹é¢çš„èƒ½åŠ›æœ‰é™ï¼Œæˆ–è€…ç”±äºæ¶æ„ç¢ç‰‡åŒ–è€Œå¯¼è‡´è§†è§‰è½¨è¿¹ä¸ä¸€è‡´ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Uni-CoTï¼Œä¸€ä¸ªç»Ÿä¸€çš„é“¾å¼æ€ç»´æ¡†æ¶ï¼Œå®ƒå¯ä»¥åœ¨å•ä¸ªç»Ÿä¸€æ¨¡å‹å†…å®ç°è¿è´¯å’ŒåŸºäºæƒ…å¢ƒçš„å¤šæ¨¡æ€æ¨ç†ã€‚ä¸»è¦æ€æƒ³æ˜¯åˆ©ç”¨æ—¢èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆå›¾åƒåˆèƒ½å¤Ÿå¯¹è§†è§‰å†…å®¹è¿›è¡Œæ¨ç†çš„æ¨¡å‹ï¼Œå¹¶æ¨¡æ‹Ÿä¸æ–­æ¼”å˜çš„è§†è§‰çŠ¶æ€ã€‚ç„¶è€Œï¼Œå¯¹äºä¸€ä¸ªç»Ÿä¸€æ¨¡å‹æ¥è¯´å®ç°è¿™ä¸€ç‚¹å¹¶éæ˜“äº‹ï¼Œè€ƒè™‘åˆ°è®¡ç®—æˆæœ¬é«˜æ˜‚å’Œè®­ç»ƒè´Ÿæ‹…æ²‰é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒUni-CoTå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ä¸¤çº§æ¨ç†æ¨¡å¼ï¼šç”¨äºé«˜çº§ä»»åŠ¡è§„åˆ’çš„å®è§‚å±‚é¢CoTå’Œç”¨äºå­ä»»åŠ¡æ‰§è¡Œçš„å¾®è§‚å±‚é¢CoTã€‚è¿™ç§è®¾è®¡æ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç»“æ„åŒ–çš„è®­ç»ƒæ¨¡å¼ï¼Œè¯¥æ¨¡å¼ç»“åˆäº†å®è§‚å±‚é¢CoTçš„äº¤é”™å›¾åƒæ–‡æœ¬ç›‘ç£ä¸å¾®è§‚å±‚é¢CoTçš„å¤šä»»åŠ¡ç›®æ ‡ã€‚è¿™äº›åˆ›æ–°ä½¿Uni-CoTèƒ½å¤Ÿè¿›è¡Œå¯æ‰©å±•å’Œè¿è´¯çš„å¤šæ¨¡æ€æ¨ç†ã€‚æ­¤å¤–ï¼Œå¾—ç›Šäºæˆ‘ä»¬çš„è®¾è®¡ï¼Œæ‰€æœ‰å®éªŒéƒ½èƒ½ä»…ä½¿ç”¨8ä¸ªå¸¦æœ‰å„80GB VRAMçš„A100 GPUé«˜æ•ˆå®Œæˆã€‚åœ¨åŸºäºæ¨ç†çš„å›¾åƒç”ŸæˆåŸºå‡†ï¼ˆWISEï¼‰å’Œç¼–è¾‘åŸºå‡†ï¼ˆRISEå’ŒKRISï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒUni-CoTå±•ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œç¡®ç«‹äº†Uni-CoTåœ¨å¤šæ¨¡æ€æ¨ç†ä¸­çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆåœ°ä½ã€‚é¡¹ç›®é¡µé¢å’Œä»£ç ï¼š<a target="_blank" rel="noopener" href="https://sais-fuxi.github.io/projects/uni-cot/">https://sais-fuxi.github.io/projects/uni-cot/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05606v1">PDF</a> <a target="_blank" rel="noopener" href="https://sais-fuxi.github.io/projects/uni-cot/">https://sais-fuxi.github.io/projects/uni-cot/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†è¢«å¹¿æ³›åº”ç”¨äºé€šè¿‡åˆ†è§£å¤æ‚ä»»åŠ¡ä¸ºæ›´ç®€å•çš„é¡ºåºå­ä»»åŠ¡æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ç„¶è€Œï¼Œå°†CoTæ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨ç†ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒé€šå¸¸éœ€è¦è§£é‡Šè§†è§‰çŠ¶æ€çš„è½¬å˜ä¸ºæ¨ç†æä¾›æ”¯æŒã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å› ä¸ºæœ‰é™çš„å»ºæ¨¡è§†è§‰çŠ¶æ€è½¬æ¢çš„èƒ½åŠ›æˆ–å› ç»“æ„ç¢ç‰‡åŒ–å¯¼è‡´çš„è§†è§‰è½¨è¿¹ä¸ä¸€è‡´è€Œé¢ä¸´å›°éš¾ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Uni-CoTï¼Œä¸€ä¸ªç»Ÿä¸€çš„é“¾å¼æ€ç»´æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€æ¨¡å‹ä¸­å®ç°è¿è´¯ä¸”æœ‰æ ¹åŸºçš„å¤šæ¨¡æ€æ¨ç†ã€‚å®ƒé€šè¿‡ç»“åˆå›¾åƒç†è§£å’Œç”Ÿæˆèƒ½åŠ›æ¥å®ç°å¯¹è§†è§‰å†…å®¹çš„æ¨ç†å’Œå»ºæ¨¡ä¸æ–­å‘å±•çš„è§†è§‰çŠ¶æ€ã€‚ç„¶è€Œï¼Œåœ¨æ„å»ºèƒ½å¤Ÿæ‰§è¡Œè¯¥åŠŸèƒ½çš„ç»Ÿä¸€æ¨¡å‹æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºè®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”è®­ç»ƒè´Ÿæ‹…æ²‰é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒUni-CoTå¼•å…¥äº†ä¸€ç§æ–°å‹çš„ä¸¤çº§æ¨ç†æ¨¡å¼ï¼šç”¨äºé«˜çº§ä»»åŠ¡è§„åˆ’çš„å®è§‚å±‚é¢CoTå’Œç”¨äºå­ä»»åŠ¡æ‰§è¡Œçš„å¾®è§‚å±‚é¢CoTã€‚è¿™ç§è®¾è®¡æ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç»“æ„åŒ–çš„è®­ç»ƒæ¨¡å¼ï¼Œè¯¥æ¨¡å¼ç»“åˆäº†å®è§‚å±‚é¢CoTçš„äº¤é”™å›¾åƒæ–‡æœ¬ç›‘ç£ä¸å¾®è§‚å±‚é¢CoTçš„å¤šä»»åŠ¡ç›®æ ‡ã€‚è¿™äº›åˆ›æ–°ä½¿Uni-CoTèƒ½å¤Ÿæ‰§è¡Œå¯æ‰©å±•ä¸”è¿è´¯çš„å¤šæ¨¡æ€æ¨ç†ã€‚æ­¤å¤–ï¼Œç”±äºæˆ‘ä»¬çš„è®¾è®¡ï¼Œæ‰€æœ‰å®éªŒéƒ½èƒ½å¤Ÿåœ¨ä»…ä½¿ç”¨æ¯å°é…å¤‡æœ‰8ä¸ªGPUçš„æœåŠ¡å™¨ä¸Šé«˜æ•ˆå®Œæˆã€‚åœ¨åŸºäºæ¨ç†çš„å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ï¼ˆWISEï¼‰å’Œç¼–è¾‘åŸºå‡†æµ‹è¯•ï¼ˆRISEå’ŒKRISï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒUni-CoTå±•ç¤ºå‡ºäº†è¶…è¶Šç°æœ‰è§£å†³æ–¹æ¡ˆçš„æ€§èƒ½å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ‰å…³é¡¹ç›®çš„è¯¦ç»†ä¿¡æ¯è¯·å‚é˜…ï¼š<a target="_blank" rel="noopener" href="https://sais-fuxi.github.io/projects/uni-cot/%E3%80%82">https://sais-fuxi.github.io/projects/uni-cot/ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Uni-CoTæˆåŠŸåœ°å°†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†åº”ç”¨äºå¤šæ¨¡æ€ä»»åŠ¡ï¼Œå®ç°äº†è¿è´¯çš„è§†è§‰è¯­è¨€æ¨ç†ã€‚</li>
<li>é€šè¿‡å¼•å…¥ä¸¤çº§æ¨ç†æ¨¡å¼ï¼ˆå®è§‚å’Œå¾®è§‚å±‚é¢ï¼‰ï¼ŒUni-CoTæ˜¾è‘—é™ä½äº†è®¡ç®—å¼€é”€ï¼Œæé«˜äº†æ¨ç†æ•ˆç‡ã€‚</li>
<li>Uni-CoTç»“åˆäº†å›¾åƒç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œèƒ½å¤Ÿå»ºæ¨¡ä¸æ–­å‘å±•çš„è§†è§‰çŠ¶æ€ï¼Œå¢å¼ºäº†å…¶åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>ç»“æ„åŒ–çš„è®­ç»ƒæ¨¡å¼ç»“åˆäº†å®è§‚å±‚é¢CoTçš„äº¤é”™å›¾åƒæ–‡æœ¬ç›‘ç£ä¸å¾®è§‚å±‚é¢CoTçš„å¤šä»»åŠ¡ç›®æ ‡ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œæ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒUni-CoTåœ¨åŸºäºæ¨ç†çš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>Uni-CoTçš„è®¾è®¡ä¼˜åŒ–ä½¿å¾—å…¶èƒ½å¤Ÿåœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸Šé«˜æ•ˆè¿è¡Œï¼Œé™ä½äº†è®­ç»ƒå’Œéƒ¨ç½²çš„æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05606">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4f8a2849ab458672503129cc4edfe748.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b85ae310a96a848f495fcaea8acd402.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab1d0a8673e7d72f507cd7db7b5bc6da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-775ba7f7244b3dbbc1d0efa72c338f02.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MathSmith-Towards-Extremely-Hard-Mathematical-Reasoning-by-Forging-Synthetic-Problems-with-a-Reinforced-Policy"><a href="#MathSmith-Towards-Extremely-Hard-Mathematical-Reasoning-by-Forging-Synthetic-Problems-with-a-Reinforced-Policy" class="headerlink" title="MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging   Synthetic Problems with a Reinforced Policy"></a>MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging   Synthetic Problems with a Reinforced Policy</h2><p><strong>Authors:Shaoxiong Zhan, Yanlin Lai, Ziyu Lu, Dahua Lin, Ziqing Yang, Fei Tang</strong></p>
<p>Large language models have achieved substantial progress in mathematical reasoning, yet their advancement is limited by the scarcity of high-quality, high-difficulty training data. Existing synthesis methods largely rely on transforming human-written templates, limiting both diversity and scalability. We propose MathSmith, a novel framework for synthesizing challenging mathematical problems to enhance LLM reasoning. Rather than modifying existing problems, MathSmith constructs new ones from scratch by randomly sampling concept-explanation pairs from PlanetMath, ensuring data independence and avoiding contamination. To increase difficulty, we design nine predefined strategies as soft constraints during rationales. We further adopts reinforcement learning to jointly optimize structural validity, reasoning complexity, and answer consistency. The length of the reasoning trace generated under autoregressive prompting is used to reflect cognitive complexity, encouraging the creation of more demanding problems aligned with long-chain-of-thought reasoning. Experiments across five benchmarks, categorized as easy &amp; medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025, OlympiadBench), show that MathSmith consistently outperforms existing baselines under both short and long CoT settings. Additionally, a weakness-focused variant generation module enables targeted improvement on specific concepts. Overall, MathSmith exhibits strong scalability, generalization, and transferability, highlighting the promise of high-difficulty synthetic data in advancing LLM reasoning capabilities. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶å‘å±•å—åˆ°é«˜è´¨é‡ã€é«˜éš¾åº¦è®­ç»ƒæ•°æ®ç¨€ç¼ºçš„é™åˆ¶ã€‚ç°æœ‰çš„åˆæˆæ–¹æ³•å¤§å¤šä¾èµ–äºè½¬æ¢äººå·¥ç¼–å†™çš„æ¨¡æ¿ï¼Œè¿™é™åˆ¶äº†å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬æå‡ºäº†MathSmithï¼Œä¸€ä¸ªåˆæˆå…·æœ‰æŒ‘æˆ˜æ€§æ•°å­¦é—®é¢˜ä»¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°å‹æ¡†æ¶ã€‚MathSmithä¸æ˜¯ä¿®æ”¹ç°æœ‰é—®é¢˜ï¼Œè€Œæ˜¯ä»é›¶å¼€å§‹æ„å»ºæ–°é—®é¢˜ï¼Œé€šè¿‡ä»PlanetMathéšæœºæŠ½å–æ¦‚å¿µè§£é‡Šå¯¹ï¼Œç¡®ä¿æ•°æ®ç‹¬ç«‹ï¼Œé¿å…æ±¡æŸ“ã€‚ä¸ºäº†æé«˜é—®é¢˜çš„éš¾åº¦ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¹ç§é¢„è®¾ç­–ç•¥ä½œä¸ºåˆç†æ¨ç†è¿‡ç¨‹ä¸­çš„è½¯çº¦æŸã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¥è”åˆä¼˜åŒ–ç»“æ„æœ‰æ•ˆæ€§ã€æ¨ç†å¤æ‚æ€§å’Œç­”æ¡ˆä¸€è‡´æ€§ã€‚åœ¨è‡ªåŠ¨å›å½’æç¤ºä¸‹äº§ç”Ÿçš„æ¨ç†è½¨è¿¹é•¿åº¦è¢«ç”¨æ¥åæ˜ è®¤çŸ¥å¤æ‚æ€§ï¼Œé¼“åŠ±åˆ›å»ºä¸é•¿é“¾æ€ç»´æ¨ç†ç›¸ç¬¦çš„æ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒï¼Œè¢«åˆ†ç±»ä¸ºç®€å•å’Œä¸­ç­‰éš¾åº¦ï¼ˆGSM8Kï¼ŒMATH-500ï¼‰ä»¥åŠé«˜éš¾åº¦ï¼ˆAIME2024ï¼ŒAIME2025ï¼ŒOlympiadBenchï¼‰ï¼Œç»“æœè¡¨æ˜MathSmithåœ¨çŸ­é“¾å’Œé•¿é“¾æ€ç»´è®¾ç½®ä¸‹å‡æŒç»­è¶…è¶Šç°æœ‰åŸºçº¿ã€‚æ­¤å¤–ï¼Œå¼±ç‚¹èšç„¦çš„å˜ä½“ç”Ÿæˆæ¨¡å—èƒ½å¤Ÿå®ç°ç‰¹å®šæ¦‚å¿µçš„é’ˆå¯¹æ€§æ”¹è¿›ã€‚æ€»ä½“ä¸Šï¼ŒMathSmithè¡¨ç°å‡ºå¼ºå¤§çš„å¯æ‰©å±•æ€§ã€æ³›åŒ–èƒ½åŠ›å’Œè¿ç§»èƒ½åŠ›ï¼Œçªæ˜¾äº†é«˜éš¾åº¦åˆæˆæ•°æ®åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05592v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å—é™äºé«˜è´¨é‡ã€é«˜éš¾åº¦è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºæ€§ã€‚ç°æœ‰åˆæˆæ–¹æ³•ä¸»è¦ä¾èµ–äººç±»ç¼–å†™æ¨¡æ¿çš„è½¬æ¢ï¼Œé™åˆ¶äº†å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºMathSmithæ¡†æ¶ï¼Œé€šè¿‡ä»PlanetMathä¸­éšæœºé‡‡æ ·æ¦‚å¿µè§£é‡Šå¯¹æ¥æ„å»ºæ–°çš„æ•°å­¦é—®é¢˜ï¼Œç¡®ä¿æ•°æ®ç‹¬ç«‹æ€§å’Œé¿å…æ±¡æŸ“ã€‚é€šè¿‡ä¹ç§é¢„è®¾ç­–ç•¥ä½œä¸ºè½¯çº¦æŸæ¥å¢åŠ é—®é¢˜éš¾åº¦ã€‚é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¥è”åˆä¼˜åŒ–ç»“æ„æœ‰æ•ˆæ€§ã€æ¨ç†å¤æ‚æ€§å’Œç­”æ¡ˆä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMathSmithåœ¨çŸ­é“¾å’Œé•¿é“¾æ€ç»´æ¨ç†è®¾ç½®ä¸‹å‡ä¼˜äºç°æœ‰åŸºå‡†æµ‹è¯•ï¼Œæ˜¾ç¤ºå‡ºå…¶å¼ºå¤§çš„å¯æ‰©å±•æ€§ã€é€šç”¨æ€§å’Œå¯è¿ç§»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢å·²å–å¾—è¿›å±•ï¼Œä½†ç¼ºä¹é«˜è´¨é‡ã€é«˜éš¾åº¦çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>ç°æœ‰æ•°å­¦é—®é¢˜çš„åˆæˆæ–¹æ³•ä¸»è¦ä¾èµ–äººç±»ç¼–å†™æ¨¡æ¿ï¼Œé™åˆ¶äº†å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>MathSmithæ¡†æ¶é€šè¿‡éšæœºé‡‡æ ·æ¦‚å¿µè§£é‡Šå¯¹æ¥æ„å»ºæ–°çš„æ•°å­¦é—®é¢˜ï¼Œç¡®ä¿æ•°æ®ç‹¬ç«‹æ€§ã€‚</li>
<li>MathSmithé‡‡ç”¨ä¹ç§é¢„è®¾ç­–ç•¥æ¥å¢åŠ é—®é¢˜éš¾åº¦ï¼Œå¹¶å€ŸåŠ©å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç»“æ„æœ‰æ•ˆæ€§ã€æ¨ç†å¤æ‚æ€§å’Œç­”æ¡ˆä¸€è‡´æ€§ã€‚</li>
<li>MathSmithç”Ÿæˆçš„æ¨ç†è½¨è¿¹é•¿åº¦åæ˜ äº†è®¤çŸ¥å¤æ‚æ€§ï¼Œé¼“åŠ±åˆ›å»ºä¸é•¿é“¾æ€ç»´æ¨ç†å¯¹é½çš„æ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMathSmithåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬å®¹æ˜“å’Œä¸­ç­‰éš¾åº¦çš„GSM8Kå’ŒMATH-500ï¼Œä»¥åŠé«˜éš¾åº¦çš„AIME2024ã€AIME2025å’ŒOlympiadBenchã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-74c9e73c00a8575183f21ffcb4d9f343.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9636410a6ac706d604480825f40f636a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47117c257600b3dc307d1807363174d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25a56c6761c0480b91ef3be33a13768d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="The-World-According-to-LLMs-How-Geographic-Origin-Influences-LLMsâ€™-Entity-Deduction-Capabilities"><a href="#The-World-According-to-LLMs-How-Geographic-Origin-Influences-LLMsâ€™-Entity-Deduction-Capabilities" class="headerlink" title="The World According to LLMs: How Geographic Origin Influences LLMsâ€™   Entity Deduction Capabilities"></a>The World According to LLMs: How Geographic Origin Influences LLMsâ€™   Entity Deduction Capabilities</h2><p><strong>Authors:Harsh Nishant Lalai, Raj Sanjay Shah, Jiaxin Pei, Sashank Varma, Yi-Chia Wang, Ali Emami</strong></p>
<p>Large Language Models (LLMs) have been extensively tuned to mitigate explicit biases, yet they often exhibit subtle implicit biases rooted in their pre-training data. Rather than directly probing LLMs with human-crafted questions that may trigger guardrails, we propose studying how models behave when they proactively ask questions themselves. The 20 Questions game, a multi-turn deduction task, serves as an ideal testbed for this purpose. We systematically evaluate geographic performance disparities in entity deduction using a new dataset, Geo20Q+, consisting of both notable people and culturally significant objects (e.g., foods, landmarks, animals) from diverse regions. We test popular LLMs across two gameplay configurations (canonical 20-question and unlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese, French, Spanish, and Turkish). Our results reveal geographic disparities: LLMs are substantially more successful at deducing entities from the Global North than the Global South, and the Global West than the Global East. While Wikipedia pageviews and pre-training corpus frequency correlate mildly with performance, they fail to fully explain these disparities. Notably, the language in which the game is played has minimal impact on performance gaps. These findings demonstrate the value of creative, free-form evaluation frameworks for uncovering subtle biases in LLMs that remain hidden in standard prompting setups. By analyzing how models initiate and pursue reasoning goals over multiple turns, we find geographic and cultural disparities embedded in their reasoning processes. We release the dataset (Geo20Q+) and code at <a target="_blank" rel="noopener" href="https://sites.google.com/view/llmbias20q/home">https://sites.google.com/view/llmbias20q/home</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»è¿›è¡Œäº†å¹¿æ³›çš„è°ƒæ•´ï¼Œä»¥å‡è½»æ˜ç¡®çš„åè§ï¼Œç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸è¡¨ç°å‡ºæºäºå…¶é¢„è®­ç»ƒæ•°æ®çš„å¾®å¦™éšå«åè§ã€‚æˆ‘ä»¬å¹¶ä¸ä¸»å¼ ç”¨å¯èƒ½è§¦å‘é˜²æŠ¤æœºåˆ¶çš„ç”±äººç±»è®¾è®¡çš„é—®é¢˜ç›´æ¥æ¢æµ‹LLMï¼Œè€Œæ˜¯æè®®ç ”ç©¶æ¨¡å‹åœ¨ä¸»åŠ¨æå‡ºé—®é¢˜æ—¶çš„è¡Œä¸ºè¡¨ç°ã€‚20é—®æ¸¸æˆä½œä¸ºä¸€ç§å¤šå›åˆæ¨ç†ä»»åŠ¡ï¼Œæ˜¯è¾¾åˆ°è¿™ä¸€ç›®çš„çš„ç»ä½³æµ‹è¯•å¹³å°ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæ–°çš„æ•°æ®é›†Geo20Q+ï¼Œå®ƒåŒ…å«æ¥è‡ªä¸åŒåœ°åŒºçš„è‘—åäººç‰©å’Œæ–‡åŒ–é‡è¦å¯¹è±¡ï¼ˆä¾‹å¦‚é£Ÿç‰©ã€åœ°æ ‡ã€åŠ¨ç‰©ï¼‰ï¼Œç³»ç»Ÿåœ°è¯„ä¼°å®ä½“æ¨ç†ä¸­çš„åœ°ç†è¡¨ç°å·®å¼‚ã€‚æˆ‘ä»¬åœ¨ä¸¤ç§æ¸¸æˆè®¾ç½®ï¼ˆæ ‡å‡†çš„20ä¸ªé—®é¢˜å’Œæ— é™åˆ¶å›åˆï¼‰å’Œä¸ƒç§è¯­è¨€ï¼ˆè‹±è¯­ã€å°åœ°è¯­ã€æ™®é€šè¯ã€æ—¥è¯­ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­å’ŒåœŸè€³å…¶è¯­ï¼‰ä¸­å¯¹æµè¡Œçš„LLMè¿›è¡Œäº†æµ‹è¯•ã€‚æˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºäº†åœ°ç†å·®å¼‚ï¼šLLMåœ¨æ¨æ–­æ¥è‡ªå…¨çƒåŒ—æ–¹å’ŒåŒ—è¥¿æ–¹çš„å®ä½“æ—¶æ¯”æ¥è‡ªå…¨çƒå—æ–¹å’Œä¸œéƒ¨çš„å®ä½“æ›´æˆåŠŸã€‚è™½ç„¶Wikipediaé¡µé¢æµè§ˆé‡å’Œé¢„è®­ç»ƒè¯­æ–™åº“é¢‘ç‡ä¸æ€§èƒ½æœ‰è½»å¾®å…³è”ï¼Œä½†å®ƒä»¬æœªèƒ½å®Œå…¨è§£é‡Šè¿™äº›å·®å¼‚ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ¸¸æˆæ‰€ç”¨çš„è¯­è¨€å¯¹æ€§èƒ½å·®è·çš„å½±å“å¾®ä¹å…¶å¾®ã€‚è¿™äº›å‘ç°è¯æ˜äº†åˆ›é€ æ€§è‡ªç”±å½¢å¼çš„è¯„ä¼°æ¡†æ¶åœ¨æ­ç¤ºLLMä¸­éšè—ç»†å¾®åè§æ–¹é¢çš„ä»·å€¼ï¼Œè¿™äº›åè§åœ¨æ ‡å‡†æç¤ºè®¾ç½®ä¸­ä»ç„¶éšè—ã€‚é€šè¿‡åˆ†ææ¨¡å‹å¦‚ä½•åœ¨å¤šå›åˆä¸­å¯åŠ¨å’Œè¿½æ±‚æ¨ç†ç›®æ ‡ï¼Œæˆ‘ä»¬å‘ç°å…¶æ¨ç†è¿‡ç¨‹ä¸­åµŒå…¥çš„åœ°ç†å’Œæ–‡åŒ–å·®å¼‚ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://sites.google.com/view/llmbias20q/home">https://sites.google.com/view/llmbias20q/home</a>å‘å¸ƒäº†æ•°æ®é›†ï¼ˆGeo20Q+ï¼‰å’Œä»£ç ã€‚</p>
</blockquote>
<hr>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05525v1">PDF</a> Conference on Language Modeling 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½ç„¶ç»è¿‡å¹¿æ³›è°ƒæ•´ä»¥å‡è½»æ˜¾æ€§åè§ï¼Œä½†å®ƒä»¬å¾€å¾€è¡¨ç°å‡ºæºäºé¢„è®­ç»ƒæ•°æ®çš„å¾®å¦™éšæ€§åè§ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ–¹æ³•ï¼Œå³é€šè¿‡æ¨¡å‹è‡ªèº«ä¸»åŠ¨æé—®çš„æ–¹å¼æ¥ç ”ç©¶å…¶è¡¨ç°ã€‚æˆ‘ä»¬ä»¥20é—®æ¸¸æˆï¼ˆä¸€ç§å¤šå›åˆæ¨ç†ä»»åŠ¡ï¼‰ä¸ºæµ‹è¯•å¹³å°ï¼Œä½¿ç”¨æ–°çš„æ•°æ®é›†Geo20Q+ï¼ŒåŒ…æ‹¬æ¥è‡ªä¸åŒåœ°åŒºçš„çŸ¥åäººç‰©å’Œå…·æœ‰æ–‡åŒ–æ„ä¹‰çš„äº‹ç‰©ï¼ˆå¦‚é£Ÿå“ã€åœ°æ ‡ã€åŠ¨ç‰©ï¼‰ã€‚æˆ‘ä»¬åœ¨ä¸¤ç§æ¸¸æˆé…ç½®ï¼ˆæ ‡å‡†çš„20ä¸ªé—®é¢˜åŠæ— é™å›åˆï¼‰å’Œä¸ƒç§è¯­è¨€ï¼ˆè‹±è¯­ã€å°åœ°è¯­ã€æ™®é€šè¯ã€æ—¥è¯­ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­å’ŒåœŸè€³å…¶è¯­ï¼‰ä¸­æµ‹è¯•äº†æµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ç»“æœå‘ç°åœ°ç†å·®å¼‚ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨æ–­å…¨çƒåŒ—æ–¹å’ŒåŒ—è¥¿æ–¹çš„å®ä½“æ—¶æ¯”å…¨çƒå—æ–¹å’Œä¸œæ–¹æ›´æˆåŠŸã€‚å°½ç®¡ç»´åŸºç™¾ç§‘é¡µé¢æµè§ˆé‡å’Œé¢„è®­ç»ƒè¯­æ–™åº“é¢‘ç‡ä¸æ€§èƒ½è½»åº¦ç›¸å…³ï¼Œä½†å®ƒä»¬æ— æ³•å®Œå…¨è§£é‡Šè¿™äº›å·®å¼‚ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ¸¸æˆæ‰€ç”¨çš„è¯­è¨€å¯¹æ€§èƒ½å·®è·çš„å½±å“å¾®ä¹å…¶å¾®ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåˆ›é€ æ€§çš„è‡ªç”±å½¢å¼è¯„ä¼°æ¡†æ¶å¯¹äºæ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¾®å¦™åè§å…·æœ‰é‡è¦æ„ä¹‰ï¼Œè¿™äº›åè§åœ¨æ ‡å‡†æç¤ºè®¾ç½®ä¸­ä»ç„¶éšè—ã€‚é€šè¿‡åˆ†ææ¨¡å‹å¦‚ä½•å¯åŠ¨å’Œè¿½æ±‚å¤šå›åˆçš„æ¨ç†ç›®æ ‡ï¼Œæˆ‘ä»¬å‘ç°å…¶æ¨ç†è¿‡ç¨‹ä¸­åµŒå…¥çš„åœ°ç†å’Œæ–‡åŒ–å·®å¼‚ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://sites.google.com/view/llmbias20q/home">https://sites.google.com/view/llmbias20q/home</a>å‘å¸ƒäº†æ•°æ®é›†ï¼ˆGeo20Q+ï¼‰å’Œä»£ç ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å±•ç°å‡ºåŸºäºé¢„è®­ç»ƒæ•°æ®çš„å¾®å¦™éšæ€§åè§ã€‚</li>
<li>é€šè¿‡æ¨¡å‹ä¸»åŠ¨æé—®æ¥ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹çš„è¡¨ç°æ˜¯ä¸€ç§æ–°é¢–ä¸”æœ‰æ•ˆçš„æ–¹æ³•ã€‚</li>
<li>20é—®æ¸¸æˆæ˜¯å¤šå›åˆæ¨ç†ä»»åŠ¡çš„ç†æƒ³æµ‹è¯•å¹³å°ã€‚</li>
<li>Geo20Q+æ•°æ®é›†åŒ…å«æ¥è‡ªä¸åŒåœ°åŒºçš„çŸ¥åäººç‰©å’Œæ–‡åŒ–ç‰©å“ï¼Œç”¨äºè¯„ä¼°åœ°ç†æ€§èƒ½å·®å¼‚ã€‚</li>
<li>LLMsåœ¨æ¨æ–­å…¨çƒåŒ—æ–¹å’ŒåŒ—è¥¿æ–¹çš„å®ä½“æ—¶è¡¨ç°æ›´å¥½ï¼Œè€Œéå…¨çƒå—æ–¹å’Œä¸œæ–¹ã€‚</li>
<li>ç»´åŸºç™¾ç§‘é¡µé¢æµè§ˆé‡å’Œé¢„è®­ç»ƒè¯­æ–™åº“é¢‘ç‡ä¸LLMsæ€§èƒ½è½»åº¦ç›¸å…³ï¼Œä½†æ— æ³•å®Œå…¨è§£é‡Šæ€§èƒ½å·®å¼‚ã€‚</li>
<li>æ¸¸æˆè¯­è¨€å¯¹LLMsæ€§èƒ½å·®è·çš„å½±å“è¾ƒå°ï¼Œå¼ºè°ƒè¯„ä¼°æ¡†æ¶çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05525">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b7d7a3391cccaf19c6994d79d33307e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f31249bae350730fe3f9e0892c954ae.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="GRAIL-Learning-to-Interact-with-Large-Knowledge-Graphs-for-Retrieval-Augmented-Reasoning"><a href="#GRAIL-Learning-to-Interact-with-Large-Knowledge-Graphs-for-Retrieval-Augmented-Reasoning" class="headerlink" title="GRAIL:Learning to Interact with Large Knowledge Graphs for Retrieval   Augmented Reasoning"></a>GRAIL:Learning to Interact with Large Knowledge Graphs for Retrieval   Augmented Reasoning</h2><p><strong>Authors:Ge Chang, Jinbo Su, Jiacheng Liu, Pengfei Yang, Yuhao Shang, Huiwen Zheng, Hongli Ma, Yan Liang, Yuanchun Li, Yunxin Liu</strong></p>
<p>Large Language Models (LLMs) integrated with Retrieval-Augmented Generation (RAG) techniques have exhibited remarkable performance across a wide range of domains. However, existing RAG approaches primarily operate on unstructured data and demonstrate limited capability in handling structured knowledge such as knowledge graphs. Meanwhile, current graph retrieval methods fundamentally struggle to capture holistic graph structures while simultaneously facing precision control challenges that manifest as either critical information gaps or excessive redundant connections, collectively undermining reasoning performance. To address this challenge, we propose GRAIL: Graph-Retrieval Augmented Interactive Learning, a framework designed to interact with large-scale graphs for retrieval-augmented reasoning. Specifically, GRAIL integrates LLM-guided random exploration with path filtering to establish a data synthesis pipeline, where a fine-grained reasoning trajectory is automatically generated for each task. Based on the synthesized data, we then employ a two-stage training process to learn a policy that dynamically decides the optimal actions at each reasoning step. The overall objective of precision-conciseness balance in graph retrieval is decoupled into fine-grained process-supervised rewards to enhance data efficiency and training stability. In practical deployment, GRAIL adopts an interactive retrieval paradigm, enabling the model to autonomously explore graph paths while dynamically balancing retrieval breadth and precision. Extensive experiments have shown that GRAIL achieves an average accuracy improvement of 21.01% and F1 improvement of 22.43% on three knowledge graph question-answering datasets. Our source code and datasets is available at <a target="_blank" rel="noopener" href="https://github.com/Changgeww/GRAIL">https://github.com/Changgeww/GRAIL</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯çš„ç»“åˆå·²åœ¨å¤šä¸ªé¢†åŸŸå±•ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RAGæ–¹æ³•ä¸»è¦æ“ä½œäºéç»“æ„åŒ–æ•°æ®ï¼Œåœ¨å¤„ç†ç»“æ„åŒ–çŸ¥è¯†ï¼Œå¦‚çŸ¥è¯†å›¾è°±æ–¹é¢çš„èƒ½åŠ›æœ‰é™ã€‚åŒæ—¶ï¼Œå½“å‰çš„å›¾æ£€ç´¢æ–¹æ³•ä»æ ¹æœ¬ä¸Šéš¾ä»¥æ•æ‰æ•´ä½“å›¾ç»“æ„ï¼ŒåŒæ—¶é¢ä¸´ç²¾åº¦æ§åˆ¶æŒ‘æˆ˜ï¼Œè¡¨ç°ä¸ºå…³é”®ä¿¡æ¯ç¼ºå¤±æˆ–è¿‡å¤šå†—ä½™è¿æ¥ï¼Œå…±åŒå½±å“æ¨ç†æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†GRAILï¼šå›¾æ£€ç´¢å¢å¼ºäº¤äº’å­¦ä¹ ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨ä¸å¤§è§„æ¨¡å›¾è¿›è¡Œäº¤äº’ä»¥è¿›è¡Œæ£€ç´¢å¢å¼ºæ¨ç†çš„æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼ŒGRAILå°†LLMå¼•å¯¼çš„éšæœºæ¢ç´¢ä¸è·¯å¾„è¿‡æ»¤ç›¸ç»“åˆï¼Œå»ºç«‹æ•°æ®åˆæˆç®¡é“ï¼Œä¸ºæ¯ä¸ªä»»åŠ¡è‡ªåŠ¨ç”Ÿæˆç²¾ç»†çš„æ¨ç†è½¨è¿¹ã€‚åŸºäºåˆæˆæ•°æ®ï¼Œç„¶åæˆ‘ä»¬é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹æ¥å­¦ä¹ ä¸€ä¸ªç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯åœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤ä¸­åŠ¨æ€å†³å®šæœ€ä½³è¡ŒåŠ¨ã€‚å›¾æ£€ç´¢ä¸­ç²¾åº¦ç®€æ´æ€§å¹³è¡¡çš„æ€»ä½“ç›®æ ‡è¢«åˆ†è§£ä¸ºç²¾ç»†çš„è¿‡ç¨‹ç›‘ç£å¥–åŠ±ï¼Œä»¥æé«˜æ•°æ®æ•ˆç‡å’Œè®­ç»ƒç¨³å®šæ€§ã€‚åœ¨å®é™…éƒ¨ç½²ä¸­ï¼ŒGRAILé‡‡ç”¨äº¤äº’å¼æ£€ç´¢èŒƒå¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»æ¢ç´¢å›¾å½¢è·¯å¾„ï¼ŒåŒæ—¶åŠ¨æ€å¹³è¡¡æ£€ç´¢å¹¿åº¦å’Œç²¾åº¦ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGRAILåœ¨ä¸‰ä¸ªçŸ¥è¯†å›¾è°±é—®ç­”æ•°æ®é›†ä¸Šå¹³å‡å‡†ç¡®ç‡æé«˜äº†21.01%ï¼ŒF1åˆ†æ•°æé«˜äº†22.43%ã€‚æˆ‘ä»¬çš„æºä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Changgeww/GRAIL">https://github.com/Changgeww/GRAIL</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05498v1">PDF</a> 9 pages,3 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ç»“åˆï¼Œåœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰RAGæ–¹æ³•ä¸»è¦å¤„ç†éç»“æ„åŒ–æ•°æ®ï¼Œå¤„ç†ç»“æ„åŒ–çŸ¥è¯†ï¼ˆå¦‚çŸ¥è¯†å›¾è°±ï¼‰çš„èƒ½åŠ›æœ‰é™ã€‚å½“å‰å›¾æ£€ç´¢æ–¹æ³•éš¾ä»¥æ•æ‰æ•´ä½“å›¾ç»“æ„ï¼ŒåŒæ—¶é¢ä¸´ç²¾åº¦æ§åˆ¶æŒ‘æˆ˜ï¼Œè¡¨ç°ä¸ºå…³é”®ä¿¡æ¯ç¼ºå¤±æˆ–è¿‡å¤šå†—ä½™è¿æ¥ï¼Œå½±å“äº†æ¨ç†æ€§èƒ½ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæå‡ºGRAILï¼šå›¾æ£€ç´¢å¢å¼ºäº¤äº’å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºä¸å¤§è§„æ¨¡å›¾è¿›è¡Œæ£€ç´¢å¢å¼ºæ¨ç†çš„äº¤äº’ã€‚GRAILç»“åˆLLMå¼•å¯¼éšæœºæ¢ç´¢ä¸è·¯å¾„è¿‡æ»¤ï¼Œå»ºç«‹æ•°æ®åˆæˆç®¡é“ï¼Œä¸ºæ¯é¡¹ä»»åŠ¡è‡ªåŠ¨ç”Ÿæˆç²¾ç»†æ¨ç†è½¨è¿¹ã€‚åŸºäºåˆæˆæ•°æ®ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹å­¦ä¹ ç­–ç•¥ï¼ŒåŠ¨æ€å†³å®šæ¯ä¸ªæ¨ç†æ­¥éª¤çš„æœ€ä½³è¡ŒåŠ¨ã€‚ç²¾åº¦ç®€æ´å¹³è¡¡çš„å›¾æ£€ç´¢æ€»ä½“ç›®æ ‡è¢«åˆ†è§£ä¸ºç²¾ç»†è¿‡ç¨‹ç›‘ç£å¥–åŠ±ï¼Œæé«˜æ•°æ®æ•ˆç‡å’Œè®­ç»ƒç¨³å®šæ€§ã€‚å®é™…åº”ç”¨ä¸­ï¼ŒGRAILé‡‡ç”¨äº¤äº’å¼æ£€ç´¢èŒƒå¼ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»æ¢ç´¢å›¾è·¯å¾„ï¼ŒåŒæ—¶åŠ¨æ€å¹³è¡¡æ£€ç´¢å¹¿åº¦å’Œç²¾åº¦ã€‚åœ¨ä¸‰ä¸ªçŸ¥è¯†å›¾è°±é—®ç­”æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGRAILå¹³å‡å‡†ç¡®ç‡æé«˜21.01%ï¼ŒF1å€¼æé«˜22.43%ã€‚æºä»£ç å’Œæ•°æ®é›†å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/Changgeww/GRAIL%E3%80%82">https://github.com/Changgeww/GRAILã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMsä¸RAGç»“åˆåœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å¤„ç†ç»“æ„åŒ–çŸ¥è¯†ï¼ˆå¦‚çŸ¥è¯†å›¾è°±ï¼‰æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å½“å‰å›¾æ£€ç´¢æ–¹æ³•éš¾ä»¥æ•æ‰å›¾çš„å®Œæ•´ç»“æ„ï¼Œé¢ä¸´ç²¾åº¦æ§åˆ¶æŒ‘æˆ˜ã€‚</li>
<li>GRAILæ¡†æ¶é€šè¿‡LLMå¼•å¯¼çš„éšæœºæ¢ç´¢å’Œè·¯å¾„è¿‡æ»¤æ¥å¼ºåŒ–å›¾æ£€ç´¢ã€‚</li>
<li>GRAILé‡‡ç”¨æ•°æ®åˆæˆç®¡é“è‡ªåŠ¨ç”Ÿæˆç²¾ç»†æ¨ç†è½¨è¿¹ã€‚</li>
<li>ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ç”¨äºå­¦ä¹ ç­–ç•¥ï¼Œå¹³è¡¡æ£€ç´¢çš„ç²¾åº¦å’Œç®€æ´æ€§ã€‚</li>
<li>ç²¾åº¦ç®€æ´å¹³è¡¡çš„å›¾æ£€ç´¢ç›®æ ‡é€šè¿‡ç²¾ç»†è¿‡ç¨‹ç›‘ç£å¥–åŠ±æ¥æé«˜æ•°æ®æ•ˆç‡å’Œè®­ç»ƒç¨³å®šæ€§ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒGRAILåœ¨çŸ¥è¯†å›¾è°±é—®ç­”æ•°æ®é›†ä¸Šçš„è¡¨ç°æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05498">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-61a927bc615ccbe594fb230ce79701b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-746faf4ef26e62c3f781f8799a51e9c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aca512179cb48e9adca3cd2cfd9a7d0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd11d2975c8844d4a04270207fc7d614.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-Human-Centric-Evaluation-of-Interaction-Aware-Automated-Vehicle-Controllers-A-Framework-and-Case-Study"><a href="#Towards-Human-Centric-Evaluation-of-Interaction-Aware-Automated-Vehicle-Controllers-A-Framework-and-Case-Study" class="headerlink" title="Towards Human-Centric Evaluation of Interaction-Aware Automated Vehicle   Controllers: A Framework and Case Study"></a>Towards Human-Centric Evaluation of Interaction-Aware Automated Vehicle   Controllers: A Framework and Case Study</h2><p><strong>Authors:Federico ScarÃ¬, Olger Siebinga, Arkady Zgonnikov</strong></p>
<p>As automated vehicles (AVs) increasingly integrate into mixed-traffic environments, evaluating their interaction with human-driven vehicles (HDVs) becomes critical. In most research focused on developing new AV control algorithms (controllers), the performance of these algorithms is assessed solely based on performance metrics such as collision avoidance or lane-keeping efficiency, while largely overlooking the human-centred dimensions of interaction with HDVs. This paper proposes a structured evaluation framework that addresses this gap by incorporating metrics grounded in the human-robot interaction literature. The framework spans four key domains: a) interaction effect, b) interaction perception, c) interaction effort, and d) interaction ability. These domains capture both the performance of the AV and its impact on human drivers around it. To demonstrate the utility of the framework, we apply it to a case study evaluating how a state-of-the-art AV controller interacts with human drivers in a merging scenario in a driving simulator. Measuring HDV-HDV interactions as a baseline, this study included one representative metric per domain: a) perceived safety, b) subjective ratings, specifically how participants perceived the other vehicleâ€™s driving behaviour (e.g., aggressiveness or predictability) , c) driver workload, and d) merging success. The results showed that incorporating metrics covering all four domains in the evaluation of AV controllers can illuminate critical differences in driver experience when interacting with AVs. This highlights the need for a more comprehensive evaluation approach. Our framework offers researchers, developers, and policymakers a systematic method for assessing AV behaviour beyond technical performance, fostering the development of AVs that are not only functionally capable but also understandable, acceptable, and safe from a human perspective. </p>
<blockquote>
<p>éšç€è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆAVsï¼‰è¶Šæ¥è¶Šå¤šåœ°èå…¥åˆ°æ··åˆäº¤é€šç¯å¢ƒä¸­ï¼Œè¯„ä¼°å®ƒä»¬ä¸äººç±»é©¾é©¶è½¦è¾†ï¼ˆHDVsï¼‰çš„äº’åŠ¨å˜å¾—è‡³å…³é‡è¦ã€‚åœ¨å¤§å¤šæ•°ä¸“æ³¨äºå¼€å‘æ–°çš„AVæ§åˆ¶ç®—æ³•ï¼ˆæ§åˆ¶å™¨ï¼‰çš„ç ”ç©¶ä¸­ï¼Œè¿™äº›ç®—æ³•çš„æ€§èƒ½ä»…åŸºäºå¦‚é¿å…ç¢°æ’æˆ–ä¿æŒè½¦é“æ•ˆç‡ç­‰æ€§èƒ½æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼Œè€Œå¾ˆå¤§ç¨‹åº¦ä¸Šå¿½è§†äº†ä¸HDVäº’åŠ¨çš„äººæ€§åŒ–ç»´åº¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»“æ„åŒ–çš„è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡èå…¥åŸºäºäººæœºäº’åŠ¨æ–‡çŒ®çš„æŒ‡æ ‡æ¥è§£å†³è¿™ä¸€ç©ºç™½ã€‚è¯¥æ¡†æ¶æ¶µç›–äº†å››ä¸ªå…³é”®é¢†åŸŸï¼ša)äº’åŠ¨æ•ˆæœï¼Œb)äº’åŠ¨æ„ŸçŸ¥ï¼Œc)äº’åŠ¨åŠªåŠ›ï¼Œd)äº’åŠ¨èƒ½åŠ›ã€‚è¿™äº›é¢†åŸŸæ—¢æ¶µç›–äº†AVçš„æ€§èƒ½ï¼Œä¹Ÿæ¶µç›–äº†å…¶å¯¹å‘¨å›´äººç±»é©¾é©¶å‘˜çš„å½±å“ã€‚ä¸ºäº†è¯æ˜æ¡†æ¶çš„å®ç”¨æ€§ï¼Œæˆ‘ä»¬å°†å…¶åº”ç”¨äºä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œè¯„ä¼°æœ€å…ˆè¿›çš„AVæ§åˆ¶å™¨åœ¨é©¾é©¶æ¨¡æ‹Ÿå™¨ä¸­çš„åˆå¹¶åœºæ™¯ä¸­ä¸äººç±»é©¾é©¶å‘˜çš„äº’åŠ¨æƒ…å†µã€‚ä»¥HDV-HDVäº’åŠ¨ä¸ºåŸºå‡†çº¿ï¼Œæœ¬ç ”ç©¶åœ¨æ¯ä¸ªé¢†åŸŸéƒ½åŒ…æ‹¬äº†ä¸€ä¸ªä»£è¡¨æ€§çš„æŒ‡æ ‡ï¼ša)æ„ŸçŸ¥å®‰å…¨ï¼Œb)ä¸»è§‚è¯„ä»·ï¼Œç‰¹åˆ«æ˜¯å‚ä¸è€…å¦‚ä½•æ„ŸçŸ¥å¦ä¸€è¾†è½¦çš„é©¾é©¶è¡Œä¸ºï¼ˆå¦‚ä¾µç•¥æ€§æˆ–å¯é¢„æµ‹æ€§ï¼‰ï¼Œc)é©¾é©¶å‘˜å·¥ä½œé‡ï¼Œd)åˆå¹¶æˆåŠŸã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨è¯„ä¼°AVæ§åˆ¶å™¨æ—¶èå…¥æ¶µç›–æ‰€æœ‰å››ä¸ªé¢†åŸŸçš„æŒ‡æ ‡å¯ä»¥æ­ç¤ºé©¾é©¶å‘˜ä¸AVäº’åŠ¨æ—¶çš„é‡è¦å·®å¼‚ã€‚è¿™å¼ºè°ƒäº†éœ€è¦ä¸€ç§æ›´å…¨é¢çš„è¯„ä¼°æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ºç ”ç©¶è€…ã€å¼€å‘è€…å’Œæ”¿ç­–åˆ¶å®šè€…æä¾›äº†ä¸€ç§ç³»ç»ŸåŒ–æ–¹æ³•ï¼Œä»¥è¯„ä¼°AVè¡Œä¸ºçš„æŠ€æœ¯æ€§èƒ½ä¹‹å¤–çš„æƒ…å†µï¼Œä¿ƒè¿›å¼€å‘ä¸ä»…åŠŸèƒ½å¼ºå¤§è€Œä¸”ä»äººç±»çš„è§’åº¦æ¥çœ‹ä¹Ÿæ˜“äºç†è§£ã€å¯æ¥å—å’Œå®‰å…¨çš„AVã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05497v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ä¸ªé’ˆå¯¹è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆAVsï¼‰ä¸äººä¸ºé©¾é©¶è½¦è¾†ï¼ˆHDVsï¼‰äº¤äº’çš„è¯„ä»·æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ¶µç›–äº†å››ä¸ªå…³é”®é¢†åŸŸï¼šäº¤äº’æ•ˆæœã€äº¤äº’æ„ŸçŸ¥ã€äº¤äº’åŠªåŠ›å’Œäº¤äº’èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡åº”ç”¨æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†å…¶å®ç”¨æ€§ï¼Œç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨è¯„ä¼°è‡ªåŠ¨é©¾é©¶è½¦è¾†æ§åˆ¶å™¨æ—¶ï¼Œæ¶µç›–è¿™å››ä¸ªé¢†åŸŸçš„æŒ‡æ ‡èƒ½å¤Ÿæ­ç¤ºé©¾é©¶è€…åœ¨ä¸è‡ªåŠ¨é©¾é©¶è½¦è¾†äº¤äº’æ—¶çš„å…³é”®ä½“éªŒå·®å¼‚ã€‚è¿™å¼ºè°ƒäº†éœ€è¦æ›´å…¨é¢ã€ç³»ç»Ÿçš„è¯„ä¼°æ–¹æ³•ã€‚æœ¬æ–‡æœ‰åŠ©äºç ”ç©¶äººå‘˜ã€å¼€å‘è€…å’Œæ”¿ç­–åˆ¶å®šè€…ä»äººç±»è§†è§’è¯„ä¼°è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„è¡Œä¸ºï¼Œæ¨åŠ¨è‡ªåŠ¨é©¾é©¶è½¦è¾†ä¸ä»…åœ¨åŠŸèƒ½ä¸Šå…·å¤‡èƒ½åŠ›ï¼Œè€Œä¸”åœ¨äººç±»ç†è§£ã€æ¥å—å’Œå®‰å…¨æ–¹é¢ä¹Ÿèƒ½å¾—åˆ°ä¿éšœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆAVsï¼‰ä¸äººä¸ºé©¾é©¶è½¦è¾†ï¼ˆHDVsï¼‰çš„äº¤äº’è¯„ä¼°è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ç ”ç©¶åœ¨è¯„ä¼°AVæ§åˆ¶ç®—æ³•æ—¶ï¼Œä¸»è¦åŸºäºç¢°æ’é¿å…å’Œè½¦é“ä¿æŒæ•ˆç‡ç­‰æ€§èƒ½æŒ‡æ ‡ï¼Œå¿½ç•¥äº†ä¸äººç±»é©¾é©¶è€…äº¤äº’çš„äººä¸ºä¸­å¿ƒç»´åº¦ã€‚</li>
<li>æå‡ºçš„è¯„ä»·æ¡†æ¶æ¶µç›–äº†å››ä¸ªå…³é”®é¢†åŸŸï¼šäº¤äº’æ•ˆæœã€äº¤äº’æ„ŸçŸ¥ã€äº¤äº’åŠªåŠ›å’Œäº¤äº’èƒ½åŠ›ï¼Œä»¥å…¨é¢è¯„ä¼°AVä¸HDVçš„äº¤äº’ã€‚</li>
<li>é€šè¿‡æ¡ˆä¾‹ç ”ç©¶ï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶çš„å®ç”¨æ€§ï¼Œå¹¶æ­ç¤ºäº†ä»…åŸºäºæŠ€æœ¯æ€§èƒ½è¯„ä¼°AVæ§åˆ¶å™¨æ—¶çš„å…³é”®ç¼ºé™·ã€‚</li>
<li>æ¶µç›–å››ä¸ªé¢†åŸŸçš„æŒ‡æ ‡èƒ½å¤Ÿæ­ç¤ºé©¾é©¶è€…åœ¨ä¸è‡ªåŠ¨é©¾é©¶è½¦è¾†äº¤äº’æ—¶çš„ä½“éªŒå·®å¼‚ã€‚</li>
<li>éœ€è¦æ›´å…¨é¢ã€ç³»ç»Ÿçš„è¯„ä¼°æ–¹æ³•æ¥è¯„ä¼°è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„è¡Œä¸ºï¼Œä»¥ç¡®ä¿å…¶ä¸ä»…åŠŸèƒ½å¼ºå¤§ï¼Œè€Œä¸”ä»äººç±»è§†è§’æ¥çœ‹æ˜“äºç†è§£ã€æ¥å—å’Œå®‰å…¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05497">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-663c5b564c250dd08175b7a401920185.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce84ef91ae57c98d3c907fec269f2a24.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="InfiAlign-A-Scalable-and-Sample-Efficient-Framework-for-Aligning-LLMs-to-Enhance-Reasoning-Capabilities"><a href="#InfiAlign-A-Scalable-and-Sample-Efficient-Framework-for-Aligning-LLMs-to-Enhance-Reasoning-Capabilities" class="headerlink" title="InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs   to Enhance Reasoning Capabilities"></a>InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs   to Enhance Reasoning Capabilities</h2><p><strong>Authors:Shuo Cai, Su Lu, Qi Zhou, Kejing Yang, Zhijie Sang, Congkai Xie, Hongxia Yang</strong></p>
<p>Large language models (LLMs) have exhibited impressive reasoning abilities on a wide range of complex tasks. However, enhancing these capabilities through post-training remains resource intensive, particularly in terms of data and computational cost. Although recent efforts have sought to improve sample efficiency through selective data curation, existing methods often rely on heuristic or task-specific strategies that hinder scalability. In this work, we introduce InfiAlign, a scalable and sample-efficient post-training framework that integrates supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to align LLMs for enhanced reasoning. At the core of InfiAlign is a robust data selection pipeline that automatically curates high-quality alignment data from open-source reasoning datasets using multidimensional quality metrics. This pipeline enables significant performance gains while drastically reducing data requirements and remains extensible to new data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model achieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only approximately 12% of the training data, and demonstrates strong generalization across diverse reasoning tasks. Additional improvements are obtained through the application of DPO, with particularly notable gains in mathematical reasoning tasks. The model achieves an average improvement of 3.89% on AIME 24&#x2F;25 benchmarks. Our results highlight the effectiveness of combining principled data selection with full-stage post-training, offering a practical solution for aligning large reasoning models in a scalable and data-efficient manner. The model checkpoints are available at <a target="_blank" rel="noopener" href="https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT">https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¹¿æ³›çš„å¤æ‚ä»»åŠ¡ä¸­å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œé€šè¿‡åæœŸè®­ç»ƒå¢å¼ºè¿™äº›èƒ½åŠ›ä»ç„¶æ˜¯èµ„æºå¯†é›†å‹çš„ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®å’Œè®¡ç®—æˆæœ¬æ–¹é¢ã€‚å°½ç®¡æœ€è¿‘çš„åŠªåŠ›è¯•å›¾é€šè¿‡é€‰æ‹©æ€§æ•°æ®æ”¶é›†æ¥æé«˜æ ·æœ¬æ•ˆç‡ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¯å‘å¼æˆ–ä»»åŠ¡ç‰¹å®šç­–ç•¥ï¼Œè¿™é˜»ç¢äº†å¯æ‰©å±•æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†InfiAlignï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•ä¸”æ ·æœ¬æ•ˆç‡é«˜çš„åæœŸè®­ç»ƒæ¡†æ¶ï¼Œå®ƒå°†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç›¸ç»“åˆï¼Œä»¥å¯¹LLMè¿›è¡Œå¢å¼ºæ¨ç†å¯¹é½ã€‚InfiAlignçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªç¨³å¥çš„æ•°æ®é€‰æ‹©ç®¡é“ï¼Œå®ƒä½¿ç”¨å¤šç»´è´¨é‡æŒ‡æ ‡è‡ªåŠ¨ä»å¼€æºæ¨ç†æ•°æ®é›†ä¸­ç­›é€‰é«˜è´¨é‡çš„å¯¹é½æ•°æ®ã€‚è¯¥ç®¡é“èƒ½å¤Ÿåœ¨å¤§å¹…é™ä½æ•°æ®éœ€æ±‚çš„åŒæ—¶å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶ä¸”å¯æ‰©å±•åˆ°æ–°çš„æ•°æ®æºã€‚å½“åº”ç”¨äºQwen2.5-Math-7B-Baseæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬çš„SFTæ¨¡å‹åœ¨æ€§èƒ½ä¸Šå¯ä»¥ä¸DeepSeek-R1-Distill-Qwen-7Bç›¸åª²ç¾ï¼ŒåŒæ—¶ä½¿ç”¨çš„æ•°æ®é‡ä»…çº¦ä¸º12%ï¼Œå¹¶ä¸”åœ¨å„ç§æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡åº”ç”¨DPOè¿˜è·å¾—äº†é¢å¤–çš„æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ”¶ç›Šã€‚è¯¥æ¨¡å‹åœ¨AIME 24&#x2F;25åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†3.89%çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†åŸåˆ™æ€§æ•°æ®é€‰æ‹©ä¸å…¨é˜¶æ®µåæœŸè®­ç»ƒç›¸ç»“åˆçš„æœ‰æ•ˆæ€§ï¼Œä¸ºä»¥å¯æ‰©å±•å’Œé«˜æ•ˆæ•°æ®çš„æ–¹å¼å¯¹é½å¤§å‹æ¨ç†æ¨¡å‹æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚è¯¥æ¨¡å‹æ£€æŸ¥ç‚¹ä½äºï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT%E3%80%82">https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05496v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¹¿æ³›å¤æ‚ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶èƒ½åŠ›å¢å¼ºé€šè¿‡è®­ç»ƒåä»ç„¶éœ€è¦å¤§é‡èµ„æºå’Œæ•°æ®è®¡ç®—æˆæœ¬ã€‚è¿‘æœŸå°½ç®¡æœ‰æ‰€åŠªåŠ›æ”¹å–„æ ·æœ¬æ•ˆç‡é€šè¿‡é€‰æ‹©æ€§æ•°æ®æ”¶é›†ï¼Œä½†ç°æœ‰æ–¹æ³•ç»å¸¸ä¾èµ–å¯å‘å¼æˆ–ä»»åŠ¡ç‰¹å®šç­–ç•¥é˜»ç¢å¯æ‰©å±•æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥InfiAlignï¼Œä¸€ä¸ªå¯æ‰©å±•ä¸”æ ·æœ¬é«˜æ•ˆçš„è®­ç»ƒåæ¡†æ¶ï¼Œèåˆäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œä»¥æé«˜LLMçš„æ¨ç†èƒ½åŠ›ã€‚InfiAlignçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªç¨³å¥çš„æ•°æ®é€‰æ‹©ç®¡é“ï¼Œå¯ä»å¼€æºæ¨ç†æ•°æ®é›†ä¸­è‡ªåŠ¨ç­›é€‰é«˜è´¨é‡å¯¹é½æ•°æ®ï¼Œä½¿ç”¨å¤šç»´è´¨é‡æŒ‡æ ‡ã€‚æ­¤ç®¡é“å¯åœ¨å¤§å¹…é™ä½æ•°æ®éœ€æ±‚çš„åŒæ—¶å®ç°æ˜¾è‘—æ€§èƒ½æå‡ï¼Œä¸”å¯¹æ–°æ•°æ®æºå…·æœ‰å¯æ‰©å±•æ€§ã€‚åº”ç”¨äºQwen2.5-Math-7B-Baseæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬çš„SFTæ¨¡å‹æ€§èƒ½ä¸DeepSeek-R1-Distill-Qwen-7Bç›¸å½“ï¼Œä»…ä½¿ç”¨çº¦12%çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶åœ¨å„ç§æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡åº”ç”¨DPOå¯è·å¾—é¢å¤–æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ã€‚æ¨¡å‹åœ¨AIME 24&#x2F;25åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡3.89%ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†ç»“åˆåŸåˆ™æ€§æ•°æ®é€‰æ‹©ä¸å…¨é˜¶æ®µè®­ç»ƒåå¯¹é½çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ï¼Œä¸ºå¤§è§„æ¨¡å’Œæ•°æ®é«˜æ•ˆçš„æ–¹å¼å¯¹é½å¤§å‹æ¨ç†æ¨¡å‹æä¾›äº†å®é™…è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>InfiAlignæ˜¯ä¸€ä¸ªç”¨äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„è®­ç»ƒåæ¡†æ¶ã€‚</li>
<li>å®ƒç»“åˆäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ã€‚</li>
<li>InfiAlignå…·æœ‰å¯æ‰©å±•æ€§å’Œæ ·æœ¬æ•ˆç‡ï¼Œé€šè¿‡è‡ªåŠ¨ç­›é€‰é«˜è´¨é‡æ•°æ®é™ä½èµ„æºæ¶ˆè€—ã€‚</li>
<li>InfiAlignèƒ½æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ï¼ŒåŒæ—¶ä½¿ç”¨è¾ƒå°‘çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>åœ¨Qwen2.5-Math-7B-Baseæ¨¡å‹ä¸Šçš„å®éªŒè¯æ˜äº†InfiAlignçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>DPOçš„åº”ç”¨è¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šã€‚</li>
<li>æ¨¡å‹åœ¨AIME 24&#x2F;25åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05496">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-da11772466f6587f6d30ee7dda611478.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c78edf199c99d8896e84ad809032d734.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-61424eb16104928441f69ea88d1020bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f881567a937c418c05c82d987361977.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01e104709f782749ff8d392703e7c939.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="TASE-Token-Awareness-and-Structured-Evaluation-for-Multilingual-Language-Models"><a href="#TASE-Token-Awareness-and-Structured-Evaluation-for-Multilingual-Language-Models" class="headerlink" title="TASE: Token Awareness and Structured Evaluation for Multilingual   Language Models"></a>TASE: Token Awareness and Structured Evaluation for Multilingual   Language Models</h2><p><strong>Authors:Chenzhuo Zhao, Xinda Wang, Yue Huang, Junting Lu, Ziqian Liu</strong></p>
<p>While large language models (LLMs) have demonstrated remarkable performance on high-level semantic tasks, they often struggle with fine-grained, token-level understanding and structural reasoningâ€“capabilities that are essential for applications requiring precision and control. We introduce TASE, a comprehensive benchmark designed to evaluate LLMsâ€™ ability to perceive and reason about token-level information across languages. TASE covers 10 tasks under two core categories: token awareness and structural understanding, spanning Chinese, English, and Korean, with a 35,927-instance evaluation set and a scalable synthetic data generation pipeline for training. Tasks include character counting, token alignment, syntactic structure parsing, and length constraint satisfaction. We evaluate over 30 leading commercial and open-source LLMs, including O3, Claude 4, Gemini 2.5 Pro, and DeepSeek-R1, and train a custom Qwen2.5-14B model using the GRPO training method. Results show that human performance significantly outpaces current LLMs, revealing persistent weaknesses in token-level reasoning. TASE sheds light on these limitations and provides a new diagnostic lens for future improvements in low-level language understanding and cross-lingual generalization. Our code and dataset are publicly available at <a target="_blank" rel="noopener" href="https://github.com/cyzcz/Tase">https://github.com/cyzcz/Tase</a> . </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é«˜å±‚æ¬¡è¯­ä¹‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºäº†å‡ºè‰²çš„æ€§èƒ½ï¼Œä½†åœ¨ç²¾ç»†ç²’åº¦ã€æ ‡è®°çº§åˆ«çš„ç†è§£å’Œç»“æ„æ€§æ¨ç†æ–¹é¢å¸¸å¸¸é‡åˆ°å›°éš¾â€”â€”è¿™äº›èƒ½åŠ›å¯¹äºéœ€è¦ç²¾ç¡®æ€§å’Œæ§åˆ¶çš„åº”ç”¨æ¥è¯´æ˜¯å¿…ä¸å¯å°‘çš„ã€‚æˆ‘ä»¬ä»‹ç»äº†TASEï¼Œè¿™æ˜¯ä¸€ä¸ªç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMå¯¹è·¨è¯­è¨€çš„æ ‡è®°çº§ä¿¡æ¯çš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚TASEæ¶µç›–ä¸¤ä¸ªæ ¸å¿ƒç±»åˆ«ä¸‹çš„10é¡¹ä»»åŠ¡ï¼šæ ‡è®°æ„è¯†å’Œç»“æ„ç†è§£ï¼Œæ¶µç›–ä¸­æ–‡ã€è‹±æ–‡å’ŒéŸ©è¯­ï¼Œè¯„ä¼°é›†åŒ…å«35927ä¸ªå®ä¾‹ï¼Œå¹¶å…·å¤‡å¯æ‰©å±•çš„åˆæˆæ•°æ®ç”Ÿæˆç®¡é“ç”¨äºè®­ç»ƒã€‚ä»»åŠ¡åŒ…æ‹¬å­—ç¬¦è®¡æ•°ã€æ ‡è®°å¯¹é½ã€å¥æ³•ç»“æ„è§£æå’Œé•¿åº¦çº¦æŸæ»¡è¶³ç­‰ã€‚æˆ‘ä»¬è¯„ä¼°äº†30å¤šä¸ªé¢†å…ˆçš„å•†ä¸šå’Œå¼€æºLLMï¼ŒåŒ…æ‹¬O3ã€Claude 4ã€Gemini 2.5 Proå’ŒDeepSeek-R1ï¼Œå¹¶ä½¿ç”¨GRPOè®­ç»ƒæ–¹æ³•è®­ç»ƒäº†ä¸€ä¸ªå®šåˆ¶çš„Qwen2. 5-14Bæ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œäººç±»æ€§èƒ½æ˜¾è‘—è¶…è¿‡å½“å‰LLMï¼Œæ­ç¤ºäº†æ ‡è®°çº§æ¨ç†çš„æŒä¹…æ€§å¼±ç‚¹ã€‚TASEæ­ç¤ºäº†è¿™äº›é™åˆ¶ï¼Œå¹¶ä¸ºæœªæ¥åœ¨ä½çº§åˆ«è¯­è¨€ç†è§£å’Œè·¨è¯­è¨€æ³›åŒ–æ–¹é¢çš„æ”¹è¿›æä¾›äº†æ–°çš„è¯Šæ–­è§†è§’ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/cyzcz/Tase%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/cyzcz/Taseä¸Šå…¬å¼€è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05468v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é«˜å±‚æ¬¡è¯­ä¹‰ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºï¼Œä½†åœ¨ç²¾ç»†ç²’åº¦çš„tokençº§åˆ«ç†è§£å’Œç»“æ„æ€§æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºè¯„ä¼°LLMå¯¹tokençº§åˆ«ä¿¡æ¯çš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TASEç»¼åˆåŸºå‡†æµ‹è¯•ã€‚TASEæ¶µç›–10é¡¹ä»»åŠ¡ï¼Œåˆ†ä¸ºä¸¤ä¸ªæ ¸å¿ƒç±»åˆ«ï¼štokenæ„è¯†å’Œç»“æ„ç†è§£ï¼Œæ¶‰åŠä¸­æ–‡ã€è‹±æ–‡å’ŒéŸ©è¯­ã€‚æˆ‘ä»¬è¯„ä¼°äº†30å¤šç§é¢†å…ˆçš„å•†ä¸šå’Œå¼€æºLLMï¼Œå¹¶å®šåˆ¶äº†Qwen2.5-14Bæ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œäººç±»æ€§èƒ½æ˜¾è‘—ä¼˜äºå½“å‰LLMï¼Œæ­ç¤ºäº†å…¶åœ¨tokençº§åˆ«æ¨ç†æ–¹é¢çš„æŒç»­å¼±ç‚¹ã€‚TASEä¸ºæœªæ¥çš„ä½çº§åˆ«è¯­è¨€ç†è§£å’Œè·¨è¯­è¨€æ³›åŒ–æ”¹è¿›æä¾›äº†æ–°çš„è¯Šæ–­è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨tokençº§åˆ«ç†è§£å’Œç»“æ„æ€§æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>TASEæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°LLMçš„åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–tokenæ„è¯†å’Œç»“æ„ç†è§£ä¸¤ä¸ªæ ¸å¿ƒç±»åˆ«ã€‚</li>
<li>TASEåŒ…å«å¤šç§è¯­è¨€ï¼ˆä¸­æ–‡ã€è‹±æ–‡å’ŒéŸ©è¯­ï¼‰ï¼Œæ¶µç›–10é¡¹ä»»åŠ¡ã€‚</li>
<li>è¯„ä¼°äº†30å¤šç§LLMçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å•†ä¸šå’Œå¼€æºæ¨¡å‹ã€‚</li>
<li>å®šåˆ¶äº†Qwen2.5-14Bæ¨¡å‹å¹¶ä½¿ç”¨GRPOè®­ç»ƒæ–¹æ³•ã€‚</li>
<li>äººç±»æ€§èƒ½æ˜¾è‘—ä¼˜äºå½“å‰LLMåœ¨tokençº§åˆ«æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>TASEä¸ºæ”¹è¿›ä½çº§åˆ«è¯­è¨€ç†è§£å’Œè·¨è¯­è¨€æ³›åŒ–æä¾›äº†æ–°è§†è§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05468">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e7b8b87e9507092f9272ebdbe9d0a657.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41f7f11251091dc718888981730f202b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-02d7c7ef25850459f423db0735f01142.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-92e13ba35b58046df25203e41af3af85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c5d55cdde0c0ce7250f649d3d13a422.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DeepPHY-Benchmarking-Agentic-VLMs-on-Physical-Reasoning"><a href="#DeepPHY-Benchmarking-Agentic-VLMs-on-Physical-Reasoning" class="headerlink" title="DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning"></a>DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning</h2><p><strong>Authors:Xinrun Xu, Pi Bu, Ye Wang, BÃ¶rje F. Karlsson, Ziming Wang, Tengtao Song, Qi Zhu, Jun Song, Zhiming Ding, Bo Zheng</strong></p>
<p>Although Vision Language Models (VLMs) exhibit strong perceptual abilities and impressive visual reasoning, they struggle with attention to detail and precise action planning in complex, dynamic environments, leading to subpar performance. Real-world tasks typically require complex interactions, advanced spatial reasoning, long-term planning, and continuous strategy refinement, usually necessitating understanding the physics rules of the target scenario. However, evaluating these capabilities in real-world scenarios is often prohibitively expensive. To bridge this gap, we introduce DeepPHY, a novel benchmark framework designed to systematically evaluate VLMsâ€™ understanding and reasoning about fundamental physical principles through a series of challenging simulated environments. DeepPHY integrates multiple physical reasoning environments of varying difficulty levels and incorporates fine-grained evaluation metrics. Our evaluation finds that even state-of-the-art VLMs struggle to translate descriptive physical knowledge into precise, predictive control. </p>
<blockquote>
<p>å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å±•ç°å‡ºå¼ºå¤§çš„æ„ŸçŸ¥èƒ½åŠ›å’Œä»¤äººå°è±¡æ·±åˆ»çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å¤æ‚çš„åŠ¨æ€ç¯å¢ƒä¸­ï¼Œå®ƒä»¬åœ¨ç»†èŠ‚å…³æ³¨å’Œç²¾ç¡®è¡ŒåŠ¨è§„åˆ’æ–¹é¢é‡åˆ°å›°éš¾ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚ç°å®ä¸–ç•Œä»»åŠ¡é€šå¸¸éœ€è¦å¤æ‚çš„äº¤äº’ã€é«˜çº§çš„ç©ºé—´æ¨ç†ã€é•¿æœŸè§„åˆ’å’Œè¿ç»­çš„ç­–ç•¥æ”¹è¿›ï¼Œé€šå¸¸éœ€è¦ç†è§£ç›®æ ‡åœºæ™¯çš„ç‰©ç†è§„åˆ™ã€‚ç„¶è€Œï¼Œåœ¨çœŸå®åœºæ™¯ä¸­å¯¹è¿™äº›èƒ½åŠ›è¿›è¡Œè¯„ä¼°é€šå¸¸æˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†DeepPHYï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹åŸºå‡†æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸€ç³»åˆ—å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨¡æ‹Ÿç¯å¢ƒç³»ç»Ÿåœ°è¯„ä¼°VLMså¯¹åŸºæœ¬ç‰©ç†åŸç†çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚DeepPHYæ•´åˆäº†ä¸åŒéš¾åº¦çº§åˆ«çš„å¤šä¸ªç‰©ç†æ¨ç†ç¯å¢ƒï¼Œå¹¶é‡‡ç”¨äº†ç²¾ç»†çš„è¯„ä¼°æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„è¯„ä¼°å‘ç°ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„VLMsä¹Ÿéš¾ä»¥å°†æè¿°æ€§çš„ç‰©ç†çŸ¥è¯†è½¬åŒ–ä¸ºç²¾ç¡®ã€é¢„æµ‹æ€§çš„æ§åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05405v1">PDF</a> 48 pages</p>
<p><strong>Summary</strong></p>
<p>åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼Œå°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å±•ç°å‡ºå¼ºå¤§çš„æ„ŸçŸ¥èƒ½åŠ›å’Œä»¤äººå°è±¡æ·±åˆ»çš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­ï¼Œå®ƒä»¬å¯¹ç»†èŠ‚çš„å…³æ³¨å’Œç²¾ç¡®çš„è¡ŒåŠ¨è§„åˆ’èƒ½åŠ›ä»æœ‰å¾…æé«˜ã€‚çœŸå®ä¸–ç•Œä»»åŠ¡é€šå¸¸éœ€è¦å¤æ‚çš„äº¤äº’ã€é«˜çº§çš„ç©ºé—´æ¨ç†ã€é•¿æœŸè§„åˆ’å’Œè¿ç»­çš„ç­–ç•¥è°ƒæ•´ï¼Œé€šå¸¸éœ€è¦ç†è§£ç›®æ ‡åœºæ™¯çš„ç‰©ç†è§„åˆ™ã€‚ä¸ºäº†è¯„ä¼°è¿™äº›èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†DeepPHYè¿™ä¸€æ–°å‹åŸºå‡†æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¸€ç³»åˆ—å…·æœ‰æŒ‘æˆ˜æ€§çš„æ¨¡æ‹Ÿç¯å¢ƒç³»ç»Ÿåœ°è¯„ä¼°VLMså¯¹åŸºæœ¬ç‰©ç†åŸç†çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚è¯„ä¼°å‘ç°ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„VLMsä¹Ÿå¾ˆéš¾å°†æè¿°æ€§çš„ç‰©ç†çŸ¥è¯†è½¬åŒ–ä¸ºç²¾ç¡®ã€é¢„æµ‹æ€§çš„æ§åˆ¶åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨å¤æ‚åŠ¨æ€ç¯å¢ƒä¸­å­˜åœ¨ç»†èŠ‚å…³æ³¨å’Œç²¾ç¡®è¡ŒåŠ¨è§„åˆ’çš„æŒ‘æˆ˜ã€‚</li>
<li>çœŸå®ä¸–ç•Œä»»åŠ¡éœ€è¦å¤æ‚çš„äº¤äº’ã€é«˜çº§çš„ç©ºé—´æ¨ç†å’Œè¿ç»­çš„ç­–ç•¥è°ƒæ•´ã€‚</li>
<li>çœŸå®ä¸–ç•Œä»»åŠ¡çš„å®Œæˆé€šå¸¸éœ€è¦ç†è§£ç›®æ ‡åœºæ™¯çš„ç‰©ç†è§„åˆ™ã€‚</li>
<li>DeepPHYæ˜¯ä¸€ä¸ªæ–°å‹åŸºå‡†æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°VLMså¯¹ç‰©ç†åŸç†çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>DeepPHYé€šè¿‡æ¨¡æ‹Ÿç¯å¢ƒè¿›è¡Œç³»ç»ŸåŒ–è¯„ä¼°ï¼ŒåŒ…å«ä¸åŒéš¾åº¦çº§åˆ«çš„å¤šä¸ªç‰©ç†æ¨ç†ç¯å¢ƒã€‚</li>
<li>è¯„ä¼°å‘ç°ï¼ŒVLMsåœ¨å°†æè¿°æ€§ç‰©ç†çŸ¥è¯†è½¬åŒ–ä¸ºç²¾ç¡®ã€é¢„æµ‹æ€§çš„æ§åˆ¶åŠ›æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05405">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b7003d6885b8d5a5388f55cd992363dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70f5c706b1b93a92d25edaf7b6c0fb5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df3882b558bf672b7065cfe2142d4d37.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="StructVRM-Aligning-Multimodal-Reasoning-with-Structured-and-Verifiable-Reward-Models"><a href="#StructVRM-Aligning-Multimodal-Reasoning-with-Structured-and-Verifiable-Reward-Models" class="headerlink" title="StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable   Reward Models"></a>StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable   Reward Models</h2><p><strong>Authors:Xiangxiang Zhang, Jingxuan Wei, Donghong Zhong, Qi Chen, Caijun Jia, Cheng Tan, Jinming Gu, Xiaobo Qin, Zhiping Liu, Liang Hu, Tong Sun, Yuchen Wu, Zewei Sun, Chenwei Lou, Hua Zheng, Tianyang Zhan, Changbao Wang, Shuangzhi Wu, Zefa Lin, Chang Guo, Sihang Yuan, Riwei Chen, Shixiong Zhao, Yingping Zhang, Gaowei Wu, Bihui Yu, Jiahui Wu, Zhehui Zhao, Qianqian Liu, Ruofeng Tang, Xingyue Huang, Bing Zhao, Mengyang Zhang, Youqiang Zhou</strong></p>
<p>Existing Vision-Language Models often struggle with complex, multi-question reasoning tasks where partial correctness is crucial for effective learning. Traditional reward mechanisms, which provide a single binary score for an entire response, are too coarse to guide models through intricate problems with multiple sub-parts. To address this, we introduce StructVRM, a method that aligns multimodal reasoning with Structured and Verifiable Reward Models. At its core is a model-based verifier trained to provide fine-grained, sub-question-level feedback, assessing semantic and mathematical equivalence rather than relying on rigid string matching. This allows for nuanced, partial credit scoring in previously intractable problem formats. Extensive experiments demonstrate the effectiveness of StructVRM. Our trained model, Seed-StructVRM, achieves state-of-the-art performance on six out of twelve public multimodal benchmarks and our newly curated, high-difficulty STEM-Bench. The success of StructVRM validates that training with structured, verifiable rewards is a highly effective approach for advancing the capabilities of multimodal models in complex, real-world reasoning domains. </p>
<blockquote>
<p>ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚çš„å¤šé—®é¢˜æ¨ç†ä»»åŠ¡æ—¶å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™äº›ä»»åŠ¡ä¸­çš„éƒ¨åˆ†æ­£ç¡®æ€§å¯¹äºæœ‰æ•ˆå­¦ä¹ è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„å¥–åŠ±æœºåˆ¶ä¸ºæ•´ä¸ªå›åº”æä¾›ä¸€ä¸ªå•ä¸€çš„äºŒå…ƒåˆ†æ•°ï¼Œè¿™åœ¨æŒ‡å¯¼æ¨¡å‹è§£å†³å…·æœ‰å¤šä¸ªå­éƒ¨åˆ†çš„é—®é¢˜æ—¶è¿‡äºç²—ç³™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†StructVRMæ–¹æ³•ï¼Œå®ƒå°†å¤šæ¨¡æ€æ¨ç†ä¸ç»“æ„åŒ–å¯éªŒè¯å¥–åŠ±æ¨¡å‹å¯¹é½ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªåŸºäºæ¨¡å‹çš„éªŒè¯å™¨ï¼Œç»è¿‡è®­ç»ƒå¯æä¾›ç²¾ç»†ç²’åº¦çš„å­é—®é¢˜çº§åˆ«åé¦ˆï¼Œè¯„ä¼°è¯­ä¹‰å’Œæ•°å­¦ç­‰ä»·æ€§ï¼Œè€Œä¸æ˜¯ä¾èµ–äºåƒµç¡¬çš„å­—ç¬¦ä¸²åŒ¹é…ã€‚è¿™å…è®¸åœ¨ä»¥å‰éš¾ä»¥è§£å†³çš„é—®é¢˜æ ¼å¼ä¸­è¿›è¡Œå¾®å¦™çš„ã€éƒ¨åˆ†ä¿¡ç”¨è¯„åˆ†ã€‚å¤§é‡å®éªŒè¯æ˜äº†StructVRMçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„è®­ç»ƒæ¨¡å‹Seed-StructVRMåœ¨åäºŒä¸ªå…¬å…±å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­çš„å…­ä¸ªä»¥åŠæˆ‘ä»¬æ–°æ•´ç†çš„é«˜éš¾åº¦STEM-Benchä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚StructVRMçš„æˆåŠŸéªŒè¯äº†ä½¿ç”¨ç»“æ„åŒ–å¯éªŒè¯å¥–åŠ±è¿›è¡Œè®­ç»ƒæ˜¯ä¸€ç§åœ¨å¤æ‚ç°å®ä¸–ç•Œæ¨ç†é¢†åŸŸæé«˜å¤šæ¨¡æ€æ¨¡å‹èƒ½åŠ›çš„é«˜æ•ˆæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05383v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨å¤æ‚çš„ã€æ¶‰åŠå¤šä¸ªé—®é¢˜çš„æ¨ç†ä»»åŠ¡ä¸­ï¼Œç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹å¸¸å¸¸ä¼šé‡åˆ°å›°éš¾ï¼Œéƒ¨åˆ†æ­£ç¡®æ€§å¯¹äºæœ‰æ•ˆå­¦ä¹ è‡³å…³é‡è¦ã€‚ä¼ ç»Ÿçš„å¥–åŠ±æœºåˆ¶ä¸ºæ•´ä¸ªå“åº”æä¾›ä¸€ä¸ªå•ä¸€çš„äºŒè¿›åˆ¶åˆ†æ•°ï¼Œè¿™å¯¹äºæŒ‡å¯¼æ¨¡å‹è§£å†³å…·æœ‰å¤šä¸ªå­éƒ¨åˆ†çš„å¤æ‚é—®é¢˜æ˜¯è¿‡äºç²—ç•¥çš„ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†StructVRMæ–¹æ³•ï¼Œå®ƒå°†å¤šæ¨¡æ€æ¨ç†ä¸ç»“æ„åŒ–å¯éªŒè¯å¥–åŠ±æ¨¡å‹å¯¹é½ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªåŸºäºæ¨¡å‹çš„éªŒè¯å™¨ï¼Œç»è¿‡è®­ç»ƒå¯æä¾›ç²¾ç»†ç²’åº¦çš„ã€å­é—®é¢˜çº§åˆ«çš„åé¦ˆï¼Œè¯„ä¼°è¯­ä¹‰å’Œæ•°å­¦ä¸Šçš„ç­‰ä»·æ€§ï¼Œè€Œéä¾èµ–åƒµç¡¬çš„å­—ç¬¦ä¸²åŒ¹é…ã€‚è¿™å…è®¸åœ¨ä»¥å‰éš¾ä»¥è§£å†³çš„é—®é¢˜æ ¼å¼ä¸­è¿›è¡Œç»†å¾®çš„ã€éƒ¨åˆ†ä¿¡ç”¨çš„è¯„åˆ†ã€‚å¹¿æ³›çš„å®éªŒè¯æ˜äº†StructVRMçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„è®­ç»ƒæ¨¡å‹Seed-StructVRMåœ¨åäºŒä¸ªå…¬å…±å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å…­ä¸ªæœ€ä½³æ€§èƒ½ï¼Œä»¥åŠæˆ‘ä»¬æ–°æ•´ç†çš„é«˜éš¾åº¦STEM-Benchä¹Ÿæ˜¯å¦‚æ­¤ã€‚StructVRMçš„æˆåŠŸéªŒè¯äº†ä½¿ç”¨ç»“æ„åŒ–å¯éªŒè¯å¥–åŠ±è¿›è¡ŒåŸ¹è®­æ˜¯ä¸€ç§åœ¨å¤æ‚çš„ç°å®ä¸–ç•Œæ¨ç†é¢†åŸŸæ¨è¿›å¤šæ¨¡æ€æ¨¡å‹èƒ½åŠ›çš„é«˜æ•ˆæ–¹æ³•ã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ul>
<li>ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤æ‚å¤šé—®é¢˜æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¸è¶³ï¼Œéƒ¨åˆ†æ­£ç¡®æ€§å¯¹æœ‰æ•ˆå­¦ä¹ è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿå¥–åŠ±æœºåˆ¶è¿‡äºç²—ç•¥ï¼Œæ— æ³•æœ‰æ•ˆæŒ‡å¯¼æ¨¡å‹è§£å†³å¤æ‚é—®é¢˜ã€‚</li>
<li>StructVRMæ–¹æ³•é€šè¿‡ç»“æ„åŒ–å¯éªŒè¯å¥–åŠ±æ¨¡å‹å¯¹é½å¤šæ¨¡æ€æ¨ç†ï¼Œæä¾›ç²¾ç»†ç²’åº¦çš„å­é—®é¢˜çº§åˆ«åé¦ˆã€‚</li>
<li>StructVRMè¯„ä¼°è¯­ä¹‰å’Œæ•°å­¦ä¸Šçš„ç­‰ä»·æ€§ï¼Œè€Œéä¾èµ–åƒµç¡¬çš„å­—ç¬¦ä¸²åŒ¹é…ï¼Œå…è®¸éƒ¨åˆ†ä¿¡ç”¨è¯„åˆ†ã€‚</li>
<li>StructVRMæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹æ€§èƒ½ï¼Œåœ¨å¤šä¸ªå…¬å…±å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—æœ€ä½³ç»“æœã€‚</li>
<li>Seed-StructVRMæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†ç»“æ„åŒ–å¯éªŒè¯å¥–åŠ±æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05383">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ccf85efd3e4ff5690cfa33a6c39faec4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-104f877b2b0726f0bb2f9ce84d9e90bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-308e5a888c17b8d269fac73c5ecdb3a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1dabc4c3200058d7a144038f97ddf958.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Does-Multimodality-Improve-Recommender-Systems-as-Expected-A-Critical-Analysis-and-Future-Directions"><a href="#Does-Multimodality-Improve-Recommender-Systems-as-Expected-A-Critical-Analysis-and-Future-Directions" class="headerlink" title="Does Multimodality Improve Recommender Systems as Expected? A Critical   Analysis and Future Directions"></a>Does Multimodality Improve Recommender Systems as Expected? A Critical   Analysis and Future Directions</h2><p><strong>Authors:Hongyu Zhou, Yinan Zhang, Aixin Sun, Zhiqi Shen</strong></p>
<p>Multimodal recommendation systems are increasingly popular for their potential to improve performance by integrating diverse data types. However, the actual benefits of this integration remain unclear, raising questions about when and how it truly enhances recommendations. In this paper, we propose a structured evaluation framework to systematically assess multimodal recommendations across four dimensions: Comparative Efficiency, Recommendation Tasks, Recommendation Stages, and Multimodal Data Integration. We benchmark a set of reproducible multimodal models against strong traditional baselines and evaluate their performance on different platforms. Our findings show that multimodal data is particularly beneficial in sparse interaction scenarios and during the recall stage of recommendation pipelines. We also observe that the importance of each modality is task-specific, where text features are more useful in e-commerce and visual features are more effective in short-video recommendations. Additionally, we explore different integration strategies and model sizes, finding that Ensemble-Based Learning outperforms Fusion-Based Learning, and that larger models do not necessarily deliver better results. To deepen our understanding, we include case studies and review findings from other recommendation domains. Our work provides practical insights for building efficient and effective multimodal recommendation systems, emphasizing the need for thoughtful modality selection, integration strategies, and model design. </p>
<blockquote>
<p>å¤šæ¨¡æ€æ¨èç³»ç»Ÿå› å…¶é€šè¿‡é›†æˆå¤šç§æ•°æ®ç±»å‹æé«˜æ€§èƒ½çš„æ½œåŠ›è€Œè¶Šæ¥è¶Šå—æ¬¢è¿ã€‚ç„¶è€Œï¼Œè¿™ç§é›†æˆçš„å®é™…æ•ˆç›Šä»ç„¶ä¸æ˜ç¡®ï¼Œå¼•å‘äº†å…³äºä½•æ—¶ä»¥åŠå¦‚ä½•çœŸæ­£å¢å¼ºæ¨èçš„ç–‘é—®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»“æ„åŒ–è¯„ä¼°æ¡†æ¶ï¼Œä»å››ä¸ªç»´åº¦ç³»ç»Ÿåœ°è¯„ä¼°å¤šæ¨¡æ€æ¨èï¼šæ¯”è¾ƒæ•ˆç‡ã€æ¨èä»»åŠ¡ã€æ¨èé˜¶æ®µå’Œå¤šæ¨¡æ€æ•°æ®é›†æˆã€‚æˆ‘ä»¬å°†ä¸€ç»„å¯å¤åˆ¶çš„å¤šæ¨¡æ€æ¨¡å‹ä¸å¼ºå¤§çš„ä¼ ç»ŸåŸºçº¿è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶åœ¨ä¸åŒçš„å¹³å°ä¸Šè¯„ä¼°å®ƒä»¬çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œå¤šæ¨¡æ€æ•°æ®åœ¨ç¨€ç–äº¤äº’åœºæ™¯å’Œæ¨èç®¡é“çš„å›æº¯é˜¶æ®µç‰¹åˆ«æœ‰ç›Šã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ï¼Œæ¯ç§æ¨¡æ€çš„é‡è¦æ€§æ˜¯ä»»åŠ¡ç‰¹å®šçš„ï¼Œå…¶ä¸­æ–‡æœ¬ç‰¹å¾åœ¨ç”µå­å•†åŠ¡ä¸­æ›´æœ‰ç”¨ï¼Œè€Œè§†è§‰ç‰¹å¾åœ¨çŸ­è§†é¢‘æ¨èä¸­æ›´æœ‰æ•ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸åŒçš„é›†æˆç­–ç•¥å’Œæ¨¡å‹å¤§å°ï¼Œå‘ç°åŸºäºé›†æˆå­¦ä¹ ä¼˜äºåŸºäºèåˆçš„å­¦ä¹ ï¼Œè€Œä¸”æ›´å¤§çš„æ¨¡å‹å¹¶ä¸ä¸€å®šäº§ç”Ÿæ›´å¥½çš„ç»“æœã€‚ä¸ºäº†æ·±å…¥äº†è§£ï¼Œæˆ‘ä»¬åŒ…å«äº†æ¡ˆä¾‹ç ”ç©¶å¹¶å›é¡¾äº†å…¶ä»–æ¨èé¢†åŸŸçš„å‘ç°ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†æ„å»ºé«˜æ•ˆä¸”å®ç”¨çš„å¤šæ¨¡æ€æ¨èç³»ç»Ÿçš„å®ç”¨è§è§£ï¼Œå¼ºè°ƒéœ€è¦æ·±æ€ç†Ÿè™‘çš„æ¨¡æ€é€‰æ‹©ã€é›†æˆç­–ç•¥å’Œæ¨¡å‹è®¾è®¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05377v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šæ¨¡æ€æ¨èç³»ç»Ÿé€šè¿‡æ•´åˆå¤šç§æ•°æ®ç±»å‹æé«˜æ€§èƒ½ï¼Œä½†å…¶å®é™…æ•ˆç›Šå°šä¸æ¸…æ¥šã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªç»“æ„åŒ–è¯„ä¼°æ¡†æ¶ï¼Œä»å››ä¸ªç»´åº¦ç³»ç»Ÿåœ°è¯„ä¼°å¤šæ¨¡æ€æ¨èï¼šæ¯”è¾ƒæ•ˆç‡ã€æ¨èä»»åŠ¡ã€æ¨èé˜¶æ®µå’Œå¤šæ¨¡æ€æ•°æ®é›†æˆã€‚å®éªŒè¡¨æ˜ï¼Œå¤šæ¨¡æ€æ•°æ®åœ¨äº¤äº’åœºæ™¯ç¨€å°‘å’Œæ¨èç®¡é“å¬å›é˜¶æ®µç‰¹åˆ«æœ‰ç›Šã€‚ä¸åŒä»»åŠ¡ä¸‹ä¸åŒæ¨¡æ€çš„é‡è¦æ€§ä¸åŒï¼Œæ–‡æœ¬ç‰¹å¾åœ¨ç”µå­å•†åŠ¡ä¸­æ›´æœ‰ç”¨ï¼Œè§†è§‰ç‰¹å¾åœ¨çŸ­è§†é¢‘æ¨èä¸­æ›´æœ‰æ•ˆã€‚æ­¤å¤–ï¼Œè¿˜å‘ç°é›†æˆç­–ç•¥å’Œæ¨¡å‹å¤§å°çš„å½±å“ï¼Œå¦‚é›†æˆå­¦ä¹ ä¼˜äºèåˆå­¦ä¹ ï¼Œå¤§æ¨¡å‹ä¸ä¸€å®šæ•ˆæœå¥½ã€‚æœ¬æ–‡æ·±åŒ–äº†æˆ‘ä»¬å¯¹å¤šæ¨¡æ€æ¨èç³»ç»Ÿçš„ç†è§£ï¼Œä¸ºæ„å»ºé«˜æ•ˆã€å®ç”¨çš„å¤šæ¨¡æ€æ¨èç³»ç»Ÿæä¾›äº†å®é™…è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ¨èç³»ç»Ÿé€šè¿‡æ•´åˆå¤šç§æ•°æ®ç±»å‹æé«˜æ€§èƒ½ã€‚</li>
<li>å¤šæ¨¡æ€æ•°æ®çš„å®é™…æ•ˆç›Šå°šä¸æ¸…æ¥šï¼Œéœ€è¦ç³»ç»Ÿåœ°è¯„ä¼°ã€‚</li>
<li>å¤šæ¨¡æ€æ•°æ®åœ¨äº¤äº’åœºæ™¯ç¨€å°‘å’Œæ¨èç®¡é“å¬å›é˜¶æ®µç‰¹åˆ«æœ‰ç›Šã€‚</li>
<li>ä¸åŒä»»åŠ¡ä¸‹ä¸åŒæ¨¡æ€çš„é‡è¦æ€§ä¸åŒï¼Œæ–‡æœ¬å’Œè§†è§‰ç‰¹å¾åœ¨ä¸åŒåœºæ™¯ä¸­æœ‰ä¸åŒæ•ˆæœã€‚</li>
<li>Ensemble-Based Learningåœ¨é›†æˆç­–ç•¥ä¸­è¡¨ç°è¾ƒå¥½ã€‚</li>
<li>æ›´å¤§çš„æ¨¡å‹ä¸ä¸€å®šèƒ½å¸¦æ¥æ›´å¥½çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05377">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-023835b59e1ddef8ca62eb4decf322b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3173774089f83b1b770126620ef9cc6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d292d69d2fba0695e3755444e805380f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Information-Theoretic-Graph-Fusion-with-Vision-Language-Action-Model-for-Policy-Reasoning-and-Dual-Robotic-Control"><a href="#Information-Theoretic-Graph-Fusion-with-Vision-Language-Action-Model-for-Policy-Reasoning-and-Dual-Robotic-Control" class="headerlink" title="Information-Theoretic Graph Fusion with Vision-Language-Action Model for   Policy Reasoning and Dual Robotic Control"></a>Information-Theoretic Graph Fusion with Vision-Language-Action Model for   Policy Reasoning and Dual Robotic Control</h2><p><strong>Authors:Shunlei Li, Longsen Gao, Jin Wang, Chang Che, Xi Xiao, Jiuwen Cao, Yingbai Hu, Hamid Reza Karimi</strong></p>
<p>Teaching robots dexterous skills from human videos remains challenging due to the reliance on low-level trajectory imitation, which fails to generalize across object types, spatial layouts, and manipulator configurations. We propose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables dual-arm robotic systems to perform task-level reasoning and execution directly from RGB and Depth human demonstrations. GF-VLA first extracts Shannon-information-based cues to identify hands and objects with the highest task relevance, then encodes these cues into temporally ordered scene graphs that capture both hand-object and object-object interactions. These graphs are fused with a language-conditioned transformer that generates hierarchical behavior trees and interpretable Cartesian motion commands. To improve execution efficiency in bimanual settings, we further introduce a cross-hand selection policy that infers optimal gripper assignment without explicit geometric reasoning. We evaluate GF-VLA on four structured dual-arm block assembly tasks involving symbolic shape construction and spatial generalization. Experimental results show that the information-theoretic scene representation achieves over 95 percent graph accuracy and 93 percent subtask segmentation, supporting the LLM planner in generating reliable and human-readable task policies. When executed by the dual-arm robot, these policies yield 94 percent grasp success, 89 percent placement accuracy, and 90 percent overall task success across stacking, letter-building, and geometric reconfiguration scenarios, demonstrating strong generalization and robustness across diverse spatial and semantic variations. </p>
<blockquote>
<p>æ•™æˆæœºå™¨äººä»äººç±»è§†é¢‘ä¸­æŒæ¡çµå·§æŠ€èƒ½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè¿™ä¾èµ–äºä½å±‚æ¬¡çš„è½¨è¿¹æ¨¡ä»¿ï¼Œæ— æ³•æ¦‚æ‹¬å„ç§å¯¹è±¡ç±»å‹ã€ç©ºé—´å¸ƒå±€å’Œæ“ä½œå™¨é…ç½®ã€‚æˆ‘ä»¬æå‡ºäº†Graph-Fused Vision-Language-Actionï¼ˆGF-VLAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä½¿åŒè‡‚æœºå™¨äººç³»ç»Ÿç›´æ¥ä»RGBå’ŒDepthäººç±»æ¼”ç¤ºä¸­è¿›è¡Œä»»åŠ¡çº§åˆ«çš„æ¨ç†å’Œæ‰§è¡Œã€‚GF-VLAé¦–å…ˆæå–åŸºäºShannonä¿¡æ¯çš„çº¿ç´¢æ¥è¯†åˆ«ä¸ä»»åŠ¡æœ€ç›¸å…³çš„æ‰‹å’Œç‰©ä½“ï¼Œç„¶åå°†è¿™äº›çº¿ç´¢ç¼–ç æˆæŒ‰æ—¶é—´é¡ºåºæ’åˆ—çš„åœºæ™¯å›¾ï¼Œæ•æ‰æ‰‹ä¸ç‰©ä½“ä»¥åŠç‰©ä½“ä¸ç‰©ä½“ä¹‹é—´çš„äº¤äº’ã€‚è¿™äº›å›¾ä¸å—è¯­è¨€æ§åˆ¶çš„å¤§å‹æ¨¡å‹è¿›è¡Œèåˆï¼Œç”Ÿæˆå±‚æ¬¡åŒ–çš„è¡Œä¸ºæ ‘å’Œå¯è§£é‡Šçš„ç¬›å¡å°”è¿åŠ¨å‘½ä»¤ã€‚ä¸ºäº†æé«˜åŒè‡‚ç¯å¢ƒä¸­çš„æ‰§è¡Œæ•ˆç‡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†è·¨æ‰‹é€‰æ‹©ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡æ— æ˜ç¡®å‡ ä½•æ¨ç†çš„æ–¹å¼æ¨æ–­æœ€ä½³çš„å¤¹æŒå™¨åˆ†é…ã€‚æˆ‘ä»¬åœ¨æ¶‰åŠç¬¦å·å½¢çŠ¶æ„é€ å’Œç©ºé—´æ³›åŒ–çš„å››ä¸ªç»“æ„åŒ–åŒè‡‚ç§¯æœ¨ç»„è£…ä»»åŠ¡ä¸Šè¯„ä¼°äº†GF-VLAã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºä¿¡æ¯è®ºçš„åœºæ™¯è¡¨ç¤ºåœ¨å›¾ä¸­å®ç°äº†è¶…è¿‡95ï¼…çš„å‡†ç¡®æ€§å’Œé«˜è¾¾93ï¼…çš„å­ä»»åŠ¡åˆ†å‰²ç‡ï¼Œæ”¯æŒå¤§å‹è¯­è¨€æ¨¡å‹è§„åˆ’å™¨ç”Ÿæˆå¯é ä¸”äººç±»å¯è¯»çš„ç­–ç•¥ã€‚å½“åŒè‡‚æœºå™¨äººæ‰§è¡Œè¿™äº›ç­–ç•¥æ—¶ï¼Œå®ƒä»¬çš„æŠ“æ¡æˆåŠŸç‡è¾¾åˆ°äº†é«˜è¾¾94ï¼…ï¼Œæ”¾ç½®å‡†ç¡®æ€§ä¸ºé«˜è¾¾89ï¼…ï¼Œå¹¶ä¸”åœ¨å †å ã€å­—æ¯æ„é€ å’Œå‡ ä½•é‡æ„åœºæ™¯ä¸­æ•´ä½“ä»»åŠ¡æˆåŠŸç‡ä¸ºé«˜è¾¾90ï¼…ï¼Œè¿™æ˜¾ç¤ºäº†åœ¨ä¸åŒç©ºé—´å’Œè¯­ä¹‰å˜åŒ–ä¸­çš„å¼ºå¤§æ³›åŒ–å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05342v1">PDF</a> Journal under review</p>
<p><strong>Summary</strong>ï¼š<br>æœºå™¨äººä»äººç±»è§†é¢‘ä¸­å­¦ä¹ çµå·§æŠ€èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºä¾èµ–äºä½çº§åˆ«çš„è½¨è¿¹æ¨¡ä»¿ï¼Œæ— æ³•æ¦‚æ‹¬ä¸åŒç±»å‹çš„å¯¹è±¡ã€ç©ºé—´å¸ƒå±€å’Œæ“ä½œé…ç½®ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Graph-Fused Vision-Language-Actionï¼ˆGF-VLAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿åŒè‡‚æœºå™¨äººç³»ç»Ÿèƒ½å¤Ÿä»RGBå’Œæ·±åº¦çš„äººç±»æ¼”ç¤ºä¸­ç›´æ¥è¿›è¡Œä»»åŠ¡çº§åˆ«çš„æ¨ç†å’Œæ‰§è¡Œã€‚GF-VLAé€šè¿‡æå–åŸºäºé¦™å†œä¿¡æ¯çš„çº¿ç´¢æ¥è¯†åˆ«æœ€å…·ä»»åŠ¡ç›¸å…³æ€§çš„æ‰‹å’Œç‰©ä½“ï¼Œç„¶åå°†è¿™äº›çº¿ç´¢ç¼–ç æˆæ•è·æ‰‹ä¸ç‰©ä½“ä¹‹é—´ä»¥åŠç‰©ä½“ä¸ç‰©ä½“ä¹‹é—´äº’åŠ¨çš„æ—¶ç©ºåœºæ™¯å›¾ã€‚è¿™äº›å›¾ä¸è¯­è¨€æ¡ä»¶ä¸‹çš„å˜å‹å™¨ç›¸èåˆï¼Œç”Ÿæˆå±‚æ¬¡åŒ–çš„è¡Œä¸ºæ ‘å’Œå¯è§£é‡Šçš„ç¬›å¡å°”è¿åŠ¨å‘½ä»¤ã€‚ä¸ºæé«˜åŒè‡‚ç¯å¢ƒä¸­çš„æ‰§è¡Œæ•ˆç‡ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†è·¨æ‰‹é€‰æ‹©ç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿåœ¨æ— éœ€æ˜¾å¼å‡ ä½•æ¨ç†çš„æƒ…å†µä¸‹æ¨æ–­æœ€ä½³å¤¹æŒå™¨åˆ†é…ã€‚æˆ‘ä»¬åœ¨æ¶‰åŠç¬¦å·å½¢çŠ¶æ„å»ºå’Œç©ºé—´æ³›åŒ–çš„å››ä¸ªç»“æ„åŒ–åŒè‡‚å—ç»„è£…ä»»åŠ¡ä¸Šè¯„ä¼°äº†GF-VLAã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¿¡æ¯ç†è®ºåœºæ™¯è¡¨ç¤ºæ³•å®ç°äº†è¶…è¿‡95%çš„å›¾å‡†ç¡®æ€§å’Œ93%çš„å­ä»»åŠ¡åˆ†å‰²ï¼Œæ”¯æŒLLMè§„åˆ’å™¨ç”Ÿæˆå¯é ä¸”æ˜“äºç†è§£çš„ä»»åŠ¡ç­–ç•¥ã€‚å½“ç”±åŒè‡‚æœºå™¨äººæ‰§è¡Œæ—¶ï¼Œè¿™äº›ç­–ç•¥åœ¨å †å ã€å­—æ¯æ„å»ºå’Œå‡ ä½•é‡æ„åœºæ™¯ä¸­åˆ†åˆ«å®ç°äº†94%çš„æŠ“å–æˆåŠŸç‡ã€89%çš„å®šä½ç²¾åº¦å’Œ90%çš„æ€»ä½“ä»»åŠ¡æˆåŠŸç‡ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œå¯¹å„ç§ç©ºé—´è¯­ä¹‰å˜åŒ–çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æœºå™¨äººä»äººç±»è§†é¢‘ä¸­å­¦ä¹ æŠ€èƒ½å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºåŸºäºä½çº§åˆ«è½¨è¿¹æ¨¡ä»¿çš„æ–¹æ³•æ— æ³•é€‚åº”ä¸åŒæƒ…å¢ƒã€‚</li>
<li>æå‡ºäº†Graph-Fused Vision-Language-Actionï¼ˆGF-VLAï¼‰æ¡†æ¶ï¼Œä½¿æœºå™¨äººèƒ½ä»RGBå’Œæ·±åº¦æ¼”ç¤ºä¸­ç›´æ¥è¿›è¡Œä»»åŠ¡çº§åˆ«çš„å­¦ä¹ å’Œæ‰§è¡Œã€‚</li>
<li>GF-VLAåˆ©ç”¨é¦™å†œä¿¡æ¯ç†è®ºæ¥è¯†åˆ«ä»»åŠ¡ç›¸å…³çš„æ‰‹å’Œç‰©ä½“ï¼Œå¹¶ç¼–ç æˆåœºæ™¯å›¾ï¼Œæ•è·æ‰‹ä¸ç‰©ä½“é—´çš„äº’åŠ¨ã€‚</li>
<li>èåˆè¯­è¨€æ¡ä»¶ä¸‹çš„å˜å‹å™¨ç”Ÿæˆè¡Œä¸ºæ ‘å’Œè¿åŠ¨å‘½ä»¤ï¼Œæé«˜æœºå™¨äººçš„ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚</li>
<li>å¼•å…¥è·¨æ‰‹é€‰æ‹©ç­–ç•¥ï¼Œä¼˜åŒ–åŒè‡‚æœºå™¨äººåœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ‰§è¡Œæ•ˆç‡ã€‚</li>
<li>åœ¨å››ä¸ªåŒè‡‚å—ç»„è£…ä»»åŠ¡ä¸Šçš„å®éªŒéªŒè¯äº†GF-VLAæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œè¡¨ç°å‡ºé«˜å‡†ç¡®ç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-93bbd2bc35161efbd6340a811acee6b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41a22450d3db1ced069fec47b3e50b75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-900bf0fa21526f23c8f16bf57273b63c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Efficient-Reasoning-for-Large-Reasoning-Language-Models-via-Certainty-Guided-Reflection-Suppression"><a href="#Efficient-Reasoning-for-Large-Reasoning-Language-Models-via-Certainty-Guided-Reflection-Suppression" class="headerlink" title="Efficient Reasoning for Large Reasoning Language Models via   Certainty-Guided Reflection Suppression"></a>Efficient Reasoning for Large Reasoning Language Models via   Certainty-Guided Reflection Suppression</h2><p><strong>Authors:Jiameng Huang, Baijiong Lin, Guhao Feng, Jierun Chen, Di He, Lu Hou</strong></p>
<p>Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought reasoning with complex reflection behaviors, typically signaled by specific trigger words (e.g., â€œWaitâ€ and â€œAlternativelyâ€) to enhance performance. However, these reflection behaviors can lead to the overthinking problem where the generation of redundant reasoning steps that unnecessarily increase token usage, raise inference costs, and reduce practical utility. In this paper, we propose Certainty-Guided Reflection Suppression (CGRS), a novel method that mitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS operates by dynamically suppressing the modelâ€™s generation of reflection triggers when it exhibits high confidence in its current response, thereby preventing redundant reflection cycles without compromising output quality. Our approach is model-agnostic, requires no retraining or architectural modifications, and can be integrated seamlessly with existing autoregressive generation pipelines. Extensive experiments across four reasoning benchmarks (i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRSâ€™s effectiveness: it reduces token usage by an average of 18.5% to 41.9% while preserving accuracy. It also achieves the optimal balance between length reduction and performance compared to state-of-the-art baselines. These results hold consistently across model architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3 family) and scales (4B to 32B parameters), highlighting CGRSâ€™s practical value for efficient reasoning. </p>
<blockquote>
<p>è¿‘æœŸçš„å¤§å‹æ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆLRLMï¼‰é‡‡ç”¨é•¿é“¾æ€ç»´æ¨ç†ï¼Œä¼´éšå¤æ‚çš„åæ€è¡Œä¸ºï¼Œé€šå¸¸é€šè¿‡ç‰¹å®šçš„è§¦å‘è¯ï¼ˆä¾‹å¦‚â€œç­‰ç­‰â€å’Œâ€œæˆ–è€…â€ï¼‰æ¥å¢å¼ºæ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›åæ€è¡Œä¸ºå¯èƒ½å¯¼è‡´è¿‡åº¦æ€è€ƒçš„é—®é¢˜ï¼Œäº§ç”Ÿå†—ä½™çš„æ¨ç†æ­¥éª¤ï¼Œä¸å¿…è¦åœ°å¢åŠ ä»¤ç‰Œä½¿ç”¨é‡ï¼Œæé«˜æ¨ç†æˆæœ¬ï¼Œå¹¶é™ä½å®ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç¡®å®šæ€§å¼•å¯¼åæ€æŠ‘åˆ¶ï¼ˆCGRSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å‡è½»LRLMä¸­è¿‡åº¦æ€è€ƒçš„æ–°æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒæ¨ç†å‡†ç¡®æ€§ã€‚CGRSé€šè¿‡åŠ¨æ€æŠ‘åˆ¶æ¨¡å‹åœ¨å¯¹å…¶å½“å‰å“åº”è¡¨ç°å‡ºé«˜ä¿¡å¿ƒæ—¶ç”Ÿæˆåæ€è§¦å‘ï¼Œä»è€Œé˜²æ­¢å†—ä½™çš„åæ€å¾ªç¯è€Œä¸æŸå®³è¾“å‡ºè´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯æ¨¡å‹æ— å…³çš„ï¼Œä¸éœ€è¦é‡æ–°è®­ç»ƒæˆ–æ¶æ„ä¿®æ”¹ï¼Œå¹¶ä¸”å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„è‡ªå›å½’ç”Ÿæˆç®¡é“ä¸­ã€‚åœ¨å››ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå³AIME24ã€AMC23ã€MATH500å’ŒGPQA-Dï¼‰ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†CGRSçš„æœ‰æ•ˆæ€§ï¼šå®ƒåœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œå¹³å‡å‡å°‘ä»¤ç‰Œä½¿ç”¨18.5%è‡³41.9%ã€‚ä¸æœ€æ–°åŸºçº¿ç›¸æ¯”ï¼Œå®ƒåœ¨é•¿åº¦å‡å°‘å’Œæ€§èƒ½ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ã€‚è¿™äº›ç»“æœåœ¨ä¸åŒçš„æ¨¡å‹æ¶æ„ï¼ˆä¾‹å¦‚DeepSeek-R1-Distillç³»åˆ—ã€QwQ-32Bå’ŒQwen3ç³»åˆ—ï¼‰å’Œè§„æ¨¡ï¼ˆ4Bè‡³32Bå‚æ•°ï¼‰ä¸ŠæŒç»­å­˜åœ¨ï¼Œè¿™çªå‡ºäº†CGRSåœ¨é«˜æ•ˆæ¨ç†ä¸­çš„å®ç”¨ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05337v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºCertainty-Guided Reflection Suppressionï¼ˆCGRSï¼‰çš„æ–¹æ³•ï¼Œç”¨äºç¼“è§£å¤§å‹æ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆLRLMsï¼‰ä¸­çš„è¿‡åº¦æ€è€ƒé—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€æŠ‘åˆ¶æ¨¡å‹ç”Ÿæˆåæ€è§¦å‘è¯æ¥å‡å°‘å†—ä½™æ¨ç†æ­¥éª¤ï¼Œä»è€Œæé«˜æ¨¡å‹çš„å®ç”¨æ€§å’Œæ•ˆç‡ã€‚CGRSæ–¹æ³•å…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–ä¿®æ”¹æ¶æ„ï¼Œå¯æ— ç¼é›†æˆåˆ°ç°æœ‰çš„è‡ªå›å½’ç”Ÿæˆç®¡é“ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCGRSåœ¨å››ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šæœ‰æ•ˆå‡å°‘äº†ä»¤ç‰Œä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†è¯­è¨€æ¨¡å‹ï¼ˆLRLMsï¼‰ä½¿ç”¨é•¿é“¾å¼æ€ç»´è¿›è¡Œå¤æ‚æ¨ç†ï¼Œä½†å¯èƒ½å¯¼è‡´è¿‡åº¦æ€è€ƒé—®é¢˜ã€‚</li>
<li>è¿‡åº¦æ€è€ƒè¡¨ç°ä¸ºç”Ÿæˆå†—ä½™æ¨ç†æ­¥éª¤ï¼Œå¢åŠ ä»¤ç‰Œä½¿ç”¨ã€æé«˜æ¨ç†æˆæœ¬å¹¶é™ä½å®ç”¨æ€§ã€‚</li>
<li>Certainty-Guided Reflection Suppressionï¼ˆCGRSï¼‰æ–¹æ³•æ—¨åœ¨ç¼“è§£è¿‡åº¦æ€è€ƒé—®é¢˜ï¼Œé€šè¿‡åŠ¨æ€æŠ‘åˆ¶æ¨¡å‹ç”Ÿæˆåæ€è§¦å‘è¯æ¥å®ç°ã€‚</li>
<li>CGRSæ–¹æ³•å…·æœ‰æ¨¡å‹æ— å…³æ€§ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–ä¿®æ”¹æ¶æ„ã€‚</li>
<li>CGRSå¯æ— ç¼é›†æˆåˆ°ç°æœ‰çš„è‡ªå›å½’ç”Ÿæˆç®¡é“ä¸­ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>CGRSåœ¨å››ä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå‡å°‘äº†ä»¤ç‰Œä½¿ç”¨ï¼Œé™ä½äº†18.5%è‡³41.9%ï¼ŒåŒæ—¶ä¿æŒå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05337">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3d3202d71aa3814e4c716c8cc56d826e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd668283e0e146e077af42343787b572.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74b832c738eedfdec19715b1f5300f4d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="ReasoningTrack-Chain-of-Thought-Reasoning-for-Long-term-Vision-Language-Tracking"><a href="#ReasoningTrack-Chain-of-Thought-Reasoning-for-Long-term-Vision-Language-Tracking" class="headerlink" title="ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language   Tracking"></a>ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language   Tracking</h2><p><strong>Authors:Xiao Wang, Liye Jin, Xufeng Lou, Shiao Wang, Lan Chen, Bo Jiang, Zhipeng Zhang</strong></p>
<p>Vision-language tracking has received increasing attention in recent years, as textual information can effectively address the inflexibility and inaccuracy associated with specifying the target object to be tracked. Existing works either directly fuse the fixed language with vision features or simply modify using attention, however, their performance is still limited. Recently, some researchers have explored using text generation to adapt to the variations in the target during tracking, however, these works fail to provide insights into the modelâ€™s reasoning process and do not fully leverage the advantages of large models, which further limits their overall performance. To address the aforementioned issues, this paper proposes a novel reasoning-based vision-language tracking framework, named ReasoningTrack, based on a pre-trained vision-language model Qwen2.5-VL. Both SFT (Supervised Fine-Tuning) and reinforcement learning GRPO are used for the optimization of reasoning and language generation. We embed the updated language descriptions and feed them into a unified tracking backbone network together with vision features. Then, we adopt a tracking head to predict the specific location of the target object. In addition, we propose a large-scale long-term vision-language tracking benchmark dataset, termed TNLLT, which contains 200 video sequences. 20 baseline visual trackers are re-trained and evaluated on this dataset, which builds a solid foundation for the vision-language visual tracking task. Extensive experiments on multiple vision-language tracking benchmark datasets fully validated the effectiveness of our proposed reasoning-based natural language generation strategy. The source code of this paper will be released on <a target="_blank" rel="noopener" href="https://github.com/Event-AHU/Open_VLTrack">https://github.com/Event-AHU/Open_VLTrack</a> </p>
<blockquote>
<p>è§†è§‰è¯­è¨€è·Ÿè¸ªè¿‘å¹´æ¥å¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œå› ä¸ºæ–‡æœ¬ä¿¡æ¯å¯ä»¥æœ‰æ•ˆåœ°è§£å†³æŒ‡å®šè¦è·Ÿè¸ªçš„ç›®æ ‡å¯¹è±¡æ—¶çš„ä¸çµæ´»æ€§å’Œå‡†ç¡®æ€§é—®é¢˜ã€‚ç°æœ‰çš„å·¥ä½œè¦ä¹ˆç›´æ¥å°†å›ºå®šè¯­è¨€ä¸è§†è§‰ç‰¹å¾èåˆï¼Œè¦ä¹ˆç®€å•åœ°ä½¿ç”¨æ³¨æ„åŠ›è¿›è¡Œä¿®æ”¹ï¼Œç„¶è€Œï¼Œå®ƒä»¬çš„æ€§èƒ½ä»ç„¶æœ‰é™ã€‚æœ€è¿‘ï¼Œä¸€äº›ç ”ç©¶äººå‘˜æ¢ç´¢äº†ä½¿ç”¨æ–‡æœ¬ç”Ÿæˆæ¥é€‚åº”è·Ÿè¸ªè¿‡ç¨‹ä¸­ç›®æ ‡çš„å˜åŒ–ï¼Œç„¶è€Œï¼Œè¿™äº›å·¥ä½œæœªèƒ½æ·±å…¥æ­ç¤ºæ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶ä¸”æ²¡æœ‰å……åˆ†åˆ©ç”¨å¤§å‹æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œè¿™è¿›ä¸€æ­¥é™åˆ¶äº†å…¶æ•´ä½“æ€§èƒ½ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¨ç†çš„è§†è§‰è¯­è¨€è·Ÿè¸ªæ–°æ¡†æ¶ï¼Œåä¸ºReasoningTrackï¼Œè¯¥æ¡†æ¶åŸºäºé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹Qwen2.5-VLã€‚æˆ‘ä»¬é‡‡ç”¨æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ GRPOå¯¹æ¨ç†å’Œè¯­è¨€ç”Ÿæˆè¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬å°†æ›´æ–°çš„è¯­è¨€æè¿°åµŒå…¥å…¶ä¸­ï¼Œå¹¶ä¸è§†è§‰ç‰¹å¾ä¸€èµ·è¾“å…¥åˆ°ç»Ÿä¸€çš„è·Ÿè¸ªä¸»å¹²ç½‘ç»œä¸­ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨è·Ÿè¸ªå¤´æ¥é¢„æµ‹ç›®æ ‡å¯¹è±¡çš„ç‰¹å®šä½ç½®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤§è§„æ¨¡é•¿æœŸè§†è§‰è¯­è¨€è·Ÿè¸ªåŸºå‡†æ•°æ®é›†TNLLTï¼Œå®ƒåŒ…å«200ä¸ªè§†é¢‘åºåˆ—ã€‚æˆ‘ä»¬åœ¨è¯¥æ•°æ®é›†ä¸Šå¯¹20ä¸ªåŸºå‡†è§†è§‰è·Ÿè¸ªå™¨è¿›è¡Œäº†é‡æ–°è®­ç»ƒå’Œè¯„ä¼°ï¼Œä¸ºè§†è§‰è¯­è¨€è§†è§‰è·Ÿè¸ªä»»åŠ¡å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚åœ¨å¤šä¸ªè§†è§‰è¯­è¨€è·Ÿè¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒå……åˆ†éªŒè¯äº†æˆ‘ä»¬æå‡ºçš„åŸºäºæ¨ç†çš„è‡ªç„¶è¯­è¨€ç”Ÿæˆç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚æœ¬æ–‡çš„æºä»£ç å°†å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Event-AHU/Open_VLTrack%E4%B8%8A%E3%80%82">https://github.com/Event-AHU/Open_VLTrackä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05221v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹Qwen2.5-VLçš„æ¨ç†å‹è§†è§‰è¯­è¨€è·Ÿè¸ªæ¡†æ¶ReasoningTrackã€‚è¯¥æ¡†æ¶ç»“åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ GRPOè¿›è¡Œä¼˜åŒ–ï¼ŒåµŒå…¥æ›´æ–°çš„è¯­è¨€æè¿°å¹¶å°†å…¶ä¸è§†è§‰ç‰¹å¾ä¸€èµ·è¾“å…¥åˆ°ç»Ÿä¸€çš„è·Ÿè¸ªä¸»å¹²ç½‘ç»œä¸­ã€‚é‡‡ç”¨è·Ÿè¸ªå¤´é¢„æµ‹ç›®æ ‡å¯¹è±¡çš„å…·ä½“ä½ç½®ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºå¤§è§„æ¨¡é•¿æœŸè§†è§‰è¯­è¨€è·Ÿè¸ªåŸºå‡†æ•°æ®é›†TNLLTï¼Œå¹¶åœ¨æ­¤æ•°æ®é›†ä¸Šé‡æ–°è®­ç»ƒå’Œè¯„ä¼°äº†20ä¸ªåŸºå‡†è§†è§‰è·Ÿè¸ªå™¨ï¼Œä¸ºè§†è§‰è¯­è¨€è·Ÿè¸ªä»»åŠ¡å¥ å®šäº†åšå®åŸºç¡€ã€‚å®éªŒè¯æ˜ï¼ŒåŸºäºæ¨ç†çš„è‡ªç„¶è¯­è¨€ç”Ÿæˆç­–ç•¥æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€è·Ÿè¸ªç»“åˆäº†æ–‡æœ¬ä¿¡æ¯ï¼Œä»¥è§£å†³ç›®æ ‡å¯¹è±¡æŒ‡å®šä¸­çš„ä¸çµæ´»æ€§å’Œä¸å‡†ç¡®æ€§é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åŒ…æ‹¬ç›´æ¥èåˆè¯­è¨€ä¸è§†è§‰ç‰¹å¾æˆ–ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œä¿®æ”¹ï¼Œä½†æ€§èƒ½æœ‰é™ã€‚</li>
<li>è®ºæ–‡æå‡ºä¸€ç§æ–°å‹æ¨ç†å‹è§†è§‰è¯­è¨€è·Ÿè¸ªæ¡†æ¶ReasoningTrackï¼ŒåŸºäºé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹Qwen2.5-VLã€‚</li>
<li>ä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ GRPOä¼˜åŒ–æ¨ç†å’Œè¯­è¨€ç”Ÿæˆã€‚</li>
<li>åµŒå…¥æ›´æ–°åçš„è¯­è¨€æè¿°ä¸è§†è§‰ç‰¹å¾ä¸€èµ·è¾“å…¥åˆ°è·Ÿè¸ªä¸»å¹²ç½‘ç»œä¸­ã€‚</li>
<li>æå‡ºå¤§è§„æ¨¡é•¿æœŸè§†è§‰è¯­è¨€è·Ÿè¸ªåŸºå‡†æ•°æ®é›†TNLLTï¼ŒåŒ…å«200ä¸ªè§†é¢‘åºåˆ—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0ea99c3906651efb5e9c6788460f98f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49f5c77325f861a816349cc3c8c5030f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d919530a603768d86d7de5f30e57430e.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="STEPWISE-CODEX-Bench-Evaluating-Complex-Multi-Function-Comprehension-and-Fine-Grained-Execution-Reasoning"><a href="#STEPWISE-CODEX-Bench-Evaluating-Complex-Multi-Function-Comprehension-and-Fine-Grained-Execution-Reasoning" class="headerlink" title="STEPWISE-CODEX-Bench: Evaluating Complex Multi-Function Comprehension   and Fine-Grained Execution Reasoning"></a>STEPWISE-CODEX-Bench: Evaluating Complex Multi-Function Comprehension   and Fine-Grained Execution Reasoning</h2><p><strong>Authors:Kaiwen Yan, Yuhang Chang, Zirui Guo, Yaling Mou, Jiang Ming, Jingwei Sun</strong></p>
<p>In recent years, large language models (LLMs) have made significant progress in code intelligence, yet systematically evaluating their code understanding and reasoning abilities remains challenging. Mainstream benchmarks such as HumanEval and MBPP primarily assess functional correctness, while reasoning benchmarks like CRUXEVAL are limited to single-function, low-complexity scenarios. As a result, advanced models achieve nearly saturated scores, limiting their discriminative power. To address this, we present STEPWISE-CODEX-Bench (SX-Bench), a novel benchmark designed for complex multi-function understanding and fine-grained execution reasoning. SX-Bench features tasks involving collaboration among multiple sub-functions (e.g., chained calls, nested loops), shifting evaluation towards overall control and data flow modeling. It defines â€œcomputation stepsâ€ as the minimal execution unit and requires models to predict the total number of steps in reasoning tasks, thereby assessing a modelâ€™s in-depth understanding of dynamic execution beyond simple I&#x2F;O matching. Evaluation on over 20 mainstream models (including 14 reasoning-enhanced models) demonstrates that SX-Bench is highly discriminative: even the state-of-the-art OpenAI-O3 achieves only 78.37 percent accuracy on Hard-Reasoning tasks, much lower than its saturated scores on previous benchmarks, thereby revealing bottlenecks in complex and fine-grained reasoning. We also release an automated pipeline combining program synthesis, symbolic execution, and LLM-aided validation for efficient benchmark generation and quality assurance. SX-Bench advances code evaluation from â€œsingle-function verificationâ€ to â€œmulti-function dynamic reasoning,â€ providing a key tool for the in-depth assessment of advanced code intelligence models. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç æ™ºèƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç³»ç»Ÿåœ°è¯„ä¼°å…¶ä»£ç ç†è§£å’Œæ¨ç†èƒ½åŠ›ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸»æµåŸºå‡†æµ‹è¯•å¦‚HumanEvalå’ŒMBPPä¸»è¦è¯„ä¼°åŠŸèƒ½æ­£ç¡®æ€§ï¼Œè€Œæ¨ç†åŸºå‡†æµ‹è¯•å¦‚CRUXEVALä»…é™äºå•ä¸€åŠŸèƒ½ã€ä½å¤æ‚åº¦çš„åœºæ™¯ã€‚å› æ­¤ï¼Œå…ˆè¿›æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­è·å¾—è¿‘ä¹é¥±å’Œçš„åˆ†æ•°ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„è¾¨åˆ«åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†STEPWISE-CODEX-Benchï¼ˆSX-Benchï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºå¤æ‚å¤šåŠŸèƒ½ç†è§£å’Œç²¾ç»†æ‰§è¡Œæ¨ç†è®¾è®¡çš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚SX-Benchæ¶‰åŠå¤šä¸ªå­åŠŸèƒ½ä¹‹é—´çš„åä½œä»»åŠ¡ï¼Œå¦‚é“¾å¼è°ƒç”¨ã€åµŒå¥—å¾ªç¯ç­‰ï¼Œå°†è¯„ä¼°è½¬å‘æ•´ä½“æ§åˆ¶å’Œæ•°æ®æµå»ºæ¨¡ã€‚å®ƒå®šä¹‰äº†â€œè®¡ç®—æ­¥éª¤â€ä½œä¸ºæœ€å°çš„æ‰§è¡Œå•å…ƒï¼Œå¹¶è¦æ±‚æ¨¡å‹é¢„æµ‹æ¨ç†ä»»åŠ¡ä¸­çš„æ€»æ­¥éª¤æ•°ï¼Œä»è€Œè¯„ä¼°æ¨¡å‹å¯¹åŠ¨æ€æ‰§è¡Œçš„æ·±å…¥ç†è§£ï¼Œè€Œä¸ä»…ä»…æ˜¯ç®€å•çš„è¾“å…¥è¾“å‡ºåŒ¹é…ã€‚å¯¹è¶…è¿‡20ä¸ªä¸»æµæ¨¡å‹çš„è¯„ä¼°ï¼ˆåŒ…æ‹¬14ä¸ªå¢å¼ºæ¨ç†æ¨¡å‹ï¼‰è¡¨æ˜ï¼ŒSX-Benchå…·æœ‰å¾ˆå¼ºçš„è¾¨åˆ«åŠ›ï¼šå³ä½¿æ˜¯æœ€æ–°ä¸€ä»£çš„OpenAI-O3åœ¨ç¡¬æ¨ç†ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ä¹Ÿåªæœ‰78.37%ï¼Œè¿œä½äºå…¶åœ¨ä»¥å‰åŸºå‡†æµ‹è¯•ä¸­çš„é¥±å’Œåˆ†æ•°ï¼Œä»è€Œæ­ç¤ºäº†å…¶åœ¨å¤æ‚å’Œç²¾ç»†æ¨ç†æ–¹é¢çš„ç“¶é¢ˆã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–ç®¡é“ï¼Œç»“åˆç¨‹åºåˆæˆã€ç¬¦å·æ‰§è¡Œå’ŒLLMè¾…åŠ©éªŒè¯ï¼Œä»¥æé«˜åŸºå‡†æµ‹è¯•ç”Ÿæˆå’Œè´¨é‡æ§åˆ¶æ•ˆç‡ã€‚SX-Benchæ¨åŠ¨ä»£ç è¯„ä¼°ä»â€œå•åŠŸèƒ½éªŒè¯â€è½¬å‘â€œå¤šåŠŸèƒ½åŠ¨æ€æ¨ç†â€ï¼Œä¸ºé«˜çº§ä»£ç æ™ºèƒ½æ¨¡å‹çš„æ·±å…¥è¯„ä¼°æä¾›äº†å…³é”®å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05193v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç æ™ºèƒ½æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç³»ç»Ÿè¯„ä¼°å…¶ä»£ç ç†è§£å’Œæ¨ç†èƒ½åŠ›æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚å½“å‰ä¸»æµåŸºå‡†æµ‹è¯•ä¸»è¦è¯„ä¼°åŠŸèƒ½æ­£ç¡®æ€§ï¼Œè€Œæ¨ç†åŸºå‡†æµ‹è¯•ä»…é™äºå•ä¸€åŠŸèƒ½ã€ä½å¤æ‚åº¦çš„åœºæ™¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†STEPWISE-CODEX-Benchï¼ˆSX-Benchï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤æ‚å¤šå‡½æ•°ç†è§£å’Œç²¾ç»†æ‰§è¡Œæ¨ç†çš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚SX-Benchçš„ä»»åŠ¡æ¶‰åŠå¤šä¸ªå­å‡½æ•°ä¹‹é—´çš„åä½œï¼Œè¦æ±‚æ¨¡å‹é¢„æµ‹æ¨ç†ä»»åŠ¡ä¸­çš„æ€»æ­¥éª¤æ•°ï¼Œä»è€Œè¯„ä¼°æ¨¡å‹å¯¹åŠ¨æ€æ‰§è¡Œçš„æ·±å…¥ç†è§£ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºSX-Benchå…·æœ‰é«˜åº¦è¾¨åˆ«åŠ›ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„OpenAI-O3åœ¨ç¡¬æ¨ç†ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ä¹Ÿåªæœ‰78.37%ï¼Œè¿œä½äºå…¶åœ¨å…ˆå‰åŸºå‡†æµ‹è¯•ä¸Šçš„é¥±å’Œåˆ†æ•°ã€‚SX-Benchçš„æ¨å‡ºï¼Œæ ‡å¿—ç€ä»£ç è¯„ä¼°ä»â€œå•åŠŸèƒ½éªŒè¯â€è·ƒè¿›åˆ°â€œå¤šåŠŸèƒ½åŠ¨æ€æ¨ç†â€ï¼Œä¸ºé«˜çº§ä»£ç æ™ºèƒ½æ¨¡å‹çš„æ·±å…¥è¯„ä¼°æä¾›äº†å…³é”®å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç æ™ºèƒ½æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†è¯„ä¼°å…¶ä»£ç ç†è§£å’Œæ¨ç†èƒ½åŠ›ä»å…·æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨åŠŸèƒ½æ­£ç¡®æ€§ï¼Œç¼ºä¹å¯¹äºå¤æ‚ã€ç²¾ç»†æ¨ç†èƒ½åŠ›çš„è¯„ä¼°ã€‚</li>
<li>STEPWISE-CODEX-Benchï¼ˆSX-Benchï¼‰æ˜¯ä¸€ç§æ–°å‹åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šå‡½æ•°ç†è§£å’Œæ‰§è¡Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>SX-Benchä»»åŠ¡æ¶‰åŠå¤šå­å‡½æ•°åä½œï¼Œè¦æ±‚æ¨¡å‹é¢„æµ‹æ¨ç†æ­¥éª¤æ•°ï¼Œä»¥è¯„ä¼°å…¶å¯¹åŠ¨æ€æ‰§è¡Œçš„æ·±å…¥ç†è§£ã€‚</li>
<li>SX-Benchå…·æœ‰é«˜åº¦è¾¨åˆ«åŠ›ï¼Œèƒ½å¤Ÿæ­ç¤ºæ¨¡å‹åœ¨å¤æ‚å’Œç²¾ç»†æ¨ç†æ–¹é¢çš„ç“¶é¢ˆã€‚</li>
<li>å…ˆè¿›æ¨¡å‹åœ¨SX-Benchä¸Šçš„è¡¨ç°è¿œä½äºå…¶åœ¨å…ˆå‰åŸºå‡†æµ‹è¯•ä¸Šçš„é¥±å’Œåˆ†æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05193">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-12540ef51a4ec8fcaf9f83ea9b839fba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8b9d06f122de84d0663f51473aea979.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9365b8a681a42495dd33ad78e6c7d4b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07df3a44f0d89dc3a0c942b7244f236b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1af6dd8115fb2f6b974f5770bfe2864.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-085d863a42334e47d42846888c73dd42.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5a5e61bf939d086b257a710c6cb80ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-266c188994aac14a64af38872ca9134e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Posterior-GRPO-Rewarding-Reasoning-Processes-in-Code-Generation"><a href="#Posterior-GRPO-Rewarding-Reasoning-Processes-in-Code-Generation" class="headerlink" title="Posterior-GRPO: Rewarding Reasoning Processes in Code Generation"></a>Posterior-GRPO: Rewarding Reasoning Processes in Code Generation</h2><p><strong>Authors:Lishui Fan, Yu Zhang, Mouxiang Chen, Zhongxin Liu</strong></p>
<p>Reinforcement learning (RL) has significantly advanced code generation for large language models (LLMs). However, current paradigms rely on outcome-based rewards from test cases, neglecting the quality of the intermediate reasoning process. While supervising the reasoning process directly is a promising direction, it is highly susceptible to reward hacking, where the policy model learns to exploit the reasoning reward signal without improving final outcomes. To address this, we introduce a unified framework that can effectively incorporate the quality of the reasoning process during RL. First, to enable reasoning evaluation, we develop LCB-RB, a benchmark comprising preference pairs of superior and inferior reasoning processes. Second, to accurately score reasoning quality, we introduce an Optimized-Degraded based (OD-based) method for reward model training. This method generates high-quality preference pairs by systematically optimizing and degrading initial reasoning paths along curated dimensions of reasoning quality, such as factual accuracy, logical rigor, and coherence. A 7B parameter reward model with this method achieves state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method that conditions process-based rewards on task success. By selectively applying rewards to the reasoning processes of only successful outcomes, P-GRPO effectively mitigates reward hacking and aligns the modelâ€™s internal reasoning with final code correctness. A 7B parameter model with P-GRPO achieves superior performance across diverse code generation tasks, outperforming outcome-only baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further demonstrate the generalizability of our approach by extending it to mathematical tasks. Our models, dataset, and code are publicly available. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ¨¡å¼ä¾èµ–äºæµ‹è¯•ç”¨ä¾‹çš„ç»“æœå¯¼å‘å¥–åŠ±ï¼Œå¿½è§†äº†ä¸­é—´æ¨ç†è¿‡ç¨‹çš„è´¨é‡ã€‚è™½ç„¶ç›´æ¥ç›‘ç£æ¨ç†è¿‡ç¨‹æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ï¼Œä½†å®ƒéå¸¸å®¹æ˜“å—åˆ°å¥–åŠ±ç ´è§£çš„å½±å“ï¼Œå³ç­–ç•¥æ¨¡å‹å­¦ä¼šåˆ©ç”¨æ¨ç†å¥–åŠ±ä¿¡å·è€Œä¸æé«˜æœ€ç»ˆçš„ç»“æœã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åœ¨RLä¸­èå…¥æ¨ç†è¿‡ç¨‹çš„è´¨é‡ã€‚é¦–å…ˆï¼Œä¸ºäº†è¿›è¡Œæ¨ç†è¯„ä¼°ï¼Œæˆ‘ä»¬å¼€å‘äº†LCB-RBåŸºå‡†æµ‹è¯•ï¼Œå®ƒåŒ…æ‹¬ä¼˜è´¨å’ŒåŠ£è´¨æ¨ç†è¿‡ç¨‹çš„åå¥½å¯¹ã€‚å…¶æ¬¡ï¼Œä¸ºäº†å‡†ç¡®åœ°è¯„åˆ†æ¨ç†è´¨é‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºä¼˜åŒ–é€€åŒ–ï¼ˆODï¼‰çš„æ–¹æ³•æ¥è¿›è¡Œå¥–åŠ±æ¨¡å‹è®­ç»ƒã€‚è¯¥æ–¹æ³•é€šè¿‡ç³»ç»Ÿåœ°ä¼˜åŒ–å’Œé€€åŒ–åˆå§‹æ¨ç†è·¯å¾„ï¼Œæ²¿ç€æ¨ç†è´¨é‡çš„ç²¾é€‰ç»´åº¦ï¼ˆå¦‚äº‹å®å‡†ç¡®æ€§ã€é€»è¾‘ä¸¥è°¨æ€§å’Œè¿è´¯æ€§ï¼‰ç”Ÿæˆé«˜è´¨é‡çš„åå¥½å¯¹ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•çš„70äº¿å‚æ•°å¥–åŠ±æ¨¡å‹åœ¨LCB-RBä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶èƒ½å¤Ÿå¾ˆå¥½åœ°æ¨å¹¿åˆ°å…¶ä»–åŸºå‡†æµ‹è¯•ã€‚æœ€åï¼Œæˆ‘ä»¬ä»‹ç»äº†åç½®GRPOï¼ˆP-GRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„RLæ–¹æ³•ï¼Œå®ƒæ ¹æ®ä»»åŠ¡æˆåŠŸæ¥åˆ¶å®šåŸºäºè¿‡ç¨‹çš„å¥–åŠ±ã€‚é€šè¿‡æœ‰é€‰æ‹©åœ°å¯¹æˆåŠŸç»“æœçš„æ¨ç†è¿‡ç¨‹æ–½åŠ å¥–åŠ±ï¼ŒP-GRPOæœ‰æ•ˆåœ°å‡è½»äº†å¥–åŠ±ç ´è§£é—®é¢˜ï¼Œå¹¶ä½¿æ¨¡å‹çš„å†…éƒ¨æ¨ç†ä¸æœ€ç»ˆçš„ä»£ç æ­£ç¡®æ€§ç›¸ç¬¦ã€‚ä½¿ç”¨P-GRPOçš„70äº¿å‚æ•°æ¨¡å‹åœ¨å¤šç§ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜è¶Šï¼Œæ¯”ä»…åŸºäºç»“æœçš„åŸºçº¿é«˜å‡º4.5%ï¼Œå…¶æ€§èƒ½ä¸GPT-4 Turboç›¸å½“ã€‚æˆ‘ä»¬è¿˜é€šè¿‡å°†å…¶æ‰©å±•åˆ°æ•°å­¦ä»»åŠ¡æ¥å±•ç¤ºæˆ‘ä»¬æ–¹æ³•çš„ä¸€èˆ¬æ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹ã€æ•°æ®é›†å’Œä»£ç éƒ½æ˜¯å…¬å¼€å¯ç”¨çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05170v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç ç”Ÿæˆä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æµ‹è¯•æ¡ˆä¾‹çš„ç»“æœå¥–åŠ±ï¼Œå¿½è§†äº†ä¸­é—´æ¨ç†è¿‡ç¨‹çš„è´¨é‡ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥æœ‰æ•ˆåœ°å°†æ¨ç†è¿‡ç¨‹çš„è´¨é‡çº³å…¥å¼ºåŒ–å­¦ä¹ è€ƒé‡ã€‚ç ”ç©¶é¦–å…ˆå¼€å‘äº†LCB-RBåŸºå‡†æµ‹è¯•é›†ï¼ŒåŒ…å«ä¼˜è´¨ä¸åŠ£è´¨æ¨ç†è¿‡ç¨‹çš„åå¥½å¯¹ï¼Œç”¨äºè¯„ä¼°æ¨ç†è´¨é‡ã€‚æ¥ç€ï¼Œæå‡ºäº†åŸºäºä¼˜åŒ–ä¸é™çº§ï¼ˆOD-basedï¼‰çš„å¥–åŠ±æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œé€šè¿‡ç³»ç»Ÿåœ°ä¼˜åŒ–å’Œé™çº§åˆå§‹æ¨ç†è·¯å¾„ï¼Œç”Ÿæˆé«˜è´¨é‡åå¥½å¯¹ã€‚ä½¿ç”¨æ­¤æ–¹æ³•ï¼Œ7Bå‚æ•°çš„å¥–åŠ±æ¨¡å‹åœ¨LCB-RBä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³å¹¶è‰¯å¥½åœ°æ¨å¹¿åˆ°å…¶ä»–åŸºå‡†æµ‹è¯•ã€‚æœ€åï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºè¿‡ç¨‹å¥–åŠ±çš„åéªŒGRPOï¼ˆP-GRPOï¼‰å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯¹æˆåŠŸçš„ä»»åŠ¡ç»“æœåº”ç”¨é€‰æ‹©æ€§å¥–åŠ±ï¼Œæœ‰æ•ˆé˜²æ­¢å¥–åŠ±è¢«æ»¥ç”¨ã€‚P-GRPOæ¨¡å‹åœ¨å¤šæ ·åŒ–çš„ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæ¯”ä»…ä¾èµ–ç»“æœåŸºçº¿æé«˜äº†4.5%ï¼Œå¹¶å¯ä¸GPT-4 Turboç›¸æå¹¶è®ºã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å°†æ–¹æ³•æ‰©å±•åˆ°æ•°å­¦ä»»åŠ¡ä¸­å±•ç¤ºäº†å…¶é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç ç”Ÿæˆä¸­å–å¾—äº†è¿›å±•ï¼Œä½†å¿½è§†äº†æ¨ç†è¿‡ç¨‹çš„è´¨é‡ã€‚</li>
<li>å¼•å…¥LCB-RBåŸºå‡†æµ‹è¯•é›†æ¥è¯„ä¼°æ¨ç†è´¨é‡ï¼ŒåŒ…å«ä¼˜è´¨ä¸åŠ£è´¨æ¨ç†è¿‡ç¨‹çš„åå¥½å¯¹ã€‚</li>
<li>æå‡ºåŸºäºä¼˜åŒ–ä¸é™çº§çš„å¥–åŠ±æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œç”Ÿæˆé«˜è´¨é‡åå¥½å¯¹å¹¶æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¼•å…¥P-GRPOå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©æ€§åº”ç”¨å¥–åŠ±æ¥é˜²æ­¢å¥–åŠ±è¢«æ»¥ç”¨ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>P-GRPOæ¨¡å‹åœ¨å¤šæ ·åŒ–çš„ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹¶åœ¨æ•°å­¦ä»»åŠ¡ä¸­å±•ç¤ºäº†é€šç”¨æ€§ã€‚</li>
<li>7Bå‚æ•°å¥–åŠ±æ¨¡å‹åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ï¼Œä¸”æ–¹æ³•å…·æœ‰è‰¯å¥½çš„æ¨å¹¿æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05170">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-92a18b12251b0a38dec249aca43f08ac.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-542a5f6c3884f25bff6a7a92d255d3ad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-260b233bac7d7cc3428d2cd8a64d944e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba226d91a6ec1cf7bd18ebca52b4bb58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7335cbf5867442a4db82c06e7c8f8e58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59e11aa2aec0c0b969c3bcc6d9778922.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="PhysPatch-A-Physically-Realizable-and-Transferable-Adversarial-Patch-Attack-for-Multimodal-Large-Language-Models-based-Autonomous-Driving-Systems"><a href="#PhysPatch-A-Physically-Realizable-and-Transferable-Adversarial-Patch-Attack-for-Multimodal-Large-Language-Models-based-Autonomous-Driving-Systems" class="headerlink" title="PhysPatch: A Physically Realizable and Transferable Adversarial Patch   Attack for Multimodal Large Language Models-based Autonomous Driving Systems"></a>PhysPatch: A Physically Realizable and Transferable Adversarial Patch   Attack for Multimodal Large Language Models-based Autonomous Driving Systems</h2><p><strong>Authors:Qi Guo, Xiaojun Jia, Shanmin Pang, Simeng Qin, Lin Wang, Ju Jia, Yang Liu, Qing Guo</strong></p>
<p>Multimodal Large Language Models (MLLMs) are becoming integral to autonomous driving (AD) systems due to their strong vision-language reasoning capabilities. However, MLLMs are vulnerable to adversarial attacks, particularly adversarial patch attacks, which can pose serious threats in real-world scenarios. Existing patch-based attack methods are primarily designed for object detection models and perform poorly when transferred to MLLM-based systems due to the latterâ€™s complex architectures and reasoning abilities. To address these limitations, we propose PhysPatch, a physically realizable and transferable adversarial patch framework tailored for MLLM-based AD systems. PhysPatch jointly optimizes patch location, shape, and content to enhance attack effectiveness and real-world applicability. It introduces a semantic-based mask initialization strategy for realistic placement, an SVD-based local alignment loss with patch-guided crop-resize to improve transferability, and a potential field-based mask refinement method. Extensive experiments across open-source, commercial, and reasoning-capable MLLMs demonstrate that PhysPatch significantly outperforms prior methods in steering MLLM-based AD systems toward target-aligned perception and planning outputs. Moreover, PhysPatch consistently places adversarial patches in physically feasible regions of AD scenes, ensuring strong real-world applicability and deployability. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”±äºå…¶å¼ºå¤§çš„è§†è§‰è¯­è¨€æ¨ç†èƒ½åŠ›ï¼Œå·²æˆä¸ºè‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰ç³»ç»Ÿçš„ä¸å¯æˆ–ç¼ºçš„éƒ¨åˆ†ã€‚ç„¶è€Œï¼ŒMLLMså®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯å¯¹æŠ—æ€§è¡¥ä¸æ”»å‡»ï¼Œè¿™åœ¨ç°å®åœºæ™¯ä¸­å¯èƒ½æ„æˆä¸¥é‡å¨èƒã€‚ç°æœ‰çš„åŸºäºè¡¥ä¸çš„æ”»å‡»æ–¹æ³•ä¸»è¦æ˜¯ä¸ºå¯¹è±¡æ£€æµ‹æ¨¡å‹è®¾è®¡çš„ï¼Œå½“è½¬ç§»åˆ°åŸºäºMLLMçš„ç³»ç»Ÿæ—¶ï¼Œç”±äºåè€…çš„å¤æ‚æ¶æ„å’Œæ¨ç†èƒ½åŠ›ï¼Œå…¶è¡¨ç°è¾ƒå·®ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†PhysPatchï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹åŸºäºMLLMçš„ADç³»ç»Ÿçš„ç‰©ç†å¯å®ç°å’Œå¯è½¬ç§»çš„å¯¹æŠ—æ€§è¡¥ä¸æ¡†æ¶ã€‚PhysPatchè”åˆä¼˜åŒ–è¡¥ä¸çš„ä½ç½®ã€å½¢çŠ¶å’Œå†…å®¹ï¼Œä»¥æé«˜æ”»å‡»çš„æœ‰æ•ˆæ€§å’Œç°å®ä¸–ç•Œçš„é€‚ç”¨æ€§ã€‚å®ƒå¼•å…¥äº†ä¸€ç§åŸºäºè¯­ä¹‰çš„æ©æ¨¡åˆå§‹åŒ–ç­–ç•¥ï¼Œç”¨äºå®ç°çœŸå®çš„æ”¾ç½®ä½ç½®ï¼Œä¸€ç§åŸºäºSVDçš„å±€éƒ¨å¯¹é½æŸå¤±ï¼Œç»“åˆè¡¥ä¸å¼•å¯¼çš„è£å‰ªå’Œç¼©æ”¾ï¼Œä»¥æé«˜å¯è½¬ç§»æ€§ï¼Œä»¥åŠä¸€ç§åŸºäºåŠ¿åœºçš„æ©æ¨¡ç»†åŒ–æ–¹æ³•ã€‚åœ¨å¼€æºã€å•†ä¸šå’Œå…·æœ‰æ¨ç†èƒ½åŠ›çš„MLLMsä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPhysPatchåœ¨å¼•å¯¼åŸºäºMLLMçš„ADç³»ç»Ÿå®ç°ç›®æ ‡å¯¹é½çš„æ„ŸçŸ¥å’Œè§„åˆ’è¾“å‡ºæ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒPhysPatchå§‹ç»ˆåœ¨ADåœºæ™¯çš„ç‰©ç†å¯è¡ŒåŒºåŸŸä¸­æ”¾ç½®å¯¹æŠ—æ€§è¡¥ä¸ï¼Œç¡®ä¿å¼ºå¤§çš„ç°å®ä¸–ç•Œé€‚ç”¨æ€§å’Œå¯éƒ¨ç½²æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05167v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰ç³»ç»Ÿä¸­æ‰®æ¼”ç€æ—¥ç›Šé‡è¦çš„è§’è‰²ï¼Œå¾—ç›Šäºå…¶å¼ºå¤§çš„è§†è§‰-è¯­è¨€æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒMLLMæ˜“å—å¯¹æŠ—æ€§æ”»å‡»å¨èƒï¼Œå°¤å…¶æ˜¯å¯¹æŠ—æ€§è¡¥ä¸æ”»å‡»ï¼Œåœ¨çœŸå®åœºæ™¯ä¸­å¯èƒ½å¸¦æ¥ä¸¥é‡å¨èƒã€‚é’ˆå¯¹ç°æœ‰è¡¥ä¸æ”»å‡»æ–¹æ³•é’ˆå¯¹å¯¹è±¡æ£€æµ‹æ¨¡å‹çš„å±€é™æ€§ï¼Œä»¥åŠMLLMç³»ç»Ÿçš„å¤æ‚æ¶æ„å’Œæ¨ç†èƒ½åŠ›è€Œæ•ˆæœä¸ä½³çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PhysPatchæ¡†æ¶ã€‚PhysPatchè”åˆä¼˜åŒ–è¡¥ä¸çš„ä½ç½®ã€å½¢çŠ¶å’Œå†…å®¹ï¼Œä»¥æé«˜æ”»å‡»æ•ˆæœå’ŒçœŸå®ä¸–ç•Œé€‚ç”¨æ€§ã€‚å®ƒé€šè¿‡åŸºäºè¯­ä¹‰çš„æ©æ¨¡åˆå§‹åŒ–ç­–ç•¥å®ç°é€¼çœŸçš„è¡¥ä¸æ”¾ç½®ï¼Œä½¿ç”¨SVDçš„å±€éƒ¨å¯¹é½æŸå¤±å’Œè¡¥ä¸å¼•å¯¼çš„è£å‰ªè°ƒæ•´ç­–ç•¥ä»¥æé«˜ä¼ è¾“æ€§ï¼Œä»¥åŠåŸºäºåŠ¿åœºçš„æ©æ¨¡ä¼˜åŒ–æ–¹æ³•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPhysPatchåœ¨ç›®æ ‡å¯¹é½æ„ŸçŸ¥å’Œè§„åˆ’è¾“å‡ºæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½æˆåŠŸå¼•å¯¼MLLMä¸ºåŸºç¡€çš„ADç³»ç»Ÿã€‚æ­¤å¤–ï¼ŒPhysPatchç¡®ä¿å¯¹æŠ—æ€§è¡¥ä¸åœ¨ADåœºæ™¯çš„åˆç†åŒºåŸŸæ”¾ç½®ï¼Œä¿è¯å…¶åœ¨çœŸå®ä¸–ç•Œçš„å¼ºå¤§é€‚ç”¨æ€§å’Œéƒ¨ç½²èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<p>ä¸€ã€MLLMsåœ¨è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œä½†æ˜“å—å¯¹æŠ—æ€§æ”»å‡»å¨èƒã€‚å°¤å…¶æ˜¯å¯¹æŠ—æ€§è¡¥ä¸æ”»å‡»åœ¨å®é™…åœºæ™¯ä¸­å¯èƒ½é€ æˆä¸¥é‡å¨èƒã€‚<br>äºŒã€ç°æœ‰è¡¥ä¸æ”»å‡»æ–¹æ³•ä¸»è¦é’ˆå¯¹å¯¹è±¡æ£€æµ‹æ¨¡å‹è®¾è®¡ï¼Œæ— æ³•æœ‰æ•ˆåº”ç”¨äºMLLMsã€‚å› ä¸ºå®ƒä»¬æ— æ³•é€‚åº”MLLMsçš„å¤æ‚æ¶æ„å’Œå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚<br>ä¸‰ã€ä¸ºäº†å…‹æœè¿™äº›å±€é™ï¼Œæˆ‘ä»¬æå‡ºäº†PhysPatchæ¡†æ¶ã€‚å®ƒé’ˆå¯¹MLLMsçš„ADç³»ç»Ÿè¿›è¡Œä¼˜åŒ–è®¾è®¡ï¼Œé€šè¿‡è”åˆä¼˜åŒ–è¡¥ä¸çš„ä½ç½®ã€å½¢çŠ¶å’Œå†…å®¹æ¥æé«˜æ”»å‡»æ•ˆæœå’ŒçœŸå®ä¸–ç•Œé€‚ç”¨æ€§ã€‚<br>å››ã€PhysPatchå¼•å…¥åŸºäºè¯­ä¹‰çš„æ©æ¨¡åˆå§‹åŒ–ç­–ç•¥å®ç°é€¼çœŸçš„è¡¥ä¸æ”¾ç½®ï¼Œå¹¶é€šè¿‡SVDçš„å±€éƒ¨å¯¹é½æŸå¤±å’Œè¡¥ä¸å¼•å¯¼çš„è£å‰ªè°ƒæ•´ç­–ç•¥æé«˜ä¼ è¾“æ€§èƒ½ã€‚<br>äº”ã€é€šè¿‡å¤§é‡å®éªŒéªŒè¯ï¼ŒPhysPatchæ˜¾è‘—æé«˜äº†æ”»å‡»æ•ˆæœï¼Œå¹¶èƒ½æˆåŠŸå¼•å¯¼MLLMä¸ºåŸºç¡€çš„ADç³»ç»Ÿäº§ç”Ÿç›®æ ‡å¯¹é½çš„æ„ŸçŸ¥å’Œè§„åˆ’è¾“å‡ºã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05167">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0f5ca5b5518e99e59e21cb3d89e0f833.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e90321b1d33c6e72ba34bc945aefbd87.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c76893218ef2f74117ed2ceff32a5c17.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b1ed0cd89e64f26275f4e7453b9f50a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77f01dc173499690d7853a52beff43ea.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Navigating-Through-Paper-Flood-Advancing-LLM-based-Paper-Evaluation-through-Domain-Aware-Retrieval-and-Latent-Reasoning"><a href="#Navigating-Through-Paper-Flood-Advancing-LLM-based-Paper-Evaluation-through-Domain-Aware-Retrieval-and-Latent-Reasoning" class="headerlink" title="Navigating Through Paper Flood: Advancing LLM-based Paper Evaluation   through Domain-Aware Retrieval and Latent Reasoning"></a>Navigating Through Paper Flood: Advancing LLM-based Paper Evaluation   through Domain-Aware Retrieval and Latent Reasoning</h2><p><strong>Authors:Wuqiang Zheng, Yiyan Xu, Xinyu Lin, Chongming Gao, Wenjie Wang, Fuli Feng</strong></p>
<p>With the rapid and continuous increase in academic publications, identifying high-quality research has become an increasingly pressing challenge. While recent methods leveraging Large Language Models (LLMs) for automated paper evaluation have shown great promise, they are often constrained by outdated domain knowledge and limited reasoning capabilities. In this work, we present PaperEval, a novel LLM-based framework for automated paper evaluation that addresses these limitations through two key components: 1) a domain-aware paper retrieval module that retrieves relevant concurrent work to support contextualized assessments of novelty and contributions, and 2) a latent reasoning mechanism that enables deep understanding of complex motivations and methodologies, along with comprehensive comparison against concurrently related work, to support more accurate and reliable evaluation. To guide the reasoning process, we introduce a progressive ranking optimization strategy that encourages the LLM to iteratively refine its predictions with an emphasis on relative comparison. Experiments on two datasets demonstrate that PaperEval consistently outperforms existing methods in both academic impact and paper quality evaluation. In addition, we deploy PaperEval in a real-world paper recommendation system for filtering high-quality papers, which has gained strong engagement on social media â€“ amassing over 8,000 subscribers and attracting over 10,000 views for many filtered high-quality papers â€“ demonstrating the practical effectiveness of PaperEval. </p>
<blockquote>
<p>éšç€å­¦æœ¯å‡ºç‰ˆç‰©çš„å¿«é€Ÿå’ŒæŒç»­å¢é•¿ï¼Œè¯†åˆ«é«˜è´¨é‡ç ”ç©¶æˆä¸ºäº†ä¸€ä¸ªæ—¥ç›Šç´§è¿«çš„æŒ‘æˆ˜ã€‚è™½ç„¶æœ€è¿‘åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè‡ªåŠ¨åŒ–è®ºæ–‡è¯„ä¼°çš„æ–¹æ³•æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸å—åˆ°è¿‡æ—¶çš„é¢†åŸŸçŸ¥è¯†å’Œæœ‰é™çš„æ¨ç†èƒ½åŠ›çš„é™åˆ¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PaperEvalï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMçš„è‡ªåŠ¨åŒ–è®ºæ–‡è¯„ä¼°æ–°é¢–æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸¤ä¸ªå…³é”®ç»„ä»¶è§£å†³è¿™äº›é™åˆ¶ï¼š1ï¼‰ä¸€ä¸ªé¢†åŸŸæ„ŸçŸ¥çš„è®ºæ–‡æ£€ç´¢æ¨¡å—ï¼Œæ£€ç´¢ç›¸å…³çš„å¹¶è¡Œå·¥ä½œä»¥æ”¯æŒå¯¹æ–°é¢–æ€§å’Œè´¡çŒ®çš„ä¸Šä¸‹æ–‡è¯„ä¼°ï¼›2ï¼‰ä¸€ç§æ½œåœ¨æ¨ç†æœºåˆ¶ï¼Œå®ç°å¯¹å¤æ‚åŠ¨æœºå’Œæ–¹æ³•è®ºçš„æ·±å…¥ç†è§£ï¼Œä»¥åŠä¸æ”¯æŒæ›´ç²¾ç¡®å’Œå¯é çš„è¯„ä¼°çš„å¹¶è¡Œç›¸å…³å·¥ä½œçš„å…¨é¢æ¯”è¾ƒã€‚ä¸ºäº†å¼•å¯¼æ¨ç†è¿‡ç¨‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ¸è¿›å¼æ’åä¼˜åŒ–ç­–ç•¥ï¼Œé¼“åŠ±LLMé€šè¿‡ç›¸å¯¹æ¯”è¾ƒæ¥è¿­ä»£ä¼˜åŒ–å…¶é¢„æµ‹ã€‚åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPaperEvalåœ¨å­¦æœ¯å½±å“åŠ›å’Œè®ºæ–‡è´¨é‡è¯„ä¼°æ–¹é¢å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†PaperEvaléƒ¨ç½²åœ¨ç°å®ä¸–ç•Œä¸­çš„è®ºæ–‡æ¨èç³»ç»Ÿä¸­ï¼Œä»¥ç­›é€‰é«˜è´¨é‡è®ºæ–‡ï¼Œå®ƒåœ¨ç¤¾äº¤åª’ä½“ä¸Šè·å¾—äº†å¼ºçƒˆçš„å…³æ³¨åº¦â€”â€”ç§¯ç´¯äº†è¶…è¿‡8,000åè®¢é˜…è€…å’Œè¶…è¿‡1ä¸‡åæµè§ˆé‡ç­›é€‰å‡ºçš„é«˜è´¨é‡è®ºæ–‡çš„è§†å›¾â€”â€”è¯æ˜äº†PaperEvalçš„å®é™…æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05129v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨åŒ–è®ºæ–‡è¯„ä»·æ¡†æ¶â€”â€”PaperEvalã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸¤ä¸ªå…³é”®ç»„ä»¶è§£å†³äº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼šä¸€æ˜¯é¢†åŸŸæ„ŸçŸ¥çš„è®ºæ–‡æ£€ç´¢æ¨¡å—ï¼Œæ”¯æŒå¯¹æ–°é¢–æ€§å’Œè´¡çŒ®çš„ä¸Šä¸‹æ–‡è¯„ä¼°ï¼›äºŒæ˜¯æ½œåœ¨æ¨ç†æœºåˆ¶ï¼Œæ”¯æŒå¯¹å¤æ‚åŠ¨æœºå’Œæ–¹æ³•çš„æ·±å…¥ç†è§£ï¼Œä»¥åŠä¸ç›¸å…³è®ºæ–‡çš„å…¨é¢æ¯”è¾ƒï¼Œä»è€Œå®ç°æ›´å‡†ç¡®å¯é çš„è¯„ä»·ã€‚å®éªŒè¯æ˜ï¼ŒPaperEvalåœ¨å­¦æœ¯å½±å“åŠ›å’Œè®ºæ–‡è´¨é‡è¯„ä»·æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå°†å…¶éƒ¨ç½²äºå®é™…è®ºæ–‡æ¨èç³»ç»Ÿä¸­ï¼Œå¯æœ‰æ•ˆç­›é€‰é«˜è´¨é‡è®ºæ–‡ï¼Œåœ¨ç¤¾ä¼šåª’ä½“ä¸Šè·å¾—å¼ºçƒˆåå“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è®ºæ–‡æ•°é‡æ¿€å¢å¯¼è‡´é«˜è´¨é‡ç ”ç©¶è¯†åˆ«æˆä¸ºæŒ‘æˆ˜ã€‚</li>
<li>PaperEvalæ¡†æ¶åˆ©ç”¨LLMè¿›è¡Œè‡ªåŠ¨åŒ–è®ºæ–‡è¯„ä»·ã€‚</li>
<li>PaperEvalå…·å¤‡é¢†åŸŸæ„ŸçŸ¥çš„è®ºæ–‡æ£€ç´¢å’Œæ½œåœ¨æ¨ç†æœºåˆ¶ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>æ£€ç´¢æ¨¡å—æ”¯æŒä¸Šä¸‹æ–‡è¯„ä¼°ï¼Œæ¨ç†æœºåˆ¶æ”¯æŒå¯¹è®ºæ–‡çš„æ·±å…¥ç†è§£å’Œå…¨é¢æ¯”è¾ƒã€‚</li>
<li>PaperEvalåœ¨å­¦æœ¯å½±å“åŠ›å’Œè®ºæ–‡è´¨é‡è¯„ä»·æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>PaperEvalå·²æˆåŠŸéƒ¨ç½²äºå®é™…è®ºæ–‡æ¨èç³»ç»Ÿï¼Œè·å¾—ç¤¾ä¼šåª’ä½“ä¸Šçš„å¼ºçƒˆåå“ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-049baae71ae433a89451cd7dfc15373e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81d3c17e38284c09946ac466d01416c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0af2e4db66ae7f481a7a3de7d8adfbdb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09acd43be786200682258f9440108171.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c2d76eecaabb6f2e1439a0841120ec56.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-25a56c6761c0480b91ef3be33a13768d.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-09  On the Generalization of SFT A Reinforcement Learning Perspective with   Reward Rectification
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-06312f0b186ac5ee71a79664d1504686.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-09  RAP Real-time Audio-driven Portrait Animation with Video Diffusion   Transformer
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29739.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
