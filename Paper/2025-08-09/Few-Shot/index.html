<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-09  SMOL-MapSeg Show Me One Label">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-20dc89bab4558739a8576f759266faff.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    51 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-09-æ›´æ–°"><a href="#2025-08-09-æ›´æ–°" class="headerlink" title="2025-08-09 æ›´æ–°"></a>2025-08-09 æ›´æ–°</h1><h2 id="SMOL-MapSeg-Show-Me-One-Label"><a href="#SMOL-MapSeg-Show-Me-One-Label" class="headerlink" title="SMOL-MapSeg: Show Me One Label"></a>SMOL-MapSeg: Show Me One Label</h2><p><strong>Authors:Yunshuang Yuan, Frank Thiemann, Thorsten Dahms, Monika Sester</strong></p>
<p>Historical maps are valuable for studying changes to the Earthâ€™s surface. With the rise of deep learning, models like UNet have been used to extract information from these maps through semantic segmentation. Recently, pre-trained foundation models have shown strong performance across domains such as autonomous driving, medical imaging, and industrial inspection. However, they struggle with historical maps. These models are trained on modern or domain-specific images, where patterns can be tied to predefined concepts through common sense or expert knowledge. Historical maps lack such consistency â€“ similar concepts can appear in vastly different shapes and styles. To address this, we propose On-Need Declarative (OND) knowledge-based prompting, which introduces explicit prompts to guide the model on what patterns correspond to which concepts. This allows users to specify the target concept and pattern during inference (on-need inference). We implement this by replacing the prompt encoder of the foundation model SAM with our OND prompting mechanism and fine-tune it on historical maps. The resulting model is called SMOL-MapSeg (Show Me One Label). Experiments show that SMOL-MapSeg can accurately segment classes defined by OND knowledge. It can also adapt to unseen classes through few-shot fine-tuning. Additionally, it outperforms a UNet-based baseline in average segmentation performance. </p>
<blockquote>
<p>å†å²åœ°å›¾å¯¹äºç ”ç©¶åœ°çƒè¡¨é¢çš„å˜åŒ–å…·æœ‰é‡è¦ä»·å€¼ã€‚éšç€æ·±åº¦å­¦ä¹ çš„å…´èµ·ï¼Œå¦‚UNetç­‰æ¨¡å‹å·²ç”¨äºé€šè¿‡è¯­ä¹‰åˆ†å‰²ä»è¿™äº›åœ°å›¾ä¸­æå–ä¿¡æ¯ã€‚æœ€è¿‘ï¼Œé¢„è®­ç»ƒçš„åŸºé‡‘ä¼šæ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—æˆåƒå’Œå·¥ä¸šæ£€æµ‹ç­‰é¢†åŸŸè¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å¤„ç†å†å²åœ°å›¾æ—¶é‡åˆ°äº†å›°éš¾ã€‚è¿™äº›æ¨¡å‹æ˜¯åœ¨ç°ä»£æˆ–ç‰¹å®šé¢†åŸŸçš„å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œå…¶ä¸­æ¨¡å¼å¯ä»¥é€šè¿‡å¸¸è¯†æˆ–ä¸“ä¸šçŸ¥è¯†ä¸é¢„å®šä¹‰çš„æ¦‚å¿µç›¸å…³è”ã€‚å†å²åœ°å›¾ç¼ºä¹è¿™ç§ä¸€è‡´æ€§â€”â€”ç›¸ä¼¼æ¦‚å¿µå¯èƒ½ä»¥éå¸¸ä¸åŒçš„å½¢çŠ¶å’Œé£æ ¼å‡ºç°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æŒ‰éœ€å£°æ˜ï¼ˆONDï¼‰çŸ¥è¯†æç¤ºæ–¹æ³•ï¼Œå®ƒé€šè¿‡å¼•å…¥æ˜ç¡®æç¤ºæ¥æŒ‡å¯¼æ¨¡å‹äº†è§£å“ªäº›æ¨¡å¼å¯¹åº”å“ªäº›æ¦‚å¿µã€‚è¿™å…è®¸ç”¨æˆ·åœ¨æ¨ç†è¿‡ç¨‹ä¸­æŒ‡å®šç›®æ ‡æ¦‚å¿µå’Œæ¨¡å¼ï¼ˆæŒ‰éœ€æ¨ç†ï¼‰ã€‚æˆ‘ä»¬é€šè¿‡æ›¿æ¢åŸºç¡€æ¨¡å‹SAMçš„æç¤ºç¼–ç å™¨ï¼Œé‡‡ç”¨æˆ‘ä»¬çš„ONDæç¤ºæœºåˆ¶å¹¶åœ¨å†å²åœ°å›¾ä¸Šå¯¹å…¶è¿›è¡Œå¾®è°ƒæ¥å®ç°è¿™ä¸€ç‚¹ã€‚ç»“æœæ¨¡å‹è¢«ç§°ä¸ºSMOL-MapSegï¼ˆæ˜¾ç¤ºæˆ‘ä¸€ä¸ªæ ‡ç­¾ï¼‰ã€‚å®éªŒè¡¨æ˜ï¼ŒSMOL-MapSegå¯ä»¥å‡†ç¡®åœ°åˆ†å‰²ç”±ONDçŸ¥è¯†å®šä¹‰çš„ç±»åˆ«ã€‚å®ƒè¿˜å¯ä»¥é€šè¿‡å°‘é‡æ ·æœ¬å¾®è°ƒæ¥é€‚åº”æœªè§è¿‡çš„ç±»åˆ«ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨å¹³å‡åˆ†å‰²æ€§èƒ½ä¸Šä¼˜äºåŸºäºUNetçš„åŸºçº¿æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05501v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å†å²åœ°å›¾å¯¹äºç ”ç©¶åœ°çƒè¡¨é¢çš„å˜åŒ–å…·æœ‰é‡è¦ä»·å€¼ã€‚éšç€æ·±åº¦å­¦ä¹ çš„å…´èµ·ï¼ŒUNetç­‰æ¨¡å‹å·²è¢«ç”¨äºä»åœ°å›¾ä¸­æå–ä¿¡æ¯ï¼Œè¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚å°½ç®¡é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹åœ¨è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒå’Œå·¥ä¸šæ£€æµ‹ç­‰é¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬å¤„ç†å†å²åœ°å›¾æ—¶å´é¢ä¸´å›°éš¾ã€‚åŸå› åœ¨äºï¼Œå†å²åœ°å›¾ç¼ºä¹ç°ä»£æˆ–ç‰¹å®šé¢†åŸŸå›¾åƒä¸­çš„ä¸€è‡´æ€§ï¼Œç›¸åŒæ¦‚å¿µå¯èƒ½å‘ˆç°å‡ºå„ç§ä¸åŒçš„å½¢æ€å’Œé£æ ¼ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºéœ€æ±‚å®£å‘Šï¼ˆONDï¼‰çŸ¥è¯†æç¤ºçš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥æ˜ç¡®çš„æç¤ºæ¥æŒ‡å¯¼æ¨¡å‹è¯†åˆ«ä¸åŒæ¨¡å¼ä¸æ¦‚å¿µä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚å®éªŒè¡¨æ˜ï¼ŒSMOL-MapSegæ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åˆ†å‰²ç”±ONDçŸ¥è¯†å®šä¹‰çš„ç±»åˆ«ï¼Œå¹¶èƒ½é€‚åº”æœªè§è¿‡çš„ç±»åˆ«è¿›è¡Œå°æ ·æœ¬å¾®è°ƒï¼Œå…¶å¹³å‡åˆ†å‰²æ€§èƒ½ä¼˜äºåŸºäºUNetçš„åŸºçº¿æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å†å²åœ°å›¾å¯¹äºç ”ç©¶åœ°çƒè¡¨é¢å˜åŒ–å…·æœ‰é‡è¦ä»·å€¼ã€‚</li>
<li>UNetç­‰æ¨¡å‹å·²è¢«ç”¨äºä»åœ°å›¾ä¸­æå–ä¿¡æ¯å¹¶è¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚</li>
<li>é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹åœ¨å¤„ç†å†å²åœ°å›¾æ—¶é¢ä¸´å›°éš¾ï¼Œå› ä¸ºå†å²åœ°å›¾ç¼ºä¹ä¸€è‡´æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºéœ€æ±‚å®£å‘Šï¼ˆONDï¼‰çŸ¥è¯†æç¤ºçš„æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>ONDçŸ¥è¯†æç¤ºå¯ä»¥æŒ‡å¯¼æ¨¡å‹è¯†åˆ«ä¸åŒæ¨¡å¼ä¸æ¦‚å¿µä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚</li>
<li>SMOL-MapSegæ¨¡å‹é€šè¿‡ç»“åˆONDçŸ¥è¯†å’Œå†å²åœ°å›¾çš„å¾®è°ƒï¼Œå®ç°äº†å‡†ç¡®çš„è¯­ä¹‰åˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05501">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20dc89bab4558739a8576f759266faff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e08c4b6578b68eb0b0554e56eb792af.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Textual-Inversion-for-Efficient-Adaptation-of-Open-Vocabulary-Object-Detectors-Without-Forgetting"><a href="#Textual-Inversion-for-Efficient-Adaptation-of-Open-Vocabulary-Object-Detectors-Without-Forgetting" class="headerlink" title="Textual Inversion for Efficient Adaptation of Open-Vocabulary Object   Detectors Without Forgetting"></a>Textual Inversion for Efficient Adaptation of Open-Vocabulary Object   Detectors Without Forgetting</h2><p><strong>Authors:Frank Ruis, Gertjan Burghouts, Hugo Kuijf</strong></p>
<p>Recent progress in large pre-trained vision language models (VLMs) has reached state-of-the-art performance on several object detection benchmarks and boasts strong zero-shot capabilities, but for optimal performance on specific targets some form of finetuning is still necessary. While the initial VLM weights allow for great few-shot transfer learning, this usually involves the loss of the original natural language querying and zero-shot capabilities. Inspired by the success of Textual Inversion (TI) in personalizing text-to-image diffusion models, we propose a similar formulation for open-vocabulary object detection. TI allows extending the VLM vocabulary by learning new or improving existing tokens to accurately detect novel or fine-grained objects from as little as three examples. The learned tokens are completely compatible with the original VLM weights while keeping them frozen, retaining the original modelâ€™s benchmark performance, and leveraging its existing capabilities such as zero-shot domain transfer (e.g., detecting a sketch of an object after training only on real photos). The storage and gradient calculations are limited to the token embedding dimension, requiring significantly less compute than full-model fine-tuning. We evaluated whether the method matches or outperforms the baseline methods that suffer from forgetting in a wide variety of quantitative and qualitative experiments. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è¿›å±•å·²ç»åœ¨å¤šä¸ªç›®æ ‡æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å…·å¤‡å¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚ä½†æ˜¯ï¼Œä¸ºäº†åœ¨ç‰¹å®šç›®æ ‡ä¸Šè·å¾—æœ€ä½³æ€§èƒ½ï¼Œä»ç„¶éœ€è¦è¿›è¡ŒæŸç§å½¢å¼çš„å¾®è°ƒã€‚è™½ç„¶åˆå§‹çš„VLMæƒé‡å…è®¸å¾ˆå¥½çš„å°‘é‡è½¬ç§»å­¦ä¹ ï¼Œä½†è¿™é€šå¸¸ä¼šå¤±å»åŸå§‹çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢å’Œé›¶æ ·æœ¬èƒ½åŠ›ã€‚å—æ–‡æœ¬åè½¬ï¼ˆTIï¼‰åœ¨ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬ä¸ºå¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹æå‡ºäº†ç±»ä¼¼çš„å…¬å¼ã€‚TIå…è®¸é€šè¿‡å­¦ä¹ æ–°ä»¤ç‰Œæˆ–æ”¹è¿›ç°æœ‰ä»¤ç‰Œæ¥æ‰©å±•VLMè¯æ±‡ï¼Œä»è€Œä»…ä»ä¸‰ä¸ªç¤ºä¾‹ä¸­å‡†ç¡®æ£€æµ‹æ–°å‹æˆ–ç»†ç²’åº¦å¯¹è±¡ã€‚æ‰€å­¦åˆ°çš„ä»¤ç‰Œä¸åŸå§‹VLMæƒé‡å®Œå…¨å…¼å®¹ï¼ŒåŒæ—¶ä¿æŒå†»ç»“çŠ¶æ€ï¼Œä¿ç•™äº†åŸå§‹æ¨¡å‹çš„åŸºå‡†æµ‹è¯•æ€§èƒ½ï¼Œå¹¶å¯ä»¥åˆ©ç”¨å…¶ç°æœ‰åŠŸèƒ½ï¼Œå¦‚é›¶æ ·æœ¬åŸŸè¿ç§»ï¼ˆä¾‹å¦‚ï¼Œåœ¨ä»…å¯¹çœŸå®ç…§ç‰‡è¿›è¡Œè®­ç»ƒåæ£€æµ‹å¯¹è±¡çš„è‰å›¾ï¼‰ã€‚å­˜å‚¨å’Œæ¢¯åº¦è®¡ç®—ä»…é™äºä»¤ç‰ŒåµŒå…¥ç»´åº¦ï¼Œå› æ­¤æ‰€éœ€çš„è®¡ç®—é‡è¿œè¿œå°äºå…¨æ¨¡å‹å¾®è°ƒã€‚æˆ‘ä»¬é€šè¿‡å„ç§å®šé‡å’Œå®šæ€§å®éªŒè¯„ä¼°äº†è¯¥æ–¹æ³•æ˜¯å¦åŒ¹é…æˆ–è¶…è¶ŠåŸºçº¿æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•ä¼šé­å—é—å¿˜çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05323v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æœ€æ–°è¿›å±•å·²åœ¨å¤šä¸ªç›®æ ‡æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå¹¶å…·å¤‡å¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œä½†é’ˆå¯¹ç‰¹å®šç›®æ ‡çš„æœ€ä½³æ€§èƒ½ä»éœ€è¦è¿›è¡ŒæŸç§å½¢å¼çš„å¾®è°ƒã€‚å°½ç®¡åˆå§‹çš„VLMæƒé‡å…è®¸å¾ˆå¥½çš„å°‘é‡è½¬ç§»å­¦ä¹ ï¼Œä½†è¿™é€šå¸¸ä¼šå¤±å»åŸå§‹çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢å’Œé›¶æ ·æœ¬èƒ½åŠ›ã€‚å—æ–‡æœ¬åè½¬ï¼ˆTIï¼‰åœ¨ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹çš„ç±»ä¼¼å…¬å¼ã€‚TIé€šè¿‡å­¦ä¹ æ–°ä»¤ç‰Œæˆ–æ”¹è¿›ç°æœ‰ä»¤ç‰Œæ¥å‡†ç¡®æ£€æµ‹æ¥è‡ªä¸‰ä¸ªç¤ºä¾‹çš„æ–°é¢–æˆ–ç»†ç²’åº¦å¯¹è±¡ã€‚æ‰€å­¦åˆ°çš„ä»¤ç‰Œä¸åŸå§‹çš„VLMæƒé‡å®Œå…¨å…¼å®¹ï¼ŒåŒæ—¶ä¿æŒå†»ç»“çŠ¶æ€ï¼Œä¿ç•™äº†åŸå§‹æ¨¡å‹çš„åŸºå‡†æµ‹è¯•æ€§èƒ½ï¼Œå¹¶åˆ©ç”¨äº†å…¶ç°æœ‰åŠŸèƒ½ï¼Œå¦‚é›¶æ ·æœ¬åŸŸè½¬ç§»ï¼ˆä¾‹å¦‚ï¼Œä»…åœ¨çœŸå®ç…§ç‰‡ä¸Šè¿›è¡Œè®­ç»ƒåæ£€æµ‹å¯¹è±¡çš„è‰å›¾ï¼‰ã€‚å­˜å‚¨å’Œæ¢¯åº¦è®¡ç®—ä»…é™äºä»¤ç‰ŒåµŒå…¥ç»´åº¦ï¼Œä¸å…¨æ¨¡å‹å¾®è°ƒç›¸æ¯”ï¼Œéœ€è¦æ›´å°‘çš„è®¡ç®—é‡ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®šé‡å’Œå®šæ€§å®éªŒï¼Œè¯„ä¼°è¯¥æ–¹æ³•æ˜¯å¦è¾¾åˆ°æˆ–è¶…è¶Šäº†åŸºçº¿æ–¹æ³•çš„è¡¨ç°ã€‚åŸºçº¿æ–¹æ³•åœ¨å¤„ç†æ—¶ä¼šé¢ä¸´é—å¿˜é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç›®æ ‡æ£€æµ‹æ–¹é¢å·²è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å‡†ï¼Œå…·å¤‡å¼ºå¤§çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>å°½ç®¡åˆå§‹æ¨¡å‹æƒé‡å¯å®ç°ä¼˜ç§€çš„å°‘é‡è½¬ç§»å­¦ä¹ ï¼Œä½†ç»´æŒåŸå§‹çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢å’Œé›¶æ ·æœ¬èƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>æ–‡æœ¬åè½¬ï¼ˆTIï¼‰æŠ€æœ¯å¯ç”¨äºå¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹ï¼Œé€šè¿‡æ–°å¢æˆ–æ”¹è¿›ä»¤ç‰Œä»¥å‡†ç¡®è¯†åˆ«æ–°é¢–æˆ–ç»†ç²’åº¦å¯¹è±¡ï¼Œä»…éœ€å°‘é‡ç¤ºä¾‹ã€‚</li>
<li>æ‰€å¼•å…¥çš„ä»¤ç‰Œä¸åŸå§‹VLMæƒé‡å…¼å®¹ï¼Œå¯ä¿æŒæ¨¡å‹æ€§èƒ½å¹¶å¢å¼ºç°æœ‰åŠŸèƒ½ï¼Œå¦‚é›¶æ ·æœ¬åŸŸè½¬ç§»ã€‚</li>
<li>è¯¥æ–¹æ³•å­˜å‚¨å’Œè®¡ç®—éœ€æ±‚ç›¸å¯¹è¾ƒä½ï¼Œä»…é™äºä»¤ç‰ŒåµŒå…¥ç»´åº¦ï¼Œè¾ƒå…¨æ¨¡å‹å¾®è°ƒæ›´ä¸ºé«˜æ•ˆã€‚</li>
<li>æ–¹æ³•åœ¨å¹¿æ³›çš„å®šé‡å’Œå®šæ€§å®éªŒä¸­è¡¨ç°ä¼˜ç§€ï¼Œä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œæœ‰æ•ˆé¿å…äº†é—å¿˜é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05323">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31d3d19681f76ccc40cd3e6fd0ad44d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b8d15b0360500c143207037a8d34773.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b00d609b4b549ad18a3d4201207cf8b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Textual-and-Visual-Guided-Task-Adaptation-for-Source-Free-Cross-Domain-Few-Shot-Segmentation"><a href="#Textual-and-Visual-Guided-Task-Adaptation-for-Source-Free-Cross-Domain-Few-Shot-Segmentation" class="headerlink" title="Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain   Few-Shot Segmentation"></a>Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain   Few-Shot Segmentation</h2><p><strong>Authors:Jianming Liu, Wenlong Qiu, Haitao Wei</strong></p>
<p>Few-Shot Segmentation(FSS) aims to efficient segmentation of new objects with few labeled samples. However, its performance significantly degrades when domain discrepancies exist between training and deployment. Cross-Domain Few-Shot Segmentation(CD-FSS) is proposed to mitigate such performance degradation. Current CD-FSS methods primarily sought to develop segmentation models on a source domain capable of cross-domain generalization. However, driven by escalating concerns over data privacy and the imperative to minimize data transfer and training expenses, the development of source-free CD-FSS approaches has become essential. In this work, we propose a source-free CD-FSS method that leverages both textual and visual information to facilitate target domain task adaptation without requiring source domain data. Specifically, we first append Task-Specific Attention Adapters (TSAA) to the feature pyramid of a pretrained backbone, which adapt multi-level features extracted from the shared pre-trained backbone to the target task. Then, the parameters of the TSAA are trained through a Visual-Visual Embedding Alignment (VVEA) module and a Text-Visual Embedding Alignment (TVEA) module. The VVEA module utilizes global-local visual features to align image features across different views, while the TVEA module leverages textual priors from pre-aligned multi-modal features (e.g., from CLIP) to guide cross-modal adaptation. By combining the outputs of these modules through dense comparison operations and subsequent fusion via skip connections, our method produces refined prediction masks. Under both 1-shot and 5-shot settings, the proposed approach achieves average segmentation accuracy improvements of 2.18% and 4.11%, respectively, across four cross-domain datasets, significantly outperforming state-of-the-art CD-FSS methods. Code are available at <a target="_blank" rel="noopener" href="https://github.com/ljm198134/TVGTANet">https://github.com/ljm198134/TVGTANet</a>. </p>
<blockquote>
<p>è·¨åŸŸå°æ ·æœ¬åˆ†å‰²ï¼ˆCD-FSSï¼‰æ—¨åœ¨è§£å†³åœ¨æ–°å¯¹è±¡åªæœ‰å°‘é‡æ ‡è®°æ ·æœ¬çš„æƒ…å†µä¸‹è¿›è¡Œæœ‰æ•ˆåˆ†å‰²çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œå½“è®­ç»ƒå’Œéƒ¨ç½²ä¹‹é—´å­˜åœ¨åŸŸå·®å¼‚æ—¶ï¼Œå…¶æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚é’ˆå¯¹è¿™ç§æ€§èƒ½ä¸‹é™ï¼Œæå‡ºäº†è·¨åŸŸå°æ ·æœ¬åˆ†å‰²ï¼ˆCD-FSSï¼‰æ–¹æ³•ï¼Œå½“å‰CD-FSSæ–¹æ³•ä¸»è¦å¯»æ±‚åœ¨æºåŸŸä¸Šå¼€å‘èƒ½å¤Ÿè·¨åŸŸæ¨å¹¿çš„åˆ†å‰²æ¨¡å‹ã€‚ç„¶è€Œï¼Œç”±äºå¯¹æ•°æ®éšç§çš„æ‹…å¿§ä»¥åŠå‡å°‘æ•°æ®ä¼ è¾“å’ŒåŸ¹è®­è´¹ç”¨çš„å¿…è¦æ€§æ—¥ç›ŠåŠ å‰§ï¼Œæ— æºåŸŸCD-FSSæ–¹æ³•çš„å‘å±•å˜å¾—è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— æºçš„CD-FSSæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯æ¥ä¿ƒè¿›ç›®æ ‡åŸŸçš„ä»»åŠ¡é€‚åº”ï¼Œæ— éœ€æºåŸŸæ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå°†ä»»åŠ¡ç‰¹å®šæ³¨æ„åŠ›é€‚é…å™¨ï¼ˆTSAAï¼‰æ·»åŠ åˆ°é¢„è®­ç»ƒéª¨å¹²çš„ç‰¹å¾é‡‘å­—å¡”ä¸Šï¼Œè¯¥é€‚é…å™¨å°†æ¥è‡ªå…±äº«é¢„è®­ç»ƒéª¨å¹²çš„å¤šçº§ç‰¹å¾é€‚åº”åˆ°ç›®æ ‡ä»»åŠ¡ã€‚ç„¶åï¼Œé€šè¿‡è§†è§‰è§†è§‰åµŒå…¥å¯¹é½ï¼ˆVVEAï¼‰æ¨¡å—å’Œæ–‡æœ¬è§†è§‰åµŒå…¥å¯¹é½ï¼ˆTVEAï¼‰æ¨¡å—è®­ç»ƒTSAAçš„å‚æ•°ã€‚VVEAæ¨¡å—åˆ©ç”¨å…¨å±€å±€éƒ¨è§†è§‰ç‰¹å¾æ¥å¯¹é½ä¸åŒè§†è§’çš„å›¾åƒç‰¹å¾ï¼Œè€ŒTVEAæ¨¡å—åˆ™åˆ©ç”¨æ¥è‡ªé¢„å¯¹é½å¤šæ¨¡æ€ç‰¹å¾ï¼ˆä¾‹å¦‚æ¥è‡ªCLIPï¼‰çš„æ–‡æœ¬å…ˆéªŒçŸ¥è¯†æ¥å¼•å¯¼è·¨æ¨¡æ€é€‚åº”ã€‚é€šè¿‡å°†è¿™äº›æ¨¡å—çš„è¾“ å‡ºé€šè¿‡å¯†é›†æ¯”è¾ƒæ“ä½œå’Œéšåçš„è·³è·ƒè¿æ¥èåˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆäº†ç²¾ç»†çš„é¢„æµ‹æ©æ¨¡ã€‚åœ¨1-shotå’Œ5-shotè®¾ç½®ä¸‹ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å››ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„å¹³å‡åˆ†å‰²ç²¾åº¦åˆ†åˆ«æé«˜äº†2.18%å’Œ4.11%ï¼Œæ˜¾è‘—ä¼˜äºæœ€æ–°çš„CD-FSSæ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ljm198134/TVGTANet%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ljm198134/TVGTANetä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05213v1">PDF</a> 10 pages,Accepted at ACMMM2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†Few-Shot Segmentationï¼ˆFSSï¼‰åœ¨æ–°å¯¹è±¡åˆ†å‰²é¢†åŸŸçš„åº”ç”¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨é¢†åŸŸå·®å¼‚æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†Cross-Domain Few-Shot Segmentationï¼ˆCD-FSSï¼‰æ¥å‡è½»æ€§èƒ½ä¸‹é™ã€‚å½“å‰CD-FSSæ–¹æ³•ä¸»è¦å¯»æ±‚åœ¨æºé¢†åŸŸå¼€å‘èƒ½å¤Ÿè·¨é¢†åŸŸæ¨å¹¿çš„åˆ†å‰²æ¨¡å‹ã€‚ç„¶è€Œï¼Œç”±äºå¯¹æ•°æ®éšç§çš„æ‹…å¿§ä»¥åŠå‡å°‘æ•°æ®ä¼ è¾“å’ŒåŸ¹è®­è´¹ç”¨çš„å¿…è¦æ€§ï¼Œæºå…è´¹çš„CD-FSSæ–¹æ³•çš„å‘å±•å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æºå…è´¹çš„CD-FSSæ–¹æ³•ï¼Œåˆ©ç”¨æ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯æ¥ä¿ƒè¿›ç›®æ ‡é¢†åŸŸçš„ä»»åŠ¡é€‚åº”ï¼Œè€Œæ— éœ€æºé¢†åŸŸæ•°æ®ã€‚é€šè¿‡æ·»åŠ ä»»åŠ¡ç‰¹å®šæ³¨æ„åŠ›é€‚é…å™¨ï¼ˆTSAAï¼‰å’Œè®­ç»ƒè¿™äº›å‚æ•°ï¼Œè¯¥æ–¹æ³•å®ç°äº†åœ¨è·¨é¢†åŸŸæ•°æ®é›†ä¸Šçš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shot Segmentation (FSS) åœ¨æ–°å¯¹è±¡åˆ†å‰²æ–¹é¢è¡¨ç°ä¼˜ç§€ï¼Œä½†åœ¨é¢†åŸŸå·®å¼‚å¤§æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>Cross-Domain Few-Shot Segmentation (CD-FSS) è¢«æå‡ºç”¨äºè§£å†³é¢†åŸŸå·®å¼‚å¸¦æ¥çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</li>
<li>å½“å‰CD-FSSæ–¹æ³•ä¸»è¦å¯»æ±‚åœ¨æºé¢†åŸŸå¼€å‘å¯è·¨é¢†åŸŸæ¨å¹¿çš„åˆ†å‰²æ¨¡å‹ã€‚</li>
<li>å‡ºäºå¯¹æ•°æ®éšç§çš„è€ƒè™‘åŠå‡å°‘æ•°æ®ä¼ è¾“å’ŒåŸ¹è®­è´¹ç”¨çš„éœ€è¦ï¼Œæºå…è´¹çš„CD-FSSæ–¹æ³•å˜å¾—é‡è¦ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡ç»“åˆæ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯ï¼Œæå‡ºäº†ä¸€ç§æºå…è´¹çš„CD-FSSæ–¹æ³•ï¼Œä¿ƒè¿›ç›®æ ‡é¢†åŸŸçš„ä»»åŠ¡é€‚åº”ï¼Œæ— éœ€æºé¢†åŸŸæ•°æ®ã€‚</li>
<li>æ–¹æ³•é€šè¿‡æ·»åŠ ä»»åŠ¡ç‰¹å®šæ³¨æ„åŠ›é€‚é…å™¨ï¼ˆTSAAï¼‰å’Œè®­ç»ƒè¿™äº›å‚æ•°ï¼Œå®ç°äº†è·¨é¢†åŸŸæ•°æ®é›†çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05213">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1e735d07e4807f3d9455aeb1501e4ee5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b22a5255033f36f7e8873c02decd9d30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a647b377400526928aea96233779cab0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-75aa781493753b79179378938b85cb97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2a51106f08793ff77e4b259ba17583b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58322df6a7b7c997c25ce6a8af65cb0f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Sculpting-Margin-Penalty-Intra-Task-Adapter-Merging-and-Classifier-Calibration-for-Few-Shot-Class-Incremental-Learning"><a href="#Sculpting-Margin-Penalty-Intra-Task-Adapter-Merging-and-Classifier-Calibration-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Sculpting Margin Penalty: Intra-Task Adapter Merging and Classifier   Calibration for Few-Shot Class-Incremental Learning"></a>Sculpting Margin Penalty: Intra-Task Adapter Merging and Classifier   Calibration for Few-Shot Class-Incremental Learning</h2><p><strong>Authors:Liang Bai, Hong Song, Jinfu Li, Yucong Lin, Jingfan Fan, Tianyu Fu, Danni Ai, Deqiang Xiao, Jian Yang</strong></p>
<p>Real-world applications often face data privacy constraints and high acquisition costs, making the assumption of sufficient training data in incremental tasks unrealistic and leading to significant performance degradation in class-incremental learning. Forward-compatible learning, which prospectively prepares for future tasks during base task training, has emerged as a promising solution for Few-Shot Class-Incremental Learning (FSCIL). However, existing methods still struggle to balance base-class discriminability and new-class generalization. Moreover, limited access to original data during incremental tasks often results in ambiguous inter-class decision boundaries. To address these challenges, we propose SMP (Sculpting Margin Penalty), a novel FSCIL method that strategically integrates margin penalties at different stages within the parameter-efficient fine-tuning paradigm. Specifically, we introduce the Margin-aware Intra-task Adapter Merging (MIAM) mechanism for base task learning. MIAM trains two sets of low-rank adapters with distinct classification losses: one with a margin penalty to enhance base-class discriminability, and the other without margin constraints to promote generalization to future new classes. These adapters are then adaptively merged to improve forward compatibility. For incremental tasks, we propose a Margin Penalty-based Classifier Calibration (MPCC) strategy to refine decision boundaries by fine-tuning classifiers on all seen classesâ€™ embeddings with a margin penalty. Extensive experiments on CIFAR100, ImageNet-R, and CUB200 demonstrate that SMP achieves state-of-the-art performance in FSCIL while maintaining a better balance between base and new classes. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œåº”ç”¨å¸¸å¸¸é¢ä¸´æ•°æ®éšç§é™åˆ¶å’Œé«˜é‡‡é›†æˆæœ¬ï¼Œä½¿å¾—å¢é‡ä»»åŠ¡ä¸­å‡è®¾å­˜åœ¨è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®å˜å¾—ä¸ç°å®ï¼Œå¹¶ä¸”åœ¨ç±»å¢é‡å­¦ä¹ ï¼ˆClass-Incremental Learningï¼‰ä¸­å¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚å‰ç»æ€§å­¦ä¹ ï¼ˆå‰ç»æ€§åœ°åœ¨åŸºç¡€ä»»åŠ¡è®­ç»ƒæ—¶å‡†å¤‡æœªæ¥çš„ä»»åŠ¡ï¼‰å·²ç»æˆä¸ºå°æ ·ä¾‹ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰çš„ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä»ç„¶éš¾ä»¥å¹³è¡¡åŸºç¡€ç±»çš„è¾¨è¯†æ€§å’Œæ–°ç±»çš„æ³›åŒ–æ€§ã€‚æ­¤å¤–ï¼Œå¢é‡ä»»åŠ¡æœŸé—´å¯¹åŸå§‹æ•°æ®çš„æœ‰é™è®¿é—®é€šå¸¸å¯¼è‡´æ¨¡ç³Šçš„ç±»é—´å†³ç­–è¾¹ç•Œã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SMPï¼ˆé›•åˆ»è¾¹ç•Œæƒ©ç½šï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„FSCILæ–¹æ³•ï¼Œå®ƒåœ¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒèŒƒå¼ä¸­ï¼Œåœ¨ä¸åŒçš„é˜¶æ®µæˆ˜ç•¥æ€§åœ°æ•´åˆäº†è¾¹ç•Œæƒ©ç½šã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢å‘åŸºç¡€ä»»åŠ¡å­¦ä¹ çš„â€œè¾¹ç•Œæ„ŸçŸ¥ä»»åŠ¡å†…é€‚é…å™¨èåˆï¼ˆMIAMï¼‰â€æœºåˆ¶ã€‚MIAMè®­ç»ƒäº†ä¸¤å¥—å…·æœ‰ä¸åŒåˆ†ç±»æŸå¤±çš„ä½ä½é€‚é…å™¨ï¼šä¸€å¥—å¸¦æœ‰è¾¹ç•Œæƒ©ç½šä»¥å¢å¼ºåŸºç¡€ç±»çš„è¾¨è¯†æ€§ï¼Œå¦ä¸€å¥—ä¸å¸¦è¾¹ç•Œçº¦æŸä»¥ä¿ƒè¿›æœªæ¥æ–°ç±»çš„æ³›åŒ–ã€‚è¿™äº›é€‚é…å™¨éšåè¿›è¡Œè‡ªé€‚åº”èåˆä»¥æé«˜å‰ç»æ€§å…¼å®¹æ€§ã€‚å¯¹äºå¢é‡ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè¾¹ç•Œæƒ©ç½šçš„åˆ†ç±»å™¨æ ¡å‡†ï¼ˆMPCCï¼‰ç­–ç•¥ï¼Œé€šè¿‡å¾®è°ƒæ‰€æœ‰å·²è§ç±»çš„åµŒå…¥ä¸Šçš„åˆ†ç±»å™¨æ¥å®Œå–„å†³ç­–è¾¹ç•Œã€‚åœ¨CIFAR100ã€ImageNet-Rå’ŒCUB200ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSMPåœ¨FSCILä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶åŸºç¡€ç±»å’Œæ–°ç±»ä¹‹é—´ä¿æŒäº†æ›´å¥½çš„å¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05094v1">PDF</a> 13 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¢å‘å°‘é‡ç±»åˆ«å¢é‡çš„å­¦ä¹ ï¼ˆFSCILï¼‰çš„æŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆã€‚é’ˆå¯¹å®é™…åº”ç”¨ä¸­çš„æ•°æ®éšç§çº¦æŸå’Œé«˜æ˜‚çš„è·å–æˆæœ¬é—®é¢˜ï¼Œæå‡ºäº†SMPæ–¹æ³•ï¼Œé€šè¿‡åœ¨ä¸åŒé˜¶æ®µå¼•å…¥è¾¹è·æƒ©ç½šï¼Œåœ¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ¡†æ¶ä¸‹å®ç°åŸºç¡€ä»»åŠ¡å­¦ä¹ å’Œå¢é‡ä»»åŠ¡å­¦ä¹ çš„å¹³è¡¡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨CIFAR100ã€ImageNet-Rå’ŒCUB200æ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®é™…åº”ç”¨ä¸­é¢ä¸´æ•°æ®éšç§çº¦æŸå’Œé«˜æ˜‚çš„è·å–æˆæœ¬é—®é¢˜ï¼Œå¯¼è‡´å°‘é‡ç±»åˆ«å¢é‡çš„å­¦ä¹ ï¼ˆFSCILï¼‰ä¸­å‡è®¾çš„å……è¶³è®­ç»ƒæ•°æ®ä¸ç°å®ï¼Œæ€§èƒ½ä¸‹é™ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥å¹³è¡¡åŸºç¡€ç±»åˆ«é‰´åˆ«åŠ›å’Œæ–°ç±»åˆ«æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>SMPæ–¹æ³•é€šè¿‡å¼•å…¥è¾¹è·æƒ©ç½šï¼Œåœ¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ¡†æ¶ä¸‹å®ç°åŸºç¡€ä»»åŠ¡å­¦ä¹ å’Œå¢é‡ä»»åŠ¡å­¦ä¹ çš„å¹³è¡¡ã€‚</li>
<li>æå‡ºäº†Margin-aware Intra-task Adapter Mergingï¼ˆMIAMï¼‰æœºåˆ¶ï¼Œé€šè¿‡è®­ç»ƒä¸¤ç»„å…·æœ‰ä¸åŒåˆ†ç±»æŸå¤±çš„ä½ä½é€‚é…å™¨ï¼Œå¢å¼ºåŸºç¡€ç±»åˆ«é‰´åˆ«åŠ›å’Œæœªæ¥æ–°ç±»åˆ«çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¯¹äºå¢é‡ä»»åŠ¡ï¼Œæå‡ºäº†åŸºäºè¾¹è·æƒ©ç½šçš„åˆ†ç±»å™¨æ ¡å‡†ï¼ˆMPCCï¼‰ç­–ç•¥ï¼Œé€šè¿‡å¾®è°ƒæ‰€æœ‰å·²è§ç±»åˆ«çš„åµŒå…¥åˆ†ç±»å™¨ï¼Œä¼˜åŒ–å†³ç­–è¾¹ç•Œã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒSMPæ–¹æ³•åœ¨CIFAR100ã€ImageNet-Rå’ŒCUB200æ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
<li>SMPæ–¹æ³•å®ç°äº†å¯¹åŸºç¡€ä»»åŠ¡å’Œå¢é‡ä»»åŠ¡çš„è‰¯å¥½å…¼å®¹æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05094">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c9cb5b9a01209642eb5c0733fc8bbbe5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9eb6718b12e8da66b087186eec4ae1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef339efe1f29fa69bf2ccd4b4eae971b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Open-world-Point-Cloud-Semantic-Segmentation-A-Human-in-the-loop-Framework"><a href="#Open-world-Point-Cloud-Semantic-Segmentation-A-Human-in-the-loop-Framework" class="headerlink" title="Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop   Framework"></a>Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop   Framework</h2><p><strong>Authors:Peng Zhang, Songru Yang, Jinsheng Sun, Weiqing Li, Zhiyong Su</strong></p>
<p>Open-world point cloud semantic segmentation (OW-Seg) aims to predict point labels of both base and novel classes in real-world scenarios. However, existing methods rely on resource-intensive offline incremental learning or densely annotated support data, limiting their practicality. To address these limitations, we propose HOW-Seg, the first human-in-the-loop framework for OW-Seg. Specifically, we construct class prototypes, the fundamental segmentation units, directly on the query data, avoiding the prototype bias caused by intra-class distribution shifts between the support and query data. By leveraging sparse human annotations as guidance, HOW-Seg enables prototype-based segmentation for both base and novel classes. Considering the lack of granularity of initial prototypes, we introduce a hierarchical prototype disambiguation mechanism to refine ambiguous prototypes, which correspond to annotations of different classes. To further enrich contextual awareness, we employ a dense conditional random field (CRF) upon the refined prototypes to optimize their label assignments. Through iterative human feedback, HOW-Seg dynamically improves its predictions, achieving high-quality segmentation for both base and novel classes. Experiments demonstrate that with sparse annotations (e.g., one-novel-class-one-click), HOW-Seg matches or surpasses the state-of-the-art generalized few-shot segmentation (GFS-Seg) method under the 5-shot setting. When using advanced backbones (e.g., Stratified Transformer) and denser annotations (e.g., 10 clicks per sub-scene), HOW-Seg achieves 85.27% mIoU on S3DIS and 66.37% mIoU on ScanNetv2, significantly outperforming alternatives. </p>
<blockquote>
<p>å¼€æ”¾ä¸–ç•Œç‚¹äº‘è¯­ä¹‰åˆ†å‰²ï¼ˆOW-Segï¼‰æ—¨åœ¨é¢„æµ‹ç°å®ä¸–ç•Œåœºæ™¯ä¸­åŸºç¡€ç±»å’Œæ–°å‹ç±»çš„ç‚¹æ ‡ç­¾ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºèµ„æºå¯†é›†å‹çš„ç¦»çº¿å¢é‡å­¦ä¹ æˆ–å¯†é›†æ³¨é‡Šçš„æ”¯æŒæ•°æ®ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„å®ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†HOW-Segï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé¢å‘OW-Segçš„äººç±»å¾ªç¯æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨æŸ¥è¯¢æ•°æ®ä¸Šç›´æ¥æ„å»ºç±»åŸå‹ï¼ˆåŸºæœ¬çš„åˆ†å‰²å•å…ƒï¼‰ï¼Œé¿å…äº†ç”±äºæ”¯æŒæ•°æ®å’ŒæŸ¥è¯¢æ•°æ®ä¹‹é—´ç±»å†…åˆ†å¸ƒå˜åŒ–æ‰€å¯¼è‡´çš„åŸå‹åè§ã€‚é€šè¿‡åˆ©ç”¨ç¨€ç–äººç±»æ³¨é‡Šä½œä¸ºæŒ‡å¯¼ï¼ŒHOW-Segèƒ½å¤Ÿå®ç°åŸºç¡€ç±»å’Œæ–°å‹ç±»çš„åŸºäºåŸå‹çš„åˆ†å‰²ã€‚è€ƒè™‘åˆ°åˆå§‹åŸå‹çš„ç²’åº¦ä¸è¶³ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ†å±‚åŸå‹æ¶ˆæ­§æœºåˆ¶æ¥ç»†åŒ–æ¨¡ç³Šçš„åŸå‹ï¼Œè¿™äº›åŸå‹å¯¹åº”äºä¸åŒç±»çš„æ³¨é‡Šã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºä¸Šä¸‹æ–‡æ„è¯†ï¼Œæˆ‘ä»¬åœ¨ç»†åŒ–åçš„åŸå‹ä¸Šé‡‡ç”¨äº†å¯†é›†çš„æ¡ä»¶éšæœºå­—æ®µï¼ˆCRFï¼‰æ¥ä¼˜åŒ–å…¶æ ‡ç­¾åˆ†é…ã€‚é€šè¿‡è¿­ä»£çš„äººç±»åé¦ˆï¼ŒHOW-Segèƒ½å¤ŸåŠ¨æ€æ”¹è¿›å…¶é¢„æµ‹ï¼Œå®ç°åŸºç¡€ç±»å’Œæ–°å‹ç±»çš„é«˜è´¨é‡åˆ†å‰²ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ç¨€ç–æ³¨é‡Šï¼ˆä¾‹å¦‚ï¼Œæ¯ä¸ªæ–°å‹ç±»åˆ«åªéœ€ä¸€æ¬¡ç‚¹å‡»ï¼‰çš„æƒ…å†µä¸‹ï¼ŒHOW-Segåœ¨5æ¬¡å°„å‡»è®¾ç½®ä¸‹ä¸æœ€æ–°çš„å¹¿ä¹‰å°‘æ ·æœ¬åˆ†å‰²ï¼ˆGFS-Segï¼‰æ–¹æ³•ç›¸åŒ¹é…æˆ–è¶…è¶Šã€‚å½“ä½¿ç”¨å…ˆè¿›çš„éª¨å¹²ç½‘ç»œï¼ˆä¾‹å¦‚åˆ†å±‚è½¬æ¢å™¨ï¼‰å’Œæ›´å¯†é›†çš„æ³¨é‡Šï¼ˆä¾‹å¦‚æ¯ä¸ªå­åœºæ™¯10æ¬¡ç‚¹å‡»ï¼‰æ—¶ï¼ŒHOW-Segåœ¨S3DISä¸Šè¾¾åˆ°äº†85.27%çš„mIoUï¼Œåœ¨ScanNetv2ä¸Šè¾¾åˆ°äº†66.37%çš„mIoUï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04962v1">PDF</a> To be published in IEEE Transactions on Circuits and Systems for   Video Technology</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é¢å‘å¼€æ”¾ä¸–ç•Œç‚¹äº‘è¯­ä¹‰åˆ†å‰²ï¼ˆOW-Segï¼‰çš„æŒ‘æˆ˜åŠç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„äººæœºååŒæ–¹æ³•â€”â€”HOW-Segï¼Œå®ç°äº†å¯¹åŸºç¡€ç±»åˆ«å’Œæ–°å‹ç±»åˆ«çš„åŸå‹åŸºç¡€åˆ†å‰²ã€‚é€šè¿‡ç¨€ç–äººå·¥æ ‡æ³¨ä½œä¸ºæŒ‡å¯¼ï¼Œåˆ©ç”¨åˆ†å±‚åŸå‹è§£ææœºåˆ¶å’Œæ¡ä»¶éšæœºåœºï¼ˆCRFï¼‰ä¼˜åŒ–æ ‡ç­¾åˆ†é…ï¼Œæé«˜åˆ†å‰²è´¨é‡ã€‚å®éªŒè¯æ˜ï¼Œåœ¨ç¨€ç–æ ‡æ³¨æ¡ä»¶ä¸‹ï¼ŒHOW-Segè¾¾åˆ°äº†æˆ–è¶…è¶Šäº†ç°æœ‰çš„å¹¿ä¹‰å°‘æ ·æœ¬åˆ†å‰²ï¼ˆGFS-Segï¼‰æ–¹æ³•ã€‚åœ¨æ›´å…ˆè¿›çš„éª¨å¹²ç½‘ç»œå’Œæ›´å¯†é›†çš„æ ‡æ³¨ä¸‹ï¼ŒHOW-Segåœ¨S3DISå’ŒScanNetv2ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾ä¸–ç•Œç‚¹äº‘è¯­ä¹‰åˆ†å‰²ï¼ˆOW-Segï¼‰çš„ç›®æ ‡æ˜¯é¢„æµ‹ç°å®åœºæ™¯ä¸­åŸºç¡€ç±»åˆ«å’Œæ–°å‹ç±»åˆ«çš„ç‚¹æ ‡ç­¾ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–èµ„æºå¯†é›†å‹çš„ç¦»çº¿å¢é‡å­¦ä¹ æˆ–å¯†é›†æ³¨é‡Šçš„æ”¯æŒæ•°æ®ï¼Œé™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚</li>
<li>HOW-Segæ˜¯é¦–ä¸ªé¢å‘OW-Segçš„äººæœºååŒæ¡†æ¶ï¼Œç›´æ¥åœ¨æŸ¥è¯¢æ•°æ®ä¸Šæ„å»ºç±»åŸå‹ï¼Œé¿å…äº†å› æ”¯æŒæ•°æ®å’ŒæŸ¥è¯¢æ•°æ®ä¹‹é—´çš„ç±»å†…åˆ†å¸ƒåç§»è€Œå¯¼è‡´çš„åŸå‹åè§ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨ç¨€ç–çš„äººå·¥æ³¨é‡Šä½œä¸ºæŒ‡å¯¼ï¼ŒHOW-Segå®ç°äº†å¯¹åŸºç¡€ç±»åˆ«å’Œæ–°å‹ç±»åˆ«çš„åŸºäºåŸå‹çš„åˆ†å‰²ã€‚</li>
<li>å¼•å…¥åˆ†å±‚åŸå‹è§£ææœºåˆ¶ï¼Œä»¥ç»†åŒ–æ¨¡ç³Šçš„åŸå‹ï¼Œå¹¶ä¼˜åŒ–æ ‡ç­¾åˆ†é…ã€‚</li>
<li>é€šè¿‡è¿­ä»£çš„äººå·¥åé¦ˆï¼ŒHOW-Segèƒ½åŠ¨æ€æ”¹è¿›å…¶é¢„æµ‹ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œåœ¨ç¨€ç–æ ‡æ³¨æ¡ä»¶ä¸‹ï¼ŒHOW-Segæ€§èƒ½ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼›åœ¨æ›´å…ˆè¿›çš„éª¨å¹²ç½‘ç»œå’Œæ›´å¯†é›†çš„æ ‡æ³¨ä¸‹ï¼Œå…¶æ€§èƒ½æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04962">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0e013d79dc27cf7140235f7f64b4eef3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d7446179313318325d82d83d70865ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-589979e3e5ac02d94c24c5ebb34459da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11ddea083ddaec4f6b7c958a4e155a93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-269851ed5ef660bbadbfdc1d6ba372f2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A-Foundational-Multi-Modal-Model-for-Few-Shot-Learning"><a href="#A-Foundational-Multi-Modal-Model-for-Few-Shot-Learning" class="headerlink" title="A Foundational Multi-Modal Model for Few-Shot Learning"></a>A Foundational Multi-Modal Model for Few-Shot Learning</h2><p><strong>Authors:Pengtao Dang, Tingbo Guo, Sha Cao, Chi Zhang</strong></p>
<p>Few-shot learning (FSL) is a machine learning paradigm that aims to generalize models from a small number of labeled examples, typically fewer than 10 per class. FSL is particularly crucial in biomedical, environmental, materials, and mechanical sciences, where samples are limited and data collection is often prohibitively costly, time-consuming, or ethically constrained. In this study, we present an innovative approach to FSL by demonstrating that a Large Multi-Modal Model (LMMM), trained on a set of independent tasks spanning diverse domains, task types, and input modalities, can substantially improve the generalization of FSL models, outperforming models based on conventional meta-learning on tasks of the same type. To support this, we first constructed a Multi-Modal Model Few-shot Dataset (M3FD, over 10K+ few-shot samples), which includes 2D RGB images, 2D&#x2F;3D medical scans, tabular and time-course datasets, from which we manually curated FSL tasks such as classification. We further introduced M3F (Multi-Modal Model for Few-shot learning framework), a novel Large Multi-Modal Model framework tailored for data-constrained scientific applications. M3F supports a wide range of scientific data types through a modular pipeline. By fine-tuning the model on M3FD, M3F improves model performance, making LMMM feasible for real-world FSL deployment. The source code is located at <a target="_blank" rel="noopener" href="https://github.com/ptdang1001/M3F">https://github.com/ptdang1001/M3F</a>. To democratize access to complex FSL data and promote reproducibility for public usage, M3FD is paired with a flexible and user-friendly tool that enables efficient querying, task-specific sampling, and preprocessing. Together, our dataset and framework offer a unified, scalable solution that significantly lowers the barrier to applying LMMMs in data-scarce scientific domains. </p>
<blockquote>
<p>å°‘é‡å­¦ä¹ ï¼ˆFSLï¼‰æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ èŒƒå¼ï¼Œæ—¨åœ¨ä»å°‘é‡æ ‡è®°æ ·æœ¬ä¸­æ¨å¹¿æ¨¡å‹ï¼Œé€šå¸¸æ¯ä¸ªç±»åˆ«å°‘äº10ä¸ªæ ·æœ¬ã€‚åœ¨ç”Ÿç‰©åŒ»ç–—ã€ç¯å¢ƒã€ææ–™å’Œæœºæ¢°ç§‘å­¦ç­‰é¢†åŸŸï¼Œç”±äºæ ·æœ¬æœ‰é™ï¼Œä¸”æ•°æ®æ”¶é›†å¾€å¾€æˆæœ¬é«˜æ˜‚ã€è€—æ—¶æˆ–å—åˆ°ä¼¦ç†é™åˆ¶ï¼Œå› æ­¤FSLå°¤ä¸ºé‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§é‡å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMMï¼‰çš„åˆ›æ–°å°‘é‡å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ¨¡å‹åœ¨æ¶µç›–ä¸åŒé¢†åŸŸã€ä»»åŠ¡ç±»å‹å’Œè¾“å…¥æ¨¡æ€çš„ç‹¬ç«‹ä»»åŠ¡é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥æ˜¾è‘—æ”¹å–„FSLæ¨¡å‹çš„æ¨å¹¿èƒ½åŠ›ï¼Œå¹¶ä¼˜äºåŸºäºä¼ ç»Ÿå…ƒå­¦ä¹ çš„ç›¸åŒç±»å‹ä»»åŠ¡æ¨¡å‹ã€‚ä¸ºæ”¯æŒè¿™ä¸€æ–¹æ³•ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºäº†å¤šæ¨¡æ€æ¨¡å‹å°‘é‡æ•°æ®é›†ï¼ˆM3FDï¼Œè¶…è¿‡10,000ä¸ªå°‘é‡æ ·æœ¬ï¼‰ï¼Œå…¶ä¸­åŒ…æ‹¬2D RGBå›¾åƒã€2D&#x2F;3DåŒ»å­¦æ‰«æã€è¡¨æ ¼å’Œæ—¶åºæ•°æ®é›†ç­‰ï¼Œæˆ‘ä»¬ä»è¿™äº›æ•°æ®é›†ä¸­æ‰‹å·¥ç­›é€‰äº†å¦‚åˆ†ç±»ç­‰FSLä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†M3Fï¼ˆå°‘é‡å­¦ä¹ çš„å¤šæ¨¡æ€æ¨¡å‹æ¡†æ¶ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹æ•°æ®å—é™ç§‘å­¦åº”ç”¨çš„æ–°å‹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹æ¡†æ¶ã€‚M3Fé€šè¿‡æ¨¡å—åŒ–ç®¡é“æ”¯æŒå¹¿æ³›çš„ç§‘å­¦æ•°æ®ç±»å‹ã€‚é€šè¿‡å¯¹M3FDè¿›è¡Œæ¨¡å‹å¾®è°ƒï¼ŒM3Fæé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œä½¿å¾—LMMMåœ¨ç°å®ä¸–ç•Œçš„FSLéƒ¨ç½²ä¸­åˆ‡å®å¯è¡Œã€‚æºä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/ptdang1001/M3F%E3%80%82%E4%B8%BA%E4%BA%86%E6%99%AE%E5%8F%8A%E5%A4%8D%E6%9D%82FSL%E6%95%B0%E6%8D%AE%E7%9A%84%E8%AE%BF%E9%97%AE%E5%B9%B6%E4%BF%83%E8%BF%9B%E5%85%AC%E4%BC%97%E4%BD%BF%E7%94%A8%E7%9A%84%E5%8F%AF%E9%87%8D%E5%A4%8D%E6%80%A7%EF%BC%8CM3FD%E4%B8%8E%E4%B8%80%E7%A7%8D%E7%81%B5%E6%B4%BB%E4%B8%94%E7%94%A8%E6%88%B7%E5%8F%8B%E5%A5%BD%E7%9A%84%E5%B7%A5%E5%85%B7%E7%9B%B8%E7%BB%93%E5%90%88%EF%BC%8C%E8%AF%A5%E5%B7%A5%E5%85%B7%E8%83%BD%E5%A4%9F%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E6%9F%A5%E8%AF%A2%E3%80%81%E9%92%88%E5%AF%B9%E7%89%B9%E5%AE%9A%E4%BB%BB%E5%8A%A1%E7%9A%84%E9%87%87%E6%A0%B7%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86%E3%80%82%E6%80%BB%E4%B9%8B%EF%BC%8C%E6%88%91%E4%BB%AC%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%A1%86%E6%9E%B6%E6%8F%90%E4%BE%9B%E7%BB%9F%E4%B8%80%E3%80%81%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%EF%BC%8C%E5%A4%A7%E5%A4%A7%E9%99%8D%E4%BD%8E%E4%BA%86%E5%9C%A8%E7%A7%91%E5%AD%A6%E9%A2%86%E5%9F%9F%E5%BA%94%E7%94%A8LMMMs%E7%9A%84%E9%97%A8%E6%A7%9B%EF%BC%8C%E5%B0%A4%E5%85%B6%E6%98%AF%E5%9C%A8%E6%95%B0%E6%8D%AE%E7%A8%80%E7%BC%BA%E7%9A%84%E9%A2%86%E5%9F%9F%E3%80%82">https://github.com/ptdang1001/M3Fã€‚ä¸ºäº†æ™®åŠå¤æ‚FSLæ•°æ®çš„è®¿é—®å¹¶ä¿ƒè¿›å…¬ä¼—ä½¿ç”¨çš„å¯é‡å¤æ€§ï¼ŒM3FDä¸ä¸€ç§çµæ´»ä¸”ç”¨æˆ·å‹å¥½çš„å·¥å…·ç›¸ç»“åˆï¼Œè¯¥å·¥å…·èƒ½å¤Ÿå®ç°é«˜æ•ˆæŸ¥è¯¢ã€é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„é‡‡æ ·å’Œé¢„å¤„ç†ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†å’Œæ¡†æ¶æä¾›ç»Ÿä¸€ã€å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œå¤§å¤§é™ä½äº†åœ¨ç§‘å­¦é¢†åŸŸåº”ç”¨LMMMsçš„é—¨æ§›ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„é¢†åŸŸã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04746v1">PDF</a> 11 pages, 5 figures</p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶æå‡ºä¸€ç§åŸºäºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMMï¼‰çš„å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰æ–°æ–¹æ³•ï¼Œé€šè¿‡è·¨ä¸åŒé¢†åŸŸã€ä»»åŠ¡ç±»å‹å’Œè¾“å…¥æ¨¡æ€çš„ç‹¬ç«‹ä»»åŠ¡é›†è¿›è¡Œè®­ç»ƒï¼Œèƒ½æ˜¾è‘—æå‡FSLæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ”¯æŒæ­¤æ–¹æ³•ï¼Œç ”ç©¶æ„å»ºäº†å¤šæ¨¡æ€æ¨¡å‹å°‘æ ·æœ¬æ•°æ®é›†ï¼ˆM3FDï¼‰ï¼Œå¹¶å¼•å…¥ä¸“ä¸ºæ•°æ®å—é™ç§‘å­¦åº”ç”¨è®¾è®¡çš„M3Fæ¡†æ¶ã€‚M3Fé€šè¿‡å¾®è°ƒæ¨¡å‹ï¼Œæå‡æ€§èƒ½ï¼Œä½¿LMMMåœ¨å°‘æ ·æœ¬å­¦ä¹ çš„å®é™…åº”ç”¨ä¸­å˜å¾—å¯è¡Œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFSLï¼‰åœ¨æ ·æœ¬æœ‰é™çš„æ•°æ®æ”¶é›†æˆæœ¬é«˜æ˜‚ã€è€—æ—¶æˆ–å—ä¼¦ç†çº¦æŸçš„é¢†åŸŸä¸­å°¤ä¸ºé‡è¦ã€‚</li>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMMï¼‰åœ¨FSLä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤šæ¨¡æ€æ•°æ®æ—¶ã€‚</li>
<li>ç ”ç©¶æ„å»ºäº†å¤šæ¨¡æ€æ¨¡å‹å°‘æ ·æœ¬æ•°æ®é›†ï¼ˆM3FDï¼‰ï¼ŒåŒ…å«å¤šç§æ•°æ®ç±»å‹ï¼Œå¦‚2D RGBå›¾åƒã€2D&#x2F;3DåŒ»ç–—æ‰«æã€è¡¨æ ¼å’Œæ—¶é—´åºåˆ—æ•°æ®é›†ã€‚</li>
<li>å¼•å…¥M3Fæ¡†æ¶ï¼Œæ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼Œå¹¶é€šè¿‡å¾®è°ƒæ¨¡å‹æå‡FSLæ€§èƒ½ã€‚</li>
<li>M3Fæ¡†æ¶ä½¿LMMMåœ¨ç°å®ä¸–ç•Œä¸­çš„å°‘æ ·æœ¬å­¦ä¹ éƒ¨ç½²å˜å¾—å¯è¡Œã€‚</li>
<li>ç ”ç©¶æä¾›çµæ´»çš„ç”¨æˆ·å‹å¥½å·¥å…·ï¼Œä¾¿äºæŸ¥è¯¢ç‰¹å®šä»»åŠ¡é‡‡æ ·å’Œé¢„å¤„ç†ï¼Œä¿ƒè¿›FSLæ•°æ®çš„å¯è®¿é—®æ€§å’Œå¤ç°æ€§ã€‚</li>
<li>æ•°æ®é›†å’Œæ¡†æ¶çš„ç»“åˆä¸ºæ•°æ®ç¨€ç¼ºçš„ç§‘å­¦é¢†åŸŸæä¾›äº†ç»Ÿä¸€ã€å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04746">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ec55672147aea8706b80ac5966c389e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32dc8b421a33a24ab2c3589486cbae95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4056281a196a85e3e403eb816cda9128.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8c8d7a4e10946c7f48b20bf02e31677.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Vision-Language-Reasoning-for-Satellite-Imagery-via-Verifiable-Rewards"><a href="#Few-Shot-Vision-Language-Reasoning-for-Satellite-Imagery-via-Verifiable-Rewards" class="headerlink" title="Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable   Rewards"></a>Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable   Rewards</h2><p><strong>Authors:Aybora Koksal, A. Aydin Alatan</strong></p>
<p>Recent advances in large language and vision-language models have enabled strong reasoning capabilities, yet they remain impractical for specialized domains like remote sensing, where annotated data is scarce and expensive. We present the first few-shot reinforcement learning with verifiable reward (RLVR) framework for satellite imagery that eliminates the need for caption supervisionâ€“relying solely on lightweight, rule-based binary or IoU-based rewards. Adapting the â€œ1-shot RLVRâ€ paradigm from language models to vision-language models, we employ policy-gradient optimization with as few as one curated example to align model outputs for satellite reasoning tasks. Comprehensive experiments across multiple remote sensing benchmarksâ€“including classification, visual question answering, and groundingâ€“show that even a single example yields substantial improvements over the base model. Scaling to 128 examples matches or exceeds models trained on thousands of annotated samples. While the extreme one-shot setting can induce mild, task-specific overfitting, our approach consistently demonstrates robust generalization and efficiency across diverse tasks. Further, we find that prompt design and loss weighting significantly influence training stability and final accuracy. Our method enables cost-effective and data-efficient development of domain-specialist vision-language reasoning models, offering a pragmatic recipe for data-scarce fields: start from a compact VLM, curate a handful of reward-checkable cases, and train via RLVR. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„è¿›æ­¥å·²ç»å…·å¤‡äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨é¥æ„Ÿç­‰ç‰¹å®šé¢†åŸŸï¼Œç”±äºæ ‡æ³¨æ•°æ®ç¨€ç¼ºä¸”æ˜‚è´µï¼Œè¿™äº›æ¨¡å‹ä»ä¸å®ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªåŸºäºå¯éªŒè¯å¥–åŠ±çš„å°‘é‡æ ·æœ¬å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ¡†æ¶ï¼Œç”¨äºå«æ˜Ÿå›¾åƒï¼Œè¯¥æ¡†æ¶æ— éœ€å­—å¹•ç›‘ç£â€”â€”ä»…ä¾èµ–è½»é‡çº§ã€åŸºäºè§„åˆ™çš„äºŒè¿›åˆ¶æˆ–åŸºäºIoUçš„å¥–åŠ±ã€‚æˆ‘ä»¬å°†è¯­è¨€æ¨¡å‹çš„â€œ1-shot RLVRâ€èŒƒå¼é€‚åº”åˆ°è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä»…ä¸€ä¸ªç²¾é€‰æ ·æœ¬è¿›è¡Œç­–ç•¥æ¢¯åº¦ä¼˜åŒ–ï¼Œä»¥å¯¹é½å«æ˜Ÿæ¨ç†ä»»åŠ¡æ¨¡å‹è¾“å‡ºã€‚åœ¨å¤šä¸ªé¥æ„ŸåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒâ€”â€”åŒ…æ‹¬åˆ†ç±»ã€è§†è§‰é—®ç­”å’Œæ¥åœ°â€”â€”è¡¨æ˜ï¼Œå³ä½¿ä¸€ä¸ªæ ·æœ¬ä¹Ÿèƒ½åœ¨åŸºå‡†æ¨¡å‹çš„åŸºç¡€ä¸Šå®ç°æ˜¾è‘—æ”¹è¿›ã€‚æ‰©å±•åˆ°128ä¸ªæ ·æœ¬å¯ä¸åœ¨æ•°åƒä¸ªæ ‡æ³¨æ ·æœ¬ä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸åŒ¹é…æˆ–è¶…è¶Šã€‚è™½ç„¶æç«¯çš„ä¸€æ¬¡æ€§å­¦ä¹ è®¾ç½®å¯èƒ½ä¼šå¯¼è‡´ç‰¹å®šä»»åŠ¡çš„è½»å¾®è¿‡æ‹Ÿåˆï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒçš„ä»»åŠ¡ä¸­å§‹ç»ˆå±•ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æç¤ºè®¾è®¡å’ŒæŸå¤±æƒé‡å¯¹è®­ç»ƒç¨³å®šæ€§å’Œæœ€ç»ˆç²¾åº¦äº§ç”Ÿæ˜¾è‘—å½±å“ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°æˆæœ¬å’Œæ•°æ®é«˜æ•ˆçš„é¢†åŸŸä¸“ä¸šè§†è§‰è¯­è¨€æ¨ç†æ¨¡å‹å¼€å‘ï¼Œä¸ºæ•°æ®ç¨€ç¼ºé¢†åŸŸæä¾›äº†å®ç”¨æ–¹æ¡ˆï¼šä»ç´§å‡‘çš„VLMå¼€å§‹ï¼Œç²¾é€‰å°‘é‡å¯éªŒè¯å¥–åŠ±çš„æ¡ˆä¾‹ï¼Œå¹¶é€šè¿‡RLVRè¿›è¡Œè®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21745v2">PDF</a> ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL). 10   pages, 3 figures, 6 tables. Our model, training code and dataset will be at   <a target="_blank" rel="noopener" href="https://github.com/aybora/FewShotReasoning">https://github.com/aybora/FewShotReasoning</a></p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•èµ‹äºˆäº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨é¥æ„Ÿç­‰ç‰¹å®šé¢†åŸŸä»ä¸é€‚ç”¨ï¼Œå› ä¸ºæ ‡æ³¨æ•°æ®ç¨€ç¼ºä¸”æ˜‚è´µã€‚æœ¬ç ”ç©¶æå‡ºäº†é¦–ä¸ªæ— éœ€å­—å¹•ç›‘ç£çš„åŸºäºå¥–åŠ±éªŒè¯çš„å°‘é‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ¡†æ¶ï¼Œç”¨äºå«æ˜Ÿå›¾åƒåˆ†æã€‚å€ŸåŠ©æ”¿ç­–æ¢¯åº¦ä¼˜åŒ–ï¼Œåªéœ€ä¸€ä¸ªç²¾é€‰ç¤ºä¾‹å³å¯å¯¹é½å«æ˜Ÿæ¨ç†ä»»åŠ¡æ¨¡å‹è¾“å‡ºã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨å•æ ·æœ¬æƒ…å¢ƒä¸‹ï¼Œè¯¥æ¨¡å‹ä¹Ÿæœ‰æ˜¾è‘—æ”¹å–„ï¼Œæ‰©å±•åˆ°å¤§é‡ç¤ºä¾‹æ—¶çš„æ€§èƒ½ä¼˜å¼‚ã€‚å°½ç®¡å•æ ·æœ¬è®¾ç½®å¯èƒ½å¯¼è‡´ç‰¹å®šä»»åŠ¡è¿‡åº¦æ‹Ÿåˆï¼Œä½†è¯¥æ–¹æ³•åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­å±•ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œæç¤ºè®¾è®¡å’ŒæŸå¤±æƒé‡å¯¹è®­ç»ƒç¨³å®šæ€§å’Œæœ€ç»ˆç²¾åº¦æœ‰é‡å¤§å½±å“ã€‚æ­¤æ–¹æ³•ä¸ºæ•°æ®ç¨€ç¼ºé¢†åŸŸæä¾›äº†å®ç”¨ç­–ç•¥ï¼šä»ç´§å‡‘çš„VLMå¼€å§‹ï¼Œç²¾é€‰å°‘é‡å¯éªŒè¯å¥–åŠ±çš„æ¡ˆä¾‹ï¼Œå¹¶é€šè¿‡RLVRè¿›è¡Œè®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸå¤§å‹è¯­è¨€å’Œè§†è§‰è¯­è¨€æ¨¡å‹å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸå¦‚é¥æ„Ÿä¸­å®ç”¨æ€§æœ‰é™ï¼Œå› æ ‡æ³¨æ•°æ®ç¨€ç¼ºä¸”æˆæœ¬é«˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¥–åŠ±éªŒè¯çš„å°‘é‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ¡†æ¶ï¼Œæ— éœ€å­—å¹•ç›‘ç£ï¼Œä»…ä¾èµ–è½»é‡çº§ã€åŸºäºè§„åˆ™çš„å¥–åŠ±è¿›è¡Œå«æ˜Ÿå›¾åƒåˆ†æã€‚</li>
<li>é€šè¿‡æ”¿ç­–æ¢¯åº¦ä¼˜åŒ–ï¼Œä»…ç”¨ä¸€ä¸ªç²¾é€‰æ ·æœ¬å³å¯å¯¹é½å«æ˜Ÿæ¨ç†ä»»åŠ¡çš„æ¨¡å‹è¾“å‡ºï¼Œå®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>æ‰©å±•åˆ°å¤§é‡ç¤ºä¾‹æ—¶æ€§èƒ½ä¼˜å¼‚ï¼Œç”šè‡³å¯ä¸æ•°åƒä¸ªæ ‡æ³¨æ ·æœ¬è®­ç»ƒçš„æ¨¡å‹ç›¸åŒ¹æ•Œæˆ–æ›´ä½³ã€‚</li>
<li>å•æ ·æœ¬è®¾ç½®å¯èƒ½å¯¼è‡´ç‰¹å®šä»»åŠ¡è¿‡åº¦æ‹Ÿåˆï¼Œä½†è¯¥æ–¹æ³•åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­å±•ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›å’Œæ•ˆç‡ã€‚</li>
<li>æç¤ºè®¾è®¡å’ŒæŸå¤±æƒé‡å¯¹æ¨¡å‹çš„è®­ç»ƒç¨³å®šæ€§å’Œæœ€ç»ˆç²¾åº¦å…·æœ‰é‡è¦å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21745">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-732b3786f4aa504a527762387ae04233.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-523cadb83435b7fe50117ac9758c77eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9da7cf5bd8e6c3b9062f2351e76d97a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f2521a48ab6d958e0c2842dd6e3b642c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Data-Leakage-and-Redundancy-in-the-LIT-PCBA-Benchmark"><a href="#Data-Leakage-and-Redundancy-in-the-LIT-PCBA-Benchmark" class="headerlink" title="Data Leakage and Redundancy in the LIT-PCBA Benchmark"></a>Data Leakage and Redundancy in the LIT-PCBA Benchmark</h2><p><strong>Authors:Amber Huang, Ian Scott Knight, Slava Naprienko</strong></p>
<p>LIT-PCBA is widely used to benchmark virtual screening models, but our audit reveals that it is fundamentally compromised. We find extensive data leakage and molecular redundancy across its splits, including 2D-identical ligands within and across partitions, pervasive analog overlap, and low-diversity query sets. In ALDH1 alone, for instance, 323 active training â€“ validation analog pairs occur at ECFP4 Tanimoto similarity $\geq 0.6$; across all targets, 2,491 2D-identical inactives appear in both training and validation, with very few corresponding actives. These overlaps allow models to succeed through scaffold memorization rather than generalization, inflating enrichment factors and AUROC scores. These flaws are not incidental â€“ they are so severe that a trivial memorization-based baseline with no learnable parameters can exploit them to match or exceed the reported performance of state-of-the-art deep learning and 3D-similarity models. As a result, nearly all published results on LIT-PCBA are undermined. Even models evaluated in â€œzero-shotâ€ mode are affected by analog leakage into the query set, weakening claims of generalization. In its current form, the benchmark does not measure a modelâ€™s ability to recover novel chemotypes and should not be taken as evidence of methodological progress.   All code, data, and baseline implementations are available at: <a target="_blank" rel="noopener" href="https://github.com/sievestack/LIT-PCBA-audit">https://github.com/sievestack/LIT-PCBA-audit</a> </p>
<blockquote>
<p>LIT-PCBAè¢«å¹¿æ³›ç”¨ä½œè™šæ‹Ÿç­›é€‰æ¨¡å‹çš„åŸºå‡†æµ‹è¯•ï¼Œä½†æˆ‘ä»¬çš„å®¡è®¡å‘ç°å…¶å­˜åœ¨æ ¹æœ¬ä¸Šçš„é—®é¢˜ã€‚æˆ‘ä»¬å‘ç°å…¶æ•°æ®åˆ†å‰²ä¸­å­˜åœ¨å¤§é‡çš„æ•°æ®æ³„éœ²å’Œåˆ†å­å†—ä½™ï¼ŒåŒ…æ‹¬åˆ†åŒºå†…å’Œè·¨åˆ†åŒºçš„2Dç›¸åŒé…ä½“ã€æ™®éçš„ç±»ä¼¼ç‰©é‡å ä»¥åŠä½å¤šæ ·æ€§çš„æŸ¥è¯¢é›†ã€‚ä»¥ALDH1ä¸ºä¾‹ï¼Œå°±æœ‰323å¯¹æ´»æ€§è®­ç»ƒ-éªŒè¯ç±»ä¼¼ç‰©åœ¨ECFP4çš„Tanimotoç›¸ä¼¼åº¦â‰¥0.6ï¼›åœ¨æ‰€æœ‰ç›®æ ‡ä¸­ï¼Œ2,491ä¸ª2Dç›¸åŒçš„ä¸æ´»è·ƒåˆ†å­åŒæ—¶å‡ºç°åœ¨è®­ç»ƒå’ŒéªŒè¯ä¸­ï¼Œè€Œå¯¹åº”çš„æ´»è·ƒåˆ†å­å´éå¸¸å°‘ã€‚è¿™äº›é‡å ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿé€šè¿‡æ”¯æ¶è®°å¿†è€Œéæ³›åŒ–æ¥æˆåŠŸå®Œæˆä»»åŠ¡ï¼Œä»è€Œæé«˜äº†å¯Œé›†å› å­å’ŒAUROCåˆ†æ•°ã€‚è¿™äº›ç¼ºé™·å¹¶éå¶ç„¶ï¼Œè€Œæ˜¯éå¸¸ä¸¥é‡ï¼Œä»¥è‡³äºä¸€ä¸ªåŸºäºè®°å¿†çš„åŸºç¡€çº¿ï¼ˆæ²¡æœ‰ä»»ä½•å¯å­¦ä¹ çš„å‚æ•°ï¼‰å¯ä»¥å……åˆ†åˆ©ç”¨å®ƒä»¬ï¼Œä»¥è¾¾åˆ°æˆ–è¶…è¿‡æœ€æ–°æ·±åº¦å­¦ä¹ å’Œ3Dç›¸ä¼¼æ€§æ¨¡å‹çš„æŠ¥å‘Šæ€§èƒ½ã€‚å› æ­¤ï¼ŒLIT-PCBAä¸Šå‡ ä¹æ‰€æœ‰çš„å·²å‘å¸ƒç»“æœéƒ½å—åˆ°äº†è´¨ç–‘ã€‚å³ä½¿åœ¨â€œé›¶å°„å‡»â€æ¨¡å¼ä¸‹è¯„ä¼°çš„æ¨¡å‹ä¹Ÿå—åˆ°äº†æŸ¥è¯¢é›†ä¸­ç±»ä¼¼ç‰©æ³„éœ²çš„å½±å“ï¼Œå‰Šå¼±äº†æ³›åŒ–çš„ä¸»å¼ ã€‚ç›®å‰ï¼Œè¯¥åŸºå‡†æµ‹è¯•å¹¶æœªè¡¡é‡æ¨¡å‹æ¢å¤æ–°å‹åŒ–å­¦ç±»å‹çš„èƒ½åŠ›ï¼Œå¹¶ä¸èƒ½ä½œä¸ºæ–¹æ³•è¿›æ­¥çš„è¯æ˜ã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®å’ŒåŸºçº¿å®æ–½å‡å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/sievestack/LIT-PCBA-audit%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sievestack/LIT-PCBA-auditæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21404v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æ­éœ²äº†è™šæ‹Ÿç­›é€‰æ¨¡å‹åŸºå‡†æµ‹è¯•ï¼ˆLIT-PCBAï¼‰å­˜åœ¨é‡å¤§æ¼æ´ï¼Œæµ‹è¯•æ•°æ®é›†å­˜åœ¨æ•°æ®æ³„éœ²ã€åˆ†å­å†—ä½™ç­‰ä¸¥é‡é—®é¢˜ï¼Œæ— æ³•çœŸå®åæ˜ æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡è¯¦ç»†åˆ†æï¼Œå‘ç°è¿™äº›é—®é¢˜å¯¼è‡´æ¨¡å‹é€šè¿‡è®°å¿†æ”¯æ¶è€Œéæ³›åŒ–æœºåˆ¶æˆåŠŸï¼Œä»è€Œå¤¸å¤§å¯Œé›†å› ç´ å’ŒAUROCåˆ†æ•°ã€‚è¿™äº›é—®é¢˜ä¸¥é‡å½±å“äº†åŸºå‡†æµ‹è¯•çš„å…¬æ­£æ€§å’Œå‡†ç¡®æ€§ï¼Œå‡ ä¹æ‰€æœ‰å·²å‘å¸ƒçš„å…³äºLIT-PCBAçš„ç»“æœéƒ½å—åˆ°äº†å½±å“ã€‚å› æ­¤ï¼Œå½“å‰å½¢å¼çš„åŸºå‡†æµ‹è¯•æ— æ³•è¡¡é‡æ¨¡å‹æ¢å¤æ–°å‹åŒ–å­¦ç±»å‹çš„èƒ½åŠ›ï¼Œä¸èƒ½ä½œä¸ºæ–¹æ³•è¿›æ­¥çš„ä¾æ®ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>LIT-PCBAè¢«å¹¿æ³›ç”¨ä½œè™šæ‹Ÿç­›é€‰æ¨¡å‹çš„åŸºå‡†æµ‹è¯•ï¼Œä½†å­˜åœ¨é‡å¤§æ¼æ´ã€‚</li>
<li>æ•°æ®æ³„éœ²å’Œåˆ†å­å†—ä½™é—®é¢˜åœ¨LIT-PCBAçš„å„åˆ†å‰²ä¸­æ™®éå­˜åœ¨ã€‚</li>
<li>å‘ç°å¤§é‡äºŒç»´ç›¸åŒé…ä½“åœ¨åˆ†å‰²å†…éƒ¨å’Œè·¨åˆ†å‰²ä¹‹é—´çš„æƒ…å†µã€‚</li>
<li>æ¨¡æ‹Ÿæ³›åŒ–è¿‡ç¨‹ä¸­çš„é”™è¯¯æƒ…å†µæ˜¯ç”±äºæ¨¡å‹é€šè¿‡è®°å¿†æ”¯æ¶è€Œéæ³›åŒ–æœºåˆ¶æˆåŠŸå¯¼è‡´çš„ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½è¯„ä¼°å—åˆ°ä¸¥é‡å½±å“ï¼Œæ•°æ®æ³„éœ²å¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›å‡å¼±ã€‚</li>
<li>å½“å‰å½¢å¼çš„åŸºå‡†æµ‹è¯•æ— æ³•åæ˜ æ¨¡å‹æ¢å¤æ–°å‹åŒ–å­¦ç±»å‹çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21404">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fa21bd6151e02f48066f2b18da26de95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1b6b1a90fc6db6837766f46ef3f0700.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-328c4c2cd32769bc3a0bb9c0ef56c961.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-883d10e33ce007b79a5a684254d2fbf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-278e06936a71e909166b5e3e4ffad485.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Can-open-source-large-language-models-be-used-for-tumor-documentation-in-Germany-â€“-An-evaluation-on-urological-doctorsâ€™-notes"><a href="#Can-open-source-large-language-models-be-used-for-tumor-documentation-in-Germany-â€“-An-evaluation-on-urological-doctorsâ€™-notes" class="headerlink" title="Can open source large language models be used for tumor documentation in   Germany? â€“ An evaluation on urological doctorsâ€™ notes"></a>Can open source large language models be used for tumor documentation in   Germany? â€“ An evaluation on urological doctorsâ€™ notes</h2><p><strong>Authors:Stefan Lenz, Arsenij Ustjanzew, Marco Jeray, Meike Ressing, Torsten Panholzer</strong></p>
<p>Tumor documentation in Germany is largely done manually, requiring reading patient records and entering data into structured databases. Large language models (LLMs) could potentially enhance this process by improving efficiency and reliability. This evaluation tests eleven different open source LLMs with sizes ranging from 1-70 billion model parameters on three basic tasks of the tumor documentation process: identifying tumor diagnoses, assigning ICD-10 codes, and extracting the date of first diagnosis. For evaluating the LLMs on these tasks, a dataset of annotated text snippets based on anonymized doctorsâ€™ notes from urology was prepared. Different prompting strategies were used to investigate the effect of the number of examples in few-shot prompting and to explore the capabilities of the LLMs in general. The models Llama 3.1 8B, Mistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks. Models with less extensive training data or having fewer than 7 billion parameters showed notably lower performance, while larger models did not display performance gains. Examples from a different medical domain than urology could also improve the outcome in few-shot prompting, which demonstrates the ability of LLMs to handle tasks needed for tumor documentation. Open source LLMs show a strong potential for automating tumor documentation. Models from 7-12 billion parameters could offer an optimal balance between performance and resource efficiency. With tailored fine-tuning and well-designed prompting, these models might become important tools for clinical documentation in the future. The code for the evaluation is available from <a target="_blank" rel="noopener" href="https://github.com/stefan-m-lenz/UroLlmEval">https://github.com/stefan-m-lenz/UroLlmEval</a>. We also release the dataset as a new valuable resource that addresses the shortage of authentic and easily accessible benchmarks in German-language medical NLP. </p>
<blockquote>
<p>åœ¨å¾·å›½ï¼Œè‚¿ç˜¤è®°å½•å·¥ä½œå¤§éƒ¨åˆ†æ˜¯é€šè¿‡æ‰‹åŠ¨å®Œæˆçš„ï¼Œéœ€è¦é˜…è¯»æ‚£è€…è®°å½•å¹¶å°†æ•°æ®è¾“å…¥ç»“æ„åŒ–æ•°æ®åº“ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ‰æœ›é€šè¿‡æé«˜æ•ˆç‡å¯é æ€§æ¥æ”¹è¿›è¿™ä¸€è¿‡ç¨‹ã€‚æœ¬æ¬¡è¯„ä¼°æµ‹è¯•äº†è§„æ¨¡ä»1äº¿åˆ°70äº¿æ¨¡å‹å‚æ•°çš„11ä¸ªä¸åŒå¼€æºLLMsï¼Œåœ¨è‚¿ç˜¤è®°å½•è¿‡ç¨‹çš„ä¸‰ä¸ªåŸºæœ¬ä»»åŠ¡ä¸Šï¼šè¯†åˆ«è‚¿ç˜¤è¯Šæ–­ã€åˆ†é…ICD-10ä»£ç ä»¥åŠæå–é¦–æ¬¡è¯Šæ–­æ—¥æœŸã€‚ä¸ºäº†è¯„ä¼°è¿™äº›LLMsåœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå‡†å¤‡äº†ä¸€ä¸ªåŸºäºæ³Œå°¿å­¦åŒ¿ååŒ»ç”Ÿç¬”è®°çš„æ ‡æ³¨æ–‡æœ¬ç‰‡æ®µæ•°æ®é›†ã€‚ä½¿ç”¨äº†ä¸åŒçš„æç¤ºç­–ç•¥æ¥ç ”ç©¶å°‘æ ·æœ¬æç¤ºä¸­ç¤ºä¾‹æ•°é‡çš„å½±å“ï¼Œå¹¶æ¢ç´¢LLMsçš„ä¸€èˆ¬èƒ½åŠ›ã€‚æ¨¡å‹Llama 3.1 8Bã€Mistral 7Bå’ŒMistral NeMo 12Båœ¨ä»»åŠ¡ä¸­è¡¨ç°ç›¸å½“å¥½ã€‚æ‹¥æœ‰è¾ƒå°‘è®­ç»ƒæ•°æ®æˆ–å‚æ•°å°‘äº7äº¿çš„æ¨¡å‹è¡¨ç°æ˜æ˜¾è¾ƒå·®ï¼Œè€Œæ›´å¤§çš„æ¨¡å‹å¹¶æ²¡æœ‰æ˜¾ç¤ºå‡ºæ€§èƒ½æå‡ã€‚æ¥è‡ªæ³Œå°¿å­¦ä»¥å¤–çš„åŒ»å­¦é¢†åŸŸçš„ä¾‹å­ä¹Ÿèƒ½æ”¹å–„å°‘æ ·æœ¬æç¤ºçš„ç»“æœï¼Œè¿™è¯æ˜äº†LLMså¤„ç†è‚¿ç˜¤è®°å½•æ‰€éœ€ä»»åŠ¡çš„èƒ½åŠ›ã€‚å¼€æºLLMsåœ¨è‡ªåŠ¨åŒ–è‚¿ç˜¤è®°å½•æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚å‚æ•°åœ¨7äº¿åˆ°12äº¿ä¹‹é—´çš„æ¨¡å‹å¯èƒ½åœ¨æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¹‹é—´æä¾›æœ€ä½³å¹³è¡¡ã€‚é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒä»¥åŠç²¾å¿ƒè®¾è®¡æç¤ºï¼Œè¿™äº›æ¨¡å‹æœ‰å¯èƒ½æˆä¸ºæœªæ¥ä¸´åºŠè®°å½•çš„é‡è¦å·¥å…·ã€‚è¯„ä¼°çš„ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/stefan-m-lenz/UroLlmEval%E8%8E%B7%E5%8F%96%E3%80%82%E6%88%91%E4%BB%AC%E8%BF%98%E5%8F%91%E5%B8%83%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BD%9C%E4%B8%BA%E6%96%B0%E7%9A%84%E5%AE%9D%E8%B4%B5%E8%B5%84%E6%BA%90%EF%BC%8C%E4%BB%A5%E8%A7%A3%E5%86%B3%E5%BE%B7%E5%9B%BD%E5%8C%BB%E5%AD%A6NLP%E9%A2%86%E5%9F%9F%E4%B8%AD%E7%9C%9F%E5%AE%9E%E3%80%81%E6%98%93%E4%BA%8E%E8%AE%BF%E9%97%AE%E7%9A%84%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E7%9F%AD%E7%BC%BA%E7%9A%84%E9%97%AE%E9%A2%98%E3%80%82">https://github.com/stefan-m-lenz/UroLlmEvalè·å–ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒè¯¥æ•°æ®é›†ä½œä¸ºæ–°çš„å®è´µèµ„æºï¼Œä»¥è§£å†³å¾·å›½åŒ»å­¦NLPé¢†åŸŸä¸­çœŸå®ã€æ˜“äºè®¿é—®çš„åŸºå‡†æµ‹è¯•æ•°æ®çŸ­ç¼ºçš„é—®é¢˜ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12106v4">PDF</a> 53 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>åœ¨è‚¿ç˜¤è®°å½•è¿‡ç¨‹ä¸­ï¼Œé‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿæé«˜æ•ˆç‡å’Œå¯é æ€§ã€‚æœ¬ç ”ç©¶å¯¹åä¸€æ¬¾ä¸åŒè§„æ¨¡çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶‰åŠè‚¿ç˜¤è¯Šæ–­è¯†åˆ«ã€ICD-10ç¼–ç åˆ†é…å’Œé¦–æ¬¡è¯Šæ–­æ—¥æœŸæå–ç­‰ä¸‰é¡¹åŸºæœ¬ä»»åŠ¡ã€‚ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹å¦‚Llama 3.1 8Bã€Mistral 7Bå’ŒMistral NeMo 12Bè¡¨ç°ä¼˜å¼‚ï¼Œè€Œè§„æ¨¡è¾ƒå°æˆ–è®­ç»ƒæ•°æ®ä¸è¶³çš„æ¨¡å‹æ€§èƒ½è¾ƒä½ã€‚ä¸åŒåŒ»å­¦é¢†åŸŸçš„æ•°æ®åœ¨å°‘é‡æç¤ºä¸­ä¹Ÿèƒ½æé«˜æ•ˆæœï¼Œè¯æ˜LLMå¤„ç†è‚¿ç˜¤è®°å½•ä»»åŠ¡çš„èƒ½åŠ›ã€‚å¼€æºLLMåœ¨è‡ªåŠ¨åŒ–è‚¿ç˜¤è®°å½•æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯å‚æ•°åœ¨7-12äº¿ä¹‹é—´çš„æ¨¡å‹ï¼Œåœ¨æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‚¿ç˜¤è®°å½•è¿‡ç¨‹ä¸­å…·æœ‰åº”ç”¨æ½œåŠ›ï¼Œèƒ½æé«˜æ•ˆç‡å’Œå¯é æ€§ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†ä¸åŒè§„æ¨¡å’Œç±»å‹çš„LLMåœ¨ä¸‰é¡¹åŸºæœ¬ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>Llama 3.1 8Bã€Mistral 7Bå’ŒMistral NeMo 12Bç­‰æ¨¡å‹åœ¨è¯„ä¼°ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½ä¸è®­ç»ƒæ•°æ®å’Œå‚æ•°è§„æ¨¡æœ‰å…³ï¼Œè¾ƒå°è§„æ¨¡çš„æ¨¡å‹è¡¨ç°è¾ƒä½ã€‚</li>
<li>ä¸åŒåŒ»å­¦é¢†åŸŸçš„æ•°æ®å¯¹äºæé«˜å°‘é‡æç¤ºçš„æ•ˆæœå…·æœ‰æ½œåŠ›ã€‚</li>
<li>å‚æ•°è§„æ¨¡åœ¨7-12äº¿ä¹‹é—´çš„æ¨¡å‹å¯èƒ½åœ¨æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12106">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4fc28baf82178a13b41990142b6ed33e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8671939663ce40fac1677377a0aed781.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Verbalized-Representation-Learning-for-Interpretable-Few-Shot-Generalization"><a href="#Verbalized-Representation-Learning-for-Interpretable-Few-Shot-Generalization" class="headerlink" title="Verbalized Representation Learning for Interpretable Few-Shot   Generalization"></a>Verbalized Representation Learning for Interpretable Few-Shot   Generalization</h2><p><strong>Authors:Cheng-Fu Yang, Da Yin, Wenbo Hu, Heng Ji, Nanyun Peng, Bolei Zhou, Kai-Wei Chang</strong></p>
<p>Humans recognize objects after observing only a few examples, a remarkable capability enabled by their inherent language understanding of the real-world environment. Developing verbalized and interpretable representation can significantly improve model generalization in low-data settings. In this work, we propose Verbalized Representation Learning (VRL), a novel approach for automatically extracting human-interpretable features for object recognition using few-shot data. Our method uniquely captures inter-class differences and intra-class commonalities in the form of natural language by employing a Vision-Language Model (VLM) to identify key discriminative features between different classes and shared characteristics within the same class. These verbalized features are then mapped to numeric vectors through the VLM. The resulting feature vectors can be further utilized to train and infer with downstream classifiers. Experimental results show that, at the same model scale, VRL achieves a 24% absolute improvement over prior state-of-the-art methods while using 95% less data and a smaller mode. Furthermore, compared to human-labeled attributes, the features learned by VRL exhibit a 20% absolute gain when used for downstream classification tasks. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/joeyy5588/VRL/tree/main">https://github.com/joeyy5588/VRL/tree/main</a>. </p>
<blockquote>
<p>äººç±»åªéœ€è¦è§‚å¯Ÿå‡ ä¸ªä¾‹å­å°±èƒ½è¯†åˆ«ç‰©ä½“ï¼Œè¿™ä¸€ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›å¾—ç›Šäºä»–ä»¬å¯¹çœŸå®ä¸–ç•Œç¯å¢ƒå›ºæœ‰çš„ç†è§£ã€‚åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œå¼€å‘è¯­è¨€åŒ–çš„å¯è§£é‡Šè¡¨ç¤ºå¯ä»¥æ˜¾è‘—æ”¹å–„æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†â€œè¯­è¨€åŒ–è¡¨ç¤ºå­¦ä¹ â€ï¼ˆVRLï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒèƒ½å¤Ÿåˆ©ç”¨å°‘é‡æ•°æ®è‡ªåŠ¨æå–ç”¨äºå¯¹è±¡è¯†åˆ«çš„å¯è§£é‡Šç‰¹å¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥è‡ªç„¶è¯­è¨€çš„å½¢å¼ç‹¬ç‰¹åœ°æ•æ‰ç±»é—´å·®å¼‚å’Œç±»å†…å…±æ€§ï¼Œé€šè¿‡é‡‡ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥è¯†åˆ«ä¸åŒç±»ä¹‹é—´çš„å…³é”®åˆ¤åˆ«ç‰¹å¾ä»¥åŠåŒä¸€ç±»å†…çš„å…±äº«ç‰¹å¾ã€‚è¿™äº›è¯­è¨€åŒ–çš„ç‰¹å¾ç„¶åé€šè¿‡VLMæ˜ å°„åˆ°æ•°å­—å‘é‡ä¸Šã€‚è¿™äº›ç‰¹å¾å‘é‡å¯è¿›ä¸€æ­¥ç”¨äºè®­ç»ƒå’Œæ¨æ–­ä¸‹æ¸¸åˆ†ç±»å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç›¸åŒæ¨¡å‹è§„æ¨¡ä¸‹ï¼ŒVRLç›¸è¾ƒäºç°æœ‰æœ€å‰æ²¿æŠ€æœ¯å®ç°äº†ç»å¯¹æ”¹å–„ç‡ä¸ºç™¾åˆ†ä¹‹äºŒåå››ï¼Œä¸”åœ¨å‡å°‘ç™¾åˆ†ä¹‹ä¹åäº”çš„æ•°æ®å’Œè¾ƒå°æ¨¡å‹çš„æƒ…å†µä¸‹ä»æœ‰æ•ˆã€‚æ­¤å¤–ï¼Œç›¸è¾ƒäºäººå·¥æ ‡æ³¨çš„å±æ€§ï¼Œä½¿ç”¨VRLå­¦ä¹ çš„ç‰¹å¾è¿›è¡Œä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡æ—¶è¡¨ç°å‡ºäº†ç»å¯¹å¢ç›Šç™¾åˆ†ä¹‹äºŒåçš„ä¼˜åŠ¿ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š&lt;<a target="_blank" rel="noopener" href="https://github.com/joeyy558">https://github.com/joeyy558</a> è®¿é—®æˆ‘ä»¬çš„ä¸»ç›®å½•ä»¥è·å–VRL&gt;ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18651v3">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVerbalized Representation Learningï¼ˆVRLï¼‰çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨å°‘é‡æ•°æ®è‡ªåŠ¨æå–äººç±»å¯è§£é‡Šçš„ç‰¹å¾è¿›è¡Œå¯¹è±¡è¯†åˆ«ã€‚é€šè¿‡é‡‡ç”¨Vision-Language Modelï¼ˆVLMï¼‰ï¼ŒVRLèƒ½å¤Ÿæ•æ‰ä¸åŒç±»åˆ«ä¹‹é—´çš„å·®å¼‚å’ŒåŒä¸€ç±»åˆ«å†…çš„å…±æ€§ï¼Œå°†è¿™äº›ç‰¹å¾è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€ï¼Œå¹¶æ˜ å°„ä¸ºæ•°å­—å‘é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ç›¸åŒçš„æ¨¡å‹è§„æ¨¡ä¸‹ï¼ŒVRLç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯å–å¾—äº†24%çš„ç»å¯¹æ”¹è¿›ï¼Œä¸”ä½¿ç”¨æ•°æ®æ›´å°‘ã€æ¨¡å‹æ›´å°ã€‚æ­¤å¤–ï¼Œç›¸è¾ƒäºäººå·¥æ ‡æ³¨çš„å±æ€§ï¼Œä½¿ç”¨VRLå­¦ä¹ çš„ç‰¹å¾è¿›è¡Œä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡æ—¶å–å¾—äº†20%çš„ç»å¯¹æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»åªéœ€è§‚å¯Ÿå°‘æ•°ä¾‹å­å°±èƒ½è¯†åˆ«ç‰©ä½“ï¼Œè¿™ä¸€èƒ½åŠ›å¾—ç›Šäºå¯¹çœŸå®ä¸–ç•Œç¯å¢ƒå†…åœ¨è¯­è¨€çš„ç†è§£ã€‚</li>
<li>å¼€å‘å¯è¯­è¨€åŒ–çš„è¡¨ç¤ºå­¦ä¹ ï¼ˆVRLï¼‰æ–¹æ³•èƒ½æ˜¾è‘—æé«˜æ¨¡å‹åœ¨æ•°æ®ç¨€ç¼ºç¯å¢ƒä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>VRLæ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æå–äººç±»å¯è§£é‡Šçš„ç‰¹å¾è¿›è¡Œå¯¹è±¡è¯†åˆ«ï¼Œåˆ©ç”¨å°‘é‡æ•°æ®ã€‚</li>
<li>VRLé‡‡ç”¨Vision-Language Modelï¼ˆVLMï¼‰æ¥è¯†åˆ«ä¸åŒç±»åˆ«ä¹‹é—´çš„å…³é”®åˆ¤åˆ«ç‰¹å¾å’ŒåŒä¸€ç±»åˆ«å†…çš„å…±äº«ç‰¹æ€§ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€ã€‚</li>
<li>VRLå°†è¯­è¨€åŒ–çš„ç‰¹å¾æ˜ å°„ä¸ºæ•°å­—å‘é‡ï¼Œè¿™äº›å‘é‡å¯ç”¨äºè®­ç»ƒå’Œæ¨æ–­ä¸‹æ¸¸åˆ†ç±»å™¨ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç›¸åŒçš„æ¨¡å‹è§„æ¨¡ä¸‹ï¼ŒVRLç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸”ä½¿ç”¨æ›´å°‘çš„æ•°æ®å’Œæ›´å°çš„æ¨¡å‹ã€‚</li>
<li>ä¸äººå·¥æ ‡æ³¨çš„å±æ€§ç›¸æ¯”ï¼Œä½¿ç”¨VRLå­¦ä¹ çš„ç‰¹å¾è¿›è¡Œä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡å–å¾—äº†æ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18651">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-18dba99dd59c22c3c29309aa84538fb6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a63c44975d3d4faa55d2ab02950c7f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4a4759ad61111ef5c4138271594c835.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CRAFT-Your-Dataset-Task-Specific-Synthetic-Dataset-Generation-Through-Corpus-Retrieval-and-Augmentation"><a href="#CRAFT-Your-Dataset-Task-Specific-Synthetic-Dataset-Generation-Through-Corpus-Retrieval-and-Augmentation" class="headerlink" title="CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through   Corpus Retrieval and Augmentation"></a>CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through   Corpus Retrieval and Augmentation</h2><p><strong>Authors:Ingo Ziegler, Abdullatif KÃ¶ksal, Desmond Elliott, Hinrich SchÃ¼tze</strong></p>
<p>Building high-quality datasets for specialized tasks is a time-consuming and resource-intensive process that often requires specialized domain knowledge. We propose Corpus Retrieval and Augmentation for Fine-Tuning (CRAFT), a method for generating synthetic datasets, given a small number of user-written few-shots that demonstrate the task to be performed. Given these examples, CRAFT uses large-scale public web-crawled corpora and similarity-based document retrieval to find other relevant human-written documents. Lastly, instruction-tuned large language models (LLMs) augment the retrieved documents into custom-formatted task samples, which then can be used for fine-tuning. We demonstrate that CRAFT can efficiently generate large-scale task-specific training datasets for four diverse tasks: biology, medicine, and commonsense question-answering (QA), as well as summarization. Our experiments show that CRAFT-based models outperform or match general LLMs on QA tasks, while exceeding models trained on human-curated summarization data by 46 preference points. CRAFT outperforms other synthetic dataset generation methods such as Self- and Evol-Instruct, and remains robust even when the quality of the initial few-shots varies. </p>
<blockquote>
<p>æ„å»ºé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„é«˜è´¨é‡æ•°æ®é›†æ˜¯ä¸€ä¸ªæ—¢è€—æ—¶åˆè€—èµ„æºçš„è¿‡ç¨‹ï¼Œé€šå¸¸éœ€è¦ç‰¹å®šçš„é¢†åŸŸçŸ¥è¯†ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºå¾®è°ƒçš„æ•°æ®é›†æ£€ç´¢ä¸æ‰©å……æ–¹æ³•ï¼ˆCRAFTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆåˆæˆæ•°æ®é›†çš„æ–¹æ³•ï¼Œåªéœ€å°‘é‡ç”¨æˆ·ç¼–å†™çš„ç®€çŸ­æŒ‡ä»¤å³å¯å±•ç¤ºè¦æ‰§è¡Œçš„ä»»åŠ¡ã€‚åŸºäºè¿™äº›ç¤ºä¾‹ï¼ŒCRAFTåˆ©ç”¨å¤§è§„æ¨¡å…¬å…±ç½‘ç»œçˆ¬è™«è¯­æ–™åº“å’ŒåŸºäºç›¸ä¼¼åº¦çš„æ–‡æ¡£æ£€ç´¢æ¥æ‰¾åˆ°å…¶ä»–ç›¸å…³çš„äººå†™æ–‡æ¡£ã€‚æœ€åï¼Œé€šè¿‡æŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£æ‰©å……ä¸ºè‡ªå®šä¹‰æ ¼å¼çš„ä»»åŠ¡æ ·æœ¬ï¼Œç„¶åå¯ç”¨äºå¾®è°ƒã€‚æˆ‘ä»¬è¯æ˜ï¼ŒCRAFTå¯ä»¥æœ‰æ•ˆåœ°ä¸ºå››ä¸ªä¸åŒçš„ä»»åŠ¡ç”Ÿæˆå¤§è§„æ¨¡çš„ä»»åŠ¡ç‰¹å®šè®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…æ‹¬ç”Ÿç‰©å­¦ã€åŒ»å­¦ã€å¸¸è¯†é—®ç­”ï¼ˆQAï¼‰ä»¥åŠæ‘˜è¦ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºCRAFTçš„æ¨¡å‹åœ¨é—®ç­”ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºæˆ–ç­‰åŒäºä¸€èˆ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨æ‘˜è¦æ–¹é¢è¶…è¿‡äº†ç»è¿‡äººå·¥ç¼–çº‚çš„æ‘˜è¦æ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼Œé«˜å‡º46ä¸ªåå¥½ç‚¹ã€‚CRAFTåœ¨å…¶ä»–åˆæˆæ•°æ®é›†ç”Ÿæˆæ–¹æ³•ï¼ˆå¦‚Self-å’ŒEvol-Instructï¼‰ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶ä¸”åœ¨åˆå§‹ç®€çŸ­æŒ‡ä»¤çš„è´¨é‡å‘ç”Ÿå˜åŒ–æ—¶ä»ç„¶ä¿æŒç¨³å¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02098v2">PDF</a> Accepted at TACL; Pre-MIT Press publication version. Code and dataset   available at: <a target="_blank" rel="noopener" href="https://github.com/ziegler-ingo/CRAFT">https://github.com/ziegler-ingo/CRAFT</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æå‡ºäº†ä¸€ç§åŸºäºè¯­æ–™åº“æ£€ç´¢å’Œæ‰©å……çš„æ–¹æ³•ï¼ˆCRAFTï¼‰ï¼Œç”¨äºç”Ÿæˆç‰¹å®šä»»åŠ¡çš„åˆæˆæ•°æ®é›†ã€‚æ­¤æ–¹æ³•åªéœ€å°‘é‡ç”¨æˆ·ç¼–å†™çš„æ ·æœ¬ç¤ºèŒƒä»»åŠ¡ï¼Œå°±èƒ½ä»å¤§è§„æ¨¡å…¬å…±ç½‘ç»œçˆ¬å–çš„è¯­æ–™åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œå¹¶ä½¿ç”¨æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ‰©å……ï¼Œå½¢æˆå®šåˆ¶çš„ä»»åŠ¡æ ·æœ¬ï¼Œå¯ç”¨äºç²¾ç»†è°ƒæ•´ã€‚å®éªŒè¡¨æ˜ï¼ŒCRAFTåœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬ç”Ÿç‰©å­¦ã€åŒ»å­¦ã€å¸¸è¯†é—®ç­”å’Œæ‘˜è¦ç­‰ã€‚ç‰¹åˆ«æ˜¯åœ¨é—®ç­”ä»»åŠ¡ä¸Šï¼ŒåŸºäºCRAFTçš„æ¨¡å‹æ€§èƒ½ä¼˜äºæˆ–åŒ¹é…é€šç”¨LLMï¼›åœ¨æ‘˜è¦ä»»åŠ¡ä¸Šï¼Œå…¶è¡¨ç°è¶…è¿‡ç»è¿‡äººå·¥æ•´ç†çš„æ•°æ®è®­ç»ƒçš„æ¨¡å‹ã€‚ç›¸è¾ƒäºå…¶ä»–åˆæˆæ•°æ®é›†ç”Ÿæˆæ–¹æ³•ï¼ŒCRAFTå…·æœ‰ç¨³å¥æ€§ï¼Œåˆå§‹æ ·æœ¬è´¨é‡æ³¢åŠ¨ä¹Ÿä¸ä¼šå½±å“å…¶æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CRAFTæ˜¯ä¸€ç§åŸºäºè¯­æ–™åº“æ£€ç´¢å’Œæ‰©å……çš„åˆæˆæ•°æ®é›†ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>CRAFTä½¿ç”¨å°‘é‡ç”¨æˆ·ç¼–å†™çš„æ ·æœ¬ç¤ºèŒƒä»»åŠ¡æ¥ç”Ÿæˆå¤§è§„æ¨¡ä»»åŠ¡ç‰¹å®šè®­ç»ƒæ•°æ®é›†ã€‚</li>
<li>CRAFTèƒ½å¤Ÿä»å¤§è§„æ¨¡å…¬å…±ç½‘ç»œçˆ¬å–çš„è¯­æ–™åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ã€‚</li>
<li>CRAFTåˆ©ç”¨æŒ‡ä»¤å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰©å……æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œå½¢æˆå®šåˆ¶çš„ä»»åŠ¡æ ·æœ¬ã€‚</li>
<li>CRAFTåœ¨ç”Ÿç‰©å­¦ã€åŒ»å­¦ã€å¸¸è¯†é—®ç­”å’Œæ‘˜è¦ç­‰å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>åœ¨é—®ç­”ä»»åŠ¡ä¸Šï¼ŒåŸºäºCRAFTçš„æ¨¡å‹æ€§èƒ½ä¼˜äºæˆ–åŒ¹é…é€šç”¨LLMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.02098">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2a9adfd838e536fcbc2f786db49957af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-42d76b246d84322f1c5364930f29a881.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04b46b8a783a2a8d1708c5c1b77b1ae0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-c4f3381c9ca1a8d2d3266d998931c981.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-09  MM2CT MR-to-CT translation for multi-modal image fusion with mamba
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4a1112d792badfedfc948aec22fe66a0.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-09  MELLA Bridging Linguistic Capability and Cultural Groundedness for   Low-Resource Language MLLMs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
