<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-08-09  MV-Debate Multi-view Agent Debate with Dynamic Reflection Gating for   Multimodal Harmful Content Detection in Social Media">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-826cd0e8631ec02eb234f09c5a99cf00.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    50 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-09-更新"><a href="#2025-08-09-更新" class="headerlink" title="2025-08-09 更新"></a>2025-08-09 更新</h1><h2 id="MV-Debate-Multi-view-Agent-Debate-with-Dynamic-Reflection-Gating-for-Multimodal-Harmful-Content-Detection-in-Social-Media"><a href="#MV-Debate-Multi-view-Agent-Debate-with-Dynamic-Reflection-Gating-for-Multimodal-Harmful-Content-Detection-in-Social-Media" class="headerlink" title="MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for   Multimodal Harmful Content Detection in Social Media"></a>MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for   Multimodal Harmful Content Detection in Social Media</h2><p><strong>Authors:Rui Lu, Jinhe Bi, Yunpu Ma, Feng Xiao, Yuntao Du, Yijun Tian</strong></p>
<p>Social media has evolved into a complex multimodal environment where text, images, and other signals interact to shape nuanced meanings, often concealing harmful intent. Identifying such intent, whether sarcasm, hate speech, or misinformation, remains challenging due to cross-modal contradictions, rapid cultural shifts, and subtle pragmatic cues. To address these challenges, we propose MV-Debate, a multi-view agent debate framework with dynamic reflection gating for unified multimodal harmful content detection. MV-Debate assembles four complementary debate agents, a surface analyst, a deep reasoner, a modality contrast, and a social contextualist, to analyze content from diverse interpretive perspectives. Through iterative debate and reflection, the agents refine responses under a reflection-gain criterion, ensuring both accuracy and efficiency. Experiments on three benchmark datasets demonstrate that MV-Debate significantly outperforms strong single-model and existing multi-agent debate baselines. This work highlights the promise of multi-agent debate in advancing reliable social intent detection in safety-critical online contexts. </p>
<blockquote>
<p>社交媒体已经演变为一个复杂的多媒体环境，文本、图像和其他信号在此环境中相互作用，形成微妙的含义，常常隐藏有害的意图。识别这种意图，无论是讽刺、仇恨言论还是错误信息，仍然是一个挑战，原因在于跨模态的矛盾、快速的文化变迁和微妙的语用线索。为了应对这些挑战，我们提出了MV-Debate，这是一个具有动态反射门控的多视图代理辩论框架，用于统一多媒体有害内容检测。MV-Debate汇集了四种互补的辩论代理，包括表面分析师、深度推理者、模态对比代理和社会语境主义者，从多种解释性视角分析内容。通过迭代辩论和反思，代理在反思增益标准下完善回应，确保准确性和效率。在三个基准数据集上的实验表明，MV-Debate显著优于强大的单模型和现有的多代理辩论基线。这项工作突出了多代理辩论在推进安全关键在线环境下的可靠社会意图检测方面的前景。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05557v1">PDF</a> </p>
<p><strong>Summary</strong><br>社交媒体的复杂性在于其已演变为一个多媒体环境，文本、图像等信号相互交织，形成微妙的含义，常常隐藏有害意图。识别这些意图（如讽刺、仇恨言论或虚假信息）颇具挑战，因为存在跨模态矛盾、文化快速变迁和微妙的语用暗示。为解决这些挑战，我们提出了MV-Debate多视角辩论框架，具有动态反思门控功能，用于统一多媒体有害内容检测。MV-Debate集结了四种互补的辩论代理，表层分析师、深度推理者、模态对比器和社交语境专家，从不同角度解析内容。通过迭代辩论和反思，代理在反思增益标准下完善回应，确保准确性和效率。在三个基准数据集上的实验表明，MV-Debate显著优于强大的单一模型和现有的多代理辩论基线。这项工作突显了多代理辩论在推进在线安全环境中可靠的社会意图检测方面的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>社交媒体已演变为一个复杂的多媒体环境，其中文本、图像等信号形成微妙的含义，隐藏有害意图的识别具有挑战性。</li>
<li>MV-Debate是一个多视角辩论框架，通过集结多种代理来解决有害内容检测问题。</li>
<li>MV-Debate包含的代理包括表层分析师、深度推理者、模态对比器和社交语境专家，各自具备独特的分析视角。</li>
<li>通过迭代辩论和反思，MV-Debate代理能够完善回应，确保准确性和效率。</li>
<li>实验证明MV-Debate在基准数据集上的表现优于单一模型和多代理辩论基线。</li>
<li>MV-Debate框架具有动态反思门控功能，这有助于应对跨模态矛盾、文化快速变迁和微妙的语用暗示等挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05557">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8e92e29d6614a95d36eca1f6a8f135af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-23e734c4fcd86657c493ad23ef3710db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-826cd0e8631ec02eb234f09c5a99cf00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e0de30c10b417758b059cf561f1c890.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="AutoIAD-Manager-Driven-Multi-Agent-Collaboration-for-Automated-Industrial-Anomaly-Detection"><a href="#AutoIAD-Manager-Driven-Multi-Agent-Collaboration-for-Automated-Industrial-Anomaly-Detection" class="headerlink" title="AutoIAD: Manager-Driven Multi-Agent Collaboration for Automated   Industrial Anomaly Detection"></a>AutoIAD: Manager-Driven Multi-Agent Collaboration for Automated   Industrial Anomaly Detection</h2><p><strong>Authors:Dongwei Ji, Bingzhang Hu, Yi Zhou</strong></p>
<p>Industrial anomaly detection (IAD) is critical for manufacturing quality control, but conventionally requires significant manual effort for various application scenarios. This paper introduces AutoIAD, a multi-agent collaboration framework, specifically designed for end-to-end automated development of industrial visual anomaly detection. AutoIAD leverages a Manager-Driven central agent to orchestrate specialized sub-agents (including Data Preparation, Data Loader, Model Designer, Trainer) and integrates a domain-specific knowledge base, which intelligently handles the entire pipeline using raw industrial image data to develop a trained anomaly detection model. We construct a comprehensive benchmark using MVTec AD datasets to evaluate AutoIAD across various LLM backends. Extensive experiments demonstrate that AutoIAD significantly outperforms existing general-purpose agentic collaboration frameworks and traditional AutoML frameworks in task completion rate and model performance (AUROC), while effectively mitigating issues like hallucination through iterative refinement. Ablation studies further confirm the crucial roles of the Manager central agent and the domain knowledge base module in producing robust and high-quality IAD solutions. </p>
<blockquote>
<p>工业异常检测（IAD）对于制造质量控制至关重要，但传统上需要针对各种应用场景投入大量手动工作。本文介绍了AutoIAD，这是一个多智能体协作框架，专为端到端的工业视觉异常检测自动化开发而设计。AutoIAD利用Manager驱动的中央智能体来协调专业的子智能体（包括数据准备、数据加载器、模型设计器、训练器），并集成一个特定的领域知识库，该知识库能够智能地处理整个管道，使用原始工业图像数据来开发经过训练的异常检测模型。我们使用MVTec AD数据集构建了一个全面的基准测试来评估AutoIAD在各种大型语言模型后端的表现。大量实验表明，AutoIAD在任务完成率和模型性能（AUROC）方面显著优于现有的通用智能体协作框架和传统AutoML框架，同时有效地解决了诸如幻觉之类的问题通过迭代优化。消融研究进一步证实了中央管理智能体和领域知识库模块在产生稳健、高质量IAD解决方案中的关键作用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05503v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>工业异常检测（IAD）对制造质量控制至关重要，但传统方法需要针对各种应用场景投入大量手动工作。本文介绍了AutoIAD，这是一个多智能体协作框架，专为端到端的工业视觉异常检测自动化开发设计。AutoIAD利用经理驱动的中央智能体来协调专业子智能体（包括数据准备、数据加载器、模型设计师、训练师），并整合领域特定知识库，智能处理整个管道，使用原始工业图像数据来开发训练好的异常检测模型。我们使用MVTec AD数据集构建了一个全面的基准测试，以评估AutoIAD在各种大型语言模型后端的表现。大量实验表明，AutoIAD在任务完成率和模型性能（AUROC）方面显著优于现有的通用智能体协作框架和传统AutoML框架，同时有效缓解了通过迭代优化产生的幻觉问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>工业异常检测（IAD）在制造质量控制中起关键作用，但传统方法需要大量手动操作。</li>
<li>AutoIAD是一个多智能体协作框架，用于端到端的工业视觉异常检测自动化开发。</li>
<li>AutoIAD利用中央智能体来管理和协调专业子智能体的工作。</li>
<li>AutoIAD集成了领域特定的知识库，以智能处理整个管道并使用原始数据训练模型。</li>
<li>与其他智能体协作框架和AutoML框架相比，AutoIAD在任务完成率和模型性能上表现更优秀。</li>
<li>AutoIAD通过迭代优化有效缓解了幻觉问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05503">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e79e976389f89661810f770fecfe44c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd13ec77df0f0c3d7363b288344ffd49.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5fc04f66fe9733b9280dac81b20325a5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a996de4376260a8afdf9ec9273d8ba27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8409e6e39b117a2641cc2f0786dbab23.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MoMA-A-Mixture-of-Multimodal-Agents-Architecture-for-Enhancing-Clinical-Prediction-Modelling"><a href="#MoMA-A-Mixture-of-Multimodal-Agents-Architecture-for-Enhancing-Clinical-Prediction-Modelling" class="headerlink" title="MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical   Prediction Modelling"></a>MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical   Prediction Modelling</h2><p><strong>Authors:Jifan Gao, Mahmudur Rahman, John Caskey, Madeline Oguss, Ann O’Rourke, Randy Brown, Anne Stey, Anoop Mayampurath, Matthew M. Churpek, Guanhua Chen, Majid Afshar</strong></p>
<p>Multimodal electronic health record (EHR) data provide richer, complementary insights into patient health compared to single-modality data. However, effectively integrating diverse data modalities for clinical prediction modeling remains challenging due to the substantial data requirements. We introduce a novel architecture, Mixture-of-Multimodal-Agents (MoMA), designed to leverage multiple large language model (LLM) agents for clinical prediction tasks using multimodal EHR data. MoMA employs specialized LLM agents (“specialist agents”) to convert non-textual modalities, such as medical images and laboratory results, into structured textual summaries. These summaries, together with clinical notes, are combined by another LLM (“aggregator agent”) to generate a unified multimodal summary, which is then used by a third LLM (“predictor agent”) to produce clinical predictions. Evaluating MoMA on three prediction tasks using real-world datasets with different modality combinations and prediction settings, MoMA outperforms current state-of-the-art methods, highlighting its enhanced accuracy and flexibility across various tasks. </p>
<blockquote>
<p>多模态电子健康记录（EHR）数据相比单模态数据，为患者健康提供了更丰富、互补的见解。然而，由于需要大量数据，有效地整合各种数据模式进行临床预测建模仍然具有挑战性。我们引入了一种新型架构——多模态代理混合体（MoMA），旨在利用多模态EHR数据和多模态大型语言模型（LLM）代理进行临床预测任务。MoMA采用专门的大型语言模型代理（“专家代理”）将非文本模式（如医疗图像和实验室结果）转换为结构化文本摘要。这些摘要与临床笔记相结合，由另一个大型语言模型（“聚合器代理”）生成统一的多模态摘要，然后第三个大型语言模型（“预测器代理”）使用该摘要生成临床预测。通过对三个使用真实世界数据集的不同预测任务模式组合和预测设置的评估，MoMA的性能优于当前的最先进方法，突出了其在不同任务上的准确性和灵活性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05492v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>多模态电子病历（EHR）数据较单模态数据能提供更丰富、更互补的病人健康信息。然而，由于数据需求量大，有效整合多种数据模式进行临床预测建模仍然具有挑战性。本研究介绍了一种新型架构——混合多模态代理（MoMA），利用多个大型语言模型（LLM）代理进行临床预测任务，使用多模态EHR数据。MoMA采用专业LLM代理转化非文本模式，如医疗图像和实验室结果，为结构化文本摘要。这些摘要与临床笔记相结合，由另一个LLM（聚合代理）生成统一的多模态摘要，随后被第三个LLM（预测代理）用于生成临床预测。在真实数据集上的三个预测任务评估显示，MoMA优于当前最先进的方法，凸显其在不同任务中的准确性和灵活性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态电子病历数据提供丰富、互补的病人健康信息。</li>
<li>有效整合多种数据模式进行临床预测建模存在挑战。</li>
<li>引入了一种新型架构MoMA，利用多个大型语言模型代理处理多模态EHR数据。</li>
<li>MoMA包括三种类型的LLM代理：专业代理、聚合代理和预测代理。</li>
<li>专业代理将非文本模式转化为结构化文本摘要。</li>
<li>MoMA在多种预测任务中表现出卓越的性能和灵活性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05492">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f8590db4c0133fdd94ef4afa36863f40.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DeepPHY-Benchmarking-Agentic-VLMs-on-Physical-Reasoning"><a href="#DeepPHY-Benchmarking-Agentic-VLMs-on-Physical-Reasoning" class="headerlink" title="DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning"></a>DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning</h2><p><strong>Authors:Xinrun Xu, Pi Bu, Ye Wang, Börje F. Karlsson, Ziming Wang, Tengtao Song, Qi Zhu, Jun Song, Zhiming Ding, Bo Zheng</strong></p>
<p>Although Vision Language Models (VLMs) exhibit strong perceptual abilities and impressive visual reasoning, they struggle with attention to detail and precise action planning in complex, dynamic environments, leading to subpar performance. Real-world tasks typically require complex interactions, advanced spatial reasoning, long-term planning, and continuous strategy refinement, usually necessitating understanding the physics rules of the target scenario. However, evaluating these capabilities in real-world scenarios is often prohibitively expensive. To bridge this gap, we introduce DeepPHY, a novel benchmark framework designed to systematically evaluate VLMs’ understanding and reasoning about fundamental physical principles through a series of challenging simulated environments. DeepPHY integrates multiple physical reasoning environments of varying difficulty levels and incorporates fine-grained evaluation metrics. Our evaluation finds that even state-of-the-art VLMs struggle to translate descriptive physical knowledge into precise, predictive control. </p>
<blockquote>
<p>尽管视觉语言模型（VLMs）表现出强大的感知能力和令人印象深刻的视觉推理能力，但在复杂、动态的环境中，它们在细节关注和精确行动规划方面存在困难，导致性能不佳。现实世界的任务通常需要复杂的交互、高级的空间推理、长期规划和不断优化的策略，通常需要理解目标场景的物理规则。然而，在真实场景中对这些能力进行评估通常成本高昂。为了弥补这一差距，我们引入了DeepPHY，这是一个新型基准框架，旨在通过一系列具有挑战性的模拟环境，系统地评估VLMs对基本物理原理的理解和推理能力。DeepPHY集成了多个难度不同的物理推理环境，并采用了精细的评估指标。我们的评估发现，即使是最先进的VLMs也很难将描述性的物理知识转化为精确、预测性的控制。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05405v1">PDF</a> 48 pages</p>
<p><strong>Summary</strong></p>
<p>视觉语言模型（VLMs）虽具有强大的感知能力和令人印象深刻的视觉推理能力，但在复杂动态环境中对细节的关注和精确行动规划方面存在困难，导致性能不佳。现实任务通常需要复杂的交互、高级空间推理、长期规划和连续的策略调整，通常需要理解目标场景的物理规则。为解决评估这些能力在现实场景中的高昂成本问题，我们提出了DeepPHY这一新型基准框架，旨在通过一系列具有挑战性的模拟环境系统地评估VLMs对基本物理原理的理解和推理能力。DeepPHY融合了不同难度级别的多个物理推理环境，并采用了精细的评价指标。评估发现，即使是最新VLMs在将描述性物理知识转化为精确预测控制方面仍面临困难。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMs在复杂动态环境中对细节关注和精确行动规划方面存在挑战。</li>
<li>完成现实任务需要VLMs理解物理规则、进行复杂交互、高级空间推理、长期规划等。</li>
<li>评估VLMs在现实场景中的能力通常成本高昂。</li>
<li>DeepPHY是一个新型基准框架，用于系统地评估VLMs在物理原理方面的理解和推理能力。</li>
<li>DeepPHY通过模拟环境评估VLMs，融合了不同难度级别的多个物理推理环境。</li>
<li>DeepPHY采用了精细的评价指标来评估VLMs的表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05405">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b7003d6885b8d5a5388f55cd992363dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70f5c706b1b93a92d25edaf7b6c0fb5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df3882b558bf672b7065cfe2142d4d37.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="The-Term-‘Agent’-Has-Been-Diluted-Beyond-Utility-and-Requires-Redefinition"><a href="#The-Term-‘Agent’-Has-Been-Diluted-Beyond-Utility-and-Requires-Redefinition" class="headerlink" title="The Term ‘Agent’ Has Been Diluted Beyond Utility and Requires   Redefinition"></a>The Term ‘Agent’ Has Been Diluted Beyond Utility and Requires   Redefinition</h2><p><strong>Authors:Brinnae Bent</strong></p>
<p>The term ‘agent’ in artificial intelligence has long carried multiple interpretations across different subfields. Recent developments in AI capabilities, particularly in large language model systems, have amplified this ambiguity, creating significant challenges in research communication, system evaluation and reproducibility, and policy development. This paper argues that the term ‘agent’ requires redefinition. Drawing from historical analysis and contemporary usage patterns, we propose a framework that defines clear minimum requirements for a system to be considered an agent while characterizing systems along a multidimensional spectrum of environmental interaction, learning and adaptation, autonomy, goal complexity, and temporal coherence. This approach provides precise vocabulary for system description while preserving the term’s historically multifaceted nature. After examining potential counterarguments and implementation challenges, we provide specific recommendations for moving forward as a field, including suggestions for terminology standardization and framework adoption. The proposed approach offers practical tools for improving research clarity and reproducibility while supporting more effective policy development. </p>
<blockquote>
<p>人工智能中的“代理”一词在不同子领域长期以来存在多种解释。人工智能能力的最近发展，特别是在大型语言模型系统方面，加剧了这种模糊性，给研究交流、系统评估和可重复性、政策制定带来了重大挑战。本文认为“代理”一词需要重新定义。我们从历史分析和当前使用模式出发，提出了一个框架，明确了被视为代理系统的最低要求，同时沿环境交互、学习和适应、自主性、目标复杂性和时间连贯性的多维光谱对系统进行特征描述。这种方法为系统描述提供了精确的词汇，同时保留了该术语历史上的多面性。在研究了潜在的反对意见和实施挑战后，我们为领域的发展提供了具体建议，包括术语标准化和框架采纳的建议。所提出的方法为改善研究的清晰度和可重复性提供了实用工具，同时支持更有效的政策制定。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05338v1">PDF</a> Accepted to AIES 2025</p>
<p><strong>总结</strong></p>
<p>随着人工智能的发展，特别是在大型语言模型系统方面，人工智能中的“代理”一词的多种解释造成了巨大的沟通障碍、系统评估和再现性挑战以及政策制定难题。本文主张重新定义“代理”的定义，从历史和当前使用模式出发，提出一个框架，明确系统被视为代理的最小要求，同时按环境交互、学习和适应、自主性、目标复杂性和时间连贯性的多维度光谱进行特征描述。该框架为系统描述提供了精确的词汇，同时保留了该术语历史上的多面性。在研究了潜在的反对论点与实施挑战后，本文给出了前进的建议，包括术语标准化和框架采纳等。该框架有助于提升研究的清晰度和再现性，同时支持更有效的政策制定。</p>
<p><strong>要点总结</strong></p>
<ol>
<li>‘代理’一词在人工智能的不同子领域存在多种解释。</li>
<li>大型语言模型系统的最新发展加剧了这一模糊性，带来了研究沟通、系统评估和策略制定方面的挑战。</li>
<li>文章主张重新定义’代理’，并提出了一个定义清晰的框架，明确了被视为代理的系统必须满足的最小要求。</li>
<li>该框架将系统特征化在一个多维度的光谱上，包括与环境互动、学习适应性、自主性、目标复杂性和时间连贯性等方面。</li>
<li>此框架为系统描述提供了精确词汇，同时保留了术语的历史多元性。</li>
<li>文章考虑了潜在的反对论点与实施挑战，并给出了前进的建议，如术语标准化和框架采纳等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05338">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a737d928a368dc5b7dbde73a6fcb2477.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5ae4ca7cd54d33d4ba23012a0c581b3.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Congestion-Mitigation-Path-Planning-for-Large-Scale-Multi-Agent-Navigation-in-Dense-Environments"><a href="#Congestion-Mitigation-Path-Planning-for-Large-Scale-Multi-Agent-Navigation-in-Dense-Environments" class="headerlink" title="Congestion Mitigation Path Planning for Large-Scale Multi-Agent   Navigation in Dense Environments"></a>Congestion Mitigation Path Planning for Large-Scale Multi-Agent   Navigation in Dense Environments</h2><p><strong>Authors:Takuro Kato, Keisuke Okumura, Yoko Sasaki, Naoya Yokomachi</strong></p>
<p>In high-density environments where numerous autonomous agents move simultaneously in a distributed manner, streamlining global flows to mitigate local congestion is crucial to maintain overall navigation efficiency. This paper introduces a novel path-planning problem, congestion mitigation path planning (CMPP), which embeds congestion directly into the cost function, defined by the usage of incoming edges along agents’ paths. CMPP assigns a flow-based multiplicative penalty to each vertex of a sparse graph, which grows steeply where frequently-traversed paths intersect, capturing the intuition that congestion intensifies where many agents enter the same area from different directions. Minimizing the total cost yields a set of coarse-level, time-independent routes that autonomous agents can follow while applying their own local collision avoidance. We formulate the problem and develop two solvers: (i) an exact mixed-integer nonlinear programming solver for small instances, and (ii) a scalable two-layer search algorithm, A-CMTS, which quickly finds suboptimal solutions for large-scale instances and iteratively refines them toward the optimum. Empirical studies show that augmenting state-of-the-art collision-avoidance planners with CMPP significantly reduces local congestion and enhances system throughput in both discrete- and continuous-space scenarios. These results indicate that CMPP improves the performance of multi-agent systems in real-world applications such as logistics and autonomous-vehicle operations. </p>
<blockquote>
<p>在高密度环境中，众多自主代理以分布式方式同时移动，优化全局流程以减轻局部拥堵对于保持整体导航效率至关重要。本文引入了一个新的路径规划问题，即拥堵缓解路径规划（CMPP），它将拥堵直接嵌入到成本函数中，该成本函数由代理路径上的传入边使用定义。CMPP为每个稀疏图的顶点分配基于流量的乘法惩罚，在经常通行的路径交汇处增长迅速，直觉地反映了从多个方向进入同一区域的区域拥堵加剧的情况。最小化总成本可以得到一组粗略的、时间独立的路线，自主代理可以遵循这些路线，同时应用自己的局部避障策略。我们制定这个问题并开发了两种求解器：（i）针对小规模实例的精确混合整数非线性规划求解器，（ii）可伸缩的两层搜索算法A-CMTS，该算法可以快速找到大规模实例的次优解，并迭代地优化它们以达到最优。实证研究表明，使用CMPP增强最先进的避障规划器可以显着减少局部拥堵，并在离散和连续空间场景中提高系统吞吐量。这些结果表明，CMPP在物流和自动驾驶车辆操作等实际应用中提高了多代理系统的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05253v1">PDF</a> Accepted for publication in IEEE Robotics and Automation Letters   (RA-L), 2025. 9 pages, 6 figures, 3 tables. (C) 2025 IEEE. CC BY 4.0 license.   Supplementary videos will be accessible via IEEE Xplore upon publication</p>
<p><strong>摘要</strong></p>
<p>在高密度环境中，众多自主代理同时分布式移动，优化全局流程以减轻局部拥堵对于维持整体导航效率至关重要。本文引入了一种新的路径规划问题——拥堵缓解路径规划（CMPP），它将拥堵直接嵌入到成本函数中，该成本函数由代理路径上的传入边使用次数定义。CMPP为每个稀疏图的顶点分配一个基于流量的乘法惩罚，在频繁通行的路径交汇处增长迅速，体现了拥堵强度随许多代理从不同方向进入同一区域而增强的直觉。最小化总成本得到一组自主代理可以遵循的粗粒度、时间独立路线，同时应用各自的局部避障策略。我们制定了问题并开发了两个求解器：（i）针对小实例的精确混合整数非线性规划求解器，（ii）针对大规模实例的快速找到次优解并进行迭代优化的两层搜索算法A-CMTS。实证研究表明，将CMPP与最新的避障规划器相结合，显著减少了局部拥堵，提高了系统吞吐量，无论是在离散空间还是连续空间场景中都是如此。这些结果表明，CMPP在物流和自动驾驶车辆操作等实际应用中提高了多智能体系统的性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>在高密度环境中，缓解局部拥堵对于维持多自主代理的整体导航效率至关重要。</li>
<li>引入新的路径规划问题——拥堵缓解路径规划（CMPP），将拥堵直接纳入成本函数。</li>
<li>CMPP通过为稀疏图的每个顶点分配基于流量的乘法惩罚，在拥堵热点处增强成本。</li>
<li>最小化成含本可以得到自主代理遵循的粗粒度路线，结合局部避障策略。</li>
<li>开发了两种求解器：混合整数非线性规划求解器和两层搜索算法A-CMTS。</li>
<li>实证研究表明，CMPP能显著减少局部拥堵并提高系统吞吐量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05253">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ae3bbac69f7aee71cc8752a8317c2181.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ca1a9635c99ab451b1039928cb4ac2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b36b540912b3009c465b093cccc012dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a40d8bba8f37fa746ef88a55b640c18.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-105b78936b145e1530d0ee360ddaa8ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d2585d8293e1e8fb52c139aa8a3a3101.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Domain-driven-Metrics-for-Reinforcement-Learning-A-Case-Study-on-Epidemic-Control-using-Agent-based-Simulation"><a href="#Domain-driven-Metrics-for-Reinforcement-Learning-A-Case-Study-on-Epidemic-Control-using-Agent-based-Simulation" class="headerlink" title="Domain-driven Metrics for Reinforcement Learning: A Case Study on   Epidemic Control using Agent-based Simulation"></a>Domain-driven Metrics for Reinforcement Learning: A Case Study on   Epidemic Control using Agent-based Simulation</h2><p><strong>Authors:Rishabh Gaur, Gaurav Deshkar, Jayanta Kshirsagar, Harshal Hayatnagarkar, Janani Venugopalan</strong></p>
<p>For the development and optimization of agent-based models (ABMs) and rational agent-based models (RABMs), optimization algorithms such as reinforcement learning are extensively used. However, assessing the performance of RL-based ABMs and RABMS models is challenging due to the complexity and stochasticity of the modeled systems, and the lack of well-standardized metrics for comparing RL algorithms. In this study, we are developing domain-driven metrics for RL, while building on state-of-the-art metrics. We demonstrate our &#96;&#96;Domain-driven-RL-metrics’’ using policy optimization on a rational ABM disease modeling case study to model masking behavior, vaccination, and lockdown in a pandemic. Our results show the use of domain-driven rewards in conjunction with traditional and state-of-the-art metrics for a few different simulation scenarios such as the differential availability of masks. </p>
<blockquote>
<p>对于基于代理的模型（ABM）和理性代理模型（RABM）的开发和优化，广泛使用了强化学习等优化算法。然而，由于所建系统的复杂性和随机性，以及缺乏用于比较强化学习算法的标准化指标，评估基于强化学习的ABM模型和RABMS模型的性能是一个挑战。本研究在最新指标的基础上，开发面向领域的强化学习指标。我们通过理性ABM疾病建模案例研究中的策略优化来展示我们的“面向领域的强化学习指标”，以模拟大流行病中的掩饰行为、疫苗接种和封锁状态。我们的结果表明，在不同的模拟场景下，结合传统和最新指标的领域驱动奖励的使用效果，如口罩的不同可用性。我们的方法在理性ABM的疾病建模中展示了有效性，提供了一种新的评估框架来捕捉与领域相关的关键性能指标。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05154v1">PDF</a> </p>
<p><strong>Summary</strong><br>在基于代理的模型（ABM）和理性代理模型（RABM）的开发和优化过程中，强化学习等优化算法被广泛应用。然而，由于系统的复杂性和随机性，以及缺乏比较强化学习算法的标准化指标，评估RL驱动的ABM和RABM模型性能面临挑战。本研究在最新指标的基础上，开发面向领域的强化学习指标。我们通过理性ABM疾病建模案例研究中的策略优化来展示我们的面向领域的强化学习指标，该案例涉及建模口罩佩戴行为、疫苗接种和疫情封锁。结果表明，在多种模拟场景下，使用面向领域的奖励与传统和最新指标相结合，如口罩的差异性供应。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习算法广泛应用于ABM和RABM模型的优化。</li>
<li>评估RL驱动的ABM和RABM模型性能具有挑战性，主要原因是系统的复杂性和随机性，以及缺乏标准化的比较指标。</li>
<li>本研究致力于开发面向领域的强化学习指标。</li>
<li>通过理性ABM疾病建模案例展示了新的面向领域的强化学习指标，包括口罩佩戴行为、疫苗接种和疫情封锁的建模。</li>
<li>结合传统和最新指标，使用面向领域的奖励在多种模拟场景下表现良好。</li>
<li>在模拟场景中考虑了口罩供应的差异性等实际情况。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05154">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-76382f269c7550c5aaa0d8f9961ff5c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c87a0417ff158fd1f97a623c2af724e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d217bbd9eca9970677ce93c322cbad4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f74054db02c788cca5420ed637c57f8b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SE-Agent-Self-Evolution-Trajectory-Optimization-in-Multi-Step-Reasoning-with-LLM-Based-Agents"><a href="#SE-Agent-Self-Evolution-Trajectory-Optimization-in-Multi-Step-Reasoning-with-LLM-Based-Agents" class="headerlink" title="SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning   with LLM-Based Agents"></a>SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning   with LLM-Based Agents</h2><p><strong>Authors:Jiaye Lin, Yifu Guo, Yuzhen Han, Sen Hu, Ziyi Ni, Licheng Wang, Mingguang Chen, Daxin Jiang, Binxing Jiao, Chen Hu, Huacan Wang</strong></p>
<p>Large Language Model (LLM)-based agents have recently shown impressive capabilities in complex reasoning and tool use via multi-step interactions with their environments. While these agents have the potential to tackle complicated tasks, their problem-solving process, i.e., agents’ interaction trajectory leading to task completion, remains underexploited. These trajectories contain rich feedback that can navigate agents toward the right directions for solving problems correctly. Although prevailing approaches, such as Monte Carlo Tree Search (MCTS), can effectively balance exploration and exploitation, they ignore the interdependence among various trajectories and lack the diversity of search spaces, which leads to redundant reasoning and suboptimal outcomes. To address these challenges, we propose SE-Agent, a Self-Evolution framework that enables Agents to optimize their reasoning processes iteratively. Our approach revisits and enhances former pilot trajectories through three key operations: revision, recombination, and refinement. This evolutionary mechanism enables two critical advantages: (1) it expands the search space beyond local optima by intelligently exploring diverse solution paths guided by previous trajectories, and (2) it leverages cross-trajectory inspiration to efficiently enhance performance while mitigating the impact of suboptimal reasoning paths. Through these mechanisms, SE-Agent achieves continuous self-evolution that incrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench Verified to resolve real-world GitHub issues. Experimental results across five strong LLMs show that integrating SE-Agent delivers up to 55% relative improvement, achieving state-of-the-art performance among all open-source agents on SWE-bench Verified. Our code and demonstration materials are publicly available at <a target="_blank" rel="noopener" href="https://github.com/JARVIS-Xs/SE-Agent">https://github.com/JARVIS-Xs/SE-Agent</a>. </p>
<blockquote>
<p>基于大规模语言模型（LLM）的代理最近表现出令人印象深刻的复杂推理和工具使用能力，这是通过与环境的多步骤交互实现的。虽然这些代理有潜力处理复杂任务，但他们的解决问题过程，即代理完成任务的交互轨迹，仍然被低估。这些轨迹包含丰富的反馈，可以为代理提供正确的方向来正确解决问题。尽管现有的方法，如蒙特卡洛树搜索（MCTS），可以有效地平衡探索和利用，但它们忽略了不同轨迹之间的相互依赖性，并且缺乏搜索空间的多样性，这导致冗余推理和次优结果。为了解决这些挑战，我们提出了SE-Agent，这是一个自我进化框架，使代理能够迭代优化他们的推理过程。我们的方法通过三种关键操作来重新访问和改进先前的轨迹：修订、重组和细化。这种进化机制带来了两个关键优势：（1）它通过智能地探索由先前轨迹引导的多样化解决方案路径，扩大了搜索空间，超越了局部最优；（2）它利用跨轨迹的灵感来有效地提高性能，同时减轻次优推理路径的影响。通过这些机制，SE-Agent实现了连续的自我进化，逐步提高了推理质量。我们在SWE-bench Verified上评估了SE-Agent，以解决现实世界中的GitHub问题。在五个强大的LLM上的实验结果表明，集成SE-Agent带来了高达55%的相对改进，在SWE-bench Verified上的开源代理中实现了最先进的性能。我们的代码和演示材料可在<a target="_blank" rel="noopener" href="https://github.com/JARVIS-Xs/SE-Agent">https://github.com/JARVIS-Xs/SE-Agent</a>公开获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02085v3">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>基于大型语言模型（LLM）的代理人在复杂推理和工具使用方面表现出强大的能力，能够通过多步骤与环境的交互完成任务。然而，他们的解题过程，即代理人的交互轨迹，仍未得到充分研究。这些轨迹包含丰富的反馈，可以引导代理人正确解决问题。虽然蒙特卡洛树搜索（MCTS）等方法可以有效地平衡探索和利用，但它们忽略了不同轨迹之间的依赖性，缺乏搜索空间的多样性，导致冗余推理和次优结果。为解决这些问题，我们提出了SE-Agent，一个自我进化框架，使代理人能够迭代优化其推理过程。我们的方法通过修订、重组和细化三个关键操作来重新访问和改进先前的轨迹。这种进化机制带来了两个关键优势：一是通过智能探索受先前轨迹指导的多样化解决方案路径，扩大搜索空间并超越局部最优；二是利用跨轨迹灵感来有效提高性能，同时减少次优推理路径的影响。通过这些机制，SE-Agent实现了连续的自我进化，逐步提高了推理质量。我们在SWE-bench Verified上评估了SE-Agent解决GitHub实际问题的能力。在五个强大的LLM上的实验结果表明，集成SE-Agent带来了高达55%的相对改进，在SWE-bench Verified上实现了优于所有开源代理的最先进性能。我们的代码和演示材料可在<a target="_blank" rel="noopener" href="https://github.com/JARVIS-Xs/SE-Agent%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/JARVIS-Xs/SE-Agent上公开访问。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>LLM代理在复杂任务中具有强大的能力，能够通过与环境的多步骤交互进行推理和工具使用。</li>
<li>代理人的解题过程（交互轨迹）尚未得到充分研究，但包含引导正确解决问题的丰富反馈。</li>
<li>现有方法如MCTS忽略了轨迹间的依赖性，缺乏搜索空间多样性，可能导致冗余推理和次优结果。</li>
<li>SE-Agent通过自我进化框架优化推理过程，通过修订、重组和细化前人的轨迹来实现连续自我进化。</li>
<li>SE-Agent扩大了搜索空间并超越局部最优，利用跨轨迹灵感提高性能并减少次优推理路径的影响。</li>
<li>在SWE-bench Verified上的实验表明，集成SE-Agent的LLM代理性能显著提高，达到最新水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02085">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-54b48f09ac4403bd42b74224c37da297.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc136467beb09e19fc1f86d24cbca8bd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="RoboMemory-A-Brain-inspired-Multi-memory-Agentic-Framework-for-Lifelong-Learning-in-Physical-Embodied-Systems"><a href="#RoboMemory-A-Brain-inspired-Multi-memory-Agentic-Framework-for-Lifelong-Learning-in-Physical-Embodied-Systems" class="headerlink" title="RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong   Learning in Physical Embodied Systems"></a>RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong   Learning in Physical Embodied Systems</h2><p><strong>Authors:Mingcong Lei, Honghao Cai, Binbin Que, Zezhou Cui, Liangchen Tan, Junkun Hong, Gehan Hu, Shuangyu Zhu, Yimou Wu, Shaohan Jiang, Ge Wang, Zhen Li, Shuguang Cui, Yiming Zhao, Yatong Han</strong></p>
<p>We present RoboMemory, a brain-inspired multi-memory framework for lifelong learning in physical embodied systems, addressing critical challenges in real-world environments: continuous learning, multi-module memory latency, task correlation capture, and infinite-loop mitigation in closed-loop planning. Grounded in cognitive neuroscience, it integrates four core modules: the Information Preprocessor (thalamus-like), the Lifelong Embodied Memory System (hippocampus-like), the Closed-Loop Planning Module (prefrontal lobe-like), and the Low-Level Executer (cerebellum-like) to enable long-term planning and cumulative learning. The Lifelong Embodied Memory System, central to the framework, alleviates inference speed issues in complex memory frameworks via parallelized updates&#x2F;retrieval across Spatial, Temporal, Episodic, and Semantic submodules. It incorporates a dynamic Knowledge Graph (KG) and consistent architectural design to enhance memory consistency and scalability. Evaluations on EmbodiedBench show RoboMemory outperforms the open-source baseline (Qwen2.5-VL-72B-Ins) by 25% in average success rate and surpasses the closed-source State-of-the-Art (SOTA) (Claude3.5-Sonnet) by 5%, establishing new SOTA. Ablation studies validate key components (critic, spatial memory, long-term memory), while real-world deployment confirms its lifelong learning capability with significantly improved success rates across repeated tasks. RoboMemory alleviates high latency challenges with scalability, serving as a foundational reference for integrating multi-modal memory systems in physical robots. </p>
<blockquote>
<p>我们提出RoboMemory，这是一个受大脑启发的用于物理实体系统的终身学习多记忆框架，解决了真实世界环境中的关键挑战：持续学习、多模块记忆延迟、任务相关性捕获以及在闭环规划中的无限循环缓解。它基于认知神经科学，集成了四个核心模块：信息预处理器（类似丘脑）、终身体验记忆系统（类似海马体）、闭环规划模块（类似前额叶）和低级执行器（类似小脑），以实现长期规划和累积学习。终身体验记忆系统是框架的核心，它通过空间、时间、情节和语义子模块的并行更新&#x2F;检索，缓解复杂记忆框架中的推理速度问题。它结合了动态知识图谱（KG）和一致性的架构设计，以增强记忆的一致性和可扩展性。在EmbodiedBench上的评估表明，RoboMemory平均成功率比开源基准（Qwen2.5-VL-72B-Ins）高出25%，并超越了闭源当前最佳技术（SOTA）（Claude3.5-Sonnet）5%，创造了新的SOTA。消融研究验证了关键组件（批评家、空间记忆、长期记忆），而实际部署证实了其终身学习能力，在重复任务中的成功率显著提高。RoboMemory通过可扩展性缓解了高延迟挑战，为在物理机器人中集成多模式记忆系统提供了基础参考。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01415v2">PDF</a> </p>
<p><strong>Summary</strong><br>RoboMemory是一个受大脑启发的多记忆框架，用于实体系统中的终身学习，并解决了真实环境中的关键挑战。它通过整合四个核心模块：信息处理器、终身实体记忆系统、闭环规划模块和低级别执行器，实现长期规划和累积学习。终身实体记忆系统是该框架的核心，通过并行更新和检索空间、时间、情景和语义子模块，缓解复杂记忆框架的推理速度问题。评估结果显示，RoboMemory在EmbodiedBench上的表现优于开源基准和闭源最新技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RoboMemory是一个用于实体系统终身学习的多记忆框架，基于认知神经科学。</li>
<li>它解决了连续学习、多模块记忆延迟、任务关联捕获和闭环规划中的无限循环缓解等真实环境挑战。</li>
<li>终身实体记忆系统是核心，通过并行更新和检索多个子模块来提高效率。</li>
<li>RoboMemory采用动态知识图和一致的设计架构，增强记忆的一致性和可扩展性。</li>
<li>与开源基准和闭源最新技术相比，RoboMemory在EmbodiedBench上的表现更优。</li>
<li>消融研究验证了关键组件的重要性，包括批评家、空间记忆和长期记忆。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01415">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-46798e71d137d07f64e7533f773c4bfa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7af23af3d693a8d7141f9b94a6387a54.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="NatureGAIA-Pushing-the-Frontiers-of-GUI-Agents-with-a-Challenging-Benchmark-and-High-Quality-Trajectory-Dataset"><a href="#NatureGAIA-Pushing-the-Frontiers-of-GUI-Agents-with-a-Challenging-Benchmark-and-High-Quality-Trajectory-Dataset" class="headerlink" title="NatureGAIA: Pushing the Frontiers of GUI Agents with a Challenging   Benchmark and High-Quality Trajectory Dataset"></a>NatureGAIA: Pushing the Frontiers of GUI Agents with a Challenging   Benchmark and High-Quality Trajectory Dataset</h2><p><strong>Authors:Zihan Zheng, Tianle Cui, Chuwen Xie, Jiahui Zhang, Jiahui Pan, Lewei He, Qianglong Chen</strong></p>
<p>The rapid advancement of Large Language Model (LLM)-driven Graphical User Interface (GUI) agents is significantly hampered by the profound limitations of existing evaluation benchmarks in terms of accuracy, reproducibility, and scalability. To address this critical gap, we introduce NaturalGAIA, a novel benchmark engineered on the principle of Causal Pathways. This design paradigm structures complex tasks into a series of programmatically verifiable atomic steps, ensuring a rigorous, fully automated, and reproducible standard for assessment. Concurrently, to mitigate the inherent capability deficits of agents, we developed LightManus, a hierarchical agent architecture specifically optimized for long-horizon tasks. We leveraged this agent to generate a high-quality, human-verified trajectory dataset that uniquely captures diverse and even self-correcting interaction patterns of LLMs. We then utilized this dataset to perform Reinforcement Fine-Tuning (RFT) on the Qwen2.5-VL-7B model. Our experiments reveal that NaturalGAIA presents a formidable challenge to current state-of-the-art LLMs; even the top-performing Claude-sonnet-4 achieved a Weighted Pathway Success Rate (WPSR) of only 34.6%. Moreover, while RFT substantially improved the smaller model’s GUI execution capabilities (WPSR increased from 3.3% to 10.8%), its performance degraded sharply when handling complex scenarios. This outcome highlights the inherent capability ceiling of smaller models when faced with comprehensive tasks that integrate perception, decision-making, and execution. This research contributes a rigorous evaluation standard and a high-quality dataset to the community, aiming to guide the future development of GUI agents. </p>
<blockquote>
<p>大型语言模型（LLM）驱动的图形用户界面（GUI）代理的快速进步受到了现有评估基准在准确性、可重复性和可扩展性方面的深刻限制的严重阻碍。为了解决这一关键差距，我们引入了NaturalGAIA，这是一个基于因果路径原理的新型基准。这种设计范式将复杂任务结构化为一系列可程序验证的原子步骤，确保评估的严格性、完全自动化和可重复性。同时，为了缓解代理固有的能力缺陷，我们开发了一种专为长期任务优化的分层代理架构LightManus。我们利用该代理生成了高质量、经人类验证的轨迹数据集，该数据集独特地捕捉了LLM多样甚至自我校正的交互模式。然后，我们使用该数据集对Qwen2.5-VL-7B模型执行强化微调（RFT）。我们的实验表明，NaturalGAIA对当前最先进的LLM提出了巨大的挑战；即使是表现最佳的Claude-sonnet-4，加权路径成功率（WPSR）也只有34.6%。此外，虽然RFT显著提高了较小模型的GUI执行能力（WPSR从3.3%提高到10.8%），但在处理复杂场景时，其性能急剧下降。这一结果突出了面对整合感知、决策和执行的综合任务时，小型模型固有的能力上限。本研究为社区提供了一个严格的评估标准和高质量的数据集，旨在指导未来GUI代理的开发。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01330v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型驱动的图形用户界面代理的发展受到现有评估基准的严重限制，如准确性、可重复性和可扩展性。为解决这一关键差距，我们推出了基于因果路径原理的新型基准测试NaturalGAIA。同时，为缓解代理的内在能力缺陷，我们开发了针对长期任务的层次型代理架构LightManus。利用该代理生成了高质量、经人工验证的轨迹数据集，独特地捕捉了大型语言模型的多样化和自我纠正交互模式。然而，实验结果揭示，即使是顶尖的表演艺术家Claude-sonnet-4在NaturalGAIA上的加权路径成功率也只有34.6%，突显出现有模型在集成感知、决策和执行的综合任务中的能力天花板。本研究为社区提供了严格的评估标准和高质量数据集，旨在指导未来图形用户界面代理的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有评估基准限制大型语言模型驱动GUI代理的发展。</li>
<li>NaturalGAIA基准测试基于因果路径设计，确保评估的严格性、自动化和可重复性。</li>
<li>LightManus代理架构针对长期任务进行优化，生成了高质量、经人工验证的轨迹数据集。</li>
<li>RFT（强化微调）能提高较小模型的GUI执行能力，但在处理复杂场景时性能急剧下降。</li>
<li>顶尖模型在NaturalGAIA上的表现显示出现有能力天花板，加权路径成功率仅为34.6%。</li>
<li>研究为社区提供了严格的评估标准和高质量数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01330">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a36d3f4ad6db401b39fb012be0e6cefd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83c931924160cf7c37d4e550119b1b1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-883146e6f65bdf11f377839e19a83490.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SciReplicate-Bench-Benchmarking-LLMs-in-Agent-driven-Algorithmic-Reproduction-from-Research-Papers"><a href="#SciReplicate-Bench-Benchmarking-LLMs-in-Agent-driven-Algorithmic-Reproduction-from-Research-Papers" class="headerlink" title="SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic   Reproduction from Research Papers"></a>SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic   Reproduction from Research Papers</h2><p><strong>Authors:Yanzheng Xiang, Hanqi Yan, Shuyin Ouyang, Lin Gui, Yulan He</strong></p>
<p>This study evaluates large language models (LLMs) in generating code from algorithm descriptions in recent NLP papers. The task requires two key competencies: (1) algorithm comprehension: synthesizing information from papers and academic literature to understand implementation logic, and (2) coding expertise: identifying dependencies and correctly implementing necessary APIs. To facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark of 100 tasks from 36 NLP papers published in 2024, featuring detailed annotations and comprehensive test cases. Building on SciReplicate-Bench, we propose Sci-Reproducer, a dual-agent framework consisting of a Paper Agent that interprets algorithmic concepts from literature and a Code Agent that retrieves dependencies from repositories and implements solutions. To assess algorithm understanding, we introduce reasoning graph accuracy, which quantifies similarity between generated and reference reasoning graphs derived from code comments and structure. For evaluating implementation quality, we employ execution accuracy, CodeBLEU, and repository dependency&#x2F;API recall metrics. In our experiments, we evaluate various powerful non-reasoning and reasoning LLMs as foundational models. The best-performing LLM using \ModelName~achieves only 39% execution accuracy, highlighting the benchmark’s difficulty. Our analysis identifies missing or inconsistent algorithm descriptions as key barriers to successful reproduction. We make available our benchmark and code at <a target="_blank" rel="noopener" href="https://github.com/xyzCS/SciReplicate-Bench">https://github.com/xyzCS/SciReplicate-Bench</a> and project homepage at <a target="_blank" rel="noopener" href="https://xyzcs.github.io/scireplicate.github.io/">https://xyzcs.github.io/scireplicate.github.io/</a>. </p>
<blockquote>
<p>本研究评估了大型语言模型（LLM）在根据最新NLP论文中的算法描述生成代码的能力。这项任务需要两个关键技能：（1）算法理解：从论文和学术文献中综合信息以理解实现逻辑；（2）编码专业知识：识别依赖关系并正确实现必要的API。为了进行严格的评估，我们推出了SciReplicate-Bench，这是一个由2024年发表的36篇NLP论文中的100个任务组成的基准测试，具有详细的注释和全面的测试用例。基于SciReplicate-Bench，我们提出了Sci-Reproducer，这是一个由Paper Agent和Code Agent组成的双代理框架，其中Paper Agent负责解释文献中的算法概念，而Code Agent负责从存储库中检索依赖关系并实现解决方案。为了评估算法理解，我们引入了推理图准确性，它量化了从代码注释和结构派生的生成推理图与参考推理图之间的相似性。为了评估实现质量，我们采用了执行准确性、CodeBLEU和存储库依赖&#x2F;API召回率指标。在我们的实验中，我们评估了各种强大的非推理和推理LLM作为基础模型。\ModelName表现最好的LLM仅达到39%的执行准确性，这突显了本基准测试的困难程度。我们的分析发现，缺失或不一致的算法描述是成功复制的主要障碍。我们的基准测试和代码可在<a target="_blank" rel="noopener" href="https://github.com/xyzCS/SciReplicate-Bench%E4%B8%8A%E8%8E%B7%E5%BE%97%EF%BC%8C%E9%A1%B9%E7%9B%AE%E4%B8%BB%E9%A1%B5%E4%B8%BAhttps://xyzcs.github.io/scireplicate.github.io/%E3%80%82">https://github.com/xyzCS/SciReplicate-Bench上获得，项目主页为https://xyzcs.github.io/scireplicate.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.00255v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了大型语言模型（LLMs）在根据近期NLP论文中的算法描述生成代码的能力。研究内容包括算法理解和编码专长两个关键技能。为严格评估，推出了SciReplicate-Bench，包含来自36篇NLP论文的100个任务，并提供了详细的注解和全面的测试用例。同时提出了Sci-Reproducer双代理框架，包括解读文献算法的Paper Agent和从仓库检索依赖并实现解决方案的Code Agent。通过推理图准确度评估算法理解，通过执行准确度、CodeBLEU以及仓库依赖&#x2F;API召回率评估实现质量。实验显示，即使是最优秀的大型语言模型，使用ModelName也只能达到39%的执行准确度，凸显了此基准测试的困难性。文章指出了算法描述的缺失或不一致是阻碍成功复现的关键因素。数据和代码已公开提供。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究评估了大型语言模型在根据NLP论文的算法描述生成代码的能力。</li>
<li>引入SciReplicate-Bench，包含来自多个NLP论文的基准测试任务，并提供详细注解和测试用例。</li>
<li>提出Sci-Reproducer双代理框架，包括Paper Agent和Code Agent，分别负责解读算法和实现代码。</li>
<li>通过多项指标评估算法理解和代码实现质量，包括推理图准确度、执行准确度、CodeBLEU以及仓库依赖&#x2F;API召回率。</li>
<li>实验显示，现有大型语言模型的性能在复现算法方面仍有限，最佳模型执行准确度仅为39%。</li>
<li>算法描述的缺失或不一致是阻碍成功复现的关键因素。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.00255">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-418037b50dd90d2816dc080497441c3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e410978de51867cd150f7d3627453c8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8a572f9fb0cc02fccde0bd660398cac5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-348e4e7f2e6f8fb76e5d7e5b10d42240.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ST-WebAgentBench-A-Benchmark-for-Evaluating-Safety-and-Trustworthiness-in-Web-Agents"><a href="#ST-WebAgentBench-A-Benchmark-for-Evaluating-Safety-and-Trustworthiness-in-Web-Agents" class="headerlink" title="ST-WebAgentBench: A Benchmark for Evaluating Safety and Trustworthiness   in Web Agents"></a>ST-WebAgentBench: A Benchmark for Evaluating Safety and Trustworthiness   in Web Agents</h2><p><strong>Authors:Ido Levy, Ben Wiesel, Sami Marreed, Alon Oved, Avi Yaeli, Segev Shlomov</strong></p>
<p>Autonomous web agents solve complex browsing tasks, yet existing benchmarks measure only whether an agent finishes a task, ignoring whether it does so safely or in a way enterprises can trust. To integrate these agents into critical workflows, safety and trustworthiness (ST) are prerequisite conditions for adoption. We introduce \textbf{\textsc{ST-WebAgentBench}}, a configurable and easily extensible suite for evaluating web agent ST across realistic enterprise scenarios. Each of its 222 tasks is paired with ST policies, concise rules that encode constraints, and is scored along six orthogonal dimensions (e.g., user consent, robustness). Beyond raw task success, we propose the \textit{Completion Under Policy} (\textit{CuP}) metric, which credits only completions that respect all applicable policies, and the \textit{Risk Ratio}, which quantifies ST breaches across dimensions. Evaluating three open state-of-the-art agents reveals that their average CuP is less than two-thirds of their nominal completion rate, exposing critical safety gaps. By releasing code, evaluation templates, and a policy-authoring interface, \href{<a target="_blank" rel="noopener" href="https://sites.google.com/view/st-webagentbench/home%7D%7B/textsc%7BST-WebAgentBench%7D%7D">https://sites.google.com/view/st-webagentbench/home}{\textsc{ST-WebAgentBench}}</a> provides an actionable first step toward deploying trustworthy web agents at scale. </p>
<blockquote>
<p>自主网络代理能够解决复杂的浏览任务，但现有的基准测试仅仅衡量代理是否完成了任务，而忽略了它是否安全完成，或是否以企业可以信任的方式完成。为了将这些代理集成到关键工作流程中，安全和可信（ST）是采纳的前提条件。我们引入了<strong>ST-WebAgentBench</strong>，这是一套可在现实企业场景下评估网络代理ST的可配置和易于扩展的方案。其222项任务都配备了ST策略，这些策略是简洁的规则，编码了约束，并沿着六个正交维度（例如用户同意、稳健性）进行评分。除了原始任务成功之外，我们提出了“政策完成度”（CuP）指标，该指标只认可尊重所有适用政策的完成度，以及“风险比率”，该比率量化了在各个维度上的ST违规行为。对三个开源的先进代理进行评估显示，他们的平均CuP低于其名义完成率的三分之二，暴露了关键的安全差距。通过发布代码、评估模板和策略编写界面，<a target="_blank" rel="noopener" href="https://sites.google.com/view/st-webagentbench/home">ST-WebAgentBench</a>为大规模部署可信赖的网络代理提供了切实可行的第一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.06703v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了现有的网络代理基准测试仅关注代理是否完成任务，而忽略其安全性和可信度的问题。为了解决这个问题，作者提出了一个可配置和易于扩展的评估套件ST-WebAgentBench，用于评估网络代理在真实企业场景中的安全性和可信度。该套件包括222个任务，每个任务都配备有安全性和可信度的策略，并沿六个正交维度进行评分。作者还提出了两个新的评估指标：完成政策下的完成率（CuP）和风险比率，以量化安全性和可信度的违反情况。评估三个最先进的开放网络代理显示，它们的平均CuP低于其名义完成率的三分之二，暴露出关键的安全漏洞。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有的网络代理基准测试主要关注任务完成率，忽略了安全性和可信度。</li>
<li>ST-WebAgentBench是一个用于评估网络代理安全性和可信度的评估套件。</li>
<li>该套件包括222个任务，每个任务都配备有安全性和可信度的策略。</li>
<li>ST-WebAgentBench沿六个正交维度（例如用户同意和稳健性）对任务进行评分。</li>
<li>提出了完成政策下的完成率（CuP）和风险比率两个新评估指标。</li>
<li>对三个最先进的网络代理的评估显示，它们的任务完成率中存在关键的安全漏洞。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.06703">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2693395174820e6a8e32f1f71fd5173f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bbc2a49ff141044297626eac7a34903.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e47531e38079dad92877a167aabbdf57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dccc79692e3d905fc6943ce54fbbfb63.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4a1112d792badfedfc948aec22fe66a0.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-08-09  MELLA Bridging Linguistic Capability and Cultural Groundedness for   Low-Resource Language MLLMs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-25a56c6761c0480b91ef3be33a13768d.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-08-09  On the Generalization of SFT A Reinforcement Learning Perspective with   Reward Rectification
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28791.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
