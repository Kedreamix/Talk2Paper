<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-09  Speech LLMs in Low-Resource Scenarios Data Volume Requirements and the   Impact of Pretraining on High-Resource Languages">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e23e6ccc7af7fd30bcf432a8645d921e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    6.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    27 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-09-æ›´æ–°"><a href="#2025-08-09-æ›´æ–°" class="headerlink" title="2025-08-09 æ›´æ–°"></a>2025-08-09 æ›´æ–°</h1><h2 id="Speech-LLMs-in-Low-Resource-Scenarios-Data-Volume-Requirements-and-the-Impact-of-Pretraining-on-High-Resource-Languages"><a href="#Speech-LLMs-in-Low-Resource-Scenarios-Data-Volume-Requirements-and-the-Impact-of-Pretraining-on-High-Resource-Languages" class="headerlink" title="Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the   Impact of Pretraining on High-Resource Languages"></a>Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the   Impact of Pretraining on High-Resource Languages</h2><p><strong>Authors:Seraphina Fong, Marco Matassoni, Alessio Brutti</strong></p>
<p>Large language models (LLMs) have demonstrated potential in handling spoken inputs for high-resource languages, reaching state-of-the-art performance in various tasks. However, their applicability is still less explored in low-resource settings. This work investigates the use of Speech LLMs for low-resource Automatic Speech Recognition using the SLAM-ASR framework, where a trainable lightweight projector connects a speech encoder and a LLM. Firstly, we assess training data volume requirements to match Whisper-only performance, re-emphasizing the challenges of limited data. Secondly, we show that leveraging mono- or multilingual projectors pretrained on high-resource languages reduces the impact of data scarcity, especially with small training sets. Using multilingual LLMs (EuroLLM, Salamandra) with whisper-large-v3-turbo, we evaluate performance on several public benchmarks, providing insights for future research on optimizing Speech LLMs for low-resource languages and multilinguality. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é«˜èµ„æºè¯­è¨€çš„å£è¯­è¾“å…¥æ–¹é¢å·²æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ä½èµ„æºç¯å¢ƒä¸­çš„é€‚ç”¨æ€§ä»ç ”ç©¶è¾ƒå°‘ã€‚æœ¬ç ”ç©¶åˆ©ç”¨SLAM-ASRæ¡†æ¶æ¢ç´¢äº†è¯­éŸ³LLMåœ¨èµ„æºä½ä¸‹çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­çš„åº”ç”¨ï¼Œå…¶ä¸­å¯è®­ç»ƒçš„è½»é‡çº§æŠ•å½±ä»ªè¿æ¥äº†è¯­éŸ³ç¼–ç å™¨LLMã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯„ä¼°äº†è¾¾åˆ°ä»…whisperæ€§èƒ½æ‰€éœ€çš„è®­ç»ƒæ•°æ®é‡ï¼Œå†æ¬¡å¼ºè°ƒæœ‰é™æ•°æ®çš„æŒ‘æˆ˜ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è¡¨æ˜åˆ©ç”¨åœ¨é«˜èµ„æºè¯­è¨€ä¸Šé¢„è®­ç»ƒçš„å•è¯­æˆ–å¤šè¯­æŠ•å½±ä»ªå¯ä»¥å‡å°‘æ•°æ®ç¨€ç¼ºçš„å½±å“ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨å°å‹è®­ç»ƒé›†æ—¶ã€‚ä½¿ç”¨å¤šè¯­è¨€LLMï¼ˆEuroLLMã€Salamandraï¼‰ä¸whisper-large-v3-turboï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªå…¬å…±åŸºå‡†æµ‹è¯•é›†ä¸Šè¯„ä¼°æ€§èƒ½ï¼Œä¸ºæœªæ¥ä¼˜åŒ–ç”¨äºä½èµ„æºè¯­è¨€å’Œå¤šç§è¯­è¨€çš„è¯­éŸ³LLMçš„ç ”ç©¶æä¾›è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05149v1">PDF</a> Accepted at Interspeech 2025. 5 pages, 2 figures, 3 tables</p>
<p><strong>æ€»ç»“</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†é«˜èµ„æºè¯­è¨€çš„å£è¯­è¾“å…¥æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œå¹¶åœ¨å„ç§ä»»åŠ¡ä¸­è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ä½èµ„æºç¯å¢ƒä¸­çš„é€‚ç”¨æ€§ä»ç„¶æœ‰å¾…æ¢ç´¢ã€‚æœ¬ç ”ç©¶ä½¿ç”¨SLAM-ASRæ¡†æ¶æ¢è®¨äº†ä½èµ„æºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä¸­ä½¿ç”¨è¯­éŸ³LLMsçš„é—®é¢˜ã€‚ä¸€ä¸ªå¯è®­ç»ƒçš„è½»é‡çº§æŠ•å½±ä»ªè¿æ¥è¯­éŸ³ç¼–ç å™¨ä¸LLMã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯„ä¼°äº†è¾¾åˆ°whisper-onlyæ€§èƒ½æ‰€éœ€çš„è®­ç»ƒæ•°æ®é‡ï¼Œå†æ¬¡å¼ºè°ƒäº†æ•°æ®æœ‰é™çš„æŒ‘æˆ˜ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åˆ©ç”¨å•è¯­ç§æˆ–å¤šè¯­ç§é¢„è®­ç»ƒåœ¨é«˜èµ„æºè¯­è¨€ä¸Šçš„æŠ•å½±ä»ªèƒ½å¤Ÿå‡å°‘æ•°æ®ç¨€ç¼ºçš„å½±å“ï¼Œç‰¹åˆ«æ˜¯å°è®­ç»ƒé›†æƒ…å†µä¸‹æ›´ä¸ºæ˜æ˜¾ã€‚é€šè¿‡ä½¿ç”¨å¤šè¯­ç§LLMï¼ˆEuroLLMã€Salamandraï¼‰ä¸whisper-large-v3-turboï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°æ€§èƒ½ï¼Œä¸ºæœªæ¥ä¼˜åŒ–è¯­éŸ³LLMsåœ¨ä½èµ„æºè¯­è¨€å’Œå¤šè¯­ç§æ–¹é¢çš„åº”ç”¨æä¾›è§è§£ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨é«˜èµ„æºè¯­è¨€çš„å£è¯­å¤„ç†æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨ä½èµ„æºç¯å¢ƒä¸­å…¶é€‚ç”¨æ€§æœ‰å¾…è¿›ä¸€æ­¥æ¢ç´¢ã€‚</li>
<li>ä½¿ç”¨SLAM-ASRæ¡†æ¶ç ”ç©¶è¯­éŸ³LLMsåœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>è®­ç»ƒæ•°æ®é‡å¯¹è¾¾åˆ°whisper-onlyæ€§èƒ½è‡³å…³é‡è¦ï¼Œçªæ˜¾æ•°æ®æœ‰é™çš„æŒ‘æˆ˜ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„å•è¯­ç§æˆ–å¤šè¯­ç§æŠ•å½±ä»ªåœ¨é«˜èµ„æºè¯­è¨€ä¸Šå¯ä»¥å‡å°‘æ•°æ®ç¨€ç¼ºçš„å½±å“ã€‚</li>
<li>å¤šè¯­ç§LLMsåœ¨è¯­éŸ³è¯†åˆ«æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å°è®­ç»ƒé›†æ—¶æ•ˆæœæ›´ä¸ºæ˜¾è‘—ã€‚</li>
<li>åœ¨å¤šä¸ªå…¬å…±åŸºå‡†æµ‹è¯•ä¸Šå¯¹æ€§èƒ½è¿›è¡Œè¯„ä¼°ï¼Œä¸ºåç»­ä¼˜åŒ–æä¾›å‚ç…§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05149">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a0351ecb2da02e764bfd9761ba4842ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40f92a068013684fccbb4eca14c42705.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ba26849196296b4abf7f9b3278835bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4b8923254d048c9a21ebabcda1c7e5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fae0b91957e59611f22c5a79d299045d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MOVER-Combining-Multiple-Meeting-Recognition-Systems"><a href="#MOVER-Combining-Multiple-Meeting-Recognition-Systems" class="headerlink" title="MOVER: Combining Multiple Meeting Recognition Systems"></a>MOVER: Combining Multiple Meeting Recognition Systems</h2><p><strong>Authors:Naoyuki Kamo, Tsubasa Ochiai, Marc Delcroix, Tomohiro Nakatani</strong></p>
<p>In this paper, we propose Meeting recognizer Output Voting Error Reduction (MOVER), a novel system combination method for meeting recognition tasks. Although there are methods to combine the output of diarization (e.g., DOVER) or automatic speech recognition (ASR) systems (e.g., ROVER), MOVER is the first approach that can combine the outputs of meeting recognition systems that differ in terms of both diarization and ASR. MOVER combines hypotheses with different time intervals and speaker labels through a five-stage process that includes speaker alignment, segment grouping, word and timing combination, etc. Experimental results on the CHiME-8 DASR task and the multi-channel track of the NOTSOFAR-1 task demonstrate that MOVER can successfully combine multiple meeting recognition systems with diverse diarization and recognition outputs, achieving relative tcpWER improvements of 9.55 % and 8.51 % over the state-of-the-art systems for both tasks. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¼šè®®è¯†åˆ«è¾“å‡ºæŠ•ç¥¨è¯¯å·®å‡å°‘ï¼ˆMOVERï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºä¼šè®®è¯†åˆ«ä»»åŠ¡çš„æ–°å‹ç³»ç»Ÿç»„åˆæ–¹æ³•ã€‚å°½ç®¡æœ‰æ–¹æ³•å¯ä»¥å°†æ—¥è®°åŒ–ï¼ˆä¾‹å¦‚DOVERï¼‰æˆ–è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„è¾“å‡ºè¿›è¡Œç»„åˆï¼ˆä¾‹å¦‚ROVERï¼‰ï¼Œä½†MOVERæ˜¯é¦–ä¸ªèƒ½å¤Ÿç»“åˆåœ¨æ—¥è®°åŒ–å’ŒASRæ–¹é¢æœ‰æ‰€ä¸åŒçš„ä¼šè®®è¯†åˆ«ç³»ç»Ÿè¾“å‡ºçš„æ–¹æ³•ã€‚MOVERé€šè¿‡åŒ…æ‹¬è¯´è¯äººå¯¹é½ã€æ®µè½åˆ†ç»„ã€è¯æ±‡å’Œæ—¶åºç»„åˆç­‰äº”ä¸ªé˜¶æ®µçš„æµç¨‹ï¼Œå°†å…·æœ‰ä¸åŒæ—¶é—´é—´éš”å’Œè¯´è¯äººæ ‡ç­¾çš„å‡è®¾è¿›è¡Œç»“åˆã€‚åœ¨CHiME-8 DASRä»»åŠ¡å’ŒNOTSOFAR-1ä»»åŠ¡çš„å¤šé€šé“è½¨é“ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMOVERèƒ½å¤ŸæˆåŠŸåœ°å°†å¤šä¸ªä¼šè®®è¯†åˆ«ç³»ç»Ÿè¿›è¡Œç»„åˆï¼Œè¿™äº›ç³»ç»Ÿå…·æœ‰å¤šæ ·åŒ–çš„æ—¥è®°åŒ–å’Œè¯†åˆ«è¾“å‡ºï¼Œç›¸å¯¹äºå½“å‰å…ˆè¿›ç³»ç»Ÿï¼Œå®ç°äº†9.55%å’Œ8.51%çš„tcpWERæ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05055v1">PDF</a> </p>
<p><strong>Summary</strong><br>ä¼šè®®è¯†åˆ«è¾“å‡ºæŠ•ç¥¨è¯¯å·®å‡å°‘ï¼ˆMOVERï¼‰æ˜¯ä¸€ç§æ–°é¢–çš„ä¼šè®®è¯†åˆ«ä»»åŠ¡ç³»ç»Ÿç»„åˆæ–¹æ³•ã€‚å®ƒç»“åˆä¸åŒæ—¶é—´æ®µå’Œè¯´è¯äººæ ‡ç­¾çš„å‡è®¾ï¼Œé€šè¿‡äº”é˜¶æ®µè¿‡ç¨‹ï¼ŒåŒ…æ‹¬è¯´è¯äººå¯¹é½ã€åˆ†æ®µåˆ†ç»„ã€è¯è¯­å’Œæ—¶é—´ç»„åˆç­‰ã€‚åœ¨CHiME-8 DASRä»»åŠ¡å’ŒNOTSOFAR-1å¤šé€šé“ä»»åŠ¡çš„å®éªŒä¸­ï¼ŒMOVERæˆåŠŸç»“åˆäº†å¤šä¸ªä¼šè®®è¯†åˆ«ç³»ç»Ÿï¼Œå®ç°äº†ç›¸å¯¹tcpWERçš„æ”¹è¿›ï¼Œè¶…è¿‡äº†ç°æœ‰ç³»ç»Ÿçš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MOVERæ˜¯ä¸€ç§ç”¨äºä¼šè®®è¯†åˆ«ä»»åŠ¡çš„å…¨æ–°ç³»ç»Ÿç»„åˆæ–¹æ³•ã€‚</li>
<li>MOVERç»“åˆäº†ä¸åŒæ—¶é—´æ®µå’Œè¯´è¯äººæ ‡ç­¾çš„å‡è®¾ã€‚</li>
<li>MOVERåŒ…å«äº”é˜¶æ®µè¿‡ç¨‹ï¼šè¯´è¯äººå¯¹é½ã€åˆ†æ®µåˆ†ç»„ã€è¯è¯­å’Œæ—¶é—´ç»„åˆç­‰ã€‚</li>
<li>CHiME-8 DASRä»»åŠ¡å’ŒNOTSOFAR-1å¤šé€šé“ä»»åŠ¡çš„å®éªŒéªŒè¯äº†MOVERçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>MOVERèƒ½å¤ŸæˆåŠŸç»“åˆå¤šä¸ªä¼šè®®è¯†åˆ«ç³»ç»Ÿï¼Œè¿™äº›ç³»ç»Ÿå¯èƒ½åœ¨è¿ªäºšé‡ŒåŒ–å’Œè¯†åˆ«è¾“å‡ºæ–¹é¢æœ‰æ‰€ä¸åŒã€‚</li>
<li>ä¸ç°æœ‰ç³»ç»Ÿç›¸æ¯”ï¼ŒMOVERåœ¨ç›¸å¯¹tcpWERæ–¹é¢å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œåˆ†åˆ«ä¸º9.55%å’Œ8.51%ã€‚</li>
<li>MOVERæ–¹æ³•ä¸ºä¼šè®®è¯†åˆ«ä»»åŠ¡çš„ç³»ç»Ÿç»„åˆæä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5fa15e55a93bac7429182b66a9e8fe98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-048a56639da08ddd663304eb8663ec6f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56d461373d2f503f3e731143b8a150f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-800cb0884570c805dca312459e4517af.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="REF-VC-Robust-Expressive-and-Fast-Zero-Shot-Voice-Conversion-with-Diffusion-Transformers"><a href="#REF-VC-Robust-Expressive-and-Fast-Zero-Shot-Voice-Conversion-with-Diffusion-Transformers" class="headerlink" title="REF-VC: Robust, Expressive and Fast Zero-Shot Voice Conversion with   Diffusion Transformers"></a>REF-VC: Robust, Expressive and Fast Zero-Shot Voice Conversion with   Diffusion Transformers</h2><p><strong>Authors:Yuepeng Jiang, Ziqian Ning, Shuai Wang, Chengjia Wang, Mengxiao Bi, Pengcheng Zhu, Lei Xie, Zhonghua Fu</strong></p>
<p>In real-world voice conversion applications, environmental noise in source speech and user demands for expressive output pose critical challenges. Traditional ASR-based methods ensure noise robustness but suppress prosody, while SSL-based models improve expressiveness but suffer from timbre leakage and noise sensitivity. This paper proposes REF-VC, a noise-robust expressive voice conversion system. Key innovations include: (1) A random erasing strategy to mitigate the information redundancy inherent in SSL feature, enhancing noise robustness and expressiveness; (2) Implicit alignment inspired by E2TTS to suppress non-essential feature reconstruction; (3) Integration of Shortcut Models to accelerate flow matching inference, significantly reducing to 4 steps. Experimental results demonstrate that our model outperforms baselines such as Seed-VC in zero-shot scenarios on the noisy set, while also performing comparably to Seed-VC on the clean set. In addition, REF-VC can be compatible with singing voice conversion within one model. </p>
<blockquote>
<p>åœ¨çœŸå®ä¸–ç•Œçš„å£°éŸ³è½¬æ¢åº”ç”¨ä¸­ï¼Œæºè¯­éŸ³ä¸­çš„ç¯å¢ƒå™ªå£°ä»¥åŠç”¨æˆ·å¯¹è¡¨ç°åŠ›è¾“å‡ºçš„éœ€æ±‚æ„æˆäº†å…³é”®çš„æŒ‘æˆ˜ã€‚åŸºäºä¼ ç»ŸASRçš„æ–¹æ³•è™½ç„¶èƒ½ä¿è¯å™ªå£°ç¨³å¥æ€§ï¼Œä½†ä¼šæŠ‘åˆ¶éŸµå¾‹ï¼Œè€ŒåŸºäºSSLçš„æ¨¡å‹è™½ç„¶èƒ½æé«˜è¡¨ç°åŠ›ï¼Œä½†å´å­˜åœ¨éŸ³è‰²æ³„éœ²å’Œå™ªå£°æ•æ„Ÿçš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†REF-VCï¼Œä¸€ç§å™ªå£°é²æ£’æ€§å¼ºçš„è¯­éŸ³è½¬æ¢ç³»ç»Ÿã€‚ä¸»è¦åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é‡‡ç”¨éšæœºæ“¦é™¤ç­–ç•¥ï¼Œå‡è½»SSLç‰¹å¾ä¸­çš„ä¿¡æ¯å†—ä½™ï¼Œå¢å¼ºå™ªå£°ç¨³å¥æ€§å’Œè¡¨ç°åŠ›ï¼›ï¼ˆ2ï¼‰å—E2TTSå¯å‘çš„éšå¯¹é½æ–¹å¼ï¼ŒæŠ‘åˆ¶éå…³é”®ç‰¹å¾é‡å»ºï¼›ï¼ˆ3ï¼‰é›†æˆå¿«æ·æ¨¡å‹ï¼ŒåŠ é€ŸåŒ¹é…æµæ¨ç†ï¼Œå¤§å¤§ç¼©çŸ­è‡³4æ­¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å™ªå£°é›†ä¸Šçš„é›¶æ ·æœ¬åœºæ™¯ä¸­ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ˆå¦‚Seed-VCï¼‰ï¼ŒåŒæ—¶åœ¨æ¸…æ´é›†ä¸Šçš„è¡¨ç°ä¸Seed-VCç›¸å½“ã€‚æ­¤å¤–ï¼ŒREF-VCå¯ä»¥åœ¨ä¸€ä¸ªæ¨¡å‹å†…å…¼å®¹æ­Œå£°è½¬æ¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04996v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹çœŸå®ä¸–ç•Œè¯­éŸ³è½¬æ¢åº”ç”¨ï¼Œæºè¯­éŸ³ä¸­çš„ç¯å¢ƒå™ªå£°å’Œç”¨æˆ·å¯¹äºè¡¨è¾¾è¾“å‡ºçš„éœ€æ±‚æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚ä¼ ç»ŸåŸºäºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„æ–¹æ³•ç¡®ä¿äº†å™ªå£°é²æ£’æ€§ï¼Œä½†æŠ‘åˆ¶äº†éŸµå¾‹ï¼›è€ŒåŸºäºè‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰çš„æ¨¡å‹åˆ™æé«˜äº†è¡¨è¾¾æ€§ï¼Œä½†å­˜åœ¨éŸ³è‰²æ³„æ¼å’Œå™ªå£°æ•æ„Ÿæ€§ã€‚æœ¬æ–‡æå‡ºREF-VCï¼Œä¸€ç§å™ªå£°é²æ£’æ€§å¼ºçš„è¡¨è¾¾æ€§è¯­éŸ³è½¬æ¢ç³»ç»Ÿã€‚ä¸»è¦åˆ›æ–°åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é‡‡ç”¨éšæœºæ“¦é™¤ç­–ç•¥ï¼Œå‡è½»SSLç‰¹å¾ä¸­çš„ä¿¡æ¯å†—ä½™ï¼Œå¢å¼ºå™ªå£°é²æ£’æ€§å’Œè¡¨è¾¾æ€§ï¼›ï¼ˆ2ï¼‰å€Ÿé‰´E2TTSçš„éšå¯¹é½æ–¹å¼ï¼ŒæŠ‘åˆ¶éå¿…è¦ç‰¹å¾é‡å»ºï¼›ï¼ˆ3ï¼‰é›†æˆShortcut ModelsåŠ é€ŸæµåŒ¹é…æ¨ç†ï¼Œå‡å°‘è‡³4æ­¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å™ªå£°é›†ä¸Šçš„é›¶æ ·æœ¬åœºæ™¯ä¸­ä¼˜äºåŸºçº¿æ–¹æ³•å¦‚Seed-VCï¼ŒåŒæ—¶åœ¨æ¸…æ´é›†ä¸Šçš„è¡¨ç°ä¸Seed-VCç›¸å½“ã€‚æ­¤å¤–ï¼ŒREF-VCå¯ä»¥åœ¨ä¸€ä¸ªæ¨¡å‹å†…å…¼å®¹æ­Œå”±è¯­éŸ³è½¬æ¢ã€‚</p>
<p><strong>è¦ç‚¹è§£æ</strong></p>
<ol>
<li>çœŸå®ä¸–ç•Œè¯­éŸ³è½¬æ¢é¢ä¸´ç¯å¢ƒå™ªå£°å’Œè¡¨è¾¾æ€§éœ€æ±‚ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»ŸASRæ–¹æ³•è™½å™ªå£°é²æ£’ä½†æŠ‘åˆ¶éŸµå¾‹ï¼Œè€ŒSSLæ¨¡å‹è™½è¡¨è¾¾æ€§å¼ºå´å­˜åœ¨éŸ³è‰²æ³„æ¼å’Œå™ªå£°æ•æ„Ÿé—®é¢˜ã€‚</li>
<li>REF-VCç³»ç»Ÿæå‡ºéšæœºæ“¦é™¤ç­–ç•¥ï¼Œæ—¨åœ¨å¹³è¡¡å™ªå£°é²æ£’æ€§å’Œè¡¨è¾¾æ€§ã€‚</li>
<li>éšå¯¹é½æ–¹å¼å’Œéå¿…è¦ç‰¹å¾æŠ‘åˆ¶è¿›ä¸€æ­¥æå‡äº†ç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>é›†æˆShortcut ModelsåŠ é€Ÿæ¨ç†è¿‡ç¨‹ï¼Œå‡å°‘è®¡ç®—æ­¥éª¤ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºREF-VCåœ¨å™ªå£°ç¯å¢ƒä¸‹çš„æ€§èƒ½ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒåŒæ—¶åœ¨æ¸…æ´ç¯å¢ƒä¸‹è¡¨ç°ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04996">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8f877add70d978972ed80fe21b34dd82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21c65ed00bb726f0549712e7004e403b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c1b30faacd5d5eb868a42f428ac420c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-601fb1ab6420d2b83d941500cce376ce.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Pitch-Accent-Detection-improves-Pretrained-Automatic-Speech-Recognition"><a href="#Pitch-Accent-Detection-improves-Pretrained-Automatic-Speech-Recognition" class="headerlink" title="Pitch Accent Detection improves Pretrained Automatic Speech Recognition"></a>Pitch Accent Detection improves Pretrained Automatic Speech Recognition</h2><p><strong>Authors:David Sasu, Natalie Schluter</strong></p>
<p>We show the performance of Automatic Speech Recognition (ASR) systems that use semi-supervised speech representations can be boosted by a complimentary pitch accent detection module, by introducing a joint ASR and pitch accent detection model. The pitch accent detection component of our model achieves a significant improvement on the state-of-the-art for the task, closing the gap in F1-score by 41%. Additionally, the ASR performance in joint training decreases WER by 28.3% on LibriSpeech, under limited resource fine-tuning. With these results, we show the importance of extending pretrained speech models to retain or re-learn important prosodic cues such as pitch accent. </p>
<blockquote>
<p>æˆ‘ä»¬å±•ç¤ºäº†é€šè¿‡ä½¿ç”¨åŠç›‘ç£è¯­éŸ³è¡¨ç¤ºï¼Œç»“åˆä¸€ä¸ªè¾…åŠ©çš„éŸ³è°ƒé‡éŸ³æ£€æµ‹æ¨¡å—ï¼Œå¯ä»¥æé«˜è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥ä¸€ä¸ªè”åˆASRå’ŒéŸ³è°ƒé‡éŸ³æ£€æµ‹æ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹çš„éŸ³è°ƒé‡éŸ³æ£€æµ‹éƒ¨åˆ†åœ¨è¯¥ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œåœ¨F1åˆ†æ•°ä¸Šç¼©å°äº†41%çš„å·®è·ã€‚æ­¤å¤–ï¼Œåœ¨è”åˆè®­ç»ƒä¸­ï¼ŒASRçš„æ€§èƒ½åœ¨LibriSpeechä¸Šé€šè¿‡æœ‰é™çš„èµ„æºå¾®è°ƒé™ä½äº†28.3%çš„è¯é”™è¯¯ç‡ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œæ‰©å±•é¢„è®­ç»ƒçš„è¯­éŸ³æ¨¡å‹ä»¥ä¿ç•™æˆ–é‡æ–°å­¦ä¹ é‡è¦çš„éŸµå¾‹çº¿ç´¢ï¼ˆå¦‚éŸ³è°ƒé‡éŸ³ï¼‰çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04814v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä½¿ç”¨åŠç›‘ç£è¯­éŸ³è¡¨ç¤ºçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„æ€§èƒ½å¯ä»¥é€šè¿‡å¼•å…¥è”åˆASRå’ŒéŸ³è°ƒé‡éŸ³æ£€æµ‹æ¨¡å‹å¾—åˆ°æå‡ã€‚è¯¥æ¨¡å‹çš„éŸ³è°ƒé‡éŸ³æ£€æµ‹éƒ¨åˆ†åœ¨ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼ŒF1å¾—åˆ†æé«˜äº†41%ã€‚æ­¤å¤–ï¼Œåœ¨LibriSpeechä¸Šè¿›è¡Œè”åˆè®­ç»ƒåï¼ŒASRæ€§èƒ½åœ¨æœ‰é™èµ„æºå¾®è°ƒçš„æƒ…å†µä¸‹é™ä½äº†28.3%çš„è¯é”™è¯¯ç‡ã€‚è¿™è¯æ˜äº†æ‰©å±•é¢„è®­ç»ƒè¯­éŸ³æ¨¡å‹ä»¥ä¿ç•™æˆ–é‡æ–°å­¦ä¹ å¦‚éŸ³è°ƒé‡éŸ³ç­‰é‡è¦éŸµå¾‹ç‰¹å¾çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥è”åˆASRå’ŒéŸ³è°ƒé‡éŸ³æ£€æµ‹æ¨¡å‹å¯ä»¥æå‡ASRç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹çš„éŸ³è°ƒé‡éŸ³æ£€æµ‹éƒ¨åˆ†åœ¨ä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼ŒF1å¾—åˆ†æé«˜äº†41%ã€‚</li>
<li>åœ¨LibriSpeechæ•°æ®é›†ä¸Šï¼Œè”åˆè®­ç»ƒåASRæ€§èƒ½å¾—åˆ°æ˜¾è‘—æé«˜ï¼Œè¯é”™è¯¯ç‡é™ä½äº†28.3%ã€‚</li>
<li>åœ¨æœ‰é™èµ„æºå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹ä¾ç„¶è¡¨ç°å‡ºè¾ƒå¥½çš„æ€§èƒ½æå‡ã€‚</li>
<li>æ‰©å±•é¢„è®­ç»ƒè¯­éŸ³æ¨¡å‹æœ‰åŠ©äºä¿ç•™æˆ–é‡æ–°å­¦ä¹ é‡è¦çš„éŸµå¾‹ç‰¹å¾ã€‚</li>
<li>ä¿ç•™æˆ–é‡æ–°å­¦ä¹ å¦‚éŸ³è°ƒé‡éŸ³ç­‰éŸµå¾‹ç‰¹å¾å¯¹æå‡ASRç³»ç»Ÿæ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04814">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d67e26b15c7bade3b9bfaf13bb116cdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c67409e5718f7a9bf24f3ba36e654625.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f444e2b748597194cb49640b339d449.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AudioGen-Omni-A-Unified-Multimodal-Diffusion-Transformer-for-Video-Synchronized-Audio-Speech-and-Song-Generation"><a href="#AudioGen-Omni-A-Unified-Multimodal-Diffusion-Transformer-for-Video-Synchronized-Audio-Speech-and-Song-Generation" class="headerlink" title="AudioGen-Omni: A Unified Multimodal Diffusion Transformer for   Video-Synchronized Audio, Speech, and Song Generation"></a>AudioGen-Omni: A Unified Multimodal Diffusion Transformer for   Video-Synchronized Audio, Speech, and Song Generation</h2><p><strong>Authors:Le Wang, Jun Wang, Chunyu Qiang, Feng Deng, Chen Zhang, Di Zhang, Kun Gai</strong></p>
<p>We present AudioGen-Omni - a unified approach based on multimodal diffusion transformers (MMDit), capable of generating high-fidelity audio, speech, and song coherently synchronized with the input video. AudioGen-Omni introduces a novel joint training paradigm that seamlessly integrates large-scale video-text-audio corpora, enabling a model capable of generating semantically rich, acoustically diverse audio conditioned on multimodal inputs and adaptable to a wide range of audio generation tasks. AudioGen-Omni employs a unified lyrics-transcription encoder that encodes graphemes and phonemes from both song and spoken inputs into dense frame-level representations. Dense frame-level representations are fused using an AdaLN-based joint attention mechanism enhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein RoPE is selectively applied to temporally structured modalities to ensure precise and robust cross-modal alignment. By unfreezing all modalities and masking missing inputs, AudioGen-Omni mitigates the semantic constraints of text-frozen paradigms, enabling effective cross-modal conditioning. This joint training approach enhances audio quality, semantic alignment, and lip-sync accuracy, while also achieving state-of-the-art results on Text-to-Audio&#x2F;Speech&#x2F;Song tasks. With an inference time of 1.91 seconds for 8 seconds of audio, it offers substantial improvements in both efficiency and generality. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†AudioGen-Omniâ€”â€”ä¸€ç§åŸºäºå¤šæ¨¡å¼æ‰©æ•£å˜å‹å™¨ï¼ˆMMDitï¼‰çš„ç»Ÿä¸€æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸è¾“å…¥è§†é¢‘åŒæ­¥çš„é«˜ä¿çœŸéŸ³é¢‘ã€è¯­éŸ³å’Œæ­Œæ›²ã€‚AudioGen-Omniå¼•å…¥äº†ä¸€ç§æ–°çš„è”åˆè®­ç»ƒæ¨¡å¼ï¼Œè¯¥æ¨¡å¼æ— ç¼é›†æˆäº†å¤§è§„æ¨¡çš„è§†é¢‘-æ–‡æœ¬-éŸ³é¢‘è¯­æ–™åº“ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šæ¨¡å¼è¾“å…¥æ¡ä»¶ä¸‹ç”Ÿæˆè¯­ä¹‰ä¸°å¯Œã€å£°éŸ³å¤šæ ·çš„éŸ³é¢‘ï¼Œå¹¶é€‚åº”å¹¿æ³›çš„éŸ³é¢‘ç”Ÿæˆä»»åŠ¡ã€‚AudioGen-Omnié‡‡ç”¨ç»Ÿä¸€çš„æ­Œè¯-è½¬å½•ç¼–ç å™¨ï¼Œå°†æ­Œæ›²å’Œå£è¯­è¾“å…¥ä¸­çš„å­—æ¯å’ŒéŸ³ç´ ç¼–ç æˆå¯†é›†çš„å¸§çº§è¡¨ç¤ºã€‚å¯†é›†çš„å¸§çº§è¡¨ç¤ºé€šè¿‡ä½¿ç”¨åŸºäºAdaLNçš„è”åˆæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œèåˆï¼Œå¢å¼ºé˜¶æ®µå¯¹é½çš„å®šå‘ä½ç½®æ³¨å…¥ï¼ˆPAAPIï¼‰ï¼Œå…¶ä¸­RoPEè¢«é€‰æ‹©æ€§åº”ç”¨äºä¸´æ—¶ç»“æ„æ¨¡æ€ï¼Œä»¥ç¡®ä¿ç²¾ç¡®å’Œç¨³å¥çš„è·¨æ¨¡æ€å¯¹é½ã€‚é€šè¿‡è§£å†»æ‰€æœ‰æ¨¡æ€å¹¶å±è”½ç¼ºå¤±çš„è¾“å…¥ï¼ŒAudioGen-Omniå‡è½»äº†æ–‡æœ¬å†»ç»“æ¨¡å¼çš„è¯­ä¹‰çº¦æŸï¼Œå®ç°äº†æœ‰æ•ˆçš„è·¨æ¨¡æ€æ¡ä»¶è®¾ç½®ã€‚è¿™ç§è”åˆè®­ç»ƒæ–¹æ³•æé«˜äº†éŸ³é¢‘è´¨é‡ã€è¯­ä¹‰å¯¹é½å’Œå”‡éƒ¨åŒæ­¥å‡†ç¡®æ€§ï¼ŒåŒæ—¶åœ¨æ–‡æœ¬åˆ°éŸ³é¢‘&#x2F;è¯­éŸ³&#x2F;æ­Œæ›²ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚å…¶æ¨ç†æ—¶é—´ä¸º1.91ç§’å¯ç”Ÿæˆ8ç§’çš„éŸ³é¢‘ï¼Œåœ¨æ•ˆç‡å’Œé€šç”¨æ€§æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00733v4">PDF</a> 12 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨ï¼ˆMMDitï¼‰çš„AudioGen-Omniç»Ÿä¸€æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸è¾“å…¥è§†é¢‘åŒæ­¥çš„é«˜ä¿çœŸéŸ³é¢‘ã€è¯­éŸ³å’Œæ­Œæ›²ã€‚å®ƒé€šè¿‡æ— ç¼é›†æˆå¤§è§„æ¨¡è§†é¢‘-æ–‡æœ¬-éŸ³é¢‘è¯­æ–™åº“ï¼Œå®ç°äº†ä¸€ç§èƒ½å¤ŸåŸºäºå¤šæ¨¡æ€è¾“å…¥ç”Ÿæˆè¯­ä¹‰ä¸°å¯Œã€å£°éŸ³å¤šæ ·çš„éŸ³é¢‘çš„æ¨¡å‹ï¼Œå¹¶é€‚åº”å¹¿æ³›çš„éŸ³é¢‘ç”Ÿæˆä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AudioGen-Omniæ˜¯ä¸€ç§åŸºäºå¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨ï¼ˆMMDitï¼‰çš„ç»Ÿä¸€æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸéŸ³é¢‘ã€è¯­éŸ³å’Œæ­Œæ›²ï¼Œä¸è¾“å…¥è§†é¢‘åŒæ­¥ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹è”åˆè®­ç»ƒèŒƒå¼ï¼Œé›†æˆå¤§è§„æ¨¡è§†é¢‘-æ–‡æœ¬-éŸ³é¢‘è¯­æ–™åº“ã€‚</li>
<li>é€šè¿‡ç»Ÿä¸€æ­Œè¯-è½¬å½•ç¼–ç å™¨ï¼Œå°†æ­Œæ›²å’Œå£è¯­çš„å­—æ¯å’ŒéŸ³ç´ ç¼–ç ä¸ºå¯†é›†å¸§çº§è¡¨ç¤ºã€‚</li>
<li>ä½¿ç”¨åŸºäºAdaLNçš„è”åˆæ³¨æ„æœºåˆ¶èåˆäº†å¯†é›†å¸§çº§è¡¨ç¤ºï¼Œå¢å¼ºäº†ç›¸ä½å¯¹é½çš„å¼‚æ„å®šä½çŒæ³¨ï¼ˆPAAPIï¼‰ã€‚</li>
<li>RoPEé€‰æ‹©æ€§åº”ç”¨äºå…·æœ‰æ—¶é—´ç»“æ„çš„æ¨¡æ€ï¼Œå®ç°ç²¾ç¡®å’Œç¨³å¥çš„è·¨æ¨¡æ€å¯¹é½ã€‚</li>
<li>é€šè¿‡è§£å†»æ‰€æœ‰æ¨¡æ€å’Œæ©ç›–ç¼ºå¤±è¾“å…¥ï¼Œç¼“è§£æ–‡æœ¬å†»ç»“èŒƒå¼çš„è¯­ä¹‰çº¦æŸï¼Œå®ç°æœ‰æ•ˆçš„è·¨æ¨¡æ€æ¡ä»¶ã€‚</li>
<li>è”åˆè®­ç»ƒæé«˜äº†éŸ³é¢‘è´¨é‡ã€è¯­ä¹‰å¯¹é½å’Œå”‡åŒæ­¥ç²¾åº¦ï¼Œå¹¶åœ¨æ–‡æœ¬åˆ°éŸ³é¢‘&#x2F;è¯­éŸ³&#x2F;æ­Œæ›²ä»»åŠ¡ä¸Šå®ç°æœ€æ–°ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a0e17d51526b5419f7d1bf9a792d51a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38e20067b86f9682aa8b7b2428f93191.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a86b4f217474a0d03ffcfc10bb25eabd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6b67d41d4c49685f7d28ce294f9f9f4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Overview-of-Automatic-Speech-Analysis-and-Technologies-for-Neurodegenerative-Disorders-Diagnosis-and-Assistive-Applications"><a href="#Overview-of-Automatic-Speech-Analysis-and-Technologies-for-Neurodegenerative-Disorders-Diagnosis-and-Assistive-Applications" class="headerlink" title="Overview of Automatic Speech Analysis and Technologies for   Neurodegenerative Disorders: Diagnosis and Assistive Applications"></a>Overview of Automatic Speech Analysis and Technologies for   Neurodegenerative Disorders: Diagnosis and Assistive Applications</h2><p><strong>Authors:Shakeel A. Sheikh, Md. Sahidullah, Ina Kodrasi</strong></p>
<p>Advancements in spoken language technologies for neurodegenerative speech disorders are crucial for meeting both clinical and technological needs. This overview paper is vital for advancing the field, as it presents a comprehensive review of state-of-the-art methods in pathological speech detection, automatic speech recognition, pathological speech intelligibility enhancement, intelligibility and severity assessment, and data augmentation approaches for pathological speech. It also highlights key challenges, such as ensuring robustness, privacy, and interpretability. The paper concludes by exploring promising future directions, including the adoption of multimodal approaches and the integration of large language models to further advance speech technologies for neurodegenerative speech disorders. </p>
<blockquote>
<p>å£è¯­æŠ€æœ¯åœ¨ç¥ç»é€€è¡Œæ€§ç–¾ç—…æ‰€è‡´è¨€è¯­éšœç¢æ–¹é¢çš„è¿›å±•å¯¹äºæ»¡è¶³ä¸´åºŠå’ŒæŠ€æœ¯éœ€æ±‚è‡³å…³é‡è¦ã€‚è¿™ç¯‡ç»¼è¿°è®ºæ–‡å¯¹æ¨è¿›è¯¥é¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ï¼Œå› ä¸ºå®ƒå…¨é¢å›é¡¾äº†ç—…ç†æ€§è¨€è¯­æ£€æµ‹ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€ç—…ç†æ€§è¨€è¯­æ¸…æ™°åº¦å¢å¼ºã€æ¸…æ™°åº¦å’Œä¸¥é‡ç¨‹åº¦è¯„ä¼°ä»¥åŠç—…ç†æ€§è¨€è¯­æ•°æ®å¢å¼ºæ–¹æ³•çš„æœ€æ–°æ–¹æ³•ã€‚å®ƒè¿˜å¼ºè°ƒäº†ç¡®ä¿ç¨³å¥æ€§ã€éšç§æ€§å’Œå¯è§£é‡Šæ€§ç­‰å…³é”®æŒ‘æˆ˜ã€‚è®ºæ–‡æœ€åæ¢è®¨äº†æœ‰å‰é€”çš„æœªæ¥æ–¹å‘ï¼ŒåŒ…æ‹¬é‡‡ç”¨å¤šæ¨¡å¼æ–¹æ³•ä»¥åŠæ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥è¿›ä¸€æ­¥æ¨è¿›ç¥ç»é€€è¡Œæ€§ç–¾ç—…æ‰€è‡´è¨€è¯­éšœç¢çš„å£è¯­æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03536v2">PDF</a> Published in IEEE Journal of Selected Topics in Signal Processing</p>
<p><strong>Summary</strong><br>     è¿™ç¯‡è®ºæ–‡æ¦‚è¿°äº†ç¥ç»é€€è¡Œæ€§ç–¾ç—…è¯­éŸ³æŠ€æœ¯çš„æœ€æ–°è¿›å±•ï¼Œæ¶µç›–äº†ç—…ç†è¯­éŸ³æ£€æµ‹ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³æ¸…æ™°åº¦æå‡ã€è¯­éŸ³æ¸…æ™°åº¦å’Œä¸¥é‡ç¨‹åº¦è¯„ä¼°ä»¥åŠæ•°æ®æ‰©å……ç­‰æ–¹é¢çš„å…ˆè¿›æ–¹æ³•ã€‚è®ºæ–‡å¼ºè°ƒäº†å…³é”®æŒ‘æˆ˜ï¼Œå¦‚ç¡®ä¿æŠ€æœ¯çš„ç¨³å¥æ€§ã€éšç§æ€§å’Œå¯è§£é‡Šæ€§ï¼Œå¹¶æ¢è®¨äº†æœªæ¥é‡‡ç”¨å¤šæ¨¡å¼æ–¹æ³•å’Œæ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹ç­‰æœ‰æœ›æ¨åŠ¨è¯¥é¢†åŸŸè¿›ä¸€æ­¥å‘å±•çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡å…¨é¢å›é¡¾äº†ç¥ç»é€€è¡Œæ€§ç–¾ç—…è¯­éŸ³æŠ€æœ¯çš„æœ€æ–°è¿›å±•ã€‚</li>
<li>ä»‹ç»äº†åŒ…æ‹¬ç—…ç†è¯­éŸ³æ£€æµ‹ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç­‰æ–¹é¢çš„å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>è®ºæ–‡å¼ºè°ƒäº†æé«˜æŠ€æœ¯ç¨³å¥æ€§ã€éšç§æ€§å’Œå¯è§£é‡Šæ€§çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>è®ºæ–‡æŒ‡å‡ºæœªæ¥å‘å±•æ–¹å‘åŒ…æ‹¬é‡‡ç”¨å¤šæ¨¡å¼æ–¹æ³•å’Œæ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>è¯¥è®ºæ–‡å¯¹äºæ¨åŠ¨ç¥ç»é€€è¡Œæ€§ç–¾ç—…è¯­éŸ³æŠ€æœ¯çš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>è®ºæ–‡æ¶‰åŠè¯­éŸ³æ¸…æ™°åº¦æå‡å’Œè¯­éŸ³æ¸…æ™°åº¦å’Œä¸¥é‡ç¨‹åº¦è¯„ä¼°ç­‰æ–¹é¢çš„å†…å®¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03536">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e23e6ccc7af7fd30bcf432a8645d921e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b2ab1c5fc8bd3351f57bb54bae6803b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b588376378354e2e06fd025c1d511d5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef40ac70b1cfd27cbde8fa24d696f1c4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Recent-Advances-in-Speech-Language-Models-A-Survey"><a href="#Recent-Advances-in-Speech-Language-Models-A-Survey" class="headerlink" title="Recent Advances in Speech Language Models: A Survey"></a>Recent Advances in Speech Language Models: A Survey</h2><p><strong>Authors:Wenqian Cui, Dianzhi Yu, Xiaoqi Jiao, Ziqiao Meng, Guangyan Zhang, Qichao Wang, Yiwen Guo, Irwin King</strong></p>
<p>Large Language Models (LLMs) have recently garnered significant attention, primarily for their capabilities in text-based interactions. However, natural human interaction often relies on speech, necessitating a shift towards voice-based models. A straightforward approach to achieve this involves a pipeline of &#96;&#96;Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)â€, where input speech is transcribed to text, processed by an LLM, and then converted back to speech. Despite being straightforward, this method suffers from inherent limitations, such as information loss during modality conversion, significant latency due to the complex pipeline, and error accumulation across the three stages. To address these issues, Speech Language Models (SpeechLMs) â€“ end-to-end models that generate speech without converting from text â€“ have emerged as a promising alternative. This survey paper provides the first comprehensive overview of recent methodologies for constructing SpeechLMs, detailing the key components of their architecture and the various training recipes integral to their development. Additionally, we systematically survey the various capabilities of SpeechLMs, categorize their evaluation metrics, and discuss the challenges and future research directions in this rapidly evolving field. The GitHub repository is available at <a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/Awesome-SpeechLM-Survey">https://github.com/dreamtheater123/Awesome-SpeechLM-Survey</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿‘æœŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œä¸»è¦å› å…¶åŸºäºæ–‡æœ¬çš„äº¤äº’èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè‡ªç„¶äººæœºäº¤äº’é€šå¸¸ä¾èµ–äºè¯­éŸ³ï¼Œå› æ­¤éœ€è¦ä½¿ç”¨è¯­éŸ³æ¨¡å‹è¿›è¡Œè½¬æ¢ã€‚ä¸€ç§å®ç°æ­¤ç›®æ ‡çš„ç›´æ¥æ–¹æ³•æ˜¯é€šè¿‡â€œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰+ LLM +æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰â€çš„æµç¨‹ï¼Œå°†è¾“å…¥è¯­éŸ³è½¬å½•ä¸ºæ–‡æœ¬ï¼Œç”±LLMè¿›è¡Œå¤„ç†ï¼Œç„¶åå†è½¬å›è¯­éŸ³ã€‚å°½ç®¡è¿™ç§æ–¹æ³•å¾ˆç›´æ¥ï¼Œä½†å®ƒå­˜åœ¨å›ºæœ‰çš„å±€é™æ€§ï¼Œä¾‹å¦‚åœ¨æ¨¡æ€è½¬æ¢è¿‡ç¨‹ä¸­çš„ä¿¡æ¯ä¸¢å¤±ã€å¤æ‚çš„ç®¡é“å¸¦æ¥çš„æ˜¾è‘—å»¶è¿Ÿä»¥åŠåœ¨ä¸‰ä¸ªé˜¶æ®µä¸­çš„é”™è¯¯ç´¯ç§¯ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œè¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSpeechLMï¼‰å‡ºç°äº†â€”â€”ä¸€ç§èƒ½å¤Ÿåœ¨ä¸è½¬æ¢ä¸ºæ–‡æœ¬çš„æƒ…å†µä¸‹ç”Ÿæˆè¯­éŸ³çš„ç«¯åˆ°ç«¯æ¨¡å‹ã€‚è¿™ç¯‡ç»¼è¿°è®ºæ–‡é¦–æ¬¡å…¨é¢æ¦‚è¿°äº†æ„å»ºSpeechLMçš„æœ€æ–°æ–¹æ³•ï¼Œè¯¦ç»†ä»‹ç»äº†å…¶å…³é”®ç»„ä»¶å’Œå¯¹å…¶å¼€å‘è‡³å…³é‡è¦çš„å„ç§åŸ¹è®­é…æ–¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç³»ç»Ÿåœ°æ¦‚è¿°äº†SpeechLMçš„å„ç§åŠŸèƒ½ï¼Œå¯¹å…¶è¯„ä¼°æŒ‡æ ‡è¿›è¡Œäº†åˆ†ç±»ï¼Œå¹¶è®¨è®ºäº†è¿™ä¸€å¿«é€Ÿå‘å±•çš„é¢†åŸŸçš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚GitHubä»“åº“åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/Awesome-SpeechLM-Survey">https://github.com/dreamtheater123/Awesome-SpeechLM-Survey</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03751v4">PDF</a> The reduced version of this paper has been accepted at ACL 2025</p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†åŸºäºæ–‡æœ¬çš„äº¤äº’æ–¹å¼å¿½ç•¥äº†è‡ªç„¶äººç±»äº¤äº’é€šå¸¸ä¾èµ–äºè¯­éŸ³çš„ç‰¹æ€§ã€‚å› æ­¤ï¼Œéœ€è¦è½¬å‘è¯­éŸ³æ¨¡å‹ã€‚å°½ç®¡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰+ LLM +æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ–¹æ³•å¯å®ç°æ­¤ç›®æ ‡ï¼Œä½†å­˜åœ¨ä¿¡æ¯æŸå¤±ã€å»¶è¿Ÿå’Œè¯¯å·®ç´¯ç§¯ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œå‡ºç°äº†è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSpeechLMsï¼‰â€”â€”æ— éœ€ä»æ–‡æœ¬è½¬æ¢å³å¯ç”Ÿæˆè¯­éŸ³çš„ç«¯åˆ°ç«¯æ¨¡å‹ã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢æ¦‚è¿°äº†SpeechLMsçš„æœ€æ–°æ–¹æ³•ï¼Œè¯¦ç»†ä»‹ç»äº†å…¶æ¶æ„çš„å…³é”®ç»„ä»¶å’Œå¼€å‘è¿‡ç¨‹ä¸­ä¸å¯æˆ–ç¼ºçš„è®­ç»ƒé£Ÿè°±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç³»ç»Ÿåœ°è°ƒæŸ¥äº†SpeechLMsçš„å„é¡¹åŠŸèƒ½ã€åˆ†ç±»äº†è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶è®¨è®ºäº†è¿™ä¸€å¿«é€Ÿå‘å±•é¢†åŸŸçš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚GitHubä»“åº“åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/Awesome-SpeechLM-Survey%E3%80%82">https://github.com/dreamtheater123/Awesome-SpeechLM-Surveyã€‚</a></p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶äººæœºäº¤äº’ä¸­éœ€è€ƒè™‘è¯­éŸ³äº¤äº’çš„é‡è¦æ€§ã€‚</li>
<li>è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSpeechLMsï¼‰ä½œä¸ºç«¯åˆ°ç«¯æ¨¡å‹ï¼Œå¯ç›´æ¥ç”Ÿæˆè¯­éŸ³ï¼Œé¿å…äº†ä¿¡æ¯æŸå¤±å’Œå¤æ‚ç®¡é“å¸¦æ¥çš„å»¶è¿Ÿé—®é¢˜ã€‚</li>
<li>SpeechLMsçš„å…³é”®ç»„ä»¶åŒ…æ‹¬æ¶æ„è®¾è®¡å’Œè®­ç»ƒé£Ÿè°±ï¼Œå…¶ä¸­æ¶‰åŠå¤šç§æŠ€æœ¯å’Œæ–¹æ³•ã€‚</li>
<li>SpeechLMså…·æœ‰å¤šç§åŠŸèƒ½ï¼Œå¦‚è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³åˆæˆã€å¯¹è¯ç³»ç»Ÿç­‰ï¼Œå…¶è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬å‡†ç¡®æ€§ã€è‡ªç„¶åº¦ç­‰ã€‚</li>
<li>ç›®å‰SpeechLMsé¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æ•°æ®æ ‡æ³¨ã€æ¨¡å‹è§„æ¨¡ä¸æ•ˆç‡å¹³è¡¡ç­‰ï¼Œå¹¶éœ€è¦æœªæ¥è¿›ä¸€æ­¥ç ”ç©¶å’Œæ”¹è¿›ã€‚</li>
<li>è¯»è€…å¯ä»¥é€šè¿‡è®¿é—®GitHubä»“åº“ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/Awesome-SpeechLM-Survey%EF%BC%89%E8%8E%B7%E5%8F%96%E6%9B%B4%E5%A4%9A%E5%85%B3%E4%BA%8ESpeechLMs%E7%9A%84%E8%AF%A6%E7%BB%86%E4%BF%A1%E5%A6%82%E5%92%8C%E7%A0%94%E7%A9%B6%E8%B5%84%E6%BA%90%E3%80%82">https://github.com/dreamtheater123/Awesome-SpeechLM-Surveyï¼‰è·å–æ›´å¤šå…³äºSpeechLMsçš„è¯¦ç»†ä¿¡æ¯å’Œç ”ç©¶èµ„æºã€‚</a></li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03751">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5110266f7ef8903ead538b6556ebf9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69237885977210d8682d6c111e5c67bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14b065330e5f9dea5cf648faf1e1cca7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c23e2f399ff886f471c12d7cc5392a15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65d1e068eda2867fe18e5d22ad33f55c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-737e22c8d6590a815288fd6b5a40fe0d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-03f5f5537bee7dae18f1588e3ac0bdfd.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="WhisperNER-Unified-Open-Named-Entity-and-Speech-Recognition"><a href="#WhisperNER-Unified-Open-Named-Entity-and-Speech-Recognition" class="headerlink" title="WhisperNER: Unified Open Named Entity and Speech Recognition"></a>WhisperNER: Unified Open Named Entity and Speech Recognition</h2><p><strong>Authors:Gil Ayache, Menachem Pirchi, Aviv Navon, Aviv Shamsian, Gill Hetz, Joseph Keshet</strong></p>
<p>Integrating named entity recognition (NER) with automatic speech recognition (ASR) can significantly enhance transcription accuracy and informativeness. In this paper, we introduce WhisperNER, a novel model that allows joint speech transcription and entity recognition. WhisperNER supports open-type NER, enabling recognition of diverse and evolving entities at inference. Building on recent advancements in open NER research, we augment a large synthetic dataset with synthetic speech samples. This allows us to train WhisperNER on a large number of examples with diverse NER tags. During training, the model is prompted with NER labels and optimized to output the transcribed utterance along with the corresponding tagged entities. To evaluate WhisperNER, we generate synthetic speech for commonly used NER benchmarks and annotate existing ASR datasets with open NER tags. Our experiments demonstrate that WhisperNER outperforms natural baselines on both out-of-domain open type NER and supervised finetuning. </p>
<blockquote>
<p>å°†å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç›¸ç»“åˆï¼Œå¯ä»¥æ˜¾è‘—æé«˜è½¬å½•å‡†ç¡®æ€§å’Œä¿¡æ¯é‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†WhisperNERï¼Œè¿™æ˜¯ä¸€ç§å…è®¸è”åˆè¯­éŸ³è½¬å½•å’Œå®ä½“è¯†åˆ«çš„æ–°å‹æ¨¡å‹ã€‚WhisperNERæ”¯æŒå¼€æ”¾å‹NERï¼Œèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­è¯†åˆ«å¤šæ ·ä¸”ä¸æ–­å‘å±•çš„å®ä½“ã€‚åŸºäºæœ€æ–°çš„å¼€æ”¾å‹NERç ”ç©¶ï¼Œæˆ‘ä»¬é€šè¿‡åˆæˆè¯­éŸ³æ ·æœ¬æ‰©å……äº†ä¸€ä¸ªå¤§å‹åˆæˆæ•°æ®é›†ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿåœ¨å¤§é‡çš„å¸¦æœ‰ä¸åŒNERæ ‡ç­¾çš„æ ·æœ¬ä¸Šè®­ç»ƒWhisperNERã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä¼šæç¤ºNERæ ‡ç­¾å¹¶è¿›è¡Œä¼˜åŒ–ï¼Œä»¥è¾“å‡ºè½¬å½•çš„è¯è¯­å’Œç›¸åº”çš„æ ‡è®°å®ä½“ã€‚ä¸ºäº†è¯„ä¼°WhisperNERçš„æ€§èƒ½ï¼Œæˆ‘ä»¬ä¸ºå¸¸ç”¨çš„NERåŸºå‡†ç”Ÿæˆåˆæˆè¯­éŸ³ï¼Œå¹¶ä½¿ç”¨å¼€æ”¾å‹NERæ ‡ç­¾æ ‡æ³¨ç°æœ‰çš„ASRæ•°æ®é›†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨ç¦»åŸŸå¼€æ”¾å‹NERè¿˜æ˜¯ç›‘ç£å¾®è°ƒä¸Šï¼ŒWhisperNERçš„è¡¨ç°éƒ½ä¼˜äºè‡ªç„¶åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08107v2">PDF</a> ASRU 2025, IEEE</p>
<p><strong>Summary</strong></p>
<p>èåˆå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å¯æœ‰æ•ˆæå‡è½¬å½•å‡†ç¡®æ€§å’Œä¿¡æ¯é‡ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹æ¨¡å‹WhisperNERï¼Œå®ƒå¯è”åˆè¿›è¡Œè¯­éŸ³è½¬å½•å’Œå®ä½“è¯†åˆ«ã€‚WhisperNERæ”¯æŒå¼€æ”¾å‹NERï¼Œå¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­è¯†åˆ«å¤šæ ·ä¸”ä¸æ–­å‘å±•çš„å®ä½“ã€‚åŸºäºæœ€æ–°çš„å¼€æ”¾å‹NERç ”ç©¶æˆæœï¼Œæˆ‘ä»¬åˆ©ç”¨åˆæˆçš„å¤§å‹æ•°æ®é›†ä¸åˆæˆè¯­éŸ³æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œä½¿WhisperNERèƒ½å¤Ÿåœ¨å¤§é‡æ ·æœ¬ä¸Šè®­ç»ƒï¼Œå¹¶æ¶µç›–å¤šæ ·çš„NERæ ‡ç­¾ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹é€šè¿‡NERæ ‡ç­¾è¿›è¡Œæç¤ºï¼Œå¹¶ä¼˜åŒ–è¾“å‡ºå¸¦ç›¸åº”æ ‡ç­¾å®ä½“çš„è½¬å½•è¯­éŸ³ã€‚ä¸ºè¯„ä¼°WhisperNERæ€§èƒ½ï¼Œæˆ‘ä»¬ä¸ºå¸¸ç”¨çš„NERåŸºå‡†æµ‹è¯•ç”Ÿæˆåˆæˆè¯­éŸ³ï¼Œå¹¶å¯¹ç°æœ‰çš„ASRæ•°æ®é›†ä½¿ç”¨å¼€æ”¾å‹NERæ ‡ç­¾è¿›è¡Œæ ‡æ³¨ã€‚å®éªŒè¡¨æ˜ï¼ŒWhisperNERåœ¨å¼€æ”¾å‹NERå’Œç»è¿‡ç›‘ç£å¾®è°ƒçš„ä»»åŠ¡ä¸Šå‡ä¼˜äºè‡ªç„¶åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•´åˆå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰èƒ½æ˜¾è‘—æå‡è½¬å½•å‡†ç¡®æ€§å’Œä¿¡æ¯é‡ã€‚</li>
<li>WhisperNERæ˜¯ä¸€ç§æ–°å‹æ¨¡å‹ï¼Œå¯åŒæ—¶å®ç°è¯­éŸ³è½¬å½•å’Œå®ä½“è¯†åˆ«ã€‚</li>
<li>WhisperNERæ”¯æŒå¼€æ”¾å‹NERï¼Œèƒ½å¤Ÿè¯†åˆ«å¤šæ ·ä¸”ä¸æ–­å‘å±•çš„å®ä½“ã€‚</li>
<li>åˆ©ç”¨åˆæˆçš„å¤§å‹æ•°æ®é›†ä¸åˆæˆè¯­éŸ³æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œå¢å¼ºWhisperNERæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡NERæ ‡ç­¾æç¤ºï¼Œå¹¶ä¼˜åŒ–è¾“å‡ºå¸¦ç›¸åº”æ ‡ç­¾å®ä½“çš„è½¬å½•è¯­éŸ³ã€‚</li>
<li>åˆæˆè¯­éŸ³ç”Ÿæˆå’Œç°æœ‰ASRæ•°æ®é›†çš„å¼€æ”¾å‹NERæ ‡ç­¾æ ‡æ³¨æ–¹æ³•ç”¨äºè¯„ä¼°WhisperNERæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08107">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e4e388a638db2ea5df73d309f077e88d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20a610eee62a9fc541ad10276c379572.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acc00d00cef937cb5e43f2d9eabe12c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23937fc617e55f95757d55401869fc98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bda470a50c548e1b7cb2371edd1d66ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19abfbf0206eaebcfd8ad5eb6ebd31b2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e3774b1aa612e2602278077444b4dd76.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c8bee61fbd138c40ce5dfeed7b6a32d4.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-09  FLUX-Makeup High-Fidelity, Identity-Consistent, and Robust Makeup   Transfer via Diffusion Transformer
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0a0e2ac850fa80c8b33071e7b1656377.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-09  UNCAGE Contrastive Attention Guidance for Masked Generative   Transformers in Text-to-Image Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
