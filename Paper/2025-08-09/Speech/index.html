<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-08-09  Speech LLMs in Low-Resource Scenarios Data Volume Requirements and the   Impact of Pretraining on High-Resource Languages">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e23e6ccc7af7fd30bcf432a8645d921e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    27 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-09-更新"><a href="#2025-08-09-更新" class="headerlink" title="2025-08-09 更新"></a>2025-08-09 更新</h1><h2 id="Speech-LLMs-in-Low-Resource-Scenarios-Data-Volume-Requirements-and-the-Impact-of-Pretraining-on-High-Resource-Languages"><a href="#Speech-LLMs-in-Low-Resource-Scenarios-Data-Volume-Requirements-and-the-Impact-of-Pretraining-on-High-Resource-Languages" class="headerlink" title="Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the   Impact of Pretraining on High-Resource Languages"></a>Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the   Impact of Pretraining on High-Resource Languages</h2><p><strong>Authors:Seraphina Fong, Marco Matassoni, Alessio Brutti</strong></p>
<p>Large language models (LLMs) have demonstrated potential in handling spoken inputs for high-resource languages, reaching state-of-the-art performance in various tasks. However, their applicability is still less explored in low-resource settings. This work investigates the use of Speech LLMs for low-resource Automatic Speech Recognition using the SLAM-ASR framework, where a trainable lightweight projector connects a speech encoder and a LLM. Firstly, we assess training data volume requirements to match Whisper-only performance, re-emphasizing the challenges of limited data. Secondly, we show that leveraging mono- or multilingual projectors pretrained on high-resource languages reduces the impact of data scarcity, especially with small training sets. Using multilingual LLMs (EuroLLM, Salamandra) with whisper-large-v3-turbo, we evaluate performance on several public benchmarks, providing insights for future research on optimizing Speech LLMs for low-resource languages and multilinguality. </p>
<blockquote>
<p>大型语言模型（LLM）在处理高资源语言的口语输入方面已显示出潜力，在各种任务中达到了最新技术性能。然而，它们在低资源环境中的适用性仍研究较少。本研究利用SLAM-ASR框架探索了语音LLM在资源低下的自动语音识别（ASR）中的应用，其中可训练的轻量级投影仪连接了语音编码器LLM。首先，我们评估了达到仅whisper性能所需的训练数据量，再次强调有限数据的挑战。其次，我们表明利用在高资源语言上预训练的单语或多语投影仪可以减少数据稀缺的影响，尤其是使用小型训练集时。使用多语言LLM（EuroLLM、Salamandra）与whisper-large-v3-turbo，我们在多个公共基准测试集上评估性能，为未来优化用于低资源语言和多种语言的语音LLM的研究提供见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05149v1">PDF</a> Accepted at Interspeech 2025. 5 pages, 2 figures, 3 tables</p>
<p><strong>总结</strong></p>
<p>大规模语言模型（LLMs）在处理高资源语言的口语输入方面显示出巨大潜力，并在各种任务中达到最新技术水平。然而，它们在低资源环境中的适用性仍然有待探索。本研究使用SLAM-ASR框架探讨了低资源自动语音识别中使用语音LLMs的问题。一个可训练的轻量级投影仪连接语音编码器与LLM。首先，我们评估了达到whisper-only性能所需的训练数据量，再次强调了数据有限的挑战。其次，我们展示了利用单语种或多语种预训练在高资源语言上的投影仪能够减少数据稀缺的影响，特别是小训练集情况下更为明显。通过使用多语种LLM（EuroLLM、Salamandra）与whisper-large-v3-turbo，我们在多个公共基准测试上评估性能，为未来优化语音LLMs在低资源语言和多语种方面的应用提供见解。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大规模语言模型在高资源语言的口语处理方面表现出卓越性能，但在低资源环境中其适用性有待进一步探索。</li>
<li>使用SLAM-ASR框架研究语音LLMs在自动语音识别方面的应用。</li>
<li>训练数据量对达到whisper-only性能至关重要，突显数据有限的挑战。</li>
<li>利用预训练的单语种或多语种投影仪在高资源语言上可以减少数据稀缺的影响。</li>
<li>多语种LLMs在语音识别方面具有优势，尤其是在处理小训练集时效果更为显著。</li>
<li>在多个公共基准测试上对性能进行评估，为后续优化提供参照。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05149">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a0351ecb2da02e764bfd9761ba4842ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-40f92a068013684fccbb4eca14c42705.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ba26849196296b4abf7f9b3278835bb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f4b8923254d048c9a21ebabcda1c7e5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fae0b91957e59611f22c5a79d299045d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MOVER-Combining-Multiple-Meeting-Recognition-Systems"><a href="#MOVER-Combining-Multiple-Meeting-Recognition-Systems" class="headerlink" title="MOVER: Combining Multiple Meeting Recognition Systems"></a>MOVER: Combining Multiple Meeting Recognition Systems</h2><p><strong>Authors:Naoyuki Kamo, Tsubasa Ochiai, Marc Delcroix, Tomohiro Nakatani</strong></p>
<p>In this paper, we propose Meeting recognizer Output Voting Error Reduction (MOVER), a novel system combination method for meeting recognition tasks. Although there are methods to combine the output of diarization (e.g., DOVER) or automatic speech recognition (ASR) systems (e.g., ROVER), MOVER is the first approach that can combine the outputs of meeting recognition systems that differ in terms of both diarization and ASR. MOVER combines hypotheses with different time intervals and speaker labels through a five-stage process that includes speaker alignment, segment grouping, word and timing combination, etc. Experimental results on the CHiME-8 DASR task and the multi-channel track of the NOTSOFAR-1 task demonstrate that MOVER can successfully combine multiple meeting recognition systems with diverse diarization and recognition outputs, achieving relative tcpWER improvements of 9.55 % and 8.51 % over the state-of-the-art systems for both tasks. </p>
<blockquote>
<p>在这篇论文中，我们提出了会议识别输出投票误差减少（MOVER）方法，这是一种用于会议识别任务的新型系统组合方法。尽管有方法可以将日记化（例如DOVER）或自动语音识别（ASR）系统的输出进行组合（例如ROVER），但MOVER是首个能够结合在日记化和ASR方面有所不同的会议识别系统输出的方法。MOVER通过包括说话人对齐、段落分组、词汇和时序组合等五个阶段的流程，将具有不同时间间隔和说话人标签的假设进行结合。在CHiME-8 DASR任务和NOTSOFAR-1任务的多通道轨道上的实验结果表明，MOVER能够成功地将多个会议识别系统进行组合，这些系统具有多样化的日记化和识别输出，相对于当前先进系统，实现了9.55%和8.51%的tcpWER改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05055v1">PDF</a> </p>
<p><strong>Summary</strong><br>会议识别输出投票误差减少（MOVER）是一种新颖的会议识别任务系统组合方法。它结合不同时间段和说话人标签的假设，通过五阶段过程，包括说话人对齐、分段分组、词语和时间组合等。在CHiME-8 DASR任务和NOTSOFAR-1多通道任务的实验中，MOVER成功结合了多个会议识别系统，实现了相对tcpWER的改进，超过了现有系统的表现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MOVER是一种用于会议识别任务的全新系统组合方法。</li>
<li>MOVER结合了不同时间段和说话人标签的假设。</li>
<li>MOVER包含五阶段过程：说话人对齐、分段分组、词语和时间组合等。</li>
<li>CHiME-8 DASR任务和NOTSOFAR-1多通道任务的实验验证了MOVER的有效性。</li>
<li>MOVER能够成功结合多个会议识别系统，这些系统可能在迪亚里化和识别输出方面有所不同。</li>
<li>与现有系统相比，MOVER在相对tcpWER方面实现了显著的改进，分别为9.55%和8.51%。</li>
<li>MOVER方法为会议识别任务的系统组合提供了新的思路和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05055">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5fa15e55a93bac7429182b66a9e8fe98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-048a56639da08ddd663304eb8663ec6f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56d461373d2f503f3e731143b8a150f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-800cb0884570c805dca312459e4517af.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="REF-VC-Robust-Expressive-and-Fast-Zero-Shot-Voice-Conversion-with-Diffusion-Transformers"><a href="#REF-VC-Robust-Expressive-and-Fast-Zero-Shot-Voice-Conversion-with-Diffusion-Transformers" class="headerlink" title="REF-VC: Robust, Expressive and Fast Zero-Shot Voice Conversion with   Diffusion Transformers"></a>REF-VC: Robust, Expressive and Fast Zero-Shot Voice Conversion with   Diffusion Transformers</h2><p><strong>Authors:Yuepeng Jiang, Ziqian Ning, Shuai Wang, Chengjia Wang, Mengxiao Bi, Pengcheng Zhu, Lei Xie, Zhonghua Fu</strong></p>
<p>In real-world voice conversion applications, environmental noise in source speech and user demands for expressive output pose critical challenges. Traditional ASR-based methods ensure noise robustness but suppress prosody, while SSL-based models improve expressiveness but suffer from timbre leakage and noise sensitivity. This paper proposes REF-VC, a noise-robust expressive voice conversion system. Key innovations include: (1) A random erasing strategy to mitigate the information redundancy inherent in SSL feature, enhancing noise robustness and expressiveness; (2) Implicit alignment inspired by E2TTS to suppress non-essential feature reconstruction; (3) Integration of Shortcut Models to accelerate flow matching inference, significantly reducing to 4 steps. Experimental results demonstrate that our model outperforms baselines such as Seed-VC in zero-shot scenarios on the noisy set, while also performing comparably to Seed-VC on the clean set. In addition, REF-VC can be compatible with singing voice conversion within one model. </p>
<blockquote>
<p>在真实世界的声音转换应用中，源语音中的环境噪声以及用户对表现力输出的需求构成了关键的挑战。基于传统ASR的方法虽然能保证噪声稳健性，但会抑制韵律，而基于SSL的模型虽然能提高表现力，但却存在音色泄露和噪声敏感的问题。本文提出了REF-VC，一种噪声鲁棒性强的语音转换系统。主要创新点包括：（1）采用随机擦除策略，减轻SSL特征中的信息冗余，增强噪声稳健性和表现力；（2）受E2TTS启发的隐对齐方式，抑制非关键特征重建；（3）集成快捷模型，加速匹配流推理，大大缩短至4步。实验结果表明，我们的模型在噪声集上的零样本场景中优于基线方法（如Seed-VC），同时在清洁集上的表现与Seed-VC相当。此外，REF-VC可以在一个模型内兼容歌声转换。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04996v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>针对真实世界语音转换应用，源语音中的环境噪声和用户对于表达输出的需求构成了重大挑战。传统基于自动语音识别（ASR）的方法确保了噪声鲁棒性，但抑制了韵律；而基于自监督学习（SSL）的模型则提高了表达性，但存在音色泄漏和噪声敏感性。本文提出REF-VC，一种噪声鲁棒性强的表达性语音转换系统。主要创新包括：（1）采用随机擦除策略，减轻SSL特征中的信息冗余，增强噪声鲁棒性和表达性；（2）借鉴E2TTS的隐对齐方式，抑制非必要特征重建；（3）集成Shortcut Models加速流匹配推理，减少至4步。实验结果表明，我们的模型在噪声集上的零样本场景中优于基线方法如Seed-VC，同时在清洁集上的表现与Seed-VC相当。此外，REF-VC可以在一个模型内兼容歌唱语音转换。</p>
<p><strong>要点解析</strong></p>
<ol>
<li>真实世界语音转换面临环境噪声和表达性需求两大挑战。</li>
<li>传统ASR方法虽噪声鲁棒但抑制韵律，而SSL模型虽表达性强却存在音色泄漏和噪声敏感问题。</li>
<li>REF-VC系统提出随机擦除策略，旨在平衡噪声鲁棒性和表达性。</li>
<li>隐对齐方式和非必要特征抑制进一步提升了系统的性能。</li>
<li>集成Shortcut Models加速推理过程，减少计算步骤。</li>
<li>实验显示REF-VC在噪声环境下的性能优于基线方法，同时在清洁环境下表现相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04996">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8f877add70d978972ed80fe21b34dd82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21c65ed00bb726f0549712e7004e403b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c1b30faacd5d5eb868a42f428ac420c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-601fb1ab6420d2b83d941500cce376ce.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Pitch-Accent-Detection-improves-Pretrained-Automatic-Speech-Recognition"><a href="#Pitch-Accent-Detection-improves-Pretrained-Automatic-Speech-Recognition" class="headerlink" title="Pitch Accent Detection improves Pretrained Automatic Speech Recognition"></a>Pitch Accent Detection improves Pretrained Automatic Speech Recognition</h2><p><strong>Authors:David Sasu, Natalie Schluter</strong></p>
<p>We show the performance of Automatic Speech Recognition (ASR) systems that use semi-supervised speech representations can be boosted by a complimentary pitch accent detection module, by introducing a joint ASR and pitch accent detection model. The pitch accent detection component of our model achieves a significant improvement on the state-of-the-art for the task, closing the gap in F1-score by 41%. Additionally, the ASR performance in joint training decreases WER by 28.3% on LibriSpeech, under limited resource fine-tuning. With these results, we show the importance of extending pretrained speech models to retain or re-learn important prosodic cues such as pitch accent. </p>
<blockquote>
<p>我们展示了通过使用半监督语音表示，结合一个辅助的音调重音检测模块，可以提高自动语音识别（ASR）系统的性能。通过引入一个联合ASR和音调重音检测模型，我们的模型的音调重音检测部分在该任务上取得了显著改进，在F1分数上缩小了41%的差距。此外，在联合训练中，ASR的性能在LibriSpeech上通过有限的资源微调降低了28.3%的词错误率。这些结果表明，扩展预训练的语音模型以保留或重新学习重要的韵律线索（如音调重音）的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04814v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>使用半监督语音表示的自动语音识别（ASR）系统的性能可以通过引入联合ASR和音调重音检测模型得到提升。该模型的音调重音检测部分在任务上实现了显著改进，F1得分提高了41%。此外，在LibriSpeech上进行联合训练后，ASR性能在有限资源微调的情况下降低了28.3%的词错误率。这证明了扩展预训练语音模型以保留或重新学习如音调重音等重要韵律特征的重要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入联合ASR和音调重音检测模型可以提升ASR系统的性能。</li>
<li>模型的音调重音检测部分在任务上实现了显著改进，F1得分提高了41%。</li>
<li>在LibriSpeech数据集上，联合训练后ASR性能得到显著提高，词错误率降低了28.3%。</li>
<li>在有限资源微调的情况下，该模型依然表现出较好的性能提升。</li>
<li>扩展预训练语音模型有助于保留或重新学习重要的韵律特征。</li>
<li>保留或重新学习如音调重音等韵律特征对提升ASR系统性能至关重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04814">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d67e26b15c7bade3b9bfaf13bb116cdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c67409e5718f7a9bf24f3ba36e654625.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f444e2b748597194cb49640b339d449.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AudioGen-Omni-A-Unified-Multimodal-Diffusion-Transformer-for-Video-Synchronized-Audio-Speech-and-Song-Generation"><a href="#AudioGen-Omni-A-Unified-Multimodal-Diffusion-Transformer-for-Video-Synchronized-Audio-Speech-and-Song-Generation" class="headerlink" title="AudioGen-Omni: A Unified Multimodal Diffusion Transformer for   Video-Synchronized Audio, Speech, and Song Generation"></a>AudioGen-Omni: A Unified Multimodal Diffusion Transformer for   Video-Synchronized Audio, Speech, and Song Generation</h2><p><strong>Authors:Le Wang, Jun Wang, Chunyu Qiang, Feng Deng, Chen Zhang, Di Zhang, Kun Gai</strong></p>
<p>We present AudioGen-Omni - a unified approach based on multimodal diffusion transformers (MMDit), capable of generating high-fidelity audio, speech, and song coherently synchronized with the input video. AudioGen-Omni introduces a novel joint training paradigm that seamlessly integrates large-scale video-text-audio corpora, enabling a model capable of generating semantically rich, acoustically diverse audio conditioned on multimodal inputs and adaptable to a wide range of audio generation tasks. AudioGen-Omni employs a unified lyrics-transcription encoder that encodes graphemes and phonemes from both song and spoken inputs into dense frame-level representations. Dense frame-level representations are fused using an AdaLN-based joint attention mechanism enhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein RoPE is selectively applied to temporally structured modalities to ensure precise and robust cross-modal alignment. By unfreezing all modalities and masking missing inputs, AudioGen-Omni mitigates the semantic constraints of text-frozen paradigms, enabling effective cross-modal conditioning. This joint training approach enhances audio quality, semantic alignment, and lip-sync accuracy, while also achieving state-of-the-art results on Text-to-Audio&#x2F;Speech&#x2F;Song tasks. With an inference time of 1.91 seconds for 8 seconds of audio, it offers substantial improvements in both efficiency and generality. </p>
<blockquote>
<p>我们提出了AudioGen-Omni——一种基于多模式扩散变压器（MMDit）的统一方法，能够生成与输入视频同步的高保真音频、语音和歌曲。AudioGen-Omni引入了一种新的联合训练模式，该模式无缝集成了大规模的视频-文本-音频语料库，使模型能够在多模式输入条件下生成语义丰富、声音多样的音频，并适应广泛的音频生成任务。AudioGen-Omni采用统一的歌词-转录编码器，将歌曲和口语输入中的字母和音素编码成密集的帧级表示。密集的帧级表示通过使用基于AdaLN的联合注意力机制进行融合，增强阶段对齐的定向位置注入（PAAPI），其中RoPE被选择性应用于临时结构模态，以确保精确和稳健的跨模态对齐。通过解冻所有模态并屏蔽缺失的输入，AudioGen-Omni减轻了文本冻结模式的语义约束，实现了有效的跨模态条件设置。这种联合训练方法提高了音频质量、语义对齐和唇部同步准确性，同时在文本到音频&#x2F;语音&#x2F;歌曲任务上取得了最先进的成果。其推理时间为1.91秒可生成8秒的音频，在效率和通用性方面都有显著提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00733v4">PDF</a> 12 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>基于多模态扩散变压器（MMDit）的AudioGen-Omni统一方法，能够生成与输入视频同步的高保真音频、语音和歌曲。它通过无缝集成大规模视频-文本-音频语料库，实现了一种能够基于多模态输入生成语义丰富、声音多样的音频的模型，并适应广泛的音频生成任务。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AudioGen-Omni是一种基于多模态扩散变压器（MMDit）的统一方法，能够生成高保真音频、语音和歌曲，与输入视频同步。</li>
<li>引入了一种新型联合训练范式，集成大规模视频-文本-音频语料库。</li>
<li>通过统一歌词-转录编码器，将歌曲和口语的字母和音素编码为密集帧级表示。</li>
<li>使用基于AdaLN的联合注意机制融合了密集帧级表示，增强了相位对齐的异构定位灌注（PAAPI）。</li>
<li>RoPE选择性应用于具有时间结构的模态，实现精确和稳健的跨模态对齐。</li>
<li>通过解冻所有模态和掩盖缺失输入，缓解文本冻结范式的语义约束，实现有效的跨模态条件。</li>
<li>联合训练提高了音频质量、语义对齐和唇同步精度，并在文本到音频&#x2F;语音&#x2F;歌曲任务上实现最新结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00733">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a0e17d51526b5419f7d1bf9a792d51a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38e20067b86f9682aa8b7b2428f93191.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a86b4f217474a0d03ffcfc10bb25eabd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6b67d41d4c49685f7d28ce294f9f9f4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Overview-of-Automatic-Speech-Analysis-and-Technologies-for-Neurodegenerative-Disorders-Diagnosis-and-Assistive-Applications"><a href="#Overview-of-Automatic-Speech-Analysis-and-Technologies-for-Neurodegenerative-Disorders-Diagnosis-and-Assistive-Applications" class="headerlink" title="Overview of Automatic Speech Analysis and Technologies for   Neurodegenerative Disorders: Diagnosis and Assistive Applications"></a>Overview of Automatic Speech Analysis and Technologies for   Neurodegenerative Disorders: Diagnosis and Assistive Applications</h2><p><strong>Authors:Shakeel A. Sheikh, Md. Sahidullah, Ina Kodrasi</strong></p>
<p>Advancements in spoken language technologies for neurodegenerative speech disorders are crucial for meeting both clinical and technological needs. This overview paper is vital for advancing the field, as it presents a comprehensive review of state-of-the-art methods in pathological speech detection, automatic speech recognition, pathological speech intelligibility enhancement, intelligibility and severity assessment, and data augmentation approaches for pathological speech. It also highlights key challenges, such as ensuring robustness, privacy, and interpretability. The paper concludes by exploring promising future directions, including the adoption of multimodal approaches and the integration of large language models to further advance speech technologies for neurodegenerative speech disorders. </p>
<blockquote>
<p>口语技术在神经退行性疾病所致言语障碍方面的进展对于满足临床和技术需求至关重要。这篇综述论文对推进该领域具有重要意义，因为它全面回顾了病理性言语检测、自动语音识别、病理性言语清晰度增强、清晰度和严重程度评估以及病理性言语数据增强方法的最新方法。它还强调了确保稳健性、隐私性和可解释性等关键挑战。论文最后探讨了有前途的未来方向，包括采用多模式方法以及整合大型语言模型，以进一步推进神经退行性疾病所致言语障碍的口语技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.03536v2">PDF</a> Published in IEEE Journal of Selected Topics in Signal Processing</p>
<p><strong>Summary</strong><br>     这篇论文概述了神经退行性疾病语音技术的最新进展，涵盖了病理语音检测、自动语音识别、语音清晰度提升、语音清晰度和严重程度评估以及数据扩充等方面的先进方法。论文强调了关键挑战，如确保技术的稳健性、隐私性和可解释性，并探讨了未来采用多模式方法和整合大型语言模型等有望推动该领域进一步发展的方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文全面回顾了神经退行性疾病语音技术的最新进展。</li>
<li>介绍了包括病理语音检测、自动语音识别等方面的先进方法。</li>
<li>论文强调了提高技术稳健性、隐私性和可解释性的关键挑战。</li>
<li>论文指出未来发展方向包括采用多模式方法和整合大型语言模型。</li>
<li>该论文对于推动神经退行性疾病语音技术的发展具有重要意义。</li>
<li>论文涉及语音清晰度提升和语音清晰度和严重程度评估等方面的内容。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.03536">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e23e6ccc7af7fd30bcf432a8645d921e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b2ab1c5fc8bd3351f57bb54bae6803b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b588376378354e2e06fd025c1d511d5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef40ac70b1cfd27cbde8fa24d696f1c4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Recent-Advances-in-Speech-Language-Models-A-Survey"><a href="#Recent-Advances-in-Speech-Language-Models-A-Survey" class="headerlink" title="Recent Advances in Speech Language Models: A Survey"></a>Recent Advances in Speech Language Models: A Survey</h2><p><strong>Authors:Wenqian Cui, Dianzhi Yu, Xiaoqi Jiao, Ziqiao Meng, Guangyan Zhang, Qichao Wang, Yiwen Guo, Irwin King</strong></p>
<p>Large Language Models (LLMs) have recently garnered significant attention, primarily for their capabilities in text-based interactions. However, natural human interaction often relies on speech, necessitating a shift towards voice-based models. A straightforward approach to achieve this involves a pipeline of &#96;&#96;Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)”, where input speech is transcribed to text, processed by an LLM, and then converted back to speech. Despite being straightforward, this method suffers from inherent limitations, such as information loss during modality conversion, significant latency due to the complex pipeline, and error accumulation across the three stages. To address these issues, Speech Language Models (SpeechLMs) – end-to-end models that generate speech without converting from text – have emerged as a promising alternative. This survey paper provides the first comprehensive overview of recent methodologies for constructing SpeechLMs, detailing the key components of their architecture and the various training recipes integral to their development. Additionally, we systematically survey the various capabilities of SpeechLMs, categorize their evaluation metrics, and discuss the challenges and future research directions in this rapidly evolving field. The GitHub repository is available at <a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/Awesome-SpeechLM-Survey">https://github.com/dreamtheater123/Awesome-SpeechLM-Survey</a> </p>
<blockquote>
<p>大型语言模型（LLM）近期引起了广泛关注，主要因其基于文本的交互能力。然而，自然人机交互通常依赖于语音，因此需要使用语音模型进行转换。一种实现此目标的直接方法是通过“自动语音识别（ASR）+ LLM +文本到语音（TTS）”的流程，将输入语音转录为文本，由LLM进行处理，然后再转回语音。尽管这种方法很直接，但它存在固有的局限性，例如在模态转换过程中的信息丢失、复杂的管道带来的显著延迟以及在三个阶段中的错误累积。为了解决这些问题，语音语言模型（SpeechLM）出现了——一种能够在不转换为文本的情况下生成语音的端到端模型。这篇综述论文首次全面概述了构建SpeechLM的最新方法，详细介绍了其关键组件和对其开发至关重要的各种培训配方。此外，我们还系统地概述了SpeechLM的各种功能，对其评估指标进行了分类，并讨论了这一快速发展的领域的挑战和未来研究方向。GitHub仓库地址：<a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/Awesome-SpeechLM-Survey">https://github.com/dreamtheater123/Awesome-SpeechLM-Survey</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03751v4">PDF</a> The reduced version of this paper has been accepted at ACL 2025</p>
<p><strong>Summary</strong>：<br>大型语言模型（LLM）已受到广泛关注，但基于文本的交互方式忽略了自然人类交互通常依赖于语音的特性。因此，需要转向语音模型。尽管自动语音识别（ASR）+ LLM +文本转语音（TTS）方法可实现此目标，但存在信息损失、延迟和误差累积等问题。为解决这些问题，出现了语音语言模型（SpeechLMs）——无需从文本转换即可生成语音的端到端模型。本文首次全面概述了SpeechLMs的最新方法，详细介绍了其架构的关键组件和开发过程中不可或缺的训练食谱。此外，我们还系统地调查了SpeechLMs的各项功能、分类了评估指标，并讨论了这一快速发展领域的挑战和未来研究方向。GitHub仓库地址为：<a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/Awesome-SpeechLM-Survey%E3%80%82">https://github.com/dreamtheater123/Awesome-SpeechLM-Survey。</a></p>
<p><strong>Key Takeaways</strong>：</p>
<ul>
<li>大型语言模型（LLMs）在自然人机交互中需考虑语音交互的重要性。</li>
<li>语音语言模型（SpeechLMs）作为端到端模型，可直接生成语音，避免了信息损失和复杂管道带来的延迟问题。</li>
<li>SpeechLMs的关键组件包括架构设计和训练食谱，其中涉及多种技术和方法。</li>
<li>SpeechLMs具有多种功能，如语音识别、语音合成、对话系统等，其评估指标包括准确性、自然度等。</li>
<li>目前SpeechLMs面临挑战，如数据标注、模型规模与效率平衡等，并需要未来进一步研究和改进。</li>
<li>读者可以通过访问GitHub仓库（<a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/Awesome-SpeechLM-Survey%EF%BC%89%E8%8E%B7%E5%8F%96%E6%9B%B4%E5%A4%9A%E5%85%B3%E4%BA%8ESpeechLMs%E7%9A%84%E8%AF%A6%E7%BB%86%E4%BF%A1%E5%A6%82%E5%92%8C%E7%A0%94%E7%A9%B6%E8%B5%84%E6%BA%90%E3%80%82">https://github.com/dreamtheater123/Awesome-SpeechLM-Survey）获取更多关于SpeechLMs的详细信息和研究资源。</a></li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03751">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c5110266f7ef8903ead538b6556ebf9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69237885977210d8682d6c111e5c67bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14b065330e5f9dea5cf648faf1e1cca7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c23e2f399ff886f471c12d7cc5392a15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65d1e068eda2867fe18e5d22ad33f55c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-737e22c8d6590a815288fd6b5a40fe0d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-03f5f5537bee7dae18f1588e3ac0bdfd.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="WhisperNER-Unified-Open-Named-Entity-and-Speech-Recognition"><a href="#WhisperNER-Unified-Open-Named-Entity-and-Speech-Recognition" class="headerlink" title="WhisperNER: Unified Open Named Entity and Speech Recognition"></a>WhisperNER: Unified Open Named Entity and Speech Recognition</h2><p><strong>Authors:Gil Ayache, Menachem Pirchi, Aviv Navon, Aviv Shamsian, Gill Hetz, Joseph Keshet</strong></p>
<p>Integrating named entity recognition (NER) with automatic speech recognition (ASR) can significantly enhance transcription accuracy and informativeness. In this paper, we introduce WhisperNER, a novel model that allows joint speech transcription and entity recognition. WhisperNER supports open-type NER, enabling recognition of diverse and evolving entities at inference. Building on recent advancements in open NER research, we augment a large synthetic dataset with synthetic speech samples. This allows us to train WhisperNER on a large number of examples with diverse NER tags. During training, the model is prompted with NER labels and optimized to output the transcribed utterance along with the corresponding tagged entities. To evaluate WhisperNER, we generate synthetic speech for commonly used NER benchmarks and annotate existing ASR datasets with open NER tags. Our experiments demonstrate that WhisperNER outperforms natural baselines on both out-of-domain open type NER and supervised finetuning. </p>
<blockquote>
<p>将命名实体识别（NER）与自动语音识别（ASR）相结合，可以显著提高转录准确性和信息量。在本文中，我们介绍了WhisperNER，这是一种允许联合语音转录和实体识别的新型模型。WhisperNER支持开放型NER，能够在推理过程中识别多样且不断发展的实体。基于最新的开放型NER研究，我们通过合成语音样本扩充了一个大型合成数据集。这使得我们能够在大量的带有不同NER标签的样本上训练WhisperNER。在训练过程中，模型会提示NER标签并进行优化，以输出转录的话语和相应的标记实体。为了评估WhisperNER的性能，我们为常用的NER基准生成合成语音，并使用开放型NER标签标注现有的ASR数据集。我们的实验表明，无论是在离域开放型NER还是监督微调上，WhisperNER的表现都优于自然基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08107v2">PDF</a> ASRU 2025, IEEE</p>
<p><strong>Summary</strong></p>
<p>融合命名实体识别（NER）与自动语音识别（ASR）可有效提升转录准确性和信息量。本文介绍了一种新型模型WhisperNER，它可联合进行语音转录和实体识别。WhisperNER支持开放型NER，可在推理过程中识别多样且不断发展的实体。基于最新的开放型NER研究成果，我们利用合成的大型数据集与合成语音样本进行训练，使WhisperNER能够在大量样本上训练，并涵盖多样的NER标签。在训练过程中，模型通过NER标签进行提示，并优化输出带相应标签实体的转录语音。为评估WhisperNER性能，我们为常用的NER基准测试生成合成语音，并对现有的ASR数据集使用开放型NER标签进行标注。实验表明，WhisperNER在开放型NER和经过监督微调的任务上均优于自然基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>整合命名实体识别（NER）与自动语音识别（ASR）能显著提升转录准确性和信息量。</li>
<li>WhisperNER是一种新型模型，可同时实现语音转录和实体识别。</li>
<li>WhisperNER支持开放型NER，能够识别多样且不断发展的实体。</li>
<li>利用合成的大型数据集与合成语音样本进行训练，增强WhisperNER模型的泛化能力。</li>
<li>模型在训练过程中通过NER标签提示，并优化输出带相应标签实体的转录语音。</li>
<li>合成语音生成和现有ASR数据集的开放型NER标签标注方法用于评估WhisperNER性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08107">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e4e388a638db2ea5df73d309f077e88d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20a610eee62a9fc541ad10276c379572.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acc00d00cef937cb5e43f2d9eabe12c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23937fc617e55f95757d55401869fc98.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bda470a50c548e1b7cb2371edd1d66ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19abfbf0206eaebcfd8ad5eb6ebd31b2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e3774b1aa612e2602278077444b4dd76.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c8bee61fbd138c40ce5dfeed7b6a32d4.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-08-09  FLUX-Makeup High-Fidelity, Identity-Consistent, and Robust Makeup   Transfer via Diffusion Transformer
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0a0e2ac850fa80c8b33071e7b1656377.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-08-09  UNCAGE Contrastive Attention Guidance for Masked Generative   Transformers in Text-to-Image Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
