<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS 方向最新论文已更新，请持续关注 Update in 2025-08-09  A Scalable Pipeline for Enabling Non-Verbal Speech Generation and   Understanding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-65d1e068eda2867fe18e5d22ad33f55c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    21 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-09-更新"><a href="#2025-08-09-更新" class="headerlink" title="2025-08-09 更新"></a>2025-08-09 更新</h1><h2 id="A-Scalable-Pipeline-for-Enabling-Non-Verbal-Speech-Generation-and-Understanding"><a href="#A-Scalable-Pipeline-for-Enabling-Non-Verbal-Speech-Generation-and-Understanding" class="headerlink" title="A Scalable Pipeline for Enabling Non-Verbal Speech Generation and   Understanding"></a>A Scalable Pipeline for Enabling Non-Verbal Speech Generation and   Understanding</h2><p><strong>Authors:Runchuan Ye, Yixuan Zhou, Renjie Yu, Zijian Lin, Kehan Li, Xiang Li, Xin Liu, Guoyang Zeng, Zhiyong Wu</strong></p>
<p>Human spoken communication involves not only lexical content but also non-verbal vocalizations (NVs) such as laughter, sighs, and coughs, which convey emotions, intentions, and social signals. However, most existing speech systems focus solely on verbal content and lack the ability to understand and generate such non-verbal cues, reducing the emotional intelligence and communicative richness of spoken interfaces. In this work, we introduce $\textbf{NonVerbalSpeech-38K}$, a large and diverse dataset for non-verbal speech generation and understanding, collected from real-world media and annotated using an automatic pipeline. The dataset contains 38,718 samples (about 131 hours) with 10 categories of non-verbal cues, such as laughter, sniff, and throat clearing. We further validate the dataset by fine-tuning state-of-the-art models, including F5-TTS and Qwen2-Audio, demonstrating its effectiveness in non-verbal speech generation and understanding tasks. Our contributions are threefold: (1) We propose a practical pipeline for building natural and diverse non-verbal speech datasets; (2) We release a large-scale dataset to advance research on non-verbal speech generation and understanding; (3) We validate the dataset’s effectiveness by demonstrating improvements in both non-verbal speech synthesis and captioning, thereby facilitating richer human-computer interaction. </p>
<blockquote>
<p>人类口语交流不仅包括词汇内容，还包括非语言性的发声（NVs），如笑声、叹息声和咳嗽声，这些声音传达情感、意图和社会信号。然而，现有的大多数语音系统只关注语言内容，缺乏理解和生成非语言线索的能力，降低了语音界面的情感智能和沟通丰富性。在这项工作中，我们介绍了<strong>NonVerbalSpeech-38K</strong>，这是一个用于非语言语音生成和理解的大型多样化数据集，数据来源于真实媒体并自动进行标注处理。该数据集包含38718个样本（约131小时），包含笑声、鼻塞和清嗓等十类非语言线索。我们进一步使用最新技术模型精细调整验证数据集，包括F5-TTS和Qwen2-Audio等，在非语言语音生成和理解任务中表现出良好的有效性。我们的贡献有三点：（1）我们提出了构建自然和非语言语音数据集的实用流程；（2）我们发布大规模数据集以促进非语言语音生成和理解的研究；（3）我们通过演示非语言语音合成和字幕中的改进验证了数据集的有效性，从而促进了更丰富的人机交互。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05385v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个名为NonVerbalSpeech-38K的大型非语言语音数据集，包含来自真实世界的媒体数据，涉及笑声、叹气等十种非语言线索，共计38,718个样本。通过改进现有模型验证了数据集的有效性，可用于非语言语音生成和理解任务，促进更丰富的人机交互。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人类口头交流不仅包括词汇内容，还包括如笑声、叹息和咳嗽等非语言声音（NVs），它们传递情感、意图和社会信号。</li>
<li>现存的语音系统主要关注语言内容，无法理解和生成非语言线索，导致语音界面缺乏情感智能和丰富的交流性。</li>
<li>引入的大型数据集NonVerbalSpeech-38K用于非语言语音生成和理解，包含真实世界媒体采集的约十万个样本，包括笑声等十类非语言线索。</li>
<li>数据集通过改进最新模型进行验证，证明了其在非语言语音生成和理解任务中的有效性。</li>
<li>数据集的构建涉及一个实用的管道流程，旨在实现自然和多样化的非语言语音数据集建设。</li>
<li>数据集的发布推动了非语言语音生成和理解领域的研究进展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05385">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3e65bb294c58c1381f0b2c4c2ebf0acd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27da7f4c6641fd1d5fa6f134c58c4a7d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-47aedca82433c15c27ff4bd0c0696b82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3293dd5b3251b7cc90fe9a6c3b84ee11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae8a3a29f4ab6c13e864a71bd019c3fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de6af782a34c4f555462b52d7a307b8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ac8df6cc5deeab380eef5fd7f1eea93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24038e5b32abdbe09faf0648ec8c9821.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Fairness-in-Dysarthric-Speech-Synthesis-Understanding-Intrinsic-Bias-in-Dysarthric-Speech-Cloning-using-F5-TTS"><a href="#Fairness-in-Dysarthric-Speech-Synthesis-Understanding-Intrinsic-Bias-in-Dysarthric-Speech-Cloning-using-F5-TTS" class="headerlink" title="Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in   Dysarthric Speech Cloning using F5-TTS"></a>Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in   Dysarthric Speech Cloning using F5-TTS</h2><p><strong>Authors:Anuprabha M, Krishna Gurugubelli, Anil Kumar Vuppala</strong></p>
<p>Dysarthric speech poses significant challenges in developing assistive technologies, primarily due to the limited availability of data. Recent advances in neural speech synthesis, especially zero-shot voice cloning, facilitate synthetic speech generation for data augmentation; however, they may introduce biases towards dysarthric speech. In this paper, we investigate the effectiveness of state-of-the-art F5-TTS in cloning dysarthric speech using TORGO dataset, focusing on intelligibility, speaker similarity, and prosody preservation. We also analyze potential biases using fairness metrics like Disparate Impact and Parity Difference to assess disparities across dysarthric severity levels. Results show that F5-TTS exhibits a strong bias toward speech intelligibility over speaker and prosody preservation in dysarthric speech synthesis. Insights from this study can help integrate fairness-aware dysarthric speech synthesis, fostering the advancement of more inclusive speech technologies. </p>
<blockquote>
<p>言语障碍者在辅助技术开发方面面临重大挑战，这主要是因为数据有限。神经网络语音合成的最新进展，尤其是零样本语音克隆，促进了数据增强中的合成语音生成；然而，它们可能对言语障碍语音产生偏见。在本文中，我们使用TORGO数据集研究最先进的F5-TTS在克隆言语障碍语音方面的有效性，重点关注可理解性、演讲者相似性和韵律保持。我们还使用如不同影响的差距和公平性差异等公平性指标，分析潜在的偏见，以评估不同言语障碍严重程度之间的差异。结果表明，在言语障碍语音合成中，F5-TTS更偏向于注重语音可理解性，而非演讲者和韵律的保持。本研究的见解有助于融入具有公平意识的言语障碍语音合成，推动更具包容性的语音技术的进步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05102v1">PDF</a> Accepted at Interspeech 2025</p>
<p><strong>摘要</strong></p>
<p>本文主要探讨了神经网络语音合成在模仿患有构音障碍人士的语音时的效能和潜在偏见问题。针对当前构音障碍语音数据的缺乏，本文使用最新的神经语音合成技术（特别是零样本语音克隆技术）进行数据增强，同时重点考察了模型的性能。通过对TORGO数据集的分析，主要关注了语音清晰度、说话人相似度和语调保持三个维度。此外，本文还通过公平性的指标如差异影响和公平差异来评估不同构音障碍程度之间的差异。研究结果表明，F5-TTS在合成构音障碍语音时更偏向于语音清晰度而非说话人和语调的保持。本文的见解有助于开发更包容的语音技术，促进公平性感知构音障碍语音合成的整合。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>神经网络语音合成在模仿构音障碍人士的语音方面面临挑战，主要原因在于相关数据的稀缺性。</li>
<li>最新的零样本语音克隆技术可用于数据增强并推进此项研究的进展。</li>
<li>对TORGO数据集的分析，涉及了语音清晰度、说话人相似性和语调保持等关键性能指标的考察。</li>
<li>F5-TTS模型在合成构音障碍语音时倾向于保证语音清晰度而非说话人的声音和语调的模仿。</li>
<li>通过差异影响和公平差异等公平性指标，发现了模型在不同构音障碍程度间的差异和潜在偏见。</li>
<li>研究结果强调了开发更包容的语音技术的重要性，以促进公平性感知构音障碍语音合成的整合。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05102">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-04ee7b1ac77850514d0481d1c59233e1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-11cb965df036a539d539002d06657872.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d352577f01a896095cd9abf96721ba8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f31d6b4e4c9415f9ea9912a565e405e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19be485cd9d2cd3196c29abf4624f25b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbaf8b862f267b0ecb30dd70a751e25d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-595a3f2489e08dd204c7dc0d37291d91.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="UniTalker-Conversational-Speech-Visual-Synthesis"><a href="#UniTalker-Conversational-Speech-Visual-Synthesis" class="headerlink" title="UniTalker: Conversational Speech-Visual Synthesis"></a>UniTalker: Conversational Speech-Visual Synthesis</h2><p><strong>Authors:Yifan Hu, Rui Liu, Yi Ren, Xiang Yin, Haizhou Li</strong></p>
<p>Conversational Speech Synthesis (CSS) is a key task in the user-agent interaction area, aiming to generate more expressive and empathetic speech for users. However, it is well-known that “listening” and “eye contact” play crucial roles in conveying emotions during real-world interpersonal communication. Existing CSS research is limited to perceiving only text and speech within the dialogue context, which restricts its effectiveness. Moreover, speech-only responses further constrain the interactive experience. To address these limitations, we introduce a Conversational Speech-Visual Synthesis (CSVS) task as an extension of traditional CSS. By leveraging multimodal dialogue context, it provides users with coherent audiovisual responses. To this end, we develop a CSVS system named UniTalker, which is a unified model that seamlessly integrates multimodal perception and multimodal rendering capabilities. Specifically, it leverages a large-scale language model to comprehensively understand multimodal cues in the dialogue context, including speaker, text, speech, and the talking-face animations. After that, it employs multi-task sequence prediction to first infer the target utterance’s emotion and then generate empathetic speech and natural talking-face animations. To ensure that the generated speech-visual content remains consistent in terms of emotion, content, and duration, we introduce three key optimizations: 1) Designing a specialized neural landmark codec to tokenize and reconstruct facial expression sequences. 2) Proposing a bimodal speech-visual hard alignment decoding strategy. 3) Applying emotion-guided rendering during the generation stage. Comprehensive objective and subjective experiments demonstrate that our model synthesizes more empathetic speech and provides users with more natural and emotionally consistent talking-face animations. </p>
<blockquote>
<p>对话语音合成（CSS）是用户代理交互领域的一项关键任务，旨在为用户生成更具表现力和同情心的语音。然而，众所周知，“倾听”和“眼神交流”在现实世界的人际交流过程中对于传达情感起着至关重要的作用。现有的CSS研究仅限于感知对话上下文中的文本和语音，这限制了其有效性。此外，仅语音响应进一步限制了交互体验。为了解决这些局限性，我们引入了对话语音视觉合成（CSVS）任务，作为传统CSS的扩展。通过利用多模式对话上下文，它为用户提供连贯的视听响应。为此，我们开发了一个名为UniTalker的CSVS系统，这是一个统一模型，能够无缝集成多模式感知和多模式渲染功能。具体来说，它利用大规模语言模型全面理解对话上下文中的多模式线索，包括说话者、文本、语音和说话面部动画。之后，它采用多任务序列预测，首先推断目标话语的情感，然后生成同情的语音和自然的说话面部动画。为了确保生成的语音视觉内容在情感、内容和持续时间方面保持一致，我们引入了三个关键优化：1）设计专门的神经地标编解码器来标记和重建面部表情序列。2）提出了一种双模态语音视觉硬对齐解码策略。3）在生成阶段应用情感引导渲染。全面的客观和主观实验表明，我们的模型合成更富有同情心的语音，为用户提供更自然、情感更一致的说话面部动画。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04585v2">PDF</a> 15 pages, 8 figures, Accepted by ACM MM 2025</p>
<p><strong>摘要</strong></p>
<p>对话语音合成（CSS）是用户代理交互领域的一项关键任务，旨在为用户生成更具表现力和同情心的语音。然而，在现实世界的人际交流中，情感和情绪的表达不仅仅依赖于语音和文字对话。现有的CSS研究局限于对话语境中的文本和语音感知，这限制了其有效性。此外，仅依赖语音的响应也限制了交互体验。为了克服这些局限性，我们提出了对话语音视觉合成（CSVS）任务作为传统CSS的扩展。通过利用多模态对话语境，它为用户提供连贯的视听响应。为此，我们开发了一个名为UniTalker的CSVS系统，这是一个统一的模型，能够无缝集成多模态感知和多模态渲染功能。该系统利用大规模语言模型全面理解对话语境中的多模态线索，包括说话者、文本、语音和说话面部动画。然后采用多任务序列预测，首先推断目标话语的情感，然后生成富有同情心的语音和自然流畅的说话面部动画。为了确保生成的语音视觉内容在情感、内容和持续时间方面保持一致，我们引入了三项关键优化措施：设计专门的神经地标编解码器来标记和重建面部表情序列；提出双模态语音视觉硬对齐解码策略；在生成阶段应用情感引导渲染。全面的客观和主观实验表明，我们的模型合成的语音更具同情心，并能为用户提供更自然、情感一致的说话面部动画响应。</p>
<p><strong>要点摘要</strong></p>
<ol>
<li>对话语音合成（CSS）旨在生成更表现力和同情心的语音，但局限于文本和语音感知，影响效果。</li>
<li>提出对话语音视觉合成（CSVS）任务，结合多模态对话语境，提供更连贯的视听响应。</li>
<li>开发CSVS系统UniTalker，整合多模态感知和多模态渲染，全面理解对话语境中的多模态线索。</li>
<li>利用多任务序列预测生成富有同情心的语音和自然的说话面部动画，确保语音视觉内容在情感、内容和持续时间上保持一致。</li>
<li>引入三项关键优化措施：神经地标编解码器、双模态语音视觉硬对齐解码策略和情感引导渲染。</li>
<li>实验证明，UniTalker合成的语音更富有同情心，提供的说话面部动画更自然和情感一致。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04585">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-eb8e5e587760e62dfaba13ab3f15fa9d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1e02f7ab8892440927e9bf107161e285.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d32a809674613a629f8b9619776632f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5c154235d7d888fa8cc9c2a3d1a7164.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8764ee499313c28aa142b7cfc64a952c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ZipVoice-Fast-and-High-Quality-Zero-Shot-Text-to-Speech-with-Flow-Matching"><a href="#ZipVoice-Fast-and-High-Quality-Zero-Shot-Text-to-Speech-with-Flow-Matching" class="headerlink" title="ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow   Matching"></a>ZipVoice: Fast and High-Quality Zero-Shot Text-to-Speech with Flow   Matching</h2><p><strong>Authors:Han Zhu, Wei Kang, Zengwei Yao, Liyong Guo, Fangjun Kuang, Zhaoqing Li, Weiji Zhuang, Long Lin, Daniel Povey</strong></p>
<p>Existing large-scale zero-shot text-to-speech (TTS) models deliver high speech quality but suffer from slow inference speeds due to massive parameters. To address this issue, this paper introduces ZipVoice, a high-quality flow-matching-based zero-shot TTS model with a compact model size and fast inference speed. Key designs include: 1) a Zipformer-based vector field estimator to maintain adequate modeling capabilities under constrained size; 2) Average upsampling-based initial speech-text alignment and Zipformer-based text encoder to improve speech intelligibility; 3) A flow distillation method to reduce sampling steps and eliminate the inference overhead associated with classifier-free guidance. Experiments on 100k hours multilingual datasets show that ZipVoice matches state-of-the-art models in speech quality, while being 3 times smaller and up to 30 times faster than a DiT-based flow-matching baseline. Codes, model checkpoints and demo samples are publicly available at <a target="_blank" rel="noopener" href="https://github.com/k2-fsa/ZipVoice">https://github.com/k2-fsa/ZipVoice</a>. </p>
<blockquote>
<p>现有的大规模零样本文本到语音（TTS）模型虽然能够提供高质量的语音，但由于参数庞大，导致推理速度较慢。针对这一问题，本文引入了ZipVoice，这是一个基于高质量流匹配的零样本TTS模型，具有较小的模型尺寸和快速的推理速度。关键设计包括：1）基于Zipformer的向量场估计器，在受限大小下保持足够的建模能力；2）基于平均上采样的初始语音文本对齐和基于Zipformer的文本编码器，以提高语音的可懂度；3）流蒸馏方法用于减少采样步骤，消除与无分类器引导相关的推理开销。在100k小时的多语种数据集上的实验表明，ZipVoice在语音质量方面与最新模型相匹配，同时比基于DiT的流匹配基准模型小3倍，速度快达30倍。代码、模型检查点和演示样本可在<a target="_blank" rel="noopener" href="https://github.com/k2-fsa/ZipVoice%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/k2-fsa/ZipVoice上公开获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.13053v3">PDF</a> Accepted in ASRU 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于流匹配的零样本文本转语音（TTS）模型ZipVoice，旨在解决现有大规模零样本TTS模型参数庞大、推理速度慢的问题。ZipVoice模型设计精巧，包括使用Zipformer向量场估计器在有限大小下保持建模能力、基于平均上采样的初始语音文本对齐和Zipformer文本编码器提高语音清晰度，以及流蒸馏方法减少采样步骤，消除无分类引导相关的推理开销。实验表明，ZipVoice在语音质量上达到最新水平，模型体积为其他模型的十分之一不到，推理速度最快可达三十倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ZipVoice是一个基于流匹配的零样本TTS模型，旨在解决大规模TTS模型推理速度慢的问题。</li>
<li>ZipVoice设计包括Zipformer向量场估计器、基于平均上采样的文本语音对齐和Zipformer文本编码器。</li>
<li>流蒸馏方法用于减少采样步骤和消除推理开销。</li>
<li>ZipVoice在语音质量上达到最新水平，同时模型体积更小，推理速度更快。</li>
<li>ZipVoice模型可通过公开的代码库和模型检查点进行访问和使用。</li>
<li>模型性能的提升是通过一系列技术改进实现的，包括模型结构设计、优化算法等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.13053">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fa4223eec2e1d9b40695f39e0690dece.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f15dafc25012a83d58bda7f03826953f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cfdbc612dea4fbf3eaf311bde278b22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ee4bd98e7aaf076640d33295ac9dbb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8aa19b6b659827edaec6dd49ff5b4b7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96791722498d4358fdc8771c5009978d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f425c862ce6e7a1120300329d9d83632.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Recent-Advances-in-Speech-Language-Models-A-Survey"><a href="#Recent-Advances-in-Speech-Language-Models-A-Survey" class="headerlink" title="Recent Advances in Speech Language Models: A Survey"></a>Recent Advances in Speech Language Models: A Survey</h2><p><strong>Authors:Wenqian Cui, Dianzhi Yu, Xiaoqi Jiao, Ziqiao Meng, Guangyan Zhang, Qichao Wang, Yiwen Guo, Irwin King</strong></p>
<p>Large Language Models (LLMs) have recently garnered significant attention, primarily for their capabilities in text-based interactions. However, natural human interaction often relies on speech, necessitating a shift towards voice-based models. A straightforward approach to achieve this involves a pipeline of &#96;&#96;Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)”, where input speech is transcribed to text, processed by an LLM, and then converted back to speech. Despite being straightforward, this method suffers from inherent limitations, such as information loss during modality conversion, significant latency due to the complex pipeline, and error accumulation across the three stages. To address these issues, Speech Language Models (SpeechLMs) – end-to-end models that generate speech without converting from text – have emerged as a promising alternative. This survey paper provides the first comprehensive overview of recent methodologies for constructing SpeechLMs, detailing the key components of their architecture and the various training recipes integral to their development. Additionally, we systematically survey the various capabilities of SpeechLMs, categorize their evaluation metrics, and discuss the challenges and future research directions in this rapidly evolving field. The GitHub repository is available at <a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/Awesome-SpeechLM-Survey">https://github.com/dreamtheater123/Awesome-SpeechLM-Survey</a> </p>
<blockquote>
<p>大型语言模型（LLM）近期备受关注，主要因其基于文本的交互能力。然而，自然人类交互通常依赖于语音，因此需要向基于语音的模型转变。实现这一目标的一种直接方法是“自动语音识别（ASR）+ LLM + 文本到语音（TTS）”的管道，其中输入语音被转录为文本，由LLM处理，然后再转回语音。尽管这种方法很直接，但它存在固有的局限性，例如在模态转换过程中的信息损失、由于复杂管道而产生的显著延迟以及三个阶段的错误累积。为解决这些问题，语音语言模型（SpeechLMs）——无需从文本转换即可生成语音的端到端模型——作为有前途的替代方案而出现。这篇综述论文首次全面概述了构建SpeechLMs的最新方法，详细描述了其架构的关键组件以及对其发展至关重要的各种训练配方。此外，我们还系统地概述了SpeechLMs的各种功能，对其评估指标进行分类，并讨论了这一快速演变领域面临的挑战和未来研究方向。GitHub仓库可通过<a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/Awesome-SpeechLM-Survey%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/dreamtheater123/Awesome-SpeechLM-Survey访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03751v4">PDF</a> The reduced version of this paper has been accepted at ACL 2025</p>
<p><strong>Summary</strong><br>大语言模型因其文本交互能力而受到广泛关注，但自然人类互动更多地依赖于语音。为实现语音交互，一种常见的方法是结合自动语音识别（ASR）、大型语言模型（LLM）和文本到语音（TTS）。然而，这种方法存在信息损失、延迟和误差累积的问题。为解决这些问题，出现了一种直接生成语音的端到端语音语言模型（SpeechLM）。这篇综述论文首次全面概述了SpeechLM的最新构建方法、关键组件、训练方法和开发策略。论文还系统地调查了SpeechLM的各种能力，对其评估指标进行了分类，并讨论了这一快速演进领域的挑战和未来研究方向。GitHub仓库地址：<a target="_blank" rel="noopener" href="https://github.com/dreamtheater123/Awesome-SpeechLM-Survey">https://github.com/dreamtheater123/Awesome-SpeechLM-Survey</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大语言模型在自然人机交互中扮演重要角色，但需要解决语音交互的问题。</li>
<li>结合ASR、LLM和TTS的常见方法存在固有的局限性。</li>
<li>语音语言模型（SpeechLM）是这些问题的一个有前途的替代解决方案，可直接生成语音而无需文本转换。</li>
<li>SpeechLM具有多种关键组件和训练策略，用于构建和优化模型性能。</li>
<li>SpeechLM具有多种能力，包括语音识别、语音合成和自然性对话等。</li>
<li>对SpeechLM的评价采用多种指标进行分类，用于系统评价模型性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03751">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c5110266f7ef8903ead538b6556ebf9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69237885977210d8682d6c111e5c67bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14b065330e5f9dea5cf648faf1e1cca7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c23e2f399ff886f471c12d7cc5392a15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65d1e068eda2867fe18e5d22ad33f55c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-737e22c8d6590a815288fd6b5a40fe0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03f5f5537bee7dae18f1588e3ac0bdfd.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-778f2362210e8c90039700d5422b2854.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-08-09  Decision-Making with Deliberation Meta-reviewing as a Document-grounded   Dialogue
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-c4f3381c9ca1a8d2d3266d998931c981.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-08-09  MM2CT MR-to-CT translation for multi-modal image fusion with mamba
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
