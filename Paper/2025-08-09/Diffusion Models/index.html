<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-09  GAP Gaussianize Any Point Clouds with Text Guidance">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-2d20e35a5c763131af23218456bdb7ac.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    52 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-09-æ›´æ–°"><a href="#2025-08-09-æ›´æ–°" class="headerlink" title="2025-08-09 æ›´æ–°"></a>2025-08-09 æ›´æ–°</h1><h2 id="GAP-Gaussianize-Any-Point-Clouds-with-Text-Guidance"><a href="#GAP-Gaussianize-Any-Point-Clouds-with-Text-Guidance" class="headerlink" title="GAP: Gaussianize Any Point Clouds with Text Guidance"></a>GAP: Gaussianize Any Point Clouds with Text Guidance</h2><p><strong>Authors:Weiqi Zhang, Junsheng Zhou, Haotian Geng, Wenyuan Zhang, Yu-Shen Liu</strong></p>
<p>3D Gaussian Splatting (3DGS) has demonstrated its advantages in achieving fast and high-quality rendering. As point clouds serve as a widely-used and easily accessible form of 3D representation, bridging the gap between point clouds and Gaussians becomes increasingly important. Recent studies have explored how to convert the colored points into Gaussians, but directly generating Gaussians from colorless 3D point clouds remains an unsolved challenge. In this paper, we propose GAP, a novel approach that gaussianizes raw point clouds into high-fidelity 3D Gaussians with text guidance. Our key idea is to design a multi-view optimization framework that leverages a depth-aware image diffusion model to synthesize consistent appearances across different viewpoints. To ensure geometric accuracy, we introduce a surface-anchoring mechanism that effectively constrains Gaussians to lie on the surfaces of 3D shapes during optimization. Furthermore, GAP incorporates a diffuse-based inpainting strategy that specifically targets at completing hard-to-observe regions. We evaluate GAP on the Point-to-Gaussian generation task across varying complexity levels, from synthetic point clouds to challenging real-world scans, and even large-scale scenes. Project Page: <a target="_blank" rel="noopener" href="https://weiqi-zhang.github.io/GAP">https://weiqi-zhang.github.io/GAP</a>. </p>
<blockquote>
<p>3Dé«˜æ–¯å–·æº…ï¼ˆ3DGSï¼‰å·²ç»æ˜¾ç¤ºå‡ºå…¶åœ¨å®ç°å¿«é€Ÿé«˜è´¨é‡æ¸²æŸ“æ–¹é¢çš„ä¼˜åŠ¿ã€‚ç”±äºç‚¹äº‘ä½œä¸ºå¹¿æ³›ä½¿ç”¨å’Œæ˜“äºè·å–çš„ä¸‰ç»´è¡¨ç¤ºå½¢å¼ï¼Œå¼¥ç‚¹äº‘å’Œé«˜æ–¯ä¹‹é—´çš„å·®è·å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚è¿‘æ¥çš„ç ”ç©¶å·²ç»æ¢ç´¢äº†å¦‚ä½•å°†å½©è‰²ç‚¹è½¬æ¢ä¸ºé«˜æ–¯ï¼Œä½†ä»æ— é¢œè‰²çš„ä¸‰ç»´ç‚¹äº‘ä¸­ç›´æ¥ç”Ÿæˆé«˜æ–¯ä»æ˜¯ä¸€ä¸ªæœªè§£å†³çš„éš¾é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•GAPï¼Œé€šè¿‡æ–‡æœ¬æŒ‡å¯¼å°†åŸå§‹ç‚¹äº‘è½¬åŒ–ä¸ºé«˜ä¿çœŸä¸‰ç»´é«˜æ–¯ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³è®¾è®¡äº†ä¸€ä¸ªå¤šè§†è§’ä¼˜åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨æ·±åº¦æ„ŸçŸ¥å›¾åƒæ‰©æ•£æ¨¡å‹æ¥åˆæˆä¸åŒè§†è§’ä¸‹çš„è¿ç»­å¤–è§‚ã€‚ä¸ºç¡®ä¿å‡ ä½•ç²¾åº¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¡¨é¢é”šå®šæœºåˆ¶ï¼Œæœ‰æ•ˆåœ°çº¦æŸé«˜æ–¯åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ä½äºä¸‰ç»´å½¢çŠ¶çš„è¡¨é¢ä¸Šã€‚æ­¤å¤–ï¼ŒGAPè¿˜èå…¥äº†åŸºäºæ‰©æ•£çš„å¡«å……ç­–ç•¥ï¼Œç‰¹åˆ«ä¸“æ³¨äºå®Œæˆéš¾ä»¥è§‚å¯Ÿçš„åŒºåŸŸã€‚æˆ‘ä»¬åœ¨ç‚¹äº‘åˆ°é«˜æ–¯ç”Ÿæˆä»»åŠ¡ä¸Šè¯„ä¼°äº†GAPï¼Œæ¶µç›–äº†ä¸åŒå¤æ‚åº¦çº§åˆ«ï¼Œä»åˆæˆç‚¹äº‘åˆ°å…·æœ‰æŒ‘æˆ˜æ€§çš„çœŸå®ä¸–ç•Œæ‰«æï¼Œç”šè‡³å¤§è§„æ¨¡åœºæ™¯ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://weiqi-zhang.github.io/GAP%E3%80%82">https://weiqi-zhang.github.io/GAPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05631v1">PDF</a> ICCV 2025. Project page: <a target="_blank" rel="noopener" href="https://weiqi-zhang.github.io/GAP">https://weiqi-zhang.github.io/GAP</a></p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åä¸ºGAPçš„æ–°æ–¹æ³•ï¼Œå¯å°†åŸå§‹ç‚¹äº‘è½¬åŒ–ä¸ºé«˜ä¿çœŸåº¦3Dé«˜æ–¯åˆ†å¸ƒï¼Œå¹¶å¼•å…¥æ–‡æœ¬æŒ‡å¯¼ã€‚é€šè¿‡è®¾è®¡å¤šè§†è§’ä¼˜åŒ–æ¡†æ¶å¹¶åˆ©ç”¨æ·±åº¦æ„ŸçŸ¥å›¾åƒæ‰©æ•£æ¨¡å‹åˆæˆä¸åŒè§†è§’ä¸‹çš„å¤–è§‚ä¸€è‡´æ€§ï¼Œä¿è¯å‡ ä½•ç²¾åº¦ï¼Œè§£å†³ä»æ— è‰²å½©ç‚¹äº‘ç›´æ¥ç”Ÿæˆé«˜æ–¯åˆ†å¸ƒçš„éš¾é¢˜ã€‚åŒæ—¶ï¼ŒGAPé‡‡ç”¨åŸºäºæ‰©æ•£çš„å¡«å……ç­–ç•¥ï¼Œç‰¹åˆ«é’ˆå¯¹éš¾ä»¥è§‚æµ‹çš„åŒºåŸŸè¿›è¡Œè¡¥å…¨ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒGAPåœ¨ç‚¹äº‘åˆ°é«˜æ–¯ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œé€‚ç”¨äºä¸åŒå¤æ‚åº¦çš„åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GAPæ–¹æ³•èƒ½å°†åŸå§‹ç‚¹äº‘è½¬åŒ–ä¸ºé«˜ä¿çœŸåº¦3Dé«˜æ–¯åˆ†å¸ƒã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šè§†è§’ä¼˜åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨æ·±åº¦æ„ŸçŸ¥å›¾åƒæ‰©æ•£æ¨¡å‹åˆæˆä¸åŒè§†è§’ä¸‹çš„å¤–è§‚ä¸€è‡´æ€§ã€‚</li>
<li>é€šè¿‡å¼•å…¥è¡¨é¢é”šå®šæœºåˆ¶ï¼Œç¡®ä¿å‡ ä½•ç²¾åº¦å¹¶çº¦æŸé«˜æ–¯åœ¨3Då½¢çŠ¶è¡¨é¢ã€‚</li>
<li>GAPé‡‡ç”¨åŸºäºæ‰©æ•£çš„å¡«å……ç­–ç•¥ï¼Œé’ˆå¯¹éš¾ä»¥è§‚æµ‹çš„åŒºåŸŸè¿›è¡Œè¡¥å…¨ã€‚</li>
<li>GAPåœ¨ç‚¹äº‘åˆ°é«˜æ–¯ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œé€‚ç”¨äºä»åˆæˆç‚¹äº‘åˆ°çœŸå®ä¸–ç•Œæ‰«æç­‰å¤šç§åœºæ™¯ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºå¤§è§„æ¨¡åœºæ™¯åŒæ ·æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bbcee14eae3b7461df6d8051e911ecd8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efbdd389f59a1471b99c91361224584d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-833d1aa7f103783e2049dec955678fa3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="UNCAGE-Contrastive-Attention-Guidance-for-Masked-Generative-Transformers-in-Text-to-Image-Generation"><a href="#UNCAGE-Contrastive-Attention-Guidance-for-Masked-Generative-Transformers-in-Text-to-Image-Generation" class="headerlink" title="UNCAGE: Contrastive Attention Guidance for Masked Generative   Transformers in Text-to-Image Generation"></a>UNCAGE: Contrastive Attention Guidance for Masked Generative   Transformers in Text-to-Image Generation</h2><p><strong>Authors:Wonjun Kang, Byeongkeun Ahn, Minjae Lee, Kevin Galim, Seunghyuk Oh, Hyung Il Koo, Nam Ik Cho</strong></p>
<p>Text-to-image (T2I) generation has been actively studied using Diffusion Models and Autoregressive Models. Recently, Masked Generative Transformers have gained attention as an alternative to Autoregressive Models to overcome the inherent limitations of causal attention and autoregressive decoding through bidirectional attention and parallel decoding, enabling efficient and high-quality image generation. However, compositional T2I generation remains challenging, as even state-of-the-art Diffusion Models often fail to accurately bind attributes and achieve proper text-image alignment. While Diffusion Models have been extensively studied for this issue, Masked Generative Transformers exhibit similar limitations but have not been explored in this context. To address this, we propose Unmasking with Contrastive Attention Guidance (UNCAGE), a novel training-free method that improves compositional fidelity by leveraging attention maps to prioritize the unmasking of tokens that clearly represent individual objects. UNCAGE consistently improves performance in both quantitative and qualitative evaluations across multiple benchmarks and metrics, with negligible inference overhead. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/furiosa-ai/uncage">https://github.com/furiosa-ai/uncage</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆå·²ç»ä½¿ç”¨æ‰©æ•£æ¨¡å‹å’Œè‡ªå›å½’æ¨¡å‹è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚æœ€è¿‘ï¼Œä½œä¸ºè‡ªå›å½’æ¨¡å‹çš„æ›¿ä»£å“ï¼Œé®ç½©ç”Ÿæˆå¼Transformeré€šè¿‡åŒå‘æ³¨æ„åŠ›å’Œå¹¶è¡Œè§£ç å…‹æœäº†å› æœæ³¨æ„åŠ›å’Œè‡ªå›å½’è§£ç çš„å›ºæœ‰å±€é™æ€§ï¼Œä»è€Œå¼•èµ·äº†äººä»¬çš„å…³æ³¨ï¼Œå¹¶èƒ½å¤Ÿå®ç°é«˜æ•ˆé«˜è´¨é‡çš„å›¾åƒç”Ÿæˆã€‚ç„¶è€Œï¼Œç»„åˆå¼T2Iç”Ÿæˆä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹ä¹Ÿç»å¸¸æ— æ³•å‡†ç¡®ç»‘å®šå±æ€§å¹¶å®ç°é€‚å½“çš„æ–‡æœ¬-å›¾åƒå¯¹é½ã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹é’ˆå¯¹è¿™ä¸€é—®é¢˜å·²ç»å¾—åˆ°äº†å¹¿æ³›ç ”ç©¶ï¼Œä½†é®ç½©ç”Ÿæˆå¼Transformerä¹Ÿè¡¨ç°å‡ºç±»ä¼¼çš„å±€é™æ€§ï¼Œä½†å°šæœªåœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­å¾—åˆ°æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ— è®­ç»ƒæ–¹æ³•â€œè§£é™¤é®ç½©ä¸å¯¹æ¯”æ³¨æ„åŠ›å¼•å¯¼â€ï¼ˆUNCAGEï¼‰ï¼Œå®ƒé€šè¿‡åˆ©ç”¨æ³¨æ„åŠ›å›¾æ¥ä¼˜å…ˆè§£é™¤ä»£è¡¨å•ä¸ªå¯¹è±¡çš„æ ‡è®°çš„é®ç½©ï¼Œä»è€Œæé«˜ç»„åˆä¿çœŸåº¦ã€‚UNCAGEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’ŒæŒ‡æ ‡ä¸‹å§‹ç»ˆæé«˜äº†å®šé‡å’Œå®šæ€§è¯„ä¼°çš„æ€§èƒ½ï¼Œä¸”æ¨ç†å¼€é”€å¾®ä¹å…¶å¾®ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/furiosa-ai/uncage">https://github.com/furiosa-ai/uncage</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05399v1">PDF</a> Code is available at <a target="_blank" rel="noopener" href="https://github.com/furiosa-ai/uncage">https://github.com/furiosa-ai/uncage</a></p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ˜¯Diffusion Modelså’ŒAutoregressive Modelsç ”ç©¶çš„çƒ­ç‚¹ã€‚è¿‘æœŸï¼ŒMasked Generative Transformerså…‹æœäº†Autoregressive Modelsçš„ä¸€äº›å±€é™æ€§ï¼Œèƒ½å¤Ÿå®ç°åŒå‘æ³¨æ„åŠ›å’Œå¹³è¡Œè§£ç çš„é«˜æ•ˆç‡å’Œé«˜è´¨é‡å›¾åƒç”Ÿæˆã€‚å°½ç®¡Diffusion Modelsé¢ä¸´å°†å±æ€§å‡†ç¡®ç»‘å®šå’Œå®ç°æ–‡æœ¬-å›¾åƒé€‚å½“å¯¹é½çš„æŒ‘æˆ˜ï¼Œè€Œé’ˆå¯¹è¿™ä¸ªé—®é¢˜çš„è§£å†³ç­–ç•¥â€œUnmasking with Contrastive Attention Guidanceâ€ï¼ˆUNCAGEï¼‰èƒ½æœ‰æ•ˆæé«˜ç»„åˆä¿çœŸåº¦ï¼Œåˆ©ç”¨æ³¨æ„åŠ›å›¾ä¼˜å…ˆè§£æ©é‚£äº›æ˜ç¡®ä»£è¡¨å•ä¸ªå¯¹è±¡çš„æ ‡è®°ã€‚UNCAGEåœ¨ä¸åŒåŸºå‡†æµ‹è¯•å’Œå„ç§è¯„ä»·æŒ‡æ ‡ä¸­çš„å®šé‡å’Œå®šæ€§è¯„ä»·éƒ½è¡¨ç°å‡ºä¸€è‡´æ€§æ”¹è¿›ï¼Œä¸”æ¨ç†å¼€é”€å¾®ä¹å…¶å¾®ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨furiosa-ai&#x2F;uncageè·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion Modelså’ŒAutoregressive Modelsåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå—åˆ°å…³æ³¨ã€‚</li>
<li>Masked Generative Transformersä½œä¸ºæ›¿ä»£Autoregressive Modelsçš„æ–¹æ³•è€Œå—åˆ°å…³æ³¨ï¼Œå› å…¶å¯å®ç°é«˜æ•ˆä¸”é«˜è´¨é‡çš„å›¾åƒç”Ÿæˆã€‚</li>
<li>å³ä½¿æ˜¯æœ€å…ˆè¿›çš„Diffusion Modelsä¹Ÿé¢ä¸´å‡†ç¡®ç»‘å®šå±æ€§å’Œå®ç°é€‚å½“æ–‡æœ¬-å›¾åƒå¯¹é½çš„æŒ‘æˆ˜ã€‚</li>
<li>Masked Generative Transformersåœ¨è§£å†³æ­¤é—®é¢˜ä¸Šä¹Ÿæœ‰ç±»ä¼¼å±€é™æ€§ï¼Œå°šæœªåœ¨è¿™ä¸€èƒŒæ™¯ä¸‹è¢«æ¢ç´¢ã€‚</li>
<li>æå‡ºäº†åä¸ºUNCAGEçš„æ–°æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„ç­–ç•¥ï¼Œé€šè¿‡åˆ©ç”¨æ³¨æ„åŠ›å›¾æ¥æé«˜ç»„åˆä¿çœŸåº¦ã€‚</li>
<li>UNCAGEåœ¨ä¸åŒåŸºå‡†æµ‹è¯•å’Œå¤šæŒ‡æ ‡è¯„ä¼°ä¸­éƒ½è¡¨ç°ä¼˜å¼‚ï¼Œä¸”æ¨ç†å¼€é”€å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-74ca7aa006ee5a5d1c44a5adda8d2296.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc1e25e2617a8df68e866961822c164a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05686a7d5439c9063c360d014d383ff4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9257d5ed41106778dc1f3aad7169eba2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Textual-Inversion-for-Efficient-Adaptation-of-Open-Vocabulary-Object-Detectors-Without-Forgetting"><a href="#Textual-Inversion-for-Efficient-Adaptation-of-Open-Vocabulary-Object-Detectors-Without-Forgetting" class="headerlink" title="Textual Inversion for Efficient Adaptation of Open-Vocabulary Object   Detectors Without Forgetting"></a>Textual Inversion for Efficient Adaptation of Open-Vocabulary Object   Detectors Without Forgetting</h2><p><strong>Authors:Frank Ruis, Gertjan Burghouts, Hugo Kuijf</strong></p>
<p>Recent progress in large pre-trained vision language models (VLMs) has reached state-of-the-art performance on several object detection benchmarks and boasts strong zero-shot capabilities, but for optimal performance on specific targets some form of finetuning is still necessary. While the initial VLM weights allow for great few-shot transfer learning, this usually involves the loss of the original natural language querying and zero-shot capabilities. Inspired by the success of Textual Inversion (TI) in personalizing text-to-image diffusion models, we propose a similar formulation for open-vocabulary object detection. TI allows extending the VLM vocabulary by learning new or improving existing tokens to accurately detect novel or fine-grained objects from as little as three examples. The learned tokens are completely compatible with the original VLM weights while keeping them frozen, retaining the original modelâ€™s benchmark performance, and leveraging its existing capabilities such as zero-shot domain transfer (e.g., detecting a sketch of an object after training only on real photos). The storage and gradient calculations are limited to the token embedding dimension, requiring significantly less compute than full-model fine-tuning. We evaluated whether the method matches or outperforms the baseline methods that suffer from forgetting in a wide variety of quantitative and qualitative experiments. </p>
<blockquote>
<p>å…³äºå¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æœ€æ–°è¿›å±•å·²ç»åœ¨å¤šä¸ªç›®æ ‡æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å…·å¤‡å¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¸ºäº†åœ¨ç‰¹å®šç›®æ ‡ä¸Šè·å¾—æœ€ä½³æ€§èƒ½ï¼Œä»éœ€è¦è¿›è¡ŒæŸç§å½¢å¼çš„å¾®è°ƒã€‚è™½ç„¶åˆå§‹çš„VLMæƒé‡å…è®¸è¿›è¡Œå‡ºè‰²çš„å°‘æ ·æœ¬è¿ç§»å­¦ä¹ ï¼Œä½†è¿™é€šå¸¸æ¶‰åŠåˆ°åŸå§‹è‡ªç„¶è¯­è¨€æŸ¥è¯¢å’Œé›¶æ ·æœ¬èƒ½åŠ›çš„æŸå¤±ã€‚å—æ–‡æœ¬åè½¬ï¼ˆTIï¼‰åœ¨ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬ä¸ºå¼€æ”¾è¯æ±‡è¡¨ç›®æ ‡æ£€æµ‹æå‡ºäº†ç±»ä¼¼çš„è§£å†³æ–¹æ¡ˆã€‚TIå…è®¸é€šè¿‡å­¦ä¹ æ–°ä»¤ç‰Œæˆ–æ”¹è¿›ç°æœ‰ä»¤ç‰Œæ¥æ‰©å±•VLMè¯æ±‡è¡¨ï¼Œä»è€Œä»…ä»ä¸‰ä¸ªç¤ºä¾‹ä¸­å‡†ç¡®æ£€æµ‹å‡ºæ–°é¢–æˆ–ç»†ç²’åº¦å¯¹è±¡ã€‚æ‰€å­¦ä¹ çš„ä»¤ç‰Œä¸åŸå§‹VLMæƒé‡å®Œå…¨å…¼å®¹ï¼ŒåŒæ—¶ä¿æŒå†»ç»“çŠ¶æ€ï¼Œä¿ç•™äº†åŸå§‹æ¨¡å‹çš„åŸºå‡†æµ‹è¯•æ€§èƒ½ï¼Œå¹¶åˆ©ç”¨äº†å…¶ç°æœ‰åŠŸèƒ½ï¼Œå¦‚é›¶æ ·æœ¬åŸŸè¿ç§»ï¼ˆä¾‹å¦‚ï¼Œåœ¨ä»…å¯¹çœŸå®ç…§ç‰‡è¿›è¡Œè®­ç»ƒåæ£€æµ‹å¯¹è±¡çš„è‰å›¾ï¼‰ã€‚å­˜å‚¨å’Œæ¢¯åº¦è®¡ç®—ä»…é™äºä»¤ç‰ŒåµŒå…¥ç»´åº¦ï¼Œå› æ­¤æ‰€éœ€çš„è®¡ç®—é‡è¿œå°äºå…¨æ¨¡å‹å¾®è°ƒã€‚æˆ‘ä»¬é€šè¿‡å¤šç§å®šé‡å’Œå®šæ€§å®éªŒè¯„ä¼°äº†è¯¥æ–¹æ³•æ˜¯å¦åŒ¹é…æˆ–ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•åœ¨å¹¿æ³›çš„ä»»åŠ¡ä¸­ä¼šå‡ºç°é—å¿˜é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05323v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æœ€æ–°è¿›å±•å·²åœ¨å¤šä¸ªç›®æ ‡æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å…·å¤‡å¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚è™½ç„¶åˆå§‹VLMæƒé‡å…è®¸å¾ˆå¥½çš„å°‘æ ·æœ¬è¿ç§»å­¦ä¹ ï¼Œä½†ä¸ºäº†ç‰¹å®šç›®æ ‡çš„æœ€ä½³æ€§èƒ½ä»éœ€è¦è¿›è¡Œå¾®è°ƒã€‚æœ¬æ–‡å—æ–‡æœ¬åè½¬ï¼ˆTIï¼‰åœ¨ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æˆåŠŸçš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºå¼€æ”¾è¯æ±‡è¡¨ç›®æ ‡æ£€æµ‹çš„ç±»ä¼¼æ–¹æ³•ã€‚TIé€šè¿‡å­¦ä¹ æ–°ä»¤ç‰Œæˆ–æ”¹è¿›ç°æœ‰ä»¤ç‰Œæ¥ç²¾ç¡®æ£€æµ‹ä»ä»…ä¸‰ä¸ªç¤ºä¾‹ä¸­çš„æ–°é¢–æˆ–ç»†ç²’åº¦å¯¹è±¡ï¼Œä»è€Œæ‰©å±•VLMè¯æ±‡é‡ã€‚æ‰€å­¦ä»¤ç‰Œä¸åŸå§‹VLMæƒé‡å®Œå…¨å…¼å®¹ï¼ŒåŒæ—¶ä¿æŒå†»ç»“çŠ¶æ€ï¼Œä¿ç•™åŸå§‹æ¨¡å‹çš„åŸºå‡†æµ‹è¯•æ€§èƒ½ï¼Œå¹¶åˆ©ç”¨å…¶ç°æœ‰åŠŸèƒ½ï¼Œå¦‚é›¶æ ·æœ¬åŸŸè½¬ç§»ã€‚å­˜å‚¨å’Œæ¢¯åº¦è®¡ç®—ä»…é™äºä»¤ç‰ŒåµŒå…¥ç»´åº¦ï¼Œè¿™éœ€è¦æ¯”å…¨æ¨¡å‹å¾®è°ƒå°‘å¾—å¤šçš„è®¡ç®—é‡ã€‚ç»è¿‡å¹¿æ³›å®šé‡å’Œå®šæ€§å®éªŒè¯„ä¼°ï¼Œè¯¥æ–¹æ³•èƒ½å¦è¾¾åˆ°æˆ–è¶…è¶ŠåŸºçº¿æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•ä¼šé­å—é—å¿˜çš„é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç›®æ ‡æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯é›¶æ ·æœ¬èƒ½åŠ›ã€‚</li>
<li>è™½ç„¶åˆå§‹VLMæƒé‡å…·æœ‰å‡ºè‰²çš„è¿ç§»å­¦ä¹ èƒ½åŠ›ï¼Œä½†ä»éœ€å¾®è°ƒä»¥ä¼˜åŒ–ç‰¹å®šç›®æ ‡çš„æ€§èƒ½ã€‚</li>
<li>å—æ–‡æœ¬åè½¬ï¼ˆTIï¼‰åœ¨ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„æˆåŠŸçš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç”¨äºå¼€æ”¾è¯æ±‡è¡¨ç›®æ ‡æ£€æµ‹çš„æ–¹æ³•ã€‚</li>
<li>TIé€šè¿‡æ‰©å±•VLMè¯æ±‡é‡æ¥ç²¾ç¡®æ£€æµ‹æ–°é¢–æˆ–ç»†ç²’åº¦å¯¹è±¡ï¼Œåªéœ€å°‘é‡ç¤ºä¾‹å³å¯å­¦ä¹ æ–°ä»¤ç‰Œæˆ–æ”¹è¿›ç°æœ‰ä»¤ç‰Œã€‚</li>
<li>æ‰€å­¦ä»¤ç‰Œä¸åŸå§‹VLMæƒé‡å…¼å®¹ï¼ŒåŒæ—¶ä¿ç•™å…¶æ€§èƒ½å¹¶å……åˆ†åˆ©ç”¨ç°æœ‰åŠŸèƒ½ï¼ˆå¦‚é›¶æ ·æœ¬åŸŸè½¬ç§»ï¼‰ã€‚</li>
<li>å­˜å‚¨å’Œæ¢¯åº¦è®¡ç®—é›†ä¸­åœ¨ä»¤ç‰ŒåµŒå…¥ç»´åº¦ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05323">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31d3d19681f76ccc40cd3e6fd0ad44d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b8d15b0360500c143207037a8d34773.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b00d609b4b549ad18a3d4201207cf8b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SGDFuse-SAM-Guided-Diffusion-for-High-Fidelity-Infrared-and-Visible-Image-Fusion"><a href="#SGDFuse-SAM-Guided-Diffusion-for-High-Fidelity-Infrared-and-Visible-Image-Fusion" class="headerlink" title="SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible   Image Fusion"></a>SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible   Image Fusion</h2><p><strong>Authors:Xiaoyang Zhang, Zhen Hua, Yakun Ju, Wei Zhou, Jun Liu, Alex C. Kot</strong></p>
<p>Infrared and visible image fusion (IVIF) aims to combine the thermal radiation information from infrared images with the rich texture details from visible images to enhance perceptual capabilities for downstream visual tasks. However, existing methods often fail to preserve key targets due to a lack of deep semantic understanding of the scene, while the fusion process itself can also introduce artifacts and detail loss, severely compromising both image quality and task performance. To address these issues, this paper proposes SGDFuse, a conditional diffusion model guided by the Segment Anything Model (SAM), to achieve high-fidelity and semantically-aware image fusion. The core of our method is to utilize high-quality semantic masks generated by SAM as explicit priors to guide the optimization of the fusion process via a conditional diffusion model. Specifically, the framework operates in a two-stage process: it first performs a preliminary fusion of multi-modal features, and then utilizes the semantic masks from SAM jointly with the preliminary fused image as a condition to drive the diffusion modelâ€™s coarse-to-fine denoising generation. This ensures the fusion process not only has explicit semantic directionality but also guarantees the high fidelity of the final result. Extensive experiments demonstrate that SGDFuse achieves state-of-the-art performance in both subjective and objective evaluations, as well as in its adaptability to downstream tasks, providing a powerful solution to the core challenges in image fusion. The code of SGDFuse is available at <a target="_blank" rel="noopener" href="https://github.com/boshizhang123/SGDFuse">https://github.com/boshizhang123/SGDFuse</a>. </p>
<blockquote>
<p>çº¢å¤–ä¸å¯è§å…‰å›¾åƒèåˆï¼ˆIVIFï¼‰æ—¨åœ¨å°†çº¢å¤–å›¾åƒä¸­çš„çƒ­è¾å°„ä¿¡æ¯ä¸å¯è§å…‰å›¾åƒä¸­çš„ä¸°å¯Œçº¹ç†ç»†èŠ‚ç›¸ç»“åˆï¼Œä»¥æé«˜ä¸‹æ¸¸è§†è§‰ä»»åŠ¡çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ç”±äºç¼ºä¹åœºæ™¯çš„æ·±åº¦è¯­ä¹‰ç†è§£è€Œæ— æ³•ä¿ç•™å…³é”®ç›®æ ‡ï¼ŒåŒæ—¶èåˆè¿‡ç¨‹æœ¬èº«ä¹Ÿå¯èƒ½å¼•å…¥ä¼ªå½±å’Œç»†èŠ‚æŸå¤±ï¼Œä¸¥é‡æŸå®³å›¾åƒè´¨é‡å’Œä»»åŠ¡æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†SGDFuseï¼Œä¸€ä¸ªç”±Segment Anything Modelï¼ˆSAMï¼‰å¼•å¯¼çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå®ç°é«˜ä¿çœŸå’Œè¯­ä¹‰æ„ŸçŸ¥çš„å›¾åƒèåˆã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯åˆ©ç”¨SAMç”Ÿæˆçš„é«˜è´¨é‡è¯­ä¹‰æ©è†œä½œä¸ºæ˜ç¡®å…ˆéªŒï¼Œé€šè¿‡æ¡ä»¶æ‰©æ•£æ¨¡å‹æŒ‡å¯¼èåˆè¿‡ç¨‹çš„ä¼˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨ä¸¤é˜¶æ®µè¿‡ç¨‹ï¼šé¦–å…ˆè¿›è¡Œå¤šæ¨¡æ€ç‰¹å¾çš„åˆæ­¥èåˆï¼Œç„¶ååˆ©ç”¨SAMçš„è¯­ä¹‰æ©è†œä¸åˆæ­¥èåˆå›¾åƒä½œä¸ºæ¡ä»¶ï¼Œé©±åŠ¨æ‰©æ•£æ¨¡å‹çš„ä»ç²—åˆ°ç»†çš„é™å™ªç”Ÿæˆã€‚è¿™ç¡®ä¿èåˆè¿‡ç¨‹ä¸ä»…å…·æœ‰æ˜ç¡®çš„è¯­ä¹‰æ–¹å‘æ€§ï¼Œè€Œä¸”è¿˜ä¿è¯æœ€ç»ˆç»“æœçš„é«˜ä¿çœŸåº¦ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSGDFuseåœ¨ä¸»è§‚å’Œå®¢è§‚è¯„ä¼°ä»¥åŠé€‚åº”ä¸‹æ¸¸ä»»åŠ¡æ–¹é¢å‡è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œä¸ºè§£å†³å›¾åƒèåˆçš„æ ¸å¿ƒæŒ‘æˆ˜æä¾›äº†å¼ºå¤§çš„è§£å†³æ–¹æ¡ˆã€‚SGDFuseçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/boshizhang123/SGDFuse">https://github.com/boshizhang123/SGDFuse</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05264v1">PDF</a> Submitted to TCSVT</p>
<p><strong>Summary</strong></p>
<pre><code>çº¢å¤–ä¸å¯è§å›¾åƒèåˆï¼ˆIVIFï¼‰ç»“åˆäº†çº¢å¤–å›¾åƒçš„çƒ­è¾å°„ä¿¡æ¯ä¸å¯è§å›¾åƒçš„ä¸°å¯Œçº¹ç†ç»†èŠ‚ï¼Œä»¥æé«˜ä¸‹æ¸¸è§†è§‰ä»»åŠ¡çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç”±äºç¼ºä¹æ·±åº¦åœºæ™¯è¯­ä¹‰ç†è§£ï¼Œå¸¸å¸¸æ— æ³•ä¿ç•™å…³é”®ç›®æ ‡ï¼ŒåŒæ—¶èåˆè¿‡ç¨‹æœ¬èº«ä¹Ÿå¯èƒ½å¼•å…¥ä¼ªå½±å’Œç»†èŠ‚æŸå¤±ï¼Œä¸¥é‡å½±å“å›¾åƒè´¨é‡å’Œä»»åŠ¡æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºSGDFuseæ–¹æ³•ï¼Œé‡‡ç”¨ä»¥Segment Anything Modelï¼ˆSAMï¼‰ä¸ºæŒ‡å¯¼çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå®ç°é«˜ä¿çœŸå’Œè¯­ä¹‰æ„ŸçŸ¥çš„å›¾åƒèåˆã€‚è¯¥æ–¹æ³•åˆ©ç”¨SAMç”Ÿæˆçš„é«˜è´¨é‡è¯­ä¹‰æ©è†œä½œä¸ºæ˜¾å¼å…ˆéªŒï¼ŒæŒ‡å¯¼èåˆè¿‡ç¨‹çš„ä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒSGDFuseåœ¨ä¸»è§‚å’Œå®¢è§‚è¯„ä¼°ä»¥åŠä¸‹æ¸¸ä»»åŠ¡é€‚åº”æ€§æ–¹é¢å‡è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œä¸ºè§£å†³å›¾åƒèåˆçš„æ ¸å¿ƒæŒ‘æˆ˜æä¾›äº†å¼ºå¤§è§£å†³æ–¹æ¡ˆã€‚

**Key Takeaways**
 
1. çº¢å¤–ä¸å¯è§å›¾åƒèåˆï¼ˆIVIFï¼‰ç»“åˆäº†çº¢å¤–å›¾åƒä¸å¯è§å›¾åƒçš„ä¿¡æ¯ï¼Œä»¥æé«˜è§†è§‰ä»»åŠ¡çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚
2. ç°æœ‰å›¾åƒèåˆæ–¹æ³•å­˜åœ¨æ— æ³•ä¿ç•™å…³é”®ç›®æ ‡çš„é—®é¢˜ï¼Œä¸”èåˆè¿‡ç¨‹å¯èƒ½å¼•å…¥ä¼ªå½±å’Œç»†èŠ‚æŸå¤±ã€‚
3. SGDFuseæ–¹æ³•é‡‡ç”¨æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆè¯­ä¹‰ç†è§£æ¥å®ç°å›¾åƒèåˆã€‚
4. SGDFuseåˆ©ç”¨SAMç”Ÿæˆçš„é«˜è´¨é‡è¯­ä¹‰æ©è†œæ¥æŒ‡å¯¼èåˆè¿‡ç¨‹çš„ä¼˜åŒ–ï¼Œç¡®ä¿èåˆç»“æœçš„é«˜ä¿çœŸå’Œè¯­ä¹‰æ„ŸçŸ¥ã€‚
5. SGDFuseåœ¨ä¸»è§‚å’Œå®¢è§‚è¯„ä¼°æ–¹é¢å‡è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„ä¸‹æ¸¸ä»»åŠ¡é€‚åº”æ€§ã€‚
6. SGDFuseçš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚
7. æ¡ä»¶æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒèåˆé¢†åŸŸå…·æœ‰æ½œåœ¨çš„åº”ç”¨å‰æ™¯ã€‚
</code></pre>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05264">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4f2b1a21f239c928eb1f7d4cd3800bbf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa1811841ad91e437cdf985d48c25a5a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-995697bb529b7d0a3c3b4ffe430d5cf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b864eb6de65af309c3e69f51fdf6e90.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PoseGen-In-Context-LoRA-Finetuning-for-Pose-Controllable-Long-Human-Video-Generation"><a href="#PoseGen-In-Context-LoRA-Finetuning-for-Pose-Controllable-Long-Human-Video-Generation" class="headerlink" title="PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human   Video Generation"></a>PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human   Video Generation</h2><p><strong>Authors:Jingxuan He, Busheng Su, Finn Wong</strong></p>
<p>Generating long, temporally coherent videos with precise control over subject identity and motion is a formidable challenge for current diffusion models, which often suffer from identity drift and are limited to short clips. We introduce PoseGen, a novel framework that generates arbitrarily long videos of a specific subject from a single reference image and a driving pose sequence. Our core innovation is an in-context LoRA finetuning strategy that injects subject appearance at the token level for identity preservation, while simultaneously conditioning on pose information at the channel level for fine-grained motion control. To overcome duration limits, PoseGen pioneers an interleaved segment generation method that seamlessly stitches video clips together, using a shared KV cache mechanism and a specialized transition process to ensure background consistency and temporal smoothness. Trained on a remarkably small 33-hour video dataset, extensive experiments show that PoseGen significantly outperforms state-of-the-art methods in identity fidelity, pose accuracy, and its unique ability to produce coherent, artifact-free videos of unlimited duration. </p>
<blockquote>
<p>ç”Ÿæˆå…·æœ‰ç²¾ç¡®æ§åˆ¶ä¸»ä½“èº«ä»½å’Œè¿åŠ¨çš„é•¿è§†é¢‘å¯¹äºå½“å‰çš„æ‰©æ•£æ¨¡å‹æ¥è¯´æ˜¯ä¸€é¡¹è‰°å·¨çš„æŒ‘æˆ˜ï¼Œè¿™äº›æ¨¡å‹ç»å¸¸é¢ä¸´èº«ä»½æ¼‚ç§»çš„é—®é¢˜ï¼Œå¹¶ä¸”ä»…é™äºçŸ­ç‰‡ã€‚æˆ‘ä»¬å¼•å…¥äº†PoseGenï¼Œä¸€ä¸ªä»å•ä¸ªå‚è€ƒå›¾åƒå’Œé©±åŠ¨å§¿æ€åºåˆ—ç”Ÿæˆç‰¹å®šä¸»é¢˜ä»»æ„é•¿åº¦è§†é¢‘çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒåˆ›æ–°ä¹‹å¤„åœ¨äºæå‡ºäº†ä¸€ç§ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„LoRAå¾®è°ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨ä»¤ç‰Œçº§åˆ«æ³¨å…¥ä¸»ä½“å¤–è§‚ä»¥å®ç°èº«ä»½ä¿ç•™ï¼ŒåŒæ—¶åœ¨é€šé“çº§åˆ«åŒæ—¶å¤„ç†å§¿æ€ä¿¡æ¯ä»¥å®ç°ç²¾ç»†è¿åŠ¨æ§åˆ¶ã€‚ä¸ºäº†å…‹æœæŒç»­æ—¶é—´é™åˆ¶ï¼ŒPoseGenå¼€åˆ›äº†ä¸€ç§äº¤æ›¿åˆ†æ®µç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ— ç¼åœ°æ‹¼æ¥è§†é¢‘ç‰‡æ®µï¼Œå¹¶ä½¿ç”¨å…±äº«çš„KVç¼“å­˜æœºåˆ¶å’Œä¸“é—¨çš„è¿‡æ¸¡è¿‡ç¨‹ä»¥ç¡®ä¿èƒŒæ™¯ä¸€è‡´æ€§å’Œæ—¶é—´å¹³æ»‘æ€§ã€‚åœ¨ä»¤äººæƒŠè®¶çš„ä»…33å°æ—¶çš„è§†é¢‘æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¤§é‡å®éªŒè¡¨æ˜ï¼ŒPoseGenåœ¨èº«ä»½ä¿çœŸåº¦ã€å§¿æ€å‡†ç¡®æ€§å’Œå…¶äº§ç”Ÿè¿è´¯ã€æ— ç‘•ç–µçš„æ— é™æ—¶é•¿è§†é¢‘çš„ç‹¬ç‰¹èƒ½åŠ›æ–¹é¢æ˜¾è‘—ä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05091v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>PoseGenæ¡†æ¶é€šè¿‡å•å¼ å‚è€ƒå›¾åƒå’ŒåŠ¨æ€å§¿æ€åºåˆ—ç”Ÿæˆç‰¹å®šä¸»ä½“çš„ä»»æ„é•¿åº¦è§†é¢‘ã€‚å…¶åˆ›æ–°çš„æ ¸å¿ƒæ˜¯ä¸Šä¸‹æ–‡LoRAå¾®è°ƒç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨ä»¤ç‰Œçº§åˆ«æ³¨å…¥ä¸»ä½“å¤–è§‚ä»¥å®ç°èº«ä»½ä¿ç•™ï¼ŒåŒæ—¶ä»¥é€šé“çº§åˆ«å¤„ç†å§¿æ€ä¿¡æ¯ä»¥å®ç°ç²¾ç»†è¿åŠ¨æ§åˆ¶ã€‚PoseGené€šè¿‡é‡‡ç”¨äº¤ç»‡æ®µç”Ÿæˆæ–¹æ³•æ¥çªç ´æ—¶é•¿é™åˆ¶ï¼Œå¹¶ä¾é å…±äº«çš„KVç¼“å­˜æœºåˆ¶å’Œç‰¹æ®Šçš„è¿‡æ¸¡æµç¨‹æ¥ç¡®ä¿èƒŒæ™¯çš„ä¸€è‡´æ€§å’Œæ—¶é—´è¿ç»­æ€§ã€‚å®éªŒè¯æ˜PoseGenæ˜¾è‘—æé«˜äº†èº«ä»½ä¿çœŸåº¦ã€å§¿æ€å‡†ç¡®åº¦å’Œåœ¨ç”Ÿæˆè¿è´¯æ— ä¼ªé€ çš„ä»»æ„æ—¶é•¿è§†é¢‘ä¸Šçš„ç‹¬ç‰¹èƒ½åŠ›ã€‚PoseGenåªåœ¨å°‘é‡çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒä¾¿è¾¾åˆ°å¼ºå¤§çš„æ•ˆæœï¼Œå¤§å¤§æå‡äº†æ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œã€‚æ€»çš„æ¥è¯´ï¼ŒPoseGenæ˜¯æ‰©æ•£æ¨¡å‹é¢†åŸŸçš„ä¸€ä¸ªé‡å¤§çªç ´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PoseGenæ¡†æ¶å¯ä»¥ä»å•å¼ å‚è€ƒå›¾åƒå’Œå§¿æ€åºåˆ—ç”Ÿæˆç‰¹å®šä¸»ä½“çš„ä»»æ„é•¿åº¦è§†é¢‘ã€‚</li>
<li>é‡‡ç”¨åˆ›æ–°çš„ä¸Šä¸‹æ–‡LoRAå¾®è°ƒç­–ç•¥ï¼Œå®ç°äº†èº«ä»½ä¿ç•™å’Œç²¾ç»†è¿åŠ¨æ§åˆ¶ã€‚</li>
<li>é€šè¿‡äº¤ç»‡æ®µç”Ÿæˆæ–¹æ³•çªç ´æ—¶é•¿é™åˆ¶ï¼Œç”Ÿæˆè¿è´¯çš„è§†é¢‘ç‰‡æ®µã€‚</li>
<li>ä½¿ç”¨å…±äº«çš„KVç¼“å­˜æœºåˆ¶å’Œè¿‡æ¸¡æµç¨‹ç¡®ä¿èƒŒæ™¯ä¸€è‡´æ€§å’Œæ—¶é—´è¿ç»­æ€§ã€‚</li>
<li>PoseGenåœ¨èº«ä»½ä¿çœŸåº¦ã€å§¿æ€å‡†ç¡®åº¦ç­‰æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å…·å¤‡ç‹¬ç‰¹çš„æ— é™æ—¶é•¿è§†é¢‘ç”Ÿæˆèƒ½åŠ›ã€‚åœ¨æœ‰é™çš„æ•°æ®é›†ä¸Šè®­ç»ƒè¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½æå‡æ‰©æ•£æ¨¡å‹èƒ½åŠ›è¾¹ç•Œçš„é‡è¦è¿›å±•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05091">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d1992a8ca07384545a6b4089781722a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2dd11a00c3955b72d1913205b721b73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2992ce6713e19677b01858930614b0ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4ebd3c922a275e37ead28f3e17f43a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab624682da6009f9d8e9015176ace752.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c46db168ab6cf188c0ec59a7199e85cf.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Single-Step-Reconstruction-Free-Anomaly-Detection-and-Segmentation-via-Diffusion-Models"><a href="#Single-Step-Reconstruction-Free-Anomaly-Detection-and-Segmentation-via-Diffusion-Models" class="headerlink" title="Single-Step Reconstruction-Free Anomaly Detection and Segmentation via   Diffusion Models"></a>Single-Step Reconstruction-Free Anomaly Detection and Segmentation via   Diffusion Models</h2><p><strong>Authors:Mehrdad Moradi, Marco Grasso, Bianca Maria Colosimo, Kamran Paynabar</strong></p>
<p>Generative models have demonstrated significant success in anomaly detection and segmentation over the past decade. Recently, diffusion models have emerged as a powerful alternative, outperforming previous approaches such as GANs and VAEs. In typical diffusion-based anomaly detection, a model is trained on normal data, and during inference, anomalous images are perturbed to a predefined intermediate step in the forward diffusion process. The corresponding normal image is then reconstructed through iterative reverse sampling.   However, reconstruction-based approaches present three major challenges: (1) the reconstruction process is computationally expensive due to multiple sampling steps, making real-time applications impractical; (2) for complex or subtle patterns, the reconstructed image may correspond to a different normal pattern rather than the original input; and (3) Choosing an appropriate intermediate noise level is challenging because it is application-dependent and often assumes prior knowledge of anomalies, an assumption that does not hold in unsupervised settings.   We introduce Reconstruction-free Anomaly Detection with Attention-based diffusion models in Real-time (RADAR), which overcomes the limitations of reconstruction-based anomaly detection. Unlike current SOTA methods that reconstruct the input image, RADAR directly produces anomaly maps from the diffusion model, improving both detection accuracy and computational efficiency. We evaluate RADAR on real-world 3D-printed material and the MVTec-AD dataset. Our approach surpasses state-of-the-art diffusion-based and statistical machine learning models across all key metrics, including accuracy, precision, recall, and F1 score. Specifically, RADAR improves F1 score by 7% on MVTec-AD and 13% on the 3D-printed material dataset compared to the next best model.   Code available at: <a target="_blank" rel="noopener" href="https://github.com/mehrdadmoradi124/RADAR">https://github.com/mehrdadmoradi124/RADAR</a> </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹åœ¨è¿‡å»åå¹´ä¸­åœ¨å¼‚å¸¸æ£€æµ‹å’Œåˆ†å‰²æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹ä½œä¸ºä¸€ç§å¼ºå¤§çš„æ›¿ä»£æ–¹æ³•å´­éœ²å¤´è§’ï¼Œè¶…è¶Šäº†è¯¸å¦‚GANå’ŒVAEç­‰å…ˆå‰çš„æ–¹æ³•ã€‚åœ¨å…¸å‹çš„åŸºäºæ‰©æ•£çš„å¼‚å¸¸æ£€æµ‹ä¸­ï¼Œæ¨¡å‹ä¼šåœ¨æ­£å¸¸æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç„¶ååœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¼‚å¸¸å›¾åƒä¼šè¢«æ‰°åŠ¨åˆ°å‰å‘æ‰©æ•£è¿‡ç¨‹çš„é¢„å®šä¸­é—´æ­¥éª¤ã€‚ç„¶åé€šè¿‡è¿­ä»£åå‘é‡‡æ ·é‡å»ºç›¸åº”çš„æ­£å¸¸å›¾åƒã€‚ç„¶è€Œï¼ŒåŸºäºé‡å»ºçš„æ–¹æ³•å­˜åœ¨ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼š(1)ç”±äºå¤šæ¬¡é‡‡æ ·æ­¥éª¤ï¼Œé‡å»ºè¿‡ç¨‹è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œä½¿å¾—å®æ—¶åº”ç”¨ä¸åˆ‡å®é™…ï¼›(2)å¯¹äºå¤æ‚æˆ–å¾®å¦™çš„æ¨¡å¼ï¼Œé‡å»ºçš„å›¾åƒå¯èƒ½å¯¹åº”äºä¸åŒçš„æ­£å¸¸æ¨¡å¼ï¼Œè€ŒéåŸå§‹è¾“å…¥ï¼›(3)é€‰æ‹©é€‚å½“çš„ä¸­é—´å™ªå£°æ°´å¹³å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒå–å†³äºåº”ç”¨ï¼Œå¹¶ä¸”ç»å¸¸å‡è®¾å¯¹å¼‚å¸¸çš„å…ˆéªŒçŸ¥è¯†ï¼Œè¿™åœ¨æ— ç›‘ç£è®¾ç½®ä¸­å¹¶ä¸æˆç«‹ã€‚æˆ‘ä»¬å¼•å…¥äº†åŸºäºæ³¨æ„åŠ›æ‰©æ•£æ¨¡å‹çš„å®æ—¶æ— é‡å»ºå¼‚å¸¸æ£€æµ‹ï¼ˆRADARï¼‰ï¼Œå…‹æœäº†åŸºäºé‡å»ºçš„å¼‚å¸¸æ£€æµ‹çš„å±€é™æ€§ã€‚ä¸å½“å‰é¡¶å°–æ–¹æ³•ä¸åŒï¼ŒRADARç›´æ¥ä»æ‰©æ•£æ¨¡å‹ä¸­ç”Ÿæˆå¼‚å¸¸æ˜ å°„ï¼Œæé«˜äº†æ£€æµ‹å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œçš„3Dæ‰“å°ææ–™å’ŒMVTec-ADæ•°æ®é›†ä¸Šè¯„ä¼°äº†RADARã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰å…³é”®æŒ‡æ ‡ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹å’Œç»Ÿè®¡æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬å‡†ç¡®æ€§ã€ç²¾ç¡®æ€§ã€å¬å›ç‡å’ŒF1åˆ†æ•°ã€‚å…·ä½“æ¥è¯´ï¼ŒRADARåœ¨MVTec-ADä¸Šçš„F1åˆ†æ•°æé«˜äº†7%ï¼Œåœ¨3Dæ‰“å°ææ–™æ•°æ®é›†ä¸Šæé«˜äº†13%ï¼Œç›¸è¾ƒäºæ¬¡ä¼˜æ¨¡å‹ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/mehrdadmoradi124/RADAR%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mehrdadmoradi124/RADARæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04818v1">PDF</a> 9 pages, 8 figures, 2 tables. Submitted to an IEEE conference</p>
<p><strong>æ‘˜è¦</strong><br>    æ‰©æ•£æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹å’Œåˆ†å‰²æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚æœ€è¿‘ï¼Œæ‰©æ•£æ¨¡å‹ä½œä¸ºä¸€ç§å¼ºå¤§çš„æ›¿ä»£æ–¹æ³•ï¼Œåœ¨å¼‚å¸¸æ£€æµ‹æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†GANså’ŒVAEsç­‰æ—©æœŸæ–¹æ³•ã€‚ç„¶è€Œï¼ŒåŸºäºé‡å»ºçš„æ–¹æ³•å­˜åœ¨ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šè®¡ç®—æˆæœ¬é«˜ã€å¯¹äºå¤æ‚æˆ–å¾®å¦™çš„æ¨¡å¼å¯èƒ½æ— æ³•å‡†ç¡®é‡å»ºä»¥åŠé€‰æ‹©åˆé€‚çš„ä¸­é—´å™ªå£°æ°´å¹³å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†æ— é‡å»ºçš„å¼‚å¸¸æ£€æµ‹é›·è¾¾ç³»ç»Ÿï¼ˆRADARï¼‰ï¼Œå®ƒä½¿ç”¨åŸºäºæ³¨æ„åŠ›çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå®æ—¶å¼‚å¸¸æ£€æµ‹ï¼Œå…‹æœäº†åŸºäºé‡å»ºæ–¹æ³•çš„å±€é™æ€§ã€‚RADARç›´æ¥ç”Ÿæˆå¼‚å¸¸åœ°å›¾ï¼Œæé«˜äº†æ£€æµ‹ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ã€‚åœ¨çœŸå®ä¸–ç•Œçš„3Dæ‰“å°ææ–™å’ŒMVTec-ADæ•°æ®é›†ä¸Šè¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰å…³é”®æŒ‡æ ‡ä¸Šéƒ½è¶…è¿‡äº†æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹å’Œç»Ÿè®¡æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚ç‰¹åˆ«æ˜¯ä¸æœ€å¥½çš„ä¸‹ä¸€ä¸ªæ¨¡å‹ç›¸æ¯”ï¼ŒRADARåœ¨MVTec-ADä¸Šçš„F1å¾—åˆ†æé«˜äº†7%ï¼Œåœ¨3Dæ‰“å°ææ–™æ•°æ®é›†ä¸Šæé«˜äº†13%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mehrdadmoradi124/RADAR">é“¾æ¥</a>æ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹å’Œåˆ†å‰²æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæˆä¸ºç”Ÿæˆæ¨¡å‹çš„æ–°ç„¦ç‚¹ã€‚</li>
<li>åŸºäºé‡å»ºçš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•å­˜åœ¨è®¡ç®—æˆæœ¬é«˜ã€å¯¹å¤æ‚æ¨¡å¼é‡å»ºä¸å‡†ç¡®ä»¥åŠé€‰æ‹©ä¸­é—´å™ªå£°æ°´å¹³çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„RADARç³»ç»Ÿä½¿ç”¨åŸºäºæ³¨æ„åŠ›çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå®æ—¶å¼‚å¸¸æ£€æµ‹ï¼Œæ— éœ€é‡å»ºï¼Œæé«˜äº†æ£€æµ‹ç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>RADARåœ¨MVTec-ADæ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œçš„3Dæ‰“å°ææ–™ä¸Šçš„è¡¨ç°è¶…è¿‡äº†å…¶ä»–æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨F1å¾—åˆ†æ–¹é¢ã€‚</li>
<li>RADARæ–¹æ³•å¯ç›´æ¥ç”Ÿæˆå¼‚å¸¸åœ°å›¾ï¼Œè¿™æ˜¯å…¶ä¸å…¶ä»–æ–¹æ³•çš„ä¸»è¦åŒºåˆ«å’Œä¼˜åŠ¿ã€‚</li>
<li>ä»£ç å·²å…¬å¼€ï¼Œä¾¿äºè¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
<li>RADARçš„å¼•å…¥ä¸ºæ‰©æ•£æ¨¡å‹åœ¨å¼‚å¸¸æ£€æµ‹é¢†åŸŸçš„åº”ç”¨å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04818">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6b07ab61159bf23e644e87e0ec188e71.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ef1ab5f24fe4e3bfd99a0f36e55f566.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8bee61fbd138c40ce5dfeed7b6a32d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aadcde60c1537e858520ae99626da1d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c23bcca194eb746474696346a8201f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc83811e1eb8e7e327d7a7e0b6ff91ca.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LumiGen-An-LVLM-Enhanced-Iterative-Framework-for-Fine-Grained-Text-to-Image-Generation"><a href="#LumiGen-An-LVLM-Enhanced-Iterative-Framework-for-Fine-Grained-Text-to-Image-Generation" class="headerlink" title="LumiGen: An LVLM-Enhanced Iterative Framework for Fine-Grained   Text-to-Image Generation"></a>LumiGen: An LVLM-Enhanced Iterative Framework for Fine-Grained   Text-to-Image Generation</h2><p><strong>Authors:Xiaoqi Dong, Xiangyu Zhou, Nicholas Evans, Yujia Lin</strong></p>
<p>Text-to-Image (T2I) generation has made significant advancements with diffusion models, yet challenges persist in handling complex instructions, ensuring fine-grained content control, and maintaining deep semantic consistency. Existing T2I models often struggle with tasks like accurate text rendering, precise pose generation, or intricate compositional coherence. Concurrently, Vision-Language Models (LVLMs) have demonstrated powerful capabilities in cross-modal understanding and instruction following. We propose LumiGen, a novel LVLM-enhanced iterative framework designed to elevate T2I model performance, particularly in areas requiring fine-grained control, through a closed-loop, LVLM-driven feedback mechanism. LumiGen comprises an Intelligent Prompt Parsing &amp; Augmentation (IPPA) module for proactive prompt enhancement and an Iterative Visual Feedback &amp; Refinement (IVFR) module, which acts as a â€œvisual criticâ€ to iteratively correct and optimize generated images. Evaluated on the challenging LongBench-T2I Benchmark, LumiGen achieves a superior average score of 3.08, outperforming state-of-the-art baselines. Notably, our framework demonstrates significant improvements in critical dimensions such as text rendering and pose expression, validating the effectiveness of LVLM integration for more controllable and higher-quality image generation. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆåœ¨æ‰©æ•£æ¨¡å‹çš„æ¨åŠ¨ä¸‹å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†åœ¨å¤„ç†å¤æ‚æŒ‡ä»¤ã€ç¡®ä¿ç²¾ç»†å†…å®¹æ§åˆ¶å’Œä¿æŒæ·±åº¦è¯­ä¹‰ä¸€è‡´æ€§æ–¹é¢ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚ç°æœ‰çš„T2Iæ¨¡å‹å¾€å¾€éš¾ä»¥å®Œæˆå‡†ç¡®æ–‡æœ¬æ¸²æŸ“ã€ç²¾ç¡®å§¿åŠ¿ç”Ÿæˆæˆ–å¤æ‚ç»„åˆè¿è´¯æ€§ç­‰ä»»åŠ¡ã€‚åŒæ—¶ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è·¨æ¨¡æ€ç†è§£å’ŒæŒ‡ä»¤éµå¾ªæ–¹é¢è¡¨ç°å‡ºäº†å¼ºå¤§çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†LumiGenï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹LVLMå¢å¼ºè¿­ä»£æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é—­ç¯LVLMé©±åŠ¨åé¦ˆæœºåˆ¶æé«˜T2Iæ¨¡å‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç²¾ç»†æ§åˆ¶é¢†åŸŸã€‚LumiGenåŒ…å«ä¸€ä¸ªæ™ºèƒ½æç¤ºè§£æä¸å¢å¼ºï¼ˆIPPAï¼‰æ¨¡å—ï¼Œç”¨äºä¸»åŠ¨æç¤ºå¢å¼ºï¼Œä»¥åŠä¸€ä¸ªè¿­ä»£è§†è§‰åé¦ˆä¸ç»†åŒ–ï¼ˆIVFRï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—å……å½“â€œè§†è§‰è¯„è®ºå®¶â€è§’è‰²ï¼Œä»¥è¿­ä»£æ–¹å¼çº æ­£å’Œä¼˜åŒ–ç”Ÿæˆçš„å›¾åƒã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„LongBench-T2IåŸºå‡†æµ‹è¯•ä¸Šï¼ŒLumiGenå–å¾—äº†å¹³å‡å¾—åˆ†3.08çš„ä¼˜å¼‚æˆç»©ï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å…³é”®ç»´åº¦ï¼ˆå¦‚æ–‡æœ¬æ¸²æŸ“å’Œå§¿åŠ¿è¡¨è¾¾ï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼ŒéªŒè¯äº†LVLMé›†æˆå¯¹äºæ›´å¯æ§å’Œæ›´é«˜è´¨é‡å›¾åƒç”Ÿæˆçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04732v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´å¤æ‚æŒ‡ä»¤å¤„ç†ã€ç²¾ç»†å†…å®¹æ§åˆ¶å’Œæ·±åº¦è¯­ä¹‰ä¸€è‡´æ€§ç­‰æŒ‘æˆ˜ã€‚ç°æœ‰æ¨¡å‹åœ¨å¤„ç†ç²¾ç¡®æ–‡æœ¬æ¸²æŸ“ã€å§¿æ€ç”Ÿæˆå’Œå¤æ‚æ„å›¾è¿è´¯æ€§ç­‰æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LumiGenï¼Œä¸€ä¸ªç”±è§†è§‰è¯­è¨€æ¨¡å‹å¢å¼ºçš„è¿­ä»£æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡é—­ç¯çš„åé¦ˆæœºåˆ¶æé«˜æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ€§èƒ½ã€‚LumiGenåŒ…æ‹¬æ™ºèƒ½æç¤ºè§£æå’Œå¢å¼ºæ¨¡å—ä»¥åŠè¿­ä»£è§†è§‰åé¦ˆå’Œä¼˜åŒ–æ¨¡å—ï¼Œèƒ½å¤Ÿåœ¨ç”Ÿæˆå›¾åƒæ—¶çº æ­£å’Œä¼˜åŒ–ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„LongBench-T2IåŸºå‡†æµ‹è¯•ä¸­ï¼ŒLumiGenè·å¾—äº†3.08çš„å¹³å‡åˆ†æ•°ï¼Œè¶…è¿‡äº†ç°æœ‰çš„æœ€æ–°æŠ€æœ¯ã€‚éªŒè¯äº†æˆ‘ä»¬æ•´åˆè§†è§‰è¯­è¨€æ¨¡å‹æé«˜å›¾åƒç”Ÿæˆæ§åˆ¶æ€§å’Œè´¨é‡çš„æ•ˆèƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢è¿›å±•æ˜¾è‘—ï¼Œä½†å­˜åœ¨å¤„ç†å¤æ‚æŒ‡ä»¤ã€ç²¾ç»†å†…å®¹æ§åˆ¶å’Œæ·±åº¦è¯­ä¹‰ä¸€è‡´æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨æ–‡æœ¬æ¸²æŸ“ã€å§¿æ€ç”Ÿæˆå’Œæ„å›¾è¿è´¯æ€§æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>LumiGenæ˜¯ä¸€ä¸ªæ–°é¢–çš„è¿­ä»£æ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>LumiGenåŒ…æ‹¬æ™ºèƒ½æç¤ºè§£æå’Œå¢å¼ºæ¨¡å—ï¼Œä»¥åŠä¸€ä¸ªè§†è§‰åé¦ˆå’Œä¼˜åŒ–æ¨¡å—ï¼Œç”¨äºçº æ­£å’Œä¼˜åŒ–ç”Ÿæˆçš„å›¾åƒã€‚</li>
<li>åœ¨LongBench-T2IåŸºå‡†æµ‹è¯•ä¸­ï¼ŒLumiGenè¡¨ç°ä¼˜è¶Šï¼Œè¯æ˜äº†æ•´åˆè§†è§‰è¯­è¨€æ¨¡å‹èƒ½æœ‰æ•ˆæé«˜å›¾åƒç”Ÿæˆçš„æ§åˆ¶æ€§å’Œè´¨é‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04732">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e11cd18a40e5e4a7b1997fa50cebbae2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Personalized-Safety-Alignment-for-Text-to-Image-Diffusion-Models"><a href="#Personalized-Safety-Alignment-for-Text-to-Image-Diffusion-Models" class="headerlink" title="Personalized Safety Alignment for Text-to-Image Diffusion Models"></a>Personalized Safety Alignment for Text-to-Image Diffusion Models</h2><p><strong>Authors:Yu Lei, Jinbin Bai, Qingyu Shi, Aosong Feng, Kaidong Yu</strong></p>
<p>Text-to-image diffusion models have revolutionized visual content generation, but current safety mechanisms apply uniform standards that often fail to account for individual user preferences. These models overlook the diverse safety boundaries shaped by factors like age, mental health, and personal beliefs. To address this, we propose Personalized Safety Alignment (PSA), a framework that allows user-specific control over safety behaviors in generative models. PSA integrates personalized user profiles into the diffusion process, adjusting the modelâ€™s behavior to match individual safety preferences while preserving image quality. We introduce a new dataset, Sage, which captures user-specific safety preferences and incorporates these profiles through a cross-attention mechanism. Experiments show that PSA outperforms existing methods in harmful content suppression and aligns generated content better with user constraints, achieving higher Win Rate and Pass Rate scores. Our code, data, and models are publicly available at <a target="_blank" rel="noopener" href="https://m-e-agi-lab.github.io/PSAlign/">https://m-e-agi-lab.github.io/PSAlign/</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹å·²ç»å½»åº•æ”¹å˜äº†è§†è§‰å†…å®¹ç”Ÿæˆçš„æ–¹å¼ï¼Œä½†å½“å‰çš„å®‰å…¨æœºåˆ¶é‡‡ç”¨ç»Ÿä¸€æ ‡å‡†ï¼Œå¾€å¾€æœªèƒ½è€ƒè™‘åˆ°ç”¨æˆ·çš„ä¸ªäººåå¥½ã€‚è¿™äº›æ¨¡å‹å¿½è§†äº†ç”±å¹´é¾„ã€å¿ƒç†å¥åº·å’Œä¸ªäººä¿¡ä»°ç­‰å› ç´ å½¢æˆçš„å¤šæ ·åŒ–çš„å®‰å…¨è¾¹ç•Œã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸ªæ€§åŒ–å®‰å…¨å¯¹é½ï¼ˆPSAï¼‰æ¡†æ¶ï¼Œå…è®¸ç”¨æˆ·å¯¹ç”Ÿæˆæ¨¡å‹ä¸­çš„å®‰å…¨è¡Œä¸ºè¿›è¡Œç‰¹å®šæ§åˆ¶ã€‚PSAå°†ä¸ªæ€§åŒ–çš„ç”¨æˆ·é…ç½®æ–‡ä»¶é›†æˆåˆ°æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œè°ƒæ•´æ¨¡å‹çš„è¡Œä¸ºä»¥åŒ¹é…ä¸ªäººçš„å®‰å…¨åå¥½ï¼ŒåŒæ—¶ä¿æŒå›¾åƒè´¨é‡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†Sageï¼Œå®ƒæ•æ‰ç”¨æˆ·ç‰¹å®šçš„å®‰å…¨åå¥½ï¼Œå¹¶é€šè¿‡äº¤å‰æ³¨æ„æœºåˆ¶èå…¥è¿™äº›é…ç½®æ–‡ä»¶ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æŠ‘åˆ¶æœ‰å®³å†…å®¹å’Œä½¿ç”Ÿæˆå†…å®¹ä¸ç”¨æˆ·çº¦æŸå¯¹é½æ–¹é¢ï¼ŒPSAä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜çš„èƒœç‡å’Œé€šè¿‡ç‡ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://m-e-agi-lab.github.io/PSAlign/">https://m-e-agi-lab.github.io/PSAlign/</a>å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01151v2">PDF</a> metadata-only revision; corrected a typo in the abstract. No changes   to the PDF content</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹å·²ç»å®ç°äº†è§†è§‰å†…å®¹ç”Ÿæˆé¢†åŸŸçš„é©å‘½æ€§è¿›å±•ï¼Œä½†å½“å‰çš„å®‰å…¨æœºåˆ¶é‡‡ç”¨ç»Ÿä¸€æ ‡å‡†ï¼Œå¾€å¾€å¿½è§†ä¸ªä½“å·®å¼‚ï¼Œå¦‚å¹´é¾„ã€å¿ƒç†å¥åº·å’Œä¸ªäººä¿¡ä»°ç­‰å½¢æˆçš„ä¸åŒå®‰å…¨è¾¹ç•Œã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸ªæ€§åŒ–å®‰å…¨å¯¹é½ï¼ˆPSAï¼‰æ¡†æ¶ï¼Œå…è®¸ç”¨æˆ·ç‰¹å®šæ§åˆ¶ç”Ÿæˆæ¨¡å‹ä¸­çš„å®‰å…¨è¡Œä¸ºã€‚PSAå°†ä¸ªæ€§åŒ–ç”¨æˆ·é…ç½®æ–‡ä»¶é›†æˆåˆ°æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡è°ƒæ•´æ¨¡å‹è¡Œä¸ºæ¥åŒ¹é…ä¸ªäººå®‰å…¨åå¥½ï¼ŒåŒæ—¶ä¿ç•™å›¾åƒè´¨é‡ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†Sageï¼Œå®ƒé€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶æ•æ‰ç”¨æˆ·ç‰¹å®šçš„å®‰å…¨åå¥½ï¼Œå¹¶å°†è¿™äº›é…ç½®æ–‡ä»¶çº³å…¥å…¶ä¸­ã€‚å®éªŒè¡¨æ˜ï¼ŒPSAåœ¨æŠ‘åˆ¶æœ‰å®³å†…å®¹å’Œä½¿ç”Ÿæˆå†…å®¹ä¸ç”¨æˆ·çº¦æŸå¯¹é½æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜çš„èƒœç‡å’Œé€šè¿‡ç‡ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://m-e-agi-lab.github.io/PSAlign/%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://m-e-agi-lab.github.io/PSAlign/å…¬å¼€è®¿é—®ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨è§†è§‰å†…å®¹ç”Ÿæˆä¸­æœ‰é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰å®‰å…¨æœºåˆ¶å¿½ç•¥ç”¨æˆ·ä¸ªä½“å·®å¼‚ã€‚</li>
<li>æå‡ºä¸ªæ€§åŒ–å®‰å…¨å¯¹é½ï¼ˆPSAï¼‰æ¡†æ¶ï¼Œç»“åˆç”¨æˆ·ç‰¹å®šé…ç½®æ–‡ä»¶ï¼Œè°ƒæ•´æ¨¡å‹è¡Œä¸ºä»¥åŒ¹é…ä¸ªäººå®‰å…¨åå¥½ã€‚</li>
<li>å¼•å…¥æ–°çš„æ•°æ®é›†Sageï¼Œé€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶æ•æ‰ç”¨æˆ·å®‰å…¨åå¥½ã€‚</li>
<li>PSAåœ¨æŠ‘åˆ¶æœ‰å®³å†…å®¹å’Œç”¨æˆ·çº¦æŸå¯¹é½æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚<br>5.PSAèƒ½æé«˜èƒœç‡å’Œé€šè¿‡ç‡ã€‚</li>
<li>ç ”ç©¶è€…çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å·²å…¬å¼€åˆ†äº«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01151">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ada2ad8248098cdb0e0cb211cd6214f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31a8b7c8eee0dae53531c06e1e48f437.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6be629e5f7645295da7b03313afaaf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c930f44235dcc2044c0bb37e71747a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0fd833250012d250004c1d5cd3cc8ea7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5abc55795f938c615b00464f6bc65d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6532cce1ca0c87d34b017d74a9e436b7.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="WeatherEdit-Controllable-Weather-Editing-with-4D-Gaussian-Field"><a href="#WeatherEdit-Controllable-Weather-Editing-with-4D-Gaussian-Field" class="headerlink" title="WeatherEdit: Controllable Weather Editing with 4D Gaussian Field"></a>WeatherEdit: Controllable Weather Editing with 4D Gaussian Field</h2><p><strong>Authors:Chenghao Qian, Wenjing Li, Yuhu Guo, Gustav Markkula</strong></p>
<p>In this work, we present WeatherEdit, a novel weather editing pipeline for generating realistic weather effects with controllable types and severity in 3D scenes. Our approach is structured into two key components: weather background editing and weather particle construction. For weather background editing, we introduce an all-in-one adapter that integrates multiple weather styles into a single pretrained diffusion model, enabling the generation of diverse weather effects in 2D image backgrounds. During inference, we design a Temporal-View (TV-) attention mechanism that follows a specific order to aggregate temporal and spatial information, ensuring consistent editing across multi-frame and multi-view images. To construct the weather particles, we first reconstruct a 3D scene using the edited images and then introduce a dynamic 4D Gaussian field to generate snowflakes, raindrops and fog in the scene. The attributes and dynamics of these particles are precisely controlled through physical-based modelling and simulation, ensuring realistic weather representation and flexible severity adjustments. Finally, we integrate the 4D Gaussian field with the 3D scene to render consistent and highly realistic weather effects. Experiments on multiple driving datasets demonstrate that WeatherEdit can generate diverse weather effects with controllable condition severity, highlighting its potential for autonomous driving simulation in adverse weather. See project page: <a target="_blank" rel="noopener" href="https://jumponthemoon.github.io/w-edit">https://jumponthemoon.github.io/w-edit</a> </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†WeatherEditï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤©æ°”ç¼–è¾‘ç®¡é“ï¼Œç”¨äºåœ¨3Dåœºæ™¯ç”Ÿæˆå…·æœ‰å¯æ§ç±»å‹å’Œä¸¥é‡ç¨‹åº¦çš„ç°å®å¤©æ°”æ•ˆæœã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šå¤©æ°”èƒŒæ™¯ç¼–è¾‘å’Œå¤©æ°”ç²’å­æ„å»ºã€‚å¯¹äºå¤©æ°”èƒŒæ™¯ç¼–è¾‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨èƒ½é€‚é…å™¨ï¼Œå°†å¤šç§å¤©æ°”é£æ ¼é›†æˆåˆ°ä¸€ä¸ªé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»è€Œåœ¨2Då›¾åƒèƒŒæ™¯ä¸­ç”Ÿæˆå¤šç§å¤©æ°”æ•ˆæœã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ—¶é—´è§†å›¾ï¼ˆTVï¼‰æ³¨æ„åŠ›æœºåˆ¶ï¼ŒæŒ‰ç…§ç‰¹å®šé¡ºåºèšåˆæ—¶é—´å’Œç©ºé—´ä¿¡æ¯ï¼Œç¡®ä¿è·¨å¤šå¸§å’Œå¤šè§†å›¾å›¾åƒçš„ä¸€è‡´ç¼–è¾‘ã€‚ä¸ºäº†æ„å»ºå¤©æ°”ç²’å­ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ç¼–è¾‘åçš„å›¾åƒé‡å»º3Dåœºæ™¯ï¼Œç„¶åå¼•å…¥åŠ¨æ€4Dé«˜æ–¯åœºæ¥åœ¨åœºæ™¯ä¸­ç”Ÿæˆé›ªèŠ±ã€é›¨æ»´å’Œé›¾ã€‚è¿™äº›ç²’å­çš„å±æ€§å’ŒåŠ¨æ€é€šè¿‡åŸºäºç‰©ç†çš„å»ºæ¨¡å’Œæ¨¡æ‹Ÿè¿›è¡Œç²¾ç¡®æ§åˆ¶ï¼Œç¡®ä¿çœŸå®çš„å¤©æ°”è¡¨ç°å’Œçµæ´»çš„ä¸¥é‡ç¨‹åº¦è°ƒæ•´ã€‚æœ€åï¼Œæˆ‘ä»¬å°†4Dé«˜æ–¯åœºä¸3Dåœºæ™¯é›†æˆåœ¨ä¸€èµ·ï¼Œå‘ˆç°ä¸€è‡´ä¸”é«˜åº¦ç°å®çš„å¤©æ°”æ•ˆæœã€‚åœ¨å¤šä¸ªé©¾é©¶æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒWeatherEditå¯ä»¥ç”Ÿæˆå…·æœ‰å¯æ§æ¡ä»¶ä¸¥é‡ç¨‹åº¦çš„å¤šç§å¤©æ°”æ•ˆæœï¼Œçªæ˜¾å…¶åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹è‡ªåŠ¨é©¾é©¶æ¨¡æ‹Ÿçš„æ½œåŠ›ã€‚æ›´å¤šè¯¦æƒ…ï¼Œè¯·è®¿é—®é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://jumponthemoon.github.io/w-edit">https://jumponthemoon.github.io/w-edit</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.20471v3">PDF</a> </p>
<p><strong>Summary</strong><br>å¤©æ°”ç¼–è¾‘æ˜¯ä¸€é¡¹æŠ€æœ¯ã€‚åœ¨è¯¥å·¥ä½œä¸­æå‡ºäº†ä¸€ç§å…¨æ–°çš„å¤©æ°”ç¼–è¾‘æµç¨‹WeatherEditï¼Œç”ŸæˆçœŸå®å¯æ§åˆ¶çš„å¤©æ°”åœºæ™¯ä¸­çš„ç‰¹æ•ˆåœºæ™¯å’Œä¸åŒç±»å‹å¤©æ°”çš„æ•ˆæœã€‚é€šè¿‡å¤©æ°”èƒŒæ™¯ç¼–è¾‘å’Œå¤©æ°”ç²’å­æ„å»ºä¸¤ä¸ªå…³é”®ç»„ä»¶æ¥å®ç°ã€‚é‡‡ç”¨ä¸€ç§é›†æˆå¤šç§å¤©æ°”é£æ ¼çš„å•ä¸€é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œè®¾è®¡äº†ä¸€ç§ç‰¹å®šçš„æ—¶é—´åºå¶-ç©ºé—´åºåˆ—ï¼Œå½¢æˆåœ¨å¤šè§’åº¦æˆåƒå’Œä¸åŒå¸§æ•°ä¸­è¿›è¡ŒæŒç»­ä¸€è‡´æ€§ç¼–è¾‘çš„æŠ€æœ¯æœºåˆ¶ï¼Œæ¥åœ¨ä¸‰ç»´åœºæ™¯æ¨¡æ‹Ÿå†ç°æ›´è‡ªç„¶çš„å¤©æ°”å˜åŒ–å’Œè°ƒæ•´ç‰¹æ•ˆå¼ºå¼±ç¨‹åº¦ã€‚åˆ©ç”¨é‡æ–°æ„é€ çš„ä¸‰ç»´åœºæ™¯åˆ›å»ºåŠ¨æ€çš„é›ªã€é›¨é›¾ç²’å­æ•ˆæœç­‰ã€‚é¡¹ç›®åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://jumponthemoon.github.io/w-edit%E3%80%82">https://jumponthemoon.github.io/w-editã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.20471">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-27e50aadd205924a43db2f3d4dc5f441.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c99a8ffee739b77f869b9ad2d2b52176.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bee1a084a3ebd9e7cbe8ba898baec8d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-19cd95c62322c30f0498c4ffd291e25b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f28456988bc0fbddfe25fd675c35011e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f68f27ff3204dad2c5f98614b05511ff.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GaSLight-Gaussian-Splats-for-Spatially-Varying-Lighting-in-HDR"><a href="#GaSLight-Gaussian-Splats-for-Spatially-Varying-Lighting-in-HDR" class="headerlink" title="GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR"></a>GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR</h2><p><strong>Authors:Christophe Bolduc, Yannick Hold-Geoffroy, Zhixin Shu, Jean-FranÃ§ois Lalonde</strong></p>
<p>We present GaSLight, a method that generates spatially-varying lighting from regular images. Our method proposes using HDR Gaussian Splats as light source representation, marking the first time regular images can serve as light sources in a 3D renderer. Our two-stage process first enhances the dynamic range of images plausibly and accurately by leveraging the priors embedded in diffusion models. Next, we employ Gaussian Splats to model 3D lighting, achieving spatially variant lighting. Our approach yields state-of-the-art results on HDR estimations and their applications in illuminating virtual objects and scenes. To facilitate the benchmarking of images as light sources, we introduce a novel dataset of calibrated and unsaturated HDR to evaluate images as light sources. We assess our method using a combination of this novel dataset and an existing dataset from the literature. Project page: <a target="_blank" rel="noopener" href="https://lvsn.github.io/gaslight/">https://lvsn.github.io/gaslight/</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†GaSLightæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä»å¸¸è§„å›¾åƒç”Ÿæˆç©ºé—´å˜åŒ–çš„å…‰ç…§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºè®®ä½¿ç”¨HDRé«˜æ–¯Splatsä½œä¸ºå…‰æºè¡¨ç¤ºï¼Œè¿™æ˜¯é¦–æ¬¡å°†å¸¸è§„å›¾åƒä½œä¸º3Dæ¸²æŸ“å™¨çš„å…‰æºã€‚æˆ‘ä»¬çš„ä¸¤é˜¶æ®µè¿‡ç¨‹é¦–å…ˆåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¸­çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»¥åˆç†ä¸”å‡†ç¡®çš„æ–¹å¼å¢å¼ºå›¾åƒçš„åŠ¨æ€èŒƒå›´ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨é«˜æ–¯Splatså¯¹3Dç…§æ˜è¿›è¡Œå»ºæ¨¡ï¼Œä»¥å®ç°ç©ºé—´å˜åŒ–çš„å…‰ç…§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨HDRä¼°è®¡åŠå…¶åº”ç”¨äºç…§æ˜è™šæ‹Ÿå¯¹è±¡å’Œåœºæ™¯æ–¹é¢äº§ç”Ÿäº†æœ€å…ˆè¿›çš„æˆæœã€‚ä¸ºäº†å°†å›¾åƒä½œä¸ºåŸºå‡†å…‰æºè¿›è¡Œè¡¡é‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°å‹æ ¡å‡†ä¸”ä¸é¥±å’Œçš„HDRæ•°æ®é›†æ¥è¯„ä¼°å›¾åƒä½œä¸ºå…‰æºã€‚æˆ‘ä»¬ä½¿ç”¨è¿™ç§æ–°å‹æ•°æ®é›†å’Œæ–‡çŒ®ä¸­çš„ç°æœ‰æ•°æ®é›†æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://lvsn.github.io/gaslight/">https://lvsn.github.io/gaslight/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10809v3">PDF</a> </p>
<p><strong>Summary</strong><br>     æˆ‘ä»¬æå‡ºäº†GaSLightæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥ä»å¸¸è§„å›¾åƒç”Ÿæˆç©ºé—´å˜åŒ–çš„å…‰ç…§ã€‚è¯¥æ–¹æ³•ä½¿ç”¨HDRé«˜æ–¯Splatsä½œä¸ºå…‰æºè¡¨ç¤ºï¼Œè¿™æ˜¯é¦–æ¬¡å°†å¸¸è§„å›¾åƒä½œä¸ºå…‰æºç”¨äºä¸‰ç»´æ¸²æŸ“ã€‚æˆ‘ä»¬çš„ä¸¤é˜¶æ®µè¿‡ç¨‹é¦–å…ˆåˆ©ç”¨æ‰©æ•£æ¨¡å‹ä¸­çš„å…ˆéªŒçŸ¥è¯†ï¼Œä»¥åˆç†ä¸”å‡†ç¡®çš„æ–¹å¼å¢å¼ºå›¾åƒçš„åŠ¨æ€èŒƒå›´ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨é«˜æ–¯Splatså¯¹ä¸‰ç»´ç…§æ˜è¿›è¡Œå»ºæ¨¡ï¼Œå®ç°ç©ºé—´å˜åŒ–çš„å…‰ç…§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨HDRä¼°è®¡åŠå…¶ç”¨äºç…§æ˜è™šæ‹Ÿå¯¹è±¡å’Œåœºæ™¯æ–¹é¢çš„åº”ç”¨æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœã€‚ä¸ºäº†å¯¹å›¾åƒä½œä¸ºå…‰æºè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°å‹çš„æ ¡å‡†å’Œä¸é¥±å’ŒHDRæ•°æ®é›†æ¥è¯„ä¼°å›¾åƒä½œä¸ºå…‰æºçš„æ•ˆæœã€‚æˆ‘ä»¬ç”¨è¯¥æ–°å‹æ•°æ®é›†å’Œæ–‡çŒ®ä¸­çš„ç°æœ‰æ•°æ®é›†æ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GaSLightæ–¹æ³•èƒ½å¤Ÿä»å¸¸è§„å›¾åƒç”Ÿæˆç©ºé—´å˜åŒ–çš„å…‰ç…§ã€‚</li>
<li>è¯¥æ–¹æ³•é¦–æ¬¡å°†å¸¸è§„å›¾åƒç”¨ä½œå…‰æºåœ¨ä¸‰ç»´æ¸²æŸ“ä¸­ã€‚</li>
<li>GaSLighté‡‡ç”¨ä¸¤é˜¶æ®µå¤„ç†è¿‡ç¨‹ï¼Œé¦–å…ˆå¢å¼ºå›¾åƒåŠ¨æ€èŒƒå›´ï¼Œç„¶ååˆ©ç”¨é«˜æ–¯Splatså»ºæ¨¡ä¸‰ç»´ç…§æ˜ã€‚</li>
<li>è¾¾åˆ°äº†HDRä¼°è®¡åŠå…¶åº”ç”¨äºè™šæ‹Ÿå¯¹è±¡å’Œåœºæ™¯ç…§æ˜çš„æœ€æ–°æ°´å¹³ç»“æœã€‚</li>
<li>ä¸ºäº†è¯„ä¼°å›¾åƒä½œä¸ºå…‰æºçš„æ•ˆæœï¼Œå¼•å…¥äº†æ–°å‹æ ¡å‡†å’Œä¸é¥±å’ŒHDRæ•°æ®é›†ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†æ–°å‹æ•°æ®é›†å’Œç°æœ‰æ•°æ®é›†è¿›è¡Œè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-06e1091965dc47bb66020a46fd9753d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9c7a000655e37ae9119ea49d8a00238.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16a1d1f424ea7d80a1e1fed27f8ee113.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b170d43e190ff1f4d2e5e50705814a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-237b280e3a6af7ce6e29d6143c2b51d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb1c392fedec80e5ec99173df203178.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df8be25bcee09780fccfbb3e5c2e0bcb.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Repurposing-2D-Diffusion-Models-with-Gaussian-Atlas-for-3D-Generation"><a href="#Repurposing-2D-Diffusion-Models-with-Gaussian-Atlas-for-3D-Generation" class="headerlink" title="Repurposing 2D Diffusion Models with Gaussian Atlas for 3D Generation"></a>Repurposing 2D Diffusion Models with Gaussian Atlas for 3D Generation</h2><p><strong>Authors:Tiange Xiang, Kai Li, Chengjiang Long, Christian HÃ¤ne, Peihong Guo, Scott Delp, Ehsan Adeli, Li Fei-Fei</strong></p>
<p>Recent advances in text-to-image diffusion models have been driven by the increasing availability of paired 2D data. However, the development of 3D diffusion models has been hindered by the scarcity of high-quality 3D data, resulting in less competitive performance compared to their 2D counterparts. To address this challenge, we propose repurposing pre-trained 2D diffusion models for 3D object generation. We introduce Gaussian Atlas, a novel representation that utilizes dense 2D grids, enabling the fine-tuning of 2D diffusion models to generate 3D Gaussians. Our approach demonstrates successful transfer learning from a pre-trained 2D diffusion model to a 2D manifold flattened from 3D structures. To support model training, we compile GaussianVerse, a large-scale dataset comprising 205K high-quality 3D Gaussian fittings of various 3D objects. Our experimental results show that text-to-image diffusion models can be effectively adapted for 3D content generation, bridging the gap between 2D and 3D modeling. </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è¿›å±•å¾—ç›Šäºé…å¯¹äºŒç»´æ•°æ®çš„æ—¥ç›Šæ™®åŠã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹é«˜è´¨é‡çš„ä¸‰ç»´æ•°æ®ï¼Œä¸‰ç»´æ‰©æ•£æ¨¡å‹çš„å‘å±•å—åˆ°äº†é˜»ç¢ï¼Œå¯¼è‡´å…¶æ€§èƒ½è¾ƒäºŒç»´æ¨¡å‹ç«äº‰åŠ›è¾ƒå¼±ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºå°†é¢„è®­ç»ƒçš„äºŒç»´æ‰©æ•£æ¨¡å‹é‡æ–°ç”¨äºä¸‰ç»´å¯¹è±¡ç”Ÿæˆã€‚æˆ‘ä»¬å¼•å…¥äº†é«˜æ–¯å›¾è°±è¿™ä¸€æ–°å‹è¡¨ç°æ–¹å¼ï¼Œå®ƒåˆ©ç”¨å¯†é›†äºŒç»´ç½‘æ ¼ï¼Œèƒ½å¤Ÿä½¿äºŒç»´æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥ç”Ÿæˆä¸‰ç»´é«˜æ–¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸå®ç°äº†ä»é¢„è®­ç»ƒçš„äºŒç»´æ‰©æ•£æ¨¡å‹åˆ°ç”±ä¸‰ç»´ç»“æ„å¹³é“ºçš„äºŒç»´æµå½¢ä¸Šçš„è¿ç§»å­¦ä¹ ã€‚ä¸ºäº†æ”¯æŒæ¨¡å‹è®­ç»ƒï¼Œæˆ‘ä»¬ç¼–è¯‘äº†GaussianVerseæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«20.5ä¸‡ä¸ªå„ç§ä¸‰ç»´å¯¹è±¡çš„é«˜è´¨é‡ä¸‰ç»´é«˜æ–¯æ‹Ÿåˆã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°é€‚åº”äºä¸‰ç»´å†…å®¹ç”Ÿæˆï¼Œå¼¥åˆäº†äºŒç»´å’Œä¸‰ç»´å»ºæ¨¡ä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15877v2">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºé¢„è®­ç»ƒçš„äºŒç»´æ‰©æ•£æ¨¡å‹è¿›è¡Œä¸‰ç»´ç‰©ä½“ç”Ÿæˆçš„æŠ€æœ¯å·²é€æ¸å—åˆ°å…³æ³¨ã€‚ç”±äºç¼ºä¹é«˜è´¨é‡çš„ä¸‰ç»´æ•°æ®ï¼Œä¸‰ç»´æ‰©æ•£æ¨¡å‹çš„å‘å±•å—åˆ°é˜»ç¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºå°†é¢„è®­ç»ƒçš„äºŒç»´æ‰©æ•£æ¨¡å‹é‡æ–°ç”¨äºä¸‰ç»´ç‰©ä½“ç”Ÿæˆï¼Œå¹¶å¼•å…¥é«˜æ–¯å›¾è°±ä½œä¸ºæ–°çš„è¡¨ç¤ºæ–¹æ³•ã€‚é€šè¿‡åˆ©ç”¨å¯†é›†äºŒç»´ç½‘æ ¼ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå¾®è°ƒäºŒç»´æ‰©æ•£æ¨¡å‹ä»¥ç”Ÿæˆä¸‰ç»´é«˜æ–¯æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°é€‚åº”ä¸‰ç»´å†…å®¹ç”Ÿæˆï¼Œä»è€Œç¼©å°äº†äºŒç»´å’Œä¸‰ç»´å»ºæ¨¡ä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜è´¨é‡çš„ä¸‰ç»´æ•°æ®ç¨€ç¼ºï¼Œé™åˆ¶äº†ä¸‰ç»´æ‰©æ•£æ¨¡å‹çš„å‘å±•ã€‚</li>
<li>é¢„è®­ç»ƒçš„äºŒç»´æ‰©æ•£æ¨¡å‹å¯ç”¨äºä¸‰ç»´ç‰©ä½“ç”Ÿæˆã€‚</li>
<li>å¼•å…¥é«˜æ–¯å›¾è°±ä½œä¸ºæ–°çš„è¡¨ç¤ºæ–¹æ³•ï¼Œåˆ©ç”¨å¯†é›†äºŒç»´ç½‘æ ¼è¿›è¡Œå¾®è°ƒã€‚</li>
<li>æ„å»ºäº†GaussianVerseæ•°æ®é›†ï¼ŒåŒ…å«205Kä¸ªé«˜è´¨é‡çš„ä¸‰ç»´é«˜æ–¯æ‹Ÿåˆæ•°æ®ã€‚</li>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¯æˆåŠŸé€‚åº”ä¸‰ç»´å†…å®¹ç”Ÿæˆã€‚</li>
<li>æœ¬æ–‡æ–¹æ³•æˆåŠŸå®ç°äº†ä»äºŒç»´åˆ°ä¸‰ç»´å»ºæ¨¡çš„è¿‡æ¸¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15877">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-472b07681422acc4f9ee884eae0da3a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd21da48977960e929021bb05551b527.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc07a13017aaa5aa86c0f05b64a9a49f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e928d55124778316396f58a05f1bea4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3513a298984a882e6d6a7522941a914d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CM-Diff-A-Single-Generative-Network-for-Bidirectional-Cross-Modality-Translation-Diffusion-Model-Between-Infrared-and-Visible-Images"><a href="#CM-Diff-A-Single-Generative-Network-for-Bidirectional-Cross-Modality-Translation-Diffusion-Model-Between-Infrared-and-Visible-Images" class="headerlink" title="CM-Diff: A Single Generative Network for Bidirectional Cross-Modality   Translation Diffusion Model Between Infrared and Visible Images"></a>CM-Diff: A Single Generative Network for Bidirectional Cross-Modality   Translation Diffusion Model Between Infrared and Visible Images</h2><p><strong>Authors:Bin Hu, Chenqiang Gao, Shurui Liu, Junjie Guo, Fang Chen, Fangcen Liu, Junwei Han</strong></p>
<p>Image translation is one of the crucial approaches for mitigating information deficiencies in the infrared and visible modalities, while also facilitating the enhancement of modality-specific datasets. However, existing methods for infrared and visible image translation either achieve unidirectional modality translation or rely on cycle consistency for bidirectional modality translation, which may result in suboptimal performance. In this work, we present the bidirectional cross-modality translation diffusion model (CM-Diff) for simultaneously modeling data distributions in both the infrared and visible modalities. We address this challenge by combining translation direction labels for guidance during training with cross-modality feature control. Specifically, we view the establishment of the mapping relationship between the two modalities as the process of learning data distributions and understanding modality differences, achieved through a novel Bidirectional Diffusion Training (BDT). Additionally, we propose a Statistical Constraint Inference (SCI) to ensure the generated image closely adheres to the data distribution of the target modality. Experimental results demonstrate the superiority of our CM-Diff over state-of-the-art methods, highlighting its potential for generating dual-modality datasets. </p>
<blockquote>
<p>å›¾åƒè½¬æ¢æ˜¯å‡è½»çº¢å¤–å’Œå¯è§æ¨¡æ€ä¿¡æ¯ç¼ºé™·çš„é‡è¦é€”å¾„ä¹‹ä¸€ï¼ŒåŒæ—¶ä¹Ÿæœ‰åŠ©äºå¢å¼ºæ¨¡æ€ç‰¹å®šæ•°æ®é›†ã€‚ç„¶è€Œï¼Œç°æœ‰çš„çº¢å¤–å’Œå¯è§å›¾åƒè½¬æ¢æ–¹æ³•è¦ä¹ˆå®ç°å•å‘æ¨¡æ€è½¬æ¢ï¼Œè¦ä¹ˆä¾èµ–äºå¾ªç¯ä¸€è‡´æ€§è¿›è¡ŒåŒå‘æ¨¡æ€è½¬æ¢ï¼Œè¿™å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŒå‘è·¨æ¨¡æ€è½¬æ¢æ‰©æ•£æ¨¡å‹ï¼ˆCM-Diffï¼‰ï¼Œä»¥åŒæ—¶å»ºæ¨¡çº¢å¤–å’Œå¯è§æ¨¡æ€çš„æ•°æ®åˆ†å¸ƒã€‚æˆ‘ä»¬é€šè¿‡ç»“åˆè®­ç»ƒè¿‡ç¨‹ä¸­çš„è½¬æ¢æ–¹å‘æ ‡ç­¾è¿›è¡Œå¼•å¯¼ä»¥åŠè·¨æ¨¡æ€ç‰¹å¾æ§åˆ¶æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†å»ºç«‹ä¸¤ç§æ¨¡æ€ä¹‹é—´çš„æ˜ å°„å…³ç³»è§†ä¸ºå­¦ä¹ æ•°æ®åˆ†å¸ƒå’Œç†è§£æ¨¡æ€å·®å¼‚çš„è¿‡ç¨‹ï¼Œé€šè¿‡ä¸€ç§æ–°çš„åŒå‘æ‰©æ•£è®­ç»ƒï¼ˆBDTï¼‰æ¥å®ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿè®¡çº¦æŸæ¨ç†ï¼ˆSCIï¼‰æ–¹æ³•ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„å›¾åƒç´§å¯†ç¬¦åˆç›®æ ‡æ¨¡æ€çš„æ•°æ®åˆ†å¸ƒã€‚å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„CM-Diffç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ï¼Œå‡¸æ˜¾äº†å…¶åœ¨ç”ŸæˆåŒæ¨¡æ€æ•°æ®é›†æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09514v2">PDF</a> </p>
<p><strong>Summary</strong><br>     çº¢å¤–ä¸å¯è§å…‰å›¾åƒè½¬æ¢æ˜¯ç¼“è§£ä¿¡æ¯ç¼ºå¤±å’Œå¢å¼ºç‰¹å®šæ¨¡æ€æ•°æ®é›†çš„å…³é”®æ–¹æ³•ã€‚ä½†ç°æœ‰æ–¹æ³•ä¸»è¦å®ç°å•å‘æ¨¡æ€è½¬æ¢æˆ–ä¾èµ–å¾ªç¯ä¸€è‡´æ€§è¿›è¡ŒåŒå‘æ¨¡æ€è½¬æ¢ï¼Œå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚æœ¬ç ”ç©¶æå‡ºäº†åŒå‘è·¨æ¨¡æ€è½¬æ¢æ‰©æ•£æ¨¡å‹ï¼ˆCM-Diffï¼‰ï¼ŒåŒæ—¶å»ºæ¨¡çº¢å¤–å’Œå¯è§å…‰æ¨¡æ€çš„æ•°æ®åˆ†å¸ƒã€‚é€šè¿‡ç»“åˆè®­ç»ƒè¿‡ç¨‹ä¸­çš„è½¬æ¢æ–¹å‘æ ‡ç­¾è¿›è¡Œå¼•å¯¼ï¼Œä»¥åŠæ§åˆ¶è·¨æ¨¡æ€ç‰¹å¾ï¼Œæ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚æ­¤å¤–è¿˜æå‡ºäº†ç»Ÿè®¡çº¦æŸæ¨æ–­ï¼ˆSCIï¼‰ï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒç´§å¯†ç¬¦åˆç›®æ ‡æ¨¡æ€çš„æ•°æ®åˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCM-Diffä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰ç”ŸæˆåŒæ¨¡æ€æ•°æ®é›†çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çº¢å¤–ä¸å¯è§å…‰å›¾åƒè½¬æ¢æ˜¯ç¼“è§£ä¿¡æ¯ç¼ºå¤±å’Œå¢å¼ºæ•°æ®é›†çš„å…³é”®æ–¹æ³•ã€‚</li>
<li>ç°æœ‰å›¾åƒè½¬æ¢æ–¹æ³•å¯èƒ½å­˜åœ¨å•å‘æˆ–å¾ªç¯ä¸€è‡´æ€§çš„é™åˆ¶ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†åŒå‘è·¨æ¨¡æ€è½¬æ¢æ‰©æ•£æ¨¡å‹ï¼ˆCM-Diffï¼‰ï¼ŒåŒæ—¶å»ºæ¨¡çº¢å¤–å’Œå¯è§å…‰æ¨¡æ€çš„æ•°æ®åˆ†å¸ƒã€‚</li>
<li>CM-Diffé€šè¿‡ç»“åˆç¿»è¯‘æ–¹å‘æ ‡ç­¾è¿›è¡Œè®­ç»ƒæŒ‡å¯¼ï¼Œå¹¶æ§åˆ¶è·¨æ¨¡æ€ç‰¹å¾æ¥è§£å†³æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ç»Ÿè®¡çº¦æŸæ¨æ–­ï¼ˆSCIï¼‰ä»¥ç¡®ä¿ç”Ÿæˆçš„å›¾åƒç¬¦åˆç›®æ ‡æ¨¡æ€çš„æ•°æ®åˆ†å¸ƒã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºCM-Diffä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09514">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cdad0a8b9d9ec383a40f231fbdc9558c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3fb8ea46b9bdd873a6d40baebd98061c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d5625370b54a1c1d18502da3e164631.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d20e35a5c763131af23218456bdb7ac.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="PromptDresser-Improving-the-Quality-and-Controllability-of-Virtual-Try-On-via-Generative-Textual-Prompt-and-Prompt-aware-Mask"><a href="#PromptDresser-Improving-the-Quality-and-Controllability-of-Virtual-Try-On-via-Generative-Textual-Prompt-and-Prompt-aware-Mask" class="headerlink" title="PromptDresser: Improving the Quality and Controllability of Virtual   Try-On via Generative Textual Prompt and Prompt-aware Mask"></a>PromptDresser: Improving the Quality and Controllability of Virtual   Try-On via Generative Textual Prompt and Prompt-aware Mask</h2><p><strong>Authors:Jeongho Kim, Hoiyeong Jin, Sunghyun Park, Jaegul Choo</strong></p>
<p>Recent virtual try-on approaches have advanced by finetuning pre-trained text-to-image diffusion models to leverage their powerful generative ability. However, the use of text prompts in virtual try-on remains underexplored. This paper tackles a text-editable virtual try-on task that modifies the clothing based on the provided clothing image while editing the wearing style (e.g., tucking style, fit) according to the text descriptions. In the text-editable virtual try-on, three key aspects exist: (i) designing rich text descriptions for paired person-clothing data to train the model, (ii) addressing the conflicts where textual information of the existing personâ€™s clothing interferes the generation of the new clothing, and (iii) adaptively adjust the inpainting mask aligned with the text descriptions, ensuring proper editing areas while preserving the original personâ€™s appearance irrelevant to the new clothing. To address these aspects, we propose PromptDresser, a text-editable virtual try-on model that leverages large multimodal model (LMM) assistance to enable high-quality and versatile manipulation based on generative text prompts. Our approach utilizes LMMs via in-context learning to generate detailed text descriptions for person and clothing images independently, including pose details and editing attributes using minimal human cost. Moreover, to ensure the editing areas, we adjust the inpainting mask depending on the text prompts adaptively. Our approach enhances text editability while effectively conveying clothing details that are difficult to capture through images alone, leading to improved image quality. Experiments show that PromptDresser significantly outperforms baselines, demonstrating superior text-driven control and versatile clothing manipulation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/rlawjdghek/PromptDresser">https://github.com/rlawjdghek/PromptDresser</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè™šæ‹Ÿè¯•ç©¿æŠ€æœ¯é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä»¥åˆ©ç”¨å…¶å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›å–å¾—äº†è¿›å±•ã€‚ç„¶è€Œï¼Œè™šæ‹Ÿè¯•ç©¿ä¸­ä½¿ç”¨æ–‡æœ¬æç¤ºå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡è§£å†³äº†ä¸€ä¸ªå¯æ–‡æœ¬ç¼–è¾‘çš„è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡æ ¹æ®æä¾›çš„æœè£…å›¾åƒè¿›è¡Œä¿®æ”¹ï¼Œå¹¶æ ¹æ®æ–‡æœ¬æè¿°å¯¹ç©¿ç€é£æ ¼ï¼ˆä¾‹å¦‚ï¼Œé¢†å£æ ·å¼ã€åˆèº«åº¦ï¼‰è¿›è¡Œç¼–è¾‘ã€‚åœ¨å¯æ–‡æœ¬ç¼–è¾‘çš„è™šæ‹Ÿè¯•ç©¿ä¸­ï¼Œå­˜åœ¨ä¸‰ä¸ªå…³é”®æ–¹é¢ï¼šï¼ˆiï¼‰ä¸ºé…å¯¹çš„äººè¡£æ•°æ®è®¾è®¡ä¸°å¯Œçš„æ–‡æœ¬æè¿°ä»¥è®­ç»ƒæ¨¡å‹ï¼Œï¼ˆiiï¼‰è§£å†³ç°æœ‰äººç‰©æœè£…çš„æ–‡æœ¬ä¿¡æ¯ä¸æ–°æœè£…ç”Ÿæˆçš„å†²çªé—®é¢˜ï¼Œï¼ˆiiiï¼‰è‡ªé€‚åº”è°ƒæ•´ä¸æ–‡æœ¬æè¿°å¯¹é½çš„ä¿®å¤æ©ç ï¼Œç¡®ä¿é€‚å½“çš„ç¼–è¾‘åŒºåŸŸåŒæ—¶ä¿ç•™ä¸åŸå§‹äººç‰©å¤–è§‚æ— å…³çš„æ–°æœè£…ã€‚ä¸ºäº†è§£å†³è¿™äº›æ–¹é¢ï¼Œæˆ‘ä»¬æå‡ºäº†PromptDresserï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ–‡æœ¬ç¼–è¾‘çš„è™šæ‹Ÿè¯•ç©¿æ¨¡å‹ï¼Œå®ƒåˆ©ç”¨å¤§å‹å¤šæ¨¡å¼æ¨¡å‹ï¼ˆLMMï¼‰çš„è¾…åŠ©åŠŸèƒ½ï¼Œå®ç°åŸºäºç”Ÿæˆæ–‡æœ¬æç¤ºçš„é«˜è´¨é‡ä¸”å¤šåŠŸèƒ½æ“ä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ åˆ©ç”¨å¤§å‹å¤šæ¨¡å¼æ¨¡å‹ï¼Œä¸ºäººç‰©å’Œæœè£…å›¾åƒç”Ÿæˆè¯¦ç»†çš„æ–‡æœ¬æè¿°ï¼ŒåŒ…æ‹¬å§¿åŠ¿ç»†èŠ‚å’Œç¼–è¾‘å±æ€§ï¼Œå‡ ä¹ä¸éœ€è¦äººåŠ›æˆæœ¬ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç¡®ä¿ç¼–è¾‘åŒºåŸŸï¼Œæˆ‘ä»¬æ ¹æ®æ–‡æœ¬æç¤ºè‡ªé€‚åº”åœ°è°ƒæ•´ä¿®å¤æ©ç ã€‚æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†æ–‡æœ¬çš„å¯ç¼–è¾‘æ€§ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°ä¼ è¾¾äº†ä»…å‡­å›¾åƒéš¾ä»¥æ•æ‰çš„æœè£…ç»†èŠ‚ï¼Œä»è€Œæé«˜äº†å›¾åƒè´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒPromptDresseræ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå±•ç°å‡ºå“è¶Šçš„æ–‡å­—é©±åŠ¨æ§åˆ¶å’Œå¤šæ ·åŒ–çš„æœè£…æ“ä½œã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/rlawjdghek/PromptDresser">https://github.com/rlawjdghek/PromptDresser</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16978v2">PDF</a> 20 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡æœ¬æè¿°è°ƒæ•´æœè£…çš„é£æ ¼ï¼ˆå¦‚ï¼šç©¿è¡£é£æ ¼ã€è´´åˆåº¦ç­‰ï¼‰çš„è™šæ‹Ÿè¯•ç©¿ä»»åŠ¡åœ¨æœ¬æ–‡ä¸­è¢«æ¢è®¨ã€‚ä¸ºæ­¤ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§åä¸ºPromptDresserçš„æ–‡æœ¬å¯ç¼–è¾‘è™šæ‹Ÿè¯•ç©¿æ¨¡å‹ï¼Œå€ŸåŠ©å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰è¿›è¡Œé«˜è´¨é‡ã€çµæ´»çš„æ“ä½œç”Ÿæˆã€‚é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ç”Ÿæˆäººç‰©å’Œæœè£…çš„ç‹¬ç«‹è¯¦ç»†æ–‡æœ¬æè¿°ï¼ŒåŒ…æ‹¬å§¿åŠ¿ç»†èŠ‚å’Œç¼–è¾‘å±æ€§ï¼Œç¡®ä¿ç¼–è¾‘åŒºåŸŸçš„åŒæ—¶ä¿æŒåŸå§‹äººç‰©å¤–è§‚ä¸å—æ–°æœè£…å½±å“ã€‚å®éªŒè¡¨æ˜ï¼ŒPromptDresseræ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ–‡æœ¬é©±åŠ¨æ§åˆ¶å’Œçµæ´»çš„æœè£…æ“ä½œèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æè¿°åœ¨è™šæ‹Ÿè¯•ç©¿ä¸­çš„åº”ç”¨è¢«æ¢è®¨ï¼Œç‰¹åˆ«æ˜¯æ ¹æ®æä¾›çš„æœè£…å›¾åƒä¿®æ”¹ç©¿è¡£é£æ ¼ã€‚</li>
<li>æå‡ºä¸€ç§åä¸ºPromptDresserçš„æ¨¡å‹ï¼Œåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰è¿›è¡Œæ–‡æœ¬å¯ç¼–è¾‘çš„è™šæ‹Ÿè¯•ç©¿ã€‚</li>
<li>é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ç”Ÿæˆè¯¦ç»†çš„æ–‡æœ¬æè¿°ï¼ŒåŒ…æ‹¬å§¿åŠ¿å’Œç¼–è¾‘å±æ€§ï¼Œä»¥æ”¯æŒæ–‡æœ¬ç¼–è¾‘åŠŸèƒ½ã€‚</li>
<li>å€ŸåŠ©æ–‡æœ¬æç¤ºè‡ªé€‚åº”è°ƒæ•´ç»˜ç”»è’™ç‰ˆï¼Œç¡®ä¿ç¼–è¾‘åŒºåŸŸçš„å‡†ç¡®æ€§å¹¶ä¿ç•™åŸå§‹å¤–è§‚ã€‚</li>
<li>PromptDresseræ¨¡å‹åœ¨å®éªŒä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜è¶Šæ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨æ–‡æœ¬é©±åŠ¨çš„æ§åˆ¶å’Œæœè£…æ“ä½œèƒ½åŠ›æ–¹é¢ã€‚</li>
<li>æ¨¡å‹ä»£ç å·²å…¬å¼€å¯è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16978">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ef7b8b3f264abc5a7a3577f9c434497a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8554144c9bb6ffeace7c8587747acde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9e22b9c6f13e1cbce11059ead7ea0f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81bf55ae6897b0e7fbbb8f3d3394ef53.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-c4f3381c9ca1a8d2d3266d998931c981.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-09  MM2CT MR-to-CT translation for multi-modal image fusion with mamba
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-538c718c7f7a929afedcd2bc3a7dafb4.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-09  GTR Improving Large 3D Reconstruction Models through Geometry and   Texture Refinement
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26548.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
