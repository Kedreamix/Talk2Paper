<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-08-09  On the Generalization of SFT A Reinforcement Learning Perspective with   Reward Rectification">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-25a56c6761c0480b91ef3be33a13768d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    79 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-09-更新"><a href="#2025-08-09-更新" class="headerlink" title="2025-08-09 更新"></a>2025-08-09 更新</h1><h2 id="On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification"><a href="#On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification" class="headerlink" title="On the Generalization of SFT: A Reinforcement Learning Perspective with   Reward Rectification"></a>On the Generalization of SFT: A Reinforcement Learning Perspective with   Reward Rectification</h2><p><strong>Authors:Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, Ming-Hsuan Yang, Xu Yang</strong></p>
<p>We present a simple yet theoretically motivated improvement to Supervised Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited generalization compared to reinforcement learning (RL). Through mathematical analysis, we reveal that standard SFT gradients implicitly encode a problematic reward structure that may severely restrict the generalization capabilities of model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing gradient updates for each token by dynamically rescaling the objective function with the probability of this token. Remarkably, this single-line code change significantly outperforms standard SFT across multiple challenging benchmarks and base models, demonstrating greatly improved generalization. Additionally, our approach shows competitive results in offline RL settings, offering an effective yet simpler alternative. This work bridges theoretical insight and practical solutions, substantially advancing SFT performance. The code will be available at <a target="_blank" rel="noopener" href="https://github.com/yongliang-wu/DFT">https://github.com/yongliang-wu/DFT</a>. </p>
<blockquote>
<p>我们对大型语言模型（LLM）的监督微调（SFT）进行了简单但理论上的改进，解决了其与强化学习（RL）相比的有限泛化能力问题。通过数学分析，我们发现标准SFT梯度隐式编码了一个有问题的奖励结构，这可能会严重限制模型的泛化能力。为了纠正这一点，我们提出了动态微调（DFT），通过动态调整目标函数与该令牌的概率来稳定每个令牌的梯度更新。值得注意的是，这一行代码的改动在多个具有挑战性的基准测试和基准模型上显著优于标准的SFT，表现出极大的泛化改进。此外，我们的方法在线下RL设置中也表现出有竞争力的结果，提供了一个有效但更简单的替代方案。这项工作将理论见解与实际解决方案相结合，极大地提高了SFT的性能。代码将在<a target="_blank" rel="noopener" href="https://github.com/yongliang-wu/DFT%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/yongliang-wu/DFT上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05629v1">PDF</a> 14 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对大型语言模型（LLM）的监督微调（SFT）的一种简单而理论上的改进方法，解决了其相对于强化学习（RL）的有限泛化问题。通过数学分析，作者发现标准SFT梯度隐含了一种可能严重限制模型泛化能力的问题奖励结构。为了解决这个问题，作者提出了动态微调（DFT），通过动态调整目标函数的概率来稳定每个标记的梯度更新。这一简单的代码改动在多个具有挑战性的基准测试和基础模型上显著优于标准的SFT，表现出更好的泛化能力。此外，该方法在离线RL设置中也表现出具有竞争力的结果，提供了一种有效且简单的替代方案。这项工作将理论见解和实用解决方案相结合，大大提高了SFT的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>监督微调（SFT）在大型语言模型（LLM）中存在有限泛化问题。</li>
<li>标准SFT梯度隐含了可能限制模型泛化能力的问题奖励结构。</li>
<li>动态微调（DFT）通过动态调整目标函数概率来稳定梯度更新，提高了模型的泛化能力。</li>
<li>DFT在多个基准测试和模型上显著优于标准SFT。</li>
<li>DFT在离线RL设置中也表现出竞争力。</li>
<li>该工作结合了理论分析和实用解决方案，推动了SFT的发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05629">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9cb25f1c0d97726188f92047e12a433e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f08fd390f6bc1a892f430ec45de7d340.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="TrajEvo-Trajectory-Prediction-Heuristics-Design-via-LLM-driven-Evolution"><a href="#TrajEvo-Trajectory-Prediction-Heuristics-Design-via-LLM-driven-Evolution" class="headerlink" title="TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven   Evolution"></a>TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven   Evolution</h2><p><strong>Authors:Zhikai Zhao, Chuanbo Hua, Federico Berto, Kanghoon Lee, Zihan Ma, Jiachen Li, Jinkyoo Park</strong></p>
<p>Trajectory prediction is a critical task in modeling human behavior, especially in safety-critical domains such as social robotics and autonomous vehicle navigation. Traditional heuristics based on handcrafted rules often lack accuracy and generalizability. Although deep learning approaches offer improved performance, they typically suffer from high computational cost, limited explainability, and, importantly, poor generalization to out-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a framework that leverages Large Language Models (LLMs) to automatically design trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to generate and refine prediction heuristics from past trajectory data. We propose two key innovations: Cross-Generation Elite Sampling to encourage population diversity, and a Statistics Feedback Loop that enables the LLM to analyze and improve alternative predictions. Our evaluations demonstrate that TrajEvo outperforms existing heuristic methods across multiple real-world datasets, and notably surpasses both heuristic and deep learning methods in generalizing to an unseen OOD real-world dataset. TrajEvo marks a promising step toward the automated design of fast, explainable, and generalizable trajectory prediction heuristics. We release our source code to facilitate future research at <a target="_blank" rel="noopener" href="https://github.com/ai4co/trajevo">https://github.com/ai4co/trajevo</a>. </p>
<blockquote>
<p>轨迹预测是模拟人类行为的关键任务，特别是在社会机器人和自动驾驶导航等安全关键领域。基于手工制定的传统启发式方法通常缺乏准确性和泛化能力。尽管深度学习方法可以提升性能，但它们通常面临计算成本高、解释性差以及对于分布外（OOD）场景的泛化能力不强等重要问题。在本文中，我们介绍了TrajEvo框架，它利用大型语言模型（LLM）自动设计轨迹预测启发式方法。TrajEvo采用进化算法根据过去的轨迹数据生成和改进预测启发式方法。我们提出了两个关键创新点：跨代精英采样以促进种群多样性，以及一个统计反馈循环，使LLM能够分析和改进替代预测。我们的评估表明，TrajEvo在多个真实世界数据集上的表现优于现有的启发式方法，并且在未见过的OOD真实世界数据集上的泛化性能显著超越了启发式和深度学习方法。TrajEvo标志着朝着自动设计快速、可解释和可泛化的轨迹预测启发式方法迈出了有前景的一步。我们在<a target="_blank" rel="noopener" href="https://github.com/ai4co/trajevo%E5%8F%91%E5%B8%83%E6%88%91%E4%BB%AC%E7%9A%84%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E4%BB%8A%E5%90%8E%E7%9A%84%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/ai4co/trajevo发布我们的源代码，以促进今后的研究。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05616v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2505.04480</p>
<p><strong>Summary</strong></p>
<p>本文介绍了TrajEvo框架，该框架利用大型语言模型（LLM）自动设计轨迹预测启发式方法。TrajEvo采用进化算法生成和细化预测启发式方法，并提出两个关键创新点：跨代精英采样以促进种群多样性，以及统计反馈循环使LLM能够分析和改进替代预测。评估结果表明，TrajEvo在多个真实数据集上的表现优于现有启发式方法，并在未见过的OOD真实数据集上的表现尤为突出。TrajEvo为快速、可解释和通用的轨迹预测启发式方法的自动化设计迈出了重要一步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TrajEvo框架利用大型语言模型（LLM）自动设计轨迹预测启发式方法，以提高准确性和泛化能力。</li>
<li>TrajEvo采用进化算法生成和细化预测启发式方法，通过Cross-Generation Elite Sampling和Statistics Feedback Loop两个关键创新点来提升性能。</li>
<li>跨代精英采样有助于鼓励种群多样性，从而提高预测模型的探索能力和创新能力。</li>
<li>统计反馈循环使LLM能够分析和改进替代预测，从而提高模型的解释性和准确性。</li>
<li>TrajEvo在多个真实数据集上的表现优于传统启发式方法和深度学习方法。</li>
<li>TrajEvo能够很好地泛化到未见过的OOD真实数据集上，显示出其强大的适应性和鲁棒性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05616">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-84e7cbc93b7b06b42fe4a97e9bbd5dab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42f5833fee3b76a9d514febf70af2b84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-298c81aa3e9ec73a129994c170d43a5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3c07295d735a67af19c04a32c478f14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-555a9dde7ab48cca0568ad468e920e6b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Shuffle-R1-Efficient-RL-framework-for-Multimodal-Large-Language-Models-via-Data-centric-Dynamic-Shuffle"><a href="#Shuffle-R1-Efficient-RL-framework-for-Multimodal-Large-Language-Models-via-Data-centric-Dynamic-Shuffle" class="headerlink" title="Shuffle-R1: Efficient RL framework for Multimodal Large Language Models   via Data-centric Dynamic Shuffle"></a>Shuffle-R1: Efficient RL framework for Multimodal Large Language Models   via Data-centric Dynamic Shuffle</h2><p><strong>Authors:Linghao Zhu, Yiran Guan, Dingkang Liang, Jianzhong Ju, Zhenbo Luo, Bin Qin, Jian Luan, Yuliang Liu, Xiang Bai</strong></p>
<p>Reinforcement learning (RL) has emerged as an effective post-training paradigm for enhancing the reasoning capabilities of multimodal large language model (MLLM). However, current RL pipelines often suffer from training inefficiencies caused by two underexplored issues: Advantage Collapsing, where most advantages in a batch concentrate near zero, and Rollout Silencing, where the proportion of rollouts contributing non-zero gradients diminishes over time. These issues lead to suboptimal gradient updates and hinder long-term learning efficiency. To address these issues, we propose Shuffle-R1, a simple yet principled framework that improves RL fine-tuning efficiency by dynamically restructuring trajectory sampling and batch composition. It introduces (1) Pairwise Trajectory Sampling, which selects high-contrast trajectories with large advantages to improve gradient signal quality, and (2) Advantage-based Trajectory Shuffle, which increases exposure of valuable rollouts through informed batch reshuffling. Experiments across multiple reasoning benchmarks show that our framework consistently outperforms strong RL baselines with minimal overhead. These results highlight the importance of data-centric adaptations for more efficient RL training in MLLM. </p>
<blockquote>
<p>强化学习（RL）已经成为提高多模态大型语言模型（MLLM）推理能力的一种有效的后训练范式。然而，当前的强化学习管道往往存在训练效率不高的现象，这主要由两个尚未被充分研究的问题引起：优势崩塌，即一批次中的大部分优势集中在零附近；以及扩展沉默问题，随着时间的推移，生成样本为非零梯度的贡献比例逐渐降低。这些问题导致了次优的梯度更新，并阻碍了长期学习效率的提升。为了解决这个问题，我们提出了Shuffle-R1框架，它通过动态重构轨迹采样和批次组合来提高强化学习的微调效率。它引入了（1）配对轨迹采样，选择具有较大优势的对比轨迹来提高梯度信号质量；（2）基于优势的轨迹洗牌，通过信息丰富的批次重洗牌来增加有价值的生成样本的曝光。在多个推理基准测试上的实验表明，我们的框架始终优于强大的强化学习基线，并且具有最小的额外开销。这些结果强调了以数据为中心的策略对于提高MLLM中的强化学习训练效率的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05612v1">PDF</a> </p>
<p><strong>Summary</strong>：强化学习（RL）作为提升多模态大型语言模型（MLLM）推理能力的后训练范式，效果显著。然而，当前RL管道存在训练效率低下的问题，主要由于优势崩塌和滚动沉默两个未充分探讨的问题。为解决这些问题，提出Shuffle-R1框架，通过动态重构轨迹采样和批次组成来提高RL微调效率。该框架引入成对轨迹采样和基于优势的轨迹洗牌，提高梯度信号质量和有价值的滚动结果的曝光度。实验表明，该框架在多个推理基准测试中始终优于强大的RL基线，且几乎不增加开销。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>强化学习（RL）能有效提升多模态大型语言模型（MLLM）的推理能力。</li>
<li>当前RL管道存在训练效率低下的问题，主要因为优势崩塌和滚动沉默。</li>
<li>Shuffle-R1框架通过动态重构轨迹采样和批次组成来提高RL微调效率。</li>
<li>Shuffle-R1引入成对轨迹采样，选择高对比度的轨迹以提高梯度信号质量。</li>
<li>Shuffle-R1的基于优势的轨迹洗牌增加有价值滚动结果的曝光度。</li>
<li>实验证明Shuffle-R1框架在多个推理基准测试中表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05612">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cc1d3a55c02ed2126dde61d9391bd17f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c2cba9eecf11bf9962322028086f1dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67b94c09311fb76397ad561c6baaba15.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef1e4d871002ce3fa6bd0f2aefc219a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94515193d32f5c47b63d72d74bdc87a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64777269f147350dce43e52793a00a66.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Uni-cot-Towards-Unified-Chain-of-Thought-Reasoning-Across-Text-and-Vision"><a href="#Uni-cot-Towards-Unified-Chain-of-Thought-Reasoning-Across-Text-and-Vision" class="headerlink" title="Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and   Vision"></a>Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and   Vision</h2><p><strong>Authors:Luozheng Qin, Jia Gong, Yuqing Sun, Tianjiao Li, Mengping Yang, Xiaomeng Yang, Chao Qu, Zhiyu Tan, Hao Li</strong></p>
<p>Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large Language Models (LLMs) by decomposing complex tasks into simpler, sequential subtasks. However, extending CoT to vision-language reasoning tasks remains challenging, as it often requires interpreting transitions of visual states to support reasoning. Existing methods often struggle with this due to limited capacity of modeling visual state transitions or incoherent visual trajectories caused by fragmented architectures.   To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought framework that enables coherent and grounded multimodal reasoning within a single unified model. The key idea is to leverage a model capable of both image understanding and generation to reason over visual content and model evolving visual states. However, empowering a unified model to achieve that is non-trivial, given the high computational cost and the burden of training. To address this, Uni-CoT introduces a novel two-level reasoning paradigm: A Macro-Level CoT for high-level task planning and A Micro-Level CoT for subtask execution. This design significantly reduces the computational overhead. Furthermore, we introduce a structured training paradigm that combines interleaved image-text supervision for macro-level CoT with multi-task objectives for micro-level CoT. Together, these innovations allow Uni-CoT to perform scalable and coherent multi-modal reasoning. Furthermore, thanks to our design, all experiments can be efficiently completed using only 8 A100 GPUs with 80GB VRAM each. Experimental results on reasoning-driven image generation benchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT demonstrates SOTA performance and strong generalization, establishing Uni-CoT as a promising solution for multi-modal reasoning. Project Page and Code: <a target="_blank" rel="noopener" href="https://sais-fuxi.github.io/projects/uni-cot/">https://sais-fuxi.github.io/projects/uni-cot/</a> </p>
<blockquote>
<p>链式思维（Chain-of-Thought，简称CoT）推理已被广泛应用于通过分解复杂任务为更简单、连续的子任务来提升大型语言模型（LLM）的性能。然而，将CoT扩展到视觉语言推理任务仍然具有挑战性，因为它通常需要解释视觉状态的过渡来支持推理。现有方法由于建模视觉状态过渡的能力有限或碎片化架构导致的视觉轨迹不一致，往往在这方面表现挣扎。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05606v1">PDF</a> <a target="_blank" rel="noopener" href="https://sais-fuxi.github.io/projects/uni-cot/">https://sais-fuxi.github.io/projects/uni-cot/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了如何将Chain-of-Thought（CoT）推理应用于增强大型语言模型（LLM）的挑战，特别是在视觉语言推理任务中的应用。针对现有方法的局限性，提出了一种统一的Chain-of-Thought框架（Uni-CoT），它能够在单个统一模型中进行连贯和基于地面的多模态推理。该框架引入了一种新颖的两级推理模式，即宏观层面的CoT用于高级任务规划，微观层面的CoT用于子任务执行，以减少计算开销。此外，还提出了一种结构化的训练模式，以支持宏观和微观两个层面的CoT的多任务目标。这些创新使得Uni-CoT在视觉语言推理任务中表现出卓越的性能和强大的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Uni-CoT框架旨在解决将Chain-of-Thought（CoT）推理应用于视觉语言推理任务的挑战。</li>
<li>它通过在单个统一模型中进行连贯和基于地面的多模态推理来实现这一目标。</li>
<li>Uni-CoT引入了一种新颖的两级推理模式，包括宏观层面的任务规划和微观层面的子任务执行。</li>
<li>框架还采用了一种结构化的训练模式来支持宏观和微观CoT的多任务目标。</li>
<li>该框架的计算开销得到了显著降低，使得实验可以在有限的计算资源下完成。</li>
<li>Uni-CoT在视觉语言推理任务中表现出卓越的性能和强大的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05606">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4f8a2849ab458672503129cc4edfe748.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b85ae310a96a848f495fcaea8acd402.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab1d0a8673e7d72f507cd7db7b5bc6da.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-775ba7f7244b3dbbc1d0efa72c338f02.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MathSmith-Towards-Extremely-Hard-Mathematical-Reasoning-by-Forging-Synthetic-Problems-with-a-Reinforced-Policy"><a href="#MathSmith-Towards-Extremely-Hard-Mathematical-Reasoning-by-Forging-Synthetic-Problems-with-a-Reinforced-Policy" class="headerlink" title="MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging   Synthetic Problems with a Reinforced Policy"></a>MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging   Synthetic Problems with a Reinforced Policy</h2><p><strong>Authors:Shaoxiong Zhan, Yanlin Lai, Ziyu Lu, Dahua Lin, Ziqing Yang, Fei Tang</strong></p>
<p>Large language models have achieved substantial progress in mathematical reasoning, yet their advancement is limited by the scarcity of high-quality, high-difficulty training data. Existing synthesis methods largely rely on transforming human-written templates, limiting both diversity and scalability. We propose MathSmith, a novel framework for synthesizing challenging mathematical problems to enhance LLM reasoning. Rather than modifying existing problems, MathSmith constructs new ones from scratch by randomly sampling concept-explanation pairs from PlanetMath, ensuring data independence and avoiding contamination. To increase difficulty, we design nine predefined strategies as soft constraints during rationales. We further adopts reinforcement learning to jointly optimize structural validity, reasoning complexity, and answer consistency. The length of the reasoning trace generated under autoregressive prompting is used to reflect cognitive complexity, encouraging the creation of more demanding problems aligned with long-chain-of-thought reasoning. Experiments across five benchmarks, categorized as easy &amp; medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025, OlympiadBench), show that MathSmith consistently outperforms existing baselines under both short and long CoT settings. Additionally, a weakness-focused variant generation module enables targeted improvement on specific concepts. Overall, MathSmith exhibits strong scalability, generalization, and transferability, highlighting the promise of high-difficulty synthetic data in advancing LLM reasoning capabilities. </p>
<blockquote>
<p>大型语言模型在数学推理方面取得了显著进展，但其进展受限于高质量、高难度训练数据的稀缺性。现有的合成方法大多依赖于转换人工编写的模板，这限制了多样性和可扩展性。我们提出了MathSmith，这是一个合成具有挑战性数学问题以增强大型语言模型推理能力的新型框架。MathSmith不是修改现有问题，而是从零开始构建新问题，通过从PlanetMath随机抽取概念解释对，确保数据独立，避免污染。为了提高难度，我们设计了九种预设策略作为推理过程中的软约束。我们进一步采用强化学习来联合优化结构有效性、推理复杂性和答案一致性。在自动回归提示下生成的推理轨迹长度被用来反映认知复杂性，鼓励创建与长链思维推理相符的更具挑战性的问题。在五个基准测试上的实验，被分类为简单和中等（GSM8K，MATH-500）以及困难（AIME2024，AIME2025，OlympiadBench），表明MathSmith在短链和长链思维设置下均优于现有基准测试。此外，弱点聚焦的变体生成模块能够实现特定概念的针对性改进。总体上，MathSmith表现出强大的可扩展性、通用性和可迁移性，突显了高难度合成数据在提升大型语言模型推理能力方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05592v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大规模语言模型在数学推理方面取得了显著进展，但其发展受限于高质量、高难度训练数据的稀缺性。现有合成方法主要依赖人工编写模板的转换，限制了多样性和可扩展性。本文提出MathSmith框架，通过从PlanetMath中随机采样概念解释对来合成具有挑战性的数学问题，以增强LLM的推理能力。该框架采用强化学习优化结构有效性、推理复杂性和答案一致性。实验结果表明，MathSmith在五个不同难度的基准测试中均表现出出色的性能，并具备强大的可扩展性、通用性和可迁移性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大规模语言模型在数学推理方面已取得进展，但缺乏高质量、高难度的训练数据限制了其进一步发展。</li>
<li>现有数学问题的合成方法主要基于人工编写模板的转换，这限制了多样性和可扩展性。</li>
<li>MathSmith框架通过随机采样概念解释对来合成新的数学问题，确保数据独立，避免污染。</li>
<li>MathSmith采用强化学习来优化结构有效性、推理复杂性和答案一致性。</li>
<li>MathSmith生成的问题与长期思维推理相契合，反映了认知复杂性。</li>
<li>实验结果表明，MathSmith在多个基准测试中表现出色，特别是在困难级别的测试中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05592">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-74c9e73c00a8575183f21ffcb4d9f343.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9636410a6ac706d604480825f40f636a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47117c257600b3dc307d1807363174d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25a56c6761c0480b91ef3be33a13768d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DART-Dual-Adaptive-Refinement-Transfer-for-Open-Vocabulary-Multi-Label-Recognition"><a href="#DART-Dual-Adaptive-Refinement-Transfer-for-Open-Vocabulary-Multi-Label-Recognition" class="headerlink" title="DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label   Recognition"></a>DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label   Recognition</h2><p><strong>Authors:Haijing Liu, Tao Pu, Hefeng Wu, Keze Wang, Liang Lin</strong></p>
<p>Open-Vocabulary Multi-Label Recognition (OV-MLR) aims to identify multiple seen and unseen object categories within an image, requiring both precise intra-class localization to pinpoint objects and effective inter-class reasoning to model complex category dependencies. While Vision-Language Pre-training (VLP) models offer a strong open-vocabulary foundation, they often struggle with fine-grained localization under weak supervision and typically fail to explicitly leverage structured relational knowledge beyond basic semantics, limiting performance especially for unseen classes. To overcome these limitations, we propose the Dual Adaptive Refinement Transfer (DART) framework. DART enhances a frozen VLP backbone via two synergistic adaptive modules. For intra-class refinement, an Adaptive Refinement Module (ARM) refines patch features adaptively, coupled with a novel Weakly Supervised Patch Selecting (WPS) loss that enables discriminative localization using only image-level labels. Concurrently, for inter-class transfer, an Adaptive Transfer Module (ATM) leverages a Class Relationship Graph (CRG), constructed using structured knowledge mined from a Large Language Model (LLM), and employs graph attention network to adaptively transfer relational information between class representations. DART is the first framework, to our knowledge, to explicitly integrate external LLM-derived relational knowledge for adaptive inter-class transfer while simultaneously performing adaptive intra-class refinement under weak supervision for OV-MLR. Extensive experiments on challenging benchmarks demonstrate that our DART achieves new state-of-the-art performance, validating its effectiveness. </p>
<blockquote>
<p>开放词汇多标签识别（OV-MLR）旨在识别图像中的多个已知和未知对象类别，这需要精确的类内定位来精确定位对象，以及有效的类间推理来模拟复杂的类别依赖关系。虽然视觉语言预训练（VLP）模型提供了强大的开放词汇基础，但它们通常在弱监督下难以实现精细的定位，并且通常无法明确利用超越基本语义的结构化关系知识，特别对于未知类别的性能有所限制。为了克服这些局限性，我们提出了双自适应细化传输（DART）框架。DART通过两个协同的自适应模块增强了冻结的VLP骨干网。对于类内细化，自适应细化模块（ARM）自适应地优化补丁特征，并结合新颖的弱监督补丁选择（WPS）损失，仅使用图像级标签实现区分定位。同时，对于类间传输，自适应传输模块（ATM）利用类关系图（CRG），该图使用从大型语言模型（LLM）中挖掘的结构化知识构建，并采用图注意力网络自适应地传输类表示之间的关系信息。据我们所知，DART是第一个明确整合外部LLM派生关系知识的框架，用于自适应的类间传输，同时执行弱监督下的自适应类内细化，以实现OV-MLR。在具有挑战性的基准测试上的广泛实验表明，我们的DART达到了新的最先进的性能，验证了其有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05585v1">PDF</a> Accepted by ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Open-Vocabulary Multi-Label Recognition（OV-MLR）的任务目标，即识别图像中的多个已知和未知对象类别。文章指出，为了实现这一目标，需要精确的类内定位和有效的类间推理。尽管Vision-Language Pre-training（VLP）模型提供了强大的开放词汇基础，但在弱监督下难以实现精细定位，并且未能明确利用结构化的关系知识，这限制了其在未知类别上的性能。为了克服这些局限性，文章提出了Dual Adaptive Refinement Transfer（DART）框架。DART通过两个协同的适应性模块增强了冻结的VLP骨干网。对于类内细化，自适应细化模块（ARM）自适应地优化补丁特征，并结合新的弱监督补丁选择（WPS）损失，使用图像级标签实现判别定位。对于类间转移，自适应转移模块（ATM）利用类关系图（CRG），该图是利用大型语言模型（LLM）挖掘的结构知识构建的，并应用图注意力网络自适应地转移类表示之间的关系信息。据我们所知，DART是第一个明确整合外部LLM派生关系知识以适应类间转移，同时执行弱监督下的自适应类内细化的框架，用于OV-MLR。实验表明，DART取得了新的最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Open-Vocabulary Multi-Label Recognition (OV-MLR) 要求同时实现精确的类内定位和有效的类间推理。</li>
<li>Vision-Language Pre-training (VLP) 模型虽然在开放词汇方面表现良好，但在弱监督下的精细定位方面存在挑战。</li>
<li>DART框架通过两个协同的适应性模块（ARM和ATM）克服了VLP模型的局限性。</li>
<li>ARM模块通过自适应优化补丁特征并结合WPS损失实现判别定位。</li>
<li>ATM模块利用大型语言模型（LLM）挖掘的结构知识构建类关系图（CRG）。</li>
<li>DART是第一个整合LLM派生关系知识的框架，用于自适应类间转移和类内细化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05585">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b2ad9f880be9504be1e6ddc5387cb804.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-935c74b538c2af6b708733599980c8cb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-341137980ff334eab2ab3d99b5e461ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ba4976fce4ae091c87fedabc552d739.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7fd23fce6f669632b65f925bf876acb8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Iterative-Learning-of-Computable-Phenotypes-for-Treatment-Resistant-Hypertension-using-Large-Language-Models"><a href="#Iterative-Learning-of-Computable-Phenotypes-for-Treatment-Resistant-Hypertension-using-Large-Language-Models" class="headerlink" title="Iterative Learning of Computable Phenotypes for Treatment Resistant   Hypertension using Large Language Models"></a>Iterative Learning of Computable Phenotypes for Treatment Resistant   Hypertension using Large Language Models</h2><p><strong>Authors:Guilherme Seidyo Imai Aldeia, Daniel S. Herman, William G. La Cava</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable capabilities for medical question answering and programming, but their potential for generating interpretable computable phenotypes (CPs) is under-explored. In this work, we investigate whether LLMs can generate accurate and concise CPs for six clinical phenotypes of varying complexity, which could be leveraged to enable scalable clinical decision support to improve care for patients with hypertension. In addition to evaluating zero-short performance, we propose and test a synthesize, execute, debug, instruct strategy that uses LLMs to generate and iteratively refine CPs using data-driven feedback. Our results show that LLMs, coupled with iterative learning, can generate interpretable and reasonably accurate programs that approach the performance of state-of-the-art ML methods while requiring significantly fewer training examples. </p>
<blockquote>
<p>大型语言模型（LLM）在医疗问题回答和编程方面展现出了显著的能力，但其在生成可解释的计算表型（CPs）方面的潜力尚未得到充分探索。在这项工作中，我们调查了LLM是否能为六种不同复杂度的临床表型生成准确且简洁的CPs，这可能用于实现可扩展的临床决策支持，以改善高血压患者的护理。除了评估零短性能外，我们还提出并测试了一种合成、执行、调试、指导策略，该策略利用LLM生成并使用数据驱动的反馈来迭代优化CPs。我们的结果表明，结合迭代学习，LLM可以生成可解释且相当准确的程序，其性能接近最新ML方法，同时显著减少了训练样本的需求。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05581v1">PDF</a> To appear in PMLR, Volume 298, Machine Learning for Healthcare, 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在医疗问题回答和编程方面展现出显著能力，但其生成可解释的计算表型（CPs）的潜力尚未被充分探索。本研究探讨LLMs是否能针对六种不同复杂度的临床表型生成准确简要的CPs，这可用于实现可扩展的临床决策支持，提高高血压患者的护理水平。除了评估零短性能外，我们还提出了一种合成、执行、调试、指导的策略，该策略利用LLMs以数据驱动反馈来生成并迭代优化CPs。结果表明，结合迭代学习，LLMs可以生成可解释且相对准确的程序，其性能接近最新机器学习方法的性能，同时显著减少了对训练样本的需求。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs具备生成可解释的计算表型（CPs）的潜力，这对于临床决策支持具有重要意义。</li>
<li>本研究探讨了LLMs在针对六种不同复杂度的临床表型生成CPs方面的能力。</li>
<li>提出了一种合成、执行、调试、指导的策略，利用LLMs和数据进行反馈以迭代优化CPs的生成。</li>
<li>LLMs结合迭代学习可以生成准确且可解释的程序，其性能接近最新机器学习方法。</li>
<li>LLMs在医疗领域的应用，如医疗问题回答，得到了进一步发展。</li>
<li>本研究的结果表明，LLMs在编程方面的能力同样值得进一步探索和应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05581">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a838ecccf04a3029f52cdcd18c591ad3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-245bc587759d64eca50f94564ccb756b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Conformal-Sets-in-Multiple-Choice-Question-Answering-under-Black-Box-Settings-with-Provable-Coverage-Guarantees"><a href="#Conformal-Sets-in-Multiple-Choice-Question-Answering-under-Black-Box-Settings-with-Provable-Coverage-Guarantees" class="headerlink" title="Conformal Sets in Multiple-Choice Question Answering under Black-Box   Settings with Provable Coverage Guarantees"></a>Conformal Sets in Multiple-Choice Question Answering under Black-Box   Settings with Provable Coverage Guarantees</h2><p><strong>Authors:Guang Yang, Xinyang Liu</strong></p>
<p>Large Language Models (LLMs) have shown remarkable progress in multiple-choice question answering (MCQA), but their inherent unreliability, such as hallucination and overconfidence, limits their application in high-risk domains. To address this, we propose a frequency-based uncertainty quantification method under black-box settings, leveraging conformal prediction (CP) to ensure provable coverage guarantees. Our approach involves multiple independent samplings of the model’s output distribution for each input, with the most frequent sample serving as a reference to calculate predictive entropy (PE). Experimental evaluations across six LLMs and four datasets (MedMCQA, MedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms logit-based PE in distinguishing between correct and incorrect predictions, as measured by AUROC. Furthermore, the method effectively controls the empirical miscoverage rate under user-specified risk levels, validating that sampling frequency can serve as a viable substitute for logit-based probabilities in black-box scenarios. This work provides a distribution-free model-agnostic framework for reliable uncertainty quantification in MCQA with guaranteed coverage, enhancing the trustworthiness of LLMs in practical applications. </p>
<blockquote>
<p>大型语言模型（LLM）在多选问答（MCQA）方面取得了显著的进步，但它们的固有不可靠性，如幻象和过度自信，限制了它们在高风险领域的应用。为了解决这一问题，我们提出了一种基于频率的不确定性量化方法，该方法在黑色背景下设置，利用适形预测（CP）确保可证明覆盖保证。我们的方法涉及对模型的输出分布进行多次独立采样，每个输入样本最频繁的出现作为参考来计算预测熵（PE）。在六个LLM和四个数据集（MedMCQA、MedQA、MMLU、MMLU-Pro）上的实验评估表明，基于频率的PE在区分正确和错误预测方面优于基于Logit的PE，这通过AUROC来衡量。此外，该方法有效地控制了用户指定风险水平下的经验误覆盖率，验证了采样频率可以作为黑色背景下基于Logit概率的可行替代方案。这项工作提供了一个分布无关模型框架，用于可靠的不确定性量化MCQA中的保证覆盖，增强了LLM在实际应用中的可信度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05544v1">PDF</a> under review</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在多选题回答（MCQA）方面取得显著进展，但其内在的不确定性，如幻象和过度自信，限制了其在高风险领域的应用。为解决这一问题，本文提出一种基于频率的不确定性量化方法，采用黑色场景下基于侏儒的预测方法确保可验证覆盖保证。该方法通过多次独立采样模型的输出分布来确定每个输入的预测概率分布中最频繁出现的样本，计算预测熵（PE）。实验表明，基于频率的PE相较于基于Logit的PE更能有效区分正确与错误的预测，并成功控制实证覆盖度在用户指定的风险水平之下。本研究提供了一个分布无关、模型通用的框架，用于可靠的不确定性量化MCQA，增强了LLM在实际应用中的可信度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在多选题回答（MCQA）方面取得显著进展，但存在内在不确定性问题。</li>
<li>提出一种基于频率的不确定性量化方法来解决这一问题。</li>
<li>采用基于侏儒的预测方法来确保可验证覆盖保证。</li>
<li>通过多次独立采样模型的输出分布来确定预测概率分布中最频繁出现的样本。</li>
<li>基于频率的PE相较于基于Logit的PE更能有效区分正确与错误的预测。</li>
<li>成功控制实证覆盖度在用户指定的风险水平之下。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05544">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-368b35971580fef96ab27036b47896f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34c5b9c3027dfc37ef679eb92690343c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d5b9c9f986076bb4d73bccd85b3edef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04a361e00caebc5ac686de11c55d7edf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26873d33aab9819cfb1d1c128c10114a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e57bf32df2587c93e47081831ab493d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="AI-vs-Human-Moderators-A-Comparative-Evaluation-of-Multimodal-LLMs-in-Content-Moderation-for-Brand-Safety"><a href="#AI-vs-Human-Moderators-A-Comparative-Evaluation-of-Multimodal-LLMs-in-Content-Moderation-for-Brand-Safety" class="headerlink" title="AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in   Content Moderation for Brand Safety"></a>AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in   Content Moderation for Brand Safety</h2><p><strong>Authors:Adi Levi, Or Levi, Sardhendu Mishra, Jonathan Morra</strong></p>
<p>As the volume of video content online grows exponentially, the demand for moderation of unsafe videos has surpassed human capabilities, posing both operational and mental health challenges. While recent studies demonstrated the merits of Multimodal Large Language Models (MLLMs) in various video understanding tasks, their application to multimodal content moderation, a domain that requires nuanced understanding of both visual and textual cues, remains relatively underexplored. In this work, we benchmark the capabilities of MLLMs in brand safety classification, a critical subset of content moderation for safe-guarding advertising integrity. To this end, we introduce a novel, multimodal and multilingual dataset, meticulously labeled by professional reviewers in a multitude of risk categories. Through a detailed comparative analysis, we demonstrate the effectiveness of MLLMs such as Gemini, GPT, and Llama in multimodal brand safety, and evaluate their accuracy and cost efficiency compared to professional human reviewers. Furthermore, we present an in-depth discussion shedding light on limitations of MLLMs and failure cases. We are releasing our dataset alongside this paper to facilitate future research on effective and responsible brand safety and content moderation. </p>
<blockquote>
<p>随着在线视频内容的数量呈指数级增长，对不安全视频的适度需求已经超越了人类的能力，带来了运营和心理健康方面的挑战。虽然最近有研究表明，多模态大型语言模型（MLLM）在各种视频理解任务中具有优势，但它们在需要视觉和文本线索细微理解的多模态内容适度应用方面，研究相对较少。在这项工作中，我们以品牌安全分类为标准，评估了多模态语言模型的能力，这是内容适度中保护广告完整性的关键子集。为此，我们引入了一个新的多模态和多语言数据集，该数据集经过专业评论人员在各种风险类别中仔细标注。通过详细比较分析，我们展示了Gemini、GPT和Llama等多模态大型语言模型在多模态品牌安全方面的有效性，并评估了它们与专业人工审核员相比的准确性和成本效益。此外，我们对多模态大型语言模型的局限性及失败案例进行了深入探讨。我们随这篇论文一起发布我们的数据集，以促进未来在有效和负责任的品牌安全和内容适度方面的研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05527v1">PDF</a> Accepted to the Computer Vision in Advertising and Marketing (CVAM)   workshop at ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>随着在线视频内容的指数级增长，对安全视频的管理需求已经超越了人工处理的能力，带来了运营和心理健康方面的挑战。最近的研究表明，多模态大型语言模型（MLLMs）在各种视频理解任务中具有优势，但在需要视觉和文本线索细微理解的多模态内容管理方面应用较少。本研究旨在评估MLLMs在品牌安全分类中的能力——内容管理的一个关键领域，以保护广告的真实性。为此，我们引入了一个新颖的多模态、多语言数据集，由专业评审人员进行了风险类别的细致标注。通过详细对比分析，我们证明了如Gemini、GPT和Llama等MLLMs在多模态品牌安全方面的有效性，并评估了它们在精度和成本效益方面相较于专业评审人员的表现。此外，我们还深入探讨了MLLMs的局限性和失败案例。同时发布的数据集将促进未来关于有效和负责任的品牌安全和内容管理的研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>在线视频内容的增长导致了对安全视频管理的巨大需求，挑战了人工处理的能力。</li>
<li>多模态大型语言模型（MLLMs）在视频理解任务中具有优势，但在多模态内容管理方面的应用仍相对较少。</li>
<li>研究聚焦于评估MLLMs在品牌安全分类方面的能力。</li>
<li>引入了一个多模态、多语言的数据集，用于评估MLLMs的性能，该数据集由专业评审人员进行细致标注。</li>
<li>对比分析了MLLMs与专业评审人员在品牌安全分类方面的表现，显示出MLLMs的有效性、精度和成本效益。</li>
<li>讨论了MLLMs的局限性和失败案例。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05527">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-be0e77bf4d422023dc2ed895f648108c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e72e38033ab58c3c60eb587eed8478c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-770f18bdb228a78c95d23edc244ce114.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa5dca44490dffa9468181110f90e8eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-174de568ec853a5b3f60e712f027f66b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49881015cdc1707f22800b2f9a0b57ec.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="The-World-According-to-LLMs-How-Geographic-Origin-Influences-LLMs’-Entity-Deduction-Capabilities"><a href="#The-World-According-to-LLMs-How-Geographic-Origin-Influences-LLMs’-Entity-Deduction-Capabilities" class="headerlink" title="The World According to LLMs: How Geographic Origin Influences LLMs’   Entity Deduction Capabilities"></a>The World According to LLMs: How Geographic Origin Influences LLMs’   Entity Deduction Capabilities</h2><p><strong>Authors:Harsh Nishant Lalai, Raj Sanjay Shah, Jiaxin Pei, Sashank Varma, Yi-Chia Wang, Ali Emami</strong></p>
<p>Large Language Models (LLMs) have been extensively tuned to mitigate explicit biases, yet they often exhibit subtle implicit biases rooted in their pre-training data. Rather than directly probing LLMs with human-crafted questions that may trigger guardrails, we propose studying how models behave when they proactively ask questions themselves. The 20 Questions game, a multi-turn deduction task, serves as an ideal testbed for this purpose. We systematically evaluate geographic performance disparities in entity deduction using a new dataset, Geo20Q+, consisting of both notable people and culturally significant objects (e.g., foods, landmarks, animals) from diverse regions. We test popular LLMs across two gameplay configurations (canonical 20-question and unlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese, French, Spanish, and Turkish). Our results reveal geographic disparities: LLMs are substantially more successful at deducing entities from the Global North than the Global South, and the Global West than the Global East. While Wikipedia pageviews and pre-training corpus frequency correlate mildly with performance, they fail to fully explain these disparities. Notably, the language in which the game is played has minimal impact on performance gaps. These findings demonstrate the value of creative, free-form evaluation frameworks for uncovering subtle biases in LLMs that remain hidden in standard prompting setups. By analyzing how models initiate and pursue reasoning goals over multiple turns, we find geographic and cultural disparities embedded in their reasoning processes. We release the dataset (Geo20Q+) and code at <a target="_blank" rel="noopener" href="https://sites.google.com/view/llmbias20q/home">https://sites.google.com/view/llmbias20q/home</a>. </p>
<blockquote>
<p>大型语言模型（LLM）已经进行了广泛的调整，以减轻明确的偏见，然而，它们通常表现出根植于预训练数据中的微妙隐含偏见。我们没有采用可能触发保护机制的由人类设计的问题来直接检测LLM，而是提出了研究模型在主动提出问题时的行为变化。20问游戏是一个多回合推理任务，作为实现这一目标的理想测试平台。我们系统地评估了使用新数据集Geo20Q+在实体推理中的地理性能差异，该数据集包含来自不同地区的知名人物和具有文化意义的事物（例如食物、地标、动物）。我们在两种游戏设置（标准的20个问题、无限回合）和七种语言（英语、印地语、普通话、日语、法语、西班牙语和土耳其语）中测试了流行的大型语言模型。我们的结果表明存在地理差异：大型语言模型在推测全球北方实体方面比全球南方更为成功，全球西方也比全球东方更为成功。虽然维基百科页面浏览量和预训练语料库频率与性能有轻微关联，但它们未能完全解释这些差异。值得注意的是，游戏所用的语言对性能差距的影响微乎其微。这些发现表明，创造性、自由形式的评估框架在揭示大型语言模型中隐藏细微偏见方面具有重要价值。通过分析模型如何在多回合中启动和追求推理目标，我们发现其推理过程中存在的地理和文化差异。我们在<a target="_blank" rel="noopener" href="https://sites.google.com/view/llmbias20q/home%E5%8F%91%E5%B8%83%E4%BA%86%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%88Geo20Q+%EF%BC%89%E5%92%8C%E4%BB%A3%E7%A0%81%E3%80%82">https://sites.google.com/view/llmbias20q/home发布了数据集（Geo20Q+）和代码。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05525v1">PDF</a> Conference on Language Modeling 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）尽管经过调整以减轻显性偏见，但它们通常表现出根植于预训练数据中的微妙隐性偏见。研究通过模型主动提问的方式来研究模型的偏见。在名为Geo20Q+的新数据集上，对模型在地理实体推理方面的表现进行了系统评估。测试了流行的大型语言模型在两种游戏玩法配置（经典的20问和无限回合制）下的表现，并在七种语言中进行。发现地理差异：大型语言模型在全球北方和西方的实体推理表现更好，而维基百科页面浏览量和预训练语料库频率对表现的影响较小，不能完全解释这些差异。语言对表现差距的影响微乎其微。这些发现表明，创造性的自由形式评估框架对于发现大型语言模型中的微妙偏见具有价值，这些偏见在标准提示设置中是隐藏的。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）即使经过调整以减轻偏见，仍存在基于预训练数据的微妙隐性偏见。</li>
<li>通过模型主动提问的方式研究模型的偏见是一种有效方法。</li>
<li>在名为Geo20Q+的新数据集上，发现大型语言模型在地理实体推理方面存在地理差异。</li>
<li>测试了大型语言模型在两种游戏玩法配置下的表现，发现其在全球北方和西方的表现更好。</li>
<li>维基百科页面浏览量和预训练语料库频率对大型语言模型表现的影响较小，不能完全解释这些差异。</li>
<li>语言对大型语言模型表现差距的影响微乎其微。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05525">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b7d7a3391cccaf19c6994d79d33307e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f31249bae350730fe3f9e0892c954ae.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Advancing-Hate-Speech-Detection-with-Transformers-Insights-from-the-MetaHate"><a href="#Advancing-Hate-Speech-Detection-with-Transformers-Insights-from-the-MetaHate" class="headerlink" title="Advancing Hate Speech Detection with Transformers: Insights from the   MetaHate"></a>Advancing Hate Speech Detection with Transformers: Insights from the   MetaHate</h2><p><strong>Authors:Santosh Chapagain, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi</strong></p>
<p>Hate speech is a widespread and harmful form of online discourse, encompassing slurs and defamatory posts that can have serious social, psychological, and sometimes physical impacts on targeted individuals and communities. As social media platforms such as X (formerly Twitter), Facebook, Instagram, Reddit, and others continue to facilitate widespread communication, they also become breeding grounds for hate speech, which has increasingly been linked to real-world hate crimes. Addressing this issue requires the development of robust automated methods to detect hate speech in diverse social media environments. Deep learning approaches, such as vanilla recurrent neural networks (RNNs), long short-term memory (LSTM), and convolutional neural networks (CNNs), have achieved good results, but are often limited by issues such as long-term dependencies and inefficient parallelization. This study represents the comprehensive exploration of transformer-based models for hate speech detection using the MetaHate dataset–a meta-collection of 36 datasets with 1.2 million social media samples. We evaluate multiple state-of-the-art transformer models, including BERT, RoBERTa, GPT-2, and ELECTRA, with fine-tuned ELECTRA achieving the highest performance (F1 score: 0.8980). We also analyze classification errors, revealing challenges with sarcasm, coded language, and label noise. </p>
<blockquote>
<p>仇恨言论是一种广泛且有害的在线话语形式，包括侮辱性和诽谤性的帖子，可能对目标个人和社区产生严重的社会、心理和有时甚至是身体影响。随着诸如X（原Twitter）、Facebook、Instagram、Reddit等社交媒体平台继续促进广泛传播沟通的同时，它们也成为了仇恨言论的滋生地，仇恨言论与真实世界的仇恨犯罪之间的联系也愈发紧密。解决这一问题需要开发稳健的自动化方法，以在多样化的社交媒体环境中检测仇恨言论。深度学习的方法，如普通的循环神经网络（RNNs）、长短时记忆网络（LSTM）和卷积神经网络（CNNs）已经取得了良好的成果，但它们常常受限于长期依赖关系和不高效的并行化等问题。本研究使用MetaHate数据集全面探索基于Transformer模型的仇恨言论检测——一个包含36个数据集和120万个社交媒体样本的元集合。我们评估了多个先进的Transformer模型，包括BERT、RoBERTa、GPT-2和ELECTRA等模型，经过精细调整的ELECTRA模型取得了最佳性能（F1分数为0.8980）。我们还分析了分类错误，揭示出讥讽性言语、隐含的语言和标签噪声带来的挑战。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04913v1">PDF</a> Accepted to the Deviant Dynamics in Digital Spaces workshop at ASONAM   2025</p>
<p><strong>Summary</strong></p>
<p>这篇文本主要讨论了网络上的仇恨言论问题，包括它对个人和社会的影响。文中提到社交媒体平台成为了仇恨言论的滋生地，这一问题已与现实世界的仇恨犯罪紧密相连。为了解决这一问题，研究者们正在开发在多样社交媒体环境中检测仇恨言论的自动化方法。当前深度学习和基于Transformer的模型都在这一领域取得了一些进展。本文重点研究了基于Transformer的模型在MetaHate数据集上的仇恨言论检测效果，其中fine-tuned ELECTRA模型表现最佳，F1分数达到0.8980。同时，文章还分析了分类错误的原因，包括讽刺、隐含语言和标签噪声等挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>仇恨言论是一种广泛且有害的网络表达形式，对目标个体和社区产生严重的社会、心理和物理影响。</li>
<li>社交媒体平台成为仇恨言论的滋生地，与现实世界的仇恨犯罪有联系。</li>
<li>自动化方法在处理多样社交媒体环境中的仇恨言论检测方面发挥着重要作用。</li>
<li>基于深度学习的模型，如RNN、LSTM和CNN等，在仇恨言论检测方面已有良好表现，但仍存在长期依赖和并行化效率低下等问题。</li>
<li>基于Transformer的模型在MetaHate数据集上进行仇恨言论检测表现出潜力。</li>
<li>fine-tuned ELECTRA模型在仇恨言论检测方面取得了最佳性能，F1分数达到0.8980。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04913">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-583af5db18abfe596e34c71933a54e7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fde913f94330f6d8e7148852c5935f6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e7075273c205cf1b766672caca61ce50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6594dbc1d95a256a0086f3c5dfd41ca.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DreamVVT-Mastering-Realistic-Video-Virtual-Try-On-in-the-Wild-via-a-Stage-Wise-Diffusion-Transformer-Framework"><a href="#DreamVVT-Mastering-Realistic-Video-Virtual-Try-On-in-the-Wild-via-a-Stage-Wise-Diffusion-Transformer-Framework" class="headerlink" title="DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a   Stage-Wise Diffusion Transformer Framework"></a>DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a   Stage-Wise Diffusion Transformer Framework</h2><p><strong>Authors:Tongchun Zuo, Zaiyu Huang, Shuliang Ning, Ente Lin, Chao Liang, Zerong Zheng, Jianwen Jiang, Yuan Zhang, Mingyuan Gao, Xin Dong</strong></p>
<p>Video virtual try-on (VVT) technology has garnered considerable academic interest owing to its promising applications in e-commerce advertising and entertainment. However, most existing end-to-end methods rely heavily on scarce paired garment-centric datasets and fail to effectively leverage priors of advanced visual models and test-time inputs, making it challenging to accurately preserve fine-grained garment details and maintain temporal consistency in unconstrained scenarios. To address these challenges, we propose DreamVVT, a carefully designed two-stage framework built upon Diffusion Transformers (DiTs), which is inherently capable of leveraging diverse unpaired human-centric data to enhance adaptability in real-world scenarios. To further leverage prior knowledge from pretrained models and test-time inputs, in the first stage, we sample representative frames from the input video and utilize a multi-frame try-on model integrated with a vision-language model (VLM), to synthesize high-fidelity and semantically consistent keyframe try-on images. These images serve as complementary appearance guidance for subsequent video generation. \textbf{In the second stage}, skeleton maps together with fine-grained motion and appearance descriptions are extracted from the input content, and these along with the keyframe try-on images are then fed into a pretrained video generation model enhanced with LoRA adapters. This ensures long-term temporal coherence for unseen regions and enables highly plausible dynamic motions. Extensive quantitative and qualitative experiments demonstrate that DreamVVT surpasses existing methods in preserving detailed garment content and temporal stability in real-world scenarios. Our project page <a target="_blank" rel="noopener" href="https://virtu-lab.github.io/">https://virtu-lab.github.io/</a> </p>
<blockquote>
<p>视频虚拟试穿（VVT）技术因其电子商务广告和娱乐方面的应用前景而引起了学术界的广泛关注。然而，大多数现有的端到端方法严重依赖于稀缺的配套服装数据集，未能有效利用先进的视觉模型的先验知识和测试时的输入，这使得在不受约束的场景中准确保留服装的精细细节并保持时间一致性具有挑战性。为了解决这些挑战，我们提出了DreamVVT，这是一个精心设计的两阶段框架，建立在扩散变压器（DiTs）之上，它本质上能够利用多样且无配套的人形数据，提高在现实场景中的适应性。为了进一步利用预训练模型的先验知识和测试时的输入，在第一阶段，我们从输入视频中抽样代表性帧，并使用一个集成了视觉语言模型（VLM）的多帧试穿模型，合成高保真和语义一致的关键帧试穿图像。这些图像作为后续视频生成的补充外观指导。在第二阶段，从输入内容中提取骨架图以及精细的运动和外观描述，这些与关键帧试穿图像一起输入到增强有LoRA适配器的预训练视频生成模型中。这确保了未见区域的长期时间连贯性，并能够实现高度逼真的动态运动。大量的定量和定性实验表明，DreamVVT在保留服装内容的详细信息和现实场景中的时间稳定性方面超越了现有方法。我们的项目页面为：[<a target="_blank" rel="noopener" href="https://virtu-lab.github.io/]">https://virtu-lab.github.io/]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02807v1">PDF</a> 18 pages, 12 figures</p>
<p><strong>Summary</strong><br>视频虚拟试穿（VVT）技术在电子商务广告和娱乐等领域受到广泛关注。然而，现有端到端方法大多依赖稀缺的配对服装数据集，难以有效利用先进视觉模型的先验知识和测试时输入的信息，难以在不受约束的场景中准确保留细致的服装细节并保持时间一致性。为解决这些挑战，我们提出DreamVVT，这是一个基于扩散转换器（DiT）的精心设计的两阶段框架，能够利用多样的人类中心化数据增强在真实场景中的适应性。首先，我们从输入视频中采样关键帧，并使用多帧试穿模型与视觉语言模型（VLM）合成高质量、语义一致的关键帧试穿图像。接着在第二阶段，我们从输入内容中提取骨架图以及细致的动态和外观描述信息，并结合关键帧试穿图像输入到增强型视频生成模型中，确保长期时间连贯性并产生高度逼真的动态效果。实验证明，DreamVVT在保留服装细节和时间稳定性方面超越现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频虚拟试穿（VVT）技术在电子商务广告和娱乐领域有广泛应用前景。</li>
<li>现有方法依赖配对服装数据集，难以在真实场景中应用。</li>
<li>DreamVVT基于扩散转换器（DiT）的两阶段框架，可利用多样的人类中心化数据。</li>
<li>第一阶段合成高质量、语义一致的关键帧试穿图像。</li>
<li>第二阶段结合骨架图、动态和外观描述信息，确保视频长期时间连贯性。</li>
<li>DreamVVT在保留服装细节和时间稳定性方面表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02807">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1b76edf22a33b41c2d24b78a524332ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7d501d3d6347bb2d3baf5948c82997c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-589204f535676714bf5117d7e124d3ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e89423f68398b79f2405a55d9098aa6f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CAMA-Enhancing-Mathematical-Reasoning-in-Large-Language-Models-with-Causal-Knowledge"><a href="#CAMA-Enhancing-Mathematical-Reasoning-in-Large-Language-Models-with-Causal-Knowledge" class="headerlink" title="CAMA: Enhancing Mathematical Reasoning in Large Language Models with   Causal Knowledge"></a>CAMA: Enhancing Mathematical Reasoning in Large Language Models with   Causal Knowledge</h2><p><strong>Authors:Lei Zan, Keli Zhang, Ruichu Cai, Lujia Pan</strong></p>
<p>Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks, yet they still struggle with complex mathematical reasoning, a challenge fundamentally rooted in deep structural dependencies. To address this challenge, we propose \textbf{CA}usal \textbf{MA}thematician (\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit, reusable mathematical structure. In the learning stage, CAMA first constructs the \textbf{M}athematical \textbf{C}ausal \textbf{G}raph (\textbf{MCG}), a high-level representation of solution strategies, by combining LLM priors with causal discovery algorithms applied to a corpus of question-solution pairs. The resulting MCG encodes essential knowledge points and their causal dependencies. To better align the graph with downstream reasoning tasks, CAMA further refines the MCG through iterative feedback derived from a selected subset of the question-solution pairs. In the reasoning stage, given a new question, CAMA dynamically extracts a task-relevant subgraph from the MCG, conditioned on both the question content and the LLM’s intermediate reasoning trace. This subgraph, which encodes the most pertinent knowledge points and their causal dependencies, is then injected back into the LLM to guide its reasoning process. Empirical results on real-world datasets show that CAMA significantly improves LLM performance on challenging mathematical problems. Furthermore, our experiments demonstrate that structured guidance consistently outperforms unstructured alternatives, and that incorporating asymmetric causal relationships yields greater improvements than using symmetric associations alone. </p>
<blockquote>
<p>大型语言模型（LLM）在多种任务中表现出强大的性能，但在复杂的数学推理方面仍存在困难，这一挑战根本源于深层的结构依赖性。为了解决这一挑战，我们提出了因果数学家（\textbf{CAMA}）这一两阶段因果框架，为LLM配备明确的可重用数学结构。在学习阶段，CAMA首先构建数学因果图（MCG），这是一种解决方案策略的高级表示，通过结合LLM的先验知识和应用于问题解决方案对语料库的因果发现算法。结果产生的MCG编码了必要的知识点和它们的因果依赖关系。为了更好地与下游推理任务对齐，CAMA通过来自问题解决方案对子集的迭代反馈进一步改进MCG。在推理阶段，对于新问题，CAMA根据问题的内容和LLM的中间推理轨迹，动态地从MCG中提取任务相关子图。这个子图编码了最相关的知识点和它们的因果依赖关系，然后注入LLM以指导其推理过程。在真实数据集上的实证结果表明，CAMA在解决具有挑战性的数学问题方面显著提高了LLM的性能。此外，我们的实验表明，结构化的指导始终优于非结构化的替代方案，而融入不对称的因果关系比仅使用对称关联能带来更大的改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02583v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在多种任务上表现出强大的性能，但在复杂的数学推理方面仍存在挑战。为解决这一挑战，提出了因果数学家（CAMA）框架，该框架分为两个阶段：学习阶段和推理阶段。学习阶段构建数学因果图（MCG），结合LLM先验知识与因果发现算法，对问题解答对进行编码，形成高层次的解决方案策略表示。推理阶段根据问题和LLM的中间推理轨迹，从MCG中动态提取相关子图，指导LLM的推理过程。实验表明，CAMA能显著提高LLM解决数学问题的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM面临复杂数学推理的挑战，需要新的方法来解决。</li>
<li>CAMA框架分为学习阶段和推理阶段，旨在提高LLM的数学推理能力。</li>
<li>学习阶段通过构建数学因果图（MCG）来编码解决方案策略。</li>
<li>推理阶段根据问题和LLM的推理轨迹从MCG中提取相关子图。</li>
<li>CAMA通过结合LLM先验知识、因果发现算法和问题解答对，提高数学问题的解决能力。</li>
<li>实证结果表明，CAMA能显著提高LLM在解决数学问题上的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02583">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-58da7f78ab2d364eb3339b882ecd3c4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb36c79e159593473a6f104fb2645231.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef83bcc7ab4751853333c5463f3b0db0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SE-Agent-Self-Evolution-Trajectory-Optimization-in-Multi-Step-Reasoning-with-LLM-Based-Agents"><a href="#SE-Agent-Self-Evolution-Trajectory-Optimization-in-Multi-Step-Reasoning-with-LLM-Based-Agents" class="headerlink" title="SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning   with LLM-Based Agents"></a>SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning   with LLM-Based Agents</h2><p><strong>Authors:Jiaye Lin, Yifu Guo, Yuzhen Han, Sen Hu, Ziyi Ni, Licheng Wang, Mingguang Chen, Daxin Jiang, Binxing Jiao, Chen Hu, Huacan Wang</strong></p>
<p>Large Language Model (LLM)-based agents have recently shown impressive capabilities in complex reasoning and tool use via multi-step interactions with their environments. While these agents have the potential to tackle complicated tasks, their problem-solving process, i.e., agents’ interaction trajectory leading to task completion, remains underexploited. These trajectories contain rich feedback that can navigate agents toward the right directions for solving problems correctly. Although prevailing approaches, such as Monte Carlo Tree Search (MCTS), can effectively balance exploration and exploitation, they ignore the interdependence among various trajectories and lack the diversity of search spaces, which leads to redundant reasoning and suboptimal outcomes. To address these challenges, we propose SE-Agent, a Self-Evolution framework that enables Agents to optimize their reasoning processes iteratively. Our approach revisits and enhances former pilot trajectories through three key operations: revision, recombination, and refinement. This evolutionary mechanism enables two critical advantages: (1) it expands the search space beyond local optima by intelligently exploring diverse solution paths guided by previous trajectories, and (2) it leverages cross-trajectory inspiration to efficiently enhance performance while mitigating the impact of suboptimal reasoning paths. Through these mechanisms, SE-Agent achieves continuous self-evolution that incrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench Verified to resolve real-world GitHub issues. Experimental results across five strong LLMs show that integrating SE-Agent delivers up to 55% relative improvement, achieving state-of-the-art performance among all open-source agents on SWE-bench Verified. Our code and demonstration materials are publicly available at <a target="_blank" rel="noopener" href="https://github.com/JARVIS-Xs/SE-Agent">https://github.com/JARVIS-Xs/SE-Agent</a>. </p>
<blockquote>
<p>基于大型语言模型（LLM）的代理最近显示出通过与其环境进行多步骤交互进行复杂推理和工具使用的令人印象深刻的能力。虽然这些代理有潜力处理复杂任务，但他们的解决问题过程，即代理完成任务的交互轨迹，仍未得到充分探索。这些轨迹包含丰富的反馈，可以引导代理正确解决问题。尽管现有的方法，如蒙特卡洛树搜索（MCTS），可以有效地平衡探索和利用，但它们忽略了各种轨迹之间的相互依赖性，并且缺乏搜索空间的多样性，这导致冗余推理和次优结果。为了解决这些挑战，我们提出了SE-Agent，一个自我进化框架，使代理能够迭代优化他们的推理过程。我们的方法通过三个关键操作：修订、重组和细化，来重新审视和改进之前的轨迹。这种进化机制带来了两个关键优势：（1）它通过智能地探索以前轨迹引导的多种解决方案路径，扩大了搜索空间，超越了局部最优；（2）它利用跨轨迹的灵感来有效地提高性能，同时减轻次优推理路径的影响。通过这些机制，SE-Agent实现了持续的自我进化，逐步提高了推理质量。我们在SWE-bench Verified上评估了SE-Agent，以解决现实世界中的GitHub问题。在五个强大的LLM上的实验结果表明，集成SE-Agent带来了高达55%的相对改进，在SWE-bench Verified上的性能优于所有开源代理，达到了最新水平。我们的代码和演示材料可在<a target="_blank" rel="noopener" href="https://github.com/JARVIS-Xs/SE-Agent%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/JARVIS-Xs/SE-Agent公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02085v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在复杂推理和工具使用方面展现出强大的能力，通过与环境的多步交互完成任务。然而，其解题过程中的交互轨迹尚未得到充分研究。这些轨迹包含丰富的反馈，可以引导模型正确解决问题。现有方法如蒙特卡洛树搜索（MCTS）虽然能平衡探索与利用，但忽略了轨迹间的相互依赖性和搜索空间的多样性，导致冗余推理和次优结果。为解决这些问题，我们提出了SE-Agent框架，通过修订、重组和优化之前的轨迹，使模型能够迭代优化推理过程。实验结果表明，SE-Agent在真实世界GitHub问题上的性能相对于其他开源模型有明显提升。代码和演示材料公开可用。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM模型在复杂推理和工具使用方面表现出强大的能力。</li>
<li>LLM模型的解题过程中的交互轨迹尚未得到充分研究，其中包含丰富的反馈。</li>
<li>现有方法如蒙特卡洛树搜索忽略了轨迹间的相互依赖性和搜索空间的多样性。</li>
<li>SE-Agent框架通过修订、重组和优化之前的轨迹，使模型能够迭代优化推理过程。</li>
<li>SE-Agent实现了对局部最优解的超越，通过智能探索多种解决方案路径来提高性能。</li>
<li>SE-Agent利用跨轨迹灵感来增强性能并减轻次优推理路径的影响。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02085">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-54b48f09ac4403bd42b74224c37da297.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc136467beb09e19fc1f86d24cbca8bd.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="The-SMeL-Test-A-simple-benchmark-for-media-literacy-in-language-models"><a href="#The-SMeL-Test-A-simple-benchmark-for-media-literacy-in-language-models" class="headerlink" title="The SMeL Test: A simple benchmark for media literacy in language models"></a>The SMeL Test: A simple benchmark for media literacy in language models</h2><p><strong>Authors:Gustaf Ahdritz, Anat Kleiman</strong></p>
<p>The internet is rife with unattributed, deliberately misleading, or otherwise untrustworthy content. Though large language models (LLMs) are often tasked with autonomous web browsing, the extent to which they have learned the simple heuristics human researchers use to navigate this noisy environment is not currently known. In this paper, we introduce the Synthetic Media Literacy Test (SMeL Test), a minimal benchmark that tests the ability of language models to actively filter out untrustworthy information in context. We benchmark a variety of commonly used instruction-tuned LLMs, including reasoning models, and find that no model consistently succeeds; while reasoning in particular is associated with higher scores, even the best API model we test hallucinates up to 70% of the time. Remarkably, larger and more capable models do not necessarily outperform their smaller counterparts. We hope our work sheds more light on this important form of hallucination and guides the development of new methods to combat it. </p>
<blockquote>
<p>互联网充斥着无属性、故意误导或其他不可信赖的内容。尽管大型语言模型（LLM）经常被赋予自主网页浏览的任务，但它们在这一嘈杂环境中学习人类研究者使用的简单启发式方法的程度目前尚不清楚。在本文中，我们引入了合成媒体素养测试（SMeL测试），这是一个最小的基准测试，旨在测试语言模型在特定情境中主动过滤掉不可靠信息的能力。我们对各种常用的指令调整型LLM进行了基准测试，包括推理模型，发现没有任何模型能够始终成功；虽然推理与更高的分数特别相关，但即使是我们测试过的最好的API模型，也有高达70%的幻觉。值得注意的是，更大、更强大的模型并不一定比小型模型表现更好。我们希望我们的工作能进一步揭示这种重要的幻觉现象，并为开发新的对抗方法提供指导。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02074v2">PDF</a> </p>
<p><strong>Summary</strong>：互联网充斥着大量未经授权、故意误导或其他不可靠的内容。大型语言模型（LLM）常被用于自动浏览网页，但它们是否掌握了人类研究者用于导航这种嘈杂环境的简单启发式技术尚不清楚。本文介绍了合成媒体素养测试（SMeL测试），这是一个基准测试，旨在测试语言模型在情境中主动过滤掉不可靠信息的能力。我们对多种常用的指令调整LLM进行了基准测试，发现没有任何模型始终表现良好；虽然推理能力尤其与高分相关，但即使是表现最佳的API模型也会达到70%的虚构程度。值得注意的是，更大、更强大的模型并不一定比小型模型表现得更好。我们希望这项工作能帮助进一步揭示这种虚构现象，并引导开发新的方法来应对它。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>互联网充斥着不可靠的内容，如未经授权的内容和故意误导信息。</li>
<li>大型语言模型（LLM）在执行自主网页浏览任务时面临挑战，特别是在过滤不可靠信息方面。</li>
<li>合成媒体素养测试（SMeL测试）被引入作为一种基准测试，以评估LLM在此方面的能力。</li>
<li>在测试中，没有一种LLM始终表现出良好的过滤能力。</li>
<li>推理能力在测试中尤为重要，但与高虚构程度相关。</li>
<li>即使是最先进的LLM也存在虚构现象，最高达到70%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02074">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a3f6ad270387a73fe6256698b4a7090b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Generative-AI-Adoption-in-Postsecondary-Education-AI-Hype-and-ChatGPT’s-Launch"><a href="#Generative-AI-Adoption-in-Postsecondary-Education-AI-Hype-and-ChatGPT’s-Launch" class="headerlink" title="Generative AI Adoption in Postsecondary Education, AI Hype, and   ChatGPT’s Launch"></a>Generative AI Adoption in Postsecondary Education, AI Hype, and   ChatGPT’s Launch</h2><p><strong>Authors:Isabel Pedersen</strong></p>
<p>The rapid integration of generative artificial intelligence (AI) into postsecondary education and many other sectors resulted in a global reckoning with this new technology. This paper contributes to the study of the multifaceted influence of generative AI, with a particular focus on OpenAI’s ChatGPT within academic settings during the first six months after the release in three specific ways. First, it scrutinizes the rise of ChatGPT as a transformative event construed through a study of mainstream discourses exhibiting AI hype. Second, it discusses the perceived implications of generative AI for writing, teaching, and learning through the lens of critical discourse analysis and critical AI studies. Third, it encourages the necessity for best practices in the adoption of generative AI technologies in education. </p>
<blockquote>
<p>人工智能的快速集成到高等教育和其他许多领域，引发了全球对这一新技术的重新评估。本文研究了生成式人工智能的多方面影响，特别是OpenAI的ChatGPT在学术环境中的影响，以三种特定方式做出了贡献。首先，它仔细研究了ChatGPT作为一场变革性事件的出现，通过主流话语的研究展现了人工智能炒作现象。其次，它通过批判话语分析和批判人工智能研究的视角，探讨了生成式人工智能对写作、教学和学习的潜在影响。最后，它强调了采用生成式人工智能技术在教育中的最佳实践的必要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01003v1">PDF</a> 19 pages</p>
<p><strong>Summary</strong><br>     本文研究了生成式人工智能（AI）在高等教育和其他领域中的快速融合所带来的多方面影响，重点关注了OpenAI的ChatGPT在学术环境中的首次六个月应用。文章分析了ChatGPT作为一个变革性事件的出现与人工智能炒作话语之间的联系，探讨了生成式人工智能对写作、教学和学习的潜在影响，并强调了教育领域采用最佳实践使用生成式AI技术的必要性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>生成式人工智能（AI）在教育和其他领域中的融合引起全球关注。</li>
<li>OpenAI的ChatGPT作为一种变革性事件被广泛研究，其在学术环境中的应用得到了特殊关注。</li>
<li>文章探讨了ChatGPT的出现与人工智能炒作话语之间的联系。</li>
<li>文章通过批判性话语分析和人工智能研究审视了生成式人工智能对写作、教学和学习的潜在影响。</li>
<li>文章强调了教育领域在使用生成式AI技术时需要遵循最佳实践的重要性。</li>
<li>文章指出，随着技术的快速发展，对生成式AI的研究和应用需要持续跟进和评估。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01003">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-73a15dc61a9e9855d99c21e241914f5c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Automated-Label-Placement-on-Maps-via-Large-Language-Models"><a href="#Automated-Label-Placement-on-Maps-via-Large-Language-Models" class="headerlink" title="Automated Label Placement on Maps via Large Language Models"></a>Automated Label Placement on Maps via Large Language Models</h2><p><strong>Authors:Harry Shomer, Jiejun Xu</strong></p>
<p>Label placement is a critical aspect of map design, serving as a form of spatial annotation that directly impacts clarity and interpretability. Despite its importance, label placement remains largely manual and difficult to scale, as existing automated systems struggle to integrate cartographic conventions, adapt to context, or interpret labeling instructions. In this work, we introduce a new paradigm for automatic label placement (ALP) that formulates the task as a data editing problem and leverages large language models (LLMs) for context-aware spatial annotation. To support this direction, we curate MAPLE, the first known benchmarking dataset for evaluating ALP on real-world maps, encompassing diverse landmark types and label placement annotations from open-source data. Our method retrieves labeling guidelines relevant to each landmark type leveraging retrieval-augmented generation (RAG), integrates them into prompts, and employs instruction-tuned LLMs to generate ideal label coordinates. We evaluate four open-source LLMs on MAPLE, analyzing both overall performance and generalization across different types of landmarks. This includes both zero-shot and instruction-tuned performance. Our results demonstrate that LLMs, when guided by structured prompts and domain-specific retrieval, can learn to perform accurate spatial edits, aligning the generated outputs with expert cartographic standards. Overall, our work presents a scalable framework for AI-assisted map finishing and demonstrates the potential of foundation models in structured data editing tasks. The code and data can be found at <a target="_blank" rel="noopener" href="https://github.com/HarryShomer/MAPLE">https://github.com/HarryShomer/MAPLE</a>. </p>
<blockquote>
<p>标签放置是地图设计中的一个关键方面，作为一种空间注释，它直接影响地图的清晰度和可解释性。尽管其重要性显著，但标签放置仍然大多依赖于手动操作，难以规模化，因为现有的自动化系统很难融入地图制作规范、适应上下文或解释标签指令。在这项工作中，我们引入了自动标签放置（ALP）的新范式，将任务形式化为数据编辑问题，并利用大型语言模型（LLM）进行上下文感知空间注释。为了支持这一方向，我们创建了MAPLE，这是首个用于评估真实世界地图上ALP性能的标准数据集，涵盖了来自开源数据的各种地标类型和标签放置注释。我们的方法检索与每种地标类型相关的标签指南，利用检索增强生成（RAG）技术将它们整合到提示中，并调用指令微调LLM生成理想的标签坐标。我们在MAPLE上评估了四个开源LLM，分析了整体性能以及在不同类型地标上的泛化能力。这包括零样本教学和指令微调后的性能。结果表明，在结构化提示和领域特定检索的指导下，LLM可以学习执行精确的空间编辑，使生成输出符合专家地图制作标准。总的来说，我们的工作为AI辅助地图制作提供了一个可扩展的框架，并展示了基础模型在结构化数据编辑任务中的潜力。代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/HarryShomer/MAPLE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HarryShomer/MAPLE找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22952v2">PDF</a> Workshop on AI for Data Editing (AI4DE) at KDD 2025</p>
<p><strong>Summary</strong><br>地图标签放置是地图设计中的关键要素，影响地图的清晰度和可解释性。现有自动标签放置系统存在困难，难以结合地图惯例、适应上下文或解释标签指令。本研究引入一种新的自动标签放置（ALP）范式，将任务表述为数据编辑问题，并利用大型语言模型（LLM）进行上下文感知的空间标注。为此，我们创建了MAPLE数据集，评估ALP在真实世界地图上的表现，涵盖多种地标类型和标签放置注释。我们的方法通过检索增强生成（RAG）检索与每种地标类型相关的标注指南，将其集成到提示中，并借助指令优化LLM生成理想的标签坐标。在MAPLE数据集上评估四个开源LLM的性能，分析整体表现和在不同类型地标上的泛化能力，包括零样本和指令优化性能。结果表明，在结构化提示和领域特定检索的指导下，LLM可以学习执行准确的空间编辑，生成输出与专家地图制作标准相符。总体而言，我们的工作提出了一个可扩展的AI辅助地图完成框架，展示了基础模型在结构化数据编辑任务中的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>标签放置是地图设计中的核心环节，直接影响地图的清晰度和可解释性。</li>
<li>现有自动标签放置系统面临集成地图惯例、适应上下文和解释标签指令的挑战。</li>
<li>引入新的自动标签放置（ALP）范式，将其表述为数据编辑问题并利用大型语言模型（LLM）解决。</li>
<li>创立MAPLE数据集，用于评估ALP在真实世界地图上的性能。</li>
<li>方法结合检索增强生成（RAG）技术，检索与地标类型相关的标注指南，并集成到LLM的提示中。</li>
<li>在MAPLE数据集上评估LLM的性能，展示其在标签放置任务中的准确性和泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22952">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ed236f45fa551bca4c190ee01210c143.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4ef35c193bd45dd530fe5c533fe7101.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-13b18f0e35d9c9c0fb9fcfa81014381c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a483514fd0c4e2bf39d533949453c01f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a93af1e0affe9d6020fd97496d728b1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3c400408fd1c2c9c296001695f90fed.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Diffusion-Beats-Autoregressive-in-Data-Constrained-Settings"><a href="#Diffusion-Beats-Autoregressive-in-Data-Constrained-Settings" class="headerlink" title="Diffusion Beats Autoregressive in Data-Constrained Settings"></a>Diffusion Beats Autoregressive in Data-Constrained Settings</h2><p><strong>Authors:Mihir Prabhudesai, Mengning Wu, Amir Zadeh, Katerina Fragkiadaki, Deepak Pathak</strong></p>
<p>Autoregressive (AR) models have long dominated the landscape of large language models, driving progress across a wide range of tasks. Recently, diffusion-based language models have emerged as a promising alternative, though their advantages over AR models remain underexplored. In this paper, we systematically study masked diffusion models in data-constrained settings-where training involves repeated passes over limited data-and find that they significantly outperform AR models when compute is abundant but data is scarce. Diffusion models make better use of repeated data, achieving lower validation loss and superior downstream performance. We interpret this advantage as implicit data augmentation: masked diffusion exposes the model to a diverse distribution of token orderings and prediction tasks, unlike AR’s fixed left-to-right factorization. We find new scaling laws for diffusion models and derive a closed-form expression for the critical compute threshold at which diffusion begins to outperform AR. These results suggest that when data, not compute, is the bottleneck, diffusion models offer a compelling alternative to the standard AR paradigm. Our code is available at: <a target="_blank" rel="noopener" href="https://diffusion-scaling.github.io/">https://diffusion-scaling.github.io</a>. </p>
<blockquote>
<p>自回归（AR）模型长期以来一直在大型语言模型领域中占据主导地位，并在各种任务中推动进展。最近，基于扩散的语言模型作为一种有前途的替代方法而出现，尽管它们相对于AR模型的优势仍未得到充分探索。在本文中，我们系统地研究了数据受限环境中掩蔽扩散模型的研究，其中训练涉及在有限数据上多次迭代，我们发现当计算资源充足但数据稀缺时，它们会大大优于AR模型。扩散模型能更好地利用重复数据，实现更低的验证损失和更出色的下游性能。我们将这种优势解释为隐式数据增强：掩蔽扩散使模型暴露于多样化的令牌顺序和预测任务分布中，这与AR的固定从左到右分解不同。我们为扩散模型找到了新的扩展定律，并推导出了临界计算阈值的闭式表达式，在这个阈值上，扩散开始优于AR。这些结果表明，当数据而不是计算成为瓶颈时，扩散模型为标准的AR范式提供了一个引人注目的替代方案。我们的代码可在：<a target="_blank" rel="noopener" href="https://diffusion-scaling.github.io上找到./">https://diffusion-scaling.github.io上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15857v5">PDF</a> Project Webpage: <a target="_blank" rel="noopener" href="https://diffusion-scaling.github.io/">https://diffusion-scaling.github.io</a></p>
<p><strong>Summary</strong></p>
<p>在大型语言模型领域，自回归（AR）模型长期占据主导地位。近期，扩散式语言模型崭露头角，成为了一种有前景的替代方案，但其相较于AR模型的优势尚未得到充分探索。本文系统地研究了数据受限情境下的掩码扩散模型，发现当计算资源充足而数据稀缺时，其显著优于AR模型。扩散模型能更好地利用重复数据，实现更低的验证损失和更出色的下游性能。本文将其优势解读为隐式数据增强：掩码扩散使模型暴露于多样化的令牌排序和预测任务分布中，不同于AR的固定从左到右的分解方式。研究发现扩散模型的新尺度定律，并推导出临界计算阈值的闭式表达式，当数据而非计算成为瓶颈时，扩散模型提供了对标准AR范式的有力替代。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散式语言模型作为大型语言模型的替代方案开始受到关注。</li>
<li>在数据受限且计算资源充足的情境下，掩码扩散模型显著优于自回归（AR）模型。</li>
<li>扩散模型能更好地利用重复数据，实现更低的验证损失。</li>
<li>扩散模型具有隐式数据增强的特性，暴露于多样化的令牌排序和预测任务分布中。</li>
<li>与AR模型的固定从左到右分解方式不同，扩散模型具备更多变性。</li>
<li>研究发现了扩散模型的新尺度定律，并推导出临界计算阈值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15857">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-43d3a680de60f891e97b80fd525a7a86.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="A-Comparative-Study-of-Specialized-LLMs-as-Dense-Retrievers"><a href="#A-Comparative-Study-of-Specialized-LLMs-as-Dense-Retrievers" class="headerlink" title="A Comparative Study of Specialized LLMs as Dense Retrievers"></a>A Comparative Study of Specialized LLMs as Dense Retrievers</h2><p><strong>Authors:Hengran Zhang, Keping Bi, Jiafeng Guo</strong></p>
<p>While large language models (LLMs) are increasingly deployed as dense retrievers, the impact of their domain-specific specialization on retrieval effectiveness remains underexplored. This investigation systematically examines how task-specific adaptations in LLMs influence their retrieval capabilities, an essential step toward developing unified retrievers capable of handling text, code, images, and multimodal content. We conduct extensive experiments with eight Qwen2.5 7B LLMs, including base, instruction-tuned, code&#x2F;math-specialized, long reasoning, and vision-language models across zero-shot retrieval settings and the supervised setting. For the zero-shot retrieval settings, we consider text retrieval from the BEIR benchmark and code retrieval from the CoIR benchmark. Further, to evaluate supervised performance, all LLMs are fine-tuned on the MS MARCO dataset. We find that mathematical specialization and the long reasoning capability cause consistent degradation in three settings, indicating conflicts between mathematical reasoning and semantic matching. The vision-language model and code-specialized LLMs demonstrate superior zero-shot performance compared to other LLMs, even surpassing BM25 on the code retrieval task, and maintain comparable performance to base LLMs in supervised settings. These findings suggest promising directions for the unified retrieval task leveraging cross-domain and cross-modal fusion. </p>
<blockquote>
<p>随着大型语言模型（LLM）越来越多地被部署为密集检索器，其特定领域的专业化对检索效果的影响尚未得到充分探索。本研究系统地调查了LLM中的特定任务适应性如何影响它们的检索能力，这是朝着开发能够处理文本、代码、图像和多模态内容的统一检索器迈出的重要一步。我们在八个Qwen2.5 7B LLM上进行了广泛实验，包括基础模型、指令调优模型、代码&#x2F;数学专业化模型、长期推理模型和视觉语言模型，在零样本检索设置和有监督设置下进行了测试。对于零样本检索设置，我们考虑了来自BEIR基准测试的文本检索和来自CoIR基准测试的代码检索。此外，为了评估监督性能，所有LLM都在MS MARCO数据集上进行微调。我们发现数学专业化知识和长期推理能力在三种设置下均导致性能持续下降，这表明数学推理和语义匹配之间存在冲突。视觉语言模型和专门针对代码设计的LLM在零样本任务中显示出卓越的性能，甚至在代码检索任务上超越了BM25，并且在有监督环境中保持了与基础LLM相当的性能。这些发现表明，利用跨域和跨模态融合的统一检索任务具有广阔的发展前景。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03958v2">PDF</a> Accepted by CCIR25 and published by Springer LNCS or LNAI</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在密集检索中的应用日益广泛，但其域特定专业化对检索效果的影响尚未得到充分探索。本研究系统地探讨了LLM的任务特定适应性对其检索能力的影响，这是朝着开发能够处理文本、代码、图像和多模态内容的统一检索器迈出的重要一步。通过对八种不同专业领域的LLM进行广泛的实验，包括基础模型、指令调优模型、代码&#x2F;数学专业模型、逻辑推理模型和视觉语言模型等，发现数学专业化和逻辑推理能力在某些设置下会导致性能下降，表明数学推理和语义匹配之间存在冲突。视觉语言模型和代码专业LLM在零射击检索任务中表现优异，甚至在代码检索任务上超越了BM25，并在监督设置中的表现与其他LLM相当。这些发现表明，利用跨域和跨模态融合的统一检索任务具有广阔的发展前景。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在密集检索中的应用广泛，但其域特定专业化对检索效果的影响尚待探索。</li>
<li>任务特定适应性对LLM的检索能力有重要影响。</li>
<li>数学专业化和逻辑推理能力在某些设置下可能导致LLM性能下降。</li>
<li>视觉语言模型和代码专业LLM在零射击检索任务中表现优异。</li>
<li>视觉语言模型和代码专业LLM在代码检索任务上可能超越现有方法。</li>
<li>在监督设置下，专业LLM的表现与其他模型相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03958">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-013a99ff3c6a8ba2f36ed89ac4ac46b2.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-826cd0e8631ec02eb234f09c5a99cf00.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-08-09  MV-Debate Multi-view Agent Debate with Dynamic Reflection Gating for   Multimodal Harmful Content Detection in Social Media
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-41a22450d3db1ced069fec47b3e50b75.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-08-09  On the Generalization of SFT A Reinforcement Learning Perspective with   Reward Rectification
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25219.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
