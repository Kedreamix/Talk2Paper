<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-09  On the Generalization of SFT A Reinforcement Learning Perspective with   Reward Rectification">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-25a56c6761c0480b91ef3be33a13768d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-09-æ›´æ–°"><a href="#2025-08-09-æ›´æ–°" class="headerlink" title="2025-08-09 æ›´æ–°"></a>2025-08-09 æ›´æ–°</h1><h2 id="On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification"><a href="#On-the-Generalization-of-SFT-A-Reinforcement-Learning-Perspective-with-Reward-Rectification" class="headerlink" title="On the Generalization of SFT: A Reinforcement Learning Perspective with   Reward Rectification"></a>On the Generalization of SFT: A Reinforcement Learning Perspective with   Reward Rectification</h2><p><strong>Authors:Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, Ming-Hsuan Yang, Xu Yang</strong></p>
<p>We present a simple yet theoretically motivated improvement to Supervised Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited generalization compared to reinforcement learning (RL). Through mathematical analysis, we reveal that standard SFT gradients implicitly encode a problematic reward structure that may severely restrict the generalization capabilities of model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing gradient updates for each token by dynamically rescaling the objective function with the probability of this token. Remarkably, this single-line code change significantly outperforms standard SFT across multiple challenging benchmarks and base models, demonstrating greatly improved generalization. Additionally, our approach shows competitive results in offline RL settings, offering an effective yet simpler alternative. This work bridges theoretical insight and practical solutions, substantially advancing SFT performance. The code will be available at <a target="_blank" rel="noopener" href="https://github.com/yongliang-wu/DFT">https://github.com/yongliang-wu/DFT</a>. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œäº†ç®€å•ä½†ç†è®ºä¸Šçš„æ”¹è¿›ï¼Œè§£å†³äº†å…¶ä¸å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç›¸æ¯”çš„æœ‰é™æ³›åŒ–èƒ½åŠ›é—®é¢˜ã€‚é€šè¿‡æ•°å­¦åˆ†æï¼Œæˆ‘ä»¬å‘ç°æ ‡å‡†SFTæ¢¯åº¦éšå¼ç¼–ç äº†ä¸€ä¸ªæœ‰é—®é¢˜çš„å¥–åŠ±ç»“æ„ï¼Œè¿™å¯èƒ½ä¼šä¸¥é‡é™åˆ¶æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†çº æ­£è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€å¾®è°ƒï¼ˆDFTï¼‰ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´ç›®æ ‡å‡½æ•°ä¸è¯¥ä»¤ç‰Œçš„æ¦‚ç‡æ¥ç¨³å®šæ¯ä¸ªä»¤ç‰Œçš„æ¢¯åº¦æ›´æ–°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸€è¡Œä»£ç çš„æ”¹åŠ¨åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•å’ŒåŸºå‡†æ¨¡å‹ä¸Šæ˜¾è‘—ä¼˜äºæ ‡å‡†çš„SFTï¼Œè¡¨ç°å‡ºæå¤§çš„æ³›åŒ–æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨çº¿ä¸‹RLè®¾ç½®ä¸­ä¹Ÿè¡¨ç°å‡ºæœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œæä¾›äº†ä¸€ä¸ªæœ‰æ•ˆä½†æ›´ç®€å•çš„æ›¿ä»£æ–¹æ¡ˆã€‚è¿™é¡¹å·¥ä½œå°†ç†è®ºè§è§£ä¸å®é™…è§£å†³æ–¹æ¡ˆç›¸ç»“åˆï¼Œæå¤§åœ°æé«˜äº†SFTçš„æ€§èƒ½ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/yongliang-wu/DFT%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/yongliang-wu/DFTä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05629v1">PDF</a> 14 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„ä¸€ç§ç®€å•è€Œç†è®ºä¸Šçš„æ”¹è¿›æ–¹æ³•ï¼Œè§£å†³äº†å…¶ç›¸å¯¹äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æœ‰é™æ³›åŒ–é—®é¢˜ã€‚é€šè¿‡æ•°å­¦åˆ†æï¼Œä½œè€…å‘ç°æ ‡å‡†SFTæ¢¯åº¦éšå«äº†ä¸€ç§å¯èƒ½ä¸¥é‡é™åˆ¶æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„é—®é¢˜å¥–åŠ±ç»“æ„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†åŠ¨æ€å¾®è°ƒï¼ˆDFTï¼‰ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´ç›®æ ‡å‡½æ•°çš„æ¦‚ç‡æ¥ç¨³å®šæ¯ä¸ªæ ‡è®°çš„æ¢¯åº¦æ›´æ–°ã€‚è¿™ä¸€ç®€å•çš„ä»£ç æ”¹åŠ¨åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•å’ŒåŸºç¡€æ¨¡å‹ä¸Šæ˜¾è‘—ä¼˜äºæ ‡å‡†çš„SFTï¼Œè¡¨ç°å‡ºæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ç¦»çº¿RLè®¾ç½®ä¸­ä¹Ÿè¡¨ç°å‡ºå…·æœ‰ç«äº‰åŠ›çš„ç»“æœï¼Œæä¾›äº†ä¸€ç§æœ‰æ•ˆä¸”ç®€å•çš„æ›¿ä»£æ–¹æ¡ˆã€‚è¿™é¡¹å·¥ä½œå°†ç†è®ºè§è§£å’Œå®ç”¨è§£å†³æ–¹æ¡ˆç›¸ç»“åˆï¼Œå¤§å¤§æé«˜äº†SFTçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å­˜åœ¨æœ‰é™æ³›åŒ–é—®é¢˜ã€‚</li>
<li>æ ‡å‡†SFTæ¢¯åº¦éšå«äº†å¯èƒ½é™åˆ¶æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„é—®é¢˜å¥–åŠ±ç»“æ„ã€‚</li>
<li>åŠ¨æ€å¾®è°ƒï¼ˆDFTï¼‰é€šè¿‡åŠ¨æ€è°ƒæ•´ç›®æ ‡å‡½æ•°æ¦‚ç‡æ¥ç¨³å®šæ¢¯åº¦æ›´æ–°ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>DFTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•å’Œæ¨¡å‹ä¸Šæ˜¾è‘—ä¼˜äºæ ‡å‡†SFTã€‚</li>
<li>DFTåœ¨ç¦»çº¿RLè®¾ç½®ä¸­ä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>è¯¥å·¥ä½œç»“åˆäº†ç†è®ºåˆ†æå’Œå®ç”¨è§£å†³æ–¹æ¡ˆï¼Œæ¨åŠ¨äº†SFTçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9cb25f1c0d97726188f92047e12a433e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f08fd390f6bc1a892f430ec45de7d340.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="TrajEvo-Trajectory-Prediction-Heuristics-Design-via-LLM-driven-Evolution"><a href="#TrajEvo-Trajectory-Prediction-Heuristics-Design-via-LLM-driven-Evolution" class="headerlink" title="TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven   Evolution"></a>TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven   Evolution</h2><p><strong>Authors:Zhikai Zhao, Chuanbo Hua, Federico Berto, Kanghoon Lee, Zihan Ma, Jiachen Li, Jinkyoo Park</strong></p>
<p>Trajectory prediction is a critical task in modeling human behavior, especially in safety-critical domains such as social robotics and autonomous vehicle navigation. Traditional heuristics based on handcrafted rules often lack accuracy and generalizability. Although deep learning approaches offer improved performance, they typically suffer from high computational cost, limited explainability, and, importantly, poor generalization to out-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a framework that leverages Large Language Models (LLMs) to automatically design trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to generate and refine prediction heuristics from past trajectory data. We propose two key innovations: Cross-Generation Elite Sampling to encourage population diversity, and a Statistics Feedback Loop that enables the LLM to analyze and improve alternative predictions. Our evaluations demonstrate that TrajEvo outperforms existing heuristic methods across multiple real-world datasets, and notably surpasses both heuristic and deep learning methods in generalizing to an unseen OOD real-world dataset. TrajEvo marks a promising step toward the automated design of fast, explainable, and generalizable trajectory prediction heuristics. We release our source code to facilitate future research at <a target="_blank" rel="noopener" href="https://github.com/ai4co/trajevo">https://github.com/ai4co/trajevo</a>. </p>
<blockquote>
<p>è½¨è¿¹é¢„æµ‹æ˜¯æ¨¡æ‹Ÿäººç±»è¡Œä¸ºçš„å…³é”®ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¤¾ä¼šæœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶å¯¼èˆªç­‰å®‰å…¨å…³é”®é¢†åŸŸã€‚åŸºäºæ‰‹å·¥åˆ¶å®šçš„ä¼ ç»Ÿå¯å‘å¼æ–¹æ³•é€šå¸¸ç¼ºä¹å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ æ–¹æ³•å¯ä»¥æå‡æ€§èƒ½ï¼Œä½†å®ƒä»¬é€šå¸¸é¢ä¸´è®¡ç®—æˆæœ¬é«˜ã€è§£é‡Šæ€§å·®ä»¥åŠå¯¹äºåˆ†å¸ƒå¤–ï¼ˆOODï¼‰åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ä¸å¼ºç­‰é‡è¦é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†TrajEvoæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨è®¾è®¡è½¨è¿¹é¢„æµ‹å¯å‘å¼æ–¹æ³•ã€‚TrajEvoé‡‡ç”¨è¿›åŒ–ç®—æ³•æ ¹æ®è¿‡å»çš„è½¨è¿¹æ•°æ®ç”Ÿæˆå’Œæ”¹è¿›é¢„æµ‹å¯å‘å¼æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šè·¨ä»£ç²¾è‹±é‡‡æ ·ä»¥ä¿ƒè¿›ç§ç¾¤å¤šæ ·æ€§ï¼Œä»¥åŠä¸€ä¸ªç»Ÿè®¡åé¦ˆå¾ªç¯ï¼Œä½¿LLMèƒ½å¤Ÿåˆ†æå’Œæ”¹è¿›æ›¿ä»£é¢„æµ‹ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒTrajEvoåœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰çš„å¯å‘å¼æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æœªè§è¿‡çš„OODçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„æ³›åŒ–æ€§èƒ½æ˜¾è‘—è¶…è¶Šäº†å¯å‘å¼å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚TrajEvoæ ‡å¿—ç€æœç€è‡ªåŠ¨è®¾è®¡å¿«é€Ÿã€å¯è§£é‡Šå’Œå¯æ³›åŒ–çš„è½¨è¿¹é¢„æµ‹å¯å‘å¼æ–¹æ³•è¿ˆå‡ºäº†æœ‰å‰æ™¯çš„ä¸€æ­¥ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/ai4co/trajevo%E5%8F%91%E5%B8%83%E6%88%91%E4%BB%AC%E7%9A%84%E6%BA%90%E4%BB%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E4%BB%8A%E5%90%8E%E7%9A%84%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/ai4co/trajevoå‘å¸ƒæˆ‘ä»¬çš„æºä»£ç ï¼Œä»¥ä¿ƒè¿›ä»Šåçš„ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05616v1">PDF</a> arXiv admin note: substantial text overlap with arXiv:2505.04480</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†TrajEvoæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨è®¾è®¡è½¨è¿¹é¢„æµ‹å¯å‘å¼æ–¹æ³•ã€‚TrajEvoé‡‡ç”¨è¿›åŒ–ç®—æ³•ç”Ÿæˆå’Œç»†åŒ–é¢„æµ‹å¯å‘å¼æ–¹æ³•ï¼Œå¹¶æå‡ºä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šè·¨ä»£ç²¾è‹±é‡‡æ ·ä»¥ä¿ƒè¿›ç§ç¾¤å¤šæ ·æ€§ï¼Œä»¥åŠç»Ÿè®¡åé¦ˆå¾ªç¯ä½¿LLMèƒ½å¤Ÿåˆ†æå’Œæ”¹è¿›æ›¿ä»£é¢„æµ‹ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒTrajEvoåœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰å¯å‘å¼æ–¹æ³•ï¼Œå¹¶åœ¨æœªè§è¿‡çš„OODçœŸå®æ•°æ®é›†ä¸Šçš„è¡¨ç°å°¤ä¸ºçªå‡ºã€‚TrajEvoä¸ºå¿«é€Ÿã€å¯è§£é‡Šå’Œé€šç”¨çš„è½¨è¿¹é¢„æµ‹å¯å‘å¼æ–¹æ³•çš„è‡ªåŠ¨åŒ–è®¾è®¡è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TrajEvoæ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨è®¾è®¡è½¨è¿¹é¢„æµ‹å¯å‘å¼æ–¹æ³•ï¼Œä»¥æé«˜å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>TrajEvoé‡‡ç”¨è¿›åŒ–ç®—æ³•ç”Ÿæˆå’Œç»†åŒ–é¢„æµ‹å¯å‘å¼æ–¹æ³•ï¼Œé€šè¿‡Cross-Generation Elite Samplingå’ŒStatistics Feedback Loopä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹æ¥æå‡æ€§èƒ½ã€‚</li>
<li>è·¨ä»£ç²¾è‹±é‡‡æ ·æœ‰åŠ©äºé¼“åŠ±ç§ç¾¤å¤šæ ·æ€§ï¼Œä»è€Œæé«˜é¢„æµ‹æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›å’Œåˆ›æ–°èƒ½åŠ›ã€‚</li>
<li>ç»Ÿè®¡åé¦ˆå¾ªç¯ä½¿LLMèƒ½å¤Ÿåˆ†æå’Œæ”¹è¿›æ›¿ä»£é¢„æµ‹ï¼Œä»è€Œæé«˜æ¨¡å‹çš„è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>TrajEvoåœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿå¯å‘å¼æ–¹æ³•å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>TrajEvoèƒ½å¤Ÿå¾ˆå¥½åœ°æ³›åŒ–åˆ°æœªè§è¿‡çš„OODçœŸå®æ•°æ®é›†ä¸Šï¼Œæ˜¾ç¤ºå‡ºå…¶å¼ºå¤§çš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05616">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-84e7cbc93b7b06b42fe4a97e9bbd5dab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42f5833fee3b76a9d514febf70af2b84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-298c81aa3e9ec73a129994c170d43a5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3c07295d735a67af19c04a32c478f14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-555a9dde7ab48cca0568ad468e920e6b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Shuffle-R1-Efficient-RL-framework-for-Multimodal-Large-Language-Models-via-Data-centric-Dynamic-Shuffle"><a href="#Shuffle-R1-Efficient-RL-framework-for-Multimodal-Large-Language-Models-via-Data-centric-Dynamic-Shuffle" class="headerlink" title="Shuffle-R1: Efficient RL framework for Multimodal Large Language Models   via Data-centric Dynamic Shuffle"></a>Shuffle-R1: Efficient RL framework for Multimodal Large Language Models   via Data-centric Dynamic Shuffle</h2><p><strong>Authors:Linghao Zhu, Yiran Guan, Dingkang Liang, Jianzhong Ju, Zhenbo Luo, Bin Qin, Jian Luan, Yuliang Liu, Xiang Bai</strong></p>
<p>Reinforcement learning (RL) has emerged as an effective post-training paradigm for enhancing the reasoning capabilities of multimodal large language model (MLLM). However, current RL pipelines often suffer from training inefficiencies caused by two underexplored issues: Advantage Collapsing, where most advantages in a batch concentrate near zero, and Rollout Silencing, where the proportion of rollouts contributing non-zero gradients diminishes over time. These issues lead to suboptimal gradient updates and hinder long-term learning efficiency. To address these issues, we propose Shuffle-R1, a simple yet principled framework that improves RL fine-tuning efficiency by dynamically restructuring trajectory sampling and batch composition. It introduces (1) Pairwise Trajectory Sampling, which selects high-contrast trajectories with large advantages to improve gradient signal quality, and (2) Advantage-based Trajectory Shuffle, which increases exposure of valuable rollouts through informed batch reshuffling. Experiments across multiple reasoning benchmarks show that our framework consistently outperforms strong RL baselines with minimal overhead. These results highlight the importance of data-centric adaptations for more efficient RL training in MLLM. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²ç»æˆä¸ºæé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¨ç†èƒ½åŠ›çš„ä¸€ç§æœ‰æ•ˆçš„åè®­ç»ƒèŒƒå¼ã€‚ç„¶è€Œï¼Œå½“å‰çš„å¼ºåŒ–å­¦ä¹ ç®¡é“å¾€å¾€å­˜åœ¨è®­ç»ƒæ•ˆç‡ä¸é«˜çš„ç°è±¡ï¼Œè¿™ä¸»è¦ç”±ä¸¤ä¸ªå°šæœªè¢«å……åˆ†ç ”ç©¶çš„é—®é¢˜å¼•èµ·ï¼šä¼˜åŠ¿å´©å¡Œï¼Œå³ä¸€æ‰¹æ¬¡ä¸­çš„å¤§éƒ¨åˆ†ä¼˜åŠ¿é›†ä¸­åœ¨é›¶é™„è¿‘ï¼›ä»¥åŠæ‰©å±•æ²‰é»˜é—®é¢˜ï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œç”Ÿæˆæ ·æœ¬ä¸ºéé›¶æ¢¯åº¦çš„è´¡çŒ®æ¯”ä¾‹é€æ¸é™ä½ã€‚è¿™äº›é—®é¢˜å¯¼è‡´äº†æ¬¡ä¼˜çš„æ¢¯åº¦æ›´æ–°ï¼Œå¹¶é˜»ç¢äº†é•¿æœŸå­¦ä¹ æ•ˆç‡çš„æå‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Shuffle-R1æ¡†æ¶ï¼Œå®ƒé€šè¿‡åŠ¨æ€é‡æ„è½¨è¿¹é‡‡æ ·å’Œæ‰¹æ¬¡ç»„åˆæ¥æé«˜å¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒæ•ˆç‡ã€‚å®ƒå¼•å…¥äº†ï¼ˆ1ï¼‰é…å¯¹è½¨è¿¹é‡‡æ ·ï¼Œé€‰æ‹©å…·æœ‰è¾ƒå¤§ä¼˜åŠ¿çš„å¯¹æ¯”è½¨è¿¹æ¥æé«˜æ¢¯åº¦ä¿¡å·è´¨é‡ï¼›ï¼ˆ2ï¼‰åŸºäºä¼˜åŠ¿çš„è½¨è¿¹æ´—ç‰Œï¼Œé€šè¿‡ä¿¡æ¯ä¸°å¯Œçš„æ‰¹æ¬¡é‡æ´—ç‰Œæ¥å¢åŠ æœ‰ä»·å€¼çš„ç”Ÿæˆæ ·æœ¬çš„æ›å…‰ã€‚åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å§‹ç»ˆä¼˜äºå¼ºå¤§çš„å¼ºåŒ–å­¦ä¹ åŸºçº¿ï¼Œå¹¶ä¸”å…·æœ‰æœ€å°çš„é¢å¤–å¼€é”€ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„ç­–ç•¥å¯¹äºæé«˜MLLMä¸­çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ•ˆç‡çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05612v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½œä¸ºæå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¨ç†èƒ½åŠ›çš„åè®­ç»ƒèŒƒå¼ï¼Œæ•ˆæœæ˜¾è‘—ã€‚ç„¶è€Œï¼Œå½“å‰RLç®¡é“å­˜åœ¨è®­ç»ƒæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œä¸»è¦ç”±äºä¼˜åŠ¿å´©å¡Œå’Œæ»šåŠ¨æ²‰é»˜ä¸¤ä¸ªæœªå……åˆ†æ¢è®¨çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºShuffle-R1æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€é‡æ„è½¨è¿¹é‡‡æ ·å’Œæ‰¹æ¬¡ç»„æˆæ¥æé«˜RLå¾®è°ƒæ•ˆç‡ã€‚è¯¥æ¡†æ¶å¼•å…¥æˆå¯¹è½¨è¿¹é‡‡æ ·å’ŒåŸºäºä¼˜åŠ¿çš„è½¨è¿¹æ´—ç‰Œï¼Œæé«˜æ¢¯åº¦ä¿¡å·è´¨é‡å’Œæœ‰ä»·å€¼çš„æ»šåŠ¨ç»“æœçš„æ›å…‰åº¦ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºå¼ºå¤§çš„RLåŸºçº¿ï¼Œä¸”å‡ ä¹ä¸å¢åŠ å¼€é”€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½æœ‰æ•ˆæå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å½“å‰RLç®¡é“å­˜åœ¨è®­ç»ƒæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œä¸»è¦å› ä¸ºä¼˜åŠ¿å´©å¡Œå’Œæ»šåŠ¨æ²‰é»˜ã€‚</li>
<li>Shuffle-R1æ¡†æ¶é€šè¿‡åŠ¨æ€é‡æ„è½¨è¿¹é‡‡æ ·å’Œæ‰¹æ¬¡ç»„æˆæ¥æé«˜RLå¾®è°ƒæ•ˆç‡ã€‚</li>
<li>Shuffle-R1å¼•å…¥æˆå¯¹è½¨è¿¹é‡‡æ ·ï¼Œé€‰æ‹©é«˜å¯¹æ¯”åº¦çš„è½¨è¿¹ä»¥æé«˜æ¢¯åº¦ä¿¡å·è´¨é‡ã€‚</li>
<li>Shuffle-R1çš„åŸºäºä¼˜åŠ¿çš„è½¨è¿¹æ´—ç‰Œå¢åŠ æœ‰ä»·å€¼æ»šåŠ¨ç»“æœçš„æ›å…‰åº¦ã€‚</li>
<li>å®éªŒè¯æ˜Shuffle-R1æ¡†æ¶åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05612">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cc1d3a55c02ed2126dde61d9391bd17f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c2cba9eecf11bf9962322028086f1dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67b94c09311fb76397ad561c6baaba15.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef1e4d871002ce3fa6bd0f2aefc219a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94515193d32f5c47b63d72d74bdc87a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64777269f147350dce43e52793a00a66.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Uni-cot-Towards-Unified-Chain-of-Thought-Reasoning-Across-Text-and-Vision"><a href="#Uni-cot-Towards-Unified-Chain-of-Thought-Reasoning-Across-Text-and-Vision" class="headerlink" title="Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and   Vision"></a>Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and   Vision</h2><p><strong>Authors:Luozheng Qin, Jia Gong, Yuqing Sun, Tianjiao Li, Mengping Yang, Xiaomeng Yang, Chao Qu, Zhiyu Tan, Hao Li</strong></p>
<p>Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large Language Models (LLMs) by decomposing complex tasks into simpler, sequential subtasks. However, extending CoT to vision-language reasoning tasks remains challenging, as it often requires interpreting transitions of visual states to support reasoning. Existing methods often struggle with this due to limited capacity of modeling visual state transitions or incoherent visual trajectories caused by fragmented architectures.   To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought framework that enables coherent and grounded multimodal reasoning within a single unified model. The key idea is to leverage a model capable of both image understanding and generation to reason over visual content and model evolving visual states. However, empowering a unified model to achieve that is non-trivial, given the high computational cost and the burden of training. To address this, Uni-CoT introduces a novel two-level reasoning paradigm: A Macro-Level CoT for high-level task planning and A Micro-Level CoT for subtask execution. This design significantly reduces the computational overhead. Furthermore, we introduce a structured training paradigm that combines interleaved image-text supervision for macro-level CoT with multi-task objectives for micro-level CoT. Together, these innovations allow Uni-CoT to perform scalable and coherent multi-modal reasoning. Furthermore, thanks to our design, all experiments can be efficiently completed using only 8 A100 GPUs with 80GB VRAM each. Experimental results on reasoning-driven image generation benchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT demonstrates SOTA performance and strong generalization, establishing Uni-CoT as a promising solution for multi-modal reasoning. Project Page and Code: <a target="_blank" rel="noopener" href="https://sais-fuxi.github.io/projects/uni-cot/">https://sais-fuxi.github.io/projects/uni-cot/</a> </p>
<blockquote>
<p>é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼Œç®€ç§°CoTï¼‰æ¨ç†å·²è¢«å¹¿æ³›åº”ç”¨äºé€šè¿‡åˆ†è§£å¤æ‚ä»»åŠ¡ä¸ºæ›´ç®€å•ã€è¿ç»­çš„å­ä»»åŠ¡æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå°†CoTæ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨ç†ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒé€šå¸¸éœ€è¦è§£é‡Šè§†è§‰çŠ¶æ€çš„è¿‡æ¸¡æ¥æ”¯æŒæ¨ç†ã€‚ç°æœ‰æ–¹æ³•ç”±äºå»ºæ¨¡è§†è§‰çŠ¶æ€è¿‡æ¸¡çš„èƒ½åŠ›æœ‰é™æˆ–ç¢ç‰‡åŒ–æ¶æ„å¯¼è‡´çš„è§†è§‰è½¨è¿¹ä¸ä¸€è‡´ï¼Œå¾€å¾€åœ¨è¿™æ–¹é¢è¡¨ç°æŒ£æ‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05606v1">PDF</a> <a target="_blank" rel="noopener" href="https://sais-fuxi.github.io/projects/uni-cot/">https://sais-fuxi.github.io/projects/uni-cot/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•å°†Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†åº”ç”¨äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰è¯­è¨€æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„Chain-of-Thoughtæ¡†æ¶ï¼ˆUni-CoTï¼‰ï¼Œå®ƒèƒ½å¤Ÿåœ¨å•ä¸ªç»Ÿä¸€æ¨¡å‹ä¸­è¿›è¡Œè¿è´¯å’ŒåŸºäºåœ°é¢çš„å¤šæ¨¡æ€æ¨ç†ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ä¸¤çº§æ¨ç†æ¨¡å¼ï¼Œå³å®è§‚å±‚é¢çš„CoTç”¨äºé«˜çº§ä»»åŠ¡è§„åˆ’ï¼Œå¾®è§‚å±‚é¢çš„CoTç”¨äºå­ä»»åŠ¡æ‰§è¡Œï¼Œä»¥å‡å°‘è®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§ç»“æ„åŒ–çš„è®­ç»ƒæ¨¡å¼ï¼Œä»¥æ”¯æŒå®è§‚å’Œå¾®è§‚ä¸¤ä¸ªå±‚é¢çš„CoTçš„å¤šä»»åŠ¡ç›®æ ‡ã€‚è¿™äº›åˆ›æ–°ä½¿å¾—Uni-CoTåœ¨è§†è§‰è¯­è¨€æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Uni-CoTæ¡†æ¶æ—¨åœ¨è§£å†³å°†Chain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†åº”ç”¨äºè§†è§‰è¯­è¨€æ¨ç†ä»»åŠ¡çš„æŒ‘æˆ˜ã€‚</li>
<li>å®ƒé€šè¿‡åœ¨å•ä¸ªç»Ÿä¸€æ¨¡å‹ä¸­è¿›è¡Œè¿è´¯å’ŒåŸºäºåœ°é¢çš„å¤šæ¨¡æ€æ¨ç†æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚</li>
<li>Uni-CoTå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„ä¸¤çº§æ¨ç†æ¨¡å¼ï¼ŒåŒ…æ‹¬å®è§‚å±‚é¢çš„ä»»åŠ¡è§„åˆ’å’Œå¾®è§‚å±‚é¢çš„å­ä»»åŠ¡æ‰§è¡Œã€‚</li>
<li>æ¡†æ¶è¿˜é‡‡ç”¨äº†ä¸€ç§ç»“æ„åŒ–çš„è®­ç»ƒæ¨¡å¼æ¥æ”¯æŒå®è§‚å’Œå¾®è§‚CoTçš„å¤šä»»åŠ¡ç›®æ ‡ã€‚</li>
<li>è¯¥æ¡†æ¶çš„è®¡ç®—å¼€é”€å¾—åˆ°äº†æ˜¾è‘—é™ä½ï¼Œä½¿å¾—å®éªŒå¯ä»¥åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹å®Œæˆã€‚</li>
<li>Uni-CoTåœ¨è§†è§‰è¯­è¨€æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05606">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4f8a2849ab458672503129cc4edfe748.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b85ae310a96a848f495fcaea8acd402.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab1d0a8673e7d72f507cd7db7b5bc6da.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-775ba7f7244b3dbbc1d0efa72c338f02.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MathSmith-Towards-Extremely-Hard-Mathematical-Reasoning-by-Forging-Synthetic-Problems-with-a-Reinforced-Policy"><a href="#MathSmith-Towards-Extremely-Hard-Mathematical-Reasoning-by-Forging-Synthetic-Problems-with-a-Reinforced-Policy" class="headerlink" title="MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging   Synthetic Problems with a Reinforced Policy"></a>MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging   Synthetic Problems with a Reinforced Policy</h2><p><strong>Authors:Shaoxiong Zhan, Yanlin Lai, Ziyu Lu, Dahua Lin, Ziqing Yang, Fei Tang</strong></p>
<p>Large language models have achieved substantial progress in mathematical reasoning, yet their advancement is limited by the scarcity of high-quality, high-difficulty training data. Existing synthesis methods largely rely on transforming human-written templates, limiting both diversity and scalability. We propose MathSmith, a novel framework for synthesizing challenging mathematical problems to enhance LLM reasoning. Rather than modifying existing problems, MathSmith constructs new ones from scratch by randomly sampling concept-explanation pairs from PlanetMath, ensuring data independence and avoiding contamination. To increase difficulty, we design nine predefined strategies as soft constraints during rationales. We further adopts reinforcement learning to jointly optimize structural validity, reasoning complexity, and answer consistency. The length of the reasoning trace generated under autoregressive prompting is used to reflect cognitive complexity, encouraging the creation of more demanding problems aligned with long-chain-of-thought reasoning. Experiments across five benchmarks, categorized as easy &amp; medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025, OlympiadBench), show that MathSmith consistently outperforms existing baselines under both short and long CoT settings. Additionally, a weakness-focused variant generation module enables targeted improvement on specific concepts. Overall, MathSmith exhibits strong scalability, generalization, and transferability, highlighting the promise of high-difficulty synthetic data in advancing LLM reasoning capabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶è¿›å±•å—é™äºé«˜è´¨é‡ã€é«˜éš¾åº¦è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºæ€§ã€‚ç°æœ‰çš„åˆæˆæ–¹æ³•å¤§å¤šä¾èµ–äºè½¬æ¢äººå·¥ç¼–å†™çš„æ¨¡æ¿ï¼Œè¿™é™åˆ¶äº†å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬æå‡ºäº†MathSmithï¼Œè¿™æ˜¯ä¸€ä¸ªåˆæˆå…·æœ‰æŒ‘æˆ˜æ€§æ•°å­¦é—®é¢˜ä»¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°å‹æ¡†æ¶ã€‚MathSmithä¸æ˜¯ä¿®æ”¹ç°æœ‰é—®é¢˜ï¼Œè€Œæ˜¯ä»é›¶å¼€å§‹æ„å»ºæ–°é—®é¢˜ï¼Œé€šè¿‡ä»PlanetMathéšæœºæŠ½å–æ¦‚å¿µè§£é‡Šå¯¹ï¼Œç¡®ä¿æ•°æ®ç‹¬ç«‹ï¼Œé¿å…æ±¡æŸ“ã€‚ä¸ºäº†æé«˜éš¾åº¦ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¹ç§é¢„è®¾ç­–ç•¥ä½œä¸ºæ¨ç†è¿‡ç¨‹ä¸­çš„è½¯çº¦æŸã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¥è”åˆä¼˜åŒ–ç»“æ„æœ‰æ•ˆæ€§ã€æ¨ç†å¤æ‚æ€§å’Œç­”æ¡ˆä¸€è‡´æ€§ã€‚åœ¨è‡ªåŠ¨å›å½’æç¤ºä¸‹ç”Ÿæˆçš„æ¨ç†è½¨è¿¹é•¿åº¦è¢«ç”¨æ¥åæ˜ è®¤çŸ¥å¤æ‚æ€§ï¼Œé¼“åŠ±åˆ›å»ºä¸é•¿é“¾æ€ç»´æ¨ç†ç›¸ç¬¦çš„æ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒï¼Œè¢«åˆ†ç±»ä¸ºç®€å•å’Œä¸­ç­‰ï¼ˆGSM8Kï¼ŒMATH-500ï¼‰ä»¥åŠå›°éš¾ï¼ˆAIME2024ï¼ŒAIME2025ï¼ŒOlympiadBenchï¼‰ï¼Œè¡¨æ˜MathSmithåœ¨çŸ­é“¾å’Œé•¿é“¾æ€ç»´è®¾ç½®ä¸‹å‡ä¼˜äºç°æœ‰åŸºå‡†æµ‹è¯•ã€‚æ­¤å¤–ï¼Œå¼±ç‚¹èšç„¦çš„å˜ä½“ç”Ÿæˆæ¨¡å—èƒ½å¤Ÿå®ç°ç‰¹å®šæ¦‚å¿µçš„é’ˆå¯¹æ€§æ”¹è¿›ã€‚æ€»ä½“ä¸Šï¼ŒMathSmithè¡¨ç°å‡ºå¼ºå¤§çš„å¯æ‰©å±•æ€§ã€é€šç”¨æ€§å’Œå¯è¿ç§»æ€§ï¼Œçªæ˜¾äº†é«˜éš¾åº¦åˆæˆæ•°æ®åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05592v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶å‘å±•å—é™äºé«˜è´¨é‡ã€é«˜éš¾åº¦è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºæ€§ã€‚ç°æœ‰åˆæˆæ–¹æ³•ä¸»è¦ä¾èµ–äººå·¥ç¼–å†™æ¨¡æ¿çš„è½¬æ¢ï¼Œé™åˆ¶äº†å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚æœ¬æ–‡æå‡ºMathSmithæ¡†æ¶ï¼Œé€šè¿‡ä»PlanetMathä¸­éšæœºé‡‡æ ·æ¦‚å¿µè§£é‡Šå¯¹æ¥åˆæˆå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦é—®é¢˜ï¼Œä»¥å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ç»“æ„æœ‰æ•ˆæ€§ã€æ¨ç†å¤æ‚æ€§å’Œç­”æ¡ˆä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMathSmithåœ¨äº”ä¸ªä¸åŒéš¾åº¦çš„åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºå‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶å…·å¤‡å¼ºå¤§çš„å¯æ‰©å±•æ€§ã€é€šç”¨æ€§å’Œå¯è¿ç§»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢å·²å–å¾—è¿›å±•ï¼Œä½†ç¼ºä¹é«˜è´¨é‡ã€é«˜éš¾åº¦çš„è®­ç»ƒæ•°æ®é™åˆ¶äº†å…¶è¿›ä¸€æ­¥å‘å±•ã€‚</li>
<li>ç°æœ‰æ•°å­¦é—®é¢˜çš„åˆæˆæ–¹æ³•ä¸»è¦åŸºäºäººå·¥ç¼–å†™æ¨¡æ¿çš„è½¬æ¢ï¼Œè¿™é™åˆ¶äº†å¤šæ ·æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>MathSmithæ¡†æ¶é€šè¿‡éšæœºé‡‡æ ·æ¦‚å¿µè§£é‡Šå¯¹æ¥åˆæˆæ–°çš„æ•°å­¦é—®é¢˜ï¼Œç¡®ä¿æ•°æ®ç‹¬ç«‹ï¼Œé¿å…æ±¡æŸ“ã€‚</li>
<li>MathSmithé‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–ç»“æ„æœ‰æ•ˆæ€§ã€æ¨ç†å¤æ‚æ€§å’Œç­”æ¡ˆä¸€è‡´æ€§ã€‚</li>
<li>MathSmithç”Ÿæˆçš„é—®é¢˜ä¸é•¿æœŸæ€ç»´æ¨ç†ç›¸å¥‘åˆï¼Œåæ˜ äº†è®¤çŸ¥å¤æ‚æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMathSmithåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨å›°éš¾çº§åˆ«çš„æµ‹è¯•ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-74c9e73c00a8575183f21ffcb4d9f343.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9636410a6ac706d604480825f40f636a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47117c257600b3dc307d1807363174d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25a56c6761c0480b91ef3be33a13768d.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DART-Dual-Adaptive-Refinement-Transfer-for-Open-Vocabulary-Multi-Label-Recognition"><a href="#DART-Dual-Adaptive-Refinement-Transfer-for-Open-Vocabulary-Multi-Label-Recognition" class="headerlink" title="DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label   Recognition"></a>DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label   Recognition</h2><p><strong>Authors:Haijing Liu, Tao Pu, Hefeng Wu, Keze Wang, Liang Lin</strong></p>
<p>Open-Vocabulary Multi-Label Recognition (OV-MLR) aims to identify multiple seen and unseen object categories within an image, requiring both precise intra-class localization to pinpoint objects and effective inter-class reasoning to model complex category dependencies. While Vision-Language Pre-training (VLP) models offer a strong open-vocabulary foundation, they often struggle with fine-grained localization under weak supervision and typically fail to explicitly leverage structured relational knowledge beyond basic semantics, limiting performance especially for unseen classes. To overcome these limitations, we propose the Dual Adaptive Refinement Transfer (DART) framework. DART enhances a frozen VLP backbone via two synergistic adaptive modules. For intra-class refinement, an Adaptive Refinement Module (ARM) refines patch features adaptively, coupled with a novel Weakly Supervised Patch Selecting (WPS) loss that enables discriminative localization using only image-level labels. Concurrently, for inter-class transfer, an Adaptive Transfer Module (ATM) leverages a Class Relationship Graph (CRG), constructed using structured knowledge mined from a Large Language Model (LLM), and employs graph attention network to adaptively transfer relational information between class representations. DART is the first framework, to our knowledge, to explicitly integrate external LLM-derived relational knowledge for adaptive inter-class transfer while simultaneously performing adaptive intra-class refinement under weak supervision for OV-MLR. Extensive experiments on challenging benchmarks demonstrate that our DART achieves new state-of-the-art performance, validating its effectiveness. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡å¤šæ ‡ç­¾è¯†åˆ«ï¼ˆOV-MLRï¼‰æ—¨åœ¨è¯†åˆ«å›¾åƒä¸­çš„å¤šä¸ªå·²çŸ¥å’ŒæœªçŸ¥å¯¹è±¡ç±»åˆ«ï¼Œè¿™éœ€è¦ç²¾ç¡®çš„ç±»å†…å®šä½æ¥ç²¾ç¡®å®šä½å¯¹è±¡ï¼Œä»¥åŠæœ‰æ•ˆçš„ç±»é—´æ¨ç†æ¥æ¨¡æ‹Ÿå¤æ‚çš„ç±»åˆ«ä¾èµ–å…³ç³»ã€‚è™½ç„¶è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰æ¨¡å‹æä¾›äº†å¼ºå¤§çš„å¼€æ”¾è¯æ±‡åŸºç¡€ï¼Œä½†å®ƒä»¬é€šå¸¸åœ¨å¼±ç›‘ç£ä¸‹éš¾ä»¥å®ç°ç²¾ç»†çš„å®šä½ï¼Œå¹¶ä¸”é€šå¸¸æ— æ³•æ˜ç¡®åˆ©ç”¨è¶…è¶ŠåŸºæœ¬è¯­ä¹‰çš„ç»“æ„åŒ–å…³ç³»çŸ¥è¯†ï¼Œç‰¹åˆ«å¯¹äºæœªçŸ¥ç±»åˆ«çš„æ€§èƒ½æœ‰æ‰€é™åˆ¶ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŒè‡ªé€‚åº”ç»†åŒ–ä¼ è¾“ï¼ˆDARTï¼‰æ¡†æ¶ã€‚DARTé€šè¿‡ä¸¤ä¸ªååŒçš„è‡ªé€‚åº”æ¨¡å—å¢å¼ºäº†å†»ç»“çš„VLPéª¨å¹²ç½‘ã€‚å¯¹äºç±»å†…ç»†åŒ–ï¼Œè‡ªé€‚åº”ç»†åŒ–æ¨¡å—ï¼ˆARMï¼‰è‡ªé€‚åº”åœ°ä¼˜åŒ–è¡¥ä¸ç‰¹å¾ï¼Œå¹¶ç»“åˆæ–°é¢–çš„å¼±ç›‘ç£è¡¥ä¸é€‰æ‹©ï¼ˆWPSï¼‰æŸå¤±ï¼Œä»…ä½¿ç”¨å›¾åƒçº§æ ‡ç­¾å®ç°åŒºåˆ†å®šä½ã€‚åŒæ—¶ï¼Œå¯¹äºç±»é—´ä¼ è¾“ï¼Œè‡ªé€‚åº”ä¼ è¾“æ¨¡å—ï¼ˆATMï¼‰åˆ©ç”¨ç±»å…³ç³»å›¾ï¼ˆCRGï¼‰ï¼Œè¯¥å›¾ä½¿ç”¨ä»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­æŒ–æ˜çš„ç»“æ„åŒ–çŸ¥è¯†æ„å»ºï¼Œå¹¶é‡‡ç”¨å›¾æ³¨æ„åŠ›ç½‘ç»œè‡ªé€‚åº”åœ°ä¼ è¾“ç±»è¡¨ç¤ºä¹‹é—´çš„å…³ç³»ä¿¡æ¯ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒDARTæ˜¯ç¬¬ä¸€ä¸ªæ˜ç¡®æ•´åˆå¤–éƒ¨LLMæ´¾ç”Ÿå…³ç³»çŸ¥è¯†çš„æ¡†æ¶ï¼Œç”¨äºè‡ªé€‚åº”çš„ç±»é—´ä¼ è¾“ï¼ŒåŒæ—¶æ‰§è¡Œå¼±ç›‘ç£ä¸‹çš„è‡ªé€‚åº”ç±»å†…ç»†åŒ–ï¼Œä»¥å®ç°OV-MLRã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DARTè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05585v1">PDF</a> Accepted by ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Open-Vocabulary Multi-Label Recognitionï¼ˆOV-MLRï¼‰çš„ä»»åŠ¡ç›®æ ‡ï¼Œå³è¯†åˆ«å›¾åƒä¸­çš„å¤šä¸ªå·²çŸ¥å’ŒæœªçŸ¥å¯¹è±¡ç±»åˆ«ã€‚æ–‡ç« æŒ‡å‡ºï¼Œä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œéœ€è¦ç²¾ç¡®çš„ç±»å†…å®šä½å’Œæœ‰æ•ˆçš„ç±»é—´æ¨ç†ã€‚å°½ç®¡Vision-Language Pre-trainingï¼ˆVLPï¼‰æ¨¡å‹æä¾›äº†å¼ºå¤§çš„å¼€æ”¾è¯æ±‡åŸºç¡€ï¼Œä½†åœ¨å¼±ç›‘ç£ä¸‹éš¾ä»¥å®ç°ç²¾ç»†å®šä½ï¼Œå¹¶ä¸”æœªèƒ½æ˜ç¡®åˆ©ç”¨ç»“æ„åŒ–çš„å…³ç³»çŸ¥è¯†ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨æœªçŸ¥ç±»åˆ«ä¸Šçš„æ€§èƒ½ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæ–‡ç« æå‡ºäº†Dual Adaptive Refinement Transferï¼ˆDARTï¼‰æ¡†æ¶ã€‚DARTé€šè¿‡ä¸¤ä¸ªååŒçš„é€‚åº”æ€§æ¨¡å—å¢å¼ºäº†å†»ç»“çš„VLPéª¨å¹²ç½‘ã€‚å¯¹äºç±»å†…ç»†åŒ–ï¼Œè‡ªé€‚åº”ç»†åŒ–æ¨¡å—ï¼ˆARMï¼‰è‡ªé€‚åº”åœ°ä¼˜åŒ–è¡¥ä¸ç‰¹å¾ï¼Œå¹¶ç»“åˆæ–°çš„å¼±ç›‘ç£è¡¥ä¸é€‰æ‹©ï¼ˆWPSï¼‰æŸå¤±ï¼Œä½¿ç”¨å›¾åƒçº§æ ‡ç­¾å®ç°åˆ¤åˆ«å®šä½ã€‚å¯¹äºç±»é—´è½¬ç§»ï¼Œè‡ªé€‚åº”è½¬ç§»æ¨¡å—ï¼ˆATMï¼‰åˆ©ç”¨ç±»å…³ç³»å›¾ï¼ˆCRGï¼‰ï¼Œè¯¥å›¾æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŒ–æ˜çš„ç»“æ„çŸ¥è¯†æ„å»ºçš„ï¼Œå¹¶åº”ç”¨å›¾æ³¨æ„åŠ›ç½‘ç»œè‡ªé€‚åº”åœ°è½¬ç§»ç±»è¡¨ç¤ºä¹‹é—´çš„å…³ç³»ä¿¡æ¯ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒDARTæ˜¯ç¬¬ä¸€ä¸ªæ˜ç¡®æ•´åˆå¤–éƒ¨LLMæ´¾ç”Ÿå…³ç³»çŸ¥è¯†ä»¥é€‚åº”ç±»é—´è½¬ç§»ï¼ŒåŒæ—¶æ‰§è¡Œå¼±ç›‘ç£ä¸‹çš„è‡ªé€‚åº”ç±»å†…ç»†åŒ–çš„æ¡†æ¶ï¼Œç”¨äºOV-MLRã€‚å®éªŒè¡¨æ˜ï¼ŒDARTå–å¾—äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Open-Vocabulary Multi-Label Recognition (OV-MLR) è¦æ±‚åŒæ—¶å®ç°ç²¾ç¡®çš„ç±»å†…å®šä½å’Œæœ‰æ•ˆçš„ç±»é—´æ¨ç†ã€‚</li>
<li>Vision-Language Pre-training (VLP) æ¨¡å‹è™½ç„¶åœ¨å¼€æ”¾è¯æ±‡æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¼±ç›‘ç£ä¸‹çš„ç²¾ç»†å®šä½æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>DARTæ¡†æ¶é€šè¿‡ä¸¤ä¸ªååŒçš„é€‚åº”æ€§æ¨¡å—ï¼ˆARMå’ŒATMï¼‰å…‹æœäº†VLPæ¨¡å‹çš„å±€é™æ€§ã€‚</li>
<li>ARMæ¨¡å—é€šè¿‡è‡ªé€‚åº”ä¼˜åŒ–è¡¥ä¸ç‰¹å¾å¹¶ç»“åˆWPSæŸå¤±å®ç°åˆ¤åˆ«å®šä½ã€‚</li>
<li>ATMæ¨¡å—åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŒ–æ˜çš„ç»“æ„çŸ¥è¯†æ„å»ºç±»å…³ç³»å›¾ï¼ˆCRGï¼‰ã€‚</li>
<li>DARTæ˜¯ç¬¬ä¸€ä¸ªæ•´åˆLLMæ´¾ç”Ÿå…³ç³»çŸ¥è¯†çš„æ¡†æ¶ï¼Œç”¨äºè‡ªé€‚åº”ç±»é—´è½¬ç§»å’Œç±»å†…ç»†åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05585">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b2ad9f880be9504be1e6ddc5387cb804.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-935c74b538c2af6b708733599980c8cb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-341137980ff334eab2ab3d99b5e461ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ba4976fce4ae091c87fedabc552d739.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7fd23fce6f669632b65f925bf876acb8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Iterative-Learning-of-Computable-Phenotypes-for-Treatment-Resistant-Hypertension-using-Large-Language-Models"><a href="#Iterative-Learning-of-Computable-Phenotypes-for-Treatment-Resistant-Hypertension-using-Large-Language-Models" class="headerlink" title="Iterative Learning of Computable Phenotypes for Treatment Resistant   Hypertension using Large Language Models"></a>Iterative Learning of Computable Phenotypes for Treatment Resistant   Hypertension using Large Language Models</h2><p><strong>Authors:Guilherme Seidyo Imai Aldeia, Daniel S. Herman, William G. La Cava</strong></p>
<p>Large language models (LLMs) have demonstrated remarkable capabilities for medical question answering and programming, but their potential for generating interpretable computable phenotypes (CPs) is under-explored. In this work, we investigate whether LLMs can generate accurate and concise CPs for six clinical phenotypes of varying complexity, which could be leveraged to enable scalable clinical decision support to improve care for patients with hypertension. In addition to evaluating zero-short performance, we propose and test a synthesize, execute, debug, instruct strategy that uses LLMs to generate and iteratively refine CPs using data-driven feedback. Our results show that LLMs, coupled with iterative learning, can generate interpretable and reasonably accurate programs that approach the performance of state-of-the-art ML methods while requiring significantly fewer training examples. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—é—®é¢˜å›ç­”å’Œç¼–ç¨‹æ–¹é¢å±•ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å…¶åœ¨ç”Ÿæˆå¯è§£é‡Šçš„è®¡ç®—è¡¨å‹ï¼ˆCPsï¼‰æ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†LLMæ˜¯å¦èƒ½ä¸ºå…­ç§ä¸åŒå¤æ‚åº¦çš„ä¸´åºŠè¡¨å‹ç”Ÿæˆå‡†ç¡®ä¸”ç®€æ´çš„CPsï¼Œè¿™å¯èƒ½ç”¨äºå®ç°å¯æ‰©å±•çš„ä¸´åºŠå†³ç­–æ”¯æŒï¼Œä»¥æ”¹å–„é«˜è¡€å‹æ‚£è€…çš„æŠ¤ç†ã€‚é™¤äº†è¯„ä¼°é›¶çŸ­æ€§èƒ½å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºå¹¶æµ‹è¯•äº†ä¸€ç§åˆæˆã€æ‰§è¡Œã€è°ƒè¯•ã€æŒ‡å¯¼ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åˆ©ç”¨LLMç”Ÿæˆå¹¶ä½¿ç”¨æ•°æ®é©±åŠ¨çš„åé¦ˆæ¥è¿­ä»£ä¼˜åŒ–CPsã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç»“åˆè¿­ä»£å­¦ä¹ ï¼ŒLLMå¯ä»¥ç”Ÿæˆå¯è§£é‡Šä¸”ç›¸å½“å‡†ç¡®çš„ç¨‹åºï¼Œå…¶æ€§èƒ½æ¥è¿‘æœ€æ–°MLæ–¹æ³•ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†è®­ç»ƒæ ·æœ¬çš„éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05581v1">PDF</a> To appear in PMLR, Volume 298, Machine Learning for Healthcare, 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—é—®é¢˜å›ç­”å’Œç¼–ç¨‹æ–¹é¢å±•ç°å‡ºæ˜¾è‘—èƒ½åŠ›ï¼Œä½†å…¶ç”Ÿæˆå¯è§£é‡Šçš„è®¡ç®—è¡¨å‹ï¼ˆCPsï¼‰çš„æ½œåŠ›å°šæœªè¢«å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶æ¢è®¨LLMsæ˜¯å¦èƒ½é’ˆå¯¹å…­ç§ä¸åŒå¤æ‚åº¦çš„ä¸´åºŠè¡¨å‹ç”Ÿæˆå‡†ç¡®ç®€è¦çš„CPsï¼Œè¿™å¯ç”¨äºå®ç°å¯æ‰©å±•çš„ä¸´åºŠå†³ç­–æ”¯æŒï¼Œæé«˜é«˜è¡€å‹æ‚£è€…çš„æŠ¤ç†æ°´å¹³ã€‚é™¤äº†è¯„ä¼°é›¶çŸ­æ€§èƒ½å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åˆæˆã€æ‰§è¡Œã€è°ƒè¯•ã€æŒ‡å¯¼çš„ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åˆ©ç”¨LLMsä»¥æ•°æ®é©±åŠ¨åé¦ˆæ¥ç”Ÿæˆå¹¶è¿­ä»£ä¼˜åŒ–CPsã€‚ç»“æœè¡¨æ˜ï¼Œç»“åˆè¿­ä»£å­¦ä¹ ï¼ŒLLMså¯ä»¥ç”Ÿæˆå¯è§£é‡Šä¸”ç›¸å¯¹å‡†ç¡®çš„ç¨‹åºï¼Œå…¶æ€§èƒ½æ¥è¿‘æœ€æ–°æœºå™¨å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†å¯¹è®­ç»ƒæ ·æœ¬çš„éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså…·å¤‡ç”Ÿæˆå¯è§£é‡Šçš„è®¡ç®—è¡¨å‹ï¼ˆCPsï¼‰çš„æ½œåŠ›ï¼Œè¿™å¯¹äºä¸´åºŠå†³ç­–æ”¯æŒå…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>æœ¬ç ”ç©¶æ¢è®¨äº†LLMsåœ¨é’ˆå¯¹å…­ç§ä¸åŒå¤æ‚åº¦çš„ä¸´åºŠè¡¨å‹ç”ŸæˆCPsæ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åˆæˆã€æ‰§è¡Œã€è°ƒè¯•ã€æŒ‡å¯¼çš„ç­–ç•¥ï¼Œåˆ©ç”¨LLMså’Œæ•°æ®è¿›è¡Œåé¦ˆä»¥è¿­ä»£ä¼˜åŒ–CPsçš„ç”Ÿæˆã€‚</li>
<li>LLMsç»“åˆè¿­ä»£å­¦ä¹ å¯ä»¥ç”Ÿæˆå‡†ç¡®ä¸”å¯è§£é‡Šçš„ç¨‹åºï¼Œå…¶æ€§èƒ½æ¥è¿‘æœ€æ–°æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>LLMsåœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ï¼Œå¦‚åŒ»ç–—é—®é¢˜å›ç­”ï¼Œå¾—åˆ°äº†è¿›ä¸€æ­¥å‘å±•ã€‚</li>
<li>æœ¬ç ”ç©¶çš„ç»“æœè¡¨æ˜ï¼ŒLLMsåœ¨ç¼–ç¨‹æ–¹é¢çš„èƒ½åŠ›åŒæ ·å€¼å¾—è¿›ä¸€æ­¥æ¢ç´¢å’Œåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a838ecccf04a3029f52cdcd18c591ad3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-245bc587759d64eca50f94564ccb756b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Conformal-Sets-in-Multiple-Choice-Question-Answering-under-Black-Box-Settings-with-Provable-Coverage-Guarantees"><a href="#Conformal-Sets-in-Multiple-Choice-Question-Answering-under-Black-Box-Settings-with-Provable-Coverage-Guarantees" class="headerlink" title="Conformal Sets in Multiple-Choice Question Answering under Black-Box   Settings with Provable Coverage Guarantees"></a>Conformal Sets in Multiple-Choice Question Answering under Black-Box   Settings with Provable Coverage Guarantees</h2><p><strong>Authors:Guang Yang, Xinyang Liu</strong></p>
<p>Large Language Models (LLMs) have shown remarkable progress in multiple-choice question answering (MCQA), but their inherent unreliability, such as hallucination and overconfidence, limits their application in high-risk domains. To address this, we propose a frequency-based uncertainty quantification method under black-box settings, leveraging conformal prediction (CP) to ensure provable coverage guarantees. Our approach involves multiple independent samplings of the modelâ€™s output distribution for each input, with the most frequent sample serving as a reference to calculate predictive entropy (PE). Experimental evaluations across six LLMs and four datasets (MedMCQA, MedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms logit-based PE in distinguishing between correct and incorrect predictions, as measured by AUROC. Furthermore, the method effectively controls the empirical miscoverage rate under user-specified risk levels, validating that sampling frequency can serve as a viable substitute for logit-based probabilities in black-box scenarios. This work provides a distribution-free model-agnostic framework for reliable uncertainty quantification in MCQA with guaranteed coverage, enhancing the trustworthiness of LLMs in practical applications. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šé€‰é—®ç­”ï¼ˆMCQAï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å®ƒä»¬çš„å›ºæœ‰ä¸å¯é æ€§ï¼Œå¦‚å¹»è±¡å’Œè¿‡åº¦è‡ªä¿¡ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨é«˜é£é™©é¢†åŸŸçš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé¢‘ç‡çš„ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨é»‘è‰²èƒŒæ™¯ä¸‹è®¾ç½®ï¼Œåˆ©ç”¨é€‚å½¢é¢„æµ‹ï¼ˆCPï¼‰ç¡®ä¿å¯è¯æ˜è¦†ç›–ä¿è¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¶‰åŠå¯¹æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒè¿›è¡Œå¤šæ¬¡ç‹¬ç«‹é‡‡æ ·ï¼Œæ¯ä¸ªè¾“å…¥æ ·æœ¬æœ€é¢‘ç¹çš„å‡ºç°ä½œä¸ºå‚è€ƒæ¥è®¡ç®—é¢„æµ‹ç†µï¼ˆPEï¼‰ã€‚åœ¨å…­ä¸ªLLMå’Œå››ä¸ªæ•°æ®é›†ï¼ˆMedMCQAã€MedQAã€MMLUã€MMLU-Proï¼‰ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒåŸºäºé¢‘ç‡çš„PEåœ¨åŒºåˆ†æ­£ç¡®å’Œé”™è¯¯é¢„æµ‹æ–¹é¢ä¼˜äºåŸºäºLogitçš„PEï¼Œè¿™é€šè¿‡AUROCæ¥è¡¡é‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåœ°æ§åˆ¶äº†ç”¨æˆ·æŒ‡å®šé£é™©æ°´å¹³ä¸‹çš„ç»éªŒè¯¯è¦†ç›–ç‡ï¼ŒéªŒè¯äº†é‡‡æ ·é¢‘ç‡å¯ä»¥ä½œä¸ºé»‘è‰²èƒŒæ™¯ä¸‹åŸºäºLogitæ¦‚ç‡çš„å¯è¡Œæ›¿ä»£æ–¹æ¡ˆã€‚è¿™é¡¹å·¥ä½œæä¾›äº†ä¸€ä¸ªåˆ†å¸ƒæ— å…³æ¨¡å‹æ¡†æ¶ï¼Œç”¨äºå¯é çš„ä¸ç¡®å®šæ€§é‡åŒ–MCQAä¸­çš„ä¿è¯è¦†ç›–ï¼Œå¢å¼ºäº†LLMåœ¨å®é™…åº”ç”¨ä¸­çš„å¯ä¿¡åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05544v1">PDF</a> under review</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šé€‰é¢˜å›ç­”ï¼ˆMCQAï¼‰æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶å†…åœ¨çš„ä¸ç¡®å®šæ€§ï¼Œå¦‚å¹»è±¡å’Œè¿‡åº¦è‡ªä¿¡ï¼Œé™åˆ¶äº†å…¶åœ¨é«˜é£é™©é¢†åŸŸçš„åº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§åŸºäºé¢‘ç‡çš„ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ï¼Œé‡‡ç”¨é»‘è‰²åœºæ™¯ä¸‹åŸºäºä¾å„’çš„é¢„æµ‹æ–¹æ³•ç¡®ä¿å¯éªŒè¯è¦†ç›–ä¿è¯ã€‚è¯¥æ–¹æ³•é€šè¿‡å¤šæ¬¡ç‹¬ç«‹é‡‡æ ·æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒæ¥ç¡®å®šæ¯ä¸ªè¾“å…¥çš„é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒä¸­æœ€é¢‘ç¹å‡ºç°çš„æ ·æœ¬ï¼Œè®¡ç®—é¢„æµ‹ç†µï¼ˆPEï¼‰ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºé¢‘ç‡çš„PEç›¸è¾ƒäºåŸºäºLogitçš„PEæ›´èƒ½æœ‰æ•ˆåŒºåˆ†æ­£ç¡®ä¸é”™è¯¯çš„é¢„æµ‹ï¼Œå¹¶æˆåŠŸæ§åˆ¶å®è¯è¦†ç›–åº¦åœ¨ç”¨æˆ·æŒ‡å®šçš„é£é™©æ°´å¹³ä¹‹ä¸‹ã€‚æœ¬ç ”ç©¶æä¾›äº†ä¸€ä¸ªåˆ†å¸ƒæ— å…³ã€æ¨¡å‹é€šç”¨çš„æ¡†æ¶ï¼Œç”¨äºå¯é çš„ä¸ç¡®å®šæ€§é‡åŒ–MCQAï¼Œå¢å¼ºäº†LLMåœ¨å®é™…åº”ç”¨ä¸­çš„å¯ä¿¡åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤šé€‰é¢˜å›ç­”ï¼ˆMCQAï¼‰æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å­˜åœ¨å†…åœ¨ä¸ç¡®å®šæ€§é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºé¢‘ç‡çš„ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨åŸºäºä¾å„’çš„é¢„æµ‹æ–¹æ³•æ¥ç¡®ä¿å¯éªŒè¯è¦†ç›–ä¿è¯ã€‚</li>
<li>é€šè¿‡å¤šæ¬¡ç‹¬ç«‹é‡‡æ ·æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒæ¥ç¡®å®šé¢„æµ‹æ¦‚ç‡åˆ†å¸ƒä¸­æœ€é¢‘ç¹å‡ºç°çš„æ ·æœ¬ã€‚</li>
<li>åŸºäºé¢‘ç‡çš„PEç›¸è¾ƒäºåŸºäºLogitçš„PEæ›´èƒ½æœ‰æ•ˆåŒºåˆ†æ­£ç¡®ä¸é”™è¯¯çš„é¢„æµ‹ã€‚</li>
<li>æˆåŠŸæ§åˆ¶å®è¯è¦†ç›–åº¦åœ¨ç”¨æˆ·æŒ‡å®šçš„é£é™©æ°´å¹³ä¹‹ä¸‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05544">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-368b35971580fef96ab27036b47896f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34c5b9c3027dfc37ef679eb92690343c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5d5b9c9f986076bb4d73bccd85b3edef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04a361e00caebc5ac686de11c55d7edf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26873d33aab9819cfb1d1c128c10114a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e57bf32df2587c93e47081831ab493d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="AI-vs-Human-Moderators-A-Comparative-Evaluation-of-Multimodal-LLMs-in-Content-Moderation-for-Brand-Safety"><a href="#AI-vs-Human-Moderators-A-Comparative-Evaluation-of-Multimodal-LLMs-in-Content-Moderation-for-Brand-Safety" class="headerlink" title="AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in   Content Moderation for Brand Safety"></a>AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in   Content Moderation for Brand Safety</h2><p><strong>Authors:Adi Levi, Or Levi, Sardhendu Mishra, Jonathan Morra</strong></p>
<p>As the volume of video content online grows exponentially, the demand for moderation of unsafe videos has surpassed human capabilities, posing both operational and mental health challenges. While recent studies demonstrated the merits of Multimodal Large Language Models (MLLMs) in various video understanding tasks, their application to multimodal content moderation, a domain that requires nuanced understanding of both visual and textual cues, remains relatively underexplored. In this work, we benchmark the capabilities of MLLMs in brand safety classification, a critical subset of content moderation for safe-guarding advertising integrity. To this end, we introduce a novel, multimodal and multilingual dataset, meticulously labeled by professional reviewers in a multitude of risk categories. Through a detailed comparative analysis, we demonstrate the effectiveness of MLLMs such as Gemini, GPT, and Llama in multimodal brand safety, and evaluate their accuracy and cost efficiency compared to professional human reviewers. Furthermore, we present an in-depth discussion shedding light on limitations of MLLMs and failure cases. We are releasing our dataset alongside this paper to facilitate future research on effective and responsible brand safety and content moderation. </p>
<blockquote>
<p>éšç€åœ¨çº¿è§†é¢‘å†…å®¹çš„æ•°é‡å‘ˆæŒ‡æ•°çº§å¢é•¿ï¼Œå¯¹ä¸å®‰å…¨è§†é¢‘çš„é€‚åº¦éœ€æ±‚å·²ç»è¶…è¶Šäº†äººç±»çš„èƒ½åŠ›ï¼Œå¸¦æ¥äº†è¿è¥å’Œå¿ƒç†å¥åº·æ–¹é¢çš„æŒ‘æˆ˜ã€‚è™½ç„¶æœ€è¿‘æœ‰ç ”ç©¶è¡¨æ˜ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å„ç§è§†é¢‘ç†è§£ä»»åŠ¡ä¸­å…·æœ‰ä¼˜åŠ¿ï¼Œä½†å®ƒä»¬åœ¨éœ€è¦è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ç»†å¾®ç†è§£çš„å¤šæ¨¡æ€å†…å®¹é€‚åº¦åº”ç”¨æ–¹é¢ï¼Œç ”ç©¶ç›¸å¯¹è¾ƒå°‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»¥å“ç‰Œå®‰å…¨åˆ†ç±»ä¸ºæ ‡å‡†ï¼Œè¯„ä¼°äº†å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œè¿™æ˜¯å†…å®¹é€‚åº¦ä¸­ä¿æŠ¤å¹¿å‘Šå®Œæ•´æ€§çš„å…³é”®å­é›†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å¤šæ¨¡æ€å’Œå¤šè¯­è¨€æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ç»è¿‡ä¸“ä¸šè¯„è®ºäººå‘˜åœ¨å„ç§é£é™©ç±»åˆ«ä¸­ä»”ç»†æ ‡æ³¨ã€‚é€šè¿‡è¯¦ç»†æ¯”è¾ƒåˆ†æï¼Œæˆ‘ä»¬å±•ç¤ºäº†Geminiã€GPTå’ŒLlamaç­‰å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€å“ç‰Œå®‰å…¨æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬ä¸ä¸“ä¸šäººå·¥å®¡æ ¸å‘˜ç›¸æ¯”çš„å‡†ç¡®æ€§å’Œæˆæœ¬æ•ˆç›Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å±€é™æ€§åŠå¤±è´¥æ¡ˆä¾‹è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚æˆ‘ä»¬éšè¿™ç¯‡è®ºæ–‡ä¸€èµ·å‘å¸ƒæˆ‘ä»¬çš„æ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›æœªæ¥åœ¨æœ‰æ•ˆå’Œè´Ÿè´£ä»»çš„å“ç‰Œå®‰å…¨å’Œå†…å®¹é€‚åº¦æ–¹é¢çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05527v1">PDF</a> Accepted to the Computer Vision in Advertising and Marketing (CVAM)   workshop at ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>éšç€åœ¨çº¿è§†é¢‘å†…å®¹çš„æŒ‡æ•°çº§å¢é•¿ï¼Œå¯¹å®‰å…¨è§†é¢‘çš„ç®¡ç†éœ€æ±‚å·²ç»è¶…è¶Šäº†äººå·¥å¤„ç†çš„èƒ½åŠ›ï¼Œå¸¦æ¥äº†è¿è¥å’Œå¿ƒç†å¥åº·æ–¹é¢çš„æŒ‘æˆ˜ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç§è§†é¢‘ç†è§£ä»»åŠ¡ä¸­å…·æœ‰ä¼˜åŠ¿ï¼Œä½†åœ¨éœ€è¦è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢ç»†å¾®ç†è§£çš„å¤šæ¨¡æ€å†…å®¹ç®¡ç†æ–¹é¢åº”ç”¨è¾ƒå°‘ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°MLLMsåœ¨å“ç‰Œå®‰å…¨åˆ†ç±»ä¸­çš„èƒ½åŠ›â€”â€”å†…å®¹ç®¡ç†çš„ä¸€ä¸ªå…³é”®é¢†åŸŸï¼Œä»¥ä¿æŠ¤å¹¿å‘Šçš„çœŸå®æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°é¢–çš„å¤šæ¨¡æ€ã€å¤šè¯­è¨€æ•°æ®é›†ï¼Œç”±ä¸“ä¸šè¯„å®¡äººå‘˜è¿›è¡Œäº†é£é™©ç±»åˆ«çš„ç»†è‡´æ ‡æ³¨ã€‚é€šè¿‡è¯¦ç»†å¯¹æ¯”åˆ†æï¼Œæˆ‘ä»¬è¯æ˜äº†å¦‚Geminiã€GPTå’ŒLlamaç­‰MLLMsåœ¨å¤šæ¨¡æ€å“ç‰Œå®‰å…¨æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬åœ¨ç²¾åº¦å’Œæˆæœ¬æ•ˆç›Šæ–¹é¢ç›¸è¾ƒäºä¸“ä¸šè¯„å®¡äººå‘˜çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ·±å…¥æ¢è®¨äº†MLLMsçš„å±€é™æ€§å’Œå¤±è´¥æ¡ˆä¾‹ã€‚åŒæ—¶å‘å¸ƒçš„æ•°æ®é›†å°†ä¿ƒè¿›æœªæ¥å…³äºæœ‰æ•ˆå’Œè´Ÿè´£ä»»çš„å“ç‰Œå®‰å…¨å’Œå†…å®¹ç®¡ç†çš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨çº¿è§†é¢‘å†…å®¹çš„å¢é•¿å¯¼è‡´äº†å¯¹å®‰å…¨è§†é¢‘ç®¡ç†çš„å·¨å¤§éœ€æ±‚ï¼ŒæŒ‘æˆ˜äº†äººå·¥å¤„ç†çš„èƒ½åŠ›ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­å…·æœ‰ä¼˜åŠ¿ï¼Œä½†åœ¨å¤šæ¨¡æ€å†…å®¹ç®¡ç†æ–¹é¢çš„åº”ç”¨ä»ç›¸å¯¹è¾ƒå°‘ã€‚</li>
<li>ç ”ç©¶èšç„¦äºè¯„ä¼°MLLMsåœ¨å“ç‰Œå®‰å…¨åˆ†ç±»æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªå¤šæ¨¡æ€ã€å¤šè¯­è¨€çš„æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°MLLMsçš„æ€§èƒ½ï¼Œè¯¥æ•°æ®é›†ç”±ä¸“ä¸šè¯„å®¡äººå‘˜è¿›è¡Œç»†è‡´æ ‡æ³¨ã€‚</li>
<li>å¯¹æ¯”åˆ†æäº†MLLMsä¸ä¸“ä¸šè¯„å®¡äººå‘˜åœ¨å“ç‰Œå®‰å…¨åˆ†ç±»æ–¹é¢çš„è¡¨ç°ï¼Œæ˜¾ç¤ºå‡ºMLLMsçš„æœ‰æ•ˆæ€§ã€ç²¾åº¦å’Œæˆæœ¬æ•ˆç›Šã€‚</li>
<li>è®¨è®ºäº†MLLMsçš„å±€é™æ€§å’Œå¤±è´¥æ¡ˆä¾‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05527">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-be0e77bf4d422023dc2ed895f648108c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e72e38033ab58c3c60eb587eed8478c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-770f18bdb228a78c95d23edc244ce114.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa5dca44490dffa9468181110f90e8eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-174de568ec853a5b3f60e712f027f66b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49881015cdc1707f22800b2f9a0b57ec.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="The-World-According-to-LLMs-How-Geographic-Origin-Influences-LLMsâ€™-Entity-Deduction-Capabilities"><a href="#The-World-According-to-LLMs-How-Geographic-Origin-Influences-LLMsâ€™-Entity-Deduction-Capabilities" class="headerlink" title="The World According to LLMs: How Geographic Origin Influences LLMsâ€™   Entity Deduction Capabilities"></a>The World According to LLMs: How Geographic Origin Influences LLMsâ€™   Entity Deduction Capabilities</h2><p><strong>Authors:Harsh Nishant Lalai, Raj Sanjay Shah, Jiaxin Pei, Sashank Varma, Yi-Chia Wang, Ali Emami</strong></p>
<p>Large Language Models (LLMs) have been extensively tuned to mitigate explicit biases, yet they often exhibit subtle implicit biases rooted in their pre-training data. Rather than directly probing LLMs with human-crafted questions that may trigger guardrails, we propose studying how models behave when they proactively ask questions themselves. The 20 Questions game, a multi-turn deduction task, serves as an ideal testbed for this purpose. We systematically evaluate geographic performance disparities in entity deduction using a new dataset, Geo20Q+, consisting of both notable people and culturally significant objects (e.g., foods, landmarks, animals) from diverse regions. We test popular LLMs across two gameplay configurations (canonical 20-question and unlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese, French, Spanish, and Turkish). Our results reveal geographic disparities: LLMs are substantially more successful at deducing entities from the Global North than the Global South, and the Global West than the Global East. While Wikipedia pageviews and pre-training corpus frequency correlate mildly with performance, they fail to fully explain these disparities. Notably, the language in which the game is played has minimal impact on performance gaps. These findings demonstrate the value of creative, free-form evaluation frameworks for uncovering subtle biases in LLMs that remain hidden in standard prompting setups. By analyzing how models initiate and pursue reasoning goals over multiple turns, we find geographic and cultural disparities embedded in their reasoning processes. We release the dataset (Geo20Q+) and code at <a target="_blank" rel="noopener" href="https://sites.google.com/view/llmbias20q/home">https://sites.google.com/view/llmbias20q/home</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»è¿›è¡Œäº†å¹¿æ³›çš„è°ƒæ•´ï¼Œä»¥å‡è½»æ˜ç¡®çš„åè§ï¼Œç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸è¡¨ç°å‡ºæ ¹æ¤äºé¢„è®­ç»ƒæ•°æ®ä¸­çš„å¾®å¦™éšå«åè§ã€‚æˆ‘ä»¬æ²¡æœ‰é‡‡ç”¨å¯èƒ½è§¦å‘ä¿æŠ¤æœºåˆ¶çš„ç”±äººç±»è®¾è®¡çš„é—®é¢˜æ¥ç›´æ¥æ£€æµ‹LLMï¼Œè€Œæ˜¯æå‡ºäº†ç ”ç©¶æ¨¡å‹åœ¨ä¸»åŠ¨æå‡ºé—®é¢˜æ—¶çš„è¡Œä¸ºå˜åŒ–ã€‚20é—®æ¸¸æˆæ˜¯ä¸€ä¸ªå¤šå›åˆæ¨ç†ä»»åŠ¡ï¼Œä½œä¸ºå®ç°è¿™ä¸€ç›®æ ‡çš„ç†æƒ³æµ‹è¯•å¹³å°ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†ä½¿ç”¨æ–°æ•°æ®é›†Geo20Q+åœ¨å®ä½“æ¨ç†ä¸­çš„åœ°ç†æ€§èƒ½å·®å¼‚ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªä¸åŒåœ°åŒºçš„çŸ¥åäººç‰©å’Œå…·æœ‰æ–‡åŒ–æ„ä¹‰çš„äº‹ç‰©ï¼ˆä¾‹å¦‚é£Ÿç‰©ã€åœ°æ ‡ã€åŠ¨ç‰©ï¼‰ã€‚æˆ‘ä»¬åœ¨ä¸¤ç§æ¸¸æˆè®¾ç½®ï¼ˆæ ‡å‡†çš„20ä¸ªé—®é¢˜ã€æ— é™å›åˆï¼‰å’Œä¸ƒç§è¯­è¨€ï¼ˆè‹±è¯­ã€å°åœ°è¯­ã€æ™®é€šè¯ã€æ—¥è¯­ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­å’ŒåœŸè€³å…¶è¯­ï¼‰ä¸­æµ‹è¯•äº†æµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜å­˜åœ¨åœ°ç†å·®å¼‚ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨æµ‹å…¨çƒåŒ—æ–¹å®ä½“æ–¹é¢æ¯”å…¨çƒå—æ–¹æ›´ä¸ºæˆåŠŸï¼Œå…¨çƒè¥¿æ–¹ä¹Ÿæ¯”å…¨çƒä¸œæ–¹æ›´ä¸ºæˆåŠŸã€‚è™½ç„¶ç»´åŸºç™¾ç§‘é¡µé¢æµè§ˆé‡å’Œé¢„è®­ç»ƒè¯­æ–™åº“é¢‘ç‡ä¸æ€§èƒ½æœ‰è½»å¾®å…³è”ï¼Œä½†å®ƒä»¬æœªèƒ½å®Œå…¨è§£é‡Šè¿™äº›å·®å¼‚ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ¸¸æˆæ‰€ç”¨çš„è¯­è¨€å¯¹æ€§èƒ½å·®è·çš„å½±å“å¾®ä¹å…¶å¾®ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåˆ›é€ æ€§ã€è‡ªç”±å½¢å¼çš„è¯„ä¼°æ¡†æ¶åœ¨æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­éšè—ç»†å¾®åè§æ–¹é¢å…·æœ‰é‡è¦ä»·å€¼ã€‚é€šè¿‡åˆ†ææ¨¡å‹å¦‚ä½•åœ¨å¤šå›åˆä¸­å¯åŠ¨å’Œè¿½æ±‚æ¨ç†ç›®æ ‡ï¼Œæˆ‘ä»¬å‘ç°å…¶æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨çš„åœ°ç†å’Œæ–‡åŒ–å·®å¼‚ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://sites.google.com/view/llmbias20q/home%E5%8F%91%E5%B8%83%E4%BA%86%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%88Geo20Q+%EF%BC%89%E5%92%8C%E4%BB%A3%E7%A0%81%E3%80%82">https://sites.google.com/view/llmbias20q/homeå‘å¸ƒäº†æ•°æ®é›†ï¼ˆGeo20Q+ï¼‰å’Œä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05525v1">PDF</a> Conference on Language Modeling 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°½ç®¡ç»è¿‡è°ƒæ•´ä»¥å‡è½»æ˜¾æ€§åè§ï¼Œä½†å®ƒä»¬é€šå¸¸è¡¨ç°å‡ºæ ¹æ¤äºé¢„è®­ç»ƒæ•°æ®ä¸­çš„å¾®å¦™éšæ€§åè§ã€‚ç ”ç©¶é€šè¿‡æ¨¡å‹ä¸»åŠ¨æé—®çš„æ–¹å¼æ¥ç ”ç©¶æ¨¡å‹çš„åè§ã€‚åœ¨åä¸ºGeo20Q+çš„æ–°æ•°æ®é›†ä¸Šï¼Œå¯¹æ¨¡å‹åœ¨åœ°ç†å®ä½“æ¨ç†æ–¹é¢çš„è¡¨ç°è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚æµ‹è¯•äº†æµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸¤ç§æ¸¸æˆç©æ³•é…ç½®ï¼ˆç»å…¸çš„20é—®å’Œæ— é™å›åˆåˆ¶ï¼‰ä¸‹çš„è¡¨ç°ï¼Œå¹¶åœ¨ä¸ƒç§è¯­è¨€ä¸­è¿›è¡Œã€‚å‘ç°åœ°ç†å·®å¼‚ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å…¨çƒåŒ—æ–¹å’Œè¥¿æ–¹çš„å®ä½“æ¨ç†è¡¨ç°æ›´å¥½ï¼Œè€Œç»´åŸºç™¾ç§‘é¡µé¢æµè§ˆé‡å’Œé¢„è®­ç»ƒè¯­æ–™åº“é¢‘ç‡å¯¹è¡¨ç°çš„å½±å“è¾ƒå°ï¼Œä¸èƒ½å®Œå…¨è§£é‡Šè¿™äº›å·®å¼‚ã€‚è¯­è¨€å¯¹è¡¨ç°å·®è·çš„å½±å“å¾®ä¹å…¶å¾®ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåˆ›é€ æ€§çš„è‡ªç”±å½¢å¼è¯„ä¼°æ¡†æ¶å¯¹äºå‘ç°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¾®å¦™åè§å…·æœ‰ä»·å€¼ï¼Œè¿™äº›åè§åœ¨æ ‡å‡†æç¤ºè®¾ç½®ä¸­æ˜¯éšè—çš„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å³ä½¿ç»è¿‡è°ƒæ•´ä»¥å‡è½»åè§ï¼Œä»å­˜åœ¨åŸºäºé¢„è®­ç»ƒæ•°æ®çš„å¾®å¦™éšæ€§åè§ã€‚</li>
<li>é€šè¿‡æ¨¡å‹ä¸»åŠ¨æé—®çš„æ–¹å¼ç ”ç©¶æ¨¡å‹çš„åè§æ˜¯ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>åœ¨åä¸ºGeo20Q+çš„æ–°æ•°æ®é›†ä¸Šï¼Œå‘ç°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åœ°ç†å®ä½“æ¨ç†æ–¹é¢å­˜åœ¨åœ°ç†å·®å¼‚ã€‚</li>
<li>æµ‹è¯•äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸¤ç§æ¸¸æˆç©æ³•é…ç½®ä¸‹çš„è¡¨ç°ï¼Œå‘ç°å…¶åœ¨å…¨çƒåŒ—æ–¹å’Œè¥¿æ–¹çš„è¡¨ç°æ›´å¥½ã€‚</li>
<li>ç»´åŸºç™¾ç§‘é¡µé¢æµè§ˆé‡å’Œé¢„è®­ç»ƒè¯­æ–™åº“é¢‘ç‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°çš„å½±å“è¾ƒå°ï¼Œä¸èƒ½å®Œå…¨è§£é‡Šè¿™äº›å·®å¼‚ã€‚</li>
<li>è¯­è¨€å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°å·®è·çš„å½±å“å¾®ä¹å…¶å¾®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05525">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b7d7a3391cccaf19c6994d79d33307e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f31249bae350730fe3f9e0892c954ae.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Advancing-Hate-Speech-Detection-with-Transformers-Insights-from-the-MetaHate"><a href="#Advancing-Hate-Speech-Detection-with-Transformers-Insights-from-the-MetaHate" class="headerlink" title="Advancing Hate Speech Detection with Transformers: Insights from the   MetaHate"></a>Advancing Hate Speech Detection with Transformers: Insights from the   MetaHate</h2><p><strong>Authors:Santosh Chapagain, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi</strong></p>
<p>Hate speech is a widespread and harmful form of online discourse, encompassing slurs and defamatory posts that can have serious social, psychological, and sometimes physical impacts on targeted individuals and communities. As social media platforms such as X (formerly Twitter), Facebook, Instagram, Reddit, and others continue to facilitate widespread communication, they also become breeding grounds for hate speech, which has increasingly been linked to real-world hate crimes. Addressing this issue requires the development of robust automated methods to detect hate speech in diverse social media environments. Deep learning approaches, such as vanilla recurrent neural networks (RNNs), long short-term memory (LSTM), and convolutional neural networks (CNNs), have achieved good results, but are often limited by issues such as long-term dependencies and inefficient parallelization. This study represents the comprehensive exploration of transformer-based models for hate speech detection using the MetaHate datasetâ€“a meta-collection of 36 datasets with 1.2 million social media samples. We evaluate multiple state-of-the-art transformer models, including BERT, RoBERTa, GPT-2, and ELECTRA, with fine-tuned ELECTRA achieving the highest performance (F1 score: 0.8980). We also analyze classification errors, revealing challenges with sarcasm, coded language, and label noise. </p>
<blockquote>
<p>ä»‡æ¨è¨€è®ºæ˜¯ä¸€ç§å¹¿æ³›ä¸”æœ‰å®³çš„åœ¨çº¿è¯è¯­å½¢å¼ï¼ŒåŒ…æ‹¬ä¾®è¾±æ€§å’Œè¯½è°¤æ€§çš„å¸–å­ï¼Œå¯èƒ½å¯¹ç›®æ ‡ä¸ªäººå’Œç¤¾åŒºäº§ç”Ÿä¸¥é‡çš„ç¤¾ä¼šã€å¿ƒç†å’Œæœ‰æ—¶ç”šè‡³æ˜¯èº«ä½“å½±å“ã€‚éšç€è¯¸å¦‚Xï¼ˆåŸTwitterï¼‰ã€Facebookã€Instagramã€Redditç­‰ç¤¾äº¤åª’ä½“å¹³å°ç»§ç»­ä¿ƒè¿›å¹¿æ³›ä¼ æ’­æ²Ÿé€šçš„åŒæ—¶ï¼Œå®ƒä»¬ä¹Ÿæˆä¸ºäº†ä»‡æ¨è¨€è®ºçš„æ»‹ç”Ÿåœ°ï¼Œä»‡æ¨è¨€è®ºä¸çœŸå®ä¸–ç•Œçš„ä»‡æ¨çŠ¯ç½ªä¹‹é—´çš„è”ç³»ä¹Ÿæ„ˆå‘ç´§å¯†ã€‚è§£å†³è¿™ä¸€é—®é¢˜éœ€è¦å¼€å‘ç¨³å¥çš„è‡ªåŠ¨åŒ–æ–¹æ³•ï¼Œä»¥åœ¨å¤šæ ·åŒ–çš„ç¤¾äº¤åª’ä½“ç¯å¢ƒä¸­æ£€æµ‹ä»‡æ¨è¨€è®ºã€‚æ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œå¦‚æ™®é€šçš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNsï¼‰ã€é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å·²ç»å–å¾—äº†è‰¯å¥½çš„æˆæœï¼Œä½†å®ƒä»¬å¸¸å¸¸å—é™äºé•¿æœŸä¾èµ–å…³ç³»å’Œä¸é«˜æ•ˆçš„å¹¶è¡ŒåŒ–ç­‰é—®é¢˜ã€‚æœ¬ç ”ç©¶ä½¿ç”¨MetaHateæ•°æ®é›†å…¨é¢æ¢ç´¢åŸºäºTransformeræ¨¡å‹çš„ä»‡æ¨è¨€è®ºæ£€æµ‹â€”â€”ä¸€ä¸ªåŒ…å«36ä¸ªæ•°æ®é›†å’Œ120ä¸‡ä¸ªç¤¾äº¤åª’ä½“æ ·æœ¬çš„å…ƒé›†åˆã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šä¸ªå…ˆè¿›çš„Transformeræ¨¡å‹ï¼ŒåŒ…æ‹¬BERTã€RoBERTaã€GPT-2å’ŒELECTRAç­‰æ¨¡å‹ï¼Œç»è¿‡ç²¾ç»†è°ƒæ•´çš„ELECTRAæ¨¡å‹å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼ˆF1åˆ†æ•°ä¸º0.8980ï¼‰ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†åˆ†ç±»é”™è¯¯ï¼Œæ­ç¤ºå‡ºè®¥è®½æ€§è¨€è¯­ã€éšå«çš„è¯­è¨€å’Œæ ‡ç­¾å™ªå£°å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.04913v1">PDF</a> Accepted to the Deviant Dynamics in Digital Spaces workshop at ASONAM   2025</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡æœ¬ä¸»è¦è®¨è®ºäº†ç½‘ç»œä¸Šçš„ä»‡æ¨è¨€è®ºé—®é¢˜ï¼ŒåŒ…æ‹¬å®ƒå¯¹ä¸ªäººå’Œç¤¾ä¼šçš„å½±å“ã€‚æ–‡ä¸­æåˆ°ç¤¾äº¤åª’ä½“å¹³å°æˆä¸ºäº†ä»‡æ¨è¨€è®ºçš„æ»‹ç”Ÿåœ°ï¼Œè¿™ä¸€é—®é¢˜å·²ä¸ç°å®ä¸–ç•Œçš„ä»‡æ¨çŠ¯ç½ªç´§å¯†ç›¸è¿ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…ä»¬æ­£åœ¨å¼€å‘åœ¨å¤šæ ·ç¤¾äº¤åª’ä½“ç¯å¢ƒä¸­æ£€æµ‹ä»‡æ¨è¨€è®ºçš„è‡ªåŠ¨åŒ–æ–¹æ³•ã€‚å½“å‰æ·±åº¦å­¦ä¹ å’ŒåŸºäºTransformerçš„æ¨¡å‹éƒ½åœ¨è¿™ä¸€é¢†åŸŸå–å¾—äº†ä¸€äº›è¿›å±•ã€‚æœ¬æ–‡é‡ç‚¹ç ”ç©¶äº†åŸºäºTransformerçš„æ¨¡å‹åœ¨MetaHateæ•°æ®é›†ä¸Šçš„ä»‡æ¨è¨€è®ºæ£€æµ‹æ•ˆæœï¼Œå…¶ä¸­fine-tuned ELECTRAæ¨¡å‹è¡¨ç°æœ€ä½³ï¼ŒF1åˆ†æ•°è¾¾åˆ°0.8980ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜åˆ†æäº†åˆ†ç±»é”™è¯¯çš„åŸå› ï¼ŒåŒ…æ‹¬è®½åˆºã€éšå«è¯­è¨€å’Œæ ‡ç­¾å™ªå£°ç­‰æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‡æ¨è¨€è®ºæ˜¯ä¸€ç§å¹¿æ³›ä¸”æœ‰å®³çš„ç½‘ç»œè¡¨è¾¾å½¢å¼ï¼Œå¯¹ç›®æ ‡ä¸ªä½“å’Œç¤¾åŒºäº§ç”Ÿä¸¥é‡çš„ç¤¾ä¼šã€å¿ƒç†å’Œç‰©ç†å½±å“ã€‚</li>
<li>ç¤¾äº¤åª’ä½“å¹³å°æˆä¸ºä»‡æ¨è¨€è®ºçš„æ»‹ç”Ÿåœ°ï¼Œä¸ç°å®ä¸–ç•Œçš„ä»‡æ¨çŠ¯ç½ªæœ‰è”ç³»ã€‚</li>
<li>è‡ªåŠ¨åŒ–æ–¹æ³•åœ¨å¤„ç†å¤šæ ·ç¤¾äº¤åª’ä½“ç¯å¢ƒä¸­çš„ä»‡æ¨è¨€è®ºæ£€æµ‹æ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚</li>
<li>åŸºäºæ·±åº¦å­¦ä¹ çš„æ¨¡å‹ï¼Œå¦‚RNNã€LSTMå’ŒCNNç­‰ï¼Œåœ¨ä»‡æ¨è¨€è®ºæ£€æµ‹æ–¹é¢å·²æœ‰è‰¯å¥½è¡¨ç°ï¼Œä½†ä»å­˜åœ¨é•¿æœŸä¾èµ–å’Œå¹¶è¡ŒåŒ–æ•ˆç‡ä½ä¸‹ç­‰é—®é¢˜ã€‚</li>
<li>åŸºäºTransformerçš„æ¨¡å‹åœ¨MetaHateæ•°æ®é›†ä¸Šè¿›è¡Œä»‡æ¨è¨€è®ºæ£€æµ‹è¡¨ç°å‡ºæ½œåŠ›ã€‚</li>
<li>fine-tuned ELECTRAæ¨¡å‹åœ¨ä»‡æ¨è¨€è®ºæ£€æµ‹æ–¹é¢å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼ŒF1åˆ†æ•°è¾¾åˆ°0.8980ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.04913">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-583af5db18abfe596e34c71933a54e7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fde913f94330f6d8e7148852c5935f6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e7075273c205cf1b766672caca61ce50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6594dbc1d95a256a0086f3c5dfd41ca.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DreamVVT-Mastering-Realistic-Video-Virtual-Try-On-in-the-Wild-via-a-Stage-Wise-Diffusion-Transformer-Framework"><a href="#DreamVVT-Mastering-Realistic-Video-Virtual-Try-On-in-the-Wild-via-a-Stage-Wise-Diffusion-Transformer-Framework" class="headerlink" title="DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a   Stage-Wise Diffusion Transformer Framework"></a>DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a   Stage-Wise Diffusion Transformer Framework</h2><p><strong>Authors:Tongchun Zuo, Zaiyu Huang, Shuliang Ning, Ente Lin, Chao Liang, Zerong Zheng, Jianwen Jiang, Yuan Zhang, Mingyuan Gao, Xin Dong</strong></p>
<p>Video virtual try-on (VVT) technology has garnered considerable academic interest owing to its promising applications in e-commerce advertising and entertainment. However, most existing end-to-end methods rely heavily on scarce paired garment-centric datasets and fail to effectively leverage priors of advanced visual models and test-time inputs, making it challenging to accurately preserve fine-grained garment details and maintain temporal consistency in unconstrained scenarios. To address these challenges, we propose DreamVVT, a carefully designed two-stage framework built upon Diffusion Transformers (DiTs), which is inherently capable of leveraging diverse unpaired human-centric data to enhance adaptability in real-world scenarios. To further leverage prior knowledge from pretrained models and test-time inputs, in the first stage, we sample representative frames from the input video and utilize a multi-frame try-on model integrated with a vision-language model (VLM), to synthesize high-fidelity and semantically consistent keyframe try-on images. These images serve as complementary appearance guidance for subsequent video generation. \textbf{In the second stage}, skeleton maps together with fine-grained motion and appearance descriptions are extracted from the input content, and these along with the keyframe try-on images are then fed into a pretrained video generation model enhanced with LoRA adapters. This ensures long-term temporal coherence for unseen regions and enables highly plausible dynamic motions. Extensive quantitative and qualitative experiments demonstrate that DreamVVT surpasses existing methods in preserving detailed garment content and temporal stability in real-world scenarios. Our project page <a target="_blank" rel="noopener" href="https://virtu-lab.github.io/">https://virtu-lab.github.io/</a> </p>
<blockquote>
<p>è§†é¢‘è™šæ‹Ÿè¯•ç©¿ï¼ˆVVTï¼‰æŠ€æœ¯å› å…¶ç”µå­å•†åŠ¡å¹¿å‘Šå’Œå¨±ä¹æ–¹é¢çš„åº”ç”¨å‰æ™¯è€Œå¼•èµ·äº†å­¦æœ¯ç•Œçš„å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„ç«¯åˆ°ç«¯æ–¹æ³•ä¸¥é‡ä¾èµ–äºç¨€ç¼ºçš„é…å¥—æœè£…æ•°æ®é›†ï¼Œæœªèƒ½æœ‰æ•ˆåˆ©ç”¨å…ˆè¿›çš„è§†è§‰æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†å’Œæµ‹è¯•æ—¶çš„è¾“å…¥ï¼Œè¿™ä½¿å¾—åœ¨ä¸å—çº¦æŸçš„åœºæ™¯ä¸­å‡†ç¡®ä¿ç•™æœè£…çš„ç²¾ç»†ç»†èŠ‚å¹¶ä¿æŒæ—¶é—´ä¸€è‡´æ€§å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DreamVVTï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œå»ºç«‹åœ¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTsï¼‰ä¹‹ä¸Šï¼Œå®ƒæœ¬è´¨ä¸Šèƒ½å¤Ÿåˆ©ç”¨å¤šæ ·ä¸”æ— é…å¥—çš„äººå½¢æ•°æ®ï¼Œæé«˜åœ¨ç°å®åœºæ™¯ä¸­çš„é€‚åº”æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†å’Œæµ‹è¯•æ—¶çš„è¾“å…¥ï¼Œåœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬ä»è¾“å…¥è§†é¢‘ä¸­æŠ½æ ·ä»£è¡¨æ€§å¸§ï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ªé›†æˆäº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¤šå¸§è¯•ç©¿æ¨¡å‹ï¼Œåˆæˆé«˜ä¿çœŸå’Œè¯­ä¹‰ä¸€è‡´çš„å…³é”®å¸§è¯•ç©¿å›¾åƒã€‚è¿™äº›å›¾åƒä½œä¸ºåç»­è§†é¢‘ç”Ÿæˆçš„è¡¥å……å¤–è§‚æŒ‡å¯¼ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œä»è¾“å…¥å†…å®¹ä¸­æå–éª¨æ¶å›¾ä»¥åŠç²¾ç»†çš„è¿åŠ¨å’Œå¤–è§‚æè¿°ï¼Œè¿™äº›ä¸å…³é”®å¸§è¯•ç©¿å›¾åƒä¸€èµ·è¾“å…¥åˆ°å¢å¼ºæœ‰LoRAé€‚é…å™¨çš„é¢„è®­ç»ƒè§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­ã€‚è¿™ç¡®ä¿äº†æœªè§åŒºåŸŸçš„é•¿æœŸæ—¶é—´è¿è´¯æ€§ï¼Œå¹¶èƒ½å¤Ÿå®ç°é«˜åº¦é€¼çœŸçš„åŠ¨æ€è¿åŠ¨ã€‚å¤§é‡çš„å®šé‡å’Œå®šæ€§å®éªŒè¡¨æ˜ï¼ŒDreamVVTåœ¨ä¿ç•™æœè£…å†…å®¹çš„è¯¦ç»†ä¿¡æ¯å’Œç°å®åœºæ™¯ä¸­çš„æ—¶é—´ç¨³å®šæ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ä¸ºï¼š[<a target="_blank" rel="noopener" href="https://virtu-lab.github.io/]">https://virtu-lab.github.io/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02807v1">PDF</a> 18 pages, 12 figures</p>
<p><strong>Summary</strong><br>è§†é¢‘è™šæ‹Ÿè¯•ç©¿ï¼ˆVVTï¼‰æŠ€æœ¯åœ¨ç”µå­å•†åŠ¡å¹¿å‘Šå’Œå¨±ä¹ç­‰é¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰ç«¯åˆ°ç«¯æ–¹æ³•å¤§å¤šä¾èµ–ç¨€ç¼ºçš„é…å¯¹æœè£…æ•°æ®é›†ï¼Œéš¾ä»¥æœ‰æ•ˆåˆ©ç”¨å…ˆè¿›è§†è§‰æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†å’Œæµ‹è¯•æ—¶è¾“å…¥çš„ä¿¡æ¯ï¼Œéš¾ä»¥åœ¨ä¸å—çº¦æŸçš„åœºæ™¯ä¸­å‡†ç¡®ä¿ç•™ç»†è‡´çš„æœè£…ç»†èŠ‚å¹¶ä¿æŒæ—¶é—´ä¸€è‡´æ€§ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºDreamVVTï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰çš„ç²¾å¿ƒè®¾è®¡çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œèƒ½å¤Ÿåˆ©ç”¨å¤šæ ·çš„äººç±»ä¸­å¿ƒåŒ–æ•°æ®å¢å¼ºåœ¨çœŸå®åœºæ™¯ä¸­çš„é€‚åº”æ€§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»è¾“å…¥è§†é¢‘ä¸­é‡‡æ ·å…³é”®å¸§ï¼Œå¹¶ä½¿ç”¨å¤šå¸§è¯•ç©¿æ¨¡å‹ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åˆæˆé«˜è´¨é‡ã€è¯­ä¹‰ä¸€è‡´çš„å…³é”®å¸§è¯•ç©¿å›¾åƒã€‚æ¥ç€åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬ä»è¾“å…¥å†…å®¹ä¸­æå–éª¨æ¶å›¾ä»¥åŠç»†è‡´çš„åŠ¨æ€å’Œå¤–è§‚æè¿°ä¿¡æ¯ï¼Œå¹¶ç»“åˆå…³é”®å¸§è¯•ç©¿å›¾åƒè¾“å…¥åˆ°å¢å¼ºå‹è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œç¡®ä¿é•¿æœŸæ—¶é—´è¿è´¯æ€§å¹¶äº§ç”Ÿé«˜åº¦é€¼çœŸçš„åŠ¨æ€æ•ˆæœã€‚å®éªŒè¯æ˜ï¼ŒDreamVVTåœ¨ä¿ç•™æœè£…ç»†èŠ‚å’Œæ—¶é—´ç¨³å®šæ€§æ–¹é¢è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘è™šæ‹Ÿè¯•ç©¿ï¼ˆVVTï¼‰æŠ€æœ¯åœ¨ç”µå­å•†åŠ¡å¹¿å‘Šå’Œå¨±ä¹é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–é…å¯¹æœè£…æ•°æ®é›†ï¼Œéš¾ä»¥åœ¨çœŸå®åœºæ™¯ä¸­åº”ç”¨ã€‚</li>
<li>DreamVVTåŸºäºæ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰çš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œå¯åˆ©ç”¨å¤šæ ·çš„äººç±»ä¸­å¿ƒåŒ–æ•°æ®ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µåˆæˆé«˜è´¨é‡ã€è¯­ä¹‰ä¸€è‡´çš„å…³é”®å¸§è¯•ç©¿å›¾åƒã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µç»“åˆéª¨æ¶å›¾ã€åŠ¨æ€å’Œå¤–è§‚æè¿°ä¿¡æ¯ï¼Œç¡®ä¿è§†é¢‘é•¿æœŸæ—¶é—´è¿è´¯æ€§ã€‚</li>
<li>DreamVVTåœ¨ä¿ç•™æœè£…ç»†èŠ‚å’Œæ—¶é—´ç¨³å®šæ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02807">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1b76edf22a33b41c2d24b78a524332ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f7d501d3d6347bb2d3baf5948c82997c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-589204f535676714bf5117d7e124d3ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e89423f68398b79f2405a55d9098aa6f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CAMA-Enhancing-Mathematical-Reasoning-in-Large-Language-Models-with-Causal-Knowledge"><a href="#CAMA-Enhancing-Mathematical-Reasoning-in-Large-Language-Models-with-Causal-Knowledge" class="headerlink" title="CAMA: Enhancing Mathematical Reasoning in Large Language Models with   Causal Knowledge"></a>CAMA: Enhancing Mathematical Reasoning in Large Language Models with   Causal Knowledge</h2><p><strong>Authors:Lei Zan, Keli Zhang, Ruichu Cai, Lujia Pan</strong></p>
<p>Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks, yet they still struggle with complex mathematical reasoning, a challenge fundamentally rooted in deep structural dependencies. To address this challenge, we propose \textbf{CA}usal \textbf{MA}thematician (\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit, reusable mathematical structure. In the learning stage, CAMA first constructs the \textbf{M}athematical \textbf{C}ausal \textbf{G}raph (\textbf{MCG}), a high-level representation of solution strategies, by combining LLM priors with causal discovery algorithms applied to a corpus of question-solution pairs. The resulting MCG encodes essential knowledge points and their causal dependencies. To better align the graph with downstream reasoning tasks, CAMA further refines the MCG through iterative feedback derived from a selected subset of the question-solution pairs. In the reasoning stage, given a new question, CAMA dynamically extracts a task-relevant subgraph from the MCG, conditioned on both the question content and the LLMâ€™s intermediate reasoning trace. This subgraph, which encodes the most pertinent knowledge points and their causal dependencies, is then injected back into the LLM to guide its reasoning process. Empirical results on real-world datasets show that CAMA significantly improves LLM performance on challenging mathematical problems. Furthermore, our experiments demonstrate that structured guidance consistently outperforms unstructured alternatives, and that incorporating asymmetric causal relationships yields greater improvements than using symmetric associations alone. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨å¤æ‚çš„æ•°å­¦æ¨ç†æ–¹é¢ä»å­˜åœ¨å›°éš¾ï¼Œè¿™ä¸€æŒ‘æˆ˜æ ¹æœ¬æºäºæ·±å±‚çš„ç»“æ„ä¾èµ–æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å› æœæ•°å­¦å®¶ï¼ˆ\textbf{CAMA}ï¼‰è¿™ä¸€ä¸¤é˜¶æ®µå› æœæ¡†æ¶ï¼Œä¸ºLLMé…å¤‡æ˜ç¡®çš„å¯é‡ç”¨æ•°å­¦ç»“æ„ã€‚åœ¨å­¦ä¹ é˜¶æ®µï¼ŒCAMAé¦–å…ˆæ„å»ºæ•°å­¦å› æœå›¾ï¼ˆMCGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è§£å†³æ–¹æ¡ˆç­–ç•¥çš„é«˜çº§è¡¨ç¤ºï¼Œé€šè¿‡ç»“åˆLLMçš„å…ˆéªŒçŸ¥è¯†å’Œåº”ç”¨äºé—®é¢˜è§£å†³æ–¹æ¡ˆå¯¹è¯­æ–™åº“çš„å› æœå‘ç°ç®—æ³•ã€‚ç»“æœäº§ç”Ÿçš„MCGç¼–ç äº†å¿…è¦çš„çŸ¥è¯†ç‚¹å’Œå®ƒä»¬çš„å› æœä¾èµ–å…³ç³»ã€‚ä¸ºäº†æ›´å¥½åœ°ä¸ä¸‹æ¸¸æ¨ç†ä»»åŠ¡å¯¹é½ï¼ŒCAMAé€šè¿‡æ¥è‡ªé—®é¢˜è§£å†³æ–¹æ¡ˆå¯¹å­é›†çš„è¿­ä»£åé¦ˆè¿›ä¸€æ­¥æ”¹è¿›MCGã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œå¯¹äºæ–°é—®é¢˜ï¼ŒCAMAæ ¹æ®é—®é¢˜çš„å†…å®¹å’ŒLLMçš„ä¸­é—´æ¨ç†è½¨è¿¹ï¼ŒåŠ¨æ€åœ°ä»MCGä¸­æå–ä»»åŠ¡ç›¸å…³å­å›¾ã€‚è¿™ä¸ªå­å›¾ç¼–ç äº†æœ€ç›¸å…³çš„çŸ¥è¯†ç‚¹å’Œå®ƒä»¬çš„å› æœä¾èµ–å…³ç³»ï¼Œç„¶åæ³¨å…¥LLMä»¥æŒ‡å¯¼å…¶æ¨ç†è¿‡ç¨‹ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒCAMAåœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦é—®é¢˜æ–¹é¢æ˜¾è‘—æé«˜äº†LLMçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œç»“æ„åŒ–çš„æŒ‡å¯¼å§‹ç»ˆä¼˜äºéç»“æ„åŒ–çš„æ›¿ä»£æ–¹æ¡ˆï¼Œè€Œèå…¥ä¸å¯¹ç§°çš„å› æœå…³ç³»æ¯”ä»…ä½¿ç”¨å¯¹ç§°å…³è”èƒ½å¸¦æ¥æ›´å¤§çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02583v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨å¤æ‚çš„æ•°å­¦æ¨ç†æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†å› æœæ•°å­¦å®¶ï¼ˆCAMAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå­¦ä¹ é˜¶æ®µå’Œæ¨ç†é˜¶æ®µã€‚å­¦ä¹ é˜¶æ®µæ„å»ºæ•°å­¦å› æœå›¾ï¼ˆMCGï¼‰ï¼Œç»“åˆLLMå…ˆéªŒçŸ¥è¯†ä¸å› æœå‘ç°ç®—æ³•ï¼Œå¯¹é—®é¢˜è§£ç­”å¯¹è¿›è¡Œç¼–ç ï¼Œå½¢æˆé«˜å±‚æ¬¡çš„è§£å†³æ–¹æ¡ˆç­–ç•¥è¡¨ç¤ºã€‚æ¨ç†é˜¶æ®µæ ¹æ®é—®é¢˜å’ŒLLMçš„ä¸­é—´æ¨ç†è½¨è¿¹ï¼Œä»MCGä¸­åŠ¨æ€æå–ç›¸å…³å­å›¾ï¼ŒæŒ‡å¯¼LLMçš„æ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒCAMAèƒ½æ˜¾è‘—æé«˜LLMè§£å†³æ•°å­¦é—®é¢˜çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMé¢ä¸´å¤æ‚æ•°å­¦æ¨ç†çš„æŒ‘æˆ˜ï¼Œéœ€è¦æ–°çš„æ–¹æ³•æ¥è§£å†³ã€‚</li>
<li>CAMAæ¡†æ¶åˆ†ä¸ºå­¦ä¹ é˜¶æ®µå’Œæ¨ç†é˜¶æ®µï¼Œæ—¨åœ¨æé«˜LLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å­¦ä¹ é˜¶æ®µé€šè¿‡æ„å»ºæ•°å­¦å› æœå›¾ï¼ˆMCGï¼‰æ¥ç¼–ç è§£å†³æ–¹æ¡ˆç­–ç•¥ã€‚</li>
<li>æ¨ç†é˜¶æ®µæ ¹æ®é—®é¢˜å’ŒLLMçš„æ¨ç†è½¨è¿¹ä»MCGä¸­æå–ç›¸å…³å­å›¾ã€‚</li>
<li>CAMAé€šè¿‡ç»“åˆLLMå…ˆéªŒçŸ¥è¯†ã€å› æœå‘ç°ç®—æ³•å’Œé—®é¢˜è§£ç­”å¯¹ï¼Œæé«˜æ•°å­¦é—®é¢˜çš„è§£å†³èƒ½åŠ›ã€‚</li>
<li>å®è¯ç»“æœè¡¨æ˜ï¼ŒCAMAèƒ½æ˜¾è‘—æé«˜LLMåœ¨è§£å†³æ•°å­¦é—®é¢˜ä¸Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02583">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58da7f78ab2d364eb3339b882ecd3c4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb36c79e159593473a6f104fb2645231.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef83bcc7ab4751853333c5463f3b0db0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="SE-Agent-Self-Evolution-Trajectory-Optimization-in-Multi-Step-Reasoning-with-LLM-Based-Agents"><a href="#SE-Agent-Self-Evolution-Trajectory-Optimization-in-Multi-Step-Reasoning-with-LLM-Based-Agents" class="headerlink" title="SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning   with LLM-Based Agents"></a>SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning   with LLM-Based Agents</h2><p><strong>Authors:Jiaye Lin, Yifu Guo, Yuzhen Han, Sen Hu, Ziyi Ni, Licheng Wang, Mingguang Chen, Daxin Jiang, Binxing Jiao, Chen Hu, Huacan Wang</strong></p>
<p>Large Language Model (LLM)-based agents have recently shown impressive capabilities in complex reasoning and tool use via multi-step interactions with their environments. While these agents have the potential to tackle complicated tasks, their problem-solving process, i.e., agentsâ€™ interaction trajectory leading to task completion, remains underexploited. These trajectories contain rich feedback that can navigate agents toward the right directions for solving problems correctly. Although prevailing approaches, such as Monte Carlo Tree Search (MCTS), can effectively balance exploration and exploitation, they ignore the interdependence among various trajectories and lack the diversity of search spaces, which leads to redundant reasoning and suboptimal outcomes. To address these challenges, we propose SE-Agent, a Self-Evolution framework that enables Agents to optimize their reasoning processes iteratively. Our approach revisits and enhances former pilot trajectories through three key operations: revision, recombination, and refinement. This evolutionary mechanism enables two critical advantages: (1) it expands the search space beyond local optima by intelligently exploring diverse solution paths guided by previous trajectories, and (2) it leverages cross-trajectory inspiration to efficiently enhance performance while mitigating the impact of suboptimal reasoning paths. Through these mechanisms, SE-Agent achieves continuous self-evolution that incrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench Verified to resolve real-world GitHub issues. Experimental results across five strong LLMs show that integrating SE-Agent delivers up to 55% relative improvement, achieving state-of-the-art performance among all open-source agents on SWE-bench Verified. Our code and demonstration materials are publicly available at <a target="_blank" rel="noopener" href="https://github.com/JARVIS-Xs/SE-Agent">https://github.com/JARVIS-Xs/SE-Agent</a>. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†æœ€è¿‘æ˜¾ç¤ºå‡ºé€šè¿‡ä¸å…¶ç¯å¢ƒè¿›è¡Œå¤šæ­¥éª¤äº¤äº’è¿›è¡Œå¤æ‚æ¨ç†å’Œå·¥å…·ä½¿ç”¨çš„ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚è™½ç„¶è¿™äº›ä»£ç†æœ‰æ½œåŠ›å¤„ç†å¤æ‚ä»»åŠ¡ï¼Œä½†ä»–ä»¬çš„è§£å†³é—®é¢˜è¿‡ç¨‹ï¼Œå³ä»£ç†å®Œæˆä»»åŠ¡çš„äº¤äº’è½¨è¿¹ï¼Œä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚è¿™äº›è½¨è¿¹åŒ…å«ä¸°å¯Œçš„åé¦ˆï¼Œå¯ä»¥å¼•å¯¼ä»£ç†æ­£ç¡®è§£å†³é—®é¢˜ã€‚å°½ç®¡ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œä½†å®ƒä»¬å¿½ç•¥äº†å„ç§è½¨è¿¹ä¹‹é—´çš„ç›¸äº’ä¾èµ–æ€§ï¼Œå¹¶ä¸”ç¼ºä¹æœç´¢ç©ºé—´çš„å¤šæ ·æ€§ï¼Œè¿™å¯¼è‡´å†—ä½™æ¨ç†å’Œæ¬¡ä¼˜ç»“æœã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SE-Agentï¼Œä¸€ä¸ªè‡ªæˆ‘è¿›åŒ–æ¡†æ¶ï¼Œä½¿ä»£ç†èƒ½å¤Ÿè¿­ä»£ä¼˜åŒ–ä»–ä»¬çš„æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸‰ä¸ªå…³é”®æ“ä½œï¼šä¿®è®¢ã€é‡ç»„å’Œç»†åŒ–ï¼Œæ¥é‡æ–°å®¡è§†å’Œæ”¹è¿›ä¹‹å‰çš„è½¨è¿¹ã€‚è¿™ç§è¿›åŒ–æœºåˆ¶å¸¦æ¥äº†ä¸¤ä¸ªå…³é”®ä¼˜åŠ¿ï¼šï¼ˆ1ï¼‰å®ƒé€šè¿‡æ™ºèƒ½åœ°æ¢ç´¢ä»¥å‰è½¨è¿¹å¼•å¯¼çš„å¤šç§è§£å†³æ–¹æ¡ˆè·¯å¾„ï¼Œæ‰©å¤§äº†æœç´¢ç©ºé—´ï¼Œè¶…è¶Šäº†å±€éƒ¨æœ€ä¼˜ï¼›ï¼ˆ2ï¼‰å®ƒåˆ©ç”¨è·¨è½¨è¿¹çš„çµæ„Ÿæ¥æœ‰æ•ˆåœ°æé«˜æ€§èƒ½ï¼ŒåŒæ—¶å‡è½»æ¬¡ä¼˜æ¨ç†è·¯å¾„çš„å½±å“ã€‚é€šè¿‡è¿™äº›æœºåˆ¶ï¼ŒSE-Agentå®ç°äº†æŒç»­çš„è‡ªæˆ‘è¿›åŒ–ï¼Œé€æ­¥æé«˜äº†æ¨ç†è´¨é‡ã€‚æˆ‘ä»¬åœ¨SWE-bench Verifiedä¸Šè¯„ä¼°äº†SE-Agentï¼Œä»¥è§£å†³ç°å®ä¸–ç•Œä¸­çš„GitHubé—®é¢˜ã€‚åœ¨äº”ä¸ªå¼ºå¤§çš„LLMä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œé›†æˆSE-Agentå¸¦æ¥äº†é«˜è¾¾55%çš„ç›¸å¯¹æ”¹è¿›ï¼Œåœ¨SWE-bench Verifiedä¸Šçš„æ€§èƒ½ä¼˜äºæ‰€æœ‰å¼€æºä»£ç†ï¼Œè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¼”ç¤ºææ–™å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JARVIS-Xs/SE-Agent%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/JARVIS-Xs/SE-Agentå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02085v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†å’Œå·¥å…·ä½¿ç”¨æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œé€šè¿‡ä¸ç¯å¢ƒçš„å¤šæ­¥äº¤äº’å®Œæˆä»»åŠ¡ã€‚ç„¶è€Œï¼Œå…¶è§£é¢˜è¿‡ç¨‹ä¸­çš„äº¤äº’è½¨è¿¹å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚è¿™äº›è½¨è¿¹åŒ…å«ä¸°å¯Œçš„åé¦ˆï¼Œå¯ä»¥å¼•å¯¼æ¨¡å‹æ­£ç¡®è§£å†³é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•å¦‚è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰è™½ç„¶èƒ½å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼Œä½†å¿½ç•¥äº†è½¨è¿¹é—´çš„ç›¸äº’ä¾èµ–æ€§å’Œæœç´¢ç©ºé—´çš„å¤šæ ·æ€§ï¼Œå¯¼è‡´å†—ä½™æ¨ç†å’Œæ¬¡ä¼˜ç»“æœã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SE-Agentæ¡†æ¶ï¼Œé€šè¿‡ä¿®è®¢ã€é‡ç»„å’Œä¼˜åŒ–ä¹‹å‰çš„è½¨è¿¹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè¿­ä»£ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSE-Agentåœ¨çœŸå®ä¸–ç•ŒGitHubé—®é¢˜ä¸Šçš„æ€§èƒ½ç›¸å¯¹äºå…¶ä»–å¼€æºæ¨¡å‹æœ‰æ˜æ˜¾æå‡ã€‚ä»£ç å’Œæ¼”ç¤ºææ–™å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMæ¨¡å‹åœ¨å¤æ‚æ¨ç†å’Œå·¥å…·ä½¿ç”¨æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>LLMæ¨¡å‹çš„è§£é¢˜è¿‡ç¨‹ä¸­çš„äº¤äº’è½¨è¿¹å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ï¼Œå…¶ä¸­åŒ…å«ä¸°å¯Œçš„åé¦ˆã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚è’™ç‰¹å¡æ´›æ ‘æœç´¢å¿½ç•¥äº†è½¨è¿¹é—´çš„ç›¸äº’ä¾èµ–æ€§å’Œæœç´¢ç©ºé—´çš„å¤šæ ·æ€§ã€‚</li>
<li>SE-Agentæ¡†æ¶é€šè¿‡ä¿®è®¢ã€é‡ç»„å’Œä¼˜åŒ–ä¹‹å‰çš„è½¨è¿¹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè¿­ä»£ä¼˜åŒ–æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>SE-Agentå®ç°äº†å¯¹å±€éƒ¨æœ€ä¼˜è§£çš„è¶…è¶Šï¼Œé€šè¿‡æ™ºèƒ½æ¢ç´¢å¤šç§è§£å†³æ–¹æ¡ˆè·¯å¾„æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>SE-Agentåˆ©ç”¨è·¨è½¨è¿¹çµæ„Ÿæ¥å¢å¼ºæ€§èƒ½å¹¶å‡è½»æ¬¡ä¼˜æ¨ç†è·¯å¾„çš„å½±å“ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-54b48f09ac4403bd42b74224c37da297.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc136467beb09e19fc1f86d24cbca8bd.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="The-SMeL-Test-A-simple-benchmark-for-media-literacy-in-language-models"><a href="#The-SMeL-Test-A-simple-benchmark-for-media-literacy-in-language-models" class="headerlink" title="The SMeL Test: A simple benchmark for media literacy in language models"></a>The SMeL Test: A simple benchmark for media literacy in language models</h2><p><strong>Authors:Gustaf Ahdritz, Anat Kleiman</strong></p>
<p>The internet is rife with unattributed, deliberately misleading, or otherwise untrustworthy content. Though large language models (LLMs) are often tasked with autonomous web browsing, the extent to which they have learned the simple heuristics human researchers use to navigate this noisy environment is not currently known. In this paper, we introduce the Synthetic Media Literacy Test (SMeL Test), a minimal benchmark that tests the ability of language models to actively filter out untrustworthy information in context. We benchmark a variety of commonly used instruction-tuned LLMs, including reasoning models, and find that no model consistently succeeds; while reasoning in particular is associated with higher scores, even the best API model we test hallucinates up to 70% of the time. Remarkably, larger and more capable models do not necessarily outperform their smaller counterparts. We hope our work sheds more light on this important form of hallucination and guides the development of new methods to combat it. </p>
<blockquote>
<p>äº’è”ç½‘å……æ–¥ç€æ— å±æ€§ã€æ•…æ„è¯¯å¯¼æˆ–å…¶ä»–ä¸å¯ä¿¡èµ–çš„å†…å®¹ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»å¸¸è¢«èµ‹äºˆè‡ªä¸»ç½‘é¡µæµè§ˆçš„ä»»åŠ¡ï¼Œä½†å®ƒä»¬åœ¨è¿™ä¸€å˜ˆæ‚ç¯å¢ƒä¸­å­¦ä¹ äººç±»ç ”ç©¶è€…ä½¿ç”¨çš„ç®€å•å¯å‘å¼æ–¹æ³•çš„ç¨‹åº¦ç›®å‰å°šä¸æ¸…æ¥šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†åˆæˆåª’ä½“ç´ å…»æµ‹è¯•ï¼ˆSMeLæµ‹è¯•ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ€å°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æµ‹è¯•è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šæƒ…å¢ƒä¸­ä¸»åŠ¨è¿‡æ»¤æ‰ä¸å¯é ä¿¡æ¯çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹å„ç§å¸¸ç”¨çš„æŒ‡ä»¤è°ƒæ•´å‹LLMè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬æ¨ç†æ¨¡å‹ï¼Œå‘ç°æ²¡æœ‰ä»»ä½•æ¨¡å‹èƒ½å¤Ÿå§‹ç»ˆæˆåŠŸï¼›è™½ç„¶æ¨ç†ä¸æ›´é«˜çš„åˆ†æ•°ç‰¹åˆ«ç›¸å…³ï¼Œä½†å³ä½¿æ˜¯æˆ‘ä»¬æµ‹è¯•è¿‡çš„æœ€å¥½çš„APIæ¨¡å‹ï¼Œä¹Ÿæœ‰é«˜è¾¾70%çš„å¹»è§‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ›´å¤§ã€æ›´å¼ºå¤§çš„æ¨¡å‹å¹¶ä¸ä¸€å®šæ¯”å°å‹æ¨¡å‹è¡¨ç°æ›´å¥½ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œèƒ½è¿›ä¸€æ­¥æ­ç¤ºè¿™ç§é‡è¦çš„å¹»è§‰ç°è±¡ï¼Œå¹¶ä¸ºå¼€å‘æ–°çš„å¯¹æŠ—æ–¹æ³•æä¾›æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02074v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šäº’è”ç½‘å……æ–¥ç€å¤§é‡æœªç»æˆæƒã€æ•…æ„è¯¯å¯¼æˆ–å…¶ä»–ä¸å¯é çš„å†…å®¹ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¸è¢«ç”¨äºè‡ªåŠ¨æµè§ˆç½‘é¡µï¼Œä½†å®ƒä»¬æ˜¯å¦æŒæ¡äº†äººç±»ç ”ç©¶è€…ç”¨äºå¯¼èˆªè¿™ç§å˜ˆæ‚ç¯å¢ƒçš„ç®€å•å¯å‘å¼æŠ€æœ¯å°šä¸æ¸…æ¥šã€‚æœ¬æ–‡ä»‹ç»äº†åˆæˆåª’ä½“ç´ å…»æµ‹è¯•ï¼ˆSMeLæµ‹è¯•ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æµ‹è¯•è¯­è¨€æ¨¡å‹åœ¨æƒ…å¢ƒä¸­ä¸»åŠ¨è¿‡æ»¤æ‰ä¸å¯é ä¿¡æ¯çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹å¤šç§å¸¸ç”¨çš„æŒ‡ä»¤è°ƒæ•´LLMè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå‘ç°æ²¡æœ‰ä»»ä½•æ¨¡å‹å§‹ç»ˆè¡¨ç°è‰¯å¥½ï¼›è™½ç„¶æ¨ç†èƒ½åŠ›å°¤å…¶ä¸é«˜åˆ†ç›¸å…³ï¼Œä½†å³ä½¿æ˜¯è¡¨ç°æœ€ä½³çš„APIæ¨¡å‹ä¹Ÿä¼šè¾¾åˆ°70%çš„è™šæ„ç¨‹åº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ›´å¤§ã€æ›´å¼ºå¤§çš„æ¨¡å‹å¹¶ä¸ä¸€å®šæ¯”å°å‹æ¨¡å‹è¡¨ç°å¾—æ›´å¥½ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½å¸®åŠ©è¿›ä¸€æ­¥æ­ç¤ºè¿™ç§è™šæ„ç°è±¡ï¼Œå¹¶å¼•å¯¼å¼€å‘æ–°çš„æ–¹æ³•æ¥åº”å¯¹å®ƒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>äº’è”ç½‘å……æ–¥ç€ä¸å¯é çš„å†…å®¹ï¼Œå¦‚æœªç»æˆæƒçš„å†…å®¹å’Œæ•…æ„è¯¯å¯¼ä¿¡æ¯ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ‰§è¡Œè‡ªä¸»ç½‘é¡µæµè§ˆä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è¿‡æ»¤ä¸å¯é ä¿¡æ¯æ–¹é¢ã€‚</li>
<li>åˆæˆåª’ä½“ç´ å…»æµ‹è¯•ï¼ˆSMeLæµ‹è¯•ï¼‰è¢«å¼•å…¥ä½œä¸ºä¸€ç§åŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°LLMåœ¨æ­¤æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>åœ¨æµ‹è¯•ä¸­ï¼Œæ²¡æœ‰ä¸€ç§LLMå§‹ç»ˆè¡¨ç°å‡ºè‰¯å¥½çš„è¿‡æ»¤èƒ½åŠ›ã€‚</li>
<li>æ¨ç†èƒ½åŠ›åœ¨æµ‹è¯•ä¸­å°¤ä¸ºé‡è¦ï¼Œä½†ä¸é«˜è™šæ„ç¨‹åº¦ç›¸å…³ã€‚</li>
<li>å³ä½¿æ˜¯æœ€å…ˆè¿›çš„LLMä¹Ÿå­˜åœ¨è™šæ„ç°è±¡ï¼Œæœ€é«˜è¾¾åˆ°70%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a3f6ad270387a73fe6256698b4a7090b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Generative-AI-Adoption-in-Postsecondary-Education-AI-Hype-and-ChatGPTâ€™s-Launch"><a href="#Generative-AI-Adoption-in-Postsecondary-Education-AI-Hype-and-ChatGPTâ€™s-Launch" class="headerlink" title="Generative AI Adoption in Postsecondary Education, AI Hype, and   ChatGPTâ€™s Launch"></a>Generative AI Adoption in Postsecondary Education, AI Hype, and   ChatGPTâ€™s Launch</h2><p><strong>Authors:Isabel Pedersen</strong></p>
<p>The rapid integration of generative artificial intelligence (AI) into postsecondary education and many other sectors resulted in a global reckoning with this new technology. This paper contributes to the study of the multifaceted influence of generative AI, with a particular focus on OpenAIâ€™s ChatGPT within academic settings during the first six months after the release in three specific ways. First, it scrutinizes the rise of ChatGPT as a transformative event construed through a study of mainstream discourses exhibiting AI hype. Second, it discusses the perceived implications of generative AI for writing, teaching, and learning through the lens of critical discourse analysis and critical AI studies. Third, it encourages the necessity for best practices in the adoption of generative AI technologies in education. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½çš„å¿«é€Ÿé›†æˆåˆ°é«˜ç­‰æ•™è‚²å’Œå…¶ä»–è®¸å¤šé¢†åŸŸï¼Œå¼•å‘äº†å…¨çƒå¯¹è¿™ä¸€æ–°æŠ€æœ¯çš„é‡æ–°è¯„ä¼°ã€‚æœ¬æ–‡ç ”ç©¶äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„å¤šæ–¹é¢å½±å“ï¼Œç‰¹åˆ«æ˜¯OpenAIçš„ChatGPTåœ¨å­¦æœ¯ç¯å¢ƒä¸­çš„å½±å“ï¼Œä»¥ä¸‰ç§ç‰¹å®šæ–¹å¼åšå‡ºäº†è´¡çŒ®ã€‚é¦–å…ˆï¼Œå®ƒä»”ç»†ç ”ç©¶äº†ChatGPTä½œä¸ºä¸€åœºå˜é©æ€§äº‹ä»¶çš„å‡ºç°ï¼Œé€šè¿‡ä¸»æµè¯è¯­çš„ç ”ç©¶å±•ç°äº†äººå·¥æ™ºèƒ½ç‚’ä½œç°è±¡ã€‚å…¶æ¬¡ï¼Œå®ƒé€šè¿‡æ‰¹åˆ¤è¯è¯­åˆ†æå’Œæ‰¹åˆ¤äººå·¥æ™ºèƒ½ç ”ç©¶çš„è§†è§’ï¼Œæ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¯¹å†™ä½œã€æ•™å­¦å’Œå­¦ä¹ çš„æ½œåœ¨å½±å“ã€‚æœ€åï¼Œå®ƒå¼ºè°ƒäº†é‡‡ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨æ•™è‚²ä¸­çš„æœ€ä½³å®è·µçš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01003v1">PDF</a> 19 pages</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç ”ç©¶äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨é«˜ç­‰æ•™è‚²å’Œå…¶ä»–é¢†åŸŸä¸­çš„å¿«é€Ÿèåˆæ‰€å¸¦æ¥çš„å¤šæ–¹é¢å½±å“ï¼Œé‡ç‚¹å…³æ³¨äº†OpenAIçš„ChatGPTåœ¨å­¦æœ¯ç¯å¢ƒä¸­çš„é¦–æ¬¡å…­ä¸ªæœˆåº”ç”¨ã€‚æ–‡ç« åˆ†æäº†ChatGPTä½œä¸ºä¸€ä¸ªå˜é©æ€§äº‹ä»¶çš„å‡ºç°ä¸äººå·¥æ™ºèƒ½ç‚’ä½œè¯è¯­ä¹‹é—´çš„è”ç³»ï¼Œæ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¯¹å†™ä½œã€æ•™å­¦å’Œå­¦ä¹ çš„æ½œåœ¨å½±å“ï¼Œå¹¶å¼ºè°ƒäº†æ•™è‚²é¢†åŸŸé‡‡ç”¨æœ€ä½³å®è·µä½¿ç”¨ç”Ÿæˆå¼AIæŠ€æœ¯çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨æ•™è‚²å’Œå…¶ä»–é¢†åŸŸä¸­çš„èåˆå¼•èµ·å…¨çƒå…³æ³¨ã€‚</li>
<li>OpenAIçš„ChatGPTä½œä¸ºä¸€ç§å˜é©æ€§äº‹ä»¶è¢«å¹¿æ³›ç ”ç©¶ï¼Œå…¶åœ¨å­¦æœ¯ç¯å¢ƒä¸­çš„åº”ç”¨å¾—åˆ°äº†ç‰¹æ®Šå…³æ³¨ã€‚</li>
<li>æ–‡ç« æ¢è®¨äº†ChatGPTçš„å‡ºç°ä¸äººå·¥æ™ºèƒ½ç‚’ä½œè¯è¯­ä¹‹é—´çš„è”ç³»ã€‚</li>
<li>æ–‡ç« é€šè¿‡æ‰¹åˆ¤æ€§è¯è¯­åˆ†æå’Œäººå·¥æ™ºèƒ½ç ”ç©¶å®¡è§†äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å¯¹å†™ä½œã€æ•™å­¦å’Œå­¦ä¹ çš„æ½œåœ¨å½±å“ã€‚</li>
<li>æ–‡ç« å¼ºè°ƒäº†æ•™è‚²é¢†åŸŸåœ¨ä½¿ç”¨ç”Ÿæˆå¼AIæŠ€æœ¯æ—¶éœ€è¦éµå¾ªæœ€ä½³å®è·µçš„é‡è¦æ€§ã€‚</li>
<li>æ–‡ç« æŒ‡å‡ºï¼Œéšç€æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹ç”Ÿæˆå¼AIçš„ç ”ç©¶å’Œåº”ç”¨éœ€è¦æŒç»­è·Ÿè¿›å’Œè¯„ä¼°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01003">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-73a15dc61a9e9855d99c21e241914f5c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Automated-Label-Placement-on-Maps-via-Large-Language-Models"><a href="#Automated-Label-Placement-on-Maps-via-Large-Language-Models" class="headerlink" title="Automated Label Placement on Maps via Large Language Models"></a>Automated Label Placement on Maps via Large Language Models</h2><p><strong>Authors:Harry Shomer, Jiejun Xu</strong></p>
<p>Label placement is a critical aspect of map design, serving as a form of spatial annotation that directly impacts clarity and interpretability. Despite its importance, label placement remains largely manual and difficult to scale, as existing automated systems struggle to integrate cartographic conventions, adapt to context, or interpret labeling instructions. In this work, we introduce a new paradigm for automatic label placement (ALP) that formulates the task as a data editing problem and leverages large language models (LLMs) for context-aware spatial annotation. To support this direction, we curate MAPLE, the first known benchmarking dataset for evaluating ALP on real-world maps, encompassing diverse landmark types and label placement annotations from open-source data. Our method retrieves labeling guidelines relevant to each landmark type leveraging retrieval-augmented generation (RAG), integrates them into prompts, and employs instruction-tuned LLMs to generate ideal label coordinates. We evaluate four open-source LLMs on MAPLE, analyzing both overall performance and generalization across different types of landmarks. This includes both zero-shot and instruction-tuned performance. Our results demonstrate that LLMs, when guided by structured prompts and domain-specific retrieval, can learn to perform accurate spatial edits, aligning the generated outputs with expert cartographic standards. Overall, our work presents a scalable framework for AI-assisted map finishing and demonstrates the potential of foundation models in structured data editing tasks. The code and data can be found at <a target="_blank" rel="noopener" href="https://github.com/HarryShomer/MAPLE">https://github.com/HarryShomer/MAPLE</a>. </p>
<blockquote>
<p>æ ‡ç­¾æ”¾ç½®æ˜¯åœ°å›¾è®¾è®¡ä¸­çš„ä¸€ä¸ªå…³é”®æ–¹é¢ï¼Œä½œä¸ºä¸€ç§ç©ºé—´æ³¨é‡Šï¼Œå®ƒç›´æ¥å½±å“åœ°å›¾çš„æ¸…æ™°åº¦å’Œå¯è§£é‡Šæ€§ã€‚å°½ç®¡å…¶é‡è¦æ€§æ˜¾è‘—ï¼Œä½†æ ‡ç­¾æ”¾ç½®ä»ç„¶å¤§å¤šä¾èµ–äºæ‰‹åŠ¨æ“ä½œï¼Œéš¾ä»¥è§„æ¨¡åŒ–ï¼Œå› ä¸ºç°æœ‰çš„è‡ªåŠ¨åŒ–ç³»ç»Ÿå¾ˆéš¾èå…¥åœ°å›¾åˆ¶ä½œè§„èŒƒã€é€‚åº”ä¸Šä¸‹æ–‡æˆ–è§£é‡Šæ ‡ç­¾æŒ‡ä»¤ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªåŠ¨æ ‡ç­¾æ”¾ç½®ï¼ˆALPï¼‰çš„æ–°èŒƒå¼ï¼Œå°†ä»»åŠ¡å½¢å¼åŒ–ä¸ºæ•°æ®ç¼–è¾‘é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥ç©ºé—´æ³¨é‡Šã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€æ–¹å‘ï¼Œæˆ‘ä»¬åˆ›å»ºäº†MAPLEï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºè¯„ä¼°çœŸå®ä¸–ç•Œåœ°å›¾ä¸ŠALPæ€§èƒ½çš„æ ‡å‡†æ•°æ®é›†ï¼Œæ¶µç›–äº†æ¥è‡ªå¼€æºæ•°æ®çš„å„ç§åœ°æ ‡ç±»å‹å’Œæ ‡ç­¾æ”¾ç½®æ³¨é‡Šã€‚æˆ‘ä»¬çš„æ–¹æ³•æ£€ç´¢ä¸æ¯ç§åœ°æ ‡ç±»å‹ç›¸å…³çš„æ ‡ç­¾æŒ‡å—ï¼Œåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯å°†å®ƒä»¬æ•´åˆåˆ°æç¤ºä¸­ï¼Œå¹¶è°ƒç”¨æŒ‡ä»¤å¾®è°ƒLLMç”Ÿæˆç†æƒ³çš„æ ‡ç­¾åæ ‡ã€‚æˆ‘ä»¬åœ¨MAPLEä¸Šè¯„ä¼°äº†å››ä¸ªå¼€æºLLMï¼Œåˆ†æäº†æ•´ä½“æ€§èƒ½ä»¥åŠåœ¨ä¸åŒç±»å‹åœ°æ ‡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™åŒ…æ‹¬é›¶æ ·æœ¬æ•™å­¦å’ŒæŒ‡ä»¤å¾®è°ƒåçš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨ç»“æ„åŒ–æç¤ºå’Œé¢†åŸŸç‰¹å®šæ£€ç´¢çš„æŒ‡å¯¼ä¸‹ï¼ŒLLMå¯ä»¥å­¦ä¹ æ‰§è¡Œç²¾ç¡®çš„ç©ºé—´ç¼–è¾‘ï¼Œä½¿ç”Ÿæˆè¾“å‡ºç¬¦åˆä¸“å®¶åœ°å›¾åˆ¶ä½œæ ‡å‡†ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸ºAIè¾…åŠ©åœ°å›¾åˆ¶ä½œæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œå¹¶å±•ç¤ºäº†åŸºç¡€æ¨¡å‹åœ¨ç»“æ„åŒ–æ•°æ®ç¼–è¾‘ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HarryShomer/MAPLE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HarryShomer/MAPLEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22952v2">PDF</a> Workshop on AI for Data Editing (AI4DE) at KDD 2025</p>
<p><strong>Summary</strong><br>åœ°å›¾æ ‡ç­¾æ”¾ç½®æ˜¯åœ°å›¾è®¾è®¡ä¸­çš„å…³é”®è¦ç´ ï¼Œå½±å“åœ°å›¾çš„æ¸…æ™°åº¦å’Œå¯è§£é‡Šæ€§ã€‚ç°æœ‰è‡ªåŠ¨æ ‡ç­¾æ”¾ç½®ç³»ç»Ÿå­˜åœ¨å›°éš¾ï¼Œéš¾ä»¥ç»“åˆåœ°å›¾æƒ¯ä¾‹ã€é€‚åº”ä¸Šä¸‹æ–‡æˆ–è§£é‡Šæ ‡ç­¾æŒ‡ä»¤ã€‚æœ¬ç ”ç©¶å¼•å…¥ä¸€ç§æ–°çš„è‡ªåŠ¨æ ‡ç­¾æ”¾ç½®ï¼ˆALPï¼‰èŒƒå¼ï¼Œå°†ä»»åŠ¡è¡¨è¿°ä¸ºæ•°æ®ç¼–è¾‘é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç©ºé—´æ ‡æ³¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†MAPLEæ•°æ®é›†ï¼Œè¯„ä¼°ALPåœ¨çœŸå®ä¸–ç•Œåœ°å›¾ä¸Šçš„è¡¨ç°ï¼Œæ¶µç›–å¤šç§åœ°æ ‡ç±»å‹å’Œæ ‡ç­¾æ”¾ç½®æ³¨é‡Šã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ£€ç´¢ä¸æ¯ç§åœ°æ ‡ç±»å‹ç›¸å…³çš„æ ‡æ³¨æŒ‡å—ï¼Œå°†å…¶é›†æˆåˆ°æç¤ºä¸­ï¼Œå¹¶å€ŸåŠ©æŒ‡ä»¤ä¼˜åŒ–LLMç”Ÿæˆç†æƒ³çš„æ ‡ç­¾åæ ‡ã€‚åœ¨MAPLEæ•°æ®é›†ä¸Šè¯„ä¼°å››ä¸ªå¼€æºLLMçš„æ€§èƒ½ï¼Œåˆ†ææ•´ä½“è¡¨ç°å’Œåœ¨ä¸åŒç±»å‹åœ°æ ‡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’ŒæŒ‡ä»¤ä¼˜åŒ–æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨ç»“æ„åŒ–æç¤ºå’Œé¢†åŸŸç‰¹å®šæ£€ç´¢çš„æŒ‡å¯¼ä¸‹ï¼ŒLLMå¯ä»¥å­¦ä¹ æ‰§è¡Œå‡†ç¡®çš„ç©ºé—´ç¼–è¾‘ï¼Œç”Ÿæˆè¾“å‡ºä¸ä¸“å®¶åœ°å›¾åˆ¶ä½œæ ‡å‡†ç›¸ç¬¦ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„å·¥ä½œæå‡ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„AIè¾…åŠ©åœ°å›¾å®Œæˆæ¡†æ¶ï¼Œå±•ç¤ºäº†åŸºç¡€æ¨¡å‹åœ¨ç»“æ„åŒ–æ•°æ®ç¼–è¾‘ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ ‡ç­¾æ”¾ç½®æ˜¯åœ°å›¾è®¾è®¡ä¸­çš„æ ¸å¿ƒç¯èŠ‚ï¼Œç›´æ¥å½±å“åœ°å›¾çš„æ¸…æ™°åº¦å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>ç°æœ‰è‡ªåŠ¨æ ‡ç­¾æ”¾ç½®ç³»ç»Ÿé¢ä¸´é›†æˆåœ°å›¾æƒ¯ä¾‹ã€é€‚åº”ä¸Šä¸‹æ–‡å’Œè§£é‡Šæ ‡ç­¾æŒ‡ä»¤çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥æ–°çš„è‡ªåŠ¨æ ‡ç­¾æ”¾ç½®ï¼ˆALPï¼‰èŒƒå¼ï¼Œå°†å…¶è¡¨è¿°ä¸ºæ•°æ®ç¼–è¾‘é—®é¢˜å¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£å†³ã€‚</li>
<li>åˆ›ç«‹MAPLEæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°ALPåœ¨çœŸå®ä¸–ç•Œåœ°å›¾ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>æ–¹æ³•ç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œæ£€ç´¢ä¸åœ°æ ‡ç±»å‹ç›¸å…³çš„æ ‡æ³¨æŒ‡å—ï¼Œå¹¶é›†æˆåˆ°LLMçš„æç¤ºä¸­ã€‚</li>
<li>åœ¨MAPLEæ•°æ®é›†ä¸Šè¯„ä¼°LLMçš„æ€§èƒ½ï¼Œå±•ç¤ºå…¶åœ¨æ ‡ç­¾æ”¾ç½®ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22952">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed236f45fa551bca4c190ee01210c143.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4ef35c193bd45dd530fe5c533fe7101.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-13b18f0e35d9c9c0fb9fcfa81014381c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a483514fd0c4e2bf39d533949453c01f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a93af1e0affe9d6020fd97496d728b1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3c400408fd1c2c9c296001695f90fed.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Diffusion-Beats-Autoregressive-in-Data-Constrained-Settings"><a href="#Diffusion-Beats-Autoregressive-in-Data-Constrained-Settings" class="headerlink" title="Diffusion Beats Autoregressive in Data-Constrained Settings"></a>Diffusion Beats Autoregressive in Data-Constrained Settings</h2><p><strong>Authors:Mihir Prabhudesai, Mengning Wu, Amir Zadeh, Katerina Fragkiadaki, Deepak Pathak</strong></p>
<p>Autoregressive (AR) models have long dominated the landscape of large language models, driving progress across a wide range of tasks. Recently, diffusion-based language models have emerged as a promising alternative, though their advantages over AR models remain underexplored. In this paper, we systematically study masked diffusion models in data-constrained settings-where training involves repeated passes over limited data-and find that they significantly outperform AR models when compute is abundant but data is scarce. Diffusion models make better use of repeated data, achieving lower validation loss and superior downstream performance. We interpret this advantage as implicit data augmentation: masked diffusion exposes the model to a diverse distribution of token orderings and prediction tasks, unlike ARâ€™s fixed left-to-right factorization. We find new scaling laws for diffusion models and derive a closed-form expression for the critical compute threshold at which diffusion begins to outperform AR. These results suggest that when data, not compute, is the bottleneck, diffusion models offer a compelling alternative to the standard AR paradigm. Our code is available at: <a target="_blank" rel="noopener" href="https://diffusion-scaling.github.io/">https://diffusion-scaling.github.io</a>. </p>
<blockquote>
<p>è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹é•¿æœŸä»¥æ¥ä¸€ç›´åœ¨å¤§å‹è¯­è¨€æ¨¡å‹é¢†åŸŸä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œå¹¶åœ¨å„ç§ä»»åŠ¡ä¸­æ¨åŠ¨è¿›å±•ã€‚æœ€è¿‘ï¼ŒåŸºäºæ‰©æ•£çš„è¯­è¨€æ¨¡å‹ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ³•è€Œå‡ºç°ï¼Œå°½ç®¡å®ƒä»¬ç›¸å¯¹äºARæ¨¡å‹çš„ä¼˜åŠ¿ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†æ•°æ®å—é™ç¯å¢ƒä¸­æ©è”½æ‰©æ•£æ¨¡å‹çš„ç ”ç©¶ï¼Œå…¶ä¸­è®­ç»ƒæ¶‰åŠåœ¨æœ‰é™æ•°æ®ä¸Šå¤šæ¬¡è¿­ä»£ï¼Œæˆ‘ä»¬å‘ç°å½“è®¡ç®—èµ„æºå……è¶³ä½†æ•°æ®ç¨€ç¼ºæ—¶ï¼Œå®ƒä»¬ä¼šå¤§å¤§ä¼˜äºARæ¨¡å‹ã€‚æ‰©æ•£æ¨¡å‹èƒ½æ›´å¥½åœ°åˆ©ç”¨é‡å¤æ•°æ®ï¼Œå®ç°æ›´ä½çš„éªŒè¯æŸå¤±å’Œæ›´å‡ºè‰²çš„ä¸‹æ¸¸æ€§èƒ½ã€‚æˆ‘ä»¬å°†è¿™ç§ä¼˜åŠ¿è§£é‡Šä¸ºéšå¼æ•°æ®å¢å¼ºï¼šæ©è”½æ‰©æ•£ä½¿æ¨¡å‹æš´éœ²äºå¤šæ ·åŒ–çš„ä»¤ç‰Œé¡ºåºå’Œé¢„æµ‹ä»»åŠ¡åˆ†å¸ƒä¸­ï¼Œè¿™ä¸ARçš„å›ºå®šä»å·¦åˆ°å³åˆ†è§£ä¸åŒã€‚æˆ‘ä»¬ä¸ºæ‰©æ•£æ¨¡å‹æ‰¾åˆ°äº†æ–°çš„æ‰©å±•å®šå¾‹ï¼Œå¹¶æ¨å¯¼å‡ºäº†ä¸´ç•Œè®¡ç®—é˜ˆå€¼çš„é—­å¼è¡¨è¾¾å¼ï¼Œåœ¨è¿™ä¸ªé˜ˆå€¼ä¸Šï¼Œæ‰©æ•£å¼€å§‹ä¼˜äºARã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå½“æ•°æ®è€Œä¸æ˜¯è®¡ç®—æˆä¸ºç“¶é¢ˆæ—¶ï¼Œæ‰©æ•£æ¨¡å‹ä¸ºæ ‡å‡†çš„ARèŒƒå¼æä¾›äº†ä¸€ä¸ªå¼•äººæ³¨ç›®çš„æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://diffusion-scaling.github.ioä¸Šæ‰¾åˆ°./">https://diffusion-scaling.github.ioä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15857v5">PDF</a> Project Webpage: <a target="_blank" rel="noopener" href="https://diffusion-scaling.github.io/">https://diffusion-scaling.github.io</a></p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹é¢†åŸŸï¼Œè‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹é•¿æœŸå æ®ä¸»å¯¼åœ°ä½ã€‚è¿‘æœŸï¼Œæ‰©æ•£å¼è¯­è¨€æ¨¡å‹å´­éœ²å¤´è§’ï¼Œæˆä¸ºäº†ä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å…¶ç›¸è¾ƒäºARæ¨¡å‹çš„ä¼˜åŠ¿å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°ç ”ç©¶äº†æ•°æ®å—é™æƒ…å¢ƒä¸‹çš„æ©ç æ‰©æ•£æ¨¡å‹ï¼Œå‘ç°å½“è®¡ç®—èµ„æºå……è¶³è€Œæ•°æ®ç¨€ç¼ºæ—¶ï¼Œå…¶æ˜¾è‘—ä¼˜äºARæ¨¡å‹ã€‚æ‰©æ•£æ¨¡å‹èƒ½æ›´å¥½åœ°åˆ©ç”¨é‡å¤æ•°æ®ï¼Œå®ç°æ›´ä½çš„éªŒè¯æŸå¤±å’Œæ›´å‡ºè‰²çš„ä¸‹æ¸¸æ€§èƒ½ã€‚æœ¬æ–‡å°†å…¶ä¼˜åŠ¿è§£è¯»ä¸ºéšå¼æ•°æ®å¢å¼ºï¼šæ©ç æ‰©æ•£ä½¿æ¨¡å‹æš´éœ²äºå¤šæ ·åŒ–çš„ä»¤ç‰Œæ’åºå’Œé¢„æµ‹ä»»åŠ¡åˆ†å¸ƒä¸­ï¼Œä¸åŒäºARçš„å›ºå®šä»å·¦åˆ°å³çš„åˆ†è§£æ–¹å¼ã€‚ç ”ç©¶å‘ç°æ‰©æ•£æ¨¡å‹çš„æ–°å°ºåº¦å®šå¾‹ï¼Œå¹¶æ¨å¯¼å‡ºä¸´ç•Œè®¡ç®—é˜ˆå€¼çš„é—­å¼è¡¨è¾¾å¼ï¼Œå½“æ•°æ®è€Œéè®¡ç®—æˆä¸ºç“¶é¢ˆæ—¶ï¼Œæ‰©æ•£æ¨¡å‹æä¾›äº†å¯¹æ ‡å‡†ARèŒƒå¼çš„æœ‰åŠ›æ›¿ä»£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£å¼è¯­è¨€æ¨¡å‹ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆå¼€å§‹å—åˆ°å…³æ³¨ã€‚</li>
<li>åœ¨æ•°æ®å—é™ä¸”è®¡ç®—èµ„æºå……è¶³çš„æƒ…å¢ƒä¸‹ï¼Œæ©ç æ‰©æ•£æ¨¡å‹æ˜¾è‘—ä¼˜äºè‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹èƒ½æ›´å¥½åœ°åˆ©ç”¨é‡å¤æ•°æ®ï¼Œå®ç°æ›´ä½çš„éªŒè¯æŸå¤±ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å…·æœ‰éšå¼æ•°æ®å¢å¼ºçš„ç‰¹æ€§ï¼Œæš´éœ²äºå¤šæ ·åŒ–çš„ä»¤ç‰Œæ’åºå’Œé¢„æµ‹ä»»åŠ¡åˆ†å¸ƒä¸­ã€‚</li>
<li>ä¸ARæ¨¡å‹çš„å›ºå®šä»å·¦åˆ°å³åˆ†è§£æ–¹å¼ä¸åŒï¼Œæ‰©æ•£æ¨¡å‹å…·å¤‡æ›´å¤šå˜æ€§ã€‚</li>
<li>ç ”ç©¶å‘ç°äº†æ‰©æ•£æ¨¡å‹çš„æ–°å°ºåº¦å®šå¾‹ï¼Œå¹¶æ¨å¯¼å‡ºä¸´ç•Œè®¡ç®—é˜ˆå€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-43d3a680de60f891e97b80fd525a7a86.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="A-Comparative-Study-of-Specialized-LLMs-as-Dense-Retrievers"><a href="#A-Comparative-Study-of-Specialized-LLMs-as-Dense-Retrievers" class="headerlink" title="A Comparative Study of Specialized LLMs as Dense Retrievers"></a>A Comparative Study of Specialized LLMs as Dense Retrievers</h2><p><strong>Authors:Hengran Zhang, Keping Bi, Jiafeng Guo</strong></p>
<p>While large language models (LLMs) are increasingly deployed as dense retrievers, the impact of their domain-specific specialization on retrieval effectiveness remains underexplored. This investigation systematically examines how task-specific adaptations in LLMs influence their retrieval capabilities, an essential step toward developing unified retrievers capable of handling text, code, images, and multimodal content. We conduct extensive experiments with eight Qwen2.5 7B LLMs, including base, instruction-tuned, code&#x2F;math-specialized, long reasoning, and vision-language models across zero-shot retrieval settings and the supervised setting. For the zero-shot retrieval settings, we consider text retrieval from the BEIR benchmark and code retrieval from the CoIR benchmark. Further, to evaluate supervised performance, all LLMs are fine-tuned on the MS MARCO dataset. We find that mathematical specialization and the long reasoning capability cause consistent degradation in three settings, indicating conflicts between mathematical reasoning and semantic matching. The vision-language model and code-specialized LLMs demonstrate superior zero-shot performance compared to other LLMs, even surpassing BM25 on the code retrieval task, and maintain comparable performance to base LLMs in supervised settings. These findings suggest promising directions for the unified retrieval task leveraging cross-domain and cross-modal fusion. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«éƒ¨ç½²ä¸ºå¯†é›†æ£€ç´¢å™¨ï¼Œå…¶ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šåŒ–å¯¹æ£€ç´¢æ•ˆæœçš„å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°è°ƒæŸ¥äº†LLMä¸­çš„ç‰¹å®šä»»åŠ¡é€‚åº”æ€§å¦‚ä½•å½±å“å®ƒä»¬çš„æ£€ç´¢èƒ½åŠ›ï¼Œè¿™æ˜¯æœç€å¼€å‘èƒ½å¤Ÿå¤„ç†æ–‡æœ¬ã€ä»£ç ã€å›¾åƒå’Œå¤šæ¨¡æ€å†…å®¹çš„ç»Ÿä¸€æ£€ç´¢å™¨è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚æˆ‘ä»¬åœ¨å…«ä¸ªQwen2.5 7B LLMä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹ã€æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ã€ä»£ç &#x2F;æ•°å­¦ä¸“ä¸šåŒ–æ¨¡å‹ã€é•¿æœŸæ¨ç†æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåœ¨é›¶æ ·æœ¬æ£€ç´¢è®¾ç½®å’Œæœ‰ç›‘ç£è®¾ç½®ä¸‹è¿›è¡Œäº†æµ‹è¯•ã€‚å¯¹äºé›¶æ ·æœ¬æ£€ç´¢è®¾ç½®ï¼Œæˆ‘ä»¬è€ƒè™‘äº†æ¥è‡ªBEIRåŸºå‡†æµ‹è¯•çš„æ–‡æœ¬æ£€ç´¢å’Œæ¥è‡ªCoIRåŸºå‡†æµ‹è¯•çš„ä»£ç æ£€ç´¢ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¯„ä¼°ç›‘ç£æ€§èƒ½ï¼Œæ‰€æœ‰LLMéƒ½åœ¨MS MARCOæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬å‘ç°æ•°å­¦ä¸“ä¸šåŒ–çŸ¥è¯†å’Œé•¿æœŸæ¨ç†èƒ½åŠ›åœ¨ä¸‰ç§è®¾ç½®ä¸‹å‡å¯¼è‡´æ€§èƒ½æŒç»­ä¸‹é™ï¼Œè¿™è¡¨æ˜æ•°å­¦æ¨ç†å’Œè¯­ä¹‰åŒ¹é…ä¹‹é—´å­˜åœ¨å†²çªã€‚è§†è§‰è¯­è¨€æ¨¡å‹å’Œä¸“é—¨é’ˆå¯¹ä»£ç è®¾è®¡çš„LLMåœ¨é›¶æ ·æœ¬ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå“è¶Šçš„æ€§èƒ½ï¼Œç”šè‡³åœ¨ä»£ç æ£€ç´¢ä»»åŠ¡ä¸Šè¶…è¶Šäº†BM25ï¼Œå¹¶ä¸”åœ¨æœ‰ç›‘ç£ç¯å¢ƒä¸­ä¿æŒäº†ä¸åŸºç¡€LLMç›¸å½“çš„æ€§èƒ½ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåˆ©ç”¨è·¨åŸŸå’Œè·¨æ¨¡æ€èåˆçš„ç»Ÿä¸€æ£€ç´¢ä»»åŠ¡å…·æœ‰å¹¿é˜”çš„å‘å±•å‰æ™¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03958v2">PDF</a> Accepted by CCIR25 and published by Springer LNCS or LNAI</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¯†é›†æ£€ç´¢ä¸­çš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œä½†å…¶åŸŸç‰¹å®šä¸“ä¸šåŒ–å¯¹æ£€ç´¢æ•ˆæœçš„å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†LLMçš„ä»»åŠ¡ç‰¹å®šé€‚åº”æ€§å¯¹å…¶æ£€ç´¢èƒ½åŠ›çš„å½±å“ï¼Œè¿™æ˜¯æœç€å¼€å‘èƒ½å¤Ÿå¤„ç†æ–‡æœ¬ã€ä»£ç ã€å›¾åƒå’Œå¤šæ¨¡æ€å†…å®¹çš„ç»Ÿä¸€æ£€ç´¢å™¨è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚é€šè¿‡å¯¹å…«ç§ä¸åŒä¸“ä¸šé¢†åŸŸçš„LLMè¿›è¡Œå¹¿æ³›çš„å®éªŒï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹ã€æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ã€ä»£ç &#x2F;æ•°å­¦ä¸“ä¸šæ¨¡å‹ã€é€»è¾‘æ¨ç†æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹ç­‰ï¼Œå‘ç°æ•°å­¦ä¸“ä¸šåŒ–å’Œé€»è¾‘æ¨ç†èƒ½åŠ›åœ¨æŸäº›è®¾ç½®ä¸‹ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œè¡¨æ˜æ•°å­¦æ¨ç†å’Œè¯­ä¹‰åŒ¹é…ä¹‹é—´å­˜åœ¨å†²çªã€‚è§†è§‰è¯­è¨€æ¨¡å‹å’Œä»£ç ä¸“ä¸šLLMåœ¨é›¶å°„å‡»æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³åœ¨ä»£ç æ£€ç´¢ä»»åŠ¡ä¸Šè¶…è¶Šäº†BM25ï¼Œå¹¶åœ¨ç›‘ç£è®¾ç½®ä¸­çš„è¡¨ç°ä¸å…¶ä»–LLMç›¸å½“ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œåˆ©ç”¨è·¨åŸŸå’Œè·¨æ¨¡æ€èåˆçš„ç»Ÿä¸€æ£€ç´¢ä»»åŠ¡å…·æœ‰å¹¿é˜”çš„å‘å±•å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¯†é›†æ£€ç´¢ä¸­çš„åº”ç”¨å¹¿æ³›ï¼Œä½†å…¶åŸŸç‰¹å®šä¸“ä¸šåŒ–å¯¹æ£€ç´¢æ•ˆæœçš„å½±å“å°šå¾…æ¢ç´¢ã€‚</li>
<li>ä»»åŠ¡ç‰¹å®šé€‚åº”æ€§å¯¹LLMçš„æ£€ç´¢èƒ½åŠ›æœ‰é‡è¦å½±å“ã€‚</li>
<li>æ•°å­¦ä¸“ä¸šåŒ–å’Œé€»è¾‘æ¨ç†èƒ½åŠ›åœ¨æŸäº›è®¾ç½®ä¸‹å¯èƒ½å¯¼è‡´LLMæ€§èƒ½ä¸‹é™ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹å’Œä»£ç ä¸“ä¸šLLMåœ¨é›¶å°„å‡»æ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹å’Œä»£ç ä¸“ä¸šLLMåœ¨ä»£ç æ£€ç´¢ä»»åŠ¡ä¸Šå¯èƒ½è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
<li>åœ¨ç›‘ç£è®¾ç½®ä¸‹ï¼Œä¸“ä¸šLLMçš„è¡¨ç°ä¸å…¶ä»–æ¨¡å‹ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03958">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-013a99ff3c6a8ba2f36ed89ac4ac46b2.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-09/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-826cd0e8631ec02eb234f09c5a99cf00.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-09  MV-Debate Multi-view Agent Debate with Dynamic Reflection Gating for   Multimodal Harmful Content Detection in Social Media
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-09/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-41a22450d3db1ced069fec47b3e50b75.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-09  On the Generalization of SFT A Reinforcement Learning Perspective with   Reward Rectification
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25219.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
