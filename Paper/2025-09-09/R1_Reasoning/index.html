<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-09-09  COGITAO A Visual Reasoning Framework To Study Compositionality &amp;   Generalization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.05249v1/page_4_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    23k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    93 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-09-更新"><a href="#2025-09-09-更新" class="headerlink" title="2025-09-09 更新"></a>2025-09-09 更新</h1><h2 id="COGITAO-A-Visual-Reasoning-Framework-To-Study-Compositionality-Generalization"><a href="#COGITAO-A-Visual-Reasoning-Framework-To-Study-Compositionality-Generalization" class="headerlink" title="COGITAO: A Visual Reasoning Framework To Study Compositionality &amp;   Generalization"></a>COGITAO: A Visual Reasoning Framework To Study Compositionality &amp;   Generalization</h2><p><strong>Authors:Yassine Taoudi-Benchekroun, Klim Troyan, Pascal Sager, Stefan Gerber, Lukas Tuggener, Benjamin Grewe</strong></p>
<p>The ability to compose learned concepts and apply them in novel settings is key to human intelligence, but remains a persistent limitation in state-of-the-art machine learning models. To address this issue, we introduce COGITAO, a modular and extensible data generation framework and benchmark designed to systematically study compositionality and generalization in visual domains. Drawing inspiration from ARC-AGI’s problem-setting, COGITAO constructs rule-based tasks which apply a set of transformations to objects in grid-like environments. It supports composition, at adjustable depth, over a set of 28 interoperable transformations, along with extensive control over grid parametrization and object properties. This flexibility enables the creation of millions of unique task rules – surpassing concurrent datasets by several orders of magnitude – across a wide range of difficulties, while allowing virtually unlimited sample generation per rule. We provide baseline experiments using state-of-the-art vision models, highlighting their consistent failures to generalize to novel combinations of familiar elements, despite strong in-domain performance. COGITAO is fully open-sourced, including all code and datasets, to support continued research in this field. </p>
<blockquote>
<p>人类智能的关键在于能够组合学到的概念并将其应用于新的情境中，但在最先进的机器学习模型中，这仍然是一个持续的局限性。为了解决这一问题，我们引入了COGITAO，这是一个模块化且可扩展的数据生成框架和基准测试，旨在系统地研究视觉领域的组合性和泛化性。COGITAO借鉴ARC-AGI的问题设置，构建基于规则的任务，对网格环境中的对象应用一系列转换。它在可调整的深度上支持28个可操作转换的组合，并对网格参数化和对象属性进行广泛的控制。这种灵活性使得能够创建数以百万计的独特任务规则——在难度范围上远远超过现有的数据集——同时允许按规则进行几乎无限的样本生成。我们使用最先进的视觉模型提供基线实验，突出显示其在面对熟悉元素的全新组合时无法泛化的持续失败，尽管其在领域内的表现强劲。COGITAO完全开源，包括所有代码和数据集，以支持该领域的持续研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05249v1">PDF</a> 10 main pages, 3 figure, appendix available</p>
<p><strong>Summary</strong></p>
<p>本文介绍了为解决机器学习中对组成概念的限制而开发的COGITAO框架。该框架具备模块化和可扩展性，能够系统地研究视觉领域的组合性和泛化能力。通过构建基于规则的任务，应用网格环境中对象的转换，COGITAO支持在可调整深度上的一组28种互操作转换。这为创建数百万种独特的任务规则提供了可能，并允许为每个规则生成几乎无限数量的样本。同时，本文提供了使用最新视觉模型的基线实验，突显了模型在熟悉元素的新组合上的泛化能力持续失败的问题。COGITAO已完全开源，包括所有代码和数据集，以支持该领域的研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>COGITAO是一个用于研究视觉领域组合性和泛化能力的模块化、可扩展的数据生成框架和基准测试平台。</li>
<li>COGITAO通过构建基于规则的任务，应用网格环境中对象的转换，支持组合性并具备广泛的网格参数和对象属性控制。</li>
<li>COGITAO能够创建数百万种独特的任务规则，并允许为每个规则生成几乎无限数量的样本。</li>
<li>最新视觉模型在熟悉元素的新组合上的泛化能力有限，尽管它们在特定领域表现良好。</li>
<li>COGITAO提供的基线实验突显了机器学习中存在的挑战，即如何更好地应用学习到的概念于新情境。</li>
<li>COGITAO框架已完全开源，包括所有代码和数据集，以促进该领域的研究进展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05249">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.05249v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.05249v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.05249v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.05249v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MM-DREX-Multimodal-Driven-Dynamic-Routing-of-LLM-Experts-for-Financial-Trading"><a href="#MM-DREX-Multimodal-Driven-Dynamic-Routing-of-LLM-Experts-for-Financial-Trading" class="headerlink" title="MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial   Trading"></a>MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial   Trading</h2><p><strong>Authors:Yang Chen, Yueheng Jiang, Zhaozhao Ma, Yuchen Cao Jacky Keung, Kun Kuang, Leilei Gan, Yiquan Wu, Fei Wu</strong></p>
<p>The inherent non-stationarity of financial markets and the complexity of multi-modal information pose significant challenges to existing quantitative trading models. Traditional methods relying on fixed structures and unimodal data struggle to adapt to market regime shifts, while large language model (LLM)-driven solutions - despite their multi-modal comprehension - suffer from static strategies and homogeneous expert designs, lacking dynamic adjustment and fine-grained decision mechanisms. To address these limitations, we propose MM-DREX: a Multimodal-driven, Dynamically-Routed EXpert framework based on large language models. MM-DREX explicitly decouples market state perception from strategy execution to enable adaptive sequential decision-making in non-stationary environments. Specifically, it (1) introduces a vision-language model (VLM)-powered dynamic router that jointly analyzes candlestick chart patterns and long-term temporal features to allocate real-time expert weights; (2) designs four heterogeneous trading experts (trend, reversal, breakout, positioning) generating specialized fine-grained sub-strategies; and (3) proposes an SFT-RL hybrid training paradigm to synergistically optimize the router’s market classification capability and experts’ risk-adjusted decision-making. Extensive experiments on multi-modal datasets spanning stocks, futures, and cryptocurrencies demonstrate that MM-DREX significantly outperforms 15 baselines (including state-of-the-art financial LLMs and deep reinforcement learning models) across key metrics: total return, Sharpe ratio, and maximum drawdown, validating its robustness and generalization. Additionally, an interpretability module traces routing logic and expert behavior in real time, providing an audit trail for strategy transparency. </p>
<blockquote>
<p>金融市场固有的非稳定性和多模态信息的复杂性对现有量化交易模型构成了重大挑战。传统方法依赖于固定结构和单模态数据，难以适应市场体制的变化。虽然大语言模型（LLM）驱动解决方案具有多模态理解能力，但它们受到静态策略和同质化专家设计的困扰，缺乏动态调整和精细决策机制。为了解决这些局限性，我们提出了基于大语言模型的MM-DREX：一个多模态驱动、动态路由专家框架。MM-DREX显式地将市场状态感知与策略执行分离，在非线性环境中实现自适应序列决策。具体来说，它（1）引入了一种由视觉语言模型（VLM）驱动的动态路由器，该路由器联合分析K线图模式和长期时间特征以分配实时专家权重；（2）设计了四种不同的交易专家（趋势、反转、突破、定位），生成专业的精细子策略；（3）提出了一种SFT-RL混合训练范式，协同优化路由器的市场分类能力和专家的风险调整决策能力。在涵盖股票、期货和加密货币的多模态数据集上的广泛实验表明，MM-DREX在关键指标上显著优于15个基准模型（包括最先进的金融LLM和深度强化学习模型），这些指标包括总回报、夏普比率和最大回撤，验证了其稳健性和泛化能力。此外，解释性模块可实时跟踪路由逻辑和专家行为，为策略透明性提供审计跟踪。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05080v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>金融市场的内在非平稳性和多模态信息的复杂性对现有量化交易模型构成重大挑战。传统的固定结构单模态数据方法难以适应市场变化，大型语言模型驱动的方案虽具备多模态理解能力却缺乏动态调整与精细决策机制。为解决这些问题，我们提出了基于大型语言模型的Multimodal驱动的、动态路由的专家框架MM-DREX。它实现了市场状态感知与策略执行的解耦，能在非平稳环境中进行自适应的序列决策。通过引入视觉语言模型驱动的动态路由器，分析蜡烛图模式和长期时间特征来分配实时专家权重；设计四种异质交易专家生成专业精细的子策略；并提出SFT-RL混合训练范式优化路由器的市场分类能力和专家的风险调整决策能力。实验证明，MM-DREX在股票、期货和加密货币的多模态数据集上显著优于15种基线方法，验证了其稳健性和泛化能力。同时，其提供的解释性模块可实时追踪路由逻辑和专家行为，提高策略透明度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>金融市场存在非平稳性和多模态信息复杂性挑战。</li>
<li>传统量化交易模型难以适应市场变化和多模态信息。</li>
<li>MM-DREX框架引入视觉语言模型驱动的动态路由器进行实时决策。</li>
<li>MM-DREX设计四种异质交易专家生成专业精细的子策略。</li>
<li>SFT-RL混合训练范式优化路由器的市场分类和专家决策能力。</li>
<li>MM-DREX在多个数据集上表现优于其他模型，具有稳健性和泛化能力。</li>
<li>MM-DREX提供解释性模块，提高策略透明度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05080">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.05080v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.05080v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.05080v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.05080v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Sticker-TTS-Learn-to-Utilize-Historical-Experience-with-a-Sticker-driven-Test-Time-Scaling-Framework"><a href="#Sticker-TTS-Learn-to-Utilize-Historical-Experience-with-a-Sticker-driven-Test-Time-Scaling-Framework" class="headerlink" title="Sticker-TTS: Learn to Utilize Historical Experience with a   Sticker-driven Test-Time Scaling Framework"></a>Sticker-TTS: Learn to Utilize Historical Experience with a   Sticker-driven Test-Time Scaling Framework</h2><p><strong>Authors:Jie Chen, Jinhao Jiang, Yingqian Min, Zican Dong, Shijie Wang, Wayne Xin Zhao, Ji-Rong Wen</strong></p>
<p>Large reasoning models (LRMs) have exhibited strong performance on complex reasoning tasks, with further gains achievable through increased computational budgets at inference. However, current test-time scaling methods predominantly rely on redundant sampling, ignoring the historical experience utilization, thereby limiting computational efficiency. To overcome this limitation, we propose Sticker-TTS, a novel test-time scaling framework that coordinates three collaborative LRMs to iteratively explore and refine solutions guided by historical attempts. At the core of our framework are distilled key conditions-termed stickers-which drive the extraction, refinement, and reuse of critical information across multiple rounds of reasoning. To further enhance the efficiency and performance of our framework, we introduce a two-stage optimization strategy that combines imitation learning with self-improvement, enabling progressive refinement. Extensive evaluations on three challenging mathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH, demonstrate that Sticker-TTS consistently surpasses strong baselines, including self-consistency and advanced reinforcement learning approaches, under comparable inference budgets. These results highlight the effectiveness of sticker-guided historical experience utilization. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Sticker-TTS">https://github.com/RUCAIBox/Sticker-TTS</a>. </p>
<blockquote>
<p>大规模推理模型（LRMs）在复杂的推理任务中表现出了强大的性能，通过增加推理时的计算预算，可以进一步获得收益。然而，当前的测试时间扩展方法主要依赖于冗余采样，忽略了历史经验的利用，从而限制了计算效率。为了克服这一局限性，我们提出了Sticker-TTS，这是一种新型的测试时间扩展框架，它协调三个协作的LRMs，以历史尝试为引导，迭代地探索和细化解决方案。我们框架的核心是提炼出的关键条件——被称为“贴纸”，它驱动了跨多个推理轮次的关键信息的提取、细化和再利用。为了进一步提高我们框架的效率和性能，我们引入了一种两阶段优化策略，将模仿学习与自我改进相结合，实现渐进细化。在三个具有挑战性的数学推理基准测试上的广泛评估，包括AIME-24、AIME-25和奥林巴斯数学，证明Sticker-TTS在可比较推理预算下，持续超越了强大的基线，包括自我一致性测试和先进的强化学习的方法。这些结果突显了贴纸引导的历史经验利用的有效性。我们的代码和数据可在<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Sticker-TTS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/RUCAIBox/Sticker-TTS找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05007v1">PDF</a> 11 pages, 1 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>大型推理模型（LRMs）在复杂推理任务上表现出强大的性能，通过增加计算预算可以在推理时进一步提高性能。然而，当前的测试时间扩展方法主要依赖于冗余采样，忽略了历史经验的利用，从而限制了计算效率。为了克服这一局限性，我们提出了Sticker-TTS这一新颖的测试时间扩展框架，它通过协调三个协作的LRMs来迭代地探索并优化解决方案，这些解决方案由历史尝试引导。框架的核心是提炼出的关键条件——被称为“贴纸”，它们驱动关键信息的提取、提炼和重用，在多个推理轮次中协同工作。通过引入两阶段优化策略，结合模仿学习与自我改进，我们的框架进一步提高了效率和性能。在AIME-24、AIME-25和OlymMATH三个具有挑战性的数学推理基准测试上的广泛评估表明，Sticker-TTS在可比较推理预算下，持续超越包括自我一致性和高级强化学习方法等强基线，这凸显了贴纸引导的历史经验利用的有效性。我们的代码和数据可以在[网站链接]（<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Sticker-TTS%EF%BC%89%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/RUCAIBox/Sticker-TTS）找到。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型推理模型（LRMs）在复杂推理任务上表现出强大的性能，可通过增加计算预算进一步提高性能。</li>
<li>当前测试时间扩展方法主要依赖冗余采样，忽略了历史经验利用，限制了计算效率。</li>
<li>Sticker-TTS框架通过协调三个协作的LRMs来迭代探索并优化解决方案，这些解决方案由历史尝试引导。</li>
<li>框架核心为提炼出的关键条件（贴纸），驱动关键信息的提取、提炼和重用。</li>
<li>引入两阶段优化策略，结合模仿学习与自我改进，提高了框架的效率和性能。</li>
<li>在多个数学推理基准测试上，Sticker-TTS持续超越现有方法，显示出其有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05007">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.05007v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.05007v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.05007v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ACE-RL-Adaptive-Constraint-Enhanced-Reward-for-Long-form-Generation-Reinforcement-Learning"><a href="#ACE-RL-Adaptive-Constraint-Enhanced-Reward-for-Long-form-Generation-Reinforcement-Learning" class="headerlink" title="ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation   Reinforcement Learning"></a>ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation   Reinforcement Learning</h2><p><strong>Authors:Jianghao Chen, Wei Sun, Qixiang Yin, Lingxing Kong, Zhixing Tan, Jiajun Zhang</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable progress in long-context understanding, yet they face significant challenges in high-quality long-form generation. Existing studies primarily suffer from two limitations: (1) A heavy reliance on scarce, high-quality long-form response data for supervised fine-tuning (SFT) or for pairwise preference reward in reinforcement learning (RL). (2) Focus on coarse-grained quality optimization dimensions, such as relevance, coherence, and helpfulness, overlooking the fine-grained specifics inherent to diverse long-form generation scenarios. To address this issue, we propose a framework using Adaptive Constraint-Enhanced reward for long-form generation Reinforcement Learning (ACE-RL). ACE-RL first automatically deconstructs each instruction into a set of fine-grained, adaptive constraint criteria by identifying its underlying intents and demands. Subsequently, we design a reward mechanism that quantifies the quality of long-form responses based on their satisfaction over corresponding constraints, converting subjective quality evaluation into constraint verification. Finally, we utilize reinforcement learning to guide models toward superior long-form generation capabilities. Experimental results demonstrate that our ACE-RL framework significantly outperforms existing SFT and RL baselines by 20.70% and 7.32% on WritingBench, and our top-performing model even surpasses proprietary systems like GPT-4o by 7.10%, providing a more effective training paradigm for LLMs to generate high-quality content across diverse long-form generation scenarios. </p>
<blockquote>
<p>大型语言模型（LLM）在理解长文本方面取得了显著的进步，但在高质量长文本生成方面仍面临重大挑战。现有研究主要存在两个局限性：（1）过于依赖稀缺的高质量长文本响应数据进行监督微调（SFT）或在强化学习（RL）中的成对偏好奖励。（2）关注粗粒度的质量优化维度，如相关性、连贯性和有用性，忽视了长文本生成场景中固有的细粒度细节。为解决这一问题，我们提出了一种自适应约束增强奖励的长文本生成强化学习框架（ACE-RL）。ACE-RL首先自动将每条指令分解为一组细粒度的自适应约束标准，通过识别其潜在意图和需求。随后，我们设计了一种奖励机制，根据长文本响应对相应约束的满足程度来量化其质量，将主观质量评价转化为约束验证。最后，我们使用强化学习来指导模型实现更出色的长文本生成能力。实验结果表明，我们的ACE-RL框架在WritingBench上的表现显著优于现有的SFT和RL基线，分别提高了20.70%和7.32%，我们表现最佳的模型甚至超过了如GPT-4o等专有系统，达到了7.10%，为LLM生成高质量内容提供了更有效的训练范式，适用于各种长文本生成场景。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04903v1">PDF</a> Under review, our code is available at <a target="_blank" rel="noopener" href="https://github.com/ZNLP/ACE-RL">https://github.com/ZNLP/ACE-RL</a></p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLM）在理解长文本方面取得了显著进展，但在高质量长文本生成方面仍面临挑战。现有研究主要存在两个局限性：一是过度依赖稀缺的高质量长文本响应数据进行监督微调（SFT）或强化学习（RL）中的配对偏好奖励；二是过于关注粗略的质量优化维度，如相关性、连贯性和有用性，忽视了长文本生成场景中固有的细微差别。为解决这一问题，我们提出了使用自适应约束增强奖励的长文本生成强化学习框架（ACE-RL）。ACE-RL首先自动将每个指令分解为一组精细的、自适应约束标准，通过识别其潜在意图和需求来实现。随后，我们设计了一种奖励机制，根据长文本响应对相应约束的满足程度来量化其质量，将主观质量评价转化为约束验证。最后，我们使用强化学习指导模型实现更出色的长文本生成能力。实验结果表明，我们的ACE-RL框架在WritingBench上的表现显著优于现有的SFT和RL基准测试，分别高出20.70%和7.32%，我们表现最佳的模型甚至超越了GPT-4o等专有系统，提高了7.10%，为LLM生成高质量内容提供了更有效的训练范式。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型（LLM）在长文本生成方面面临挑战，尤其是在高质量内容生成方面。</li>
<li>现有研究主要受制于对高质量长文本响应数据的依赖以及过于粗略的质量优化维度。</li>
<li>ACE-RL框架通过自动解析指令为精细的、自适应约束标准来解决这些问题。</li>
<li>奖励机制基于约束满足程度来量化长文本响应的质量，将主观评价转化为约束验证。</li>
<li>强化学习用于指导模型实现更出色的长文本生成能力。</li>
<li>ACE-RL框架在WritingBench上的表现优于现有基准测试，显示出其有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04903">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04903v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04903v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04903v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04903v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04903v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04903v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TalkToAgent-A-Human-centric-Explanation-of-Reinforcement-Learning-Agents-with-Large-Language-Models"><a href="#TalkToAgent-A-Human-centric-Explanation-of-Reinforcement-Learning-Agents-with-Large-Language-Models" class="headerlink" title="TalkToAgent: A Human-centric Explanation of Reinforcement Learning   Agents with Large Language Models"></a>TalkToAgent: A Human-centric Explanation of Reinforcement Learning   Agents with Large Language Models</h2><p><strong>Authors:Haechang Kim, Hao Chen, Can Li, Jong Min Lee</strong></p>
<p>Explainable Reinforcement Learning (XRL) has emerged as a promising approach in improving the transparency of Reinforcement Learning (RL) agents. However, there remains a gap between complex RL policies and domain experts, due to the limited comprehensibility of XRL results and isolated coverage of current XRL approaches that leave users uncertain about which tools to employ. To address these challenges, we introduce TalkToAgent, a multi-agent Large Language Models (LLM) framework that delivers interactive, natural language explanations for RL policies. The architecture with five specialized LLM agents (Coordinator, Explainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically map user queries to relevant XRL tools and clarify an agent’s actions in terms of either key state variables, expected outcomes, or counterfactual explanations. Moreover, our approach extends previous counterfactual explanations by deriving alternative scenarios from qualitative behavioral descriptions, or even new rule-based policies. We validated TalkToAgent on quadruple-tank process control problem, a well-known nonlinear control benchmark. Results demonstrated that TalkToAgent successfully mapped user queries into XRL tasks with high accuracy, and coder-debugger interactions minimized failures in counterfactual generation. Furthermore, qualitative evaluation confirmed that TalkToAgent effectively interpreted agent’s actions and contextualized their meaning within the problem domain. </p>
<blockquote>
<p>可解释性强化学习（XRL）作为一种有前景的方法，在提高强化学习（RL）代理的透明度方面展现出巨大潜力。然而，由于XRL结果的可理解性有限以及当前XRL方法的孤立覆盖，使得复杂RL策略与领域专家之间存在差距，使用户对使用哪种工具感到不确定。为了应对这些挑战，我们引入了TalkToAgent，这是一个多代理的大型语言模型（LLM）框架，为RL策略提供交互式自然语言解释。该架构包含五个专业LLM代理（协调器、解释器、编码器、评估器和调试器），使TalkToAgent能够自动将用户查询映射到相关的XRL工具，并根据关键状态变量、预期结果或反事实解释来澄清代理的行动。此外，我们的方法通过从定性行为描述或甚至新的基于规则的策略中推导出替代场景，扩展了之前的反事实解释。我们在著名的非线性控制基准——四罐过程控制问题上验证了TalkToAgent。结果表明，TalkToAgent能够准确地将用户查询映射到XRL任务，并且编码器和调试器之间的交互减少了反事实生成中的失败。此外，定性评估证实，TalkToAgent有效地解释了代理的行动，并将它们的问题域背景联系起来。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04809v1">PDF</a> 31 pages total</p>
<p><strong>Summary</strong>：<br>强化学习（RL）的透明度问题可通过解释性强化学习（XRL）改善。然而，由于XRL结果的可理解性有限以及当前XRL方法的孤立性，用户和领域专家之间仍存在差距。为解决这些挑战，我们推出了TalkToAgent，这是一个多智能体的自然语言模型框架，能为RL策略提供交互式自然语言解释。TalkToAgent拥有五个专业智能体（协调器、解释器、编码员、评估器和调试器），能自动将用户查询映射到相关的XRL工具，并能就关键状态变量、预期结果或反事实解释来澄清智能体的行动。此外，我们的方法通过从定性行为描述或新的规则基础策略中推导出替代场景，扩展了之前的反事实解释。在四倍水箱过程控制问题上验证了TalkToAgent的有效性，其成功地将用户查询映射到XRL任务中，且编码员-调试器互动减少了反事实生成中的失败。此外，定性评估证实TalkToAgent有效地解释了智能体的行动并将其置于问题域中。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>TalkToAgent是一个多智能体的自然语言模型框架，旨在解决解释性强化学习（XRL）在用户和领域专家之间的鸿沟问题。</li>
<li>它包含五个专业智能体，能自动将用户查询映射到相关的XRL工具，提供交互式自然语言解释。</li>
<li>TalkToAgent可以提供关于智能体行动的关键状态变量、预期结果或反事实解释的解释。</li>
<li>该方法扩展了之前的反事实解释，通过定性行为描述和新的规则基础策略来推导替代场景。</li>
<li>在四倍水箱过程控制问题上的验证显示，TalkToAgent能成功地将用户查询映射到XRL任务中，并具有高度的准确性。</li>
<li>编码员和调试器之间的互动减少了反事实生成中的失败。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04809">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04809v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04809v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04809v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04809v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Mind-the-Gap-Evaluating-Model-and-Agentic-Level-Vulnerabilities-in-LLMs-with-Action-Graphs"><a href="#Mind-the-Gap-Evaluating-Model-and-Agentic-Level-Vulnerabilities-in-LLMs-with-Action-Graphs" class="headerlink" title="Mind the Gap: Evaluating Model- and Agentic-Level Vulnerabilities in   LLMs with Action Graphs"></a>Mind the Gap: Evaluating Model- and Agentic-Level Vulnerabilities in   LLMs with Action Graphs</h2><p><strong>Authors:Ilham Wicaksono, Zekun Wu, Theo King, Adriano Koshiyama, Philip Treleaven</strong></p>
<p>As large language models transition to agentic systems, current safety evaluation frameworks face critical gaps in assessing deployment-specific risks. We introduce AgentSeer, an observability-based evaluation framework that decomposes agentic executions into granular action and component graphs, enabling systematic agentic-situational assessment. Through cross-model validation on GPT-OSS-20B and Gemini-2.0-flash using HarmBench single turn and iterative refinement attacks, we demonstrate fundamental differences between model-level and agentic-level vulnerability profiles. Model-level evaluation reveals baseline differences: GPT-OSS-20B (39.47% ASR) versus Gemini-2.0-flash (50.00% ASR), with both models showing susceptibility to social engineering while maintaining logic-based attack resistance. However, agentic-level assessment exposes agent-specific risks invisible to traditional evaluation. We discover “agentic-only” vulnerabilities that emerge exclusively in agentic contexts, with tool-calling showing 24-60% higher ASR across both models. Cross-model analysis reveals universal agentic patterns, agent transfer operations as highest-risk tools, semantic rather than syntactic vulnerability mechanisms, and context-dependent attack effectiveness, alongside model-specific security profiles in absolute ASR levels and optimal injection strategies. Direct attack transfer from model-level to agentic contexts shows degraded performance (GPT-OSS-20B: 57% human injection ASR; Gemini-2.0-flash: 28%), while context-aware iterative attacks successfully compromise objectives that failed at model-level, confirming systematic evaluation gaps. These findings establish the urgent need for agentic-situation evaluation paradigms, with AgentSeer providing the standardized methodology and empirical validation. </p>
<blockquote>
<p>随着大型语言模型向代理系统过渡，当前的安全评估框架在评估特定部署风险方面存在重大缺陷。我们引入了基于观察力的评估框架AgentSeer，它将代理执行分解为细粒度的操作和组件图，从而能够进行系统的代理情境评估。我们通过使用HarmBench的单轮和迭代改进攻击对GPT-OSS-20B和Gemini-2.0-flash进行跨模型验证，展示了模型级和代理级漏洞分布之间的基本差异。模型级评估揭示了基线差异：GPT-OSS-20B的ASR（攻击成功率）为39.47%，而Gemini-2.0-flash的ASR为50.00%，且两个模型都表现出对社会工程的易感性，同时保持基于逻辑的攻击抵抗能力。然而，代理级评估暴露了与代理相关的特定风险，这些风险在传统的评估中是不可见的。我们发现了一些“仅与代理相关的”漏洞，这些漏洞仅在代理环境中出现，工具调用的ASR在两种模型中高出24-60%。跨模型分析揭示了通用的代理模式、代理转移操作作为最高风险工具、语义而非句法漏洞机制以及与上下文相关的攻击有效性以及模型特定的安全配置文件在绝对ASR水平和最佳注入策略方面。直接从模型级别攻击转移到代理环境显示性能下降（GPT-OSS-20B：人类注入ASR为57%；Gemini-2.0-flash：28%），而上下文感知的迭代攻击成功地实现了对模型级别未能实现的目标的攻击，这证实了系统评估的差距。这些发现凸显了代理情境评估模式的迫切需要，而AgentSeer提供了标准化的方法和实证验证。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04802v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>随着大型语言模型向代理系统过渡，现有的安全评估框架在评估部署特定风险时存在重大缺陷。本文介绍了基于观察力的评估框架AgentSeer，它将代理执行分解为细粒度的动作和组件图，以实现系统的代理情境评估。通过对GPT-OSS-20B和Gemini-2.0-flash的跨模型验证，展示了模型级和代理级漏洞分布的根差异。模型级别的评估显示基线差异：GPT-OSS-20B的攻击成功率（ASR）为39.47%，而Gemini-2.0-flash的ASR为50.0%。两者都显示出对社会工程的脆弱性，同时保持基于逻辑的攻击抵抗能力。然而，代理级别的评估揭示了传统评估无法发现的代理特定风险。我们发现“仅代理”的漏洞仅在代理环境中出现，工具调用在两者中的ASR高出24-60%。跨模型分析揭示了通用的代理模式，代理传输操作作为最高风险工具，语义而非语法漏洞机制以及上下文相关的攻击有效性，以及模型特定的安全概况（绝对ASR水平和最佳注入策略）。直接从模型级别转移到代理环境的攻击显示性能下降（GPT-OSS-20B：人类注入ASR为57%；Gemini-2.0-flash为28%），而上下文感知的迭代攻击成功实现了对模型级别失败目标的攻击，证实了系统评估的差距。这些发现凸显了对代理情境评估范式的迫切需求，而AgentSeer提供了标准化的方法和实证验证。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型向代理系统过渡过程中，现有的安全评估框架存在缺陷，无法有效评估部署特定的风险。</li>
<li>AgentSeer框架能够通过观察力评估代理系统的安全性，将代理执行分解为细粒度的动作和组件图。</li>
<li>模型级和代理级漏洞分布存在根本差异，需要区分对待。</li>
<li>GPT-OSS-20B和Gemini-2.0-flash在模型级别存在不同的安全基线，且都面临社会工程攻击的脆弱性。</li>
<li>代理级别的评估揭示了传统评估无法发现的“仅代理”漏洞和代理特定风险。</li>
<li>跨模型分析发现通用的代理模式，如代理传输操作的高风险性和语义漏洞机制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04802">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04802v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04802v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04802v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Personality-as-a-Probe-for-LLM-Evaluation-Method-Trade-offs-and-Downstream-Effects"><a href="#Personality-as-a-Probe-for-LLM-Evaluation-Method-Trade-offs-and-Downstream-Effects" class="headerlink" title="Personality as a Probe for LLM Evaluation: Method Trade-offs and   Downstream Effects"></a>Personality as a Probe for LLM Evaluation: Method Trade-offs and   Downstream Effects</h2><p><strong>Authors:Gunmay Handa, Zekun Wu, Adriano Koshiyama, Philip Treleaven</strong></p>
<p>Personality manipulation in large language models (LLMs) is increasingly applied in customer service and agentic scenarios, yet its mechanisms and trade-offs remain unclear. We present a systematic study of personality control using the Big Five traits, comparing in-context learning (ICL), parameter-efficient fine-tuning (PEFT), and mechanistic steering (MS). Our contributions are fourfold. First, we construct a contrastive dataset with balanced high&#x2F;low trait responses, enabling effective steering vector computation and fair cross-method evaluation. Second, we introduce a unified evaluation framework based on within-run $\Delta$ analysis that disentangles, reasoning capability, agent performance, and demographic bias across MMLU, GAIA, and BBQ benchmarks. Third, we develop trait purification techniques to separate openness from conscientiousness, addressing representational overlap in trait encoding. Fourth, we propose a three-level stability framework that quantifies method-, trait-, and combination-level robustness, offering practical guidance under deployment constraints. Experiments on Gemma-2-2B-IT and LLaMA-3-8B-Instruct reveal clear trade-offs: ICL achieves strong alignment with minimal capability loss, PEFT delivers the highest alignment at the cost of degraded task performance, and MS provides lightweight runtime control with competitive effectiveness. Trait-level analysis shows openness as uniquely challenging, agreeableness as most resistant to ICL, and personality encoding consolidating around intermediate layers. Taken together, these results establish personality manipulation as a multi-level probe into behavioral representation, linking surface conditioning, parameter encoding, and activation-level steering, and positioning mechanistic steering as a lightweight alternative to fine-tuning for both deployment and interpretability. </p>
<blockquote>
<p>人格操控在大规模语言模型（LLM）中的应用越来越多地应用于客户服务和代理场景，但其机制和权衡取舍仍不明确。我们对使用大五特质进行人格控制进行了系统研究，对比了上下文学习（ICL）、参数效率微调（PEFT）和机制性引导（MS）。我们的贡献有四个。首先，我们构建了一个对比数据集，其中包含平衡的高&#x2F;低特质响应，使得能够有效地计算引导向量并进行跨方法的公平评估。其次，我们基于内部运行$\Delta$分析引入了一个统一的评估框架，该框架能够解开推理能力、代理性能和人口统计偏差在MMLU、GAIA和BBQ基准测试中的关联。第三，我们开发了特质净化技术，以分离开放性和尽责性，解决特质编码中的代表性重叠问题。第四，我们提出了一个三级稳定性框架，该框架量化方法、特质和组合层面的稳健性，为部署约束下的实际操作提供指导。在Gemma-2-2B-IT和LLaMA-3-8B-Instruct上的实验显示出明确的权衡：ICL实现强对齐且能力损失最小，PEFT以任务性能下降为代价实现最高对齐，而MS提供轻量级的运行时控制并具有竞争性的有效性。特质层面的分析表明，开放性具有独特挑战性，宜人性最难以通过ICL改变，人格编码集中在中间层。总的来说，这些结果确立了人格操控作为行为表示的多层次探针，将表面条件、参数编码和激活级引导联系起来，并将机制性引导定位为部署和可解释性方面的轻量级替代方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04794v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文系统研究了使用大语言模型（LLM）进行人格控制的技术，包括上下文学习（ICL）、参数效率微调（PEFT）和机制控制（MS）。文章构建了对比数据集，提出了统一评估框架，发展了特质净化技术，并提出了三级稳定性框架，以量化方法、特质和组合层面的稳健性。实验结果显示不同方法各有优缺点，其中ICL对齐效果好且能力损失小，PEFT对齐度最高但任务性能下降，MS提供轻量级运行时控制且效果较好。研究结果表明人格操控是一个多层次的行为表征探针，涉及表面条件、参数编码和激活层面的操控。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大语言模型（LLM）在客户服务等场景中应用越来越广泛的人格操控技术。</li>
<li>通过使用对比数据集，进行基于Big Five特质的人格控制研究。</li>
<li>构建统一评估框架用于分析推理能力、代理性能和人口统计偏见。</li>
<li>发展特质净化技术以解决特质编码中的代表性重叠问题。</li>
<li>提出三级稳定性框架以量化不同方法的稳健性。</li>
<li>实验结果显示不同人格操控方法各有优缺点，包括ICL、PEFT和MS。</li>
<li>人格操控涉及表面条件、参数编码和激活层面的操控，研究提供对该领域的深入理解。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04794">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04794v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04794v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="WildScore-Benchmarking-MLLMs-in-the-Wild-Symbolic-Music-Reasoning"><a href="#WildScore-Benchmarking-MLLMs-in-the-Wild-Symbolic-Music-Reasoning" class="headerlink" title="WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning"></a>WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning</h2><p><strong>Authors:Gagan Mundada, Yash Vishe, Amit Namburi, Xin Xu, Zachary Novack, Julian McAuley, Junda Wu</strong></p>
<p>Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various vision-language tasks. However, their reasoning abilities in the multimodal symbolic music domain remain largely unexplored. We introduce WildScore, the first in-the-wild multimodal symbolic music reasoning and analysis benchmark, designed to evaluate MLLMs’ capacity to interpret real-world music scores and answer complex musicological queries. Each instance in WildScore is sourced from genuine musical compositions and accompanied by authentic user-generated questions and discussions, capturing the intricacies of practical music analysis. To facilitate systematic evaluation, we propose a systematic taxonomy, comprising both high-level and fine-grained musicological ontologies. Furthermore, we frame complex music reasoning as multiple-choice question answering, enabling controlled and scalable assessment of MLLMs’ symbolic music understanding. Empirical benchmarking of state-of-the-art MLLMs on WildScore reveals intriguing patterns in their visual-symbolic reasoning, uncovering both promising directions and persistent challenges for MLLMs in symbolic music reasoning and analysis. We release the dataset and code. </p>
<blockquote>
<p>近期多模态大型语言模型（MLLMs）的进步在各种视觉语言任务中表现出了令人印象深刻的能力。然而，它们在多模态符号音乐领域的推理能力仍未得到充分的探索。我们引入了WildScore，这是首个实际场景中的多模态符号音乐推理与分析基准测试，旨在评估MLLMs解读现实音乐乐谱和回答复杂音乐学问题的能力。WildScore中的每个实例都来源于真实的音乐作品，并附有真实的用户生成的问题和讨论，捕捉实际音乐分析的细微之处。为了促进系统评估，我们提出了一个系统的分类法，包括高级和精细的音乐学本体。此外，我们将复杂的音乐推理构建为选择题回答的形式，以实现对MLLMs符号音乐理解的受控和可扩展评估。在WildScore上对最新MLLMs的实证基准测试揭示了它们在视觉符号推理中的有趣模式，既揭示了MLLMs在符号音乐推理和分析方面的有前途的方向，也揭示了持续存在的挑战。我们公开了数据集和代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04744v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>最新进展的多模态大型语言模型（MLLMs）在各种视觉语言任务中表现出强大的能力，但在多模态符号音乐领域的推理能力尚未得到充分探索。本文介绍了WildScore，首个面向真实世界的多模态符号音乐推理与分析基准测试，旨在评估MLLMs解读真实世界乐谱和回答复杂音乐学查询的能力。WildScore的每个实例都来源于真实的音乐作品，并附有用户生成的问题和讨论，捕捉实际音乐分析的细节。本文还提出了一种系统的分类法，包括高级和精细的音乐学本体，以促进系统评估。通过将复杂的音乐推理作为选择题来回答，实现了对MLLMs符号音乐理解的受控和可扩展评估。对最新MLLMs在WildScore上的实证基准测试揭示了它们在视觉符号推理中的有趣模式，为MLLMs在符号音乐推理和分析中揭示了有前途的方向和持续存在的挑战。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>多模态大型语言模型（MLLMs）在视觉语言任务中表现出强大的能力。</li>
<li>WildScore是首个面向真实世界的多模态符号音乐推理与分析基准测试。</li>
<li>WildScore实例源于真实音乐作品，并附有用户生成的问题和讨论。</li>
<li>提出了系统的分类法，包括高级和精细的音乐学本体，以促进系统评估。</li>
<li>复杂音乐推理被形式化为选择题回答，以实现系统评估的受控和可扩展性。</li>
<li>实证基准测试揭示了MLLMs在符号音乐推理中的有趣模式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04744">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04744v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04744v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04744v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04744v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04744v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04744v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04744v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Evaluating-NL2SQL-via-SQL2NL"><a href="#Evaluating-NL2SQL-via-SQL2NL" class="headerlink" title="Evaluating NL2SQL via SQL2NL"></a>Evaluating NL2SQL via SQL2NL</h2><p><strong>Authors:Mohammadtaher Safarzadeh, Afshin Oroojlooyjadid, Dan Roth</strong></p>
<p>Robust evaluation in the presence of linguistic variation is key to understanding the generalization capabilities of Natural Language to SQL (NL2SQL) models, yet existing benchmarks rarely address this factor in a systematic or controlled manner. We propose a novel schema-aligned paraphrasing framework that leverages SQL-to-NL (SQL2NL) to automatically generate semantically equivalent, lexically diverse queries while maintaining alignment with the original schema and intent. This enables the first targeted evaluation of NL2SQL robustness to linguistic variation in isolation-distinct from prior work that primarily investigates ambiguity or schema perturbations. Our analysis reveals that state-of-the-art models are far more brittle than standard benchmarks suggest. For example, LLaMa3.3-70B exhibits a 10.23% drop in execution accuracy (from 77.11% to 66.9%) on paraphrased Spider queries, while LLaMa3.1-8B suffers an even larger drop of nearly 20% (from 62.9% to 42.5%). Smaller models (e.g., GPT-4o mini) are disproportionately affected. We also find that robustness degradation varies significantly with query complexity, dataset, and domain – highlighting the need for evaluation frameworks that explicitly measure linguistic generalization to ensure reliable performance in real-world settings. </p>
<blockquote>
<p>在存在语言变体的情况下进行稳健评估是了解自然语言到SQL（NL2SQL）模型的泛化能力的关键，但现有基准测试很少以系统或受控的方式处理这一因素。我们提出了一种新型的模式对齐的改述框架，该框架利用SQL到自然语言（SQL2NL）自动生成语义等效、词汇丰富的查询，同时保持与原始模式和意图的对齐。这使得首次针对NL2SQL对语言变化的稳健性进行定向评估，与主要调查模糊性或模式扰动的先前工作相区别。我们的分析表明，最先进模型的稳健性远远低于标准基准测试所显示的情况。例如，LLaMa3.3-70B在改述的Spider查询上的执行精度下降了10.23%（从77.11%降至66.9%），而LLaMa3.1-8B的下降幅度更大，近乎20%（从62.9%降至42.5%）。较小的模型（例如GPT-4o mini）受到的影响尤为严重。我们还发现，稳健性退化与查询复杂性、数据集和领域之间有着显著差异——这凸显了需要评估框架明确测量语言泛化能力，以确保在真实环境中的可靠性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04657v1">PDF</a> Accepted to EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>本文关注自然语言到SQL（NL2SQL）模型的通用化能力评价，特别是在存在语言变异的情况下。文章提出了一种新的模式对齐的改述框架，该框架利用SQL到自然语言（SQL2NL）自动生成语义等效、词汇丰富的查询，同时保持与原始模式和意图的对齐。该框架使首次对NL2SQL在语言变异方面的稳健性进行有针对性的评估成为可能，与主要关注歧义或模式扰动的研究不同。分析表明，最先进的模型比标准基准测试所显示的要脆弱得多。例如，LLaMa3.3-70B在改述的Spider查询上执行精度下降了10.23%（从77.11%降至66.9%），LLaMa3.1-8B的下降幅度更大（从62.9%降至42.5%）。较小的模型受影响尤为严重。此外，还发现稳健性退化与查询复杂性、数据集和领域变化密切相关，这突显了需要评估框架明确测量语言泛化以确保在真实环境中的可靠性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语言变异是评估NL2SQL模型通用化能力的重要因素，现有基准测试很少以系统或受控的方式处理这一因素。</li>
<li>提出了一种新的模式对齐改述框架，利用SQL2NL生成语义等效、词汇丰富的查询，同时保持与原始模式意图的对齐。</li>
<li>文章对NL2SQL模型在语言变异方面的稳健性进行了首次有针对性的评估。</li>
<li>最先进的模型在改述查询上的表现比标准基准测试所显示的更为脆弱。</li>
<li>不同模型对语言变异的敏感度不同，较小的模型受影响尤为严重。</li>
<li>稳健性退化与查询复杂性、数据集和领域变化密切相关。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04657">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04657v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04657v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04657v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04657v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04657v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AraHalluEval-A-Fine-grained-Hallucination-Evaluation-Framework-for-Arabic-LLMs"><a href="#AraHalluEval-A-Fine-grained-Hallucination-Evaluation-Framework-for-Arabic-LLMs" class="headerlink" title="AraHalluEval: A Fine-grained Hallucination Evaluation Framework for   Arabic LLMs"></a>AraHalluEval: A Fine-grained Hallucination Evaluation Framework for   Arabic LLMs</h2><p><strong>Authors:Aisha Alansari, Hamzah Luqman</strong></p>
<p>Recently, extensive research on the hallucination of the large language models (LLMs) has mainly focused on the English language. Despite the growing number of multilingual and Arabic-specific LLMs, evaluating LLMs’ hallucination in the Arabic context remains relatively underexplored. The knowledge gap is particularly pressing given Arabic’s widespread use across many regions and its importance in global communication and media. This paper presents the first comprehensive hallucination evaluation of Arabic and multilingual LLMs on two critical Arabic natural language generation tasks: generative question answering (GQA) and summarization. This study evaluates a total of 12 LLMs, including 4 Arabic pre-trained models, 4 multilingual models, and 4 reasoning-based models. To assess the factual consistency and faithfulness of LLMs’ outputs, we developed a fine-grained hallucination evaluation framework consisting of 12 fine-grained hallucination indicators that represent the varying characteristics of each task. The results reveal that factual hallucinations are more prevalent than faithfulness errors across all models and tasks. Notably, the Arabic pre-trained model Allam consistently demonstrates lower hallucination rates than multilingual models and a comparative performance with reasoning-based models. The code is available at: \href{<a target="_blank" rel="noopener" href="https://github.com/aishaalansari57/AraHalluEval%7D%7BGithub">https://github.com/aishaalansari57/AraHalluEval}{Github</a> link}. </p>
<blockquote>
<p>最近，关于大型语言模型（LLM）的幻觉的广泛研究主要集中在英语上。尽管多语言化和阿拉伯语言特定的LLM数量不断增加，但在阿拉伯语境下评估LLM的幻觉仍然相对缺乏研究。考虑到阿拉伯语在许多地区的广泛使用及其在全球通信和媒体中的重要性，知识差距尤为紧迫。本文首次对阿拉伯语和多语言LLM的幻觉进行了全面的评估，涉及两个关键的阿拉伯语自然语言生成任务：生成性问题回答（GQA）和摘要。本研究共评估了12个LLM，包括4个阿拉伯语预训练模型、4个多语言模型和4个基于推理的模型。为了评估LLM输出的事实一致性和忠实度，我们开发了一个精细的幻觉评估框架，包含12个精细的幻觉指标，代表每个任务的不同特点。结果表明，事实幻觉在所有模型和任务中比忠实度错误更为普遍。值得注意的是，阿拉伯语预训练模型Allam的幻觉率始终低于多语言模型，与基于推理的模型相比表现。代码可在Github链接：<a target="_blank" rel="noopener" href="https://github.com/aishaalansari57/AraHalluEval">https://github.com/aishaalansari57/AraHalluEval</a>处获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04656v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文首次全面评估了阿拉伯文和跨语言的大型语言模型（LLMs）在生成问题回答（GQA）和摘要生成两个关键任务中的幻觉现象。研究涉及总计12种LLMs，包括4种阿拉伯预训练模型、4种跨语言模型和4种基于推理的模型。文章制定了精细的幻觉评估框架，包含代表每个任务不同特点的12个精细指标，用于评估LLMs输出的真实性和可靠性。结果发现，在所有模型和任务中，事实幻觉普遍高于真实错误。尤其是阿拉伯预训练模型“Allam”的幻觉率较低，与基于推理的模型表现相当。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章首次全面评估了阿拉伯文和跨语言的大型语言模型（LLMs）在阿拉伯语境中的幻觉现象。</li>
<li>研究涉及了多种不同的LLMs模型，包括阿拉伯预训练模型、跨语言模型和基于推理的模型。</li>
<li>制定了精细的幻觉评估框架，用于评估LLMs在关键任务中的表现。</li>
<li>评估结果显示事实幻觉在所有模型和任务中普遍存在。</li>
<li>阿拉伯预训练模型“Allam”的幻觉率较低，表现良好。</li>
<li>与基于推理的模型相比，“Allam”模型具有相对较好的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04656">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04656v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04656v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.04656v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="UI-TARS-2-Technical-Report-Advancing-GUI-Agent-with-Multi-Turn-Reinforcement-Learning"><a href="#UI-TARS-2-Technical-Report-Advancing-GUI-Agent-with-Multi-Turn-Reinforcement-Learning" class="headerlink" title="UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn   Reinforcement Learning"></a>UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn   Reinforcement Learning</h2><p><strong>Authors:Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, Qinyu Luo, Shihao Liang, Shijue Huang, Wanjun Zhong, Yining Ye, Yujia Qin, Yuwen Xiong, Yuxin Song, Zhiyong Wu, Aoyan Li, Bo Li, Chen Dun, Chong Liu, Daoguang Zan, Fuxing Leng, Hanbin Wang, Hao Yu, Haobin Chen, Hongyi Guo, Jing Su, Jingjia Huang, Kai Shen, Kaiyu Shi, Lin Yan, Peiyao Zhao, Pengfei Liu, Qinghao Ye, Renjie Zheng, Shulin Xin, Wayne Xin Zhao, Wen Heng, Wenhao Huang, Wenqian Wang, Xiaobo Qin, Yi Lin, Youbin Wu, Zehui Chen, Zihao Wang, Baoquan Zhong, Xinchun Zhang, Xujing Li, Yuanfan Li, Zhongkai Zhao, Chengquan Jiang, Faming Wu, Haotian Zhou, Jinlin Pang, Li Han, Qi Liu, Qianli Ma, Siyao Liu, Songhua Cai, Wenqi Fu, Xin Liu, Yaohui Wang, Zhi Zhang, Bo Zhou, Guoliang Li, Jiajun Shi, Jiale Yang, Jie Tang, Li Li, Qihua Han, Taoran Lu, Woyu Lin, Xiaokang Tong, Xinyao Li, Yichi Zhang, Yu Miao, Zhengxuan Jiang, Zili Li, Ziyuan Zhao, Chenxin Li, Dehua Ma, Feng Lin, Ge Zhang, Haihua Yang, Hangyu Guo, Hongda Zhu, Jiaheng Liu, Junda Du, Kai Cai, Kuanye Li, Lichen Yuan, Meilan Han, Minchao Wang, Shuyue Guo, Tianhao Cheng, Xiaobo Ma, Xiaojun Xiao, Xiaolong Huang, Xinjie Chen, Yidi Du, Yilin Chen, Yiwen Wang, Zhaojian Li, Zhenzhu Yang, Zhiyuan Zeng, Chaolin Jin, Chen Li, Hao Chen, Haoli Chen, Jian Chen, Qinghao Zhao, Guang Shi</strong></p>
<p>The development of autonomous agents for graphical user interfaces (GUIs) presents major challenges in artificial intelligence. While recent advances in native agent models have shown promise by unifying perception, reasoning, action, and memory through end-to-end learning, open problems remain in data scalability, multi-turn reinforcement learning (RL), the limitations of GUI-only operation, and environment stability. In this technical report, we present UI-TARS-2, a native GUI-centered agent model that addresses these challenges through a systematic training methodology: a data flywheel for scalable data generation, a stabilized multi-turn RL framework, a hybrid GUI environment that integrates file systems and terminals, and a unified sandbox platform for large-scale rollouts. Empirical evaluation demonstrates that UI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5. On GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on WindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines such as Claude and OpenAI agents. In game environments, it attains a mean normalized score of 59.8 across a 15-game suite-roughly 60% of human-level performance-and remains competitive with frontier proprietary models (e.g., OpenAI o3) on LMGame-Bench. Additionally, the model can generalize to long-horizon information-seeking tasks and software engineering benchmarks, highlighting its robustness across diverse agent tasks. Detailed analyses of training dynamics further provide insights into achieving stability and efficiency in large-scale agent RL. These results underscore UI-TARS-2’s potential to advance the state of GUI agents and exhibit strong generalization to real-world interactive scenarios. </p>
<blockquote>
<p>图形用户界面（GUI）自主代理技术的发展在人工智能领域带来了重大挑战。尽管最近在原生代理模型方面的进展通过端到端学习将感知、推理、动作和记忆统一起来而显示出希望，但在数据可扩展性、多轮强化学习（RL）、仅基于GUI的操作局限性以及环境稳定性等方面仍存在开放性问题。在本技术报告中，我们提出了UI-TARS-2，这是一种以GUI为中心的代理模型，通过系统的训练方法解决这些挑战：用于可扩展数据生成的数据飞轮、稳定的多轮RL框架、集成文件系统和终端的混合GUI环境以及用于大规模推出的统一沙箱平台。经验评估表明，UI-TARS-2在其前身UI-TARS-1.5的基础上取得了显著改进。在GUI基准测试中，它在Online-Mind2Web上达到88.2，在OSWorld上达到47.5，在WindowsAgentArena上达到50.6，在AndroidWorld上达到73.3，超越了Claude和OpenAI代理等强劲基准线。在游戏环境中，它在15款游戏套件中的平均归一化得分为59.8——约为人类性能的60%，并且在LMGame-Bench上与前沿专有模型（例如OpenAI o3）保持竞争力。此外，该模型可以推广到长周期信息搜索任务和软件工程基准测试，突显其在各种代理任务中的稳健性。对训练动态的详细分析进一步提供了在大型代理RL中实现稳定性和效率的见解。这些结果强调了UI-TARS-2在推进GUI代理状态方面的潜力，并表现出对现实世界交互场景的强烈泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02544v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>在图形用户界面（GUI）的自主代理开发领域，人工智能面临着重大挑战。尽管基于端到端学习的原生代理模型在统一感知、推理、行动和记忆方面展现出前景，但仍存在数据可扩展性、多回合强化学习（RL）的稳定性、GUI操作的局限性以及环境稳定性等问题。本技术报告介绍了UI-TARS-2，一种以GUI为中心的原生代理模型，它通过系统化的训练方法解决这些挑战：用于可扩展数据生成的“数据飞轮”、稳定的多回合RL框架、集成文件系统和终端的混合GUI环境以及用于大规模展开的统一沙盘平台。实证评估表明，UI-TARS-2相较于其前身UI-TARS-1.5取得了显著改进。在GUI基准测试中，它在Online-Mind2Web上达到88.2，OSWorld上达到47.5，WindowsAgentArena上达到50.6，AndroidWorld上达到73.3，超越了Claude和OpenAI等强大基线代理。在游戏环境中，它在包含多个游戏的套件中平均标准化得分达到平均水平的约六成左右，并在LMGame-Bench上与前沿专有模型（如OpenAI o3）保持竞争力。此外，该模型能够泛化到长周期信息搜索任务和软件工程基准测试，表现出在多样化代理任务中的稳健性。对训练动态的深入分析进一步提供了实现大规模代理RL的稳定性和效率的策略。这些结果凸显了UI-TARS-2在推进GUI代理状态方面的潜力及其在现实世界交互场景中的强大泛化能力。 </p>
<p><strong>关键见解</strong></p>
<ol>
<li>GUI自主代理开发在人工智能领域面临重大挑战，如数据可扩展性、多回合强化学习稳定性等。</li>
<li>UI-TARS-2模型通过系统化的训练方法解决这些挑战，包括数据生成、稳定的多回合RL框架等。</li>
<li>UI-TARS-2在多个GUI基准测试中表现优异，显著超越先前模型。</li>
<li>在游戏环境中，UI-TARS-2展现出强大的性能，平均得分接近人类水平的六成左右。</li>
<li>该模型能泛化到长周期信息搜索任务和软件工程基准测试，表明其在多样化任务中的稳健性。</li>
<li>详细分析提供了实现大规模代理RL稳定性和效率的策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02544">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.02544v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.02544v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.02544v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.02544v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="R-4B-Incentivizing-General-Purpose-Auto-Thinking-Capability-in-MLLMs-via-Bi-Mode-Annealing-and-Reinforce-Learning"><a href="#R-4B-Incentivizing-General-Purpose-Auto-Thinking-Capability-in-MLLMs-via-Bi-Mode-Annealing-and-Reinforce-Learning" class="headerlink" title="R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs   via Bi-Mode Annealing and Reinforce Learning"></a>R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs   via Bi-Mode Annealing and Reinforce Learning</h2><p><strong>Authors:Qi Yang, Bolin Ni, Shiming Xiang, Han Hu, Houwen Peng, Jie Jiang</strong></p>
<p>Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking capabilities have demonstrated remarkable performance on complex reasoning problems. However, this thinking process is redundant for simple problems solvable without complex reasoning. To address this inefficiency, we propose R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on problem complexity. The central idea of R-4B is to empower the model with both thinking and non-thinking capabilities using bi-mode annealing, and apply Bi-mode Policy Optimization (BPO) to improve the model’s accuracy in determining whether to activate the thinking process. Specifically, we first train the model on a carefully curated dataset spanning various topics, which contains samples from both thinking and non-thinking modes. Then it undergoes a second phase of training under an improved GRPO framework, where the policy model is forced to generate responses from both modes for each input query. Experimental results show that R-4B achieves state-of-the-art performance across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks and achieves performance comparable to larger models such as Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower computational cost. </p>
<blockquote>
<p>配备有逐步思考能力（step-by-step thinking capabilities）的多模态大型语言模型（MLLMs）在复杂的推理问题上表现出了显著的性能。然而，对于不需要复杂推理就能解决的简单问题，这种思考过程是冗余的。为了解决这种低效问题，我们提出了R-4B，一种能够自适应判断何时思考的自思考MLLM模型。R-4B的核心思想是利用双模式退火技术赋予模型思考和不用思考的能力，并通过双模式策略优化（BPO）提高模型在决定是否需要启动思考过程时的准确性。具体来说，我们首先在一个精心挑选的涵盖各种主题的数据集上训练模型，该数据集包含来自思考和不用思考两种模式的样本。然后在改进后的GRPO框架下进入第二阶段训练，在这一阶段，策略模型必须为每个输入查询生成两种模式的答案。实验结果表明，R-4B在25项具有挑战性的基准测试中达到了最新水平，它在大多数任务中超越了Qwen2.5-VL-7B的表现，在推理密集型基准测试中达到了与更大模型（如Kimi-VL-A3B-Thinking-2506的（16B））相当的性能表现，同时降低了计算成本。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21113v2">PDF</a> 20 pages, 14 figures, 5 tables</p>
<p><strong>Summary</strong><br>多模态大型语言模型在复杂推理问题上表现出显著性能。但它们在解决无需复杂推理的简单问题时存在冗余思考过程。为解决这一问题，我们提出了自适应思考的模型R-4B，它能根据问题复杂度来决定何时思考。R-4B的核心思想是利用双模式退火技术赋予模型思考和非思考能力，并通过双模式策略优化（BPO）提高模型决定何时启动思考过程的准确性。实验结果显示，R-4B在25项具有挑战性的基准测试中取得了最新技术成果，并在大多数任务中优于Qwen2.5-VL-7B模型，同时在推理密集型基准测试上的性能与更大的模型如Kimi-VL-A3B-Thinking-2506（16B）相当，但计算成本更低。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型在复杂推理问题上表现优秀。</li>
<li>R-4B模型能根据问题复杂度自适应决定何时思考。</li>
<li>R-4B模型通过双模式退火技术实现思考和思考能力的切换。</li>
<li>双模式策略优化（BPO）提高了模型决定启动思考过程的准确性。</li>
<li>R-4B模型在多个基准测试中表现优异，优于某些现有模型。</li>
<li>R-4B模型的性能与更大模型相当，但计算成本更低。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21113">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.21113v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.21113v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.21113v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.21113v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.21113v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.21113v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.21113v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MedGR-2-Breaking-the-Data-Barrier-for-Medical-Reasoning-via-Generative-Reward-Learning"><a href="#MedGR-2-Breaking-the-Data-Barrier-for-Medical-Reasoning-via-Generative-Reward-Learning" class="headerlink" title="MedGR$^2$: Breaking the Data Barrier for Medical Reasoning via   Generative Reward Learning"></a>MedGR$^2$: Breaking the Data Barrier for Medical Reasoning via   Generative Reward Learning</h2><p><strong>Authors:Weihai Zhi, Jiayan Guo, Shangyang Li</strong></p>
<p>The application of Vision-Language Models (VLMs) in medicine is critically hampered by the scarcity of high-quality, expert-annotated data. Supervised Fine-Tuning (SFT) on existing datasets often leads to poor generalization on unseen modalities and tasks, while Reinforcement Learning (RL), a promising alternative, is stymied by the lack of reliable reward signals in this data-scarce domain. To break this impasse, we introduce Generative Reward Learning for Medical Reasoning (MedGR$^2$), a novel framework that creates a self-improving virtuous cycle. MedGR$^2$ co-develops a data generator and a reward model, enabling the automated, continuous creation of high-quality, multi-modal medical data that serves as both a superior training source for SFT and RL. Our experiments demonstrate that SFT with MedGR$^2$-produced data already surpasses baselines trained on large-scale, human-curated datasets. Crucially, when leveraging this data for RL via Group Relative Policy Optimization (GRPO), our model achieves state-of-the-art cross-modality and cross-task generalization, significantly outperforming specialized RL-based methods. Furthermore, our compact model, empowered by MedGR$^2$, achieves performance competitive with foundation models possessing over 10 times more parameters. MedGR$^2$ presents a new paradigm for data-efficient learning in high-stakes domains, transforming the problem from data scarcity to data generation and unlocking the full potential of RL for building truly generalizable medical AI. </p>
<blockquote>
<p>在医学领域应用视觉语言模型（VLMs）严重受限于高质量、专家标注数据的稀缺性。在现有数据集上进行的有监督微调（SFT）往往导致在未见过的新模态和任务上泛化能力较差，而强化学习（RL）作为一种有前途的替代方案，由于在这个数据稀缺领域缺乏可靠的奖励信号而受到阻碍。为了打破这一僵局，我们引入了医疗推理的生成奖励学习（MedGR²）这一新型框架，它创造了一个自我改进的良好循环。MedGR²共同开发了一个数据生成器和奖励模型，能够实现高质量、多模态医疗数据的自动化、连续创建，这些数据既可作为有监督微调（SFT）的优质训练源，也可作为强化学习（RL）的训练源。我们的实验表明，使用MedGR²生成的数据进行有监督微调已经超越了在大规模人工整理数据集上训练的基线。关键的是，通过利用群体相对策略优化（GRPO）对此数据进行强化学习，我们的模型实现了跨模态和跨任务的最新最先进的泛化能力，显著优于专门的基于RL的方法。此外，我们的紧凑模型在MedGR²的支持下，性能与拥有超过十倍参数的基础模型相竞争。MedGR²为高风险领域的数据高效学习提供了新的范式，从数据稀缺性转变为数据生成，并释放了强化学习在构建真正通用医疗人工智能方面的全部潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20549v1">PDF</a> 8 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>该文介绍了在医学领域应用视觉语言模型（VLMs）面临的挑战，包括高质量专家标注数据的稀缺性。现有的监督微调（SFT）方法往往无法很好地适应未见过的模态和任务。为解决这些问题，提出了一种名为MedGR²的新型框架，该框架能够共同开发数据生成器和奖励模型，实现高质量多模态医学数据的自动连续生成。实验表明，使用MedGR²生成的数据进行SFT已经超越了基线水平。当利用此数据进行强化学习时，通过集团相对策略优化（GRPO），模型实现了跨模态和跨任务的最佳泛化性能，显著优于专门的RL方法。此外，由MedGR²赋能的紧凑模型实现了与拥有超过十倍参数的基础模型相竞争的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedGR²框架解决了医学领域应用视觉语言模型（VLMs）时高质量专家标注数据稀缺的问题。</li>
<li>现有的监督微调（SFT）方法在新模态和任务上泛化能力有限。</li>
<li>MedGR²能够共同开发数据生成器和奖励模型，实现高质量多模态医学数据的自动连续生成。</li>
<li>使用MedGR²生成的数据进行SFT已超越基线水平。</li>
<li>结合MedGR²和强化学习（RL）通过集团相对策略优化（GRPO）实现了跨模态和跨任务的最佳泛化性能。</li>
<li>MedGR²赋能的紧凑模型性能与大型基础模型相竞争。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20549">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.20549v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.20549v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.20549v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.20549v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.20549v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.20549v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AI-SearchPlanner-Modular-Agentic-Search-via-Pareto-Optimal-Multi-Objective-Reinforcement-Learning"><a href="#AI-SearchPlanner-Modular-Agentic-Search-via-Pareto-Optimal-Multi-Objective-Reinforcement-Learning" class="headerlink" title="AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal   Multi-Objective Reinforcement Learning"></a>AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal   Multi-Objective Reinforcement Learning</h2><p><strong>Authors:Lang Mei, Zhihan Yang, Chong Chen</strong></p>
<p>Recent studies have explored integrating Large Language Models (LLMs) with search engines to leverage both the LLMs’ internal pre-trained knowledge and external information. Specially, reinforcement learning (RL) has emerged as a promising paradigm for enhancing LLM reasoning through multi-turn interactions with search engines. However, existing RL-based search agents rely on a single LLM to handle both search planning and question-answering (QA) tasks in an end-to-end manner, which limits their ability to optimize both capabilities simultaneously. In practice, sophisticated AI search systems often employ a large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a more effective and efficient approach is to utilize a small, trainable LLM dedicated to search planning. In this paper, we propose \textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to enhance the performance of frozen QA models by focusing on search planning. Specifically, our approach introduces three key innovations: 1) Decoupling the Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to achieve the objectives. Extensive experiments on real-world datasets demonstrate that AI SearchPlanner outperforms existing RL-based search agents in both effectiveness and efficiency, while exhibiting strong generalization capabilities across diverse frozen QA models and data domains. </p>
<blockquote>
<p>最近的研究探讨了如何将大型语言模型（LLM）与搜索引擎相结合，以利用LLM的内部预训练知识和外部信息。特别是，强化学习（RL）已经成为一种有前景的模式，通过多轮与搜索引擎的交互来增强LLM的推理能力。然而，现有的基于RL的搜索代理依赖于单个LLM以端到端的方式同时处理搜索规划和问答（QA）任务，这限制了它们同时优化这两种功能的能力。在实践中，复杂的AI搜索系统通常会使用大型、固定的LLM（如GPT-4、DeepSeek-R1）来确保高质量的问答。因此，一个更有效和高效的方法是使用一个小型的、可训练的LLM专门用于搜索规划。在本文中，我们提出了\textbf{AI-SearchPlanner}，这是一种新型的强化学习框架，旨在通过专注于搜索规划来提高固定问答模型的性能。具体来说，我们的方法引入了三个关键的创新点：1）搜索规划器和生成器的架构解耦，2）搜索规划的双奖励对齐，以及3）规划和成本的帕累托优化，以实现目标。在真实数据集上的大量实验表明，AI SearchPlanner在有效性和效率方面优于现有的基于RL的搜索代理，同时在不同的固定问答模型和数据领域表现出强大的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20368v2">PDF</a> </p>
<p><strong>Summary</strong><br>     近期研究探索了将大型语言模型（LLM）与搜索引擎相结合的方法，利用LLM的内部预训练知识和外部信息。强化学习（RL）已成为增强LLM推理能力的一种有前途的模式，通过与搜索引擎的多轮交互实现。然而，现有的基于RL的搜索代理依赖于单一的LLM来同时处理搜索规划和问答任务，这限制了它们同时优化这两种功能的能力。实践中，复杂的AI搜索系统通常使用大型、固定的LLM（如GPT-4、DeepSeek-R1）来保证高质量的问答。因此，一个更有效和高效的方法是使用专注于搜索规划的小型、可训练的LLM。本文提出了名为AI-SearchPlanner的新型强化学习框架，旨在通过专注于搜索规划来提高固定问答模型的表现。实验证明，AI SearchPlanner在有效性和效率上均优于现有的基于RL的搜索代理，同时在不同的固定问答模型和数据领域表现出强大的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）与搜索引擎的结合研究旨在结合LLM的内部预训练知识和外部信息。</li>
<li>强化学习（RL）已用于增强LLM的推理能力，通过多轮交互实现。</li>
<li>现有基于RL的搜索代理使用单一LLM处理搜索规划和问答任务，存在优化限制。</li>
<li>复杂AI搜索系统依赖大型、固定的LLM来保证高质量问答。</li>
<li>AI-SearchPlanner框架通过专注于搜索规划来提高固定问答模型的表现。</li>
<li>AI SearchPlanner引入三个关键创新点：解耦搜索规划器和生成器的架构、双重奖励对齐用于搜索规划、规划效用和成本的帕累托优化。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20368">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.20368v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.20368v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.20368v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="FedReFT-Federated-Representation-Fine-Tuning-with-All-But-Me-Aggregation"><a href="#FedReFT-Federated-Representation-Fine-Tuning-with-All-But-Me-Aggregation" class="headerlink" title="FedReFT: Federated Representation Fine-Tuning with All-But-Me   Aggregation"></a>FedReFT: Federated Representation Fine-Tuning with All-But-Me   Aggregation</h2><p><strong>Authors:Fatema Siddika, Md Anwar Hossen, J. Pablo Muñoz, Tanya Roosta, Anuj Sharma, Ali Jannesari</strong></p>
<p>Parameter-efficient fine-tuning (PEFT) has attracted significant attention for adapting large pre-trained models by modifying a small subset of parameters. Recently, Representation Fine-tuning (ReFT) has emerged as an effective alternative. ReFT shifts the fine-tuning paradigm from updating model weights to directly manipulating hidden representations that capture rich semantic information, and performs better than state-of-the-art PEFTs in standalone settings. However, its application in Federated Learning (FL) remains challenging due to heterogeneity in clients’ data distributions, model capacities, and computational resources. To address these challenges, we introduce Federated Representation Fine-Tuning (FedReFT), a novel approach to fine-tune the client’s hidden representation. FedReFT applies sparse intervention layers to steer hidden representations directly, offering a lightweight and semantically rich fine-tuning alternative ideal for edge devices. However, representation-level updates are especially vulnerable to aggregation mismatch under different task heterogeneity, where naive averaging can corrupt semantic alignment. To mitigate this issue, we propose All-But-Me (ABM) aggregation, where each client receives the aggregated updates of others and partially incorporates them, enabling stable and personalized learning by balancing local focus with global knowledge. We evaluate FedReFT on commonsense reasoning, arithmetic reasoning, instruction-tuning, and GLUE, where it consistently outperforms state-of-the-art PEFT methods in FL, achieving 7x-15x higher parameter efficiency compared to leading LoRA-based approaches. </p>
<blockquote>
<p>参数高效微调（PEFT）通过修改一小部分参数来适应大型预训练模型，已经引起了人们的广泛关注。最近，表示微调（ReFT）作为一种有效的替代方法应运而生。ReFT将微调范式从更新模型权重转向直接操作隐藏表示，这些隐藏表示捕捉了丰富的语义信息，并在独立设置中表现得比最先进的PEFT更好。然而，其在联邦学习（FL）中的应用仍然具有挑战性，主要是由于客户端数据分布、模型容量和计算资源的异质性。为了解决这些挑战，我们引入了联邦表示微调（FedReFT）这一新型隐藏表示微调方法。FedReFT通过稀疏干预层直接引导隐藏表示，提供了一种轻量级且语义丰富的微调替代方案，特别适合边缘设备。然而，在任务异质性下，表示级更新特别容易受到聚合不匹配的影响，简单的平均可能会破坏语义对齐。为了缓解这个问题，我们提出了“除了我”（ABM）聚合方法，每个客户端接收其他客户端的聚合更新，并部分地融入它们，通过平衡本地焦点和全局知识来实现稳定和个人化的学习。我们在常识推理、算术推理、指令调整和GLUE上对FedReFT进行了评估，它始终在FL中超越最先进的PEFT方法，与领先的LoRA方法相比，实现了7倍至15倍更高的参数效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20295v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对联邦学习中的表示精细调整（ReFT）的新方法——Federated Representation Fine-Tuning（FedReFT）。FedReFT通过在客户端应用稀疏干预层来直接引导隐藏表示，提供了一种轻量级且语义丰富的精细调整替代方案，尤其适用于边缘设备。为解决不同任务异质性下的聚合不匹配问题，提出All-But-Me（ABM）聚合策略，使每个客户端在平衡局部焦点与全局知识的同时，实现稳定且个性化的学习。在常识推理、算术推理、指令调整和GLUE等任务上评估时，FedReFT在联邦学习中持续超越先进的参数效率微调方法，相较于领先的LoRA方法实现了7倍至15倍更高的参数效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Federated Representation Fine-Tuning (FedReFT) 是一个新的方法，用于在联邦学习中精细调整表示（ReFT）。</li>
<li>FedReFT 通过在客户端应用稀疏干预层直接引导隐藏表示，适合在边缘设备使用。</li>
<li>联邦学习中存在数据分布、模型容量和计算资源异质性的问题。</li>
<li>为解决不同任务异质性下的聚合不匹配问题，FedReFT 提出了 All-But-Me（ABM）聚合策略。</li>
<li>ABM 聚合策略使每个客户端能平衡局部焦点和全局知识，实现稳定且个性化的学习。</li>
<li>FedReFT 在多个任务上表现出优异的性能，持续超越先进的参数效率微调方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20295">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.20295v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.20295v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.20295v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.20295v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.20295v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.20295v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.20295v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Memory-R1-Enhancing-Large-Language-Model-Agents-to-Manage-and-Utilize-Memories-via-Reinforcement-Learning"><a href="#Memory-R1-Enhancing-Large-Language-Model-Agents-to-Manage-and-Utilize-Memories-via-Reinforcement-Learning" class="headerlink" title="Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize   Memories via Reinforcement Learning"></a>Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize   Memories via Reinforcement Learning</h2><p><strong>Authors:Sikuan Yan, Xiufeng Yang, Zuchao Huang, Ercong Nie, Zifeng Ding, Zonggen Li, Xiaowen Ma, Hinrich Schütze, Volker Tresp, Yunpu Ma</strong></p>
<p>Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of NLP tasks, but they remain fundamentally stateless, constrained by limited context windows that hinder long-horizon reasoning. Recent efforts to address this limitation often augment LLMs with an external memory bank, yet most existing pipelines are static and heuristic-driven, lacking any learned mechanism for deciding what to store, update, or retrieve. We present Memory-R1, a reinforcement learning (RL) framework that equips LLMs with the ability to actively manage and utilize external memory through two specialized agents: a Memory Manager that learns to perform structured memory operations, including adding, updating, deleting, or taking no operation on memory entries; and an Answer Agent that selects the most relevant entries and reasons over them to produce an answer. Both agents are fine-tuned with outcome-driven RL (PPO and GRPO), enabling adaptive memory management and utilization with minimal supervision. With as few as 152 question-answer pairs and a corresponding temporal memory bank for training, Memory-R1 outperforms the strongest existing baseline and demonstrates strong generalization across diverse question types and LLM backbones. Beyond presenting an effective approach, this work provides insights into how RL can unlock more agentic, memory-aware behavior in LLMs, pointing toward richer, more persistent reasoning systems. </p>
<blockquote>
<p>大型语言模型（LLM）在广泛的NLP任务中表现出了令人印象深刻的能力，但它们本质上是无状态的，受限于有限的上下文窗口，阻碍了长期推理。最近为解决这一局限性的努力通常是通过使用外部记忆库来增强LLM，但大多数现有管道都是静态的和启发式驱动的，缺乏任何学习机制来决定存储、更新或检索什么。我们提出了Memory-R1，这是一个强化学习（RL）框架，它为LLM提供了主动管理和利用外部记忆的能力，通过两个专用代理实现：内存管理器学习执行结构化内存操作，包括添加、更新、删除或对内存条目不执行任何操作；答案代理选择最相关的条目并对它们进行推理以产生答案。两个代理都使用以结果驱动的强化学习（PPO和GRPO）进行微调，实现自适应内存管理和利用，监督最少。仅使用152个问答对和相应的临时记忆库进行训练，Memory-R1超越了最强的现有基线，并在各种问题和LLM主干上表现出强大的泛化能力。除了提供一种有效的方法外，这项工作还揭示了强化学习如何解锁LLM中更具代理性的、具有记忆意识的行为，为更丰富、更持久的推理系统指明了方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19828v3">PDF</a> work in progress</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在多个NLP任务中表现出强大的能力，但它们本质上是无状态的，受限于短小的上下文窗口，影响长远推理。最近的研究尝试通过外部记忆库来增强LLMs的能力，但现有管道大多是静态的、启发式驱动的，缺乏学习机制来决定存储、更新或检索什么。本文提出Memory-R1，一个强化学习（RL）框架，为LLMs配备主动管理和利用外部记忆的能力，包括两个专门代理：内存管理器学习执行结构化内存操作，包括添加、更新、删除或不对内存条目进行操作；答案代理选择最相关的条目并进行推理以产生答案。两个代理均采用目标驱动RL（PPO和GRPO）进行微调，可在极少监督下实现自适应内存管理和利用。仅使用少量问答对和相应的时序记忆库进行训练，Memory-R1即可超越最强现有基线，在多种问题和LLM主干上展现出强大的泛化能力。本文不仅提供了一种有效的方法，还揭示了强化学习如何解锁LLMs中更智能、记忆感知的行为，指向更丰富、更持久的推理系统。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在处理长远推理时受限于上下文窗口。</li>
<li>现有增强LLMs的方法大多采用静态、启发式驱动的方式，缺乏学习机制来决定内存管理。</li>
<li>Memory-R1是一个强化学习框架，为LLMs配备主动管理和利用外部记忆的能力。</li>
<li>Memory-R1包括两个专门代理：内存管理器和答案代理，分别负责结构化内存操作和答案生成。</li>
<li>Memory-R1通过目标驱动RL进行微调，实现自适应内存管理和利用。</li>
<li>仅使用少量问答对进行训练，Memory-R1即可超越现有最强基线，展现出强大的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19828">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.19828v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.19828v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.19828v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.19828v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.19828v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Understanding-Tool-Integrated-Reasoning"><a href="#Understanding-Tool-Integrated-Reasoning" class="headerlink" title="Understanding Tool-Integrated Reasoning"></a>Understanding Tool-Integrated Reasoning</h2><p><strong>Authors:Heng Lin, Zhongwen Xu</strong></p>
<p>We study why Tool-Integrated Reasoning (TIR) makes Large Language Models (LLMs) more capable. While LLMs integrated with tools like Python code interpreters show great promise, a principled theory explaining why this paradigm is effective has been missing. This work provides the first formal proof that TIR fundamentally expands an LLM’s capabilities. We demonstrate that tools enable a strict expansion of the model’s empirical and feasible support, breaking the capability ceiling of pure-text models by unlocking problem-solving strategies that are otherwise impossible or intractably verbose. To guide model behavior without compromising training stability and performance, we also introduce Advantage Shaping Policy Optimization (ASPO), a novel algorithm that directly modifies the advantage function to guide the policy behavior. We conduct comprehensive experiments on challenging mathematical benchmarks, leveraging a Python interpreter as the external tool. Our results show that the TIR model decisively outperforms its pure-text counterpart on the pass@k metric. Crucially, this advantage is not confined to computationally-intensive problems but extends to those requiring significant abstract insight. We further identify the emergent cognitive patterns that illustrate how models learn to think with tools. Finally, we report improved tool usage behavior with early code invocation and much more interactive turns with ASPO. Overall, our work provides the first principled explanation for TIR’s success, shifting the focus from the mere fact that tools work to why and how they enable more powerful reasoning. </p>
<blockquote>
<p>我们研究为什么工具集成推理（TIR）能够使大型语言模型（LLM）更具能力。虽然像Python代码解释器等工具与LLM的集成展现出巨大的潜力，但缺乏一个原则性的理论来解释这一模式为何有效。这项工作提供了首个正式证明，证明TIR从根本上扩展了LLM的能力。我们证明，工具使模型的实证和可行支持得到了严格的扩展，通过解锁问题解决方案策略，打破了纯文本模型的能力上限，否则这些策略是不可能或极其冗长的。为了在不损害训练稳定性和性能的情况下引导模型行为，我们还引入了优势塑形策略优化（ASPO），这是一种直接修改优势函数以引导策略行为的新型算法。我们在具有挑战性的数学基准测试上进行了全面的实验，利用Python解释器作为外部工具。结果表明，在k指标方面，TIR模型显著优于纯文本模型。关键的是，这一优势不仅限于计算密集型问题，而且扩展到了需要重大抽象洞察力的问题。我们进一步确定了新兴的认知模式，说明了模型如何使用工具进行思维。最后，我们报告了使用ASP0的早期代码调用改善了工具使用行为，以及与更多交互式回合的互动。总的来说，我们的工作提供了对TIR成功的首次原则性解释，将重点从工具工作的简单事实转向为什么以及如何使它们能够更强大的推理能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19201v1">PDF</a> </p>
<p><strong>Summary</strong><br>在结合了工具（如Python解释器）的大型语言模型（LLMs）中，我们研究工具集成推理（TIR）为什么能够增强模型的能力。这项工作提供了第一个正式证明，证明TIR从根本上扩大了LLM的能力。工具的使用使得模型能够解决之前无法实现或难以表达的问题解决策略，突破了纯文本模型的能力上限。我们提出了一种新的算法优势塑形策略优化（ASPO），该算法通过直接修改优势函数来指导模型行为，同时不损害训练稳定性和性能。实验结果表明，在具有挑战性的数学基准测试中，TIR模型在pass@k指标上明显超越了纯文本模型。关键的是，这种优势不仅限于计算密集型问题，而且扩展到了需要大量抽象洞察力的问题。我们的工作提供了对TIR成功的首次原则性解释，将重点从工具工作的单纯事实转向工具和如何使推理更强大的原因和方式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TIR能够增强LLM的能力：研究表明工具集成推理（TIR）可以使大型语言模型（LLMs）更加强大，这一发现有着重大的理论和实践意义。</li>
<li>TIR扩大了LLM的可行性和实证支持：工具的使用使得LLM能够解决更复杂的问题，突破了纯文本模型的能力限制。</li>
<li>ASPO算法用于指导模型行为：介绍了一种新的算法优势塑形策略优化（ASPO），它可以在不损害训练稳定性和性能的情况下，直接修改优势函数来指导模型行为。</li>
<li>TIR模型在基准测试中表现优异：在具有挑战性的数学基准测试中，结合工具的模型（如Python解释器）显著优于纯文本模型。</li>
<li>TIR的优势不仅限于计算密集型问题：除了计算密集型问题，这种优势还体现在需要抽象洞察力的问题解决上。</li>
<li>TIR的成功得到了原则性解释：我们的工作提供了对工具集成推理（TIR）成功的首次原则性解释，深入探讨了其背后的原理和机制。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19201">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.19201v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.19201v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="STARec-An-Efficient-Agent-Framework-for-Recommender-Systems-via-Autonomous-Deliberate-Reasoning"><a href="#STARec-An-Efficient-Agent-Framework-for-Recommender-Systems-via-Autonomous-Deliberate-Reasoning" class="headerlink" title="STARec: An Efficient Agent Framework for Recommender Systems via   Autonomous Deliberate Reasoning"></a>STARec: An Efficient Agent Framework for Recommender Systems via   Autonomous Deliberate Reasoning</h2><p><strong>Authors:Chenghao Wu, Ruiyang Ren, Junjie Zhang, Ruirui Wang, Zhongrui Ma, Qi Ye, Wayne Xin Zhao</strong></p>
<p>While modern recommender systems are instrumental in navigating information abundance, they remain fundamentally limited by static user modeling and reactive decision-making paradigms. Current large language model (LLM)-based agents inherit these shortcomings through their overreliance on heuristic pattern matching, yielding recommendations prone to shallow correlation bias, limited causal inference, and brittleness in sparse-data scenarios. We introduce STARec, a slow-thinking augmented agent framework that endows recommender systems with autonomous deliberative reasoning capabilities. Each user is modeled as an agent with parallel cognitions: fast response for immediate interactions and slow reasoning that performs chain-of-thought rationales. To cultivate intrinsic slow thinking, we develop anchored reinforcement training - a two-stage paradigm combining structured knowledge distillation from advanced reasoning models with preference-aligned reward shaping. This hybrid approach scaffolds agents in acquiring foundational capabilities (preference summarization, rationale generation) while enabling dynamic policy adaptation through simulated feedback loops. Experiments on MovieLens 1M and Amazon CDs benchmarks demonstrate that STARec achieves substantial performance gains compared with state-of-the-art baselines, despite using only 0.4% of the full training data. </p>
<blockquote>
<p>尽管现代推荐系统在信息丰富性导航中发挥了重要作用，但它们仍然受到静态用户建模和反应式决策范式的根本性限制。当前基于大型语言模型的代理通过过度依赖启发式模式匹配继承了这些缺点，导致推荐结果容易出现浅相关性偏见、有限的因果推理以及在数据稀疏场景中的脆弱性。我们引入了STARec，这是一种慢思考增强代理框架，为推荐系统赋予自主推理能力。每个用户都被建模为一个拥有并行认知的代理：快速响应即时交互和慢速推理，进行链式思维推理。为了培养内在慢思考，我们开发了锚定强化训练——一个两阶段范式，结合从高级推理模型进行结构化知识蒸馏和偏好对齐奖励塑造。这种混合方法使代理在获取基础能力（偏好总结、理由生成）的同时，通过模拟反馈循环实现动态策略适应。在MovieLens 1M和亚马逊CD基准测试上的实验表明，STARec与最新基准相比实现了显著的性能提升，尽管只使用了全部训练数据的0.4%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18812v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>面对信息爆炸的时代，推荐系统在帮助人们导航信息方面发挥了关键作用，但仍受限于静态用户建模和反应式决策范式。基于大型语言模型的推荐代理也面临着启发式模式匹配的缺陷，导致推荐结果易受表面相关性偏见、有限的因果推断和稀疏数据场景中的脆弱性影响。为解决这些问题，本文提出了STARec框架，为推荐系统赋予自主推理能力。通过模拟用户的并行认知过程，STARec实现了快速响应即时交互和慢思考进行推理的能力。通过锚定强化训练和模拟反馈循环，STARec在获取基础能力的同时，实现了动态策略调整。实验结果表明，即使在训练数据量仅为全量的0.4%的情况下，STARec相较于现有先进技术仍取得了显著的性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>推荐系统在现代信息导航中至关重要，但仍受静态用户建模和反应式决策的限制。</li>
<li>基于大型语言模型的推荐代理存在启发式模式匹配的缺陷，导致推荐易受表面相关性偏见影响。</li>
<li>STARec框架通过模拟用户并行认知过程，融合了快速响应和慢思考推理能力。</li>
<li>锚定强化训练结合结构化知识蒸馏和偏好对齐奖励塑形，培养代理的内在慢思考能力。</li>
<li>STARec通过模拟反馈循环实现了动态策略调整。</li>
<li>实验结果表明，STARec在资源有限的情况下仍显著优于现有技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18812">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.18812v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.18812v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.18812v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.18812v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.18812v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="ThinkDial-An-Open-Recipe-for-Controlling-Reasoning-Effort-in-Large-Language-Models"><a href="#ThinkDial-An-Open-Recipe-for-Controlling-Reasoning-Effort-in-Large-Language-Models" class="headerlink" title="ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large   Language Models"></a>ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large   Language Models</h2><p><strong>Authors:Qianyu He, Siyu Yuan, Xuefeng Li, Mingxuan Wang, Jiangjie Chen</strong></p>
<p>Large language models (LLMs) with chain-of-thought reasoning have demonstrated remarkable problem-solving capabilities, but controlling their computational effort remains a significant challenge for practical deployment. Recent proprietary systems like OpenAI’s gpt-oss series have introduced discrete operational modes for intuitive reasoning control, but the open-source community has largely failed to achieve such capabilities. In this paper, we introduce ThinkDial, the first open-recipe end-to-end framework that successfully implements gpt-oss-style controllable reasoning through discrete operational modes. Our system enables seamless switching between three distinct reasoning regimes: High mode (full reasoning capability), Medium mode (50 percent token reduction with &lt;10 percent performance degradation), and Low mode (75 percent token reduction with &lt;15 percent performance degradation). We achieve this through an end-to-end training paradigm that integrates budget-mode control throughout the entire pipeline: budget-mode supervised fine-tuning that embeds controllable reasoning capabilities directly into the learning process, and two-phase budget-aware reinforcement learning with adaptive reward shaping. Extensive experiments demonstrate that ThinkDial achieves target compression-performance trade-offs with clear response length reductions while maintaining performance thresholds. The framework also exhibits strong generalization capabilities on out-of-distribution tasks. </p>
<blockquote>
<p>带有思维链推理的大型语言模型（LLMs）已经展现出卓越的解决问题能力，但在实际控制其计算工作量方面仍存在重大挑战，这对于实际部署具有重要意义。像OpenAI的GPT-OSS系列等最近的专有系统已经引入了用于直观推理控制的离散操作模式，但开源社区在很大程度上未能实现此类功能。在本文中，我们介绍了ThinkDial，它是第一个成功实现通过离散操作模式进行GPT-OSS风格可控推理的开源端到端框架。我们的系统能够在三种不同的推理模式之间进行无缝切换：高级模式（具备完全推理能力）、中级模式（令牌减少50%，性能降低不到10%）和低级模式（令牌减少7 结，性能降低不到15%）。我们通过一种端到端的训练范式实现了这一点，该范式在整个管道中集成了预算模式控制：预算模式监督微调将可控推理能力直接嵌入学习过程，以及具有自适应奖励塑造的两阶段预算感知强化学习。大量实验表明，ThinkDial实现了目标压缩与性能的权衡，在减少响应长度的同时保持性能阈值。该框架在超出分布的任务中也表现出强大的泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18773v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）具备出色的解决问题的能力，但控制其计算工作量仍是实际应用中的一大挑战。本文介绍了ThinkDial，首个成功实现类似OpenAI gpt-oss系列的可控推理的开源端到端框架。该框架通过离散操作模式，能够在三种不同的推理模式之间进行无缝切换：高性能模式、中等性能模式和低性能模式。这通过一种端到端的训练范式实现，该范式在整个管道中融入预算模式控制，包括预算模式监督微调，将可控推理能力直接嵌入学习过程，以及带有自适应奖励塑造的两阶段预算感知强化学习。实验表明，ThinkDial能够在减少响应长度的同时实现目标压缩性能折衷，并在跨分布任务上展现出强大的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在解决问题方面表现出色，但计算工作量的控制仍是实际应用中的难题。</li>
<li>ThinkDial是首个成功实现类似OpenAI gpt-oss系列的可控推理的开源端到端框架。</li>
<li>ThinkDial支持三种不同的推理模式：高性能模式、中等性能模式和低性能模式。</li>
<li>ThinkDial通过预算模式控制和端到端的训练范式实现性能优化。</li>
<li>该框架实现了预算模式监督微调和两阶段预算感知强化学习，以融入预算模式控制。</li>
<li>实验表明ThinkDial能够在减少响应长度的同时实现目标压缩性能折衷。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18773">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.18773v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.18773v1/page_1_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="OwlCap-Harmonizing-Motion-Detail-for-Video-Captioning-via-HMD-270K-and-Caption-Set-Equivalence-Reward"><a href="#OwlCap-Harmonizing-Motion-Detail-for-Video-Captioning-via-HMD-270K-and-Caption-Set-Equivalence-Reward" class="headerlink" title="OwlCap: Harmonizing Motion-Detail for Video Captioning via HMD-270K and   Caption Set Equivalence Reward"></a>OwlCap: Harmonizing Motion-Detail for Video Captioning via HMD-270K and   Caption Set Equivalence Reward</h2><p><strong>Authors:Chunlin Zhong, Qiuxia Hou, Zhangjun Zhou, Shuang Hao, Haonan Lu, Yanhao Zhang, He Tang, Xiang Bai</strong></p>
<p>Video captioning aims to generate comprehensive and coherent descriptions of the video content, contributing to the advancement of both video understanding and generation. However, existing methods often suffer from motion-detail imbalance, as models tend to overemphasize one aspect while neglecting the other. This imbalance results in incomplete captions, which in turn leads to a lack of consistency in video understanding and generation. To address this issue, we propose solutions from two aspects: 1) Data aspect: We constructed the Harmonizing Motion-Detail 270K (HMD-270K) dataset through a two-stage pipeline: Motion-Detail Fusion (MDF) and Fine-Grained Examination (FGE). 2) Optimization aspect: We introduce the Caption Set Equivalence Reward (CSER) based on Group Relative Policy Optimization (GRPO). CSER enhances completeness and accuracy in capturing both motion and details through unit-to-set matching and bidirectional validation. Based on the HMD-270K supervised fine-tuning and GRPO post-training with CSER, we developed OwlCap, a powerful video captioning multi-modal large language model (MLLM) with motion-detail balance. Experimental results demonstrate that OwlCap achieves significant improvements compared to baseline models on two benchmarks: the detail-focused VDC (+4.2 Acc) and the motion-focused DREAM-1K (+4.6 F1). The HMD-270K dataset and OwlCap model will be publicly released to facilitate video captioning research community advancements. </p>
<blockquote>
<p>视频字幕生成旨在生成对视频内容的全面和连贯的描述，促进视频理解和生成的发展。然而，现有方法常常存在运动细节失衡的问题，模型往往过分强调一方面而忽视另一方面。这种不平衡导致字幕不完整，进而造成视频理解和生成的不一致性。为了解决这一问题，我们从两个方面提出了解决方案：1）数据方面：我们通过两阶段管道——运动细节融合（MDF）和精细颗粒度检查（FGE）——构建了和谐运动细节27万（HMD-270K）数据集。2）优化方面：我们引入了基于群体相对策略优化（GRPO）的字幕集等价奖励（CSER）。CSER通过单元到集合的匹配和双向验证，提高了捕捉运动和细节的完整性和准确性。基于HMD-270K的监督微调以及GRPO的后续训练和CSER，我们开发了一款强大的视频字幕多模态大型语言模型OwlCap，实现了运动细节的均衡。实验结果表明，OwlCap在两个基准测试上较基线模型取得了显著改进：注重细节的VDC（+4.2准确率）和注重运动的DREAM-1K（+4.6 F1）。HMD-27万数据集和OwlCap模型将公开发布，以促进视频字幕研究领域的进步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18634v2">PDF</a> 9 pages, 6figures</p>
<p><strong>摘要</strong><br>该文本介绍了视频描述生成的研究现状以及存在的挑战。为了解决现有方法中运动细节失衡的问题，该研究从数据方面和优化方面提出了解决方案，构建了HMD-270K数据集，并开发了基于该数据集的OwlCap模型，通过视频内容捕获完整且连贯的描述。该模型在VDC和DREAM-1K两个基准测试集上取得了显著的改进。数据集和模型将公开发布以促进视频描述生成研究的发展。整体工作着重于提升视频内容的理解以及视频描述的完整性及连贯性。简要概括为：“解决视频描述生成中的运动细节失衡问题，推出新数据集及模型以改进视频内容理解和描述连贯性。” </p>
<p><strong>关键见解</strong></p>
<ul>
<li>视频描述生成旨在全面连贯地描述视频内容，促进视频理解和生成的发展。</li>
<li>当前方法存在运动细节失衡的问题，导致描述不完整和缺乏一致性。</li>
<li>从数据方面，构建了HMD-270K数据集，通过运动细节融合和精细粒度检查两个阶段进行。</li>
<li>从优化方面，引入了基于组相对策略优化的字幕集等价奖励（CSER），提高捕捉运动和细节的完整性和准确性。</li>
<li>使用HMD-270K数据集进行精细调整，结合CSER的GRPO后训练，开发了平衡运动细节的OwlCap视频描述多模态大型语言模型（MLLM）。</li>
<li>实验结果表明，OwlCap在VDC和DREAM-1K两个基准测试集上较基线模型有显著改善。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18634">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.18634v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.18634v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.18634v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.18634v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.18634v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2508.18634v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-09/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-09/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-09/LLM/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04650v1/page_0_0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-09-09  Scaling Performance of Large Language Model Pretraining
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-08/TTS/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-08\./crop_TTS/2409.10969v2/page_5_2.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-09-08  Enhancing Code-switched Text-to-Speech Synthesis Capability in Large   Language Models with only Monolingual Corpora
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29885.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
