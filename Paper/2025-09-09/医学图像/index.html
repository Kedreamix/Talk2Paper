<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-09  VLSM-Ensemble Ensembling CLIP-based Vision-Language Models for Enhanced   Medical Image Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ca6e177dc7ad33e2d5a139b77a731ea6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-09-æ›´æ–°"><a href="#2025-09-09-æ›´æ–°" class="headerlink" title="2025-09-09 æ›´æ–°"></a>2025-09-09 æ›´æ–°</h1><h2 id="VLSM-Ensemble-Ensembling-CLIP-based-Vision-Language-Models-for-Enhanced-Medical-Image-Segmentation"><a href="#VLSM-Ensemble-Ensembling-CLIP-based-Vision-Language-Models-for-Enhanced-Medical-Image-Segmentation" class="headerlink" title="VLSM-Ensemble: Ensembling CLIP-based Vision-Language Models for Enhanced   Medical Image Segmentation"></a>VLSM-Ensemble: Ensembling CLIP-based Vision-Language Models for Enhanced   Medical Image Segmentation</h2><p><strong>Authors:Julia Dietlmeier, Oluwabukola Grace Adegboro, Vayangi Ganepola, Claudia Mazo, Noel E. Oâ€™Connor</strong></p>
<p>Vision-language models and their adaptations to image segmentation tasks present enormous potential for producing highly accurate and interpretable results. However, implementations based on CLIP and BiomedCLIP are still lagging behind more sophisticated architectures such as CRIS. In this work, instead of focusing on text prompt engineering as is the norm, we attempt to narrow this gap by showing how to ensemble vision-language segmentation models (VLSMs) with a low-complexity CNN. By doing so, we achieve a significant Dice score improvement of 6.3% on the BKAI polyp dataset using the ensembled BiomedCLIPSeg, while other datasets exhibit gains ranging from 1% to 6%. Furthermore, we provide initial results on additional four radiology and non-radiology datasets. We conclude that ensembling works differently across these datasets (from outperforming to underperforming the CRIS model), indicating a topic for future investigation by the community. The code is available at <a target="_blank" rel="noopener" href="https://github.com/juliadietlmeier/VLSM-Ensemble">https://github.com/juliadietlmeier/VLSM-Ensemble</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹åŠå…¶åœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„é€‚åº”æ€§è¡¨ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿäº§ç”Ÿé«˜åº¦å‡†ç¡®å’Œå¯è§£é‡Šçš„ç»“æœã€‚ç„¶è€Œï¼ŒåŸºäºCLIPå’ŒBiomedCLIPçš„å®æ–½ä»ç„¶è½åäºæ›´å…ˆè¿›çš„æ¶æ„ï¼Œå¦‚CRISã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰å…³æ³¨å¸¸è§çš„æ–‡æœ¬æç¤ºå·¥ç¨‹ï¼Œè€Œæ˜¯å°è¯•é€šè¿‡å±•ç¤ºå¦‚ä½•å°†è§†è§‰è¯­è¨€åˆ†å‰²æ¨¡å‹ï¼ˆVLSMï¼‰ä¸ä½å¤æ‚åº¦CNNç»“åˆæ¥ç¼©å°è¿™ä¸€å·®è·ã€‚é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬åœ¨BKAIæ¯è‚‰æ•°æ®é›†ä¸Šä½¿ç”¨é›†æˆçš„BiomedCLIPSegå®ç°äº†6.3%çš„Diceå¾—åˆ†æå‡ï¼Œè€Œå…¶ä»–æ•°æ®é›†åˆ™æ˜¾ç¤ºå‡ºä»1%åˆ°6%ä¸ç­‰çš„å¢ç›Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†å…³äºå…¶ä»–å››ä¸ªæ”¾å°„ç§‘å’Œéæ”¾å°„ç§‘æ•°æ®é›†çš„åˆæ­¥ç»“æœã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œé›†æˆæ¨¡å‹åœ¨è¿™äº›æ•°æ®é›†ä¸Šçš„è¡¨ç°å„ä¸ç›¸åŒï¼ˆä»è¶…è¶Šåˆ°ä½äºCRISæ¨¡å‹ï¼‰ï¼Œè¿™ä¸ºæœªæ¥ç¤¾åŒºçš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªè¯é¢˜ã€‚ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/juliadietlmeier/VLSM-Ensemble%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/juliadietlmeier/VLSM-Ensembleè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05154v1">PDF</a> Medical Imaging with Deep Learning (MIDL 2025) short paper</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„åº”ç”¨æ½œåŠ›ï¼Œé€šè¿‡é›†æˆä½å¤æ‚åº¦çš„CNNä¸è§†è§‰è¯­è¨€åˆ†å‰²æ¨¡å‹ï¼ˆVLSMsï¼‰ï¼Œå®ç°äº†åœ¨BKAIæ¯è‚‰æ•°æ®é›†ä¸ŠDiceå¾—åˆ†æé«˜6.3%çš„æ˜¾è‘—æˆæœã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶è¿˜æ¶‰åŠå››ä¸ªæ”¾å°„ç§‘å’Œéæ”¾å°„ç§‘æ•°æ®é›†çš„åˆæ­¥ç»“æœï¼Œå¹¶æŒ‡å‡ºé›†æˆæ•ˆæœåœ¨ä¸åŒæ•°æ®é›†ä¸Šå­˜åœ¨å·®å¼‚ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>é€šè¿‡é›†æˆä½å¤æ‚åº¦çš„CNNä¸è§†è§‰è¯­è¨€åˆ†å‰²æ¨¡å‹ï¼ˆVLSMsï¼‰ï¼Œå®ç°äº†åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šçš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>åœ¨BKAIæ¯è‚‰æ•°æ®é›†ä¸Šï¼Œé›†æˆçš„BiomedCLIPSegæ¨¡å‹å®ç°äº†Diceå¾—åˆ†æé«˜6.3%ã€‚</li>
<li>ç ”ç©¶æ¶‰åŠå››ä¸ªæ”¾å°„ç§‘å’Œéæ”¾å°„ç§‘æ•°æ®é›†çš„åˆæ­¥å®éªŒç»“æœã€‚</li>
<li>é›†æˆæ•ˆæœåœ¨ä¸åŒæ•°æ®é›†ä¸Šè¡¨ç°ä¸åŒï¼Œæ—¢æœ‰è¶…è¿‡CRISæ¨¡å‹çš„æƒ…å†µï¼Œä¹Ÿæœ‰æœªèƒ½è¶…è¶Šçš„æƒ…å†µã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ï¼Œå³è¿›ä¸€æ­¥æ¢ç´¢é›†æˆæ–¹æ³•åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05154">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0c253970f27d6d3b398290973fa17060.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cd2284b228d5e3386908446fea5542e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-afee9ca9aff610cbddcd9d5e291c86dd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Semi-supervised-Deep-Transfer-for-Regression-without-Domain-Alignment"><a href="#Semi-supervised-Deep-Transfer-for-Regression-without-Domain-Alignment" class="headerlink" title="Semi-supervised Deep Transfer for Regression without Domain Alignment"></a>Semi-supervised Deep Transfer for Regression without Domain Alignment</h2><p><strong>Authors:Mainak Biswas, Ambedkar Dukkipati, Devarajan Sridharan</strong></p>
<p>Deep learning models deployed in real-world applications (e.g., medicine) face challenges because source models do not generalize well to domain-shifted target data. Many successful domain adaptation (DA) approaches require full access to source data. Yet, such requirements are unrealistic in scenarios where source data cannot be shared either because of privacy concerns or because it is too large and incurs prohibitive storage or computational costs. Moreover, resource constraints may limit the availability of labeled targets. We illustrate this challenge in a neuroscience setting where source data are unavailable, labeled target data are meager, and predictions involve continuous-valued outputs. We build upon Contradistinguisher (CUDA), an efficient framework that learns a shared model across the labeled source and unlabeled target samples, without intermediate representation alignment. Yet, CUDA was designed for unsupervised DA, with full access to source data, and for classification tasks. We develop CRAFT â€“ a Contradistinguisher-based Regularization Approach for Flexible Training â€“ for source-free (SF), semi-supervised transfer of pretrained models in regression tasks. We showcase the efficacy of CRAFT in two neuroscience settings: gaze prediction with electroencephalography (EEG) data and &#96;&#96;brain ageâ€™â€™ prediction with structural MRI data. For both datasets, CRAFT yielded up to 9% improvement in root-mean-squared error (RMSE) over fine-tuned models when labeled training examples were scarce. Moreover, CRAFT leveraged unlabeled target data and outperformed four competing state-of-the-art source-free domain adaptation models by more than 3%. Lastly, we demonstrate the efficacy of CRAFT on two other real-world regression benchmarks. We propose CRAFT as an efficient approach for source-free, semi-supervised deep transfer for regression that is ubiquitous in biology and medicine. </p>
<blockquote>
<p>åœ¨ç°å®ä¸–ç•Œåº”ç”¨ï¼ˆå¦‚åŒ»å­¦ï¼‰ä¸­éƒ¨ç½²çš„æ·±åº¦å­¦ä¹ æ¨¡å‹é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºæºæ¨¡å‹ä¸èƒ½å¾ˆå¥½åœ°æ¨å¹¿åˆ°åŸŸåç§»çš„ç›®æ ‡æ•°æ®ã€‚è®¸å¤šæˆåŠŸçš„åŸŸé€‚åº”ï¼ˆDAï¼‰æ–¹æ³•éœ€è¦å®Œå…¨è®¿é—®æºæ•°æ®ã€‚ç„¶è€Œï¼Œåœ¨æºæ•°æ®å› éšç§æ‹…å¿§æˆ–æ•°æ®è¿‡å¤§å¯¼è‡´å­˜å‚¨å’Œè®¡ç®—æˆæœ¬é«˜æ˜‚è€Œæ— æ³•å…±äº«çš„æƒ…å†µä¸‹ï¼Œè¿™ç§è¦æ±‚æ˜¯ä¸ç°å®çš„ã€‚æ­¤å¤–ï¼Œèµ„æºçº¦æŸå¯èƒ½é™åˆ¶æ ‡è®°ç›®æ ‡çš„å¯ç”¨æ€§ã€‚æˆ‘ä»¬ä»¥ç¥ç»ç§‘å­¦ç¯å¢ƒä¸ºä¾‹æ¥è¯´æ˜è¿™ä¸€æŒ‘æˆ˜ï¼Œåœ¨è¯¥ç¯å¢ƒä¸­æºæ•°æ®ä¸å¯ç”¨ï¼Œæ ‡è®°çš„ç›®æ ‡æ•°æ®å¾ˆå°‘ï¼Œé¢„æµ‹æ¶‰åŠè¿ç»­å€¼è¾“å‡ºã€‚æˆ‘ä»¬åŸºäºContradistinguisherï¼ˆCUDAï¼‰æ„å»ºäº†ä¸€ä¸ªæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨æ ‡è®°æºå’Œæ— æ ‡ç­¾ç›®æ ‡æ ·æœ¬ä¸Šå­¦ä¹ å…±äº«æ¨¡å‹çš„æœ‰æ•ˆæ¡†æ¶ï¼Œæ— éœ€ä¸­é—´è¡¨ç¤ºå¯¹é½ã€‚ç„¶è€Œï¼ŒCUDAæ˜¯ä¸ºå…·æœ‰å®Œæ•´æºæ•°æ®è®¿é—®æƒé™çš„æ— ç›‘ç£DAå’Œåˆ†ç±»ä»»åŠ¡è€Œè®¾è®¡çš„ã€‚æˆ‘ä»¬å¼€å‘äº†CRAFTâ€“ä¸€ç§åŸºäºContradistinguisherçš„æ­£åˆ™åŒ–çµæ´»è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºæ— æºï¼ˆSFï¼‰åŠç›‘ç£é¢„è®­ç»ƒæ¨¡å‹çš„è¿ç§»ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªç¥ç»ç§‘å­¦ç¯å¢ƒä¸­å±•ç¤ºäº†CRAFTçš„æœ‰æ•ˆæ€§ï¼šä½¿ç”¨è„‘ç”µå›¾ï¼ˆEEGï¼‰æ•°æ®çš„ç›®å…‰é¢„æµ‹å’Œä½¿ç”¨ç»“æ„MRIæ•°æ®çš„â€œè„‘å¹´é¾„â€é¢„æµ‹ã€‚å¯¹äºè¿™ä¸¤ä¸ªæ•°æ®é›†ï¼Œå½“æ ‡è®°çš„è®­ç»ƒæ ·æœ¬ç¨€ç¼ºæ—¶ï¼Œä¸å¾®è°ƒæ¨¡å‹ç›¸æ¯”ï¼ŒCRAFTåœ¨å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰ä¸Šæœ€å¤šæé«˜äº†9%ã€‚æ­¤å¤–ï¼ŒCRAFTåˆ©ç”¨æ— æ ‡ç­¾çš„ç›®æ ‡æ•°æ®å¹¶è¶…è¶Šäº†å…¶ä»–å››ä¸ªç«äº‰æ€§çš„å‰æ²¿æ— æºåŸŸé€‚åº”æ¨¡å‹è¶…è¿‡3%ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨å¦å¤–ä¸¤ä¸ªç°å®ä¸–ç•Œå›å½’åŸºå‡†æµ‹è¯•ä¸Šè¯æ˜äº†CRAFTçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æå‡ºCRAFTä½œä¸ºä¸€ç§åœ¨æ— æºã€åŠç›‘ç£æ·±åº¦è¿ç§»ä¸­æœ‰æ•ˆçš„æ–¹æ³•ï¼Œè¿™åœ¨ç”Ÿç‰©å­¦å’ŒåŒ»å­¦ä¸­æ˜¯æ™®éå­˜åœ¨çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05092v1">PDF</a> 15 pages, 6 figures, International Conference on Computer Vision 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å®é™…åº”ç”¨ï¼ˆå¦‚åŒ»å­¦ï¼‰ä¸­é¢ä¸´çš„æŒ‘æˆ˜æ˜¯æºæ¨¡å‹ä¸èƒ½å¾ˆå¥½åœ°æ³›åŒ–åˆ°åŸŸè½¬ç§»çš„ç›®æ ‡æ•°æ®ã€‚è®¸å¤šæˆåŠŸçš„åŸŸé€‚åº”ï¼ˆDAï¼‰æ–¹æ³•éœ€è¦å®Œå…¨è®¿é—®æºæ•°æ®ï¼Œä½†åœ¨éšç§æ‹…å¿§æˆ–æºæ•°æ®è¿‡å¤§å¯¼è‡´å­˜å‚¨å’Œè®¡ç®—æˆæœ¬é«˜æ˜‚çš„æƒ…å†µä¸‹ï¼Œè¿™ç§è¦æ±‚ä¸åˆ‡å®é™…ã€‚æ­¤å¤–ï¼Œèµ„æºçº¦æŸå¯èƒ½é™åˆ¶æ ‡è®°ç›®æ ‡çš„å¯ç”¨æ€§ã€‚æˆ‘ä»¬åœ¨ç¥ç»ç§‘å­¦ç¯å¢ƒä¸­è¯´æ˜äº†è¿™ä¸€æŒ‘æˆ˜ï¼Œè¯¥ç¯å¢ƒä¸­æºæ•°æ®ä¸å¯ç”¨ï¼Œæ ‡è®°çš„ç›®æ ‡æ•°æ®å¾ˆå°‘ï¼Œé¢„æµ‹æ¶‰åŠè¿ç»­å€¼è¾“å‡ºã€‚æˆ‘ä»¬åŸºäºContradistinguisherï¼ˆCUDAï¼‰æ„å»ºäº†ä¸€ä¸ªæ¨¡å‹ï¼Œå¯ä»¥åœ¨æ ‡è®°çš„æºæ ·æœ¬å’Œæ— æ ‡è®°çš„ç›®æ ‡æ ·æœ¬ä¸Šå­¦ä¹ å…±äº«æ¨¡å‹ï¼Œè€Œæ— éœ€ä¸­é—´è¡¨ç¤ºå¯¹é½ã€‚ç„¶è€Œï¼ŒCUDAæ˜¯ä¸ºå…·æœ‰å®Œæ•´æºæ•°æ®å’Œæ— ç›‘ç£çš„DAä»¥åŠåˆ†ç±»ä»»åŠ¡è€Œè®¾è®¡çš„ã€‚æˆ‘ä»¬å¼€å‘äº†CRAFTâ€”â€”ä¸€ç§åŸºäºContradistinguisherçš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œç”¨äºæºå…è´¹çš„åŠç›‘ç£é¢„è®­ç»ƒæ¨¡å‹å›å½’ä»»åŠ¡ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªç¥ç»ç§‘å­¦ç¯å¢ƒä¸­å±•ç¤ºäº†CRAFTçš„æœ‰æ•ˆæ€§ï¼šä½¿ç”¨è„‘ç”µå›¾ï¼ˆEEGï¼‰æ•°æ®çš„ç›®å…‰é¢„æµ‹å’Œç”¨ç»“æ„MRIæ•°æ®è¿›è¡Œâ€œè„‘å¹´é¾„â€é¢„æµ‹ã€‚å¯¹äºè¿™ä¸¤ä¸ªæ•°æ®é›†ï¼Œå½“æ ‡è®°è®­ç»ƒæ ·æœ¬ç¨€ç¼ºæ—¶ï¼ŒCRAFTåœ¨å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰ä¸Šæœ€å¤šæé«˜äº†9%ï¼Œå¹¶ä¸”ç›¸è¾ƒäºä»…ä¾èµ–æ ‡ç­¾æ•°æ®çš„æ¨¡å‹å±•ç°å‡ºäº†æ›´å¼ºå¤§çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒCRAFTåˆ©ç”¨æ— æ ‡ç­¾çš„ç›®æ ‡æ•°æ®å¹¶è¶…è¶Šäº†å››ç§ç«äº‰æ€§çš„å‰æ²¿æºè‡ªç”±åŸŸé€‚åº”æ¨¡å‹è¶…è¿‡3%ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨å¦å¤–ä¸¤ä¸ªçœŸå®ä¸–ç•Œçš„å›å½’åŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†CRAFTçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬æå‡ºCRAFTæ˜¯ä¸€ç§é«˜æ•ˆçš„æºè‡ªç”±ã€åŠç›‘ç£æ·±åº¦è½¬ç§»å›å½’æ–¹æ³•ï¼Œåœ¨ç”Ÿç‰©å­¦å’ŒåŒ»å­¦ä¸­æ™®éå­˜åœ¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´åŸŸé€‚åº”æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æºæ•°æ®æ— æ³•æ³›åŒ–åˆ°ç›®æ ‡æ•°æ®æ—¶ã€‚</li>
<li>å½“å‰è®¸å¤šåŸŸé€‚åº”æ–¹æ³•è¦æ±‚å®Œå…¨è®¿é—®æºæ•°æ®ï¼Œè¿™åœ¨éšç§æˆ–æˆæœ¬çº¦æŸä¸‹ä¸åˆ‡å®é™…ã€‚</li>
<li>CRAFTåŸºäºContradistinguisheræ„å»ºï¼Œå¯åœ¨æºæ•°æ®ä¸å¯ç”¨æˆ–åŠç›‘ç£è®¾ç½®ä¸­è¿›è¡ŒåŸŸé€‚åº”ã€‚</li>
<li>CRAFTåœ¨ç¥ç»ç§‘å­¦ç¯å¢ƒä¸­ï¼ˆå¦‚EEGå’ŒMRIæ•°æ®ï¼‰è¿›è¡Œå›å½’ä»»åŠ¡æ—¶è¡¨ç°å‡ºè‰²ã€‚</li>
<li>CRAFTåœ¨æ ‡è®°è®­ç»ƒæ ·æœ¬ç¨€ç¼ºæ—¶æé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†å…¶ä»–æ–¹æ³•ã€‚</li>
<li>CRAFTåˆ©ç”¨æ— æ ‡ç­¾ç›®æ ‡æ•°æ®å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05092">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d48526bb78cd413ea55771bd88dcb40a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd055b65386c4dba8bde460e0461b7fe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-400facd8197d26638aacd5e7202810b1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Towards-Efficient-Pixel-Labeling-for-Industrial-Anomaly-Detection-and-Localization"><a href="#Towards-Efficient-Pixel-Labeling-for-Industrial-Anomaly-Detection-and-Localization" class="headerlink" title="Towards Efficient Pixel Labeling for Industrial Anomaly Detection and   Localization"></a>Towards Efficient Pixel Labeling for Industrial Anomaly Detection and   Localization</h2><p><strong>Authors:Jingqi Wu, Hanxi Li, Lin Yuanbo Wu, Hao Chen, Deyin Liu, Peng Wang</strong></p>
<p>Industrial product inspection is often performed using Anomaly Detection (AD) frameworks trained solely on non-defective samples. Although defective samples can be collected during production, leveraging them usually requires pixel-level annotations, limiting scalability. To address this, we propose ADClick, an Interactive Image Segmentation (IIS) algorithm for industrial anomaly detection. ADClick generates pixel-wise anomaly annotations from only a few user clicks and a brief textual description, enabling precise and efficient labeling that significantly improves AD model performance (e.g., AP &#x3D; 96.1% on MVTec AD). We further introduce ADClick-Seg, a cross-modal framework that aligns visual features and textual prompts via a prototype-based approach for anomaly detection and localization. By combining pixel-level priors with language-guided cues, ADClick-Seg achieves state-of-the-art results on the challenging &#96;&#96;Multi-classâ€™â€™ AD task (AP &#x3D; 80.0%, PRO &#x3D; 97.5%, Pixel-AUROC &#x3D; 99.1% on MVTec AD). </p>
<blockquote>
<p>å·¥ä¸šäº§å“æ£€æµ‹é€šå¸¸ä½¿ç”¨ä»…å¯¹éç¼ºé™·æ ·æœ¬è¿›è¡Œè®­ç»ƒçš„å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰æ¡†æ¶æ¥å®Œæˆã€‚è™½ç„¶ç”Ÿäº§è¿‡ç¨‹ä¸­å¯ä»¥æ”¶é›†ç¼ºé™·æ ·æœ¬ï¼Œä½†åˆ©ç”¨å®ƒä»¬é€šå¸¸éœ€è¦è¿›è¡Œåƒç´ çº§æ ‡æ³¨ï¼Œè¿™é™åˆ¶äº†å…¶å¯æ‰©å±•æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ADClickï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå·¥ä¸šå¼‚å¸¸æ£€æµ‹çš„äº¤äº’å¼å›¾åƒåˆ†å‰²ï¼ˆIISï¼‰ç®—æ³•ã€‚ADClickä»…é€šè¿‡å°‘é‡ç”¨æˆ·ç‚¹å‡»å’Œç®€çŸ­æ–‡æœ¬æè¿°å³å¯ç”Ÿæˆåƒç´ çº§çš„å¼‚å¸¸æ ‡æ³¨ï¼Œä»è€Œå®ç°ç²¾ç¡®é«˜æ•ˆçš„æ ‡æ³¨ï¼Œæ˜¾è‘—æé«˜äº†å¼‚å¸¸æ£€æµ‹æ¨¡å‹æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼Œåœ¨MVTec ADä¸Šçš„å¹³å‡ç²¾åº¦ä¸º96.1%ï¼‰ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ADClick-Segï¼Œè¿™æ˜¯ä¸€ä¸ªè·¨æ¨¡æ€æ¡†æ¶ï¼Œé€šè¿‡åŸºäºåŸå‹çš„æ–¹æ³•å¯¹é½è§†è§‰ç‰¹å¾å’Œæ–‡æœ¬æç¤ºï¼Œç”¨äºå¼‚å¸¸æ£€æµ‹å’Œå®šä½ã€‚é€šè¿‡å°†åƒç´ çº§å…ˆéªŒçŸ¥è¯†ä¸è¯­è¨€å¼•å¯¼çº¿ç´¢ç›¸ç»“åˆï¼ŒADClick-Segåœ¨å¤šç±»åˆ«çš„å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æˆæœï¼ˆåœ¨MVTec ADä¸Šçš„å¹³å‡ç²¾åº¦ä¸º80.0%ï¼Œå¬å›ç‡ä¸º97.5%ï¼Œåƒç´ çº§AUROCä¸º99.1%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05034v1">PDF</a> </p>
<p><strong>Summary</strong><br>å·¥ä¸šäº§å“æ£€æµ‹é€šå¸¸ä½¿ç”¨å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰æ¡†æ¶è¿›è¡Œï¼Œè¯¥æ¡†æ¶ä»…å¯¹éç¼ºé™·æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚è™½ç„¶å¯ä»¥åœ¨ç”Ÿäº§æœŸé—´æ”¶é›†ç¼ºé™·æ ·æœ¬ï¼Œä½†åˆ©ç”¨å®ƒä»¬é€šå¸¸éœ€è¦åƒç´ çº§æ³¨é‡Šï¼Œé™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ADClickï¼Œè¿™æ˜¯ä¸€ç§äº¤äº’å¼å›¾åƒåˆ†å‰²ï¼ˆIISï¼‰ç®—æ³•ï¼Œç”¨äºå·¥ä¸šå¼‚å¸¸æ£€æµ‹ã€‚ADClickä»…é€šè¿‡å°‘é‡ç”¨æˆ·ç‚¹å‡»å’Œç®€çŸ­æ–‡å­—æè¿°å³å¯ç”Ÿæˆåƒç´ çº§å¼‚å¸¸æ³¨é‡Šï¼Œå®ç°ç²¾ç¡®é«˜æ•ˆçš„æ ‡æ³¨ï¼Œæ˜¾è‘—æé«˜ADæ¨¡å‹æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼Œåœ¨MVTec ADä¸Šçš„AP&#x3D;96.1%ï¼‰ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ADClick-Segï¼Œè¿™æ˜¯ä¸€ä¸ªè·¨æ¨¡æ€æ¡†æ¶ï¼Œé€šè¿‡åŸºäºåŸå‹çš„æ–¹æ³•å¯¹é½è§†è§‰ç‰¹å¾å’Œæ–‡æœ¬æç¤ºï¼Œç”¨äºå¼‚å¸¸æ£€æµ‹å’Œå®šä½ã€‚é€šè¿‡ç»“åˆåƒç´ çº§å…ˆéªŒçŸ¥è¯†å’Œè¯­è¨€å¼•å¯¼çº¿ç´¢ï¼ŒADClick-Segåœ¨å¤šç±»ADä»»åŠ¡ä¸Šå®ç°äº†æœ€ä½³ç»“æœï¼ˆåœ¨MVTec ADä¸Šçš„AP&#x3D;80.0%ï¼ŒPRO&#x3D;97.5%ï¼ŒPixel-AUROC&#x3D;99.1%ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰åœ¨å·¥ä¸šäº§å“æ£€æµ‹ä¸­ä¸»è¦ä¾èµ–éç¼ºé™·æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚</li>
<li>ç¼ºé™·æ ·æœ¬çš„åˆ©ç”¨å—é™äºåƒç´ çº§æ³¨é‡Šçš„éœ€æ±‚ï¼Œé™åˆ¶äº†å¼‚å¸¸æ£€æµ‹æ¨¡å‹çš„æ‰©å±•æ€§ã€‚</li>
<li>ADClickæ˜¯ä¸€ç§äº¤äº’å¼å›¾åƒåˆ†å‰²ç®—æ³•ï¼Œé€šè¿‡å°‘é‡ç”¨æˆ·ç‚¹å‡»å’Œæ–‡æœ¬æè¿°ç”Ÿæˆåƒç´ çº§å¼‚å¸¸æ³¨é‡Šï¼Œæé«˜ADæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ADClickåœ¨MVTec ADä¸Šçš„å¼‚å¸¸æ£€æµ‹å‡†ç¡®ç‡è¾¾åˆ°äº†96.1%ã€‚</li>
<li>ADClick-Segæ˜¯ä¸€ä¸ªè·¨æ¨¡æ€æ¡†æ¶ï¼Œç»“åˆäº†è§†è§‰ç‰¹å¾å’Œæ–‡æœ¬æç¤ºï¼Œç”¨äºæ›´å‡†ç¡®çš„å¼‚å¸¸æ£€æµ‹å’Œå®šä½ã€‚</li>
<li>ADClick-Segåœ¨MVTec ADä¸Šçš„å¤šç±»å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼ŒåŒ…æ‹¬APã€PROå’ŒPixel-AUROCç­‰æŒ‡æ ‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05034">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-850b47e32b8cdd23ea4cf82ff63596ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acaa73c61d05978da47d8a5f8fe9a3d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86c657b78fd7dd5b04400e3abaa0557b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c567d25af54ccd67578f9c7ad948728.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34ca6a56c8855eb9229363585f497f35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1276c34348f923f80bd44faa1b648960.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="XRISM-observations-of-solar-flare-X-ray-emission-reflected-in-the-Earthâ€™s-atmosphere"><a href="#XRISM-observations-of-solar-flare-X-ray-emission-reflected-in-the-Earthâ€™s-atmosphere" class="headerlink" title="XRISM observations of solar flare X-ray emission reflected in the   Earthâ€™s atmosphere"></a>XRISM observations of solar flare X-ray emission reflected in the   Earthâ€™s atmosphere</h2><p><strong>Authors:Hiromasa Suzuki, Jun Kurashima, Koji Mori, Satoru Katsuda, Shun Inoue, Daiki Ishi, Eugene M. Churazov, Rashid A. Sunyaev, Ildar Khabibullin, Tsunefumi Mizuno, Caroline Kilbourne, Yuichiro Ezoe, Hiroshi Nakajima, Kosuke Sato, Eric Miller, Kyoko Matsushita</strong></p>
<p>The X-ray Imaging and Spectroscopy Mission (XRISM), launched into low-Earth orbit in 2023, observes the reflection of solar flare X-rays in the Earthâ€™s atmosphere as a by-product of celestial observations. Using a $\sim$one-year data set covering from October 2023 to November 2024, we report on our first results of the measurement of the metal abundance pattern and high-resolution Fe-K spectroscopy. The abundances of Mg, Si, S, Ar, Ca, and Fe measured with the CCD detector Xtend during M- and X-class flares show the inverse-first-ionization-potential (inverse-FIP) effect, which is consistent with the results of Katsuda et al., ApJ, 2020 using the Suzaku satellite. The abundances of Si, S, and Ar are found to decrease with increasing flare magnitude, which is consistent with the theoretical model by Laming (Laming, ApJ, 2021), whereas Ca exhibits an opposite trend. The large effective area and field of view of Xtend allow us to trace the evolution of the abundances in several X-class flare loops on a timescale of a few 100 s, finding an enrichment of low-FIP elements before flare peaks. The high-resolution Fe-K spectrum obtained with the microcalorimeter Resolve successfully separates the Rayleigh- and Compton-scattered Fe XXIV&#x2F;XXV lines and neutral or low-ionized Fe-K$\alpha$ lines. The neutral&#x2F;low-ionized Fe-K$\alpha$ equivalent width shows an anti-correlation with hard X-ray flux with the best-fit power-law slope of $-0.14 \pm 0.09$, suggesting that hard X-rays from flare loops are stimulating the Fe K$\alpha$ fluorescence. This work demonstrates that XRISM can be a powerful tool in the field of solar physics, offering valuable high-statistic CCD data and high-resolution microcalorimeter spectra in the energy range extending to the Fe-K band. </p>
<blockquote>
<p>Xå°„çº¿æˆåƒä¸å…‰è°±ä»»åŠ¡ï¼ˆXRISMï¼‰äº2023å¹´å‘å°„è‡³ä½åœ°çƒè½¨é“ï¼Œä½œä¸ºå¤©ä½“è§‚æµ‹çš„å‰¯äº§å“ï¼Œè§‚æµ‹å¤ªé˜³è€€æ–‘Xå°„çº¿åœ¨åœ°çƒå¤§æ°”ä¸­çš„åå°„ã€‚æˆ‘ä»¬ä½¿ç”¨ä»2023å¹´10æœˆè‡³2024å¹´11æœˆçš„çº¦ä¸€å¹´æ•°æ®é›†ï¼ŒæŠ¥å‘Šäº†æˆ‘ä»¬é¦–æ¬¡æµ‹é‡çš„é‡‘å±ä¸°åº¦æ¨¡å¼å’Œé«˜åˆ†è¾¨ç‡Fe-Kå…‰è°±çš„ç»“æœã€‚ä½¿ç”¨CCDæ£€æµ‹å™¨Xtendåœ¨Mçº§å’ŒXçº§è€€æ–‘æœŸé—´æµ‹é‡çš„Mgã€Siã€Sã€Arã€Caå’ŒFeçš„ä¸°åº¦è¡¨ç°å‡ºé€†é¦–ç”µç¦»ç”µä½ï¼ˆinverse-FIPï¼‰æ•ˆåº”ï¼Œè¿™ä¸ä½¿ç”¨Suzakuå«æ˜Ÿçš„Katsudaç­‰äººçš„ç»“æœä¸€è‡´ï¼ˆApJï¼Œ2020å¹´ï¼‰ã€‚éšç€è€€æ–‘å¼ºåº¦çš„å¢åŠ ï¼ŒSiã€Så’ŒArçš„ä¸°åº¦å‡å°‘ï¼Œè¿™ä¸Lamingçš„ç†è®ºæ¨¡å‹ä¸€è‡´ï¼ˆLamingï¼ŒApJï¼Œ2021å¹´ï¼‰ï¼Œè€ŒCaåˆ™è¡¨ç°å‡ºç›¸åçš„è¶‹åŠ¿ã€‚Xtendçš„å¤§æœ‰æ•ˆé¢ç§¯å’Œè§†é‡ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¿½è¸ªå‡ ä¸ªXçº§è€€æ–‘ç¯åœ¨æ•°ç™¾ç§’æ—¶é—´å°ºåº¦ä¸Šçš„ä¸°åº¦æ¼”å˜ï¼Œå‘ç°åœ¨è€€æ–‘å³°å€¼ä¹‹å‰ä½FIPå…ƒç´ çš„å¯Œé›†ã€‚ä½¿ç”¨å¾®çƒ­é‡è®¡Resolveè·å¾—çš„é«˜åˆ†è¾¨ç‡Fe-Kå…‰è°±æˆåŠŸåœ°åˆ†ç¦»äº†Rayleighå’ŒComptonæ•£å°„çš„Fe XXIV&#x2F;XXVçº¿ä»¥åŠä¸­æ€§æˆ–ä½ç”µç¦»çš„Fe-KÎ±çº¿ã€‚ä¸­æ€§&#x2F;ä½ç”µç¦»Fe-KÎ±ç­‰ä»·å®½åº¦ä¸ç¡¬Xå°„çº¿æµé‡å‘ˆåç›¸å…³ï¼Œæœ€ä½³æ‹Ÿåˆå¹‚å¾‹æ–œç‡ä¸º$-0.14 \pm 0.09$ï¼Œè¿™è¡¨æ˜æ¥è‡ªè€€æ–‘ç¯çš„ç¡¬Xå°„çº¿æ¿€å‘äº†Fe KÎ±è§å…‰ã€‚è¿™é¡¹å·¥ä½œè¡¨æ˜ï¼ŒXRISMåœ¨å¤ªé˜³ç‰©ç†å­¦é¢†åŸŸå¯ä»¥æˆä¸ºä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œæä¾›æœ‰ä»·å€¼çš„é«˜ç»Ÿè®¡CCDæ•°æ®å’Œé«˜åˆ†è¾¨ç‡å¾®çƒ­é‡è®¡å…‰è°±ï¼Œèƒ½é‡èŒƒå›´å»¶ä¼¸åˆ°Fe-Kæ³¢æ®µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05029v1">PDF</a> 12 pages, 15 figures, accepted for publication in PASJ</p>
<p><strong>Summary</strong></p>
<p>XRISMä»»åŠ¡è§‚æµ‹å¤ªé˜³è€€æ–‘Xå°„çº¿åœ¨åœ°çƒå¤§æ°”ä¸­çš„åå°„ï¼Œå…¶é¦–æ¬¡ç»“æœæ­ç¤ºäº†é‡‘å±ä¸°åº¦æ¨¡å¼å’Œé«˜åˆ†è¾¨ç‡Fe-Kå…‰è°±çš„æµ‹é‡ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé‡‘å±å…ƒç´ ä¸°åº¦ä¸è€€æ–‘ç­‰çº§æœ‰å…³ï¼Œå¹¶å‘ˆç°å‡ºé€†FIPæ•ˆåº”ã€‚XRISMçš„Xtendæ¢æµ‹å™¨èƒ½å¤Ÿè¿½è¸ªå‡ ä¸ªXçº§è€€æ–‘ç¯ä¸­å…ƒç´ çš„ä¸°åº¦æ¼”å˜ï¼Œè€Œé«˜åˆ†è¾¨ç‡çš„Resolveå¾®çƒ­é‡è®¡æˆåŠŸåˆ†ç¦»äº†Fe XXIV&#x2F;XXVçº¿å’Œä¸­æ€§æˆ–ä½ç”µç¦»çš„Fe-KÎ±çº¿ã€‚è¿™é¡¹ç ”ç©¶å±•ç¤ºäº†XRISMåœ¨å¤ªé˜³ç‰©ç†å­¦é¢†åŸŸçš„å¼ºå¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>XRISMä»»åŠ¡åœ¨åœ°çƒä½è½¨é“ä¸Šè§‚æµ‹åˆ°å¤ªé˜³è€€æ–‘Xå°„çº¿åå°„ç°è±¡ã€‚</li>
<li>é‡‘å±å…ƒç´ ä¸°åº¦ä¸è€€æ–‘ç­‰çº§ç›¸å…³ï¼Œè¡¨ç°å‡ºé€†FIPæ•ˆåº”ã€‚</li>
<li>Xtendæ¢æµ‹å™¨èƒ½å¤Ÿè¿½è¸ªå¤šä¸ªXçº§è€€æ–‘ç¯ä¸­å…ƒç´ çš„ä¸°åº¦æ¼”å˜ã€‚</li>
<li>é«˜åˆ†è¾¨ç‡çš„Resolveå¾®çƒ­é‡è®¡æˆåŠŸåˆ†ç¦»äº†Feçš„ä¸åŒå½¢æ€çº¿ã€‚</li>
<li>ä¸­æ€§&#x2F;ä½ç”µç¦»çš„Fe-KÎ±ç­‰æ•ˆå®½åº¦ä¸ç¡¬Xå°„çº¿æµé‡å‘ˆè´Ÿç›¸å…³ã€‚</li>
<li>ç¡¬Xå°„çº¿å¯èƒ½åˆºæ¿€Fe KÎ±è§å…‰ã€‚</li>
<li>XRISMåœ¨å¤ªé˜³ç‰©ç†å­¦é¢†åŸŸå…·æœ‰å¼ºå¤§çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05029">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b646d4d6281f6829784c483429fef82.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-205aa878fdf6953db7b789d7941bfdc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfc8054066283bfa06aec23519144a6e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28d1cc214c2fec0ce25c28b399832953.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6e1b303ebd08dc0cf7d6e45c03bc1a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-484ae4deaccfa6628e8748cbd8aa28c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b48f6fe43fe03d161a03dc5a537c800.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca6e177dc7ad33e2d5a139b77a731ea6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39e7a197e893c0ac813739b2485de6ad.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Interpretable-Deep-Transfer-Learning-for-Breast-Ultrasound-Cancer-Detection-A-Multi-Dataset-Study"><a href="#Interpretable-Deep-Transfer-Learning-for-Breast-Ultrasound-Cancer-Detection-A-Multi-Dataset-Study" class="headerlink" title="Interpretable Deep Transfer Learning for Breast Ultrasound Cancer   Detection: A Multi-Dataset Study"></a>Interpretable Deep Transfer Learning for Breast Ultrasound Cancer   Detection: A Multi-Dataset Study</h2><p><strong>Authors:Mohammad Abbadi, Yassine Himeur, Shadi Atalla, Wathiq Mansoor</strong></p>
<p>Breast cancer remains a leading cause of cancer-related mortality among women worldwide. Ultrasound imaging, widely used due to its safety and cost-effectiveness, plays a key role in early detection, especially in patients with dense breast tissue. This paper presents a comprehensive study on the application of machine learning and deep learning techniques for breast cancer classification using ultrasound images. Using datasets such as BUSI, BUS-BRA, and BrEaST-Lesions USG, we evaluate classical machine learning models (SVM, KNN) and deep convolutional neural networks (ResNet-18, EfficientNet-B0, GoogLeNet). Experimental results show that ResNet-18 achieves the highest accuracy (99.7%) and perfect sensitivity for malignant lesions. Classical ML models, though outperformed by CNNs, achieve competitive performance when enhanced with deep feature extraction. Grad-CAM visualizations further improve model transparency by highlighting diagnostically relevant image regions. These findings support the integration of AI-based diagnostic tools into clinical workflows and demonstrate the feasibility of deploying high-performing, interpretable systems for ultrasound-based breast cancer detection. </p>
<blockquote>
<p>ä¹³è…ºç™Œä»ç„¶æ˜¯å…¨çƒå¥³æ€§ç™Œç—‡æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚è¶…å£°æˆåƒå› å…¶å®‰å…¨æ€§å’Œæˆæœ¬æ•ˆç›Šè€Œå¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œåœ¨æ—©æœŸæ£€æµ‹ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¹³è…ºç»„ç»‡è‡´å¯†çš„æ‚£è€…ä¸­ã€‚æœ¬æ–‡å…¨é¢ç ”ç©¶äº†åº”ç”¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯å¯¹è¶…å£°å›¾åƒè¿›è¡Œä¹³è…ºç™Œåˆ†ç±»çš„æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨BUSIã€BUS-BRAå’ŒBrEaST-Lesions USGç­‰æ•°æ®é›†ï¼Œè¯„ä¼°äº†ç»å…¸æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆSVMã€KNNï¼‰å’Œæ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆResNet-18ã€EfficientNet-B0ã€GoogLeNetï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒResNet-18çš„å‡†ç¡®ç‡æœ€é«˜ï¼ˆ99.7%ï¼‰ï¼Œå¯¹æ¶æ€§ç—…å˜çš„æ•æ„Ÿæ€§å®Œç¾ã€‚è™½ç„¶è¢«å·ç§¯ç¥ç»ç½‘ç»œè¶…è¶Šï¼Œä½†ç»å…¸æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨å¢å¼ºæ·±åº¦ç‰¹å¾æå–åè¡¨ç°è‰¯å¥½ã€‚Grad-CAMå¯è§†åŒ–é€šè¿‡çªå‡ºæ˜¾ç¤ºè¯Šæ–­ç›¸å…³çš„å›¾åƒåŒºåŸŸï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„é€æ˜åº¦ã€‚è¿™äº›å‘ç°æ”¯æŒå°†åŸºäºäººå·¥æ™ºèƒ½çš„è¯Šæ–­å·¥å…·æ•´åˆåˆ°ä¸´åºŠå·¥ä½œæµç¨‹ä¸­ï¼Œå¹¶å±•ç¤ºäº†éƒ¨ç½²é«˜æ€§èƒ½ã€å¯è§£é‡Šçš„è¶…å£°ä¹³è…ºç™Œæ£€æµ‹ç³»ç»Ÿå¯è¡Œæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05004v1">PDF</a> 6 pages, 2 figures and 1 table</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åº”ç”¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨è¶…å£°å›¾åƒä¸Šè¿›è¡Œä¹³è…ºç™Œåˆ†ç±»çš„åº”ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒResNet-18æ¨¡å‹åœ¨æ¶æ€§ç—…å˜æ£€æµ‹æ–¹é¢å–å¾—äº†æœ€é«˜å‡†ç¡®æ€§ï¼ˆ99.7%ï¼‰å’Œå®Œç¾çš„æ•æ„Ÿæ€§ã€‚æ¢¯åº¦åŠ æƒç±»æ¿€æ´»æ˜ å°„ï¼ˆGrad-CAMï¼‰å¯è§†åŒ–æé«˜äº†æ¨¡å‹çš„é€æ˜åº¦ï¼Œçªå‡ºäº†è¯Šæ–­ç›¸å…³çš„å›¾åƒåŒºåŸŸã€‚è¿™äº›å‘ç°æ”¯æŒå°†äººå·¥æ™ºèƒ½è¯Šæ–­å·¥å…·é›†æˆåˆ°ä¸´åºŠå·¥ä½œæµç¨‹ä¸­ï¼Œå¹¶å±•ç¤ºäº†éƒ¨ç½²é«˜æ€§èƒ½ã€å¯è§£é‡Šçš„è¶…å£°ä¹³è…ºç™Œæ£€æµ‹ç³»ç»Ÿçš„å¯è¡Œæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºç™Œæ˜¯ä¸–ç•ŒèŒƒå›´å†…å¯¼è‡´å¥³æ€§æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚</li>
<li>è¶…å£°æ³¢æˆåƒåœ¨ä¹³è…ºç™Œçš„æ—©æœŸæ£€æµ‹ä¸­æ‰®æ¼”ç€å…³é”®è§’è‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¹³è…ºç»„ç»‡å¯†é›†çš„æ‚£è€…ä¸­ã€‚</li>
<li>æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨è¶…å£°å›¾åƒä¹³è…ºç™Œåˆ†ç±»æ–¹é¢è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>ResNet-18æ¨¡å‹åœ¨æ¶æ€§ç—…å˜æ£€æµ‹æ–¹é¢å–å¾—äº†æœ€é«˜å‡†ç¡®æ€§ï¼ˆ99.7%ï¼‰å’Œå®Œç¾çš„æ•æ„Ÿæ€§ã€‚</li>
<li>ç»å…¸æœºå™¨å­¦ä¹ æ¨¡å‹è™½ç„¶è¢«å·ç§¯ç¥ç»ç½‘ç»œæ‰€è¶…è¶Šï¼Œä½†åœ¨æ·±åº¦ç‰¹å¾æå–çš„å¢å¼ºä¸‹ä»è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>Grad-CAMå¯è§†åŒ–æé«˜äº†æ¨¡å‹çš„é€æ˜åº¦ï¼Œæœ‰åŠ©äºè§£é‡Šè¯Šæ–­è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05004">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de8ce3ed75ea7c2392ebfbfc0e4a74b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb89b6b84da5d759f4751b14ccfb1313.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe78e8c7602c8bcf3100b104c0ca6963.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fc973166dc53a5a509d3d89a442a979c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67ac24d9b7448e990cffed3b74b3f0d6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="INR-meets-Multi-Contrast-MRI-Reconstruction"><a href="#INR-meets-Multi-Contrast-MRI-Reconstruction" class="headerlink" title="INR meets Multi-Contrast MRI Reconstruction"></a>INR meets Multi-Contrast MRI Reconstruction</h2><p><strong>Authors:Natascha Niessen, Carolin M. Pirkl, Ana Beatriz Solana, Hannah Eichhorn, Veronika Spieker, Wenqi Huang, Tim Sprenger, Marion I. Menzel, Julia A. Schnabel</strong></p>
<p>Multi-contrast MRI sequences allow for the acquisition of images with varying tissue contrast within a single scan. The resulting multi-contrast images can be used to extract quantitative information on tissue microstructure. To make such multi-contrast sequences feasible for clinical routine, the usually very long scan times need to be shortened e.g. through undersampling in k-space. However, this comes with challenges for the reconstruction. In general, advanced reconstruction techniques such as compressed sensing or deep learning-based approaches can enable the acquisition of high-quality images despite the acceleration. In this work, we leverage redundant anatomical information of multi-contrast sequences to achieve even higher acceleration rates. We use undersampling patterns that capture the contrast information located at the k-space center, while performing complementary undersampling across contrasts for high frequencies. To reconstruct this highly sparse k-space data, we propose an implicit neural representation (INR) network that is ideal for using the complementary information acquired across contrasts as it jointly reconstructs all contrast images. We demonstrate the benefits of our proposed INR method by applying it to multi-contrast MRI using the MPnRAGE sequence, where it outperforms the state-of-the-art parallel imaging compressed sensing (PICS) reconstruction method, even at higher acceleration factors. </p>
<blockquote>
<p>å¤šå¯¹æ¯”åº¦MRIåºåˆ—èƒ½å¤Ÿåœ¨å•æ¬¡æ‰«æä¸­è·å–å…·æœ‰ä¸åŒç»„ç»‡å¯¹æ¯”åº¦çš„å›¾åƒã€‚å¾—åˆ°çš„å¤šå¯¹æ¯”åº¦å›¾åƒå¯ç”¨äºæå–æœ‰å…³ç»„ç»‡å¾®è§‚ç»“æ„çš„å®šé‡ä¿¡æ¯ã€‚ä¸ºäº†ä½¿è¿™æ ·çš„å¤šå¯¹æ¯”åº¦åºåˆ—é€‚ç”¨äºä¸´åºŠå¸¸è§„åº”ç”¨ï¼Œé€šå¸¸éœ€è¦ç¼©çŸ­éå¸¸é•¿çš„æ‰«ææ—¶é—´ï¼Œä¾‹å¦‚é€šè¿‡kç©ºé—´ä¸­çš„æ¬ é‡‡æ ·ã€‚ç„¶è€Œï¼Œè¿™ç»™é‡å»ºå¸¦æ¥äº†æŒ‘æˆ˜ã€‚é€šå¸¸ï¼Œå…ˆè¿›çš„é‡å»ºæŠ€æœ¯ï¼Œå¦‚å‹ç¼©æ„ŸçŸ¥æˆ–åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œå°½ç®¡åŠ é€Ÿäº†æ‰«æï¼Œä½†ä»èƒ½è·å–é«˜è´¨é‡çš„å›¾åƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤šå¯¹æ¯”åº¦åºåˆ—çš„å†—ä½™è§£å‰–ä¿¡æ¯æ¥å®ç°æ›´é«˜çš„åŠ é€Ÿç‡ã€‚æˆ‘ä»¬ä½¿ç”¨æ¬ é‡‡æ ·æ¨¡å¼æ¥æ•è·ä½äºkç©ºé—´ä¸­å¿ƒçš„å¯¹æ¯”åº¦ä¿¡æ¯ï¼ŒåŒæ—¶åœ¨ä¸åŒå¯¹æ¯”åº¦ä¹‹é—´è¿›è¡Œäº’è¡¥æ¬ é‡‡æ ·ä»¥è·å–é«˜é¢‘ä¿¡æ¯ã€‚ä¸ºäº†é‡å»ºè¿™ç§é«˜åº¦ç¨€ç–çš„kç©ºé—´æ•°æ®ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰ç½‘ç»œï¼Œå®ƒéå¸¸é€‚åˆä½¿ç”¨ä¸åŒå¯¹æ¯”åº¦ä¹‹é—´è·å–çš„äº’è¡¥ä¿¡æ¯è¿›è¡Œè”åˆé‡å»ºæ‰€æœ‰å¯¹æ¯”åº¦å›¾åƒã€‚æˆ‘ä»¬é€šè¿‡å°†æ‰€æå‡ºçš„æ–¹æ³•åº”ç”¨äºä½¿ç”¨MPnRAGEåºåˆ—çš„å¤šå¯¹æ¯”åº¦MRIï¼Œå±•ç¤ºäº†å…¶ä¼˜åŠ¿ï¼Œå³ä½¿åœ¨è¾ƒé«˜çš„åŠ é€Ÿå› å­ä¸‹ï¼Œè¯¥æ–¹æ³•ä¹Ÿä¼˜äºæœ€æ–°çš„å¹¶è¡Œæˆåƒå‹ç¼©æ„ŸçŸ¥ï¼ˆPICSï¼‰é‡å»ºæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04888v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨å¤šå¯¹æ¯”åº¦MRIåºåˆ—å†—ä½™è§£å‰–å­¦ä¿¡æ¯å®ç°æ›´é«˜åŠ é€Ÿç‡çš„æ–¹æ³•ã€‚é€šè¿‡åœ¨kç©ºé—´ä¸­å¿ƒæ•æ‰å¯¹æ¯”ä¿¡æ¯ï¼Œå¹¶è·¨å¯¹æ¯”åº¦è¿›è¡Œäº’è¡¥æ¬ é‡‡æ ·ï¼Œä»¥åŠä½¿ç”¨ç†æƒ³äºåˆ©ç”¨è·¨å¯¹æ¯”åº¦è·å–äº’è¡¥ä¿¡æ¯çš„éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰ç½‘ç»œè¿›è¡Œé‡å»ºï¼Œå®ç°äº†é«˜è´¨é‡å›¾åƒçš„æ¢å¤ã€‚åœ¨MPnRAGEåºåˆ—çš„å¤šå¯¹æ¯”åº¦MRIä¸­åº”ç”¨æ­¤æ–¹æ³•ï¼Œå³ä½¿åœ¨è¾ƒé«˜çš„åŠ é€Ÿå› å­ä¸‹ä¹Ÿä¼˜äºç°æœ‰çš„å¹¶è¡Œæˆåƒå‹ç¼©æ„ŸçŸ¥ï¼ˆPICSï¼‰é‡å»ºæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šå¯¹æ¯”åº¦MRIåºåˆ—èƒ½å¤Ÿåœ¨å•æ¬¡æ‰«æä¸­è·å–å…·æœ‰ä¸åŒç»„ç»‡å¯¹æ¯”åº¦çš„å›¾åƒã€‚</li>
<li>é‡å»ºæŠ€æœ¯å¦‚å‹ç¼©æ„ŸçŸ¥æˆ–æ·±åº¦å­¦ä¹ æ–¹æ³•å¯åŠ é€Ÿå›¾åƒè·å–å¹¶ä¿æŒé«˜è´¨é‡ã€‚</li>
<li>åˆ©ç”¨å¤šå¯¹æ¯”åº¦åºåˆ—çš„å†—ä½™è§£å‰–å­¦ä¿¡æ¯å¯å®ç°æ›´é«˜çš„åŠ é€Ÿç‡ã€‚</li>
<li>é€šè¿‡å¯¹kç©ºé—´ä¸­å¿ƒçš„å¯¹æ¯”ä¿¡æ¯è¿›è¡Œæ¬ é‡‡æ ·ï¼Œå¹¶è·¨å¯¹æ¯”åº¦è¿›è¡Œäº’è¡¥æ¬ é‡‡æ ·ï¼Œå¤„ç†é«˜åº¦ç¨€ç–çš„kç©ºé—´æ•°æ®ã€‚</li>
<li>æå‡ºä½¿ç”¨éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰ç½‘ç»œè¿›è¡Œé‡å»ºï¼Œè¯¥ç½‘ç»œå¯åˆ©ç”¨è·¨å¯¹æ¯”åº¦çš„äº’è¡¥ä¿¡æ¯ã€‚</li>
<li>åœ¨MPnRAGEåºåˆ—çš„å¤šå¯¹æ¯”åº¦MRIä¸­åº”ç”¨äº†è¯¥æ–¹æ³•ï¼Œè¡¨ç°å‡ºä¼˜äºç°æœ‰PICSé‡å»ºæ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04888">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1941f84920d59ec1d25ad8dc55136c45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db83032e2aef44b64af16dae866c3fee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1c5ab7532867ffad104dc6f64af1398.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Cryo-RL-automating-prostate-cancer-cryoablation-planning-with-reinforcement-learning"><a href="#Cryo-RL-automating-prostate-cancer-cryoablation-planning-with-reinforcement-learning" class="headerlink" title="Cryo-RL: automating prostate cancer cryoablation planning with   reinforcement learning"></a>Cryo-RL: automating prostate cancer cryoablation planning with   reinforcement learning</h2><p><strong>Authors:Trixia Simangan, Ahmed Nadeem Abbasi, Yipeng Hu, Shaheer U. Saeed</strong></p>
<p>Cryoablation is a minimally invasive localised treatment for prostate cancer that destroys malignant tissue during de-freezing, while sparing surrounding healthy structures. Its success depends on accurate preoperative planning of cryoprobe placements to fully cover the tumour and avoid critical anatomy. This planning is currently manual, expertise-dependent, and time-consuming, leading to variability in treatment quality and limited scalability. In this work, we introduce Cryo-RL, a reinforcement learning framework that models cryoablation planning as a Markov decision process and learns an optimal policy for cryoprobe placement. Within a simulated environment that models clinical constraints and stochastic intraoperative variability, an agent sequentially selects cryoprobe positions and ice sphere diameters. Guided by a reward function based on tumour coverage, this agent learns a cryoablation strategy that leads to optimal cryoprobe placements without the need for any manually-designed plans. Evaluated on 583 retrospective prostate cancer cases, Cryo-RL achieved over 8 percentage-point Dice improvements compared with the best automated baselines, based on geometric optimisation, and matched human expert performance while requiring substantially less planning time. These results highlight the potential of reinforcement learning to deliver clinically viable, reproducible, and efficient cryoablation plans. </p>
<blockquote>
<p>å†·å†»æ¶ˆèæ˜¯ä¸€ç§é’ˆå¯¹å‰åˆ—è…ºç™Œçš„å¾®åˆ›å±€éƒ¨æ²»ç–—æ–¹æ³•ï¼Œåœ¨è§£å†»è¿‡ç¨‹ä¸­ç ´åæ¶æ€§ç»„ç»‡ï¼ŒåŒæ—¶ä¿ç•™å‘¨å›´çš„å¥åº·ç»“æ„ã€‚å…¶æˆåŠŸå–å†³äºå†·å†»æ¢é’ˆæ”¾ç½®çš„æœ¯å‰è§„åˆ’å‡†ç¡®ï¼Œä»¥å…¨é¢è¦†ç›–è‚¿ç˜¤å¹¶é¿å…å…³é”®è§£å‰–ç»“æ„ã€‚å½“å‰çš„è§„åˆ’æ˜¯æ‰‹åŠ¨è¿›è¡Œçš„ï¼Œä¾èµ–äºä¸“å®¶ç»éªŒï¼Œå¹¶ä¸”è€—æ—¶ï¼Œå¯¼è‡´æ²»ç–—è´¨é‡å­˜åœ¨å·®å¼‚æ€§ä¸”æ‰©å±•æ€§æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å†·å†»å¼ºåŒ–å­¦ä¹ ï¼ˆCryo-RLï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå°†å†·å†»æ¶ˆèè§„åˆ’å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶å­¦ä¹ å†·å†»æ¢é’ˆæ”¾ç½®çš„æœ€ä¼˜ç­–ç•¥ã€‚åœ¨ä¸€ä¸ªæ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼Œè¯¥ç¯å¢ƒæ¨¡æ‹Ÿäº†ä¸´åºŠçº¦æŸå’Œæœ¯ä¸­éšæœºå˜åŒ–ï¼Œæ™ºèƒ½ä½“æŒ‰é¡ºåºé€‰æ‹©å†·å†»æ¢é’ˆçš„ä½ç½®å’Œå†°çƒç›´å¾„ã€‚åœ¨è‚¿ç˜¤è¦†ç›–ç‡çš„å¥–åŠ±å‡½æ•°æŒ‡å¯¼ä¸‹ï¼Œæ™ºèƒ½ä½“å­¦ä¹ äº†ä¸€ç§å†·å†»æ¶ˆèç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿå®ç°æœ€ä¼˜çš„å†·å†»æ¢é’ˆæ”¾ç½®ï¼Œæ— éœ€ä»»ä½•æ‰‹åŠ¨è®¾è®¡è®¡åˆ’ã€‚åœ¨583ä¾‹å›é¡¾æ€§å‰åˆ—è…ºç™Œç—…ä¾‹ä¸­ï¼Œä¸åŸºäºå‡ ä½•ä¼˜åŒ–çš„æœ€ä½³è‡ªåŠ¨åŒ–åŸºçº¿ç›¸æ¯”ï¼ŒCryo-RLå®ç°äº†è¶…è¿‡8ä¸ªç™¾åˆ†ç‚¹çš„Diceæ”¹å–„ï¼Œå¹¶è¾¾åˆ°äº†äººç±»ä¸“å®¶çš„è¡¨ç°æ°´å¹³ï¼ŒåŒæ—¶å¤§å¤§å‡å°‘äº†è§„åˆ’æ—¶é—´ã€‚è¿™äº›ç»“æœçªå‡ºäº†å¼ºåŒ–å­¦ä¹ åœ¨æä¾›ä¸´åºŠå¯è¡Œã€å¯å¤åˆ¶å’Œé«˜æ•ˆçš„å†·å†»æ¶ˆèè®¡åˆ’æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04886v1">PDF</a> Accepted at MICAD (Medical Imaging and Computer-Aided Diagnosis) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å†·å†»æ¶ˆèï¼ˆCryoablationï¼‰æ²»ç–—å‰åˆ—è…ºç™Œçš„æ–¹æ³•ï¼Œå¹¶æŒ‡å‡ºå…¶æˆåŠŸä¾èµ–äºç²¾ç¡®çš„æœ¯å‰è§„åˆ’å†·å†»æ¢é’ˆæ”¾ç½®ä½ç½®ã€‚æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶Cryo-RLï¼Œæ¨¡æ‹Ÿå†·å†»æ¶ˆèæ‰‹æœ¯çš„è®¡åˆ’è¿‡ç¨‹å¹¶è‡ªåŠ¨é€‰æ‹©æœ€ä½³çš„å†·å†»æ¢é’ˆä½ç½®å’Œå†°çƒç›´å¾„ã€‚ç›¸æ¯”ä¼ ç»Ÿçš„å‡ ä½•ä¼˜åŒ–æ–¹æ³•å’Œä¸“å®¶æ‰‹åŠ¨è§„åˆ’ï¼ŒCryo-RLåœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­å®ç°äº†æ›´é«˜çš„è‚¿ç˜¤è¦†ç›–ç‡å’Œæ›´çŸ­çš„è§„åˆ’æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å†·å†»æ¶ˆèæ˜¯ä¸€ç§æ²»ç–—å‰åˆ—è…ºç™Œçš„å¾®åˆ›å±€éƒ¨ç–—æ³•ï¼Œé€šè¿‡å†·å†»ç ´åæ¶æ€§ç»„ç»‡ï¼ŒåŒæ—¶ä¿ç•™å‘¨å›´å¥åº·ç»“æ„ã€‚</li>
<li>å†·å†»æ¶ˆèçš„æˆåŠŸå–å†³äºæœ¯å‰å¯¹å†·å†»æ¢é’ˆæ”¾ç½®ä½ç½®çš„ç²¾ç¡®è§„åˆ’ï¼Œä»¥è¦†ç›–æ•´ä¸ªè‚¿ç˜¤å¹¶é¿å…å…³é”®éƒ¨ä½ã€‚</li>
<li>å½“å‰è§„åˆ’ä¾èµ–äºæ‰‹åŠ¨æ“ä½œå’Œä¸“å®¶ç»éªŒï¼Œå…·æœ‰è€—æ—¶ä¸”å¯å˜æ€§å¤§çš„ç¼ºç‚¹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶Cryo-RLï¼Œæ¨¡æ‹Ÿå†·å†»æ¶ˆèæ‰‹æœ¯çš„è®¡åˆ’è¿‡ç¨‹ã€‚</li>
<li>Cryo-RLå°†å†·å†»æ¶ˆèæ‰‹æœ¯è§„åˆ’å»ºæ¨¡ä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå­¦ä¹ å†·å†»æ¢é’ˆæ”¾ç½®çš„æœ€ä¼˜ç­–ç•¥ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼ŒCryo-RLå®ç°äº†é«˜æ•ˆçš„å†·å†»æ¶ˆèè®¡åˆ’ï¼Œä¸å‡ ä½•ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”ï¼ŒDiceæ”¹å–„ç‡è¶…è¿‡8ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶è¾¾åˆ°äº†ä¸äººç±»ä¸“å®¶çš„åŒ¹é…æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04886">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7f13001f725b80b54f7b26913e7c9fa3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8711b3b95e78473a90ad9a2f64e6e4fe.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Comparative-Evaluation-of-Traditional-and-Deep-Learning-Feature-Matching-Algorithms-using-Chandrayaan-2-Lunar-Data"><a href="#Comparative-Evaluation-of-Traditional-and-Deep-Learning-Feature-Matching-Algorithms-using-Chandrayaan-2-Lunar-Data" class="headerlink" title="Comparative Evaluation of Traditional and Deep Learning Feature Matching   Algorithms using Chandrayaan-2 Lunar Data"></a>Comparative Evaluation of Traditional and Deep Learning Feature Matching   Algorithms using Chandrayaan-2 Lunar Data</h2><p><strong>Authors:R. Makharia, J. G. Singla,  Amitabh, N. Dube, H. Sharma</strong></p>
<p>Accurate image registration is critical for lunar exploration, enabling surface mapping, resource localization, and mission planning. Aligning data from diverse lunar sensors â€“ optical (e.g., Orbital High Resolution Camera, Narrow and Wide Angle Cameras), hyperspectral (Imaging Infrared Spectrometer), and radar (e.g., Dual-Frequency Synthetic Aperture Radar, Selene&#x2F;Kaguya mission) â€“ is challenging due to differences in resolution, illumination, and sensor distortion. We evaluate five feature matching algorithms: SIFT, ASIFT, AKAZE, RIFT2, and SuperGlue (a deep learning-based matcher), using cross-modality image pairs from equatorial and polar regions. A preprocessing pipeline is proposed, including georeferencing, resolution alignment, intensity normalization, and enhancements like adaptive histogram equalization, principal component analysis, and shadow correction. SuperGlue consistently yields the lowest root mean square error and fastest runtimes. Classical methods such as SIFT and AKAZE perform well near the equator but degrade under polar lighting. The results highlight the importance of preprocessing and learning-based approaches for robust lunar image registration across diverse conditions. </p>
<blockquote>
<p>ç²¾ç¡®å›¾åƒé…å‡†å¯¹äºæœˆçƒæ¢ç´¢è‡³å…³é‡è¦ï¼Œå¯å®ç°è¡¨é¢åœ°å›¾ç»˜åˆ¶ã€èµ„æºå®šä½å’Œä»»åŠ¡è§„åˆ’ã€‚ç”±äºåˆ†è¾¨ç‡ã€ç…§æ˜å’Œä¼ æ„Ÿå™¨å¤±çœŸç­‰æ–¹é¢çš„å·®å¼‚ï¼Œå¯¹é½æ¥è‡ªå„ç§æœˆçƒä¼ æ„Ÿå™¨ï¼ˆå…‰å­¦ä¼ æ„Ÿå™¨ï¼Œä¾‹å¦‚è½¨é“é«˜åˆ†è¾¨ç‡ç›¸æœºã€å¹¿è§’é•œå¤´å’Œçª„è§’ç›¸æœºã€æˆåƒçº¢å¤–å…‰è°±ä»ªä»¥åŠé›·è¾¾ä¼ æ„Ÿå™¨ï¼Œä¾‹å¦‚åŒé¢‘åˆæˆå­”å¾„é›·è¾¾å’ŒSelene&#x2F;Kaguyaä»»åŠ¡ï¼‰çš„æ•°æ®æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬è¯„ä¼°äº†äº”ç§ç‰¹å¾åŒ¹é…ç®—æ³•ï¼šSIFTã€ASIFTã€AKAZEã€RIFT2å’ŒåŸºäºæ·±åº¦å­¦ä¹ çš„åŒ¹é…å™¨SuperGlueï¼Œä½¿ç”¨çš„æ˜¯èµ¤é“å’Œæåœ°åœ°åŒºçš„è·¨æ¨¡æ€å›¾åƒå¯¹ã€‚æå‡ºäº†é¢„å¤„ç†ç®¡é“ï¼ŒåŒ…æ‹¬åœ°ç†å‚è€ƒã€åˆ†è¾¨ç‡å¯¹é½ã€å¼ºåº¦å½’ä¸€åŒ–ä»¥åŠè‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–ã€ä¸»æˆåˆ†åˆ†æå’Œé˜´å½±æ ¡æ­£ç­‰å¢å¼ºåŠŸèƒ½ã€‚SuperGlueå§‹ç»ˆäº§ç”Ÿæœ€ä½çš„å‡æ–¹æ ¹è¯¯å·®å’Œæœ€å¿«çš„è¿è¡Œæ—¶é—´ã€‚åƒSIFTå’ŒAKAZEè¿™æ ·çš„ç»å…¸æ–¹æ³•åœ¨èµ¤é“é™„è¿‘è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æåœ°å…‰ç…§æ¡ä»¶ä¸‹æ€§èƒ½ä¸‹é™ã€‚ç»“æœå¼ºè°ƒäº†é¢„å¤„ç†å’ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•å¯¹äºåœ¨å„ç§æ¡ä»¶ä¸‹å®ç°ç¨³å¥çš„æœˆçƒå›¾åƒé…å‡†çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04775v1">PDF</a> 27 pages, 11 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>ç²¾å‡†å›¾åƒé…å‡†å¯¹äºæœˆçƒæ¢ç´¢è‡³å…³é‡è¦ï¼Œå¯å®ç°è¡¨é¢åœ°å›¾ç»˜åˆ¶ã€èµ„æºå®šä½å’Œä»»åŠ¡è§„åˆ’ã€‚é’ˆå¯¹å…‰å­¦ã€é«˜å…‰è°±å’Œé›·è¾¾ç­‰ä¸åŒæœˆçƒä¼ æ„Ÿå™¨çš„æ•°æ®å¯¹é½å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå­˜åœ¨åˆ†è¾¨ç‡ã€ç…§æ˜å’Œä¼ æ„Ÿå™¨å¤±çœŸç­‰æ–¹é¢çš„å·®å¼‚ã€‚è¯„ä¼°äº†äº”ç§ç‰¹å¾åŒ¹é…ç®—æ³•ï¼ŒåŒ…æ‹¬SIFTã€ASIFTã€AKAZEã€RIFT2å’ŒåŸºäºæ·±åº¦å­¦ä¹ çš„SuperGlueåŒ¹é…å™¨ã€‚ç»è¿‡åœ°ç†å‚è€ƒã€åˆ†è¾¨ç‡å¯¹é½ã€å¼ºåº¦å½’ä¸€åŒ–ç­‰é¢„å¤„ç†æµç¨‹ï¼ŒSuperGlueè¡¨ç°æœ€ä½³ï¼Œäº§ç”Ÿæœ€ä½çš„å‡æ–¹æ ¹è¯¯å·®å¹¶å…·å¤‡æœ€å¿«çš„è¿è¡Œæ—¶é—´ã€‚å¤å…¸æ–¹æ³•å¦‚SIFTå’ŒAKAZEåœ¨æ¥è¿‘èµ¤é“æ—¶è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æåœ°ç…§æ˜æ¡ä»¶ä¸‹æ€§èƒ½ä¸‹é™ã€‚ç»“æœå¼ºè°ƒäº†åœ¨å„ç§æ¡ä»¶ä¸‹å®ç°ç¨³å¥çš„æœˆçƒå›¾åƒé…å‡†æ—¶é¢„å¤„ç†å’ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç²¾å‡†å›¾åƒé…å‡†åœ¨æœˆçƒæ¢ç´¢ä¸­éå¸¸é‡è¦ï¼Œæœ‰åŠ©äºè¡¨é¢åœ°å›¾ç»˜åˆ¶ã€èµ„æºå®šä½å’Œä»»åŠ¡è§„åˆ’ã€‚</li>
<li>å¯¹é½æ¥è‡ªä¸åŒæœˆçƒä¼ æ„Ÿå™¨çš„æ•°æ®ï¼ˆå¦‚å…‰å­¦ã€é«˜å…‰è°±å’Œé›·è¾¾ï¼‰å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå­˜åœ¨åˆ†è¾¨ç‡ã€ç…§æ˜å’Œä¼ æ„Ÿå™¨å¤±çœŸå·®å¼‚ã€‚</li>
<li>è¯„ä¼°äº†äº”ç§ç‰¹å¾åŒ¹é…ç®—æ³•ï¼ŒåŒ…æ‹¬SIFTã€ASIFTã€AKAZEã€RIFT2å’ŒSuperGlueï¼Œå…¶ä¸­SuperGlueè¡¨ç°æœ€ä½³ã€‚</li>
<li>é¢„å¤„ç†æµç¨‹ï¼Œå¦‚åœ°ç†å‚è€ƒã€åˆ†è¾¨ç‡å¯¹é½å’Œå¼ºåº¦å½’ä¸€åŒ–ï¼Œå¯¹äºå®ç°ç¨³å¥çš„æœˆçƒå›¾åƒé…å‡†è‡³å…³é‡è¦ã€‚</li>
<li>SuperGlueäº§ç”Ÿæœ€ä½çš„å‡æ–¹æ ¹è¯¯å·®ï¼Œå¹¶å…·å¤‡æœ€å¿«çš„è¿è¡Œæ—¶é—´ã€‚</li>
<li>å¤å…¸ç‰¹å¾åŒ¹é…æ–¹æ³•ï¼ˆå¦‚SIFTå’ŒAKAZEï¼‰åœ¨ç‰¹å®šæ¡ä»¶ä¸‹è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æåœ°ç…§æ˜æ¡ä»¶ä¸‹å¯èƒ½æ€§èƒ½ä¸‹é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04775">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-53a9e27aca224ddd05501032fd960ab3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee678f3c36b9ad7ee17e7bcdcd144202.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Enhancing-Self-Driving-Segmentation-in-Adverse-Weather-Conditions-A-Dual-Uncertainty-Aware-Training-Approach-to-SAM-Optimization"><a href="#Enhancing-Self-Driving-Segmentation-in-Adverse-Weather-Conditions-A-Dual-Uncertainty-Aware-Training-Approach-to-SAM-Optimization" class="headerlink" title="Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A   Dual Uncertainty-Aware Training Approach to SAM Optimization"></a>Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A   Dual Uncertainty-Aware Training Approach to SAM Optimization</h2><p><strong>Authors:Dharsan Ravindran, Kevin Wang, Zhuoyuan Cao, Saleh Abdelrahman, Jeffery Wu</strong></p>
<p>Recent advances in vision foundation models, such as the Segment Anything Model (SAM) and its successor SAM2, have achieved state-of-the-art performance on general image segmentation benchmarks. However, these models struggle in adverse weather conditions where visual ambiguity is high, largely due to their lack of uncertainty quantification. Inspired by progress in medical imaging, where uncertainty-aware training has improved reliability in ambiguous cases, we investigate two approaches to enhance segmentation robustness for autonomous driving. First, we introduce a multi-step finetuning procedure for SAM2 that incorporates uncertainty metrics directly into the loss function, improving overall scene recognition. Second, we adapt the Uncertainty-Aware Adapter (UAT), originally designed for medical image segmentation, to driving contexts. We evaluate both methods on CamVid, BDD100K, and GTA driving datasets. Experiments show that UAT-SAM outperforms standard SAM in extreme weather, while SAM2 with uncertainty-aware loss achieves improved performance across diverse driving scenes. These findings underscore the value of explicit uncertainty modeling for safety-critical autonomous driving in challenging environments. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆå¦‚Segment Anything Modelï¼ˆSAMï¼‰åŠå…¶åç»§è€…SAM2ï¼‰åœ¨é€šç”¨å›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹è¡¨ç°è¾ƒå·®ï¼Œå°¤å…¶åœ¨è§†è§‰æ¨¡ç³Šçš„æƒ…å†µä¸‹ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬ç¼ºä¹ä¸ç¡®å®šæ€§é‡åŒ–ã€‚å—åŒ»å­¦æˆåƒé¢†åŸŸè¿›æ­¥å¯å‘ï¼Œä¸ç¡®å®šæ€§æ„ŸçŸ¥è®­ç»ƒå·²æé«˜äº†æ¨¡ç³Šç—…ä¾‹çš„å¯é æ€§ã€‚æˆ‘ä»¬ç ”ç©¶ä¸¤ç§å¢å¼ºè‡ªåŠ¨é©¾é©¶åˆ†å‰²ç¨³å¥æ€§çš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸ºSAM2å¼•å…¥äº†ä¸€ä¸ªå¤šæ­¥éª¤å¾®è°ƒç¨‹åºï¼Œå°†ä¸ç¡®å®šæ€§æŒ‡æ ‡ç›´æ¥çº³å…¥æŸå¤±å‡½æ•°ï¼Œä»è€Œæé«˜æ•´ä½“åœºæ™¯è¯†åˆ«èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è°ƒæ•´åŸæœ¬ä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²è®¾è®¡çš„Uncertainty-Aware Adapterï¼ˆUATï¼‰ï¼Œä»¥é€‚åº”é©¾é©¶ç¯å¢ƒã€‚æˆ‘ä»¬åœ¨CamVidã€BDD100Kå’ŒGTAé©¾é©¶æ•°æ®é›†ä¸Šè¯„ä¼°äº†è¿™ä¸¤ç§æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒUAT-SAMåœ¨æç«¯å¤©æ°”æ¡ä»¶ä¸‹è¡¨ç°ä¼˜äºæ ‡å‡†SAMï¼Œè€Œå…·æœ‰ä¸ç¡®å®šæ€§æ„ŸçŸ¥æŸå¤±çš„SAM2åœ¨ä¸åŒé©¾é©¶åœºæ™¯ä¸­çš„æ€§èƒ½æœ‰æ‰€æé«˜ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­ï¼Œå¯¹å…³é”®è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿè¿›è¡Œæ˜ç¡®çš„ä¸ç¡®å®šæ€§å»ºæ¨¡çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04735v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>éšç€åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œå°¤å…¶æ˜¯é€šè¿‡ä¸ç¡®å®šæ€§çš„æ„ŸçŸ¥è®­ç»ƒç­–ç•¥çš„å¼•å…¥ï¼Œç°å·²æé«˜SAMæ¨¡å‹å’ŒSAM2æ¨¡å‹ç­‰åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ã€‚é€šè¿‡å¯¹åŸæœ‰æ¨¡å‹è¿›è¡Œå¤šæ­¥éª¤å¾®è°ƒä»¥åŠé€‚é…ä¸ç¡®å®šæ€§æ„ŸçŸ¥é€‚é…å™¨UATçš„åº”ç”¨ï¼Œè¿™ä¸¤è€…åœ¨è‡ªåŠ¨é©¾é©¶çš„ä¸Šä¸‹æ–‡ä¸­å±•ç¤ºå‡ºå…¶å¯¹é©¾é©¶ç¯å¢ƒè¯†åˆ«å’Œå¤©æ°”å¹²æ‰°æŠ‘åˆ¶çš„æå‡ä½œç”¨ã€‚ç ”ç©¶è¡¨æ˜æ˜ç¡®çš„æ¨¡å‹ä¸ç¡®å®šæ€§å¯¹äºç¡®ä¿åœ¨å¤æ‚ç¯å¢ƒä¸­é©¾é©¶çš„å®‰å…¨æ€§è‡³å…³é‡è¦ã€‚ç ”ç©¶è¯æ˜ä¸ç¡®å®šæ€§çš„é‡åŒ–ç­–ç•¥èƒ½æœ‰æ•ˆæ”¹å–„é€šç”¨å›¾åƒåˆ†å‰²æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œæœ‰åŠ©äºåœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„åœºæ™¯è¯†åˆ«ä¸è‡ªä¸»é©¾é©¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ul>
<li>æœ€æ–°è¿›å±•çš„åŒ»å­¦å›¾åƒæŠ€æœ¯æä¾›äº†ä¸ç¡®å®šæ€§çš„æ„ŸçŸ¥è®­ç»ƒç­–ç•¥ï¼Œæœ‰åŠ©äºæé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹ï¼Œä¸ç¡®å®šæ€§æ„ŸçŸ¥è®­ç»ƒå¯¹æ¨¡å‹æ€§èƒ½çš„æå‡å°¤ä¸ºé‡è¦ã€‚è¿™æ˜¯å› ä¸ºæ¶åŠ£å¤©æ°”æ¡ä»¶å¯¼è‡´è§†è§‰æ¨¡ç³Šå’Œä¸ç¡®å®šæ€§å¢åŠ ã€‚å› æ­¤ï¼Œç¼ºä¹ä¸ç¡®å®šæ€§é‡åŒ–çš„æ¨¡å‹å¯èƒ½æ— æ³•å‡†ç¡®å¤„ç†è¿™äº›æƒ…å†µã€‚é€šè¿‡å¯¹æ¨¡å‹çš„æ”¹è¿›å’Œä¼˜åŒ–ï¼Œå¯ä»¥æœ‰æ•ˆåœ°è§£å†³è¿™ä¸€é—®é¢˜ã€‚å¼•å…¥ä¸ç¡®å®šæ€§æ„ŸçŸ¥è®­ç»ƒç­–ç•¥çš„æ¨¡å‹å¯ä»¥æ›´å¥½åœ°é€‚åº”å„ç§ç¯å¢ƒå¹¶å¢å¼ºåœºæ™¯è¯†åˆ«èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-66a1be77bc3026ad2747f2f7d0726d8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b4e1733c22e1a7cfe449890f916ed57.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4fda3411f13f19724023cc7fee490ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8f368cf587481a069d27e0db4611ba9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f18cd453c16cae784b2dc4b936c79919.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d21100b1005c2cf98c1db964b444eba3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8e41667357c220a3c061a116d90c4f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-392f89ecf6cd8c8b4a912603d8dce278.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Drawing2CAD-Sequence-to-Sequence-Learning-for-CAD-Generation-from-Vector-Drawings"><a href="#Drawing2CAD-Sequence-to-Sequence-Learning-for-CAD-Generation-from-Vector-Drawings" class="headerlink" title="Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from   Vector Drawings"></a>Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from   Vector Drawings</h2><p><strong>Authors:Feiwei Qin, Shichao Lu, Junhao Hou, Changmiao Wang, Meie Fang, Ligang Liu</strong></p>
<p>Computer-Aided Design (CAD) generative modeling is driving significant innovations across industrial applications. Recent works have shown remarkable progress in creating solid models from various inputs such as point clouds, meshes, and text descriptions. However, these methods fundamentally diverge from traditional industrial workflows that begin with 2D engineering drawings. The automatic generation of parametric CAD models from these 2D vector drawings remains underexplored despite being a critical step in engineering design. To address this gap, our key insight is to reframe CAD generation as a sequence-to-sequence learning problem where vector drawing primitives directly inform the generation of parametric CAD operations, preserving geometric precision and design intent throughout the transformation process. We propose Drawing2CAD, a framework with three key technical components: a network-friendly vector primitive representation that preserves precise geometric information, a dual-decoder transformer architecture that decouples command type and parameter generation while maintaining precise correspondence, and a soft target distribution loss function accommodating inherent flexibility in CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing, a dataset of paired engineering drawings and parametric CAD models, and conduct thorough experiments to demonstrate the effectiveness of our method. Code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/lllssc/Drawing2CAD">https://github.com/lllssc/Drawing2CAD</a>. </p>
<blockquote>
<p>è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ç”Ÿæˆå»ºæ¨¡æ­£åœ¨æ¨åŠ¨å·¥ä¸šåº”ç”¨çš„é‡å¤§åˆ›æ–°ã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œæ˜¾ç¤ºï¼Œä»ç‚¹äº‘ã€ç½‘æ ¼å’Œæ–‡æœ¬æè¿°ç­‰å„ç§è¾“å…¥åˆ›å»ºå®ä½“æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»æ ¹æœ¬ä¸Šåç¦»äº†å§‹äºäºŒç»´å·¥ç¨‹ç»˜å›¾çš„ä¼ ç»Ÿå·¥ä¸šå·¥ä½œæµç¨‹ã€‚å°½ç®¡ä»äºŒç»´çŸ¢é‡å›¾ä¸­è‡ªåŠ¨ç”Ÿæˆå‚æ•°åŒ–CADæ¨¡å‹æ˜¯å·¥ç¨‹è®¾è®¡ä¸­çš„å…³é”®æ­¥éª¤ï¼Œä½†è¿™ä¸€é¢†åŸŸçš„æ¢ç´¢ä»ç„¶ä¸è¶³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬çš„æ ¸å¿ƒè§è§£æ˜¯å°†CADç”Ÿæˆé‡æ–°æ„å»ºä¸ºä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„å­¦ä¹ é—®é¢˜ï¼Œå…¶ä¸­çŸ¢é‡ç»˜å›¾å…ƒç´ ç›´æ¥ä¸ºå‚æ•°åŒ–CADæ“ä½œç”Ÿæˆæä¾›ä¿¡æ¯ï¼Œå¹¶åœ¨æ•´ä¸ªè½¬æ¢è¿‡ç¨‹ä¸­ä¿æŒå‡ ä½•ç²¾åº¦å’Œè®¾è®¡æ„å›¾ã€‚æˆ‘ä»¬æå‡ºäº†Drawing2CADæ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªå…³é”®æŠ€æœ¯ç»„ä»¶ï¼šä¸€ç§å‹å¥½çš„ç½‘ç»œçŸ¢é‡å…ƒç´ è¡¨ç¤ºï¼Œä¿ç•™ç²¾ç¡®çš„å‡ ä½•ä¿¡æ¯ï¼›ä¸€ç§åŒè§£ç å™¨è½¬æ¢å™¨æ¶æ„ï¼Œè¯¥æ¶æ„åœ¨ä¿æŒç²¾ç¡®å¯¹åº”çš„åŒæ—¶è§£è€¦å‘½ä»¤ç±»å‹å’Œå‚æ•°ç”Ÿæˆï¼›ä¸€ç§é€‚åº”CADå‚æ•°å›ºæœ‰çµæ´»æ€§çš„è½¯ç›®æ ‡åˆ†å¸ƒæŸå¤±å‡½æ•°ã€‚ä¸ºäº†è®­ç»ƒå’Œè¯„ä¼°Drawing2CADï¼Œæˆ‘ä»¬åˆ›å»ºäº†CAD-VGDrawingæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«é…å¯¹çš„å·¥ç¨‹å›¾çº¸å’Œå‚æ•°åŒ–CADæ¨¡å‹ï¼Œå¹¶é€šè¿‡å…¨é¢çš„å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lllssc/Drawing2CAD">https://github.com/lllssc/Drawing2CAD</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18733v4">PDF</a> Accepted to ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰çš„ç”Ÿæˆå»ºæ¨¡æ­£åœ¨æ¨åŠ¨å·¥ä¸šåº”ç”¨çš„é‡å¤§åˆ›æ–°ã€‚è¿‘æœŸä½œå“å±•ç¤ºäº†ä»ç‚¹äº‘ã€ç½‘æ ¼å’Œæ–‡æœ¬æè¿°ç­‰åˆ›å»ºå®ä½“æ¨¡å‹çš„æ˜¾è‘—è¿›å±•ï¼Œä½†ä¸ä¼ ç»Ÿä»äºŒç»´å·¥ç¨‹å›¾çº¸å¼€å§‹çš„å·¥ä¸šå·¥ä½œæµç¨‹ç›¸æ¯”ï¼Œè¿™äº›æ–¹æ³•ä»å­˜åœ¨æ˜æ˜¾å·®å¼‚ã€‚é’ˆå¯¹è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡çš„å…³é”®è§è§£æ˜¯å°†CADç”Ÿæˆé‡æ–°æ„å»ºä¸ºåºåˆ—åˆ°åºåˆ—çš„å­¦ä¹ é—®é¢˜ï¼Œå…¶ä¸­çŸ¢é‡ç»˜å›¾åŸå§‹å…ƒç´ ç›´æ¥å‘ŠçŸ¥å‚æ•°åŒ–CADæ“ä½œç”Ÿæˆè¿‡ç¨‹ï¼Œä¿æŒå‡ ä½•ç²¾åº¦å’Œè®¾è®¡æ„å›¾åœ¨è½¬æ¢è¿‡ç¨‹ä¸­çš„ä¿ç•™ã€‚æœ¬æ–‡æå‡ºDrawing2CADæ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªå…³é”®æŠ€æœ¯éƒ¨åˆ†ï¼šç½‘ç»œå‹å¥½çš„çŸ¢é‡åŸå§‹å…ƒç´ è¡¨ç¤ºæ³•ä¿ç•™ç²¾ç¡®å‡ ä½•ä¿¡æ¯ï¼ŒåŒè§£ç å™¨è½¬æ¢æ¶æ„å°†å‘½ä»¤ç±»å‹å’Œå‚æ•°ç”Ÿæˆè§£è€¦åŒæ—¶ä¿æŒç²¾ç¡®å¯¹åº”å…³ç³»ï¼Œä»¥åŠé€‚åº”CADå‚æ•°å›ºæœ‰çµæ´»æ€§çš„è½¯ç›®æ ‡åˆ†å¸ƒæŸå¤±å‡½æ•°ã€‚ä¸ºè®­ç»ƒå’Œè¯„ä¼°Drawing2CADï¼Œæˆ‘ä»¬åˆ›å»ºäº†CAD-VGDrawingæ•°æ®é›†ï¼ŒåŒ…å«é…å¯¹å·¥ç¨‹å›¾çº¸å’Œå‚æ•°åŒ–CADæ¨¡å‹ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨é“¾æ¥ä¸­æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CADç”Ÿæˆå»ºæ¨¡åœ¨å·¥ä¸šåº”ç”¨ä¸­å…·æœ‰æ˜¾è‘—åˆ›æ–°æ„ä¹‰ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ä»ç‚¹äº‘ã€ç½‘æ ¼å’Œæ–‡æœ¬æè¿°åˆ›å»ºå®ä½“æ¨¡å‹ï¼Œä½†æœªèƒ½æ¶µç›–ä¼ ç»Ÿå·¥ä¸šå·¥ä½œæµç¨‹ä¸­çš„äºŒç»´å·¥ç¨‹å›¾çº¸ã€‚</li>
<li>Drawing2CADæ¡†æ¶è§£å†³äº†ä»äºŒç»´çŸ¢é‡å›¾çº¸è‡ªåŠ¨ç”Ÿæˆå‚æ•°åŒ–CADæ¨¡å‹çš„é—®é¢˜ï¼Œä¿ç•™äº†å‡ ä½•ç²¾åº¦å’Œè®¾è®¡æ„å›¾ã€‚</li>
<li>Drawing2CADæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®æŠ€æœ¯éƒ¨åˆ†ï¼šçŸ¢é‡åŸå§‹å…ƒç´ è¡¨ç¤ºæ³•ã€åŒè§£ç å™¨è½¬æ¢æ¶æ„å’Œè½¯ç›®æ ‡åˆ†å¸ƒæŸå¤±å‡½æ•°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-257dfc43466d6c682d0f193eb94fa95e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9a6f608734611a8290a155c28d0607b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4a777e3ed07fc578600ff59e1b92881d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Prompt-based-Multimodal-Semantic-Communication-for-Multi-spectral-Image-Segmentation"><a href="#Prompt-based-Multimodal-Semantic-Communication-for-Multi-spectral-Image-Segmentation" class="headerlink" title="Prompt-based Multimodal Semantic Communication for Multi-spectral Image   Segmentation"></a>Prompt-based Multimodal Semantic Communication for Multi-spectral Image   Segmentation</h2><p><strong>Authors:Haoshuo Zhang, Yufei Bo, Hongwei Zhang, Meixia Tao</strong></p>
<p>Multimodal semantic communication has gained widespread attention due to its ability to enhance downstream task performance. A key challenge in such systems is the effective fusion of features from different modalities, which requires the extraction of rich and diverse semantic representations from each modality. To this end, we propose ProMSC-MIS, a Prompt-based Multimodal Semantic Communication system for Multi-spectral Image Segmentation. Specifically, we propose a pre-training algorithm where features from one modality serve as prompts for another, guiding unimodal semantic encoders to learn diverse and complementary semantic representations. We further introduce a semantic fusion module that combines cross-attention mechanisms and squeeze-and-excitation (SE) networks to effectively fuse cross-modal features. Simulation results show that ProMSC-MIS significantly outperforms benchmark methods across various channel-source compression levels, while maintaining low computational complexity and storage overhead. Our scheme has great potential for applications such as autonomous driving and nighttime surveillance. </p>
<blockquote>
<p>å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡å› å…¶èƒ½å¢å¼ºä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚æ­¤ç±»ç³»ç»Ÿçš„å…³é”®æŒ‘æˆ˜åœ¨äºæœ‰æ•ˆåœ°èåˆä¸åŒæ¨¡æ€çš„ç‰¹å¾ï¼Œè¿™éœ€è¦ä»æ¯ä¸ªæ¨¡æ€ä¸­æå–ä¸°å¯Œå’Œå¤šæ ·çš„è¯­ä¹‰è¡¨ç¤ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæç¤ºçš„å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡ç³»ç»ŸProMSC-MISï¼ˆç”¨äºå¤šå…‰è°±å›¾åƒåˆ†å‰²ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é¢„è®­ç»ƒç®—æ³•ï¼Œå…¶ä¸­ä¸€ä¸ªæ¨¡æ€çš„ç‰¹å¾ä¸ºå¦ä¸€ä¸ªæ¨¡æ€æä¾›æç¤ºï¼Œå¼•å¯¼å•æ¨¡æ€è¯­ä¹‰ç¼–ç å™¨å­¦ä¹ å¤šæ ·ä¸”äº’è¡¥çš„è¯­ä¹‰è¡¨ç¤ºã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªè¯­ä¹‰èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—ç»“åˆäº†äº¤å‰æ³¨æ„æœºåˆ¶å’ŒæŒ¤å‹æ¿€å‘ï¼ˆSEï¼‰ç½‘ç»œï¼Œä»¥æœ‰æ•ˆåœ°èåˆè·¨æ¨¡æ€ç‰¹å¾ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œåœ¨å„ç§é€šé“æºå‹ç¼©çº§åˆ«ä¸Šï¼ŒProMSC-MISæ˜æ˜¾ä¼˜äºåŸºå‡†æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„è®¡ç®—å¤æ‚æ€§å’Œå­˜å‚¨å¼€é”€ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆåœ¨è‡ªåŠ¨é©¾é©¶å’Œå¤œé—´ç›‘æ§ç­‰åº”ç”¨ä¸­å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.17920v2">PDF</a> The full-length version, arXiv:2508.20057, has been updated</p>
<p><strong>æ‘˜è¦</strong><br>    åŸºäºæç¤ºçš„å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡ç³»ç»Ÿç”¨äºå¤šå…‰è°±å›¾åƒåˆ†å‰²ï¼Œæå‡ºProMSC-MISç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé€šè¿‡è·¨æ¨¡æ€ç‰¹å¾èåˆæå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼Œé‡‡ç”¨é¢„è®­ç»ƒç®—æ³•å®ç°ä¸åŒæ¨¡æ€ç‰¹å¾çš„æœ‰æ•ˆèåˆï¼Œå¹¶å¼•å…¥è¯­ä¹‰èåˆæ¨¡å—ç»“åˆäº¤å‰æ³¨æ„åŠ›æœºåˆ¶å’ŒæŒ¤å‹æ¿€åŠ±ç½‘ç»œè¿›è¡Œè·¨æ¨¡æ€ç‰¹å¾çš„æœ‰æ•ˆèåˆã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼ŒProMSC-MISåœ¨å„ç§é€šé“æºå‹ç¼©çº§åˆ«ä¸Šæ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œä¸”è®¡ç®—å¤æ‚åº¦å’Œå­˜å‚¨å¼€é”€è¾ƒä½ï¼Œå…·æœ‰è‡ªåŠ¨é©¾é©¶å’Œå¤œé—´ç›‘æ§ç­‰åº”ç”¨çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡å› å…¶èƒ½æå‡ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>ProMSC-MISç³»ç»Ÿæ˜¯ä¸€ç§åŸºäºæç¤ºçš„å¤šæ¨¡æ€è¯­ä¹‰é€šä¿¡ç³»ç»Ÿï¼Œç”¨äºå¤šå…‰è°±å›¾åƒåˆ†å‰²ã€‚</li>
<li>æå‡ºé¢„è®­ç»ƒç®—æ³•å®ç°ä¸åŒæ¨¡æ€ç‰¹å¾çš„æœ‰æ•ˆèåˆã€‚</li>
<li>å¼•å…¥è¯­ä¹‰èåˆæ¨¡å—ç»“åˆäº¤å‰æ³¨æ„åŠ›æœºåˆ¶å’ŒæŒ¤å‹æ¿€åŠ±ç½‘ç»œã€‚</li>
<li>ä»¿çœŸç»“æœè¡¨æ˜ProMSC-MISåœ¨å¤šç§é€šé“æºå‹ç¼©çº§åˆ«ä¸Šæ€§èƒ½ä¼˜è¶Šã€‚</li>
<li>ProMSC-MISç³»ç»Ÿå…·æœ‰ä½è®¡ç®—å¤æ‚åº¦å’Œå­˜å‚¨å¼€é”€çš„ç‰¹ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.17920">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7f8128674e050dda63db32a566a2d88e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-906df5e6d0177bd26a0e640d7a7d646a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa1fdbffb2b5201107cec69ec7190f54.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b6f854056bd2c18f713e13643a671e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fdc3787ba76f3ce1a274cadeba405bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b652c2b2e1b6f929579c4680a03edfb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-024e1be695b44c9ebc5041965db187fe.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Multimodal-Medical-Endoscopic-Image-Analysis-via-Progressive-Disentangle-aware-Contrastive-Learning"><a href="#Multimodal-Medical-Endoscopic-Image-Analysis-via-Progressive-Disentangle-aware-Contrastive-Learning" class="headerlink" title="Multimodal Medical Endoscopic Image Analysis via Progressive   Disentangle-aware Contrastive Learning"></a>Multimodal Medical Endoscopic Image Analysis via Progressive   Disentangle-aware Contrastive Learning</h2><p><strong>Authors:Junhao Wu, Yun Li, Junhao Li, Jingliang Bian, Xiaomao Fan, Wenbin Lei, Ruxin Wang</strong></p>
<p>Accurate segmentation of laryngo-pharyngeal tumors is crucial for precise diagnosis and effective treatment planning. However, traditional single-modality imaging methods often fall short of capturing the complex anatomical and pathological features of these tumors. In this study, we present an innovative multi-modality representation learning framework based on the &#96;Align-Disentangle-Fusionâ€™ mechanism that seamlessly integrates 2D White Light Imaging (WLI) and Narrow Band Imaging (NBI) pairs to enhance segmentation performance. A cornerstone of our approach is multi-scale distribution alignment, which mitigates modality discrepancies by aligning features across multiple transformer layers. Furthermore, a progressive feature disentanglement strategy is developed with the designed preliminary disentanglement and disentangle-aware contrastive learning to effectively separate modality-specific and shared features, enabling robust multimodal contrastive learning and efficient semantic fusion. Comprehensive experiments on multiple datasets demonstrate that our method consistently outperforms state-of-the-art approaches, achieving superior accuracy across diverse real clinical scenarios. </p>
<blockquote>
<p>å–‰å’½è‚¿ç˜¤å‡†ç¡®åˆ†å‰²å¯¹äºç²¾ç¡®è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„å•ä¸€æˆåƒæ¨¡å¼æ–¹æ³•å¾€å¾€éš¾ä»¥æ•æ‰è¿™äº›è‚¿ç˜¤çš„å¤æ‚è§£å‰–å’Œç—…ç†ç‰¹å¾ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºâ€œå¯¹é½-åˆ†ç¦»-èåˆâ€æœºåˆ¶çš„åˆ›æ–°å¤šæ¨¡å¼è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œæ— ç¼é›†æˆäº†2Dç™½å…‰æˆåƒï¼ˆWLIï¼‰å’Œçª„å¸¦æˆåƒï¼ˆNBIï¼‰å¯¹ï¼Œä»¥æé«˜åˆ†å‰²æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„ä¸€ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†æ˜¯å¤šå°ºåº¦åˆ†å¸ƒå¯¹é½ï¼Œå®ƒé€šè¿‡å¯¹é½å¤šä¸ªè½¬æ¢å™¨å±‚ä¸­çš„ç‰¹å¾æ¥ç¼“è§£æ¨¡å¼å·®å¼‚ã€‚æ­¤å¤–ï¼Œå¼€å‘äº†ä¸€ç§æ¸è¿›çš„ç‰¹å¾åˆ†ç¦»ç­–ç•¥ï¼Œé€šè¿‡è®¾è®¡åˆæ­¥åˆ†ç¦»å’Œè§£çº ç¼ æ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ ï¼Œæœ‰æ•ˆåœ°åˆ†ç¦»äº†æ¨¡å¼ç‰¹å®šç‰¹å¾å’Œå…±äº«ç‰¹å¾ï¼Œå®ç°äº†ç¨³å¥çš„å¤šæ¨¡å¼å¯¹æ¯”å­¦ä¹ å’Œæœ‰æ•ˆçš„è¯­ä¹‰èåˆã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨å¤šç§çœŸå®ä¸´åºŠåœºæ™¯ä¸­å®ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16882v1">PDF</a> 12 pages,6 figures, 6 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºâ€å¯¹é½-åˆ†ç¦»-èåˆâ€æœºåˆ¶çš„å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å‡†ç¡®åˆ†å‰²å–‰å’½è‚¿ç˜¤ï¼Œä¸ºç²¾ç¡®è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’æä¾›æ”¯æŒã€‚è¯¥æ¡†æ¶æ— ç¼é›†æˆäº†2Dç™½å…‰æˆåƒï¼ˆWLIï¼‰å’Œçª„å¸¦æˆåƒï¼ˆNBIï¼‰å¯¹ï¼Œä»¥æé«˜åˆ†å‰²æ€§èƒ½ã€‚å¤šå°ºåº¦åˆ†å¸ƒå¯¹é½æ˜¯è¯¥æ–¹æ³•çš„æ ¸å¿ƒï¼Œé€šè¿‡å¤šä¸ªtransformerå±‚ç‰¹å¾å¯¹é½ï¼Œç¼“è§£äº†æ¨¡æ€å·®å¼‚ã€‚åŒæ—¶ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼€å‘äº†ä¸€ç§æ¸è¿›çš„ç‰¹å¾åˆ†ç¦»ç­–ç•¥ï¼Œé€šè¿‡åˆæ­¥çš„ç‰¹å¾åˆ†ç¦»å’Œæ„ŸçŸ¥åˆ†ç¦»çš„å¯¹æ¯”å­¦ä¹ ï¼Œæœ‰æ•ˆåœ°åˆ†ç¦»äº†æ¨¡æ€ç‰¹å®šç‰¹å¾å’Œå…±äº«ç‰¹å¾ï¼Œå®ç°äº†ç¨³å¥çš„å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ å’Œé«˜æ•ˆçš„è¯­ä¹‰èåˆã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾ƒå…¶ä»–å‰æ²¿æŠ€æœ¯è¡¨ç°æ›´ä¼˜ï¼Œåœ¨å„ç§çœŸå®ä¸´åºŠåœºæ™¯ä¸­å‡å®ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å–‰å’½è‚¿ç˜¤å‡†ç¡®åˆ†å‰²å¯¹ç²¾ç¡®è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿå•ä¸€æˆåƒæ¨¡å¼åœ¨æ•æ‰è‚¿ç˜¤å¤æ‚è§£å‰–å’Œç—…ç†ç‰¹å¾æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œé›†æˆäº†2Dç™½å…‰æˆåƒå’Œçª„å¸¦æˆåƒï¼Œä»¥æé«˜åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>å¤šå°ºåº¦åˆ†å¸ƒå¯¹é½æ˜¯è¯¥æ¡†æ¶çš„æ ¸å¿ƒï¼Œæœ‰åŠ©äºç¼“è§£ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>æ¸è¿›çš„ç‰¹å¾åˆ†ç¦»ç­–ç•¥èƒ½æœ‰æ•ˆåˆ†ç¦»æ¨¡æ€ç‰¹å®šç‰¹å¾å’Œå…±äº«ç‰¹å¾ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å¯¹æ¯”å­¦ä¹ å’Œè¯­ä¹‰èåˆå®ç°äº†ç¨³å¥çš„å¤šæ¨¡æ€å­¦ä¹ ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾ƒå…¶ä»–æŠ€æœ¯è¡¨ç°ä¼˜è¶Šï¼Œå…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16882">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-63e3c2352f5743d0064ebe66876104f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0688f3c97906c94c21448d25ac776ebb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-319276a72c9eb46a34d0671d7f1109f0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TPA-Temporal-Prompt-Alignment-for-Fetal-Congenital-Heart-Defect-Classification"><a href="#TPA-Temporal-Prompt-Alignment-for-Fetal-Congenital-Heart-Defect-Classification" class="headerlink" title="TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect   Classification"></a>TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect   Classification</h2><p><strong>Authors:Darya Taratynova, Alya Almsouti, Beknur Kalmakhanbet, Numan Saeed, Mohammad Yaqub</strong></p>
<p>Congenital heart defect (CHD) detection in ultrasound videos is hindered by image noise and probe positioning variability. While automated methods can reduce operator dependence, current machine learning approaches often neglect temporal information, limit themselves to binary classification, and do not account for prediction calibration. We propose Temporal Prompt Alignment (TPA), a method leveraging foundation image-text model and prompt-aware contrastive learning to classify fetal CHD on cardiac ultrasound videos. TPA extracts features from each frame of video subclips using an image encoder, aggregates them with a trainable temporal extractor to capture heart motion, and aligns the video representation with class-specific text prompts via a margin-hinge contrastive loss. To enhance calibration for clinical reliability, we introduce a Conditional Variational Autoencoder Style Modulation (CVAESM) module, which learns a latent style vector to modulate embeddings and quantifies classification uncertainty. Evaluated on a private dataset for CHD detection and on a large public dataset, EchoNet-Dynamic, for systolic dysfunction, TPA achieves state-of-the-art macro F1 scores of 85.40% for CHD diagnosis, while also reducing expected calibration error by 5.38% and adaptive ECE by 6.8%. On EchoNet-Dynamicâ€™s three-class task, it boosts macro F1 by 4.73% (from 53.89% to 58.62%). Temporal Prompt Alignment (TPA) is a framework for fetal congenital heart defect (CHD) classification in ultrasound videos that integrates temporal modeling, prompt-aware contrastive learning, and uncertainty quantification. </p>
<blockquote>
<p>å…ˆå¤©æ€§å¿ƒè„ç¼ºé™·ï¼ˆCHDï¼‰åœ¨è¶…å£°è§†é¢‘ä¸­çš„æ£€æµ‹å—åˆ°å›¾åƒå™ªå£°å’Œæ¢å¤´å®šä½å˜åŒ–çš„å½±å“ã€‚è™½ç„¶è‡ªåŠ¨åŒ–æ–¹æ³•å¯ä»¥å‡å°‘å¯¹æ“ä½œå‘˜çš„ä¾èµ–ï¼Œä½†å½“å‰çš„æœºå™¨å­¦ä¹ æ–¹æ³•å¸¸å¸¸å¿½ç•¥äº†æ—¶é—´ä¿¡æ¯ï¼Œä»…é™äºäºŒå…ƒåˆ†ç±»ï¼Œå¹¶ä¸”æ²¡æœ‰è€ƒè™‘åˆ°é¢„æµ‹æ ¡å‡†ã€‚æˆ‘ä»¬æå‡ºäº†â€œæ—¶é—´æç¤ºå¯¹é½â€ï¼ˆTPAï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åŸºç¡€å›¾åƒæ–‡æœ¬æ¨¡å‹å’Œæç¤ºæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ æ¥å¯¹èƒå„¿CHDè¿›è¡Œå¿ƒè„è¶…å£°è§†é¢‘åˆ†ç±»ã€‚TPAé€šè¿‡å›¾åƒç¼–ç å™¨ä»è§†é¢‘çš„æ¯ä¸ªç‰‡æ®µä¸­æå–ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨å¯è®­ç»ƒçš„æ—¶é—´æå–å™¨æ¥æ•æ‰å¿ƒè„è¿åŠ¨ï¼Œç„¶åé€šè¿‡è¾¹è·é“°é“¾å¯¹æ¯”æŸå¤±å°†è§†é¢‘è¡¨ç¤ºä¸ç‰¹å®šç±»åˆ«çš„æ–‡æœ¬æç¤ºå¯¹é½ã€‚ä¸ºäº†æé«˜ä¸´åºŠå¯é æ€§çš„æ ¡å‡†ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¡ä»¶å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨é£æ ¼è°ƒåˆ¶ï¼ˆCVAESMï¼‰æ¨¡å—ï¼Œå®ƒå­¦ä¹ æ½œåœ¨çš„é£æ ¼å‘é‡æ¥è°ƒåˆ¶åµŒå…¥å¹¶é‡åŒ–åˆ†ç±»çš„ä¸ç¡®å®šæ€§ã€‚åœ¨é’ˆå¯¹CHDæ£€æµ‹çš„ç§æœ‰æ•°æ®é›†å’Œç”¨äºæ”¶ç¼©åŠŸèƒ½éšœç¢çš„å¤§å‹å…¬å…±æ•°æ®é›†EchoNet-Dynamicä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒTPAè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å®è§‚F1åˆ†æ•°ï¼ŒCHDè¯Šæ–­çš„F1åˆ†æ•°ä¸º85.40%ï¼ŒåŒæ—¶é™ä½äº†æœŸæœ›æ ¡å‡†è¯¯å·®5.38%å’Œè‡ªé€‚åº”ECE 6.8%ã€‚åœ¨EchoNet-Dynamicçš„ä¸‰ç±»ä»»åŠ¡ä¸­ï¼Œå®ƒæé«˜äº†å®è§‚F1åˆ†æ•°4.73%ï¼ˆä»53.89%æé«˜åˆ°58.62%ï¼‰ã€‚æ—¶é—´æç¤ºå¯¹é½ï¼ˆTPAï¼‰æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºè¶…å£°è§†é¢‘ä¸­èƒå„¿å…ˆå¤©æ€§å¿ƒè„ç¼ºé™·ï¼ˆCHDï¼‰çš„åˆ†ç±»ï¼Œå®ƒé›†æˆäº†æ—¶é—´å»ºæ¨¡ã€æç¤ºæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ å’Œä¸ç¡®å®šæ€§é‡åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15298v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå›¾åƒ-æ–‡æœ¬æ¨¡å‹å’Œæç¤ºæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ çš„å…ˆå¤©æ€§å¿ƒè„ç¼ºé™·ï¼ˆCHDï¼‰è§†é¢‘åˆ†ç±»æ–¹æ³•ï¼Œç§°ä¸ºTemporal Prompt Alignmentï¼ˆTPAï¼‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨è§†é¢‘å­å‰ªè¾‘çš„æ¯ä¸€å¸§ç‰¹å¾ï¼Œç»“åˆå¿ƒè„è¿åŠ¨çš„æ—¶é—´ä¿¡æ¯ï¼Œé€šè¿‡ä¸ç±»åˆ«ç‰¹å®šçš„æ–‡æœ¬æç¤ºå¯¹é½çš„è§†é¢‘è¡¨ç¤ºï¼Œå®ç°CHDçš„è‡ªåŠ¨åˆ†ç±»ã€‚ä¸ºæé«˜ä¸´åºŠå¯é æ€§ï¼Œå¼•å…¥æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨é£æ ¼è°ƒåˆ¶æ¨¡å—ï¼Œå­¦ä¹ æ½œåœ¨çš„é£æ ¼å‘é‡ä»¥è°ƒåˆ¶åµŒå…¥å¹¶é‡åŒ–åˆ†ç±»ä¸ç¡®å®šæ€§ã€‚åœ¨ç§äººæ•°æ®é›†å’Œå…¬å…±æ•°æ®é›†EchoNet-Dynamicä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒTPAåœ¨CHDæ£€æµ‹æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å®è§‚F1åˆ†æ•°ï¼ŒåŒæ—¶é™ä½äº†é¢„æœŸæ ¡å‡†è¯¯å·®å’Œè‡ªé€‚åº”ECEã€‚æ€»ä½“è€Œè¨€ï¼ŒTPAæ˜¯ä¸€ä¸ªé›†æˆäº†æ—¶é—´å»ºæ¨¡ã€æç¤ºæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ å’Œä¸ç¡®å®šæ€§é‡åŒ–çš„æ¡†æ¶ï¼Œç”¨äºèƒå„¿å…ˆå¤©æ€§å¿ƒè„ç¼ºé™·ï¼ˆCHDï¼‰çš„è¶…å£°è§†é¢‘åˆ†ç±»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>1.TPAåˆ©ç”¨å›¾åƒ-æ–‡æœ¬æ¨¡å‹å’Œæç¤ºæ„ŸçŸ¥å¯¹æ¯”å­¦ä¹ è¿›è¡Œåˆ†ç±»ã€‚</p>
<p>2.TPAæå–è§†é¢‘æ¯ä¸€å¸§çš„ç‰¹å¾å¹¶ç»“åˆå¿ƒè„è¿åŠ¨çš„æ—¶é—´ä¿¡æ¯ã€‚</p>
<p>3.è§†é¢‘è¡¨ç¤ºä¸ç±»åˆ«ç‰¹å®šçš„æ–‡æœ¬æç¤ºé€šè¿‡è¾¹ç¼˜é“°é“¾å¯¹æ¯”æŸå¤±è¿›è¡Œå¯¹é½ã€‚</p>
<p>4.å¼•å…¥æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨é£æ ¼è°ƒåˆ¶æ¨¡å—ä»¥æé«˜åˆ†ç±»çš„å¯é æ€§å¹¶é‡åŒ–ä¸ç¡®å®šæ€§ã€‚</p>
<p>5.åœ¨ç§äººæ•°æ®é›†å’Œå…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºTPAè¡¨ç°ä¼˜è¶Šã€‚</p>
<p>6.TPAè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å®è§‚F1åˆ†æ•°ï¼Œé™ä½é¢„æœŸæ ¡å‡†è¯¯å·®å’Œè‡ªé€‚åº”ECEã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15298">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5c8ec27b1ef769ae35c3fd749ae3fae1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9c9db43549a41cab485975de73cf808.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1009bc92687fed60d1a4721a21222e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52c859f24dd0635029fbebedaf093932.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Optimizing-Convolution-Direction-and-Template-Selection-for-Difference-Image-Analysis"><a href="#Optimizing-Convolution-Direction-and-Template-Selection-for-Difference-Image-Analysis" class="headerlink" title="Optimizing Convolution Direction and Template Selection for Difference   Image Analysis"></a>Optimizing Convolution Direction and Template Selection for Difference   Image Analysis</h2><p><strong>Authors:Rodrigo Angulo, Armin Rest, William P. Blair, Jacob Jencson, David A. Coulter, Qinan Wang, Ryan J. Foley, Charles D. Kilpatrick, Xiaolong Li, CÃ©sar Rojas-Bravo, Anthony L. Piro</strong></p>
<p>Difference image analysis (DIA) is a powerful tool for studying time-variable phenomena, and has been used by many time-domain surveys. Most DIA algorithms involve matching the spatially-varying PSF shape between science and template images, and then convolving that shape in one image to match the other. The wrong choice of which image to convolve can introduce one of the largest sources of artifacts in the final difference image. We introduce a quantitative metric to determine the optimal convolution direction that depends not only on the sharpness of the images measured by their FWHM, but also on their exposure depths. With this metric, the optimal convolution direction can be determined a priori, depending only on the FWHM and depth of the images. This not only simplifies the process, but also makes it more robust and less prone to creating sub-optimal difference images due to the wrong choice of the convolution direction. As an additional benefit, for a large set of images, we define a Figure-of-Merit based on this metric, which allows us to rank a list of images and determine the ones best suited to be used as templates, thus streamlining and automating the data reduction process. </p>
<blockquote>
<p>å·®å¼‚å›¾åƒåˆ†æï¼ˆDIAï¼‰æ˜¯ç ”ç©¶æ—¶å˜ç°è±¡çš„æœ‰åŠ›å·¥å…·ï¼Œå·²è¢«è®¸å¤šæ—¶åŸŸè°ƒæŸ¥æ‰€ä½¿ç”¨ã€‚å¤§å¤šæ•°DIAç®—æ³•éƒ½æ¶‰åŠåŒ¹é…ç§‘å­¦å’Œæ¨¡æ¿å›¾åƒä¹‹é—´ç©ºé—´å˜åŒ–çš„PSFå½¢çŠ¶ï¼Œç„¶åå°†è¯¥å½¢çŠ¶å·ç§¯ä»¥åŒ¹é…å¦ä¸€ä¸ªå›¾åƒã€‚é€‰æ‹©é”™è¯¯çš„å›¾åƒè¿›è¡Œå·ç§¯å¯èƒ½ä¼šå¼•å…¥æœ€ç»ˆå·®å¼‚å›¾åƒä¸­æœ€å¤§çš„ä¼ªå½±æ¥æºä¹‹ä¸€ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå®šé‡æŒ‡æ ‡ï¼Œä»¥ç¡®å®šæœ€ä½³çš„å·ç§¯æ–¹å‘ï¼Œè¯¥æ–¹å‘ä¸ä»…å–å†³äºé€šè¿‡å…¶FWHMæµ‹é‡çš„å›¾åƒæ¸…æ™°åº¦ï¼Œè¿˜å–å†³äºå®ƒä»¬çš„æ›å…‰æ·±åº¦ã€‚ä½¿ç”¨è¿™ä¸ªæŒ‡æ ‡ï¼Œå¯ä»¥æ ¹æ®å›¾åƒçš„FWHMå’Œæ·±åº¦é¢„å…ˆç¡®å®šæœ€ä½³çš„å·ç§¯æ–¹å‘ã€‚è¿™ä¸ä»…ç®€åŒ–äº†æµç¨‹ï¼Œè€Œä¸”ä½¿å…¶æ›´åŠ ç¨³å¥ï¼Œå¹¶ä¸”ç”±äºé€‰æ‹©äº†é”™è¯¯çš„å·ç§¯æ–¹å‘è€Œä¸å¤ªå®¹æ˜“äº§ç”Ÿæ¬¡ä¼˜å·®å¼‚å›¾åƒã€‚ä½œä¸ºé™„åŠ å¥½å¤„ï¼Œå¯¹äºå¤§é‡å›¾åƒï¼Œæˆ‘ä»¬åŸºäºè¯¥æŒ‡æ ‡å®šä¹‰äº†ä¸€ä¸ªå“è´¨å› æ•°ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå¯¹å›¾åƒåˆ—è¡¨è¿›è¡Œæ’åï¼Œå¹¶ç¡®å®šæœ€é€‚åˆç”¨ä½œæ¨¡æ¿çš„å›¾åƒï¼Œä»è€Œç®€åŒ–å’Œè‡ªåŠ¨åŒ–æ•°æ®ç¼©å‡è¿‡ç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10155v2">PDF</a> 17 pages, 11 figures</p>
<p><strong>æ‘˜è¦</strong><br>     å·®å¼‚å›¾åƒåˆ†æï¼ˆDIAï¼‰æ˜¯ç ”ç©¶æ—¶å˜ç°è±¡çš„æœ‰åŠ›å·¥å…·ï¼Œå·²è¢«è®¸å¤šæ—¶åŸŸè°ƒæŸ¥æ‰€ä½¿ç”¨ã€‚å¤§å¤šæ•°DIAç®—æ³•æ¶‰åŠåŒ¹é…ç§‘å­¦å’Œæ¨¡æ¿å›¾åƒä¹‹é—´ç©ºé—´å˜åŒ–çš„ç‚¹æ‰©æ•£å‡½æ•°ï¼ˆPSFï¼‰å½¢çŠ¶ï¼Œç„¶åå°†è¯¥å½¢çŠ¶å·ç§¯ä»¥åŒ¹é…å¦ä¸€å›¾åƒã€‚é€‰æ‹©å“ªä¸ªå›¾åƒè¿›è¡Œå·ç§¯çš„é”™è¯¯é€‰æ‹©å¯èƒ½æ˜¯æœ€ç»ˆå·®å¼‚å›¾åƒä¸­æœ€å¤§çš„ä¼ªå½±æ¥æºä¹‹ä¸€ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å®šé‡æŒ‡æ ‡æ¥ç¡®å®šæœ€ä½³å·ç§¯æ–¹å‘ï¼Œè¯¥æ–¹å‘ä¸ä»…å–å†³äºé€šè¿‡å…¶FWHMæµ‹é‡çš„å›¾åƒæ¸…æ™°åº¦ï¼Œè¿˜å–å†³äºå®ƒä»¬çš„æ›å…‰æ·±åº¦ã€‚é€šè¿‡è¿™ç§æŒ‡æ ‡ï¼Œå¯ä»¥äº‹å…ˆç¡®å®šæœ€ä½³å·ç§¯æ–¹å‘ï¼Œè¿™å–å†³äºå›¾åƒçš„FWHMå’Œæ·±åº¦ã€‚è¿™ä¸ä»…ç®€åŒ–äº†æµç¨‹ï¼Œè€Œä¸”ä½¿å…¶æ›´åŠ ç¨³å¥ï¼Œä¸å¤ªå®¹æ˜“äº§ç”Ÿç”±äºå·ç§¯æ–¹å‘é€‰æ‹©é”™è¯¯è€Œå¯¼è‡´çš„ä¸ç†æƒ³çš„å·®å¼‚å›¾åƒã€‚æ­¤å¤–ï¼Œå¯¹äºå¤§é‡å›¾åƒï¼Œæˆ‘ä»¬åŸºäºè¯¥æŒ‡æ ‡å®šä¹‰äº†ä¸€ä¸ªå“è´¨å› æ•°ï¼Œå¯ä»¥è®©æˆ‘ä»¬å¯¹å›¾åƒåˆ—è¡¨è¿›è¡Œæ’åå¹¶ç¡®å®šæœ€é€‚åˆç”¨ä½œæ¨¡æ¿çš„å›¾åƒï¼Œä»è€Œç®€åŒ–å’Œè‡ªåŠ¨åŒ–æ•°æ®ç¼©å‡è¿‡ç¨‹ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>å·®å¼‚å›¾åƒåˆ†æï¼ˆDIAï¼‰æ˜¯ç ”ç©¶æ—¶å˜ç°è±¡çš„é‡è¦å·¥å…·ï¼Œå¹¿æ³›åº”ç”¨äºæ—¶åŸŸè°ƒæŸ¥ã€‚</li>
<li>DIAç®—æ³•çš„å…³é”®æ­¥éª¤ä¹‹ä¸€æ˜¯åŒ¹é…ç§‘å­¦å’Œæ¨¡æ¿å›¾åƒä¹‹é—´çš„ç‚¹æ‰©æ•£å‡½æ•°ï¼ˆPSFï¼‰å½¢çŠ¶ã€‚</li>
<li>å·ç§¯æ–¹å‘çš„æ­£ç¡®é€‰æ‹©å¯¹äºé¿å…å·®å¼‚å›¾åƒä¸­çš„ä¼ªå½±è‡³å…³é‡è¦ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§å®šé‡æŒ‡æ ‡æ¥ç¡®å®šæœ€ä½³å·ç§¯æ–¹å‘ï¼Œè¯¥æŒ‡æ ‡è€ƒè™‘äº†å›¾åƒçš„æ¸…æ™°åº¦å’Œæ›å…‰æ·±åº¦ã€‚</li>
<li>é€šè¿‡è¯¥æŒ‡æ ‡ï¼Œå¯ä»¥é¢„å…ˆç¡®å®šæœ€ä½³å·ç§¯æ–¹å‘ï¼Œä»è€Œæé«˜æµç¨‹çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚</li>
<li>å®šä¹‰äº†ä¸€ä¸ªå“è´¨å› æ•°ï¼Œå¯ä»¥æ ¹æ®è¯¥æŒ‡æ ‡å¯¹å¤§é‡å›¾åƒè¿›è¡Œæ’åï¼Œç¡®å®šæœ€é€‚åˆä½œä¸ºæ¨¡æ¿çš„å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10155">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d471fde8e6c6a976490c8ccaf63598ef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-783f138a4baa603e0c1e78044e9b116a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75d892c0024e7f95f5e2d29c26b09712.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b8e6a502ac5f3fc7159f9296819ab1f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e39c41b0103aefdac112ceed752c2939.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ebe8e536c8bdb247f3a74a12e714dd1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Single-Domain-Generalization-for-Multimodal-Cross-Cancer-Prognosis-via-Dirac-Rebalancer-and-Distribution-Entanglement"><a href="#Single-Domain-Generalization-for-Multimodal-Cross-Cancer-Prognosis-via-Dirac-Rebalancer-and-Distribution-Entanglement" class="headerlink" title="Single Domain Generalization for Multimodal Cross-Cancer Prognosis via   Dirac Rebalancer and Distribution Entanglement"></a>Single Domain Generalization for Multimodal Cross-Cancer Prognosis via   Dirac Rebalancer and Distribution Entanglement</h2><p><strong>Authors:Jia-Xuan Jiang, Jiashuai Liu, Hongtao Wu, Yifeng Wu, Zhong Wang, Qi Bi, Yefeng Zheng</strong></p>
<p>Deep learning has shown remarkable performance in integrating multimodal data for survival prediction. However, existing multimodal methods mainly focus on single cancer types and overlook the challenge of generalization across cancers. In this work, we are the first to reveal that multimodal prognosis models often generalize worse than unimodal ones in cross-cancer scenarios, despite the critical need for such robustness in clinical practice. To address this, we propose a new task: Cross-Cancer Single Domain Generalization for Multimodal Prognosis, which evaluates whether models trained on a single cancer type can generalize to unseen cancers. We identify two key challenges: degraded features from weaker modalities and ineffective multimodal integration. To tackle these, we introduce two plug-and-play modules: Sparse Dirac Information Rebalancer (SDIR) and Cancer-aware Distribution Entanglement (CADE). SDIR mitigates the dominance of strong features by applying Bernoulli-based sparsification and Dirac-inspired stabilization to enhance weaker modality signals. CADE, designed to synthesize the target domain distribution, fuses local morphological cues and global gene expression in latent space. Experiments on a four-cancer-type benchmark demonstrate superior generalization, laying the foundation for practical, robust cross-cancer multimodal prognosis. Code is available at <a target="_blank" rel="noopener" href="https://github.com/HopkinsKwong/MCCSDG">https://github.com/HopkinsKwong/MCCSDG</a> </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ åœ¨èåˆå¤šæ¨¡æ€æ•°æ®è¿›è¡Œç”Ÿå­˜é¢„æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•ä¸€ç™Œç—‡ç±»å‹ä¸Šï¼Œå¿½è§†äº†è·¨ç™Œç—‡çš„æ³›åŒ–æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æ­ç¤ºï¼Œå°½ç®¡åœ¨ä¸´åºŠå®è·µä¸­å¯¹è¿™ç§ç¨³å¥æ€§æœ‰ç€è¿«åˆ‡éœ€æ±‚ï¼Œä½†åœ¨è·¨ç™Œç—‡çš„æƒ…å†µä¸‹ï¼Œå¤šæ¨¡æ€é¢„åæ¨¡å‹çš„æ³›åŒ–æ€§èƒ½å¾€å¾€æ¯”å•æ¨¡æ€æ¨¡å‹æ›´å·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ä»»åŠ¡ï¼šè·¨ç™Œç—‡å•åŸŸæ³›åŒ–çš„å¤šæ¨¡æ€é¢„åæ¨¡å‹è¯„ä¼°ï¼Œè¯¥ä»»åŠ¡æ—¨åœ¨è¯„ä¼°åœ¨å•ä¸€ç™Œç—‡ç±»å‹ä¸Šè®­ç»ƒçš„æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿæ¨å¹¿åˆ°æœªè§è¿‡çš„ç™Œç—‡ç±»å‹ã€‚æˆ‘ä»¬ç¡®å®šäº†ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šæ¥è‡ªè¾ƒå¼±æ¨¡æ€çš„ç‰¹å¾é€€åŒ–ä»¥åŠæ— æ•ˆçš„å¤šæ¨¡æ€èåˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§å³æ’å³ç”¨çš„æ¨¡å—ï¼šç¨€ç–ç‹„æ‹‰å…‹ä¿¡æ¯å¹³è¡¡å™¨ï¼ˆSDIRï¼‰å’Œç™Œç—‡æ„ŸçŸ¥åˆ†å¸ƒçº ç¼ ï¼ˆCADEï¼‰ã€‚SDIRé€šè¿‡åº”ç”¨åŸºäºä¼¯åŠªåˆ©çš„ç¨€ç–åŒ–å’Œç‹„æ‹‰å…‹å¯å‘å¼çš„ç¨³å®šåŒ–æ¥å‡è½»å¼ºç‰¹å¾çš„ä¸»å¯¼ä½œç”¨ï¼Œä»è€Œå¢å¼ºå¼±æ¨¡æ€ä¿¡å·ã€‚è€ŒCADEæ—¨åœ¨åˆæˆç›®æ ‡åŸŸåˆ†å¸ƒï¼Œèåˆäº†å±€éƒ¨å½¢æ€çº¿ç´¢å’Œå…¨å±€åŸºå› è¡¨è¾¾éšæ€§ç©ºé—´çš„ä¿¡æ¯ã€‚åœ¨å››ç§ç™Œç—‡ç±»å‹çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒæ˜¾ç¤ºäº†ä¼˜è¶Šçš„æ³›åŒ–æ€§èƒ½ï¼Œè¿™ä¸ºå®é™…åº”ç”¨ä¸­çš„ç¨³å¥è·¨ç™Œç—‡å¤šæ¨¡æ€é¢„åæ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡é“¾æ¥ <a target="_blank" rel="noopener" href="https://github.com/HopkinsKwong/MCCSDG">https://github.com/HopkinsKwong/MCCSDG</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08340v2">PDF</a> Accepted by ACMMM 25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ­ç¤ºäº†è·¨ç™Œç—‡çš„å¤šæ¨¡æ€é¢„åæ¨¡å‹çš„æ³›åŒ–æ€§èƒ½è¾ƒå·®çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†è§£å†³è¯¥é—®é¢˜çš„æ–°ä»»åŠ¡å’ŒæŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæ–‡ç« å¼•å…¥äº†Sparse Diracä¿¡æ¯å¹³è¡¡å™¨å’Œç™Œç—‡æ„ŸçŸ¥åˆ†å¸ƒçº ç¼ ä¸¤ä¸ªå³æ’å³ç”¨æ¨¡å—ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å››ç™Œç§åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ•°æ®èåˆåœ¨ç”Ÿå­˜é¢„æµ‹ä¸­è¡¨ç°å“è¶Šï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å•ä¸€ç™Œç§ï¼Œå¿½è§†æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è·¨ç™Œç§çš„å¤šæ¨¡æ€é¢„åæ¨¡å‹æ³›åŒ–æ€§èƒ½è¾ƒå·®ï¼Œéœ€è¦æé«˜ã€‚</li>
<li>è·¨ç™Œç§å•åŸŸæ³›åŒ–çš„æ–°ä»»åŠ¡è¢«æå‡ºï¼Œä»¥è¯„ä¼°åœ¨æœªè§è¿‡çš„ç™Œç—‡ä¸Šè®­ç»ƒçš„æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬ï¼šæ¥è‡ªè¾ƒå¼±æ¨¡æ€çš„ç‰¹å¾é€€åŒ–ä»¥åŠæ— æ•ˆçš„å¤šæ¨¡æ€èåˆã€‚</li>
<li>å¼•å…¥Sparse Diracä¿¡æ¯å¹³è¡¡å™¨ï¼ˆSDIRï¼‰å’Œç™Œç—‡æ„ŸçŸ¥åˆ†å¸ƒçº ç¼ ï¼ˆCADEï¼‰ä¸¤ä¸ªæ¨¡å—æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>SDIRé€šè¿‡åº”ç”¨åŸºäºä¼¯åŠªåˆ©çš„æ–¹æ³•ç¨³å®šDiracæ¥å¢å¼ºè¾ƒå¼±æ¨¡æ€ä¿¡å·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08340">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d06bba357104caa40ce3a7d01415f21c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b549d307d38ba547e6a10d0aa55dbec6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-971c1238d6615d65d1f2b9632e4a0835.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-28a270ff61ea1cc66632bb603bf5be9b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7871cdca79650f7d84c1507e067d749e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Colorectal-Cancer-Tumor-Grade-Segmentation-in-Digital-Histopathology-Images-From-Giga-to-Mini-Challenge"><a href="#Colorectal-Cancer-Tumor-Grade-Segmentation-in-Digital-Histopathology-Images-From-Giga-to-Mini-Challenge" class="headerlink" title="Colorectal Cancer Tumor Grade Segmentation in Digital Histopathology   Images: From Giga to Mini Challenge"></a>Colorectal Cancer Tumor Grade Segmentation in Digital Histopathology   Images: From Giga to Mini Challenge</h2><p><strong>Authors:Alper Bahcekapili, Duygu Arslan, Umut Ozdemir, Berkay Ozkirli, Emre Akbas, Ahmet Acar, Gozde B. Akar, Bingdou He, Shuoyu Xu, Umit Mert Caglar, Alptekin Temizel, Guillaume Picaud, Marc Chaumont, GÃ©rard Subsol, Luc TÃ©ot, Fahad Alsharekh, Shahad Alghannam, Hexiang Mao, Wenhua Zhang</strong></p>
<p>Colorectal cancer (CRC) is the third most diagnosed cancer and the second leading cause of cancer-related death worldwide. Accurate histopathological grading of CRC is essential for prognosis and treatment planning but remains a subjective process prone to observer variability and limited by global shortages of trained pathologists. To promote automated and standardized solutions, we organized the ICIP Grand Challenge on Colorectal Cancer Tumor Grading and Segmentation using the publicly available METU CCTGS dataset. The dataset comprises 103 whole-slide images with expert pixel-level annotations for five tissue classes. Participants submitted segmentation masks via Codalab, evaluated using metrics such as macro F-score and mIoU. Among 39 participating teams, six outperformed the Swin Transformer baseline (62.92 F-score). This paper presents an overview of the challenge, dataset, and the top-performing methods </p>
<blockquote>
<p>ç»“ç›´è‚ ç™Œï¼ˆCRCï¼‰æ˜¯å…¨çƒè¯Šæ–­ç‡ç¬¬ä¸‰é«˜çš„ç™Œç—‡ï¼Œä¹Ÿæ˜¯å¯¼è‡´ç™Œç—‡ç›¸å…³æ­»äº¡çš„ç¬¬äºŒå¤§ä¸»è¦åŸå› ã€‚CRCçš„å‡†ç¡®ç»„ç»‡ç—…ç†å­¦åˆ†çº§å¯¹é¢„åå’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ï¼Œä½†ä»æ˜¯ä¸€ä¸ªä¸»è§‚è¿‡ç¨‹ï¼Œå®¹æ˜“å—è§‚å¯Ÿè€…å˜å¼‚æ€§çš„å½±å“ï¼Œå¹¶å—åˆ°å…¨çƒè®­ç»ƒæœ‰ç´ ç—…ç†å­¦å®¶çŸ­ç¼ºçš„é™åˆ¶ã€‚ä¸ºäº†ä¿ƒè¿›è‡ªåŠ¨åŒ–å’Œæ ‡å‡†åŒ–è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬åˆ©ç”¨å…¬å¼€å¯ç”¨çš„METU CCTGSæ•°æ®é›†ï¼Œç»„ç»‡äº†å›½é™…å›¾åƒæŒ‘æˆ˜èµ›ï¼ˆICIPï¼‰ç»“ç›´è‚ ç™Œè‚¿ç˜¤åˆ†çº§å’Œåˆ†å‰²æŒ‘æˆ˜èµ›ã€‚è¯¥æ•°æ®é›†åŒ…å«103å¼ å…¨ç‰‡å›¾åƒï¼ŒåŒ…å«äº”ä¸ªç»„ç»‡ç±»åˆ«çš„ä¸“å®¶åƒç´ çº§æ³¨é‡Šã€‚å‚èµ›è€…é€šè¿‡Codalabæäº¤åˆ†å‰²æ©è†œï¼Œä½¿ç”¨å®è§‚Fåˆ†æ•°å’ŒmIoUç­‰åº¦é‡æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚åœ¨3 9æ”¯å‚èµ›é˜Ÿä¼ä¸­ï¼Œæœ‰å…­æ”¯é˜Ÿä¼çš„è¡¨ç°è¶…è¿‡äº†Swin TransformeråŸºçº¿ï¼ˆFåˆ†æ•°ä¸º62.92ï¼‰ã€‚æœ¬æ–‡ä»‹ç»äº†æŒ‘æˆ˜ã€æ•°æ®é›†å’Œè¡¨ç°æœ€ä½³çš„æ–¹æ³•æ¦‚å†µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04681v3">PDF</a> Accepted Grand Challenge Paper ICIP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç»“ç›´è‚ ç™Œï¼ˆCRCï¼‰çš„å…¨çƒæ€§è¯Šæ–­ä¸æ²»ç–—ç°çŠ¶ï¼Œå¼ºè°ƒäº†å‡†ç¡®ç—…ç†åˆ†çº§çš„é‡è¦æ€§åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºæ¨è¿›è‡ªåŠ¨åŒ–ä¸æ ‡å‡†åŒ–è§£å†³æ–¹æ¡ˆï¼Œä½œè€…ç»„ç»‡äº†ä¸€åœºå¤§è§„æ¨¡æŒ‘æˆ˜â€”â€”ä½¿ç”¨METU CCTGSæ•°æ®é›†è¿›è¡Œç»“ç›´è‚ ç™Œè‚¿ç˜¤åˆ†çº§ä¸åˆ†å‰²çš„ICIPæŒ‘æˆ˜èµ›ã€‚æ•°æ®é›†åŒ…å«å¸¦æœ‰ä¸“å®¶åƒç´ çº§æ³¨é‡Šçš„äº”ç±»ç»„ç»‡çš„103å¼ å…¨å¹»ç¯ç‰‡å›¾åƒã€‚é€šè¿‡Codalabæäº¤åˆ†å‰²æ©è†œï¼Œå¹¶ä½¿ç”¨å®è§‚Få¾—åˆ†å’ŒmIoUç­‰æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚å…±æœ‰39æ”¯é˜Ÿä¼å‚ä¸æŒ‘æˆ˜ï¼Œå…¶ä¸­å…­æ”¯é˜Ÿä¼çš„è¡¨ç°è¶…è¿‡äº†åŸºçº¿æ¨¡å‹çš„æ€§èƒ½ï¼ˆSwin Transformerï¼ŒFå¾—åˆ†ä¸º62.92ï¼‰ã€‚æœ¬æ–‡å›é¡¾äº†æŒ‘æˆ˜èµ›æ¦‚å†µã€æ•°æ®é›†ä»¥åŠè¡¨ç°æœ€å¥½çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç»“ç›´è‚ ç™Œæ˜¯å…¨çƒç¬¬ä¸‰å¸¸è§çš„ç™Œç—‡è¯Šæ–­ç±»å‹å’Œç¬¬äºŒå¤§ç™Œç—‡è‡´æ­»åŸå› ã€‚</li>
<li>å‡†ç¡®çš„ç—…ç†åˆ†çº§å¯¹ç»“ç›´è‚ ç™Œçš„é¢„åå’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ï¼Œä½†ä»ç„¶æ˜¯ä¸»è§‚è¿‡ç¨‹ï¼Œå­˜åœ¨è§‚å¯Ÿè€…å·®å¼‚å’Œç—…ç†åŒ»ç”Ÿå…¨çƒçŸ­ç¼ºçš„é™åˆ¶ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œä¸¾åŠäº†ICIPæŒ‘æˆ˜èµ›ï¼Œæ—¨åœ¨é€šè¿‡METU CCTGSæ•°æ®é›†å®ç°ç»“ç›´è‚ ç™Œè‚¿ç˜¤çš„è‡ªåŠ¨å’Œæ ‡å‡†åŒ–åˆ†çº§ä¸åˆ†å‰²ã€‚</li>
<li>æ•°æ®é›†åŒ…å«å¸¦æœ‰ä¸“å®¶åƒç´ çº§æ³¨é‡Šçš„äº”ç±»ç»„ç»‡çš„å…¨å¹»ç¯ç‰‡å›¾åƒã€‚</li>
<li>æŒ‘æˆ˜èµ›å¸å¼•äº†39æ”¯é˜Ÿä¼å‚ä¸ï¼Œå…¶ä¸­å…­æ”¯é˜Ÿä¼çš„è¡¨ç°è¶…è¿‡äº†åŸºçº¿æ¨¡å‹ï¼ˆSwin Transformerï¼‰çš„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04681">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ce5bd74eb9a88977b313246d3f5383ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-733225c26805d90a0479ebbd56811d27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a106fa56f65291ded955c1532ba660f5.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Grid-Reg-Detector-Free-Gridized-Feature-Learning-and-Matching-for-Large-Scale-SAR-Optical-Image-Registration"><a href="#Grid-Reg-Detector-Free-Gridized-Feature-Learning-and-Matching-for-Large-Scale-SAR-Optical-Image-Registration" class="headerlink" title="Grid-Reg: Detector-Free Gridized Feature Learning and Matching for   Large-Scale SAR-Optical Image Registration"></a>Grid-Reg: Detector-Free Gridized Feature Learning and Matching for   Large-Scale SAR-Optical Image Registration</h2><p><strong>Authors:Xiaochen Wei, Weiwei Guo, Zenghui Zhang, Wenxian Yu</strong></p>
<p>It is highly challenging to register large-scale, heterogeneous SAR and optical images, particularly across platforms, due to significant geometric, radiometric, and temporal differences, which most existing methods struggle to address. To overcome these challenges, we propose Grid-Reg, a grid-based multimodal registration framework comprising a domain-robust descriptor extraction network, Hybrid Siamese Correlation Metric Learning Network (HSCMLNet), and a grid-based solver (Grid-Solver) for transformation parameter estimation. In heterogeneous imagery with large modality gaps and geometric differences, obtaining accurate correspondences is inherently difficult. To robustly measure similarity between gridded patches, HSCMLNet integrates a hybrid Siamese module with a correlation metric learning module (CMLModule) based on equiangular unit basis vectors (EUBVs), together with a manifold consistency loss to promote modality-invariant, discriminative feature learning. The Grid-Solver estimates transformation parameters by minimizing a global grid matching loss through a progressive dual-loop search strategy to reliably find patch correspondences across entire images. Furthermore, we curate a challenging benchmark dataset for SAR-to-optical registration using real-world UAV MiniSAR data and Google Earth optical imagery. Extensive experiments demonstrate that our proposed approach achieves superior performance over state-of-the-art methods. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡ã€å¼‚è´¨SARå’Œå…‰å­¦å›¾åƒé…å‡†ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨å¹³å°çš„æƒ…å†µä¸‹ï¼Œç”±äºå‡ ä½•ã€è¾å°„å’Œæ—¶é—´çš„æ˜¾è‘—å·®å¼‚ï¼Œé¢ä¸´å·¨å¤§çš„æŒ‘æˆ˜ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•éƒ½éš¾ä»¥è§£å†³è¿™äº›é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Grid-Regï¼Œè¿™æ˜¯ä¸€ç§åŸºäºç½‘æ ¼çš„å¤šæ¨¡å¼é…å‡†æ¡†æ¶ï¼ŒåŒ…æ‹¬åŸŸç¨³å¥æè¿°ç¬¦æå–ç½‘ç»œã€æ··åˆSiameseå…³è”åº¦é‡å­¦ä¹ ç½‘ç»œï¼ˆHSCMLNetï¼‰å’ŒåŸºäºç½‘æ ¼çš„æ±‚è§£å™¨ï¼ˆGrid-Solverï¼‰ç”¨äºå˜æ¢å‚æ•°ä¼°è®¡ã€‚åœ¨å…·æœ‰å¤§æ¨¡æ€å·®è·å’Œå‡ ä½•å·®å¼‚çš„éå‡åŒ€å›¾åƒä¸­ï¼Œè·å¾—å‡†ç¡®çš„å¯¹åº”å…³ç³»æ˜¯å›ºæœ‰çš„å›°éš¾ã€‚ä¸ºäº†ç¨³å¥åœ°æµ‹é‡ç½‘æ ¼åŒ–è¡¥ä¸ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼ŒHSCMLNetå°†æ··åˆSiameseæ¨¡å—ä¸åŸºäºç­‰è§’å•ä½åŸºå‘é‡ï¼ˆEUBVsï¼‰çš„å…³è”åº¦é‡å­¦ä¹ æ¨¡å—ï¼ˆCMLModuleï¼‰ç›¸ç»“åˆï¼Œä»¥åŠæµå½¢ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥ä¿ƒè¿›æ¨¡æ€ä¸å˜ã€æœ‰åˆ¤åˆ«åŠ›çš„ç‰¹å¾å­¦ä¹ ã€‚Grid-Solveré€šè¿‡æ¸è¿›çš„åŒå¾ªç¯æœç´¢ç­–ç•¥æ¥æœ€å°åŒ–å…¨å±€ç½‘æ ¼åŒ¹é…æŸå¤±ï¼Œä»è€Œå¯é åœ°æ‰¾åˆ°æ•´ä¸ªå›¾åƒä¸­çš„è¡¥ä¸å¯¹åº”å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨çœŸå®çš„æ— äººæœºMiniSARæ•°æ®å’ŒGoogleåœ°çƒå…‰å­¦å›¾åƒï¼Œåˆ›å»ºäº†ä¸€ä¸ªç”¨äºSARåˆ°å…‰å­¦é…å‡†çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æ•°æ®é›†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨æœ€æ–°æŠ€æœ¯æ–¹æ³•ä¸Šå®ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04233v2">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹å¤§è§„æ¨¡ã€å¼‚æ„SARå’Œå…‰å­¦å›¾åƒï¼Œç‰¹åˆ«æ˜¯è·¨å¹³å°å›¾åƒæ³¨å†Œçš„æŒ‘æˆ˜ï¼Œæå‡ºGrid-Regç½‘æ ¼åŸºå¤šæ¨¡æ€æ³¨å†Œæ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬åŸŸç¨³å¥æè¿°ç¬¦æå–ç½‘ç»œã€æ··åˆSiameseå…³è”åº¦é‡å­¦ä¹ ç½‘ç»œï¼ˆHSCMLNetï¼‰å’Œç”¨äºå˜æ¢å‚æ•°ä¼°è®¡çš„ç½‘æ ¼åŸºæ±‚è§£å™¨ï¼ˆGrid-Solverï¼‰ã€‚é€šè¿‡ç»“åˆç½‘æ ¼åŒ–è¡¥ä¸çš„ç›¸ä¼¼æ€§åº¦é‡ï¼ŒHSCMLNetä½¿ç”¨æ··åˆSiameseæ¨¡å—ä¸åŸºäºç­‰è§’å•ä½åŸºå‘é‡ï¼ˆEUBVsï¼‰çš„å…³è”åº¦é‡å­¦ä¹ æ¨¡å—ï¼ˆCMLModuleï¼‰ï¼Œå€ŸåŠ©æµå½¢ä¸€è‡´æ€§æŸå¤±ä¿ƒè¿›æ¨¡æ€ä¸å˜ã€é‰´åˆ«æ€§ç‰¹å¾å­¦ä¹ ã€‚Grid-Solveré€šè¿‡æ¸è¿›çš„åŒå¾ªç¯æœç´¢ç­–ç•¥æœ€å°åŒ–å…¨å±€ç½‘æ ¼åŒ¹é…æŸå¤±ï¼Œä»¥å¯é åœ°æ‰¾åˆ°æ•´ä¸ªå›¾åƒä¸­çš„è¡¥ä¸å¯¹åº”å…³ç³»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤§è§„æ¨¡ã€å¼‚æ„SARå’Œå…‰å­¦å›¾åƒæ³¨å†Œæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨å¹³å°æƒ…å†µä¸‹ã€‚</li>
<li>æå‡ºGrid-Regæ¡†æ¶æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åŸŸç¨³å¥æè¿°ç¬¦æå–ç½‘ç»œã€HSCMLNetå’ŒGrid-Solverã€‚</li>
<li>HSCMLNetç»“åˆäº†æ··åˆSiameseæ¨¡å—å’ŒåŸºäºEUBVsçš„CMLModuleï¼Œç”¨äºåœ¨ç½‘æ ¼åŒ–è¡¥ä¸ä¹‹é—´å®ç°ç¨³å¥çš„ç›¸ä¼¼æ€§åº¦é‡ã€‚</li>
<li>Grid-Solveré€šè¿‡æœ€å°åŒ–å…¨å±€ç½‘æ ¼åŒ¹é…æŸå¤±æ¥ä¼°è®¡å˜æ¢å‚æ•°ã€‚</li>
<li>ä½¿ç”¨æ— äººæœºSARæ•°æ®å’ŒGoogle Earthå…‰å­¦å›¾åƒåˆ›å»ºäº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æ•°æ®é›†ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºå¤„ç†å¤§è§„æ¨¡ã€å¼‚æ„å›¾åƒæ³¨å†Œé—®é¢˜å…·æœ‰å®ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04233">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dee850f2a799e9c074f96b3e206b7138.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-060d1af62f8f9e7c4269b6bd8c8b5534.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-faafe1340b9f4957b36722c0204c630f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1680d53e1172abba601c817bd9390a9b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a660924701353adffbe39c97b9fb9eff.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="LKA-Large-Kernel-Adapter-for-Enhanced-Medical-Image-Classification"><a href="#LKA-Large-Kernel-Adapter-for-Enhanced-Medical-Image-Classification" class="headerlink" title="LKA: Large Kernel Adapter for Enhanced Medical Image Classification"></a>LKA: Large Kernel Adapter for Enhanced Medical Image Classification</h2><p><strong>Authors:Ziquan Zhu, Si-Yuan Lu, Tianjin Huang, Lu Liu, Zhe Liu</strong></p>
<p>Despite the notable success of current Parameter-Efficient Fine-Tuning (PEFT) methods across various domains, their effectiveness on medical datasets falls short of expectations. This limitation arises from two key factors: (1) medical images exhibit extensive anatomical variation and low contrast, necessitating a large receptive field to capture critical features, and (2) existing PEFT methods do not explicitly address the enhancement of receptive fields. To overcome these challenges, we propose the Large Kernel Adapter (LKA), designed to expand the receptive field while maintaining parameter efficiency. The proposed LKA consists of three key components: down-projection, channel-wise large kernel convolution, and up-projection. Through extensive experiments on various datasets and pre-trained models, we demonstrate that the incorporation of a larger kernel size is pivotal in enhancing the adaptation of pre-trained models for medical image analysis. Our proposed LKA outperforms 11 commonly used PEFT methods, surpassing the state-of-the-art by 3.5% in top-1 accuracy across five medical datasets. </p>
<blockquote>
<p>å°½ç®¡å½“å‰å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åœ¨ä¸åŒé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œå®ƒä»¬åœ¨åŒ»å­¦æ•°æ®é›†ä¸Šçš„æ•ˆæœå´æœªèƒ½è¾¾åˆ°é¢„æœŸã€‚è¿™ä¸€å±€é™æ€§æºäºä¸¤ä¸ªå…³é”®å› ç´ ï¼šï¼ˆ1ï¼‰åŒ»å­¦å›¾åƒè¡¨ç°å‡ºå¹¿æ³›çš„è§£å‰–å˜å¼‚å’Œä½å¯¹æ¯”åº¦ï¼Œéœ€è¦è¾ƒå¤§çš„æ„Ÿå—é‡æ¥æ•æ‰å…³é”®ç‰¹å¾ï¼›ï¼ˆ2ï¼‰ç°æœ‰çš„PEFTæ–¹æ³•æ²¡æœ‰æ˜ç¡®è§£å†³æ„Ÿå—é‡å¢å¼ºçš„é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤§å‹å†…æ ¸é€‚é…å™¨ï¼ˆLKAï¼‰ï¼Œæ—¨åœ¨åœ¨ä¿æŒå‚æ•°æ•ˆç‡çš„åŒæ—¶æ‰©å¤§æ„Ÿå—é‡ã€‚æ‰€æå‡ºçš„LKAç”±ä¸‰ä¸ªå…³é”®ç»„ä»¶ç»„æˆï¼šä¸‹æŠ•å½±ã€é€šé“å¤§å‹å†…æ ¸å·ç§¯å’Œä¸ŠæŠ•å½±ã€‚é€šè¿‡å¯¹å„ç§æ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ä½¿ç”¨æ›´å¤§çš„å†…æ ¸å°ºå¯¸å¯¹äºå¢å¼ºé¢„è®­ç»ƒæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„é€‚åº”æ€§è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºçš„å¤§å‹å†…æ ¸é€‚é…å™¨ï¼ˆLKAï¼‰åœ¨äº”ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šè¶…è¿‡äº†11ç§å¸¸ç”¨çš„PEFTæ–¹æ³•ï¼Œåœ¨top-1å‡†ç¡®ç‡ä¸Šè¶…è¶Šäº†æœ€æ–°æŠ€æœ¯3.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19118v4">PDF</a> The manuscript has been withdrawn in order to revise key technical   components and improve experimental validation. We plan to substantially   update the model design and resubmit after further evaluation</p>
<p><strong>Summary</strong><br>     å½“å‰å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åœ¨å¤šé¢†åŸŸè¡¨ç°æ˜¾è‘—ï¼Œä½†åœ¨åŒ»å­¦æ•°æ®é›†ä¸Šçš„æ•ˆæœä¸å°½å¦‚äººæ„ã€‚åŒ»å­¦å›¾åƒå…·æœ‰å¹¿æ³›çš„è§£å‰–å˜å¼‚å’Œä½å¯¹æ¯”åº¦ï¼Œéœ€è¦è¾ƒå¤§çš„æ„Ÿå—é‡æ•æ‰å…³é”®ç‰¹å¾ï¼Œè€Œç°æœ‰PEFTæ–¹æ³•æœªæ˜ç¡®è§£å†³æ„Ÿå—é‡å¢å¼ºé—®é¢˜ã€‚ä¸ºå…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºLarge Kernel Adapterï¼ˆLKAï¼‰ï¼Œæ—¨åœ¨æ‰©å¤§æ„Ÿå—é‡çš„åŒæ—¶ä¿æŒå‚æ•°æ•ˆç‡ã€‚LKAåŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šä¸‹æŠ•å½±ã€é€šé“å¤§å†…æ ¸å·ç§¯å’Œä¸ŠæŠ•å½±ã€‚åœ¨å¤šä¸ªæ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨æ›´å¤§çš„å†…æ ¸å°ºå¯¸å¯¹äºæé«˜é¢„è®­ç»ƒæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„é€‚åº”æ€§è‡³å…³é‡è¦ã€‚æ‰€æå‡ºçš„LKAåœ¨äº”ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šè¶…è¶Š11ç§å¸¸ç”¨çš„PEFTæ–¹æ³•ï¼Œåœ¨top-1å‡†ç¡®ç‡ä¸Šé«˜å‡ºæœ€æ–°æŠ€æœ¯3.5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰PEFTæ–¹æ³•åœ¨åŒ»å­¦æ•°æ®é›†ä¸Šçš„æ•ˆæœæœ‰é™ï¼Œä¸»è¦å› ä¸ºåŒ»å­¦å›¾åƒç‰¹æ€§ï¼ˆå¦‚è§£å‰–å˜å¼‚å’Œä½å¯¹æ¯”åº¦ï¼‰éœ€è¦æ›´å¤§çš„æ„Ÿå—é‡ã€‚</li>
<li>ç°æœ‰PEFTæ–¹æ³•æœªå……åˆ†è§£å†³æ„Ÿå—é‡å¢å¼ºçš„é—®é¢˜ã€‚</li>
<li>ä¸ºæ”¹å–„åœ¨åŒ»å­¦å›¾åƒä¸Šçš„è¡¨ç°ï¼Œæå‡ºäº†Large Kernel Adapterï¼ˆLKAï¼‰ã€‚</li>
<li>LKAåŒ…å«ä¸‹æŠ•å½±ã€é€šé“å¤§å†…æ ¸å·ç§¯å’Œä¸ŠæŠ•å½±ä¸‰ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œæ›´å¤§çš„å†…æ ¸å°ºå¯¸å¯¹æé«˜é¢„è®­ç»ƒæ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„é€‚åº”æ€§è‡³å…³é‡è¦ã€‚</li>
<li>LKAåœ¨å¤šä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šè¶…è¶Šå¤šç§PEFTæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6567a60b94af6d8ebe56a6fb372993e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbec7e26dc8fb1af5f39c9886ac4b020.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60d3822294460fec056aef4a4ced486a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55936770fbc3ceffe9ede2093420c107.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Multimodal-Medical-Image-Binding-via-Shared-Text-Embeddings"><a href="#Multimodal-Medical-Image-Binding-via-Shared-Text-Embeddings" class="headerlink" title="Multimodal Medical Image Binding via Shared Text Embeddings"></a>Multimodal Medical Image Binding via Shared Text Embeddings</h2><p><strong>Authors:Yunhao Liu, Suyang Xi, Shiqi Liu, Hong Ding, Chicheng Jin, Chong Zhong, Junjun He, Catherine C. Liu, Yiqing Shen</strong></p>
<p>Medical image analysis increasingly relies on the integration of multiple imaging modalities to capture complementary anatomical and functional information, enabling more accurate diagnosis and treatment planning. Achieving aligned feature representations across these diverse modalities is therefore important for effective multimodal analysis. While contrastive language-image pre-training (CLIP) and its variant have enabled image-text alignments, they require explicitly paired data between arbitrary two modalities, which is difficult to acquire in medical contexts. To address the gap, we present Multimodal Medical Image Binding with Text (M\textsuperscript{3}Bind), a novel pre-training framework that enables seamless alignment of multiple medical imaging modalities through a shared text representation space without requiring explicit paired data between any two medical image modalities. Specifically, based on the insight that different images can naturally bind with text, M\textsuperscript{3}Bind first fine-tunes pre-trained CLIP-like image-text models to align their modality-specific text embedding space while preserving their original image-text alignments. Subsequently, we distill these modality-specific text encoders into a unified model, creating a shared text embedding space. Experiments on X-ray, CT, retina, ECG, and pathological images on multiple downstream tasks demonstrate that M\textsuperscript{3}Bind achieves state-of-the-art performance in zero-shot, few-shot classification and cross-modal retrieval tasks compared to its CLIP-like counterparts. These results validate M\textsuperscript{3}Bindâ€™s effectiveness in achieving cross-image-modal alignment for medical analysis. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†æè¶Šæ¥è¶Šä¾èµ–äºå¤šç§æˆåƒæ¨¡å¼çš„é›†æˆï¼Œä»¥æ•è·äº’è¡¥çš„è§£å‰–å’ŒåŠŸèƒ½æ€§ä¿¡æ¯ï¼Œä»è€Œå®ç°æ›´å‡†ç¡®çš„è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’ã€‚å› æ­¤ï¼Œå®ç°è¿™äº›ä¸åŒæ¨¡å¼ä¹‹é—´çš„å¯¹é½ç‰¹å¾è¡¨ç¤ºå¯¹äºæœ‰æ•ˆçš„å¤šæ¨¡æ€åˆ†æè‡³å…³é‡è¦ã€‚è™½ç„¶å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åŠå…¶å˜ä½“å®ç°äº†å›¾åƒæ–‡æœ¬å¯¹é½ï¼Œä½†å®ƒä»¬éœ€è¦åœ¨ä»»æ„ä¸¤ç§æ¨¡å¼ä¹‹é—´æ˜ç¡®é…å¯¹çš„æ•°æ®ï¼Œè¿™åœ¨åŒ»å­¦èƒŒæ™¯ä¸‹å¾ˆéš¾è·å–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€åŒ»å­¦å›¾åƒä¸æ–‡æœ¬ç»‘å®šï¼ˆM\textsuperscript{3}Bindï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡å…±äº«æ–‡æœ¬è¡¨ç¤ºç©ºé—´æ— ç¼å¯¹é½å¤šç§åŒ»å­¦æˆåƒæ¨¡å¼ï¼Œè€Œæ— éœ€åœ¨ä»»æ„ä¸¤ç§åŒ»å­¦å›¾åƒæ¨¡å¼ä¹‹é—´æ˜ç¡®é…å¯¹æ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼ŒåŸºäºä¸åŒå›¾åƒå¯ä»¥è‡ªç„¶ç»‘å®šæ–‡æœ¬çš„è§è§£ï¼ŒM\textsuperscript{3}Bindé¦–å…ˆå¾®è°ƒé¢„è®­ç»ƒçš„CLIPç±»ä¼¼å›¾åƒæ–‡æœ¬æ¨¡å‹ï¼Œä»¥å¯¹é½å…¶ç‰¹å®šçš„æ–‡æœ¬åµŒå…¥ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å…¶åŸå§‹çš„å›¾åƒæ–‡æœ¬å¯¹é½ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¿™äº›ç‰¹å®šäºæ¨¡æ€çš„æ–‡æœ¬ç¼–ç å™¨è’¸é¦åˆ°ç»Ÿä¸€æ¨¡å‹ä¸­ï¼Œåˆ›å»ºä¸€ä¸ªå…±äº«çš„æ–‡æœ¬åµŒå…¥ç©ºé—´ã€‚åœ¨Xå…‰ã€CTã€è§†ç½‘è†œã€å¿ƒç”µå›¾å’Œç—…ç†å›¾åƒç­‰å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸CLIPç±»ä¼¼çš„å¯¹æ ‡æ¨¡å‹ç›¸æ¯”ï¼ŒM\textsuperscript{3}Bindåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬åˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœéªŒè¯äº†M\textsuperscript{3}Bindåœ¨å®ç°åŒ»å­¦åˆ†æä¸­çš„è·¨å›¾åƒæ¨¡æ€å¯¹é½çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18072v2">PDF</a> 10 pages, 3 figures</p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒåˆ†æè¶Šæ¥è¶Šä¾èµ–äºå¤šç§æˆåƒæ¨¡å¼çš„æ•´åˆï¼Œä»¥æ•æ‰äº’è¡¥çš„è§£å‰–å’ŒåŠŸèƒ½æ€§ä¿¡æ¯ï¼Œä½¿è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’æ›´åŠ å‡†ç¡®ã€‚å®ç°è¿™äº›ä¸åŒæ¨¡å¼ä¹‹é—´çš„å¯¹é½ç‰¹å¾è¡¨ç¤ºå¯¹äºæœ‰æ•ˆçš„å¤šæ¨¡å¼åˆ†æè‡³å…³é‡è¦ã€‚ä¸ºè§£å†³åŒ»ç–—ç¯å¢ƒä¸­éš¾ä»¥è·å–ä»»æ„ä¸¤ç§æ¨¡å¼ä¹‹é—´çš„æ˜ç¡®é…å¯¹æ•°æ®çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡å¼åŒ»å­¦å›¾åƒä¸æ–‡æœ¬ç»‘å®šï¼ˆM\textsuperscript{3}Bindï¼‰çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿåœ¨æ— éœ€æ˜ç¡®é…å¯¹æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å…±äº«æ–‡æœ¬è¡¨ç¤ºç©ºé—´å®ç°å¤šç§åŒ»å­¦æˆåƒæ¨¡å¼æ— ç¼å¯¹é½ã€‚M\textsuperscript{3}BindåŸºäºä¸åŒå›¾åƒè‡ªç„¶ç»‘å®šæ–‡æœ¬çš„ç†è§£ï¼Œé¦–å…ˆå¾®è°ƒé¢„è®­ç»ƒçš„CLIPç±»å›¾åƒ-æ–‡æœ¬æ¨¡å‹ï¼Œä»¥å¯¹é½å…¶æ¨¡æ€ç‰¹å®šçš„æ–‡æœ¬åµŒå…¥ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™å…¶åŸå§‹çš„å›¾åƒ-æ–‡æœ¬å¯¹é½ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¿™äº›æ¨¡æ€ç‰¹å®šçš„æ–‡æœ¬ç¼–ç å™¨è’¸é¦åˆ°ç»Ÿä¸€æ¨¡å‹ä¸­ï¼Œåˆ›å»ºå…±äº«æ–‡æœ¬åµŒå…¥ç©ºé—´ã€‚åœ¨Xå…‰ã€CTã€è§†ç½‘è†œã€å¿ƒç”µå›¾å’Œç—…ç†å›¾åƒç­‰å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸CLIPç±»æ¨¡å‹ç›¸æ¯”ï¼ŒM\textsuperscript{3}Bindåœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬åˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚è¿™éªŒè¯äº†M\textsuperscript{3}Bindåœ¨åŒ»å­¦åˆ†æä¸­çš„è·¨å›¾åƒæ¨¡æ€å¯¹é½çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†æé€šè¿‡æ•´åˆå¤šç§æˆåƒæ¨¡å¼æ¥æé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>å®ç°ä¸åŒåŒ»å­¦æˆåƒæ¨¡å¼ä¹‹é—´çš„ç‰¹å¾å¯¹é½å¯¹äºå¤šæ¨¡å¼åˆ†æè‡³å…³é‡è¦ã€‚</li>
<li>M\textsuperscript{3}Bindæ˜¯ä¸€ç§æ–°å‹é¢„è®­ç»ƒæ¡†æ¶ï¼Œèƒ½åœ¨æ— éœ€é…å¯¹æ•°æ®çš„æƒ…å†µä¸‹å®ç°å¤šç§åŒ»å­¦æˆåƒæ¨¡å¼çš„æ— ç¼å¯¹é½ã€‚</li>
<li>M\textsuperscript{3}BindåŸºäºä¸åŒå›¾åƒè‡ªç„¶ç»‘å®šæ–‡æœ¬çš„ç†è§£è¿›è¡Œå·¥ä½œã€‚</li>
<li>M\textsuperscript{3}Bindé¦–å…ˆå¾®è°ƒé¢„è®­ç»ƒçš„CLIPç±»å›¾åƒ-æ–‡æœ¬æ¨¡å‹ä»¥è¿›è¡Œæ¨¡æ€å¯¹é½ï¼Œå¹¶ä¿ç•™åŸå§‹å›¾åƒ-æ–‡æœ¬å¯¹é½åŠŸèƒ½ã€‚</li>
<li>M\textsuperscript{3}Bindé€šè¿‡åœ¨å…±äº«æ–‡æœ¬åµŒå…¥ç©ºé—´ä¸­è’¸é¦æ¨¡æ€ç‰¹å®šçš„æ–‡æœ¬ç¼–ç å™¨æ¥åˆ›å»ºç»Ÿä¸€çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18072">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ef015a129faca98e01aaedfced5ec22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c09dc1487f22721e705fc5f091bc94c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37c752e54b84b29b76715a071871650d.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="FlatCAD-Fast-Curvature-Regularization-of-Neural-SDFs-for-CAD-Models"><a href="#FlatCAD-Fast-Curvature-Regularization-of-Neural-SDFs-for-CAD-Models" class="headerlink" title="FlatCAD: Fast Curvature Regularization of Neural SDFs for CAD Models"></a>FlatCAD: Fast Curvature Regularization of Neural SDFs for CAD Models</h2><p><strong>Authors:Haotian Yin, Aleksander Plocharski, Michal Jan Wlodarczyk, Mikolaj Kida, Przemyslaw Musialski</strong></p>
<p>Neural signed-distance fields (SDFs) are a versatile backbone for neural geometry representation, but enforcing CAD-style developability usually requires Gaussian-curvature penalties with full Hessian evaluation and second-order differentiation, which are costly in memory and time. We introduce an off-diagonal Weingarten loss that regularizes only the mixed shape operator term that represents the gap between principal curvatures and flattens the surface. We present two variants: a finite-difference version using six SDF evaluations plus one gradient, and an auto-diff version using a single Hessian-vector product. Both converge to the exact mixed term and preserve the intended geometric properties without assembling the full Hessian. On the ABC benchmarks the losses match or exceed Hessian-based baselines while cutting GPU memory and training time by roughly a factor of two. The method is drop-in and framework-agnostic, enabling scalable curvature-aware SDF learning for engineering-grade shape reconstruction. Our code is available at <a target="_blank" rel="noopener" href="https://flatcad.github.io/">https://flatcad.github.io/</a>. </p>
<blockquote>
<p>ç¥ç»è·ç¦»åœºï¼ˆSDFsï¼‰æ˜¯ç¥ç»å‡ ä½•è¡¨ç¤ºçš„ä¸€ä¸ªå¤šåŠŸèƒ½åŸºç¡€ï¼Œä½†å¼ºåˆ¶å®æ–½CADé£æ ¼çš„å¼€å‘æ€§é€šå¸¸éœ€è¦å¸¦æœ‰å…¨Hessianè¯„ä¼°å’ŒäºŒé˜¶å¯¼æ•°çš„é«˜æ–¯æ›²ç‡æƒ©ç½šï¼Œè¿™åœ¨å†…å­˜å’Œæ—¶é—´ä¸Šæˆæœ¬å¾ˆé«˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§éå¯¹è§’WeingartenæŸå¤±ï¼Œè¯¥æŸå¤±ä»…å¯¹æ··åˆå½¢çŠ¶è¿ç®—ç¬¦é¡¹è¿›è¡Œæ­£åˆ™åŒ–ï¼Œè¯¥é¡¹ä»£è¡¨ä¸»æ›²ç‡ä¹‹é—´çš„å·®è·å¹¶å¹³æ•´è¡¨é¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªç‰ˆæœ¬ï¼šä¸€ä¸ªä½¿ç”¨å…­ä¸ªSDFè¯„ä¼°å’Œä¸€ä¸ªæ¢¯åº¦çš„æœ‰é™å·®åˆ†ç‰ˆæœ¬ï¼Œä»¥åŠä¸€ä¸ªä½¿ç”¨å•ä¸ªHessian-vectoräº§å“çš„è‡ªåŠ¨å·®åˆ†ç‰ˆæœ¬ã€‚ä¸¤è€…éƒ½æ”¶æ•›åˆ°ç²¾ç¡®çš„æ··åˆé¡¹å¹¶ä¿ç•™äº†é¢„æœŸçš„å‡ ä½•å±æ€§ï¼Œè€Œæ— éœ€ç»„è£…å®Œæ•´çš„HessiançŸ©é˜µã€‚åœ¨ABCåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¿™äº›æŸå¤±ä¸åŸºäºHessiançš„åŸºçº¿ç›¸åŒ¹é…æˆ–è¡¨ç°æ›´å¥½ï¼ŒåŒæ—¶å°†GPUå†…å­˜å’Œè®­ç»ƒæ—¶é—´å‡å°‘äº†å¤§çº¦ä¸€åŠã€‚è¯¥æ–¹æ³•å³æ’å³ç”¨ï¼Œæ¡†æ¶ä¸­ç«‹ï¼Œå¯å®ç°å·¥ç¨‹çº§å½¢çŠ¶é‡å»ºçš„è§„æ¨¡åŒ–æ›²ç‡æ„ŸçŸ¥SDFå­¦ä¹ ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://flatcad.github.io/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://flatcad.github.io/ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.16627v2">PDF</a> Computer Graphics Forum, Proceedings of Pacific Graphics 2025, 12   pages, 10 figures, preprint</p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œç¬¦å·è·ç¦»åœºï¼ˆSDFsï¼‰åœ¨ç¥ç»å‡ ä½•è¡¨ç¤ºä¸­å…·æœ‰é€šç”¨æ€§ï¼Œä½†å¼ºåˆ¶å®æ–½CADé£æ ¼çš„å¼€å‘æ€§é€šå¸¸éœ€è¦é«˜æ–¯æ›²ç‡æƒ©ç½šå¹¶è¿›è¡Œå…¨é¢Hessianè¯„ä¼°å’ŒäºŒæ¬¡å¾®åˆ†ï¼Œè¿™åœ¨å†…å­˜å’Œæ—¶é—´ä¸Šæˆæœ¬è¾ƒé«˜ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§ç¦»å¯¹è§’çº¿WeingartenæŸå¤±ï¼Œä»…å¯¹æ··åˆå½¢çŠ¶æ“ä½œç¬¦æœ¯è¯­è¿›è¡Œæ­£åˆ™åŒ–ï¼Œè¯¥æœ¯è¯­ä»£è¡¨äº†ä¸»æ›²ç‡ä¹‹é—´çš„å·®è·å¹¶å‹å¹³äº†è¡¨é¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸¤ä¸ªç‰ˆæœ¬ï¼šä¸€ä¸ªä½¿ç”¨å…­ä¸ªSDFè¯„ä¼°å’Œä¸€æ¬¡æ¢¯åº¦çš„æœ‰é™å·®åˆ†ç‰ˆæœ¬ï¼Œä»¥åŠä¸€ä¸ªä½¿ç”¨å•ä¸ªHessian-vectoräº§å“çš„è‡ªåŠ¨å¾®åˆ†ç‰ˆæœ¬ã€‚ä¸¤è€…éƒ½æ”¶æ•›åˆ°ç²¾ç¡®çš„æ··åˆæœ¯è¯­å¹¶ä¿æŒé¢„æœŸçš„å‡ ä½•å±æ€§ï¼Œè€Œæ— éœ€ç»„è£…å®Œæ•´çš„Hessianã€‚åœ¨ABCåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¿™äº›æŸå¤±ä¸åŸºäºHessiançš„åŸºçº¿ç›¸åŒ¹é…æˆ–è¶…è¶Šï¼ŒåŒæ—¶å°†GPUå†…å­˜å’Œè®­ç»ƒæ—¶é—´å¤§è‡´å‡å°‘äº†ä¸€åŠã€‚è¯¥æ–¹æ³•å…·æœ‰å¯æ›¿ä»£æ€§å’Œæ¡†æ¶ç‹¬ç«‹æ€§ï¼Œå¯å®ç°ç”¨äºå·¥ç¨‹çº§å½¢çŠ¶é‡å»ºçš„å¯æ‰©å±•æ›²ç‡æ„ŸçŸ¥SDFå­¦ä¹ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œç¬¦å·è·ç¦»åœºï¼ˆSDFsï¼‰æ˜¯ç¥ç»å‡ ä½•è¡¨ç¤ºä¸­çš„é€šç”¨å·¥å…·ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡é«˜æ–¯æ›²ç‡æƒ©ç½šå’Œå…¨é¢çš„Hessianè¯„ä¼°æ¥æ‰§è¡ŒCADé£æ ¼çš„å¼€å‘æ€§è¦æ±‚ï¼Œè¿™å¢åŠ äº†å†…å­˜å’Œæ—¶é—´æˆæœ¬ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ç¦»å¯¹è§’çº¿WeingartenæŸå¤±æ–¹æ³•ï¼Œä»…å¯¹æ··åˆå½¢çŠ¶æ“ä½œç¬¦è¿›è¡Œæ­£åˆ™åŒ–ï¼Œä»£è¡¨ä¸»æ›²ç‡ä¹‹é—´çš„å·®è·å¹¶å¹³æ»‘è¡¨é¢ã€‚</li>
<li>æå‡ºäº†ä¸¤ç§æ–°æ–¹æ³•æ¥æ±‚è§£è¿™ç§æŸå¤±ï¼šæœ‰é™å·®åˆ†ç‰ˆæœ¬å’Œè‡ªåŠ¨å¾®åˆ†ç‰ˆæœ¬ã€‚</li>
<li>è¿™ä¸¤ä¸ªæ–¹æ³•éƒ½èƒ½åœ¨æ— éœ€å®Œæ•´Hessiançš„æƒ…å†µä¸‹æ”¶æ•›åˆ°ç²¾ç¡®çš„æ··åˆæœ¯è¯­å¹¶ä¿æŒé¢„æœŸçš„å‡ ä½•å±æ€§ã€‚</li>
<li>åœ¨ABCåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•åœ¨åŒ¹é…æˆ–è¶…è¶ŠåŸºäºHessiançš„åŸºçº¿çš„åŒæ—¶ï¼Œå¯å°†GPUå†…å­˜å’Œè®­ç»ƒæ—¶é—´å¤§è‡´å‡å°‘ä¸€åŠã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.16627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8402e0ddeaa15a57e3e010d3ff260018.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef3dd1e2c62b12dec77ba64728225333.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-340359d2800b16f3d5c92b7ef7e4dc19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14639545a6cddb0ce5ca4f5cf288295a.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-09/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3792724b660c72d3400c541ca1789e19.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-09  Sticker-TTS Learn to Utilize Historical Experience with a   Sticker-driven Test-Time Scaling Framework
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-09/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fd5a87aae30513f8d86b01c6276963c6.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-09  Painting the market generative diffusion models for financial limit   order book simulation and forecasting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30806.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
