<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-09-09  Scaling Performance of Large Language Model Pretraining">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04650v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-02
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    17.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    72 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-09-更新"><a href="#2025-09-09-更新" class="headerlink" title="2025-09-09 更新"></a>2025-09-09 更新</h1><h2 id="Scaling-Performance-of-Large-Language-Model-Pretraining"><a href="#Scaling-Performance-of-Large-Language-Model-Pretraining" class="headerlink" title="Scaling Performance of Large Language Model Pretraining"></a>Scaling Performance of Large Language Model Pretraining</h2><p><strong>Authors:Alexander Interrante-Grant, Carla Varela-Rosa, Suhaas Narayan, Chris Connelly, Albert Reuther</strong></p>
<p>Large language models (LLMs) show best-in-class performance across a wide range of natural language processing applications. Training these models is an extremely computationally expensive task; frontier Artificial Intelligence (AI) research companies are investing billions of dollars into supercomputing infrastructure to train progressively larger models on increasingly massive datasets. Unfortunately, information about the scaling performance and training considerations of these large training pipelines is scarce in public literature. Working with large-scale datasets and models can be complex and practical recommendations are scarce in the public literature for tuning training performance when scaling up large language models. In this paper, we aim to demystify the large language model pretraining pipeline somewhat - in particular with respect to distributed training, managing large datasets across hundreds of nodes, and scaling up data parallelism with an emphasis on fully leveraging available GPU compute capacity. </p>
<blockquote>
<p>大型语言模型（LLM）在广泛的自然语言处理应用中表现出卓越的性能。训练这些模型是一项极其耗费计算资源的任务；前沿的人工智能（AI）研究公司正在投入数十亿美元用于超级计算基础设施，以便在日益庞大的数据集上训练规模越来越大的模型。然而，关于这些大规模训练管道的可扩展性能和训练注意事项的信息在公开文献中非常稀缺。与大规模数据集和模型协同工作可能很复杂，公开文献中关于调整大型语言模型训练性能以扩展规模的实用建议很少。本文旨在揭开大型语言模型预训练管道的部分奥秘，特别是关于分布式训练、在数百个节点上管理大规模数据集以及扩展数据并行性，重点充分利用可用的GPU计算能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05258v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大规模语言模型（LLM）在自然语言处理应用方面表现出卓越的性能，但训练这些模型需要极高的计算成本。前沿人工智能（AI）研究公司正在投入巨资建设超级计算基础设施，以在日益庞大的数据集上训练规模更大的模型。然而，关于这些大规模训练管道的性能缩放和培训考虑因素的信息在公开文献中很少见。本文旨在阐明大规模语言模型预训练管道，特别是关于分布式训练、管理数百个节点上的大型数据集以及强调充分利用可用GPU计算能力的数据并行性扩展等方面。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大规模语言模型（LLM）在自然语言处理应用上表现卓越。</li>
<li>训练大规模语言模型需要极高的计算成本，前沿AI公司为此投入巨资。</li>
<li>关于大规模语言模型训练管道的性能缩放和培训考虑的信息在公开文献中稀缺。</li>
<li>分布式训练是大型语言模型预训练的关键方面。</li>
<li>管理数百个节点上的大型数据集是大型语言模型训练的挑战之一。</li>
<li>数据并行性是扩展大型语言模型预训练的重要手段。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05258">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.05258v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.05258v1/page_1_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MM-DREX-Multimodal-Driven-Dynamic-Routing-of-LLM-Experts-for-Financial-Trading"><a href="#MM-DREX-Multimodal-Driven-Dynamic-Routing-of-LLM-Experts-for-Financial-Trading" class="headerlink" title="MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial   Trading"></a>MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial   Trading</h2><p><strong>Authors:Yang Chen, Yueheng Jiang, Zhaozhao Ma, Yuchen Cao Jacky Keung, Kun Kuang, Leilei Gan, Yiquan Wu, Fei Wu</strong></p>
<p>The inherent non-stationarity of financial markets and the complexity of multi-modal information pose significant challenges to existing quantitative trading models. Traditional methods relying on fixed structures and unimodal data struggle to adapt to market regime shifts, while large language model (LLM)-driven solutions - despite their multi-modal comprehension - suffer from static strategies and homogeneous expert designs, lacking dynamic adjustment and fine-grained decision mechanisms. To address these limitations, we propose MM-DREX: a Multimodal-driven, Dynamically-Routed EXpert framework based on large language models. MM-DREX explicitly decouples market state perception from strategy execution to enable adaptive sequential decision-making in non-stationary environments. Specifically, it (1) introduces a vision-language model (VLM)-powered dynamic router that jointly analyzes candlestick chart patterns and long-term temporal features to allocate real-time expert weights; (2) designs four heterogeneous trading experts (trend, reversal, breakout, positioning) generating specialized fine-grained sub-strategies; and (3) proposes an SFT-RL hybrid training paradigm to synergistically optimize the router’s market classification capability and experts’ risk-adjusted decision-making. Extensive experiments on multi-modal datasets spanning stocks, futures, and cryptocurrencies demonstrate that MM-DREX significantly outperforms 15 baselines (including state-of-the-art financial LLMs and deep reinforcement learning models) across key metrics: total return, Sharpe ratio, and maximum drawdown, validating its robustness and generalization. Additionally, an interpretability module traces routing logic and expert behavior in real time, providing an audit trail for strategy transparency. </p>
<blockquote>
<p>金融市场固有的非平稳性和多模态信息的复杂性对现有量化交易模型构成了重大挑战。传统方法依赖于固定结构和单模态数据，难以适应市场状态变化，而大型语言模型（LLM）驱动解决方案虽然具有多模态理解能力，但却存在静态策略和同质化专家设计的问题，缺乏动态调整和精细决策机制。为了解决这些局限性，我们提出了基于大型语言模型的MM-DREX：一个多模态驱动、动态路由的专家框架。MM-DREX显式地将市场状态感知与策略执行解耦，以实现在非平稳环境中的自适应序列决策。具体来说，它（1）引入了一个由视觉语言模型（VLM）驱动的动态路由器，该路由器联合分析K线图模式和长期时间特征来分配实时专家权重；（2）设计了四种不同的交易专家（趋势、反转、突破、定位），生成专业的精细子策略；（3）提出了一种SFT-RL混合训练范式，协同优化路由器的市场分类能力和专家的风险调整决策能力。在涵盖股票、期货和加密货币的多模态数据集上的大量实验表明，MM-DREX在关键指标上显著优于15个基准模型（包括最先进的金融LLM和深度强化学习模型），这些指标包括总收益、夏普比率和最大回撤，验证了其稳健性和泛化能力。此外，解释性模块可实时跟踪路由逻辑和专家行为，为策略透明性提供审计跟踪。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05080v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>金融市场的内在非稳定性和多模态信息的复杂性对现有量化交易模型构成重大挑战。传统方法难以适应市场状态变化，而大型语言模型（LLM）虽然具备多模态理解能力，但策略静态且缺乏动态调整和精细决策机制。为此，提出MM-DREX框架，结合多模态驱动和动态路由技术，实现自适应序列决策。该框架通过引入视觉语言模型驱动的动态路由器分析蜡烛图模式和长期时间特征，设计四种异质交易专家生成精细子策略，并提出SFT-RL混合训练范式优化市场分类能力和风险调整决策。实验证明MM-DREX在股票、期货和加密货币等多模式数据集上显著优于15种基线方法，具有稳健性和泛化能力。同时，提供实时解释模块，追踪路由逻辑和专家行为，确保策略透明度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>金融市场非稳定性和多模态信息带来挑战：传统量化交易模型难以适应市场变化，需要新的解决方案。</li>
<li>LLM在多模态理解方面具有优势，但存在策略静态和缺乏动态调整的问题。</li>
<li>MM-DREX框架结合多模态驱动和动态路由技术，实现自适应序列决策。</li>
<li>MM-DREX通过视觉语言模型分析市场模式，并设计四种交易专家生成子策略。</li>
<li>SFT-RL混合训练范式优化市场分类和风险管理决策。</li>
<li>实验证明MM-DREX在多种数据集上表现优异，具有稳健性和泛化能力。</li>
<li>MM-DREX提供策略透明度，通过实时解释模块追踪路由逻辑和专家行为。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05080">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.05080v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.05080v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.05080v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.05080v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Comparative-Analysis-of-Transformer-Models-in-Disaster-Tweet-Classification-for-Public-Safety"><a href="#Comparative-Analysis-of-Transformer-Models-in-Disaster-Tweet-Classification-for-Public-Safety" class="headerlink" title="Comparative Analysis of Transformer Models in Disaster Tweet   Classification for Public Safety"></a>Comparative Analysis of Transformer Models in Disaster Tweet   Classification for Public Safety</h2><p><strong>Authors:Sharif Noor Zisad, Ragib Hasan</strong></p>
<p>Twitter and other social media platforms have become vital sources of real time information during disasters and public safety emergencies. Automatically classifying disaster related tweets can help emergency services respond faster and more effectively. Traditional Machine Learning (ML) models such as Logistic Regression, Naive Bayes, and Support Vector Machines have been widely used for this task, but they often fail to understand the context or deeper meaning of words, especially when the language is informal, metaphorical, or ambiguous. We posit that, in this context, transformer based models can perform better than traditional ML models. In this paper, we evaluate the effectiveness of transformer based models, including BERT, DistilBERT, RoBERTa, and DeBERTa, for classifying disaster related tweets. These models are compared with traditional ML approaches to highlight the performance gap. Experimental results show that BERT achieved the highest accuracy (91%), significantly outperforming traditional models like Logistic Regression and Naive Bayes (both at 82%). The use of contextual embeddings and attention mechanisms allows transformer models to better understand subtle language in tweets, where traditional ML models fall short. This research demonstrates that transformer architectures are far more suitable for public safety applications, offering improved accuracy, deeper language understanding, and better generalization across real world social media text. </p>
<blockquote>
<p>推特和其他社交媒体平台在灾难和公共安全紧急事件期间已成为实时信息的重要来源。自动分类与灾难相关的推特可以帮助紧急服务更快更高效地响应。逻辑回归、朴素贝叶斯和支持向量机等传统机器学习（ML）模型在此任务中得到了广泛的应用，但它们往往无法理解语境或单词的深层含义，尤其是在语言非正式、隐晦或含糊不清的情况下。我们认为，在这种情况下，基于变压器的模型可以比传统ML模型表现得更好。在本文中，我们评估了基于变压器的模型，包括BERT、DistilBERT、RoBERTa和DeBERTa，在分类灾难相关推特方面的有效性。这些模型与传统ML方法进行比较，以突出性能差异。实验结果表明，BERT的准确率最高（91%），显著优于逻辑回归和朴素贝叶斯等传统模型（均为82%）。上下文嵌入和注意力机制的使用使变压器模型能够更好地理解推特中的细微语言，这是传统ML模型所无法做到的。这项研究表明，变压器架构非常适合公共安全应用，具有更高的准确性、更深的语言理解能力和在现实社交媒体文本中的更好泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04650v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>社交媒体如Twitter等已成为灾害和公共安全紧急事件实时信息的重要来源。自动分类与灾害相关的推文有助于应急服务更快、更高效地响应。虽然传统机器学习模型（如逻辑回归、朴素贝叶斯和支持向量机）已广泛应用于此任务，但它们往往无法理解单词的语境或深层含义，尤其是语言非正式、隐晦或模糊时。我们认为，在这种情况下，基于变压器的模型可以比传统ML模型表现得更好。本文评估了BERT、DistilBERT、RoBERTa和DeBERTa等基于变压器的模型在分类灾害相关推文中的有效性。与传统ML方法相比，这些模型突出了性能差距。实验结果表明，BERT的准确率最高（91%），显著优于逻辑回归和朴素贝叶斯等传统模型（均为82%）。使用上下文嵌入和注意力机制使得变压器模型能够更好地理解推文中的细微语言，这是传统ML模型所无法做到的。研究表明，对于公共安全应用，变压器架构更为适合，具有更高的准确性、更深的语言理解能力和更好的社交媒体的文本概括能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>社交媒体成为灾害和紧急事件的重要信息来源，自动分类相关内容有助于提高应急响应效率。</li>
<li>传统机器学习模型在分类灾害相关推文时存在语境理解不足的问题。</li>
<li>基于变压器的模型，如BERT、DistilBERT等，在分类灾害相关推文方面表现出优异性能。</li>
<li>BERT模型在准确率方面最高，达到91%。</li>
<li>变压器模型能够利用上下文嵌入和注意力机制更好地理解推文中的细微语言和深层含义。</li>
<li>相对于传统ML模型，变压器模型更适合于公共安全应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04650">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04650v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04650v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04650v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04650v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04650v1/page_4_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04650v1/page_4_3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Aesthetic-Image-Captioning-with-Saliency-Enhanced-MLLMs"><a href="#Aesthetic-Image-Captioning-with-Saliency-Enhanced-MLLMs" class="headerlink" title="Aesthetic Image Captioning with Saliency Enhanced MLLMs"></a>Aesthetic Image Captioning with Saliency Enhanced MLLMs</h2><p><strong>Authors:Yilin Tao, Jiashui Huang, Huaze Xu, Ling Shao</strong></p>
<p>Aesthetic Image Captioning (AIC) aims to generate textual descriptions of image aesthetics, becoming a key research direction in the field of computational aesthetics. In recent years, pretrained Multimodal Large Language Models (MLLMs) have advanced rapidly, leading to a significant increase in image aesthetics research that integrates both visual and textual modalities. However, most existing studies on image aesthetics primarily focus on predicting aesthetic ratings and have shown limited application in AIC. Existing AIC works leveraging MLLMs predominantly rely on fine-tuning methods without specifically adapting MLLMs to focus on target aesthetic content. To address this limitation, we propose the Aesthetic Saliency Enhanced Multimodal Large Language Model (ASE-MLLM), an end-to-end framework that explicitly incorporates aesthetic saliency into MLLMs. Within this framework, we introduce the Image Aesthetic Saliency Module (IASM), which efficiently and effectively extracts aesthetic saliency features from images. Additionally, we design IAS-ViT as the image encoder for MLLMs, this module fuses aesthetic saliency features with original image features via a cross-attention mechanism. To the best of our knowledge, ASE-MLLM is the first framework to integrate image aesthetic saliency into MLLMs specifically for AIC tasks. Extensive experiments demonstrated that our approach significantly outperformed traditional methods and generic MLLMs on current mainstream AIC benchmarks, achieving state-of-the-art (SOTA) performance. </p>
<blockquote>
<p>美学图像标题生成（AIC）旨在生成图像美学描述的文本，已成为计算美学领域的关键研究方向。近年来，预训练的多模态大型语言模型（MLLM）迅速发展，使得融合视觉和文本模态的图像美学研究大幅增加。然而，大多数现有的图像美学研究主要集中在预测美学评分上，在AIC中的应用有限。现有利用MLLM的AIC工作主要依赖于微调方法，而没有专门调整MLLM以专注于目标美学内容。为了解决这一局限性，我们提出了美学显著性增强多模态大型语言模型（ASE-MLLM），这是一个端到端的框架，显式地将美学显著性纳入MLLM。在该框架中，我们引入了图像美学显著性模块（IASM），该模块能够高效地从图像中提取美学显著性特征。此外，我们设计了IAS-ViT作为MLLM的图像编码器，该模块通过交叉注意机制将美学显著性特征与原始图像特征融合。据我们所知，ASE-MLLM是第一个将图像美学显著性融入MLLM的框架，专门用于AIC任务。大量实验表明，我们的方法在当前主流AIC基准测试上显著优于传统方法和通用MLLM，达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04378v2">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了美学图像标注（AIC）的目标和重要性，指出近年来多模态大型语言模型（MLLMs）的快速发展推动了图像美学研究的进步。然而，大多数现有研究主要关注预测美学评分，在AIC中的应用有限。为此，本文提出了一个结合美学显著性的多模态大型语言模型（ASE-MLLM）框架，通过引入图像美学显著性模块（IASM）和IAS-ViT图像编码器，实现了对目标美学内容的特定适应。据我们所知，这是第一个专门针对AIC任务的结合图像美学显著性的MLLM框架。实验表明，该方法在主流AIC基准测试上显著优于传统方法和通用MLLMs，达到了最新性能水平。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>美学图像标注（AIC）是计算美学领域的关键研究方向，旨在生成图像美学的文本描述。</li>
<li>近期多模态大型语言模型（MLLMs）的快速发展推动了图像美学研究的进步。</li>
<li>现有图像美学研究主要关注预测美学评分，在AIC中的应用有限。</li>
<li>提出的ASE-MLLM框架通过结合图像美学显著性模块（IASM）和IAS-ViT图像编码器，实现了对目标美学内容的特定适应。</li>
<li>ASE-MLLM框架是首个专门针对AIC任务的结合图像美学显著性的MLLM框架。</li>
<li>实验表明，ASE-MLLM框架在主流AIC基准测试上显著优于传统方法和通用MLLMs。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04378">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04378v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04378v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04378v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04378v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04378v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="The-Personality-Illusion-Revealing-Dissociation-Between-Self-Reports-Behavior-in-LLMs"><a href="#The-Personality-Illusion-Revealing-Dissociation-Between-Self-Reports-Behavior-in-LLMs" class="headerlink" title="The Personality Illusion: Revealing Dissociation Between Self-Reports &amp;   Behavior in LLMs"></a>The Personality Illusion: Revealing Dissociation Between Self-Reports &amp;   Behavior in LLMs</h2><p><strong>Authors:Pengrui Han, Rafal Kocielnik, Peiyang Song, Ramit Debnath, Dean Mobbs, Anima Anandkumar, R. Michael Alvarez</strong></p>
<p>Personality traits have long been studied as predictors of human behavior. Recent advances in Large Language Models (LLMs) suggest similar patterns may emerge in artificial systems, with advanced LLMs displaying consistent behavioral tendencies resembling human traits like agreeableness and self-regulation. Understanding these patterns is crucial, yet prior work primarily relied on simplified self-reports and heuristic prompting, with little behavioral validation. In this study, we systematically characterize LLM personality across three dimensions: (1) the dynamic emergence and evolution of trait profiles throughout training stages; (2) the predictive validity of self-reported traits in behavioral tasks; and (3) the impact of targeted interventions, such as persona injection, on both self-reports and behavior. Our findings reveal that instructional alignment (e.g., RLHF, instruction tuning) significantly stabilizes trait expression and strengthens trait correlations in ways that mirror human data. However, these self-reported traits do not reliably predict behavior, and observed associations often diverge from human patterns. While persona injection successfully steers self-reports in the intended direction, it exerts little or inconsistent effect on actual behavior. By distinguishing surface-level trait expression from behavioral consistency, our findings challenge assumptions about LLM personality and underscore the need for deeper evaluation in alignment and interpretability. </p>
<blockquote>
<p>人格特质长期以来一直被研究作为人类行为的预测因素。最近，大型语言模型（LLM）的进步表明，人工系统中可能出现类似的模式，高级LLM表现出与人类的特质如友善和自我调节一致的行为倾向。了解这些模式至关重要，但之前的工作主要依赖于简化的自我报告和启发式提示，很少进行行为验证。在这项研究中，我们系统地描述了LLM人格的三个方面：（1）特质剖面在训练阶段的动态出现和演变；（2）自我报告特质在行为任务中的预测效度；（3）有针对性的干预（如人格注入）对自我报告和行为的影响。我们的研究发现，指令对齐（例如RLHF、指令微调）显著稳定了特质表达，并以与人类数据相似的方式加强了特质相关性。然而，这些自我报告的特质并不能可靠地预测行为，观察到的关联往往与人类模式相悖。虽然人格注入成功引导了预期的自我报告方向，但对实际行为的影响较小或不一致。通过区分表面层次的特质表达和行为的连贯性，我们的研究对LLM人格假设提出了挑战，并强调了对齐和可解释性方面更深层评估的必要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03730v2">PDF</a> We make public all code and source data at   <a target="_blank" rel="noopener" href="https://github.com/psychology-of-AI/Personality-Illusion">https://github.com/psychology-of-AI/Personality-Illusion</a> for full   reproducibility</p>
<p><strong>Summary</strong></p>
<p>近期研究表明，大型语言模型（LLM）展现出与人类相似的性格特质，如友善性和自我调节等。本研究旨在系统地探讨LLM性格特征的三维特征：训练阶段特质特征的动态出现与演变、自我报告特质在行为任务中的预测效度以及目标干预（如人格注入）对自我报告和行为的影响。研究发现，指令对齐（如RLHF、指令微调）显著稳定了特质表达并增强了特质间的关联，但与人类数据相比，自我报告的特质并不能可靠地预测行为，观察到的关联常常与人类的模式不符。人格注入虽然能成功引导自我报告朝预定方向进行，但对实际行为的影响较小或不一致。本研究结果挑战了关于LLM性格的假设，并强调需要更深入地评估对齐和可解释性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM展现出与人类相似的性格特质，如友善性和自我调节。</li>
<li>训练阶段特质特征的动态出现与演变是LLM研究的重要方面。</li>
<li>自我报告特质在行为任务中的预测效度有限，观察到的关联常与人类模式不符。</li>
<li>指令对齐能显著稳定特质表达并增强特质间的关联。</li>
<li>人格注入对自我报告的影响较大，但对实际行为的影响较小或不一致。</li>
<li>需要更深入地评估LLM的对齐和可解释性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03730">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.03730v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.03730v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.03730v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.03730v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MultiStream-LLM-Bridging-Modalities-for-Robust-Sign-Language-Translation"><a href="#MultiStream-LLM-Bridging-Modalities-for-Robust-Sign-Language-Translation" class="headerlink" title="MultiStream-LLM: Bridging Modalities for Robust Sign Language   Translation"></a>MultiStream-LLM: Bridging Modalities for Robust Sign Language   Translation</h2><p><strong>Authors:Marshall Thomas, Edward Fish, Richard Bowden</strong></p>
<p>Despite progress in gloss-free Sign Language Translation (SLT), monolithic end-to-end models consistently fail on two critical components of natural signing: the precise recognition of high-speed fingerspelling and the integration of asynchronous non-manual cues from the face. Recent progress in Automated Sign Language Translation with Large Language Models has side stepped this challenge, forcing a single network to learn these simultaneously resulting in poor performance when tasked with translating crucial information such as names,places, and technical terms. We introduce MultiStream-LLM, a modular framework designed to overcome these limitations. Our approach employs separate, specialized predictors for continuous signing, fingerspelling, and lipreading. Each expert network first decodes its specific modality into a sequence of tokens. These parallel streams are then fused by a lightweight transformer that resolves temporal misalignments before passing the combined representation to a Large Language Model (LLM) for final sentence generation. Our method establishes a new state-of-the-art on the How2Sign benchmark with a BLEU-4 score of 23.5 and achieves 73.2% letter accuracy on the challenging ChicagoFSWildPlus fingerspelling dataset. These results validate our core hypothesis: by isolating and solving distinct recogni tion tasks before fusion, our multi-expert approach provides a more powerful and effective pathway to robust, high-fidelity sign language translation. </p>
<blockquote>
<p>尽管在无字幕手语翻译（SLT）方面取得了进展，但单一的全端到端的模型始终在自然手语的两个关键组成部分上表现不佳：无法精确识别高速手指拼写以及无法整合来自面部的异步非手动线索。最近，基于大型语言模型的自动手语翻译领域的进展回避了这一挑战，迫使单一网络同时学习这些技能，导致在翻译名字、地点和技术术语等重要信息时表现不佳。我们引入了MultiStream-LLM，这是一个旨在克服这些限制的模块化框架。我们的方法采用单独的专用预测器进行连续手语、手指拼写和唇读。每个专家网络首先将其特定模式解码为一系列令牌。然后，这些并行流通过一个轻量级变压器融合，解决时间错位问题，然后将组合表示传递给大型语言模型（LLM）以生成最终句子。我们的方法在How2Sign基准测试上建立了最新技术水准，BLEU-4得分为23.5，在具有挑战性的ChicagoFSWildPlus手指拼写数据集上实现了73.2%的字母准确率。这些结果验证了我们的核心假设：通过在融合之前隔离并解决不同的识别任务，我们的多专家方法提供了更强大和有效的途径来实现稳健、高保真手语翻译。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00030v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了多流大型语言模型（MultiStream-LLM）在自动手语翻译中的应用。针对手语翻译中的高速指拼和面部非手动暗示的识别问题，提出了一种模块化框架。该框架采用专门预测器处理连续手语、指拼和唇读等任务，通过轻量级转换器融合并行数据流，解决时间错位问题，最终由大型语言模型生成句子。该方法在How2Sign基准测试中取得了BLEU-4分数为23.5的新水平，并在挑战性较大的ChicagoFSWildPlus指拼数据集上达到了73.2%的字母准确率。证明通过将不同识别任务分离并解决后再进行融合的多专家方法，能更强大有效地实现稳健、高保真手语翻译。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MultiStream-LLM是一个模块化框架，用于解决手语翻译中的高速指拼和面部非手动暗示识别问题。</li>
<li>该框架采用专门预测器处理连续手语、指拼和唇读等任务，以改善性能。</li>
<li>通过轻量级转换器融合并行数据流，解决时间错位问题。</li>
<li>最终输出由大型语言模型生成句子。</li>
<li>在How2Sign基准测试中取得了较高的BLEU-4分数，验证了该方法的有效性。</li>
<li>在挑战性较大的指拼数据集上达到了较高的字母准确率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00030">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.00030v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.00030v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.00030v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.00030v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Simple-Yet-Effective-An-Information-Theoretic-Approach-to-Multi-LLM-Uncertainty-Quantification"><a href="#Simple-Yet-Effective-An-Information-Theoretic-Approach-to-Multi-LLM-Uncertainty-Quantification" class="headerlink" title="Simple Yet Effective: An Information-Theoretic Approach to Multi-LLM   Uncertainty Quantification"></a>Simple Yet Effective: An Information-Theoretic Approach to Multi-LLM   Uncertainty Quantification</h2><p><strong>Authors:Maya Kruse, Majid Afshar, Saksham Khatwani, Anoop Mayampurath, Guanhua Chen, Yanjun Gao</strong></p>
<p>Large language models (LLMs) often behave inconsistently across inputs, indicating uncertainty and motivating the need for its quantification in high-stakes settings. Prior work on calibration and uncertainty quantification often focuses on individual models, overlooking the potential of model diversity. We hypothesize that LLMs make complementary predictions due to differences in training and the Zipfian nature of language, and that aggregating their outputs leads to more reliable uncertainty estimates. To leverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a simple information-theoretic method that uses Jensen-Shannon Divergence to identify and aggregate well-calibrated subsets of LLMs. Experiments on binary prediction tasks demonstrate improved calibration and predictive performance compared to single-model and na&quot;ive ensemble baselines. In addition, we explore using MUSE as guided signals with chain-of-thought distillation to fine-tune LLMs for calibration. MUSE is available at:<a target="_blank" rel="noopener" href="https://github.com/LARK-NLP-Lab/MUSE">https://github.com/LARK-NLP-Lab/MUSE</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在输入上常常表现出不一致的行为，这体现了其不确定性，并激发了在高风险环境中对其进行量化评估的需求。先前关于校准和不确定性量化的工作通常侧重于单个模型，忽略了模型多样性的潜力。我们假设LLM由于训练和语言分布的Zipf属性而做出互补预测，并且由于聚合它们的输出而得到更可靠的不确定性估计。为了利用这一点，我们提出了MUSE（通过子集集合的多LLM不确定性），这是一种简单的信息理论方法，使用Jensen-Shannon散度来识别和聚合校准良好的LLM子集。在二元预测任务上的实验表明，与单模型和非精校准集合基线相比，该方法提高了校准和预测性能。此外，我们探索了使用MUSE作为引导信号，结合思考链蒸馏来微调LLM的校准。MUSE可在以下网址获得：<a target="_blank" rel="noopener" href="https://github.com/LARK-NLP-Lab/MUSE">https://github.com/LARK-NLP-Lab/MUSE</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07236v2">PDF</a> Accepted to EMNLP 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在不同输入下表现出不一致的行为，这体现了不确定性，在高风险环境中需要对其进行量化评估。现有关于校准和不确定性量化的研究往往集中在单个模型上，忽略了模型多样性的潜力。假设LLM因训练和语言分布的Zipf属性做出互补预测，汇聚它们的输出能提供更可靠的不确定性估计。为此，我们提出使用MUSE（通过子集集合利用多LLM不确定性），这是一种基于信息论的简单方法，利用Jensen-Shannon Divergence来识别和汇聚校准良好的LLM子集。在二元预测任务上的实验表明，与单模型和基线集合相比，MUSE提高了校准度和预测性能。此外，我们还探索了使用MUSE作为带有思维链蒸馏的引导信号来微调LLM的校准度。MUSE已公开于GitHub：<a target="_blank" rel="noopener" href="https://github.com/LARK-NLP-Lab/MUSE%E3%80%82">https://github.com/LARK-NLP-Lab/MUSE。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在不同输入下表现出不一致行为，体现不确定性，在高风险环境中需要量化评估。</li>
<li>现有研究多关注单个模型的校准和不确定性量化，忽略了模型多样性潜力。</li>
<li>LLM因训练和语言分布的Zipf属性做出互补预测。</li>
<li>MUSE方法利用信息论原理识别和汇聚校准良好的LLM子集，提高预测可靠性。</li>
<li>在二元预测任务上，MUSE相比单模型和基线集合表现出更好的校准度和预测性能。</li>
<li>MUSE可作为引导信号，通过思维链蒸馏方式提高LLM的校准度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07236">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2507.07236v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2507.07236v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2507.07236v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2507.07236v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2507.07236v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2507.07236v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ParEval-Repo-A-Benchmark-Suite-for-Evaluating-LLMs-with-Repository-level-HPC-Translation-Tasks"><a href="#ParEval-Repo-A-Benchmark-Suite-for-Evaluating-LLMs-with-Repository-level-HPC-Translation-Tasks" class="headerlink" title="ParEval-Repo: A Benchmark Suite for Evaluating LLMs with   Repository-level HPC Translation Tasks"></a>ParEval-Repo: A Benchmark Suite for Evaluating LLMs with   Repository-level HPC Translation Tasks</h2><p><strong>Authors:Joshua H. Davis, Daniel Nichols, Ishan Khillan, Abhinav Bhatele</strong></p>
<p>GPGPU architectures have become significantly more diverse in recent years, which has led to an emergence of a variety of specialized programming models and software stacks to support them. Portable programming models exist, but they require significant developer effort to port to and optimize for different hardware architectures. Large language models (LLMs) may help to reduce this programmer burden. In this paper, we present a novel benchmark and testing framework, ParEval-Repo, which can be used to evaluate the efficacy of LLM-based approaches in automatically translating entire codebases across GPGPU execution models. ParEval-Repo includes several scientific computing and AI mini-applications in a range of programming models and levels of repository complexity. We use ParEval-Repo to evaluate a range of state-of-the-art open-source and commercial LLMs, with both a non-agentic and a top-down agentic approach. We assess code generated by the LLMs and approaches in terms of compilability, functional correctness, categories of build errors, and the cost of translation in terms of the number of inference tokens. Our results demonstrate that LLM translation of scientific applications is feasible for small programs but difficulty with generating functional build systems and cross-file dependencies pose challenges in scaling to larger codebases. </p>
<blockquote>
<p>GPGPU架构近年来变得更为多样化，这导致了出现了多种专用的编程模型和软件栈来支持它们。虽然存在可移植的编程模型，但它们需要在不同的硬件架构上进行移植和优化，需要开发者付出大量的努力。大型语言模型（LLM）可能有助于减轻程序员的负担。在本文中，我们提出了一个新型的基准测试和评估框架ParEval-Repo，它可用于评估基于LLM的方法在GPGPU执行模型上自动翻译整个代码库的有效性。ParEval-Repo包含多种编程模型和各种仓库复杂度的科学计算和人工智能小型应用程序。我们使用ParEval-Repo评估了一系列最先进的开源和商业LLM，包括非代理方法和自上而下的代理方法。我们根据可编译性、功能正确性、构建错误类别以及推理令牌数量来评估LLM和方法生成的代码。我们的结果表明，LLM在科学应用程序的翻译方面是可行的，但对于生成功能性构建系统和跨文件依赖方面存在困难，这使得在大规模代码库上的扩展具有挑战性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20938v2">PDF</a> 10 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新型评估框架ParEval-Repo，用于评估大型语言模型（LLM）在GPGPU架构上自动翻译代码库的效果。该框架包含多个科学计算和AI小型应用程序，涵盖不同的编程模型和仓库复杂性级别。评估结果显示，LLM在翻译科学应用程序方面对于小型程序是可行的，但在生成功能构建系统和处理跨文件依赖方面仍存在挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPGPU架构的多样性导致了多种专用编程模型和软件堆栈的出现。</li>
<li>便携式编程模型需要开发者投入大量努力来适应和优化不同的硬件架构。</li>
<li>大型语言模型（LLM）有助于减轻程序员负担。</li>
<li>ParEval-Repo是一个新型评估框架，可用于评估LLM在GPGPU执行模型上自动翻译代码库的效果。</li>
<li>ParEval-Repo包含多个科学计算和AI小型应用程序，涵盖不同的编程模型和仓库复杂性级别。</li>
<li>LLM在翻译科学应用程序方面对于小型程序是可行的，但在生成功能构建系统和处理跨文件依赖方面存在挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20938">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2506.20938v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2506.20938v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2506.20938v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2506.20938v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Don’t-Make-It-Up-Preserving-Ignorance-Awareness-in-LLM-Fine-Tuning"><a href="#Don’t-Make-It-Up-Preserving-Ignorance-Awareness-in-LLM-Fine-Tuning" class="headerlink" title="Don’t Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning"></a>Don’t Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning</h2><p><strong>Authors:William F. Shen, Xinchi Qiu, Nicola Cancedda, Nicholas D. Lane</strong></p>
<p>Existing work on mitigating catastrophic forgetting during large language models (LLMs) fine-tuning for new knowledge instances has primarily focused on preserving performance on previously seen data, while critically overlooking the collapse of essential capabilities instilled through alignment, most notably the model’s ability to faithfully express epistemic uncertainty (a property we term ‘Ignorance Awareness’). In this work, we formalize the notion of Ignorance Awareness and illustrate that conventional fine-tuning methods can result in substantial activation displacement. This displacement undermines the critical capability of ignorance awareness, leading to undesirable behaviors such as hallucinations. To address this challenge, we introduce SEAT, a simple and principled fine-tuning approach that not only enables the model to effectively acquire new knowledge instances but also preserves its aligned ignorance awareness. SEAT integrates two key components: (1) sparse tuning that constrains activation drift, and (2) a novel entity perturbation method designed to counter knowledge entanglement. Experimental results demonstrate that, across both real-world and synthetic datasets, SEAT significantly outperforms baselines in preserving ignorance awareness while retaining optimal fine-tuning performance, offering a more robust solution for LLM fine-tuning. </p>
<blockquote>
<p>关于在大规模语言模型（LLM）针对新知识实例进行微调时缓解灾难性遗忘的现有工作主要集中在保持对先前数据的性能上，而严重忽视了通过对齐灌输的基本能力的崩溃，尤其是模型忠实表达认识不确定性的能力（我们称之为“无知意识”）。在这项工作中，我们正式提出无知意识的概念，并说明传统的微调方法可能导致显著的激活位移。这种位移会破坏无知意识的关键能力，从而导致出现如幻觉等不想要的行为。为了解决这一挑战，我们引入了SEAT，这是一种简单且基于原则的微调方法，它不仅能使模型有效地获取新知识实例，而且还能保持其对齐的无知意识。SEAT集成了两个关键组件：（1）稀疏调整，用于限制激活漂移；（2）一种新型的实体扰动方法，旨在对抗知识纠缠。实验结果表明，无论是在现实世界数据集还是合成数据集上，SEAT在保持无知意识的同时，在微调性能上显著优于基准线，为LLM微调提供了更稳健的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14387v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在大型语言模型（LLM）微调新知识实例时，现有工作主要关注如何在先前数据上保持性能，却忽视了通过对齐灌输的关键能力的崩溃，尤其是模型忠实表达认知不确定性的能力（我们称之为“无知意识”）。在这项工作中，我们正式提出了无知意识的概念，并说明传统微调方法可能导致显著的活动位移。这种位移破坏了无知意识的关键能力，从而导致出现如幻想等不想要的行为。为解决这一挑战，我们引入了SEAT，这是一种简单且以原则为基础的微调方法，不仅能使模型有效地获取新知识实例，而且还能保持其对齐的无知意识。SEAT集成了两个关键组件：（1）稀疏调优，约束活动漂移；（2）一种新型实体扰动方法，旨在对抗知识纠缠。实验结果表明，无论是在现实世界还是合成数据集上，SEAT在保持无知意识方面显著优于基线，同时保持最佳的微调性能，为LLM微调提供了更稳健的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有LLM微调工作主要关注保持先前数据性能，忽视了关键能力的崩溃，特别是忠实表达认知不确定性的能力（无知意识）。</li>
<li>传统微调方法可能导致活动位移，破坏无知意识。</li>
<li>SEAT是一种新型微调方法，旨在解决这一问题，同时保留新知识的获取和对齐的无知意识。</li>
<li>SEAT包含两个关键组件：稀疏调优和实体扰动方法。</li>
<li>稀疏调优用于约束活动漂移。</li>
<li>实体扰动方法旨在对抗知识纠缠。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14387">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2506.14387v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2506.14387v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2506.14387v2/page_2_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="First-Steps-Towards-Overhearing-LLM-Agents-A-Case-Study-With-Dungeons-Dragons-Gameplay"><a href="#First-Steps-Towards-Overhearing-LLM-Agents-A-Case-Study-With-Dungeons-Dragons-Gameplay" class="headerlink" title="First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons &amp;   Dragons Gameplay"></a>First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons &amp;   Dragons Gameplay</h2><p><strong>Authors:Andrew Zhu, Evan Osgood, Chris Callison-Burch</strong></p>
<p>Much work has been done on conversational LLM agents which directly assist human users with tasks. We present an alternative paradigm for interacting with LLM agents, which we call “overhearing agents”. These overhearing agents do not actively participate in conversation – instead, they “listen in” on human-to-human conversations and perform background tasks or provide suggestions to assist the user. In this work, we explore the overhearing agents paradigm through the lens of Dungeons &amp; Dragons gameplay. We present an in-depth study using large multimodal audio-language models as overhearing agents to assist a Dungeon Master. We perform a human evaluation to examine the helpfulness of such agents and find that some large audio-language models have the emergent ability to perform overhearing agent tasks using implicit audio cues. Finally, we release Python libraries and our project code to support further research into the overhearing agents paradigm at <a target="_blank" rel="noopener" href="https://github.com/zhudotexe/overhearing_agents">https://github.com/zhudotexe/overhearing_agents</a>. </p>
<blockquote>
<p>关于对话式LLM代理的研究已经取得了大量成果，这些代理能够直接帮助人类用户完成任务。本文介绍了一种与LLM代理交互的替代模式，我们称之为“旁听代理”。这些旁听代理不会主动参与对话——相反，它们会“听取”人类之间的对话，并在后台执行任务或提供建议以帮助用户。在这项工作中，我们从《龙与地下城》游戏的角度探讨了旁听代理模式。我们深入研究了使用大型多模态音频语言模型作为旁听代理来帮助地下城大师。我们进行了一项人类评估，以检查这些代理的有用性，并发现某些大型音频语言模型已经具备了利用隐性音频线索执行旁听代理任务的能力。最后，我们在<a target="_blank" rel="noopener" href="https://github.com/zhudotexe/overhearing_agents%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86Python%E5%BA%93%E5%92%8C%E9%A1%B9%E7%9B%AE%E4%BB%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E6%94%AF%E6%8C%81%E5%AF%B9%E6%97%81%E5%90%AC%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/zhudotexe/overhearing_agents上发布了Python库和项目代码，以支持对旁听代理模式的进一步研究。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22809v2">PDF</a> 9 pages, 5 figures. COLM 2025 Workshop on AI Agents</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为“overhearing agents”的新型人机交互模式。不同于传统的对话式LLM代理，overhearing agents不会主动参与对话，而是会倾听人类之间的对话并在后台执行任务或提供建议以协助用户。本文着重通过“Dungeons &amp; Dragons”游戏的视角，探讨overhearing agents的研究和应用。研究发现，大型多媒体音频语言模型可以作为协助游戏主持人（Dungeon Master）的overhearing agents。通过人类评估发现，某些大型音频语言模型能够利用隐式音频线索完成overhearing代理任务。最后，本文发布了Python库和项目代码以支持对overhearing代理模式的进一步研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了一种新型的人机交互模式——overhearing agents。</li>
<li>Overhearing agents不主动参与对话，而是倾听并完成任务或提供建议。</li>
<li>通过“Dungeons &amp; Dragons”游戏视角探讨了overhearing agents的应用和研究。</li>
<li>大型多媒体音频语言模型可以作为协助游戏主持人的overhearing agents。</li>
<li>一些大型音频语言模型具备利用隐式音频线索完成overhearing代理任务的能力。</li>
<li>通过人类评估验证了overhearing代理的效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22809">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.22809v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.22809v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.22809v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.22809v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.22809v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="RAVEN-Query-Guided-Representation-Alignment-for-Question-Answering-over-Audio-Video-Embedded-Sensors-and-Natural-Language"><a href="#RAVEN-Query-Guided-Representation-Alignment-for-Question-Answering-over-Audio-Video-Embedded-Sensors-and-Natural-Language" class="headerlink" title="RAVEN: Query-Guided Representation Alignment for Question Answering over   Audio, Video, Embedded Sensors, and Natural Language"></a>RAVEN: Query-Guided Representation Alignment for Question Answering over   Audio, Video, Embedded Sensors, and Natural Language</h2><p><strong>Authors:Subrata Biswas, Mohammad Nur Hossain Khan, Bashima Islam</strong></p>
<p>Multimodal question answering (QA) often requires identifying which video, audio, or sensor tokens are relevant to the question. Yet modality disagreements are common: off-camera speech, background noise, or motion outside the field of view often mislead fusion models that weight all streams equally. We present RAVEN, a unified QA architecture whose core is QuART, a query-conditioned cross-modal gating module that assigns scalar relevance scores to each token across modalities, enabling the model to amplify informative signals and suppress distractors before fusion. RAVEN is trained through a three-stage pipeline comprising unimodal pretraining, query-aligned fusion, and disagreement-oriented fine-tuning – each stage targeting a distinct challenge in multi-modal reasoning: representation quality, cross-modal relevance, and robustness to modality mismatch. To support training and evaluation, we release AVS-QA, a dataset of 300K synchronized Audio–Video-Sensor streams paired with automatically generated question-answer pairs. Experimental results on seven multi-modal QA benchmarks – including egocentric and exocentric tasks – show that RAVEN achieves up to 14.5% and 8.0% gains in accuracy compared to state-of-the-art multi-modal large language models, respectively. Incorporating sensor data provides an additional 16.4% boost, and the model remains robust under modality corruption, outperforming SOTA baselines by 50.23%. Our code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/BASHLab/RAVEN">https://github.com/BASHLab/RAVEN</a>. </p>
<blockquote>
<p>多模态问答（QA）通常需要识别哪些视频、音频或传感器标记与问题相关。然而，模态冲突很常见：离镜头外的语音、背景噪音或视野外的动作经常误导将所有流平等对待的融合模型。我们提出了RAVEN，这是一种统一问答架构，其核心是QuART，一种查询条件跨模态门控模块，它为每个模态的标记分配标量相关性分数，使模型能够在融合之前放大信息信号并抑制干扰因素。RAVEN通过包括单模态预训练、查询对齐融合和面向分歧的微调的三个阶段管道进行训练——每个阶段都针对多模态推理中的不同挑战：表示质量、跨模态相关性和对模态不匹配的稳健性。为了支持训练和评估，我们发布了AVS-QA数据集，其中包含30万个自动生成的配对问题答案的同步音频-视频-传感器流。在七个多模态问答基准测试上的实验结果表明，与最新的多模态大型语言模型相比，RAVEN在准确率上分别提高了14.5%和8.0%。加入传感器数据提供了额外的16.4%的提升，并且在模态损坏的情况下，该模型仍然稳健，优于最新基准测试50.23%。我们的代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/BASHLab/RAVEN%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/BASHLab/RAVEN上获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17114v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了RAVEN架构在多模态问答（QA）中的创新应用。针对视频、音频和传感器数据的融合问题，RAVEN通过引入QuART模块，为不同模态的令牌分配标量相关性分数，提升信息信号的强度并抑制干扰因素。通过三个阶段训练管道，包括单模态预训练、查询对齐融合和面向分歧的微调，RAVEN解决了多模态推理中的三大挑战。同时，为了支持训练和评估，发布了AVS-QA数据集。实验结果表明，RAVEN相较于目前领先的多模态大型语言模型有显著提升。结合传感器数据带来的准确性提高额外达16.4%，并在模态损坏的情况下保持稳健性，超出基线模型50.23%。代码和数据集已在指定网站公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAVEN是一个多模态问答架构，针对视频、音频和传感器数据的融合问题提出创新解决方案。</li>
<li>RAVEN的核心是QuART模块，能根据查询条件为不同模态的令牌分配相关性分数。</li>
<li>RAVEN通过三个阶段训练管道解决多模态推理中的关键挑战。</li>
<li>AVS-QA数据集的发布支持了RAVEN的训练和评估。</li>
<li>实验结果显示，RAVEN相较于现有模型显著提高多模态问答的准确性。</li>
<li>结合传感器数据可进一步提高RAVEN的准确性达16.4%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17114">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.17114v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.17114v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.17114v3/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.17114v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.17114v3/page_4_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ArtRAG-Retrieval-Augmented-Generation-with-Structured-Context-for-Visual-Art-Understanding"><a href="#ArtRAG-Retrieval-Augmented-Generation-with-Structured-Context-for-Visual-Art-Understanding" class="headerlink" title="ArtRAG: Retrieval-Augmented Generation with Structured Context for   Visual Art Understanding"></a>ArtRAG: Retrieval-Augmented Generation with Structured Context for   Visual Art Understanding</h2><p><strong>Authors:Shuai Wang, Ivona Najdenkoska, Hongyi Zhu, Stevan Rudinac, Monika Kackovic, Nachoem Wijnberg, Marcel Worring</strong></p>
<p>Understanding visual art requires reasoning across multiple perspectives – cultural, historical, and stylistic – beyond mere object recognition. While recent multimodal large language models (MLLMs) perform well on general image captioning, they often fail to capture the nuanced interpretations that fine art demands. We propose ArtRAG, a novel, training-free framework that combines structured knowledge with retrieval-augmented generation (RAG) for multi-perspective artwork explanation. ArtRAG automatically constructs an Art Context Knowledge Graph (ACKG) from domain-specific textual sources, organizing entities such as artists, movements, themes, and historical events into a rich, interpretable graph. At inference time, a multi-granular structured retriever selects semantically and topologically relevant subgraphs to guide generation. This enables MLLMs to produce contextually grounded, culturally informed art descriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG outperforms several heavily trained baselines. Human evaluations further confirm that ArtRAG generates coherent, insightful, and culturally enriched interpretations. </p>
<blockquote>
<p>理解视觉艺术需要超越单纯物体识别的多个角度——文化、历史和风格——的推理。虽然最近的多模态大型语言模型（MLLMs）在一般图像标题创作方面表现良好，但它们往往无法捕捉到艺术所要求的微妙解读。我们提出了ArtRAG，这是一个新型的无训练框架，它将结构化知识与检索增强生成（RAG）相结合，用于多角度艺术品解释。ArtRAG自动从特定领域的文本源构建艺术上下文知识图谱（ACKG），将艺术家、运动、主题和历史事件等实体组织成一个丰富、可解释的图谱。在推理过程中，多粒度结构化检索器选择语义和拓扑上相关的子图来引导生成。这使得MLLM能够产生具有上下文和文化内涵的艺术描述。在SemArt和Artpedia数据集上的实验表明，ArtRAG优于几个经过大量训练的基础模型。人类评估进一步证实，ArtRAG生成的解释具有连贯性、洞察力和文化丰富性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06020v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>艺术理解需要跨越文化、历史和风格等多个角度进行推理，而不仅仅是简单的物体识别。近期多模态大型语言模型在一般图像描述方面表现良好，但在对美术作品的细致解读上常显不足。本文提出了ArtRAG框架，这是一个结合结构化知识与检索增强生成（RAG）的无训练框架，用于多视角艺术作品解释。ArtRAG自动从特定文本源构建艺术语境知识图谱（ACKG），将艺术家、运动、主题和历史事件等实体组织成丰富且可解释的图谱。在推理过程中，多粒度结构化检索器选择语义和拓扑上相关的子图来指导生成。这使得MLLMs能够产生具有语境和文化背景的艺术描述。在SemArt和Artpedia数据集上的实验表明，ArtRAG优于多个经过训练的重基模型。人类评估进一步证实，ArtRAG生成的解释具有连贯性、洞察力和文化丰富性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>理解视觉艺术需要跨越文化、历史和风格等多角度进行推理。</li>
<li>现有的多模态大型语言模型在美术作品的细致解读上可能存在不足。</li>
<li>ArtRAG是一个结合结构化知识与检索增强生成的框架，用于多视角艺术作品解释。</li>
<li>ArtRAG自动构建艺术语境知识图谱（ACKG），组织相关实体以提供丰富且可解释的信息。</li>
<li>多粒度结构化检索器选择相关子图来指导生成，产生具有语境和文化背景的艺术描述。</li>
<li>ArtRAG在SemArt和Artpedia数据集上的表现优于多个训练有素的基线模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06020">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.06020v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.06020v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.06020v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.06020v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.06020v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.06020v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Simulate-Personas-with-Reversed-Performance-A-Benchmark-for-Counterfactual-Instruction-Following"><a href="#Can-LLMs-Simulate-Personas-with-Reversed-Performance-A-Benchmark-for-Counterfactual-Instruction-Following" class="headerlink" title="Can LLMs Simulate Personas with Reversed Performance? A Benchmark for   Counterfactual Instruction Following"></a>Can LLMs Simulate Personas with Reversed Performance? A Benchmark for   Counterfactual Instruction Following</h2><p><strong>Authors:Sai Adith Senthil Kumar, Hao Yan, Saipavan Perepa, Murong Yue, Ziyu Yao</strong></p>
<p>Large Language Models (LLMs) are now increasingly widely used to simulate personas in virtual environments, leveraging their instruction-following capability. However, we discovered that even state-of-the-art LLMs cannot simulate personas with reversed performance (e.g., student personas with low proficiency in educational settings), which impairs the simulation diversity and limits the practical applications of the simulated environments. In this work, using mathematical reasoning as a representative scenario, we propose the first benchmark dataset for evaluating LLMs on simulating personas with reversed performance, a capability that we dub “counterfactual instruction following”. We evaluate both open-weight and closed-source LLMs on this task and find that LLMs, including the OpenAI o1 reasoning model, all struggle to follow counterfactual instructions for simulating reversedly performing personas. Intersectionally simulating both the performance level and the race population of a persona worsens the effect even further. These results highlight the challenges of counterfactual instruction following and the need for further research. </p>
<blockquote>
<p>大型语言模型（LLM）现在越来越广泛地用于模拟虚拟环境中的角色，利用其遵循指令的能力。然而，我们发现即使是最先进的大型语言模型也无法模拟具有反向表现的角色（例如在教育环境中表现不佳的学生角色），这损害了模拟的多样性并限制了模拟环境的实际应用。在这项工作中，我们以数学推理为代表场景，提出了评估大型语言模型模拟具有反向表现角色的第一个基准数据集，我们将这种能力称为“反事实指令遵循”。我们在此任务上评估了开放权重和封闭式的大型语言模型，发现包括OpenAI o1推理模型在内的大型语言模型都难以遵循模拟反向表现角色的反事实指令。同时模拟角色的表现水平和种族人口进一步加剧了这种效果。这些结果突出了反事实指令遵循的挑战和进一步研究的必要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06460v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLM模型被广泛用于模拟虚拟环境中的角色，但其难以模拟反向表现的角色，限制了模拟环境的多样性和实际应用。本研究提出一个评估LLM模拟反向表现角色的能力的新基准数据集，称为“反事实指令跟随”。评估发现，包括OpenAI o1推理模型在内的LLM都难以遵循反事实指令模拟反向表现角色，且同时模拟角色的表现水平和种族人口特征会进一步加剧效果。这凸显了反事实指令跟随的挑战和进一步研究的必要性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM模型广泛用于模拟虚拟环境中的角色。</li>
<li>LLM难以模拟具有反向表现的角色，限制了模拟环境的多样性和实际应用。</li>
<li>提出一个评估LLM模拟反向表现角色的能力的新基准数据集。</li>
<li>包括OpenAI o1推理模型在内的LLM在模拟反向表现角色时难以遵循反事实指令。</li>
<li>同时模拟角色的表现水平和种族人口特征会进一步加剧这一挑战。</li>
<li>反事实指令跟随是一个重要的研究方向，需要进一步探索和研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06460">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2504.06460v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2504.06460v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2504.06460v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="STADE-Standard-Deviation-as-a-Pruning-Metric"><a href="#STADE-Standard-Deviation-as-a-Pruning-Metric" class="headerlink" title="STADE: Standard Deviation as a Pruning Metric"></a>STADE: Standard Deviation as a Pruning Metric</h2><p><strong>Authors:Diego Coello de Portugal Mecke, Haya Alyoussef, Maximilian Stubbemann, Ilia Koloiarov, Tom Hanika, Lars Schmidt-Thieme</strong></p>
<p>Recently, Large Language Models (LLMs) have become very widespread and are used to solve a wide variety of tasks. To successfully handle these tasks, LLMs require longer training times and larger model sizes. This makes LLMs ideal candidates for pruning methods that reduce computational demands while maintaining performance. Previous methods require a retraining phase after pruning to maintain the original model’s performance. However, state-of-the-art pruning methods, such as Wanda, prune the model without retraining, making the pruning process faster and more efficient. Building upon Wanda’s work, this study provides a theoretical explanation of why the method is effective and leverages these insights to enhance the pruning process. Specifically, a theoretical analysis of the pruning problem reveals a common scenario in Machine Learning where Wanda is the optimal pruning method. Furthermore, this analysis is extended to cases where Wanda is no longer optimal, leading to the development of a new method, STADE, based on the standard deviation of the input. From a theoretical standpoint, STADE demonstrates better generality across different scenarios. Finally, extensive experiments on Llama and Open Pre-trained Transformers (OPT) models validate these theoretical findings, showing that depending on the training conditions, Wanda’s optimal performance varies as predicted by the theoretical framework. These insights contribute to a more robust understanding of pruning strategies and their practical implications. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/Coello-dev/STADE/">https://github.com/Coello-dev/STADE/</a> </p>
<blockquote>
<p>最近，大型语言模型（LLM）已经变得非常普遍，并用于解决各种任务。为了成功处理这些任务，LLM需要更长的训练时间和更大的模型大小。这使得LLM成为修剪方法的理想候选者，这些方法可以在保持性能的同时减少计算需求。之前的方法需要在修剪后进行重训以保持原始模型的性能。然而，最新颖的修剪方法，如旺达（Wanda），无需重训即可修剪模型，使修剪过程更快、更高效。基于旺达的工作，本研究提供了该方法有效的理论解释，并利用这些见解来改进修剪过程。具体来说，对修剪问题的理论分通过机器学习中的一个常见情景揭示了旺达（Wanda）成为最优修剪方法的原因。此外，该分析扩展到了旺达不再是最优的情况，从而开发出一种新的方法——STADE，该方法基于输入的标凓差。从理论角度看，STADE在不同场景中具有更好的通用性。最后，对Llama和开放预训练转换器（OPT）模型的广泛实验验证了这些理论发现，表明根据不同的训练条件，旺达的最佳性能如理论框架所预测的那样会有所变化。这些见解为更稳健地理解修剪策略及其实际应用提供了贡献。代码可用在：<a target="_blank" rel="noopener" href="https://github.com/Coello-dev/STADE/">https://github.com/Coello-dev/STADE/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22451v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLM训练时间长、模型规模大，适合采用剪枝方法降低计算需求并保持性能。最新剪枝方法如Wanda无需重训，提升剪枝效率。本研究从理论角度解释了Wanda方法的有效性，并据此优化了剪枝过程。同时提出了一种新的基于输入标准差的方法STADE，其表现较优于Wanda在某些情况下的通用性。实验验证显示，理论框架预测了Wanda在不同训练条件下的最佳性能。这些见解有助于更深入地理解剪枝策略及其实际应用。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM因其长时间训练和大规模模型特点，适合使用剪枝方法来优化计算需求和性能保持。</li>
<li>最新剪枝方法如Wanda无需重训阶段，提升了剪枝过程的效率和速度。</li>
<li>本研究提供了对Wanda方法有效性的理论解释，并基于这些洞察优化了剪枝过程。</li>
<li>研究提出了一种新的剪枝方法STADE，其在不同情境下的通用性表现优于Wanda。</li>
<li>STADE的理论分析扩展了对剪枝问题的理解。</li>
<li>实验结果验证了理论框架的预测，表明Wanda的最佳性能取决于训练条件。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22451">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.22451v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.22451v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.22451v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.22451v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.22451v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.22451v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Instruction-Oriented-Preference-Alignment-for-Enhancing-Multi-Modal-Comprehension-Capability-of-MLLMs"><a href="#Instruction-Oriented-Preference-Alignment-for-Enhancing-Multi-Modal-Comprehension-Capability-of-MLLMs" class="headerlink" title="Instruction-Oriented Preference Alignment for Enhancing Multi-Modal   Comprehension Capability of MLLMs"></a>Instruction-Oriented Preference Alignment for Enhancing Multi-Modal   Comprehension Capability of MLLMs</h2><p><strong>Authors:Zitian Wang, Yue Liao, Kang Rong, Fengyun Rao, Yibo Yang, Si Liu</strong></p>
<p>Preference alignment has emerged as an effective strategy to enhance the performance of Multimodal Large Language Models (MLLMs) following supervised fine-tuning. While existing preference alignment methods predominantly target hallucination factors, they overlook the factors essential for multi-modal comprehension capabilities, often narrowing their improvements on hallucination mitigation. To bridge this gap, we propose Instruction-oriented Preference Alignment (IPA), a scalable framework designed to automatically construct alignment preferences grounded in instruction fulfillment efficacy. Our method involves an automated preference construction coupled with a dedicated verification process that identifies instruction-oriented factors, avoiding significant variability in response representations. Additionally, IPA incorporates a progressive preference collection pipeline, further recalling challenging samples through model self-evolution and reference-guided refinement. Experiments conducted on Qwen2VL-7B demonstrate IPA’s effectiveness across multiple benchmarks, including hallucination evaluation, visual question answering, and text understanding tasks, highlighting its capability to enhance general comprehension. </p>
<blockquote>
<p>偏好对齐作为一种有效的策略，在监督微调后用于提高多模态大语言模型（MLLMs）的性能。尽管现有的偏好对齐方法主要着眼于幻觉因素，但它们忽视了多模态理解能力所必需的因素，通常仅在缓解幻觉方面有所改善。为了弥补这一差距，我们提出了面向指令的偏好对齐（IPA）这一可扩展框架，旨在根据指令执行效率自动构建偏好对齐。我们的方法包括自动化偏好构建以及与专用验证过程的结合，可识别面向指令的因素，避免响应表示中的重大差异。此外，IPA还采用渐进的偏好收集管道，通过模型自我进化以及参考引导细化来进一步回忆具有挑战性的样本。在Qwen2VL-7B上进行的实验证明了IPA在多基准测试中的有效性，包括幻觉评估、视觉问答和文本理解任务等，突显其在提高整体理解能力方面的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20309v2">PDF</a> Accepted by ICCV 2025</p>
<p><strong>摘要</strong><br>    提出了一种名为指令导向偏好对齐（IPA）的可扩展框架，用于自动构建基于指令执行效率的对齐偏好，以提高多模态大语言模型（MLLMs）的性能。IPA通过自动化偏好构建和专用验证过程，识别指令导向因素，避免响应表示的显著变化。此外，IPA还采用渐进式偏好收集管道，通过模型自我进化参考引导细化来进一步回忆挑战样本。实验表明，IPA在多个基准测试中均表现出有效性和优越性，包括幻视评估、视觉问答和文本理解任务，突显其在提高整体理解能力方面的潜力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>偏好对齐已被证明是增强多模态大语言模型性能的有效策略。</li>
<li>现有偏好对齐方法主要关注幻视因素，忽略了多模态理解能力的关键因素。</li>
<li>指令导向偏好对齐（IPA）框架旨在自动构建基于指令执行效率的对齐偏好，以弥补现有方法的不足。</li>
<li>IPA通过自动化偏好构建和专用验证过程，有效识别指令导向因素，确保模型响应的稳定性。</li>
<li>IPA采用渐进式偏好收集管道，通过模型自我进化参考引导细化，能够进一步回忆并处理挑战样本。</li>
<li>实验结果表明，IPA在多个任务上表现出色，包括幻视评估、视觉问答和文本理解。</li>
<li>IPA提高了模型的整体理解能力，为未来多模态语言模型的发展提供了新方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20309">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.20309v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.20309v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.20309v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.20309v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.20309v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="InteLiPlan-An-Interactive-Lightweight-LLM-Based-Planner-for-Domestic-Robot-Autonomy"><a href="#InteLiPlan-An-Interactive-Lightweight-LLM-Based-Planner-for-Domestic-Robot-Autonomy" class="headerlink" title="InteLiPlan: An Interactive Lightweight LLM-Based Planner for Domestic   Robot Autonomy"></a>InteLiPlan: An Interactive Lightweight LLM-Based Planner for Domestic   Robot Autonomy</h2><p><strong>Authors:Kim Tien Ly, Kai Lu, Ioannis Havoutis</strong></p>
<p>We introduce an interactive LLM-based framework designed to enhance the autonomy and robustness of domestic robots, targeting embodied intelligence. Our approach reduces reliance on large-scale data and incorporates a robot-agnostic pipeline that embodies an LLM. Our framework, InteLiPlan, ensures that the LLM’s decision-making capabilities are effectively aligned with robotic functions, enhancing operational robustness and adaptability, while our human-in-the-loop mechanism allows for real-time human intervention when user instruction is required. We evaluate our method in both simulation and on the real Toyota Human Support Robot and Anymal D-Unitree Z1 platforms. Our method achieves a 95% success rate in the ‘fetch me’ task completion with failure recovery, highlighting its capability in both failure reasoning and task planning. InteLiPlan achieves comparable performance to state-of-the-art large-scale LLM-based robotics planners, while using only real-time onboard computing. </p>
<blockquote>
<p>我们引入了一个基于大型语言模型（LLM）的交互式框架，旨在提高家用机器人的自主性和稳健性，主要面向体现智能。我们的方法减少对大规模数据的依赖，并融入一个通用的机器人管道，其中包含了大型语言模型。我们的框架InteLiPlan确保大型语言模型的决策能力与机器人功能有效对齐，提高操作稳健性和适应性。同时，我们的人机交互机制在用户需要指令时允许实时人工干预。我们在模拟环境和真实的丰田人机支持机器人以及Anymal D-Unitree Z1平台上评估了我们的方法。我们的方法在“帮我取东西”的任务完成中实现了95%的成功率，并在失败恢复中表现出色，突显了其在故障推理和任务规划方面的能力。InteLiPlan与最新的大型语言模型机器人规划器相比具有竞争力，同时仅使用实时机载计算。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.14506v3">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了一个基于LLM的交互式框架InteLiPlan，旨在提高家用机器人的自主性和稳健性，主要目标是实现机器人智能。该方法减少了大规模数据的依赖，并融入了LLM的机器人通用管道。InteLiPlan确保LLM的决策能力与机器人功能有效对齐，提高了操作稳健性和适应性。此外，其人类介入机制允许在用户指导需要时实时介入。在模拟和真实丰田人类支持机器人以及Anymal D-Unitree Z1平台上进行了评估，完成“请递给我”任务的成功率达到了95%，展现出在失败恢复、故障推理和任务规划方面的能力。InteLiPlan在与当前大型LLM基于机器人的规划器相当的性能表现中取得了优越的表现，但只使用了实时的机上计算能力。该框架将为下一代机器人的发展提供关键思路和方法支持。<br>  <strong>关键见解</strong></p>
<ol>
<li>介绍了基于LLM的交互式框架InteLiPlan，用于提高家用机器人的自主性和稳健性。</li>
<li>该框架融入了一种机器人通用管道结构，包含了LLM模型元素和功能特性优化元素组合构成的指令对齐与校正模式生成引擎以执行任务行为学习结果的动作方案策略生成模块。这种设计提高了机器人操作的稳健性和适应性。同时减少了大规模数据的依赖。通过该框架，机器人可以更加智能地完成任务，减少人为干预的需求。 </li>
<li>InteLiPlan允许实时的人类介入机制在用户指导需要时发挥作用，确保了系统的灵活性和高效性。此外还具备自主决策能力。机器人可以在执行任务过程中根据环境变化做出判断和调整，提高任务的完成率和效率。这种机制确保了用户与机器人之间的顺畅交流，提高了用户体验。 </li>
<li>通过模拟和实际实验验证发现，该框架的性能优越表现在智能感知处理场景的分析任务和行为能力匹配学习模型中评估标准的成绩具有领先地位能够实现95%的成功率完成“请递给我”任务并展现出故障恢复能力。这表明该框架在实际应用中具有高度的可靠性和稳定性。 </li>
<li>InteLiPlan的性能表现与当前的先进的大型LLM基于机器人的规划器相比颇具竞争力同时具有快速的运算处理响应实时配合以常规更新系统运行新出文档节点归纳错误特征和完备映射处置文书复杂性等特性。这些特性使得InteLiPlan在实际应用中更加灵活高效可靠能够满足各种复杂场景的需求。 </li>
<li>InteLiPlan框架的设计思想具有创新性能够为下一代机器人的发展提供关键思路和方法支持推动了人工智能领域的发展并展现出广泛的应用前景。其灵活性和可扩展性使得该框架能够适应未来机器人技术的快速发展并满足不断变化的市场需求。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.14506">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2409.14506v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2409.14506v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2409.14506v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2409.14506v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2409.14506v3/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2409.14506v3/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2409.14506v3/page_5_2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Bitune-Leveraging-Bidirectional-Attention-to-Improve-Decoder-Only-LLMs"><a href="#Bitune-Leveraging-Bidirectional-Attention-to-Improve-Decoder-Only-LLMs" class="headerlink" title="Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs"></a>Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs</h2><p><strong>Authors:Dawid J. Kopiczko, Tijmen Blankevoort, Yuki M. Asano</strong></p>
<p>Decoder-only large language models typically rely solely on masked causal attention, which limits their expressiveness by restricting information flow to one direction. We propose Bitune, a method that enhances pretrained decoder-only LLMs by incorporating bidirectional attention into prompt processing. We evaluate Bitune in instruction-tuning and question-answering settings, showing significant improvements in performance on commonsense reasoning, arithmetic, and language understanding tasks. Furthermore, extensive ablation studies validate the role of each component of the method, and demonstrate that Bitune is compatible with various parameter-efficient finetuning techniques and full model finetuning. </p>
<blockquote>
<p>解码器仅大型语言模型通常仅依赖于掩码因果注意力，这将信息流动限制在一个方向上，从而限制了其表现力。我们提出了Bitune方法，它通过将在提示处理中融入双向注意力，增强了预训练解码器仅大型语言模型（LLM）的功能。我们在指令调整和问答环境中评估了Bitune，在常识推理、算术和语言理解任务上的性能表现出显著提高。此外，广泛的消融研究验证了该方法各组成部分的作用，并证明Bitune与各种参数有效的微调技术和全模型微调兼容。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14862v2">PDF</a> </p>
<p><strong>Summary</strong>：<br>解码器仅大型语言模型通常仅依赖于掩码因果注意力，这限制了其表现力，因为信息流向被限制在一个方向上。我们提出了Bitune方法，它通过引入双向注意力来提升预训练的解码器仅LLMs在提示处理方面的性能。在指令调整和问答设置中评估Bitune时，显示其在常识推理、算术和语言理解任务上的性能显著提高。此外，广泛的消融研究验证了该方法各组件的作用，并证明Bitune可以与各种参数高效的微调技术和全模型微调兼容。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>解码器仅大型语言模型受限于掩码因果注意力，信息流向单一。</li>
<li>Bitune方法通过引入双向注意力提升预训练解码器仅LLMs的性能。</li>
<li>Bitune在指令调整和问答设置中表现出显著的性能提升，特别是在常识推理、算术和语言理解任务上。</li>
<li>消融研究验证了Bitune方法各组件的作用。</li>
<li>Bitune与参数高效的微调技术和全模型微调兼容。</li>
<li>Bitune方法可能有助于解决解码器仅LLMs的表达局限性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14862">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2405.14862v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2405.14862v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2405.14862v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2405.14862v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2405.14862v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Why-Not-Transform-Chat-Large-Language-Models-to-Non-English"><a href="#Why-Not-Transform-Chat-Large-Language-Models-to-Non-English" class="headerlink" title="Why Not Transform Chat Large Language Models to Non-English?"></a>Why Not Transform Chat Large Language Models to Non-English?</h2><p><strong>Authors:Xiang Geng, Ming Zhu, Jiahuan Li, Zhejian Lai, Wei Zou, Shuaijie She, Jiaxin Guo, Xiaofeng Zhao, Yinglu Li, Yuang Li, Chang Su, Yanqing Zhao, Xinglin Lyu, Min Zhang, Jiajun Chen, Hao Yang, Shujian Huang</strong></p>
<p>The scarcity of non-English data limits the development of non-English large language models (LLMs). Transforming English-centric LLMs to non-English has been identified as an effective and resource-efficient method. Previous works start from base LLMs and perform knowledge distillation (KD) with data generated by stronger LLMs, e.g. GPT-4. Compared to base LLMs, chat LLMs are further optimized for advanced abilities, e.g. multi-turn conversation and human preference alignment, and thus more powerful in both helpfulness and safety. However, transforming a chat LLM involves two critical issues: (1) How can we effectively transfer advanced abilities without their supervised data? (2) How can we prevent the original knowledge from catastrophic forgetting during transformation? We target these issues by introducing a simple framework called TransLLM. For the first issue, TransLLM divides the transfer problem into some common sub-tasks with the translation chain-of-thought, which uses the translation as the bridge between English and non-English step-by-step. We further enhance the performance of sub-tasks with publicly available data. For the second issue, we propose a method comprising two synergistic components: low-rank adaptation for training to maintain the original LLM parameters, and recovery KD, which utilizes data generated by the chat LLM itself to recover the original knowledge from the frozen parameters. In the experiments, we transform the LLaMA-2-chat-7B to the Thai language. Our method, using only single-turn data, outperforms strong baselines and ChatGPT on multi-turn benchmark MT-bench. Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4. Code is available at <a target="_blank" rel="noopener" href="https://github.com/hy5468/TransLLM">https://github.com/hy5468/TransLLM</a>. </p>
<blockquote>
<p>非英语数据的稀缺限制了非英语大型语言模型（LLM）的发展。将英语中心主义的LLM转变为非英语已被证明是一种有效且资源高效的方法。以前的工作从基础LLM开始，使用由更强大的LLM（例如GPT-4）生成的数据进行知识蒸馏（KD）。与基础LLM相比，聊天LLM针对高级能力进行了进一步优化，例如多轮对话和人类偏好对齐，因此在有用性和安全性方面都更加强大。然而，将聊天LLM转换涉及两个关键问题：（1）如何在没有监督数据的情况下有效地转移高级能力？（2）如何在转换过程中防止原有知识的灾难性遗忘？我们通过一个名为TransLLM的简单框架来解决这些问题。对于第一个问题，TransLLM将转移问题分解为一些具有翻译思维链的共同子任务，利用翻译作为英语和非英语之间的桥梁，逐步进行。我们进一步使用公开数据提高了子任务的性能。对于第二个问题，我们提出了一种由两个协同组件组成的方法：低秩适应训练以保持原始LLM参数，以及恢复KD，后者利用聊天LLM本身生成的数据从冻结的参数中恢复原始知识。在实验中，我们将LLaMA-2-chat-7B转变为泰语。我们的方法仅使用单轮数据，在多轮基准测试MT-bench上的表现优于强大的基准测试和ChatGPT。此外，我们的方法在没有安全数据的情况下，在安全基准测试AdvBench中拒绝更多有害的查询比ChatGPT和GPT-4更多。代码可在<a target="_blank" rel="noopener" href="https://github.com/hy5468/TransLLM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hy5468/TransLLM获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.13923v3">PDF</a> The article has been accepted by Frontiers of Computer Science (FCS),   with the DOI: {10.1007&#x2F;s11704-025-50646-z}</p>
<p><strong>Summary</strong></p>
<p>本文探讨了非英语大型语言模型（LLM）发展的局限性，并指出将英语中心主义的LLM转型为非英语LLM是一种有效且资源高效的方法。文章介绍了一个名为TransLLM的框架，该框架旨在解决将聊天LLM转型为非英语LLM时面临的挑战，包括如何有效转移高级能力以及如何在转型过程中防止原有知识灾难性遗忘的问题。实验表明，该框架在多任务基准测试中表现优异，且能有效提高安全性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>非英语数据稀缺限制了非英语大型语言模型（LLM）的发展。</li>
<li>将英语中心的LLM转型为非英语LLM是一种有效且资源高效的方法。</li>
<li>TransLLM框架旨在解决聊天LLM转型中的两个关键问题：有效转移高级能力和防止原有知识的灾难性遗忘。</li>
<li>TransLLM通过翻译思维链将转移问题分为一些通用子任务，并使用公开数据提高子任务性能。</li>
<li>TransLLM提出的方法包括两个协同组件：低秩适应训练和恢复知识蒸馏。</li>
<li>实验表明，TransLLM框架在多任务基准测试中表现优于强基线和ChatGPT。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.13923">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2405.13923v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2405.13923v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2405.13923v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2405.13923v3/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2405.13923v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-09/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-09/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-09/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_Agent/2508.18708v2/page_1_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-09-09  TalkToAgent A Human-centric Explanation of Reinforcement Learning   Agents with Large Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-09/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.05249v1/page_4_0.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-09-09  COGITAO A Visual Reasoning Framework To Study Compositionality &   Generalization
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29474.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
