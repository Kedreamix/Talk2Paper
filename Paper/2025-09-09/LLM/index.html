<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-09  Scaling Performance of Large Language Model Pretraining">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04650v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    72 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-09-æ›´æ–°"><a href="#2025-09-09-æ›´æ–°" class="headerlink" title="2025-09-09 æ›´æ–°"></a>2025-09-09 æ›´æ–°</h1><h2 id="Scaling-Performance-of-Large-Language-Model-Pretraining"><a href="#Scaling-Performance-of-Large-Language-Model-Pretraining" class="headerlink" title="Scaling Performance of Large Language Model Pretraining"></a>Scaling Performance of Large Language Model Pretraining</h2><p><strong>Authors:Alexander Interrante-Grant, Carla Varela-Rosa, Suhaas Narayan, Chris Connelly, Albert Reuther</strong></p>
<p>Large language models (LLMs) show best-in-class performance across a wide range of natural language processing applications. Training these models is an extremely computationally expensive task; frontier Artificial Intelligence (AI) research companies are investing billions of dollars into supercomputing infrastructure to train progressively larger models on increasingly massive datasets. Unfortunately, information about the scaling performance and training considerations of these large training pipelines is scarce in public literature. Working with large-scale datasets and models can be complex and practical recommendations are scarce in the public literature for tuning training performance when scaling up large language models. In this paper, we aim to demystify the large language model pretraining pipeline somewhat - in particular with respect to distributed training, managing large datasets across hundreds of nodes, and scaling up data parallelism with an emphasis on fully leveraging available GPU compute capacity. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¹¿æ³›çš„è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚è®­ç»ƒè¿™äº›æ¨¡å‹æ˜¯ä¸€é¡¹æå…¶è€—è´¹è®¡ç®—èµ„æºçš„ä»»åŠ¡ï¼›å‰æ²¿çš„äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç ”ç©¶å…¬å¸æ­£åœ¨æŠ•å…¥æ•°åäº¿ç¾å…ƒç”¨äºè¶…çº§è®¡ç®—åŸºç¡€è®¾æ–½ï¼Œä»¥ä¾¿åœ¨æ—¥ç›Šåºå¤§çš„æ•°æ®é›†ä¸Šè®­ç»ƒè§„æ¨¡è¶Šæ¥è¶Šå¤§çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œå…³äºè¿™äº›å¤§è§„æ¨¡è®­ç»ƒç®¡é“çš„å¯æ‰©å±•æ€§èƒ½å’Œè®­ç»ƒæ³¨æ„äº‹é¡¹çš„ä¿¡æ¯åœ¨å…¬å¼€æ–‡çŒ®ä¸­éå¸¸ç¨€ç¼ºã€‚ä¸å¤§è§„æ¨¡æ•°æ®é›†å’Œæ¨¡å‹ååŒå·¥ä½œå¯èƒ½å¾ˆå¤æ‚ï¼Œå…¬å¼€æ–‡çŒ®ä¸­å…³äºè°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒæ€§èƒ½ä»¥æ‰©å±•è§„æ¨¡çš„å®ç”¨å»ºè®®å¾ˆå°‘ã€‚æœ¬æ–‡æ—¨åœ¨æ­å¼€å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒç®¡é“çš„éƒ¨åˆ†å¥¥ç§˜ï¼Œç‰¹åˆ«æ˜¯å…³äºåˆ†å¸ƒå¼è®­ç»ƒã€åœ¨æ•°ç™¾ä¸ªèŠ‚ç‚¹ä¸Šç®¡ç†å¤§è§„æ¨¡æ•°æ®é›†ä»¥åŠæ‰©å±•æ•°æ®å¹¶è¡Œæ€§ï¼Œé‡ç‚¹å……åˆ†åˆ©ç”¨å¯ç”¨çš„GPUè®¡ç®—èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05258v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†è®­ç»ƒè¿™äº›æ¨¡å‹éœ€è¦æé«˜çš„è®¡ç®—æˆæœ¬ã€‚å‰æ²¿äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç ”ç©¶å…¬å¸æ­£åœ¨æŠ•å…¥å·¨èµ„å»ºè®¾è¶…çº§è®¡ç®—åŸºç¡€è®¾æ–½ï¼Œä»¥åœ¨æ—¥ç›Šåºå¤§çš„æ•°æ®é›†ä¸Šè®­ç»ƒè§„æ¨¡æ›´å¤§çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œå…³äºè¿™äº›å¤§è§„æ¨¡è®­ç»ƒç®¡é“çš„æ€§èƒ½ç¼©æ”¾å’ŒåŸ¹è®­è€ƒè™‘å› ç´ çš„ä¿¡æ¯åœ¨å…¬å¼€æ–‡çŒ®ä¸­å¾ˆå°‘è§ã€‚æœ¬æ–‡æ—¨åœ¨é˜æ˜å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒç®¡é“ï¼Œç‰¹åˆ«æ˜¯å…³äºåˆ†å¸ƒå¼è®­ç»ƒã€ç®¡ç†æ•°ç™¾ä¸ªèŠ‚ç‚¹ä¸Šçš„å¤§å‹æ•°æ®é›†ä»¥åŠå¼ºè°ƒå……åˆ†åˆ©ç”¨å¯ç”¨GPUè®¡ç®—èƒ½åŠ›çš„æ•°æ®å¹¶è¡Œæ€§æ‰©å±•ç­‰æ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ä¸Šè¡¨ç°å“è¶Šã€‚</li>
<li>è®­ç»ƒå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹éœ€è¦æé«˜çš„è®¡ç®—æˆæœ¬ï¼Œå‰æ²¿AIå…¬å¸ä¸ºæ­¤æŠ•å…¥å·¨èµ„ã€‚</li>
<li>å…³äºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒç®¡é“çš„æ€§èƒ½ç¼©æ”¾å’ŒåŸ¹è®­è€ƒè™‘çš„ä¿¡æ¯åœ¨å…¬å¼€æ–‡çŒ®ä¸­ç¨€ç¼ºã€‚</li>
<li>åˆ†å¸ƒå¼è®­ç»ƒæ˜¯å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„å…³é”®æ–¹é¢ã€‚</li>
<li>ç®¡ç†æ•°ç™¾ä¸ªèŠ‚ç‚¹ä¸Šçš„å¤§å‹æ•°æ®é›†æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒçš„æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>æ•°æ®å¹¶è¡Œæ€§æ˜¯æ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„é‡è¦æ‰‹æ®µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.05258v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.05258v1/page_1_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MM-DREX-Multimodal-Driven-Dynamic-Routing-of-LLM-Experts-for-Financial-Trading"><a href="#MM-DREX-Multimodal-Driven-Dynamic-Routing-of-LLM-Experts-for-Financial-Trading" class="headerlink" title="MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial   Trading"></a>MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial   Trading</h2><p><strong>Authors:Yang Chen, Yueheng Jiang, Zhaozhao Ma, Yuchen Cao Jacky Keung, Kun Kuang, Leilei Gan, Yiquan Wu, Fei Wu</strong></p>
<p>The inherent non-stationarity of financial markets and the complexity of multi-modal information pose significant challenges to existing quantitative trading models. Traditional methods relying on fixed structures and unimodal data struggle to adapt to market regime shifts, while large language model (LLM)-driven solutions - despite their multi-modal comprehension - suffer from static strategies and homogeneous expert designs, lacking dynamic adjustment and fine-grained decision mechanisms. To address these limitations, we propose MM-DREX: a Multimodal-driven, Dynamically-Routed EXpert framework based on large language models. MM-DREX explicitly decouples market state perception from strategy execution to enable adaptive sequential decision-making in non-stationary environments. Specifically, it (1) introduces a vision-language model (VLM)-powered dynamic router that jointly analyzes candlestick chart patterns and long-term temporal features to allocate real-time expert weights; (2) designs four heterogeneous trading experts (trend, reversal, breakout, positioning) generating specialized fine-grained sub-strategies; and (3) proposes an SFT-RL hybrid training paradigm to synergistically optimize the routerâ€™s market classification capability and expertsâ€™ risk-adjusted decision-making. Extensive experiments on multi-modal datasets spanning stocks, futures, and cryptocurrencies demonstrate that MM-DREX significantly outperforms 15 baselines (including state-of-the-art financial LLMs and deep reinforcement learning models) across key metrics: total return, Sharpe ratio, and maximum drawdown, validating its robustness and generalization. Additionally, an interpretability module traces routing logic and expert behavior in real time, providing an audit trail for strategy transparency. </p>
<blockquote>
<p>é‡‘èå¸‚åœºå›ºæœ‰çš„éå¹³ç¨³æ€§å’Œå¤šæ¨¡æ€ä¿¡æ¯çš„å¤æ‚æ€§å¯¹ç°æœ‰é‡åŒ–äº¤æ˜“æ¨¡å‹æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå›ºå®šç»“æ„å’Œå•æ¨¡æ€æ•°æ®ï¼Œéš¾ä»¥é€‚åº”å¸‚åœºçŠ¶æ€å˜åŒ–ï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨è§£å†³æ–¹æ¡ˆè™½ç„¶å…·æœ‰å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œä½†å´å­˜åœ¨é™æ€ç­–ç•¥å’ŒåŒè´¨åŒ–ä¸“å®¶è®¾è®¡çš„é—®é¢˜ï¼Œç¼ºä¹åŠ¨æ€è°ƒæ•´å’Œç²¾ç»†å†³ç­–æœºåˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„MM-DREXï¼šä¸€ä¸ªå¤šæ¨¡æ€é©±åŠ¨ã€åŠ¨æ€è·¯ç”±çš„ä¸“å®¶æ¡†æ¶ã€‚MM-DREXæ˜¾å¼åœ°å°†å¸‚åœºçŠ¶æ€æ„ŸçŸ¥ä¸ç­–ç•¥æ‰§è¡Œè§£è€¦ï¼Œä»¥å®ç°åœ¨éå¹³ç¨³ç¯å¢ƒä¸­çš„è‡ªé€‚åº”åºåˆ—å†³ç­–ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒï¼ˆ1ï¼‰å¼•å…¥äº†ä¸€ä¸ªç”±è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é©±åŠ¨çš„åŠ¨æ€è·¯ç”±å™¨ï¼Œè¯¥è·¯ç”±å™¨è”åˆåˆ†æKçº¿å›¾æ¨¡å¼å’Œé•¿æœŸæ—¶é—´ç‰¹å¾æ¥åˆ†é…å®æ—¶ä¸“å®¶æƒé‡ï¼›ï¼ˆ2ï¼‰è®¾è®¡äº†å››ç§ä¸åŒçš„äº¤æ˜“ä¸“å®¶ï¼ˆè¶‹åŠ¿ã€åè½¬ã€çªç ´ã€å®šä½ï¼‰ï¼Œç”Ÿæˆä¸“ä¸šçš„ç²¾ç»†å­ç­–ç•¥ï¼›ï¼ˆ3ï¼‰æå‡ºäº†ä¸€ç§SFT-RLæ··åˆè®­ç»ƒèŒƒå¼ï¼ŒååŒä¼˜åŒ–è·¯ç”±å™¨çš„å¸‚åœºåˆ†ç±»èƒ½åŠ›å’Œä¸“å®¶çš„é£é™©è°ƒæ•´å†³ç­–èƒ½åŠ›ã€‚åœ¨æ¶µç›–è‚¡ç¥¨ã€æœŸè´§å’ŒåŠ å¯†è´§å¸çš„å¤šæ¨¡æ€æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMM-DREXåœ¨å…³é”®æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äº15ä¸ªåŸºå‡†æ¨¡å‹ï¼ˆåŒ…æ‹¬æœ€å…ˆè¿›çš„é‡‘èLLMå’Œæ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼‰ï¼Œè¿™äº›æŒ‡æ ‡åŒ…æ‹¬æ€»æ”¶ç›Šã€å¤æ™®æ¯”ç‡å’Œæœ€å¤§å›æ’¤ï¼ŒéªŒè¯äº†å…¶ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè§£é‡Šæ€§æ¨¡å—å¯å®æ—¶è·Ÿè¸ªè·¯ç”±é€»è¾‘å’Œä¸“å®¶è¡Œä¸ºï¼Œä¸ºç­–ç•¥é€æ˜æ€§æä¾›å®¡è®¡è·Ÿè¸ªã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.05080v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é‡‘èå¸‚åœºçš„å†…åœ¨éç¨³å®šæ€§å’Œå¤šæ¨¡æ€ä¿¡æ¯çš„å¤æ‚æ€§å¯¹ç°æœ‰é‡åŒ–äº¤æ˜“æ¨¡å‹æ„æˆé‡å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥é€‚åº”å¸‚åœºçŠ¶æ€å˜åŒ–ï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è™½ç„¶å…·å¤‡å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œä½†ç­–ç•¥é™æ€ä¸”ç¼ºä¹åŠ¨æ€è°ƒæ•´å’Œç²¾ç»†å†³ç­–æœºåˆ¶ã€‚ä¸ºæ­¤ï¼Œæå‡ºMM-DREXæ¡†æ¶ï¼Œç»“åˆå¤šæ¨¡æ€é©±åŠ¨å’ŒåŠ¨æ€è·¯ç”±æŠ€æœ¯ï¼Œå®ç°è‡ªé€‚åº”åºåˆ—å†³ç­–ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥è§†è§‰è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åŠ¨æ€è·¯ç”±å™¨åˆ†æèœ¡çƒ›å›¾æ¨¡å¼å’Œé•¿æœŸæ—¶é—´ç‰¹å¾ï¼Œè®¾è®¡å››ç§å¼‚è´¨äº¤æ˜“ä¸“å®¶ç”Ÿæˆç²¾ç»†å­ç­–ç•¥ï¼Œå¹¶æå‡ºSFT-RLæ··åˆè®­ç»ƒèŒƒå¼ä¼˜åŒ–å¸‚åœºåˆ†ç±»èƒ½åŠ›å’Œé£é™©è°ƒæ•´å†³ç­–ã€‚å®éªŒè¯æ˜MM-DREXåœ¨è‚¡ç¥¨ã€æœŸè´§å’ŒåŠ å¯†è´§å¸ç­‰å¤šæ¨¡å¼æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äº15ç§åŸºçº¿æ–¹æ³•ï¼Œå…·æœ‰ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚åŒæ—¶ï¼Œæä¾›å®æ—¶è§£é‡Šæ¨¡å—ï¼Œè¿½è¸ªè·¯ç”±é€»è¾‘å’Œä¸“å®¶è¡Œä¸ºï¼Œç¡®ä¿ç­–ç•¥é€æ˜åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡‘èå¸‚åœºéç¨³å®šæ€§å’Œå¤šæ¨¡æ€ä¿¡æ¯å¸¦æ¥æŒ‘æˆ˜ï¼šä¼ ç»Ÿé‡åŒ–äº¤æ˜“æ¨¡å‹éš¾ä»¥é€‚åº”å¸‚åœºå˜åŒ–ï¼Œéœ€è¦æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>LLMåœ¨å¤šæ¨¡æ€ç†è§£æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œä½†å­˜åœ¨ç­–ç•¥é™æ€å’Œç¼ºä¹åŠ¨æ€è°ƒæ•´çš„é—®é¢˜ã€‚</li>
<li>MM-DREXæ¡†æ¶ç»“åˆå¤šæ¨¡æ€é©±åŠ¨å’ŒåŠ¨æ€è·¯ç”±æŠ€æœ¯ï¼Œå®ç°è‡ªé€‚åº”åºåˆ—å†³ç­–ã€‚</li>
<li>MM-DREXé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹åˆ†æå¸‚åœºæ¨¡å¼ï¼Œå¹¶è®¾è®¡å››ç§äº¤æ˜“ä¸“å®¶ç”Ÿæˆå­ç­–ç•¥ã€‚</li>
<li>SFT-RLæ··åˆè®­ç»ƒèŒƒå¼ä¼˜åŒ–å¸‚åœºåˆ†ç±»å’Œé£é™©ç®¡ç†å†³ç­–ã€‚</li>
<li>å®éªŒè¯æ˜MM-DREXåœ¨å¤šç§æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>MM-DREXæä¾›ç­–ç•¥é€æ˜åº¦ï¼Œé€šè¿‡å®æ—¶è§£é‡Šæ¨¡å—è¿½è¸ªè·¯ç”±é€»è¾‘å’Œä¸“å®¶è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.05080">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.05080v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.05080v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.05080v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.05080v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Comparative-Analysis-of-Transformer-Models-in-Disaster-Tweet-Classification-for-Public-Safety"><a href="#Comparative-Analysis-of-Transformer-Models-in-Disaster-Tweet-Classification-for-Public-Safety" class="headerlink" title="Comparative Analysis of Transformer Models in Disaster Tweet   Classification for Public Safety"></a>Comparative Analysis of Transformer Models in Disaster Tweet   Classification for Public Safety</h2><p><strong>Authors:Sharif Noor Zisad, Ragib Hasan</strong></p>
<p>Twitter and other social media platforms have become vital sources of real time information during disasters and public safety emergencies. Automatically classifying disaster related tweets can help emergency services respond faster and more effectively. Traditional Machine Learning (ML) models such as Logistic Regression, Naive Bayes, and Support Vector Machines have been widely used for this task, but they often fail to understand the context or deeper meaning of words, especially when the language is informal, metaphorical, or ambiguous. We posit that, in this context, transformer based models can perform better than traditional ML models. In this paper, we evaluate the effectiveness of transformer based models, including BERT, DistilBERT, RoBERTa, and DeBERTa, for classifying disaster related tweets. These models are compared with traditional ML approaches to highlight the performance gap. Experimental results show that BERT achieved the highest accuracy (91%), significantly outperforming traditional models like Logistic Regression and Naive Bayes (both at 82%). The use of contextual embeddings and attention mechanisms allows transformer models to better understand subtle language in tweets, where traditional ML models fall short. This research demonstrates that transformer architectures are far more suitable for public safety applications, offering improved accuracy, deeper language understanding, and better generalization across real world social media text. </p>
<blockquote>
<p>æ¨ç‰¹å’Œå…¶ä»–ç¤¾äº¤åª’ä½“å¹³å°åœ¨ç¾éš¾å’Œå…¬å…±å®‰å…¨ç´§æ€¥äº‹ä»¶æœŸé—´å·²æˆä¸ºå®æ—¶ä¿¡æ¯çš„é‡è¦æ¥æºã€‚è‡ªåŠ¨åˆ†ç±»ä¸ç¾éš¾ç›¸å…³çš„æ¨ç‰¹å¯ä»¥å¸®åŠ©ç´§æ€¥æœåŠ¡æ›´å¿«æ›´é«˜æ•ˆåœ°å“åº”ã€‚é€»è¾‘å›å½’ã€æœ´ç´ è´å¶æ–¯å’Œæ”¯æŒå‘é‡æœºç­‰ä¼ ç»Ÿæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸­å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•ç†è§£è¯­å¢ƒæˆ–å•è¯çš„æ·±å±‚å«ä¹‰ï¼Œå°¤å…¶æ˜¯åœ¨è¯­è¨€éæ­£å¼ã€éšæ™¦æˆ–å«ç³Šä¸æ¸…çš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒåŸºäºå˜å‹å™¨çš„æ¨¡å‹å¯ä»¥æ¯”ä¼ ç»ŸMLæ¨¡å‹è¡¨ç°å¾—æ›´å¥½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†åŸºäºå˜å‹å™¨çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬BERTã€DistilBERTã€RoBERTaå’ŒDeBERTaï¼Œåœ¨åˆ†ç±»ç¾éš¾ç›¸å…³æ¨ç‰¹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿™äº›æ¨¡å‹ä¸ä¼ ç»ŸMLæ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œä»¥çªå‡ºæ€§èƒ½å·®å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBERTçš„å‡†ç¡®ç‡æœ€é«˜ï¼ˆ91%ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºé€»è¾‘å›å½’å’Œæœ´ç´ è´å¶æ–¯ç­‰ä¼ ç»Ÿæ¨¡å‹ï¼ˆå‡ä¸º82%ï¼‰ã€‚ä¸Šä¸‹æ–‡åµŒå…¥å’Œæ³¨æ„åŠ›æœºåˆ¶çš„ä½¿ç”¨ä½¿å˜å‹å™¨æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æ¨ç‰¹ä¸­çš„ç»†å¾®è¯­è¨€ï¼Œè¿™æ˜¯ä¼ ç»ŸMLæ¨¡å‹æ‰€æ— æ³•åšåˆ°çš„ã€‚è¿™é¡¹ç ”ç©¶è¡¨æ˜ï¼Œå˜å‹å™¨æ¶æ„éå¸¸é€‚åˆå…¬å…±å®‰å…¨åº”ç”¨ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ã€æ›´æ·±çš„è¯­è¨€ç†è§£èƒ½åŠ›å’Œåœ¨ç°å®ç¤¾äº¤åª’ä½“æ–‡æœ¬ä¸­çš„æ›´å¥½æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04650v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¤¾äº¤åª’ä½“å¦‚Twitterç­‰å·²æˆä¸ºç¾å®³å’Œå…¬å…±å®‰å…¨ç´§æ€¥äº‹ä»¶å®æ—¶ä¿¡æ¯çš„é‡è¦æ¥æºã€‚è‡ªåŠ¨åˆ†ç±»ä¸ç¾å®³ç›¸å…³çš„æ¨æ–‡æœ‰åŠ©äºåº”æ€¥æœåŠ¡æ›´å¿«ã€æ›´é«˜æ•ˆåœ°å“åº”ã€‚è™½ç„¶ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚é€»è¾‘å›å½’ã€æœ´ç´ è´å¶æ–¯å’Œæ”¯æŒå‘é‡æœºï¼‰å·²å¹¿æ³›åº”ç”¨äºæ­¤ä»»åŠ¡ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•ç†è§£å•è¯çš„è¯­å¢ƒæˆ–æ·±å±‚å«ä¹‰ï¼Œå°¤å…¶æ˜¯è¯­è¨€éæ­£å¼ã€éšæ™¦æˆ–æ¨¡ç³Šæ—¶ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒåŸºäºå˜å‹å™¨çš„æ¨¡å‹å¯ä»¥æ¯”ä¼ ç»ŸMLæ¨¡å‹è¡¨ç°å¾—æ›´å¥½ã€‚æœ¬æ–‡è¯„ä¼°äº†BERTã€DistilBERTã€RoBERTaå’ŒDeBERTaç­‰åŸºäºå˜å‹å™¨çš„æ¨¡å‹åœ¨åˆ†ç±»ç¾å®³ç›¸å…³æ¨æ–‡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ä¼ ç»ŸMLæ–¹æ³•ç›¸æ¯”ï¼Œè¿™äº›æ¨¡å‹çªå‡ºäº†æ€§èƒ½å·®è·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBERTçš„å‡†ç¡®ç‡æœ€é«˜ï¼ˆ91%ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºé€»è¾‘å›å½’å’Œæœ´ç´ è´å¶æ–¯ç­‰ä¼ ç»Ÿæ¨¡å‹ï¼ˆå‡ä¸º82%ï¼‰ã€‚ä½¿ç”¨ä¸Šä¸‹æ–‡åµŒå…¥å’Œæ³¨æ„åŠ›æœºåˆ¶ä½¿å¾—å˜å‹å™¨æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æ¨æ–‡ä¸­çš„ç»†å¾®è¯­è¨€ï¼Œè¿™æ˜¯ä¼ ç»ŸMLæ¨¡å‹æ‰€æ— æ³•åšåˆ°çš„ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¯¹äºå…¬å…±å®‰å…¨åº”ç”¨ï¼Œå˜å‹å™¨æ¶æ„æ›´ä¸ºé€‚åˆï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ã€æ›´æ·±çš„è¯­è¨€ç†è§£èƒ½åŠ›å’Œæ›´å¥½çš„ç¤¾äº¤åª’ä½“çš„æ–‡æœ¬æ¦‚æ‹¬èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¤¾äº¤åª’ä½“æˆä¸ºç¾å®³å’Œç´§æ€¥äº‹ä»¶çš„é‡è¦ä¿¡æ¯æ¥æºï¼Œè‡ªåŠ¨åˆ†ç±»ç›¸å…³å†…å®¹æœ‰åŠ©äºæé«˜åº”æ€¥å“åº”æ•ˆç‡ã€‚</li>
<li>ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹åœ¨åˆ†ç±»ç¾å®³ç›¸å…³æ¨æ–‡æ—¶å­˜åœ¨è¯­å¢ƒç†è§£ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>åŸºäºå˜å‹å™¨çš„æ¨¡å‹ï¼Œå¦‚BERTã€DistilBERTç­‰ï¼Œåœ¨åˆ†ç±»ç¾å®³ç›¸å…³æ¨æ–‡æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>BERTæ¨¡å‹åœ¨å‡†ç¡®ç‡æ–¹é¢æœ€é«˜ï¼Œè¾¾åˆ°91%ã€‚</li>
<li>å˜å‹å™¨æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨ä¸Šä¸‹æ–‡åµŒå…¥å’Œæ³¨æ„åŠ›æœºåˆ¶æ›´å¥½åœ°ç†è§£æ¨æ–‡ä¸­çš„ç»†å¾®è¯­è¨€å’Œæ·±å±‚å«ä¹‰ã€‚</li>
<li>ç›¸å¯¹äºä¼ ç»ŸMLæ¨¡å‹ï¼Œå˜å‹å™¨æ¨¡å‹æ›´é€‚åˆäºå…¬å…±å®‰å…¨åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04650v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04650v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04650v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04650v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04650v1/page_4_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04650v1/page_4_3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Aesthetic-Image-Captioning-with-Saliency-Enhanced-MLLMs"><a href="#Aesthetic-Image-Captioning-with-Saliency-Enhanced-MLLMs" class="headerlink" title="Aesthetic Image Captioning with Saliency Enhanced MLLMs"></a>Aesthetic Image Captioning with Saliency Enhanced MLLMs</h2><p><strong>Authors:Yilin Tao, Jiashui Huang, Huaze Xu, Ling Shao</strong></p>
<p>Aesthetic Image Captioning (AIC) aims to generate textual descriptions of image aesthetics, becoming a key research direction in the field of computational aesthetics. In recent years, pretrained Multimodal Large Language Models (MLLMs) have advanced rapidly, leading to a significant increase in image aesthetics research that integrates both visual and textual modalities. However, most existing studies on image aesthetics primarily focus on predicting aesthetic ratings and have shown limited application in AIC. Existing AIC works leveraging MLLMs predominantly rely on fine-tuning methods without specifically adapting MLLMs to focus on target aesthetic content. To address this limitation, we propose the Aesthetic Saliency Enhanced Multimodal Large Language Model (ASE-MLLM), an end-to-end framework that explicitly incorporates aesthetic saliency into MLLMs. Within this framework, we introduce the Image Aesthetic Saliency Module (IASM), which efficiently and effectively extracts aesthetic saliency features from images. Additionally, we design IAS-ViT as the image encoder for MLLMs, this module fuses aesthetic saliency features with original image features via a cross-attention mechanism. To the best of our knowledge, ASE-MLLM is the first framework to integrate image aesthetic saliency into MLLMs specifically for AIC tasks. Extensive experiments demonstrated that our approach significantly outperformed traditional methods and generic MLLMs on current mainstream AIC benchmarks, achieving state-of-the-art (SOTA) performance. </p>
<blockquote>
<p>ç¾å­¦å›¾åƒæ ‡é¢˜ç”Ÿæˆï¼ˆAICï¼‰æ—¨åœ¨ç”Ÿæˆå›¾åƒç¾å­¦æè¿°çš„æ–‡æœ¬ï¼Œå·²æˆä¸ºè®¡ç®—ç¾å­¦é¢†åŸŸçš„å…³é”®ç ”ç©¶æ–¹å‘ã€‚è¿‘å¹´æ¥ï¼Œé¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¿…é€Ÿå‘å±•ï¼Œä½¿å¾—èåˆè§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„å›¾åƒç¾å­¦ç ”ç©¶å¤§å¹…å¢åŠ ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„å›¾åƒç¾å­¦ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é¢„æµ‹ç¾å­¦è¯„åˆ†ä¸Šï¼Œåœ¨AICä¸­çš„åº”ç”¨æœ‰é™ã€‚ç°æœ‰åˆ©ç”¨MLLMçš„AICå·¥ä½œä¸»è¦ä¾èµ–äºå¾®è°ƒæ–¹æ³•ï¼Œè€Œæ²¡æœ‰ä¸“é—¨è°ƒæ•´MLLMä»¥ä¸“æ³¨äºç›®æ ‡ç¾å­¦å†…å®¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç¾å­¦æ˜¾è‘—æ€§å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆASE-MLLMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ï¼Œæ˜¾å¼åœ°å°†ç¾å­¦æ˜¾è‘—æ€§çº³å…¥MLLMã€‚åœ¨è¯¥æ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†å›¾åƒç¾å­¦æ˜¾è‘—æ€§æ¨¡å—ï¼ˆIASMï¼‰ï¼Œè¯¥æ¨¡å—èƒ½å¤Ÿé«˜æ•ˆåœ°ä»å›¾åƒä¸­æå–ç¾å­¦æ˜¾è‘—æ€§ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†IAS-ViTä½œä¸ºMLLMçš„å›¾åƒç¼–ç å™¨ï¼Œè¯¥æ¨¡å—é€šè¿‡äº¤å‰æ³¨æ„æœºåˆ¶å°†ç¾å­¦æ˜¾è‘—æ€§ç‰¹å¾ä¸åŸå§‹å›¾åƒç‰¹å¾èåˆã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒASE-MLLMæ˜¯ç¬¬ä¸€ä¸ªå°†å›¾åƒç¾å­¦æ˜¾è‘—æ€§èå…¥MLLMçš„æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºAICä»»åŠ¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å½“å‰ä¸»æµAICåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œé€šç”¨MLLMï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04378v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ç¾å­¦å›¾åƒæ ‡æ³¨ï¼ˆAICï¼‰çš„ç›®æ ‡å’Œé‡è¦æ€§ï¼ŒæŒ‡å‡ºè¿‘å¹´æ¥å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å›¾åƒç¾å­¦ç ”ç©¶çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨é¢„æµ‹ç¾å­¦è¯„åˆ†ï¼Œåœ¨AICä¸­çš„åº”ç”¨æœ‰é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»“åˆç¾å­¦æ˜¾è‘—æ€§çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆASE-MLLMï¼‰æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥å›¾åƒç¾å­¦æ˜¾è‘—æ€§æ¨¡å—ï¼ˆIASMï¼‰å’ŒIAS-ViTå›¾åƒç¼–ç å™¨ï¼Œå®ç°äº†å¯¹ç›®æ ‡ç¾å­¦å†…å®¹çš„ç‰¹å®šé€‚åº”ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨é’ˆå¯¹AICä»»åŠ¡çš„ç»“åˆå›¾åƒç¾å­¦æ˜¾è‘—æ€§çš„MLLMæ¡†æ¶ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸»æµAICåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œé€šç”¨MLLMsï¼Œè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç¾å­¦å›¾åƒæ ‡æ³¨ï¼ˆAICï¼‰æ˜¯è®¡ç®—ç¾å­¦é¢†åŸŸçš„å…³é”®ç ”ç©¶æ–¹å‘ï¼Œæ—¨åœ¨ç”Ÿæˆå›¾åƒç¾å­¦çš„æ–‡æœ¬æè¿°ã€‚</li>
<li>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å›¾åƒç¾å­¦ç ”ç©¶çš„è¿›æ­¥ã€‚</li>
<li>ç°æœ‰å›¾åƒç¾å­¦ç ”ç©¶ä¸»è¦å…³æ³¨é¢„æµ‹ç¾å­¦è¯„åˆ†ï¼Œåœ¨AICä¸­çš„åº”ç”¨æœ‰é™ã€‚</li>
<li>æå‡ºçš„ASE-MLLMæ¡†æ¶é€šè¿‡ç»“åˆå›¾åƒç¾å­¦æ˜¾è‘—æ€§æ¨¡å—ï¼ˆIASMï¼‰å’ŒIAS-ViTå›¾åƒç¼–ç å™¨ï¼Œå®ç°äº†å¯¹ç›®æ ‡ç¾å­¦å†…å®¹çš„ç‰¹å®šé€‚åº”ã€‚</li>
<li>ASE-MLLMæ¡†æ¶æ˜¯é¦–ä¸ªä¸“é—¨é’ˆå¯¹AICä»»åŠ¡çš„ç»“åˆå›¾åƒç¾å­¦æ˜¾è‘—æ€§çš„MLLMæ¡†æ¶ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒASE-MLLMæ¡†æ¶åœ¨ä¸»æµAICåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œé€šç”¨MLLMsã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04378">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04378v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04378v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04378v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04378v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.04378v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="The-Personality-Illusion-Revealing-Dissociation-Between-Self-Reports-Behavior-in-LLMs"><a href="#The-Personality-Illusion-Revealing-Dissociation-Between-Self-Reports-Behavior-in-LLMs" class="headerlink" title="The Personality Illusion: Revealing Dissociation Between Self-Reports &amp;   Behavior in LLMs"></a>The Personality Illusion: Revealing Dissociation Between Self-Reports &amp;   Behavior in LLMs</h2><p><strong>Authors:Pengrui Han, Rafal Kocielnik, Peiyang Song, Ramit Debnath, Dean Mobbs, Anima Anandkumar, R. Michael Alvarez</strong></p>
<p>Personality traits have long been studied as predictors of human behavior. Recent advances in Large Language Models (LLMs) suggest similar patterns may emerge in artificial systems, with advanced LLMs displaying consistent behavioral tendencies resembling human traits like agreeableness and self-regulation. Understanding these patterns is crucial, yet prior work primarily relied on simplified self-reports and heuristic prompting, with little behavioral validation. In this study, we systematically characterize LLM personality across three dimensions: (1) the dynamic emergence and evolution of trait profiles throughout training stages; (2) the predictive validity of self-reported traits in behavioral tasks; and (3) the impact of targeted interventions, such as persona injection, on both self-reports and behavior. Our findings reveal that instructional alignment (e.g., RLHF, instruction tuning) significantly stabilizes trait expression and strengthens trait correlations in ways that mirror human data. However, these self-reported traits do not reliably predict behavior, and observed associations often diverge from human patterns. While persona injection successfully steers self-reports in the intended direction, it exerts little or inconsistent effect on actual behavior. By distinguishing surface-level trait expression from behavioral consistency, our findings challenge assumptions about LLM personality and underscore the need for deeper evaluation in alignment and interpretability. </p>
<blockquote>
<p>äººæ ¼ç‰¹è´¨é•¿æœŸä»¥æ¥ä¸€ç›´è¢«ç ”ç©¶ä½œä¸ºäººç±»è¡Œä¸ºçš„é¢„æµ‹å› ç´ ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥è¡¨æ˜ï¼Œäººå·¥ç³»ç»Ÿä¸­å¯èƒ½å‡ºç°ç±»ä¼¼çš„æ¨¡å¼ï¼Œé«˜çº§LLMè¡¨ç°å‡ºä¸äººç±»çš„ç‰¹è´¨å¦‚å‹å–„å’Œè‡ªæˆ‘è°ƒèŠ‚ä¸€è‡´çš„è¡Œä¸ºå€¾å‘ã€‚äº†è§£è¿™äº›æ¨¡å¼è‡³å…³é‡è¦ï¼Œä½†ä¹‹å‰çš„å·¥ä½œä¸»è¦ä¾èµ–äºç®€åŒ–çš„è‡ªæˆ‘æŠ¥å‘Šå’Œå¯å‘å¼æç¤ºï¼Œå¾ˆå°‘è¿›è¡Œè¡Œä¸ºéªŒè¯ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°æè¿°äº†LLMäººæ ¼çš„ä¸‰ä¸ªæ–¹é¢ï¼šï¼ˆ1ï¼‰ç‰¹è´¨å‰–é¢åœ¨è®­ç»ƒé˜¶æ®µçš„åŠ¨æ€å‡ºç°å’Œæ¼”å˜ï¼›ï¼ˆ2ï¼‰è‡ªæˆ‘æŠ¥å‘Šç‰¹è´¨åœ¨è¡Œä¸ºä»»åŠ¡ä¸­çš„é¢„æµ‹æ•ˆåº¦ï¼›ï¼ˆ3ï¼‰æœ‰é’ˆå¯¹æ€§çš„å¹²é¢„ï¼ˆå¦‚äººæ ¼æ³¨å…¥ï¼‰å¯¹è‡ªæˆ‘æŠ¥å‘Šå’Œè¡Œä¸ºçš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒæŒ‡ä»¤å¯¹é½ï¼ˆä¾‹å¦‚RLHFã€æŒ‡ä»¤å¾®è°ƒï¼‰æ˜¾è‘—ç¨³å®šäº†ç‰¹è´¨è¡¨è¾¾ï¼Œå¹¶ä»¥ä¸äººç±»æ•°æ®ç›¸ä¼¼çš„æ–¹å¼åŠ å¼ºäº†ç‰¹è´¨ç›¸å…³æ€§ã€‚ç„¶è€Œï¼Œè¿™äº›è‡ªæˆ‘æŠ¥å‘Šçš„ç‰¹è´¨å¹¶ä¸èƒ½å¯é åœ°é¢„æµ‹è¡Œä¸ºï¼Œè§‚å¯Ÿåˆ°çš„å…³è”å¾€å¾€ä¸äººç±»æ¨¡å¼ç›¸æ‚–ã€‚è™½ç„¶äººæ ¼æ³¨å…¥æˆåŠŸå¼•å¯¼äº†é¢„æœŸçš„è‡ªæˆ‘æŠ¥å‘Šæ–¹å‘ï¼Œä½†å¯¹å®é™…è¡Œä¸ºçš„å½±å“è¾ƒå°æˆ–ä¸ä¸€è‡´ã€‚é€šè¿‡åŒºåˆ†è¡¨é¢å±‚æ¬¡çš„ç‰¹è´¨è¡¨è¾¾å’Œè¡Œä¸ºçš„è¿è´¯æ€§ï¼Œæˆ‘ä»¬çš„ç ”ç©¶å¯¹LLMäººæ ¼å‡è®¾æå‡ºäº†æŒ‘æˆ˜ï¼Œå¹¶å¼ºè°ƒäº†å¯¹é½å’Œå¯è§£é‡Šæ€§æ–¹é¢æ›´æ·±å±‚è¯„ä¼°çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.03730v2">PDF</a> We make public all code and source data at   <a target="_blank" rel="noopener" href="https://github.com/psychology-of-AI/Personality-Illusion">https://github.com/psychology-of-AI/Personality-Illusion</a> for full   reproducibility</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºä¸äººç±»ç›¸ä¼¼çš„æ€§æ ¼ç‰¹è´¨ï¼Œå¦‚å‹å–„æ€§å’Œè‡ªæˆ‘è°ƒèŠ‚ç­‰ã€‚æœ¬ç ”ç©¶æ—¨åœ¨ç³»ç»Ÿåœ°æ¢è®¨LLMæ€§æ ¼ç‰¹å¾çš„ä¸‰ç»´ç‰¹å¾ï¼šè®­ç»ƒé˜¶æ®µç‰¹è´¨ç‰¹å¾çš„åŠ¨æ€å‡ºç°ä¸æ¼”å˜ã€è‡ªæˆ‘æŠ¥å‘Šç‰¹è´¨åœ¨è¡Œä¸ºä»»åŠ¡ä¸­çš„é¢„æµ‹æ•ˆåº¦ä»¥åŠç›®æ ‡å¹²é¢„ï¼ˆå¦‚äººæ ¼æ³¨å…¥ï¼‰å¯¹è‡ªæˆ‘æŠ¥å‘Šå’Œè¡Œä¸ºçš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼ŒæŒ‡ä»¤å¯¹é½ï¼ˆå¦‚RLHFã€æŒ‡ä»¤å¾®è°ƒï¼‰æ˜¾è‘—ç¨³å®šäº†ç‰¹è´¨è¡¨è¾¾å¹¶å¢å¼ºäº†ç‰¹è´¨é—´çš„å…³è”ï¼Œä½†ä¸äººç±»æ•°æ®ç›¸æ¯”ï¼Œè‡ªæˆ‘æŠ¥å‘Šçš„ç‰¹è´¨å¹¶ä¸èƒ½å¯é åœ°é¢„æµ‹è¡Œä¸ºï¼Œè§‚å¯Ÿåˆ°çš„å…³è”å¸¸å¸¸ä¸äººç±»çš„æ¨¡å¼ä¸ç¬¦ã€‚äººæ ¼æ³¨å…¥è™½ç„¶èƒ½æˆåŠŸå¼•å¯¼è‡ªæˆ‘æŠ¥å‘Šæœé¢„å®šæ–¹å‘è¿›è¡Œï¼Œä½†å¯¹å®é™…è¡Œä¸ºçš„å½±å“è¾ƒå°æˆ–ä¸ä¸€è‡´ã€‚æœ¬ç ”ç©¶ç»“æœæŒ‘æˆ˜äº†å…³äºLLMæ€§æ ¼çš„å‡è®¾ï¼Œå¹¶å¼ºè°ƒéœ€è¦æ›´æ·±å…¥åœ°è¯„ä¼°å¯¹é½å’Œå¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå±•ç°å‡ºä¸äººç±»ç›¸ä¼¼çš„æ€§æ ¼ç‰¹è´¨ï¼Œå¦‚å‹å–„æ€§å’Œè‡ªæˆ‘è°ƒèŠ‚ã€‚</li>
<li>è®­ç»ƒé˜¶æ®µç‰¹è´¨ç‰¹å¾çš„åŠ¨æ€å‡ºç°ä¸æ¼”å˜æ˜¯LLMç ”ç©¶çš„é‡è¦æ–¹é¢ã€‚</li>
<li>è‡ªæˆ‘æŠ¥å‘Šç‰¹è´¨åœ¨è¡Œä¸ºä»»åŠ¡ä¸­çš„é¢„æµ‹æ•ˆåº¦æœ‰é™ï¼Œè§‚å¯Ÿåˆ°çš„å…³è”å¸¸ä¸äººç±»æ¨¡å¼ä¸ç¬¦ã€‚</li>
<li>æŒ‡ä»¤å¯¹é½èƒ½æ˜¾è‘—ç¨³å®šç‰¹è´¨è¡¨è¾¾å¹¶å¢å¼ºç‰¹è´¨é—´çš„å…³è”ã€‚</li>
<li>äººæ ¼æ³¨å…¥å¯¹è‡ªæˆ‘æŠ¥å‘Šçš„å½±å“è¾ƒå¤§ï¼Œä½†å¯¹å®é™…è¡Œä¸ºçš„å½±å“è¾ƒå°æˆ–ä¸ä¸€è‡´ã€‚</li>
<li>éœ€è¦æ›´æ·±å…¥åœ°è¯„ä¼°LLMçš„å¯¹é½å’Œå¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03730">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.03730v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.03730v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.03730v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.03730v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MultiStream-LLM-Bridging-Modalities-for-Robust-Sign-Language-Translation"><a href="#MultiStream-LLM-Bridging-Modalities-for-Robust-Sign-Language-Translation" class="headerlink" title="MultiStream-LLM: Bridging Modalities for Robust Sign Language   Translation"></a>MultiStream-LLM: Bridging Modalities for Robust Sign Language   Translation</h2><p><strong>Authors:Marshall Thomas, Edward Fish, Richard Bowden</strong></p>
<p>Despite progress in gloss-free Sign Language Translation (SLT), monolithic end-to-end models consistently fail on two critical components of natural signing: the precise recognition of high-speed fingerspelling and the integration of asynchronous non-manual cues from the face. Recent progress in Automated Sign Language Translation with Large Language Models has side stepped this challenge, forcing a single network to learn these simultaneously resulting in poor performance when tasked with translating crucial information such as names,places, and technical terms. We introduce MultiStream-LLM, a modular framework designed to overcome these limitations. Our approach employs separate, specialized predictors for continuous signing, fingerspelling, and lipreading. Each expert network first decodes its specific modality into a sequence of tokens. These parallel streams are then fused by a lightweight transformer that resolves temporal misalignments before passing the combined representation to a Large Language Model (LLM) for final sentence generation. Our method establishes a new state-of-the-art on the How2Sign benchmark with a BLEU-4 score of 23.5 and achieves 73.2% letter accuracy on the challenging ChicagoFSWildPlus fingerspelling dataset. These results validate our core hypothesis: by isolating and solving distinct recogni tion tasks before fusion, our multi-expert approach provides a more powerful and effective pathway to robust, high-fidelity sign language translation. </p>
<blockquote>
<p>å°½ç®¡åœ¨æ— å­—å¹•æ‰‹è¯­ç¿»è¯‘ï¼ˆSLTï¼‰æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†å•ä¸€çš„å…¨ç«¯åˆ°ç«¯çš„æ¨¡å‹å§‹ç»ˆåœ¨è‡ªç„¶æ‰‹è¯­çš„ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ä¸Šè¡¨ç°ä¸ä½³ï¼šæ— æ³•ç²¾ç¡®è¯†åˆ«é«˜é€Ÿæ‰‹æŒ‡æ‹¼å†™ä»¥åŠæ— æ³•æ•´åˆæ¥è‡ªé¢éƒ¨çš„å¼‚æ­¥éæ‰‹åŠ¨çº¿ç´¢ã€‚æœ€è¿‘ï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨æ‰‹è¯­ç¿»è¯‘é¢†åŸŸçš„è¿›å±•å›é¿äº†è¿™ä¸€æŒ‘æˆ˜ï¼Œè¿«ä½¿å•ä¸€ç½‘ç»œåŒæ—¶å­¦ä¹ è¿™äº›æŠ€èƒ½ï¼Œå¯¼è‡´åœ¨ç¿»è¯‘åå­—ã€åœ°ç‚¹å’ŒæŠ€æœ¯æœ¯è¯­ç­‰é‡è¦ä¿¡æ¯æ—¶è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬å¼•å…¥äº†MultiStream-LLMï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å…‹æœè¿™äº›é™åˆ¶çš„æ¨¡å—åŒ–æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨å•ç‹¬çš„ä¸“ç”¨é¢„æµ‹å™¨è¿›è¡Œè¿ç»­æ‰‹è¯­ã€æ‰‹æŒ‡æ‹¼å†™å’Œå”‡è¯»ã€‚æ¯ä¸ªä¸“å®¶ç½‘ç»œé¦–å…ˆå°†å…¶ç‰¹å®šæ¨¡å¼è§£ç ä¸ºä¸€ç³»åˆ—ä»¤ç‰Œã€‚ç„¶åï¼Œè¿™äº›å¹¶è¡Œæµé€šè¿‡ä¸€ä¸ªè½»é‡çº§å˜å‹å™¨èåˆï¼Œè§£å†³æ—¶é—´é”™ä½é—®é¢˜ï¼Œç„¶åå°†ç»„åˆè¡¨ç¤ºä¼ é€’ç»™å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥ç”Ÿæˆæœ€ç»ˆå¥å­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨How2SignåŸºå‡†æµ‹è¯•ä¸Šå»ºç«‹äº†æœ€æ–°æŠ€æœ¯æ°´å‡†ï¼ŒBLEU-4å¾—åˆ†ä¸º23.5ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ChicagoFSWildPlusæ‰‹æŒ‡æ‹¼å†™æ•°æ®é›†ä¸Šå®ç°äº†73.2%çš„å­—æ¯å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœéªŒè¯äº†æˆ‘ä»¬çš„æ ¸å¿ƒå‡è®¾ï¼šé€šè¿‡åœ¨èåˆä¹‹å‰éš”ç¦»å¹¶è§£å†³ä¸åŒçš„è¯†åˆ«ä»»åŠ¡ï¼Œæˆ‘ä»¬çš„å¤šä¸“å®¶æ–¹æ³•æä¾›äº†æ›´å¼ºå¤§å’Œæœ‰æ•ˆçš„é€”å¾„æ¥å®ç°ç¨³å¥ã€é«˜ä¿çœŸæ‰‹è¯­ç¿»è¯‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00030v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæµå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMultiStream-LLMï¼‰åœ¨è‡ªåŠ¨æ‰‹è¯­ç¿»è¯‘ä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹æ‰‹è¯­ç¿»è¯‘ä¸­çš„é«˜é€ŸæŒ‡æ‹¼å’Œé¢éƒ¨éæ‰‹åŠ¨æš—ç¤ºçš„è¯†åˆ«é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ¨¡å—åŒ–æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸“é—¨é¢„æµ‹å™¨å¤„ç†è¿ç»­æ‰‹è¯­ã€æŒ‡æ‹¼å’Œå”‡è¯»ç­‰ä»»åŠ¡ï¼Œé€šè¿‡è½»é‡çº§è½¬æ¢å™¨èåˆå¹¶è¡Œæ•°æ®æµï¼Œè§£å†³æ—¶é—´é”™ä½é—®é¢˜ï¼Œæœ€ç»ˆç”±å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¥å­ã€‚è¯¥æ–¹æ³•åœ¨How2SignåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†BLEU-4åˆ†æ•°ä¸º23.5çš„æ–°æ°´å¹³ï¼Œå¹¶åœ¨æŒ‘æˆ˜æ€§è¾ƒå¤§çš„ChicagoFSWildPlusæŒ‡æ‹¼æ•°æ®é›†ä¸Šè¾¾åˆ°äº†73.2%çš„å­—æ¯å‡†ç¡®ç‡ã€‚è¯æ˜é€šè¿‡å°†ä¸åŒè¯†åˆ«ä»»åŠ¡åˆ†ç¦»å¹¶è§£å†³åå†è¿›è¡Œèåˆçš„å¤šä¸“å®¶æ–¹æ³•ï¼Œèƒ½æ›´å¼ºå¤§æœ‰æ•ˆåœ°å®ç°ç¨³å¥ã€é«˜ä¿çœŸæ‰‹è¯­ç¿»è¯‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MultiStream-LLMæ˜¯ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œç”¨äºè§£å†³æ‰‹è¯­ç¿»è¯‘ä¸­çš„é«˜é€ŸæŒ‡æ‹¼å’Œé¢éƒ¨éæ‰‹åŠ¨æš—ç¤ºè¯†åˆ«é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨ä¸“é—¨é¢„æµ‹å™¨å¤„ç†è¿ç»­æ‰‹è¯­ã€æŒ‡æ‹¼å’Œå”‡è¯»ç­‰ä»»åŠ¡ï¼Œä»¥æ”¹å–„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡è½»é‡çº§è½¬æ¢å™¨èåˆå¹¶è¡Œæ•°æ®æµï¼Œè§£å†³æ—¶é—´é”™ä½é—®é¢˜ã€‚</li>
<li>æœ€ç»ˆè¾“å‡ºç”±å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¥å­ã€‚</li>
<li>åœ¨How2SignåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†è¾ƒé«˜çš„BLEU-4åˆ†æ•°ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>åœ¨æŒ‘æˆ˜æ€§è¾ƒå¤§çš„æŒ‡æ‹¼æ•°æ®é›†ä¸Šè¾¾åˆ°äº†è¾ƒé«˜çš„å­—æ¯å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00030">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.00030v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.00030v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.00030v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2509.00030v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Simple-Yet-Effective-An-Information-Theoretic-Approach-to-Multi-LLM-Uncertainty-Quantification"><a href="#Simple-Yet-Effective-An-Information-Theoretic-Approach-to-Multi-LLM-Uncertainty-Quantification" class="headerlink" title="Simple Yet Effective: An Information-Theoretic Approach to Multi-LLM   Uncertainty Quantification"></a>Simple Yet Effective: An Information-Theoretic Approach to Multi-LLM   Uncertainty Quantification</h2><p><strong>Authors:Maya Kruse, Majid Afshar, Saksham Khatwani, Anoop Mayampurath, Guanhua Chen, Yanjun Gao</strong></p>
<p>Large language models (LLMs) often behave inconsistently across inputs, indicating uncertainty and motivating the need for its quantification in high-stakes settings. Prior work on calibration and uncertainty quantification often focuses on individual models, overlooking the potential of model diversity. We hypothesize that LLMs make complementary predictions due to differences in training and the Zipfian nature of language, and that aggregating their outputs leads to more reliable uncertainty estimates. To leverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a simple information-theoretic method that uses Jensen-Shannon Divergence to identify and aggregate well-calibrated subsets of LLMs. Experiments on binary prediction tasks demonstrate improved calibration and predictive performance compared to single-model and na&quot;ive ensemble baselines. In addition, we explore using MUSE as guided signals with chain-of-thought distillation to fine-tune LLMs for calibration. MUSE is available at:<a target="_blank" rel="noopener" href="https://github.com/LARK-NLP-Lab/MUSE">https://github.com/LARK-NLP-Lab/MUSE</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¾“å…¥ä¸Šå¸¸å¸¸è¡¨ç°å‡ºä¸ä¸€è‡´çš„è¡Œä¸ºï¼Œè¿™ä½“ç°äº†å…¶ä¸ç¡®å®šæ€§ï¼Œå¹¶æ¿€å‘äº†åœ¨é«˜é£é™©ç¯å¢ƒä¸­å¯¹å…¶è¿›è¡Œé‡åŒ–è¯„ä¼°çš„éœ€æ±‚ã€‚å…ˆå‰å…³äºæ ¡å‡†å’Œä¸ç¡®å®šæ€§é‡åŒ–çš„å·¥ä½œé€šå¸¸ä¾§é‡äºå•ä¸ªæ¨¡å‹ï¼Œå¿½ç•¥äº†æ¨¡å‹å¤šæ ·æ€§çš„æ½œåŠ›ã€‚æˆ‘ä»¬å‡è®¾LLMç”±äºè®­ç»ƒå’Œè¯­è¨€åˆ†å¸ƒçš„Zipfå±æ€§è€Œåšå‡ºäº’è¡¥é¢„æµ‹ï¼Œå¹¶ä¸”ç”±äºèšåˆå®ƒä»¬çš„è¾“å‡ºè€Œå¾—åˆ°æ›´å¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚ä¸ºäº†åˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†MUSEï¼ˆé€šè¿‡å­é›†é›†åˆçš„å¤šLLMä¸ç¡®å®šæ€§ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„ä¿¡æ¯ç†è®ºæ–¹æ³•ï¼Œä½¿ç”¨Jensen-Shannonæ•£åº¦æ¥è¯†åˆ«å’Œèšåˆæ ¡å‡†è‰¯å¥½çš„LLMå­é›†ã€‚åœ¨äºŒå…ƒé¢„æµ‹ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸å•æ¨¡å‹å’Œéç²¾æ ¡å‡†é›†åˆåŸºçº¿ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æé«˜äº†æ ¡å‡†å’Œé¢„æµ‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä½¿ç”¨MUSEä½œä¸ºå¼•å¯¼ä¿¡å·ï¼Œç»“åˆæ€è€ƒé“¾è’¸é¦æ¥å¾®è°ƒLLMçš„æ ¡å‡†ã€‚MUSEå¯åœ¨ä»¥ä¸‹ç½‘å€è·å¾—ï¼š<a target="_blank" rel="noopener" href="https://github.com/LARK-NLP-Lab/MUSE">https://github.com/LARK-NLP-Lab/MUSE</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07236v2">PDF</a> Accepted to EMNLP 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒè¾“å…¥ä¸‹è¡¨ç°å‡ºä¸ä¸€è‡´çš„è¡Œä¸ºï¼Œè¿™ä½“ç°äº†ä¸ç¡®å®šæ€§ï¼Œåœ¨é«˜é£é™©ç¯å¢ƒä¸­éœ€è¦å¯¹å…¶è¿›è¡Œé‡åŒ–è¯„ä¼°ã€‚ç°æœ‰å…³äºæ ¡å‡†å’Œä¸ç¡®å®šæ€§é‡åŒ–çš„ç ”ç©¶å¾€å¾€é›†ä¸­åœ¨å•ä¸ªæ¨¡å‹ä¸Šï¼Œå¿½ç•¥äº†æ¨¡å‹å¤šæ ·æ€§çš„æ½œåŠ›ã€‚å‡è®¾LLMå› è®­ç»ƒå’Œè¯­è¨€åˆ†å¸ƒçš„Zipfå±æ€§åšå‡ºäº’è¡¥é¢„æµ‹ï¼Œæ±‡èšå®ƒä»¬çš„è¾“å‡ºèƒ½æä¾›æ›´å¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨MUSEï¼ˆé€šè¿‡å­é›†é›†åˆåˆ©ç”¨å¤šLLMä¸ç¡®å®šæ€§ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºä¿¡æ¯è®ºçš„ç®€å•æ–¹æ³•ï¼Œåˆ©ç”¨Jensen-Shannon Divergenceæ¥è¯†åˆ«å’Œæ±‡èšæ ¡å‡†è‰¯å¥½çš„LLMå­é›†ã€‚åœ¨äºŒå…ƒé¢„æµ‹ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸å•æ¨¡å‹å’ŒåŸºçº¿é›†åˆç›¸æ¯”ï¼ŒMUSEæé«˜äº†æ ¡å‡†åº¦å’Œé¢„æµ‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†ä½¿ç”¨MUSEä½œä¸ºå¸¦æœ‰æ€ç»´é“¾è’¸é¦çš„å¼•å¯¼ä¿¡å·æ¥å¾®è°ƒLLMçš„æ ¡å‡†åº¦ã€‚MUSEå·²å…¬å¼€äºGitHubï¼š<a target="_blank" rel="noopener" href="https://github.com/LARK-NLP-Lab/MUSE%E3%80%82">https://github.com/LARK-NLP-Lab/MUSEã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ä¸åŒè¾“å…¥ä¸‹è¡¨ç°å‡ºä¸ä¸€è‡´è¡Œä¸ºï¼Œä½“ç°ä¸ç¡®å®šæ€§ï¼Œåœ¨é«˜é£é™©ç¯å¢ƒä¸­éœ€è¦é‡åŒ–è¯„ä¼°ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¤šå…³æ³¨å•ä¸ªæ¨¡å‹çš„æ ¡å‡†å’Œä¸ç¡®å®šæ€§é‡åŒ–ï¼Œå¿½ç•¥äº†æ¨¡å‹å¤šæ ·æ€§æ½œåŠ›ã€‚</li>
<li>LLMå› è®­ç»ƒå’Œè¯­è¨€åˆ†å¸ƒçš„Zipfå±æ€§åšå‡ºäº’è¡¥é¢„æµ‹ã€‚</li>
<li>MUSEæ–¹æ³•åˆ©ç”¨ä¿¡æ¯è®ºåŸç†è¯†åˆ«å’Œæ±‡èšæ ¡å‡†è‰¯å¥½çš„LLMå­é›†ï¼Œæé«˜é¢„æµ‹å¯é æ€§ã€‚</li>
<li>åœ¨äºŒå…ƒé¢„æµ‹ä»»åŠ¡ä¸Šï¼ŒMUSEç›¸æ¯”å•æ¨¡å‹å’ŒåŸºçº¿é›†åˆè¡¨ç°å‡ºæ›´å¥½çš„æ ¡å‡†åº¦å’Œé¢„æµ‹æ€§èƒ½ã€‚</li>
<li>MUSEå¯ä½œä¸ºå¼•å¯¼ä¿¡å·ï¼Œé€šè¿‡æ€ç»´é“¾è’¸é¦æ–¹å¼æé«˜LLMçš„æ ¡å‡†åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07236">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2507.07236v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2507.07236v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2507.07236v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2507.07236v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2507.07236v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2507.07236v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ParEval-Repo-A-Benchmark-Suite-for-Evaluating-LLMs-with-Repository-level-HPC-Translation-Tasks"><a href="#ParEval-Repo-A-Benchmark-Suite-for-Evaluating-LLMs-with-Repository-level-HPC-Translation-Tasks" class="headerlink" title="ParEval-Repo: A Benchmark Suite for Evaluating LLMs with   Repository-level HPC Translation Tasks"></a>ParEval-Repo: A Benchmark Suite for Evaluating LLMs with   Repository-level HPC Translation Tasks</h2><p><strong>Authors:Joshua H. Davis, Daniel Nichols, Ishan Khillan, Abhinav Bhatele</strong></p>
<p>GPGPU architectures have become significantly more diverse in recent years, which has led to an emergence of a variety of specialized programming models and software stacks to support them. Portable programming models exist, but they require significant developer effort to port to and optimize for different hardware architectures. Large language models (LLMs) may help to reduce this programmer burden. In this paper, we present a novel benchmark and testing framework, ParEval-Repo, which can be used to evaluate the efficacy of LLM-based approaches in automatically translating entire codebases across GPGPU execution models. ParEval-Repo includes several scientific computing and AI mini-applications in a range of programming models and levels of repository complexity. We use ParEval-Repo to evaluate a range of state-of-the-art open-source and commercial LLMs, with both a non-agentic and a top-down agentic approach. We assess code generated by the LLMs and approaches in terms of compilability, functional correctness, categories of build errors, and the cost of translation in terms of the number of inference tokens. Our results demonstrate that LLM translation of scientific applications is feasible for small programs but difficulty with generating functional build systems and cross-file dependencies pose challenges in scaling to larger codebases. </p>
<blockquote>
<p>GPGPUæ¶æ„è¿‘å¹´æ¥å˜å¾—æ›´ä¸ºå¤šæ ·åŒ–ï¼Œè¿™å¯¼è‡´äº†å‡ºç°äº†å¤šç§ä¸“ç”¨çš„ç¼–ç¨‹æ¨¡å‹å’Œè½¯ä»¶æ ˆæ¥æ”¯æŒå®ƒä»¬ã€‚è™½ç„¶å­˜åœ¨å¯ç§»æ¤çš„ç¼–ç¨‹æ¨¡å‹ï¼Œä½†å®ƒä»¬éœ€è¦åœ¨ä¸åŒçš„ç¡¬ä»¶æ¶æ„ä¸Šè¿›è¡Œç§»æ¤å’Œä¼˜åŒ–ï¼Œéœ€è¦å¼€å‘è€…ä»˜å‡ºå¤§é‡çš„åŠªåŠ›ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯èƒ½æœ‰åŠ©äºå‡è½»ç¨‹åºå‘˜çš„è´Ÿæ‹…ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°å‹çš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶ParEval-Repoï¼Œå®ƒå¯ç”¨äºè¯„ä¼°åŸºäºLLMçš„æ–¹æ³•åœ¨GPGPUæ‰§è¡Œæ¨¡å‹ä¸Šè‡ªåŠ¨ç¿»è¯‘æ•´ä¸ªä»£ç åº“çš„æœ‰æ•ˆæ€§ã€‚ParEval-RepoåŒ…å«å¤šç§ç¼–ç¨‹æ¨¡å‹å’Œå„ç§ä»“åº“å¤æ‚åº¦çš„ç§‘å­¦è®¡ç®—å’Œäººå·¥æ™ºèƒ½å°å‹åº”ç”¨ç¨‹åºã€‚æˆ‘ä»¬ä½¿ç”¨ParEval-Repoè¯„ä¼°äº†ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„å¼€æºå’Œå•†ä¸šLLMï¼ŒåŒ…æ‹¬éä»£ç†æ–¹æ³•å’Œè‡ªä¸Šè€Œä¸‹çš„ä»£ç†æ–¹æ³•ã€‚æˆ‘ä»¬æ ¹æ®å¯ç¼–è¯‘æ€§ã€åŠŸèƒ½æ­£ç¡®æ€§ã€æ„å»ºé”™è¯¯ç±»åˆ«ä»¥åŠæ¨ç†ä»¤ç‰Œæ•°é‡æ¥è¯„ä¼°LLMå’Œæ–¹æ³•ç”Ÿæˆçš„ä»£ç ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒLLMåœ¨ç§‘å­¦åº”ç”¨ç¨‹åºçš„ç¿»è¯‘æ–¹é¢æ˜¯å¯è¡Œçš„ï¼Œä½†å¯¹äºç”ŸæˆåŠŸèƒ½æ€§æ„å»ºç³»ç»Ÿå’Œè·¨æ–‡ä»¶ä¾èµ–æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™ä½¿å¾—åœ¨å¤§è§„æ¨¡ä»£ç åº“ä¸Šçš„æ‰©å±•å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20938v2">PDF</a> 10 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹è¯„ä¼°æ¡†æ¶ParEval-Repoï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨GPGPUæ¶æ„ä¸Šè‡ªåŠ¨ç¿»è¯‘ä»£ç åº“çš„æ•ˆæœã€‚è¯¥æ¡†æ¶åŒ…å«å¤šä¸ªç§‘å­¦è®¡ç®—å’ŒAIå°å‹åº”ç”¨ç¨‹åºï¼Œæ¶µç›–ä¸åŒçš„ç¼–ç¨‹æ¨¡å‹å’Œä»“åº“å¤æ‚æ€§çº§åˆ«ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒLLMåœ¨ç¿»è¯‘ç§‘å­¦åº”ç”¨ç¨‹åºæ–¹é¢å¯¹äºå°å‹ç¨‹åºæ˜¯å¯è¡Œçš„ï¼Œä½†åœ¨ç”ŸæˆåŠŸèƒ½æ„å»ºç³»ç»Ÿå’Œå¤„ç†è·¨æ–‡ä»¶ä¾èµ–æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPGPUæ¶æ„çš„å¤šæ ·æ€§å¯¼è‡´äº†å¤šç§ä¸“ç”¨ç¼–ç¨‹æ¨¡å‹å’Œè½¯ä»¶å †æ ˆçš„å‡ºç°ã€‚</li>
<li>ä¾¿æºå¼ç¼–ç¨‹æ¨¡å‹éœ€è¦å¼€å‘è€…æŠ•å…¥å¤§é‡åŠªåŠ›æ¥é€‚åº”å’Œä¼˜åŒ–ä¸åŒçš„ç¡¬ä»¶æ¶æ„ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ‰åŠ©äºå‡è½»ç¨‹åºå‘˜è´Ÿæ‹…ã€‚</li>
<li>ParEval-Repoæ˜¯ä¸€ä¸ªæ–°å‹è¯„ä¼°æ¡†æ¶ï¼Œå¯ç”¨äºè¯„ä¼°LLMåœ¨GPGPUæ‰§è¡Œæ¨¡å‹ä¸Šè‡ªåŠ¨ç¿»è¯‘ä»£ç åº“çš„æ•ˆæœã€‚</li>
<li>ParEval-RepoåŒ…å«å¤šä¸ªç§‘å­¦è®¡ç®—å’ŒAIå°å‹åº”ç”¨ç¨‹åºï¼Œæ¶µç›–ä¸åŒçš„ç¼–ç¨‹æ¨¡å‹å’Œä»“åº“å¤æ‚æ€§çº§åˆ«ã€‚</li>
<li>LLMåœ¨ç¿»è¯‘ç§‘å­¦åº”ç”¨ç¨‹åºæ–¹é¢å¯¹äºå°å‹ç¨‹åºæ˜¯å¯è¡Œçš„ï¼Œä½†åœ¨ç”ŸæˆåŠŸèƒ½æ„å»ºç³»ç»Ÿå’Œå¤„ç†è·¨æ–‡ä»¶ä¾èµ–æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20938">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2506.20938v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2506.20938v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2506.20938v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2506.20938v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Donâ€™t-Make-It-Up-Preserving-Ignorance-Awareness-in-LLM-Fine-Tuning"><a href="#Donâ€™t-Make-It-Up-Preserving-Ignorance-Awareness-in-LLM-Fine-Tuning" class="headerlink" title="Donâ€™t Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning"></a>Donâ€™t Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning</h2><p><strong>Authors:William F. Shen, Xinchi Qiu, Nicola Cancedda, Nicholas D. Lane</strong></p>
<p>Existing work on mitigating catastrophic forgetting during large language models (LLMs) fine-tuning for new knowledge instances has primarily focused on preserving performance on previously seen data, while critically overlooking the collapse of essential capabilities instilled through alignment, most notably the modelâ€™s ability to faithfully express epistemic uncertainty (a property we term â€˜Ignorance Awarenessâ€™). In this work, we formalize the notion of Ignorance Awareness and illustrate that conventional fine-tuning methods can result in substantial activation displacement. This displacement undermines the critical capability of ignorance awareness, leading to undesirable behaviors such as hallucinations. To address this challenge, we introduce SEAT, a simple and principled fine-tuning approach that not only enables the model to effectively acquire new knowledge instances but also preserves its aligned ignorance awareness. SEAT integrates two key components: (1) sparse tuning that constrains activation drift, and (2) a novel entity perturbation method designed to counter knowledge entanglement. Experimental results demonstrate that, across both real-world and synthetic datasets, SEAT significantly outperforms baselines in preserving ignorance awareness while retaining optimal fine-tuning performance, offering a more robust solution for LLM fine-tuning. </p>
<blockquote>
<p>å…³äºåœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é’ˆå¯¹æ–°çŸ¥è¯†å®ä¾‹è¿›è¡Œå¾®è°ƒæ—¶ç¼“è§£ç¾éš¾æ€§é—å¿˜çš„ç°æœ‰å·¥ä½œä¸»è¦é›†ä¸­åœ¨ä¿æŒå¯¹å…ˆå‰æ•°æ®çš„æ€§èƒ½ä¸Šï¼Œè€Œä¸¥é‡å¿½è§†äº†é€šè¿‡å¯¹é½çŒè¾“çš„åŸºæœ¬èƒ½åŠ›çš„å´©æºƒï¼Œå°¤å…¶æ˜¯æ¨¡å‹å¿ å®è¡¨è¾¾è®¤è¯†ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ— çŸ¥æ„è¯†â€ï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ­£å¼æå‡ºæ— çŸ¥æ„è¯†çš„æ¦‚å¿µï¼Œå¹¶è¯´æ˜ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•å¯èƒ½å¯¼è‡´æ˜¾è‘—çš„æ¿€æ´»ä½ç§»ã€‚è¿™ç§ä½ç§»ä¼šç ´åæ— çŸ¥æ„è¯†çš„å…³é”®èƒ½åŠ›ï¼Œä»è€Œå¯¼è‡´å‡ºç°å¦‚å¹»è§‰ç­‰ä¸æƒ³è¦çš„è¡Œä¸ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SEATï¼Œè¿™æ˜¯ä¸€ç§ç®€å•ä¸”åŸºäºåŸåˆ™çš„å¾®è°ƒæ–¹æ³•ï¼Œå®ƒä¸ä»…èƒ½ä½¿æ¨¡å‹æœ‰æ•ˆåœ°è·å–æ–°çŸ¥è¯†å®ä¾‹ï¼Œè€Œä¸”è¿˜èƒ½ä¿æŒå…¶å¯¹é½çš„æ— çŸ¥æ„è¯†ã€‚SEATé›†æˆäº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰ç¨€ç–è°ƒæ•´ï¼Œç”¨äºé™åˆ¶æ¿€æ´»æ¼‚ç§»ï¼›ï¼ˆ2ï¼‰ä¸€ç§æ–°å‹çš„å®ä½“æ‰°åŠ¨æ–¹æ³•ï¼Œæ—¨åœ¨å¯¹æŠ—çŸ¥è¯†çº ç¼ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨ç°å®ä¸–ç•Œæ•°æ®é›†è¿˜æ˜¯åˆæˆæ•°æ®é›†ä¸Šï¼ŒSEATåœ¨ä¿æŒæ— çŸ¥æ„è¯†çš„åŒæ—¶ï¼Œåœ¨å¾®è°ƒæ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŸºå‡†çº¿ï¼Œä¸ºLLMå¾®è°ƒæä¾›äº†æ›´ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.14387v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾®è°ƒæ–°çŸ¥è¯†å®ä¾‹æ—¶ï¼Œç°æœ‰å·¥ä½œä¸»è¦å…³æ³¨å¦‚ä½•åœ¨å…ˆå‰æ•°æ®ä¸Šä¿æŒæ€§èƒ½ï¼Œå´å¿½è§†äº†é€šè¿‡å¯¹é½çŒè¾“çš„å…³é”®èƒ½åŠ›çš„å´©æºƒï¼Œå°¤å…¶æ˜¯æ¨¡å‹å¿ å®è¡¨è¾¾è®¤çŸ¥ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ— çŸ¥æ„è¯†â€ï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ­£å¼æå‡ºäº†æ— çŸ¥æ„è¯†çš„æ¦‚å¿µï¼Œå¹¶è¯´æ˜ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•å¯èƒ½å¯¼è‡´æ˜¾è‘—çš„æ´»åŠ¨ä½ç§»ã€‚è¿™ç§ä½ç§»ç ´åäº†æ— çŸ¥æ„è¯†çš„å…³é”®èƒ½åŠ›ï¼Œä»è€Œå¯¼è‡´å‡ºç°å¦‚å¹»æƒ³ç­‰ä¸æƒ³è¦çš„è¡Œä¸ºã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SEATï¼Œè¿™æ˜¯ä¸€ç§ç®€å•ä¸”ä»¥åŸåˆ™ä¸ºåŸºç¡€çš„å¾®è°ƒæ–¹æ³•ï¼Œä¸ä»…èƒ½ä½¿æ¨¡å‹æœ‰æ•ˆåœ°è·å–æ–°çŸ¥è¯†å®ä¾‹ï¼Œè€Œä¸”è¿˜èƒ½ä¿æŒå…¶å¯¹é½çš„æ— çŸ¥æ„è¯†ã€‚SEATé›†æˆäº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰ç¨€ç–è°ƒä¼˜ï¼Œçº¦æŸæ´»åŠ¨æ¼‚ç§»ï¼›ï¼ˆ2ï¼‰ä¸€ç§æ–°å‹å®ä½“æ‰°åŠ¨æ–¹æ³•ï¼Œæ—¨åœ¨å¯¹æŠ—çŸ¥è¯†çº ç¼ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨ç°å®ä¸–ç•Œè¿˜æ˜¯åˆæˆæ•°æ®é›†ä¸Šï¼ŒSEATåœ¨ä¿æŒæ— çŸ¥æ„è¯†æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿ï¼ŒåŒæ—¶ä¿æŒæœ€ä½³çš„å¾®è°ƒæ€§èƒ½ï¼Œä¸ºLLMå¾®è°ƒæä¾›äº†æ›´ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰LLMå¾®è°ƒå·¥ä½œä¸»è¦å…³æ³¨ä¿æŒå…ˆå‰æ•°æ®æ€§èƒ½ï¼Œå¿½è§†äº†å…³é”®èƒ½åŠ›çš„å´©æºƒï¼Œç‰¹åˆ«æ˜¯å¿ å®è¡¨è¾¾è®¤çŸ¥ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ï¼ˆæ— çŸ¥æ„è¯†ï¼‰ã€‚</li>
<li>ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•å¯èƒ½å¯¼è‡´æ´»åŠ¨ä½ç§»ï¼Œç ´åæ— çŸ¥æ„è¯†ã€‚</li>
<li>SEATæ˜¯ä¸€ç§æ–°å‹å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒåŒæ—¶ä¿ç•™æ–°çŸ¥è¯†çš„è·å–å’Œå¯¹é½çš„æ— çŸ¥æ„è¯†ã€‚</li>
<li>SEATåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šç¨€ç–è°ƒä¼˜å’Œå®ä½“æ‰°åŠ¨æ–¹æ³•ã€‚</li>
<li>ç¨€ç–è°ƒä¼˜ç”¨äºçº¦æŸæ´»åŠ¨æ¼‚ç§»ã€‚</li>
<li>å®ä½“æ‰°åŠ¨æ–¹æ³•æ—¨åœ¨å¯¹æŠ—çŸ¥è¯†çº ç¼ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.14387">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2506.14387v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2506.14387v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2506.14387v2/page_2_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="First-Steps-Towards-Overhearing-LLM-Agents-A-Case-Study-With-Dungeons-Dragons-Gameplay"><a href="#First-Steps-Towards-Overhearing-LLM-Agents-A-Case-Study-With-Dungeons-Dragons-Gameplay" class="headerlink" title="First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons &amp;   Dragons Gameplay"></a>First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons &amp;   Dragons Gameplay</h2><p><strong>Authors:Andrew Zhu, Evan Osgood, Chris Callison-Burch</strong></p>
<p>Much work has been done on conversational LLM agents which directly assist human users with tasks. We present an alternative paradigm for interacting with LLM agents, which we call â€œoverhearing agentsâ€. These overhearing agents do not actively participate in conversation â€“ instead, they â€œlisten inâ€ on human-to-human conversations and perform background tasks or provide suggestions to assist the user. In this work, we explore the overhearing agents paradigm through the lens of Dungeons &amp; Dragons gameplay. We present an in-depth study using large multimodal audio-language models as overhearing agents to assist a Dungeon Master. We perform a human evaluation to examine the helpfulness of such agents and find that some large audio-language models have the emergent ability to perform overhearing agent tasks using implicit audio cues. Finally, we release Python libraries and our project code to support further research into the overhearing agents paradigm at <a target="_blank" rel="noopener" href="https://github.com/zhudotexe/overhearing_agents">https://github.com/zhudotexe/overhearing_agents</a>. </p>
<blockquote>
<p>å…³äºå¯¹è¯å¼LLMä»£ç†çš„ç ”ç©¶å·²ç»å–å¾—äº†å¤§é‡æˆæœï¼Œè¿™äº›ä»£ç†èƒ½å¤Ÿç›´æ¥å¸®åŠ©äººç±»ç”¨æˆ·å®Œæˆä»»åŠ¡ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ä¸LLMä»£ç†äº¤äº’çš„æ›¿ä»£æ¨¡å¼ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ—å¬ä»£ç†â€ã€‚è¿™äº›æ—å¬ä»£ç†ä¸ä¼šä¸»åŠ¨å‚ä¸å¯¹è¯â€”â€”ç›¸åï¼Œå®ƒä»¬ä¼šâ€œå¬å–â€äººç±»ä¹‹é—´çš„å¯¹è¯ï¼Œå¹¶åœ¨åå°æ‰§è¡Œä»»åŠ¡æˆ–æä¾›å»ºè®®ä»¥å¸®åŠ©ç”¨æˆ·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»ã€Šé¾™ä¸åœ°ä¸‹åŸã€‹æ¸¸æˆçš„è§’åº¦æ¢è®¨äº†æ—å¬ä»£ç†æ¨¡å¼ã€‚æˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†ä½¿ç”¨å¤§å‹å¤šæ¨¡æ€éŸ³é¢‘è¯­è¨€æ¨¡å‹ä½œä¸ºæ—å¬ä»£ç†æ¥å¸®åŠ©åœ°ä¸‹åŸå¤§å¸ˆã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹äººç±»è¯„ä¼°ï¼Œä»¥æ£€æŸ¥è¿™äº›ä»£ç†çš„æœ‰ç”¨æ€§ï¼Œå¹¶å‘ç°æŸäº›å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹å·²ç»å…·å¤‡äº†åˆ©ç”¨éšæ€§éŸ³é¢‘çº¿ç´¢æ‰§è¡Œæ—å¬ä»£ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhudotexe/overhearing_agents%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86Python%E5%BA%93%E5%92%8C%E9%A1%B9%E7%9B%AE%E4%BB%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E6%94%AF%E6%8C%81%E5%AF%B9%E6%97%81%E5%90%AC%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%E7%9A%84%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/zhudotexe/overhearing_agentsä¸Šå‘å¸ƒäº†Pythonåº“å’Œé¡¹ç›®ä»£ç ï¼Œä»¥æ”¯æŒå¯¹æ—å¬ä»£ç†æ¨¡å¼çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.22809v2">PDF</a> 9 pages, 5 figures. COLM 2025 Workshop on AI Agents</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºâ€œoverhearing agentsâ€çš„æ–°å‹äººæœºäº¤äº’æ¨¡å¼ã€‚ä¸åŒäºä¼ ç»Ÿçš„å¯¹è¯å¼LLMä»£ç†ï¼Œoverhearing agentsä¸ä¼šä¸»åŠ¨å‚ä¸å¯¹è¯ï¼Œè€Œæ˜¯ä¼šå€¾å¬äººç±»ä¹‹é—´çš„å¯¹è¯å¹¶åœ¨åå°æ‰§è¡Œä»»åŠ¡æˆ–æä¾›å»ºè®®ä»¥ååŠ©ç”¨æˆ·ã€‚æœ¬æ–‡ç€é‡é€šè¿‡â€œDungeons &amp; Dragonsâ€æ¸¸æˆçš„è§†è§’ï¼Œæ¢è®¨overhearing agentsçš„ç ”ç©¶å’Œåº”ç”¨ã€‚ç ”ç©¶å‘ç°ï¼Œå¤§å‹å¤šåª’ä½“éŸ³é¢‘è¯­è¨€æ¨¡å‹å¯ä»¥ä½œä¸ºååŠ©æ¸¸æˆä¸»æŒäººï¼ˆDungeon Masterï¼‰çš„overhearing agentsã€‚é€šè¿‡äººç±»è¯„ä¼°å‘ç°ï¼ŒæŸäº›å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨éšå¼éŸ³é¢‘çº¿ç´¢å®Œæˆoverhearingä»£ç†ä»»åŠ¡ã€‚æœ€åï¼Œæœ¬æ–‡å‘å¸ƒäº†Pythonåº“å’Œé¡¹ç›®ä»£ç ä»¥æ”¯æŒå¯¹overhearingä»£ç†æ¨¡å¼çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§æ–°å‹çš„äººæœºäº¤äº’æ¨¡å¼â€”â€”overhearing agentsã€‚</li>
<li>Overhearing agentsä¸ä¸»åŠ¨å‚ä¸å¯¹è¯ï¼Œè€Œæ˜¯å€¾å¬å¹¶å®Œæˆä»»åŠ¡æˆ–æä¾›å»ºè®®ã€‚</li>
<li>é€šè¿‡â€œDungeons &amp; Dragonsâ€æ¸¸æˆè§†è§’æ¢è®¨äº†overhearing agentsçš„åº”ç”¨å’Œç ”ç©¶ã€‚</li>
<li>å¤§å‹å¤šåª’ä½“éŸ³é¢‘è¯­è¨€æ¨¡å‹å¯ä»¥ä½œä¸ºååŠ©æ¸¸æˆä¸»æŒäººçš„overhearing agentsã€‚</li>
<li>ä¸€äº›å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹å…·å¤‡åˆ©ç”¨éšå¼éŸ³é¢‘çº¿ç´¢å®Œæˆoverhearingä»£ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡äººç±»è¯„ä¼°éªŒè¯äº†overhearingä»£ç†çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.22809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.22809v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.22809v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.22809v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.22809v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.22809v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="RAVEN-Query-Guided-Representation-Alignment-for-Question-Answering-over-Audio-Video-Embedded-Sensors-and-Natural-Language"><a href="#RAVEN-Query-Guided-Representation-Alignment-for-Question-Answering-over-Audio-Video-Embedded-Sensors-and-Natural-Language" class="headerlink" title="RAVEN: Query-Guided Representation Alignment for Question Answering over   Audio, Video, Embedded Sensors, and Natural Language"></a>RAVEN: Query-Guided Representation Alignment for Question Answering over   Audio, Video, Embedded Sensors, and Natural Language</h2><p><strong>Authors:Subrata Biswas, Mohammad Nur Hossain Khan, Bashima Islam</strong></p>
<p>Multimodal question answering (QA) often requires identifying which video, audio, or sensor tokens are relevant to the question. Yet modality disagreements are common: off-camera speech, background noise, or motion outside the field of view often mislead fusion models that weight all streams equally. We present RAVEN, a unified QA architecture whose core is QuART, a query-conditioned cross-modal gating module that assigns scalar relevance scores to each token across modalities, enabling the model to amplify informative signals and suppress distractors before fusion. RAVEN is trained through a three-stage pipeline comprising unimodal pretraining, query-aligned fusion, and disagreement-oriented fine-tuning â€“ each stage targeting a distinct challenge in multi-modal reasoning: representation quality, cross-modal relevance, and robustness to modality mismatch. To support training and evaluation, we release AVS-QA, a dataset of 300K synchronized Audioâ€“Video-Sensor streams paired with automatically generated question-answer pairs. Experimental results on seven multi-modal QA benchmarks â€“ including egocentric and exocentric tasks â€“ show that RAVEN achieves up to 14.5% and 8.0% gains in accuracy compared to state-of-the-art multi-modal large language models, respectively. Incorporating sensor data provides an additional 16.4% boost, and the model remains robust under modality corruption, outperforming SOTA baselines by 50.23%. Our code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/BASHLab/RAVEN">https://github.com/BASHLab/RAVEN</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€é—®ç­”ï¼ˆQAï¼‰é€šå¸¸éœ€è¦è¯†åˆ«å“ªäº›è§†é¢‘ã€éŸ³é¢‘æˆ–ä¼ æ„Ÿå™¨æ ‡è®°ä¸é—®é¢˜ç›¸å…³ã€‚ç„¶è€Œï¼Œæ¨¡æ€å†²çªå¾ˆå¸¸è§ï¼šç¦»é•œå¤´å¤–çš„è¯­éŸ³ã€èƒŒæ™¯å™ªéŸ³æˆ–è§†é‡å¤–çš„åŠ¨ä½œç»å¸¸è¯¯å¯¼å°†æ‰€æœ‰æµå¹³ç­‰å¯¹å¾…çš„èåˆæ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†RAVENï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€é—®ç­”æ¶æ„ï¼Œå…¶æ ¸å¿ƒæ˜¯QuARTï¼Œä¸€ç§æŸ¥è¯¢æ¡ä»¶è·¨æ¨¡æ€é—¨æ§æ¨¡å—ï¼Œå®ƒä¸ºæ¯ä¸ªæ¨¡æ€çš„æ ‡è®°åˆ†é…æ ‡é‡ç›¸å…³æ€§åˆ†æ•°ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨èåˆä¹‹å‰æ”¾å¤§ä¿¡æ¯ä¿¡å·å¹¶æŠ‘åˆ¶å¹²æ‰°å› ç´ ã€‚RAVENé€šè¿‡åŒ…æ‹¬å•æ¨¡æ€é¢„è®­ç»ƒã€æŸ¥è¯¢å¯¹é½èåˆå’Œé¢å‘åˆ†æ­§çš„å¾®è°ƒçš„ä¸‰ä¸ªé˜¶æ®µç®¡é“è¿›è¡Œè®­ç»ƒâ€”â€”æ¯ä¸ªé˜¶æ®µéƒ½é’ˆå¯¹å¤šæ¨¡æ€æ¨ç†ä¸­çš„ä¸åŒæŒ‘æˆ˜ï¼šè¡¨ç¤ºè´¨é‡ã€è·¨æ¨¡æ€ç›¸å…³æ€§å’Œå¯¹æ¨¡æ€ä¸åŒ¹é…çš„ç¨³å¥æ€§ã€‚ä¸ºäº†æ”¯æŒè®­ç»ƒå’Œè¯„ä¼°ï¼Œæˆ‘ä»¬å‘å¸ƒäº†AVS-QAæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«30ä¸‡ä¸ªè‡ªåŠ¨ç”Ÿæˆçš„é…å¯¹é—®é¢˜ç­”æ¡ˆçš„åŒæ­¥éŸ³é¢‘-è§†é¢‘-ä¼ æ„Ÿå™¨æµã€‚åœ¨ä¸ƒä¸ªå¤šæ¨¡æ€é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒRAVENåœ¨å‡†ç¡®ç‡ä¸Šåˆ†åˆ«æé«˜äº†14.5%å’Œ8.0%ã€‚åŠ å…¥ä¼ æ„Ÿå™¨æ•°æ®æä¾›äº†é¢å¤–çš„16.4%çš„æå‡ï¼Œå¹¶ä¸”åœ¨æ¨¡æ€æŸåçš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹ä»ç„¶ç¨³å¥ï¼Œä¼˜äºæœ€æ–°åŸºå‡†æµ‹è¯•50.23%ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BASHLab/RAVEN%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/BASHLab/RAVENä¸Šè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17114v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†RAVENæ¶æ„åœ¨å¤šæ¨¡æ€é—®ç­”ï¼ˆQAï¼‰ä¸­çš„åˆ›æ–°åº”ç”¨ã€‚é’ˆå¯¹è§†é¢‘ã€éŸ³é¢‘å’Œä¼ æ„Ÿå™¨æ•°æ®çš„èåˆé—®é¢˜ï¼ŒRAVENé€šè¿‡å¼•å…¥QuARTæ¨¡å—ï¼Œä¸ºä¸åŒæ¨¡æ€çš„ä»¤ç‰Œåˆ†é…æ ‡é‡ç›¸å…³æ€§åˆ†æ•°ï¼Œæå‡ä¿¡æ¯ä¿¡å·çš„å¼ºåº¦å¹¶æŠ‘åˆ¶å¹²æ‰°å› ç´ ã€‚é€šè¿‡ä¸‰ä¸ªé˜¶æ®µè®­ç»ƒç®¡é“ï¼ŒåŒ…æ‹¬å•æ¨¡æ€é¢„è®­ç»ƒã€æŸ¥è¯¢å¯¹é½èåˆå’Œé¢å‘åˆ†æ­§çš„å¾®è°ƒï¼ŒRAVENè§£å†³äº†å¤šæ¨¡æ€æ¨ç†ä¸­çš„ä¸‰å¤§æŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œä¸ºäº†æ”¯æŒè®­ç»ƒå’Œè¯„ä¼°ï¼Œå‘å¸ƒäº†AVS-QAæ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRAVENç›¸è¾ƒäºç›®å‰é¢†å…ˆçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æœ‰æ˜¾è‘—æå‡ã€‚ç»“åˆä¼ æ„Ÿå™¨æ•°æ®å¸¦æ¥çš„å‡†ç¡®æ€§æé«˜é¢å¤–è¾¾16.4%ï¼Œå¹¶åœ¨æ¨¡æ€æŸåçš„æƒ…å†µä¸‹ä¿æŒç¨³å¥æ€§ï¼Œè¶…å‡ºåŸºçº¿æ¨¡å‹50.23%ã€‚ä»£ç å’Œæ•°æ®é›†å·²åœ¨æŒ‡å®šç½‘ç«™å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAVENæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€é—®ç­”æ¶æ„ï¼Œé’ˆå¯¹è§†é¢‘ã€éŸ³é¢‘å’Œä¼ æ„Ÿå™¨æ•°æ®çš„èåˆé—®é¢˜æå‡ºåˆ›æ–°è§£å†³æ–¹æ¡ˆã€‚</li>
<li>RAVENçš„æ ¸å¿ƒæ˜¯QuARTæ¨¡å—ï¼Œèƒ½æ ¹æ®æŸ¥è¯¢æ¡ä»¶ä¸ºä¸åŒæ¨¡æ€çš„ä»¤ç‰Œåˆ†é…ç›¸å…³æ€§åˆ†æ•°ã€‚</li>
<li>RAVENé€šè¿‡ä¸‰ä¸ªé˜¶æ®µè®­ç»ƒç®¡é“è§£å†³å¤šæ¨¡æ€æ¨ç†ä¸­çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>AVS-QAæ•°æ®é›†çš„å‘å¸ƒæ”¯æŒäº†RAVENçš„è®­ç»ƒå’Œè¯„ä¼°ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRAVENç›¸è¾ƒäºç°æœ‰æ¨¡å‹æ˜¾è‘—æé«˜å¤šæ¨¡æ€é—®ç­”çš„å‡†ç¡®æ€§ã€‚</li>
<li>ç»“åˆä¼ æ„Ÿå™¨æ•°æ®å¯è¿›ä¸€æ­¥æé«˜RAVENçš„å‡†ç¡®æ€§è¾¾16.4%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17114">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.17114v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.17114v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.17114v3/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.17114v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.17114v3/page_4_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ArtRAG-Retrieval-Augmented-Generation-with-Structured-Context-for-Visual-Art-Understanding"><a href="#ArtRAG-Retrieval-Augmented-Generation-with-Structured-Context-for-Visual-Art-Understanding" class="headerlink" title="ArtRAG: Retrieval-Augmented Generation with Structured Context for   Visual Art Understanding"></a>ArtRAG: Retrieval-Augmented Generation with Structured Context for   Visual Art Understanding</h2><p><strong>Authors:Shuai Wang, Ivona Najdenkoska, Hongyi Zhu, Stevan Rudinac, Monika Kackovic, Nachoem Wijnberg, Marcel Worring</strong></p>
<p>Understanding visual art requires reasoning across multiple perspectives â€“ cultural, historical, and stylistic â€“ beyond mere object recognition. While recent multimodal large language models (MLLMs) perform well on general image captioning, they often fail to capture the nuanced interpretations that fine art demands. We propose ArtRAG, a novel, training-free framework that combines structured knowledge with retrieval-augmented generation (RAG) for multi-perspective artwork explanation. ArtRAG automatically constructs an Art Context Knowledge Graph (ACKG) from domain-specific textual sources, organizing entities such as artists, movements, themes, and historical events into a rich, interpretable graph. At inference time, a multi-granular structured retriever selects semantically and topologically relevant subgraphs to guide generation. This enables MLLMs to produce contextually grounded, culturally informed art descriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG outperforms several heavily trained baselines. Human evaluations further confirm that ArtRAG generates coherent, insightful, and culturally enriched interpretations. </p>
<blockquote>
<p>ç†è§£è§†è§‰è‰ºæœ¯éœ€è¦è¶…è¶Šå•çº¯ç‰©ä½“è¯†åˆ«çš„å¤šä¸ªè§’åº¦â€”â€”æ–‡åŒ–ã€å†å²å’Œé£æ ¼â€”â€”çš„æ¨ç†ã€‚è™½ç„¶æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä¸€èˆ¬å›¾åƒæ ‡é¢˜åˆ›ä½œæ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•æ•æ‰åˆ°è‰ºæœ¯æ‰€è¦æ±‚çš„å¾®å¦™è§£è¯»ã€‚æˆ‘ä»¬æå‡ºäº†ArtRAGï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„æ— è®­ç»ƒæ¡†æ¶ï¼Œå®ƒå°†ç»“æ„åŒ–çŸ¥è¯†ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç›¸ç»“åˆï¼Œç”¨äºå¤šè§’åº¦è‰ºæœ¯å“è§£é‡Šã€‚ArtRAGè‡ªåŠ¨ä»ç‰¹å®šé¢†åŸŸçš„æ–‡æœ¬æºæ„å»ºè‰ºæœ¯ä¸Šä¸‹æ–‡çŸ¥è¯†å›¾è°±ï¼ˆACKGï¼‰ï¼Œå°†è‰ºæœ¯å®¶ã€è¿åŠ¨ã€ä¸»é¢˜å’Œå†å²äº‹ä»¶ç­‰å®ä½“ç»„ç»‡æˆä¸€ä¸ªä¸°å¯Œã€å¯è§£é‡Šçš„å›¾è°±ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¤šç²’åº¦ç»“æ„åŒ–æ£€ç´¢å™¨é€‰æ‹©è¯­ä¹‰å’Œæ‹“æ‰‘ä¸Šç›¸å…³çš„å­å›¾æ¥å¼•å¯¼ç”Ÿæˆã€‚è¿™ä½¿å¾—MLLMèƒ½å¤Ÿäº§ç”Ÿå…·æœ‰ä¸Šä¸‹æ–‡å’Œæ–‡åŒ–å†…æ¶µçš„è‰ºæœ¯æè¿°ã€‚åœ¨SemArtå’ŒArtpediaæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒArtRAGä¼˜äºå‡ ä¸ªç»è¿‡å¤§é‡è®­ç»ƒçš„åŸºç¡€æ¨¡å‹ã€‚äººç±»è¯„ä¼°è¿›ä¸€æ­¥è¯å®ï¼ŒArtRAGç”Ÿæˆçš„è§£é‡Šå…·æœ‰è¿è´¯æ€§ã€æ´å¯ŸåŠ›å’Œæ–‡åŒ–ä¸°å¯Œæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06020v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è‰ºæœ¯ç†è§£éœ€è¦è·¨è¶Šæ–‡åŒ–ã€å†å²å’Œé£æ ¼ç­‰å¤šä¸ªè§’åº¦è¿›è¡Œæ¨ç†ï¼Œè€Œä¸ä»…ä»…æ˜¯ç®€å•çš„ç‰©ä½“è¯†åˆ«ã€‚è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸€èˆ¬å›¾åƒæè¿°æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¯¹ç¾æœ¯ä½œå“çš„ç»†è‡´è§£è¯»ä¸Šå¸¸æ˜¾ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†ArtRAGæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆç»“æ„åŒ–çŸ¥è¯†ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ— è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºå¤šè§†è§’è‰ºæœ¯ä½œå“è§£é‡Šã€‚ArtRAGè‡ªåŠ¨ä»ç‰¹å®šæ–‡æœ¬æºæ„å»ºè‰ºæœ¯è¯­å¢ƒçŸ¥è¯†å›¾è°±ï¼ˆACKGï¼‰ï¼Œå°†è‰ºæœ¯å®¶ã€è¿åŠ¨ã€ä¸»é¢˜å’Œå†å²äº‹ä»¶ç­‰å®ä½“ç»„ç»‡æˆä¸°å¯Œä¸”å¯è§£é‡Šçš„å›¾è°±ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œå¤šç²’åº¦ç»“æ„åŒ–æ£€ç´¢å™¨é€‰æ‹©è¯­ä¹‰å’Œæ‹“æ‰‘ä¸Šç›¸å…³çš„å­å›¾æ¥æŒ‡å¯¼ç”Ÿæˆã€‚è¿™ä½¿å¾—MLLMsèƒ½å¤Ÿäº§ç”Ÿå…·æœ‰è¯­å¢ƒå’Œæ–‡åŒ–èƒŒæ™¯çš„è‰ºæœ¯æè¿°ã€‚åœ¨SemArtå’ŒArtpediaæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒArtRAGä¼˜äºå¤šä¸ªç»è¿‡è®­ç»ƒçš„é‡åŸºæ¨¡å‹ã€‚äººç±»è¯„ä¼°è¿›ä¸€æ­¥è¯å®ï¼ŒArtRAGç”Ÿæˆçš„è§£é‡Šå…·æœ‰è¿è´¯æ€§ã€æ´å¯ŸåŠ›å’Œæ–‡åŒ–ä¸°å¯Œæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç†è§£è§†è§‰è‰ºæœ¯éœ€è¦è·¨è¶Šæ–‡åŒ–ã€å†å²å’Œé£æ ¼ç­‰å¤šè§’åº¦è¿›è¡Œæ¨ç†ã€‚</li>
<li>ç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¾æœ¯ä½œå“çš„ç»†è‡´è§£è¯»ä¸Šå¯èƒ½å­˜åœ¨ä¸è¶³ã€‚</li>
<li>ArtRAGæ˜¯ä¸€ä¸ªç»“åˆç»“æ„åŒ–çŸ¥è¯†ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ¡†æ¶ï¼Œç”¨äºå¤šè§†è§’è‰ºæœ¯ä½œå“è§£é‡Šã€‚</li>
<li>ArtRAGè‡ªåŠ¨æ„å»ºè‰ºæœ¯è¯­å¢ƒçŸ¥è¯†å›¾è°±ï¼ˆACKGï¼‰ï¼Œç»„ç»‡ç›¸å…³å®ä½“ä»¥æä¾›ä¸°å¯Œä¸”å¯è§£é‡Šçš„ä¿¡æ¯ã€‚</li>
<li>å¤šç²’åº¦ç»“æ„åŒ–æ£€ç´¢å™¨é€‰æ‹©ç›¸å…³å­å›¾æ¥æŒ‡å¯¼ç”Ÿæˆï¼Œäº§ç”Ÿå…·æœ‰è¯­å¢ƒå’Œæ–‡åŒ–èƒŒæ™¯çš„è‰ºæœ¯æè¿°ã€‚</li>
<li>ArtRAGåœ¨SemArtå’ŒArtpediaæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå¤šä¸ªè®­ç»ƒæœ‰ç´ çš„åŸºçº¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06020">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.06020v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.06020v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.06020v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.06020v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.06020v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2505.06020v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Simulate-Personas-with-Reversed-Performance-A-Benchmark-for-Counterfactual-Instruction-Following"><a href="#Can-LLMs-Simulate-Personas-with-Reversed-Performance-A-Benchmark-for-Counterfactual-Instruction-Following" class="headerlink" title="Can LLMs Simulate Personas with Reversed Performance? A Benchmark for   Counterfactual Instruction Following"></a>Can LLMs Simulate Personas with Reversed Performance? A Benchmark for   Counterfactual Instruction Following</h2><p><strong>Authors:Sai Adith Senthil Kumar, Hao Yan, Saipavan Perepa, Murong Yue, Ziyu Yao</strong></p>
<p>Large Language Models (LLMs) are now increasingly widely used to simulate personas in virtual environments, leveraging their instruction-following capability. However, we discovered that even state-of-the-art LLMs cannot simulate personas with reversed performance (e.g., student personas with low proficiency in educational settings), which impairs the simulation diversity and limits the practical applications of the simulated environments. In this work, using mathematical reasoning as a representative scenario, we propose the first benchmark dataset for evaluating LLMs on simulating personas with reversed performance, a capability that we dub â€œcounterfactual instruction followingâ€. We evaluate both open-weight and closed-source LLMs on this task and find that LLMs, including the OpenAI o1 reasoning model, all struggle to follow counterfactual instructions for simulating reversedly performing personas. Intersectionally simulating both the performance level and the race population of a persona worsens the effect even further. These results highlight the challenges of counterfactual instruction following and the need for further research. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç°åœ¨è¶Šæ¥è¶Šå¹¿æ³›åœ°ç”¨äºæ¨¡æ‹Ÿè™šæ‹Ÿç¯å¢ƒä¸­çš„è§’è‰²ï¼Œåˆ©ç”¨å…¶éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°å³ä½¿æ˜¯æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¹Ÿæ— æ³•æ¨¡æ‹Ÿå…·æœ‰åå‘è¡¨ç°çš„è§’è‰²ï¼ˆä¾‹å¦‚åœ¨æ•™è‚²ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³çš„å­¦ç”Ÿè§’è‰²ï¼‰ï¼Œè¿™æŸå®³äº†æ¨¡æ‹Ÿçš„å¤šæ ·æ€§å¹¶é™åˆ¶äº†æ¨¡æ‹Ÿç¯å¢ƒçš„å®é™…åº”ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»¥æ•°å­¦æ¨ç†ä¸ºä»£è¡¨åœºæ™¯ï¼Œæå‡ºäº†è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡æ‹Ÿå…·æœ‰åå‘è¡¨ç°è§’è‰²çš„ç¬¬ä¸€ä¸ªåŸºå‡†æ•°æ®é›†ï¼Œæˆ‘ä»¬å°†è¿™ç§èƒ½åŠ›ç§°ä¸ºâ€œåäº‹å®æŒ‡ä»¤éµå¾ªâ€ã€‚æˆ‘ä»¬åœ¨æ­¤ä»»åŠ¡ä¸Šè¯„ä¼°äº†å¼€æ”¾æƒé‡å’Œå°é—­å¼çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‘ç°åŒ…æ‹¬OpenAI o1æ¨ç†æ¨¡å‹åœ¨å†…çš„å¤§å‹è¯­è¨€æ¨¡å‹éƒ½éš¾ä»¥éµå¾ªæ¨¡æ‹Ÿåå‘è¡¨ç°è§’è‰²çš„åäº‹å®æŒ‡ä»¤ã€‚åŒæ—¶æ¨¡æ‹Ÿè§’è‰²çš„è¡¨ç°æ°´å¹³å’Œç§æ—äººå£è¿›ä¸€æ­¥åŠ å‰§äº†è¿™ç§æ•ˆæœã€‚è¿™äº›ç»“æœçªå‡ºäº†åäº‹å®æŒ‡ä»¤éµå¾ªçš„æŒ‘æˆ˜å’Œè¿›ä¸€æ­¥ç ”ç©¶çš„å¿…è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06460v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMæ¨¡å‹è¢«å¹¿æ³›ç”¨äºæ¨¡æ‹Ÿè™šæ‹Ÿç¯å¢ƒä¸­çš„è§’è‰²ï¼Œä½†å…¶éš¾ä»¥æ¨¡æ‹Ÿåå‘è¡¨ç°çš„è§’è‰²ï¼Œé™åˆ¶äº†æ¨¡æ‹Ÿç¯å¢ƒçš„å¤šæ ·æ€§å’Œå®é™…åº”ç”¨ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ä¸ªè¯„ä¼°LLMæ¨¡æ‹Ÿåå‘è¡¨ç°è§’è‰²çš„èƒ½åŠ›çš„æ–°åŸºå‡†æ•°æ®é›†ï¼Œç§°ä¸ºâ€œåäº‹å®æŒ‡ä»¤è·Ÿéšâ€ã€‚è¯„ä¼°å‘ç°ï¼ŒåŒ…æ‹¬OpenAI o1æ¨ç†æ¨¡å‹åœ¨å†…çš„LLMéƒ½éš¾ä»¥éµå¾ªåäº‹å®æŒ‡ä»¤æ¨¡æ‹Ÿåå‘è¡¨ç°è§’è‰²ï¼Œä¸”åŒæ—¶æ¨¡æ‹Ÿè§’è‰²çš„è¡¨ç°æ°´å¹³å’Œç§æ—äººå£ç‰¹å¾ä¼šè¿›ä¸€æ­¥åŠ å‰§æ•ˆæœã€‚è¿™å‡¸æ˜¾äº†åäº‹å®æŒ‡ä»¤è·Ÿéšçš„æŒ‘æˆ˜å’Œè¿›ä¸€æ­¥ç ”ç©¶çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMæ¨¡å‹å¹¿æ³›ç”¨äºæ¨¡æ‹Ÿè™šæ‹Ÿç¯å¢ƒä¸­çš„è§’è‰²ã€‚</li>
<li>LLMéš¾ä»¥æ¨¡æ‹Ÿå…·æœ‰åå‘è¡¨ç°çš„è§’è‰²ï¼Œé™åˆ¶äº†æ¨¡æ‹Ÿç¯å¢ƒçš„å¤šæ ·æ€§å’Œå®é™…åº”ç”¨ã€‚</li>
<li>æå‡ºä¸€ä¸ªè¯„ä¼°LLMæ¨¡æ‹Ÿåå‘è¡¨ç°è§’è‰²çš„èƒ½åŠ›çš„æ–°åŸºå‡†æ•°æ®é›†ã€‚</li>
<li>åŒ…æ‹¬OpenAI o1æ¨ç†æ¨¡å‹åœ¨å†…çš„LLMåœ¨æ¨¡æ‹Ÿåå‘è¡¨ç°è§’è‰²æ—¶éš¾ä»¥éµå¾ªåäº‹å®æŒ‡ä»¤ã€‚</li>
<li>åŒæ—¶æ¨¡æ‹Ÿè§’è‰²çš„è¡¨ç°æ°´å¹³å’Œç§æ—äººå£ç‰¹å¾ä¼šè¿›ä¸€æ­¥åŠ å‰§è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>åäº‹å®æŒ‡ä»¤è·Ÿéšæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ï¼Œéœ€è¦è¿›ä¸€æ­¥æ¢ç´¢å’Œç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06460">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2504.06460v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2504.06460v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2504.06460v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="STADE-Standard-Deviation-as-a-Pruning-Metric"><a href="#STADE-Standard-Deviation-as-a-Pruning-Metric" class="headerlink" title="STADE: Standard Deviation as a Pruning Metric"></a>STADE: Standard Deviation as a Pruning Metric</h2><p><strong>Authors:Diego Coello de Portugal Mecke, Haya Alyoussef, Maximilian Stubbemann, Ilia Koloiarov, Tom Hanika, Lars Schmidt-Thieme</strong></p>
<p>Recently, Large Language Models (LLMs) have become very widespread and are used to solve a wide variety of tasks. To successfully handle these tasks, LLMs require longer training times and larger model sizes. This makes LLMs ideal candidates for pruning methods that reduce computational demands while maintaining performance. Previous methods require a retraining phase after pruning to maintain the original modelâ€™s performance. However, state-of-the-art pruning methods, such as Wanda, prune the model without retraining, making the pruning process faster and more efficient. Building upon Wandaâ€™s work, this study provides a theoretical explanation of why the method is effective and leverages these insights to enhance the pruning process. Specifically, a theoretical analysis of the pruning problem reveals a common scenario in Machine Learning where Wanda is the optimal pruning method. Furthermore, this analysis is extended to cases where Wanda is no longer optimal, leading to the development of a new method, STADE, based on the standard deviation of the input. From a theoretical standpoint, STADE demonstrates better generality across different scenarios. Finally, extensive experiments on Llama and Open Pre-trained Transformers (OPT) models validate these theoretical findings, showing that depending on the training conditions, Wandaâ€™s optimal performance varies as predicted by the theoretical framework. These insights contribute to a more robust understanding of pruning strategies and their practical implications. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/Coello-dev/STADE/">https://github.com/Coello-dev/STADE/</a> </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å˜å¾—éå¸¸æ™®éï¼Œå¹¶ç”¨äºè§£å†³å„ç§ä»»åŠ¡ã€‚ä¸ºäº†æˆåŠŸå¤„ç†è¿™äº›ä»»åŠ¡ï¼ŒLLMéœ€è¦æ›´é•¿çš„è®­ç»ƒæ—¶é—´å’Œæ›´å¤§çš„æ¨¡å‹å¤§å°ã€‚è¿™ä½¿å¾—LLMæˆä¸ºä¿®å‰ªæ–¹æ³•çš„ç†æƒ³å€™é€‰è€…ï¼Œè¿™äº›æ–¹æ³•å¯ä»¥åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å‡å°‘è®¡ç®—éœ€æ±‚ã€‚ä¹‹å‰çš„æ–¹æ³•éœ€è¦åœ¨ä¿®å‰ªåè¿›è¡Œé‡è®­ä»¥ä¿æŒåŸå§‹æ¨¡å‹çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæœ€æ–°é¢–çš„ä¿®å‰ªæ–¹æ³•ï¼Œå¦‚æ—ºè¾¾ï¼ˆWandaï¼‰ï¼Œæ— éœ€é‡è®­å³å¯ä¿®å‰ªæ¨¡å‹ï¼Œä½¿ä¿®å‰ªè¿‡ç¨‹æ›´å¿«ã€æ›´é«˜æ•ˆã€‚åŸºäºæ—ºè¾¾çš„å·¥ä½œï¼Œæœ¬ç ”ç©¶æä¾›äº†è¯¥æ–¹æ³•æœ‰æ•ˆçš„ç†è®ºè§£é‡Šï¼Œå¹¶åˆ©ç”¨è¿™äº›è§è§£æ¥æ”¹è¿›ä¿®å‰ªè¿‡ç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹ä¿®å‰ªé—®é¢˜çš„ç†è®ºåˆ†é€šè¿‡æœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªå¸¸è§æƒ…æ™¯æ­ç¤ºäº†æ—ºè¾¾ï¼ˆWandaï¼‰æˆä¸ºæœ€ä¼˜ä¿®å‰ªæ–¹æ³•çš„åŸå› ã€‚æ­¤å¤–ï¼Œè¯¥åˆ†ææ‰©å±•åˆ°äº†æ—ºè¾¾ä¸å†æ˜¯æœ€ä¼˜çš„æƒ…å†µï¼Œä»è€Œå¼€å‘å‡ºä¸€ç§æ–°çš„æ–¹æ³•â€”â€”STADEï¼Œè¯¥æ–¹æ³•åŸºäºè¾“å…¥çš„æ ‡å‡“å·®ã€‚ä»ç†è®ºè§’åº¦çœ‹ï¼ŒSTADEåœ¨ä¸åŒåœºæ™¯ä¸­å…·æœ‰æ›´å¥½çš„é€šç”¨æ€§ã€‚æœ€åï¼Œå¯¹Llamaå’Œå¼€æ”¾é¢„è®­ç»ƒè½¬æ¢å™¨ï¼ˆOPTï¼‰æ¨¡å‹çš„å¹¿æ³›å®éªŒéªŒè¯äº†è¿™äº›ç†è®ºå‘ç°ï¼Œè¡¨æ˜æ ¹æ®ä¸åŒçš„è®­ç»ƒæ¡ä»¶ï¼Œæ—ºè¾¾çš„æœ€ä½³æ€§èƒ½å¦‚ç†è®ºæ¡†æ¶æ‰€é¢„æµ‹çš„é‚£æ ·ä¼šæœ‰æ‰€å˜åŒ–ã€‚è¿™äº›è§è§£ä¸ºæ›´ç¨³å¥åœ°ç†è§£ä¿®å‰ªç­–ç•¥åŠå…¶å®é™…åº”ç”¨æä¾›äº†è´¡çŒ®ã€‚ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/Coello-dev/STADE/">https://github.com/Coello-dev/STADE/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22451v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMè®­ç»ƒæ—¶é—´é•¿ã€æ¨¡å‹è§„æ¨¡å¤§ï¼Œé€‚åˆé‡‡ç”¨å‰ªææ–¹æ³•é™ä½è®¡ç®—éœ€æ±‚å¹¶ä¿æŒæ€§èƒ½ã€‚æœ€æ–°å‰ªææ–¹æ³•å¦‚Wandaæ— éœ€é‡è®­ï¼Œæå‡å‰ªææ•ˆç‡ã€‚æœ¬ç ”ç©¶ä»ç†è®ºè§’åº¦è§£é‡Šäº†Wandaæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ®æ­¤ä¼˜åŒ–äº†å‰ªæè¿‡ç¨‹ã€‚åŒæ—¶æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºè¾“å…¥æ ‡å‡†å·®çš„æ–¹æ³•STADEï¼Œå…¶è¡¨ç°è¾ƒä¼˜äºWandaåœ¨æŸäº›æƒ…å†µä¸‹çš„é€šç”¨æ€§ã€‚å®éªŒéªŒè¯æ˜¾ç¤ºï¼Œç†è®ºæ¡†æ¶é¢„æµ‹äº†Wandaåœ¨ä¸åŒè®­ç»ƒæ¡ä»¶ä¸‹çš„æœ€ä½³æ€§èƒ½ã€‚è¿™äº›è§è§£æœ‰åŠ©äºæ›´æ·±å…¥åœ°ç†è§£å‰ªæç­–ç•¥åŠå…¶å®é™…åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMå› å…¶é•¿æ—¶é—´è®­ç»ƒå’Œå¤§è§„æ¨¡æ¨¡å‹ç‰¹ç‚¹ï¼Œé€‚åˆä½¿ç”¨å‰ªææ–¹æ³•æ¥ä¼˜åŒ–è®¡ç®—éœ€æ±‚å’Œæ€§èƒ½ä¿æŒã€‚</li>
<li>æœ€æ–°å‰ªææ–¹æ³•å¦‚Wandaæ— éœ€é‡è®­é˜¶æ®µï¼Œæå‡äº†å‰ªæè¿‡ç¨‹çš„æ•ˆç‡å’Œé€Ÿåº¦ã€‚</li>
<li>æœ¬ç ”ç©¶æä¾›äº†å¯¹Wandaæ–¹æ³•æœ‰æ•ˆæ€§çš„ç†è®ºè§£é‡Šï¼Œå¹¶åŸºäºè¿™äº›æ´å¯Ÿä¼˜åŒ–äº†å‰ªæè¿‡ç¨‹ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å‰ªææ–¹æ³•STADEï¼Œå…¶åœ¨ä¸åŒæƒ…å¢ƒä¸‹çš„é€šç”¨æ€§è¡¨ç°ä¼˜äºWandaã€‚</li>
<li>STADEçš„ç†è®ºåˆ†ææ‰©å±•äº†å¯¹å‰ªæé—®é¢˜çš„ç†è§£ã€‚</li>
<li>å®éªŒç»“æœéªŒè¯äº†ç†è®ºæ¡†æ¶çš„é¢„æµ‹ï¼Œè¡¨æ˜Wandaçš„æœ€ä½³æ€§èƒ½å–å†³äºè®­ç»ƒæ¡ä»¶ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22451">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.22451v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.22451v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.22451v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.22451v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.22451v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.22451v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Instruction-Oriented-Preference-Alignment-for-Enhancing-Multi-Modal-Comprehension-Capability-of-MLLMs"><a href="#Instruction-Oriented-Preference-Alignment-for-Enhancing-Multi-Modal-Comprehension-Capability-of-MLLMs" class="headerlink" title="Instruction-Oriented Preference Alignment for Enhancing Multi-Modal   Comprehension Capability of MLLMs"></a>Instruction-Oriented Preference Alignment for Enhancing Multi-Modal   Comprehension Capability of MLLMs</h2><p><strong>Authors:Zitian Wang, Yue Liao, Kang Rong, Fengyun Rao, Yibo Yang, Si Liu</strong></p>
<p>Preference alignment has emerged as an effective strategy to enhance the performance of Multimodal Large Language Models (MLLMs) following supervised fine-tuning. While existing preference alignment methods predominantly target hallucination factors, they overlook the factors essential for multi-modal comprehension capabilities, often narrowing their improvements on hallucination mitigation. To bridge this gap, we propose Instruction-oriented Preference Alignment (IPA), a scalable framework designed to automatically construct alignment preferences grounded in instruction fulfillment efficacy. Our method involves an automated preference construction coupled with a dedicated verification process that identifies instruction-oriented factors, avoiding significant variability in response representations. Additionally, IPA incorporates a progressive preference collection pipeline, further recalling challenging samples through model self-evolution and reference-guided refinement. Experiments conducted on Qwen2VL-7B demonstrate IPAâ€™s effectiveness across multiple benchmarks, including hallucination evaluation, visual question answering, and text understanding tasks, highlighting its capability to enhance general comprehension. </p>
<blockquote>
<p>åå¥½å¯¹é½ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥ï¼Œåœ¨ç›‘ç£å¾®è°ƒåç”¨äºæé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ€§èƒ½ã€‚å°½ç®¡ç°æœ‰çš„åå¥½å¯¹é½æ–¹æ³•ä¸»è¦ç€çœ¼äºå¹»è§‰å› ç´ ï¼Œä½†å®ƒä»¬å¿½è§†äº†å¤šæ¨¡æ€ç†è§£èƒ½åŠ›æ‰€å¿…éœ€çš„å› ç´ ï¼Œé€šå¸¸ä»…åœ¨ç¼“è§£å¹»è§‰æ–¹é¢æœ‰æ‰€æ”¹å–„ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘æŒ‡ä»¤çš„åå¥½å¯¹é½ï¼ˆIPAï¼‰è¿™ä¸€å¯æ‰©å±•æ¡†æ¶ï¼Œæ—¨åœ¨æ ¹æ®æŒ‡ä»¤æ‰§è¡Œæ•ˆç‡è‡ªåŠ¨æ„å»ºåå¥½å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬è‡ªåŠ¨åŒ–åå¥½æ„å»ºä»¥åŠä¸ä¸“ç”¨éªŒè¯è¿‡ç¨‹çš„ç»“åˆï¼Œå¯è¯†åˆ«é¢å‘æŒ‡ä»¤çš„å› ç´ ï¼Œé¿å…å“åº”è¡¨ç¤ºä¸­çš„é‡å¤§å·®å¼‚ã€‚æ­¤å¤–ï¼ŒIPAè¿˜é‡‡ç”¨æ¸è¿›çš„åå¥½æ”¶é›†ç®¡é“ï¼Œé€šè¿‡æ¨¡å‹è‡ªæˆ‘è¿›åŒ–ä»¥åŠå‚è€ƒå¼•å¯¼ç»†åŒ–æ¥è¿›ä¸€æ­¥å›å¿†å…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬ã€‚åœ¨Qwen2VL-7Bä¸Šè¿›è¡Œçš„å®éªŒè¯æ˜äº†IPAåœ¨å¤šåŸºå‡†æµ‹è¯•ä¸­çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬å¹»è§‰è¯„ä¼°ã€è§†è§‰é—®ç­”å’Œæ–‡æœ¬ç†è§£ä»»åŠ¡ç­‰ï¼Œçªæ˜¾å…¶åœ¨æé«˜æ•´ä½“ç†è§£èƒ½åŠ›æ–¹é¢çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20309v2">PDF</a> Accepted by ICCV 2025</p>
<p><strong>æ‘˜è¦</strong><br>    æå‡ºäº†ä¸€ç§åä¸ºæŒ‡ä»¤å¯¼å‘åå¥½å¯¹é½ï¼ˆIPAï¼‰çš„å¯æ‰©å±•æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨æ„å»ºåŸºäºæŒ‡ä»¤æ‰§è¡Œæ•ˆç‡çš„å¯¹é½åå¥½ï¼Œä»¥æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ€§èƒ½ã€‚IPAé€šè¿‡è‡ªåŠ¨åŒ–åå¥½æ„å»ºå’Œä¸“ç”¨éªŒè¯è¿‡ç¨‹ï¼Œè¯†åˆ«æŒ‡ä»¤å¯¼å‘å› ç´ ï¼Œé¿å…å“åº”è¡¨ç¤ºçš„æ˜¾è‘—å˜åŒ–ã€‚æ­¤å¤–ï¼ŒIPAè¿˜é‡‡ç”¨æ¸è¿›å¼åå¥½æ”¶é›†ç®¡é“ï¼Œé€šè¿‡æ¨¡å‹è‡ªæˆ‘è¿›åŒ–å‚è€ƒå¼•å¯¼ç»†åŒ–æ¥è¿›ä¸€æ­¥å›å¿†æŒ‘æˆ˜æ ·æœ¬ã€‚å®éªŒè¡¨æ˜ï¼ŒIPAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºæœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ï¼ŒåŒ…æ‹¬å¹»è§†è¯„ä¼°ã€è§†è§‰é—®ç­”å’Œæ–‡æœ¬ç†è§£ä»»åŠ¡ï¼Œçªæ˜¾å…¶åœ¨æé«˜æ•´ä½“ç†è§£èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åå¥½å¯¹é½å·²è¢«è¯æ˜æ˜¯å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æœ‰æ•ˆç­–ç•¥ã€‚</li>
<li>ç°æœ‰åå¥½å¯¹é½æ–¹æ³•ä¸»è¦å…³æ³¨å¹»è§†å› ç´ ï¼Œå¿½ç•¥äº†å¤šæ¨¡æ€ç†è§£èƒ½åŠ›çš„å…³é”®å› ç´ ã€‚</li>
<li>æŒ‡ä»¤å¯¼å‘åå¥½å¯¹é½ï¼ˆIPAï¼‰æ¡†æ¶æ—¨åœ¨è‡ªåŠ¨æ„å»ºåŸºäºæŒ‡ä»¤æ‰§è¡Œæ•ˆç‡çš„å¯¹é½åå¥½ï¼Œä»¥å¼¥è¡¥ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚</li>
<li>IPAé€šè¿‡è‡ªåŠ¨åŒ–åå¥½æ„å»ºå’Œä¸“ç”¨éªŒè¯è¿‡ç¨‹ï¼Œæœ‰æ•ˆè¯†åˆ«æŒ‡ä»¤å¯¼å‘å› ç´ ï¼Œç¡®ä¿æ¨¡å‹å“åº”çš„ç¨³å®šæ€§ã€‚</li>
<li>IPAé‡‡ç”¨æ¸è¿›å¼åå¥½æ”¶é›†ç®¡é“ï¼Œé€šè¿‡æ¨¡å‹è‡ªæˆ‘è¿›åŒ–å‚è€ƒå¼•å¯¼ç»†åŒ–ï¼Œèƒ½å¤Ÿè¿›ä¸€æ­¥å›å¿†å¹¶å¤„ç†æŒ‘æˆ˜æ ·æœ¬ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒIPAåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬å¹»è§†è¯„ä¼°ã€è§†è§‰é—®ç­”å’Œæ–‡æœ¬ç†è§£ã€‚</li>
<li>IPAæé«˜äº†æ¨¡å‹çš„æ•´ä½“ç†è§£èƒ½åŠ›ï¼Œä¸ºæœªæ¥å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å‘å±•æä¾›äº†æ–°æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20309">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.20309v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.20309v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.20309v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.20309v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2503.20309v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="InteLiPlan-An-Interactive-Lightweight-LLM-Based-Planner-for-Domestic-Robot-Autonomy"><a href="#InteLiPlan-An-Interactive-Lightweight-LLM-Based-Planner-for-Domestic-Robot-Autonomy" class="headerlink" title="InteLiPlan: An Interactive Lightweight LLM-Based Planner for Domestic   Robot Autonomy"></a>InteLiPlan: An Interactive Lightweight LLM-Based Planner for Domestic   Robot Autonomy</h2><p><strong>Authors:Kim Tien Ly, Kai Lu, Ioannis Havoutis</strong></p>
<p>We introduce an interactive LLM-based framework designed to enhance the autonomy and robustness of domestic robots, targeting embodied intelligence. Our approach reduces reliance on large-scale data and incorporates a robot-agnostic pipeline that embodies an LLM. Our framework, InteLiPlan, ensures that the LLMâ€™s decision-making capabilities are effectively aligned with robotic functions, enhancing operational robustness and adaptability, while our human-in-the-loop mechanism allows for real-time human intervention when user instruction is required. We evaluate our method in both simulation and on the real Toyota Human Support Robot and Anymal D-Unitree Z1 platforms. Our method achieves a 95% success rate in the â€˜fetch meâ€™ task completion with failure recovery, highlighting its capability in both failure reasoning and task planning. InteLiPlan achieves comparable performance to state-of-the-art large-scale LLM-based robotics planners, while using only real-time onboard computing. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„äº¤äº’å¼æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å®¶ç”¨æœºå™¨äººçš„è‡ªä¸»æ€§å’Œç¨³å¥æ€§ï¼Œä¸»è¦é¢å‘ä½“ç°æ™ºèƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•å‡å°‘å¯¹å¤§è§„æ¨¡æ•°æ®çš„ä¾èµ–ï¼Œå¹¶èå…¥ä¸€ä¸ªé€šç”¨çš„æœºå™¨äººç®¡é“ï¼Œå…¶ä¸­åŒ…å«äº†å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¡†æ¶InteLiPlanç¡®ä¿å¤§å‹è¯­è¨€æ¨¡å‹çš„å†³ç­–èƒ½åŠ›ä¸æœºå™¨äººåŠŸèƒ½æœ‰æ•ˆå¯¹é½ï¼Œæé«˜æ“ä½œç¨³å¥æ€§å’Œé€‚åº”æ€§ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„äººæœºäº¤äº’æœºåˆ¶åœ¨ç”¨æˆ·éœ€è¦æŒ‡ä»¤æ—¶å…è®¸å®æ—¶äººå·¥å¹²é¢„ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿç¯å¢ƒå’ŒçœŸå®çš„ä¸°ç”°äººæœºæ”¯æŒæœºå™¨äººä»¥åŠAnymal D-Unitree Z1å¹³å°ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨â€œå¸®æˆ‘å–ä¸œè¥¿â€çš„ä»»åŠ¡å®Œæˆä¸­å®ç°äº†95%çš„æˆåŠŸç‡ï¼Œå¹¶åœ¨å¤±è´¥æ¢å¤ä¸­è¡¨ç°å‡ºè‰²ï¼Œçªæ˜¾äº†å…¶åœ¨æ•…éšœæ¨ç†å’Œä»»åŠ¡è§„åˆ’æ–¹é¢çš„èƒ½åŠ›ã€‚InteLiPlanä¸æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹æœºå™¨äººè§„åˆ’å™¨ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶ä»…ä½¿ç”¨å®æ—¶æœºè½½è®¡ç®—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.14506v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåŸºäºLLMçš„äº¤äº’å¼æ¡†æ¶InteLiPlanï¼Œæ—¨åœ¨æé«˜å®¶ç”¨æœºå™¨äººçš„è‡ªä¸»æ€§å’Œç¨³å¥æ€§ï¼Œä¸»è¦ç›®æ ‡æ˜¯å®ç°æœºå™¨äººæ™ºèƒ½ã€‚è¯¥æ–¹æ³•å‡å°‘äº†å¤§è§„æ¨¡æ•°æ®çš„ä¾èµ–ï¼Œå¹¶èå…¥äº†LLMçš„æœºå™¨äººé€šç”¨ç®¡é“ã€‚InteLiPlanç¡®ä¿LLMçš„å†³ç­–èƒ½åŠ›ä¸æœºå™¨äººåŠŸèƒ½æœ‰æ•ˆå¯¹é½ï¼Œæé«˜äº†æ“ä½œç¨³å¥æ€§å’Œé€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œå…¶äººç±»ä»‹å…¥æœºåˆ¶å…è®¸åœ¨ç”¨æˆ·æŒ‡å¯¼éœ€è¦æ—¶å®æ—¶ä»‹å…¥ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸°ç”°äººç±»æ”¯æŒæœºå™¨äººä»¥åŠAnymal D-Unitree Z1å¹³å°ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå®Œæˆâ€œè¯·é€’ç»™æˆ‘â€ä»»åŠ¡çš„æˆåŠŸç‡è¾¾åˆ°äº†95%ï¼Œå±•ç°å‡ºåœ¨å¤±è´¥æ¢å¤ã€æ•…éšœæ¨ç†å’Œä»»åŠ¡è§„åˆ’æ–¹é¢çš„èƒ½åŠ›ã€‚InteLiPlanåœ¨ä¸å½“å‰å¤§å‹LLMåŸºäºæœºå™¨äººçš„è§„åˆ’å™¨ç›¸å½“çš„æ€§èƒ½è¡¨ç°ä¸­å–å¾—äº†ä¼˜è¶Šçš„è¡¨ç°ï¼Œä½†åªä½¿ç”¨äº†å®æ—¶çš„æœºä¸Šè®¡ç®—èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å°†ä¸ºä¸‹ä¸€ä»£æœºå™¨äººçš„å‘å±•æä¾›å…³é”®æ€è·¯å’Œæ–¹æ³•æ”¯æŒã€‚<br>  <strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä»‹ç»äº†åŸºäºLLMçš„äº¤äº’å¼æ¡†æ¶InteLiPlanï¼Œç”¨äºæé«˜å®¶ç”¨æœºå™¨äººçš„è‡ªä¸»æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶èå…¥äº†ä¸€ç§æœºå™¨äººé€šç”¨ç®¡é“ç»“æ„ï¼ŒåŒ…å«äº†LLMæ¨¡å‹å…ƒç´ å’ŒåŠŸèƒ½ç‰¹æ€§ä¼˜åŒ–å…ƒç´ ç»„åˆæ„æˆçš„æŒ‡ä»¤å¯¹é½ä¸æ ¡æ­£æ¨¡å¼ç”Ÿæˆå¼•æ“ä»¥æ‰§è¡Œä»»åŠ¡è¡Œä¸ºå­¦ä¹ ç»“æœçš„åŠ¨ä½œæ–¹æ¡ˆç­–ç•¥ç”Ÿæˆæ¨¡å—ã€‚è¿™ç§è®¾è®¡æé«˜äº†æœºå™¨äººæ“ä½œçš„ç¨³å¥æ€§å’Œé€‚åº”æ€§ã€‚åŒæ—¶å‡å°‘äº†å¤§è§„æ¨¡æ•°æ®çš„ä¾èµ–ã€‚é€šè¿‡è¯¥æ¡†æ¶ï¼Œæœºå™¨äººå¯ä»¥æ›´åŠ æ™ºèƒ½åœ°å®Œæˆä»»åŠ¡ï¼Œå‡å°‘äººä¸ºå¹²é¢„çš„éœ€æ±‚ã€‚ </li>
<li>InteLiPlanå…è®¸å®æ—¶çš„äººç±»ä»‹å…¥æœºåˆ¶åœ¨ç”¨æˆ·æŒ‡å¯¼éœ€è¦æ—¶å‘æŒ¥ä½œç”¨ï¼Œç¡®ä¿äº†ç³»ç»Ÿçš„çµæ´»æ€§å’Œé«˜æ•ˆæ€§ã€‚æ­¤å¤–è¿˜å…·å¤‡è‡ªä¸»å†³ç­–èƒ½åŠ›ã€‚æœºå™¨äººå¯ä»¥åœ¨æ‰§è¡Œä»»åŠ¡è¿‡ç¨‹ä¸­æ ¹æ®ç¯å¢ƒå˜åŒ–åšå‡ºåˆ¤æ–­å’Œè°ƒæ•´ï¼Œæé«˜ä»»åŠ¡çš„å®Œæˆç‡å’Œæ•ˆç‡ã€‚è¿™ç§æœºåˆ¶ç¡®ä¿äº†ç”¨æˆ·ä¸æœºå™¨äººä¹‹é—´çš„é¡ºç•…äº¤æµï¼Œæé«˜äº†ç”¨æˆ·ä½“éªŒã€‚ </li>
<li>é€šè¿‡æ¨¡æ‹Ÿå’Œå®é™…å®éªŒéªŒè¯å‘ç°ï¼Œè¯¥æ¡†æ¶çš„æ€§èƒ½ä¼˜è¶Šè¡¨ç°åœ¨æ™ºèƒ½æ„ŸçŸ¥å¤„ç†åœºæ™¯çš„åˆ†æä»»åŠ¡å’Œè¡Œä¸ºèƒ½åŠ›åŒ¹é…å­¦ä¹ æ¨¡å‹ä¸­è¯„ä¼°æ ‡å‡†çš„æˆç»©å…·æœ‰é¢†å…ˆåœ°ä½èƒ½å¤Ÿå®ç°95%çš„æˆåŠŸç‡å®Œæˆâ€œè¯·é€’ç»™æˆ‘â€ä»»åŠ¡å¹¶å±•ç°å‡ºæ•…éšœæ¢å¤èƒ½åŠ›ã€‚è¿™è¡¨æ˜è¯¥æ¡†æ¶åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰é«˜åº¦çš„å¯é æ€§å’Œç¨³å®šæ€§ã€‚ </li>
<li>InteLiPlançš„æ€§èƒ½è¡¨ç°ä¸å½“å‰çš„å…ˆè¿›çš„å¤§å‹LLMåŸºäºæœºå™¨äººçš„è§„åˆ’å™¨ç›¸æ¯”é¢‡å…·ç«äº‰åŠ›åŒæ—¶å…·æœ‰å¿«é€Ÿçš„è¿ç®—å¤„ç†å“åº”å®æ—¶é…åˆä»¥å¸¸è§„æ›´æ–°ç³»ç»Ÿè¿è¡Œæ–°å‡ºæ–‡æ¡£èŠ‚ç‚¹å½’çº³é”™è¯¯ç‰¹å¾å’Œå®Œå¤‡æ˜ å°„å¤„ç½®æ–‡ä¹¦å¤æ‚æ€§ç­‰ç‰¹æ€§ã€‚è¿™äº›ç‰¹æ€§ä½¿å¾—InteLiPlanåœ¨å®é™…åº”ç”¨ä¸­æ›´åŠ çµæ´»é«˜æ•ˆå¯é èƒ½å¤Ÿæ»¡è¶³å„ç§å¤æ‚åœºæ™¯çš„éœ€æ±‚ã€‚ </li>
<li>InteLiPlanæ¡†æ¶çš„è®¾è®¡æ€æƒ³å…·æœ‰åˆ›æ–°æ€§èƒ½å¤Ÿä¸ºä¸‹ä¸€ä»£æœºå™¨äººçš„å‘å±•æä¾›å…³é”®æ€è·¯å’Œæ–¹æ³•æ”¯æŒæ¨åŠ¨äº†äººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•å¹¶å±•ç°å‡ºå¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚å…¶çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ä½¿å¾—è¯¥æ¡†æ¶èƒ½å¤Ÿé€‚åº”æœªæ¥æœºå™¨äººæŠ€æœ¯çš„å¿«é€Ÿå‘å±•å¹¶æ»¡è¶³ä¸æ–­å˜åŒ–çš„å¸‚åœºéœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.14506">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2409.14506v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2409.14506v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2409.14506v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2409.14506v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2409.14506v3/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2409.14506v3/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2409.14506v3/page_5_2.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Bitune-Leveraging-Bidirectional-Attention-to-Improve-Decoder-Only-LLMs"><a href="#Bitune-Leveraging-Bidirectional-Attention-to-Improve-Decoder-Only-LLMs" class="headerlink" title="Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs"></a>Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs</h2><p><strong>Authors:Dawid J. Kopiczko, Tijmen Blankevoort, Yuki M. Asano</strong></p>
<p>Decoder-only large language models typically rely solely on masked causal attention, which limits their expressiveness by restricting information flow to one direction. We propose Bitune, a method that enhances pretrained decoder-only LLMs by incorporating bidirectional attention into prompt processing. We evaluate Bitune in instruction-tuning and question-answering settings, showing significant improvements in performance on commonsense reasoning, arithmetic, and language understanding tasks. Furthermore, extensive ablation studies validate the role of each component of the method, and demonstrate that Bitune is compatible with various parameter-efficient finetuning techniques and full model finetuning. </p>
<blockquote>
<p>è§£ç å™¨ä»…å¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸ä»…ä¾èµ–äºæ©ç å› æœæ³¨æ„åŠ›ï¼Œè¿™å°†ä¿¡æ¯æµåŠ¨é™åˆ¶åœ¨ä¸€ä¸ªæ–¹å‘ä¸Šï¼Œä»è€Œé™åˆ¶äº†å…¶è¡¨ç°åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†Bituneæ–¹æ³•ï¼Œå®ƒé€šè¿‡å°†åœ¨æç¤ºå¤„ç†ä¸­èå…¥åŒå‘æ³¨æ„åŠ›ï¼Œå¢å¼ºäº†é¢„è®­ç»ƒè§£ç å™¨ä»…å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠŸèƒ½ã€‚æˆ‘ä»¬åœ¨æŒ‡ä»¤è°ƒæ•´å’Œé—®ç­”ç¯å¢ƒä¸­è¯„ä¼°äº†Bituneï¼Œåœ¨å¸¸è¯†æ¨ç†ã€ç®—æœ¯å’Œè¯­è¨€ç†è§£ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¡¨ç°å‡ºæ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œå¹¿æ³›çš„æ¶ˆèç ”ç©¶éªŒè¯äº†è¯¥æ–¹æ³•å„ç»„æˆéƒ¨åˆ†çš„ä½œç”¨ï¼Œå¹¶è¯æ˜Bituneä¸å„ç§å‚æ•°æœ‰æ•ˆçš„å¾®è°ƒæŠ€æœ¯å’Œå…¨æ¨¡å‹å¾®è°ƒå…¼å®¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14862v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è§£ç å™¨ä»…å¤§å‹è¯­è¨€æ¨¡å‹é€šå¸¸ä»…ä¾èµ–äºæ©ç å› æœæ³¨æ„åŠ›ï¼Œè¿™é™åˆ¶äº†å…¶è¡¨ç°åŠ›ï¼Œå› ä¸ºä¿¡æ¯æµå‘è¢«é™åˆ¶åœ¨ä¸€ä¸ªæ–¹å‘ä¸Šã€‚æˆ‘ä»¬æå‡ºäº†Bituneæ–¹æ³•ï¼Œå®ƒé€šè¿‡å¼•å…¥åŒå‘æ³¨æ„åŠ›æ¥æå‡é¢„è®­ç»ƒçš„è§£ç å™¨ä»…LLMsåœ¨æç¤ºå¤„ç†æ–¹é¢çš„æ€§èƒ½ã€‚åœ¨æŒ‡ä»¤è°ƒæ•´å’Œé—®ç­”è®¾ç½®ä¸­è¯„ä¼°Bituneæ—¶ï¼Œæ˜¾ç¤ºå…¶åœ¨å¸¸è¯†æ¨ç†ã€ç®—æœ¯å’Œè¯­è¨€ç†è§£ä»»åŠ¡ä¸Šçš„æ€§èƒ½æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼Œå¹¿æ³›çš„æ¶ˆèç ”ç©¶éªŒè¯äº†è¯¥æ–¹æ³•å„ç»„ä»¶çš„ä½œç”¨ï¼Œå¹¶è¯æ˜Bituneå¯ä»¥ä¸å„ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯å’Œå…¨æ¨¡å‹å¾®è°ƒå…¼å®¹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è§£ç å™¨ä»…å¤§å‹è¯­è¨€æ¨¡å‹å—é™äºæ©ç å› æœæ³¨æ„åŠ›ï¼Œä¿¡æ¯æµå‘å•ä¸€ã€‚</li>
<li>Bituneæ–¹æ³•é€šè¿‡å¼•å…¥åŒå‘æ³¨æ„åŠ›æå‡é¢„è®­ç»ƒè§£ç å™¨ä»…LLMsçš„æ€§èƒ½ã€‚</li>
<li>Bituneåœ¨æŒ‡ä»¤è°ƒæ•´å’Œé—®ç­”è®¾ç½®ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¸¸è¯†æ¨ç†ã€ç®—æœ¯å’Œè¯­è¨€ç†è§£ä»»åŠ¡ä¸Šã€‚</li>
<li>æ¶ˆèç ”ç©¶éªŒè¯äº†Bituneæ–¹æ³•å„ç»„ä»¶çš„ä½œç”¨ã€‚</li>
<li>Bituneä¸å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯å’Œå…¨æ¨¡å‹å¾®è°ƒå…¼å®¹ã€‚</li>
<li>Bituneæ–¹æ³•å¯èƒ½æœ‰åŠ©äºè§£å†³è§£ç å™¨ä»…LLMsçš„è¡¨è¾¾å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14862">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2405.14862v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2405.14862v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2405.14862v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2405.14862v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2405.14862v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Why-Not-Transform-Chat-Large-Language-Models-to-Non-English"><a href="#Why-Not-Transform-Chat-Large-Language-Models-to-Non-English" class="headerlink" title="Why Not Transform Chat Large Language Models to Non-English?"></a>Why Not Transform Chat Large Language Models to Non-English?</h2><p><strong>Authors:Xiang Geng, Ming Zhu, Jiahuan Li, Zhejian Lai, Wei Zou, Shuaijie She, Jiaxin Guo, Xiaofeng Zhao, Yinglu Li, Yuang Li, Chang Su, Yanqing Zhao, Xinglin Lyu, Min Zhang, Jiajun Chen, Hao Yang, Shujian Huang</strong></p>
<p>The scarcity of non-English data limits the development of non-English large language models (LLMs). Transforming English-centric LLMs to non-English has been identified as an effective and resource-efficient method. Previous works start from base LLMs and perform knowledge distillation (KD) with data generated by stronger LLMs, e.g. GPT-4. Compared to base LLMs, chat LLMs are further optimized for advanced abilities, e.g. multi-turn conversation and human preference alignment, and thus more powerful in both helpfulness and safety. However, transforming a chat LLM involves two critical issues: (1) How can we effectively transfer advanced abilities without their supervised data? (2) How can we prevent the original knowledge from catastrophic forgetting during transformation? We target these issues by introducing a simple framework called TransLLM. For the first issue, TransLLM divides the transfer problem into some common sub-tasks with the translation chain-of-thought, which uses the translation as the bridge between English and non-English step-by-step. We further enhance the performance of sub-tasks with publicly available data. For the second issue, we propose a method comprising two synergistic components: low-rank adaptation for training to maintain the original LLM parameters, and recovery KD, which utilizes data generated by the chat LLM itself to recover the original knowledge from the frozen parameters. In the experiments, we transform the LLaMA-2-chat-7B to the Thai language. Our method, using only single-turn data, outperforms strong baselines and ChatGPT on multi-turn benchmark MT-bench. Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4. Code is available at <a target="_blank" rel="noopener" href="https://github.com/hy5468/TransLLM">https://github.com/hy5468/TransLLM</a>. </p>
<blockquote>
<p>éè‹±è¯­æ•°æ®çš„ç¨€ç¼ºé™åˆ¶äº†éè‹±è¯­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ã€‚å°†è‹±è¯­ä¸­å¿ƒä¸»ä¹‰çš„LLMè½¬å˜ä¸ºéè‹±è¯­å·²è¢«è¯æ˜æ˜¯ä¸€ç§æœ‰æ•ˆä¸”èµ„æºé«˜æ•ˆçš„æ–¹æ³•ã€‚ä»¥å‰çš„å·¥ä½œä»åŸºç¡€LLMå¼€å§‹ï¼Œä½¿ç”¨ç”±æ›´å¼ºå¤§çš„LLMï¼ˆä¾‹å¦‚GPT-4ï¼‰ç”Ÿæˆçš„æ•°æ®è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰ã€‚ä¸åŸºç¡€LLMç›¸æ¯”ï¼ŒèŠå¤©LLMé’ˆå¯¹é«˜çº§èƒ½åŠ›è¿›è¡Œäº†è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œä¾‹å¦‚å¤šè½®å¯¹è¯å’Œäººç±»åå¥½å¯¹é½ï¼Œå› æ­¤åœ¨æœ‰ç”¨æ€§å’Œå®‰å…¨æ€§æ–¹é¢éƒ½æ›´åŠ å¼ºå¤§ã€‚ç„¶è€Œï¼Œå°†èŠå¤©LLMè½¬æ¢æ¶‰åŠä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šï¼ˆ1ï¼‰å¦‚ä½•åœ¨æ²¡æœ‰ç›‘ç£æ•°æ®çš„æƒ…å†µä¸‹æœ‰æ•ˆåœ°è½¬ç§»é«˜çº§èƒ½åŠ›ï¼Ÿï¼ˆ2ï¼‰å¦‚ä½•åœ¨è½¬æ¢è¿‡ç¨‹ä¸­é˜²æ­¢åŸæœ‰çŸ¥è¯†çš„ç¾éš¾æ€§é—å¿˜ï¼Ÿæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªåä¸ºTransLLMçš„ç®€å•æ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å¯¹äºç¬¬ä¸€ä¸ªé—®é¢˜ï¼ŒTransLLMå°†è½¬ç§»é—®é¢˜åˆ†è§£ä¸ºä¸€äº›å…·æœ‰ç¿»è¯‘æ€ç»´é“¾çš„å…±åŒå­ä»»åŠ¡ï¼Œåˆ©ç”¨ç¿»è¯‘ä½œä¸ºè‹±è¯­å’Œéè‹±è¯­ä¹‹é—´çš„æ¡¥æ¢ï¼Œé€æ­¥è¿›è¡Œã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨å…¬å¼€æ•°æ®æé«˜äº†å­ä»»åŠ¡çš„æ€§èƒ½ã€‚å¯¹äºç¬¬äºŒä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”±ä¸¤ä¸ªååŒç»„ä»¶ç»„æˆçš„æ–¹æ³•ï¼šä½ç§©é€‚åº”è®­ç»ƒä»¥ä¿æŒåŸå§‹LLMå‚æ•°ï¼Œä»¥åŠæ¢å¤KDï¼Œåè€…åˆ©ç”¨èŠå¤©LLMæœ¬èº«ç”Ÿæˆçš„æ•°æ®ä»å†»ç»“çš„å‚æ•°ä¸­æ¢å¤åŸå§‹çŸ¥è¯†ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬å°†LLaMA-2-chat-7Bè½¬å˜ä¸ºæ³°è¯­ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»…ä½¿ç”¨å•è½®æ•°æ®ï¼Œåœ¨å¤šè½®åŸºå‡†æµ‹è¯•MT-benchä¸Šçš„è¡¨ç°ä¼˜äºå¼ºå¤§çš„åŸºå‡†æµ‹è¯•å’ŒChatGPTã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ²¡æœ‰å®‰å…¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œåœ¨å®‰å…¨åŸºå‡†æµ‹è¯•AdvBenchä¸­æ‹’ç»æ›´å¤šæœ‰å®³çš„æŸ¥è¯¢æ¯”ChatGPTå’ŒGPT-4æ›´å¤šã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hy5468/TransLLM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hy5468/TransLLMè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.13923v3">PDF</a> The article has been accepted by Frontiers of Computer Science (FCS),   with the DOI: {10.1007&#x2F;s11704-025-50646-z}</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†éè‹±è¯­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‘å±•çš„å±€é™æ€§ï¼Œå¹¶æŒ‡å‡ºå°†è‹±è¯­ä¸­å¿ƒä¸»ä¹‰çš„LLMè½¬å‹ä¸ºéè‹±è¯­LLMæ˜¯ä¸€ç§æœ‰æ•ˆä¸”èµ„æºé«˜æ•ˆçš„æ–¹æ³•ã€‚æ–‡ç« ä»‹ç»äº†ä¸€ä¸ªåä¸ºTransLLMçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨è§£å†³å°†èŠå¤©LLMè½¬å‹ä¸ºéè‹±è¯­LLMæ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¦‚ä½•æœ‰æ•ˆè½¬ç§»é«˜çº§èƒ½åŠ›ä»¥åŠå¦‚ä½•åœ¨è½¬å‹è¿‡ç¨‹ä¸­é˜²æ­¢åŸæœ‰çŸ¥è¯†ç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”èƒ½æœ‰æ•ˆæé«˜å®‰å…¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éè‹±è¯­æ•°æ®ç¨€ç¼ºé™åˆ¶äº†éè‹±è¯­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•ã€‚</li>
<li>å°†è‹±è¯­ä¸­å¿ƒçš„LLMè½¬å‹ä¸ºéè‹±è¯­LLMæ˜¯ä¸€ç§æœ‰æ•ˆä¸”èµ„æºé«˜æ•ˆçš„æ–¹æ³•ã€‚</li>
<li>TransLLMæ¡†æ¶æ—¨åœ¨è§£å†³èŠå¤©LLMè½¬å‹ä¸­çš„ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šæœ‰æ•ˆè½¬ç§»é«˜çº§èƒ½åŠ›å’Œé˜²æ­¢åŸæœ‰çŸ¥è¯†çš„ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>TransLLMé€šè¿‡ç¿»è¯‘æ€ç»´é“¾å°†è½¬ç§»é—®é¢˜åˆ†ä¸ºä¸€äº›é€šç”¨å­ä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨å…¬å¼€æ•°æ®æé«˜å­ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>TransLLMæå‡ºçš„æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªååŒç»„ä»¶ï¼šä½ç§©é€‚åº”è®­ç»ƒå’Œæ¢å¤çŸ¥è¯†è’¸é¦ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒTransLLMæ¡†æ¶åœ¨å¤šä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå¼ºåŸºçº¿å’ŒChatGPTã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.13923">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2405.13923v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2405.13923v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2405.13923v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2405.13923v3/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_LLM/2405.13923v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-09/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-09/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-09/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_Agent/2508.18708v2/page_1_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-09  TalkToAgent A Human-centric Explanation of Reinforcement Learning   Agents with Large Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-09/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-09\./crop_R1_Reasoning/2509.05249v1/page_4_0.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-09  COGITAO A Visual Reasoning Framework To Study Compositionality &   Generalization
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30806.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
