<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Text-to-Motion">
    <meta name="description" content="Text-to-Motion 方向最新论文已更新，请持续关注 Update in 2024-12-15  AToM Aligning Text-to-Motion Model at Event-Level with GPT-4Vision   Reward">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Text-to-Motion | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-60e3091bd944b75b61c9a29dd78e81bc.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Text-to-Motion</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Text-to-Motion/">
                                <span class="chip bg-color">Text-to-Motion</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                Text-to-Motion
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-12-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-15
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    14.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    51 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-12-15-更新"><a href="#2024-12-15-更新" class="headerlink" title="2024-12-15 更新"></a>2024-12-15 更新</h1><h2 id="AToM-Aligning-Text-to-Motion-Model-at-Event-Level-with-GPT-4Vision-Reward"><a href="#AToM-Aligning-Text-to-Motion-Model-at-Event-Level-with-GPT-4Vision-Reward" class="headerlink" title="AToM: Aligning Text-to-Motion Model at Event-Level with GPT-4Vision   Reward"></a>AToM: Aligning Text-to-Motion Model at Event-Level with GPT-4Vision   Reward</h2><p><strong>Authors:Haonan Han, Xiangzuo Wu, Huan Liao, Zunnan Xu, Zhongyuan Hu, Ronghui Li, Yachao Zhang, Xiu Li</strong></p>
<p>Recently, text-to-motion models have opened new possibilities for creating realistic human motion with greater efficiency and flexibility. However, aligning motion generation with event-level textual descriptions presents unique challenges due to the complex relationship between textual prompts and desired motion outcomes. To address this, we introduce AToM, a framework that enhances the alignment between generated motion and text prompts by leveraging reward from GPT-4Vision. AToM comprises three main stages: Firstly, we construct a dataset MotionPrefer that pairs three types of event-level textual prompts with generated motions, which cover the integrity, temporal relationship and frequency of motion. Secondly, we design a paradigm that utilizes GPT-4Vision for detailed motion annotation, including visual data formatting, task-specific instructions and scoring rules for each sub-task. Finally, we fine-tune an existing text-to-motion model using reinforcement learning guided by this paradigm. Experimental results demonstrate that AToM significantly improves the event-level alignment quality of text-to-motion generation. </p>
<blockquote>
<p>最近，文本到动作模型以其更高的效率和灵活性为创建逼真的人类动作提供了新的可能性。然而，由于文本提示和期望的动作结果之间的复杂关系，将动作生成与事件级别的文本描述对齐呈现出独特的挑战。为了解决这一问题，我们引入了AToM框架，该框架通过利用GPT-4Vision的奖励来增强生成动作与文本提示之间的对齐。AToM主要包括三个阶段：首先，我们构建了一个MotionPrefer数据集，该数据集将三种事件级别的文本提示与生成的动作配对，涵盖了动作的完整性、时间关系和频率。其次，我们设计了一种利用GPT-4Vision进行详细动作注释的方法，包括视觉数据格式化、针对特定任务的指令以及每个子任务的评分规则。最后，我们使用由此范式引导的强化学习对现有的文本到动作模型进行微调。实验结果表明，AToM显著提高了文本到动作生成的事件级对齐质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18654v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>文本到动作模型为创建真实人类运动提供了新的可能性，但其对齐运动生成与事件级别文本描述带来了独特挑战。为解决这个问题，引入AToM框架，通过利用GPT-4Vision的奖励增强生成运动与文本提示的对齐。AToM包含三个阶段：构建MotionPrefer数据集，设计利用GPT-4Vision进行详细运动注释的模式，以及微调现有文本到动作模型。实验结果证明，AToM显著提高了文本到动作生成的事件级别对齐质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到动作模型能够高效灵活地创建真实人类运动。</li>
<li>事件级别文本描述与运动生成对齐存在独特挑战。</li>
<li>AToM框架通过利用GPT-4Vision的奖励增强生成运动与文本提示的对齐。</li>
<li>AToM包含构建MotionPrefer数据集、设计利用GPT-4Vision进行运动注释的模式以及微调现有文本到动作模型三个阶段。</li>
<li>MotionPrefer数据集包含关于运动的完整性、时间关系和频率的三种类型的文本提示与生成运动的配对。</li>
<li>GPT-4Vision用于详细运动注释，包括视觉数据格式化、任务特定指令和每个子任务的计分规则。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18654">Cool Papers</a></strong> </p>
<details>
  <summary>点击展开 Cool Papers 详细信息</summary>
  <p class="faq-q"><strong>Q</strong>: 这篇论文试图解决什么问题？</p>
<p class="faq-a"><strong>A</strong>: 这篇论文试图解决的主要问题是文本到动作（text-to-motion）模型在根据文本提示生成人体动作时与事件级别的文本描述对齐的挑战。具体来说，挑战存在于以下几个方面：

<ol>
<li><p><strong>复杂文本提示与期望动作结果之间的复杂关系</strong>：由于文本提示和期望动作结果之间存在复杂的映射关系，模型很难准确地将复杂的文本描述（例如，包含多个动作事件或具有时间关系和指定频率的动作）映射到相应的动作序列。</p>
</li>
<li><p><strong>数据稀缺和粗糙的文本描述</strong>：现有的文本-动作对数据稀缺，且文本描述通常是粗糙的，覆盖的动作场景有限，这限制了模型对新颖描述的泛化能力。</p>
</li>
<li><p><strong>模型泛化能力有限</strong>：由于上述限制，模型在零样本和多事件场景下的性能表现不佳，难以处理复杂的文本提示。</p>
</li>
</ol>
<p>为了解决这些问题，论文提出了一个名为AToM的框架，该框架通过利用GPT-4Vision提供的奖励来增强生成动作和文本提示之间的对齐度。AToM框架包含三个主要阶段：构建与事件级别的文本提示配对的数据集、设计利用GPT-4Vision进行详细动作标注的范式，以及使用这种范式指导的强化学习来微调现有的文本到动作模型。实验结果表明，AToM显著提高了文本到动作生成的事件级别对齐质量。</p></p>
<p class="faq-q"><strong>Q</strong>: 有哪些相关研究？</p>
<p class="faq-a"><strong>A</strong>: 论文中提到了多个与文本到动作生成（Text-to-Motion, T2M）相关的研究工作，以下是一些主要的相关研究：

<ol>
<li><p><strong>Text2Action</strong> [2]：利用基于SEQ2SEQ模型的GAN将简短描述映射到人体动作。</p>
</li>
<li><p><strong>Language2Pose</strong> [3]：引入了一种课程学习方法，用于开发文本和姿态的关节级嵌入。</p>
</li>
<li><p><strong>Lin et al.</strong> [18]：提出了一个端到端的SEQ2SEQ模型，用于生成更真实的动画。</p>
</li>
<li><p><strong>Ghosh et al.</strong> [10]：开发了一个分层的双流序列模型，能够处理描述多个动作的长句子。</p>
</li>
<li><p><strong>MotionCLIP</strong> [30]：将人类动作流形与CLIP空间对齐，赋予模型零样本能力。</p>
</li>
<li><p><strong>TEACH</strong> [5]：基于Transformer的模型，能够生成遵循复杂、顺序动作指令的真实3D人体动作。</p>
</li>
<li><p><strong>TEMOS</strong> [36]：使用基于Transformer的VAE和额外的文本编码器进行多对象3D场景生成和编辑。</p>
</li>
<li><p><strong>T2M-GPT</strong> [33]：结合VQ-VAE和GPT获得高质量的离散表示，实现有竞争力的动作生成结果。</p>
</li>
<li><p><strong>MotionDiffuse</strong> [34]：基于扩散模型，允许对身体部位进行细粒度控制。</p>
</li>
<li><p><strong>MDM</strong> [27]：无分类器的扩散模型，预测动作样本而非噪声，便于几何损失的应用。</p>
</li>
<li><p><strong>MLD</strong> [9]：使用潜在扩散模型进一步推进动作生成。</p>
</li>
<li><p><strong>MotionGPT</strong> [15]：开发了统一的大型动作语言模型，通过离散向量量化表示人类动作。</p>
</li>
</ol>
<p>除了这些，论文还讨论了与人类&#x2F;AI反馈对齐模型相关的研究，包括：</p>
<ul>
<li><p><strong>Reinforcement Learning from Human Feedback (RLHF)</strong> [7, 22]：用于模型对齐的技术，特别是在目标复杂或不明确的情况下。</p>
</li>
<li><p><strong>Direct Preference Optimization (DPO)</strong> [25] 和 <strong>Slic-hf</strong> [37]：直接基于人类偏好优化模型策略的方法。</p>
</li>
<li><p><strong>InstructMotion</strong> [29]：第一个通过人类反馈的强化学习微调文本到动作模型的工作。</p>
</li>
<li><p><strong>RLAIF</strong> [8]：展示了另一种有前景的替代方案，即使用AI反馈进行模型对齐。</p>
</li>
</ul>
<p>这些研究构成了文本到动作生成领域的研究基础，并为本文提出的AToM框架提供了理论和技术背景。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文如何解决这个问题？</p>
<p class="faq-a"><strong>A</strong>: 论文通过提出一个名为AToM（Aligning Text-to-Motion Model at Event-Level with GPT-4Vision Reward）的框架来解决文本到动作模型与事件级别文本描述对齐的问题。AToM框架主要包含以下三个阶段：

<h3 id="1-数据集构建（Dataset-Construction）"><a href="#1-数据集构建（Dataset-Construction）" class="headerlink" title="1. 数据集构建（Dataset Construction）"></a>1. 数据集构建（Dataset Construction）</h3><ul>
<li>首先，研究者们构建了一个名为MotionPrefer的数据集，该数据集将三种类型的事件级别文本提示与生成的动作配对，涵盖了动作的完整性、时间关系和频率。</li>
<li>使用GPT-4生成初始文本提示，然后输入到动作生成模型（例如MotionGPT）中，为每个文本提示生成多个不同的动作。</li>
<li>总共包含5,276个提示和47.1k个动作样本。</li>
</ul>
<h3 id="2-奖励范式设计（Reward-Paradigm-Design）"><a href="#2-奖励范式设计（Reward-Paradigm-Design）" class="headerlink" title="2. 奖励范式设计（Reward Paradigm Design）"></a>2. 奖励范式设计（Reward Paradigm Design）</h3><ul>
<li>将MotionGPT生成的动作序列渲染成视频，并从中采样一系列帧。</li>
<li>将采样的动作帧和对应的文本提示输入到GPT-4Vision模型中，利用该模型评估文本和动作之间的对齐得分。</li>
<li>根据给定的评分规则为每个子任务（完整性、时间顺序和频率）计算对齐得分，生成带有对齐得分的文本-动作对，构成MotionPrefer数据集。</li>
</ul>
<h3 id="3-文本到动作模型微调（Text-to-Motion-Model-Fine-tuning）"><a href="#3-文本到动作模型微调（Text-to-Motion-Model-Fine-tuning）" class="headerlink" title="3. 文本到动作模型微调（Text-to-Motion Model Fine-tuning）"></a>3. 文本到动作模型微调（Text-to-Motion Model Fine-tuning）</h3><ul>
<li>使用MotionPrefer数据集和GPT-4Vision的对齐得分作为反馈，微调现有的文本到动作模型（如MotionGPT）。</li>
<li>采用LoRA（Low-Rank Adaptation）和IPO（一种强化学习策略）算法进行微调，以提高模型在特定任务上的对齐性能。</li>
<li>实验结果表明，AToM显著提高了文本到动作生成的事件级别对齐质量。</li>
</ul>
<p>通过这三个阶段，AToM框架能够有效地利用视觉语言大模型（如GPT-4Vision）的反馈，显著提高文本到动作模型在事件级别上的对齐质量，从而在文本提示和生成动作之间建立更加精确的对应关系。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文做了哪些实验？</p>
<p class="faq-a"><strong>A</strong>: 论文中进行了一系列实验来评估AToM框架的有效性，这些实验主要分为以下几个部分：

<h3 id="1-实施细节（Implementation-Details）"><a href="#1-实施细节（Implementation-Details）" class="headerlink" title="1. 实施细节（Implementation Details）"></a>1. 实施细节（Implementation Details）</h3><ul>
<li><strong>数据集</strong>：使用预训练的MotionGPT模型，并在MotionPrefer数据集的三个子集上进行微调，分别针对时间、完整性和频率任务。</li>
<li><strong>实现具体内容</strong>：设置了学习率、批量大小和训练周期等超参数，并使用了AdamW优化器和余弦学习率调度器。</li>
<li><strong>评估指标</strong>：包括多模态距离（MM-Dist）、R-Precision、FID、多样性和MModality等，用于评估动作质量和文本-动作对齐度。</li>
</ul>
<h3 id="2-主要结果（Main-Results）"><a href="#2-主要结果（Main-Results）" class="headerlink" title="2. 主要结果（Main Results）"></a>2. 主要结果（Main Results）</h3><ul>
<li><strong>定量实验</strong>：在时间、频率和完整性任务中，AToM在多个评估指标上一致优于基线模型（MotionGPT和InstructMotion），显示出更好的文本-动作对齐度、动作质量和生成真实性。</li>
<li><strong>定性实验</strong>：通过视觉样本比较，展示了预训练模型（MotionGPT）和微调后的模型（AToM）在生成忠实度上的差异，AToM在多样化动作事件、复杂时间顺序和特定频率方面表现更优。</li>
<li><strong>用户研究</strong>：50名参与者对AToM和MotionGPT生成的动作在时间、频率和完整性方面进行了评估，AToM在所有子任务中的胜率均高于MotionGPT。</li>
</ul>
<h3 id="3-消融研究（Ablation-Study）"><a href="#3-消融研究（Ablation-Study）" class="headerlink" title="3. 消融研究（Ablation Study）"></a>3. 消融研究（Ablation Study）</h3><ul>
<li><strong>动作注入形式</strong>：比较了三种不同的动作注入策略（逐帧、全图像、轨迹图像）对GPT-4V提问的影响。</li>
<li><strong>分数过滤</strong>：研究了在偏好对构建中使用分数过滤的效果，只有评分高于3的样本被视为正面样本。</li>
<li><strong>LoRA利用</strong>：探讨了LoRA在微调中的效果，与传统微调方法相比，LoRA能够更有效地调整模型参数。</li>
<li><strong>强化学习策略</strong>：比较了不同的强化学习策略（IPO、DPO、KTO、PPO）在生成模型微调后的表现。</li>
<li><strong>动作序列采样步长</strong>：研究了不同帧采样间隔对对齐和质量指标的影响。</li>
</ul>
<h3 id="4-额外结果（Additional-Results）"><a href="#4-额外结果（Additional-Results）" class="headerlink" title="4. 额外结果（Additional Results）"></a>4. 额外结果（Additional Results）</h3><ul>
<li><strong>更多定性结果</strong>：提供了额外的AToM和基线模型生成的视觉样本比较。</li>
<li><strong>微调迭代次数</strong>：探讨了微调迭代次数对模型性能的影响。</li>
<li><strong>IPO超参数β的影响</strong>：分析了IPO策略中超参数β对对齐和质量指标的影响。</li>
<li><strong>偏好数据集体积对模型性能的影响</strong>：研究了偏好对数量对模型性能的影响。</li>
</ul>
<p>这些实验全面评估了AToM框架在不同方面的表现，并与现有技术进行了比较，证明了其在提高文本到动作模型对齐质量方面的有效性。</p></p>
<p class="faq-q"><strong>Q</strong>: 有什么可以进一步探索的点？</p>
<p class="faq-a"><strong>A</strong>: 基于论文内容和实验结果，以下是一些可以进一步探索的点：

<h3 id="1-扩展数据集的多样性和复杂性"><a href="#1-扩展数据集的多样性和复杂性" class="headerlink" title="1. 扩展数据集的多样性和复杂性"></a>1. 扩展数据集的多样性和复杂性</h3><ul>
<li><strong>跨领域数据集</strong>：探索将AToM框架应用于更广泛的动作类型和场景，例如体育活动、舞蹈和日常复杂交互。</li>
<li><strong>多语言支持</strong>：研究模型对不同语言文本提示的泛化能力，并构建多语言数据集进行训练和测试。</li>
</ul>
<h3 id="2-增强模型的解释性和可视化"><a href="#2-增强模型的解释性和可视化" class="headerlink" title="2. 增强模型的解释性和可视化"></a>2. 增强模型的解释性和可视化</h3><ul>
<li><strong>注意力机制分析</strong>：通过引入注意力机制来分析模型在处理特定文本提示时关注的区域，增强模型的解释性。</li>
<li><strong>生成过程可视化</strong>：研究生成过程中的关键帧和动作变化，以可视化的方式展示模型是如何根据文本提示生成动作的。</li>
</ul>
<h3 id="3-探索不同的强化学习策略"><a href="#3-探索不同的强化学习策略" class="headerlink" title="3. 探索不同的强化学习策略"></a>3. 探索不同的强化学习策略</h3><ul>
<li><strong>混合策略</strong>：结合人类反馈和AI反馈的优势，开发混合强化学习策略以进一步提高模型性能。</li>
<li><strong>自适应策略</strong>：研究自适应调整强化学习策略中的参数，以动态适应不同的训练阶段和任务需求。</li>
</ul>
<h3 id="4-提升模型的鲁棒性和泛化能力"><a href="#4-提升模型的鲁棒性和泛化能力" class="headerlink" title="4. 提升模型的鲁棒性和泛化能力"></a>4. 提升模型的鲁棒性和泛化能力</h3><ul>
<li><strong>对抗训练</strong>：通过引入对抗样本来测试和增强模型的鲁棒性，特别是在面对异常或模糊的文本提示时。</li>
<li><strong>零样本学习</strong>：探索模型在零样本或少样本情况下的泛化能力，尤其是在未见过的动作类型或描述上。</li>
</ul>
<h3 id="5-跨模态应用和集成"><a href="#5-跨模态应用和集成" class="headerlink" title="5. 跨模态应用和集成"></a>5. 跨模态应用和集成</h3><ul>
<li><strong>动作识别和预测</strong>：将AToM框架应用于动作识别和预测任务，评估模型在视频理解和未来动作预测方面的表现。</li>
<li><strong>虚拟现实和游戏</strong>：将模型集成到虚拟现实和游戏引擎中，实现基于自然语言指令的实时动作生成和控制。</li>
</ul>
<h3 id="6-优化和加速训练过程"><a href="#6-优化和加速训练过程" class="headerlink" title="6. 优化和加速训练过程"></a>6. 优化和加速训练过程</h3><ul>
<li><strong>分布式训练</strong>：研究如何利用分布式计算资源来加速模型的训练过程。</li>
<li><strong>模型压缩和加速</strong>：探索模型压缩技术，减少模型大小和计算需求，使其更适合在资源受限的设备上运行。</li>
</ul>
<h3 id="7-社会和伦理考量"><a href="#7-社会和伦理考量" class="headerlink" title="7. 社会和伦理考量"></a>7. 社会和伦理考量</h3><ul>
<li><strong>偏见和公平性</strong>：评估和解决模型可能存在的偏见问题，确保生成的动作多样性和包容性。</li>
<li><strong>用户隐私和数据安全</strong>：在使用真实世界数据进行训练和测试时，确保遵守数据保护法规和用户隐私。</li>
</ul>
<p>这些探索点可以帮助研究者们更深入地理解AToM框架的潜力和局限性，并推动文本到动作生成技术的发展。</p></p>
<p class="faq-q"><strong>Q</strong>: 总结一下论文的主要内容</p>
<p class="faq-a"><strong>A</strong>: 这篇论文介绍了一个名为AToM（Aligning Text-to-Motion Model at Event-Level with GPT-4Vision Reward）的框架，旨在提高文本到动作（Text-to-Motion, T2M）模型在事件级别上与文本提示对齐的质量。以下是论文的主要内容总结：

<h3 id="1-问题背景"><a href="#1-问题背景" class="headerlink" title="1. 问题背景"></a>1. 问题背景</h3><ul>
<li>文本到动作模型能够根据文本提示生成人体动作，但在处理复杂的、多动作事件的文本描述时面临挑战。</li>
<li>现有模型在泛化到未见过的文本描述和多事件场景时受限，需要更精细的对齐评估。</li>
</ul>
<h3 id="2-AToM框架"><a href="#2-AToM框架" class="headerlink" title="2. AToM框架"></a>2. AToM框架</h3><ul>
<li><strong>数据集构建</strong>：创建了一个名为MotionPrefer的数据集，包含5.3K个文本提示和80K个动作偏好对，涵盖动作的完整性、时间关系和频率。</li>
<li><strong>奖励范式设计</strong>：利用GPT-4Vision对生成的动作序列和文本描述之间的对齐进行评分，基于给定的评分规则计算对齐得分。</li>
<li><strong>模型微调</strong>：使用MotionPrefer数据集和GPT-4Vision的对齐得分作为反馈，通过LoRA和IPO强化学习策略微调现有的文本到动作模型。</li>
</ul>
<h3 id="3-实验结果"><a href="#3-实验结果" class="headerlink" title="3. 实验结果"></a>3. 实验结果</h3><ul>
<li><strong>定量实验</strong>：AToM在多个评估指标上优于基线模型，显示出更好的文本-动作对齐度、动作质量和生成真实性。</li>
<li><strong>定性实验</strong>：通过视觉样本比较，展示了AToM在生成忠实度上的优势。</li>
<li><strong>用户研究</strong>：AToM在所有子任务中的用户评估胜率均高于基线模型。</li>
</ul>
<h3 id="4-消融研究"><a href="#4-消融研究" class="headerlink" title="4. 消融研究"></a>4. 消融研究</h3><ul>
<li>探讨了不同的动作注入策略、分数过滤、LoRA利用和强化学习策略对模型性能的影响。</li>
</ul>
<h3 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h3><ul>
<li>AToM框架有效地利用视觉语言大模型的反馈，显著提高了文本到动作生成的事件级别对齐质量，为文本提示和生成动作之间建立了更精确的对应关系。</li>
</ul>
<p>这篇论文通过提出一个新的框架，解决了文本到动作模型在复杂文本描述下对齐质量不足的问题，并展示了通过利用先进的视觉语言模型进行微调可以显著提升模型性能。</p></p>
<p class="faq-q"><strong>Q</strong>: 想要进一步了解论文</p>
<p class="faq-a"><strong>A</strong>: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a href="http://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是AToM: Aligning Text-to-Motion Model at Event-Level with GPT-4Vision Reward，链接是 https://arxiv.org/pdf/2411.18654 ，已有的FAQ链接是 https://papers.cool/arxiv/kimi?paper=2411.18654 。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false" target="_blank"><strong>这里 <i class="fa fa-external-link"></i></strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>

</details>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-67bedfd09d127c6913540ee8041d9f83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35a2861cecba4e90e3e2897c574fd872.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db1e663201e0a4cae7d04c2d2be8ce02.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60e3091bd944b75b61c9a29dd78e81bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a7be69c546c41e545a7743ad77f769a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7ab4c3d998cabe6a28d6a4b563f68b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c9b60ca77764d0367d8b86edf517272e.jpg" align="middle">
</details>



<h2 id="LEAD-Latent-Realignment-for-Human-Motion-Diffusion"><a href="#LEAD-Latent-Realignment-for-Human-Motion-Diffusion" class="headerlink" title="LEAD: Latent Realignment for Human Motion Diffusion"></a>LEAD: Latent Realignment for Human Motion Diffusion</h2><p><strong>Authors:Nefeli Andreou, Xi Wang, Victoria Fernández Abrevaya, Marie-Paule Cani, Yiorgos Chrysanthou, Vicky Kalogeiton</strong></p>
<p>Our goal is to generate realistic human motion from natural language. Modern methods often face a trade-off between model expressiveness and text-to-motion alignment. Some align text and motion latent spaces but sacrifice expressiveness; others rely on diffusion models producing impressive motions, but lacking semantic meaning in their latent space. This may compromise realism, diversity, and applicability. Here, we address this by combining latent diffusion with a realignment mechanism, producing a novel, semantically structured space that encodes the semantics of language. Leveraging this capability, we introduce the task of textual motion inversion to capture novel motion concepts from a few examples. For motion synthesis, we evaluate LEAD on HumanML3D and KIT-ML and show comparable performance to the state-of-the-art in terms of realism, diversity, and text-motion consistency. Our qualitative analysis and user study reveal that our synthesized motions are sharper, more human-like and comply better with the text compared to modern methods. For motion textual inversion, our method demonstrates improved capacity in capturing out-of-distribution characteristics in comparison to traditional VAEs. </p>
<blockquote>
<p>我们的目标是从自然语言生成逼真的人类运动。现代方法经常在模型表达力和文本到运动的对齐之间面临权衡。一些方法对齐文本和运动潜在空间，但牺牲了表达力；另一些方法依赖扩散模型产生令人印象深刻的运动，但其潜在空间中缺乏语义含义。这可能会损害真实性、多样性和适用性。在这里，我们通过结合潜在扩散和重新对齐机制来解决这个问题，产生了一个新颖、语义结构化的空间，该空间编码了语言的语义。利用这一能力，我们引入了文本运动反转任务，从少数几个例子中捕捉新的运动概念。对于运动合成，我们在HumanML3D和KIT-ML上评估了LEAD，并在真实性、多样性和文本运动一致性方面显示出与最新技术相当的性能。我们的定性分析和用户研究结果表明，与我们合成的运动相比，我们的运动更加清晰、更像人类，并且更符合文本描述。对于运动文本反转，我们的方法在捕捉超出分布特性的方面表现出了比传统VAEs更强的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.14508v1">PDF</a> </p>
<p><strong>Summary</strong><br>文本的目标是从自然语言生成逼真的人类运动。现代方法常常面临模型表达力和文本到运动对齐之间的权衡。本研究通过结合潜在扩散和重新对齐机制，创建了一个新颖、语义结构化的空间，编码语言语义。此外，本研究引入了文本运动反转任务来从少量样本中捕获新的运动概念。在HumanML3D和KIT-ML上的运动合成评估显示，其在逼真性、多样性和文本运动一致性方面表现出与最新技术相当的性能。我们的定性分析和用户研究表明，合成的运动更加清晰、更像人类，并且更符合文本描述。对于运动文本反转，该方法在捕获非标准特征方面表现出优于传统VAE的能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>目标是从自然语言生成逼真的人类运动。</li>
<li>现代方法在模型表达力和文本到运动对齐之间存在权衡。</li>
<li>结合潜在扩散和重新对齐机制，创建语义结构化空间，编码语言语义。</li>
<li>引入文本运动反转任务，从少量样本中捕获新的运动概念。</li>
<li>在HumanML3D和KIT-ML上的运动合成评估表现与最新技术相当。</li>
<li>定性分析和用户研究表明合成的运动更加清晰、更像人类，更符合文本描述。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.14508">Cool Papers</a></strong> </p>
<details>
  <summary>点击展开 Cool Papers 详细信息</summary>
  <p class="faq-q"><strong>Q</strong>: 这篇论文试图解决什么问题？</p>
<p class="faq-a"><strong>A</strong>: 这篇论文试图解决的问题是如何从自然语言描述中生成逼真的人类运动（Text-to-Motion, T2M）。具体来说，论文面临的挑战包括：

<ol>
<li><p><strong>模型表达性与文本到运动对齐之间的权衡</strong>：一些方法通过将文本和运动的潜在空间对齐来实现，但牺牲了模型的表达性；而其他方法依赖于扩散模型生成令人印象深刻的运动，但这些运动缺乏潜在空间中的语义含义。这可能会影响生成运动的真实性、多样性和适用性。</p>
</li>
<li><p><strong>潜在空间中的语义结构缺失</strong>：现有的扩散模型虽然能够生成令人印象深刻的运动，但观察到合成的动画可能不会一致地遵循输入文本，并且可能仍然生成不真实的运动。论文提出假设，一个语义结构化的运动潜在空间（即继承语言空间的一些丰富属性的空间）可以促进和改进文本到运动生成的任务。</p>
</li>
<li><p><strong>运动领域的个性化生成</strong>：在图像领域，扩散模型已经超越了通用文本到图像生成，向着基于几个示例合成难以描述的概念的个性化生成发展。然而，据作者所知，没有运动扩散工作展示出执行反演或个性化的能力。</p>
</li>
</ol>
<p>为了解决这些问题，论文提出了LEAD（Latent rEAlignment for human motion Diffusion），这是一个基于潜在扩散的新文本到运动模型，通过引入一个投影模块来解决潜在空间中缺乏语义结构的问题。此外，论文还引入了运动文本反演（Motion Textual Inversion, MTI）的任务，并展示了提出的投影器如何改善这一任务。</p></p>
<p class="faq-q"><strong>Q</strong>: 有哪些相关研究？</p>
<p class="faq-a"><strong>A</strong>: 相关研究主要围绕以下几个领域：

<ol>
<li><p><strong>文本到运动（Text-to-Motion, T2M）生成</strong>：这是创建反映给定语言指令的类人运动的过程。此任务由于其在3D内容创建中的潜在应用以及在机器人技术、娱乐和虚拟现实等领域的众多应用而受到显著关注。相关工作包括TEMOS、L2JP、MotionCLIP等，这些方法构建了一个共同的潜在空间，同时编码自然语言和运动。</p>
</li>
<li><p><strong>多模态运动合成</strong>：研究工作探索了使用文本、图像、音频、音乐和场景等作为条件来控制运动生成的方法。例如，一些工作通过将自然语言转换为潜在空间，作为条件信号来生成运动。</p>
</li>
<li><p><strong>运动扩散模型（Motion Diffusion Models）</strong>：扩散模型已经在文本到图像生成中展示了前所未有的能力，最近其在文本到运动任务中的潜力也被探索。例如，基于 transformer 的运动扩散模型（MDM）和使用跨模态线性变换器作为主干来增加生成运动多样性的 MotionDiffuse。</p>
</li>
<li><p><strong>文本反演在扩散模型中的应用（Textual Inversion in Diffusion Models）</strong>：文本反演的目标是将其他模态的数据反演到语言潜在空间，以便在预训练模型中引入新概念。例如，Gal 等人的工作通过优化方法来解决图像反演问题，通过视觉重建作为指导来找到对应新概念的标记嵌入。</p>
</li>
<li><p><strong>运动表示学习</strong>：包括使用VAEs和Transformer架构来学习运动数据的有效表示，例如在HumanML3D和KIT-ML数据集上进行评估。</p>
</li>
<li><p><strong>运动数据集</strong>：如HumanML3D和KIT-ML，这些数据集提供了大量的人类运动序列和相应的文本描述，用于训练和评估T2M模型。</p>
</li>
<li><p><strong>评估指标</strong>：用于评估生成运动的真实性、文本运动一致性、生成多样性和多模态匹配的指标，如Fréchet Inception Distance (FID)、多模态距离 (MMdist) 和 R-precision。</p>
</li>
</ol>
<p>这些研究构成了LEAD方法的理论和实证基础，并帮助展示了该方法在文本到运动生成和运动文本反演任务中的潜力和优势。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文如何解决这个问题？</p>
<p class="faq-a"><strong>A</strong>: 论文通过提出一个名为LEAD（Latent rEAlignment for human motion Diffusion）的新文本到运动模型来解决这个问题。LEAD模型的核心思想是结合潜在扩散模型和一个重新对齐机制，以创建一个新颖的、语义结构化的空间，该空间编码语言的语义。以下是该方法的关键组成部分和解决步骤：

<h3 id="1-架构组成"><a href="#1-架构组成" class="headerlink" title="1. 架构组成"></a>1. 架构组成</h3><p>LEAD模型由四个主要模块组成：</p>
<ul>
<li>一个运动VAE（variational autoencoder），</li>
<li>一个文本编码器，</li>
<li>一个条件扩散模型，</li>
<li>一个新颖的投影模块（projector module）。</li>
</ul>
<h3 id="2-运动VAE和文本编码器"><a href="#2-运动VAE和文本编码器" class="headerlink" title="2. 运动VAE和文本编码器"></a>2. 运动VAE和文本编码器</h3><ul>
<li>使用一个基于transformer的运动VAE来学习和表示运动序列的潜在空间。</li>
<li>使用CLIP模型的文本编码器将输入文本转换为潜在的嵌入空间。</li>
</ul>
<h3 id="3-条件扩散模型"><a href="#3-条件扩散模型" class="headerlink" title="3. 条件扩散模型"></a>3. 条件扩散模型</h3><ul>
<li>采用基于transformer的扩散模型，该模型能够根据当前的噪声版本、时间步和条件向量（文本嵌入）来预测噪声。</li>
</ul>
<h3 id="4-投影模块"><a href="#4-投影模块" class="headerlink" title="4. 投影模块"></a>4. 投影模块</h3><ul>
<li>这是LEAD的核心创新点，包括一个编码器（Eproj）和一个解码器（Dproj）。编码器将VAE的潜在表示转换为与CLIP对齐的新嵌入空间，解码器再将其转换回VAE空间。</li>
<li>通过训练这个模块，使得转换后的潜在表示与CLIP的文本嵌入空间更加一致，从而改善语义结构。</li>
</ul>
<h3 id="5-训练和损失函数"><a href="#5-训练和损失函数" class="headerlink" title="5. 训练和损失函数"></a>5. 训练和损失函数</h3><ul>
<li>首先训练运动VAE。</li>
<li>然后冻结VAE和文本编码器，训练扩散模型。</li>
<li>最后，训练投影模块，使用对齐损失（alignment loss）和重构损失（reconstruction loss）。</li>
</ul>
<h3 id="6-推断过程"><a href="#6-推断过程" class="headerlink" title="6. 推断过程"></a>6. 推断过程</h3><ul>
<li>通过标准的逆扩散过程生成潜在噪声向量，并使用条件CLIP嵌入逐步去噪。</li>
<li>将清洁的潜在表示通过投影模块，并最终通过VAE解码得到运动序列。</li>
</ul>
<h3 id="7-运动文本反演（Motion-Textual-Inversion-MTI）"><a href="#7-运动文本反演（Motion-Textual-Inversion-MTI）" class="headerlink" title="7. 运动文本反演（Motion Textual Inversion, MTI）"></a>7. 运动文本反演（Motion Textual Inversion, MTI）</h3><ul>
<li>引入MTI任务，通过优化给定示例运动的文本嵌入，以生成具有特定特征的运动序列。</li>
<li>使用重构损失在重新对齐的空间中学习新嵌入，以更好地捕捉输入运动的特征。</li>
</ul>
<p>通过以上步骤，LEAD模型能够生成与文本描述更加一致且逼真的人类运动序列，同时在生成过程中保持多样性。此外，通过MTI任务，LEAD能够从少量示例中捕捉新运动概念，为个性化运动生成铺平了道路。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文做了哪些实验？</p>
<p class="faq-a"><strong>A</strong>: 论文中进行了一系列实验来评估LEAD模型在文本到运动生成（Text-to-Motion, T2M）和运动文本反演（Motion Textual Inversion, MTI）任务上的性能。以下是主要的实验内容和结果：

<h3 id="1-数据集和评估指标"><a href="#1-数据集和评估指标" class="headerlink" title="1. 数据集和评估指标"></a>1. 数据集和评估指标</h3><ul>
<li><strong>数据集</strong>：使用了两个标准数据集HumanML3D和KIT-ML进行T2M任务的评估。HumanML3D包含14,616个人体姿态序列和44,970个描述，而KIT-ML包含6,353个文本描述对应3,911个动作。</li>
<li><strong>评估指标</strong>：包括Fréchet Inception Distance (FID)、多模态距离（MMdist）、R-precision、生成多样性（Div）和多模态性（MModality）等，用于评估生成动作的真实性、文本与动作的一致性、生成多样性和多模态匹配。</li>
</ul>
<h3 id="2-文本到运动生成（T2M）"><a href="#2-文本到运动生成（T2M）" class="headerlink" title="2. 文本到运动生成（T2M）"></a>2. 文本到运动生成（T2M）</h3><ul>
<li>与现有的SOTA模型（如MLD、MotionLCM等）在HumanML3D和KIT-ML数据集上进行了比较。</li>
<li>LEAD在运动真实性（FID）上取得了显著的改进，与其他方法相比，在保持多样性的同时，实现了更好的真实性和文本动作一致性。</li>
</ul>
<h3 id="3-运动文本反演（MTI）"><a href="#3-运动文本反演（MTI）" class="headerlink" title="3. 运动文本反演（MTI）"></a>3. 运动文本反演（MTI）</h3><ul>
<li>在HumanML3D的子集上进行了定量比较，评估了在不同空间（VAE空间、运动特征空间和重新对齐的空间）上进行MTI的效果。</li>
<li>结果表明，在重新对齐的空间上进行MTI可以改善生成动作的真实性和多模态对齐，证明了重新对齐机制在优化过程中的有效性。</li>
</ul>
<h3 id="4-用户研究"><a href="#4-用户研究" class="headerlink" title="4. 用户研究"></a>4. 用户研究</h3><ul>
<li>进行了用户研究，比较了LEAD与MLD和MotionCLIP在运动真实性和文本动作相关性上的表现。</li>
<li>用户研究结果显示，LEAD生成的动作在真实性和与文本的匹配度上都优于其他方法。</li>
</ul>
<h3 id="5-LEAD潜在空间的可重用性"><a href="#5-LEAD潜在空间的可重用性" class="headerlink" title="5. LEAD潜在空间的可重用性"></a>5. LEAD潜在空间的可重用性</h3><ul>
<li>研究了LEAD潜在投影器在不同运动潜在扩散模型（如MotionLCM）上的泛化能力。</li>
<li>结果表明，即使在不同的文本编码方式下，LEAD的潜在空间也能改善文本动作对齐和动作质量，显示了潜在空间的鲁棒性和可转移性。</li>
</ul>
<h3 id="6-消融研究"><a href="#6-消融研究" class="headerlink" title="6. 消融研究"></a>6. 消融研究</h3><ul>
<li>对LEAD模型的架构设计和训练损失进行了消融研究，以评估不同组件对最终性能的影响。</li>
<li>结果表明，使用基于transformer的架构和所有提出的损失函数能够实现最佳性能。</li>
</ul>
<p>这些实验全面评估了LEAD模型在文本到运动生成和个性化运动生成任务中的有效性，并与现有方法进行了比较，证明了LEAD在生成逼真、多样化且与文本描述一致的人类运动方面的优势。</p></p>
<p class="faq-q"><strong>Q</strong>: 有什么可以进一步探索的点？</p>
<p class="faq-a"><strong>A</strong>: 尽管LEAD模型在文本到运动生成和运动文本反演任务上取得了显著的成果，但仍有一些方面可以进一步探索和改进：

<h3 id="1-处理更长的文本描述"><a href="#1-处理更长的文本描述" class="headerlink" title="1. 处理更长的文本描述"></a>1. 处理更长的文本描述</h3><p>论文提到LEAD在处理长和连续的文本描述时可能会遇到挑战，生成的运动可能无法完全符合文本描述。因此，研究如何改进模型以更好地理解和处理更长的文本序列是一个重要的研究方向。</p>
<h3 id="2-提高动作的复杂性和自然性"><a href="#2-提高动作的复杂性和自然性" class="headerlink" title="2. 提高动作的复杂性和自然性"></a>2. 提高动作的复杂性和自然性</h3><p>尽管LEAD生成的动作更加逼真，但仍然可能存在一些动作不自然或不符合物理规律的情况，如脚部滑动等。研究如何进一步优化模型以生成更加复杂和自然的动作是一个有价值的探索方向。</p>
<h3 id="3-扩展数据集和多样性"><a href="#3-扩展数据集和多样性" class="headerlink" title="3. 扩展数据集和多样性"></a>3. 扩展数据集和多样性</h3><p>LEAD模型主要在HumanML3D和KIT-ML数据集上进行训练和评估。将模型扩展到更多样化的数据集，并探索如何生成更多样化的动作，特别是在不同场景和风格下的动作，可以进一步提高模型的泛化能力。</p>
<h3 id="4-提升个性化生成的能力"><a href="#4-提升个性化生成的能力" class="headerlink" title="4. 提升个性化生成的能力"></a>4. 提升个性化生成的能力</h3><p>虽然LEAD引入了运动文本反演任务来实现一定程度的个性化生成，但如何进一步提升模型的个性化生成能力，使其能够根据更少的示例或更抽象的描述生成具有特定特征的动作，仍然是一个值得探索的问题。</p>
<h3 id="5-交互式应用和实时生成"><a href="#5-交互式应用和实时生成" class="headerlink" title="5. 交互式应用和实时生成"></a>5. 交互式应用和实时生成</h3><p>将LEAD模型应用于交互式环境，如虚拟现实或游戏，需要模型能够实时生成动作。研究如何优化模型以满足实时生成的需求，同时保持动作的质量和多样性，具有重要的实际应用价值。</p>
<h3 id="6-多模态输入的融合"><a href="#6-多模态输入的融合" class="headerlink" title="6. 多模态输入的融合"></a>6. 多模态输入的融合</h3><p>考虑将更多的模态信息，如音频或视觉信息，与文本描述一起作为输入，以实现更丰富和动态的动作生成。研究如何有效地融合多模态输入以提高动作生成的准确性和自然性是一个有趣的研究方向。</p>
<h3 id="7-可解释性和控制性"><a href="#7-可解释性和控制性" class="headerlink" title="7. 可解释性和控制性"></a>7. 可解释性和控制性</h3><p>提高模型的可解释性，让研究人员和用户更好地理解模型的决策过程，以及如何通过特定的输入控制生成的动作，可以增加模型的透明度和可用性。</p>
<h3 id="8-伦理和隐私问题"><a href="#8-伦理和隐私问题" class="headerlink" title="8. 伦理和隐私问题"></a>8. 伦理和隐私问题</h3><p>随着个性化运动生成技术的发展，需要考虑其潜在的伦理和隐私问题，如如何防止滥用技术生成虚假内容，以及如何确保在生成动作时尊重个人隐私和授权。</p>
<p>通过在这些方向上进行进一步的研究和探索，可以不断提升基于文本的动作生成技术的性能和应用范围，推动相关领域的技术进步。</p></p>
<p class="faq-q"><strong>Q</strong>: 总结一下论文的主要内容</p>
<p class="faq-a"><strong>A</strong>: 这篇论文提出了一个名为LEAD（Latent rEAlignment for human motion Diffusion）的新型文本到运动（Text-to-Motion, T2M）生成模型。LEAD模型的核心贡献在于引入了一个潜在对齐机制，通过这一机制，模型能够生成与输入文本描述在语义上更一致、更逼真的人类运动序列。下面是该论文的主要内容总结：

<h3 id="1-问题陈述"><a href="#1-问题陈述" class="headerlink" title="1. 问题陈述"></a>1. 问题陈述</h3><ul>
<li>生成与文本描述相符的逼真人类运动是一个挑战，因为自然语言与人类运动之间存在较大的差异。</li>
<li>现有方法在模型表达性和文本到运动对齐之间存在权衡，导致生成的运动可能缺乏语义一致性或多样性。</li>
</ul>
<h3 id="2-LEAD模型"><a href="#2-LEAD模型" class="headerlink" title="2. LEAD模型"></a>2. LEAD模型</h3><ul>
<li>LEAD基于潜在扩散模型，通过一个新颖的投影模块将运动VAE的潜在空间与语言模型（如CLIP）对齐，以获得更好的语义结构。</li>
<li>该模型由四个主要模块组成：运动VAE、文本编码器、扩散模型和投影模块。</li>
</ul>
<h3 id="3-方法"><a href="#3-方法" class="headerlink" title="3. 方法"></a>3. 方法</h3><ul>
<li><strong>训练</strong>：分三个阶段训练LEAD，首先训练运动VAE，然后是扩散模型，最后训练投影模块。</li>
<li><strong>推断</strong>：在推断时，通过逆扩散过程生成潜在噪声向量，使用文本嵌入作为条件，并通过投影模块生成最终的运动序列。</li>
<li><strong>运动文本反演（MTI）</strong>：提出了一个新的任务，通过优化给定示例运动的文本嵌入，以生成具有特定特征的运动序列。</li>
</ul>
<h3 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h3><ul>
<li>在HumanML3D和KIT-ML数据集上评估LEAD，与现有方法比较，显示了在运动真实性、文本运动一致性等方面的改进。</li>
<li>用户研究结果表明，与现有方法相比，LEAD生成的运动在真实性和文本相关性方面得到了更高的评价。</li>
<li>对于MTI任务，LEAD在生成具有特定特征的运动方面表现出更好的性能。</li>
</ul>
<h3 id="5-贡献"><a href="#5-贡献" class="headerlink" title="5. 贡献"></a>5. 贡献</h3><ul>
<li>提出了LEAD模型，通过潜在对齐机制在文本到运动生成任务中取得了新的状态。</li>
<li>引入了运动文本反演任务，并展示了重新对齐的潜在空间在生成特定特征运动方面的优势。</li>
</ul>
<h3 id="6-社会影响和局限性"><a href="#6-社会影响和局限性" class="headerlink" title="6. 社会影响和局限性"></a>6. 社会影响和局限性</h3><ul>
<li>论文讨论了个性化运动生成技术的潜在风险，包括滥用技术生成虚假内容和隐私问题。</li>
<li>指出了LEAD的局限性，包括在处理长文本描述和生成复杂动作时的挑战。</li>
</ul>
<p>总体而言，LEAD通过其创新的潜在对齐机制，在文本到运动生成领域提供了一种新的解决方案，能够生成更逼真、更符合文本描述的人类运动序列，并为个性化运动生成开辟了新的可能性。</p></p>
<p class="faq-q"><strong>Q</strong>: 想要进一步了解论文</p>
<p class="faq-a"><strong>A</strong>: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a href="http://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是LEAD: Latent Realignment for Human Motion Diffusion，链接是 https://arxiv.org/pdf/2410.14508 ，已有的FAQ链接是 https://papers.cool/arxiv/kimi?paper=2410.14508 。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false" target="_blank"><strong>这里 <i class="fa fa-external-link"></i></strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>

</details>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2321c479c4912317989e0f60a0e0af0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34cc5029417f28afdc1f3ce6da4100d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfdd0a90ef7c71fd8036f242ae1843c2.jpg" align="middle">
</details>



<h2 id="Versatile-Motion-Language-Models-for-Multi-Turn-Interactive-Agents"><a href="#Versatile-Motion-Language-Models-for-Multi-Turn-Interactive-Agents" class="headerlink" title="Versatile Motion Language Models for Multi-Turn Interactive Agents"></a>Versatile Motion Language Models for Multi-Turn Interactive Agents</h2><p><strong>Authors:Jeongeun Park, Sungjoon Choi, Sangdoo Yun</strong></p>
<p>Recent advancements in large language models (LLMs) have greatly enhanced their ability to generate natural and contextually relevant text, making AI interactions more human-like. However, generating and understanding interactive human-like motion, where two individuals engage in coordinated movements, remains a challenge due to the complexity of modeling these coordinated interactions. Furthermore, a versatile model is required to handle diverse interactive scenarios, such as chat systems that follow user instructions or adapt to their assigned role while adjusting interaction dynamics. To tackle this problem, we introduce VIM, short for the Versatile Interactive Motion language model, which integrates both language and motion modalities to effectively understand, generate, and control interactive motions in multi-turn conversational contexts. To address the scarcity of multi-turn interactive motion data, we introduce a synthetic dataset, INERT-MT2, where we utilize pre-trained models to create diverse instructional datasets with interactive motion. Our approach first trains a motion tokenizer that encodes interactive motions into residual discrete tokens. In the pretraining stage, the model learns to align motion and text representations with these discrete tokens. During the instruction fine-tuning stage, VIM adapts to multi-turn conversations using the INTER-MT2 dataset. We evaluate the versatility of our method across motion-related tasks, motion to text, text to motion, reaction generation, motion editing, and reasoning about motion sequences. The results highlight the versatility and effectiveness of proposed method in handling complex interactive motion synthesis. </p>
<blockquote>
<p>近期大型语言模型（LLM）的进展极大地提升了生成自然和语境相关文本的能力，使得人工智能交互更加人性化。然而，生成和理解交互式人类运动——两个人进行协调运动——仍然是一个挑战，原因在于对协调交互进行建模的复杂性。此外，需要一种通用模型来处理各种交互场景，例如在遵循用户指令或适应分配的角色时调整交互动力的聊天系统。为了解决这个问题，我们引入了VIM，即通用交互式运动语言模型的简称，它结合了语言和运动模式，以有效地理解、生成和控制多轮对话语境中的交互式运动。为了解决多轮交互式运动数据的稀缺问题，我们引入了一个合成数据集INERT-MT2，我们利用预训练模型创建具有交互式运动的各种指令数据集。我们的方法首先训练一个运动令牌化器，该令牌化器将交互式运动编码为残余离散令牌。在预训练阶段，模型学习使用这些离散令牌对齐运动和文本表示。在指令微调阶段，VIM适应多轮对话，使用INTER-MT2数据集。我们评估了我们的方法在动作相关任务、动作到文本、文本到动作、反应生成、动作编辑和运动序列推理方面的通用性。结果突出了该方法在处理复杂交互式运动合成中的通用性和有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05628v3">PDF</a> <a target="_blank" rel="noopener" href="https://vim-motion-language.github.io/">https://vim-motion-language.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的进展提升了生成自然、语境相关文本的能力，使得人工智能交互更加人性化。然而，生成和理解交互式的类人动作（如两人协同动作）仍存在挑战。为应对这一问题，我们推出了VIM（通用交互式动作语言模型），它结合了语言和动作模态，能在多轮对话语境中有效地理解、生成和控制交互式动作。我们利用预训练模型创建了INERT-MT2合成数据集，以应对多轮交互式动作数据的稀缺问题。VIM首先训练动作标记器，将交互式动作编码为剩余离散标记。在预训练阶段，模型学习用这些离散标记对齐动作和文本表示。在指令微调阶段，VIM使用INTER-MT2数据集适应多轮对话。我们在多个动作相关任务上评估了该方法的多功能性，包括动作转文本、文本转动作、反应生成、动作编辑和推理动作序列。结果突显了该方法在处理复杂交互式动作合成方面的多功能性和有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型的进步使得AI文本生成更加自然和语境化，提升了人机交互的人性化程度。</li>
<li>交互式类人动作生成和理解仍是人工智能领域的一大挑战，需要复杂建模。</li>
<li>VIM模型结合了语言和动作模态，能有效处理多种交互式场景，如遵循用户指令的聊天系统。</li>
<li>INERT-MT2合成数据集解决了多轮交互式动作数据的稀缺问题。</li>
<li>VIM通过训练动作标记器，将交互式动作编码为离散标记，促进动作和文本的相互理解。</li>
<li>模型的预训练阶段使其学习动作和文本的表示对齐，而在指令微调阶段则适应多轮对话。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05628">Cool Papers</a></strong> </p>
<details>
  <summary>点击展开 Cool Papers 详细信息</summary>
  <p class="faq-q"><strong>Q</strong>: 这篇论文试图解决什么问题？</p>
<p class="faq-a"><strong>A</strong>: 这篇论文试图解决的问题是如何生成和理解交互式的人类运动，尤其是在多轮对话中的协调运动。尽管大型语言模型（LLMs）在生成类人文本和理解复杂语言交互方面取得了进展，但现有的方法通常只关注单一任务，如文本到运动或运动到文本的翻译，并且只考虑单一动作，没有交互。这限制了代理处理涉及多个代理、复杂交互和多轮对话场景的能力。

<p>论文中提到的挑战包括：</p>
<ol>
<li><p><strong>多轮交互运动数据的稀缺性</strong>：缺乏包含两个个体之间交互运动以及多轮对话指令的数据集，这使得模型难以学习交互运动的细节和多轮动态。</p>
</li>
<li><p><strong>交互运动的复杂性</strong>：交互运动涉及两个人之间的协调动作，这要求模型能够理解和控制交互，允许用户提供指令、分配角色或修改行为。</p>
</li>
<li><p><strong>模型的多样性和灵活性</strong>：需要一个多功能的模型来处理多种交互场景，例如遵循用户指令的聊天系统，或在调整交互动态时适应分配的角色。</p>
</li>
</ol>
<p>为了解决这些问题，论文介绍了VIM（Versatile Interactive Motion language model），这是一个集成了语言和运动模式的模型，可以有效理解、生成和控制多轮对话背景下的交互运动。此外，论文还介绍了一个合成数据集INTER-MT2，旨在通过使用预训练模型创建多样化的指令数据集和交互运动，来解决多轮交互运动数据不足的问题。</p></p>
<p class="faq-q"><strong>Q</strong>: 有哪些相关研究？</p>
<p class="faq-a"><strong>A</strong>: 根据论文内容，相关研究包括以下几个方面：

<ol>
<li><p><strong>人类运动建模与控制</strong>：</p>
<ul>
<li>基于扩散的方法，例如MDM (Tevet et al., 2023)，FG-T2M (Wang et al., 2023)，和MotionDiffuse (Zhang et al., 2024a)，擅长从文本合成逼真的人类运动。</li>
<li>基于Transformer模型与向量量化，例如TM2T (Guo et al., 2022) 和 T2M-GPT (Zhang et al., 2023)，有效捕捉复杂的运动模式。</li>
<li>MoMASK (Guo et al., 2024a) 通过残差tokenizers提高运动粒度。</li>
<li>FineMoGEN (Zhang et al., 2024c) 基于用户指令提供细粒度的运动合成。</li>
<li>MEOs (Goel et al., 2024) 使用字幕和大型语言模型来识别帧和身体部位进行编辑，而MotionFix (Athanasiou et al., 2024) 则在源运动和编辑文本的条件下对扩散模型进行条件化，以实现无缝的运动编辑。</li>
</ul>
</li>
<li><p><strong>运动语言模型</strong>：</p>
<ul>
<li>MotionGPT (Jiang et al., 2023) 在统一框架上展示了运动理解和生成的多功能性。</li>
<li>MotionChain (Jiang et al., 2024) 引入了一个多轮对话系统，用于在对话上下文中解释和生成运动，包括图像输入。</li>
<li>AvatarGPT (Zhou et al., 2024) 集成了运动生成和规划能力。</li>
<li>M3-GPT (Luo et al., 2024) 模型了语音、音乐、文本和运动的互换性。</li>
</ul>
</li>
<li><p><strong>人与人之间交互运动建模</strong>：</p>
<ul>
<li>多人交互数据集例如Inter-X (Xu et al., 2024a) 和 InterHuman (Liang et al., 2024) 已被开发。</li>
<li>InterGEN (Xu et al., 2024a) 引入了扩散模型，具有空间约束损失。</li>
<li>PriorMDM (Shafir et al., 2024) 利用预训练的运动扩散模型和瘦通信块。</li>
<li>ReMoS (Ghosh et al., 2023) 合成反应运动，使用时空交叉注意力，而ReGenNet (Xu et al., 2024b) 则采用基于距离的交互损失的transformer模型来预测人类反应。</li>
</ul>
</li>
</ol>
<p>这些相关研究展示了在运动生成、控制、理解和交互式运动建模方面的最新进展，同时也指出了现有方法在处理多轮交互和复杂推理能力方面的局限性。论文提出的VIM模型和INTER-MT2数据集旨在通过结合这些领域的最新技术来解决这些挑战。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文如何解决这个问题？</p>
<p class="faq-a"><strong>A</strong>: 论文通过以下几个关键步骤解决多轮交互式运动建模和理解的问题：

<ol>
<li><p><strong>提出VIM模型</strong>：VIM（Versatile Interactive Motion language model）是一个多模态语言模型，集成了语言和运动模式，有效理解、生成和控制多轮对话背景下的交互运动。</p>
</li>
<li><p><strong>创建INTER-MT2数据集</strong>：为了解决多轮交互运动数据的稀缺性，论文提出了一个新的合成数据集INTER-MT2，它包含了多轮对话中的多样化指令和交互运动。</p>
</li>
<li><p><strong>设计训练流程</strong>：VIM模型的训练过程包括三个阶段：</p>
<ul>
<li><strong>运动标记器训练</strong>：训练一个运动标记器，将交互运动编码为残差离散标记。</li>
<li><strong>预训练</strong>：在预训练阶段，模型学习将运动和文本表示与这些离散标记对齐。</li>
<li><strong>指令微调</strong>：使用INTER-MT2数据集进行指令微调，以处理更复杂和多轮的指令。</li>
</ul>
</li>
<li><p><strong>多模态序列处理</strong>：VIM能够同时处理输入和输出两种模态（运动和文本），使用统一的词汇表来整合运动标记和文本标记。</p>
</li>
<li><p><strong>评估方法</strong>：论文提出了新的评估协议，通过运动相关任务（如运动到文本、文本到运动、反应生成、运动编辑和运动序列推理）来评估VIM的性能。</p>
</li>
<li><p><strong>交互式任务设计</strong>：论文设计了高级交互式运动任务，如运动推理和编辑，要求模型基于对话上下文预测过去或未来的事件，或对当前运动进行推理。</p>
</li>
<li><p><strong>实验验证</strong>：通过实验，论文评估了VIM在生成详细的基于运动的聊天回应、进行复杂推理以及在传统运动相关任务中的表现。</p>
</li>
<li><p><strong>消融研究</strong>：论文还进行了消融研究，比较了不同的运动标记器（如VQ-VAE与RQ-VAE）对模型性能的影响。</p>
</li>
</ol>
<p>通过这些方法，论文旨在提高模型在复杂交互式运动合成任务中的性能，并使其能够更好地理解和生成与多轮对话相关的交互式运动。</p></p>
<p class="faq-q"><strong>Q</strong>: 论文做了哪些实验？</p>
<p class="faq-a"><strong>A</strong>: 论文中进行的实验旨在评估VIM模型在处理多轮交互式运动任务方面的能力。具体实验包括：

<ol>
<li><p><strong>运动推理（Motion Reasoning）</strong>：</p>
<ul>
<li>通过预测交互式运动的过去或未来事件，或解释当前运动，评估模型的推理能力。</li>
<li>使用大型语言模型（LLMs）评估生成文本内容的一致性、准确性和自然性。</li>
<li>应用了诸如ROUGE-L、METEOR和MAUVE等计算语言指标来定量评估生成文本与INTER-MT2测试集中标记文本的相关性、准确性和自然性。</li>
</ul>
</li>
<li><p><strong>运动编辑（Motion Editing）</strong>：</p>
<ul>
<li>目标是基于用户指令修改参考运动，通过用户研究比较编辑后的运动样本。</li>
<li>使用5点Likert量表评估内容相似性、指令一致性和运动质量。</li>
<li>使用Frechet Inception Distance（FID）和平均每个关节位置误差（MPJPE）等数据驱动指标来评估生成编辑运动的质量。</li>
</ul>
</li>
<li><p><strong>传统运动相关任务</strong>：</p>
<ul>
<li>在InterHuman和Inter-X数据集的测试集上评估了运动到文本（M2T）、文本到运动（T2M）和反应生成任务。</li>
<li>报告了检索精度、FID、MPJPE等指标来评估模型性能。</li>
</ul>
</li>
<li><p><strong>消融研究（Ablation Studies）</strong>：</p>
<ul>
<li>对比了使用VQ-VAE和RQ-VAE作为运动标记器的模型变体。</li>
<li>评估了这些变体在运动推理、编辑、M2T、T2M和反应生成任务上的性能。</li>
</ul>
</li>
<li><p><strong>基线比较</strong>：</p>
<ul>
<li>将VIM与一些基线方法进行了比较，包括两阶段方法和修改过的MotionGPT模型。</li>
</ul>
</li>
<li><p><strong>定性结果</strong>：</p>
<ul>
<li>展示了VIM在运动编辑、运动推理、M2T、T2M和反应生成任务上的定性结果。</li>
</ul>
</li>
</ol>
<p>这些实验全面评估了VIM模型在多轮交互式运动理解、生成和编辑任务中的有效性和灵活性。通过与传统基线方法的比较，论文证明了VIM在处理复杂交互式运动合成中的优越性能。</p></p>
<p class="faq-q"><strong>Q</strong>: 有什么可以进一步探索的点？</p>
<p class="faq-a"><strong>A</strong>: 尽管论文提出的VIM模型在多轮交互式运动建模方面取得了显著进展，但仍有一些潜在的研究方向和改进空间可以进一步探索：

<ol>
<li><p><strong>数据集的多样性和规模</strong>：尽管INTER-MT2数据集为模型提供了多轮交互式运动的示例，但扩大数据集的规模和多样性可以进一步提高模型的泛化能力和鲁棒性。</p>
</li>
<li><p><strong>更复杂的交互场景</strong>：当前的研究主要关注两个个体之间的交互。未来的工作可以探索涉及更多参与者的复杂交互场景。</p>
</li>
<li><p><strong>个性化和适应性</strong>：模型可以进一步开发以更好地适应不同用户的具体需求和偏好。这可能涉及用户个性化和上下文感知的运动生成。</p>
</li>
<li><p><strong>实时性能和资源效率</strong>：研究如何优化模型以实现实时应用和更高效的计算资源使用，这对于实际应用尤为重要。</p>
</li>
<li><p><strong>细粒度的运动控制</strong>：虽然VIM能够处理交互运动，但进一步提高运动细节的控制能力，如精确的手势和面部表情，可以增加交互的真实感和自然性。</p>
</li>
<li><p><strong>多模态输入和输出</strong>：扩展模型以处理和生成除了文本和运动之外的其他模态，如音频或图像，可能会进一步增强交互体验。</p>
</li>
<li><p><strong>可解释性和透明度</strong>：提高模型决策过程的可解释性，帮助用户理解模型生成特定运动的原因，这对于建立用户信任和接受度至关重要。</p>
</li>
<li><p><strong>交互式应用的实验</strong>：在实际的交互式环境中测试VIM模型，例如虚拟现实、游戏或机器人，以评估其在现实世界条件下的性能。</p>
</li>
<li><p><strong>伦理和隐私问题</strong>：在处理涉及人类运动和交互的数据时，确保遵守伦理准则和隐私法规是至关重要的。</p>
</li>
<li><p><strong>错误处理和异常管理</strong>：研究模型在遇到错误或异常输入时的鲁棒性和恢复能力，提高系统的稳定性和可靠性。</p>
</li>
<li><p><strong>跨领域应用</strong>：探索VIM模型在其他领域的应用潜力，如健康监测、体育训练分析或社交互动研究。</p>
</li>
</ol>
<p>通过这些进一步的探索和研究，可以不断提升交互式运动语言模型的性能和应用范围。</p></p>
<p class="faq-q"><strong>Q</strong>: 总结一下论文的主要内容</p>
<p class="faq-a"><strong>A</strong>: 这篇论文的主要内容可以总结如下：

<ol>
<li><p><strong>问题定义</strong>：论文针对的是在多轮对话中生成和理解两个人之间交互式运动的问题。现有的大型语言模型虽然在文本生成和理解方面取得了进展，但在处理涉及多个代理、复杂交互和多轮对话的交互式运动方面存在挑战。</p>
</li>
<li><p><strong>VIM模型介绍</strong>：为了解决这个问题，论文提出了一个名为VIM（Versatile Interactive Motion language model）的模型。VIM模型集成了语言和运动两种模态，可以有效地理解、生成和控制多轮对话中的交互运动。</p>
</li>
<li><p><strong>INTER-MT2数据集</strong>：为了克服多轮交互运动数据的稀缺性，论文创建了一个合成数据集INTER-MT2，它包含了多轮对话中的多样化指令和交互运动。</p>
</li>
<li><p><strong>训练流程</strong>：VIM模型的训练分为三个阶段：</p>
<ul>
<li>运动标记器训练：训练一个运动标记器，将交互运动编码为残差离散标记。</li>
<li>预训练：在预训练阶段，模型学习将运动和文本表示与这些离散标记对齐。</li>
<li>指令微调：使用INTER-MT2数据集进行指令微调，以处理更复杂和多轮的指令。</li>
</ul>
</li>
<li><p><strong>评估方法</strong>：论文提出了新的评估协议，通过运动相关任务（如运动到文本、文本到运动、反应生成、运动编辑和运动序列推理）来评估VIM的性能。</p>
</li>
<li><p><strong>实验结果</strong>：通过一系列实验，论文证明了VIM在生成详细的基于运动的聊天回应、进行复杂推理以及在传统运动相关任务中的表现优于现有方法。</p>
</li>
<li><p><strong>消融研究</strong>：论文还进行了消融研究，比较了不同的运动标记器（如VQ-VAE与RQ-VAE）对模型性能的影响。</p>
</li>
<li><p><strong>结论</strong>：VIM模型在多轮交互式运动建模方面展现了强大的性能，能够理解和生成与多轮对话相关的交互式运动。</p>
</li>
<li><p><strong>局限性和未来工作</strong>：论文讨论了模型的局限性，如处理复杂或未见过动作的能力有限，序列长度过长可能导致效率问题，以及个性化和可解释性方面的挑战。论文还提出了一些未来可能的研究方向。</p>
</li>
</ol>
<p>整体而言，这篇论文提出了一个创新的模型来处理多轮交互式运动的建模和理解问题，并通过实验验证了其有效性。</p></p>
<p class="faq-q"><strong>Q</strong>: 想要进一步了解论文</p>
<p class="faq-a"><strong>A</strong>: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a href="http://kimi.moonshot.cn/_prefill_chat?prefill_prompt=我们要讨论的论文是Versatile Motion Langauge Models for Multi-Turn Interactive Agents，链接是 https://arxiv.org/pdf/2410.05628 ，已有的FAQ链接是 https://papers.cool/arxiv/kimi?paper=2410.05628 。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false" target="_blank"><strong>这里 <i class="fa fa-external-link"></i></strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>

</details>


<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-46182a14b923ab7f9512e5512c8b1d8d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa6dffabfc26f9112a4c5fd87dbf468b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8febdf6753ab60fb0cd43a2e6ce10d52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b828e3f80e631acd82bd6348bb315ed6.jpg" align="middle">
</details>




                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-12-15/Text-to-Motion/">https://kedreamix.github.io/Talk2Paper/Paper/2024-12-15/Text-to-Motion/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Text-to-Motion/">
                                    <span class="chip bg-color">Text-to-Motion</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-16/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cb8bb46eff23ad43643fd7d60c871e9e.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2024-12-16  On-Device Emoji Classifier Trained with GPT-based Data Augmentation for   a Mobile Keyboard
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-12-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-12-15/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4b925361d082a9da8d98f25841024d8b.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2024-12-15  NestedMorph Enhancing Deformable Medical Image Registration with Nested   Attention Mechanisms
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-12-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
