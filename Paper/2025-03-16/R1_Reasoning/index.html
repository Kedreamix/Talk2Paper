<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-16  GoT Unleashing Reasoning Capability of Multimodal Large Language Model   for Visual Generation and Editing">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-7036ff4da4fb676bcedd4eeb47950e21.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-16-æ›´æ–°"><a href="#2025-03-16-æ›´æ–°" class="headerlink" title="2025-03-16 æ›´æ–°"></a>2025-03-16 æ›´æ–°</h1><h2 id="GoT-Unleashing-Reasoning-Capability-of-Multimodal-Large-Language-Model-for-Visual-Generation-and-Editing"><a href="#GoT-Unleashing-Reasoning-Capability-of-Multimodal-Large-Language-Model-for-Visual-Generation-and-Editing" class="headerlink" title="GoT: Unleashing Reasoning Capability of Multimodal Large Language Model   for Visual Generation and Editing"></a>GoT: Unleashing Reasoning Capability of Multimodal Large Language Model   for Visual Generation and Editing</h2><p><strong>Authors:Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Xihui Liu, Hongsheng Li</strong></p>
<p>Current image generation and editing methods primarily process textual prompts as direct inputs without reasoning about visual composition and explicit operations. We present Generation Chain-of-Thought (GoT), a novel paradigm that enables generation and editing through an explicit language reasoning process before outputting images. This approach transforms conventional text-to-image generation and editing into a reasoning-guided framework that analyzes semantic relationships and spatial arrangements. We define the formulation of GoT and construct large-scale GoT datasets containing over 9M samples with detailed reasoning chains capturing semantic-spatial relationships. To leverage the advantages of GoT, we implement a unified framework that integrates Qwen2.5-VL for reasoning chain generation with an end-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance Module. Experiments show our GoT framework achieves excellent performance on both generation and editing tasks, with significant improvements over baselines. Additionally, our approach enables interactive visual generation, allowing users to explicitly modify reasoning steps for precise image adjustments. GoT pioneers a new direction for reasoning-driven visual generation and editing, producing images that better align with human intent. To facilitate future research, we make our datasets, code, and pretrained models publicly available at <a target="_blank" rel="noopener" href="https://github.com/rongyaofang/GoT">https://github.com/rongyaofang/GoT</a>. </p>
<blockquote>
<p>å½“å‰å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹æ³•ä¸»è¦å°†æ–‡æœ¬æç¤ºä½œä¸ºç›´æ¥è¾“å…¥è¿›è¡Œå¤„ç†ï¼Œè€Œæ²¡æœ‰å¯¹è§†è§‰æ„å›¾å’Œæ˜ç¡®æ“ä½œè¿›è¡Œæ¨ç†ã€‚æˆ‘ä»¬æå‡ºäº†â€œæ€ç»´é“¾ç”Ÿæˆâ€ï¼ˆGeneration Chain-of-Thoughtï¼ŒGoTï¼‰è¿™ä¸€æ–°å‹èŒƒå¼ï¼Œå®ƒèƒ½å¤Ÿåœ¨è¾“å‡ºå›¾åƒä¹‹å‰ï¼Œé€šè¿‡æ˜ç¡®çš„è¯­è¨€æ¨ç†è¿‡ç¨‹å®ç°å›¾åƒçš„ç”Ÿæˆå’Œç¼–è¾‘ã€‚è¿™ç§æ–¹æ³•å°†ä¼ ç»Ÿçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œç¼–è¾‘è¿‡ç¨‹è½¬å˜ä¸ºä¸€ä¸ªä»¥æ¨ç†å¼•å¯¼çš„åˆ†ææ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåˆ†æè¯­ä¹‰å…³ç³»å’Œç©ºé—´å¸ƒå±€ã€‚æˆ‘ä»¬å®šä¹‰äº†GoTçš„å…¬å¼ï¼Œå¹¶æ„å»ºäº†å¤§è§„æ¨¡çš„GoTæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡900ä¸‡ä¸ªæ ·æœ¬ï¼Œè¯¦ç»†çš„æ¨ç†é“¾æ•æ‰äº†è¯­ä¹‰-ç©ºé—´å…³ç³»ã€‚ä¸ºäº†åˆ©ç”¨GoTçš„ä¼˜åŠ¿ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†ç”¨äºæ¨ç†é“¾ç”Ÿæˆçš„Qwen2.5-VLä¸ç«¯åˆ°ç«¯çš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶ç”±æˆ‘ä»¬æ–°é¢–çš„è¯­ä¹‰-ç©ºé—´å¼•å¯¼æ¨¡å—å¢å¼ºã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„GoTæ¡†æ¶åœ¨ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå®ç°äº¤äº’å¼è§†è§‰ç”Ÿæˆï¼Œè®©ç”¨æˆ·èƒ½å¤Ÿæ˜ç¡®ä¿®æ”¹æ¨ç†æ­¥éª¤ä»¥è¿›è¡Œç²¾ç¡®å›¾åƒè°ƒæ•´ã€‚GoTå¼€åˆ›äº†ä¸€ä¸ªä»¥æ¨ç†é©±åŠ¨è§†è§‰ç”Ÿæˆå’Œç¼–è¾‘çš„æ–°æ–¹å‘ï¼Œç”Ÿæˆçš„å›¾åƒæ›´å¥½åœ°ç¬¦åˆäººç±»æ„å›¾ã€‚ä¸ºäº†æ–¹ä¾¿æœªæ¥ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/rongyaofang/GoT%E5%85%AC%E5%BC%80%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%81%E4%BB%A3%E7%A0%81%E5%92%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E3%80%82">https://github.com/rongyaofang/GoTå…¬å¼€äº†æˆ‘ä»¬çš„æ•°æ®é›†ã€ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10639v1">PDF</a> Dataset and models are released in <a target="_blank" rel="noopener" href="https://github.com/rongyaofang/GoT">https://github.com/rongyaofang/GoT</a></p>
<p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºäº†ä¸€ç§åä¸ºâ€œæ€ç»´é“¾ç”Ÿæˆâ€ï¼ˆGoTï¼‰çš„æ–°èŒƒå¼ï¼Œå®ƒä½¿å›¾åƒç”Ÿæˆå’Œç¼–è¾‘èƒ½å¤Ÿé€šè¿‡åœ¨è¾“å‡ºå›¾åƒä¹‹å‰è¿›è¡Œæ˜ç¡®çš„é€»è¾‘æ¨ç†è¿‡ç¨‹æ¥å®ç°ã€‚è¯¥æ–¹æ³•å°†ä¼ ç»Ÿçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’Œç¼–è¾‘è½¬å˜ä¸ºç”±æ¨ç†å¼•å¯¼çš„åˆ†æè¯­ä¹‰å…³ç³»å’Œç©ºé—´æ’åˆ—çš„æ¡†æ¶ã€‚å®éªŒè¡¨æ˜ï¼ŒGoTæ¡†æ¶åœ¨ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶ä¸”ä¸åŸºçº¿ç›¸æ¯”æœ‰æ˜¾è‘—æ”¹å–„ã€‚æ­¤å¤–ï¼ŒGoTå¼€å¯äº†æ¨ç†é©±åŠ¨è§†è§‰ç”Ÿæˆå’Œç¼–è¾‘çš„æ–°æ–¹å‘ï¼Œç”Ÿæˆçš„å›¾åƒæ›´ç¬¦åˆäººç±»æ„å›¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GoTæ˜¯ä¸€ç§æ–°çš„å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹æ³•ï¼Œå®ƒé€šè¿‡æ˜ç¡®çš„è¯­è¨€æ¨ç†è¿‡ç¨‹å®ç°å›¾åƒè¾“å‡ºã€‚</li>
<li>GoTå°†æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆå’Œç¼–è¾‘è½¬å˜ä¸ºæ¨ç†å¼•å¯¼çš„æ¡†æ¶ï¼Œåˆ†æè¯­ä¹‰å…³ç³»å’Œç©ºé—´æ’åˆ—ã€‚</li>
<li>GoTæ¡†æ¶åŒ…æ‹¬å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡900ä¸‡ä¸ªæ ·æœ¬çš„è¯¦ç»†æ¨ç†é“¾ï¼Œæ•æ‰è¯­ä¹‰-ç©ºé—´å…³ç³»ã€‚</li>
<li>GoTæ¡†æ¶é›†æˆäº†Qwen2.5-VLè¿›è¡Œæ¨ç†é“¾ç”Ÿæˆï¼Œå¹¶é€šè¿‡ç«¯åˆ°ç«¯çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒç”Ÿæˆã€‚</li>
<li>GoTæ–¹æ³•å…·æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶åœ¨ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šã€‚</li>
<li>GoTæ–¹æ³•å…è®¸ç”¨æˆ·æ˜¾å¼ä¿®æ”¹æ¨ç†æ­¥éª¤ï¼Œå®ç°ç²¾ç¡®çš„å›¾åƒè°ƒæ•´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10639">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a045f0153212d728b24a826020406583.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5a9d5272a621f3db6277ffed7e2614d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b1bba85b6372539976092d24981b24e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-90e259d326292169340539650ba89083.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="TH-Bench-Evaluating-Evading-Attacks-via-Humanizing-AI-Text-on-Machine-Generated-Text-Detectors"><a href="#TH-Bench-Evaluating-Evading-Attacks-via-Humanizing-AI-Text-on-Machine-Generated-Text-Detectors" class="headerlink" title="TH-Bench: Evaluating Evading Attacks via Humanizing AI Text on   Machine-Generated Text Detectors"></a>TH-Bench: Evaluating Evading Attacks via Humanizing AI Text on   Machine-Generated Text Detectors</h2><p><strong>Authors:Jingyi Zheng, Junfeng Wang, Zhen Sun, Wenhan Dong, Yule Liu, Xinlei He</strong></p>
<p>As Large Language Models (LLMs) advance, Machine-Generated Texts (MGTs) have become increasingly fluent, high-quality, and informative. Existing wide-range MGT detectors are designed to identify MGTs to prevent the spread of plagiarism and misinformation. However, adversaries attempt to humanize MGTs to evade detection (named evading attacks), which requires only minor modifications to bypass MGT detectors. Unfortunately, existing attacks generally lack a unified and comprehensive evaluation framework, as they are assessed using different experimental settings, model architectures, and datasets. To fill this gap, we introduce the Text-Humanization Benchmark (TH-Bench), the first comprehensive benchmark to evaluate evading attacks against MGT detectors. TH-Bench evaluates attacks across three key dimensions: evading effectiveness, text quality, and computational overhead. Our extensive experiments evaluate 6 state-of-the-art attacks against 13 MGT detectors across 6 datasets, spanning 19 domains and generated by 11 widely used LLMs. Our findings reveal that no single evading attack excels across all three dimensions. Through in-depth analysis, we highlight the strengths and limitations of different attacks. More importantly, we identify a trade-off among three dimensions and propose two optimization insights. Through preliminary experiments, we validate their correctness and effectiveness, offering potential directions for future research. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ï¼Œæœºå™¨ç”Ÿæˆæ–‡æœ¬ï¼ˆMGTï¼‰çš„æµç•…åº¦ã€è´¨é‡å’Œä¿¡æ¯é‡è¶Šæ¥è¶Šé«˜ã€‚ç°æœ‰çš„å¹¿æ³›MGTæ£€æµ‹å™¨æ—¨åœ¨è¯†åˆ«MGTï¼Œä»¥é˜²æ­¢æŠ„è¢­å’Œè¯¯å¯¼ä¿¡æ¯çš„ä¼ æ’­ã€‚ç„¶è€Œï¼Œæ”»å‡»è€…è¯•å›¾ä½¿MGTäººæ€§åŒ–ä»¥èº²é¿æ£€æµ‹ï¼ˆç§°ä¸ºèº²é¿æ”»å‡»ï¼‰ï¼Œåªéœ€è¿›è¡Œå¾®å°çš„ä¿®æ”¹å°±èƒ½ç»•è¿‡MGTæ£€æµ‹å™¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ”»å‡»é€šå¸¸ç¼ºä¹ç»Ÿä¸€å’Œå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œå› ä¸ºå®ƒä»¬çš„è¯„ä¼°ä½¿ç”¨çš„æ˜¯ä¸åŒçš„å®éªŒè®¾ç½®ã€æ¨¡å‹æ¶æ„å’Œæ•°æ®é›†ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–‡æœ¬äººæ€§åŒ–åŸºå‡†æµ‹è¯•ï¼ˆTH-Benchï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢è¯„ä¼°é’ˆå¯¹MGTæ£€æµ‹å™¨çš„èº²é¿æ”»å‡»æ€§èƒ½çš„åŸºå‡†æµ‹è¯•ã€‚TH-Benchä»ä¸‰ä¸ªå…³é”®ç»´åº¦è¯„ä¼°æ”»å‡»ï¼šèº²é¿æœ‰æ•ˆæ€§ã€æ–‡æœ¬è´¨é‡å’Œè®¡ç®—å¼€é”€ã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒè¯„ä¼°äº†é’ˆå¯¹13ç§MGTæ£€æµ‹å™¨çš„6ç§æœ€å…ˆè¿›çš„æ”»å‡»æ–¹æ³•ï¼Œè·¨è¶Šæ¶µç›–æ–‡æœ¬ç”ŸæˆçœŸå®æ€§çš„19ä¸ªé¢†åŸŸç”Ÿæˆçš„ç”±åŒ…å«ä¸»æµå¹¿æ³›ä½¿ç”¨çš„å…­ç§LLMç”Ÿæˆã€‚æˆ‘ä»¬å‘ç°æ²¡æœ‰ä»»ä½•ä¸€ç§èº²é¿æ”»å‡»åœ¨æ‰€æœ‰ä¸‰ä¸ªç»´åº¦ä¸Šéƒ½è¡¨ç°å‡ºè‰²ã€‚é€šè¿‡æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬çªå‡ºäº†å„ç§æ”»å‡»çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸‰ä¸ªç»´åº¦ä¹‹é—´çš„æƒè¡¡å¹¶æå‡ºäº†ä¸¤ä¸ªä¼˜åŒ–è§è§£ã€‚é€šè¿‡åˆæ­¥å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†å®ƒä»¬çš„æ­£ç¡®æ€§å’Œæœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ½œåœ¨æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08708v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•ï¼Œæœºå™¨ç”Ÿæˆæ–‡æœ¬ï¼ˆMGTï¼‰çš„æµç•…æ€§ã€è´¨é‡å’Œä¿¡æ¯é‡ä¸æ–­æå‡ã€‚ç°æœ‰çš„å¹¿æ³›MGTæ£€æµ‹å™¨æ—¨åœ¨é˜²æ­¢æŠ„è¢­å’Œè¯¯å¯¼ä¿¡æ¯çš„ä¼ æ’­ã€‚ç„¶è€Œï¼Œæ”»å‡»è€…è¯•å›¾é€šè¿‡ä½¿æœºå™¨ç”Ÿæˆæ–‡æœ¬æ›´å…·äººæ€§åŒ–ä»¥èº²é¿æ£€æµ‹ï¼ˆç§°ä¸ºèº²é¿æ”»å‡»ï¼‰ï¼Œä»…éœ€è¦å¾®å°ä¿®æ”¹å³å¯ç»•è¿‡MGTæ£€æµ‹å™¨ã€‚ä¸ºäº†å¡«è¡¥ç°æœ‰ç ”ç©¶ç¼ºå°‘ç»Ÿä¸€å…¨é¢çš„è¯„ä¼°æ¡†æ¶çš„ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†æ–‡æœ¬äººæ€§åŒ–åŸºå‡†æµ‹è¯•ï¼ˆTH-Benchï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢è¯„ä¼°èº²é¿æ”»å‡»å¯¹MGTæ£€æµ‹å™¨å½±å“çš„åŸºå‡†æµ‹è¯•ã€‚TH-Benchä»ä¸‰ä¸ªå…³é”®ç»´åº¦è¯„ä¼°æ”»å‡»ï¼šèº²é¿æ•ˆæœã€æ–‡æœ¬è´¨é‡å’Œè®¡ç®—å¼€é”€ã€‚é€šè¿‡å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯„ä¼°äº†6ç§æœ€å…ˆè¿›çš„æ”»å‡»åœ¨13ä¸ªMGTæ£€æµ‹å™¨ä¸Šçš„è¡¨ç°ï¼Œæ¶‰åŠ6ä¸ªæ•°æ®é›†ã€æ¶µç›–19ä¸ªé¢†åŸŸå¹¶ç”±11ç§å¹¿æ³›ä½¿ç”¨çš„LLMç”Ÿæˆã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œæ²¡æœ‰ä»»ä½•ä¸€ç§èº²é¿æ”»å‡»èƒ½åœ¨æ‰€æœ‰ä¸‰ä¸ªç»´åº¦ä¸Šéƒ½è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚é€šè¿‡æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬çªå‡ºäº†ä¸åŒæ”»å‡»çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸‰ä¸ªç»´åº¦ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶æå‡ºäº†ä¸¤ç§ä¼˜åŒ–è§è§£ã€‚åˆæ­¥å®éªŒéªŒè¯äº†å®ƒä»¬çš„æ­£ç¡®æ€§å’Œæœ‰æ•ˆæ€§ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ½œåœ¨æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä½¿å¾—æœºå™¨ç”Ÿæˆæ–‡æœ¬ï¼ˆMGTï¼‰è¶Šæ¥è¶Šæµç•…ã€é«˜è´¨é‡å’Œå¯Œæœ‰ä¿¡æ¯é‡ã€‚</li>
<li>ç°æœ‰çš„MGTæ£€æµ‹å™¨æ—¨åœ¨é˜²æ­¢æŠ„è¢­å’Œè¯¯å¯¼ä¿¡æ¯çš„ä¼ æ’­ï¼Œä½†æ”»å‡»è€…é€šè¿‡ä½¿MGTæ›´å…·äººæ€§åŒ–ä»¥èº²é¿æ£€æµ‹ã€‚</li>
<li>æ–‡æœ¬äººæ€§åŒ–åŸºå‡†æµ‹è¯•ï¼ˆTH-Benchï¼‰æ˜¯é¦–ä¸ªå…¨é¢è¯„ä¼°èº²é¿æ”»å‡»å¯¹MGTæ£€æµ‹å™¨å½±å“çš„åŸºå‡†ï¼Œä»èº²é¿æ•ˆæœã€æ–‡æœ¬è´¨é‡å’Œè®¡ç®—å¼€é”€ä¸‰ä¸ªå…³é”®ç»´åº¦è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œæ²¡æœ‰ä¸€ç§èº²é¿æ”»å‡»èƒ½åœ¨æ‰€æœ‰ç»´åº¦ä¸Šéƒ½è¡¨ç°æœ€å¥½ã€‚</li>
<li>ä¸åŒæ”»å‡»æ–¹æ³•æœ‰å…¶ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚</li>
<li>å­˜åœ¨ä¸‰ä¸ªç»´åº¦ä¹‹é—´çš„æƒè¡¡ï¼Œéœ€è¦ä¼˜åŒ–ä»¥æ‰¾åˆ°æœ€ä½³è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08708">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cd3c1270af873676d76f1859215598e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-538c52b8cb7bf441bea7dfac1ade9c50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1c921d9325137bfa404d018e6c14e54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-297c710826289c0d475defe9b0bf9d12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d928f284f321a9f20ae340a75c1a519d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ecc53315f85ed4856bc0217dd7063b2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ProtTeX-Structure-In-Context-Reasoning-and-Editing-of-Proteins-with-Large-Language-Models"><a href="#ProtTeX-Structure-In-Context-Reasoning-and-Editing-of-Proteins-with-Large-Language-Models" class="headerlink" title="ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with   Large Language Models"></a>ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with   Large Language Models</h2><p><strong>Authors:Zicheng Ma, Chuanliu Fan, Zhicong Wang, Zhenyu Chen, Xiaohan Lin, Yanheng Li, Shihao Feng, Jun Zhang, Ziqiang Cao, Yi Qin Gao</strong></p>
<p>Large language models have made remarkable progress in the field of molecular science, particularly in understanding and generating functional small molecules. This success is largely attributed to the effectiveness of molecular tokenization strategies. In protein science, the amino acid sequence serves as the sole tokenizer for LLMs. However, many fundamental challenges in protein science are inherently structure-dependent. The absence of structure-aware tokens significantly limits the capabilities of LLMs for comprehensive biomolecular comprehension and multimodal generation. To address these challenges, we introduce a novel framework, ProtTeX, which tokenizes the protein sequences, structures, and textual information into a unified discrete space. This innovative approach enables joint training of the LLM exclusively through the Next-Token Prediction paradigm, facilitating multimodal protein reasoning and generation. ProtTeX enables general LLMs to perceive and process protein structures through sequential text input, leverage structural information as intermediate reasoning components, and generate or manipulate structures via sequential text output. Experiments demonstrate that our model achieves significant improvements in protein function prediction, outperforming the state-of-the-art domain expert model with a twofold increase in accuracy. Our framework enables high-quality conformational generation and customizable protein design. For the first time, we demonstrate that by adopting the standard training and inference pipelines from the LLM domain, ProtTeX empowers decoder-only LLMs to effectively address diverse spectrum of protein-related tasks. </p>
<blockquote>
<p>åœ¨åˆ†å­ç§‘å­¦é¢†åŸŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç†è§£å’Œç”ŸæˆåŠŸèƒ½æ€§å°åˆ†å­æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚è¿™ä¸€æˆåŠŸåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå½’åŠŸäºåˆ†å­æ ‡è®°ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚åœ¨è›‹ç™½è´¨ç§‘å­¦é¢†åŸŸï¼Œæ°¨åŸºé…¸åºåˆ—æ˜¯LLMå”¯ä¸€çš„åˆ†è¯å™¨ã€‚ç„¶è€Œï¼Œè›‹ç™½è´¨ç§‘å­¦ä¸­çš„è®¸å¤šåŸºæœ¬æŒ‘æˆ˜æœ¬è´¨ä¸Šæ˜¯ç»“æ„ä¾èµ–çš„ã€‚ç¼ºä¹ç»“æ„æ„ŸçŸ¥æ ‡è®°æ˜¾è‘—é™åˆ¶äº†LLMè¿›è¡Œå…¨é¢ç”Ÿç‰©åˆ†å­ç†è§£å’Œå¤šæ¨¡å¼ç”Ÿæˆçš„èƒ½åŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹æ¡†æ¶ProtTeXï¼Œå®ƒå°†è›‹ç™½è´¨åºåˆ—ã€ç»“æ„å’Œæ–‡æœ¬ä¿¡æ¯æ ‡è®°ä¸ºç»Ÿä¸€çš„ç¦»æ•£ç©ºé—´ã€‚è¿™ç§åˆ›æ–°æ–¹æ³•é€šè¿‡ä»…é€šè¿‡ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹èŒƒå¼æ¥è”åˆè®­ç»ƒLLMï¼Œä¿ƒè¿›å¤šæ¨¡å¼è›‹ç™½è´¨æ¨ç†å’Œç”Ÿæˆã€‚ProtTeXä½¿é€šç”¨LLMèƒ½å¤Ÿé€šè¿‡é¡ºåºæ–‡æœ¬è¾“å…¥æ„ŸçŸ¥å’Œå¤„ç†è›‹ç™½è´¨ç»“æ„ï¼Œåˆ©ç”¨ç»“æ„ä¿¡æ¯ä½œä¸ºä¸­é—´æ¨ç†æˆåˆ†ï¼Œå¹¶é€šè¿‡é¡ºåºæ–‡æœ¬è¾“å‡ºç”Ÿæˆæˆ–æ“ä½œç»“æ„ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œä¼˜äºæœ€å…ˆè¿›çš„é¢†åŸŸä¸“å®¶æ¨¡å‹ï¼Œå‡†ç¡®æ€§æé«˜äº†ä¸¤å€ã€‚æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„ç»“æ„ç”Ÿæˆå’Œå¯å®šåˆ¶çš„è›‹ç™½è´¨è®¾è®¡ã€‚é¦–æ¬¡å±•ç¤ºäº†é€šè¿‡é‡‡ç”¨LLMé¢†åŸŸçš„æ ‡å‡†è®­ç»ƒå’Œæ¨ç†ç®¡é“ï¼ŒProtTeXä½¿ä»…è§£ç LLMèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³å¤šæ ·åŒ–çš„è›‹ç™½è´¨ç›¸å…³ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08179v3">PDF</a> 26 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ†å­ç§‘å­¦é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶åœ¨ç†è§£å’Œç”ŸæˆåŠŸèƒ½æ€§å°åˆ†å­æ–¹é¢ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ¡†æ¶ProtTeXï¼Œå°†è›‹ç™½è´¨åºåˆ—ã€ç»“æ„å’Œæ–‡æœ¬ä¿¡æ¯ç»Ÿä¸€è½¬åŒ–ä¸ºç¦»æ•£ç©ºé—´ï¼Œä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ„ŸçŸ¥å’Œå¤„ç†è›‹ç™½è´¨ç»“æ„ï¼Œå®ç°å¤šæ¨¡æ€è›‹ç™½è´¨æ¨ç†å’Œç”Ÿæˆã€‚è¯¥æ¡†æ¶å®ç°äº†è›‹ç™½è´¨ç»“æ„çš„é¡ºåºæ–‡æœ¬è¾“å…¥å’Œè¾“å‡ºï¼Œæé«˜äº†è›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œå¹¶å®ç°äº†é«˜è´¨é‡çš„æ„è±¡ç”Ÿæˆå’Œå¯å®šåˆ¶çš„è›‹ç™½è´¨è®¾è®¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ†å­ç§‘å­¦é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶åœ¨ç†è§£å’Œç”ŸæˆåŠŸèƒ½æ€§å°åˆ†å­æ–¹é¢ã€‚</li>
<li>è›‹ç™½è´¨ç§‘å­¦ä¸­çš„è®¸å¤šåŸºæœ¬æŒ‘æˆ˜ä¸ç»“æ„å¯†åˆ‡ç›¸å…³ï¼Œä½†è¯­è¨€æ¨¡å‹åœ¨ç»“æ„æ„ŸçŸ¥æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>ProtTeXæ¡†æ¶å°†è›‹ç™½è´¨åºåˆ—ã€ç»“æ„å’Œæ–‡æœ¬ä¿¡æ¯ç»Ÿä¸€è½¬åŒ–ä¸ºç¦»æ•£ç©ºé—´ï¼Œè§£å†³è¯­è¨€æ¨¡å‹çš„ç»“æ„æ„ŸçŸ¥å±€é™æ€§ã€‚</li>
<li>ProtTeXæ¡†æ¶å®ç°äº†å¤šæ¨¡æ€è›‹ç™½è´¨æ¨ç†å’Œç”Ÿæˆï¼Œé€šè¿‡é¡ºåºæ–‡æœ¬è¾“å…¥å’Œè¾“å‡ºæ¥å¤„ç†è›‹ç™½è´¨ç»“æ„ã€‚</li>
<li>è¯¥æ¡†æ¶æé«˜äº†è›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œå¹¶å®ç°äº†é«˜è´¨é‡çš„æ„è±¡ç”Ÿæˆå’Œå¯å®šåˆ¶çš„è›‹ç™½è´¨è®¾è®¡ã€‚</li>
<li>ProtTeXæ¡†æ¶é‡‡ç”¨æ ‡å‡†è®­ç»ƒå’Œæ¨ç†ç®¡é“ï¼Œä½¿è§£ç å™¨ä»…å¤§å‹è¯­è¨€æ¨¡å‹å°±èƒ½æœ‰æ•ˆå¤„ç†å„ç§è›‹ç™½è´¨ç›¸å…³ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08179">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4383a8376a851ba85d8d08fbacecd7a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db0e3482fc07cdac6ee38b638c29c236.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Toward-Stable-World-Models-Measuring-and-Addressing-World-Instability-in-Generative-Environments"><a href="#Toward-Stable-World-Models-Measuring-and-Addressing-World-Instability-in-Generative-Environments" class="headerlink" title="Toward Stable World Models: Measuring and Addressing World Instability   in Generative Environments"></a>Toward Stable World Models: Measuring and Addressing World Instability   in Generative Environments</h2><p><strong>Authors:Soonwoo Kwon, Jin-Young Kim, Hyojun Go, Kyungjune Baek</strong></p>
<p>We present a novel study on enhancing the capability of preserving the content in world models, focusing on a property we term World Stability. Recent diffusion-based generative models have advanced the synthesis of immersive and realistic environments that are pivotal for applications such as reinforcement learning and interactive game engines. However, while these models excel in quality and diversity, they often neglect the preservation of previously generated scenes over timeâ€“a shortfall that can introduce noise into agent learning and compromise performance in safety-critical settings. In this work, we introduce an evaluation framework that measures world stability by having world models perform a sequence of actions followed by their inverses to return to their initial viewpoint, thereby quantifying the consistency between the starting and ending observations. Our comprehensive assessment of state-of-the-art diffusion-based world models reveals significant challenges in achieving high world stability. Moreover, we investigate several improvement strategies to enhance world stability. Our results underscore the importance of world stability in world modeling and provide actionable insights for future research in this domain. </p>
<blockquote>
<p>æˆ‘ä»¬é’ˆå¯¹æé«˜ä¸–ç•Œæ¨¡å‹ä¸­å†…å®¹ä¿å­˜èƒ½åŠ›è¿™ä¸€é¢†åŸŸè¿›è¡Œäº†ä¸€é¡¹æ–°å‹ç ”ç©¶ï¼Œé‡ç‚¹åœ¨äºæˆ‘ä»¬æ‰€ç§°çš„â€œä¸–ç•Œç¨³å®šæ€§â€è¿™ä¸€å±æ€§ã€‚è¿‘æœŸåŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹åœ¨åˆæˆæ²‰æµ¸å¼å’Œé€¼çœŸçš„ç¯å¢ƒæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œè¿™å¯¹äºå¼ºåŒ–å­¦ä¹ å’Œäº¤äº’å¼æ¸¸æˆå¼•æ“ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå°½ç®¡è¿™äº›æ¨¡å‹åœ¨è´¨é‡å’Œå¤šæ ·æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½è§†äº†éšæ—¶é—´æ¨ç§»å¯¹å…ˆå‰ç”Ÿæˆåœºæ™¯çš„ä¿ç•™â€”â€”è¿™ä¸€ç¼ºé™·å¯èƒ½ä¼šä¸ºä»£ç†å­¦ä¹ å¼•å…¥å™ªå£°ï¼Œå¹¶åœ¨å®‰å…¨å…³é”®è®¾ç½®ä¸­å½±å“æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡è®©ä¸–ç•Œæ¨¡å‹æ‰§è¡Œä¸€ç³»åˆ—åŠ¨ä½œåŠå…¶é€†åŠ¨ä½œæ¥è¿”å›åˆ°å…¶åˆå§‹è§‚ç‚¹ï¼Œä»è€Œè¡¡é‡ä¸–ç•Œç¨³å®šæ€§ï¼Œè¿›è€Œé‡åŒ–åˆå§‹è§‚æµ‹å’Œæœ€ç»ˆè§‚æµ‹ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„åŸºäºæ‰©æ•£çš„ä¸–ç•Œæ¨¡å‹çš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼Œå®ç°é«˜ä¸–ç•Œç¨³å®šæ€§å­˜åœ¨é‡å¤§æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†å¤šç§æ”¹è¿›ç­–ç•¥æ¥æé«˜ä¸–ç•Œç¨³å®šæ€§ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†ä¸–ç•Œç¨³å®šæ€§åœ¨ä¸–ç•Œå»ºæ¨¡ä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶æä¾›äº†å¯æ“ä½œçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08122v1">PDF</a> Preprint</p>
<p><strong>Summary</strong><br>ä¸–ç•Œæ¨¡å‹å†…å®¹ä¿ç•™èƒ½åŠ›æå‡ç ”ç©¶çš„æ–°æˆæœï¼Œé‡ç‚¹åœ¨äºå®ç°â€œä¸–ç•Œç¨³å®šæ€§â€ã€‚ç°æœ‰çš„æ‰©æ•£å¼ç”Ÿæˆæ¨¡å‹è™½ç„¶èƒ½å¤Ÿåˆæˆæ²‰æµ¸å¼çš„çœŸå®ç¯å¢ƒï¼Œä½†å¯¹äºç»´æŒå…ˆå‰ç”Ÿæˆçš„åœºæ™¯ç¨³å®šæ€§æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚æœ¬ç ”ç©¶é€šè¿‡è®¾è®¡è¯„ä¼°æ¡†æ¶æ¥é‡åŒ–ä¸–ç•Œç¨³å®šæ€§ï¼Œå‘ç°ç°æœ‰æ¨¡å‹é¢ä¸´æŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œç ”ç©¶æå‡ºå¤šç§æå‡ä¸–ç•Œç¨³å®šæ€§çš„ç­–ç•¥ï¼Œå¼ºè°ƒäº†ä¸–ç•Œç¨³å®šæ€§åœ¨ä¸–ç•Œæ¨¡å‹ä¸­çš„é‡è¦æ€§å¹¶ä¸ºæœªæ¥ç ”ç©¶æä¾›æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å…³æ³¨æ‰©æ•£å¼ç”Ÿæˆæ¨¡å‹çš„ä¸–ç•Œæ¨¡å‹å†…å®¹ä¿ç•™èƒ½åŠ›æå‡ã€‚</li>
<li>æå‡ºâ€œä¸–ç•Œç¨³å®šæ€§â€ä½œä¸ºè¡¡é‡æ¨¡å‹æ€§èƒ½çš„é‡è¦æŒ‡æ ‡ã€‚</li>
<li>è®¾è®¡è¯„ä¼°æ¡†æ¶æ¥é‡åŒ–ä¸–ç•Œç¨³å®šæ€§ï¼Œé€šè¿‡è®©æ¨¡å‹æ‰§è¡Œä¸€ç³»åˆ—åŠ¨ä½œå†é€†å‘æ‰§è¡Œä»¥æ¯”è¾ƒåˆå§‹å’Œæœ€ç»ˆè§‚å¯Ÿç»“æœçš„ä¸€è‡´æ€§ã€‚</li>
<li>å‘ç°ç°æœ‰æ‰©æ•£å¼ä¸–ç•Œæ¨¡å‹åœ¨ä¸–ç•Œç¨³å®šæ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºå¤šç§å¢å¼ºä¸–ç•Œç¨³å®šæ€§çš„ç­–ç•¥ã€‚</li>
<li>å¼ºè°ƒä¸–ç•Œç¨³å®šæ€§å¯¹ä¸–ç•Œæ¨¡å‹çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d513f249d58b801fe3757d8e02d86e09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7036ff4da4fb676bcedd4eeb47950e21.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ebc19089a9007959df4289fbd3d48a6f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-General-Framework-to-Evaluate-Methods-for-Assessing-Dimensions-of-Lexical-Semantic-Change-Using-LLM-Generated-Synthetic-Data"><a href="#A-General-Framework-to-Evaluate-Methods-for-Assessing-Dimensions-of-Lexical-Semantic-Change-Using-LLM-Generated-Synthetic-Data" class="headerlink" title="A General Framework to Evaluate Methods for Assessing Dimensions of   Lexical Semantic Change Using LLM-Generated Synthetic Data"></a>A General Framework to Evaluate Methods for Assessing Dimensions of   Lexical Semantic Change Using LLM-Generated Synthetic Data</h2><p><strong>Authors:Naomi Baes, RaphaÃ«l Merx, Nick Haslam, Ekaterina Vylomova, Haim Dubossarsky</strong></p>
<p>Lexical Semantic Change (LSC) offers insights into cultural and social dynamics. Yet, the validity of methods for measuring kinds of LSC has yet to be established due to the absence of historical benchmark datasets. To address this gap, we develop a novel three-stage evaluation framework that involves: 1) creating a scalable, domain-general methodology for generating synthetic datasets that simulate theory-driven LSC across time, leveraging In-Context Learning and a lexical database; 2) using these datasets to evaluate the effectiveness of various methods; and 3) assessing their suitability for specific dimensions and domains. We apply this framework to simulate changes across key dimensions of LSC (SIB: Sentiment, Intensity, and Breadth) using examples from psychology, and evaluate the sensitivity of selected methods to detect these artificially induced changes. Our findings support the utility of the synthetic data approach, validate the efficacy of tailored methods for detecting synthetic changes in SIB, and reveal that a state-of-the-art LSC model faces challenges in detecting affective dimensions of LSC. This framework provides a valuable tool for dimension- and domain-specific bench-marking and evaluation of LSC methods, with particular benefits for the social sciences. </p>
<blockquote>
<p>è¯æ±‡è¯­ä¹‰å˜åŒ–ï¼ˆLSCï¼‰æä¾›äº†å¯¹æ–‡åŒ–å’Œç¤¾ä¼šåŠ¨æ€çš„æ´å¯ŸåŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å†å²åŸºå‡†æ•°æ®é›†ï¼Œè¡¡é‡å„ç§LSCæ–¹æ³•çš„æœ‰æ•ˆæ€§å°šæœªå»ºç«‹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°çš„ä¸‰é˜¶æ®µè¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬ï¼š1ï¼‰åˆ›å»ºä¸€ç§å¯æ‰©å±•çš„ã€é€šç”¨çš„æ–¹æ³•ï¼Œç”Ÿæˆæ¨¡æ‹Ÿè·¨æ—¶é—´ç†è®ºé©±åŠ¨çš„LSCçš„åˆæˆæ•°æ®é›†ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ å’Œè¯æ±‡æ•°æ®åº“ï¼›2ï¼‰ä½¿ç”¨è¿™äº›æ•°æ®é›†æ¥è¯„ä¼°å„ç§æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼›3ï¼‰è¯„ä¼°å®ƒä»¬åœ¨ç‰¹å®šç»´åº¦å’Œé¢†åŸŸçš„é€‚ç”¨æ€§ã€‚æˆ‘ä»¬ä»¥å¿ƒç†å­¦ä¸­çš„ä¾‹å­ä¸ºåŸºç¡€ï¼Œåº”ç”¨è¿™ä¸€æ¡†æ¶æ¥æ¨¡æ‹ŸLSCå…³é”®ç»´åº¦çš„å˜åŒ–ï¼ˆSIBï¼šæƒ…æ„Ÿã€å¼ºåº¦å’Œå¹¿åº¦ï¼‰ï¼Œå¹¶è¯„ä¼°æ‰€é€‰æ–¹æ³•å¯¹æ£€æµ‹è¿™äº›äººä¸ºè¯±å¯¼å˜åŒ–çš„æ•æ„Ÿæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ”¯æŒåˆæˆæ•°æ®æ–¹æ³•çš„å®ç”¨æ€§ï¼ŒéªŒè¯äº†é’ˆå¯¹æ£€æµ‹SIBä¸­åˆæˆå˜åŒ–é‡èº«å®šåˆ¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ­ç¤ºæœ€å…ˆè¿›çš„LSCæ¨¡å‹åœ¨æ£€æµ‹LSCçš„æƒ…æ„Ÿç»´åº¦æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶ä¸ºç‰¹å®šç»´åº¦å’Œé¢†åŸŸçš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°LSCæ–¹æ³•æä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·ï¼Œå¯¹ç¤¾ä¼šç§‘å­¦é¢†åŸŸå°¤å…¶æœ‰ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08042v1">PDF</a> 36 pages, under review</p>
<p><strong>Summary</strong><br>æ–‡æœ¬è®¨è®ºäº†è¯æ±‡è¯­ä¹‰å˜åŒ–ï¼ˆLSCï¼‰çš„é—®é¢˜å’ŒæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ç¼ºå°‘ç”¨äºè¡¡é‡LSCç±»å‹çš„æœ‰æ•ˆæ–¹æ³•å’Œå†å²åŸºå‡†æ•°æ®é›†çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…å¼€å‘äº†ä¸€ç§æ–°å‹çš„ä¸‰é˜¶æ®µè¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬åˆ›å»ºä¸€ç§é€šç”¨æ€§è¾ƒå¼ºçš„æ–¹æ³•ç”Ÿæˆåˆæˆæ•°æ®é›†ï¼Œä»¥æ¨¡æ‹Ÿæ—¶é—´ä¸Šçš„ç†è®ºé©±åŠ¨çš„LSCå˜åŒ–ï¼›ä½¿ç”¨è¿™äº›æ•°æ®é›†è¯„ä¼°å„ç§æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼›è¯„ä¼°è¿™äº›æ–¹æ³•åœ¨ä¸åŒç»´åº¦å’Œé¢†åŸŸçš„é€‚ç”¨æ€§ã€‚ç ”ç©¶è€…åº”ç”¨æ­¤æ¡†æ¶æ¨¡æ‹Ÿäº†å¿ƒç†å­¦é¢†åŸŸçš„å…³é”®ç»´åº¦å˜åŒ–ï¼Œå¹¶å‘ç°æŸäº›æ–¹æ³•èƒ½å¤Ÿå¾ˆå¥½åœ°æ£€æµ‹è¿™äº›äººä¸ºå¼•èµ·çš„å˜åŒ–ã€‚è¯¥æ¡†æ¶ä¸ºè¯„ä¼°ç‰¹å®šé¢†åŸŸå’Œç‰¹å®šç»´åº¦çš„LSCæ–¹æ³•æä¾›äº†å®è´µçš„å·¥å…·ã€‚å¯¹äºç¤¾ä¼šç§‘å­¦é¢†åŸŸå°¤å…¶å…·æœ‰å®é™…æ„ä¹‰ã€‚è¿™ä¸€ç ”ç©¶å¯¹äºæ¨è¿›LSCé¢†åŸŸçš„ç†è®ºå‘å±•å’Œå®é™…åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¼ºä¹å†å²åŸºå‡†æ•°æ®é›†ï¼Œå¯¼è‡´æ— æ³•ç¡®å®šè¡¡é‡LSCçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸‰é˜¶æ®µè¯„ä¼°æ¡†æ¶ã€‚</li>
<li>æ¡†æ¶ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨è¯­å¢ƒå­¦ä¹ å’Œè¯æ±‡æ•°æ®åº“åˆ›å»ºäº†é€šç”¨çš„æ–¹æ³•ç”Ÿæˆåˆæˆæ•°æ®é›†æ¥æ¨¡æ‹Ÿæ—¶é—´é©±åŠ¨ä¸‹çš„ç†è®ºæ€§LSCå˜åŒ–ã€‚è¿™äº›åˆæˆæ•°æ®é›†ç”¨äºè¯„ä¼°å„ç§æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µè¯„ä¼°è¿™äº›æ–¹æ³•åœ¨ä¸åŒç»´åº¦å’Œé¢†åŸŸçš„é€‚ç”¨æ€§ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿã€å¼ºåº¦å’Œå¹¿åº¦ç­‰å…³é”®ç»´åº¦ã€‚åœ¨å¿ƒç†å­¦é¢†åŸŸçš„åº”ç”¨ä¸ºä¾‹å±•ç¤ºäº†è¿™ä¸€ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c3d9c1b7770e03a4c69cd3ea68da2d8c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-32edd36da55a694a3419ffc4894bf256.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.08042v1/page_2_1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f824ef98b1de429850fba3fb1f36dc0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Exploring-Bias-in-over-100-Text-to-Image-Generative-Models"><a href="#Exploring-Bias-in-over-100-Text-to-Image-Generative-Models" class="headerlink" title="Exploring Bias in over 100 Text-to-Image Generative Models"></a>Exploring Bias in over 100 Text-to-Image Generative Models</h2><p><strong>Authors:Jordan Vice, Naveed Akhtar, Richard Hartley, Ajmal Mian</strong></p>
<p>We investigate bias trends in text-to-image generative models over time, focusing on the increasing availability of models through open platforms like Hugging Face. While these platforms democratize AI, they also facilitate the spread of inherently biased models, often shaped by task-specific fine-tuning. Ensuring ethical and transparent AI deployment requires robust evaluation frameworks and quantifiable bias metrics. To this end, we assess bias across three key dimensions: (i) distribution bias, (ii) generative hallucination, and (iii) generative miss-rate. Analyzing over 100 models, we reveal how bias patterns evolve over time and across generative tasks. Our findings indicate that artistic and style-transferred models exhibit significant bias, whereas foundation models, benefiting from broader training distributions, are becoming progressively less biased. By identifying these systemic trends, we contribute a large-scale evaluation corpus to inform bias research and mitigation strategies, fostering more responsible AI development.   Keywords: Bias, Ethical AI, Text-to-Image, Generative Models, Open-Source Models </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­çš„åè§è¶‹åŠ¿éšæ—¶é—´å˜åŒ–çš„æƒ…å†µï¼Œé‡ç‚¹å…³æ³¨é€šè¿‡Hugging Faceç­‰å¼€æ”¾å¹³å°æ—¥ç›Šæ™®åŠçš„æ¨¡å‹ã€‚è¿™äº›å¹³å°è™½ç„¶å®ç°äº†äººå·¥æ™ºèƒ½çš„æ™®åŠï¼Œä½†ä¹Ÿä¿ƒè¿›äº†å›ºæœ‰åè§æ¨¡å‹çš„ä¼ æ’­ï¼Œè¿™äº›æ¨¡å‹å¾€å¾€é€šè¿‡ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒè€Œå½¢æˆã€‚ç¡®ä¿äººå·¥æ™ºèƒ½çš„ä¼¦ç†å’Œé€æ˜éƒ¨ç½²éœ€è¦å¼ºå¤§çš„è¯„ä¼°æ¡†æ¶å’Œå¯é‡åŒ–çš„åè§æŒ‡æ ‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä»ä¸‰ä¸ªå…³é”®ç»´åº¦è¯„ä¼°åè§ï¼šï¼ˆiï¼‰åˆ†å¸ƒåè§ï¼Œï¼ˆiiï¼‰ç”Ÿæˆå¹»è§‰ï¼Œï¼ˆiiiï¼‰ç”Ÿæˆé”™è¯¯ç‡ã€‚é€šè¿‡åˆ†æè¶…è¿‡100ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬æ­ç¤ºäº†åè§æ¨¡å¼éšæ—¶é—´ä»¥åŠè·¨ç”Ÿæˆä»»åŠ¡å¦‚ä½•æ¼”å˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè‰ºæœ¯å‹å’Œé£æ ¼è½¬æ¢æ¨¡å‹å­˜åœ¨æ˜æ˜¾çš„åè§ï¼Œè€Œå¾—ç›Šäºæ›´å¹¿æ³›çš„è®­ç»ƒåˆ†å¸ƒçš„åŸºç¡€æ¨¡å‹åˆ™å˜å¾—è¶Šæ¥è¶Šå°‘åè§ã€‚é€šè¿‡è¯†åˆ«è¿™äº›ç³»ç»Ÿæ€§è¶‹åŠ¿ï¼Œæˆ‘ä»¬ä¸ºåè§ç ”ç©¶å’Œç¼“è§£ç­–ç•¥æä¾›äº†å¤§è§„æ¨¡è¯„ä¼°è¯­æ–™åº“ï¼Œä¿ƒè¿›äº†æ›´è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½å‘å±•ã€‚å…³é”®è¯ï¼šåè§ã€ä¼¦ç†äººå·¥æ™ºèƒ½ã€æ–‡æœ¬åˆ°å›¾åƒã€ç”Ÿæˆæ¨¡å‹ã€å¼€æºæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08012v1">PDF</a> Accepted to ICLR 2025 Workshop on Open Science for Foundation Models   (SCI-FM)</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ç ”ç©¶äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­çš„åè§è¶‹åŠ¿ã€‚éšç€Hugging Faceç­‰å¼€æ”¾å¹³å°æä¾›çš„æ¨¡å‹è¶Šæ¥è¶Šå¤šï¼Œè™½ç„¶æ°‘ä¸»åŒ–äº†äººå·¥æ™ºèƒ½ï¼Œä½†ä¹Ÿä¿ƒè¿›äº†å›ºæœ‰åè§æ¨¡å‹çš„ä¼ æ’­ã€‚ä¸ºç¡®ä¿äººå·¥æ™ºèƒ½çš„ä¼¦ç†å’Œé€æ˜éƒ¨ç½²ï¼Œéœ€è¦å¼ºå¤§çš„è¯„ä¼°æ¡†æ¶å’Œå¯é‡åŒ–çš„åè§æŒ‡æ ‡ã€‚æ–‡ç« å¯¹åè§è¿›è¡Œäº†ä¸‰ä¸ªå…³é”®ç»´åº¦çš„è¯„ä¼°ï¼šåˆ†å¸ƒåè§ã€ç”Ÿæˆå¹»æƒ³å’Œç”Ÿæˆè¯¯æŠ¥ç‡ã€‚é€šè¿‡åˆ†æè¶…è¿‡ä¸€ç™¾ä¸ªæ¨¡å‹ï¼Œæ­ç¤ºäº†åè§æ¨¡å¼éšæ—¶é—´ä»¥åŠè·¨ç”Ÿæˆä»»åŠ¡çš„å‘å±•å˜åŒ–ã€‚ç ”ç©¶å‘ç°è‰ºæœ¯å‹å’Œé£æ ¼è½¬æ¢æ¨¡å‹å­˜åœ¨æ˜¾è‘—åè§ï¼Œè€Œå—ç›Šäºæ›´å¹¿æ³›è®­ç»ƒåˆ†å¸ƒçš„åŸºç¡€æ¨¡å‹åˆ™å˜å¾—è¶Šæ¥è¶Šå°‘åè§ã€‚å…³é”®è¯ï¼šåè§ã€ä¼¦ç†äººå·¥æ™ºèƒ½ã€æ–‡æœ¬åˆ°å›¾åƒã€ç”Ÿæˆæ¨¡å‹ã€å¼€æºæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾å¹³å°å¦‚Hugging Faceä¿ƒè¿›äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ™®åŠï¼Œä½†ä¹Ÿå¸¦æ¥äº†åè§æ¨¡å‹çš„ä¼ æ’­é—®é¢˜ã€‚</li>
<li>åè§è¶‹åŠ¿çš„ç ”ç©¶æ˜¯ç¡®ä¿äººå·¥æ™ºèƒ½ä¼¦ç†å’Œé€æ˜éƒ¨ç½²çš„å…³é”®ã€‚</li>
<li>ä¸‰ä¸ªå…³é”®ç»´åº¦ç”¨äºè¯„ä¼°åè§ï¼šåˆ†å¸ƒåè§ã€ç”Ÿæˆå¹»æƒ³å’Œç”Ÿæˆè¯¯æŠ¥ç‡ã€‚</li>
<li>è‰ºæœ¯å‹å’Œé£æ ¼è½¬æ¢æ¨¡å‹å­˜åœ¨æ˜¾è‘—åè§é—®é¢˜ã€‚</li>
<li>åŸºç¡€æ¨¡å‹å› å—ç›Šäºæ›´å¹¿æ³›çš„è®­ç»ƒåˆ†å¸ƒï¼Œåœ¨åè§é—®é¢˜ä¸Šè¡¨ç°é€æ¸æ”¹å–„ã€‚</li>
<li>æ–‡ç« è´¡çŒ®äº†ä¸€ä¸ªå¤§è§„æ¨¡è¯„ä¼°è¯­æ–™åº“ï¼Œä»¥æ¨åŠ¨åè§ç ”ç©¶å’Œç¼“è§£ç­–ç•¥çš„åˆ¶å®šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08012">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d97b360f43dabba7078cf254586ea04d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e03463d0843b75a721199810a664959.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62419b4d93fc8ce716ed06e152441d31.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MoRE-Unlocking-Scalability-in-Reinforcement-Learning-for-Quadruped-Vision-Language-Action-Models"><a href="#MoRE-Unlocking-Scalability-in-Reinforcement-Learning-for-Quadruped-Vision-Language-Action-Models" class="headerlink" title="MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped   Vision-Language-Action Models"></a>MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped   Vision-Language-Action Models</h2><p><strong>Authors:Han Zhao, Wenxuan Song, Donglin Wang, Xinyang Tong, Pengxiang Ding, Xuelian Cheng, Zongyuan Ge</strong></p>
<p>Developing versatile quadruped robots that can smoothly perform various actions and tasks in real-world environments remains a significant challenge. This paper introduces a novel vision-language-action (VLA) model, mixture of robotic experts (MoRE), for quadruped robots that aim to introduce reinforcement learning (RL) for fine-tuning large-scale VLA models with a large amount of mixed-quality data. MoRE integrates multiple low-rank adaptation modules as distinct experts within a dense multi-modal large language model (MLLM), forming a sparse-activated mixture-of-experts model. This design enables the model to effectively adapt to a wide array of downstream tasks. Moreover, we employ a reinforcement learning-based training objective to train our model as a Q-function after deeply exploring the structural properties of our tasks. Effective learning from automatically collected mixed-quality data enhances data efficiency and model performance. Extensive experiments demonstrate that MoRE outperforms all baselines across six different skills and exhibits superior generalization capabilities in out-of-distribution scenarios. We further validate our method in real-world scenarios, confirming the practicality of our approach and laying a solid foundation for future research on multi-task learning in quadruped robots. </p>
<blockquote>
<p>å¼€å‘èƒ½å¤Ÿåœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­æµç•…æ‰§è¡Œå„ç§åŠ¨ä½œå’Œä»»åŠ¡çš„é€šç”¨å››è¶³æœºå™¨äººä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºå››è¶³æœºå™¨äººçš„æ–°å‹è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œå³æœºå™¨äººä¸“å®¶æ··åˆç‰©ï¼ˆMoREï¼‰ï¼Œæ—¨åœ¨å¼•å…¥å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹å¤§è§„æ¨¡VLAæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¤„ç†å¤§é‡æ··åˆè´¨é‡çš„æ•°æ®ã€‚MoREåœ¨å¯†é›†çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­é›†æˆäº†å¤šä¸ªä½é˜¶é€‚åº”æ¨¡å—ä½œä¸ºä¸åŒçš„ä¸“å®¶ï¼Œå½¢æˆäº†ä¸€ä¸ªç¨€ç–æ¿€æ´»çš„ä¸“å®¶æ··åˆæ¨¡å‹ã€‚è¿™ç§è®¾è®¡ä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°é€‚åº”å„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ·±å…¥æ¢ç´¢ä»»åŠ¡ç»“æ„å±æ€§çš„åŸºç¡€ä¸Šï¼Œé‡‡ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒç›®æ ‡å¯¹æ¨¡å‹è¿›è¡ŒQå‡½æ•°è®­ç»ƒã€‚ä»è‡ªåŠ¨æ”¶é›†çš„æ··åˆè´¨é‡æ•°æ®ä¸­æœ‰æ•ˆå­¦ä¹ æé«˜äº†æ•°æ®æ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMoREåœ¨å…­ç§ä¸åŒæŠ€èƒ½ä¸Šè¶…è¶Šäº†æ‰€æœ‰åŸºçº¿ï¼Œå¹¶åœ¨è¶…å‡ºåˆ†å¸ƒçš„åœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€»æ‹¬èƒ½åŠ›ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åœ¨ç°å®åœºæ™¯ä¸­éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯å®äº†æˆ‘ä»¬çš„æ–¹æ³•çš„å®ç”¨æ€§ï¼Œå¹¶ä¸ºæœªæ¥å››è¶³æœºå™¨äººå¤šä»»åŠ¡å­¦ä¹ çš„ç ”ç©¶å¥ å®šäº†åšå®åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08007v1">PDF</a> Accepted by ICRA 2025</p>
<p><strong>Summary</strong><br>å¤šè¶³æœºå™¨äººé¢å¯¹å®é™…ç¯å¢ƒæ‰§è¡Œä»»åŠ¡å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„æ··åˆè¯­è¨€æ¨¡å‹ï¼ˆMoREï¼‰ï¼Œå®ƒé›†æˆäº†å¼ºåŒ–å­¦ä¹ ç”¨äºè°ƒæ•´å¤§è§„æ¨¡æ¨¡å‹ä»¥é€‚åº”å„ç§å¤æ‚æ•°æ®ï¼Œä»è€Œæå‡æœºå™¨äººçš„çµæ´»æ€§ã€‚MoREæ¨¡å‹è®¾è®¡çµæ´»ï¼Œèƒ½é€‚åº”å¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¹¶åœ¨å®éªŒç¯å¢ƒä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨å¤šè¶³æœºå™¨äººé¢†åŸŸå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ï¼Œä¸ºæœªæ¥å¤šä»»åŠ¡å­¦ä¹ ç ”ç©¶æä¾›äº†åšå®çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä»‹ç»äº†ä¸€ç§é’ˆå¯¹å¤šè¶³æœºå™¨äººçš„æ–°å‹æ··åˆè¯­è¨€æ¨¡å‹ï¼ˆMoREï¼‰ã€‚</li>
<li>MoREæ¨¡å‹é›†æˆäº†å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¤§è§„æ¨¡æ¨¡å‹çš„ç²¾ç»†è°ƒæ•´ï¼Œé€‚åº”å¤æ‚æ•°æ®ã€‚</li>
<li>MoREé€šè¿‡ç»“åˆå¤šä¸ªä½é˜¶é€‚åº”æ¨¡å—ä½œä¸ºä¸“å®¶ï¼Œå½¢æˆç¨€ç–æ¿€æ´»çš„ä¸“å®¶æ··åˆæ¨¡å‹ï¼Œå¢å¼ºäº†æ¨¡å‹çš„é€‚åº”æ€§ã€‚</li>
<li>é‡‡ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒç›®æ ‡è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>MoREæ¨¡å‹åœ¨å¤šç§æŠ€èƒ½ä¸Šè¡¨ç°å‡ºè¶…è¶ŠåŸºå‡†çš„æ€§èƒ½ï¼Œå¹¶åœ¨ç¦»ç¾¤åœºæ™¯ä¸­å±•ç°å‡ºä¼˜ç§€çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>MoREæ¨¡å‹åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„åº”ç”¨éªŒè¯äº†å…¶å®ç”¨æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08007">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e5d94f12484d0aa6e48eb349eeece2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b61c78ef60925a9c5b53bce41f460b6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-784e77c4f8956384ca170103fda84e9b.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.08007v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.08007v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.08007v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="AlphaDrive-Unleashing-the-Power-of-VLMs-in-Autonomous-Driving-via-Reinforcement-Learning-and-Reasoning"><a href="#AlphaDrive-Unleashing-the-Power-of-VLMs-in-Autonomous-Driving-via-Reinforcement-Learning-and-Reasoning" class="headerlink" title="AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via   Reinforcement Learning and Reasoning"></a>AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via   Reinforcement Learning and Reasoning</h2><p><strong>Authors:Bo Jiang, Shaoyu Chen, Qian Zhang, Wenyu Liu, Xinggang Wang</strong></p>
<p>OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level performance in complex domains like mathematics and science, with reinforcement learning (RL) and reasoning playing a crucial role. In autonomous driving, recent end-to-end models have greatly improved planning performance but still struggle with long-tailed problems due to limited common sense and reasoning abilities. Some studies integrate vision-language models (VLMs) into autonomous driving, but they typically rely on pre-trained models with simple supervised fine-tuning (SFT) on driving data, without further exploration of training strategies or optimizations specifically tailored for planning. In this paper, we propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous driving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning and employs a two-stage planning reasoning training strategy that combines SFT with RL. As a result, AlphaDrive significantly improves both planning performance and training efficiency compared to using only SFT or without reasoning. Moreover, we are also excited to discover that, following RL training, AlphaDrive exhibits some emergent multimodal planning capabilities, which is critical for improving driving safety and efficiency. To the best of our knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning reasoning into autonomous driving. Code will be released to facilitate future research. </p>
<blockquote>
<p>OpenAI o1å’ŒDeepSeek R1åœ¨è¯¸å¦‚æ•°å­¦å’Œç§‘å­¦ç­‰å¤æ‚é¢†åŸŸè¾¾åˆ°äº†ç”šè‡³è¶…è¶Šäº†äººç±»ä¸“å®¶çš„æ€§èƒ½æ°´å¹³ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ¨ç†èµ·åˆ°äº†è‡³å…³é‡è¦çš„ä½œç”¨ã€‚åœ¨è‡ªåŠ¨é©¾é©¶æ–¹é¢ï¼Œæœ€è¿‘çš„ç«¯åˆ°ç«¯æ¨¡å‹åœ¨è§„åˆ’æ€§èƒ½ä¸Šæœ‰äº†å¾ˆå¤§çš„æé«˜ï¼Œä½†ç”±äºå¸¸è¯†å’Œæ¨ç†èƒ½åŠ›çš„æœ‰é™ï¼Œä»ç„¶é¢ä¸´ç€é•¿å°¾é—®é¢˜ã€‚ä¸€äº›ç ”ç©¶å°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰èå…¥è‡ªåŠ¨é©¾é©¶ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºåœ¨é©¾é©¶æ•°æ®ä¸Šè¿›è¡Œç®€å•ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ²¡æœ‰è¿›ä¸€æ­¥æ¢ç´¢é’ˆå¯¹è§„åˆ’çš„åŸ¹è®­ç­–ç•¥æˆ–ä¼˜åŒ–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AlphaDriveï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè‡ªåŠ¨é©¾é©¶ä¸­VLMçš„RLå’Œæ¨ç†æ¡†æ¶ã€‚AlphaDriveå¼•å…¥äº†å››ç§é’ˆå¯¹è§„åˆ’è®¾è®¡çš„GRPOåŸºäºRLçš„å¥–åŠ±ï¼Œå¹¶é‡‡ç”¨äº†ä¸€ç§ä¸¤é˜¶æ®µè§„åˆ’æ¨ç†åŸ¹è®­ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ç»“åˆäº†SFTä¸RLã€‚å› æ­¤ï¼Œä¸ä»…ä½¿ç”¨SFTæˆ–æ²¡æœ‰æ¨ç†çš„æƒ…å†µç›¸æ¯”ï¼ŒAlphaDriveæ˜¾è‘—æé«˜äº†è§„åˆ’æ€§èƒ½å’ŒåŸ¹è®­æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°ï¼Œç»è¿‡RLè®­ç»ƒåï¼ŒAlphaDriveè¡¨ç°å‡ºä¸€äº›æ–°å…´çš„å¤šå…ƒæ¨¡å¼è§„åˆ’èƒ½åŠ›ï¼Œè¿™å¯¹äºæé«˜é©¾é©¶çš„å®‰å…¨æ€§å’Œæ•ˆç‡è‡³å…³é‡è¦ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒAlphaDriveæ˜¯é¦–ä¸ªå°†åŸºäºGRPOçš„RLä¸è§„åˆ’æ¨ç†é›†æˆåˆ°è‡ªåŠ¨é©¾é©¶ä¸­çš„ã€‚æˆ‘ä»¬å°†å‘å¸ƒä»£ç ä»¥ä¿ƒè¿›æœªæ¥çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07608v1">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://github.com/hustvl/AlphaDrive">https://github.com/hustvl/AlphaDrive</a></p>
<p><strong>Summary</strong></p>
<p>OpenAIå’ŒDeepSeekåœ¨å¤æ‚é¢†åŸŸå¦‚æ•°å­¦å’Œç§‘å­¦æ–¹é¢è¾¾åˆ°äº†ç”šè‡³è¶…è¶Šäº†äººç±»ä¸“å®¶çš„æ°´å¹³ï¼Œå¼ºåŒ–å­¦ä¹ å’Œæ¨ç†åœ¨å…¶ä¸­å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸï¼Œå°½ç®¡è¿‘æœŸç«¯åˆ°ç«¯æ¨¡å‹æå‡äº†è§„åˆ’æ€§èƒ½ï¼Œä½†ä»é¢ä¸´é•¿å°¾é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œæ¨ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹æ¡†æ¶AlphaDriveï¼Œå®ƒé€šè¿‡å®šåˆ¶åŒ–çš„å¥–åŠ±å’Œä¸¤é˜¶æ®µè§„åˆ’æ¨ç†è®­ç»ƒç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†è§„åˆ’æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ï¼Œå¹¶å±•ç°å‡ºå¤šæ¨¡å¼è§„åˆ’èƒ½åŠ›ï¼Œæé«˜äº†é©¾é©¶å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚AlphaDriveæ˜¯é¦–ä¸ªå°†åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ ä¸è§„åˆ’æ¨ç†ç»“åˆåˆ°è‡ªåŠ¨é©¾é©¶ä¸­çš„æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenAIå’ŒDeepSeekå·²åœ¨æ•°å­¦å’Œç§‘å­¦ç­‰é¢†åŸŸè¾¾åˆ°æˆ–è¶…è¶Šäººç±»ä¸“å®¶æ°´å¹³ï¼Œå¼ºåŒ–å­¦ä¹ å’Œæ¨ç†èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>è‡ªåŠ¨é©¾é©¶ä¸­çš„ç«¯åˆ°ç«¯æ¨¡å‹è™½æå‡äº†è§„åˆ’æ€§èƒ½ï¼Œä½†ä»é¢ä¸´é•¿å°¾é—®é¢˜ã€‚</li>
<li>AlphaDriveæ˜¯ä¸€ä¸ªç»“åˆå¼ºåŒ–å­¦ä¹ å’Œæ¨ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œé’ˆå¯¹è‡ªåŠ¨é©¾é©¶ä¸­çš„è§„åˆ’é—®é¢˜ã€‚</li>
<li>AlphaDriveé€šè¿‡å®šåˆ¶åŒ–çš„å¥–åŠ±å’Œä¸¤é˜¶æ®µè§„åˆ’æ¨ç†è®­ç»ƒç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†è§„åˆ’æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ã€‚</li>
<li>AlphaDriveå±•ç°å‡ºå¤šæ¨¡å¼è§„åˆ’èƒ½åŠ›ï¼Œè¿™å¯¹äºæé«˜é©¾é©¶å®‰å…¨æ€§å’Œæ•ˆç‡è‡³å…³é‡è¦ã€‚</li>
<li>AlphaDriveæ˜¯é¦–ä¸ªå°†åŸºäºGRPOçš„å¼ºåŒ–å­¦ä¹ ä¸è§„åˆ’æ¨ç†ç»“åˆåˆ°è‡ªåŠ¨é©¾é©¶çš„æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07608">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.07608v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.07608v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.07608v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.07608v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Optimizing-Test-Time-Compute-via-Meta-Reinforcement-Fine-Tuning"><a href="#Optimizing-Test-Time-Compute-via-Meta-Reinforcement-Fine-Tuning" class="headerlink" title="Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning"></a>Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning</h2><p><strong>Authors:Yuxiao Qu, Matthew Y. R. Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching, Ruslan Salakhutdinov, Aviral Kumar</strong></p>
<p>Training models to effectively use test-time compute is crucial for improving the reasoning performance of LLMs. Current methods mostly do so via fine-tuning on search traces or running RL with 0&#x2F;1 outcome reward, but do these approaches efficiently utilize test-time compute? Would these approaches continue to scale as the budget improves? In this paper, we try to answer these questions. We formalize the problem of optimizing test-time compute as a meta-reinforcement learning (RL) problem, which provides a principled perspective on spending test-time compute. This perspective enables us to view the long output stream from the LLM as consisting of several episodes run at test time and leads us to use a notion of cumulative regret over output tokens as a way to measure the efficacy of test-time compute. Akin to how RL algorithms can best tradeoff exploration and exploitation over training, minimizing cumulative regret would also provide the best balance between exploration and exploitation in the token stream. While we show that state-of-the-art models do not minimize regret, one can do so by maximizing a dense reward bonus in conjunction with the outcome 0&#x2F;1 reward RL. This bonus is the â€˜â€™progressâ€™â€™ made by each subsequent block in the output stream, quantified by the change in the likelihood of eventual success. Using these insights, we develop Meta Reinforcement Fine-Tuning, or MRT, a new class of fine-tuning methods for optimizing test-time compute. MRT leads to a 2-3x relative gain in performance and roughly a 1.5x gain in token efficiency for math reasoning compared to outcome-reward RL. </p>
<blockquote>
<p>è®­ç»ƒæ¨¡å‹ä»¥æœ‰æ•ˆåˆ©ç”¨æµ‹è¯•æ—¶çš„è®¡ç®—èµ„æºå¯¹äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½è‡³å…³é‡è¦ã€‚ç›®å‰çš„æ–¹æ³•å¤§å¤šæ˜¯é€šè¿‡æœç´¢ç—•è¿¹è¿›è¡Œå¾®è°ƒæˆ–ä½¿ç”¨0&#x2F;1ç»“æœå¥–åŠ±è¿è¡Œå¼ºåŒ–å­¦ä¹ æ¥å®ç°ï¼Œä½†è¿™äº›æ–¹æ³•æ˜¯å¦æœ‰æ•ˆåœ°åˆ©ç”¨äº†æµ‹è¯•æ—¶çš„è®¡ç®—èµ„æºï¼Ÿéšç€é¢„ç®—çš„æé«˜ï¼Œè¿™äº›æ–¹æ³•æ˜¯å¦ä¼šç»§ç»­æ‰©å±•ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯•å›¾å›ç­”è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬å°†ä¼˜åŒ–æµ‹è¯•æ—¶è®¡ç®—çš„é—®é¢˜å½¢å¼åŒ–ä¸ºå…ƒå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é—®é¢˜ï¼Œè¿™ä¸ºæµ‹è¯•æ—¶è®¡ç®—æä¾›äº†æœ‰åŸåˆ™çš„è§†è§’ã€‚è¿™ä¸ªè§†è§’ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†å¤§å‹è¯­è¨€æ¨¡å‹çš„é•¿æœŸè¾“å‡ºè§†ä¸ºåœ¨æµ‹è¯•æ—¶è¿è¡Œçš„å¤šä¸ªç‰‡æ®µï¼Œå¹¶å¼•å¯¼æˆ‘ä»¬ä½¿ç”¨è¾“å‡ºæ ‡è®°çš„ç´¯ç§¯é—æ†¾æ¥è¡¡é‡æµ‹è¯•æ—¶è®¡ç®—çš„æœ‰æ•ˆæ€§ã€‚ä¸å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¦‚ä½•æœ€å¥½åœ°å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ä¸€æ ·ï¼Œæœ€å°åŒ–ç´¯ç§¯é—æ†¾ä¹Ÿå°†åœ¨æ ‡è®°æµä¸­æä¾›æ¢ç´¢å’Œåˆ©ç”¨ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ã€‚æˆ‘ä»¬è™½ç„¶è¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹å¹¶æ²¡æœ‰æœ€å°åŒ–é—æ†¾ï¼Œä½†å¯ä»¥é€šè¿‡ç»“åˆä½¿ç”¨å¯†é›†å¥–åŠ±å¥–é‡‘å’Œç»“æœ0&#x2F;1å¥–åŠ±å¼ºåŒ–å­¦ä¹ æ¥å®ç°è¿™ä¸€ç‚¹ã€‚è¿™ä¸€å¥–é‡‘æ˜¯ç”±è¾“å‡ºæµä¸­æ¯ä¸ªåç»­å—æ‰€å–å¾—çš„â€œè¿›å±•â€æ‰€é‡åŒ–çš„æœ€ç»ˆæˆåŠŸçš„å¯èƒ½æ€§å˜åŒ–ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬å¼€å‘äº†å…ƒå¼ºåŒ–ç²¾ç»†è°ƒæ•´ï¼ˆMRTï¼‰ï¼Œè¿™æ˜¯ä¸€ç±»æ–°çš„ç²¾ç»†è°ƒæ•´æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–æµ‹è¯•æ—¶çš„è®¡ç®—ã€‚ä¸ç»“æœå¥–åŠ±å¼ºåŒ–å­¦ä¹ ç›¸æ¯”ï¼ŒMRTåœ¨æ•°å­¦æ¨ç†æ–¹é¢å®ç°äº†ç›¸å¯¹æ€§èƒ½2-3å€çš„æå‡ï¼Œä»¥åŠå¤§çº¦1.5å€çš„æ ‡è®°æ•ˆç‡æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07572v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æµ‹è¯•æ—¶é—´è®¡ç®—ä½¿ç”¨è¿›è¡Œäº†ä¼˜åŒ–ç ”ç©¶ã€‚æ–‡ç« é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è§†è§’æ¥è§£å†³æµ‹è¯•æ—¶é—´è®¡ç®—ä¼˜åŒ–é—®é¢˜ï¼Œå°†é•¿æœŸè¾“å‡ºæµè§†ä¸ºå¤šä¸ªæµ‹è¯•æ—¶çš„ç‰‡æ®µï¼Œå¹¶æå‡ºç´¯ç§¯é—æ†¾ä½œä¸ºè¡¡é‡æµ‹è¯•æ—¶é—´è®¡ç®—æ•ˆç‡çš„æ ‡å‡†ã€‚æ–‡ç« æŒ‡å‡ºå½“å‰ä¸»æµæ–¹æ³•å¹¶æœªæœ€å°åŒ–é—æ†¾ï¼Œå¹¶æå‡ºäº†ç»“åˆå¯†é›†å¥–åŠ±å¥–é‡‘å’Œç»“æœ0&#x2F;1å¥–åŠ±RLçš„æ–¹æ³•æ¥å®ç°æœ€å°åŒ–é—æ†¾ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæ–‡ç« å¼€å‘äº†ä¸€ç§æ–°çš„å¾®è°ƒæ–¹æ³•â€”â€”å…ƒå¼ºåŒ–å¾®è°ƒï¼ˆMRTï¼‰ï¼Œç”¨äºä¼˜åŒ–æµ‹è¯•æ—¶é—´è®¡ç®—ã€‚ä¸ç»“æœå¥–åŠ±RLç›¸æ¯”ï¼ŒMRTåœ¨æ¨ç†æ€§èƒ½ä¸Šå®ç°äº†ç›¸å¯¹2-3å€çš„å¢ç›Šï¼Œå¹¶åœ¨æ ‡è®°æ•ˆç‡ä¸Šå®ç°äº†å¤§çº¦1.5å€çš„å¢ç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®­ç»ƒæ¨¡å‹ä»¥æœ‰æ•ˆåˆ©ç”¨æµ‹è¯•æ—¶é—´è®¡ç®—å¯¹äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦é€šè¿‡å¾®è°ƒæœç´¢ç—•è¿¹æˆ–ä½¿ç”¨ç»“æœå¥–åŠ±å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–æµ‹è¯•æ—¶é—´è®¡ç®—ã€‚</li>
<li>è¯¥æ–‡å°†æµ‹è¯•æ—¶é—´è®¡ç®—ä¼˜åŒ–é—®é¢˜å½¢å¼åŒ–ä¸ºä¸€ä¸ªå…ƒå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é—®é¢˜ï¼Œæä¾›äº†ä¸€ç§åŸåˆ™æ€§çš„è§†è§’æ¥åˆ†é…æµ‹è¯•æ—¶é—´è®¡ç®—èµ„æºã€‚</li>
<li>ç´¯ç§¯é—æ†¾æ˜¯è¡¡é‡æµ‹è¯•æ—¶é—´è®¡ç®—æ•ˆç‡çš„æœ‰æ•ˆæŒ‡æ ‡ï¼Œæ—¨åœ¨å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨è¾“å‡ºä»¤ç‰Œä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹å¹¶æ²¡æœ‰æœ€å°åŒ–ç´¯ç§¯é—æ†¾ï¼Œä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ç»“åˆå¯†é›†å¥–åŠ±å¥–é‡‘å’Œç»“æœå¥–åŠ±RLçš„æ–¹æ³•æ¥å®ç°æœ€å°åŒ–é—æ†¾ã€‚</li>
<li>åŸºäºä¸Šè¿°è§è§£ï¼Œæ–‡ç« å¼•å…¥äº†ä¸€ç§æ–°å‹çš„å¾®è°ƒæ–¹æ³•â€”â€”å…ƒå¼ºåŒ–å¾®è°ƒï¼ˆMRTï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07572">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.07572v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.07572v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.07572v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="VisRL-Intention-Driven-Visual-Perception-via-Reinforced-Reasoning"><a href="#VisRL-Intention-Driven-Visual-Perception-via-Reinforced-Reasoning" class="headerlink" title="VisRL: Intention-Driven Visual Perception via Reinforced Reasoning"></a>VisRL: Intention-Driven Visual Perception via Reinforced Reasoning</h2><p><strong>Authors:Zhangquan Chen, Xufang Luo, Dongsheng Li</strong></p>
<p>Visual understanding is inherently intention-driven - humans selectively focus on different regions of a scene based on their goals. Recent advances in large multimodal models (LMMs) enable flexible expression of such intentions through natural language, allowing queries to guide visual reasoning processes. Frameworks like Visual Chain-of-Thought have demonstrated the benefit of incorporating explicit reasoning steps, where the model predicts a focus region before answering a query. However, existing approaches rely heavily on supervised training with annotated intermediate bounding boxes, which severely limits scalability due to the combinatorial explosion of intention-region pairs. To overcome this limitation, we propose VisRL, the first framework that applies reinforcement learning (RL) to the problem of intention-driven visual perception. VisRL optimizes the entire visual reasoning process using only reward signals. By treating intermediate focus selection as a internal decision optimized through trial-and-error, our method eliminates the need for costly region annotations while aligning more closely with how humans learn to perceive the world. Extensive experiments across multiple benchmarks show that VisRL consistently outperforms strong baselines, demonstrating both its effectiveness and its strong generalization across different LMMs. Our code is available at this <a target="_blank" rel="noopener" href="https://github.com/zhangquanchen/VisRL">URL</a>. </p>
<blockquote>
<p>è§†è§‰ç†è§£æœ¬è´¨ä¸Šæ˜¯ç›®æ ‡é©±åŠ¨çš„â€”â€”äººç±»ä¼šæ ¹æ®è‡ªå·±çš„ç›®æ ‡é€‰æ‹©æ€§åœ°å…³æ³¨åœºæ™¯ä¸­çš„ä¸åŒåŒºåŸŸã€‚æœ€è¿‘ï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„è¿›æ­¥èƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€çµæ´»åœ°è¡¨è¾¾æ„å›¾ï¼Œå…è®¸æŸ¥è¯¢å¼•å¯¼è§†è§‰æ¨ç†è¿‡ç¨‹ã€‚åƒâ€œè§†è§‰æ€ç»´é“¾â€è¿™æ ·çš„æ¡†æ¶å·²ç»è¯æ˜äº†åŠ å…¥æ˜ç¡®æ¨ç†æ­¥éª¤çš„å¥½å¤„ï¼Œæ¨¡å‹åœ¨å›ç­”æŸ¥è¯¢ä¹‹å‰ä¼šé¢„æµ‹ä¸€ä¸ªå…³æ³¨åŒºåŸŸã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºä½¿ç”¨æ ‡æ³¨çš„ä¸­é—´è¾¹ç•Œæ¡†è¿›è¡Œæœ‰ç›‘ç£è®­ç»ƒï¼Œè¿™ç”±äºæ„å›¾åŒºåŸŸå¯¹çš„ç»„åˆçˆ†ç‚¸è€Œä¸¥é‡é™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†VisRLï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åº”ç”¨äºç›®æ ‡é©±åŠ¨è§†è§‰æ„ŸçŸ¥é—®é¢˜çš„æ¡†æ¶ã€‚VisRLä½¿ç”¨å¥–åŠ±ä¿¡å·ä¼˜åŒ–æ•´ä¸ªè§†è§‰æ¨ç†è¿‡ç¨‹ã€‚é€šè¿‡å°†ä¸­é—´ç„¦ç‚¹é€‰æ‹©è§†ä¸ºé€šè¿‡è¯•é”™ä¼˜åŒ–çš„å†…éƒ¨å†³ç­–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¶ˆé™¤äº†å¯¹æ˜‚è´µçš„åŒºåŸŸæ ‡æ³¨çš„éœ€æ±‚ï¼ŒåŒæ—¶æ›´è´´è¿‘äººç±»æ„ŸçŸ¥ä¸–ç•Œçš„å­¦ä¹ æ–¹å¼ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒVisRLæŒç»­è¶…è¶Šå¼ºå¤§çš„åŸºå‡†æµ‹è¯•ï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒLMMsä¸­çš„æœ‰æ•ˆæ€§å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhangquanchen/VisRL">URL</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07523v1">PDF</a> 18pages,11 figures</p>
<p><strong>Summary</strong><br>è§†è§‰ç†è§£æœ¬è´¨ä¸Šæ˜¯ç›®æ ‡é©±åŠ¨çš„ï¼Œäººç±»ä¼šæ ¹æ®ç›®æ ‡æœ‰é€‰æ‹©åœ°å…³æ³¨åœºæ™¯çš„ä¸åŒåŒºåŸŸã€‚æœ€è¿‘çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„è¿›æ­¥å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€è¡¨è¾¾è¿™ç§æ„å›¾ï¼Œé€šè¿‡æŸ¥è¯¢å¼•å¯¼è§†è§‰æ¨ç†è¿‡ç¨‹ã€‚VisRLæ˜¯é¦–ä¸ªå°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åº”ç”¨äºç›®æ ‡é©±åŠ¨è§†è§‰æ„ŸçŸ¥é—®é¢˜çš„æ¡†æ¶ã€‚å®ƒé€šè¿‡å¥–åŠ±ä¿¡å·ä¼˜åŒ–æ•´ä¸ªè§†è§‰æ¨ç†è¿‡ç¨‹ï¼Œé€šè¿‡å°†ä¸­é—´ç„¦ç‚¹é€‰æ‹©è§†ä¸ºé€šè¿‡è¯•é”™ä¼˜åŒ–çš„å†…éƒ¨å†³ç­–ï¼Œæ—¢æ¶ˆé™¤äº†å¯¹æ˜‚è´µçš„åŒºåŸŸæ³¨é‡Šçš„éœ€æ±‚ï¼Œåˆæ›´è´´è¿‘äººç±»æ„ŸçŸ¥ä¸–ç•Œçš„å­¦ä¹ æ–¹å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰ç†è§£æ˜¯äººç±»æœ‰ç›®æ ‡çš„è¡Œä¸ºï¼Œäººç±»ä¼šåŸºäºç›®æ ‡é€‰æ‹©æ€§å…³æ³¨åœºæ™¯çš„ä¸åŒéƒ¨åˆ†ã€‚</li>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰çš„è¿›æ­¥è®©äººç±»æ„å›¾çš„è¡¨è¾¾æ›´çµæ´»ï¼Œå¯ä»¥é€šè¿‡æŸ¥è¯¢å¼•å¯¼è§†è§‰æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–æ ‡æ³¨çš„ä¸­é—´è¾¹ç•Œæ¡†è¿›è¡Œè®­ç»ƒï¼Œé™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚</li>
<li>VisRLæ¡†æ¶é¦–æ¬¡å°†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åº”ç”¨äºç›®æ ‡é©±åŠ¨çš„è§†è§‰æ„ŸçŸ¥é—®é¢˜ã€‚</li>
<li>VisRLä¼˜åŒ–æ•´ä¸ªè§†è§‰æ¨ç†è¿‡ç¨‹ä»…ä½¿ç”¨å¥–åŠ±ä¿¡å·ã€‚</li>
<li>VisRLé€šè¿‡è¯•é”™ä¼˜åŒ–ä¸­é—´ç„¦ç‚¹é€‰æ‹©ï¼Œæ— éœ€æ˜‚è´µçš„åŒºåŸŸæ ‡æ³¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07523">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.07523v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.07523v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.07523v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.07523v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GRITHopper-Decomposition-Free-Multi-Hop-Dense-Retrieval"><a href="#GRITHopper-Decomposition-Free-Multi-Hop-Dense-Retrieval" class="headerlink" title="GRITHopper: Decomposition-Free Multi-Hop Dense Retrieval"></a>GRITHopper: Decomposition-Free Multi-Hop Dense Retrieval</h2><p><strong>Authors:Justus-Jonas Erker, Nils Reimers, Iryna Gurevych</strong></p>
<p>Decomposition-based multi-hop retrieval methods rely on many autoregressive steps to break down complex queries, which breaks end-to-end differentiability and is computationally expensive. Decomposition-free methods tackle this, but current decomposition-free approaches struggle with longer multi-hop problems and generalization to out-of-distribution data. To address these challenges, we introduce GRITHopper-7B, a novel multi-hop dense retrieval model that achieves state-of-the-art performance on both in-distribution and out-of-distribution benchmarks. GRITHopper combines generative and representational instruction tuning by integrating causal language modeling with dense retrieval training. Through controlled studies, we find that incorporating additional context after the retrieval process, referred to as post-retrieval language modeling, enhances dense retrieval performance. By including elements such as final answers during training, the model learns to better contextualize and retrieve relevant information. GRITHopper-7B offers a robust, scalable, and generalizable solution for multi-hop dense retrieval, and we release it to the community for future research and applications requiring multi-hop reasoning and retrieval capabilities. </p>
<blockquote>
<p>åŸºäºåˆ†è§£çš„å¤šè·³æ£€ç´¢æ–¹æ³•ä¾èµ–äºè®¸å¤šè‡ªå›å½’æ­¥éª¤æ¥åˆ†è§£å¤æ‚æŸ¥è¯¢ï¼Œè¿™ç ´åäº†ç«¯åˆ°ç«¯çš„å¯åŒºåˆ†æ€§ï¼Œå¹¶ä¸”è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚æ— åˆ†è§£çš„æ–¹æ³•è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œä½†å½“å‰çš„æ— åˆ†è§£æ–¹æ³•åœ¨å¤„ç†è¾ƒé•¿çš„å¤šè·³é—®é¢˜å’Œæ³›åŒ–åˆ°åˆ†å¸ƒå¤–æ•°æ®æ—¶é¢ä¸´å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†GRITHopper-7Bï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šè·³å¯†é›†æ£€ç´¢æ¨¡å‹ï¼Œåœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–çš„åŸºå‡†æµ‹è¯•ä¸­å‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚GRITHopperé€šè¿‡é›†æˆå› æœè¯­è¨€å»ºæ¨¡ä¸å¯†é›†æ£€ç´¢è®­ç»ƒï¼Œè°ƒæ•´äº†ç”Ÿæˆå’Œä»£è¡¨æ€§æŒ‡ä»¤ã€‚é€šè¿‡å¯¹ç…§ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°æ£€ç´¢è¿‡ç¨‹åèå…¥é¢å¤–çš„ä¸Šä¸‹æ–‡â€”â€”è¢«ç§°ä¸ºåæ£€ç´¢è¯­è¨€å»ºæ¨¡ï¼Œèƒ½å¢å¼ºå¯†é›†æ£€ç´¢çš„æ€§èƒ½ã€‚é€šè¿‡åœ¨è®­ç»ƒä¸­åŠ å…¥æœ€åç­”æ¡ˆç­‰å…ƒç´ ï¼Œæ¨¡å‹å­¦ä¼šäº†æ›´å¥½åœ°ä¸Šä¸‹æ–‡åŒ–å’Œæ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚GRITHopper-7Bä¸ºå¤šè·³å¯†é›†æ£€ç´¢æä¾›äº†ç¨³å¥ã€å¯æ‰©å±•å’Œé€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬å°†å…¶å‘å¸ƒç»™ç¤¾åŒºï¼Œä»¥ä¾›æœªæ¥ç ”ç©¶å’Œéœ€è¦å¤šè·³æ¨ç†å’Œæ£€ç´¢èƒ½åŠ›çš„åº”ç”¨ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07519v1">PDF</a> Under Review at ACL Rolling Review (ARR)</p>
<p><strong>Summary</strong></p>
<p>åˆ†è§£å¼å¤šè·³æ£€ç´¢æ–¹æ³•ä¾èµ–äºå¤šä¸ªè‡ªå›å½’æ­¥éª¤æ¥åˆ†è§£å¤æ‚æŸ¥è¯¢ï¼Œè¿™ç ´åäº†ç«¯åˆ°ç«¯çš„å¯åŒºåˆ†æ€§ï¼Œå¹¶ä¸”è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚æ— åˆ†è§£æ–¹æ³•è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œä½†å½“å‰çš„æ— åˆ†è§£æ–¹æ³•åœ¨å¤„ç†è¾ƒé•¿çš„å¤šè·³é—®é¢˜å’Œæ³›åŒ–åˆ°åˆ†å¸ƒå¤–æ•°æ®æ—¶é‡åˆ°å›°éš¾ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†GRITHopper-7Bï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šè·³å¯†é›†æ£€ç´¢æ¨¡å‹ï¼Œåœ¨å†…å¤–åˆ†å¸ƒåŸºå‡†æµ‹è¯•ä¸Šå‡å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚GRITHopperé€šè¿‡æ•´åˆå› æœè¯­è¨€å»ºæ¨¡ä¸å¯†é›†æ£€ç´¢è®­ç»ƒï¼Œå®ç°äº†ç”Ÿæˆæ€§å’Œä»£è¡¨æ€§æŒ‡ä»¤è°ƒæ•´ã€‚é€šè¿‡å¯¹ç…§ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°ï¼Œåœ¨æ£€ç´¢è¿‡ç¨‹åå¢åŠ é¢å¤–çš„ä¸Šä¸‹æ–‡ï¼Œå³æ‰€è°“çš„åæ£€ç´¢è¯­è¨€å»ºæ¨¡ï¼Œå¯ä»¥å¢å¼ºå¯†é›†æ£€ç´¢çš„æ€§èƒ½ã€‚é€šè¿‡åœ¨è®­ç»ƒä¸­åŒ…å«æœ€ç»ˆç­”æ¡ˆç­‰å…ƒç´ ï¼Œæ¨¡å‹å­¦ä¼šäº†æ›´å¥½åœ°ä¸Šä¸‹æ–‡åŒ–å’Œæ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚GRITHopper-7Bä¸ºéœ€è¦å¤šè·³æ¨ç†å’Œæ£€ç´¢èƒ½åŠ›çš„æœªæ¥ç ”ç©¶ä¸åº”ç”¨æä¾›äº†ç¨³å¥ã€å¯æ‰©å±•å’Œé€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè·³æ£€ç´¢é¢ä¸´åˆ†è§£å¤æ‚æŸ¥è¯¢çš„æŒ‘æˆ˜å’Œè®¡ç®—æˆæœ¬é—®é¢˜ã€‚</li>
<li>å½“å‰çš„æ— åˆ†è§£æ–¹æ³•åœ¨å¤„ç†é•¿å¤šè·³é—®é¢˜å’Œæ³›åŒ–åˆ°åˆ†å¸ƒå¤–æ•°æ®æ—¶å­˜åœ¨å›°éš¾ã€‚</li>
<li>GRITHopper-7Bæ˜¯ä¸€ä¸ªæ–°å‹çš„å¤šè·³å¯†é›†æ£€ç´¢æ¨¡å‹ï¼Œå®ç°ç”Ÿæˆæ€§å’Œä»£è¡¨æ€§æŒ‡ä»¤è°ƒæ•´ã€‚</li>
<li>GRITHopperç»“åˆäº†å› æœè¯­è¨€å»ºæ¨¡å’Œå¯†é›†æ£€ç´¢è®­ç»ƒã€‚</li>
<li>åæ£€ç´¢è¯­è¨€å»ºæ¨¡ï¼ˆåœ¨æ£€ç´¢è¿‡ç¨‹åå¢åŠ é¢å¤–çš„ä¸Šä¸‹æ–‡ï¼‰å¢å¼ºäº†å¯†é›†æ£€ç´¢çš„æ€§èƒ½ã€‚</li>
<li>åœ¨è®­ç»ƒä¸­åŒ…å«æœ€ç»ˆç­”æ¡ˆç­‰å…ƒç´ ä½¿æ¨¡å‹èƒ½æ›´å¥½åœ°ä¸Šä¸‹æ–‡åŒ–å’Œæ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07519">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.07519v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.07519v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.07519v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Vision-R1-Incentivizing-Reasoning-Capability-in-Multimodal-Large-Language-Models"><a href="#Vision-R1-Incentivizing-Reasoning-Capability-in-Multimodal-Large-Language-Models" class="headerlink" title="Vision-R1: Incentivizing Reasoning Capability in Multimodal Large   Language Models"></a>Vision-R1: Incentivizing Reasoning Capability in Multimodal Large   Language Models</h2><p><strong>Authors:Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, Shaohui Lin</strong></p>
<p>DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the modelâ€™s ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of $\sim$6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. The datasets and code will be released in: <a target="_blank" rel="noopener" href="https://github.com/Osilly/Vision-R1">https://github.com/Osilly/Vision-R1</a> . </p>
<blockquote>
<p>DeepSeek-R1-Zeroå·²æˆåŠŸå±•ç¤ºäº†å¤§è¯­è¨€æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å‡ºç°æ¨ç†èƒ½åŠ›ã€‚å—æ­¤çªç ´å¯å‘ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¦‚ä½•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æé«˜å°è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›´æ¥ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒå¾ˆéš¾åœ¨å°å‹è¯­è¨€æ¨¡å‹ä¸­æ¿€æ´»å¤æ‚çš„æ¨ç†èƒ½åŠ›ï¼Œå¦‚æé—®å’Œåæ€ï¼Œè¿™æ˜¯å› ä¸ºç¼ºä¹å¤§é‡é«˜è´¨é‡çš„å¤šæ¨¡æ€æ¨ç†æ•°æ®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€å°å‹è¯­è¨€æ¨¡å‹â€”â€”Vision-R1ï¼Œä»¥æå‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ©ç”¨ç°æœ‰çš„å°å‹è¯­è¨€æ¨¡å‹å’ŒDeepSeek-R1é€šè¿‡æ¨¡æ€æ¡¥æ¥å’Œæ•°æ®è¿‡æ»¤æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„å¤šæ¨¡æ€å› æœæ¨ç†æ•°æ®é›†Vision-R1-coldæ•°æ®é›†ï¼Œå…¶è§„æ¨¡ä¸ºäºŒåä¸‡ï¼Œæ— éœ€äººå·¥æ ‡æ³¨ï¼Œç”¨ä½œVision-R1çš„å†·å¯åŠ¨åˆå§‹åŒ–æ•°æ®ã€‚ä¸ºäº†ç¼“è§£å†·å¯åŠ¨åè¿‡åº¦æ€è€ƒå¸¦æ¥çš„ä¼˜åŒ–æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ¸è¿›æ€è€ƒæŠ‘åˆ¶è®­ç»ƒï¼ˆPTSTï¼‰ç­–ç•¥ï¼Œå¹¶é‡‡ç”¨å¸¦æœ‰ç¡¬æ ¼å¼åŒ–ç»“æœå¥–åŠ±å‡½æ•°çš„ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¥é€æ­¥æ”¹è¿›æ¨¡å‹åœ¨ä¸‡çº§å¤šæ¨¡æ€æ•°å­¦æ•°æ®é›†ä¸Šå­¦ä¹ æ­£ç¡®ä¸”å¤æ‚æ¨ç†è¿‡ç¨‹çš„èƒ½åŠ›ã€‚å…¨é¢çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å„ç§å¤šæ¨¡æ€æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†çº¦6%ã€‚Vision-R1-7Båœ¨æ•°å­¦è§†é‡åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡ä¸º73.5%ï¼Œä»…æ¯”é¢†å…ˆçš„æ¨ç†æ¨¡å‹OpenAI O1ä½0.4%ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Osilly/Vision-R-%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/Osilly/Vision-R-å‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06749v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²åœ¨LLMsä¸­å±•ç°å‡ºæ¨ç†èƒ½åŠ›ã€‚ä¸ºè§£å†³ç¼ºä¹é«˜è´¨é‡å¤šæ¨¡æ€æ¨ç†æ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº†ä½¿ç”¨Vision-R1çš„æ¨¡å‹ä»¥å¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚æ¨¡å‹é€šè¿‡ä½¿ç”¨å·²å­˜åœ¨çš„MLLMä¸DeepSeek-R1é€šè¿‡æ¨¡æ€æ¡¥æ¥å’Œè¿‡æ»¤æŠ€æœ¯æ„å»ºäº†ä¸€ä¸ªæ— äººå·¥æ ‡æ³¨çš„20ä¸‡æ¡å¤šæ¨¡æ€CoTæ•°æ®é›†ä½œä¸ºå†·å¯åŠ¨æ•°æ®ã€‚é‡‡ç”¨æ¸è¿›æ€è€ƒæŠ‘åˆ¶è®­ç»ƒï¼ˆPTSTï¼‰ç­–ç•¥å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æŠ€æœ¯æå‡æ¨¡å‹çš„ä¼˜åŒ–æ•ˆç‡ã€‚åœ¨å¤šç§å¤šæ¨¡æ€æ•°å­¦æ¨ç†æµ‹è¯•ä¸­å¹³å‡æå‡äº†çº¦6%ã€‚è¯¥æ¨¡å‹åœ¨MathVistaåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†73.5%çš„å‡†ç¡®ç‡ï¼Œä¸é¢†å…ˆçš„æ¨ç†æ¨¡å‹OpenAI O1ç›¸æ¯”ä»…ä½0.4%ã€‚æ•°æ®é›†å’Œä»£ç å°†åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-R1-Zeroå±•ç¤ºäº†é€šè¿‡å¼ºåŒ–å­¦ä¹ ä½¿LLMsäº§ç”Ÿæ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•ã€‚</li>
<li>RLç›´æ¥è®­ç»ƒåœ¨MLLMä¸­æ¿€æ´»å¤æ‚æ¨ç†èƒ½åŠ›ï¼ˆå¦‚è´¨ç–‘å’Œåæ€ï¼‰å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºç¼ºä¹é«˜è´¨é‡å¤šæ¨¡æ€æ¨ç†æ•°æ®ã€‚</li>
<li>æå‡ºVision-R1æ¨¡å‹ä»¥å¢å¼ºå¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨ç°æœ‰MLLMå’ŒDeepSeek-R1æ„å»ºäº†æ— äººå·¥æ ‡æ³¨çš„å¤šæ¨¡æ€CoTæ•°æ®é›†Vision-R1-coldä½œä¸ºå†·å¯åŠ¨æ•°æ®ã€‚</li>
<li>é‡‡ç”¨PTSTç­–ç•¥å’ŒGRPOæŠ€æœ¯è§£å†³ä¼˜åŒ–æŒ‘æˆ˜å¹¶æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªå¤šæ¨¡æ€æ•°å­¦æ¨ç†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡æå‡çº¦6%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06749">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.06749v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.06749v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.06749v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.06749v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="CalliReader-Contextualizing-Chinese-Calligraphy-via-an-Embedding-Aligned-Vision-Language-Model"><a href="#CalliReader-Contextualizing-Chinese-Calligraphy-via-an-Embedding-Aligned-Vision-Language-Model" class="headerlink" title="CalliReader: Contextualizing Chinese Calligraphy via an   Embedding-Aligned Vision-Language Model"></a>CalliReader: Contextualizing Chinese Calligraphy via an   Embedding-Aligned Vision-Language Model</h2><p><strong>Authors:Yuxuan Luo, Jiaqi Tang, Chenyi Huang, Feiyang Hao, Zhouhui Lian</strong></p>
<p>Chinese calligraphy, a UNESCO Heritage, remains computationally challenging due to visual ambiguity and cultural complexity. Existing AI systems fail to contextualize their intricate scripts, because of limited annotated data and poor visual-semantic alignment. We propose CalliReader, a vision-language model (VLM) that solves the Chinese Calligraphy Contextualization (CC$^2$) problem through three innovations: (1) character-wise slicing for precise character extraction and sorting, (2) CalliAlign for visual-text token compression and alignment, (3) embedding instruction tuning (e-IT) for improving alignment and addressing data scarcity. We also build CalliBench, the first benchmark for full-page calligraphic contextualization, addressing three critical issues in previous OCR and VQA approaches: fragmented context, shallow reasoning, and hallucination. Extensive experiments including user studies have been conducted to verify our CalliReaderâ€™s \textbf{superiority to other state-of-the-art methods and even human professionals in page-level calligraphy recognition and interpretation}, achieving higher accuracy while reducing hallucination. Comparisons with reasoning models highlight the importance of accurate recognition as a prerequisite for reliable comprehension. Quantitative analyses validate CalliReaderâ€™s efficiency; evaluations on document and real-world benchmarks confirm its robust generalization ability. </p>
<blockquote>
<p>ä¸­å›½ä¹¦æ³•å›½ç²¹ï¼Œä½œä¸ºè”åˆå›½æ•™ç§‘æ–‡ç»„ç»‡ï¼ˆUNESCOï¼‰é—äº§ï¼Œä»ç„¶å…·æœ‰è®¡ç®—ä¸Šçš„æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå­˜åœ¨è§†è§‰æ¨¡ç³Šå’Œæ–‡åŒ–å¤æ‚æ€§ã€‚ç°æœ‰çš„AIç³»ç»Ÿæ— æ³•å¯¹å…¶å¤æ‚çš„è„šæœ¬è¿›è¡Œè¯­å¢ƒåŒ–ç†è§£ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºæ ‡æ³¨æ•°æ®æœ‰é™ä»¥åŠè§†è§‰è¯­ä¹‰å¯¹é½ä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†CalliReaderï¼Œä¸€ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå®ƒé€šè¿‡ä¸‰é¡¹åˆ›æ–°è§£å†³äº†ä¸­æ–‡ä¹¦æ³•è¯­å¢ƒåŒ–ï¼ˆCC$^2$ï¼‰é—®é¢˜ï¼š1ï¼‰å­—ç¬¦çº§åˆ‡ç‰‡è¿›è¡Œç²¾ç¡®å­—ç¬¦æå–å’Œæ’åºï¼›2ï¼‰CalliAlignè¿›è¡Œè§†è§‰æ–‡æœ¬æ ‡è®°å‹ç¼©å’Œå¯¹é½ï¼›3ï¼‰åµŒå…¥æŒ‡ä»¤å¾®è°ƒï¼ˆe-ITï¼‰ä»¥æé«˜å¯¹é½å¹¶è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚æˆ‘ä»¬è¿˜å»ºç«‹äº†CalliBenchï¼Œé¦–ä¸ªå…¨é¡µä¹¦æ³•è¯­å¢ƒåŒ–çš„åŸºå‡†æµ‹è¯•ï¼Œè§£å†³äº†ä¹‹å‰OCRå’ŒVQAæ–¹æ³•ä¸­çš„ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–ã€æ¨ç†æµ…æ˜¾å’Œå¹»è§‰ã€‚è¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬ç”¨æˆ·ç ”ç©¶ï¼Œä»¥éªŒè¯æˆ‘ä»¬çš„CalliReaderåœ¨é¡µçº§ä¹¦æ³•è¯†åˆ«å’Œè§£é‡Šæ–¹é¢ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ç”šè‡³ä¸“ä¸šä¹¦æ³•å®¶ï¼Œåœ¨æé«˜å‡†ç¡®æ€§çš„åŒæ—¶å‡å°‘äº†å¹»è§‰ã€‚ä¸æ¨ç†æ¨¡å‹çš„æ¯”è¾ƒçªå‡ºäº†å‡†ç¡®è¯†åˆ«ä½œä¸ºå¯é ç†è§£å…ˆå†³æ¡ä»¶çš„é‡è¦æ€§ã€‚å®šé‡åˆ†æéªŒè¯äº†CalliReaderçš„æ•ˆç‡ï¼›å¯¹æ–‡æ¡£å’ŒçœŸå®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•è¯„ä¼°è¯å®äº†å…¶ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06472v2">PDF</a> 11 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸­æ–‡ä¹¦æ³•è¿™ä¸€è”åˆå›½æ•™ç§‘æ–‡ç»„ç»‡é—äº§åœ¨è®¡ç®—ä¸Šæ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è§†è§‰æ¨¡ç³Šå’Œæ–‡åŒ–å¤æ‚æ€§ã€‚ç°æœ‰çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿå› ç¼ºä¹æ ‡æ³¨æ•°æ®å’Œè§†è§‰è¯­ä¹‰å¯¹é½ä¸ä½³è€Œæ— æ³•é€‚åº”å¤æ‚çš„ä¹¦æ³•æ–‡æœ¬ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†CalliReaderï¼Œè¿™æ˜¯ä¸€ç§è§£å†³ä¸­æ–‡ä¹¦æ³•ä¸Šä¸‹æ–‡åŒ–é—®é¢˜çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸‰é¡¹åˆ›æ–°æŠ€æœ¯ï¼šå­—ç¬¦çº§åˆ‡ç‰‡è¿›è¡Œç²¾ç¡®å­—ç¬¦æå–å’Œæ’åºã€CalliAlignè¿›è¡Œè§†è§‰æ–‡æœ¬ç¬¦å·å‹ç¼©å’Œå¯¹é½ä»¥åŠåµŒå…¥æŒ‡ä»¤è°ƒä¼˜æ”¹å–„å¯¹é½å¹¶åº”å¯¹æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å»ºç«‹äº†CalliBenchï¼Œé¦–ä¸ªå…¨é¡µä¹¦æ³•ä¸Šä¸‹æ–‡åŒ–çš„åŸºå‡†æµ‹è¯•ï¼Œè§£å†³äº†ä¹‹å‰OCRå’ŒVQAæ–¹æ³•ä¸­çš„ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–ã€æ¨ç†æµ…æ˜¾å’Œå¹»è§‰ç°è±¡ã€‚ç»è¿‡å¤§é‡å®éªŒå’Œç”¨æˆ·ç ”ç©¶éªŒè¯ï¼ŒCalliReaderåœ¨é¡µçº§ä¹¦æ³•è¯†åˆ«å’Œè§£é‡Šæ–¹é¢ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•å’Œä¸“ä¸šäººå£«ï¼Œå®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´ä½çš„å¹»è§‰ç°è±¡ã€‚ä¸æ¨ç†æ¨¡å‹çš„å¯¹æ¯”çªæ˜¾äº†å‡†ç¡®è¯†åˆ«ä½œä¸ºå¯é ç†è§£çš„å‰æçš„é‡è¦æ€§ã€‚å®šé‡åˆ†æéªŒè¯äº†CalliReaderçš„æ•ˆç‡ï¼Œå…¶åœ¨æ–‡æ¡£å’ŒçœŸå®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¯æ˜äº†å…¶ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸­æ–‡ä¹¦æ³•ä½œä¸ºè”åˆå›½æ•™ç§‘æ–‡ç»„ç»‡é—äº§åœ¨è®¡ç®—ä¸Šé¢ä¸´è§†è§‰æ¨¡ç³Šå’Œæ–‡åŒ–å¤æ‚æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰AIç³»ç»Ÿåœ¨å¤„ç†å¤æ‚ä¹¦æ³•æ–‡æœ¬æ—¶å­˜åœ¨å›°éš¾ï¼Œä¸»è¦ç”±äºæ ‡æ³¨æ•°æ®æœ‰é™å’Œè§†è§‰è¯­ä¹‰å¯¹é½ä¸ä½³ã€‚</li>
<li>CalliReaderæ˜¯ä¸€ç§è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡å­—ç¬¦çº§åˆ‡ç‰‡ã€CalliAlignå’ŒåµŒå…¥æŒ‡ä»¤è°ƒä¼˜ç­‰æŠ€æœ¯è§£å†³ä¸­æ–‡ä¹¦æ³•ä¸Šä¸‹æ–‡åŒ–é—®é¢˜ã€‚</li>
<li>CalliBenchæ˜¯å…¨é¡µä¹¦æ³•ä¸Šä¸‹æ–‡åŒ–çš„é¦–ä¸ªåŸºå‡†æµ‹è¯•ï¼Œè§£å†³äº†OCRå’ŒVQAæ–¹æ³•ä¸­çš„ä¸‰ä¸ªå…³é”®é—®é¢˜ã€‚</li>
<li>CalliReaderåœ¨é¡µçº§ä¹¦æ³•è¯†åˆ«å’Œè§£é‡Šæ–¹é¢ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•å’Œä¸“ä¸šäººå£«ï¼Œå…·æœ‰é«˜å‡†ç¡®æ€§å’Œä½å¹»è§‰ç°è±¡ã€‚</li>
<li>å‡†ç¡®è¯†åˆ«æ˜¯å¯é ç†è§£çš„å‰æï¼Œä¸æ¨ç†æ¨¡å‹çš„å¯¹æ¯”çªæ˜¾äº†è¿™ä¸€ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06472">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.06472v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.06472v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.06472v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.06472v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.06472v2/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.06472v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Rank-R1-Enhancing-Reasoning-in-LLM-based-Document-Rerankers-via-Reinforcement-Learning"><a href="#Rank-R1-Enhancing-Reasoning-in-LLM-based-Document-Rerankers-via-Reinforcement-Learning" class="headerlink" title="Rank-R1: Enhancing Reasoning in LLM-based Document Rerankers via   Reinforcement Learning"></a>Rank-R1: Enhancing Reasoning in LLM-based Document Rerankers via   Reinforcement Learning</h2><p><strong>Authors:Shengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin, Guido Zuccon</strong></p>
<p>In this paper, we introduce Rank-R1, a novel LLM-based reranker that performs reasoning over both the user query and candidate documents before performing the ranking task. Existing document reranking methods based on large language models (LLMs) typically rely on prompting or fine-tuning LLMs to order or label candidate documents according to their relevance to a query. For Rank-R1, we use a reinforcement learning algorithm along with only a small set of relevance labels (without any reasoning supervision) to enhance the reasoning ability of LLM-based rerankers. Our hypothesis is that adding reasoning capabilities to the rerankers can improve their relevance assessement and ranking capabilities. Our experiments on the TREC DL and BRIGHT datasets show that Rank-R1 is highly effective, especially for complex queries. In particular, we find that Rank-R1 achieves effectiveness on in-domain datasets at par with that of supervised fine-tuning methods, but utilizing only 18% of the training data used by the fine-tuning methods. We also find that the model largely outperforms zero-shot and supervised fine-tuning when applied to out-of-domain datasets featuring complex queries, especially when a 14B-size model is used. Finally, we qualitatively observe that Rank-R1â€™s reasoning process improves the explainability of the ranking results, opening new opportunities for search engine results presentation and fruition. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Rank-R1ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°å‹é‡æ’å™¨ï¼ˆRERankerï¼‰ï¼Œåœ¨è¿›è¡Œæ’åä»»åŠ¡ä¹‹å‰ï¼Œå®ƒå¯¹ç”¨æˆ·æŸ¥è¯¢å’Œå€™é€‰æ–‡æ¡£è¿›è¡Œæ¨ç†ã€‚åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç°æœ‰æ–‡æ¡£é‡æ’æ–¹æ³•é€šå¸¸ä¾èµ–äºæç¤ºæˆ–å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œä»¥æ ¹æ®å…¶ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§å¯¹å€™é€‰æ–‡æ¡£è¿›è¡Œæ’åºæˆ–æ ‡è®°ã€‚å¯¹äºRank-R1ï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨ä¸€å°å¥—ç›¸å…³æ€§æ ‡ç­¾ï¼ˆæ— éœ€ä»»ä½•æ¨ç†ç›‘ç£ï¼‰ç»“åˆå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»¥æé«˜åŸºäºLLMçš„é‡æ’å™¨çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å‡è®¾æ˜¯ï¼Œå‘é‡æ’å™¨æ·»åŠ æ¨ç†åŠŸèƒ½å¯ä»¥æé«˜å…¶ç›¸å…³æ€§è¯„ä¼°å’Œæ’åèƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨TREC DLå’ŒBRIGHTæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRank-R1éå¸¸æœ‰æ•ˆï¼Œå°¤å…¶é€‚ç”¨äºå¤æ‚æŸ¥è¯¢ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å‘ç°Rank-R1åœ¨åŸŸå†…æ•°æ®é›†ä¸Šçš„æ•ˆæœä¸ç›‘ç£å¾®è°ƒæ–¹æ³•ç›¸å½“ï¼Œä½†ä»…ä½¿ç”¨äº†å¾®è°ƒæ–¹æ³•æ‰€ä½¿ç”¨çš„18%çš„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œè¯¥æ¨¡å‹åœ¨å…·æœ‰å¤æ‚æŸ¥è¯¢çš„åŸŸå¤–æ•°æ®é›†ä¸Šçš„è¡¨ç°å¤§å¤§è¶…è¿‡äº†é›¶æ ·æœ¬å’Œç›‘ç£å¾®è°ƒï¼Œå°¤å…¶æ˜¯å½“ä½¿ç”¨14Bå¤§å°æ¨¡å‹æ—¶ã€‚æœ€åï¼Œæˆ‘ä»¬ä»æœ¬è´¨ä¸Šè§‚å¯Ÿåˆ°ï¼ŒRank-R1çš„æ¨ç†è¿‡ç¨‹æé«˜äº†æ’åç»“æœçš„å¯è§£é‡Šæ€§ï¼Œä¸ºæœç´¢å¼•æ“ç»“æœçš„å‘ˆç°å’Œäº«å—å¸¦æ¥äº†æ–°çš„æœºä¼šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06034v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Rank-R1ï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°é¢–æ’åºå™¨ã€‚è¯¥æ’åºå™¨åœ¨ç”¨æˆ·æŸ¥è¯¢å’Œå€™é€‰æ–‡æ¡£ä¸Šè¿›è¡Œæ¨ç†ï¼Œç„¶åè¿›è¡Œæ’åºä»»åŠ¡ã€‚ä¸ç°æœ‰çš„åŸºäºLLMçš„æ–‡æ¡£æ’åºæ–¹æ³•ä¸åŒï¼ŒRank-R1é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä»…ä½¿ç”¨å°‘é‡ç›¸å…³æ€§æ ‡ç­¾ï¼ˆæ— éœ€æ¨ç†ç›‘ç£ï¼‰æ¥æé«˜LLMæ’åºå™¨çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒRank-R1åœ¨å¤æ‚æŸ¥è¯¢æ–¹é¢å°¤å…¶æœ‰æ•ˆï¼Œå¯¹äºå†…éƒ¨æ•°æ®é›†å‡ ä¹è¾¾åˆ°ç›‘ç£å¾®è°ƒæ–¹æ³•çš„æ•ˆæœï¼Œä½†ä»…ä½¿ç”¨å…¶18%çš„è®­ç»ƒæ•°æ®ã€‚åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šï¼Œå°¤å…¶æ˜¯ä½¿ç”¨14Bå¤§å°çš„æ¨¡å‹æ—¶ï¼Œè¯¥æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼ŒRank-R1çš„æ¨ç†è¿‡ç¨‹æé«˜äº†æ’åç»“æœçš„å¯è§£é‡Šæ€§ï¼Œä¸ºæœç´¢å¼•æ“ç»“æœçš„å±•ç¤ºå’Œç”¨æˆ·ä½“éªŒæä¾›äº†æ–°çš„æœºä¼šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Rank-R1æ˜¯ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ’åºå™¨ï¼Œç»“åˆäº†å¼ºåŒ–å­¦ä¹ ç®—æ³•è¿›è¡Œæ–‡æ¡£æ’åºã€‚</li>
<li>ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒRank-R1ä»…ä½¿ç”¨å°‘é‡ç›¸å…³æ€§æ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€æ¨ç†ç›‘ç£ã€‚</li>
<li>Rank-R1åœ¨å¤æ‚æŸ¥è¯¢æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¯¹äºå†…éƒ¨æ•°æ®é›†å‡ ä¹è¾¾åˆ°ç›‘ç£å¾®è°ƒçš„æ•ˆæœï¼Œä½†ä½¿ç”¨äº†è¾ƒå°‘çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨å¤§å‹æ¨¡å‹æ—¶ï¼ŒRank-R1æ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬å’Œç›‘ç£å¾®è°ƒæ–¹æ³•ã€‚</li>
<li>Rank-R1çš„æ¨ç†è¿‡ç¨‹æé«˜äº†æ’åç»“æœçš„å¯è§£é‡Šæ€§ï¼Œå¢å¼ºäº†æœç´¢ç»“æœçš„å‘ˆç°å’Œç”¨æˆ·ä½“éªŒã€‚</li>
<li>è¯¥ç ”ç©¶æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„ç»“åˆæ¨ç†èƒ½åŠ›çš„å¤§è¯­è¨€æ¨¡å‹æ’åºå™¨çš„æ–¹æ³•ï¼Œä¸ºæœªæ¥æœç´¢å¼•æ“çš„å‘å±•æä¾›äº†æ–°çš„è§†è§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06034">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.06034v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.06034v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.06034v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.06034v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.06034v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MastermindEval-A-Simple-But-Scalable-Reasoning-Benchmark"><a href="#MastermindEval-A-Simple-But-Scalable-Reasoning-Benchmark" class="headerlink" title="MastermindEval: A Simple But Scalable Reasoning Benchmark"></a>MastermindEval: A Simple But Scalable Reasoning Benchmark</h2><p><strong>Authors:Jonas Golde, Patrick Haller, Fabio Barth, Alan Akbik</strong></p>
<p>Recent advancements in large language models (LLMs) have led to remarkable performance across a wide range of language understanding and mathematical tasks. As a result, increasing attention has been given to assessing the true reasoning capabilities of LLMs, driving research into commonsense, numerical, logical, and qualitative reasoning. However, with the rapid progress of reasoning-focused models such as OpenAIâ€™s o1 and DeepSeekâ€™s R1, there has been a growing demand for reasoning benchmarks that can keep pace with ongoing model developments. In this paper, we introduce MastermindEval, a simple, scalable, and interpretable deductive reasoning benchmark inspired by the board game Mastermind. Our benchmark supports two evaluation paradigms: (1) agentic evaluation, in which the model autonomously plays the game, and (2) deductive reasoning evaluation, in which the model is given a pre-played game state with only one possible valid code to infer. In our experimental results we (1) find that even easy Mastermind instances are difficult for current models and (2) demonstrate that the benchmark is scalable to possibly more advanced models in the future Furthermore, we investigate possible reasons why models cannot deduce the final solution and find that current models are limited in deducing the concealed code as the number of statement to combine information from is increasing. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•åœ¨å„ç§è¯­è¨€ç†è§£å’Œæ•°å­¦ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆç»©ã€‚å› æ­¤ï¼Œäººä»¬è¶Šæ¥è¶Šå…³æ³¨è¯„ä¼°LLMçš„çœŸæ­£æ¨ç†èƒ½åŠ›ï¼Œæ¨åŠ¨äº†å¸¸è¯†æ¨ç†ã€æ•°å€¼æ¨ç†ã€é€»è¾‘å’Œå®šæ€§æ¨ç†çš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œéšç€ä»¥OpenAIçš„o1å’ŒDeepSeekçš„R1ç­‰ä¸ºä»£è¡¨çš„æ¨ç†é‡ç‚¹æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹èƒ½å¤Ÿè·Ÿä¸Šå½“å‰æ¨¡å‹å‘å±•èŠ‚å¥çš„æ¨ç†åŸºå‡†æµ‹è¯•çš„éœ€æ±‚ä¹Ÿåœ¨å¢é•¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MastermindEvalï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•ã€å¯æ‰©å±•ã€å¯è§£é‡Šçš„ä»¥çŒœè°œæ¸¸æˆMastermindä¸ºçµæ„Ÿå¯å‘è€Œè¯ç”Ÿçš„æ¼”ç»æ¨ç†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ”¯æŒä¸¤ç§è¯„ä¼°èŒƒå¼ï¼šï¼ˆ1ï¼‰è‡ªä¸»è¯„ä¼°ï¼Œæ¨¡å‹è‡ªä¸»ç©æ¸¸æˆï¼›ï¼ˆ2ï¼‰æ¼”ç»æ¨ç†è¯„ä¼°ï¼Œæ¨¡å‹æ¥æ”¶ä¸€ä¸ªå·²ç»è¿›è¡Œè¿‡çš„æ¸¸æˆçŠ¶æ€ï¼Œå¹¶åªèƒ½æ¨æ–­å‡ºä¸€ä¸ªæœ‰æ•ˆçš„ä»£ç ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒç»“æœä¸­ï¼Œï¼ˆ1ï¼‰æˆ‘ä»¬å‘ç°å³ä½¿æ˜¯ç®€å•çš„çŒœè°œå®ä¾‹å¯¹äºå½“å‰æ¨¡å‹æ¥è¯´ä¹Ÿæ˜¯å›°éš¾çš„ï¼›ï¼ˆ2ï¼‰æˆ‘ä»¬è¯æ˜äº†è¯¥åŸºå‡†æµ‹è¯•åœ¨æœªæ¥å¯æ‰©å±•è‡³æ›´å…ˆè¿›çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†æ¨¡å‹æ— æ³•æ¨æ–­å‡ºæœ€ç»ˆè§£å†³æ–¹æ¡ˆçš„å¯èƒ½åŸå› ï¼Œå¹¶å‘ç°éšç€ç»“åˆä¿¡æ¯æ‰€éœ€é™ˆè¿°æ•°é‡çš„å¢åŠ ï¼Œå½“å‰æ¨¡å‹åœ¨æ¨æ–­éšè—ä»£ç æ–¹é¢çš„èƒ½åŠ›æœ‰é™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05891v4">PDF</a> 9 pages, 2 figures, 4 tables. In: ICLR 2025 Workshop on Reasoning and   Planning for Large Language Models</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥åœ¨å„ç§è¯­è¨€ç†è§£å’Œæ•°å­¦ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œå¼•å‘äº†äººä»¬å¯¹æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å…³æ³¨ã€‚ä¸ºè¯„ä¼°æ¨¡å‹åœ¨å¸¸è¯†æ¨ç†ã€æ•°å€¼æ¨ç†ã€é€»è¾‘å’Œå®šæ€§æ¨ç†ç­‰æ–¹é¢çš„èƒ½åŠ›ï¼Œäººä»¬è®¾è®¡äº†ä¸€ç§åŸºäºæ™ºåŠ›æ¸¸æˆçš„ç®€å•æ˜“æ‡‚çš„å¯æ‰©å±•æ¨ç†åŸºå‡†æµ‹è¯•MastermindEvalã€‚åˆæ­¥å®éªŒå‘ç°å³ä½¿æ˜¯ç®€å•çš„Mastermindå®ä¾‹å¯¹ç°æœ‰çš„æ¨¡å‹ä¹Ÿæœ‰éš¾åº¦ï¼Œè¯¥åŸºå‡†æµ‹è¯•å¯æ‰©å±•åˆ°æ›´å…ˆè¿›çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œæ¨¡å‹åœ¨æ¨å¯¼éšè—ä»£ç æ—¶å­˜åœ¨å±€é™æ€§ï¼Œå½“éœ€è¦ç»„åˆçš„ä¿¡æ¯é‡å¢åŠ æ—¶ï¼Œæ¨¡å‹æ— æ³•æ¨æ–­å‡ºæœ€ç»ˆè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥å·²æ˜¾è‘—æé«˜äº†å…¶åœ¨è¯­è¨€ç†è§£å’Œæ•°å­¦ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>éšç€æ¨¡å‹çš„å‘å±•ï¼Œå¯¹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›è¯„ä¼°éœ€æ±‚æ—¥ç›Šå¢åŠ ï¼ŒåŒ…æ‹¬å¸¸è¯†æ¨ç†ã€æ•°å€¼æ¨ç†ã€é€»è¾‘å’Œå®šæ€§æ¨ç†ç­‰æ–¹é¢ã€‚</li>
<li>MastermindEvalä½œä¸ºä¸€ç§åŸºäºæ™ºåŠ›æ¸¸æˆçš„æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MastermindEvalæ”¯æŒä¸¤ç§è¯„ä¼°æ¨¡å¼ï¼šè‡ªä¸»è¯„ä¼°æ¨¡å¼å’Œé€»è¾‘æ¨ç†è¯„ä¼°æ¨¡å¼ã€‚</li>
<li>å®éªŒå‘ç°ï¼Œç°æœ‰çš„æ¨¡å‹åœ¨é¢å¯¹ç®€å•çš„Mastermindå®ä¾‹æ—¶éƒ½æ„Ÿåˆ°å›°éš¾ã€‚</li>
<li>MastermindEvalåŸºå‡†æµ‹è¯•å…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¯é€‚ç”¨äºæœªæ¥æ›´å…ˆè¿›çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05891">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.05891v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.05891v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.05891v4/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.05891v4/page_5_1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="WritingBench-A-Comprehensive-Benchmark-for-Generative-Writing"><a href="#WritingBench-A-Comprehensive-Benchmark-for-Generative-Writing" class="headerlink" title="WritingBench: A Comprehensive Benchmark for Generative Writing"></a>WritingBench: A Comprehensive Benchmark for Generative Writing</h2><p><strong>Authors:Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, Fei Huang</strong></p>
<p>Recent advancements in large language models (LLMs) have significantly enhanced text generation capabilities, yet evaluating their performance in generative writing remains a challenge. Existing benchmarks primarily focus on generic text generation or limited in writing tasks, failing to capture the diverse requirements of high-quality written contents across various domains. To bridge this gap, we present WritingBench, a comprehensive benchmark designed to evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing creative, persuasive, informative, and technical writing. We further propose a query-dependent evaluation framework that empowers LLMs to dynamically generate instance-specific assessment criteria. This framework is complemented by a fine-tuned critic model for criteria-aware scoring, enabling evaluations in style, format and length. The frameworkâ€™s validity is further demonstrated by its data curation capability, which enables 7B-parameter models to approach state-of-the-art (SOTA) performance. We open-source the benchmark, along with evaluation tools and modular framework components, to advance the development of LLMs in writing. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æå¤§åœ°æå‡äº†æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œä½†åœ¨ç”Ÿæˆå†™ä½œä¸­è¯„ä¼°å…¶æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨é€šç”¨æ–‡æœ¬ç”Ÿæˆæˆ–æœ‰é™çš„å†™ä½œä»»åŠ¡ï¼Œæ— æ³•æ•æ‰è·¨ä¸åŒé¢†åŸŸé«˜è´¨é‡å†™ä½œå†…å®¹çš„å¤šæ ·åŒ–è¦æ±‚ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†WritingBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°LLMåœ¨6ä¸ªæ ¸å¿ƒå†™ä½œé¢†åŸŸå’Œ100ä¸ªå­é¢†åŸŸä¸­çš„è¡¨ç°ï¼Œæ¶µç›–åˆ›é€ æ€§ã€è¯´æœåŠ›ã€ä¿¡æ¯æ€§å’ŒæŠ€æœ¯æ€§å†™ä½œã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªæŸ¥è¯¢ç›¸å…³çš„è¯„ä¼°æ¡†æ¶ï¼Œä½¿LLMèƒ½å¤ŸåŠ¨æ€ç”Ÿæˆç‰¹å®šå®ä¾‹çš„è¯„ä¼°æ ‡å‡†ã€‚è¯¥æ¡†æ¶é€šè¿‡å¾®è°ƒåçš„æ‰¹è¯„æ¨¡å‹è¿›è¡Œæ ‡å‡†æ„ŸçŸ¥è¯„åˆ†ï¼Œä»¥é£æ ¼ã€æ ¼å¼å’Œé•¿åº¦ä¸ºè¯„ä¼°ä¾æ®ã€‚è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§è¿›ä¸€æ­¥ä½“ç°åœ¨å…¶æ•°æ®æ•´åˆèƒ½åŠ›ä¸Šï¼Œä½¿æ‹¥æœ‰7Bå‚æ•°çš„æ¨¡å‹èƒ½å¤Ÿæ¥è¿‘æœ€ä½³ï¼ˆSOTAï¼‰æ€§èƒ½ã€‚æˆ‘ä»¬å¼€æºåŸºå‡†æµ‹è¯•ã€è¯„ä¼°å·¥å…·å’Œæ¨¡å—åŒ–æ¡†æ¶ç»„ä»¶ï¼Œä»¥ä¿ƒè¿›å†™ä½œé¢†åŸŸLLMçš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05244v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ä¸Šæœ‰äº†æ˜¾è‘—æå‡ï¼Œä½†è¯„ä¼°å…¶ç”Ÿæˆå†™ä½œæ€§èƒ½ä»å…·æŒ‘æˆ˜ã€‚ç°æœ‰è¯„ä¼°æ ‡å‡†ä¸»è¦å…³æ³¨é€šç”¨æ–‡æœ¬ç”Ÿæˆæˆ–æœ‰é™å†™ä½œä»»åŠ¡ï¼Œæ— æ³•æ•æ‰ä¸åŒé¢†åŸŸé«˜è´¨é‡å†…å®¹çš„å¤šæ ·éœ€æ±‚ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºWritingBenchï¼Œä¸€ä¸ªè·¨6å¤§å†™ä½œé¢†åŸŸå’Œ100ä¸ªå­é¢†åŸŸçš„ç»¼åˆæ€§è¯„ä¼°æ ‡å‡†ï¼Œæ¶µç›–åˆ›æ„ã€è¯´æœã€ä¿¡æ¯å’ŒæŠ€æœ¯å†™ä½œã€‚æˆ‘ä»¬è¿˜æå‡ºä¸€ä¸ªæŸ¥è¯¢ä¾èµ–è¯„ä¼°æ¡†æ¶ï¼Œä½¿LLMèƒ½å¤ŸåŠ¨æ€ç”Ÿæˆç‰¹å®šå®ä¾‹çš„è¯„ä¼°æ ‡å‡†ã€‚ç»“åˆå¾®è°ƒè¯„è®ºå®¶æ¨¡å‹è¿›è¡Œæ ‡å‡†æ„ŸçŸ¥è¯„åˆ†ï¼Œå¯åœ¨é£æ ¼ã€æ ¼å¼å’Œé•¿åº¦æ–¹é¢è¿›è¡Œè¯„ä¼°ã€‚è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§é€šè¿‡å…¶æ•°æ®æ•´åˆèƒ½åŠ›å¾—åˆ°è¿›ä¸€æ­¥è¯æ˜ï¼Œä½¿7Bå‚æ•°æ¨¡å‹æ¥è¿‘æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æˆ‘ä»¬å¼€æºæ­¤è¯„ä¼°æ ‡å‡†ã€è¯„ä¼°å·¥å…·å’Œæ¨¡å—åŒ–æ¡†æ¶ç»„ä»¶ï¼Œä»¥ä¿ƒè¿›å†™ä½œé¢†åŸŸLLMçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è¯„ä¼°å…¶å†™ä½œæ€§èƒ½ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ ‡å‡†ä¸»è¦é›†ä¸­åœ¨é€šç”¨æ–‡æœ¬ç”Ÿæˆæˆ–æœ‰é™çš„å†™ä½œä»»åŠ¡ä¸Šï¼Œä¸èƒ½æ»¡è¶³è·¨é¢†åŸŸé«˜è´¨é‡å†…å®¹çš„å¤šæ ·åŒ–éœ€æ±‚ã€‚</li>
<li>WritingBenchæ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„è¯„ä¼°æ ‡å‡†ï¼Œè¦†ç›–åˆ›æ„ã€è¯´æœã€ä¿¡æ¯å’ŒæŠ€æœ¯å†™ä½œç­‰å¤šä¸ªé¢†åŸŸã€‚</li>
<li>å¼•å…¥æŸ¥è¯¢ä¾èµ–è¯„ä¼°æ¡†æ¶ï¼Œä½¿LLMèƒ½å¤ŸåŠ¨æ€ç”Ÿæˆå…·ä½“çš„è¯„ä¼°æ ‡å‡†ã€‚</li>
<li>ç»“åˆå¾®è°ƒè¯„è®ºå®¶æ¨¡å‹è¿›è¡Œæ ‡å‡†æ„ŸçŸ¥è¯„åˆ†ï¼Œæ¶µç›–é£æ ¼ã€æ ¼å¼å’Œé•¿åº¦ç­‰æ–¹é¢çš„è¯„ä¼°ã€‚</li>
<li>æ¡†æ¶çš„æœ‰æ•ˆæ€§é€šè¿‡å…¶æ•°æ®æ•´åˆèƒ½åŠ›å¾—åˆ°éªŒè¯ï¼Œä½¿æ¨¡å‹æ€§èƒ½æ¥è¿‘æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05244">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.05244v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.05244v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.05244v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.05244v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.05244v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.05244v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.05244v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="R1-Zeroâ€™s-â€œAha-Momentâ€-in-Visual-Reasoning-on-a-2B-Non-SFT-Model"><a href="#R1-Zeroâ€™s-â€œAha-Momentâ€-in-Visual-Reasoning-on-a-2B-Non-SFT-Model" class="headerlink" title="R1-Zeroâ€™s â€œAha Momentâ€ in Visual Reasoning on a 2B Non-SFT Model"></a>R1-Zeroâ€™s â€œAha Momentâ€ in Visual Reasoning on a 2B Non-SFT Model</h2><p><strong>Authors:Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, Cho-Jui Hsieh</strong></p>
<p>Recently DeepSeek R1 demonstrated how reinforcement learning with simple rule-based incentives can enable autonomous development of complex reasoning in large language models, characterized by the â€œaha momentâ€, in which the model manifest self-reflection and increased response length during training. However, attempts to extend this success to multimodal reasoning often failed to reproduce these key characteristics. In this report, we present the first successful replication of these emergent characteristics for multimodal reasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying reinforcement learning directly on the SAT dataset, our model achieves 59.47% accuracy on CVBench, outperforming the base model by approximately ~30% and exceeding both SFT setting by ~2%. In addition, we share our failed attempts and insights in attempting to achieve R1-like reasoning using RL with instruct models. aiming to shed light on the challenges involved. Our key observations include: (1) applying RL on instruct model often results in trivial reasoning trajectories, and (2) naive length reward are ineffective in eliciting reasoning capabilities. The project code is available at <a target="_blank" rel="noopener" href="https://github.com/turningpoint-ai/VisualThinker-R1-Zero">https://github.com/turningpoint-ai/VisualThinker-R1-Zero</a> </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒDeepSeek R1å±•ç¤ºäº†å¦‚ä½•é€šè¿‡åŸºäºç®€å•è§„åˆ™çš„æ¿€åŠ±æ¥åº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œä»è€Œåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è‡ªä¸»å‘å±•å‡ºå¤æ‚çš„æ¨ç†èƒ½åŠ›ï¼Œå…¶ç‰¹ç‚¹æ˜¯è¡¨ç°ä¸ºâ€œå•Šå“ˆæ—¶åˆ»â€ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¡¨ç°å‡ºè‡ªæˆ‘åæ€å’Œå“åº”é•¿åº¦çš„å¢åŠ ã€‚ç„¶è€Œï¼Œå°è¯•å°†è¿™ç§æˆåŠŸæ‰©å±•åˆ°å¤šæ¨¡æ€æ¨ç†æ—¶ï¼Œå¾€å¾€æ— æ³•é‡ç°è¿™äº›å…³é”®ç‰¹å¾ã€‚åœ¨æœ¬æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡åœ¨ä»…å¯¹éSFT 2Bæ¨¡å‹è¿›è¡Œå¤šæ¨¡æ€æ¨ç†æ—¶æˆåŠŸå¤åˆ¶äº†è¿™äº›çªå‘ç‰¹å¾ã€‚æˆ‘ä»¬ä»¥Qwen2-VL-2Bä¸ºèµ·ç‚¹ï¼Œç›´æ¥åœ¨SATæ•°æ®é›†ä¸Šåº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨CVBenchä¸Šè¾¾åˆ°äº†59.47%çš„å‡†ç¡®ç‡ï¼Œæ¯”åŸºç¡€æ¨¡å‹é«˜å‡ºçº¦30%ï¼Œå¹¶æ¯”SFTè®¾ç½®é«˜å‡ºçº¦2%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ†äº«äº†åœ¨ä½¿ç”¨RLè¿›è¡ŒæŒ‡ä»¤æ¨¡å‹ä»¥å®ç°R1å¼æ¨ç†çš„å°è¯•ä¸­çš„å¤±è´¥ç»éªŒå’Œè§è§£ï¼Œä»¥æœŸé˜æ˜æ‰€æ¶‰åŠçš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ä¸»è¦è§‚å¯Ÿç»“æœåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰åœ¨æŒ‡ä»¤æ¨¡å‹ä¸Šåº”ç”¨RLé€šå¸¸ä¼šå¯¼è‡´å¾®ä¸è¶³é“çš„æ¨ç†è½¨è¿¹ï¼Œï¼ˆ2ï¼‰ç®€å•çš„é•¿åº¦å¥–åŠ±åœ¨æ¿€å‘æ¨ç†èƒ½åŠ›æ–¹é¢æ— æ•ˆã€‚é¡¹ç›®ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/turningpoint-ai/VisualThinker-R1-Zero%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/turningpoint-ai/VisualThinker-R1-Zeroæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05132v2">PDF</a> 10 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DeepSeek R1å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸åŸºäºç®€å•è§„åˆ™çš„æ¿€åŠ±å®ç°å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªä¸»å‘å±•å¤æ‚æ¨ç†çš„æ¼”ç¤ºã€‚ç ”ç©¶ä¸­å‘ç°åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­ï¼Œä¸€äº›å…³é”®çš„ç‰¹æ€§æœªèƒ½å¤åˆ¶æˆåŠŸã€‚ä½†æŠ¥å‘Šæå‡ºé¦–æ¬¡æˆåŠŸå¤åˆ¶è¿™äº›ç‰¹æ€§ç”¨äºéSFT 2Bæ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†ä¸Šï¼Œåˆ©ç”¨å¼ºåŒ–å­¦ä¹ ç›´æ¥åº”ç”¨åœ¨SATæ•°æ®é›†ä¸Šçš„Qwen2-VL-2Bæ¨¡å‹å®ç°å‡†ç¡®ç‡ä¸ºCVBenchçš„59.47%ï¼Œç›¸æ¯”åŸºç¡€æ¨¡å‹æé«˜çº¦30%ï¼Œæ¯”SFTè®¾ç½®æé«˜çº¦2%ã€‚æ­¤å¤–ï¼ŒæŠ¥å‘Šè¿˜åˆ†äº«äº†å°è¯•ä½¿ç”¨RLè¿›è¡ŒæŒ‡ä»¤æ¨¡å‹æ—¶çš„å¤±è´¥ç»éªŒå’Œè§‚å¯Ÿç»“æœï¼ŒåŒ…æ‹¬RLåœ¨æŒ‡ä»¤æ¨¡å‹ä¸Šå¾€å¾€å¯¼è‡´å¹³åº¸çš„æ¨ç†è½¨è¿¹ä»¥åŠå•çº¯é•¿åº¦å¥–åŠ±æ— æ³•æ¿€å‘æ¨ç†èƒ½åŠ›ã€‚é¡¹ç›®ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/turningpoint-ai/VisualThinker-R1-Zero%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/turningpoint-ai/VisualThinker-R1-Zeroæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œç®€å•è§„åˆ™æ¿€åŠ±å®ç°å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªä¸»å‘å±•å¤æ‚æ¨ç†æ¼”ç¤ºã€‚</li>
<li>ç ”ç©¶é¢ä¸´å¤šæ¨¡æ€æ¨ç†çš„æŒ‘æˆ˜ï¼Œä½†æˆåŠŸå¤åˆ¶æŸäº›ç‰¹æ€§åœ¨éSFT 2Bæ¨¡å‹ä¸Šã€‚</li>
<li>Qwen2-VL-2Bæ¨¡å‹åœ¨SATæ•°æ®é›†ä¸Šåº”ç”¨å¼ºåŒ–å­¦ä¹ å®ç°é«˜å‡†ç¡®ç‡ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½ç›¸è¾ƒäºåŸºç¡€æ¨¡å‹æå‡çº¦30%ï¼Œç›¸è¾ƒäºSFTè®¾ç½®æå‡çº¦2%ã€‚</li>
<li>æŠ¥å‘Šåˆ†äº«äº†ä½¿ç”¨RLè¿›è¡ŒæŒ‡ä»¤æ¨¡å‹çš„å¤±è´¥ç»éªŒå’Œè§‚å¯Ÿç»“æœã€‚</li>
<li>RLåœ¨æŒ‡ä»¤æ¨¡å‹ä¸Šå¯èƒ½å¯¼è‡´å¹³åº¸æ¨ç†è½¨è¿¹çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05132">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.05132v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.05132v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.05132v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.05132v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.05132v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.05132v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Seeing-is-Understanding-Unlocking-Causal-Attention-into-Modality-Mutual-Attention-for-Multimodal-LLMs"><a href="#Seeing-is-Understanding-Unlocking-Causal-Attention-into-Modality-Mutual-Attention-for-Multimodal-LLMs" class="headerlink" title="Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual   Attention for Multimodal LLMs"></a>Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual   Attention for Multimodal LLMs</h2><p><strong>Authors:Wei-Yao Wang, Zhao Wang, Helen Suzuki, Yoshiyuki Kobayashi</strong></p>
<p>Recent Multimodal Large Language Models (MLLMs) have demonstrated significant progress in perceiving and reasoning over multimodal inquiries, ushering in a new research era for foundation models. However, vision-language misalignment in MLLMs has emerged as a critical challenge, where the textual responses generated by these models are not factually aligned with the given text-image inputs. Existing efforts to address vision-language misalignment have focused on developing specialized vision-language connectors or leveraging visual instruction tuning from diverse domains. In this paper, we tackle this issue from a fundamental yet unexplored perspective by revisiting the core architecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs consisting of a causal attention mechanism, which limits the ability of the earlier modalities (e.g., images) to incorporate information from the latter modalities (e.g., text). To address this problem, we propose \MapleLeaf AKI, a novel MLLM that unlocks causal attention into modality-mutual attention (MMA) to enable image tokens to attend to text tokens. This simple yet effective design allows AKI to achieve superior performance in 12 multimodal understanding benchmarks (+7.2% on average) without introducing additional parameters and increasing training time. Our MMA design is intended to be generic, allowing for application across various modalities, and scalable to accommodate diverse multimodal scenarios. The code and model are publicly available at <a target="_blank" rel="noopener" href="https://github.com/sony/aki">https://github.com/sony/aki</a> to encourage further advancements in MLLMs across various directions. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ„ŸçŸ¥å’Œç†è§£å¤šæ¨¡æ€æŸ¥è¯¢æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ºåŸºç¡€æ¨¡å‹å¼€å¯äº†æ–°çš„ç ”ç©¶æ—¶ä»£ã€‚ç„¶è€Œï¼ŒMLLMsä¸­çš„è§†è§‰è¯­è¨€é”™ä½é—®é¢˜å·²æˆä¸ºä¸€é¡¹å…³é”®æŒ‘æˆ˜ï¼Œè¯¥æŒ‘æˆ˜ä¸­ï¼Œè¿™äº›æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å›åº”ä¸ç»™å®šçš„æ–‡æœ¬å›¾åƒè¾“å…¥åœ¨äº‹å®ä¸Šå¹¶ä¸å¯¹é½ã€‚ä¸ºè§£å†³è§†è§‰è¯­è¨€é”™ä½é—®é¢˜ï¼Œç°æœ‰åŠªåŠ›ä¸»è¦é›†ä¸­åœ¨å¼€å‘ä¸“é—¨çš„è§†è§‰è¯­è¨€è¿æ¥å™¨æˆ–ä»å„ç§é¢†åŸŸä¸­åˆ©ç”¨è§†è§‰æŒ‡ä»¤è°ƒæ•´ã€‚æœ¬æ–‡ä»ä¸€ä¸ªåŸºæœ¬ä¸”æœªè¢«æ¢ç´¢çš„è§†è§’æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡é‡æ–°è®¾è®¡MLLMsçš„æ ¸å¿ƒæ¶æ„ã€‚å¤§å¤šæ•°MLLMsé€šå¸¸å»ºç«‹åœ¨ä»…è§£ç çš„LLMsä¸Šï¼Œç”±å› æœæ³¨æ„æœºåˆ¶ç»„æˆï¼Œè¿™é™åˆ¶äº†æ—©æœŸæ¨¡æ€ï¼ˆä¾‹å¦‚å›¾åƒï¼‰èå…¥åæœŸæ¨¡æ€ï¼ˆä¾‹å¦‚æ–‡æœ¬ï¼‰ä¿¡æ¯çš„èƒ½åŠ›ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MapleLeafAKIï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹MLLMï¼Œå®ƒå°†å› æœæ³¨æ„è§£é”ä¸ºæ¨¡æ€ç›¸äº’æ³¨æ„ï¼ˆMMAï¼‰ï¼Œä½¿å›¾åƒæ ‡è®°èƒ½å¤Ÿå…³æ³¨æ–‡æœ¬æ ‡è®°ã€‚è¿™ç§ç®€å•è€Œæœ‰æ•ˆçš„è®¾è®¡ä½¿AKIèƒ½å¤Ÿåœ¨12ä¸ªå¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•ä¸­å®ç°å“è¶Šæ€§èƒ½ï¼ˆå¹³å‡æé«˜7.2%ï¼‰ï¼ŒåŒæ—¶ä¸å¼•å…¥é¢å¤–å‚æ•°å¹¶å¢åŠ è®­ç»ƒæ—¶é—´ã€‚æˆ‘ä»¬çš„MMAè®¾è®¡æ˜¯é€šç”¨çš„ï¼Œå¯åº”ç”¨äºå„ç§æ¨¡æ€ï¼Œå¹¶ä¸”å¯æ‰©å±•ä»¥é€‚åº”å„ç§å¤šæ¨¡æ€åœºæ™¯ã€‚ä»£ç å’Œæ¨¡å‹å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/sony/aki%E5%85%AC%E5%BC%80%EF%BC%8C%E4%BB%A5%E9%BC%93%E5%8A%B1%E5%9C%A8MLLMs%E7%9A%84%E5%90%84%E4%B8%AA%E6%96%B9%E9%9D%A2%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%8F%96%E5%BE%97%E8%BF%9B%E5%B1%95%E3%80%82">https://github.com/sony/akiå…¬å¼€ï¼Œä»¥é¼“åŠ±åœ¨MLLMsçš„å„ä¸ªæ–¹é¢è¿›ä¸€æ­¥å–å¾—è¿›å±•ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02597v2">PDF</a> Preprint</p>
<p><strong>Summary</strong>ï¼šæœ€æ–°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ„ŸçŸ¥å’Œæ¨ç†å¤šæ¨¡æ€æŸ¥è¯¢æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå¼€å¯äº†åŸºç¡€æ¨¡å‹çš„æ–°ç ”ç©¶æ—¶ä»£ã€‚ç„¶è€Œï¼ŒMLLMsä¸­çš„è§†è§‰è¯­è¨€ä¸åŒ¹é…é—®é¢˜å·²æˆä¸ºå…³é”®æŒ‘æˆ˜ï¼Œç°æœ‰åŠªåŠ›ä¸»è¦é›†ä¸­åœ¨å¼€å‘ä¸“é—¨çš„è§†è§‰è¯­è¨€è¿æ¥å™¨æˆ–åˆ©ç”¨ä¸åŒé¢†åŸŸçš„è§†è§‰æŒ‡ä»¤è°ƒæ•´ã€‚æœ¬æ–‡ä»ä¸€ä¸ªåŸºæœ¬ä½†æœªè¢«æ¢ç´¢çš„è§†è§’å‡ºå‘ï¼Œé€šè¿‡é‡æ–°è®¾è®¡MLLMçš„æ ¸å¿ƒæ¶æ„æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚å¤§å¤šæ•°MLLMséƒ½æ˜¯å»ºç«‹åœ¨åªè§£ç çš„LLMsä¸Šï¼Œé‡‡ç”¨å› æœæ³¨æ„æœºåˆ¶ï¼Œè¿™é™åˆ¶äº†æ—©æœŸæ¨¡æ€ï¼ˆå¦‚å›¾åƒï¼‰å¸æ”¶åæœŸæ¨¡æ€ï¼ˆå¦‚æ–‡æœ¬ï¼‰ä¿¡æ¯çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºâ€œMapleLeafAKIâ€ï¼Œä¸€ç§æ–°å‹MLLMï¼Œè§£é”å› æœæ³¨æ„åŠ›åˆ°æ¨¡æ€ç›¸äº’æ³¨æ„åŠ›ï¼ˆMMAï¼‰ï¼Œä½¿å›¾åƒä»¤ç‰Œèƒ½å¤Ÿå…³æ³¨æ–‡æœ¬ä»¤ç‰Œã€‚è¿™ç§ç®€å•è€Œæœ‰æ•ˆçš„è®¾è®¡ä½¿AKIåœ¨12ä¸ªå¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ˆ+7.2ï¼…çš„å¹³å‡å€¼ï¼‰ï¼ŒåŒæ—¶æ²¡æœ‰å¼•å…¥é¢å¤–çš„å‚æ•°å¹¶å¢åŠ äº†è®­ç»ƒæ—¶é—´ã€‚æˆ‘ä»¬çš„MMAè®¾è®¡æ˜¯é€šç”¨çš„ï¼Œå¯åº”ç”¨äºå„ç§æ¨¡æ€ï¼Œå¹¶å¯æ‰©å±•ä»¥é€‚åº”å„ç§å¤šæ¨¡æ€åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>MLLMsåœ¨å¤šæ¨¡æ€æŸ¥è¯¢çš„æ„ŸçŸ¥å’Œæ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œä½†é¢ä¸´è§†è§‰è¯­è¨€ä¸åŒ¹é…çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰è§£å†³æ–¹æ¡ˆä¸»è¦å…³æ³¨å¼€å‘ä¸“é—¨çš„è§†è§‰è¯­è¨€è¿æ¥å™¨æˆ–åˆ©ç”¨ä¸åŒé¢†åŸŸçš„è§†è§‰æŒ‡ä»¤è°ƒæ•´ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡é‡æ–°è®¾è®¡MLLMçš„æ ¸å¿ƒæ¶æ„æ¥è§£å†³è§†è§‰è¯­è¨€ä¸åŒ¹é…é—®é¢˜ï¼Œå¼ºè°ƒæ¨¡æ€ç›¸äº’æ³¨æ„åŠ›ï¼ˆMMAï¼‰çš„é‡è¦æ€§ã€‚</li>
<li>å¤§å¤šæ•°MLLMså»ºç«‹åœ¨åªè§£ç çš„LLMsä¸Šï¼Œé‡‡ç”¨å› æœæ³¨æ„æœºåˆ¶ï¼Œé™åˆ¶äº†ä¸åŒæ¨¡æ€é—´çš„ä¿¡æ¯äº¤äº’ã€‚</li>
<li>æå‡ºçš„æ–°å‹MLLMâ€œMapleLeafAKIâ€é€šè¿‡è§£é”å› æœæ³¨æ„åŠ›åˆ°MMAï¼Œå®ç°äº†å›¾åƒä»¤ç‰Œå’Œæ–‡æœ¬ä»¤ç‰Œçš„äº’åŠ¨ã€‚</li>
<li>MapleLeafAKIåœ¨å¤šä¸ªå¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œæ€§èƒ½æå‡æ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.02597v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.02597v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.02597v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.02597v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.02597v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Evaluating-System-1-vs-2-Reasoning-Approaches-for-Zero-Shot-Time-Series-Forecasting-A-Benchmark-and-Insights"><a href="#Evaluating-System-1-vs-2-Reasoning-Approaches-for-Zero-Shot-Time-Series-Forecasting-A-Benchmark-and-Insights" class="headerlink" title="Evaluating System 1 vs. 2 Reasoning Approaches for Zero-Shot Time-Series   Forecasting: A Benchmark and Insights"></a>Evaluating System 1 vs. 2 Reasoning Approaches for Zero-Shot Time-Series   Forecasting: A Benchmark and Insights</h2><p><strong>Authors:Haoxin Liu, Zhiyuan Zhao, Shiduo Li, B. Aditya Prakash</strong></p>
<p>Reasoning ability is crucial for solving challenging tasks. With the advancement of foundation models, such as the emergence of large language models (LLMs), a wide range of reasoning strategies has been proposed, including test-time enhancements, such as Chain-ofThought, and post-training optimizations, as used in DeepSeek-R1. While these reasoning strategies have demonstrated effectiveness across various challenging language or vision tasks, their applicability and impact on time-series forecasting (TSF), particularly the challenging zero-shot TSF, remain largely unexplored. In particular, it is unclear whether zero-shot TSF benefits from reasoning and, if so, what types of reasoning strategies are most effective. To bridge this gap, we propose ReC4TS, the first benchmark that systematically evaluates the effectiveness of popular reasoning strategies when applied to zero-shot TSF tasks. ReC4TS conducts comprehensive evaluations across datasets spanning eight domains, covering both unimodal and multimodal with short-term and longterm forecasting tasks. More importantly, ReC4TS provides key insights: (1) Self-consistency emerges as the most effective test-time reasoning strategy; (2) Group-relative policy optimization emerges as a more suitable approach for incentivizing reasoning ability during post-training; (3) Multimodal TSF benefits more from reasoning strategies compared to unimodal TSF. Beyond these insights, ReC4TS establishes two pioneering starting blocks to support future zero-shot TSF reasoning research: (1) A novel dataset, TimeThinking, containing forecasting samples annotated with reasoning trajectories from multiple advanced LLMs, and (2) A new and simple test-time scaling-law validated on foundational TSF models enabled by self-consistency reasoning strategy. All data and code are publicly accessible at: <a target="_blank" rel="noopener" href="https://github.com/AdityaLab/OpenTimeR">https://github.com/AdityaLab/OpenTimeR</a> </p>
<blockquote>
<p>æ¨ç†èƒ½åŠ›å¯¹äºè§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡è‡³å…³é‡è¦ã€‚éšç€åŸºç¡€æ¨¡å‹çš„è¿›æ­¥ï¼Œå¦‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‡ºç°ï¼Œå·²ç»æå‡ºäº†å¤šç§æ¨ç†ç­–ç•¥ï¼ŒåŒ…æ‹¬æµ‹è¯•æ—¶çš„å¢å¼ºï¼ˆå¦‚Chain-of-Thoughtï¼‰å’Œè®­ç»ƒåçš„ä¼˜åŒ–ï¼ˆå¦‚DeepSeek-R1ä¸­æ‰€ä½¿ç”¨çš„ï¼‰ã€‚è™½ç„¶è¿™äº›æ¨ç†ç­–ç•¥åœ¨å„ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„è¯­è¨€æˆ–è§†è§‰ä»»åŠ¡ä¸­å·²æ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§ï¼Œä½†å®ƒä»¬åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰ä¸­çš„åº”ç”¨åŠå…¶å¯¹é›¶æ ·æœ¬TSFçš„å½±å“åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªè¢«æ¢ç´¢ã€‚å°¤å…¶æ˜¯å°šä¸æ¸…æ¥šé›¶æ ·æœ¬TSFæ˜¯å¦å—ç›Šäºæ¨ç†ï¼Œå¦‚æœå—ç›Šï¼Œé‚£ä¹ˆå“ªäº›ç±»å‹çš„æ¨ç†ç­–ç•¥æœ€ä¸ºæœ‰æ•ˆã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ReC4TSï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªç³»ç»Ÿåœ°è¯„ä¼°æµè¡Œæ¨ç†ç­–ç•¥åœ¨é›¶æ ·æœ¬TSFä»»åŠ¡ä¸Šæœ‰æ•ˆæ€§çš„åŸºå‡†æµ‹è¯•ã€‚ReC4TSåœ¨æ¶µç›–å•æ¨¡æ€å’Œå¤šæ¨¡æ€ä»¥åŠçŸ­æœŸå’Œé•¿æœŸé¢„æµ‹ä»»åŠ¡çš„å…«ä¸ªé¢†åŸŸçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒReC4TSæä¾›äº†å…³é”®çš„è§è§£ï¼šï¼ˆ1ï¼‰è‡ªæ´½æ€§è¢«è¯æ˜æ˜¯æœ€æœ‰æ•ˆçš„æµ‹è¯•æ—¶æ¨ç†ç­–ç•¥ï¼›ï¼ˆ2ï¼‰ç›¸å¯¹ç»„ç­–ç•¥ä¼˜åŒ–æ˜¯æ¿€åŠ±è®­ç»ƒåæ¨ç†èƒ½åŠ›çš„æ›´åˆé€‚æ–¹æ³•ï¼›ï¼ˆ3ï¼‰å¤šæ¨¡æ€TSFæ¯”å•æ¨¡æ€TSFæ›´èƒ½å—ç›Šäºæ¨ç†ç­–ç•¥ã€‚é™¤äº†è¿™äº›è§è§£ä¹‹å¤–ï¼ŒReC4TSå»ºç«‹äº†ä¸¤ä¸ªå¼€åˆ›æ€§çš„èµ·ç‚¹ï¼Œä»¥æ”¯æŒæœªæ¥çš„é›¶æ ·æœ¬TSFæ¨ç†ç ”ç©¶ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªæ–°çš„æ•°æ®é›†TimeThinkingï¼Œå…¶ä¸­åŒ…å«è¢«å¤šä¸ªå…ˆè¿›LLMçš„æ¨ç†è½¨è¿¹æ³¨é‡Šçš„é¢„æµ‹æ ·æœ¬ï¼›ï¼ˆ2ï¼‰é€šè¿‡è‡ªæ´½æ¨ç†ç­–ç•¥éªŒè¯çš„åŸºç¡€TSFæ¨¡å‹çš„æ–°ç®€å•æµ‹è¯•æ—¶é—´å°ºåº¦å®šå¾‹ã€‚æ‰€æœ‰æ•°æ®å’Œä»£ç å‡å¯å…¬å¼€è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/AdityaLab/OpenTimeR">https://github.com/AdityaLab/OpenTimeR</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01895v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ¨ç†èƒ½åŠ›åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰ä¸­çš„é‡è¦ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯é›¶æ ·æœ¬TSFã€‚æ–‡ç« ä»‹ç»äº†ReC4TSè¿™ä¸€é¦–ä¸ªé’ˆå¯¹é›¶æ ·æœ¬TSFä»»åŠ¡çš„æ¨ç†ç­–ç•¥è¯„ä¼°åŸºå‡†æµ‹è¯•ï¼Œå®ƒåœ¨å…«ä¸ªé¢†åŸŸçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå¹¶æä¾›äº†ä¸€ç³»åˆ—å…³é”®è§è§£ã€‚åŒæ—¶ï¼Œå®ƒè¿˜å»ºç«‹äº†ä¸¤ä¸ªæ”¯æŒæœªæ¥é›¶æ ·æœ¬TSFæ¨ç†ç ”ç©¶çš„èµ·ç‚¹ï¼ŒåŒ…æ‹¬ä¸€ä¸ªæ–°çš„æ•°æ®é›†å’Œæ—¶é—´æµ‹è¯•çš„è‡ªæ´½æ¨ç†ç­–ç•¥çš„éªŒè¯ã€‚ReC4TSçš„è´¡çŒ®å…¬å¼€å¯è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ¨ç†èƒ½åŠ›å¯¹äºè§£å†³æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆTSFï¼‰çš„æŒ‘æˆ˜æ€§ä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>ReC4TSæ˜¯é¦–ä¸ªé’ˆå¯¹é›¶æ ·æœ¬TSFä»»åŠ¡çš„æ¨ç†ç­–ç•¥è¯„ä¼°åŸºå‡†æµ‹è¯•ã€‚</li>
<li>ReC4TSåœ¨å…«ä¸ªé¢†åŸŸçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œæ¶µç›–å•æ¨¡æ€å’Œå¤šæ¨¡æ€ï¼ŒçŸ­æœŸå’Œé•¿æœŸé¢„æµ‹ä»»åŠ¡ã€‚</li>
<li>è‡ªæ´½æ€§è¢«è¯æ˜æ˜¯æœ€æœ‰æ•ˆçš„æµ‹è¯•æ—¶é—´æ¨ç†ç­–ç•¥ã€‚</li>
<li>Group-relativeæ”¿ç­–ä¼˜åŒ–æ˜¯æ¿€åŠ±åè®­ç»ƒé˜¶æ®µæ¨ç†èƒ½åŠ›çš„æ›´åˆé€‚æ–¹æ³•ã€‚</li>
<li>å¤šæ¨¡æ€TSFæ¯”å•æ¨¡æ€TSFæ›´å¤šåœ°å—ç›Šäºæ¨ç†ç­–ç•¥ã€‚</li>
<li>ReC4TSå»ºç«‹äº†ä¸€ä¸ªæ–°æ•°æ®é›†TimeThinkingï¼ŒåŒ…å«å¤šä¸ªé«˜çº§LLMçš„é¢„æµ‹æ ·æœ¬æ ‡æ³¨çš„æ¨ç†è½¨è¿¹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01895">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.01895v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.01895v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.01895v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.01895v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.01895v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.01895v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.01895v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.01895v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="CE-U-Cross-Entropy-Unlearning"><a href="#CE-U-Cross-Entropy-Unlearning" class="headerlink" title="CE-U: Cross Entropy Unlearning"></a>CE-U: Cross Entropy Unlearning</h2><p><strong>Authors:Bo Yang</strong></p>
<p>Large language models inadvertently memorize sensitive data from their massive pretraining corpora \cite{jang2022knowledge}. In this work, we propose CE-U (Cross Entropy Unlearning), a novel loss function for unlearning. CE-U addresses fundamental limitations of gradient ascent approaches which suffer from instability due to vanishing gradients when model confidence is high and gradient exploding when confidence is low. We also unify standard cross entropy supervision and cross entropy unlearning into a single framework. Notably, on the TOFU benchmark for unlearning \cite{maini2024tofu}, CE-U achieves state-of-the-art results on LLaMA2-7B forgetting, even without the use of any extra reference model or additional positive samples. Our theoretical analysis further reveals that the gradient instability issues also exist in popular reinforcement learning algorithms like DPO \cite{rafailov2023direct} and GRPO \cite{Shao2024DeepSeekMath}, as they include a gradient ascent component. This suggests that applying CE-U principles to reinforcement learning could be a promising direction for improving stability and convergence. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹æ— æ„ä¸­ä»å…¶åºå¤§çš„é¢„è®­ç»ƒè¯­æ–™åº“ä¸­è®°å¿†æ•æ„Ÿæ•°æ®\cite{jang2022knowledge}ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CE-Uï¼ˆäº¤å‰ç†µé—å¿˜ï¼‰è¿™ä¸€æ–°å‹é—å¿˜æŸå¤±å‡½æ•°ã€‚CE-Uè§£å†³äº†æ¢¯åº¦ä¸Šå‡æ–¹æ³•çš„åŸºæœ¬å±€é™æ€§ï¼Œè¿™äº›æ–¹æ³•åœ¨æ¨¡å‹ç½®ä¿¡åº¦è¾ƒé«˜æ—¶å› æ¢¯åº¦æ¶ˆå¤±è€Œé¢ä¸´ä¸ç¨³å®šé—®é¢˜ï¼Œåœ¨ç½®ä¿¡åº¦è¾ƒä½æ—¶åˆ™é¢ä¸´æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ã€‚æˆ‘ä»¬è¿˜ç»Ÿä¸€äº†æ ‡å‡†äº¤å‰ç†µç›‘ç£å’Œäº¤å‰ç†µé—å¿˜åˆ°ä¸€ä¸ªå•ä¸€æ¡†æ¶ä¸­ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨é—å¿˜çš„TOFUåŸºå‡†æµ‹è¯•\cite{maini2024tofu}ä¸Šï¼Œå³ä½¿åœ¨æœªä½¿ç”¨ä»»ä½•é¢å¤–å‚è€ƒæ¨¡å‹æˆ–é¢å¤–æ­£æ ·æœ¬çš„æƒ…å†µä¸‹ï¼ŒCE-Uåœ¨LLaMA2-7Bçš„é—å¿˜æ–¹é¢ä¹Ÿå–å¾—äº†æœ€æ–°æˆæœã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æè¿›ä¸€æ­¥æ­ç¤ºï¼Œæ¢¯åº¦ä¸ç¨³å®šé—®é¢˜ä¹Ÿå­˜åœ¨äºæµè¡Œçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸­ï¼Œå¦‚DPO\cite{rafailov2023direct}å’ŒGRPO\cite{Shao2024DeepSeekMath}ï¼Œå› ä¸ºå®ƒä»¬åŒ…å«æ¢¯åº¦ä¸Šå‡ç»„ä»¶ã€‚è¿™è¡¨æ˜å°†CE-UåŸåˆ™åº”ç”¨äºå¼ºåŒ–å­¦ä¹ å¯èƒ½æ˜¯æé«˜ç¨³å®šæ€§å’Œæ”¶æ•›æ€§çš„ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01224v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢„è®­ç»ƒè¯­æ–™åº“ä¸­æ— æ„é—´è®°å¿†æ•æ„Ÿæ•°æ®ã€‚æœ¬ç ”ç©¶æå‡ºCE-Uï¼ˆäº¤å‰ç†µé—å¿˜ï¼‰ä½œä¸ºæ–°çš„é—å¿˜æŸå¤±å‡½æ•°ã€‚CE-Uè§£å†³äº†æ¢¯åº¦ä¸Šå‡æ–¹æ³•çš„åŸºæœ¬å±€é™ï¼Œè§£å†³äº†æ¨¡å‹ä¿¡å¿ƒé«˜æ—¶æ¢¯åº¦æ¶ˆå¤±å’Œä¿¡å¿ƒä½æ—¶æ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ ‡å‡†äº¤å‰ç†µç›‘ç£å’Œäº¤å‰ç†µé—å¿˜ç»Ÿä¸€åˆ°ä¸€ä¸ªæ¡†æ¶ä¸­ã€‚åœ¨TOFUé—å¿˜åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCE-Uåœ¨LLaMA2-7Bçš„é—å¿˜æ–¹é¢å–å¾—äº†æœ€æ–°æˆæœï¼Œå³ä½¿æ²¡æœ‰ä½¿ç”¨ä»»ä½•é¢å¤–çš„å‚è€ƒæ¨¡å‹æˆ–é¢å¤–çš„æ­£æ ·æœ¬ã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æè¿˜è¡¨æ˜ï¼Œæ¢¯åº¦ä¸ç¨³å®šé—®é¢˜ä¹Ÿå­˜åœ¨äºæµè¡Œçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸­ï¼Œå¦‚DPOå’ŒGRPOï¼Œå› ä¸ºå®ƒä»¬åŒ…å«æ¢¯åº¦ä¸Šå‡æˆåˆ†ã€‚è¿™è¡¨æ˜å°†CE-UåŸåˆ™åº”ç”¨äºå¼ºåŒ–å­¦ä¹ å¯èƒ½æ˜¯æé«˜ç¨³å®šæ€§å’Œæ”¶æ•›æ€§çš„æœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ä¼šæ— æ„ä¸­è®°å¿†é¢„è®­ç»ƒè¯­æ–™åº“ä¸­çš„æ•æ„Ÿæ•°æ®ã€‚</li>
<li>æå‡ºäº†CE-Uï¼ˆäº¤å‰ç†µé—å¿˜ï¼‰ä½œä¸ºæ–°çš„é—å¿˜æŸå¤±å‡½æ•°ï¼Œè§£å†³äº†æ¢¯åº¦ä¸Šå‡æ–¹æ³•çš„ç¨³å®šæ€§é—®é¢˜ã€‚</li>
<li>CE-Uå°†æ ‡å‡†äº¤å‰ç†µç›‘ç£å’Œäº¤å‰ç†µé—å¿˜ç»“åˆåˆ°ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ä¸­ã€‚</li>
<li>åœ¨TOFUåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCE-Uåœ¨LLaMAæ¨¡å‹é—å¿˜æ–¹é¢å–å¾—æœ€æ–°æˆæœã€‚</li>
<li>CE-Uå®ç°æ— éœ€é¢å¤–çš„å‚è€ƒæ¨¡å‹æˆ–æ­£æ ·æœ¬ã€‚</li>
<li>æ¢¯åº¦ä¸ç¨³å®šé—®é¢˜ä¸ä»…å­˜åœ¨äºä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ ç®—æ³•ä¸­ï¼Œä¹Ÿå­˜åœ¨äºå¼ºåŒ–å­¦ä¹ ç®—æ³•å¦‚DPOå’ŒGRPOä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01224">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.01224v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.01224v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-03-16\./crop_R1_Reasoning/2503.01224v3/page_4_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-16/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-16/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-16/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0b6344b2168562c1a3761c012e5ffec6.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-16  Minimal Time Series Transformer
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-15/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-75e4f9539d0c6f9f3bd659d9d449032d.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-15  MotionScript Natural Language Descriptions for Expressive 3D Human   Motions
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18293.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
