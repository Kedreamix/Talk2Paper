<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-03-16  Minimal Time Series Transformer">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0b6344b2168562c1a3761c012e5ffec6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    76 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-16-更新"><a href="#2025-03-16-更新" class="headerlink" title="2025-03-16 更新"></a>2025-03-16 更新</h1><h2 id="Minimal-Time-Series-Transformer"><a href="#Minimal-Time-Series-Transformer" class="headerlink" title="Minimal Time Series Transformer"></a>Minimal Time Series Transformer</h2><p><strong>Authors:Joni-Kristian Kämäräinen</strong></p>
<p>Transformer is the state-of-the-art model for many natural language processing, computer vision, and audio analysis problems. Transformer effectively combines information from the past input and output samples in auto-regressive manner so that each sample becomes aware of all inputs and outputs. In sequence-to-sequence (Seq2Seq) modeling, the transformer processed samples become effective in predicting the next output. Time series forecasting is a Seq2Seq problem. The original architecture is defined for discrete input and output sequence tokens, but to adopt it for time series, the model must be adapted for continuous data. This work introduces minimal adaptations to make the original transformer architecture suitable for continuous value time series data. </p>
<blockquote>
<p>Transformer模型是很多自然语言处理、计算机视觉和音频分析问题的最先进模型。Transformer有效地以自回归方式结合了来自过去输入和输出样本的信息，从而使每个样本都能感知到所有的输入和输出。在序列到序列（Seq2Seq）建模中，经过Transformer处理的样本在预测下一个输出时非常有效。时间序列预测是一个Seq2Seq问题。原始架构是为离散输入输出序列标记定义的，但要将其用于时间序列，必须对模型进行适应以处理连续数据。这项工作对原始Transformer架构进行了最小的调整，使其适用于连续值时间序列数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09791v1">PDF</a> 8 pages, 8 figures</p>
<p><strong>Summary</strong>：Transformer模型结合了过往输入和输出样本的信息，以自回归的方式使每个样本都能感知到所有的输入和输出，适用于许多自然语言处理、计算机视觉和音频分析等问题。在时间序列预测这类序列到序列（Seq2Seq）问题上，经过适应连续数据的改进后的Transformer模型表现出了高效的预测能力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>Transformer模型结合了过往输入和输出样本的信息。</li>
<li>Transformer以自回归的方式处理信息，使每个样本都能感知到所有的输入和输出。</li>
<li>Transformer模型适用于自然语言处理、计算机视觉和音频分析等多种问题。</li>
<li>时间序列预测是序列到序列（Seq2Seq）问题。</li>
<li>原始的Transformer架构是为离散输入和输出序列令牌定义的。</li>
<li>为了将Transformer模型应用于时间序列数据，必须对模型进行适应以处理连续数据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09791">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5629f86f576a2032a08d3b1e6dd00b83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a68697e498a5bdfbe01afeed3866d40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-290e9bdfcc1d5375f072571b45ec56ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dc738325a9aad4ee6a41076903d2a25c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2ba83069a612f41cb1be1c660dd837b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4e7dd9363211de554ac39768bc13ae54.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9f7a88a9e964170ce79b566d1994889.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ce0067f8f4dc03a84d21531049aed63.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="BIMBA-Selective-Scan-Compression-for-Long-Range-Video-Question-Answering"><a href="#BIMBA-Selective-Scan-Compression-for-Long-Range-Video-Question-Answering" class="headerlink" title="BIMBA: Selective-Scan Compression for Long-Range Video Question   Answering"></a>BIMBA: Selective-Scan Compression for Long-Range Video Question   Answering</h2><p><strong>Authors:Md Mohaiminul Islam, Tushar Nagarajan, Huiyu Wang, Gedas Bertasius, Lorenzo Torresani</strong></p>
<p>Video Question Answering (VQA) in long videos poses the key challenge of extracting relevant information and modeling long-range dependencies from many redundant frames. The self-attention mechanism provides a general solution for sequence modeling, but it has a prohibitive cost when applied to a massive number of spatiotemporal tokens in long videos. Most prior methods rely on compression strategies to lower the computational cost, such as reducing the input length via sparse frame sampling or compressing the output sequence passed to the large language model (LLM) via space-time pooling. However, these naive approaches over-represent redundant information and often miss salient events or fast-occurring space-time patterns. In this work, we introduce BIMBA, an efficient state-space model to handle long-form videos. Our model leverages the selective scan algorithm to learn to effectively select critical information from high-dimensional video and transform it into a reduced token sequence for efficient LLM processing. Extensive experiments demonstrate that BIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks, including PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and Video-MME. Code, and models are publicly available at <a target="_blank" rel="noopener" href="https://sites.google.com/view/bimba-mllm">https://sites.google.com/view/bimba-mllm</a>. </p>
<blockquote>
<p>视频问答（VQA）在长视频中面临的关键挑战是从大量冗余帧中提取相关信息并建模长距离依赖关系。自注意力机制为序列建模提供了一种通用解决方案，但当应用于长视频中的大量时空令牌时，其成本高昂。大多数先前的方法依赖于压缩策略来降低计算成本，例如通过稀疏帧采样减少输入长度，或通过时空池化压缩传递给大型语言模型（LLM）的输出序列。然而，这些简单的方法过于强调冗余信息，并且经常忽略重要事件或快速发生的时空模式。在这项工作中，我们引入了BIMBA，这是一种处理长格式视频的高效状态空间模型。我们的模型利用选择性扫描算法来学习有效地从高维视频中选择关键信息，并将其转换为减少的令牌序列，以便大型语言模型进行高效处理。大量实验表明，BIMBA在多个长格式VQA基准测试中达到了最新水平，包括PerceptionTest、NExT-QA、EgoSchema、VNBench、LongVideoBench和Video-MME。代码和模型可在<a target="_blank" rel="noopener" href="https://sites.google.com/view/bimba-mllm%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://sites.google.com/view/bimba-mllm公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09590v2">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了处理长视频VQA（视频问题回答）的挑战，包括从大量冗余帧中提取相关信息和建模长距离依赖关系。现有方法多采用压缩策略降低计算成本，但存在过度代表冗余信息、忽略重要事件或快速时空模式的问题。本文提出BIMBA模型，利用选择性扫描算法从高维视频中选择关键信息，转化为简化的令牌序列，供LLM高效处理。实验证明BIMBA在多个长视频VQA基准测试中达到最新水平，包括PerceptionTest、NExT-QA等。模型与代码公开于：<a target="_blank" rel="noopener" href="https://sites.google.com/view/bimba-mllm%E3%80%82">https://sites.google.com/view/bimba-mllm。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>处理长视频的VQA面临提取相关信息和建模长距离依赖关系的挑战。</li>
<li>现有方法使用压缩策略以降低计算成本，但可能忽略重要信息。</li>
<li>BIMBA模型利用选择性扫描算法从高维视频中选择关键信息。</li>
<li>BIMBA将关键信息转化为简化的令牌序列供LLM高效处理。</li>
<li>BIMBA在多个长视频VQA基准测试中达到最新水平。</li>
<li>模型与代码已公开，便于他人使用和研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09590">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7f84c6cc66c2ee34bcf35e9c861c87c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdfc74efca2a7b717a229f97b7813cca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-86d329099c75da7556c054ea1ec044c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6053314104e86321fb09c324e3bf8a9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MastermindEval-A-Simple-But-Scalable-Reasoning-Benchmark"><a href="#MastermindEval-A-Simple-But-Scalable-Reasoning-Benchmark" class="headerlink" title="MastermindEval: A Simple But Scalable Reasoning Benchmark"></a>MastermindEval: A Simple But Scalable Reasoning Benchmark</h2><p><strong>Authors:Jonas Golde, Patrick Haller, Fabio Barth, Alan Akbik</strong></p>
<p>Recent advancements in large language models (LLMs) have led to remarkable performance across a wide range of language understanding and mathematical tasks. As a result, increasing attention has been given to assessing the true reasoning capabilities of LLMs, driving research into commonsense, numerical, logical, and qualitative reasoning. However, with the rapid progress of reasoning-focused models such as OpenAI’s o1 and DeepSeek’s R1, there has been a growing demand for reasoning benchmarks that can keep pace with ongoing model developments. In this paper, we introduce MastermindEval, a simple, scalable, and interpretable deductive reasoning benchmark inspired by the board game Mastermind. Our benchmark supports two evaluation paradigms: (1) agentic evaluation, in which the model autonomously plays the game, and (2) deductive reasoning evaluation, in which the model is given a pre-played game state with only one possible valid code to infer. In our experimental results we (1) find that even easy Mastermind instances are difficult for current models and (2) demonstrate that the benchmark is scalable to possibly more advanced models in the future Furthermore, we investigate possible reasons why models cannot deduce the final solution and find that current models are limited in deducing the concealed code as the number of statement to combine information from is increasing. </p>
<blockquote>
<p>大型语言模型（LLM）的最新进展在广泛的自然语言理解和数学任务中取得了显著的成果。因此，人们越来越关注评估LLM的真正推理能力，推动了常识推理、数值推理、逻辑和定性推理的研究。然而，随着以推理为重点的模型如OpenAI的o1和DeepSeek的R1的快速发展，对能与当前模型发展同步的推理基准测试的需求也在增长。在本文中，我们介绍了MastermindEval，这是一个受棋盘游戏《Mastermind》启发的简单、可扩展和可解释的演绎推理基准测试。我们的基准测试支持两种评估模式：（1）自主评估模式，模型自主玩游戏；（2）演绎推理评估模式，给定一个预先进行的游戏状态，模型需要通过推理找出唯一的正确答案。我们的实验结果表明：（1）即使是简单的Mastermind实例对于当前模型来说也是困难的；（2）该基准测试可以扩展到未来可能更先进的模型。此外，我们还调查了模型无法推断出最终解决方案的可能原因，并发现随着需要组合信息来推断答案的陈述数量增加，当前模型在推断隐藏代码方面的能力受到限制。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05891v4">PDF</a> 9 pages, 2 figures, 4 tables. In: ICLR 2025 Workshop on Reasoning and   Planning for Large Language Models</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的最新进展在各种语言理解和数学任务上取得了显著的成绩，引发了人们对评估其真正推理能力的关注，推动了常识、数值、逻辑和定性推理的研究。为此，本文提出了MastermindEval基准测试，这是一个简单、可扩展且可解释的推理基准测试，灵感来源于猜谜游戏Mastermind。该基准测试支持两种评估模式：自主游戏评估和推理评估。实验结果表明，即使是简单的猜谜实例对于当前模型来说也是困难的，并且该基准测试可以扩展到更先进的模型。此外，本文还探讨了模型无法推断最终解决方案的可能原因，发现当前模型在处理信息组合时存在局限性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM的最新进展在各种语言理解和数学任务上取得了显著成绩。</li>
<li>人们需要评估LLM的真正推理能力，这推动了常识、数值、逻辑和定性推理的研究。</li>
<li>MastermindEval基准测试是一个简单、可扩展且可解释的推理基准测试，灵感来源于猜谜游戏Mastermind。</li>
<li>MastermindEval支持两种评估模式：自主游戏评估和推理评估。</li>
<li>实验发现，即使是简单的猜谜实例对于当前模型来说也是困难的。</li>
<li>MastermindEval基准测试可以扩展到更先进的模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05891">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8151909c9828f5f282094b4734b8794b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d864a105f003beb4d82ca2d8ca48bcd7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9417ec18fef4236b19fbdf03aa03f636.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-981cc37b1f842857a47af413369acb78.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Seeing-is-Understanding-Unlocking-Causal-Attention-into-Modality-Mutual-Attention-for-Multimodal-LLMs"><a href="#Seeing-is-Understanding-Unlocking-Causal-Attention-into-Modality-Mutual-Attention-for-Multimodal-LLMs" class="headerlink" title="Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual   Attention for Multimodal LLMs"></a>Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual   Attention for Multimodal LLMs</h2><p><strong>Authors:Wei-Yao Wang, Zhao Wang, Helen Suzuki, Yoshiyuki Kobayashi</strong></p>
<p>Recent Multimodal Large Language Models (MLLMs) have demonstrated significant progress in perceiving and reasoning over multimodal inquiries, ushering in a new research era for foundation models. However, vision-language misalignment in MLLMs has emerged as a critical challenge, where the textual responses generated by these models are not factually aligned with the given text-image inputs. Existing efforts to address vision-language misalignment have focused on developing specialized vision-language connectors or leveraging visual instruction tuning from diverse domains. In this paper, we tackle this issue from a fundamental yet unexplored perspective by revisiting the core architecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs consisting of a causal attention mechanism, which limits the ability of the earlier modalities (e.g., images) to incorporate information from the latter modalities (e.g., text). To address this problem, we propose \MapleLeaf AKI, a novel MLLM that unlocks causal attention into modality-mutual attention (MMA) to enable image tokens to attend to text tokens. This simple yet effective design allows AKI to achieve superior performance in 12 multimodal understanding benchmarks (+7.2% on average) without introducing additional parameters and increasing training time. Our MMA design is intended to be generic, allowing for application across various modalities, and scalable to accommodate diverse multimodal scenarios. The code and model are publicly available at <a target="_blank" rel="noopener" href="https://github.com/sony/aki">https://github.com/sony/aki</a> to encourage further advancements in MLLMs across various directions. </p>
<blockquote>
<p>最近的多模态大型语言模型（MLLMs）在感知和处理多模态查询方面取得了显著进步，为基础模型领域开启了新的研究时代。然而，MLLMs中的视觉语言不匹配问题已成为一项关键挑战，其中这些模型生成的文本响应与给定的文本图像输入并不符合事实。为解决视觉语言不匹配问题，现有工作主要集中在开发专门的视觉语言连接器或从各种领域利用视觉指令调整。本文从一个基本但尚未被探索的视角来解决这个问题，通过重新审视MLLMs的核心架构。大多数MLLMs通常建立在仅解码器的大型语言模型上，包括因果注意机制，这限制了早期模态（例如图像）融入后期模态（例如文本）信息的能力。针对这一问题，我们提出了MapleLeafAKI，这是一种新型MLLM，它将因果注意力解锁为模态相互注意力（MMA），使图像标记能够关注文本标记。这种简单而有效的设计使AKI能够在12个多模态理解基准测试中实现卓越性能（平均提高7.2%），同时不引入额外参数并增加训练时间。我们的MMA设计旨在具有通用性，可应用于各种模态，并且可扩展以适应各种多模态场景。代码和模型已在<a target="_blank" rel="noopener" href="https://github.com/sony/aki%E5%85%AC%E5%BC%80%EF%BC%8C%E4%BB%A5%E9%BC%93%E5%8A%B1%E5%9C%A8MLLMs%E7%9A%84%E5%90%84%E4%B8%AA%E9%A2%86%E5%9F%9F%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%8F%96%E5%BE%97%E8%BF%9B%E5%B1%95%E3%80%82">https://github.com/sony/aki公开，以鼓励在MLLMs的各个领域进一步取得进展。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02597v2">PDF</a> Preprint</p>
<p><strong>摘要</strong><br>多模态大型语言模型（MLLMs）在感知和推理多模态查询方面取得了显著进展。然而，MLLMs中的视觉语言错位成为一项关键挑战，模型的文本回复与给定的文本图像输入无法事实对齐。现有解决视觉语言错位问题的尝试主要集中开发专用的视觉语言连接器或使用来自不同域的视觉指令微调方法上。本文站在全新视角上重新审视了MLLM的核心架构来解决这一问题。大多数MLLM都是基于仅解码器的大型语言模型构建的，由因果注意力机制主导，这限制了早期模态（如图像）融入后期模态（如文本）信息的能力。为解决此问题，我们推出了新型MLLM“MapleLeafAKI”，它解锁了因果注意力以促成模态间相互注意力（MMA），让图像标记能够关注文本标记。这一简洁而高效的设计使得AKI能够在无需增加额外参数和培训时间的情况下，在十二个多模态理解基准测试中平均提升了7.2%的性能。我们的MMA设计旨在通用性，可以应用于各种模态，并可以适应多样化的多模态场景。我们的代码和模型已在<a target="_blank" rel="noopener" href="https://github.com/sony/aki%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%EF%BC%8C%E4%BB%A5%E9%BC%93%E5%8A%B1%E5%9C%A8%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8D%E5%90%8C%E6%96%B9%E5%90%91%E4%B8%8A%E5%8F%96%E5%BE%97%E8%BF%9B%E4%B8%80%E6%AD%A5%E8%BF%9B%E5%B1%95%E3%80%82">https://github.com/sony/aki公开提供，以鼓励在多模态语言模型的不同方向上取得进一步进展。</a></p>
<p><strong>要点速览</strong></p>
<ol>
<li>MLLMs在多模态查询感知和推理上表现出显著进步，但视觉语言错位成为一大挑战。</li>
<li>现有解决策略主要围绕开发特定视觉语言连接器或利用视觉指令微调方法展开。</li>
<li>本文通过重新审视MLLM的核心架构来解决视觉语言错位问题。</li>
<li>MLLMs主要由解码器的大型语言模型构建，受因果注意力机制限制，早期模态难以融入后期模态信息。</li>
<li>提出新型MLLM“MapleLeafAKI”，解锁因果注意力以促成模态间相互注意力（MMA）。</li>
<li>AKI设计允许图像标记关注文本标记，从而在多模态理解基准测试中实现了显著性能提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02597">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-a396e6995cde83d8d492daec7337312d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e073b1ad99a96acc38117fbd1a333035.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35223e55116e75992b76fe7ff97b1574.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c646ce9fb294b1e9730b899e36de328.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0c84f01c51ad0762dc24abedfc986e6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DataMan-Data-Manager-for-Pre-training-Large-Language-Models"><a href="#DataMan-Data-Manager-for-Pre-training-Large-Language-Models" class="headerlink" title="DataMan: Data Manager for Pre-training Large Language Models"></a>DataMan: Data Manager for Pre-training Large Language Models</h2><p><strong>Authors:Ru Peng, Kexin Yang, Yawen Zeng, Junyang Lin, Dayiheng Liu, Junbo Zhao</strong></p>
<p>The performance emergence of large language models (LLMs) driven by data scaling laws makes the selection of pre-training data increasingly important. However, existing methods rely on limited heuristics and human intuition, lacking comprehensive and clear guidelines. To address this, we are inspired by &#96;&#96;reverse thinking’’ – prompting LLMs to self-identify which criteria benefit its performance. As its pre-training capabilities are related to perplexity (PPL), we derive 14 quality criteria from the causes of text perplexity anomalies and introduce 15 common application domains to support domain mixing. In this paper, we train a Data Manager (DataMan) to learn quality ratings and domain recognition from pointwise rating, and use it to annotate a 447B token pre-training corpus with 14 quality ratings and domain type. Our experiments validate our approach, using DataMan to select 30B tokens to train a 1.3B-parameter language model, demonstrating significant improvements in in-context learning (ICL), perplexity, and instruction-following ability over the state-of-the-art baseline. The best-performing model, based on the Overall Score l&#x3D;5 surpasses a model trained with 50% more data using uniform sampling. We continue pre-training with high-rated, domain-specific data annotated by DataMan to enhance domain-specific ICL performance and thus verify DataMan’s domain mixing ability. Our findings emphasize the importance of quality ranking, the complementary nature of quality criteria, and their low correlation with perplexity, analyzing misalignment between PPL and ICL performance. We also thoroughly analyzed our pre-training dataset, examining its composition, the distribution of quality ratings, and the original document sources. </p>
<blockquote>
<p>大型语言模型（LLM）在数据规模法则的推动下性能日益显现，这使得预训练数据的选择变得越来越重要。然而，现有方法依赖于有限的启发式方法和人类直觉，缺乏全面、明确的指导。为了解决这一问题，我们从“逆向思维”中汲取灵感，引导LLM自我识别哪些标准对其性能有益。由于其预训练能力与困惑度（PPL）相关，我们从文本困惑度异常的原因中得出14个质量标准，并引入15个常见应用领域以支持领域混合。在本文中，我们训练了一个数据管理器（DataMan）来学习定点评分中的质量评分和领域识别，并使用它对一个由语料库自动处理生成的包含447亿个标记的预训练语料库进行标注，其中包括了质量评分和领域类型信息共涵盖14个指标。我们的实验验证了使用DataMan选取的语料对语言模型进行训练的有效性。我们使用该工具选取了价值30亿个标记的数据集来训练一个规模为1.3亿参数的LLM模型，并在上下文学习（ICL）、困惑度和指令执行能力方面实现了显著的改进，超过了最先进的基线模型。基于总体得分表现最好的模型（总体得分l&#x3D;5）超过了使用均匀采样方法训练的拥有更多数据（多出50%）的模型。我们继续利用DataMan标注的高质量、特定领域的语料进行预训练，以提高特定领域的上下文学习能力并验证DataMan的领域混合能力。我们的研究强调了质量排名的重要性以及质量标准的互补性及其与困惑度的低相关性，同时分析了困惑度与上下文学习能力之间的不匹配现象。我们还对我们的预训练数据集进行了全面的分析，考察了数据集的组成结构、质量评分的分布情况以及原始文档来源。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19363v2">PDF</a> ICLR2025 paper</p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLM）的性能涌现，依赖于数据规模定律的驱动，使得预训练数据的选择变得至关重要。然而，现有方法依赖于有限的启发式方法和人类直觉，缺乏全面清晰的指导方针。本文受“逆向思维”启发，促使LLM自我识别哪些标准对其性能有益。与预训练能力相关的困惑度（PPL），我们从文本困惑度异常的原因中得出14个质量标准和引入的15个常见应用领域来支持领域混合。本文通过训练一个数据管理器（DataMan）来学习质量评估和领域识别，并使用它对一个规模为447B令牌的预训练语料库进行注释。实验验证表明，通过DataMan选定的高质量训练语料可以显著改善最先进的基线模型的上下文学习能力、困惑度和指令执行能力。基于总体评分l&#x3D;5的最佳性能模型超过了使用均匀采样训练的拥有更多数据的模型。通过DataMan标注的高质量领域特定数据继续预训练，以增强领域特定的上下文学习能力并验证其领域混合能力。本文强调了质量排名的重要性、质量标准的互补性以及它们与困惑度的低相关性，分析了困惑度与上下文学习能力之间的不一致性。同时，本文对预训练数据集进行了全面的分析，包括其构成、质量评级的分布以及原始文档来源。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>LLM性能的提升与数据规模相关，使得选择高质量的预训练数据至关重要。</li>
<li>现有方法在选择预训练数据时缺乏清晰的指导方针，主要依赖启发式方法和人类直觉。</li>
<li>本文受到“逆向思维”启发，让LLM自我识别对性能有益的标准。</li>
<li>引入基于困惑度的质量标准来评估LLM的预训练效果。</li>
<li>开发了一个数据管理器（DataMan）来注释预训练语料库的质量和领域分类。使用这一工具可以提升LLM模型的上下文学习能力、困惑度和指令执行能力。</li>
<li>最佳性能的模型超越了使用更多数据但采用均匀采样训练的模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19363">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4cbba117c694ebc399a87bed055ffc5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a8660bf9f614b01461c87853a5e3c97.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Property-Enhanced-Instruction-Tuning-for-Multi-task-Molecule-Generation-with-Large-Language-Models"><a href="#Property-Enhanced-Instruction-Tuning-for-Multi-task-Molecule-Generation-with-Large-Language-Models" class="headerlink" title="Property Enhanced Instruction Tuning for Multi-task Molecule Generation   with Large Language Models"></a>Property Enhanced Instruction Tuning for Multi-task Molecule Generation   with Large Language Models</h2><p><strong>Authors:Xuan Lin, Long Chen, Yile Wang, Xiangxiang Zeng, Philip S. Yu</strong></p>
<p>Large language models (LLMs) are widely applied in various natural language processing tasks such as question answering and machine translation. However, due to the lack of labeled data and the difficulty of manual annotation for biochemical properties, the performance for molecule generation tasks is still limited, especially for tasks involving multi-properties constraints. In this work, we present a two-step framework PEIT (Property Enhanced Instruction Tuning) to improve LLMs for molecular-related tasks. In the first step, we use textual descriptions, SMILES, and biochemical properties as multimodal inputs to pre-train a model called PEIT-GEN, by aligning multi-modal representations to synthesize instruction data. In the second step, we fine-tune existing open-source LLMs with the synthesized data, the resulting PEIT-LLM can handle molecule captioning, text-based molecule generation, molecular property prediction, and our newly proposed multi-constraint molecule generation tasks. Experimental results show that our pre-trained PEIT-GEN outperforms MolT5 and BioT5 in molecule captioning, demonstrating modalities align well between textual descriptions, structures, and biochemical properties. Furthermore, PEIT-LLM shows promising improvements in multi-task molecule generation, proving the scalability of the PEIT framework for various molecular tasks. We release the code, constructed instruction data, and model checkpoints in <a target="_blank" rel="noopener" href="https://github.com/chenlong164/PEIT">https://github.com/chenlong164/PEIT</a>. </p>
<blockquote>
<p>大型语言模型（LLM）广泛应用于各种自然语言处理任务，如问答和机器翻译。然而，由于缺乏标记数据和生化属性手动标注的困难，分子生成任务的效果仍然受限，尤其是对于涉及多属性约束的任务。在这项工作中，我们提出了一个两步框架PEIT（属性增强指令调整）来提高LLM在分子相关任务上的性能。第一步，我们使用文本描述、SMILES和生物化学属性作为多模式输入来预训练一个名为PEIT-GEN的模型，通过对齐多模式表示来合成指令数据。在第二步中，我们使用合成数据对现有的开源LLM进行微调，得到的PEIT-LLM可以处理分子描述、基于文本的分子生成、分子属性预测以及我们新提出的多约束分子生成任务。实验结果表明，我们的预训练PEIT-GEN在分子描述方面优于MolT5和BioT5，证明了文本描述、结构和生物化学属性之间的模态对齐良好。此外，PEIT-LLM在多任务分子生成方面显示出有希望的改进，证明了PEIT框架对各种分子任务的可扩展性。我们在<a target="_blank" rel="noopener" href="https://github.com/chenlong164/PEIT">https://github.com/chenlong164/PEIT</a>上发布了代码、构建好的指令数据和模型检查点。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18084v3">PDF</a> 9</p>
<p><strong>Summary</strong><br>大语言模型在自然语言处理任务中广泛应用，但在分子生成任务上的表现受限。本研究提出了一种名为PEIT的两步框架，以提高LLM在分子相关任务上的性能。首先使用文本描述、SMILES和生物化学属性等多模式输入进行预训练；其次用合成数据微调现有开源LLM。PEIT框架能处理分子描述、文本基础分子生成、分子属性预测以及新提出的多约束分子生成任务。实验结果显示，预训练的PEIT-GEN在分子描述上优于MolT5和BioT5，证明文本描述、结构和生物化学属性之间的对齐良好。同时，PEIT-LLM在多任务分子生成上显示出有希望的改进，证明PEIT框架对各种分子任务的可扩展性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在分子生成任务上的表现受限于缺乏标注数据和手动注释的困难。</li>
<li>PEIT是一个两阶段框架，旨在提高LLMs在分子相关任务上的性能。</li>
<li>在预训练阶段，使用文本描述、SMILES和生物化学属性等多模式输入合成指令数据。</li>
<li>PEIT-GEN在分子描述上优于MolT5和BioT5，显示不同模态之间的良好对齐。</li>
<li>PEIT-LLM能处理多种分子相关任务，包括分子描述、文本基础分子生成、分子属性预测以及多约束分子生成。</li>
<li>实验结果证明了PEIT框架的有效性和可扩展性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18084">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e82b2a2c73dd0b57a0aa87a22a8e2d90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-431301f75dfab204d4322d959489b384.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c6e299ad16040444d5c2bb0c92c3a51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb2d9703cb52ff46f3f1b18e5846c929.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70ee1e58c3186455090ca950fb68190f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="QUART-Online-Latency-Free-Large-Multimodal-Language-Model-for-Quadruped-Robot-Learning"><a href="#QUART-Online-Latency-Free-Large-Multimodal-Language-Model-for-Quadruped-Robot-Learning" class="headerlink" title="QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped   Robot Learning"></a>QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped   Robot Learning</h2><p><strong>Authors:Xinyang Tong, Pengxiang Ding, Yiguo Fan, Donglin Wang, Wenjie Zhang, Can Cui, Mingyang Sun, Han Zhao, Hongyin Zhang, Yonghao Dang, Siteng Huang, Shangke Lyu</strong></p>
<p>This paper addresses the inherent inference latency challenges associated with deploying multimodal large language models (MLLM) in quadruped vision-language-action (QUAR-VLA) tasks. Our investigation reveals that conventional parameter reduction techniques ultimately impair the performance of the language foundation model during the action instruction tuning phase, making them unsuitable for this purpose. We introduce a novel latency-free quadruped MLLM model, dubbed QUART-Online, designed to enhance inference efficiency without degrading the performance of the language foundation model. By incorporating Action Chunk Discretization (ACD), we compress the original action representation space, mapping continuous action values onto a smaller set of discrete representative vectors while preserving critical information. Subsequently, we fine-tune the MLLM to integrate vision, language, and compressed actions into a unified semantic space. Experimental results demonstrate that QUART-Online operates in tandem with the existing MLLM system, achieving real-time inference in sync with the underlying controller frequency, significantly boosting the success rate across various tasks by 65%. Our project page is <a target="_blank" rel="noopener" href="https://quart-online.github.io/">https://quart-online.github.io</a>. </p>
<blockquote>
<p>本文关注在多足视觉语言动作（QUAR-VLA）任务中部署多模态大型语言模型（MLLM）所面临的固有推理延迟挑战。我们的调查发现，传统的参数减少技术最终会损害语言基础模型在动作指令调整阶段的性能，使其不适合此目的。我们引入了一种新型的无延迟多足MLLM模型，名为QUART-Online，旨在提高推理效率，同时不降低语言基础模型的性能。通过引入动作块离散化（ACD），我们压缩了原始动作表示空间，将连续动作值映射到一组较小的离散代表向量上，同时保留关键信息。随后，我们对MLLM进行微调，以将视觉、语言和压缩动作集成到统一的语义空间中。实验结果表明，QUART-Online与现有的MLLM系统协同工作，实现与底层控制器频率同步的实时推理，在各种任务中的成功率提高了65%。我们的项目页面是<a target="_blank" rel="noopener" href="https://quart-online.github.io./">https://quart-online.github.io。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15576v3">PDF</a> Accepted to ICRA 2025; Github page: <a target="_blank" rel="noopener" href="https://quart-online.github.io/">https://quart-online.github.io</a></p>
<p><strong>Summary</strong><br>    该论文解决在四足机器人视觉语言动作任务（QUAR-VLA）中部署多模态大型语言模型（MLLM）时面临的固有推理延迟挑战。研究引入了新型无延迟四足MLLM模型QUART-Online，旨在提高推理效率同时不降低语言基础模型性能。通过采用动作块离散化（ACD），压缩原始动作表示空间，将连续动作值映射到一组较小的离散代表向量上，同时保留关键信息。实验结果表明，QUART-Online与现有MLLM系统协同工作，实现了实时推理，与底层控制器频率同步，成功提高了各种任务的成功率达65%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文聚焦于多模态大型语言模型在四足机器人视觉语言动作任务中的推理延迟问题。</li>
<li>传统参数减少技术会降低语言基础模型在动作指令调整阶段的性能，不适合用于此场景。</li>
<li>QUART-Online模型旨在提高推理效率而不损害语言基础模型的性能。</li>
<li>通过动作块离散化技术压缩动作表示空间，实现关键信息的保留和连续动作值的离散化映射。</li>
<li>QUART-Online与现有MLLM系统协同工作，实现了实时推理。</li>
<li>模型成功提高了各种任务的成功率达65%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15576">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6a460f7f1407380f91ed6c5744bd3e4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9587b2ec63f4522f9b93e86cd333bbca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f0f512213c2d131bfc2dfdba8e8798d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8eacc8e2eab7d6a93a4845c1715bfba8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-433f563f11014a70cad7974ef854750f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Causal-World-Model-Underlying-Next-Token-Prediction-in-GPT"><a href="#A-Causal-World-Model-Underlying-Next-Token-Prediction-in-GPT" class="headerlink" title="A Causal World Model Underlying Next Token Prediction in GPT"></a>A Causal World Model Underlying Next Token Prediction in GPT</h2><p><strong>Authors:Raanan Y. Rohekar, Yaniv Gurwicz, Sungduk Yu, Estelle Aflalo, Vasudev Lal</strong></p>
<p>Are generative pre-trained transformer (GPT) models only trained to predict the next token, or do they implicitly learn a world model from which a sequence is generated one token at a time? We examine this question by deriving a causal interpretation of the attention mechanism in GPT, and suggesting a causal world model that arises from this interpretation. Furthermore, we propose that GPT-models, at inference time, can be utilized for zero-shot causal structure learning for in-distribution sequences. Empirical evaluation is conducted in a controlled synthetic environment using the setup and rules of the Othello board game. A GPT, pre-trained on real-world games played with the intention of winning, is tested on synthetic data that only adheres to the game rules, oblivious to the goal of winning. We find that the GPT model is likely to generate moves that adhere to the game rules for sequences for which a causal structure is encoded in the attention mechanism with high confidence. In general, in cases for which the GPT model generates moves that do not adhere to the game rules, it also fails to capture any causal structure. </p>
<blockquote>
<p>GPT预训练生成模型是否仅经过训练以预测下一个令牌，还是它们会隐式学习一个世界模型，从这个模型中按次序生成序列的每个令牌？我们通过推导GPT中注意力机制的因果解释来回答这个问题，并提出由此产生的因果世界模型。此外，我们提出，在推理阶段，GPT模型可用于进行零基础因果结构学习以生成内部分布序列。我们在Othello游戏的控制合成环境中进行实证研究，以测试和评估GPT的能力表现。此游戏中的GPT预训练采用了在真实世界中赢得比赛的策略和目标。我们对仅遵循游戏规则且忽略获胜目标的合成数据进行测试。我们发现，GPT模型在生成遵循游戏规则序列的回合时非常擅长处理含有高信心因果结构的令牌。总而言之，当GPT模型生成的游戏规则不符动作回合时，它会完全忽略任何因果结构。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07446v2">PDF</a> AAAI 2025 Workshop on Artificial Intelligence with Causal Techniques</p>
<p><strong>Summary</strong>：<br>GPT模型是否仅通过预测下一个令牌进行训练，还是它们会隐式地学习一个世界模型，从而逐个生成序列？本文通过推导GPT中注意力机制的因果解释，提出了由此产生的因果世界模型。此外，我们提出GPT模型在推理时可用于零射因果结构学习，用于生成符合分布序列的序列。在Othello游戏的受控合成环境中进行了实证评估。经过在真实世界游戏中预训练的GPT模型，对合成数据的测试仅遵循游戏规则，而忽略了获胜的目标。我们发现GPT模型很可能生成符合游戏规则的行动序列，其中注意力机制编码了高置信度的因果结构。一般来说，如果GPT模型生成的行动不符合游戏规则，它也无法捕捉到任何因果结构。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>GPT模型不仅预测下一个令牌，还隐式地学习一个世界模型。</li>
<li>通过注意力机制的因果解释，提出了因果世界模型。</li>
<li>GPT模型在推理过程中可用于零射因果结构学习。</li>
<li>在Othello游戏的受控环境中评估了GPT模型的性能。</li>
<li>GPT模型在遵循游戏规则的合成数据测试中表现出良好的性能。</li>
<li>GPT模型能够生成符合游戏规则的行动序列，这得益于注意力机制中的高置信度因果结构。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07446">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f5ae3315e5d45c62e7f91f0f8e72d1d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85edd2595c9011bb4bc1c0312716da1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-847b8bdb0e466fb7c60be69480966409.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-519d5369ef24c2d4d8d876068977f2e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8eac38080baaa8521b008cd07fc2466b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TED-VITON-Transformer-Empowered-Diffusion-Models-for-Virtual-Try-On"><a href="#TED-VITON-Transformer-Empowered-Diffusion-Models-for-Virtual-Try-On" class="headerlink" title="TED-VITON: Transformer-Empowered Diffusion Models for Virtual Try-On"></a>TED-VITON: Transformer-Empowered Diffusion Models for Virtual Try-On</h2><p><strong>Authors:Zhenchen Wan, Yanwu Xu, Zhaoqing Wang, Feng Liu, Tongliang Liu, Mingming Gong</strong></p>
<p>Recent advancements in Virtual Try-On (VTO) have demonstrated exceptional efficacy in generating realistic images and preserving garment details, largely attributed to the robust generative capabilities of text-to-image (T2I) diffusion backbones. However, the T2I models that underpin these methods have become outdated, thereby limiting the potential for further improvement in VTO. Additionally, current methods face notable challenges in accurately rendering text on garments without distortion and preserving fine-grained details, such as textures and material fidelity. The emergence of Diffusion Transformer (DiT) based T2I models has showcased impressive performance and offers a promising opportunity for advancing VTO. Directly applying existing VTO techniques to transformer-based T2I models is ineffective due to substantial architectural differences, which hinder their ability to fully leverage the models’ advanced capabilities for improved text generation. To address these challenges and unlock the full potential of DiT-based T2I models for VTO, we propose TED-VITON, a novel framework that integrates a Garment Semantic (GS) Adapter for enhancing garment-specific features, a Text Preservation Loss to ensure accurate and distortion-free text rendering, and a constraint mechanism to generate prompts by optimizing Large Language Model (LLM). These innovations enable state-of-the-art (SOTA) performance in visual quality and text fidelity, establishing a new benchmark for VTO task. Project page: <a target="_blank" rel="noopener" href="https://zhenchenwan.github.io/TED-VITON/">https://zhenchenwan.github.io/TED-VITON/</a> </p>
<blockquote>
<p>最近虚拟试穿（VTO）的进展表明，在生成逼真图像和保留服装细节方面表现出卓越的效力，这主要归功于文本到图像（T2I）扩散背骨的强大生成能力。然而，支撑这些方法所采用的T2I模型已经过时，从而限制了VTO的进一步改进潜力。此外，当前的方法在将文本准确渲染到服装上并保持精细细节（如纹理和材料保真度）方面面临显著挑战。基于扩散变压器（DiT）的T2I模型的兴起表现出了令人印象深刻的表现，并为推进VTO提供了富有希望的机会。由于现有的VTO技术在结构上存在较大差异，直接将其应用于基于transformer的T2I模型效果不佳，这阻碍了它们充分利用模型进行改进文本生成的能力。为了解决这些挑战并释放基于DiT的T2I模型在VTO方面的潜力，我们提出了TED-VITON这一全新框架。它集成了服装语义（GS）适配器以增强服装特定特征、文本保留损失以确保准确且无失真文本渲染，以及一种约束机制，通过优化大型语言模型（LLM）来生成提示。这些创新使我们在视觉质量和文本保真度方面达到了最新水平，为VTO任务建立了新的基准。项目页面：<a target="_blank" rel="noopener" href="https://zhenchenwan.github.io/TED-VITON/">TED-VITON链接</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17017v3">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://github.com/ZhenchenWan/TED-VITON">https://github.com/ZhenchenWan/TED-VITON</a></p>
<p><strong>摘要</strong></p>
<p>最新的虚拟试穿（VTO）技术进展显著，能生成逼真的图像并保留服装细节，主要得益于文本到图像（T2I）扩散背骨的强大生成能力。然而，支撑这些方法的T2I模型已过时，限制了VTO的进一步改进潜力。此外，当前方法在准确渲染服装上的文本、避免失真以及保留精细纹理和材质保真度方面面临挑战。基于扩散变压器（DiT）的T2I模型的出现展示了令人印象深刻的表现，为VTO的进展提供了有前景的机会。由于现有VTO技术直接应用于基于变压器的T2I模型存在架构差异，无法充分利用其先进功能来改善文本生成。为解决这些挑战，解锁DiT基于T2I模型的VTO潜力，我们提出TED-VITON框架，集成服装语义（GS）适配器以增强服装特定特征、文本保留损失以确保准确且无失真的文本渲染，以及通过优化大型语言模型（LLM）生成提示的约束机制。这些创新在视觉质量和文本忠实度方面达到最新水平，为VTO任务建立了新基准。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>虚拟试穿（VTO）技术最新进展得益于文本到图像（T2I）扩散模型的强大生成能力。</li>
<li>当前VTO技术面临渲染服装细节和文本准确性的挑战。</li>
<li>基于扩散变压器（DiT）的T2I模型展现出卓越性能，为VTO发展带来希望。</li>
<li>直接应用现有VTO技术于DiT模型无效，因两者架构差异。</li>
<li>TED-VITON框架旨在解决这些挑战，集成了服装语义适配器、文本保留损失和大型语言模型优化机制。</li>
<li>TED-VITON框架在视觉质量和文本忠实度方面达到最新水平。</li>
<li>项目页面提供了更多关于TED-VITON框架的详细信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17017">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5f2816198db50b600856e00dba43629a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-624981cbbb2b2ec4bc338e6720c90618.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd2932cabf207839a175207d5a120d38.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Memorization-in-Attention-only-Transformers"><a href="#Memorization-in-Attention-only-Transformers" class="headerlink" title="Memorization in Attention-only Transformers"></a>Memorization in Attention-only Transformers</h2><p><strong>Authors:Léo Dana, Muni Sreenivas Pydi, Yann Chevaleyre</strong></p>
<p>Recent research has explored the memorization capacity of multi-head attention, but these findings are constrained by unrealistic limitations on the context size. We present a novel proof for language-based Transformers that extends the current hypothesis to any context size. Our approach improves upon the state-of-the-art by achieving more effective exact memorization with an attention layer, while also introducing the concept of approximate memorization of distributions. Through experimental validation, we demonstrate that our proposed bounds more accurately reflect the true memorization capacity of language models, and provide a precise comparison with prior work. </p>
<blockquote>
<p>近期研究已经探索了多头注意力的记忆能力，但这些发现受到上下文大小不现实的限制。我们为基于语言的Transformer提出了一种新方法，将当前假设扩展到任何上下文大小。我们的方法通过注意力层实现了更有效的精确记忆，同时引入了分布近似记忆的概念，从而改进了当前先进技术。通过实验验证，我们证明了我们提出的界限更准确地反映了语言模型的真实记忆能力，并与先前的工作进行了精确比较。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10115v2">PDF</a> 16 pages, 6 figures, submitted to AISTATS 2025,</p>
<p><strong>Summary</strong></p>
<p>近期研究探讨了多头注意力机制的记忆能力，但受限于上下文大小的非现实限制。我们提出了一种针对语言Transformer的新证明方法，将当前假设扩展到任何上下文大小。我们的方法通过注意力层实现了更有效的精确记忆，并引入了近似记忆分布的概念。实验验证显示，我们提出的边界更准确地反映了语言模型的真实记忆能力，并与先前的工作进行了精确比较。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究对多头注意力机制的记忆能力进行了探索。</li>
<li>现有研究受限于上下文大小的非现实限制。</li>
<li>提出了一种新的针对语言Transformer的证明方法，将假设扩展到任何上下文大小。</li>
<li>方法实现了更有效的精确记忆，并引入了近似记忆分布的概念。</li>
<li>实验验证显示，新提出的边界更准确地反映了语言模型的记忆能力。</li>
<li>与先前的研究相比，新方法提供了更精确的比较。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10115">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0bac5151f9d97871c01d5990837325f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f88aa989e45a9ba33ecf84291636a074.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb5b72417aac8c5505d3f345964b8656.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="YouTube-Comments-Decoded-Leveraging-LLMs-for-Low-Resource-Language-Classification"><a href="#YouTube-Comments-Decoded-Leveraging-LLMs-for-Low-Resource-Language-Classification" class="headerlink" title="YouTube Comments Decoded: Leveraging LLMs for Low Resource Language   Classification"></a>YouTube Comments Decoded: Leveraging LLMs for Low Resource Language   Classification</h2><p><strong>Authors:Aniket Deroy, Subhankar Maity</strong></p>
<p>Sarcasm detection is a significant challenge in sentiment analysis, particularly due to its nature of conveying opinions where the intended meaning deviates from the literal expression. This challenge is heightened in social media contexts where code-mixing, especially in Dravidian languages, is prevalent. Code-mixing involves the blending of multiple languages within a single utterance, often with non-native scripts, complicating the task for systems trained on monolingual data. This shared task introduces a novel gold standard corpus designed for sarcasm and sentiment detection within code-mixed texts, specifically in Tamil-English and Malayalam-English languages. The primary objective of this task is to identify sarcasm and sentiment polarity within a code-mixed dataset of Tamil-English and Malayalam-English comments and posts collected from social media platforms. Each comment or post is annotated at the message level for sentiment polarity, with particular attention to the challenges posed by class imbalance, reflecting real-world scenarios.In this work, we experiment with state-of-the-art large language models like GPT-3.5 Turbo via prompting to classify comments into sarcastic or non-sarcastic categories. We obtained a macro-F1 score of 0.61 for Tamil language. We obtained a macro-F1 score of 0.50 for Malayalam language. </p>
<blockquote>
<p>讽刺检测是情感分析中的一个重大挑战，尤其是因为其传达意见的本质，即所表达的意义与字面意思存在偏差。在社会媒体语境中，这个挑战更为突出，其中混用语言现象尤为普遍，尤其是在德拉维迪亚语言中。语言混用涉及在一个单一的言语中混合多种语言，通常带有非母语脚本，为那些经过单语数据训练的系统增加了任务复杂性。本次共享任务引入了一个新型的金标准语料库，该语料库旨在用于混用语言文本中的讽刺和情感检测，特别是在泰米尔语-英语和马拉雅拉姆语-英语中。该任务的主要目标是识别社交媒体平台上收集的泰米尔语-英语和马拉雅拉姆语-英语混合数据集中的讽刺和情感极性。每条评论或帖子都在消息级别进行情感极性注释，特别关注类别不平衡所带来的挑战，反映真实世界场景。在这项工作中，我们通过提示与最先进的大型语言模型（如GPT-3.5 Turbo）进行实验，将评论分类为讽刺或非讽刺类别。我们为泰米尔语获得了0.61的宏观F1分数。为马拉雅拉姆语获得了0.50的宏观F1分数。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.05039v2">PDF</a> Updated and Final Version</p>
<p><strong>Summary</strong></p>
<p>本文介绍了在情感分析中检测讽刺的难点，特别是在社交媒体环境中语言混杂的情境下更加困难。本研究开发了一种新型金标准语料库，用于识别混合代码文本中的讽刺和情感极性，尤其是泰米尔语和马拉雅拉姆语与英语混合的情况。本研究使用先进的自然语言处理模型GPT-3.5 Turbo进行分类实验，泰米尔语的宏F1分数为0.61，马拉雅拉姆语的宏F1分数为0.5。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>讽刺检测是情感分析中的一大挑战，尤其是在社交媒体语境中语言混杂的情况下更为困难。</li>
<li>代码混合涉及在单个话语中混合多种语言和非母语脚本，这增加了任务复杂性。</li>
<li>本研究引入了一种新型金标准语料库，用于识别混合代码文本中的讽刺和情感极性。</li>
<li>实验使用GPT-3.5 Turbo等先进技术大型语言模型进行。</li>
<li>泰米尔语的宏F1分数为0.61，显示出相对较好的性能。</li>
<li>马拉雅拉姆语的宏F1分数为0.5，需要进一步改进。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.05039">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2c01ecf94292c5673012d68c390a7e84.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Revealing-and-Reducing-Gender-Biases-in-Vision-and-Language-Assistants-VLAs"><a href="#Revealing-and-Reducing-Gender-Biases-in-Vision-and-Language-Assistants-VLAs" class="headerlink" title="Revealing and Reducing Gender Biases in Vision and Language Assistants   (VLAs)"></a>Revealing and Reducing Gender Biases in Vision and Language Assistants   (VLAs)</h2><p><strong>Authors:Leander Girrbach, Stephan Alaniz, Yiran Huang, Trevor Darrell, Zeynep Akata</strong></p>
<p>Pre-trained large language models (LLMs) have been reliably integrated with visual input for multimodal tasks. The widespread adoption of instruction-tuned image-to-text vision-language assistants (VLAs) like LLaVA and InternVL necessitates evaluating gender biases. We study gender bias in 22 popular open-source VLAs with respect to personality traits, skills, and occupations. Our results show that VLAs replicate human biases likely present in the data, such as real-world occupational imbalances. Similarly, they tend to attribute more skills and positive personality traits to women than to men, and we see a consistent tendency to associate negative personality traits with men. To eliminate the gender bias in these models, we find that fine-tuning-based debiasing methods achieve the best trade-off between debiasing and retaining performance on downstream tasks. We argue for pre-deploying gender bias assessment in VLAs and motivate further development of debiasing strategies to ensure equitable societal outcomes. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ExplainableML/vla-gender-bias">https://github.com/ExplainableML/vla-gender-bias</a>. </p>
<blockquote>
<p>预训练大型语言模型（LLM）已可靠地与视觉输入相结合，用于多模式任务。随着指令调整图像到文本视觉语言助理（VLAs）如LLaVA和InternVL的广泛应用，对性别偏见进行评估是必要的。我们研究了22个流行开源VLAs中的性别偏见，涉及人格特质、技能和职业。我们的结果表明，VLAs复制了数据中可能存在的人类偏见，如现实世界中职业分布不均等。同样，它们倾向于将更多的技能和积极的人格特质归因于女性而非男性，并且我们观察到一种持续的趋势，即将消极的人格特质与男性联系在一起。为了消除这些模型中的性别偏见，我们发现基于微调去偏方法在实现去偏并保持下游任务性能之间达到最佳平衡。我们主张在VLAs中预先部署性别偏见评估，并推动进一步开发去偏策略，以确保公平的社会结果。相关代码可访问 <a target="_blank" rel="noopener" href="https://github.com/ExplainableML/vlas-gender-bias">https://github.com/ExplainableML/vlas-gender-bias</a> 了解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.19314v2">PDF</a> Accepted at ICLR 2025</p>
<p><strong>摘要</strong></p>
<p>预训练大型语言模型（LLM）已可靠地用于多模态任务中的视觉输入。随着指令调整型图像到文本的视觉语言助手（VLAs）如LLaVA和InternVL的广泛应用，需要评估性别偏见。本研究对22个流行的开源VLAs中的性别偏见进行了人格特质、技能和职业方面的分析。结果表明，VLAs复制了数据中可能存在的人类偏见，如现实世界中职业分布的不平衡。此外，它们更倾向于为女性分配更多技能和积极的人格特质，相较于男性来说。还存在一种一贯的倾向，即将消极的人格特质与男性联系在一起。为了消除这些模型中的性别偏见，我们发现基于微调技术的去偏方法在实现去偏并保持下游任务性能方面的最佳权衡。我们主张在VLAs中预先部署性别偏见评估，并推动进一步开发去偏策略，以确保公平的社会结果。相关代码可在<a target="_blank" rel="noopener" href="https://github.com/ExplainableML/vla-gender-bias%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ExplainableML/vla-gender-bias找到。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>预训练的大型语言模型在多模态任务中已与视觉输入可靠集成。</li>
<li>广泛应用的视觉语言助手（VLAs）存在性别偏见问题，需要进行评估。</li>
<li>VLAs复制了数据中可能存在的性别偏见，如职业分布不平衡。</li>
<li>VLAs倾向于为女性分配更多技能和积极的人格特质。</li>
<li>存在将消极人格特质与男性联系在一起的倾向。</li>
<li>基于微调技术的去偏方法在去偏和保持下游任务性能方面表现最佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.19314">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ce6cbbb4785b38eddc1a2a3f36d1807a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d597630a209719bef21cf190240b068b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-910e24df412a331499dbc9631ff58310.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb5f79f1a33575a7f357f66575d58848.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="BioMistral-NLU-Towards-More-Generalizable-Medical-Language-Understanding-through-Instruction-Tuning"><a href="#BioMistral-NLU-Towards-More-Generalizable-Medical-Language-Understanding-through-Instruction-Tuning" class="headerlink" title="BioMistral-NLU: Towards More Generalizable Medical Language   Understanding through Instruction Tuning"></a>BioMistral-NLU: Towards More Generalizable Medical Language   Understanding through Instruction Tuning</h2><p><strong>Authors:Yujuan Velvin Fu, Giridhar Kaushik Ramachandran, Namu Park, Kevin Lybarger, Fei Xia, Ozlem Uzuner, Meliha Yetisgen</strong></p>
<p>Large language models (LLMs) such as ChatGPT are fine-tuned on large and diverse instruction-following corpora, and can generalize to new tasks. However, those instruction-tuned LLMs often perform poorly in specialized medical natural language understanding (NLU) tasks that require domain knowledge, granular text comprehension, and structured data extraction. To bridge the gap, we: (1) propose a unified prompting format for 7 important NLU tasks, (2) curate an instruction-tuning dataset, MNLU-Instruct, utilizing diverse existing open-source medical NLU corpora, and (3) develop BioMistral-NLU, a generalizable medical NLU model, through fine-tuning BioMistral on MNLU-Instruct. We evaluate BioMistral-NLU in a zero-shot setting, across 6 important NLU tasks, from two widely adopted medical NLU benchmarks: BLUE and BLURB. Our experiments show that our BioMistral-NLU outperforms the original BioMistral, as well as the proprietary LLMs - ChatGPT and GPT-4. Our dataset-agnostic prompting strategy and instruction tuning step over diverse NLU tasks enhance LLMs’ generalizability across diverse medical NLU tasks. Our ablation experiments show that instruction-tuning on a wider variety of tasks, even when the total number of training instances remains constant, enhances downstream zero-shot generalization. </p>
<blockquote>
<p>大型语言模型（LLM）如ChatGPT是在大量且多样的指令遵循语料库上进行微调，并能推广至新任务。然而，这些经过指令调校的LLM在需要领域知识、精细文本理解和结构化数据提取的专门医学自然语言理解（NLU）任务中表现往往不佳。为了弥补这一差距，我们：(1)为7个重要的NLU任务提出了统一的提示格式，(2)利用多样的现有开源医学NLU语料库，制作了一个指令调校数据集MNLU-Instruct，(3)通过在MNLU-Instruct上对BioMistral进行微调，开发了可通用的医学NLU模型BioMistral-NLU。我们在两个广泛采用的医学NLU基准测试中，对6个重要的NLU任务对BioMistral-NLU进行了零样本设置下的评估：BLUE和BLURB。实验表明，我们的BioMistral-NLU在各方面都超越了原始的BioMistral以及专有LLM——ChatGPT和GPT-4。我们的数据集无关的提示策略以及指令调校步骤在不同的NLU任务上增强了LLM在多种医学NLU任务中的通用性。我们的消融实验表明，在更广泛的任务上进行指令调校，即使训练实例的总数保持不变，也能增强下游零样本推广能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.18955v2">PDF</a> 3 figures an 5 tables; Accepted by AMIA 2025 Informatics Summit</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）如ChatGPT在指令遵循语料库上进行微调，可以适应新任务。但在需要领域知识、精细文本理解和结构化数据提取的特定医疗自然语言理解（NLU）任务中，这些指令微调LLM的表现往往不佳。为了弥补这一差距，我们提出了统一的提示格式，为7个重要的NLU任务制作了一个指令调整数据集MNLU-Instruct，并开发了通过MNLU-Instruct细化的通用医疗NLU模型BioMistral-NLU。我们在零样本设置中对BioMistral-NLU进行了评估，涉及两个广泛采用的医疗NLU基准测试中的6个重要NLU任务。实验表明，BioMistral-NLU的表现优于原始的BioMistral以及专有LLM——ChatGPT和GPT-4。我们的数据集无关提示策略和指令微调步骤在不同的NLU任务上增强了LLM的泛化能力。消融实验表明，在总训练实例数保持不变的情况下，对更多样化的任务进行指令微调，可以提高下游零样本泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs如ChatGPT在特定医疗NLU任务中表现不佳，需要领域知识和结构化数据提取等。</li>
<li>提出了一种统一的提示格式，为多种医疗NLU任务提供解决方案。</li>
<li>制作了一个指令调整数据集MNLU-Instruct，利用现有开源医疗NLU语料库。</li>
<li>开发了BioMistral-NLU模型，通过MNLU-Instruct细化的表现优于原始BioMistral和专有LLM。</li>
<li>数据集无关的提示策略和指令微调步骤增强了LLM在多样化医疗NLU任务上的泛化能力。</li>
<li>消融实验表明，在保持总训练实例数不变的情况下，对更多样化的任务进行指令微调可以提高下游零样本泛化能力。</li>
<li>此研究为提升LLMs在特定领域（如医疗）的任务表现提供了新的思路和方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.18955">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-2ab63a3d6109cdddc7991fa7b16190da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1f59a3560e2194913f20742112049d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6826bae7516632467541a2163ea029e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfb109778e6fc541d3769be428eb8658.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Latent-Space-Chain-of-Embedding-Enables-Output-free-LLM-Self-Evaluation"><a href="#Latent-Space-Chain-of-Embedding-Enables-Output-free-LLM-Self-Evaluation" class="headerlink" title="Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation"></a>Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation</h2><p><strong>Authors:Yiming Wang, Pei Zhang, Baosong Yang, Derek F. Wong, Rui Wang</strong></p>
<p>LLM self-evaluation relies on the LLM’s own ability to estimate response correctness, which can greatly improve its deployment reliability. In this research track, we propose the Chain-of-Embedding (CoE) in the latent space to enable LLMs to perform output-free self-evaluation. CoE consists of all progressive hidden states produced during the inference time, which can be treated as the latent thinking path of LLMs. We find that when LLMs respond correctly and incorrectly, their CoE features differ, these discrepancies assist us in estimating LLM response correctness. Experiments in four diverse domains and seven LLMs fully demonstrate the effectiveness of our method. Meanwhile, its label-free design intent without any training and millisecond-level computational cost ensures real-time feedback in large-scale scenarios. More importantly, we provide interesting insights into LLM response correctness from the perspective of hidden state changes inside LLMs. </p>
<blockquote>
<p>LLM的自我评估依赖于其自身评估响应正确性的能力，这可以大大提高其部署可靠性。在本研究轨迹中，我们提出了潜在空间中的嵌入链（CoE）技术，使LLM能够执行无输出自我评估。CoE由推理时间期间产生的所有渐进隐藏状态组成，可视为LLM的潜在思考路径。我们发现，当LLM正确回答和错误回答时，它们的CoE特征有所不同，这些差异有助于我们评估LLM响应的正确性。在四个不同领域和七个LLM上的实验充分证明了我们方法的有效性。同时，其无需标签的设计意图无需任何训练和毫秒级的计算成本，可确保大规模场景中的实时反馈。更重要的是，我们从LLM内部隐藏状态变化的角度，为LLM响应正确性提供了有趣的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13640v2">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>LLM的自评估能力依靠其自我判断响应正确性的能力，可大幅提高部署可靠性。本研究提出了基于嵌入链（CoE）的潜在空间方法，使LLM能够无输出地进行自我评估。嵌入链包含推理过程中的所有渐进隐藏状态，可视为LLM的潜在思考路径。研究发现，当LLM的响应正确与否时，其嵌入链的特征存在差异，这些差异有助于我们判断LLM的响应正确性。在多领域和多LLM的实验中验证了该方法的有效性。此外，其无需标签的设计意图无需任何训练，毫秒级的计算成本保证了大规模场景下的实时反馈。更重要的是，本研究从LLM内部隐藏状态变化的角度提供了关于响应正确性的有趣见解。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM的自评估能力可以依靠其自身判断响应的正确性，从而提高部署可靠性。</li>
<li>提出了基于嵌入链（CoE）的潜在空间方法，使LLM能够无输出地自我评估。</li>
<li>嵌入链包含推理过程中的所有渐进隐藏状态，反映LLM的潜在思考路径。</li>
<li>LLM在正确和错误响应时，其嵌入链特征存在差异，有助于判断其响应正确性。</li>
<li>在多领域和多LLM的实验中验证了该方法的有效性。</li>
<li>无需标签的设计使得该方法无需额外训练，计算成本低，可实现实时反馈。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13640">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-39665de00ff70101b44070cb910e068f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ffa2006006ad1a04ad37418be30fa50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61ff01a523591a9430b351da86da146a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d046294797b0ab7aaf1dfc4c4e51a752.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Emotion-Aware-Embedding-Fusion-in-LLMs-Flan-T5-LLAMA-2-DeepSeek-R1-and-ChatGPT-4-for-Intelligent-Response-Generation"><a href="#Emotion-Aware-Embedding-Fusion-in-LLMs-Flan-T5-LLAMA-2-DeepSeek-R1-and-ChatGPT-4-for-Intelligent-Response-Generation" class="headerlink" title="Emotion-Aware Embedding Fusion in LLMs (Flan-T5, LLAMA 2, DeepSeek-R1,   and ChatGPT 4) for Intelligent Response Generation"></a>Emotion-Aware Embedding Fusion in LLMs (Flan-T5, LLAMA 2, DeepSeek-R1,   and ChatGPT 4) for Intelligent Response Generation</h2><p><strong>Authors:Abdur Rasool, Muhammad Irfan Shahzad, Hafsa Aslam, Vincent Chan, Muhammad Ali Arshad</strong></p>
<p>Empathetic and coherent responses are critical in auto-mated chatbot-facilitated psychotherapy. This study addresses the challenge of enhancing the emotional and contextual understanding of large language models (LLMs) in psychiatric applications. We introduce Emotion-Aware Embedding Fusion, a novel framework integrating hierarchical fusion and attention mechanisms to prioritize semantic and emotional features in therapy transcripts. Our approach combines multiple emotion lexicons, including NRC Emotion Lexicon, VADER, WordNet, and SentiWordNet, with state-of-the-art LLMs such as Flan-T5, LLAMA 2, DeepSeek-R1, and ChatGPT 4. Therapy session transcripts, comprising over 2,000 samples are segmented into hierarchical levels (word, sentence, and session) using neural networks, while hierarchical fusion combines these features with pooling techniques to refine emotional representations. Atten-tion mechanisms, including multi-head self-attention and cross-attention, further prioritize emotional and contextual features, enabling temporal modeling of emotion-al shifts across sessions. The processed embeddings, computed using BERT, GPT-3, and RoBERTa are stored in the Facebook AI similarity search vector database, which enables efficient similarity search and clustering across dense vector spaces. Upon user queries, relevant segments are retrieved and provided as context to LLMs, enhancing their ability to generate empathetic and con-textually relevant responses. The proposed framework is evaluated across multiple practical use cases to demonstrate real-world applicability, including AI-driven therapy chatbots. The system can be integrated into existing mental health platforms to generate personalized responses based on retrieved therapy session data. </p>
<blockquote>
<p>在自动聊天机器人辅助的心理治疗（psychotherapy）中，体贴且连贯的回应至关重要。本研究旨在解决增强精神病学应用中的大型语言模型（LLM）的情感和上下文理解能力的挑战。我们引入了情感感知嵌入融合（Emotion-Aware Embedding Fusion）这一新型框架，该框架结合了层次融合和注意力机制，以优先处理治疗记录中的语义和情感特征。我们的方法结合了多个情感词典，包括NRC情感词典、VADER、WordNet和SentiWordNet，以及最先进的LLM，如Flan-T5、LLAMA 2、DeepSeek-R1和ChatGPT 4。治疗会话记录由超过两千个样本组成，它们通过神经网络分层划分到不同级别（如单词、句子和会话），而层次融合则使用池技术结合这些特征来优化情感表达。注意力机制包括多头自注意力机制和交叉注意力机制，它们进一步强调情感和上下文特征，实现跨会话的情感变化的时间建模。使用BERT、GPT-3和RoBERTa计算得到的处理过的嵌入存储在Facebook AI相似性搜索向量数据库中，这有助于在密集向量空间中实现高效相似性搜索和聚类。在用户查询时，相关片段被检索出来并提供给LLM作为上下文，从而增强它们生成体贴且上下文相关的响应的能力。所提出的框架经过多个实际应用案例的评估，证明了其在现实世界中的适用性，包括AI驱动的疗法聊天机器人。该系统可以集成到现有的心理健康平台中，根据检索到的治疗会话数据生成个性化响应。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.01306v2">PDF</a> </p>
<p><strong>Summary</strong><br>在自动聊天机器人辅助的心理治疗中，共情和连贯的回应至关重要。本研究旨在提高大语言模型在心理治疗应用中的情感理解和语境理解能力。我们提出了情感感知嵌入融合框架，该框架结合了层次融合和注意力机制，以优先处理治疗记录中的语义和情感特征。该框架结合了多个情感词典，包括NRC情感词典、VADER、WordNet和SentiWordNet等，以及先进的LLM模型，如Flan-T5、LLAMA 2等。通过神经网络将治疗会话记录分段，利用层次融合和池化技术组合这些特征，以优化情感表达。注意力机制有助于优先处理情感和语境特征，实现跨会话的情感变化建模。该研究还使用了BERT、GPT-3和RoBERTa等模型生成的嵌入向量，并利用Facebook AI相似性搜索向量数据库进行高效相似性搜索和聚类。用户查询时，可检索相关片段作为上下文，增强LLM生成共情和语境相关响应的能力。该框架在多个实际应用场景中进行了评估，证明了其在现实世界的适用性，包括AI驱动的治疗聊天机器人。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>情感感知嵌入融合框架用于增强大语言模型在心理治疗中的情感理解和语境理解能力。</li>
<li>框架结合了层次融合和注意力机制，以优先处理治疗记录中的语义和情感特征。</li>
<li>结合了多个情感词典和先进的LLM模型。</li>
<li>治疗会话记录被分段并优化情感表达。</li>
<li>注意力机制有助于跨会话的情感变化建模。</li>
<li>利用BERT、GPT-3和RoBERTa等模型生成的嵌入向量进行相似性搜索和聚类。</li>
<li>该框架适用于AI驱动的治疗聊天机器人，可生成个性化的响应。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.01306">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f0289f2bacfdb2679370bda5dfd43f06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0596cc08ab795b4a047674ea67a1d4fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad3a38ea661e8f934d3233b0412e3bbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab6b06acb53034a45b21c0d9ebe38c40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c1795ba97ee27c77ad7d99d7097e53d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Procedure-Aware-Surgical-Video-language-Pretraining-with-Hierarchical-Knowledge-Augmentation"><a href="#Procedure-Aware-Surgical-Video-language-Pretraining-with-Hierarchical-Knowledge-Augmentation" class="headerlink" title="Procedure-Aware Surgical Video-language Pretraining with Hierarchical   Knowledge Augmentation"></a>Procedure-Aware Surgical Video-language Pretraining with Hierarchical   Knowledge Augmentation</h2><p><strong>Authors:Kun Yuan, Vinkle Srivastav, Nassir Navab, Nicolas Padoy</strong></p>
<p>Surgical video-language pretraining (VLP) faces unique challenges due to the knowledge domain gap and the scarcity of multi-modal data. This study aims to bridge the gap by addressing issues regarding textual information loss in surgical lecture videos and the spatial-temporal challenges of surgical VLP. We propose a hierarchical knowledge augmentation approach and a novel Procedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining (PeskaVLP) framework to tackle these issues. The knowledge augmentation uses large language models (LLM) for refining and enriching surgical concepts, thus providing comprehensive language supervision and reducing the risk of overfitting. PeskaVLP combines language supervision with visual self-supervision, constructing hard negative samples and employing a Dynamic Time Warping (DTW) based loss function to effectively comprehend the cross-modal procedural alignment. Extensive experiments on multiple public surgical scene understanding and cross-modal retrieval datasets show that our proposed method significantly improves zero-shot transferring performance and offers a generalist visual representation for further advancements in surgical scene understanding.The code is available at <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SurgVLP">https://github.com/CAMMA-public/SurgVLP</a> </p>
<blockquote>
<p>手术视频语言预训练（VLP）面临着知识域差距和多模态数据稀缺所带来的独特挑战。本研究旨在通过解决手术讲座视频中文本信息丢失以及手术VLP的时空挑战来弥补这一差距。我们提出了一种分层知识增强方法和一种新颖的手术编码知识增强视频语言预训练（PeskaVLP）框架来解决这些问题。知识增强利用大型语言模型（LLM）来提炼和丰富手术概念，从而为手术语言提供全面的监督，降低过拟合的风险。PeskaVLP结合了语言监督和视觉自监督，构建硬负样本并采用基于动态时间规整（DTW）的损失函数，以有效地理解跨模态过程对齐。在多个公共手术场景理解和跨模态检索数据集上的广泛实验表明，我们提出的方法显著提高了零样本迁移性能，并为手术场景理解的进一步进展提供了通用的视觉表示。代码可从 <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SurgVLP">https://github.com/CAMMA-public/SurgVLP</a> 获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.00263v2">PDF</a> Accepted at the 38th Conference on Neural Information Processing   Systems (NeurIPS 2024 Spolight)</p>
<p><strong>Summary</strong></p>
<p>针对手术视频语言预训练（VLP）中的知识域差距和多模态数据稀缺问题，本研究提出一种基于层次知识增强和新型手术知识编码的视频语言预训练框架（PeskaVLP）。利用大型语言模型（LLM）进行手术概念精细化和丰富化，提供全面的语言监督并降低过拟合风险。结合语言监督和视觉自监督，构建硬负样本并采用基于动态时间规整（DTW）的损失函数，实现跨模态过程对齐的有效理解。在多个公共手术场景理解和跨模态检索数据集上的实验表明，所提方法显著提高了零样本迁移性能，并为手术场景理解的进一步进展提供了通用的视觉表示。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>手术视频语言预训练（VLP）面临知识域差距和多模态数据稀缺的挑战。</li>
<li>提出一种基于层次知识增强的方法，使用大型语言模型（LLM）来丰富手术概念。</li>
<li>PeskaVLP框架结合了语言监督和视觉自监督，以改善跨模态对齐。</li>
<li>通过构建硬负样本和采用基于动态时间规整（DTW）的损失函数，提高了模型性能。</li>
<li>在多个数据集上的实验证明了该方法在零样本迁移性能上的显著提高。</li>
<li>所提方法为手术场景理解提供了通用的视觉表示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.00263">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-44b2841cc61618cdf035053b6d2d34d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2803a96859fdab26884a8de0c0a59615.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e66746cceb53cf91e966757ec04405d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1d43a5ac28f6f1c6f9c307fe583cabc.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Dynamic-Mixture-of-Experts-An-Auto-Tuning-Approach-for-Efficient-Transformer-Models"><a href="#Dynamic-Mixture-of-Experts-An-Auto-Tuning-Approach-for-Efficient-Transformer-Models" class="headerlink" title="Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient   Transformer Models"></a>Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient   Transformer Models</h2><p><strong>Authors:Yongxin Guo, Zhenglin Cheng, Xiaoying Tang, Zhaopeng Tu, Tao Lin</strong></p>
<p>The Sparse Mixture of Experts (SMoE) has been widely employed to enhance the efficiency of training and inference for Transformer-based foundational models, yielding promising results.However, the performance of SMoE heavily depends on the choice of hyper-parameters, such as the number of experts and the number of experts to be activated (referred to as top-k), resulting in significant computational overhead due to the extensive model training by searching over various hyper-parameter configurations. As a remedy, we introduce the Dynamic Mixture of Experts (DynMoE) technique. DynMoE incorporates (1) a novel gating method that enables each token to automatically determine the number of experts to activate. (2) An adaptive process automatically adjusts the number of experts during training. Extensive numerical results across Vision, Language, and Vision-Language tasks demonstrate the effectiveness of our approach to achieve competitive performance compared to GMoE for vision and language tasks, and MoE-LLaVA for vision-language tasks, while maintaining efficiency by activating fewer parameters. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/LINs-lab/DynMoE">https://github.com/LINs-lab/DynMoE</a>. </p>
<blockquote>
<p>稀疏专家混合（SMoE）已被广泛应用于提高基于Transformer的基础模型的训练和推理效率，并产生了有前景的结果。然而，SMoE的性能在很大程度上取决于超参数的选择，如专家数量和要激活的专家数量（称为top-k），由于需要在各种超参数配置上进行广泛的模型训练，因此产生了巨大的计算开销。为了解决这个问题，我们引入了动态专家混合（DynMoE）技术。DynMoE结合了（1）一种新型的门控方法，使每个令牌能够自动确定要激活的专家数量。（2）一个自适应过程会在训练过程中自动调整专家数量。在视觉、语言和视觉语言任务上的大量数值结果表明，我们的方法在实现与GMoE在视觉和语言任务以及MoE-LLaVA在视觉语言任务上的竞争力同时，通过激活更少的参数来保持高效性。我们的代码位于<a target="_blank" rel="noopener" href="https://github.com/LINs-lab/DynMoE%E3%80%82">https://github.com/LINs-lab/DynMoE。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14297v4">PDF</a> ICLR 2025</p>
<p><strong>摘要</strong></p>
<p>Sparse Mixture of Experts (SMoE)在提高基于Transformer的模型的训练和推理效率方面表现出色。然而，其性能高度依赖于超参数的选择，如专家数量和激活的专家数量（称为top-k），导致因搜索各种超参数配置而产生巨大的计算开销。为解决这一问题，我们引入了Dynamic Mixture of Experts (DynMoE)技术。DynMoE包括（1）一种新型的门控方法，使每个令牌能够自动确定要激活的专家数量；（2）一个自适应过程，可在训练过程中自动调整专家的数量。广泛的数值结果表明，我们的方法在视觉、语言和视觉语言任务上实现了与GMoE和MoE-LLaVA相比具有竞争力的性能，同时通过激活较少的参数来提高效率。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/LINs-lab/DynMoE%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/LINs-lab/DynMoE获取。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>Sparse Mixture of Experts (SMoE)增强了Transformer模型训练和推理的效率。</li>
<li>SMoE性能受超参数选择影响，如专家数量和激活的专家数量。</li>
<li>为解决SMoE的问题，提出了Dynamic Mixture of Experts (DynMoE)技术。</li>
<li>DynMoE包含新型门控方法和自适应过程。</li>
<li>门控方法使每个令牌能自动确定激活的专家数量。</li>
<li>自适应过程在训练过程中自动调整专家数量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14297">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-43bab51fa230f7e0867486709a0df065.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ba4959898627c6480023e633406c9c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e419fa47f56d75cfca81335be9d52e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c3aaca7b1772b08eb90f985827eda13.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Closed-Loop-Open-Vocabulary-Mobile-Manipulation-with-GPT-4V"><a href="#Closed-Loop-Open-Vocabulary-Mobile-Manipulation-with-GPT-4V" class="headerlink" title="Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V"></a>Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V</h2><p><strong>Authors:Peiyuan Zhi, Zhiyuan Zhang, Yu Zhao, Muzhi Han, Zeyu Zhang, Zhitian Li, Ziyuan Jiao, Baoxiong Jia, Siyuan Huang</strong></p>
<p>Autonomous robot navigation and manipulation in open environments require reasoning and replanning with closed-loop feedback. In this work, we present COME-robot, the first closed-loop robotic system utilizing the GPT-4V vision-language foundation model for open-ended reasoning and adaptive planning in real-world scenarios.COME-robot incorporates two key innovative modules: (i) a multi-level open-vocabulary perception and situated reasoning module that enables effective exploration of the 3D environment and target object identification using commonsense knowledge and situated information, and (ii) an iterative closed-loop feedback and restoration mechanism that verifies task feasibility, monitors execution success, and traces failure causes across different modules for robust failure recovery. Through comprehensive experiments involving 8 challenging real-world mobile and tabletop manipulation tasks, COME-robot demonstrates a significant improvement in task success rate (~35%) compared to state-of-the-art methods. We further conduct comprehensive analyses to elucidate how COME-robot’s design facilitates failure recovery, free-form instruction following, and long-horizon task planning. </p>
<blockquote>
<p>自主机器人在开放环境中的导航和操控需要进行闭环反馈的推理和再规划。在这项工作中，我们推出了COME-robot，这是第一个利用GPT-4V视觉语言基础模型进行开放式推理和现实世界场景自适应规划的闭环机器人系统。COME-robot包含两个关键的创新模块：（i）多层次开放词汇感知和情境推理模块，利用常识知识和情境信息，实现有效的三维环境探索和目标对象识别；（ii）迭代闭环反馈和恢复机制，验证任务可行性，监控执行成功与否，并跟踪不同模块的失败原因，以实现稳健的故障恢复。通过对涉及8项具有挑战性的现实移动操作和桌面操控任务的全面实验，COME-robot与现有技术相比，在任务成功率方面取得了显著的提高（约35%）。我们还进行了全面的分析，以阐明COME-robot的设计如何促进故障恢复、自由形式的指令遵循和长期任务规划。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.10220v2">PDF</a> 6 pages, Accepted at 2025 IEEE ICRA, website:   <a target="_blank" rel="noopener" href="https://come-robot.github.io/">https://come-robot.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>基于GPT-4V视觉语言基础模型的闭环机器人系统COME-robot，实现了开放环境下的自主机器人导航和操控。该系统通过两级创新模块实现开放式词汇感知和情境推理，以及迭代式闭环反馈和恢复机制，有效提高了任务成功率。在包含多种挑战性移动和桌面操作任务的实验中，与现有技术相比，COME-robot的任务成功率显著提高约35%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>COME-robot是首个利用GPT-4V视觉语言基础模型进行开放式推理和自适应规划的闭环机器人系统。</li>
<li>COME-robot具有两级创新模块：一级是多级开放式词汇感知和情境推理模块，使机器人能够有效地探索三维环境并识别目标物体；另一级是迭代式闭环反馈和恢复机制，用于验证任务的可行性、监控执行成功情况并追踪不同模块的失败原因，从而实现稳健的故障恢复。</li>
<li>COME-robot在多种挑战性移动和桌面操作任务中展示了显著的性能提升，任务成功率提高约35%。</li>
<li>COME-robot的设计有助于故障恢复、自由形式指令跟随和长期任务规划。</li>
<li>多级开放式词汇感知和情境推理模块使得机器人能够利用常识知识和情境信息进行有效的环境探索和物体识别。</li>
<li>迭代式闭环反馈和恢复机制确保机器人在执行复杂任务时能够实时监控并调整，以实现更高的鲁棒性和适应性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.10220">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-307a85ab3bb86d778d1ffc276c372e51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a8722611880acac448e2ea8ef2bfbbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03123ac6e50b23526f60d30b231fdc4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0c21b754e3cbae1672937fe8662c431.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f062968810f260296d853e489ee6033.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="GraphEdit-Large-Language-Models-for-Graph-Structure-Learning"><a href="#GraphEdit-Large-Language-Models-for-Graph-Structure-Learning" class="headerlink" title="GraphEdit: Large Language Models for Graph Structure Learning"></a>GraphEdit: Large Language Models for Graph Structure Learning</h2><p><strong>Authors:Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Kangkang Lu, Zhiyong Huang, Chao Huang</strong></p>
<p>Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies and interactions among nodes in graph-structured data by generating novel graph structures. Graph Neural Networks (GNNs) have emerged as promising GSL solutions, utilizing recursive message passing to encode node-wise inter-dependencies. However, many existing GSL methods heavily depend on explicit graph structural information as supervision signals, leaving them susceptible to challenges such as data noise and sparsity. In this work, we propose GraphEdit, an approach that leverages large language models (LLMs) to learn complex node relationships in graph-structured data. By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning. Our approach not only effectively denoises noisy connections but also identifies node-wise dependencies from a global perspective, providing a comprehensive understanding of the graph structure. We conduct extensive experiments on multiple benchmark datasets to demonstrate the effectiveness and robustness of GraphEdit across various settings. We have made our model implementation available at: <a target="_blank" rel="noopener" href="https://github.com/HKUDS/GraphEdit">https://github.com/HKUDS/GraphEdit</a>. </p>
<blockquote>
<p>图结构学习（GSL）旨在通过生成新的图结构来捕捉图结构数据中节点之间的内在依赖性和交互作用。图神经网络（GNNs）作为一种有前景的GSL解决方案出现，它利用递归消息传递来编码节点间的依赖性。然而，许多现有的GSL方法严重依赖于明确的图结构信息作为监督信号，这使得它们容易受到数据噪声和稀疏性等的挑战。在这项工作中，我们提出了GraphEdit方法，它利用大型语言模型（LLM）来学习图结构数据中的复杂节点关系。通过指令调优图结构增强LLM的推理能力，我们旨在克服与明确的图结构信息相关的局限性，提高图结构学习的可靠性。我们的方法不仅有效地消除了噪声连接，还从全局角度识别节点依赖关系，为图结构提供了全面的理解。我们在多个基准数据集上进行了广泛的实验，以证明GraphEdit在各种设置下的有效性和稳健性。我们的模型实现可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/HKUDS/GraphEdit%E3%80%82">https://github.com/HKUDS/GraphEdit。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.15183v5">PDF</a> </p>
<p><strong>Summary</strong><br>GraphEdit利用大型语言模型（LLM）学习图结构数据中的复杂节点关系，通过指令调整增强LLM的推理能力，旨在克服与显式图结构信息相关的局限性，提高图结构学习的可靠性。此方法不仅能有效去除噪声连接，还能从全局角度识别节点依赖关系，为图结构提供全面的理解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GraphEdit专注于通过生成新型图结构来捕捉图结构数据中节点之间的内在依赖性和交互。</li>
<li>图神经网络（GNNs）作为图结构学习（GSL）的解决方法，通过递归消息传递来编码节点间的依赖性。</li>
<li>许多现有的GSL方法严重依赖于明确的图结构信息作为监督信号，这使其面临数据噪声和稀疏性挑战。</li>
<li>GraphEdit利用大型语言模型（LLM）来学习复杂的节点关系，并旨在克服这些局限性。</li>
<li>通过指令调整增强LLM的推理能力，GraphEdit不仅能有效去除噪声连接，还能从全局角度理解图结构。</li>
<li>GraphEdit在多个基准数据集上进行了广泛的实验，证明了其在不同设置下的有效性和稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.15183">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-623fa890107d452268fe1f877bcbf7bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b6344b2168562c1a3761c012e5ffec6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-700bf5be04a2adf0e84bebb45e0ffe78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ad1356fe9f5aa8042506417866c461a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95dd68802936f42cc6758a73e6110017.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46b68490bd665eb15683e1cbc6a3a909.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-16/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-16/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-16/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4d56c02d4d522dde64ae67f260834dc1.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-03-16  StereoCrafter-Zero Zero-Shot Stereo Video Generation with Noisy Restart
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-16/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-7036ff4da4fb676bcedd4eeb47950e21.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-03-16  GoT Unleashing Reasoning Capability of Multimodal Large Language Model   for Visual Generation and Editing
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23154.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
