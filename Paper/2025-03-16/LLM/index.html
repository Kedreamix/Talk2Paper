<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-16  Minimal Time Series Transformer">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0b6344b2168562c1a3761c012e5ffec6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-16-æ›´æ–°"><a href="#2025-03-16-æ›´æ–°" class="headerlink" title="2025-03-16 æ›´æ–°"></a>2025-03-16 æ›´æ–°</h1><h2 id="Minimal-Time-Series-Transformer"><a href="#Minimal-Time-Series-Transformer" class="headerlink" title="Minimal Time Series Transformer"></a>Minimal Time Series Transformer</h2><p><strong>Authors:Joni-Kristian KÃ¤mÃ¤rÃ¤inen</strong></p>
<p>Transformer is the state-of-the-art model for many natural language processing, computer vision, and audio analysis problems. Transformer effectively combines information from the past input and output samples in auto-regressive manner so that each sample becomes aware of all inputs and outputs. In sequence-to-sequence (Seq2Seq) modeling, the transformer processed samples become effective in predicting the next output. Time series forecasting is a Seq2Seq problem. The original architecture is defined for discrete input and output sequence tokens, but to adopt it for time series, the model must be adapted for continuous data. This work introduces minimal adaptations to make the original transformer architecture suitable for continuous value time series data. </p>
<blockquote>
<p>Transformeræ¨¡å‹æ˜¯å¾ˆå¤šè‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’ŒéŸ³é¢‘åˆ†æé—®é¢˜çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚Transformeræœ‰æ•ˆåœ°ä»¥è‡ªå›å½’æ–¹å¼ç»“åˆäº†æ¥è‡ªè¿‡å»è¾“å…¥å’Œè¾“å‡ºæ ·æœ¬çš„ä¿¡æ¯ï¼Œä»è€Œä½¿æ¯ä¸ªæ ·æœ¬éƒ½èƒ½æ„ŸçŸ¥åˆ°æ‰€æœ‰çš„è¾“å…¥å’Œè¾“å‡ºã€‚åœ¨åºåˆ—åˆ°åºåˆ—ï¼ˆSeq2Seqï¼‰å»ºæ¨¡ä¸­ï¼Œç»è¿‡Transformerå¤„ç†çš„æ ·æœ¬åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªè¾“å‡ºæ—¶éå¸¸æœ‰æ•ˆã€‚æ—¶é—´åºåˆ—é¢„æµ‹æ˜¯ä¸€ä¸ªSeq2Seqé—®é¢˜ã€‚åŸå§‹æ¶æ„æ˜¯ä¸ºç¦»æ•£è¾“å…¥è¾“å‡ºåºåˆ—æ ‡è®°å®šä¹‰çš„ï¼Œä½†è¦å°†å…¶ç”¨äºæ—¶é—´åºåˆ—ï¼Œå¿…é¡»å¯¹æ¨¡å‹è¿›è¡Œé€‚åº”ä»¥å¤„ç†è¿ç»­æ•°æ®ã€‚è¿™é¡¹å·¥ä½œå¯¹åŸå§‹Transformeræ¶æ„è¿›è¡Œäº†æœ€å°çš„è°ƒæ•´ï¼Œä½¿å…¶é€‚ç”¨äºè¿ç»­å€¼æ—¶é—´åºåˆ—æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09791v1">PDF</a> 8 pages, 8 figures</p>
<p><strong>Summary</strong>ï¼šTransformeræ¨¡å‹ç»“åˆäº†è¿‡å¾€è¾“å…¥å’Œè¾“å‡ºæ ·æœ¬çš„ä¿¡æ¯ï¼Œä»¥è‡ªå›å½’çš„æ–¹å¼ä½¿æ¯ä¸ªæ ·æœ¬éƒ½èƒ½æ„ŸçŸ¥åˆ°æ‰€æœ‰çš„è¾“å…¥å’Œè¾“å‡ºï¼Œé€‚ç”¨äºè®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’ŒéŸ³é¢‘åˆ†æç­‰é—®é¢˜ã€‚åœ¨æ—¶é—´åºåˆ—é¢„æµ‹è¿™ç±»åºåˆ—åˆ°åºåˆ—ï¼ˆSeq2Seqï¼‰é—®é¢˜ä¸Šï¼Œç»è¿‡é€‚åº”è¿ç»­æ•°æ®çš„æ”¹è¿›åçš„Transformeræ¨¡å‹è¡¨ç°å‡ºäº†é«˜æ•ˆçš„é¢„æµ‹èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Transformeræ¨¡å‹ç»“åˆäº†è¿‡å¾€è¾“å…¥å’Œè¾“å‡ºæ ·æœ¬çš„ä¿¡æ¯ã€‚</li>
<li>Transformerä»¥è‡ªå›å½’çš„æ–¹å¼å¤„ç†ä¿¡æ¯ï¼Œä½¿æ¯ä¸ªæ ·æœ¬éƒ½èƒ½æ„ŸçŸ¥åˆ°æ‰€æœ‰çš„è¾“å…¥å’Œè¾“å‡ºã€‚</li>
<li>Transformeræ¨¡å‹é€‚ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’ŒéŸ³é¢‘åˆ†æç­‰å¤šç§é—®é¢˜ã€‚</li>
<li>æ—¶é—´åºåˆ—é¢„æµ‹æ˜¯åºåˆ—åˆ°åºåˆ—ï¼ˆSeq2Seqï¼‰é—®é¢˜ã€‚</li>
<li>åŸå§‹çš„Transformeræ¶æ„æ˜¯ä¸ºç¦»æ•£è¾“å…¥å’Œè¾“å‡ºåºåˆ—ä»¤ç‰Œå®šä¹‰çš„ã€‚</li>
<li>ä¸ºäº†å°†Transformeræ¨¡å‹åº”ç”¨äºæ—¶é—´åºåˆ—æ•°æ®ï¼Œå¿…é¡»å¯¹æ¨¡å‹è¿›è¡Œé€‚åº”ä»¥å¤„ç†è¿ç»­æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09791">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5629f86f576a2032a08d3b1e6dd00b83.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a68697e498a5bdfbe01afeed3866d40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-290e9bdfcc1d5375f072571b45ec56ce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dc738325a9aad4ee6a41076903d2a25c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b2ba83069a612f41cb1be1c660dd837b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4e7dd9363211de554ac39768bc13ae54.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9f7a88a9e964170ce79b566d1994889.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ce0067f8f4dc03a84d21531049aed63.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="BIMBA-Selective-Scan-Compression-for-Long-Range-Video-Question-Answering"><a href="#BIMBA-Selective-Scan-Compression-for-Long-Range-Video-Question-Answering" class="headerlink" title="BIMBA: Selective-Scan Compression for Long-Range Video Question   Answering"></a>BIMBA: Selective-Scan Compression for Long-Range Video Question   Answering</h2><p><strong>Authors:Md Mohaiminul Islam, Tushar Nagarajan, Huiyu Wang, Gedas Bertasius, Lorenzo Torresani</strong></p>
<p>Video Question Answering (VQA) in long videos poses the key challenge of extracting relevant information and modeling long-range dependencies from many redundant frames. The self-attention mechanism provides a general solution for sequence modeling, but it has a prohibitive cost when applied to a massive number of spatiotemporal tokens in long videos. Most prior methods rely on compression strategies to lower the computational cost, such as reducing the input length via sparse frame sampling or compressing the output sequence passed to the large language model (LLM) via space-time pooling. However, these naive approaches over-represent redundant information and often miss salient events or fast-occurring space-time patterns. In this work, we introduce BIMBA, an efficient state-space model to handle long-form videos. Our model leverages the selective scan algorithm to learn to effectively select critical information from high-dimensional video and transform it into a reduced token sequence for efficient LLM processing. Extensive experiments demonstrate that BIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks, including PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and Video-MME. Code, and models are publicly available at <a target="_blank" rel="noopener" href="https://sites.google.com/view/bimba-mllm">https://sites.google.com/view/bimba-mllm</a>. </p>
<blockquote>
<p>è§†é¢‘é—®ç­”ï¼ˆVQAï¼‰åœ¨é•¿è§†é¢‘ä¸­é¢ä¸´çš„å…³é”®æŒ‘æˆ˜æ˜¯ä»å¤§é‡å†—ä½™å¸§ä¸­æå–ç›¸å…³ä¿¡æ¯å¹¶å»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸ºåºåˆ—å»ºæ¨¡æä¾›äº†ä¸€ç§é€šç”¨è§£å†³æ–¹æ¡ˆï¼Œä½†å½“åº”ç”¨äºé•¿è§†é¢‘ä¸­çš„å¤§é‡æ—¶ç©ºä»¤ç‰Œæ—¶ï¼Œå…¶æˆæœ¬é«˜æ˜‚ã€‚å¤§å¤šæ•°å…ˆå‰çš„æ–¹æ³•ä¾èµ–äºå‹ç¼©ç­–ç•¥æ¥é™ä½è®¡ç®—æˆæœ¬ï¼Œä¾‹å¦‚é€šè¿‡ç¨€ç–å¸§é‡‡æ ·å‡å°‘è¾“å…¥é•¿åº¦ï¼Œæˆ–é€šè¿‡æ—¶ç©ºæ± åŒ–å‹ç¼©ä¼ é€’ç»™å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºåºåˆ—ã€‚ç„¶è€Œï¼Œè¿™äº›ç®€å•çš„æ–¹æ³•è¿‡äºå¼ºè°ƒå†—ä½™ä¿¡æ¯ï¼Œå¹¶ä¸”ç»å¸¸å¿½ç•¥é‡è¦äº‹ä»¶æˆ–å¿«é€Ÿå‘ç”Ÿçš„æ—¶ç©ºæ¨¡å¼ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†BIMBAï¼Œè¿™æ˜¯ä¸€ç§å¤„ç†é•¿æ ¼å¼è§†é¢‘çš„é«˜æ•ˆçŠ¶æ€ç©ºé—´æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹åˆ©ç”¨é€‰æ‹©æ€§æ‰«æç®—æ³•æ¥å­¦ä¹ æœ‰æ•ˆåœ°ä»é«˜ç»´è§†é¢‘ä¸­é€‰æ‹©å…³é”®ä¿¡æ¯ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå‡å°‘çš„ä»¤ç‰Œåºåˆ—ï¼Œä»¥ä¾¿å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé«˜æ•ˆå¤„ç†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBIMBAåœ¨å¤šä¸ªé•¿æ ¼å¼VQAåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ŒåŒ…æ‹¬PerceptionTestã€NExT-QAã€EgoSchemaã€VNBenchã€LongVideoBenchå’ŒVideo-MMEã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://sites.google.com/view/bimba-mllm%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://sites.google.com/view/bimba-mllmå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09590v2">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤„ç†é•¿è§†é¢‘VQAï¼ˆè§†é¢‘é—®é¢˜å›ç­”ï¼‰çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä»å¤§é‡å†—ä½™å¸§ä¸­æå–ç›¸å…³ä¿¡æ¯å’Œå»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚ç°æœ‰æ–¹æ³•å¤šé‡‡ç”¨å‹ç¼©ç­–ç•¥é™ä½è®¡ç®—æˆæœ¬ï¼Œä½†å­˜åœ¨è¿‡åº¦ä»£è¡¨å†—ä½™ä¿¡æ¯ã€å¿½ç•¥é‡è¦äº‹ä»¶æˆ–å¿«é€Ÿæ—¶ç©ºæ¨¡å¼çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºBIMBAæ¨¡å‹ï¼Œåˆ©ç”¨é€‰æ‹©æ€§æ‰«æç®—æ³•ä»é«˜ç»´è§†é¢‘ä¸­é€‰æ‹©å…³é”®ä¿¡æ¯ï¼Œè½¬åŒ–ä¸ºç®€åŒ–çš„ä»¤ç‰Œåºåˆ—ï¼Œä¾›LLMé«˜æ•ˆå¤„ç†ã€‚å®éªŒè¯æ˜BIMBAåœ¨å¤šä¸ªé•¿è§†é¢‘VQAåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼ŒåŒ…æ‹¬PerceptionTestã€NExT-QAç­‰ã€‚æ¨¡å‹ä¸ä»£ç å…¬å¼€äºï¼š<a target="_blank" rel="noopener" href="https://sites.google.com/view/bimba-mllm%E3%80%82">https://sites.google.com/view/bimba-mllmã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤„ç†é•¿è§†é¢‘çš„VQAé¢ä¸´æå–ç›¸å…³ä¿¡æ¯å’Œå»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä½¿ç”¨å‹ç¼©ç­–ç•¥ä»¥é™ä½è®¡ç®—æˆæœ¬ï¼Œä½†å¯èƒ½å¿½ç•¥é‡è¦ä¿¡æ¯ã€‚</li>
<li>BIMBAæ¨¡å‹åˆ©ç”¨é€‰æ‹©æ€§æ‰«æç®—æ³•ä»é«˜ç»´è§†é¢‘ä¸­é€‰æ‹©å…³é”®ä¿¡æ¯ã€‚</li>
<li>BIMBAå°†å…³é”®ä¿¡æ¯è½¬åŒ–ä¸ºç®€åŒ–çš„ä»¤ç‰Œåºåˆ—ä¾›LLMé«˜æ•ˆå¤„ç†ã€‚</li>
<li>BIMBAåœ¨å¤šä¸ªé•¿è§†é¢‘VQAåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
<li>æ¨¡å‹ä¸ä»£ç å·²å…¬å¼€ï¼Œä¾¿äºä»–äººä½¿ç”¨å’Œç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09590">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7f84c6cc66c2ee34bcf35e9c861c87c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdfc74efca2a7b717a229f97b7813cca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-86d329099c75da7556c054ea1ec044c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6053314104e86321fb09c324e3bf8a9.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MastermindEval-A-Simple-But-Scalable-Reasoning-Benchmark"><a href="#MastermindEval-A-Simple-But-Scalable-Reasoning-Benchmark" class="headerlink" title="MastermindEval: A Simple But Scalable Reasoning Benchmark"></a>MastermindEval: A Simple But Scalable Reasoning Benchmark</h2><p><strong>Authors:Jonas Golde, Patrick Haller, Fabio Barth, Alan Akbik</strong></p>
<p>Recent advancements in large language models (LLMs) have led to remarkable performance across a wide range of language understanding and mathematical tasks. As a result, increasing attention has been given to assessing the true reasoning capabilities of LLMs, driving research into commonsense, numerical, logical, and qualitative reasoning. However, with the rapid progress of reasoning-focused models such as OpenAIâ€™s o1 and DeepSeekâ€™s R1, there has been a growing demand for reasoning benchmarks that can keep pace with ongoing model developments. In this paper, we introduce MastermindEval, a simple, scalable, and interpretable deductive reasoning benchmark inspired by the board game Mastermind. Our benchmark supports two evaluation paradigms: (1) agentic evaluation, in which the model autonomously plays the game, and (2) deductive reasoning evaluation, in which the model is given a pre-played game state with only one possible valid code to infer. In our experimental results we (1) find that even easy Mastermind instances are difficult for current models and (2) demonstrate that the benchmark is scalable to possibly more advanced models in the future Furthermore, we investigate possible reasons why models cannot deduce the final solution and find that current models are limited in deducing the concealed code as the number of statement to combine information from is increasing. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•åœ¨å¹¿æ³›çš„è‡ªç„¶è¯­è¨€ç†è§£å’Œæ•°å­¦ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚å› æ­¤ï¼Œäººä»¬è¶Šæ¥è¶Šå…³æ³¨è¯„ä¼°LLMçš„çœŸæ­£æ¨ç†èƒ½åŠ›ï¼Œæ¨åŠ¨äº†å¸¸è¯†æ¨ç†ã€æ•°å€¼æ¨ç†ã€é€»è¾‘å’Œå®šæ€§æ¨ç†çš„ç ”ç©¶ã€‚ç„¶è€Œï¼Œéšç€ä»¥æ¨ç†ä¸ºé‡ç‚¹çš„æ¨¡å‹å¦‚OpenAIçš„o1å’ŒDeepSeekçš„R1çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹èƒ½ä¸å½“å‰æ¨¡å‹å‘å±•åŒæ­¥çš„æ¨ç†åŸºå‡†æµ‹è¯•çš„éœ€æ±‚ä¹Ÿåœ¨å¢é•¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MastermindEvalï¼Œè¿™æ˜¯ä¸€ä¸ªå—æ£‹ç›˜æ¸¸æˆã€ŠMastermindã€‹å¯å‘çš„ç®€å•ã€å¯æ‰©å±•å’Œå¯è§£é‡Šçš„æ¼”ç»æ¨ç†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ”¯æŒä¸¤ç§è¯„ä¼°æ¨¡å¼ï¼šï¼ˆ1ï¼‰è‡ªä¸»è¯„ä¼°æ¨¡å¼ï¼Œæ¨¡å‹è‡ªä¸»ç©æ¸¸æˆï¼›ï¼ˆ2ï¼‰æ¼”ç»æ¨ç†è¯„ä¼°æ¨¡å¼ï¼Œç»™å®šä¸€ä¸ªé¢„å…ˆè¿›è¡Œçš„æ¸¸æˆçŠ¶æ€ï¼Œæ¨¡å‹éœ€è¦é€šè¿‡æ¨ç†æ‰¾å‡ºå”¯ä¸€çš„æ­£ç¡®ç­”æ¡ˆã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼šï¼ˆ1ï¼‰å³ä½¿æ˜¯ç®€å•çš„Mastermindå®ä¾‹å¯¹äºå½“å‰æ¨¡å‹æ¥è¯´ä¹Ÿæ˜¯å›°éš¾çš„ï¼›ï¼ˆ2ï¼‰è¯¥åŸºå‡†æµ‹è¯•å¯ä»¥æ‰©å±•åˆ°æœªæ¥å¯èƒ½æ›´å…ˆè¿›çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è°ƒæŸ¥äº†æ¨¡å‹æ— æ³•æ¨æ–­å‡ºæœ€ç»ˆè§£å†³æ–¹æ¡ˆçš„å¯èƒ½åŸå› ï¼Œå¹¶å‘ç°éšç€éœ€è¦ç»„åˆä¿¡æ¯æ¥æ¨æ–­ç­”æ¡ˆçš„é™ˆè¿°æ•°é‡å¢åŠ ï¼Œå½“å‰æ¨¡å‹åœ¨æ¨æ–­éšè—ä»£ç æ–¹é¢çš„èƒ½åŠ›å—åˆ°é™åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05891v4">PDF</a> 9 pages, 2 figures, 4 tables. In: ICLR 2025 Workshop on Reasoning and   Planning for Large Language Models</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•åœ¨å„ç§è¯­è¨€ç†è§£å’Œæ•°å­¦ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆç»©ï¼Œå¼•å‘äº†äººä»¬å¯¹è¯„ä¼°å…¶çœŸæ­£æ¨ç†èƒ½åŠ›çš„å…³æ³¨ï¼Œæ¨åŠ¨äº†å¸¸è¯†ã€æ•°å€¼ã€é€»è¾‘å’Œå®šæ€§æ¨ç†çš„ç ”ç©¶ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†MastermindEvalåŸºå‡†æµ‹è¯•ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•ã€å¯æ‰©å±•ä¸”å¯è§£é‡Šçš„æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œçµæ„Ÿæ¥æºäºçŒœè°œæ¸¸æˆMastermindã€‚è¯¥åŸºå‡†æµ‹è¯•æ”¯æŒä¸¤ç§è¯„ä¼°æ¨¡å¼ï¼šè‡ªä¸»æ¸¸æˆè¯„ä¼°å’Œæ¨ç†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯ç®€å•çš„çŒœè°œå®ä¾‹å¯¹äºå½“å‰æ¨¡å‹æ¥è¯´ä¹Ÿæ˜¯å›°éš¾çš„ï¼Œå¹¶ä¸”è¯¥åŸºå‡†æµ‹è¯•å¯ä»¥æ‰©å±•åˆ°æ›´å…ˆè¿›çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†æ¨¡å‹æ— æ³•æ¨æ–­æœ€ç»ˆè§£å†³æ–¹æ¡ˆçš„å¯èƒ½åŸå› ï¼Œå‘ç°å½“å‰æ¨¡å‹åœ¨å¤„ç†ä¿¡æ¯ç»„åˆæ—¶å­˜åœ¨å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„æœ€æ–°è¿›å±•åœ¨å„ç§è¯­è¨€ç†è§£å’Œæ•°å­¦ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆç»©ã€‚</li>
<li>äººä»¬éœ€è¦è¯„ä¼°LLMçš„çœŸæ­£æ¨ç†èƒ½åŠ›ï¼Œè¿™æ¨åŠ¨äº†å¸¸è¯†ã€æ•°å€¼ã€é€»è¾‘å’Œå®šæ€§æ¨ç†çš„ç ”ç©¶ã€‚</li>
<li>MastermindEvalåŸºå‡†æµ‹è¯•æ˜¯ä¸€ä¸ªç®€å•ã€å¯æ‰©å±•ä¸”å¯è§£é‡Šçš„æ¨ç†åŸºå‡†æµ‹è¯•ï¼Œçµæ„Ÿæ¥æºäºçŒœè°œæ¸¸æˆMastermindã€‚</li>
<li>MastermindEvalæ”¯æŒä¸¤ç§è¯„ä¼°æ¨¡å¼ï¼šè‡ªä¸»æ¸¸æˆè¯„ä¼°å’Œæ¨ç†è¯„ä¼°ã€‚</li>
<li>å®éªŒå‘ç°ï¼Œå³ä½¿æ˜¯ç®€å•çš„çŒœè°œå®ä¾‹å¯¹äºå½“å‰æ¨¡å‹æ¥è¯´ä¹Ÿæ˜¯å›°éš¾çš„ã€‚</li>
<li>MastermindEvalåŸºå‡†æµ‹è¯•å¯ä»¥æ‰©å±•åˆ°æ›´å…ˆè¿›çš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05891">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8151909c9828f5f282094b4734b8794b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d864a105f003beb4d82ca2d8ca48bcd7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9417ec18fef4236b19fbdf03aa03f636.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-981cc37b1f842857a47af413369acb78.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Seeing-is-Understanding-Unlocking-Causal-Attention-into-Modality-Mutual-Attention-for-Multimodal-LLMs"><a href="#Seeing-is-Understanding-Unlocking-Causal-Attention-into-Modality-Mutual-Attention-for-Multimodal-LLMs" class="headerlink" title="Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual   Attention for Multimodal LLMs"></a>Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual   Attention for Multimodal LLMs</h2><p><strong>Authors:Wei-Yao Wang, Zhao Wang, Helen Suzuki, Yoshiyuki Kobayashi</strong></p>
<p>Recent Multimodal Large Language Models (MLLMs) have demonstrated significant progress in perceiving and reasoning over multimodal inquiries, ushering in a new research era for foundation models. However, vision-language misalignment in MLLMs has emerged as a critical challenge, where the textual responses generated by these models are not factually aligned with the given text-image inputs. Existing efforts to address vision-language misalignment have focused on developing specialized vision-language connectors or leveraging visual instruction tuning from diverse domains. In this paper, we tackle this issue from a fundamental yet unexplored perspective by revisiting the core architecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs consisting of a causal attention mechanism, which limits the ability of the earlier modalities (e.g., images) to incorporate information from the latter modalities (e.g., text). To address this problem, we propose \MapleLeaf AKI, a novel MLLM that unlocks causal attention into modality-mutual attention (MMA) to enable image tokens to attend to text tokens. This simple yet effective design allows AKI to achieve superior performance in 12 multimodal understanding benchmarks (+7.2% on average) without introducing additional parameters and increasing training time. Our MMA design is intended to be generic, allowing for application across various modalities, and scalable to accommodate diverse multimodal scenarios. The code and model are publicly available at <a target="_blank" rel="noopener" href="https://github.com/sony/aki">https://github.com/sony/aki</a> to encourage further advancements in MLLMs across various directions. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ„ŸçŸ¥å’Œå¤„ç†å¤šæ¨¡æ€æŸ¥è¯¢æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä¸ºåŸºç¡€æ¨¡å‹é¢†åŸŸå¼€å¯äº†æ–°çš„ç ”ç©¶æ—¶ä»£ã€‚ç„¶è€Œï¼ŒMLLMsä¸­çš„è§†è§‰è¯­è¨€ä¸åŒ¹é…é—®é¢˜å·²æˆä¸ºä¸€é¡¹å…³é”®æŒ‘æˆ˜ï¼Œå…¶ä¸­è¿™äº›æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å“åº”ä¸ç»™å®šçš„æ–‡æœ¬å›¾åƒè¾“å…¥å¹¶ä¸ç¬¦åˆäº‹å®ã€‚ä¸ºè§£å†³è§†è§‰è¯­è¨€ä¸åŒ¹é…é—®é¢˜ï¼Œç°æœ‰å·¥ä½œä¸»è¦é›†ä¸­åœ¨å¼€å‘ä¸“é—¨çš„è§†è§‰è¯­è¨€è¿æ¥å™¨æˆ–ä»å„ç§é¢†åŸŸåˆ©ç”¨è§†è§‰æŒ‡ä»¤è°ƒæ•´ã€‚æœ¬æ–‡ä»ä¸€ä¸ªåŸºæœ¬ä½†å°šæœªè¢«æ¢ç´¢çš„è§†è§’æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡é‡æ–°å®¡è§†MLLMsçš„æ ¸å¿ƒæ¶æ„ã€‚å¤§å¤šæ•°MLLMsé€šå¸¸å»ºç«‹åœ¨ä»…è§£ç å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šï¼ŒåŒ…æ‹¬å› æœæ³¨æ„æœºåˆ¶ï¼Œè¿™é™åˆ¶äº†æ—©æœŸæ¨¡æ€ï¼ˆä¾‹å¦‚å›¾åƒï¼‰èå…¥åæœŸæ¨¡æ€ï¼ˆä¾‹å¦‚æ–‡æœ¬ï¼‰ä¿¡æ¯çš„èƒ½åŠ›ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MapleLeafAKIï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹MLLMï¼Œå®ƒå°†å› æœæ³¨æ„åŠ›è§£é”ä¸ºæ¨¡æ€ç›¸äº’æ³¨æ„åŠ›ï¼ˆMMAï¼‰ï¼Œä½¿å›¾åƒæ ‡è®°èƒ½å¤Ÿå…³æ³¨æ–‡æœ¬æ ‡è®°ã€‚è¿™ç§ç®€å•è€Œæœ‰æ•ˆçš„è®¾è®¡ä½¿AKIèƒ½å¤Ÿåœ¨12ä¸ªå¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•ä¸­å®ç°å“è¶Šæ€§èƒ½ï¼ˆå¹³å‡æé«˜7.2%ï¼‰ï¼ŒåŒæ—¶ä¸å¼•å…¥é¢å¤–å‚æ•°å¹¶å¢åŠ è®­ç»ƒæ—¶é—´ã€‚æˆ‘ä»¬çš„MMAè®¾è®¡æ—¨åœ¨å…·æœ‰é€šç”¨æ€§ï¼Œå¯åº”ç”¨äºå„ç§æ¨¡æ€ï¼Œå¹¶ä¸”å¯æ‰©å±•ä»¥é€‚åº”å„ç§å¤šæ¨¡æ€åœºæ™¯ã€‚ä»£ç å’Œæ¨¡å‹å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/sony/aki%E5%85%AC%E5%BC%80%EF%BC%8C%E4%BB%A5%E9%BC%93%E5%8A%B1%E5%9C%A8MLLMs%E7%9A%84%E5%90%84%E4%B8%AA%E9%A2%86%E5%9F%9F%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%8F%96%E5%BE%97%E8%BF%9B%E5%B1%95%E3%80%82">https://github.com/sony/akiå…¬å¼€ï¼Œä»¥é¼“åŠ±åœ¨MLLMsçš„å„ä¸ªé¢†åŸŸè¿›ä¸€æ­¥å–å¾—è¿›å±•ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02597v2">PDF</a> Preprint</p>
<p><strong>æ‘˜è¦</strong><br>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ„ŸçŸ¥å’Œæ¨ç†å¤šæ¨¡æ€æŸ¥è¯¢æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼ŒMLLMsä¸­çš„è§†è§‰è¯­è¨€é”™ä½æˆä¸ºä¸€é¡¹å…³é”®æŒ‘æˆ˜ï¼Œæ¨¡å‹çš„æ–‡æœ¬å›å¤ä¸ç»™å®šçš„æ–‡æœ¬å›¾åƒè¾“å…¥æ— æ³•äº‹å®å¯¹é½ã€‚ç°æœ‰è§£å†³è§†è§‰è¯­è¨€é”™ä½é—®é¢˜çš„å°è¯•ä¸»è¦é›†ä¸­å¼€å‘ä¸“ç”¨çš„è§†è§‰è¯­è¨€è¿æ¥å™¨æˆ–ä½¿ç”¨æ¥è‡ªä¸åŒåŸŸçš„è§†è§‰æŒ‡ä»¤å¾®è°ƒæ–¹æ³•ä¸Šã€‚æœ¬æ–‡ç«™åœ¨å…¨æ–°è§†è§’ä¸Šé‡æ–°å®¡è§†äº†MLLMçš„æ ¸å¿ƒæ¶æ„æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚å¤§å¤šæ•°MLLMéƒ½æ˜¯åŸºäºä»…è§£ç å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºçš„ï¼Œç”±å› æœæ³¨æ„åŠ›æœºåˆ¶ä¸»å¯¼ï¼Œè¿™é™åˆ¶äº†æ—©æœŸæ¨¡æ€ï¼ˆå¦‚å›¾åƒï¼‰èå…¥åæœŸæ¨¡æ€ï¼ˆå¦‚æ–‡æœ¬ï¼‰ä¿¡æ¯çš„èƒ½åŠ›ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†æ–°å‹MLLMâ€œMapleLeafAKIâ€ï¼Œå®ƒè§£é”äº†å› æœæ³¨æ„åŠ›ä»¥ä¿ƒæˆæ¨¡æ€é—´ç›¸äº’æ³¨æ„åŠ›ï¼ˆMMAï¼‰ï¼Œè®©å›¾åƒæ ‡è®°èƒ½å¤Ÿå…³æ³¨æ–‡æœ¬æ ‡è®°ã€‚è¿™ä¸€ç®€æ´è€Œé«˜æ•ˆçš„è®¾è®¡ä½¿å¾—AKIèƒ½å¤Ÿåœ¨æ— éœ€å¢åŠ é¢å¤–å‚æ•°å’ŒåŸ¹è®­æ—¶é—´çš„æƒ…å†µä¸‹ï¼Œåœ¨åäºŒä¸ªå¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡äº†7.2%çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„MMAè®¾è®¡æ—¨åœ¨é€šç”¨æ€§ï¼Œå¯ä»¥åº”ç”¨äºå„ç§æ¨¡æ€ï¼Œå¹¶å¯ä»¥é€‚åº”å¤šæ ·åŒ–çš„å¤šæ¨¡æ€åœºæ™¯ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/sony/aki%E5%85%AC%E5%BC%80%E6%8F%90%E4%BE%9B%EF%BC%8C%E4%BB%A5%E9%BC%93%E5%8A%B1%E5%9C%A8%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8D%E5%90%8C%E6%96%B9%E5%90%91%E4%B8%8A%E5%8F%96%E5%BE%97%E8%BF%9B%E4%B8%80%E6%AD%A5%E8%BF%9B%E5%B1%95%E3%80%82">https://github.com/sony/akiå…¬å¼€æä¾›ï¼Œä»¥é¼“åŠ±åœ¨å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„ä¸åŒæ–¹å‘ä¸Šå–å¾—è¿›ä¸€æ­¥è¿›å±•ã€‚</a></p>
<p><strong>è¦ç‚¹é€Ÿè§ˆ</strong></p>
<ol>
<li>MLLMsåœ¨å¤šæ¨¡æ€æŸ¥è¯¢æ„ŸçŸ¥å’Œæ¨ç†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—è¿›æ­¥ï¼Œä½†è§†è§‰è¯­è¨€é”™ä½æˆä¸ºä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰è§£å†³ç­–ç•¥ä¸»è¦å›´ç»•å¼€å‘ç‰¹å®šè§†è§‰è¯­è¨€è¿æ¥å™¨æˆ–åˆ©ç”¨è§†è§‰æŒ‡ä»¤å¾®è°ƒæ–¹æ³•å±•å¼€ã€‚</li>
<li>æœ¬æ–‡é€šè¿‡é‡æ–°å®¡è§†MLLMçš„æ ¸å¿ƒæ¶æ„æ¥è§£å†³è§†è§‰è¯­è¨€é”™ä½é—®é¢˜ã€‚</li>
<li>MLLMsä¸»è¦ç”±è§£ç å™¨çš„å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºï¼Œå—å› æœæ³¨æ„åŠ›æœºåˆ¶é™åˆ¶ï¼Œæ—©æœŸæ¨¡æ€éš¾ä»¥èå…¥åæœŸæ¨¡æ€ä¿¡æ¯ã€‚</li>
<li>æå‡ºæ–°å‹MLLMâ€œMapleLeafAKIâ€ï¼Œè§£é”å› æœæ³¨æ„åŠ›ä»¥ä¿ƒæˆæ¨¡æ€é—´ç›¸äº’æ³¨æ„åŠ›ï¼ˆMMAï¼‰ã€‚</li>
<li>AKIè®¾è®¡å…è®¸å›¾åƒæ ‡è®°å…³æ³¨æ–‡æœ¬æ ‡è®°ï¼Œä»è€Œåœ¨å¤šæ¨¡æ€ç†è§£åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a396e6995cde83d8d492daec7337312d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e073b1ad99a96acc38117fbd1a333035.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35223e55116e75992b76fe7ff97b1574.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c646ce9fb294b1e9730b899e36de328.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0c84f01c51ad0762dc24abedfc986e6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DataMan-Data-Manager-for-Pre-training-Large-Language-Models"><a href="#DataMan-Data-Manager-for-Pre-training-Large-Language-Models" class="headerlink" title="DataMan: Data Manager for Pre-training Large Language Models"></a>DataMan: Data Manager for Pre-training Large Language Models</h2><p><strong>Authors:Ru Peng, Kexin Yang, Yawen Zeng, Junyang Lin, Dayiheng Liu, Junbo Zhao</strong></p>
<p>The performance emergence of large language models (LLMs) driven by data scaling laws makes the selection of pre-training data increasingly important. However, existing methods rely on limited heuristics and human intuition, lacking comprehensive and clear guidelines. To address this, we are inspired by &#96;&#96;reverse thinkingâ€™â€™ â€“ prompting LLMs to self-identify which criteria benefit its performance. As its pre-training capabilities are related to perplexity (PPL), we derive 14 quality criteria from the causes of text perplexity anomalies and introduce 15 common application domains to support domain mixing. In this paper, we train a Data Manager (DataMan) to learn quality ratings and domain recognition from pointwise rating, and use it to annotate a 447B token pre-training corpus with 14 quality ratings and domain type. Our experiments validate our approach, using DataMan to select 30B tokens to train a 1.3B-parameter language model, demonstrating significant improvements in in-context learning (ICL), perplexity, and instruction-following ability over the state-of-the-art baseline. The best-performing model, based on the Overall Score l&#x3D;5 surpasses a model trained with 50% more data using uniform sampling. We continue pre-training with high-rated, domain-specific data annotated by DataMan to enhance domain-specific ICL performance and thus verify DataManâ€™s domain mixing ability. Our findings emphasize the importance of quality ranking, the complementary nature of quality criteria, and their low correlation with perplexity, analyzing misalignment between PPL and ICL performance. We also thoroughly analyzed our pre-training dataset, examining its composition, the distribution of quality ratings, and the original document sources. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°æ®è§„æ¨¡æ³•åˆ™çš„æ¨åŠ¨ä¸‹æ€§èƒ½æ—¥ç›Šæ˜¾ç°ï¼Œè¿™ä½¿å¾—é¢„è®­ç»ƒæ•°æ®çš„é€‰æ‹©å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºæœ‰é™çš„å¯å‘å¼æ–¹æ³•å’Œäººç±»ç›´è§‰ï¼Œç¼ºä¹å…¨é¢ã€æ˜ç¡®çš„æŒ‡å¯¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ä»â€œé€†å‘æ€ç»´â€ä¸­æ±²å–çµæ„Ÿï¼Œå¼•å¯¼LLMè‡ªæˆ‘è¯†åˆ«å“ªäº›æ ‡å‡†å¯¹å…¶æ€§èƒ½æœ‰ç›Šã€‚ç”±äºå…¶é¢„è®­ç»ƒèƒ½åŠ›ä¸å›°æƒ‘åº¦ï¼ˆPPLï¼‰ç›¸å…³ï¼Œæˆ‘ä»¬ä»æ–‡æœ¬å›°æƒ‘åº¦å¼‚å¸¸çš„åŸå› ä¸­å¾—å‡º14ä¸ªè´¨é‡æ ‡å‡†ï¼Œå¹¶å¼•å…¥15ä¸ªå¸¸è§åº”ç”¨é¢†åŸŸä»¥æ”¯æŒé¢†åŸŸæ··åˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªæ•°æ®ç®¡ç†å™¨ï¼ˆDataManï¼‰æ¥å­¦ä¹ å®šç‚¹è¯„åˆ†ä¸­çš„è´¨é‡è¯„åˆ†å’Œé¢†åŸŸè¯†åˆ«ï¼Œå¹¶ä½¿ç”¨å®ƒå¯¹ä¸€ä¸ªç”±è¯­æ–™åº“è‡ªåŠ¨å¤„ç†ç”Ÿæˆçš„åŒ…å«447äº¿ä¸ªæ ‡è®°çš„é¢„è®­ç»ƒè¯­æ–™åº“è¿›è¡Œæ ‡æ³¨ï¼Œå…¶ä¸­åŒ…æ‹¬äº†è´¨é‡è¯„åˆ†å’Œé¢†åŸŸç±»å‹ä¿¡æ¯å…±æ¶µç›–14ä¸ªæŒ‡æ ‡ã€‚æˆ‘ä»¬çš„å®éªŒéªŒè¯äº†ä½¿ç”¨DataMané€‰å–çš„è¯­æ–™å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œè®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬ä½¿ç”¨è¯¥å·¥å…·é€‰å–äº†ä»·å€¼30äº¿ä¸ªæ ‡è®°çš„æ•°æ®é›†æ¥è®­ç»ƒä¸€ä¸ªè§„æ¨¡ä¸º1.3äº¿å‚æ•°çš„LLMæ¨¡å‹ï¼Œå¹¶åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ã€å›°æƒ‘åº¦å’ŒæŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›æ–¹é¢å®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹ã€‚åŸºäºæ€»ä½“å¾—åˆ†è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼ˆæ€»ä½“å¾—åˆ†l&#x3D;5ï¼‰è¶…è¿‡äº†ä½¿ç”¨å‡åŒ€é‡‡æ ·æ–¹æ³•è®­ç»ƒçš„æ‹¥æœ‰æ›´å¤šæ•°æ®ï¼ˆå¤šå‡º50%ï¼‰çš„æ¨¡å‹ã€‚æˆ‘ä»¬ç»§ç»­åˆ©ç”¨DataManæ ‡æ³¨çš„é«˜è´¨é‡ã€ç‰¹å®šé¢†åŸŸçš„è¯­æ–™è¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥æé«˜ç‰¹å®šé¢†åŸŸçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›å¹¶éªŒè¯DataMançš„é¢†åŸŸæ··åˆèƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†è´¨é‡æ’åçš„é‡è¦æ€§ä»¥åŠè´¨é‡æ ‡å‡†çš„äº’è¡¥æ€§åŠå…¶ä¸å›°æƒ‘åº¦çš„ä½ç›¸å…³æ€§ï¼ŒåŒæ—¶åˆ†æäº†å›°æƒ‘åº¦ä¸ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ä¹‹é—´çš„ä¸åŒ¹é…ç°è±¡ã€‚æˆ‘ä»¬è¿˜å¯¹æˆ‘ä»¬çš„é¢„è®­ç»ƒæ•°æ®é›†è¿›è¡Œäº†å…¨é¢çš„åˆ†æï¼Œè€ƒå¯Ÿäº†æ•°æ®é›†çš„ç»„æˆç»“æ„ã€è´¨é‡è¯„åˆ†çš„åˆ†å¸ƒæƒ…å†µä»¥åŠåŸå§‹æ–‡æ¡£æ¥æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.19363v2">PDF</a> ICLR2025 paper</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½æ¶Œç°ï¼Œä¾èµ–äºæ•°æ®è§„æ¨¡å®šå¾‹çš„é©±åŠ¨ï¼Œä½¿å¾—é¢„è®­ç»ƒæ•°æ®çš„é€‰æ‹©å˜å¾—è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºæœ‰é™çš„å¯å‘å¼æ–¹æ³•å’Œäººç±»ç›´è§‰ï¼Œç¼ºä¹å…¨é¢æ¸…æ™°çš„æŒ‡å¯¼æ–¹é’ˆã€‚æœ¬æ–‡å—â€œé€†å‘æ€ç»´â€å¯å‘ï¼Œä¿ƒä½¿LLMè‡ªæˆ‘è¯†åˆ«å“ªäº›æ ‡å‡†å¯¹å…¶æ€§èƒ½æœ‰ç›Šã€‚ä¸é¢„è®­ç»ƒèƒ½åŠ›ç›¸å…³çš„å›°æƒ‘åº¦ï¼ˆPPLï¼‰ï¼Œæˆ‘ä»¬ä»æ–‡æœ¬å›°æƒ‘åº¦å¼‚å¸¸çš„åŸå› ä¸­å¾—å‡º14ä¸ªè´¨é‡æ ‡å‡†å’Œå¼•å…¥çš„15ä¸ªå¸¸è§åº”ç”¨é¢†åŸŸæ¥æ”¯æŒé¢†åŸŸæ··åˆã€‚æœ¬æ–‡é€šè¿‡è®­ç»ƒä¸€ä¸ªæ•°æ®ç®¡ç†å™¨ï¼ˆDataManï¼‰æ¥å­¦ä¹ è´¨é‡è¯„ä¼°å’Œé¢†åŸŸè¯†åˆ«ï¼Œå¹¶ä½¿ç”¨å®ƒå¯¹ä¸€ä¸ªè§„æ¨¡ä¸º447Bä»¤ç‰Œçš„é¢„è®­ç»ƒè¯­æ–™åº“è¿›è¡Œæ³¨é‡Šã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼Œé€šè¿‡DataMané€‰å®šçš„é«˜è´¨é‡è®­ç»ƒè¯­æ–™å¯ä»¥æ˜¾è‘—æ”¹å–„æœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€å›°æƒ‘åº¦å’ŒæŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ã€‚åŸºäºæ€»ä½“è¯„åˆ†l&#x3D;5çš„æœ€ä½³æ€§èƒ½æ¨¡å‹è¶…è¿‡äº†ä½¿ç”¨å‡åŒ€é‡‡æ ·è®­ç»ƒçš„æ‹¥æœ‰æ›´å¤šæ•°æ®çš„æ¨¡å‹ã€‚é€šè¿‡DataManæ ‡æ³¨çš„é«˜è´¨é‡é¢†åŸŸç‰¹å®šæ•°æ®ç»§ç»­é¢„è®­ç»ƒï¼Œä»¥å¢å¼ºé¢†åŸŸç‰¹å®šçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›å¹¶éªŒè¯å…¶é¢†åŸŸæ··åˆèƒ½åŠ›ã€‚æœ¬æ–‡å¼ºè°ƒäº†è´¨é‡æ’åçš„é‡è¦æ€§ã€è´¨é‡æ ‡å‡†çš„äº’è¡¥æ€§ä»¥åŠå®ƒä»¬ä¸å›°æƒ‘åº¦çš„ä½ç›¸å…³æ€§ï¼Œåˆ†æäº†å›°æƒ‘åº¦ä¸ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ã€‚åŒæ—¶ï¼Œæœ¬æ–‡å¯¹é¢„è®­ç»ƒæ•°æ®é›†è¿›è¡Œäº†å…¨é¢çš„åˆ†æï¼ŒåŒ…æ‹¬å…¶æ„æˆã€è´¨é‡è¯„çº§çš„åˆ†å¸ƒä»¥åŠåŸå§‹æ–‡æ¡£æ¥æºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMæ€§èƒ½çš„æå‡ä¸æ•°æ®è§„æ¨¡ç›¸å…³ï¼Œä½¿å¾—é€‰æ‹©é«˜è´¨é‡çš„é¢„è®­ç»ƒæ•°æ®è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨é€‰æ‹©é¢„è®­ç»ƒæ•°æ®æ—¶ç¼ºä¹æ¸…æ™°çš„æŒ‡å¯¼æ–¹é’ˆï¼Œä¸»è¦ä¾èµ–å¯å‘å¼æ–¹æ³•å’Œäººç±»ç›´è§‰ã€‚</li>
<li>æœ¬æ–‡å—åˆ°â€œé€†å‘æ€ç»´â€å¯å‘ï¼Œè®©LLMè‡ªæˆ‘è¯†åˆ«å¯¹æ€§èƒ½æœ‰ç›Šçš„æ ‡å‡†ã€‚</li>
<li>å¼•å…¥åŸºäºå›°æƒ‘åº¦çš„è´¨é‡æ ‡å‡†æ¥è¯„ä¼°LLMçš„é¢„è®­ç»ƒæ•ˆæœã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªæ•°æ®ç®¡ç†å™¨ï¼ˆDataManï¼‰æ¥æ³¨é‡Šé¢„è®­ç»ƒè¯­æ–™åº“çš„è´¨é‡å’Œé¢†åŸŸåˆ†ç±»ã€‚ä½¿ç”¨è¿™ä¸€å·¥å…·å¯ä»¥æå‡LLMæ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€å›°æƒ‘åº¦å’ŒæŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ã€‚</li>
<li>æœ€ä½³æ€§èƒ½çš„æ¨¡å‹è¶…è¶Šäº†ä½¿ç”¨æ›´å¤šæ•°æ®ä½†é‡‡ç”¨å‡åŒ€é‡‡æ ·è®­ç»ƒçš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.19363">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4cbba117c694ebc399a87bed055ffc5c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a8660bf9f614b01461c87853a5e3c97.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Property-Enhanced-Instruction-Tuning-for-Multi-task-Molecule-Generation-with-Large-Language-Models"><a href="#Property-Enhanced-Instruction-Tuning-for-Multi-task-Molecule-Generation-with-Large-Language-Models" class="headerlink" title="Property Enhanced Instruction Tuning for Multi-task Molecule Generation   with Large Language Models"></a>Property Enhanced Instruction Tuning for Multi-task Molecule Generation   with Large Language Models</h2><p><strong>Authors:Xuan Lin, Long Chen, Yile Wang, Xiangxiang Zeng, Philip S. Yu</strong></p>
<p>Large language models (LLMs) are widely applied in various natural language processing tasks such as question answering and machine translation. However, due to the lack of labeled data and the difficulty of manual annotation for biochemical properties, the performance for molecule generation tasks is still limited, especially for tasks involving multi-properties constraints. In this work, we present a two-step framework PEIT (Property Enhanced Instruction Tuning) to improve LLMs for molecular-related tasks. In the first step, we use textual descriptions, SMILES, and biochemical properties as multimodal inputs to pre-train a model called PEIT-GEN, by aligning multi-modal representations to synthesize instruction data. In the second step, we fine-tune existing open-source LLMs with the synthesized data, the resulting PEIT-LLM can handle molecule captioning, text-based molecule generation, molecular property prediction, and our newly proposed multi-constraint molecule generation tasks. Experimental results show that our pre-trained PEIT-GEN outperforms MolT5 and BioT5 in molecule captioning, demonstrating modalities align well between textual descriptions, structures, and biochemical properties. Furthermore, PEIT-LLM shows promising improvements in multi-task molecule generation, proving the scalability of the PEIT framework for various molecular tasks. We release the code, constructed instruction data, and model checkpoints in <a target="_blank" rel="noopener" href="https://github.com/chenlong164/PEIT">https://github.com/chenlong164/PEIT</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¹¿æ³›åº”ç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚é—®ç­”å’Œæœºå™¨ç¿»è¯‘ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ ‡è®°æ•°æ®å’Œç”ŸåŒ–å±æ€§æ‰‹åŠ¨æ ‡æ³¨çš„å›°éš¾ï¼Œåˆ†å­ç”Ÿæˆä»»åŠ¡çš„æ•ˆæœä»ç„¶å—é™ï¼Œå°¤å…¶æ˜¯å¯¹äºæ¶‰åŠå¤šå±æ€§çº¦æŸçš„ä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤æ­¥æ¡†æ¶PEITï¼ˆå±æ€§å¢å¼ºæŒ‡ä»¤è°ƒæ•´ï¼‰æ¥æé«˜LLMåœ¨åˆ†å­ç›¸å…³ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬ä½¿ç”¨æ–‡æœ¬æè¿°ã€SMILESå’Œç”Ÿç‰©åŒ–å­¦å±æ€§ä½œä¸ºå¤šæ¨¡å¼è¾“å…¥æ¥é¢„è®­ç»ƒä¸€ä¸ªåä¸ºPEIT-GENçš„æ¨¡å‹ï¼Œé€šè¿‡å¯¹é½å¤šæ¨¡å¼è¡¨ç¤ºæ¥åˆæˆæŒ‡ä»¤æ•°æ®ã€‚åœ¨ç¬¬äºŒæ­¥ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨åˆæˆæ•°æ®å¯¹ç°æœ‰çš„å¼€æºLLMè¿›è¡Œå¾®è°ƒï¼Œå¾—åˆ°çš„PEIT-LLMå¯ä»¥å¤„ç†åˆ†å­æè¿°ã€åŸºäºæ–‡æœ¬çš„åˆ†å­ç”Ÿæˆã€åˆ†å­å±æ€§é¢„æµ‹ä»¥åŠæˆ‘ä»¬æ–°æå‡ºçš„å¤šçº¦æŸåˆ†å­ç”Ÿæˆä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒPEIT-GENåœ¨åˆ†å­æè¿°æ–¹é¢ä¼˜äºMolT5å’ŒBioT5ï¼Œè¯æ˜äº†æ–‡æœ¬æè¿°ã€ç»“æ„å’Œç”Ÿç‰©åŒ–å­¦å±æ€§ä¹‹é—´çš„æ¨¡æ€å¯¹é½è‰¯å¥½ã€‚æ­¤å¤–ï¼ŒPEIT-LLMåœ¨å¤šä»»åŠ¡åˆ†å­ç”Ÿæˆæ–¹é¢æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„æ”¹è¿›ï¼Œè¯æ˜äº†PEITæ¡†æ¶å¯¹å„ç§åˆ†å­ä»»åŠ¡çš„å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/chenlong164/PEIT">https://github.com/chenlong164/PEIT</a>ä¸Šå‘å¸ƒäº†ä»£ç ã€æ„å»ºå¥½çš„æŒ‡ä»¤æ•°æ®å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.18084v3">PDF</a> 9</p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†åœ¨åˆ†å­ç”Ÿæˆä»»åŠ¡ä¸Šçš„è¡¨ç°å—é™ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºPEITçš„ä¸¤æ­¥æ¡†æ¶ï¼Œä»¥æé«˜LLMåœ¨åˆ†å­ç›¸å…³ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚é¦–å…ˆä½¿ç”¨æ–‡æœ¬æè¿°ã€SMILESå’Œç”Ÿç‰©åŒ–å­¦å±æ€§ç­‰å¤šæ¨¡å¼è¾“å…¥è¿›è¡Œé¢„è®­ç»ƒï¼›å…¶æ¬¡ç”¨åˆæˆæ•°æ®å¾®è°ƒç°æœ‰å¼€æºLLMã€‚PEITæ¡†æ¶èƒ½å¤„ç†åˆ†å­æè¿°ã€æ–‡æœ¬åŸºç¡€åˆ†å­ç”Ÿæˆã€åˆ†å­å±æ€§é¢„æµ‹ä»¥åŠæ–°æå‡ºçš„å¤šçº¦æŸåˆ†å­ç”Ÿæˆä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé¢„è®­ç»ƒçš„PEIT-GENåœ¨åˆ†å­æè¿°ä¸Šä¼˜äºMolT5å’ŒBioT5ï¼Œè¯æ˜æ–‡æœ¬æè¿°ã€ç»“æ„å’Œç”Ÿç‰©åŒ–å­¦å±æ€§ä¹‹é—´çš„å¯¹é½è‰¯å¥½ã€‚åŒæ—¶ï¼ŒPEIT-LLMåœ¨å¤šä»»åŠ¡åˆ†å­ç”Ÿæˆä¸Šæ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„æ”¹è¿›ï¼Œè¯æ˜PEITæ¡†æ¶å¯¹å„ç§åˆ†å­ä»»åŠ¡çš„å¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨åˆ†å­ç”Ÿæˆä»»åŠ¡ä¸Šçš„è¡¨ç°å—é™äºç¼ºä¹æ ‡æ³¨æ•°æ®å’Œæ‰‹åŠ¨æ³¨é‡Šçš„å›°éš¾ã€‚</li>
<li>PEITæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜LLMsåœ¨åˆ†å­ç›¸å…³ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œä½¿ç”¨æ–‡æœ¬æè¿°ã€SMILESå’Œç”Ÿç‰©åŒ–å­¦å±æ€§ç­‰å¤šæ¨¡å¼è¾“å…¥åˆæˆæŒ‡ä»¤æ•°æ®ã€‚</li>
<li>PEIT-GENåœ¨åˆ†å­æè¿°ä¸Šä¼˜äºMolT5å’ŒBioT5ï¼Œæ˜¾ç¤ºä¸åŒæ¨¡æ€ä¹‹é—´çš„è‰¯å¥½å¯¹é½ã€‚</li>
<li>PEIT-LLMèƒ½å¤„ç†å¤šç§åˆ†å­ç›¸å…³ä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ†å­æè¿°ã€æ–‡æœ¬åŸºç¡€åˆ†å­ç”Ÿæˆã€åˆ†å­å±æ€§é¢„æµ‹ä»¥åŠå¤šçº¦æŸåˆ†å­ç”Ÿæˆã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†PEITæ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.18084">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e82b2a2c73dd0b57a0aa87a22a8e2d90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-431301f75dfab204d4322d959489b384.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c6e299ad16040444d5c2bb0c92c3a51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb2d9703cb52ff46f3f1b18e5846c929.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70ee1e58c3186455090ca950fb68190f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="QUART-Online-Latency-Free-Large-Multimodal-Language-Model-for-Quadruped-Robot-Learning"><a href="#QUART-Online-Latency-Free-Large-Multimodal-Language-Model-for-Quadruped-Robot-Learning" class="headerlink" title="QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped   Robot Learning"></a>QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped   Robot Learning</h2><p><strong>Authors:Xinyang Tong, Pengxiang Ding, Yiguo Fan, Donglin Wang, Wenjie Zhang, Can Cui, Mingyang Sun, Han Zhao, Hongyin Zhang, Yonghao Dang, Siteng Huang, Shangke Lyu</strong></p>
<p>This paper addresses the inherent inference latency challenges associated with deploying multimodal large language models (MLLM) in quadruped vision-language-action (QUAR-VLA) tasks. Our investigation reveals that conventional parameter reduction techniques ultimately impair the performance of the language foundation model during the action instruction tuning phase, making them unsuitable for this purpose. We introduce a novel latency-free quadruped MLLM model, dubbed QUART-Online, designed to enhance inference efficiency without degrading the performance of the language foundation model. By incorporating Action Chunk Discretization (ACD), we compress the original action representation space, mapping continuous action values onto a smaller set of discrete representative vectors while preserving critical information. Subsequently, we fine-tune the MLLM to integrate vision, language, and compressed actions into a unified semantic space. Experimental results demonstrate that QUART-Online operates in tandem with the existing MLLM system, achieving real-time inference in sync with the underlying controller frequency, significantly boosting the success rate across various tasks by 65%. Our project page is <a target="_blank" rel="noopener" href="https://quart-online.github.io/">https://quart-online.github.io</a>. </p>
<blockquote>
<p>æœ¬æ–‡å…³æ³¨åœ¨å¤šè¶³è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆQUAR-VLAï¼‰ä»»åŠ¡ä¸­éƒ¨ç½²å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ‰€é¢ä¸´çš„å›ºæœ‰æ¨ç†å»¶è¿ŸæŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥å‘ç°ï¼Œä¼ ç»Ÿçš„å‚æ•°å‡å°‘æŠ€æœ¯æœ€ç»ˆä¼šæŸå®³è¯­è¨€åŸºç¡€æ¨¡å‹åœ¨åŠ¨ä½œæŒ‡ä»¤è°ƒæ•´é˜¶æ®µçš„æ€§èƒ½ï¼Œä½¿å…¶ä¸é€‚åˆæ­¤ç›®çš„ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ— å»¶è¿Ÿå¤šè¶³MLLMæ¨¡å‹ï¼Œåä¸ºQUART-Onlineï¼Œæ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¸é™ä½è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥åŠ¨ä½œå—ç¦»æ•£åŒ–ï¼ˆACDï¼‰ï¼Œæˆ‘ä»¬å‹ç¼©äº†åŸå§‹åŠ¨ä½œè¡¨ç¤ºç©ºé—´ï¼Œå°†è¿ç»­åŠ¨ä½œå€¼æ˜ å°„åˆ°ä¸€ç»„è¾ƒå°çš„ç¦»æ•£ä»£è¡¨å‘é‡ä¸Šï¼ŒåŒæ—¶ä¿ç•™å…³é”®ä¿¡æ¯ã€‚éšåï¼Œæˆ‘ä»¬å¯¹MLLMè¿›è¡Œå¾®è°ƒï¼Œä»¥å°†è§†è§‰ã€è¯­è¨€å’Œå‹ç¼©åŠ¨ä½œé›†æˆåˆ°ç»Ÿä¸€çš„è¯­ä¹‰ç©ºé—´ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQUART-Onlineä¸ç°æœ‰çš„MLLMç³»ç»ŸååŒå·¥ä½œï¼Œå®ç°ä¸åº•å±‚æ§åˆ¶å™¨é¢‘ç‡åŒæ­¥çš„å®æ—¶æ¨ç†ï¼Œåœ¨å„ç§ä»»åŠ¡ä¸­çš„æˆåŠŸç‡æé«˜äº†65%ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯<a target="_blank" rel="noopener" href="https://quart-online.github.io./">https://quart-online.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.15576v3">PDF</a> Accepted to ICRA 2025; Github page: <a target="_blank" rel="noopener" href="https://quart-online.github.io/">https://quart-online.github.io</a></p>
<p><strong>Summary</strong><br>    è¯¥è®ºæ–‡è§£å†³åœ¨å››è¶³æœºå™¨äººè§†è§‰è¯­è¨€åŠ¨ä½œä»»åŠ¡ï¼ˆQUAR-VLAï¼‰ä¸­éƒ¨ç½²å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ—¶é¢ä¸´çš„å›ºæœ‰æ¨ç†å»¶è¿ŸæŒ‘æˆ˜ã€‚ç ”ç©¶å¼•å…¥äº†æ–°å‹æ— å»¶è¿Ÿå››è¶³MLLMæ¨¡å‹QUART-Onlineï¼Œæ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡åŒæ—¶ä¸é™ä½è¯­è¨€åŸºç¡€æ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡é‡‡ç”¨åŠ¨ä½œå—ç¦»æ•£åŒ–ï¼ˆACDï¼‰ï¼Œå‹ç¼©åŸå§‹åŠ¨ä½œè¡¨ç¤ºç©ºé—´ï¼Œå°†è¿ç»­åŠ¨ä½œå€¼æ˜ å°„åˆ°ä¸€ç»„è¾ƒå°çš„ç¦»æ•£ä»£è¡¨å‘é‡ä¸Šï¼ŒåŒæ—¶ä¿ç•™å…³é”®ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQUART-Onlineä¸ç°æœ‰MLLMç³»ç»ŸååŒå·¥ä½œï¼Œå®ç°äº†å®æ—¶æ¨ç†ï¼Œä¸åº•å±‚æ§åˆ¶å™¨é¢‘ç‡åŒæ­¥ï¼ŒæˆåŠŸæé«˜äº†å„ç§ä»»åŠ¡çš„æˆåŠŸç‡è¾¾65%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡èšç„¦äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å››è¶³æœºå™¨äººè§†è§‰è¯­è¨€åŠ¨ä½œä»»åŠ¡ä¸­çš„æ¨ç†å»¶è¿Ÿé—®é¢˜ã€‚</li>
<li>ä¼ ç»Ÿå‚æ•°å‡å°‘æŠ€æœ¯ä¼šé™ä½è¯­è¨€åŸºç¡€æ¨¡å‹åœ¨åŠ¨ä½œæŒ‡ä»¤è°ƒæ•´é˜¶æ®µçš„æ€§èƒ½ï¼Œä¸é€‚åˆç”¨äºæ­¤åœºæ™¯ã€‚</li>
<li>QUART-Onlineæ¨¡å‹æ—¨åœ¨æé«˜æ¨ç†æ•ˆç‡è€Œä¸æŸå®³è¯­è¨€åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡åŠ¨ä½œå—ç¦»æ•£åŒ–æŠ€æœ¯å‹ç¼©åŠ¨ä½œè¡¨ç¤ºç©ºé—´ï¼Œå®ç°å…³é”®ä¿¡æ¯çš„ä¿ç•™å’Œè¿ç»­åŠ¨ä½œå€¼çš„ç¦»æ•£åŒ–æ˜ å°„ã€‚</li>
<li>QUART-Onlineä¸ç°æœ‰MLLMç³»ç»ŸååŒå·¥ä½œï¼Œå®ç°äº†å®æ—¶æ¨ç†ã€‚</li>
<li>æ¨¡å‹æˆåŠŸæé«˜äº†å„ç§ä»»åŠ¡çš„æˆåŠŸç‡è¾¾65%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.15576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6a460f7f1407380f91ed6c5744bd3e4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9587b2ec63f4522f9b93e86cd333bbca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f0f512213c2d131bfc2dfdba8e8798d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8eacc8e2eab7d6a93a4845c1715bfba8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-433f563f11014a70cad7974ef854750f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Causal-World-Model-Underlying-Next-Token-Prediction-in-GPT"><a href="#A-Causal-World-Model-Underlying-Next-Token-Prediction-in-GPT" class="headerlink" title="A Causal World Model Underlying Next Token Prediction in GPT"></a>A Causal World Model Underlying Next Token Prediction in GPT</h2><p><strong>Authors:Raanan Y. Rohekar, Yaniv Gurwicz, Sungduk Yu, Estelle Aflalo, Vasudev Lal</strong></p>
<p>Are generative pre-trained transformer (GPT) models only trained to predict the next token, or do they implicitly learn a world model from which a sequence is generated one token at a time? We examine this question by deriving a causal interpretation of the attention mechanism in GPT, and suggesting a causal world model that arises from this interpretation. Furthermore, we propose that GPT-models, at inference time, can be utilized for zero-shot causal structure learning for in-distribution sequences. Empirical evaluation is conducted in a controlled synthetic environment using the setup and rules of the Othello board game. A GPT, pre-trained on real-world games played with the intention of winning, is tested on synthetic data that only adheres to the game rules, oblivious to the goal of winning. We find that the GPT model is likely to generate moves that adhere to the game rules for sequences for which a causal structure is encoded in the attention mechanism with high confidence. In general, in cases for which the GPT model generates moves that do not adhere to the game rules, it also fails to capture any causal structure. </p>
<blockquote>
<p>GPTé¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹æ˜¯å¦ä»…ç»è¿‡è®­ç»ƒä»¥é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œï¼Œè¿˜æ˜¯å®ƒä»¬ä¼šéšå¼å­¦ä¹ ä¸€ä¸ªä¸–ç•Œæ¨¡å‹ï¼Œä»è¿™ä¸ªæ¨¡å‹ä¸­æŒ‰æ¬¡åºç”Ÿæˆåºåˆ—çš„æ¯ä¸ªä»¤ç‰Œï¼Ÿæˆ‘ä»¬é€šè¿‡æ¨å¯¼GPTä¸­æ³¨æ„åŠ›æœºåˆ¶çš„å› æœè§£é‡Šæ¥å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œå¹¶æå‡ºç”±æ­¤äº§ç”Ÿçš„å› æœä¸–ç•Œæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºï¼Œåœ¨æ¨ç†é˜¶æ®µï¼ŒGPTæ¨¡å‹å¯ç”¨äºè¿›è¡Œé›¶åŸºç¡€å› æœç»“æ„å­¦ä¹ ä»¥ç”Ÿæˆå†…éƒ¨åˆ†å¸ƒåºåˆ—ã€‚æˆ‘ä»¬åœ¨Othelloæ¸¸æˆçš„æ§åˆ¶åˆæˆç¯å¢ƒä¸­è¿›è¡Œå®è¯ç ”ç©¶ï¼Œä»¥æµ‹è¯•å’Œè¯„ä¼°GPTçš„èƒ½åŠ›è¡¨ç°ã€‚æ­¤æ¸¸æˆä¸­çš„GPTé¢„è®­ç»ƒé‡‡ç”¨äº†åœ¨çœŸå®ä¸–ç•Œä¸­èµ¢å¾—æ¯”èµ›çš„ç­–ç•¥å’Œç›®æ ‡ã€‚æˆ‘ä»¬å¯¹ä»…éµå¾ªæ¸¸æˆè§„åˆ™ä¸”å¿½ç•¥è·èƒœç›®æ ‡çš„åˆæˆæ•°æ®è¿›è¡Œæµ‹è¯•ã€‚æˆ‘ä»¬å‘ç°ï¼ŒGPTæ¨¡å‹åœ¨ç”Ÿæˆéµå¾ªæ¸¸æˆè§„åˆ™åºåˆ—çš„å›åˆæ—¶éå¸¸æ“…é•¿å¤„ç†å«æœ‰é«˜ä¿¡å¿ƒå› æœç»“æ„çš„ä»¤ç‰Œã€‚æ€»è€Œè¨€ä¹‹ï¼Œå½“GPTæ¨¡å‹ç”Ÿæˆçš„æ¸¸æˆè§„åˆ™ä¸ç¬¦åŠ¨ä½œå›åˆæ—¶ï¼Œå®ƒä¼šå®Œå…¨å¿½ç•¥ä»»ä½•å› æœç»“æ„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07446v2">PDF</a> AAAI 2025 Workshop on Artificial Intelligence with Causal Techniques</p>
<p><strong>Summary</strong>ï¼š<br>GPTæ¨¡å‹æ˜¯å¦ä»…é€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œè¿›è¡Œè®­ç»ƒï¼Œè¿˜æ˜¯å®ƒä»¬ä¼šéšå¼åœ°å­¦ä¹ ä¸€ä¸ªä¸–ç•Œæ¨¡å‹ï¼Œä»è€Œé€ä¸ªç”Ÿæˆåºåˆ—ï¼Ÿæœ¬æ–‡é€šè¿‡æ¨å¯¼GPTä¸­æ³¨æ„åŠ›æœºåˆ¶çš„å› æœè§£é‡Šï¼Œæå‡ºäº†ç”±æ­¤äº§ç”Ÿçš„å› æœä¸–ç•Œæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºGPTæ¨¡å‹åœ¨æ¨ç†æ—¶å¯ç”¨äºé›¶å°„å› æœç»“æ„å­¦ä¹ ï¼Œç”¨äºç”Ÿæˆç¬¦åˆåˆ†å¸ƒåºåˆ—çš„åºåˆ—ã€‚åœ¨Othelloæ¸¸æˆçš„å—æ§åˆæˆç¯å¢ƒä¸­è¿›è¡Œäº†å®è¯è¯„ä¼°ã€‚ç»è¿‡åœ¨çœŸå®ä¸–ç•Œæ¸¸æˆä¸­é¢„è®­ç»ƒçš„GPTæ¨¡å‹ï¼Œå¯¹åˆæˆæ•°æ®çš„æµ‹è¯•ä»…éµå¾ªæ¸¸æˆè§„åˆ™ï¼Œè€Œå¿½ç•¥äº†è·èƒœçš„ç›®æ ‡ã€‚æˆ‘ä»¬å‘ç°GPTæ¨¡å‹å¾ˆå¯èƒ½ç”Ÿæˆç¬¦åˆæ¸¸æˆè§„åˆ™çš„è¡ŒåŠ¨åºåˆ—ï¼Œå…¶ä¸­æ³¨æ„åŠ›æœºåˆ¶ç¼–ç äº†é«˜ç½®ä¿¡åº¦çš„å› æœç»“æ„ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœGPTæ¨¡å‹ç”Ÿæˆçš„è¡ŒåŠ¨ä¸ç¬¦åˆæ¸¸æˆè§„åˆ™ï¼Œå®ƒä¹Ÿæ— æ³•æ•æ‰åˆ°ä»»ä½•å› æœç»“æ„ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>GPTæ¨¡å‹ä¸ä»…é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œï¼Œè¿˜éšå¼åœ°å­¦ä¹ ä¸€ä¸ªä¸–ç•Œæ¨¡å‹ã€‚</li>
<li>é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶çš„å› æœè§£é‡Šï¼Œæå‡ºäº†å› æœä¸–ç•Œæ¨¡å‹ã€‚</li>
<li>GPTæ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯ç”¨äºé›¶å°„å› æœç»“æ„å­¦ä¹ ã€‚</li>
<li>åœ¨Othelloæ¸¸æˆçš„å—æ§ç¯å¢ƒä¸­è¯„ä¼°äº†GPTæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>GPTæ¨¡å‹åœ¨éµå¾ªæ¸¸æˆè§„åˆ™çš„åˆæˆæ•°æ®æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>GPTæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆç¬¦åˆæ¸¸æˆè§„åˆ™çš„è¡ŒåŠ¨åºåˆ—ï¼Œè¿™å¾—ç›Šäºæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„é«˜ç½®ä¿¡åº¦å› æœç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f5ae3315e5d45c62e7f91f0f8e72d1d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85edd2595c9011bb4bc1c0312716da1c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-847b8bdb0e466fb7c60be69480966409.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-519d5369ef24c2d4d8d876068977f2e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8eac38080baaa8521b008cd07fc2466b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="TED-VITON-Transformer-Empowered-Diffusion-Models-for-Virtual-Try-On"><a href="#TED-VITON-Transformer-Empowered-Diffusion-Models-for-Virtual-Try-On" class="headerlink" title="TED-VITON: Transformer-Empowered Diffusion Models for Virtual Try-On"></a>TED-VITON: Transformer-Empowered Diffusion Models for Virtual Try-On</h2><p><strong>Authors:Zhenchen Wan, Yanwu Xu, Zhaoqing Wang, Feng Liu, Tongliang Liu, Mingming Gong</strong></p>
<p>Recent advancements in Virtual Try-On (VTO) have demonstrated exceptional efficacy in generating realistic images and preserving garment details, largely attributed to the robust generative capabilities of text-to-image (T2I) diffusion backbones. However, the T2I models that underpin these methods have become outdated, thereby limiting the potential for further improvement in VTO. Additionally, current methods face notable challenges in accurately rendering text on garments without distortion and preserving fine-grained details, such as textures and material fidelity. The emergence of Diffusion Transformer (DiT) based T2I models has showcased impressive performance and offers a promising opportunity for advancing VTO. Directly applying existing VTO techniques to transformer-based T2I models is ineffective due to substantial architectural differences, which hinder their ability to fully leverage the modelsâ€™ advanced capabilities for improved text generation. To address these challenges and unlock the full potential of DiT-based T2I models for VTO, we propose TED-VITON, a novel framework that integrates a Garment Semantic (GS) Adapter for enhancing garment-specific features, a Text Preservation Loss to ensure accurate and distortion-free text rendering, and a constraint mechanism to generate prompts by optimizing Large Language Model (LLM). These innovations enable state-of-the-art (SOTA) performance in visual quality and text fidelity, establishing a new benchmark for VTO task. Project page: <a target="_blank" rel="noopener" href="https://zhenchenwan.github.io/TED-VITON/">https://zhenchenwan.github.io/TED-VITON/</a> </p>
<blockquote>
<p>æœ€è¿‘è™šæ‹Ÿè¯•ç©¿ï¼ˆVTOï¼‰çš„è¿›å±•è¡¨æ˜ï¼Œåœ¨ç”Ÿæˆé€¼çœŸå›¾åƒå’Œä¿ç•™æœè£…ç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ•ˆåŠ›ï¼Œè¿™ä¸»è¦å½’åŠŸäºæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£èƒŒéª¨çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ”¯æ’‘è¿™äº›æ–¹æ³•æ‰€é‡‡ç”¨çš„T2Iæ¨¡å‹å·²ç»è¿‡æ—¶ï¼Œä»è€Œé™åˆ¶äº†VTOçš„è¿›ä¸€æ­¥æ”¹è¿›æ½œåŠ›ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ–¹æ³•åœ¨å°†æ–‡æœ¬å‡†ç¡®æ¸²æŸ“åˆ°æœè£…ä¸Šå¹¶ä¿æŒç²¾ç»†ç»†èŠ‚ï¼ˆå¦‚çº¹ç†å’Œææ–™ä¿çœŸåº¦ï¼‰æ–¹é¢é¢ä¸´æ˜¾è‘—æŒ‘æˆ˜ã€‚åŸºäºæ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰çš„T2Iæ¨¡å‹çš„å…´èµ·è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ï¼Œå¹¶ä¸ºæ¨è¿›VTOæä¾›äº†å¯Œæœ‰å¸Œæœ›çš„æœºä¼šã€‚ç”±äºç°æœ‰çš„VTOæŠ€æœ¯åœ¨ç»“æ„ä¸Šå­˜åœ¨è¾ƒå¤§å·®å¼‚ï¼Œç›´æ¥å°†å…¶åº”ç”¨äºåŸºäºtransformerçš„T2Iæ¨¡å‹æ•ˆæœä¸ä½³ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬å……åˆ†åˆ©ç”¨æ¨¡å‹è¿›è¡Œæ”¹è¿›æ–‡æœ¬ç”Ÿæˆçš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜å¹¶é‡Šæ”¾åŸºäºDiTçš„T2Iæ¨¡å‹åœ¨VTOæ–¹é¢çš„æ½œåŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†TED-VITONè¿™ä¸€å…¨æ–°æ¡†æ¶ã€‚å®ƒé›†æˆäº†æœè£…è¯­ä¹‰ï¼ˆGSï¼‰é€‚é…å™¨ä»¥å¢å¼ºæœè£…ç‰¹å®šç‰¹å¾ã€æ–‡æœ¬ä¿ç•™æŸå¤±ä»¥ç¡®ä¿å‡†ç¡®ä¸”æ— å¤±çœŸæ–‡æœ¬æ¸²æŸ“ï¼Œä»¥åŠä¸€ç§çº¦æŸæœºåˆ¶ï¼Œé€šè¿‡ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ç”Ÿæˆæç¤ºã€‚è¿™äº›åˆ›æ–°ä½¿æˆ‘ä»¬åœ¨è§†è§‰è´¨é‡å’Œæ–‡æœ¬ä¿çœŸåº¦æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œä¸ºVTOä»»åŠ¡å»ºç«‹äº†æ–°çš„åŸºå‡†ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://zhenchenwan.github.io/TED-VITON/">TED-VITONé“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17017v3">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://github.com/ZhenchenWan/TED-VITON">https://github.com/ZhenchenWan/TED-VITON</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ€æ–°çš„è™šæ‹Ÿè¯•ç©¿ï¼ˆVTOï¼‰æŠ€æœ¯è¿›å±•æ˜¾è‘—ï¼Œèƒ½ç”Ÿæˆé€¼çœŸçš„å›¾åƒå¹¶ä¿ç•™æœè£…ç»†èŠ‚ï¼Œä¸»è¦å¾—ç›Šäºæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£èƒŒéª¨çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ”¯æ’‘è¿™äº›æ–¹æ³•çš„T2Iæ¨¡å‹å·²è¿‡æ—¶ï¼Œé™åˆ¶äº†VTOçš„è¿›ä¸€æ­¥æ”¹è¿›æ½œåŠ›ã€‚æ­¤å¤–ï¼Œå½“å‰æ–¹æ³•åœ¨å‡†ç¡®æ¸²æŸ“æœè£…ä¸Šçš„æ–‡æœ¬ã€é¿å…å¤±çœŸä»¥åŠä¿ç•™ç²¾ç»†çº¹ç†å’Œæè´¨ä¿çœŸåº¦æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚åŸºäºæ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰çš„T2Iæ¨¡å‹çš„å‡ºç°å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ï¼Œä¸ºVTOçš„è¿›å±•æä¾›äº†æœ‰å‰æ™¯çš„æœºä¼šã€‚ç”±äºç°æœ‰VTOæŠ€æœ¯ç›´æ¥åº”ç”¨äºåŸºäºå˜å‹å™¨çš„T2Iæ¨¡å‹å­˜åœ¨æ¶æ„å·®å¼‚ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å…¶å…ˆè¿›åŠŸèƒ½æ¥æ”¹å–„æ–‡æœ¬ç”Ÿæˆã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè§£é”DiTåŸºäºT2Iæ¨¡å‹çš„VTOæ½œåŠ›ï¼Œæˆ‘ä»¬æå‡ºTED-VITONæ¡†æ¶ï¼Œé›†æˆæœè£…è¯­ä¹‰ï¼ˆGSï¼‰é€‚é…å™¨ä»¥å¢å¼ºæœè£…ç‰¹å®šç‰¹å¾ã€æ–‡æœ¬ä¿ç•™æŸå¤±ä»¥ç¡®ä¿å‡†ç¡®ä¸”æ— å¤±çœŸçš„æ–‡æœ¬æ¸²æŸ“ï¼Œä»¥åŠé€šè¿‡ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆæç¤ºçš„çº¦æŸæœºåˆ¶ã€‚è¿™äº›åˆ›æ–°åœ¨è§†è§‰è´¨é‡å’Œæ–‡æœ¬å¿ å®åº¦æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œä¸ºVTOä»»åŠ¡å»ºç«‹äº†æ–°åŸºå‡†ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è™šæ‹Ÿè¯•ç©¿ï¼ˆVTOï¼‰æŠ€æœ¯æœ€æ–°è¿›å±•å¾—ç›Šäºæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>å½“å‰VTOæŠ€æœ¯é¢ä¸´æ¸²æŸ“æœè£…ç»†èŠ‚å’Œæ–‡æœ¬å‡†ç¡®æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>åŸºäºæ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰çš„T2Iæ¨¡å‹å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸ºVTOå‘å±•å¸¦æ¥å¸Œæœ›ã€‚</li>
<li>ç›´æ¥åº”ç”¨ç°æœ‰VTOæŠ€æœ¯äºDiTæ¨¡å‹æ— æ•ˆï¼Œå› ä¸¤è€…æ¶æ„å·®å¼‚ã€‚</li>
<li>TED-VITONæ¡†æ¶æ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œé›†æˆäº†æœè£…è¯­ä¹‰é€‚é…å™¨ã€æ–‡æœ¬ä¿ç•™æŸå¤±å’Œå¤§å‹è¯­è¨€æ¨¡å‹ä¼˜åŒ–æœºåˆ¶ã€‚</li>
<li>TED-VITONæ¡†æ¶åœ¨è§†è§‰è´¨é‡å’Œæ–‡æœ¬å¿ å®åº¦æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
<li>é¡¹ç›®é¡µé¢æä¾›äº†æ›´å¤šå…³äºTED-VITONæ¡†æ¶çš„è¯¦ç»†ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17017">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5f2816198db50b600856e00dba43629a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-624981cbbb2b2ec4bc338e6720c90618.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd2932cabf207839a175207d5a120d38.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Memorization-in-Attention-only-Transformers"><a href="#Memorization-in-Attention-only-Transformers" class="headerlink" title="Memorization in Attention-only Transformers"></a>Memorization in Attention-only Transformers</h2><p><strong>Authors:LÃ©o Dana, Muni Sreenivas Pydi, Yann Chevaleyre</strong></p>
<p>Recent research has explored the memorization capacity of multi-head attention, but these findings are constrained by unrealistic limitations on the context size. We present a novel proof for language-based Transformers that extends the current hypothesis to any context size. Our approach improves upon the state-of-the-art by achieving more effective exact memorization with an attention layer, while also introducing the concept of approximate memorization of distributions. Through experimental validation, we demonstrate that our proposed bounds more accurately reflect the true memorization capacity of language models, and provide a precise comparison with prior work. </p>
<blockquote>
<p>è¿‘æœŸç ”ç©¶å·²ç»æ¢ç´¢äº†å¤šå¤´æ³¨æ„åŠ›çš„è®°å¿†èƒ½åŠ›ï¼Œä½†è¿™äº›å‘ç°å—åˆ°ä¸Šä¸‹æ–‡å¤§å°ä¸ç°å®çš„é™åˆ¶ã€‚æˆ‘ä»¬ä¸ºåŸºäºè¯­è¨€çš„Transformeræå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå°†å½“å‰å‡è®¾æ‰©å±•åˆ°ä»»ä½•ä¸Šä¸‹æ–‡å¤§å°ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ³¨æ„åŠ›å±‚å®ç°äº†æ›´æœ‰æ•ˆçš„ç²¾ç¡®è®°å¿†ï¼ŒåŒæ—¶å¼•å…¥äº†åˆ†å¸ƒè¿‘ä¼¼è®°å¿†çš„æ¦‚å¿µï¼Œä»è€Œæ”¹è¿›äº†å½“å‰å…ˆè¿›æŠ€æœ¯ã€‚é€šè¿‡å®éªŒéªŒè¯ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬æå‡ºçš„ç•Œé™æ›´å‡†ç¡®åœ°åæ˜ äº†è¯­è¨€æ¨¡å‹çš„çœŸå®è®°å¿†èƒ½åŠ›ï¼Œå¹¶ä¸å…ˆå‰çš„å·¥ä½œè¿›è¡Œäº†ç²¾ç¡®æ¯”è¾ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10115v2">PDF</a> 16 pages, 6 figures, submitted to AISTATS 2025,</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç ”ç©¶æ¢è®¨äº†å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„è®°å¿†èƒ½åŠ›ï¼Œä½†å—é™äºä¸Šä¸‹æ–‡å¤§å°çš„éç°å®é™åˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹è¯­è¨€Transformerçš„æ–°è¯æ˜æ–¹æ³•ï¼Œå°†å½“å‰å‡è®¾æ‰©å±•åˆ°ä»»ä½•ä¸Šä¸‹æ–‡å¤§å°ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ³¨æ„åŠ›å±‚å®ç°äº†æ›´æœ‰æ•ˆçš„ç²¾ç¡®è®°å¿†ï¼Œå¹¶å¼•å…¥äº†è¿‘ä¼¼è®°å¿†åˆ†å¸ƒçš„æ¦‚å¿µã€‚å®éªŒéªŒè¯æ˜¾ç¤ºï¼Œæˆ‘ä»¬æå‡ºçš„è¾¹ç•Œæ›´å‡†ç¡®åœ°åæ˜ äº†è¯­è¨€æ¨¡å‹çš„çœŸå®è®°å¿†èƒ½åŠ›ï¼Œå¹¶ä¸å…ˆå‰çš„å·¥ä½œè¿›è¡Œäº†ç²¾ç¡®æ¯”è¾ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¯¹å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„è®°å¿†èƒ½åŠ›è¿›è¡Œäº†æ¢ç´¢ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å—é™äºä¸Šä¸‹æ–‡å¤§å°çš„éç°å®é™åˆ¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„é’ˆå¯¹è¯­è¨€Transformerçš„è¯æ˜æ–¹æ³•ï¼Œå°†å‡è®¾æ‰©å±•åˆ°ä»»ä½•ä¸Šä¸‹æ–‡å¤§å°ã€‚</li>
<li>æ–¹æ³•å®ç°äº†æ›´æœ‰æ•ˆçš„ç²¾ç¡®è®°å¿†ï¼Œå¹¶å¼•å…¥äº†è¿‘ä¼¼è®°å¿†åˆ†å¸ƒçš„æ¦‚å¿µã€‚</li>
<li>å®éªŒéªŒè¯æ˜¾ç¤ºï¼Œæ–°æå‡ºçš„è¾¹ç•Œæ›´å‡†ç¡®åœ°åæ˜ äº†è¯­è¨€æ¨¡å‹çš„è®°å¿†èƒ½åŠ›ã€‚</li>
<li>ä¸å…ˆå‰çš„ç ”ç©¶ç›¸æ¯”ï¼Œæ–°æ–¹æ³•æä¾›äº†æ›´ç²¾ç¡®çš„æ¯”è¾ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10115">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0bac5151f9d97871c01d5990837325f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f88aa989e45a9ba33ecf84291636a074.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb5b72417aac8c5505d3f345964b8656.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="YouTube-Comments-Decoded-Leveraging-LLMs-for-Low-Resource-Language-Classification"><a href="#YouTube-Comments-Decoded-Leveraging-LLMs-for-Low-Resource-Language-Classification" class="headerlink" title="YouTube Comments Decoded: Leveraging LLMs for Low Resource Language   Classification"></a>YouTube Comments Decoded: Leveraging LLMs for Low Resource Language   Classification</h2><p><strong>Authors:Aniket Deroy, Subhankar Maity</strong></p>
<p>Sarcasm detection is a significant challenge in sentiment analysis, particularly due to its nature of conveying opinions where the intended meaning deviates from the literal expression. This challenge is heightened in social media contexts where code-mixing, especially in Dravidian languages, is prevalent. Code-mixing involves the blending of multiple languages within a single utterance, often with non-native scripts, complicating the task for systems trained on monolingual data. This shared task introduces a novel gold standard corpus designed for sarcasm and sentiment detection within code-mixed texts, specifically in Tamil-English and Malayalam-English languages. The primary objective of this task is to identify sarcasm and sentiment polarity within a code-mixed dataset of Tamil-English and Malayalam-English comments and posts collected from social media platforms. Each comment or post is annotated at the message level for sentiment polarity, with particular attention to the challenges posed by class imbalance, reflecting real-world scenarios.In this work, we experiment with state-of-the-art large language models like GPT-3.5 Turbo via prompting to classify comments into sarcastic or non-sarcastic categories. We obtained a macro-F1 score of 0.61 for Tamil language. We obtained a macro-F1 score of 0.50 for Malayalam language. </p>
<blockquote>
<p>è®½åˆºæ£€æµ‹æ˜¯æƒ…æ„Ÿåˆ†æä¸­çš„ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å› ä¸ºå…¶ä¼ è¾¾æ„è§çš„æœ¬è´¨ï¼Œå³æ‰€è¡¨è¾¾çš„æ„ä¹‰ä¸å­—é¢æ„æ€å­˜åœ¨åå·®ã€‚åœ¨ç¤¾ä¼šåª’ä½“è¯­å¢ƒä¸­ï¼Œè¿™ä¸ªæŒ‘æˆ˜æ›´ä¸ºçªå‡ºï¼Œå…¶ä¸­æ··ç”¨è¯­è¨€ç°è±¡å°¤ä¸ºæ™®éï¼Œå°¤å…¶æ˜¯åœ¨å¾·æ‹‰ç»´è¿ªäºšè¯­è¨€ä¸­ã€‚è¯­è¨€æ··ç”¨æ¶‰åŠåœ¨ä¸€ä¸ªå•ä¸€çš„è¨€è¯­ä¸­æ··åˆå¤šç§è¯­è¨€ï¼Œé€šå¸¸å¸¦æœ‰éæ¯è¯­è„šæœ¬ï¼Œä¸ºé‚£äº›ç»è¿‡å•è¯­æ•°æ®è®­ç»ƒçš„ç³»ç»Ÿå¢åŠ äº†ä»»åŠ¡å¤æ‚æ€§ã€‚æœ¬æ¬¡å…±äº«ä»»åŠ¡å¼•å…¥äº†ä¸€ä¸ªæ–°å‹çš„é‡‘æ ‡å‡†è¯­æ–™åº“ï¼Œè¯¥è¯­æ–™åº“æ—¨åœ¨ç”¨äºæ··ç”¨è¯­è¨€æ–‡æœ¬ä¸­çš„è®½åˆºå’Œæƒ…æ„Ÿæ£€æµ‹ï¼Œç‰¹åˆ«æ˜¯åœ¨æ³°ç±³å°”è¯­-è‹±è¯­å’Œé©¬æ‹‰é›…æ‹‰å§†è¯­-è‹±è¯­ä¸­ã€‚è¯¥ä»»åŠ¡çš„ä¸»è¦ç›®æ ‡æ˜¯è¯†åˆ«ç¤¾äº¤åª’ä½“å¹³å°ä¸Šæ”¶é›†çš„æ³°ç±³å°”è¯­-è‹±è¯­å’Œé©¬æ‹‰é›…æ‹‰å§†è¯­-è‹±è¯­æ··åˆæ•°æ®é›†ä¸­çš„è®½åˆºå’Œæƒ…æ„Ÿææ€§ã€‚æ¯æ¡è¯„è®ºæˆ–å¸–å­éƒ½åœ¨æ¶ˆæ¯çº§åˆ«è¿›è¡Œæƒ…æ„Ÿææ€§æ³¨é‡Šï¼Œç‰¹åˆ«å…³æ³¨ç±»åˆ«ä¸å¹³è¡¡æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œåæ˜ çœŸå®ä¸–ç•Œåœºæ™¯ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æç¤ºä¸æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-3.5 Turboï¼‰è¿›è¡Œå®éªŒï¼Œå°†è¯„è®ºåˆ†ç±»ä¸ºè®½åˆºæˆ–éè®½åˆºç±»åˆ«ã€‚æˆ‘ä»¬ä¸ºæ³°ç±³å°”è¯­è·å¾—äº†0.61çš„å®è§‚F1åˆ†æ•°ã€‚ä¸ºé©¬æ‹‰é›…æ‹‰å§†è¯­è·å¾—äº†0.50çš„å®è§‚F1åˆ†æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.05039v2">PDF</a> Updated and Final Version</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨æƒ…æ„Ÿåˆ†æä¸­æ£€æµ‹è®½åˆºçš„éš¾ç‚¹ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¤¾äº¤åª’ä½“ç¯å¢ƒä¸­è¯­è¨€æ··æ‚çš„æƒ…å¢ƒä¸‹æ›´åŠ å›°éš¾ã€‚æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ç§æ–°å‹é‡‘æ ‡å‡†è¯­æ–™åº“ï¼Œç”¨äºè¯†åˆ«æ··åˆä»£ç æ–‡æœ¬ä¸­çš„è®½åˆºå’Œæƒ…æ„Ÿææ€§ï¼Œå°¤å…¶æ˜¯æ³°ç±³å°”è¯­å’Œé©¬æ‹‰é›…æ‹‰å§†è¯­ä¸è‹±è¯­æ··åˆçš„æƒ…å†µã€‚æœ¬ç ”ç©¶ä½¿ç”¨å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹GPT-3.5 Turboè¿›è¡Œåˆ†ç±»å®éªŒï¼Œæ³°ç±³å°”è¯­çš„å®F1åˆ†æ•°ä¸º0.61ï¼Œé©¬æ‹‰é›…æ‹‰å§†è¯­çš„å®F1åˆ†æ•°ä¸º0.5ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è®½åˆºæ£€æµ‹æ˜¯æƒ…æ„Ÿåˆ†æä¸­çš„ä¸€å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ç¤¾äº¤åª’ä½“è¯­å¢ƒä¸­è¯­è¨€æ··æ‚çš„æƒ…å†µä¸‹æ›´ä¸ºå›°éš¾ã€‚</li>
<li>ä»£ç æ··åˆæ¶‰åŠåœ¨å•ä¸ªè¯è¯­ä¸­æ··åˆå¤šç§è¯­è¨€å’Œéæ¯è¯­è„šæœ¬ï¼Œè¿™å¢åŠ äº†ä»»åŠ¡å¤æ‚æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹é‡‘æ ‡å‡†è¯­æ–™åº“ï¼Œç”¨äºè¯†åˆ«æ··åˆä»£ç æ–‡æœ¬ä¸­çš„è®½åˆºå’Œæƒ…æ„Ÿææ€§ã€‚</li>
<li>å®éªŒä½¿ç”¨GPT-3.5 Turboç­‰å…ˆè¿›æŠ€æœ¯å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œã€‚</li>
<li>æ³°ç±³å°”è¯­çš„å®F1åˆ†æ•°ä¸º0.61ï¼Œæ˜¾ç¤ºå‡ºç›¸å¯¹è¾ƒå¥½çš„æ€§èƒ½ã€‚</li>
<li>é©¬æ‹‰é›…æ‹‰å§†è¯­çš„å®F1åˆ†æ•°ä¸º0.5ï¼Œéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.05039">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2c01ecf94292c5673012d68c390a7e84.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Revealing-and-Reducing-Gender-Biases-in-Vision-and-Language-Assistants-VLAs"><a href="#Revealing-and-Reducing-Gender-Biases-in-Vision-and-Language-Assistants-VLAs" class="headerlink" title="Revealing and Reducing Gender Biases in Vision and Language Assistants   (VLAs)"></a>Revealing and Reducing Gender Biases in Vision and Language Assistants   (VLAs)</h2><p><strong>Authors:Leander Girrbach, Stephan Alaniz, Yiran Huang, Trevor Darrell, Zeynep Akata</strong></p>
<p>Pre-trained large language models (LLMs) have been reliably integrated with visual input for multimodal tasks. The widespread adoption of instruction-tuned image-to-text vision-language assistants (VLAs) like LLaVA and InternVL necessitates evaluating gender biases. We study gender bias in 22 popular open-source VLAs with respect to personality traits, skills, and occupations. Our results show that VLAs replicate human biases likely present in the data, such as real-world occupational imbalances. Similarly, they tend to attribute more skills and positive personality traits to women than to men, and we see a consistent tendency to associate negative personality traits with men. To eliminate the gender bias in these models, we find that fine-tuning-based debiasing methods achieve the best trade-off between debiasing and retaining performance on downstream tasks. We argue for pre-deploying gender bias assessment in VLAs and motivate further development of debiasing strategies to ensure equitable societal outcomes. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ExplainableML/vla-gender-bias">https://github.com/ExplainableML/vla-gender-bias</a>. </p>
<blockquote>
<p>é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å¯é åœ°ä¸è§†è§‰è¾“å…¥ç›¸ç»“åˆï¼Œç”¨äºå¤šæ¨¡å¼ä»»åŠ¡ã€‚éšç€æŒ‡ä»¤è°ƒæ•´å›¾åƒåˆ°æ–‡æœ¬è§†è§‰è¯­è¨€åŠ©ç†ï¼ˆVLAsï¼‰å¦‚LLaVAå’ŒInternVLçš„å¹¿æ³›åº”ç”¨ï¼Œå¯¹æ€§åˆ«åè§è¿›è¡Œè¯„ä¼°æ˜¯å¿…è¦çš„ã€‚æˆ‘ä»¬ç ”ç©¶äº†22ä¸ªæµè¡Œå¼€æºVLAsä¸­çš„æ€§åˆ«åè§ï¼Œæ¶‰åŠäººæ ¼ç‰¹è´¨ã€æŠ€èƒ½å’ŒèŒä¸šã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒVLAså¤åˆ¶äº†æ•°æ®ä¸­å¯èƒ½å­˜åœ¨çš„äººç±»åè§ï¼Œå¦‚ç°å®ä¸–ç•Œä¸­èŒä¸šåˆ†å¸ƒä¸å‡ç­‰ã€‚åŒæ ·ï¼Œå®ƒä»¬å€¾å‘äºå°†æ›´å¤šçš„æŠ€èƒ½å’Œç§¯æçš„äººæ ¼ç‰¹è´¨å½’å› äºå¥³æ€§è€Œéç”·æ€§ï¼Œå¹¶ä¸”æˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸€ç§æŒç»­çš„è¶‹åŠ¿ï¼Œå³å°†æ¶ˆæçš„äººæ ¼ç‰¹è´¨ä¸ç”·æ€§è”ç³»åœ¨ä¸€èµ·ã€‚ä¸ºäº†æ¶ˆé™¤è¿™äº›æ¨¡å‹ä¸­çš„æ€§åˆ«åè§ï¼Œæˆ‘ä»¬å‘ç°åŸºäºå¾®è°ƒå»åæ–¹æ³•åœ¨å®ç°å»åå¹¶ä¿æŒä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚æˆ‘ä»¬ä¸»å¼ åœ¨VLAsä¸­é¢„å…ˆéƒ¨ç½²æ€§åˆ«åè§è¯„ä¼°ï¼Œå¹¶æ¨åŠ¨è¿›ä¸€æ­¥å¼€å‘å»åç­–ç•¥ï¼Œä»¥ç¡®ä¿å…¬å¹³çš„ç¤¾ä¼šç»“æœã€‚ç›¸å…³ä»£ç å¯è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/ExplainableML/vlas-gender-bias">https://github.com/ExplainableML/vlas-gender-bias</a> äº†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.19314v2">PDF</a> Accepted at ICLR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å¯é åœ°ç”¨äºå¤šæ¨¡æ€ä»»åŠ¡ä¸­çš„è§†è§‰è¾“å…¥ã€‚éšç€æŒ‡ä»¤è°ƒæ•´å‹å›¾åƒåˆ°æ–‡æœ¬çš„è§†è§‰è¯­è¨€åŠ©æ‰‹ï¼ˆVLAsï¼‰å¦‚LLaVAå’ŒInternVLçš„å¹¿æ³›åº”ç”¨ï¼Œéœ€è¦è¯„ä¼°æ€§åˆ«åè§ã€‚æœ¬ç ”ç©¶å¯¹22ä¸ªæµè¡Œçš„å¼€æºVLAsä¸­çš„æ€§åˆ«åè§è¿›è¡Œäº†äººæ ¼ç‰¹è´¨ã€æŠ€èƒ½å’ŒèŒä¸šæ–¹é¢çš„åˆ†æã€‚ç»“æœè¡¨æ˜ï¼ŒVLAså¤åˆ¶äº†æ•°æ®ä¸­å¯èƒ½å­˜åœ¨çš„äººç±»åè§ï¼Œå¦‚ç°å®ä¸–ç•Œä¸­èŒä¸šåˆ†å¸ƒçš„ä¸å¹³è¡¡ã€‚æ­¤å¤–ï¼Œå®ƒä»¬æ›´å€¾å‘äºä¸ºå¥³æ€§åˆ†é…æ›´å¤šæŠ€èƒ½å’Œç§¯æçš„äººæ ¼ç‰¹è´¨ï¼Œç›¸è¾ƒäºç”·æ€§æ¥è¯´ã€‚è¿˜å­˜åœ¨ä¸€ç§ä¸€è´¯çš„å€¾å‘ï¼Œå³å°†æ¶ˆæçš„äººæ ¼ç‰¹è´¨ä¸ç”·æ€§è”ç³»åœ¨ä¸€èµ·ã€‚ä¸ºäº†æ¶ˆé™¤è¿™äº›æ¨¡å‹ä¸­çš„æ€§åˆ«åè§ï¼Œæˆ‘ä»¬å‘ç°åŸºäºå¾®è°ƒæŠ€æœ¯çš„å»åæ–¹æ³•åœ¨å®ç°å»åå¹¶ä¿æŒä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æ–¹é¢çš„æœ€ä½³æƒè¡¡ã€‚æˆ‘ä»¬ä¸»å¼ åœ¨VLAsä¸­é¢„å…ˆéƒ¨ç½²æ€§åˆ«åè§è¯„ä¼°ï¼Œå¹¶æ¨åŠ¨è¿›ä¸€æ­¥å¼€å‘å»åç­–ç•¥ï¼Œä»¥ç¡®ä¿å…¬å¹³çš„ç¤¾ä¼šç»“æœã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ExplainableML/vla-gender-bias%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ExplainableML/vla-gender-biasæ‰¾åˆ°ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å·²ä¸è§†è§‰è¾“å…¥å¯é é›†æˆã€‚</li>
<li>å¹¿æ³›åº”ç”¨çš„è§†è§‰è¯­è¨€åŠ©æ‰‹ï¼ˆVLAsï¼‰å­˜åœ¨æ€§åˆ«åè§é—®é¢˜ï¼Œéœ€è¦è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>VLAså¤åˆ¶äº†æ•°æ®ä¸­å¯èƒ½å­˜åœ¨çš„æ€§åˆ«åè§ï¼Œå¦‚èŒä¸šåˆ†å¸ƒä¸å¹³è¡¡ã€‚</li>
<li>VLAså€¾å‘äºä¸ºå¥³æ€§åˆ†é…æ›´å¤šæŠ€èƒ½å’Œç§¯æçš„äººæ ¼ç‰¹è´¨ã€‚</li>
<li>å­˜åœ¨å°†æ¶ˆæäººæ ¼ç‰¹è´¨ä¸ç”·æ€§è”ç³»åœ¨ä¸€èµ·çš„å€¾å‘ã€‚</li>
<li>åŸºäºå¾®è°ƒæŠ€æœ¯çš„å»åæ–¹æ³•åœ¨å»åå’Œä¿æŒä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æ–¹é¢è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.19314">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ce6cbbb4785b38eddc1a2a3f36d1807a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d597630a209719bef21cf190240b068b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-910e24df412a331499dbc9631ff58310.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb5f79f1a33575a7f357f66575d58848.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="BioMistral-NLU-Towards-More-Generalizable-Medical-Language-Understanding-through-Instruction-Tuning"><a href="#BioMistral-NLU-Towards-More-Generalizable-Medical-Language-Understanding-through-Instruction-Tuning" class="headerlink" title="BioMistral-NLU: Towards More Generalizable Medical Language   Understanding through Instruction Tuning"></a>BioMistral-NLU: Towards More Generalizable Medical Language   Understanding through Instruction Tuning</h2><p><strong>Authors:Yujuan Velvin Fu, Giridhar Kaushik Ramachandran, Namu Park, Kevin Lybarger, Fei Xia, Ozlem Uzuner, Meliha Yetisgen</strong></p>
<p>Large language models (LLMs) such as ChatGPT are fine-tuned on large and diverse instruction-following corpora, and can generalize to new tasks. However, those instruction-tuned LLMs often perform poorly in specialized medical natural language understanding (NLU) tasks that require domain knowledge, granular text comprehension, and structured data extraction. To bridge the gap, we: (1) propose a unified prompting format for 7 important NLU tasks, (2) curate an instruction-tuning dataset, MNLU-Instruct, utilizing diverse existing open-source medical NLU corpora, and (3) develop BioMistral-NLU, a generalizable medical NLU model, through fine-tuning BioMistral on MNLU-Instruct. We evaluate BioMistral-NLU in a zero-shot setting, across 6 important NLU tasks, from two widely adopted medical NLU benchmarks: BLUE and BLURB. Our experiments show that our BioMistral-NLU outperforms the original BioMistral, as well as the proprietary LLMs - ChatGPT and GPT-4. Our dataset-agnostic prompting strategy and instruction tuning step over diverse NLU tasks enhance LLMsâ€™ generalizability across diverse medical NLU tasks. Our ablation experiments show that instruction-tuning on a wider variety of tasks, even when the total number of training instances remains constant, enhances downstream zero-shot generalization. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ChatGPTæ˜¯åœ¨å¤§é‡ä¸”å¤šæ ·çš„æŒ‡ä»¤éµå¾ªè¯­æ–™åº“ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¹¶èƒ½æ¨å¹¿è‡³æ–°ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™äº›ç»è¿‡æŒ‡ä»¤è°ƒæ ¡çš„LLMåœ¨éœ€è¦é¢†åŸŸçŸ¥è¯†ã€ç²¾ç»†æ–‡æœ¬ç†è§£å’Œç»“æ„åŒ–æ•°æ®æå–çš„ä¸“é—¨åŒ»å­¦è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰ä»»åŠ¡ä¸­è¡¨ç°å¾€å¾€ä¸ä½³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬ï¼š(1)ä¸º7ä¸ªé‡è¦çš„NLUä»»åŠ¡æå‡ºäº†ç»Ÿä¸€çš„æç¤ºæ ¼å¼ï¼Œ(2)åˆ©ç”¨å¤šæ ·çš„ç°æœ‰å¼€æºåŒ»å­¦NLUè¯­æ–™åº“ï¼Œåˆ¶ä½œäº†ä¸€ä¸ªæŒ‡ä»¤è°ƒæ ¡æ•°æ®é›†MNLU-Instructï¼Œ(3)é€šè¿‡åœ¨MNLU-Instructä¸Šå¯¹BioMistralè¿›è¡Œå¾®è°ƒï¼Œå¼€å‘äº†å¯é€šç”¨çš„åŒ»å­¦NLUæ¨¡å‹BioMistral-NLUã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå¹¿æ³›é‡‡ç”¨çš„åŒ»å­¦NLUåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¯¹6ä¸ªé‡è¦çš„NLUä»»åŠ¡å¯¹BioMistral-NLUè¿›è¡Œäº†é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„è¯„ä¼°ï¼šBLUEå’ŒBLURBã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„BioMistral-NLUåœ¨å„æ–¹é¢éƒ½è¶…è¶Šäº†åŸå§‹çš„BioMistralä»¥åŠä¸“æœ‰LLMâ€”â€”ChatGPTå’ŒGPT-4ã€‚æˆ‘ä»¬çš„æ•°æ®é›†æ— å…³çš„æç¤ºç­–ç•¥ä»¥åŠæŒ‡ä»¤è°ƒæ ¡æ­¥éª¤åœ¨ä¸åŒçš„NLUä»»åŠ¡ä¸Šå¢å¼ºäº†LLMåœ¨å¤šç§åŒ»å­¦NLUä»»åŠ¡ä¸­çš„é€šç”¨æ€§ã€‚æˆ‘ä»¬çš„æ¶ˆèå®éªŒè¡¨æ˜ï¼Œåœ¨æ›´å¹¿æ³›çš„ä»»åŠ¡ä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒæ ¡ï¼Œå³ä½¿è®­ç»ƒå®ä¾‹çš„æ€»æ•°ä¿æŒä¸å˜ï¼Œä¹Ÿèƒ½å¢å¼ºä¸‹æ¸¸é›¶æ ·æœ¬æ¨å¹¿èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.18955v2">PDF</a> 3 figures an 5 tables; Accepted by AMIA 2025 Informatics Summit</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚ChatGPTåœ¨æŒ‡ä»¤éµå¾ªè¯­æ–™åº“ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥é€‚åº”æ–°ä»»åŠ¡ã€‚ä½†åœ¨éœ€è¦é¢†åŸŸçŸ¥è¯†ã€ç²¾ç»†æ–‡æœ¬ç†è§£å’Œç»“æ„åŒ–æ•°æ®æå–çš„ç‰¹å®šåŒ»ç–—è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰ä»»åŠ¡ä¸­ï¼Œè¿™äº›æŒ‡ä»¤å¾®è°ƒLLMçš„è¡¨ç°å¾€å¾€ä¸ä½³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†ç»Ÿä¸€çš„æç¤ºæ ¼å¼ï¼Œä¸º7ä¸ªé‡è¦çš„NLUä»»åŠ¡åˆ¶ä½œäº†ä¸€ä¸ªæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†MNLU-Instructï¼Œå¹¶å¼€å‘äº†é€šè¿‡MNLU-Instructç»†åŒ–çš„é€šç”¨åŒ»ç–—NLUæ¨¡å‹BioMistral-NLUã€‚æˆ‘ä»¬åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­å¯¹BioMistral-NLUè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶‰åŠä¸¤ä¸ªå¹¿æ³›é‡‡ç”¨çš„åŒ»ç–—NLUåŸºå‡†æµ‹è¯•ä¸­çš„6ä¸ªé‡è¦NLUä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼ŒBioMistral-NLUçš„è¡¨ç°ä¼˜äºåŸå§‹çš„BioMistralä»¥åŠä¸“æœ‰LLMâ€”â€”ChatGPTå’ŒGPT-4ã€‚æˆ‘ä»¬çš„æ•°æ®é›†æ— å…³æç¤ºç­–ç•¥å’ŒæŒ‡ä»¤å¾®è°ƒæ­¥éª¤åœ¨ä¸åŒçš„NLUä»»åŠ¡ä¸Šå¢å¼ºäº†LLMçš„æ³›åŒ–èƒ½åŠ›ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼Œåœ¨æ€»è®­ç»ƒå®ä¾‹æ•°ä¿æŒä¸å˜çš„æƒ…å†µä¸‹ï¼Œå¯¹æ›´å¤šæ ·åŒ–çš„ä»»åŠ¡è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œå¯ä»¥æé«˜ä¸‹æ¸¸é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså¦‚ChatGPTåœ¨ç‰¹å®šåŒ»ç–—NLUä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œéœ€è¦é¢†åŸŸçŸ¥è¯†å’Œç»“æ„åŒ–æ•°æ®æå–ç­‰ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æç¤ºæ ¼å¼ï¼Œä¸ºå¤šç§åŒ»ç–—NLUä»»åŠ¡æä¾›è§£å†³æ–¹æ¡ˆã€‚</li>
<li>åˆ¶ä½œäº†ä¸€ä¸ªæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†MNLU-Instructï¼Œåˆ©ç”¨ç°æœ‰å¼€æºåŒ»ç–—NLUè¯­æ–™åº“ã€‚</li>
<li>å¼€å‘äº†BioMistral-NLUæ¨¡å‹ï¼Œé€šè¿‡MNLU-Instructç»†åŒ–çš„è¡¨ç°ä¼˜äºåŸå§‹BioMistralå’Œä¸“æœ‰LLMã€‚</li>
<li>æ•°æ®é›†æ— å…³çš„æç¤ºç­–ç•¥å’ŒæŒ‡ä»¤å¾®è°ƒæ­¥éª¤å¢å¼ºäº†LLMåœ¨å¤šæ ·åŒ–åŒ»ç–—NLUä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ¶ˆèå®éªŒè¡¨æ˜ï¼Œåœ¨ä¿æŒæ€»è®­ç»ƒå®ä¾‹æ•°ä¸å˜çš„æƒ…å†µä¸‹ï¼Œå¯¹æ›´å¤šæ ·åŒ–çš„ä»»åŠ¡è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒå¯ä»¥æé«˜ä¸‹æ¸¸é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºæå‡LLMsåœ¨ç‰¹å®šé¢†åŸŸï¼ˆå¦‚åŒ»ç–—ï¼‰çš„ä»»åŠ¡è¡¨ç°æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.18955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2ab63a3d6109cdddc7991fa7b16190da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1f59a3560e2194913f20742112049d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6826bae7516632467541a2163ea029e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfb109778e6fc541d3769be428eb8658.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Latent-Space-Chain-of-Embedding-Enables-Output-free-LLM-Self-Evaluation"><a href="#Latent-Space-Chain-of-Embedding-Enables-Output-free-LLM-Self-Evaluation" class="headerlink" title="Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation"></a>Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation</h2><p><strong>Authors:Yiming Wang, Pei Zhang, Baosong Yang, Derek F. Wong, Rui Wang</strong></p>
<p>LLM self-evaluation relies on the LLMâ€™s own ability to estimate response correctness, which can greatly improve its deployment reliability. In this research track, we propose the Chain-of-Embedding (CoE) in the latent space to enable LLMs to perform output-free self-evaluation. CoE consists of all progressive hidden states produced during the inference time, which can be treated as the latent thinking path of LLMs. We find that when LLMs respond correctly and incorrectly, their CoE features differ, these discrepancies assist us in estimating LLM response correctness. Experiments in four diverse domains and seven LLMs fully demonstrate the effectiveness of our method. Meanwhile, its label-free design intent without any training and millisecond-level computational cost ensures real-time feedback in large-scale scenarios. More importantly, we provide interesting insights into LLM response correctness from the perspective of hidden state changes inside LLMs. </p>
<blockquote>
<p>LLMçš„è‡ªæˆ‘è¯„ä¼°ä¾èµ–äºå…¶è‡ªèº«è¯„ä¼°å“åº”æ­£ç¡®æ€§çš„èƒ½åŠ›ï¼Œè¿™å¯ä»¥å¤§å¤§æé«˜å…¶éƒ¨ç½²å¯é æ€§ã€‚åœ¨æœ¬ç ”ç©¶è½¨è¿¹ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨ç©ºé—´ä¸­çš„åµŒå…¥é“¾ï¼ˆCoEï¼‰æŠ€æœ¯ï¼Œä½¿LLMèƒ½å¤Ÿæ‰§è¡Œæ— è¾“å‡ºè‡ªæˆ‘è¯„ä¼°ã€‚CoEç”±æ¨ç†æ—¶é—´æœŸé—´äº§ç”Ÿçš„æ‰€æœ‰æ¸è¿›éšè—çŠ¶æ€ç»„æˆï¼Œå¯è§†ä¸ºLLMçš„æ½œåœ¨æ€è€ƒè·¯å¾„ã€‚æˆ‘ä»¬å‘ç°ï¼Œå½“LLMæ­£ç¡®å›ç­”å’Œé”™è¯¯å›ç­”æ—¶ï¼Œå®ƒä»¬çš„CoEç‰¹å¾æœ‰æ‰€ä¸åŒï¼Œè¿™äº›å·®å¼‚æœ‰åŠ©äºæˆ‘ä»¬è¯„ä¼°LLMå“åº”çš„æ­£ç¡®æ€§ã€‚åœ¨å››ä¸ªä¸åŒé¢†åŸŸå’Œä¸ƒä¸ªLLMä¸Šçš„å®éªŒå……åˆ†è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚åŒæ—¶ï¼Œå…¶æ— éœ€æ ‡ç­¾çš„è®¾è®¡æ„å›¾æ— éœ€ä»»ä½•è®­ç»ƒå’Œæ¯«ç§’çº§çš„è®¡ç®—æˆæœ¬ï¼Œå¯ç¡®ä¿å¤§è§„æ¨¡åœºæ™¯ä¸­çš„å®æ—¶åé¦ˆã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬ä»LLMå†…éƒ¨éšè—çŠ¶æ€å˜åŒ–çš„è§’åº¦ï¼Œä¸ºLLMå“åº”æ­£ç¡®æ€§æä¾›äº†æœ‰è¶£çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13640v2">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>LLMçš„è‡ªè¯„ä¼°èƒ½åŠ›ä¾é å…¶è‡ªæˆ‘åˆ¤æ–­å“åº”æ­£ç¡®æ€§çš„èƒ½åŠ›ï¼Œå¯å¤§å¹…æé«˜éƒ¨ç½²å¯é æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†åŸºäºåµŒå…¥é“¾ï¼ˆCoEï¼‰çš„æ½œåœ¨ç©ºé—´æ–¹æ³•ï¼Œä½¿LLMèƒ½å¤Ÿæ— è¾“å‡ºåœ°è¿›è¡Œè‡ªæˆ‘è¯„ä¼°ã€‚åµŒå…¥é“¾åŒ…å«æ¨ç†è¿‡ç¨‹ä¸­çš„æ‰€æœ‰æ¸è¿›éšè—çŠ¶æ€ï¼Œå¯è§†ä¸ºLLMçš„æ½œåœ¨æ€è€ƒè·¯å¾„ã€‚ç ”ç©¶å‘ç°ï¼Œå½“LLMçš„å“åº”æ­£ç¡®ä¸å¦æ—¶ï¼Œå…¶åµŒå…¥é“¾çš„ç‰¹å¾å­˜åœ¨å·®å¼‚ï¼Œè¿™äº›å·®å¼‚æœ‰åŠ©äºæˆ‘ä»¬åˆ¤æ–­LLMçš„å“åº”æ­£ç¡®æ€§ã€‚åœ¨å¤šé¢†åŸŸå’Œå¤šLLMçš„å®éªŒä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå…¶æ— éœ€æ ‡ç­¾çš„è®¾è®¡æ„å›¾æ— éœ€ä»»ä½•è®­ç»ƒï¼Œæ¯«ç§’çº§çš„è®¡ç®—æˆæœ¬ä¿è¯äº†å¤§è§„æ¨¡åœºæ™¯ä¸‹çš„å®æ—¶åé¦ˆã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæœ¬ç ”ç©¶ä»LLMå†…éƒ¨éšè—çŠ¶æ€å˜åŒ–çš„è§’åº¦æä¾›äº†å…³äºå“åº”æ­£ç¡®æ€§çš„æœ‰è¶£è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMçš„è‡ªè¯„ä¼°èƒ½åŠ›å¯ä»¥ä¾é å…¶è‡ªèº«åˆ¤æ–­å“åº”çš„æ­£ç¡®æ€§ï¼Œä»è€Œæé«˜éƒ¨ç½²å¯é æ€§ã€‚</li>
<li>æå‡ºäº†åŸºäºåµŒå…¥é“¾ï¼ˆCoEï¼‰çš„æ½œåœ¨ç©ºé—´æ–¹æ³•ï¼Œä½¿LLMèƒ½å¤Ÿæ— è¾“å‡ºåœ°è‡ªæˆ‘è¯„ä¼°ã€‚</li>
<li>åµŒå…¥é“¾åŒ…å«æ¨ç†è¿‡ç¨‹ä¸­çš„æ‰€æœ‰æ¸è¿›éšè—çŠ¶æ€ï¼Œåæ˜ LLMçš„æ½œåœ¨æ€è€ƒè·¯å¾„ã€‚</li>
<li>LLMåœ¨æ­£ç¡®å’Œé”™è¯¯å“åº”æ—¶ï¼Œå…¶åµŒå…¥é“¾ç‰¹å¾å­˜åœ¨å·®å¼‚ï¼Œæœ‰åŠ©äºåˆ¤æ–­å…¶å“åº”æ­£ç¡®æ€§ã€‚</li>
<li>åœ¨å¤šé¢†åŸŸå’Œå¤šLLMçš„å®éªŒä¸­éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ— éœ€æ ‡ç­¾çš„è®¾è®¡ä½¿å¾—è¯¥æ–¹æ³•æ— éœ€é¢å¤–è®­ç»ƒï¼Œè®¡ç®—æˆæœ¬ä½ï¼Œå¯å®ç°å®æ—¶åé¦ˆã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13640">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-39665de00ff70101b44070cb910e068f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ffa2006006ad1a04ad37418be30fa50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61ff01a523591a9430b351da86da146a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d046294797b0ab7aaf1dfc4c4e51a752.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Emotion-Aware-Embedding-Fusion-in-LLMs-Flan-T5-LLAMA-2-DeepSeek-R1-and-ChatGPT-4-for-Intelligent-Response-Generation"><a href="#Emotion-Aware-Embedding-Fusion-in-LLMs-Flan-T5-LLAMA-2-DeepSeek-R1-and-ChatGPT-4-for-Intelligent-Response-Generation" class="headerlink" title="Emotion-Aware Embedding Fusion in LLMs (Flan-T5, LLAMA 2, DeepSeek-R1,   and ChatGPT 4) for Intelligent Response Generation"></a>Emotion-Aware Embedding Fusion in LLMs (Flan-T5, LLAMA 2, DeepSeek-R1,   and ChatGPT 4) for Intelligent Response Generation</h2><p><strong>Authors:Abdur Rasool, Muhammad Irfan Shahzad, Hafsa Aslam, Vincent Chan, Muhammad Ali Arshad</strong></p>
<p>Empathetic and coherent responses are critical in auto-mated chatbot-facilitated psychotherapy. This study addresses the challenge of enhancing the emotional and contextual understanding of large language models (LLMs) in psychiatric applications. We introduce Emotion-Aware Embedding Fusion, a novel framework integrating hierarchical fusion and attention mechanisms to prioritize semantic and emotional features in therapy transcripts. Our approach combines multiple emotion lexicons, including NRC Emotion Lexicon, VADER, WordNet, and SentiWordNet, with state-of-the-art LLMs such as Flan-T5, LLAMA 2, DeepSeek-R1, and ChatGPT 4. Therapy session transcripts, comprising over 2,000 samples are segmented into hierarchical levels (word, sentence, and session) using neural networks, while hierarchical fusion combines these features with pooling techniques to refine emotional representations. Atten-tion mechanisms, including multi-head self-attention and cross-attention, further prioritize emotional and contextual features, enabling temporal modeling of emotion-al shifts across sessions. The processed embeddings, computed using BERT, GPT-3, and RoBERTa are stored in the Facebook AI similarity search vector database, which enables efficient similarity search and clustering across dense vector spaces. Upon user queries, relevant segments are retrieved and provided as context to LLMs, enhancing their ability to generate empathetic and con-textually relevant responses. The proposed framework is evaluated across multiple practical use cases to demonstrate real-world applicability, including AI-driven therapy chatbots. The system can be integrated into existing mental health platforms to generate personalized responses based on retrieved therapy session data. </p>
<blockquote>
<p>åœ¨è‡ªåŠ¨èŠå¤©æœºå™¨äººè¾…åŠ©çš„å¿ƒç†æ²»ç–—ï¼ˆpsychotherapyï¼‰ä¸­ï¼Œä½“è´´ä¸”è¿è´¯çš„å›åº”è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¢å¼ºç²¾ç¥ç—…å­¦åº”ç”¨ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æƒ…æ„Ÿå’Œä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†æƒ…æ„Ÿæ„ŸçŸ¥åµŒå…¥èåˆï¼ˆEmotion-Aware Embedding Fusionï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å±‚æ¬¡èåˆå’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥ä¼˜å…ˆå¤„ç†æ²»ç–—è®°å½•ä¸­çš„è¯­ä¹‰å’Œæƒ…æ„Ÿç‰¹å¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†å¤šä¸ªæƒ…æ„Ÿè¯å…¸ï¼ŒåŒ…æ‹¬NRCæƒ…æ„Ÿè¯å…¸ã€VADERã€WordNetå’ŒSentiWordNetï¼Œä»¥åŠæœ€å…ˆè¿›çš„LLMï¼Œå¦‚Flan-T5ã€LLAMA 2ã€DeepSeek-R1å’ŒChatGPT 4ã€‚æ²»ç–—ä¼šè¯è®°å½•ç”±è¶…è¿‡ä¸¤åƒä¸ªæ ·æœ¬ç»„æˆï¼Œå®ƒä»¬é€šè¿‡ç¥ç»ç½‘ç»œåˆ†å±‚åˆ’åˆ†åˆ°ä¸åŒçº§åˆ«ï¼ˆå¦‚å•è¯ã€å¥å­å’Œä¼šè¯ï¼‰ï¼Œè€Œå±‚æ¬¡èåˆåˆ™ä½¿ç”¨æ± æŠ€æœ¯ç»“åˆè¿™äº›ç‰¹å¾æ¥ä¼˜åŒ–æƒ…æ„Ÿè¡¨è¾¾ã€‚æ³¨æ„åŠ›æœºåˆ¶åŒ…æ‹¬å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œäº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒä»¬è¿›ä¸€æ­¥å¼ºè°ƒæƒ…æ„Ÿå’Œä¸Šä¸‹æ–‡ç‰¹å¾ï¼Œå®ç°è·¨ä¼šè¯çš„æƒ…æ„Ÿå˜åŒ–çš„æ—¶é—´å»ºæ¨¡ã€‚ä½¿ç”¨BERTã€GPT-3å’ŒRoBERTaè®¡ç®—å¾—åˆ°çš„å¤„ç†è¿‡çš„åµŒå…¥å­˜å‚¨åœ¨Facebook AIç›¸ä¼¼æ€§æœç´¢å‘é‡æ•°æ®åº“ä¸­ï¼Œè¿™æœ‰åŠ©äºåœ¨å¯†é›†å‘é‡ç©ºé—´ä¸­å®ç°é«˜æ•ˆç›¸ä¼¼æ€§æœç´¢å’Œèšç±»ã€‚åœ¨ç”¨æˆ·æŸ¥è¯¢æ—¶ï¼Œç›¸å…³ç‰‡æ®µè¢«æ£€ç´¢å‡ºæ¥å¹¶æä¾›ç»™LLMä½œä¸ºä¸Šä¸‹æ–‡ï¼Œä»è€Œå¢å¼ºå®ƒä»¬ç”Ÿæˆä½“è´´ä¸”ä¸Šä¸‹æ–‡ç›¸å…³çš„å“åº”çš„èƒ½åŠ›ã€‚æ‰€æå‡ºçš„æ¡†æ¶ç»è¿‡å¤šä¸ªå®é™…åº”ç”¨æ¡ˆä¾‹çš„è¯„ä¼°ï¼Œè¯æ˜äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„é€‚ç”¨æ€§ï¼ŒåŒ…æ‹¬AIé©±åŠ¨çš„ç–—æ³•èŠå¤©æœºå™¨äººã€‚è¯¥ç³»ç»Ÿå¯ä»¥é›†æˆåˆ°ç°æœ‰çš„å¿ƒç†å¥åº·å¹³å°ä¸­ï¼Œæ ¹æ®æ£€ç´¢åˆ°çš„æ²»ç–—ä¼šè¯æ•°æ®ç”Ÿæˆä¸ªæ€§åŒ–å“åº”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.01306v2">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨è‡ªåŠ¨èŠå¤©æœºå™¨äººè¾…åŠ©çš„å¿ƒç†æ²»ç–—ä¸­ï¼Œå…±æƒ…å’Œè¿è´¯çš„å›åº”è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹åœ¨å¿ƒç†æ²»ç–—åº”ç”¨ä¸­çš„æƒ…æ„Ÿç†è§£å’Œè¯­å¢ƒç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†æƒ…æ„Ÿæ„ŸçŸ¥åµŒå…¥èåˆæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å±‚æ¬¡èåˆå’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥ä¼˜å…ˆå¤„ç†æ²»ç–—è®°å½•ä¸­çš„è¯­ä¹‰å’Œæƒ…æ„Ÿç‰¹å¾ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¤šä¸ªæƒ…æ„Ÿè¯å…¸ï¼ŒåŒ…æ‹¬NRCæƒ…æ„Ÿè¯å…¸ã€VADERã€WordNetå’ŒSentiWordNetç­‰ï¼Œä»¥åŠå…ˆè¿›çš„LLMæ¨¡å‹ï¼Œå¦‚Flan-T5ã€LLAMA 2ç­‰ã€‚é€šè¿‡ç¥ç»ç½‘ç»œå°†æ²»ç–—ä¼šè¯è®°å½•åˆ†æ®µï¼Œåˆ©ç”¨å±‚æ¬¡èåˆå’Œæ± åŒ–æŠ€æœ¯ç»„åˆè¿™äº›ç‰¹å¾ï¼Œä»¥ä¼˜åŒ–æƒ…æ„Ÿè¡¨è¾¾ã€‚æ³¨æ„åŠ›æœºåˆ¶æœ‰åŠ©äºä¼˜å…ˆå¤„ç†æƒ…æ„Ÿå’Œè¯­å¢ƒç‰¹å¾ï¼Œå®ç°è·¨ä¼šè¯çš„æƒ…æ„Ÿå˜åŒ–å»ºæ¨¡ã€‚è¯¥ç ”ç©¶è¿˜ä½¿ç”¨äº†BERTã€GPT-3å’ŒRoBERTaç­‰æ¨¡å‹ç”Ÿæˆçš„åµŒå…¥å‘é‡ï¼Œå¹¶åˆ©ç”¨Facebook AIç›¸ä¼¼æ€§æœç´¢å‘é‡æ•°æ®åº“è¿›è¡Œé«˜æ•ˆç›¸ä¼¼æ€§æœç´¢å’Œèšç±»ã€‚ç”¨æˆ·æŸ¥è¯¢æ—¶ï¼Œå¯æ£€ç´¢ç›¸å…³ç‰‡æ®µä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå¢å¼ºLLMç”Ÿæˆå…±æƒ…å’Œè¯­å¢ƒç›¸å…³å“åº”çš„èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªå®é™…åº”ç”¨åœºæ™¯ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶åœ¨ç°å®ä¸–ç•Œçš„é€‚ç”¨æ€§ï¼ŒåŒ…æ‹¬AIé©±åŠ¨çš„æ²»ç–—èŠå¤©æœºå™¨äººã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æƒ…æ„Ÿæ„ŸçŸ¥åµŒå…¥èåˆæ¡†æ¶ç”¨äºå¢å¼ºå¤§è¯­è¨€æ¨¡å‹åœ¨å¿ƒç†æ²»ç–—ä¸­çš„æƒ…æ„Ÿç†è§£å’Œè¯­å¢ƒç†è§£èƒ½åŠ›ã€‚</li>
<li>æ¡†æ¶ç»“åˆäº†å±‚æ¬¡èåˆå’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥ä¼˜å…ˆå¤„ç†æ²»ç–—è®°å½•ä¸­çš„è¯­ä¹‰å’Œæƒ…æ„Ÿç‰¹å¾ã€‚</li>
<li>ç»“åˆäº†å¤šä¸ªæƒ…æ„Ÿè¯å…¸å’Œå…ˆè¿›çš„LLMæ¨¡å‹ã€‚</li>
<li>æ²»ç–—ä¼šè¯è®°å½•è¢«åˆ†æ®µå¹¶ä¼˜åŒ–æƒ…æ„Ÿè¡¨è¾¾ã€‚</li>
<li>æ³¨æ„åŠ›æœºåˆ¶æœ‰åŠ©äºè·¨ä¼šè¯çš„æƒ…æ„Ÿå˜åŒ–å»ºæ¨¡ã€‚</li>
<li>åˆ©ç”¨BERTã€GPT-3å’ŒRoBERTaç­‰æ¨¡å‹ç”Ÿæˆçš„åµŒå…¥å‘é‡è¿›è¡Œç›¸ä¼¼æ€§æœç´¢å’Œèšç±»ã€‚</li>
<li>è¯¥æ¡†æ¶é€‚ç”¨äºAIé©±åŠ¨çš„æ²»ç–—èŠå¤©æœºå™¨äººï¼Œå¯ç”Ÿæˆä¸ªæ€§åŒ–çš„å“åº”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.01306">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f0289f2bacfdb2679370bda5dfd43f06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0596cc08ab795b4a047674ea67a1d4fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad3a38ea661e8f934d3233b0412e3bbd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab6b06acb53034a45b21c0d9ebe38c40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c1795ba97ee27c77ad7d99d7097e53d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Procedure-Aware-Surgical-Video-language-Pretraining-with-Hierarchical-Knowledge-Augmentation"><a href="#Procedure-Aware-Surgical-Video-language-Pretraining-with-Hierarchical-Knowledge-Augmentation" class="headerlink" title="Procedure-Aware Surgical Video-language Pretraining with Hierarchical   Knowledge Augmentation"></a>Procedure-Aware Surgical Video-language Pretraining with Hierarchical   Knowledge Augmentation</h2><p><strong>Authors:Kun Yuan, Vinkle Srivastav, Nassir Navab, Nicolas Padoy</strong></p>
<p>Surgical video-language pretraining (VLP) faces unique challenges due to the knowledge domain gap and the scarcity of multi-modal data. This study aims to bridge the gap by addressing issues regarding textual information loss in surgical lecture videos and the spatial-temporal challenges of surgical VLP. We propose a hierarchical knowledge augmentation approach and a novel Procedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining (PeskaVLP) framework to tackle these issues. The knowledge augmentation uses large language models (LLM) for refining and enriching surgical concepts, thus providing comprehensive language supervision and reducing the risk of overfitting. PeskaVLP combines language supervision with visual self-supervision, constructing hard negative samples and employing a Dynamic Time Warping (DTW) based loss function to effectively comprehend the cross-modal procedural alignment. Extensive experiments on multiple public surgical scene understanding and cross-modal retrieval datasets show that our proposed method significantly improves zero-shot transferring performance and offers a generalist visual representation for further advancements in surgical scene understanding.The code is available at <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SurgVLP">https://github.com/CAMMA-public/SurgVLP</a> </p>
<blockquote>
<p>æ‰‹æœ¯è§†é¢‘è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰é¢ä¸´ç€çŸ¥è¯†åŸŸå·®è·å’Œå¤šæ¨¡æ€æ•°æ®ç¨€ç¼ºæ‰€å¸¦æ¥çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡è§£å†³æ‰‹æœ¯è®²åº§è§†é¢‘ä¸­æ–‡æœ¬ä¿¡æ¯ä¸¢å¤±ä»¥åŠæ‰‹æœ¯VLPçš„æ—¶ç©ºæŒ‘æˆ˜æ¥å¼¥è¡¥è¿™ä¸€å·®è·ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å±‚çŸ¥è¯†å¢å¼ºæ–¹æ³•å’Œä¸€ç§æ–°é¢–çš„æ‰‹æœ¯ç¼–ç çŸ¥è¯†å¢å¼ºè§†é¢‘è¯­è¨€é¢„è®­ç»ƒï¼ˆPeskaVLPï¼‰æ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚çŸ¥è¯†å¢å¼ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æç‚¼å’Œä¸°å¯Œæ‰‹æœ¯æ¦‚å¿µï¼Œä»è€Œä¸ºæ‰‹æœ¯è¯­è¨€æä¾›å…¨é¢çš„ç›‘ç£ï¼Œé™ä½è¿‡æ‹Ÿåˆçš„é£é™©ã€‚PeskaVLPç»“åˆäº†è¯­è¨€ç›‘ç£å’Œè§†è§‰è‡ªç›‘ç£ï¼Œæ„å»ºç¡¬è´Ÿæ ·æœ¬å¹¶é‡‡ç”¨åŸºäºåŠ¨æ€æ—¶é—´è§„æ•´ï¼ˆDTWï¼‰çš„æŸå¤±å‡½æ•°ï¼Œä»¥æœ‰æ•ˆåœ°ç†è§£è·¨æ¨¡æ€è¿‡ç¨‹å¯¹é½ã€‚åœ¨å¤šä¸ªå…¬å…±æ‰‹æœ¯åœºæ™¯ç†è§£å’Œè·¨æ¨¡æ€æ£€ç´¢æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•æ˜¾è‘—æé«˜äº†é›¶æ ·æœ¬è¿ç§»æ€§èƒ½ï¼Œå¹¶ä¸ºæ‰‹æœ¯åœºæ™¯ç†è§£çš„è¿›ä¸€æ­¥è¿›å±•æä¾›äº†é€šç”¨çš„è§†è§‰è¡¨ç¤ºã€‚ä»£ç å¯ä» <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SurgVLP">https://github.com/CAMMA-public/SurgVLP</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.00263v2">PDF</a> Accepted at the 38th Conference on Neural Information Processing   Systems (NeurIPS 2024 Spolight)</p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹æ‰‹æœ¯è§†é¢‘è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰ä¸­çš„çŸ¥è¯†åŸŸå·®è·å’Œå¤šæ¨¡æ€æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºå±‚æ¬¡çŸ¥è¯†å¢å¼ºå’Œæ–°å‹æ‰‹æœ¯çŸ¥è¯†ç¼–ç çš„è§†é¢‘è¯­è¨€é¢„è®­ç»ƒæ¡†æ¶ï¼ˆPeskaVLPï¼‰ã€‚åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ‰‹æœ¯æ¦‚å¿µç²¾ç»†åŒ–å’Œä¸°å¯ŒåŒ–ï¼Œæä¾›å…¨é¢çš„è¯­è¨€ç›‘ç£å¹¶é™ä½è¿‡æ‹Ÿåˆé£é™©ã€‚ç»“åˆè¯­è¨€ç›‘ç£å’Œè§†è§‰è‡ªç›‘ç£ï¼Œæ„å»ºç¡¬è´Ÿæ ·æœ¬å¹¶é‡‡ç”¨åŸºäºåŠ¨æ€æ—¶é—´è§„æ•´ï¼ˆDTWï¼‰çš„æŸå¤±å‡½æ•°ï¼Œå®ç°è·¨æ¨¡æ€è¿‡ç¨‹å¯¹é½çš„æœ‰æ•ˆç†è§£ã€‚åœ¨å¤šä¸ªå…¬å…±æ‰‹æœ¯åœºæ™¯ç†è§£å’Œè·¨æ¨¡æ€æ£€ç´¢æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•æ˜¾è‘—æé«˜äº†é›¶æ ·æœ¬è¿ç§»æ€§èƒ½ï¼Œå¹¶ä¸ºæ‰‹æœ¯åœºæ™¯ç†è§£çš„è¿›ä¸€æ­¥è¿›å±•æä¾›äº†é€šç”¨çš„è§†è§‰è¡¨ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰‹æœ¯è§†é¢‘è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰é¢ä¸´çŸ¥è¯†åŸŸå·®è·å’Œå¤šæ¨¡æ€æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºå±‚æ¬¡çŸ¥è¯†å¢å¼ºçš„æ–¹æ³•ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ä¸°å¯Œæ‰‹æœ¯æ¦‚å¿µã€‚</li>
<li>PeskaVLPæ¡†æ¶ç»“åˆäº†è¯­è¨€ç›‘ç£å’Œè§†è§‰è‡ªç›‘ç£ï¼Œä»¥æ”¹å–„è·¨æ¨¡æ€å¯¹é½ã€‚</li>
<li>é€šè¿‡æ„å»ºç¡¬è´Ÿæ ·æœ¬å’Œé‡‡ç”¨åŸºäºåŠ¨æ€æ—¶é—´è§„æ•´ï¼ˆDTWï¼‰çš„æŸå¤±å‡½æ•°ï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬è¿ç§»æ€§èƒ½ä¸Šçš„æ˜¾è‘—æé«˜ã€‚</li>
<li>æ‰€ææ–¹æ³•ä¸ºæ‰‹æœ¯åœºæ™¯ç†è§£æä¾›äº†é€šç”¨çš„è§†è§‰è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.00263">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-44b2841cc61618cdf035053b6d2d34d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2803a96859fdab26884a8de0c0a59615.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e66746cceb53cf91e966757ec04405d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1d43a5ac28f6f1c6f9c307fe583cabc.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Dynamic-Mixture-of-Experts-An-Auto-Tuning-Approach-for-Efficient-Transformer-Models"><a href="#Dynamic-Mixture-of-Experts-An-Auto-Tuning-Approach-for-Efficient-Transformer-Models" class="headerlink" title="Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient   Transformer Models"></a>Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient   Transformer Models</h2><p><strong>Authors:Yongxin Guo, Zhenglin Cheng, Xiaoying Tang, Zhaopeng Tu, Tao Lin</strong></p>
<p>The Sparse Mixture of Experts (SMoE) has been widely employed to enhance the efficiency of training and inference for Transformer-based foundational models, yielding promising results.However, the performance of SMoE heavily depends on the choice of hyper-parameters, such as the number of experts and the number of experts to be activated (referred to as top-k), resulting in significant computational overhead due to the extensive model training by searching over various hyper-parameter configurations. As a remedy, we introduce the Dynamic Mixture of Experts (DynMoE) technique. DynMoE incorporates (1) a novel gating method that enables each token to automatically determine the number of experts to activate. (2) An adaptive process automatically adjusts the number of experts during training. Extensive numerical results across Vision, Language, and Vision-Language tasks demonstrate the effectiveness of our approach to achieve competitive performance compared to GMoE for vision and language tasks, and MoE-LLaVA for vision-language tasks, while maintaining efficiency by activating fewer parameters. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/LINs-lab/DynMoE">https://github.com/LINs-lab/DynMoE</a>. </p>
<blockquote>
<p>ç¨€ç–ä¸“å®¶æ··åˆï¼ˆSMoEï¼‰å·²è¢«å¹¿æ³›åº”ç”¨äºæé«˜åŸºäºTransformerçš„åŸºç¡€æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ï¼Œå¹¶äº§ç”Ÿäº†æœ‰å‰æ™¯çš„ç»“æœã€‚ç„¶è€Œï¼ŒSMoEçš„æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºè¶…å‚æ•°çš„é€‰æ‹©ï¼Œå¦‚ä¸“å®¶æ•°é‡å’Œè¦æ¿€æ´»çš„ä¸“å®¶æ•°é‡ï¼ˆç§°ä¸ºtop-kï¼‰ï¼Œç”±äºéœ€è¦åœ¨å„ç§è¶…å‚æ•°é…ç½®ä¸Šè¿›è¡Œå¹¿æ³›çš„æ¨¡å‹è®­ç»ƒï¼Œå› æ­¤äº§ç”Ÿäº†å·¨å¤§çš„è®¡ç®—å¼€é”€ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŠ¨æ€ä¸“å®¶æ··åˆï¼ˆDynMoEï¼‰æŠ€æœ¯ã€‚DynMoEç»“åˆäº†ï¼ˆ1ï¼‰ä¸€ç§æ–°å‹çš„é—¨æ§æ–¹æ³•ï¼Œä½¿æ¯ä¸ªä»¤ç‰Œèƒ½å¤Ÿè‡ªåŠ¨ç¡®å®šè¦æ¿€æ´»çš„ä¸“å®¶æ•°é‡ã€‚ï¼ˆ2ï¼‰ä¸€ä¸ªè‡ªé€‚åº”è¿‡ç¨‹ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨è°ƒæ•´ä¸“å®¶æ•°é‡ã€‚åœ¨è§†è§‰ã€è¯­è¨€å’Œè§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šçš„å¤§é‡æ•°å€¼ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å®ç°ä¸GMoEåœ¨è§†è§‰å’Œè¯­è¨€ä»»åŠ¡ä»¥åŠMoE-LLaVAåœ¨è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šçš„ç«äº‰åŠ›åŒæ—¶ï¼Œé€šè¿‡æ¿€æ´»æ›´å°‘çš„å‚æ•°æ¥ä¿æŒé«˜æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/LINs-lab/DynMoE%E3%80%82">https://github.com/LINs-lab/DynMoEã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14297v4">PDF</a> ICLR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>Sparse Mixture of Experts (SMoE)åœ¨æé«˜åŸºäºTransformerçš„æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå…¶æ€§èƒ½é«˜åº¦ä¾èµ–äºè¶…å‚æ•°çš„é€‰æ‹©ï¼Œå¦‚ä¸“å®¶æ•°é‡å’Œæ¿€æ´»çš„ä¸“å®¶æ•°é‡ï¼ˆç§°ä¸ºtop-kï¼‰ï¼Œå¯¼è‡´å› æœç´¢å„ç§è¶…å‚æ•°é…ç½®è€Œäº§ç”Ÿå·¨å¤§çš„è®¡ç®—å¼€é”€ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Dynamic Mixture of Experts (DynMoE)æŠ€æœ¯ã€‚DynMoEåŒ…æ‹¬ï¼ˆ1ï¼‰ä¸€ç§æ–°å‹çš„é—¨æ§æ–¹æ³•ï¼Œä½¿æ¯ä¸ªä»¤ç‰Œèƒ½å¤Ÿè‡ªåŠ¨ç¡®å®šè¦æ¿€æ´»çš„ä¸“å®¶æ•°é‡ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªè‡ªé€‚åº”è¿‡ç¨‹ï¼Œå¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨è°ƒæ•´ä¸“å®¶çš„æ•°é‡ã€‚å¹¿æ³›çš„æ•°å€¼ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰ã€è¯­è¨€å’Œè§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šå®ç°äº†ä¸GMoEå’ŒMoE-LLaVAç›¸æ¯”å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼ŒåŒæ—¶é€šè¿‡æ¿€æ´»è¾ƒå°‘çš„å‚æ•°æ¥æé«˜æ•ˆç‡ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LINs-lab/DynMoE%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/LINs-lab/DynMoEè·å–ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Sparse Mixture of Experts (SMoE)å¢å¼ºäº†Transformeræ¨¡å‹è®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡ã€‚</li>
<li>SMoEæ€§èƒ½å—è¶…å‚æ•°é€‰æ‹©å½±å“ï¼Œå¦‚ä¸“å®¶æ•°é‡å’Œæ¿€æ´»çš„ä¸“å®¶æ•°é‡ã€‚</li>
<li>ä¸ºè§£å†³SMoEçš„é—®é¢˜ï¼Œæå‡ºäº†Dynamic Mixture of Experts (DynMoE)æŠ€æœ¯ã€‚</li>
<li>DynMoEåŒ…å«æ–°å‹é—¨æ§æ–¹æ³•å’Œè‡ªé€‚åº”è¿‡ç¨‹ã€‚</li>
<li>é—¨æ§æ–¹æ³•ä½¿æ¯ä¸ªä»¤ç‰Œèƒ½è‡ªåŠ¨ç¡®å®šæ¿€æ´»çš„ä¸“å®¶æ•°é‡ã€‚</li>
<li>è‡ªé€‚åº”è¿‡ç¨‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨è°ƒæ•´ä¸“å®¶æ•°é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14297">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-43bab51fa230f7e0867486709a0df065.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ba4959898627c6480023e633406c9c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e419fa47f56d75cfca81335be9d52e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c3aaca7b1772b08eb90f985827eda13.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Closed-Loop-Open-Vocabulary-Mobile-Manipulation-with-GPT-4V"><a href="#Closed-Loop-Open-Vocabulary-Mobile-Manipulation-with-GPT-4V" class="headerlink" title="Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V"></a>Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V</h2><p><strong>Authors:Peiyuan Zhi, Zhiyuan Zhang, Yu Zhao, Muzhi Han, Zeyu Zhang, Zhitian Li, Ziyuan Jiao, Baoxiong Jia, Siyuan Huang</strong></p>
<p>Autonomous robot navigation and manipulation in open environments require reasoning and replanning with closed-loop feedback. In this work, we present COME-robot, the first closed-loop robotic system utilizing the GPT-4V vision-language foundation model for open-ended reasoning and adaptive planning in real-world scenarios.COME-robot incorporates two key innovative modules: (i) a multi-level open-vocabulary perception and situated reasoning module that enables effective exploration of the 3D environment and target object identification using commonsense knowledge and situated information, and (ii) an iterative closed-loop feedback and restoration mechanism that verifies task feasibility, monitors execution success, and traces failure causes across different modules for robust failure recovery. Through comprehensive experiments involving 8 challenging real-world mobile and tabletop manipulation tasks, COME-robot demonstrates a significant improvement in task success rate (~35%) compared to state-of-the-art methods. We further conduct comprehensive analyses to elucidate how COME-robotâ€™s design facilitates failure recovery, free-form instruction following, and long-horizon task planning. </p>
<blockquote>
<p>è‡ªä¸»æœºå™¨äººåœ¨å¼€æ”¾ç¯å¢ƒä¸­çš„å¯¼èˆªå’Œæ“æ§éœ€è¦è¿›è¡Œé—­ç¯åé¦ˆçš„æ¨ç†å’Œå†è§„åˆ’ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†COME-robotï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåˆ©ç”¨GPT-4Vè§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹è¿›è¡Œå¼€æ”¾å¼æ¨ç†å’Œç°å®ä¸–ç•Œåœºæ™¯è‡ªé€‚åº”è§„åˆ’çš„é—­ç¯æœºå™¨äººç³»ç»Ÿã€‚COME-robotåŒ…å«ä¸¤ä¸ªå…³é”®çš„åˆ›æ–°æ¨¡å—ï¼šï¼ˆiï¼‰å¤šå±‚æ¬¡å¼€æ”¾è¯æ±‡æ„ŸçŸ¥å’Œæƒ…å¢ƒæ¨ç†æ¨¡å—ï¼Œåˆ©ç”¨å¸¸è¯†çŸ¥è¯†å’Œæƒ…å¢ƒä¿¡æ¯ï¼Œå®ç°æœ‰æ•ˆçš„ä¸‰ç»´ç¯å¢ƒæ¢ç´¢å’Œç›®æ ‡å¯¹è±¡è¯†åˆ«ï¼›ï¼ˆiiï¼‰è¿­ä»£é—­ç¯åé¦ˆå’Œæ¢å¤æœºåˆ¶ï¼ŒéªŒè¯ä»»åŠ¡å¯è¡Œæ€§ï¼Œç›‘æ§æ‰§è¡ŒæˆåŠŸä¸å¦ï¼Œå¹¶è·Ÿè¸ªä¸åŒæ¨¡å—çš„å¤±è´¥åŸå› ï¼Œä»¥å®ç°ç¨³å¥çš„æ•…éšœæ¢å¤ã€‚é€šè¿‡å¯¹æ¶‰åŠ8é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ç°å®ç§»åŠ¨æ“ä½œå’Œæ¡Œé¢æ“æ§ä»»åŠ¡çš„å…¨é¢å®éªŒï¼ŒCOME-robotä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œåœ¨ä»»åŠ¡æˆåŠŸç‡æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æé«˜ï¼ˆçº¦35%ï¼‰ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å…¨é¢çš„åˆ†æï¼Œä»¥é˜æ˜COME-robotçš„è®¾è®¡å¦‚ä½•ä¿ƒè¿›æ•…éšœæ¢å¤ã€è‡ªç”±å½¢å¼çš„æŒ‡ä»¤éµå¾ªå’Œé•¿æœŸä»»åŠ¡è§„åˆ’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.10220v2">PDF</a> 6 pages, Accepted at 2025 IEEE ICRA, website:   <a target="_blank" rel="noopener" href="https://come-robot.github.io/">https://come-robot.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºGPT-4Vè§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹çš„é—­ç¯æœºå™¨äººç³»ç»ŸCOME-robotï¼Œå®ç°äº†å¼€æ”¾ç¯å¢ƒä¸‹çš„è‡ªä¸»æœºå™¨äººå¯¼èˆªå’Œæ“æ§ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ä¸¤çº§åˆ›æ–°æ¨¡å—å®ç°å¼€æ”¾å¼è¯æ±‡æ„ŸçŸ¥å’Œæƒ…å¢ƒæ¨ç†ï¼Œä»¥åŠè¿­ä»£å¼é—­ç¯åé¦ˆå’Œæ¢å¤æœºåˆ¶ï¼Œæœ‰æ•ˆæé«˜äº†ä»»åŠ¡æˆåŠŸç‡ã€‚åœ¨åŒ…å«å¤šç§æŒ‘æˆ˜æ€§ç§»åŠ¨å’Œæ¡Œé¢æ“ä½œä»»åŠ¡çš„å®éªŒä¸­ï¼Œä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒCOME-robotçš„ä»»åŠ¡æˆåŠŸç‡æ˜¾è‘—æé«˜çº¦35%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>COME-robotæ˜¯é¦–ä¸ªåˆ©ç”¨GPT-4Vè§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹è¿›è¡Œå¼€æ”¾å¼æ¨ç†å’Œè‡ªé€‚åº”è§„åˆ’çš„é—­ç¯æœºå™¨äººç³»ç»Ÿã€‚</li>
<li>COME-robotå…·æœ‰ä¸¤çº§åˆ›æ–°æ¨¡å—ï¼šä¸€çº§æ˜¯å¤šçº§å¼€æ”¾å¼è¯æ±‡æ„ŸçŸ¥å’Œæƒ…å¢ƒæ¨ç†æ¨¡å—ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæœ‰æ•ˆåœ°æ¢ç´¢ä¸‰ç»´ç¯å¢ƒå¹¶è¯†åˆ«ç›®æ ‡ç‰©ä½“ï¼›å¦ä¸€çº§æ˜¯è¿­ä»£å¼é—­ç¯åé¦ˆå’Œæ¢å¤æœºåˆ¶ï¼Œç”¨äºéªŒè¯ä»»åŠ¡çš„å¯è¡Œæ€§ã€ç›‘æ§æ‰§è¡ŒæˆåŠŸæƒ…å†µå¹¶è¿½è¸ªä¸åŒæ¨¡å—çš„å¤±è´¥åŸå› ï¼Œä»è€Œå®ç°ç¨³å¥çš„æ•…éšœæ¢å¤ã€‚</li>
<li>COME-robotåœ¨å¤šç§æŒ‘æˆ˜æ€§ç§»åŠ¨å’Œæ¡Œé¢æ“ä½œä»»åŠ¡ä¸­å±•ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä»»åŠ¡æˆåŠŸç‡æé«˜çº¦35%ã€‚</li>
<li>COME-robotçš„è®¾è®¡æœ‰åŠ©äºæ•…éšœæ¢å¤ã€è‡ªç”±å½¢å¼æŒ‡ä»¤è·Ÿéšå’Œé•¿æœŸä»»åŠ¡è§„åˆ’ã€‚</li>
<li>å¤šçº§å¼€æ”¾å¼è¯æ±‡æ„ŸçŸ¥å’Œæƒ…å¢ƒæ¨ç†æ¨¡å—ä½¿å¾—æœºå™¨äººèƒ½å¤Ÿåˆ©ç”¨å¸¸è¯†çŸ¥è¯†å’Œæƒ…å¢ƒä¿¡æ¯è¿›è¡Œæœ‰æ•ˆçš„ç¯å¢ƒæ¢ç´¢å’Œç‰©ä½“è¯†åˆ«ã€‚</li>
<li>è¿­ä»£å¼é—­ç¯åé¦ˆå’Œæ¢å¤æœºåˆ¶ç¡®ä¿æœºå™¨äººåœ¨æ‰§è¡Œå¤æ‚ä»»åŠ¡æ—¶èƒ½å¤Ÿå®æ—¶ç›‘æ§å¹¶è°ƒæ•´ï¼Œä»¥å®ç°æ›´é«˜çš„é²æ£’æ€§å’Œé€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.10220">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-307a85ab3bb86d778d1ffc276c372e51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a8722611880acac448e2ea8ef2bfbbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03123ac6e50b23526f60d30b231fdc4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0c21b754e3cbae1672937fe8662c431.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f062968810f260296d853e489ee6033.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="GraphEdit-Large-Language-Models-for-Graph-Structure-Learning"><a href="#GraphEdit-Large-Language-Models-for-Graph-Structure-Learning" class="headerlink" title="GraphEdit: Large Language Models for Graph Structure Learning"></a>GraphEdit: Large Language Models for Graph Structure Learning</h2><p><strong>Authors:Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Kangkang Lu, Zhiyong Huang, Chao Huang</strong></p>
<p>Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies and interactions among nodes in graph-structured data by generating novel graph structures. Graph Neural Networks (GNNs) have emerged as promising GSL solutions, utilizing recursive message passing to encode node-wise inter-dependencies. However, many existing GSL methods heavily depend on explicit graph structural information as supervision signals, leaving them susceptible to challenges such as data noise and sparsity. In this work, we propose GraphEdit, an approach that leverages large language models (LLMs) to learn complex node relationships in graph-structured data. By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning. Our approach not only effectively denoises noisy connections but also identifies node-wise dependencies from a global perspective, providing a comprehensive understanding of the graph structure. We conduct extensive experiments on multiple benchmark datasets to demonstrate the effectiveness and robustness of GraphEdit across various settings. We have made our model implementation available at: <a target="_blank" rel="noopener" href="https://github.com/HKUDS/GraphEdit">https://github.com/HKUDS/GraphEdit</a>. </p>
<blockquote>
<p>å›¾ç»“æ„å­¦ä¹ ï¼ˆGSLï¼‰æ—¨åœ¨é€šè¿‡ç”Ÿæˆæ–°çš„å›¾ç»“æ„æ¥æ•æ‰å›¾ç»“æ„æ•°æ®ä¸­èŠ‚ç‚¹ä¹‹é—´çš„å†…åœ¨ä¾èµ–æ€§å’Œäº¤äº’ä½œç”¨ã€‚å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„GSLè§£å†³æ–¹æ¡ˆå‡ºç°ï¼Œå®ƒåˆ©ç”¨é€’å½’æ¶ˆæ¯ä¼ é€’æ¥ç¼–ç èŠ‚ç‚¹é—´çš„ä¾èµ–æ€§ã€‚ç„¶è€Œï¼Œè®¸å¤šç°æœ‰çš„GSLæ–¹æ³•ä¸¥é‡ä¾èµ–äºæ˜ç¡®çš„å›¾ç»“æ„ä¿¡æ¯ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œè¿™ä½¿å¾—å®ƒä»¬å®¹æ˜“å—åˆ°æ•°æ®å™ªå£°å’Œç¨€ç–æ€§ç­‰çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†GraphEditæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å­¦ä¹ å›¾ç»“æ„æ•°æ®ä¸­çš„å¤æ‚èŠ‚ç‚¹å…³ç³»ã€‚é€šè¿‡æŒ‡ä»¤è°ƒä¼˜å›¾ç»“æ„å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬æ—¨åœ¨å…‹æœä¸æ˜ç¡®çš„å›¾ç»“æ„ä¿¡æ¯ç›¸å…³çš„å±€é™æ€§ï¼Œæé«˜å›¾ç»“æ„å­¦ä¹ çš„å¯é æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æœ‰æ•ˆåœ°æ¶ˆé™¤äº†å™ªå£°è¿æ¥ï¼Œè¿˜ä»å…¨å±€è§’åº¦è¯†åˆ«èŠ‚ç‚¹ä¾èµ–å…³ç³»ï¼Œä¸ºå›¾ç»“æ„æä¾›äº†å…¨é¢çš„ç†è§£ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œä»¥è¯æ˜GraphEditåœ¨å„ç§è®¾ç½®ä¸‹çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹å®ç°å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/HKUDS/GraphEdit%E3%80%82">https://github.com/HKUDS/GraphEditã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.15183v5">PDF</a> </p>
<p><strong>Summary</strong><br>GraphEditåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­¦ä¹ å›¾ç»“æ„æ•°æ®ä¸­çš„å¤æ‚èŠ‚ç‚¹å…³ç³»ï¼Œé€šè¿‡æŒ‡ä»¤è°ƒæ•´å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ï¼Œæ—¨åœ¨å…‹æœä¸æ˜¾å¼å›¾ç»“æ„ä¿¡æ¯ç›¸å…³çš„å±€é™æ€§ï¼Œæé«˜å›¾ç»“æ„å­¦ä¹ çš„å¯é æ€§ã€‚æ­¤æ–¹æ³•ä¸ä»…èƒ½æœ‰æ•ˆå»é™¤å™ªå£°è¿æ¥ï¼Œè¿˜èƒ½ä»å…¨å±€è§’åº¦è¯†åˆ«èŠ‚ç‚¹ä¾èµ–å…³ç³»ï¼Œä¸ºå›¾ç»“æ„æä¾›å…¨é¢çš„ç†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GraphEditä¸“æ³¨äºé€šè¿‡ç”Ÿæˆæ–°å‹å›¾ç»“æ„æ¥æ•æ‰å›¾ç»“æ„æ•°æ®ä¸­èŠ‚ç‚¹ä¹‹é—´çš„å†…åœ¨ä¾èµ–æ€§å’Œäº¤äº’ã€‚</li>
<li>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ä½œä¸ºå›¾ç»“æ„å­¦ä¹ ï¼ˆGSLï¼‰çš„è§£å†³æ–¹æ³•ï¼Œé€šè¿‡é€’å½’æ¶ˆæ¯ä¼ é€’æ¥ç¼–ç èŠ‚ç‚¹é—´çš„ä¾èµ–æ€§ã€‚</li>
<li>è®¸å¤šç°æœ‰çš„GSLæ–¹æ³•ä¸¥é‡ä¾èµ–äºæ˜ç¡®çš„å›¾ç»“æ„ä¿¡æ¯ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œè¿™ä½¿å…¶é¢ä¸´æ•°æ®å™ªå£°å’Œç¨€ç–æ€§æŒ‘æˆ˜ã€‚</li>
<li>GraphEditåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å­¦ä¹ å¤æ‚çš„èŠ‚ç‚¹å…³ç³»ï¼Œå¹¶æ—¨åœ¨å…‹æœè¿™äº›å±€é™æ€§ã€‚</li>
<li>é€šè¿‡æŒ‡ä»¤è°ƒæ•´å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ï¼ŒGraphEditä¸ä»…èƒ½æœ‰æ•ˆå»é™¤å™ªå£°è¿æ¥ï¼Œè¿˜èƒ½ä»å…¨å±€è§’åº¦ç†è§£å›¾ç»“æ„ã€‚</li>
<li>GraphEditåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒè®¾ç½®ä¸‹çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.15183">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-623fa890107d452268fe1f877bcbf7bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b6344b2168562c1a3761c012e5ffec6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-700bf5be04a2adf0e84bebb45e0ffe78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ad1356fe9f5aa8042506417866c461a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-95dd68802936f42cc6758a73e6110017.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46b68490bd665eb15683e1cbc6a3a909.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-16/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-16/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-16/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4d56c02d4d522dde64ae67f260834dc1.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-16  StereoCrafter-Zero Zero-Shot Stereo Video Generation with Noisy Restart
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-16/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-7036ff4da4fb676bcedd4eeb47950e21.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-16  GoT Unleashing Reasoning Capability of Multimodal Large Language Model   for Visual Generation and Editing
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32251.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
