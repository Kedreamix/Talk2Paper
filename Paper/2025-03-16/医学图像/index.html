<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-03-16  Quenching and recovery of persistent X-ray emission during a superburst   in 4U 1820$-$30">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-be9edefd2530b672c1b1316199272d95.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    40 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-16-更新"><a href="#2025-03-16-更新" class="headerlink" title="2025-03-16 更新"></a>2025-03-16 更新</h1><h2 id="Quenching-and-recovery-of-persistent-X-ray-emission-during-a-superburst-in-4U-1820-30"><a href="#Quenching-and-recovery-of-persistent-X-ray-emission-during-a-superburst-in-4U-1820-30" class="headerlink" title="Quenching and recovery of persistent X-ray emission during a superburst   in 4U 1820$-$30"></a>Quenching and recovery of persistent X-ray emission during a superburst   in 4U 1820$-$30</h2><p><strong>Authors:Zhijiao Peng, Zhaosheng Li, Yuanyue Pan, Tao Fu, Wenhui Yu, Yupeng Chen, Shu Zhang, Maurizio Falanga, Shuang-Nan Zhang</strong></p>
<p>We report the superburst from 4U 1820–30 in 2021 observed by the Monitor of All-sky X-ray Image and Neutron star Interior Composition Explorer (NICER). During the tail of the superburst, we found that the NICER light curve unexpectedly increased from 1080 to 2204 ${\rm counts<del>s^{-1}}$ over 6.89 hr. From the time-resolved superburst spectra, we estimated the burst decay time of $\approx2.5$ hr, the ignition column depth of $\approx0.3\times 10^{12}</del>{\rm g <del>cm^{-2}}$, the energy release per unit mass of $\approx2.4\times 10^{17}</del>{\rm erg<del>g^{-1}}$, the fluence of $\approx4.1\times 10^{-4}</del>{\rm erg<del>cm^{-2}}$, and the total energy release of $\approx3.5\times10^{42}$ erg. Notably, we found a gradual increase in the Componization flux from $8.9\times 10^{-10}</del>{\rm erg<del>s^{-1}</del>cm^{-2}}$ to the preburst level during the superburst. This increase can be interpreted as a consequence of superburst radiation depleting the inner accretion disk, leading to a near-complete quenching of the persistent emission. As the burst radiation decayed, the inner accretion disk gradually returned to its preburst state, as evidenced by the best-fit spectral parameters. Additionally, we observed a prominent absorption line that exhibited a gravitational redshift, shifting from 4.15 to 3.62 keV during the recovery phase of persistent emission. This absorption feature likely originates from the inner accretion disk rather than from burst emission on the neutron star (NS) surface. The observed changes in the absorption line energy suggest that the inner disk approached the NS to a distance as close as $\approx17$ km. </p>
<blockquote>
<p>我们报告了利用全天空X射线图像监视器和中子星内部结构探测器（NICER）观测到的来自4U 1820-30的超超爆发事件。在超爆发的尾部，我们发现NICER的光度曲线在长达6.89小时内意外从每秒的1080计数上升至每秒的2204计数。根据时间解析的超爆发光谱，我们估计了大约2.5小时的爆发衰减时间、大约深度为$0.3\times 10^{12}$克每立方厘米的点火柱、每单位质量的能量释放约为$2.4\times 10^{17}$尔格每克、流强度约为$4.1\times 成分流量的逐渐增加可解读为超爆发辐射耗尽了内盘积物质，导致持久性发射几乎完全熄灭。随着爆发辐射的衰减，内盘积物质逐渐恢复到爆发前的状态，最佳拟合光谱参数可以证明这一点。此外，我们观察到一条明显的吸收线展现出重力红移现象，在持久性发射的恢复阶段从4.15 keV降至3.62 keV。这一吸收特征可能源自内盘积物质而非中子星表面上的爆发发射。观察到的吸收线能量的变化表明内盘接近中子星至约17公里的距离。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05785v2">PDF</a> published in ApJ</p>
<p><strong>Summary</strong></p>
<p>文中报道了利用全天空X射线图像监视器和中子星内部结构探测器（NICER）观测到的来自4U 1820-30的超爆发事件。超爆发尾期出现意外的光变曲线增长，且光谱特征揭示了一系列重要参数和现象。包括超爆发的衰减时间、点火柱深度、单位质量的能量释放等参数的估算，以及持久发射逐渐被超级爆发辐射耗尽后的恢复过程，都展现了中子星及其环境的复杂行为。文中还观察到由重力红移产生的显著吸收线，其能量的变化表明内盘靠近中子星的距离达到了近似值。此超爆发事件揭示了中子星内部结构和物理过程的宝贵信息。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>2021年利用Monitor of All-sky X-ray Image和NICER观测了来自4U 1820-30的超爆发事件。</li>
<li>超爆发尾期观察到NICER光变曲线意外增长。</li>
<li>通过时间解析超爆发光谱，估算了超爆发的衰减时间、点火柱深度等参数。</li>
<li>观察到持久发射在超爆发过程中逐渐减弱，并在超爆发后恢复期逐渐恢复的现象。</li>
<li>揭示了超爆发辐射可能耗尽内积盘，导致持久发射几乎完全熄灭的解释。</li>
<li>恢复阶段的显著吸收线展示重力红移，表明内盘可能靠近中子星至近距离。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05785">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-22adf6c2815d8d6a31069f19d78794eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf9d44fc102054d42c87fb52f5fc372c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-577bcd74a88b5b2f36bce31c17755501.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be9edefd2530b672c1b1316199272d95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30431c83b9d87ab283711b7bc5024bb6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fa28b0c8cb071210153c4b14e0ccd828.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90ab0ee2060f9a37d85d2165a94801be.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Continuous-K-space-Recovery-Network-with-Image-Guidance-for-Fast-MRI-Reconstruction"><a href="#Continuous-K-space-Recovery-Network-with-Image-Guidance-for-Fast-MRI-Reconstruction" class="headerlink" title="Continuous K-space Recovery Network with Image Guidance for Fast MRI   Reconstruction"></a>Continuous K-space Recovery Network with Image Guidance for Fast MRI   Reconstruction</h2><p><strong>Authors:Yucong Meng, Zhiwei Yang, Minghong Duan, Yonghong Shi, Zhijian Song</strong></p>
<p>Magnetic resonance imaging (MRI) is a crucial tool for clinical diagnosis while facing the challenge of long scanning time. To reduce the acquisition time, fast MRI reconstruction aims to restore high-quality images from the undersampled k-space. Existing methods typically train deep learning models to map the undersampled data to artifact-free MRI images. However, these studies often overlook the unique properties of k-space and directly apply general networks designed for image processing to k-space recovery, leaving the precise learning of k-space largely underexplored. In this work, we propose a continuous k-space recovery network from a new perspective of implicit neural representation with image domain guidance, which boosts the performance of MRI reconstruction. Specifically, (1) an implicit neural representation based encoder-decoder structure is customized to continuously query unsampled k-values. (2) an image guidance module is designed to mine the semantic information from the low-quality MRI images to further guide the k-space recovery. (3) a multi-stage training strategy is proposed to recover dense k-space progressively. Extensive experiments conducted on CC359, fastMRI, and IXI datasets demonstrate the effectiveness of our method and its superiority over other competitors. </p>
<blockquote>
<p>磁共振成像（MRI）是临床诊断中的重要工具，但同时也面临着扫描时间长这一挑战。为了缩短采集时间，快速MRI重建旨在从欠采样的k空间中恢复高质量图像。现有方法通常训练深度学习模型，将欠采样数据映射到无伪影的MRI图像。然而，这些研究往往忽视了k空间的独特属性，并直接使用为图像处理设计的通用网络进行k空间恢复，使得k空间的精确学习在很大程度上被忽视。在这项工作中，我们从隐式神经表示的新角度提出了一个连续的k空间恢复网络，并结合图像域指导提高了MRI重建的性能。具体来说，（1）基于隐式神经表示的编码器-解码器结构被定制为连续查询未采样的k值。（2）设计了一个图像引导模块，从低质量的MRI图像中提取语义信息，进一步指导k空间的恢复。（3）提出了一种多阶段训练策略，以逐步恢复密集的k空间。在CC359、fastMRI和IXI数据集上进行的广泛实验证明了我们的方法的有效性及其对其他竞争对手的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11282v2">PDF</a> </p>
<p><strong>Summary</strong><br>     本研究提出了基于隐式神经表示的新视角的连续k-空间恢复网络，用于加速磁共振成像（MRI）重建。该网络结合图像域指导，提升了MRI重建性能。通过采用隐式神经表示编码器-解码器结构、设计图像指导模块以及采用多阶段训练策略，实现了对未采样k值的连续查询和密集k-空间的逐步恢复。在CC359、fastMRI和IXI数据集上的实验证明了该方法的有效性及优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>磁共振成像（MRI）面临长时间扫描的挑战，快速MRI重建旨在从欠采样的k-空间恢复高质量图像。</li>
<li>现有方法通常训练深度学习模型将欠采样数据映射到无伪影的MRI图像，但忽略了k-空间的独特属性。</li>
<li>本研究从隐式神经表示的新视角提出了连续k-空间恢复网络，可实现对未采样k值的连续查询。</li>
<li>设计中包含了图像域指导模块，挖掘低质量MRI图像中的语义信息以指导k-空间恢复。</li>
<li>研究采用多阶段训练策略，旨在逐步恢复密集k-空间。</li>
<li>在多个数据集上的实验证明了该方法的有效性及优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11282">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-01c5ab4d24a3068d487ca04b0dc92c6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3f85ffe5a51b812219e1d887cf73f35.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3aa89e39a6468eb19e893ad341771cf6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa21df63ddc1babe8a7d98fa0f3c202b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-733908e8baaa4e1988ce290a080e278a.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CAD-MLLM-Unifying-Multimodality-Conditioned-CAD-Generation-With-MLLM"><a href="#CAD-MLLM-Unifying-Multimodality-Conditioned-CAD-Generation-With-MLLM" class="headerlink" title="CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM"></a>CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM</h2><p><strong>Authors:Jingwei Xu, Zibo Zhao, Chenyu Wang, Wen Liu, Yi Ma, Shenghua Gao</strong></p>
<p>This paper aims to design a unified Computer-Aided Design (CAD) generation system that can easily generate CAD models based on the user’s inputs in the form of textual description, images, point clouds, or even a combination of them. Towards this goal, we introduce the CAD-MLLM, the first system capable of generating parametric CAD models conditioned on the multimodal input. Specifically, within the CAD-MLLM framework, we leverage the command sequences of CAD models and then employ advanced large language models (LLMs) to align the feature space across these diverse multi-modalities data and CAD models’ vectorized representations. To facilitate the model training, we design a comprehensive data construction and annotation pipeline that equips each CAD model with corresponding multimodal data. Our resulting dataset, named Omni-CAD, is the first multimodal CAD dataset that contains textual description, multi-view images, points, and command sequence for each CAD model. It contains approximately 450K instances and their CAD construction sequences. To thoroughly evaluate the quality of our generated CAD models, we go beyond current evaluation metrics that focus on reconstruction quality by introducing additional metrics that assess topology quality and surface enclosure extent. Extensive experimental results demonstrate that CAD-MLLM significantly outperforms existing conditional generative methods and remains highly robust to noises and missing points. The project page and more visualizations can be found at: <a target="_blank" rel="noopener" href="https://cad-mllm.github.io/">https://cad-mllm.github.io/</a> </p>
<blockquote>
<p>本文旨在设计一个统一的计算机辅助设计（CAD）生成系统，该系统能够根据用户的文本描述、图像、点云或它们的组合等输入形式，轻松生成CAD模型。为实现这一目标，我们引入了CAD-MLLM系统，这是第一个能够根据多模式输入生成参数化CAD模型的系统。具体而言，在CAD-MLLM框架内，我们利用CAD模型的命令序列，然后采用先进的大型语言模型（LLM），以对齐这些多样化的多模式数据以及CAD模型的向量表示之间的特征空间。为了促进模型训练，我们设计了一个全面的数据构建和注释管道，为每个CAD模型配备相应的多模式数据。我们构建的数据集名为Omni-CAD，这是第一个多模式CAD数据集，包含每个CAD模型的文本描述、多视图图像、点和命令序列。它包含大约45万个实例及其CAD构建序列。为了全面评估我们生成的CAD模型的质量，我们超越了当前以重建质量为中心的评估指标，引入了其他评估拓扑质量和表面封闭程度的指标。大量的实验结果表明，CAD-MLLM显著优于现有的条件生成方法，并且对噪声和缺失点具有高度鲁棒性。项目页面和更多可视化内容可在：<a target="_blank" rel="noopener" href="https://cad-mllm.github.io/%E6%89%BE%E5%88%B0%E3%80%82">https://cad-mllm.github.io/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04954v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://cad-mllm.github.io/">https://cad-mllm.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个统一计算机辅助设计（CAD）生成系统，该系统能够根据用户的文本描述、图像、点云等多种输入形式，轻松生成CAD模型。为达成此目标，引入CAD-MLLM系统，该系统能够基于多模态输入生成参数化CAD模型。通过设计综合数据构建和标注管道，为每个CAD模型配备对应的多模态数据，创建名为Omni-CAD的多模态CAD数据集。此外，还引入了拓扑质量和表面封闭程度等评估指标来全面评估生成的CAD模型质量。实验结果表明，CAD-MLLM显著优于现有条件生成方法，对噪声和缺失点具有高度的鲁棒性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文设计了一个统一的CAD生成系统，能够根据用户的多种输入形式（文本描述、图像、点云等）轻松生成CAD模型。</li>
<li>引入了CAD-MLLM系统，能够基于多模态输入生成参数化CAD模型。</li>
<li>创建了名为Omni-CAD的多模态CAD数据集，包含文本描述、多视角图像、点云和CAD模型的命令序列。</li>
<li>为评估生成的CAD模型质量，引入了拓扑质量和表面封闭程度的评估指标。</li>
<li>CAD-MLLM系统显著优于现有的条件生成方法。</li>
<li>CAD-MLLM系统对噪声和缺失点具有高度的鲁棒性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04954">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7088fbcd552e1bf2507a0edf0ee73286.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b2ecdff10c7d53be81da25427c5b376.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-689a6a8af7d9ef00a7a19d214791ca36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48d1788ce71122afadf4360588aff38d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cc2e3d3875f434e1813e6e0307bdc627.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-155a346409c29e6a0e7d4a2355c4db36.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LLM-HDR-Bridging-LLM-based-Perception-and-Self-Supervision-for-Unpaired-LDR-to-HDR-Image-Reconstruction"><a href="#LLM-HDR-Bridging-LLM-based-Perception-and-Self-Supervision-for-Unpaired-LDR-to-HDR-Image-Reconstruction" class="headerlink" title="LLM-HDR: Bridging LLM-based Perception and Self-Supervision for Unpaired   LDR-to-HDR Image Reconstruction"></a>LLM-HDR: Bridging LLM-based Perception and Self-Supervision for Unpaired   LDR-to-HDR Image Reconstruction</h2><p><strong>Authors:Hrishav Bakul Barua, Kalin Stefanov, Lemuel Lai En Che, Abhinav Dhall, KokSheik Wong, Ganesh Krishnasamy</strong></p>
<p>The translation of Low Dynamic Range (LDR) to High Dynamic Range (HDR) images is an important computer vision task. There is a significant amount of research utilizing both conventional non-learning methods and modern data-driven approaches, focusing on using both single-exposed and multi-exposed LDR for HDR image reconstruction. However, most current state-of-the-art methods require high-quality paired {LDR,HDR} datasets for model training. In addition, there is limited literature on using unpaired datasets for this task, that is, the model learns a mapping between domains, i.e., {LDR,HDR}. This paper proposes LLM-HDR, a method that integrates the perception of Large Language Models (LLM) into a modified semantic- and cycle-consistent adversarial architecture that utilizes unpaired {LDR,HDR} datasets for training. The method introduces novel artifact- and exposure-aware generators to address visual artifact removal and an encoder and loss to address semantic consistency, another under-explored topic. LLM-HDR is the first to use an LLM for the {LDR,HDR} translation task in a self-supervised setup. The method achieves state-of-the-art performance across several benchmark datasets and reconstructs high-quality HDR images. The official website of this work is available at: <a target="_blank" rel="noopener" href="https://github.com/HrishavBakulBarua/LLM-HDR">https://github.com/HrishavBakulBarua/LLM-HDR</a> </p>
<blockquote>
<p>从低动态范围（LDR）图像翻译到高动态范围（HDR）图像是一项重要的计算机视觉任务。许多研究都利用传统的非学习方法和现代的数据驱动方法，侧重于使用单曝光和多曝光的LDR进行HDR图像重建。然而，目前大多数最先进的方法都需要高质量配对的{LDR，HDR}数据集来进行模型训练。此外，关于使用未配对数据集进行此任务的文献很少，也就是说，模型学习域之间的映射，即{LDR，HDR}。本文提出了LLM-HDR方法，它将大型语言模型（LLM）的感知能力集成到一个经过修改的语义和循环一致的对抗性架构中，该架构利用未配对的{LDR，HDR}数据集进行训练。该方法引入了新型伪影和曝光感知生成器来解决视觉伪影去除问题，以及解决语义一致性这一尚未被深入探讨的话题的编码器和损失函数。LLM-HDR是第一个在自监督设置中使用LLM进行{LDR，HDR}翻译任务的方法。该方法在多个基准数据集上达到了最先进的性能，并重建了高质量的HDR图像。该工作的官方网站地址为：<a target="_blank" rel="noopener" href="https://github.com/HrishavBakulBarua/LLM-HDR">https://github.com/HrishavBakulBarua/LLM-HDR</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15068v2">PDF</a> </p>
<p><strong>Summary</strong><br>    本文提出一种名为LLM-HDR的方法，它将大型语言模型（LLM）感知融入修改后的语义和循环一致对抗架构中，利用无配对{LDR，HDR}数据集进行训练。该方法引入新型伪影和曝光感知生成器以解决视觉伪影去除问题，并解决了语义一致性的另一个未探讨话题。LLM-HDR是首个在自监督设置中利用LLM进行{LDR，HDR}翻译任务的方法。该方法在多个基准数据集上实现了最先进的性能，并能重建高质量HDR图像。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLM-HDR方法结合了大型语言模型（LLM）感知技术。</li>
<li>方法采用修改后的语义和循环一致对抗架构。</li>
<li>该方法利用无配对{LDR，HDR}数据集进行训练。</li>
<li>LLM-HDR引入伪影和曝光感知生成器解决视觉伪影问题。</li>
<li>方法解决了语义一致性的未探讨话题。</li>
<li>LLM-HDR是首个在自监督设置中应用LLM进行HDR图像翻译的方法。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15068">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ed0943f3f67423ae0c92ddaeb82166ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c796c8b662950542046b576ea2142a76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f7d1f0997d46770f2f236f98bb63377.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Co-learning-Single-Step-Diffusion-Upsampler-and-Downsampler-with-Two-Discriminators-and-Distillation"><a href="#Co-learning-Single-Step-Diffusion-Upsampler-and-Downsampler-with-Two-Discriminators-and-Distillation" class="headerlink" title="Co-learning Single-Step Diffusion Upsampler and Downsampler with Two   Discriminators and Distillation"></a>Co-learning Single-Step Diffusion Upsampler and Downsampler with Two   Discriminators and Distillation</h2><p><strong>Authors:Sohwi Kim, Tae-Kyun Kim</strong></p>
<p>Super-resolution (SR) aims to reconstruct high-resolution (HR) images from their low-resolution (LR) counterparts, often relying on effective downsampling to generate diverse and realistic training pairs. In this work, we propose a co-learning framework that jointly optimizes a single-step diffusion-based upsampler and a learnable downsampler, enhanced by two discriminators and a cyclic distillation strategy. Our learnable downsampler is designed to better capture realistic degradation patterns while preserving structural details in the LR domain, which is crucial for enhancing SR performance. By leveraging a diffusion-based approach, our model generates diverse LR-HR pairs during training, enabling robust learning across varying degradations. We demonstrate the effectiveness of our method on both general real-world and domain-specific face SR tasks, achieving state-of-the-art performance in both fidelity and perceptual quality. Our approach not only improves efficiency with a single inference step but also ensures high-quality image reconstruction, bridging the gap between synthetic and real-world SR scenarios. </p>
<blockquote>
<p>超分辨率（SR）旨在从低分辨率（LR）图像重建高分辨率（HR）图像，通常依赖于有效的下采样来生成多样且现实的训练对。在这项工作中，我们提出了一种联合优化一步扩散式上采样器和可学习下采样器的协同学习框架，它由两个鉴别器和循环蒸馏策略增强。我们的可学习下采样器旨在更好地捕捉现实的退化模式，同时保留低分辨率域中的结构细节，这对于提高超分辨率性能至关重要。通过利用基于扩散的方法，我们的模型在训练过程中生成多样的LR-HR对，使模型能够在各种退化情况下进行稳健学习。我们在一般的现实世界和特定领域的面部SR任务上都证明了我们的方法的有效性，在保真度和感知质量方面都达到了最新性能。我们的方法不仅通过一次推理步骤提高了效率，而且确保了高质量的图像重建，缩小了合成和现实世界SR场景之间的差距。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07663v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种联合优化单步扩散式上采样器和可学习下采样器的协同学习框架，通过两个鉴别器和循环蒸馏策略进行增强。可学习下采样器能够更好捕捉现实退化模式，同时在低分辨率领域保留结构细节，对提升超分辨率性能至关重要。利用扩散式方法，该模型在训练过程中生成多样的低分辨率-高分辨率图像对，使模型能够在各种退化情况下进行稳健学习。在通用现实世界和特定领域的面部超分辨率任务上，该方法取得了先进性能，在保真度和感知质量方面均表现优异。此方法不仅提高了效率，只需单步推理，而且保证了高质量图像重建，缩小了合成和真实世界超分辨率场景之间的差距。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一个联合优化单步扩散式上采样器和可学习下采样器的协同学习框架。</li>
<li>可学习下采样器能捕捉现实退化模式并保留低分辨率领域的结构细节。</li>
<li>利用扩散式方法生成多样的低分辨率-高分辨率图像对，提升模型的稳健性。</li>
<li>在面部超分辨率任务上取得了先进性能。</li>
<li>方法提高了效率，只需单步推理。</li>
<li>保证了高质量图像重建。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.07663">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cbcb729ba1d1f944a7aad21a46ad84b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9d39eebd9daafa28fc17aa0d2cd8d8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ec83d9c00fde7426985c6d2de4fbdd7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64437cde0ef2285269cce6e541833750.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-84ecd6186390d0134d3742512ac0a5f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74d7e08c8950d604cd0cf2cec7aef7c4.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Brain-Tumor-Classification-on-MRI-in-Light-of-Molecular-Markers"><a href="#Brain-Tumor-Classification-on-MRI-in-Light-of-Molecular-Markers" class="headerlink" title="Brain Tumor Classification on MRI in Light of Molecular Markers"></a>Brain Tumor Classification on MRI in Light of Molecular Markers</h2><p><strong>Authors:Jun Liu, Geng Yuan, Weihao Zeng, Hao Tang, Wenbin Zhang, Xue Lin, XiaoLin Xu, Dong Huang, Yanzhi Wang</strong></p>
<p>In research findings, co-deletion of the 1p&#x2F;19q gene is associated with clinical outcomes in low-grade gliomas. The ability to predict 1p19q status is critical for treatment planning and patient follow-up. This study aims to utilize a specially MRI-based convolutional neural network for brain cancer detection. Although public networks such as RestNet and AlexNet can effectively diagnose brain cancers using transfer learning, the model includes quite a few weights that have nothing to do with medical images. As a result, the diagnostic results are unreliable by the transfer learning model. To deal with the problem of trustworthiness, we create the model from the ground up, rather than depending on a pre-trained model. To enable flexibility, we combined convolution stacking with a dropout and full connect operation, it improved performance by reducing overfitting. During model training, we also supplement the given dataset and inject Gaussian noise. We use three–fold cross-validation to train the best selection model. Comparing InceptionV3, VGG16, and MobileNetV2 fine-tuned with pre-trained models, our model produces better results. On an validation set of 125 codeletion vs. 31 not codeletion images, the proposed network achieves 96.37% percent F1-score, 97.46% percent precision, and 96.34% percent recall when classifying 1p&#x2F;19q codeletion and not codeletion images. </p>
<blockquote>
<p>在研究过程中发现，1p&#x2F;19q基因的联合缺失与低级别胶质瘤的临床结果有关。预测1p19q状态对于治疗计划和患者随访至关重要。本研究旨在利用基于MRI的卷积神经网络进行脑癌检测。虽然RestNet和AlexNet等公共网络可以通过迁移学习有效地诊断脑癌，但模型中有很多权重与医学图像无关。因此，迁移学习模型的诊断结果不可靠。为了解决可信度问题，我们从零开始构建模型，而不是依赖于预训练模型。为了实现灵活性，我们将卷积堆叠与丢弃和完全连接操作相结合，通过减少过拟合提高了性能。在模型训练过程中，我们还补充了给定的数据集并注入了高斯噪声。我们使用三折交叉验证来训练最佳选择模型。将InceptionV3、VGG16和MobileNetV2与预训练模型微调相比，我们的模型产生了更好的结果。在125张代码缺失与31张非代码缺失图像的验证集上，所提出的网络在分类1p&#x2F;19q代码缺失和非代码缺失图像时，达到了96.37%的F1分数、97.46%的精确度和96.34%的召回率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.19583v2">PDF</a> ICAI’22 - The 24th International Conference on Artificial   Intelligence, The 2022 World Congress in Computer Science, Computer   Engineering, &amp; Applied Computing (CSCE’22), Las Vegas, USA. The paper   acceptance rate 17% for regular papers. The publication of the CSCE 2022   conference proceedings has been delayed due to the pandemic</p>
<p><strong>Summary</strong></p>
<p>该研究探讨了通过磁共振成像技术构建的卷积神经网络对低级胶质瘤进行诊断的方法。针对现有转移学习模型存在的可靠性问题，研究团队构建了全新的模型，该模型通过卷积堆叠、dropout操作和全连接操作相结合的方式提高了性能并降低了过拟合现象。在模型训练过程中，研究团队还补充了数据集并加入了高斯噪声以增强模型的灵活性。最终，该模型在验证集上取得了较高的分类性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究发现共同缺失的1p&#x2F;19q基因与低级别胶质瘤的临床结果有关。</li>
<li>预测1p&#x2F;19q状态对治疗计划和患者随访至关重要。</li>
<li>研究旨在利用基于MRI的卷积神经网络进行脑癌检测。</li>
<li>虽然公共网络如RestNet和AlexNet可通过转移学习有效诊断脑癌，但其诊断结果存在可靠性问题。</li>
<li>为解决可靠性问题，研究团队构建了全新的模型，该模型结合了卷积堆叠、dropout和全连接操作来提高性能并降低过拟合现象。</li>
<li>在模型训练过程中，补充数据集并注入高斯噪声以增强模型的灵活性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.19583">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-27f88ee8cdaadbee2c03db7351299ceb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ff84d93aef756890e2e355e419c0227.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b66ed32d7df1eac87cb1528592ebee8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2465ace51409c291120c1b39011ea43e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4247b1dbc3cb7bc0c6315c37e9155e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f8830741816d921c0895e0b9be5429c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="QueryCAD-Grounded-Question-Answering-for-CAD-Models"><a href="#QueryCAD-Grounded-Question-Answering-for-CAD-Models" class="headerlink" title="QueryCAD: Grounded Question Answering for CAD Models"></a>QueryCAD: Grounded Question Answering for CAD Models</h2><p><strong>Authors:Claudius Kienle, Benjamin Alt, Darko Katic, Rainer Jäkel, Jan Peters</strong></p>
<p>CAD models are widely used in industry and are essential for robotic automation processes. However, these models are rarely considered in novel AI-based approaches, such as the automatic synthesis of robot programs, as there are no readily available methods that would allow CAD models to be incorporated for the analysis, interpretation, or extraction of information. To address these limitations, we propose QueryCAD, the first system designed for CAD question answering, enabling the extraction of precise information from CAD models using natural language queries. QueryCAD incorporates SegCAD, an open-vocabulary instance segmentation model we developed to identify and select specific parts of the CAD model based on part descriptions. We further propose a CAD question answering benchmark to evaluate QueryCAD and establish a foundation for future research. Lastly, we integrate QueryCAD within an automatic robot program synthesis framework, validating its ability to enhance deep-learning solutions for robotics by enabling them to process CAD models (<a target="_blank" rel="noopener" href="https://claudius-kienle.github.com/querycad">https://claudius-kienle.github.com/querycad</a>). </p>
<blockquote>
<p>CAD模型在工业中得到了广泛应用，是机器人自动化流程中的关键。然而，在基于AI的新方法中，如机器人程序的自动合成，这些模型很少被考虑在内。因为没有现成的方法可以让CAD模型用于分析、解释或提取信息。为了解决这些局限性，我们提出了QueryCAD系统，这是首个为CAD问答设计的系统，通过自然语言查询能够从CAD模型中提取精确信息。QueryCAD集成了我们开发的SegCAD模型，这是一种开放式词汇实例分割模型，能够根据零件描述来识别和选择CAD模型中的特定部分。我们还提出了一个CAD问答基准测试来评估QueryCAD，并为未来的研究奠定了基础。最后，我们将QueryCAD集成到一个自动机器人程序合成框架中，验证了其通过处理CAD模型提升机器人深度学习能力的能力（<a target="_blank" rel="noopener" href="https://claudius-kienle.github.com/querycad%EF%BC%89%E3%80%82">https://claudius-kienle.github.com/querycad）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08704v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CAD模型在工业中广泛应用，对机器人自动化流程至关重要。但在基于AI的自动机器人程序合成等新型方法中，CAD模型的应用受限，缺乏对其分析、解读或信息提取的方法。为解决此问题，提出QueryCAD系统，利用自然语言查询从CAD模型中提取精确信息。该系统包含SegCAD，一个基于开放词汇的实例分割模型，可基于部分描述识别并选择CAD模型的具体部分。此外，建立CAD问答基准测试评估QueryCAD并为未来研究奠定基础。最后，将QueryCAD集成到自动机器人程序合成框架中，验证其处理CAD模型的能力，从而提升机器人深度学习的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CAD模型在工业机器人自动化中占据重要地位，但在新型AI方法中的应用受限。</li>
<li>QueryCAD系统填补这一空白，利用自然语言查询实现从CAD模型中提取精确信息。</li>
<li>QueryCAD包含SegCAD实例分割模型，可识别并选择CAD模型中的特定部分。</li>
<li>建立CAD问答基准测试以评估QueryCAD的性能，并为未来研究提供基础。</li>
<li>QueryCAD成功集成到自动机器人程序合成框架中。</li>
<li>QueryCAD能增强深度学习的机器人解决方案，使其具备处理CAD模型的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08704">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7eb1f7155e0452be853ffc181abd7855.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29adb0073e32ff46033fb5d49ff2b915.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0b6301907af5000996e8cc3ce2f6561.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a81371ae65ddacaac35e798e62c9b66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1a740930cdbf321cef797795942e8b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a77f863fe4e44736d43a4e997df58e07.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Direct3γ-A-Pipeline-for-Direct-Three-gamma-PET-Image-Reconstruction"><a href="#Direct3γ-A-Pipeline-for-Direct-Three-gamma-PET-Image-Reconstruction" class="headerlink" title="Direct3γ: A Pipeline for Direct Three-gamma PET Image   Reconstruction"></a>Direct3γ: A Pipeline for Direct Three-gamma PET Image   Reconstruction</h2><p><strong>Authors:Youness Mellak, Alexandre Bousse, Thibaut Merlin, Debora Giovagnoli, Dimitris Visvikis</strong></p>
<p>This paper presents a novel image reconstruction pipeline for three-gamma (3-{\gamma}) positron emission tomography (PET) aimed at improving spatial resolution and reducing noise in nuclear medicine. The proposed Direct3{\gamma} pipeline addresses the inherent challenges in 3-{\gamma} PET systems, such as detector imperfections and uncertainty in photon interaction points. A key feature of the pipeline is its ability to determine the order of interactions through a model trained on Monte Carlo (MC) simulations using the Geant4 Application for Tomography Emission (GATE) toolkit, thus providing the necessary information to construct Compton cones which intersect with the line of response (LOR) to provide an estimate of the emission point. The pipeline processes 3-{\gamma} PET raw data, reconstructs histoimages by propagating energy and spatial uncertainties along the LOR, and applies a 3-D convolutional neural network (CNN) to refine these intermediate images into high-quality reconstructions. To further enhance image quality, the pipeline leverages both supervised learning and adversarial losses, with the latter preserving fine structural details. Experimental results demonstrate that Direct3{\gamma} consistently outperforms conventional 200-ps time-of-flight (TOF) PET in terms of SSIM and PSNR. </p>
<blockquote>
<p>本文提出了一种针对三伽马（3-γ）正电子发射断层扫描（PET）的新型图像重建流程，旨在提高核医学中的空间分辨率并降低噪声。所提出的Direct3γ流程解决了3-γ PET系统固有的挑战，如探测器缺陷和光子交互点的不确定性。该流程的一个关键功能是，它能够通过使用Geant4发射断层扫描应用程序（GATE）工具包进行蒙特卡洛（MC）模拟训练模型来确定交互的顺序，从而提供构建交于响应线（LOR）的康普顿锥所需的信息，以估计发射点。该流程处理3-γ PET原始数据，通过传播能量和空间不确定性沿LOR重建直方图像，并应用三维卷积神经网络（CNN）将这些中间图像精细化为高质量重建。为了进一步改善图像质量，该流程结合了监督学习和对抗性损失，后者能够保留精细的结构细节。实验结果表明，Direct3γ在结构相似性度量（SSIM）和峰值信噪比（PSNR）方面始终优于传统的200皮秒飞行时间（TOF）PET。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.18337v3">PDF</a> 10 pages, 11 figures, 2 tables</p>
<p><strong>Summary</strong><br>     本文提出了一种针对三伽马（3-γ）正电子发射断层扫描（PET）图像重建的新流程，旨在改善空间分辨率并减少核医学中的噪声。Direct3γ流程解决了3-γ PET系统的固有挑战，如探测器的不完美和光子交互点的不确定性。该流程通过训练模型使用蒙特卡洛（MC）模拟确定交互顺序，构建交于响应线（LOR）的康普顿锥以估算发射点。该流程处理3-γ PET原始数据，通过传播能量和空间不确定性沿LOR重建组织图像，并应用三维卷积神经网络（CNN）对中间图像进行高质量重建。为进一步改善图像质量，该流程结合了监督学习和对抗损失，后者保留了精细结构细节。实验结果表明，Direct3γ在结构相似性指数（SSIM）和峰值信噪比（PSNR）方面均优于传统的200皮秒飞行时间（TOF）PET。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>该论文介绍了一种针对三伽马（3-γ）PET的新型图像重建流程。</li>
<li>Direct3γ流程旨在提高空间分辨率并降低核医学中的噪声。</li>
<li>该流程解决了探测器不完美和光子交互点的不确定性等3-γ PET系统的固有挑战。</li>
<li>通过训练模型并使用蒙特卡洛模拟确定交互顺序，构建康普顿锥来估算发射点。</li>
<li>该流程利用CNN对图像进行高质量重建，并通过传播能量和空间不确定性沿LOR生成组织图像。</li>
<li>Direct3γ流程结合了监督学习和对抗损失来改善图像质量，保留精细结构细节。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.18337">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cc5ecdf31bc8ba51be1fa0028a0c32af.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f597977d942388305fafd98bb0fe2260.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36a87e8dd81f7cd8c896f7ddf66864fb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4c2c099135b66d5c49b3c1fce146fdc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29caeeb92e880f4aab286063404de0e0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LiteNeXt-A-Novel-Lightweight-ConvMixer-based-Model-with-Self-embedding-Representation-Parallel-for-Medical-Image-Segmentation"><a href="#LiteNeXt-A-Novel-Lightweight-ConvMixer-based-Model-with-Self-embedding-Representation-Parallel-for-Medical-Image-Segmentation" class="headerlink" title="LiteNeXt: A Novel Lightweight ConvMixer-based Model with Self-embedding   Representation Parallel for Medical Image Segmentation"></a>LiteNeXt: A Novel Lightweight ConvMixer-based Model with Self-embedding   Representation Parallel for Medical Image Segmentation</h2><p><strong>Authors:Ngoc-Du Tran, Thi-Thao Tran, Quang-Huy Nguyen, Manh-Hung Vu, Van-Truong Pham</strong></p>
<p>The emergence of deep learning techniques has advanced the image segmentation task, especially for medical images. Many neural network models have been introduced in the last decade bringing the automated segmentation accuracy close to manual segmentation. However, cutting-edge models like Transformer-based architectures rely on large scale annotated training data, and are generally designed with densely consecutive layers in the encoder, decoder, and skip connections resulting in large number of parameters. Additionally, for better performance, they often be pretrained on a larger data, thus requiring large memory size and increasing resource expenses. In this study, we propose a new lightweight but efficient model, namely LiteNeXt, based on convolutions and mixing modules with simplified decoder, for medical image segmentation. The model is trained from scratch with small amount of parameters (0.71M) and Giga Floating Point Operations Per Second (0.42). To handle boundary fuzzy as well as occlusion or clutter in objects especially in medical image regions, we propose the Marginal Weight Loss that can help effectively determine the marginal boundary between object and background. Additionally, the Self-embedding Representation Parallel technique is proposed as an innovative data augmentation strategy that utilizes the network architecture itself for self-learning augmentation, enhancing feature extraction robustness without external data. Experiments on public datasets including Data Science Bowls, GlaS, ISIC2018, PH2, Sunnybrook, and Lung X-ray data show promising results compared to other state-of-the-art CNN-based and Transformer-based architectures. Our code is released at: <a target="_blank" rel="noopener" href="https://github.com/tranngocduvnvp/LiteNeXt">https://github.com/tranngocduvnvp/LiteNeXt</a>. </p>
<blockquote>
<p>随着深度学习技术的兴起，图像分割任务，尤其是医学图像分割，已经得到了极大的推进。过去十年中，已经引入了许多神经网络模型，使自动化分割的准确度接近手动分割。然而，最前沿的模型，如基于Transformer的架构，依赖于大规模标注的训练数据，并且通常具有编码器、解码器和跳过连接中的密集连续层，导致参数数量庞大。另外，为了获得更好的性能，它们通常需要在更大的数据上进行预训练，因此需要较大的内存并增加了资源开销。在本研究中，我们提出了一种新的轻量化但高效的模型，名为LiteNeXt，用于医学图像分割，该模型基于卷积和混合模块，并使用简化的解码器。该模型从零开始训练，参数较少（0.71M），并且每秒可进行0.42Giga浮点运算。为了处理医学图像区域中边界模糊以及对象遮挡或杂乱的情况，我们提出了边缘权重损失（Marginal Weight Loss），可以帮助有效地确定对象与背景之间的边缘边界。此外，还提出了自嵌入表示并行技术（Self-embedding Representation Parallel），这是一种创新的数据增强策略，利用网络架构本身进行自学习增强，提高特征提取的稳健性而无需外部数据。在公开数据集上的实验表明，与其他最先进的CNN和Transformer架构相比，我们的方法在Data Science Bowls、GlaS、ISIC2018、PH2、Sunnybrook和肺部X射线数据上均显示出有前景的结果。我们的代码已发布在：<a target="_blank" rel="noopener" href="https://github.com/tranngocduvnvp/LiteNeXt%E3%80%82">https://github.com/tranngocduvnvp/LiteNeXt。</a></p>
</blockquote>
<p><strong>解释</strong>：</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15779v2">PDF</a> This manuscript has been accepted by Biomedical Signal Processing and   Control</p>
<p><strong>Summary</strong><br>     深度学习技术的出现推动了图像分割任务的发展，特别是在医学图像领域。一项新研究提出了一种轻量级但高效的模型LiteNeXt，用于医学图像分割。该模型基于卷积和混合模块，具有简化的解码器，可有效处理医学图像区域中边界模糊、遮挡或杂乱的问题。同时，该研究还提出了边际权重损失和自我嵌入表示并行等创新策略，实验结果显示其性能优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习技术推动了医学图像分割的进展，许多神经网络模型提高了自动化分割的准确性。</li>
<li>最新模型如基于Transformer的架构需要大规模注释训练数据，并且参数较多。</li>
<li>LiteNeXt模型是一个轻量级但高效的医学图像分割模型，基于卷积和混合模块，具有简化的解码器。</li>
<li>LiteNeXt模型可处理医学图像中的边界模糊、遮挡或杂乱问题，通过边际权重损失有效确定物体与背景之间的边界。</li>
<li>自我嵌入表示并行是一种利用网络架构进行自我学习增强的创新数据增强策略，提高了特征提取的稳健性。</li>
<li>在公开数据集上的实验结果显示，LiteNeXt模型与其他先进的CNN和Transformer架构相比具有前景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.15779">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2fdd76dfe1422046c5e96eb83a2d3a8e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ce4446c79c60821ebb8662595e67763f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DeepThalamus-A-novel-deep-learning-method-for-automatic-segmentation-of-brain-thalamic-nuclei-from-multimodal-ultra-high-resolution-MRI"><a href="#DeepThalamus-A-novel-deep-learning-method-for-automatic-segmentation-of-brain-thalamic-nuclei-from-multimodal-ultra-high-resolution-MRI" class="headerlink" title="DeepThalamus: A novel deep learning method for automatic segmentation of   brain thalamic nuclei from multimodal ultra-high resolution MRI"></a>DeepThalamus: A novel deep learning method for automatic segmentation of   brain thalamic nuclei from multimodal ultra-high resolution MRI</h2><p><strong>Authors:Marina Ruiz-Perez, Sergio Morell-Ortega, Marien Gadea, Roberto Vivo-Hernando, Gregorio Rubio, Fernando Aparici, Mariam de la Iglesia-Vaya, Thomas Tourdias, Pierrick Coupé, José V. Manjón</strong></p>
<p>The implication of the thalamus in multiple neurological pathologies makes it a structure of interest for volumetric analysis. In the present work, we have designed and implemented a multimodal volumetric deep neural network for the segmentation of thalamic nuclei at ultra-high resolution (0.125 mm3). Current tools either operate at standard resolution (1 mm3) or use monomodal data. To achieve the proposed objective, first, a database of semiautomatically segmented thalamic nuclei was created using ultra-high resolution T1, T2 and White Matter nulled (WMn) images. Then, a novel Deep learning based strategy was designed to obtain the automatic segmentations and trained to improve its robustness and accuaracy using a semisupervised approach. The proposed method was compared with a related state-of-the-art method showing competitive results both in terms of segmentation quality and efficiency. To make the proposed method fully available to the scientific community, a full pipeline able to work with monomodal standard resolution T1 images is also proposed. </p>
<blockquote>
<p>丘脑在多种神经病理中的影响使其成为体积分析感兴趣的结构。在本研究中，我们设计并实现了一种多模态体积深度神经网络，用于超高分辨率（0.125 mm3）下的丘脑核分割。当前工具要么在标准分辨率（1 mm3）下运行，要么使用单模态数据。为了实现既定目标，首先使用超高分辨率的T1、T2和White Matter nulled（WMn）图像创建了半自动分割的丘脑核数据库。然后，设计了一种基于深度学习的新策略来获得自动分割，并使用半监督方法进行训练以提高其稳健性和准确性。所提出的方法与一种相关的高级方法进行了比较，在分割质量和效率方面都显示出具有竞争力的结果。为了使所提出的方法完全可供科学界使用，还提出了一种能够处理单模态标准分辨率T1图像的全流程。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.07751v2">PDF</a> </p>
<p><strong>Summary</strong><br>     本研究设计并实施了一种多模态体积深度神经网络，用于超高分辨率（0.125 mm³）丘脑核分割。当前工具操作于标准分辨率或使用单模态数据。本研究创建了一个半自动分割丘脑核数据库，使用超高分辨率T1、T2和白色物质空化（WMn）图像，然后设计了一种新型的基于深度学习的自动分割策略并进行了训练。本方法与相关领域的前沿方法相比在分割质量和效率方面展现了竞争力，并提出了适用于单模态标准分辨率T1图像的完整流程。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究探讨了丘脑在多种神经病理学中的作用，并对其进行体积分析。</li>
<li>设计并实施了一种多模态体积深度神经网络用于超高分辨率丘脑核分割。</li>
<li>当前工具存在操作于标准分辨率或使用单模态数据的局限。</li>
<li>通过创建半自动分割丘脑核数据库并使用超高分辨率图像进行训练，提高了方法的稳健性和准确性。</li>
<li>本方法与相关领域的前沿方法相比在分割质量和效率方面展现了竞争力。</li>
<li>研究提出了一种完整的流程，能够处理单模态标准分辨率T1图像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.07751">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1b389f5081328cb0935b2454ca680d1d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-16/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-16/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6ab0166324c140954021b1012ef6c3ae.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-03-18  VideoMind A Chain-of-LoRA Agent for Long Video Reasoning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-18
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-16/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4d56c02d4d522dde64ae67f260834dc1.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-03-16  StereoCrafter-Zero Zero-Shot Stereo Video Generation with Noisy Restart
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23827k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
