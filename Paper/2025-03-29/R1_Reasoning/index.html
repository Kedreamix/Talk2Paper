<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-29  Video-R1 Reinforcing Video Reasoning in MLLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-5d4020fd202d1674da42cb3789c92c23.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    88 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-29-æ›´æ–°"><a href="#2025-03-29-æ›´æ–°" class="headerlink" title="2025-03-29 æ›´æ–°"></a>2025-03-29 æ›´æ–°</h1><h2 id="Video-R1-Reinforcing-Video-Reasoning-in-MLLMs"><a href="#Video-R1-Reinforcing-Video-Reasoning-in-MLLMs" class="headerlink" title="Video-R1: Reinforcing Video Reasoning in MLLMs"></a>Video-R1: Reinforcing Video Reasoning in MLLMs</h2><p><strong>Authors:Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, Xiangyu Yue</strong></p>
<p>Inspired by DeepSeek-R1â€™s success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for eliciting video reasoning within multimodal large language models (MLLMs). However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data. To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning. Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process. We have constructed two datasets: Video-R1-COT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data. Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc. Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All codes, models, data are released. </p>
<blockquote>
<p>å—DeepSeek-R1é€šè¿‡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¿€å‘æ¨ç†èƒ½åŠ›çš„æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æ¨å‡ºVideo-R1ï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•åœ¨ç³»ç»Ÿæ¢ç´¢R1èŒƒå¼ä»¥æ¿€å‘å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„è§†é¢‘æ¨ç†ã€‚ç„¶è€Œï¼Œå°†GRPOç®—æ³•ç›´æ¥åº”ç”¨äºè§†é¢‘æ¨ç†çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒé¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šï¼ˆiï¼‰è§†é¢‘æ¨ç†ç¼ºä¹æ—¶é—´å»ºæ¨¡ï¼›ï¼ˆiiï¼‰é«˜è´¨é‡è§†é¢‘æ¨ç†æ•°æ®çš„ç¨€ç¼ºã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºT-GRPOç®—æ³•ï¼Œè¯¥ç®—æ³•é¼“åŠ±æ¨¡å‹åˆ©ç”¨è§†é¢‘ä¸­çš„æ—¶é—´ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸æ˜¯ä»…ä¾èµ–è§†é¢‘æ•°æ®ï¼Œè€Œæ˜¯å°†é«˜è´¨é‡å›¾åƒæ¨ç†æ•°æ®çº³å…¥è®­ç»ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸¤ä¸ªæ•°æ®é›†ï¼šç”¨äºSFTå†·å¯åŠ¨çš„Video-R1-COT-165kå’Œç”¨äºRLè®­ç»ƒçš„è§†é¢‘R1-260kï¼Œä¸¤è€…éƒ½åŒ…å«å›¾åƒå’Œè§†é¢‘æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideo-R1åœ¨è§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå¦‚VideoMMMUå’ŒVSI-Benchï¼‰ä»¥åŠé€šç”¨è§†é¢‘åŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬MVBenchå’ŒTempCompassç­‰ï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒVideo-R1-7Båœ¨è§†é¢‘ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•VSI-benchä¸Šè¾¾åˆ°äº†35.8%çš„å‡†ç¡®ç‡ï¼Œè¶…è¿‡äº†å•†ä¸šä¸“æœ‰æ¨¡å‹GPT-4oã€‚æ‰€æœ‰ä»£ç ã€æ¨¡å‹ã€æ•°æ®å‡å·²å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21776v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://github.com/tulerfeng/Video-R1">https://github.com/tulerfeng/Video-R1</a></p>
<p><strong>Summary</strong><br>åŸºäºDeepSeek-R1é€šè¿‡è§„åˆ™å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¿€å‘æ¨ç†èƒ½åŠ›çš„æˆåŠŸï¼Œæˆ‘ä»¬é¦–æ¬¡å°è¯•æ¨å‡ºVideo-R1ï¼Œæ—¨åœ¨ç³»ç»Ÿæ¢ç´¢R1èŒƒå¼åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­çš„è§†é¢‘æ¨ç†åº”ç”¨ã€‚ä¸ºåº”å¯¹è§†é¢‘æ¨ç†ä¸­ç¼ºä¹æ—¶é—´å»ºæ¨¡å’Œé«˜è´¨é‡è§†é¢‘æ¨ç†æ•°æ®ç¨€ç¼ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºT-GRPOç®—æ³•å¹¶æ•´åˆå›¾åƒæ¨ç†æ•°æ®ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVideo-R1åœ¨è§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•VideoMMMUå’ŒVSI-Benchä¸Šè¡¨ç°æ˜¾è‘—ï¼Œä¸”åœ¨é€šç”¨è§†é¢‘åŸºå‡†æµ‹è¯•MVBenchå’ŒTempCompassç­‰ä¸Šä¹Ÿæœ‰è‰¯å¥½è¡¨ç°ã€‚æˆ‘ä»¬å‘å¸ƒä»£ç ã€æ¨¡å‹å’Œå…¨éƒ¨æ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Video-R1é¦–æ¬¡å°è¯•ç³»ç»Ÿæ¢ç´¢R1èŒƒå¼åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§†é¢‘æ¨ç†åº”ç”¨ã€‚</li>
<li>Video-R1é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬ç¼ºä¹è§†é¢‘æ¨ç†çš„æ—¶é—´å»ºæ¨¡å’Œé«˜è´¨é‡è§†é¢‘æ¨ç†æ•°æ®çš„ç¨€ç¼ºæ€§ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºT-GRPOç®—æ³•æ¥é¼“åŠ±æ¨¡å‹åˆ©ç”¨è§†é¢‘ä¸­çš„æ—¶é—´ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚</li>
<li>ç»“åˆé«˜è´¨é‡å›¾åƒæ¨ç†æ•°æ®ä»¥å¢å¼ºæ¨¡å‹è®­ç»ƒæ•ˆæœã€‚</li>
<li>Video-R1åœ¨å¤šä¸ªè§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬VideoMMMUå’ŒVSI-Benchç­‰ã€‚</li>
<li>Video-R1åœ¨é€šç”¨è§†é¢‘åŸºå‡†æµ‹è¯•ä¸Šä¹Ÿæœ‰è‰¯å¥½è¡¨ç°ï¼Œå¦‚MVBenchå’ŒTempCompassç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21776">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9e596413b0399fd7c3ae1db4e68e0947.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04b0a066291d6d6862848451fd241ae9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37bfe5371e2dfc7f4fc8cbcb303172bd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VBench-2-0-Advancing-Video-Generation-Benchmark-Suite-for-Intrinsic-Faithfulness"><a href="#VBench-2-0-Advancing-Video-Generation-Benchmark-Suite-for-Intrinsic-Faithfulness" class="headerlink" title="VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic   Faithfulness"></a>VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic   Faithfulness</h2><p><strong>Authors:Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, Ziwei Liu</strong></p>
<p>Video generation has advanced significantly, evolving from producing unrealistic outputs to generating videos that appear visually convincing and temporally coherent. To evaluate these video generative models, benchmarks such as VBench have been developed to assess their faithfulness, measuring factors like per-frame aesthetics, temporal consistency, and basic prompt adherence. However, these aspects mainly represent superficial faithfulness, which focus on whether the video appears visually convincing rather than whether it adheres to real-world principles. While recent models perform increasingly well on these metrics, they still struggle to generate videos that are not just visually plausible but fundamentally realistic. To achieve real â€œworld modelsâ€ through video generation, the next frontier lies in intrinsic faithfulness to ensure that generated videos adhere to physical laws, commonsense reasoning, anatomical correctness, and compositional integrity. Achieving this level of realism is essential for applications such as AI-assisted filmmaking and simulated world modeling. To bridge this gap, we introduce VBench-2.0, a next-generation benchmark designed to automatically evaluate video generative models for their intrinsic faithfulness. VBench-2.0 assesses five key dimensions: Human Fidelity, Controllability, Creativity, Physics, and Commonsense, each further broken down into fine-grained capabilities. Tailored for individual dimensions, our evaluation framework integrates generalists such as state-of-the-art VLMs and LLMs, and specialists, including anomaly detection methods proposed for video generation. We conduct extensive annotations to ensure alignment with human judgment. By pushing beyond superficial faithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new standard for the next generation of video generative models in pursuit of intrinsic faithfulness. </p>
<blockquote>
<p>è§†é¢‘ç”ŸæˆæŠ€æœ¯å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä»äº§ç”Ÿä¸ç°å®çš„è¾“å‡ºå‘å±•åˆ°ç”Ÿæˆåœ¨è§†è§‰ä¸Šä»¤äººä¿¡æœå¹¶ä¸”åœ¨æ—¶é—´ä¸Šè¿è´¯çš„è§†é¢‘ã€‚ä¸ºäº†è¯„ä¼°è¿™äº›è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æ€§èƒ½ï¼Œå·²ç»å¼€å‘å‡ºäº†åƒVBenchè¿™æ ·çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ¥è¯„ä¼°å®ƒä»¬çš„å¿ å®åº¦ï¼Œæµ‹é‡è¯¸å¦‚æ¯å¸§ç¾å­¦ã€æ—¶é—´ä¸€è‡´æ€§å’ŒåŸºæœ¬æç¤ºéµå¾ªç­‰å› ç´ ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹é¢ä¸»è¦ä»£è¡¨äº†è¡¨é¢å¿ å®åº¦ï¼Œé‡ç‚¹åœ¨äºè§†é¢‘åœ¨è§†è§‰ä¸Šæ˜¯å¦ä»¤äººä¿¡æœï¼Œè€Œä¸æ˜¯æ˜¯å¦éµå¾ªç°å®ä¸–ç•Œçš„åŸåˆ™ã€‚å°½ç®¡æœ€è¿‘çš„æ¨¡å‹åœ¨è¿™äº›æŒ‡æ ‡ä¸Šçš„è¡¨ç°è¶Šæ¥è¶Šå¥½ï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥ç”Ÿæˆä¸ä»…ä»…æ˜¯è§†è§‰ä¸Šå¯ä¿¡ã€è€Œæ˜¯ä»æ ¹æœ¬ä¸Šç°å®çš„è§†é¢‘ã€‚é€šè¿‡è§†é¢‘ç”Ÿæˆå®ç°çœŸæ­£çš„â€œä¸–ç•Œæ¨¡å‹â€ï¼Œä¸‹ä¸€ä¸ªå‰æ²¿åœ¨äºå†…åœ¨å¿ å®åº¦ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„è§†é¢‘éµå¾ªç‰©ç†å®šå¾‹ã€å¸¸è¯†æ¨ç†ã€è§£å‰–æ­£ç¡®æ€§å’Œæ„å›¾å®Œæ•´æ€§ã€‚å®ç°è¿™ç§ç¨‹åº¦çš„çœŸå®æ€§å¯¹äºAIè¾…åŠ©ç”µå½±åˆ¶ä½œå’Œæ¨¡æ‹Ÿä¸–ç•Œå»ºæ¨¡ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VBench-2..ä¸‹ä¸€ä»£åŸºå‡†æµ‹è¯•å¹³å°æ—¨åœ¨è‡ªåŠ¨è¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å†…åœ¨å¿ å®åº¦ã€‚VBench-2.è¯„ä¼°äº”ä¸ªå…³é”®ç»´åº¦ï¼šäººç±»ä¿çœŸåº¦ã€å¯æ§æ€§ã€åˆ›é€ åŠ›ã€ç‰©ç†å­¦å’Œå¸¸è¯†ï¼Œæ¯ä¸ªç»´åº¦éƒ½è¿›ä¸€æ­¥ç»†åŒ–ä¸ºç²¾ç»†åŠŸèƒ½ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶é’ˆå¯¹å„ä¸ªç»´åº¦è¿›è¡Œå®šåˆ¶ï¼Œé›†æˆäº†è¯¸å¦‚æœ€æ–°æŠ€æœ¯ä¸­çš„å¤§å‹è§†é¢‘æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ç­‰é€šç”¨æ¨¡å‹ä»¥åŠé’ˆå¯¹è§†é¢‘ç”Ÿæˆçš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ç­‰ä¸“å®¶ç³»ç»Ÿã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„æ³¨é‡Šä»¥ç¡®ä¿ä¸äººç±»åˆ¤æ–­ä¸€è‡´ã€‚é€šè¿‡è¶…è¶Šè¡¨é¢å¿ å®åº¦è€Œè¿½æ±‚å†…åœ¨å¿ å®åº¦ï¼ŒVBench-æ—¨åœ¨æ ‘ç«‹è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„æ–°ä¸€ä»£æ ‡å‡†ï¼Œä»¥è¿½æ±‚å†…åœ¨å¿ å®åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21755v1">PDF</a> Equal contributions from first two authors. Project page:   <a target="_blank" rel="noopener" href="https://vchitect.github.io/VBench-2.0-project/">https://vchitect.github.io/VBench-2.0-project/</a> Code:   <a target="_blank" rel="noopener" href="https://github.com/Vchitect/VBench">https://github.com/Vchitect/VBench</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†é¢‘ç”ŸæˆæŠ€æœ¯å·²æ˜¾è‘—è¿›æ­¥ï¼Œä»äº§ç”Ÿä¸çœŸå®çš„è¾“å‡ºå‘å±•åˆ°èƒ½å¤Ÿç”Ÿæˆè§†è§‰ä¸Šä»¤äººä¿¡æœä¸”æ—¶é—´è¿è´¯çš„è§†é¢‘ã€‚ä¸ºè¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œå·²å¼€å‘å‡ºè¯¸å¦‚VBenchç­‰åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°å…¶å¿ å®åº¦ï¼Œä¸»è¦è¡¡é‡æ¯å¸§ç¾å­¦ã€æ—¶é—´è¿è´¯æ€§å’ŒåŸºæœ¬æç¤ºéµå¾ªç­‰å› ç´ ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹é¢ä¸»è¦ä»£è¡¨è¡¨é¢å¿ å®åº¦ï¼Œå…³æ³¨çš„æ˜¯è§†é¢‘æ˜¯å¦è§†è§‰ä¸Šä»¤äººä¿¡æœï¼Œè€Œä¸æ˜¯æ˜¯å¦éµå¾ªç°å®ä¸–ç•Œçš„åŸåˆ™ã€‚å°½ç®¡æœ€è¿‘æ¨¡å‹åœ¨è¿™äº›æŒ‡æ ‡ä¸Šçš„è¡¨ç°è¶Šæ¥è¶Šå¥½ï¼Œä½†å®ƒä»¬ä»ç„¶éš¾ä»¥ç”Ÿæˆä¸ä»…æ˜¯è§†è§‰ä¸Šå¯ä¿¡çš„ï¼Œè€Œä¸”ä»æ ¹æœ¬ä¸Šç°å®çš„è§†é¢‘ã€‚ä¸ºå®ç°é€šè¿‡è§†é¢‘ç”Ÿæˆçš„çœŸå®â€œä¸–ç•Œæ¨¡å‹â€ï¼Œä¸‹ä¸€ä¸ªå‰æ²¿åœ¨äºå†…åœ¨å¿ å®åº¦ï¼Œä»¥ç¡®ä¿ç”Ÿæˆçš„è§†é¢‘éµå¾ªç‰©ç†å®šå¾‹ã€å¸¸è¯†æ¨ç†ã€è§£å‰–æ­£ç¡®æ€§å’Œç»„æˆå®Œæ•´æ€§ã€‚ä¸ºäº†å®ç°è¿™ä¸€çº§åˆ«çš„ç°å®ä¸»ä¹‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†VBench-2.0ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è‡ªåŠ¨è¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å†…åœ¨å¿ å®åº¦çš„ä¸‹ä¸€ä»£åŸºå‡†æµ‹è¯•ã€‚VBench-2.0è¯„ä¼°äº†äº”ä¸ªå…³é”®ç»´åº¦ï¼šäººç±»ä¿çœŸåº¦ã€å¯æ§æ€§ã€åˆ›é€ åŠ›ã€ç‰©ç†å­¦å’Œå¸¸è¯†ï¼Œæ¯ä¸ªç»´åº¦è¿›ä¸€æ­¥ç»†åŒ–ä¸ºç²¾ç»†åŠŸèƒ½ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¡†æ¶æ•´åˆäº†é€šç”¨äººå‘˜ï¼ˆå¦‚æœ€å…ˆè¿›çš„è§†é¢‘è¯­è¨€æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰å’Œä¸“å®¶ï¼ˆåŒ…æ‹¬é’ˆå¯¹è§†é¢‘ç”Ÿæˆæå‡ºçš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼‰ã€‚é€šè¿‡è¶…è¶Šè¡¨é¢å¿ å®åº¦è¿½æ±‚å†…åœ¨å¿ å®åº¦ï¼ŒVBench-2.0æ—¨åœ¨ä¸ºè¿½æ±‚å†…åœ¨å¿ å®åº¦çš„ä¸‹ä¸€ä»£è§†é¢‘ç”Ÿæˆæ¨¡å‹è®¾å®šæ–°æ ‡å‡†ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘ç”ŸæˆæŠ€æœ¯å·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä»äº§ç”Ÿä¸çœŸå®çš„è¾“å‡ºåˆ°ç”Ÿæˆè§†è§‰ä¸Šæœ‰è¯´æœåŠ›çš„è§†é¢‘ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•å¦‚VBenchä¸»è¦è¡¡é‡è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„è¡¨é¢å¿ å®åº¦ã€‚</li>
<li>å†…åœ¨å¿ å®åº¦æ˜¯ç¡®ä¿ç”Ÿæˆçš„è§†é¢‘éµå¾ªç‰©ç†å®šå¾‹ã€å¸¸è¯†æ¨ç†ç­‰çš„å…³é”®å› ç´ ã€‚</li>
<li>VBench-2.0æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è‡ªåŠ¨è¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å†…åœ¨å¿ å®åº¦ã€‚</li>
<li>VBench-2.0è¯„ä¼°äº†äº”ä¸ªå…³é”®ç»´åº¦ï¼šäººç±»ä¿çœŸåº¦ã€å¯æ§æ€§ã€åˆ›é€ åŠ›ã€ç‰©ç†å­¦å’Œå¸¸è¯†ã€‚</li>
<li>VBench-2.0ç»“åˆäº†é€šç”¨æ¨¡å‹å’Œé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„ä¸“å®¶æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21755">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a51404916ce9b01eaf4fb3568de65fea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c5bbf906b71202a547ee1d02414ba8c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3247569ef18eb868733e3fcb775f1eea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b104b35342f81ffe9a591bc11e7f634.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7682aca6cd54265734723500a3157454.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GateLens-A-Reasoning-Enhanced-LLM-Agent-for-Automotive-Software-Release-Analytics"><a href="#GateLens-A-Reasoning-Enhanced-LLM-Agent-for-Automotive-Software-Release-Analytics" class="headerlink" title="GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release   Analytics"></a>GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release   Analytics</h2><p><strong>Authors:Arsham Gholamzadeh Khoee, Shuai Wang, Yinan Yu, Robert Feldt, Dhasarathy Parthasarathy</strong></p>
<p>Ensuring the reliability and effectiveness of software release decisions is critical, particularly in safety-critical domains like automotive systems. Precise analysis of release validation data, often presented in tabular form, plays a pivotal role in this process. However, traditional methods that rely on manual analysis of extensive test datasets and validation metrics are prone to delays and high costs. Large Language Models (LLMs) offer a promising alternative but face challenges in analytical reasoning, contextual understanding, handling out-of-scope queries, and processing structured test data consistently; limitations that hinder their direct application in safety-critical scenarios. This paper introduces GateLens, an LLM-based tool for analyzing tabular data in the automotive domain. GateLens translates natural language queries into Relational Algebra (RA) expressions and then generates optimized Python code. It outperforms the baseline system on benchmarking datasets, achieving higher F1 scores and handling complex and ambiguous queries with greater robustness. Ablation studies confirm the critical role of the RA module, with performance dropping sharply when omitted. Industrial evaluations reveal that GateLens reduces analysis time by over 80% while maintaining high accuracy and reliability. As demonstrated by presented results, GateLens achieved high performance without relying on few-shot examples, showcasing strong generalization across various query types from diverse company roles. Insights from deploying GateLens with a partner automotive company offer practical guidance for integrating AI into critical workflows such as release validation. Results show that by automating test result analysis, GateLens enables faster, more informed, and dependable release decisions, and can thus advance software scalability and reliability in automotive systems. </p>
<blockquote>
<p>ç¡®ä¿è½¯ä»¶å‘å¸ƒå†³ç­–çš„å¯ä¿¡åº¦å’Œæœ‰æ•ˆæ€§è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æ±½è½¦ç³»ç»Ÿè¿™æ ·çš„å®‰å…¨å…³é”®é¢†åŸŸã€‚ä»¥è¡¨æ ¼å½¢å¼å‘ˆç°çš„å‘å¸ƒéªŒè¯æ•°æ®çš„ç²¾ç¡®åˆ†æåœ¨æ­¤è¿‡ç¨‹ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ–¹æ³•ä¾èµ–äºå¯¹å¤§é‡æµ‹è¯•æ•°æ®é›†å’ŒéªŒè¯æŒ‡æ ‡çš„æ‰‹åŠ¨åˆ†æï¼Œå®¹æ˜“å‡ºç°å»¶è¿Ÿå’Œæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†åœ¨åˆ†ææ¨ç†ã€ä¸Šä¸‹æ–‡ç†è§£ã€å¤„ç†è¶…å‡ºèŒƒå›´æŸ¥è¯¢å’Œå¤„ç†ç»“æ„åŒ–æµ‹è¯•æ•°æ®æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼›è¿™äº›å±€é™æ€§é˜»ç¢äº†å®ƒä»¬åœ¨å®‰å…¨å…³é”®åœºæ™¯ä¸­çš„ç›´æ¥åº”ç”¨ã€‚æœ¬æ–‡ä»‹ç»äº†GateLensï¼Œä¸€ä¸ªåŸºäºLLMçš„æ±½è½¦é¢†åŸŸè¡¨æ ¼æ•°æ®åˆ†æå·¥å…·ã€‚GateLenså°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç¿»è¯‘æˆå…³ç³»ä»£æ•°ï¼ˆRAï¼‰è¡¨è¾¾å¼ï¼Œç„¶åç”Ÿæˆä¼˜åŒ–çš„Pythonä»£ç ã€‚å®ƒåœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶Šäº†åŸºå‡†ç³»ç»Ÿï¼Œå®ç°äº†æ›´é«˜çš„F1åˆ†æ•°ï¼Œå¹¶èƒ½æ›´ç¨³å¥åœ°å¤„ç†å¤æ‚å’Œæ¨¡ç³ŠæŸ¥è¯¢ã€‚æ¶ˆèç ”ç©¶è¯å®äº†RAæ¨¡å—çš„å…³é”®ä½œç”¨ï¼Œåœ¨çœç•¥è¯¥æ¨¡å—æ—¶æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚å·¥ä¸šè¯„ä¼°è¡¨æ˜ï¼ŒGateLenså°†åˆ†ææ—¶é—´å‡å°‘äº†80%ä»¥ä¸Šï¼ŒåŒæ—¶ä¿æŒäº†é«˜å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚æ­£å¦‚æ‰€å±•ç¤ºçš„ç»“æœï¼ŒGateLensåœ¨ä¸ä¾èµ–å°‘é‡ç¤ºä¾‹çš„æƒ…å†µä¸‹å®ç°äº†é«˜æ€§èƒ½ï¼Œå±•ç¤ºäº†åœ¨å„ç§æŸ¥è¯¢ç±»å‹ä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ï¼Œè¿™äº›æŸ¥è¯¢ç±»å‹æ¥è‡ªå„ç§å…¬å¸è§’è‰²ã€‚é€šè¿‡ä¸åˆä½œä¼™ä¼´æ±½è½¦å…¬å¸éƒ¨ç½²GateLensæ‰€è·å¾—çš„è§è§£ä¸ºå°†äººå·¥æ™ºèƒ½é›†æˆåˆ°å…³é”®å·¥ä½œæµç¨‹ï¼ˆå¦‚å‘å¸ƒéªŒè¯ï¼‰æä¾›äº†å®é™…æŒ‡å¯¼ã€‚ç»“æœè¡¨æ˜ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æµ‹è¯•ç»“æœåˆ†æï¼ŒGateLensèƒ½å¤Ÿåšå‡ºæ›´å¿«ã€æ›´å¯é ã€æ›´æœ‰æ ¹æ®çš„å‘å¸ƒå†³ç­–ï¼Œä»è€Œå¯ä»¥æé«˜æ±½è½¦ç³»ç»Ÿçš„è½¯ä»¶å¯æ‰©å±•æ€§å’Œå¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21735v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºGateLensçš„åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å·¥å…·ï¼Œç”¨äºåˆ†ææ±½è½¦é¢†åŸŸçš„è¡¨æ ¼æ•°æ®ã€‚å®ƒèƒ½å¤Ÿå°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬åŒ–ä¸ºå…³ç³»ä»£æ•°è¡¨è¾¾å¼ï¼Œå¹¶ç”Ÿæˆä¼˜åŒ–çš„Pythonä»£ç ã€‚ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒGateLensåœ¨å¤„ç†æ±½è½¦è½¯ä»¶å‘å¸ƒå†³ç­–çš„æ•°æ®åˆ†ææ—¶è¡¨ç°å‡ºæ›´é«˜çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å®ƒèƒ½å¤Ÿå‡å°‘åˆ†ææ—¶é—´å¹¶ç»´æŒé«˜å¯é æ€§å’Œç²¾ç¡®åº¦ï¼Œæ¨è¿›äº†è½¯ä»¶åœ¨æ±½è½¦ç³»ç»Ÿçš„å¯æ‰©å±•æ€§å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ±½è½¦ç³»ç»Ÿè½¯ä»¶çš„å‘å¸ƒå†³ç­–éœ€è¦ç¡®ä¿å¯é æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†å¤§é‡æµ‹è¯•æ•°æ®é›†å’ŒéªŒè¯æŒ‡æ ‡æ—¶å­˜åœ¨å»¶è¿Ÿå’Œæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚</li>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ºè½¯ä»¶åˆ†ææä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†åœ¨æ±½è½¦å®‰å…¨å…³é”®åœºæ™¯ä¸­ç›´æ¥åº”ç”¨æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>GateLensæ˜¯ä¸€æ¬¾åŸºäºLLMçš„å·¥å…·ï¼Œç”¨äºåˆ†ææ±½è½¦é¢†åŸŸçš„è¡¨æ ¼æ•°æ®ï¼Œå¯å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬åŒ–ä¸ºå…³ç³»ä»£æ•°è¡¨è¾¾å¼å¹¶ç”ŸæˆPythonä»£ç ã€‚</li>
<li>GateLensåœ¨åŸºå‡†æµ‹è¯•æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºåŸºçº¿ç³»ç»Ÿï¼Œè¡¨ç°å‡ºæ›´é«˜çš„F1åˆ†æ•°å’Œå¤„ç†å¤æ‚æ¨¡ç³ŠæŸ¥è¯¢çš„é²æ£’æ€§ã€‚</li>
<li>å·¥ä¸šè¯„ä¼°æ˜¾ç¤ºï¼ŒGateLenså‡å°‘äº†è¶…è¿‡80%çš„åˆ†ææ—¶é—´ï¼ŒåŒæ—¶ä¿æŒäº†é«˜å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-969864f4b13b8661d59a174baefc12a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cf232579565cb8bffe5338a1b5c0030.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a00367f5e0b3c08cb2b51990be17ff9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca2772be09acc17b6ffd015d49b82cbc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ReaRAG-Knowledge-guided-Reasoning-Enhances-Factuality-of-Large-Reasoning-Models-with-Iterative-Retrieval-Augmented-Generation"><a href="#ReaRAG-Knowledge-guided-Reasoning-Enhances-Factuality-of-Large-Reasoning-Models-with-Iterative-Retrieval-Augmented-Generation" class="headerlink" title="ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large   Reasoning Models with Iterative Retrieval Augmented Generation"></a>ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large   Reasoning Models with Iterative Retrieval Augmented Generation</h2><p><strong>Authors:Zhicheng Lee, Shulin Cao, Jinxin Liu, Jiajie Zhang, Weichuan Liu, Xiaoyin Che, Lei Hou, Juanzi Li</strong></p>
<p>Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy. While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks. To address this, we propose ReaRAG, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations. Our solution includes a novel data construction framework with an upper bound on the reasoning chain length. Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from a predefined action space (Search and Finish). For Search action, a query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later. This process iterates until a Finish action is chosen. Benefiting from ReaRAGâ€™s strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA. Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory. Our study enhances LRMsâ€™ factuality while effectively integrating robust reasoning for Retrieval-Augmented Generation (RAG). </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å±•ç°å‡ºæ˜¾è‘—çš„æ¨ç†èƒ½åŠ›ï¼Œä½†ä¸»è¦ä¾èµ–äºå‚æ•°çŸ¥è¯†ï¼Œè¿™é™åˆ¶äº†äº‹å®å‡†ç¡®æ€§ã€‚è™½ç„¶æœ€è¿‘çš„å·¥ä½œä¸ºåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„LRMsé…å¤‡äº†æ£€ç´¢èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨æ¨ç†æ—¶è¿‡äºæ·±æ€ç†Ÿè™‘ï¼Œç¼ºä¹ç¨³å¥æ€§ï¼Œé™ä½äº†åœ¨é—®ç­”ï¼ˆQAï¼‰ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ReaRAGï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºäº‹å®æ€§çš„æ¨ç†æ¨¡å‹ï¼Œå®ƒå¯ä»¥åœ¨ä¸è¿›è¡Œè¿‡å¤šè¿­ä»£çš„æƒ…å†µä¸‹æ¢ç´¢å„ç§æŸ¥è¯¢ã€‚æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆåŒ…æ‹¬ä¸€ä¸ªå…·æœ‰æ¨ç†é“¾é•¿åº¦ä¸Šé™çš„æ–°å‹æ•°æ®æ„å»ºæ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ©ç”¨LRMè¿›è¡Œæ·±æ€ç†Ÿè™‘çš„ç”Ÿæˆï¼Œç„¶åä»é¢„å®šçš„åŠ¨ä½œç©ºé—´ä¸­é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼ˆæœç´¢å’Œå®Œæˆï¼‰ã€‚å¯¹äºæœç´¢åŠ¨ä½œï¼Œä¼šå¯¹RAGå¼•æ“æ‰§è¡Œä¸€ä¸ªæŸ¥è¯¢ï¼Œç»“æœå°†ä½œä¸ºè§‚å¯Ÿç»“æœè¿”å›ï¼Œä»¥æŒ‡å¯¼åç»­çš„æ¨ç†æ­¥éª¤ã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šè¿­ä»£ï¼Œç›´åˆ°é€‰æ‹©ä¸€ä¸ªå®ŒæˆåŠ¨ä½œã€‚å¾—ç›ŠäºReaRAGçš„å¼ºå¤§æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šè·³é—®ç­”ä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¿‡äº†ç°æœ‰åŸºçº¿ã€‚è¿›ä¸€æ­¥çš„åˆ†æçªå‡ºäº†å…¶å¼ºå¤§çš„åæ€èƒ½åŠ›ï¼Œèƒ½å¤Ÿè¯†åˆ«é”™è¯¯å¹¶ä¼˜åŒ–å…¶æ¨ç†è½¨è¿¹ã€‚æˆ‘ä»¬çš„ç ”ç©¶æé«˜äº†LRMsçš„äº‹å®æ€§ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°ç»“åˆäº†ç”¨äºå¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰çš„ç¨³å¥æ¨ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21729v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹å±•ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ï¼Œä½†ä¸»è¦ä¾èµ–å‚æ•°çŸ¥è¯†ï¼Œå½±å“äº‹å®å‡†ç¡®æ€§ã€‚æœ€æ–°ç»“åˆå¼ºåŒ–å­¦ä¹ ä¸æ£€ç´¢èƒ½åŠ›çš„å·¥ä½œè™½æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œä½†å­˜åœ¨è¿‡åº¦æ€è€ƒå’Œç¼ºä¹ç¨³å¥æ¨ç†çš„é—®é¢˜ï¼Œé™ä½äº†åœ¨é—®ç­”ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚æœ¬ç ”ç©¶æå‡ºReaRAGæ¨¡å‹ï¼Œé€šè¿‡æ•°æ®æ„å»ºæ–°æ¡†æ¶å’Œé™åˆ¶æ¨ç†é“¾é•¿åº¦ï¼Œå¢å¼ºæ¨¡å‹çš„äº‹å®æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚ReaRAGæ¨¡å‹èƒ½ç”Ÿæˆæ·±æ€ç†Ÿè™‘çš„æ€è€ƒï¼Œä»é¢„è®¾çš„åŠ¨ä½œç©ºé—´ä¸­é€‰æ‹©åŠ¨ä½œï¼ˆæœç´¢å’Œå®Œæˆï¼‰ã€‚æœç´¢åŠ¨ä½œä¼šæŸ¥è¯¢RAGå¼•æ“ï¼Œç»“æœä¼šå¼•å¯¼åç»­çš„æ¨ç†æ­¥éª¤ã€‚ReaRAGåœ¨å¤šæ­¥é—®ç­”ä»»åŠ¡ä¸Šè¶…è¶Šç°æœ‰åŸºçº¿ï¼Œå±•ç°å‡ºå¼ºå¤§çš„çº é”™èƒ½åŠ›å’Œä¼˜åŒ–æ¨ç†è½¨è¿¹çš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶æé«˜äº†å¤§å‹æ¨ç†æ¨¡å‹çš„äº‹å®æ€§ï¼ŒåŒæ—¶å®ç°äº†æ£€ç´¢å¢å¼ºç”Ÿæˆå¼æ¨¡å‹çš„ç¨³å¥æ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ä¸»è¦ä¾èµ–å‚æ•°çŸ¥è¯†ï¼Œå½±å“äº‹å®å‡†ç¡®æ€§ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä¸æ£€ç´¢èƒ½åŠ›ç»“åˆçš„å·¥ä½œå­˜åœ¨è¿‡åº¦æ€è€ƒå’Œç¼ºä¹ç¨³å¥æ¨ç†çš„é—®é¢˜ã€‚</li>
<li>ReaRAGæ¨¡å‹é€šè¿‡æ•°æ®æ„å»ºæ–°æ¡†æ¶å’Œé™åˆ¶æ¨ç†é“¾é•¿åº¦ï¼Œå¢å¼ºäº‹å®æ€§å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>ReaRAGæ¨¡å‹èƒ½ç”Ÿæˆæ·±æ€ç†Ÿè™‘çš„æ€è€ƒï¼Œå¹¶èƒ½ä»é¢„è®¾åŠ¨ä½œç©ºé—´ä¸­é€‰æ‹©åŠ¨ä½œï¼ˆæœç´¢å’Œå®Œæˆï¼‰ã€‚</li>
<li>ReaRAGæ¨¡å‹é€šè¿‡æœç´¢åŠ¨ä½œæŸ¥è¯¢RAGå¼•æ“ï¼Œç»“æœå¼•å¯¼åç»­æ¨ç†æ­¥éª¤ã€‚</li>
<li>ReaRAGåœ¨å¤šæ­¥é—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå…·å¤‡å¼ºå¤§çš„çº é”™å’Œä¼˜åŒ–æ¨ç†è½¨è¿¹èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-28ffccd5c567771badc595bf90bfdb02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5beaa5fb2d12eadd28e6fc41ce584d3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2209859632255ba8f9bfbdbb09dc5fb4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3eb5e78cf74eb765321c4f1ae3b6715a.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Collab-Controlled-Decoding-using-Mixture-of-Agents-for-LLM-Alignment"><a href="#Collab-Controlled-Decoding-using-Mixture-of-Agents-for-LLM-Alignment" class="headerlink" title="Collab: Controlled Decoding using Mixture of Agents for LLM Alignment"></a>Collab: Controlled Decoding using Mixture of Agents for LLM Alignment</h2><p><strong>Authors:Souradip Chakraborty, Sujay Bhatt, Udari Madhushani Sehwag, Soumya Suvra Ghosal, Jiahao Qiu, Mengdi Wang, Dinesh Manocha, Furong Huang, Alec Koppel, Sumitra Ganesh</strong></p>
<p>Alignment of Large Language models (LLMs) is crucial for safe and trustworthy deployment in applications. Reinforcement learning from human feedback (RLHF) has emerged as an effective technique to align LLMs to human preferences and broader utilities, but it requires updating billions of model parameters, which is computationally expensive. Controlled Decoding, by contrast, provides a mechanism for aligning a model at inference time without retraining. However, single-agent decoding approaches often struggle to adapt to diverse tasks due to the complexity and variability inherent in these tasks. To strengthen the test-time performance w.r.t the target task, we propose a mixture of agent-based decoding strategies leveraging the existing off-the-shelf aligned LLM policies. Treating each prior policy as an agent in the spirit of mixture of agent collaboration, we develop a decoding method that allows for inference-time alignment through a token-level selection strategy among multiple agents. For each token, the most suitable LLM is dynamically chosen from a pool of models based on a long-term utility metric. This policy-switching mechanism ensures optimal model selection at each step, enabling efficient collaboration and alignment among LLMs during decoding. Theoretical analysis of our proposed algorithm establishes optimal performance with respect to the target task represented via a target reward for the given off-the-shelf models. We conduct comprehensive empirical evaluations with open-source aligned models on diverse tasks and preferences, which demonstrates the merits of this approach over single-agent decoding baselines. Notably, Collab surpasses the current SoTA decoding strategy, achieving an improvement of up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹é½å¯¹äºå…¶åœ¨åº”ç”¨ä¸­çš„å®‰å…¨å’Œå¯ä¿¡èµ–éƒ¨ç½²è‡³å…³é‡è¦ã€‚å¼ºåŒ–å­¦ä¹ é€šè¿‡äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²ç»æˆä¸ºä¸€ç§æœ‰æ•ˆçš„æŠ€æœ¯ï¼Œç”¨äºå°†LLMä¸äººç±»åå¥½å’Œæ›´å¹¿æ³›çš„å®ç”¨æ€§å¯¹é½ï¼Œä½†å®ƒéœ€è¦æ›´æ–°æ•°åäº¿çš„æ¨¡å‹å‚æ•°ï¼Œè¿™åœ¨è®¡ç®—ä¸Šæ˜¯æ˜‚è´µçš„ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå—æ§è§£ç æä¾›äº†ä¸€ç§åœ¨æ¨ç†é˜¶æ®µå¯¹é½æ¨¡å‹è€Œæ— éœ€é‡æ–°è®­ç»ƒæœºåˆ¶ã€‚ç„¶è€Œï¼Œç”±äºä»»åŠ¡çš„å¤æ‚æ€§å’Œå¯å˜æ€§ï¼Œå•ä»£ç†è§£ç æ–¹æ³•é€šå¸¸éš¾ä»¥é€‚åº”å„ç§ä»»åŠ¡ã€‚ä¸ºäº†å¢å¼ºé’ˆå¯¹ç›®æ ‡ä»»åŠ¡çš„æµ‹è¯•æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä»£ç†çš„è§£ç ç­–ç•¥æ··åˆæ–¹æ³•ï¼Œåˆ©ç”¨ç°æœ‰çš„å³ç”¨å‹å¯¹é½LLMç­–ç•¥ã€‚å°†æ¯ä¸ªå…ˆå‰ç­–ç•¥è§†ä¸ºä»£ç†ï¼Œåœ¨ä»£ç†åä½œçš„æ··åˆç²¾ç¥ä¸‹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§è§£ç æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…è®¸é€šè¿‡æ ‡è®°çº§åˆ«çš„é€‰æ‹©ç­–ç•¥åœ¨å¤šä¸ªä»£ç†ä¹‹é—´è¿›è¡Œæ¨ç†æ—¶çš„å¯¹é½ã€‚å¯¹äºæ¯ä¸ªæ ‡è®°ï¼Œæœ€åˆé€‚çš„LLMå°†åŸºäºé•¿æœŸæ•ˆç”¨æŒ‡æ ‡ä»æ¨¡å‹æ± ä¸­åŠ¨æ€é€‰æ‹©ã€‚è¿™ç§ç­–ç•¥åˆ‡æ¢æœºåˆ¶ç¡®ä¿äº†æ¯ä¸€æ­¥çš„æœ€ä¼˜æ¨¡å‹é€‰æ‹©ï¼Œä½¿å¾—LLMåœ¨è§£ç è¿‡ç¨‹ä¸­èƒ½å¤Ÿè¿›è¡Œé«˜æ•ˆçš„åä½œå’Œå¯¹é½ã€‚æˆ‘ä»¬æå‡ºç®—æ³•çš„ç†è®ºåˆ†æè¯æ˜äº†é’ˆå¯¹ç»™å®šå³ç”¨å‹æ¨¡å‹é€šè¿‡ç›®æ ‡å¥–åŠ±è¡¨ç¤ºçš„ç›®æ ‡ä»»åŠ¡çš„æœ€ä¼˜æ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨å¼€æºå¯¹é½æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡å’Œåå¥½ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®è¯è¯„ä¼°ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•ç›¸å¯¹äºå•ä»£ç†è§£ç åŸºçº¿æ–¹æ³•çš„ä¼˜åŠ¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒCollabè¶…è¶Šäº†å½“å‰çš„æœ€æ–°è§£ç ç­–ç•¥ï¼Œåœ¨å¹³å‡å¥–åŠ±æ–¹é¢æé«˜äº†1.56å€ï¼Œåœ¨åŸºäºGPT-4çš„èƒœç‡ä¸­è¾¾åˆ°äº†71.89%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21720v1">PDF</a> Accepted to ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹é½å¯¹äºå…¶åœ¨åº”ç”¨ä¸­çš„å®‰å…¨å¯ä¿¡éƒ¨ç½²è‡³å…³é‡è¦ã€‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„å¯¹é½æŠ€æœ¯ï¼Œä½†éœ€è¦æ›´æ–°æ•°åäº¿çš„æ¨¡å‹å‚æ•°ï¼Œè®¡ç®—æˆæœ¬é«˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå—æ§è§£ç æä¾›äº†ä¸€ç§åœ¨æ¨ç†é˜¶æ®µå¯¹é½æ¨¡å‹çš„æœºåˆ¶ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚ç„¶è€Œï¼Œå•æ™ºèƒ½ä½“è§£ç æ–¹æ³•ç”±äºä»»åŠ¡çš„å¤æ‚æ€§å’Œå¯å˜æ€§ï¼Œå¾€å¾€éš¾ä»¥é€‚åº”å¤šæ ·åŒ–çš„ä»»åŠ¡ã€‚ä¸ºäº†å¼ºåŒ–é’ˆå¯¹ç›®æ ‡ä»»åŠ¡çš„æµ‹è¯•æ€§èƒ½ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç°æœ‰ç°æˆçš„å¯¹é½LLMç­–ç•¥çš„æ··åˆå¤šæ™ºèƒ½ä½“è§£ç ç­–ç•¥ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§è§£ç æ–¹æ³•ï¼Œå…è®¸åœ¨ä»¤ç‰Œçº§åˆ«ä¸Šé€‰æ‹©å¤šä¸ªæ™ºèƒ½ä½“ä¹‹é—´çš„ç­–ç•¥ï¼Œä»è€Œå®ç°åœ¨æ¨ç†é˜¶æ®µçš„æ¨¡å‹å¯¹é½ã€‚å¯¹äºæ¯ä¸ªä»¤ç‰Œï¼Œæ ¹æ®é•¿æœŸæ•ˆç”¨æŒ‡æ ‡ä»æ¨¡å‹æ± ä¸­åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„LLMã€‚è¯¥ç­–ç•¥åˆ‡æ¢æœºåˆ¶ç¡®ä¿äº†æ¯ä¸€æ­¥çš„æœ€ä¼˜æ¨¡å‹é€‰æ‹©ï¼Œå®ç°äº†LLMä¹‹é—´çš„æœ‰æ•ˆåä½œå’Œå¯¹é½ã€‚ç†è®ºåˆ†æå’Œå®è¯è¯„ä¼°å‡è¯æ˜äº†è¯¥æ–¹æ³•åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šçš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹é½å¯¹äºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„éƒ¨ç½²è‡³å…³é‡è¦ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰è™½ç„¶æ˜¯ä¸€ç§æœ‰æ•ˆçš„LLMå¯¹é½æŠ€æœ¯ï¼Œä½†éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚</li>
<li>å—æ§è§£ç æä¾›äº†ä¸€ç§åœ¨æ¨ç†é˜¶æ®µå¯¹é½æ¨¡å‹çš„æ›¿ä»£æ–¹æ³•ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>å•æ™ºèƒ½ä½“è§£ç æ–¹æ³•åœ¨åº”å¯¹å¤šæ ·åŒ–ä»»åŠ¡æ—¶é¢ä¸´å›°éš¾ï¼Œå› ä¸ºä»»åŠ¡çš„å¤æ‚æ€§å’Œå¯å˜æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆå¤šæ™ºèƒ½ä½“è§£ç ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åˆ©ç”¨ç°æœ‰çš„å¯¹é½LLMç­–ç•¥ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µé€šè¿‡ä»¤ç‰Œçº§åˆ«çš„é€‰æ‹©å®ç°æ¨¡å‹å¯¹é½ã€‚</li>
<li>è¯¥ç­–ç•¥åˆ‡æ¢æœºåˆ¶ç¡®ä¿äº†æ¯ä¸€æ­¥çš„æœ€ä¼˜æ¨¡å‹é€‰æ‹©ï¼Œæé«˜äº†LLMä¹‹é—´çš„åä½œå’Œå¯¹é½æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21720">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f2bcafac38c5f4feadfcb30c5879d763.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3d7423ae4bc4a972d2e101e40529a5c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MAVERIX-Multimodal-Audio-Visual-Evaluation-Reasoning-IndeX"><a href="#MAVERIX-Multimodal-Audio-Visual-Evaluation-Reasoning-IndeX" class="headerlink" title="MAVERIX: Multimodal Audio-Visual Evaluation Reasoning IndeX"></a>MAVERIX: Multimodal Audio-Visual Evaluation Reasoning IndeX</h2><p><strong>Authors:Liuyue Xie, George Z. Wei, Avik Kuthiala, Ce Zheng, Ananya Bal, Mosam Dabhi, Liting Wen, Taru Rustagi, Ethan Lai, Sushil Khyalia, Rohan Choudhury, Morteza Ziyadi, Xu Zhang, Hao Yang, LÃ¡szlÃ³ A. Jeni</strong></p>
<p>Frontier models have either been language-only or have primarily focused on vision and language modalities. Although recent advancements in models with vision and audio understanding capabilities have shown substantial progress, the field lacks a standardized evaluation framework for thoroughly assessing their cross-modality perception performance. We introduce MAVERIX~(Multimodal Audio-Visual Evaluation Reasoning IndeX), a novel benchmark with 700 videos and 2,556 questions explicitly designed to evaluate multimodal models through tasks that necessitate close integration of video and audio information. MAVERIX uniquely provides models with audiovisual tasks, closely mimicking the multimodal perceptual experiences available to humans during inference and decision-making processes. To our knowledge, MAVERIX is the first benchmark aimed explicitly at assessing comprehensive audiovisual integration. Experiments with state-of-the-art models, including Gemini 1.5 Pro and o1, show performance approaching human levels (around 70% accuracy), while human experts reach near-ceiling performance (95.1%). With standardized evaluation protocols, a rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a challenging testbed for advancing audiovisual multimodal intelligence. </p>
<blockquote>
<p>å‰æ²¿æ¨¡å‹è¦ä¹ˆæ˜¯å•ä¸€è¯­è¨€æ¨¡å‹ï¼Œè¦ä¹ˆä¸»è¦å…³æ³¨è§†è§‰å’Œè¯­è¨€æ¨¡æ€ã€‚å°½ç®¡æœ€è¿‘å…·æœ‰è§†è§‰å’ŒéŸ³é¢‘ç†è§£èƒ½åŠ›çš„æ¨¡å‹çš„æœ€æ–°è¿›å±•å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†è¯¥é¢†åŸŸç¼ºä¹ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶æ¥å…¨é¢è¯„ä¼°å®ƒä»¬çš„è·¨æ¨¡æ€æ„ŸçŸ¥æ€§èƒ½ã€‚æˆ‘ä»¬å¼•å…¥äº†MAVERIXï¼ˆå¤šæ¨¡æ€éŸ³é¢‘è§†è§‰è¯„ä¼°æ¨ç†æŒ‡æ•°ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«700ä¸ªè§†é¢‘å’Œ2556ä¸ªæ˜ç¡®è®¾è®¡çš„é—®é¢˜ï¼Œæ—¨åœ¨é€šè¿‡éœ€è¦å¯†åˆ‡æ•´åˆè§†é¢‘å’ŒéŸ³é¢‘ä¿¡æ¯çš„ä»»åŠ¡æ¥è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹ã€‚MAVERIXç‹¬ç‰¹åœ°æä¾›äº†è§†å¬ä»»åŠ¡ï¼Œç´§å¯†æ¨¡ä»¿äººç±»åœ¨æ¨ç†å’Œå†³ç­–è¿‡ç¨‹ä¸­çš„å¤šæ¨¡æ€æ„ŸçŸ¥ä½“éªŒã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒMAVERIXæ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨æ—¨åœ¨è¯„ä¼°å…¨é¢è§†å¬æ•´åˆèƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ä½¿ç”¨æœ€å‰æ²¿çš„æ¨¡å‹è¿›è¡Œçš„å®éªŒï¼ŒåŒ…æ‹¬Gemini 1.5 Proå’Œo1ï¼Œæ˜¾ç¤ºå…¶æ€§èƒ½æ¥è¿‘äººç±»æ°´å¹³ï¼ˆçº¦70%çš„å‡†ç¡®ç‡ï¼‰ï¼Œè€Œäººç±»ä¸“å®¶åˆ™æ¥è¿‘å¤©èŠ±æ¿æ€§èƒ½ï¼ˆ95.1%ï¼‰ã€‚é€šè¿‡æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ã€ä¸¥æ ¼çš„æ³¨é‡Šç®¡é“å’Œå…¬å…±å·¥å…·åŒ…ï¼ŒMAVERIXä¸ºæ¨è¿›è§†å¬å¤šæ¨¡æ€æ™ºèƒ½æä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•å¹³å°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21699v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€æ„ŸçŸ¥æŠ€æœ¯è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰å’ŒéŸ³é¢‘ç†è§£æ–¹é¢ã€‚ç„¶è€Œï¼Œç¼ºä¹ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶æ¥å…¨é¢è¯„ä¼°è·¨æ¨¡æ€æ„ŸçŸ¥æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡ä»‹ç»äº†MAVERIXï¼ˆå¤šæ¨¡æ€éŸ³é¢‘è§†è§‰è¯„ä¼°æ¨ç†æŒ‡æ•°ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«700ä¸ªè§†é¢‘å’Œ2556ä¸ªé—®é¢˜çš„æ–°å‹åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡éœ€è¦ç´§å¯†èåˆè§†é¢‘å’ŒéŸ³é¢‘ä¿¡æ¯çš„ä»»åŠ¡æ¥è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹ã€‚MAVERIXç‹¬ç‰¹åœ°æä¾›äº†è§†å¬ä»»åŠ¡ï¼Œç´§å¯†æ¨¡ä»¿äººç±»åœ¨æ¨ç†å’Œå†³ç­–è¿‡ç¨‹ä¸­çš„å¤šæ¨¡æ€æ„ŸçŸ¥ä½“éªŒã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒMAVERIXæ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨æ˜ç¡®è¯„ä¼°å…¨é¢è§†å¬æ•´åˆèƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ä½¿ç”¨æœ€å‰æ²¿çš„æ¨¡å‹ï¼ˆå¦‚åŒå­åº§ 1.5 ä¸“ä¸šç‰ˆå’Œ o1ï¼‰è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œå…¶æ€§èƒ½æ¥è¿‘äººç±»æ°´å¹³ï¼ˆçº¦70%å‡†ç¡®ç‡ï¼‰ï¼Œè€Œäººç±»ä¸“å®¶åˆ™è¾¾åˆ°äº†è¿‘ä¸Šé™æ€§èƒ½ï¼ˆ95.1%ï¼‰ã€‚MAVERIXçš„å»ºç«‹ä¸ºæ¨è¿›è§†å¬å¤šæ¨¡æ€æ™ºèƒ½æä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•å¹³å°ï¼ŒåŒ…æ‹¬æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ã€ä¸¥æ ¼çš„æ³¨é‡Šç®¡é“å’Œå…¬å…±å·¥å…·åŒ…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§æ–°å‹åŸºå‡†æµ‹è¯•MAVERIXï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹çš„è·¨æ¨¡æ€æ„ŸçŸ¥æ€§èƒ½ã€‚</li>
<li>MAVERIXåŒ…å«700ä¸ªè§†é¢‘å’Œ2556ä¸ªé—®é¢˜ï¼Œè®¾è®¡æœ‰è§†å¬ä»»åŠ¡ï¼Œæ¨¡æ‹Ÿäººç±»çš„å¤šåª’ä½“æ„ŸçŸ¥ä½“éªŒã€‚</li>
<li>ç›®å‰æœ€å‰æ²¿çš„æ¨¡å‹åœ¨MAVERIXä¸Šçš„æ€§èƒ½æ¥è¿‘äººç±»æ°´å¹³ï¼ˆçº¦70%å‡†ç¡®ç‡ï¼‰ã€‚</li>
<li>äººç±»ä¸“å®¶åœ¨MAVERIXä¸Šçš„è¡¨ç°æ¥è¿‘ä¸Šé™ï¼ˆ95.1%ï¼‰ã€‚</li>
<li>MAVERIXæä¾›äº†æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®ã€ä¸¥æ ¼çš„æ³¨é‡Šç®¡é“å’Œå…¬å…±å·¥å…·åŒ…ã€‚</li>
<li>MAVERIXçš„å»ºç«‹ä¸ºæ¨è¿›è§†å¬å¤šæ¨¡æ€æ™ºèƒ½çš„å‘å±•æä¾›äº†ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æµ‹è¯•å¹³å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21699">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd650d9b0064a35740c1d5152439d888.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9794dc3c3c9c651d7bd7aaebddcfbace.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-851f3bf1a53e329e5cbe707b73492d55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c8da5dcd63dc6b506f2816011e653b81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-801d6c5befe136c0f01df564d9d984e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea95c9848a589e461e212049d089c15a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Embodied-Reasoner-Synergizing-Visual-Search-Reasoning-and-Action-for-Embodied-Interactive-Tasks"><a href="#Embodied-Reasoner-Synergizing-Visual-Search-Reasoning-and-Action-for-Embodied-Interactive-Tasks" class="headerlink" title="Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for   Embodied Interactive Tasks"></a>Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for   Embodied Interactive Tasks</h2><p><strong>Authors:Wenqi Zhang, Mengna Wang, Gangao Liu, Xu Huixin, Yiwei Jiang, Yongliang Shen, Guiyang Hou, Zhe Zheng, Hang Zhang, Xin Li, Weiming Lu, Peng Li, Yueting Zhuang</strong></p>
<p>Recent advances in deep thinking models have demonstrated remarkable reasoning capabilities on mathematical and coding tasks. However, their effectiveness in embodied domains which require continuous interaction with environments through image action interleaved trajectories remains largely -unexplored. We present Embodied Reasoner, a model that extends o1 style reasoning to interactive embodied search tasks. Unlike mathematical reasoning that relies primarily on logical deduction, embodied scenarios demand spatial understanding, temporal reasoning, and ongoing self-reflection based on interaction history. To address these challenges, we synthesize 9.3k coherent Observation-Thought-Action trajectories containing 64k interactive images and 90k diverse thinking processes (analysis, spatial reasoning, reflection, planning, and verification). We develop a three-stage training pipeline that progressively enhances the modelâ€™s capabilities through imitation learning, self-exploration via rejection sampling, and self-correction through reflection tuning. The evaluation shows that our model significantly outperforms those advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and Claude-3.7 by +9%, 24%, and +13%. Analysis reveals our model exhibits fewer repeated searches and logical inconsistencies, with particular advantages in complex long-horizon tasks. Real-world environments also show our superiority while exhibiting fewer repeated searches and logical inconsistency cases. </p>
<blockquote>
<p>è¿‘æœŸæ·±åº¦æ€è€ƒæ¨¡å‹åœ¨æ•°å­¦å’Œç¼–ç ä»»åŠ¡ä¸Šå±•ç°å‡ºå“è¶Šçš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨éœ€è¦ä¸ç¯å¢ƒæŒç»­äº¤äº’çš„å›¾åƒåŠ¨ä½œè½¨è¿¹æ–¹é¢çš„å®é™…åº”ç”¨é¢†åŸŸä¸­çš„æ•ˆæœä»ç„¶å¾ˆå¤§ç¨‹åº¦ä¸Šæœªè¢«æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†Embodied Reasoneræ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†ä¸€é˜¶æ¨ç†æ‰©å±•åˆ°äº¤äº’å¼å®ä½“æœç´¢ä»»åŠ¡ã€‚ä¸ä¸»è¦ä¾èµ–äºé€»è¾‘æ¼”ç»çš„æ•°å­¦æ¨ç†ä¸åŒï¼Œå®ä½“åœºæ™¯éœ€è¦ç©ºé—´ç†è§£ã€æ—¶é—´æ¨ç†ä»¥åŠåŸºäºäº¤äº’å†å²çš„æŒç»­è‡ªæˆ‘åæ€ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ç»¼åˆäº†9.3kä¸ªè¿è´¯çš„è§‚å¯Ÿ-æ€è€ƒ-è¡ŒåŠ¨è½¨è¿¹ï¼ŒåŒ…å«6.4ä¸‡å¼ äº¤äº’å›¾åƒå’Œ9ä¸‡å¤šä¸ªå¤šæ ·åŒ–çš„æ€è€ƒè¿‡ç¨‹ï¼ˆåˆ†æã€ç©ºé—´æ¨ç†ã€åæ€ã€è§„åˆ’å’ŒéªŒè¯ï¼‰ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåˆ†ä¸‰é˜¶æ®µçš„è®­ç»ƒç®¡é“ï¼Œé€šè¿‡æ¨¡ä»¿å­¦ä¹ ã€é€šè¿‡æ‹’ç»é‡‡æ ·çš„è‡ªæˆ‘æ¢ç´¢ä»¥åŠé€šè¿‡åæ€è°ƒæ•´çš„è‡ªæˆ‘ä¿®æ­£ï¼Œé€æ­¥å¢å¼ºæ¨¡å‹çš„èƒ½åŠ›ã€‚è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ˜¾è‘—ä¼˜äºå…ˆè¿›çš„è§†è§‰æ¨ç†æ¨¡å‹ï¼Œä¾‹å¦‚ï¼Œå®ƒæ¯”OpenAIçš„ä¸€é˜¶ã€ä¸‰é˜¶å°æ¨¡å‹å’ŒClaude-3.7é«˜å‡º+9%ã€24%å’Œ+13%ã€‚åˆ†ææ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤æ‚çš„é•¿æœŸä»»åŠ¡ä¸­å…·æœ‰è¾ƒå°‘çš„é‡å¤æœç´¢å’Œé€»è¾‘ä¸ä¸€è‡´æ€§ï¼Œå…·æœ‰ç‰¹åˆ«çš„ä¼˜åŠ¿ã€‚åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„è¡¨ç°ä¹Ÿè¯æ˜äº†æˆ‘ä»¬çš„ä¼˜è¶Šæ€§ï¼Œè¡¨ç°å‡ºè¾ƒå°‘çš„é‡å¤æœç´¢å’Œé€»è¾‘ä¸ä¸€è‡´æƒ…å†µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21696v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/zwq2018/embodied_reasoner">https://github.com/zwq2018/embodied_reasoner</a> Dataset:   <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/zwq2018/embodied_reasoner">https://huggingface.co/datasets/zwq2018/embodied_reasoner</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¿‘æœŸæ·±åº¦æ€è€ƒæ¨¡å‹åœ¨ä½“æ„Ÿé¢†åŸŸï¼ˆéœ€è¦ä¸ç¯å¢ƒè¿ç»­äº’åŠ¨çš„ä»»åŠ¡ï¼‰ä¸­çš„è¡¨ç°ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¨¡å‹â€”â€”Embodied Reasonerã€‚è¯¥æ¨¡å‹èƒ½åº”ç”¨äºäº’åŠ¨å¼ä½“æ„Ÿæœç´¢ä»»åŠ¡ï¼Œå¹¶é€šè¿‡åˆæˆObservation-Thought-Actionè½¨è¿¹æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶åœ¨ç©ºé—´ç†è§£ã€æ—¶é—´æ¨ç†å’ŒæŒç»­è‡ªæˆ‘åæ€æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚é€šè¿‡ä¸‰ä¸ªé˜¶æ®µçš„å­¦ä¹ ç®¡é“ï¼Œæ¨¡å‹èƒ½åŠ›é€æ­¥æå‡ï¼Œå¹¶åœ¨å¤æ‚é•¿å‘¨æœŸä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚ç›¸è¾ƒäºå…¶ä»–å…ˆè¿›çš„è§†è§‰æ¨ç†æ¨¡å‹ï¼ŒEmbodied Reasonerè¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦æ€è€ƒæ¨¡å‹åœ¨ä½“æ„Ÿé¢†åŸŸï¼ˆéœ€è¦ä¸ç¯å¢ƒè¿ç»­äº’åŠ¨çš„ä»»åŠ¡ï¼‰ä¸­çš„è¡¨ç°è¢«æå‡ºå¹¶å—åˆ°å…³æ³¨ã€‚</li>
<li>Embodied Reasoneræ¨¡å‹è¢«å¼•å…¥ï¼Œé€‚ç”¨äºäº’åŠ¨å¼ä½“æ„Ÿæœç´¢ä»»åŠ¡ã€‚</li>
<li>æ¨¡å‹é€šè¿‡åˆæˆObservation-Thought-Actionè½¨è¿¹æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¼ºè°ƒç©ºé—´ç†è§£ã€æ—¶é—´æ¨ç†å’ŒæŒç»­è‡ªæˆ‘åæ€çš„é‡è¦æ€§ã€‚</li>
<li>æ¨¡å‹ç»è¿‡ä¸‰ä¸ªé˜¶æ®µçš„å­¦ä¹ ç®¡é“ï¼ŒåŒ…æ‹¬æ¨¡ä»¿å­¦ä¹ ã€è‡ªæˆ‘æ¢ç´¢å’Œè‡ªæˆ‘æ ¡æ­£ï¼Œé€æ­¥æå‡èƒ½åŠ›ã€‚</li>
<li>ä¸å…¶ä»–è§†è§‰æ¨ç†æ¨¡å‹ç›¸æ¯”ï¼ŒEmbodied Reasonerå±•ç°å‡ºæ›´é«˜çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚é•¿å‘¨æœŸä»»åŠ¡ä¸­ã€‚</li>
<li>æ¨¡å‹åœ¨å®é™…ç¯å¢ƒä¸­çš„è¡¨ç°ä¹Ÿä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå…·æœ‰è¾ƒå°‘çš„é‡å¤æœç´¢å’Œé€»è¾‘ä¸ä¸€è‡´æƒ…å†µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21696">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4fa3b3583809467b8ce7ac78161f7c20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c947b0b5c722c39dd72ca4b374a6d0d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-872f634a4b5c5c239cdf40cb14b2bb0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3602b6feedcb6995476c355a6821aeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a698191c01813091c48767830722d4e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="JiraiBench-A-Bilingual-Benchmark-for-Evaluating-Large-Language-Modelsâ€™-Detection-of-Human-Self-Destructive-Behavior-Content-in-Jirai-Community"><a href="#JiraiBench-A-Bilingual-Benchmark-for-Evaluating-Large-Language-Modelsâ€™-Detection-of-Human-Self-Destructive-Behavior-Content-in-Jirai-Community" class="headerlink" title="JiraiBench: A Bilingual Benchmark for Evaluating Large Language Modelsâ€™   Detection of Human Self-Destructive Behavior Content in Jirai Community"></a>JiraiBench: A Bilingual Benchmark for Evaluating Large Language Modelsâ€™   Detection of Human Self-Destructive Behavior Content in Jirai Community</h2><p><strong>Authors:Yunze Xiao, Tingyu He, Lionel Z. Wang, Yiming Ma, Xingyu Song, Xiaohang Xu, Irene Li, Ka Chung Ng</strong></p>
<p>This paper introduces JiraiBench, the first bilingual benchmark for evaluating large language modelsâ€™ effectiveness in detecting self-destructive content across Chinese and Japanese social media communities. Focusing on the transnational â€œJiraiâ€ (landmine) online subculture that encompasses multiple forms of self-destructive behaviors including drug overdose, eating disorders, and self-harm, we present a comprehensive evaluation framework incorporating both linguistic and cultural dimensions. Our dataset comprises 10,419 Chinese posts and 5,000 Japanese posts with multidimensional annotation along three behavioral categories, achieving substantial inter-annotator agreement. Experimental evaluations across four state-of-the-art models reveal significant performance variations based on instructional language, with Japanese prompts unexpectedly outperforming Chinese prompts when processing Chinese content. This emergent cross-cultural transfer suggests that cultural proximity can sometimes outweigh linguistic similarity in detection tasks. Cross-lingual transfer experiments with fine-tuned models further demonstrate the potential for knowledge transfer between these language systems without explicit target language training. These findings highlight the need for culturally-informed approaches to multilingual content moderation and provide empirical evidence for the importance of cultural context in developing more effective detection systems for vulnerable online communities. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†JiraiBenchï¼Œå®ƒæ˜¯é¦–ä¸ªåŒè¯­åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ£€æµ‹å’Œè¯†åˆ«ä¸­æ–‡å’Œæ—¥è¯­ç¤¾äº¤åª’ä½“ç¤¾åŒºä¸­çš„è‡ªæˆ‘ç ´åæ€§å†…å®¹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æœ¬æ–‡é‡ç‚¹å…³æ³¨è·¨å›½ç•Œçš„â€œJiraiâ€ï¼ˆåœ°é›·ï¼‰åœ¨çº¿äºšæ–‡åŒ–ï¼Œè¿™ç§æ–‡åŒ–åŒ…å«å¤šç§å½¢å¼çš„è‡ªæˆ‘ç ´åæ€§è¡Œä¸ºï¼ŒåŒ…æ‹¬è¿‡é‡æœè¯ã€é¥®é£Ÿå¤±è°ƒå’Œè‡ªæˆ‘ä¼¤å®³ç­‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œå…¶ä¸­åŒ…æ‹¬è¯­è¨€å’Œæ–‡åŒ–çš„åŒé‡ç»´åº¦ã€‚æˆ‘ä»¬çš„æ•°æ®é›†åŒ…å«ä¸­å›½å¸–å­åä¸‡å››åƒä¸€ç™¾ä¹åä¹ä¸ªå’Œæ—¥æœ¬å¸–å­äº”åƒä¸ªï¼Œæ¶µç›–äº†ä¸‰ä¸ªè¡Œä¸ºç±»åˆ«çš„å¤šç»´åº¦æ³¨é‡Šï¼Œå®ç°äº†æ˜¾è‘—çš„åˆ†æå¸ˆé—´ä¸€è‡´æ€§ã€‚åœ¨å››ç§æœ€æ–°æ¨¡å‹ä¸Šçš„å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œæ ¹æ®æŒ‡ä»¤æ€§è¯­è¨€çš„ä¸åŒï¼Œæ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå…¶ä¸­åœ¨å¤„ç†ä¸­æ–‡å†…å®¹æ—¶ï¼Œæ—¥è¯­æç¤ºæ„å¤–åœ°ä¼˜äºä¸­æ–‡æç¤ºã€‚è¿™ç§è·¨æ–‡åŒ–è¿ç§»çš„å‡ºç°è¡¨æ˜ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ–‡åŒ–æ¥è¿‘åº¦å¯èƒ½ä¼šè¶…è¿‡è¯­è¨€ç›¸ä¼¼æ€§åœ¨æ£€æµ‹ä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚å¯¹å¾®è°ƒæ¨¡å‹çš„è·¨è¯­è¨€è¿ç§»å®éªŒè¿›ä¸€æ­¥è¯æ˜äº†åœ¨è¿™äº›è¯­è¨€ç³»ç»Ÿä¹‹é—´è¿›è¡ŒçŸ¥è¯†è¿ç§»çš„æ½œåŠ›ï¼Œæ— éœ€æ˜ç¡®çš„ç›®æ ‡è¯­è¨€è®­ç»ƒã€‚è¿™äº›å‘ç°çªæ˜¾äº†æ–‡åŒ–èƒŒæ™¯ä¸‹çš„è·¨è¯­è¨€æ–¹æ³•å¯¹äºå¤šè¯­è¨€å†…å®¹ç®¡ç†çš„å¿…è¦æ€§ï¼Œå¹¶ä¸ºå¼€å‘é’ˆå¯¹è„†å¼±åœ¨çº¿ç¤¾åŒºçš„æ›´æœ‰æ•ˆæ£€æµ‹ç³»ç»Ÿåœ¨æ–‡åŒ–èƒŒæ™¯æ–¹é¢çš„é‡è¦æ€§æä¾›äº†å®è¯è¯æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21679v1">PDF</a> 20 pages, 1 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†JiraiBenchï¼Œé¦–ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸­æ–‡å’Œæ—¥è¯­ç¤¾äº¤åª’ä½“ç¤¾åŒºä¸­æ£€æµ‹è‡ªæˆ‘æ¯ç­å†…å®¹æ•ˆæœçš„åŒè¯­åŸºå‡†æµ‹è¯•ã€‚æ–‡ç« ä»¥è·¨å›½â€œJiraiâ€ï¼ˆåœ°é›·ï¼‰åœ¨çº¿äºšæ–‡åŒ–ä¸ºèƒŒæ™¯ï¼Œæ¶µç›–è¯ç‰©è¿‡é‡ã€é¥®é£Ÿå¤±è°ƒå’Œè‡ªæˆ‘ä¼¤å®³ç­‰å¤šç§è‡ªæˆ‘æ¯ç­è¡Œä¸ºï¼Œæå‡ºäº†ä¸€ä¸ªç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èå…¥äº†è¯­è¨€å’Œæ–‡åŒ–çš„åŒé‡ç»´åº¦ã€‚æ•°æ®é›†åŒ…å«10,419æ¡ä¸­æ–‡å¸–å­å’Œ5,000æ¡æ—¥è¯­å¸–å­ï¼ŒæŒ‰ä¸‰ä¸ªè¡Œä¸ºç±»åˆ«è¿›è¡Œå¤šç»´åº¦æ ‡æ³¨ï¼Œå¹¶å®ç°äº†æ˜¾è‘—çš„è·¨æ ‡æ³¨è€…ä¸€è‡´æ€§ã€‚å®éªŒè¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒåŸºäºæŒ‡ç¤ºæ€§è¯­è¨€çš„æ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œä¸”åœ¨å¤„ç†ä¸­æ–‡å†…å®¹æ—¶ï¼Œæ—¥è¯­æç¤ºå‡ºä¹æ„æ–™åœ°ä¼˜äºä¸­æ–‡æç¤ºã€‚è¿™ç§è·¨æ–‡åŒ–è½¬ç§»çš„å‡ºç°è¡¨æ˜ï¼Œåœ¨æŸäº›æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œæ–‡åŒ–æ¥è¿‘æ€§æœ‰æ—¶å¯èƒ½æ¯”è¯­è¨€ç›¸ä¼¼æ€§æ›´é‡è¦ã€‚å¯¹ç²¾ç»†è°ƒæ•´è¿‡çš„æ¨¡å‹è¿›è¡Œè·¨è¯­è¨€è½¬ç§»å®éªŒï¼Œè¿›ä¸€æ­¥è¯æ˜äº†åœ¨è¿™äº›è¯­è¨€ç³»ç»Ÿä¹‹é—´è¿›è¡ŒçŸ¥è¯†è½¬ç§»è€Œä¸è¿›è¡Œæ˜ç¡®çš„ç›®æ ‡è¯­è¨€åŸ¹è®­çš„æ½œåŠ›ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨å¤šå…ƒè¯­è¨€å†…å®¹ç®¡ç†ä¸­è€ƒè™‘æ–‡åŒ–å› ç´ çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå¼€å‘é’ˆå¯¹è„†å¼±åœ¨çº¿ç¤¾åŒºçš„æ›´æœ‰æ•ˆæ£€æµ‹ç³»ç»Ÿæä¾›äº†å®è¯è¯æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>JiraiBenchæ˜¯é¦–ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„åŒè¯­åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å…¶åœ¨ä¸­æ–‡å’Œæ—¥è¯­ç¤¾äº¤åª’ä½“ä¸­æ£€æµ‹è‡ªæˆ‘æ¯ç­å†…å®¹çš„æ•ˆæœã€‚</li>
<li>è¯„ä¼°æ¡†æ¶ç»¼åˆè€ƒè™‘äº†è¯­è¨€å’Œæ–‡åŒ–çš„åŒé‡ç»´åº¦ï¼Œåæ˜ äº†è·¨å›½åœ¨çº¿äºšæ–‡åŒ–â€œJiraiâ€çš„ç‰¹ç‚¹ã€‚</li>
<li>æ•°æ®é›†åŒ…å«ä¸­æ–‡å’Œæ—¥è¯­çš„ç¤¾äº¤åª’ä½“å¸–å­ï¼Œå¹¶è¿›è¡Œäº†å¤šç»´åº¦çš„æ ‡æ³¨ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¤„ç†æŸäº›ä»»åŠ¡æ—¶ï¼Œæ–‡åŒ–æ¥è¿‘æ€§æ¯”è¯­è¨€ç›¸ä¼¼æ€§æ›´é‡è¦ã€‚</li>
<li>ä¸åŒè¯­è¨€çš„æç¤ºå¯¹æ¨¡å‹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œæ—¥è¯­æç¤ºåœ¨å¤„ç†ä¸­æ–‡å†…å®¹æ—¶è¡¨ç°å‡ºæ„å¤–çš„ä¼˜åŠ¿ã€‚</li>
<li>è·¨è¯­è¨€è½¬ç§»å®éªŒè¯æ˜äº†ä¸åŒè¯­è¨€ç³»ç»Ÿé—´çš„çŸ¥è¯†è½¬ç§»æ½œåŠ›ï¼Œæ— éœ€æ˜ç¡®çš„ç›®æ ‡è¯­è¨€åŸ¹è®­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1c6cf02a68b4b487d71cb0c92b4981ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a6f0b26deea0262ebbf18bf554ee1b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54e6e07730dec6ab60c1af107a14fb4f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SWI-Speaking-with-Intent-in-Large-Language-Models"><a href="#SWI-Speaking-with-Intent-in-Large-Language-Models" class="headerlink" title="SWI: Speaking with Intent in Large Language Models"></a>SWI: Speaking with Intent in Large Language Models</h2><p><strong>Authors:Yuwei Yin, EunJeong Hwang, Giuseppe Carenini</strong></p>
<p>Intent, typically clearly formulated and planned, functions as a cognitive framework for reasoning and problem-solving. This paper introduces the concept of Speaking with Intent (SWI) in large language models (LLMs), where the explicitly generated intent encapsulates the modelâ€™s underlying intention and provides high-level planning to guide subsequent analysis and communication. By emulating deliberate and purposeful thoughts in the human mind, SWI is hypothesized to enhance the reasoning capabilities and generation quality of LLMs. Extensive experiments on mathematical reasoning benchmarks consistently demonstrate the superiority of Speaking with Intent over Baseline (i.e., generation without explicit intent). Moreover, SWI outperforms answer-trigger prompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive performance with the strong method ARR (Analyzing, Retrieving, and Reasoning). Additionally, the effectiveness and generalizability of SWI are solidified on reasoning-intensive question answering (QA) and text summarization benchmarks, where SWI brings consistent improvement to the Baseline generation. In text summarization, SWI-generated summaries exhibit greater accuracy, conciseness, and factual correctness, with fewer hallucinations. Furthermore, human evaluations verify the coherence, effectiveness, and interpretability of the intent produced by SWI. This proof-of-concept study creates a novel avenue for enhancing LLMsâ€™ reasoning abilities with cognitive notions. </p>
<blockquote>
<p>æ„å›¾é€šå¸¸è¢«æ˜ç¡®åˆ¶å®šå’Œè®¡åˆ’ï¼Œä½œä¸ºæ¨ç†å’Œè§£å†³é—®é¢˜çš„è®¤çŸ¥æ¡†æ¶ã€‚æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„â€œå¸¦æœ‰æ„å›¾çš„è¡¨è¿°â€ï¼ˆSpeaking with Intentï¼Œç®€ç§°SWIï¼‰è¿™ä¸€æ¦‚å¿µï¼Œå…¶ä¸­æ˜ç¡®ç”Ÿæˆçš„æ„å›¾åŒ…å«äº†æ¨¡å‹çš„åŸºæœ¬æ„å›¾ï¼Œå¹¶ä¸ºéšåçš„åˆ†æå’Œé€šä¿¡æä¾›äº†é«˜çº§è§„åˆ’ã€‚é€šè¿‡æ¨¡æ‹Ÿäººç±»æ€ç»´ä¸­çš„æ·±æ€ç†Ÿè™‘å’Œç›®çš„æ€§æ€è€ƒï¼Œå‡è®¾SWIå¯ä»¥æé«˜LLMçš„æ¨ç†èƒ½åŠ›å’Œç”Ÿæˆè´¨é‡ã€‚åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒä¸­ï¼Œå¸¦æœ‰æ„å›¾çš„è¡¨è¿°å§‹ç»ˆè¡¨ç°å‡ºä¼˜äºåŸºå‡†çº¿ï¼ˆå³æ— æ˜ç¡®æ„å›¾çš„ç”Ÿæˆï¼‰ã€‚æ­¤å¤–ï¼ŒSWIçš„è¡¨ç°ä¼˜äºç­”æ¡ˆè§¦å‘æç¤ºæ–¹æ³•Chain-of-Thoughtå’ŒPlan-and-Solveï¼Œå¹¶ä¸å¼ºå¤§çš„æ–¹æ³•ARRï¼ˆåˆ†æã€æ£€ç´¢å’Œæ¨ç†ï¼‰ä¿æŒç«äº‰åŠ›ã€‚å¦å¤–ï¼Œåœ¨éœ€è¦å¤§é‡æ¨ç†çš„é—®ç­”å’Œæ–‡æœ¬æ‘˜è¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSWIçš„æœ‰æ•ˆæ€§æ³›åŒ–æ€§å¾—åˆ°äº†å·©å›ºï¼Œå®ƒä¸ºåŸºå‡†çº¿ç”Ÿæˆå¸¦æ¥äº†æŒç»­çš„æ”¹è¿›ã€‚åœ¨æ–‡æœ¬æ‘˜è¦ä¸­ï¼Œç”±SWIç”Ÿæˆçš„æ‘˜è¦è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ã€ç®€æ´æ€§å’Œäº‹å®æ­£ç¡®æ€§ï¼Œå¹»è§‰æ›´å°‘ã€‚æ­¤å¤–ï¼Œäººç±»è¯„ä¼°éªŒè¯äº†ç”±SWIäº§ç”Ÿçš„æ„å›¾çš„è¿è´¯æ€§ã€æœ‰æ•ˆæ€§å’Œå¯è§£é‡Šæ€§ã€‚è¿™é¡¹æ¦‚å¿µéªŒè¯ç ”ç©¶ä¸ºåˆ©ç”¨è®¤çŸ¥æ¦‚å¿µå¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›å¼€è¾Ÿäº†ä¸€æ¡æ–°é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21544v1">PDF</a> 24 pages. Code: <a target="_blank" rel="noopener" href="https://github.com/YuweiYin/SWI">https://github.com/YuweiYin/SWI</a></p>
<p><strong>Summary</strong>ï¼š<br>æ„å›¾é€šå¸¸è¢«æ˜ç¡®åœ°åˆ¶å®šå’Œè®¡åˆ’ï¼Œä½œä¸ºæ¨ç†å’Œè§£å†³é—®é¢˜çš„è®¤çŸ¥æ¡†æ¶ã€‚æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„â€œå¸¦æ„å›¾è¡¨è¾¾ï¼ˆSWIï¼‰â€çš„æ¦‚å¿µï¼Œæ˜¾å¼ç”Ÿæˆçš„æ„å›¾æ¶µç›–äº†æ¨¡å‹çš„åŸºæœ¬æ„å›¾ï¼Œå¹¶æä¾›é«˜çº§è§„åˆ’æ¥æŒ‡å¯¼åç»­åˆ†æå’Œé€šä¿¡ã€‚é€šè¿‡æ¨¡æ‹Ÿäººç±»æ€ç»´çš„æ·±æ€ç†Ÿè™‘å’Œç›®çš„æ€§ï¼Œå‡è®¾SWIå¯ä»¥æé«˜LLMçš„æ¨ç†èƒ½åŠ›å’Œç”Ÿæˆè´¨é‡ã€‚åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸æ— æ˜ç¡®æ„å›¾çš„åŸºçº¿ç”Ÿæˆç›¸æ¯”ï¼Œå¸¦æ„å›¾è¡¨è¾¾å…·æœ‰ä¼˜è¶Šæ€§ã€‚æ­¤å¤–ï¼Œåœ¨æ–‡æœ¬æ‘˜è¦åŸºå‡†æµ‹è¯•ä¸Šï¼Œå¸¦æ„å›¾è¡¨è¾¾ä¹Ÿå¸¦æ¥äº†æŒç»­çš„æ”¹è¿›ã€‚äººç±»è¯„ä¼°éªŒè¯äº†ç”±å¸¦æ„å›¾è¡¨è¾¾äº§ç”Ÿçš„æ„å›¾çš„è¿è´¯æ€§ã€æœ‰æ•ˆæ€§å’Œå¯è§£é‡Šæ€§ã€‚è¿™é¡¹æ¦‚å¿µéªŒè¯ç ”ç©¶ä¸ºåˆ©ç”¨è®¤çŸ¥æ¦‚å¿µå¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›å¼€è¾Ÿäº†ä¸€æ¡æ–°é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è®ºæ–‡æå‡ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¸¦æ„å›¾è¡¨è¾¾ï¼ˆSWIï¼‰æ¦‚å¿µï¼Œæ—¨åœ¨å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œç”Ÿæˆè´¨é‡ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿäººç±»æ€ç»´çš„æ·±æ€ç†Ÿè™‘å’Œç›®çš„æ€§ï¼Œå‡è®¾å¹¶éªŒè¯äº†å¸¦æ„å›¾è¡¨è¾¾å¯¹äºæå‡æ¨¡å‹æ€§èƒ½çš„é‡è¦æ€§ã€‚</li>
<li>ä¸åŸºçº¿ç”Ÿæˆç›¸æ¯”ï¼Œå¸¦æ„å›¾è¡¨è¾¾åœ¨æ•°å­¦æ¨ç†ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>å¸¦æ„å›¾è¡¨è¾¾åœ¨æ•°å­¦æ¨ç†å’Œæ–‡æœ¬æ‘˜è¦æ–¹é¢çš„å®éªŒè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</li>
<li>åœ¨æ–‡æœ¬æ‘˜è¦ä»»åŠ¡ä¸­ï¼Œå¸¦æ„å›¾è¡¨è¾¾ç”Ÿæˆçš„æ‘˜è¦æ›´å‡†ç¡®ã€ç®€æ´ã€ç¬¦åˆäº‹å®ä¸”å‡å°‘äº†è™šæ„æƒ…å†µçš„å‡ºç°ã€‚</li>
<li>äººç±»è¯„ä¼°æ˜¾ç¤ºå¸¦æ„å›¾è¡¨è¾¾çš„æ„å›¾å…·æœ‰è‰¯å¥½çš„è¿è´¯æ€§ã€æœ‰æ•ˆæ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21544">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bd046f266ae6527b977db91ed0cfb4c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-769b75edb145147f853fdb2f62926e64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5546f28d60e625754b80d779154d0a55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48527aea34ee0a0a94664d3a7756103a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b04c4eac737aceab5a93ae91593bd93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a58477b77438b1bf1b95c3e342f3d6da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea85a8f58bd643fee45ccbdfd6095a92.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Graph-to-Vision-Multi-graph-Understanding-and-Reasoning-using-Vision-Language-Models"><a href="#Graph-to-Vision-Multi-graph-Understanding-and-Reasoning-using-Vision-Language-Models" class="headerlink" title="Graph-to-Vision: Multi-graph Understanding and Reasoning using   Vision-Language Models"></a>Graph-to-Vision: Multi-graph Understanding and Reasoning using   Vision-Language Models</h2><p><strong>Authors:Ruizhou Li, Haiyun Jiang</strong></p>
<p>Graph Neural Networks (GNNs), as the dominant paradigm for graph-structured learning, have long faced dual challenges of exponentially escalating computational complexity and inadequate cross-scenario generalization capability. With the rapid advancement of multimodal learning, Vision-Language Models (VLMs) have demonstrated exceptional cross-modal relational reasoning capabilities and generalization capacities, thereby opening up novel pathways for overcoming the inherent limitations of conventional graph learning paradigms. However, current research predominantly concentrates on investigating the single-graph reasoning capabilities of VLMs, which fundamentally fails to address the critical requirement for coordinated reasoning across multiple heterogeneous graph data in real-world application scenarios. To address these limitations, we propose the first multi-graph joint reasoning benchmark for VLMs. Our benchmark encompasses four graph categories: knowledge graphs, flowcharts, mind maps, and route maps,with each graph group accompanied by three progressively challenging instruction-response pairs. Leveraging this benchmark, we conducted comprehensive capability assessments of state-of-the-art VLMs and performed fine-tuning on open-source models. This study not only addresses the underexplored evaluation gap in multi-graph reasoning for VLMs but also empirically validates their generalization superiority in graph-structured learning. </p>
<blockquote>
<p>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ä½œä¸ºå›¾ç»“æ„å­¦ä¹ çš„ä¸»æµèŒƒå¼ï¼Œé•¿æœŸä»¥æ¥ä¸€ç›´é¢ä¸´ç€è®¡ç®—å¤æ‚åº¦æŒ‡æ•°çº§ä¸Šå‡å’Œè·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„åŒé‡æŒ‘æˆ˜ã€‚éšç€å¤šæ¨¡æ€å­¦ä¹ çš„å¿«é€Ÿå‘å±•ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¡¨ç°å‡ºäº†å“è¶Šçš„å¤šæ¨¡æ€å…³ç³»æ¨ç†èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œå¼€è¾Ÿäº†å…‹æœä¼ ç»Ÿå›¾å­¦ä¹ èŒƒå¼å›ºæœ‰å±€é™æ€§çš„æ–°é€”å¾„ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è°ƒæŸ¥VLMsçš„å•å›¾æ¨ç†èƒ½åŠ›ä¸Šï¼Œè¿™ä»æ ¹æœ¬ä¸Šæœªèƒ½æ»¡è¶³ç°å®ä¸–ç•Œåº”ç”¨åœºæ™¯ä¸­è·¨å¤šä¸ªå¼‚æ„å›¾æ•°æ®çš„ååŒæ¨ç†çš„è¿«åˆ‡éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªé’ˆå¯¹VLMsçš„å¤šå›¾è”åˆæ¨ç†åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•åŒ…å«å››ç§ç±»å‹çš„å›¾ï¼šçŸ¥è¯†å›¾è°±ã€æµç¨‹å›¾ã€æ€ç»´å¯¼å›¾å’Œè·¯çº¿å›¾ï¼Œæ¯ç§ç±»å‹çš„å›¾éƒ½é…å¤‡äº†ä¸‰ç»„é€æ­¥æŒ‘æˆ˜æ€§çš„æŒ‡ä»¤-å“åº”å¯¹ã€‚åˆ©ç”¨è¿™ä¸€åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„VLMsè¿›è¡Œäº†å…¨é¢çš„èƒ½åŠ›è¯„ä¼°ï¼Œå¹¶å¯¹å¼€æºæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚è¿™é¡¹ç ”ç©¶ä¸ä»…è§£å†³äº†VLMsåœ¨å›¾æ¨ç†æ–¹é¢çš„è¯„ä¼°ç¼ºå£ï¼Œè€Œä¸”ä»å®è¯ä¸ŠéªŒè¯äº†å®ƒä»¬åœ¨å›¾ç»“æ„å­¦ä¹ ä¸­çš„æ³›åŒ–ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21435v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Graphç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰é¢ä¸´è®¡ç®—å¤æ‚æ€§æ€¥å‰§ä¸Šå‡å’Œè·¨åœºæ™¯æ³›åŒ–èƒ½åŠ›ä¸è¶³çš„åŒé‡æŒ‘æˆ˜ã€‚éšç€å¤šæ¨¡æ€å­¦ä¹ ï¼ˆMultimodal Learningï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å±•ç°å‡ºå¼ºå¤§çš„è·¨æ¨¡æ€å…³ç³»æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå…‹æœä¼ ç»Ÿå›¾å­¦ä¹ èŒƒå¼çš„å±€é™æ€§æä¾›äº†æ–°é€”å¾„ã€‚ç„¶è€Œï¼Œå½“å‰ç ”ç©¶ä¸»è¦å…³æ³¨VLMsçš„å•å›¾æ¨ç†èƒ½åŠ›ï¼Œæœªèƒ½æ»¡è¶³ç°å®åº”ç”¨ä¸­å¤šå¼‚æ„å›¾æ•°æ®çš„ååŒæ¨ç†éœ€æ±‚ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºå¤šå›¾è”åˆæ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆMulti-Graph Joint Reasoning Benchmarkï¼‰ï¼Œæ¶µç›–çŸ¥è¯†å›¾è°±ã€æµç¨‹å›¾ã€æ€ç»´å¯¼å›¾å’Œè·¯çº¿å›¾å››ç±»å›¾ã€‚é€šè¿‡è¿™ä¸€åŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„VLMsè¿›è¡Œäº†å…¨é¢çš„èƒ½åŠ›è¯„ä¼°ï¼Œå¹¶å¯¹å¼€æºæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒã€‚è¿™ä¸ä»…å¡«è¡¥äº†VLMsåœ¨å¤šå›¾æ¨ç†æ–¹é¢çš„è¯„ä¼°ç©ºç™½ï¼Œè€Œä¸”å®è¯äº†å…¶åœ¨å›¾ç»“æ„å­¦ä¹ ä¸­çš„æ³›åŒ–ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Graph Neural Networks (GNNs) é¢ä¸´è®¡ç®—å¤æ‚æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„æŒ‘æˆ˜ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€å­¦ä¹ ä¸­å±•ç°å‡ºå¼ºå¤§çš„è·¨æ¨¡æ€å…³ç³»æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸»è¦å…³æ³¨VLMsçš„å•å›¾æ¨ç†èƒ½åŠ›ï¼Œç¼ºä¹å¤šå¼‚æ„å›¾æ•°æ®çš„ååŒæ¨ç†ç ”ç©¶ã€‚</li>
<li>é¦–æ¬¡æå‡ºå¤šå›¾è”åˆæ¨ç†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬çŸ¥è¯†å›¾è°±ã€æµç¨‹å›¾ã€æ€ç»´å¯¼å›¾å’Œè·¯çº¿å›¾å››ç±»ã€‚</li>
<li>é€šè¿‡åŸºå‡†æµ‹è¯•ï¼Œå¯¹VLMsè¿›è¡Œäº†å…¨é¢çš„èƒ½åŠ›è¯„ä¼°ã€‚</li>
<li>å®è¯äº†VLMsåœ¨å›¾ç»“æ„å­¦ä¹ ä¸­çš„æ³›åŒ–ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21435">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-508cc224cb4f25d7efc104185861fb1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24f8c5510e70634b665bee47030f1890.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f842635baafd3bbef8d353afd57e9e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5a550296bc1685abe615c3a9a0c55ea.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Challenging-the-Boundaries-of-Reasoning-An-Olympiad-Level-Math-Benchmark-for-Large-Language-Models"><a href="#Challenging-the-Boundaries-of-Reasoning-An-Olympiad-Level-Math-Benchmark-for-Large-Language-Models" class="headerlink" title="Challenging the Boundaries of Reasoning: An Olympiad-Level Math   Benchmark for Large Language Models"></a>Challenging the Boundaries of Reasoning: An Olympiad-Level Math   Benchmark for Large Language Models</h2><p><strong>Authors:Haoxiang Sun, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, Lei Fang, Ji-Rong Wen</strong></p>
<p>In recent years, the rapid development of large reasoning models has resulted in the saturation of existing benchmarks for evaluating mathematical reasoning, highlighting the urgent need for more challenging and rigorous evaluation frameworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level mathematical benchmark, designed to rigorously test the complex reasoning capabilities of LLMs. OlymMATH features 200 meticulously curated problems, each manually verified and available in parallel English and Chinese versions. The problems are systematically organized into two distinct difficulty tiers: (1) AIME-level problems (easy) that establish a baseline for mathematical reasoning assessment, and (2) significantly more challenging problems (hard) designed to push the boundaries of current state-of-the-art models. In our benchmark, these problems span four core mathematical fields, each including a verifiable numerical solution to enable objective, rule-based evaluation. Empirical results underscore the significant challenge presented by OlymMATH, with state-of-the-art models including DeepSeek-R1 and OpenAIâ€™s o3-mini demonstrating notably limited accuracy on the hard subset. Furthermore, the benchmark facilitates comprehensive bilingual assessment of mathematical reasoning abilities-a critical dimension that remains largely unaddressed in mainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark at the STILL project: <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Slow_Thinking_with_LLMs">https://github.com/RUCAIBox/Slow_Thinking_with_LLMs</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹æ¨ç†æ¨¡å‹çš„å¿«é€Ÿå‘å±•å¯¼è‡´ç°æœ‰æ•°å­¦æ¨ç†è¯„ä¼°åŸºå‡†æµ‹è¯•è¶‹äºé¥±å’Œï¼Œè¿™å‡¸æ˜¾äº†å¯¹æ›´å…·æŒ‘æˆ˜æ€§å’Œä¸¥æ ¼çš„è¯„ä¼°æ¡†æ¶çš„è¿«åˆ‡éœ€æ±‚ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†OlymMATHï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¥¥æ—åŒ¹å…‹çº§åˆ«çš„æ•°å­¦åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ä¸¥æ ¼æµ‹è¯•å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚OlymMATHåŒ…å«200ä¸ªç²¾å¿ƒæŒ‘é€‰çš„é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜éƒ½ç»è¿‡æ‰‹åŠ¨éªŒè¯ï¼Œå¹¶æœ‰è‹±æ–‡å’Œä¸­æ–‡çš„å¹³è¡Œç‰ˆæœ¬ã€‚è¿™äº›é—®é¢˜è¢«ç³»ç»Ÿåœ°åˆ†ä¸ºä¸¤ä¸ªéš¾åº¦ç­‰çº§ï¼šä¸€æ˜¯AIMEçº§åˆ«çš„é—®é¢˜ï¼ˆç®€å•ï¼‰ï¼Œä¸ºæ•°å­¦æ¨ç†è¯„ä¼°å»ºç«‹åŸºå‡†çº¿ï¼›äºŒæ˜¯æ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼ˆå›°éš¾ï¼‰ï¼Œæ—¨åœ¨æŒ‘æˆ˜å½“å‰æœ€å‰æ²¿æ¨¡å‹çš„æé™ã€‚åœ¨æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¿™äº›é—®é¢˜æ¶‰åŠå››ä¸ªæ ¸å¿ƒæ•°å­¦é¢†åŸŸï¼Œæ¯ä¸ªé¢†åŸŸéƒ½åŒ…æ‹¬å¯éªŒè¯çš„æ•°å€¼è§£å†³æ–¹æ¡ˆï¼Œä»¥å®ç°å®¢è§‚ã€åŸºäºè§„åˆ™çš„è¯„ä¼°ã€‚ç»éªŒç»“æœçªæ˜¾äº†OlymMATHçš„å·¨å¤§æŒ‘æˆ˜ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹åŒ…æ‹¬DeepSeek-R1å’ŒOpenAIçš„o3-miniåœ¨å›°éš¾å­é›†ä¸Šæ˜¾ç¤ºå‡ºæ˜æ˜¾çš„å‡†ç¡®æ€§é™åˆ¶ã€‚æ­¤å¤–ï¼Œè¯¥åŸºå‡†æµ‹è¯•è¿˜å¯ä»¥å…¨é¢è¯„ä¼°åŒè¯­æ•°å­¦æ¨ç†èƒ½åŠ›â€”â€”è¿™æ˜¯ä¸»æµæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å°šæœªæ¶‰åŠçš„å…³é”®ç»´åº¦ã€‚æˆ‘ä»¬åœ¨STILLé¡¹ç›®çš„ç½‘å€ä¸Šå‘å¸ƒäº†OlymMATHåŸºå‡†æµ‹è¯•ï¼š<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Slow_Thinking_with_LLMs">https://github.com/RUCAIBox/Slow_Thinking_with_LLMs</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21380v1">PDF</a> Technical Report on Slow Thinking with LLMs: Evaluation Benchmark</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†è¿‘æœŸå¤§å‹æ¨ç†æ¨¡å‹å¿«é€Ÿå‘å±•å¯¼è‡´ç°æœ‰è¯„ä¼°åŸºå‡†æµ‹è¯•é¥±å’Œçš„é—®é¢˜ï¼Œå¹¶æŒ‡å‡ºéœ€è¦æ›´å…·æŒ‘æˆ˜æ€§å’Œä¸¥æ ¼æ€§çš„è¯„ä¼°æ¡†æ¶ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ–°å‹çš„å¥¥èµ›çº§æ•°å­¦åŸºå‡†æµ‹è¯•â€”â€”OlymMATHï¼Œæ—¨åœ¨ä¸¥æ ¼æµ‹è¯•å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤æ‚æ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…å«200ä¸ªç²¾å¿ƒç­–åˆ’çš„é—®é¢˜ï¼Œåˆ†ä¸ºä¸¤ä¸ªéš¾åº¦ç­‰çº§ï¼Œæ¶µç›–å››ä¸ªæ ¸å¿ƒæ•°å­¦é¢†åŸŸï¼Œæ¯ä¸ªé—®é¢˜éƒ½æœ‰å¯éªŒè¯çš„æ•°å€¼è§£ï¼Œä»¥ä¾¿è¿›è¡Œå®¢è§‚ã€åŸºäºè§„åˆ™çš„è¯„ä»·ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒOlymMATHå…·æœ‰æ˜¾è‘—æŒ‘æˆ˜æ€§ï¼Œç›®å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨è¯¥åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡è¾ƒä½ã€‚æ­¤å¤–ï¼Œè¯¥åŸºå‡†æµ‹è¯•è¿˜ä¾¿äºå¯¹ä¸»æµæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•å°šæœªæ¶µç›–çš„é‡è¦åŒè¯­è¯„ä¼°ç»´åº¦è¿›è¡Œæ•°å­¦æ¨ç†èƒ½åŠ›è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹çš„å¿«é€Ÿå‘å±•å¯¼è‡´ç°æœ‰æ•°å­¦æ¨ç†è¯„ä¼°åŸºå‡†æµ‹è¯•çš„é¥±å’Œã€‚</li>
<li>æ¨å‡ºæ–°å‹å¥¥èµ›çº§æ•°å­¦åŸºå‡†æµ‹è¯•OlymMATHï¼Œç”¨ä»¥ä¸¥æ ¼æµ‹è¯•å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>OlymMATHåŒ…å«200ä¸ªæ‰‹å·¥éªŒè¯çš„é—®é¢˜ï¼Œåˆ†ä¸ºä¸¤ä¸ªéš¾åº¦ç­‰çº§ã€‚</li>
<li>æ¶µç›–å››ä¸ªæ ¸å¿ƒæ•°å­¦é¢†åŸŸï¼Œæ¯ä¸ªé—®é¢˜éƒ½æœ‰å¯éªŒè¯çš„æ•°å€¼è§£ã€‚</li>
<li>OlymMATHå…·æœ‰æ˜¾è‘—æŒ‘æˆ˜æ€§ï¼Œç›®å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨è¯¥åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡è¾ƒä½ã€‚</li>
<li>OlymMATHä¾¿äºè¿›è¡ŒåŒè¯­è¯„ä¼°ï¼Œè¿™æ˜¯ä¸»æµæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•å°šæœªå……åˆ†æ¶µç›–çš„é‡è¦ç»´åº¦ã€‚</li>
<li>OlymMATHåŸºå‡†æµ‹è¯•å·²åœ¨STILLé¡¹ç›®ä¸Šå‘è¡¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Slow_Thinking_with_LLMs%E3%80%82">https://github.com/RUCAIBox/Slow_Thinking_with_LLMsã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21380">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1f96ac6c2c467a3ca1aca9fb6f025682.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a7bab3a0578da0f50abe87649da8d1b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a9d79c252915de6067b957be1c420c14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19ec01916c228b7e6b891b4bb32464f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dac4b5f0569c455c0226457a545f3b7b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22b673cb6864734d42e7c916d32b018a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Reinforced-Model-Merging"><a href="#Reinforced-Model-Merging" class="headerlink" title="Reinforced Model Merging"></a>Reinforced Model Merging</h2><p><strong>Authors:Jiaqi Han, Jingwen Ye, Shunyu Liu, Haofei Zhang, Jie Song, Zunlei Feng, Mingli Song</strong></p>
<p>The success of large language models has garnered widespread attention for model merging techniques, especially training-free methods which combine model capabilities within the parameter space. However, two challenges remain: (1) uniform treatment of all parameters leads to performance degradation; (2) search-based algorithms are often inefficient. In this paper, we present an innovative framework termed Reinforced Model Merging (RMM), which encompasses an environment and agent tailored for merging tasks. These components interact to execute layer-wise merging actions, aiming to search the optimal merging architecture. Notably, RMM operates without any gradient computations on the original models, rendering it feasible for edge devices. Furthermore, by utilizing data subsets during the evaluation process, we addressed the bottleneck in the reward feedback phase, thereby accelerating RMM by up to 100 times. Extensive experiments demonstrate that RMM achieves state-of-the-art performance across various vision and NLP datasets and effectively overcomes the limitations of the existing baseline methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/WuDiHJQ/Reinforced-Model-Merging">https://github.com/WuDiHJQ/Reinforced-Model-Merging</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„æˆåŠŸå¼•å‘äº†äººä»¬å¯¹æ¨¡å‹åˆå¹¶æŠ€æœ¯çš„å¹¿æ³›å…³æ³¨ï¼Œå°¤å…¶æ˜¯æ— éœ€è®­ç»ƒçš„å‚æ•°ç©ºé—´å†…æ¨¡å‹èƒ½åŠ›ç»„åˆæ–¹æ³•ã€‚ç„¶è€Œï¼Œä»å­˜åœ¨ä¸¤ä¸ªæŒ‘æˆ˜ï¼š(1)å¯¹æ‰€æœ‰å‚æ•°è¿›è¡Œç»Ÿä¸€å¤„ç†ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼›(2)åŸºäºæœç´¢çš„ç®—æ³•é€šå¸¸æ•ˆç‡ä¸é«˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºå¼ºåŒ–æ¨¡å‹åˆå¹¶(RMM)çš„åˆ›æ–°æ¡†æ¶ï¼Œå®ƒåŒ…å«ä¸€ä¸ªé’ˆå¯¹åˆå¹¶ä»»åŠ¡é‡èº«å®šåˆ¶çš„ç¯å¢ƒå’Œä»£ç†ã€‚è¿™äº›ç»„ä»¶ç›¸äº’ä½œç”¨æ‰§è¡Œé€å±‚åˆå¹¶æ“ä½œï¼Œæ—¨åœ¨æœç´¢æœ€ä½³çš„åˆå¹¶æ¶æ„ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒRMMåœ¨åŸå§‹æ¨¡å‹ä¸Šæ²¡æœ‰ä»»ä½•æ¢¯åº¦è®¡ç®—ï¼Œä½¿å…¶é€‚ç”¨äºè¾¹ç¼˜è®¾å¤‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ä½¿ç”¨æ•°æ®å­é›†ï¼Œæˆ‘ä»¬è§£å†³äº†å¥–åŠ±åé¦ˆé˜¶æ®µçš„ç“¶é¢ˆé—®é¢˜ï¼Œä»è€Œä½¿RMMçš„åŠ é€Ÿé€Ÿåº¦æé«˜é«˜è¾¾100å€ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRMMåœ¨ä¸åŒç±»å‹çš„è§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶æœ‰æ•ˆåœ°å…‹æœäº†ç°æœ‰åŸºå‡†æ–¹æ³•çš„å±€é™æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/WuDiHJQ/Reinforced-Model-Merging%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/WuDiHJQ/Reinforced-Model-Mergingä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21272v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºReinforced Model Mergingï¼ˆRMMï¼‰çš„åˆ›æ–°æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åˆå¹¶è¿‡ç¨‹ä¸­çš„ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼šå‚æ•°å¤„ç†ä¸å‡å¯¼è‡´æ€§èƒ½ä¸‹é™ä»¥åŠæœç´¢ç®—æ³•æ•ˆç‡ä½ä¸‹ã€‚RMMæ¡†æ¶é€šè¿‡å®šåˆ¶çš„ç¯å¢ƒå’Œä»£ç†æ‰§è¡Œå±‚çº§çš„åˆå¹¶åŠ¨ä½œï¼Œæ—¨åœ¨æœç´¢æœ€ä½³çš„åˆå¹¶æ¶æ„ã€‚å…¶ç‰¹ç‚¹æ˜¯æ— éœ€å¯¹åŸå§‹æ¨¡å‹è¿›è¡Œæ¢¯åº¦è®¡ç®—ï¼Œé€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šè¿è¡Œã€‚æ­¤å¤–ï¼Œé€šè¿‡è¯„ä¼°è¿‡ç¨‹ä¸­çš„æ•°æ®å­é›†ä½¿ç”¨ï¼Œè§£å†³äº†å¥–åŠ±åé¦ˆé˜¶æ®µçš„ç“¶é¢ˆé—®é¢˜ï¼Œä½¿RMMåŠ é€Ÿè¾¾ç™¾å€ã€‚å®éªŒè¡¨æ˜ï¼ŒRMMåœ¨å¤šç§è§†è§‰å’ŒNLPæ•°æ®é›†ä¸Šå®ç°æœ€ä½³æ€§èƒ½ï¼Œå¹¶æˆåŠŸå…‹æœç°æœ‰åŸºçº¿æ–¹æ³•çš„å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Reinforced Model Mergingï¼ˆRMMï¼‰æ¡†æ¶æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åˆå¹¶ä¸­çš„æŒ‘æˆ˜ï¼Œå¦‚æ€§èƒ½é™æ•ˆå’Œæœç´¢ç®—æ³•æ•ˆç‡ä½ä¸‹ã€‚</li>
<li>RMMé€šè¿‡å®šåˆ¶çš„ç¯å¢ƒå’Œä»£ç†è¿›è¡Œå±‚çº§çš„æ¨¡å‹åˆå¹¶ï¼Œæ—¨åœ¨æ‰¾åˆ°æœ€ä½³çš„åˆå¹¶æ¶æ„ã€‚</li>
<li>RMMåœ¨åˆå¹¶è¿‡ç¨‹ä¸­ä¸éœ€è¦å¯¹åŸå§‹æ¨¡å‹è¿›è¡Œæ¢¯åº¦è®¡ç®—ï¼Œé€‚åˆåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šè¿è¡Œã€‚</li>
<li>ä½¿ç”¨æ•°æ®å­é›†è¿›è¡Œè¯„ä¼°ï¼Œè§£å†³äº†å¥–åŠ±åé¦ˆé˜¶æ®µçš„ç“¶é¢ˆï¼Œæå¤§åŠ é€Ÿäº†RMMçš„è¿è¡Œé€Ÿåº¦ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒRMMåœ¨å¤šç§è§†è§‰å’ŒNLPæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</li>
<li>RMMæˆåŠŸå…‹æœäº†ç°æœ‰åŸºçº¿æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21272">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-897fa46c3a307ff566101f9cc16f214e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d67b8b05ed8c3a8505ee48b17a9f1a97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-670f7defb4641b8b4a1b85e5f883096c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-749b32045852e87d16e968641a85eba6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d4020fd202d1674da42cb3789c92c23.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d73cc1d08c5096782412400ff5a6e929.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Learn-by-Reasoning-Analogical-Weight-Generation-for-Few-Shot-Class-Incremental-Learning"><a href="#Learn-by-Reasoning-Analogical-Weight-Generation-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Learn by Reasoning: Analogical Weight Generation for Few-Shot   Class-Incremental Learning"></a>Learn by Reasoning: Analogical Weight Generation for Few-Shot   Class-Incremental Learning</h2><p><strong>Authors:Jizhou Han, Chenhao Ding, Yuhang He, Songlin Dong, Qiang Wang, Xinyuan Gao, Yihong Gong</strong></p>
<p>Few-shot class-incremental Learning (FSCIL) enables models to learn new classes from limited data while retaining performance on previously learned classes. Traditional FSCIL methods often require fine-tuning parameters with limited new class data and suffer from a separation between learning new classes and utilizing old knowledge. Inspired by the analogical learning mechanisms of the human brain, we propose a novel analogical generative method. Our approach includes the Brain-Inspired Analogical Generator (BiAG), which derives new class weights from existing classes without parameter fine-tuning during incremental stages. BiAG consists of three components: Weight Self-Attention Module (WSA), Weight &amp; Prototype Analogical Attention Module (WPAA), and Semantic Conversion Module (SCM). SCM uses Neural Collapse theory for semantic conversion, WSA supplements new class weights, and WPAA computes analogies to generate new class weights. Experiments on miniImageNet, CUB-200, and CIFAR-100 datasets demonstrate that our method achieves higher final and average accuracy compared to SOTA methods. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰é™çš„æ ·æœ¬ä¸­å­¦ä¹ æ–°ç±»åˆ«ï¼ŒåŒæ—¶ä¿ç•™å…ˆå‰å­¦ä¹ ç±»åˆ«çš„æ€§èƒ½ã€‚ä¼ ç»Ÿçš„FSCILæ–¹æ³•é€šå¸¸éœ€è¦åˆ©ç”¨æœ‰é™çš„æ ·æœ¬æ•°æ®å¯¹å‚æ•°è¿›è¡Œå¾®è°ƒï¼Œå¹¶ä¸”å­˜åœ¨å­¦ä¹ æ–°ç±»åˆ«å’Œåˆ©ç”¨æ—§çŸ¥è¯†ä¹‹é—´çš„åˆ†ç¦»é—®é¢˜ã€‚å—äººç±»å¤§è„‘ç±»æ¯”å­¦ä¹ æœºåˆ¶çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç±»æ¯”ç”Ÿæˆæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬åŸºäºå¤§è„‘å¯å‘çš„ç±»æ¯”ç”Ÿæˆå™¨ï¼ˆBiAGï¼‰ï¼Œèƒ½å¤Ÿåœ¨å¢é‡é˜¶æ®µä»ç°æœ‰ç±»åˆ«ä¸­æ¨å¯¼å‡ºæ–°ç±»åˆ«çš„æƒé‡ï¼Œæ— éœ€å¯¹å‚æ•°è¿›è¡Œå¾®è°ƒã€‚BiAGåŒ…å«ä¸‰ä¸ªç»„ä»¶ï¼šæƒé‡è‡ªå…³æ³¨æ¨¡å—ï¼ˆWSAï¼‰ã€æƒé‡ä¸åŸå‹ç±»æ¯”å…³æ³¨æ¨¡å—ï¼ˆWPAAï¼‰å’Œè¯­ä¹‰è½¬æ¢æ¨¡å—ï¼ˆSCMï¼‰ã€‚SCMåˆ©ç”¨ç¥ç»å´©æºƒç†è®ºè¿›è¡Œè¯­ä¹‰è½¬æ¢ï¼ŒWSAè¡¥å……æ–°ç±»åˆ«æƒé‡ï¼ŒWPAAè®¡ç®—ç±»æ¯”ä»¥ç”Ÿæˆæ–°ç±»åˆ«æƒé‡ã€‚åœ¨miniImageNetã€CUB-200å’ŒCIFAR-100æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºæœ€æ–°æ–¹æ³•å–å¾—äº†æ›´é«˜çš„æœ€ç»ˆå’Œå¹³å‡å‡†ç¡®ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21258v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å°‘é‡ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰é™çš„æ•°æ®ä¸­å­¦ä¹ æ–°ç±»åˆ«ï¼ŒåŒæ—¶ä¿ç•™å¯¹å…ˆå‰å­¦ä¹ ç±»åˆ«çš„æ€§èƒ½ã€‚å—äººç±»å¤§è„‘ç±»æ¯”å­¦ä¹ æœºåˆ¶çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç±»æ¯”ç”Ÿæˆæ–¹æ³•ï¼ŒåŒ…æ‹¬Brain-Inspired Analogical Generatorï¼ˆBiAGï¼‰ã€‚BiAGé€šè¿‡æƒé‡è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼ˆWSAï¼‰ã€æƒé‡ä¸åŸå‹ç±»æ¯”æ³¨æ„åŠ›æ¨¡å—ï¼ˆWPAAï¼‰å’Œè¯­ä¹‰è½¬æ¢æ¨¡å—ï¼ˆSCMï¼‰ä¸‰ä¸ªç»„ä»¶ï¼Œä»ç°æœ‰ç±»åˆ«ä¸­æ¨å¯¼å‡ºæ–°ç±»åˆ«çš„æƒé‡ï¼Œæ— éœ€åœ¨å¢é‡é˜¶æ®µå¾®è°ƒå‚æ•°ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨miniImageNetã€CUB-200å’ŒCIFAR-100æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæœ€ç»ˆçš„å¹³å‡å‡†ç¡®ç‡æ›´é«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FSCILå…è®¸æ¨¡å‹åœ¨æœ‰é™æ•°æ®ä¸­å­¦ä¹ æ–°ç±»åˆ«ï¼ŒåŒæ—¶ä¿æŒå¯¹æ—§ç±»åˆ«çš„æ€§èƒ½ã€‚</li>
<li>æå‡ºçš„Brain-Inspired Analogical Generatorï¼ˆBiAGï¼‰æ–¹æ³•èƒ½å¤Ÿä»ç°æœ‰ç±»åˆ«ä¸­æ¨å¯¼å‡ºæ–°ç±»åˆ«çš„æƒé‡ã€‚</li>
<li>BiAGåŒ…å«ä¸‰ä¸ªç»„ä»¶ï¼šWeight Self-Attention Moduleï¼ˆWSAï¼‰ã€Weight &amp; Prototype Analogical Attention Moduleï¼ˆWPAAï¼‰å’ŒSemantic Conversion Moduleï¼ˆSCMï¼‰ã€‚</li>
<li>WSAä¸ºæ–°è¯¾ç¨‹æä¾›æƒé‡è¡¥å……ï¼ŒWPAAè®¡ç®—ç±»æ¯”ä»¥ç”Ÿæˆæ–°è¯¾ç¨‹æƒé‡ã€‚</li>
<li>è¯­ä¹‰è½¬æ¢æ¨¡å—ä½¿ç”¨ç¥ç»å´©æºƒç†è®ºè¿›è¡Œè¯­ä¹‰è½¬æ¢ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-61ac50a4a5b820901dc637c13c2f09f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99488976e4f2a87bd1d46fc627d728c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f474caa54f3f71888c3be278d26c1b14.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="FakeReasoning-Towards-Generalizable-Forgery-Detection-and-Reasoning"><a href="#FakeReasoning-Towards-Generalizable-Forgery-Detection-and-Reasoning" class="headerlink" title="FakeReasoning: Towards Generalizable Forgery Detection and Reasoning"></a>FakeReasoning: Towards Generalizable Forgery Detection and Reasoning</h2><p><strong>Authors:Yueying Gao, Dongliang Chang, Bingyao Yu, Haotian Qin, Lei Chen, Kongming Liang, Zhanyu Ma</strong></p>
<p>Accurate and interpretable detection of AI-generated images is essential for mitigating risks associated with AI misuse. However, the substantial domain gap among generative models makes it challenging to develop a generalizable forgery detection model. Moreover, since every pixel in an AI-generated image is synthesized, traditional saliency-based forgery explanation methods are not well suited for this task. To address these challenges, we propose modeling AI-generated image detection and explanation as a Forgery Detection and Reasoning task (FDR-Task), leveraging vision-language models (VLMs) to provide accurate detection through structured and reliable reasoning over forgery attributes. To facilitate this task, we introduce the Multi-Modal Forgery Reasoning dataset (MMFR-Dataset), a large-scale dataset containing 100K images across 10 generative models, with 10 types of forgery reasoning annotations, enabling comprehensive evaluation of FDR-Task. Additionally, we propose FakeReasoning, a forgery detection and reasoning framework with two key components. First, Forgery-Aligned Contrastive Learning enhances VLMsâ€™ understanding of forgery-related semantics through both cross-modal and intra-modal contrastive learning between images and forgery attribute reasoning. Second, a Classification Probability Mapper bridges the optimization gap between forgery detection and language modeling by mapping the output logits of VLMs to calibrated binary classification probabilities. Experiments across multiple generative models demonstrate that FakeReasoning not only achieves robust generalization but also outperforms state-of-the-art methods on both detection and reasoning tasks. </p>
<blockquote>
<p>å‡†ç¡®ä¸”å¯è§£é‡Šçš„AIç”Ÿæˆå›¾åƒæ£€æµ‹å¯¹äºç¼“è§£ä¸AIè¯¯ç”¨ç›¸å…³çš„é£é™©è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”Ÿæˆæ¨¡å‹ä¹‹é—´çš„å·¨å¤§é¢†åŸŸå·®è·ä½¿å¾—å¼€å‘å¯é€šç”¨çš„ä¼ªé€ æ£€æµ‹æ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œç”±äºAIç”Ÿæˆçš„å›¾åƒä¸­çš„æ¯ä¸ªåƒç´ éƒ½æ˜¯åˆæˆçš„ï¼Œå› æ­¤ä¼ ç»Ÿçš„åŸºäºæ˜¾è‘—æ€§çš„ä¼ªé€ è§£é‡Šæ–¹æ³•å¹¶ä¸é€‚åˆæ­¤ä»»åŠ¡ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å°†AIç”Ÿæˆå›¾åƒçš„æ£€æµ‹å’Œè§£é‡Šå»ºæ¨¡ä¸ºä¼ªé€ æ£€æµ‹ä¸æ¨ç†ä»»åŠ¡ï¼ˆFDR-Taskï¼‰ï¼Œåˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡ç»“æ„åŒ–ä¸”å¯é çš„æ¨ç†æä¾›å‡†ç¡®çš„æ£€æµ‹ã€‚ä¸ºäº†ä¿ƒè¿›æ­¤ä»»åŠ¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šæ¨¡æ€ä¼ªé€ æ¨ç†æ•°æ®é›†ï¼ˆMMFR-Datasetï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«è·¨è¶Šåä¸ªç”Ÿæˆæ¨¡å‹çš„åä¸‡å¼ å›¾åƒçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¸¦æœ‰åç§ä¼ªé€ æ¨ç†æ³¨é‡Šç±»å‹ï¼Œå¯å…¨é¢è¯„ä¼°FDRä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†FakeReasoningè¿™ä¸€ä¼ªé€ æ£€æµ‹ä¸æ¨ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚é¦–å…ˆï¼Œä¼ªé€ å¯¹é½å¯¹æ¯”å­¦ä¹ é€šè¿‡å›¾åƒå’Œä¼ªé€ å±æ€§æ¨ç†ä¹‹é—´çš„è·¨æ¨¡æ€å’Œæ¨¡æ€å†…å¯¹æ¯”å­¦ä¹ å¢å¼ºVLMså¯¹ä¼ªé€ ç›¸å…³è¯­ä¹‰çš„ç†è§£ã€‚å…¶æ¬¡ï¼Œåˆ†ç±»æ¦‚ç‡æ˜ å°„å™¨é€šè¿‡æ˜ å°„VLMsçš„è¾“å‡ºé€»è¾‘å€¼åˆ°æ ¡å‡†çš„äºŒå…ƒåˆ†ç±»æ¦‚ç‡æ¥ç¼©å°ä¼ªé€ æ£€æµ‹å’Œè¯­è¨€å»ºæ¨¡ä¹‹é—´çš„ä¼˜åŒ–å·®è·ã€‚åœ¨å¤šä¸ªç”Ÿæˆæ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFakeReasoningä¸ä»…å®ç°äº†ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œä¸”åœ¨æ£€æµ‹å’Œæ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21210v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹äººå·¥æ™ºèƒ½ç”Ÿæˆå›¾åƒçš„æ£€æµ‹ä¸è§£é‡Šéš¾é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå…¨æ–°çš„ä»»åŠ¡â€”â€”ä¼ªé€ æ£€æµ‹ä¸æ¨ç†ä»»åŠ¡ï¼ˆFDR-Taskï¼‰ã€‚ä¸ºäº†åº”å¯¹ä¸åŒç”Ÿæˆæ¨¡å‹é—´çš„é¢†åŸŸå·®è·ï¼Œåˆ©ç”¨è·¨æ¨¡æ€å’Œæ¨¡æ€å†…çš„å¯¹æ¯”å­¦ä¹ å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¯¹ä¼ªé€ ç›¸å…³è¯­ä¹‰çš„ç†è§£ã€‚åŒæ—¶ï¼Œå¼•å…¥å¤§å‹å¤šæ¨¡æ€ä¼ªé€ æ¨ç†æ•°æ®é›†MMFR-Datasetï¼ŒåŒ…å«10ä¸‡ä¸ªè·¨è¶Š10ç§ç”Ÿæˆæ¨¡å‹çš„å›¾åƒå’Œ10ç§ä¼ªé€ æ¨ç†æ³¨é‡Šã€‚å®éªŒè¯æ˜ï¼ŒFakeReasoningæ¡†æ¶åœ¨æ£€æµ‹ä¸æ¨ç†ä»»åŠ¡ä¸Šå®ç°äº†ç¨³å¥çš„æ³›åŒ–æ€§èƒ½ï¼Œå¹¶ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>AIç”Ÿæˆå›¾åƒçš„æ£€æµ‹ä¸è§£é‡Šå¯¹ç¼“è§£AIè¯¯ç”¨é£é™©è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥åº”å¯¹ä¸åŒç”Ÿæˆæ¨¡å‹é—´çš„é¢†åŸŸå·®è·ï¼Œå› æ­¤éœ€è¦ä¸€ç§æ›´å…·æ³›åŒ–èƒ½åŠ›çš„ä¼ªé€ æ£€æµ‹æ¨¡å‹ã€‚</li>
<li>ä¼ ç»ŸåŸºäºæ˜¾è‘—æ€§çš„ä¼ªé€ è§£é‡Šæ–¹æ³•ä¸é€‚ç”¨äºAIç”Ÿæˆå›¾åƒçš„æ£€æµ‹ä¸è§£é‡Šä»»åŠ¡ã€‚</li>
<li>æå‡ºFDR-Taskï¼ˆä¼ªé€ æ£€æµ‹ä¸æ¨ç†ä»»åŠ¡ï¼‰ï¼Œå°†AIç”Ÿæˆå›¾åƒçš„æ£€æµ‹ä¸è§£é‡Šå»ºæ¨¡ä¸ºä¸€é¡¹æ–°çš„ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥å¤§å‹å¤šæ¨¡æ€ä¼ªé€ æ¨ç†æ•°æ®é›†MMFR-Datasetï¼ŒåŒ…å«å¤šç§ç”Ÿæˆæ¨¡å‹çš„å›¾åƒå’Œå¤šç§ä¼ªé€ æ¨ç†æ³¨é‡Šã€‚</li>
<li>FakeReasoningæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šForgery-Aligned Contrastive Learningå’ŒClassification Probability Mapperã€‚å‰è€…é€šè¿‡è·¨æ¨¡æ€å’Œæ¨¡æ€å†…çš„å¯¹æ¯”å­¦ä¹ å¢å¼ºVLMså¯¹ä¼ªé€ è¯­ä¹‰çš„ç†è§£ï¼Œåè€…ç¼©å°äº†ä¼˜åŒ–å·®è·ï¼Œæ˜ å°„VLMsçš„è¾“å‡ºé€»è¾‘åˆ°æ ¡å‡†çš„äºŒå…ƒåˆ†ç±»æ¦‚ç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21210">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e683cceb496cd0c6a465fd3195ed477.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a518a3bb769d0a1330fbd22823f18eb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d31c21a90a5eae4cd7a5f3f7fa6f84c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71ccf830b2ea919007ea359a406182bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b13c3977e260adc8483894f9ae9dd91.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Operating-Room-Workflow-Analysis-via-Reasoning-Segmentation-over-Digital-Twins"><a href="#Operating-Room-Workflow-Analysis-via-Reasoning-Segmentation-over-Digital-Twins" class="headerlink" title="Operating Room Workflow Analysis via Reasoning Segmentation over Digital   Twins"></a>Operating Room Workflow Analysis via Reasoning Segmentation over Digital   Twins</h2><p><strong>Authors:Yiqing Shen, Chenjia Li, Bohan Liu, Cheng-Yi Li, Tito Porras, Mathias Unberath</strong></p>
<p>Analyzing operating room (OR) workflows to derive quantitative insights into OR efficiency is important for hospitals to maximize patient care and financial sustainability. Prior work on OR-level workflow analysis has relied on end-to-end deep neural networks. While these approaches work well in constrained settings, they are limited to the conditions specified at development time and do not offer the flexibility necessary to accommodate the OR workflow analysis needs of various OR scenarios (e.g., large academic center vs. rural provider) without data collection, annotation, and retraining. Reasoning segmentation (RS) based on foundation models offers this flexibility by enabling automated analysis of OR workflows from OR video feeds given only an implicit text query related to the objects of interest. Due to the reliance on large language model (LLM) fine-tuning, current RS approaches struggle with reasoning about semantic&#x2F;spatial relationships and show limited generalization to OR video due to variations in visual characteristics and domain-specific terminology. To address these limitations, we first propose a novel digital twin (DT) representation that preserves both semantic and spatial relationships between the various OR components. Then, building on this foundation, we propose ORDiRS (Operating Room Digital twin representation for Reasoning Segmentation), an LLM-tuning-free RS framework that reformulates RS into a â€œreason-retrieval-synthesizeâ€ paradigm. Finally, we present ORDiRS-Agent, an LLM-based agent that decomposes OR workflow analysis queries into manageable RS sub-queries and generates responses by combining detailed textual explanations with supporting visual evidence from RS. Experimental results on both an in-house and a public OR dataset demonstrate that our ORDiRS achieves a cIoU improvement of 6.12%-9.74% compared to the existing state-of-the-arts. </p>
<blockquote>
<p>åˆ†ææ‰‹æœ¯å®¤ï¼ˆORï¼‰çš„å·¥ä½œæµç¨‹ä»¥å¾—å‡ºå…³äºæ‰‹æœ¯å®¤æ•ˆç‡çš„å®šé‡è§è§£ï¼Œå¯¹äºåŒ»é™¢æœ€å¤§é™åº¦åœ°æ”¹å–„æ‚£è€…æŠ¤ç†å’Œè´¢åŠ¡å¯æŒç»­æ€§è‡³å…³é‡è¦ã€‚å…ˆå‰å…³äºæ‰‹æœ¯å®¤çº§åˆ«çš„å·¥ä½œæµåˆ†æä¾èµ–äºç«¯åˆ°ç«¯çš„æ·±åº¦ç¥ç»ç½‘ç»œã€‚è™½ç„¶è¿™äº›æ–¹æ³•åœ¨å—æ§ç¯å¢ƒä¸­æ•ˆæœå¾ˆå¥½ï¼Œä½†å®ƒä»¬ä»…é™äºå¼€å‘æ—¶æŒ‡å®šçš„æ¡ä»¶ï¼Œå¹¶ä¸æä¾›é€‚åº”å„ç§æ‰‹æœ¯å®¤æƒ…æ™¯ï¼ˆä¾‹å¦‚å¤§å‹å­¦æœ¯ä¸­å¿ƒä¸ä¹¡æ‘åŒ»ç–—æœºæ„ï¼‰æ‰€éœ€çš„çµæ´»æ€§ï¼Œè€Œæ— éœ€è¿›è¡Œæ•°æ®é‡‡é›†ã€æ ‡æ³¨å’Œé‡æ–°è®­ç»ƒã€‚åŸºäºåŸºç¡€æ¨¡å‹çš„æ¨ç†åˆ†å‰²ï¼ˆRSï¼‰é€šè¿‡ä»…ä¸æ„Ÿå…´è¶£çš„å¯¹è±¡ç›¸å…³çš„éšå«æ–‡æœ¬æŸ¥è¯¢è‡ªåŠ¨åˆ†ææ‰‹æœ¯å®¤å·¥ä½œæµï¼Œä»è€Œæä¾›äº†è¿™ç§çµæ´»æ€§ã€‚ç”±äºä¾èµ–äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¾®è°ƒï¼Œå½“å‰çš„RSæ–¹æ³•åœ¨å¤„ç†è¯­ä¹‰&#x2F;ç©ºé—´å…³ç³»æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå¹¶ä¸”åœ¨é¢å¯¹æ‰‹æœ¯å®¤è§†é¢‘çš„è§†è§‰ç‰¹å¾å’Œé¢†åŸŸç‰¹å®šæœ¯è¯­çš„å˜ä½“æ—¶ï¼Œå…¶æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ•°å­—å­ªç”Ÿï¼ˆDTï¼‰è¡¨ç¤ºæ³•ï¼Œè¯¥è¡¨ç¤ºæ³•å¯ä»¥ä¿ç•™æ‰‹æœ¯å®¤å„ç»„ä»¶ä¹‹é—´çš„è¯­ä¹‰å’Œç©ºé—´å…³ç³»ã€‚ç„¶åï¼Œåœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†ORDiRSï¼ˆæ‰‹æœ¯å®¤æ•°å­—å­ªç”Ÿæ¨ç†åˆ†å‰²ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€LLMè°ƒæ•´çš„RSæ¡†æ¶ï¼Œå®ƒå°†RSé‡æ–°åˆ¶å®šä¸ºâ€œæ¨ç†-æ£€ç´¢-åˆæˆâ€çš„æ¨¡å¼ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†åŸºäºLLMçš„ORDiRS-Agentï¼Œå®ƒå°†æ‰‹æœ¯å®¤å·¥ä½œæµç¨‹åˆ†ææŸ¥è¯¢åˆ†è§£ä¸ºå¯ç®¡ç†çš„RSå­æŸ¥è¯¢ï¼Œå¹¶é€šè¿‡ç»“åˆè¯¦ç»†çš„æ–‡æœ¬è§£é‡Šå’Œæ¥è‡ªRSçš„æ”¯æŒè§†è§‰è¯æ®æ¥ç”Ÿæˆå“åº”ã€‚åœ¨å†…éƒ¨å’Œå…¬å…±æ‰‹æœ¯å®¤æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ORDiRSä¸ç°æœ‰æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼ŒcIoUæé«˜äº†6.12%-9.74%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21054v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰‹æœ¯å®¤ï¼ˆORï¼‰å·¥ä½œæµçš„åˆ†æå¯¹äºåŒ»é™¢æå‡ç—…æ‚£ç…§æŠ¤åŠè´¢åŠ¡å¯æŒç»­æ€§è‡³å…³é‡è¦ã€‚ä¹‹å‰çš„ORçº§åˆ«å·¥ä½œæµåˆ†æå¤§å¤šä¾èµ–æ·±åº¦ç¥ç»ç½‘ç»œã€‚è™½ç„¶è¿™äº›æ–¹æ³•åœ¨å—é™ç¯å¢ƒä¸‹æ•ˆæœè‰¯å¥½ï¼Œä½†å®ƒä»¬éš¾ä»¥é€‚åº”ä¸åŒORåœºæ™¯çš„éœ€æ±‚ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ•°å­—åŒèƒèƒï¼ˆDTï¼‰çš„æ‰‹æœ¯å®¤å·¥ä½œæµåˆ†ææ¡†æ¶ï¼Œæ— éœ€è°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè¯¥æ¡†æ¶å°†æ¨ç†åˆ†å‰²é‡æ–°æ„å»ºä¸ºâ€œæ¨ç†-æ£€ç´¢-åˆæˆâ€çš„æ¨¡å¼ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥å¼€å‘äº†åä¸ºORDiRS-Agentçš„LLMåŸºç¡€ä»£ç†ï¼Œè¯¥ä»£ç†å¯å°†å¤æ‚çš„ORå·¥ä½œæµåˆ†ææŸ¥è¯¢åˆ†è§£ä¸ºå¯ç®¡ç†çš„å­æŸ¥è¯¢ï¼Œå¹¶é€šè¿‡ç»“åˆè¯¦ç»†çš„æ–‡æœ¬è§£é‡Šå’Œæ¥è‡ªæ¨ç†æ£€ç´¢çš„è§†è§‰è¯æ®æ¥ç”Ÿæˆå“åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„ORDiRSåœ¨å®Œå…¨é›†æˆçš„äº¤å¹¶æ¯”ï¼ˆcIoUï¼‰ä¸Šæé«˜äº†6.12%-9.74%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆ†ææ‰‹æœ¯å®¤å·¥ä½œæµå¯¹äºåŒ»é™¢çš„è¿è¥æ•ˆç‡è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰çš„æ·±åº¦ç¥ç»ç½‘ç»œæ–¹æ³•è™½ç„¶æœ‰æ•ˆï¼Œä½†ç¼ºä¹é€‚åº”ä¸åŒæ‰‹æœ¯å®¤åœºæ™¯å˜åŒ–çš„çµæ´»æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„æ•°å­—åŒèƒèƒè¡¨ç¤ºæ³•ï¼Œèƒ½å¤Ÿä¿ç•™æ‰‹æœ¯å®¤ç»„ä»¶é—´çš„è¯­ä¹‰å’Œç©ºé—´å…³ç³»ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§æ— éœ€è°ƒæ•´å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†åˆ†å‰²æ¡†æ¶â€”â€”ORDiRSã€‚</li>
<li>ORDiRSæ¡†æ¶é‡æ–°å®šä¹‰äº†â€œæ¨ç†-æ£€ç´¢-åˆæˆâ€çš„æ¨¡å¼ä»¥æ”¹è¿›åˆ†æã€‚</li>
<li>å¼€å‘äº†ORDiRS-Agentï¼Œèƒ½æœ‰æ•ˆåˆ†è§£å¤æ‚æŸ¥è¯¢å¹¶ç”Ÿæˆè¯¦ç»†çš„è§£é‡Šå’Œè§†è§‰è¯æ®æ”¯æŒçš„å“åº”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21054">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-db4e399227654086755d1fc473fa9146.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b071141bbc0c208a75d08cd5d63c652.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aebf899b7e660e012e445f5d20625a57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20e2009e732e4f86ec29473453d22519.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Generating-Synthetic-Data-with-Formal-Privacy-Guarantees-State-of-the-Art-and-the-Road-Ahead"><a href="#Generating-Synthetic-Data-with-Formal-Privacy-Guarantees-State-of-the-Art-and-the-Road-Ahead" class="headerlink" title="Generating Synthetic Data with Formal Privacy Guarantees: State of the   Art and the Road Ahead"></a>Generating Synthetic Data with Formal Privacy Guarantees: State of the   Art and the Road Ahead</h2><p><strong>Authors:Viktor Schlegel, Anil A Bharath, Zilong Zhao, Kevin Yee</strong></p>
<p>Privacy-preserving synthetic data offers a promising solution to harness segregated data in high-stakes domains where information is compartmentalized for regulatory, privacy, or institutional reasons. This survey provides a comprehensive framework for understanding the landscape of privacy-preserving synthetic data, presenting the theoretical foundations of generative models and differential privacy followed by a review of state-of-the-art methods across tabular data, images, and text. Our synthesis of evaluation approaches highlights the fundamental trade-off between utility for down-stream tasks and privacy guarantees, while identifying critical research gaps: the lack of realistic benchmarks representing specialized domains and insufficient empirical evaluations required to contextualise formal guarantees.   Through empirical analysis of four leading methods on five real-world datasets from specialized domains, we demonstrate significant performance degradation under realistic privacy constraints ($\epsilon \leq 4$), revealing a substantial gap between results reported on general domain benchmarks and performance on domain-specific data. %Our findings highlight key challenges including unaccounted privacy leakage, insufficient empirical verification of formal guarantees, and a critical deficit of realistic benchmarks. These challenges underscore the need for robust evaluation frameworks, standardized benchmarks for specialized domains, and improved techniques to address the unique requirements of privacy-sensitive fields such that this technology can deliver on its considerable potential. </p>
<blockquote>
<p>éšç§ä¿æŠ¤åˆæˆæ•°æ®ä¸ºåœ¨ç›‘ç®¡ã€éšç§æˆ–åˆ¶åº¦åŸå› ä¸‹ä¿¡æ¯ç»†åˆ†çš„é«˜é£é™©é¢†åŸŸåˆ©ç”¨åˆ†æ•£æ•°æ®æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚è¿™ç¯‡ç»¼è¿°æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œç”¨äºäº†è§£éšç§ä¿æŠ¤åˆæˆæ•°æ®çš„æ™¯è§‚ï¼Œä»‹ç»äº†ç”Ÿæˆæ¨¡å‹å’Œå·®åˆ†éšç§çš„ç†è®ºåŸºç¡€ï¼Œå¹¶å›é¡¾äº†è¡¨æ ¼æ•°æ®ã€å›¾åƒå’Œæ–‡æœ¬çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚æˆ‘ä»¬å¯¹è¯„ä¼°æ–¹æ³•çš„ç»¼åˆçªå‡ºäº†ä¸‹æ¸¸ä»»åŠ¡çš„å®ç”¨æ€§ä¸éšç§ä¿è¯ä¹‹é—´çš„åŸºæœ¬æƒè¡¡ï¼ŒåŒæ—¶è¯†åˆ«äº†å…³é”®çš„ç ”ç©¶ç©ºç™½ï¼šç¼ºä¹ä»£è¡¨ä¸“ä¸šé¢†åŸŸå’Œç¼ºä¹å®è¯è¯„ä¼°çš„ç°å®åŸºå‡†æµ‹è¯•ï¼Œæ— æ³•å°†æ­£å¼ä¿è¯ç½®äºç‰¹å®šæƒ…å¢ƒä¸­ã€‚é€šè¿‡å¯¹å››ç§é¢†å…ˆæ–¹æ³•åœ¨äº”ä¸ªä¸“ä¸šé¢†åŸŸçœŸå®æ•°æ®é›†ä¸Šçš„å®è¯åˆ†æï¼Œæˆ‘ä»¬åœ¨ç°å®çš„éšç§çº¦æŸï¼ˆÎµâ‰¤4ï¼‰ä¸‹è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œè¿™æ­ç¤ºäº†ä¸ä¸€èˆ¬åŸºå‡†æµ‹è¯•ä¸Šçš„æŠ¥å‘Šç»“æœç›¸æ¯”åœ¨ç‰¹å®šé¢†åŸŸæ•°æ®ä¸Šæ€§èƒ½ä¸Šå­˜åœ¨æ˜¾è‘—å·®è·ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†é‡è¦æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æœªè€ƒè™‘çš„éšç§æ³„éœ²ã€æ­£å¼ä¿è¯çš„å®è¯éªŒè¯ä¸è¶³ä»¥åŠç¼ºä¹çœŸå®åŸºå‡†æµ‹è¯•çš„å…³é”®ç¼ºé™·ã€‚è¿™äº›æŒ‘æˆ˜å¼ºè°ƒäº†å»ºç«‹ç¨³å¥çš„è¯„ä¼°æ¡†æ¶ã€é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„æ ‡å‡†åŒ–åŸºå‡†å’Œæ”¹è¿›æŠ€æœ¯çš„å¿…è¦æ€§ï¼Œä»¥ä¾¿å®ç°éšç§æ•æ„Ÿé¢†åŸŸçš„ç‹¬ç‰¹éœ€æ±‚ï¼Œä»è€Œä½¿è¿™é¡¹æŠ€æœ¯å‘æŒ¥å…¶å·¨å¤§çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20846v1">PDF</a> 23 pages + references + Appendix. Preprint</p>
<p><strong>Summary</strong><br>éšç§ä¿æŠ¤åˆæˆæ•°æ®åœ¨è§£å†³é«˜é£é™©é¢†åŸŸä¸­çš„åˆ†ç«‹æ•°æ®é—®é¢˜æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿¡æ¯å› ç›‘ç®¡ã€éšç§æˆ–æœºæ„åŸå› è€Œè¢«éš”ç¦»çš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡æä¾›äº†ç†è§£éšç§ä¿æŠ¤åˆæˆæ•°æ®æ™¯è§‚çš„ç»¼åˆæ¡†æ¶ï¼Œä»‹ç»äº†ç”Ÿæˆæ¨¡å‹å’Œå·®åˆ†éšç§çš„ç†è®ºåŸºç¡€ï¼Œå¹¶è¯„è¿°äº†è·¨è¡¨æ ¼æ•°æ®ã€å›¾åƒå’Œæ–‡æœ¬çš„æœ€æ–°æ–¹æ³•ã€‚æœ¬æ–‡çš„è¯„ä»·æ–¹æ³•åˆæˆæ­ç¤ºäº†ä¸‹æ¸¸ä»»åŠ¡çš„å®ç”¨æ€§ä¸éšç§ä¿è¯ä¹‹é—´çš„åŸºæœ¬æƒè¡¡ï¼Œå¹¶ç¡®å®šäº†ç ”ç©¶ç©ºç™½ï¼ŒåŒ…æ‹¬ç¼ºä¹ä»£è¡¨ä¸“ä¸šé¢†åŸŸçš„ç°å®åŸºå‡†æµ‹è¯•ä»¥åŠç¼ºä¹å°†å½¢å¼ä¿è¯ç½®äºæƒ…å¢ƒä¸­çš„å®è¯è¯„ä¼°ã€‚é€šè¿‡å¯¹å››ç§é¢†å…ˆæ–¹æ³•åœ¨äº”ä¸ªä¸“ä¸šé¢†åŸŸçš„çœŸå®æ•°æ®é›†ä¸Šçš„å®è¯åˆ†æï¼Œæˆ‘ä»¬åœ¨ç°å®çš„éšç§çº¦æŸä¸‹æ˜¾ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼ˆÎµâ‰¤4ï¼‰ï¼Œæ­ç¤ºäº†é€šç”¨é¢†åŸŸåŸºå‡†æµ‹è¯•æŠ¥å‘Šçš„ç»“æœä¸ç‰¹å®šé¢†åŸŸæ•°æ®æ€§èƒ½ä¹‹é—´çš„å·¨å¤§å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšç§ä¿æŠ¤åˆæˆæ•°æ®åœ¨é«˜é£é™©é¢†åŸŸè§£å†³åˆ†ç«‹æ•°æ®é—®é¢˜æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>æ–‡ç« æä¾›äº†ç†è§£éšç§ä¿æŠ¤åˆæˆæ•°æ®æ™¯è§‚çš„ç»¼åˆæ¡†æ¶ï¼ŒåŒ…æ‹¬ç”Ÿæˆæ¨¡å‹å’Œå·®åˆ†éšç§çš„ç†è®ºåŸºç¡€ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨ç‰¹å®šé¢†åŸŸçš„çœŸå®æ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ€§èƒ½ä¸‹é™ï¼Œåœ¨ç°å®çš„éšç§çº¦æŸä¸‹æ•ˆæœä¸ä½³ã€‚</li>
<li>ç»¼è¿°äº†æœ€æ–°çš„è¡¨æ ¼æ•°æ®ã€å›¾åƒå’Œæ–‡æœ¬å¤„ç†æ–¹æ³•ã€‚</li>
<li>æ­ç¤ºäº†ä¸‹æ¸¸ä»»åŠ¡å®ç”¨æ€§ä¸éšç§ä¿è¯ä¹‹é—´çš„æƒè¡¡å…³ç³»ã€‚</li>
<li>ç ”ç©¶å­˜åœ¨å¤šä¸ªå…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æœªè€ƒè™‘çš„éšç§æ³„éœ²ã€å®è¯éªŒè¯çš„ç¼ºä¹ä»¥åŠé’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„ç°å®åŸºå‡†æµ‹è¯•çš„ä¸è¶³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20846">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-421e46534f5da535fd6f0e0b50fd9aeb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4ff8de4c0f9e406522d81b204560132.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69dc08551263ab85c11c4c324a047fa5.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CEFW-A-Comprehensive-Evaluation-Framework-for-Watermark-in-Large-Language-Models"><a href="#CEFW-A-Comprehensive-Evaluation-Framework-for-Watermark-in-Large-Language-Models" class="headerlink" title="CEFW: A Comprehensive Evaluation Framework for Watermark in Large   Language Models"></a>CEFW: A Comprehensive Evaluation Framework for Watermark in Large   Language Models</h2><p><strong>Authors:Shuhao Zhang, Bo Cheng, Jiale Han, Yuli Chen, Zhixuan Wu, Changbao Li, Pingli Gu</strong></p>
<p>Text watermarking provides an effective solution for identifying synthetic text generated by large language models. However, existing techniques often focus on satisfying specific criteria while ignoring other key aspects, lacking a unified evaluation. To fill this gap, we propose the Comprehensive Evaluation Framework for Watermark (CEFW), a unified framework that comprehensively evaluates watermarking methods across five key dimensions: ease of detection, fidelity of text quality, minimal embedding cost, robustness to adversarial attacks, and imperceptibility to prevent imitation or forgery. By assessing watermarks according to all these key criteria, CEFW offers a thorough evaluation of their practicality and effectiveness. Moreover, we introduce a simple and effective watermarking method called Balanced Watermark (BW), which guarantees robustness and imperceptibility through balancing the way watermark information is added. Extensive experiments show that BW outperforms existing methods in overall performance across all evaluation dimensions. We release our code to the community for future research. <a target="_blank" rel="noopener" href="https://github.com/DrankXs/BalancedWatermark">https://github.com/DrankXs/BalancedWatermark</a>. </p>
<blockquote>
<p>æ–‡æœ¬æ°´å°ä¸ºè¯†åˆ«å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ–‡æœ¬æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰æŠ€æœ¯å¾€å¾€åªå…³æ³¨æ»¡è¶³ç‰¹å®šæ ‡å‡†è€Œå¿½è§†å…¶ä»–å…³é”®æ–¹é¢ï¼Œç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°ä½“ç³»ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†æ°´å°ç»¼åˆè¯„ä¼°æ¡†æ¶ï¼ˆCEFWï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢è¯„ä¼°æ°´å°æ–¹æ³•çš„ç»Ÿä¸€æ¡†æ¶ï¼Œæ¶µç›–äº†äº”ä¸ªå…³é”®ç»´åº¦ï¼šæ£€æµ‹ä¾¿æ·æ€§ã€æ–‡æœ¬è´¨é‡ä¿çœŸåº¦ã€æœ€å°åµŒå…¥æˆæœ¬ã€å¯¹æŠ—æ”»å‡»çš„é²æ£’æ€§ä»¥åŠé˜²æ­¢æ¨¡ä»¿æˆ–ä¼ªé€ çš„ä¸å¯å¯Ÿè§‰æ€§ã€‚é€šè¿‡è¿™äº›å…³é”®æ ‡å‡†æ¥è¯„ä¼°æ°´å°ï¼ŒCEFWæä¾›äº†å¯¹å…¶å®ç”¨æ€§å’Œæœ‰æ•ˆæ€§çš„å…¨é¢è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ°´å°æ–¹æ³•â€”â€”å¹³è¡¡æ°´å°ï¼ˆBWï¼‰ï¼Œå®ƒé€šè¿‡å¹³è¡¡æ°´å°ä¿¡æ¯æ·»åŠ çš„æ–¹å¼ï¼Œä¿è¯äº†é²æ£’æ€§å’Œä¸å¯å¯Ÿè§‰æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBWåœ¨æ‰€æœ‰è¯„ä¼°ç»´åº¦ä¸Šçš„æ€»ä½“æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬å‘ç¤¾åŒºå‘å¸ƒæˆ‘ä»¬çš„ä»£ç ä»¥ä¾›æœªæ¥ç ”ç©¶ä½¿ç”¨ã€‚ç›¸å…³é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/DrankXs/BalancedWatermark">https://github.com/DrankXs/BalancedWatermark</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20802v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æ°´å°ä¸ºè¯†åˆ«å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ–‡æœ¬æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚é’ˆå¯¹ç°æœ‰æŠ€æœ¯ç¼ºä¹ç»Ÿä¸€è¯„ä¼°çš„é—®é¢˜ï¼Œæå‡ºäº†ç»¼åˆæ°´å°è¯„ä»·æ¡†æ¶ï¼ˆCEFWï¼‰ï¼Œä»äº”ä¸ªå…³é”®ç»´åº¦å…¨é¢è¯„ä¼°æ°´å°æ–¹æ³•ï¼šæ£€æµ‹ä¾¿æ·æ€§ã€æ–‡æœ¬è´¨é‡ä¿çœŸåº¦ã€åµŒå…¥æˆæœ¬æœ€å°åŒ–ã€å¯¹æŠ—æ”»å‡»çš„é²æ£’æ€§ï¼Œä»¥åŠé˜²æ­¢æ¨¡ä»¿æˆ–ä¼ªé€ çš„ä¸å¯å¯Ÿè§‰æ€§ã€‚CEFWä¸ºæ°´å°çš„å®é™…æ•ˆæœå’Œå®ç”¨æ€§æä¾›äº†å…¨é¢çš„è¯„ä¼°ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ°´å°æ–¹æ³•â€”â€”å¹³è¡¡æ°´å°ï¼ˆBWï¼‰ï¼Œé€šè¿‡å¹³è¡¡æ·»åŠ æ°´å°ä¿¡æ¯çš„æ–¹å¼ï¼Œä¿è¯äº†å…¶é²æ£’æ€§å’Œä¸å¯å¯Ÿè§‰æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒBWåœ¨æ‰€æœ‰çš„è¯„ä¼°ç»´åº¦ä¸Šæ•´ä½“æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬æ°´å°æ˜¯è¯†åˆ«å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ–‡æœ¬çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</li>
<li>å½“å‰æŠ€æœ¯ç¼ºä¹ç»Ÿä¸€çš„æ°´å°è¯„ä»·æ¡†æ¶ï¼Œå¯¼è‡´éš¾ä»¥å…¨é¢è¯„ä¼°æ°´å°æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>ç»¼åˆæ°´å°è¯„ä»·æ¡†æ¶ï¼ˆCEFWï¼‰ä»äº”ä¸ªå…³é”®ç»´åº¦å…¨é¢è¯„ä¼°æ°´å°æ–¹æ³•ï¼šæ£€æµ‹ä¾¿æ·æ€§ã€æ–‡æœ¬è´¨é‡ä¿çœŸåº¦ç­‰ã€‚</li>
<li>CEFWèƒ½å…¨é¢è¯„ä¼°æ°´å°çš„å®é™…æ•ˆæœå’Œå®ç”¨æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ç®€å•æœ‰æ•ˆçš„æ°´å°æ–¹æ³•â€”â€”å¹³è¡¡æ°´å°ï¼ˆBWï¼‰ã€‚</li>
<li>å¹³è¡¡æ°´å°é€šè¿‡å¹³è¡¡æ·»åŠ æ°´å°ä¿¡æ¯çš„æ–¹å¼ï¼Œä¿è¯äº†å…¶é²æ£’æ€§å’Œä¸å¯å¯Ÿè§‰æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20802">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b1e00da0045b8f50ebe202f6cc797342.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-836c0829611571f3c5f35f1f68af0a1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58a20c0f1df8b163b627dd2fd39127f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8068078809456dfff1b1073ea218421.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Reason-RFT-Reinforcement-Fine-Tuning-for-Visual-Reasoning"><a href="#Reason-RFT-Reinforcement-Fine-Tuning-for-Visual-Reasoning" class="headerlink" title="Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning"></a>Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning</h2><p><strong>Authors:Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang</strong></p>
<p>Visual reasoning abilities play a crucial role in understanding complex multimodal data, advancing both domain-specific applications and artificial general intelligence (AGI). Existing methods improve VLM reasoning via Chain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated training data to enhance visual reasoning capabilities. However, this training paradigm may lead to overfitting and cognitive rigidity, restricting the modelâ€™s ability to transfer visual reasoning skills across domains and limiting its real-world applicability. To address these limitations, we propose Reason-RFT, a novel reinforcement fine-tuning framework that significantly enhances generalization capabilities in visual reasoning tasks. Reason-RFT introduces a two-phase training framework for visual reasoning: (1) Supervised Fine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the reasoning potential of Vision-Language Models (VLMs), followed by (2) Group Relative Policy Optimization (GRPO)-based reinforcement learning that generates multiple reasoning-response pairs, significantly enhancing generalization in visual reasoning tasks. To evaluate Reason-RFTâ€™s visual reasoning capabilities, we reconstructed a comprehensive dataset spanning visual counting, structure perception, and spatial transformation. Experimental results demonstrate Reasoning-RFTâ€™s three key advantages: (1) Performance Enhancement: achieving state-of-the-art results across multiple tasks, outperforming most mainstream open-source and proprietary models; (2) Generalization Superiority: consistently maintaining robust performance across diverse tasks and domains, outperforming alternative training paradigms; (3) Data Efficiency: excelling in few-shot learning scenarios while surpassing full-dataset SFT baselines. Project website: <a target="_blank" rel="noopener" href="https://tanhuajie.github.io/ReasonRFT">https://tanhuajie.github.io/ReasonRFT</a> </p>
<blockquote>
<p>è§†è§‰æ¨ç†èƒ½åŠ›åœ¨ç†è§£å¤æ‚çš„å¤šæ¨¡æ€æ•°æ®ã€æ¨åŠ¨ç‰¹å®šé¢†åŸŸåº”ç”¨å’Œé€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰å‘å±•æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æå‡è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä½¿ç”¨ç²¾å¿ƒæ ‡æ³¨çš„è®­ç»ƒæ•°æ®å¢å¼ºè§†è§‰æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ç§è®­ç»ƒæ¨¡å¼å¯èƒ½ä¼šå¯¼è‡´è¿‡åº¦æ‹Ÿåˆå’Œè®¤çŸ¥åƒµåŒ–ï¼Œé™åˆ¶æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸè½¬ç§»è§†è§‰æ¨ç†æŠ€èƒ½çš„èƒ½åŠ›ï¼Œå¹¶é™åˆ¶å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Reason-RFTï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œèƒ½æ˜¾è‘—æé«˜è§†è§‰æ¨ç†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚Reason-RFTå¼•å…¥äº†è§†è§‰æ¨ç†çš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼šé¦–å…ˆä½¿ç”¨ç²¾é€‰çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œæ¿€æ´»è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†æ½œåŠ›ï¼Œç„¶ååŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ç”Ÿæˆå¤šä¸ªæ¨ç†å“åº”å¯¹ï¼Œæ˜¾è‘—å¢å¼ºè§†è§‰æ¨ç†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è¯„ä¼°Reason-RFTçš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬é‡å»ºäº†ä¸€ä¸ªç»¼åˆæ•°æ®é›†ï¼Œæ¶µç›–äº†è§†è§‰è®¡æ•°ã€ç»“æ„æ„ŸçŸ¥å’Œç©ºé—´è½¬æ¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReasoning-RFTå…·æœ‰ä¸‰å¤§ä¼˜åŠ¿ï¼šï¼ˆ1ï¼‰æ€§èƒ½æå‡ï¼šåœ¨å¤šä»»åŠ¡ä¸Šå–å¾—æœ€å…ˆè¿›çš„æˆæœï¼Œè¶…è¶Šå¤§å¤šæ•°ä¸»æµå¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼›ï¼ˆ2ï¼‰æ³›åŒ–ä¼˜åŠ¿ï¼šåœ¨ä¸åŒä»»åŠ¡å’Œé¢†åŸŸä¸Šå§‹ç»ˆä¿æŒè‰¯å¥½çš„æ€§èƒ½ï¼Œè¶…è¶Šå…¶ä»–è®­ç»ƒæ¨¡å¼ï¼›ï¼ˆ3ï¼‰æ•°æ®æ•ˆç‡ï¼šåœ¨å°‘é‡å­¦ä¹ åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šå…¨æ•°æ®é›†çš„SFTåŸºå‡†ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://tanhuajie.github.io/ReasonRFT">https://tanhuajie.github.io/ReasonRFT</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20752v2">PDF</a> 35 pages, 22 figures</p>
<p><strong>Summary</strong></p>
<p>è§†è§‰æ¨ç†èƒ½åŠ›åœ¨å¤„ç†å¤æ‚å¤šæ¨¡æ€æ•°æ®ã€æ¨åŠ¨é¢†åŸŸç‰¹å®šåº”ç”¨å’Œé€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰å‘å±•æ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç›‘ç£å¾®è°ƒæå‡è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä½†éœ€åº”å¯¹è¿‡æ‹Ÿåˆå’Œè®¤çŸ¥åƒµåŒ–é—®é¢˜ï¼Œé™åˆ¶äº†æ¨¡å‹è·¨åŸŸè½¬ç§»è§†è§‰æ¨ç†æŠ€èƒ½çš„èƒ½åŠ›åŠå…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨èŒƒå›´ã€‚ä¸ºåº”å¯¹è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºReason-RFTæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆä½¿ç”¨ç²¾é€‰çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ•°æ®æ¿€æ´»è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†æ½œåŠ›ï¼Œç„¶åé‡‡ç”¨åŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ç”Ÿæˆå¤šä¸ªæ¨ç†å“åº”å¯¹ï¼Œä»è€Œæ˜¾è‘—æé«˜è§†è§‰æ¨ç†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReasoning-RFTå…·æœ‰ä¸‰å¤§ä¼˜åŠ¿ï¼šæ€§èƒ½æå‡ã€æ³›åŒ–ä¼˜åŠ¿å’Œæ•°æ®æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æ¨ç†åœ¨å¤„ç†å¤æ‚å¤šæ¨¡æ€æ•°æ®å’Œæ¨åŠ¨äººå·¥æ™ºèƒ½é¢†åŸŸå‘å±•ä¸Šæ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>å½“å‰æ–¹æ³•é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç›‘ç£å¾®è°ƒæå‡è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨è¿‡æ‹Ÿåˆå’Œè®¤çŸ¥åƒµåŒ–é—®é¢˜ã€‚</li>
<li>Reason-RFTæ¡†æ¶æ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒåŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>å®éªŒè¡¨æ˜Reasoning-RFTåœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå…·æœ‰æ€§èƒ½æå‡ã€æ³›åŒ–ä¼˜åŠ¿å’Œæ•°æ®æ•ˆç‡ä¸‰å¤§ä¼˜ç‚¹ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å¤šç§ä»»åŠ¡ä¸Šå®ç°æœ€ä½³ç»“æœï¼Œå¹¶ä¼˜äºä¸»æµå¼€æºå’Œä¸“æœ‰æ¨¡å‹ã€‚</li>
<li>Reason-RFTæ¡†æ¶èƒ½å¤Ÿä¿æŒè·¨ä¸åŒä»»åŠ¡å’Œé¢†åŸŸçš„ç¨³å¥æ€§èƒ½ï¼Œå¹¶ä¼˜äºå…¶ä»–è®­ç»ƒèŒƒå¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a9d5452c8940dfb88c42167b3a4eb281.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ff52be7618643b781f0b047988a2a27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06b95518386236ce42fcfb65735cb601.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="RGB-Th-Bench-A-Dense-benchmark-for-Visual-Thermal-Understanding-of-Vision-Language-Models"><a href="#RGB-Th-Bench-A-Dense-benchmark-for-Visual-Thermal-Understanding-of-Vision-Language-Models" class="headerlink" title="RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of   Vision Language Models"></a>RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of   Vision Language Models</h2><p><strong>Authors:Mehdi Moshtaghi, Siavash H. Khajavi, Joni Pajarinen</strong></p>
<p>We introduce RGB-Th-Bench, the first benchmark designed to evaluate the ability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs. While VLMs have demonstrated remarkable progress in visual reasoning and multimodal understanding, their evaluation has been predominantly limited to RGB-based benchmarks, leaving a critical gap in assessing their capabilities in infrared vision tasks. Existing visible-infrared datasets are either task-specific or lack high-quality annotations necessary for rigorous model evaluation. To address these limitations, RGB-Th-Bench provides a comprehensive evaluation framework covering 14 distinct skill dimensions, with a total of 1,600+ expert-annotated Yes&#x2F;No questions. The benchmark employs two accuracy metrics: a standard question-level accuracy and a stricter skill-level accuracy, which evaluates model robustness across multiple questions within each skill dimension. This design ensures a thorough assessment of model performance, including resilience to adversarial and hallucinated responses. We conduct extensive evaluations on 19 state-of-the-art VLMs, revealing significant performance gaps in RGB-Thermal understanding. Our results show that even the strongest models struggle with thermal image comprehension, with performance heavily constrained by their RGB-based capabilities. Additionally, the lack of large-scale application-specific and expert-annotated thermal-caption-pair datasets in pre-training is an important reason of the observed performance gap. RGB-Th-Bench highlights the urgent need for further advancements in multimodal learning to bridge the gap between visible and thermal image understanding. The dataset is available through this link, and the evaluation code will also be made publicly available. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºRGB-Th-Benchï¼Œè¿™æ˜¯é¦–ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç†è§£RGB-çƒ­æˆåƒå›¾åƒå¯¹èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚å°½ç®¡VLMsåœ¨è§†è§‰æ¨ç†å’Œå¤šæ¨¡æ€ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å…¶è¯„ä¼°ä¸»è¦å±€é™äºåŸºäºRGBçš„åŸºå‡†æµ‹è¯•ï¼Œåœ¨è¯„ä¼°çº¢å¤–è§†è§‰ä»»åŠ¡çš„èƒ½åŠ›æ–¹é¢å­˜åœ¨å…³é”®å·®è·ã€‚ç°æœ‰çš„å¯è§å…‰-çº¢å¤–æ•°æ®é›†æ˜¯ç‰¹å®šä»»åŠ¡çš„ï¼Œæˆ–è€…ç¼ºä¹è¿›è¡Œä¸¥æ ¼æ¨¡å‹è¯„ä¼°æ‰€éœ€çš„é«˜è´¨é‡æ³¨é‡Šã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼ŒRGB-Th-Benchæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–14ä¸ªä¸åŒçš„æŠ€èƒ½ç»´åº¦ï¼Œæ€»å…±æœ‰1600å¤šä¸ªä¸“å®¶æ³¨é‡Šçš„æ˜¯éé—®é¢˜ã€‚è¯¥åŸºå‡†æµ‹è¯•é‡‡ç”¨ä¸¤é¡¹å‡†ç¡®åº¦æŒ‡æ ‡ï¼šæ ‡å‡†çš„é—®ç­”å‡†ç¡®åº¦å’Œä¸¥æ ¼çš„æŠ€èƒ½å‡†ç¡®åº¦ï¼Œåè€…è¯„ä¼°æ¨¡å‹åœ¨å„ä¸ªæŠ€èƒ½ç»´åº¦å†…è·¨å¤šä¸ªé—®é¢˜çš„ç¨³å¥æ€§ã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ŒåŒ…æ‹¬å¯¹æŠ—æ€§å’Œå¹»è§‰å“åº”çš„éŸ§æ€§ã€‚æˆ‘ä»¬å¯¹19ä¸ªæœ€å…ˆè¿›VLMè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå‘ç°RGB-çƒ­ç†è§£æ–¹é¢çš„æ€§èƒ½å·®è·å¾ˆå¤§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å¼ºå¤§çš„æ¨¡å‹åœ¨çƒ­æˆåƒç†è§£æ–¹é¢ä¹Ÿé¢ä¸´å›°éš¾ï¼Œå…¶æ€§èƒ½ä¸¥é‡å—åˆ°åŸºäºRGBçš„èƒ½åŠ›çš„é™åˆ¶ã€‚æ­¤å¤–ï¼Œç¼ºä¹å¤§è§„æ¨¡ç‰¹å®šåº”ç”¨å’Œä¸“å®¶æ³¨é‡Šçš„çƒ­æˆåƒå­—å¹•å¯¹æ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒæ˜¯è§‚å¯Ÿåˆ°æ€§èƒ½å·®è·çš„é‡è¦åŸå› ã€‚RGB-Th-Benchå¼ºè°ƒäº†åœ¨å¤šæ¨¡æ€å­¦ä¹ æ–¹é¢è¿›ä¸€æ­¥å‘å±•çš„è¿«åˆ‡éœ€æ±‚ï¼Œä»¥ç¼©å°å¯è§å…‰å’Œçƒ­æˆåƒç†è§£ä¹‹é—´çš„å·®è·ã€‚æ•°æ®é›†å¯é€šè¿‡æ­¤é“¾æ¥è·å–ï¼Œè¯„ä¼°ä»£ç ä¹Ÿå°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19654v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æ¨å‡ºé¦–ä¸ªé’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„RGB-Th-BenchåŸºå‡†æµ‹è¯•å¹³å°ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹ç†è§£RGB-Thermalå›¾åƒå¯¹çš„èƒ½åŠ›ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å±€é™äºRGBåŸºå‡†æµ‹è¯•ï¼Œç¼ºä¹è¯„ä¼°çº¢å¤–è§†è§‰ä»»åŠ¡èƒ½åŠ›çš„å·¥å…·ã€‚RGB-Th-Benchæä¾›å…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–14ä¸ªä¸åŒçš„æŠ€èƒ½ç»´åº¦ï¼Œå…±æœ‰1,600å¤šä¸ªä¸“å®¶æ ‡æ³¨çš„æ˜¯éé—®é¢˜ã€‚é‡‡ç”¨ä¸¤ç§å‡†ç¡®åº¦æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼Œç¡®ä¿å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ŒåŒ…æ‹¬å¯¹æŠ—æ€§å’Œè™šæ„å“åº”çš„æŠµæŠ—åŠ›ã€‚å¯¹19ä¸ªæœ€å…ˆè¿›çš„VLMsè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå‘ç°RGB-Thermalç†è§£çš„æ€§èƒ½å·®è·æ˜¾è‘—ã€‚æœ€å¼ºæ¨¡å‹åœ¨çƒ­å›¾åƒç†è§£æ–¹é¢ä»æœ‰å›°éš¾ï¼Œæ€§èƒ½å—RGBèƒ½åŠ›é™åˆ¶ã€‚å‘¼åè¿›ä¸€æ­¥æ”¹è¿›å¤šæ¨¡æ€å­¦ä¹ ï¼Œä»¥ç¼©å°å¯è§ä¸çƒ­å›¾åƒç†è§£ä¹‹é—´çš„å·®è·ã€‚æ•°æ®é›†å¯é€šè¿‡é“¾æ¥è·å–ï¼Œè¯„ä¼°ä»£ç ä¹Ÿå°†å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>RGB-Th-Benchæ˜¯é¦–ä¸ªè¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç†è§£RGB-Thermalå›¾åƒå¯¹èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å±€é™äºRGBå›¾åƒï¼Œç¼ºä¹è¯„ä¼°æ¨¡å‹åœ¨çº¢å¤–è§†è§‰ä»»åŠ¡ä¸­èƒ½åŠ›çš„å·¥å…·ã€‚</li>
<li>RGB-Th-Benchæä¾›å…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–å¤šç§æŠ€èƒ½ç»´åº¦å’Œå‡†ç¡®åº¦æŒ‡æ ‡ï¼Œç¡®ä¿å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æœ€å…ˆè¿›çš„VLMsåœ¨RGB-Thermalç†è§£æ–¹é¢å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œæœ€å¼ºæ¨¡å‹åœ¨çƒ­å›¾åƒç†è§£æ–¹é¢ä»æœ‰å›°éš¾ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½å—RGBèƒ½åŠ›é™åˆ¶ï¼Œè¡¨æ˜å¤šæ¨¡æ€å­¦ä¹ çš„æ”¹è¿›æ˜¯è¿«åˆ‡éœ€è¦çš„ã€‚</li>
<li>ç¼ºä¹å¤§è§„æ¨¡åº”ç”¨ç‰¹å®šçš„å’Œä¸“å®¶æ ‡æ³¨çš„çƒ­å›¾åƒæ•°æ®é›†æ˜¯æ€§èƒ½å·®è·çš„é‡è¦åŸå› ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f34d7182ac87e63d15736f092a8e7549.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae8ab5b082a3aa22932ff77e1f766159.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fc2f715e0a6ab3a45ba716411ba2b7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20e03af4f8759c03c7cb145cc681646e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc59117a2b12e3a86a0c6538d24ebc3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-869be15a878958a4f0bfb1e9b34ac6d5.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="SoK-How-Robust-is-Audio-Watermarking-in-Generative-AI-models"><a href="#SoK-How-Robust-is-Audio-Watermarking-in-Generative-AI-models" class="headerlink" title="SoK: How Robust is Audio Watermarking in Generative AI models?"></a>SoK: How Robust is Audio Watermarking in Generative AI models?</h2><p><strong>Authors:Yizhu Wen, Ashwin Innuganti, Aaron Bien Ramos, Hanqing Guo, Qiben Yan</strong></p>
<p>Audio watermarking is increasingly used to verify the provenance of AI-generated content, enabling applications such as detecting AI-generated speech, protecting music IP, and defending against voice cloning. To be effective, audio watermarks must resist removal attacks that distort signals to evade detection. While many schemes claim robustness, these claims are typically tested in isolation and against a limited set of attacks. A systematic evaluation against diverse removal attacks is lacking, hindering practical deployment. In this paper, we investigate whether recent watermarking schemes that claim robustness can withstand a broad range of removal attacks. First, we introduce a taxonomy covering 22 audio watermarking schemes. Next, we summarize their underlying technologies and potential vulnerabilities. We then present a large-scale empirical study to assess their robustness. To support this, we build an evaluation framework encompassing 22 types of removal attacks (109 configurations) including signal-level, physical-level, and AI-induced distortions. We reproduce 9 watermarking schemes using open-source code, identify 8 new highly effective attacks, and highlight 11 key findings that expose the fundamental limitations of these methods across 3 public datasets. Our results reveal that none of the surveyed schemes can withstand all tested distortions. This evaluation offers a comprehensive view of how current watermarking methods perform under real-world threats. Our demo and code are available at <a target="_blank" rel="noopener" href="https://sokaudiowm.github.io/">https://sokaudiowm.github.io/</a>. </p>
<blockquote>
<p>éŸ³é¢‘æ°´å°æŠ€æœ¯è¶Šæ¥è¶Šå¤šåœ°ç”¨äºéªŒè¯äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹çš„å‡ºå¤„ï¼Œæ”¯æŒæ£€æµ‹äººå·¥æ™ºèƒ½ç”Ÿæˆçš„è¯­éŸ³ã€ä¿æŠ¤éŸ³ä¹çŸ¥è¯†äº§æƒå’Œé˜²èŒƒå£°éŸ³å…‹éš†ç­‰åº”ç”¨ã€‚è¦æœ‰æ•ˆå‘æŒ¥ä½œç”¨ï¼ŒéŸ³é¢‘æ°´å°å¿…é¡»æŠµæŠ—èƒ½å¤Ÿæ‰­æ›²ä¿¡å·ä»¥é€ƒé¿æ£€æµ‹çš„å»é™¤æ”»å‡»ã€‚å°½ç®¡è®¸å¤šæ–¹æ¡ˆéƒ½å£°ç§°å…·æœ‰ç¨³å¥æ€§ï¼Œä½†è¿™äº›å£°ç§°é€šå¸¸æ˜¯åœ¨å­¤ç«‹çš„æƒ…å†µä¸‹å’Œæœ‰é™çš„æ”»å‡»é›†ä¸Šè¿›è¡Œæµ‹è¯•çš„ã€‚ç¼ºä¹é’ˆå¯¹å¤šç§å»é™¤æ”»å‡»çš„ç³»ç»Ÿè¯„ä¼°ï¼Œé˜»ç¢äº†å®é™…éƒ¨ç½²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†æœ€è¿‘å£°ç§°å…·æœ‰ç¨³å¥æ€§çš„æ°´å°æ–¹æ¡ˆæ˜¯å¦èƒ½å¤Ÿæ‰¿å—å¹¿æ³›çš„å»é™¤æ”»å‡»ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»äº†æ¶µç›–22ç§éŸ³é¢‘æ°´å°æ–¹æ¡ˆçš„åˆ†ç±»ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ€»ç»“äº†å…¶åŸºç¡€æŠ€æœ¯å’Œæ½œåœ¨æ¼æ´ã€‚ç„¶åï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å¤§è§„æ¨¡å®è¯ç ”ç©¶æ¥è¯„ä¼°å…¶ç¨³å¥æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬22ç§ç±»å‹çš„å»é™¤æ”»å‡»ï¼ˆ109ç§é…ç½®ï¼‰ï¼ŒåŒ…æ‹¬ä¿¡å·çº§ã€ç‰©ç†çº§å’Œäººå·¥æ™ºèƒ½å¼•èµ·çš„å¤±çœŸã€‚æˆ‘ä»¬ä½¿ç”¨å¼€æºä»£ç é‡æ–°åˆ¶ä½œäº†9ç§æ°´å°æ–¹æ¡ˆï¼Œè¯†åˆ«äº†8ç§æ–°çš„é«˜åº¦æœ‰æ•ˆçš„æ”»å‡»ï¼Œå¹¶å¼ºè°ƒäº†11ä¸ªå…³é”®å‘ç°ï¼Œæ­ç¤ºäº†è¿™äº›æ–¹æ³•åœ¨3ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„æ ¹æœ¬å±€é™æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ‰€è°ƒæŸ¥çš„æ–¹æ¡ˆä¸­æ²¡æœ‰å“ªä¸€ç§èƒ½å¤Ÿæ‰¿å—æ‰€æœ‰æµ‹è¯•çš„å¤±çœŸã€‚è¿™ä¸€è¯„ä¼°æä¾›äº†å½“å‰æ°´å°æ–¹æ³•åœ¨ç°å®ä¸–ç•Œå¨èƒä¸‹çš„å…¨é¢è§†å›¾ã€‚æˆ‘ä»¬çš„æ¼”ç¤ºå’Œä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://sokaudiowm.github.io/]%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://sokaudiowm.github.io/]ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.19176v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†éŸ³é¢‘æ°´å°æŠ€æœ¯åœ¨éªŒè¯AIç”Ÿæˆå†…å®¹æ–¹é¢çš„åº”ç”¨ï¼ŒåŒ…æ‹¬æ£€æµ‹AIç”Ÿæˆçš„è¯­éŸ³ã€ä¿æŠ¤éŸ³ä¹çŸ¥è¯†äº§æƒå’Œé˜²èŒƒå£°éŸ³å…‹éš†ã€‚ä¸ºäº†æœ‰æ•ˆåº”å¯¹å„ç§ç§»é™¤æ”»å‡»ï¼Œæ–‡ç« å¯¹ä¸åŒçš„éŸ³é¢‘æ°´å°æ–¹æ¡ˆè¿›è¡Œäº†ç³»ç»Ÿçš„è¯„ä¼°ï¼Œå‘ç°ç›®å‰çš„æ°´å°æ–¹æ¡ˆæ™®éå­˜åœ¨æ¼æ´ï¼Œæ— æ³•åº”å¯¹æ‰€æœ‰æµ‹è¯•ä¸­çš„å¤±çœŸã€‚è¯¥è¯„ä¼°ä¸ºå½“å‰æ°´å°æ–¹æ³•åœ¨çœŸå®å¨èƒä¸‹çš„è¡¨ç°æä¾›äº†å…¨é¢çš„è§†è§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘æ°´å°æŠ€æœ¯ç”¨äºéªŒè¯AIç”Ÿæˆå†…å®¹çš„æ¥æºï¼Œå¦‚æ£€æµ‹AIç”Ÿæˆçš„è¯­éŸ³ã€ä¿æŠ¤éŸ³ä¹çŸ¥è¯†äº§æƒå’Œé˜²èŒƒå£°éŸ³å…‹éš†ã€‚</li>
<li>ç›®å‰çš„æ°´å°æ–¹æ¡ˆç¼ºä¹å¯¹å„ç§ç§»é™¤æ”»å‡»çš„å…¨é¢è¯„ä¼°ï¼Œé™åˆ¶äº†å…¶å®è·µåº”ç”¨ã€‚</li>
<li>æ–‡ç« å¼•å…¥äº†ä¸€ä¸ªåŒ…å«22ç§éŸ³é¢‘æ°´å°æ–¹æ¡ˆçš„åˆ†ç±»ï¼Œå¹¶æ¦‚è¿°äº†å®ƒä»¬çš„åŸºç¡€æŠ€æœ¯å’Œæ½œåœ¨æ¼æ´ã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡å®è¯ç ”ç©¶è¯„ä¼°äº†æ°´å°æ–¹æ¡ˆçš„ç¨³å¥æ€§ï¼Œå»ºç«‹äº†åŒ…å«22ç§ç±»å‹ï¼ˆå…±109ç§é…ç½®ï¼‰çš„ç§»é™¤æ”»å‡»è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>ç ”ç©¶äººå‘˜å¤ç°äº†9ç§æ°´å°æ–¹æ¡ˆå¹¶å‘ç°äº†8ç§æ–°çš„é«˜æ•ˆæ”»å‡»æ–¹å¼ã€‚</li>
<li>è°ƒæŸ¥ç»“æœæ­ç¤ºäº†è¿™äº›æ–¹æ³•çš„æ ¹æœ¬å±€é™æ€§ï¼Œå³æ²¡æœ‰ä¸€ç§æ–¹æ¡ˆèƒ½å¤ŸæŠµå¾¡æ‰€æœ‰æµ‹è¯•çš„å¤±çœŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.19176">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab757a1d56f5201d709efe96a3d58f27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ae6b0f30071f630656015fb9bbeea3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8653c0789b546e57c1d27bd23639e09e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3736acb676bae171e1f460cf29ed7044.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6ff7648c6efba1ec4a8ab3c97ef303d.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-29/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-29/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-29/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c3b9e5fdecfd4fa108ec125db048b93f.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-29  Video-R1 Reinforcing Video Reasoning in MLLMs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-28/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0ec8137fb5402ec14830f20113f37f69.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-28  Perceptually Accurate 3D Talking Head Generation New Definitions,   Speech-Mesh Representation, and Evaluation Metrics
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-28
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24595.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
