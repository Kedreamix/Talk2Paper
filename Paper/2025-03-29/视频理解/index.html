<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="视频理解">
    <meta name="description" content="视频理解 方向最新论文已更新，请持续关注 Update in 2025-03-29  Mobile-VideoGPT Fast and Accurate Video Understanding Language Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>视频理解 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-6b2f931e94b94da6c54be25993bb758a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">视频理解</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">视频理解</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                视频理解
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    29 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-29-更新"><a href="#2025-03-29-更新" class="headerlink" title="2025-03-29 更新"></a>2025-03-29 更新</h1><h2 id="Mobile-VideoGPT-Fast-and-Accurate-Video-Understanding-Language-Model"><a href="#Mobile-VideoGPT-Fast-and-Accurate-Video-Understanding-Language-Model" class="headerlink" title="Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model"></a>Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model</h2><p><strong>Authors:Abdelrahman Shaker, Muhammad Maaz, Chenhui Gou, Hamid Rezatofighi, Salman Khan, Fahad Shahbaz Khan</strong></p>
<p>Video understanding models often struggle with high computational requirements, extensive parameter counts, and slow inference speed, making them inefficient for practical use. To tackle these challenges, we propose Mobile-VideoGPT, an efficient multimodal framework designed to operate with fewer than a billion parameters. Unlike traditional video large multimodal models (LMMs), Mobile-VideoGPT consists of lightweight dual visual encoders, efficient projectors, and a small language model (SLM), enabling real-time throughput. To further improve efficiency, we present an Attention-Based Frame Scoring mechanism to select the key-frames, along with an efficient token projector that prunes redundant visual tokens and preserves essential contextual cues. We evaluate our model across well-established six video understanding benchmarks (e.g., MVBench, EgoSchema, NextQA, and PercepTest). Our results show that Mobile-VideoGPT-0.5B can generate up to 46 tokens per second while outperforming existing state-of-the-art 0.5B-parameter models by 6 points on average with 40% fewer parameters and more than 2x higher throughput. Our code and models are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/Amshaker/Mobile-VideoGPT">https://github.com/Amshaker/Mobile-VideoGPT</a>. </p>
<blockquote>
<p>视频理解模型通常面临高计算要求、大量参数和缓慢推理速度的难题，使得它们在实际应用中效率低下。为了应对这些挑战，我们提出了Mobile-VideoGPT，这是一个高效的多媒体框架，设计用于处理不到十亿个参数。与传统的视频大型多媒体模型（LMM）不同，Mobile-VideoGPT由轻量级的双视觉编码器、高效投影仪和小型语言模型（SLM）组成，可实现实时处理。为了进一步提高效率，我们提出了一种基于注意力的帧评分机制来选择关键帧，以及一个高效的令牌投影仪，可以删除冗余的视觉令牌并保留重要的上下文线索。我们在六个公认的视频理解基准测试（例如MVBench、EgoSchema、NextQA和PercepTest）上评估了我们的模型。结果表明，Mobile-VideoGPT-0.5B每秒可以生成高达46个令牌，同时在性能上超过了现有的最先进的0.5B参数模型，平均高出6个点，同时参数减少了40%，吞吐量提高了两倍以上。我们的代码和模型可在以下网址公开获取：<a target="_blank" rel="noopener" href="https://github.com/Amshaker/Mobile-VideoGPT">https://github.com/Amshaker/Mobile-VideoGPT</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21782v1">PDF</a> Technical Report. Project Page:   <a target="_blank" rel="noopener" href="https://amshaker.github.io/Mobile-VideoGPT">https://amshaker.github.io/Mobile-VideoGPT</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对视频理解模型存在的计算要求高、参数多、推理速度慢等问题，提出的Mobile-VideoGPT高效多模态框架。该框架使用轻量级双视觉编码器、高效投影器和小型语言模型，实现实时处理速度。同时，引入基于注意力的帧评分机制和有效的令牌投影器，以提高效率并保留关键上下文线索。在多个视频理解基准测试中，Mobile-VideoGPT-0.5B表现出卓越性能，生成令牌速度高达每秒46个，同时在平均指标上优于现有最先进的0.5B参数模型，且使用参数更少、吞吐量更高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mobile-VideoGPT是一个高效的多模态框架，旨在解决视频理解模型的高计算要求和低效率问题。</li>
<li>该框架使用轻量级组件，如双视觉编码器、高效投影器和小型语言模型（SLM），以实现实时处理速度。</li>
<li>引入基于注意力的帧评分机制，以选择关键帧，提高模型效率。</li>
<li>通过有效的令牌投影器，去除冗余视觉令牌，保留关键上下文线索。</li>
<li>Mobile-VideoGPT-0.5B在多个视频理解基准测试中表现优越，生成令牌速度高。</li>
<li>与现有最先进的0.5B参数模型相比，Mobile-VideoGPT平均指标更优，且使用更少的参数和更高的吞吐量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21782">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-11f359574ae9cb3de2928c17222db416.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e16eafac9a1d01d84d7f803f5362990.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f2c7d9cb9a5b352d0a277aba9b36f8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-168de3e7f71bafa11a2e77be45a24487.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="BOLT-Boost-Large-Vision-Language-Model-Without-Training-for-Long-form-Video-Understanding"><a href="#BOLT-Boost-Large-Vision-Language-Model-Without-Training-for-Long-form-Video-Understanding" class="headerlink" title="BOLT: Boost Large Vision-Language Model Without Training for Long-form   Video Understanding"></a>BOLT: Boost Large Vision-Language Model Without Training for Long-form   Video Understanding</h2><p><strong>Authors:Shuming Liu, Chen Zhao, Tianqi Xu, Bernard Ghanem</strong></p>
<p>Large video-language models (VLMs) have demonstrated promising progress in various video understanding tasks. However, their effectiveness in long-form video analysis is constrained by limited context windows. Traditional approaches, such as uniform frame sampling, often inevitably allocate resources to irrelevant content, diminishing their effectiveness in real-world scenarios. In this paper, we introduce BOLT, a method to BOost Large VLMs without additional Training through a comprehensive study of frame selection strategies. First, to enable a more realistic evaluation of VLMs in long-form video understanding, we propose a multi-source retrieval evaluation setting. Our findings reveal that uniform sampling performs poorly in noisy contexts, underscoring the importance of selecting the right frames. Second, we explore several frame selection strategies based on query-frame similarity and analyze their effectiveness at inference time. Our results show that inverse transform sampling yields the most significant performance improvement, increasing accuracy on the Video-MME benchmark from 53.8% to 56.1% and MLVU benchmark from 58.9% to 63.4%. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/sming256/BOLT">https://github.com/sming256/BOLT</a>. </p>
<blockquote>
<p>大型视频语言模型（VLMs）在各种视频理解任务中取得了令人瞩目的进展。然而，它们在长视频分析中的有效性受限于有限的上下文窗口。传统方法（如均匀帧采样）通常不可避免地会将资源分配给无关内容，从而降低了它们在现实场景中的有效性。在本文中，我们介绍了BOLT，这是一种通过深入研究帧选择策略，无需额外训练即可提升大型VLM性能的方法。首先，为了更现实地评估VLM在长视频理解中的性能，我们提出了一个跨源检索评估环境。我们的研究结果表明，在嘈杂的上下文中均匀采样表现较差，突显了选择正确帧的重要性。其次，我们基于查询帧相似性探索了多种帧选择策略，并分析了它们在推理阶段的效率。我们的结果表明，逆变换采样带来了最显著的性能提升，在Video-MME基准测试集上的准确率从53.8%提高到56.1%，在MLVU基准测试集上的准确率从58.9%提高到63.4%。我们的代码位于<a target="_blank" rel="noopener" href="https://github.com/sming256/BOLT%E3%80%82">https://github.com/sming256/BOLT。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21483v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong><br>大型视频语言模型（VLMs）在视频理解任务中展现出巨大潜力，但在长视频分析中受限于有限的上下文窗口。本文提出BOLT方法，通过深入研究帧选择策略，提升VLMs性能而无需额外训练。首先，提出多源检索评估设置，更现实地评估VLMs在长视频理解中的表现。研究发现，均匀采样在嘈杂环境下表现不佳，强调选择正确帧的重要性。其次，探索基于查询帧相似性的多种帧选择策略，并在推理时间分析它们的效率。结果显示，逆变换采样带来最显著的性能提升，在Video-MME和MLVU基准测试上的准确率分别提升至56.1%和63.4%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型视频语言模型（VLMs）在视频理解任务中表现优异，但在长视频分析中存在上下文窗口限制。</li>
<li>提出BOLT方法，通过帧选择策略提升VLMs性能，无需额外训练。</li>
<li>引入多源检索评估设置，更现实地评估VLMs在长视频理解中的效果。</li>
<li>均匀采样在嘈杂环境下表现不佳，需要选择正确的帧。</li>
<li>逆变换采样策略能显著提升性能。</li>
<li>BOLT方法在Video-MME和MLVU基准测试上实现性能提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21483">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a8b7d5a282937dd84915992aa36f0454.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b14a829dadea67f7884a01d43a24a929.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ab1c900bf76363a1513ff7752ccf8fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21d3c0b66335caa448030c940d56d0b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8458bf191dc14341f2f5d52ce3633650.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RoadSocial-A-Diverse-VideoQA-Dataset-and-Benchmark-for-Road-Event-Understanding-from-Social-Video-Narratives"><a href="#RoadSocial-A-Diverse-VideoQA-Dataset-and-Benchmark-for-Road-Event-Understanding-from-Social-Video-Narratives" class="headerlink" title="RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event   Understanding from Social Video Narratives"></a>RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event   Understanding from Social Video Narratives</h2><p><strong>Authors:Chirag Parikh, Deepti Rawat, Rakshitha R. T., Tathagata Ghosh, Ravi Kiran Sarvadevabhatla</strong></p>
<p>We introduce RoadSocial, a large-scale, diverse VideoQA dataset tailored for generic road event understanding from social media narratives. Unlike existing datasets limited by regional bias, viewpoint bias and expert-driven annotations, RoadSocial captures the global complexity of road events with varied geographies, camera viewpoints (CCTV, handheld, drones) and rich social discourse. Our scalable semi-automatic annotation framework leverages Text LLMs and Video LLMs to generate comprehensive question-answer pairs across 12 challenging QA tasks, pushing the boundaries of road event understanding. RoadSocial is derived from social media videos spanning 14M frames and 414K social comments, resulting in a dataset with 13.2K videos, 674 tags and 260K high-quality QA pairs. We evaluate 18 Video LLMs (open-source and proprietary, driving-specific and general-purpose) on our road event understanding benchmark. We also demonstrate RoadSocial’s utility in improving road event understanding capabilities of general-purpose Video LLMs. </p>
<blockquote>
<p>我们介绍了RoadSocial，这是一个针对社交媒体叙事中通用道路事件理解的大规模、多样化的VideoQA数据集。与现有受地区偏见、观点偏见和专家驱动标注限制的数据集不同，RoadSocial通过捕捉全球各地的道路事件，捕捉到了多样化的地理、相机视角（闭路电视、手持相机、无人机等）和丰富的社会话语的复杂性。我们的可扩展的半自动标注框架利用文本LLM和视频LLM生成涵盖12项挑战性问答任务的全面问答对，从而推动道路事件理解的边界。RoadSocial来源于社交媒体视频，包含1400万帧和41.4万条社交评论，形成了一个包含1.32万段视频、674个标签和26万条高质量问答对的数据集。我们在道路事件理解基准测试上评估了18个视频LLM（包括开源和专有、专用驾驶和通用）。我们还展示了RoadSocial在提高通用视频LLM对道路事件理解能力方面的实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21459v1">PDF</a> Accepted at CVPR 2025; Project Page: <a target="_blank" rel="noopener" href="https://roadsocial.github.io/">https://roadsocial.github.io/</a></p>
<p><strong>摘要</strong></p>
<p>RoadSocial是一个大规模、多样化的VideoQA数据集，专为从社交媒体叙事中理解通用道路事件而设计。与其他受地域偏见、视角偏见和专家驱动标注限制的现有数据集不同，RoadSocial捕捉了全球道路事件的复杂性，具有多样的地理、摄像头视角（如CCTV、手持设备、无人机）和丰富的社会话语。我们的可扩展的半自动标注框架利用文本LLM和视频LLM生成涵盖12项具有挑战性的问答任务的全面问答对，突破了对道路事件理解的界限。RoadSocial来源于包含14M帧和41.4万条社会评论的社交媒体视频，形成了一个包含1.32万个视频、674个标签和26万条高质量问答对的数据集。我们在道路事件理解基准测试上评估了18个视频LLM（包括开源和专有、驾驶专用和通用）。我们还展示了RoadSocial在提高通用视频LLM对道路事件的理解能力方面的实用性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>RoadSocial是一个针对社交媒体中道路事件理解的大规模、多样化VideoQA数据集。</li>
<li>与现有数据集相比，RoadSocial更具全球性和复杂性，涵盖多样的地理和摄像头视角，以及丰富的社会话语。</li>
<li>采用半自动标注框架，利用文本LLM和视频LLM生成全面的问答对。</li>
<li>RoadSocial包含来自社交媒体视频的13.2K视频、674个标签和26万条高质量问答对。</li>
<li>对18个视频LLM进行了道路事件理解基准测试，包括开源和专有、驾驶专用和通用LLM。</li>
<li>RoadSocial有助于提高通用视频LLM对道路事件的理解能力。</li>
<li>RoadSocial数据集的创建和应用为道路事件理解研究提供了新的视角和挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21459">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-bc2162c96c165763fda0a81846d9da88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb3721b9969be3241956ef5dc1c88d5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e892ae2f32ad130fffd09392e8424e75.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e65b23169f08c07bafe63560d12bab14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-976dacfec493773bde3769b50373ca58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f02c7d61b1af907b2adc4eca13f85a60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1f35de2384f60894dbe953911348c18.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SlowFast-LLaVA-1-5-A-Family-of-Token-Efficient-Video-Large-Language-Models-for-Long-Form-Video-Understanding"><a href="#SlowFast-LLaVA-1-5-A-Family-of-Token-Efficient-Video-Large-Language-Models-for-Long-Form-Video-Understanding" class="headerlink" title="SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language   Models for Long-Form Video Understanding"></a>SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language   Models for Long-Form Video Understanding</h2><p><strong>Authors:Mingze Xu, Mingfei Gao, Shiyu Li, Jiasen Lu, Zhe Gan, Zhengfeng Lai, Meng Cao, Kai Kang, Yinfei Yang, Afshin Dehghan</strong></p>
<p>We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of video large language models (LLMs) offering a token-efficient solution for long-form video understanding. We incorporate the two-stream SlowFast mechanism into a streamlined training pipeline, and perform joint video-image training on a carefully curated data mixture of only publicly available datasets. Our primary focus is on highly efficient model scales (1B and 3B), demonstrating that even relatively small Video LLMs can achieve state-of-the-art performance on video understanding, meeting the demand for mobile-friendly models. Experimental results demonstrate that SF-LLaVA-1.5 achieves superior performance on a wide range of video and image tasks, with robust results at all model sizes (ranging from 1B to 7B). Notably, SF-LLaVA-1.5 achieves state-of-the-art results in long-form video understanding (e.g., LongVideoBench and MLVU) and excels at small scales across various video benchmarks. </p>
<blockquote>
<p>我们介绍了SlowFast-LLaVA-1.5（简称SF-LLaVA-1.5），这是一系列视频大型语言模型（LLM），为长视频理解提供了一种高效的令牌解决方案。我们将双流的SlowFast机制纳入简化的训练管道中，并在经过精心筛选的仅公开可用的数据集混合体上进行联合视频图像训练。我们的主要焦点是高效模型规模（1B和3B），证明即使相对较小的视频LLM也可以在视频理解方面实现最先进的性能，满足对移动友好型模型的需求。实验结果表明，SF-LLaVA-1.5在广泛的视频和图像任务上表现卓越，在各种模型规模（从1B到7B）下均表现出稳健的结果。值得注意的是，SF-LLaVA-1.5在长视频理解方面达到了最新水平（例如LongVideoBench和MLVU），并在各种视频基准测试中表现出色，尤其在小规模测试中表现尤为突出。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18943v2">PDF</a> Technical report</p>
<p><strong>Summary</strong></p>
<p>SF-LLaVA-1.5是一种高效的视频大型语言模型（LLM），采用两流的SlowFast机制进行训练，并在公开数据集上进行联合视频图像训练。该模型关注高效模型规模，展示即使在较小的视频LLM中也能实现视频理解的卓越性能。实验结果显示，SF-LLaVA-1.5在各种视频和图像任务上表现优异，模型大小从百亿到七十亿不等，且在长视频理解方面达到业界领先。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SF-LLaVA-1.5是一个针对长形式视频理解的高效视频大型语言模型。</li>
<li>采用两流的SlowFast机制进行训练以提高模型的效率。</li>
<li>模型在公开数据集上进行联合视频图像训练，增强了模型的泛化能力。</li>
<li>模型关注高效模型规模，满足移动设备的性能需求。</li>
<li>实验结果显示SF-LLaVA-1.5在各种视频和图像任务上表现卓越。</li>
<li>SF-LLaVA-1.5在长视频理解方面达到了业界领先的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18943">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c3f437c7b274db96bd67abb60034ac2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85da2a7be139f5e8ab48b701a4b7ccad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac8628e0afa40c6539ed32e41c52e319.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5050ab2a7b818501e85769024de54621.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="OVO-Bench-How-Far-is-Your-Video-LLMs-from-Real-World-Online-Video-Understanding"><a href="#OVO-Bench-How-Far-is-Your-Video-LLMs-from-Real-World-Online-Video-Understanding" class="headerlink" title="OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video   Understanding?"></a>OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video   Understanding?</h2><p><strong>Authors:Yifei Li, Junbo Niu, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, Xiaoyi Dong, Haodong Duan, Shuangrui Ding, Rui Qian, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang</strong></p>
<p>Temporal Awareness, the ability to reason dynamically based on the timestamp when a question is raised, is the key distinction between offline and online video LLMs. Unlike offline models, which rely on complete videos for static, post hoc analysis, online models process video streams incrementally and dynamically adapt their responses based on the timestamp at which the question is posed. Despite its significance, temporal awareness has not been adequately evaluated in existing benchmarks. To fill this gap, we present OVO-Bench (Online-VideO-Benchmark), a novel video benchmark that emphasizes the importance of timestamps for advanced online video understanding capability benchmarking. OVO-Bench evaluates the ability of video LLMs to reason and respond to events occurring at specific timestamps under three distinct scenarios: (1) Backward tracing: trace back to past events to answer the question. (2) Real-time understanding: understand and respond to events as they unfold at the current timestamp. (3) Forward active responding: delay the response until sufficient future information becomes available to answer the question accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videos and approximately human-curated 2,800 fine-grained meta-annotations with precise timestamps. We combine automated generation pipelines with human curation. With these high-quality samples, we further developed an evaluation pipeline to systematically query video LLMs along the video timeline. Evaluations of nine Video-LLMs reveal that, despite advancements on traditional benchmarks, current models struggle with online video understanding, showing a significant gap compared to human agents. We hope OVO-Bench will drive progress in video LLMs and inspire future research in online video reasoning. Our benchmark and code can be accessed at <a target="_blank" rel="noopener" href="https://github.com/JoeLeelyf/OVO-Bench">https://github.com/JoeLeelyf/OVO-Bench</a>. </p>
<blockquote>
<p>时间感知能力是区分离线与在线视频LLM的关键区别，它可以根据提出问题的时间戳进行动态推理。与依赖完整视频进行静态事后分析的离线模型不同，在线模型会逐步处理视频流，并根据提出问题的时间戳动态调整其响应。尽管时间感知能力非常重要，但在现有的基准测试中并未得到足够评估。为了填补这一空白，我们推出了OVO-Bench（在线视频基准测试），这是一个新的视频基准测试，它强调时间戳对于高级在线视频能力评估的重要性。OVO-Bench评估视频LLM在三种不同场景下对发生在特定时间戳的事件进行推理和响应的能力：(1)向后追踪：追溯过去的事件以回答问题。(2)实时理解：理解并响应当前时间戳上发生的事件。(3)向前主动响应：延迟响应，直到收集到足够的未来信息来准确回答问题。OVO-Bench包含12项任务，包含644个独特视频和约由人类精心制作的2800个精细元注释以及精确的时间戳。我们将自动化生成管道与人类制作相结合。通过这些高质量的样本，我们进一步开发了一个评估管道，以系统地查询视频LLM沿视频时间线的表现。对九个视频LLM的评估表明，尽管在传统基准测试上取得了进展，但当前模型在在线视频理解方面仍然面临困难，与人类代理人相比存在明显差距。我们希望OVO-Bench能够促进视频LLM的进步，并激发未来在线视频推理的研究灵感。我们的基准测试和代码可在<a target="_blank" rel="noopener" href="https://github.com/JoeLeelyf/OVO-Bench">https://github.com/JoeLeelyf/OVO-Bench</a>访问。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05510v2">PDF</a> CVPR 2025</p>
<p><strong>摘要</strong></p>
<p>在线视频理解的关键是时间感知能力，即基于问题提出的时间戳进行动态推理的能力。与依赖完整视频进行静态、事后分析的离线模型不同，在线模型能够增量处理视频流并根据问题的时间戳动态调整其响应。尽管时间感知能力非常重要，但在现有的基准测试中并未得到充分评估。为填补这一空白，本文提出了OVO-Bench（在线视频基准测试），这是一个重视时间戳重要性的新型视频基准测试，用于评估视频LLM（大型语言模型）的高级在线视频理解能力的基准测试。OVO-Bench评估视频LLM在三个不同场景下的特定时间戳上推理和回应事件的能力：（1）向后追踪：追溯过去的事件来回答问题；（2）实时理解：理解并响应当前时间戳上发生的事件；（3）向前主动响应：延迟响应，直到未来有足够的信息来准确回答问题。OVO-Bench包含12项任务，由644个独特视频和约经人工精细标注的2800个精确时间戳组成。我们结合自动化生成管道和人工审核，通过这些高质量样本进一步开发了一个评估管道，以系统地查询视频LLM沿视频时间线的表现。对九个视频LLM的评估表明，尽管在传统基准测试上有所进展，但当前模型在线视频理解方面仍存在差距，与人类代理人相比存在较大差距。我们希望OVO-Bench能够促进视频LLM的进步，并激发未来在线视频推理的研究灵感。我们的基准测试和代码可访问于：<a target="_blank" rel="noopener" href="https://github.com/JoeLeelyf/OVO-Bench%E3%80%82">https://github.com/JoeLeelyf/OVO-Bench。</a></p>
<p><strong>关键见解</strong></p>
<ol>
<li>在线视频理解的关键是时间感知能力，即根据问题提出的时间戳进行动态推理的能力。</li>
<li>OVO-Bench是首个重视时间感知的视频基准测试，旨在评估视频LLM的在线视频理解能力。</li>
<li>OVO-Bench包含多种任务场景，包括向后追踪、实时理解和向前主动响应等。</li>
<li>当前视频LLM模型在线视频理解方面仍存在挑战，与人类表现有较大差距。</li>
<li>OVO-Bench提供了高质量的视频样本和精细标注，有助于促进视频LLM的研究进展。</li>
<li>通过OVO-Bench的评估结果，发现现有视频LLM在应对未来信息的需求方面存在不足。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05510">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8d4f7c0c310402b52381598edbc4ba17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-515709833fe3448488c84c965eabae59.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6b2f931e94b94da6c54be25993bb758a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LongViTU-Instruction-Tuning-for-Long-Form-Video-Understanding"><a href="#LongViTU-Instruction-Tuning-for-Long-Form-Video-Understanding" class="headerlink" title="LongViTU: Instruction Tuning for Long-Form Video Understanding"></a>LongViTU: Instruction Tuning for Long-Form Video Understanding</h2><p><strong>Authors:Rujie Wu, Xiaojian Ma, Hai Ci, Yue Fan, Yuxuan Wang, Haozhe Zhao, Qing Li, Yizhou Wang</strong></p>
<p>This paper introduces LongViTU, a large-scale (~121k QA pairs, ~900h videos), automatically generated dataset for long-form video understanding. We propose a systematic approach that organizes videos into a hierarchical tree structure for QA generation and incorporates self-revision mechanisms to ensure high-quality QA pairs. Each QA pair in LongViTU features: 1) long-term context (average certificate length of 4.6 minutes); 2) rich knowledge and condensed reasoning (commonsense, causality, planning, etc.)). We also offer explicit timestamp annotations of relevant events for each QA pair. We have conducted extensive human studies on LongViTU, and the results prove the quality of our dataset. To better evaluate the challenges posed by LongViTU’s emphasis on long-term context and condensed reasoning, we manually curate a subset of LongViTU into a benchmark. Evaluations using a state-of-the-art open-source model (LongVU), a proprietary model (Gemini-1.5-Pro), and human annotators yield GPT-4 scores of 49.9, 52.3, and 81.0, respectively, underscoring the substantial difficulty presented by LongViTU questions. Performing supervised fine-tuning (SFT) of LongVU and LLaVA-Video on LongViTU data results in average performance gains of 2.5% and 3.7%, respectively, across a suite of long video understanding benchmarks (EgoSchema, VideoMME-Long, MLVU, LVBench). </p>
<blockquote>
<p>本文介绍了LongViTU，这是一个大规模（<del>12.1万对问答对，</del>900小时视频）的自动生成的用于长格式视频理解的数据集。我们提出了一种系统的方法，将视频组织成层次树结构来进行问答生成，并引入了自我修正机制以确保高质量的问答对。LongViTU中的每个问答对具有以下特点：1）长期上下文（平均证书长度为4.6分钟）；2）丰富的知识和精炼的推理（常识、因果、规划等）。我们还为每个问答对提供了相关事件的时间戳注释。我们对LongViTU进行了广泛的人类研究，结果证明了数据集的质量。为了更好地评估LongViTU对长期上下文和精炼推理的重视所带来的挑战，我们从LongViTU中手动筛选出一个基准数据集。使用最先进的开源模型（LongVU）、专有模型（Gemini-1.5-Pro）和人类注释者进行评估，分别得到GPT-4的分数为49.9、52.3和81.0，这凸显了LongViTU问题所构成的重大挑战。对LongVU和LLaVA-Video进行LongViTU数据的监督微调（SFT）后，在多个长视频理解基准测试（EgoSchema、VideoMME-Long、MLVU、LVBench）上的性能平均提高了2.5%和3.7%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05037v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了LongViTU数据集，这是一个大规模自动生成的用于长视频理解的数据集。该数据集采用系统性方法组织视频为层次树结构进行问答生成，并引入自我修正机制确保高质量的问答对。LongViTU中的每个问答对都具有长期上下文、丰富知识和精炼推理的特点。此外，还为每个问答对提供了相关事件的时间戳注释。通过广泛的人类研究验证了数据集的质量。为了评估LongViTU对长期上下文和精炼推理的重视所带来的挑战，我们从LongViTU中手动筛选出一个基准测试集。使用先进开源模型LongVU、专有模型Gemini-1.5-Pro和人类评估者的评估结果，表明LongViTU问题的实质性难度。通过对LongVU和LLaVA-Video进行LongViTU数据的监督微调（SFT），在一系列长视频理解基准测试（EgoSchema、VideoMME-Long、MLVU、LVBench）上的平均性能分别提高了2.5%和3.7%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LongViTU是一个大规模自动生成的用于长视频理解的数据集。</li>
<li>采用系统性方法组织视频为层次树结构进行问答生成，确保高质量问答对。</li>
<li>每个问答对具有长期上下文、丰富知识和精炼推理的特点。</li>
<li>数据集提供每个问答对的相关事件时间戳注释。</li>
<li>通过广泛的人类研究验证了数据集质量。</li>
<li>LongViTU问题的实质难度得到了评估，先进模型与人类的性能表现存在差异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05037">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-96c450ffc8c91688615f5250909516e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5bb4c4e24540f347c1e82bb98919eff1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9f94432f162f57caf3bd0075a66569f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb4b68920225285d2c7dfdd9ae84f57a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddea5facaee8dd23263520cc24205af8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Video-3D-LLM-Learning-Position-Aware-Video-Representation-for-3D-Scene-Understanding"><a href="#Video-3D-LLM-Learning-Position-Aware-Video-Representation-for-3D-Scene-Understanding" class="headerlink" title="Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene   Understanding"></a>Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene   Understanding</h2><p><strong>Authors:Duo Zheng, Shijia Huang, Liwei Wang</strong></p>
<p>The rapid advancement of Multimodal Large Language Models (MLLMs) has significantly impacted various multimodal tasks. However, these models face challenges in tasks that require spatial understanding within 3D environments. Efforts to enhance MLLMs, such as incorporating point cloud features, have been made, yet a considerable gap remains between the models’ learned representations and the inherent complexity of 3D scenes. This discrepancy largely stems from the training of MLLMs on predominantly 2D data, which restricts their effectiveness in comprehending 3D spaces. To address this issue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM, for 3D scene understanding. By treating 3D scenes as dynamic videos and incorporating 3D position encoding into these representations, our Video-3D LLM aligns video representations with real-world spatial contexts more accurately. In addition, we have implemented a maximum coverage sampling technique to optimize the trade-off between computational cost and performance. Extensive experiments demonstrate that our model achieves state-of-the-art performance on several 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D. </p>
<blockquote>
<p>随着多模态大型语言模型（MLLMs）的快速发展，它对各种多模态任务产生了重大影响。然而，这些模型在处理需要在三维环境中进行空间理解的任务时面临挑战。为了增强MLLMs的功能，已经做出了努力，如融入点云特征等，但模型学到的表示与三维场景固有复杂性之间仍存在较大差距。这种差异主要源于MLLMs主要在二维数据上进行训练，这限制了它们在理解三维空间方面的有效性。为了解决这个问题，本文提出了一种新型通用模型，即Video-3D LLM，用于三维场景理解。通过将三维场景视为动态视频并将三维位置编码融入这些表示中，我们的Video-3D LLM能够更准确地使视频表示与真实世界空间上下文对齐。此外，我们实现了一种最大覆盖采样技术，以优化计算成本与性能之间的平衡。大量实验表明，我们的模型在多个三维场景理解基准测试中达到了最先进的性能，包括ScanRefer、Multi3DRefer、Scan2Cap、ScanQA和SQA3D。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00493v2">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong><br>多任务模态的大型语言模型（MLLMs）的快速进步对多种模态的任务产生了重大影响，但在需要理解三维环境空间的任务中仍面临挑战。为弥补这一差距，本文提出了一种新的通用模型——Video-3D LLM，用于三维场景理解。通过把三维场景当作动态视频并融入三维位置编码，Video-3D LLM能更准确地匹配视频表现和真实世界空间背景。同时，我们实施了一种最大覆盖采样技术，以优化计算成本和性能之间的平衡。实验证明，我们的模型在多个三维场景理解基准测试中达到了最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）的进步推动了多种模态任务的进展。</li>
<li>MLLMs在处理需要三维空间理解的任务时存在挑战。</li>
<li>当前模型主要基于二维数据的训练，限制了其在三维空间理解中的有效性。</li>
<li>为解决上述问题，提出了一种新的通用模型——Video-3D LLM用于三维场景理解。</li>
<li>Video-3D LLM通过将三维场景视为动态视频并融入三维位置编码，更准确地匹配视频表现和真实世界空间背景。</li>
<li>Video-3D LLM采用了最大覆盖采样技术来平衡计算成本和性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00493">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-5549b56d5bced2d0142d116c8e7ad12c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-896c941d18251cc0ca3eac34edbed3fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abb980361656247b3f5521a5b2060717.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-29/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-29/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">视频理解</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-29/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1f1f842d6870075589b69c1fa51223da.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-03-29  Test-Time Visual In-Context Tuning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-29/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-149af2b24c3ff7f7e8e6c651c07b3699.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-03-29  MotionDiff Training-free Zero-shot Interactive Motion Editing via   Flow-assisted Multi-view Diffusion
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">22950.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
