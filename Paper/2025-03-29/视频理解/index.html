<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="è§†é¢‘ç†è§£">
    <meta name="description" content="è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-29  Mobile-VideoGPT Fast and Accurate Video Understanding Language Model">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>è§†é¢‘ç†è§£ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-6b2f931e94b94da6c54be25993bb758a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">è§†é¢‘ç†è§£</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                è§†é¢‘ç†è§£
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    7.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    29 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-29-æ›´æ–°"><a href="#2025-03-29-æ›´æ–°" class="headerlink" title="2025-03-29 æ›´æ–°"></a>2025-03-29 æ›´æ–°</h1><h2 id="Mobile-VideoGPT-Fast-and-Accurate-Video-Understanding-Language-Model"><a href="#Mobile-VideoGPT-Fast-and-Accurate-Video-Understanding-Language-Model" class="headerlink" title="Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model"></a>Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model</h2><p><strong>Authors:Abdelrahman Shaker, Muhammad Maaz, Chenhui Gou, Hamid Rezatofighi, Salman Khan, Fahad Shahbaz Khan</strong></p>
<p>Video understanding models often struggle with high computational requirements, extensive parameter counts, and slow inference speed, making them inefficient for practical use. To tackle these challenges, we propose Mobile-VideoGPT, an efficient multimodal framework designed to operate with fewer than a billion parameters. Unlike traditional video large multimodal models (LMMs), Mobile-VideoGPT consists of lightweight dual visual encoders, efficient projectors, and a small language model (SLM), enabling real-time throughput. To further improve efficiency, we present an Attention-Based Frame Scoring mechanism to select the key-frames, along with an efficient token projector that prunes redundant visual tokens and preserves essential contextual cues. We evaluate our model across well-established six video understanding benchmarks (e.g., MVBench, EgoSchema, NextQA, and PercepTest). Our results show that Mobile-VideoGPT-0.5B can generate up to 46 tokens per second while outperforming existing state-of-the-art 0.5B-parameter models by 6 points on average with 40% fewer parameters and more than 2x higher throughput. Our code and models are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/Amshaker/Mobile-VideoGPT">https://github.com/Amshaker/Mobile-VideoGPT</a>. </p>
<blockquote>
<p>è§†é¢‘ç†è§£æ¨¡å‹é€šå¸¸é¢ä¸´é«˜è®¡ç®—è¦æ±‚ã€å¤§é‡å‚æ•°å’Œç¼“æ…¢æ¨ç†é€Ÿåº¦çš„éš¾é¢˜ï¼Œä½¿å¾—å®ƒä»¬åœ¨å®é™…åº”ç”¨ä¸­æ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Mobile-VideoGPTï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å¤šåª’ä½“æ¡†æ¶ï¼Œè®¾è®¡ç”¨äºå¤„ç†ä¸åˆ°åäº¿ä¸ªå‚æ•°ã€‚ä¸ä¼ ç»Ÿçš„è§†é¢‘å¤§å‹å¤šåª’ä½“æ¨¡å‹ï¼ˆLMMï¼‰ä¸åŒï¼ŒMobile-VideoGPTç”±è½»é‡çº§çš„åŒè§†è§‰ç¼–ç å™¨ã€é«˜æ•ˆæŠ•å½±ä»ªå’Œå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ç»„æˆï¼Œå¯å®ç°å®æ—¶å¤„ç†ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„å¸§è¯„åˆ†æœºåˆ¶æ¥é€‰æ‹©å…³é”®å¸§ï¼Œä»¥åŠä¸€ä¸ªé«˜æ•ˆçš„ä»¤ç‰ŒæŠ•å½±ä»ªï¼Œå¯ä»¥åˆ é™¤å†—ä½™çš„è§†è§‰ä»¤ç‰Œå¹¶ä¿ç•™é‡è¦çš„ä¸Šä¸‹æ–‡çº¿ç´¢ã€‚æˆ‘ä»¬åœ¨å…­ä¸ªå…¬è®¤çš„è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚MVBenchã€EgoSchemaã€NextQAå’ŒPercepTestï¼‰ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼ŒMobile-VideoGPT-0.5Bæ¯ç§’å¯ä»¥ç”Ÿæˆé«˜è¾¾46ä¸ªä»¤ç‰Œï¼ŒåŒæ—¶åœ¨æ€§èƒ½ä¸Šè¶…è¿‡äº†ç°æœ‰çš„æœ€å…ˆè¿›çš„0.5Bå‚æ•°æ¨¡å‹ï¼Œå¹³å‡é«˜å‡º6ä¸ªç‚¹ï¼ŒåŒæ—¶å‚æ•°å‡å°‘äº†40%ï¼Œååé‡æé«˜äº†ä¸¤å€ä»¥ä¸Šã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/Amshaker/Mobile-VideoGPT">https://github.com/Amshaker/Mobile-VideoGPT</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21782v1">PDF</a> Technical Report. Project Page:   <a target="_blank" rel="noopener" href="https://amshaker.github.io/Mobile-VideoGPT">https://amshaker.github.io/Mobile-VideoGPT</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è§†é¢‘ç†è§£æ¨¡å‹å­˜åœ¨çš„è®¡ç®—è¦æ±‚é«˜ã€å‚æ•°å¤šã€æ¨ç†é€Ÿåº¦æ…¢ç­‰é—®é¢˜ï¼Œæå‡ºçš„Mobile-VideoGPTé«˜æ•ˆå¤šæ¨¡æ€æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä½¿ç”¨è½»é‡çº§åŒè§†è§‰ç¼–ç å™¨ã€é«˜æ•ˆæŠ•å½±å™¨å’Œå°å‹è¯­è¨€æ¨¡å‹ï¼Œå®ç°å®æ—¶å¤„ç†é€Ÿåº¦ã€‚åŒæ—¶ï¼Œå¼•å…¥åŸºäºæ³¨æ„åŠ›çš„å¸§è¯„åˆ†æœºåˆ¶å’Œæœ‰æ•ˆçš„ä»¤ç‰ŒæŠ•å½±å™¨ï¼Œä»¥æé«˜æ•ˆç‡å¹¶ä¿ç•™å…³é”®ä¸Šä¸‹æ–‡çº¿ç´¢ã€‚åœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMobile-VideoGPT-0.5Bè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç”Ÿæˆä»¤ç‰Œé€Ÿåº¦é«˜è¾¾æ¯ç§’46ä¸ªï¼ŒåŒæ—¶åœ¨å¹³å‡æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„0.5Bå‚æ•°æ¨¡å‹ï¼Œä¸”ä½¿ç”¨å‚æ•°æ›´å°‘ã€ååé‡æ›´é«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mobile-VideoGPTæ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è§†é¢‘ç†è§£æ¨¡å‹çš„é«˜è®¡ç®—è¦æ±‚å’Œä½æ•ˆç‡é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶ä½¿ç”¨è½»é‡çº§ç»„ä»¶ï¼Œå¦‚åŒè§†è§‰ç¼–ç å™¨ã€é«˜æ•ˆæŠ•å½±å™¨å’Œå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ï¼Œä»¥å®ç°å®æ—¶å¤„ç†é€Ÿåº¦ã€‚</li>
<li>å¼•å…¥åŸºäºæ³¨æ„åŠ›çš„å¸§è¯„åˆ†æœºåˆ¶ï¼Œä»¥é€‰æ‹©å…³é”®å¸§ï¼Œæé«˜æ¨¡å‹æ•ˆç‡ã€‚</li>
<li>é€šè¿‡æœ‰æ•ˆçš„ä»¤ç‰ŒæŠ•å½±å™¨ï¼Œå»é™¤å†—ä½™è§†è§‰ä»¤ç‰Œï¼Œä¿ç•™å…³é”®ä¸Šä¸‹æ–‡çº¿ç´¢ã€‚</li>
<li>Mobile-VideoGPT-0.5Båœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œç”Ÿæˆä»¤ç‰Œé€Ÿåº¦é«˜ã€‚</li>
<li>ä¸ç°æœ‰æœ€å…ˆè¿›çš„0.5Bå‚æ•°æ¨¡å‹ç›¸æ¯”ï¼ŒMobile-VideoGPTå¹³å‡æŒ‡æ ‡æ›´ä¼˜ï¼Œä¸”ä½¿ç”¨æ›´å°‘çš„å‚æ•°å’Œæ›´é«˜çš„ååé‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-11f359574ae9cb3de2928c17222db416.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e16eafac9a1d01d84d7f803f5362990.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f2c7d9cb9a5b352d0a277aba9b36f8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-168de3e7f71bafa11a2e77be45a24487.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="BOLT-Boost-Large-Vision-Language-Model-Without-Training-for-Long-form-Video-Understanding"><a href="#BOLT-Boost-Large-Vision-Language-Model-Without-Training-for-Long-form-Video-Understanding" class="headerlink" title="BOLT: Boost Large Vision-Language Model Without Training for Long-form   Video Understanding"></a>BOLT: Boost Large Vision-Language Model Without Training for Long-form   Video Understanding</h2><p><strong>Authors:Shuming Liu, Chen Zhao, Tianqi Xu, Bernard Ghanem</strong></p>
<p>Large video-language models (VLMs) have demonstrated promising progress in various video understanding tasks. However, their effectiveness in long-form video analysis is constrained by limited context windows. Traditional approaches, such as uniform frame sampling, often inevitably allocate resources to irrelevant content, diminishing their effectiveness in real-world scenarios. In this paper, we introduce BOLT, a method to BOost Large VLMs without additional Training through a comprehensive study of frame selection strategies. First, to enable a more realistic evaluation of VLMs in long-form video understanding, we propose a multi-source retrieval evaluation setting. Our findings reveal that uniform sampling performs poorly in noisy contexts, underscoring the importance of selecting the right frames. Second, we explore several frame selection strategies based on query-frame similarity and analyze their effectiveness at inference time. Our results show that inverse transform sampling yields the most significant performance improvement, increasing accuracy on the Video-MME benchmark from 53.8% to 56.1% and MLVU benchmark from 58.9% to 63.4%. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/sming256/BOLT">https://github.com/sming256/BOLT</a>. </p>
<blockquote>
<p>å¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å„ç§è§†é¢‘ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººç©ç›®çš„è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨é•¿è§†é¢‘åˆ†æä¸­çš„æœ‰æ•ˆæ€§å—é™äºæœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£ã€‚ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚å‡åŒ€å¸§é‡‡æ ·ï¼‰é€šå¸¸ä¸å¯é¿å…åœ°ä¼šå°†èµ„æºåˆ†é…ç»™æ— å…³å†…å®¹ï¼Œä»è€Œé™ä½äº†å®ƒä»¬åœ¨ç°å®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†BOLTï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æ·±å…¥ç ”ç©¶å¸§é€‰æ‹©ç­–ç•¥ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯æå‡å¤§å‹VLMæ€§èƒ½çš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œä¸ºäº†æ›´ç°å®åœ°è¯„ä¼°VLMåœ¨é•¿è§†é¢‘ç†è§£ä¸­çš„æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè·¨æºæ£€ç´¢è¯„ä¼°ç¯å¢ƒã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨å˜ˆæ‚çš„ä¸Šä¸‹æ–‡ä¸­å‡åŒ€é‡‡æ ·è¡¨ç°è¾ƒå·®ï¼Œçªæ˜¾äº†é€‰æ‹©æ­£ç¡®å¸§çš„é‡è¦æ€§ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åŸºäºæŸ¥è¯¢å¸§ç›¸ä¼¼æ€§æ¢ç´¢äº†å¤šç§å¸§é€‰æ‹©ç­–ç•¥ï¼Œå¹¶åˆ†æäº†å®ƒä»¬åœ¨æ¨ç†é˜¶æ®µçš„æ•ˆç‡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé€†å˜æ¢é‡‡æ ·å¸¦æ¥äº†æœ€æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨Video-MMEåŸºå‡†æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ä»53.8%æé«˜åˆ°56.1%ï¼Œåœ¨MLVUåŸºå‡†æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ä»58.9%æé«˜åˆ°63.4%ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/sming256/BOLT%E3%80%82">https://github.com/sming256/BOLTã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21483v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong><br>å¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨é•¿è§†é¢‘åˆ†æä¸­å—é™äºæœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£ã€‚æœ¬æ–‡æå‡ºBOLTæ–¹æ³•ï¼Œé€šè¿‡æ·±å…¥ç ”ç©¶å¸§é€‰æ‹©ç­–ç•¥ï¼Œæå‡VLMsæ€§èƒ½è€Œæ— éœ€é¢å¤–è®­ç»ƒã€‚é¦–å…ˆï¼Œæå‡ºå¤šæºæ£€ç´¢è¯„ä¼°è®¾ç½®ï¼Œæ›´ç°å®åœ°è¯„ä¼°VLMsåœ¨é•¿è§†é¢‘ç†è§£ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œå‡åŒ€é‡‡æ ·åœ¨å˜ˆæ‚ç¯å¢ƒä¸‹è¡¨ç°ä¸ä½³ï¼Œå¼ºè°ƒé€‰æ‹©æ­£ç¡®å¸§çš„é‡è¦æ€§ã€‚å…¶æ¬¡ï¼Œæ¢ç´¢åŸºäºæŸ¥è¯¢å¸§ç›¸ä¼¼æ€§çš„å¤šç§å¸§é€‰æ‹©ç­–ç•¥ï¼Œå¹¶åœ¨æ¨ç†æ—¶é—´åˆ†æå®ƒä»¬çš„æ•ˆç‡ã€‚ç»“æœæ˜¾ç¤ºï¼Œé€†å˜æ¢é‡‡æ ·å¸¦æ¥æœ€æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨Video-MMEå’ŒMLVUåŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«æå‡è‡³56.1%å’Œ63.4%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†é¢‘ç†è§£ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨é•¿è§†é¢‘åˆ†æä¸­å­˜åœ¨ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ã€‚</li>
<li>æå‡ºBOLTæ–¹æ³•ï¼Œé€šè¿‡å¸§é€‰æ‹©ç­–ç•¥æå‡VLMsæ€§èƒ½ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚</li>
<li>å¼•å…¥å¤šæºæ£€ç´¢è¯„ä¼°è®¾ç½®ï¼Œæ›´ç°å®åœ°è¯„ä¼°VLMsåœ¨é•¿è§†é¢‘ç†è§£ä¸­çš„æ•ˆæœã€‚</li>
<li>å‡åŒ€é‡‡æ ·åœ¨å˜ˆæ‚ç¯å¢ƒä¸‹è¡¨ç°ä¸ä½³ï¼Œéœ€è¦é€‰æ‹©æ­£ç¡®çš„å¸§ã€‚</li>
<li>é€†å˜æ¢é‡‡æ ·ç­–ç•¥èƒ½æ˜¾è‘—æå‡æ€§èƒ½ã€‚</li>
<li>BOLTæ–¹æ³•åœ¨Video-MMEå’ŒMLVUåŸºå‡†æµ‹è¯•ä¸Šå®ç°æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21483">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a8b7d5a282937dd84915992aa36f0454.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b14a829dadea67f7884a01d43a24a929.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ab1c900bf76363a1513ff7752ccf8fa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21d3c0b66335caa448030c940d56d0b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8458bf191dc14341f2f5d52ce3633650.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RoadSocial-A-Diverse-VideoQA-Dataset-and-Benchmark-for-Road-Event-Understanding-from-Social-Video-Narratives"><a href="#RoadSocial-A-Diverse-VideoQA-Dataset-and-Benchmark-for-Road-Event-Understanding-from-Social-Video-Narratives" class="headerlink" title="RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event   Understanding from Social Video Narratives"></a>RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event   Understanding from Social Video Narratives</h2><p><strong>Authors:Chirag Parikh, Deepti Rawat, Rakshitha R. T., Tathagata Ghosh, Ravi Kiran Sarvadevabhatla</strong></p>
<p>We introduce RoadSocial, a large-scale, diverse VideoQA dataset tailored for generic road event understanding from social media narratives. Unlike existing datasets limited by regional bias, viewpoint bias and expert-driven annotations, RoadSocial captures the global complexity of road events with varied geographies, camera viewpoints (CCTV, handheld, drones) and rich social discourse. Our scalable semi-automatic annotation framework leverages Text LLMs and Video LLMs to generate comprehensive question-answer pairs across 12 challenging QA tasks, pushing the boundaries of road event understanding. RoadSocial is derived from social media videos spanning 14M frames and 414K social comments, resulting in a dataset with 13.2K videos, 674 tags and 260K high-quality QA pairs. We evaluate 18 Video LLMs (open-source and proprietary, driving-specific and general-purpose) on our road event understanding benchmark. We also demonstrate RoadSocialâ€™s utility in improving road event understanding capabilities of general-purpose Video LLMs. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†RoadSocialï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ç¤¾äº¤åª’ä½“å™äº‹ä¸­é€šç”¨é“è·¯äº‹ä»¶ç†è§£çš„å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„VideoQAæ•°æ®é›†ã€‚ä¸ç°æœ‰å—åœ°åŒºåè§ã€è§‚ç‚¹åè§å’Œä¸“å®¶é©±åŠ¨æ ‡æ³¨é™åˆ¶çš„æ•°æ®é›†ä¸åŒï¼ŒRoadSocialé€šè¿‡æ•æ‰å…¨çƒå„åœ°çš„é“è·¯äº‹ä»¶ï¼Œæ•æ‰åˆ°äº†å¤šæ ·åŒ–çš„åœ°ç†ã€ç›¸æœºè§†è§’ï¼ˆé—­è·¯ç”µè§†ã€æ‰‹æŒç›¸æœºã€æ— äººæœºç­‰ï¼‰å’Œä¸°å¯Œçš„ç¤¾ä¼šè¯è¯­çš„å¤æ‚æ€§ã€‚æˆ‘ä»¬çš„å¯æ‰©å±•çš„åŠè‡ªåŠ¨æ ‡æ³¨æ¡†æ¶åˆ©ç”¨æ–‡æœ¬LLMå’Œè§†é¢‘LLMç”Ÿæˆæ¶µç›–12é¡¹æŒ‘æˆ˜æ€§é—®ç­”ä»»åŠ¡çš„å…¨é¢é—®ç­”å¯¹ï¼Œä»è€Œæ¨åŠ¨é“è·¯äº‹ä»¶ç†è§£çš„è¾¹ç•Œã€‚RoadSocialæ¥æºäºç¤¾äº¤åª’ä½“è§†é¢‘ï¼ŒåŒ…å«1400ä¸‡å¸§å’Œ41.4ä¸‡æ¡ç¤¾äº¤è¯„è®ºï¼Œå½¢æˆäº†ä¸€ä¸ªåŒ…å«1.32ä¸‡æ®µè§†é¢‘ã€674ä¸ªæ ‡ç­¾å’Œ26ä¸‡æ¡é«˜è´¨é‡é—®ç­”å¯¹çš„æ•°æ®é›†ã€‚æˆ‘ä»¬åœ¨é“è·¯äº‹ä»¶ç†è§£åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†18ä¸ªè§†é¢‘LLMï¼ˆåŒ…æ‹¬å¼€æºå’Œä¸“æœ‰ã€ä¸“ç”¨é©¾é©¶å’Œé€šç”¨ï¼‰ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†RoadSocialåœ¨æé«˜é€šç”¨è§†é¢‘LLMå¯¹é“è·¯äº‹ä»¶ç†è§£èƒ½åŠ›æ–¹é¢çš„å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21459v1">PDF</a> Accepted at CVPR 2025; Project Page: <a target="_blank" rel="noopener" href="https://roadsocial.github.io/">https://roadsocial.github.io/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>RoadSocialæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„VideoQAæ•°æ®é›†ï¼Œä¸“ä¸ºä»ç¤¾äº¤åª’ä½“å™äº‹ä¸­ç†è§£é€šç”¨é“è·¯äº‹ä»¶è€Œè®¾è®¡ã€‚ä¸å…¶ä»–å—åœ°åŸŸåè§ã€è§†è§’åè§å’Œä¸“å®¶é©±åŠ¨æ ‡æ³¨é™åˆ¶çš„ç°æœ‰æ•°æ®é›†ä¸åŒï¼ŒRoadSocialæ•æ‰äº†å…¨çƒé“è·¯äº‹ä»¶çš„å¤æ‚æ€§ï¼Œå…·æœ‰å¤šæ ·çš„åœ°ç†ã€æ‘„åƒå¤´è§†è§’ï¼ˆå¦‚CCTVã€æ‰‹æŒè®¾å¤‡ã€æ— äººæœºï¼‰å’Œä¸°å¯Œçš„ç¤¾ä¼šè¯è¯­ã€‚æˆ‘ä»¬çš„å¯æ‰©å±•çš„åŠè‡ªåŠ¨æ ‡æ³¨æ¡†æ¶åˆ©ç”¨æ–‡æœ¬LLMå’Œè§†é¢‘LLMç”Ÿæˆæ¶µç›–12é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®ç­”ä»»åŠ¡çš„å…¨é¢é—®ç­”å¯¹ï¼Œçªç ´äº†å¯¹é“è·¯äº‹ä»¶ç†è§£çš„ç•Œé™ã€‚RoadSocialæ¥æºäºåŒ…å«14Må¸§å’Œ41.4ä¸‡æ¡ç¤¾ä¼šè¯„è®ºçš„ç¤¾äº¤åª’ä½“è§†é¢‘ï¼Œå½¢æˆäº†ä¸€ä¸ªåŒ…å«1.32ä¸‡ä¸ªè§†é¢‘ã€674ä¸ªæ ‡ç­¾å’Œ26ä¸‡æ¡é«˜è´¨é‡é—®ç­”å¯¹çš„æ•°æ®é›†ã€‚æˆ‘ä»¬åœ¨é“è·¯äº‹ä»¶ç†è§£åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†18ä¸ªè§†é¢‘LLMï¼ˆåŒ…æ‹¬å¼€æºå’Œä¸“æœ‰ã€é©¾é©¶ä¸“ç”¨å’Œé€šç”¨ï¼‰ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†RoadSocialåœ¨æé«˜é€šç”¨è§†é¢‘LLMå¯¹é“è·¯äº‹ä»¶çš„ç†è§£èƒ½åŠ›æ–¹é¢çš„å®ç”¨æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>RoadSocialæ˜¯ä¸€ä¸ªé’ˆå¯¹ç¤¾äº¤åª’ä½“ä¸­é“è·¯äº‹ä»¶ç†è§£çš„å¤§è§„æ¨¡ã€å¤šæ ·åŒ–VideoQAæ•°æ®é›†ã€‚</li>
<li>ä¸ç°æœ‰æ•°æ®é›†ç›¸æ¯”ï¼ŒRoadSocialæ›´å…·å…¨çƒæ€§å’Œå¤æ‚æ€§ï¼Œæ¶µç›–å¤šæ ·çš„åœ°ç†å’Œæ‘„åƒå¤´è§†è§’ï¼Œä»¥åŠä¸°å¯Œçš„ç¤¾ä¼šè¯è¯­ã€‚</li>
<li>é‡‡ç”¨åŠè‡ªåŠ¨æ ‡æ³¨æ¡†æ¶ï¼Œåˆ©ç”¨æ–‡æœ¬LLMå’Œè§†é¢‘LLMç”Ÿæˆå…¨é¢çš„é—®ç­”å¯¹ã€‚</li>
<li>RoadSocialåŒ…å«æ¥è‡ªç¤¾äº¤åª’ä½“è§†é¢‘çš„13.2Kè§†é¢‘ã€674ä¸ªæ ‡ç­¾å’Œ26ä¸‡æ¡é«˜è´¨é‡é—®ç­”å¯¹ã€‚</li>
<li>å¯¹18ä¸ªè§†é¢‘LLMè¿›è¡Œäº†é“è·¯äº‹ä»¶ç†è§£åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬å¼€æºå’Œä¸“æœ‰ã€é©¾é©¶ä¸“ç”¨å’Œé€šç”¨LLMã€‚</li>
<li>RoadSocialæœ‰åŠ©äºæé«˜é€šç”¨è§†é¢‘LLMå¯¹é“è·¯äº‹ä»¶çš„ç†è§£èƒ½åŠ›ã€‚</li>
<li>RoadSocialæ•°æ®é›†çš„åˆ›å»ºå’Œåº”ç”¨ä¸ºé“è·¯äº‹ä»¶ç†è§£ç ”ç©¶æä¾›äº†æ–°çš„è§†è§’å’ŒæŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21459">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bc2162c96c165763fda0a81846d9da88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb3721b9969be3241956ef5dc1c88d5d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e892ae2f32ad130fffd09392e8424e75.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e65b23169f08c07bafe63560d12bab14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-976dacfec493773bde3769b50373ca58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f02c7d61b1af907b2adc4eca13f85a60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1f35de2384f60894dbe953911348c18.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SlowFast-LLaVA-1-5-A-Family-of-Token-Efficient-Video-Large-Language-Models-for-Long-Form-Video-Understanding"><a href="#SlowFast-LLaVA-1-5-A-Family-of-Token-Efficient-Video-Large-Language-Models-for-Long-Form-Video-Understanding" class="headerlink" title="SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language   Models for Long-Form Video Understanding"></a>SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language   Models for Long-Form Video Understanding</h2><p><strong>Authors:Mingze Xu, Mingfei Gao, Shiyu Li, Jiasen Lu, Zhe Gan, Zhengfeng Lai, Meng Cao, Kai Kang, Yinfei Yang, Afshin Dehghan</strong></p>
<p>We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of video large language models (LLMs) offering a token-efficient solution for long-form video understanding. We incorporate the two-stream SlowFast mechanism into a streamlined training pipeline, and perform joint video-image training on a carefully curated data mixture of only publicly available datasets. Our primary focus is on highly efficient model scales (1B and 3B), demonstrating that even relatively small Video LLMs can achieve state-of-the-art performance on video understanding, meeting the demand for mobile-friendly models. Experimental results demonstrate that SF-LLaVA-1.5 achieves superior performance on a wide range of video and image tasks, with robust results at all model sizes (ranging from 1B to 7B). Notably, SF-LLaVA-1.5 achieves state-of-the-art results in long-form video understanding (e.g., LongVideoBench and MLVU) and excels at small scales across various video benchmarks. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†SlowFast-LLaVA-1.5ï¼ˆç®€ç§°SF-LLaVA-1.5ï¼‰ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä¸ºé•¿è§†é¢‘ç†è§£æä¾›äº†ä¸€ç§é«˜æ•ˆçš„ä»¤ç‰Œè§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å°†åŒæµçš„SlowFastæœºåˆ¶çº³å…¥ç®€åŒ–çš„è®­ç»ƒç®¡é“ä¸­ï¼Œå¹¶åœ¨ç»è¿‡ç²¾å¿ƒç­›é€‰çš„ä»…å…¬å¼€å¯ç”¨çš„æ•°æ®é›†æ··åˆä½“ä¸Šè¿›è¡Œè”åˆè§†é¢‘å›¾åƒè®­ç»ƒã€‚æˆ‘ä»¬çš„ä¸»è¦ç„¦ç‚¹æ˜¯é«˜æ•ˆæ¨¡å‹è§„æ¨¡ï¼ˆ1Bå’Œ3Bï¼‰ï¼Œè¯æ˜å³ä½¿ç›¸å¯¹è¾ƒå°çš„è§†é¢‘LLMä¹Ÿå¯ä»¥åœ¨è§†é¢‘ç†è§£æ–¹é¢å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ»¡è¶³å¯¹ç§»åŠ¨å‹å¥½å‹æ¨¡å‹çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSF-LLaVA-1.5åœ¨å¹¿æ³›çš„è§†é¢‘å’Œå›¾åƒä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œåœ¨å„ç§æ¨¡å‹è§„æ¨¡ï¼ˆä»1Båˆ°7Bï¼‰ä¸‹å‡è¡¨ç°å‡ºç¨³å¥çš„ç»“æœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSF-LLaVA-1.5åœ¨é•¿è§†é¢‘ç†è§£æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ˆä¾‹å¦‚LongVideoBenchå’ŒMLVUï¼‰ï¼Œå¹¶åœ¨å„ç§è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨å°è§„æ¨¡æµ‹è¯•ä¸­è¡¨ç°å°¤ä¸ºçªå‡ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18943v2">PDF</a> Technical report</p>
<p><strong>Summary</strong></p>
<p>SF-LLaVA-1.5æ˜¯ä¸€ç§é«˜æ•ˆçš„è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œé‡‡ç”¨ä¸¤æµçš„SlowFastæœºåˆ¶è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨å…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œè”åˆè§†é¢‘å›¾åƒè®­ç»ƒã€‚è¯¥æ¨¡å‹å…³æ³¨é«˜æ•ˆæ¨¡å‹è§„æ¨¡ï¼Œå±•ç¤ºå³ä½¿åœ¨è¾ƒå°çš„è§†é¢‘LLMä¸­ä¹Ÿèƒ½å®ç°è§†é¢‘ç†è§£çš„å“è¶Šæ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSF-LLaVA-1.5åœ¨å„ç§è§†é¢‘å’Œå›¾åƒä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ¨¡å‹å¤§å°ä»ç™¾äº¿åˆ°ä¸ƒåäº¿ä¸ç­‰ï¼Œä¸”åœ¨é•¿è§†é¢‘ç†è§£æ–¹é¢è¾¾åˆ°ä¸šç•Œé¢†å…ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SF-LLaVA-1.5æ˜¯ä¸€ä¸ªé’ˆå¯¹é•¿å½¢å¼è§†é¢‘ç†è§£çš„é«˜æ•ˆè§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>é‡‡ç”¨ä¸¤æµçš„SlowFastæœºåˆ¶è¿›è¡Œè®­ç»ƒä»¥æé«˜æ¨¡å‹çš„æ•ˆç‡ã€‚</li>
<li>æ¨¡å‹åœ¨å…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œè”åˆè§†é¢‘å›¾åƒè®­ç»ƒï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹å…³æ³¨é«˜æ•ˆæ¨¡å‹è§„æ¨¡ï¼Œæ»¡è¶³ç§»åŠ¨è®¾å¤‡çš„æ€§èƒ½éœ€æ±‚ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºSF-LLaVA-1.5åœ¨å„ç§è§†é¢‘å’Œå›¾åƒä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šã€‚</li>
<li>SF-LLaVA-1.5åœ¨é•¿è§†é¢‘ç†è§£æ–¹é¢è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18943">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3f437c7b274db96bd67abb60034ac2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85da2a7be139f5e8ab48b701a4b7ccad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac8628e0afa40c6539ed32e41c52e319.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5050ab2a7b818501e85769024de54621.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="OVO-Bench-How-Far-is-Your-Video-LLMs-from-Real-World-Online-Video-Understanding"><a href="#OVO-Bench-How-Far-is-Your-Video-LLMs-from-Real-World-Online-Video-Understanding" class="headerlink" title="OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video   Understanding?"></a>OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video   Understanding?</h2><p><strong>Authors:Yifei Li, Junbo Niu, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, Xiaoyi Dong, Haodong Duan, Shuangrui Ding, Rui Qian, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang</strong></p>
<p>Temporal Awareness, the ability to reason dynamically based on the timestamp when a question is raised, is the key distinction between offline and online video LLMs. Unlike offline models, which rely on complete videos for static, post hoc analysis, online models process video streams incrementally and dynamically adapt their responses based on the timestamp at which the question is posed. Despite its significance, temporal awareness has not been adequately evaluated in existing benchmarks. To fill this gap, we present OVO-Bench (Online-VideO-Benchmark), a novel video benchmark that emphasizes the importance of timestamps for advanced online video understanding capability benchmarking. OVO-Bench evaluates the ability of video LLMs to reason and respond to events occurring at specific timestamps under three distinct scenarios: (1) Backward tracing: trace back to past events to answer the question. (2) Real-time understanding: understand and respond to events as they unfold at the current timestamp. (3) Forward active responding: delay the response until sufficient future information becomes available to answer the question accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videos and approximately human-curated 2,800 fine-grained meta-annotations with precise timestamps. We combine automated generation pipelines with human curation. With these high-quality samples, we further developed an evaluation pipeline to systematically query video LLMs along the video timeline. Evaluations of nine Video-LLMs reveal that, despite advancements on traditional benchmarks, current models struggle with online video understanding, showing a significant gap compared to human agents. We hope OVO-Bench will drive progress in video LLMs and inspire future research in online video reasoning. Our benchmark and code can be accessed at <a target="_blank" rel="noopener" href="https://github.com/JoeLeelyf/OVO-Bench">https://github.com/JoeLeelyf/OVO-Bench</a>. </p>
<blockquote>
<p>æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›æ˜¯åŒºåˆ†ç¦»çº¿ä¸åœ¨çº¿è§†é¢‘LLMçš„å…³é”®åŒºåˆ«ï¼Œå®ƒå¯ä»¥æ ¹æ®æå‡ºé—®é¢˜çš„æ—¶é—´æˆ³è¿›è¡ŒåŠ¨æ€æ¨ç†ã€‚ä¸ä¾èµ–å®Œæ•´è§†é¢‘è¿›è¡Œé™æ€äº‹ååˆ†æçš„ç¦»çº¿æ¨¡å‹ä¸åŒï¼Œåœ¨çº¿æ¨¡å‹ä¼šé€æ­¥å¤„ç†è§†é¢‘æµï¼Œå¹¶æ ¹æ®æå‡ºé—®é¢˜çš„æ—¶é—´æˆ³åŠ¨æ€è°ƒæ•´å…¶å“åº”ã€‚å°½ç®¡æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›éå¸¸é‡è¦ï¼Œä½†åœ¨ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸­å¹¶æœªå¾—åˆ°è¶³å¤Ÿè¯„ä¼°ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†OVO-Benchï¼ˆåœ¨çº¿è§†é¢‘åŸºå‡†æµ‹è¯•ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„è§†é¢‘åŸºå‡†æµ‹è¯•ï¼Œå®ƒå¼ºè°ƒæ—¶é—´æˆ³å¯¹äºé«˜çº§åœ¨çº¿è§†é¢‘èƒ½åŠ›è¯„ä¼°çš„é‡è¦æ€§ã€‚OVO-Benchè¯„ä¼°è§†é¢‘LLMåœ¨ä¸‰ç§ä¸åŒåœºæ™¯ä¸‹å¯¹å‘ç”Ÿåœ¨ç‰¹å®šæ—¶é—´æˆ³çš„äº‹ä»¶è¿›è¡Œæ¨ç†å’Œå“åº”çš„èƒ½åŠ›ï¼š(1)å‘åè¿½è¸ªï¼šè¿½æº¯è¿‡å»çš„äº‹ä»¶ä»¥å›ç­”é—®é¢˜ã€‚(2)å®æ—¶ç†è§£ï¼šç†è§£å¹¶å“åº”å½“å‰æ—¶é—´æˆ³ä¸Šå‘ç”Ÿçš„äº‹ä»¶ã€‚(3)å‘å‰ä¸»åŠ¨å“åº”ï¼šå»¶è¿Ÿå“åº”ï¼Œç›´åˆ°æ”¶é›†åˆ°è¶³å¤Ÿçš„æœªæ¥ä¿¡æ¯æ¥å‡†ç¡®å›ç­”é—®é¢˜ã€‚OVO-BenchåŒ…å«12é¡¹ä»»åŠ¡ï¼ŒåŒ…å«644ä¸ªç‹¬ç‰¹è§†é¢‘å’Œçº¦ç”±äººç±»ç²¾å¿ƒåˆ¶ä½œçš„2800ä¸ªç²¾ç»†å…ƒæ³¨é‡Šä»¥åŠç²¾ç¡®çš„æ—¶é—´æˆ³ã€‚æˆ‘ä»¬å°†è‡ªåŠ¨åŒ–ç”Ÿæˆç®¡é“ä¸äººç±»åˆ¶ä½œç›¸ç»“åˆã€‚é€šè¿‡è¿™äº›é«˜è´¨é‡çš„æ ·æœ¬ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ä¸ªè¯„ä¼°ç®¡é“ï¼Œä»¥ç³»ç»Ÿåœ°æŸ¥è¯¢è§†é¢‘LLMæ²¿è§†é¢‘æ—¶é—´çº¿çš„è¡¨ç°ã€‚å¯¹ä¹ä¸ªè§†é¢‘LLMçš„è¯„ä¼°è¡¨æ˜ï¼Œå°½ç®¡åœ¨ä¼ ç»ŸåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†è¿›å±•ï¼Œä½†å½“å‰æ¨¡å‹åœ¨åœ¨çº¿è§†é¢‘ç†è§£æ–¹é¢ä»ç„¶é¢ä¸´å›°éš¾ï¼Œä¸äººç±»ä»£ç†äººç›¸æ¯”å­˜åœ¨æ˜æ˜¾å·®è·ã€‚æˆ‘ä»¬å¸Œæœ›OVO-Benchèƒ½å¤Ÿä¿ƒè¿›è§†é¢‘LLMçš„è¿›æ­¥ï¼Œå¹¶æ¿€å‘æœªæ¥åœ¨çº¿è§†é¢‘æ¨ç†çš„ç ”ç©¶çµæ„Ÿã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JoeLeelyf/OVO-Bench">https://github.com/JoeLeelyf/OVO-Bench</a>è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05510v2">PDF</a> CVPR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨çº¿è§†é¢‘ç†è§£çš„å…³é”®æ˜¯æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›ï¼Œå³åŸºäºé—®é¢˜æå‡ºçš„æ—¶é—´æˆ³è¿›è¡ŒåŠ¨æ€æ¨ç†çš„èƒ½åŠ›ã€‚ä¸ä¾èµ–å®Œæ•´è§†é¢‘è¿›è¡Œé™æ€ã€äº‹ååˆ†æçš„ç¦»çº¿æ¨¡å‹ä¸åŒï¼Œåœ¨çº¿æ¨¡å‹èƒ½å¤Ÿå¢é‡å¤„ç†è§†é¢‘æµå¹¶æ ¹æ®é—®é¢˜çš„æ—¶é—´æˆ³åŠ¨æ€è°ƒæ•´å…¶å“åº”ã€‚å°½ç®¡æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›éå¸¸é‡è¦ï¼Œä½†åœ¨ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸­å¹¶æœªå¾—åˆ°å……åˆ†è¯„ä¼°ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†OVO-Benchï¼ˆåœ¨çº¿è§†é¢‘åŸºå‡†æµ‹è¯•ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è§†æ—¶é—´æˆ³é‡è¦æ€§çš„æ–°å‹è§†é¢‘åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°è§†é¢‘LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰çš„é«˜çº§åœ¨çº¿è§†é¢‘ç†è§£èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚OVO-Benchè¯„ä¼°è§†é¢‘LLMåœ¨ä¸‰ä¸ªä¸åŒåœºæ™¯ä¸‹çš„ç‰¹å®šæ—¶é—´æˆ³ä¸Šæ¨ç†å’Œå›åº”äº‹ä»¶çš„èƒ½åŠ›ï¼šï¼ˆ1ï¼‰å‘åè¿½è¸ªï¼šè¿½æº¯è¿‡å»çš„äº‹ä»¶æ¥å›ç­”é—®é¢˜ï¼›ï¼ˆ2ï¼‰å®æ—¶ç†è§£ï¼šç†è§£å¹¶å“åº”å½“å‰æ—¶é—´æˆ³ä¸Šå‘ç”Ÿçš„äº‹ä»¶ï¼›ï¼ˆ3ï¼‰å‘å‰ä¸»åŠ¨å“åº”ï¼šå»¶è¿Ÿå“åº”ï¼Œç›´åˆ°æœªæ¥æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å‡†ç¡®å›ç­”é—®é¢˜ã€‚OVO-BenchåŒ…å«12é¡¹ä»»åŠ¡ï¼Œç”±644ä¸ªç‹¬ç‰¹è§†é¢‘å’Œçº¦ç»äººå·¥ç²¾ç»†æ ‡æ³¨çš„2800ä¸ªç²¾ç¡®æ—¶é—´æˆ³ç»„æˆã€‚æˆ‘ä»¬ç»“åˆè‡ªåŠ¨åŒ–ç”Ÿæˆç®¡é“å’Œäººå·¥å®¡æ ¸ï¼Œé€šè¿‡è¿™äº›é«˜è´¨é‡æ ·æœ¬è¿›ä¸€æ­¥å¼€å‘äº†ä¸€ä¸ªè¯„ä¼°ç®¡é“ï¼Œä»¥ç³»ç»Ÿåœ°æŸ¥è¯¢è§†é¢‘LLMæ²¿è§†é¢‘æ—¶é—´çº¿çš„è¡¨ç°ã€‚å¯¹ä¹ä¸ªè§†é¢‘LLMçš„è¯„ä¼°è¡¨æ˜ï¼Œå°½ç®¡åœ¨ä¼ ç»ŸåŸºå‡†æµ‹è¯•ä¸Šæœ‰æ‰€è¿›å±•ï¼Œä½†å½“å‰æ¨¡å‹åœ¨çº¿è§†é¢‘ç†è§£æ–¹é¢ä»å­˜åœ¨å·®è·ï¼Œä¸äººç±»ä»£ç†äººç›¸æ¯”å­˜åœ¨è¾ƒå¤§å·®è·ã€‚æˆ‘ä»¬å¸Œæœ›OVO-Benchèƒ½å¤Ÿä¿ƒè¿›è§†é¢‘LLMçš„è¿›æ­¥ï¼Œå¹¶æ¿€å‘æœªæ¥åœ¨çº¿è§†é¢‘æ¨ç†çš„ç ”ç©¶çµæ„Ÿã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œä»£ç å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/JoeLeelyf/OVO-Bench%E3%80%82">https://github.com/JoeLeelyf/OVO-Benchã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åœ¨çº¿è§†é¢‘ç†è§£çš„å…³é”®æ˜¯æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›ï¼Œå³æ ¹æ®é—®é¢˜æå‡ºçš„æ—¶é—´æˆ³è¿›è¡ŒåŠ¨æ€æ¨ç†çš„èƒ½åŠ›ã€‚</li>
<li>OVO-Benchæ˜¯é¦–ä¸ªé‡è§†æ—¶é—´æ„ŸçŸ¥çš„è§†é¢‘åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°è§†é¢‘LLMçš„åœ¨çº¿è§†é¢‘ç†è§£èƒ½åŠ›ã€‚</li>
<li>OVO-BenchåŒ…å«å¤šç§ä»»åŠ¡åœºæ™¯ï¼ŒåŒ…æ‹¬å‘åè¿½è¸ªã€å®æ—¶ç†è§£å’Œå‘å‰ä¸»åŠ¨å“åº”ç­‰ã€‚</li>
<li>å½“å‰è§†é¢‘LLMæ¨¡å‹åœ¨çº¿è§†é¢‘ç†è§£æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œä¸äººç±»è¡¨ç°æœ‰è¾ƒå¤§å·®è·ã€‚</li>
<li>OVO-Benchæä¾›äº†é«˜è´¨é‡çš„è§†é¢‘æ ·æœ¬å’Œç²¾ç»†æ ‡æ³¨ï¼Œæœ‰åŠ©äºä¿ƒè¿›è§†é¢‘LLMçš„ç ”ç©¶è¿›å±•ã€‚</li>
<li>é€šè¿‡OVO-Benchçš„è¯„ä¼°ç»“æœï¼Œå‘ç°ç°æœ‰è§†é¢‘LLMåœ¨åº”å¯¹æœªæ¥ä¿¡æ¯çš„éœ€æ±‚æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05510">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8d4f7c0c310402b52381598edbc4ba17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-515709833fe3448488c84c965eabae59.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6b2f931e94b94da6c54be25993bb758a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LongViTU-Instruction-Tuning-for-Long-Form-Video-Understanding"><a href="#LongViTU-Instruction-Tuning-for-Long-Form-Video-Understanding" class="headerlink" title="LongViTU: Instruction Tuning for Long-Form Video Understanding"></a>LongViTU: Instruction Tuning for Long-Form Video Understanding</h2><p><strong>Authors:Rujie Wu, Xiaojian Ma, Hai Ci, Yue Fan, Yuxuan Wang, Haozhe Zhao, Qing Li, Yizhou Wang</strong></p>
<p>This paper introduces LongViTU, a large-scale (~121k QA pairs, ~900h videos), automatically generated dataset for long-form video understanding. We propose a systematic approach that organizes videos into a hierarchical tree structure for QA generation and incorporates self-revision mechanisms to ensure high-quality QA pairs. Each QA pair in LongViTU features: 1) long-term context (average certificate length of 4.6 minutes); 2) rich knowledge and condensed reasoning (commonsense, causality, planning, etc.)). We also offer explicit timestamp annotations of relevant events for each QA pair. We have conducted extensive human studies on LongViTU, and the results prove the quality of our dataset. To better evaluate the challenges posed by LongViTUâ€™s emphasis on long-term context and condensed reasoning, we manually curate a subset of LongViTU into a benchmark. Evaluations using a state-of-the-art open-source model (LongVU), a proprietary model (Gemini-1.5-Pro), and human annotators yield GPT-4 scores of 49.9, 52.3, and 81.0, respectively, underscoring the substantial difficulty presented by LongViTU questions. Performing supervised fine-tuning (SFT) of LongVU and LLaVA-Video on LongViTU data results in average performance gains of 2.5% and 3.7%, respectively, across a suite of long video understanding benchmarks (EgoSchema, VideoMME-Long, MLVU, LVBench). </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†LongViTUï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ï¼ˆ<del>12.1ä¸‡å¯¹é—®ç­”å¯¹ï¼Œ</del>900å°æ—¶è§†é¢‘ï¼‰çš„è‡ªåŠ¨ç”Ÿæˆçš„ç”¨äºé•¿æ ¼å¼è§†é¢‘ç†è§£çš„æ•°æ®é›†ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„æ–¹æ³•ï¼Œå°†è§†é¢‘ç»„ç»‡æˆå±‚æ¬¡æ ‘ç»“æ„æ¥è¿›è¡Œé—®ç­”ç”Ÿæˆï¼Œå¹¶å¼•å…¥äº†è‡ªæˆ‘ä¿®æ­£æœºåˆ¶ä»¥ç¡®ä¿é«˜è´¨é‡çš„é—®ç­”å¯¹ã€‚LongViTUä¸­çš„æ¯ä¸ªé—®ç­”å¯¹å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š1ï¼‰é•¿æœŸä¸Šä¸‹æ–‡ï¼ˆå¹³å‡è¯ä¹¦é•¿åº¦ä¸º4.6åˆ†é’Ÿï¼‰ï¼›2ï¼‰ä¸°å¯Œçš„çŸ¥è¯†å’Œç²¾ç‚¼çš„æ¨ç†ï¼ˆå¸¸è¯†ã€å› æœã€è§„åˆ’ç­‰ï¼‰ã€‚æˆ‘ä»¬è¿˜ä¸ºæ¯ä¸ªé—®ç­”å¯¹æä¾›äº†ç›¸å…³äº‹ä»¶çš„æ—¶é—´æˆ³æ³¨é‡Šã€‚æˆ‘ä»¬å¯¹LongViTUè¿›è¡Œäº†å¹¿æ³›çš„äººç±»ç ”ç©¶ï¼Œç»“æœè¯æ˜äº†æ•°æ®é›†çš„è´¨é‡ã€‚ä¸ºäº†æ›´å¥½åœ°è¯„ä¼°LongViTUå¯¹é•¿æœŸä¸Šä¸‹æ–‡å’Œç²¾ç‚¼æ¨ç†çš„é‡è§†æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä»LongViTUä¸­æ‰‹åŠ¨ç­›é€‰å‡ºä¸€ä¸ªåŸºå‡†æ•°æ®é›†ã€‚ä½¿ç”¨æœ€å…ˆè¿›çš„å¼€æºæ¨¡å‹ï¼ˆLongVUï¼‰ã€ä¸“æœ‰æ¨¡å‹ï¼ˆGemini-1.5-Proï¼‰å’Œäººç±»æ³¨é‡Šè€…è¿›è¡Œè¯„ä¼°ï¼Œåˆ†åˆ«å¾—åˆ°GPT-4çš„åˆ†æ•°ä¸º49.9ã€52.3å’Œ81.0ï¼Œè¿™å‡¸æ˜¾äº†LongViTUé—®é¢˜æ‰€æ„æˆçš„é‡å¤§æŒ‘æˆ˜ã€‚å¯¹LongVUå’ŒLLaVA-Videoè¿›è¡ŒLongViTUæ•°æ®çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åï¼Œåœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ï¼ˆEgoSchemaã€VideoMME-Longã€MLVUã€LVBenchï¼‰ä¸Šçš„æ€§èƒ½å¹³å‡æé«˜äº†2.5%å’Œ3.7%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05037v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†LongViTUæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡è‡ªåŠ¨ç”Ÿæˆçš„ç”¨äºé•¿è§†é¢‘ç†è§£çš„æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†é‡‡ç”¨ç³»ç»Ÿæ€§æ–¹æ³•ç»„ç»‡è§†é¢‘ä¸ºå±‚æ¬¡æ ‘ç»“æ„è¿›è¡Œé—®ç­”ç”Ÿæˆï¼Œå¹¶å¼•å…¥è‡ªæˆ‘ä¿®æ­£æœºåˆ¶ç¡®ä¿é«˜è´¨é‡çš„é—®ç­”å¯¹ã€‚LongViTUä¸­çš„æ¯ä¸ªé—®ç­”å¯¹éƒ½å…·æœ‰é•¿æœŸä¸Šä¸‹æ–‡ã€ä¸°å¯ŒçŸ¥è¯†å’Œç²¾ç‚¼æ¨ç†çš„ç‰¹ç‚¹ã€‚æ­¤å¤–ï¼Œè¿˜ä¸ºæ¯ä¸ªé—®ç­”å¯¹æä¾›äº†ç›¸å…³äº‹ä»¶çš„æ—¶é—´æˆ³æ³¨é‡Šã€‚é€šè¿‡å¹¿æ³›çš„äººç±»ç ”ç©¶éªŒè¯äº†æ•°æ®é›†çš„è´¨é‡ã€‚ä¸ºäº†è¯„ä¼°LongViTUå¯¹é•¿æœŸä¸Šä¸‹æ–‡å’Œç²¾ç‚¼æ¨ç†çš„é‡è§†æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä»LongViTUä¸­æ‰‹åŠ¨ç­›é€‰å‡ºä¸€ä¸ªåŸºå‡†æµ‹è¯•é›†ã€‚ä½¿ç”¨å…ˆè¿›å¼€æºæ¨¡å‹LongVUã€ä¸“æœ‰æ¨¡å‹Gemini-1.5-Proå’Œäººç±»è¯„ä¼°è€…çš„è¯„ä¼°ç»“æœï¼Œè¡¨æ˜LongViTUé—®é¢˜çš„å®è´¨æ€§éš¾åº¦ã€‚é€šè¿‡å¯¹LongVUå’ŒLLaVA-Videoè¿›è¡ŒLongViTUæ•°æ®çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œåœ¨ä¸€ç³»åˆ—é•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ï¼ˆEgoSchemaã€VideoMME-Longã€MLVUã€LVBenchï¼‰ä¸Šçš„å¹³å‡æ€§èƒ½åˆ†åˆ«æé«˜äº†2.5%å’Œ3.7%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LongViTUæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡è‡ªåŠ¨ç”Ÿæˆçš„ç”¨äºé•¿è§†é¢‘ç†è§£çš„æ•°æ®é›†ã€‚</li>
<li>é‡‡ç”¨ç³»ç»Ÿæ€§æ–¹æ³•ç»„ç»‡è§†é¢‘ä¸ºå±‚æ¬¡æ ‘ç»“æ„è¿›è¡Œé—®ç­”ç”Ÿæˆï¼Œç¡®ä¿é«˜è´¨é‡é—®ç­”å¯¹ã€‚</li>
<li>æ¯ä¸ªé—®ç­”å¯¹å…·æœ‰é•¿æœŸä¸Šä¸‹æ–‡ã€ä¸°å¯ŒçŸ¥è¯†å’Œç²¾ç‚¼æ¨ç†çš„ç‰¹ç‚¹ã€‚</li>
<li>æ•°æ®é›†æä¾›æ¯ä¸ªé—®ç­”å¯¹çš„ç›¸å…³äº‹ä»¶æ—¶é—´æˆ³æ³¨é‡Šã€‚</li>
<li>é€šè¿‡å¹¿æ³›çš„äººç±»ç ”ç©¶éªŒè¯äº†æ•°æ®é›†è´¨é‡ã€‚</li>
<li>LongViTUé—®é¢˜çš„å®è´¨éš¾åº¦å¾—åˆ°äº†è¯„ä¼°ï¼Œå…ˆè¿›æ¨¡å‹ä¸äººç±»çš„æ€§èƒ½è¡¨ç°å­˜åœ¨å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05037">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96c450ffc8c91688615f5250909516e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5bb4c4e24540f347c1e82bb98919eff1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9f94432f162f57caf3bd0075a66569f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb4b68920225285d2c7dfdd9ae84f57a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ddea5facaee8dd23263520cc24205af8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Video-3D-LLM-Learning-Position-Aware-Video-Representation-for-3D-Scene-Understanding"><a href="#Video-3D-LLM-Learning-Position-Aware-Video-Representation-for-3D-Scene-Understanding" class="headerlink" title="Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene   Understanding"></a>Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene   Understanding</h2><p><strong>Authors:Duo Zheng, Shijia Huang, Liwei Wang</strong></p>
<p>The rapid advancement of Multimodal Large Language Models (MLLMs) has significantly impacted various multimodal tasks. However, these models face challenges in tasks that require spatial understanding within 3D environments. Efforts to enhance MLLMs, such as incorporating point cloud features, have been made, yet a considerable gap remains between the modelsâ€™ learned representations and the inherent complexity of 3D scenes. This discrepancy largely stems from the training of MLLMs on predominantly 2D data, which restricts their effectiveness in comprehending 3D spaces. To address this issue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM, for 3D scene understanding. By treating 3D scenes as dynamic videos and incorporating 3D position encoding into these representations, our Video-3D LLM aligns video representations with real-world spatial contexts more accurately. In addition, we have implemented a maximum coverage sampling technique to optimize the trade-off between computational cost and performance. Extensive experiments demonstrate that our model achieves state-of-the-art performance on several 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D. </p>
<blockquote>
<p>éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå®ƒå¯¹å„ç§å¤šæ¨¡æ€ä»»åŠ¡äº§ç”Ÿäº†é‡å¤§å½±å“ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å¤„ç†éœ€è¦åœ¨ä¸‰ç»´ç¯å¢ƒä¸­è¿›è¡Œç©ºé—´ç†è§£çš„ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†å¢å¼ºMLLMsçš„åŠŸèƒ½ï¼Œå·²ç»åšå‡ºäº†åŠªåŠ›ï¼Œå¦‚èå…¥ç‚¹äº‘ç‰¹å¾ç­‰ï¼Œä½†æ¨¡å‹å­¦åˆ°çš„è¡¨ç¤ºä¸ä¸‰ç»´åœºæ™¯å›ºæœ‰å¤æ‚æ€§ä¹‹é—´ä»å­˜åœ¨è¾ƒå¤§å·®è·ã€‚è¿™ç§å·®å¼‚ä¸»è¦æºäºMLLMsä¸»è¦åœ¨äºŒç»´æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç†è§£ä¸‰ç»´ç©ºé—´æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹é€šç”¨æ¨¡å‹ï¼Œå³Video-3D LLMï¼Œç”¨äºä¸‰ç»´åœºæ™¯ç†è§£ã€‚é€šè¿‡å°†ä¸‰ç»´åœºæ™¯è§†ä¸ºåŠ¨æ€è§†é¢‘å¹¶å°†ä¸‰ç»´ä½ç½®ç¼–ç èå…¥è¿™äº›è¡¨ç¤ºä¸­ï¼Œæˆ‘ä»¬çš„Video-3D LLMèƒ½å¤Ÿæ›´å‡†ç¡®åœ°ä½¿è§†é¢‘è¡¨ç¤ºä¸çœŸå®ä¸–ç•Œç©ºé—´ä¸Šä¸‹æ–‡å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ç§æœ€å¤§è¦†ç›–é‡‡æ ·æŠ€æœ¯ï¼Œä»¥ä¼˜åŒ–è®¡ç®—æˆæœ¬ä¸æ€§èƒ½ä¹‹é—´çš„å¹³è¡¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªä¸‰ç»´åœºæ™¯ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ScanReferã€Multi3DReferã€Scan2Capã€ScanQAå’ŒSQA3Dã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00493v2">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong><br>å¤šä»»åŠ¡æ¨¡æ€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿè¿›æ­¥å¯¹å¤šç§æ¨¡æ€çš„ä»»åŠ¡äº§ç”Ÿäº†é‡å¤§å½±å“ï¼Œä½†åœ¨éœ€è¦ç†è§£ä¸‰ç»´ç¯å¢ƒç©ºé—´çš„ä»»åŠ¡ä¸­ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é€šç”¨æ¨¡å‹â€”â€”Video-3D LLMï¼Œç”¨äºä¸‰ç»´åœºæ™¯ç†è§£ã€‚é€šè¿‡æŠŠä¸‰ç»´åœºæ™¯å½“ä½œåŠ¨æ€è§†é¢‘å¹¶èå…¥ä¸‰ç»´ä½ç½®ç¼–ç ï¼ŒVideo-3D LLMèƒ½æ›´å‡†ç¡®åœ°åŒ¹é…è§†é¢‘è¡¨ç°å’ŒçœŸå®ä¸–ç•Œç©ºé—´èƒŒæ™¯ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å®æ–½äº†ä¸€ç§æœ€å¤§è¦†ç›–é‡‡æ ·æŠ€æœ¯ï¼Œä»¥ä¼˜åŒ–è®¡ç®—æˆæœ¬å’Œæ€§èƒ½ä¹‹é—´çš„å¹³è¡¡ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªä¸‰ç»´åœºæ™¯ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†å¤šç§æ¨¡æ€ä»»åŠ¡çš„è¿›å±•ã€‚</li>
<li>MLLMsåœ¨å¤„ç†éœ€è¦ä¸‰ç»´ç©ºé—´ç†è§£çš„ä»»åŠ¡æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ¨¡å‹ä¸»è¦åŸºäºäºŒç»´æ•°æ®çš„è®­ç»ƒï¼Œé™åˆ¶äº†å…¶åœ¨ä¸‰ç»´ç©ºé—´ç†è§£ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„é€šç”¨æ¨¡å‹â€”â€”Video-3D LLMç”¨äºä¸‰ç»´åœºæ™¯ç†è§£ã€‚</li>
<li>Video-3D LLMé€šè¿‡å°†ä¸‰ç»´åœºæ™¯è§†ä¸ºåŠ¨æ€è§†é¢‘å¹¶èå…¥ä¸‰ç»´ä½ç½®ç¼–ç ï¼Œæ›´å‡†ç¡®åœ°åŒ¹é…è§†é¢‘è¡¨ç°å’ŒçœŸå®ä¸–ç•Œç©ºé—´èƒŒæ™¯ã€‚</li>
<li>Video-3D LLMé‡‡ç”¨äº†æœ€å¤§è¦†ç›–é‡‡æ ·æŠ€æœ¯æ¥å¹³è¡¡è®¡ç®—æˆæœ¬å’Œæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00493">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5549b56d5bced2d0142d116c8e7ad12c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-896c941d18251cc0ca3eac34edbed3fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abb980361656247b3f5521a5b2060717.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-29/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-29/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-29/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1f1f842d6870075589b69c1fa51223da.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-29  Test-Time Visual In-Context Tuning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-29/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-149af2b24c3ff7f7e8e6c651c07b3699.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-29  MotionDiff Training-free Zero-shot Interactive Motion Editing via   Flow-assisted Multi-view Diffusion
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32127.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
