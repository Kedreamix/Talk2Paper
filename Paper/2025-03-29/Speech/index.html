<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-29  COMI-LINGUA Expert Annotated Large-Scale Dataset for Multitask NLP in   Hindi-English Code-Mixing">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-228c51afc32494b560f0472c73ed6167.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    55 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-29-æ›´æ–°"><a href="#2025-03-29-æ›´æ–°" class="headerlink" title="2025-03-29 æ›´æ–°"></a>2025-03-29 æ›´æ–°</h1><h2 id="COMI-LINGUA-Expert-Annotated-Large-Scale-Dataset-for-Multitask-NLP-in-Hindi-English-Code-Mixing"><a href="#COMI-LINGUA-Expert-Annotated-Large-Scale-Dataset-for-Multitask-NLP-in-Hindi-English-Code-Mixing" class="headerlink" title="COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in   Hindi-English Code-Mixing"></a>COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in   Hindi-English Code-Mixing</h2><p><strong>Authors:Rajvee Sheth, Himanshu Beniwal, Mayank Singh</strong></p>
<p>The rapid growth of digital communication has driven the widespread use of code-mixing, particularly Hindi-English, in multilingual communities. Existing datasets often focus on romanized text, have limited scope, or rely on synthetic data, which fails to capture realworld language nuances. Human annotations are crucial for assessing the naturalness and acceptability of code-mixed text. To address these challenges, We introduce COMI-LINGUA, the largest manually annotated dataset for code-mixed text, comprising 100,970 instances evaluated by three expert annotators in both Devanagari and Roman scripts. The dataset supports five fundamental NLP tasks: Language Identification, Matrix Language Identification, Part-of-Speech Tagging, Named Entity Recognition, and Translation. We evaluate LLMs on these tasks using COMILINGUA, revealing limitations in current multilingual modeling strategies and emphasizing the need for improved code-mixed text processing capabilities. COMI-LINGUA is publically availabe at: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA">https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA</a>. </p>
<blockquote>
<p>æ•°å­—é€šä¿¡çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†è·¨è¯­è¨€ç¤¾åŒºä¸­æ··åˆä»£ç ï¼Œç‰¹åˆ«æ˜¯å°åº¦æ··åˆè‹±è¯­çš„å¹¿æ³›ä½¿ç”¨ã€‚ç°æœ‰çš„æ•°æ®é›†å¾€å¾€ä¾§é‡äºç½—é©¬åŒ–æ–‡æœ¬ï¼ŒèŒƒå›´æœ‰é™ï¼Œæˆ–è€…ä¾èµ–äºåˆæˆæ•°æ®ï¼Œæ— æ³•æ•æ‰ç°å®ä¸–ç•Œçš„è¯­è¨€ç»†å¾®å·®åˆ«ã€‚äººç±»æ³¨é‡Šå¯¹äºè¯„ä¼°æ··åˆä»£ç çš„è‡ªç„¶æ€§å’Œå¯æ¥å—æ€§è‡³å…³é‡è¦ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†COMI-LINGUAï¼Œè¿™æ˜¯æ··åˆä»£ç æœ€å¤§çš„æ‰‹åŠ¨æ³¨é‡Šæ•°æ®é›†ï¼ŒåŒ…å«ç”±Devanagariå’Œç½—é©¬è„šæœ¬ä¸­çš„ä¸‰åä¸“ä¸šæ³¨é‡Šè€…è¯„ä¼°çš„100970ä¸ªå®ä¾‹ã€‚è¯¥æ•°æ®é›†æ”¯æŒäº”é¡¹åŸºæœ¬NLPä»»åŠ¡ï¼šè¯­è¨€è¯†åˆ«ã€çŸ©é˜µè¯­è¨€è¯†åˆ«ã€è¯æ€§æ ‡æ³¨ã€å‘½åå®ä½“è¯†åˆ«å’Œç¿»è¯‘ã€‚æˆ‘ä»¬ä½¿ç”¨COMI-LINGUAå¯¹è¿™äº›ä»»åŠ¡è¿›è¡Œè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ­ç¤ºäº†å½“å‰å¤šè¯­è¨€å»ºæ¨¡ç­–ç•¥çš„å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†æé«˜æ··åˆæ–‡æœ¬å¤„ç†èƒ½åŠ›çš„å¿…è¦æ€§ã€‚COMI-LINGUAå…¬å¼€å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA%E3%80%82">https://huggingface.co/datasets/LingoIITGN/COMI-LINGUAã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21670v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šéšç€æ•°å­—é€šä¿¡çš„å¿«é€Ÿå‘å±•ï¼Œå¤šè¯­è¨€ç¤¾åŒºä¸­æ··åˆä»£ç çš„ä½¿ç”¨è¶Šæ¥è¶Šæ™®éï¼Œå°¤å…¶æ˜¯å°åº¦è¯­å’Œè‹±è¯­çš„æ··åˆã€‚ç°æœ‰çš„æ•°æ®é›†ä¸»è¦å…³æ³¨ç½—é©¬åŒ–æ–‡æœ¬ï¼Œå­˜åœ¨èŒƒå›´æœ‰é™æˆ–ä¾èµ–åˆæˆæ•°æ®çš„é—®é¢˜ï¼Œæ— æ³•æ•æ‰çœŸå®ä¸–ç•Œçš„è¯­è¨€ç»†å¾®å·®åˆ«ã€‚ä¸ºäº†è¯„ä¼°æ··åˆæ–‡æœ¬çš„è‡ªç„¶æ€§å’Œå¯æ¥å—æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†COMI-LINGUAæ•°æ®é›†ï¼Œè¿™æ˜¯æœ€å¤§çš„æ‰‹åŠ¨æ³¨é‡Šçš„æ··åˆæ–‡æœ¬æ•°æ®é›†ï¼ŒåŒ…å«ç”±ä¸‰ä½ä¸“å®¶æ³¨é‡Šè€…åœ¨Devanagariå’ŒRomanè„šæœ¬ä¸­è¯„ä¼°çš„10ä¸‡å¤šä¸ªå®ä¾‹ã€‚è¯¥æ•°æ®é›†æ”¯æŒäº”é¡¹åŸºæœ¬NLPä»»åŠ¡ï¼ŒåŒ…æ‹¬è¯­è¨€è¯†åˆ«ã€çŸ©é˜µè¯­è¨€è¯†åˆ«ã€è¯æ€§æ ‡æ³¨ã€å‘½åå®ä½“è¯†åˆ«å’Œç¿»è¯‘ã€‚æˆ‘ä»¬åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ­ç¤ºäº†å½“å‰å¤šè¯­è¨€å»ºæ¨¡ç­–ç•¥çš„å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†æ”¹è¿›æ··åˆæ–‡æœ¬å¤„ç†èƒ½åŠ›çš„å¿…è¦æ€§ã€‚COMI-LINGUAæ•°æ®é›†å¯åœ¨huggingface.co&#x2F;datasets&#x2F;LingoIITGN&#x2F;COMI-LINGUAè·å–ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ•°å­—é€šä¿¡çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†å¤šè¯­è¨€ç¤¾åŒºä¸­æ··åˆä»£ç ï¼ˆå°¤å…¶æ˜¯å°åº¦è¯­å’Œè‹±è¯­ï¼‰çš„å¹¿æ³›ä½¿ç”¨ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†å­˜åœ¨å±€é™æ€§ï¼Œä¸»è¦å…³æ³¨ç½—é©¬åŒ–æ–‡æœ¬æˆ–ä¾èµ–åˆæˆæ•°æ®ï¼Œæ— æ³•æ•æ‰çœŸå®è¯­è¨€çš„ç»†å¾®å·®åˆ«ã€‚</li>
<li>äººç±»æ³¨é‡Šå¯¹äºè¯„ä¼°æ··åˆæ–‡æœ¬çš„è‡ªç„¶æ€§å’Œå¯æ¥å—æ€§è‡³å…³é‡è¦ã€‚</li>
<li>å¼•å…¥äº†COMI-LINGUAæ•°æ®é›†ï¼Œè¿™æ˜¯æœ€å¤§çš„æ‰‹åŠ¨æ³¨é‡Šçš„æ··åˆæ–‡æœ¬æ•°æ®é›†ã€‚</li>
<li>è¯¥æ•°æ®é›†åŒ…å«äº”é¡¹åŸºæœ¬NLPä»»åŠ¡ï¼šè¯­è¨€è¯†åˆ«ã€çŸ©é˜µè¯­è¨€è¯†åˆ«ã€è¯æ€§æ ‡æ³¨ã€å‘½åå®ä½“è¯†åˆ«å’Œç¿»è¯‘ã€‚</li>
<li>åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œæ­ç¤ºäº†å½“å‰å¤šè¯­è¨€å»ºæ¨¡ç­–ç•¥çš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21670">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a22753c70e23c407b69349747a309b24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-239e2704d1291dad2b651ef75ac8ce76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4617835b0d5a6e5eb509441e09e1ef5a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-41681cec68525e941da3e105173c9457.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Audio-driven-Gesture-Generation-via-Deviation-Feature-in-the-Latent-Space"><a href="#Audio-driven-Gesture-Generation-via-Deviation-Feature-in-the-Latent-Space" class="headerlink" title="Audio-driven Gesture Generation via Deviation Feature in the Latent   Space"></a>Audio-driven Gesture Generation via Deviation Feature in the Latent   Space</h2><p><strong>Authors:Jiahui Chen, Yang Huan, Runhua Shi, Chanfan Ding, Xiaoqi Mo, Siyu Xiong, Yinong He</strong></p>
<p>Gestures are essential for enhancing co-speech communication, offering visual emphasis and complementing verbal interactions. While prior work has concentrated on point-level motion or fully supervised data-driven methods, we focus on co-speech gestures, advocating for weakly supervised learning and pixel-level motion deviations. We introduce a weakly supervised framework that learns latent representation deviations, tailored for co-speech gesture video generation. Our approach employs a diffusion model to integrate latent motion features, enabling more precise and nuanced gesture representation. By leveraging weakly supervised deviations in latent space, we effectively generate hand gestures and mouth movements, crucial for realistic video production. Experiments show our method significantly improves video quality, surpassing current state-of-the-art techniques. </p>
<blockquote>
<p>æ‰‹åŠ¿å¯¹äºå¢å¼ºå…±è¯­æ²Ÿé€šã€æä¾›è§†è§‰é‡ç‚¹å’Œè¡¥å……è¨€è¯­äº¤æµè‡³å…³é‡è¦ã€‚è™½ç„¶æ—©æœŸçš„ç ”ç©¶é›†ä¸­åœ¨ç‚¹çŠ¶è¿åŠ¨æˆ–å®Œå…¨åŸºäºæ•°æ®çš„ç›‘ç£æ–¹æ³•ä¸Šï¼Œä½†æˆ‘ä»¬ä¸“æ³¨äºå…±è¯­æ‰‹åŠ¿ï¼Œæå€¡å¼±ç›‘ç£å­¦ä¹ å’Œåƒç´ çº§è¿åŠ¨åå·®ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¼±ç›‘ç£æ¡†æ¶æ¥å­¦ä¹ æ½œåœ¨è¡¨ç¤ºçš„åå·®ï¼Œä¸“é—¨ç”¨äºå…±è¯­æ‰‹åŠ¿è§†é¢‘çš„ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨æ‰©æ•£æ¨¡å‹æ¥æ•´åˆæ½œåœ¨çš„è¿åŠ¨ç‰¹å¾ï¼Œä½¿æ‰‹åŠ¿è¡¨è¾¾æ›´åŠ ç²¾ç¡®å’Œç»†è‡´ã€‚é€šè¿‡åˆ©ç”¨æ½œåœ¨ç©ºé—´ä¸­çš„å¼±ç›‘ç£åå·®ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰æ•ˆåœ°ç”Ÿæˆå¯¹çœŸå®è§†é¢‘åˆ¶ä½œè‡³å…³é‡è¦çš„æ‰‹åŠ¿å’Œå£å‹è¿åŠ¨ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†è§†é¢‘è´¨é‡ï¼Œè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21616v1">PDF</a> 6 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒå…±è¯­æ‰‹åŠ¿çš„é‡è¦æ€§ï¼Œæå‡ºä¸€ç§é’ˆå¯¹å…±è¯­æ‰‹åŠ¿è§†é¢‘çš„å¼±ç›‘ç£å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ‰©æ•£æ¨¡å‹æ•´åˆæ½œåœ¨è¿åŠ¨ç‰¹å¾ï¼Œå­¦ä¹ æ½œåœ¨è¡¨ç¤ºåå·®ï¼Œä»¥ç”Ÿæˆæ›´ç²¾ç¡®å’Œç»†å¾®çš„æ‰‹åŠ¿è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆç”Ÿæˆæ‰‹åŠ¿å’Œå£å‹åŠ¨ä½œï¼Œæ˜¾è‘—æé«˜äº†è§†é¢‘è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…±è¯­æ‰‹åŠ¿åœ¨å¢å¼ºæ²Ÿé€šã€è§†è§‰å¼ºè°ƒå’Œè¡¥å……è¨€è¯­äº¤äº’æ–¹é¢è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰å·¥ä½œä¸»è¦é›†ä¸­åœ¨ç‚¹çŠ¶è¿åŠ¨æˆ–å…¨ç›‘ç£æ•°æ®é©±åŠ¨çš„æ–¹æ³•ä¸Šï¼Œè€Œæœ¬æ–‡ä¸“æ³¨äºå…±è¯­æ‰‹åŠ¿ã€‚</li>
<li>æå€¡å¼±ç›‘ç£å­¦ä¹ æ–¹æ³•å’Œåƒç´ çº§è¿åŠ¨åå·®ã€‚</li>
<li>å¼•å…¥ä¸€ç§å¼±ç›‘ç£æ¡†æ¶ï¼Œç”¨äºå­¦ä¹ é’ˆå¯¹å…±è¯­æ‰‹åŠ¿è§†é¢‘çš„æ½œåœ¨è¡¨ç¤ºåå·®ã€‚</li>
<li>ä½¿ç”¨æ‰©æ•£æ¨¡å‹æ•´åˆæ½œåœ¨è¿åŠ¨ç‰¹å¾ï¼Œå®ç°æ›´ç²¾ç¡®å’Œç»†å¾®çš„æ‰‹åŠ¿è¡¨ç¤ºã€‚</li>
<li>åˆ©ç”¨å¼±ç›‘ç£ä¸‹çš„æ½œåœ¨ç©ºé—´åå·®æœ‰æ•ˆç”Ÿæˆæ‰‹åŠ¿å’Œå£å‹åŠ¨ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21616">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7138059a063342f592d75c6a9945c15b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e7115f2323a1e7d0f0297174dd40f65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d4fbe00df14c2e1d8ae7f82ff6831fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2668786f39d724ebed68e6da2cb4613f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84fc6360676a42906040df9b4ab89780.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-376cfee76908c62a587e55d9c2d8d693.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a518ed8fa81b909c27c903140a12dc32.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Magnitude-Phase-Dual-Path-Speech-Enhancement-Network-based-on-Self-Supervised-Embedding-and-Perceptual-Contrast-Stretch-Boosting"><a href="#Magnitude-Phase-Dual-Path-Speech-Enhancement-Network-based-on-Self-Supervised-Embedding-and-Perceptual-Contrast-Stretch-Boosting" class="headerlink" title="Magnitude-Phase Dual-Path Speech Enhancement Network based on   Self-Supervised Embedding and Perceptual Contrast Stretch Boosting"></a>Magnitude-Phase Dual-Path Speech Enhancement Network based on   Self-Supervised Embedding and Perceptual Contrast Stretch Boosting</h2><p><strong>Authors:Alimjan Mattursun, Liejun Wang, Yinfeng Yu, Chunyang Ma</strong></p>
<p>Speech self-supervised learning (SSL) has made great progress in various speech processing tasks, but there is still room for improvement in speech enhancement (SE). This paper presents BSP-MPNet, a dual-path framework that combines self-supervised features with magnitude-phase information for SE. The approach starts by applying the perceptual contrast stretching (PCS) algorithm to enhance the magnitude-phase spectrum. A magnitude-phase 2D coarse (MP-2DC) encoder then extracts coarse features from the enhanced spectrum. Next, a feature-separating self-supervised learning (FS-SSL) model generates self-supervised embeddings for the magnitude and phase components separately. These embeddings are fused to create cross-domain feature representations. Finally, two parallel RNN-enhanced multi-attention (REMA) mask decoders refine the features, apply them to the mask, and reconstruct the speech signal. We evaluate BSP-MPNet on the VoiceBank+DEMAND and WHAMR! datasets. Experimental results show that BSP-MPNet outperforms existing methods under various noise conditions, providing new directions for self-supervised speech enhancement research. The implementation of the BSP-MPNet code is available online\footnote[2]{<a target="_blank" rel="noopener" href="https://github.com/AlimMat/BSP-MPNet">https://github.com/AlimMat/BSP-MPNet</a>. \label{s1}} </p>
<blockquote>
<p>è¯­éŸ³è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨å„é¡¹è¯­éŸ³å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†å·¨å¤§çš„è¿›æ­¥ï¼Œä½†åœ¨è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ–¹é¢ä»æœ‰æ”¹è¿›ç©ºé—´ã€‚æœ¬æ–‡æå‡ºäº†BSP-MPNetï¼Œè¿™æ˜¯ä¸€ä¸ªåŒè·¯å¾„æ¡†æ¶ï¼Œç»“åˆäº†è‡ªç›‘ç£ç‰¹å¾å’Œå¹…åº¦ç›¸ä½ä¿¡æ¯ç”¨äºSEã€‚è¯¥æ–¹æ³•é¦–å…ˆåº”ç”¨æ„ŸçŸ¥å¯¹æ¯”åº¦æ‹‰ä¼¸ï¼ˆPCSï¼‰ç®—æ³•å¢å¼ºå¹…åº¦ç›¸ä½è°±ã€‚ç„¶åï¼Œå¹…åº¦ç›¸ä½2Dç²—ç•¥ï¼ˆMP-2DCï¼‰ç¼–ç å™¨ä»å¢å¼ºåçš„è°±ä¸­æå–ç²—ç•¥ç‰¹å¾ã€‚æ¥ä¸‹æ¥ï¼Œç‰¹å¾åˆ†ç¦»è‡ªç›‘ç£å­¦ä¹ ï¼ˆFS-SSLï¼‰æ¨¡å‹åˆ†åˆ«ä¸ºå¹…åº¦å’Œç›¸ä½åˆ†é‡ç”Ÿæˆè‡ªç›‘ç£åµŒå…¥ã€‚è¿™äº›åµŒå…¥èåˆåœ¨ä¸€èµ·ä»¥åˆ›å»ºè·¨åŸŸç‰¹å¾è¡¨ç¤ºã€‚æœ€åï¼Œä¸¤ä¸ªå¹¶è¡ŒRNNå¢å¼ºå¤šæ³¨æ„åŠ›ï¼ˆREMAï¼‰æ©ç è§£ç å™¨å¯¹ç‰¹å¾è¿›è¡Œç²¾ç‚¼ï¼Œåº”ç”¨äºæ©ç ï¼Œå¹¶é‡å»ºè¯­éŸ³ä¿¡å·ã€‚æˆ‘ä»¬åœ¨VoiceBank+DEMANDå’ŒWHAMRï¼æ•°æ®é›†ä¸Šè¯„ä¼°äº†BSP-MPNetã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å„ç§å™ªå£°æ¡ä»¶ä¸‹ï¼ŒBSP-MPNetä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºè‡ªç›‘ç£è¯­éŸ³å¢å¼ºç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚BSP-MPNetçš„ä»£ç å®ç°å¯åœ¨ç½‘ä¸Šæ‰¾åˆ°[^2]ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/AlimMat/BSP-MPNet%E3%80%82/label%7Bs1%7D%EF%BC%89%E3%80%82">https://github.com/AlimMat/BSP-MPNetã€‚\label{s1}ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21571v1">PDF</a> Main paper (6 pages). Accepted for publication by ICME 2025</p>
<p><strong>Summary</strong>ï¼š<br>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè‡ªç›‘ç£å­¦ä¹ çš„è¯­éŸ³å¢å¼ºæ–¹æ³•BSP-MPNetï¼Œè¯¥æ–¹æ³•ç»“åˆåŒè·¯å¾„æ¡†æ¶ä¸å¹…åº¦ç›¸ä½ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§å™ªå£°æ¡ä»¶ä¸‹è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºè‡ªç›‘ç£è¯­éŸ³å¢å¼ºç ”ç©¶æä¾›äº†æ–°çš„æ–¹å‘ã€‚å…¶ä»£ç å®ç°å·²åœ¨çº¿å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æœ¬ç ”ç©¶æå‡ºäº†BSP-MPNetæ¨¡å‹ï¼Œä¸€ä¸ªç»“åˆè‡ªç›‘ç£å­¦ä¹ ä¸å¹…åº¦ç›¸ä½ä¿¡æ¯çš„åŒè·¯å¾„æ¡†æ¶ç”¨äºè¯­éŸ³å¢å¼ºã€‚</li>
<li>åˆ©ç”¨æ„ŸçŸ¥å¯¹æ¯”æ‹‰ä¼¸ï¼ˆPCSï¼‰ç®—æ³•å¢å¼ºå¹…åº¦ç›¸ä½è°±ã€‚</li>
<li>é‡‡ç”¨å¹…åº¦ç›¸ä½2Dç²—ç¼–ç ï¼ˆMP-2DCï¼‰ä»å¢å¼ºè°±ä¸­æå–ç‰¹å¾ã€‚</li>
<li>ç‰¹å¾åˆ†ç¦»è‡ªç›‘ç£å­¦ä¹ ï¼ˆFS-SSLï¼‰æ¨¡å‹ä¸ºå¹…åº¦å’Œç›¸ä½æˆåˆ†åˆ†åˆ«ç”Ÿæˆè‡ªç›‘ç£åµŒå…¥ã€‚</li>
<li>é€šè¿‡èåˆåµŒå…¥åˆ›å»ºè·¨åŸŸç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>ä¸¤ä¸ªå¹¶è¡ŒRNNå¢å¼ºå¤šæ³¨æ„åŠ›ï¼ˆREMAï¼‰æ©ç è§£ç å™¨å¯¹ç‰¹å¾è¿›è¡Œç²¾ç‚¼ï¼Œåº”ç”¨äºæ©ç å¹¶é‡å»ºè¯­éŸ³ä¿¡å·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21571">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-650354bdf53f41590d3547484dcd67fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-087672adc7be68eb8af6b977118fd85f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b3be224e99e922daef1aef825d11399.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e8cc8e2aa8be0309348ab7ed99bf475.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe45c54311ee11f075ecaafd350632f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d70835eb14cc20adf7f527710970142.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VALLR-Visual-ASR-Language-Model-for-Lip-Reading"><a href="#VALLR-Visual-ASR-Language-Model-for-Lip-Reading" class="headerlink" title="VALLR: Visual ASR Language Model for Lip Reading"></a>VALLR: Visual ASR Language Model for Lip Reading</h2><p><strong>Authors:Marshall Thomas, Edward Fish, Richard Bowden</strong></p>
<p>Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complex task requiring the interpretation of spoken language exclusively from visual cues, primarily lip movements and facial expressions. This task is especially challenging due to the absence of auditory information and the inherent ambiguity when visually distinguishing phonemes that have overlapping visemes where different phonemes appear identical on the lips. Current methods typically attempt to predict words or characters directly from these visual cues, but this approach frequently encounters high error rates due to coarticulation effects and viseme ambiguity. We propose a novel two-stage, phoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) that addresses these longstanding challenges. First, our model predicts a compact sequence of phonemes from visual inputs using a Video Transformer with a CTC head, thereby reducing the task complexity and achieving robust speaker invariance. This phoneme output then serves as the input to a fine-tuned Large Language Model (LLM), which reconstructs coherent words and sentences by leveraging broader linguistic context. Unlike existing methods that either predict words directly-often faltering on visually similar phonemes-or rely on large-scale multimodal pre-training, our approach explicitly encodes intermediate linguistic structure while remaining highly data efficient. We demonstrate state-of-the-art performance on two challenging datasets, LRS2 and LRS3, where our method achieves significant reductions in Word Error Rate (WER) achieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled data than the next best approach. </p>
<blockquote>
<p>å”‡è¯»æˆ–è§†è§‰è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆV-ASRï¼‰æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œå®ƒè¦æ±‚ä»…é€šè¿‡è§†è§‰çº¿ç´¢ï¼ˆä¸»è¦æ˜¯å˜´å”‡åŠ¨ä½œå’Œé¢éƒ¨è¡¨æƒ…ï¼‰æ¥è§£é‡Šå£è¯­ã€‚ç”±äºç¼ºå°‘å¬è§‰ä¿¡æ¯ä»¥åŠè§†è§‰ä¸ŠåŒºåˆ†å…·æœ‰é‡å å¯è§è¯­éŸ³çš„ä¸åŒè¯­éŸ³çš„å›ºæœ‰æ¨¡ç³Šæ€§ï¼Œè¿™é¡¹ä»»åŠ¡å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å½“å‰çš„æ–¹æ³•é€šå¸¸å°è¯•ç›´æ¥ä»è¿™äº›è§†è§‰çº¿ç´¢ä¸­é¢„æµ‹å•è¯æˆ–å­—ç¬¦ï¼Œä½†ç”±äºååŒå‘éŸ³æ•ˆåº”å’Œå¯è§è¯­éŸ³çš„æ¨¡ç³Šæ€§ï¼Œè¿™ç§æ–¹æ³•ç»å¸¸é‡åˆ°è¾ƒé«˜çš„é”™è¯¯ç‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹è§†è§‰è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆV-ASRï¼‰çš„ä¸¤é˜¶æ®µã€ä»¥è¯­éŸ³ä¸ºæ ¸å¿ƒçš„æ–°å‹æ¡†æ¶ï¼Œä»¥è§£å†³è¿™äº›é•¿æœŸå­˜åœ¨çš„æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨å¸¦æœ‰CTCå¤´éƒ¨çš„è§†é¢‘è½¬æ¢å™¨ä»è§†è§‰è¾“å…¥ä¸­é¢„æµ‹ç´§å‡‘çš„è¯­éŸ³åºåˆ—ï¼Œä»è€Œé™ä½ä»»åŠ¡å¤æ‚æ€§å¹¶å®ç°ç¨³å¥çš„è¯´è¯è€…ä¸å˜æ€§ã€‚ç„¶åï¼Œè¿™ä¸ªè¯­éŸ³è¾“å‡ºä½œä¸ºå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å…¥ï¼Œè¯¥æ¨¡å‹é€šè¿‡åˆ©ç”¨æ›´å¹¿æ³›çš„è¯­å¢ƒé‡å»ºè¿è´¯çš„å•è¯å’Œå¥å­ã€‚ä¸ç°æœ‰çš„ç›´æ¥é¢„æµ‹å•è¯çš„æ–¹æ³•æˆ–åœ¨è§†è§‰ä¸Šç›¸ä¼¼çš„è¯­éŸ³ä¸Šç»å¸¸å‡ºé”™çš„æ–¹æ³•ä¸åŒï¼Œæˆ–è€…ä¾èµ–äºå¤§è§„æ¨¡å¤šæ¨¡å¼é¢„è®­ç»ƒçš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜ç¡®ç¼–ç ä¸­é—´çš„è¯­è¨€ç»“æ„ï¼ŒåŒæ—¶ä¿æŒé«˜åº¦æ•°æ®æ•ˆç‡ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†LRS2å’ŒLRS3ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„è¡¨ç°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨LRS3ä¸Šå®ç°äº†æ˜¾è‘—çš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½ï¼Œå°½ç®¡æˆ‘ä»¬ä½¿ç”¨çš„æ ‡è®°æ•°æ®æ¯”ä¸‹ä¸€ä¸ªæœ€ä½³æ–¹æ³•å°‘äº†99.4%ï¼Œä½†ä»è¾¾åˆ°äº†SOTAçš„WERä¸º18.7ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21408v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†è§‰è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆV-ASRï¼‰çš„æ–°æŒ‘æˆ˜åŠæ–°æ–¹æ³•ã€‚é’ˆå¯¹ä»…é€šè¿‡è§†è§‰çº¿ç´¢è§£è¯»å£è¯­çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„ä¸¤é˜¶æ®µã€ä»¥éŸ³ç´ ä¸ºä¸­å¿ƒçš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä½¿ç”¨è§†é¢‘å˜å‹å™¨ï¼ˆVideo Transformerï¼‰ç»“åˆCTCå¤´éƒ¨é¢„æµ‹ç´§å‡‘çš„éŸ³ç´ åºåˆ—ï¼Œå®ç°ç¨³å¥çš„è¯´è¯è€…ä¸å˜æ€§ï¼Œå¹¶å°†é¢„æµ‹çš„éŸ³ç´ ä½œä¸ºç²¾ç»†è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å…¥ï¼Œé‡å»ºè¿è´¯çš„å•è¯å’Œå¥å­ã€‚è¯¥æ–¹æ³•åœ¨LRS2å’ŒLRS3ä¸¤ä¸ªæŒ‘æˆ˜æ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œæ˜¾è‘—é™ä½äº†è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨è¾ƒå°‘æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Lip Readingæˆ–Visual Automatic Speech Recognition (V-ASR) æ˜¯é€šè¿‡è§†è§‰çº¿ç´¢è§£è¯»å£è¯­çš„å¤æ‚ä»»åŠ¡ã€‚</li>
<li>å½“å‰æ–¹æ³•ç›´æ¥é€šè¿‡è§†è§‰çº¿ç´¢é¢„æµ‹å•è¯æˆ–å­—ç¬¦ï¼Œä½†å­˜åœ¨é«˜è¯¯å·®ç‡ã€‚</li>
<li>æå‡ºçš„ä¸¤é˜¶æ®µã€ä»¥éŸ³ç´ ä¸ºä¸­å¿ƒçš„æ¡†æ¶ä½¿ç”¨Video Transformerå’ŒCTCå¤´éƒ¨é¢„æµ‹éŸ³ç´ åºåˆ—ï¼Œå®ç°ç¨³å¥çš„è¯´è¯è€…ä¸å˜æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶ä½¿ç”¨LLMé‡å»ºè¿è´¯çš„å•è¯å’Œå¥å­ï¼Œåˆ©ç”¨æ›´å¹¿æ³›çš„è¯­å¢ƒã€‚</li>
<li>æ–¹æ³•åœ¨LRS2å’ŒLRS3æ•°æ®é›†ä¸Šå®ç°å“è¶Šæ€§èƒ½ï¼Œæ˜¾è‘—å‡å°‘è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ã€‚</li>
<li>å³ä½¿åœ¨ä½¿ç”¨è¾ƒå°‘æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•ä¹Ÿè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21408">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c406b8e2a843b9da3efc6f6860129089.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56162d5dbff7fed6b08de856ff5d46f3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-77d491e0f0b54b1f0a646aa92199c12f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87e4e2c38f27967c8c496695a177bf7f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-71-2-Î¼-W-Speech-Recognition-Accelerator-with-Recurrent-Spiking-Neural-Network"><a href="#A-71-2-Î¼-W-Speech-Recognition-Accelerator-with-Recurrent-Spiking-Neural-Network" class="headerlink" title="A 71.2-$Î¼$W Speech Recognition Accelerator with Recurrent Spiking   Neural Network"></a>A 71.2-$Î¼$W Speech Recognition Accelerator with Recurrent Spiking   Neural Network</h2><p><strong>Authors:Chih-Chyau Yang, Tian-Sheuan Chang</strong></p>
<p>This paper introduces a 71.2-$\mu$W speech recognition accelerator designed for edge devicesâ€™ real-time applications, emphasizing an ultra low power design. Achieved through algorithm and hardware co-optimizations, we propose a compact recurrent spiking neural network with two recurrent layers, one fully connected layer, and a low time step (1 or 2). The 2.79-MB model undergoes pruning and 4-bit fixed-point quantization, shrinking it by 96.42% to 0.1 MB. On the hardware front, we take advantage of \textit{mixed-level pruning}, \textit{zero-skipping} and \textit{merged spike} techniques, reducing complexity by 90.49% to 13.86 MMAC&#x2F;S. The \textit{parallel time-step execution} addresses inter-time-step data dependencies and enables weight buffer power savings through weight sharing. Capitalizing on the sparse spike activity, an input broadcasting scheme eliminates zero computations, further saving power. Implemented on the TSMC 28-nm process, the design operates in real time at 100 kHz, consuming 71.2 $\mu$W, surpassing state-of-the-art designs. At 500 MHz, it has 28.41 TOPS&#x2F;W and 1903.11 GOPS&#x2F;mm$^2$ in energy and area efficiency, respectively. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€æ¬¾é’ˆå¯¹è¾¹ç¼˜è®¾å¤‡å®æ—¶åº”ç”¨è®¾è®¡çš„71.2å¾®ç“¦è¯­éŸ³è¯†åˆ«åŠ é€Ÿå™¨ï¼Œé‡ç‚¹å¼ºè°ƒäº†è¶…ä½åŠŸè€—è®¾è®¡ã€‚é€šè¿‡ç®—æ³•å’Œç¡¬ä»¶çš„ååŒä¼˜åŒ–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç´§å‡‘çš„å¾ªç¯è„‰å†²ç¥ç»ç½‘ç»œï¼ŒåŒ…å«ä¸¤å±‚å¾ªç¯å±‚ã€ä¸€å±‚å…¨è¿æ¥å±‚ä»¥åŠè¾ƒä½çš„æ—¶é—´æ­¥é•¿ï¼ˆ1æˆ–2ï¼‰ã€‚è¯¥æ¨¡å‹å¤§å°ä¸º2.79MBï¼Œç»è¿‡å‰ªæå’Œ4ä½å®šç‚¹é‡åŒ–å¤„ç†ï¼Œç¼©å°äº†96.42%ï¼Œè¾¾åˆ°ä»…0.1MBçš„å¤§å°ã€‚åœ¨ç¡¬ä»¶æ–¹é¢ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤šçº§å‰ªæã€é›¶è·³è¿‡å’Œåˆå¹¶è„‰å†²ç­‰æŠ€æœ¯ï¼Œå°†å¤æ‚åº¦é™ä½äº†90.49%ï¼Œè¾¾åˆ°æ¯ç§’å¤„ç†13.86ç™¾ä¸‡ä¹˜åŠ æ“ä½œï¼ˆMMAC&#x2F;Sï¼‰ã€‚å¹¶è¡Œæ—¶é—´æ­¥æ‰§è¡Œè§£å†³äº†æ—¶é—´æ­¥ä¹‹é—´çš„æ•°æ®ä¾èµ–é—®é¢˜ï¼Œå¹¶é€šè¿‡æƒé‡å…±äº«å®ç°äº†æƒé‡ç¼“å†²å™¨çš„èŠ‚èƒ½ã€‚åˆ©ç”¨ç¨€ç–è„‰å†²æ´»åŠ¨ï¼Œè¾“å…¥å¹¿æ’­æ–¹æ¡ˆæ¶ˆé™¤äº†é›¶è®¡ç®—ï¼Œè¿›ä¸€æ­¥èŠ‚çœäº†åŠŸè€—ã€‚è¯¥è®¾è®¡é‡‡ç”¨å°ç§¯ç”µï¼ˆTSMCï¼‰çš„28çº³ç±³å·¥è‰ºå®ç°ï¼Œèƒ½ä»¥å®æ—¶é€Ÿåº¦100kHzè¿è¡Œï¼ŒåŠŸè€—ä»…ä¸º71.2å¾®ç“¦ï¼Œè¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›çš„è®¾è®¡ã€‚åœ¨500å…†èµ«å…¹çš„é¢‘ç‡ä¸‹ï¼Œå…¶åœ¨èƒ½æ•ˆå’Œé¢ç§¯æ•ˆç‡æ–¹é¢åˆ†åˆ«è¾¾åˆ°äº†æ¯ç“¦ç‰¹å¤„ç†æ¬¡æ•°ï¼ˆTOPS&#x2F;Wï¼‰ä¸º28.41å’Œæ¯å¹³æ–¹æ¯«ç±³å¤„ç†æ¬¡æ•°ï¼ˆGOPS&#x2F;mmÂ²ï¼‰ä¸ºæ¯ç§’ç™¾ä¸‡æŒ‡ä»¤æ“ä½œï¼ˆOPSï¼‰ã€‚åˆ©ç”¨è¶…ç¨€ç–è„‰å†²ä¿¡å·å¹²æ‰°æ¢æµ‹è¿™ä¸€æ“ä½œä»¥é™ä½å™ªå£°ï¼Œå®é™…åº”ç”¨å±•ç¤ºè¯¥è®¾è®¡å¯å¤§å¹…æé«˜èƒ½æºæ•ˆç‡å’Œè®¡ç®—æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21337v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€æ¬¾é’ˆå¯¹è¾¹ç¼˜è®¾å¤‡å®æ—¶åº”ç”¨è®¾è®¡çš„71.2Î¼Wè¯­éŸ³è¯†åˆ«åŠ é€Ÿå™¨ï¼Œé‡ç‚¹å®ç°è¶…ä½åŠŸè€—è®¾è®¡ã€‚é€šè¿‡ç®—æ³•å’Œç¡¬ä»¶ååŒä¼˜åŒ–ï¼Œæå‡ºäº†ä¸€ç§ç´§å‡‘çš„å¾ªç¯è„‰å†²ç¥ç»ç½‘ç»œï¼ŒåŒ…å«ä¸¤å±‚å¾ªç¯å±‚ã€ä¸€å±‚å…¨è¿æ¥å±‚ï¼Œå¹¶é‡‡ç”¨è¾ƒä½çš„æ—¶é—´æ­¥é•¿ï¼ˆ1æˆ–2ï¼‰ã€‚æ¨¡å‹ç»è¿‡å‰ªæå’Œ4ä½å®šç‚¹é‡åŒ–ï¼Œç¼©å°äº†96.42%è‡³0.1MBã€‚åœ¨ç¡¬ä»¶æ–¹é¢ï¼Œåˆ©ç”¨æ··åˆçº§åˆ«å‰ªæã€é›¶è·³è¿‡å’Œåˆå¹¶è„‰å†²ç­‰æŠ€æœ¯ï¼Œå¤æ‚åº¦é™ä½äº†90.49%è‡³13.86MMAC&#x2F;Sã€‚é€šè¿‡å¹¶è¡Œæ—¶é—´æ­¥æ‰§è¡Œè§£å†³è·¨æ—¶é—´æ­¥æ•°æ®ä¾èµ–é—®é¢˜ï¼Œå¹¶é€šè¿‡æƒé‡å…±äº«å®ç°æƒé‡ç¼“å†²åŒºåŠŸè€—èŠ‚çœã€‚åˆ©ç”¨ç¨€ç–è„‰å†²æ´»åŠ¨ï¼Œè¾“å…¥å¹¿æ’­æ–¹æ¡ˆæ¶ˆé™¤äº†é›¶è®¡ç®—ï¼Œè¿›ä¸€æ­¥èŠ‚çœäº†åŠŸè€—ã€‚åœ¨TSMC 28çº³ç±³å·¥è‰ºä¸Šå®ç°çš„è®¾è®¡ï¼Œå®æ—¶è¿è¡Œé¢‘ç‡ä¸º100 kHzï¼ŒåŠŸè€—ä¸º71.2Î¼Wï¼Œè¶…è¶Šäº†ç°æœ‰è®¾è®¡ã€‚åœ¨èƒ½é‡å’Œé¢ç§¯æ•ˆç‡æ–¹é¢ï¼Œè¯¥è®¾è®¡åœ¨500 MHzæ—¶è¾¾åˆ°äº†28.41 TOPS&#x2F;Wå’Œ1903.11 GOPS&#x2F;mmÂ²ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†é’ˆå¯¹è¾¹ç¼˜è®¾å¤‡å®æ—¶åº”ç”¨çš„è¶…ä½åŠŸè€—è¯­éŸ³è¯†åˆ«åŠ é€Ÿå™¨è®¾è®¡ã€‚</li>
<li>é€šè¿‡ç®—æ³•å’Œç¡¬ä»¶ååŒä¼˜åŒ–ï¼Œå®ç°äº†ç´§å‡‘çš„å¾ªç¯è„‰å†²ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚</li>
<li>æ¨¡å‹ç»è¿‡å‰ªæå’Œé‡åŒ–ï¼Œå¤§å¹…ç¼©å°äº†æ¨¡å‹å¤§å°ã€‚</li>
<li>åˆ©ç”¨æ··åˆçº§åˆ«å‰ªæã€é›¶è·³è¿‡å’Œåˆå¹¶è„‰å†²ç­‰æŠ€æœ¯ï¼Œé™ä½äº†ç¡¬ä»¶å¤æ‚åº¦ã€‚</li>
<li>é‡‡ç”¨äº†å¹¶è¡Œæ—¶é—´æ­¥æ‰§è¡ŒæŠ€æœ¯ï¼Œè§£å†³äº†æ•°æ®ä¾èµ–é—®é¢˜ï¼Œå¹¶é™ä½äº†åŠŸè€—ã€‚</li>
<li>è¾“å…¥å¹¿æ’­æ–¹æ¡ˆæ¶ˆé™¤äº†é›¶è®¡ç®—ï¼Œè¿›ä¸€æ­¥æå‡äº†èƒ½æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21337">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-02a352d6c83fb43d41e2be2dd5ed8a18.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5427d68791f22e13a1a0d6f2a0bc787a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ed56e1dc97d1642e60356ffbc82570a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4893546cde0b589dcd4c656b60bcd410.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-922fa974361ba9d3b61bdc1eaedad00f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3d09b4c8d0f8a968ca11a31fa3e2390.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a13f8ec9410e225021d3950a2e832862.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c9d0a0a2c003328db4d792350a0e9a6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Leveraging-LLMs-with-Iterative-Loop-Structure-for-Enhanced-Social-Intelligence-in-Video-Question-Answering"><a href="#Leveraging-LLMs-with-Iterative-Loop-Structure-for-Enhanced-Social-Intelligence-in-Video-Question-Answering" class="headerlink" title="Leveraging LLMs with Iterative Loop Structure for Enhanced Social   Intelligence in Video Question Answering"></a>Leveraging LLMs with Iterative Loop Structure for Enhanced Social   Intelligence in Video Question Answering</h2><p><strong>Authors:Erika Mori, Yue Qiu, Hirokatsu Kataoka, Yoshimitsu Aoki</strong></p>
<p>Social intelligence, the ability to interpret emotions, intentions, and behaviors, is essential for effective communication and adaptive responses. As robots and AI systems become more prevalent in caregiving, healthcare, and education, the demand for AI that can interact naturally with humans grows. However, creating AI that seamlessly integrates multiple modalities, such as vision and speech, remains a challenge. Current video-based methods for social intelligence rely on general video recognition or emotion recognition techniques, often overlook the unique elements inherent in human interactions. To address this, we propose the Looped Video Debating (LVD) framework, which integrates Large Language Models (LLMs) with visual information, such as facial expressions and body movements, to enhance the transparency and reliability of question-answering tasks involving human interaction videos. Our results on the Social-IQ 2.0 benchmark show that LVD achieves state-of-the-art performance without fine-tuning. Furthermore, supplementary human annotations on existing datasets provide insights into the modelâ€™s accuracy, guiding future improvements in AI-driven social intelligence. </p>
<blockquote>
<p>ç¤¾ä¼šæ™ºèƒ½ï¼Œå³è§£è¯»æƒ…æ„Ÿã€æ„å›¾å’Œè¡Œä¸ºçš„èƒ½åŠ›ï¼Œå¯¹äºæœ‰æ•ˆæ²Ÿé€šå’Œé€‚åº”æ€§ååº”è‡³å…³é‡è¦ã€‚éšç€æœºå™¨äººåœ¨æŠ¤ç†ã€åŒ»ç–—å’Œæ•™è‚²é¢†åŸŸçš„ä½¿ç”¨æ—¥ç›Šæ™®åŠï¼Œå¯¹èƒ½å¤Ÿè‡ªç„¶ä¸äººç±»äº’åŠ¨çš„AIçš„éœ€æ±‚ä¹Ÿåœ¨å¢é•¿ã€‚ç„¶è€Œï¼Œåˆ›å»ºæ— ç¼é›†æˆå¤šç§æ¨¡å¼ï¼ˆå¦‚è§†è§‰å’Œè¯­éŸ³ï¼‰çš„AIä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å½“å‰åŸºäºè§†é¢‘çš„ç¤¾ä¼šæ™ºèƒ½æ–¹æ³•ä¾èµ–äºé€šç”¨è§†é¢‘è¯†åˆ«æˆ–æƒ…æ„Ÿè¯†åˆ«æŠ€æœ¯ï¼Œå¾€å¾€ä¼šå¿½ç•¥äººç±»äº’åŠ¨ä¸­å›ºæœ‰çš„ç‹¬ç‰¹å…ƒç´ ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¾ªç¯è§†é¢‘è¾©è®ºï¼ˆLVDï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸é¢éƒ¨è¡¨æƒ…å’Œèº«ä½“åŠ¨ä½œç­‰è§†è§‰ä¿¡æ¯ç›¸ç»“åˆï¼Œæé«˜äº†æ¶‰åŠäººç±»äº’åŠ¨è§†é¢‘çš„é—®ç­”ä»»åŠ¡çš„é€æ˜åº¦å’Œå¯é æ€§ã€‚æˆ‘ä»¬åœ¨Social-IQ 2.0åŸºå‡†æµ‹è¯•ä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒLVDåœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¯¹ç°æœ‰æ•°æ®é›†è¿›è¡Œè¡¥å……äººå·¥æ³¨é‡Šï¼Œä¸ºæ¨¡å‹çš„å‡†ç¡®æ€§æä¾›äº†è§è§£ï¼ŒæŒ‡å¯¼æœªæ¥AIé©±åŠ¨çš„ç¤¾ä¼šæ™ºèƒ½çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21190v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ç¤¾ä¼šæ™ºèƒ½ï¼Œå³è§£è¯»æƒ…æ„Ÿã€æ„å›¾å’Œè¡Œä¸ºçš„èƒ½åŠ›ï¼Œå¯¹äºæœ‰æ•ˆæ²Ÿé€šå’Œé€‚åº”ååº”è‡³å…³é‡è¦ã€‚éšç€æœºå™¨äººå’Œäººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨æŠ¤ç†ã€åŒ»ç–—å’Œæ•™è‚²ç­‰é¢†åŸŸçš„æ™®åŠï¼Œå¯¹èƒ½è‡ªç„¶ä¸äººç±»äº’åŠ¨çš„äººå·¥æ™ºèƒ½çš„éœ€æ±‚ä¸æ–­å¢é•¿ã€‚ç„¶è€Œï¼Œåˆ›å»ºèƒ½æ— ç¼é›†æˆå¤šç§æ¨¡å¼ï¼ˆå¦‚è§†è§‰å’Œè¯­éŸ³ï¼‰çš„äººå·¥æ™ºèƒ½ä»æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¾ªç¯è§†é¢‘è¾©è®ºï¼ˆLVDï¼‰æ¡†æ¶ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸é¢éƒ¨è¡¨æƒ…å’Œèº«ä½“åŠ¨ä½œç­‰è§†è§‰ä¿¡æ¯ç›¸ç»“åˆï¼Œæé«˜äº†æ¶‰åŠäººç±»äº’åŠ¨è§†é¢‘çš„é—®ç­”ä»»åŠ¡çš„é€æ˜åº¦å’Œå¯é æ€§ã€‚æˆ‘ä»¬åœ¨Social-IQ 2.0åŸºå‡†æµ‹è¯•ä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒLVDåœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹å³å¯å®ç°æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¯¹ç°æœ‰æ•°æ®é›†è¿›è¡Œçš„äººç±»é™„åŠ æ³¨é‡Šæä¾›äº†å…³äºæ¨¡å‹å‡†ç¡®æ€§çš„è§è§£ï¼Œä¸ºæ”¹è¿›AIé©±åŠ¨çš„ç¤¾ä¼šæ™ºèƒ½æä¾›äº†æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¤¾ä¼šæ™ºèƒ½å¯¹äºæœ‰æ•ˆæ²Ÿé€šå’Œé€‚åº”ååº”è‡³å…³é‡è¦ã€‚</li>
<li>éšç€AIåœ¨å¤šä¸ªé¢†åŸŸçš„åº”ç”¨å¢é•¿ï¼Œå¯¹èƒ½è‡ªç„¶ä¸äººç±»äº’åŠ¨çš„æ™ºèƒ½ä½“çš„éœ€æ±‚ä¹Ÿåœ¨å¢é•¿ã€‚</li>
<li>åˆ›å»ºèƒ½æ— ç¼é›†æˆå¤šç§æ¨¡å¼ï¼ˆå¦‚è§†è§‰å’Œè¯­éŸ³ï¼‰çš„AIæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„LVDæ¡†æ¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰ä¿¡æ¯ï¼Œæé«˜äº†æ¶‰åŠäººç±»äº’åŠ¨è§†é¢‘çš„AIæ€§èƒ½ã€‚</li>
<li>LVDæ¡†æ¶åœ¨Social-IQ 2.0åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
<li>é€šè¿‡äººç±»é™„åŠ æ³¨é‡Šå¯¹ç°æœ‰æ•°æ®é›†è¿›è¡Œåˆ†æï¼Œä¸ºæ”¹è¿›AIçš„ç¤¾ä¼šæ™ºèƒ½æä¾›äº†æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6da3a2a0b9c6e198fff88b81e886e21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecf190d7ae25a331145f1fa020ed2efa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ce671957ba285a8951cb5886676a62c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cec6d42f47057337904ff6f06d37db1f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae817b53913468bbc0bc9c54bfe22ff8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-000f67cabcff25a71284cb2d54fd582f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90c6e58e58bc0f4801d8bf85e496b0c0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="GatedxLSTM-A-Multimodal-Affective-Computing-Approach-for-Emotion-Recognition-in-Conversations"><a href="#GatedxLSTM-A-Multimodal-Affective-Computing-Approach-for-Emotion-Recognition-in-Conversations" class="headerlink" title="GatedxLSTM: A Multimodal Affective Computing Approach for Emotion   Recognition in Conversations"></a>GatedxLSTM: A Multimodal Affective Computing Approach for Emotion   Recognition in Conversations</h2><p><strong>Authors:Yupei Li, Qiyang Sun, Sunil Munthumoduku Krishna Murthy, Emran Alturki, BjÃ¶rn W. Schuller</strong></p>
<p>Affective Computing (AC) is essential for advancing Artificial General Intelligence (AGI), with emotion recognition serving as a key component. However, human emotions are inherently dynamic, influenced not only by an individualâ€™s expressions but also by interactions with others, and single-modality approaches often fail to capture their full dynamics. Multimodal Emotion Recognition (MER) leverages multiple signals but traditionally relies on utterance-level analysis, overlooking the dynamic nature of emotions in conversations. Emotion Recognition in Conversation (ERC) addresses this limitation, yet existing methods struggle to align multimodal features and explain why emotions evolve within dialogues. To bridge this gap, we propose GatedxLSTM, a novel speech-text multimodal ERC model that explicitly considers voice and transcripts of both the speaker and their conversational partner(s) to identify the most influential sentences driving emotional shifts. By integrating Contrastive Language-Audio Pretraining (CLAP) for improved cross-modal alignment and employing a gating mechanism to emphasise emotionally impactful utterances, GatedxLSTM enhances both interpretability and performance. Additionally, the Dialogical Emotion Decoder (DED) refines emotion predictions by modelling contextual dependencies. Experiments on the IEMOCAP dataset demonstrate that GatedxLSTM achieves state-of-the-art (SOTA) performance among open-source methods in four-class emotion classification. These results validate its effectiveness for ERC applications and provide an interpretability analysis from a psychological perspective. </p>
<blockquote>
<p>æƒ…æ„Ÿè®¡ç®—ï¼ˆACï¼‰å¯¹æ¨åŠ¨äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰çš„å‘å±•è‡³å…³é‡è¦ï¼Œæƒ…æ„Ÿè¯†åˆ«æ˜¯å…¶ä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚ç„¶è€Œï¼Œäººç±»æƒ…æ„Ÿæœ¬è´¨ä¸Šæ˜¯åŠ¨æ€çš„ï¼Œä¸ä»…å—åˆ°ä¸ªäººè¡¨è¾¾çš„å½±å“ï¼Œè¿˜å—åˆ°ä¸ä»–äººäº’åŠ¨çš„å½±å“ï¼Œå•ä¸€æ¨¡å¼çš„æ–¹æ³•å¾€å¾€æ— æ³•æ•æ‰å…¶å…¨éƒ¨åŠ¨æ€ã€‚å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ï¼ˆMERï¼‰åˆ©ç”¨å¤šç§ä¿¡å·ï¼Œä½†ä¼ ç»Ÿä¸Šä¾èµ–äºè¯è¯­å±‚é¢çš„åˆ†æï¼Œå¿½è§†äº†æƒ…æ„Ÿå¯¹è¯ä¸­çš„åŠ¨æ€ç‰¹å¾ã€‚å¯¹è¯ä¸­çš„æƒ…æ„Ÿè¯†åˆ«ï¼ˆERCï¼‰è§£å†³äº†è¿™ä¸€å±€é™æ€§ï¼Œä½†ç°æœ‰æ–¹æ³•éš¾ä»¥å¯¹é½å¤šæ¨¡æ€ç‰¹å¾å¹¶è§£é‡Šæƒ…æ„Ÿå¦‚ä½•åœ¨å¯¹è¯ä¸­æ¼”å˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†GatedxLSTMï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„è¯­éŸ³æ–‡æœ¬å¤šæ¨¡æ€ERCæ¨¡å‹ï¼Œå®ƒæ˜ç¡®è€ƒè™‘äº†è¯´è¯è€…å’Œä»–ä»¬çš„å¯¹è¯ä¼™ä¼´çš„è¯­éŸ³å’Œæ–‡æœ¬è½¬å½•ï¼Œä»¥è¯†åˆ«æ¨åŠ¨æƒ…æ„Ÿå˜åŒ–çš„æœ€æœ‰å½±å“åŠ›çš„å¥å­ã€‚é€šè¿‡é›†æˆå¯¹æ¯”è¯­è¨€éŸ³é¢‘é¢„è®­ç»ƒï¼ˆCLAPï¼‰ä»¥æ”¹è¿›è·¨æ¨¡æ€å¯¹é½ï¼Œå¹¶é‡‡ç”¨é—¨æ§æœºåˆ¶æ¥å¼ºè°ƒæƒ…æ„Ÿä¸Šæœ‰å½±å“åŠ›çš„è¨€è®ºï¼ŒGatedxLSTMæé«˜äº†å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¯¹è¯æƒ…æ„Ÿè§£ç å™¨ï¼ˆDEDï¼‰é€šè¿‡æ¨¡æ‹Ÿä¸Šä¸‹æ–‡ä¾èµ–æ€§æ¥å®Œå–„æƒ…æ„Ÿé¢„æµ‹ã€‚åœ¨IEMOCAPæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGatedxLSTMåœ¨å››ç±»æƒ…æ„Ÿåˆ†ç±»ä¸­å®ç°äº†å¼€æºæ–¹æ³•ä¸­çš„æœ€æ–°æŠ€æœ¯è¡¨ç°ã€‚è¿™äº›ç»“æœéªŒè¯äº†å…¶åœ¨ERCåº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä»å¿ƒç†è§’åº¦æä¾›äº†å¯è§£é‡Šæ€§åˆ†æã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20919v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æƒ…æ„Ÿè®¡ç®—åœ¨æ¨è¿›äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½æ–¹é¢çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºæƒ…æ„Ÿè¯†åˆ«æ˜¯å…¶ä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚ç„¶è€Œï¼Œäººç±»æƒ…æ„Ÿå…·æœ‰åŠ¨æ€æ€§ï¼Œå•æ¨¡æ€æ–¹æ³•æ— æ³•æ•æ‰å…¶å…¨é¢ä¿¡æ¯ã€‚å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ï¼ˆMERï¼‰é‡‡ç”¨å¤šç§ä¿¡å·ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•ä¾§é‡äºè¯è¯­å±‚é¢çš„åˆ†æï¼Œå¿½ç•¥äº†æƒ…æ„Ÿå¯¹è¯ä¸­çš„åŠ¨æ€å˜åŒ–ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†åŸºäºé—¨æ§é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆGatedxLSTMï¼‰çš„è¯­éŸ³æ–‡æœ¬å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½æ˜ç¡®è€ƒè™‘è¯´è¯è€…åŠå…¶å¯¹è¯ä¼™ä¼´çš„å£°éŸ³å’Œæ–‡å­—ï¼Œä»¥è¯†åˆ«æ¨åŠ¨æƒ…æ„Ÿå˜åŒ–çš„å…³é”®å¥å­ã€‚è¯¥æ¨¡å‹é€šè¿‡å¯¹æ¯”è¯­è¨€éŸ³é¢‘é¢„è®­ç»ƒï¼ˆCLAPï¼‰æé«˜è·¨æ¨¡æ€å¯¹é½èƒ½åŠ›ï¼Œå¹¶é‡‡ç”¨é—¨æ§æœºåˆ¶å¼ºè°ƒæƒ…æ„Ÿå½±å“å¤§çš„è¯è¯­ã€‚æ­¤å¤–ï¼Œå¯¹è¯æƒ…æ„Ÿè§£ç å™¨ï¼ˆDEDï¼‰é€šè¿‡æ¨¡æ‹Ÿä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼Œæé«˜äº†æƒ…æ„Ÿé¢„æµ‹çš„å‡†ç¡®æ€§ã€‚åœ¨IEMOCAPæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGatedxLSTMåœ¨å››ç±»åˆ«æƒ…æ„Ÿåˆ†ç±»ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ï¼ŒéªŒè¯äº†å…¶åœ¨æƒ…æ„Ÿè¯†åˆ«å¯¹è¯åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä»å¿ƒç†è§’åº¦æä¾›äº†å¯è§£é‡Šæ€§åˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æƒ…æ„Ÿè®¡ç®—å¯¹æ¨è¿›äººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½è‡³å…³é‡è¦ï¼Œå…¶ä¸­æƒ…æ„Ÿè¯†åˆ«æ˜¯å…³é”®ç»„æˆéƒ¨åˆ†ã€‚</li>
<li>äººç±»æƒ…æ„Ÿå…·æœ‰åŠ¨æ€æ€§ï¼Œå•æ¨¡æ€æ–¹æ³•æ— æ³•å…¨é¢æ•æ‰ã€‚</li>
<li>å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ï¼ˆMERï¼‰é‡‡ç”¨å¤šç§ä¿¡å·ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•å¿½ç•¥æƒ…æ„Ÿå¯¹è¯ä¸­çš„åŠ¨æ€å˜åŒ–ã€‚</li>
<li>æå‡ºçš„GatedxLSTMæ¨¡å‹èƒ½è¯†åˆ«æ¨åŠ¨æƒ…æ„Ÿå˜åŒ–çš„å…³é”®å¥å­ï¼Œè€ƒè™‘è¯´è¯è€…åŠå…¶å¯¹è¯ä¼™ä¼´çš„å£°éŸ³å’Œæ–‡å­—ã€‚</li>
<li>GatedxLSTMæ¨¡å‹é€šè¿‡å¯¹æ¯”è¯­è¨€éŸ³é¢‘é¢„è®­ç»ƒï¼ˆCLAPï¼‰æé«˜è·¨æ¨¡æ€å¯¹é½èƒ½åŠ›ã€‚</li>
<li>GatedxLSTMæ¨¡å‹é‡‡ç”¨é—¨æ§æœºåˆ¶å¼ºè°ƒæƒ…æ„Ÿå½±å“å¤§çš„è¯è¯­ï¼Œå¹¶é€šè¿‡å¯¹è¯æƒ…æ„Ÿè§£ç å™¨ï¼ˆDEDï¼‰æé«˜æƒ…æ„Ÿé¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20919">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe7ef5931671d10629b669c1e4cf6445.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-544a749a584ee0efded1ba38a0fd0868.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-228c51afc32494b560f0472c73ed6167.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aab0b42d3df2dd7a775b07c383679cf1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Sparse-Alignment-Enhanced-Latent-Diffusion-Transformer-for-Zero-Shot-Speech-Synthesis"><a href="#Sparse-Alignment-Enhanced-Latent-Diffusion-Transformer-for-Zero-Shot-Speech-Synthesis" class="headerlink" title="Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot   Speech Synthesis"></a>Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot   Speech Synthesis</h2><p><strong>Authors:Ziyue Jiang, Yi Ren, Ruiqi Li, Shengpeng Ji, Boyang Zhang, Zhenhui Ye, Chen Zhang, Bai Jionghao, Xiaoda Yang, Jialong Zuo, Yu Zhang, Rui Liu, Xiang Yin, Zhou Zhao</strong></p>
<p>While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness, mainstream systems still suffer from issues related to speech-text alignment modeling: 1) models without explicit speech-text alignment modeling exhibit less robustness, especially for hard sentences in practical applications; 2) predefined alignment-based models suffer from naturalness constraints of forced alignments. This paper introduces \textit{MegaTTS 3}, a TTS system featuring an innovative sparse alignment algorithm that guides the latent diffusion transformer (DiT). Specifically, we provide sparse alignment boundaries to MegaTTS 3 to reduce the difficulty of alignment without limiting the search space, thereby achieving high naturalness. Moreover, we employ a multi-condition classifier-free guidance strategy for accent intensity adjustment and adopt the piecewise rectified flow technique to accelerate the generation process. Experiments demonstrate that MegaTTS 3 achieves state-of-the-art zero-shot TTS speech quality and supports highly flexible control over accent intensity. Notably, our system can generate high-quality one-minute speech with only 8 sampling steps. Audio samples are available at <a target="_blank" rel="noopener" href="https://sditdemo.github.io/sditdemo/">https://sditdemo.github.io/sditdemo/</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œé›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹åœ¨è¯­éŸ³è´¨é‡å’Œè¡¨ç°åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œä½†ä¸»æµç³»ç»Ÿä»å­˜åœ¨ä¸è¯­éŸ³æ–‡æœ¬å¯¹é½å»ºæ¨¡ç›¸å…³çš„é—®é¢˜ï¼š1ï¼‰æ²¡æœ‰æ˜ç¡®çš„è¯­éŸ³æ–‡æœ¬å¯¹é½å»ºæ¨¡çš„æ¨¡å‹è¡¨ç°å‡ºè¾ƒä½çš„ç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å®é™…åº”ç”¨çš„å¤æ‚å¥å­ä¸­ï¼›2ï¼‰åŸºäºé¢„å®šä¹‰å¯¹é½çš„æ¨¡å‹å—åˆ°å¼ºåˆ¶å¯¹é½çš„è‡ªç„¶æ€§çº¦æŸã€‚æœ¬æ–‡ä»‹ç»äº†<em>MegaTTS 3</em>ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰åˆ›æ–°ç¨€ç–å¯¹é½ç®—æ³•çš„TTSç³»ç»Ÿï¼Œè¯¥ç®—æ³•æŒ‡å¯¼æ½œåœ¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºMegaTTS 3æä¾›ç¨€ç–å¯¹é½è¾¹ç•Œï¼Œä»¥å‡å°‘å¯¹é½éš¾åº¦è€Œä¸é™åˆ¶æœç´¢ç©ºé—´ï¼Œä»è€Œå®ç°é«˜è‡ªç„¶åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šæ¡ä»¶æ— åˆ†ç±»æŒ‡å¯¼ç­–ç•¥è¿›è¡Œå£éŸ³å¼ºåº¦è°ƒæ•´ï¼Œå¹¶é‡‡ç”¨åˆ†æ®µæ ¡æ­£æµæŠ€æœ¯æ¥åŠ é€Ÿç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒMegaTTS 3è¾¾åˆ°äº†æœ€å…ˆè¿›çš„é›¶æ ·æœ¬TTSè¯­éŸ³è´¨é‡ï¼Œå¹¶å¯¹å£éŸ³å¼ºåº¦å®ç°äº†é«˜åº¦çµæ´»çš„æ§åˆ¶ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåªéœ€8ä¸ªé‡‡æ ·æ­¥éª¤å°±èƒ½ç”Ÿæˆé«˜è´¨é‡çš„ä¸€åˆ†é’Ÿè¯­éŸ³ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://sditdemo.github.io/sditdemo/%E6%89%BE%E5%88%B0%E3%80%82">https://sditdemo.github.io/sditdemo/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18924v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ€æ–°ä¸€ä»£çš„é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹è™½ç„¶å¤§å¹…æå‡äº†è¯­éŸ³è´¨é‡å’Œè¡¨è¾¾åŠ›ï¼Œä½†ä¸»æµç³»ç»Ÿä»å­˜åœ¨è¯­éŸ³ä¸æ–‡æœ¬å¯¹é½å»ºæ¨¡çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMegaTTS 3çš„TTSç³»ç»Ÿï¼Œé‡‡ç”¨åˆ›æ–°çš„ç¨€ç–å¯¹é½ç®—æ³•å¼•å¯¼æ½œåœ¨æ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰ã€‚é€šè¿‡æä¾›ç¨€ç–å¯¹é½è¾¹ç•Œé™ä½å¯¹é½éš¾åº¦ä¸”ä¸é™åˆ¶æœç´¢ç©ºé—´ï¼Œæé«˜äº†è‡ªç„¶åº¦ã€‚åŒæ—¶é‡‡ç”¨å¤šæ¡ä»¶æ— åˆ†ç±»æŒ‡å¯¼ç­–ç•¥è°ƒèŠ‚å£éŸ³å¼ºåº¦å¹¶é‡‡ç”¨åˆ†æ®µæ•´æµæµæŠ€æœ¯åŠ é€Ÿç”Ÿæˆè¿‡ç¨‹ã€‚å®éªŒè¯æ˜MegaTTS 3å®ç°äº†é›¶æ ·æœ¬TTSçš„æœ€ä¼˜è¯­éŸ³è´¨é‡ï¼Œå¹¶æ”¯æŒé«˜åº¦çµæ´»çš„å£éŸ³å¼ºåº¦æ§åˆ¶ã€‚å¯ç”Ÿæˆä»…éœ€8æ­¥é‡‡æ ·çš„ä¸€åˆ†é’Ÿé«˜è´¨é‡è¯­éŸ³ã€‚å…·ä½“å‚è§[<a target="_blank" rel="noopener" href="https://sditdemo.github.io/sditdemo/]%E4%BA%86%E8%A7%A3%E9%9F%B3%E9%A2%91%E6%A0%B7%E6%9C%AC%E3%80%82">https://sditdemo.github.io/sditdemo/]äº†è§£éŸ³é¢‘æ ·æœ¬ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸»æµæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ä»å­˜åœ¨è¯­éŸ³ä¸æ–‡æœ¬å¯¹é½å»ºæ¨¡çš„é—®é¢˜ã€‚</li>
<li>MegaTTS 3ç³»ç»Ÿé‡‡ç”¨åˆ›æ–°çš„ç¨€ç–å¯¹é½ç®—æ³•ï¼Œæé«˜äº†è¯­éŸ³è‡ªç„¶åº¦ã€‚</li>
<li>MegaTTS 3å®ç°äº†é›¶æ ·æœ¬TTSçš„æœ€ä¼˜è¯­éŸ³è´¨é‡ã€‚</li>
<li>ç³»ç»Ÿæ”¯æŒé«˜åº¦çµæ´»çš„å£éŸ³å¼ºåº¦æ§åˆ¶ã€‚</li>
<li>MegaTTS 3é‡‡ç”¨åˆ†æ®µæ•´æµæµæŠ€æœ¯ä»¥åŠ é€Ÿè¯­éŸ³ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>ç³»ç»Ÿå¯ç”Ÿæˆä¸€åˆ†é’Ÿçš„é«˜è´¨é‡è¯­éŸ³ï¼Œä»…éœ€è¦8æ­¥é‡‡æ ·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18924">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b83eb1e59083ca253b5f5d680b25e034.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f63c305d51b567c1b33e75f11ea31ced.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-799c5f97dff2642f57db63aacabdbaa7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4dc21c79b3bf4b146770c255229fcba.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="VERSA-A-Versatile-Evaluation-Toolkit-for-Speech-Audio-and-Music"><a href="#VERSA-A-Versatile-Evaluation-Toolkit-for-Speech-Audio-and-Music" class="headerlink" title="VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music"></a>VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music</h2><p><strong>Authors:Jiatong Shi, Hye-jin Shim, Jinchuan Tian, Siddhant Arora, Haibin Wu, Darius Petermann, Jia Qi Yip, You Zhang, Yuxun Tang, Wangyou Zhang, Dareen Safar Alharthi, Yichen Huang, Koichi Saito, Jionghao Han, Yiwen Zhao, Chris Donahue, Shinji Watanabe</strong></p>
<p>In this work, we introduce VERSA, a unified and standardized evaluation toolkit designed for various speech, audio, and music signals. The toolkit features a Pythonic interface with flexible configuration and dependency control, making it user-friendly and efficient. With full installation, VERSA offers 65 metrics with 729 metric variations based on different configurations. These metrics encompass evaluations utilizing diverse external resources, including matching and non-matching reference audio, text transcriptions, and text captions. As a lightweight yet comprehensive toolkit, VERSA is versatile to support the evaluation of a wide range of downstream scenarios. To demonstrate its capabilities, this work highlights example use cases for VERSA, including audio coding, speech synthesis, speech enhancement, singing synthesis, and music generation. The toolkit is available at <a target="_blank" rel="noopener" href="https://github.com/wavlab-speech/versa">https://github.com/wavlab-speech/versa</a>. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†VERSAï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºå„ç§è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹ä¿¡å·è®¾è®¡çš„ç»Ÿä¸€ã€æ ‡å‡†åŒ–çš„è¯„ä¼°å·¥å…·åŒ…ã€‚è¯¥å·¥å…·åŒ…å…·æœ‰Pythoné£æ ¼çš„æ¥å£ï¼Œå…·æœ‰çµæ´»çš„é…ç½®å’Œä¾èµ–æ§åˆ¶ï¼Œä½¿å…¶å‹å¥½ä¸”é«˜æ•ˆã€‚åœ¨å®Œå…¨å®‰è£…åï¼ŒVERSAæä¾›åŸºäºä¸åŒé…ç½®çš„65ç§æŒ‡æ ‡å’Œ729ç§æŒ‡æ ‡å˜ä½“ã€‚è¿™äº›æŒ‡æ ‡æ¶µç›–äº†åˆ©ç”¨å¤šç§å¤–éƒ¨èµ„æºçš„è¯„ä¼°ï¼ŒåŒ…æ‹¬åŒ¹é…å’ŒéåŒ¹é…çš„å‚è€ƒéŸ³é¢‘ã€æ–‡æœ¬è½¬å½•å’Œæ–‡æœ¬æè¿°ã€‚ä½œä¸ºä¸€ä¸ªè½»ä¾¿è€Œå…¨é¢çš„å·¥å…·åŒ…ï¼ŒVERSAæ”¯æŒå¯¹å„ç§ä¸‹æ¸¸åœºæ™¯çš„è¯„ä¼°ã€‚ä¸ºäº†è¯æ˜å…¶èƒ½åŠ›ï¼Œè¿™é¡¹å·¥ä½œé‡ç‚¹ä»‹ç»äº†VERSAçš„ç¤ºä¾‹ç”¨ä¾‹ï¼ŒåŒ…æ‹¬éŸ³é¢‘ç¼–ç ã€è¯­éŸ³åˆæˆã€è¯­éŸ³å¢å¼ºã€æ­Œå”±åˆæˆå’ŒéŸ³ä¹ç”Ÿæˆã€‚è¯¥å·¥å…·åŒ…å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wavlab-speech/versa%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wavlab-speech/versaä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17667v2">PDF</a> </p>
<p><strong>Summary</strong><br>VERSAæ˜¯ä¸€ä¸ªç»Ÿä¸€æ ‡å‡†åŒ–çš„è¯„ä¼°å·¥å…·åŒ…ï¼Œé€‚ç”¨äºå„ç§è¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹ä¿¡å·çš„è¯„ä»·ã€‚å®ƒå…·å¤‡Pythonicæ¥å£ã€çµæ´»çš„é…ç½®å’Œä¾èµ–æ§åˆ¶ï¼Œç”¨æˆ·å‹å¥½ä¸”é«˜æ•ˆã€‚VERSAæä¾›65ç§åº¦é‡æŒ‡æ ‡ï¼ŒåŸºäºä¸åŒé…ç½®æœ‰729ç§åº¦é‡æŒ‡æ ‡å˜åŒ–ï¼Œæ¶µç›–åˆ©ç”¨å¤šç§å¤–éƒ¨èµ„æºçš„è¯„ä¼°ï¼Œå¦‚åŒ¹é…å’ŒéåŒ¹é…å‚è€ƒéŸ³é¢‘ã€æ–‡æœ¬è½¬å½•å’Œæ–‡æœ¬å­—å¹•ã€‚å®ƒæ”¯æŒå¤šç§ä¸‹æ¸¸åœºæ™¯çš„è¯„ä»·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VERSAæ˜¯ä¸€ä¸ªç»Ÿä¸€æ ‡å‡†åŒ–çš„è¯„ä»·å·¥å…·åŒ…ï¼Œé€‚ç”¨äºè¯­éŸ³ã€éŸ³é¢‘å’ŒéŸ³ä¹ä¿¡å·çš„è¯„ä»·ã€‚</li>
<li>VERSAå…·å¤‡Pythonicæ¥å£ï¼Œå…·å¤‡çµæ´»çš„é…ç½®å’Œä¾èµ–æ§åˆ¶ã€‚</li>
<li>VERSAæä¾›65ç§åº¦é‡æŒ‡æ ‡ï¼ŒåŸºäºä¸åŒé…ç½®æœ‰729ç§åº¦é‡æŒ‡æ ‡å˜åŒ–ã€‚</li>
<li>VERSAçš„åº¦é‡æŒ‡æ ‡æ¶µç›–åˆ©ç”¨å¤šç§å¤–éƒ¨èµ„æºçš„è¯„ä¼°ï¼Œå¦‚åŒ¹é…å’ŒéåŒ¹é…å‚è€ƒéŸ³é¢‘ã€æ–‡æœ¬è½¬å½•å’Œæ–‡æœ¬å­—å¹•ã€‚</li>
<li>VERSAæ”¯æŒå¤šç§ä¸‹æ¸¸åœºæ™¯çš„è¯„ä»·ï¼ŒåŒ…æ‹¬éŸ³é¢‘ç¼–ç ã€è¯­éŸ³åˆæˆã€è¯­éŸ³å¢å¼ºã€æ­Œå”±åˆæˆå’ŒéŸ³ä¹ç”Ÿæˆç­‰ã€‚</li>
<li>VERSAåœ¨GitHubä¸Šæœ‰å…¬å¼€çš„ä»£ç åº“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-49dd14a08aac38b6c02b55a3089ebb77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef56415eb65cfb8300e0a1913078f61c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f8e0e799de48fe8975d415cbff636bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11c60c703837d6d4aba282bfeb1cd3b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6be0d11933d9ac99b6081aaa2e0804c5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Towards-Controllable-Speech-Synthesis-in-the-Era-of-Large-Language-Models-A-Survey"><a href="#Towards-Controllable-Speech-Synthesis-in-the-Era-of-Large-Language-Models-A-Survey" class="headerlink" title="Towards Controllable Speech Synthesis in the Era of Large Language   Models: A Survey"></a>Towards Controllable Speech Synthesis in the Era of Large Language   Models: A Survey</h2><p><strong>Authors:Tianxin Xie, Yan Rong, Pengfei Zhang, Wenwu Wang, Li Liu</strong></p>
<p>Text-to-speech (TTS), also known as speech synthesis, is a prominent research area that aims to generate natural-sounding human speech from text. Recently, with the increasing industrial demand, TTS technologies have evolved beyond synthesizing human-like speech to enabling controllable speech generation. This includes fine-grained control over various attributes of synthesized speech such as emotion, prosody, timbre, and duration. In addition, advancements in deep learning, such as diffusion and large language models, have significantly enhanced controllable TTS over the past several years. In this work, we conduct a comprehensive survey of controllable TTS, covering approaches ranging from basic control techniques to methods utilizing natural language prompts, aiming to provide a clear understanding of the current state of research. We examine the general controllable TTS pipeline, challenges, model architectures, and control strategies, offering a comprehensive and clear taxonomy of existing methods. Additionally, we provide a detailed summary of datasets and evaluation metrics and shed some light on the applications and future directions of controllable TTS. To the best of our knowledge, this survey paper provides the first comprehensive review of emerging controllable TTS methods, which can serve as a beneficial resource for both academic researchers and industrial practitioners. </p>
<blockquote>
<p>æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ï¼Œä¹Ÿç§°ä¸ºè¯­éŸ³åˆæˆï¼Œæ˜¯ä¸€ä¸ªçªå‡ºçš„ç ”ç©¶é¢†åŸŸï¼Œæ—¨åœ¨ä»æ–‡æœ¬ç”Ÿæˆå¬èµ·æ¥å¾ˆè‡ªç„¶çš„äººç±»è¯­éŸ³ã€‚æœ€è¿‘ï¼Œéšç€å·¥ä¸šéœ€æ±‚çš„å¢åŠ ï¼ŒTTSæŠ€æœ¯å·²ç»è¶…è¶Šäº†åˆæˆç±»ä¼¼äººç±»çš„è¯­éŸ³ï¼Œå®ç°äº†å¯æ§çš„è¯­éŸ³ç”Ÿæˆã€‚è¿™åŒ…æ‹¬å¯¹åˆæˆè¯­éŸ³çš„å„ç§å±æ€§çš„ç²¾ç»†æ§åˆ¶ï¼Œå¦‚æƒ…ç»ªã€è¯­è°ƒã€éŸ³è´¨å’ŒæŒç»­æ—¶é—´ã€‚æ­¤å¤–ï¼Œæ·±åº¦å­¦ä¹ ä¸­çš„æ‰©æ•£å’Œå¤§å‹è¯­è¨€æ¨¡å‹ç­‰æŠ€æœ¯çš„è¿›æ­¥ï¼Œåœ¨è¿‡å»çš„å‡ å¹´é‡Œæå¤§åœ°å¢å¼ºäº†å¯æ§TTSã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹å¯æ§TTSè¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ï¼Œæ¶µç›–äº†ä»åŸºæœ¬æ§åˆ¶æŠ€æœ¯åˆ°åˆ©ç”¨è‡ªç„¶è¯­è¨€æç¤ºçš„æ–¹æ³•ï¼Œæ—¨åœ¨æä¾›å¯¹å½“å‰ç ”ç©¶çŠ¶æ€çš„æ¸…æ™°ç†è§£ã€‚æˆ‘ä»¬ç ”ç©¶äº†é€šç”¨çš„å¯æ§TTSç®¡é“ã€æŒ‘æˆ˜ã€æ¨¡å‹æ¶æ„å’Œæ§åˆ¶ç­–ç•¥ï¼Œæä¾›äº†ç°æœ‰æ–¹æ³•çš„å…¨é¢æ¸…æ™°çš„åˆ†ç±»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡è¿›è¡Œäº†è¯¦ç»†çš„æ€»ç»“ï¼Œå¹¶æŒ‡å‡ºäº†å¯æ§TTSçš„åº”ç”¨å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™ç¯‡ç»¼è¿°è®ºæ–‡æä¾›äº†å¯¹æ–°å…´çš„å¯æ§TTSæ–¹æ³•çš„é¦–æ¬¡å…¨é¢å›é¡¾ï¼Œå¯ä»¥ä½œä¸ºå­¦æœ¯ç ”ç©¶äººå‘˜å’Œå·¥ä¸šä»ä¸šè€…çš„æœ‰ç›Šèµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06602v2">PDF</a> A comprehensive survey on controllable TTS, 26 pages, 7 tables, 6   figures, 317 references. Under review</p>
<p><strong>Summary</strong><br>     æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰åˆç§°è¯­éŸ³åˆæˆï¼Œæ˜¯ç”Ÿæˆè‡ªç„¶æµç•…çš„äººç±»è¯­éŸ³çš„é‡è¦ç ”ç©¶é¢†åŸŸã€‚è¿‘å¹´æ¥ï¼Œéšç€å·¥ä¸šéœ€æ±‚çš„å¢åŠ ï¼ŒTTSæŠ€æœ¯ä¸ä»…åˆæˆç±»ä¼¼äººç±»çš„å£°éŸ³ï¼Œè¿˜èƒ½å®ç°å¯æ§çš„è¯­éŸ³ç”Ÿæˆï¼ŒåŒ…æ‹¬ç²¾ç»†æ§åˆ¶åˆæˆè¯­éŸ³çš„å„ç§å±æ€§ï¼Œå¦‚æƒ…æ„Ÿã€è¯­è°ƒã€éŸ³è´¨å’ŒæŒç»­æ—¶é—´ã€‚æ·±åº¦å­¦ä¹ é¢†åŸŸçš„è¿›æ­¥ï¼Œå¦‚æ‰©æ•£æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨è¿‡å»å‡ å¹´ä¸­æ˜¾è‘—å¢å¼ºäº†å¯æ§TTSçš„æ€§èƒ½ã€‚æœ¬æ–‡å…¨é¢ç»¼è¿°äº†å¯æ§TTSï¼Œæ¶µç›–ä»åŸºæœ¬æ§åˆ¶æŠ€æœ¯åˆ°åˆ©ç”¨è‡ªç„¶è¯­è¨€æç¤ºçš„æ–¹æ³•ï¼Œæ—¨åœ¨æä¾›å¯¹å½“å‰ç ”ç©¶çŠ¶æ€çš„æ¸…æ™°ç†è§£ã€‚æˆ‘ä»¬è€ƒå¯Ÿäº†å¯æ§TTSçš„é€šç”¨æµç¨‹ã€æŒ‘æˆ˜ã€æ¨¡å‹æ¶æ„å’Œæ§åˆ¶ç­–ç•¥ï¼Œå¯¹ç°æœ‰æ–¹æ³•è¿›è¡Œäº†å…¨é¢æ¸…æ™°çš„åˆ†ç±»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯¦ç»†æ€»ç»“äº†æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶æŒ‡å‡ºäº†å¯æ§TTSçš„åº”ç”¨å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTSï¼ˆæ–‡æœ¬è½¬è¯­éŸ³ï¼‰ä¸ä»…æ˜¯ç”Ÿæˆç±»ä¼¼äººç±»çš„å£°éŸ³ï¼Œè¿˜èƒ½å®ç°å¯æ§çš„è¯­éŸ³ç”Ÿæˆã€‚</li>
<li>å¯æ§TTSå…è®¸å¯¹åˆæˆè¯­éŸ³çš„å„ç§å±æ€§è¿›è¡Œç²¾ç»†æ§åˆ¶ï¼Œå¦‚æƒ…æ„Ÿã€è¯­è°ƒã€éŸ³è´¨å’ŒæŒç»­æ—¶é—´ã€‚</li>
<li>æ·±åº¦å­¦ä¹ é¢†åŸŸçš„è¿›æ­¥ï¼Œå¦‚æ‰©æ•£æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—å¢å¼ºäº†å¯æ§TTSçš„æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡å…¨é¢æ¦‚è¿°äº†å¯æ§TTSçš„ç ”ç©¶ç°çŠ¶ï¼ŒåŒ…æ‹¬å…¶é€šç”¨æµç¨‹ã€æŒ‘æˆ˜ã€æ¨¡å‹æ¶æ„å’Œæ§åˆ¶ç­–ç•¥ã€‚</li>
<li>æ–‡ç« è¯¦ç»†é˜è¿°äº†å¯æ§TTSçš„æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>å¯æ§TTSå…·æœ‰å¹¿æ³›çš„åº”ç”¨åœºæ™¯ï¼Œå¹¶åœ¨æœªæ¥å…·æœ‰å·¨å¤§çš„å‘å±•æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06602">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77b9d1b4adf952efd84a9a0f647cb16e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6793a177689270ec51d736f2677c63a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-914ceb695b61c14f39efea85c78546a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53c531461a37a5313b4c529845b753e2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SF-Speech-Straightened-Flow-for-Zero-Shot-Voice-Clone"><a href="#SF-Speech-Straightened-Flow-for-Zero-Shot-Voice-Clone" class="headerlink" title="SF-Speech: Straightened Flow for Zero-Shot Voice Clone"></a>SF-Speech: Straightened Flow for Zero-Shot Voice Clone</h2><p><strong>Authors:Xuyuan Li, Zengqiang Shang, Hua Hua, Peiyang Shi, Chen Yang, Li Wang, Pengyuan Zhang</strong></p>
<p>Recently, neural ordinary differential equations (ODE) models trained with flow matching have achieved impressive performance on the zero-shot voice clone task. Nevertheless, postulating standard Gaussian noise as the initial distribution of ODE gives rise to numerous intersections within the fitted targets of flow matching, which presents challenges to model training and enhances the curvature of the learned generated trajectories. These curved trajectories restrict the capacity of ODE models for generating desirable samples with a few steps. This paper proposes SF-Speech, a novel voice clone model based on ODE and in-context learning. Unlike the previous works, SF-Speech adopts a lightweight multi-stage module to generate a more deterministic initial distribution for ODE. Without introducing any additional loss function, we effectively straighten the curved reverse trajectories of the ODE model by jointly training it with the proposed module. Experiment results on datasets of various scales show that SF-Speech outperforms the state-of-the-art zero-shot TTS methods and requires only a quarter of the solver steps, resulting in a generation speed approximately 3.7 times that of Voicebox and E2 TTS. Audio samples are available at the demo page\footnote{[Online] Available: <a target="_blank" rel="noopener" href="https://lixuyuan102.github.io/Demo/%7D">https://lixuyuan102.github.io/Demo/}</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œä½¿ç”¨æµåŒ¹é…è®­ç»ƒçš„ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æ¨¡å‹åœ¨é›¶æ ·æœ¬è¯­éŸ³å…‹éš†ä»»åŠ¡ä¸Šå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå‡è®¾æ ‡å‡†é«˜æ–¯å™ªå£°ä½œä¸ºODEçš„åˆå§‹åˆ†å¸ƒä¼šå¯¼è‡´æµåŒ¹é…çš„ç›®æ ‡æ‹Ÿåˆä¸­å‡ºç°è®¸å¤šäº¤ç‚¹ï¼Œè¿™ç»™æ¨¡å‹è®­ç»ƒå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå¹¶å¢å¼ºäº†å­¦ä¹ ç”Ÿæˆè½¨è¿¹çš„æ›²ç‡ã€‚è¿™äº›å¼¯æ›²çš„è½¨è¿¹é™åˆ¶äº†ODEæ¨¡å‹åœ¨å‡ æ­¥å†…ç”Ÿæˆç†æƒ³æ ·æœ¬çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†SF-Speechï¼Œè¿™æ˜¯ä¸€ç§åŸºäºODEå’Œä¸Šä¸‹æ–‡å­¦ä¹ çš„æ–°å‹è¯­éŸ³å…‹éš†æ¨¡å‹ã€‚ä¸åŒäºä»¥å‰çš„å·¥ä½œï¼ŒSF-Speeché‡‡ç”¨è½»é‡çº§çš„å¤šé˜¶æ®µæ¨¡å—æ¥ä¸ºODEç”Ÿæˆæ›´ç¡®å®šçš„åˆå§‹åˆ†å¸ƒã€‚æˆ‘ä»¬æ²¡æœ‰å¼•å…¥ä»»ä½•é¢å¤–çš„æŸå¤±å‡½æ•°ï¼Œè€Œæ˜¯é€šè¿‡ä¸æ‰€æå‡ºçš„æ¨¡å—è¿›è¡Œè”åˆè®­ç»ƒï¼Œæœ‰æ•ˆåœ°æ ¡æ­£äº†ODEæ¨¡å‹çš„å¼¯æ›²åå‘è½¨è¿¹ã€‚åœ¨å„ç§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSF-Speechä¼˜äºæœ€æ–°çš„é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢æ–¹æ³•ï¼Œå¹¶ä¸”ä»…éœ€è¦å››åˆ†ä¹‹ä¸€çš„æ±‚è§£å™¨æ­¥éª¤ï¼Œç”Ÿæˆé€Ÿåº¦å¤§çº¦æ˜¯Voiceboxå’ŒE2 TTSçš„3.7å€ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨æ¼”ç¤ºé¡µé¢è·å¾—^[å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å¾—ï¼š<a target="_blank" rel="noopener" href="https://lixuyuan102.github.io/Demo/]^%E3%80%82">https://lixuyuan102.github.io/Demo/]^ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12399v2">PDF</a> Accepted by IEEE Transactions on Audio, Speech and Language   Processing</p>
<p><strong>Summary</strong></p>
<p>ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æ¨¡å‹åœ¨é›¶æ ·æœ¬è¯­éŸ³å…‹éš†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä»¤äººç©ç›®çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå°†æ ‡å‡†é«˜æ–¯å™ªå£°è®¾ä¸ºODEçš„åˆå§‹åˆ†å¸ƒä¼šäº§ç”Ÿè¯¸å¤šé—®é¢˜ï¼Œå¦‚ç›®æ ‡æµåŠ¨çš„åŒ¹é…ä¸­å‡ºç°è®¸å¤šäº¤ç‚¹ï¼Œè¿™ç»™æ¨¡å‹è®­ç»ƒå¸¦æ¥æŒ‘æˆ˜ï¼Œå¹¶å¢åŠ äº†å­¦ä¹ è½¨è¿¹çš„æ›²ç‡ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºODEå’Œä¸Šä¸‹æ–‡å­¦ä¹ çš„å…¨æ–°è¯­éŸ³å…‹éš†æ¨¡å‹SF-Speechã€‚ä¸åŒäºä»¥å¾€çš„å·¥ä½œï¼ŒSF-Speeché‡‡ç”¨è½»é‡çº§çš„å¤šé˜¶æ®µæ¨¡å—æ¥ç”Ÿæˆæ›´ç¡®å®šçš„ODEåˆå§‹åˆ†å¸ƒã€‚æ— éœ€å¼•å…¥ä»»ä½•é¢å¤–çš„æŸå¤±å‡½æ•°ï¼Œé€šè¿‡ä¸æ‰€ææ¨¡å—è”åˆè®­ç»ƒï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°æ ¡æ­£äº†ODEæ¨¡å‹çš„å¼¯æ›²åå‘è½¨è¿¹ã€‚åœ¨å¤šç§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSF-Speechä¼˜äºç›®å‰æœ€å…ˆè¿›çš„é›¶æ ·æœ¬TTSæ–¹æ³•ï¼Œä¸”æ‰€éœ€çš„æ±‚è§£å™¨æ­¥éª¤ä»…ä¸ºå››åˆ†ä¹‹ä¸€ï¼Œç”Ÿæˆé€Ÿåº¦å¤§çº¦æ˜¯Voiceboxå’ŒE2 TTSçš„3.7å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æ¨¡å‹åœ¨é›¶æ ·æœ¬è¯­éŸ³å…‹éš†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>å°†æ ‡å‡†é«˜æ–¯å™ªå£°è®¾ä¸ºODEåˆå§‹åˆ†å¸ƒä¼šäº§ç”Ÿäº¤ç‚¹é—®é¢˜ï¼Œå¢åŠ æ¨¡å‹è®­ç»ƒéš¾åº¦å’Œå­¦ä¹ è½¨è¿¹çš„æ›²ç‡ã€‚</li>
<li>SF-Speechæ˜¯ä¸€ç§åŸºäºODEå’Œä¸Šä¸‹æ–‡å­¦ä¹ çš„è¯­éŸ³å…‹éš†æ¨¡å‹ï¼Œé‡‡ç”¨è½»é‡çº§å¤šé˜¶æ®µæ¨¡å—ç”Ÿæˆæ›´ç¡®å®šçš„åˆå§‹åˆ†å¸ƒã€‚</li>
<li>SF-Speeché€šè¿‡è”åˆè®­ç»ƒæ ¡æ­£ODEæ¨¡å‹çš„å¼¯æ›²åå‘è½¨è¿¹ï¼Œæ— éœ€é¢å¤–æŸå¤±å‡½æ•°ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºSF-Speechä¼˜äºç°æœ‰é›¶æ ·æœ¬TTSæ–¹æ³•ï¼Œæ±‚è§£å™¨æ­¥éª¤å‡å°‘è‡³å››åˆ†ä¹‹ä¸€ã€‚</li>
<li>SF-Speechç”Ÿæˆé€Ÿåº¦è¾ƒå¿«ï¼Œå¤§çº¦æ˜¯Voiceboxå’ŒE2 TTSçš„3.7å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3f54ed6c1c036704cf78583341190999.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c42ed00e1a01701ddea1b8235a065904.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4806d14cab1e4eda416bd5e6b2c0f646.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bda72226910d0d2c9b20440eecf366b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-051e36ed8c714e7c8b117910852132c8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Vibravox-A-Dataset-of-French-Speech-Captured-with-Body-conduction-Audio-Sensors"><a href="#Vibravox-A-Dataset-of-French-Speech-Captured-with-Body-conduction-Audio-Sensors" class="headerlink" title="Vibravox: A Dataset of French Speech Captured with Body-conduction Audio   Sensors"></a>Vibravox: A Dataset of French Speech Captured with Body-conduction Audio   Sensors</h2><p><strong>Authors:Julien Hauret, Malo Olivier, Thomas Joubaud, Christophe Langrenne, Sarah PoirÃ©e, VÃ©ronique Zimpfer, Ã‰ric Bavu</strong></p>
<p>Vibravox is a dataset compliant with the General Data Protection Regulation (GDPR) containing audio recordings using five different body-conduction audio sensors: two in-ear microphones, two bone conduction vibration pickups, and a laryngophone. The dataset also includes audio data from an airborne microphone used as a reference. The Vibravox corpus contains 45 hours per sensor of speech samples and physiological sounds recorded by 188 participants under different acoustic conditions imposed by a high order ambisonics 3D spatializer. Annotations about the recording conditions and linguistic transcriptions are also included in the corpus. We conducted a series of experiments on various speech-related tasks, including speech recognition, speech enhancement, and speaker verification. These experiments were carried out using state-of-the-art models to evaluate and compare their performances on signals captured by the different audio sensors offered by the Vibravox dataset, with the aim of gaining a better grasp of their individual characteristics. </p>
<blockquote>
<p>Vibravoxæ˜¯ä¸€ä¸ªç¬¦åˆé€šç”¨æ•°æ®ä¿æŠ¤æ¡ä¾‹ï¼ˆGDPRï¼‰è¦æ±‚çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä½¿ç”¨äº”ç§ä¸åŒçš„ä½“ä¼ éŸ³é¢‘ä¼ æ„Ÿå™¨å½•åˆ¶çš„éŸ³é¢‘è®°å½•ï¼šä¸¤ä¸ªå…¥è€³å¼éº¦å…‹é£ï¼Œä¸¤ä¸ªéª¨ä¼ å¯¼æŒ¯åŠ¨æ‹¾éŸ³å™¨å’Œä¸€ä¸ªå–‰ä¼ å£°å™¨ã€‚è¯¥æ•°æ®é›†è¿˜åŒ…æ‹¬ç”¨ä½œå‚è€ƒçš„ç©ºæ°”ä¼ æ’­éº¦å…‹é£çš„éŸ³é¢‘æ•°æ®ã€‚Vibravoxè¯­æ–™åº“åŒ…å«ç”±188åå‚ä¸è€…åœ¨ç”±é«˜çº§ä¸‰ç»´ç¯ç»•å£°ç³»ç»Ÿæ–½åŠ çš„ä¸åŒå£°å­¦æ¡ä»¶ä¸‹å½•åˆ¶çš„æ¯ä¼ æ„Ÿå™¨45å°æ—¶çš„è¯­éŸ³æ ·æœ¬å’Œç”Ÿç†å£°éŸ³ã€‚å…³äºå½•éŸ³æ¡ä»¶å’Œè¯­è¨€è½¬å½•çš„æ³¨é‡Šä¹ŸåŒ…å«åœ¨è¯­æ–™åº“ä¸­ã€‚æˆ‘ä»¬åœ¨å„ç§ä¸è¯­éŸ³ç›¸å…³çš„ä»»åŠ¡ä¸Šè¿›è¡Œäº†ä¸€ç³»åˆ—å®éªŒï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³å¢å¼ºå’Œè¯­éŸ³éªŒè¯ã€‚è¿™äº›å®éªŒä½¿ç”¨çš„æ˜¯æœ€å‰æ²¿çš„æ¨¡å‹ï¼Œæ—¨åœ¨è¯„ä¼°å’Œæ¯”è¾ƒå®ƒä»¬åœ¨Vibravoxæ•°æ®é›†æä¾›çš„ä¸åŒéŸ³é¢‘ä¼ æ„Ÿå™¨æ•è·çš„ä¿¡å·ä¸Šçš„æ€§èƒ½ï¼Œä»¥ä¾¿æ›´å¥½åœ°äº†è§£å®ƒä»¬å„è‡ªçš„ç‰¹æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.11828v4">PDF</a> 23 pages, 42 figures</p>
<p><strong>Summary</strong></p>
<p>Vibravoxæ•°æ®é›†åŒ…å«ç¬¦åˆGDPRè§„å®šçš„éŸ³é¢‘è®°å½•ï¼Œä½¿ç”¨äº†äº”ç§ä¸åŒçš„ä½“ä¼ éŸ³é¢‘ä¼ æ„Ÿå™¨ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªè€³å†…éº¦å…‹é£ã€ä¸¤ä¸ªéª¨ä¼ å¯¼æŒ¯åŠ¨æ‹¾éŸ³å™¨å’Œä¸€ä¸ªå–‰å¤´è¯ç­’ã€‚æ­¤å¤–ï¼Œè¿˜åŒ…æ‹¬ä½œä¸ºå‚è€ƒçš„ç©ºæ°”ä¼ æ’­éº¦å…‹é£å½•åˆ¶çš„éŸ³é¢‘æ•°æ®ã€‚è¯¥æ•°æ®é›†åŒ…å«ç”±188åå‚ä¸è€…åœ¨ç”±é«˜çº§ä¸‰ç»´ç©ºé—´åŒ–ç¨‹åºäº§ç”Ÿä¸åŒå£°å­¦æ¡ä»¶ä¸‹å½•åˆ¶çš„å…±45å°æ—¶çš„è¯­éŸ³æ ·æœ¬å’Œç”Ÿç†å£°éŸ³è®°å½•ã€‚æ­¤å¤–ï¼Œè¿˜åŒ…æ‹¬æœ‰å…³å½•éŸ³æ¡ä»¶å’Œè¯­è¨€è½¬å½•çš„æ³¨é‡Šã€‚ç ”ç©¶å›¢é˜Ÿå¯¹è¯¥æ•°æ®é›†è¿›è¡Œäº†ä¸€ç³»åˆ—å®éªŒï¼Œæ¶µç›–äº†è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³å¢å¼ºå’Œè¯­éŸ³éªŒè¯ç­‰å¤šç§ä»»åŠ¡ã€‚ä½¿ç”¨å…ˆè¿›çš„æ¨¡å‹æ¥è¯„ä¼°ä¸åŒéŸ³é¢‘ä¼ æ„Ÿå™¨æ€§èƒ½ï¼Œæ—¨åœ¨æ›´å¥½åœ°äº†è§£å®ƒä»¬å„è‡ªçš„ç‰¹æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vibravoxæ˜¯ä¸€ä¸ªç¬¦åˆGDPRè§„å®šçš„æ•°æ®é›†ï¼ŒåŒ…å«å¤šç§ä½“ä¼ éŸ³é¢‘ä¼ æ„Ÿå™¨å½•åˆ¶çš„éŸ³é¢‘è®°å½•ã€‚</li>
<li>æ•°æ®é›†åŒ…å«ç”±ä¸åŒå£°å­¦æ¡ä»¶ä¸‹å½•åˆ¶çš„è¯­éŸ³æ ·æœ¬å’Œç”Ÿç†å£°éŸ³è®°å½•ã€‚</li>
<li>æ•°æ®é›†ä¸­è¿˜åŒ…æ‹¬æœ‰å…³å½•éŸ³æ¡ä»¶å’Œè¯­è¨€è½¬å½•çš„æ³¨é‡Šã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿè¿›è¡Œäº†åŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³å¢å¼ºå’Œè¯­éŸ³éªŒè¯åœ¨å†…çš„å¤šç§ä»»åŠ¡å®éªŒã€‚</li>
<li>å®éªŒä½¿ç”¨äº†å…ˆè¿›çš„æ¨¡å‹æ¥è¯„ä¼°ä¸åŒéŸ³é¢‘ä¼ æ„Ÿå™¨çš„æ€§èƒ½ã€‚</li>
<li>æ•°æ®é›†æ—¨åœ¨å¸®åŠ©æ›´å¥½åœ°ç†è§£ä¸åŒéŸ³é¢‘ä¼ æ„Ÿå™¨çš„ç‰¹æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.11828">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c1805c2f9d77ee62c04165affe2299d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebf0c2475e261b89eef72f21689cce74.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-47753aa1aba71aff8361e14b9da73dcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da645be8e11a2dfad8b2c7e65d7b8d31.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88dbeb8df06a498e2d8bc742531555e7.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Whistle-Data-Efficient-Multilingual-and-Crosslingual-Speech-Recognition-via-Weakly-Phonetic-Supervision"><a href="#Whistle-Data-Efficient-Multilingual-and-Crosslingual-Speech-Recognition-via-Weakly-Phonetic-Supervision" class="headerlink" title="Whistle: Data-Efficient Multilingual and Crosslingual Speech Recognition   via Weakly Phonetic Supervision"></a>Whistle: Data-Efficient Multilingual and Crosslingual Speech Recognition   via Weakly Phonetic Supervision</h2><p><strong>Authors:Saierdaer Yusuyin, Te Ma, Hao Huang, Wenbo Zhao, Zhijian Ou</strong></p>
<p>There exist three approaches for multilingual and crosslingual automatic speech recognition (MCL-ASR) - supervised pretraining with phonetic or graphemic transcription, and self-supervised pretraining. We find that pretraining with phonetic supervision has been underappreciated so far for MCL-ASR, while conceptually it is more advantageous for information sharing between different languages. This paper explores the approach of pretraining with weakly phonetic supervision towards data-efficient MCL-ASR, which is called Whistle. We relax the requirement of gold-standard human-validated phonetic transcripts, and obtain International Phonetic Alphabet (IPA) based transcription by leveraging the LanguageNet grapheme-to-phoneme (G2P) models. We construct a common experimental setup based on the CommonVoice dataset, called CV-Lang10, with 10 seen languages and 2 unseen languages. A set of experiments are conducted on CV-Lang10 to compare, as fair as possible, the three approaches under the common setup for MCL-ASR. Experiments demonstrate the advantages of phoneme-based models (Whistle) for MCL-ASR, in terms of speech recognition for seen languages, crosslingual performance for unseen languages with different amounts of few-shot data, overcoming catastrophic forgetting, and training efficiency. It is found that when training data is more limited, phoneme supervision can achieve better results compared to subword supervision and self-supervision, thereby providing higher data-efficiency. To support reproducibility and promote future research along this direction, we release the code, models and data for the entire pipeline of Whistle at <a target="_blank" rel="noopener" href="https://github.com/thu-spmi/CAT/tree/master/egs/cv-lang10">https://github.com/thu-spmi/CAT/tree/master/egs/cv-lang10</a>. </p>
<blockquote>
<p>å¯¹äºå¤šè¯­è¨€å’Œè·¨è¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆMCL-ASRï¼‰ï¼Œå­˜åœ¨ä¸‰ç§æ–¹æ³•ï¼šåŸºäºè¯­éŸ³æˆ–å­—å½¢è½¬å½•çš„æœ‰ç›‘ç£é¢„è®­ç»ƒä»¥åŠè‡ªç›‘ç£é¢„è®­ç»ƒã€‚æˆ‘ä»¬å‘ç°è¿„ä»Šä¸ºæ­¢ï¼Œå¯¹äºMCL-ASRè€Œè¨€ï¼Œæœ‰ç›‘ç£çš„è¯­éŸ³é¢„è®­ç»ƒä¸€ç›´è¢«ä½ä¼°äº†ï¼Œè™½ç„¶åœ¨æ¦‚å¿µä¸Šå®ƒå¯¹äºä¸åŒè¯­è¨€é—´çš„ä¿¡æ¯å…±äº«æ›´ä¸ºæœ‰åˆ©ã€‚æœ¬æ–‡æ¢è®¨äº†å¼±è¯­éŸ³ç›‘ç£çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œä»¥æé«˜æ•°æ®æ•ˆç‡çš„å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ•ˆç‡ï¼Œè¿™è¢«ç§°ä¸ºâ€œå“¨å­â€ã€‚æˆ‘ä»¬æ”¾å®½äº†é‡‘æ ‡å‡†äººå·¥éªŒè¯çš„è¯­éŸ³è½¬å½•è¦æ±‚ï¼Œå¹¶åˆ©ç”¨LanguageNetçš„å­—å½¢åˆ°éŸ³ç´ ï¼ˆG2Pï¼‰æ¨¡å‹è·å¾—åŸºäºå›½é™…éŸ³æ ‡ï¼ˆIPAï¼‰çš„è½¬å½•ã€‚æˆ‘ä»¬åœ¨CommonVoiceæ•°æ®é›†çš„åŸºç¡€ä¸Šæ„å»ºäº†ä¸€ä¸ªé€šç”¨å®éªŒè®¾ç½®ï¼Œç§°ä¸ºCV-Lang10ï¼ŒåŒ…å«10ç§å·²çŸ¥è¯­è¨€å’Œ2ç§æœªçŸ¥è¯­è¨€ã€‚åœ¨CV-Lang10ä¸Šè¿›è¡Œä¸€ç³»åˆ—å®éªŒï¼Œå°½å¯èƒ½å…¬å¹³åœ°æ¯”è¾ƒä¸‰ç§MCL-ASRæ–¹æ³•ã€‚å®éªŒè¯æ˜äº†åŸºäºéŸ³ç´ çš„æ¨¡å‹ï¼ˆå“¨å­ï¼‰åœ¨MCL-ASRæ–¹é¢çš„ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬è¯†åˆ«å·²çŸ¥è¯­è¨€çš„è¯­éŸ³ã€å¯¹å…·æœ‰ä¸åŒæ•°é‡å°‘é‡æ•°æ®çš„æœªçŸ¥è¯­è¨€çš„è·¨è¯­è¨€æ€§èƒ½ã€å…‹æœç¾éš¾æ€§é—å¿˜å’Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚å‘ç°å½“è®­ç»ƒæ•°æ®æ›´åŠ æœ‰é™æ—¶ï¼Œä¸å•è¯ç›‘ç£å’Œè‡ªç›‘ç£ç›¸æ¯”ï¼ŒéŸ³ç´ ç›‘ç£å¯ä»¥è¾¾åˆ°æ›´å¥½çš„æ•ˆæœï¼Œä»è€Œæé«˜äº†æ•°æ®æ•ˆç‡ã€‚ä¸ºäº†æ”¯æŒå¯å¤åˆ¶æ€§å’Œä¿ƒè¿›æœªæ¥åœ¨è¿™ä¸€æ–¹å‘çš„ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/thu-spmi/CAT/tree/master/egs/cv-lang10">https://github.com/thu-spmi/CAT/tree/master/egs/cv-lang10</a>å‘å¸ƒäº†å“¨å­æ•´ä¸ªç®¡é“çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02166v2">PDF</a> Accepted by IEEE-TASLP</p>
<p><strong>æ‘˜è¦</strong><br>è¯¥è®ºæ–‡æ¢ç´¢äº†ä½¿ç”¨å¼±è¯­éŸ³ç›‘ç£è¿›è¡Œå¤šè¯­è¨€å’Œè·¨è¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆMCL-ASRï¼‰çš„æ–¹æ³•ï¼Œå³ç§°ä¸ºâ€œå“¨å£°â€ï¼ˆWhistleï¼‰çš„æŠ€æœ¯ã€‚å®ƒé€šè¿‡ä½¿ç”¨LanguageNetçš„å­—æ¯åˆ°è¯­éŸ³ï¼ˆG2Pï¼‰æ¨¡å‹æ¥è·å¾—åŸºäºå›½é™…éŸ³æ ‡ï¼ˆIPAï¼‰çš„è½¬å½•ï¼Œä»è€Œæ”¾å®½äº†å¯¹é‡‘æ ‡å‡†äººç±»éªŒè¯è¯­éŸ³è½¬å½•çš„è¦æ±‚ã€‚è®ºæ–‡æ„å»ºäº†ä¸€ä¸ªåŸºäºCommonVoiceæ•°æ®é›†çš„å®éªŒè®¾ç½®CV-Lang10ï¼ŒåŒ…å«10ç§å·²çŸ¥è¯­è¨€å’Œä¸¤ç§æœªçŸ¥è¯­è¨€ã€‚ä¸€ç³»åˆ—å®éªŒåœ¨CV-Lang10ä¸Šè¿›è¡Œï¼Œä»¥å°½å¯èƒ½å…¬å¹³çš„æ–¹å¼æ¯”è¾ƒä¸‰ç§MCL-ASRæ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒåŸºäºéŸ³ç´ æ¨¡å‹çš„å“¨å£°ï¼ˆWhistleï¼‰åœ¨MCL-ASRæ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å·²çŸ¥è¯­è¨€çš„è¯­éŸ³è¯†åˆ«ã€ä¸åŒå°‘é‡æ•°æ®çš„æœªçŸ¥è¯­è¨€çš„è·¨è¯­è¨€æ€§èƒ½ã€å…‹æœç¾éš¾æ€§é—å¿˜å’Œæé«˜è®­ç»ƒæ•ˆç‡æ–¹é¢ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œå½“è®­ç»ƒæ•°æ®æœ‰é™æ—¶ï¼ŒéŸ³ç´ ç›‘ç£å¯ä»¥å®ç°æ¯”å­è¯ç›‘ç£å’Œè‡ªæˆ‘ç›‘ç£æ›´å¥½çš„ç»“æœï¼Œä»è€Œæé«˜äº†æ•°æ®æ•ˆç‡ã€‚ä¸ºäº†æ”¯æŒå¯é‡å¤æ€§å’Œä¿ƒè¿›è¿™ä¸€æ–¹å‘ä¸Šçš„æœªæ¥ç ”ç©¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†å“¨å£°æ•´ä¸ªæµç¨‹çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®ºæ–‡ä»‹ç»äº†ä¸‰ç§å¤šè¯­è¨€å’Œè·¨è¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆMCL-ASRï¼‰çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä½¿ç”¨è¯­éŸ³æˆ–å­—æ¯è½¬å½•çš„æœ‰ç›‘ç£é¢„è®­ç»ƒä»¥åŠè‡ªç›‘ç£é¢„è®­ç»ƒã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œè¿„ä»Šä¸ºæ­¢å¯¹ä½¿ç”¨è¯­éŸ³ç›‘ç£è¿›è¡ŒMCL-ASRçš„è¯„ä¼°ä¸è¶³ï¼Œä½†ä»æ¦‚å¿µä¸Šè®²ï¼Œå®ƒå¯¹ä¸åŒè¯­è¨€ä¹‹é—´çš„ä¿¡æ¯å…±äº«æ›´æœ‰åˆ©ã€‚</li>
<li>è®ºæ–‡é€šè¿‡å¼•å…¥å¼±è¯­éŸ³ç›‘ç£æ–¹æ³•ï¼ˆç§°ä¸ºâ€œå“¨å£°â€ï¼‰æ¢ç´¢äº†æ•°æ®é«˜æ•ˆMCL-ASRçš„å¯è¡Œæ€§ã€‚é€šè¿‡åˆ©ç”¨LanguageNetçš„å­—æ¯åˆ°è¯­éŸ³ï¼ˆG2Pï¼‰æ¨¡å‹è·å¾—å›½é™…éŸ³æ ‡ï¼ˆIPAï¼‰ä¸ºåŸºç¡€çš„è½¬å½•ï¼Œé™ä½äº†å¯¹äººç±»éªŒè¯è¯­éŸ³è½¬å½•çš„é«˜æ ‡å‡†éœ€æ±‚ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒåŸºäºéŸ³ç´ çš„æ¨¡å‹ï¼ˆå“¨å£°ï¼‰åœ¨MCL-ASRæ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å·²çŸ¥è¯­è¨€çš„è¯­éŸ³è¯†åˆ«å’ŒæœªçŸ¥è¯­è¨€çš„è·¨è¯­è¨€æ€§èƒ½ä¸Šã€‚</li>
<li>å½“è®­ç»ƒæ•°æ®æœ‰é™æ—¶ï¼ŒéŸ³ç´ ç›‘ç£ç›¸è¾ƒäºå­è¯ç›‘ç£å’Œè‡ªç›‘ç£èƒ½æ›´å¥½åœ°å®ç°ç»“æœï¼Œè¡¨ç°å‡ºè¾ƒé«˜çš„æ•°æ®æ•ˆç‡ã€‚</li>
<li>è®ºæ–‡æ„å»ºäº†ä¸€ä¸ªå®éªŒè®¾ç½®CV-Lang10æ¥æ¯”è¾ƒä¸‰ç§MCL-ASRæ–¹æ³•åœ¨å„ç§æƒ…å†µä¸‹çš„è¡¨ç°ã€‚è¯¥è®¾ç½®åŸºäºCommonVoiceæ•°æ®é›†ï¼ŒåŒ…å«å¤šç§è¯­è¨€å’Œä¸åŒæ•°é‡çš„å°‘æ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.02166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-80fb18902e5db09ed16f2f996824fde0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de896ec51cbddef9b695822ccb19bc9d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45d66f7e06e1909abb010bd885538119.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Learning-Multi-modal-Representations-by-Watching-Hundreds-of-Surgical-Video-Lectures"><a href="#Learning-Multi-modal-Representations-by-Watching-Hundreds-of-Surgical-Video-Lectures" class="headerlink" title="Learning Multi-modal Representations by Watching Hundreds of Surgical   Video Lectures"></a>Learning Multi-modal Representations by Watching Hundreds of Surgical   Video Lectures</h2><p><strong>Authors:Kun Yuan, Vinkle Srivastav, Tong Yu, Joel L. Lavanchy, Jacques Marescaux, Pietro Mascagni, Nassir Navab, Nicolas Padoy</strong></p>
<p>Recent advancements in surgical computer vision applications have been driven by vision-only models, which do not explicitly integrate the rich semantics of language into their design. These methods rely on manually annotated surgical videos to predict a fixed set of object categories, limiting their generalizability to unseen surgical procedures and downstream tasks. In this work, we put forward the idea that the surgical video lectures available through open surgical e-learning platforms can provide effective vision and language supervisory signals for multi-modal representation learning without relying on manual annotations. We address the surgery-specific linguistic challenges present in surgical video lectures by employing multiple complementary automatic speech recognition systems to generate text transcriptions. We then present a novel method, SurgVLP - Surgical Vision Language Pre-training, for multi-modal representation learning. Extensive experiments across diverse surgical procedures and tasks demonstrate that the multi-modal representations learned by SurgVLP exhibit strong transferability and adaptability in surgical video analysis. Furthermore, our zero-shot evaluations highlight SurgVLPâ€™s potential as a general-purpose foundation model for surgical workflow analysis, reducing the reliance on extensive manual annotations for downstream tasks, and facilitating adaptation methods such as few-shot learning to build a scalable and data-efficient solution for various downstream surgical applications. The <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SurgVLP">training code</a> and <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/PeskaVLP">weights</a> are public. </p>
<blockquote>
<p>è¿‘æœŸå¤–ç§‘è®¡ç®—æœºè§†è§‰åº”ç”¨çš„è¿›å±•ä¸»è¦å¾—ç›Šäºä»…ä¾èµ–è§†è§‰çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨è®¾è®¡æ—¶å¹¶æ²¡æœ‰æ˜ç¡®èå…¥ä¸°å¯Œçš„è¯­è¨€è¯­ä¹‰ã€‚è¿™äº›æ–¹æ³•ä¾èµ–äºæ‰‹åŠ¨æ ‡æ³¨çš„æ‰‹æœ¯è§†é¢‘æ¥é¢„æµ‹å›ºå®šçš„ç›®æ ‡ç±»åˆ«é›†ï¼Œå¯¼è‡´å…¶éš¾ä»¥æ¨å¹¿åˆ°æœªè§è¿‡çš„æ‰‹æœ¯ç¨‹åºå’Œä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡å…¬å¼€æ‰‹æœ¯ç”µå­å­¦ä¹ å¹³å°è·å–çš„æ‰‹æœ¯è§†é¢‘è®²åº§å¯ä»¥æä¾›æœ‰æ•ˆçš„è§†è§‰å’Œè¯­è¨€ç›‘ç£ä¿¡å·ï¼Œç”¨äºå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼Œè€Œæ— éœ€ä¾èµ–æ‰‹åŠ¨æ³¨é‡Šã€‚æˆ‘ä»¬é‡‡ç”¨å¤šä¸ªäº’è¡¥çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿæ¥è§£å†³æ‰‹æœ¯è§†é¢‘è®²åº§ä¸­ç‰¹æœ‰çš„è¯­è¨€æŒ‘æˆ˜ï¼Œä»¥ç”Ÿæˆæ–‡æœ¬è½¬å½•ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºSurgVLPï¼ˆæ‰‹æœ¯è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼‰ï¼Œç”¨äºå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ã€‚åœ¨å¤šç§æ‰‹æœ¯ç¨‹åºå’Œä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSurgVLPæ‰€å­¦ä¹ çš„å¤šæ¨¡æ€è¡¨ç¤ºåœ¨æ‰‹æœ¯è§†é¢‘åˆ†æä¸­å…·æœ‰å¼ºå¤§çš„è¿ç§»æ€§å’Œé€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„é›¶æ ·æœ¬è¯„ä¼°å‡¸æ˜¾äº†SurgVLPä½œä¸ºæ‰‹æœ¯å·¥ä½œæµç¨‹åˆ†æçš„é€šç”¨åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ï¼Œå‡å°‘äº†ä¸‹æ¸¸ä»»åŠ¡å¯¹å¤§é‡æ‰‹åŠ¨æ³¨é‡Šçš„ä¾èµ–ï¼Œå¹¶ä¿ƒè¿›äº†è¯¸å¦‚å°æ ·æœ¬å­¦ä¹ ç­‰é€‚åº”æ–¹æ³•ï¼Œä¸ºå„ç§ä¸‹æ¸¸æ‰‹æœ¯åº”ç”¨æ„å»ºå¯æ‰©å±•å’Œé«˜æ•ˆçš„æ•°æ®è§£å†³æ–¹æ¡ˆã€‚<a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SurgVLP">è®­ç»ƒä»£ç </a>å’Œ<a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/PeskaVLP">æƒé‡</a>å‡å·²å…¬å¼€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15220v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤–ç§‘è®¡ç®—æœºè§†è§‰åº”ç”¨çš„è¿›å±•ä¸»è¦å¾—ç›Šäºä»…ä¾èµ–è§†è§‰çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹æ²¡æœ‰å°†ä¸°å¯Œçš„è¯­è¨€è¯­ä¹‰æ•´åˆåˆ°è®¾è®¡ä¸­ï¼Œå› æ­¤å®ƒä»¬åœ¨é¢„æµ‹æœªçŸ¥æ‰‹æœ¯ç¨‹åºå’Œä¸‹æ¸¸ä»»åŠ¡æ–¹é¢çš„æ³›åŒ–èƒ½åŠ›å—é™ã€‚æœ¬ç ”ç©¶åˆ©ç”¨å…¬å¼€å¤–ç§‘ç”µå­å­¦ä¹ å¹³å°ä¸Šçš„æ‰‹æœ¯è§†é¢‘è®²åº§ï¼Œæå‡ºä¸€ç§æ— éœ€æ‰‹åŠ¨æ³¨é‡Šçš„å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ã€‚é€šè¿‡é‡‡ç”¨å¤šç§äº’è¡¥çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼Œåº”å¯¹æ‰‹æœ¯è§†é¢‘è®²åº§ä¸­ç‰¹æœ‰çš„è¯­è¨€å­¦æŒ‘æˆ˜ï¼Œç”Ÿæˆæ–‡æœ¬è½¬å½•ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•SurgVLPâ€”â€”å¤–ç§‘è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼Œç”¨äºå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ã€‚åœ¨å¤šç§æ‰‹æœ¯ç¨‹åºå’Œä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSurgVLPæ‰€å­¦ä¹ çš„å¤šæ¨¡æ€è¡¨ç¤ºå…·æœ‰å¼ºå¤§çš„å¯è½¬ç§»æ€§å’Œé€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„é›¶æ ·æœ¬è¯„ä¼°çªæ˜¾äº†SurgVLPä½œä¸ºå¤–ç§‘æ‰‹æœ¯å·¥ä½œæµåˆ†æé€šç”¨åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ï¼Œå‡å°‘ä¸‹æ¸¸ä»»åŠ¡å¯¹å¤§é‡æ‰‹åŠ¨æ³¨é‡Šçš„ä¾èµ–ï¼Œå¹¶ä¿ƒè¿›å¦‚å°æ ·æœ¬å­¦ä¹ ç­‰é€‚åº”æ–¹æ³•ï¼Œä¸ºå„ç§ä¸‹æ¸¸å¤–ç§‘åº”ç”¨æ„å»ºå¯æ‰©å±•å’Œé«˜æ•ˆçš„æ•°æ®è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸå¤–ç§‘è®¡ç®—æœºè§†è§‰åº”ç”¨çš„è¿›å±•ä¸»è¦å¾—ç›Šäºä»…ä¾èµ–è§†è§‰çš„æ¨¡å‹ï¼Œä½†å®ƒä»¬åœ¨é¢„æµ‹æœªçŸ¥æ‰‹æœ¯ç¨‹åºå’Œä¸‹æ¸¸ä»»åŠ¡æ–¹é¢çš„æ³›åŒ–èƒ½åŠ›å—é™ã€‚</li>
<li>æ‰‹æœ¯è§†é¢‘è®²åº§æä¾›äº†æœ‰æ•ˆçš„è§†è§‰å’Œè¯­è¨€ç›‘ç£ä¿¡å·ï¼Œå¯ç”¨äºå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼Œæ— éœ€ä¾èµ–æ‰‹åŠ¨æ³¨é‡Šã€‚</li>
<li>é‡‡ç”¨å¤šç§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿåº”å¯¹æ‰‹æœ¯è§†é¢‘è®²åº§ä¸­çš„è¯­è¨€å­¦æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ–¹æ³•SurgVLPï¼Œå…·æœ‰å¼ºå¤§çš„å¯è½¬ç§»æ€§å’Œé€‚åº”æ€§ã€‚</li>
<li>SurgVLPåœ¨å¤šç§æ‰‹æœ¯ç¨‹åºå’Œä»»åŠ¡ä¸Šçš„å®éªŒè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>SurgVLPå…·æœ‰æ½œåŠ›æˆä¸ºå¤–ç§‘æ‰‹æœ¯å·¥ä½œæµåˆ†æé€šç”¨åŸºç¡€æ¨¡å‹ï¼Œå¹¶å‡å°‘ä¸‹æ¸¸ä»»åŠ¡å¯¹å¤§é‡æ‰‹åŠ¨æ³¨é‡Šçš„ä¾èµ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2307.15220">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90ed6c7b07edbbcc0c936921a71fd51f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1403ccbf9c4143513d490e722b7e05c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c3add5ad8609d1635f8f4d1ed6baed8.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-29/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-29/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-29/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d64ca03cb901b32beb4f899df70c0079.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-29  Debiasing Kernel-Based Generative Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-29/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-fb1ffc368df48c5be1483058f98def21.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-29  Flip Learning Weakly Supervised Erase to Segment Nodules in Breast   Ultrasound
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29058.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
