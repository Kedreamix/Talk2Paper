<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-03-29  COMI-LINGUA Expert Annotated Large-Scale Dataset for Multitask NLP in   Hindi-English Code-Mixing">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-228c51afc32494b560f0472c73ed6167.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    55 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-29-更新"><a href="#2025-03-29-更新" class="headerlink" title="2025-03-29 更新"></a>2025-03-29 更新</h1><h2 id="COMI-LINGUA-Expert-Annotated-Large-Scale-Dataset-for-Multitask-NLP-in-Hindi-English-Code-Mixing"><a href="#COMI-LINGUA-Expert-Annotated-Large-Scale-Dataset-for-Multitask-NLP-in-Hindi-English-Code-Mixing" class="headerlink" title="COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in   Hindi-English Code-Mixing"></a>COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in   Hindi-English Code-Mixing</h2><p><strong>Authors:Rajvee Sheth, Himanshu Beniwal, Mayank Singh</strong></p>
<p>The rapid growth of digital communication has driven the widespread use of code-mixing, particularly Hindi-English, in multilingual communities. Existing datasets often focus on romanized text, have limited scope, or rely on synthetic data, which fails to capture realworld language nuances. Human annotations are crucial for assessing the naturalness and acceptability of code-mixed text. To address these challenges, We introduce COMI-LINGUA, the largest manually annotated dataset for code-mixed text, comprising 100,970 instances evaluated by three expert annotators in both Devanagari and Roman scripts. The dataset supports five fundamental NLP tasks: Language Identification, Matrix Language Identification, Part-of-Speech Tagging, Named Entity Recognition, and Translation. We evaluate LLMs on these tasks using COMILINGUA, revealing limitations in current multilingual modeling strategies and emphasizing the need for improved code-mixed text processing capabilities. COMI-LINGUA is publically availabe at: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA">https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA</a>. </p>
<blockquote>
<p>数字通信的快速发展推动了跨语言社区中混合代码，特别是印度混合英语的广泛使用。现有的数据集往往侧重于罗马化文本，范围有限，或者依赖于合成数据，无法捕捉现实世界的语言细微差别。人类注释对于评估混合代码的自然性和可接受性至关重要。为了应对这些挑战，我们推出了COMI-LINGUA，这是混合代码最大的手动注释数据集，包含由Devanagari和罗马脚本中的三名专业注释者评估的100970个实例。该数据集支持五项基本NLP任务：语言识别、矩阵语言识别、词性标注、命名实体识别和翻译。我们使用COMI-LINGUA对这些任务进行评估大型语言模型，揭示了当前多语言建模策略的局限性，并强调了提高混合文本处理能力的必要性。COMI-LINGUA公开可用：<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA%E3%80%82">https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21670v1">PDF</a> </p>
<p><strong>Summary</strong>：随着数字通信的快速发展，多语言社区中混合代码的使用越来越普遍，尤其是印度语和英语的混合。现有的数据集主要关注罗马化文本，存在范围有限或依赖合成数据的问题，无法捕捉真实世界的语言细微差别。为了评估混合文本的自然性和可接受性，我们引入了COMI-LINGUA数据集，这是最大的手动注释的混合文本数据集，包含由三位专家注释者在Devanagari和Roman脚本中评估的10万多个实例。该数据集支持五项基本NLP任务，包括语言识别、矩阵语言识别、词性标注、命名实体识别和翻译。我们在这些任务上评估了大型语言模型，揭示了当前多语言建模策略的局限性，并强调了改进混合文本处理能力的必要性。COMI-LINGUA数据集可在huggingface.co&#x2F;datasets&#x2F;LingoIITGN&#x2F;COMI-LINGUA获取。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>数字通信的快速发展推动了多语言社区中混合代码（尤其是印度语和英语）的广泛使用。</li>
<li>现有数据集存在局限性，主要关注罗马化文本或依赖合成数据，无法捕捉真实语言的细微差别。</li>
<li>人类注释对于评估混合文本的自然性和可接受性至关重要。</li>
<li>引入了COMI-LINGUA数据集，这是最大的手动注释的混合文本数据集。</li>
<li>该数据集包含五项基本NLP任务：语言识别、矩阵语言识别、词性标注、命名实体识别和翻译。</li>
<li>在这些任务上评估大型语言模型时，揭示了当前多语言建模策略的局限性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21670">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a22753c70e23c407b69349747a309b24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-239e2704d1291dad2b651ef75ac8ce76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4617835b0d5a6e5eb509441e09e1ef5a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-41681cec68525e941da3e105173c9457.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Audio-driven-Gesture-Generation-via-Deviation-Feature-in-the-Latent-Space"><a href="#Audio-driven-Gesture-Generation-via-Deviation-Feature-in-the-Latent-Space" class="headerlink" title="Audio-driven Gesture Generation via Deviation Feature in the Latent   Space"></a>Audio-driven Gesture Generation via Deviation Feature in the Latent   Space</h2><p><strong>Authors:Jiahui Chen, Yang Huan, Runhua Shi, Chanfan Ding, Xiaoqi Mo, Siyu Xiong, Yinong He</strong></p>
<p>Gestures are essential for enhancing co-speech communication, offering visual emphasis and complementing verbal interactions. While prior work has concentrated on point-level motion or fully supervised data-driven methods, we focus on co-speech gestures, advocating for weakly supervised learning and pixel-level motion deviations. We introduce a weakly supervised framework that learns latent representation deviations, tailored for co-speech gesture video generation. Our approach employs a diffusion model to integrate latent motion features, enabling more precise and nuanced gesture representation. By leveraging weakly supervised deviations in latent space, we effectively generate hand gestures and mouth movements, crucial for realistic video production. Experiments show our method significantly improves video quality, surpassing current state-of-the-art techniques. </p>
<blockquote>
<p>手势对于增强共语沟通、提供视觉重点和补充言语交流至关重要。虽然早期的研究集中在点状运动或完全基于数据的监督方法上，但我们专注于共语手势，提倡弱监督学习和像素级运动偏差。我们引入了一个弱监督框架来学习潜在表示的偏差，专门用于共语手势视频的生成。我们的方法采用扩散模型来整合潜在的运动特征，使手势表达更加精确和细致。通过利用潜在空间中的弱监督偏差，我们可以有效地生成对真实视频制作至关重要的手势和口型运动。实验表明，我们的方法显著提高了视频质量，超越了当前最先进的技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21616v1">PDF</a> 6 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>本文强调共语手势的重要性，提出一种针对共语手势视频的弱监督学习框架。该框架利用扩散模型整合潜在运动特征，学习潜在表示偏差，以生成更精确和细微的手势表示。实验表明，该方法能有效生成手势和口型动作，显著提高了视频质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>共语手势在增强沟通、视觉强调和补充言语交互方面至关重要。</li>
<li>现有工作主要集中在点状运动或全监督数据驱动的方法上，而本文专注于共语手势。</li>
<li>提倡弱监督学习方法和像素级运动偏差。</li>
<li>引入一种弱监督框架，用于学习针对共语手势视频的潜在表示偏差。</li>
<li>使用扩散模型整合潜在运动特征，实现更精确和细微的手势表示。</li>
<li>利用弱监督下的潜在空间偏差有效生成手势和口型动作。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21616">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7138059a063342f592d75c6a9945c15b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e7115f2323a1e7d0f0297174dd40f65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d4fbe00df14c2e1d8ae7f82ff6831fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2668786f39d724ebed68e6da2cb4613f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84fc6360676a42906040df9b4ab89780.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-376cfee76908c62a587e55d9c2d8d693.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a518ed8fa81b909c27c903140a12dc32.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Magnitude-Phase-Dual-Path-Speech-Enhancement-Network-based-on-Self-Supervised-Embedding-and-Perceptual-Contrast-Stretch-Boosting"><a href="#Magnitude-Phase-Dual-Path-Speech-Enhancement-Network-based-on-Self-Supervised-Embedding-and-Perceptual-Contrast-Stretch-Boosting" class="headerlink" title="Magnitude-Phase Dual-Path Speech Enhancement Network based on   Self-Supervised Embedding and Perceptual Contrast Stretch Boosting"></a>Magnitude-Phase Dual-Path Speech Enhancement Network based on   Self-Supervised Embedding and Perceptual Contrast Stretch Boosting</h2><p><strong>Authors:Alimjan Mattursun, Liejun Wang, Yinfeng Yu, Chunyang Ma</strong></p>
<p>Speech self-supervised learning (SSL) has made great progress in various speech processing tasks, but there is still room for improvement in speech enhancement (SE). This paper presents BSP-MPNet, a dual-path framework that combines self-supervised features with magnitude-phase information for SE. The approach starts by applying the perceptual contrast stretching (PCS) algorithm to enhance the magnitude-phase spectrum. A magnitude-phase 2D coarse (MP-2DC) encoder then extracts coarse features from the enhanced spectrum. Next, a feature-separating self-supervised learning (FS-SSL) model generates self-supervised embeddings for the magnitude and phase components separately. These embeddings are fused to create cross-domain feature representations. Finally, two parallel RNN-enhanced multi-attention (REMA) mask decoders refine the features, apply them to the mask, and reconstruct the speech signal. We evaluate BSP-MPNet on the VoiceBank+DEMAND and WHAMR! datasets. Experimental results show that BSP-MPNet outperforms existing methods under various noise conditions, providing new directions for self-supervised speech enhancement research. The implementation of the BSP-MPNet code is available online\footnote[2]{<a target="_blank" rel="noopener" href="https://github.com/AlimMat/BSP-MPNet">https://github.com/AlimMat/BSP-MPNet</a>. \label{s1}} </p>
<blockquote>
<p>语音自监督学习（SSL）在各项语音处理任务中取得了巨大的进步，但在语音增强（SE）方面仍有改进空间。本文提出了BSP-MPNet，这是一个双路径框架，结合了自监督特征和幅度相位信息用于SE。该方法首先应用感知对比度拉伸（PCS）算法增强幅度相位谱。然后，幅度相位2D粗略（MP-2DC）编码器从增强后的谱中提取粗略特征。接下来，特征分离自监督学习（FS-SSL）模型分别为幅度和相位分量生成自监督嵌入。这些嵌入融合在一起以创建跨域特征表示。最后，两个并行RNN增强多注意力（REMA）掩码解码器对特征进行精炼，应用于掩码，并重建语音信号。我们在VoiceBank+DEMAND和WHAMR！数据集上评估了BSP-MPNet。实验结果表明，在各种噪声条件下，BSP-MPNet优于现有方法，为自监督语音增强研究提供了新的方向。BSP-MPNet的代码实现可在网上找到[^2]（<a target="_blank" rel="noopener" href="https://github.com/AlimMat/BSP-MPNet%E3%80%82/label%7Bs1%7D%EF%BC%89%E3%80%82">https://github.com/AlimMat/BSP-MPNet。\label{s1}）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21571v1">PDF</a> Main paper (6 pages). Accepted for publication by ICME 2025</p>
<p><strong>Summary</strong>：<br>本研究提出了一种基于自监督学习的语音增强方法BSP-MPNet，该方法结合双路径框架与幅度相位信息。实验结果表明，该方法在各种噪声条件下表现优于现有方法，为自监督语音增强研究提供了新的方向。其代码实现已在线公开。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>本研究提出了BSP-MPNet模型，一个结合自监督学习与幅度相位信息的双路径框架用于语音增强。</li>
<li>利用感知对比拉伸（PCS）算法增强幅度相位谱。</li>
<li>采用幅度相位2D粗编码（MP-2DC）从增强谱中提取特征。</li>
<li>特征分离自监督学习（FS-SSL）模型为幅度和相位成分分别生成自监督嵌入。</li>
<li>通过融合嵌入创建跨域特征表示。</li>
<li>两个并行RNN增强多注意力（REMA）掩码解码器对特征进行精炼，应用于掩码并重建语音信号。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21571">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-650354bdf53f41590d3547484dcd67fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-087672adc7be68eb8af6b977118fd85f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b3be224e99e922daef1aef825d11399.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e8cc8e2aa8be0309348ab7ed99bf475.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe45c54311ee11f075ecaafd350632f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d70835eb14cc20adf7f527710970142.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VALLR-Visual-ASR-Language-Model-for-Lip-Reading"><a href="#VALLR-Visual-ASR-Language-Model-for-Lip-Reading" class="headerlink" title="VALLR: Visual ASR Language Model for Lip Reading"></a>VALLR: Visual ASR Language Model for Lip Reading</h2><p><strong>Authors:Marshall Thomas, Edward Fish, Richard Bowden</strong></p>
<p>Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complex task requiring the interpretation of spoken language exclusively from visual cues, primarily lip movements and facial expressions. This task is especially challenging due to the absence of auditory information and the inherent ambiguity when visually distinguishing phonemes that have overlapping visemes where different phonemes appear identical on the lips. Current methods typically attempt to predict words or characters directly from these visual cues, but this approach frequently encounters high error rates due to coarticulation effects and viseme ambiguity. We propose a novel two-stage, phoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) that addresses these longstanding challenges. First, our model predicts a compact sequence of phonemes from visual inputs using a Video Transformer with a CTC head, thereby reducing the task complexity and achieving robust speaker invariance. This phoneme output then serves as the input to a fine-tuned Large Language Model (LLM), which reconstructs coherent words and sentences by leveraging broader linguistic context. Unlike existing methods that either predict words directly-often faltering on visually similar phonemes-or rely on large-scale multimodal pre-training, our approach explicitly encodes intermediate linguistic structure while remaining highly data efficient. We demonstrate state-of-the-art performance on two challenging datasets, LRS2 and LRS3, where our method achieves significant reductions in Word Error Rate (WER) achieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled data than the next best approach. </p>
<blockquote>
<p>唇读或视觉自动语音识别（V-ASR）是一项复杂的任务，它要求仅通过视觉线索（主要是嘴唇动作和面部表情）来解释口语。由于缺少听觉信息以及视觉上区分具有重叠可见语音的不同语音的固有模糊性，这项任务尤其具有挑战性。当前的方法通常尝试直接从这些视觉线索中预测单词或字符，但由于协同发音效应和可见语音的模糊性，这种方法经常遇到较高的错误率。我们提出了一种针对视觉自动语音识别（V-ASR）的两阶段、以语音为核心的新型框架，以解决这些长期存在的挑战。首先，我们的模型使用带有CTC头部的视频转换器从视觉输入中预测紧凑的语音序列，从而降低任务复杂性并实现稳健的说话者不变性。然后，这个语音输出作为微调的大型语言模型（LLM）的输入，该模型通过利用更广泛的语境重建连贯的单词和句子。与现有的直接预测单词的方法或在视觉上相似的语音上经常出错的方法不同，或者依赖于大规模多模式预训练的方法不同，我们的方法明确编码中间的语言结构，同时保持高度数据效率。我们在两个具有挑战性的数据集LRS2和LRS3上展示了最先进的表现，我们的方法在LRS3上实现了显著的单词错误率（WER）降低，尽管我们使用的标记数据比下一个最佳方法少了99.4%，但仍达到了SOTA的WER为18.7。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21408v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了视觉自动语音识别（V-ASR）的新挑战及新方法。针对仅通过视觉线索解读口语的问题，提出了一种新颖的两阶段、以音素为中心的框架。该框架使用视频变压器（Video Transformer）结合CTC头部预测紧凑的音素序列，实现稳健的说话者不变性，并将预测的音素作为精细调整的大型语言模型（LLM）的输入，重建连贯的单词和句子。该方法在LRS2和LRS3两个挑战数据集上实现了卓越的性能，显著降低了词错误率（WER），尤其是在使用较少标注数据的情况下。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Lip Reading或Visual Automatic Speech Recognition (V-ASR) 是通过视觉线索解读口语的复杂任务。</li>
<li>当前方法直接通过视觉线索预测单词或字符，但存在高误差率。</li>
<li>提出的两阶段、以音素为中心的框架使用Video Transformer和CTC头部预测音素序列，实现稳健的说话者不变性。</li>
<li>该框架使用LLM重建连贯的单词和句子，利用更广泛的语境。</li>
<li>方法在LRS2和LRS3数据集上实现卓越性能，显著减少词错误率（WER）。</li>
<li>即使在使用较少标注数据的情况下，该方法也达到了最先进的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21408">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c406b8e2a843b9da3efc6f6860129089.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56162d5dbff7fed6b08de856ff5d46f3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-77d491e0f0b54b1f0a646aa92199c12f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87e4e2c38f27967c8c496695a177bf7f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-71-2-μ-W-Speech-Recognition-Accelerator-with-Recurrent-Spiking-Neural-Network"><a href="#A-71-2-μ-W-Speech-Recognition-Accelerator-with-Recurrent-Spiking-Neural-Network" class="headerlink" title="A 71.2-$μ$W Speech Recognition Accelerator with Recurrent Spiking   Neural Network"></a>A 71.2-$μ$W Speech Recognition Accelerator with Recurrent Spiking   Neural Network</h2><p><strong>Authors:Chih-Chyau Yang, Tian-Sheuan Chang</strong></p>
<p>This paper introduces a 71.2-$\mu$W speech recognition accelerator designed for edge devices’ real-time applications, emphasizing an ultra low power design. Achieved through algorithm and hardware co-optimizations, we propose a compact recurrent spiking neural network with two recurrent layers, one fully connected layer, and a low time step (1 or 2). The 2.79-MB model undergoes pruning and 4-bit fixed-point quantization, shrinking it by 96.42% to 0.1 MB. On the hardware front, we take advantage of \textit{mixed-level pruning}, \textit{zero-skipping} and \textit{merged spike} techniques, reducing complexity by 90.49% to 13.86 MMAC&#x2F;S. The \textit{parallel time-step execution} addresses inter-time-step data dependencies and enables weight buffer power savings through weight sharing. Capitalizing on the sparse spike activity, an input broadcasting scheme eliminates zero computations, further saving power. Implemented on the TSMC 28-nm process, the design operates in real time at 100 kHz, consuming 71.2 $\mu$W, surpassing state-of-the-art designs. At 500 MHz, it has 28.41 TOPS&#x2F;W and 1903.11 GOPS&#x2F;mm$^2$ in energy and area efficiency, respectively. </p>
<blockquote>
<p>本文介绍了一款针对边缘设备实时应用设计的71.2微瓦语音识别加速器，重点强调了超低功耗设计。通过算法和硬件的协同优化，我们提出了一种紧凑的循环脉冲神经网络，包含两层循环层、一层全连接层以及较低的时间步长（1或2）。该模型大小为2.79MB，经过剪枝和4位定点量化处理，缩小了96.42%，达到仅0.1MB的大小。在硬件方面，我们利用多级剪枝、零跳过和合并脉冲等技术，将复杂度降低了90.49%，达到每秒处理13.86百万乘加操作（MMAC&#x2F;S）。并行时间步执行解决了时间步之间的数据依赖问题，并通过权重共享实现了权重缓冲器的节能。利用稀疏脉冲活动，输入广播方案消除了零计算，进一步节省了功耗。该设计采用台积电（TSMC）的28纳米工艺实现，能以实时速度100kHz运行，功耗仅为71.2微瓦，超越了当前最先进的设计。在500兆赫兹的频率下，其在能效和面积效率方面分别达到了每瓦特处理次数（TOPS&#x2F;W）为28.41和每平方毫米处理次数（GOPS&#x2F;mm²）为每秒百万指令操作（OPS）。利用超稀疏脉冲信号干扰探测这一操作以降低噪声，实际应用展示该设计可大幅提高能源效率和计算性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21337v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了一款针对边缘设备实时应用设计的71.2μW语音识别加速器，重点实现超低功耗设计。通过算法和硬件协同优化，提出了一种紧凑的循环脉冲神经网络，包含两层循环层、一层全连接层，并采用较低的时间步长（1或2）。模型经过剪枝和4位定点量化，缩小了96.42%至0.1MB。在硬件方面，利用混合级别剪枝、零跳过和合并脉冲等技术，复杂度降低了90.49%至13.86MMAC&#x2F;S。通过并行时间步执行解决跨时间步数据依赖问题，并通过权重共享实现权重缓冲区功耗节省。利用稀疏脉冲活动，输入广播方案消除了零计算，进一步节省了功耗。在TSMC 28纳米工艺上实现的设计，实时运行频率为100 kHz，功耗为71.2μW，超越了现有设计。在能量和面积效率方面，该设计在500 MHz时达到了28.41 TOPS&#x2F;W和1903.11 GOPS&#x2F;mm²。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>提出了针对边缘设备实时应用的超低功耗语音识别加速器设计。</li>
<li>通过算法和硬件协同优化，实现了紧凑的循环脉冲神经网络模型。</li>
<li>模型经过剪枝和量化，大幅缩小了模型大小。</li>
<li>利用混合级别剪枝、零跳过和合并脉冲等技术，降低了硬件复杂度。</li>
<li>采用了并行时间步执行技术，解决了数据依赖问题，并降低了功耗。</li>
<li>输入广播方案消除了零计算，进一步提升了能效。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21337">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-02a352d6c83fb43d41e2be2dd5ed8a18.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5427d68791f22e13a1a0d6f2a0bc787a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ed56e1dc97d1642e60356ffbc82570a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4893546cde0b589dcd4c656b60bcd410.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-922fa974361ba9d3b61bdc1eaedad00f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a3d09b4c8d0f8a968ca11a31fa3e2390.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a13f8ec9410e225021d3950a2e832862.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c9d0a0a2c003328db4d792350a0e9a6.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Leveraging-LLMs-with-Iterative-Loop-Structure-for-Enhanced-Social-Intelligence-in-Video-Question-Answering"><a href="#Leveraging-LLMs-with-Iterative-Loop-Structure-for-Enhanced-Social-Intelligence-in-Video-Question-Answering" class="headerlink" title="Leveraging LLMs with Iterative Loop Structure for Enhanced Social   Intelligence in Video Question Answering"></a>Leveraging LLMs with Iterative Loop Structure for Enhanced Social   Intelligence in Video Question Answering</h2><p><strong>Authors:Erika Mori, Yue Qiu, Hirokatsu Kataoka, Yoshimitsu Aoki</strong></p>
<p>Social intelligence, the ability to interpret emotions, intentions, and behaviors, is essential for effective communication and adaptive responses. As robots and AI systems become more prevalent in caregiving, healthcare, and education, the demand for AI that can interact naturally with humans grows. However, creating AI that seamlessly integrates multiple modalities, such as vision and speech, remains a challenge. Current video-based methods for social intelligence rely on general video recognition or emotion recognition techniques, often overlook the unique elements inherent in human interactions. To address this, we propose the Looped Video Debating (LVD) framework, which integrates Large Language Models (LLMs) with visual information, such as facial expressions and body movements, to enhance the transparency and reliability of question-answering tasks involving human interaction videos. Our results on the Social-IQ 2.0 benchmark show that LVD achieves state-of-the-art performance without fine-tuning. Furthermore, supplementary human annotations on existing datasets provide insights into the model’s accuracy, guiding future improvements in AI-driven social intelligence. </p>
<blockquote>
<p>社会智能，即解读情感、意图和行为的能力，对于有效沟通和适应性反应至关重要。随着机器人在护理、医疗和教育领域的使用日益普及，对能够自然与人类互动的AI的需求也在增长。然而，创建无缝集成多种模式（如视觉和语音）的AI仍然是一个挑战。当前基于视频的社会智能方法依赖于通用视频识别或情感识别技术，往往会忽略人类互动中固有的独特元素。为解决这一问题，我们提出了循环视频辩论（LVD）框架，该框架将大型语言模型（LLM）与面部表情和身体动作等视觉信息相结合，提高了涉及人类互动视频的问答任务的透明度和可靠性。我们在Social-IQ 2.0基准测试上的结果表明，LVD在不进行微调的情况下达到了最先进的性能。此外，对现有数据集进行补充人工注释，为模型的准确性提供了见解，指导未来AI驱动的社会智能的改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21190v1">PDF</a> </p>
<p><strong>Summary</strong><br>     社会智能，即解读情感、意图和行为的能力，对于有效沟通和适应反应至关重要。随着机器人和人工智能系统在护理、医疗和教育等领域的普及，对能自然与人类互动的人工智能的需求不断增长。然而，创建能无缝集成多种模式（如视觉和语音）的人工智能仍是一个挑战。为解决此问题，我们提出了循环视频辩论（LVD）框架，它将大型语言模型（LLM）与面部表情和身体动作等视觉信息相结合，提高了涉及人类互动视频的问答任务的透明度和可靠性。我们在Social-IQ 2.0基准测试上的结果表明，LVD在不进行微调的情况下即可实现最佳性能。此外，对现有数据集进行的人类附加注释提供了关于模型准确性的见解，为改进AI驱动的社会智能提供了指导。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>社会智能对于有效沟通和适应反应至关重要。</li>
<li>随着AI在多个领域的应用增长，对能自然与人类互动的智能体的需求也在增长。</li>
<li>创建能无缝集成多种模式（如视觉和语音）的AI是一个挑战。</li>
<li>提出的LVD框架结合了大型语言模型和视觉信息，提高了涉及人类互动视频的AI性能。</li>
<li>LVD框架在Social-IQ 2.0基准测试上实现了最佳性能。</li>
<li>通过人类附加注释对现有数据集进行分析，为改进AI的社会智能提供了指导。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21190">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c6da3a2a0b9c6e198fff88b81e886e21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecf190d7ae25a331145f1fa020ed2efa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ce671957ba285a8951cb5886676a62c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cec6d42f47057337904ff6f06d37db1f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae817b53913468bbc0bc9c54bfe22ff8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-000f67cabcff25a71284cb2d54fd582f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90c6e58e58bc0f4801d8bf85e496b0c0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="GatedxLSTM-A-Multimodal-Affective-Computing-Approach-for-Emotion-Recognition-in-Conversations"><a href="#GatedxLSTM-A-Multimodal-Affective-Computing-Approach-for-Emotion-Recognition-in-Conversations" class="headerlink" title="GatedxLSTM: A Multimodal Affective Computing Approach for Emotion   Recognition in Conversations"></a>GatedxLSTM: A Multimodal Affective Computing Approach for Emotion   Recognition in Conversations</h2><p><strong>Authors:Yupei Li, Qiyang Sun, Sunil Munthumoduku Krishna Murthy, Emran Alturki, Björn W. Schuller</strong></p>
<p>Affective Computing (AC) is essential for advancing Artificial General Intelligence (AGI), with emotion recognition serving as a key component. However, human emotions are inherently dynamic, influenced not only by an individual’s expressions but also by interactions with others, and single-modality approaches often fail to capture their full dynamics. Multimodal Emotion Recognition (MER) leverages multiple signals but traditionally relies on utterance-level analysis, overlooking the dynamic nature of emotions in conversations. Emotion Recognition in Conversation (ERC) addresses this limitation, yet existing methods struggle to align multimodal features and explain why emotions evolve within dialogues. To bridge this gap, we propose GatedxLSTM, a novel speech-text multimodal ERC model that explicitly considers voice and transcripts of both the speaker and their conversational partner(s) to identify the most influential sentences driving emotional shifts. By integrating Contrastive Language-Audio Pretraining (CLAP) for improved cross-modal alignment and employing a gating mechanism to emphasise emotionally impactful utterances, GatedxLSTM enhances both interpretability and performance. Additionally, the Dialogical Emotion Decoder (DED) refines emotion predictions by modelling contextual dependencies. Experiments on the IEMOCAP dataset demonstrate that GatedxLSTM achieves state-of-the-art (SOTA) performance among open-source methods in four-class emotion classification. These results validate its effectiveness for ERC applications and provide an interpretability analysis from a psychological perspective. </p>
<blockquote>
<p>情感计算（AC）对推动人工智能通用智能（AGI）的发展至关重要，情感识别是其中的关键组成部分。然而，人类情感本质上是动态的，不仅受到个人表达的影响，还受到与他人互动的影响，单一模式的方法往往无法捕捉其全部动态。多模态情感识别（MER）利用多种信号，但传统上依赖于话语层面的分析，忽视了情感对话中的动态特征。对话中的情感识别（ERC）解决了这一局限性，但现有方法难以对齐多模态特征并解释情感如何在对话中演变。为了弥补这一差距，我们提出了GatedxLSTM，这是一种新颖的语音文本多模态ERC模型，它明确考虑了说话者和他们的对话伙伴的语音和文本转录，以识别推动情感变化的最有影响力的句子。通过集成对比语言音频预训练（CLAP）以改进跨模态对齐，并采用门控机制来强调情感上有影响力的言论，GatedxLSTM提高了可解释性和性能。此外，对话情感解码器（DED）通过模拟上下文依赖性来完善情感预测。在IEMOCAP数据集上的实验表明，GatedxLSTM在四类情感分类中实现了开源方法中的最新技术表现。这些结果验证了其在ERC应用中的有效性，并从心理角度提供了可解释性分析。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20919v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了情感计算在推进人工智能通用智能方面的重要性，并指出情感识别是其中的关键组成部分。然而，人类情感具有动态性，单模态方法无法捕捉其全面信息。多模态情感识别（MER）采用多种信号，但传统方法侧重于话语层面的分析，忽略了情感对话中的动态变化。为此，提出了基于门控长短期记忆网络（GatedxLSTM）的语音文本多模态情感识别模型，该模型能明确考虑说话者及其对话伙伴的声音和文字，以识别推动情感变化的关键句子。该模型通过对比语言音频预训练（CLAP）提高跨模态对齐能力，并采用门控机制强调情感影响大的话语。此外，对话情感解码器（DED）通过模拟上下文依赖关系，提高了情感预测的准确性。在IEMOCAP数据集上的实验表明，GatedxLSTM在四类别情感分类上实现了最佳性能，验证了其在情感识别对话应用中的有效性，并从心理角度提供了可解释性分析。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>情感计算对推进人工智能通用智能至关重要，其中情感识别是关键组成部分。</li>
<li>人类情感具有动态性，单模态方法无法全面捕捉。</li>
<li>多模态情感识别（MER）采用多种信号，但传统方法忽略情感对话中的动态变化。</li>
<li>提出的GatedxLSTM模型能识别推动情感变化的关键句子，考虑说话者及其对话伙伴的声音和文字。</li>
<li>GatedxLSTM模型通过对比语言音频预训练（CLAP）提高跨模态对齐能力。</li>
<li>GatedxLSTM模型采用门控机制强调情感影响大的话语，并通过对话情感解码器（DED）提高情感预测准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20919">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fe7ef5931671d10629b669c1e4cf6445.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-544a749a584ee0efded1ba38a0fd0868.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-228c51afc32494b560f0472c73ed6167.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aab0b42d3df2dd7a775b07c383679cf1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Sparse-Alignment-Enhanced-Latent-Diffusion-Transformer-for-Zero-Shot-Speech-Synthesis"><a href="#Sparse-Alignment-Enhanced-Latent-Diffusion-Transformer-for-Zero-Shot-Speech-Synthesis" class="headerlink" title="Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot   Speech Synthesis"></a>Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot   Speech Synthesis</h2><p><strong>Authors:Ziyue Jiang, Yi Ren, Ruiqi Li, Shengpeng Ji, Boyang Zhang, Zhenhui Ye, Chen Zhang, Bai Jionghao, Xiaoda Yang, Jialong Zuo, Yu Zhang, Rui Liu, Xiang Yin, Zhou Zhao</strong></p>
<p>While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness, mainstream systems still suffer from issues related to speech-text alignment modeling: 1) models without explicit speech-text alignment modeling exhibit less robustness, especially for hard sentences in practical applications; 2) predefined alignment-based models suffer from naturalness constraints of forced alignments. This paper introduces \textit{MegaTTS 3}, a TTS system featuring an innovative sparse alignment algorithm that guides the latent diffusion transformer (DiT). Specifically, we provide sparse alignment boundaries to MegaTTS 3 to reduce the difficulty of alignment without limiting the search space, thereby achieving high naturalness. Moreover, we employ a multi-condition classifier-free guidance strategy for accent intensity adjustment and adopt the piecewise rectified flow technique to accelerate the generation process. Experiments demonstrate that MegaTTS 3 achieves state-of-the-art zero-shot TTS speech quality and supports highly flexible control over accent intensity. Notably, our system can generate high-quality one-minute speech with only 8 sampling steps. Audio samples are available at <a target="_blank" rel="noopener" href="https://sditdemo.github.io/sditdemo/">https://sditdemo.github.io/sditdemo/</a>. </p>
<blockquote>
<p>近年来，零样本文本到语音（TTS）模型在语音质量和表现力方面取得了显著改进，但主流系统仍存在与语音文本对齐建模相关的问题：1）没有明确的语音文本对齐建模的模型表现出较低的稳健性，特别是在实际应用的复杂句子中；2）基于预定义对齐的模型受到强制对齐的自然性约束。本文介绍了<em>MegaTTS 3</em>，这是一个具有创新稀疏对齐算法的TTS系统，该算法指导潜在扩散变压器（DiT）。具体来说，我们为MegaTTS 3提供稀疏对齐边界，以减少对齐难度而不限制搜索空间，从而实现高自然度。此外，我们采用多条件无分类指导策略进行口音强度调整，并采用分段校正流技术来加速生成过程。实验表明，MegaTTS 3达到了最先进的零样本TTS语音质量，并对口音强度实现了高度灵活的控制。值得注意的是，我们的系统只需8个采样步骤就能生成高质量的一分钟语音。音频样本可在<a target="_blank" rel="noopener" href="https://sditdemo.github.io/sditdemo/%E6%89%BE%E5%88%B0%E3%80%82">https://sditdemo.github.io/sditdemo/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18924v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>最新一代的零样本文本转语音（TTS）模型虽然大幅提升了语音质量和表达力，但主流系统仍存在语音与文本对齐建模的问题。本文提出一种名为MegaTTS 3的TTS系统，采用创新的稀疏对齐算法引导潜在扩散转换器（DiT）。通过提供稀疏对齐边界降低对齐难度且不限制搜索空间，提高了自然度。同时采用多条件无分类指导策略调节口音强度并采用分段整流流技术加速生成过程。实验证明MegaTTS 3实现了零样本TTS的最优语音质量，并支持高度灵活的口音强度控制。可生成仅需8步采样的一分钟高质量语音。具体参见[<a target="_blank" rel="noopener" href="https://sditdemo.github.io/sditdemo/]%E4%BA%86%E8%A7%A3%E9%9F%B3%E9%A2%91%E6%A0%B7%E6%9C%AC%E3%80%82">https://sditdemo.github.io/sditdemo/]了解音频样本。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>主流文本转语音（TTS）模型仍存在语音与文本对齐建模的问题。</li>
<li>MegaTTS 3系统采用创新的稀疏对齐算法，提高了语音自然度。</li>
<li>MegaTTS 3实现了零样本TTS的最优语音质量。</li>
<li>系统支持高度灵活的口音强度控制。</li>
<li>MegaTTS 3采用分段整流流技术以加速语音生成过程。</li>
<li>系统可生成一分钟的高质量语音，仅需要8步采样。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18924">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b83eb1e59083ca253b5f5d680b25e034.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f63c305d51b567c1b33e75f11ea31ced.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-799c5f97dff2642f57db63aacabdbaa7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4dc21c79b3bf4b146770c255229fcba.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="VERSA-A-Versatile-Evaluation-Toolkit-for-Speech-Audio-and-Music"><a href="#VERSA-A-Versatile-Evaluation-Toolkit-for-Speech-Audio-and-Music" class="headerlink" title="VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music"></a>VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music</h2><p><strong>Authors:Jiatong Shi, Hye-jin Shim, Jinchuan Tian, Siddhant Arora, Haibin Wu, Darius Petermann, Jia Qi Yip, You Zhang, Yuxun Tang, Wangyou Zhang, Dareen Safar Alharthi, Yichen Huang, Koichi Saito, Jionghao Han, Yiwen Zhao, Chris Donahue, Shinji Watanabe</strong></p>
<p>In this work, we introduce VERSA, a unified and standardized evaluation toolkit designed for various speech, audio, and music signals. The toolkit features a Pythonic interface with flexible configuration and dependency control, making it user-friendly and efficient. With full installation, VERSA offers 65 metrics with 729 metric variations based on different configurations. These metrics encompass evaluations utilizing diverse external resources, including matching and non-matching reference audio, text transcriptions, and text captions. As a lightweight yet comprehensive toolkit, VERSA is versatile to support the evaluation of a wide range of downstream scenarios. To demonstrate its capabilities, this work highlights example use cases for VERSA, including audio coding, speech synthesis, speech enhancement, singing synthesis, and music generation. The toolkit is available at <a target="_blank" rel="noopener" href="https://github.com/wavlab-speech/versa">https://github.com/wavlab-speech/versa</a>. </p>
<blockquote>
<p>在这项工作中，我们介绍了VERSA，这是一个为各种语音、音频和音乐信号设计的统一、标准化的评估工具包。该工具包具有Python风格的接口，具有灵活的配置和依赖控制，使其友好且高效。在完全安装后，VERSA提供基于不同配置的65种指标和729种指标变体。这些指标涵盖了利用多种外部资源的评估，包括匹配和非匹配的参考音频、文本转录和文本描述。作为一个轻便而全面的工具包，VERSA支持对各种下游场景的评估。为了证明其能力，这项工作重点介绍了VERSA的示例用例，包括音频编码、语音合成、语音增强、歌唱合成和音乐生成。该工具包可在<a target="_blank" rel="noopener" href="https://github.com/wavlab-speech/versa%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wavlab-speech/versa中找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.17667v2">PDF</a> </p>
<p><strong>Summary</strong><br>VERSA是一个统一标准化的评估工具包，适用于各种语音、音频和音乐信号的评价。它具备Pythonic接口、灵活的配置和依赖控制，用户友好且高效。VERSA提供65种度量指标，基于不同配置有729种度量指标变化，涵盖利用多种外部资源的评估，如匹配和非匹配参考音频、文本转录和文本字幕。它支持多种下游场景的评价。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VERSA是一个统一标准化的评价工具包，适用于语音、音频和音乐信号的评价。</li>
<li>VERSA具备Pythonic接口，具备灵活的配置和依赖控制。</li>
<li>VERSA提供65种度量指标，基于不同配置有729种度量指标变化。</li>
<li>VERSA的度量指标涵盖利用多种外部资源的评估，如匹配和非匹配参考音频、文本转录和文本字幕。</li>
<li>VERSA支持多种下游场景的评价，包括音频编码、语音合成、语音增强、歌唱合成和音乐生成等。</li>
<li>VERSA在GitHub上有公开的代码库。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.17667">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-49dd14a08aac38b6c02b55a3089ebb77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef56415eb65cfb8300e0a1913078f61c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f8e0e799de48fe8975d415cbff636bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11c60c703837d6d4aba282bfeb1cd3b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6be0d11933d9ac99b6081aaa2e0804c5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Towards-Controllable-Speech-Synthesis-in-the-Era-of-Large-Language-Models-A-Survey"><a href="#Towards-Controllable-Speech-Synthesis-in-the-Era-of-Large-Language-Models-A-Survey" class="headerlink" title="Towards Controllable Speech Synthesis in the Era of Large Language   Models: A Survey"></a>Towards Controllable Speech Synthesis in the Era of Large Language   Models: A Survey</h2><p><strong>Authors:Tianxin Xie, Yan Rong, Pengfei Zhang, Wenwu Wang, Li Liu</strong></p>
<p>Text-to-speech (TTS), also known as speech synthesis, is a prominent research area that aims to generate natural-sounding human speech from text. Recently, with the increasing industrial demand, TTS technologies have evolved beyond synthesizing human-like speech to enabling controllable speech generation. This includes fine-grained control over various attributes of synthesized speech such as emotion, prosody, timbre, and duration. In addition, advancements in deep learning, such as diffusion and large language models, have significantly enhanced controllable TTS over the past several years. In this work, we conduct a comprehensive survey of controllable TTS, covering approaches ranging from basic control techniques to methods utilizing natural language prompts, aiming to provide a clear understanding of the current state of research. We examine the general controllable TTS pipeline, challenges, model architectures, and control strategies, offering a comprehensive and clear taxonomy of existing methods. Additionally, we provide a detailed summary of datasets and evaluation metrics and shed some light on the applications and future directions of controllable TTS. To the best of our knowledge, this survey paper provides the first comprehensive review of emerging controllable TTS methods, which can serve as a beneficial resource for both academic researchers and industrial practitioners. </p>
<blockquote>
<p>文本转语音（TTS），也称为语音合成，是一个突出的研究领域，旨在从文本生成听起来很自然的人类语音。最近，随着工业需求的增加，TTS技术已经超越了合成类似人类的语音，实现了可控的语音生成。这包括对合成语音的各种属性的精细控制，如情绪、语调、音质和持续时间。此外，深度学习中的扩散和大型语言模型等技术的进步，在过去的几年里极大地增强了可控TTS。在这项工作中，我们对可控TTS进行了全面的调查，涵盖了从基本控制技术到利用自然语言提示的方法，旨在提供对当前研究状态的清晰理解。我们研究了通用的可控TTS管道、挑战、模型架构和控制策略，提供了现有方法的全面清晰的分类。此外，我们还对数据集和评估指标进行了详细的总结，并指出了可控TTS的应用和未来发展方向。据我们所知，这篇综述论文提供了对新兴的可控TTS方法的首次全面回顾，可以作为学术研究人员和工业从业者的有益资源。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.06602v2">PDF</a> A comprehensive survey on controllable TTS, 26 pages, 7 tables, 6   figures, 317 references. Under review</p>
<p><strong>Summary</strong><br>     文本转语音（TTS）又称语音合成，是生成自然流畅的人类语音的重要研究领域。近年来，随着工业需求的增加，TTS技术不仅合成类似人类的声音，还能实现可控的语音生成，包括精细控制合成语音的各种属性，如情感、语调、音质和持续时间。深度学习领域的进步，如扩散模型和大型语言模型，在过去几年中显著增强了可控TTS的性能。本文全面综述了可控TTS，涵盖从基本控制技术到利用自然语言提示的方法，旨在提供对当前研究状态的清晰理解。我们考察了可控TTS的通用流程、挑战、模型架构和控制策略，对现有方法进行了全面清晰的分类。此外，我们还详细总结了数据集和评估指标，并指出了可控TTS的应用和未来发展方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TTS（文本转语音）不仅是生成类似人类的声音，还能实现可控的语音生成。</li>
<li>可控TTS允许对合成语音的各种属性进行精细控制，如情感、语调、音质和持续时间。</li>
<li>深度学习领域的进步，如扩散模型和大型语言模型，显著增强了可控TTS的性能。</li>
<li>本文全面概述了可控TTS的研究现状，包括其通用流程、挑战、模型架构和控制策略。</li>
<li>文章详细阐述了可控TTS的数据集和评估指标。</li>
<li>可控TTS具有广泛的应用场景，并在未来具有巨大的发展潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.06602">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-77b9d1b4adf952efd84a9a0f647cb16e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6793a177689270ec51d736f2677c63a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-914ceb695b61c14f39efea85c78546a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53c531461a37a5313b4c529845b753e2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SF-Speech-Straightened-Flow-for-Zero-Shot-Voice-Clone"><a href="#SF-Speech-Straightened-Flow-for-Zero-Shot-Voice-Clone" class="headerlink" title="SF-Speech: Straightened Flow for Zero-Shot Voice Clone"></a>SF-Speech: Straightened Flow for Zero-Shot Voice Clone</h2><p><strong>Authors:Xuyuan Li, Zengqiang Shang, Hua Hua, Peiyang Shi, Chen Yang, Li Wang, Pengyuan Zhang</strong></p>
<p>Recently, neural ordinary differential equations (ODE) models trained with flow matching have achieved impressive performance on the zero-shot voice clone task. Nevertheless, postulating standard Gaussian noise as the initial distribution of ODE gives rise to numerous intersections within the fitted targets of flow matching, which presents challenges to model training and enhances the curvature of the learned generated trajectories. These curved trajectories restrict the capacity of ODE models for generating desirable samples with a few steps. This paper proposes SF-Speech, a novel voice clone model based on ODE and in-context learning. Unlike the previous works, SF-Speech adopts a lightweight multi-stage module to generate a more deterministic initial distribution for ODE. Without introducing any additional loss function, we effectively straighten the curved reverse trajectories of the ODE model by jointly training it with the proposed module. Experiment results on datasets of various scales show that SF-Speech outperforms the state-of-the-art zero-shot TTS methods and requires only a quarter of the solver steps, resulting in a generation speed approximately 3.7 times that of Voicebox and E2 TTS. Audio samples are available at the demo page\footnote{[Online] Available: <a target="_blank" rel="noopener" href="https://lixuyuan102.github.io/Demo/%7D">https://lixuyuan102.github.io/Demo/}</a>. </p>
<blockquote>
<p>最近，使用流匹配训练的神经常微分方程（ODE）模型在零样本语音克隆任务上取得了令人印象深刻的性能。然而，假设标准高斯噪声作为ODE的初始分布会导致流匹配的目标拟合中出现许多交点，这给模型训练带来了挑战，并增强了学习生成轨迹的曲率。这些弯曲的轨迹限制了ODE模型在几步内生成理想样本的能力。本文提出了SF-Speech，这是一种基于ODE和上下文学习的新型语音克隆模型。不同于以前的工作，SF-Speech采用轻量级的多阶段模块来为ODE生成更确定的初始分布。我们没有引入任何额外的损失函数，而是通过与所提出的模块进行联合训练，有效地校正了ODE模型的弯曲反向轨迹。在各种规模数据集上的实验结果表明，SF-Speech优于最新的零样本文本到语音转换方法，并且仅需要四分之一的求解器步骤，生成速度大约是Voicebox和E2 TTS的3.7倍。音频样本可在演示页面获得^[可通过以下网址获得：<a target="_blank" rel="noopener" href="https://lixuyuan102.github.io/Demo/]^%E3%80%82">https://lixuyuan102.github.io/Demo/]^。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12399v2">PDF</a> Accepted by IEEE Transactions on Audio, Speech and Language   Processing</p>
<p><strong>Summary</strong></p>
<p>神经常微分方程（ODE）模型在零样本语音克隆任务上表现出令人瞩目的性能。然而，将标准高斯噪声设为ODE的初始分布会产生诸多问题，如目标流动的匹配中出现许多交点，这给模型训练带来挑战，并增加了学习轨迹的曲率。本文提出一种基于ODE和上下文学习的全新语音克隆模型SF-Speech。不同于以往的工作，SF-Speech采用轻量级的多阶段模块来生成更确定的ODE初始分布。无需引入任何额外的损失函数，通过与所提模块联合训练，我们有效地校正了ODE模型的弯曲反向轨迹。在多种规模数据集上的实验结果表明，SF-Speech优于目前最先进的零样本TTS方法，且所需的求解器步骤仅为四分之一，生成速度大约是Voicebox和E2 TTS的3.7倍。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经常微分方程（ODE）模型在零样本语音克隆任务上表现优异。</li>
<li>将标准高斯噪声设为ODE初始分布会产生交点问题，增加模型训练难度和学习轨迹的曲率。</li>
<li>SF-Speech是一种基于ODE和上下文学习的语音克隆模型，采用轻量级多阶段模块生成更确定的初始分布。</li>
<li>SF-Speech通过联合训练校正ODE模型的弯曲反向轨迹，无需额外损失函数。</li>
<li>实验结果显示SF-Speech优于现有零样本TTS方法，求解器步骤减少至四分之一。</li>
<li>SF-Speech生成速度较快，大约是Voicebox和E2 TTS的3.7倍。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12399">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3f54ed6c1c036704cf78583341190999.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c42ed00e1a01701ddea1b8235a065904.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4806d14cab1e4eda416bd5e6b2c0f646.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bda72226910d0d2c9b20440eecf366b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-051e36ed8c714e7c8b117910852132c8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Vibravox-A-Dataset-of-French-Speech-Captured-with-Body-conduction-Audio-Sensors"><a href="#Vibravox-A-Dataset-of-French-Speech-Captured-with-Body-conduction-Audio-Sensors" class="headerlink" title="Vibravox: A Dataset of French Speech Captured with Body-conduction Audio   Sensors"></a>Vibravox: A Dataset of French Speech Captured with Body-conduction Audio   Sensors</h2><p><strong>Authors:Julien Hauret, Malo Olivier, Thomas Joubaud, Christophe Langrenne, Sarah Poirée, Véronique Zimpfer, Éric Bavu</strong></p>
<p>Vibravox is a dataset compliant with the General Data Protection Regulation (GDPR) containing audio recordings using five different body-conduction audio sensors: two in-ear microphones, two bone conduction vibration pickups, and a laryngophone. The dataset also includes audio data from an airborne microphone used as a reference. The Vibravox corpus contains 45 hours per sensor of speech samples and physiological sounds recorded by 188 participants under different acoustic conditions imposed by a high order ambisonics 3D spatializer. Annotations about the recording conditions and linguistic transcriptions are also included in the corpus. We conducted a series of experiments on various speech-related tasks, including speech recognition, speech enhancement, and speaker verification. These experiments were carried out using state-of-the-art models to evaluate and compare their performances on signals captured by the different audio sensors offered by the Vibravox dataset, with the aim of gaining a better grasp of their individual characteristics. </p>
<blockquote>
<p>Vibravox是一个符合通用数据保护条例（GDPR）要求的数据集，其中包含使用五种不同的体传音频传感器录制的音频记录：两个入耳式麦克风，两个骨传导振动拾音器和一个喉传声器。该数据集还包括用作参考的空气传播麦克风的音频数据。Vibravox语料库包含由188名参与者在由高级三维环绕声系统施加的不同声学条件下录制的每传感器45小时的语音样本和生理声音。关于录音条件和语言转录的注释也包含在语料库中。我们在各种与语音相关的任务上进行了一系列实验，包括语音识别、语音增强和语音验证。这些实验使用的是最前沿的模型，旨在评估和比较它们在Vibravox数据集提供的不同音频传感器捕获的信号上的性能，以便更好地了解它们各自的特性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.11828v4">PDF</a> 23 pages, 42 figures</p>
<p><strong>Summary</strong></p>
<p>Vibravox数据集包含符合GDPR规定的音频记录，使用了五种不同的体传音频传感器，包括两个耳内麦克风、两个骨传导振动拾音器和一个喉头话筒。此外，还包括作为参考的空气传播麦克风录制的音频数据。该数据集包含由188名参与者在由高级三维空间化程序产生不同声学条件下录制的共45小时的语音样本和生理声音记录。此外，还包括有关录音条件和语言转录的注释。研究团队对该数据集进行了一系列实验，涵盖了语音识别、语音增强和语音验证等多种任务。使用先进的模型来评估不同音频传感器性能，旨在更好地了解它们各自的特性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vibravox是一个符合GDPR规定的数据集，包含多种体传音频传感器录制的音频记录。</li>
<li>数据集包含由不同声学条件下录制的语音样本和生理声音记录。</li>
<li>数据集中还包括有关录音条件和语言转录的注释。</li>
<li>研究团队进行了包括语音识别、语音增强和语音验证在内的多种任务实验。</li>
<li>实验使用了先进的模型来评估不同音频传感器的性能。</li>
<li>数据集旨在帮助更好地理解不同音频传感器的特性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.11828">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c1805c2f9d77ee62c04165affe2299d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebf0c2475e261b89eef72f21689cce74.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-47753aa1aba71aff8361e14b9da73dcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da645be8e11a2dfad8b2c7e65d7b8d31.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88dbeb8df06a498e2d8bc742531555e7.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Whistle-Data-Efficient-Multilingual-and-Crosslingual-Speech-Recognition-via-Weakly-Phonetic-Supervision"><a href="#Whistle-Data-Efficient-Multilingual-and-Crosslingual-Speech-Recognition-via-Weakly-Phonetic-Supervision" class="headerlink" title="Whistle: Data-Efficient Multilingual and Crosslingual Speech Recognition   via Weakly Phonetic Supervision"></a>Whistle: Data-Efficient Multilingual and Crosslingual Speech Recognition   via Weakly Phonetic Supervision</h2><p><strong>Authors:Saierdaer Yusuyin, Te Ma, Hao Huang, Wenbo Zhao, Zhijian Ou</strong></p>
<p>There exist three approaches for multilingual and crosslingual automatic speech recognition (MCL-ASR) - supervised pretraining with phonetic or graphemic transcription, and self-supervised pretraining. We find that pretraining with phonetic supervision has been underappreciated so far for MCL-ASR, while conceptually it is more advantageous for information sharing between different languages. This paper explores the approach of pretraining with weakly phonetic supervision towards data-efficient MCL-ASR, which is called Whistle. We relax the requirement of gold-standard human-validated phonetic transcripts, and obtain International Phonetic Alphabet (IPA) based transcription by leveraging the LanguageNet grapheme-to-phoneme (G2P) models. We construct a common experimental setup based on the CommonVoice dataset, called CV-Lang10, with 10 seen languages and 2 unseen languages. A set of experiments are conducted on CV-Lang10 to compare, as fair as possible, the three approaches under the common setup for MCL-ASR. Experiments demonstrate the advantages of phoneme-based models (Whistle) for MCL-ASR, in terms of speech recognition for seen languages, crosslingual performance for unseen languages with different amounts of few-shot data, overcoming catastrophic forgetting, and training efficiency. It is found that when training data is more limited, phoneme supervision can achieve better results compared to subword supervision and self-supervision, thereby providing higher data-efficiency. To support reproducibility and promote future research along this direction, we release the code, models and data for the entire pipeline of Whistle at <a target="_blank" rel="noopener" href="https://github.com/thu-spmi/CAT/tree/master/egs/cv-lang10">https://github.com/thu-spmi/CAT/tree/master/egs/cv-lang10</a>. </p>
<blockquote>
<p>对于多语言和跨语言自动语音识别（MCL-ASR），存在三种方法：基于语音或字形转录的有监督预训练以及自监督预训练。我们发现迄今为止，对于MCL-ASR而言，有监督的语音预训练一直被低估了，虽然在概念上它对于不同语言间的信息共享更为有利。本文探讨了弱语音监督的预训练方法，以提高数据效率的多语言自动语音识别效率，这被称为“哨子”。我们放宽了金标准人工验证的语音转录要求，并利用LanguageNet的字形到音素（G2P）模型获得基于国际音标（IPA）的转录。我们在CommonVoice数据集的基础上构建了一个通用实验设置，称为CV-Lang10，包含10种已知语言和2种未知语言。在CV-Lang10上进行一系列实验，尽可能公平地比较三种MCL-ASR方法。实验证明了基于音素的模型（哨子）在MCL-ASR方面的优势，包括识别已知语言的语音、对具有不同数量少量数据的未知语言的跨语言性能、克服灾难性遗忘和提高训练效率。发现当训练数据更加有限时，与单词监督和自监督相比，音素监督可以达到更好的效果，从而提高了数据效率。为了支持可复制性和促进未来在这一方向的研究，我们在<a target="_blank" rel="noopener" href="https://github.com/thu-spmi/CAT/tree/master/egs/cv-lang10">https://github.com/thu-spmi/CAT/tree/master/egs/cv-lang10</a>发布了哨子整个管道的代码、模型和数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02166v2">PDF</a> Accepted by IEEE-TASLP</p>
<p><strong>摘要</strong><br>该论文探索了使用弱语音监督进行多语言和跨语言自动语音识别（MCL-ASR）的方法，即称为“哨声”（Whistle）的技术。它通过使用LanguageNet的字母到语音（G2P）模型来获得基于国际音标（IPA）的转录，从而放宽了对金标准人类验证语音转录的要求。论文构建了一个基于CommonVoice数据集的实验设置CV-Lang10，包含10种已知语言和两种未知语言。一系列实验在CV-Lang10上进行，以尽可能公平的方式比较三种MCL-ASR方法。实验表明，基于音素模型的哨声（Whistle）在MCL-ASR方面具有优势，特别是在已知语言的语音识别、不同少量数据的未知语言的跨语言性能、克服灾难性遗忘和提高训练效率方面。研究还发现，当训练数据有限时，音素监督可以实现比子词监督和自我监督更好的结果，从而提高了数据效率。为了支持可重复性和促进这一方向上的未来研究，我们发布了哨声整个流程的代码、模型和数据。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>论文介绍了三种多语言和跨语言自动语音识别（MCL-ASR）的方法，包括使用语音或字母转录的有监督预训练以及自监督预训练。</li>
<li>研究发现，迄今为止对使用语音监督进行MCL-ASR的评估不足，但从概念上讲，它对不同语言之间的信息共享更有利。</li>
<li>论文通过引入弱语音监督方法（称为“哨声”）探索了数据高效MCL-ASR的可行性。通过利用LanguageNet的字母到语音（G2P）模型获得国际音标（IPA）为基础的转录，降低了对人类验证语音转录的高标准需求。</li>
<li>实验表明，基于音素的模型（哨声）在MCL-ASR方面具有优势，特别是在已知语言的语音识别和未知语言的跨语言性能上。</li>
<li>当训练数据有限时，音素监督相较于子词监督和自监督能更好地实现结果，表现出较高的数据效率。</li>
<li>论文构建了一个实验设置CV-Lang10来比较三种MCL-ASR方法在各种情况下的表现。该设置基于CommonVoice数据集，包含多种语言和不同数量的少数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.02166">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-80fb18902e5db09ed16f2f996824fde0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de896ec51cbddef9b695822ccb19bc9d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45d66f7e06e1909abb010bd885538119.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Learning-Multi-modal-Representations-by-Watching-Hundreds-of-Surgical-Video-Lectures"><a href="#Learning-Multi-modal-Representations-by-Watching-Hundreds-of-Surgical-Video-Lectures" class="headerlink" title="Learning Multi-modal Representations by Watching Hundreds of Surgical   Video Lectures"></a>Learning Multi-modal Representations by Watching Hundreds of Surgical   Video Lectures</h2><p><strong>Authors:Kun Yuan, Vinkle Srivastav, Tong Yu, Joel L. Lavanchy, Jacques Marescaux, Pietro Mascagni, Nassir Navab, Nicolas Padoy</strong></p>
<p>Recent advancements in surgical computer vision applications have been driven by vision-only models, which do not explicitly integrate the rich semantics of language into their design. These methods rely on manually annotated surgical videos to predict a fixed set of object categories, limiting their generalizability to unseen surgical procedures and downstream tasks. In this work, we put forward the idea that the surgical video lectures available through open surgical e-learning platforms can provide effective vision and language supervisory signals for multi-modal representation learning without relying on manual annotations. We address the surgery-specific linguistic challenges present in surgical video lectures by employing multiple complementary automatic speech recognition systems to generate text transcriptions. We then present a novel method, SurgVLP - Surgical Vision Language Pre-training, for multi-modal representation learning. Extensive experiments across diverse surgical procedures and tasks demonstrate that the multi-modal representations learned by SurgVLP exhibit strong transferability and adaptability in surgical video analysis. Furthermore, our zero-shot evaluations highlight SurgVLP’s potential as a general-purpose foundation model for surgical workflow analysis, reducing the reliance on extensive manual annotations for downstream tasks, and facilitating adaptation methods such as few-shot learning to build a scalable and data-efficient solution for various downstream surgical applications. The <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SurgVLP">training code</a> and <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/PeskaVLP">weights</a> are public. </p>
<blockquote>
<p>近期外科计算机视觉应用的进展主要得益于仅依赖视觉的模型，这些模型在设计时并没有明确融入丰富的语言语义。这些方法依赖于手动标注的手术视频来预测固定的目标类别集，导致其难以推广到未见过的手术程序和下游任务。在此工作中，我们提出通过公开手术电子学习平台获取的手术视频讲座可以提供有效的视觉和语言监督信号，用于多模态表示学习，而无需依赖手动注释。我们采用多个互补的自动语音识别系统来解决手术视频讲座中特有的语言挑战，以生成文本转录。然后，我们提出了一种新的方法，称为SurgVLP（手术视觉语言预训练），用于多模态表示学习。在多种手术程序和任务上的广泛实验表明，SurgVLP所学习的多模态表示在手术视频分析中具有强大的迁移性和适应性。此外，我们的零样本评估凸显了SurgVLP作为手术工作流程分析的通用基础模型的潜力，减少了下游任务对大量手动注释的依赖，并促进了诸如小样本学习等适应方法，为各种下游手术应用构建可扩展和高效的数据解决方案。<a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SurgVLP">训练代码</a>和<a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/PeskaVLP">权重</a>均已公开。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15220v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期外科计算机视觉应用的进展主要得益于仅依赖视觉的模型。然而，这些模型没有将丰富的语言语义整合到设计中，因此它们在预测未知手术程序和下游任务方面的泛化能力受限。本研究利用公开外科电子学习平台上的手术视频讲座，提出一种无需手动注释的多模态表示学习方法。通过采用多种互补的自动语音识别系统，应对手术视频讲座中特有的语言学挑战，生成文本转录。然后，我们提出了一种新的方法SurgVLP——外科视觉语言预训练，用于多模态表示学习。在多种手术程序和任务上的实验表明，SurgVLP所学习的多模态表示具有强大的可转移性和适应性。此外，我们的零样本评估突显了SurgVLP作为外科手术工作流分析通用基础模型的潜力，减少下游任务对大量手动注释的依赖，并促进如小样本学习等适应方法，为各种下游外科应用构建可扩展和高效的数据解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近期外科计算机视觉应用的进展主要得益于仅依赖视觉的模型，但它们在预测未知手术程序和下游任务方面的泛化能力受限。</li>
<li>手术视频讲座提供了有效的视觉和语言监督信号，可用于多模态表示学习，无需依赖手动注释。</li>
<li>采用多种自动语音识别系统应对手术视频讲座中的语言学挑战。</li>
<li>提出了一种新的多模态表示学习方法SurgVLP，具有强大的可转移性和适应性。</li>
<li>SurgVLP在多种手术程序和任务上的实验证明了其有效性。</li>
<li>SurgVLP具有潜力成为外科手术工作流分析通用基础模型，并减少下游任务对大量手动注释的依赖。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2307.15220">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-90ed6c7b07edbbcc0c936921a71fd51f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1403ccbf9c4143513d490e722b7e05c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c3add5ad8609d1635f8f4d1ed6baed8.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-29/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-29/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-29/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d64ca03cb901b32beb4f899df70c0079.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-03-29  Debiasing Kernel-Based Generative Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-29/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-fb1ffc368df48c5be1483058f98def21.jpg" class="responsive-img" alt="医学影像/Breast Ultrasound">
                        
                        <span class="card-title">医学影像/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学影像/Breast Ultrasound 方向最新论文已更新，请持续关注 Update in 2025-03-29  Flip Learning Weakly Supervised Erase to Segment Nodules in Breast   Ultrasound
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    医学影像/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">医学影像/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29058.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
