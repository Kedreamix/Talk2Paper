<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-29  GateLens A Reasoning-Enhanced LLM Agent for Automotive Software Release   Analytics">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-89cc0906c5d63612e405868f2b912cfb.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    58 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-29-æ›´æ–°"><a href="#2025-03-29-æ›´æ–°" class="headerlink" title="2025-03-29 æ›´æ–°"></a>2025-03-29 æ›´æ–°</h1><h2 id="GateLens-A-Reasoning-Enhanced-LLM-Agent-for-Automotive-Software-Release-Analytics"><a href="#GateLens-A-Reasoning-Enhanced-LLM-Agent-for-Automotive-Software-Release-Analytics" class="headerlink" title="GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release   Analytics"></a>GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release   Analytics</h2><p><strong>Authors:Arsham Gholamzadeh Khoee, Shuai Wang, Yinan Yu, Robert Feldt, Dhasarathy Parthasarathy</strong></p>
<p>Ensuring the reliability and effectiveness of software release decisions is critical, particularly in safety-critical domains like automotive systems. Precise analysis of release validation data, often presented in tabular form, plays a pivotal role in this process. However, traditional methods that rely on manual analysis of extensive test datasets and validation metrics are prone to delays and high costs. Large Language Models (LLMs) offer a promising alternative but face challenges in analytical reasoning, contextual understanding, handling out-of-scope queries, and processing structured test data consistently; limitations that hinder their direct application in safety-critical scenarios. This paper introduces GateLens, an LLM-based tool for analyzing tabular data in the automotive domain. GateLens translates natural language queries into Relational Algebra (RA) expressions and then generates optimized Python code. It outperforms the baseline system on benchmarking datasets, achieving higher F1 scores and handling complex and ambiguous queries with greater robustness. Ablation studies confirm the critical role of the RA module, with performance dropping sharply when omitted. Industrial evaluations reveal that GateLens reduces analysis time by over 80% while maintaining high accuracy and reliability. As demonstrated by presented results, GateLens achieved high performance without relying on few-shot examples, showcasing strong generalization across various query types from diverse company roles. Insights from deploying GateLens with a partner automotive company offer practical guidance for integrating AI into critical workflows such as release validation. Results show that by automating test result analysis, GateLens enables faster, more informed, and dependable release decisions, and can thus advance software scalability and reliability in automotive systems. </p>
<blockquote>
<p>ç¡®ä¿è½¯ä»¶å‘å¸ƒå†³ç­–çš„å¯ä¿¡åº¦å’Œæœ‰æ•ˆæ€§è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æ±½è½¦ç³»ç»Ÿè¿™æ ·çš„å®‰å…¨å…³é”®é¢†åŸŸã€‚å‘å¸ƒéªŒè¯æ•°æ®çš„ç²¾ç¡®åˆ†æåœ¨æ­¤è¿‡ç¨‹ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œè¿™äº›æ•°æ®é€šå¸¸å‘ˆç°ä¸ºè¡¨æ ¼å½¢å¼ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ–¹æ³•ä¾èµ–äºå¯¹å¤§é‡æµ‹è¯•æ•°æ®é›†å’ŒéªŒè¯æŒ‡æ ‡çš„æ‰‹åŠ¨åˆ†æï¼Œå®¹æ˜“å‡ºç°å»¶è¿Ÿå’Œæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†åœ¨åˆ†ææ¨ç†ã€ä¸Šä¸‹æ–‡ç†è§£ã€å¤„ç†è¶…å‡ºèŒƒå›´æŸ¥è¯¢ä»¥åŠå¤„ç†ç»“æ„åŒ–æµ‹è¯•æ•°æ®çš„è¿è´¯æ€§æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼›è¿™äº›å±€é™æ€§é˜»ç¢äº†å®ƒä»¬åœ¨å®‰å…¨å…³é”®åœºæ™¯ä¸­çš„ç›´æ¥åº”ç”¨ã€‚æœ¬æ–‡ä»‹ç»äº†GateLensï¼Œä¸€ä¸ªåŸºäºLLMçš„æ±½è½¦é¢†åŸŸè¡¨æ ¼æ•°æ®åˆ†æå·¥å…·ã€‚GateLenså°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç¿»è¯‘æˆå…³ç³»ä»£æ•°ï¼ˆRAï¼‰è¡¨è¾¾å¼ï¼Œç„¶åç”Ÿæˆä¼˜åŒ–çš„Pythonä»£ç ã€‚å®ƒåœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºåŸºå‡†ç³»ç»Ÿï¼Œå®ç°äº†æ›´é«˜çš„F1åˆ†æ•°ï¼Œå¹¶æ›´ç¨³å¥åœ°å¤„ç†å¤æ‚å’Œæ¨¡ç³ŠæŸ¥è¯¢ã€‚æ¶ˆèç ”ç©¶è¯å®äº†RAæ¨¡å—çš„å…³é”®ä½œç”¨ï¼Œåœ¨çœç•¥è¯¥æ¨¡å—æ—¶æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚å·¥ä¸šè¯„ä¼°è¡¨æ˜ï¼ŒGateLenså°†åˆ†ææ—¶é—´å‡å°‘äº†80%ä»¥ä¸Šï¼ŒåŒæ—¶ä¿æŒäº†é«˜å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚æ‰€å‘ˆç°çš„ç»“æœè¯æ˜ï¼ŒGateLensåœ¨ä¸éœ€è¦ä¾èµ–å°‘é‡ç¤ºä¾‹çš„æƒ…å†µä¸‹å®ç°äº†é«˜æ€§èƒ½ï¼Œå±•ç¤ºäº†åœ¨å„ç§æŸ¥è¯¢ç±»å‹å’Œå…¬å¸è§’è‰²ä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡ä¸ä¸€å®¶æ±½è½¦åˆä½œä¼™ä¼´å…¬å¸éƒ¨ç½²GateLensæ‰€è·å¾—çš„è§è§£ï¼Œä¸ºå°†äººå·¥æ™ºèƒ½é›†æˆåˆ°å…³é”®å·¥ä½œæµç¨‹ï¼ˆå¦‚å‘å¸ƒéªŒè¯ï¼‰ä¸­æä¾›äº†å®é™…æŒ‡å¯¼ã€‚ç»“æœè¡¨æ˜ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æµ‹è¯•ç»“æœåˆ†æï¼ŒGateLensèƒ½å¤Ÿåšå‡ºæ›´å¿«ã€æ›´æ˜æ™ºã€æ›´å¯é çš„å‘å¸ƒå†³ç­–ï¼Œä»è€Œæ¨åŠ¨æ±½è½¦ç³»ç»Ÿè½¯ä»¶çš„å¯æ‰©å±•æ€§å’Œå¯é æ€§æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21735v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€æ¬¾åä¸ºGateLensçš„åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å·¥å…·ï¼Œç”¨äºåˆ†ææ±½è½¦é¢†åŸŸçš„è¡¨æ ¼æ•°æ®ã€‚GateLensèƒ½å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç¿»è¯‘æˆå…³ç³»ä»£æ•°ï¼ˆRAï¼‰è¡¨è¾¾å¼ï¼Œå¹¶ç”Ÿæˆä¼˜åŒ–çš„Pythonä»£ç ï¼Œå®ç°å¯¹æ±½è½¦ç³»ç»Ÿæµ‹è¯•æ•°æ®çš„å¿«é€Ÿåˆ†æã€‚ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒGateLensè¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿåœ¨å‡å°‘80%åˆ†ææ—¶é—´çš„åŒæ—¶ç»´æŒé«˜å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚é€šè¿‡åœ¨å®é™…å·¥ä¸šç¯å¢ƒä¸­çš„éƒ¨ç½²è¯„ä»·ï¼ŒéªŒè¯äº†GateLensåœ¨æé«˜è½¯ä»¶å‘å¸ƒå†³ç­–çš„é€Ÿåº¦å’Œå¯é æ€§æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è½¯ä»¶å‘å¸ƒå†³ç­–åœ¨å®‰å…¨æ€§å…³é”®é¢†åŸŸå¦‚æ±½è½¦ç³»ç»Ÿä¸­è‡³å…³é‡è¦ï¼Œéœ€è¦ç²¾ç¡®åˆ†æå‘å¸ƒéªŒè¯æ•°æ®ã€‚</li>
<li>ä¼ ç»Ÿæ‰‹åŠ¨åˆ†ææ–¹æ³•å­˜åœ¨å»¶è¿Ÿå’Œæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†é¢ä¸´è§£æå¤æ‚æ•°æ®çš„æŒ‘æˆ˜ã€‚</li>
<li>GateLensæ˜¯ä¸€æ¬¾åŸºäºLLMçš„å·¥å…·ï¼Œç”¨äºåˆ†ææ±½è½¦é¢†åŸŸçš„è¡¨æ ¼æ•°æ®ï¼Œå¯å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬åŒ–ä¸ºå…³ç³»ä»£æ•°è¡¨è¾¾å¼å¹¶ç”ŸæˆPythonä»£ç ã€‚</li>
<li>GateLensåœ¨åŸºå‡†æµ‹è¯•æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬é«˜F1åˆ†æ•°å’Œç¨³å¥å¤„ç†å¤æ‚æ¨¡ç³ŠæŸ¥è¯¢çš„èƒ½åŠ›ã€‚</li>
<li>å·¥ä¸šè¯„ä¼°æ˜¾ç¤ºï¼ŒGateLenså‡å°‘äº†è¶…è¿‡80%çš„åˆ†ææ—¶é—´å¹¶ä¿æŒé«˜å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-969864f4b13b8661d59a174baefc12a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cf232579565cb8bffe5338a1b5c0030.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a00367f5e0b3c08cb2b51990be17ff9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca2772be09acc17b6ffd015d49b82cbc.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Cooking-Task-Planning-using-LLM-and-Verified-by-Graph-Network"><a href="#Cooking-Task-Planning-using-LLM-and-Verified-by-Graph-Network" class="headerlink" title="Cooking Task Planning using LLM and Verified by Graph Network"></a>Cooking Task Planning using LLM and Verified by Graph Network</h2><p><strong>Authors:Ryunosuke Takebayashi, Vitor Hideyo Isume, Takuya Kiyokawa, Weiwei Wan, Kensuke Harada</strong></p>
<p>Cooking tasks remain a challenging problem for robotics due to their complexity. Videos of people cooking are a valuable source of information for such task, but introduces a lot of variability in terms of how to translate this data to a robotic environment. This research aims to streamline this process, focusing on the task plan generation step, by using a Large Language Model (LLM)-based Task and Motion Planning (TAMP) framework to autonomously generate cooking task plans from videos with subtitles, and execute them. Conventional LLM-based task planning methods are not well-suited for interpreting the cooking video data due to uncertainty in the videos, and the risk of hallucination in its output. To address both of these problems, we explore using LLMs in combination with Functional Object-Oriented Networks (FOON), to validate the plan and provide feedback in case of failure. This combination can generate task sequences with manipulation motions that are logically correct and executable by a robot. We compare the execution of the generated plans for 5 cooking recipes from our approach against the plans generated by a few-shot LLM-only approach for a dual-arm robot setup. It could successfully execute 4 of the plans generated by our approach, whereas only 1 of the plans generated by solely using the LLM could be executed. </p>
<blockquote>
<p>çƒ¹é¥ªä»»åŠ¡å¯¹äºæœºå™¨äººæ¥è¯´ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰å¤æ‚æ€§ã€‚äººä»¬çƒ¹é¥ªçš„è§†é¢‘æ˜¯æ­¤ç±»ä»»åŠ¡çš„æœ‰ä»·å€¼çš„ä¿¡æ¯æ¥æºï¼Œä½†å¦‚ä½•å°†è¿™äº›æ•°æ®è½¬åŒ–ä¸ºæœºå™¨äººç¯å¢ƒå´å­˜åœ¨å¤§é‡çš„å˜æ•°ã€‚æœ¬ç ”ç©¶æ—¨åœ¨ç®€åŒ–è¿™ä¸€è¿‡ç¨‹ï¼Œé‡ç‚¹ç ”ç©¶ä»»åŠ¡è®¡åˆ’ç”Ÿæˆæ­¥éª¤ï¼Œåˆ©ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»»åŠ¡ä¸è¿åŠ¨è§„åˆ’ï¼ˆTAMPï¼‰æ¡†æ¶ï¼Œä»å¸¦æœ‰å­—å¹•çš„è§†é¢‘ä¸­è‡ªä¸»ç”Ÿæˆçƒ¹é¥ªä»»åŠ¡è®¡åˆ’å¹¶æ‰§è¡Œã€‚ä¼ ç»Ÿçš„åŸºäºLLMçš„ä»»åŠ¡è§„åˆ’æ–¹æ³•ä¸é€‚åˆè§£é‡Šçƒ¹é¥ªè§†é¢‘æ•°æ®ï¼Œå› ä¸ºè§†é¢‘ä¸­å­˜åœ¨ä¸ç¡®å®šæ€§ï¼Œä¸”å…¶è¾“å‡ºå­˜åœ¨å¹»è§‰é£é™©ã€‚ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢å°†LLMä¸é¢å‘åŠŸèƒ½çš„ç½‘ç»œï¼ˆFOONï¼‰ç›¸ç»“åˆï¼Œä»¥éªŒè¯è®¡åˆ’å¹¶åœ¨å¤±è´¥æ—¶æä¾›åé¦ˆã€‚è¿™ç§ç»“åˆå¯ä»¥ç”Ÿæˆé€»è¾‘æ­£ç¡®ä¸”æœºå™¨äººå¯æ‰§è¡Œçš„å¸¦æœ‰æ“ä½œè¿åŠ¨çš„ä»»åŠ¡åºåˆ—ã€‚æˆ‘ä»¬å°†é€šè¿‡æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä½¿ç”¨å‡ æ¬¡å°„å‡»LLMçš„æ–¹æ³•ç”Ÿæˆçš„è®¡åˆ’å¯¹æ¯”ï¼Œæ‰§è¡Œäº†äº”ç§çƒ¹é¥ªé£Ÿè°±çš„è®¡åˆ’ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„è®¡åˆ’ä¸­æœ‰å››ä¸ªæˆåŠŸæ‰§è¡Œï¼Œè€Œä»…ä½¿ç”¨LLMç”Ÿæˆçš„è®¡åˆ’ä¸­åªæœ‰ä¸€ä¸ªå¯ä»¥æ‰§è¡Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21564v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œä»»åŠ¡ä¸è¿åŠ¨è§„åˆ’ï¼ˆTAMPï¼‰æ¡†æ¶ï¼Œä»å¸¦æœ‰å­—å¹•çš„çƒ¹é¥ªè§†é¢‘è‡ªä¸»ç”Ÿæˆä»»åŠ¡è®¡åˆ’ï¼Œå¹¶æ‰§è¡Œçš„æµç¨‹ã€‚é’ˆå¯¹çƒ¹é¥ªè§†é¢‘æ•°æ®çš„ä¸ç¡®å®šæ€§åŠè¾“å‡ºä¸­çš„å¹»è§†é£é™©ï¼Œç»“åˆåŠŸèƒ½é¢å‘å¯¹è±¡ç½‘ç»œï¼ˆFOONï¼‰è¿›è¡Œè®¡åˆ’éªŒè¯å’Œåé¦ˆã€‚è¯¥ç»„åˆèƒ½ä¸ºæœºå™¨äººç”Ÿæˆé€»è¾‘æ­£ç¡®ä¸”å¯æ‰§è¡Œçš„æ“æ§åŠ¨ä½œåºåˆ—ã€‚åœ¨åŒæœºæ¢°è‡‚æœºå™¨äººè®¾ç½®ä¸Šï¼Œå¯¹æ¯”äº†ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä½¿ç”¨å°‘æ•°LLMç”Ÿæˆè®¡åˆ’çš„æ‰§è¡Œæ•ˆæœï¼Œæˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸæ‰§è¡Œäº†4ä¸ªè®¡åˆ’ï¼Œè€Œä»…ä½¿ç”¨LLMçš„è®¡åˆ’ä»…æœ‰1ä¸ªå¯ä»¥æ‰§è¡Œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çƒ¹é¥ªä»»åŠ¡å¯¹æœºå™¨äººè€Œè¨€ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦å¤æ‚çš„æŠ€æœ¯è§£å†³ã€‚</li>
<li>çƒ¹é¥ªè§†é¢‘æ˜¯ä¸€ä¸ªæœ‰ä»·å€¼çš„ä¿¡æ¯æ¥æºï¼Œä½†å°†å…¶è½¬åŒ–ä¸ºæœºå™¨äººç¯å¢ƒå­˜åœ¨å¾ˆå¤§çš„å˜æ•°ã€‚</li>
<li>ç ”ç©¶æ—¨åœ¨é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œä»»åŠ¡ä¸è¿åŠ¨è§„åˆ’ï¼ˆTAMPï¼‰æ¡†æ¶ç®€åŒ–è¿™ä¸€è¿‡ç¨‹ã€‚</li>
<li>ç»“åˆåŠŸèƒ½é¢å‘å¯¹è±¡ç½‘ç»œï¼ˆFOONï¼‰å¯ä»¥è§£å†³è§†é¢‘æ•°æ®çš„ä¸ç¡®å®šæ€§åŠå¹»è§†è¾“å‡ºé—®é¢˜ã€‚</li>
<li>è¯¥ç»„åˆèƒ½ä¸ºæœºå™¨äººç”Ÿæˆé€»è¾‘æ­£ç¡®ä¸”å¯æ‰§è¡Œçš„æ“æ§åŠ¨ä½œåºåˆ—ã€‚</li>
<li>å¯¹æ¯”å®éªŒæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰§è¡Œçƒ¹é¥ªä»»åŠ¡æ—¶è¡¨ç°å‡ºæ›´é«˜çš„æˆåŠŸç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21564">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dbd8e7f54efd5d78fec88964b5416740.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a88ba3c630dbc935673e5e1691e319ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcc537641b95ff330eb8f92ded375aea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60aa728776b08fde00b74dbda87a9318.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0adeb10cca0ed42cc1387419dd9c626.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f15a74f434c119a266a238fd847060e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a55caa1e056d5e0b41c04e668ecf12a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb51d35d316c5904561cb3df16508de5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec5db3ad946472667a00ccad288d5ee4.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="From-User-Preferences-to-Optimization-Constraints-Using-Large-Language-Models"><a href="#From-User-Preferences-to-Optimization-Constraints-Using-Large-Language-Models" class="headerlink" title="From User Preferences to Optimization Constraints Using Large Language   Models"></a>From User Preferences to Optimization Constraints Using Large Language   Models</h2><p><strong>Authors:Manuela Sanguinetti, Alessandra Perniciano, Luca Zedda, Andrea Loddo, Cecilia Di Ruberto, Maurizio Atzori</strong></p>
<p>This work explores using Large Language Models (LLMs) to translate user preferences into energy optimization constraints for home appliances. We describe a task where natural language user utterances are converted into formal constraints for smart appliances, within the broader context of a renewable energy community (REC) and in the Italian scenario. We evaluate the effectiveness of various LLMs currently available for Italian in translating these preferences resorting to classical zero-shot, one-shot, and few-shot learning settings, using a pilot dataset of Italian user requests paired with corresponding formal constraint representation. Our contributions include establishing a baseline performance for this task, publicly releasing the dataset and code for further research, and providing insights on observed best practices and limitations of LLMs in this particular domain </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†ç”¨æˆ·åå¥½ç¿»è¯‘ä¸ºå®¶ç”¨ç”µå™¨èƒ½æºä¼˜åŒ–çº¦æŸçš„ä»»åŠ¡ã€‚åœ¨å¯å†ç”Ÿèƒ½æºç¤¾åŒºï¼ˆRECï¼‰çš„å®è§‚èƒŒæ™¯ä¸‹ï¼Œä»¥åŠåœ¨æ„å¤§åˆ©åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬æè¿°äº†ä¸€ç§å°†è‡ªç„¶è¯­è¨€ç”¨æˆ·è¡¨è¾¾è½¬æ¢ä¸ºæ™ºèƒ½ç”µå™¨çš„æ­£å¼çº¦æŸçš„ä»»åŠ¡ã€‚æˆ‘ä»¬è¯„ä¼°äº†ç›®å‰å¯ç”¨äºæ„å¤§åˆ©è¯­çš„å¤šç§LLMåœ¨æ­¤ç±»åå¥½ç¿»è¯‘ä¸­çš„æœ‰æ•ˆæ€§ï¼Œé‡‡ç”¨ç»å…¸çš„é›¶æ ·æœ¬ã€å•æ ·æœ¬å’Œå°‘é‡æ ·æœ¬å­¦ä¹ åœºæ™¯ï¼Œå¹¶ä½¿ç”¨é…æœ‰ç›¸åº”æ­£å¼çº¦æŸè¡¨ç¤ºçš„æ„å¤§åˆ©ç”¨æˆ·è¯·æ±‚è¯•ç‚¹æ•°æ®é›†ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬ä¸ºè¿™é¡¹ä»»åŠ¡è®¾å®šåŸºå‡†æ€§èƒ½ï¼Œå…¬å¼€å‘å¸ƒæ•°æ®é›†å’Œä»£ç ä»¥ä¾›è¿›ä¸€æ­¥ç ”ç©¶ï¼Œå¹¶æä¾›å…³äºåœ¨æ­¤ç‰¹å®šé¢†åŸŸä¸­è§‚å¯Ÿåˆ°çš„æœ€ä½³å®è·µå’ŒLLMå±€é™æ€§çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21360v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å°†ç”¨æˆ·åå¥½è½¬åŒ–ä¸ºå®¶åº­èƒ½æºä¼˜åŒ–çº¦æŸçš„ç ”ç©¶ã€‚ç ”ç©¶åœ¨ä¸€ä¸ªå¯å†ç”Ÿèƒ½æºç¤¾åŒºï¼ˆRECï¼‰çš„èƒŒæ™¯ä¸‹ï¼Œå°†è‡ªç„¶è¯­è¨€ç”¨æˆ·è¯­å¥è½¬åŒ–ä¸ºæ™ºèƒ½å®¶ç”µçš„æ­£å¼çº¦æŸã€‚æ–‡ç« ä»¥æ„å¤§åˆ©ä¸ºèƒŒæ™¯ï¼Œè¯„ä¼°äº†å½“å‰å¯ç”¨çš„å„ç§æ„å¤§åˆ©è¯­LLMsåœ¨é‡‡ç”¨ç»å…¸é›¶æ ·æœ¬ã€ä¸€æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ è®¾ç½®ä¸‹è½¬æ¢è¿™äº›åå¥½çš„æœ‰æ•ˆæ€§ã€‚æœ¬æ–‡çš„è´¡çŒ®åŒ…æ‹¬ä¸ºè¿™é¡¹ä»»åŠ¡å»ºç«‹åŸºå‡†æ€§èƒ½ï¼Œå…¬å¼€å‘å¸ƒæ•°æ®é›†å’Œä»£ç ä»¥ä¾›è¿›ä¸€æ­¥ç ”ç©¶ï¼Œå¹¶æä¾›åœ¨æ­¤ç‰¹å®šé¢†åŸŸè§‚å¯Ÿåˆ°çš„æœ€ä½³å®è·µå’ŒLLMså±€é™æ€§çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å°†ç”¨æˆ·åå¥½è½¬åŒ–ä¸ºå®¶åº­èƒ½æºä¼˜åŒ–çº¦æŸã€‚</li>
<li>ç ”ç©¶èƒŒæ™¯è®¾ç½®åœ¨å¯å†ç”Ÿèƒ½æºç¤¾åŒºï¼ˆRECï¼‰ï¼Œå¹¶ä¸“æ³¨äºæ„å¤§åˆ©åœºæ™¯ã€‚</li>
<li>è¯„ä¼°äº†å¤šç§LLMsåœ¨è½¬åŒ–ç”¨æˆ·åå¥½çš„æœ‰æ•ˆæ€§ï¼Œè¿™äº›åå¥½é€šè¿‡ç»å…¸å­¦ä¹ è®¾ç½®ï¼ˆé›¶æ ·æœ¬ã€ä¸€æ ·æœ¬å’Œå°‘æ ·æœ¬ï¼‰å®ç°ã€‚</li>
<li>å»ºç«‹æ­¤ä»»åŠ¡çš„åŸºç¡€æ€§èƒ½æ ‡å‡†ã€‚</li>
<li>å…¬å¼€æ•°æ®é›†å’Œä»£ç ä»¥ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
<li>æä¾›å…³äºLLMsåœ¨æ­¤ç‰¹å®šé¢†åŸŸçš„æœ€ä½³å®è·µå’Œè§‚å¯Ÿåˆ°çš„å±€é™æ€§è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21360">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7597b9b18d93e058ff511e49c37ccb7b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Learn-by-Reasoning-Analogical-Weight-Generation-for-Few-Shot-Class-Incremental-Learning"><a href="#Learn-by-Reasoning-Analogical-Weight-Generation-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Learn by Reasoning: Analogical Weight Generation for Few-Shot   Class-Incremental Learning"></a>Learn by Reasoning: Analogical Weight Generation for Few-Shot   Class-Incremental Learning</h2><p><strong>Authors:Jizhou Han, Chenhao Ding, Yuhang He, Songlin Dong, Qiang Wang, Xinyuan Gao, Yihong Gong</strong></p>
<p>Few-shot class-incremental Learning (FSCIL) enables models to learn new classes from limited data while retaining performance on previously learned classes. Traditional FSCIL methods often require fine-tuning parameters with limited new class data and suffer from a separation between learning new classes and utilizing old knowledge. Inspired by the analogical learning mechanisms of the human brain, we propose a novel analogical generative method. Our approach includes the Brain-Inspired Analogical Generator (BiAG), which derives new class weights from existing classes without parameter fine-tuning during incremental stages. BiAG consists of three components: Weight Self-Attention Module (WSA), Weight &amp; Prototype Analogical Attention Module (WPAA), and Semantic Conversion Module (SCM). SCM uses Neural Collapse theory for semantic conversion, WSA supplements new class weights, and WPAA computes analogies to generate new class weights. Experiments on miniImageNet, CUB-200, and CIFAR-100 datasets demonstrate that our method achieves higher final and average accuracy compared to SOTA methods. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰é™æ•°æ®ä¸Šå­¦ä¹ æ–°ç±»åˆ«ï¼ŒåŒæ—¶ä¿æŒå¯¹å…ˆå‰å­¦ä¹ ç±»åˆ«çš„æ€§èƒ½ã€‚ä¼ ç»Ÿçš„FSCILæ–¹æ³•é€šå¸¸éœ€è¦åˆ©ç”¨æœ‰é™çš„æ–°ç±»åˆ«æ•°æ®è¿›è¡Œå‚æ•°å¾®è°ƒï¼Œå¹¶ä¸”å­˜åœ¨å­¦ä¹ æ–°ç±»åˆ«å’Œåˆ©ç”¨æ—§çŸ¥è¯†ä¹‹é—´çš„åˆ†ç¦»é—®é¢˜ã€‚å—äººç±»å¤§è„‘ç±»æ¯”å­¦ä¹ æœºåˆ¶çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ç±»æ¯”ç”Ÿæˆæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬è„‘å¯å‘ç±»æ¯”ç”Ÿæˆå™¨ï¼ˆBiAGï¼‰ï¼Œå®ƒèƒ½å¤Ÿåœ¨å¢é‡é˜¶æ®µä»ç°æœ‰ç±»åˆ«ä¸­æ¨å¯¼å‡ºæ–°çš„ç±»åˆ«æƒé‡ï¼Œè€Œæ— éœ€è¿›è¡Œå‚æ•°å¾®è°ƒã€‚BiAGåŒ…å«ä¸‰ä¸ªç»„ä»¶ï¼šæƒé‡è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼ˆWSAï¼‰ã€æƒé‡ä¸åŸå‹ç±»æ¯”æ³¨æ„åŠ›æ¨¡å—ï¼ˆWPAAï¼‰å’Œè¯­ä¹‰è½¬æ¢æ¨¡å—ï¼ˆSCMï¼‰ã€‚SCMä½¿ç”¨ç¥ç»å´©æºƒç†è®ºè¿›è¡Œè¯­ä¹‰è½¬æ¢ï¼ŒWSAè¡¥å……æ–°çš„ç±»åˆ«æƒé‡ï¼ŒWPAAè®¡ç®—ç±»æ¯”ä»¥ç”Ÿæˆæ–°çš„ç±»åˆ«æƒé‡ã€‚åœ¨miniImageNetã€CUB-200å’ŒCIFAR-100æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯æ–¹æ³•è¾¾åˆ°äº†æ›´é«˜çš„æœ€ç»ˆå’Œå¹³å‡å‡†ç¡®ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21258v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>Few-shotç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰ä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æœ‰é™çš„æ•°æ®ä¸­å­¦ä¹ æ–°ç±»åˆ«ï¼ŒåŒæ—¶ä¿æŒå¯¹å…ˆå‰å­¦ä¹ ç±»åˆ«çš„æ€§èƒ½ã€‚ä¼ ç»ŸFSCILæ–¹æ³•é€šå¸¸éœ€è¦åˆ©ç”¨æœ‰é™çš„æ–°ç±»åˆ«æ•°æ®è¿›è¡Œå‚æ•°å¾®è°ƒï¼Œå¹¶é¢ä¸´å­¦ä¹ æ–°ç±»åˆ«å’Œåˆ©ç”¨æ—§çŸ¥è¯†ä¹‹é—´çš„åˆ†ç¦»é—®é¢˜ã€‚å—äººç±»å¤§è„‘ç±»æ¯”å­¦ä¹ æœºåˆ¶çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç±»æ¯”ç”Ÿæˆæ–¹æ³•ï¼ŒåŒ…æ‹¬Brain-Inspired Analogical Generatorï¼ˆBiAGï¼‰ã€‚BiAGé€šè¿‡ç°æœ‰ç±»åˆ«å¯¼å‡ºæ–°ç±»åˆ«æƒé‡ï¼Œåœ¨å¢é‡é˜¶æ®µæ— éœ€å‚æ•°å¾®è°ƒã€‚å®ƒç”±ä¸‰ä¸ªç»„ä»¶ç»„æˆï¼šWeight Self-Attention Moduleï¼ˆWSAï¼‰ï¼ŒWeight &amp; Prototype Analogical Attention Moduleï¼ˆWPAAï¼‰å’ŒSemantic Conversion Moduleï¼ˆSCMï¼‰ã€‚SCMåˆ©ç”¨ç¥ç»ç½‘ç»œå´©æºƒç†è®ºè¿›è¡Œè¯­ä¹‰è½¬æ¢ï¼ŒWSAè¡¥å……æ–°ç±»åˆ«æƒé‡ï¼ŒWPAAè®¡ç®—ç±»æ¯”ä»¥ç”Ÿæˆæ–°ç±»åˆ«æƒé‡ã€‚åœ¨miniImageNetã€CUB-200å’ŒCIFAR-100æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯æ–¹æ³•å…·æœ‰æ›´é«˜çš„æœ€ç»ˆå’Œå¹³å‡å‡†ç¡®ç‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Few-shotç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰å…è®¸æ¨¡å‹åœ¨æœ‰é™æ•°æ®ä¸­å­¦ä¹ æ–°ç±»åˆ«ï¼ŒåŒæ—¶ä¿æŒå¯¹æ—§çŸ¥è¯†çš„æ€§èƒ½ã€‚</li>
<li>ä¼ ç»ŸFSCILæ–¹æ³•éœ€è¦å¾®è°ƒå‚æ•°ï¼Œé¢ä¸´å­¦ä¹ æ–°ç±»åˆ«å’Œåˆ©ç”¨æ—§çŸ¥è¯†ä¹‹é—´çš„åˆ†ç¦»é—®é¢˜ã€‚</li>
<li>Brain-Inspired Analogical Generatorï¼ˆBiAGï¼‰å¯ä»¥ä»ç°æœ‰ç±»åˆ«å¯¼å‡ºæ–°ç±»åˆ«æƒé‡ï¼Œæ— éœ€åœ¨å¢é‡é˜¶æ®µè¿›è¡Œå‚æ•°å¾®è°ƒã€‚</li>
<li>BiAGåŒ…å«ä¸‰ä¸ªç»„ä»¶ï¼šWeight Self-Attention Moduleï¼ˆWSAï¼‰ï¼ŒWeight &amp; Prototype Analogical Attention Moduleï¼ˆWPAAï¼‰å’ŒSemantic Conversion Moduleï¼ˆSCMï¼‰ã€‚</li>
<li>WSAæ¨¡å—ç”¨äºè¡¥å……æ–°ç±»åˆ«æƒé‡ã€‚</li>
<li>WPAAæ¨¡å—è®¡ç®—ç±»æ¯”ä»¥ç”Ÿæˆæ–°ç±»åˆ«æƒé‡ï¼Œæœ‰åŠ©äºæ¨¡å‹çš„çµæ´»æ€§å’Œé€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21258">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-61ac50a4a5b820901dc637c13c2f09f7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-99488976e4f2a87bd1d46fc627d728c3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f474caa54f3f71888c3be278d26c1b14.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="The-Devil-is-in-Low-Level-Features-for-Cross-Domain-Few-Shot-Segmentation"><a href="#The-Devil-is-in-Low-Level-Features-for-Cross-Domain-Few-Shot-Segmentation" class="headerlink" title="The Devil is in Low-Level Features for Cross-Domain Few-Shot   Segmentation"></a>The Devil is in Low-Level Features for Cross-Domain Few-Shot   Segmentation</h2><p><strong>Authors:Yuhan Liu, Yixiong Zou, Yuhua Li, Ruixuan Li</strong></p>
<p>Cross-Domain Few-Shot Segmentation (CDFSS) is proposed to transfer the pixel-level segmentation capabilities learned from large-scale source-domain datasets to downstream target-domain datasets, with only a few annotated images per class. In this paper, we focus on a well-observed but unresolved phenomenon in CDFSS: for target domains, particularly those distant from the source domain, segmentation performance peaks at the very early epochs, and declines sharply as the source-domain training proceeds. We delve into this phenomenon for an interpretation: low-level features are vulnerable to domain shifts, leading to sharper loss landscapes during the source-domain training, which is the devil of CDFSS. Based on this phenomenon and interpretation, we further propose a method that includes two plug-and-play modules: one to flatten the loss landscapes for low-level features during source-domain training as a novel sharpness-aware minimization method, and the other to directly supplement target-domain information to the model during target-domain testing by low-level-based calibration. Extensive experiments on four target datasets validate our rationale and demonstrate that our method surpasses the state-of-the-art method in CDFSS signifcantly by 3.71% and 5.34% average MIoU in 1-shot and 5-shot scenarios, respectively. </p>
<blockquote>
<p>è·¨åŸŸå°æ ·æœ¬åˆ†å‰²ï¼ˆCDFSSï¼‰æ—¨åœ¨å°†åœ¨å¤§è§„æ¨¡æºåŸŸæ•°æ®é›†ä¸Šå­¦ä¹ åˆ°çš„åƒç´ çº§åˆ†å‰²èƒ½åŠ›è½¬ç§»åˆ°ä¸‹æ¸¸ç›®æ ‡åŸŸæ•°æ®é›†ä¸Šï¼Œå¹¶ä¸”æ¯ä¸ªç±»åˆ«åªæœ‰å°‘é‡æ ‡æ³¨å›¾åƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹å…³æ³¨CDFSSä¸­ä¸€ä¸ªå¸¸è§ä½†å°šæœªè§£å†³çš„é—®é¢˜ï¼šå¯¹äºç›®æ ‡åŸŸï¼Œå°¤å…¶æ˜¯é‚£äº›è¿œç¦»æºåŸŸçš„ç›®æ ‡åŸŸï¼Œåˆ†å‰²æ€§èƒ½åœ¨æœ€åˆå‡ ä¸ªè®­ç»ƒå‘¨æœŸè¾¾åˆ°å³°å€¼ï¼Œéšç€æºåŸŸè®­ç»ƒçš„è¿›è¡Œï¼Œæ€§èƒ½ä¼šæ€¥å‰§ä¸‹é™ã€‚æˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†è¿™ä¸€ç°è±¡ï¼Œå¹¶å¯¹å…¶è¿›è¡Œäº†è§£é‡Šï¼šä½çº§ç‰¹å¾å®¹æ˜“å—åˆ°åŸŸè¿ç§»çš„å½±å“ï¼Œå¯¼è‡´æºåŸŸè®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±æ™¯è§‚æ›´åŠ é™¡å³­ï¼Œè¿™æ˜¯CDFSSçš„éš¾é¢˜ã€‚åŸºäºè¿™ç§ç°è±¡å’Œè§£é‡Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼šä¸€ä¸ªåœ¨æºåŸŸè®­ç»ƒæœŸé—´é€šè¿‡ä¸€ç§æ–°çš„å°–é”åº¦æ„ŸçŸ¥æœ€å°åŒ–æ–¹æ³•æ¥å¹³æ»‘ä½çº§ç‰¹å¾çš„æŸå¤±æ™¯è§‚ï¼Œå¦ä¸€ä¸ªé€šè¿‡åœ¨ç›®æ ‡åŸŸæµ‹è¯•æœŸé—´åŸºäºä½çº§ç‰¹å¾è¿›è¡Œæ ¡å‡†æ¥ç›´æ¥è¡¥å……ç›®æ ‡åŸŸä¿¡æ¯åˆ°æ¨¡å‹ä¸­ã€‚åœ¨å››ä¸ªç›®æ ‡æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„åˆç†æ€§ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨CDFSSä¸­è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œåœ¨1-shotå’Œ5-shotåœºæ™¯ä¸­å¹³å‡MIoUåˆ†åˆ«æé«˜äº†3.71%å’Œ5.34%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21150v1">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong>ï¼šè·¨åŸŸå°æ ·æœ¬åˆ†å‰²ï¼ˆCDFSSï¼‰æ—¨åœ¨å°†ä»å¤§è§„æ¨¡æºåŸŸæ•°æ®é›†å­¦åˆ°çš„åƒç´ çº§åˆ†å‰²èƒ½åŠ›è½¬ç§»åˆ°ä¸‹æ¸¸ç›®æ ‡åŸŸæ•°æ®é›†ï¼Œè€Œæ¯ä¸ªç±»åˆ«åªéœ€è¦å°‘é‡æ ‡æ³¨å›¾åƒã€‚æœ¬æ–‡é’ˆå¯¹CDFSSä¸­è§‚å¯Ÿåˆ°çš„ç°è±¡è¿›è¡Œç ”ç©¶ï¼Œå³åœ¨ç›®æ ‡åŸŸï¼Œå°¤å…¶æ˜¯ä¸æºåŸŸè·ç¦»è¾ƒè¿œçš„é¢†åŸŸï¼Œåˆ†å‰²æ€§èƒ½åœ¨åˆæœŸè¾¾åˆ°å³°å€¼åéšç€æºåŸŸè®­ç»ƒçš„è¿›è¡Œè€Œæ€¥å‰§ä¸‹é™ã€‚åŸºäºå¯¹è¿™ç§ç°è±¡çš„è§£è¯»ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå³æ’å³ç”¨çš„æ¨¡å—ï¼šä¸€ä¸ªåœ¨æºåŸŸè®­ç»ƒæœŸé—´å¯¹ä½å±‚æ¬¡ç‰¹å¾æŸå¤±æ™¯è§‚è¿›è¡Œå¹³æ»‘å¤„ç†çš„æ–°é¢–é”åº¦æ„ŸçŸ¥æœ€å°åŒ–æ–¹æ³•ï¼Œå¦ä¸€ä¸ªé€šè¿‡ä½å±‚æ¬¡ç‰¹å¾æ ¡å‡†åœ¨ç›®æ ‡åŸŸæµ‹è¯•æœŸé—´ç›´æ¥å‘æ¨¡å‹è¡¥å……ç›®æ ‡åŸŸä¿¡æ¯ã€‚åœ¨å››ä¸ªç›®æ ‡æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„åˆç†æ€§ï¼Œå¹¶ä¸”åœ¨CDFSSé¢†åŸŸä¸­è¶…è¿‡äº†æœ€æ–°æ–¹æ³•çš„å¹³å‡MIoUæŒ‡æ ‡ï¼Œåœ¨å•æ ·æœ¬å’Œäº”æ ·æœ¬åœºæ™¯ä¸­åˆ†åˆ«æé«˜äº†3.71%å’Œ5.34%ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>Cross-Domain Few-Shot Segmentation (CDFSS)æ—¨åœ¨åˆ©ç”¨æºåŸŸæ•°æ®é›†çš„åˆ†å‰²èƒ½åŠ›åœ¨ç›®æ ‡åŸŸè¿›è¡Œå°æ ·æœ¬åˆ†å‰²ã€‚</li>
<li>é’ˆå¯¹CDFSSä¸­è§‚å¯Ÿåˆ°çš„ç°è±¡ï¼šåœ¨ç›®æ ‡åŸŸä¸Šï¼Œå°¤å…¶åœ¨è¿œç¦»æºåŸŸçš„é¢†åŸŸï¼Œåˆ†å‰²æ€§èƒ½åœ¨åˆæœŸè¾¾åˆ°å³°å€¼åä¼šéšè®­ç»ƒè¿›è¡Œæ€¥å‰§ä¸‹é™ã€‚</li>
<li>è¯¥ç°è±¡çš„åŸå› æ˜¯ä½å±‚æ¬¡ç‰¹å¾å®¹æ˜“å—åˆ°é¢†åŸŸåç§»çš„å½±å“ï¼Œå¯¼è‡´åœ¨æºåŸŸè®­ç»ƒæœŸé—´æŸå¤±æ™¯è§‚æ›´åŠ å°–é”ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸¤ä¸ªæ¨¡å—ï¼šä¸€ä¸ªç”¨äºåœ¨æºåŸŸè®­ç»ƒæœŸé—´å¹³æ»‘ä½å±‚æ¬¡ç‰¹å¾çš„æŸå¤±æ™¯è§‚ï¼›å¦ä¸€ä¸ªç”¨äºåœ¨ç›®æ ‡åŸŸæµ‹è¯•æœŸé—´è¡¥å……ç›®æ ‡åŸŸä¿¡æ¯ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•åœ¨CDFSSé¢†åŸŸæœ‰æ‰€æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å•æ ·æœ¬å’Œäº”æ ·æœ¬åœºæ™¯ä¸‹çš„å¹³å‡MIoUæŒ‡æ ‡ã€‚</li>
<li>æ–¹æ³•å…·æœ‰æ™®éé€‚ç”¨æ€§ï¼Œåœ¨å››ä¸ªç›®æ ‡æ•°æ®é›†ä¸Šå‡éªŒè¯äº†å…¶åˆç†æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21150">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bd4d0937a2032ef0621807f0250b0132.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed9cc4e4dd165a874383cbb18808e833.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e895e829b02a2b8516d064d91d3b37f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-22979fce2c64bf38d4261c48dfeba506.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-febef7c8951b446fa5fb54f1224f6380.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21252b2f1dac5495ddcb918aa24f8a15.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="â€œWhose-Side-Are-You-On-â€-Estimating-Ideology-of-Political-and-News-Content-Using-Large-Language-Models-and-Few-shot-Demonstration-Selection"><a href="#â€œWhose-Side-Are-You-On-â€-Estimating-Ideology-of-Political-and-News-Content-Using-Large-Language-Models-and-Few-shot-Demonstration-Selection" class="headerlink" title="â€œWhose Side Are You On?â€ Estimating Ideology of Political and News   Content Using Large Language Models and Few-shot Demonstration Selection"></a>â€œWhose Side Are You On?â€ Estimating Ideology of Political and News   Content Using Large Language Models and Few-shot Demonstration Selection</h2><p><strong>Authors:Muhammad Haroon, Magdalena Wojcieszak, Anshuman Chhabra</strong></p>
<p>The rapid growth of social media platforms has led to concerns about radicalization, filter bubbles, and content bias. Existing approaches to classifying ideology are limited in that they require extensive human effort, the labeling of large datasets, and are not able to adapt to evolving ideological contexts. This paper explores the potential of Large Language Models (LLMs) for classifying the political ideology of online content in the context of the two-party US political spectrum through in-context learning (ICL). Our extensive experiments involving demonstration selection in label-balanced fashion, conducted on three datasets comprising news articles and YouTube videos, reveal that our approach significantly outperforms zero-shot and traditional supervised methods. Additionally, we evaluate the influence of metadata (e.g., content source and descriptions) on ideological classification and discuss its implications. Finally, we show how providing the source for political and non-political content influences the LLMâ€™s classification. </p>
<blockquote>
<p>ç¤¾äº¤åª’ä½“å¹³å°çš„å¿«é€Ÿå‘å±•å¼•å‘äº†äººä»¬å¯¹æç«¯åŒ–ã€ä¿¡æ¯èŒ§æˆ¿å’Œå†…å®¹åå‘çš„æ‹…å¿§ã€‚ç°æœ‰çš„æ„è¯†å½¢æ€åˆ†ç±»æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦å¤§é‡çš„äººåŠ›æŠ•å…¥ï¼Œå¯¹å¤§é‡æ•°æ®è¿›è¡Œæ ‡æ³¨ï¼Œå¹¶ä¸”æ— æ³•é€‚åº”ä¸æ–­å˜åŒ–çš„æ„è¯†å½¢æ€ç¯å¢ƒã€‚æœ¬æ–‡é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŸºäºä¸¤å…šç¾å›½æ”¿æ²»è°±ç³»çš„åœ¨çº¿å†…å®¹æ”¿æ²»æ„è¯†å½¢æ€åˆ†ç±»ä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŒ…å«æ–°é—»æ–‡ç« å’ŒYouTubeè§†é¢‘çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹³è¡¡æ ‡ç­¾çš„æ¼”ç¤ºé€‰æ‹©å®éªŒï¼Œç»“æœæ˜¾ç¤ºæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬å’Œä¼ ç»Ÿç›‘ç£æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†å…ƒæ•°æ®ï¼ˆå¦‚å†…å®¹æ¥æºå’Œæè¿°ï¼‰å¯¹æ„è¯†å½¢æ€åˆ†ç±»çš„å½±å“ï¼Œå¹¶è®¨è®ºäº†å…¶æ„ä¹‰ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†æä¾›æ”¿æ²»å’Œéæ”¿æ²»å†…å®¹çš„æ¥æºå¦‚ä½•å½±å“LLMçš„åˆ†ç±»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20797v1">PDF</a> </p>
<p><strong>Summary</strong><br>ç¤¾äº¤åª’ä½“å¹³å°çš„å¿«é€Ÿå‘å±•å¼•å‘äº†äººä»¬å¯¹æç«¯åŒ–ã€ä¿¡æ¯èŒ§æˆ¿å’Œå†…å®¹åè§çš„æ‹…å¿§ã€‚ç°æœ‰åˆ†ç±»æ„è¯†å½¢æ€çš„æ–¹æ³•éœ€è¦å¤§é‡äººåŠ›ã€æ ‡æ³¨å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¹¶ä¸”æ— æ³•é€‚åº”ä¸æ–­å˜åŒ–çš„æ„è¯†å½¢æ€ç¯å¢ƒã€‚æœ¬æ–‡æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰å¯¹ç¾å›½ä¸¤å…šæ”¿æ²»å…‰è°±èƒŒæ™¯ä¸‹åœ¨çº¿å†…å®¹çš„æ”¿æ²»æ„è¯†å½¢æ€è¿›è¡Œåˆ†ç±»çš„æ½œåŠ›ã€‚åœ¨ä¸‰ä¸ªåŒ…å«æ–°é—»æ–‡ç« å’ŒYouTubeè§†é¢‘çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºé›¶æ ·æœ¬å’Œä¼ ç»Ÿç›‘ç£æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†å…ƒæ•°æ®ï¼ˆå¦‚å†…å®¹æ¥æºå’Œæè¿°ï¼‰å¯¹æ„è¯†å½¢æ€åˆ†ç±»çš„å½±å“ï¼Œå¹¶è®¨è®ºäº†å…¶å½±å“ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†æä¾›æ”¿æ²»å’Œéæ”¿æ²»å†…å®¹çš„æ¥æºå¦‚ä½•å½±å“LLMçš„åˆ†ç±»æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¤¾äº¤åª’ä½“å¹³å°çš„å¿«é€Ÿå‘å±•å¼•å‘äº†å…³äºæç«¯åŒ–ã€ä¿¡æ¯èŒ§æˆ¿å’Œå†…å®¹åè§çš„æ‹…å¿§ã€‚</li>
<li>ç°æœ‰æ„è¯†å½¢æ€åˆ†ç±»æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ”¹è¿›ä»¥é€‚åº”ä¸æ–­å˜åŒ–çš„ç¯å¢ƒã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åˆ†ç±»æ”¿æ²»æ„è¯†å½¢æ€æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ï¼ŒLLMåœ¨åˆ†ç±»ç¾å›½ä¸¤å…šæ”¿æ²»å…‰è°±ä¸‹çš„åœ¨çº¿å†…å®¹æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>åœ¨æ–°é—»æ–‡ç« å’ŒYouTubeè§†é¢‘æ•°æ®ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒLLMæ–¹æ³•ä¼˜äºé›¶æ ·æœ¬å’Œä¼ ç»Ÿç›‘ç£æ–¹æ³•ã€‚</li>
<li>å…ƒæ•°æ®å¯¹æ„è¯†å½¢æ€åˆ†ç±»æœ‰å½±å“ï¼Œè¿™ä¸€ç‚¹åœ¨ç ”ç©¶ä¸­å¾—åˆ°äº†è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20797">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb1fc9506a3d073c280cd630e2ee4bb2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-226b84c939c44ed81f555cb09b4ab973.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fede8efc1bedaac3835207db32deca1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-58c59a2656dbb00135d4f7ff6f8d4c15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd8cfb35c20bf721d476f3071ea646d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-997d45dfd22a9808942835916c7ee307.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Reason-RFT-Reinforcement-Fine-Tuning-for-Visual-Reasoning"><a href="#Reason-RFT-Reinforcement-Fine-Tuning-for-Visual-Reasoning" class="headerlink" title="Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning"></a>Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning</h2><p><strong>Authors:Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang</strong></p>
<p>Visual reasoning abilities play a crucial role in understanding complex multimodal data, advancing both domain-specific applications and artificial general intelligence (AGI). Existing methods improve VLM reasoning via Chain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated training data to enhance visual reasoning capabilities. However, this training paradigm may lead to overfitting and cognitive rigidity, restricting the modelâ€™s ability to transfer visual reasoning skills across domains and limiting its real-world applicability. To address these limitations, we propose Reason-RFT, a novel reinforcement fine-tuning framework that significantly enhances generalization capabilities in visual reasoning tasks. Reason-RFT introduces a two-phase training framework for visual reasoning: (1) Supervised Fine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the reasoning potential of Vision-Language Models (VLMs), followed by (2) Group Relative Policy Optimization (GRPO)-based reinforcement learning that generates multiple reasoning-response pairs, significantly enhancing generalization in visual reasoning tasks. To evaluate Reason-RFTâ€™s visual reasoning capabilities, we reconstructed a comprehensive dataset spanning visual counting, structure perception, and spatial transformation. Experimental results demonstrate Reasoning-RFTâ€™s three key advantages: (1) Performance Enhancement: achieving state-of-the-art results across multiple tasks, outperforming most mainstream open-source and proprietary models; (2) Generalization Superiority: consistently maintaining robust performance across diverse tasks and domains, outperforming alternative training paradigms; (3) Data Efficiency: excelling in few-shot learning scenarios while surpassing full-dataset SFT baselines. Project website: <a target="_blank" rel="noopener" href="https://tanhuajie.github.io/ReasonRFT">https://tanhuajie.github.io/ReasonRFT</a> </p>
<blockquote>
<p>è§†è§‰æ¨ç†èƒ½åŠ›åœ¨ç†è§£å¤æ‚çš„å¤šæ¨¡æ€æ•°æ®ã€æ¨åŠ¨é¢†åŸŸç‰¹å®šåº”ç”¨å’Œäººå·¥æ™ºèƒ½é€šç”¨æ™ºèƒ½ï¼ˆAGIï¼‰æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡åŸºäºæ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼Œç®€ç§°CoTï¼‰çš„ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼‰æ¥æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVision-Language Modelsï¼Œç®€ç§°VLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿ç”¨ç²¾å¿ƒæ ‡æ³¨çš„è®­ç»ƒæ•°æ®ä»¥å¢å¼ºè§†è§‰æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ç§è®­ç»ƒæ¨¡å¼å¯èƒ½ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆå’Œè®¤çŸ¥åƒµåŒ–ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸåº”ç”¨è§†è§‰æ¨ç†æŠ€èƒ½çš„èƒ½åŠ›ï¼Œä»¥åŠå…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„é€‚ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Reason-RFTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜è§†è§‰æ¨ç†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚Reason-RFTå¼•å…¥äº†ç”¨äºè§†è§‰æ¨ç†çš„ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼šé¦–å…ˆï¼Œä½¿ç”¨ç²¾é€‰çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼Œç®€ç§°SFTï¼‰ï¼Œä»¥æ¿€å‘è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¨ç†æ½œåŠ›ï¼›å…¶æ¬¡ï¼ŒåŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGroup Relative Policy Optimizationï¼Œç®€ç§°GRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ç”Ÿæˆå¤šä¸ªæ¨ç†å“åº”å¯¹ï¼Œè¿™æ˜¾è‘—æé«˜äº†è§†è§‰æ¨ç†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è¯„ä¼°Reason-RFTçš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬é‡æ–°æ„å»ºäº†ä¸€ä¸ªç»¼åˆæ•°æ®é›†ï¼Œæ¶µç›–äº†è§†è§‰è®¡æ•°ã€ç»“æ„æ„ŸçŸ¥å’Œç©ºé—´è½¬æ¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReasoning-RFTå…·æœ‰ä¸‰å¤§ä¼˜åŠ¿ï¼šï¼ˆ1ï¼‰æ€§èƒ½å¢å¼ºï¼šåœ¨å¤šä»»åŠ¡ä¸Šå®ç°æœ€ä½³ç»“æœï¼Œä¼˜äºå¤§å¤šæ•°ä¸»æµå¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼›ï¼ˆ2ï¼‰æ³›åŒ–ä¼˜åŠ¿ï¼šåœ¨å„ç§ä»»åŠ¡å’Œé¢†åŸŸä¸­å§‹ç»ˆä¿æŒè‰¯å¥½çš„æ€§èƒ½ï¼Œä¼˜äºå…¶ä»–è®­ç»ƒæ¨¡å¼ï¼›ï¼ˆ3ï¼‰æ•°æ®æ•ˆç‡ï¼šåœ¨å°‘æ•°å­¦ä¹ åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šå…¨æ•°æ®é›†çš„SFTåŸºå‡†ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://tanhuajie.github.io/ReasonRFT">https://tanhuajie.github.io/ReasonRFT</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20752v2">PDF</a> 35 pages, 22 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†è§‰æ¨ç†èƒ½åŠ›å¯¹äºç†è§£å¤æ‚çš„å¤šæ¨¡æ€æ•°æ®ã€æ¨åŠ¨é¢†åŸŸç‰¹å®šåº”ç”¨å’Œé€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„å‘å±•èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç°æœ‰æ–¹æ³•é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆå’Œè®¤çŸ¥åƒµåŒ–ï¼Œé™åˆ¶æ¨¡å‹è·¨åŸŸè½¬ç§»è§†è§‰æ¨ç†æŠ€èƒ½çš„èƒ½åŠ›åŠå…¶åœ¨å®é™…ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Reason-RFTï¼Œä¸€ç§æ–°å‹å¼ºåŒ–å¾®è°ƒæ¡†æ¶ï¼Œæ˜¾è‘—æé«˜äº†è§†è§‰æ¨ç†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚Reason-RFTå¼•å…¥äº†ä¸¤é˜¶æ®µè®­ç»ƒæ¡†æ¶ï¼šé¦–å…ˆä½¿ç”¨ç²¾é€‰çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œæ¿€æ´»è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†æ½œåŠ›ï¼Œæ¥ç€é‡‡ç”¨åŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ç”Ÿæˆå¤šä¸ªæ¨ç†-å“åº”å¯¹ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºè§†è§‰æ¨ç†ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºè¯„ä¼°Reason-RFTçš„è§†è§‰æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬é‡å»ºäº†ä¸€ä¸ªç»¼åˆæ•°æ®é›†ï¼Œæ¶µç›–è§†è§‰è®¡æ•°ã€ç»“æ„æ„ŸçŸ¥å’Œç©ºé—´å˜æ¢ã€‚å®éªŒç»“æœè¡¨æ˜Reasoning-RFTå…·æœ‰ä¸‰å¤§ä¼˜åŠ¿ï¼šæ€§èƒ½æå‡ã€æ³›åŒ–ä¼˜åŠ¿å’Œæ•°æ®æ•ˆç‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†è§‰æ¨ç†èƒ½åŠ›åœ¨ç†è§£å¤æ‚å¤šæ¨¡æ€æ•°æ®å’Œæ¨åŠ¨äººå·¥æ™ºèƒ½å‘å±•æ–¹é¢è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç›‘ç£å¾®è°ƒæå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨è¿‡æ‹Ÿåˆå’Œè®¤çŸ¥åƒµåŒ–é—®é¢˜ã€‚</li>
<li>Reason-RFTæ¡†æ¶é€šè¿‡å¼•å…¥ä¸¤é˜¶æ®µè®­ç»ƒæ³•å¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒåŸºäºç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>Reason-RFTåœ¨è§†è§‰è®¡æ•°ã€ç»“æ„æ„ŸçŸ¥å’Œç©ºé—´å˜æ¢ç­‰å¤šé¡¹ä»»åŠ¡ä¸Šå®ç°å“è¶Šæ€§èƒ½ï¼Œä¼˜äºä¸»æµå¼€æºå’Œä¸“æœ‰æ¨¡å‹ã€‚</li>
<li>Reason-RFTåœ¨ä¸åŒä»»åŠ¡å’Œé¢†åŸŸä¸Šè¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å…¶ä»–è®­ç»ƒèŒƒå¼ã€‚</li>
<li>åœ¨å°æ ·å­¦ä¹ åœºæ™¯ä¸‹ï¼ŒReason-RFTçš„æ•°æ®æ•ˆç‡è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å…¨æ•°æ®é›†çš„SFTåŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20752">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a9d5452c8940dfb88c42167b3a4eb281.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ff52be7618643b781f0b047988a2a27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06b95518386236ce42fcfb65735cb601.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Cognitive-Mental-LLM-Evaluating-Reasoning-in-Large-Language-Models-for-Mental-Health-Prediction-via-Online-Text"><a href="#Cognitive-Mental-LLM-Evaluating-Reasoning-in-Large-Language-Models-for-Mental-Health-Prediction-via-Online-Text" class="headerlink" title="Cognitive-Mental-LLM: Evaluating Reasoning in Large Language Models for   Mental Health Prediction via Online Text"></a>Cognitive-Mental-LLM: Evaluating Reasoning in Large Language Models for   Mental Health Prediction via Online Text</h2><p><strong>Authors:Avinash Patil, Amardeep Kour Gedhu</strong></p>
<p>Large Language Models (LLMs) have demonstrated potential in predicting mental health outcomes from online text, yet traditional classification methods often lack interpretability and robustness. This study evaluates structured reasoning techniques-Chain-of-Thought (CoT), Self-Consistency (SC-CoT), and Tree-of-Thought (ToT)-to improve classification accuracy across multiple mental health datasets sourced from Reddit. We analyze reasoning-driven prompting strategies, including Zero-shot CoT and Few-shot CoT, using key performance metrics such as Balanced Accuracy, F1 score, and Sensitivity&#x2F;Specificity. Our findings indicate that reasoning-enhanced techniques improve classification performance over direct prediction, particularly in complex cases. Compared to baselines such as Zero Shot non-CoT Prompting, and fine-tuned pre-trained transformers such as BERT and Mental-RoBerta, and fine-tuned Open Source LLMs such as Mental Alpaca and Mental-Flan-T5, reasoning-driven LLMs yield notable gains on datasets like Dreaddit (+0.52% over M-LLM, +0.82% over BERT) and SDCNL (+4.67% over M-LLM, +2.17% over BERT). However, performance declines in Depression Severity, and CSSRS predictions suggest dataset-specific limitations, likely due to our using a more extensive test set. Among prompting strategies, Few-shot CoT consistently outperforms others, reinforcing the effectiveness of reasoning-driven LLMs. Nonetheless, dataset variability highlights challenges in model reliability and interpretability. This study provides a comprehensive benchmark of reasoning-based LLM techniques for mental health text classification. It offers insights into their potential for scalable clinical applications while identifying key challenges for future improvements. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²æ˜¾ç¤ºå‡ºä»åœ¨çº¿æ–‡æœ¬é¢„æµ‹å¿ƒç†å¥åº·ç»“æœçš„æ½œåŠ›ï¼Œä½†ä¼ ç»Ÿåˆ†ç±»æ–¹æ³•å¾€å¾€ç¼ºä¹å¯è§£é‡Šæ€§å’Œç¨³å¥æ€§ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†åŸºäºç»“æ„åŒ–æ¨ç†æŠ€æœ¯â€”â€”æ€ç»´é“¾ï¼ˆCoTï¼‰ã€è‡ªæˆ‘ä¸€è‡´æ€§ï¼ˆSC-CoTï¼‰å’Œæ€ç»´æ ‘ï¼ˆToTï¼‰â€”â€”ä»¥æé«˜åœ¨å¤šä¸ªæ¥è‡ªRedditçš„å¿ƒç†å¥åº·æ•°æ®é›†ä¸Šçš„åˆ†ç±»å‡†ç¡®ç‡ã€‚æˆ‘ä»¬åˆ†æäº†ä»¥æ¨ç†ä¸ºæ ¸å¿ƒçš„æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬CoTå’Œå°‘æ ·æœ¬CoTï¼Œä½¿ç”¨å¹³è¡¡ç²¾åº¦ã€F1åˆ†æ•°ã€çµæ•åº¦å’Œç‰¹å¼‚åº¦ç­‰å…³é”®æ€§èƒ½æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨æ¨ç†å¢å¼ºæŠ€æœ¯çš„åˆ†ç±»æ€§èƒ½ä¼˜äºç›´æ¥é¢„æµ‹ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æƒ…å†µä¸‹ã€‚ä¸åŸºçº¿æ–¹æ³•ï¼ˆå¦‚é›¶æ ·æœ¬éCoTæç¤ºï¼‰ä»¥åŠå¾®è°ƒè¿‡çš„é¢„è®­ç»ƒè½¬æ¢å™¨ï¼ˆå¦‚BERTå’ŒMental-RoBertaï¼‰ä»¥åŠå¾®è°ƒè¿‡çš„å¼€æºLLMï¼ˆå¦‚Mental Alpacaå’ŒMental-Flan-T5ï¼‰ç›¸æ¯”ï¼ŒåŸºäºæ¨ç†çš„LLMåœ¨æ•°æ®é›†ï¼ˆå¦‚Dreadditï¼ˆ+ 0.52%ç›¸å¯¹äºM-LLMï¼Œ+ 0.8iç›¸å¯¹äºBERTï¼‰å’ŒSDCNLï¼ˆ+ 4.67%ç›¸å¯¹äºM-LLMï¼Œ+ 2.17%ç›¸å¯¹äºBERTï¼‰ï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚ç„¶è€Œï¼Œåœ¨æŠ‘éƒç—‡ä¸¥é‡ç¨‹åº¦å’ŒCSSRSé¢„æµ‹æ–¹é¢çš„è¡¨ç°ä¸‹é™è¡¨æ˜ï¼Œç‰¹å®šæ•°æ®é›†å­˜åœ¨å±€é™æ€§ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºæˆ‘ä»¬ä½¿ç”¨äº†æ›´å¹¿æ³›çš„æµ‹è¯•é›†ã€‚åœ¨æç¤ºç­–ç•¥ä¸­ï¼Œå°‘æ ·æœ¬CoTå§‹ç»ˆè¡¨ç°æœ€å¥½ï¼Œè¿™è¿›ä¸€æ­¥è¯æ˜äº†åŸºäºæ¨ç†çš„LLMçš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œæ•°æ®é›†çš„å˜åŒ–ä¹Ÿçªæ˜¾äº†æ¨¡å‹å¯é æ€§å’Œå¯è§£é‡Šæ€§çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶ä¸ºåŸºäºæ¨ç†çš„LLMæŠ€æœ¯åœ¨å¿ƒç†å¥åº·æ–‡æœ¬åˆ†ç±»æ–¹é¢æä¾›äº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ã€‚å®ƒä¸ºå¯æ‰©å±•çš„ä¸´åºŠåº”ç”¨æ½œåŠ›æä¾›äº†è§è§£ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥æ”¹è¿›çš„å…³é”®æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10095v2">PDF</a> 8 pages, 4 Figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é¢„æµ‹ç½‘ç»œæ–‡æœ¬ä¸­çš„å¿ƒç†å¥åº·ç»“æœæ–¹é¢çš„æ½œåŠ›ã€‚é’ˆå¯¹ä¼ ç»Ÿåˆ†ç±»æ–¹æ³•ç¼ºä¹è§£é‡Šæ€§å’Œç¨³å¥æ€§çš„é—®é¢˜ï¼Œæœ¬ç ”ç©¶è¯„ä¼°äº†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ã€è‡ªæˆ‘ä¸€è‡´æ€§ï¼ˆSC-CoTï¼‰å’Œæ€ç»´æ ‘ï¼ˆToTï¼‰ç­‰ç»“æ„åŒ–æ¨ç†æŠ€æœ¯ï¼Œä»¥æé«˜åœ¨å¤šä¸ªæ¥è‡ªRedditçš„å¿ƒç†å¥åº·æ•°æ®é›†ä¸Šçš„åˆ†ç±»å‡†ç¡®æ€§ã€‚ç ”ç©¶åˆ†æäº†åŸºäºæ¨ç†çš„æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬é›¶å°„CoTå’Œå°‘å°„CoTï¼Œå¹¶ä½¿ç”¨å¹³è¡¡ç²¾åº¦ã€F1åˆ†æ•°å’Œæ•æ„Ÿæ€§&#x2F;ç‰¹å¼‚æ€§ç­‰å…³é”®æ€§èƒ½æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿æ–¹æ³•å’Œç²¾ç»†è°ƒæ•´çš„é¢„è®­ç»ƒè½¬æ¢å™¨ç›¸æ¯”ï¼Œæ¨ç†å¢å¼ºæŠ€æœ¯èƒ½æé«˜åˆ†ç±»æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œåœ¨æŠ‘éƒç—‡ä¸¥é‡æ€§å’ŒCSSRSé¢„æµ‹æ–¹é¢çš„æ€§èƒ½ä¸‹é™è¡¨æ˜ï¼Œæ•°æ®é›†ç‰¹å®šå±€é™æ€§å¯èƒ½å­˜åœ¨ã€‚æœ¬ç ”ç©¶æä¾›äº†åŸºäºæ¨ç†çš„LLMæŠ€æœ¯åœ¨å¿ƒç†å¥åº·æ–‡æœ¬åˆ†ç±»æ–¹é¢çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œä¸ºä¸´åºŠåº”ç”¨çš„æ½œåœ¨å¯èƒ½æ€§æä¾›äº†è§è§£ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥æ”¹è¿›çš„å…³é”®æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿä»ç½‘ç»œæ–‡æœ¬ä¸­é¢„æµ‹å¿ƒç†å¥åº·ç»“æœã€‚</li>
<li>ä¼ ç»Ÿåˆ†ç±»æ–¹æ³•åœ¨å¿ƒç†å¥åº·æ–‡æœ¬åˆ†ç±»ä¸­ç¼ºä¹è§£é‡Šæ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>ç»“æ„åŒ–æ¨ç†æŠ€æœ¯å¦‚é“¾å¼æ€ç»´ï¼ˆCoTï¼‰èƒ½æé«˜åˆ†ç±»å‡†ç¡®æ€§ã€‚</li>
<li>åŸºäºæ¨ç†çš„æç¤ºç­–ç•¥å¦‚å°‘å°„CoTåœ¨å¤æ‚æƒ…å†µä¸‹è¡¨ç°æ›´å¥½ã€‚</li>
<li>ä¸åŸºçº¿æ–¹æ³•å’Œç²¾ç»†è°ƒæ•´çš„é¢„è®­ç»ƒè½¬æ¢å™¨ç›¸æ¯”ï¼Œæ¨ç†å¢å¼ºæŠ€æœ¯æ˜¾è‘—æé«˜åˆ†ç±»æ€§èƒ½ã€‚</li>
<li>æ•°æ®é›†ç‰¹å®šå±€é™æ€§å­˜åœ¨ï¼Œæ€§èƒ½åœ¨ç‰¹å®šé¢„æµ‹æ–¹é¢å¯èƒ½ä¸‹é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-70600bf4741e0223deb9ebe05aeeec09.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9820f88b053584216b6cb1ecd79a38a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9980ea44e20e355761d5ff240cb0f49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80bab825098126a3e54ec3b5d30ffdcb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a05609bde858186c3bde655d9f9c1d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df5db2d2d1a6c1d5c76adc5e9babde10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4938e95a73fd349d987b93e18ebf5bf2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="JOG3R-Towards-3D-Consistent-Video-Generators"><a href="#JOG3R-Towards-3D-Consistent-Video-Generators" class="headerlink" title="JOG3R: Towards 3D-Consistent Video Generators"></a>JOG3R: Towards 3D-Consistent Video Generators</h2><p><strong>Authors:Chun-Hao Paul Huang, Niloy Mitra, Hyeonho Jeong, Jae Shin Yoon, Duygu Ceylan</strong></p>
<p>Emergent capabilities of image generators have led to many impactful zero- or few-shot applications. Inspired by this success, we investigate whether video generators similarly exhibit 3D-awareness. Using structure-from-motion as a 3D-aware task, we test if intermediate features of a video generator - OpenSora in our case - can support camera pose estimation. Surprisingly, at first, we only find a weak correlation between the two tasks. Deeper investigation reveals that although the video generator produces plausible video frames, the frames themselves are not truly 3D-consistent. Instead, we propose to jointly train for the two tasks, using photometric generation and 3D aware errors. Specifically, we find that SoTA video generation and camera pose estimation (i.e.,DUSt3R [79]) networks share common structures, and propose an architecture that unifies the two. The proposed unified model, named \nameMethod, produces camera pose estimates with competitive quality while producing 3D-consistent videos. In summary, we propose the first unified video generator that is 3D-consistent, generates realistic video frames, and can potentially be repurposed for other 3D-aware tasks. </p>
<blockquote>
<p>å›¾åƒç”Ÿæˆå™¨çš„çªå‘èƒ½åŠ›å·²ç»å¯¼è‡´äº†è®¸å¤šå…·æœ‰å½±å“åŠ›çš„é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬åº”ç”¨ã€‚å—æ­¤æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬è°ƒæŸ¥è§†é¢‘ç”Ÿæˆå™¨æ˜¯å¦åŒæ ·å…·å¤‡3Dæ„ŸçŸ¥èƒ½åŠ›ã€‚æˆ‘ä»¬ä½¿ç”¨ä»è¿åŠ¨ä¸­è·å–ç»“æ„ä½œä¸º3Dæ„ŸçŸ¥ä»»åŠ¡ï¼Œæµ‹è¯•è§†é¢‘ç”Ÿæˆå™¨çš„ä¸­é—´ç‰¹å¾ï¼ˆåœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ä¸ºOpenSoraï¼‰æ˜¯å¦èƒ½å¤Ÿæ”¯æŒç›¸æœºå§¿æ€ä¼°è®¡ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œèµ·åˆæˆ‘ä»¬åªåœ¨ä¸¤é¡¹ä»»åŠ¡ä¹‹é—´å‘ç°äº†å¾®å¼±çš„ç›¸å…³æ€§ã€‚æ›´æ·±å…¥çš„ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡è§†é¢‘ç”Ÿæˆå™¨å¯ä»¥ç”Ÿæˆåˆç†çš„è§†é¢‘å¸§ï¼Œä½†è¿™äº›å¸§æœ¬èº«å¹¶ä¸çœŸæ­£å…·æœ‰3Dä¸€è‡´æ€§ã€‚ç›¸åï¼Œæˆ‘ä»¬æå‡ºè”åˆè®­ç»ƒè¿™ä¸¤ä¸ªä»»åŠ¡ï¼Œä½¿ç”¨å…‰åº¦ç”Ÿæˆå’Œ3Dæ„ŸçŸ¥é”™è¯¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å‘ç°æœ€æ–°çš„è§†é¢‘ç”Ÿæˆå’Œç›¸æœºå§¿æ€ä¼°è®¡ï¼ˆå³DUSt3R[79]ï¼‰ç½‘ç»œå…·æœ‰å…±åŒçš„ç»“æ„ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç»Ÿä¸€ä¸¤è€…çš„æ¶æ„ã€‚æ‰€æå‡ºçš„ç»Ÿä¸€æ¨¡å‹è¢«ç§°ä¸º\nameMethodï¼Œå®ƒäº§ç”Ÿçš„ç›¸æœºå§¿æ€ä¼°è®¡å…·æœ‰ç«äº‰æ€§çš„è´¨é‡ï¼ŒåŒæ—¶ç”Ÿæˆå…·æœ‰3Dä¸€è‡´æ€§çš„è§†é¢‘ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªç»Ÿä¸€çš„è§†é¢‘ç”Ÿæˆå™¨ï¼Œå®ƒå…·æœ‰3Dä¸€è‡´æ€§ï¼Œå¯ä»¥ç”Ÿæˆé€¼çœŸçš„è§†é¢‘å¸§ï¼Œå¹¶ä¸”å¯èƒ½ç”¨äºå…¶ä»–3Dæ„ŸçŸ¥ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01409v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ–°ä¸€ä»£å›¾åƒç”Ÿæˆå™¨çš„æ¶Œç°èƒ½åŠ›ä¸ºå¾ˆå¤šé›¶æ ·æœ¬æˆ–å°æ ·ä¾‹åº”ç”¨å¸¦æ¥äº†å½±å“ã€‚æœ¬ç ”ç©¶å—æ­¤å¯å‘ï¼Œæ¢ç´¢è§†é¢‘ç”Ÿæˆå™¨æ˜¯å¦åŒæ ·å…·å¤‡3Dæ„ŸçŸ¥èƒ½åŠ›ã€‚é€šè¿‡ç»“æ„ä»è¿åŠ¨ä½œä¸º3Dæ„ŸçŸ¥ä»»åŠ¡è¿›è¡Œæµ‹è¯•ï¼Œæˆ‘ä»¬å‘ç°è§†é¢‘ç”Ÿæˆå™¨çš„ä¸­é—´ç‰¹å¾å¹¶ä¸ç›´æ¥æ”¯æŒç›¸æœºå§¿æ€ä¼°è®¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºè”åˆè®­ç»ƒä¸¤ä¸ªä»»åŠ¡çš„æ–¹æ³•ï¼Œä½¿ç”¨å…‰åº¦ç”Ÿæˆå’Œ3Dæ„ŸçŸ¥è¯¯å·®ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç»Ÿä¸€æ¨¡å‹ï¼Œåä¸ºâ€œnameMethodâ€ï¼Œè¯¥æ¨¡å‹èƒ½åœ¨ç”Ÿæˆ3Dä¸€è‡´è§†é¢‘çš„åŒæ—¶ï¼Œæä¾›å…·æœ‰ç«äº‰åŠ›çš„ç›¸æœºå§¿æ€ä¼°è®¡è´¨é‡ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªç»Ÿä¸€çš„ã€èƒ½ç”Ÿæˆ3Dä¸€è‡´è§†é¢‘çš„è§†é¢‘ç”Ÿæˆå™¨ï¼Œå¯ä¸ºå…¶ä»–3Dæ„ŸçŸ¥ä»»åŠ¡æä¾›æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘ç”Ÿæˆå™¨éœ€æ¢ç´¢æ˜¯å¦å…·å¤‡3Dæ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>ç»“æ„ä»è¿åŠ¨ä½œä¸ºæµ‹è¯•ä»»åŠ¡æ­ç¤ºè§†é¢‘ç”Ÿæˆå™¨ä¸­é—´ç‰¹å¾å¹¶ä¸ç›´æ¥æ”¯æŒç›¸æœºå§¿æ€ä¼°è®¡ã€‚</li>
<li>è§†é¢‘ç”Ÿæˆå™¨äº§ç”Ÿçš„è§†é¢‘å¸§è™½ç„¶çœ‹ä¼¼åˆç†ï¼Œä½†å¹¶ä¸å…·å¤‡çœŸæ­£çš„3Dä¸€è‡´æ€§ã€‚</li>
<li>æå‡ºè”åˆè®­ç»ƒç›¸æœºå§¿æ€ä¼°è®¡å’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡çš„æ–¹æ³•ã€‚</li>
<li>SoTAè§†é¢‘ç”Ÿæˆå’Œç›¸æœºå§¿æ€ä¼°è®¡ç½‘ç»œå…·æœ‰å…±åŒç»“æ„ã€‚</li>
<li>æå‡ºçš„ç»Ÿä¸€æ¨¡å‹â€œnameMethodâ€èƒ½ç”Ÿæˆå…·æœ‰ç«äº‰åŠ›çš„ç›¸æœºå§¿æ€ä¼°è®¡è´¨é‡ï¼ŒåŒæ—¶äº§ç”Ÿ3Dä¸€è‡´çš„è§†é¢‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01409">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c401c669952d4b38450d104c5ed63671.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-96d2d1c128fe46e0b11fb0b23fde26d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89cc0906c5d63612e405868f2b912cfb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8511d692756fa545390e7ff2c2d05890.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SF-Speech-Straightened-Flow-for-Zero-Shot-Voice-Clone"><a href="#SF-Speech-Straightened-Flow-for-Zero-Shot-Voice-Clone" class="headerlink" title="SF-Speech: Straightened Flow for Zero-Shot Voice Clone"></a>SF-Speech: Straightened Flow for Zero-Shot Voice Clone</h2><p><strong>Authors:Xuyuan Li, Zengqiang Shang, Hua Hua, Peiyang Shi, Chen Yang, Li Wang, Pengyuan Zhang</strong></p>
<p>Recently, neural ordinary differential equations (ODE) models trained with flow matching have achieved impressive performance on the zero-shot voice clone task. Nevertheless, postulating standard Gaussian noise as the initial distribution of ODE gives rise to numerous intersections within the fitted targets of flow matching, which presents challenges to model training and enhances the curvature of the learned generated trajectories. These curved trajectories restrict the capacity of ODE models for generating desirable samples with a few steps. This paper proposes SF-Speech, a novel voice clone model based on ODE and in-context learning. Unlike the previous works, SF-Speech adopts a lightweight multi-stage module to generate a more deterministic initial distribution for ODE. Without introducing any additional loss function, we effectively straighten the curved reverse trajectories of the ODE model by jointly training it with the proposed module. Experiment results on datasets of various scales show that SF-Speech outperforms the state-of-the-art zero-shot TTS methods and requires only a quarter of the solver steps, resulting in a generation speed approximately 3.7 times that of Voicebox and E2 TTS. Audio samples are available at the demo page\footnote{[Online] Available: <a target="_blank" rel="noopener" href="https://lixuyuan102.github.io/Demo/%7D">https://lixuyuan102.github.io/Demo/}</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œä½¿ç”¨æµåŒ¹é…è®­ç»ƒçš„ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æ¨¡å‹åœ¨é›¶æ ·æœ¬è¯­éŸ³å…‹éš†ä»»åŠ¡ä¸Šå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå‡è®¾æ ‡å‡†é«˜æ–¯å™ªå£°ä½œä¸ºODEçš„åˆå§‹åˆ†å¸ƒä¼šå¯¼è‡´æµåŒ¹é…çš„æ‹Ÿåˆç›®æ ‡å†…éƒ¨å‡ºç°è®¸å¤šäº¤é›†ï¼Œè¿™ç»™æ¨¡å‹è®­ç»ƒå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œå¹¶å¢å¼ºäº†å­¦ä¹ ç”Ÿæˆè½¨è¿¹çš„æ›²ç‡ã€‚è¿™äº›å¼¯æ›²çš„è½¨è¿¹é™åˆ¶äº†ODEæ¨¡å‹åœ¨å‡ æ­¥å†…ç”Ÿæˆç†æƒ³æ ·æœ¬çš„èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†SF-Speechï¼Œè¿™æ˜¯ä¸€ç§åŸºäºODEå’Œä¸Šä¸‹æ–‡å­¦ä¹ çš„æ–°å‹è¯­éŸ³å…‹éš†æ¨¡å‹ã€‚ä¸åŒäºä»¥å‰çš„å·¥ä½œï¼ŒSF-Speeché‡‡ç”¨è½»é‡çº§çš„å¤šé˜¶æ®µæ¨¡å—æ¥ä¸ºODEç”Ÿæˆæ›´ç¡®å®šçš„åˆå§‹åˆ†å¸ƒã€‚æˆ‘ä»¬æ²¡æœ‰å¼•å…¥ä»»ä½•é¢å¤–çš„æŸå¤±å‡½æ•°ï¼Œè€Œæ˜¯é€šè¿‡ä¸æ‰€æå‡ºæ¨¡å—è¿›è¡Œè”åˆè®­ç»ƒï¼Œæœ‰æ•ˆåœ°æ ¡æ­£äº†ODEæ¨¡å‹çš„å¼¯æ›²åå‘è½¨è¿¹ã€‚åœ¨å„ç§è§„æ¨¡çš„æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSF-Speechä¼˜äºæœ€å…ˆè¿›çš„é›¶æ ·æœ¬TTSæ–¹æ³•ï¼Œå¹¶ä¸”ä»…éœ€å››åˆ†ä¹‹ä¸€çš„æ±‚è§£å™¨æ­¥éª¤ï¼Œç”Ÿæˆé€Ÿåº¦å¤§çº¦æ˜¯Voiceboxå’ŒE2 TTSçš„3.7å€ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨æ¼”ç¤ºé¡µé¢è·å¾—\footnote{[åœ¨çº¿]å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://lixuyuan102.github.io/Demo/%7D%E3%80%82">https://lixuyuan102.github.io/Demo/}ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.12399v2">PDF</a> Accepted by IEEE Transactions on Audio, Speech and Language   Processing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ çš„è¯­éŸ³å…‹éš†æ¨¡å‹SF-Speechã€‚è¯¥æ¨¡å‹é€šè¿‡é‡‡ç”¨è½»é‡çº§çš„å¤šé˜¶æ®µæ¨¡å—ç”Ÿæˆæ›´ç¡®å®šçš„åˆå§‹åˆ†å¸ƒï¼Œæœ‰æ•ˆæ‹‰ç›´ODEæ¨¡å‹çš„åå‘æ›²çº¿è½¨è¿¹ï¼Œä»è€Œæé«˜ç”Ÿæˆæ ·æœ¬çš„è´¨é‡å’Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSF-Speechåœ¨é›¶æ ·æœ¬TTSæ–¹æ³•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”æ±‚è§£æ­¥éª¤ä»…éœ€å››åˆ†ä¹‹ä¸€ï¼Œç”Ÿæˆé€Ÿåº¦å¤§çº¦æ˜¯Voiceboxå’ŒE2 TTSçš„3.7å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰æ¨¡å‹åœ¨é›¶æ ·æœ¬è¯­éŸ³å…‹éš†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨æ ‡å‡†é«˜æ–¯å™ªå£°ä½œä¸ºODEçš„åˆå§‹åˆ†å¸ƒä¼šå¯¼è‡´ç›®æ ‡æ‹Ÿåˆä¸­å‡ºç°å¤šä¸ªäº¤é›†ï¼Œä¸ºæ¨¡å‹è®­ç»ƒå¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>SF-Speechæ˜¯ä¸€ä¸ªæ–°é¢–çš„è¯­éŸ³å…‹éš†æ¨¡å‹ï¼ŒåŸºäºODEå’Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>SF-Speeché‡‡ç”¨è½»é‡çº§çš„å¤šé˜¶æ®µæ¨¡å—æ¥ç”Ÿæˆæ›´ç¡®å®šçš„åˆå§‹åˆ†å¸ƒï¼Œæœ‰æ•ˆæ‹‰ç›´ODEæ¨¡å‹çš„åå‘è½¨è¿¹ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSF-Speechåœ¨å¤šç§æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰é›¶æ ·æœ¬TTSæ–¹æ³•ã€‚</li>
<li>SF-Speechå‡å°‘äº†ODEæ±‚è§£çš„æ­¥éª¤éœ€æ±‚ï¼Œå¤§å¹…åº¦æé«˜äº†ç”Ÿæˆè¯­éŸ³çš„é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.12399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3f54ed6c1c036704cf78583341190999.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c42ed00e1a01701ddea1b8235a065904.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4806d14cab1e4eda416bd5e6b2c0f646.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bda72226910d0d2c9b20440eecf366b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-051e36ed8c714e7c8b117910852132c8.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Whistle-Data-Efficient-Multilingual-and-Crosslingual-Speech-Recognition-via-Weakly-Phonetic-Supervision"><a href="#Whistle-Data-Efficient-Multilingual-and-Crosslingual-Speech-Recognition-via-Weakly-Phonetic-Supervision" class="headerlink" title="Whistle: Data-Efficient Multilingual and Crosslingual Speech Recognition   via Weakly Phonetic Supervision"></a>Whistle: Data-Efficient Multilingual and Crosslingual Speech Recognition   via Weakly Phonetic Supervision</h2><p><strong>Authors:Saierdaer Yusuyin, Te Ma, Hao Huang, Wenbo Zhao, Zhijian Ou</strong></p>
<p>There exist three approaches for multilingual and crosslingual automatic speech recognition (MCL-ASR) - supervised pretraining with phonetic or graphemic transcription, and self-supervised pretraining. We find that pretraining with phonetic supervision has been underappreciated so far for MCL-ASR, while conceptually it is more advantageous for information sharing between different languages. This paper explores the approach of pretraining with weakly phonetic supervision towards data-efficient MCL-ASR, which is called Whistle. We relax the requirement of gold-standard human-validated phonetic transcripts, and obtain International Phonetic Alphabet (IPA) based transcription by leveraging the LanguageNet grapheme-to-phoneme (G2P) models. We construct a common experimental setup based on the CommonVoice dataset, called CV-Lang10, with 10 seen languages and 2 unseen languages. A set of experiments are conducted on CV-Lang10 to compare, as fair as possible, the three approaches under the common setup for MCL-ASR. Experiments demonstrate the advantages of phoneme-based models (Whistle) for MCL-ASR, in terms of speech recognition for seen languages, crosslingual performance for unseen languages with different amounts of few-shot data, overcoming catastrophic forgetting, and training efficiency. It is found that when training data is more limited, phoneme supervision can achieve better results compared to subword supervision and self-supervision, thereby providing higher data-efficiency. To support reproducibility and promote future research along this direction, we release the code, models and data for the entire pipeline of Whistle at <a target="_blank" rel="noopener" href="https://github.com/thu-spmi/CAT/tree/master/egs/cv-lang10">https://github.com/thu-spmi/CAT/tree/master/egs/cv-lang10</a>. </p>
<blockquote>
<p>å¯¹äºå¤šè¯­è¨€å’Œè·¨è¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆMCL-ASRï¼‰ï¼Œå­˜åœ¨ä¸‰ç§æ–¹æ³•ï¼šå¸¦æœ‰è¯­éŸ³æˆ–å­—å½¢è½¬å½•çš„ç›‘ç£é¢„è®­ç»ƒä»¥åŠè‡ªç›‘ç£é¢„è®­ç»ƒã€‚æˆ‘ä»¬å‘ç°è¿„ä»Šä¸ºæ­¢ï¼Œå¯¹äºMCL-ASRè€Œè¨€ï¼Œç›‘ç£é¢„è®­ç»ƒçš„é‡è¦æ€§å°šæœªå¾—åˆ°å……åˆ†è®¤å¯ï¼Œå°½ç®¡åœ¨æ¦‚å¿µä¸Šå®ƒåœ¨ä¸åŒè¯­è¨€ä¹‹é—´çš„ä¿¡æ¯å…±äº«æ–¹é¢å…·æœ‰æ›´å¤šä¼˜åŠ¿ã€‚æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å¼±è¯­éŸ³ç›‘ç£è¿›è¡Œé¢„è®­ç»ƒçš„æ–¹æ³•ï¼Œä»¥å®ç°æ•°æ®é«˜æ•ˆçš„å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼Œç§°ä¸ºâ€œå£å“¨â€ï¼ˆWhistleï¼‰ã€‚æˆ‘ä»¬æ”¾å®½äº†å¯¹é‡‘æ ‡å‡†äººå·¥éªŒè¯è¯­éŸ³è½¬å½•çš„è¦æ±‚ï¼Œå¹¶åˆ©ç”¨LanguageNetçš„å­—å½¢åˆ°è¯­éŸ³ï¼ˆG2Pï¼‰æ¨¡å‹è·å–åŸºäºå›½é™…è¯­éŸ³å­—æ¯è¡¨çš„è½¬å½•ã€‚æˆ‘ä»¬åŸºäºCommonVoiceæ•°æ®é›†æ„å»ºäº†ä¸€ä¸ªé€šç”¨å®éªŒè®¾ç½®ï¼Œç§°ä¸ºCV-Lang10ï¼ŒåŒ…å«10ç§å¯è§è¯­è¨€å’Œä¸¤ç§æœªè§è¯­è¨€ã€‚åœ¨CV-Lang10ä¸Šè¿›è¡Œäº†ä¸€ç³»åˆ—å®éªŒï¼Œå°½å¯èƒ½å…¬å¹³åœ°æ¯”è¾ƒä¸‰ç§MCL-ASRæ–¹æ³•çš„é¢„è®­ç»ƒæ–¹æ³•ã€‚å®éªŒè¯æ˜äº†åŸºäºéŸ³ç´ æ¨¡å‹çš„å£å“¨åœ¨å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ–¹é¢çš„ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬è¯†åˆ«å¯è§è¯­è¨€çš„è¯­éŸ³ã€åœ¨ä¸åŒæ•°é‡çš„å°‘é‡æ•°æ®ä¸‹å¯¹æœªè§è¯­è¨€çš„è·¨è¯­è¨€æ€§èƒ½ã€å…‹æœç¾éš¾æ€§é—å¿˜ä»¥åŠè®­ç»ƒæ•ˆç‡æ–¹é¢ã€‚å‘ç°å½“è®­ç»ƒæ•°æ®æ›´åŠ æœ‰é™æ—¶ï¼ŒéŸ³ç´ ç›‘ç£å¯ä»¥æ¯”è¯ç´ ç›‘ç£å’Œè‡ªæˆ‘ç›‘ç£å®ç°æ›´å¥½çš„ç»“æœï¼Œä»è€Œæä¾›äº†æ›´é«˜çš„æ•°æ®æ•ˆç‡ã€‚ä¸ºäº†æ”¯æŒå¯é‡å¤æ€§å’Œä¿ƒè¿›æœªæ¥åœ¨è¿™ä¸€æ–¹å‘çš„ç ”ç©¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†æ•´ä¸ªå£å“¨ç®¡é“çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®ï¼š<a target="_blank" rel="noopener" href="https://github.com/thu-spmi/CAT/tree/master/egs/cv-lang10">https://github.com/thu-spmi/CAT/tree/master/egs/cv-lang10</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02166v2">PDF</a> Accepted by IEEE-TASLP</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šè¯­è¨€å’Œè·¨è¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆMCL-ASRï¼‰çš„ä¸‰ç§é¢„è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬è¯­éŸ³æˆ–å­—å½¢è½¬å½•çš„æœ‰ç›‘ç£é¢„è®­ç»ƒã€è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œä»¥åŠå¼±è¯­éŸ³ç›‘ç£é¢„è®­ç»ƒã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå¼±è¯­éŸ³ç›‘ç£é¢„è®­ç»ƒç­–ç•¥åœ¨æ•°æ®æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¯­éŸ³ç›‘ç£ç›¸è¾ƒäºå­è¯ç›‘ç£å’Œè‡ªç›‘ç£èƒ½å–å¾—æ›´å¥½çš„ç»“æœã€‚å®éªŒè¯æ˜ï¼Œè¯¥ç­–ç•¥åœ¨å·²çŸ¥è¯­è¨€çš„è¯­éŸ³è¯†åˆ«ã€ä¸åŒå°‘é‡æ•°æ®çš„æœªçŸ¥è¯­è¨€çš„è·¨è¯­è¨€æ€§èƒ½ã€å…‹æœç¾éš¾æ€§é—å¿˜å’Œæé«˜è®­ç»ƒæ•ˆç‡ç­‰æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šè¯­è¨€å’Œè·¨è¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆMCL-ASRï¼‰å­˜åœ¨ä¸‰ç§é¢„è®­ç»ƒç­–ç•¥ï¼šè¯­éŸ³æˆ–å­—å½¢è½¬å½•çš„æœ‰ç›‘ç£é¢„è®­ç»ƒã€è‡ªç›‘ç£é¢„è®­ç»ƒä»¥åŠå¼±è¯­éŸ³ç›‘ç£é¢„è®­ç»ƒã€‚</li>
<li>å¼±è¯­éŸ³ç›‘ç£é¢„è®­ç»ƒç­–ç•¥åœ¨æ•°æ®æ•ˆç‡æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯å½“è®­ç»ƒæ•°æ®æœ‰é™æ—¶ï¼Œè¯­éŸ³ç›‘ç£ç›¸è¾ƒäºå…¶ä»–æ–¹æ³•èƒ½å–å¾—æ›´å¥½çš„ç»“æœã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œå¼±è¯­éŸ³ç›‘ç£é¢„è®­ç»ƒç­–ç•¥åœ¨å·²çŸ¥è¯­è¨€çš„è¯­éŸ³è¯†åˆ«å’ŒæœªçŸ¥è¯­è¨€çš„è·¨è¯­è¨€æ€§èƒ½ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥ç­–ç•¥æœ‰åŠ©äºå…‹æœç¾éš¾æ€§é—å¿˜å¹¶æé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>ä¸ºäº†æ”¯æŒå¯é‡å¤æ€§å’Œä¿ƒè¿›æœªæ¥ç ”ç©¶ï¼Œç ”ç©¶è€…å…¬å¼€äº†æ•´ä¸ªç®¡é“çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®ã€‚</li>
<li>ä½¿ç”¨LanguageNetçš„grapheme-to-phonemeï¼ˆG2Pï¼‰æ¨¡å‹æ¥è·å¾—åŸºäºå›½é™…éŸ³æ ‡ï¼ˆIPAï¼‰çš„è½¬å½•ï¼Œé™ä½äº†å¯¹é‡‘æ ‡å‡†äººç±»éªŒè¯è¯­éŸ³è½¬å½•çš„è¦æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.02166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-80fb18902e5db09ed16f2f996824fde0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de896ec51cbddef9b695822ccb19bc9d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45d66f7e06e1909abb010bd885538119.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MoReVQA-Exploring-Modular-Reasoning-Models-for-Video-Question-Answering"><a href="#MoReVQA-Exploring-Modular-Reasoning-Models-for-Video-Question-Answering" class="headerlink" title="MoReVQA: Exploring Modular Reasoning Models for Video Question Answering"></a>MoReVQA: Exploring Modular Reasoning Models for Video Question Answering</h2><p><strong>Authors:Juhong Min, Shyamal Buch, Arsha Nagrani, Minsu Cho, Cordelia Schmid</strong></p>
<p>This paper addresses the task of video question answering (videoQA) via a decomposed multi-stage, modular reasoning framework. Previous modular methods have shown promise with a single planning stage ungrounded in visual content. However, through a simple and effective baseline, we find that such systems can lead to brittle behavior in practice for challenging videoQA settings. Thus, unlike traditional single-stage planning methods, we propose a multi-stage system consisting of an event parser, a grounding stage, and a final reasoning stage in conjunction with an external memory. All stages are training-free, and performed using few-shot prompting of large models, creating interpretable intermediate outputs at each stage. By decomposing the underlying planning and task complexity, our method, MoReVQA, improves over prior work on standard videoQA benchmarks (NExT-QA, iVQA, EgoSchema, ActivityNet-QA) with state-of-the-art results, and extensions to related tasks (grounded videoQA, paragraph captioning). </p>
<blockquote>
<p>æœ¬æ–‡é€šè¿‡ä¸€ä¸ªåˆ†è§£çš„å¤šé˜¶æ®µã€æ¨¡å—åŒ–æ¨ç†æ¡†æ¶æ¥è§£å†³è§†é¢‘é—®ç­”ï¼ˆVideoQAï¼‰çš„ä»»åŠ¡ã€‚è™½ç„¶ä¹‹å‰çš„æ¨¡å—åŒ–æ–¹æ³•åœ¨å•ä¸€è§„åˆ’é˜¶æ®µæœªåŸºäºè§†è§‰å†…å®¹å±•ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ï¼Œä½†æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„åŸºçº¿å‘ç°ï¼Œåœ¨æŒ‘æˆ˜æ€§è¾ƒå¤§çš„è§†é¢‘é—®ç­”åœºæ™¯ä¸­ï¼Œè¿™æ ·çš„ç³»ç»Ÿå®é™…ä¸Šå¯èƒ½å¯¼è‡´è¡Œä¸ºå˜å¾—è„†å¼±ã€‚å› æ­¤ï¼Œä¸åŒäºä¼ ç»Ÿçš„å•é˜¶æ®µè§„åˆ’æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µç³»ç»Ÿï¼Œè¯¥ç³»ç»ŸåŒ…æ‹¬äº‹ä»¶è§£æå™¨ã€å®šä½é˜¶æ®µå’Œä¸å¤–éƒ¨è®°å¿†ç›¸ç»“åˆçš„æœ€ç»ˆæ¨ç†é˜¶æ®µã€‚æ‰€æœ‰é˜¶æ®µå‡æ— éœ€è®­ç»ƒï¼Œåˆ©ç”¨å¤§å‹æ¨¡å‹çš„å°‘é‡æç¤ºè¿›è¡Œå°‘æ•°é•œå¤´æç¤ºå³å¯å®Œæˆï¼Œæ¯ä¸ªé˜¶æ®µéƒ½ä¼šäº§ç”Ÿå¯è§£é‡Šçš„ä¸­é—´è¾“å‡ºã€‚é€šè¿‡åˆ†è§£åº•å±‚è§„åˆ’å’Œä»»åŠ¡å¤æ‚æ€§ï¼Œæˆ‘ä»¬çš„MoReVQAæ–¹æ³•åœ¨ä¼ ç»Ÿè§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ï¼ˆNExT-QAã€iVQAã€EgoSchemaã€ActivityNet-QAï¼‰ä¸Šå®ç°äº†è¶…è¶Šå…ˆå‰å·¥ä½œçš„æœ€æ–°ç»“æœï¼Œå¹¶ä¸”å¯ä»¥æ‰©å±•åˆ°ç›¸å…³ä»»åŠ¡ï¼ˆåŸºäºå®šä½çš„è§†é¢‘é—®ç­”ã€æ®µè½æ ‡æ³¨ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.06511v2">PDF</a> CVPR 2024; updated NExT-GQA results in Appendix</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç”¨äºè§†é¢‘é—®ç­”ï¼ˆVideoQAï¼‰çš„åˆ†è§£å¼å¤šé˜¶æ®µæ¨¡å—åŒ–æ¨ç†æ¡†æ¶ã€‚é’ˆå¯¹ä¼ ç»Ÿå•é˜¶æ®µè§„åˆ’æ–¹æ³•åœ¨å®è·µä¸­é¢å¯¹æŒ‘æˆ˜è§†é¢‘é—®ç­”åœºæ™¯æ—¶è¡¨ç°å‡ºçš„è„†å¼±æ€§ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µç³»ç»Ÿï¼ŒåŒ…æ‹¬äº‹ä»¶è§£æå™¨ã€å®šä½é˜¶æ®µå’Œæœ€ç»ˆæ¨ç†é˜¶æ®µä¸å¤–éƒ¨å­˜å‚¨ç›¸ç»“åˆã€‚æ‰€æœ‰é˜¶æ®µéƒ½æ˜¯æ— è®­ç»ƒçš„ï¼Œåˆ©ç”¨å¤§å‹æ¨¡å‹çš„å°‘é‡æç¤ºè¿›è¡Œæ‰§è¡Œï¼Œæ¯ä¸ªé˜¶æ®µéƒ½èƒ½äº§ç”Ÿå¯è§£é‡Šçš„ä¸­é—´è¾“å‡ºã€‚é€šè¿‡åˆ†è§£è§„åˆ’å’Œä»»åŠ¡å¤æ‚æ€§ï¼ŒMoReVQAæ–¹æ³•åœ¨æ ‡å‡†è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ï¼ˆå¦‚NExT-QAã€iVQAã€EgoSchemaã€ActivityNet-QAï¼‰ä¸Šå–å¾—äº†ä¼˜äºå…ˆå‰å·¥ä½œçš„æœ€æ–°ç»“æœï¼Œå¹¶æ‰©å±•åˆ°ç›¸å…³ä»»åŠ¡ï¼ˆå¦‚åŸºäºå®šä½çš„è§†é¢‘é—®ç­”ã€æ®µè½æ ‡é¢˜ç”Ÿæˆï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘é—®ç­”æ–¹æ³•ï¼Œå³åˆ†è§£å¼å¤šé˜¶æ®µæ¨¡å—åŒ–æ¨ç†æ¡†æ¶ã€‚</li>
<li>é’ˆå¯¹ä¼ ç»Ÿå•é˜¶æ®µè§„åˆ’æ–¹æ³•åœ¨è§†é¢‘é—®ç­”ä¸­çš„å±€é™æ€§ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§å¤šé˜¶æ®µç³»ç»Ÿã€‚</li>
<li>è¯¥ç³»ç»ŸåŒ…æ‹¬äº‹ä»¶è§£æå™¨ã€å®šä½é˜¶æ®µå’Œæœ€ç»ˆæ¨ç†é˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰å…¶ç‹¬ç‰¹çš„åŠŸèƒ½ã€‚</li>
<li>ç³»ç»Ÿç»“åˆäº†å¤–éƒ¨å­˜å‚¨ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢ä¸è§†é¢‘å†…å®¹ç›¸å…³çš„ä¿¡æ¯ã€‚</li>
<li>æ‰€æœ‰é˜¶æ®µéƒ½æ˜¯æ— è®­ç»ƒçš„ï¼Œåˆ©ç”¨å¤§å‹æ¨¡å‹çš„å°‘é‡æç¤ºè¿›è¡Œæ‰§è¡Œã€‚</li>
<li>MoReVQAæ–¹æ³•åœ¨å¤šä¸ªæ ‡å‡†è§†é¢‘é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€æ–°ç»“æœï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.06511">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f3876e30bcd8a69dacbc4b683d5f0b25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0783aaa8817a2662b847469a195f02f9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cd87dfb19568a61852fda6c7bd512357.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17ad09f2688126639287ebea8a605673.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Holistic-Evaluation-of-Piano-Sound-Quality"><a href="#A-Holistic-Evaluation-of-Piano-Sound-Quality" class="headerlink" title="A Holistic Evaluation of Piano Sound Quality"></a>A Holistic Evaluation of Piano Sound Quality</h2><p><strong>Authors:Monan Zhou, Shangda Wu, Shaohua Ji, Zijin Li, Wei Li</strong></p>
<p>This paper aims to develop a holistic evaluation method for piano sound quality to assist in purchasing decisions. Unlike previous studies that focused on the effect of piano performance techniques on sound quality, this study evaluates the inherent sound quality of different pianos. To derive quality evaluation systems, the study uses subjective questionnaires based on a piano sound quality dataset. The method selects the optimal piano classification models by comparing the fine-tuning results of different pre-training models of Convolutional Neural Networks (CNN). To improve the interpretability of the models, the study applies Equivalent Rectangular Bandwidth (ERB) analysis. The results reveal that musically trained individuals are better able to distinguish between the sound quality differences of different pianos. The best fine-tuned CNN pre-trained backbone achieves a high accuracy of 98.3% as the piano classifier. However, the dataset is limited, and the audio is sliced to increase its quantity, resulting in a lack of diversity and balance, so we use focal loss to reduce the impact of data imbalance. To optimize the method, the dataset will be expanded, or few-shot learning techniques will be employed in future research. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨å¼€å‘ä¸€ç§å…¨é¢çš„é’¢ç´éŸ³è´¨è¯„ä¼°æ–¹æ³•ï¼Œä»¥è¾…åŠ©è´­ä¹°å†³ç­–ã€‚ä¸åŒäºä»¥å¾€ä¸“æ³¨äºé’¢ç´æ¼”å¥æŠ€å·§å¯¹éŸ³è´¨å½±å“çš„ç ”ç©¶ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°ä¸åŒé’¢ç´çš„å›ºæœ‰éŸ³è´¨ã€‚ä¸ºäº†å¾—å‡ºè´¨é‡è¯„ä¼°ç³»ç»Ÿï¼Œç ”ç©¶åŸºäºé’¢ç´éŸ³è´¨æ•°æ®é›†é‡‡ç”¨ä¸»è§‚é—®å·è°ƒæŸ¥çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æ¯”è¾ƒä¸åŒé¢„è®­ç»ƒæ¨¡å‹çš„å¾®è°ƒç»“æœï¼Œé€‰æ‹©æœ€ä½³çš„é’¢ç´åˆ†ç±»æ¨¡å‹ã€‚ä¸ºäº†æé«˜æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œç ”ç©¶é‡‡ç”¨äº†ç­‰æ•ˆçŸ©å½¢å¸¦å®½ï¼ˆERBï¼‰åˆ†æã€‚ç»“æœè¡¨æ˜ï¼Œå—è¿‡éŸ³ä¹è®­ç»ƒçš„äººæ›´èƒ½åŒºåˆ†ä¸åŒé’¢ç´çš„éŸ³è´¨å·®å¼‚ã€‚æœ€ä½³çš„å¾®è°ƒCNNé¢„è®­ç»ƒéª¨å¹²ä½œä¸ºé’¢ç´åˆ†ç±»å™¨ï¼Œå®ç°äº†é«˜è¾¾98.3%çš„å‡†ç¡®ç‡ã€‚ç„¶è€Œï¼Œæ•°æ®é›†æœ‰é™ï¼ŒéŸ³é¢‘è¢«åˆ‡ç‰‡ä»¥å¢åŠ å…¶æ•°é‡ï¼Œå¯¼è‡´ç¼ºä¹å¤šæ ·æ€§å’Œå¹³è¡¡æ€§ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨ç„¦ç‚¹æŸå¤±æ¥å‡å°‘æ•°æ®ä¸å¹³è¡¡çš„å½±å“ã€‚ä¸ºäº†ä¼˜åŒ–è¯¥æ–¹æ³•ï¼Œæœªæ¥å°†æ‰©å¤§æ•°æ®é›†æˆ–é‡‡ç”¨å°æ ·æœ¬å­¦ä¹ æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04722v2">PDF</a> 15 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨å¼€å‘ä¸€ç§å…¨é¢çš„é’¢ç´éŸ³è´¨è¯„ä¼°æ–¹æ³•ï¼Œä»¥è¾…åŠ©è´­ä¹°å†³ç­–ã€‚ç ”ç©¶ä¸åŒäºä»¥å¾€å…³æ³¨é’¢ç´æ¼”å¥æŠ€å·§å¯¹éŸ³è´¨å½±å“çš„ç ”ç©¶ï¼Œè€Œæ˜¯è¯„ä¼°ä¸åŒé’¢ç´çš„å›ºæœ‰éŸ³è´¨ã€‚ç ”ç©¶ä½¿ç”¨åŸºäºé’¢ç´éŸ³è´¨æ•°æ®é›†çš„ä¸»è§‚é—®å·æ¥æ¨å¯¼è´¨é‡è¯„ä¼°ç³»ç»Ÿï¼Œé€šè¿‡æ¯”è¾ƒä¸åŒé¢„è®­ç»ƒæ¨¡å‹çš„å¾®è°ƒç»“æœæ¥é€‰æ‹©æœ€ä½³çš„é’¢ç´åˆ†ç±»æ¨¡å‹ã€‚ä¸ºæé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œç ”ç©¶åº”ç”¨äº†ç­‰æ•ˆçŸ©å½¢å¸¦å®½ï¼ˆERBï¼‰åˆ†æã€‚ç»“æœæ˜¾ç¤ºï¼Œå—è¿‡éŸ³ä¹è®­ç»ƒçš„äººæ›´èƒ½åŒºåˆ†ä¸åŒé’¢ç´çš„éŸ³è´¨å·®å¼‚ã€‚æœ€ä½³å¾®è°ƒCNNé¢„è®­ç»ƒä¸»å¹²ä½œä¸ºé’¢ç´åˆ†ç±»å™¨ï¼Œå‡†ç¡®ç‡é«˜è¾¾98.3%ã€‚ä½†æ•°æ®é›†æœ‰é™ï¼ŒéŸ³é¢‘è¢«åˆ‡ç‰‡ä»¥å¢åŠ æ•°é‡ï¼Œå¯¼è‡´ç¼ºä¹å¤šæ ·æ€§å’Œå¹³è¡¡æ€§ï¼Œå› æ­¤ä½¿ç”¨ç„¦ç‚¹æŸå¤±æ¥å‡å°‘æ•°æ®ä¸å¹³è¡¡çš„å½±å“ã€‚æœªæ¥ç ”ç©¶å°†æ‰©å¤§æ•°æ®é›†æˆ–é‡‡ç”¨å°‘é‡å­¦ä¹ æŠ€æœ¯æ¥ä¼˜åŒ–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ—¨åœ¨å¼€å‘å…¨é¢çš„é’¢ç´éŸ³è´¨è¯„ä¼°æ–¹æ³•ï¼Œä¾§é‡äºä¸åŒé’¢ç´çš„å›ºæœ‰éŸ³è´¨ã€‚</li>
<li>é€šè¿‡ä¸»è§‚é—®å·å’ŒåŸºäºæ•°æ®é›†çš„æ–¹æ³•æ¨å¯¼è´¨é‡è¯„ä¼°ç³»ç»Ÿã€‚</li>
<li>ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶é€šè¿‡å¾®è°ƒé€‰æ‹©æœ€ä½³é’¢ç´åˆ†ç±»æ¨¡å‹ã€‚</li>
<li>åº”ç”¨ç­‰æ•ˆçŸ©å½¢å¸¦å®½ï¼ˆERBï¼‰åˆ†ææé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>è°ƒæŸ¥æ˜¾ç¤ºï¼Œå—è¿‡éŸ³ä¹è®­ç»ƒçš„äººæ›´èƒ½åŒºåˆ†ä¸åŒé’¢ç´çš„éŸ³è´¨å·®å¼‚ã€‚</li>
<li>æœ€ä½³é’¢ç´åˆ†ç±»æ¨¡å‹çš„å‡†ç¡®ç‡é«˜è¾¾98.3%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.04722">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c1967dbffcacea616f7162e6324f070d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2104d7163056a68a5f71e6e6198337cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ed02193f2135c9d56dd83e195be3db6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9dce1c3b72c3ff601ac512173c4cd29.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Learning-Multi-modal-Representations-by-Watching-Hundreds-of-Surgical-Video-Lectures"><a href="#Learning-Multi-modal-Representations-by-Watching-Hundreds-of-Surgical-Video-Lectures" class="headerlink" title="Learning Multi-modal Representations by Watching Hundreds of Surgical   Video Lectures"></a>Learning Multi-modal Representations by Watching Hundreds of Surgical   Video Lectures</h2><p><strong>Authors:Kun Yuan, Vinkle Srivastav, Tong Yu, Joel L. Lavanchy, Jacques Marescaux, Pietro Mascagni, Nassir Navab, Nicolas Padoy</strong></p>
<p>Recent advancements in surgical computer vision applications have been driven by vision-only models, which do not explicitly integrate the rich semantics of language into their design. These methods rely on manually annotated surgical videos to predict a fixed set of object categories, limiting their generalizability to unseen surgical procedures and downstream tasks. In this work, we put forward the idea that the surgical video lectures available through open surgical e-learning platforms can provide effective vision and language supervisory signals for multi-modal representation learning without relying on manual annotations. We address the surgery-specific linguistic challenges present in surgical video lectures by employing multiple complementary automatic speech recognition systems to generate text transcriptions. We then present a novel method, SurgVLP - Surgical Vision Language Pre-training, for multi-modal representation learning. Extensive experiments across diverse surgical procedures and tasks demonstrate that the multi-modal representations learned by SurgVLP exhibit strong transferability and adaptability in surgical video analysis. Furthermore, our zero-shot evaluations highlight SurgVLPâ€™s potential as a general-purpose foundation model for surgical workflow analysis, reducing the reliance on extensive manual annotations for downstream tasks, and facilitating adaptation methods such as few-shot learning to build a scalable and data-efficient solution for various downstream surgical applications. The <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SurgVLP">training code</a> and <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/PeskaVLP">weights</a> are public. </p>
<blockquote>
<p>è¿‘æœŸå¤–ç§‘è®¡ç®—æœºè§†è§‰åº”ç”¨çš„è¿›å±•ä¸»è¦å¾—ç›Šäºä»…ä¾èµ–è§†è§‰çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨è®¾è®¡æ—¶å¹¶æ²¡æœ‰æ˜ç¡®èå…¥ä¸°å¯Œçš„è¯­è¨€è¯­ä¹‰ã€‚è¿™äº›æ–¹æ³•ä¾èµ–äºæ‰‹åŠ¨æ ‡æ³¨çš„æ‰‹æœ¯è§†é¢‘æ¥é¢„æµ‹å›ºå®šçš„ç›®æ ‡ç±»åˆ«ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬å¯¹æœªè§è¿‡çš„æ‰‹æœ¯ç¨‹åºå’Œä¸‹æ¸¸ä»»åŠ¡çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡å…¬å¼€çš„å¤–ç§‘ç”µå­å­¦ä¹ å¹³å°è·å–çš„æ‰‹æœ¯è§†é¢‘è®²åº§å¯ä»¥æä¾›æœ‰æ•ˆçš„è§†è§‰å’Œè¯­è¨€ç›‘ç£ä¿¡å·ï¼Œç”¨äºå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼Œè€Œæ— éœ€ä¾èµ–æ‰‹åŠ¨æ³¨é‡Šã€‚ä¸ºäº†è§£å†³æ‰‹æœ¯è§†é¢‘è®²åº§ä¸­ç‰¹æœ‰çš„è¯­è¨€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¤šä¸ªäº’è¡¥çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿæ¥ç”Ÿæˆæ–‡æœ¬è½¬å½•ã€‚ç„¶åæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³SurgVLPâ€”â€”å¤–ç§‘è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼Œç”¨äºå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ã€‚åœ¨å¤šç§æ‰‹æœ¯ç¨‹åºå’Œä»»åŠ¡ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSurgVLPæ‰€å­¦ä¹ çš„å¤šæ¨¡æ€è¡¨ç¤ºåœ¨æ‰‹æœ¯è§†é¢‘åˆ†æä¸­å…·æœ‰å¾ˆå¼ºçš„å¯è¿ç§»æ€§å’Œé€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„é›¶æ ·æœ¬è¯„ä¼°çªæ˜¾äº†SurgVLPä½œä¸ºå¤–ç§‘å·¥ä½œæµç¨‹åˆ†æçš„é€šç”¨åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ï¼Œå‡å°‘äº†ä¸‹æ¸¸ä»»åŠ¡å¯¹å¤§é‡æ‰‹åŠ¨æ³¨é‡Šçš„ä¾èµ–ï¼Œå¹¶ä¿ƒè¿›äº†å°‘æ ·æœ¬å­¦ä¹ ç­‰é€‚åº”æ–¹æ³•ï¼Œä¸ºå„ç§ä¸‹æ¸¸å¤–ç§‘åº”ç”¨æ„å»ºå¯æ‰©å±•å’Œé«˜æ•ˆçš„æ•°æ®è§£å†³æ–¹æ¡ˆã€‚<a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SurgVLP">è®­ç»ƒä»£ç </a>å’Œ<a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/PeskaVLP">æƒé‡</a>å‡å·²å…¬å¼€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15220v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æœ€è¿‘å¤–ç§‘è§†è§‰åº”ç”¨æ–¹é¢çš„è¿›å±•ï¼Œè¿™äº›è¿›å±•ä¸»è¦å¾—ç›Šäºä»…ä¾èµ–è§†è§‰çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨è®¾è®¡æ—¶å¹¶æœªæ˜ç¡®èå…¥ä¸°å¯Œçš„è¯­è¨€è¯­ä¹‰ï¼Œé™åˆ¶äº†å…¶åœ¨æœªè§è¿‡çš„æ‰‹æœ¯ç¨‹åºå’Œä¸‹æ¸¸ä»»åŠ¡ä¸­çš„é€šç”¨æ€§ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†åˆ©ç”¨å…¬å¼€æ‰‹æœ¯ç”µå­å­¦ä¹ å¹³å°ä¸Šçš„æ‰‹æœ¯è§†é¢‘è®²åº§è¿›è¡Œå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ çš„æ–°æ€è·¯ï¼Œé€šè¿‡å¤šä¸ªäº’è¡¥çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿåº”å¯¹æ‰‹æœ¯è§†é¢‘è®²åº§ä¸­çš„è¯­è¨€æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§æ–°å‹æ–¹æ³•SurgVLPï¼Œç”¨äºè¿›è¡Œå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ã€‚å®éªŒè¯æ˜ï¼Œé€šè¿‡SurgVLPå­¦åˆ°çš„å¤šæ¨¡æ€è¡¨ç¤ºå…·æœ‰å¼ºå¤§çš„è¿ç§»æ€§å’Œé€‚åº”æ€§ã€‚è¯¥æ¨¡å‹çš„é›¶æ ·æœ¬è¯„ä¼°å‡¸æ˜¾äº†å…¶ä½œä¸ºæ‰‹æœ¯å·¥ä½œæµç¨‹åˆ†æé€šç”¨åŸºç¡€æ¨¡å‹çš„æ½œåŠ›ï¼Œå‡å°‘äº†ä¸‹æ¸¸ä»»åŠ¡å¯¹å¤§é‡æ‰‹åŠ¨æ³¨é‡Šçš„ä¾èµ–ï¼Œå¹¶ä¿ƒè¿›äº†å¦‚å°æ ·æœ¬å­¦ä¹ ç­‰é€‚åº”æ–¹æ³•çš„å¼€å‘ï¼Œä¸ºå„ç§ä¸‹æ¸¸æ‰‹æœ¯åº”ç”¨æ„å»ºå¯æ‰©å±•å’Œé«˜æ•ˆçš„æ•°æ®è§£å†³æ–¹æ¡ˆã€‚æ¨¡å‹å’Œæƒé‡å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸå¤–ç§‘è§†è§‰åº”ç”¨çš„è¿›å±•ä¸»è¦ä¾èµ–ä»…ä¾èµ–è§†è§‰çš„æ¨¡å‹é©±åŠ¨ï¼Œä½†ç¼ºä¹æ•´åˆä¸°å¯Œçš„è¯­è¨€è¯­ä¹‰ã€‚</li>
<li>æ‰‹åŠ¨æ³¨é‡Šçš„æ‰‹æœ¯è§†é¢‘é™åˆ¶äº†æ¨¡å‹çš„é€šç”¨æ€§ã€‚</li>
<li>åˆ©ç”¨å…¬å¼€æ‰‹æœ¯ç”µå­å­¦ä¹ å¹³å°ä¸Šçš„æ‰‹æœ¯è§†é¢‘è®²åº§è¿›è¡Œå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ æ˜¯ä¸€ç§æ–°æ€è·¯ã€‚</li>
<li>é€šè¿‡å¤šä¸ªè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿåº”å¯¹æ‰‹æœ¯è§†é¢‘è®²åº§ä¸­çš„è¯­è¨€æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ–¹æ³•SurgVLPç”¨äºå¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>SurgVLPæ¨¡å‹å…·æœ‰è¾ƒå¼ºçš„è¿ç§»æ€§å’Œé€‚åº”æ€§ï¼Œå·²è¿›è¡Œå¹¿æ³›å®éªŒéªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2307.15220">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90ed6c7b07edbbcc0c936921a71fd51f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1403ccbf9c4143513d490e722b7e05c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c3add5ad8609d1635f8f4d1ed6baed8.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-29/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-29/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-29/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-149af2b24c3ff7f7e8e6c651c07b3699.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-29  MotionDiff Training-free Zero-shot Interactive Motion Editing via   Flow-assisted Multi-view Diffusion
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-29/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7696588ea2e40da3db57c7071a79a81a.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-29  MemInsight Autonomous Memory Augmentation for LLM Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">18179.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
