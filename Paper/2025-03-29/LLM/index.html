<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-29  Video-R1 Reinforcing Video Reasoning in MLLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c3b9e5fdecfd4fa108ec125db048b93f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-29
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-29-æ›´æ–°"><a href="#2025-03-29-æ›´æ–°" class="headerlink" title="2025-03-29 æ›´æ–°"></a>2025-03-29 æ›´æ–°</h1><h2 id="Video-R1-Reinforcing-Video-Reasoning-in-MLLMs"><a href="#Video-R1-Reinforcing-Video-Reasoning-in-MLLMs" class="headerlink" title="Video-R1: Reinforcing Video Reasoning in MLLMs"></a>Video-R1: Reinforcing Video Reasoning in MLLMs</h2><p><strong>Authors:Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, Xiangyu Yue</strong></p>
<p>Inspired by DeepSeek-R1â€™s success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for eliciting video reasoning within multimodal large language models (MLLMs). However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data. To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning. Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process. We have constructed two datasets: Video-R1-COT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data. Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc. Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All codes, models, data are released. </p>
<blockquote>
<p>å—DeepSeek-R1é€šè¿‡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¿€å‘æ¨ç†èƒ½åŠ›æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æ¨å‡ºVideo-R1ï¼Œä½œä¸ºé¦–æ¬¡ç³»ç»Ÿæ€§æ¢ç´¢åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­è¿›è¡Œè§†é¢‘æ¨ç†çš„R1èŒƒå¼ã€‚ç„¶è€Œï¼Œç›´æ¥ä½¿ç”¨åŸºäºGRPOç®—æ³•çš„RLè®­ç»ƒè¿›è¡Œè§†é¢‘æ¨ç†é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šï¼ˆiï¼‰è§†é¢‘æ¨ç†ç¼ºä¹æ—¶é—´å»ºæ¨¡ï¼›ï¼ˆiiï¼‰é«˜è´¨é‡è§†é¢‘æ¨ç†æ•°æ®ç¨€ç¼ºã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºT-GRPOç®—æ³•ï¼Œè¯¥ç®—æ³•é¼“åŠ±æ¨¡å‹åœ¨æ¨ç†æ—¶åˆ©ç”¨è§†é¢‘ä¸­çš„æ—¶é—´ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸æ˜¯ä»…ä¾èµ–è§†é¢‘æ•°æ®ï¼Œè€Œæ˜¯å°†é«˜è´¨é‡å›¾åƒæ¨ç†æ•°æ®çº³å…¥è®­ç»ƒè¿‡ç¨‹ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸¤ä¸ªæ•°æ®é›†ï¼šç”¨äºSFTå†·å¯åŠ¨çš„Video-R1-COT-165kå’Œç”¨äºRLè®­ç»ƒçš„è§†é¢‘R1-260kï¼Œä¸¤è€…éƒ½åŒ…å«å›¾åƒå’Œè§†é¢‘æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideo-R1åœ¨è§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå¦‚VideoMMMUå’ŒVSI-Benchï¼‰ä»¥åŠé€šç”¨è§†é¢‘åŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬MVBenchå’ŒTempCompassç­‰ï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒVideo-R1-7Båœ¨è§†é¢‘ç©ºé—´æ¨ç†åŸºå‡†VSI-benchä¸Šè¾¾åˆ°äº†35.8%çš„å‡†ç¡®ç‡ï¼Œè¶…è¿‡äº†å•†ä¸šä¸“æœ‰æ¨¡å‹GPT-4oã€‚æ‰€æœ‰ä»£ç ã€æ¨¡å‹ã€æ•°æ®å‡å·²å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21776v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://github.com/tulerfeng/Video-R1">https://github.com/tulerfeng/Video-R1</a></p>
<p><strong>Summary</strong><br>åŸºäºDeepSeek-R1é€šè¿‡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¿€å‘æ¨ç†èƒ½åŠ›çš„æˆåŠŸï¼Œæˆ‘ä»¬æ¨å‡ºVideo-R1ï¼Œé¦–æ¬¡å°è¯•åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­ç³»ç»Ÿåœ°æ¢ç´¢R1èŒƒå¼ä»¥æ¿€å‘è§†é¢‘æ¨ç†èƒ½åŠ›ã€‚é’ˆå¯¹ç›´æ¥åº”ç”¨RLè®­ç»ƒå’ŒGRPOç®—æ³•è¿›è¡Œè§†é¢‘æ¨ç†æ‰€é¢ä¸´çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†T-GRPOç®—æ³•ï¼Œé¼“åŠ±æ¨¡å‹åˆ©ç”¨è§†é¢‘ä¸­çš„æ—¶é—´ä¿¡æ¯è¿›è¡Œæ¨ç†ï¼›åŒæ—¶ï¼Œæˆ‘ä»¬ä¸åªæ˜¯ä¾èµ–è§†é¢‘æ•°æ®ï¼Œè¿˜ç»“åˆäº†é«˜è´¨é‡å›¾åƒæ¨ç†æ•°æ®æ¥è®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideo-R1åœ¨è§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆå¦‚VideoMMMUå’ŒVSI-Benchï¼‰ä»¥åŠé€šç”¨è§†é¢‘åŸºå‡†æµ‹è¯•ï¼ˆå¦‚MVBenchå’ŒTempCompassç­‰ï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ç‰¹åˆ«æ˜¯Video-R1-7Båœ¨VSI-benchè§†é¢‘ç©ºé—´æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†35.8%ï¼Œè¶…è¿‡äº†å•†ä¸šä¸“æœ‰æ¨¡å‹GPT-4oã€‚æ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å‡å·²å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Video-R1æ˜¯é¦–ä¸ªå°è¯•åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ç³»ç»Ÿåœ°æ¢ç´¢R1èŒƒå¼ä»¥æ¿€å‘è§†é¢‘æ¨ç†èƒ½åŠ›çš„é¡¹ç›®ã€‚</li>
<li>ç›´æ¥åº”ç”¨RLè®­ç»ƒå’ŒGRPOç®—æ³•è¿›è¡Œè§†é¢‘æ¨ç†é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šç¼ºä¹è§†é¢‘æ¨ç†çš„æ—¶é—´å»ºæ¨¡å’Œé«˜è´¨é‡è§†é¢‘æ¨ç†æ•°æ®çš„ç¨€ç¼ºæ€§ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†T-GRPOç®—æ³•ï¼Œé¼“åŠ±æ¨¡å‹åˆ©ç”¨è§†é¢‘ä¸­çš„æ—¶é—´ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚</li>
<li>é™¤äº†è§†é¢‘æ•°æ®å¤–ï¼Œè¿˜ç»“åˆäº†é«˜è´¨é‡å›¾åƒæ¨ç†æ•°æ®æ¥è¿›è¡Œè®­ç»ƒã€‚</li>
<li>Video-R1åœ¨å¤šä¸ªè§†é¢‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>Video-R1-7Båœ¨VSI-benchä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†35.8%ï¼Œè¶…è¿‡äº†GPT-4oã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21776">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9e596413b0399fd7c3ae1db4e68e0947.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04b0a066291d6d6862848451fd241ae9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37bfe5371e2dfc7f4fc8cbcb303172bd.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MemInsight-Autonomous-Memory-Augmentation-for-LLM-Agents"><a href="#MemInsight-Autonomous-Memory-Augmentation-for-LLM-Agents" class="headerlink" title="MemInsight: Autonomous Memory Augmentation for LLM Agents"></a>MemInsight: Autonomous Memory Augmentation for LLM Agents</h2><p><strong>Authors:Rana Salama, Jason Cai, Michelle Yuan, Anna Currey, Monica Sunkara, Yi Zhang, Yassine Benajiba</strong></p>
<p>Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools. A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge. However, the growing memory size and need for semantic structuring pose significant challenges. In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms. By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses. We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization. On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%. Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval. Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†å·²ç»è¿›åŒ–åˆ°å¯ä»¥æ™ºèƒ½åœ°å¤„ç†ä¿¡æ¯ã€åšå‡ºå†³ç­–ä»¥åŠä¸ç”¨æˆ·æˆ–å·¥å…·è¿›è¡Œäº¤äº’ã€‚ä¸€ä¸ªå…³é”®çš„èƒ½åŠ›æ˜¯èåˆäº†é•¿æœŸè®°å¿†èƒ½åŠ›ï¼Œä½¿è¿™äº›ä»£ç†èƒ½å¤Ÿåˆ©ç”¨å†å²äº’åŠ¨å’ŒçŸ¥è¯†ã€‚ç„¶è€Œï¼Œä¸æ–­å¢é•¿çš„å†…å­˜éœ€æ±‚å’Œè¯­ä¹‰ç»“æ„çš„éœ€æ±‚æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªä¸»è®°å¿†å¢å¼ºæ–¹æ³•MemInsightï¼Œä»¥å¢å¼ºè¯­ä¹‰æ•°æ®è¡¨ç¤ºå’Œæ£€ç´¢æœºåˆ¶ã€‚é€šè¿‡åˆ©ç”¨å¯¹å†å²äº’åŠ¨çš„è‡ªä¸»å¢å¼ºï¼ŒLLMä»£ç†èƒ½å¤Ÿæä¾›æ›´å‡†ç¡®å’Œæƒ…å¢ƒåŒ–çš„å›åº”ã€‚æˆ‘ä»¬é€šè¿‡ä¸‰é¡¹ä»»åŠ¡åœºæ™¯å®è¯éªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåˆ†åˆ«æ˜¯ä¼šè¯æ¨èã€é—®é¢˜å›ç­”å’Œäº‹ä»¶æ‘˜è¦ã€‚åœ¨LLM-REDIALæ•°æ®é›†ä¸Šï¼ŒMemInsightå°†æ¨èçš„è¯´æœåŠ›æé«˜äº†14%ã€‚æ­¤å¤–ï¼Œåœ¨LoCoMoæ£€ç´¢çš„å¬å›ç‡æ–¹é¢ï¼Œå®ƒè¶…è¶Šäº†RAGåŸºçº¿34%ã€‚æˆ‘ä»¬çš„å®è¯ç»“æœè¡¨æ˜ï¼ŒMemInsightåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå¢å¼ºLLMä»£ç†çš„ä¸Šä¸‹æ–‡æ€§èƒ½æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21760v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMæ™ºèƒ½ä»£ç†é€šè¿‡æ•´åˆé•¿æœŸè®°å¿†èƒ½åŠ›å¤„ç†ä¿¡æ¯ã€åšå‡ºå†³ç­–å’Œä¸ç”¨æˆ·æˆ–å·¥å…·äº’åŠ¨ã€‚ç„¶è€Œï¼Œéšç€è®°å¿†è§„æ¨¡çš„æ‰©å¤§å’Œè¯­ä¹‰ç»“æ„çš„éœ€æ±‚å¢åŠ ï¼Œå¸¦æ¥äº†æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§è‡ªä¸»è®°å¿†å¢å¼ºæ–¹æ³•MemInsightï¼Œæ”¹è¿›è¯­ä¹‰æ•°æ®è¡¨ç¤ºå’Œæ£€ç´¢æœºåˆ¶ã€‚é€šè¿‡åˆ©ç”¨è‡ªä¸»å¢å¼ºå¯¹å†å²äº’åŠ¨çš„åˆ©ç”¨ï¼ŒLLMä»£ç†èƒ½æä¾›æ›´å‡†ç¡®å’Œæƒ…å¢ƒåŒ–çš„å›åº”ã€‚åœ¨å¯¹è¯æ¨èã€é—®ç­”å’Œäº‹ä»¶æ‘˜è¦ä¸‰ä¸ªä»»åŠ¡åœºæ™¯ä¸­ï¼ŒMemInsightæ–¹æ³•å‡å¾—åˆ°å®è¯éªŒè¯ã€‚åœ¨LLM-REDIALæ•°æ®é›†ä¸Šï¼ŒMemInsightå°†æ¨èçš„è¯´æœåŠ›æé«˜äº†14%ã€‚åœ¨LoCoMoæ£€ç´¢çš„å¬å›ç‡æ–¹é¢ï¼Œå®ƒæ¯”RAGåŸºçº¿é«˜å‡º34%ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒMemInsightåœ¨å¤šä¸ªä»»åŠ¡ä¸Šå¢å¼ºäº†LLMä»£ç†çš„ä¸Šä¸‹æ–‡æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMæ™ºèƒ½ä»£ç†å…·å¤‡é•¿æœŸè®°å¿†èƒ½åŠ›ï¼Œèƒ½å¤„ç†ä¿¡æ¯ã€åšå‡ºå†³ç­–å’Œäº’åŠ¨ã€‚</li>
<li>è®°å¿†è§„æ¨¡çš„æ‰©å¤§å’Œè¯­ä¹‰ç»“æ„éœ€æ±‚å¢åŠ å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºè‡ªä¸»è®°å¿†å¢å¼ºæ–¹æ³•MemInsightï¼Œæ”¹è¿›è¯­ä¹‰æ•°æ®è¡¨ç¤ºå’Œæ£€ç´¢ã€‚</li>
<li>MemInsightåˆ©ç”¨è‡ªä¸»å¢å¼ºå¯¹å†å²äº’åŠ¨çš„åˆ©ç”¨ï¼Œæé«˜LLMä»£ç†å›åº”çš„å‡†ç¡®æ€§å’Œæƒ…å¢ƒåŒ–ã€‚</li>
<li>åœ¨å¯¹è¯æ¨èã€é—®ç­”å’Œäº‹ä»¶æ‘˜è¦ç­‰ä»»åŠ¡åœºæ™¯ä¸­ï¼ŒMemInsightæ–¹æ³•è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>åœ¨LLM-REDIALæ•°æ®é›†ä¸Šï¼ŒMemInsightæé«˜äº†æ¨èçš„è¯´æœåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e91a9573b05df4b023ed2b223399c11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db87d8cdbf51454a273a2820a52b56aa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-996772cfac705757995523e0bf0c7fd9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a65f462bf9aca403adadd2b38630cced.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GateLens-A-Reasoning-Enhanced-LLM-Agent-for-Automotive-Software-Release-Analytics"><a href="#GateLens-A-Reasoning-Enhanced-LLM-Agent-for-Automotive-Software-Release-Analytics" class="headerlink" title="GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release   Analytics"></a>GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release   Analytics</h2><p><strong>Authors:Arsham Gholamzadeh Khoee, Shuai Wang, Yinan Yu, Robert Feldt, Dhasarathy Parthasarathy</strong></p>
<p>Ensuring the reliability and effectiveness of software release decisions is critical, particularly in safety-critical domains like automotive systems. Precise analysis of release validation data, often presented in tabular form, plays a pivotal role in this process. However, traditional methods that rely on manual analysis of extensive test datasets and validation metrics are prone to delays and high costs. Large Language Models (LLMs) offer a promising alternative but face challenges in analytical reasoning, contextual understanding, handling out-of-scope queries, and processing structured test data consistently; limitations that hinder their direct application in safety-critical scenarios. This paper introduces GateLens, an LLM-based tool for analyzing tabular data in the automotive domain. GateLens translates natural language queries into Relational Algebra (RA) expressions and then generates optimized Python code. It outperforms the baseline system on benchmarking datasets, achieving higher F1 scores and handling complex and ambiguous queries with greater robustness. Ablation studies confirm the critical role of the RA module, with performance dropping sharply when omitted. Industrial evaluations reveal that GateLens reduces analysis time by over 80% while maintaining high accuracy and reliability. As demonstrated by presented results, GateLens achieved high performance without relying on few-shot examples, showcasing strong generalization across various query types from diverse company roles. Insights from deploying GateLens with a partner automotive company offer practical guidance for integrating AI into critical workflows such as release validation. Results show that by automating test result analysis, GateLens enables faster, more informed, and dependable release decisions, and can thus advance software scalability and reliability in automotive systems. </p>
<blockquote>
<p>ç¡®ä¿è½¯ä»¶å‘å¸ƒå†³ç­–çš„å¯ä¿¡åº¦å’Œæœ‰æ•ˆæ€§è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æ±½è½¦ç³»ç»Ÿè¿™æ ·çš„å®‰å…¨å…³é”®é¢†åŸŸã€‚ä»¥è¡¨æ ¼å½¢å¼å‘ˆç°çš„è½¯ä»¶å‘å¸ƒéªŒè¯æ•°æ®çš„ç²¾ç¡®åˆ†æåœ¨æ­¤è¿‡ç¨‹ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ–¹æ³•ä¾èµ–äºå¯¹å¤§é‡æµ‹è¯•æ•°æ®é›†å’ŒéªŒè¯æŒ‡æ ‡çš„æ‰‹åŠ¨åˆ†æï¼Œå®¹æ˜“å‡ºç°å»¶è¿Ÿå’Œæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†åœ¨åˆ†ææ¨ç†ã€ä¸Šä¸‹æ–‡ç†è§£ã€å¤„ç†è¶…å‡ºèŒƒå›´æŸ¥è¯¢å’Œå¤„ç†ç»“æ„åŒ–æµ‹è¯•æ•°æ®ä¸€è‡´æ€§æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼›è¿™äº›å±€é™æ€§é˜»ç¢äº†å®ƒä»¬åœ¨å®‰å…¨å…³é”®åœºæ™¯ä¸­çš„ç›´æ¥åº”ç”¨ã€‚æœ¬æ–‡ä»‹ç»äº†GateLensï¼Œä¸€ä¸ªåŸºäºLLMçš„æ±½è½¦é¢†åŸŸè¡¨æ ¼æ•°æ®åˆ†æå·¥å…·ã€‚GateLenså°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç¿»è¯‘æˆå…³ç³»ä»£æ•°ï¼ˆRAï¼‰è¡¨è¾¾å¼ï¼Œç„¶åç”Ÿæˆä¼˜åŒ–çš„Pythonä»£ç ã€‚å®ƒåœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºåŸºå‡†ç³»ç»Ÿï¼Œå®ç°äº†æ›´é«˜çš„F1åˆ†æ•°ï¼Œå¹¶æ›´ç¨³å¥åœ°å¤„ç†å¤æ‚å’Œæ¨¡ç³ŠæŸ¥è¯¢ã€‚æ¶ˆèç ”ç©¶è¯å®äº†RAæ¨¡å—çš„å…³é”®ä½œç”¨ï¼Œåœ¨çœç•¥è¯¥æ¨¡å—åæ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚å·¥ä¸šè¯„ä¼°è¡¨æ˜ï¼ŒGateLensåœ¨ä¿æŒé«˜å‡†ç¡®æ€§å’Œå¯é æ€§çš„åŒæ—¶ï¼Œå°†åˆ†ææ—¶é—´å‡å°‘äº†80%ä»¥ä¸Šã€‚ç»“æœè¡¨æ˜ï¼ŒGateLensåœ¨ä¸ä¾èµ–å°‘é‡ç¤ºä¾‹çš„æƒ…å†µä¸‹å®ç°äº†é«˜æ€§èƒ½ï¼Œå±•ç¤ºäº†åœ¨å„ç§æŸ¥è¯¢ç±»å‹ä¸­ä»å„ç§å…¬å¸è§’è‰²ä¸­å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡ä¸åˆä½œä¼™ä¼´æ±½è½¦å…¬å¸éƒ¨ç½²GateLensæ‰€è·å¾—çš„è§è§£ä¸ºå°†äººå·¥æ™ºèƒ½é›†æˆåˆ°å…³é”®å·¥ä½œæµç¨‹ï¼ˆå¦‚å‘å¸ƒéªŒè¯ï¼‰ä¸­æä¾›äº†å®é™…æŒ‡å¯¼ã€‚ç»“æœè¡¨æ˜ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æµ‹è¯•ç»“æœåˆ†æï¼ŒGateLensèƒ½å¤Ÿåšå‡ºæ›´å¿«ã€æ›´æ˜æ™ºã€æ›´å¯é çš„å‘å¸ƒå†³ç­–ï¼Œä»è€Œæé«˜æ±½è½¦ç³»ç»Ÿè½¯ä»¶çš„å¯æ‰©å±•æ€§å’Œå¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21735v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è½¯ä»¶å‘å¸ƒå†³ç­–åœ¨å®‰å…¨æ€§å…³é”®é¢†åŸŸå¦‚æ±½è½¦ç³»ç»Ÿä¸­çš„ä½œç”¨è‡³å…³é‡è¦ã€‚ç²¾ç¡®åˆ†æä»¥è¡¨æ ¼å½¢å¼å‘ˆç°çš„å‘ç‰ˆéªŒè¯æ•°æ®åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå¯¹å¤§é‡æµ‹è¯•æ•°æ®é›†å’ŒéªŒè¯æŒ‡æ ‡çš„æ‰‹åŠ¨åˆ†æï¼Œå®¹æ˜“å‡ºç°å»¶è¯¯å’Œæˆæœ¬é«˜æ˜‚ã€‚æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å·¥å…·â€”â€”GateLensï¼Œç”¨äºæ±½è½¦é¢†åŸŸçš„è¡¨æ ¼æ•°æ®åˆ†æã€‚GateLenså°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç¿»è¯‘æˆå…³ç³»ä»£æ•°ï¼ˆRAï¼‰è¡¨è¾¾å¼ï¼Œå¹¶ç”Ÿæˆä¼˜åŒ–çš„Pythonä»£ç ã€‚ä¸åŸºå‡†ç³»ç»Ÿç›¸æ¯”ï¼Œå®ƒåœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°æ›´ä¼˜ï¼Œå®ç°æ›´é«˜çš„F1åˆ†æ•°å¹¶å¤„ç†å¤æ‚å’Œæ¨¡ç³ŠæŸ¥è¯¢çš„é²æ£’æ€§æ›´å¼ºã€‚å·¥ä¸šè¯„ä¼°æ˜¾ç¤ºï¼ŒGateLenså‡å°‘äº†è¶…è¿‡80%çš„åˆ†ææ—¶é—´ï¼ŒåŒæ—¶ä¿æŒäº†é«˜å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚é€šè¿‡å±•ç¤ºç»“æœï¼Œè¯æ˜GateLensåœ¨ä¸ä¾èµ–å°‘æ•°ç¤ºä¾‹çš„æƒ…å†µä¸‹å®ç°äº†é«˜æ€§èƒ½ï¼Œå±•ç¤ºäº†åœ¨ä¸åŒæŸ¥è¯¢ç±»å‹å’Œå„ç§å…¬å¸è§’è‰²ä¸­çš„å¼ºå¤§é€šç”¨æ€§ã€‚ä¸åˆä½œä¼™ä¼´æ±½è½¦å…¬å¸å…±åŒéƒ¨ç½²GateLensçš„ç»éªŒæä¾›äº†å°†äººå·¥æ™ºèƒ½é›†æˆåˆ°å…³é”®å·¥ä½œæµç¨‹ï¼ˆå¦‚å‘å¸ƒéªŒè¯ï¼‰çš„å®é™…æŒ‡å¯¼ã€‚ç»“æœè¡¨æ˜ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æµ‹è¯•ç»“æœåˆ†æï¼ŒGateLensæœ‰åŠ©äºæ›´å¿«ã€æ›´æ˜æ™ºã€æ›´å¯é çš„å‘å¸ƒå†³ç­–ï¼Œä»è€Œä¿ƒè¿›æ±½è½¦ç³»ç»Ÿä¸­è½¯ä»¶çš„å¯æ‰©å±•æ€§å’Œå¯é æ€§ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è½¯ä»¶å‘å¸ƒå†³ç­–åœ¨å®‰å…¨æ€§å…³é”®é¢†åŸŸçš„é‡è¦æ€§ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•åœ¨è½¯ä»¶å‘å¸ƒéªŒè¯æ•°æ®åˆ†æä¸­çš„å±€é™æ€§ã€‚</li>
<li>LLMåœ¨è½¯ä»¶æ•°æ®åˆ†æä¸­çš„åº”ç”¨æ½œåŠ›åŠå…¶æŒ‘æˆ˜ã€‚</li>
<li>GateLenså·¥å…·å¼•å…¥ï¼šåŸºäºLLMè¿›è¡Œæ±½è½¦é¢†åŸŸè¡¨æ ¼æ•°æ®åˆ†æçš„æ–¹æ³•ã€‚</li>
<li>GateLensé€šè¿‡RAæ¨¡å—å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬åŒ–ä¸ºPythonä»£ç çš„èƒ½åŠ›åŠå…¶æ€§èƒ½ä¼˜åŠ¿ã€‚</li>
<li>GateLensåœ¨å·¥ä¸šç¯å¢ƒä¸­çš„æ€§èƒ½è¯„ä¼°å’Œæ•ˆç›Šï¼šå‡å°‘äº†åˆ†ææ—¶é—´å¹¶æé«˜äº†å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-969864f4b13b8661d59a174baefc12a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cf232579565cb8bffe5338a1b5c0030.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a00367f5e0b3c08cb2b51990be17ff9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca2772be09acc17b6ffd015d49b82cbc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Effective-Skill-Unlearning-through-Intervention-and-Abstention"><a href="#Effective-Skill-Unlearning-through-Intervention-and-Abstention" class="headerlink" title="Effective Skill Unlearning through Intervention and Abstention"></a>Effective Skill Unlearning through Intervention and Abstention</h2><p><strong>Authors:Yongce Li, Chung-En Sun, Tsui-Wei Weng</strong></p>
<p>Large language Models (LLMs) have demonstrated remarkable skills across various domains. Understanding the mechanisms behind their abilities and implementing controls over them is becoming increasingly important for developing better models. In this paper, we focus on skill unlearning in LLMs, specifically unlearning a particular skill while retaining their overall capabilities. We introduce two lightweight, training-free machine skill unlearning techniques for LLMs. First, we observe that the pre-activation distribution of neurons in each Feed-Forward Layer (FFL) differs when the model demonstrates different skills. Additionally, we find that queries triggering the same skill cluster within the FFL key space and can be separated from other queries using a hypercube. Based on these observations, we propose two lightweight, training-free skill unlearning methods via \textit{intervention} and \textit{abstention} respectively: \texttt{Neuron Adjust} and \texttt{Key Space Detection}. We evaluate our methods on unlearning math-solving, Python-coding, and comprehension skills across seven different languages. The results demonstrate their strong unlearning capabilities for the designated skills. Specifically, \texttt{Key Space Detection} achieves over 80% relative performance drop on the forgetting skill and less than 10% relative performance drop on other skills and the modelâ€™s general knowledge (MMLU) for most unlearning tasks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Trustworthy-ML-Lab/effective_skill_unlearning">https://github.com/Trustworthy-ML-Lab/effective_skill_unlearning</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§é¢†åŸŸè¡¨ç°å‡ºäº†å“è¶Šçš„æŠ€èƒ½ã€‚äº†è§£å®ƒä»¬èƒ½åŠ›èƒŒåçš„æœºåˆ¶å¹¶å¯¹å®ƒä»¬è¿›è¡Œæ§åˆ¶ï¼Œå¯¹äºå¼€å‘æ›´å¥½çš„æ¨¡å‹æ¥è¯´å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å…³æ³¨LLMä¸­çš„æŠ€èƒ½é—å¿˜ï¼Œç‰¹åˆ«æ˜¯é—å¿˜ç‰¹å®šæŠ€èƒ½çš„åŒæ—¶ä¿æŒå…¶æ•´ä½“èƒ½åŠ›ã€‚æˆ‘ä»¬ä¸ºLLMå¼•å…¥ä¸¤ç§è½»é‡çº§çš„ã€æ— éœ€è®­ç»ƒçš„æŠ€èƒ½é—å¿˜æŠ€æœ¯ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æ¯ä¸ªå‰é¦ˆå±‚ï¼ˆFFLï¼‰çš„ç¥ç»å…ƒé¢„æ¿€æ´»åˆ†å¸ƒåœ¨ä¸åŒæŠ€èƒ½å±•ç¤ºæ—¶å­˜åœ¨å·®å¼‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°è§¦å‘ç›¸åŒæŠ€èƒ½çš„æŸ¥è¯¢ä¼šåœ¨FFLå¯†é’¥ç©ºé—´å†…èšç°‡ï¼Œå¯ä»¥ä½¿ç”¨è¶…ç«‹æ–¹ä½“å°†å…¶ä¸å…¶ä»–æŸ¥è¯¢åˆ†å¼€ã€‚åŸºäºè¿™äº›è§‚å¯Ÿï¼Œæˆ‘ä»¬åˆ†åˆ«é€šè¿‡â€œå¹²é¢„â€å’Œâ€œå¼ƒæƒâ€æå‡ºäº†ä¸¤ç§è½»é‡çº§çš„ã€æ— éœ€è®­ç»ƒçš„æŠ€èƒ½é—å¿˜æ–¹æ³•ï¼šç¥ç»å…ƒè°ƒæ•´å’Œå¯†é’¥ç©ºé—´æ£€æµ‹ã€‚æˆ‘ä»¬åœ¨ä¸ƒç§ä¸åŒè¯­è¨€ä¸Šå¯¹æ•°å­¦æ±‚è§£ã€Pythonç¼–ç å’Œç†è§£æŠ€èƒ½çš„é—å¿˜è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜å®ƒä»¬åœ¨æŒ‡å®šæŠ€èƒ½ä¸Šçš„é—å¿˜èƒ½åŠ›å¾ˆå¼ºã€‚å…·ä½“æ¥è¯´ï¼Œå¯†é’¥ç©ºé—´æ£€æµ‹åœ¨é—å¿˜æŠ€èƒ½ä¸Šå®ç°äº†è¶…è¿‡80%çš„ç›¸å¯¹æ€§èƒ½ä¸‹é™ï¼Œè€Œåœ¨å…¶ä»–æŠ€èƒ½å’Œæ¨¡å‹çš„ä¸€èˆ¬çŸ¥è¯†ï¼ˆMMLUï¼‰ä¸Šï¼Œå¯¹äºå¤§å¤šæ•°é—å¿˜ä»»åŠ¡ï¼Œç›¸å¯¹æ€§èƒ½ä¸‹é™ä¸åˆ°10%ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/Trustworthy-ML-Lab/effective_skill_unlearning%E3%80%82">https://github.com/Trustworthy-ML-Lab/effective_skill_unlearningã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21730v1">PDF</a> Accepted to NAACL 2025 main conference</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æŠ€èƒ½å¿˜å´ç ”ç©¶å·²ç»å¼•èµ·å…³æ³¨ã€‚æœ¬æ–‡æå‡ºä¸¤ç§è½»é‡çº§çš„ã€æ— éœ€è®­ç»ƒçš„è¯­è¨€æ¨¡å‹æŠ€èƒ½å¿˜å´æŠ€æœ¯ï¼Œåˆ†åˆ«ä¸ºâ€œç¥ç»å…ƒè°ƒæ•´â€å’Œâ€œå…³é”®ç©ºé—´æ£€æµ‹â€ã€‚è¿™ä¸¤ç§æ–¹æ³•åŸºäºç¥ç»å…ƒçš„é¢„æ¿€æ´»åˆ†å¸ƒå’Œå…³é”®ç©ºé—´å†…çš„æŸ¥è¯¢èšç±»è§‚å¯Ÿï¼Œå¯ä»¥åœ¨ä¸è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°ç‰¹å®šæŠ€èƒ½çš„å¿˜å´ï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹çš„æ•´ä½“èƒ½åŠ›ã€‚åœ¨å¤šç§è¯­è¨€å’ŒæŠ€èƒ½ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•åœ¨å¿˜å´æŒ‡å®šæŠ€èƒ½æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„æŠ€èƒ½å¿˜å´å¯¹äºå¼€å‘æ›´å¥½çš„æ¨¡å‹è‡³å…³é‡è¦ã€‚</li>
<li>æœ¬æ–‡ä»‹ç»ä¸¤ç§è½»é‡çº§çš„ã€æ— éœ€è®­ç»ƒçš„æŠ€èƒ½å¿˜å´æŠ€æœ¯ï¼šç¥ç»å…ƒè°ƒæ•´å’Œå…³é”®ç©ºé—´æ£€æµ‹ã€‚</li>
<li>ç¥ç»å…ƒé¢„æ¿€æ´»åˆ†å¸ƒå’Œå…³é”®ç©ºé—´å†…çš„æŸ¥è¯¢èšç±»æ˜¯æŠ€èƒ½å¿˜å´çš„å…³é”®ã€‚</li>
<li>ç¥ç»å…ƒè°ƒæ•´é€šè¿‡å¹²é¢„æ–¹å¼å®ç°æŠ€èƒ½å¿˜å´ã€‚</li>
<li>å…³é”®ç©ºé—´æ£€æµ‹é€šè¿‡è¯†åˆ«å¹¶éš”ç¦»ç‰¹å®šæŠ€èƒ½ç›¸å…³æŸ¥è¯¢æ¥å®ç°æŠ€èƒ½å¿˜å´ã€‚</li>
<li>åœ¨å¤šç§è¯­è¨€å’ŒæŠ€èƒ½ä¸Šçš„å®éªŒéªŒè¯äº†è¿™ä¸¤ç§æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21730">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3b9e5fdecfd4fa108ec125db048b93f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b569851e05c955b646a3abef4c7cac15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00d156a16ce327f94563e214119bf1cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-696b25e763728f4acd0609cd8e283bd2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d019ea2765fc7cd516ec92a1db9216b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b659ce0c212dac50b1cfed3e7e9bdb31.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b2eb8ed28afa6c731d0c2ec42a03686.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Collab-Controlled-Decoding-using-Mixture-of-Agents-for-LLM-Alignment"><a href="#Collab-Controlled-Decoding-using-Mixture-of-Agents-for-LLM-Alignment" class="headerlink" title="Collab: Controlled Decoding using Mixture of Agents for LLM Alignment"></a>Collab: Controlled Decoding using Mixture of Agents for LLM Alignment</h2><p><strong>Authors:Souradip Chakraborty, Sujay Bhatt, Udari Madhushani Sehwag, Soumya Suvra Ghosal, Jiahao Qiu, Mengdi Wang, Dinesh Manocha, Furong Huang, Alec Koppel, Sumitra Ganesh</strong></p>
<p>Alignment of Large Language models (LLMs) is crucial for safe and trustworthy deployment in applications. Reinforcement learning from human feedback (RLHF) has emerged as an effective technique to align LLMs to human preferences and broader utilities, but it requires updating billions of model parameters, which is computationally expensive. Controlled Decoding, by contrast, provides a mechanism for aligning a model at inference time without retraining. However, single-agent decoding approaches often struggle to adapt to diverse tasks due to the complexity and variability inherent in these tasks. To strengthen the test-time performance w.r.t the target task, we propose a mixture of agent-based decoding strategies leveraging the existing off-the-shelf aligned LLM policies. Treating each prior policy as an agent in the spirit of mixture of agent collaboration, we develop a decoding method that allows for inference-time alignment through a token-level selection strategy among multiple agents. For each token, the most suitable LLM is dynamically chosen from a pool of models based on a long-term utility metric. This policy-switching mechanism ensures optimal model selection at each step, enabling efficient collaboration and alignment among LLMs during decoding. Theoretical analysis of our proposed algorithm establishes optimal performance with respect to the target task represented via a target reward for the given off-the-shelf models. We conduct comprehensive empirical evaluations with open-source aligned models on diverse tasks and preferences, which demonstrates the merits of this approach over single-agent decoding baselines. Notably, Collab surpasses the current SoTA decoding strategy, achieving an improvement of up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹é½å¯¹äºå…¶åœ¨åº”ç”¨ä¸­çš„å®‰å…¨å’Œå¯ä¿¡éƒ¨ç½²è‡³å…³é‡è¦ã€‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä½œä¸ºä¸€ç§æœ‰æ•ˆçš„æŠ€æœ¯ï¼Œèƒ½å¤Ÿå°†LLMä¸äººç±»åå¥½å’Œæ›´å¹¿æ³›çš„å®ç”¨æ€§å¯¹é½ï¼Œä½†æ›´æ–°æ•°åäº¿æ¨¡å‹å‚æ•°çš„éœ€æ±‚å¸¦æ¥äº†å·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå—æ§è§£ç æä¾›äº†ä¸€ç§åœ¨æ¨ç†æ—¶å¯¹é½æ¨¡å‹è€Œæ— éœ€é‡æ–°è®­ç»ƒæœºåˆ¶ã€‚ç„¶è€Œï¼Œç”±äºä»»åŠ¡çš„å¤æ‚æ€§å’Œå›ºæœ‰å˜åŒ–æ€§ï¼Œå•ä¸€æ™ºèƒ½ä½“è§£ç æ–¹æ³•å¾€å¾€éš¾ä»¥é€‚åº”å¤šæ ·åŒ–çš„ä»»åŠ¡ã€‚ä¸ºäº†åŠ å¼ºé’ˆå¯¹ç›®æ ‡ä»»åŠ¡çš„æµ‹è¯•æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ™ºèƒ½ä½“çš„è§£ç ç­–ç•¥æ··åˆæ–¹æ³•ï¼Œåˆ©ç”¨ç°æœ‰çš„ç°æˆçš„å¯¹é½LLMç­–ç•¥ã€‚å°†æ¯ä¸ªå…ˆå‰ç­–ç•¥è§†ä¸ºæ™ºèƒ½ä½“ï¼Œåœ¨æ™ºèƒ½ä½“åä½œæ··åˆçš„ç²¾ç¥ä¸‹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§è§£ç æ–¹æ³•ï¼Œå®ƒå…è®¸é€šè¿‡æ ‡è®°çº§åˆ«çš„é€‰æ‹©ç­–ç•¥åœ¨ä¸åŒçš„æ™ºèƒ½ä½“ä¹‹é—´è¿›è¡Œæ¨ç†æ—¶å¯¹é½ã€‚å¯¹äºæ¯ä¸ªæ ‡è®°ï¼Œä¼šä»æ¨¡å‹ä¸­åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„LLMï¼Œé€‰æ‹©ä¾æ®æ˜¯é•¿æœŸæ•ˆç”¨æŒ‡æ ‡ã€‚è¿™ç§ç­–ç•¥åˆ‡æ¢æœºåˆ¶ç¡®ä¿äº†æ¯ä¸€æ­¥çš„æœ€ä¼˜æ¨¡å‹é€‰æ‹©ï¼Œä½¿å¾—LLMåœ¨è§£ç è¿‡ç¨‹ä¸­èƒ½å¤Ÿè¿›è¡Œæœ‰æ•ˆçš„åä½œå’Œå¯¹é½ã€‚æˆ‘ä»¬æå‡ºç®—æ³•çš„ç†è®ºåˆ†æè¯æ˜äº†å¯¹äºç»™å®šç°æˆçš„æ¨¡å‹ï¼Œé€šè¿‡ç›®æ ‡å¥–åŠ±è¡¨ç¤ºçš„ç›®æ ‡ä»»åŠ¡çš„æ€§èƒ½æœ€ä¼˜ã€‚æˆ‘ä»¬åœ¨å¤šæ ·åŒ–çš„ä»»åŠ¡å’Œåå¥½ä¸Šå¯¹å¼€æºå¯¹é½æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„å®è¯è¯„ä¼°ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•ç›¸å¯¹äºå•ä¸€æ™ºèƒ½ä½“è§£ç åŸºçº¿æ–¹æ³•çš„ä¼˜åŠ¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒCollabè¶…è¶Šäº†å½“å‰çš„æœ€æ–°è§£ç ç­–ç•¥ï¼Œå¹³å‡å¥–åŠ±æé«˜äº†1.56å€ï¼ŒGPT-4çš„èƒœç‡æé«˜äº†71.89%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21720v1">PDF</a> Accepted to ICLR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹é½å¯¹äºå…¶åœ¨åº”ç”¨ä¸­çš„å®‰å…¨å’Œå¯ä¿¡éƒ¨ç½²è‡³å…³é‡è¦ã€‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„å¯¹é½æŠ€æœ¯ï¼Œä½†éœ€è¦æ›´æ–°æ•°åäº¿çš„æ¨¡å‹å‚æ•°ï¼Œè®¡ç®—æˆæœ¬é«˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå—æ§è§£ç æä¾›äº†ä¸€ç§åœ¨æ¨ç†é˜¶æ®µå¯¹é½æ¨¡å‹è€Œæ— éœ€é‡æ–°è®­ç»ƒçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œå•ä»£ç†è§£ç æ–¹æ³•å¾€å¾€éš¾ä»¥é€‚åº”å¤šæ ·åŒ–çš„ä»»åŠ¡ã€‚ä¸ºäº†å¼ºåŒ–å…¶åœ¨ç›®æ ‡ä»»åŠ¡çš„æµ‹è¯•æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä»£ç†çš„è§£ç ç­–ç•¥æ··åˆæ–¹æ³•ï¼Œåˆ©ç”¨ç°æœ‰çš„å¯¹é½LLMç­–ç•¥ã€‚æˆ‘ä»¬å°†æ¯ä¸ªå…ˆå‰ç­–ç•¥è§†ä¸ºä¸€ç§ä»£ç†ï¼Œå¼€å‘äº†ä¸€ç§è§£ç æ–¹æ³•ï¼Œå…è®¸åœ¨å¤šä¸ªä»£ç†ä¹‹é—´è¿›è¡Œä»¤ç‰Œçº§åˆ«çš„é€‰æ‹©ç­–ç•¥ï¼Œä»è€Œåœ¨æ¨ç†é˜¶æ®µè¿›è¡Œå¯¹é½ã€‚å¯¹äºæ¯ä¸ªä»¤ç‰Œï¼Œä»æ¨¡å‹æ± ä¸­åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„LLMï¼ŒåŸºäºé•¿æœŸæ•ˆç”¨æŒ‡æ ‡ã€‚è¿™ç§ç­–ç•¥åˆ‡æ¢æœºåˆ¶ç¡®ä¿äº†æ¯æ­¥çš„æœ€ä¼˜æ¨¡å‹é€‰æ‹©ï¼Œä½¿å¾—LLMåœ¨è§£ç è¿‡ç¨‹ä¸­çš„åä½œå’Œå¯¹é½æ›´åŠ é«˜æ•ˆã€‚æˆ‘ä»¬é€šè¿‡å¯¹ç°æœ‰æ¨¡å‹è¿›è¡Œç†è®ºåˆ†æå’Œåœ¨å¤šæ ·åŒ–ä»»åŠ¡å’Œåå¥½ä¸Šçš„å®è¯è¯„ä¼°ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•ä¼˜äºå•ä»£ç†è§£ç åŸºçº¿ã€‚ç‰¹åˆ«åœ°ï¼Œâ€œCollabâ€ç­–ç•¥è¶…è¶Šäº†å½“å‰æœ€ä½³è§£ç ç­–ç•¥ï¼Œå¹³å‡å¥–åŠ±æé«˜äº†1.56å€ï¼ŒGPT-4çš„èƒœç‡æé«˜äº†71.89%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMçš„å¯¹é½å¯¹äºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å®‰å…¨å’Œå¯é éƒ¨ç½²è‡³å…³é‡è¦ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„LLMå¯¹é½æŠ€æœ¯ï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>å—æ§è§£ç æ˜¯ä¸€ç§åœ¨æ¨ç†é˜¶æ®µå¯¹é½LLMè€Œæ— éœ€é‡æ–°è®­ç»ƒçš„æ–¹æ³•ã€‚</li>
<li>å•ä»£ç†è§£ç æ–¹æ³•åœ¨é¢å¯¹å¤šæ ·åŒ–ä»»åŠ¡æ—¶å¯èƒ½éš¾ä»¥é€‚åº”ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºä»£ç†çš„è§£ç ç­–ç•¥æ··åˆæ–¹æ³•ï¼Œåˆ©ç”¨ç°æœ‰çš„å¯¹é½LLMç­–ç•¥ã€‚</li>
<li>å¯¹æ¯ä¸ªä»¤ç‰Œï¼Œè¯¥æ–¹æ³•ä»æ¨¡å‹æ± ä¸­åŠ¨æ€é€‰æ‹©æœ€åˆé€‚çš„LLMã€‚</li>
<li>â€œCollabâ€ç­–ç•¥åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å½“å‰æœ€ä½³è§£ç ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21720">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f2bcafac38c5f4feadfcb30c5879d763.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3d7423ae4bc4a972d2e101e40529a5c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Enhancing-Repository-Level-Software-Repair-via-Repository-Aware-Knowledge-Graphs"><a href="#Enhancing-Repository-Level-Software-Repair-via-Repository-Aware-Knowledge-Graphs" class="headerlink" title="Enhancing Repository-Level Software Repair via Repository-Aware   Knowledge Graphs"></a>Enhancing Repository-Level Software Repair via Repository-Aware   Knowledge Graphs</h2><p><strong>Authors:Boyang Yang, Haoye Tian, Jiadong Ren, Shunfu Jin, Yang Liu, Feng Liu, Bach Le</strong></p>
<p>Repository-level software repair faces challenges in bridging semantic gaps between issue descriptions and code patches. Existing approaches, which mostly depend on large language models (LLMs), suffer from semantic ambiguities, limited structural context understanding, and insufficient reasoning capability. To address these limitations, we propose KGCompass with two innovations: (1) a novel repository-aware knowledge graph (KG) that accurately links repository artifacts (issues and pull requests) and codebase entities (files, classes, and functions), allowing us to effectively narrow down the vast search space to only 20 most relevant functions with accurate candidate bug locations and contextual information, and (2) a path-guided repair mechanism that leverages KG-mined entity path, tracing through which allows us to augment LLMs with relevant contextual information to generate precise patches along with their explanations. Experimental results in the SWE-Bench-Lite demonstrate that KGCompass achieves state-of-the-art repair performance (45.67%) and function-level localization accuracy (51.33%) across open-source approaches, costing only $0.20 per repair. Our analysis reveals that among successfully localized bugs, 69.7% require multi-hop traversals through the knowledge graph, without which LLM-based approaches struggle to accurately locate bugs. The knowledge graph built in KGCompass is language agnostic and can be incrementally updated, making it a practical solution for real-world development environments. </p>
<blockquote>
<p>è½¯ä»¶ä»“åº“çº§ä¿®å¤é¢ä¸´ç€ç¼©å°é—®é¢˜æè¿°å’Œä»£ç è¡¥ä¸ä¹‹é—´è¯­ä¹‰å·®è·çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å¤§å¤šä¾èµ–äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå­˜åœ¨è¯­ä¹‰æ¨¡ç³Šã€ç»“æ„ä¸Šä¸‹æ–‡ç†è§£æœ‰é™å’Œæ¨ç†èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†KGCompassï¼Œå…¶ä¸­åŒ…å«ä¸¤é¡¹åˆ›æ–°ï¼šï¼ˆ1ï¼‰ä¸€ç§æ–°å‹ä»“åº“æ„ŸçŸ¥çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ï¼Œèƒ½å¤Ÿå‡†ç¡®é“¾æ¥ä»“åº“å·¥ä»¶ï¼ˆé—®é¢˜å’Œæ‹‰å–è¯·æ±‚ï¼‰å’Œä»£ç åº“å®ä½“ï¼ˆæ–‡ä»¶ã€ç±»å’Œå‡½æ•°ï¼‰ï¼Œä»è€Œæœ‰æ•ˆåœ°å°†åºå¤§çš„æœç´¢ç©ºé—´ç¼©å°åˆ°ä»…20ä¸ªæœ€ç›¸å…³çš„å‡½æ•°ï¼Œå¹¶å‡†ç¡®æä¾›å€™é€‰é”™è¯¯ä½ç½®å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼›ï¼ˆ2ï¼‰ä¸€ç§è·¯å¾„å¼•å¯¼ä¿®å¤æœºåˆ¶ï¼Œå®ƒåˆ©ç”¨çŸ¥è¯†å›¾è°±æŒ–æ˜çš„å®ä½“è·¯å¾„ï¼Œé€šè¿‡è·Ÿè¸ªç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å¢å¼ºLLMçš„èƒ½åŠ›ï¼Œç”Ÿæˆç²¾ç¡®çš„è¡¥ä¸åŠå…¶è§£é‡Šã€‚åœ¨SWE-Bench-Liteçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒKGCompassåœ¨å¼€æºæ–¹æ³•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„ä¿®å¤æ€§èƒ½ï¼ˆ45.67%ï¼‰å’ŒåŠŸèƒ½çº§åˆ«çš„å®šä½ç²¾åº¦ï¼ˆ51.33%ï¼‰ï¼Œæ¯æ¬¡ä¿®å¤æˆæœ¬ä»…ä¸º0.2ç¾å…ƒã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œåœ¨æˆåŠŸå®šä½çš„æ¼æ´ä¸­ï¼Œæœ‰69.7%éœ€è¦é€šè¿‡çŸ¥è¯†å›¾è°±è¿›è¡Œå¤šè·³éå†ï¼Œæ²¡æœ‰è¿™äº›çŸ¥è¯†å›¾è°±çš„å¸®åŠ©ï¼ŒåŸºäºLLMçš„æ–¹æ³•å¾ˆéš¾å‡†ç¡®å®šä½æ¼æ´ã€‚KGCompassä¸­æ„å»ºçš„çŸ¥è¯†å›¾è°±ä¸è¯­è¨€æ— å…³ï¼Œå¯ä»¥å¢é‡æ›´æ–°ï¼Œä½¿å…¶æˆä¸ºé€‚ç”¨äºçœŸå®å¼€å‘ç¯å¢ƒçš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21710v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹è½¯ä»¶ä¿®å¤è¿‡ç¨‹ä¸­è¯­ä¹‰é¸¿æ²Ÿé—®é¢˜çš„KGCompassæ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆé€šè¿‡æ„å»ºçŸ¥è¯†å›¾è°±å’Œè·¯å¾„å¼•å¯¼ä¿®å¤æœºåˆ¶ï¼Œå®ç°äº†å¯¹ä»£ç åº“ä¸­é—®é¢˜æè¿°ä¸ä»£ç è¡¥ä¸ä¹‹é—´è¯­ä¹‰å·®å¼‚çš„å‡†ç¡®ç†è§£å’Œè¡”æ¥ã€‚çŸ¥è¯†å›¾è°±å¯ç¼©å°å¤§è§„æ¨¡æœç´¢ç©ºé—´å¹¶ç²¾å‡†å®šä½åˆ°ä»£ç åŠŸèƒ½å®ä½“ï¼Œä»è€Œæé«˜è½¯ä»¶ä¿®å¤çš„å‡†ç¡®ç‡å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ¡ˆåŸºäºçŸ¥è¯†å›¾è°±æŒ–æ˜çš„å®ä½“è·¯å¾„æŒ‡å¯¼ä¿®å¤æœºåˆ¶ï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ï¼Œç”Ÿæˆç²¾å‡†çš„ä»£ç è¡¥ä¸åŠå…¶è§£é‡Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒKGCompassåœ¨SWE-Bench-Liteæ•°æ®é›†ä¸Šå–å¾—äº†é¢†å…ˆçš„ä¿®å¤æ€§èƒ½å’Œå‡½æ•°å®šä½ç²¾åº¦ï¼Œä¸”æˆæœ¬è¾ƒä½ã€‚åˆ†ææ˜¾ç¤ºï¼ŒæˆåŠŸå®šä½çš„é—®é¢˜ä¸­æœ‰ç›¸å½“ä¸€éƒ¨åˆ†éœ€è¦è·¨å¤šä¸ªèŠ‚ç‚¹åœ¨çŸ¥è¯†å›¾è°±ä¸­è¿›è¡Œéå†ï¼Œè¿™å¯¹äºå•çº¯ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ¡ˆæ¥è¯´æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚KGCompassæ„å»ºçš„çŸ¥è¯†å›¾è°±å…·æœ‰è¯­è¨€æ— å…³æ€§å’Œå¯å¢é‡æ›´æ–°çš„ç‰¹ç‚¹ï¼Œé€‚ç”¨äºå®é™…å¼€å‘ç¯å¢ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>KGCompassè§£å†³äº†è½¯ä»¶ä¿®å¤ä¸­è¯­ä¹‰é¸¿æ²Ÿçš„é—®é¢˜ï¼Œé€šè¿‡æ„å»ºçŸ¥è¯†å›¾è°±å’Œè·¯å¾„å¼•å¯¼ä¿®å¤æœºåˆ¶æé«˜äº†è½¯ä»¶ä¿®å¤çš„å‡†ç¡®ç‡å’Œæ•ˆç‡ã€‚</li>
<li>çŸ¥è¯†å›¾è°±èƒ½å¤Ÿç¼©å°æœç´¢ç©ºé—´å¹¶ç²¾å‡†å®šä½åˆ°ä»£ç åŠŸèƒ½å®ä½“ï¼Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚</li>
<li>KGCompassåœ¨SWE-Bench-Liteæ•°æ®é›†ä¸Šå®ç°äº†é¢†å…ˆçš„ä¿®å¤æ€§èƒ½å’Œå‡½æ•°å®šä½ç²¾åº¦ï¼Œæˆæœ¬è¾ƒä½ã€‚</li>
<li>æˆåŠŸå®šä½çš„é—®é¢˜ä¸­æœ‰ç›¸å½“ä¸€éƒ¨åˆ†éœ€è¦è·¨å¤šä¸ªèŠ‚ç‚¹åœ¨çŸ¥è¯†å›¾è°±ä¸­è¿›è¡Œéå†ï¼Œå•çº¯ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ¡ˆéš¾ä»¥è§£å†³ã€‚</li>
<li>KGCompassæ„å»ºçš„çŸ¥è¯†å›¾è°±å…·æœ‰è¯­è¨€æ— å…³æ€§å’Œå¯å¢é‡æ›´æ–°çš„ç‰¹ç‚¹ã€‚</li>
<li>KGCompassæ–¹æ¡ˆåŒ…æ‹¬ä¸¤ä¸ªåˆ›æ–°ç‚¹ï¼šæ„å»ºä»“åº“æ„ŸçŸ¥çš„çŸ¥è¯†å›¾è°±å’Œåˆ©ç”¨çŸ¥è¯†å›¾è°±è¿›è¡Œè·¯å¾„å¼•å¯¼ä¿®å¤æœºåˆ¶ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c51b823dd89d9a1ebe2b19c73fe1c422.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1662298090a6d57a8b82312d91102132.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f0287d502ec78ba581ea4545e2f9c63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cecfaac79e2d7f39a7d2cca6b5774c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b33f82ca01f99762240632439463da13.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-138655542f66fd7bc91b3e418c9fb10e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6ec3056f61426f6fa16930f5586191d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="JiraiBench-A-Bilingual-Benchmark-for-Evaluating-Large-Language-Modelsâ€™-Detection-of-Human-Self-Destructive-Behavior-Content-in-Jirai-Community"><a href="#JiraiBench-A-Bilingual-Benchmark-for-Evaluating-Large-Language-Modelsâ€™-Detection-of-Human-Self-Destructive-Behavior-Content-in-Jirai-Community" class="headerlink" title="JiraiBench: A Bilingual Benchmark for Evaluating Large Language Modelsâ€™   Detection of Human Self-Destructive Behavior Content in Jirai Community"></a>JiraiBench: A Bilingual Benchmark for Evaluating Large Language Modelsâ€™   Detection of Human Self-Destructive Behavior Content in Jirai Community</h2><p><strong>Authors:Yunze Xiao, Tingyu He, Lionel Z. Wang, Yiming Ma, Xingyu Song, Xiaohang Xu, Irene Li, Ka Chung Ng</strong></p>
<p>This paper introduces JiraiBench, the first bilingual benchmark for evaluating large language modelsâ€™ effectiveness in detecting self-destructive content across Chinese and Japanese social media communities. Focusing on the transnational â€œJiraiâ€ (landmine) online subculture that encompasses multiple forms of self-destructive behaviors including drug overdose, eating disorders, and self-harm, we present a comprehensive evaluation framework incorporating both linguistic and cultural dimensions. Our dataset comprises 10,419 Chinese posts and 5,000 Japanese posts with multidimensional annotation along three behavioral categories, achieving substantial inter-annotator agreement. Experimental evaluations across four state-of-the-art models reveal significant performance variations based on instructional language, with Japanese prompts unexpectedly outperforming Chinese prompts when processing Chinese content. This emergent cross-cultural transfer suggests that cultural proximity can sometimes outweigh linguistic similarity in detection tasks. Cross-lingual transfer experiments with fine-tuned models further demonstrate the potential for knowledge transfer between these language systems without explicit target language training. These findings highlight the need for culturally-informed approaches to multilingual content moderation and provide empirical evidence for the importance of cultural context in developing more effective detection systems for vulnerable online communities. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†JiraiBenchï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸­æ–‡å’Œæ—¥æ–‡ç¤¾äº¤åª’ä½“ç¤¾åŒºä¸­æ£€æµ‹è‡ªæˆ‘æ¯ç­æ€§å†…å®¹æ•ˆæœçš„åŒè¯­åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬ä»¥è·¨å›½ç•Œçš„â€œJiraiâ€ï¼ˆåœ°é›·ï¼‰åœ¨çº¿äºšæ–‡åŒ–ä¸ºç„¦ç‚¹ï¼Œè¯¥æ–‡åŒ–åŒ…å«å¤šç§è‡ªæˆ‘æ¯ç­æ€§è¡Œä¸ºï¼ŒåŒ…æ‹¬è¯ç‰©è¿‡é‡ã€é¥®é£Ÿå¤±è°ƒå’Œè‡ªæˆ‘ä¼¤å®³ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†è¯­è¨€å’Œæ–‡åŒ–çš„åŒé‡ç»´åº¦ã€‚æˆ‘ä»¬çš„æ•°æ®é›†åŒ…å«10419ä¸ªä¸­æ–‡å¸–å­å’Œ5000ä¸ªæ—¥æ–‡å¸–å­ï¼Œè¿™äº›å¸–å­æŒ‰ç…§ä¸‰ç§è¡Œä¸ºç±»åˆ«è¿›è¡Œäº†å¤šç»´æ³¨é‡Šï¼Œå¹¶å®ç°äº†ç›¸å½“å¤§çš„æ ‡æ³¨é—´ä¸€è‡´æ€§ã€‚å¯¹å››ç§æœ€å…ˆè¿›æ¨¡å‹çš„å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œä¸åŒè¯­è¨€çš„æç¤ºæŒ‡å¯¼ä¸‹çš„æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œåœ¨å¤„ç†ä¸­æ–‡å†…å®¹æ—¶ï¼Œæ—¥è¯­æç¤ºå‡ºä¹æ„æ–™åœ°ä¼˜äºä¸­æ–‡æç¤ºã€‚è¿™ç§è·¨æ–‡åŒ–çš„è½¬ç§»è¡¨æ˜ï¼Œæ–‡åŒ–ç›¸ä¼¼æ€§æœ‰æ—¶ä¸å¦‚æ–‡åŒ–æ¥è¿‘åº¦åœ¨æ£€æµ‹ä»»åŠ¡ä¸­çš„é‡è¦æ€§å¤§ã€‚ç»è¿‡ç²¾ç»†è°ƒæ•´çš„æ¨¡å‹çš„è¯­è¨€è½¬ç§»å®éªŒè¿›ä¸€æ­¥è¯æ˜äº†åœ¨è¿™äº›è¯­è¨€ç³»ç»Ÿä¹‹é—´è¿›è¡ŒçŸ¥è¯†è½¬ç§»è€Œä¸è¿›è¡Œæ˜ç¡®çš„ç›®æ ‡è¯­è¨€è®­ç»ƒçš„æ½œåŠ›ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†è·¨æ–‡åŒ–å†…å®¹å®¡æŸ¥çš„éœ€è¦ï¼Œå¹¶ä¸ºå¼€å‘é’ˆå¯¹è„†å¼±åœ¨çº¿ç¤¾åŒºçš„æ›´æœ‰æ•ˆçš„æ£€æµ‹ç³»ç»Ÿæä¾›äº†é‡è¦çš„æ–‡åŒ–èƒŒæ™¯çš„å®è¯è¯æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21679v1">PDF</a> 20 pages, 1 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†JiraiBenchï¼Œé¦–ä¸ªé’ˆå¯¹ä¸­æ–‡å’Œæ—¥è¯­ç¤¾äº¤åª’ä½“ç¤¾åŒºä¸­è‡ªæˆ‘ç ´åæ€§å†…å®¹æ£€æµ‹æ•ˆæœè¯„ä¼°çš„åŒè¯­åŸºå‡†æµ‹è¯•ã€‚æ–‡ç« èšç„¦äºè·¨å›½â€œJiraiâ€ï¼ˆåœ°é›·ï¼‰åœ¨çº¿äºšæ–‡åŒ–ï¼Œæ¶µç›–å¤šç§è‡ªæˆ‘ç ´åæ€§è¡Œä¸ºï¼Œå¦‚è¯ç‰©è¿‡é‡ã€é¥®é£Ÿå¤±è°ƒå’Œè‡ªæˆ‘ä¼¤å®³ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†è¯­è¨€å’Œæ–‡åŒ–çš„ç»´åº¦ã€‚æ•°æ®é›†åŒ…å«10,419ç¯‡ä¸­æ–‡å¸–å­å’Œ5,000ç¯‡æ—¥è¯­å¸–å­ï¼ŒæŒ‰ä¸‰ä¸ªè¡Œä¸ºç±»åˆ«è¿›è¡Œå¤šç»´åº¦æ³¨é‡Šï¼Œå®ç°äº†æ˜¾è‘—çš„æ ‡æ³¨é—´ä¸€è‡´æ€§ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒåŸºäºæŒ‡ä»¤è¯­è¨€çš„æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œåœ¨å¤„ç†ä¸­æ–‡å†…å®¹æ—¶ï¼Œæ—¥è¯­æç¤ºå‡ºäººæ„æ–™åœ°ä¼˜äºä¸­æ–‡æç¤ºã€‚è¿™ç§è·¨æ–‡åŒ–çš„è½¬ç§»è¡¨æ˜ï¼Œåœ¨æŸäº›æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œæ–‡åŒ–æ¥è¿‘åº¦æœ‰æ—¶ä¼šè¶…è¿‡è¯­è¨€ç›¸ä¼¼æ€§ã€‚å¯¹ç²¾ç»†è°ƒæ•´è¿‡çš„æ¨¡å‹è¿›è¡Œè·¨è¯­è¨€è½¬ç§»å®éªŒï¼Œè¿›ä¸€æ­¥è¯æ˜äº†åœ¨ä¸åŒè¯­è¨€ç³»ç»Ÿä¹‹é—´è¿›è¡ŒçŸ¥è¯†è½¬ç§»è€Œä¸è¿›è¡Œæ˜ç¡®çš„ç›®æ ‡è¯­è¨€è®­ç»ƒçš„æ½œåŠ›ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†è·¨æ–‡åŒ–æ–¹æ³•åœ¨å¤šå…ƒè¯­è¨€å†…å®¹ç®¡ç†ä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºå¼€å‘é’ˆå¯¹è„†å¼±åœ¨çº¿ç¤¾åŒºçš„æ›´æœ‰æ•ˆæ£€æµ‹ç³»ç»Ÿæä¾›äº†å®è¯è¯æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« ä»‹ç»äº†JiraiBenchï¼Œå®ƒæ˜¯é¦–ä¸ªé’ˆå¯¹è‡ªæˆ‘ç ´åæ€§å†…å®¹æ£€æµ‹çš„åŒè¯­åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æµ‹è¯•æ¶µç›–ä¸­æ–‡å’Œæ—¥è¯­ç¤¾äº¤åª’ä½“ä¸Šçš„â€œJiraiâ€åœ¨çº¿äºšæ–‡åŒ–å†…å®¹ï¼ŒåŒ…æ‹¬å¤šç§è‡ªæˆ‘ç ´åæ€§è¡Œä¸ºçš„è¯„ä¼°ã€‚</li>
<li>è¯„ä¼°æ¡†æ¶ç»“åˆäº†è¯­è¨€å’Œæ–‡åŒ–çš„ç»´åº¦ã€‚</li>
<li>æ•°æ®é›†åŒ…å«å¤§é‡ä¸­æ–‡å’Œæ—¥è¯­å¸–å­ï¼Œæ ‡æ³¨ä¸€è‡´æ€§å¼ºã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œä¸åŒè¯­è¨€çš„æŒ‡ä»¤ä¼šå½±å“æ¨¡å‹æ€§èƒ½ï¼Œæœ‰æ—¶æ—¥è¯­æç¤ºåœ¨ä¸­æ–‡å†…å®¹æ£€æµ‹ä¸­è¡¨ç°æ›´å¥½ã€‚</li>
<li>è·¨è¯­è¨€å®éªŒè¡¨æ˜ä¸åŒè¯­è¨€ç³»ç»Ÿé—´çš„çŸ¥è¯†è½¬ç§»æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1c6cf02a68b4b487d71cb0c92b4981ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a6f0b26deea0262ebbf18bf554ee1b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54e6e07730dec6ab60c1af107a14fb4f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Cooking-Task-Planning-using-LLM-and-Verified-by-Graph-Network"><a href="#Cooking-Task-Planning-using-LLM-and-Verified-by-Graph-Network" class="headerlink" title="Cooking Task Planning using LLM and Verified by Graph Network"></a>Cooking Task Planning using LLM and Verified by Graph Network</h2><p><strong>Authors:Ryunosuke Takebayashi, Vitor Hideyo Isume, Takuya Kiyokawa, Weiwei Wan, Kensuke Harada</strong></p>
<p>Cooking tasks remain a challenging problem for robotics due to their complexity. Videos of people cooking are a valuable source of information for such task, but introduces a lot of variability in terms of how to translate this data to a robotic environment. This research aims to streamline this process, focusing on the task plan generation step, by using a Large Language Model (LLM)-based Task and Motion Planning (TAMP) framework to autonomously generate cooking task plans from videos with subtitles, and execute them. Conventional LLM-based task planning methods are not well-suited for interpreting the cooking video data due to uncertainty in the videos, and the risk of hallucination in its output. To address both of these problems, we explore using LLMs in combination with Functional Object-Oriented Networks (FOON), to validate the plan and provide feedback in case of failure. This combination can generate task sequences with manipulation motions that are logically correct and executable by a robot. We compare the execution of the generated plans for 5 cooking recipes from our approach against the plans generated by a few-shot LLM-only approach for a dual-arm robot setup. It could successfully execute 4 of the plans generated by our approach, whereas only 1 of the plans generated by solely using the LLM could be executed. </p>
<blockquote>
<p>çƒ¹é¥ªä»»åŠ¡ä»ç„¶æ˜¯æœºå™¨äººæŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå› ä¸ºè¿™äº›ä»»åŠ¡çš„å¤æ‚æ€§ã€‚äººä»¬çƒ¹é¥ªçš„è§†é¢‘æ˜¯è¿™ç±»ä»»åŠ¡çš„é‡è¦ä¿¡æ¯æ¥æºï¼Œä½†å¦‚ä½•å°†è¿™ç±»æ•°æ®è½¬åŒ–ä¸ºæœºå™¨äººç¯å¢ƒå­˜åœ¨å¾ˆå¤§çš„å˜æ•°ã€‚æœ¬ç ”ç©¶æ—¨åœ¨ç®€åŒ–è¿™ä¸€è¿‡ç¨‹ï¼Œé‡ç‚¹å…³æ³¨ä»»åŠ¡è®¡åˆ’ç”Ÿæˆæ­¥éª¤ï¼Œé€šè¿‡ä½¿ç”¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»»åŠ¡å’ŒåŠ¨ä½œè§„åˆ’ï¼ˆTAMPï¼‰æ¡†æ¶ï¼Œè‡ªä¸»åœ°ä»å¸¦å­—å¹•çš„è§†é¢‘ç”Ÿæˆçƒ¹é¥ªä»»åŠ¡è®¡åˆ’ï¼Œå¹¶æ‰§è¡Œå®ƒä»¬ã€‚ç”±äºè§†é¢‘ä¸­çš„ä¸ç¡®å®šæ€§ä»¥åŠè¾“å‡ºçš„è™šæ„é£é™©ï¼Œä¼ ç»Ÿçš„åŸºäºLLMçš„ä»»åŠ¡è§„åˆ’æ–¹æ³•å¹¶ä¸é€‚åˆè§£é‡Šçƒ¹é¥ªè§†é¢‘æ•°æ®ã€‚ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢å°†LLMä¸é¢å‘åŠŸèƒ½å¯¹è±¡çš„ç½‘ç»œï¼ˆFOONï¼‰ç›¸ç»“åˆï¼Œä»¥éªŒè¯è®¡åˆ’å¹¶åœ¨å¤±è´¥æ—¶æä¾›åé¦ˆã€‚è¿™ç§ç»“åˆå¯ä»¥ç”Ÿæˆé€»è¾‘æ­£ç¡®ä¸”æœºå™¨äººå¯æ‰§è¡Œçš„å¸¦æœ‰æ“ä½œåŠ¨ä½œçš„ä»»åŠ¡åºåˆ—ã€‚æˆ‘ä»¬å°†ä»æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„5ä¸ªçƒ¹é¥ªé£Ÿè°±çš„ä»»åŠ¡æ‰§è¡Œä¸ä»…ä½¿ç”¨LLMç”Ÿæˆçš„ä»»åŠ¡è®¡åˆ’è¿›è¡Œæ¯”è¾ƒï¼Œç”¨äºåŒæœºæ¢°è‡‚æœºå™¨äººè®¾ç½®ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„ä»»åŠ¡è®¡åˆ’ä¸­æœ‰å››ä¸ªå¯ä»¥æˆåŠŸæ‰§è¡Œï¼Œè€Œä»…ä½¿ç”¨LLMç”Ÿæˆçš„ä»»åŠ¡è®¡åˆ’ä¸­åªæœ‰ä¸€ä¸ªå¯ä»¥æ‰§è¡Œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21564v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œä»»åŠ¡ä¸åŠ¨ä½œè§„åˆ’ï¼ˆTAMPï¼‰æ¡†æ¶ï¼Œä»å¸¦æœ‰å­—å¹•çš„çƒ¹é¥ªè§†é¢‘è‡ªä¸»ç”Ÿæˆä»»åŠ¡è®¡åˆ’å¹¶æ‰§è¡Œçš„é—®é¢˜ã€‚é’ˆå¯¹çƒ¹é¥ªè§†é¢‘æ•°æ®çš„ä¸ç¡®å®šæ€§åŠè¾“å‡ºå¯èƒ½å‡ºç°çš„å¹»è§‰é£é™©ï¼Œç»“åˆåŠŸèƒ½é¢å‘å¯¹è±¡ç½‘ç»œï¼ˆFOONï¼‰è¿›è¡ŒéªŒè¯å’Œåé¦ˆã€‚è¯¥ç»„åˆèƒ½ç”Ÿæˆé€»è¾‘æ­£ç¡®ã€æœºå™¨äººå¯æ‰§è¡Œçš„å¸¦æ“ä½œåŠ¨ä½œçš„ä»»åŠ¡åºåˆ—ã€‚å¯¹æ¯”å®éªŒæ˜¾ç¤ºï¼Œå¯¹äºåŒæœºæ¢°è‡‚æœºå™¨äººè®¾ç½®ï¼Œä½¿ç”¨æ­¤æ–¹æ³•ç”Ÿæˆçš„è®¡åˆ’æˆåŠŸæ‰§è¡Œäº†å››é¡¹çƒ¹é¥ªé£Ÿè°±ï¼Œè€Œä»…ä½¿ç”¨LLMçš„è®¡åˆ’ä»…æˆåŠŸæ‰§è¡Œä¸€é¡¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çƒ¹é¥ªä»»åŠ¡ä»ç„¶æ˜¯æœºå™¨äººæŠ€æœ¯é¢ä¸´çš„æŒ‘æˆ˜ä¹‹ä¸€ï¼Œè€Œè§†é¢‘ä¸­çš„çƒ¹é¥ªè¿‡ç¨‹æ˜¯ä¸€ä¸ªæœ‰ä»·å€¼çš„ä¿¡æ¯æ¥æºã€‚</li>
<li>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œä»»åŠ¡è§„åˆ’å¯¹äºçƒ¹é¥ªè§†é¢‘æ•°æ®çš„è§£è¯»å­˜åœ¨ä¸ç¡®å®šæ€§åŠè¾“å‡ºå¹»è§‰çš„é£é™©ã€‚</li>
<li>æœ¬æ–‡ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’ŒåŸºäºåŠŸèƒ½é¢å‘å¯¹è±¡ç½‘ç»œï¼ˆFOONï¼‰çš„æ–¹æ³•æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç»„åˆå¤§å‹è¯­è¨€æ¨¡å‹å’ŒFOONç½‘ç»œï¼Œèƒ½å¤Ÿç”Ÿæˆé€»è¾‘æ­£ç¡®ä¸”æœºå™¨äººå¯æ‰§è¡Œçš„å¸¦æ“ä½œåŠ¨ä½œçš„ä»»åŠ¡åºåˆ—ã€‚</li>
<li>å¯¹æ¯”å®éªŒè¡¨æ˜ï¼Œå¯¹äºåŒæœºæ¢°è‡‚æœºå™¨äººè®¾ç½®ï¼Œä½¿ç”¨ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’ŒFOONçš„æ–¹æ³•ç”Ÿæˆçš„è®¡åˆ’æ‰§è¡ŒæˆåŠŸç‡æ›´é«˜ã€‚</li>
<li>æœ¬æ–‡æ—¨åœ¨é€šè¿‡è‡ªåŠ¨åŒ–ç”Ÿæˆä»»åŠ¡è®¡åˆ’æ¥ç®€åŒ–ä»çƒ¹é¥ªè§†é¢‘åˆ°æœºå™¨äººç¯å¢ƒçš„ç¿»è¯‘è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21564">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dbd8e7f54efd5d78fec88964b5416740.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a88ba3c630dbc935673e5e1691e319ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcc537641b95ff330eb8f92ded375aea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60aa728776b08fde00b74dbda87a9318.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f0adeb10cca0ed42cc1387419dd9c626.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f15a74f434c119a266a238fd847060e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a55caa1e056d5e0b41c04e668ecf12a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb51d35d316c5904561cb3df16508de5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec5db3ad946472667a00ccad288d5ee4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SWI-Speaking-with-Intent-in-Large-Language-Models"><a href="#SWI-Speaking-with-Intent-in-Large-Language-Models" class="headerlink" title="SWI: Speaking with Intent in Large Language Models"></a>SWI: Speaking with Intent in Large Language Models</h2><p><strong>Authors:Yuwei Yin, EunJeong Hwang, Giuseppe Carenini</strong></p>
<p>Intent, typically clearly formulated and planned, functions as a cognitive framework for reasoning and problem-solving. This paper introduces the concept of Speaking with Intent (SWI) in large language models (LLMs), where the explicitly generated intent encapsulates the modelâ€™s underlying intention and provides high-level planning to guide subsequent analysis and communication. By emulating deliberate and purposeful thoughts in the human mind, SWI is hypothesized to enhance the reasoning capabilities and generation quality of LLMs. Extensive experiments on mathematical reasoning benchmarks consistently demonstrate the superiority of Speaking with Intent over Baseline (i.e., generation without explicit intent). Moreover, SWI outperforms answer-trigger prompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive performance with the strong method ARR (Analyzing, Retrieving, and Reasoning). Additionally, the effectiveness and generalizability of SWI are solidified on reasoning-intensive question answering (QA) and text summarization benchmarks, where SWI brings consistent improvement to the Baseline generation. In text summarization, SWI-generated summaries exhibit greater accuracy, conciseness, and factual correctness, with fewer hallucinations. Furthermore, human evaluations verify the coherence, effectiveness, and interpretability of the intent produced by SWI. This proof-of-concept study creates a novel avenue for enhancing LLMsâ€™ reasoning abilities with cognitive notions. </p>
<blockquote>
<p>æ„å›¾é€šå¸¸è¢«æ˜ç¡®åˆ¶å®šå’Œè®¡åˆ’ï¼Œä½œä¸ºæ¨ç†å’Œè§£å†³é—®é¢˜çš„è®¤çŸ¥æ¡†æ¶ã€‚æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„â€œæœ‰æ„è¡¨è¾¾â€ï¼ˆSWIï¼‰çš„æ¦‚å¿µï¼Œå…¶ä¸­æ˜ç¡®ç”Ÿæˆçš„æ„å›¾æ¶µç›–äº†æ¨¡å‹çš„åŸºæœ¬æ„å›¾ï¼Œå¹¶ä¸ºéšåçš„åˆ†æå’Œé€šä¿¡æä¾›äº†é«˜çº§è§„åˆ’ã€‚é€šè¿‡æ¨¡æ‹Ÿäººç±»å¿ƒæ™ºä¸­çš„æœ‰æ„è¯†å’Œæœ‰ç›®çš„çš„æ€è€ƒï¼Œå‡è®¾SWIå¯ä»¥æé«˜LLMçš„æ¨ç†èƒ½åŠ›å’Œç”Ÿæˆè´¨é‡ã€‚åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒä¸­ï¼Œä¸åŸºçº¿ï¼ˆå³æ— æ˜ç¡®æ„å›¾çš„ç”Ÿæˆï¼‰ç›¸æ¯”ï¼Œâ€œæœ‰æ„è¡¨è¾¾â€å§‹ç»ˆè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚è€Œä¸”ï¼Œåœ¨æ¨ç†å¯†é›†çš„é—®é¢˜å›ç­”å’Œæ–‡æœ¬æ‘˜è¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œä¸â€œæ€ç»´é“¾â€å’Œâ€œè®¡åˆ’å¹¶è§£å†³â€çš„è§¦å‘æç¤ºæ–¹æ³•ç›¸æ¯”ï¼Œå°½ç®¡åœ¨æŸäº›æ–¹é¢æ€§èƒ½ä¸å¼ºå¤§çš„ARRï¼ˆåˆ†æã€æ£€ç´¢å’Œæ¨ç†ï¼‰æ–¹æ³•ç›¸å½“ï¼Œä½†SWIçš„æœ‰æ•ˆæ€§ä¹Ÿæ›´ä¸ºçªå‡ºã€‚æ­¤å¤–ï¼Œå®ƒåœ¨åŸºçº¿ç”Ÿæˆçš„åŸºç¡€ä¸Šå¸¦æ¥äº†æŒç»­çš„æ”¹è¿›ã€‚åœ¨æ–‡æœ¬æ‘˜è¦ä¸­ï¼Œç”±SWIç”Ÿæˆçš„æ‘˜è¦å±•ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ã€ç®€æ´æ€§å’Œäº‹å®æ­£ç¡®æ€§ï¼Œå‡å°‘äº†å¹»è§‰ç°è±¡çš„å‡ºç°ã€‚æ­¤å¤–ï¼Œäººç±»è¯„ä¼°éªŒè¯äº†ç”±SWIäº§ç”Ÿçš„æ„å›¾çš„è¿è´¯æ€§ã€æœ‰æ•ˆæ€§å’Œå¯è§£é‡Šæ€§ã€‚è¿™é¡¹æ¦‚å¿µéªŒè¯ç ”ç©¶ä¸ºåˆ©ç”¨è®¤çŸ¥æ¦‚å¿µæé«˜LLMçš„æ¨ç†èƒ½åŠ›å¼€è¾Ÿäº†ä¸€æ¡æ–°çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21544v1">PDF</a> 24 pages. Code: <a target="_blank" rel="noopener" href="https://github.com/YuweiYin/SWI">https://github.com/YuweiYin/SWI</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¼•å…¥â€œå¸¦æ„å›¾çš„è¯´è¯â€ï¼ˆSpeaking with Intentï¼Œç®€ç§°SWIï¼‰çš„æ¦‚å¿µã€‚é€šè¿‡æ˜ç¡®ç”Ÿæˆçš„æ„å›¾ï¼Œæ¨¡æ‹Ÿäººç±»çš„æ·±æ€ç†Ÿè™‘å’Œç›®çš„æ€§æ€è€ƒï¼Œæé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œç”Ÿæˆè´¨é‡ã€‚å®éªŒè¯æ˜ï¼Œç›¸è¾ƒäºåŸºçº¿ç”Ÿæˆæ–¹æ³•ï¼ˆæ— æ˜ç¡®æ„å›¾çš„ç”Ÿæˆæ–¹å¼ï¼‰ï¼Œå¸¦æ„å›¾çš„è¯´è¯åœ¨æ•°å­¦æ¨ç†å’Œæ–‡æœ¬æ€»ç»“ç­‰æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚åŒæ—¶ï¼Œäººç±»è¯„ä¼°ä¹ŸéªŒè¯äº†å…¶ç”Ÿæˆçš„æ„å›¾çš„è¿è´¯æ€§ã€æœ‰æ•ˆæ€§å’Œå¯è§£é‡Šæ€§ã€‚æœ¬æ–‡å¼€åˆ›äº†ä¸€æ¡åˆ©ç”¨è®¤çŸ¥æ¦‚å¿µå¢å¼ºLLMæ¨ç†èƒ½åŠ›çš„æ–°é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼•å…¥â€œå¸¦æ„å›¾çš„è¯´è¯â€ï¼ˆSpeaking with Intentï¼Œç®€ç§°SWIï¼‰æ¦‚å¿µï¼Œä½œä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„è®¤çŸ¥æ¡†æ¶ï¼Œç”¨äºæé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œç”Ÿæˆè´¨é‡ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿäººç±»çš„æ·±æ€ç†Ÿè™‘å’Œç›®çš„æ€§æ€è€ƒï¼Œæ˜ç¡®ç”Ÿæˆçš„æ„å›¾æœ‰åŠ©äºå¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œç›¸è¾ƒäºåŸºçº¿ç”Ÿæˆæ–¹æ³•ï¼Œå¸¦æ„å›¾çš„è¯´è¯åœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
<li>å¸¦æ„å›¾çš„è¯´è¯åœ¨æ–‡æœ¬æ€»ç»“æ–¹é¢è¡¨ç°ä¼˜ç§€ï¼Œèƒ½æé«˜å‡†ç¡®æ€§ã€ç®€æ´æ€§å’Œäº‹å®æ­£ç¡®æ€§ï¼Œå‡å°‘è™šæ„å†…å®¹ã€‚</li>
<li>äººç±»è¯„ä¼°éªŒè¯äº†å¸¦æ„å›¾çš„è¯´è¯ç”Ÿæˆçš„è¿è´¯æ€§ã€æœ‰æ•ˆæ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>å¸¦æ„å›¾çš„è¯´è¯åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼ŒåŒ…æ‹¬ä¸å¼ºå¤§çš„ARRæ–¹æ³•ç›¸æ¯”ä¿æŒç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21544">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd046f266ae6527b977db91ed0cfb4c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-769b75edb145147f853fdb2f62926e64.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5546f28d60e625754b80d779154d0a55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48527aea34ee0a0a94664d3a7756103a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b04c4eac737aceab5a93ae91593bd93.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a58477b77438b1bf1b95c3e342f3d6da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea85a8f58bd643fee45ccbdfd6095a92.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SlowFast-LLaVA-1-5-A-Family-of-Token-Efficient-Video-Large-Language-Models-for-Long-Form-Video-Understanding"><a href="#SlowFast-LLaVA-1-5-A-Family-of-Token-Efficient-Video-Large-Language-Models-for-Long-Form-Video-Understanding" class="headerlink" title="SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language   Models for Long-Form Video Understanding"></a>SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language   Models for Long-Form Video Understanding</h2><p><strong>Authors:Mingze Xu, Mingfei Gao, Shiyu Li, Jiasen Lu, Zhe Gan, Zhengfeng Lai, Meng Cao, Kai Kang, Yinfei Yang, Afshin Dehghan</strong></p>
<p>We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family of video large language models (LLMs) offering a token-efficient solution for long-form video understanding. We incorporate the two-stream SlowFast mechanism into a streamlined training pipeline, and perform joint video-image training on a carefully curated data mixture of only publicly available datasets. Our primary focus is on highly efficient model scales (1B and 3B), demonstrating that even relatively small Video LLMs can achieve state-of-the-art performance on video understanding, meeting the demand for mobile-friendly models. Experimental results demonstrate that SF-LLaVA-1.5 achieves superior performance on a wide range of video and image tasks, with robust results at all model sizes (ranging from 1B to 7B). Notably, SF-LLaVA-1.5 achieves state-of-the-art results in long-form video understanding (e.g., LongVideoBench and MLVU) and excels at small scales across various video benchmarks. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†SlowFast-LLaVA-1.5ï¼ˆç®€ç§°SF-LLaVA-1.5ï¼‰ï¼Œè¿™æ˜¯ä¸€æ¬¾è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»åˆ—ï¼Œä¸ºé•¿è§†é¢‘ç†è§£æä¾›äº†ä¸€ç§æ ‡è®°æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å°†åŒæµSlowFastæœºåˆ¶çº³å…¥ç®€åŒ–çš„è®­ç»ƒç®¡é“ï¼Œå¹¶åœ¨ä»…å…¬å¼€æ•°æ®é›†ç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®æ··åˆä¸Šè¿›è¡Œè”åˆè§†é¢‘å›¾åƒè®­ç»ƒã€‚æˆ‘ä»¬çš„ä¸»è¦ç„¦ç‚¹æ˜¯é«˜æ•ˆæ¨¡å‹è§„æ¨¡ï¼ˆ1Bå’Œ3Bï¼‰ï¼Œè¯æ˜å³ä½¿ç›¸å¯¹è¾ƒå°çš„è§†é¢‘LLMä¹Ÿå¯ä»¥åœ¨è§†é¢‘ç†è§£æ–¹é¢å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ»¡è¶³å¯¹ç§»åŠ¨å‹å¥½å‹æ¨¡å‹çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSF-LLaVA-1.5åœ¨å¹¿æ³›çš„è§†é¢‘å’Œå›¾åƒä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œåœ¨å„ç§æ¨¡å‹è§„æ¨¡ï¼ˆä»1Båˆ°7Bï¼‰ä¸Šå‡è¡¨ç°å‡ºç¨³å¥çš„ç»“æœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSF-LLaVA-1.5åœ¨é•¿è§†é¢‘ç†è§£ï¼ˆä¾‹å¦‚LongVideoBenchå’ŒMLVUï¼‰æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå¹¶åœ¨å„ç§è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨å°è§„æ¨¡æµ‹è¯•ä¸­è¡¨ç°å°¤ä¸ºçªå‡ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18943v2">PDF</a> Technical report</p>
<p><strong>Summary</strong></p>
<p>SF-LLaVA-1.5ç³»åˆ—è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é‡‡ç”¨é«˜æ•ˆçš„tokenè§£å†³æ–¹æ¡ˆï¼Œç”¨äºé•¿æ ¼å¼è§†é¢‘ç†è§£ã€‚è¯¥æ¨¡å‹ç»“åˆäº†ä¸¤æµSlowFastæœºåˆ¶ï¼Œå¹¶åœ¨ç²¾å¿ƒæŒ‘é€‰çš„å…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œè”åˆè§†é¢‘å›¾åƒè®­ç»ƒã€‚ç ”ç©¶é‡ç‚¹æ˜¯å°è§„æ¨¡æ¨¡å‹ï¼ˆè§„æ¨¡ä¸º1Bå’Œ3Bï¼‰ï¼Œå³ä½¿ç›¸å¯¹è¾ƒå°çš„è§†é¢‘LLMä¹Ÿèƒ½åœ¨è§†é¢‘ç†è§£æ–¹é¢å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ»¡è¶³å¯¹ç§»åŠ¨å‹å¥½å‹æ¨¡å‹çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSF-LLaVA-1.5åœ¨å„ç§è§†é¢‘å’Œå›¾åƒä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œåœ¨å„ç§æ¨¡å‹è§„æ¨¡ä¸‹å‡è¡¨ç°ç¨³å¥ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿æ ¼å¼è§†é¢‘ç†è§£æ–¹é¢è¾¾åˆ°ä¸šç•Œé¢†å…ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SF-LLaVA-1.5æ˜¯ä¸€ä¸ªç”¨äºé•¿æ ¼å¼è§†é¢‘ç†è§£çš„æ–°å‹è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹å®¶æ—ã€‚</li>
<li>ç»“åˆäº†ä¸¤æµSlowFastæœºåˆ¶ä»¥æé«˜æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨ç²¾å¿ƒæŒ‘é€‰çš„å…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¼ºè°ƒé«˜åº¦æ•ˆç‡æ¨¡å‹è§„æ¨¡ï¼ˆå°¤å…¶æ˜¯1Bå’Œ3Bï¼‰ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜SF-LLaVA-1.5åœ¨å„ç§è§†é¢‘å’Œå›¾åƒä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ€§èƒ½ç¨³å¥ã€‚</li>
<li>æ¨¡å‹åœ¨é•¿æ ¼å¼è§†é¢‘ç†è§£æ–¹é¢è¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚</li>
<li>SF-LLaVA-1.5é€‚ç”¨äºç§»åŠ¨åº”ç”¨ï¼Œæ»¡è¶³ç§»åŠ¨å‹å¥½å‹æ¨¡å‹çš„éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18943">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3f437c7b274db96bd67abb60034ac2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85da2a7be139f5e8ab48b701a4b7ccad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac8628e0afa40c6539ed32e41c52e319.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5050ab2a7b818501e85769024de54621.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Adaptive-Rank-Allocation-Speeding-Up-Modern-Transformers-with-RaNA-Adapters"><a href="#Adaptive-Rank-Allocation-Speeding-Up-Modern-Transformers-with-RaNA-Adapters" class="headerlink" title="Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA   Adapters"></a>Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA   Adapters</h2><p><strong>Authors:Roberto Garcia, Jerry Liu, Daniel Sorvisto, Sabri Eyuboglu</strong></p>
<p>Large Language Models (LLMs) are computationally intensive, particularly during inference. Neuron-adaptive techniques, which selectively activate neurons in Multi-Layer Perceptron (MLP) layers, offer some speedups but suffer from limitations in modern Transformers. These include reliance on sparse activations, incompatibility with attention layers, and the use of costly neuron masking techniques. To address these issues, we propose the Adaptive Rank Allocation framework and introduce the Rank and Neuron Allocator (RaNA) adapter. RaNA adapters leverage rank adapters, which operate on linear layers by applying both low-rank matrix decompositions and adaptive masking to efficiently allocate compute without depending on activation sparsity. This enables RaNA to be generally applied to MLPs and linear components of attention modules, while eliminating the need for expensive maskers found in neuron-adaptive methods. Notably, when compared to neuron adapters, RaNA improves perplexity by up to 7 points and increases accuracy by up to 8 percentage-points when reducing FLOPs by $\sim$44% in state-of-the-art Transformer architectures. These results position RaNA as a robust solution for improving inference efficiency in modern Transformer architectures. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®¡ç®—å¯†é›†ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†é˜¶æ®µã€‚ç¥ç»å…ƒè‡ªé€‚åº”æŠ€æœ¯é€šè¿‡é€‰æ‹©æ€§æ¿€æ´»å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰å±‚ä¸­çš„ç¥ç»å…ƒæä¾›äº†ä¸€äº›åŠ é€Ÿï¼Œä½†åœ¨ç°ä»£Transformerä¸­å—åˆ°ä¸€äº›é™åˆ¶ã€‚è¿™äº›é™åˆ¶åŒ…æ‹¬ä¾èµ–äºç¨€ç–æ¿€æ´»ã€ä¸æ³¨æ„åŠ›å±‚ä¸å…¼å®¹ä»¥åŠä½¿ç”¨æ˜‚è´µçš„ç¥ç»å…ƒæ©è”½æŠ€æœ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”æ’ååˆ†é…æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†Rankå’Œç¥ç»å…ƒåˆ†é…å™¨ï¼ˆRaNAï¼‰é€‚é…å™¨ã€‚RaNAé€‚é…å™¨åˆ©ç”¨æ’åé€‚é…å™¨ï¼Œè¯¥æ’åé€‚é…å™¨é€šè¿‡åº”ç”¨ä½é˜¶çŸ©é˜µåˆ†è§£å’Œè‡ªé€‚åº”æ©è”½æ¥æ“ä½œçº¿æ€§å±‚ï¼Œä»è€Œæœ‰æ•ˆåœ°åˆ†é…è®¡ç®—èµ„æºï¼Œè€Œæ— éœ€ä¾èµ–æ¿€æ´»ç¨€ç–æ€§ã€‚è¿™ä½¿å¾—RaNAèƒ½å¤Ÿå¹¿æ³›åº”ç”¨äºMLPå’Œæ³¨æ„åŠ›æ¨¡å—çš„çº¿æ€§ç»„ä»¶ï¼ŒåŒæ—¶æ¶ˆé™¤äº†ç¥ç»å…ƒè‡ªé€‚åº”æ–¹æ³•ä¸­å‘ç°çš„æ˜‚è´µæ©è”½å™¨çš„éœ€æ±‚ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸ç¥ç»å…ƒé€‚é…å™¨ç›¸æ¯”ï¼ŒRaNAåœ¨å‡å°‘çº¦44%çš„æµ®ç‚¹è¿ç®—é‡ï¼ˆFLOPsï¼‰çš„æƒ…å†µä¸‹ï¼Œå›°æƒ‘åº¦é™ä½äº†æœ€å¤š7ä¸ªç‚¹ï¼Œå‡†ç¡®ç‡æé«˜äº†æœ€å¤š8ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™äº›ç»“æœä½¿RaNAæˆä¸ºæé«˜ç°ä»£Transformeræ¶æ„æ¨ç†æ•ˆç‡çš„ä¸€ç§ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18216v1">PDF</a> 16 pages, 5 figures. ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºLLMçš„å¤§å‹è¯­è¨€æ¨¡å‹è®¡ç®—å¯†é›†ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†é˜¶æ®µã€‚ç¥ç»é€‚åº”æŠ€æœ¯é€šè¿‡åœ¨å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰å±‚ä¸­é€‰æ‹©æ€§æ¿€æ´»ç¥ç»å…ƒæä¾›äº†ä¸€äº›åŠ é€Ÿï¼Œä½†åœ¨ç°ä»£Transformerä¸­ä»å­˜åœ¨ä¾èµ–ç¨€ç–æ¿€æ´»ã€ä¸å…¼å®¹æ³¨æ„åŠ›å±‚ä»¥åŠä½¿ç”¨æ˜‚è´µçš„ç¥ç»å…ƒæ©ç æŠ€æœ¯ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”æ’ååˆ†é…æ¡†æ¶ï¼Œå¹¶å¼•å…¥äº†åä¸ºRaNAçš„é€‚é…å™¨ã€‚RaNAé€‚é…å™¨åˆ©ç”¨æ’åé€‚é…å™¨ï¼Œé€šè¿‡åœ¨çº¿æ€§å±‚ä¸Šåº”ç”¨ä½ç§©çŸ©é˜µåˆ†è§£å’Œè‡ªé€‚åº”æ©ç è¿›è¡Œæœ‰æ•ˆè®¡ç®—åˆ†é…ï¼Œä¸ä¾èµ–äºæ¿€æ´»ç¨€ç–æ€§ã€‚è¿™ä½¿å¾—RaNAå¯å¹¿æ³›åº”ç”¨äºMLPå’Œæ³¨æ„åŠ›æ¨¡å—çš„çº¿æ€§ç»„ä»¶ï¼ŒåŒæ—¶æ¶ˆé™¤äº†ç¥ç»å…ƒè‡ªé€‚åº”æ–¹æ³•ä¸­å‘ç°çš„æ˜‚è´µæ©è”½å™¨éœ€æ±‚ã€‚åœ¨å‡å°‘æµ®ç‚¹è¿ç®—é‡çº¦44%çš„æƒ…å†µä¸‹ï¼ŒRaNAä¸ç¥ç»å…ƒé€‚é…å™¨ç›¸æ¯”ï¼Œåœ¨æœ€å…ˆè¿›çš„Transformeræ¶æ„ä¸­é™ä½äº†å›°æƒ‘åº¦è¾¾7ç‚¹ï¼Œå¹¶æé«˜äº†å‡†ç¡®æ€§è¾¾8ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™äº›ç»“æœç¡®ç«‹äº†RaNAåœ¨æé«˜ç°ä»£Transformeræ¶æ„æ¨ç†æ•ˆç‡æ–¹é¢çš„ç¨³å¥è§£å†³æ–¹æ¡ˆåœ°ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†é˜¶æ®µè®¡ç®—å¯†é›†ã€‚</li>
<li>ç¥ç»é€‚åº”æŠ€æœ¯åœ¨å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰å±‚ä¸­é€‰æ‹©æ€§æ¿€æ´»ç¥ç»å…ƒä»¥æä¾›åŠ é€Ÿï¼Œä½†åœ¨ç°ä»£Transformerä¸­æœ‰å±€é™æ€§ã€‚</li>
<li>RaNAæ¡†æ¶é€šè¿‡ç»“åˆä½ç§©çŸ©é˜µåˆ†è§£å’Œè‡ªé€‚åº”æ©ç ï¼Œåœ¨çº¿æ€§å±‚ä¸Šè¿›è¡Œè®¡ç®—åˆ†é…ã€‚</li>
<li>RaNAé€‚ç”¨äºMLPå’Œæ³¨æ„åŠ›æ¨¡å—çš„çº¿æ€§ç»„ä»¶ï¼Œæ— éœ€æ˜‚è´µçš„æ©è”½å™¨ã€‚</li>
<li>RaNAåœ¨å‡å°‘è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œæé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå›°æƒ‘åº¦ã€‚</li>
<li>ä¸ç¥ç»å…ƒé€‚é…å™¨ç›¸æ¯”ï¼ŒRaNAåœ¨å‡å°‘æµ®ç‚¹è¿ç®—é‡çº¦44%çš„æƒ…å†µä¸‹è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18216">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eab15cf704a2b13ab67e2dab8a3fccfd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b1a974f521b0b5bcaacd7f6a07f4a34.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Detection-of-Somali-written-Fake-News-and-Toxic-Messages-on-the-Social-Media-Using-Transformer-based-Language-Models"><a href="#Detection-of-Somali-written-Fake-News-and-Toxic-Messages-on-the-Social-Media-Using-Transformer-based-Language-Models" class="headerlink" title="Detection of Somali-written Fake News and Toxic Messages on the Social   Media Using Transformer-based Language Models"></a>Detection of Somali-written Fake News and Toxic Messages on the Social   Media Using Transformer-based Language Models</h2><p><strong>Authors:Muhidin A. Mohamed, Shuab D. Ahmed, Yahye A. Isse, Hanad M. Mohamed, Fuad M. Hassan, Houssein A. Assowe</strong></p>
<p>The fact that everyone with a social media account can create and share content, and the increasing public reliance on social media platforms as a news and information source bring about significant challenges such as misinformation, fake news, harmful content, etc. Although human content moderation may be useful to an extent and used by these platforms to flag posted materials, the use of AI models provides a more sustainable, scalable, and effective way to mitigate these harmful contents. However, low-resourced languages such as the Somali language face limitations in AI automation, including scarce annotated training datasets and lack of language models tailored to their unique linguistic characteristics. This paper presents part of our ongoing research work to bridge some of these gaps for the Somali language. In particular, we created two human-annotated social-media-sourced Somali datasets for two downstream applications, fake news &amp; toxicity classification, and developed a transformer-based monolingual Somali language model (named SomBERTa) â€“ the first of its kind to the best of our knowledge. SomBERTa is then fine-tuned and evaluated on toxic content, fake news and news topic classification datasets. Comparative evaluation analysis of the proposed model against related multilingual models (e.g., AfriBERTa, AfroXLMR, etc) demonstrated that SomBERTa consistently outperformed these comparators in both fake news and toxic content classification tasks while achieving the best average accuracy (87.99%) across all tasks. This research contributes to Somali NLP by offering a foundational language model and a replicable framework for other low-resource languages, promoting digital and AI inclusivity and linguistic diversity. </p>
<blockquote>
<p>æ‹¥æœ‰ç¤¾äº¤åª’ä½“è´¦æˆ·çš„ç”¨æˆ·éƒ½å¯ä»¥åˆ›å»ºå’Œåˆ†äº«å†…å®¹ï¼Œä¸”å…¬ä¼—å¯¹ç¤¾äº¤åª’ä½“å¹³å°çš„æ–°é—»å’Œä¿¡æ¯æ¥æºçš„ä¾èµ–ç¨‹åº¦ä¸æ–­å¢åŠ ï¼Œè¿™å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œä¾‹å¦‚å‡æ¶ˆæ¯ã€é”™è¯¯ä¿¡æ¯ã€æœ‰å®³å†…å®¹ç­‰ã€‚è™½ç„¶åœ¨ä¸€å®šç¨‹åº¦ä¸Šäººç±»å†…å®¹å®¡æ ¸å¯¹è¿™äº›å¹³å°æ ‡è®°å‘å¸ƒææ–™æ˜¯æœ‰ç”¨çš„ï¼Œä½†äººå·¥æ™ºèƒ½æ¨¡å‹çš„ä½¿ç”¨æä¾›äº†ä¸€ç§æ›´å¯æŒç»­ã€å¯æ‰©å±•å’Œæœ‰æ•ˆçš„ç¼“è§£è¿™äº›æœ‰å®³å†…å®¹çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œå¯¹äºç´¢é©¬é‡Œè¯­ç­‰ä½èµ„æºè¯­è¨€æ¥è¯´ï¼Œäººå·¥æ™ºèƒ½è‡ªåŠ¨åŒ–é¢ä¸´ç€æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è®­ç»ƒæ•°æ®é›†æ ‡æ³¨ç¨€ç¼ºä»¥åŠç¼ºä¹é’ˆå¯¹å…¶ç‹¬ç‰¹è¯­è¨€ç‰¹æ€§é‡èº«å®šåˆ¶çš„è¯­è¨€æ¨¡å‹ã€‚è¿™ç¯‡è®ºæ–‡å±•ç¤ºäº†æˆ‘ä»¬æ­£åœ¨è¿›è¡Œçš„ç ”ç©¶å·¥ä½œçš„ä¸€éƒ¨åˆ†ï¼Œæ—¨åœ¨ç¼©å°è¿™äº›å·®è·å¹¶ä¸ºç´¢é©¬é‡Œè¯­æä¾›æ”¯æŒã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªåŸºäºç¤¾äº¤åª’ä½“æ¥æºçš„ç´¢é©¬é‡Œæ•°æ®é›†ï¼Œç”¨äºä¸¤ä¸ªä¸‹æ¸¸åº”ç”¨â€”â€”å‡æ–°é—»å’Œæœ‰æ¯’å†…å®¹åˆ†ç±»ã€‚å¹¶ä¸”æˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºå˜æ¢å™¨çš„å•è¯­ç§ç´¢é©¬é‡Œè¯­è¨€æ¨¡å‹ï¼ˆåä¸ºSomBERTaï¼‰â€”â€”æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯è¯¥é¢†åŸŸé¦–åˆ›ã€‚éšåå¯¹SomBERTaè¿›è¡Œå¾®è°ƒå¹¶åœ¨æœ‰æ¯’å†…å®¹ã€å‡æ–°é—»å’Œæ–°é—»ä¸»é¢˜åˆ†ç±»æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚å¯¹æ‰€æå‡ºçš„æ¨¡å‹ä¸ç›¸å…³å¤šè¯­ç§æ¨¡å‹ï¼ˆå¦‚AfriBERTaã€AfroXLMRç­‰ï¼‰çš„æ¯”è¾ƒè¯„ä¼°åˆ†æè¡¨æ˜ï¼Œåœ¨å‡æ–°é—»å’Œæœ‰æ¯’å†…å®¹åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒSomBERTaå‡ä¼˜äºè¿™äº›æ¯”è¾ƒæ¨¡å‹ï¼Œå¹¶åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å–å¾—äº†æœ€ä½³å¹³å‡å‡†ç¡®ç‡ï¼ˆ87.99%ï¼‰ã€‚è¿™é¡¹ç ”ç©¶ä¸ºç´¢é©¬é‡ŒNLPé¢†åŸŸæä¾›äº†åŸºç¡€è¯­è¨€æ¨¡å‹å’Œå¯å¤åˆ¶æ¡†æ¶ï¼Œå¯¹äºå…¶ä»–ä½èµ„æºè¯­è¨€ä¹Ÿå…·æœ‰å€Ÿé‰´æ„ä¹‰ï¼Œæ¨åŠ¨äº†æ•°å­—äººå·¥æ™ºèƒ½åŒ…å®¹æ€§å’Œè¯­è¨€å¤šæ ·æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18117v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç¤¾äº¤åª’ä½“è´¦å·çš„æ™®åŠä»¥åŠå…¬ä¼—å¯¹ç¤¾äº¤åª’ä½“å¹³å°ä½œä¸ºæ–°é—»å’Œä¿¡æ¯æ¥æºçš„ä¾èµ–ï¼Œå¸¦æ¥äº†è¯¸å¦‚é”™è¯¯ä¿¡æ¯ã€è™šå‡æ–°é—»ã€æœ‰å®³å†…å®¹ç­‰æŒ‘æˆ˜ã€‚äººå·¥æ™ºèƒ½æ¨¡å‹çš„ä½¿ç”¨ä¸ºè§£å†³è¿™äº›é—®é¢˜æä¾›äº†æ›´å¯æŒç»­ã€å¯æ‰©å±•å’Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œä½†ä½èµ„æºè¯­è¨€å¦‚ç´¢é©¬é‡Œè¯­åœ¨äººå·¥æ™ºèƒ½è‡ªåŠ¨åŒ–æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è®­ç»ƒæ•°æ®é›†ç¨€å°‘å’Œç¼ºä¹é’ˆå¯¹å…¶ç‹¬ç‰¹è¯­è¨€ç‰¹æ€§çš„è¯­è¨€æ¨¡å‹ã€‚æœ¬æ–‡å±•ç¤ºäº†æˆ‘ä»¬æ­£åœ¨è¿›è¡Œçš„éƒ¨åˆ†ç ”ç©¶å·¥ä½œï¼Œæ—¨åœ¨ç¼©å°è¿™äº›å·®è·å¹¶ä¸ºç´¢é©¬é‡Œè¯­æ„å»ºè¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªç”±ç¤¾äº¤åª’ä½“æºæä¾›çš„äººç±»æ³¨é‡Šç´¢é©¬é‡Œæ•°æ®é›†ï¼Œç”¨äºä¸¤ä¸ªä¸‹æ¸¸åº”ç”¨â€”â€”è™šå‡æ–°é—»å’Œæ¯’æ€§åˆ†ç±»ï¼Œå¹¶å¼€å‘äº†ä¸€ç§åŸºäºè½¬æ¢å™¨çš„å•è¯­ç´¢é©¬é‡Œè¯­è¨€æ¨¡å‹ï¼ˆåä¸ºSomBERTaï¼‰ã€‚SomBERTaç»è¿‡å¾®è°ƒå¹¶åœ¨æœ‰æ¯’å†…å®¹ã€è™šå‡æ–°é—»å’Œæ–°é—»ä¸»é¢˜åˆ†ç±»æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚ä¸ç›¸å…³å¤šè¯­ç§æ¨¡å‹çš„æ¯”è¾ƒåˆ†æè¡¨æ˜ï¼ŒSomBERTaåœ¨è™šå‡æ–°é—»å’Œæœ‰æ¯’å†…å®¹åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°æ›´ä½³ï¼Œå¹¶åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­å–å¾—æœ€ä½³å¹³å‡å‡†ç¡®ç‡ï¼ˆ87.99%ï¼‰ã€‚è¯¥ç ”ç©¶ä¸ºç´¢é©¬é‡Œè‡ªç„¶è¯­è¨€å¤„ç†æä¾›äº†åŸºç¡€è¯­è¨€æ¨¡å‹å’Œå¯å¤åˆ¶çš„æ¡†æ¶ï¼Œå¯¹å…¶ä»–ä½èµ„æºè¯­è¨€å…·æœ‰å€Ÿé‰´æ„ä¹‰ï¼Œä¿ƒè¿›äº†æ•°å­—ä¸äººå·¥æ™ºèƒ½çš„åŒ…å®¹æ€§å’Œè¯­è¨€å¤šæ ·æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç¤¾äº¤åª’ä½“å†…å®¹çš„åˆ›å»ºå’Œåˆ†äº«å¸¦æ¥äº†é”™è¯¯ä¿¡æ¯ã€è™šå‡æ–°é—»ã€æœ‰å®³å†…å®¹ç­‰æŒ‘æˆ˜ã€‚</li>
<li>AIæ¨¡å‹åœ¨è§£å†³ç¤¾äº¤åª’ä½“å†…å®¹é—®é¢˜æ–¹é¢è¡¨ç°å‡ºæ›´å¯æŒç»­ã€å¯æ‰©å±•å’Œæœ‰æ•ˆçš„ä¼˜åŠ¿ã€‚</li>
<li>ä½èµ„æºè¯­è¨€å¦‚ç´¢é©¬é‡Œè¯­åœ¨AIè‡ªåŠ¨åŒ–æ–¹é¢é¢ä¸´æ•°æ®ç¨€å°‘å’Œè¯­è¨€æ¨¡å‹ç¼ºä¹çš„æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡ä¸ºç´¢é©¬é‡Œè¯­åˆ›å»ºäº†ä¸¤ä¸ªç¤¾äº¤åª’ä½“æ¥æºçš„æ³¨é‡Šæ•°æ®é›†ï¼Œç”¨äºè™šå‡æ–°é—»å’Œæ¯’æ€§åˆ†ç±»ã€‚</li>
<li>å¼€å‘äº†åŸºäºè½¬æ¢å™¨çš„å•è¯­ç´¢é©¬é‡Œè¯­è¨€æ¨¡å‹SomBERTaï¼Œè¿™æ˜¯é’ˆå¯¹ç´¢é©¬é‡Œè¯­çš„é¦–æ¬¡å°è¯•ã€‚</li>
<li>SomBERTaåœ¨è™šå‡æ–°é—»å’Œæœ‰æ¯’å†…å®¹åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡å‡†ç¡®ç‡è¾¾87.99%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18117">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-65a1b3a762a674f56c042c29db1b5948.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f7e29213ee430ed28b3afe31777c348.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa9627fbd5f1aab6ddb2b2b33d2bf017.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01c48da51fc364472b88cc3e59534a8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e41b44d9809a4c981ab63f0949919cf.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Temporal-Relation-Extraction-in-Clinical-Texts-A-Span-based-Graph-Transformer-Approach"><a href="#Temporal-Relation-Extraction-in-Clinical-Texts-A-Span-based-Graph-Transformer-Approach" class="headerlink" title="Temporal Relation Extraction in Clinical Texts: A Span-based Graph   Transformer Approach"></a>Temporal Relation Extraction in Clinical Texts: A Span-based Graph   Transformer Approach</h2><p><strong>Authors:Rochana Chaturvedi, Peyman Baghershahi, Sourav Medya, Barbara Di Eugenio</strong></p>
<p>Temporal information extraction from unstructured text is essential for contextualizing events and deriving actionable insights, particularly in the medical domain. We address the task of extracting clinical events and their temporal relations using the well-studied I2B2 2012 Temporal Relations Challenge corpus. This task is inherently challenging due to complex clinical language, long documents, and sparse annotations. We introduce GRAPHTREX, a novel method integrating span-based entity-relation extraction, clinical large pre-trained language models (LPLMs), and Heterogeneous Graph Transformers (HGT) to capture local and global dependencies. Our HGT component facilitates information propagation across the document through innovative global landmarks that bridge distant entities. Our method improves the state-of-the-art with 5.5% improvement in the tempeval $F_1$ score over the previous best and up to 8.9% improvement on long-range relations, which presents a formidable challenge. This work not only advances temporal information extraction but also lays the groundwork for improved diagnostic and prognostic models through enhanced temporal reasoning. </p>
<blockquote>
<p>ä»éç»“æ„åŒ–æ–‡æœ¬ä¸­æå–æ—¶é—´ä¿¡æ¯æ˜¯ä¸Šä¸‹æ–‡æƒ…å¢ƒåŒ–äº‹ä»¶å’Œè·å–å¯æ“ä½œæ´å¯Ÿçš„å…³é”®ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—é¢†åŸŸã€‚æˆ‘ä»¬åˆ©ç”¨ç»è¿‡æ·±å…¥ç ”ç©¶çš„I2B2 2012æ—¶é—´å…³ç³»æŒ‘æˆ˜è¯­æ–™åº“ï¼Œè§£å†³æå–ä¸´åºŠäº‹ä»¶åŠå…¶æ—¶é—´å…³ç³»çš„ä»»åŠ¡ã€‚ç”±äºå¤æ‚çš„ä¸´åºŠè¯­è¨€ã€é•¿æ–‡æ¡£å’Œç¨€ç–çš„æ³¨é‡Šï¼Œæ­¤ä»»åŠ¡æœ¬è´¨ä¸Šå…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†GRAPHTREXï¼Œè¿™æ˜¯ä¸€ç§é›†æˆåŸºäºèŒƒå›´å®ä½“å…³ç³»æå–ã€ä¸´åºŠå¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆLPLMsï¼‰å’Œå¼‚è´¨å›¾å˜æ¢å™¨ï¼ˆHGTï¼‰çš„æ–°æ–¹æ³•ï¼Œä»¥æ•è·å±€éƒ¨å’Œå…¨å±€ä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬çš„HGTç»„ä»¶é€šè¿‡åˆ›æ–°çš„å…¨å±€åœ°æ ‡ï¼Œä¿ƒè¿›è·¨è¶Šæ–‡æ¡£çš„ä¿¡æ¯ä¼ æ’­ï¼Œä»è€Œè¿æ¥é¥è¿œçš„å®ä½“ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç°æœ‰æŠ€æœ¯çš„åŸºç¡€ä¸Šæ”¹è¿›äº†5.5%ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é•¿ç¨‹å…³ç³»æ—¶ï¼Œæ”¹è¿›å¹…åº¦é«˜è¾¾8.9%ï¼Œè¿™æ˜¯ä¸€ä¸ªç›¸å½“å¤§çš„æŒ‘æˆ˜ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æ¨åŠ¨äº†æ—¶é—´ä¿¡æ¯çš„æå–å‘å±•ï¼Œè€Œä¸”é€šè¿‡å¢å¼ºæ—¶é—´æ¨ç†ä¸ºæ”¹è¿›è¯Šæ–­å’Œé¢„åæ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18085v1">PDF</a> Introducing a novel method for joint extraction of medical events and   temporal relations from free-text, leveraging clinical LPLMs and   Heterogeneous Graph Transformers, achieving a 5.5% improvement over the   previous state-of-the-art and up to 8.9% on long-range relations</p>
<p><strong>Summary</strong></p>
<p>ä¸´åºŠæ–‡æœ¬ä¸­çš„æ—¶åºä¿¡æ¯æå–å¯¹äºäº‹ä»¶ä¸Šä¸‹æ–‡åŒ–å’Œè·å–å¯æ“ä½œçš„è§è§£è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—é¢†åŸŸã€‚ç ”ç©¶ä½¿ç”¨I2B2 2012æ—¶åºå…³ç³»æŒ‘æˆ˜è¯­æ–™åº“ï¼Œæå‡ºä¸€ç§æ–°æ–¹æ³•GRAPHTREXï¼Œæ•´åˆåŸºäºèŒƒå›´çš„å®ä½“å…³ç³»æå–ã€ä¸´åºŠå¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œå¼‚è´¨å›¾è½¬æ¢å™¨ï¼Œä»¥æ•æ‰å±€éƒ¨å’Œå…¨å±€ä¾èµ–å…³ç³»ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ›æ–°çš„å…¨çƒåœ°æ ‡ï¼Œä¿ƒè¿›äº†è·¨è¶Šæ–‡æ¡£çš„ä¿¡æ¯ä¼ æ’­ï¼Œå¹¶æ”¹å–„äº†çŠ¶æ€æœ€ä¼˜çš„5.5%çš„tempeval F1åˆ†æ•°ï¼Œå¹¶åœ¨é•¿è·ç¦»å…³ç³»ä¸Šæé«˜äº†8.9%ï¼Œè¿™æ„æˆäº†ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚è¿™é¡¹å·¥ä½œä¸ä»…æ¨åŠ¨äº†æ—¶åºä¿¡æ¯æå–çš„è¿›æ­¥ï¼Œè€Œä¸”é€šè¿‡å¢å¼ºæ—¶åºæ¨ç†ä¸ºæ”¹è¿›è¯Šæ–­å’Œé¢„åæ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶åºä¿¡æ¯æå–å¯¹äºç†è§£ä¸´åºŠäº‹ä»¶çš„ä¸Šä¸‹æ–‡å’Œè·å–è§è§£è‡³å…³é‡è¦ã€‚</li>
<li>I2B2 2012 Temporal Relations Challengeè¯­æ–™åº“è¢«ç”¨äºç ”ç©¶ã€‚</li>
<li>GRAPHTREXæ–¹æ³•ç»“åˆäº†å¤šç§æŠ€æœ¯ï¼šåŸºäºèŒƒå›´çš„å®ä½“å…³ç³»æå–ã€ä¸´åºŠå¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œå¼‚è´¨å›¾è½¬æ¢å™¨ã€‚</li>
<li>HGTç»„ä»¶é€šè¿‡å…¨çƒåœ°æ ‡ä¿ƒè¿›äº†æ–‡æ¡£ä¸­çš„ä¿¡æ¯ä¼ æ’­ã€‚</li>
<li>GRAPHTREXæ–¹æ³•æ”¹å–„äº†ç°æœ‰æŠ€æœ¯çš„tempeval F1åˆ†æ•°ï¼Œå¹¶ä¸”åœ¨å¤„ç†é•¿è·ç¦»å…³ç³»æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ä»…æ¨åŠ¨äº†æ—¶åºä¿¡æ¯æå–çš„å‘å±•ï¼Œè€Œä¸”ä¸ºæ”¹è¿›è¯Šæ–­å’Œé¢„åæ¨¡å‹æ‰“ä¸‹äº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-341b6ae55dd416402126bfc0e374b56f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2bdfbbdb8a5e93a8d4cc2dcee55eedf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c47d9085b90d6e4f91ac243934183f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94fbe0d18bea9cd3e04a1564d0fe1d9f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="CardioTabNet-A-Novel-Hybrid-Transformer-Model-for-Heart-Disease-Prediction-using-Tabular-Medical-Data"><a href="#CardioTabNet-A-Novel-Hybrid-Transformer-Model-for-Heart-Disease-Prediction-using-Tabular-Medical-Data" class="headerlink" title="CardioTabNet: A Novel Hybrid Transformer Model for Heart Disease   Prediction using Tabular Medical Data"></a>CardioTabNet: A Novel Hybrid Transformer Model for Heart Disease   Prediction using Tabular Medical Data</h2><p><strong>Authors:Md. Shaheenur Islam Sumon, Md. Sakib Bin Islam, Md. Sohanur Rahman, Md. Sakib Abrar Hossain, Amith Khandakar, Anwarul Hasan, M Murugappan, Muhammad E. H. Chowdhury</strong></p>
<p>The early detection and prediction of cardiovascular diseases are crucial for reducing the severe morbidity and mortality associated with these conditions worldwide. A multi-headed self-attention mechanism, widely used in natural language processing (NLP), is operated by Transformers to understand feature interactions in feature spaces. However, the relationships between various features within biological systems remain ambiguous in these spaces, highlighting the necessity of early detection and prediction of cardiovascular diseases to reduce the severe morbidity and mortality with these conditions worldwide. We handle this issue with CardioTabNet, which exploits the strength of tab transformer to extract feature space which carries strong understanding of clinical cardiovascular data and its feature ranking. As a result, performance of downstream classical models significantly showed outstanding result. Our study utilizes the open-source dataset for heart disease prediction with 1190 instances and 11 features. In total, 11 features are divided into numerical (age, resting blood pressure, cholesterol, maximum heart rate, old peak, weight, and fasting blood sugar) and categorical (resting ECG, exercise angina, and ST slope). Tab transformer was used to extract important features and ranked them using random forest (RF) feature ranking algorithm. Ten machine-learning models were used to predict heart disease using selected features. After extracting high-quality features, the top downstream model (a hyper-tuned ExtraTree classifier) achieved an average accuracy rate of 94.1% and an average Area Under Curve (AUC) of 95.0%. Furthermore, a nomogram analysis was conducted to evaluate the modelâ€™s effectiveness in cardiovascular risk assessment. A benchmarking study was conducted using state-of-the-art models to evaluate our transformer-driven framework. </p>
<blockquote>
<p>æ—©æœŸå‘ç°å’Œé¢„æµ‹å¿ƒè¡€ç®¡ç–¾ç—…å¯¹äºå‡å°‘è¿™äº›ç–¾ç—…åœ¨å…¨çƒèŒƒå›´å†…å¯¼è‡´çš„ä¸¥é‡å‘ç—…ç‡å’Œæ­»äº¡ç‡è‡³å…³é‡è¦ã€‚å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œå¹¶ç”±Transformerè¿›è¡Œæ“ä½œä»¥ç†è§£ç‰¹å¾ç©ºé—´ä¸­çš„ç‰¹å¾äº¤äº’ã€‚ç„¶è€Œï¼Œåœ¨è¿™äº›ç©ºé—´ä¸­ï¼Œç”Ÿç‰©ç³»ç»Ÿå†…å„ç§ç‰¹å¾ä¹‹é—´çš„å…³ç³»ä»ç„¶æ¨¡ç³Šï¼Œè¿™å†æ¬¡å¼ºè°ƒäº†æ—©æœŸå‘ç°å’Œé¢„æµ‹å¿ƒè¡€ç®¡ç–¾ç—…ä»¥å‡å°‘è¿™äº›ç–¾ç—…åœ¨å…¨çƒèŒƒå›´å†…å¯¼è‡´çš„ä¸¥é‡å‘ç—…ç‡å’Œæ­»äº¡ç‡çš„å¿…è¦æ€§ã€‚æˆ‘ä»¬é€šè¿‡CardioTabNetè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå®ƒåˆ©ç”¨tab transformerçš„ä¼˜åŠ¿æ¥æå–å¯¹ä¸´åºŠå¿ƒè¡€ç®¡æ•°æ®æœ‰æ·±åˆ»ç†è§£çš„ç‰¹å¾ç©ºé—´åŠå…¶ç‰¹å¾æ’åã€‚å› æ­¤ï¼Œä¸‹æ¸¸ç»å…¸æ¨¡å‹çš„è¡¨ç°æ˜¾ç¤ºå‡ºäº†å‡ºè‰²çš„ç»“æœã€‚æˆ‘ä»¬çš„ç ”ç©¶åˆ©ç”¨å¼€æ”¾æºä»£ç æ•°æ®é›†è¿›è¡Œå¿ƒè„ç—…é¢„æµ‹ï¼ŒåŒ…å«1190ä¸ªå®ä¾‹å’Œ11ä¸ªç‰¹å¾ã€‚æ€»å…±çš„11ä¸ªç‰¹å¾åˆ†ä¸ºæ•°å€¼å‹ï¼ˆå¹´é¾„ã€é™æ¯è¡€å‹ã€èƒ†å›ºé†‡ã€æœ€å¤§å¿ƒç‡ã€æ—§å³°å€¼ã€ä½“é‡å’Œç©ºè…¹è¡€ç³–ï¼‰å’Œåˆ†ç±»å‹ï¼ˆé™æ¯å¿ƒç”µå›¾ã€è¿åŠ¨æ€§å¿ƒç»ç—›å’ŒSTæ–œç‡ï¼‰ã€‚Tab transformerè¢«ç”¨æ¥æå–é‡è¦ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨éšæœºæ£®æ—ï¼ˆRFï¼‰ç‰¹å¾æ’åç®—æ³•è¿›è¡Œæ’åã€‚ä½¿ç”¨é€‰å®šçš„ç‰¹å¾ï¼Œå…±é‡‡ç”¨äº†åç§æœºå™¨å­¦ä¹ æ¨¡å‹æ¥é¢„æµ‹å¿ƒè„ç—…ã€‚åœ¨æå–é«˜è´¨é‡ç‰¹å¾åï¼Œé¡¶çº§ä¸‹æ¸¸æ¨¡å‹ï¼ˆç»è¿‡è¶…å‚æ•°è°ƒæ•´çš„ExtraTreeåˆ†ç±»å™¨ï¼‰çš„å¹³å‡å‡†ç¡®ç‡ä¸º94.1%ï¼Œå¹³å‡æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰ä¸º95.0%ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†åˆ—çº¿å›¾åˆ†æï¼Œä»¥è¯„ä¼°æ¨¡å‹åœ¨å¿ƒè¡€ç®¡é£é™©è¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä½¿ç”¨æœ€æ–°æ¨¡å‹è¿›è¡Œäº†ä¸€é¡¹åŸºå‡†æµ‹è¯•ç ”ç©¶ï¼Œä»¥è¯„ä¼°æˆ‘ä»¬åŸºäºtransformerçš„æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17664v1">PDF</a> This paper is currently under review in the Health Information   Science and Systems journal</p>
<p><strong>Summary</strong><br>å¿ƒè¡€ç®¡ç–¾ç—…æ—©æœŸæ£€æµ‹å’Œé¢„æµ‹å¯¹äºé™ä½å…¨çƒç›¸å…³ä¸¥é‡å‘ç—…ç‡å’Œæ­»äº¡ç‡è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶é‡‡ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡CardioTabNetåˆ©ç”¨è¡¨è½¬æ¢å™¨çš„ä¼˜åŠ¿æå–ç‰¹å¾ç©ºé—´ï¼Œå¯¹å¿ƒè¡€ç®¡ç–¾ç—…æ•°æ®è¿›è¡Œæ·±å…¥ç†è§£å¹¶æ’åã€‚ç ”ç©¶ä½¿ç”¨å¼€æºæ•°æ®é›†ï¼Œé‡‡ç”¨åç§æœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹å¿ƒè„ç—…ï¼Œå¹¶åˆ©ç”¨éšæœºæ£®æ—ç®—æ³•å¯¹ç‰¹å¾è¿›è¡Œæ’åã€‚æœ€é«˜æ€§èƒ½æ¨¡å‹è¾¾åˆ°å¹³å‡å‡†ç¡®ç‡94.1%ï¼Œå¹³å‡AUCå€¼95.0%ã€‚ç ”ç©¶è¿˜é€šè¿‡æ ‡å‡†å¯¹æ¯”ç ”ç©¶å’Œåˆ—çº¿å›¾åˆ†æè¯„ä¼°äº†æ¨¡å‹åœ¨å¿ƒè¡€ç®¡ç–¾ç—…é£é™©è¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§ã€‚æœ¬ç ”ç©¶ç»“æœçªå‡ºäº†åŸºäºè¡¨è½¬æ¢å™¨çš„ç‰¹å¾æå–æ–¹æ³•åœ¨å¿ƒè¡€ç®¡ç–¾ç—…é¢„æµ‹ä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ—©æœŸæ£€æµ‹å’Œé¢„æµ‹å¿ƒè¡€ç®¡ç–¾ç—…å¯¹äºå‡å°‘å…¨çƒå‘ç—…ç‡å’Œæ­»äº¡ç‡éå¸¸é‡è¦ã€‚</li>
<li>å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ç”¨äºç†è§£ç‰¹å¾é—´çš„ç›¸äº’ä½œç”¨ã€‚</li>
<li>CardioTabNetåˆ©ç”¨è¡¨è½¬æ¢å™¨çš„ä¼˜åŠ¿æå–ç‰¹å¾ç©ºé—´å¹¶æ·±å…¥ç†è§£å¿ƒè¡€ç®¡ä¸´åºŠæ•°æ®ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨å¼€æºæ•°æ®é›†è¿›è¡Œå¿ƒè„ç—…é¢„æµ‹ï¼Œæ¶‰åŠå¤šç§æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>é€šè¿‡éšæœºæ£®æ—ç®—æ³•å¯¹ç‰¹å¾è¿›è¡Œæ’åï¼Œæœ€é«˜æ€§èƒ½æ¨¡å‹å¹³å‡å‡†ç¡®ç‡94.1%ï¼Œå¹³å‡AUCå€¼95.0%ã€‚</li>
<li>ç ”ç©¶é€šè¿‡åˆ—çº¿å›¾åˆ†æå’Œæ ‡å‡†å¯¹æ¯”ç ”ç©¶è¯„ä¼°äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17664">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1616c86c764887e2ab781c249ccda966.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80106469257415bc1d79c13a0ceb09ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-370e0ea52026c5358710ecb3e4db5330.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ea9d94a2eccddbfea9458763d2d907a.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="OmniScience-A-Domain-Specialized-LLM-for-Scientific-Reasoning-and-Discovery"><a href="#OmniScience-A-Domain-Specialized-LLM-for-Scientific-Reasoning-and-Discovery" class="headerlink" title="OmniScience: A Domain-Specialized LLM for Scientific Reasoning and   Discovery"></a>OmniScience: A Domain-Specialized LLM for Scientific Reasoning and   Discovery</h2><p><strong>Authors:Vignesh Prabhakar, Md Amirul Islam, Adam Atanas, Yao-Ting Wang, Joah Han, Aastha Jhunjhunwala, Rucha Apte, Robert Clark, Kang Xu, Zihan Wang, Kai Liu</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable potential in advancing scientific knowledge and addressing complex challenges. In this work, we introduce OmniScience, a specialized large reasoning model for general science, developed through three key components: (1) domain adaptive pretraining on a carefully curated corpus of scientific literature, (2) instruction tuning on a specialized dataset to guide the model in following domain-specific tasks, and (3) reasoning-based knowledge distillation through fine-tuning to significantly enhance its ability to generate contextually relevant and logically sound responses. We demonstrate the versatility of OmniScience by developing a battery agent that efficiently ranks molecules as potential electrolyte solvents or additives. Comprehensive evaluations reveal that OmniScience is competitive with state-of-the-art large reasoning models on the GPQA Diamond and domain-specific battery benchmarks, while outperforming all public reasoning and non-reasoning models with similar parameter counts. We further demonstrate via ablation experiments that domain adaptive pretraining and reasoning-based knowledge distillation are critical to attain our performance levels, across benchmarks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨è¿›ç§‘å­¦çŸ¥è¯†å’Œåº”å¯¹å¤æ‚æŒ‘æˆ˜æ–¹é¢æ˜¾ç¤ºå‡ºæ˜¾è‘—æ½œåŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†OmniScienceï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹é€šç”¨ç§‘å­¦çš„ç‰¹æ®Šå¤§å‹æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡ä¸‰ä¸ªå…³é”®ç»„ä»¶å¼€å‘ï¼šï¼ˆ1ï¼‰åœ¨ç²¾å¿ƒç­›é€‰çš„ç§‘å­¦æ–‡çŒ®è¯­æ–™åº“ä¸Šè¿›è¡Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼Œï¼ˆ2ï¼‰åœ¨ä¸“é—¨çš„æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹æ‰§è¡Œç‰¹å®šé¢†åŸŸçš„ä»»åŠ¡ï¼Œï¼ˆ3ï¼‰é€šè¿‡å¾®è°ƒè¿›è¡ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦ï¼Œä»¥æ˜¾è‘—æé«˜å…¶ç”Ÿæˆè¯­å¢ƒç›¸å…³å’Œé€»è¾‘ä¸¥è°¨å“åº”çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡å¼€å‘ç”µæ± ä»£ç†æ¥å±•ç¤ºOmniScienceçš„é€šç”¨æ€§ï¼Œè¯¥ä»£ç†å¯ä»¥æœ‰æ•ˆåœ°å¯¹åˆ†å­è¿›è¡Œæ’åï¼Œä½œä¸ºæ½œåœ¨çš„ç”µè§£è´¨æº¶å‰‚æˆ–æ·»åŠ å‰‚ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒOmniScienceåœ¨GPQA Diamondå’Œç‰¹å®šé¢†åŸŸçš„ç”µæ± åŸºå‡†æµ‹è¯•ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œä¼˜äºæ‰€æœ‰å…·æœ‰ç›¸ä¼¼å‚æ•°æ•°é‡çš„å…¬å…±æ¨ç†å’Œéæ¨ç†æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜é€šè¿‡æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒå’ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦å¯¹äºè¾¾åˆ°æˆ‘ä»¬çš„æ€§èƒ½æ°´å¹³è‡³å…³é‡è¦ï¼Œè·¨è¶Šå„ä¸ªåŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17604v1">PDF</a> </p>
<p><strong>æ€»ç»“</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨è¿›ç§‘å­¦çŸ¥è¯†åŠåº”å¯¹å¤æ‚æŒ‘æˆ˜æ–¹é¢å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€æ¬¾é’ˆå¯¹é€šç”¨ç§‘å­¦çš„ç‰¹æ®Šå¤§å‹æ¨ç†æ¨¡å‹â€”â€”OmniScienceï¼Œå…¶å¼€å‘æ¶‰åŠä¸‰ä¸ªå…³é”®éƒ¨åˆ†ï¼š1ï¼‰åœ¨ç²¾å¿ƒç­›é€‰çš„ç§‘å­¦æ–‡çŒ®è¯­æ–™åº“ä¸Šè¿›è¡Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼›2ï¼‰åœ¨ä¸“é—¨æ•°æ®é›†ä¸Šè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹æ‰§è¡Œç‰¹å®šé¢†åŸŸä»»åŠ¡ï¼›3ï¼‰é€šè¿‡å¾®è°ƒè¿›è¡ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦ï¼Œä»¥æ˜¾è‘—å¢å¼ºæ¨¡å‹ç”Ÿæˆè¯­å¢ƒç›¸å…³å’Œé€»è¾‘ä¸¥è°¨å›åº”çš„èƒ½åŠ›ã€‚é€šè¿‡å¼€å‘ç”µæ± ä»£ç†ï¼Œæˆ‘ä»¬å±•ç¤ºäº†OmniScienceçš„é€šç”¨æ€§ï¼Œè¯¥ä»£ç†èƒ½å¤Ÿé«˜æ•ˆåœ°å¯¹åˆ†å­è¿›è¡Œæ½œåœ¨ç”µè§£è´¨æº¶å‰‚æˆ–æ·»åŠ å‰‚çš„æ’åã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒOmniScienceåœ¨GPQA Diamondå’Œç‰¹å®šé¢†åŸŸç”µæ± åŸºå‡†æµ‹è¯•ä¸Šï¼Œä¸æœ€æ–°å¤§å‹æ¨ç†æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶åœ¨å‚æ•°æ•°é‡ç›¸ä¼¼çš„æ‰€æœ‰å…¬å…±æ¨ç†å’Œéæ¨ç†æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ã€‚é€šè¿‡æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯æ˜ï¼Œé¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒå’ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦å¯¹äºè¾¾åˆ°æˆ‘ä»¬çš„æ€§èƒ½æ°´å¹³è‡³å…³é‡è¦ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨è¿›ç§‘å­¦çŸ¥è¯†å’Œåº”å¯¹å¤æ‚æŒ‘æˆ˜æ–¹é¢å…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>OmniScienceæ˜¯ä¸€æ¬¾é’ˆå¯¹é€šç”¨ç§‘å­¦çš„ç‰¹æ®Šå¤§å‹æ¨ç†æ¨¡å‹ï¼ŒåŒ…å«é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒã€æŒ‡ä»¤è°ƒæ•´ã€å’ŒåŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦ä¸‰ä¸ªå…³é”®éƒ¨åˆ†ã€‚</li>
<li>OmniScienceèƒ½å¤Ÿé€šè¿‡å¼€å‘ç”µæ± ä»£ç†å±•ç¤ºå…¶é€šç”¨æ€§ï¼Œè¯¥ä»£ç†èƒ½å¤Ÿé«˜æ•ˆæ’åæ½œåœ¨ç”µè§£è´¨æº¶å‰‚æˆ–æ·»åŠ å‰‚ã€‚</li>
<li>ç»¼åˆè¯„ä¼°æ˜¾ç¤ºï¼ŒOmniScienceåœ¨GPQA Diamondå’Œç‰¹å®šé¢†åŸŸç”µæ± åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>OmniScienceåœ¨åŒç±»æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œå°¤å…¶æ˜¯åœ¨å‚æ•°æ•°é‡ç›¸ä¼¼çš„æƒ…å†µä¸‹ã€‚</li>
<li>æ¶ˆèå®éªŒè¯æ˜é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒå¯¹äºOmniScienceçš„æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>åŸºäºæ¨ç†çš„çŸ¥è¯†è’¸é¦ä¹Ÿæ˜¯è¾¾åˆ°OmniScienceé«˜æ€§èƒ½æ°´å¹³çš„å…³é”®å› ç´ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b70ad4c15e88b84b87948f40dfe23253.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cddc57a4b1e46185ec103df103a43ee.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Modifying-Large-Language-Model-Post-Training-for-Diverse-Creative-Writing"><a href="#Modifying-Large-Language-Model-Post-Training-for-Diverse-Creative-Writing" class="headerlink" title="Modifying Large Language Model Post-Training for Diverse Creative   Writing"></a>Modifying Large Language Model Post-Training for Diverse Creative   Writing</h2><p><strong>Authors:John Joon Young Chung, Vishakh Padmakumar, Melissa Roemmele, Yuqian Sun, Max Kreminski</strong></p>
<p>As creative writing tasks do not have singular correct answers, large language models (LLMs) trained to perform these tasks should be able to generate diverse valid outputs. However, LLM post-training often focuses on improving generation quality but neglects to facilitate output diversity. Hence, in creative writing generation, we investigate post-training approaches to promote both output diversity and quality. Our core idea is to include deviation â€“ the degree of difference between a training sample and all other samples with the same prompt â€“ in the training objective to facilitate learning from rare high-quality instances. By adopting our approach to direct preference optimization (DPO) and odds ratio preference optimization (ORPO), we demonstrate that we can promote the output diversity of trained models while minimally decreasing quality. Our best model with 8B parameters could achieve on-par diversity as a human-created dataset while having output quality similar to the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We further validate our approaches with a human evaluation, an ablation, and a comparison to an existing diversification approach, DivPO. </p>
<blockquote>
<p>ç”±äºåˆ›é€ æ€§å†™ä½œä»»åŠ¡æ²¡æœ‰å•ä¸€çš„æ­£ç¡®ç­”æ¡ˆï¼Œå› æ­¤è®­ç»ƒç”¨äºæ‰§è¡Œè¿™äº›ä»»åŠ¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”è¯¥èƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–çš„æœ‰æ•ˆè¾“å‡ºã€‚ç„¶è€Œï¼ŒLLMçš„åæœŸè®­ç»ƒé€šå¸¸ä¾§é‡äºæé«˜ç”Ÿæˆè´¨é‡ï¼Œå´å¿½è§†äº†ä¿ƒè¿›è¾“å‡ºçš„å¤šæ ·æ€§ã€‚å› æ­¤ï¼Œåœ¨åˆ›é€ æ€§å†™ä½œç”Ÿæˆä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¿ƒè¿›è¾“å‡ºå¤šæ ·æ€§å’Œè´¨é‡çš„åæœŸè®­ç»ƒæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†åå·®çº³å…¥è®­ç»ƒç›®æ ‡ä¸­ï¼Œåå·®æ˜¯æŒ‡ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ä¸å…·æœ‰ç›¸åŒæç¤ºçš„æ‰€æœ‰å…¶ä»–æ ·æœ¬ä¹‹é—´çš„å·®å¼‚ç¨‹åº¦ï¼Œä»¥ä¿ƒè¿›ä»ç½•è§çš„é«˜è´¨é‡å®ä¾‹ä¸­å­¦ä¹ ã€‚é€šè¿‡é‡‡ç”¨æˆ‘ä»¬çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œèµ”ç‡æ¯”ç‡åå¥½ä¼˜åŒ–ï¼ˆORPOï¼‰æ–¹æ³•ï¼Œæˆ‘ä»¬è¯æ˜äº†å¯ä»¥åœ¨ä¸æ˜¾è‘—é™ä½è´¨é‡çš„æƒ…å†µä¸‹ä¿ƒè¿›è®­ç»ƒæ¨¡å‹çš„è¾“å‡ºå¤šæ ·æ€§ã€‚æˆ‘ä»¬æœ€å¥½çš„åŒ…å«8äº¿å‚æ•°çš„æ¨¡å‹å¯ä»¥è¾¾åˆ°ä¸äººç±»åˆ›å»ºæ•°æ®é›†ç›¸å½“çš„å¤šæ ·æ€§ï¼ŒåŒæ—¶å…¶è¾“å‡ºè´¨é‡ä¸æˆ‘ä»¬æ‰€è€ƒå¯Ÿçš„æœ€ä½³æŒ‡ä»¤è°ƒæ•´æ¨¡å‹GPT-4oå’ŒDeepSeek-R1ç›¸ä¼¼ã€‚æˆ‘ä»¬é€šè¿‡äººç±»è¯„ä¼°ã€æ¶ˆé™¤å¯¹æ¯”ä»¥åŠä¸ç°æœ‰çš„å¤šæ ·åŒ–æ–¹æ³•DivPOçš„æ¯”è¾ƒï¼Œè¿›ä¸€æ­¥éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17126v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åˆ›ä½œå†™ä½œä»»åŠ¡ä¸­èƒ½å¤Ÿç”Ÿæˆå¤šæ ·çš„æœ‰æ•ˆè¾“å‡ºï¼Œä½†ç°æœ‰çš„è®­ç»ƒåå¤„ç†æ–¹å¼å¾€å¾€ä¾§é‡äºæé«˜ç”Ÿæˆè´¨é‡è€Œå¿½ç•¥äº†è¾“å‡ºå¤šæ ·æ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºåœ¨è®­ç»ƒç›®æ ‡ä¸­åŠ å…¥åå·®çš„æ–¹æ³•ï¼Œåˆ©ç”¨ç½•è§çš„é«˜è´¨é‡å®ä¾‹ä¿ƒè¿›å­¦ä¹ ã€‚é€šè¿‡é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œèµ”ç‡æ¯”ç‡åå¥½ä¼˜åŒ–ï¼ˆORPOï¼‰çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒè¾“å‡ºè´¨é‡çš„åŒæ—¶æé«˜æ¨¡å‹çš„è¾“å‡ºå¤šæ ·æ€§ã€‚æœ€ä½³æ¨¡å‹çš„å‚æ•°è¾¾åˆ°8Bï¼Œå…¶è¾“å‡ºå¤šæ ·æ€§ä¸äººç±»åˆ›å»ºçš„æ•°æ®é›†ç›¸å½“ï¼Œè¾“å‡ºè´¨é‡ä¹Ÿä¸æœ€ä½³æŒ‡ä»¤è°ƒæ•´æ¨¡å‹GPT-4oå’ŒDeepSeek-R1ç›¸ä¼¼ã€‚ç»è¿‡äººå·¥è¯„ä¼°ã€æ¶ˆå»æ³•å’Œä¸ç°æœ‰å¤šæ ·åŒ–æ–¹æ³•DivPOçš„æ¯”è¾ƒï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨åˆ›ä½œå†™ä½œä»»åŠ¡ä¸­å¯ä»¥ç”Ÿæˆå¤šæ ·çš„æœ‰æ•ˆè¾“å‡ºï¼Œä½†ç°æœ‰çš„è®­ç»ƒåå¤„ç†æ–¹å¼å¿½ç•¥äº†è¾“å‡ºå¤šæ ·æ€§ã€‚</li>
<li>ç ”ç©¶è€…æå‡ºåœ¨è®­ç»ƒç›®æ ‡ä¸­åŠ å…¥åå·®çš„æ–¹æ³•ï¼Œåˆ©ç”¨ç½•è§çš„é«˜è´¨é‡å®ä¾‹ä¿ƒè¿›å­¦ä¹ ã€‚</li>
<li>é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œèµ”ç‡æ¯”ç‡åå¥½ä¼˜åŒ–ï¼ˆORPOï¼‰çš„æ–¹æ³•å¯ä»¥æé«˜æ¨¡å‹çš„è¾“å‡ºå¤šæ ·æ€§ã€‚</li>
<li>æœ€ä½³æ¨¡å‹çš„å‚æ•°è¾¾åˆ°8Bï¼Œå…¶è¾“å‡ºå¤šæ ·æ€§ä¸äººç±»åˆ›å»ºçš„æ•°æ®é›†ç›¸å½“ã€‚</li>
<li>æœ€ä½³æ¨¡å‹çš„è¾“å‡ºè´¨é‡ä¸æœ€ä½³æŒ‡ä»¤è°ƒæ•´æ¨¡å‹GPT-4oå’ŒDeepSeek-R1ç›¸ä¼¼ã€‚</li>
<li>ç»è¿‡äººå·¥è¯„ä¼°éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17126">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ae51e11273571eb295e3b9d4e9c2c1d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c3538f9f10a90f3e5278cbe76ae0e7e.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="LoRASculpt-Sculpting-LoRA-for-Harmonizing-General-and-Specialized-Knowledge-in-Multimodal-Large-Language-Models"><a href="#LoRASculpt-Sculpting-LoRA-for-Harmonizing-General-and-Specialized-Knowledge-in-Multimodal-Large-Language-Models" class="headerlink" title="LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized   Knowledge in Multimodal Large Language Models"></a>LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized   Knowledge in Multimodal Large Language Models</h2><p><strong>Authors:Jian Liang, Wenke Huang, Guancheng Wan, Qu Yang, Mang Ye</strong></p>
<p>While Multimodal Large Language Models (MLLMs) excel at generalizing across modalities and tasks, effectively adapting them to specific downstream tasks while simultaneously retaining both general and specialized knowledge remains challenging. Although Low-Rank Adaptation (LoRA) is widely used to efficiently acquire specialized knowledge in MLLMs, it introduces substantial harmful redundancy during visual instruction tuning, which exacerbates the forgetting of general knowledge and degrades downstream task performance. To address this issue, we propose LoRASculpt to eliminate harmful redundant parameters, thereby harmonizing general and specialized knowledge. Specifically, under theoretical guarantees, we introduce sparse updates into LoRA to discard redundant parameters effectively. Furthermore, we propose a Conflict Mitigation Regularizer to refine the update trajectory of LoRA, mitigating knowledge conflicts with the pretrained weights. Extensive experimental results demonstrate that even at very high degree of sparsity ($\le$ 5%), our method simultaneously enhances generalization and downstream task performance. This confirms that our approach effectively mitigates the catastrophic forgetting issue and further promotes knowledge harmonization in MLLMs. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è·¨æ¨¡æ€å’Œä»»åŠ¡æ³›åŒ–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å¦‚ä½•æœ‰æ•ˆåœ°å°†å®ƒä»¬é€‚åº”ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒæ—¶ä¿ç•™ä¸€èˆ¬å’Œä¸“ä¸šçŸ¥è¯†ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è™½ç„¶ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è¢«å¹¿æ³›ç”¨äºåœ¨MLLMsä¸­é«˜æ•ˆåœ°è·å–ä¸“ä¸šçŸ¥è¯†ï¼Œä½†åœ¨è§†è§‰æŒ‡ä»¤è°ƒæ•´è¿‡ç¨‹ä¸­ä¼šå¼•å…¥å¤§é‡çš„æœ‰å®³å†—ä½™ï¼Œè¿™åŠ å‰§äº†å¯¹ä¸€èˆ¬çŸ¥è¯†çš„é—å¿˜å¹¶é™ä½äº†ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºLoRASculptæ¥æ¶ˆé™¤æœ‰å®³çš„å†—ä½™å‚æ•°ï¼Œä»è€Œåè°ƒä¸€èˆ¬çŸ¥è¯†å’Œä¸“ä¸šçŸ¥è¯†ã€‚å…·ä½“åœ°è¯´ï¼Œåœ¨ç†è®ºä¿è¯ä¸‹ï¼Œæˆ‘ä»¬å°†ç¨€ç–æ›´æ–°å¼•å…¥LoRAï¼Œä»¥æœ‰æ•ˆåœ°ä¸¢å¼ƒå†—ä½™å‚æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å†²çªç¼“è§£æ­£åˆ™åŒ–å™¨ï¼Œä»¥ä¼˜åŒ–LoRAçš„æ›´æ–°è½¨è¿¹ï¼Œç¼“è§£ä¸é¢„è®­ç»ƒæƒé‡çš„çŸ¥è¯†å†²çªã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨éå¸¸é«˜çš„ç¨€ç–åº¦ï¼ˆâ‰¤5%ï¼‰ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åŒæ—¶æé«˜äº†æ³›åŒ–å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚è¿™è¯å®äº†æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°ç¼“è§£äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå¹¶è¿›ä¸€æ­¥ä¿ƒè¿›äº†MLLMsä¸­çš„çŸ¥è¯†åè°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.16843v1">PDF</a> Accepted by CVPR 2025</p>
<p><strong>æ‘˜è¦</strong><br>    å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è·¨æ¨¡æ€å’Œä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›å‡ºè‰²ï¼Œä½†åœ¨ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡ä¸­æœ‰æ•ˆé€‚åº”å¹¶åŒæ—¶ä¿ç•™ä¸€èˆ¬å’Œä¸“ä¸šçŸ¥è¯†ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è™½ç„¶ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å¹¿æ³›ç”¨äºåœ¨MLLMsä¸­é«˜æ•ˆè·å–ä¸“ä¸šçŸ¥è¯†ï¼Œä½†åœ¨è§†è§‰æŒ‡ä»¤è°ƒæ•´æ—¶å¼•å…¥äº†å¤§é‡æœ‰å®³å†—ä½™ï¼Œè¿™åŠ å‰§äº†å¯¹ä¸€èˆ¬çŸ¥è¯†çš„é—å¿˜å¹¶é™ä½äº†ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºLoRASculptä»¥æ¶ˆé™¤æœ‰å®³å†—ä½™å‚æ•°ï¼Œä»è€Œåè°ƒä¸€èˆ¬çŸ¥è¯†å’Œä¸“ä¸šçŸ¥è¯†ã€‚å…·ä½“è€Œè¨€ï¼Œåœ¨ç†è®ºä¿è¯ä¸‹ï¼Œæˆ‘ä»¬å°†ç¨€ç–æ›´æ–°å¼•å…¥LoRAä»¥æœ‰æ•ˆåœ°æ¶ˆé™¤å†—ä½™å‚æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å†²çªç¼“è§£æ­£åˆ™åŒ–å™¨ï¼Œä»¥ä¼˜åŒ–LoRAçš„æ›´æ–°è½¨è¿¹ï¼Œç¼“è§£ä¸é¢„è®­ç»ƒæƒé‡çš„çŸ¥è¯†å†²çªã€‚å¤§é‡å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨éå¸¸é«˜çš„ç¨€ç–åº¦ï¼ˆâ‰¤5%ï¼‰ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åŒæ—¶æé«˜äº†æ³›åŒ–å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚è¿™è¯å®æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°ç¼“è§£äº†ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œå¹¶è¿›ä¸€æ­¥ä¿ƒè¿›äº†MLLMsä¸­çš„çŸ¥è¯†åè°ƒã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>MLLMsåœ¨è·¨æ¨¡æ€å’Œä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›å¼ºå¤§ï¼Œä½†é€‚åº”ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡æ—¶ä¿ç•™ä¸€èˆ¬å’Œä¸“ä¸šçŸ¥è¯†å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>LoRAåœ¨è·å–ä¸“ä¸šçŸ¥è¯†æ—¶å¼•å…¥æœ‰å®³å†—ä½™ï¼Œå½±å“ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>LoRASculpté€šè¿‡æ¶ˆé™¤å†—ä½™å‚æ•°è§£å†³è¿™ä¸€é—®é¢˜ï¼Œåè°ƒä¸€èˆ¬çŸ¥è¯†å’Œä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>LoRASculptä½¿ç”¨ç¨€ç–æ›´æ–°æœ‰æ•ˆæ¶ˆé™¤å†—ä½™å‚æ•°ã€‚</li>
<li>å¼•å…¥å†²çªç¼“è§£æ­£åˆ™åŒ–å™¨ï¼Œä¼˜åŒ–æ›´æ–°è½¨è¿¹ï¼Œç¼“è§£ä¸é¢„è®­ç»ƒæƒé‡çš„çŸ¥è¯†å†²çªã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒLoRASculptåœ¨è¾ƒé«˜ç¨€ç–åº¦ä¸‹æé«˜äº†æ³›åŒ–å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>LoRASculptæœ‰æ•ˆç¼“è§£ç¾éš¾æ€§é—å¿˜é—®é¢˜ï¼Œä¿ƒè¿›MLLMsä¸­çš„çŸ¥è¯†åè°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.16843">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eb615781eca9eda82b961164a220484c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85fdd1284d0b11cfb9c16fcff1494c0b.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Leveraging-MoE-based-Large-Language-Model-for-Zero-Shot-Multi-Task-Semantic-Communication"><a href="#Leveraging-MoE-based-Large-Language-Model-for-Zero-Shot-Multi-Task-Semantic-Communication" class="headerlink" title="Leveraging MoE-based Large Language Model for Zero-Shot Multi-Task   Semantic Communication"></a>Leveraging MoE-based Large Language Model for Zero-Shot Multi-Task   Semantic Communication</h2><p><strong>Authors:Sin-Yu Huang, Renjie Liao, Vincent W. S. Wong</strong></p>
<p>Multi-task semantic communication (SC) can reduce the computational resources in wireless systems since retraining is not required when switching between tasks. However, existing approaches typically rely on task-specific embeddings to identify the intended task, necessitating retraining the entire model when given a new task. Consequently, this drives the need for a multi-task SC system that can handle new tasks without additional training, known as zero-shot learning. Inspired by the superior zero-shot capabilities of large language models (LLMs), we leverage pre-trained instruction-tuned LLMs, referred to as fine-tuned language net (FLAN), to improve the generalization capability. We incorporate a mixture-of-experts (MoE) architecture in the FLAN model and propose MoE-FLAN-SC architecture for multi-task SC systems. Our proposed MoE-FLAN-SC architecture can further improve the performance of FLAN-T5 model without increasing the computational cost. Moreover, we design a multi-task feature extraction module (FEM) which can adaptively extract relevant features across various tasks given the provided features and signal-to-noise ratio (SNR). Simulation results show that our proposed MoE-FLAN-SC architecture outperforms three state-of-the-art models in terms of the average accuracy on four different unseen tasks. </p>
<blockquote>
<p>å¤šä»»åŠ¡è¯­ä¹‰é€šä¿¡ï¼ˆSCï¼‰èƒ½å¤ŸèŠ‚çœæ— çº¿ç³»ç»Ÿä¸­çš„è®¡ç®—èµ„æºï¼Œå› ä¸ºä»»åŠ¡é—´åˆ‡æ¢æ—¶æ— éœ€é‡æ–°è®­ç»ƒã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„åµŒå…¥æ¥è¯†åˆ«æ„å›¾ä»»åŠ¡ï¼Œå½“ç»™å®šæ–°ä»»åŠ¡æ—¶éœ€è¦é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹ã€‚å› æ­¤ï¼Œè¿™å¼•å‘äº†å¯¹èƒ½å¤Ÿå¤„ç†æ–°ä»»åŠ¡è€Œæ— éœ€é¢å¤–è®­ç»ƒçš„å¤šä»»åŠ¡SCç³»ç»Ÿçš„éœ€æ±‚ï¼Œè¿™è¢«ç§°ä¸ºé›¶å°„å‡»å­¦ä¹ ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å“è¶Šé›¶å°„å‡»èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„æŒ‡ä»¤è°ƒæ•´LLMï¼Œç§°ä¸ºç²¾ç»†è°ƒæ•´è¯­è¨€ç½‘ç»œï¼ˆFLANï¼‰ï¼Œä»¥æé«˜æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬å°†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„èå…¥FLANæ¨¡å‹ï¼Œå¹¶æå‡ºMoE-FLAN-SCæ¶æ„ç”¨äºå¤šä»»åŠ¡SCç³»ç»Ÿã€‚æˆ‘ä»¬æå‡ºçš„MoE-FLAN-SCæ¶æ„å¯ä»¥è¿›ä¸€æ­¥æé«˜FLAN-T5æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸å¢åŠ è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šä»»åŠ¡ç‰¹å¾æå–æ¨¡å—ï¼ˆFEMï¼‰ï¼Œè¯¥æ¨¡å—å¯ä»¥è‡ªé€‚åº”åœ°æå–ç»™å®šç‰¹å¾å’Œä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ä¸‹å„ç§ä»»åŠ¡çš„ç›¸å…³ç‰¹å¾ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„MoE-FLAN-SCæ¶æ„åœ¨å››ä¸ªä¸åŒæœªè§ä»»åŠ¡ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ä¼˜äºä¸‰ç§æœ€æ–°æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.15722v2">PDF</a> Accepted by IEEE International Conference on Communications (ICC),   June 2025, Montreal, Canada</p>
<p><strong>Summary</strong></p>
<p>å¤šä»»åŠ¡è¯­ä¹‰é€šä¿¡ï¼ˆSCï¼‰èƒ½å‡å°‘æ— çº¿ç³»ç»Ÿä¸­çš„è®¡ç®—èµ„æºï¼Œå› ä¸ºä»»åŠ¡åˆ‡æ¢æ—¶æ— éœ€é‡æ–°è®­ç»ƒã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–ä»»åŠ¡ç‰¹å®šåµŒå…¥æ¥è¯†åˆ«ç›®æ ‡ä»»åŠ¡ï¼Œå½“é¢ä¸´æ–°ä»»åŠ¡æ—¶éœ€è¦é‡æ–°è®­ç»ƒæ•´ä¸ªæ¨¡å‹ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§èƒ½å¤„ç†æ–°ä»»åŠ¡è€Œæ— éœ€é¢å¤–è®­ç»ƒçš„å¤šä»»åŠ¡SCç³»ç»Ÿï¼Œå³é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‡ºè‰²é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›çš„å¯å‘ï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„æŒ‡ä»¤è°ƒæ•´LLMï¼ˆç§°ä¸ºç²¾ç»†è°ƒæ•´è¯­è¨€ç½‘ç»œï¼ˆFLANï¼‰ï¼‰æ¥æé«˜æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬å°†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„èå…¥FLANæ¨¡å‹ï¼Œå¹¶æå‡ºMoE-FLAN-SCæ¶æ„ç”¨äºå¤šä»»åŠ¡SCç³»ç»Ÿã€‚æˆ‘ä»¬æå‡ºçš„MoE-FLAN-SCæ¶æ„èƒ½åœ¨ä¸å¢åŠ è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹æé«˜FLAN-T5æ¨¡å‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå¤šä»»åŠ¡ç‰¹å¾æå–æ¨¡å—ï¼ˆFEMï¼‰ï¼Œå¯ä»¥è‡ªé€‚åº”åœ°æå–å„ç§ä»»åŠ¡çš„ç›¸å…³ç‰¹å¾ï¼Œå¹¶è€ƒè™‘æä¾›çš„ç‰¹å¾å’Œä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ã€‚æ¨¡æ‹Ÿç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„MoE-FLAN-SCæ¶æ„åœ¨å››ä¸ªä¸åŒæœªè§ä»»åŠ¡ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ä¼˜äºä¸‰ç§æœ€æ–°æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šä»»åŠ¡è¯­ä¹‰é€šä¿¡ï¼ˆSCï¼‰èƒ½å‡å°‘æ— çº¿ç³»ç»Ÿä¸­çš„è®¡ç®—èµ„æºï¼Œä¸”åˆ‡æ¢ä»»åŠ¡æ—¶æ— éœ€é‡æ–°è®­ç»ƒã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–ä»»åŠ¡ç‰¹å®šåµŒå…¥æ¥è¯†åˆ«ç›®æ ‡ä»»åŠ¡ï¼Œè¿™åœ¨æ–°ä»»åŠ¡å¤„ç†æ—¶å¸¦æ¥ä¸ä¾¿ã€‚</li>
<li>éœ€è¦ä¸€ç§å…·æœ‰é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›çš„å¤šä»»åŠ¡SCç³»ç»Ÿæ¥å¤„ç†æ–°ä»»åŠ¡è€Œæ— éœ€é¢å¤–è®­ç»ƒã€‚</li>
<li>å—LLMå‡ºè‰²é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›çš„å¯å‘ï¼Œç»“åˆé¢„è®­ç»ƒæŒ‡ä»¤è°ƒæ•´çš„LLMï¼ˆFLANï¼‰æå‡æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>èå…¥æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„åˆ°FLANæ¨¡å‹ä¸­ï¼Œæå‡ºMoE-FLAN-SCæ¶æ„ç”¨äºå¤šä»»åŠ¡SCç³»ç»Ÿï¼Œæé«˜æ€§èƒ½ä¸”è®¡ç®—æˆæœ¬ä¸å¢ã€‚</li>
<li>è®¾è®¡å¤šä»»åŠ¡ç‰¹å¾æå–æ¨¡å—ï¼ˆFEMï¼‰ï¼Œèƒ½è‡ªé€‚åº”æå–ä¸åŒä»»åŠ¡çš„ç›¸å…³ç‰¹å¾ï¼Œå¹¶è€ƒè™‘ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.15722">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0d9a28790a76e150345b9ca04de041d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ac83f6ab5cd9785f5cba1910eda3bb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe92b68231391ecc7ad906534068be2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d1f369b724789f119cec5941d99fb4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04e770da0e96338e2d6785093b505f1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca3cfe37918e796e5148de6d2477c106.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="X2I-Seamless-Integration-of-Multimodal-Understanding-into-Diffusion-Transformer-via-Attention-Distillation"><a href="#X2I-Seamless-Integration-of-Multimodal-Understanding-into-Diffusion-Transformer-via-Attention-Distillation" class="headerlink" title="X2I: Seamless Integration of Multimodal Understanding into Diffusion   Transformer via Attention Distillation"></a>X2I: Seamless Integration of Multimodal Understanding into Diffusion   Transformer via Attention Distillation</h2><p><strong>Authors:Jian Ma, Qirong Peng, Xu Guo, Chen Chen, Haonan Lu, Zhenyu Yang</strong></p>
<p>Text-to-image (T2I) models are well known for their ability to produce highly realistic images, while multimodal large language models (MLLMs) are renowned for their proficiency in understanding and integrating multiple modalities. However, currently there is no straightforward and efficient framework to transfer the multimodal comprehension abilities of MLLMs to T2I models to enable them to understand multimodal inputs. In this paper, we propose the X2I framework, which endows Diffusion Transformer (DiT) models with the capability to comprehend various modalities, including multilingual text, screenshot documents, images, videos, and audio. X2I is trained using merely 100K English corpus with 160 GPU hours. Building on the DiT teacher model, we adopt an innovative distillation method to extract the inference capabilities of the teacher model and design a lightweight AlignNet structure to serve as an intermediate bridge. Compared to the teacher model, X2I shows a decrease in performance degradation of less than 1% while gaining various multimodal understanding abilities, including multilingual to image, image to image, image-text to image, video to image, audio to image, and utilizing creative fusion to enhance imagery. Furthermore, it is applicable for LoRA training in the context of image-text to image generation, filling a void in the industry in this area. We further design a simple LightControl to enhance the fidelity of instructional image editing. Finally, extensive experiments demonstrate the effectiveness, efficiency, multifunctionality, and transferability of our X2I. The open-source code and checkpoints for X2I can be found at the following link: <a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/X2I">https://github.com/OPPO-Mente-Lab/X2I</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹ä»¥å…¶ç”Ÿæˆé«˜åº¦é€¼çœŸçš„å›¾åƒçš„èƒ½åŠ›è€Œé—»åï¼Œè€Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åˆ™ä»¥å…¶åœ¨ç†è§£å’Œèåˆå¤šç§æ¨¡æ€æ–¹é¢çš„ç†Ÿç»ƒç¨‹åº¦è€Œé—»åã€‚ç„¶è€Œï¼Œç›®å‰å°šæ²¡æœ‰ç®€å•æœ‰æ•ˆçš„æ¡†æ¶èƒ½å¤Ÿå°†MLLMçš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›è½¬ç§»åˆ°T2Iæ¨¡å‹ä¸Šï¼Œä»¥ä¾¿ä½¿å…¶èƒ½å¤Ÿç†è§£å¤šæ¨¡æ€è¾“å…¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†X2Iæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èµ‹äºˆäº†Diffusion Transformerï¼ˆDiTï¼‰æ¨¡å‹ç†è§£å„ç§æ¨¡æ€çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤šè¯­è¨€æ–‡æœ¬ã€æˆªå›¾æ–‡æ¡£ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ã€‚X2Iä»…ä½¿ç”¨100Kè‹±æ–‡è¯­æ–™åº“å’Œ160 GPUå°æ—¶è¿›è¡Œè®­ç»ƒã€‚åŸºäºDiTæ•™å¸ˆæ¨¡å‹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åˆ›æ–°çš„çŸ¥è¯†è’¸é¦æ–¹æ³•æ¥æå–æ•™å¸ˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§çš„AlignNetç»“æ„ä½œä¸ºä¸­é—´æ¡¥æ¢ã€‚ä¸æ•™æ¨¡å‹ç›¸æ¯”ï¼ŒX2Içš„æ€§èƒ½ä¸‹é™ç¨‹åº¦ä¸åˆ°1%ï¼ŒåŒæ—¶è·å¾—äº†å¤šç§å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤šè¯­è¨€åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒã€å›¾åƒæ–‡æœ¬åˆ°å›¾åƒã€è§†é¢‘åˆ°å›¾åƒã€éŸ³é¢‘åˆ°å›¾åƒï¼Œå¹¶åˆ©ç”¨åˆ›æ„èåˆå¢å¼ºå›¾åƒã€‚æ­¤å¤–ï¼Œå®ƒé€‚ç”¨äºå›¾åƒæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„LoRAè®­ç»ƒï¼Œå¡«è¡¥äº†è¡Œä¸šç©ºç™½ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªç®€å•çš„LightControlæ¥å¢å¼ºæŒ‡ä»¤æ€§å›¾åƒç¼–è¾‘çš„ä¿çœŸåº¦ã€‚æœ€åï¼Œå¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„X2Içš„æœ‰æ•ˆæ€§ã€æ•ˆç‡ã€å¤šåŠŸèƒ½æ€§å’Œå¯è½¬ç§»æ€§ã€‚X2Içš„å¼€æºä»£ç å’Œæ£€æŸ¥ç‚¹å¯ä»¥åœ¨ä»¥ä¸‹é“¾æ¥æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/X2I%E3%80%82">https://github.com/OPPO-Mente-Lab/X2Iã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06134v2">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/X2I">https://github.com/OPPO-Mente-Lab/X2I</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºX2Içš„æ¡†æ¶ï¼Œå®ƒèµ‹äºˆDiffusion Transformerï¼ˆDiTï¼‰æ¨¡å‹ç†è§£å¤šç§æ¨¡å¼çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤šè¯­è¨€æ–‡æœ¬ã€æˆªå›¾æ–‡æ¡£ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ã€‚è¯¥æ¡†æ¶ä½¿ç”¨ä»…100Kçš„è‹±æ–‡è¯­æ–™åº“å’Œ160ä¸ªGPUå°æ—¶è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡åˆ›æ–°çš„è’¸é¦æ–¹æ³•å’Œè½»é‡çº§çš„AlignNetç»“æ„ï¼Œå®ç°äº†å¯¹å¤šç§æ¨¡æ€çš„æ·±å…¥ç†è§£ã€‚ä¸DiTæ•™å¸ˆæ¨¡å‹ç›¸æ¯”ï¼ŒX2Iåœ¨è·å¾—å¤šç§æ¨¡æ€ç†è§£èƒ½åŠ›çš„åŒæ—¶ï¼Œæ€§èƒ½ä¸‹é™ä¸åˆ°1%ï¼Œå¹¶å±•ç¤ºäº†åœ¨å„ç§æ¨¡æ€åˆ°å›¾åƒè½¬æ¢ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå®ƒé€‚ç”¨äºå›¾åƒæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„LoRAè®­ç»ƒï¼Œå¡«è¡¥äº†è¡Œä¸šç©ºç™½ã€‚å¹¶è®¾è®¡äº†ä¸€ä¸ªç®€å•çš„LightControlï¼Œä»¥æé«˜æŒ‡å¯¼å›¾åƒç¼–è¾‘çš„ä¿çœŸåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>X2Iæ¡†æ¶æˆåŠŸå°†Diffusion Transformeræ¨¡å‹æ‰©å±•ä¸ºå¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼Œæ¶µç›–å¤šç§è¾“å…¥æ¨¡å¼ï¼Œå¦‚å¤šè¯­è¨€æ–‡æœ¬ã€æˆªå›¾ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ã€‚</li>
<li>ä»…ä½¿ç”¨100Kçš„è‹±æ–‡è¯­æ–™åº“å’Œ160GPUå°æ—¶çš„è®­ç»ƒï¼Œå®ç°äº†é«˜æ•ˆçš„æ¨¡å‹è®­ç»ƒã€‚</li>
<li>é€šè¿‡åˆ›æ–°çš„è’¸é¦æ–¹æ³•å’Œè½»é‡çº§AlignNetç»“æ„ï¼Œæœ‰æ•ˆæå–æ•™å¸ˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä¸æ•™å¸ˆæ¨¡å‹ç›¸æ¯”ï¼ŒX2Iæ€§èƒ½é™ä½ä¸åˆ°1%ï¼Œåœ¨å¤šç§æ¨¡æ€åˆ°å›¾åƒè½¬æ¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>é€‚ç”¨äºå›¾åƒæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„LoRAè®­ç»ƒï¼Œå¡«è¡¥äº†è¡Œä¸šå†…çš„ç©ºç™½ã€‚</li>
<li>è®¾è®¡äº†LightControlï¼Œæé«˜äº†æŒ‡å¯¼å›¾åƒç¼–è¾‘çš„ä¿çœŸåº¦ã€‚</li>
<li>è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº†X2Içš„æœ‰æ•ˆæ€§ã€æ•ˆç‡ã€å¤šåŠŸèƒ½æ€§å’Œå¯è¿ç§»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-82691c6da1017990ccc6b7b8ae408a76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b3e3af48b03626df0feaccd1d6680432.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aafcc25b3b4a234332cc53cd5fe5d6b6.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="HybridNorm-Towards-Stable-and-Efficient-Transformer-Training-via-Hybrid-Normalization"><a href="#HybridNorm-Towards-Stable-and-Efficient-Transformer-Training-via-Hybrid-Normalization" class="headerlink" title="HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid   Normalization"></a>HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid   Normalization</h2><p><strong>Authors:Zhijian Zhuo, Yutao Zeng, Ya Wang, Sijun Zhang, Jian Yang, Xiaoqing Li, Xun Zhou, Jinwen Ma</strong></p>
<p>Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, challenges remain in training deep transformer networks, especially regarding the location of layer normalization. While Pre-Norm structures facilitate easier training due to their more prominent identity path, they often yield suboptimal performance compared to Post-Norm. In this paper, we propose $\textbf{HybridNorm}$, a straightforward yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. This design not only stabilizes training but also enhances performance, particularly in the context of LLMs. Comprehensive experiments in both dense and sparse architectures show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches, achieving state-of-the-art results across various benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/BryceZhuo/HybridNorm">https://github.com/BryceZhuo/HybridNorm</a>. </p>
<blockquote>
<p>Transformeræ¶æ„åœ¨ä¼—å¤šæœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­å·²æˆä¸ºå®é™…åº”ç”¨çš„æ ‡é…ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ã€‚å°½ç®¡å…¶æ€§èƒ½æ˜¾è‘—ï¼Œä½†åœ¨è®­ç»ƒæ·±åº¦Transformerç½‘ç»œæ—¶ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å…³äºå±‚å½’ä¸€åŒ–çš„ä½ç½®ã€‚è™½ç„¶Pre-Normç»“æ„ç”±äºå…¶æ›´çªå‡ºçš„èº«ä»½è·¯å¾„è€Œæ˜“äºè®­ç»ƒï¼Œä½†å®ƒé€šå¸¸äº§ç”Ÿçš„æ€§èƒ½ä¸å¦‚Post-Normã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†$\textbf{HybridNorm}$ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ··åˆå½’ä¸€åŒ–ç­–ç•¥ï¼Œå®ƒç»“åˆäº†Pre-Normå’ŒPost-Normæ–¹æ³•çš„ä¼˜åŠ¿ã€‚å…·ä½“æ¥è¯´ï¼ŒHybridNormåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­é‡‡ç”¨QKVå½’ä¸€åŒ–ï¼Œå¹¶åœ¨æ¯ä¸ªTransformerå—çš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ä¸­ä½¿ç”¨Post-Normã€‚è¿™ç§è®¾è®¡ä¸ä»…ç¨³å®šè®­ç»ƒï¼Œè¿˜æé«˜äº†æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒŒæ™¯ä¸‹ã€‚åœ¨å¯†é›†å’Œç¨€ç–æ¶æ„ä¸­çš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒHybridNormå§‹ç»ˆä¼˜äºPre-Normå’ŒPost-Normæ–¹æ³•ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å–å¾—æœ€æ–°ç»“æœã€‚è¿™äº›å‘ç°çªæ˜¾äº†HybridNormä½œä¸ºæ›´ç¨³å®šã€æ›´æœ‰æ•ˆçš„æŠ€æœ¯ï¼Œå¯ä»¥æ”¹å–„æ·±åº¦Transformeræ¨¡å‹çš„è®­ç»ƒå’Œæ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BryceZhuo/HybridNorm">https://github.com/BryceZhuo/HybridNorm</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04598v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºäº†HybridNormï¼Œä¸€ç§ç»“åˆäº†Pre-Normå’ŒPost-Normä¼˜åŠ¿çš„æ··åˆå½’ä¸€åŒ–ç­–ç•¥ã€‚åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­é‡‡ç”¨QKVå½’ä¸€åŒ–ï¼Œå¹¶åœ¨æ¯ä¸ªtransformerå—çš„feed-forwardç½‘ç»œï¼ˆFFNï¼‰ä¸­é‡‡ç”¨Post-Normã€‚è¿™ç§æ–¹æ³•æ—¢ç¨³å®šè®­ç»ƒï¼Œåˆæå‡æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒŒæ™¯ä¸‹ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒHybridNormåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºPre-Normå’ŒPost-Normæ–¹æ³•ï¼Œå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚HybridNormå…·æœ‰æ½œåŠ›æˆä¸ºæ”¹è¿›æ·±åº¦transformeræ¨¡å‹è®­ç»ƒå’Œæ€§èƒ½çš„æ›´ç¨³å®šã€æ›´æœ‰æ•ˆçš„æŠ€æœ¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Transformerså·²æˆä¸ºå¤šç§æœºå™¨å­¦ä¹ ä»»åŠ¡çš„é»˜è®¤æ¶æ„ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ã€‚</li>
<li>å°½ç®¡Transformerè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è®­ç»ƒæ·±åº¦ç½‘ç»œæ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå…¶ä¸­ä¹‹ä¸€æ˜¯å±‚å½’ä¸€åŒ–çš„ä½ç½®é—®é¢˜ã€‚</li>
<li>Pre-Normç»“æ„å› æ›´çªå‡ºçš„èº«ä»½è·¯å¾„è€Œæ˜“äºè®­ç»ƒï¼Œä½†ç›¸æ¯”Post-Normï¼Œå…¶æ€§èƒ½å¾€å¾€ä¸ä½³ã€‚</li>
<li>HybridNormç»“åˆäº†Pre-Normå’ŒPost-Normçš„ä¼˜ç‚¹ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ··åˆå½’ä¸€åŒ–ç­–ç•¥ã€‚</li>
<li>HybridNormåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­é‡‡ç”¨QKVå½’ä¸€åŒ–ï¼Œå¹¶åœ¨feed-forwardç½‘ç»œï¼ˆFFNï¼‰ä¸­ä½¿ç”¨Post-Normã€‚</li>
<li>ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒHybridNormåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œä¸ä»…ç¨³å®šè®­ç»ƒï¼Œè€Œä¸”æé«˜äº†æ€§èƒ½ã€‚</li>
<li>HybridNormå…·æœ‰æ½œåŠ›æˆä¸ºæ”¹è¿›æ·±åº¦transformeræ¨¡å‹è®­ç»ƒå’Œæ€§èƒ½çš„æœ‰æ•ˆæŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04598">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-706416057954bf8f87c96588957a814f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38768c5a780053b49471795c3bc0d1bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-265decdf333aa45e77cfaae4f052a9f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9aef4003dc529e26c3a9ea37f603a70.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-802e62de0b02ea4289c1066d35f3fee4.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-29/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-29/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-29/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7696588ea2e40da3db57c7071a79a81a.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-29  MemInsight Autonomous Memory Augmentation for LLM Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-29
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-29/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5d4020fd202d1674da42cb3789c92c23.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-29  Video-R1 Reinforcing Video Reasoning in MLLMs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-29
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24595.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
