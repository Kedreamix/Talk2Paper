<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-07-15  MIDI-VALLE Improving Expressive Piano Performance Synthesis Through   Neural Codec Language Modelling">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-91d1847feafaa6c1fd0fa7687a0d373e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    5.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    23 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-15-更新"><a href="#2025-07-15-更新" class="headerlink" title="2025-07-15 更新"></a>2025-07-15 更新</h1><h2 id="MIDI-VALLE-Improving-Expressive-Piano-Performance-Synthesis-Through-Neural-Codec-Language-Modelling"><a href="#MIDI-VALLE-Improving-Expressive-Piano-Performance-Synthesis-Through-Neural-Codec-Language-Modelling" class="headerlink" title="MIDI-VALLE: Improving Expressive Piano Performance Synthesis Through   Neural Codec Language Modelling"></a>MIDI-VALLE: Improving Expressive Piano Performance Synthesis Through   Neural Codec Language Modelling</h2><p><strong>Authors:Jingjing Tang, Xin Wang, Zhe Zhang, Junichi Yamagishi, Geraint Wiggins, George Fazekas</strong></p>
<p>Generating expressive audio performances from music scores requires models to capture both instrument acoustics and human interpretation. Traditional music performance synthesis pipelines follow a two-stage approach, first generating expressive performance MIDI from a score, then synthesising the MIDI into audio. However, the synthesis models often struggle to generalise across diverse MIDI sources, musical styles, and recording environments. To address these challenges, we propose MIDI-VALLE, a neural codec language model adapted from the VALLE framework, which was originally designed for zero-shot personalised text-to-speech (TTS) synthesis. For performance MIDI-to-audio synthesis, we improve the architecture to condition on a reference audio performance and its corresponding MIDI. Unlike previous TTS-based systems that rely on piano rolls, MIDI-VALLE encodes both MIDI and audio as discrete tokens, facilitating a more consistent and robust modelling of piano performances. Furthermore, the model’s generalisation ability is enhanced by training on an extensive and diverse piano performance dataset. Evaluation results show that MIDI-VALLE significantly outperforms a state-of-the-art baseline, achieving over 75% lower Frechet Audio Distance on the ATEPP and Maestro datasets. In the listening test, MIDI-VALLE received 202 votes compared to 58 for the baseline, demonstrating improved synthesis quality and generalisation across diverse performance MIDI inputs. </p>
<blockquote>
<p>从乐谱生成富有表现力的音频表演需要模型捕捉乐器的声学特性和人类解释。传统音乐表演合成管道遵循两阶段方法，首先根据乐谱生成富有表现力的性能MIDI，然后将MIDI合成音频。然而，合成模型往往难以在不同MIDI源、音乐风格和录音环境之间进行泛化。为了解决这些挑战，我们提出了MIDI-VALLE，这是一个基于VALLE框架的神经网络编解码器语言模型。VALLE框架最初是为零样本个性化文本到语音（TTS）合成而设计的。对于性能MIDI到音频合成，我们改进了架构，使其依赖于参考音频表演及其对应的MIDI。与之前的基于TTS的系统不同，MIDI-VALLE将MIDI和音频都编码为离散令牌，从而更一致、更稳健地建模钢琴演奏。此外，通过在广泛且多样的钢琴表演数据集上进行训练，增强了模型的泛化能力。评估结果表明，MIDI-VALLE显著优于最先进的基线模型，在ATEPP和Maestro数据集上的Frechet音频距离降低了75%以上。在聆听测试中，MIDI-VALLE获得了202票，而基线模型仅获得58票，这证明了其在合成质量和不同性能MIDI输入的泛化方面的改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08530v1">PDF</a> Accepted by ISMIR 2025</p>
<p><strong>Summary</strong>：</p>
<p>提出了一种基于神经网络的语言模型MIDI-VALLE，用于从乐谱生成表达性音频表演。该模型适应于VALLE框架，并改进了架构以参考音频表演和其对应的MIDI。MIDI-VALLE在性能MIDI到音频合成方面表现出卓越的效果，通过编码MIDI和音频作为离散符号，实现了更一致和稳健的钢琴表演建模。模型在大量和多样化的钢琴表演数据集上进行训练，增强了其泛化能力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>MIDI-VALLE是一个基于神经网络的语言模型，用于从音乐乐谱生成表达性音频表演。</li>
<li>该模型采用VALLE框架，并进行了改进，以参考音频表演和其对应的MIDI。</li>
<li>MIDI-VALLE通过将MIDI和音频编码为离散符号，实现了更一致和稳健的钢琴表演建模。</li>
<li>模型在广泛的多样化钢琴表演数据集上训练，增强了其泛化能力。</li>
<li>MIDI-VALLE显著优于现有技术基线，在ATEPP和Maestro数据集上的Frechet Audio Distance降低了75%以上。</li>
<li>听觉测试表明，MIDI-VALLE的合成质量和泛化能力得到了显著提高，获得了202票相对于基线的58票。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08530">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3c93c74d72de36bd1b8d363a7e3ae663.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-edcc70467f3a9f18b1ac61847e5017d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afd55da3c99e80e0f22508f2328f15b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80d49da0e4516c6cfac71a7a17c7c702.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91d1847feafaa6c1fd0fa7687a0d373e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6bae166c75251b2a218fa23012d2e6f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ILT-Iterative-LoRA-Training-through-Focus-Feedback-Fix-for-Multilingual-Speech-Recognition"><a href="#ILT-Iterative-LoRA-Training-through-Focus-Feedback-Fix-for-Multilingual-Speech-Recognition" class="headerlink" title="ILT-Iterative LoRA Training through Focus-Feedback-Fix for Multilingual   Speech Recognition"></a>ILT-Iterative LoRA Training through Focus-Feedback-Fix for Multilingual   Speech Recognition</h2><p><strong>Authors:Qingliang Meng, Hao Wu, Wei Liang, Wei Xu, Qing Zhao</strong></p>
<p>The deep integration of large language models and automatic speech recognition systems has become a promising research direction with high practical value. To address the overfitting issue commonly observed in Low-Rank Adaptation (LoRA) during the supervised fine-tuning (SFT) stage, this work proposes an innovative training paradigm Iterative LoRA Training (ILT) in combination with an Iterative Pseudo Labeling strategy, effectively enhancing the theoretical upper bound of model performance. Based on Whisper-large-v3 and Qwen2-Audio, we conduct systematic experiments using a three-stage training process: Focus Training, Feed Back Training, and Fix Training. Experimental results demonstrate the effectiveness of the proposed method. Furthermore, the MegaAIS research team applied this technique in the Interspeech 2025 Multilingual Conversational Speech Language Modeling Challenge (MLC-SLM), achieving 4th in Track 1 (Multilingual ASR Task) and 1st place in Track 2 (Speech Separation and Recognition Task), showcasing the practical feasibility and strong application potential of our approach. </p>
<blockquote>
<p>大规模语言模型和自动语音识别系统的深度融合已成为具有极高实用价值的研究方向。针对低秩适配（LoRA）在监督微调（SFT）阶段经常出现的过拟合问题，本研究提出了一种创新的训练范式——迭代LoRA训练（ILT），并结合迭代伪标签策略，有效地提高了模型性能的理论上限。基于Whisper-large-v3和Qwen2-Audio，我们采用三阶段训练过程进行系统实验：专注训练、反馈训练和固定训练。实验结果证明了该方法的有效性。此外，MegaAIS研究团队在Interspeech 2025多语种对话语音语言建模挑战赛（MLC-SLM）中应用了这一技术，在赛道1（多语种ASR任务）中获得第4名，赛道2（语音分离与识别任务）中获得第1名，展示了我们方法的实际可行性和强大的应用潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08477v1">PDF</a> Accepted By Interspeech 2025 MLC-SLM workshop as a Research Paper</p>
<p><strong>Summary</strong></p>
<p>大型语言模型与自动语音识别系统的深度整合已成为具有实际价值的研究方向。为解决低秩适配（LoRA）在监督微调（SFT）阶段常见的过拟合问题，本研究提出迭代低秩适配训练（ILT）结合迭代伪标签策略，有效提高了模型性能的理论上限。基于Whisper-large-v3和Qwen2-Audio，我们采用三阶段训练过程进行系统化实验：专注训练、反馈训练和固定训练。实验结果证明了该方法的有效性。此外，MegaAIS研究团队在Interspeech 2025多语言对话语音识别语言建模挑战赛（MLC-SLM）中应用此技术，分别在多语言ASR任务和语音分离与识别任务中取得第4名和第1名的成绩，展示了该方法的实际应用潜力和强大竞争力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型与自动语音识别系统的深度整合具有实际价值。</li>
<li>迭代低秩适配训练（ILT）结合迭代伪标签策略解决了低秩适配（LoRA）在监督微调阶段的过拟合问题。</li>
<li>三阶段训练过程包括专注训练、反馈训练和固定训练。</li>
<li>基于Whisper-large-v3和Qwen2-Audio的系统化实验证明了该方法的有效性。</li>
<li>MegaAIS研究团队在MLC-SLM比赛中取得了显著成绩，展示了该方法的实际应用潜力。</li>
<li>该方法在多语言ASR任务中取得了第4名的成绩。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08477">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-fa0a70581c20bf0808b15299a341648f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11e1811f371ee9727b9f328519c36bb8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab1099b778b1fa9609bb529706a7b6ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ee977f5e4157cbe18d7ae5740dc6abe.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RawTFNet-A-Lightweight-CNN-Architecture-for-Speech-Anti-spoofing"><a href="#RawTFNet-A-Lightweight-CNN-Architecture-for-Speech-Anti-spoofing" class="headerlink" title="RawTFNet: A Lightweight CNN Architecture for Speech Anti-spoofing"></a>RawTFNet: A Lightweight CNN Architecture for Speech Anti-spoofing</h2><p><strong>Authors:Yang Xiao, Ting Dang, Rohan Kumar Das</strong></p>
<p>Automatic speaker verification (ASV) systems are often affected by spoofing attacks. Recent transformer-based models have improved anti-spoofing performance by learning strong feature representations. However, these models usually need high computing power. To address this, we introduce RawTFNet, a lightweight CNN model designed for audio signals. The RawTFNet separates feature processing along time and frequency dimensions, which helps to capture the fine-grained details of synthetic speech. We tested RawTFNet on the ASVspoof 2021 LA and DF evaluation datasets. The results show that RawTFNet reaches comparable performance to that of the state-of-the-art models, while also using fewer computing resources. The code and models will be made publicly available. </p>
<blockquote>
<p>自动说话人验证（ASV）系统经常受到欺骗攻击的影响。最近基于Transformer的模型通过学习强大的特征表示，提高了防欺骗性能。然而，这些模型通常需要大量的计算能力。为了解决这一问题，我们引入了RawTFNet，这是一个为音频信号设计的轻量级CNN模型。RawTFNet在时间维度和频率维度上分离特征处理，有助于捕捉合成语音的精细细节。我们在ASVspoof 2021 LA和DF评估数据集上测试了RawTFNet。结果表明，RawTFNet的性能达到了最新模型的水平，同时使用的计算资源更少。代码和模型将公开发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08227v1">PDF</a> Submitted to APSIPA ASC 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了自动说话人验证（ASV）系统面临的新型欺骗攻击问题。为了改善对欺骗行为的防御能力并减少计算需求，提出了一个名为RawTFNet的轻量级CNN模型。该模型能够沿时间和频率维度分离特征处理，以捕捉合成语音的精细细节。在ASVspoof 2021 LA和DF评估数据集上的测试显示，RawTFNet与最先进的模型相比表现相当，同时使用的计算资源更少。代码和模型将公开发布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动说话人验证（ASV）系统易受欺骗攻击的影响。</li>
<li>最新基于Transformer的模型通过强大的特征表示提高了抗欺骗性能。</li>
<li>RawTFNet是一个轻量级的CNN模型，针对音频信号设计，旨在解决计算资源需求高的问题。</li>
<li>RawTFNet能够沿时间和频率维度分离特征处理，以捕捉合成语音的精细细节。</li>
<li>在ASVspoof 2021 LA和DF评估数据集上的测试显示，RawTFNet性能与最先进的模型相当。</li>
<li>RawTFNet使用较少的计算资源。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08227">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-67f3036ab2994f288b72e5665efb600b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47e23a3ce8958916e78152bd4534195b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-93f90270ea58cf2fdbf53a84cf1b51b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b3ecd41c2f65d168bc9731b36cb98ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-217b405c2ac5bb700f5ea2941d1ae499.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DARAS-Dynamic-Audio-Room-Acoustic-Synthesis-for-Blind-Room-Impulse-Response-Estimation"><a href="#DARAS-Dynamic-Audio-Room-Acoustic-Synthesis-for-Blind-Room-Impulse-Response-Estimation" class="headerlink" title="DARAS: Dynamic Audio-Room Acoustic Synthesis for Blind Room Impulse   Response Estimation"></a>DARAS: Dynamic Audio-Room Acoustic Synthesis for Blind Room Impulse   Response Estimation</h2><p><strong>Authors:Chunxi Wang, Maoshen Jia, Wenyu Jin</strong></p>
<p>Room Impulse Responses (RIRs) accurately characterize acoustic properties of indoor environments and play a crucial role in applications such as speech enhancement, speech recognition, and audio rendering in augmented reality (AR) and virtual reality (VR). Existing blind estimation methods struggle to achieve practical accuracy. To overcome this challenge, we propose the dynamic audio-room acoustic synthesis (DARAS) model, a novel deep learning framework that is explicitly designed for blind RIR estimation from monaural reverberant speech signals. First, a dedicated deep audio encoder effectively extracts relevant nonlinear latent space features. Second, the Mamba-based self-supervised blind room parameter estimation (MASS-BRPE) module, utilizing the efficient Mamba state space model (SSM), accurately estimates key room acoustic parameters and features. Third, the system incorporates a hybrid-path cross-attention feature fusion module, enhancing deep integration between audio and room acoustic features. Finally, our proposed dynamic acoustic tuning (DAT) decoder adaptively segments early reflections and late reverberation to improve the realism of synthesized RIRs. Experimental results, including a MUSHRA-based subjective listening study, demonstrate that DARAS substantially outperforms existing baseline models, providing a robust and effective solution for practical blind RIR estimation in real-world acoustic environments. </p>
<blockquote>
<p>室内环境的声学特性可以通过房间脉冲响应（RIRs）来准确表征，其在语音增强、语音识别以及增强现实（AR）和虚拟现实（VR）中的音频渲染等应用中发挥着至关重要的作用。现有的盲估计方法很难达到实用的准确度。为了应对这一挑战，我们提出了动态音频房间声学合成（DARAS）模型，这是一种专门为从单声道混响语音信号中盲估计RIRs而设计的新型深度学习框架。首先，专用的深度音频编码器有效地提取了相关的非线性潜在空间特征。其次，基于Mamba的自监督盲房间参数估计（MASS-BRPE）模块，利用高效的Mamba状态空间模型（SSM），准确估计了关键的房间声学参数和特征。第三，系统结合了一个混合路径交叉注意特征融合模块，增强了音频和房间声学特征之间的深度集成。最后，我们提出的动态声学调整（DAT）解码器自适应地分割早期反射和后期回响，提高了合成RIRs的真实性。包括基于MUSHRA的主观听觉研究在内的实验结果证明，DARAS在真实世界声学环境中显著优于现有基线模型，为实用盲RIR估计提供了稳健有效的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08135v1">PDF</a> 14 pages, 9 figures, submitted to IEEE&#x2F;ACM Transactions on Audio,   Speech, and Language Processing</p>
<p><strong>Summary</strong></p>
<p>室内环境声音特性的精准描述对于语音增强、语音识别以及增强现实（AR）和虚拟现实（VR）中的音频渲染等应用至关重要。现有盲估计方法难以达到实用精度。为此，我们提出了动态音频房间声学合成（DARAS）模型，这是一种专门设计用于从单声道混响语音信号中盲估计RIR的深度学习框架。它通过深度音频编码器提取非线性潜在空间特征，利用基于Mamba的SSM模块估计房间声学参数，融合音频与房间声学特征，并通过动态声学调音解码器提高合成RIR的真实感。实验结果表明，DARAS在真实世界声学环境中显著优于现有基线模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RIRs在室内环境声音特性表征中起关键作用，对语音增强和识别等应用至关重要。</li>
<li>现有盲估计方法存在实践中的准确性挑战。</li>
<li>DARAS模型是一个深度学习框架，旨在从混响的语音信号中盲估计RIR。</li>
<li>DARAS包含深度音频编码器、基于Mamba的SSM模块、特征融合模块和动态声学调音解码器。</li>
<li>深度音频编码器能有效提取非线性潜在空间特征。</li>
<li>基于Mamba的SSM模块用于准确估计关键房间声学参数。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08135">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2f46350f9f60ed11f1a6f552945584bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7eccd25906e072f0ade3804e976dfc24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb2a9eeb69047b6fc8f3d76477b9bba9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55dd36aeda90d58f8bb74d816e8c030f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Modele-physique-variationnel-pour-l’estimation-de-reponses-impulsionnelles-de-salles"><a href="#Modele-physique-variationnel-pour-l’estimation-de-reponses-impulsionnelles-de-salles" class="headerlink" title="Modèle physique variationnel pour l’estimation de réponses   impulsionnelles de salles"></a>Modèle physique variationnel pour l’estimation de réponses   impulsionnelles de salles</h2><p><strong>Authors:Louis Lalay, Mathieu Fontaine, Roland Badeau</strong></p>
<p>Room impulse response estimation is essential for tasks like speech dereverberation, which improves automatic speech recognition. Most existing methods rely on either statistical signal processing or deep neural networks designed to replicate signal processing principles. However, combining statistical and physical modeling for RIR estimation remains largely unexplored. This paper proposes a novel approach integrating both aspects through a theoretically grounded model. The RIR is decomposed into interpretable parameters: white Gaussian noise filtered by a frequency-dependent exponential decay (e.g. modeling wall absorption) and an autoregressive filter (e.g. modeling microphone response). A variational free-energy cost function enables practical parameter estimation. As a proof of concept, we show that given dry and reverberant speech signals, the proposed method outperforms classical deconvolution in noisy environments, as validated by objective metrics. </p>
<blockquote>
<p>房间脉冲响应（Room Impulse Response，简称RIR）估计是语音去混响任务的关键，该任务能提高自动语音识别效果。现有的大多数方法依赖于统计信号处理或模拟信号处理原理的深度神经网络。然而，将统计和物理建模结合用于RIR估计仍然是一个未被充分研究的领域。本文提出了一个结合这两方面理论的新方法。RIR被分解为可解释的参数：经过频率相关指数衰减（如模拟墙壁吸收）和白高斯噪声滤波的以及一个自回归滤波器（如模拟麦克风响应）。一个变分自由能代价函数实现了实际的参数估计。作为概念验证，我们展示了给定干燥和混响语音信号的情况下，在噪声环境中，所提出的方法在客观指标上的表现优于经典的去卷积方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08051v1">PDF</a> in French language. GRETSI, Aug 2025, Strasbourg (67000), France</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种结合统计和物理建模的房间脉冲响应（Room Impulse Response，RIR）估计新方法。该方法将RIR分解成可解释的参数，包括通过频率相关指数衰减模拟墙壁吸收和通过自回归滤波器模拟麦克风响应的白色高斯噪声。采用变分自由能成本函数实现参数估计。相较于经典去卷积方法，本文方法在噪声环境下表现更优。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文强调了房间脉冲响应估计在语音去混响任务中的重要性，该任务能提高自动语音识别效果。</li>
<li>现有方法主要依赖统计信号处理或深度神经网络来模拟信号处理原理，但结合统计和物理建模的RIR估计尚未得到充分研究。</li>
<li>本文提出了一种结合统计和物理建模的新方法，将RIR分解成可解释的参数，包括模拟墙壁吸收和麦克风响应的模型。</li>
<li>采用变分自由能成本函数进行参数估计，实现理论支撑和实践应用的有效结合。</li>
<li>与经典去卷积方法相比，本文方法在噪声环境下的表现更优越。</li>
<li>本文不仅提供了一个理论框架，还有具体的实验结果，通过客观度量验证了其性能优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08051">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c9e48079e2512a3d9af16d9b1095355f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3102a99b97345b570b82db859f4fa83c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1ded785586bd4b7f90d0ef013e2f559.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Riemannian-Time-Warping-Multiple-Sequence-Alignment-in-Curved-Spaces"><a href="#Riemannian-Time-Warping-Multiple-Sequence-Alignment-in-Curved-Spaces" class="headerlink" title="Riemannian Time Warping: Multiple Sequence Alignment in Curved Spaces"></a>Riemannian Time Warping: Multiple Sequence Alignment in Curved Spaces</h2><p><strong>Authors:Julian Richter, Christopher Erdös, Christian Scheurer, Jochen J. Steil, Niels Dehio</strong></p>
<p>Temporal alignment of multiple signals through time warping is crucial in many fields, such as classification within speech recognition or robot motion learning. Almost all related works are limited to data in Euclidean space. Although an attempt was made in 2011 to adapt this concept to unit quaternions, a general extension to Riemannian manifolds remains absent. Given its importance for numerous applications in robotics and beyond, we introduce Riemannian Time Warping (RTW). This novel approach efficiently aligns multiple signals by considering the geometric structure of the Riemannian manifold in which the data is embedded. Extensive experiments on synthetic and real-world data, including tests with an LBR iiwa robot, demonstrate that RTW consistently outperforms state-of-the-art baselines in both averaging and classification tasks. </p>
<blockquote>
<p>多信号的时间扭曲的时间对齐在多个领域（如语音识别或机器人运动学习中的分类）中至关重要。几乎所有相关工作都局限于欧几里得空间中的数据。尽管在2011年有人试图将此概念适应于单位四元数，但尚未将其推广到黎曼流形。考虑到其在机器人技术等多个领域的重要性，我们引入了黎曼时间扭曲（RTW）。这种方法通过考虑数据嵌入的黎曼流形的几何结构来有效地对齐多个信号。在合成数据和真实世界数据上的广泛实验以及对LBR iiwa机器人的测试表明，无论是在平均任务还是分类任务中，RTW始终优于最新的基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01635v2">PDF</a> </p>
<p><strong>总结</strong></p>
<p>本文主要介绍了针对黎曼流形上的时间序列数据的时间拉伸方法（Riemannian Time Warping，RTW）。考虑到数据内嵌的黎曼流形的几何结构，该方法有效地对齐多个信号，且在合成数据和真实世界数据上的大量实验以及对LBR iiwa机器人的测试均证明了其在平均和分类任务上的一致优秀表现。该方法的引入对机器人技术等领域具有重要意义。</p>
<p><strong>要点提炼</strong></p>
<ol>
<li>时间拉伸技术在许多领域如语音识别或机器人运动学习中至关重要。</li>
<li>目前相关工作大多局限于欧几里得空间的数据处理。</li>
<li>虽然已有尝试将时间拉伸概念适应于单位四元数，但其在黎曼流形上的通用扩展仍然缺失。</li>
<li>引入的Riemannian Time Warping（RTW）方法考虑到数据内嵌的黎曼流形的几何结构，有效对齐多个信号。</li>
<li>实验证明，RTW在平均和分类任务上均表现出卓越性能，且表现稳定。</li>
<li>RTW方法具有广泛的应用前景，特别是在机器人技术领域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01635">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2e5f74c62ee6bce420abda027b9d7faa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83ae0378cbf3da662aecbd7a75af4895.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19b4aef7943e0bf946ecfb0942eb60c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8edf569a39679f1d05b6f32c032f3104.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3b6a9ee7f143c78e36bedd816a53046.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-171135d0842e0bea48b3d3981853b3fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d612bf694e16f7e4fe76604d00050cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-850b38fd781563baf865fb4c81ce0712.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c3f2f01f840277aabb6cd8c43d49675.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="End-to-end-multi-channel-speaker-extraction-and-binaural-speech-synthesis"><a href="#End-to-end-multi-channel-speaker-extraction-and-binaural-speech-synthesis" class="headerlink" title="End-to-end multi-channel speaker extraction and binaural speech   synthesis"></a>End-to-end multi-channel speaker extraction and binaural speech   synthesis</h2><p><strong>Authors:Cheng Chi, Xiaoyu Li, Yuxuan Ke, Qunping Ni, Yao Ge, Xiaodong Li, Chengshi Zheng</strong></p>
<p>Speech clarity and spatial audio immersion are the two most critical factors in enhancing remote conferencing experiences. Existing methods are often limited: either due to the lack of spatial information when using only one microphone, or because their performance is highly dependent on the accuracy of direction-of-arrival estimation when using microphone array. To overcome this issue, we introduce an end-to-end deep learning framework that has the capacity of mapping multi-channel noisy and reverberant signals to clean and spatialized binaural speech directly. This framework unifies source extraction, noise suppression, and binaural rendering into one network. In this framework, a novel magnitude-weighted interaural level difference loss function is proposed that aims to improve the accuracy of spatial rendering. Extensive evaluations show that our method outperforms established baselines in terms of both speech quality and spatial fidelity. </p>
<blockquote>
<p>语音清晰度与空间音频沉浸感是提升远程会议体验的两个关键因素。现有方法往往存在局限性：有的因为只使用单个麦克风而缺乏空间信息；有的在面对噪音和使用回声信号时，其性能高度依赖于到达方向估计的准确性。为了克服这一问题，我们引入了一种端到端的深度学习框架，该框架具有将多通道噪音和回声信号直接映射为干净且空间化的双耳语音的能力。该框架将声源提取、降噪和双耳渲染集成到一个网络中。在该框架中，提出了一种新颖的幅度加权双耳水平差损失函数，旨在提高空间渲染的准确性。大量评估表明，我们的方法在语音质量和空间保真度方面都优于既定的基线方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.05739v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种端到端的深度学习框架，该框架能够将从多通道噪声和混响信号映射到干净且空间化的双耳语音。此框架融合了源提取、噪声抑制和双耳渲染，旨在提高远程会议体验的关键要素——语音清晰度和空间音频沉浸感。通过引入一种新的幅度加权耳间水平差损失函数，提高了空间渲染的准确性。实验评估表明，该方法在语音质量和空间保真度方面均优于现有基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音清晰度和空间音频沉浸感是增强远程会议体验的两个关键因素。</li>
<li>现有方法因只使用单一麦克风缺乏空间信息，或因使用麦克风阵列时方向到达估计的准确性而受限。</li>
<li>提出一种端到端的深度学习框架，整合源提取、噪声抑制和双耳渲染。</li>
<li>引入新的幅度加权耳间水平差损失函数，以提高空间渲染的准确性。</li>
<li>该框架能提高语音质量和空间保真度。</li>
<li>相比现有基线，此框架在多个方面的性能表现更优。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.05739">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2f128e0475f362abe26b1a5a2427cf86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b81546d22e7e63f29f9a28eb8ba66617.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df0e74fc052fdaa4a796d4eb8b9564c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06e65c06a7862cfa0083c49f9653cb62.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-15/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-15/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-15/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1747f79b835510c7748e17d50823c705.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-07-15  Image Translation with Kernel Prediction Networks for Semantic   Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-15/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1cf4f2ef235a1b3be34ad7fc00fb35f2.jpg" class="responsive-img" alt="人脸相关">
                        
                        <span class="card-title">人脸相关</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            人脸相关 方向最新论文已更新，请持续关注 Update in 2025-07-15  DArFace Deformation Aware Robustness for Low Quality Face Recognition
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    人脸相关
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">人脸相关</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">23827k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
