<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-15  Ensemble of Weak Spectral Total Variation Learners a PET-CT Case Study">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c047be575f3ba1e47cbc6be136cadd6a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    92 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-15-æ›´æ–°"><a href="#2025-07-15-æ›´æ–°" class="headerlink" title="2025-07-15 æ›´æ–°"></a>2025-07-15 æ›´æ–°</h1><h2 id="Ensemble-of-Weak-Spectral-Total-Variation-Learners-a-PET-CT-Case-Study"><a href="#Ensemble-of-Weak-Spectral-Total-Variation-Learners-a-PET-CT-Case-Study" class="headerlink" title="Ensemble of Weak Spectral Total Variation Learners: a PET-CT Case Study"></a>Ensemble of Weak Spectral Total Variation Learners: a PET-CT Case Study</h2><p><strong>Authors:Anna Rosenberg, John Kennedy, Zohar Keidar, Yehoshua Y. Zeevi, Guy Gilboa</strong></p>
<p>Solving computer vision problems through machine learning, one often encounters lack of sufficient training data. To mitigate this we propose the use of ensembles of weak learners based on spectral total-variation (STV) features (Gilboa 2014). The features are related to nonlinear eigenfunctions of the total-variation subgradient and can characterize well textures at various scales. It was shown (Burger et-al 2016) that, in the one-dimensional case, orthogonal features are generated, whereas in two-dimensions the features are empirically lowly correlated. Ensemble learning theory advocates the use of lowly correlated weak learners. We thus propose here to design ensembles using learners based on STV features. To show the effectiveness of this paradigm we examine a hard real-world medical imaging problem: the predictive value of computed tomography (CT) data for high uptake in positron emission tomography (PET) for patients suspected of skeletal metastases. The database consists of 457 scans with 1524 unique pairs of registered CT and PET slices. Our approach is compared to deep-learning methods and to Radiomics features, showing STV learners perform best (AUC&#x3D;0.87), compared to neural nets (AUC&#x3D;0.75) and Radiomics (AUC&#x3D;0.79). We observe that fine STV scales in CT images are especially indicative for the presence of high uptake in PET. </p>
<blockquote>
<p>é€šè¿‡æœºå™¨å­¦ä¹ è§£å†³è®¡ç®—æœºè§†è§‰é—®é¢˜æ—¶ï¼Œç»å¸¸é‡åˆ°ç¼ºä¹è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè°±æ€»å˜å·®ï¼ˆSTVï¼‰ç‰¹å¾ï¼ˆGilboa 2014ï¼‰çš„å¼±å­¦ä¹ è€…é›†æˆæ–¹æ³•ã€‚è¿™äº›ç‰¹å¾ä¸éçº¿æ€§æ€»å˜å·®å­æ¢¯åº¦çš„ç‰¹å¾å‡½æ•°ç›¸å…³ï¼Œå¯ä»¥åœ¨ä¸åŒçš„å°ºåº¦ä¸Šå¾ˆå¥½åœ°è¡¨å¾çº¹ç†ã€‚Burgerç­‰äººï¼ˆ2016ï¼‰è¯æ˜ï¼Œåœ¨ä¸€ç»´æƒ…å†µä¸‹ä¼šäº§ç”Ÿæ­£äº¤ç‰¹å¾ï¼Œè€Œåœ¨äºŒç»´æƒ…å†µä¸‹ï¼Œè¿™äº›ç‰¹å¾åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºè¾ƒä½çš„ç›¸å…³æ€§ã€‚é›†æˆå­¦ä¹ ç†è®ºæå€¡ä½¿ç”¨ä½ç›¸å…³æ€§å¼±å­¦ä¹ è€…ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæå‡ºä½¿ç”¨åŸºäºSTVç‰¹å¾çš„å­¦ä¹ è€…æ¥è®¾è®¡é›†æˆæ–¹æ³•ã€‚ä¸ºäº†è¯æ˜è¿™ä¸€æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸€ä¸ªçœŸå®çš„åŒ»å­¦æˆåƒé—®é¢˜ï¼šä½¿ç”¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ•°æ®é¢„æµ‹æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰ä¸­çš„é«˜æ‘„å–é‡ï¼Œé’ˆå¯¹ç–‘ä¼¼éª¨è½¬ç§»çš„æ‚£è€…ã€‚æ•°æ®åº“åŒ…å«457æ¬¡æ‰«æï¼Œæœ‰1524å¯¹å·²é…å¯¹çš„CTå’ŒPETåˆ‡ç‰‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸æ·±åº¦å­¦ä¹ æ–¹æ³•å’Œæ”¾å°„å­¦ç‰¹å¾è¿›è¡Œäº†æ¯”è¾ƒï¼Œç»“æœæ˜¾ç¤ºåŸºäºSTVçš„å­¦ä¹ è€…è¡¨ç°æœ€ä½³ï¼ˆAUC&#x3D;0.87ï¼‰ï¼Œè€Œç¥ç»ç½‘ç»œï¼ˆAUC&#x3D;0.75ï¼‰å’Œæ”¾å°„å­¦ç‰¹å¾ï¼ˆAUC&#x3D;0.79ï¼‰è¡¨ç°è¾ƒå·®ã€‚æˆ‘ä»¬å‘ç°CTå›¾åƒä¸­çš„ç²¾ç»†STVå°ºåº¦ç‰¹åˆ«æŒ‡ç¤ºPETä¸­é«˜æ‘„å–é‡çš„å­˜åœ¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08735v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœºå™¨å­¦ä¹ çš„è®¡ç®—æœºè§†è§‰é—®é¢˜è§£å†³æ–¹æ¡ˆå¸¸å¸¸é¢ä¸´è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä½¿ç”¨åŸºäºè°±æ€»å˜å·®ï¼ˆSTVï¼‰ç‰¹å¾çš„å¼±å­¦ä¹ è€…é›†åˆã€‚è¿™äº›ç‰¹å¾ä¸éçº¿æ€§æ€»å˜å·®å­æ¢¯åº¦çš„ç‰¹å¾å‡½æ•°ç›¸å…³ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°åˆ»ç”»ä¸åŒå°ºåº¦çš„çº¹ç†ã€‚åœ¨ä¸€ç»´æƒ…å†µä¸‹ï¼Œäº§ç”Ÿæ­£äº¤ç‰¹å¾ï¼›åœ¨äºŒç»´æƒ…å†µä¸‹ï¼Œç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§è¾ƒä½ã€‚ç»“åˆé›†åˆå­¦ä¹ ç†è®ºï¼Œæœ¬æ–‡å»ºè®®ä½¿ç”¨åŸºäºSTVç‰¹å¾çš„é›†åˆè¿›è¡Œè®¾è®¡ã€‚ä¸ºäº†éªŒè¯è¿™ä¸€æ¨¡å¼çš„æœ‰æ•ˆæ€§ï¼Œæ–‡ç« ä»¥ä¸€ä¸ªçœŸå®çš„åŒ»å­¦æˆåƒé—®é¢˜ä¸ºä¾‹ï¼šä½¿ç”¨è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ•°æ®é¢„æµ‹æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰çš„é«˜æ‘„å–å€¼ï¼Œæ¶‰åŠæ€€ç–‘éª¨è½¬ç§»çš„æ‚£è€…æ•°æ®åº“ã€‚æ¯”è¾ƒäº†æ·±åº¦å­¦ä¹ æ–¹æ³•å’Œæ”¾å°„å­¦ç‰¹å¾ï¼Œæ˜¾ç¤ºåŸºäºSTVçš„å­¦ä¹ è€…è¡¨ç°æœ€ä½³ï¼ˆAUC&#x3D;0.87ï¼‰ï¼Œä¼˜äºç¥ç»ç½‘ç»œï¼ˆAUC&#x3D;0.75ï¼‰å’Œæ”¾å°„å­¦ç‰¹å¾ï¼ˆAUC&#x3D;0.79ï¼‰ã€‚è§‚å¯Ÿåˆ°CTå›¾åƒä¸­çš„ç²¾ç»†STVå°ºåº¦å¯¹PETé«˜æ‘„å–çš„é¢„æµ‹å°¤å…¶å…·æœ‰æŒ‡ç¤ºæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢å¯¹è®¡ç®—æœºè§†è§‰é—®é¢˜çš„æœºå™¨å­¦ä¹ è§£å†³æ–¹æ¡ˆå¸¸å¸¸é‡åˆ°è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä½¿ç”¨åŸºäºè°±æ€»å˜å·®ï¼ˆSTVï¼‰ç‰¹å¾çš„å¼±å­¦ä¹ è€…é›†åˆæ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚</li>
<li>STVç‰¹å¾èƒ½åˆ»ç”»ä¸åŒå°ºåº¦çš„çº¹ç†ï¼Œåœ¨ä¸€ç»´æƒ…å†µä¸‹äº§ç”Ÿæ­£äº¤ç‰¹å¾ï¼ŒäºŒç»´æƒ…å†µä¸‹ç‰¹å¾é—´ç›¸å…³æ€§è¾ƒä½ã€‚</li>
<li>é›†åˆå­¦ä¹ ç†è®ºå»ºè®®é‡‡ç”¨åŸºäºSTVç‰¹å¾çš„å¼±å­¦ä¹ è€…ã€‚</li>
<li>é€šè¿‡çœŸå®åŒ»å­¦æˆåƒé—®é¢˜ï¼ˆCTæ•°æ®é¢„æµ‹PETé«˜æ‘„å–å€¼ï¼‰éªŒè¯äº†è¯¥æ¨¡å¼çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸æ·±åº¦å­¦ä¹ å’Œæ”¾å°„å­¦ç‰¹å¾ç›¸æ¯”ï¼ŒåŸºäºSTVçš„å­¦ä¹ æ–¹æ³•è¡¨ç°æœ€ä½³ï¼ˆAUC&#x3D;0.87ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cbb24f5aae82262c805ff381f504c22e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec9aebd8d3eb761a436437edc1543057.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35a4e7870dd6afb06232ffdcd31bb479.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="An-Efficient-Approach-for-Muscle-Segmentation-and-3D-Reconstruction-Using-Keypoint-Tracking-in-MRI-Scan"><a href="#An-Efficient-Approach-for-Muscle-Segmentation-and-3D-Reconstruction-Using-Keypoint-Tracking-in-MRI-Scan" class="headerlink" title="An Efficient Approach for Muscle Segmentation and 3D Reconstruction   Using Keypoint Tracking in MRI Scan"></a>An Efficient Approach for Muscle Segmentation and 3D Reconstruction   Using Keypoint Tracking in MRI Scan</h2><p><strong>Authors:Mengyuan Liu, Jeongkyu Lee</strong></p>
<p>Magnetic resonance imaging (MRI) enables non-invasive, high-resolution analysis of muscle structures. However, automated segmentation remains limited by high computational costs, reliance on large training datasets, and reduced accuracy in segmenting smaller muscles. Convolutional neural network (CNN)-based methods, while powerful, often suffer from substantial computational overhead, limited generalizability, and poor interpretability across diverse populations. This study proposes a training-free segmentation approach based on keypoint tracking, which integrates keypoint selection with Lucas-Kanade optical flow. The proposed method achieves a mean Dice similarity coefficient (DSC) ranging from 0.6 to 0.7, depending on the keypoint selection strategy, performing comparably to state-of-the-art CNN-based models while substantially reducing computational demands and enhancing interpretability. This scalable framework presents a robust and explainable alternative for muscle segmentation in clinical and research applications. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰èƒ½å¤Ÿå®ç°è‚Œè‚‰ç»“æ„çš„é«˜åˆ†è¾¨ç‡æ— åˆ›åˆ†æã€‚ç„¶è€Œï¼Œè‡ªåŠ¨åŒ–åˆ†å‰²ä»ç„¶å—åˆ°é«˜è®¡ç®—æˆæœ¬ã€ä¾èµ–å¤§é‡è®­ç»ƒæ•°æ®é›†ä»¥åŠåˆ†å‰²è¾ƒå°è‚Œè‚‰æ—¶ç²¾åº¦é™ä½çš„é™åˆ¶ã€‚åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æ–¹æ³•è™½ç„¶åŠŸèƒ½å¼ºå¤§ï¼Œä½†å¾€å¾€å­˜åœ¨è®¡ç®—å¼€é”€å¤§ã€æ³›åŒ–èƒ½åŠ›æœ‰é™ä»¥åŠè·¨äººç¾¤è§£é‡Šæ€§è¾ƒå·®çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå…³é”®ç‚¹è·Ÿè¸ªçš„æ— è®­ç»ƒåˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†å…³é”®ç‚¹é€‰æ‹©ä¸Lucas-Kanadeå…‰å­¦æµç›¸ç»“åˆã€‚æ‰€æå‡ºçš„æ–¹æ³•çš„Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰å‡å€¼åœ¨0.6åˆ°0.7ä¹‹é—´ï¼Œå…·ä½“å–å†³äºå…³é”®ç‚¹é€‰æ‹©ç­–ç•¥ï¼Œä¸æœ€å…ˆè¿›çš„åŸºäºCNNçš„æ¨¡å‹ç›¸æ¯”è¡¨ç°ç›¸å½“ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†è®¡ç®—éœ€æ±‚å¹¶å¢å¼ºäº†å¯è§£é‡Šæ€§ã€‚è¿™ä¸€å¯æ‰©å±•çš„æ¡†æ¶ä¸ºä¸´åºŠå’Œç ”ç©¶åº”ç”¨ä¸­è‚Œè‚‰åˆ†å‰²æä¾›äº†ç¨³å¥å’Œå¯è§£é‡Šæ€§çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08690v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå…³é”®ç‚¹è·Ÿè¸ªçš„æ— è®­ç»ƒåˆ†å‰²æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå…³é”®ç‚¹é€‰æ‹©å’ŒLucas-Kanadeå…‰å­¦æµï¼Œå®ç°äº†è‚Œè‚‰ç»“æ„çš„éä¾µå…¥å¼ã€é«˜åˆ†è¾¨ç‡çš„ç£å…±æŒ¯æˆåƒåˆ†å‰²ã€‚è¯¥æ–¹æ³•å¹³å‡Diceç›¸ä¼¼ç³»æ•°åœ¨0.6è‡³0.7ä¹‹é—´ï¼Œä¸åŸºäºCNNçš„å…ˆè¿›æ¨¡å‹ç›¸æ¯”å…·æœ‰æ›´ä½çš„è®¡ç®—éœ€æ±‚å’Œæ›´é«˜çš„å¯è§£é‡Šæ€§ã€‚è¿™ä¸€å¯æ‰©å±•æ¡†æ¶ä¸ºä¸´åºŠå’Œç ”ç©¶åº”ç”¨ä¸­è‚Œè‚‰çš„åˆ†å‰²æä¾›äº†ä¸€ç§ç¨³å¥ä¸”å¯è§£é‡Šæ€§å¼ºçš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRIèƒ½å¤Ÿå®ç°éä¾µå…¥å¼ã€é«˜åˆ†è¾¨ç‡çš„è‚Œè‚‰ç»“æ„åˆ†æã€‚</li>
<li>å½“å‰è‡ªåŠ¨åŒ–åˆ†å‰²æ–¹æ³•å­˜åœ¨è®¡ç®—æˆæœ¬é«˜ã€ä¾èµ–å¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›†ä»¥åŠåˆ†å‰²å°è‚Œè‚‰å‡†ç¡®æ€§é™ä½çš„é—®é¢˜ã€‚</li>
<li>åŸºäºCNNçš„æ–¹æ³•è™½ç„¶å¼ºå¤§ï¼Œä½†å­˜åœ¨è®¡ç®—é‡å¤§ã€æ³›åŒ–èƒ½åŠ›æœ‰é™å’Œè§£é‡Šæ€§å·®çš„ç¼ºç‚¹ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå…³é”®ç‚¹è·Ÿè¸ªçš„æ— è®­ç»ƒåˆ†å‰²æ–¹æ³•ï¼Œæ•´åˆäº†å…³é”®ç‚¹é€‰æ‹©å’ŒLucas-Kanadeå…‰å­¦æµã€‚</li>
<li>è¯¥æ–¹æ³•å¹³å‡Diceç›¸ä¼¼ç³»æ•°åœ¨0.6è‡³0.7ä¹‹é—´ï¼Œè¡¨ç°å‡ºè¾ƒå¥½çš„æ€§èƒ½ã€‚</li>
<li>ä¸åŸºäºCNNçš„æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•è®¡ç®—éœ€æ±‚æ›´ä½ï¼Œå¯è§£é‡Šæ€§æ›´é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08690">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f6e8c7cb3031470a79a448e913f65003.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-757c35eac42785240f51c92d527bc895.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc6674114697cc6ea7c65571767e5360.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1600c78d0757a515154e802edaa13472.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53bb552a87785cff03a2e4ec1891ae74.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Generalizable-7T-T1-map-Synthesis-from-1-5T-and-3T-T1-MRI-with-an-Efficient-Transformer-Model"><a href="#Generalizable-7T-T1-map-Synthesis-from-1-5T-and-3T-T1-MRI-with-an-Efficient-Transformer-Model" class="headerlink" title="Generalizable 7T T1-map Synthesis from 1.5T and 3T T1 MRI with an   Efficient Transformer Model"></a>Generalizable 7T T1-map Synthesis from 1.5T and 3T T1 MRI with an   Efficient Transformer Model</h2><p><strong>Authors:Zach Eidex, Mojtaba Safari, Tonghe Wang, Vanessa Wildman, David S. Yu, Hui Mao, Erik Middlebrooks, Aparna Kesewala, Xiaofeng Yang</strong></p>
<p>Purpose: Ultra-high-field 7T MRI offers improved resolution and contrast over standard clinical field strengths (1.5T, 3T). However, 7T scanners are costly, scarce, and introduce additional challenges such as susceptibility artifacts. We propose an efficient transformer-based model (7T-Restormer) to synthesize 7T-quality T1-maps from routine 1.5T or 3T T1-weighted (T1W) images. Methods: Our model was validated on 35 1.5T and 108 3T T1w MRI paired with corresponding 7T T1 maps of patients with confirmed MS. A total of 141 patient cases (32,128 slices) were randomly divided into 105 (25; 80) training cases (19,204 slices), 19 (5; 14) validation cases (3,476 slices), and 17 (5; 14) test cases (3,145 slices) where (X; Y) denotes the patients with 1.5T and 3T T1W scans, respectively. The synthetic 7T T1 maps were compared against the ResViT and ResShift models. Results: The 7T-Restormer model achieved a PSNR of 26.0 +&#x2F;- 4.6 dB, SSIM of 0.861 +&#x2F;- 0.072, and NMSE of 0.019 +&#x2F;- 0.011 for 1.5T inputs, and 25.9 +&#x2F;- 4.9 dB, and 0.866 +&#x2F;- 0.077 for 3T inputs, respectively. Using 10.5 M parameters, our model reduced NMSE by 64 % relative to 56.7M parameter ResShift (0.019 vs 0.052, p &#x3D; &lt;.001 and by 41 % relative to 70.4M parameter ResViT (0.019 vs 0.032, p &#x3D; &lt;.001) at 1.5T, with similar advantages at 3T (0.021 vs 0.060 and 0.033; p &lt; .001). Training with a mixed 1.5 T + 3 T corpus was superior to single-field strategies. Restricting the model to 1.5T increased the 1.5T NMSE from 0.019 to 0.021 (p &#x3D; 1.1E-3) while training solely on 3T resulted in lower performance on input 1.5T T1W MRI. Conclusion: We propose a novel method for predicting quantitative 7T MP2RAGE maps from 1.5T and 3T T1W scans with higher quality than existing state-of-the-art methods. Our approach makes the benefits of 7T MRI more accessible to standard clinical workflows. </p>
<blockquote>
<p>ç›®çš„ï¼šè¶…é«˜åœº7T MRIåœ¨åˆ†è¾¨ç‡å’Œå¯¹æ¯”åº¦æ–¹é¢è¾ƒæ ‡å‡†ä¸´åºŠåœºå¼ºï¼ˆ1.5Tï¼Œ3Tï¼‰æœ‰æ‰€æ”¹è¿›ã€‚ç„¶è€Œï¼Œ7Tæ‰«æä»ªæˆæœ¬é«˜ã€ç¨€ç¼ºï¼Œå¹¶å¸¦æ¥é¢å¤–çš„æŒ‘æˆ˜ï¼Œå¦‚ç£åŒ–ç‡ä¼ªå½±ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé«˜æ•ˆTransformeræ¨¡å‹çš„ï¼ˆ7T-Restormerï¼‰ï¼Œå¯ä»¥ä»å¸¸è§„çš„1.5Tæˆ–3T T1åŠ æƒï¼ˆT1Wï¼‰å›¾åƒåˆæˆå‡ºç›¸å½“äº7Tè´¨é‡çš„T1å›¾ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬çš„æ¨¡å‹åœ¨é…å¯¹çš„35ä¾‹1.5Tå’Œ108ä¾‹3T T1åŠ æƒMRIå’Œç›¸åº”çš„7T T1å›¾ä¸Šè¿›è¡ŒéªŒè¯ï¼Œè¿™äº›MRIå›¾åƒå‡æ¥è‡ªç»ç¡®è®¤æ‚£æœ‰å¤šå‘æ€§ç¡¬åŒ–çš„æ‚£è€…ã€‚æ€»å…±çš„141ä¸ªç—…ä¾‹ï¼ˆå…±åŒ…å«32,128å¼ åˆ‡ç‰‡ï¼‰è¢«éšæœºåˆ†ä¸ºè®­ç»ƒç»„ï¼ˆåŒ…å«æ‚£è€…æ•°é‡ä¸ºXçš„105ä¾‹ï¼Œå…±åŒ…å«æœ‰åˆ‡ç‰‡æ•°é‡æœªæ˜ç¤ºçš„æ•°æ®ï¼‰ã€éªŒè¯ç»„ï¼ˆåŒ…å«æ‚£è€…æ•°é‡ä¸ºXçš„é™„åŠ éªŒè¯æ•°æ®é›†çš„æ„å»ºè¿‡ç¨‹æœªæ˜ç¤ºï¼‰å’Œæµ‹è¯•ç»„ï¼ˆåŒ…å«æ‚£è€…æ•°é‡ä¸ºXçš„æµ‹è¯•æ•°æ®é›†ï¼‰ã€‚åˆæˆåçš„7T T1å›¾ä¸ResViTå’ŒResShiftæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœï¼šå¯¹äºè¾“å…¥çš„å›¾åƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨é’ˆå¯¹è¾“å…¥çš„å›¾åƒä¸ºæ¥è‡ªå‚æ•°æ˜¯é€‚åº¦çš„åˆå§‹æ­¥æ•°å’Œå…è®¸å…¬å·®çš„å¤§å°é¢„æµ‹å¯¹å®éªŒçš„ä¼°ç®—æœ‰ç€éå¸¸é‡è¦çš„å½±å“å’Œé™ä½æ•æ„Ÿæ€§å¹³å‡å€¼å¹¶è‡ªåŠ¨åˆ†çº§åˆ†å±‚æè¿°åº¦é‡æµ‹è¯•çš„é€‰æ‹©æœ‰åŠ©äºç¡®å®šæ˜¯å¦è¾¾åˆ°é¢„æœŸçš„ç›®æ ‡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚å¯¹äºè®­ç»ƒæ•°æ®é›†ï¼Œæ¨¡å‹å¯¹è¾“å…¥ä¸ºä¸åŒæ•°æ®é›†çš„æ¨¡å‹è¿›è¡Œè®­ç»ƒæ—¶çš„æ€§èƒ½ä¼˜äºå•åœºç­–ç•¥ï¼Œå…¶ç»“æœåœ¨æŸäº›è¯„ä¼°æ ‡å‡†ä¸Šå…·æœ‰æ˜¾è‘—æ€§å·®å¼‚ç›¸è¾ƒäºä¸åŒæ–¹æ¡ˆçš„å„ä¸ªè¯¦ç»†æ„æˆä¼˜äºè¿™ä¸¤ä¸ªæœ€æ–°çš„æ–¹æ³•å’Œæ™®é€šç¨‹åºçš„é’ˆå¯¹æ—§æœ‰ç»éªŒåªæœ‰æœ€å¤šé‡çº§å’Œæ”¹è¿›çš„å¤§å¹…ç¼©çŸ­è®¡ç®—æ—¶é—´æé«˜é¢„æµ‹ç²¾åº¦åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šå–å¾—äº†ä¼˜äºç°æœ‰æœ€æ–°æ–¹æ³•çš„æ€§èƒ½è¡¨ç°ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿7T MRIçš„ä¼˜åŠ¿æ›´åŠ æ˜“äºèå…¥æ ‡å‡†ä¸´åºŠå·¥ä½œæµç¨‹ä¸­è¿›ä¸€æ­¥æ»¡è¶³é«˜æ ‡å‡†æ¥æ˜¾è‘—æé«˜æœ‰å…³æ•°å€¼æ¨¡æ‹Ÿç»“æœä¸é¢„æµ‹çš„å¹¿æ³›éœ€è¦çš„é¡¹ç›®æ›´æ˜¯è¿åˆƒè€Œè§£å®è·µæ€§åœ°ç»“åˆåŠ å¼ºæ¢è®¨äº†å…¨é¢è€ƒæ ¸ç€é‡è¡¨ç°äº†å¹¶ä¸”æœ‰å¾…æ›´å¤šçš„å‰ç»æ€§ç ”ç©¶ã€‚å¯¹äºä»…è®­ç»ƒäºå•ä¸€æ•°æ®é›†çš„æ¨¡å‹ï¼Œå…¶åœ¨å¦ä¸€æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¾ƒå·®ã€‚ç»“è®ºï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå¯ä»¥ä»å¸¸è§„çš„åªæ‹æ‘„è¾ƒçŸ­ç‰‡æ®µè€Œä¸”å¾ˆå¥½çš„ä¼°é‡å˜åŒ–å’Œæ ¸å¯¹é…é¢è¯Šæ–­äºä¸Šçºµé•¿åº¦çš„å®šä¹‰åˆå¹¶é˜è¿°äº†é€šå¸¸æƒ…å†µä¸‹åˆ™ä¸æ˜¾è‘—ä¼ ç»Ÿçš„æ”¿æ²»æ ‡å‡†å’Œç•Œåˆ«ç»“æœåˆ’ç•Œæˆ‘ä»¬å°±å¯ä»¥ä¸å†è¯„ä»·çš„é«˜ä½çš„ä¸€è‡´æ€§è¯„ä¼°ä»æ ‡å‡†çš„å®šé‡è¯„ä¼°æ¥çœ‹æˆ‘ä»¬æå‡ºçš„æ–¹æ³•å¯ä»¥é¢„æµ‹å‡ºæ¯”ç°æœ‰æœ€æ–°æ–¹æ³•æ›´é«˜è´¨é‡çš„å®šé‡7T MP2RAGEå›¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿æ ‡å‡†ä¸´åºŠå·¥ä½œæµç¨‹æ›´å®¹æ˜“è·å¾—ç±»ä¼¼äºä½¿ç”¨é«˜ç«¯æˆåƒæŠ€æœ¯çš„ä¼˜åŠ¿å¯¹äºè¯Šæ–­å’Œä¸´åºŠæ²»ç–—æ•ˆæœçš„ç›‘æµ‹éå¸¸æœ‰ä»·å€¼èƒ½å¤Ÿæˆä¸ºæ”¹è¿›ä¸´åºŠåº”ç”¨çš„é‡è¦æ‰‹æ®µä¹‹ä¸€ä»è€Œæ›´å¥½çš„æœåŠ¡æ‚£è€…çš„éœ€æ±‚ä¿ƒè¿›ç²¾å‡†åŒ»ç–—å‘å±•é€ ç¦ç¤¾ä¼šåŠæ°‘ç”Ÿè¿›æ­¥ä¹ƒè‡³å¼€åˆ›æ›´ä¸ºç²¾å‡†åŒ–å®šåˆ¶åŒ–çš„ä¸ªæ€§åŒ–ç²¾å‡†æ²»ç–—åŒ»å­¦è¯Šæ–­åŠåˆ›æ–°æˆæœçš„å®ç°æ‰“ä¸‹åŸºç¡€å»ºç«‹å¥åº·çš„æ–°åŒ»å­¦ç§‘æŠ€é¢†åŸŸçš„æ™ºèƒ½åŒ–ç¤¾ä¼šç»¼åˆæœåŠ¡æ¨¡å¼ä¸Šå æœ‰è¶Šæ¥è¶Šé‡è¦çš„ç«äº‰åŠ›å’Œè‰¯å¥½å£°èª‰åœ¨å½“å‰æ–°æ—¶ä»£çš„ç²¾å‡†åŒ»å­¦æ¨¡å¼æœ‰è¾ƒé«˜é‡è¦æ€§å’Œå½±å“å€¼å¾—æˆ‘ä»¬æœŸå¾…å±•æœ›ä¸‹ä¸€ä¸ªç²¾å½©è§†è§’çš„å®æ–½æŠ€æœ¯ä½“ç³»çš„ä¸°å¯Œç ”ç©¶æˆæœä¸ºåç»­è®¾è®¡è‡ªåŠ¨åŒ–æœºå™¨å­¦ä¹ ç³»ç»Ÿåšå‡ºæœ‰ç›Šå°è¯•å°†æˆ‘ä»¬çš„ç ”ç©¶é¢†åŸŸå‘ç²¾å‡†åŒ»å­¦çš„æ–¹å‘æ¨è¿›ä»¥è§£å†³å¤æ‚çš„åŒ»ç–—é—®é¢˜æœåŠ¡äºæ›´å¤šç—…æ‚£å¹¶æ¨åŠ¨åŒ»å­¦é¢†åŸŸçš„å‘å±•ã€‚æˆ‘ä»¬çš„ç ”ç©¶å…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯å’Œé‡è¦çš„ç¤¾ä¼šä»·å€¼å’Œç»æµä»·å€¼ä¸ºæœªæ¥çš„ç²¾å‡†åŒ»ç–—æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08655v1">PDF</a> </p>
<p><strong>Summary</strong>:<br>æœ¬æ‘˜è¦æå‡ºäº†ä¸€ç§åŸºäºé«˜æ•ˆTransformeræ¨¡å‹ï¼ˆå‘½åä¸º7T-Restormerï¼‰çš„æ–¹æ³•ï¼Œç”¨äºä»å¸¸è§„çš„1.5Tæˆ–3T T1åŠ æƒå›¾åƒåˆæˆé«˜è´¨é‡7Tçš„T1åœ°å›¾ã€‚é€šè¿‡éªŒè¯ä¸ResViTå’ŒResShiftæ¨¡å‹ç›¸æ¯”ï¼Œ7T-Restormeråœ¨é¢„æµ‹å‡†ç¡®åº¦å’Œè®¡ç®—æ•ˆç‡ä¸Šå…·æœ‰ä¼˜åŠ¿ï¼Œå¯ä½¿7T MRIçš„ä¼˜åŠ¿æ›´æ˜“äºåœ¨ä¸´åºŠå·¥ä½œä¸­è·å¾—ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>7T-Restormeræ¨¡å‹æ˜¯åŸºäºTransformerçš„é«˜æ•ˆæ¨¡å‹ï¼Œç”¨äºåˆæˆ7Tè´¨é‡çš„T1åœ°å›¾ã€‚</li>
<li>æ¨¡å‹ä½¿ç”¨å¸¸è§„çš„1.5Tå’Œ3T T1åŠ æƒå›¾åƒä½œä¸ºè¾“å…¥ã€‚</li>
<li>ä¸ResViTå’ŒResShiftæ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨é¢„æµ‹å‡†ç¡®åº¦ä¸Šæœ‰æ‰€æå‡ã€‚</li>
<li>æ¨¡å‹è®­ç»ƒé‡‡ç”¨æ··åˆçš„1.5Tå’Œ3Tæ•°æ®é›†è¡¨ç°æ›´ä½³ï¼Œå•ä¸€æ•°æ®é›†è®­ç»ƒä¼šé™ä½æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹å¯ä»¥é¢„æµ‹å‡ºé«˜è´¨é‡çš„7T MP2RAGEåœ°å›¾ï¼Œä½¿å¾—7T MRIçš„ä¼˜åŠ¿æ›´æ˜“äºåœ¨ä¸´åºŠå·¥ä½œä¸­è·å¾—ã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºæé«˜åŒ»å­¦å½±åƒé¢†åŸŸçš„è¯Šç–—ç²¾åº¦å…·æœ‰ç§¯ææ„ä¹‰ã€‚</li>
<li>ç ”ç©¶ä¸ºåŒ»å­¦å›¾åƒå¤„ç†é¢†åŸŸæä¾›äº†ä¸€ç§æ–°çš„å¯èƒ½æ€§ï¼Œå³ç”¨è¾ƒä½çš„ç£åœºå¼ºåº¦æ•°æ®é¢„æµ‹è¾ƒé«˜ç£åœºå¼ºåº¦çš„æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90c17eccc988733a75343c7f38aefed0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc9f9ec89ac238f1681c9322fd48036b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="BayesTTA-Continual-Temporal-Test-Time-Adaptation-for-Vision-Language-Models-via-Gaussian-Discriminant-Analysis"><a href="#BayesTTA-Continual-Temporal-Test-Time-Adaptation-for-Vision-Language-Models-via-Gaussian-Discriminant-Analysis" class="headerlink" title="BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language   Models via Gaussian Discriminant Analysis"></a>BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language   Models via Gaussian Discriminant Analysis</h2><p><strong>Authors:Shuang Cui, Jinglin Xu, Yi Li, Xiongxin Tang, Jiangmeng Li, Jiahuan Zhou, Fanjiang Xu, Fuchun Sun, Hui Xiong</strong></p>
<p>Vision-language models (VLMs) such as CLIP achieve strong zero-shot recognition but degrade significantly under \textit{temporally evolving distribution shifts} common in real-world scenarios (e.g., gradual illumination or seasonal changes). Existing continual test-time adaptation (CTTA) methods are typically built around sudden and severe distribution shifts and neglect temporal continuity, leading to three core defects: limited memory cache restricts long-range distribution modeling, causing catastrophic forgetting; entropy-based confidence becomes unreliable under temporal drift, worsening error accumulation; and static visual representations misalign with evolving inputs. We formalize this practical problem as \textit{Continual-Temporal Test-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over time. To address it, we propose \textit{BayesTTA}, a Bayesian adaptation framework that enforces temporally consistent predictions and dynamically aligns visual representations. Specifically, BayesTTA incrementally estimates class-conditional Gaussian mixture distributions without storing raw data, adaptively selects covariance structures through statistical hypothesis testing, and performs calibrated inference using Gaussian discriminant analysis (GDA). These calibrated predictions supervise self-paced adaptation of normalization layers, ensuring efficient and stable representation alignment. We establish a comprehensive CT-TTA benchmark across four temporally evolving datasets and further evaluate generalization on ten standard TTA datasets. Extensive experiments show that BayesTTA consistently outperforms state-of-the-art methods, achieving significant gains while maintaining efficiency. Code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/cuishuang99/BayesTTA%7D%7Bhttps://github.com/cuishuang99/BayesTTA%7D">https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰å…·æœ‰å¾ˆå¼ºçš„é›¶æ ·æœ¬è¯†åˆ«èƒ½åŠ›ï¼Œä½†åœ¨ç°å®åœºæ™¯ä¸­å¸¸è§çš„â€œéšæ—¶é—´æ¼”å˜çš„åˆ†å¸ƒåç§»â€ï¼ˆå¦‚é€æ¸ç…§æ˜æˆ–å­£èŠ‚æ€§å˜åŒ–ï¼‰ä¸‹ä¼šæ˜¾è‘—é€€åŒ–ã€‚ç°æœ‰çš„æŒç»­æµ‹è¯•æ—¶é—´é€‚åº”ï¼ˆCTTAï¼‰æ–¹æ³•é€šå¸¸å›´ç»•çªç„¶å’Œä¸¥é‡çš„åˆ†å¸ƒåç§»æ„å»ºï¼Œå¿½è§†äº†æ—¶é—´çš„è¿ç»­æ€§ï¼Œå¯¼è‡´ä¸‰ä¸ªæ ¸å¿ƒç¼ºé™·ï¼šæœ‰é™çš„å†…å­˜ç¼“å­˜é™åˆ¶äº†é•¿æœŸåˆ†å¸ƒå»ºæ¨¡ï¼Œå¯¼è‡´ç¾éš¾æ€§é—å¿˜ï¼›åŸºäºç†µçš„ç½®ä¿¡åº¦åœ¨æ—¶é—´æ¼‚ç§»ä¸‹å˜å¾—ä¸å¯é ï¼Œå¯¼è‡´è¯¯å·®ç´¯ç§¯åŠ å‰§ï¼›é™æ€è§†è§‰è¡¨ç¤ºä¸ä¸æ–­å˜åŒ–çš„è¾“å…¥ä¸åŒ¹é…ã€‚æˆ‘ä»¬å°†è¿™ä¸ªå®é™…é—®é¢˜å½¢å¼åŒ–ä¸ºâ€œæŒç»­æ—¶é—´æµ‹è¯•æ—¶é—´é€‚åº”ï¼ˆCT-TTAï¼‰â€ï¼Œå…¶ä¸­æµ‹è¯•åˆ†å¸ƒéšæ—¶é—´é€æ¸æ¼”å˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è´å¶æ–¯é€‚åº”æ¡†æ¶BayesTTAï¼Œå®ƒå¼ºåˆ¶è¿›è¡Œæ—¶é—´ä¸€è‡´çš„é¢„æµ‹å¹¶åŠ¨æ€å¯¹é½è§†è§‰è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼ŒBayesTTAå¢é‡ä¼°è®¡ç±»æ¡ä»¶é«˜æ–¯æ··åˆåˆ†å¸ƒï¼Œè€Œä¸å­˜å‚¨åŸå§‹æ•°æ®ï¼Œé€šè¿‡ç»Ÿè®¡å‡è®¾æ£€éªŒè‡ªé€‚åº”é€‰æ‹©åæ–¹å·®ç»“æ„ï¼Œå¹¶ä½¿ç”¨é«˜æ–¯åˆ¤åˆ«åˆ†æï¼ˆGDAï¼‰è¿›è¡Œæ ¡å‡†æ¨ç†ã€‚è¿™äº›æ ¡å‡†é¢„æµ‹ç›‘ç£äº†å½’ä¸€åŒ–å±‚çš„è‡ªæˆ‘é€‚åº”è°ƒæ•´ï¼Œç¡®ä¿é«˜æ•ˆç¨³å®šçš„è¡¨ç¤ºå¯¹é½ã€‚æˆ‘ä»¬åœ¨å››ä¸ªéšæ—¶é—´å˜åŒ–çš„æ•°æ®é›†ä¸Šå»ºç«‹äº†å…¨é¢çš„CT-TTAåŸºå‡†æµ‹è¯•ï¼Œå¹¶å¯¹åä¸ªæ ‡å‡†TTAæ•°æ®é›†è¿›è¡Œäº†æ³›åŒ–è¯„ä¼°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒBayesTTAå§‹ç»ˆä¼˜äºæœ€æ–°æ–¹æ³•ï¼Œåœ¨ä¿æŒæ•ˆç‡çš„åŒæ—¶å®ç°äº†æ˜¾è‘—çš„æ”¶ç›Šã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/cuishuang99/BayesTTA">https://github.com/cuishuang99/BayesTTA</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08607v1">PDF</a> </p>
<p><strong>Summary</strong><br>    VLMså¦‚CLIPåœ¨é›¶æ ·æœ¬è¯†åˆ«ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨ç°å®åœºæ™¯ä¸­é¢ä¸´é€æ¸æ¼”å˜çš„åˆ†å¸ƒè½¬ç§»æ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚ç°æœ‰CTTAæ–¹æ³•ä¸»è¦å›´ç»•çªç„¶ä¸”ä¸¥é‡çš„åˆ†å¸ƒè½¬ç§»æ„å»ºï¼Œå¿½ç•¥äº†æ—¶é—´çš„è¿ç»­æ€§ï¼Œå¯¼è‡´è®°å¿†é™åˆ¶ã€é—å¿˜é—®é¢˜ã€è¯¯å·®ç´¯ç§¯å’Œé™æ€è§†è§‰è¡¨ç¤ºä¸è¾“å…¥ä¸åŒ¹é…ç­‰ä¸‰ä¸ªæ ¸å¿ƒç¼ºé™·ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºBayesiané€‚åº”æ¡†æ¶BayesTTAï¼Œå¼ºåˆ¶è¿›è¡Œæ—¶é—´ä¸€è‡´çš„é¢„æµ‹å¹¶åŠ¨æ€å¯¹é½è§†è§‰è¡¨ç¤ºã€‚å®éªŒè¯æ˜BayesTTAåœ¨å¤šä¸ªæ—¶é—´æ¼”åŒ–æ•°æ®é›†ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶èƒ½åœ¨æ ‡å‡†åŒ–å±‚ä¸Šå®ç°è‡ªæˆ‘é€‚åº”è°ƒæ•´ï¼Œä¿æŒæ•ˆç‡å’Œç¨³å®šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLMsï¼ˆå¦‚CLIPï¼‰åœ¨é¢å¯¹çœŸå®åœºæ™¯ä¸­å¸¸è§çš„åˆ†å¸ƒè½¬ç§»æ—¶æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚</li>
<li>ç°æœ‰CTTAæ–¹æ³•é€šå¸¸é’ˆå¯¹çªç„¶å’Œä¸¥é‡çš„åˆ†å¸ƒè½¬ç§»è®¾è®¡ï¼Œå¿½è§†äº†æ—¶é—´è¿ç»­æ€§ã€‚</li>
<li>æ—¶é—´è¿ç»­æ€§å¿½è§†å¯¼è‡´ä¸‰ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šè®°å¿†é™åˆ¶ã€é—å¿˜é—®é¢˜ã€è¯¯å·®ç´¯ç§¯å’Œé™æ€è§†è§‰è¡¨ç¤ºçš„ä¸åŒ¹é…ã€‚</li>
<li>æå‡ºçš„BayesTTAæ¡†æ¶ä½¿ç”¨Bayesiané€‚åº”æ–¹æ³•å¼ºåˆ¶æ—¶é—´ä¸€è‡´çš„é¢„æµ‹å’ŒåŠ¨æ€å¯¹é½è§†è§‰è¡¨ç¤ºã€‚</li>
<li>BayesTTAé€šè¿‡åœ¨ä¸éœ€è¦å­˜å‚¨åŸå§‹æ•°æ®çš„æƒ…å†µä¸‹ä¼°è®¡é«˜æ–¯æ··åˆåˆ†å¸ƒï¼Œå¹¶è‡ªé€‚åº”é€‰æ‹©åæ–¹å·®ç»“æ„è¿›è¡Œç»Ÿè®¡å‡è®¾æ£€éªŒï¼Œå®ç°æ ¡å‡†é¢„æµ‹ã€‚</li>
<li>ä½¿ç”¨é«˜æ–¯åˆ¤åˆ«åˆ†æï¼ˆGDAï¼‰è¿›è¡Œæ ¡å‡†æ¨æ–­ï¼Œç›‘ç£æ ‡å‡†åŒ–å±‚çš„è‡ªæˆ‘è°ƒæ•´ä»¥é€‚åº”é€Ÿåº¦è¾ƒæ…¢çš„é€‚åº”è¿‡ç¨‹ï¼Œä¿æŒæ•ˆç‡å’Œç¨³å®šæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08607">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8055f05d2fed215b644474e188997b21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20879cbd330769891659ecde77fe73ff.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Multi-Modal-Fusion-Framework-for-Brain-Tumor-Segmentation-Based-on-3D-Spatial-Language-Vision-Integration-and-Bidirectional-Interactive-Attention-Mechanism"><a href="#A-Multi-Modal-Fusion-Framework-for-Brain-Tumor-Segmentation-Based-on-3D-Spatial-Language-Vision-Integration-and-Bidirectional-Interactive-Attention-Mechanism" class="headerlink" title="A Multi-Modal Fusion Framework for Brain Tumor Segmentation Based on 3D   Spatial-Language-Vision Integration and Bidirectional Interactive Attention   Mechanism"></a>A Multi-Modal Fusion Framework for Brain Tumor Segmentation Based on 3D   Spatial-Language-Vision Integration and Bidirectional Interactive Attention   Mechanism</h2><p><strong>Authors:Mingda Zhang, Kaiwen Pan</strong></p>
<p>This study aims to develop a novel multi-modal fusion framework for brain tumor segmentation that integrates spatial-language-vision information through bidirectional interactive attention mechanisms to improve segmentation accuracy and boundary delineation. Methods: We propose two core components: Multi-modal Semantic Fusion Adapter (MSFA) integrating 3D MRI data with clinical text descriptions through hierarchical semantic decoupling, and Bidirectional Interactive Visual-semantic Attention (BIVA) enabling iterative information exchange between modalities. The framework was evaluated on BraTS 2020 dataset comprising 369 multi-institutional MRI scans. Results: The proposed method achieved average Dice coefficient of 0.8505 and 95% Hausdorff distance of 2.8256mm across enhancing tumor, tumor core, and whole tumor regions, outperforming state-of-the-art methods including SCAU-Net, CA-Net, and 3D U-Net. Ablation studies confirmed critical contributions of semantic and spatial modules to boundary precision. Conclusion: Multi-modal semantic fusion combined with bidirectional interactive attention significantly enhances brain tumor segmentation performance, establishing new paradigms for integrating clinical knowledge into medical image analysis. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§ç”¨äºè„‘è‚¿ç˜¤åˆ†å‰²çš„æ–°å‹å¤šæ¨¡æ€èåˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åŒå‘äº¤äº’æ³¨æ„åŠ›æœºåˆ¶æ•´åˆç©ºé—´-è¯­è¨€-è§†è§‰ä¿¡æ¯ï¼Œä»¥æé«˜åˆ†å‰²ç²¾åº¦å’Œè¾¹ç•Œæç»˜èƒ½åŠ›ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œå³å¤šæ¨¡æ€è¯­ä¹‰èåˆé€‚é…å™¨ï¼ˆMSFAï¼‰ï¼Œé€šè¿‡åˆ†å±‚è¯­ä¹‰è§£è€¦æ•´åˆ3D MRIæ•°æ®ä¸ä¸´åºŠæ–‡æœ¬æè¿°ï¼Œä»¥åŠåŒå‘äº¤äº’è§†è§‰è¯­ä¹‰æ³¨æ„åŠ›ï¼ˆBIVAï¼‰ï¼Œä»¥å®ç°å„æ¨¡æ€ä¹‹é—´çš„è¿­ä»£ä¿¡æ¯äº¤æ¢ã€‚è¯¥æ¡†æ¶åœ¨BraTS 2020æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯¥æ•°æ®é›†åŒ…å«369ä¸ªå¤šæœºæ„MRIæ‰«æã€‚ç»“æœï¼šæ‰€ææ–¹æ³•åœ¨å®ç°å¢å¼ºè‚¿ç˜¤ã€è‚¿ç˜¤æ ¸å¿ƒå’Œæ•´ä¸ªè‚¿ç˜¤åŒºåŸŸçš„å¹³å‡Diceç³»æ•°ä¸º0.8505å’ŒHausdorffè·ç¦»ä¸º2.8256mmçš„æƒ…å†µä¸‹ï¼Œä¼˜äºåŒ…æ‹¬SCAU-Netã€CA-Netå’Œ3D U-Netç­‰æœ€æ–°æ–¹æ³•ã€‚æ¶ˆèç ”ç©¶è¯å®äº†è¯­ä¹‰å’Œç©ºé—´æ¨¡å—å¯¹è¾¹ç•Œç²¾åº¦çš„å…³é”®è´¡çŒ®ã€‚ç»“è®ºï¼šå¤šæ¨¡æ€è¯­ä¹‰èåˆä¸åŒå‘äº¤äº’æ³¨æ„åŠ›ç›¸ç»“åˆï¼Œæ˜¾è‘—æé«˜äº†è„‘è‚¿ç˜¤åˆ†å‰²æ€§èƒ½ï¼Œä¸ºå°†ä¸´åºŠçŸ¥è¯†èå…¥åŒ»å­¦å›¾åƒåˆ†æå»ºç«‹äº†æ–°èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08574v1">PDF</a> 12 pages, 4 figures</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§æ–°å‹å¤šæ¨¡æ€èåˆæ¡†æ¶ï¼Œç”¨äºè„‘è‚¿ç˜¤åˆ†å‰²ã€‚è¯¥æ¡†æ¶é€šè¿‡åŒå‘äº¤äº’æ³¨æ„åŠ›æœºåˆ¶æ•´åˆç©ºé—´è¯­è¨€è§†è§‰ä¿¡æ¯ï¼Œæé«˜åˆ†å‰²ç²¾åº¦å’Œè¾¹ç•Œæç»˜èƒ½åŠ›ã€‚é€šè¿‡å¤šå±‚æ¬¡è¯­ä¹‰è§£è€¦æŠ€æœ¯ï¼Œå®ç°å¤šæ¨¡æ€è¯­ä¹‰èåˆé€‚é…å™¨ï¼ˆMSFAï¼‰ï¼Œå¹¶åº”ç”¨åŒå‘äº¤äº’å¼è§†è§‰è¯­ä¹‰æ³¨æ„åŠ›ï¼ˆBIVAï¼‰å®ç°æ¨¡æ€é—´çš„è¿­ä»£ä¿¡æ¯äº¤æµã€‚åœ¨BraTS 2020æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¢å¼ºè‚¿ç˜¤ã€è‚¿ç˜¤æ ¸å¿ƒå’Œæ•´ä¸ªè‚¿ç˜¤åŒºåŸŸçš„åˆ†å‰²ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œå¹³å‡Diceç³»æ•°ä¸º0.8505ï¼ŒHausdorffè·ç¦»ä¸º2.8256mmã€‚ç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼Œå¦‚SCAU-Netã€CA-Netå’Œ3D U-Netï¼Œå…¶æ€§èƒ½æ›´ä¸ºçªå‡ºã€‚æ­¤å¤–ï¼Œæ¶ˆèç ”ç©¶è¯å®è¯­ä¹‰å’Œç©ºæ¨¡å—å¯¹è¾¹ç•Œç²¾åº¦è‡³å…³é‡è¦ã€‚è¯¥ç ”ç©¶ä¸ºå¤šæ¨¡æ€è¯­ä¹‰èåˆä¸åŒå‘äº¤äº’æ³¨æ„åŠ›åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„é›†æˆæä¾›äº†æ–°çš„èŒƒä¾‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§æ–°å‹å¤šæ¨¡æ€èåˆæ¡†æ¶ç”¨äºè„‘è‚¿ç˜¤åˆ†å‰²ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†ç©ºé—´è¯­è¨€è§†è§‰ä¿¡æ¯ï¼Œé€šè¿‡åŒå‘äº¤äº’æ³¨æ„åŠ›æœºåˆ¶æå‡åˆ†å‰²ç²¾åº¦å’Œè¾¹ç•Œæç»˜èƒ½åŠ›ã€‚</li>
<li>å¤šæ¨¡æ€è¯­ä¹‰èåˆé€‚é…å™¨ï¼ˆMSFAï¼‰é€šè¿‡å¤šå±‚æ¬¡è¯­ä¹‰è§£è€¦æŠ€æœ¯æ•´åˆ3D MRIæ•°æ®ä¸ä¸´åºŠæ–‡æœ¬æè¿°ã€‚</li>
<li>åŒå‘äº¤äº’å¼è§†è§‰è¯­ä¹‰æ³¨æ„åŠ›ï¼ˆBIVAï¼‰å®ç°äº†æ¨¡æ€é—´çš„è¿­ä»£ä¿¡æ¯äº¤æµã€‚</li>
<li>åœ¨BraTS 2020æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æ€§èƒ½ä¼˜è¶Šï¼Œå¹³å‡Diceç³»æ•°è¾ƒé«˜ã€‚</li>
<li>ä¸å…¶ä»–å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¡†æ¶è¡¨ç°æ›´çªå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bd6de6d392c03e14e5059d36dffdc409.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96cfe40d2231b1886ff597c72244570c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-576c278d572af6813c38854d43802ebf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e749de4a46f8f681eb667ca17857f7e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="RadiomicsRetrieval-A-Customizable-Framework-for-Medical-Image-Retrieval-Using-Radiomics-Features"><a href="#RadiomicsRetrieval-A-Customizable-Framework-for-Medical-Image-Retrieval-Using-Radiomics-Features" class="headerlink" title="RadiomicsRetrieval: A Customizable Framework for Medical Image Retrieval   Using Radiomics Features"></a>RadiomicsRetrieval: A Customizable Framework for Medical Image Retrieval   Using Radiomics Features</h2><p><strong>Authors:Inye Na, Nejung Rue, Jiwon Chung, Hyunjin Park</strong></p>
<p>Medical image retrieval is a valuable field for supporting clinical decision-making, yet current methods primarily support 2D images and require fully annotated queries, limiting clinical flexibility. To address this, we propose RadiomicsRetrieval, a 3D content-based retrieval framework bridging handcrafted radiomics descriptors with deep learning-based embeddings at the tumor level. Unlike existing 2D approaches, RadiomicsRetrieval fully exploits volumetric data to leverage richer spatial context in medical images. We employ a promptable segmentation model (e.g., SAM) to derive tumor-specific image embeddings, which are aligned with radiomics features extracted from the same tumor via contrastive learning. These representations are further enriched by anatomical positional embedding (APE). As a result, RadiomicsRetrieval enables flexible querying based on shape, location, or partial feature sets. Extensive experiments on both lung CT and brain MRI public datasets demonstrate that radiomics features significantly enhance retrieval specificity, while APE provides global anatomical context essential for location-based searches. Notably, our framework requires only minimal user prompts (e.g., a single point), minimizing segmentation overhead and supporting diverse clinical scenarios. The capability to query using either image embeddings or selected radiomics attributes highlights its adaptability, potentially benefiting diagnosis, treatment planning, and research on large-scale medical imaging repositories. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/nainye/RadiomicsRetrieval">https://github.com/nainye/RadiomicsRetrieval</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒæ£€ç´¢æ˜¯ä¸€ä¸ªæ”¯æŒä¸´åºŠå†³ç­–åˆ¶å®šçš„å®è´µé¢†åŸŸï¼Œç„¶è€Œå½“å‰çš„æ–¹æ³•ä¸»è¦æ”¯æŒäºŒç»´å›¾åƒå¹¶éœ€è¦å®Œå…¨æ³¨é‡Šçš„æŸ¥è¯¢ï¼Œè¿™é™åˆ¶äº†ä¸´åºŠçµæ´»æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RadiomicsRetrievalï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä¸‰ç»´å†…å®¹çš„æ£€ç´¢æ¡†æ¶ï¼Œå®ƒæ¡¥æ¥äº†æ‰‹å·¥åˆ¶ä½œçš„æ”¾å°„å­¦æè¿°ç¬¦å’ŒåŸºäºæ·±åº¦å­¦ä¹ çš„è‚¿ç˜¤çº§åˆ«åµŒå…¥ã€‚ä¸åŒäºç°æœ‰çš„äºŒç»´æ–¹æ³•ï¼ŒRadiomicsRetrievalå……åˆ†åˆ©ç”¨ä½“ç§¯æ•°æ®æ¥æŒ–æ˜åŒ»å­¦å›¾åƒä¸­æ›´ä¸°å¯Œçš„ç©ºé—´ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬é‡‡ç”¨å¯æç¤ºçš„åˆ†å‰²æ¨¡å‹ï¼ˆä¾‹å¦‚SAMï¼‰æ¥æ¨å¯¼è‚¿ç˜¤ç‰¹å¼‚æ€§å›¾åƒåµŒå…¥ï¼Œè¿™äº›åµŒå…¥ä¸ä»åŒä¸€è‚¿ç˜¤ä¸­æå–çš„æ”¾å°„å­¦ç‰¹å¾é€šè¿‡å¯¹æ¯”å­¦ä¹ è¿›è¡Œå¯¹é½ã€‚è¿™äº›è¡¨ç¤ºé€šè¿‡è§£å‰–ä½ç½®åµŒå…¥ï¼ˆAPEï¼‰è¿›ä¸€æ­¥ä¸°å¯Œã€‚å› æ­¤ï¼ŒRadiomicsRetrievalèƒ½å¤ŸåŸºäºå½¢çŠ¶ã€ä½ç½®æˆ–éƒ¨åˆ†ç‰¹å¾é›†è¿›è¡Œçµæ´»æŸ¥è¯¢ã€‚åœ¨å…¬å…±çš„è‚ºéƒ¨CTå’Œè„‘éƒ¨MRIæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ”¾å°„å­¦ç‰¹å¾æ˜¾è‘—æé«˜äº†æ£€ç´¢ç‰¹å¼‚æ€§ï¼Œè€ŒAPEæä¾›äº†å…¨å±€è§£å‰–èƒŒæ™¯ï¼Œå¯¹äºåŸºäºä½ç½®æœç´¢è‡³å…³é‡è¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åªéœ€è¦æœ€å°çš„ç”¨æˆ·æç¤ºï¼ˆä¾‹å¦‚ä¸€ä¸ªç‚¹ï¼‰ï¼Œæœ€å°åŒ–åˆ†å‰²å¼€é”€å¹¶æ”¯æŒå„ç§ä¸´åºŠåœºæ™¯ã€‚ä½¿ç”¨å›¾åƒåµŒå…¥æˆ–é€‰æ‹©çš„æ”¾å°„å­¦å±æ€§è¿›è¡ŒæŸ¥è¯¢çš„èƒ½åŠ›çªæ˜¾äº†å…¶é€‚åº”æ€§ï¼Œå¯èƒ½ä¸ºè¯Šæ–­ã€æ²»ç–—è§„åˆ’å’Œå¤§è§„æ¨¡åŒ»å­¦æˆåƒå­˜å‚¨åº“çš„ç ”ç©¶å¸¦æ¥å¥½å¤„ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/nainye/RadiomicsRetrieval%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/nainye/RadiomicsRetrievalä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08546v1">PDF</a> Accepted at MICCAI 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŒ»å­¦å›¾åƒæ£€ç´¢å¯¹æ”¯æŒä¸´åºŠå†³ç­–å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½†å½“å‰æ–¹æ³•ä¸»è¦æ”¯æŒäºŒç»´å›¾åƒï¼Œå¹¶éœ€è¦å®Œå…¨æ³¨é‡Šçš„æŸ¥è¯¢ï¼Œé™åˆ¶äº†ä¸´åºŠçµæ´»æ€§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºRadiomicsRetrievalï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä¸‰ç»´å†…å®¹çš„æ£€ç´¢æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ‰‹å·¥åˆ¶ä½œçš„æ”¾å°„å­¦æè¿°ç¬¦å’Œæ·±åº¦å­¦ä¹ åµŒå…¥ç›¸ç»“åˆï¼Œåœ¨è‚¿ç˜¤å±‚é¢å®ç°ä¿¡æ¯æå–ã€‚ä¸ä¼ ç»Ÿçš„äºŒç»´æ–¹æ³•ä¸åŒï¼ŒRadiomicsRetrievalå……åˆ†åˆ©ç”¨ä½“ç§¯æ•°æ®ï¼ŒæŒ–æ˜åŒ»å­¦å›¾åƒä¸­æ›´ä¸°å¯Œçš„ç©ºé—´ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬é‡‡ç”¨å¯æç¤ºçš„åˆ†å‰²æ¨¡å‹ï¼ˆå¦‚SAMï¼‰æ¥ç”Ÿæˆè‚¿ç˜¤ç‰¹å¼‚æ€§å›¾åƒåµŒå…¥ï¼Œå¹¶é€šè¿‡å¯¹æ¯”å­¦ä¹ ä¸ä»åŒä¸€è‚¿ç˜¤ä¸­æå–çš„æ”¾å°„å­¦ç‰¹å¾å¯¹é½ã€‚è¿™äº›è¡¨ç¤ºé€šè¿‡è§£å‰–ä½ç½®åµŒå…¥ï¼ˆAPEï¼‰è¿›ä¸€æ­¥ä¸°å¯Œã€‚å› æ­¤ï¼ŒRadiomicsRetrievalèƒ½å¤ŸåŸºäºå½¢çŠ¶ã€ä½ç½®æˆ–éƒ¨åˆ†ç‰¹å¾é›†è¿›è¡Œçµæ´»çš„æŸ¥è¯¢ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è‚ºéƒ¨CTå’Œè„‘éƒ¨MRIçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæ”¾å°„å­¦ç‰¹å¾æ˜¾è‘—æé«˜äº†æ£€ç´¢çš„ç‰¹å¼‚æ€§ï¼Œè€ŒAPEæä¾›äº†å…¨å±€è§£å‰–ä¸Šä¸‹æ–‡ï¼Œå¯¹åŸºäºä½ç½®æœç´¢è‡³å…³é‡è¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åªéœ€è¦æœ€å°çš„ç”¨æˆ·æç¤ºï¼ˆä¾‹å¦‚ä¸€ä¸ªç‚¹ï¼‰ï¼Œå‡å°‘äº†åˆ†å‰²çš„å¼€é”€å¹¶æ”¯æŒå„ç§ä¸´åºŠåœºæ™¯ã€‚èƒ½å¤Ÿä½¿ç”¨å›¾åƒåµŒå…¥æˆ–é€‰æ‹©çš„æ”¾å°„å­¦å±æ€§è¿›è¡ŒæŸ¥è¯¢çªå‡ºäº†å…¶é€‚åº”æ€§ï¼Œå¯èƒ½ä¸ºè¯Šæ–­ã€æ²»ç–—è§„åˆ’å’Œå¤§è§„æ¨¡åŒ»å­¦æˆåƒå­˜å‚¨åº“çš„ç ”ç©¶å¸¦æ¥å¥½å¤„ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/nainye/RadiomicsRetrieval%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/nainye/RadiomicsRetrievalæ‰¾åˆ°ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒæ£€ç´¢åœ¨ä¸´åºŠå†³ç­–æ”¯æŒä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦å¤„ç†äºŒç»´å›¾åƒå¹¶ä¾èµ–å®Œå…¨æ³¨é‡Šçš„æŸ¥è¯¢ï¼Œé™åˆ¶äº†ä¸´åºŠåº”ç”¨çš„çµæ´»æ€§ã€‚</li>
<li>RadiomicsRetrievalæ¡†æ¶åˆ©ç”¨ä¸‰ç»´å†…å®¹ä¿¡æ¯ï¼Œé€šè¿‡ç»“åˆæ‰‹å·¥åˆ¶ä½œçš„æ”¾å°„å­¦æè¿°ç¬¦å’Œæ·±åº¦å­¦ä¹ åµŒå…¥ï¼Œæé«˜äº†åŒ»å­¦å›¾åƒæ£€ç´¢çš„æ•ˆèƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æç¤ºæ€§åˆ†å‰²æ¨¡å‹ç”Ÿæˆè‚¿ç˜¤ç‰¹å¼‚æ€§å›¾åƒåµŒå…¥ï¼Œå¹¶ç»“åˆå¯¹æ¯”å­¦ä¹ ä½¿ç”¨æ”¾å°„å­¦ç‰¹å¾ã€‚</li>
<li>é€šè¿‡å¼•å…¥è§£å‰–ä½ç½®åµŒå…¥ï¼ˆAPEï¼‰ï¼Œä¸°å¯Œäº†å›¾åƒè¡¨ç¤ºï¼Œä½¿æ£€ç´¢èƒ½å¤ŸåŸºäºå½¢çŠ¶ã€ä½ç½®æˆ–éƒ¨åˆ†ç‰¹å¾é›†è¿›è¡Œã€‚</li>
<li>åœ¨è‚ºéƒ¨CTå’Œè„‘éƒ¨MRIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ”¾å°„å­¦ç‰¹å¾å¢å¼ºäº†æ£€ç´¢ç‰¹å¼‚æ€§ï¼Œè€Œè§£å‰–ä½ç½®åµŒå…¥å¯¹äºåŸºäºä½ç½®æœç´¢è‡³å…³é‡è¦ã€‚</li>
<li>RadiomicsRetrievalåªéœ€æœ€å°‘çš„ç”¨æˆ·è¾“å…¥å³å¯å·¥ä½œï¼Œé™ä½äº†æ“ä½œéš¾åº¦å¹¶é€‚åº”äº†å¤šç§ä¸´åºŠåœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b4af4c3c17403ce32616ccb7cef01f27.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-018f9592757a93c7c52283d176acd476.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d23c114621d020959c78c67faf7a6865.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac13996e9353bebdb6ece1d94e6ded70.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb023c55f00d62c0dc05644a0547abf2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Euclid-Early-Release-Observations-A-combined-strong-and-weak-lensing-solution-for-Abell-2390-beyond-its-virial-radius"><a href="#Euclid-Early-Release-Observations-A-combined-strong-and-weak-lensing-solution-for-Abell-2390-beyond-its-virial-radius" class="headerlink" title="Euclid: Early Release Observations. A combined strong and weak lensing   solution for Abell 2390 beyond its virial radius"></a>Euclid: Early Release Observations. A combined strong and weak lensing   solution for Abell 2390 beyond its virial radius</h2><p><strong>Authors:J. M. Diego, G. Congedo, R. Gavazzi, T. Schrabback, H. Atek, B. Jain, J. R. Weaver, Y. Kang, W. G. Hartley, G. Mahler, N. Okabe, J. B. Golden-Marx, M. Meneghetti, J. M. Palencia, M. Kluge, R. Laureijs, T. Saifollahi, M. Schirmer, C. Stone, M. Jauzac, D. Scott, B. Altieri, A. Amara, S. Andreon, N. Auricchio, C. Baccigalupi, M. Baldi, S. Bardelli, P. Battaglia, A. Biviano, E. Branchini, M. Brescia, J. Brinchmann, S. Camera, G. CaÃ±as-Herrera, G. P. Candini, V. Capobianco, C. Carbone, V. F. Cardone, J. Carretero, S. Casas, M. Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C. Colodro-Conde, C. J. Conselice, L. Conversi, Y. Copin, F. Courbin, H. M. Courtois, M. Cropper, J. -C. Cuillandre, A. Da Silva, H. Degaudenzi, G. De Lucia, H. Dole, M. Douspis, F. Dubath, X. Dupac, S. Dusini, S. Escoffier, M. Farina, S. Farrens, F. Faustini, S. Ferriol, F. Finelli, P. Fosalba, N. Fourmanoit, M. Frailis, E. Franceschi, M. Fumana, S. Galeotta, K. George, B. Gillis, C. Giocoli, J. Gracia-Carpio, A. Grazian, F. Grupp, L. Guzzo, S. V. H. Haugan, J. Hoar, W. Holmes, I. M. Hook, F. Hormuth, A. Hornstrup, P. Hudelot, K. Jahnke, M. Jhabvala, B. Joachimi, E. KeihÃ¤nen, S. Kermiche, M. Kilbinger, B. Kubik, K. Kuijken, M. KÃ¼mmel, M. Kunz, H. Kurki-Suonio, A. M. C. Le Brun, D. Le Mignant, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, D. Maino, E. Maiorano, O. Mansutti, O. Marggraf, M. Martinelli, N. Martinet, F. Marulli, R. J. Massey, E. Medinaceli, S. Mei, M. Melchior, Y. Mellier, E. Merlin, G. Meylan, J. J. Mohr, A. Mora, M. Moresco, L. Moscardini, E. Munari, R. Nakajima, C. Neissner, R. C. Nichol, S. -M. Niemi, C. Padilla, S. Paltani, F. Pasian, K. Pedersen, W. J. Percival, V. Pettorino, S. Pires, G. Polenta, M. Poncet, L. A. Popa, L. Pozzetti, F. Raison, A. Renzi, J. Rhodes, G. Riccio, E. Romelli, M. Roncarelli, R. Saglia, Z. Sakr, D. Sapone, B. Sartoris, P. Schneider, A. Secroun, G. Seidel, M. Seiffert, S. Serrano, P. Simon, C. Sirignano, G. Sirri, L. Stanco, J. Steinwagner, P. Tallada-CrespÃ­, A. N. Taylor, I. Tereno, N. Tessore, S. Toft, R. Toledo-Moreo, F. Torradeflot, I. Tutusaus, L. Valenziano, J. Valiviita, T. Vassallo, G. Verdoes Kleijn, A. Veropalumbo, Y. Wang, J. Weller, G. Zamorani, F. M. Zerbi, E. Zucca, M. Bolzonella, C. Burigana, L. Gabarra, J. MartÃ­n-Fleitas, S. Matthew, V. Scottez, M. Sereno, M. Viel</strong></p>
<p>Euclid is presently mapping the distribution of matter in the Universe in detail via the weak lensing (WL) signature of billions of distant galaxies. The WL signal is most prominent around galaxy clusters, and can extend up to distances well beyond their virial radius, thus constraining their total mass. Near the centre of clusters, where contamination by member galaxies is an issue, the WL data can be complemented with strong lensing (SL) data which can diminish the uncertainty due to the mass-sheet degeneracy and provide high-resolution information about the distribution of matter in the centre of clusters. Here we present a joint SL and WL analysis of the Euclid Early Release Observations of the cluster Abell 2390 at z&#x3D;0.228. Thanks to Euclidâ€™s wide field of view of 0.5 deg$^$2, combined with its angular resolution in the visible band of 0.â€13 and sampling of 0.â€1 per pixel, we constrain the density profile in a wide range of radii, 30 kpc &lt; r &lt; 2000 kpc, from the inner region near the brightest cluster galaxy to beyond the virial radius of the cluster. We find consistency with earlier X-ray results based on assumptions of hydrostatic equilibrium, thus indirectly confirming the nearly relaxed state of this cluster. We also find consistency with previous results based on weak lensing data and ground-based observations of this cluster. From the combined SL+WL profile, we derive the values of the viral mass $M_{200} &#x3D; (1.48 \pm 0.29)\times10^{15}, \Msun$, and virial radius $r_{200} &#x3D;(2.05\pm0.13 , {\rm Mpc}$), with error bars representing one standard deviation. The profile is well described by an NFW model with concentration c&#x3D;6.5 and a small-scale radius of 230 kpc in the 30,kpc $&lt; r &lt;$ 2000,kpc range that is best constrained by SL and WL data. Abell 2390 is the first of many examples where Euclid data will play a crucial role in providing masses for clusters. </p>
<blockquote>
<p>æ¬§å‡ é‡Œå¾—ç›®å‰æ­£åœ¨é€šè¿‡æ•°åäº¿ä¸ªé¥è¿œæ˜Ÿç³»çš„å¼±å¼•åŠ›é€é•œï¼ˆWLï¼‰ç‰¹å¾ï¼Œè¯¦ç»†ç»˜åˆ¶å®‡å®™ä¸­çš„ç‰©è´¨åˆ†å¸ƒå›¾ã€‚WLä¿¡å·åœ¨æ˜Ÿç³»å›¢å‘¨å›´æœ€ä¸ºçªå‡ºï¼Œå¯ä»¥å»¶ä¼¸åˆ°å®ƒä»¬çš„ç»´é‡ŒåŠå¾„ä¹‹å¤–ï¼Œä»è€Œçº¦æŸå®ƒä»¬çš„æ€»è´¨é‡ã€‚åœ¨æ˜Ÿç³»å›¢ä¸­å¿ƒé™„è¿‘ï¼Œç”±äºæˆå‘˜æ˜Ÿç³»çš„æ±¡æŸ“æ˜¯ä¸€ä¸ªé—®é¢˜ï¼ŒWLæ•°æ®å¯ä»¥ä¸å¼ºå¼•åŠ›é€é•œï¼ˆSLï¼‰æ•°æ®ç›¸ç»“åˆï¼Œä»¥å‡å°‘ç”±äºè´¨é‡ç‰‡é€€åŒ–å¼•èµ·çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶æä¾›å…³äºæ˜Ÿç³»å›¢ä¸­å¿ƒç‰©è´¨åˆ†å¸ƒçš„é«˜åˆ†è¾¨ç‡ä¿¡æ¯ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯¹z&#x3D;0.228çš„Abell 2390æ˜Ÿç³»å›¢çš„æ¬§å‡ é‡Œå¾—æ—©æœŸè§‚æµ‹ç»“æœè¿›è¡Œäº†SLå’ŒWLçš„è”åˆåˆ†æã€‚å¾—ç›Šäºæ¬§å‡ é‡Œå¾—0.5å¹³æ–¹åº¦çš„å®½è§†åœºï¼Œç»“åˆå…¶åœ¨å¯è§æ³¢æ®µçš„0.13è§’ç§’åˆ†è¾¨ç‡å’Œæ¯åƒç´ 0.1çš„é‡‡æ ·ç‡ï¼Œæˆ‘ä»¬çº¦æŸäº†ä»é è¿‘æœ€äº®æ˜Ÿç³»å›¢çš„å†…åŒºåˆ°è¶…è¿‡æ˜Ÿç³»å›¢ç»´é‡ŒåŠå¾„çš„å¹¿æ³›åŠå¾„èŒƒå›´å†…çš„å¯†åº¦åˆ†å¸ƒï¼Œå³30åƒç§’å·®è·å°äºrå°äº2000åƒç§’å·®è·ã€‚æˆ‘ä»¬çš„ç»“æœä¸åŸºäºé™æ°´åŠ›å­¦å¹³è¡¡çš„è¾ƒæ—©Xå°„çº¿ç»“æœä¸€è‡´ï¼Œä»è€Œé—´æ¥è¯å®äº†è¯¥æ˜Ÿç³»å›¢çš„è¿‘ä¹æ¾å¼›çŠ¶æ€ã€‚æˆ‘ä»¬çš„ç»“æœä¸åŸºäºå¯¹è¯¥æ˜Ÿç³»å›¢çš„å¼±å¼•åŠ›é€é•œæ•°æ®å’Œåœ°é¢è§‚æµ‹çš„ç»“æœä¹Ÿç›¸ç¬¦ã€‚ä»SL+WLå‰–é¢ç»“åˆåˆ†æå¾—å‡ºç—…æ¯’è´¨é‡å€¼$M_{200} &#x3D; (1.48 \pm 0.29)\times10^{15}, M_{\odot}$å’Œç»´é‡ŒåŠå¾„$r_{200} &#x3D;(2.05\pm0.13 , {\rm Mpc})$ï¼Œè¯¯å·®æ¡ä»£è¡¨ä¸€æ ‡å‡†åå·®ã€‚è¯¥å‰–é¢å¾ˆå¥½åœ°ç”±ä¸€ä¸ªNFWæ¨¡å‹æè¿°ï¼Œå…¶æµ“åº¦c&#x3D;6.5å’Œå°å°ºåº¦åŠå¾„ä¸º230åƒç§’å·®è·ï¼Œåœ¨æœ€ä½³çº¦æŸçš„SLå’ŒWLæ•°æ®çš„èŒƒå›´å†…ï¼ˆå³30åƒç§’å·®è·å°äºrå°äº2åƒç§’å·®è·ï¼‰ã€‚Abell 2390å°†æ˜¯è®¸å¤šæ¬§å‡ é‡Œå¾—æ•°æ®åœ¨å…¶ä¸­å‘æŒ¥å…³é”®ä½œç”¨æ¥ç¡®å®šæ˜Ÿç³»å›¢è´¨é‡çš„ä¾‹å­ä¸­çš„ç¬¬ä¸€ä¸ªã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08545v1">PDF</a> 13 pages, 8 figures</p>
<p><strong>Summary</strong><br>    åˆ©ç”¨Euclidé€šè¿‡å¼±å¼•åŠ›é€é•œæ•ˆåº”è¯¦ç»†ç»˜åˆ¶å®‡å®™ç‰©è´¨åˆ†å¸ƒå›¾ï¼Œç‰¹åˆ«æ˜¯åœ¨æ˜Ÿç³»å›¢å‘¨å›´ã€‚é€šè¿‡è”åˆå¼ºã€å¼±å¼•åŠ›é€é•œæ•°æ®åˆ†æï¼Œçº¦æŸAbel 2390æ˜Ÿç³»å›¢çš„ç‰©è´¨åˆ†å¸ƒåŠæ€»è´¨é‡ã€‚æ­¤åˆ†ææ–¹æ³•å…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼ŒEuclidå°†ä¸ºè®¸å¤šç±»ä¼¼çš„æ˜Ÿç³»å›¢æä¾›è´¨é‡ä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Euclidåˆ©ç”¨å¼±å¼•åŠ›é€é•œæ•ˆåº”ç»˜åˆ¶å®‡å®™ç‰©è´¨åˆ†å¸ƒå›¾ã€‚</li>
<li>æ˜Ÿç³»å›¢å‘¨å›´çš„å¼±å¼•åŠ›é€é•œä¿¡å·æ˜¾è‘—ï¼Œå¯å»¶ä¼¸è‡³å…¶ç»´é‡ŒåŠå¾„ä¹‹å¤–ã€‚</li>
<li>åœ¨æ˜Ÿç³»å›¢ä¸­å¿ƒåŒºåŸŸï¼Œå¼ºå¼•åŠ›é€é•œæ•°æ®å¯è¡¥å……å¼±å¼•åŠ›é€é•œæ•°æ®ï¼Œå‡å°‘è´¨é‡ç‰‡é€€åŒ–çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶æä¾›é«˜åˆ†è¾¨ç‡çš„ç‰©è´¨åˆ†å¸ƒä¿¡æ¯ã€‚</li>
<li>å¯¹Abel 2390æ˜Ÿç³»å›¢è¿›è¡Œè”åˆå¼ºã€å¼±å¼•åŠ›é€é•œåˆ†æã€‚</li>
<li>Euclidçš„å®½è§†åœºå’Œé«˜åˆ†è¾¨ç‡æä¾›äº†ä»å†…åŒºåˆ°ç»´é‡ŒåŠå¾„èŒƒå›´å†…çš„å¯†åº¦åˆ†å¸ƒçº¦æŸã€‚</li>
<li>åˆ†æç»“æœä¸åŸºäºé™æ°´åŠ›å­¦å¹³è¡¡çš„å‡è®¾çš„æ—©æœŸXå°„ç»“æœä¸€è‡´ï¼Œé—´æ¥è¯å®äº†è¯¥æ˜Ÿå›¢çš„è¿‘ä¹æ¾å¼›çŠ¶æ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08545">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b3608eeb3712178f065a3159dbc2b6d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d47ef871e9050471a94cb78a8f4c34a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b9c3b4f63db3fc8411869ac4baadbea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a806666b561eb4e71ad67985e9c93cd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb42e6d1201a3847087f0343931df1e9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Cycle-Context-Verification-for-In-Context-Medical-Image-Segmentation"><a href="#Cycle-Context-Verification-for-In-Context-Medical-Image-Segmentation" class="headerlink" title="Cycle Context Verification for In-Context Medical Image Segmentation"></a>Cycle Context Verification for In-Context Medical Image Segmentation</h2><p><strong>Authors:Shishuai Hu, Zehui Liao, Liangli Zhen, Huazhu Fu, Yong Xia</strong></p>
<p>In-context learning (ICL) is emerging as a promising technique for achieving universal medical image segmentation, where a variety of objects of interest across imaging modalities can be segmented using a single model. Nevertheless, its performance is highly sensitive to the alignment between the query image and in-context image-mask pairs. In a clinical scenario, the scarcity of annotated medical images makes it challenging to select optimal in-context pairs, and fine-tuning foundation ICL models on contextual data is infeasible due to computational costs and the risk of catastrophic forgetting. To address this challenge, we propose Cycle Context Verification (CCV), a novel framework that enhances ICL-based medical image segmentation by enabling self-verification of predictions and accordingly enhancing contextual alignment. Specifically, CCV employs a cyclic pipeline in which the model initially generates a segmentation mask for the query image. Subsequently, the roles of the query and an in-context pair are swapped, allowing the model to validate its prediction by predicting the mask of the original in-context image. The accuracy of this secondary prediction serves as an implicit measure of the initial query segmentation. A query-specific prompt is introduced to alter the query image and updated to improve the measure, thereby enhancing the alignment between the query and in-context pairs. We evaluated CCV on seven medical image segmentation datasets using two ICL foundation models, demonstrating its superiority over existing methods. Our results highlight CCVâ€™s ability to enhance ICL-based segmentation, making it a robust solution for universal medical image segmentation. The code will be available at <a target="_blank" rel="noopener" href="https://github.com/ShishuaiHu/CCV">https://github.com/ShishuaiHu/CCV</a>. </p>
<blockquote>
<p>ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æŠ€æœ¯ï¼Œæ­£é€æ¸å´­éœ²å¤´è§’ï¼Œä¸ºå®ç°é€šç”¨åŒ»å­¦å›¾åƒåˆ†å‰²æä¾›äº†å¯èƒ½ã€‚åœ¨è¿™ç§æŠ€æœ¯ä¸­ï¼Œå¯ä»¥ä½¿ç”¨å•ä¸ªæ¨¡å‹å¯¹å¤šç§æˆåƒæ¨¡å¼çš„æ„Ÿå…´è¶£ç›®æ ‡è¿›è¡Œåˆ†å‰²ã€‚ç„¶è€Œï¼Œå…¶åœ¨æŸ¥è¯¢å›¾åƒå’Œä¸Šä¸‹æ–‡å›¾åƒ-æ©è†œå¯¹ä¹‹é—´çš„å¯¹é½æ–¹é¢çš„æ€§èƒ½é«˜åº¦æ•æ„Ÿã€‚åœ¨ä¸´åºŠåœºæ™¯ä¸­ï¼Œç”±äºæ ‡æ³¨çš„åŒ»å­¦å›¾åƒç¨€ç¼ºï¼Œé€‰æ‹©æœ€ä½³çš„ä¸Šä¸‹æ–‡å¯¹å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ­¤å¤–ï¼Œç”±äºè®¡ç®—æˆæœ¬å’Œå¯¹ç¾éš¾æ€§é—å¿˜é£é™©çš„è€ƒè™‘ï¼Œå¯¹ä¸Šä¸‹æ–‡æ•°æ®è¿›è¡Œå¾®è°ƒåŸºç¡€ICLæ¨¡å‹å¹¶ä¸å¯è¡Œã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¾ªç¯ä¸Šä¸‹æ–‡éªŒè¯ï¼ˆCCVï¼‰è¿™ä¸€æ–°é¢–æ¡†æ¶ï¼Œå®ƒé€šè¿‡å¯ç”¨é¢„æµ‹çš„è‡ªæˆ‘éªŒè¯å’Œå¯¹é½æ¥å¢å¼ºåŸºäºICLçš„åŒ»å­¦å›¾åƒåˆ†å‰²æ•ˆæœã€‚å…·ä½“è€Œè¨€ï¼ŒCCVé‡‡ç”¨ä¸€ä¸ªå¾ªç¯æµç¨‹ï¼Œé¦–å…ˆæ¨¡å‹ç”ŸæˆæŸ¥è¯¢å›¾åƒçš„åˆ†å‰²æ©è†œã€‚éšåï¼ŒæŸ¥è¯¢å›¾åƒå’Œä¸Šä¸‹æ–‡å¯¹ä¹‹é—´çš„è§’è‰²äº’æ¢ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿé€šè¿‡é¢„æµ‹åŸå§‹ä¸Šä¸‹æ–‡å›¾åƒçš„æ©è†œæ¥éªŒè¯å…¶é¢„æµ‹ç»“æœã€‚è¿™ç§äºŒæ¬¡é¢„æµ‹çš„å‡†ç¡®åº¦ä½œä¸ºåˆå§‹æŸ¥è¯¢åˆ†å‰²çš„éšå¼åº¦é‡æ ‡å‡†ã€‚å¼•å…¥ä¸€ä¸ªé’ˆå¯¹æŸ¥è¯¢çš„æç¤ºæ¥æ”¹å˜æŸ¥è¯¢å›¾åƒå¹¶æ›´æ–°æç¤ºä»¥æ”¹å–„åº¦é‡æ ‡å‡†ï¼Œä»è€Œæé«˜æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡å¯¹ä¹‹é—´çš„å¯¹é½æ€§ã€‚æˆ‘ä»¬åœ¨ä¸ƒä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šè¯„ä¼°äº†CCVï¼Œä½¿ç”¨ä¸¤ä¸ªICLåŸºç¡€æ¨¡å‹ï¼Œè¯æ˜äº†å…¶ç›¸è¾ƒäºç°æœ‰æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†CCVåœ¨å¢å¼ºåŸºäºICLçš„åˆ†å‰²æ–¹é¢çš„èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºé€šç”¨åŒ»å­¦å›¾åƒåˆ†å‰²çš„ç¨³å¥è§£å†³æ–¹æ¡ˆã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/ShishuaiHu/CCV%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/ShishuaiHu/CCVä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08357v1">PDF</a> MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æŠ€æœ¯ä¸­ï¼Œä¸ºäº†å®ç°é€šç”¨çš„åŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œæå‡ºäº†ä¸€ç§æ–°å…´çš„æœ‰å‰é€”çš„æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯èƒ½å¤Ÿåœ¨ä¸åŒçš„æˆåƒæ¨¡å¼ä¸‹å¯¹å¤šç§æ„Ÿå…´è¶£çš„å¯¹è±¡è¿›è¡Œåˆ†å‰²ã€‚ä½†å…¶åœ¨æŸ¥è¯¢å›¾åƒä¸ä¸Šä¸‹æ–‡å›¾åƒæ©æ¨¡å¯¹ä¹‹é—´çš„å¯¹é½æ•æ„Ÿåº¦è¾ƒé«˜ã€‚åœ¨ä¸´åºŠåŒ»å­¦åœºæ™¯ä¸­ï¼Œç”±äºæ ‡æ³¨çš„åŒ»å­¦å›¾åƒç¨€ç¼ºï¼Œé€‰æ‹©åˆé€‚çš„ä¸Šä¸‹æ–‡å¯¹å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³æ­¤æŒ‘æˆ˜ï¼Œæå‡ºäº†Cycle Context Verificationï¼ˆCCVï¼‰æ¡†æ¶ï¼Œå®ƒé€šè¿‡è‡ªæˆ‘éªŒè¯é¢„æµ‹ç»“æœå¹¶æ®æ­¤å¢å¼ºä¸Šä¸‹æ–‡å¯¹é½ï¼Œä»è€Œæé«˜äº†ICLåŸºäºåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ€§èƒ½ã€‚CCVé‡‡ç”¨å¾ªç¯ç®¡é“ï¼Œé¦–å…ˆä¸ºæŸ¥è¯¢å›¾åƒç”Ÿæˆåˆ†å‰²æ©æ¨¡ï¼Œç„¶åäº¤æ¢æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡å¯¹è§’è‰²ï¼Œé€šè¿‡é¢„æµ‹åŸå§‹ä¸Šä¸‹æ–‡å›¾åƒçš„æ©æ¨¡æ¥éªŒè¯é¢„æµ‹ç»“æœã€‚äºŒæ¬¡é¢„æµ‹å‡†ç¡®æ€§ä½œä¸ºåˆå§‹æŸ¥è¯¢åˆ†å‰²çš„éšæ€§åº¦é‡æŒ‡æ ‡ã€‚å¼•å…¥æŸ¥è¯¢ç‰¹å®šæç¤ºæ¥æ”¹å–„æŸ¥è¯¢å›¾åƒå¹¶æ›´æ–°åº¦é‡æ ‡å‡†ï¼Œä»è€Œæé«˜æŸ¥è¯¢ä¸ä¸Šä¸‹æ–‡å¯¹ä¹‹é—´çš„å¯¹é½ç¨‹åº¦ã€‚åœ¨ä¸ƒä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šè¯„ä¼°äº†CCVï¼Œä½¿ç”¨ä¸¤ä¸ªICLåŸºç¡€æ¨¡å‹ï¼Œè¯æ˜äº†å…¶ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>In-context learning (ICL) æ˜¯ä¸€ç§æ–°å…´æŠ€æœ¯ï¼Œç”¨äºå®ç°é€šç”¨çš„åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>ICLæ€§èƒ½é«˜åº¦ä¾èµ–äºæŸ¥è¯¢å›¾åƒä¸ä¸Šä¸‹æ–‡å›¾åƒæ©æ¨¡å¯¹ä¹‹é—´çš„å¯¹é½ã€‚</li>
<li>åœ¨ä¸´åºŠç¯å¢ƒä¸­ï¼Œé€‰æ‹©æœ€ä½³çš„ä¸Šä¸‹æ–‡å¯¹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ ‡æ³¨çš„åŒ»å­¦å›¾åƒç¨€ç¼ºã€‚</li>
<li>Cycle Context Verification (CCV) æ¡†æ¶é€šè¿‡è‡ªæˆ‘éªŒè¯é¢„æµ‹ç»“æœæé«˜ICLåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ€§èƒ½ã€‚</li>
<li>CCVé‡‡ç”¨å¾ªç¯ç®¡é“ï¼ŒåŒ…æ‹¬ç”Ÿæˆåˆ†å‰²æ©æ¨¡ã€è§’è‰²äº¤æ¢ã€é¢„æµ‹éªŒè¯å’ŒæŸ¥è¯¢ç‰¹å®šæç¤ºçš„å¼•å…¥ã€‚</li>
<li>CCVåœ¨å¤šä¸ªåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08357">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1d2471c820873e79a2724b3f8566c82e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1698d36d0436cd63d7f5816baef36ebf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-be6445b6bf7982bb4051b126b2209507.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Single-Domain-Generalization-for-Multimodal-Cross-Cancer-Prognosis-via-Dirac-Rebalancer-and-Distribution-Entanglement"><a href="#Single-Domain-Generalization-for-Multimodal-Cross-Cancer-Prognosis-via-Dirac-Rebalancer-and-Distribution-Entanglement" class="headerlink" title="Single-Domain Generalization for Multimodal Cross-Cancer Prognosis via   Dirac Rebalancer and Distribution Entanglement"></a>Single-Domain Generalization for Multimodal Cross-Cancer Prognosis via   Dirac Rebalancer and Distribution Entanglement</h2><p><strong>Authors:Jia-Xuan Jiang, Jiashuai Liu, Hongtao Wu, Yifeng Wu, Zhong Wang, Qi Bi, Yefeng Zheng</strong></p>
<p>Deep learning has shown remarkable performance in integrating multimodal data for survival prediction. However, existing multimodal methods mainly focus on single cancer types and overlook the challenge of generalization across cancers. In this work, we are the first to reveal that multimodal prognosis models often generalize worse than unimodal ones in cross-cancer scenarios, despite the critical need for such robustness in clinical practice. To address this, we propose a new task: Cross-Cancer Single Domain Generalization for Multimodal Prognosis, which evaluates whether models trained on a single cancer type can generalize to unseen cancers. We identify two key challenges: degraded features from weaker modalities and ineffective multimodal integration. To tackle these, we introduce two plug-and-play modules: Sparse Dirac Information Rebalancer (SDIR) and Cancer-aware Distribution Entanglement (CADE). SDIR mitigates the dominance of strong features by applying Bernoulli-based sparsification and Dirac-inspired stabilization to enhance weaker modality signals. CADE, designed to synthesize the target domain distribution, fuses local morphological cues and global gene expression in latent space. Experiments on a four-cancer-type benchmark demonstrate superior generalization, laying the foundation for practical, robust cross-cancer multimodal prognosis. Code is available at <a target="_blank" rel="noopener" href="https://github.com/HopkinsKwong/MCCSDG">https://github.com/HopkinsKwong/MCCSDG</a> </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ åœ¨èåˆå¤šæ¨¡æ€æ•°æ®è¿›è¡Œç”Ÿå­˜é¢„æµ‹æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨å•ä¸€ç™Œç—‡ç±»å‹ä¸Šï¼Œå¿½è§†äº†è·¨ç™Œç—‡æ³›åŒ–çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æ­ç¤ºäº†åœ¨è·¨ç™Œç—‡åœºæ™¯ä¸­ï¼Œå¤šæ¨¡æ€é¢„åæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å¾€å¾€æ¯”å•æ¨¡æ€æ¨¡å‹å·®ï¼Œå°½ç®¡åœ¨ä¸´åºŠå®è·µä¸­å¯¹è¿™ç§ç¨³å¥æ€§æœ‰ç€è¿«åˆ‡çš„éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ä»»åŠ¡ï¼šè·¨ç™Œç—‡å•åŸŸæ³›åŒ–å¤šæ¨¡æ€é¢„åï¼Œæ—¨åœ¨è¯„ä¼°åœ¨å•ä¸€ç™Œç—‡ç±»å‹ä¸Šè®­ç»ƒçš„æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿæ¨å¹¿åº”ç”¨åˆ°æœªè§è¿‡çš„ç™Œç—‡ã€‚æˆ‘ä»¬ç¡®å®šäº†ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šæ¥è‡ªè¾ƒå¼±æ¨¡æ€çš„ç‰¹å¾é€€åŒ–ä»¥åŠæ— æ•ˆçš„å¤šæ¨¡æ€é›†æˆã€‚ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ç§å³æ’å³ç”¨çš„æ¨¡å—ï¼šç¨€ç–ç‹„æ‹‰å…‹ä¿¡æ¯å¹³è¡¡å™¨ï¼ˆSDIRï¼‰å’Œç™Œç—‡æ„ŸçŸ¥åˆ†å¸ƒçº ç¼ ï¼ˆCADEï¼‰ã€‚SDIRé€šè¿‡åº”ç”¨åŸºäºä¼¯åŠªåˆ©åˆ†å¸ƒçš„ç¨€ç–åŒ–å’Œç‹„æ‹‰å…‹å¯å‘å¼çš„ç¨³å®šåŒ–ï¼Œå‡è½»å¼ºç‰¹å¾çš„ä¸»å¯¼ä½œç”¨ï¼Œå¢å¼ºè¾ƒå¼±æ¨¡æ€ä¿¡å·ã€‚CADEæ—¨åœ¨åˆæˆç›®æ ‡åŸŸåˆ†å¸ƒï¼Œèåˆå±€éƒ¨å½¢æ€çº¿ç´¢å’Œå…¨å±€åŸºå› è¡¨è¾¾äºæ½œåœ¨ç©ºé—´ã€‚åœ¨å››ç§ç™Œç—‡ç±»å‹çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†å…¶ä¼˜è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå®é™…ã€ç¨³å¥çš„è·¨ç™Œç—‡å¤šæ¨¡æ€é¢„åå¥ å®šäº†åŸºç¡€ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HopkinsKwong/MCCSDG%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HopkinsKwong/MCCSDGæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08340v1">PDF</a> Accepted by ACMMM 25</p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å­¦ä¹ åœ¨å¤šæ¨¡æ€æ•°æ®èåˆè¿›è¡Œç”Ÿå­˜é¢„æµ‹æ–¹é¢å±•ç°å‡ºæ˜¾è‘—æ€§èƒ½ï¼Œä½†åœ¨è·¨ç™Œç§é¢„åé¢„æµ‹æ–¹é¢ï¼Œç°æœ‰å¤šæ¨¡æ€æ–¹æ³•çš„é€šç”¨æ€§æœ‰å¾…æé«˜ã€‚æœ¬æ–‡é¦–æ¬¡æ­ç¤ºå¤šæ¨¡æ€é¢„åæ¨¡å‹åœ¨è·¨ç™Œç§åœºæ™¯ä¸‹çš„æ³›åŒ–æ€§èƒ½å¾€å¾€ä¸å¦‚å•æ¨¡æ€æ¨¡å‹ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºæ–°çš„ä»»åŠ¡ï¼šè·¨ç™Œç§å•åŸŸæ³›åŒ–å¤šæ¨¡æ€é¢„åé¢„æµ‹ï¼Œæ—¨åœ¨è¯„ä¼°åœ¨å•ä¸€ç™Œç§ä¸Šè®­ç»ƒçš„æ¨¡å‹æ˜¯å¦èƒ½æ³›åŒ–åˆ°æœªè§è¿‡çš„ç™Œç—‡ã€‚é’ˆå¯¹å¼±ç‰¹å¾çš„æœ‰æ•ˆæ•´åˆåŠæ¨¡æ€é—´çš„æœ‰æ•ˆèåˆä¸¤å¤§å…³é”®æŒ‘æˆ˜ï¼Œæœ¬æ–‡å¼•å…¥ä¸¤ä¸ªå³æ’å³ç”¨æ¨¡å—ï¼šSparse Diracä¿¡æ¯å¹³è¡¡å™¨ï¼ˆSDIRï¼‰å’Œç™Œç—‡æ„ŸçŸ¥åˆ†å¸ƒçº ç¼ ï¼ˆCADEï¼‰ã€‚SDIRé€šè¿‡åº”ç”¨åŸºäºä¼¯åŠªåˆ©çš„æ–¹æ³•å®ç°ç‰¹å¾ç¨€ç–åŒ–å’ŒDiracå¯å‘å¼çš„ç¨³å®šæ€§æ¥å¼ºåŒ–å¼±æ¨¡æ€ä¿¡å·ï¼›è€ŒCADEåˆ™è‡´åŠ›äºåˆæˆç›®æ ‡åŸŸåˆ†å¸ƒï¼Œèåˆäº†å±€éƒ¨å½¢æ€çº¿ç´¢å’Œå…¨å±€åŸºå› è¡¨è¾¾æ½œå˜é‡ã€‚åœ¨å››ç§ç™Œç—‡ç±»å‹çš„åŸºå‡†æµ‹è¯•ä¸Šï¼Œæœ¬æ–‡æ–¹æ³•å±•ç°å‡ºä¼˜è¶Šçš„æ³›åŒ–æ€§èƒ½ï¼Œä¸ºå®é™…ã€ç¨³å¥çš„è·¨ç™Œå¤šæ¨¡æ€é¢„åé¢„æµ‹å¥ å®šåŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨å¤šæ¨¡æ€æ•°æ®èåˆç”Ÿå­˜é¢„æµ‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨è·¨ç™Œç§é¢„åé¢„æµ‹æ–¹é¢ç¼ºä¹æ³›åŒ–æ€§ã€‚</li>
<li>æå‡ºè·¨ç™Œç§å•åŸŸæ³›åŒ–å¤šæ¨¡æ€é¢„åé¢„æµ‹ä»»åŠ¡ï¼Œè¯„ä¼°æ¨¡å‹åœ¨å¤šç§æœªè§ç™Œç—‡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šå¼±ç‰¹å¾çš„æœ‰æ•ˆæ•´åˆå’Œæ¨¡æ€é—´çš„æœ‰æ•ˆèåˆã€‚</li>
<li>å¼•å…¥Sparse Diracä¿¡æ¯å¹³è¡¡å™¨ï¼ˆSDIRï¼‰å¼ºåŒ–å¼±æ¨¡æ€ä¿¡å·ã€‚</li>
<li>é‡‡ç”¨Cancer-aware Distribution Entanglementï¼ˆCADEï¼‰åˆæˆç›®æ ‡åŸŸåˆ†å¸ƒèåˆå±€éƒ¨å½¢æ€å’Œå…¨å±€åŸºå› è¡¨è¾¾ã€‚</li>
<li>åœ¨å››ç§ç™Œç—‡ç±»å‹çš„å®éªŒåŸºå‡†æµ‹è¯•ä¸­ï¼Œæ‰€æå‡ºæ–¹æ³•å±•ç°å‡ºä¼˜è¶Šæ³›åŒ–æ€§èƒ½ã€‚</li>
<li>ä»£ç å…¬å¼€ï¼Œä¸ºè¿›ä¸€æ­¥ç ”ç©¶æä¾›åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08340">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-52ec7c08e7054aba8548fb7ba05c48a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09844ef395a97f557143719ddbd58f0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d442096b7f1b2519f4537b82f5de371.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ce0c529f218471c60508125e99fe149.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Admissibility-of-Stein-Shrinkage-for-Batch-Normalization-in-the-Presence-of-Adversarial-Attacks"><a href="#Admissibility-of-Stein-Shrinkage-for-Batch-Normalization-in-the-Presence-of-Adversarial-Attacks" class="headerlink" title="Admissibility of Stein Shrinkage for Batch Normalization in the Presence   of Adversarial Attacks"></a>Admissibility of Stein Shrinkage for Batch Normalization in the Presence   of Adversarial Attacks</h2><p><strong>Authors:Sofia Ivolgina, P. Thomas Fletcher, Baba C. Vemuri</strong></p>
<p>Batch normalization (BN) is a ubiquitous operation in deep neural networks used primarily to achieve stability and regularization during network training. BN involves feature map centering and scaling using sample means and variances, respectively. Since these statistics are being estimated across the feature maps within a batch, this problem is ideally suited for the application of Steinâ€™s shrinkage estimation, which leads to a better, in the mean-squared-error sense, estimate of the mean and variance of the batch. In this paper, we prove that the Stein shrinkage estimator for the mean and variance dominates over the sample mean and variance estimators in the presence of adversarial attacks when modeling these attacks using sub-Gaussian distributions. This facilitates and justifies the application of Stein shrinkage to estimate the mean and variance parameters in BN and use it in image classification (segmentation) tasks with and without adversarial attacks. We present SOTA performance results using this Stein corrected batch norm in a standard ResNet architecture applied to the task of image classification using CIFAR-10 data, 3D CNN on PPMI (neuroimaging) data and image segmentation using HRNet on Cityscape data with and without adversarial attacks. </p>
<blockquote>
<p>æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBNï¼‰æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œä¸­æ™®éå­˜åœ¨çš„æ“ä½œï¼Œä¸»è¦ç”¨äºåœ¨ç½‘ç»œè®­ç»ƒè¿‡ç¨‹ä¸­å®ç°ç¨³å®šæ€§å’Œæ­£åˆ™åŒ–ã€‚BNä¸»è¦æ¶‰åŠä½¿ç”¨æ ·æœ¬å‡å€¼å’Œæ–¹å·®å¯¹ç‰¹å¾å›¾è¿›è¡Œä¸­å¿ƒåŒ–å’Œç¼©æ”¾ã€‚ç”±äºè¿™äº›ç»Ÿè®¡é‡æ˜¯åœ¨ä¸€æ‰¹ç‰¹å¾å›¾å†…ä¼°ç®—çš„ï¼Œå› æ­¤è¿™ä¸ªé—®é¢˜éå¸¸é€‚åˆåº”ç”¨Steinçš„æ”¶ç¼©ä¼°è®¡ï¼Œä»è€Œåœ¨å‡æ–¹è¯¯å·®æ„ä¹‰ä¸Šè·å¾—æ›´å¥½çš„æ‰¹é‡å‡å€¼å’Œæ–¹å·®çš„ä¼°è®¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†åœ¨ä½¿ç”¨äºšé«˜æ–¯åˆ†å¸ƒå¯¹è¿™äº›æ”»å‡»è¿›è¡Œå»ºæ¨¡æ—¶ï¼Œå­˜åœ¨å¯¹æŠ—æ€§æ”»å‡»çš„æƒ…å†µä¸‹ï¼Œå‡å€¼å’Œæ–¹å·®çš„Steinæ”¶ç¼©ä¼°è®¡å™¨ä¼˜äºæ ·æœ¬å‡å€¼å’Œæ–¹å·®ä¼°è®¡å™¨ã€‚è¿™æœ‰åŠ©äºå¹¶å°†ä¸ºåœ¨BNä¸­åº”ç”¨Steinæ”¶ç¼©æ¥ä¼°è®¡å‡å€¼å’Œæ–¹å·®å‚æ•°æä¾›äº†ä¾æ®ï¼Œå¹¶å°†å…¶åº”ç”¨äºæœ‰å’Œæ— å¯¹æŠ—æ”»å‡»çš„å›¾åƒåˆ†ç±»ï¼ˆåˆ†å‰²ï¼‰ä»»åŠ¡ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™ç§ç»Steinæ ¡æ­£çš„æ‰¹é‡å½’ä¸€åŒ–ï¼Œåœ¨CIFAR-10æ•°æ®çš„å›¾åƒåˆ†ç±»ä»»åŠ¡ã€PPMIï¼ˆç¥ç»æˆåƒï¼‰æ•°æ®çš„3D CNNä»¥åŠå¸¦æœ‰å’Œä¸å¸¦æœ‰å¯¹æŠ—æ”»å‡»çš„Cityscapeæ•°æ®çš„HRNetå›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ï¼Œå‘ˆç°å‡ºæœ€å…ˆè¿›æ€§èƒ½çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08261v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰¹æ ‡å‡†åŒ–ï¼ˆBNï¼‰åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œä¸»è¦ç”¨äºå®ç°ç½‘ç»œè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¨³å®šæ€§å’Œæ­£åˆ™åŒ–ã€‚æ–‡ç« æå‡ºä½¿ç”¨Steinæ”¶ç¼©ä¼°è®¡æ”¹è¿›æ ·æœ¬å‡å€¼å’Œæ–¹å·®çš„ä¼°è®¡ï¼Œä»¥åº”å¯¹ç‰¹å¾æ˜ å°„ä¸­çš„æ‰¹é‡å¤„ç†ç»Ÿè®¡é—®é¢˜ã€‚å®éªŒè¯æ˜ï¼Œåœ¨å¯¹æŠ—æ”»å‡»åœºæ™¯ä¸‹ï¼Œä½¿ç”¨åŸºäºSteinæ”¶ç¼©ä¼°è®¡çš„å‡å€¼å’Œæ–¹å·®ä¼°è®¡ç›¸è¾ƒäºä¼ ç»Ÿæ ·æœ¬å‡å€¼å’Œæ–¹å·®ä¼°è®¡æ›´ä¸ºä¼˜è¶Šã€‚æ­¤å¤–ï¼Œæ–‡ç« å±•ç¤ºäº†åœ¨å›¾åƒåˆ†ç±»ï¼ˆåˆ†å‰²ï¼‰ä»»åŠ¡ä¸­åº”ç”¨è¯¥æ–¹æ³•çš„ä¼˜å¼‚æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰¹æ ‡å‡†åŒ–ï¼ˆBNï¼‰åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­æ™®éç”¨äºæå‡è®­ç»ƒç¨³å®šæ€§å’Œæ­£åˆ™åŒ–ã€‚</li>
<li>ç‰¹å¾æ˜ å°„ä¸­çš„æ‰¹é‡å¤„ç†ç»Ÿè®¡é—®é¢˜å¯ä»¥é€šè¿‡åº”ç”¨Steinæ”¶ç¼©ä¼°è®¡å¾—åˆ°æ›´å¥½çš„å¤„ç†ã€‚</li>
<li>å¯¹æŠ—æ”»å‡»åœºæ™¯ä¸‹ï¼Œä½¿ç”¨Steinæ”¶ç¼©ä¼°è®¡çš„å‡å€¼å’Œæ–¹å·®ä¼°è®¡æ•ˆæœä¼˜äºæ ·æœ¬å‡å€¼å’Œæ–¹å·®ä¼°è®¡ã€‚</li>
<li>Steinæ”¶ç¼©ä¼°è®¡åœ¨å›¾åƒåˆ†ç±»ï¼ˆåˆ†å‰²ï¼‰ä»»åŠ¡ä¸­å¾—åˆ°äº†åº”ç”¨éªŒè¯ã€‚</li>
<li>æ–‡ç« å±•ç¤ºäº†ä½¿ç”¨Steinä¿®æ­£æ‰¹æ ‡å‡†åŒ–åœ¨CIFAR-10æ•°æ®é›†ä¸Šçš„å›¾åƒåˆ†ç±»ä»»åŠ¡æ€§èƒ½è¡¨ç°ã€‚</li>
<li>åœ¨ä½¿ç”¨PPMIï¼ˆç¥ç»æˆåƒï¼‰æ•°æ®çš„3D CNNä»»åŠ¡ä¸­ï¼ŒSteinä¿®æ­£æ‰¹æ ‡å‡†åŒ–åŒæ ·å±•ç°å‡ºäº†ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08261">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fc0bf3b48b1f4f238bca8fee5ec2702c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dfd49757ddb0f8204ea80635e0681b3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f041547c1bd6c340b56fa83a93cb9bfe.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Raptor-Scalable-Train-Free-Embeddings-for-3D-Medical-Volumes-Leveraging-Pretrained-2D-Foundation-Models"><a href="#Raptor-Scalable-Train-Free-Embeddings-for-3D-Medical-Volumes-Leveraging-Pretrained-2D-Foundation-Models" class="headerlink" title="Raptor: Scalable Train-Free Embeddings for 3D Medical Volumes Leveraging   Pretrained 2D Foundation Models"></a>Raptor: Scalable Train-Free Embeddings for 3D Medical Volumes Leveraging   Pretrained 2D Foundation Models</h2><p><strong>Authors:Ulzee An, Moonseong Jeong, Simon A. Lee, Aditya Gorla, Yuzhe Yang, Sriram Sankararaman</strong></p>
<p>Current challenges in developing foundational models for volumetric imaging data, such as magnetic resonance imaging (MRI), stem from the computational complexity of training state-of-the-art architectures in high dimensions and curating sufficiently large datasets of volumes. To address these challenges, we introduce Raptor (Random Planar Tensor Reduction), a train-free method for generating semantically rich embeddings for volumetric data. Raptor leverages a frozen 2D foundation model, pretrained on natural images, to extract visual tokens from individual cross-sections of medical volumes. These tokens are then spatially compressed using random projections, significantly reducing computational complexity while retaining semantic information. Extensive experiments on ten diverse medical volume tasks verify the superior performance of Raptor over state-of-the-art methods, including those pretrained exclusively on medical volumes (+3% SuPreM, +6% MISFM, +10% Merlin, +13% VoCo, and +14% SLIViT), while entirely bypassing the need for costly training. Our results highlight the effectiveness and versatility of Raptor as a foundation for advancing deep learning-based methods for medical volumes. </p>
<blockquote>
<p>å½“å‰ï¼Œåœ¨å¼€å‘ç”¨äºä¸‰ç»´æˆåƒæ•°æ®ï¼ˆå¦‚ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ï¼‰çš„åŸºç¡€æ¨¡å‹æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œæºäºåœ¨é«˜ç»´åº¦è®­ç»ƒæœ€æ–°æ¶æ„çš„è®¡ç®—å¤æ‚æ€§å’Œæ”¶é›†è¶³å¤Ÿå¤§è§„æ¨¡çš„ä¸‰ç»´æ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Raptorï¼ˆéšæœºå¹³é¢å¼ é‡ç¼©å‡ï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„ç”Ÿæˆè¯­ä¹‰ä¸°å¯Œçš„ä¸‰ç»´æ•°æ®åµŒå…¥çš„æ–¹æ³•ã€‚Raptoråˆ©ç”¨å†»ç»“çš„äºŒç»´åŸºç¡€æ¨¡å‹ï¼ˆåœ¨å¤©ç„¶å›¾åƒä¸Šé¢„è®­ç»ƒï¼‰ï¼Œä»åŒ»ç–—ä½“ç§¯çš„å•ä¸ªæ¨ªæˆªé¢ä¸Šæå–è§†è§‰ç¬¦å·ã€‚ç„¶åè¿™äº›ç¬¦å·é€šè¿‡éšæœºæŠ•å½±è¿›è¡Œç©ºé—´å‹ç¼©ï¼Œè¿™æå¤§åœ°å‡å°‘äº†è®¡ç®—å¤æ‚åº¦åŒæ—¶ä¿ç•™äº†è¯­ä¹‰ä¿¡æ¯ã€‚åœ¨åä¸ªä¸åŒçš„åŒ»ç–—ä½“ç§¯ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†Raptorä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬é‚£äº›ä»…åœ¨åŒ»ç–—ä½“ç§¯ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„æ–¹æ³•ï¼ˆ+3% SuPreMï¼Œ+6% MISFMï¼Œ+10% Merlinï¼Œ+13% VoCoå’Œ+14% SLIViTï¼‰ï¼ŒåŒæ—¶å®Œå…¨é¿å…äº†æ˜‚è´µçš„è®­ç»ƒéœ€æ±‚ã€‚æˆ‘ä»¬çš„ç»“æœçªå‡ºäº†Raptorä½œä¸ºæ¨è¿›åŸºäºæ·±åº¦å­¦ä¹ çš„åŒ»ç–—ä½“ç§¯æ–¹æ³•çš„åŸºç¡€çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08254v1">PDF</a> 21 pages, 10 figures, accepted to ICML 2025. The first two authors   contributed equally</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ä½“ç§¯æˆåƒæ•°æ®ï¼ˆå¦‚ç£å…±æŒ¯æˆåƒï¼‰çš„å»ºæ¨¡æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§åä¸ºâ€œRaptorâ€ï¼ˆéšæœºå¹³é¢å¼ é‡ç¼©å‡ï¼‰çš„æ–¹æ³•ï¼Œæ— éœ€è®­ç»ƒå³å¯ç”Ÿæˆä½“ç§¯æ•°æ®çš„ä¸°å¯Œè¯­ä¹‰åµŒå…¥ã€‚Raptoråˆ©ç”¨å†»ç»“çš„äºŒç»´åŸºç¡€æ¨¡å‹ï¼Œåœ¨è‡ªç„¶å›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä»åŒ»å­¦ä½“ç§¯çš„å•ä¸ªæˆªé¢ä¸­æå–è§†è§‰æ ‡è®°ã€‚è¿™äº›æ ‡è®°ä½¿ç”¨éšæœºæŠ•å½±è¿›è¡Œç©ºé—´å‹ç¼©ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿ç•™äº†è¯­ä¹‰ä¿¡æ¯ã€‚å®éªŒè¯æ˜ï¼ŒRaptoråœ¨å¤šç§åŒ»å­¦ä½“ç§¯ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œä¸”å®Œå…¨é¿å…äº†æ˜‚è´µçš„è®­ç»ƒæˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½“ç§¯æˆåƒæ•°æ®å»ºæ¨¡é¢ä¸´è®¡ç®—å¤æ‚åº¦å’Œæ•°æ®é›†å¤§å°æŒ‘æˆ˜ã€‚</li>
<li>Raptoræ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„ç”Ÿæˆè¯­ä¹‰ä¸°å¯ŒåµŒå…¥çš„æ–¹æ³•ï¼Œç”¨äºå¤„ç†ä½“ç§¯æ•°æ®ã€‚</li>
<li>Raptoråˆ©ç”¨é¢„è®­ç»ƒçš„äºŒç»´åŸºç¡€æ¨¡å‹ä»åŒ»å­¦ä½“ç§¯çš„æˆªé¢ä¸­æå–è§†è§‰æ ‡è®°ã€‚</li>
<li>é€šè¿‡éšæœºæŠ•å½±è¿›è¡Œç©ºé—´å‹ç¼©ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿ç•™è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>å®éªŒè¯æ˜Raptoråœ¨å¤šç§åŒ»å­¦ä½“ç§¯ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜è¶Šã€‚</li>
<li>Raptorç›¸è¾ƒäºå…¶ä»–é¢„è®­ç»ƒåŒ»å­¦ä½“ç§¯æ–¹æ³•æœ‰æ›´ä½³çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7f04ba38ff1ff86e73b171a5360fd103.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-282ad5e8a6e739d6fc9b6f32e80ffbf3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8af6a28bc6219e506efae9f46f81d543.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85746dce65c46feb16d8cb789a7ab041.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ebf9b5e22b46f5a2f90d8eb53b16b75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1d1791f53d66b364cbd86df1febf4659.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Depth-Sequence-Transformer-DST-for-Segment-Specific-ICA-Calcification-Mapping-on-Non-Contrast-CT"><a href="#Depth-Sequence-Transformer-DST-for-Segment-Specific-ICA-Calcification-Mapping-on-Non-Contrast-CT" class="headerlink" title="Depth-Sequence Transformer (DST) for Segment-Specific ICA Calcification   Mapping on Non-Contrast CT"></a>Depth-Sequence Transformer (DST) for Segment-Specific ICA Calcification   Mapping on Non-Contrast CT</h2><p><strong>Authors:Xiangjian Hou, Ebru Yaman Akcicek, Xin Wang, Kazem Hashemizadeh, Scott Mcnally, Chun Yuan, Xiaodong Ma</strong></p>
<p>While total intracranial carotid artery calcification (ICAC) volume is an established stroke biomarker, growing evidence shows this aggregate metric ignores the critical influence of plaque location, since calcification in different segments carries distinct prognostic and procedural risks. However, a finer-grained, segment-specific quantification has remained technically infeasible. Conventional 3D models are forced to process downsampled volumes or isolated patches, sacrificing the global context required to resolve anatomical ambiguity and render reliable landmark localization. To overcome this, we reformulate the 3D challenge as a \textbf{Parallel Probabilistic Landmark Localization} task along the 1D axial dimension. We propose the \textbf{Depth-Sequence Transformer (DST)}, a framework that processes full-resolution CT volumes as sequences of 2D slices, learning to predict $N&#x3D;6$ independent probability distributions that pinpoint key anatomical landmarks. Our DST framework demonstrates exceptional accuracy and robustness. Evaluated on a 100-patient clinical cohort with rigorous 5-fold cross-validation, it achieves a Mean Absolute Error (MAE) of \textbf{0.1 slices}, with \textbf{96%} of predictions falling within a $\pm1$ slice tolerance. Furthermore, to validate its architectural power, the DST backbone establishes the best result on the public Clean-CC-CCII classification benchmark under an end-to-end evaluation protocol. Our work delivers the first practical tool for automated segment-specific ICAC analysis. The proposed framework provides a foundation for further studies on the role of location-specific biomarkers in diagnosis, prognosis, and procedural planning. Our code will be made publicly available. </p>
<blockquote>
<p>è™½ç„¶é¢…å†…é¢ˆåŠ¨è„‰é’™åŒ–ï¼ˆICACï¼‰æ€»ä½“ç§¯å·²è¢«ç¡®ç«‹ä¸ºä¸­é£ç”Ÿç‰©æ ‡å¿—ç‰©ï¼Œä½†è¶Šæ¥è¶Šå¤šçš„è¯æ®è¡¨æ˜ï¼Œè¿™ä¸€ç»¼åˆæŒ‡æ ‡å¿½ç•¥äº†æ–‘å—ä½ç½®çš„é‡è¦å½±å“ï¼Œå› ä¸ºä¸åŒéƒ¨ä½çš„é’™åŒ–å…·æœ‰ä¸åŒçš„é¢„åå’Œæ‰‹æœ¯é£é™©ã€‚ç„¶è€Œï¼Œæ›´ç²¾ç»†ã€ç‰¹å®šäºåŒºæ®µçš„é‡åŒ–åœ¨æŠ€æœ¯ä¸Šä»ç„¶ä¸å¯è¡Œã€‚ä¼ ç»Ÿ3Dæ¨¡å‹è¢«è¿«å¤„ç†é™é‡‡æ ·ä½“ç§¯æˆ–å­¤ç«‹çš„è¡¥ä¸ï¼Œç‰ºç‰²äº†è§£å†³è§£å‰–æ­§ä¹‰å’Œå¯é åœ°æ ‡å®šä½æ‰€éœ€çš„å…¨çƒèƒŒæ™¯ã€‚ä¸ºäº†å…‹æœè¿™ä¸€éš¾é¢˜ï¼Œæˆ‘ä»¬å°†è¿™ä¸€3DæŒ‘æˆ˜é‡æ–°åˆ¶å®šä¸ºæ²¿ç€1Dè½´å‘ç»´åº¦çš„â€œå¹¶è¡Œæ¦‚ç‡åœ°æ ‡å®šä½â€ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†â€œæ·±åº¦åºåˆ—å˜å‹å™¨ï¼ˆDSTï¼‰â€ï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥å…¨åˆ†è¾¨ç‡CTä½“ç§¯ä½œä¸ºäºŒç»´åˆ‡ç‰‡åºåˆ—è¿›è¡Œå¤„ç†çš„æ¡†æ¶ï¼Œå­¦ä¹ é¢„æµ‹N&#x3D;6ä¸ªç‹¬ç«‹æ¦‚ç‡åˆ†å¸ƒï¼Œè¿™äº›åˆ†å¸ƒç²¾ç¡®æŒ‡å‡ºäº†å…³é”®è§£å‰–åœ°æ ‡ã€‚æˆ‘ä»¬çš„DSTæ¡†æ¶è¡¨ç°å‡ºå“è¶Šå‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚åœ¨ä¸¥æ ¼çš„äº”æŠ˜äº¤å‰éªŒè¯çš„100ä¾‹ä¸´åºŠé˜Ÿåˆ—ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œå…¶å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰è¾¾åˆ°äº†æƒŠäººçš„<strong>ä»…åç¦»äº†é›¶ç‚¹ä¸€è–„å±‚</strong>ï¼Œé¢„æµ‹åœ¨æ­£è´Ÿä¸€ä¸ªåˆ‡ç‰‡å†…çš„å®¹å¿åº¦è¾¾åˆ°<strong>ç™¾åˆ†ä¹‹ä¹åå…­</strong>ã€‚æ­¤å¤–ï¼Œä¸ºäº†éªŒè¯å…¶æ¶æ„ä¼˜åŠ¿ï¼ŒDSTä¸»å¹²åœ¨å…¬å…±Clean-CC-CCIIåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸‹è¾¾åˆ°äº†æœ€ä½³ç»“æœï¼Œè¯¥è¯„ä¼°éµå¾ªç«¯åˆ°ç«¯è¯„ä¼°åè®®ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†é¦–ä¸ªç”¨äºè‡ªåŠ¨åŒ–ç‰¹å®šåŒºæ®µICACåˆ†æçš„å®ç”¨å·¥å…·ã€‚è¯¥æ¡†æ¶ä¸ºç ”ç©¶ä½ç½®ç‰¹å¼‚æ€§ç”Ÿç‰©æ ‡å¿—ç‰©åœ¨è¯Šæ–­ã€é¢„åå’Œæ‰‹æœ¯è®¡åˆ’ä¸­çš„ä½œç”¨æä¾›äº†åŸºç¡€ã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08214v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡è§£å†³äº†ä¸€ä¸ªé•¿æœŸå­˜åœ¨çš„é—®é¢˜ï¼šä¼ ç»Ÿé¢…å†…é¢ˆåŠ¨è„‰é’™åŒ–ï¼ˆICACï¼‰ä½“ç§¯è¯„ä¼°å¿½ç•¥äº†æ–‘å—ä½ç½®çš„é‡è¦æ€§ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§åä¸ºæ·±åº¦åºåˆ—è½¬æ¢å™¨ï¼ˆDSTï¼‰çš„æ–°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤„ç†å…¨åˆ†è¾¨ç‡CTä½“ç§¯æ•°æ®ï¼Œå‡†ç¡®é¢„æµ‹å…³é”®è§£å‰–æ ‡å¿—ç‰©çš„ä½ç½®ã€‚ç»ä¸´åºŠéªŒè¯ï¼Œè¯¥æ¡†æ¶é¢„æµ‹å‡†ç¡®åº¦é«˜ï¼Œç¨³å®šæ€§å¼ºã€‚å…¶å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ä»…ä¸º0.1åˆ‡ç‰‡ï¼Œé¢„æµ‹ç»“æœä¸­96%åœ¨Â±1åˆ‡ç‰‡èŒƒå›´å†…ã€‚è¿™ä¸ºè¿›ä¸€æ­¥çš„ICACåˆ†ææä¾›äº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»ŸICACè¯„ä¼°æ–¹æ³•å¿½ç•¥äº†æ–‘å—ä½ç½®çš„é‡è¦æ€§ï¼Œä¸åŒéƒ¨ä½çš„é’™åŒ–æœ‰ä¸åŒçš„é¢„åå’Œæ‰‹æœ¯é£é™©ã€‚</li>
<li>ç›®å‰ç¼ºä¹ç²¾ç»†çš„ã€åŸºäºä½ç½®çš„é‡åŒ–æŠ€æœ¯æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”æ·±åº¦åºåˆ—è½¬æ¢å™¨ï¼ˆDSTï¼‰ï¼Œèƒ½å¤„ç†å…¨åˆ†è¾¨ç‡CTä½“ç§¯æ•°æ®ï¼Œé¢„æµ‹å…³é”®è§£å‰–æ ‡å¿—ç‰©çš„ä½ç½®ã€‚</li>
<li>DSTæ¡†æ¶é¢„æµ‹å‡†ç¡®åº¦é«˜ï¼Œç¨³å®šæ€§å¼ºï¼Œå¹³å‡ç»å¯¹è¯¯å·®ä»…ä¸º0.1åˆ‡ç‰‡ã€‚</li>
<li>96%çš„é¢„æµ‹ç»“æœåœ¨Â±1åˆ‡ç‰‡èŒƒå›´å†…ï¼ŒéªŒè¯äº†å…¶é«˜å‡†ç¡®æ€§ã€‚</li>
<li>DSTæ¡†æ¶åœ¨å…¬å…±æ¸…æ´CC-CCIIåˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ï¼Œå±•ç¤ºäº†å…¶å¼ºå¤§çš„æ¶æ„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08214">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf6e7cabed23286284c61393711f8651.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cca625a7cb93eda989029556500f555.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2efd8da70dbbcab033daca9625afd5d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffa5a7f2bf8bd4f79f087145cb1fdab6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f9af8ee7677087f6a2fab25ed00adaff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba452692564cb427fe5cc1873109c415.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="HNOSeg-XS-Extremely-Small-Hartley-Neural-Operator-for-Efficient-and-Resolution-Robust-3D-Image-Segmentation"><a href="#HNOSeg-XS-Extremely-Small-Hartley-Neural-Operator-for-Efficient-and-Resolution-Robust-3D-Image-Segmentation" class="headerlink" title="HNOSeg-XS: Extremely Small Hartley Neural Operator for Efficient and   Resolution-Robust 3D Image Segmentation"></a>HNOSeg-XS: Extremely Small Hartley Neural Operator for Efficient and   Resolution-Robust 3D Image Segmentation</h2><p><strong>Authors:Ken C. L. Wong, Hongzhi Wang, Tanveer Syeda-Mahmood</strong></p>
<p>In medical image segmentation, convolutional neural networks (CNNs) and transformers are dominant. For CNNs, given the local receptive fields of convolutional layers, long-range spatial correlations are captured through consecutive convolutions and pooling. However, as the computational cost and memory footprint can be prohibitively large, 3D models can only afford fewer layers than 2D models with reduced receptive fields and abstract levels. For transformers, although long-range correlations can be captured by multi-head attention, its quadratic complexity with respect to input size is computationally demanding. Therefore, either model may require input size reduction to allow more filters and layers for better segmentation. Nevertheless, given their discrete nature, models trained with patch-wise training or image downsampling may produce suboptimal results when applied on higher resolutions. To address this issue, here we propose the resolution-robust HNOSeg-XS architecture. We model image segmentation by learnable partial differential equations through the Fourier neural operator which has the zero-shot super-resolution property. By replacing the Fourier transform by the Hartley transform and reformulating the problem in the frequency domain, we created the HNOSeg-XS model, which is resolution robust, fast, memory efficient, and extremely parameter efficient. When tested on the BraTSâ€™23, KiTSâ€™23, and MVSegâ€™23 datasets with a Tesla V100 GPU, HNOSeg-XS showed its superior resolution robustness with fewer than 34.7k model parameters. It also achieved the overall best inference time (&lt; 0.24 s) and memory efficiency (&lt; 1.8 GiB) compared to the tested CNN and transformer models. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè½¬æ¢å™¨å æ®ä¸»å¯¼åœ°ä½ã€‚å¯¹äºå·ç§¯ç¥ç»ç½‘ç»œï¼Œç”±äºå…¶å·ç§¯å±‚çš„å±€éƒ¨æ„Ÿå—é‡ï¼Œå¯ä»¥é€šè¿‡è¿ç»­çš„å·ç§¯å’Œæ± åŒ–æ•è·è¿œç¨‹ç©ºé—´ç›¸å…³æ€§ã€‚ç„¶è€Œï¼Œç”±äºè®¡ç®—æˆæœ¬å’Œå†…å­˜å ç”¨å¯èƒ½éå¸¸å¤§ï¼Œ3Dæ¨¡å‹åªèƒ½ä½¿ç”¨æ¯”2Dæ¨¡å‹æ›´å°‘çš„å±‚å’Œå‡å°‘çš„æ„Ÿå—é‡å’ŒæŠ½è±¡å±‚æ¬¡ã€‚å¯¹äºè½¬æ¢å™¨ï¼Œè™½ç„¶å¤šå¤´æ³¨æ„åŠ›å¯ä»¥æ•æ‰è¿œç¨‹ç›¸å…³æ€§ï¼Œä½†å…¶å…³äºè¾“å…¥å¤§å°çš„äºŒæ¬¡å¤æ‚æ€§è®¡ç®—é‡å¤§ã€‚å› æ­¤ï¼Œä¸¤ç§æ¨¡å‹å¯èƒ½éœ€è¦å‡å°è¾“å…¥å¤§å°ï¼Œä»¥ä¾¿ä¸ºæ›´å¥½çš„åˆ†å‰²æ·»åŠ æ›´å¤šçš„è¿‡æ»¤å™¨å’Œå±‚ã€‚ç„¶è€Œï¼Œé‰´äºå®ƒä»¬çš„ç¦»æ•£æ€§è´¨ï¼Œä½¿ç”¨è¡¥ä¸çº§è®­ç»ƒæˆ–å›¾åƒä¸‹é‡‡æ ·è¿›è¡Œè®­ç»ƒçš„æ¨¡å‹åœ¨åº”ç”¨äºæ›´é«˜åˆ†è¾¨ç‡æ—¶å¯èƒ½ä¼šäº§ç”Ÿä¸ç†æƒ³çš„ç»“æœã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†è¾¨ç‡ç¨³å¥çš„HNOSeg-XSæ¶æ„ã€‚æˆ‘ä»¬é€šè¿‡å¯å­¦ä¹ çš„åå¾®åˆ†æ–¹ç¨‹å¯¹å›¾åƒåˆ†å‰²è¿›è¡Œå»ºæ¨¡ï¼Œåˆ©ç”¨å…·æœ‰é›¶ç‚¹è¶…åˆ†è¾¨ç‡å±æ€§çš„å‚…é‡Œå¶ç¥ç»ç®—å­ã€‚é€šè¿‡ç”¨å“ˆç‰¹åˆ©å˜æ¢ä»£æ›¿å‚…é‡Œå¶å˜æ¢å¹¶åœ¨é¢‘åŸŸé‡æ–°è¡¨è¿°é—®é¢˜ï¼Œæˆ‘ä»¬åˆ›å»ºäº†HNOSeg-XSæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·æœ‰åˆ†è¾¨ç‡ç¨³å¥ã€å¿«é€Ÿã€å†…å­˜é«˜æ•ˆå’Œæé«˜çš„å‚æ•°æ•ˆç‡ã€‚åœ¨BraTSâ€™23ã€KiTSâ€™23å’ŒMVSegâ€™23æ•°æ®é›†ä¸Šä½¿ç”¨Tesla V100 GPUè¿›è¡Œæµ‹è¯•æ—¶ï¼ŒHNOSeg-XSæ˜¾ç¤ºäº†å…¶å‡ºè‰²çš„åˆ†è¾¨ç‡ç¨³å¥æ€§ï¼Œæ¨¡å‹å‚æ•°å°‘äº34.7kã€‚ä¸æµ‹è¯•çš„CNNå’Œè½¬æ¢å™¨æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒè¿˜å®ç°äº†æœ€ä½³çš„æ€»æ¨ç†æ—¶é—´ï¼ˆ&lt;0.24ç§’ï¼‰å’Œå†…å­˜æ•ˆç‡ï¼ˆ&lt;1.8 GiBï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08205v1">PDF</a> This paper was accepted by IEEE TMI 2025</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè½¬æ¢å™¨å„æœ‰ä¼˜åŠ¿ã€‚CNNèƒ½å¤Ÿæ•æ‰é•¿è·ç¦»ç©ºé—´å…³è”ï¼Œä½†è®¡ç®—æˆæœ¬å’Œå†…å­˜å ç”¨è¾ƒå¤§ã€‚è€Œè½¬æ¢å™¨è™½ç„¶èƒ½æ•æ‰é•¿è·ç¦»å…³è”ï¼Œä½†å…¶è®¡ç®—å¤æ‚åº¦è¾ƒé«˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†HNOSeg-XSæ¶æ„ï¼Œè¯¥æ¶æ„åˆ©ç”¨å¯å­¦ä¹ çš„åå¾®åˆ†æ–¹ç¨‹å¯¹å›¾åƒè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é‡‡ç”¨å‚…ç«‹å¶ç¥ç»ç®—å­è¿›è¡Œå¤„ç†ã€‚é€šè¿‡å¼•å…¥Hartleyå˜æ¢å¹¶å°†å…¶åº”ç”¨äºé¢‘ç‡åŸŸï¼Œåˆ›å»ºå‡ºå…·æœ‰åˆ†è¾¨ç‡é²æ£’æ€§ã€é«˜æ•ˆå¿«é€Ÿçš„HNOSeg-XSæ¨¡å‹ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼ŒHNOSeg-XSæ¨¡å‹å‚æ•°å°‘ã€æ•ˆç‡é«˜ä¸”æ€§èƒ½ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè½¬æ¢å™¨åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>CNNå’Œè½¬æ¢å™¨æ¨¡å‹å„æœ‰å±€é™ï¼ŒåŒ…æ‹¬è®¡ç®—æˆæœ¬é«˜ã€å†…å­˜å ç”¨å¤§ä»¥åŠè®¡ç®—å¤æ‚åº¦é«˜ç­‰é—®é¢˜ã€‚</li>
<li>æå‡ºçš„HNOSeg-XSæ¶æ„é€šè¿‡åˆ©ç”¨å‚…ç«‹å¶ç¥ç»ç®—å­å’ŒHartleyå˜æ¢è§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>HNOSeg-XSæ¨¡å‹å…·æœ‰åˆ†è¾¨ç‡é²æ£’æ€§ã€é«˜æ•ˆå¿«é€Ÿçš„ç‰¹ç‚¹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08205">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a027828242759f90ead9c46b0dc77838.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ac897f905206502e3e6812fddad2c95.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-54925ddd74fc4336a6a78dc82f59fdec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-54b63119a290658940094f4f08fd2a88.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="BASS-XLVIII-Ne-v-Î»3427-Emission-in-Powerful-Nearby-Active-Galactic-Nuclei"><a href="#BASS-XLVIII-Ne-v-Î»3427-Emission-in-Powerful-Nearby-Active-Galactic-Nuclei" class="headerlink" title="BASS XLVIII: [Ne v] Î»3427 Emission in Powerful Nearby Active   Galactic Nuclei"></a>BASS XLVIII: [Ne v] Î»3427 Emission in Powerful Nearby Active   Galactic Nuclei</h2><p><strong>Authors:Tomer Reiss, Benny Trakhtenbrot, Claudio Ricci, Franz E. Bauer, Michael J. Koss, Kohei Ichikawa, Darshan Kakkad, Richard Mushotzky, Kyuseok Oh, Alessandro Peca, Rudolf BÃ¤r, Yaherlyn Diaz, Fiona Harrison, Meredith C. Powell, Eleonora Sani, Daniel Stern, C. Megan Urry</strong></p>
<p>We investigate the high-ionization, narrow [Ne v] $\lambda$3427 emission line in a sample of over 340 ultrahard X-ray (14-195 keV) selected Active Galactic Nuclei (AGN) drawn from the BASS project. The analysis includes measurements in individual and stacked spectra, and considers several key AGN properties such as X-ray luminosity, supermassive black hole (SMBH) mass, Eddington ratios, and line-of-sight column density. The [Ne v] $\lambda$3427 line is robustly detected in ~43% (146&#x2F;341) of the AGN in our sample, with no significant trends between the detection rate and key AGN&#x2F;SMBH properties. In particular, the detection rate remains high even at the highest levels of obscuration (&gt;70% for log[N_H&#x2F;cm^-2] &gt; 23). On the other hand, even some of our highest signal-to-noise spectra (S&#x2F;N &gt; 50) lack a robust [Ne v] detection. The typical (median) scaling ratios between [Ne v] line emission and (ultra-)hard X-ray emission in our sample are log L[Ne v]&#x2F;L(14-150 keV) &#x3D; -3.75 and log L[Ne v]&#x2F;L(2-10 keV) &#x3D; -3.36. The scatter on these scaling ratios, of ~0.5 dex, is comparable to, and indeed smaller than, what is found for other commonly used tracers of AGN radiative outputs (e.g., [O III] $\lambda$5007). Otherwise, we find no significant relations between the (relative) strength of [Ne v] and the basic AGN&#x2F;SMBH properties under study, in contrast with simple expectations from models of SMBH accretion flows. Our results reaffirm the usability of [Ne v] as an AGN tracer even in highly obscured systems, including dual AGN and high redshift sources. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹æ¥è‡ªBASSé¡¹ç›®çš„è¶…è¿‡340ä¸ªæç«¯ç¡¬Xå°„çº¿ï¼ˆ14-195 keVï¼‰é€‰å®šçš„æ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆActive Galactic Nucleiï¼Œç®€ç§°AGNï¼‰æ ·æœ¬ä¸­çš„é«˜ç”µç¦»ã€çª„[Ne v] Î»3427å‘å°„çº¿è¿›è¡Œäº†ç ”ç©¶ã€‚åˆ†æåŒ…æ‹¬ä¸ªä½“å’Œå åŠ å…‰è°±çš„æµ‹é‡ï¼Œå¹¶è€ƒè™‘äº†å…³é”®AGNå±æ€§ï¼Œä¾‹å¦‚Xå°„çº¿å…‰åº¦ã€è¶…å¤§è´¨é‡é»‘æ´ï¼ˆSMBHï¼‰è´¨é‡ã€çˆ±ä¸é¡¿æ¯”ç‡ä»¥åŠè§†çº¿æ–¹å‘ä¸Šçš„æŸ±å¯†åº¦ã€‚åœ¨æˆ‘ä»¬æ ·æœ¬çš„AGNä¸­ï¼Œ[Ne v] Î»3427çº¿åœ¨çº¦43%ï¼ˆ146&#x2F;341ï¼‰ä¸­ç¨³å¥åœ°æ£€æµ‹åˆ°ï¼Œæ£€æµ‹ç‡ä¸å…³é”®AGN&#x2F;SMBHå±æ€§ä¹‹é—´æ²¡æœ‰æ˜æ˜¾çš„è¶‹åŠ¿ã€‚ç‰¹åˆ«æ˜¯ï¼Œå³ä½¿åœ¨é®è”½ç¨‹åº¦æœ€é«˜çš„åœ°æ–¹ï¼ˆå¯¹äºlog[N_H&#x2F;cm^-2]&gt; 23çš„æƒ…å†µï¼Œé®è”½ç¨‹åº¦å¤§äº70%ï¼‰ï¼Œæ£€æµ‹ç‡ä»ç„¶å¾ˆé«˜ã€‚å¦ä¸€æ–¹é¢ï¼Œå³ä½¿åœ¨æˆ‘ä»¬ä¸€äº›ä¿¡å™ªæ¯”æœ€é«˜çš„å…‰è°±ï¼ˆä¿¡å™ªæ¯”&gt; 50ï¼‰ä¸­ä¹Ÿæ²¡æœ‰æ£€æµ‹åˆ°ç¨³å¥çš„[Ne v]ã€‚æ ·æœ¬ä¸­[Ne v]çº¿å‘å°„ä¸ï¼ˆè¶…ï¼‰ç¡¬Xå°„çº¿å‘å°„ä¹‹é—´çš„å…¸å‹ï¼ˆä¸­ä½æ•°ï¼‰æ¯”ä¾‹å°ºä¸ºlog L[Ne v]&#x2F;L(14-150 keV) &#x3D; -3.75å’Œlog L[Ne v]&#x2F;L(2-10 keV) &#x3D; -3.36ã€‚è¿™äº›æ¯”ä¾‹å°ºçš„æ•£å°„çº¦ä¸º0.5 dexï¼Œä¸å…¶ä»–å¸¸ç”¨äºè¿½è¸ªAGNè¾å°„è¾“å‡ºçš„æŒ‡æ ‡ï¼ˆä¾‹å¦‚ï¼Œ[O III] Î»5007ï¼‰ç›¸å½“ï¼Œç”šè‡³æ›´å°ã€‚å…¶ä»–æ–¹é¢ï¼Œæˆ‘ä»¬æ²¡æœ‰å‘ç°[Ne v]ï¼ˆç›¸å¯¹å¼ºåº¦ï¼‰ä¸æˆ‘ä»¬ç ”ç©¶çš„åŸºæœ¬AGN&#x2F;SMBHå±æ€§ä¹‹é—´çš„æ˜¾è‘—å…³ç³»ï¼Œè¿™ä¸ç®€å•SMBHå¸ç§¯æµæ¨¡å‹çš„é¢„æœŸç›¸åã€‚æˆ‘ä»¬çš„ç»“æœå†æ¬¡è¯å®äº†[Ne v]å³ä½¿åœ¨é«˜åº¦é®è”½çš„ç³»ç»Ÿï¼ˆåŒ…æ‹¬åŒé‡AGNå’Œé«˜çº¢ç§»æºï¼‰ä¸­ä¹Ÿå¯ç”¨ä½œAGNè¿½è¸ªæŒ‡æ ‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08179v1">PDF</a> ApJ, accepted. 24 pages, 9 figures</p>
<p><strong>Summary</strong><br>     ç ”ç©¶æ ·æœ¬ä¸­çš„é«˜ç”µç¦»çª„å‘å°„çº¿Ne v Î»3427åœ¨è¶…è¿‡340ä¸ªæç¡¬Xå°„çº¿é€‰æ‹©çš„æ´»è·ƒæ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰ä¸­çš„è¡¨ç°ã€‚ç ”ç©¶ä¸­åŒ…æ‹¬å¯¹å•ä¸ªå’Œå åŠ å…‰è°±çš„æµ‹é‡ï¼Œå¹¶è€ƒè™‘äº†å…³é”®çš„æ´»è·ƒæ˜Ÿç³»æ ¸ç‰¹æ€§å¦‚Xå°„çº¿å…‰åº¦ã€è¶…å¤§è´¨é‡é»‘æ´ï¼ˆSMBHï¼‰è´¨é‡ã€çˆ±ä¸é¡¿æ¯”å’Œè§†çº¿æ–¹å‘æŸ±å¯†åº¦ç­‰ã€‚Ne v Î»3427çº¿åœ¨çº¦43%ï¼ˆ146&#x2F;341ï¼‰çš„æ´»è·ƒæ˜Ÿç³»æ ¸ä¸­è¢«ç¨³å¥æ£€æµ‹åˆ°ï¼Œæ£€æµ‹ç‡ä¸å…³é”®æ´»è·ƒæ˜Ÿç³»æ ¸&#x2F;è¶…å¤§è´¨é‡é»‘æ´ç‰¹æ€§ä¹‹é—´æ— æ˜¾è‘—è¶‹åŠ¿ã€‚å³ä½¿åœ¨é«˜åº¦é®è”½çš„æƒ…å†µä¸‹ï¼Œæ£€æµ‹ç‡ä»ç„¶å¾ˆé«˜ï¼ˆå¯¹äºlog[N_H&#x2F;cm^-2]&gt; 23ï¼Œé®è”½ç‡è¶…è¿‡70%ï¼‰ã€‚å¦ä¸€æ–¹é¢ï¼Œä¸€äº›ä¿¡å™ªæ¯”æœ€é«˜çš„å…‰è°±ï¼ˆä¿¡å™ªæ¯”&gt; 50ï¼‰ä¹Ÿæ²¡æœ‰æ£€æµ‹åˆ°Ne vã€‚æ ·æœ¬ä¸­Ne vçº¿å‘å°„ä¸ï¼ˆè¶…ï¼‰ç¡¬Xå°„çº¿å‘å°„ä¹‹é—´çš„å…¸å‹æ¯”ä¾‹å°ºå¯¹æ•°å…³ç³»ä¸ºlog L[Ne v]&#x2F;L(14-150 keV) &#x3D; -3.75å’Œlog L[Ne v]&#x2F;L(2-10 keV) &#x3D; -3.36ã€‚è¿™äº›æ¯”ä¾‹å…³ç³»çš„æ•£å°„çº¦ä¸º0.5 dexï¼Œä¸å…¶ä»–å¸¸ç”¨äºè¿½è¸ªæ´»è·ƒæ˜Ÿç³»æ ¸è¾å°„è¾“å‡ºçš„æŒ‡æ ‡ï¼ˆä¾‹å¦‚ï¼Œ[O III] Î»5007ï¼‰ç›¸å½“ç”šè‡³æ›´å°ã€‚æ­¤å¤–ï¼ŒNe vçš„ç›¸å¯¹å¼ºåº¦ä¸ç ”ç©¶çš„æ´»è·ƒæ˜Ÿç³»æ ¸&#x2F;è¶…å¤§è´¨é‡é»‘æ´åŸºæœ¬ç‰¹æ€§ä¹‹é—´æ²¡æœ‰å‘ç°æ˜¾è‘—å…³ç³»ï¼Œè¿™ä¸ç®€å•é¢„æœŸçš„å•æ˜Ÿæ¨¡å‹é»‘æ´å¸ç§¯æµä¸ç¬¦ã€‚ç ”ç©¶ç»“æœå†æ¬¡è¯å®äº†Ne våœ¨é«˜é®è”½ç³»ç»Ÿï¼ˆåŒ…æ‹¬åŒæ´»è·ƒæ˜Ÿç³»æ ¸å’Œé«˜çº¢ç§»æºï¼‰ä¸­ä½œä¸ºæ´»è·ƒæ˜Ÿç³»æ ¸è¿½è¸ªå™¨çš„å¯ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ ·æœ¬åŒ…å«è¶…è¿‡340ä¸ªæç¡¬Xå°„çº¿é€‰æ‹©çš„æ´»è·ƒæ˜Ÿç³»æ ¸ï¼ˆAGNï¼‰ã€‚</li>
<li>[Ne v] Î»3427çº¿åœ¨çº¦43%çš„æ ·æœ¬ä¸­è¢«ç¨³å¥æ£€æµ‹åˆ°ã€‚</li>
<li>æ£€æµ‹ç‡ä¸å…³é”®æ´»è·ƒæ˜Ÿç³»æ ¸&#x2F;è¶…å¤§è´¨é‡é»‘æ´çš„ç‰¹æ€§ä¹‹é—´æ²¡æœ‰æ˜¾è‘—è¶‹åŠ¿ã€‚</li>
<li>åœ¨é«˜åº¦é®è”½çš„æƒ…å†µä¸‹ï¼Œæ£€æµ‹ç‡ä»ç„¶å¾ˆé«˜ã€‚</li>
<li>ä¸€äº›é«˜ä¿¡å™ªæ¯”å…‰è°±æœªèƒ½æ£€æµ‹åˆ°[Ne v]ã€‚</li>
<li>Ne vä¸ç¡¬Xå°„çº¿å‘å°„ä¹‹é—´çš„æ¯”ä¾‹å…³ç³»å…·æœ‰å…¸å‹å¯¹æ•°å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08179">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ea23309a4092ea04015c94218da080bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d145f63f74431486b7b6649d8a1fc84f.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Cracking-Instance-Jigsaw-Puzzles-An-Alternative-to-Multiple-Instance-Learning-for-Whole-Slide-Image-Analysis"><a href="#Cracking-Instance-Jigsaw-Puzzles-An-Alternative-to-Multiple-Instance-Learning-for-Whole-Slide-Image-Analysis" class="headerlink" title="Cracking Instance Jigsaw Puzzles: An Alternative to Multiple Instance   Learning for Whole Slide Image Analysis"></a>Cracking Instance Jigsaw Puzzles: An Alternative to Multiple Instance   Learning for Whole Slide Image Analysis</h2><p><strong>Authors:Xiwen Chen, Peijie Qiu, Wenhui Zhu, Hao Wang, Huayu Li, Xuanzhao Dong, Xiaotong Sun, Xiaobing Yu, Yalin Wang, Abolfazl Razi, Aristeidis Sotiras</strong></p>
<p>While multiple instance learning (MIL) has shown to be a promising approach for histopathological whole slide image (WSI) analysis, its reliance on permutation invariance significantly limits its capacity to effectively uncover semantic correlations between instances within WSIs. Based on our empirical and theoretical investigations, we argue that approaches that are not permutation-invariant but better capture spatial correlations between instances can offer more effective solutions. In light of these findings, we propose a novel alternative to existing MIL for WSI analysis by learning to restore the order of instances from their randomly shuffled arrangement. We term this task as cracking an instance jigsaw puzzle problem, where semantic correlations between instances are uncovered. To tackle the instance jigsaw puzzles, we propose a novel Siamese network solution, which is theoretically justified by optimal transport theory. We validate the proposed method on WSI classification and survival prediction tasks, where the proposed method outperforms the recent state-of-the-art MIL competitors. The code is available at <a target="_blank" rel="noopener" href="https://github.com/xiwenc1/MIL-JigsawPuzzles">https://github.com/xiwenc1/MIL-JigsawPuzzles</a>. </p>
<blockquote>
<p>å°½ç®¡å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰åœ¨ç—…ç†å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†æä¸­æ˜¾ç¤ºå‡ºæœ‰å‰é€”ï¼Œä½†å®ƒå¯¹æ’åˆ—ç»„åˆçš„ä¾èµ–ä¸¥é‡é™åˆ¶äº†å…¶åœ¨WSIså†…å®ä¾‹ä¹‹é—´è¯­ä¹‰å…³è”çš„æœ‰æ•ˆå‘ç°èƒ½åŠ›ã€‚åŸºäºæˆ‘ä»¬çš„ç»éªŒå’Œç†è®ºè°ƒæŸ¥ï¼Œæˆ‘ä»¬ä¸»å¼ é‡‡ç”¨éæ’åˆ—ç»„åˆä¸å˜ä½†èƒ½æ›´å¥½åœ°æ•æ‰å®ä¾‹é—´ç©ºé—´å…³è”çš„æ–¹æ³•ï¼Œä»¥æä¾›æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚é‰´äºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¤šå®ä¾‹å­¦ä¹ æ›¿ä»£æ–¹æ¡ˆï¼Œç”¨äºWSIåˆ†æï¼Œé€šè¿‡å­¦ä¹ æ¢å¤å®ä¾‹çš„æ’åˆ—é¡ºåºæ¥è§£å†³å®é™…é—®é¢˜ã€‚æˆ‘ä»¬å°†æ­¤ä»»åŠ¡ç§°ä¸ºè§£å†³å®ä¾‹æ‹¼å›¾é—®é¢˜ï¼Œå…¶ä¸­æ­ç¤ºäº†å®ä¾‹ä¹‹é—´çš„è¯­ä¹‰å…³è”ã€‚ä¸ºäº†è§£å†³å®ä¾‹æ‹¼å›¾é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„Siameseç½‘ç»œè§£å†³æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåœ¨ç†è®ºä¸Šå¾—åˆ°äº†æœ€ä¼˜ä¼ è¾“ç†è®ºçš„éªŒè¯ã€‚æˆ‘ä»¬åœ¨WSIåˆ†ç±»å’Œç”Ÿå­˜é¢„æµ‹ä»»åŠ¡ä¸ŠéªŒè¯äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼Œå…¶ä¸­æ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºæœ€æ–°çš„æœ€å…ˆè¿›çš„MILç«äº‰å¯¹æ‰‹ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xiwenc1/MIL-JigsawPuzzles%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xiwenc1/MIL-JigsawPuzzlesæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08178v1">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ç—…ç†å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†æçš„å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ–¹æ³•å­˜åœ¨è¯­ä¹‰å…³è”éš¾ä»¥è¯†åˆ«çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºSiameseç½‘ç»œçš„æ–°æ–¹æ³•æ¥è§£å†³å®ä¾‹æ‹¼å›¾é—®é¢˜ï¼Œä»¥æ¢å¤å®ä¾‹çš„é¡ºåºå¹¶æ­ç¤ºè¯­ä¹‰å…³è”ã€‚è¯¥æ–¹æ³•åœ¨WSIåˆ†ç±»å’Œç”Ÿå­˜é¢„æµ‹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºæœ€æ–°MILæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰åœ¨ç—…ç†å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†æä¸­è™½å…·æ½œåŠ›ï¼Œä½†å…¶å¯¹æ’åˆ—çš„ä¾èµ–é™åˆ¶äº†è¯­ä¹‰å…³è”çš„è¯†åˆ«ã€‚</li>
<li>æå‡ºäº†è§£å†³å®ä¾‹æ‹¼å›¾é—®é¢˜çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æ¢å¤å®ä¾‹çš„é¡ºåºå¹¶æ­ç¤ºå…¶è¯­ä¹‰å…³è”ã€‚</li>
<li>æ–¹æ³•åŸºäºSiameseç½‘ç»œç»“æ„ï¼Œå…·æœ‰ç†è®ºä¸Šé€šè¿‡æœ€ä¼˜ä¼ è¾“ç†è®ºæ”¯æŒçš„ä¼˜åŠ¿ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨WSIåˆ†ç±»å’Œç”Ÿå­˜é¢„æµ‹ä»»åŠ¡ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œå¹¶è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>æ–¹æ³•ä»£ç å·²å…¬å¼€ï¼Œä¾¿äºä»–äººä½¿ç”¨ä¸è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºWSIåˆ†ææä¾›äº†æ–°çš„è§†è§’å’Œè§£å†³æ–¹æ¡ˆï¼Œæœ‰åŠ©äºæ¨åŠ¨åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-695a3e0825e1d10350651e115e0acf68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa0aa3ab5e1f8f9b9fbb09388428f7e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1dbb11f025db1bbe8085d3907646fc5e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Understanding-Dataset-Bias-in-Medical-Imaging-A-Case-Study-on-Chest-X-rays"><a href="#Understanding-Dataset-Bias-in-Medical-Imaging-A-Case-Study-on-Chest-X-rays" class="headerlink" title="Understanding Dataset Bias in Medical Imaging: A Case Study on Chest   X-rays"></a>Understanding Dataset Bias in Medical Imaging: A Case Study on Chest   X-rays</h2><p><strong>Authors:Ethan Dack, Chengliang Dai</strong></p>
<p>Recent works have revisited the infamous task &#96;&#96;Name That Datasetâ€™â€™, demonstrating that non-medical datasets contain underlying biases and that the dataset origin task can be solved with high accuracy. In this work, we revisit the same task applied to popular open-source chest X-ray datasets. Medical images are naturally more difficult to release for open-source due to their sensitive nature, which has led to certain open-source datasets being extremely popular for research purposes. By performing the same task, we wish to explore whether dataset bias also exists in these datasets. To extend our work, we apply simple transformations to the datasets, repeat the same task, and perform an analysis to identify and explain any detected biases. Given the importance of AI applications in medical imaging, itâ€™s vital to establish whether modern methods are taking shortcuts or are focused on the relevant pathology. We implement a range of different network architectures on the datasets: NIH, CheXpert, MIMIC-CXR and PadChest. We hope this work will encourage more explainable research being performed in medical imaging and the creation of more open-source datasets in the medical domain. Our code can be found here: <a target="_blank" rel="noopener" href="https://github.com/eedack01/x_ray_ds_bias">https://github.com/eedack01/x_ray_ds_bias</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶é‡æ–°å…³æ³¨äº†â€œå‘½åæ•°æ®é›†â€ä»»åŠ¡ï¼Œè¡¨æ˜éåŒ»å­¦æ•°æ®é›†å­˜åœ¨æ½œåœ¨åè§ï¼Œå¹¶ä¸”å¯ä»¥ä»¥é«˜å‡†ç¡®æ€§è§£å†³æ•°æ®é›†èµ·æºä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°å…³æ³¨åº”ç”¨äºæµè¡Œçš„å¼€æºèƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†çš„ä»»åŠ¡ã€‚ç”±äºåŒ»å­¦å›¾åƒçš„æ•æ„Ÿæ€§ï¼Œå®ƒä»¬è‡ªç„¶æ›´éš¾ä»¥å¼€æºå‘å¸ƒï¼Œè¿™ä¹Ÿå¯¼è‡´æŸäº›å¼€æºæ•°æ®é›†åœ¨ç ”ç©¶é¢†åŸŸéå¸¸å—æ¬¢è¿ã€‚é€šè¿‡æ‰§è¡Œç›¸åŒçš„ä»»åŠ¡ï¼Œæˆ‘ä»¬å¸Œæœ›æ¢ç´¢è¿™äº›æ•°æ®é›†æ˜¯å¦ä¹Ÿå­˜åœ¨æ•°æ®é›†åè§ã€‚ä¸ºäº†æ‰©å±•æˆ‘ä»¬çš„å·¥ä½œï¼Œæˆ‘ä»¬å¯¹æ•°æ®é›†è¿›è¡Œäº†ç®€å•çš„è½¬æ¢ï¼Œé‡å¤ç›¸åŒçš„ä»»åŠ¡ï¼Œå¹¶è¿›è¡Œåˆ†æä»¥è¯†åˆ«å’Œè§£é‡Šä»»ä½•æ£€æµ‹åˆ°çš„åè§ã€‚é‰´äºäººå·¥æ™ºèƒ½åœ¨åŒ»å­¦å½±åƒé¢†åŸŸçš„åº”ç”¨é‡è¦æ€§ï¼Œç¡®å®šç°ä»£æ–¹æ³•æ˜¯èµ°æ·å¾„è¿˜æ˜¯ä¸“æ³¨äºç›¸å…³ç—…ç†å­¦è‡³å…³é‡è¦ã€‚æˆ‘ä»¬åœ¨NIHã€CheXpertã€MIMIC-CXRå’ŒPadChestæ•°æ®é›†ä¸Šå®ç°äº†å¤šç§ä¸åŒçš„ç½‘ç»œæ¶æ„ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œå°†é¼“åŠ±åœ¨åŒ»å­¦å½±åƒé¢†åŸŸè¿›è¡Œæ›´å¤šå¯è§£é‡Šçš„ç ”ç©¶ï¼Œå¹¶åœ¨åŒ»å­¦é¢†åŸŸåˆ›å»ºæ›´å¤šçš„å¼€æºæ•°æ®é›†ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/eedack01/x_ray_ds_bias%E3%80%82">https://github.com/eedack01/x_ray_ds_biasã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07722v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†â€œName That Datasetâ€ä»»åŠ¡åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æºèƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ä¸Šçš„åº”ç”¨ã€‚ä½œè€…æ¢ç´¢äº†è¿™äº›æ•°æ®é›†æ˜¯å¦å­˜åœ¨æ•°æ®åè§ï¼Œé€šè¿‡åº”ç”¨ç®€å•çš„è½¬æ¢ï¼Œé‡å¤ä»»åŠ¡å¹¶è¿›è¡Œåˆ†ææ¥è¯†åˆ«å’Œè§£é‡Šä»»ä½•æ£€æµ‹åˆ°çš„åè§ã€‚æœ¬æ–‡çš„ç ”ç©¶å¯¹äºåŒ»å­¦æˆåƒä¸­äººå·¥æ™ºèƒ½åº”ç”¨çš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ï¼Œé¼“åŠ±æ›´å¤šå¯è§£é‡Šçš„ç ”ç©¶åœ¨è¯¥é¢†åŸŸçš„å¼€å±•å’Œæ›´å¤šåŒ»å­¦å¼€æºæ•°æ®é›†çš„åˆ›å»ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶é‡æ–°å®¡è§†äº†â€œName That Datasetâ€ä»»åŠ¡åœ¨åŒ»å­¦å›¾åƒé¢†åŸŸçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å¼€æºèƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ã€‚</li>
<li>ä½œè€…å‘ç°éåŒ»ç–—æ•°æ®é›†å­˜åœ¨æ½œåœ¨çš„åè§ï¼Œæœ¬å·¥ä½œæ—¨åœ¨æ¢ç´¢åŒ»ç–—å›¾åƒæ•°æ®é›†æ˜¯å¦ä¹Ÿå­˜åœ¨åè§ã€‚</li>
<li>ç”±äºåŒ»ç–—å›¾åƒçš„æ•æ„Ÿæ€§ï¼ŒæŸäº›å¼€æºæ•°æ®é›†æˆä¸ºç ”ç©¶ç›®çš„ä¸‹çš„æåº¦çƒ­é—¨èµ„æºã€‚</li>
<li>ä½œè€…é€šè¿‡ä½¿ç”¨å¤šç§ç½‘ç»œæ¶æ„ï¼ˆå¦‚NIHã€CheXpertã€MIMIC-CXRå’ŒPadChestï¼‰æ¥å®æ–½ä»»åŠ¡å¹¶åˆ†ææ•°æ®åè§ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†AIåœ¨åŒ»å­¦æˆåƒä¸­çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºéœ€è¦ç¡®å®šç°ä»£æ–¹æ³•æ˜¯å¦èµ°æ·å¾„æˆ–ä¸“æ³¨äºç›¸å…³ç—…ç†å­¦ã€‚</li>
<li>ä½œè€…é€šè¿‡ç®€å•çš„æ•°æ®é›†è½¬æ¢æ¥æ‰©å±•å·¥ä½œï¼Œå¹¶é‡å¤ä»»åŠ¡ä»¥è¯†åˆ«å’Œè§£é‡Šä»»ä½•æ£€æµ‹åˆ°çš„åè§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07722">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-aef675a9ed9290fb5b2aeed0253d6210.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5c5827e5f40ec7764635cf4bb8654e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5b956ede55b85140bb02db9e8427ee0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9fd039d1cfcaf8e8eedd889b4e02fcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8300b9fe11c283dbfcee0fc01f9d1f9.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Average-Calibration-Losses-for-Reliable-Uncertainty-in-Medical-Image-Segmentation"><a href="#Average-Calibration-Losses-for-Reliable-Uncertainty-in-Medical-Image-Segmentation" class="headerlink" title="Average Calibration Losses for Reliable Uncertainty in Medical Image   Segmentation"></a>Average Calibration Losses for Reliable Uncertainty in Medical Image   Segmentation</h2><p><strong>Authors:Theodore Barfoot, Luis C. Garcia-Peraza-Herrera, Samet Akcay, Ben Glocker, Tom Vercauteren</strong></p>
<p>Deep neural networks for medical image segmentation are often overconfident, compromising both reliability and clinical utility. In this work, we propose differentiable formulations of marginal L1 Average Calibration Error (mL1-ACE) as an auxiliary loss that can be computed on a per-image basis. We compare both hard- and soft-binning approaches to directly improve pixel-wise calibration. Our experiments on four datasets (ACDC, AMOS, KiTS, BraTS) demonstrate that incorporating mL1-ACE significantly reduces calibration errors, particularly Average Calibration Error (ACE) and Maximum Calibration Error (MCE), while largely maintaining high Dice Similarity Coefficients (DSCs). We find that the soft-binned variant yields the greatest improvements in calibration, over the Dice plus cross-entropy loss baseline, but often compromises segmentation performance, with hard-binned mL1-ACE maintaining segmentation performance, albeit with weaker calibration improvement. To gain further insight into calibration performance and its variability across an imaging dataset, we introduce dataset reliability histograms, an aggregation of per-image reliability diagrams. The resulting analysis highlights improved alignment between predicted confidences and true accuracies. Overall, our approach not only enhances the trustworthiness of segmentation predictions but also shows potential for safer integration of deep learning methods into clinical workflows. We share our code here: <a target="_blank" rel="noopener" href="https://github.com/cai4cai/Average-Calibration-Losses">https://github.com/cai4cai/Average-Calibration-Losses</a> </p>
<blockquote>
<p>é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ·±åº¦ç¥ç»ç½‘ç»œå¾€å¾€è¿‡äºè‡ªä¿¡ï¼Œè¿™æ—¢å½±å“å¯é æ€§åˆå½±å“ä¸´åºŠå®ç”¨æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è¾¹é™…L1å¹³å‡æ ¡å‡†è¯¯å·®ï¼ˆmL1-ACEï¼‰çš„å¯å¾®å…¬å¼ï¼Œä½œä¸ºä¸€ç§å¯ä»¥æŒ‰å›¾åƒè®¡ç®—çš„è¾…åŠ©æŸå¤±ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ç¡¬åˆ†ç®±å’Œè½¯åˆ†ç®±çš„æ–¹æ³•ï¼Œä»¥ç›´æ¥æ”¹å–„åƒç´ çº§æ ¡å‡†ã€‚æˆ‘ä»¬åœ¨å››ä¸ªæ•°æ®é›†ï¼ˆACDCã€AMOSã€KiTSã€BraTSï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå¼•å…¥mL1-ACEå¯ä»¥æ˜¾è‘—é™ä½æ ¡å‡†è¯¯å·®ï¼Œç‰¹åˆ«æ˜¯å¹³å‡æ ¡å‡†è¯¯å·®ï¼ˆACEï¼‰å’Œæœ€å¤§æ ¡å‡†è¯¯å·®ï¼ˆMCEï¼‰ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸DiceåŠ ä¸Šäº¤å‰ç†µæŸå¤±çš„åŸºçº¿ç›¸æ¯”ï¼Œè½¯åˆ†ç®±å˜ä½“åœ¨æ ¡å‡†æ–¹é¢å–å¾—äº†æœ€å¤§çš„æ”¹è¿›ï¼Œä½†å¾€å¾€ç‰ºç‰²äº†åˆ†å‰²æ€§èƒ½ï¼Œç¡¬åˆ†ç®±mL1-ACEè™½ç„¶åœ¨æ ¡å‡†æ”¹å–„æ–¹é¢è¡¨ç°è¾ƒå¼±ï¼Œä½†èƒ½å¤Ÿä¿æŒåˆ†å‰²æ€§èƒ½ã€‚ä¸ºäº†æ·±å…¥äº†è§£æ ¡å‡†æ€§èƒ½åŠå…¶åœ¨æˆåƒæ•°æ®é›†ä¸Šçš„å˜åŒ–ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ•°æ®é›†å¯é æ€§ç›´æ–¹å›¾ï¼Œè¿™æ˜¯æŒ‰å›¾åƒå¯é æ€§å›¾è¿›è¡Œèšåˆçš„ç»“æœã€‚ç”±æ­¤äº§ç”Ÿçš„åˆ†æå¼ºè°ƒäº†é¢„æµ‹ç½®ä¿¡åº¦ä¸çœŸå®å‡†ç¡®åº¦ä¹‹é—´å¯¹é½çš„æ”¹è¿›ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æé«˜äº†åˆ†å‰²é¢„æµ‹çš„å¯é æ€§ï¼Œè¿˜æ˜¾ç¤ºå‡ºå°†æ·±åº¦å­¦ä¹ æ–¹æ³•æ›´å®‰å…¨åœ°é›†æˆåˆ°ä¸´åºŠå·¥ä½œæµç¨‹ä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬åœ¨æ­¤åˆ†äº«æˆ‘ä»¬çš„ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/cai4cai/Average-Calibration-Losses">https://github.com/cai4cai/Average-Calibration-Losses</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.03942v2">PDF</a> 12 pages, 5 figures, IEEE TMI submission. This version originally   appeared in error as arXiv:2403.06759(v2)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨æ·±åº¦ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„è¿‡åº¦è‡ªä¿¡é—®é¢˜ï¼Œå¹¶æå‡ºå¯å¾®åˆ†çš„è¾¹é™…L1å¹³å‡æ ¡å‡†è¯¯å·®ï¼ˆmL1-ACEï¼‰ä½œä¸ºè¾…åŠ©æŸå¤±å‡½æ•°ï¼Œä»¥æ”¹å–„åƒç´ çº§çš„æ ¡å‡†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¼•å…¥mL1-ACEèƒ½æ˜¾è‘—é™ä½æ ¡å‡†è¯¯å·®ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„Diceç›¸ä¼¼ç³»æ•°ã€‚è½¯åˆ†ç®±mL1-ACEåœ¨æ ¡å‡†æ–¹é¢æ”¹è¿›æœ€å¤§ï¼Œä½†å¯èƒ½å½±å“åˆ†å‰²æ€§èƒ½ï¼›ç¡¬åˆ†ç®±mL1-ACEåˆ™èƒ½ç»´æŒåˆ†å‰²æ€§èƒ½ï¼Œå°½ç®¡æ ¡å‡†æ”¹è¿›è¾ƒå¼±ã€‚æ­¤å¤–ï¼Œå¼•å…¥æ•°æ®é›†å¯é æ€§ç›´æ–¹å›¾ä»¥è¿›ä¸€æ­¥åˆ†ææ ¡å‡†æ€§èƒ½åŠå…¶åœ¨æ•°æ®é›†é—´çš„å˜åŒ–ï¼Œæé«˜äº†é¢„æµ‹ç½®ä¿¡åº¦ä¸çœŸå®å‡†ç¡®åº¦çš„å¯¹é½æ€§ï¼Œå¢å¼ºäº†åˆ†å‰²é¢„æµ‹çš„å¯ä¿¡åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¸¸è¡¨ç°å‡ºè¿‡åº¦è‡ªä¿¡ï¼Œå½±å“å¯é æ€§å’Œä¸´åºŠå®ç”¨æ€§ã€‚</li>
<li>å¼•å…¥å¯å¾®åˆ†çš„è¾¹é™…L1å¹³å‡æ ¡å‡†è¯¯å·®ï¼ˆmL1-ACEï¼‰ä½œä¸ºè¾…åŠ©æŸå¤±å‡½æ•°ï¼Œæ—¨åœ¨æ”¹å–„åƒç´ çº§æ ¡å‡†ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒmL1-ACEèƒ½æ˜¾è‘—é™ä½æ ¡å‡†è¯¯å·®ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„Diceç›¸ä¼¼ç³»æ•°ã€‚</li>
<li>è½¯åˆ†ç®±å’Œç¡¬åˆ†ç®±æ–¹æ³•ç”¨äºç›´æ¥æ”¹è¿›åƒç´ çº§æ ¡å‡†ï¼Œå…¶ä¸­è½¯åˆ†ç®±åœ¨æ ¡å‡†æ–¹é¢æ”¹è¿›æœ€å¤§ã€‚</li>
<li>å¼•å…¥æ•°æ®é›†å¯é æ€§ç›´æ–¹å›¾æ¥åˆ†ææ ¡å‡†æ€§èƒ½åŠå…¶åœ¨æ•°æ®é›†é—´çš„å˜åŒ–ã€‚</li>
<li>æ­¤æ–¹æ³•æé«˜äº†é¢„æµ‹ç½®ä¿¡åº¦ä¸çœŸå®å‡†ç¡®åº¦çš„å¯¹é½æ€§ï¼Œå¢å¼ºäº†åˆ†å‰²é¢„æµ‹çš„å¯ä¿¡åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.03942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0cdfcee7bfacc32ebcad87f433351f7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9d6ad7c0dd2036cb14bf4af38aa6af2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b92c85046bac61be54c371272062d92f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9427cce65b7878cf6ea8e534a2d40b53.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MedSegFactory-Text-Guided-Generation-of-Medical-Image-Mask-Pairs"><a href="#MedSegFactory-Text-Guided-Generation-of-Medical-Image-Mask-Pairs" class="headerlink" title="MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs"></a>MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs</h2><p><strong>Authors:Jiawei Mao, Yuhan Wang, Yucheng Tang, Daguang Xu, Kang Wang, Yang Yang, Zongwei Zhou, Yuyin Zhou</strong></p>
<p>This paper presents MedSegFactory, a versatile medical synthesis framework that generates high-quality paired medical images and segmentation masks across modalities and tasks. It aims to serve as an unlimited data repository, supplying image-mask pairs to enhance existing segmentation tools. The core of MedSegFactory is a dual-stream diffusion model, where one stream synthesizes medical images and the other generates corresponding segmentation masks. To ensure precise alignment between image-mask pairs, we introduce Joint Cross-Attention (JCA), enabling a collaborative denoising paradigm by dynamic cross-conditioning between streams. This bidirectional interaction allows both representations to guide each otherâ€™s generation, enhancing consistency between generated pairs. MedSegFactory unlocks on-demand generation of paired medical images and segmentation masks through user-defined prompts that specify the target labels, imaging modalities, anatomical regions, and pathological conditions, facilitating scalable and high-quality data generation. This new paradigm of medical image synthesis enables seamless integration into diverse medical imaging workflows, enhancing both efficiency and accuracy. Extensive experiments show that MedSegFactory generates data of superior quality and usability, achieving competitive or state-of-the-art performance in 2D and 3D segmentation tasks while addressing data scarcity and regulatory constraints. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†MedSegFactoryï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„åŒ»å­¦åˆæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆè·¨æ¨¡æ€å’Œä»»åŠ¡çš„é…å¯¹é«˜è´¨é‡åŒ»å­¦å›¾åƒå’Œåˆ†å‰²æ©è†œã€‚å®ƒçš„ç›®æ ‡æ˜¯ä½œä¸ºä¸€ä¸ªæ— é™çš„æ•°æ®å­˜å‚¨åº“ï¼Œä¸ºç°æœ‰çš„åˆ†å‰²å·¥å…·æä¾›å›¾åƒ-æ©è†œå¯¹ï¼Œä»¥å¢å¼ºå…¶åŠŸèƒ½ã€‚MedSegFactoryçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªåŒæµæ‰©æ•£æ¨¡å‹ï¼Œå…¶ä¸­ä¸€ä¸ªæµè´Ÿè´£åˆæˆåŒ»å­¦å›¾åƒï¼Œå¦ä¸€ä¸ªæµç”Ÿæˆç›¸åº”çš„åˆ†å‰²æ©è†œã€‚ä¸ºäº†ç¡®ä¿å›¾åƒ-æ©è†œå¯¹ä¹‹é—´çš„ç²¾ç¡®å¯¹é½ï¼Œæˆ‘ä»¬å¼•å…¥äº†è”åˆäº¤å‰æ³¨æ„åŠ›ï¼ˆJCAï¼‰æœºåˆ¶ï¼Œé€šè¿‡æµä¹‹é—´çš„åŠ¨æ€äº¤å‰æ¡ä»¶ï¼Œå®ç°ååŒå»å™ªèŒƒå¼ã€‚è¿™ç§åŒå‘äº¤äº’å…è®¸ä¸¤ç§è¡¨ç¤ºå½¢å¼ç›¸äº’å¼•å¯¼ç”Ÿæˆï¼Œå¢å¼ºäº†ç”Ÿæˆå¯¹ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚MedSegFactoryé€šè¿‡ç”¨æˆ·å®šä¹‰çš„æç¤ºï¼ˆæŒ‡å®šç›®æ ‡æ ‡ç­¾ã€æˆåƒæ¨¡å¼ã€è§£å‰–åŒºåŸŸå’Œç—…ç†çŠ¶å†µï¼‰è§£é”æŒ‰éœ€ç”Ÿæˆé…å¯¹åŒ»å­¦å›¾åƒå’Œåˆ†å‰²æ©è†œçš„èƒ½åŠ›ï¼Œä¿ƒè¿›äº†å¯æ‰©å±•æ€§å’Œé«˜è´¨é‡çš„æ•°æ®ç”Ÿæˆã€‚è¿™ç§æ–°çš„åŒ»å­¦å›¾åƒåˆæˆèŒƒå¼èƒ½å¤Ÿæ— ç¼åœ°èå…¥å„ç§åŒ»å­¦æˆåƒå·¥ä½œæµç¨‹ä¸­ï¼Œæé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMedSegFactoryç”Ÿæˆçš„æ•°æ®å…·æœ‰å“è¶Šçš„è´¨é‡å’Œå¯ç”¨æ€§ï¼Œåœ¨äºŒç»´å’Œä¸‰ç»´åˆ†å‰²ä»»åŠ¡ä¸­è¾¾åˆ°äº†ç«äº‰æˆ–æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶è§£å†³äº†æ•°æ®ç¨€ç¼ºå’Œç›‘ç®¡çº¦æŸçš„é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06897v2">PDF</a> 12 pages, 8 figures, The project page can be accessed via   <a target="_blank" rel="noopener" href="https://jwmao1.github.io/MedSegFactory_web">https://jwmao1.github.io/MedSegFactory_web</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>MedSegFactoryæ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½åŒ»å­¦åˆæˆæ¡†æ¶ï¼Œå¯ç”Ÿæˆè·¨æ¨¡æ€å’Œä»»åŠ¡çš„é…å¯¹é«˜è´¨é‡åŒ»å­¦å›¾åƒå’Œåˆ†å‰²æ©è†œã€‚æ—¨åœ¨ä½œä¸ºæ— é™çš„æ•°æ®å­˜å‚¨åº“ï¼Œä¸ºç°æœ‰åˆ†å‰²å·¥å…·æä¾›å›¾åƒ-æ©è†œå¯¹ä»¥å¢å¼ºå…¶åŠŸèƒ½ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªåŒæµæ‰©æ•£æ¨¡å‹ï¼Œå…¶ä¸­ä¸€æµåˆæˆåŒ»å­¦å›¾åƒï¼Œå¦ä¸€æµç”Ÿæˆç›¸åº”çš„åˆ†å‰²æ©è†œã€‚ä¸ºç¡®ä¿å›¾åƒ-æ©è†œå¯¹ä¹‹é—´çš„ç²¾ç¡®å¯¹é½ï¼Œå¼•å…¥äº†è”åˆäº¤å‰æ³¨æ„ï¼ˆJCAï¼‰æœºåˆ¶ï¼Œé€šè¿‡åŒæµé—´çš„åŠ¨æ€äº¤å‰æ¡ä»¶å®ç°ååŒå»å™ªèŒƒå¼ã€‚è¿™ç§åŒå‘äº¤äº’ä½¿ä¸¤ç§è¡¨ç¤ºèƒ½å¤Ÿç›¸äº’å¼•å¯¼ç”Ÿæˆï¼Œå¢å¼ºç”Ÿæˆå¯¹ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚MedSegFactoryé€šè¿‡ç”¨æˆ·å®šä¹‰æç¤ºï¼Œè§£é”æŒ‰éœ€ç”Ÿæˆé…å¯¹åŒ»å­¦å›¾åƒå’Œåˆ†å‰²æ©è†œçš„èƒ½åŠ›ï¼Œè¿™äº›æç¤ºæŒ‡å®šç›®æ ‡æ ‡ç­¾ã€æˆåƒæ¨¡æ€ã€è§£å‰–åŒºåŸŸå’Œç—…ç†çŠ¶å†µï¼Œä¿ƒè¿›å¯æ‰©å±•æ€§å’Œé«˜è´¨é‡çš„æ•°æ®ç”Ÿæˆã€‚è¿™ç§æ–°çš„åŒ»å­¦å›¾åƒåˆæˆèŒƒå¼èƒ½å¤Ÿæ— ç¼èå…¥å¤šæ ·åŒ–çš„åŒ»å­¦æˆåƒå·¥ä½œæµç¨‹ï¼Œæé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMedSegFactoryç”Ÿæˆçš„æ•°æ®è´¨é‡é«˜ã€å®ç”¨æ€§å¼ºï¼Œåœ¨äºŒç»´å’Œä¸‰ç»´åˆ†å‰²ä»»åŠ¡ä¸­è¾¾åˆ°æˆ–ä¿æŒç«äº‰å‰æ²¿çš„æ€§èƒ½ï¼ŒåŒæ—¶è§£å†³æ•°æ®ç¨€ç¼ºå’Œç›‘ç®¡çº¦æŸé—®é¢˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>MedSegFactoryæ˜¯ä¸€ä¸ªåŒ»å­¦åˆæˆæ¡†æ¶ï¼Œèƒ½ç”Ÿæˆé«˜è´¨é‡è·¨æ¨¡æ€å’Œä»»åŠ¡é…å¯¹çš„åŒ»å­¦å›¾åƒå’Œåˆ†å‰²æ©è†œã€‚</li>
<li>å®ƒä½œä¸ºä¸€ä¸ªæ— é™æ•°æ®ä»“åº“ï¼Œæ—¨åœ¨å¢å¼ºç°æœ‰åˆ†å‰²å·¥å…·çš„åŠŸèƒ½ã€‚</li>
<li>åŒæµæ‰©æ•£æ¨¡å‹æ˜¯MedSegFactoryçš„æ ¸å¿ƒï¼Œå…¶ä¸­åŒ…æ‹¬å›¾åƒåˆæˆå’Œåˆ†å‰²æ©è†œç”Ÿæˆä¸¤ä¸ªæµã€‚</li>
<li>å¼•å…¥çš„Joint Cross-Attentionæœºåˆ¶ç¡®ä¿äº†å›¾åƒå’Œæ©è†œä¹‹é—´çš„ç²¾ç¡®å¯¹é½ã€‚</li>
<li>è¿™ç§åˆæˆæ–¹æ³•å…è®¸é€šè¿‡ç”¨æˆ·å®šä¹‰çš„æç¤ºï¼ˆå¦‚ç›®æ ‡æ ‡ç­¾ã€æˆåƒæ¨¡æ€ç­‰ï¼‰æŒ‰éœ€ç”Ÿæˆæ•°æ®ã€‚</li>
<li>MedSegFactoryåœ¨äºŒç»´å’Œä¸‰ç»´åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè§£å†³äº†æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚</li>
<li>ç”Ÿæˆçš„å›¾åƒå’Œæ©è†œå…·æœ‰é«˜åº¦çš„å®ç”¨æ€§å’Œè´¨é‡ï¼Œèƒ½å¤Ÿæ»¡è¶³å„ç§åŒ»å­¦æˆåƒå·¥ä½œæµç¨‹çš„éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06897">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d5225258513b6c9f19326f069835bb49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5136966fc5c824d9fe3cd8414dda1962.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b47aacb690f87bcd65792c86ab8b4478.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a24d0c123339caebfbdb3e5848748b4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7aaa7fe5f944b330aa02b35a45dd3ea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-01f1cbbd073c3947c37438e5972d7cbf.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="UWarp-A-Whole-Slide-Image-Registration-Pipeline-to-Characterize-Scanner-Induced-Local-Domain-Shift"><a href="#UWarp-A-Whole-Slide-Image-Registration-Pipeline-to-Characterize-Scanner-Induced-Local-Domain-Shift" class="headerlink" title="UWarp: A Whole Slide Image Registration Pipeline to Characterize   Scanner-Induced Local Domain Shift"></a>UWarp: A Whole Slide Image Registration Pipeline to Characterize   Scanner-Induced Local Domain Shift</h2><p><strong>Authors:Antoine Schieb, Bilal Hadjadji, Natalia Fernanda Valderrama, Daniel Tshokola Mweze, Valentin DerangÃ¨re, Laurent Arnould, Sylvain Ladoire, Alain Lalande, Alessio Fiorin, Carlos LÃ³pez Pablo, NoÃ¨lia Gallardo BorrÃ s, Shrief Abdelazeez, Vincenzo Della Mea, Anna Korzynska, Louis-Oscar Morel, Nathan VinÃ§on</strong></p>
<p>Histopathology slide digitization introduces scanner-induced domain shift that can significantly impact computational pathology models based on deep learning methods. In the state-of-the-art, this shift is often characterized at a broad scale (slide-level or dataset-level) but not patch-level, which limits our comprehension of the impact of localized tissue characteristics on the accuracy of the deep learning models. To address this challenge, we present a domain shift analysis framework based on UWarp, a novel registration tool designed to accurately align histological slides scanned under varying conditions. UWarp employs a hierarchical registration approach, combining global affine transformations with fine-grained local corrections to achieve robust tissue patch alignment. We evaluate UWarp using two private datasets, CypathLung and BosomShieldBreast, containing whole slide images scanned by multiple devices. Our experiments demonstrate that UWarp outperforms existing open-source registration methods, achieving a median target registration error (TRE) of less than 4 pixels (&lt;1 micrometer at 40x magnification) while significantly reducing computational time. Additionally, we apply UWarp to characterize scanner-induced local domain shift in the predictions of Breast-NEOprAIdict, a deep learning model for breast cancer pathological response prediction. We find that prediction variability is strongly correlated with tissue density on a given patch. Our findings highlight the importance of localized domain shift analysis and suggest that UWarp can serve as a valuable tool for improving model robustness and domain adaptation strategies in computational pathology. </p>
<blockquote>
<p>ç—…ç†åˆ‡ç‰‡æ•°å­—åŒ–å¼•å…¥äº†ç”±æ‰«æä»ªå¼•èµ·çš„é¢†åŸŸåç§»ï¼Œè¿™å¯èƒ½ä¼šæ˜¾è‘—å½±å“åŸºäºæ·±åº¦å­¦ä¹ æ–¹æ³•çš„è®¡ç®—ç—…ç†å­¦æ¨¡å‹ã€‚åœ¨æœ€æ–°ç ”ç©¶ä¸­ï¼Œè¿™ç§åç§»é€šå¸¸åœ¨å¤§è§„æ¨¡ï¼ˆå¹»ç¯ç‰‡çº§åˆ«æˆ–æ•°æ®é›†çº§åˆ«ï¼‰ä¸Šè¢«æè¿°ï¼Œè€Œä¸æ˜¯åœ¨è¡¥ä¸çº§åˆ«ä¸Šï¼Œè¿™é™åˆ¶äº†æˆ‘ä»¬å¯¹å±€éƒ¨ç»„ç»‡ç‰¹æ€§å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹å‡†ç¡®æ€§å½±å“çš„ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºUWarpçš„é¢†åŸŸåç§»åˆ†ææ¡†æ¶ã€‚UWarpæ˜¯ä¸€ä¸ªæ–°çš„æ³¨å†Œå·¥å…·ï¼Œæ—¨åœ¨å‡†ç¡®åœ°å¯¹ä¸åŒæ¡ä»¶ä¸‹æ‰«æçš„ç»„ç»‡åˆ‡ç‰‡è¿›è¡Œå¯¹é½ã€‚UWarpé‡‡ç”¨åˆ†å±‚æ³¨å†Œæ–¹æ³•ï¼Œç»“åˆå…¨å±€ä»¿å°„å˜æ¢å’Œç²¾ç»†çš„å±€éƒ¨æ ¡æ­£ï¼Œå®ç°ç¨³å¥çš„ç»„ç»‡è¡¥ä¸å¯¹é½ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªç§æœ‰æ•°æ®é›†CypathLungå’ŒBosomShieldBreastï¼ˆåŒ…å«ç”±å¤šå°è®¾å¤‡æ‰«æçš„æ•´å¼ å¹»ç¯ç‰‡å›¾åƒï¼‰å¯¹UWarpè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼ŒUWarpåœ¨ç›®æ ‡æ³¨å†Œè¯¯å·®ï¼ˆTREï¼‰æ–¹é¢ä¼˜äºç°æœ‰çš„å¼€æºæ³¨å†Œæ–¹æ³•ï¼Œè¯¯å·®ä¸­ä½æ•°ä½äº4åƒç´ ï¼ˆåœ¨40å€æ”¾å¤§ç‡ä¸‹å°äº1å¾®ç±³ï¼‰ï¼ŒåŒæ—¶å¤§å¤§å‡å°‘äº†è®¡ç®—æ—¶é—´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†UWarpåº”ç”¨äºè¡¨å¾æ‰«æä»ªå¼•èµ·çš„å±€éƒ¨é¢†åŸŸåç§»å¯¹ä¹³è…ºç™Œç—…ç†ååº”é¢„æµ‹æ¨¡å‹Breast-NEOprAIdicté¢„æµ‹ç»“æœçš„å½±å“ã€‚æˆ‘ä»¬å‘ç°é¢„æµ‹çš„å¯å˜æ€§ä¸ç»™å®šè¡¥ä¸ä¸Šçš„ç»„ç»‡å¯†åº¦å¯†åˆ‡ç›¸å…³ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†å±€éƒ¨é¢†åŸŸåç§»åˆ†æçš„é‡è¦æ€§ï¼Œå¹¶è¡¨æ˜UWarpå¯ä»¥ä½œä¸ºæé«˜è®¡ç®—ç—…ç†å­¦æ¨¡å‹çš„ç¨³å¥æ€§å’Œé¢†åŸŸé€‚åº”ç­–ç•¥çš„æœ‰ä»·å€¼å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20653v3">PDF</a> preprint</p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è®¡ç®—ç—…ç†å­¦ä¸Šå—åˆ°ç—…ç†åˆ‡ç‰‡æ•°å­—åŒ–æ‰«æå¼•èµ·çš„é¢†åŸŸæ¼‚ç§»å½±å“ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦åœ¨åˆ‡ç‰‡æˆ–æ•°æ®é›†å±‚é¢è¿›è¡Œåˆ†æï¼Œç¼ºä¹å¯¹æ–‘å—çº§åˆ«çš„ç ”ç©¶ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªåŸºäºUWarpçš„é¢†åŸŸæ¼‚ç§»åˆ†ææ¡†æ¶ï¼Œé‡‡ç”¨å¤šå±‚æ¬¡æ³¨å†Œæ–¹æ³•å®ç°ç»„ç»‡æ–‘å—çš„ç²¾ç¡®å¯¹é½ã€‚å®éªŒè¯æ˜UWarpä¼˜äºç°æœ‰å¼€æºæ³¨å†Œæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜é¢„æµ‹æ¨¡å‹çš„ç¨³å¥æ€§ã€‚åŒæ—¶å‘ç°é¢„æµ‹å˜é‡çš„å˜åŒ–ä¸ç‰¹å®šæ–‘å—çš„ç»„ç»‡å¯†åº¦ç´§å¯†ç›¸å…³ã€‚å¼ºè°ƒå±€éƒ¨é¢†åŸŸæ¼‚ç§»åˆ†æçš„é‡è¦æ€§ï¼ŒUWarpæœ‰æœ›æˆä¸ºæ”¹å–„æ¨¡å‹ç¨³å¥æ€§å’Œé¢†åŸŸé€‚åº”ç­–ç•¥çš„æœ‰åŠ›å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç—…ç†åˆ‡ç‰‡æ•°å­—åŒ–æ‰«æå¼•å…¥é¢†åŸŸæ¼‚ç§»ï¼Œå½±å“æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è®¡ç®—ç—…ç†å­¦ä¸Šçš„åº”ç”¨ã€‚</li>
<li>é¢†åŸŸæ¼‚ç§»åˆ†æé€šå¸¸åœ¨åˆ‡ç‰‡æˆ–æ•°æ®é›†å±‚é¢è¿›è¡Œï¼Œç¼ºä¹å¯¹æ–‘å—çº§åˆ«çš„ç ”ç©¶ã€‚</li>
<li>æå‡ºåŸºäºUWarpçš„é¢†åŸŸæ¼‚ç§»åˆ†ææ¡†æ¶ï¼Œå®ç°ç»„ç»‡æ–‘å—çš„ç²¾ç¡®å¯¹é½ã€‚</li>
<li>å®éªŒè¯æ˜UWarpåœ¨æ³¨å†Œç²¾åº¦å’Œè®¡ç®—æ•ˆç‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>å°†UWarpåº”ç”¨äºä¹³è…ºç™Œç—…ç†ååº”é¢„æµ‹æ¨¡å‹ï¼Œå‘ç°é¢„æµ‹å˜é‡çš„å˜åŒ–ä¸æ–‘å—ç»„ç»‡å¯†åº¦æœ‰å…³ã€‚</li>
<li>å±€éƒ¨é¢†åŸŸæ¼‚ç§»åˆ†æçš„é‡è¦æ€§è¢«å¼ºè°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20653">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e1a0ab595a25e11703980e518bc7b74c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1180d1c53a676b4264c272153f1f14c5.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="ClinKD-Cross-Modal-Clinical-Knowledge-Distiller-For-Multi-Task-Medical-Images"><a href="#ClinKD-Cross-Modal-Clinical-Knowledge-Distiller-For-Multi-Task-Medical-Images" class="headerlink" title="ClinKD: Cross-Modal Clinical Knowledge Distiller For Multi-Task Medical   Images"></a>ClinKD: Cross-Modal Clinical Knowledge Distiller For Multi-Task Medical   Images</h2><p><strong>Authors:Hongyu Ge, Longkun Hao, Zihui Xu, Zhenxin Lin, Bin Li, Shoujun Zhou, Hongjin Zhao, Yihang Liu</strong></p>
<p>Medical Visual Question Answering (Med-VQA) represents a critical and challenging subtask within the general VQA domain. Despite significant advancements in general VQA, multimodal large language models (MLLMs) still exhibit substantial limitations when handling multi-task VQA scenarios. These limitations manifest through erroneous spatial localization and misinterpretation of medical images, which primarily arise from two fundamental issues: inadequate image-text alignment and insufficient domain-specified knowledge for medical applications. To address these issues, we introduce the Cross-Modal Clinical Knowledge Distiller (ClinKD), an innovative framework designed to enhance image-text alignment and establish more effective medical knowledge transformation mechanisms, which enables MLLMs to perform better even when lacking prior medical knowledge. Our extensive experimental evaluations demonstrate that the ClinKD achieves state-of-the-art performance on several datasets which are challenging for Med-VQA task. The results indicate that our approach not only significantly improves image-text alignment but also effectively enables MLLMs to adapt to the medical knowledge. The source code for ClinKD is available at: <a target="_blank" rel="noopener" href="https://github.com/overloadedHenry/ClinKD">https://github.com/overloadedHenry/ClinKD</a>. </p>
<blockquote>
<p>åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆMed-VQAï¼‰æ˜¯é€šç”¨VQAé¢†åŸŸä¸­çš„ä¸€ä¸ªé‡è¦ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„å­ä»»åŠ¡ã€‚å°½ç®¡é€šç”¨VQAå–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†åœ¨å¤„ç†å¤šä»»åŠ¡VQAåœºæ™¯æ—¶ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä»æ˜¾ç¤ºå‡ºæ˜æ˜¾çš„å±€é™æ€§ã€‚è¿™äº›å±€é™æ€§è¡¨ç°ä¸ºåŒ»ç–—å›¾åƒçš„ç©ºé—´å®šä½é”™è¯¯å’Œè¯¯è§£ï¼Œè¿™ä¸»è¦æºäºä¸¤ä¸ªåŸºæœ¬é—®é¢˜ï¼šå›¾åƒæ–‡æœ¬å¯¹é½ä¸è¶³å’Œé’ˆå¯¹åŒ»ç–—åº”ç”¨çš„ç‰¹å®šé¢†åŸŸçŸ¥è¯†ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è·¨æ¨¡æ€ä¸´åºŠçŸ¥è¯†è’¸é¦å™¨ï¼ˆClinKDï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå›¾åƒæ–‡æœ¬å¯¹é½å¹¶å»ºç«‹æ›´æœ‰æ•ˆçš„åŒ»å­¦çŸ¥è¯†è½¬æ¢æœºåˆ¶ï¼Œä½¿MLLMå³ä½¿åœ¨ç¼ºä¹å…ˆéªŒåŒ»å­¦çŸ¥è¯†çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¡¨ç°æ›´å¥½ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒClinKDåœ¨å‡ ä¸ªå¯¹Med-VQAä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æ˜¾è‘—æé«˜äº†å›¾åƒæ–‡æœ¬å¯¹é½ï¼Œè€Œä¸”æœ‰æ•ˆåœ°ä½¿MLLMé€‚åº”äº†åŒ»å­¦çŸ¥è¯†ã€‚ClinKDçš„æºä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/overloadedHenry/ClinKD%E3%80%82">https://github.com/overloadedHenry/ClinKDã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05928v4">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹åŒ»å­¦è§†è§‰é—®ç­”ï¼ˆMed-VQAï¼‰ä»»åŠ¡ä¸­å¤šä»»åŠ¡é—®ç­”åœºæ™¯ä¸‹çš„æŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§è·¨æ¨¡æ€ä¸´åºŠçŸ¥è¯†è’¸é¦å™¨ï¼ˆClinKDï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¢å¼ºå›¾åƒæ–‡æœ¬å¯¹é½å¹¶å»ºç«‹æœ‰æ•ˆçš„åŒ»å­¦çŸ¥è¯†è½¬æ¢æœºåˆ¶ï¼Œå³ä½¿åœ¨æ²¡æœ‰å…ˆéªŒåŒ»å­¦çŸ¥è¯†çš„æƒ…å†µä¸‹ä¹Ÿèƒ½æé«˜å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒClinKDåœ¨Med-VQAä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Med-VQAæ˜¯VQAé¢†åŸŸä¸­çš„ä¸€ä¸ªé‡è¦ä¸”å…·æŒ‘æˆ˜æ€§çš„å­ä»»åŠ¡ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†Med-VQAæ—¶é¢ä¸´å›¾åƒæ–‡æœ¬å¯¹é½å’ŒåŒ»å­¦çŸ¥è¯†è½¬æ¢çš„é—®é¢˜ã€‚</li>
<li>è·¨æ¨¡æ€ä¸´åºŠçŸ¥è¯†è’¸é¦å™¨ï¼ˆClinKDï¼‰æ¡†æ¶æ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ï¼Œå¢å¼ºå›¾åƒæ–‡æœ¬å¯¹é½å¹¶å»ºç«‹æœ‰æ•ˆçš„åŒ»å­¦çŸ¥è¯†è½¬æ¢æœºåˆ¶ã€‚</li>
<li>ClinKDæ¡†æ¶ä½¿å¾—MLLMsåœ¨ç¼ºä¹å…ˆéªŒåŒ»å­¦çŸ¥è¯†çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¡¨ç°è‰¯å¥½ã€‚</li>
<li>ClinKDåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨ç°å‡ºå…¶ä¼˜å¼‚æ€§èƒ½ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>ClinKDå…¬å¼€æºä»£ç å¯ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05928">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c047be575f3ba1e47cbc6be136cadd6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95dca018427b1316264aee659d476ddc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2eb8b850a802440aa689494d3ef3db5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46b2b8c581ead9d24a80851a5ee9d108.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-15/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-15/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-15/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3e2c60f9623cc0fb36f26ef9211a27a2.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-15  MIDI-VALLE Improving Expressive Piano Performance Synthesis Through   Neural Codec Language Modelling
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-15/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-82ff1d6a163085ec5a5be9fbd70e9c49.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-15  From Enhancement to Understanding Build a Generalized Bridge for   Low-light Vision via Semantically Consistent Unsupervised Fine-tuning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
