<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-15  Lumos-1 On Autoregressive Video Generation from a Unified Model   Perspective">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-0516b4c33873e7226f7ba21e7a9a7be8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    71 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-15-æ›´æ–°"><a href="#2025-07-15-æ›´æ–°" class="headerlink" title="2025-07-15 æ›´æ–°"></a>2025-07-15 æ›´æ–°</h1><h2 id="Lumos-1-On-Autoregressive-Video-Generation-from-a-Unified-Model-Perspective"><a href="#Lumos-1-On-Autoregressive-Video-Generation-from-a-Unified-Model-Perspective" class="headerlink" title="Lumos-1: On Autoregressive Video Generation from a Unified Model   Perspective"></a>Lumos-1: On Autoregressive Video Generation from a Unified Model   Perspective</h2><p><strong>Authors:Hangjie Yuan, Weihua Chen, Jun Cen, Hu Yu, Jingyun Liang, Shuning Chang, Zhihui Lin, Tao Feng, Pengwei Liu, Jiazheng Xing, Hao Luo, Jiasheng Tang, Fan Wang, Yi Yang</strong></p>
<p>Autoregressive large language models (LLMs) have unified a vast range of language tasks, inspiring preliminary efforts in autoregressive video generation. Existing autoregressive video generators either diverge from standard LLM architectures, depend on bulky external text encoders, or incur prohibitive latency due to next-token decoding. In this paper, we introduce Lumos-1, an autoregressive video generator that retains the LLM architecture with minimal architectural modifications. To inject spatiotemporal correlations in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE scheme that preserves the original textual RoPE while providing comprehensive frequency spectra and scaled 3D positions for modeling multimodal spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy that obeys intra-frame bidirectionality and inter-frame temporal causality. Based on this dependency strategy, we identify the issue of frame-wise loss imbalance caused by spatial information redundancy and solve it by proposing Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal tube masking during training with a compatible inference-time masking policy to avoid quality degradation. By using memory-efficient training techniques, we pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/alibaba-damo-academy/Lumos">https://github.com/alibaba-damo-academy/Lumos</a>. </p>
<blockquote>
<p>è‡ªå›å½’å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»ç»Ÿä¸€äº†å¤šç§è¯­è¨€ä»»åŠ¡ï¼Œä¸ºè‡ªå›å½’è§†é¢‘ç”Ÿæˆæä¾›äº†åˆæ­¥çš„æ€è·¯ã€‚ç°æœ‰çš„è‡ªå›å½’è§†é¢‘ç”Ÿæˆå™¨è¦ä¹ˆä¸æ ‡å‡†LLMæ¶æ„ä¸åŒï¼Œä¾èµ–äºåºå¤§çš„å¤–éƒ¨æ–‡æœ¬ç¼–ç å™¨ï¼Œè¦ä¹ˆç”±äºä¸‹ä¸€ä¸ªä»¤ç‰Œè§£ç è€Œäº§ç”Ÿè¿‡é«˜çš„å»¶è¿Ÿã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Lumos-1ï¼Œè¿™æ˜¯ä¸€æ¬¾ä¿ç•™LLMæ¶æ„å¹¶è¿›è¡Œäº†æœ€å°ä¿®æ”¹çš„è‡ªå›å½’è§†é¢‘ç”Ÿæˆå™¨ã€‚ä¸ºäº†å°†æ—¶ç©ºç›¸å…³æ€§æ³¨å…¥LLMä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†èå…¥3D RoPEçš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯Šæ–­äº†å…¶ä¸å¹³è¡¡çš„é¢‘ç‡è°±èŒƒå›´ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†MM-RoPEï¼Œè¿™æ˜¯ä¸€ç§RoPEæ–¹æ¡ˆï¼Œæ—¢ä¿ç•™äº†åŸå§‹çš„æ–‡æœ¬RoPEï¼Œåˆæä¾›äº†å…¨é¢çš„é¢‘ç‡è°±å’Œç¼©æ”¾çš„ä¸‰ç»´ä½ç½®ï¼Œç”¨äºæ¨¡æ‹Ÿå¤šæ¨¡æ€æ—¶ç©ºæ•°æ®ã€‚æ­¤å¤–ï¼ŒLumos-1é‡‡ç”¨äº†ä¸€ç§éµå¾ªå¸§å†…åŒå‘æ€§å’Œå¸§é—´æ—¶é—´å› æœå…³ç³»çš„ä»¤ç‰Œä¾èµ–ç­–ç•¥ã€‚åŸºäºè¿™ç§ä¾èµ–ç­–ç•¥ï¼Œæˆ‘ä»¬å‘ç°äº†ç”±äºç©ºé—´ä¿¡æ¯å†—ä½™å¯¼è‡´çš„å¸§çº§æŸå¤±ä¸å¹³è¡¡é—®é¢˜ï¼Œå¹¶æå‡ºäº†è‡ªå›å½’ç¦»æ•£æ‰©æ•£å¼ºåˆ¶ï¼ˆAR-DFï¼‰æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚AR-DFåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†æ—¶é—´ç®¡æ©ç ï¼Œå¹¶ä½¿ç”¨å…¼å®¹çš„æ¨ç†æ—¶é—´æ©ç ç­–ç•¥ï¼Œä»¥é¿å…è´¨é‡ä¸‹é™ã€‚é€šè¿‡ä½¿ç”¨é«˜æ•ˆçš„å†…å­˜è®­ç»ƒæŠ€æœ¯ï¼Œæˆ‘ä»¬åªç”¨äº†48ä¸ªGPUå¯¹Lumos-1è¿›è¡Œäº†é¢„è®­ç»ƒï¼Œåœ¨GenEvalä¸Šçš„æ€§èƒ½ä¸EMU3ç›¸å½“ï¼Œåœ¨VBench-I2Vä¸Šçš„COSMOS-Video2Worldå’ŒVBench-T2Vä¸Šçš„OpenSoraPlanä¹Ÿè¡¨ç°å‡ºè‰²ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/alibaba-damo-academy/Lumos%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/alibaba-damo-academy/Lumosä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08801v1">PDF</a> Code and Models: <a target="_blank" rel="noopener" href="https://github.com/alibaba-damo-academy/Lumos">https://github.com/alibaba-damo-academy/Lumos</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†Lumos-1è¿™ç§åŸºäºLLMæ¶æ„çš„è‡ªåŠ¨è§†é¢‘ç”Ÿæˆå™¨ã€‚ä¸ºæ³¨å…¥æ—¶ç©ºç›¸å…³æ€§ï¼Œè®ºæ–‡è¯†åˆ«äº†3D RoPEçš„æœ‰æ•ˆæ€§å¹¶è¯Šæ–­äº†å…¶ä¸å¹³è¡¡çš„é¢‘ç‡è°±èŒƒå›´é—®é¢˜ï¼Œäºæ˜¯æå‡ºäº†MM-RoPEæ–¹æ¡ˆã€‚åŒæ—¶ï¼ŒLumos-1é‡‡å–éµå¾ªå¸§å†…åŒå‘æ€§å’Œå¸§é—´æ—¶åºå› æœå…³ç³»çš„ä»¤ç‰Œä¾èµ–ç­–ç•¥ã€‚é’ˆå¯¹ç”±ç©ºé—´ä¿¡æ¯å†—ä½™å¼•èµ·çš„å¸§æŸå¤±ä¸å¹³è¡¡é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†åŸºäºè‡ªåŠ¨å›å½’ç¦»æ•£æ‰©æ•£å¼ºè¿«ï¼ˆAR-DFï¼‰çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡é‡‡ç”¨é«˜æ•ˆçš„è®­ç»ƒæ–¹æ³•ï¼ŒLumos-1åœ¨æœ‰é™çš„GPUèµ„æºä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œåœ¨GenEvalçš„EMU3ä¸Šå®ç°äº†æ¯”è¾ƒæ€§èƒ½ï¼Œå¹¶åœ¨VBenchä¸Šçš„COSMOS-Video2Worldå’ŒOpenSoraPlanä¸Šè¿›è¡Œäº†éªŒè¯ã€‚ä»£ç å’Œæ¨¡å‹å¯åœ¨GitHubä¸Šæ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Lumos-1æ˜¯é¦–ä¸ªåŸºäºLLMæ¶æ„çš„è‡ªåŠ¨è§†é¢‘ç”Ÿæˆå™¨ï¼Œå…·æœ‰ç»Ÿä¸€å¤šç§è¯­è¨€ä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥MM-RoPEæ–¹æ¡ˆä»¥æ”¹è¿›åŸæœ‰çš„RoPEè®¾è®¡ï¼Œè€ƒè™‘äº†æ›´å…¨é¢çš„é¢‘ç‡è°±å’Œä¸‰ç»´ä½ç½®å»ºæ¨¡é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨ç‰¹å®šçš„ä»¤ç‰Œä¾èµ–ç­–ç•¥æ¥ä¿æŒå¸§å†…åŒå‘æ€§å’Œå¸§é—´æ—¶åºå› æœæ€§ã€‚</li>
<li>æå‡ºAR-DFæ¥è§£å†³å› ç©ºé—´ä¿¡æ¯å†—ä½™å¯¼è‡´çš„å¸§æŸå¤±ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>Lumos-1é€šè¿‡å†…å­˜é«˜æ•ˆè®­ç»ƒæŠ€æœ¯åœ¨æœ‰é™çš„GPUèµ„æºä¸Šå®ç°äº†è‰¯å¥½çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>Lumos-1åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸ç°æœ‰æŠ€æœ¯ç›¸å½“çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒ…æ‹¬GenEvalçš„EMU3ã€VBenchä¸Šçš„COSMOS-Video2Worldå’ŒOpenSoraPlanç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08801">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-aaf372f1b3f23d2c63aa54dfa3203cd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9fa3d887b84c19f081e129f0f7ea207.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-71e1d6eaecccf418df27a239ea16910d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7dd6b2c04b74ca691dd7085ebcfd00d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e42829556126edcabded3543c4c227fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a33e557e77bc462338e63a5b62d2a95e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ec211097436c3b5bee6c33903800d80.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="One-Token-to-Fool-LLM-as-a-Judge"><a href="#One-Token-to-Fool-LLM-as-a-Judge" class="headerlink" title="One Token to Fool LLM-as-a-Judge"></a>One Token to Fool LLM-as-a-Judge</h2><p><strong>Authors:Yulai Zhao, Haolin Liu, Dian Yu, S. Y. Kung, Haitao Mi, Dong Yu</strong></p>
<p>Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are increasingly adopted in reinforcement learning with verifiable rewards (RLVR). They are often preferred over rigid rule-based metrics, especially for complex reasoning tasks involving free-form outputs. In this paradigm, an LLM is typically prompted to compare a candidate answer against a ground-truth reference and assign a binary reward indicating correctness. Despite the seeming simplicity of this comparison task, we find that generative reward models exhibit surprising vulnerabilities to superficial manipulations: non-word symbols (e.g., â€œ:â€ or â€œ.â€) or reasoning openers like â€œThought process:â€ and â€œLetâ€™s solve this problem step by step.â€ can often lead to false positive rewards. We demonstrate that this weakness is widespread across LLMs, datasets, and prompt formats, posing a serious threat for core algorithmic paradigms that rely on generative reward models, such as rejection sampling, preference optimization, and RLVR. To mitigate this issue, we introduce a simple yet effective data augmentation strategy and train a new generative reward model with substantially improved robustness. Our findings highlight the urgent need for more reliable LLM-based evaluation methods. We release our robust, general-domain reward model and its synthetic training data at <a target="_blank" rel="noopener" href="https://huggingface.co/sarosavo/Master-RM">https://huggingface.co/sarosavo/Master-RM</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/sarosavo/Master-RM">https://huggingface.co/datasets/sarosavo/Master-RM</a>. </p>
<blockquote>
<p>ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆä¹Ÿç§°ä¸ºLLMä½œä¸ºè¯„åˆ¤è€…ï¼‰ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯„ä¼°ç­”æ¡ˆè´¨é‡ï¼Œåœ¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ä¸­è¶Šæ¥è¶Šè¢«é‡‡ç”¨ã€‚å®ƒä»¬é€šå¸¸è¢«ä¼˜å…ˆé€‰æ‹©ç”¨äºæ¶‰åŠè‡ªç”±å½¢å¼è¾“å‡ºçš„å¤æ‚æ¨ç†ä»»åŠ¡ï¼Œè€Œä¸æ˜¯åƒµåŒ–çš„åŸºäºè§„åˆ™çš„åº¦é‡æ ‡å‡†ã€‚åœ¨è¿™ç§æ¨¡å¼ä¸‹ï¼Œé€šå¸¸ä¼šæç¤ºLLMå°†å€™é€‰ç­”æ¡ˆä¸çœŸå®ç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒï¼Œå¹¶åˆ†é…ä¸€ä¸ªäºŒè¿›åˆ¶å¥–åŠ±æ¥è¡¨ç¤ºæ­£ç¡®æ€§ã€‚å°½ç®¡è¿™ç§æ¯”è¾ƒä»»åŠ¡çœ‹ä¼¼ç®€å•ï¼Œä½†æˆ‘ä»¬å‘ç°ç”Ÿæˆå¥–åŠ±æ¨¡å‹å¯¹è¡¨é¢æ“çºµè¡¨ç°å‡ºä»¤äººæƒŠè®¶çš„è„†å¼±æ€§ï¼šéå•è¯ç¬¦å·ï¼ˆä¾‹å¦‚ï¼šâ€œï¼šâ€æˆ–â€œã€‚â€ï¼‰æˆ–æ¨ç†å¼€åœºè¯å¦‚â€œæ€è€ƒè¿‡ç¨‹ï¼šâ€å’Œâ€œè®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚â€å¯èƒ½ä¼šå¯¼è‡´é”™è¯¯çš„æ­£é¢å¥–åŠ±ã€‚æˆ‘ä»¬è¯æ˜è¿™ç§å¼±ç‚¹åœ¨LLMã€æ•°æ®é›†å’Œæç¤ºæ ¼å¼ä¸­æ˜¯æ™®éå­˜åœ¨çš„ï¼Œå¯¹ä¾èµ–ç”Ÿæˆå¥–åŠ±æ¨¡å‹çš„æ ¸å¿ƒç®—æ³•èŒƒå¼æ„æˆä¸¥é‡å¨èƒï¼Œå¦‚æ‹’ç»æŠ½æ ·ã€åå¥½ä¼˜åŒ–å’ŒRLVRã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œå¹¶è®­ç»ƒäº†ä¸€ä¸ªæ–°çš„ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·æœ‰æ›´å¼ºçš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ›´å¯é çš„åŸºäºLLMçš„è¯„ä¼°æ–¹æ³•è¿«åˆ‡éœ€è¦ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/sarosavo/Master-RM">https://huggingface.co/sarosavo/Master-RM</a>å’Œ<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/sarosavo/Master-RM">https://huggingface.co/datasets/sarosavo/Master-RM</a>å‘å¸ƒäº†æˆ‘ä»¬ç¨³å¥çš„é€šç”¨å¥–åŠ±æ¨¡å‹å’Œåˆæˆè®­ç»ƒæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08794v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºå¥–åŠ±æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œç”¨äºè¯„ä¼°ç­”æ¡ˆè´¨é‡ã€‚ç„¶è€Œï¼Œè¿™ç§æ¨¡å‹åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ—¶å­˜åœ¨æ¼æ´ï¼Œå®¹æ˜“è¢«è¡¨é¢æ“çºµæ‰€å½±å“ï¼Œå¯¼è‡´é”™è¯¯å¥–åŠ±ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§æ–°çš„å¢å¼ºå¥–åŠ±æ¨¡å‹ï¼Œå¹¶å‘å¸ƒäº†ç›¸å…³èµ„æºã€‚è¿™å‡¸æ˜¾äº†æ›´å¯é çš„LLMè¯„ä¼°æ–¹æ³•çš„è¿«åˆ‡éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆLLMs-as-judgesï¼‰é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°ç­”æ¡ˆè´¨é‡ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ä¸­å—åˆ°é’çã€‚</li>
<li>ä¸åˆšæ€§è§„åˆ™åŸºç¡€çš„åº¦é‡ç›¸æ¯”ï¼Œè¿™ç§æ¨¡å‹æ›´é€‚åˆå¤„ç†æ¶‰åŠè‡ªç”±å½¢å¼è¾“å‡ºçš„å¤æ‚æ¨ç†ä»»åŠ¡ã€‚</li>
<li>ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹å­˜åœ¨æ¼æ´ï¼Œå®¹æ˜“è¢«éå•è¯ç¬¦å·å’Œæ¨ç†å¼€åœºç™½ç­‰è¡¨é¢æ“çºµå½±å“ï¼Œå¯¼è‡´é”™è¯¯å¥–åŠ±ã€‚</li>
<li>é—®é¢˜å¹¿æ³›å­˜åœ¨äºLLMã€æ•°æ®é›†å’Œæç¤ºæ ¼å¼ä¸­ï¼Œå¯¹ä¾èµ–ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹çš„æ ¸å¿ƒç®—æ³•èŒƒå¼æ„æˆä¸¥é‡å¨èƒã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿé€šè¿‡ç®€å•çš„æ•°æ®å¢å¼ºç­–ç•¥è®­ç»ƒäº†æ–°çš„ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼Œæé«˜äº†ç¨³å¥æ€§ã€‚</li>
<li>ç°æœ‰çš„LLMè¯„ä¼°æ–¹æ³•å­˜åœ¨ä¸è¶³ï¼Œéœ€è¦æ›´å¯é çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08794">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ffad66e2acee4b44c1665bca67858df9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa0de9f4e66f4c6bf33aedbe167e4723.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f47710730fdca2a4825394e8b95b0c8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="BlockFFN-Towards-End-Side-Acceleration-Friendly-Mixture-of-Experts-with-Chunk-Level-Activation-Sparsity"><a href="#BlockFFN-Towards-End-Side-Acceleration-Friendly-Mixture-of-Experts-with-Chunk-Level-Activation-Sparsity" class="headerlink" title="BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with   Chunk-Level Activation Sparsity"></a>BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with   Chunk-Level Activation Sparsity</h2><p><strong>Authors:Chenyang Song, Weilin Zhao, Xu Han, Chaojun Xiao, Yingfa Chen, Yuxuan Li, Zhiyuan Liu, Maosong Sun</strong></p>
<p>To alleviate the computational burden of large language models (LLMs), architectures with activation sparsity, represented by mixture-of-experts (MoE), have attracted increasing attention. However, the non-differentiable and inflexible routing of vanilla MoE hurts model performance. Moreover, while each token activates only a few parameters, these sparsely-activated architectures exhibit low chunk-level sparsity, indicating that the union of multiple consecutive tokens activates a large ratio of parameters. Such a sparsity pattern is unfriendly for acceleration under low-resource conditions (e.g., end-side devices) and incompatible with mainstream acceleration techniques (e.g., speculative decoding). To address these challenges, we introduce a novel MoE architecture, BlockFFN, as well as its efficient training and deployment techniques. Specifically, we use a router integrating ReLU activation and RMSNorm for differentiable and flexible routing. Next, to promote both token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training objectives are designed, making BlockFFN more acceleration-friendly. Finally, we implement efficient acceleration kernels, combining activation sparsity and speculative decoding for the first time. The experimental results demonstrate the superior performance of BlockFFN over other MoE baselines, achieving over 80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\times$ speedup on real end-side devices than dense models. All codes and checkpoints are available publicly (<a target="_blank" rel="noopener" href="https://github.com/thunlp/BlockFFN">https://github.com/thunlp/BlockFFN</a>). </p>
<blockquote>
<p>ä¸ºç¼“è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®¡ç®—è´Ÿæ‹…ï¼Œä»¥æ··åˆä¸“å®¶ï¼ˆMoEï¼‰ä¸ºä»£è¡¨çš„æ¿€æ´»ç¨€ç–æ€§æ¶æ„å¸å¼•äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ç„¶è€Œï¼Œæ ‡å‡†MoEçš„éå¯å¾®åˆ†å’Œåƒµç¡¬è·¯ç”±ä¼šæŸå®³æ¨¡å‹æ€§èƒ½ã€‚å°½ç®¡æ¯ä¸ªä»¤ç‰Œåªæ¿€æ´»å°‘æ•°å‚æ•°ï¼Œä½†è¿™äº›ç¨€ç–æ¿€æ´»çš„æ¶æ„è¡¨ç°å‡ºè¾ƒä½çš„å—çº§ç¨€ç–æ€§ï¼Œè¡¨æ˜å¤šä¸ªè¿ç»­ä»¤ç‰Œçš„è”åˆæ¿€æ´»äº†å¾ˆå¤§æ¯”ä¾‹çš„å‚æ•°ã€‚è¿™ç§ç¨€ç–æ¨¡å¼ä¸åˆ©äºä½èµ„æºæ¡ä»¶ä¸‹çš„åŠ é€Ÿï¼ˆä¾‹å¦‚ï¼Œè¾¹ç¼˜è®¾å¤‡ï¼‰ï¼Œå¹¶ä¸”ä¸ä¸»æµåŠ é€ŸæŠ€æœ¯ï¼ˆä¾‹å¦‚ï¼Œæ¨æµ‹è§£ç ï¼‰ä¸å…¼å®¹ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹MoEæ¶æ„BlockFFNåŠå…¶é«˜æ•ˆçš„è®­ç»ƒå’Œéƒ¨ç½²æŠ€æœ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨é›†æˆReLUæ¿€æ´»å’ŒRMSNormçš„è·¯ç”±å™¨å®ç°å¯å¾®åˆ†å’Œçµæ´»è·¯ç”±ã€‚æ¥ä¸‹æ¥ï¼Œä¸ºä¿ƒè¿›ä»¤ç‰Œçº§ç¨€ç–æ€§ï¼ˆTLSï¼‰å’Œå—çº§ç¨€ç–æ€§ï¼ˆCLSï¼‰ï¼Œè®¾è®¡äº†CLSæ„ŸçŸ¥è®­ç»ƒç›®æ ‡ï¼Œä½¿BlockFFNæ›´æ˜“äºåŠ é€Ÿã€‚æœ€åï¼Œæˆ‘ä»¬é¦–æ¬¡ç»“åˆæ¿€æ´»ç¨€ç–æ€§å’Œæ¨æµ‹è§£ç å®ç°äº†é«˜æ•ˆçš„åŠ é€Ÿå†…æ ¸ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBlockFFNä¼˜äºå…¶ä»–MoEåŸºçº¿ï¼Œå®ç°è¶…è¿‡80%çš„TLSå’Œ70%çš„8ä»¤ç‰ŒCLSã€‚æˆ‘ä»¬çš„å†…æ ¸åœ¨å®é™…è¾¹ç¼˜è®¾å¤‡ä¸Šçš„é€Ÿåº¦æ¯”å¯†é›†æ¨¡å‹æœ€é«˜å¿«3.67å€ã€‚æ‰€æœ‰ä»£ç å’Œæ£€æŸ¥ç‚¹å‡å·²å…¬å¼€ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/thunlp/BlockFFN%EF%BC%89%E3%80%82">https://github.com/thunlp/BlockFFNï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08771v1">PDF</a> 21 pages, 7 figures, 15 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸ºäº†è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®¡ç®—è´Ÿæ‹…é—®é¢˜ï¼Œå¼•å…¥äº†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„ä¸­çš„æ¿€æ´»ç¨€ç–æ€§ã€‚é’ˆå¯¹ä¼ ç»ŸMoEçš„éå¯å¾®å’Œä¸å¯çµæ´»è·¯ç”±çš„é—®é¢˜ï¼Œæå‡ºäº†æ–°å‹çš„MoEæ¶æ„BlockFFNï¼Œå¹¶ç»“åˆé«˜æ•ˆçš„è®­ç»ƒå’Œéƒ¨ç½²æŠ€æœ¯ã€‚BlockFFNä½¿ç”¨é›†æˆReLUæ¿€æ´»å’ŒRMSNormçš„è·¯ç”±å™¨å®ç°å¯å¾®å’Œçµæ´»è·¯ç”±ã€‚é€šè¿‡è®¾è®¡é¢å‘å—çº§ç¨€ç–æ€§ï¼ˆCLSï¼‰çš„è®­ç»ƒç›®æ ‡ï¼Œä¿ƒè¿›äº†ä»¤ç‰Œçº§ç¨€ç–æ€§ï¼ˆTLSï¼‰å’ŒCLSçš„æé«˜ï¼Œä½¿å¾—BlockFFNæ›´é€‚ç”¨äºåŠ é€Ÿã€‚æ­¤å¤–ï¼Œç»“åˆæ¿€æ´»ç¨€ç–æ€§å’Œæ¨æµ‹è§£ç ï¼Œé¦–æ¬¡å®ç°äº†é«˜æ•ˆçš„åŠ é€Ÿå†…æ ¸ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBlockFFNä¼˜äºå…¶ä»–MoEåŸºçº¿ï¼Œè¾¾åˆ°è¶…è¿‡80%çš„TLSå’Œ70%çš„8ä»¤ç‰ŒCLSã€‚åœ¨çœŸå®ç«¯ä¾§è®¾å¤‡ä¸Šï¼Œç›¸æ¯”å¯†é›†æ¨¡å‹ï¼Œæˆ‘ä»¬çš„å†…æ ¸å®ç°äº†é«˜è¾¾3.67å€çš„åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®¡ç®—è´Ÿæ‹…æˆä¸ºç ”ç©¶ç„¦ç‚¹ï¼Œæ¿€æ´»ç¨€ç–æ€§çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„è¢«è§†ä¸ºè§£å†³æ–¹æ¡ˆã€‚</li>
<li>ä¼ ç»ŸMoEå­˜åœ¨éå¯å¾®å’Œä¸å¯çµæ´»è·¯ç”±çš„é—®é¢˜ï¼Œå› æ­¤æå‡ºæ–°å‹MoEæ¶æ„BlockFFNã€‚</li>
<li>BlockFFNä½¿ç”¨é›†æˆReLUæ¿€æ´»å’ŒRMSNormçš„è·¯ç”±å™¨ï¼Œå®ç°å¯å¾®å’Œçµæ´»è·¯ç”±ã€‚</li>
<li>è®¾è®¡é¢å‘å—çº§ç¨€ç–æ€§ï¼ˆCLSï¼‰çš„è®­ç»ƒç›®æ ‡ï¼Œä»¥æå‡ä»¤ç‰Œçº§ç¨€ç–æ€§ï¼ˆTLSï¼‰ï¼Œä½¿æ¨¡å‹æ›´é€‚ç”¨äºåŠ é€Ÿã€‚</li>
<li>ç»“åˆæ¿€æ´»ç¨€ç–æ€§å’Œæ¨æµ‹è§£ç ï¼Œé¦–æ¬¡å®ç°é«˜æ•ˆçš„åŠ é€Ÿå†…æ ¸ã€‚</li>
<li>BlockFFNæ€§èƒ½ä¼˜è¶Šï¼Œè¶…è¿‡å…¶ä»–MoEåŸºçº¿ï¼Œå®ç°é«˜TLSå’ŒCLSã€‚</li>
<li>åœ¨çœŸå®ç«¯ä¾§è®¾å¤‡ä¸Šï¼ŒBlockFFNç›¸æ¯”å¯†é›†æ¨¡å‹å®ç°äº†æ˜¾è‘—çš„åŠ é€Ÿæ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08771">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7a84d75a20b8ede4f29f72e4658aa2c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4e36a2d7dc3625377023551ede1326d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0795083e8670e89706de2707be534d00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d663e0e000164056ac37a9d388eed7ba.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LLMCup-Ranking-Enhanced-Comment-Updating-with-LLMs"><a href="#LLMCup-Ranking-Enhanced-Comment-Updating-with-LLMs" class="headerlink" title="LLMCup: Ranking-Enhanced Comment Updating with LLMs"></a>LLMCup: Ranking-Enhanced Comment Updating with LLMs</h2><p><strong>Authors:Hua Ge, Juan Zhai, Minxue Pan, Fusen He, Ziyue Tan</strong></p>
<p>While comments are essential for enhancing code readability and maintainability in modern software projects, developers are often motivated to update code but not comments, leading to outdated or inconsistent documentation that hinders future understanding and maintenance. Recent approaches such as CUP and HebCup have attempted automatic comment updating using neural sequence-to-sequence models and heuristic rules, respectively. However, these methods can miss or misinterpret crucial information during comment updating, resulting in inaccurate comments, and they often struggle with complex update scenarios. Given these challenges, a promising direction lies in leveraging large language models (LLMs), which have shown impressive performance in software engineering tasks such as comment generation, code synthesis, and program repair. This suggests their strong potential to capture the logic behind code modifications - an ability that is crucial for the task of comment updating. Nevertheless, selecting an appropriate prompt strategy for an LLM on each update case remains challenging. To address this, we propose a novel comment updating framework, LLMCup, which first uses multiple prompt strategies to provide diverse candidate updated comments via an LLM, and then employs a ranking model, CupRank, to select the best candidate as final updated comment. Experimental results demonstrate the effectiveness of LLMCup, with improvements over state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy, 10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in SentenceBert similarity. Furthermore, a user study shows that comments updated by LLMCup sometimes surpass human-written updates, highlighting the importance of incorporating human evaluation in comment quality assessment. </p>
<blockquote>
<p>åœ¨ç°ä»£è½¯ä»¶é¡¹ç›®ä¸­ï¼Œæ³¨é‡Šå¯¹äºæé«˜ä»£ç çš„å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§è‡³å…³é‡è¦ã€‚å°½ç®¡å¼€å‘è€…æœ‰åŠ¨åŠ›æ›´æ–°ä»£ç ï¼Œä½†å¾€å¾€å¿½è§†æ›´æ–°æ³¨é‡Šï¼Œè¿™å¯¼è‡´æ–‡æ¡£è¿‡æ—¶æˆ–ä¸ä¸€è‡´ï¼Œé˜»ç¢äº†æœªæ¥çš„ç†è§£å’Œç»´æŠ¤ã€‚è¿‘æœŸçš„æ–¹æ³•ï¼Œå¦‚CUPå’ŒHebCupï¼Œåˆ†åˆ«å°è¯•ä½¿ç”¨ç¥ç»åºåˆ—åˆ°åºåˆ—æ¨¡å‹å’Œå¯å‘å¼è§„åˆ™è¿›è¡Œè‡ªåŠ¨æ³¨é‡Šæ›´æ–°ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨æ›´æ–°æ³¨é‡Šæ—¶å¯èƒ½ä¼šé—æ¼æˆ–è¯¯è§£å…³é”®ä¿¡æ¯ï¼Œå¯¼è‡´æ³¨é‡Šä¸å‡†ç¡®ï¼Œå¹¶ä¸”åœ¨å¤„ç†å¤æ‚çš„æ›´æ–°åœºæ™¯æ—¶ç»å¸¸é‡åˆ°å›°éš¾ã€‚è€ƒè™‘åˆ°è¿™äº›æŒ‘æˆ˜ï¼Œä¸€ä¸ªå‰æ™¯å…‰æ˜çš„æ–¹å‘æ˜¯åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå…¶åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡å¦‚æ³¨é‡Šç”Ÿæˆã€ä»£ç åˆæˆå’Œç¨‹åºä¿®å¤ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚è¿™è¡¨æ˜LLMæœ‰èƒ½åŠ›æ•æ‰ä»£ç ä¿®æ”¹èƒŒåçš„é€»è¾‘â€”â€”è¿™å¯¹äºæ³¨é‡Šæ›´æ–°ä»»åŠ¡è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œé’ˆå¯¹æ¯ä¸ªæ›´æ–°æ¡ˆä¾‹é€‰æ‹©é€‚å½“çš„æç¤ºç­–ç•¥å¯¹äºLLMä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ³¨é‡Šæ›´æ–°æ¡†æ¶LLMCupã€‚å®ƒé¦–å…ˆä½¿ç”¨å¤šç§æç¤ºç­–ç•¥ï¼Œé€šè¿‡LLMæä¾›å¤šæ ·çš„å€™é€‰æ›´æ–°æ³¨é‡Šï¼Œç„¶åé‡‡ç”¨æ’åæ¨¡å‹CupRanké€‰æ‹©æœ€ä½³å€™é€‰ä½œä¸ºæœ€ç»ˆçš„æ›´æ–°æ³¨é‡Šã€‚å®éªŒç»“æœè¡¨æ˜LLMCupçš„æœ‰æ•ˆæ€§ï¼Œå…¶åœ¨å‡†ç¡®ç‡ä¸Šè¾ƒæœ€æ–°åŸºçº¿æ–¹æ³•ï¼ˆCUPå’ŒHebCupï¼‰æé«˜äº†49.0%-116.9%ï¼ŒBLEU-4æé«˜äº†10.8%-20%ï¼ŒMETEORæé«˜äº†4.6%ï¼ŒF1æé«˜äº†0.9%-1.9%ï¼ŒSentenceBertç›¸ä¼¼åº¦æé«˜äº†2.1%-3.4%ã€‚æ­¤å¤–ï¼Œä¸€é¡¹ç”¨æˆ·ç ”ç©¶è¿˜æ˜¾ç¤ºï¼ŒLLMCupæ›´æ–°çš„æ³¨é‡Šæœ‰æ—¶è¶…è¶Šäº†äººå·¥æ›´æ–°çš„æ³¨é‡Šï¼Œè¿™å‡¸æ˜¾äº†åœ¨è¯„ä¼°æ³¨é‡Šè´¨é‡æ—¶èå…¥äººå·¥è¯„ä¼°çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08671v1">PDF</a> 13 pages, 10 figures</p>
<p><strong>æ‘˜è¦</strong><br>ä»£ç æ³¨é‡Šåœ¨ç°ä»£è½¯ä»¶é¡¹ç›®ä¸­å¯¹å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§è‡³å…³é‡è¦ã€‚ä½†å¼€å‘è€…æ›´æ–°ä»£ç è€Œä¸æ›´æ–°æ³¨é‡Šçš„æƒ…å†µæ—¶æœ‰å‘ç”Ÿï¼Œå¯¼è‡´æ–‡æ¡£è¿‡æ—¶æˆ–ä¸ä¸€è‡´ï¼Œå½±å“æœªæ¥çš„ç†è§£å’Œç»´æŠ¤ã€‚å½“å‰æ–¹æ³•å¦‚CUPå’ŒHebCupå°è¯•é€šè¿‡ç¥ç»åºåˆ—åˆ°åºåˆ—æ¨¡å‹å’Œå¯å‘å¼è§„åˆ™è‡ªåŠ¨æ›´æ–°æ³¨é‡Šï¼Œä½†ä»æœ‰ä¸è¶³ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰æ½œåŠ›æ•è·ä»£ç ä¿®æ”¹èƒŒåçš„é€»è¾‘ï¼Œä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æ–°çš„æ–¹å‘ã€‚æå‡ºä¸€ç§æ–°çš„æ³¨é‡Šæ›´æ–°æ¡†æ¶LLMCupï¼Œé‡‡ç”¨å¤šç§æç¤ºç­–ç•¥é€šè¿‡LLMæä¾›å¤šæ ·çš„å€™é€‰æ›´æ–°æ³¨é‡Šï¼Œå¹¶ä½¿ç”¨æ’åæ¨¡å‹CupRanké€‰æ‹©æœ€ä½³å€™é€‰ä½œä¸ºæœ€ç»ˆæ›´æ–°æ³¨é‡Šã€‚å®éªŒç»“æœè¯æ˜LLMCupçš„æœ‰æ•ˆæ€§ï¼Œå‡†ç¡®ç‡è¾ƒç°æœ‰æ–¹æ³•æé«˜49.0%-116.9%ï¼ŒBLEU-4æé«˜10.8%-20%ï¼ŒMETEORæé«˜4.6%ï¼ŒF1æé«˜0.9%-1.9%ï¼ŒSentenceBertç›¸ä¼¼æ€§æé«˜2.1%-3.4%ã€‚ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼ŒLLMCupæ›´æ–°çš„æ³¨é‡Šæœ‰æ—¶è¶…è¶Šäººå·¥æ›´æ–°ï¼Œè¡¨æ˜åœ¨æ³¨é‡Šè´¨é‡è¯„ä¼°ä¸­èå…¥äººå·¥è¯„ä¼°çš„é‡è¦æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼€å‘è€…ç»å¸¸å¿½è§†æ›´æ–°æ³¨é‡Šï¼Œå¯¼è‡´æ–‡æ¡£è¿‡æ—¶æˆ–ä¸ä¸€è‡´ï¼Œå½±å“æœªæ¥çš„ç†è§£å’Œç»´æŠ¤ã€‚</li>
<li>å½“å‰è‡ªåŠ¨æ³¨é‡Šæ›´æ–°æ–¹æ³•å¦‚CUPå’ŒHebCupå­˜åœ¨ä¸è¶³ï¼Œå¯èƒ½é—æ¼æˆ–è¯¯è§£é‡è¦ä¿¡æ¯ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå¦‚æ³¨é‡Šç”Ÿæˆã€ä»£ç åˆæˆå’Œç¨‹åºä¿®å¤ï¼Œå…·æœ‰æ•è·ä»£ç ä¿®æ”¹èƒŒåé€»è¾‘çš„å¼ºå¤§æ½œåŠ›ã€‚</li>
<li>æå‡ºä¸€ç§æ–°çš„æ³¨é‡Šæ›´æ–°æ¡†æ¶LLMCupï¼Œé‡‡ç”¨å¤šç§æç¤ºç­–ç•¥å¹¶é€šè¿‡æ’åæ¨¡å‹é€‰æ‹©æœ€ä½³å€™é€‰æ³¨é‡Šã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜LLMCupåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¾ƒç°æœ‰æ–¹æ³•æœ‰æ‰€æ”¹è¿›ã€‚</li>
<li>ç”¨æˆ·ç ”ç©¶æ˜¾æ˜¾ç¤ºLLMCupæ›´æ–°çš„æ³¨é‡Šæœ‰æ—¶è¶…è¶Šäººå·¥æ›´æ–°ã€‚</li>
<li>åœ¨æ³¨é‡Šè´¨é‡è¯„ä¼°ä¸­èå…¥äººå·¥è¯„ä¼°çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08671">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0ea85de2311d1e33024bd2c963eaf5f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-161445029a70ff260e793df5aa7523bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e30d5389324fa375e8f84015762829b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16301bd26a9309c738b690283e1a20ae.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="KELPS-A-Framework-for-Verified-Multi-Language-Autoformalization-via-Semantic-Syntactic-Alignment"><a href="#KELPS-A-Framework-for-Verified-Multi-Language-Autoformalization-via-Semantic-Syntactic-Alignment" class="headerlink" title="KELPS: A Framework for Verified Multi-Language Autoformalization via   Semantic-Syntactic Alignment"></a>KELPS: A Framework for Verified Multi-Language Autoformalization via   Semantic-Syntactic Alignment</h2><p><strong>Authors:Jiyao Zhang, Chengli Zhong, Hui Xu, Qige Li, Yi Zhou</strong></p>
<p>Modern large language models (LLMs) show promising progress in formalizing informal mathematics into machine-verifiable theorems. However, these methods still face bottlenecks due to the limited quantity and quality of multilingual parallel corpora. In this paper, we propose a novel neuro-symbolic framework KELPS (Knowledge-Equation based Logical Processing System) to address these problems. KELPS is an iterative framework for translating, synthesizing, and filtering informal data into multiple formal languages (Lean, Coq, and Isabelle). First, we translate natural language into Knowledge Equations (KEs), a novel language that we designed, theoretically grounded in assertional logic. Next, we convert them to target languages through rigorously defined rules that preserve both syntactic structure and semantic meaning. This process yielded a parallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic accuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3 (81%) and Herald (81.3%) across multiple datasets. All datasets and codes are available in the supplementary materials. </p>
<blockquote>
<p>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å°†éæ­£å¼æ•°å­¦å½¢å¼åŒ–ä¸ºå¯éªŒè¯çš„æœºå™¨å®šç†æ–¹é¢å–å¾—äº†ä»¤äººç©ç›®çš„è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºå¤šè¯­è¨€å¹³è¡Œè¯­æ–™åº“çš„æ•°é‡å’Œè´¨é‡æœ‰é™ï¼Œè¿™äº›æ–¹æ³•ä»ç„¶é¢ä¸´ç“¶é¢ˆã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ç¥ç»ç¬¦å·æ¡†æ¶KELPSï¼ˆåŸºäºçŸ¥è¯†æ–¹ç¨‹çš„é€»è¾‘å¤„ç†ç³»ç»Ÿï¼‰ã€‚KELPSæ˜¯ä¸€ä¸ªè¿­ä»£æ¡†æ¶ï¼Œç”¨äºå°†éæ­£å¼æ•°æ®ç¿»è¯‘æˆå¤šç§æ­£å¼è¯­è¨€ï¼ˆLeanã€Coqå’ŒIsabelleï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†è‡ªç„¶è¯­è¨€ç¿»è¯‘æˆæˆ‘ä»¬è®¾è®¡çš„ä¸€ç§æ–°å‹çŸ¥è¯†æ–¹ç¨‹ï¼ˆKEï¼‰ï¼Œè¯¥æ–¹ç¨‹ä»¥æ–­è¨€é€»è¾‘ä¸ºç†è®ºåŸºç¡€ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é€šè¿‡ä¸¥æ ¼å®šä¹‰çš„è§„åˆ™å°†å®ƒä»¬è½¬æ¢ä¸ºç›®æ ‡è¯­è¨€ï¼Œè¿™äº›è§„åˆ™æ—¢ä¿ç•™äº†å¥æ³•ç»“æ„åˆä¿ç•™äº†è¯­ä¹‰å«ä¹‰ã€‚è¿™ä¸€è¿‡ç¨‹äº§ç”Ÿäº†è¶…è¿‡6ä¸‡ä¸ªé—®é¢˜çš„å¹³è¡Œè¯­æ–™åº“ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨MiniF2Fä¸Šè¾¾åˆ°äº†88.9%çš„å¥æ³•å‡†ç¡®ç‡ï¼ˆpass@1ï¼‰ï¼Œä¼˜äºå¤šä¸ªæ•°æ®é›†ä¸­çš„æœ€ä½³æ¨¡å‹ï¼Œå¦‚Deepseek-V3ï¼ˆ81%ï¼‰å’ŒHeraldï¼ˆ81.3%ï¼‰ã€‚æ‰€æœ‰æ•°æ®é›†å’Œä»£ç éƒ½å¯åœ¨è¡¥å……ææ–™ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08665v1">PDF</a> Accepted by the ICML 2025 AI4MATH Workshop. 22 pages, 16 figures, 2   tables</p>
<p><strong>Summary</strong><br>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°†éæ­£å¼æ•°å­¦å½¢å¼åŒ–ä¸ºå¯éªŒè¯çš„æœºå™¨å®šç†æ–¹é¢å–å¾—äº†è¿›å±•ã€‚ç„¶è€Œï¼Œç”±äºå¤šè¯­è¨€å¹³è¡Œè¯­æ–™åº“çš„æ•°é‡å’Œè´¨é‡æœ‰é™ï¼Œè¿™äº›æ–¹æ³•ä»é¢ä¸´ç“¶é¢ˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ç¥ç»ç¬¦å·æ¡†æ¶KELPSï¼ˆåŸºäºçŸ¥è¯†æ–¹ç¨‹çš„é€»è¾‘å¤„ç†ç³»ç»Ÿï¼‰ï¼Œä»¥è§£å†³è¿™äº›é—®é¢˜ã€‚KELPSæ˜¯ä¸€ä¸ªè¿­ä»£æ¡†æ¶ï¼Œå¯å°†éæ­£å¼æ•°æ®ç¿»è¯‘æˆå¤šç§æ­£å¼è¯­è¨€ï¼ˆLeanã€Coqå’ŒIsabelleï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†è‡ªç„¶è¯­è¨€ç¿»è¯‘æˆæˆ‘ä»¬è®¾è®¡çš„çŸ¥è¯†æ–¹ç¨‹ï¼ˆKEsï¼‰ï¼Œåœ¨æ–­è¨€é€»è¾‘ä¸­æœ‰ç†è®ºæ ¹æ®ã€‚ç„¶åï¼Œé€šè¿‡ä¸¥æ ¼å®šä¹‰çš„è§„åˆ™å°†å®ƒä»¬è½¬æ¢ä¸ºç›®æ ‡è¯­è¨€ï¼ŒåŒæ—¶ä¿æŒè¯­æ³•ç»“æ„å’Œè¯­ä¹‰æ„ä¹‰ã€‚æ­¤è¿‡ç¨‹äº§ç”Ÿäº†è¶…è¿‡6ä¸‡ä¸ªé—®é¢˜çš„å¹³è¡Œè¯­æ–™åº“ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨MiniF2Fä¸Šè¾¾åˆ°äº†88.9%çš„è¯­æ³•å‡†ç¡®æ€§ï¼ˆpass@1ï¼‰ï¼Œä¼˜äºDeepseek-V3ï¼ˆ81%ï¼‰å’ŒHeraldï¼ˆ81.3%ï¼‰ç­‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°†éæ­£å¼æ•°å­¦å½¢å¼åŒ–ä¸ºæœºå™¨å®šç†æ–¹é¢å–å¾—è¿›å±•ã€‚</li>
<li>å¤šè¯­è¨€å¹³è¡Œè¯­æ–™åº“çš„æ•°é‡å’Œè´¨é‡é™åˆ¶æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹åº”ç”¨çš„ä¸»è¦ç“¶é¢ˆã€‚</li>
<li>KELPSæ¡†æ¶è¢«æå‡ºæ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå®ƒæ˜¯ä¸€ä¸ªèƒ½å¤Ÿå°†éæ­£å¼æ•°æ®ç¿»è¯‘æˆå¤šç§æ­£å¼è¯­è¨€çš„è¿­ä»£æ¡†æ¶ã€‚</li>
<li>KELPSé€šè¿‡ç¿»è¯‘è‡ªç„¶è¯­è¨€åˆ°çŸ¥è¯†æ–¹ç¨‹ï¼ˆKEsï¼‰æ¥å·¥ä½œï¼Œè¿™æ˜¯åŸºäºæ–­è¨€é€»è¾‘çš„æ–°è¯­è¨€ã€‚</li>
<li>çŸ¥è¯†æ–¹ç¨‹è¿›ä¸€æ­¥è¢«è½¬æ¢æˆç›®æ ‡è¯­è¨€ï¼ŒåŒæ—¶ä¿æŒè¯­æ³•ç»“æ„å’Œè¯­ä¹‰æ„ä¹‰ã€‚</li>
<li>KELPSæ¡†æ¶äº§ç”Ÿäº†è¶…è¿‡6ä¸‡ä¸ªé—®é¢˜çš„å¹³è¡Œè¯­æ–™åº“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08665">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-869d7dc440a66f40d83baf1b17f9d838.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d4770f6b0279209c77fa037a1169baf7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9385c9f0171f649c98d80c5b2c5ec9e5.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Introspection-of-Thought-Helps-AI-Agents"><a href="#Introspection-of-Thought-Helps-AI-Agents" class="headerlink" title="Introspection of Thought Helps AI Agents"></a>Introspection of Thought Helps AI Agents</h2><p><strong>Authors:Haoran Sun, Shaoning Zeng</strong></p>
<p>AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to perform interpretation and inference in text and image tasks without post-training, where LLMs and MLLMs play the most critical role and determine the initial ability and limitations of AI Agents. Usually, AI Agents utilize sophisticated prompt engineering and external reasoning framework to obtain a promising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought and Image-of-Thought. However, they are still constrained by the inherent limitations of LLM in understanding natural language, and the iterative reasoning process will generate a large amount of inference cost. To this end, we propose a novel AI Agent Reasoning Framework with Introspection of Thought (INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute programmatic dialogue reasoning processes following the code in prompt. Therefore, self-denial and reflection occur within LLM instead of outside LLM, which can reduce token cost effectively. Through our experiments on six benchmarks for three different tasks, the effectiveness of INoT is verified, with an average improvement of 7.95% in performance, exceeding the baselines. Furthermore, the token cost of INoT is lower on average than the best performing method at baseline by 58.3%. In addition, we demonstrate the versatility of INoT in image interpretation and inference through verification experiments. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ä»£ç†ï¼ˆAI Agentsï¼‰ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œæ— éœ€åè®­ç»ƒçš„æ–‡æœ¬å’Œå›¾åƒä»»åŠ¡ä¸­çš„è§£é‡Šå’Œæ¨æ–­ã€‚åœ¨è¿™é‡Œï¼ŒLLMså’ŒMLLMsèµ·åˆ°æœ€å…³é”®çš„çš„ä½œç”¨å¹¶å†³å®šäº†AIä»£ç†çš„åˆå§‹èƒ½åŠ›å’Œå±€é™æ€§ã€‚é€šå¸¸ï¼ŒAIä»£ç†ä¼šåˆ©ç”¨å¤æ‚çš„æç¤ºå·¥ç¨‹å’Œå¤–éƒ¨æ¨ç†æ¡†æ¶æ¥è·å¾—ä¸LLMsçš„æœ‰å‰é€”çš„äº¤äº’ï¼Œä¾‹å¦‚â€œæ€ç»´é“¾â€ã€â€œæ€ç»´è¿­ä»£â€å’Œâ€œæ€ç»´å›¾åƒâ€ã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶å—åˆ°LLMåœ¨ç†è§£è‡ªç„¶è¯­è¨€æ–¹é¢å›ºæœ‰å±€é™æ€§çš„åˆ¶çº¦ï¼Œè€Œä¸”è¿­ä»£æ¨ç†è¿‡ç¨‹ä¼šäº§ç”Ÿå¤§é‡çš„æ¨ç†æˆæœ¬ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å…·æœ‰æ€æƒ³å†…çœï¼ˆINoTï¼‰çš„AIä»£ç†æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡è®¾è®¡æ–°çš„LLM-Readæç¤ºä»£ç æ¥å®ç°ã€‚å®ƒä½¿LLMèƒ½å¤Ÿæ ¹æ®æç¤ºä¸­çš„ä»£ç æ‰§è¡Œç¨‹åºåŒ–å¯¹è¯æ¨ç†è¿‡ç¨‹ã€‚å› æ­¤ï¼Œè‡ªæˆ‘å¦å®šå’Œåæ€å‘ç”Ÿåœ¨LLMå†…éƒ¨è€Œä¸æ˜¯å¤–éƒ¨ï¼Œè¿™å¯ä»¥æœ‰æ•ˆåœ°é™ä½ä»¤ç‰Œæˆæœ¬ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªä¸åŒä»»åŠ¡çš„å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†å®éªŒï¼ŒéªŒè¯äº†INoTçš„æœ‰æ•ˆæ€§ï¼Œæ€§èƒ½å¹³å‡æé«˜äº†7.95%ï¼Œè¶…è¿‡äº†åŸºçº¿æ°´å¹³ã€‚æ­¤å¤–ï¼ŒINoTçš„å¹³å‡ä»¤ç‰Œæˆæœ¬æ¯”åŸºçº¿ä¸­è¡¨ç°æœ€å¥½çš„æ–¹æ³•é™ä½äº†58.3%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡éªŒè¯å®éªŒå±•ç¤ºäº†INoTåœ¨å›¾åƒè§£é‡Šå’Œæ¨æ–­æ–¹é¢çš„é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08664v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰æ˜¯AIä»£ç†è¿›è¡Œæ–‡æœ¬å’Œå›¾åƒä»»åŠ¡è§£é‡Šå’Œæ¨ç†çš„å…³é”®ã€‚å°½ç®¡AIä»£ç†é‡‡ç”¨å…ˆè¿›çš„æç¤ºå·¥ç¨‹å’Œå¤–éƒ¨æ¨ç†æ¡†æ¶ä¸LLMäº¤äº’ï¼Œä½†å®ƒä»¬ä»å—é™äºLLMç†è§£è‡ªç„¶è¯­è¨€çš„å›ºæœ‰å±€é™æ€§ï¼Œå¹¶ä¸”è¿­ä»£æ¨ç†è¿‡ç¨‹ä¼šäº§ç”Ÿå¤§é‡æ¨ç†æˆæœ¬ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å…·æœ‰æ€æƒ³å†…çœï¼ˆINoTï¼‰çš„æ–°å‹AIä»£ç†æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡è®¾è®¡æ–°çš„LLMé˜…è¯»æç¤ºä»£ç ï¼Œä½¿LLMèƒ½å¤ŸæŒ‰ç…§æç¤ºä¸­çš„ä»£ç æ‰§è¡Œç¨‹åºåŒ–å¯¹è¯æ¨ç†è¿‡ç¨‹ã€‚è¿™å‡å°‘äº†ä»¤ç‰Œæˆæœ¬ï¼Œå¹¶åœ¨ä¸‰ä¸ªä¸åŒä»»åŠ¡çš„å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹³å‡æ€§èƒ½æå‡7.95%ï¼Œå¹¶ä¸”æ¯”åŸºçº¿æ–¹æ³•çš„ä»¤ç‰Œæˆæœ¬ä½58.3%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡éªŒè¯å®éªŒå±•ç¤ºäº†INoTåœ¨å›¾åƒè§£é‡Šå’Œæ¨ç†ä¸­çš„é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIä»£ç†ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰è¿›è¡Œæ–‡æœ¬å’Œå›¾åƒçš„è§£è¯»å’Œæ¨ç†ã€‚</li>
<li>AIä»£ç†é€šå¸¸é‡‡ç”¨å…ˆè¿›çš„æç¤ºå·¥ç¨‹å’Œå¤–éƒ¨æ¨ç†æ¡†æ¶ä¸LLMäº¤äº’ã€‚</li>
<li>LLMåœ¨ç†è§£è‡ªç„¶è¯­è¨€æ–¹é¢å­˜åœ¨å›ºæœ‰å±€é™æ€§ï¼Œè¿­ä»£æ¨ç†è¿‡ç¨‹ä¼šäº§ç”Ÿå¤§é‡æ¨ç†æˆæœ¬ã€‚</li>
<li>æå‡ºäº†å…·æœ‰æ€æƒ³å†…çœï¼ˆINoTï¼‰çš„AIä»£ç†æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡è®¾è®¡æ–°çš„LLMé˜…è¯»æç¤ºä»£ç æ¥å‡å°‘ä»¤ç‰Œæˆæœ¬ã€‚</li>
<li>INoTæ¡†æ¶åœ¨ä¸‰ä¸ªä¸åŒä»»åŠ¡çš„å…­ä¸ªåŸºå‡†æµ‹è¯•ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹³å‡æ€§èƒ½æå‡7.95%ã€‚</li>
<li>INoTæ¡†æ¶çš„ä»¤ç‰Œæˆæœ¬æ¯”åŸºçº¿æ–¹æ³•çš„å¹³å‡æˆæœ¬ä½58.3%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08664">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-790313e4a316087d1ae8c1d9abe1f860.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-947632e67b9934be300ee5ef5fcdc6e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ddf88d08663934b4afe48abc5f4476c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Leanabell-Prover-V2-Verifier-integrated-Reasoning-for-Formal-Theorem-Proving-via-Reinforcement-Learning"><a href="#Leanabell-Prover-V2-Verifier-integrated-Reasoning-for-Formal-Theorem-Proving-via-Reinforcement-Learning" class="headerlink" title="Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem   Proving via Reinforcement Learning"></a>Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem   Proving via Reinforcement Learning</h2><p><strong>Authors:Xingguang Ji, Yahui Liu, Qi Wang, Jingyuan Zhang, Yang Yue, Rui Shi, Chenxi Sun, Fuzheng Zhang, Guorui Zhou, Kun Gai</strong></p>
<p>We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that can produce formal theorem proofs in Lean 4, with verifier-integrated Long Chain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we continual to choose to posttrain existing strong prover models for further performance improvement. In our V2 version, we mainly upgrade the Reinforcement Learning (RL) with feedback provided by the Lean 4 verifier. Crucially, verifier feedback, such as indicating success or detailing specific errors, allows the LLM to become &#96;&#96;self-awareâ€™â€™ of the correctness of its own reasoning process and learn to reflexively correct errors. Leanabell-Prover-V2 directly optimizes LLM reasoning trajectories with multi-turn verifier interactions, together with feedback token masking for stable RL training and a simple reward strategy. Experiments show that Leanabell-Prover-V2 improves performance by 3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with DeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data and models are available at: <a target="_blank" rel="noopener" href="https://github.com/Leanabell-LM/Leanabell-Prover-V2">https://github.com/Leanabell-LM/Leanabell-Prover-V2</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºLeanabell-Prover-V2ï¼Œè¿™æ˜¯ä¸€æ¬¾7Bå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œèƒ½å¤Ÿåœ¨Lean 4ä¸­äº§ç”Ÿæ­£å¼å®šç†è¯æ˜ï¼Œå¹¶é›†æˆéªŒè¯å™¨çš„é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰ã€‚ç»§æˆ‘ä»¬ä¹‹å‰çš„Leanabell-Prover-V1å·¥ä½œä¹‹åï¼Œæˆ‘ä»¬é€‰æ‹©åœ¨ç°æœ‰å¼ºå¤§çš„è¯æ˜æ¨¡å‹ä¸Šè¿›è¡Œåç»­è®­ç»ƒï¼Œä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚åœ¨V2ç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦ä½¿ç”¨Lean 4éªŒè¯å™¨æä¾›çš„åé¦ˆæ¥å‡çº§å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚å…³é”®çš„æ˜¯ï¼ŒéªŒè¯å™¨çš„åé¦ˆï¼Œå¦‚æŒ‡ç¤ºæˆåŠŸæˆ–è¯¦ç»†åˆ—å‡ºå…·ä½“é”™è¯¯ï¼Œå¯ä»¥è®©LLMå¯¹å…¶è‡ªèº«çš„æ¨ç†è¿‡ç¨‹çš„æ­£ç¡®æ€§äº§ç”Ÿâ€œè‡ªæˆ‘æ„è¯†â€ï¼Œå¹¶å­¦ä¼šåå°„æ€§åœ°çº æ­£é”™è¯¯ã€‚Leanabell-Prover-V2é€šè¿‡å¤šè½®éªŒè¯å™¨äº¤äº’ç›´æ¥ä¼˜åŒ–LLMçš„æ¨ç†è½¨è¿¹ï¼ŒåŒæ—¶é‡‡ç”¨åé¦ˆä»¤ç‰Œé®è”½æ¥ç¨³å®šRLè®­ç»ƒå¹¶ç®€åŒ–å¥–åŠ±ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨MiniF2Fæµ‹è¯•é›†ä¸Šï¼ŒLeanabell-Prover-V2ä½¿ç”¨Kiminia-Prover-Preview-Distill-7Bæé«˜äº†3.2%ï¼ˆpass@128ï¼‰çš„æ€§èƒ½ï¼Œä½¿ç”¨DeepSeek-Prover-V2-7Bæé«˜äº†2.0%ï¼ˆpass@128ï¼‰çš„æ€§èƒ½ã€‚ç›¸å…³æºä»£ç ã€ç²¾é€‰æ•°æ®å’Œæ¨¡å‹å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/Leanabell-LM/Leanabell-Prover-V2%E3%80%82">https://github.com/Leanabell-LM/Leanabell-Prover-V2ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08649v1">PDF</a> 23 pages, 13 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>Leanabell-Prover-V2æ˜¯ä¸€æ¬¾7Bå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œèƒ½åœ¨Lean 4ä¸­ç”Ÿæˆæ­£å¼å®šç†è¯æ˜ï¼Œå¹¶æ•´åˆéªŒè¯å™¨é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰ã€‚ç›¸è¾ƒäºå…ˆå‰çš„Leanabell-Prover-V1ï¼Œæˆ‘ä»¬æŒç»­å¯¹å¼ºæ•ˆéªŒè¯å™¨æ¨¡å‹è¿›è¡Œåè®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚åœ¨V2ç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦å‡çº§å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æŠ€æœ¯ï¼Œç»“åˆLean 4éªŒè¯å™¨æä¾›çš„åé¦ˆã€‚éªŒè¯å™¨åé¦ˆï¼Œå¦‚æŒ‡ç¤ºæˆåŠŸæˆ–è¯¦ç»†åˆ—å‡ºç‰¹å®šé”™è¯¯ï¼Œè®©LLMå¯¹å…¶è‡ªèº«æ¨ç†è¿‡ç¨‹çš„æ­£ç¡®æ€§æœ‰â€œè‡ªæˆ‘æ„è¯†â€ï¼Œå¹¶å­¦ä¼šåæ€çº æ­£é”™è¯¯ã€‚Leanabell-Prover-V2ç›´æ¥ä¼˜åŒ–LLMæ¨ç†è½¨è¿¹ï¼Œé€šè¿‡å¤šå›åˆéªŒè¯å™¨äº’åŠ¨ã€åé¦ˆä»¤ç‰Œé®è”½ç¨³å®šRLè®­ç»ƒåŠç®€å•å¥–åŠ±ç­–ç•¥å®ç°ã€‚å®éªŒæ˜¾ç¤ºï¼Œåœ¨MiniF2Fæµ‹è¯•é›†ä¸Šï¼ŒLeanabell-Prover-V2ä½¿ç”¨Kimina-Prover-Preview-Distill-7Bæé«˜æ€§èƒ½3.2%ï¼ˆpass@128ï¼‰ï¼Œä½¿ç”¨DeepSeek-Prover-V2-7Bæé«˜æ€§èƒ½2.0%ï¼ˆpass@128ï¼‰ã€‚ç›¸å…³æºä»£ç ã€ç²¾é€‰æ•°æ®å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Leanabell-LM/Leanabell-Prover-V2">é“¾æ¥</a>è·å–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>Leanabell-Prover-V2æ˜¯ç¬¬ä¸€æ¬¾èƒ½å¤Ÿäº§ç”ŸLean 4å½¢å¼å®šç†è¯æ˜çš„7Bå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡æ•´åˆéªŒè¯å™¨çš„é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰å¢å¼ºäº†å…¶æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä¸å…ˆå‰çš„ç‰ˆæœ¬ç›¸æ¯”ï¼ŒLeanabell-Prover-V2åœ¨åè®­ç»ƒå¼ºæ•ˆéªŒè¯å™¨æ¨¡å‹æ–¹é¢å–å¾—äº†è¿›ä¸€æ­¥çš„æ€§èƒ½æå‡ã€‚</li>
<li>è¯¥æ¨¡å‹ä¸»è¦å‡çº§äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æŠ€æœ¯ï¼Œåˆ©ç”¨Lean 4éªŒè¯å™¨æä¾›çš„åé¦ˆè¿›è¡Œè®­ç»ƒã€‚</li>
<li>éªŒè¯å™¨åé¦ˆä½¿LLMèƒ½å¤Ÿå¯¹å…¶è‡ªèº«æ¨ç†è¿‡ç¨‹çš„æ­£ç¡®æ€§è¿›è¡Œåæ€å’Œçº æ­£ã€‚</li>
<li>Leanabell-Prover-V2é€šè¿‡å¤šå›åˆéªŒè¯å™¨äº’åŠ¨ç›´æ¥ä¼˜åŒ–LLMæ¨ç†è½¨è¿¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08649">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d7ba44307128eaf43c22c10fed3ac552.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bc562050867aac4277b26a8ac88d470.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8430de49304e62c86535ef4072c5387d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="NL-in-the-Middle-Code-Translation-with-LLMs-and-Intermediate-Representations"><a href="#NL-in-the-Middle-Code-Translation-with-LLMs-and-Intermediate-Representations" class="headerlink" title="NL in the Middle: Code Translation with LLMs and Intermediate   Representations"></a>NL in the Middle: Code Translation with LLMs and Intermediate   Representations</h2><p><strong>Authors:Chi-en Amy Tai, Pengyu Nie, Lukasz Golab, Alexander Wong</strong></p>
<p>Studies show that large language models (LLMs) produce buggy code translations. One avenue to improve translation accuracy is through intermediate representations, which could provide structured insights to guide the modelâ€™s understanding. We explore whether code translation using LLMs can benefit from intermediate representations via natural language (NL) and abstract syntax trees (ASTs). Since prompt engineering greatly affects LLM performance, we consider several ways to integrate these representations, from one-shot to chain-of-thought (CoT) prompting. Using Open Gpt4 8X7B and specialized StarCoder and CodeGen models on popular code translation benchmarks (CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs best, with an increase of 13.8% and 6.7%, respectively, in successful translations for the best-performing model (Open Gpt4 8X7B) compared to the zero-shot prompt. </p>
<blockquote>
<p>ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº§ç”Ÿçš„ä»£ç ç¿»è¯‘å­˜åœ¨é”™è¯¯ã€‚æé«˜ç¿»è¯‘å‡†ç¡®æ€§çš„ä¸€ä¸ªé€”å¾„æ˜¯é€šè¿‡ä¸­é—´è¡¨ç¤ºï¼Œè¿™å¯èƒ½ä¸ºæ¨¡å‹çš„ç†è§£æä¾›ç»“æ„åŒ–æ´å¯Ÿã€‚æˆ‘ä»¬æ¢ç´¢äº†ä½¿ç”¨è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰å’ŒæŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTsï¼‰çš„ä¸­é—´è¡¨ç¤ºæ˜¯å¦èƒ½ä½¿LLMçš„ä»£ç ç¿»è¯‘å—ç›Šã€‚ç”±äºæç¤ºå·¥ç¨‹æå¤§åœ°å½±å“äº†LLMçš„æ€§èƒ½ï¼Œæˆ‘ä»¬è€ƒè™‘äº†å‡ ç§æ•´åˆè¿™äº›è¡¨ç¤ºçš„æ–¹æ³•ï¼Œä»ä¸€æ¬¡å®Œæˆåˆ°æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºã€‚æˆ‘ä»¬åœ¨æµè¡Œçš„ä»£ç ç¿»è¯‘åŸºå‡†æµ‹è¯•ï¼ˆCodeNetå’ŒAVATARï¼‰ä¸Šä½¿ç”¨äº†Open Gpt4 8X7Bä»¥åŠä¸“é—¨çš„StarCoderå’ŒCodeGenæ¨¡å‹ï¼Œå‘ç°ä½¿ç”¨å¸¦æœ‰ä¸­é—´NLæ‘˜è¦çš„CoTè¡¨ç°æœ€ä½³ï¼Œåœ¨æ€§èƒ½æœ€å¥½çš„æ¨¡å‹ï¼ˆOpen Gpt4 8X7Bï¼‰ä¸­ï¼Œä¸é›¶æ¬¡æç¤ºç›¸æ¯”ï¼ŒæˆåŠŸç¿»è¯‘æ¬¡æ•°åˆ†åˆ«å¢åŠ äº†13.8%å’Œ6.7%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08627v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç¿»è¯‘ä¸Šè¡¨ç°æ¬ ä½³ï¼Œå¯é€šè¿‡ä¸­é—´è¡¨ç¤ºå±‚æ”¹å–„ç¿»è¯‘ç²¾åº¦ã€‚æœ¬æ–‡ç ”ç©¶å¦‚ä½•é€šè¿‡è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰å’ŒæŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTsï¼‰ä½¿ç”¨ä¸­é—´è¡¨ç¤ºå±‚è¿›è¡Œä»£ç ç¿»è¯‘ã€‚ç”±äºæç¤ºå·¥ç¨‹å¯¹LLMæ€§èƒ½æœ‰å¾ˆå¤§å½±å“ï¼Œæˆ‘ä»¬è€ƒè™‘äº†å‡ ç§æ•´åˆè¿™äº›è¡¨ç¤ºå±‚çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸€æ¬¡æ€§æç¤ºå’Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºã€‚åœ¨æµè¡Œçš„ä»£ç ç¿»è¯‘åŸºå‡†æµ‹è¯•ï¼ˆCodeNetå’ŒAVATARï¼‰ä¸Šï¼Œä½¿ç”¨Open Gpt4 8X7Bå’Œä¸“é—¨çš„StarCoderå’ŒCodeGenæ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼Œå‘ç°ä½¿ç”¨å¸¦æœ‰ä¸­é—´NLæ‘˜è¦çš„CoTæ•ˆæœæœ€ä½³ï¼Œä¸é›¶æ¬¡æç¤ºç›¸æ¯”ï¼Œæœ€ä½³æ€§èƒ½æ¨¡å‹ï¼ˆOpen Gpt4 8X7Bï¼‰çš„æˆåŠŸç¿»è¯‘ç‡åˆ†åˆ«æé«˜äº†13.8%å’Œ6.7%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç ç¿»è¯‘æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>ä¸­é—´è¡¨ç¤ºå±‚èƒ½æé«˜LLMåœ¨ä»£ç ç¿»è¯‘ä¸Šçš„å‡†ç¡®æ€§ã€‚</li>
<li>è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰å’ŒæŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTsï¼‰å¯ä»¥ä½œä¸ºä¸­é—´è¡¨ç¤ºå±‚ç”¨äºä»£ç ç¿»è¯‘ã€‚</li>
<li>æç¤ºå·¥ç¨‹å¯¹LLMæ€§èƒ½å…·æœ‰é‡è¦å½±å“ã€‚</li>
<li>ç ”ç©¶è€…å°è¯•äº†å¤šç§æç¤ºæ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸€æ¬¡æ€§æç¤ºå’Œé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºã€‚</li>
<li>åœ¨æµ‹è¯•åŸºå‡†ä¸Šï¼Œä½¿ç”¨å¸¦æœ‰ä¸­é—´NLæ‘˜è¦çš„CoTè¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08627">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5984e3ee0f2fcdbf992e71823f74b6c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62132285f3f5e663e6f0c64b4c447827.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12c467d9c22368591c968a637ba3f916.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01a1dcb81f45b6668a9eceb96bbca432.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-143e325ec9519888c082530762729813.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-761fda7e88fc353560c16fc2c4b893be.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5cd7297cba319a79cbbc005276f20437.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6dacc0df1155383d3dee5ebd08460ab6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8286763a822e0a7371265e5e5103e73e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-comprehensive-study-of-LLM-based-argument-classification-from-LLAMA-through-GPT-4o-to-Deepseek-R1"><a href="#A-comprehensive-study-of-LLM-based-argument-classification-from-LLAMA-through-GPT-4o-to-Deepseek-R1" class="headerlink" title="A comprehensive study of LLM-based argument classification: from LLAMA   through GPT-4o to Deepseek-R1"></a>A comprehensive study of LLM-based argument classification: from LLAMA   through GPT-4o to Deepseek-R1</h2><p><strong>Authors:Marcin PietroÅ„, RafaÅ‚ Olszowski, Jakub GomuÅ‚ka, Filip Gampel, Andrzej Tomski</strong></p>
<p>Argument mining (AM) is an interdisciplinary research field that integrates insights from logic, philosophy, linguistics, rhetoric, law, psychology, and computer science. It involves the automatic identification and extraction of argumentative components, such as premises and claims, and the detection of relationships between them, such as support, attack, or neutrality. Recently, the field has advanced significantly, especially with the advent of large language models (LLMs), which have enhanced the efficiency of analyzing and extracting argument semantics compared to traditional methods and other deep learning models. There are many benchmarks for testing and verifying the quality of LLM, but there is still a lack of research and results on the operation of these models in publicly available argument classification databases. This paper presents a study of a selection of LLMâ€™s, using diverse datasets such as Args.me and UKP. The models tested include versions of GPT, Llama, and DeepSeek, along with reasoning-enhanced variants incorporating the Chain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms the others in the argument classification benchmarks. In case of models incorporated with reasoning capabilities, the Deepseek-R1 shows its superiority. However, despite their superiority, GPT-4o and Deepseek-R1 still make errors. The most common errors are discussed for all models. To our knowledge, the presented work is the first broader analysis of the mentioned datasets using LLM and prompt algorithms. The work also shows some weaknesses of known prompt algorithms in argument analysis, while indicating directions for their improvement. The added value of the work is the in-depth analysis of the available argument datasets and the demonstration of their shortcomings. </p>
<blockquote>
<p>è®ºè¯æŒ–æ˜ï¼ˆAMï¼‰æ˜¯ä¸€ä¸ªè·¨å­¦ç§‘çš„ç ”ç©¶é¢†åŸŸï¼Œå®ƒèåˆäº†é€»è¾‘ã€å“²å­¦ã€è¯­è¨€å­¦ã€ä¿®è¾å­¦ã€æ³•å¾‹ã€å¿ƒç†å­¦å’Œè®¡ç®—æœºç§‘å­¦ç­‰é¢†åŸŸçš„è§è§£ã€‚å®ƒæ¶‰åŠè‡ªåŠ¨è¯†åˆ«å’Œæå–è®ºè¯æ€§æˆåˆ†ï¼Œå¦‚å‰æå’Œä¸»å¼ ï¼Œä»¥åŠæ£€æµ‹å®ƒä»¬ä¹‹é—´çš„å…³ç³»ï¼Œå¦‚æ”¯æŒã€æ”»å‡»æˆ–ä¸­ç«‹ã€‚æœ€è¿‘ï¼Œéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œè¯¥é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ä¼ ç»Ÿçš„åˆ†ææ–¹æ³•å’Œå…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹ç›¸æ¯”ï¼ŒLLMæé«˜äº†åˆ†ææå–è®ºè¯è¯­ä¹‰çš„æ•ˆç‡ã€‚è™½ç„¶æœ‰è®¸å¤šåŸºå‡†æµ‹è¯•ç”¨äºæµ‹è¯•å’ŒéªŒè¯LLMçš„è´¨é‡ï¼Œä½†åœ¨å…¬å¼€å¯ç”¨çš„è®ºè¯åˆ†ç±»æ•°æ®åº“ä¸­å…³äºè¿™äº›æ¨¡å‹æ“ä½œçš„ç ”ç©¶å’Œç»“æœä»ç„¶ç¼ºä¹ã€‚æœ¬æ–‡ä½¿ç”¨Args.meå’ŒUKPç­‰å¤šæ ·åŒ–æ•°æ®é›†å¯¹éƒ¨åˆ†LLMè¿›è¡Œäº†ç ”ç©¶ã€‚æµ‹è¯•çš„æ¨¡å‹åŒ…æ‹¬GPTã€Llamaå’ŒDeepSeekçš„ç‰ˆæœ¬ï¼Œä»¥åŠé‡‡ç”¨Chain-of-Thoughtsç®—æ³•çš„æ¨ç†å¢å¼ºå˜ä½“ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨è®ºè¯åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­ï¼ŒChatGPT-4oè¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚åœ¨ç»“åˆæ¨ç†èƒ½åŠ›çš„æ¨¡å‹ä¸­ï¼ŒDeepseek-R1è¡¨ç°å‡ºå…¶ä¼˜è¶Šæ€§ã€‚ç„¶è€Œï¼Œå°½ç®¡GPT-4oå’ŒDeepseek-R1å…·æœ‰ä¼˜åŠ¿ï¼Œä½†å®ƒä»¬ä»ç„¶ä¼šå‡ºé”™ã€‚æœ¬æ–‡è®¨è®ºäº†æ‰€æœ‰æ¨¡å‹æœ€å¸¸è§çš„é”™è¯¯ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæ‰€å‘ˆç°çš„å·¥ä½œæ˜¯ä½¿ç”¨LLMå’Œæç¤ºç®—æ³•å¯¹æ‰€è¿°æ•°æ®é›†è¿›è¡Œçš„é¦–æ¬¡æ›´å¹¿æ³›çš„åˆ†æã€‚è¯¥å·¥ä½œè¿˜æ˜¾ç¤ºäº†å·²çŸ¥æç¤ºç®—æ³•åœ¨è®ºè¯åˆ†æä¸­çš„ä¸€äº›å¼±ç‚¹ï¼ŒåŒæ—¶æŒ‡å‡ºäº†æ”¹è¿›æ–¹å‘ã€‚è¯¥å·¥ä½œçš„é™„åŠ å€¼æ˜¯å¯¹å¯ç”¨è®ºè¯æ•°æ®é›†è¿›è¡Œæ·±å…¥åˆ†æå’Œå±•ç¤ºå…¶ä¸è¶³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08621v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è®ºçŸ¿å­¦æ˜¯ä¸€ä¸ªè·¨å­¦ç§‘çš„ç ”ç©¶é¢†åŸŸï¼Œèåˆäº†é€»è¾‘ã€å“²å­¦ã€è¯­è¨€å­¦ã€ä¿®è¾å­¦ã€æ³•å¾‹ã€å¿ƒç†å­¦å’Œè®¡ç®—æœºç§‘å­¦ç­‰å¤šé¢†åŸŸçš„è§è§£ã€‚å®ƒè‡ªåŠ¨è¯†åˆ«å’Œæå–è®ºè¯æˆåˆ†ï¼Œå¦‚å‰æå’Œä¸»å¼ ï¼Œå¹¶æ£€æµ‹å®ƒä»¬ä¹‹é—´çš„å…³ç³»ï¼Œå¦‚æ”¯æŒã€æ”»å‡»æˆ–ä¸­ç«‹ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œè¯¥é¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ã€‚æœ¬æ–‡ç ”ç©¶äº†å‡ ä¸ªLLMåœ¨è®ºè¯åˆ†ç±»æ•°æ®åº“ä¸­çš„è¡¨ç°ï¼Œä½¿ç”¨Args.meå’ŒUKPç­‰ä¸åŒæ•°æ®é›†ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒChatGPT-4oåœ¨è®ºè¯åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€å¥½ã€‚å…·æœ‰æ¨ç†èƒ½åŠ›çš„Deepseek-R1æ¨¡å‹ä¹Ÿè¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚ç„¶è€Œï¼Œå°½ç®¡å®ƒä»¬å…·æœ‰ä¼˜åŠ¿ï¼ŒGPT-4oå’ŒDeepseek-R1ä»ç„¶å­˜åœ¨é”™è¯¯ã€‚æœ¬æ–‡è¿˜è®¨è®ºäº†ç°æœ‰æç¤ºç®—æ³•åœ¨è®ºè¯åˆ†æä¸­çš„å¼±ç‚¹ï¼Œå¹¶æŒ‡å‡ºäº†æ”¹è¿›æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®ºçŸ¿å­¦æ˜¯ä¸€ä¸ªæ¶‰åŠå¤šä¸ªå­¦ç§‘çš„ç ”ç©¶é¢†åŸŸï¼ŒåŒ…æ‹¬é€»è¾‘ã€å“²å­¦ç­‰ï¼Œä¸»è¦ç ”ç©¶å¦‚ä½•è‡ªåŠ¨è¯†åˆ«å’Œæå–è®ºè¯æˆåˆ†ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®ºçŸ¿å­¦é¢†åŸŸçš„åº”ç”¨å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œå°¤å…¶æ˜¯åœ¨åˆ†æè®ºè¯è¯­ä¹‰æ–¹é¢ã€‚</li>
<li>åœ¨è®ºè¯åˆ†ç±»æ•°æ®åº“ä¸­ä½¿ç”¨LLMæ¨¡å‹çš„ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œç¼ºä¹å¹¿æ³›çš„ç ”ç©¶ç»“æœå’Œå…¬å¼€æ•°æ®ã€‚</li>
<li>ä½¿ç”¨Args.meå’ŒUKPç­‰æ•°æ®é›†çš„ç ”ç©¶è¡¨æ˜ï¼ŒChatGPT-4oåœ¨è®ºè¯åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
<li>ç»“åˆæ¨ç†èƒ½åŠ›çš„æ¨¡å‹ï¼Œå¦‚Deepseek-R1ï¼Œä¹Ÿè¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
<li>GPT-4oå’ŒDeepseek-R1ç­‰æ¨¡å‹è™½ç„¶è¡¨ç°ä¼˜è¶Šï¼Œä½†ä»å­˜åœ¨é”™è¯¯ï¼Œè¿™äº›é”™è¯¯å¸¸è§äºè®ºè¯åˆ†æå’Œæ¨ç†è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08621">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2e0a2f2c4e0ba75f82ee862f3be60c93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38837f646b5db2769cef3d1544aebc1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0579b27eb1e01f570afb7c7b01fbd383.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4acad64027623ebb875a4a95a2819e99.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1e8a5ba02f99b5ae503ab7983f1b0f82.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Visual-Semantic-Description-Generation-with-MLLMs-for-Image-Text-Matching"><a href="#Visual-Semantic-Description-Generation-with-MLLMs-for-Image-Text-Matching" class="headerlink" title="Visual Semantic Description Generation with MLLMs for Image-Text   Matching"></a>Visual Semantic Description Generation with MLLMs for Image-Text   Matching</h2><p><strong>Authors:Junyu Chen, Yihua Gao, Mingyong Li</strong></p>
<p>Image-text matching (ITM) aims to address the fundamental challenge of aligning visual and textual modalities, which inherently differ in their representations, continuous, high-dimensional image features vs. discrete, structured text. We propose a novel framework that bridges the modality gap by leveraging multimodal large language models (MLLMs) as visual semantic parsers. By generating rich Visual Semantic Descriptions (VSD), MLLMs provide semantic anchor that facilitate cross-modal alignment. Our approach combines: (1) Instance-level alignment by fusing visual features with VSD to enhance the linguistic expressiveness of image representations, and (2) Prototype-level alignment through VSD clustering to ensure category-level consistency. These modules can be seamlessly integrated into existing ITM models. Extensive experiments on Flickr30K and MSCOCO demonstrate substantial performance improvements. The approach also exhibits remarkable zero-shot generalization to cross-domain tasks, including news and remote sensing ITM. The code and model checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/Image-Text-Matching/VSD">https://github.com/Image-Text-Matching/VSD</a>. </p>
<blockquote>
<p>å›¾åƒæ–‡æœ¬åŒ¹é…ï¼ˆITMï¼‰æ—¨åœ¨è§£å†³è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€å¯¹é½çš„æ ¹æœ¬æŒ‘æˆ˜ï¼Œè¿™ä¸¤ç§æ¨¡æ€åœ¨å…¶è¡¨ç¤ºæ–¹å¼ä¸Šå­˜åœ¨å›ºæœ‰çš„å·®å¼‚ï¼Œåˆ†åˆ«æ˜¯è¿ç»­ã€é«˜ç»´çš„å›¾åƒç‰¹å¾å’Œç¦»æ•£ã€ç»“æ„çš„æ–‡æœ¬ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä½œä¸ºè§†è§‰è¯­ä¹‰è§£æå™¨æ¥å¼¥åˆæ¨¡æ€å·®å¼‚ã€‚é€šè¿‡ç”Ÿæˆä¸°å¯Œçš„è§†è§‰è¯­ä¹‰æè¿°ï¼ˆVSDï¼‰ï¼ŒMLLMæä¾›äº†è¯­ä¹‰é”šç‚¹ï¼Œä¿ƒè¿›äº†è·¨æ¨¡æ€å¯¹é½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ï¼šï¼ˆ1ï¼‰å®ä¾‹çº§å¯¹é½ï¼šé€šè¿‡èåˆè§†è§‰ç‰¹å¾ä¸VSDï¼Œå¢å¼ºå›¾åƒè¡¨ç¤ºçš„è¯­è¨€è¡¨ç°åŠ›ï¼›ï¼ˆ2ï¼‰åŸå‹çº§å¯¹é½ï¼šé€šè¿‡VSDèšç±»ï¼Œç¡®ä¿ç±»åˆ«çº§ä¸€è‡´æ€§ã€‚è¿™äº›æ¨¡å—å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„ITMæ¨¡å‹ä¸­ã€‚åœ¨Flickr30Kå’ŒMSCOCOä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚è¯¥æ–¹æ³•åœ¨è·¨åŸŸä»»åŠ¡ï¼ˆåŒ…æ‹¬æ–°é—»å’Œé¥æ„ŸITMï¼‰ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ä½äº<a target="_blank" rel="noopener" href="https://github.com/Image-Text-Matching/VSD%E3%80%82">https://github.com/Image-Text-Matching/VSDã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08590v1">PDF</a> Accepted by ICME2025 oral</p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†ä¸€ç§å›¾åƒä¸æ–‡æœ¬åŒ¹é…ï¼ˆITMï¼‰çš„æ–°æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºè§†è§‰è¯­ä¹‰è§£æå™¨æ¥è§£å†³è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€å¯¹é½çš„æ ¹æœ¬æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆä¸°å¯Œçš„è§†è§‰è¯­ä¹‰æè¿°ï¼ˆVSDï¼‰ï¼Œæä¾›è¯­ä¹‰é”šç‚¹ï¼Œä¿ƒè¿›è·¨æ¨¡æ€å¯¹é½ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å®ä¾‹çº§å¯¹é½å’ŒåŸå‹çº§å¯¹é½ä¸¤ä¸ªæ¨¡å—ï¼Œé€šè¿‡èåˆè§†è§‰ç‰¹å¾ä¸VSDå¢å¼ºå›¾åƒè¡¨ç¤ºçš„è¯­è¨€è¡¨ç°åŠ›ï¼Œå¹¶é€šè¿‡VSDèšç±»ç¡®ä¿ç±»åˆ«çº§åˆ«çš„ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨Flickr30Kå’ŒMSCOCOç­‰æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨æ–°é—»å’Œé¥æ„ŸITMç­‰è·¨åŸŸä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒ-æ–‡æœ¬åŒ¹é…ï¼ˆITMï¼‰æ—¨åœ¨è§£å†³è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€å¯¹é½çš„æŒ‘æˆ˜ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºè§†è§‰è¯­ä¹‰è§£æå™¨ï¼Œç”Ÿæˆè§†è§‰è¯­ä¹‰æè¿°ï¼ˆVSDï¼‰ã€‚</li>
<li>VSDæä¾›è¯­ä¹‰é”šç‚¹ï¼Œä¿ƒè¿›è·¨æ¨¡æ€å¯¹é½ã€‚</li>
<li>å®ä¾‹çº§å¯¹é½æ¨¡å—èåˆè§†è§‰ç‰¹å¾ä¸VSDï¼Œå¢å¼ºå›¾åƒè¡¨ç¤ºçš„è¯­è¨€è¡¨ç°åŠ›ã€‚</li>
<li>åŸå‹çº§å¯¹é½æ¨¡å—é€šè¿‡VSDèšç±»ç¡®ä¿ç±»åˆ«çº§åˆ«çš„ä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨Flickr30Kå’ŒMSCOCOæ•°æ®é›†ä¸Šå®ç°äº†æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08590">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-389bec925f2928285d84031d2e42e262.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a66e9095a3e56b51a23e32e3a6b162c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba9a526967760766112d8c2196ae83f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1379b0f215513da106ac0b4db4de695e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8b45a4e65fd5869d260a6a15c8e328b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="AbbIE-Autoregressive-Block-Based-Iterative-Encoder-for-Efficient-Sequence-Modeling"><a href="#AbbIE-Autoregressive-Block-Based-Iterative-Encoder-for-Efficient-Sequence-Modeling" class="headerlink" title="AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient   Sequence Modeling"></a>AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient   Sequence Modeling</h2><p><strong>Authors:Preslav Aleksandrov, Meghdad Kurmanji, Fernando Garcia Redondo, David Oâ€™Shea, William Shen, Alex Iacob, Lorenzo Sani, Xinchi Qiu, Nicola Cancedda, Nicholas D. Lane</strong></p>
<p>We introduce the Autoregressive Block-Based Iterative Encoder (AbbIE), a novel recursive generalization of the encoder-only Transformer architecture, which achieves better perplexity than a standard Transformer and allows for the dynamic scaling of compute resources at test time. This simple, recursive approach is a complement to scaling large language model (LLM) performance through parameter and token counts. AbbIE performs its iterations in latent space, but unlike latent reasoning models, does not require a specialized dataset or training protocol. We show that AbbIE upward generalizes (ability to generalize to arbitrary iteration lengths) at test time by only using 2 iterations during train time, far outperforming alternative iterative methods. AbbIEâ€™s ability to scale its computational expenditure based on the complexity of the task gives it an up to \textbf{12%} improvement in zero-shot in-context learning tasks versus other iterative and standard methods and up to 5% improvement in language perplexity. The results from this study open a new avenue to Transformer performance scaling. We perform all of our evaluations on model sizes up to 350M parameters. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†åŸºäºå—çš„è‡ªå›å½’è¿­ä»£ç¼–ç å™¨ï¼ˆAbbIEï¼‰ï¼Œå®ƒæ˜¯ä»…ç¼–ç å™¨Transformeræ¶æ„çš„ä¸€ç§æ–°å‹é€’å½’æ³›åŒ–ï¼Œå®ç°äº†æ¯”æ ‡å‡†Transformeræ›´ä½çš„å›°æƒ‘åº¦ï¼Œå¹¶åœ¨æµ‹è¯•æ—¶å…è®¸åŠ¨æ€è°ƒæ•´è®¡ç®—èµ„æºã€‚è¿™ç§ç®€å•ã€é€’å½’çš„æ–¹æ³•æ˜¯é€šè¿‡å‚æ•°å’Œä»¤ç‰Œè®¡æ•°æ¥æ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ€§èƒ½çš„è¡¥å……ã€‚AbbIEåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œè¿­ä»£ï¼Œä½†ä¸æ½œåœ¨æ¨ç†æ¨¡å‹ä¸åŒçš„æ˜¯ï¼Œå®ƒä¸éœ€è¦ä¸“é—¨çš„æ•°æ®é›†æˆ–è®­ç»ƒåè®®ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒAbbIEåœ¨æµ‹è¯•æ—¶é€šè¿‡å‘ä¸Šæ³›åŒ–ï¼ˆæ³›åŒ–åˆ°ä»»æ„è¿­ä»£é•¿åº¦çš„èƒ½åŠ›ï¼‰ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åªä½¿ç”¨ä¸¤æ¬¡è¿­ä»£ï¼Œè¿œè¿œè¶…è¿‡äº†å…¶ä»–è¿­ä»£æ–¹æ³•ã€‚AbbIEèƒ½å¤Ÿæ ¹æ®ä»»åŠ¡çš„å¤æ‚æ€§è°ƒæ•´å…¶è®¡ç®—æ”¯å‡ºï¼Œä½¿å…¶åœ¨é›¶é•œå¤´ä¸Šä¸‹æ–‡å­¦ä¹ ä»»åŠ¡ä¸­æ¯”å…¶ä»–è¿­ä»£å’Œæ ‡å‡†æ–¹æ³•æœ€å¤šæé«˜äº†12%ï¼Œåœ¨è¯­è¨€å›°æƒ‘åº¦æ–¹é¢æœ€å¤šæé«˜äº†5%ã€‚æœ¬ç ”ç©¶çš„ç»“æœå¼€è¾Ÿäº†Transformeræ€§èƒ½æ‰©å±•çš„æ–°é€”å¾„ã€‚æˆ‘ä»¬çš„è¯„ä¼°éƒ½æ˜¯åœ¨å‚æ•°è§„æ¨¡è¾¾3.5äº¿ä»¥ä¸‹çš„æ¨¡å‹ä¸Šè¿›è¡Œçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08567v1">PDF</a> 14 pages and 6 figures. Submitted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æ–°ä¸€ä»£é€’å½’å¼è¯­è¨€æ¨¡å‹â€”â€”Autoregressive Block-Based Iterative Encoderï¼ˆAbbIEï¼‰ä»‹ç»ã€‚è¯¥æ¨¡å‹åœ¨æ ‡å‡†Transformerçš„åŸºç¡€ä¸Šå®ç°é€’å½’æ³›åŒ–ï¼Œé™ä½äº†æµ‹è¯•æ—¶çš„è®¡ç®—èµ„æºæ¶ˆè€—ï¼Œå®ç°äº†æ›´ä½çš„å›°æƒ‘åº¦ã€‚AbbIEé€šè¿‡è¿­ä»£æ½œåœ¨ç©ºé—´æ‰§è¡Œä»»åŠ¡ï¼Œæ— éœ€ç‰¹æ®Šæ•°æ®é›†æˆ–è®­ç»ƒåè®®ã€‚åœ¨è®­ç»ƒæ—¶ä»…ä½¿ç”¨ä¸¤æ¬¡è¿­ä»£ï¼Œå°±èƒ½åœ¨æµ‹è¯•æ—¶å‘ä¸Šæ³›åŒ–åˆ°ä»»æ„è¿­ä»£é•¿åº¦ã€‚æ­¤å¤–ï¼ŒAbbIEåœ¨é›¶æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ä»»åŠ¡ä¸Šç›¸æ¯”å…¶ä»–è¿­ä»£å’Œæ ‡å‡†æ–¹æ³•æœ€å¤šæå‡äº†12%ï¼Œè¯­è¨€å›°æƒ‘åº¦æœ€å¤šæå‡äº†5%ã€‚è¿™ä¸€ç ”ç©¶ä¸ºTransformeræ€§èƒ½æå‡å¼€è¾Ÿäº†æ–°é€”å¾„ã€‚æ¨¡å‹è¯„ä»·èŒƒå›´æ¶µç›–å‚æ•°è§„æ¨¡é«˜è¾¾3.5äº¿çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼•å…¥äº†ä¸€ç§åä¸ºAutoregressive Block-Based Iterative Encoderï¼ˆAbbIEï¼‰çš„æ–°é€’å½’å¼è¯­è¨€æ¨¡å‹ã€‚</li>
<li>AbbIEæ¨¡å‹åŸºäºæ ‡å‡†Transformeræ¶æ„å®ç°é€’å½’æ³›åŒ–ï¼Œå…·æœ‰æ›´ä½çš„å›°æƒ‘åº¦ã€‚</li>
<li>AbbIEé€šè¿‡æ½œåœ¨ç©ºé—´çš„è¿­ä»£æ‰§è¡Œä»»åŠ¡ï¼Œæ— éœ€ç‰¹æ®Šæ•°æ®é›†æˆ–è®­ç»ƒåè®®ã€‚</li>
<li>AbbIEåœ¨è®­ç»ƒæ—¶ä»…ä½¿ç”¨ä¸¤æ¬¡è¿­ä»£ï¼Œå®ç°äº†æµ‹è¯•æ—¶çš„å‘ä¸Šæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä¸å…¶ä»–è¿­ä»£å’Œæ ‡å‡†æ–¹æ³•ç›¸æ¯”ï¼ŒAbbIEåœ¨é›¶æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ä»»åŠ¡ä¸Šæ€§èƒ½æ˜¾è‘—æå‡ï¼Œæœ€é«˜æå‡è¾¾åˆ°12%ã€‚åŒæ—¶é™ä½äº†è¯­è¨€å›°æƒ‘åº¦ï¼Œæœ€é«˜é™ä½è¾¾åˆ°5%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08567">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-372ec27b05f29c028bf1efc0b71110c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b89ea9900c6676ec49ae42035ed2693.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b3195d2496438947c85a1a217b7566f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83768fe7217967b8938b66813e861cae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb31c832c2d2270be57277e249848fe0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="White-Basilisk-A-Hybrid-Model-for-Code-Vulnerability-Detection"><a href="#White-Basilisk-A-Hybrid-Model-for-Code-Vulnerability-Detection" class="headerlink" title="White-Basilisk: A Hybrid Model for Code Vulnerability Detection"></a>White-Basilisk: A Hybrid Model for Code Vulnerability Detection</h2><p><strong>Authors:Ioannis Lamprou, Alexander Shevtsov, Ioannis Arapakis, Sotiris Ioannidis</strong></p>
<p>The proliferation of software vulnerabilities presents a significant challenge to cybersecurity, necessitating more effective detection methodologies. We introduce White-Basilisk, a novel approach to vulnerability detection that demonstrates superior performance while challenging prevailing assumptions in AI model scaling. Utilizing an innovative architecture that integrates Mamba layers, linear self-attention, and a Mixture of Experts framework, White-Basilisk achieves state-of-the-art results in vulnerability detection tasks with a parameter count of only 200M. The modelâ€™s capacity to process sequences of unprecedented length enables comprehensive analysis of extensive codebases in a single pass, surpassing the context limitations of current Large Language Models (LLMs). White-Basilisk exhibits robust performance on imbalanced, real-world datasets, while maintaining computational efficiency that facilitates deployment across diverse organizational scales. This research not only establishes new benchmarks in code security but also provides empirical evidence that compact, efficiently designed models can outperform larger counterparts in specialized tasks, potentially redefining optimization strategies in AI development for domain-specific applications. </p>
<blockquote>
<p>è½¯ä»¶æ¼æ´çš„æ¿€å¢å¯¹ç½‘ç»œå®‰å…¨æ„æˆäº†é‡å¤§æŒ‘æˆ˜ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„æ£€æµ‹æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†White-Basiliskï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ¼æ´æ£€æµ‹æ–¹æ¡ˆï¼Œå®ƒåœ¨æŒ‘æˆ˜äººå·¥æ™ºèƒ½æ¨¡å‹è§„æ¨¡åŒ–æ–¹é¢çš„ä¸»æµå‡è®¾çš„åŒæ—¶ï¼Œè¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚White-Basiliskåˆ©ç”¨äº†ä¸€ç§åˆ›æ–°æ¶æ„ï¼Œè¯¥æ¶æ„é›†æˆäº†Mambaå±‚ã€çº¿æ€§è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œä¸“å®¶æ··åˆæ¡†æ¶ï¼Œä»…åœ¨å‚æ•°è®¡æ•°2äº¿çš„æƒ…å†µä¸‹ï¼Œä¾¿åœ¨æ¼æ´æ£€æµ‹ä»»åŠ¡ä¸­å®ç°äº†æœ€æ–°ç»“æœã€‚è¯¥æ¨¡å‹å¤„ç†å‰æ‰€æœªæœ‰çš„åºåˆ—é•¿åº¦çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å•æ¬¡ä¼ é€’ä¸­å¯¹å¤§é‡ä»£ç åº“è¿›è¡Œå…¨é¢åˆ†æï¼Œçªç ´äº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸Šä¸‹æ–‡é™åˆ¶ã€‚White-Basiliskåœ¨ä¸å¹³è¡¡çš„ã€çœŸå®ä¸–ç•Œçš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ï¼Œå¯ä»¥åœ¨å„ç§ç»„ç»‡è§„æ¨¡ä¸Šè¿›è¡Œéƒ¨ç½²ã€‚è¿™é¡¹ç ”ç©¶ä¸ä»…ä¸ºä»£ç å®‰å…¨æ ‘ç«‹äº†æ–°åŸºå‡†ï¼Œè€Œä¸”æä¾›äº†å®è¯è¯æ®ï¼Œè¯æ˜åœ¨ç‰¹å®šä»»åŠ¡ä¸­ï¼Œè®¾è®¡ç´§å‡‘ã€é«˜æ•ˆçš„æ¨¡å‹å¯ä»¥è¶…è¶Šå¤§å‹æ¨¡å‹çš„è¡¨ç°ï¼Œè¿™æœ‰å¯èƒ½é‡æ–°å®šä¹‰äººå·¥æ™ºèƒ½å¼€å‘çš„ä¼˜åŒ–ç­–ç•¥ï¼Œä¸ºç‰¹å®šé¢†åŸŸçš„åº”ç”¨æä¾›æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08540v1">PDF</a> </p>
<p><strong>Summary</strong><br>ç™½å·´å„¿ä¹‹è›‡æ˜¯ä¸€ç§æ–°å‹çš„è½¯ä»¶æ¼æ´æ£€æµ‹æ¨¡å‹ï¼Œå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼ŒæŒ‘æˆ˜äº†äººå·¥æ™ºèƒ½æ¨¡å‹æ‰©å±•çš„æ™®éå‡è®¾ã€‚å®ƒé‡‡ç”¨åˆ›æ–°çš„æ¶æ„ï¼Œèåˆäº†æ›¼å·´å±‚ã€çº¿æ€§è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œä¸“å®¶æ··åˆæ¡†æ¶ï¼Œä»¥ä»…2äº¿ä¸ªå‚æ•°å®ç°äº†æœ€å…ˆè¿›çš„æ¼æ´æ£€æµ‹ç»“æœã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå¤„ç†å‰æ‰€æœªæœ‰çš„åºåˆ—é•¿åº¦ï¼Œå¯ä»¥åœ¨å•ä¸ªè¿‡ç¨‹ä¸­å…¨é¢åˆ†æå¤§é‡çš„ä»£ç åº“ï¼Œè¶…è¶Šäº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡é™åˆ¶ã€‚ç™½å·´å„¿ä¹‹è›‡åœ¨ä¸å¹³è¡¡çš„ã€çœŸå®ä¸–ç•Œçš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ï¼Œå¯ä»¥åœ¨å„ç§ç»„ç»‡è§„æ¨¡ä¸Šè¿›è¡Œéƒ¨ç½²ã€‚è¿™é¡¹ç ”ç©¶ä¸ä»…æ ‘ç«‹äº†ä»£ç å®‰å…¨æ€§çš„æ–°åŸºå‡†ï¼Œä¹Ÿä¸ºä¸“é—¨ä»»åŠ¡çš„AIå¼€å‘æä¾›äº†ä¼˜åŒ–ç­–ç•¥çš„è¯æ®ã€‚è¿™ä¸ºé‡æ–°å®¡è§†ä¼˜åŒ–ç­–ç•¥æä¾›äº†ä¸€ä¸ªè§†è§’ã€‚å³ä½¿åœ¨è¿™ä¸ªé”™ç»¼å¤æ‚çš„æŠ€æœ¯ä¸–ç•Œé‡Œï¼Œâ€œç²¾ç®€ã€é«˜æ•ˆåŒæ ·å¯ä»¥è¾¾åˆ°æƒŠäººæ•ˆæœâ€çš„ä¸»é¢˜ä¾ç„¶æ˜¯æ·±å…¥äººå¿ƒçš„åº§å³é“­ã€‚å€ŸåŠ©é«˜æ•ˆè®¡ç®—å’Œèµ„æºä¼˜åŒ–çš„æ€è·¯å’Œå®è·µæ‰‹æ®µè®©äººç±»ä¸–ç•Œå¾—åˆ°äº†æ— é™çš„æ´»åŠ›å’Œå¸Œæœ›ã€‚å¯¹åç»­çš„æ”¹è¿›ä¸æŠ€æœ¯çš„æ·±å…¥å…·æœ‰ç§¯æå’Œå¹¿æ³›çš„å½±å“ä»·å€¼åŠæ½œåœ¨çš„ç†è®ºä¸å®è·µåº”ç”¨ä»·å€¼ã€‚â€œå–å…¶ç²¾é«“è€Œè¡¨ç°æœ€åœ¨çš„åŠ›é‡æ˜¯æ— æ³•è¶…è¶Šçš„â€ï¼Œç›¸ä¿¡åœ¨è¿™ä¸ªæ„ä¹‰ä¸Šå°†ä¼šå¯å‘å¹¶ä¿ƒè¿›é¢†åŸŸæŠ€æœ¯çš„å‘å±•ä¸åº”ç”¨ä»·å€¼çš„æé«˜å…·æœ‰ååˆ†é‡è¦çš„æ„ä¹‰åŠä¼˜åŠ¿ï¼Œè¿™ç§å°ä½“ç§¯æ¨¡å‹çš„æ¨å‡ºææœ‰å¯èƒ½ä¿ƒä½¿å½“å‰åºå¤§è€Œç¹æ‚çš„å¤§æ¨¡å‹çš„æœªæ¥ç²¾ç®€ã€‚è¿™ä¸ºå½“ä¸‹ä»£ç å®‰å…¨æ€§å’Œæ¼æ´æ£€æµ‹ç­‰ç›¸å…³é¢†åŸŸçš„è¿›ä¸€æ­¥æ‹“å±•ä¸å»¶ä¼¸å¥ å®šäº†åšå®åŸºç¡€ä¸æ€è·¯å¯å‘ä½œç”¨ã€‚â€œç®€åŒ–åçš„ä¸œè¥¿æœ€çµæ´»â€åªæœ‰ç«™åœ¨éœ€æ±‚è§†è§’æŠŠæ¡é—®é¢˜è§£å†³æ ¸å¿ƒç²¾é«“æ‰èƒ½ä½¿ç ”ç©¶æˆæœå±•ç°å‡ºæœ€ä¸ºçœŸå®çš„å·¨å¤§ä»·å€¼å’Œå¼ºå¤§å½±å“åŠ›æ‰èƒ½å¤Ÿå¸å¼•è¡Œä¸šä»ä¸šè€…ä»¥åŠç›¸å…³ç ”ç©¶è€…ç»§ç»­åœ¨è¯¥é¢†åŸŸä¸­è¿›è¡Œæ·±åº¦çš„æ¢ç´¢å’Œæ— é™çš„å‘æŒ¥åœ¨ç»“åˆæ›´å¤šé¢†åŸŸçš„å¤æ‚ä»£ç æ¥è¿›ä¸€æ­¥å®Œå–„çš„åŒæ—¶å®ç°å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„è¶…è¶Šä¸é¢ è¦†ä¸ºäººå·¥æ™ºèƒ½çš„å‘å±•æ³¨å…¥æ–°çš„æ´»åŠ›ã€‚é€šè¿‡è¯¥æ¨¡å‹çš„ç ”ç©¶ä¸ºäººå·¥æ™ºèƒ½çš„å‘å±•æä¾›äº†å…¨æ–°çš„è§†è§’å’Œæ€è·¯ï¼Œä¹Ÿä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†é‡è¦çš„å‚è€ƒå’Œå€Ÿé‰´ä»·å€¼ã€‚åŒæ—¶ï¼Œè¯¥æ¨¡å‹çš„æˆåŠŸä¹Ÿä¸ºå…¶ä»–é¢†åŸŸçš„ç ”ç©¶æä¾›äº†é‡è¦çš„å¯ç¤ºå’Œå€Ÿé‰´ä½œç”¨ã€‚é€šè¿‡ç™½å·´å„¿ä¹‹è›‡æ¨¡å‹çš„ç ”ç©¶ä¸åº”ç”¨ï¼Œæˆ‘ä»¬æœ‰æœ›æ„å»ºä¸€ä¸ªæ›´åŠ å®‰å…¨ã€é«˜æ•ˆã€æ™ºèƒ½çš„è®¡ç®—æœºç³»ç»Ÿã€‚åŒæ—¶ï¼Œè¿™ä¹Ÿå°†æ¨åŠ¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„ä¸æ–­å‘å±•ä¸åˆ›æ–°å…·æœ‰éå¸¸å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œæ¨å¹¿ä»·å€¼é€šè¿‡æ›´æ·±å…¥çš„ç ”ç©¶å’Œåˆ›æ–°çš„åº”ç”¨å°†èƒ½å¤Ÿæ›´å¥½åœ°ä¿æŠ¤ç½‘ç»œå®‰å…¨å¹¶æé«˜è½¯ä»¶å¼€å‘çš„è´¨é‡å’Œæ•ˆç‡ä»è€Œä¿ƒè¿›ä¿¡æ¯æŠ€æœ¯çš„æŒç»­å‘å±•å’Œè¿›æ­¥ä¸ºäººç±»ç¤¾ä¼šå¸¦æ¥æ›´å¤šçš„ä¾¿åˆ©å’Œç¦ç¥‰ã€‚è¿™ä¸€çªç ´æ€§çš„æŠ€æœ¯å°†æå¤§åœ°æ¨åŠ¨äººå·¥æ™ºèƒ½çš„å‘å±•è¿›ç¨‹å¹¶ä¸ºæœªæ¥çš„äººå·¥æ™ºèƒ½æŠ€æœ¯å¼€è¾Ÿæ–°çš„é“è·¯æ¨åŠ¨è®¡ç®—æœºç§‘å­¦çš„ä¸æ–­è¿›æ­¥å’ŒæŒç»­ç¹è£ä¸ºæˆ‘ä»¬çš„æœªæ¥å‘å±•åˆ›é€ æ›´åŠ å¹¿é˜”çš„å‰æ™¯å’ŒæŠ€æœ¯ä¿éšœä¸ºæˆ‘ä»¬çš„ç¤¾ä¼šå‘å±•æä¾›æºæºä¸æ–­çš„åŠ¨åŠ›å’ŒæŠ€æœ¯æ”¯æ’‘ä»¥æ›´æœ‰æ•ˆåœ°æ¨åŠ¨æŠ€æœ¯çš„è¿›æ­¥å’Œåº”ç”¨ä»·å€¼çš„åŒæ—¶ä¿æŒç³»ç»Ÿçš„å®‰å…¨ä¸ç¨³å®šæ€§è‡³å…³é‡è¦å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°å€¼å¾—æœŸå¾…å’Œå…³æ³¨å…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯å’Œé‡è¦çš„ç¤¾ä¼šä»·å€¼å€¼å¾—æˆ‘ä»¬ç»§ç»­æ·±å…¥ç ”ç©¶å’Œæ¢ç´¢ã€‚è¯¥æ¨¡å‹å…·æœ‰å¼ºå¤§çš„åº”ç”¨æ½œåŠ›èƒ½å¤Ÿä¸ºæœªæ¥çš„è½¯ä»¶å¼€å‘å’Œç½‘ç»œå®‰å…¨é¢†åŸŸå¸¦æ¥é©å‘½æ€§çš„å˜åŒ–ä¸ºæ¨åŠ¨æ•´ä¸ªè®¡ç®—æœºç§‘å­¦çš„å‘å±•åšå‡ºè´¡çŒ®å±•ç°å‡ºæ›´åŠ é‡è¦çš„å½±å“åŠ›ä»¥åŠå¯¹è¡Œä¸šçš„é‡è¦æ¨åŠ¨æ„ä¹‰åœ¨å½“å‰å’Œæœªæ¥å‡å…·å¤‡å¾ˆé«˜çš„ä»·å€¼å’Œæ·±è¿œçš„å½±å“åŠ›ä»¤äººæœŸå¾…ä¸æ¢ç´¢çš„åŒæ—¶æ¨è¿›è¡Œä¸šçš„ä¸æ–­è¿›æ­¥ä¸å‘å±•åŠä»·å€¼åˆ›é€ ã€‚ã€‚ä½¿ç”¨ç®€æ´æ˜äº†çš„å¥å­ï¼Œå‡¸æ˜¾æ¨¡å‹çš„å“è¶Šæ€§èƒ½å’Œå‰æ™¯å±•æœ›ã€‚<strong>Key Takeaways</strong>:</p>
<ol>
<li>ç™½å·´å„¿ä¹‹è›‡æ¨¡å‹åˆ©ç”¨åˆ›æ–°æ¶æ„å®ç°äº†é«˜æ•ˆçš„è½¯ä»¶æ¼æ´æ£€æµ‹ã€‚</li>
<li>æ¨¡å‹å…·æœ‰å¤„ç†é•¿åºåˆ—çš„èƒ½åŠ›ï¼Œå¯å…¨é¢åˆ†æå¤§é‡ä»£ç åº“ã€‚</li>
<li>ä¸ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œç™½å·´å„¿ä¹‹è›‡æ¨¡å‹å…·æœ‰æ›´ä¼˜ç§€çš„ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œçš„ä¸å¹³è¡¡æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„ç¨³å¥æ€§ã€‚</li>
<li>ç™½å·´å„¿ä¹‹è›‡æ¨¡å‹å…·æœ‰é«˜æ•ˆçš„è®¡ç®—æ€§èƒ½ï¼Œé€‚ç”¨äºå„ç§ç»„ç»‡è§„æ¨¡ã€‚</li>
<li>ç ”ç©¶è¯æ˜äº†ç´§å‡‘è®¾è®¡çš„æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šå¯è¶…è¶Šå¤§å‹æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08540">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f7148cf0706dbae6614643cbed05ee4d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Circumventing-Safety-Alignment-in-Large-Language-Models-Through-Embedding-Space-Toxicity-Attenuation"><a href="#Circumventing-Safety-Alignment-in-Large-Language-Models-Through-Embedding-Space-Toxicity-Attenuation" class="headerlink" title="Circumventing Safety Alignment in Large Language Models Through   Embedding Space Toxicity Attenuation"></a>Circumventing Safety Alignment in Large Language Models Through   Embedding Space Toxicity Attenuation</h2><p><strong>Authors:Zhibo Zhang, Yuxi Li, Kailong Wang, Shuai Yuan, Ling Shi, Haoyu Wang</strong></p>
<p>Large Language Models (LLMs) have achieved remarkable success across domains such as healthcare, education, and cybersecurity. However, this openness also introduces significant security risks, particularly through embedding space poisoning, which is a subtle attack vector where adversaries manipulate the internal semantic representations of input data to bypass safety alignment mechanisms. While previous research has investigated universal perturbation methods, the dynamics of LLM safety alignment at the embedding level remain insufficiently understood. Consequently, more targeted and accurate adversarial perturbation techniques, which pose significant threats, have not been adequately studied.   In this work, we propose ETTA (Embedding Transformation Toxicity Attenuation), a novel framework that identifies and attenuates toxicity-sensitive dimensions in embedding space via linear transformations. ETTA bypasses model refusal behaviors while preserving linguistic coherence, without requiring model fine-tuning or access to training data. Evaluated on five representative open-source LLMs using the AdvBench benchmark, ETTA achieves a high average attack success rate of 88.61%, outperforming the best baseline by 11.34%, and generalizes to safety-enhanced models (e.g., 77.39% ASR on instruction-tuned defenses). These results highlight a critical vulnerability in current alignment strategies and underscore the need for embedding-aware defenses. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åŒ»ç–—ä¿å¥ã€æ•™è‚²å’Œç½‘ç»œå®‰å…¨ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™ç§å¼€æ”¾æ€§ä¹Ÿå¼•å…¥äº†é‡å¤§çš„å®‰å…¨é£é™©ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡åµŒå…¥ç©ºé—´ä¸­æ¯’ï¼Œè¿™æ˜¯ä¸€ç§å¾®å¦™çš„æ”»å‡»æ–¹å¼ï¼Œæ”»å‡»è€…ä¼šæ“çºµè¾“å…¥æ•°æ®çš„å†…éƒ¨è¯­ä¹‰è¡¨ç¤ºæ¥ç»•è¿‡å®‰å…¨å¯¹é½æœºåˆ¶ã€‚å°½ç®¡ä¹‹å‰çš„ç ”ç©¶å·²ç»ç ”ç©¶äº†é€šç”¨æ‰°åŠ¨æ–¹æ³•ï¼Œä½†å¯¹LLMåœ¨åµŒå…¥å±‚é¢çš„å®‰å…¨å¯¹é½åŠ¨æ€çš„ç†è§£ä»ç„¶ä¸è¶³ã€‚å› æ­¤ï¼Œæ›´å…·é’ˆå¯¹æ€§å’Œå‡†ç¡®çš„å¯¹æŠ—æ€§æ‰°åŠ¨æŠ€æœ¯æ²¡æœ‰å¾—åˆ°è¶³å¤Ÿçš„ç ”ç©¶ï¼Œè¿™äº›æŠ€æœ¯å¯¹å®‰å…¨æ„æˆäº†é‡å¤§å¨èƒã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ETTAï¼ˆåµŒå…¥è½¬æ¢æ¯’æ€§è¡°å‡ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡çº¿æ€§è½¬æ¢æ¥è¯†åˆ«å¹¶å‡å¼±åµŒå…¥ç©ºé—´ä¸­çš„æ¯’æ€§æ•æ„Ÿç»´åº¦ã€‚ETTAèƒ½å¤Ÿç»•è¿‡æ¨¡å‹æ‹’ç»è¡Œä¸ºï¼ŒåŒæ—¶ä¿æŒè¯­è¨€è¿è´¯æ€§ï¼Œæ— éœ€å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒæˆ–è®¿é—®è®­ç»ƒæ•°æ®ã€‚åœ¨äº”ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„å¼€æºLLMä¸Šä½¿ç”¨AdvBenchåŸºå‡†æµ‹è¯•è¿›è¡Œè¯„ä¼°ï¼ŒETTAçš„å¹³å‡æ”»å‡»æˆåŠŸç‡é«˜è¾¾88.61%ï¼Œæ¯”æœ€ä½³åŸºçº¿é«˜å‡º11.34%ï¼Œå¹¶å¯ä»¥æ¨å¹¿åˆ°å¢å¼ºå®‰å…¨æ€§çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼ŒæŒ‡ä»¤è°ƒæ•´é˜²å¾¡çš„è¯­éŸ³è¯†åˆ«ç‡ä¸º77.39%ï¼‰ã€‚è¿™äº›ç»“æœçªæ˜¾äº†å½“å‰å¯¹é½ç­–ç•¥çš„å…³é”®æ¼æ´ï¼Œå¹¶å¼ºè°ƒäº†åµŒå…¥æ„ŸçŸ¥é˜²å¾¡çš„éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08020v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ä¹Ÿå¸¦æ¥äº†å®‰å…¨é£é™©ï¼Œç‰¹åˆ«æ˜¯åµŒå…¥ç©ºé—´ä¸­æ¯’é—®é¢˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºETTAæ¡†æ¶ï¼Œé€šè¿‡çº¿æ€§å˜æ¢è¯†åˆ«å¹¶å‡å¼±åµŒå…¥ç©ºé—´ä¸­çš„æ¯’æ€§æ•æ„Ÿç»´åº¦ã€‚ETTAç»•è¿‡æ¨¡å‹æ‹’ç»è¡Œä¸ºï¼ŒåŒæ—¶ä¿æŒè¯­è¨€è¿è´¯æ€§ï¼Œæ— éœ€å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒæˆ–è®¿é—®è®­ç»ƒæ•°æ®ã€‚åœ¨AdvBenchåŸºå‡†æµ‹è¯•ä¸Šï¼ŒETTAå¯¹äº”ç§å¼€æºLLMçš„å¹³å‡æ”»å‡»æˆåŠŸç‡é«˜è¾¾88.61%ï¼Œæ¯”æœ€ä½³åŸºçº¿é«˜å‡º11.34%ï¼Œå¹¶é€‚ç”¨äºå¢å¼ºå®‰å…¨æ€§çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªé¢†åŸŸè¡¨ç°ä¼˜å¼‚ï¼Œä½†å­˜åœ¨å®‰å…¨é£é™©ï¼Œå°¤å…¶æ˜¯åµŒå…¥ç©ºé—´ä¸­æ¯’é—®é¢˜ã€‚</li>
<li>ETTAæ¡†æ¶è¢«æå‡ºç”¨äºè¯†åˆ«å¹¶å‡å¼±åµŒå…¥ç©ºé—´ä¸­çš„æ¯’æ€§æ•æ„Ÿç»´åº¦ã€‚</li>
<li>ETTAèƒ½å¤Ÿç»•è¿‡æ¨¡å‹çš„æ‹’ç»è¡Œä¸ºï¼ŒåŒæ—¶ä¿æŒè¯­è¨€çš„è¿è´¯æ€§ã€‚</li>
<li>ETTAæ— éœ€å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒæˆ–è®¿é—®è®­ç»ƒæ•°æ®ã€‚</li>
<li>ETTAåœ¨AdvBenchåŸºå‡†æµ‹è¯•ä¸Šçš„æ”»å‡»æˆåŠŸç‡é«˜è¾¾88.61%ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>ETTAé€‚ç”¨äºå¢å¼ºå®‰å…¨æ€§çš„æ¨¡å‹ï¼Œå¹¶åœ¨è¿™äº›æ¨¡å‹ä¸Šå–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08020">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0516b4c33873e7226f7ba21e7a9a7be8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d841936e6144189dedccb2b272cb0468.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4c1925fb4c42776a449cd5d1abe02a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04323d6bf89987526a339c7d59bf3c00.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Hallucination-Stations-On-Some-Basic-Limitations-of-Transformer-Based-Language-Models"><a href="#Hallucination-Stations-On-Some-Basic-Limitations-of-Transformer-Based-Language-Models" class="headerlink" title="Hallucination Stations: On Some Basic Limitations of Transformer-Based   Language Models"></a>Hallucination Stations: On Some Basic Limitations of Transformer-Based   Language Models</h2><p><strong>Authors:Varin Sikka, Vishal Sikka</strong></p>
<p>With widespread adoption of transformer-based language models in AI, there is significant interest in the limits of LLMs capabilities, specifically so-called hallucinations, occurrences in which LLMs provide spurious, factually incorrect or nonsensical information when prompted on certain subjects. Furthermore, there is growing interest in agentic uses of LLMs - that is, using LLMs to create agents that act autonomously or semi-autonomously to carry out various tasks, including tasks with applications in the real world. This makes it important to understand the types of tasks LLMs can and cannot perform. We explore this topic from the perspective of the computational complexity of LLM inference. We show that LLMs are incapable of carrying out computational and agentic tasks beyond a certain complexity, and further that LLMs are incapable of verifying the accuracy of tasks beyond a certain complexity. We present examples of both, then discuss some consequences of this work. </p>
<blockquote>
<p>éšç€åŸºäºå˜å‹å™¨æ¶æ„çš„è¯­è¨€æ¨¡å‹åœ¨äººå·¥æ™ºèƒ½ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›æé™äº§ç”Ÿäº†æµ“åšçš„å…´è¶£ï¼Œç‰¹åˆ«æ˜¯æ‰€è°“çš„â€œå¹»è§‰â€ç°è±¡ã€‚å½“é’ˆå¯¹æŸäº›ä¸»é¢˜æç¤ºæ—¶ï¼ŒLLMä¼šäº§ç”Ÿè™šå‡ã€äº‹å®é”™è¯¯æˆ–æ— æ„ä¹‰çš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œäººä»¬è¿˜è¶Šæ¥è¶Šå…³æ³¨LLMçš„ä»£ç†ä½¿ç”¨ï¼Œå³ä½¿ç”¨LLMåˆ›å»ºèƒ½å¤Ÿè‡ªä¸»æˆ–åŠè‡ªä¸»æ‰§è¡Œå„ç§ä»»åŠ¡çš„ä»£ç†ï¼ŒåŒ…æ‹¬åœ¨ç°å®ä¸–ç•Œä¸­åº”ç”¨çš„ä»»åŠ¡ã€‚è¿™ä½¿æˆ‘ä»¬æœ‰å¿…è¦äº†è§£LLMèƒ½å¤Ÿæ‰§è¡Œå’Œä¸èƒ½æ‰§è¡Œçš„ä»»åŠ¡ç±»å‹ã€‚æˆ‘ä»¬ä»è®¡ç®—å¤æ‚æ€§çš„è§’åº¦æ¢è®¨è¿™ä¸€ä¸»é¢˜ï¼Œå¯¹LLMæ¨ç†è¿›è¡Œæ¢è®¨ã€‚æˆ‘ä»¬è¯æ˜äº†LLMæ— æ³•æ‰§è¡Œè¶…å‡ºä¸€å®šå¤æ‚åº¦çš„è®¡ç®—å’Œä»£ç†ä»»åŠ¡ï¼Œå¹¶ä¸”è¿›ä¸€æ­¥è¯æ˜LLMæ— æ³•éªŒè¯è¶…å‡ºä¸€å®šå¤æ‚åº¦çš„ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬æä¾›äº†è¿™ä¸¤æ–¹é¢çš„ä¾‹å­ï¼Œç„¶åè®¨è®ºè¿™é¡¹å·¥ä½œçš„åæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07505v2">PDF</a> 6 pages; to be submitted to AAAI-26 after reviews</p>
<p><strong>Summary</strong></p>
<p>éšç€åŸºäºTransformerçš„è¯­è¨€æ¨¡å‹åœ¨AIä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›æé™ï¼Œç‰¹åˆ«æ˜¯æ‰€è°“çš„â€œå¹»è§‰â€ç°è±¡ï¼Œäººä»¬äº§ç”Ÿäº†æå¤§çš„å…´è¶£ã€‚æœ¬æ–‡æ¢è®¨äº†LLMçš„è®¡ç®—å¤æ‚æ€§ï¼Œå±•ç¤ºäº†LLMæ— æ³•æ‰§è¡Œè¶…å‡ºä¸€å®šå¤æ‚åº¦çš„è®¡ç®—å’Œä»£ç†ä»»åŠ¡ï¼Œä¹Ÿæ— æ³•éªŒè¯è¶…å‡ºä¸€å®šå¤æ‚åº¦çš„ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså­˜åœ¨èƒ½åŠ›æé™ï¼Œæ— æ³•æ‰§è¡Œè¶…å‡ºä¸€å®šå¤æ‚åº¦çš„è®¡ç®—å’Œä»£ç†ä»»åŠ¡ã€‚</li>
<li>LLMsåœ¨ç‰¹å®šä¸»é¢˜æç¤ºä¸‹å¯èƒ½ä¼šæä¾›é”™è¯¯ã€æ— æ„ä¹‰çš„ä¿¡æ¯ï¼Œå³æ‰€è°“çš„â€œå¹»è§‰â€ç°è±¡ã€‚</li>
<li>LLMsæ— æ³•éªŒè¯è¶…å‡ºä¸€å®šå¤æ‚åº¦çš„ä»»åŠ¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>æœ¬æ–‡æ¢è®¨äº†LLMçš„è®¡ç®—å¤æ‚æ€§ï¼Œå¹¶æä¾›äº†ç›¸å…³çš„å®ä¾‹è¯´æ˜ã€‚</li>
<li>äº†è§£å’ŒæŒæ¡LLMçš„èƒ½åŠ›èŒƒå›´å¯¹äºæœ‰æ•ˆä½¿ç”¨å®ƒä»¬è‡³å…³é‡è¦ã€‚</li>
<li>LLMsåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„ä»£ç†ä½¿ç”¨éœ€è¦è°¨æ…ï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½æ— æ³•è‡ªä¸»æˆ–åŠè‡ªä¸»åœ°å®Œæˆå¤æ‚çš„ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07505">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ed939c004b571d3ae8ab7eea004369d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Open-Source-Planning-Control-System-with-Language-Agents-for-Autonomous-Scientific-Discovery"><a href="#Open-Source-Planning-Control-System-with-Language-Agents-for-Autonomous-Scientific-Discovery" class="headerlink" title="Open Source Planning &amp; Control System with Language Agents for   Autonomous Scientific Discovery"></a>Open Source Planning &amp; Control System with Language Agents for   Autonomous Scientific Discovery</h2><p><strong>Authors:Licong Xu, Milind Sarkar, Anto I. Lonappan, ÃÃ±igo Zubeldia, Pablo Villanueva-Domingo, Santiago Casas, Christian Fidler, Chetana Amancharla, Ujjwal Tiwari, Adrian Bayer, Chadi Ait Ekioui, Miles Cranmer, Adrian Dimitrov, James Fergusson, Kahaan Gandhi, Sven Krippendorf, Andrew Laverick, Julien Lesgourgues, Antony Lewis, Thomas Meier, Blake Sherwin, Kristen Surrao, Francisco Villaescusa-Navarro, Chi Wang, Xueqing Xu, Boris Bolliet</strong></p>
<p>We present a multi-agent system for automation of scientific research tasks, cmbagent (<a target="_blank" rel="noopener" href="https://github.com/CMBAgents/cmbagent">https://github.com/CMBAgents/cmbagent</a>). The system is formed by about 30 Large Language Model (LLM) agents and implements a Planning &amp; Control strategy to orchestrate the agentic workflow, with no human-in-the-loop at any point. Each agent specializes in a different task (performing retrieval on scientific papers and codebases, writing code, interpreting results, critiquing the output of other agents) and the system is able to execute code locally. We successfully apply cmbagent to carry out a PhD level cosmology task (the measurement of cosmological parameters using supernova data) and evaluate its performance on two benchmark sets, finding superior performance over state-of-the-art LLMs. The source code is available on GitHub, demonstration videos are also available, and the system is deployed on HuggingFace and will be available on the cloud. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºä¸€ä¸ªç”¨äºè‡ªåŠ¨åŒ–ç§‘ç ”ä»»åŠ¡çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œåä¸ºcmbagentï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/CMBAgents/cmbagent%EF%BC%89%E3%80%82%E8%AF%A5%E7%B3%BB%E7%BB%9F%E7%94%B1%E5%A4%A7%E7%BA%A630%E4%B8%AA%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E6%99%BA%E8%83%BD%E4%BD%93%E7%BB%84%E6%88%90%EF%BC%8C%E9%87%87%E7%94%A8%E8%A7%84%E5%88%92%E4%B8%8E%E6%8E%A7%E5%88%B6%E7%AD%96%E7%95%A5%E6%9D%A5%E5%8D%8F%E8%B0%83%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%EF%BC%8C%E4%B8%94%E5%9C%A8%E4%BB%BB%E4%BD%95%E6%97%B6%E5%88%BB%E9%83%BD%E4%B8%8D%E9%9C%80%E8%A6%81%E4%BA%BA%E5%B7%A5%E4%BB%8B%E5%85%A5%E3%80%82%E6%AF%8F%E4%B8%AA%E6%99%BA%E8%83%BD%E4%BD%93%E9%83%BD%E4%B8%93%E6%B3%A8%E4%BA%8E%E4%B8%8D%E5%90%8C%E7%9A%84%E4%BB%BB%E5%8A%A1%EF%BC%88%E5%A6%82%E6%89%A7%E8%A1%8C%E7%A7%91%E5%AD%A6%E8%AE%BA%E6%96%87%E5%92%8C%E4%BB%A3%E7%A0%81%E5%BA%93%E7%9A%84%E6%A3%80%E7%B4%A2%E3%80%81%E7%BC%96%E5%86%99%E4%BB%A3%E7%A0%81%E3%80%81%E8%A7%A3%E8%AF%BB%E7%BB%93%E6%9E%9C%E3%80%81%E8%AF%84%E4%BB%B7%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E8%BE%93%E5%87%BA%EF%BC%89%EF%BC%8C%E5%B9%B6%E4%B8%94%E7%B3%BB%E7%BB%9F%E8%83%BD%E5%A4%9F%E5%9C%A8%E6%9C%AC%E5%9C%B0%E6%89%A7%E8%A1%8C%E4%BB%A3%E7%A0%81%E3%80%82%E6%88%91%E4%BB%AC%E6%88%90%E5%8A%9F%E5%9C%B0%E5%B0%86cmbagent%E5%BA%94%E7%94%A8%E4%BA%8E%E6%89%A7%E8%A1%8C%E5%8D%9A%E5%A3%AB%E7%BA%A7%E5%88%AB%E7%9A%84%E5%AE%87%E5%AE%99%E5%AD%A6%E4%BB%BB%E5%8A%A1%EF%BC%88%E4%BD%BF%E7%94%A8%E8%B6%85%E6%96%B0%E6%98%9F%E6%95%B0%E6%8D%AE%E6%B5%8B%E9%87%8F%E5%AE%87%E5%AE%99%E5%AD%A6%E5%8F%82%E6%95%B0%EF%BC%89%EF%BC%8C%E5%B9%B6%E5%9C%A8%E4%B8%A4%E4%B8%AA%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E9%9B%86%E4%B8%8A%E8%AF%84%E4%BC%B0%E5%85%B6%E6%80%A7%E8%83%BD%EF%BC%8C%E5%8F%91%E7%8E%B0%E5%85%B6%E6%80%A7%E8%83%BD%E4%BC%98%E4%BA%8E%E7%8E%B0%E6%9C%89%E7%9A%84%E6%9C%80%E5%85%88%E8%BF%9B%E7%9A%84%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E3%80%82%E6%BA%90%E4%BB%A3%E7%A0%81%E5%B7%B2%E5%9C%A8GitHub%E4%B8%8A%E6%8F%90%E4%BE%9B%EF%BC%8C%E6%BC%94%E7%A4%BA%E8%A7%86%E9%A2%91%E4%B9%9F%E5%B7%B2%E5%8F%91%E5%B8%83%EF%BC%8C%E8%AF%A5%E7%B3%BB%E7%BB%9F%E5%B7%B2%E9%83%A8%E7%BD%B2%E5%9C%A8HuggingFace%E4%B8%8A%EF%BC%8C%E5%B9%B6%E5%B0%86%E5%8F%AF%E5%9C%A8%E4%BA%91%E7%AB%AF%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/CMBAgents/cmbagentï¼‰ã€‚è¯¥ç³»ç»Ÿç”±å¤§çº¦30ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™ºèƒ½ä½“ç»„æˆï¼Œé‡‡ç”¨è§„åˆ’ä¸æ§åˆ¶ç­–ç•¥æ¥åè°ƒæ™ºèƒ½ä½“çš„å·¥ä½œæµç¨‹ï¼Œä¸”åœ¨ä»»ä½•æ—¶åˆ»éƒ½ä¸éœ€è¦äººå·¥ä»‹å…¥ã€‚æ¯ä¸ªæ™ºèƒ½ä½“éƒ½ä¸“æ³¨äºä¸åŒçš„ä»»åŠ¡ï¼ˆå¦‚æ‰§è¡Œç§‘å­¦è®ºæ–‡å’Œä»£ç åº“çš„æ£€ç´¢ã€ç¼–å†™ä»£ç ã€è§£è¯»ç»“æœã€è¯„ä»·å…¶ä»–æ™ºèƒ½ä½“çš„è¾“å‡ºï¼‰ï¼Œå¹¶ä¸”ç³»ç»Ÿèƒ½å¤Ÿåœ¨æœ¬åœ°æ‰§è¡Œä»£ç ã€‚æˆ‘ä»¬æˆåŠŸåœ°å°†cmbagentåº”ç”¨äºæ‰§è¡Œåšå£«çº§åˆ«çš„å®‡å®™å­¦ä»»åŠ¡ï¼ˆä½¿ç”¨è¶…æ–°æ˜Ÿæ•°æ®æµ‹é‡å®‡å®™å­¦å‚æ•°ï¼‰ï¼Œå¹¶åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šè¯„ä¼°å…¶æ€§èƒ½ï¼Œå‘ç°å…¶æ€§èƒ½ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æºä»£ç å·²åœ¨GitHubä¸Šæä¾›ï¼Œæ¼”ç¤ºè§†é¢‘ä¹Ÿå·²å‘å¸ƒï¼Œè¯¥ç³»ç»Ÿå·²éƒ¨ç½²åœ¨HuggingFaceä¸Šï¼Œå¹¶å°†å¯åœ¨äº‘ç«¯ä½¿ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07257v2">PDF</a> Accepted contribution to the ICML 2025 Workshop on Machine Learning   for Astrophysics. Code: <a target="_blank" rel="noopener" href="https://github.com/CMBAgents/cmbagent">https://github.com/CMBAgents/cmbagent</a> Videos:   <a target="_blank" rel="noopener" href="https://www.youtube.com/@cmbagent">https://www.youtube.com/@cmbagent</a> HuggingFace:   <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/astropilot-ai/cmbagent">https://huggingface.co/spaces/astropilot-ai/cmbagent</a> Cloud:   <a target="_blank" rel="noopener" href="https://cmbagent.cloud/">https://cmbagent.cloud</a></p>
<p><strong>Summary</strong></p>
<p>ä¸€ä¸ªé›†æˆäº†çº¦30ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè¢«å¼€å‘å‡ºæ¥ï¼Œç”¨äºè‡ªåŠ¨åŒ–ç§‘ç ”ä»»åŠ¡ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨è§„åˆ’å’Œæ§åˆ¶ç­–ç•¥æ¥åè°ƒæ™ºèƒ½ä½“çš„å·¥ä½œæµç¨‹ï¼Œæ•´ä¸ªè¿‡ç¨‹ä¸­æ— éœ€äººå·¥å‚ä¸ã€‚æ¯ä¸ªæ™ºèƒ½ä½“éƒ½èƒ½å®Œæˆä¸åŒçš„ä»»åŠ¡ï¼Œå¦‚æ£€ç´¢ç§‘å­¦è®ºæ–‡å’Œä»£ç åº“ã€ç¼–å†™ä»£ç ã€è§£è¯»ç»“æœã€è¯„ä¼°å…¶ä»–æ™ºèƒ½ä½“çš„è¾“å‡ºç­‰ã€‚è¯¥ç³»ç»Ÿå·²æˆåŠŸåº”ç”¨äºä¸€é¡¹åšå£«çº§åˆ«çš„å®‡å®™å­¦ä»»åŠ¡ï¼ˆä½¿ç”¨è¶…æ–°æ˜Ÿæ•°æ®æµ‹é‡å®‡å®™å­¦å‚æ•°ï¼‰ï¼Œå¹¶åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šè¡¨ç°å‡ºè¶…è¶Šå½“å‰å…ˆè¿›LLMçš„æ€§èƒ½ã€‚æºä»£ç å·²åœ¨GitHubä¸Šæä¾›ï¼Œè¿˜æœ‰æ¼”ç¤ºè§†é¢‘ï¼Œè¯¥ç³»ç»Ÿå·²éƒ¨ç½²åœ¨HuggingFaceä¸Šï¼Œå¹¶å°†å¯åœ¨äº‘ç«¯ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç”¨äºè‡ªåŠ¨åŒ–ç§‘ç ”ä»»åŠ¡ã€‚</li>
<li>ç³»ç»Ÿé›†æˆäº†çº¦30ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>é‡‡ç”¨è§„åˆ’å’Œæ§åˆ¶ç­–ç•¥åè°ƒæ™ºèƒ½ä½“çš„å·¥ä½œæµç¨‹ï¼Œæ— éœ€äººå·¥å‚ä¸ã€‚</li>
<li>æ™ºèƒ½ä½“å¯å®Œæˆä¸åŒä»»åŠ¡ï¼Œå¦‚æ£€ç´¢ã€ç¼–å†™ä»£ç ã€è§£è¯»ç»“æœã€è¯„ä¼°è¾“å‡ºç­‰ã€‚</li>
<li>æˆåŠŸåº”ç”¨äºåšå£«çº§åˆ«çš„å®‡å®™å­¦ä»»åŠ¡ï¼Œæµ‹é‡å®‡å®™å­¦å‚æ•°ã€‚</li>
<li>åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šè¡¨ç°å‡ºè¶…è¶Šå½“å‰å…ˆè¿›LLMçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07257">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ee18fe38d8501ef5bb0b058306af4a3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f2f31b594fbad129469e80447815f92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bb365510d5fd9e59e822406ce69b5d8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Mind-the-Memory-Gap-Unveiling-GPU-Bottlenecks-in-Large-Batch-LLM-Inference"><a href="#Mind-the-Memory-Gap-Unveiling-GPU-Bottlenecks-in-Large-Batch-LLM-Inference" class="headerlink" title="Mind the Memory Gap: Unveiling GPU Bottlenecks in Large-Batch LLM   Inference"></a>Mind the Memory Gap: Unveiling GPU Bottlenecks in Large-Batch LLM   Inference</h2><p><strong>Authors:Pol G. Recasens, Ferran Agullo, Yue Zhu, Chen Wang, Eun Kyung Lee, Olivier Tardieu, Jordi Torres, Josep Ll. Berral</strong></p>
<p>Large language models have been widely adopted across different tasks, but their auto-regressive generation nature often leads to inefficient resource utilization during inference. While batching is commonly used to increase throughput, performance gains plateau beyond a certain batch size, especially with smaller models, a phenomenon that existing literature typically explains as a shift to the compute-bound regime. In this paper, through an in-depth GPU-level analysis, we reveal that large-batch inference remains memory-bound, with most GPU compute capabilities underutilized due to DRAM bandwidth saturation as the primary bottleneck. To address this, we propose a Batching Configuration Advisor (BCA) that optimizes memory allocation, reducing GPU memory requirements with minimal impact on throughput. The freed memory and underutilized GPU compute capabilities can then be leveraged by concurrent workloads. Specifically, we use model replication to improve serving throughput and GPU utilization. Our findings challenge conventional assumptions about LLM inference, offering new insights and practical strategies for improving resource utilization, particularly for smaller language models. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/FerranAgulloLopez/vLLMBatchingMemoryGap">https://github.com/FerranAgulloLopez/vLLMBatchingMemoryGap</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å·²è¢«å¹¿æ³›åº”ç”¨äºå„ç§ä»»åŠ¡ï¼Œä½†å…¶è‡ªå›å½’ç”Ÿæˆç‰¹æ€§å¾€å¾€å¯¼è‡´æ¨ç†è¿‡ç¨‹ä¸­çš„èµ„æºåˆ©ç”¨ç‡ä½ä¸‹ã€‚è™½ç„¶æ‰¹å¤„ç†é€šå¸¸ç”¨äºæé«˜ååé‡ï¼Œä½†åœ¨è¾¾åˆ°ä¸€å®šæ‰¹é‡å¤§å°åï¼Œæ€§èƒ½æå‡ä¼šé‡åˆ°ç“¶é¢ˆï¼Œå°¤å…¶æ˜¯å¯¹äºè¾ƒå°çš„æ¨¡å‹ï¼Œç°æœ‰æ–‡çŒ®é€šå¸¸å°†æ­¤ç°è±¡è§£é‡Šä¸ºè½¬å‘è®¡ç®—å—é™çŠ¶æ€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ·±å…¥çš„GPUçº§åˆ«åˆ†ææ­ç¤ºï¼Œå¤§æ‰¹é‡æ¨ç†ä»ç„¶æ˜¯å†…å­˜å—é™çš„ï¼Œç”±äºDRAMå¸¦å®½é¥±å’Œæ˜¯ä¸»è¦ç“¶é¢ˆï¼Œå¯¼è‡´å¤§å¤šæ•°GPUè®¡ç®—èƒ½åŠ›æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ‰¹å¤„ç†é…ç½®é¡¾é—®ï¼ˆBCAï¼‰ï¼Œé€šè¿‡ä¼˜åŒ–å†…å­˜åˆ†é…ï¼Œä»¥æœ€å°é™åº¦åœ°å½±å“ååé‡çš„æ–¹å¼é™ä½GPUå†…å­˜è¦æ±‚ã€‚é‡Šæ”¾çš„å†…å­˜å’Œæœªå……åˆ†åˆ©ç”¨çš„GPUè®¡ç®—èƒ½åŠ›å¯ä»¥è¢«å¹¶å‘å·¥ä½œè´Ÿè½½æ‰€åˆ©ç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡å‹å¤åˆ¶æ¥æé«˜æœåŠ¡ååé‡å’ŒGPUåˆ©ç”¨ç‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶æŒ‘æˆ˜äº†å…³äºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†çš„å¸¸è§„å‡è®¾ï¼Œä¸ºæ”¹è¿›èµ„æºåˆ©ç”¨æä¾›äº†æ–°è§è§£å’Œå®é™…ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè¾ƒå°çš„è¯­è¨€æ¨¡å‹ã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/FerranAgulloLopez/vLLMBatchingMemoryGap%E3%80%82">https://github.com/FerranAgulloLopez/vLLMBatchingMemoryGapã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08311v2">PDF</a> Pol G. Recasens, Ferran Agullo: equal contribution. Paper accepted at   IEEE CLOUD 2025</p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹å¹¿æ³›åº”ç”¨äºå„ç§ä»»åŠ¡ï¼Œä½†å…¶è‡ªå›å½’ç”Ÿæˆç‰¹æ€§å¯¼è‡´æ¨ç†è¿‡ç¨‹ä¸­çš„èµ„æºåˆ©ç”¨æ•ˆç‡ä½ä¸‹ã€‚æ‰¹å¤„ç†é€šå¸¸ç”¨äºæé«˜ååé‡ï¼Œä½†è¶…è¿‡ä¸€å®šæ‰¹æ¬¡å¤§å°åæ€§èƒ½æå‡å¹³å°åŒ–ï¼Œç‰¹åˆ«æ˜¯å¯¹å°æ¨¡å‹è€Œè¨€ã€‚æœ¬æ–‡æ·±å…¥GPUå±‚é¢åˆ†æï¼Œæ­ç¤ºå¤§æ‰¹é‡æ¨ç†ä»å­˜åœ¨å†…å­˜ç“¶é¢ˆé—®é¢˜ï¼ŒGPUè®¡ç®—èƒ½åŠ›å¤§éƒ¨åˆ†æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ï¼Œä¸»è¦æ˜¯ç”±äºDRAMå¸¦å®½é¥±å’Œæ‰€è‡´ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºæ‰¹å¤„ç†é…ç½®é¡¾é—®ï¼ˆBCAï¼‰ï¼Œä¼˜åŒ–å†…å­˜åˆ†é…ï¼Œä»¥å‡å°å¯¹ååé‡çš„å½±å“ã€‚é‡Šæ”¾çš„å†…å­˜å’Œæœªå……åˆ†åˆ©ç”¨çš„GPUè®¡ç®—èƒ½åŠ›å¯ç”¨äºå¹¶å‘å·¥ä½œè´Ÿè½½ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡æ¨¡å‹å¤åˆ¶æé«˜æœåŠ¡ååé‡å’ŒGPUåˆ©ç”¨ç‡ã€‚æœ¬æ–‡æŒ‘æˆ˜äº†å…³äºLLMæ¨ç†çš„å¸¸è§„å‡è®¾ï¼Œä¸ºæ”¹è¿›èµ„æºåˆ©ç”¨æä¾›äº†æ–°è§è§£å’Œå®ç”¨ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯å¯¹å°è¯­è¨€æ¨¡å‹è€Œè¨€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹çš„è‡ªå›å½’ç”Ÿæˆç‰¹æ€§å¯¼è‡´æ¨ç†è¿‡ç¨‹ä¸­çš„èµ„æºåˆ©ç”¨æ•ˆç‡ä½ä¸‹ï¼Œç‰¹åˆ«æ˜¯å¯¹å°æ¨¡å‹è€Œè¨€ï¼Œæ€§èƒ½æå‡åœ¨è¶…è¿‡ä¸€å®šæ‰¹æ¬¡å¤§å°åè¶‹äºå¹³ç¨³ã€‚</li>
<li>é€šè¿‡æ·±å…¥GPUå±‚é¢çš„åˆ†æï¼Œå‘ç°å¤§æ‰¹é‡æ¨ç†å­˜åœ¨å†…å­˜ç“¶é¢ˆé—®é¢˜ï¼Œå¤§éƒ¨åˆ†GPUè®¡ç®—èƒ½åŠ›æœªè¢«å……åˆ†åˆ©ç”¨ï¼Œä¸»è¦åŸå› æ˜¯DRAMå¸¦å®½é¥±å’Œã€‚</li>
<li>æå‡ºæ‰¹å¤„ç†é…ç½®é¡¾é—®ï¼ˆBCAï¼‰ä»¥ä¼˜åŒ–å†…å­˜åˆ†é…ï¼Œå‡å°å¯¹ååé‡çš„å½±å“ï¼Œå¹¶é€šè¿‡é‡Šæ”¾çš„å†…å­˜å’Œæœªå……åˆ†åˆ©ç”¨çš„GPUè®¡ç®—èƒ½åŠ›æ¥æé«˜å¹¶å‘å·¥ä½œè´Ÿè½½çš„æ•ˆç‡ã€‚</li>
<li>é€šè¿‡æ¨¡å‹å¤åˆ¶æ¥æé«˜æœåŠ¡ååé‡å’ŒGPUåˆ©ç”¨ç‡æ˜¯ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥ã€‚</li>
<li>æœ¬æ–‡æŒ‘æˆ˜äº†å…³äºLLMæ¨ç†çš„å¸¸è§„å‡è®¾ï¼Œä¸ºæ”¹è¿›èµ„æºåˆ©ç”¨æä¾›äº†æ–°çš„è§è§£å’Œå®ç”¨ç­–ç•¥ã€‚</li>
<li>å…¬å¼€å¯ç”¨çš„ä»£ç ä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›äº†å®ç°ä¸Šè¿°ç­–ç•¥çš„å·¥å…·å’Œå‚è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08311">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-741688a12360f89f9163800f4eed2660.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9aeb367cd9a953c47b4c57a47ef981c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aed7fb6d9674f034232fa89ba56b458c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ef5bbfc53ab7807864eb41bb9590ebe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81ef23aa771d99e8bbd22731b4e2ab40.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a841f41ef1031810e12150ed1fea05b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2569c29575533dc6eac667f204afaf6.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Weak-to-Strong-Jailbreaking-on-Large-Language-Models"><a href="#Weak-to-Strong-Jailbreaking-on-Large-Language-Models" class="headerlink" title="Weak-to-Strong Jailbreaking on Large Language Models"></a>Weak-to-Strong Jailbreaking on Large Language Models</h2><p><strong>Authors:Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, William Yang Wang</strong></p>
<p>Large language models (LLMs) are vulnerable to jailbreak attacks - resulting in harmful, unethical, or biased text generations. However, existing jailbreaking methods are computationally costly. In this paper, we propose the weak-to-strong jailbreaking attack, an efficient inference time attack for aligned LLMs to produce harmful text. Our key intuition is based on the observation that jailbroken and aligned models only differ in their initial decoding distributions. The weak-to-strong attackâ€™s key technical insight is using two smaller models (a safe and an unsafe one) to adversarially modify a significantly larger safe modelâ€™s decoding probabilities. We evaluate the weak-to-strong attack on 5 diverse open-source LLMs from 3 organizations. The results show our method can increase the misalignment rate to over 99% on two datasets with just one forward pass per example. Our study exposes an urgent safety issue that needs to be addressed when aligning LLMs. As an initial attempt, we propose a defense strategy to protect against such attacks, but creating more advanced defenses remains challenging. The code for replicating the method is available at <a target="_blank" rel="noopener" href="https://github.com/XuandongZhao/weak-to-strong">https://github.com/XuandongZhao/weak-to-strong</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®¹æ˜“å—åˆ°è¶Šç‹±æ”»å‡»ï¼Œå¯¼è‡´ç”Ÿæˆæœ‰å®³ã€ä¸é“å¾·æˆ–åè§çš„æ–‡æœ¬ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¶Šç‹±æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¼±åˆ°å¼ºè¶Šç‹±æ”»å‡»ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„æ¨ç†æ—¶é—´æ”»å‡»ï¼Œç”¨äºé’ˆå¯¹å¯¹é½çš„å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆæœ‰å®³æ–‡æœ¬ã€‚æˆ‘ä»¬çš„ä¸»è¦ç›´è§‰æ˜¯åŸºäºè¿™æ ·çš„è§‚å¯Ÿï¼šè¶Šç‹±å’Œå¯¹é½çš„æ¨¡å‹åªåœ¨åˆå§‹è§£ç åˆ†å¸ƒä¸Šæœ‰æ‰€ä¸åŒã€‚å¼±åˆ°å¼ºæ”»å‡»çš„å…³é”®æŠ€æœ¯è§è§£æ˜¯ä½¿ç”¨ä¸¤ä¸ªè¾ƒå°çš„æ¨¡å‹ï¼ˆä¸€ä¸ªå®‰å…¨æ¨¡å‹å’Œä¸€ä¸ªä¸å®‰å…¨æ¨¡å‹ï¼‰æ¥å¯¹æŠ—æ€§åœ°ä¿®æ”¹ä¸€ä¸ªæ›´å¤§çš„å®‰å…¨æ¨¡å‹çš„è§£ç æ¦‚ç‡ã€‚æˆ‘ä»¬åœ¨æ¥è‡ªä¸‰ä¸ªç»„ç»‡çš„äº”ä¸ªå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¯„ä¼°äº†å¼±åˆ°å¼ºæ”»å‡»ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šå°†é”™ä½ç‡æé«˜åˆ°è¶…è¿‡99%ï¼Œæ¯ä¸ªä¾‹å­åªéœ€è¿›è¡Œä¸€æ¬¡æ­£å‘ä¼ é€’ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†å½“å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œéœ€è¦è§£å†³ä¸€ä¸ªç´§æ€¥çš„å®‰å…¨é—®é¢˜ã€‚ä½œä¸ºä¸€ç§åˆæ­¥å°è¯•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é˜²å¾¡ç­–ç•¥æ¥é˜²æ­¢æ­¤ç±»æ”»å‡»ï¼Œä½†åˆ›å»ºæ›´å…ˆè¿›çš„é˜²å¾¡æ‰‹æ®µä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å¤åˆ¶è¯¥æ–¹æ³•çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/XuandongZhao/weak-to-strong">https://github.com/XuandongZhao/weak-to-strong</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.17256v4">PDF</a> ICML 2025</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜“å—â€œè¶Šç‹±æ”»å‡»â€ï¼Œäº§ç”Ÿæœ‰å®³ã€ä¸é“å¾·æˆ–åå‘çš„æ–‡æœ¬ç”Ÿæˆã€‚ç°æœ‰è¶Šç‹±æ–¹æ³•è®¡ç®—æˆæœ¬é«˜ã€‚æœ¬æ–‡æå‡ºå¼±åˆ°å¼ºè¶Šç‹±æ”»å‡»ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„æ¨ç†æ—¶é—´æ”»å‡»ï¼Œç”¨äºé’ˆå¯¹å¯¹é½çš„å¤§å‹è¯­è¨€æ¨¡å‹äº§ç”Ÿæœ‰å®³æ–‡æœ¬ã€‚æˆ‘ä»¬çš„å…³é”®ç›´è§‰æ˜¯åŸºäºè§‚å¯Ÿï¼Œå³è¶Šç‹±å’Œå¯¹é½æ¨¡å‹ä¹‹é—´çš„åŒºåˆ«ä»…åœ¨äºå…¶åˆå§‹è§£ç åˆ†å¸ƒã€‚å¼±åˆ°å¼ºæ”»å‡»çš„å…³é”®æŠ€æœ¯è§è§£æ˜¯ä½¿ç”¨ä¸¤ä¸ªè¾ƒå°çš„æ¨¡å‹ï¼ˆä¸€ä¸ªå®‰å…¨æ¨¡å‹å’Œä¸€ä¸ªä¸å®‰å…¨æ¨¡å‹ï¼‰æ¥å¯¹æŠ—æ€§åœ°ä¿®æ”¹ä¸€ä¸ªæ›´å¤§çš„å®‰å…¨æ¨¡å‹çš„è§£ç æ¦‚ç‡ã€‚æˆ‘ä»¬åœ¨äº”ä¸ªå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¯„ä¼°äº†å¼±åˆ°å¼ºæ”»å‡»ï¼Œæ¥è‡ªä¸‰ä¸ªç»„ç»‡ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šå°†é”™ä½ç‡æé«˜åˆ°è¶…è¿‡99%ï¼Œæ¯ä¸ªç¤ºä¾‹åªéœ€è¿›è¡Œä¸€æ¬¡å‰å‘ä¼ é€’ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†ä¸€ä¸ªç´§æ€¥çš„å®‰å…¨é—®é¢˜ï¼Œéœ€è¦åœ¨å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹æ—¶è§£å†³ã€‚ä½œä¸ºåˆæ­¥å°è¯•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é˜²å¾¡ç­–ç•¥æ¥é˜²æ­¢æ­¤ç±»æ”»å‡»ï¼Œä½†åˆ›å»ºæ›´å…ˆè¿›çš„é˜²å¾¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å¯å¤åˆ¶è¯¥æ–¹æ³•çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/XuandongZhao/weak-to-strong">https://github.com/XuandongZhao/weak-to-strong</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsæ˜“å—åä¸ºâ€œè¶Šç‹±æ”»å‡»â€çš„å®‰å…¨å¨èƒï¼Œå¯å¯¼è‡´ç”Ÿæˆæœ‰å®³ã€ä¸é“å¾·æˆ–åå‘æ–‡æœ¬ã€‚</li>
<li>ç°å­˜çš„è¶Šç‹±è®¡ç®—æ–¹æ³•æ™®éè®¡ç®—æˆæœ¬é«˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼±åˆ°å¼ºè¶Šç‹±æ”»å‡»æ–¹æ³•ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæœ‰å®³æ–‡æœ¬ç”Ÿæˆã€‚</li>
<li>å¼±åˆ°å¼ºæ”»å‡»çš„æ ¸å¿ƒåœ¨äºåˆ©ç”¨ä¸¤ä¸ªè¾ƒå°æ¨¡å‹æ¥ä¿®æ”¹æ›´å¤§æ¨¡å‹çš„è§£ç æ¦‚ç‡ã€‚</li>
<li>åœ¨äº”ä¸ªä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æé«˜é”™ä½ç‡è‡³99%ä»¥ä¸Šã€‚</li>
<li>ç ”ç©¶å‡¸æ˜¾äº†å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹æ—¶çš„ç´§æ€¥å®‰å…¨é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.17256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d2a3750d1c31bd154d29ef184146d782.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f338bbab6820a6802371d02328f413c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9506ab26adfc6608d9078259eb5ab3fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55ba39c18bc457bb3f75da136820f581.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-15/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-15/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-15/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-da22ec7125b2e2c1673700753687782e.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-15  Introspection of Thought Helps AI Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-15/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1c388731392af5f3642be6b0ee85e3b6.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-15  One Token to Fool LLM-as-a-Judge
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
