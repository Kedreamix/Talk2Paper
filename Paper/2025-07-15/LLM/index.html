<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-07-15  Lumos-1 On Autoregressive Video Generation from a Unified Model   Perspective">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-0516b4c33873e7226f7ba21e7a9a7be8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    17.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    71 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-15-更新"><a href="#2025-07-15-更新" class="headerlink" title="2025-07-15 更新"></a>2025-07-15 更新</h1><h2 id="Lumos-1-On-Autoregressive-Video-Generation-from-a-Unified-Model-Perspective"><a href="#Lumos-1-On-Autoregressive-Video-Generation-from-a-Unified-Model-Perspective" class="headerlink" title="Lumos-1: On Autoregressive Video Generation from a Unified Model   Perspective"></a>Lumos-1: On Autoregressive Video Generation from a Unified Model   Perspective</h2><p><strong>Authors:Hangjie Yuan, Weihua Chen, Jun Cen, Hu Yu, Jingyun Liang, Shuning Chang, Zhihui Lin, Tao Feng, Pengwei Liu, Jiazheng Xing, Hao Luo, Jiasheng Tang, Fan Wang, Yi Yang</strong></p>
<p>Autoregressive large language models (LLMs) have unified a vast range of language tasks, inspiring preliminary efforts in autoregressive video generation. Existing autoregressive video generators either diverge from standard LLM architectures, depend on bulky external text encoders, or incur prohibitive latency due to next-token decoding. In this paper, we introduce Lumos-1, an autoregressive video generator that retains the LLM architecture with minimal architectural modifications. To inject spatiotemporal correlations in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE scheme that preserves the original textual RoPE while providing comprehensive frequency spectra and scaled 3D positions for modeling multimodal spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy that obeys intra-frame bidirectionality and inter-frame temporal causality. Based on this dependency strategy, we identify the issue of frame-wise loss imbalance caused by spatial information redundancy and solve it by proposing Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal tube masking during training with a compatible inference-time masking policy to avoid quality degradation. By using memory-efficient training techniques, we pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code and models are available at <a target="_blank" rel="noopener" href="https://github.com/alibaba-damo-academy/Lumos">https://github.com/alibaba-damo-academy/Lumos</a>. </p>
<blockquote>
<p>自回归大型语言模型（LLM）已经统一了多种语言任务，为自回归视频生成提供了初步的思路。现有的自回归视频生成器要么与标准LLM架构不同，依赖于庞大的外部文本编码器，要么由于下一个令牌解码而产生过高的延迟。在本文中，我们介绍了Lumos-1，这是一款保留LLM架构并进行了最小修改的自回归视频生成器。为了将时空相关性注入LLM中，我们确定了融入3D RoPE的有效性，并诊断了其不平衡的频率谱范围。因此，我们提出了MM-RoPE，这是一种RoPE方案，既保留了原始的文本RoPE，又提供了全面的频率谱和缩放的三维位置，用于模拟多模态时空数据。此外，Lumos-1采用了一种遵循帧内双向性和帧间时间因果关系的令牌依赖策略。基于这种依赖策略，我们发现了由于空间信息冗余导致的帧级损失不平衡问题，并提出了自回归离散扩散强制（AR-DF）来解决这个问题。AR-DF在训练过程中引入了时间管掩码，并使用兼容的推理时间掩码策略，以避免质量下降。通过使用高效的内存训练技术，我们只用了48个GPU对Lumos-1进行了预训练，在GenEval上的性能与EMU3相当，在VBench-I2V上的COSMOS-Video2World和VBench-T2V上的OpenSoraPlan也表现出色。代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/alibaba-damo-academy/Lumos%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/alibaba-damo-academy/Lumos上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08801v1">PDF</a> Code and Models: <a target="_blank" rel="noopener" href="https://github.com/alibaba-damo-academy/Lumos">https://github.com/alibaba-damo-academy/Lumos</a></p>
<p><strong>Summary</strong></p>
<p>该文介绍了Lumos-1这种基于LLM架构的自动视频生成器。为注入时空相关性，论文识别了3D RoPE的有效性并诊断了其不平衡的频率谱范围问题，于是提出了MM-RoPE方案。同时，Lumos-1采取遵循帧内双向性和帧间时序因果关系的令牌依赖策略。针对由空间信息冗余引起的帧损失不平衡问题，论文提出了基于自动回归离散扩散强迫（AR-DF）的解决方案。通过采用高效的训练方法，Lumos-1在有限的GPU资源上进行了预训练，在GenEval的EMU3上实现了比较性能，并在VBench上的COSMOS-Video2World和OpenSoraPlan上进行了验证。代码和模型可在GitHub上找到。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Lumos-1是首个基于LLM架构的自动视频生成器，具有统一多种语言任务的能力。</li>
<li>引入MM-RoPE方案以改进原有的RoPE设计，考虑了更全面的频率谱和三维位置建模问题。</li>
<li>采用特定的令牌依赖策略来保持帧内双向性和帧间时序因果性。</li>
<li>提出AR-DF来解决因空间信息冗余导致的帧损失不平衡问题。</li>
<li>Lumos-1通过内存高效训练技术在有限的GPU资源上实现了良好的性能表现。</li>
<li>Lumos-1在多个基准测试中实现了与现有技术相当的性能表现，包括GenEval的EMU3、VBench上的COSMOS-Video2World和OpenSoraPlan等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08801">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-aaf372f1b3f23d2c63aa54dfa3203cd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9fa3d887b84c19f081e129f0f7ea207.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-71e1d6eaecccf418df27a239ea16910d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7dd6b2c04b74ca691dd7085ebcfd00d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e42829556126edcabded3543c4c227fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a33e557e77bc462338e63a5b62d2a95e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ec211097436c3b5bee6c33903800d80.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="One-Token-to-Fool-LLM-as-a-Judge"><a href="#One-Token-to-Fool-LLM-as-a-Judge" class="headerlink" title="One Token to Fool LLM-as-a-Judge"></a>One Token to Fool LLM-as-a-Judge</h2><p><strong>Authors:Yulai Zhao, Haolin Liu, Dian Yu, S. Y. Kung, Haitao Mi, Dong Yu</strong></p>
<p>Generative reward models (also known as LLMs-as-judges), which use large language models (LLMs) to evaluate answer quality, are increasingly adopted in reinforcement learning with verifiable rewards (RLVR). They are often preferred over rigid rule-based metrics, especially for complex reasoning tasks involving free-form outputs. In this paradigm, an LLM is typically prompted to compare a candidate answer against a ground-truth reference and assign a binary reward indicating correctness. Despite the seeming simplicity of this comparison task, we find that generative reward models exhibit surprising vulnerabilities to superficial manipulations: non-word symbols (e.g., “:” or “.”) or reasoning openers like “Thought process:” and “Let’s solve this problem step by step.” can often lead to false positive rewards. We demonstrate that this weakness is widespread across LLMs, datasets, and prompt formats, posing a serious threat for core algorithmic paradigms that rely on generative reward models, such as rejection sampling, preference optimization, and RLVR. To mitigate this issue, we introduce a simple yet effective data augmentation strategy and train a new generative reward model with substantially improved robustness. Our findings highlight the urgent need for more reliable LLM-based evaluation methods. We release our robust, general-domain reward model and its synthetic training data at <a target="_blank" rel="noopener" href="https://huggingface.co/sarosavo/Master-RM">https://huggingface.co/sarosavo/Master-RM</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/sarosavo/Master-RM">https://huggingface.co/datasets/sarosavo/Master-RM</a>. </p>
<blockquote>
<p>生成奖励模型（也称为LLM作为评判者），使用大型语言模型（LLM）来评估答案质量，在可验证奖励的强化学习（RLVR）中越来越被采用。它们通常被优先选择用于涉及自由形式输出的复杂推理任务，而不是僵化的基于规则的度量标准。在这种模式下，通常会提示LLM将候选答案与真实答案进行比较，并分配一个二进制奖励来表示正确性。尽管这种比较任务看似简单，但我们发现生成奖励模型对表面操纵表现出令人惊讶的脆弱性：非单词符号（例如：“：”或“。”）或推理开场词如“思考过程：”和“让我们一步一步解决这个问题。”可能会导致错误的正面奖励。我们证明这种弱点在LLM、数据集和提示格式中是普遍存在的，对依赖生成奖励模型的核心算法范式构成严重威胁，如拒绝抽样、偏好优化和RLVR。为了缓解这个问题，我们引入了一种简单有效的数据增强策略，并训练了一个新的生成奖励模型，该模型具有更强的稳健性。我们的研究结果表明，更可靠的基于LLM的评估方法迫切需要。我们在<a target="_blank" rel="noopener" href="https://huggingface.co/sarosavo/Master-RM">https://huggingface.co/sarosavo/Master-RM</a>和<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/sarosavo/Master-RM">https://huggingface.co/datasets/sarosavo/Master-RM</a>发布了我们稳健的通用奖励模型和合成训练数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08794v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）作为奖励模型在强化学习可验证奖励（RLVR）中越来越受欢迎，用于评估答案质量。然而，这种模型在处理复杂推理任务时存在漏洞，容易被表面操纵所影响，导致错误奖励。研究团队提出了一种新的增强奖励模型，并发布了相关资源。这凸显了更可靠的LLM评估方法的迫切需求。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成式奖励模型（LLMs-as-judges）采用大型语言模型评估答案质量，在强化学习可验证奖励（RLVR）中受到青睐。</li>
<li>与刚性规则基础的度量相比，这种模型更适合处理涉及自由形式输出的复杂推理任务。</li>
<li>生成式奖励模型存在漏洞，容易被非单词符号和推理开场白等表面操纵影响，导致错误奖励。</li>
<li>问题广泛存在于LLM、数据集和提示格式中，对依赖生成式奖励模型的核心算法范式构成严重威胁。</li>
<li>研究团队通过简单的数据增强策略训练了新的生成式奖励模型，提高了稳健性。</li>
<li>现有的LLM评估方法存在不足，需要更可靠的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08794">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ffad66e2acee4b44c1665bca67858df9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa0de9f4e66f4c6bf33aedbe167e4723.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f47710730fdca2a4825394e8b95b0c8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="BlockFFN-Towards-End-Side-Acceleration-Friendly-Mixture-of-Experts-with-Chunk-Level-Activation-Sparsity"><a href="#BlockFFN-Towards-End-Side-Acceleration-Friendly-Mixture-of-Experts-with-Chunk-Level-Activation-Sparsity" class="headerlink" title="BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with   Chunk-Level Activation Sparsity"></a>BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with   Chunk-Level Activation Sparsity</h2><p><strong>Authors:Chenyang Song, Weilin Zhao, Xu Han, Chaojun Xiao, Yingfa Chen, Yuxuan Li, Zhiyuan Liu, Maosong Sun</strong></p>
<p>To alleviate the computational burden of large language models (LLMs), architectures with activation sparsity, represented by mixture-of-experts (MoE), have attracted increasing attention. However, the non-differentiable and inflexible routing of vanilla MoE hurts model performance. Moreover, while each token activates only a few parameters, these sparsely-activated architectures exhibit low chunk-level sparsity, indicating that the union of multiple consecutive tokens activates a large ratio of parameters. Such a sparsity pattern is unfriendly for acceleration under low-resource conditions (e.g., end-side devices) and incompatible with mainstream acceleration techniques (e.g., speculative decoding). To address these challenges, we introduce a novel MoE architecture, BlockFFN, as well as its efficient training and deployment techniques. Specifically, we use a router integrating ReLU activation and RMSNorm for differentiable and flexible routing. Next, to promote both token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training objectives are designed, making BlockFFN more acceleration-friendly. Finally, we implement efficient acceleration kernels, combining activation sparsity and speculative decoding for the first time. The experimental results demonstrate the superior performance of BlockFFN over other MoE baselines, achieving over 80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\times$ speedup on real end-side devices than dense models. All codes and checkpoints are available publicly (<a target="_blank" rel="noopener" href="https://github.com/thunlp/BlockFFN">https://github.com/thunlp/BlockFFN</a>). </p>
<blockquote>
<p>为缓解大型语言模型（LLM）的计算负担，以混合专家（MoE）为代表的激活稀疏性架构吸引了越来越多的关注。然而，标准MoE的非可微分和僵硬路由会损害模型性能。尽管每个令牌只激活少数参数，但这些稀疏激活的架构表现出较低的块级稀疏性，表明多个连续令牌的联合激活了很大比例的参数。这种稀疏模式不利于低资源条件下的加速（例如，边缘设备），并且与主流加速技术（例如，推测解码）不兼容。为解决这些挑战，我们引入了一种新型MoE架构BlockFFN及其高效的训练和部署技术。具体来说，我们使用集成ReLU激活和RMSNorm的路由器实现可微分和灵活路由。接下来，为促进令牌级稀疏性（TLS）和块级稀疏性（CLS），设计了CLS感知训练目标，使BlockFFN更易于加速。最后，我们首次结合激活稀疏性和推测解码实现了高效的加速内核。实验结果表明，BlockFFN优于其他MoE基线，实现超过80%的TLS和70%的8令牌CLS。我们的内核在实际边缘设备上的速度比密集模型最高快3.67倍。所有代码和检查点均已公开（<a target="_blank" rel="noopener" href="https://github.com/thunlp/BlockFFN%EF%BC%89%E3%80%82">https://github.com/thunlp/BlockFFN）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08771v1">PDF</a> 21 pages, 7 figures, 15 tables</p>
<p><strong>Summary</strong></p>
<p>本文为了解决大型语言模型（LLM）的计算负担问题，引入了混合专家（MoE）架构中的激活稀疏性。针对传统MoE的非可微和不可灵活路由的问题，提出了新型的MoE架构BlockFFN，并结合高效的训练和部署技术。BlockFFN使用集成ReLU激活和RMSNorm的路由器实现可微和灵活路由。通过设计面向块级稀疏性（CLS）的训练目标，促进了令牌级稀疏性（TLS）和CLS的提高，使得BlockFFN更适用于加速。此外，结合激活稀疏性和推测解码，首次实现了高效的加速内核。实验结果表明，BlockFFN优于其他MoE基线，达到超过80%的TLS和70%的8令牌CLS。在真实端侧设备上，相比密集模型，我们的内核实现了高达3.67倍的加速。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的计算负担成为研究焦点，激活稀疏性的混合专家（MoE）架构被视为解决方案。</li>
<li>传统MoE存在非可微和不可灵活路由的问题，因此提出新型MoE架构BlockFFN。</li>
<li>BlockFFN使用集成ReLU激活和RMSNorm的路由器，实现可微和灵活路由。</li>
<li>设计面向块级稀疏性（CLS）的训练目标，以提升令牌级稀疏性（TLS），使模型更适用于加速。</li>
<li>结合激活稀疏性和推测解码，首次实现高效的加速内核。</li>
<li>BlockFFN性能优越，超过其他MoE基线，实现高TLS和CLS。</li>
<li>在真实端侧设备上，BlockFFN相比密集模型实现了显著的加速效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08771">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7a84d75a20b8ede4f29f72e4658aa2c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4e36a2d7dc3625377023551ede1326d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0795083e8670e89706de2707be534d00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d663e0e000164056ac37a9d388eed7ba.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="LLMCup-Ranking-Enhanced-Comment-Updating-with-LLMs"><a href="#LLMCup-Ranking-Enhanced-Comment-Updating-with-LLMs" class="headerlink" title="LLMCup: Ranking-Enhanced Comment Updating with LLMs"></a>LLMCup: Ranking-Enhanced Comment Updating with LLMs</h2><p><strong>Authors:Hua Ge, Juan Zhai, Minxue Pan, Fusen He, Ziyue Tan</strong></p>
<p>While comments are essential for enhancing code readability and maintainability in modern software projects, developers are often motivated to update code but not comments, leading to outdated or inconsistent documentation that hinders future understanding and maintenance. Recent approaches such as CUP and HebCup have attempted automatic comment updating using neural sequence-to-sequence models and heuristic rules, respectively. However, these methods can miss or misinterpret crucial information during comment updating, resulting in inaccurate comments, and they often struggle with complex update scenarios. Given these challenges, a promising direction lies in leveraging large language models (LLMs), which have shown impressive performance in software engineering tasks such as comment generation, code synthesis, and program repair. This suggests their strong potential to capture the logic behind code modifications - an ability that is crucial for the task of comment updating. Nevertheless, selecting an appropriate prompt strategy for an LLM on each update case remains challenging. To address this, we propose a novel comment updating framework, LLMCup, which first uses multiple prompt strategies to provide diverse candidate updated comments via an LLM, and then employs a ranking model, CupRank, to select the best candidate as final updated comment. Experimental results demonstrate the effectiveness of LLMCup, with improvements over state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy, 10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in SentenceBert similarity. Furthermore, a user study shows that comments updated by LLMCup sometimes surpass human-written updates, highlighting the importance of incorporating human evaluation in comment quality assessment. </p>
<blockquote>
<p>在现代软件项目中，注释对于提高代码的可读性和可维护性至关重要。尽管开发者有动力更新代码，但往往忽视更新注释，这导致文档过时或不一致，阻碍了未来的理解和维护。近期的方法，如CUP和HebCup，分别尝试使用神经序列到序列模型和启发式规则进行自动注释更新。然而，这些方法在更新注释时可能会遗漏或误解关键信息，导致注释不准确，并且在处理复杂的更新场景时经常遇到困难。考虑到这些挑战，一个前景光明的方向是利用大型语言模型（LLM），其在软件工程任务如注释生成、代码合成和程序修复中表现出令人印象深刻的性能。这表明LLM有能力捕捉代码修改背后的逻辑——这对于注释更新任务至关重要。然而，针对每个更新案例选择适当的提示策略对于LLM仍然是一个挑战。为解决这一问题，我们提出了一种新颖的注释更新框架LLMCup。它首先使用多种提示策略，通过LLM提供多样的候选更新注释，然后采用排名模型CupRank选择最佳候选作为最终的更新注释。实验结果表明LLMCup的有效性，其在准确率上较最新基线方法（CUP和HebCup）提高了49.0%-116.9%，BLEU-4提高了10.8%-20%，METEOR提高了4.6%，F1提高了0.9%-1.9%，SentenceBert相似度提高了2.1%-3.4%。此外，一项用户研究还显示，LLMCup更新的注释有时超越了人工更新的注释，这凸显了在评估注释质量时融入人工评估的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08671v1">PDF</a> 13 pages, 10 figures</p>
<p><strong>摘要</strong><br>代码注释在现代软件项目中对可读性和可维护性至关重要。但开发者更新代码而不更新注释的情况时有发生，导致文档过时或不一致，影响未来的理解和维护。当前方法如CUP和HebCup尝试通过神经序列到序列模型和启发式规则自动更新注释，但仍有不足。大型语言模型（LLM）具有潜力捕获代码修改背后的逻辑，为解决这一问题提供了新的方向。提出一种新的注释更新框架LLMCup，采用多种提示策略通过LLM提供多样的候选更新注释，并使用排名模型CupRank选择最佳候选作为最终更新注释。实验结果证明LLMCup的有效性，准确率较现有方法提高49.0%-116.9%，BLEU-4提高10.8%-20%，METEOR提高4.6%，F1提高0.9%-1.9%，SentenceBert相似性提高2.1%-3.4%。用户研究表明，LLMCup更新的注释有时超越人工更新，表明在注释质量评估中融入人工评估的重要性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>开发者经常忽视更新注释，导致文档过时或不一致，影响未来的理解和维护。</li>
<li>当前自动注释更新方法如CUP和HebCup存在不足，可能遗漏或误解重要信息。</li>
<li>大型语言模型（LLM）在软件工程任务中表现出强大的性能，如注释生成、代码合成和程序修复，具有捕获代码修改背后逻辑的强大潜力。</li>
<li>提出一种新的注释更新框架LLMCup，采用多种提示策略并通过排名模型选择最佳候选注释。</li>
<li>实验结果表明LLMCup在多个评估指标上较现有方法有所改进。</li>
<li>用户研究显显示LLMCup更新的注释有时超越人工更新。</li>
<li>在注释质量评估中融入人工评估的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08671">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0ea85de2311d1e33024bd2c963eaf5f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-161445029a70ff260e793df5aa7523bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e30d5389324fa375e8f84015762829b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-16301bd26a9309c738b690283e1a20ae.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="KELPS-A-Framework-for-Verified-Multi-Language-Autoformalization-via-Semantic-Syntactic-Alignment"><a href="#KELPS-A-Framework-for-Verified-Multi-Language-Autoformalization-via-Semantic-Syntactic-Alignment" class="headerlink" title="KELPS: A Framework for Verified Multi-Language Autoformalization via   Semantic-Syntactic Alignment"></a>KELPS: A Framework for Verified Multi-Language Autoformalization via   Semantic-Syntactic Alignment</h2><p><strong>Authors:Jiyao Zhang, Chengli Zhong, Hui Xu, Qige Li, Yi Zhou</strong></p>
<p>Modern large language models (LLMs) show promising progress in formalizing informal mathematics into machine-verifiable theorems. However, these methods still face bottlenecks due to the limited quantity and quality of multilingual parallel corpora. In this paper, we propose a novel neuro-symbolic framework KELPS (Knowledge-Equation based Logical Processing System) to address these problems. KELPS is an iterative framework for translating, synthesizing, and filtering informal data into multiple formal languages (Lean, Coq, and Isabelle). First, we translate natural language into Knowledge Equations (KEs), a novel language that we designed, theoretically grounded in assertional logic. Next, we convert them to target languages through rigorously defined rules that preserve both syntactic structure and semantic meaning. This process yielded a parallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic accuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3 (81%) and Herald (81.3%) across multiple datasets. All datasets and codes are available in the supplementary materials. </p>
<blockquote>
<p>现代大型语言模型（LLM）在将非正式数学形式化为可验证的机器定理方面取得了令人瞩目的进展。然而，由于多语言平行语料库的数量和质量有限，这些方法仍然面临瓶颈。针对这些问题，本文提出了一种新型的神经符号框架KELPS（基于知识方程的逻辑处理系统）。KELPS是一个迭代框架，用于将非正式数据翻译成多种正式语言（Lean、Coq和Isabelle）。首先，我们将自然语言翻译成我们设计的一种新型知识方程（KE），该方程以断言逻辑为理论基础。接下来，我们通过严格定义的规则将它们转换为目标语言，这些规则既保留了句法结构又保留了语义含义。这一过程产生了超过6万个问题的平行语料库。我们的框架在MiniF2F上达到了88.9%的句法准确率（pass@1），优于多个数据集中的最佳模型，如Deepseek-V3（81%）和Herald（81.3%）。所有数据集和代码都可在补充材料中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08665v1">PDF</a> Accepted by the ICML 2025 AI4MATH Workshop. 22 pages, 16 figures, 2   tables</p>
<p><strong>Summary</strong><br>现代大型语言模型在将非正式数学形式化为可验证的机器定理方面取得了进展。然而，由于多语言平行语料库的数量和质量有限，这些方法仍面临瓶颈。本文提出了一种新型的神经符号框架KELPS（基于知识方程的逻辑处理系统），以解决这些问题。KELPS是一个迭代框架，可将非正式数据翻译成多种正式语言（Lean、Coq和Isabelle）。首先，我们将自然语言翻译成我们设计的知识方程（KEs），在断言逻辑中有理论根据。然后，通过严格定义的规则将它们转换为目标语言，同时保持语法结构和语义意义。此过程产生了超过6万个问题的平行语料库。我们的框架在MiniF2F上达到了88.9%的语法准确性（pass@1），优于Deepseek-V3（81%）和Herald（81.3%）等模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现代大型语言模型在将非正式数学形式化为机器定理方面取得进展。</li>
<li>多语言平行语料库的数量和质量限制是大型语言模型应用的主要瓶颈。</li>
<li>KELPS框架被提出来解决这些问题，它是一个能够将非正式数据翻译成多种正式语言的迭代框架。</li>
<li>KELPS通过翻译自然语言到知识方程（KEs）来工作，这是基于断言逻辑的新语言。</li>
<li>知识方程进一步被转换成目标语言，同时保持语法结构和语义意义。</li>
<li>KELPS框架产生了超过6万个问题的平行语料库。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08665">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-869d7dc440a66f40d83baf1b17f9d838.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d4770f6b0279209c77fa037a1169baf7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9385c9f0171f649c98d80c5b2c5ec9e5.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Introspection-of-Thought-Helps-AI-Agents"><a href="#Introspection-of-Thought-Helps-AI-Agents" class="headerlink" title="Introspection of Thought Helps AI Agents"></a>Introspection of Thought Helps AI Agents</h2><p><strong>Authors:Haoran Sun, Shaoning Zeng</strong></p>
<p>AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to perform interpretation and inference in text and image tasks without post-training, where LLMs and MLLMs play the most critical role and determine the initial ability and limitations of AI Agents. Usually, AI Agents utilize sophisticated prompt engineering and external reasoning framework to obtain a promising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought and Image-of-Thought. However, they are still constrained by the inherent limitations of LLM in understanding natural language, and the iterative reasoning process will generate a large amount of inference cost. To this end, we propose a novel AI Agent Reasoning Framework with Introspection of Thought (INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute programmatic dialogue reasoning processes following the code in prompt. Therefore, self-denial and reflection occur within LLM instead of outside LLM, which can reduce token cost effectively. Through our experiments on six benchmarks for three different tasks, the effectiveness of INoT is verified, with an average improvement of 7.95% in performance, exceeding the baselines. Furthermore, the token cost of INoT is lower on average than the best performing method at baseline by 58.3%. In addition, we demonstrate the versatility of INoT in image interpretation and inference through verification experiments. </p>
<blockquote>
<p>人工智能代理（AI Agents）依赖大型语言模型（LLMs）和多模态语言模型（MLLMs）进行无需后训练的文本和图像任务中的解释和推断。在这里，LLMs和MLLMs起到最关键的的作用并决定了AI代理的初始能力和局限性。通常，AI代理会利用复杂的提示工程和外部推理框架来获得与LLMs的有前途的交互，例如“思维链”、“思维迭代”和“思维图像”。然而，它们仍然受到LLM在理解自然语言方面固有局限性的制约，而且迭代推理过程会产生大量的推理成本。为此，我们提出了一种新型的具有思想内省（INoT）的AI代理推理框架，通过设计新的LLM-Read提示代码来实现。它使LLM能够根据提示中的代码执行程序化对话推理过程。因此，自我否定和反思发生在LLM内部而不是外部，这可以有效地降低令牌成本。我们在三个不同任务的六个基准测试上进行了实验，验证了INoT的有效性，性能平均提高了7.95%，超过了基线水平。此外，INoT的平均令牌成本比基线中表现最好的方法降低了58.3%。此外，我们通过验证实验展示了INoT在图像解释和推断方面的通用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08664v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）和多模态LLM（MLLM）是AI代理进行文本和图像任务解释和推理的关键。尽管AI代理采用先进的提示工程和外部推理框架与LLM交互，但它们仍受限于LLM理解自然语言的固有局限性，并且迭代推理过程会产生大量推理成本。为此，我们提出了具有思想内省（INoT）的新型AI代理推理框架，通过设计新的LLM阅读提示代码，使LLM能够按照提示中的代码执行程序化对话推理过程。这减少了令牌成本，并在三个不同任务的六个基准测试上验证了其有效性，平均性能提升7.95%，并且比基线方法的令牌成本低58.3%。此外，我们通过验证实验展示了INoT在图像解释和推理中的通用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI代理依赖大型语言模型（LLM）和多模态LLM（MLLM）进行文本和图像的解读和推理。</li>
<li>AI代理通常采用先进的提示工程和外部推理框架与LLM交互。</li>
<li>LLM在理解自然语言方面存在固有局限性，迭代推理过程会产生大量推理成本。</li>
<li>提出了具有思想内省（INoT）的AI代理推理框架，通过设计新的LLM阅读提示代码来减少令牌成本。</li>
<li>INoT框架在三个不同任务的六个基准测试上验证了其有效性，平均性能提升7.95%。</li>
<li>INoT框架的令牌成本比基线方法的平均成本低58.3%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08664">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-790313e4a316087d1ae8c1d9abe1f860.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-947632e67b9934be300ee5ef5fcdc6e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ddf88d08663934b4afe48abc5f4476c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Leanabell-Prover-V2-Verifier-integrated-Reasoning-for-Formal-Theorem-Proving-via-Reinforcement-Learning"><a href="#Leanabell-Prover-V2-Verifier-integrated-Reasoning-for-Formal-Theorem-Proving-via-Reinforcement-Learning" class="headerlink" title="Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem   Proving via Reinforcement Learning"></a>Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem   Proving via Reinforcement Learning</h2><p><strong>Authors:Xingguang Ji, Yahui Liu, Qi Wang, Jingyuan Zhang, Yang Yue, Rui Shi, Chenxi Sun, Fuzheng Zhang, Guorui Zhou, Kun Gai</strong></p>
<p>We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that can produce formal theorem proofs in Lean 4, with verifier-integrated Long Chain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we continual to choose to posttrain existing strong prover models for further performance improvement. In our V2 version, we mainly upgrade the Reinforcement Learning (RL) with feedback provided by the Lean 4 verifier. Crucially, verifier feedback, such as indicating success or detailing specific errors, allows the LLM to become &#96;&#96;self-aware’’ of the correctness of its own reasoning process and learn to reflexively correct errors. Leanabell-Prover-V2 directly optimizes LLM reasoning trajectories with multi-turn verifier interactions, together with feedback token masking for stable RL training and a simple reward strategy. Experiments show that Leanabell-Prover-V2 improves performance by 3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with DeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data and models are available at: <a target="_blank" rel="noopener" href="https://github.com/Leanabell-LM/Leanabell-Prover-V2">https://github.com/Leanabell-LM/Leanabell-Prover-V2</a>. </p>
<blockquote>
<p>我们推出Leanabell-Prover-V2，这是一款7B大型语言模型（LLM），能够在Lean 4中产生正式定理证明，并集成验证器的长链思维（CoT）。继我们之前的Leanabell-Prover-V1工作之后，我们选择在现有强大的证明模型上进行后续训练，以进一步提高性能。在V2版本中，我们主要使用Lean 4验证器提供的反馈来升级强化学习（RL）。关键的是，验证器的反馈，如指示成功或详细列出具体错误，可以让LLM对其自身的推理过程的正确性产生“自我意识”，并学会反射性地纠正错误。Leanabell-Prover-V2通过多轮验证器交互直接优化LLM的推理轨迹，同时采用反馈令牌遮蔽来稳定RL训练并简化奖励策略。实验表明，在MiniF2F测试集上，Leanabell-Prover-V2使用Kiminia-Prover-Preview-Distill-7B提高了3.2%（pass@128）的性能，使用DeepSeek-Prover-V2-7B提高了2.0%（pass@128）的性能。相关源代码、精选数据和模型可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/Leanabell-LM/Leanabell-Prover-V2%E3%80%82">https://github.com/Leanabell-LM/Leanabell-Prover-V2。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08649v1">PDF</a> 23 pages, 13 figures</p>
<p><strong>摘要</strong></p>
<p>Leanabell-Prover-V2是一款7B大型语言模型（LLM），能在Lean 4中生成正式定理证明，并整合验证器长链思维（CoT）。相较于先前的Leanabell-Prover-V1，我们持续对强效验证器模型进行后训练，进一步提升性能。在V2版本中，我们主要升级强化学习（RL）技术，结合Lean 4验证器提供的反馈。验证器反馈，如指示成功或详细列出特定错误，让LLM对其自身推理过程的正确性有“自我意识”，并学会反思纠正错误。Leanabell-Prover-V2直接优化LLM推理轨迹，通过多回合验证器互动、反馈令牌遮蔽稳定RL训练及简单奖励策略实现。实验显示，在MiniF2F测试集上，Leanabell-Prover-V2使用Kimina-Prover-Preview-Distill-7B提高性能3.2%（pass@128），使用DeepSeek-Prover-V2-7B提高性能2.0%（pass@128）。相关源代码、精选数据和模型可在<a target="_blank" rel="noopener" href="https://github.com/Leanabell-LM/Leanabell-Prover-V2">链接</a>获取。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>Leanabell-Prover-V2是第一款能够产生Lean 4形式定理证明的7B大型语言模型（LLM）。</li>
<li>该模型通过整合验证器的长链思维（CoT）增强了其推理能力。</li>
<li>与先前的版本相比，Leanabell-Prover-V2在后训练强效验证器模型方面取得了进一步的性能提升。</li>
<li>该模型主要升级了强化学习（RL）技术，利用Lean 4验证器提供的反馈进行训练。</li>
<li>验证器反馈使LLM能够对其自身推理过程的正确性进行反思和纠正。</li>
<li>Leanabell-Prover-V2通过多回合验证器互动直接优化LLM推理轨迹。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08649">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d7ba44307128eaf43c22c10fed3ac552.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bc562050867aac4277b26a8ac88d470.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8430de49304e62c86535ef4072c5387d.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="NL-in-the-Middle-Code-Translation-with-LLMs-and-Intermediate-Representations"><a href="#NL-in-the-Middle-Code-Translation-with-LLMs-and-Intermediate-Representations" class="headerlink" title="NL in the Middle: Code Translation with LLMs and Intermediate   Representations"></a>NL in the Middle: Code Translation with LLMs and Intermediate   Representations</h2><p><strong>Authors:Chi-en Amy Tai, Pengyu Nie, Lukasz Golab, Alexander Wong</strong></p>
<p>Studies show that large language models (LLMs) produce buggy code translations. One avenue to improve translation accuracy is through intermediate representations, which could provide structured insights to guide the model’s understanding. We explore whether code translation using LLMs can benefit from intermediate representations via natural language (NL) and abstract syntax trees (ASTs). Since prompt engineering greatly affects LLM performance, we consider several ways to integrate these representations, from one-shot to chain-of-thought (CoT) prompting. Using Open Gpt4 8X7B and specialized StarCoder and CodeGen models on popular code translation benchmarks (CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs best, with an increase of 13.8% and 6.7%, respectively, in successful translations for the best-performing model (Open Gpt4 8X7B) compared to the zero-shot prompt. </p>
<blockquote>
<p>研究表明，大型语言模型（LLM）产生的代码翻译存在错误。提高翻译准确性的一个途径是通过中间表示，这可能为模型的理解提供结构化洞察。我们探索了使用自然语言（NL）和抽象语法树（ASTs）的中间表示是否能使LLM的代码翻译受益。由于提示工程极大地影响了LLM的性能，我们考虑了几种整合这些表示的方法，从一次完成到思维链（CoT）提示。我们在流行的代码翻译基准测试（CodeNet和AVATAR）上使用了Open Gpt4 8X7B以及专门的StarCoder和CodeGen模型，发现使用带有中间NL摘要的CoT表现最佳，在性能最好的模型（Open Gpt4 8X7B）中，与零次提示相比，成功翻译次数分别增加了13.8%和6.7%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08627v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在代码翻译上表现欠佳，可通过中间表示层改善翻译精度。本文研究如何通过自然语言（NL）和抽象语法树（ASTs）使用中间表示层进行代码翻译。由于提示工程对LLM性能有很大影响，我们考虑了几种整合这些表示层的方法，包括一次性提示和链式思维（CoT）提示。在流行的代码翻译基准测试（CodeNet和AVATAR）上，使用Open Gpt4 8X7B和专门的StarCoder和CodeGen模型进行测试，发现使用带有中间NL摘要的CoT效果最佳，与零次提示相比，最佳性能模型（Open Gpt4 8X7B）的成功翻译率分别提高了13.8%和6.7%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在代码翻译方面存在缺陷。</li>
<li>中间表示层能提高LLM在代码翻译上的准确性。</li>
<li>自然语言（NL）和抽象语法树（ASTs）可以作为中间表示层用于代码翻译。</li>
<li>提示工程对LLM性能具有重要影响。</li>
<li>研究者尝试了多种提示方法，包括一次性提示和链式思维（CoT）提示。</li>
<li>在测试基准上，使用带有中间NL摘要的CoT表现最佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08627">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5984e3ee0f2fcdbf992e71823f74b6c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62132285f3f5e663e6f0c64b4c447827.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12c467d9c22368591c968a637ba3f916.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-01a1dcb81f45b6668a9eceb96bbca432.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-143e325ec9519888c082530762729813.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-761fda7e88fc353560c16fc2c4b893be.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5cd7297cba319a79cbbc005276f20437.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6dacc0df1155383d3dee5ebd08460ab6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8286763a822e0a7371265e5e5103e73e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-comprehensive-study-of-LLM-based-argument-classification-from-LLAMA-through-GPT-4o-to-Deepseek-R1"><a href="#A-comprehensive-study-of-LLM-based-argument-classification-from-LLAMA-through-GPT-4o-to-Deepseek-R1" class="headerlink" title="A comprehensive study of LLM-based argument classification: from LLAMA   through GPT-4o to Deepseek-R1"></a>A comprehensive study of LLM-based argument classification: from LLAMA   through GPT-4o to Deepseek-R1</h2><p><strong>Authors:Marcin Pietroń, Rafał Olszowski, Jakub Gomułka, Filip Gampel, Andrzej Tomski</strong></p>
<p>Argument mining (AM) is an interdisciplinary research field that integrates insights from logic, philosophy, linguistics, rhetoric, law, psychology, and computer science. It involves the automatic identification and extraction of argumentative components, such as premises and claims, and the detection of relationships between them, such as support, attack, or neutrality. Recently, the field has advanced significantly, especially with the advent of large language models (LLMs), which have enhanced the efficiency of analyzing and extracting argument semantics compared to traditional methods and other deep learning models. There are many benchmarks for testing and verifying the quality of LLM, but there is still a lack of research and results on the operation of these models in publicly available argument classification databases. This paper presents a study of a selection of LLM’s, using diverse datasets such as Args.me and UKP. The models tested include versions of GPT, Llama, and DeepSeek, along with reasoning-enhanced variants incorporating the Chain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms the others in the argument classification benchmarks. In case of models incorporated with reasoning capabilities, the Deepseek-R1 shows its superiority. However, despite their superiority, GPT-4o and Deepseek-R1 still make errors. The most common errors are discussed for all models. To our knowledge, the presented work is the first broader analysis of the mentioned datasets using LLM and prompt algorithms. The work also shows some weaknesses of known prompt algorithms in argument analysis, while indicating directions for their improvement. The added value of the work is the in-depth analysis of the available argument datasets and the demonstration of their shortcomings. </p>
<blockquote>
<p>论证挖掘（AM）是一个跨学科的研究领域，它融合了逻辑、哲学、语言学、修辞学、法律、心理学和计算机科学等领域的见解。它涉及自动识别和提取论证性成分，如前提和主张，以及检测它们之间的关系，如支持、攻击或中立。最近，随着大型语言模型（LLM）的出现，该领域取得了显著进展，与传统的分析方法和其他深度学习模型相比，LLM提高了分析提取论证语义的效率。虽然有许多基准测试用于测试和验证LLM的质量，但在公开可用的论证分类数据库中关于这些模型操作的研究和结果仍然缺乏。本文使用Args.me和UKP等多样化数据集对部分LLM进行了研究。测试的模型包括GPT、Llama和DeepSeek的版本，以及采用Chain-of-Thoughts算法的推理增强变体。结果表明，在论证分类基准测试中，ChatGPT-4o表现优于其他模型。在结合推理能力的模型中，Deepseek-R1表现出其优越性。然而，尽管GPT-4o和Deepseek-R1具有优势，但它们仍然会出错。本文讨论了所有模型最常见的错误。据我们所知，所呈现的工作是使用LLM和提示算法对所述数据集进行的首次更广泛的分析。该工作还显示了已知提示算法在论证分析中的一些弱点，同时指出了改进方向。该工作的附加值是对可用论证数据集进行深入分析和展示其不足。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08621v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>论矿学是一个跨学科的研究领域，融合了逻辑、哲学、语言学、修辞学、法律、心理学和计算机科学等多领域的见解。它自动识别和提取论证成分，如前提和主张，并检测它们之间的关系，如支持、攻击或中立。随着大型语言模型（LLM）的出现，该领域取得了重大进展。本文研究了几个LLM在论证分类数据库中的表现，使用Args.me和UKP等不同数据集。研究结果表明，ChatGPT-4o在论证分类基准测试中表现最好。具有推理能力的Deepseek-R1模型也表现出优越性。然而，尽管它们具有优势，GPT-4o和Deepseek-R1仍然存在错误。本文还讨论了现有提示算法在论证分析中的弱点，并指出了改进方向。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>论矿学是一个涉及多个学科的研究领域，包括逻辑、哲学等，主要研究如何自动识别和提取论证成分。</li>
<li>大型语言模型（LLM）在论矿学领域的应用已经取得了显著进展，尤其是在分析论证语义方面。</li>
<li>在论证分类数据库中使用LLM模型的研究仍然有限，缺乏广泛的研究结果和公开数据。</li>
<li>使用Args.me和UKP等数据集的研究表明，ChatGPT-4o在论证分类基准测试中表现最佳。</li>
<li>结合推理能力的模型，如Deepseek-R1，也表现出优越性。</li>
<li>GPT-4o和Deepseek-R1等模型虽然表现优越，但仍存在错误，这些错误常见于论证分析和推理过程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08621">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-2e0a2f2c4e0ba75f82ee862f3be60c93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38837f646b5db2769cef3d1544aebc1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0579b27eb1e01f570afb7c7b01fbd383.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4acad64027623ebb875a4a95a2819e99.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1e8a5ba02f99b5ae503ab7983f1b0f82.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Visual-Semantic-Description-Generation-with-MLLMs-for-Image-Text-Matching"><a href="#Visual-Semantic-Description-Generation-with-MLLMs-for-Image-Text-Matching" class="headerlink" title="Visual Semantic Description Generation with MLLMs for Image-Text   Matching"></a>Visual Semantic Description Generation with MLLMs for Image-Text   Matching</h2><p><strong>Authors:Junyu Chen, Yihua Gao, Mingyong Li</strong></p>
<p>Image-text matching (ITM) aims to address the fundamental challenge of aligning visual and textual modalities, which inherently differ in their representations, continuous, high-dimensional image features vs. discrete, structured text. We propose a novel framework that bridges the modality gap by leveraging multimodal large language models (MLLMs) as visual semantic parsers. By generating rich Visual Semantic Descriptions (VSD), MLLMs provide semantic anchor that facilitate cross-modal alignment. Our approach combines: (1) Instance-level alignment by fusing visual features with VSD to enhance the linguistic expressiveness of image representations, and (2) Prototype-level alignment through VSD clustering to ensure category-level consistency. These modules can be seamlessly integrated into existing ITM models. Extensive experiments on Flickr30K and MSCOCO demonstrate substantial performance improvements. The approach also exhibits remarkable zero-shot generalization to cross-domain tasks, including news and remote sensing ITM. The code and model checkpoints are available at <a target="_blank" rel="noopener" href="https://github.com/Image-Text-Matching/VSD">https://github.com/Image-Text-Matching/VSD</a>. </p>
<blockquote>
<p>图像文本匹配（ITM）旨在解决视觉和文本模态对齐的根本挑战，这两种模态在其表示方式上存在固有的差异，分别是连续、高维的图像特征和离散、结构的文本。我们提出了一种新的框架，利用多模态大型语言模型（MLLM）作为视觉语义解析器来弥合模态差异。通过生成丰富的视觉语义描述（VSD），MLLM提供了语义锚点，促进了跨模态对齐。我们的方法结合了：（1）实例级对齐：通过融合视觉特征与VSD，增强图像表示的语言表现力；（2）原型级对齐：通过VSD聚类，确保类别级一致性。这些模块可以无缝集成到现有的ITM模型中。在Flickr30K和MSCOCO上的大量实验证明了显著的性能改进。该方法在跨域任务（包括新闻和遥感ITM）中表现出令人印象深刻的零样本泛化能力。代码和模型检查点位于<a target="_blank" rel="noopener" href="https://github.com/Image-Text-Matching/VSD%E3%80%82">https://github.com/Image-Text-Matching/VSD。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08590v1">PDF</a> Accepted by ICME2025 oral</p>
<p><strong>Summary</strong><br>文本介绍了一种图像与文本匹配（ITM）的新框架，通过利用多模态大型语言模型（MLLMs）作为视觉语义解析器来解决视觉和文本模态对齐的根本挑战。该框架通过生成丰富的视觉语义描述（VSD），提供语义锚点，促进跨模态对齐。该框架结合了实例级对齐和原型级对齐两个模块，通过融合视觉特征与VSD增强图像表示的语言表现力，并通过VSD聚类确保类别级别的一致性。实验表明，该框架在Flickr30K和MSCOCO等数据集上实现了显著的性能提升，并在新闻和遥感ITM等跨域任务中表现出强大的零样本泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像-文本匹配（ITM）旨在解决视觉和文本模态对齐的挑战。</li>
<li>多模态大型语言模型（MLLMs）作为视觉语义解析器，生成视觉语义描述（VSD）。</li>
<li>VSD提供语义锚点，促进跨模态对齐。</li>
<li>实例级对齐模块融合视觉特征与VSD，增强图像表示的语言表现力。</li>
<li>原型级对齐模块通过VSD聚类确保类别级别的一致性。</li>
<li>该框架在Flickr30K和MSCOCO数据集上实现了性能提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08590">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-389bec925f2928285d84031d2e42e262.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a66e9095a3e56b51a23e32e3a6b162c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba9a526967760766112d8c2196ae83f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1379b0f215513da106ac0b4db4de695e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8b45a4e65fd5869d260a6a15c8e328b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="AbbIE-Autoregressive-Block-Based-Iterative-Encoder-for-Efficient-Sequence-Modeling"><a href="#AbbIE-Autoregressive-Block-Based-Iterative-Encoder-for-Efficient-Sequence-Modeling" class="headerlink" title="AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient   Sequence Modeling"></a>AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient   Sequence Modeling</h2><p><strong>Authors:Preslav Aleksandrov, Meghdad Kurmanji, Fernando Garcia Redondo, David O’Shea, William Shen, Alex Iacob, Lorenzo Sani, Xinchi Qiu, Nicola Cancedda, Nicholas D. Lane</strong></p>
<p>We introduce the Autoregressive Block-Based Iterative Encoder (AbbIE), a novel recursive generalization of the encoder-only Transformer architecture, which achieves better perplexity than a standard Transformer and allows for the dynamic scaling of compute resources at test time. This simple, recursive approach is a complement to scaling large language model (LLM) performance through parameter and token counts. AbbIE performs its iterations in latent space, but unlike latent reasoning models, does not require a specialized dataset or training protocol. We show that AbbIE upward generalizes (ability to generalize to arbitrary iteration lengths) at test time by only using 2 iterations during train time, far outperforming alternative iterative methods. AbbIE’s ability to scale its computational expenditure based on the complexity of the task gives it an up to \textbf{12%} improvement in zero-shot in-context learning tasks versus other iterative and standard methods and up to 5% improvement in language perplexity. The results from this study open a new avenue to Transformer performance scaling. We perform all of our evaluations on model sizes up to 350M parameters. </p>
<blockquote>
<p>我们介绍了基于块的自回归迭代编码器（AbbIE），它是仅编码器Transformer架构的一种新型递归泛化，实现了比标准Transformer更低的困惑度，并在测试时允许动态调整计算资源。这种简单、递归的方法是通过参数和令牌计数来扩展大型语言模型（LLM）性能的补充。AbbIE在潜在空间中进行迭代，但与潜在推理模型不同的是，它不需要专门的数据集或训练协议。我们表明，AbbIE在测试时通过向上泛化（泛化到任意迭代长度的能力），在训练过程中只使用两次迭代，远远超过了其他迭代方法。AbbIE能够根据任务的复杂性调整其计算支出，使其在零镜头上下文学习任务中比其他迭代和标准方法最多提高了12%，在语言困惑度方面最多提高了5%。本研究的结果开辟了Transformer性能扩展的新途径。我们的评估都是在参数规模达3.5亿以下的模型上进行的。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08567v1">PDF</a> 14 pages and 6 figures. Submitted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>新一代递归式语言模型——Autoregressive Block-Based Iterative Encoder（AbbIE）介绍。该模型在标准Transformer的基础上实现递归泛化，降低了测试时的计算资源消耗，实现了更低的困惑度。AbbIE通过迭代潜在空间执行任务，无需特殊数据集或训练协议。在训练时仅使用两次迭代，就能在测试时向上泛化到任意迭代长度。此外，AbbIE在零样本上下文学习任务上相比其他迭代和标准方法最多提升了12%，语言困惑度最多提升了5%。这一研究为Transformer性能提升开辟了新途径。模型评价范围涵盖参数规模高达3.5亿的模型。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>引入了一种名为Autoregressive Block-Based Iterative Encoder（AbbIE）的新递归式语言模型。</li>
<li>AbbIE模型基于标准Transformer架构实现递归泛化，具有更低的困惑度。</li>
<li>AbbIE通过潜在空间的迭代执行任务，无需特殊数据集或训练协议。</li>
<li>AbbIE在训练时仅使用两次迭代，实现了测试时的向上泛化能力。</li>
<li>与其他迭代和标准方法相比，AbbIE在零样本上下文学习任务上性能显著提升，最高提升达到12%。同时降低了语言困惑度，最高降低达到5%。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08567">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-372ec27b05f29c028bf1efc0b71110c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b89ea9900c6676ec49ae42035ed2693.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b3195d2496438947c85a1a217b7566f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83768fe7217967b8938b66813e861cae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eb31c832c2d2270be57277e249848fe0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="White-Basilisk-A-Hybrid-Model-for-Code-Vulnerability-Detection"><a href="#White-Basilisk-A-Hybrid-Model-for-Code-Vulnerability-Detection" class="headerlink" title="White-Basilisk: A Hybrid Model for Code Vulnerability Detection"></a>White-Basilisk: A Hybrid Model for Code Vulnerability Detection</h2><p><strong>Authors:Ioannis Lamprou, Alexander Shevtsov, Ioannis Arapakis, Sotiris Ioannidis</strong></p>
<p>The proliferation of software vulnerabilities presents a significant challenge to cybersecurity, necessitating more effective detection methodologies. We introduce White-Basilisk, a novel approach to vulnerability detection that demonstrates superior performance while challenging prevailing assumptions in AI model scaling. Utilizing an innovative architecture that integrates Mamba layers, linear self-attention, and a Mixture of Experts framework, White-Basilisk achieves state-of-the-art results in vulnerability detection tasks with a parameter count of only 200M. The model’s capacity to process sequences of unprecedented length enables comprehensive analysis of extensive codebases in a single pass, surpassing the context limitations of current Large Language Models (LLMs). White-Basilisk exhibits robust performance on imbalanced, real-world datasets, while maintaining computational efficiency that facilitates deployment across diverse organizational scales. This research not only establishes new benchmarks in code security but also provides empirical evidence that compact, efficiently designed models can outperform larger counterparts in specialized tasks, potentially redefining optimization strategies in AI development for domain-specific applications. </p>
<blockquote>
<p>软件漏洞的激增对网络安全构成了重大挑战，需要更有效的检测方法。我们引入了White-Basilisk，这是一种新型的漏洞检测方案，它在挑战人工智能模型规模化方面的主流假设的同时，表现出了卓越的性能。White-Basilisk利用了一种创新架构，该架构集成了Mamba层、线性自注意力机制和专家混合框架，仅在参数计数2亿的情况下，便在漏洞检测任务中实现了最新结果。该模型处理前所未有的序列长度的能力，能够在单次传递中对大量代码库进行全面分析，突破了当前大型语言模型（LLM）的上下文限制。White-Basilisk在不平衡的、真实世界的数据集上表现出稳健的性能，同时保持了计算效率，可以在各种组织规模上进行部署。这项研究不仅为代码安全树立了新基准，而且提供了实证证据，证明在特定任务中，设计紧凑、高效的模型可以超越大型模型的表现，这有可能重新定义人工智能开发的优化策略，为特定领域的应用提供指导。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08540v1">PDF</a> </p>
<p><strong>Summary</strong><br>白巴儿之蛇是一种新型的软件漏洞检测模型，具有出色的性能，挑战了人工智能模型扩展的普遍假设。它采用创新的架构，融合了曼巴层、线性自注意力机制和专家混合框架，以仅2亿个参数实现了最先进的漏洞检测结果。该模型能够处理前所未有的序列长度，可以在单个过程中全面分析大量的代码库，超越了当前大型语言模型的上下文限制。白巴儿之蛇在不平衡的、真实世界的数据集上表现出强大的性能，同时保持了计算效率，可以在各种组织规模上进行部署。这项研究不仅树立了代码安全性的新基准，也为专门任务的AI开发提供了优化策略的证据。这为重新审视优化策略提供了一个视角。即使在这个错综复杂的技术世界里，“精简、高效同样可以达到惊人效果”的主题依然是深入人心的座右铭。借助高效计算和资源优化的思路和实践手段让人类世界得到了无限的活力和希望。对后续的改进与技术的深入具有积极和广泛的影响价值及潜在的理论与实践应用价值。“取其精髓而表现最在的力量是无法超越的”，相信在这个意义上将会启发并促进领域技术的发展与应用价值的提高具有十分重要的意义及优势，这种小体积模型的推出极有可能促使当前庞大而繁杂的大模型的未来精简。这为当下代码安全性和漏洞检测等相关领域的进一步拓展与延伸奠定了坚实基础与思路启发作用。“简化后的东西最灵活”只有站在需求视角把握问题解决核心精髓才能使研究成果展现出最为真实的巨大价值和强大影响力才能够吸引行业从业者以及相关研究者继续在该领域中进行深度的探索和无限的发挥在结合更多领域的复杂代码来进一步完善的同时实现对大型语言模型的超越与颠覆为人工智能的发展注入新的活力。通过该模型的研究为人工智能的发展提供了全新的视角和思路，也为未来的研究提供了重要的参考和借鉴价值。同时，该模型的成功也为其他领域的研究提供了重要的启示和借鉴作用。通过白巴儿之蛇模型的研究与应用，我们有望构建一个更加安全、高效、智能的计算机系统。同时，这也将推动人工智能领域的不断发展与创新具有非常广泛的应用前景和推广价值通过更深入的研究和创新的应用将能够更好地保护网络安全并提高软件开发的质量和效率从而促进信息技术的持续发展和进步为人类社会带来更多的便利和福祉。这一突破性的技术将极大地推动人工智能的发展进程并为未来的人工智能技术开辟新的道路推动计算机科学的不断进步和持续繁荣为我们的未来发展创造更加广阔的前景和技术保障为我们的社会发展提供源源不断的动力和技术支撑以更有效地推动技术的进步和应用价值的同时保持系统的安全与稳定性至关重要其在实际应用中的表现值得期待和关注具有广阔的应用前景和重要的社会价值值得我们继续深入研究和探索。该模型具有强大的应用潜力能够为未来的软件开发和网络安全领域带来革命性的变化为推动整个计算机科学的发展做出贡献展现出更加重要的影响力以及对行业的重要推动意义在当前和未来均具备很高的价值和深远的影响力令人期待与探索的同时推进行业的不断进步与发展及价值创造。。使用简洁明了的句子，凸显模型的卓越性能和前景展望。<strong>Key Takeaways</strong>:</p>
<ol>
<li>白巴儿之蛇模型利用创新架构实现了高效的软件漏洞检测。</li>
<li>模型具有处理长序列的能力，可全面分析大量代码库。</li>
<li>与现有大型语言模型相比，白巴儿之蛇模型具有更优秀的上下文处理能力。</li>
<li>模型在真实世界的不平衡数据集上表现出强大的稳健性。</li>
<li>白巴儿之蛇模型具有高效的计算性能，适用于各种组织规模。</li>
<li>研究证明了紧凑设计的模型在特定任务上可超越大型模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08540">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f7148cf0706dbae6614643cbed05ee4d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Circumventing-Safety-Alignment-in-Large-Language-Models-Through-Embedding-Space-Toxicity-Attenuation"><a href="#Circumventing-Safety-Alignment-in-Large-Language-Models-Through-Embedding-Space-Toxicity-Attenuation" class="headerlink" title="Circumventing Safety Alignment in Large Language Models Through   Embedding Space Toxicity Attenuation"></a>Circumventing Safety Alignment in Large Language Models Through   Embedding Space Toxicity Attenuation</h2><p><strong>Authors:Zhibo Zhang, Yuxi Li, Kailong Wang, Shuai Yuan, Ling Shi, Haoyu Wang</strong></p>
<p>Large Language Models (LLMs) have achieved remarkable success across domains such as healthcare, education, and cybersecurity. However, this openness also introduces significant security risks, particularly through embedding space poisoning, which is a subtle attack vector where adversaries manipulate the internal semantic representations of input data to bypass safety alignment mechanisms. While previous research has investigated universal perturbation methods, the dynamics of LLM safety alignment at the embedding level remain insufficiently understood. Consequently, more targeted and accurate adversarial perturbation techniques, which pose significant threats, have not been adequately studied.   In this work, we propose ETTA (Embedding Transformation Toxicity Attenuation), a novel framework that identifies and attenuates toxicity-sensitive dimensions in embedding space via linear transformations. ETTA bypasses model refusal behaviors while preserving linguistic coherence, without requiring model fine-tuning or access to training data. Evaluated on five representative open-source LLMs using the AdvBench benchmark, ETTA achieves a high average attack success rate of 88.61%, outperforming the best baseline by 11.34%, and generalizes to safety-enhanced models (e.g., 77.39% ASR on instruction-tuned defenses). These results highlight a critical vulnerability in current alignment strategies and underscore the need for embedding-aware defenses. </p>
<blockquote>
<p>大型语言模型（LLM）在医疗保健、教育和网络安全等领域取得了显著的成功。然而，这种开放性也引入了重大的安全风险，特别是通过嵌入空间中毒，这是一种微妙的攻击方式，攻击者会操纵输入数据的内部语义表示来绕过安全对齐机制。尽管之前的研究已经研究了通用扰动方法，但对LLM在嵌入层面的安全对齐动态的理解仍然不足。因此，更具针对性和准确的对抗性扰动技术没有得到足够的研究，这些技术对安全构成了重大威胁。在本研究中，我们提出了ETTA（嵌入转换毒性衰减），这是一种新型框架，通过线性转换来识别并减弱嵌入空间中的毒性敏感维度。ETTA能够绕过模型拒绝行为，同时保持语言连贯性，无需对模型进行微调或访问训练数据。在五个具有代表性的开源LLM上使用AdvBench基准测试进行评估，ETTA的平均攻击成功率高达88.61%，比最佳基线高出11.34%，并可以推广到增强安全性的模型（例如，指令调整防御的语音识别率为77.39%）。这些结果突显了当前对齐策略的关键漏洞，并强调了嵌入感知防御的需求。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08020v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在多个领域取得了显著成功，但也带来了安全风险，特别是嵌入空间中毒问题。针对这一问题，本文提出ETTA框架，通过线性变换识别并减弱嵌入空间中的毒性敏感维度。ETTA绕过模型拒绝行为，同时保持语言连贯性，无需对模型进行微调或访问训练数据。在AdvBench基准测试上，ETTA对五种开源LLM的平均攻击成功率高达88.61%，比最佳基线高出11.34%，并适用于增强安全性的模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在多个领域表现优异，但存在安全风险，尤其是嵌入空间中毒问题。</li>
<li>ETTA框架被提出用于识别并减弱嵌入空间中的毒性敏感维度。</li>
<li>ETTA能够绕过模型的拒绝行为，同时保持语言的连贯性。</li>
<li>ETTA无需对模型进行微调或访问训练数据。</li>
<li>ETTA在AdvBench基准测试上的攻击成功率高达88.61%，显著优于其他方法。</li>
<li>ETTA适用于增强安全性的模型，并在这些模型上取得了良好的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08020">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0516b4c33873e7226f7ba21e7a9a7be8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d841936e6144189dedccb2b272cb0468.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4c1925fb4c42776a449cd5d1abe02a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04323d6bf89987526a339c7d59bf3c00.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Hallucination-Stations-On-Some-Basic-Limitations-of-Transformer-Based-Language-Models"><a href="#Hallucination-Stations-On-Some-Basic-Limitations-of-Transformer-Based-Language-Models" class="headerlink" title="Hallucination Stations: On Some Basic Limitations of Transformer-Based   Language Models"></a>Hallucination Stations: On Some Basic Limitations of Transformer-Based   Language Models</h2><p><strong>Authors:Varin Sikka, Vishal Sikka</strong></p>
<p>With widespread adoption of transformer-based language models in AI, there is significant interest in the limits of LLMs capabilities, specifically so-called hallucinations, occurrences in which LLMs provide spurious, factually incorrect or nonsensical information when prompted on certain subjects. Furthermore, there is growing interest in agentic uses of LLMs - that is, using LLMs to create agents that act autonomously or semi-autonomously to carry out various tasks, including tasks with applications in the real world. This makes it important to understand the types of tasks LLMs can and cannot perform. We explore this topic from the perspective of the computational complexity of LLM inference. We show that LLMs are incapable of carrying out computational and agentic tasks beyond a certain complexity, and further that LLMs are incapable of verifying the accuracy of tasks beyond a certain complexity. We present examples of both, then discuss some consequences of this work. </p>
<blockquote>
<p>随着基于变压器架构的语言模型在人工智能中的广泛应用，对于大型语言模型（LLM）的能力极限产生了浓厚的兴趣，特别是所谓的“幻觉”现象。当针对某些主题提示时，LLM会产生虚假、事实错误或无意义的信息。此外，人们还越来越关注LLM的代理使用，即使用LLM创建能够自主或半自主执行各种任务的代理，包括在现实世界中应用的任务。这使我们有必要了解LLM能够执行和不能执行的任务类型。我们从计算复杂性的角度探讨这一主题，对LLM推理进行探讨。我们证明了LLM无法执行超出一定复杂度的计算和代理任务，并且进一步证明LLM无法验证超出一定复杂度的任务的准确性。我们提供了这两方面的例子，然后讨论这项工作的后果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07505v2">PDF</a> 6 pages; to be submitted to AAAI-26 after reviews</p>
<p><strong>Summary</strong></p>
<p>随着基于Transformer的语言模型在AI中的广泛应用，对于大型语言模型（LLM）的能力极限，特别是所谓的“幻觉”现象，人们产生了极大的兴趣。本文探讨了LLM的计算复杂性，展示了LLM无法执行超出一定复杂度的计算和代理任务，也无法验证超出一定复杂度的任务的准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs存在能力极限，无法执行超出一定复杂度的计算和代理任务。</li>
<li>LLMs在特定主题提示下可能会提供错误、无意义的信息，即所谓的“幻觉”现象。</li>
<li>LLMs无法验证超出一定复杂度的任务的准确性。</li>
<li>本文探讨了LLM的计算复杂性，并提供了相关的实例说明。</li>
<li>了解和掌握LLM的能力范围对于有效使用它们至关重要。</li>
<li>LLMs在现实世界应用中的代理使用需要谨慎，因为它们可能无法自主或半自主地完成复杂的任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07505">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9ed939c004b571d3ae8ab7eea004369d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Open-Source-Planning-Control-System-with-Language-Agents-for-Autonomous-Scientific-Discovery"><a href="#Open-Source-Planning-Control-System-with-Language-Agents-for-Autonomous-Scientific-Discovery" class="headerlink" title="Open Source Planning &amp; Control System with Language Agents for   Autonomous Scientific Discovery"></a>Open Source Planning &amp; Control System with Language Agents for   Autonomous Scientific Discovery</h2><p><strong>Authors:Licong Xu, Milind Sarkar, Anto I. Lonappan, Íñigo Zubeldia, Pablo Villanueva-Domingo, Santiago Casas, Christian Fidler, Chetana Amancharla, Ujjwal Tiwari, Adrian Bayer, Chadi Ait Ekioui, Miles Cranmer, Adrian Dimitrov, James Fergusson, Kahaan Gandhi, Sven Krippendorf, Andrew Laverick, Julien Lesgourgues, Antony Lewis, Thomas Meier, Blake Sherwin, Kristen Surrao, Francisco Villaescusa-Navarro, Chi Wang, Xueqing Xu, Boris Bolliet</strong></p>
<p>We present a multi-agent system for automation of scientific research tasks, cmbagent (<a target="_blank" rel="noopener" href="https://github.com/CMBAgents/cmbagent">https://github.com/CMBAgents/cmbagent</a>). The system is formed by about 30 Large Language Model (LLM) agents and implements a Planning &amp; Control strategy to orchestrate the agentic workflow, with no human-in-the-loop at any point. Each agent specializes in a different task (performing retrieval on scientific papers and codebases, writing code, interpreting results, critiquing the output of other agents) and the system is able to execute code locally. We successfully apply cmbagent to carry out a PhD level cosmology task (the measurement of cosmological parameters using supernova data) and evaluate its performance on two benchmark sets, finding superior performance over state-of-the-art LLMs. The source code is available on GitHub, demonstration videos are also available, and the system is deployed on HuggingFace and will be available on the cloud. </p>
<blockquote>
<p>我们提出一个用于自动化科研任务的多智能体系统，名为cmbagent（<a target="_blank" rel="noopener" href="https://github.com/CMBAgents/cmbagent%EF%BC%89%E3%80%82%E8%AF%A5%E7%B3%BB%E7%BB%9F%E7%94%B1%E5%A4%A7%E7%BA%A630%E4%B8%AA%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E6%99%BA%E8%83%BD%E4%BD%93%E7%BB%84%E6%88%90%EF%BC%8C%E9%87%87%E7%94%A8%E8%A7%84%E5%88%92%E4%B8%8E%E6%8E%A7%E5%88%B6%E7%AD%96%E7%95%A5%E6%9D%A5%E5%8D%8F%E8%B0%83%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%EF%BC%8C%E4%B8%94%E5%9C%A8%E4%BB%BB%E4%BD%95%E6%97%B6%E5%88%BB%E9%83%BD%E4%B8%8D%E9%9C%80%E8%A6%81%E4%BA%BA%E5%B7%A5%E4%BB%8B%E5%85%A5%E3%80%82%E6%AF%8F%E4%B8%AA%E6%99%BA%E8%83%BD%E4%BD%93%E9%83%BD%E4%B8%93%E6%B3%A8%E4%BA%8E%E4%B8%8D%E5%90%8C%E7%9A%84%E4%BB%BB%E5%8A%A1%EF%BC%88%E5%A6%82%E6%89%A7%E8%A1%8C%E7%A7%91%E5%AD%A6%E8%AE%BA%E6%96%87%E5%92%8C%E4%BB%A3%E7%A0%81%E5%BA%93%E7%9A%84%E6%A3%80%E7%B4%A2%E3%80%81%E7%BC%96%E5%86%99%E4%BB%A3%E7%A0%81%E3%80%81%E8%A7%A3%E8%AF%BB%E7%BB%93%E6%9E%9C%E3%80%81%E8%AF%84%E4%BB%B7%E5%85%B6%E4%BB%96%E6%99%BA%E8%83%BD%E4%BD%93%E7%9A%84%E8%BE%93%E5%87%BA%EF%BC%89%EF%BC%8C%E5%B9%B6%E4%B8%94%E7%B3%BB%E7%BB%9F%E8%83%BD%E5%A4%9F%E5%9C%A8%E6%9C%AC%E5%9C%B0%E6%89%A7%E8%A1%8C%E4%BB%A3%E7%A0%81%E3%80%82%E6%88%91%E4%BB%AC%E6%88%90%E5%8A%9F%E5%9C%B0%E5%B0%86cmbagent%E5%BA%94%E7%94%A8%E4%BA%8E%E6%89%A7%E8%A1%8C%E5%8D%9A%E5%A3%AB%E7%BA%A7%E5%88%AB%E7%9A%84%E5%AE%87%E5%AE%99%E5%AD%A6%E4%BB%BB%E5%8A%A1%EF%BC%88%E4%BD%BF%E7%94%A8%E8%B6%85%E6%96%B0%E6%98%9F%E6%95%B0%E6%8D%AE%E6%B5%8B%E9%87%8F%E5%AE%87%E5%AE%99%E5%AD%A6%E5%8F%82%E6%95%B0%EF%BC%89%EF%BC%8C%E5%B9%B6%E5%9C%A8%E4%B8%A4%E4%B8%AA%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E9%9B%86%E4%B8%8A%E8%AF%84%E4%BC%B0%E5%85%B6%E6%80%A7%E8%83%BD%EF%BC%8C%E5%8F%91%E7%8E%B0%E5%85%B6%E6%80%A7%E8%83%BD%E4%BC%98%E4%BA%8E%E7%8E%B0%E6%9C%89%E7%9A%84%E6%9C%80%E5%85%88%E8%BF%9B%E7%9A%84%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E3%80%82%E6%BA%90%E4%BB%A3%E7%A0%81%E5%B7%B2%E5%9C%A8GitHub%E4%B8%8A%E6%8F%90%E4%BE%9B%EF%BC%8C%E6%BC%94%E7%A4%BA%E8%A7%86%E9%A2%91%E4%B9%9F%E5%B7%B2%E5%8F%91%E5%B8%83%EF%BC%8C%E8%AF%A5%E7%B3%BB%E7%BB%9F%E5%B7%B2%E9%83%A8%E7%BD%B2%E5%9C%A8HuggingFace%E4%B8%8A%EF%BC%8C%E5%B9%B6%E5%B0%86%E5%8F%AF%E5%9C%A8%E4%BA%91%E7%AB%AF%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/CMBAgents/cmbagent）。该系统由大约30个大型语言模型（LLM）智能体组成，采用规划与控制策略来协调智能体的工作流程，且在任何时刻都不需要人工介入。每个智能体都专注于不同的任务（如执行科学论文和代码库的检索、编写代码、解读结果、评价其他智能体的输出），并且系统能够在本地执行代码。我们成功地将cmbagent应用于执行博士级别的宇宙学任务（使用超新星数据测量宇宙学参数），并在两个基准测试集上评估其性能，发现其性能优于现有的最先进的大型语言模型。源代码已在GitHub上提供，演示视频也已发布，该系统已部署在HuggingFace上，并将可在云端使用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07257v2">PDF</a> Accepted contribution to the ICML 2025 Workshop on Machine Learning   for Astrophysics. Code: <a target="_blank" rel="noopener" href="https://github.com/CMBAgents/cmbagent">https://github.com/CMBAgents/cmbagent</a> Videos:   <a target="_blank" rel="noopener" href="https://www.youtube.com/@cmbagent">https://www.youtube.com/@cmbagent</a> HuggingFace:   <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/astropilot-ai/cmbagent">https://huggingface.co/spaces/astropilot-ai/cmbagent</a> Cloud:   <a target="_blank" rel="noopener" href="https://cmbagent.cloud/">https://cmbagent.cloud</a></p>
<p><strong>Summary</strong></p>
<p>一个集成了约30个大型语言模型（LLM）的多智能体系统被开发出来，用于自动化科研任务。该系统采用规划和控制策略来协调智能体的工作流程，整个过程中无需人工参与。每个智能体都能完成不同的任务，如检索科学论文和代码库、编写代码、解读结果、评估其他智能体的输出等。该系统已成功应用于一项博士级别的宇宙学任务（使用超新星数据测量宇宙学参数），并在两个基准测试集上表现出超越当前先进LLM的性能。源代码已在GitHub上提供，还有演示视频，该系统已部署在HuggingFace上，并将可在云端使用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了一个多智能体系统，用于自动化科研任务。</li>
<li>系统集成了约30个大型语言模型（LLM）。</li>
<li>采用规划和控制策略协调智能体的工作流程，无需人工参与。</li>
<li>智能体可完成不同任务，如检索、编写代码、解读结果、评估输出等。</li>
<li>成功应用于博士级别的宇宙学任务，测量宇宙学参数。</li>
<li>在两个基准测试集上表现出超越当前先进LLM的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07257">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ee18fe38d8501ef5bb0b058306af4a3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f2f31b594fbad129469e80447815f92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bb365510d5fd9e59e822406ce69b5d8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Mind-the-Memory-Gap-Unveiling-GPU-Bottlenecks-in-Large-Batch-LLM-Inference"><a href="#Mind-the-Memory-Gap-Unveiling-GPU-Bottlenecks-in-Large-Batch-LLM-Inference" class="headerlink" title="Mind the Memory Gap: Unveiling GPU Bottlenecks in Large-Batch LLM   Inference"></a>Mind the Memory Gap: Unveiling GPU Bottlenecks in Large-Batch LLM   Inference</h2><p><strong>Authors:Pol G. Recasens, Ferran Agullo, Yue Zhu, Chen Wang, Eun Kyung Lee, Olivier Tardieu, Jordi Torres, Josep Ll. Berral</strong></p>
<p>Large language models have been widely adopted across different tasks, but their auto-regressive generation nature often leads to inefficient resource utilization during inference. While batching is commonly used to increase throughput, performance gains plateau beyond a certain batch size, especially with smaller models, a phenomenon that existing literature typically explains as a shift to the compute-bound regime. In this paper, through an in-depth GPU-level analysis, we reveal that large-batch inference remains memory-bound, with most GPU compute capabilities underutilized due to DRAM bandwidth saturation as the primary bottleneck. To address this, we propose a Batching Configuration Advisor (BCA) that optimizes memory allocation, reducing GPU memory requirements with minimal impact on throughput. The freed memory and underutilized GPU compute capabilities can then be leveraged by concurrent workloads. Specifically, we use model replication to improve serving throughput and GPU utilization. Our findings challenge conventional assumptions about LLM inference, offering new insights and practical strategies for improving resource utilization, particularly for smaller language models. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/FerranAgulloLopez/vLLMBatchingMemoryGap">https://github.com/FerranAgulloLopez/vLLMBatchingMemoryGap</a>. </p>
<blockquote>
<p>大型语言模型已被广泛应用于各种任务，但其自回归生成特性往往导致推理过程中的资源利用率低下。虽然批处理通常用于提高吞吐量，但在达到一定批量大小后，性能提升会遇到瓶颈，尤其是对于较小的模型，现有文献通常将此现象解释为转向计算受限状态。在本文中，我们通过深入的GPU级别分析揭示，大批量推理仍然是内存受限的，由于DRAM带宽饱和是主要瓶颈，导致大多数GPU计算能力未得到充分利用。为了解决这一问题，我们提出了批处理配置顾问（BCA），通过优化内存分配，以最小限度地影响吞吐量的方式降低GPU内存要求。释放的内存和未充分利用的GPU计算能力可以被并发工作负载所利用。具体来说，我们使用模型复制来提高服务吞吐量和GPU利用率。我们的研究挑战了关于大型语言模型推理的常规假设，为改进资源利用提供了新见解和实际策略，特别是对于较小的语言模型。代码公开在<a target="_blank" rel="noopener" href="https://github.com/FerranAgulloLopez/vLLMBatchingMemoryGap%E3%80%82">https://github.com/FerranAgulloLopez/vLLMBatchingMemoryGap。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08311v2">PDF</a> Pol G. Recasens, Ferran Agullo: equal contribution. Paper accepted at   IEEE CLOUD 2025</p>
<p><strong>Summary</strong><br>大语言模型广泛应用于各种任务，但其自回归生成特性导致推理过程中的资源利用效率低下。批处理通常用于提高吞吐量，但超过一定批次大小后性能提升平台化，特别是对小模型而言。本文深入GPU层面分析，揭示大批量推理仍存在内存瓶颈问题，GPU计算能力大部分未得到充分利用，主要是由于DRAM带宽饱和所致。为解决这一问题，本文提出批处理配置顾问（BCA），优化内存分配，以减小对吞吐量的影响。释放的内存和未充分利用的GPU计算能力可用于并发工作负载。具体来说，通过模型复制提高服务吞吐量和GPU利用率。本文挑战了关于LLM推理的常规假设，为改进资源利用提供了新见解和实用策略，特别是对小语言模型而言。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大语言模型的自回归生成特性导致推理过程中的资源利用效率低下，特别是对小模型而言，性能提升在超过一定批次大小后趋于平稳。</li>
<li>通过深入GPU层面的分析，发现大批量推理存在内存瓶颈问题，大部分GPU计算能力未被充分利用，主要原因是DRAM带宽饱和。</li>
<li>提出批处理配置顾问（BCA）以优化内存分配，减小对吞吐量的影响，并通过释放的内存和未充分利用的GPU计算能力来提高并发工作负载的效率。</li>
<li>通过模型复制来提高服务吞吐量和GPU利用率是一种有效的策略。</li>
<li>本文挑战了关于LLM推理的常规假设，为改进资源利用提供了新的见解和实用策略。</li>
<li>公开可用的代码为研究人员和实践者提供了实现上述策略的工具和参考。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08311">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-741688a12360f89f9163800f4eed2660.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9aeb367cd9a953c47b4c57a47ef981c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aed7fb6d9674f034232fa89ba56b458c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ef5bbfc53ab7807864eb41bb9590ebe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81ef23aa771d99e8bbd22731b4e2ab40.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a841f41ef1031810e12150ed1fea05b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2569c29575533dc6eac667f204afaf6.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Weak-to-Strong-Jailbreaking-on-Large-Language-Models"><a href="#Weak-to-Strong-Jailbreaking-on-Large-Language-Models" class="headerlink" title="Weak-to-Strong Jailbreaking on Large Language Models"></a>Weak-to-Strong Jailbreaking on Large Language Models</h2><p><strong>Authors:Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, William Yang Wang</strong></p>
<p>Large language models (LLMs) are vulnerable to jailbreak attacks - resulting in harmful, unethical, or biased text generations. However, existing jailbreaking methods are computationally costly. In this paper, we propose the weak-to-strong jailbreaking attack, an efficient inference time attack for aligned LLMs to produce harmful text. Our key intuition is based on the observation that jailbroken and aligned models only differ in their initial decoding distributions. The weak-to-strong attack’s key technical insight is using two smaller models (a safe and an unsafe one) to adversarially modify a significantly larger safe model’s decoding probabilities. We evaluate the weak-to-strong attack on 5 diverse open-source LLMs from 3 organizations. The results show our method can increase the misalignment rate to over 99% on two datasets with just one forward pass per example. Our study exposes an urgent safety issue that needs to be addressed when aligning LLMs. As an initial attempt, we propose a defense strategy to protect against such attacks, but creating more advanced defenses remains challenging. The code for replicating the method is available at <a target="_blank" rel="noopener" href="https://github.com/XuandongZhao/weak-to-strong">https://github.com/XuandongZhao/weak-to-strong</a> </p>
<blockquote>
<p>大型语言模型（LLM）容易受到越狱攻击，导致生成有害、不道德或偏见的文本。然而，现有的越狱方法计算成本高昂。在本文中，我们提出了弱到强越狱攻击，这是一种高效的推理时间攻击，用于针对对齐的大型语言模型生成有害文本。我们的主要直觉是基于这样的观察：越狱和对齐的模型只在初始解码分布上有所不同。弱到强攻击的关键技术见解是使用两个较小的模型（一个安全模型和一个不安全模型）来对抗性地修改一个更大的安全模型的解码概率。我们在来自三个组织的五个开源大型语言模型上评估了弱到强攻击。结果表明，我们的方法可以在两个数据集上将错位率提高到超过99%，每个例子只需进行一次正向传递。我们的研究揭示了当对齐大型语言模型时，需要解决一个紧急的安全问题。作为一种初步尝试，我们提出了一种防御策略来防止此类攻击，但创建更先进的防御手段仍然是一个挑战。复制该方法的代码可在<a target="_blank" rel="noopener" href="https://github.com/XuandongZhao/weak-to-strong">https://github.com/XuandongZhao/weak-to-strong</a> 找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.17256v4">PDF</a> ICML 2025</p>
<p><strong>Summary</strong><br>大型语言模型（LLM）易受“越狱攻击”，产生有害、不道德或偏向的文本生成。现有越狱方法计算成本高。本文提出弱到强越狱攻击，这是一种高效的推理时间攻击，用于针对对齐的大型语言模型产生有害文本。我们的关键直觉是基于观察，即越狱和对齐模型之间的区别仅在于其初始解码分布。弱到强攻击的关键技术见解是使用两个较小的模型（一个安全模型和一个不安全模型）来对抗性地修改一个更大的安全模型的解码概率。我们在五个开源的大型语言模型上评估了弱到强攻击，来自三个组织。结果表明，我们的方法可以在两个数据集上将错位率提高到超过99%，每个示例只需进行一次前向传递。我们的研究揭示了一个紧急的安全问题，需要在对齐大型语言模型时解决。作为初步尝试，我们提出了一种防御策略来防止此类攻击，但创建更先进的防御仍然具有挑战性。可复制该方法的代码位于<a target="_blank" rel="noopener" href="https://github.com/XuandongZhao/weak-to-strong">https://github.com/XuandongZhao/weak-to-strong</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs易受名为“越狱攻击”的安全威胁，可导致生成有害、不道德或偏向文本。</li>
<li>现存的越狱计算方法普遍计算成本高。</li>
<li>本文提出了一种新的弱到强越狱攻击方法，能够高效地对大型语言模型进行有害文本生成。</li>
<li>弱到强攻击的核心在于利用两个较小模型来修改更大模型的解码概率。</li>
<li>在五个不同的大型语言模型上的实验表明，该方法能显著提高错位率至99%以上。</li>
<li>研究凸显了对齐大型语言模型时的紧急安全问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.17256">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d2a3750d1c31bd154d29ef184146d782.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f338bbab6820a6802371d02328f413c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9506ab26adfc6608d9078259eb5ab3fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55ba39c18bc457bb3f75da136820f581.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-15/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-15/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-15/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-da22ec7125b2e2c1673700753687782e.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-07-15  Introspection of Thought Helps AI Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-15/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1c388731392af5f3642be6b0ee85e3b6.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-07-15  One Token to Fool LLM-as-a-Judge
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
