<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-07-15  From Enhancement to Understanding Build a Generalized Bridge for   Low-light Vision via Semantically Consistent Unsupervised Fine-tuning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-82ff1d6a163085ec5a5be9fbd70e9c49.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    38 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-15-更新"><a href="#2025-07-15-更新" class="headerlink" title="2025-07-15 更新"></a>2025-07-15 更新</h1><h2 id="From-Enhancement-to-Understanding-Build-a-Generalized-Bridge-for-Low-light-Vision-via-Semantically-Consistent-Unsupervised-Fine-tuning"><a href="#From-Enhancement-to-Understanding-Build-a-Generalized-Bridge-for-Low-light-Vision-via-Semantically-Consistent-Unsupervised-Fine-tuning" class="headerlink" title="From Enhancement to Understanding: Build a Generalized Bridge for   Low-light Vision via Semantically Consistent Unsupervised Fine-tuning"></a>From Enhancement to Understanding: Build a Generalized Bridge for   Low-light Vision via Semantically Consistent Unsupervised Fine-tuning</h2><p><strong>Authors:Sen Wang, Shao Zeng, Tianjun Gu, Zhizhong Zhang, Ruixin Zhang, Shouhong Ding, Jingyun Zhang, Jun Wang, Xin Tan, Yuan Xie, Lizhuang Ma</strong></p>
<p>Low-level enhancement and high-level visual understanding in low-light vision have traditionally been treated separately. Low-light enhancement improves image quality for downstream tasks, but existing methods rely on physical or geometric priors, limiting generalization. Evaluation mainly focuses on visual quality rather than downstream performance. Low-light visual understanding, constrained by scarce labeled data, primarily uses task-specific domain adaptation, which lacks scalability. To address these challenges, we build a generalized bridge between low-light enhancement and low-light understanding, which we term Generalized Enhancement For Understanding (GEFU). This paradigm improves both generalization and scalability. To address the diverse causes of low-light degradation, we leverage pretrained generative diffusion models to optimize images, achieving zero-shot generalization performance. Building on this, we propose Semantically Consistent Unsupervised Fine-tuning (SCUF). Specifically, to overcome text prompt limitations, we introduce an illumination-aware image prompt to explicitly guide image generation and propose a cycle-attention adapter to maximize its semantic potential. To mitigate semantic degradation in unsupervised training, we propose caption and reflectance consistency to learn high-level semantics and image-level spatial semantics. Extensive experiments demonstrate that our proposed method outperforms current state-of-the-art methods in traditional image quality and GEFU tasks including classification, detection, and semantic segmentation. </p>
<blockquote>
<p>在低光环境下，低层次的图像增强和高层次的视觉理解传统上被视为两个独立处理的任务。低光增强旨在提高图像的下游任务性能，但现有的方法依赖于物理或几何先验知识，这限制了其泛化能力。评估主要集中在视觉质量上，而不是下游性能。由于标注数据的稀缺，低光视觉理解主要依赖于特定任务的域适应方法，这缺乏可扩展性。为了应对这些挑战，我们在低光增强和低光理解之间建立了一个通用的桥梁，我们称之为“广义增强理解（GEFU）”。这种范式提高了泛化能力和可扩展性。为了应对低光降质的多种原因，我们利用预训练的生成扩散模型来优化图像，实现零样本泛化性能。在此基础上，我们提出了语义一致的监督微调方法（SCUF）。具体来说，为了克服文本提示的局限性，我们引入了光照感知图像提示来明确指导图像生成，并提出了循环注意力适配器来最大化其语义潜力。为了缓解无监督训练中的语义退化问题，我们提出了标题和反射一致性来学习高级语义和图像级别的空间语义。大量实验表明，我们提出的方法在图像质量和GEFU任务（包括分类、检测和语义分割）上的表现均优于当前最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08380v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong>：</p>
<p>传统上，低光环境下的图像增强与视觉理解是分开处理的。低光增强旨在提高图像质量以便后续任务，但现有方法依赖于物理或几何先验，限制了其泛化能力。评价主要关注视觉质量而非后续任务性能。而低光视觉理解受限于稀缺的标签数据，主要使用任务特定的域适应方法，缺乏可扩展性。为应对这些挑战，我们建立了一座连接低光增强与理解的桥梁——我们称之为通用增强理解（GEFU）范式，以提高泛化能力和可扩展性。为解决低光降质的多种原因，我们利用预训练的生成扩散模型优化图像，实现零样本泛化性能。在此基础上，我们提出了语义一致的无监督微调（SCUF）。为克服文本提示的局限性，我们引入了光照感知图像提示来明确指导图像生成，并提出循环注意力适配器来最大化其语义潜力。为减轻无监督训练中的语义退化问题，我们提出标题和反射一致性来学习高级语义和图像级空间语义。实验证明，我们的方法在图像质量及包括分类、检测和语义分割的GEFU任务上均优于当前先进方法。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>传统上，低光环境下的图像增强与视觉理解是分开处理的，存在泛化性和数据瓶颈问题。</li>
<li>提出了一种新的范式——广义增强理解（GEFU），连接低光增强与理解，提高泛化性和可扩展性。</li>
<li>利用预训练的生成扩散模型优化图像，实现零样本泛化性能。</li>
<li>引入光照感知图像提示和循环注意力适配器，以克服文本提示的局限性并最大化语义潜力。</li>
<li>提出语义一致的无监督微调（SCUF）方法，结合标题和反射一致性，学习高级和图像级空间语义。</li>
<li>实验证明，该方法在图像质量和多种任务（包括分类、检测和语义分割）上均表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08380">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d66755bf1a20417f6fe22444d40d6129.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87a0a69081a82f074b445312202d6492.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f7f5bdc64817e7671deb1e79aa1b6f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f47ec606728d3f069d4004440204314.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdeee1d52ac7602094d3b01e24a5bdf3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5186d1450d99c1a4251a0b4886b74cb5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Single-Step-Latent-Diffusion-for-Underwater-Image-Restoration"><a href="#Single-Step-Latent-Diffusion-for-Underwater-Image-Restoration" class="headerlink" title="Single-Step Latent Diffusion for Underwater Image Restoration"></a>Single-Step Latent Diffusion for Underwater Image Restoration</h2><p><strong>Authors:Jiayi Wu, Tianfu Wang, Md Abu Bakr Siddique, Md Jahidul Islam, Cornelia Fermuller, Yiannis Aloimonos, Christopher A. Metzler</strong></p>
<p>Underwater image restoration algorithms seek to restore the color, contrast, and appearance of a scene that is imaged underwater. They are a critical tool in applications ranging from marine ecology and aquaculture to underwater construction and archaeology. While existing pixel-domain diffusion-based image restoration approaches are effective at restoring simple scenes with limited depth variation, they are computationally intensive and often generate unrealistic artifacts when applied to scenes with complex geometry and significant depth variation. In this work we overcome these limitations by combining a novel network architecture (SLURPP) with an accurate synthetic data generation pipeline. SLURPP combines pretrained latent diffusion models – which encode strong priors on the geometry and depth of scenes – with an explicit scene decomposition – which allows one to model and account for the effects of light attenuation and backscattering. To train SLURPP we design a physics-based underwater image synthesis pipeline that applies varied and realistic underwater degradation effects to existing terrestrial image datasets. This approach enables the generation of diverse training data with dense medium&#x2F;degradation annotations. We evaluate our method extensively on both synthetic and real-world benchmarks and demonstrate state-of-the-art performance. Notably, SLURPP is over 200X faster than existing diffusion-based methods while offering ~ 3 dB improvement in PSNR on synthetic benchmarks. It also offers compelling qualitative improvements on real-world data. Project website <a target="_blank" rel="noopener" href="https://tianfwang.github.io/slurpp/">https://tianfwang.github.io/slurpp/</a>. </p>
<blockquote>
<p>水下图像恢复算法旨在恢复水下场景的色调、对比度和外观。它们在海洋生态、水产养殖、水下建筑和考古等应用中都是关键工具。虽然现有的像素域扩散图像恢复方法在恢复深度变化有限的简单场景方面非常有效，但它们在处理具有复杂几何和显著深度变化的场景时计算量大，并且经常产生不现实的伪影。在这项工作中，我们通过结合新型网络架构（SLURPP）与精确合成数据生成管道来克服这些限制。SLURPP结合了预训练的潜在扩散模型（对场景的几何和深度具有强烈的先验信息）和明确的场景分解（允许对光的衰减和后向散射进行建模和补偿）。为了训练SLURPP，我们设计了一个基于物理的水下图像合成管道，该管道对现有的陆地图像数据集应用了多样化和现实的水下退化效果。这种方法能够生成具有密集介质&#x2F;退化注释的多样化训练数据。我们在合成和实际基准测试上对我们的方法进行了广泛评估，并展示了其卓越的性能。值得注意的是，SLURPP的速度是现有扩散方法的逾两百倍，同时在合成基准上提供约提高PSNR达约等于减少到百分之零点五虚比重的保真度的优势。它在实际数据上还拥有引人注目的定性改进。项目网站：[<a target="_blank" rel="noopener" href="https://tianfwang.github.io/slurpp/]">https://tianfwang.github.io/slurpp/]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07878v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了水下图像恢复算法的重要性及其应用场景，如海洋生态、水产养殖业、水下建设和考古等。针对现有像素域扩散型图像恢复方法在复杂场景和显著深度变化方面的局限性，本文提出了一种新型网络架构SLURPP和精确合成数据生成管道的结合。SLURPP结合了预训练的潜在扩散模型和对场景进行明确分解的方法，以模拟和解释光线衰减和背散射的影响。本文还设计了一个基于物理的水下图像合成管道来训练SLURPP，通过对现有地面图像数据集应用各种现实的水下退化效应，生成具有密集介质&#x2F;退化注释的多样化训练数据。在合成和真实世界基准测试上，该方法均表现出卓越的性能，尤其是速度比现有扩散方法快200倍以上，同时在合成基准测试上提高了约3dB的PSNR。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>水下图像恢复算法在多个领域有广泛应用，如海洋生态、水产养殖业等。</li>
<li>现有像素域扩散型图像恢复方法在复杂场景和显著深度变化方面存在局限性。</li>
<li>本文提出了新型网络架构SLURPP，结合了预训练的潜在扩散模型和场景明确分解方法。</li>
<li>SLURPP通过结合物理模型，能够模拟和解释光线衰减和背散射的影响。</li>
<li>设计了一个基于物理的水下图像合成管道来训练SLURPP，生成具有密集介质&#x2F;退化注释的多样化训练数据。</li>
<li>SLURPP在合成和真实世界基准测试上都表现出卓越性能。</li>
<li>SLURPP相比现有扩散方法，速度更快，同时在性能上也有显著提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07878">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a36277c62bb22635059e9df246bd8ba9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fb537cfb9da269838e9c391caa3be003.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-82ff1d6a163085ec5a5be9fbd70e9c49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad79e38ac4b2877f9ab97a8b30f02e3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-427702c23a50056833be6150f5bc0161.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbe6906a401e7d5b08ca87b21a47dac7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Vision-Language-Vision-Auto-Encoder-Scalable-Knowledge-Distillation-from-Diffusion-Models"><a href="#Vision-Language-Vision-Auto-Encoder-Scalable-Knowledge-Distillation-from-Diffusion-Models" class="headerlink" title="Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation   from Diffusion Models"></a>Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation   from Diffusion Models</h2><p><strong>Authors:Tiezheng Zhang, Yitong Li, Yu-cheng Chou, Jieneng Chen, Alan Yuille, Chen Wei, Junfei Xiao</strong></p>
<p>Building state-of-the-art Vision-Language Models (VLMs) with strong captioning capabilities typically necessitates training on billions of high-quality image-text pairs, requiring millions of GPU hours. This paper introduces the Vision-Language-Vision (VLV) auto-encoder framework, which strategically leverages key pretrained components: a vision encoder, the decoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large Language Model (LLM). Specifically, we establish an information bottleneck by regularizing the language representation space, achieved through freezing the pretrained T2I diffusion decoder. Our VLV pipeline effectively distills knowledge from the text-conditioned diffusion model using continuous embeddings, demonstrating comprehensive semantic understanding via high-quality reconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the intermediate language representations into detailed descriptions, we construct a state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o and Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and significantly reduces data requirements; by primarily utilizing single-modal images for training and maximizing the utility of existing pretrained models (image encoder, T2I diffusion model, and LLM), it circumvents the need for massive paired image-text datasets, keeping the total training expenditure under $1,000 USD. </p>
<blockquote>
<p>构建具有强大描述功能的最新视觉语言模型（VLMs）通常需要训练数十亿高质量图像文本对，并需要数百万GPU小时。本文介绍了视觉语言视觉（VLV）自动编码器框架，该框架战略性地利用了关键预训练组件：视觉编码器、文本到图像（T2I）扩散模型的解码器，以及随后的大型语言模型（LLM）。具体来说，我们通过正则化语言表示空间来建立信息瓶颈，这是通过冻结预训练的T2I扩散解码器来实现的。我们的VLV管道有效地从文本条件扩散模型中蒸馏知识，使用连续嵌入来展示高质量重建的综合语义理解。此外，我们通过微调预训练的大型语言模型来解码中间语言表示以生成详细描述，从而构建了一个与GPT-4o和Gemini 2.0 Flash等领先模型相当的最先进（SoTA）描述器。我们的方法展示了出色的成本效益，并大大降低了数据要求；它主要通过使用单模态图像进行训练并最大限度地利用现有预训练模型（图像编码器、T2I扩散模型和LLM），从而避免了需要大量配对图像文本数据集的需求，将总训练费用控制在1000美元以内。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07104v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://lambert-x.github.io/Vision-Language-Vision/">https://lambert-x.github.io/Vision-Language-Vision/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了Vision-Language-Vision（VLV）自编码器框架，通过战略性地利用关键预训练组件，如视觉编码器、文本到图像（T2I）扩散模型的解码器以及大型语言模型（LLM），实现了具有强大描述能力的视觉语言模型（VLM）。通过冻结预训练的T2I扩散解码器，建立信息瓶颈，并通过连续嵌入从文本条件扩散模型中蒸馏知识。该方法能够生成高质量重建，展现了全面的语义理解。通过微调预训练LLM以解码中间语言表示，构建了一个与GPT-4o和Gemini 2.0 Flash等领先模型相当的最先进（SoTA）描述器。该方法具有卓越的成本效益，显著降低了数据要求，主要利用单模态图像进行训练并最大化现有预训练模型（图像编码器、T2I扩散模型和LLM）的效用，无需大规模配对图像文本数据集，总培训费用低于1000美元。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了Vision-Language-Vision（VLV）自编码器框架，融合了视觉编码器和文本到图像扩散模型的解码器。</li>
<li>通过建立信息瓶颈和冻结预训练的T2I扩散解码器，实现了高效知识蒸馏。</li>
<li>利用连续嵌入生成高质量重建，展现了全面的语义理解。</li>
<li>通过微调预训练的大型语言模型（LLM），构建了先进的描述能力，与现有领先模型相竞争。</li>
<li>方法具有卓越的成本效益，显著降低了数据要求。</li>
<li>主要利用单模态图像进行训练，最大化现有预训练模型的效用。</li>
<li>该方法无需大规模配对图像文本数据集，总培训费用低于1000美元。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07104">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ffab7832c3e271fd26ed1923a97334bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57502e97dfe7bdfd13ab7190e87cff60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-faa9bb464ba0d9269a60a6e8256e9df5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Lighting-the-Night-with-Generative-Artificial-Intelligence"><a href="#Lighting-the-Night-with-Generative-Artificial-Intelligence" class="headerlink" title="Lighting the Night with Generative Artificial Intelligence"></a>Lighting the Night with Generative Artificial Intelligence</h2><p><strong>Authors:Tingting Zhou, Feng Zhang, Haoyang Fu, Baoxiang Pan, Renhe Zhang, Feng Lu, Zhixin Yang</strong></p>
<p>The visible light reflectance data from geostationary satellites is crucial for meteorological observations and plays an important role in weather monitoring and forecasting. However, due to the lack of visible light at night, it is impossible to conduct continuous all-day weather observations using visible light reflectance data. This study pioneers the use of generative diffusion models to address this limitation. Based on the multi-band thermal infrared brightness temperature data from the Advanced Geostationary Radiation Imager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we developed a high-precision visible light reflectance generative model, called Reflectance Diffusion (RefDiff), which enables 0.47<del>\mu\mathrm{m}, 0.65</del>\mu\mathrm{m}, and 0.825~\mu\mathrm{m} bands visible light reflectance generation at night. Compared to the classical models, RefDiff not only significantly improves accuracy through ensemble averaging but also provides uncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90, with particularly significant improvements in areas with complex cloud structures and thick clouds. The model’s nighttime generation capability was validated using VIIRS nighttime product, demonstrating comparable performance to its daytime counterpart. In summary, this research has made substantial progress in the ability to generate visible light reflectance at night, with the potential to expand the application of nighttime visible light data. </p>
<blockquote>
<p>地球静止卫星的可见光反射数据对于气象观测至关重要，并在天气监测和预报中扮演重要角色。然而，由于夜间缺乏可见光，无法连续进行全天候天气观测使用可见光反射数据。本研究率先使用生成式扩散模型来解决这一限制。基于风云四号B（FY4B）地球静止卫星上的高级地球静止辐射成像仪（AGRI）的多波段热红外亮度温度数据，我们开发了一种高精度可见光反射生成模型，名为Reflectance Diffusion（RefDiff），该模型能够在夜间生成0.47μm、0.65μm和0.825μm波段的可见光反射。与传统的模型相比，RefDiff不仅通过集合平均显著提高精度，还提供不确定性估计。具体来说，RefDiff的SSIM指数可达到0.90，在具有复杂云结构和厚云区域表现出特别显著的改进。该模型的夜间生成能力已使用VIIRS夜间产品进行了验证，表现出与日间产品相当的性能。总之，这项研究在夜间生成可见光反射方面取得了实质性进展，有可能扩大夜间可见光数据的应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22511v2">PDF</a> Title corrected (Lightning to Lighting); terminology updated   (retrieval to generative)</p>
<p><strong>Summary</strong></p>
<p>基于地球静止卫星的可见光反射数据对于气象观测至关重要，但夜间缺乏可见光导致无法全天候持续观测。本研究创新性地使用生成扩散模型来解决这一问题，利用风云四号B卫星上的高级地球静止辐射成像仪的多波段热红外亮度温度数据，开发出高精度可见光反射生成模型——Reflectance Diffusion（RefDiff）。该模型能够在夜间生成0.47μm、0.65μm和0.825μm波段的可见光反射，相比传统模型，不仅通过集成平均显著提高精度，还提供不确定性估计。RefDiff的SSIM指数可达0.90，在复杂云结构和厚云区域表现尤其显著。该模型夜间生成能力已使用VIIRS夜间产品验证，表现出与日间相当的性能。总之，该研究在夜间生成可见光反射方面取得显著进展，有望扩大夜间可见光数据的应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>可见光反射数据对于气象观测、天气监测和预报至关重要。</li>
<li>夜间缺乏可见光导致无法全天候进行天气观测。</li>
<li>研究使用生成扩散模型解决这一问题。</li>
<li>开发的高精度可见光反射生成模型RefDiff，基于多波段热红外亮度温度数据。</li>
<li>RefDiff能在夜间生成特定波段的可见光反射。</li>
<li>RefDiff相比传统模型有更高的精度和不确定性估计能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22511">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-71ea374c78137c6fd6e9c30404565f47.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Interpreting-Large-Text-to-Image-Diffusion-Models-with-Dictionary-Learning"><a href="#Interpreting-Large-Text-to-Image-Diffusion-Models-with-Dictionary-Learning" class="headerlink" title="Interpreting Large Text-to-Image Diffusion Models with Dictionary   Learning"></a>Interpreting Large Text-to-Image Diffusion Models with Dictionary   Learning</h2><p><strong>Authors:Stepan Shabalin, Ayush Panda, Dmitrii Kharlapenko, Abdur Raheem Ali, Yixiong Hao, Arthur Conmy</strong></p>
<p>Sparse autoencoders are a promising new approach for decomposing language model activations for interpretation and control. They have been applied successfully to vision transformer image encoders and to small-scale diffusion models. Inference-Time Decomposition of Activations (ITDA) is a recently proposed variant of dictionary learning that takes the dictionary to be a set of data points from the activation distribution and reconstructs them with gradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large text-to-image diffusion model, Flux 1, and consider the interpretability of embeddings of both by introducing a visual automated interpretation pipeline. We find that SAEs accurately reconstruct residual stream embeddings and beat MLP neurons on interpretability. We are able to use SAE features to steer image generation through activation addition. We find that ITDA has comparable interpretability to SAEs. </p>
<blockquote>
<p>稀疏自动编码器是一种很有前途的新方法，用于分解语言模型的激活，以进行解释和控制。它们已成功应用于视觉转换图像编码器和小规模扩散模型。推理时间活动分解（ITDA）是最近提出的词典学习变体，它将词典视为激活分布中的数据点集，并通过梯度追求进行重建。我们将稀疏自动编码器（SAE）和ITDA应用于大型文本到图像的扩散模型Flux 1，并通过引入视觉自动化解释管道来考虑两者的嵌入解释性。我们发现SAE能够准确重建残差流嵌入，在解释性方面优于MLP神经元。我们能够使用SAE特性通过激活添加来引导图像生成。我们发现ITDA的解释性与SAE相当。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24360v3">PDF</a> 10 pages, 10 figures, Mechanistic Interpretability for Vision at CVPR   2025</p>
<p><strong>Summary</strong></p>
<p>稀疏自编码器是一种新兴的有前途的方法，用于分解语言模型的激活以实现解释和控制。它已成功应用于视觉转换器图像编码器和小型扩散模型。我们将其应用于大型文本到图像扩散模型Flux 1，并通过引入视觉自动化解释管道来考虑两者的嵌入解释性。研究发现，稀疏自编码器能准确重建残差流嵌入，在解释性方面优于多层感知机的神经元。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>稀疏自编码器（SAE）是一种新兴方法，用于分解语言模型的激活，以提高解释性和控制性。</li>
<li>ITDA是字典学习的一种新变体，使用数据点重建激活分布。</li>
<li>SAEs能准确重建残差流嵌入，且在解释性方面优于MLP神经元。</li>
<li>ITDA在解释性与SAE特征上具有相当性。</li>
<li>SAE特征可用于通过激活添加来引导图像生成。</li>
<li>在大型文本到图像扩散模型Flux 1中应用SAE和ITDA，考虑了它们的嵌入解释性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24360">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b93b635e4da1d70e4311442bd3798d72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85d5cf7e137c8a9f731c89fabcf0dc3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00f53b550bf888b45775dd5f51c7753f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6bb641b17ba340ec54f8f88b3482f7b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f5f0fecdab31a42e9fc0ea9e9d89070.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5818ab8151185c72e477a07048e98941.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MedSegFactory-Text-Guided-Generation-of-Medical-Image-Mask-Pairs"><a href="#MedSegFactory-Text-Guided-Generation-of-Medical-Image-Mask-Pairs" class="headerlink" title="MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs"></a>MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs</h2><p><strong>Authors:Jiawei Mao, Yuhan Wang, Yucheng Tang, Daguang Xu, Kang Wang, Yang Yang, Zongwei Zhou, Yuyin Zhou</strong></p>
<p>This paper presents MedSegFactory, a versatile medical synthesis framework that generates high-quality paired medical images and segmentation masks across modalities and tasks. It aims to serve as an unlimited data repository, supplying image-mask pairs to enhance existing segmentation tools. The core of MedSegFactory is a dual-stream diffusion model, where one stream synthesizes medical images and the other generates corresponding segmentation masks. To ensure precise alignment between image-mask pairs, we introduce Joint Cross-Attention (JCA), enabling a collaborative denoising paradigm by dynamic cross-conditioning between streams. This bidirectional interaction allows both representations to guide each other’s generation, enhancing consistency between generated pairs. MedSegFactory unlocks on-demand generation of paired medical images and segmentation masks through user-defined prompts that specify the target labels, imaging modalities, anatomical regions, and pathological conditions, facilitating scalable and high-quality data generation. This new paradigm of medical image synthesis enables seamless integration into diverse medical imaging workflows, enhancing both efficiency and accuracy. Extensive experiments show that MedSegFactory generates data of superior quality and usability, achieving competitive or state-of-the-art performance in 2D and 3D segmentation tasks while addressing data scarcity and regulatory constraints. </p>
<blockquote>
<p>本文介绍了MedSegFactory，这是一个通用的医学合成框架，能够生成跨模态和任务的高质量配对医学图像和分割掩膜。它的目标是作为一个无限的数据仓库，提供图像-掩膜对，以增强现有的分割工具。MedSegFactory的核心是一个双流扩散模型，其中一个流用于合成医学图像，另一个流用于生成相应的分割掩膜。为了确保图像-掩膜对之间的精确对齐，我们引入了联合交叉注意（JCA）机制，通过流之间的动态交叉条件，实现协同去噪范式。这种双向交互允许两种表示形式相互引导生成，增强生成对之间的一致性。MedSegFactory通过用户定义的提示（指定目标标签、成像模式、解剖区域和病理状况）解锁按需生成的配对医学图像和分割掩膜，促进可扩展和高质量的数据生成。这种新的医学图像合成范式能够无缝地融入各种医学成像工作流程，提高效率和准确性。大量实验表明，MedSegFactory生成的数据具有卓越的质量和可用性，在二维和三维分割任务中达到了竞争或最先进的性能，同时解决了数据稀缺和监管约束的问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06897v2">PDF</a> 12 pages, 8 figures, The project page can be accessed via   <a target="_blank" rel="noopener" href="https://jwmao1.github.io/MedSegFactory_web">https://jwmao1.github.io/MedSegFactory_web</a></p>
<p><strong>Summary</strong></p>
<p>MedSegFactory是一个通用医疗合成框架，能生成跨模态和任务的配对高质量医疗图像和分割掩膜。它旨在作为一个无限的数据仓库，为现有的分割工具提供图像-掩膜对以增强其性能。该框架的核心是双重流扩散模型，一个流负责合成医疗图像，另一个流生成相应的分割掩膜。通过引入联合交叉注意（JCA）机制，确保图像-掩膜对之间的精确对齐。JCA实现了流之间的动态交叉条件，推动协同去噪模式。这种双向交互允许两个表示相互引导生成，增强生成配对的一致性。MedSegFactory通过用户定义提示实现按需生成配对医疗图像和分割掩膜，提示可指定目标标签、成像模式、解剖区域和病理状况等。这一全新的医疗图像合成模式可无缝融入多样化的医疗成像工作流程，提高效率和准确性。实验表明，MedSegFactory生成的数据质量高、实用性强，在二维和三维分割任务上表现优异，解决了数据稀缺和监管约束问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedSegFactory是一个医疗合成框架，能生成高质量配对医疗图像和分割掩膜。</li>
<li>它作为一个无限数据仓库，为现有分割工具提供图像-掩膜对。</li>
<li>双重流扩散模型是框架的核心，包括图像合成流和分割掩膜生成流。</li>
<li>引入的联合交叉注意（JCA）机制确保图像和掩膜之间的精确对齐。</li>
<li>用户可以通过定义提示（如目标标签、成像模式等）实现按需生成图像和掩膜。</li>
<li>MedSegFactory能提高医疗成像工作流程的效率和准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06897">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d5225258513b6c9f19326f069835bb49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5136966fc5c824d9fe3cd8414dda1962.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b47aacb690f87bcd65792c86ab8b4478.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a24d0c123339caebfbdb3e5848748b4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7aaa7fe5f944b330aa02b35a45dd3ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01f1cbbd073c3947c37438e5972d7cbf.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="X-Dancer-Expressive-Music-to-Human-Dance-Video-Generation"><a href="#X-Dancer-Expressive-Music-to-Human-Dance-Video-Generation" class="headerlink" title="X-Dancer: Expressive Music to Human Dance Video Generation"></a>X-Dancer: Expressive Music to Human Dance Video Generation</h2><p><strong>Authors:Zeyuan Chen, Hongyi Xu, Guoxian Song, You Xie, Chenxu Zhang, Xin Chen, Chao Wang, Di Chang, Linjie Luo</strong></p>
<p>We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image. As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize extended and music-synchronized token sequences for 2D body, head and hands poses, which then guide a diffusion model to produce coherent and realistic dance video frames. Unlike traditional methods that primarily generate human motion in 3D, X-Dancer addresses data limitations and enhances scalability by modeling a wide spectrum of 2D dance motions, capturing their nuanced alignment with musical beats through readily available monocular videos. To achieve this, we first build a spatially compositional token representation from 2D human pose labels associated with keypoint confidences, encoding both large articulated body movements (e.g., upper and lower body) and fine-grained motions (e.g., head and hands). We then design a music-to-motion transformer model that autoregressively generates music-aligned dance pose token sequences, incorporating global attention to both musical style and prior motion context. Finally we leverage a diffusion backbone to animate the reference image with these synthesized pose tokens through AdaIN, forming a fully differentiable end-to-end framework. Experimental results demonstrate that X-Dancer is able to produce both diverse and characterized dance videos, substantially outperforming state-of-the-art methods in term of diversity, expressiveness and realism. Code and model will be available for research purposes. </p>
<blockquote>
<p>我们介绍了X-Dancer，这是一种新型零样本音乐驱动图像动画管道，它可以从单个静态图像中创建多样且长程的逼真人类舞蹈视频。其核心是统一transformer扩散框架，它采用自回归transformer模型合成扩展的与音乐同步的令牌序列，用于二维身体、头部和手部姿势。然后这些序列引导扩散模型生成连贯且逼真的舞蹈视频帧。与传统的主要在3D中生成人类运动的方法不同，X-Dancer通过建模大量的二维舞蹈动作来解决数据限制并增强可扩展性，通过可获得的单目视频捕捉其与音乐节奏的微妙对齐。为实现这一点，我们首先根据与关键点置信度相关的二维人体姿势标签构建空间组合令牌表示，编码大型关节身体运动（例如上下身体）和精细运动（例如头部和手部）。然后我们设计了一个音乐到运动的transformer模型，该模型自回归地生成与音乐对齐的舞蹈姿势令牌序列，并融入全局注意力以关注音乐风格和先前的运动上下文。最后我们利用扩散主干通过这些合成的姿势令牌对参考图像进行动画处理，形成一个完全可微分的端到端框架。实验结果表明，X-Dancer能够产生多样且具特色的舞蹈视频，在多样性、表现力和逼真度方面大大优于现有先进技术的方法。代码和模型将用于研究目的。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17414v2">PDF</a> ICCV 2025. Project Page: <a target="_blank" rel="noopener" href="https://zeyuan-chen.com/X-Dancer/">https://zeyuan-chen.com/X-Dancer/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了X-Dancer，一种新颖的零样本音乐驱动图像动画管道，能够从单张静态图像创建多样且长程逼真的舞蹈视频。其核心是统一transformer-diffusion框架，包含一个自回归transformer模型，用于合成扩展且与音乐同步的token序列，用于指导扩散模型生成连贯且逼真的舞蹈视频帧。不同于传统方法主要在3D中生成人体运动，X-Dancer解决了数据限制并增强了可扩展性，通过建模丰富的2D舞蹈动作并捕捉其与音乐节奏的微妙对齐，通过可获得的单目视频实现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>X-Dancer是一种零样本音乐驱动图像动画管道，能从单张静态图像创建舞蹈视频。</li>
<li>它采用统一的transformer-diffusion框架，包含自回归transformer模型，用于合成扩展且与音乐同步的token序列。</li>
<li>X-Dancer通过建模丰富的2D舞蹈动作并捕捉其与音乐节奏的微妙对齐，实现了数据限制和可扩展性的增强。</li>
<li>它通过关联人体关键点置信度的空间组合token表示来编码大型关节运动和精细动作。</li>
<li>X-Dancer设计了一个音乐到运动的transformer模型，能够自回归地生成与音乐风格和历史运动上下文对齐的舞蹈姿势token序列。</li>
<li>通过利用扩散模型和后端动画技术，X-Dancer能够以合成的姿势token激活参考图像，形成一个完全可微分的端到端框架。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17414">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-469c57d225a45963a6723f05b6f17800.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab6217adb34d69ac043e42b1d9b8e289.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7db7895eb0de189c748235333aeb25bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e44c3e266824cefaeccf7207bd50d409.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FreeScale-Unleashing-the-Resolution-of-Diffusion-Models-via-Tuning-Free-Scale-Fusion"><a href="#FreeScale-Unleashing-the-Resolution-of-Diffusion-Models-via-Tuning-Free-Scale-Fusion" class="headerlink" title="FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free   Scale Fusion"></a>FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free   Scale Fusion</h2><p><strong>Authors:Haonan Qiu, Shiwei Zhang, Yujie Wei, Ruihang Chu, Hangjie Yuan, Xiang Wang, Yingya Zhang, Ziwei Liu</strong></p>
<p>Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. To tackle this challenge, we propose FreeScale, a tuning-free inference paradigm to enable higher-resolution visual generation via scale fusion. Specifically, FreeScale processes information from different receptive scales and then fuses it by extracting desired frequency components. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Notably, compared with previous best-performing methods, FreeScale unlocks the 8k-resolution text-to-image generation for the first time. </p>
<blockquote>
<p>视觉扩散模型取得了显著的进步，但由于缺乏高分辨率数据和计算资源的限制，它们通常只能在有限的分辨率上进行训练，这阻碍了它们在更高分辨率下生成高保真图像或视频的能力。近期的研究工作已经探索了无微调策略，以展示预训练模型未开发的高分辨率视觉生成潜力。然而，这些方法仍倾向于产生带有重复模式的低质量视觉内容。关键障碍在于模型生成超过其训练分辨率的视觉内容时不可避免的高频信息增加，这会导致由累积误差引起的不可取的重复模式。为了应对这一挑战，我们提出了FreeScale，这是一种无需微调推理范式，通过尺度融合实现高分辨率视觉生成。具体来说，FreeScale处理来自不同感受尺度的信息，然后通过提取所需的频率成分将其融合。大量实验验证了我们的范式在扩展图像和视频模型的分辨率视觉生成能力方面的优越性。值得注意的是，与以前表现最佳的方法相比，FreeScale首次实现了8k分辨率的文本到图像生成。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09626v2">PDF</a> ICCV 2025, Project Page:   <a target="_blank" rel="noopener" href="http://haonanqiu.com/projects/FreeScale.html">http://haonanqiu.com/projects/FreeScale.html</a>, Code Repo:   <a target="_blank" rel="noopener" href="https://github.com/ali-vilab/FreeScale">https://github.com/ali-vilab/FreeScale</a></p>
<p><strong>Summary</strong><br>     视觉扩散模型虽有所进展，但由于缺乏高分辨率数据和计算资源的限制，通常只在有限的分辨率上进行训练，这限制了其生成高分辨率图像或视频的能力。最近的研究尝试采用无微调策略来展示预训练模型在更高分辨率视觉生成方面的潜力，但这些方法仍易产生带有重复模式的低质量视觉内容。关键在于，当模型生成超过其训练分辨率的视觉内容时，高频信息的增加是不可避免的，这会导致由累积误差引起的重复模式。为解决这一挑战，我们提出了FreeScale，一种无需调参的推理范式，通过尺度融合实现更高分辨率的视觉生成。实验证明，我们的范式在图像和视频模型的更高分辨率视觉生成能力方面具有优越性。值得注意的是，与以前表现最佳的方法相比，FreeScale首次实现了8k分辨率的文本到图像生成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉扩散模型因缺乏高分辨率数据和计算资源限制而通常在有限分辨率下训练。</li>
<li>缺乏高分辨率训练限制了视觉扩散模型生成高质量图像和视频的能力。</li>
<li>现有研究尝试采用无微调策略来优化预训练模型在更高分辨率视觉生成方面的表现。</li>
<li>当前方法仍面临生成低质量视觉内容的问题，包含重复模式。</li>
<li>模型生成超过训练分辨率的视觉内容时，高频信息的增加会导致重复模式的问题。</li>
<li>提出的FreeScale方法通过尺度融合实现无需调参的更高分辨率视觉生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09626">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-af118821250c88abcbe8a04be24887f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9ddc7a42c21365b359274c4d31f8ad8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-edba077de551e21a399ddf924ebd6c3a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf392d9d0ae69566c4f61893f9c632eb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Amortized-Posterior-Sampling-with-Diffusion-Prior-Distillation"><a href="#Amortized-Posterior-Sampling-with-Diffusion-Prior-Distillation" class="headerlink" title="Amortized Posterior Sampling with Diffusion Prior Distillation"></a>Amortized Posterior Sampling with Diffusion Prior Distillation</h2><p><strong>Authors:Abbas Mammadov, Hyungjin Chung, Jong Chul Ye</strong></p>
<p>We propose Amortized Posterior Sampling (APS), a novel variational inference approach for efficient posterior sampling in inverse problems. Our method trains a conditional flow model to minimize the divergence between the variational distribution and the posterior distribution implicitly defined by the diffusion model. This results in a powerful, amortized sampler capable of generating diverse posterior samples with a single neural function evaluation, generalizing across various measurements. Unlike existing methods, our approach is unsupervised, requires no paired training data, and is applicable to both Euclidean and non-Euclidean domains. We demonstrate its effectiveness on a range of tasks, including image restoration, manifold signal reconstruction, and climate data imputation. APS significantly outperforms existing approaches in computational efficiency while maintaining competitive reconstruction quality, enabling real-time, high-quality solutions to inverse problems across diverse domains. </p>
<blockquote>
<p>我们提出了摊销后采样（Amortized Posterior Sampling，APS），这是一种针对反问题中有效后验采样的新型变分推断方法。我们的方法训练了一个条件流模型，以最小化变分分布与扩散模型隐式定义的后验分布之间的散度。这产生了一个强大的摊销采样器，能够在一次神经网络函数评估中生成多样化的后验样本，并可在各种测量中通用。与现有方法不同，我们的方法是无监督的，不需要配对训练数据，适用于欧几里得和非欧几里得领域。我们在图像恢复、流形信号重建和气候数据插补等一系列任务中展示了其有效性。APS在计算效率方面显著优于现有方法，同时保持有竞争力的重建质量，为不同领域的反问题提供实时高质量解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.17907v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>提出一种名为Amortized Posterior Sampling（APS）的新型变分推断方法，用于高效解决反问题中的后采样问题。通过训练条件流模型，最小化变分分布与扩散模型隐式定义的后验分布之间的分歧，从而创建一个强大的、一次性采样器，能够在单一神经网络函数评估中生成多样化的后验样本，并广泛应用于各种测量。该方法无需监督，无需配对训练数据，适用于欧几里得和非欧几里得领域。在图像恢复、流形信号重建和气候数据插补等任务上表现出色，在计算效率上显著优于现有方法，同时保持竞争力的重建质量，为不同领域的反问题提供实时高质量解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入Amortized Posterior Sampling（APS）方法，一种针对反问题中后采样的新型变分推断方法。</li>
<li>通过训练条件流模型，最小化变分分布与扩散模型后验分布之间的分歧。</li>
<li>APS方法创建一个强大的、一次性采样器，能够在单一神经网络函数评估中生成多样化后验样本。</li>
<li>APS方法具有通用性，可应用于各种测量、图像恢复、流形信号重建和气候数据插补等任务。</li>
<li>该方法无需监督，无需配对训练数据，适用于欧几里得和非欧几里得领域。</li>
<li>APS在计算效率上显著优于现有方法，同时保持竞争力的重建质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.17907">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1cd95b67ae60eaf394633e660e2bd7d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7206a1c059e5a0e9c2366da19d63da0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74b12f7c9f7dd85d8d4847cacef3cfab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdd6c230c35ee5e3a3fc72451aded3fc.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-15/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-15/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-15/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c047be575f3ba1e47cbc6be136cadd6a.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-07-15  Ensemble of Weak Spectral Total Variation Learners a PET-CT Case Study
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-15/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-71d52238ae8c73c5dfdd11b671f91485.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2025-07-15  RegGS Unposed Sparse Views Gaussian Splatting with 3DGS Registration
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
