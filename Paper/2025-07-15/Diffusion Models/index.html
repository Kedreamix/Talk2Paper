<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-15  From Enhancement to Understanding Build a Generalized Bridge for   Low-light Vision via Semantically Consistent Unsupervised Fine-tuning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-82ff1d6a163085ec5a5be9fbd70e9c49.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    38 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-15-æ›´æ–°"><a href="#2025-07-15-æ›´æ–°" class="headerlink" title="2025-07-15 æ›´æ–°"></a>2025-07-15 æ›´æ–°</h1><h2 id="From-Enhancement-to-Understanding-Build-a-Generalized-Bridge-for-Low-light-Vision-via-Semantically-Consistent-Unsupervised-Fine-tuning"><a href="#From-Enhancement-to-Understanding-Build-a-Generalized-Bridge-for-Low-light-Vision-via-Semantically-Consistent-Unsupervised-Fine-tuning" class="headerlink" title="From Enhancement to Understanding: Build a Generalized Bridge for   Low-light Vision via Semantically Consistent Unsupervised Fine-tuning"></a>From Enhancement to Understanding: Build a Generalized Bridge for   Low-light Vision via Semantically Consistent Unsupervised Fine-tuning</h2><p><strong>Authors:Sen Wang, Shao Zeng, Tianjun Gu, Zhizhong Zhang, Ruixin Zhang, Shouhong Ding, Jingyun Zhang, Jun Wang, Xin Tan, Yuan Xie, Lizhuang Ma</strong></p>
<p>Low-level enhancement and high-level visual understanding in low-light vision have traditionally been treated separately. Low-light enhancement improves image quality for downstream tasks, but existing methods rely on physical or geometric priors, limiting generalization. Evaluation mainly focuses on visual quality rather than downstream performance. Low-light visual understanding, constrained by scarce labeled data, primarily uses task-specific domain adaptation, which lacks scalability. To address these challenges, we build a generalized bridge between low-light enhancement and low-light understanding, which we term Generalized Enhancement For Understanding (GEFU). This paradigm improves both generalization and scalability. To address the diverse causes of low-light degradation, we leverage pretrained generative diffusion models to optimize images, achieving zero-shot generalization performance. Building on this, we propose Semantically Consistent Unsupervised Fine-tuning (SCUF). Specifically, to overcome text prompt limitations, we introduce an illumination-aware image prompt to explicitly guide image generation and propose a cycle-attention adapter to maximize its semantic potential. To mitigate semantic degradation in unsupervised training, we propose caption and reflectance consistency to learn high-level semantics and image-level spatial semantics. Extensive experiments demonstrate that our proposed method outperforms current state-of-the-art methods in traditional image quality and GEFU tasks including classification, detection, and semantic segmentation. </p>
<blockquote>
<p>åœ¨ä½å…‰ç¯å¢ƒä¸‹ï¼Œä½å±‚æ¬¡çš„å›¾åƒå¢å¼ºå’Œé«˜å±‚æ¬¡çš„è§†è§‰ç†è§£ä¼ ç»Ÿä¸Šè¢«è§†ä¸ºä¸¤ä¸ªç‹¬ç«‹å¤„ç†çš„ä»»åŠ¡ã€‚ä½å…‰å¢å¼ºæ—¨åœ¨æé«˜å›¾åƒçš„ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼Œä½†ç°æœ‰çš„æ–¹æ³•ä¾èµ–äºç‰©ç†æˆ–å‡ ä½•å…ˆéªŒçŸ¥è¯†ï¼Œè¿™é™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨è§†è§‰è´¨é‡ä¸Šï¼Œè€Œä¸æ˜¯ä¸‹æ¸¸æ€§èƒ½ã€‚ç”±äºæ ‡æ³¨æ•°æ®çš„ç¨€ç¼ºï¼Œä½å…‰è§†è§‰ç†è§£ä¸»è¦ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„åŸŸé€‚åº”æ–¹æ³•ï¼Œè¿™ç¼ºä¹å¯æ‰©å±•æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åœ¨ä½å…‰å¢å¼ºå’Œä½å…‰ç†è§£ä¹‹é—´å»ºç«‹äº†ä¸€ä¸ªé€šç”¨çš„æ¡¥æ¢ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œå¹¿ä¹‰å¢å¼ºç†è§£ï¼ˆGEFUï¼‰â€ã€‚è¿™ç§èŒƒå¼æé«˜äº†æ³›åŒ–èƒ½åŠ›å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºäº†åº”å¯¹ä½å…‰é™è´¨çš„å¤šç§åŸå› ï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹æ¥ä¼˜åŒ–å›¾åƒï¼Œå®ç°é›¶æ ·æœ¬æ³›åŒ–æ€§èƒ½ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†è¯­ä¹‰ä¸€è‡´çš„ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼ˆSCUFï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†å…‹æœæ–‡æœ¬æç¤ºçš„å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…‰ç…§æ„ŸçŸ¥å›¾åƒæç¤ºæ¥æ˜ç¡®æŒ‡å¯¼å›¾åƒç”Ÿæˆï¼Œå¹¶æå‡ºäº†å¾ªç¯æ³¨æ„åŠ›é€‚é…å™¨æ¥æœ€å¤§åŒ–å…¶è¯­ä¹‰æ½œåŠ›ã€‚ä¸ºäº†ç¼“è§£æ— ç›‘ç£è®­ç»ƒä¸­çš„è¯­ä¹‰é€€åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ ‡é¢˜å’Œåå°„ä¸€è‡´æ€§æ¥å­¦ä¹ é«˜çº§è¯­ä¹‰å’Œå›¾åƒçº§åˆ«çš„ç©ºé—´è¯­ä¹‰ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å›¾åƒè´¨é‡å’ŒGEFUä»»åŠ¡ï¼ˆåŒ…æ‹¬åˆ†ç±»ã€æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ï¼‰ä¸Šçš„è¡¨ç°å‡ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08380v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>ä¼ ç»Ÿä¸Šï¼Œä½å…‰ç¯å¢ƒä¸‹çš„å›¾åƒå¢å¼ºä¸è§†è§‰ç†è§£æ˜¯åˆ†å¼€å¤„ç†çš„ã€‚ä½å…‰å¢å¼ºæ—¨åœ¨æé«˜å›¾åƒè´¨é‡ä»¥ä¾¿åç»­ä»»åŠ¡ï¼Œä½†ç°æœ‰æ–¹æ³•ä¾èµ–äºç‰©ç†æˆ–å‡ ä½•å…ˆéªŒï¼Œé™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚è¯„ä»·ä¸»è¦å…³æ³¨è§†è§‰è´¨é‡è€Œéåç»­ä»»åŠ¡æ€§èƒ½ã€‚è€Œä½å…‰è§†è§‰ç†è§£å—é™äºç¨€ç¼ºçš„æ ‡ç­¾æ•°æ®ï¼Œä¸»è¦ä½¿ç”¨ä»»åŠ¡ç‰¹å®šçš„åŸŸé€‚åº”æ–¹æ³•ï¼Œç¼ºä¹å¯æ‰©å±•æ€§ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€åº§è¿æ¥ä½å…‰å¢å¼ºä¸ç†è§£çš„æ¡¥æ¢â€”â€”æˆ‘ä»¬ç§°ä¹‹ä¸ºé€šç”¨å¢å¼ºç†è§£ï¼ˆGEFUï¼‰èŒƒå¼ï¼Œä»¥æé«˜æ³›åŒ–èƒ½åŠ›å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºè§£å†³ä½å…‰é™è´¨çš„å¤šç§åŸå› ï¼Œæˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹ä¼˜åŒ–å›¾åƒï¼Œå®ç°é›¶æ ·æœ¬æ³›åŒ–æ€§èƒ½ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†è¯­ä¹‰ä¸€è‡´çš„æ— ç›‘ç£å¾®è°ƒï¼ˆSCUFï¼‰ã€‚ä¸ºå…‹æœæ–‡æœ¬æç¤ºçš„å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…‰ç…§æ„ŸçŸ¥å›¾åƒæç¤ºæ¥æ˜ç¡®æŒ‡å¯¼å›¾åƒç”Ÿæˆï¼Œå¹¶æå‡ºå¾ªç¯æ³¨æ„åŠ›é€‚é…å™¨æ¥æœ€å¤§åŒ–å…¶è¯­ä¹‰æ½œåŠ›ã€‚ä¸ºå‡è½»æ— ç›‘ç£è®­ç»ƒä¸­çš„è¯­ä¹‰é€€åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºæ ‡é¢˜å’Œåå°„ä¸€è‡´æ€§æ¥å­¦ä¹ é«˜çº§è¯­ä¹‰å’Œå›¾åƒçº§ç©ºé—´è¯­ä¹‰ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒè´¨é‡åŠåŒ…æ‹¬åˆ†ç±»ã€æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²çš„GEFUä»»åŠ¡ä¸Šå‡ä¼˜äºå½“å‰å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ä¼ ç»Ÿä¸Šï¼Œä½å…‰ç¯å¢ƒä¸‹çš„å›¾åƒå¢å¼ºä¸è§†è§‰ç†è§£æ˜¯åˆ†å¼€å¤„ç†çš„ï¼Œå­˜åœ¨æ³›åŒ–æ€§å’Œæ•°æ®ç“¶é¢ˆé—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„èŒƒå¼â€”â€”å¹¿ä¹‰å¢å¼ºç†è§£ï¼ˆGEFUï¼‰ï¼Œè¿æ¥ä½å…‰å¢å¼ºä¸ç†è§£ï¼Œæé«˜æ³›åŒ–æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹ä¼˜åŒ–å›¾åƒï¼Œå®ç°é›¶æ ·æœ¬æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>å¼•å…¥å…‰ç…§æ„ŸçŸ¥å›¾åƒæç¤ºå’Œå¾ªç¯æ³¨æ„åŠ›é€‚é…å™¨ï¼Œä»¥å…‹æœæ–‡æœ¬æç¤ºçš„å±€é™æ€§å¹¶æœ€å¤§åŒ–è¯­ä¹‰æ½œåŠ›ã€‚</li>
<li>æå‡ºè¯­ä¹‰ä¸€è‡´çš„æ— ç›‘ç£å¾®è°ƒï¼ˆSCUFï¼‰æ–¹æ³•ï¼Œç»“åˆæ ‡é¢˜å’Œåå°„ä¸€è‡´æ€§ï¼Œå­¦ä¹ é«˜çº§å’Œå›¾åƒçº§ç©ºé—´è¯­ä¹‰ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒè´¨é‡å’Œå¤šç§ä»»åŠ¡ï¼ˆåŒ…æ‹¬åˆ†ç±»ã€æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²ï¼‰ä¸Šå‡è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08380">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d66755bf1a20417f6fe22444d40d6129.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87a0a69081a82f074b445312202d6492.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f7f5bdc64817e7671deb1e79aa1b6f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f47ec606728d3f069d4004440204314.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fdeee1d52ac7602094d3b01e24a5bdf3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5186d1450d99c1a4251a0b4886b74cb5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Single-Step-Latent-Diffusion-for-Underwater-Image-Restoration"><a href="#Single-Step-Latent-Diffusion-for-Underwater-Image-Restoration" class="headerlink" title="Single-Step Latent Diffusion for Underwater Image Restoration"></a>Single-Step Latent Diffusion for Underwater Image Restoration</h2><p><strong>Authors:Jiayi Wu, Tianfu Wang, Md Abu Bakr Siddique, Md Jahidul Islam, Cornelia Fermuller, Yiannis Aloimonos, Christopher A. Metzler</strong></p>
<p>Underwater image restoration algorithms seek to restore the color, contrast, and appearance of a scene that is imaged underwater. They are a critical tool in applications ranging from marine ecology and aquaculture to underwater construction and archaeology. While existing pixel-domain diffusion-based image restoration approaches are effective at restoring simple scenes with limited depth variation, they are computationally intensive and often generate unrealistic artifacts when applied to scenes with complex geometry and significant depth variation. In this work we overcome these limitations by combining a novel network architecture (SLURPP) with an accurate synthetic data generation pipeline. SLURPP combines pretrained latent diffusion models â€“ which encode strong priors on the geometry and depth of scenes â€“ with an explicit scene decomposition â€“ which allows one to model and account for the effects of light attenuation and backscattering. To train SLURPP we design a physics-based underwater image synthesis pipeline that applies varied and realistic underwater degradation effects to existing terrestrial image datasets. This approach enables the generation of diverse training data with dense medium&#x2F;degradation annotations. We evaluate our method extensively on both synthetic and real-world benchmarks and demonstrate state-of-the-art performance. Notably, SLURPP is over 200X faster than existing diffusion-based methods while offering ~ 3 dB improvement in PSNR on synthetic benchmarks. It also offers compelling qualitative improvements on real-world data. Project website <a target="_blank" rel="noopener" href="https://tianfwang.github.io/slurpp/">https://tianfwang.github.io/slurpp/</a>. </p>
<blockquote>
<p>æ°´ä¸‹å›¾åƒæ¢å¤ç®—æ³•æ—¨åœ¨æ¢å¤æ°´ä¸‹åœºæ™¯çš„è‰²è°ƒã€å¯¹æ¯”åº¦å’Œå¤–è§‚ã€‚å®ƒä»¬åœ¨æµ·æ´‹ç”Ÿæ€ã€æ°´äº§å…»æ®–ã€æ°´ä¸‹å»ºç­‘å’Œè€ƒå¤ç­‰åº”ç”¨ä¸­éƒ½æ˜¯å…³é”®å·¥å…·ã€‚è™½ç„¶ç°æœ‰çš„åƒç´ åŸŸæ‰©æ•£å›¾åƒæ¢å¤æ–¹æ³•åœ¨æ¢å¤æ·±åº¦å˜åŒ–æœ‰é™çš„ç®€å•åœºæ™¯æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†å®ƒä»¬åœ¨å¤„ç†å…·æœ‰å¤æ‚å‡ ä½•å’Œæ˜¾è‘—æ·±åº¦å˜åŒ–çš„åœºæ™¯æ—¶è®¡ç®—é‡å¤§ï¼Œå¹¶ä¸”ç»å¸¸äº§ç”Ÿä¸ç°å®çš„ä¼ªå½±ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ç»“åˆæ–°å‹ç½‘ç»œæ¶æ„ï¼ˆSLURPPï¼‰ä¸ç²¾ç¡®åˆæˆæ•°æ®ç”Ÿæˆç®¡é“æ¥å…‹æœè¿™äº›é™åˆ¶ã€‚SLURPPç»“åˆäº†é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆå¯¹åœºæ™¯çš„å‡ ä½•å’Œæ·±åº¦å…·æœ‰å¼ºçƒˆçš„å…ˆéªŒä¿¡æ¯ï¼‰å’Œæ˜ç¡®çš„åœºæ™¯åˆ†è§£ï¼ˆå…è®¸å¯¹å…‰çš„è¡°å‡å’Œåå‘æ•£å°„è¿›è¡Œå»ºæ¨¡å’Œè¡¥å¿ï¼‰ã€‚ä¸ºäº†è®­ç»ƒSLURPPï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºç‰©ç†çš„æ°´ä¸‹å›¾åƒåˆæˆç®¡é“ï¼Œè¯¥ç®¡é“å¯¹ç°æœ‰çš„é™†åœ°å›¾åƒæ•°æ®é›†åº”ç”¨äº†å¤šæ ·åŒ–å’Œç°å®çš„æ°´ä¸‹é€€åŒ–æ•ˆæœã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆå…·æœ‰å¯†é›†ä»‹è´¨&#x2F;é€€åŒ–æ³¨é‡Šçš„å¤šæ ·åŒ–è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬åœ¨åˆæˆå’Œå®é™…åŸºå‡†æµ‹è¯•ä¸Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œå¹¶å±•ç¤ºäº†å…¶å“è¶Šçš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSLURPPçš„é€Ÿåº¦æ˜¯ç°æœ‰æ‰©æ•£æ–¹æ³•çš„é€¾ä¸¤ç™¾å€ï¼ŒåŒæ—¶åœ¨åˆæˆåŸºå‡†ä¸Šæä¾›çº¦æé«˜PSNRè¾¾çº¦ç­‰äºå‡å°‘åˆ°ç™¾åˆ†ä¹‹é›¶ç‚¹äº”è™šæ¯”é‡çš„ä¿çœŸåº¦çš„ä¼˜åŠ¿ã€‚å®ƒåœ¨å®é™…æ•°æ®ä¸Šè¿˜æ‹¥æœ‰å¼•äººæ³¨ç›®çš„å®šæ€§æ”¹è¿›ã€‚é¡¹ç›®ç½‘ç«™ï¼š[<a target="_blank" rel="noopener" href="https://tianfwang.github.io/slurpp/]">https://tianfwang.github.io/slurpp/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07878v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†æ°´ä¸‹å›¾åƒæ¢å¤ç®—æ³•çš„é‡è¦æ€§åŠå…¶åº”ç”¨åœºæ™¯ï¼Œå¦‚æµ·æ´‹ç”Ÿæ€ã€æ°´äº§å…»æ®–ä¸šã€æ°´ä¸‹å»ºè®¾å’Œè€ƒå¤ç­‰ã€‚é’ˆå¯¹ç°æœ‰åƒç´ åŸŸæ‰©æ•£å‹å›¾åƒæ¢å¤æ–¹æ³•åœ¨å¤æ‚åœºæ™¯å’Œæ˜¾è‘—æ·±åº¦å˜åŒ–æ–¹é¢çš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ç½‘ç»œæ¶æ„SLURPPå’Œç²¾ç¡®åˆæˆæ•°æ®ç”Ÿæˆç®¡é“çš„ç»“åˆã€‚SLURPPç»“åˆäº†é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œå¯¹åœºæ™¯è¿›è¡Œæ˜ç¡®åˆ†è§£çš„æ–¹æ³•ï¼Œä»¥æ¨¡æ‹Ÿå’Œè§£é‡Šå…‰çº¿è¡°å‡å’ŒèƒŒæ•£å°„çš„å½±å“ã€‚æœ¬æ–‡è¿˜è®¾è®¡äº†ä¸€ä¸ªåŸºäºç‰©ç†çš„æ°´ä¸‹å›¾åƒåˆæˆç®¡é“æ¥è®­ç»ƒSLURPPï¼Œé€šè¿‡å¯¹ç°æœ‰åœ°é¢å›¾åƒæ•°æ®é›†åº”ç”¨å„ç§ç°å®çš„æ°´ä¸‹é€€åŒ–æ•ˆåº”ï¼Œç”Ÿæˆå…·æœ‰å¯†é›†ä»‹è´¨&#x2F;é€€åŒ–æ³¨é‡Šçš„å¤šæ ·åŒ–è®­ç»ƒæ•°æ®ã€‚åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šï¼Œè¯¥æ–¹æ³•å‡è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯é€Ÿåº¦æ¯”ç°æœ‰æ‰©æ•£æ–¹æ³•å¿«200å€ä»¥ä¸Šï¼ŒåŒæ—¶åœ¨åˆæˆåŸºå‡†æµ‹è¯•ä¸Šæé«˜äº†çº¦3dBçš„PSNRã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°´ä¸‹å›¾åƒæ¢å¤ç®—æ³•åœ¨å¤šä¸ªé¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ï¼Œå¦‚æµ·æ´‹ç”Ÿæ€ã€æ°´äº§å…»æ®–ä¸šç­‰ã€‚</li>
<li>ç°æœ‰åƒç´ åŸŸæ‰©æ•£å‹å›¾åƒæ¢å¤æ–¹æ³•åœ¨å¤æ‚åœºæ™¯å’Œæ˜¾è‘—æ·±åº¦å˜åŒ–æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†æ–°å‹ç½‘ç»œæ¶æ„SLURPPï¼Œç»“åˆäº†é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹å’Œåœºæ™¯æ˜ç¡®åˆ†è§£æ–¹æ³•ã€‚</li>
<li>SLURPPé€šè¿‡ç»“åˆç‰©ç†æ¨¡å‹ï¼Œèƒ½å¤Ÿæ¨¡æ‹Ÿå’Œè§£é‡Šå…‰çº¿è¡°å‡å’ŒèƒŒæ•£å°„çš„å½±å“ã€‚</li>
<li>è®¾è®¡äº†ä¸€ä¸ªåŸºäºç‰©ç†çš„æ°´ä¸‹å›¾åƒåˆæˆç®¡é“æ¥è®­ç»ƒSLURPPï¼Œç”Ÿæˆå…·æœ‰å¯†é›†ä»‹è´¨&#x2F;é€€åŒ–æ³¨é‡Šçš„å¤šæ ·åŒ–è®­ç»ƒæ•°æ®ã€‚</li>
<li>SLURPPåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šéƒ½è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>SLURPPç›¸æ¯”ç°æœ‰æ‰©æ•£æ–¹æ³•ï¼Œé€Ÿåº¦æ›´å¿«ï¼ŒåŒæ—¶åœ¨æ€§èƒ½ä¸Šä¹Ÿæœ‰æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07878">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a36277c62bb22635059e9df246bd8ba9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fb537cfb9da269838e9c391caa3be003.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-82ff1d6a163085ec5a5be9fbd70e9c49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad79e38ac4b2877f9ab97a8b30f02e3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-427702c23a50056833be6150f5bc0161.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbe6906a401e7d5b08ca87b21a47dac7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Vision-Language-Vision-Auto-Encoder-Scalable-Knowledge-Distillation-from-Diffusion-Models"><a href="#Vision-Language-Vision-Auto-Encoder-Scalable-Knowledge-Distillation-from-Diffusion-Models" class="headerlink" title="Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation   from Diffusion Models"></a>Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation   from Diffusion Models</h2><p><strong>Authors:Tiezheng Zhang, Yitong Li, Yu-cheng Chou, Jieneng Chen, Alan Yuille, Chen Wei, Junfei Xiao</strong></p>
<p>Building state-of-the-art Vision-Language Models (VLMs) with strong captioning capabilities typically necessitates training on billions of high-quality image-text pairs, requiring millions of GPU hours. This paper introduces the Vision-Language-Vision (VLV) auto-encoder framework, which strategically leverages key pretrained components: a vision encoder, the decoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large Language Model (LLM). Specifically, we establish an information bottleneck by regularizing the language representation space, achieved through freezing the pretrained T2I diffusion decoder. Our VLV pipeline effectively distills knowledge from the text-conditioned diffusion model using continuous embeddings, demonstrating comprehensive semantic understanding via high-quality reconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the intermediate language representations into detailed descriptions, we construct a state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o and Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and significantly reduces data requirements; by primarily utilizing single-modal images for training and maximizing the utility of existing pretrained models (image encoder, T2I diffusion model, and LLM), it circumvents the need for massive paired image-text datasets, keeping the total training expenditure under $1,000 USD. </p>
<blockquote>
<p>æ„å»ºå…·æœ‰å¼ºå¤§æè¿°åŠŸèƒ½çš„æœ€æ–°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šå¸¸éœ€è¦è®­ç»ƒæ•°åäº¿é«˜è´¨é‡å›¾åƒæ–‡æœ¬å¯¹ï¼Œå¹¶éœ€è¦æ•°ç™¾ä¸‡GPUå°æ—¶ã€‚æœ¬æ–‡ä»‹ç»äº†è§†è§‰è¯­è¨€è§†è§‰ï¼ˆVLVï¼‰è‡ªåŠ¨ç¼–ç å™¨æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æˆ˜ç•¥æ€§åœ°åˆ©ç”¨äº†å…³é”®é¢„è®­ç»ƒç»„ä»¶ï¼šè§†è§‰ç¼–ç å™¨ã€æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„è§£ç å™¨ï¼Œä»¥åŠéšåçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡æ­£åˆ™åŒ–è¯­è¨€è¡¨ç¤ºç©ºé—´æ¥å»ºç«‹ä¿¡æ¯ç“¶é¢ˆï¼Œè¿™æ˜¯é€šè¿‡å†»ç»“é¢„è®­ç»ƒçš„T2Iæ‰©æ•£è§£ç å™¨æ¥å®ç°çš„ã€‚æˆ‘ä»¬çš„VLVç®¡é“æœ‰æ•ˆåœ°ä»æ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸­è’¸é¦çŸ¥è¯†ï¼Œä½¿ç”¨è¿ç»­åµŒå…¥æ¥å±•ç¤ºé«˜è´¨é‡é‡å»ºçš„ç»¼åˆè¯­ä¹‰ç†è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥è§£ç ä¸­é—´è¯­è¨€è¡¨ç¤ºä»¥ç”Ÿæˆè¯¦ç»†æè¿°ï¼Œä»è€Œæ„å»ºäº†ä¸€ä¸ªä¸GPT-4oå’ŒGemini 2.0 Flashç­‰é¢†å…ˆæ¨¡å‹ç›¸å½“çš„æœ€å…ˆè¿›ï¼ˆSoTAï¼‰æè¿°å™¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•å±•ç¤ºäº†å‡ºè‰²çš„æˆæœ¬æ•ˆç›Šï¼Œå¹¶å¤§å¤§é™ä½äº†æ•°æ®è¦æ±‚ï¼›å®ƒä¸»è¦é€šè¿‡ä½¿ç”¨å•æ¨¡æ€å›¾åƒè¿›è¡Œè®­ç»ƒå¹¶æœ€å¤§é™åº¦åœ°åˆ©ç”¨ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå›¾åƒç¼–ç å™¨ã€T2Iæ‰©æ•£æ¨¡å‹å’ŒLLMï¼‰ï¼Œä»è€Œé¿å…äº†éœ€è¦å¤§é‡é…å¯¹å›¾åƒæ–‡æœ¬æ•°æ®é›†çš„éœ€æ±‚ï¼Œå°†æ€»è®­ç»ƒè´¹ç”¨æ§åˆ¶åœ¨1000ç¾å…ƒä»¥å†…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07104v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://lambert-x.github.io/Vision-Language-Vision/">https://lambert-x.github.io/Vision-Language-Vision/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Vision-Language-Visionï¼ˆVLVï¼‰è‡ªç¼–ç å™¨æ¡†æ¶ï¼Œé€šè¿‡æˆ˜ç•¥æ€§åœ°åˆ©ç”¨å…³é”®é¢„è®­ç»ƒç»„ä»¶ï¼Œå¦‚è§†è§‰ç¼–ç å™¨ã€æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„è§£ç å™¨ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå®ç°äº†å…·æœ‰å¼ºå¤§æè¿°èƒ½åŠ›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚é€šè¿‡å†»ç»“é¢„è®­ç»ƒçš„T2Iæ‰©æ•£è§£ç å™¨ï¼Œå»ºç«‹ä¿¡æ¯ç“¶é¢ˆï¼Œå¹¶é€šè¿‡è¿ç»­åµŒå…¥ä»æ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹ä¸­è’¸é¦çŸ¥è¯†ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡é‡å»ºï¼Œå±•ç°äº†å…¨é¢çš„è¯­ä¹‰ç†è§£ã€‚é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒLLMä»¥è§£ç ä¸­é—´è¯­è¨€è¡¨ç¤ºï¼Œæ„å»ºäº†ä¸€ä¸ªä¸GPT-4oå’ŒGemini 2.0 Flashç­‰é¢†å…ˆæ¨¡å‹ç›¸å½“çš„æœ€å…ˆè¿›ï¼ˆSoTAï¼‰æè¿°å™¨ã€‚è¯¥æ–¹æ³•å…·æœ‰å“è¶Šçš„æˆæœ¬æ•ˆç›Šï¼Œæ˜¾è‘—é™ä½äº†æ•°æ®è¦æ±‚ï¼Œä¸»è¦åˆ©ç”¨å•æ¨¡æ€å›¾åƒè¿›è¡Œè®­ç»ƒå¹¶æœ€å¤§åŒ–ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå›¾åƒç¼–ç å™¨ã€T2Iæ‰©æ•£æ¨¡å‹å’ŒLLMï¼‰çš„æ•ˆç”¨ï¼Œæ— éœ€å¤§è§„æ¨¡é…å¯¹å›¾åƒæ–‡æœ¬æ•°æ®é›†ï¼Œæ€»åŸ¹è®­è´¹ç”¨ä½äº1000ç¾å…ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†Vision-Language-Visionï¼ˆVLVï¼‰è‡ªç¼–ç å™¨æ¡†æ¶ï¼Œèåˆäº†è§†è§‰ç¼–ç å™¨å’Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è§£ç å™¨ã€‚</li>
<li>é€šè¿‡å»ºç«‹ä¿¡æ¯ç“¶é¢ˆå’Œå†»ç»“é¢„è®­ç»ƒçš„T2Iæ‰©æ•£è§£ç å™¨ï¼Œå®ç°äº†é«˜æ•ˆçŸ¥è¯†è’¸é¦ã€‚</li>
<li>åˆ©ç”¨è¿ç»­åµŒå…¥ç”Ÿæˆé«˜è´¨é‡é‡å»ºï¼Œå±•ç°äº†å…¨é¢çš„è¯­ä¹‰ç†è§£ã€‚</li>
<li>é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ„å»ºäº†å…ˆè¿›çš„æè¿°èƒ½åŠ›ï¼Œä¸ç°æœ‰é¢†å…ˆæ¨¡å‹ç›¸ç«äº‰ã€‚</li>
<li>æ–¹æ³•å…·æœ‰å“è¶Šçš„æˆæœ¬æ•ˆç›Šï¼Œæ˜¾è‘—é™ä½äº†æ•°æ®è¦æ±‚ã€‚</li>
<li>ä¸»è¦åˆ©ç”¨å•æ¨¡æ€å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œæœ€å¤§åŒ–ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹çš„æ•ˆç”¨ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€å¤§è§„æ¨¡é…å¯¹å›¾åƒæ–‡æœ¬æ•°æ®é›†ï¼Œæ€»åŸ¹è®­è´¹ç”¨ä½äº1000ç¾å…ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07104">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ffab7832c3e271fd26ed1923a97334bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57502e97dfe7bdfd13ab7190e87cff60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-faa9bb464ba0d9269a60a6e8256e9df5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Lighting-the-Night-with-Generative-Artificial-Intelligence"><a href="#Lighting-the-Night-with-Generative-Artificial-Intelligence" class="headerlink" title="Lighting the Night with Generative Artificial Intelligence"></a>Lighting the Night with Generative Artificial Intelligence</h2><p><strong>Authors:Tingting Zhou, Feng Zhang, Haoyang Fu, Baoxiang Pan, Renhe Zhang, Feng Lu, Zhixin Yang</strong></p>
<p>The visible light reflectance data from geostationary satellites is crucial for meteorological observations and plays an important role in weather monitoring and forecasting. However, due to the lack of visible light at night, it is impossible to conduct continuous all-day weather observations using visible light reflectance data. This study pioneers the use of generative diffusion models to address this limitation. Based on the multi-band thermal infrared brightness temperature data from the Advanced Geostationary Radiation Imager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we developed a high-precision visible light reflectance generative model, called Reflectance Diffusion (RefDiff), which enables 0.47<del>\mu\mathrm{m}, 0.65</del>\mu\mathrm{m}, and 0.825~\mu\mathrm{m} bands visible light reflectance generation at night. Compared to the classical models, RefDiff not only significantly improves accuracy through ensemble averaging but also provides uncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90, with particularly significant improvements in areas with complex cloud structures and thick clouds. The modelâ€™s nighttime generation capability was validated using VIIRS nighttime product, demonstrating comparable performance to its daytime counterpart. In summary, this research has made substantial progress in the ability to generate visible light reflectance at night, with the potential to expand the application of nighttime visible light data. </p>
<blockquote>
<p>åœ°çƒé™æ­¢å«æ˜Ÿçš„å¯è§å…‰åå°„æ•°æ®å¯¹äºæ°”è±¡è§‚æµ‹è‡³å…³é‡è¦ï¼Œå¹¶åœ¨å¤©æ°”ç›‘æµ‹å’Œé¢„æŠ¥ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚ç„¶è€Œï¼Œç”±äºå¤œé—´ç¼ºä¹å¯è§å…‰ï¼Œæ— æ³•è¿ç»­è¿›è¡Œå…¨å¤©å€™å¤©æ°”è§‚æµ‹ä½¿ç”¨å¯è§å…‰åå°„æ•°æ®ã€‚æœ¬ç ”ç©¶ç‡å…ˆä½¿ç”¨ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹æ¥è§£å†³è¿™ä¸€é™åˆ¶ã€‚åŸºäºé£äº‘å››å·Bï¼ˆFY4Bï¼‰åœ°çƒé™æ­¢å«æ˜Ÿä¸Šçš„é«˜çº§åœ°çƒé™æ­¢è¾å°„æˆåƒä»ªï¼ˆAGRIï¼‰çš„å¤šæ³¢æ®µçƒ­çº¢å¤–äº®åº¦æ¸©åº¦æ•°æ®ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é«˜ç²¾åº¦å¯è§å…‰åå°„ç”Ÿæˆæ¨¡å‹ï¼Œåä¸ºReflectance Diffusionï¼ˆRefDiffï¼‰ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å¤œé—´ç”Ÿæˆ0.47Î¼mã€0.65Î¼må’Œ0.825Î¼mæ³¢æ®µçš„å¯è§å…‰åå°„ã€‚ä¸ä¼ ç»Ÿçš„æ¨¡å‹ç›¸æ¯”ï¼ŒRefDiffä¸ä»…é€šè¿‡é›†åˆå¹³å‡æ˜¾è‘—æé«˜ç²¾åº¦ï¼Œè¿˜æä¾›ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚å…·ä½“æ¥è¯´ï¼ŒRefDiffçš„SSIMæŒ‡æ•°å¯è¾¾åˆ°0.90ï¼Œåœ¨å…·æœ‰å¤æ‚äº‘ç»“æ„å’Œåšäº‘åŒºåŸŸè¡¨ç°å‡ºç‰¹åˆ«æ˜¾è‘—çš„æ”¹è¿›ã€‚è¯¥æ¨¡å‹çš„å¤œé—´ç”Ÿæˆèƒ½åŠ›å·²ä½¿ç”¨VIIRSå¤œé—´äº§å“è¿›è¡Œäº†éªŒè¯ï¼Œè¡¨ç°å‡ºä¸æ—¥é—´äº§å“ç›¸å½“çš„æ€§èƒ½ã€‚æ€»ä¹‹ï¼Œè¿™é¡¹ç ”ç©¶åœ¨å¤œé—´ç”Ÿæˆå¯è§å…‰åå°„æ–¹é¢å–å¾—äº†å®è´¨æ€§è¿›å±•ï¼Œæœ‰å¯èƒ½æ‰©å¤§å¤œé—´å¯è§å…‰æ•°æ®çš„åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22511v2">PDF</a> Title corrected (Lightning to Lighting); terminology updated   (retrieval to generative)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºåœ°çƒé™æ­¢å«æ˜Ÿçš„å¯è§å…‰åå°„æ•°æ®å¯¹äºæ°”è±¡è§‚æµ‹è‡³å…³é‡è¦ï¼Œä½†å¤œé—´ç¼ºä¹å¯è§å…‰å¯¼è‡´æ— æ³•å…¨å¤©å€™æŒç»­è§‚æµ‹ã€‚æœ¬ç ”ç©¶åˆ›æ–°æ€§åœ°ä½¿ç”¨ç”Ÿæˆæ‰©æ•£æ¨¡å‹æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œåˆ©ç”¨é£äº‘å››å·Bå«æ˜Ÿä¸Šçš„é«˜çº§åœ°çƒé™æ­¢è¾å°„æˆåƒä»ªçš„å¤šæ³¢æ®µçƒ­çº¢å¤–äº®åº¦æ¸©åº¦æ•°æ®ï¼Œå¼€å‘å‡ºé«˜ç²¾åº¦å¯è§å…‰åå°„ç”Ÿæˆæ¨¡å‹â€”â€”Reflectance Diffusionï¼ˆRefDiffï¼‰ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å¤œé—´ç”Ÿæˆ0.47Î¼mã€0.65Î¼må’Œ0.825Î¼mæ³¢æ®µçš„å¯è§å…‰åå°„ï¼Œç›¸æ¯”ä¼ ç»Ÿæ¨¡å‹ï¼Œä¸ä»…é€šè¿‡é›†æˆå¹³å‡æ˜¾è‘—æé«˜ç²¾åº¦ï¼Œè¿˜æä¾›ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚RefDiffçš„SSIMæŒ‡æ•°å¯è¾¾0.90ï¼Œåœ¨å¤æ‚äº‘ç»“æ„å’Œåšäº‘åŒºåŸŸè¡¨ç°å°¤å…¶æ˜¾è‘—ã€‚è¯¥æ¨¡å‹å¤œé—´ç”Ÿæˆèƒ½åŠ›å·²ä½¿ç”¨VIIRSå¤œé—´äº§å“éªŒè¯ï¼Œè¡¨ç°å‡ºä¸æ—¥é—´ç›¸å½“çš„æ€§èƒ½ã€‚æ€»ä¹‹ï¼Œè¯¥ç ”ç©¶åœ¨å¤œé—´ç”Ÿæˆå¯è§å…‰åå°„æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œæœ‰æœ›æ‰©å¤§å¤œé—´å¯è§å…‰æ•°æ®çš„åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯è§å…‰åå°„æ•°æ®å¯¹äºæ°”è±¡è§‚æµ‹ã€å¤©æ°”ç›‘æµ‹å’Œé¢„æŠ¥è‡³å…³é‡è¦ã€‚</li>
<li>å¤œé—´ç¼ºä¹å¯è§å…‰å¯¼è‡´æ— æ³•å…¨å¤©å€™è¿›è¡Œå¤©æ°”è§‚æµ‹ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨ç”Ÿæˆæ‰©æ•£æ¨¡å‹è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>å¼€å‘çš„é«˜ç²¾åº¦å¯è§å…‰åå°„ç”Ÿæˆæ¨¡å‹RefDiffï¼ŒåŸºäºå¤šæ³¢æ®µçƒ­çº¢å¤–äº®åº¦æ¸©åº¦æ•°æ®ã€‚</li>
<li>RefDiffèƒ½åœ¨å¤œé—´ç”Ÿæˆç‰¹å®šæ³¢æ®µçš„å¯è§å…‰åå°„ã€‚</li>
<li>RefDiffç›¸æ¯”ä¼ ç»Ÿæ¨¡å‹æœ‰æ›´é«˜çš„ç²¾åº¦å’Œä¸ç¡®å®šæ€§ä¼°è®¡èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22511">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71ea374c78137c6fd6e9c30404565f47.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Interpreting-Large-Text-to-Image-Diffusion-Models-with-Dictionary-Learning"><a href="#Interpreting-Large-Text-to-Image-Diffusion-Models-with-Dictionary-Learning" class="headerlink" title="Interpreting Large Text-to-Image Diffusion Models with Dictionary   Learning"></a>Interpreting Large Text-to-Image Diffusion Models with Dictionary   Learning</h2><p><strong>Authors:Stepan Shabalin, Ayush Panda, Dmitrii Kharlapenko, Abdur Raheem Ali, Yixiong Hao, Arthur Conmy</strong></p>
<p>Sparse autoencoders are a promising new approach for decomposing language model activations for interpretation and control. They have been applied successfully to vision transformer image encoders and to small-scale diffusion models. Inference-Time Decomposition of Activations (ITDA) is a recently proposed variant of dictionary learning that takes the dictionary to be a set of data points from the activation distribution and reconstructs them with gradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large text-to-image diffusion model, Flux 1, and consider the interpretability of embeddings of both by introducing a visual automated interpretation pipeline. We find that SAEs accurately reconstruct residual stream embeddings and beat MLP neurons on interpretability. We are able to use SAE features to steer image generation through activation addition. We find that ITDA has comparable interpretability to SAEs. </p>
<blockquote>
<p>ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨æ˜¯ä¸€ç§å¾ˆæœ‰å‰é€”çš„æ–°æ–¹æ³•ï¼Œç”¨äºåˆ†è§£è¯­è¨€æ¨¡å‹çš„æ¿€æ´»ï¼Œä»¥è¿›è¡Œè§£é‡Šå’Œæ§åˆ¶ã€‚å®ƒä»¬å·²æˆåŠŸåº”ç”¨äºè§†è§‰è½¬æ¢å›¾åƒç¼–ç å™¨å’Œå°è§„æ¨¡æ‰©æ•£æ¨¡å‹ã€‚æ¨ç†æ—¶é—´æ´»åŠ¨åˆ†è§£ï¼ˆITDAï¼‰æ˜¯æœ€è¿‘æå‡ºçš„è¯å…¸å­¦ä¹ å˜ä½“ï¼Œå®ƒå°†è¯å…¸è§†ä¸ºæ¿€æ´»åˆ†å¸ƒä¸­çš„æ•°æ®ç‚¹é›†ï¼Œå¹¶é€šè¿‡æ¢¯åº¦è¿½æ±‚è¿›è¡Œé‡å»ºã€‚æˆ‘ä»¬å°†ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰å’ŒITDAåº”ç”¨äºå¤§å‹æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹Flux 1ï¼Œå¹¶é€šè¿‡å¼•å…¥è§†è§‰è‡ªåŠ¨åŒ–è§£é‡Šç®¡é“æ¥è€ƒè™‘ä¸¤è€…çš„åµŒå…¥è§£é‡Šæ€§ã€‚æˆ‘ä»¬å‘ç°SAEèƒ½å¤Ÿå‡†ç¡®é‡å»ºæ®‹å·®æµåµŒå…¥ï¼Œåœ¨è§£é‡Šæ€§æ–¹é¢ä¼˜äºMLPç¥ç»å…ƒã€‚æˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨SAEç‰¹æ€§é€šè¿‡æ¿€æ´»æ·»åŠ æ¥å¼•å¯¼å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬å‘ç°ITDAçš„è§£é‡Šæ€§ä¸SAEç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24360v3">PDF</a> 10 pages, 10 figures, Mechanistic Interpretability for Vision at CVPR   2025</p>
<p><strong>Summary</strong></p>
<p>ç¨€ç–è‡ªç¼–ç å™¨æ˜¯ä¸€ç§æ–°å…´çš„æœ‰å‰é€”çš„æ–¹æ³•ï¼Œç”¨äºåˆ†è§£è¯­è¨€æ¨¡å‹çš„æ¿€æ´»ä»¥å®ç°è§£é‡Šå’Œæ§åˆ¶ã€‚å®ƒå·²æˆåŠŸåº”ç”¨äºè§†è§‰è½¬æ¢å™¨å›¾åƒç¼–ç å™¨å’Œå°å‹æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬å°†å…¶åº”ç”¨äºå¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹Flux 1ï¼Œå¹¶é€šè¿‡å¼•å…¥è§†è§‰è‡ªåŠ¨åŒ–è§£é‡Šç®¡é“æ¥è€ƒè™‘ä¸¤è€…çš„åµŒå…¥è§£é‡Šæ€§ã€‚ç ”ç©¶å‘ç°ï¼Œç¨€ç–è‡ªç¼–ç å™¨èƒ½å‡†ç¡®é‡å»ºæ®‹å·®æµåµŒå…¥ï¼Œåœ¨è§£é‡Šæ€§æ–¹é¢ä¼˜äºå¤šå±‚æ„ŸçŸ¥æœºçš„ç¥ç»å…ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰æ˜¯ä¸€ç§æ–°å…´æ–¹æ³•ï¼Œç”¨äºåˆ†è§£è¯­è¨€æ¨¡å‹çš„æ¿€æ´»ï¼Œä»¥æé«˜è§£é‡Šæ€§å’Œæ§åˆ¶æ€§ã€‚</li>
<li>ITDAæ˜¯å­—å…¸å­¦ä¹ çš„ä¸€ç§æ–°å˜ä½“ï¼Œä½¿ç”¨æ•°æ®ç‚¹é‡å»ºæ¿€æ´»åˆ†å¸ƒã€‚</li>
<li>SAEsèƒ½å‡†ç¡®é‡å»ºæ®‹å·®æµåµŒå…¥ï¼Œä¸”åœ¨è§£é‡Šæ€§æ–¹é¢ä¼˜äºMLPç¥ç»å…ƒã€‚</li>
<li>ITDAåœ¨è§£é‡Šæ€§ä¸SAEç‰¹å¾ä¸Šå…·æœ‰ç›¸å½“æ€§ã€‚</li>
<li>SAEç‰¹å¾å¯ç”¨äºé€šè¿‡æ¿€æ´»æ·»åŠ æ¥å¼•å¯¼å›¾åƒç”Ÿæˆã€‚</li>
<li>åœ¨å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹Flux 1ä¸­åº”ç”¨SAEå’ŒITDAï¼Œè€ƒè™‘äº†å®ƒä»¬çš„åµŒå…¥è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24360">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b93b635e4da1d70e4311442bd3798d72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85d5cf7e137c8a9f731c89fabcf0dc3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00f53b550bf888b45775dd5f51c7753f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6bb641b17ba340ec54f8f88b3482f7b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f5f0fecdab31a42e9fc0ea9e9d89070.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5818ab8151185c72e477a07048e98941.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MedSegFactory-Text-Guided-Generation-of-Medical-Image-Mask-Pairs"><a href="#MedSegFactory-Text-Guided-Generation-of-Medical-Image-Mask-Pairs" class="headerlink" title="MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs"></a>MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs</h2><p><strong>Authors:Jiawei Mao, Yuhan Wang, Yucheng Tang, Daguang Xu, Kang Wang, Yang Yang, Zongwei Zhou, Yuyin Zhou</strong></p>
<p>This paper presents MedSegFactory, a versatile medical synthesis framework that generates high-quality paired medical images and segmentation masks across modalities and tasks. It aims to serve as an unlimited data repository, supplying image-mask pairs to enhance existing segmentation tools. The core of MedSegFactory is a dual-stream diffusion model, where one stream synthesizes medical images and the other generates corresponding segmentation masks. To ensure precise alignment between image-mask pairs, we introduce Joint Cross-Attention (JCA), enabling a collaborative denoising paradigm by dynamic cross-conditioning between streams. This bidirectional interaction allows both representations to guide each otherâ€™s generation, enhancing consistency between generated pairs. MedSegFactory unlocks on-demand generation of paired medical images and segmentation masks through user-defined prompts that specify the target labels, imaging modalities, anatomical regions, and pathological conditions, facilitating scalable and high-quality data generation. This new paradigm of medical image synthesis enables seamless integration into diverse medical imaging workflows, enhancing both efficiency and accuracy. Extensive experiments show that MedSegFactory generates data of superior quality and usability, achieving competitive or state-of-the-art performance in 2D and 3D segmentation tasks while addressing data scarcity and regulatory constraints. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†MedSegFactoryï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„åŒ»å­¦åˆæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿç”Ÿæˆè·¨æ¨¡æ€å’Œä»»åŠ¡çš„é«˜è´¨é‡é…å¯¹åŒ»å­¦å›¾åƒå’Œåˆ†å‰²æ©è†œã€‚å®ƒçš„ç›®æ ‡æ˜¯ä½œä¸ºä¸€ä¸ªæ— é™çš„æ•°æ®ä»“åº“ï¼Œæä¾›å›¾åƒ-æ©è†œå¯¹ï¼Œä»¥å¢å¼ºç°æœ‰çš„åˆ†å‰²å·¥å…·ã€‚MedSegFactoryçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªåŒæµæ‰©æ•£æ¨¡å‹ï¼Œå…¶ä¸­ä¸€ä¸ªæµç”¨äºåˆæˆåŒ»å­¦å›¾åƒï¼Œå¦ä¸€ä¸ªæµç”¨äºç”Ÿæˆç›¸åº”çš„åˆ†å‰²æ©è†œã€‚ä¸ºäº†ç¡®ä¿å›¾åƒ-æ©è†œå¯¹ä¹‹é—´çš„ç²¾ç¡®å¯¹é½ï¼Œæˆ‘ä»¬å¼•å…¥äº†è”åˆäº¤å‰æ³¨æ„ï¼ˆJCAï¼‰æœºåˆ¶ï¼Œé€šè¿‡æµä¹‹é—´çš„åŠ¨æ€äº¤å‰æ¡ä»¶ï¼Œå®ç°ååŒå»å™ªèŒƒå¼ã€‚è¿™ç§åŒå‘äº¤äº’å…è®¸ä¸¤ç§è¡¨ç¤ºå½¢å¼ç›¸äº’å¼•å¯¼ç”Ÿæˆï¼Œå¢å¼ºç”Ÿæˆå¯¹ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚MedSegFactoryé€šè¿‡ç”¨æˆ·å®šä¹‰çš„æç¤ºï¼ˆæŒ‡å®šç›®æ ‡æ ‡ç­¾ã€æˆåƒæ¨¡å¼ã€è§£å‰–åŒºåŸŸå’Œç—…ç†çŠ¶å†µï¼‰è§£é”æŒ‰éœ€ç”Ÿæˆçš„é…å¯¹åŒ»å­¦å›¾åƒå’Œåˆ†å‰²æ©è†œï¼Œä¿ƒè¿›å¯æ‰©å±•å’Œé«˜è´¨é‡çš„æ•°æ®ç”Ÿæˆã€‚è¿™ç§æ–°çš„åŒ»å­¦å›¾åƒåˆæˆèŒƒå¼èƒ½å¤Ÿæ— ç¼åœ°èå…¥å„ç§åŒ»å­¦æˆåƒå·¥ä½œæµç¨‹ï¼Œæé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMedSegFactoryç”Ÿæˆçš„æ•°æ®å…·æœ‰å“è¶Šçš„è´¨é‡å’Œå¯ç”¨æ€§ï¼Œåœ¨äºŒç»´å’Œä¸‰ç»´åˆ†å‰²ä»»åŠ¡ä¸­è¾¾åˆ°äº†ç«äº‰æˆ–æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶è§£å†³äº†æ•°æ®ç¨€ç¼ºå’Œç›‘ç®¡çº¦æŸçš„é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.06897v2">PDF</a> 12 pages, 8 figures, The project page can be accessed via   <a target="_blank" rel="noopener" href="https://jwmao1.github.io/MedSegFactory_web">https://jwmao1.github.io/MedSegFactory_web</a></p>
<p><strong>Summary</strong></p>
<p>MedSegFactoryæ˜¯ä¸€ä¸ªé€šç”¨åŒ»ç–—åˆæˆæ¡†æ¶ï¼Œèƒ½ç”Ÿæˆè·¨æ¨¡æ€å’Œä»»åŠ¡çš„é…å¯¹é«˜è´¨é‡åŒ»ç–—å›¾åƒå’Œåˆ†å‰²æ©è†œã€‚å®ƒæ—¨åœ¨ä½œä¸ºä¸€ä¸ªæ— é™çš„æ•°æ®ä»“åº“ï¼Œä¸ºç°æœ‰çš„åˆ†å‰²å·¥å…·æä¾›å›¾åƒ-æ©è†œå¯¹ä»¥å¢å¼ºå…¶æ€§èƒ½ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒæ˜¯åŒé‡æµæ‰©æ•£æ¨¡å‹ï¼Œä¸€ä¸ªæµè´Ÿè´£åˆæˆåŒ»ç–—å›¾åƒï¼Œå¦ä¸€ä¸ªæµç”Ÿæˆç›¸åº”çš„åˆ†å‰²æ©è†œã€‚é€šè¿‡å¼•å…¥è”åˆäº¤å‰æ³¨æ„ï¼ˆJCAï¼‰æœºåˆ¶ï¼Œç¡®ä¿å›¾åƒ-æ©è†œå¯¹ä¹‹é—´çš„ç²¾ç¡®å¯¹é½ã€‚JCAå®ç°äº†æµä¹‹é—´çš„åŠ¨æ€äº¤å‰æ¡ä»¶ï¼Œæ¨åŠ¨ååŒå»å™ªæ¨¡å¼ã€‚è¿™ç§åŒå‘äº¤äº’å…è®¸ä¸¤ä¸ªè¡¨ç¤ºç›¸äº’å¼•å¯¼ç”Ÿæˆï¼Œå¢å¼ºç”Ÿæˆé…å¯¹çš„ä¸€è‡´æ€§ã€‚MedSegFactoryé€šè¿‡ç”¨æˆ·å®šä¹‰æç¤ºå®ç°æŒ‰éœ€ç”Ÿæˆé…å¯¹åŒ»ç–—å›¾åƒå’Œåˆ†å‰²æ©è†œï¼Œæç¤ºå¯æŒ‡å®šç›®æ ‡æ ‡ç­¾ã€æˆåƒæ¨¡å¼ã€è§£å‰–åŒºåŸŸå’Œç—…ç†çŠ¶å†µç­‰ã€‚è¿™ä¸€å…¨æ–°çš„åŒ»ç–—å›¾åƒåˆæˆæ¨¡å¼å¯æ— ç¼èå…¥å¤šæ ·åŒ–çš„åŒ»ç–—æˆåƒå·¥ä½œæµç¨‹ï¼Œæé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒMedSegFactoryç”Ÿæˆçš„æ•°æ®è´¨é‡é«˜ã€å®ç”¨æ€§å¼ºï¼Œåœ¨äºŒç»´å’Œä¸‰ç»´åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè§£å†³äº†æ•°æ®ç¨€ç¼ºå’Œç›‘ç®¡çº¦æŸé—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MedSegFactoryæ˜¯ä¸€ä¸ªåŒ»ç–—åˆæˆæ¡†æ¶ï¼Œèƒ½ç”Ÿæˆé«˜è´¨é‡é…å¯¹åŒ»ç–—å›¾åƒå’Œåˆ†å‰²æ©è†œã€‚</li>
<li>å®ƒä½œä¸ºä¸€ä¸ªæ— é™æ•°æ®ä»“åº“ï¼Œä¸ºç°æœ‰åˆ†å‰²å·¥å…·æä¾›å›¾åƒ-æ©è†œå¯¹ã€‚</li>
<li>åŒé‡æµæ‰©æ•£æ¨¡å‹æ˜¯æ¡†æ¶çš„æ ¸å¿ƒï¼ŒåŒ…æ‹¬å›¾åƒåˆæˆæµå’Œåˆ†å‰²æ©è†œç”Ÿæˆæµã€‚</li>
<li>å¼•å…¥çš„è”åˆäº¤å‰æ³¨æ„ï¼ˆJCAï¼‰æœºåˆ¶ç¡®ä¿å›¾åƒå’Œæ©è†œä¹‹é—´çš„ç²¾ç¡®å¯¹é½ã€‚</li>
<li>ç”¨æˆ·å¯ä»¥é€šè¿‡å®šä¹‰æç¤ºï¼ˆå¦‚ç›®æ ‡æ ‡ç­¾ã€æˆåƒæ¨¡å¼ç­‰ï¼‰å®ç°æŒ‰éœ€ç”Ÿæˆå›¾åƒå’Œæ©è†œã€‚</li>
<li>MedSegFactoryèƒ½æé«˜åŒ»ç–—æˆåƒå·¥ä½œæµç¨‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.06897">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d5225258513b6c9f19326f069835bb49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5136966fc5c824d9fe3cd8414dda1962.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b47aacb690f87bcd65792c86ab8b4478.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a24d0c123339caebfbdb3e5848748b4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7aaa7fe5f944b330aa02b35a45dd3ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01f1cbbd073c3947c37438e5972d7cbf.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="X-Dancer-Expressive-Music-to-Human-Dance-Video-Generation"><a href="#X-Dancer-Expressive-Music-to-Human-Dance-Video-Generation" class="headerlink" title="X-Dancer: Expressive Music to Human Dance Video Generation"></a>X-Dancer: Expressive Music to Human Dance Video Generation</h2><p><strong>Authors:Zeyuan Chen, Hongyi Xu, Guoxian Song, You Xie, Chenxu Zhang, Xin Chen, Chao Wang, Di Chang, Linjie Luo</strong></p>
<p>We present X-Dancer, a novel zero-shot music-driven image animation pipeline that creates diverse and long-range lifelike human dance videos from a single static image. As its core, we introduce a unified transformer-diffusion framework, featuring an autoregressive transformer model that synthesize extended and music-synchronized token sequences for 2D body, head and hands poses, which then guide a diffusion model to produce coherent and realistic dance video frames. Unlike traditional methods that primarily generate human motion in 3D, X-Dancer addresses data limitations and enhances scalability by modeling a wide spectrum of 2D dance motions, capturing their nuanced alignment with musical beats through readily available monocular videos. To achieve this, we first build a spatially compositional token representation from 2D human pose labels associated with keypoint confidences, encoding both large articulated body movements (e.g., upper and lower body) and fine-grained motions (e.g., head and hands). We then design a music-to-motion transformer model that autoregressively generates music-aligned dance pose token sequences, incorporating global attention to both musical style and prior motion context. Finally we leverage a diffusion backbone to animate the reference image with these synthesized pose tokens through AdaIN, forming a fully differentiable end-to-end framework. Experimental results demonstrate that X-Dancer is able to produce both diverse and characterized dance videos, substantially outperforming state-of-the-art methods in term of diversity, expressiveness and realism. Code and model will be available for research purposes. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†X-Dancerï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹é›¶æ ·æœ¬éŸ³ä¹é©±åŠ¨å›¾åƒåŠ¨ç”»ç®¡é“ï¼Œå®ƒå¯ä»¥ä»å•ä¸ªé™æ€å›¾åƒä¸­åˆ›å»ºå¤šæ ·ä¸”é•¿ç¨‹çš„é€¼çœŸäººç±»èˆè¹ˆè§†é¢‘ã€‚å…¶æ ¸å¿ƒæ˜¯ç»Ÿä¸€transformeræ‰©æ•£æ¡†æ¶ï¼Œå®ƒé‡‡ç”¨è‡ªå›å½’transformeræ¨¡å‹åˆæˆæ‰©å±•çš„ä¸éŸ³ä¹åŒæ­¥çš„ä»¤ç‰Œåºåˆ—ï¼Œç”¨äºäºŒç»´èº«ä½“ã€å¤´éƒ¨å’Œæ‰‹éƒ¨å§¿åŠ¿ã€‚ç„¶åè¿™äº›åºåˆ—å¼•å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¿è´¯ä¸”é€¼çœŸçš„èˆè¹ˆè§†é¢‘å¸§ã€‚ä¸ä¼ ç»Ÿçš„ä¸»è¦åœ¨3Dä¸­ç”Ÿæˆäººç±»è¿åŠ¨çš„æ–¹æ³•ä¸åŒï¼ŒX-Danceré€šè¿‡å»ºæ¨¡å¤§é‡çš„äºŒç»´èˆè¹ˆåŠ¨ä½œæ¥è§£å†³æ•°æ®é™åˆ¶å¹¶å¢å¼ºå¯æ‰©å±•æ€§ï¼Œé€šè¿‡å¯è·å¾—çš„å•ç›®è§†é¢‘æ•æ‰å…¶ä¸éŸ³ä¹èŠ‚å¥çš„å¾®å¦™å¯¹é½ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆæ ¹æ®ä¸å…³é”®ç‚¹ç½®ä¿¡åº¦ç›¸å…³çš„äºŒç»´äººä½“å§¿åŠ¿æ ‡ç­¾æ„å»ºç©ºé—´ç»„åˆä»¤ç‰Œè¡¨ç¤ºï¼Œç¼–ç å¤§å‹å…³èŠ‚èº«ä½“è¿åŠ¨ï¼ˆä¾‹å¦‚ä¸Šä¸‹èº«ä½“ï¼‰å’Œç²¾ç»†è¿åŠ¨ï¼ˆä¾‹å¦‚å¤´éƒ¨å’Œæ‰‹éƒ¨ï¼‰ã€‚ç„¶åæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªéŸ³ä¹åˆ°è¿åŠ¨çš„transformeræ¨¡å‹ï¼Œè¯¥æ¨¡å‹è‡ªå›å½’åœ°ç”Ÿæˆä¸éŸ³ä¹å¯¹é½çš„èˆè¹ˆå§¿åŠ¿ä»¤ç‰Œåºåˆ—ï¼Œå¹¶èå…¥å…¨å±€æ³¨æ„åŠ›ä»¥å…³æ³¨éŸ³ä¹é£æ ¼å’Œå…ˆå‰çš„è¿åŠ¨ä¸Šä¸‹æ–‡ã€‚æœ€åæˆ‘ä»¬åˆ©ç”¨æ‰©æ•£ä¸»å¹²é€šè¿‡è¿™äº›åˆæˆçš„å§¿åŠ¿ä»¤ç‰Œå¯¹å‚è€ƒå›¾åƒè¿›è¡ŒåŠ¨ç”»å¤„ç†ï¼Œå½¢æˆä¸€ä¸ªå®Œå…¨å¯å¾®åˆ†çš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒX-Dancerèƒ½å¤Ÿäº§ç”Ÿå¤šæ ·ä¸”å…·ç‰¹è‰²çš„èˆè¹ˆè§†é¢‘ï¼Œåœ¨å¤šæ ·æ€§ã€è¡¨ç°åŠ›å’Œé€¼çœŸåº¦æ–¹é¢å¤§å¤§ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯çš„æ–¹æ³•ã€‚ä»£ç å’Œæ¨¡å‹å°†ç”¨äºç ”ç©¶ç›®çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17414v2">PDF</a> ICCV 2025. Project Page: <a target="_blank" rel="noopener" href="https://zeyuan-chen.com/X-Dancer/">https://zeyuan-chen.com/X-Dancer/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†X-Dancerï¼Œä¸€ç§æ–°é¢–çš„é›¶æ ·æœ¬éŸ³ä¹é©±åŠ¨å›¾åƒåŠ¨ç”»ç®¡é“ï¼Œèƒ½å¤Ÿä»å•å¼ é™æ€å›¾åƒåˆ›å»ºå¤šæ ·ä¸”é•¿ç¨‹é€¼çœŸçš„èˆè¹ˆè§†é¢‘ã€‚å…¶æ ¸å¿ƒæ˜¯ç»Ÿä¸€transformer-diffusionæ¡†æ¶ï¼ŒåŒ…å«ä¸€ä¸ªè‡ªå›å½’transformeræ¨¡å‹ï¼Œç”¨äºåˆæˆæ‰©å±•ä¸”ä¸éŸ³ä¹åŒæ­¥çš„tokenåºåˆ—ï¼Œç”¨äºæŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆè¿è´¯ä¸”é€¼çœŸçš„èˆè¹ˆè§†é¢‘å¸§ã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•ä¸»è¦åœ¨3Dä¸­ç”Ÿæˆäººä½“è¿åŠ¨ï¼ŒX-Dancerè§£å†³äº†æ•°æ®é™åˆ¶å¹¶å¢å¼ºäº†å¯æ‰©å±•æ€§ï¼Œé€šè¿‡å»ºæ¨¡ä¸°å¯Œçš„2Dèˆè¹ˆåŠ¨ä½œå¹¶æ•æ‰å…¶ä¸éŸ³ä¹èŠ‚å¥çš„å¾®å¦™å¯¹é½ï¼Œé€šè¿‡å¯è·å¾—çš„å•ç›®è§†é¢‘å®ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>X-Danceræ˜¯ä¸€ç§é›¶æ ·æœ¬éŸ³ä¹é©±åŠ¨å›¾åƒåŠ¨ç”»ç®¡é“ï¼Œèƒ½ä»å•å¼ é™æ€å›¾åƒåˆ›å»ºèˆè¹ˆè§†é¢‘ã€‚</li>
<li>å®ƒé‡‡ç”¨ç»Ÿä¸€çš„transformer-diffusionæ¡†æ¶ï¼ŒåŒ…å«è‡ªå›å½’transformeræ¨¡å‹ï¼Œç”¨äºåˆæˆæ‰©å±•ä¸”ä¸éŸ³ä¹åŒæ­¥çš„tokenåºåˆ—ã€‚</li>
<li>X-Danceré€šè¿‡å»ºæ¨¡ä¸°å¯Œçš„2Dèˆè¹ˆåŠ¨ä½œå¹¶æ•æ‰å…¶ä¸éŸ³ä¹èŠ‚å¥çš„å¾®å¦™å¯¹é½ï¼Œå®ç°äº†æ•°æ®é™åˆ¶å’Œå¯æ‰©å±•æ€§çš„å¢å¼ºã€‚</li>
<li>å®ƒé€šè¿‡å…³è”äººä½“å…³é”®ç‚¹ç½®ä¿¡åº¦çš„ç©ºé—´ç»„åˆtokenè¡¨ç¤ºæ¥ç¼–ç å¤§å‹å…³èŠ‚è¿åŠ¨å’Œç²¾ç»†åŠ¨ä½œã€‚</li>
<li>X-Dancerè®¾è®¡äº†ä¸€ä¸ªéŸ³ä¹åˆ°è¿åŠ¨çš„transformeræ¨¡å‹ï¼Œèƒ½å¤Ÿè‡ªå›å½’åœ°ç”Ÿæˆä¸éŸ³ä¹é£æ ¼å’Œå†å²è¿åŠ¨ä¸Šä¸‹æ–‡å¯¹é½çš„èˆè¹ˆå§¿åŠ¿tokenåºåˆ—ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹å’Œåç«¯åŠ¨ç”»æŠ€æœ¯ï¼ŒX-Dancerèƒ½å¤Ÿä»¥åˆæˆçš„å§¿åŠ¿tokenæ¿€æ´»å‚è€ƒå›¾åƒï¼Œå½¢æˆä¸€ä¸ªå®Œå…¨å¯å¾®åˆ†çš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17414">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-469c57d225a45963a6723f05b6f17800.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab6217adb34d69ac043e42b1d9b8e289.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7db7895eb0de189c748235333aeb25bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e44c3e266824cefaeccf7207bd50d409.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FreeScale-Unleashing-the-Resolution-of-Diffusion-Models-via-Tuning-Free-Scale-Fusion"><a href="#FreeScale-Unleashing-the-Resolution-of-Diffusion-Models-via-Tuning-Free-Scale-Fusion" class="headerlink" title="FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free   Scale Fusion"></a>FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free   Scale Fusion</h2><p><strong>Authors:Haonan Qiu, Shiwei Zhang, Yujie Wei, Ruihang Chu, Hangjie Yuan, Xiang Wang, Yingya Zhang, Ziwei Liu</strong></p>
<p>Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. To tackle this challenge, we propose FreeScale, a tuning-free inference paradigm to enable higher-resolution visual generation via scale fusion. Specifically, FreeScale processes information from different receptive scales and then fuses it by extracting desired frequency components. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Notably, compared with previous best-performing methods, FreeScale unlocks the 8k-resolution text-to-image generation for the first time. </p>
<blockquote>
<p>è§†è§‰æ‰©æ•£æ¨¡å‹å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†ç”±äºç¼ºä¹é«˜åˆ†è¾¨ç‡æ•°æ®å’Œè®¡ç®—èµ„æºçš„é™åˆ¶ï¼Œå®ƒä»¬é€šå¸¸åªèƒ½åœ¨æœ‰é™çš„åˆ†è¾¨ç‡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨æ›´é«˜åˆ†è¾¨ç‡ä¸‹ç”Ÿæˆé«˜ä¿çœŸå›¾åƒæˆ–è§†é¢‘çš„èƒ½åŠ›ã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œå·²ç»æ¢ç´¢äº†æ— å¾®è°ƒç­–ç•¥ï¼Œä»¥å±•ç¤ºé¢„è®­ç»ƒæ¨¡å‹æœªå¼€å‘çš„é«˜åˆ†è¾¨ç‡è§†è§‰ç”Ÿæˆæ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»å€¾å‘äºäº§ç”Ÿå¸¦æœ‰é‡å¤æ¨¡å¼çš„ä½è´¨é‡è§†è§‰å†…å®¹ã€‚å…³é”®éšœç¢åœ¨äºæ¨¡å‹ç”Ÿæˆè¶…è¿‡å…¶è®­ç»ƒåˆ†è¾¨ç‡çš„è§†è§‰å†…å®¹æ—¶ä¸å¯é¿å…çš„é«˜é¢‘ä¿¡æ¯å¢åŠ ï¼Œè¿™ä¼šå¯¼è‡´ç”±ç´¯ç§¯è¯¯å·®å¼•èµ·çš„ä¸å¯å–çš„é‡å¤æ¨¡å¼ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†FreeScaleï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€å¾®è°ƒæ¨ç†èŒƒå¼ï¼Œé€šè¿‡å°ºåº¦èåˆå®ç°é«˜åˆ†è¾¨ç‡è§†è§‰ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼ŒFreeScaleå¤„ç†æ¥è‡ªä¸åŒæ„Ÿå—å°ºåº¦çš„ä¿¡æ¯ï¼Œç„¶åé€šè¿‡æå–æ‰€éœ€çš„é¢‘ç‡æˆåˆ†å°†å…¶èåˆã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„èŒƒå¼åœ¨æ‰©å±•å›¾åƒå’Œè§†é¢‘æ¨¡å‹çš„åˆ†è¾¨ç‡è§†è§‰ç”Ÿæˆèƒ½åŠ›æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸ä»¥å‰è¡¨ç°æœ€ä½³çš„æ–¹æ³•ç›¸æ¯”ï¼ŒFreeScaleé¦–æ¬¡å®ç°äº†8kåˆ†è¾¨ç‡çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09626v2">PDF</a> ICCV 2025, Project Page:   <a target="_blank" rel="noopener" href="http://haonanqiu.com/projects/FreeScale.html">http://haonanqiu.com/projects/FreeScale.html</a>, Code Repo:   <a target="_blank" rel="noopener" href="https://github.com/ali-vilab/FreeScale">https://github.com/ali-vilab/FreeScale</a></p>
<p><strong>Summary</strong><br>     è§†è§‰æ‰©æ•£æ¨¡å‹è™½æœ‰æ‰€è¿›å±•ï¼Œä½†ç”±äºç¼ºä¹é«˜åˆ†è¾¨ç‡æ•°æ®å’Œè®¡ç®—èµ„æºçš„é™åˆ¶ï¼Œé€šå¸¸åªåœ¨æœ‰é™çš„åˆ†è¾¨ç‡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™é™åˆ¶äº†å…¶ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒæˆ–è§†é¢‘çš„èƒ½åŠ›ã€‚æœ€è¿‘çš„ç ”ç©¶å°è¯•é‡‡ç”¨æ— å¾®è°ƒç­–ç•¥æ¥å±•ç¤ºé¢„è®­ç»ƒæ¨¡å‹åœ¨æ›´é«˜åˆ†è¾¨ç‡è§†è§‰ç”Ÿæˆæ–¹é¢çš„æ½œåŠ›ï¼Œä½†è¿™äº›æ–¹æ³•ä»æ˜“äº§ç”Ÿå¸¦æœ‰é‡å¤æ¨¡å¼çš„ä½è´¨é‡è§†è§‰å†…å®¹ã€‚å…³é”®åœ¨äºï¼Œå½“æ¨¡å‹ç”Ÿæˆè¶…è¿‡å…¶è®­ç»ƒåˆ†è¾¨ç‡çš„è§†è§‰å†…å®¹æ—¶ï¼Œé«˜é¢‘ä¿¡æ¯çš„å¢åŠ æ˜¯ä¸å¯é¿å…çš„ï¼Œè¿™ä¼šå¯¼è‡´ç”±ç´¯ç§¯è¯¯å·®å¼•èµ·çš„é‡å¤æ¨¡å¼ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†FreeScaleï¼Œä¸€ç§æ— éœ€è°ƒå‚çš„æ¨ç†èŒƒå¼ï¼Œé€šè¿‡å°ºåº¦èåˆå®ç°æ›´é«˜åˆ†è¾¨ç‡çš„è§†è§‰ç”Ÿæˆã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„èŒƒå¼åœ¨å›¾åƒå’Œè§†é¢‘æ¨¡å‹çš„æ›´é«˜åˆ†è¾¨ç‡è§†è§‰ç”Ÿæˆèƒ½åŠ›æ–¹é¢å…·æœ‰ä¼˜è¶Šæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸ä»¥å‰è¡¨ç°æœ€ä½³çš„æ–¹æ³•ç›¸æ¯”ï¼ŒFreeScaleé¦–æ¬¡å®ç°äº†8kåˆ†è¾¨ç‡çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æ‰©æ•£æ¨¡å‹å› ç¼ºä¹é«˜åˆ†è¾¨ç‡æ•°æ®å’Œè®¡ç®—èµ„æºé™åˆ¶è€Œé€šå¸¸åœ¨æœ‰é™åˆ†è¾¨ç‡ä¸‹è®­ç»ƒã€‚</li>
<li>ç¼ºä¹é«˜åˆ†è¾¨ç‡è®­ç»ƒé™åˆ¶äº†è§†è§‰æ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å›¾åƒå’Œè§†é¢‘çš„èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å°è¯•é‡‡ç”¨æ— å¾®è°ƒç­–ç•¥æ¥ä¼˜åŒ–é¢„è®­ç»ƒæ¨¡å‹åœ¨æ›´é«˜åˆ†è¾¨ç‡è§†è§‰ç”Ÿæˆæ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>å½“å‰æ–¹æ³•ä»é¢ä¸´ç”Ÿæˆä½è´¨é‡è§†è§‰å†…å®¹çš„é—®é¢˜ï¼ŒåŒ…å«é‡å¤æ¨¡å¼ã€‚</li>
<li>æ¨¡å‹ç”Ÿæˆè¶…è¿‡è®­ç»ƒåˆ†è¾¨ç‡çš„è§†è§‰å†…å®¹æ—¶ï¼Œé«˜é¢‘ä¿¡æ¯çš„å¢åŠ ä¼šå¯¼è‡´é‡å¤æ¨¡å¼çš„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„FreeScaleæ–¹æ³•é€šè¿‡å°ºåº¦èåˆå®ç°æ— éœ€è°ƒå‚çš„æ›´é«˜åˆ†è¾¨ç‡è§†è§‰ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af118821250c88abcbe8a04be24887f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a9ddc7a42c21365b359274c4d31f8ad8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-edba077de551e21a399ddf924ebd6c3a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf392d9d0ae69566c4f61893f9c632eb.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Amortized-Posterior-Sampling-with-Diffusion-Prior-Distillation"><a href="#Amortized-Posterior-Sampling-with-Diffusion-Prior-Distillation" class="headerlink" title="Amortized Posterior Sampling with Diffusion Prior Distillation"></a>Amortized Posterior Sampling with Diffusion Prior Distillation</h2><p><strong>Authors:Abbas Mammadov, Hyungjin Chung, Jong Chul Ye</strong></p>
<p>We propose Amortized Posterior Sampling (APS), a novel variational inference approach for efficient posterior sampling in inverse problems. Our method trains a conditional flow model to minimize the divergence between the variational distribution and the posterior distribution implicitly defined by the diffusion model. This results in a powerful, amortized sampler capable of generating diverse posterior samples with a single neural function evaluation, generalizing across various measurements. Unlike existing methods, our approach is unsupervised, requires no paired training data, and is applicable to both Euclidean and non-Euclidean domains. We demonstrate its effectiveness on a range of tasks, including image restoration, manifold signal reconstruction, and climate data imputation. APS significantly outperforms existing approaches in computational efficiency while maintaining competitive reconstruction quality, enabling real-time, high-quality solutions to inverse problems across diverse domains. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†æ‘Šé”€åé‡‡æ ·ï¼ˆAmortized Posterior Samplingï¼ŒAPSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹åé—®é¢˜ä¸­æœ‰æ•ˆåéªŒé‡‡æ ·çš„æ–°å‹å˜åˆ†æ¨æ–­æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•è®­ç»ƒäº†ä¸€ä¸ªæ¡ä»¶æµæ¨¡å‹ï¼Œä»¥æœ€å°åŒ–å˜åˆ†åˆ†å¸ƒä¸æ‰©æ•£æ¨¡å‹éšå¼å®šä¹‰çš„åéªŒåˆ†å¸ƒä¹‹é—´çš„æ•£åº¦ã€‚è¿™äº§ç”Ÿäº†ä¸€ä¸ªå¼ºå¤§çš„æ‘Šé”€é‡‡æ ·å™¨ï¼Œèƒ½å¤Ÿåœ¨ä¸€æ¬¡ç¥ç»ç½‘ç»œå‡½æ•°è¯„ä¼°ä¸­ç”Ÿæˆå¤šæ ·åŒ–çš„åéªŒæ ·æœ¬ï¼Œå¹¶å¯åœ¨å„ç§æµ‹é‡ä¸­é€šç”¨ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯æ— ç›‘ç£çš„ï¼Œä¸éœ€è¦é…å¯¹è®­ç»ƒæ•°æ®ï¼Œé€‚ç”¨äºæ¬§å‡ é‡Œå¾—å’Œéæ¬§å‡ é‡Œå¾—é¢†åŸŸã€‚æˆ‘ä»¬åœ¨å›¾åƒæ¢å¤ã€æµå½¢ä¿¡å·é‡å»ºå’Œæ°”å€™æ•°æ®æ’è¡¥ç­‰ä¸€ç³»åˆ—ä»»åŠ¡ä¸­å±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚APSåœ¨è®¡ç®—æ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒæœ‰ç«äº‰åŠ›çš„é‡å»ºè´¨é‡ï¼Œä¸ºä¸åŒé¢†åŸŸçš„åé—®é¢˜æä¾›å®æ—¶é«˜è´¨é‡è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.17907v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æå‡ºä¸€ç§åä¸ºAmortized Posterior Samplingï¼ˆAPSï¼‰çš„æ–°å‹å˜åˆ†æ¨æ–­æ–¹æ³•ï¼Œç”¨äºé«˜æ•ˆè§£å†³åé—®é¢˜ä¸­çš„åé‡‡æ ·é—®é¢˜ã€‚é€šè¿‡è®­ç»ƒæ¡ä»¶æµæ¨¡å‹ï¼Œæœ€å°åŒ–å˜åˆ†åˆ†å¸ƒä¸æ‰©æ•£æ¨¡å‹éšå¼å®šä¹‰çš„åéªŒåˆ†å¸ƒä¹‹é—´çš„åˆ†æ­§ï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªå¼ºå¤§çš„ã€ä¸€æ¬¡æ€§é‡‡æ ·å™¨ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€ç¥ç»ç½‘ç»œå‡½æ•°è¯„ä¼°ä¸­ç”Ÿæˆå¤šæ ·åŒ–çš„åéªŒæ ·æœ¬ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºå„ç§æµ‹é‡ã€‚è¯¥æ–¹æ³•æ— éœ€ç›‘ç£ï¼Œæ— éœ€é…å¯¹è®­ç»ƒæ•°æ®ï¼Œé€‚ç”¨äºæ¬§å‡ é‡Œå¾—å’Œéæ¬§å‡ é‡Œå¾—é¢†åŸŸã€‚åœ¨å›¾åƒæ¢å¤ã€æµå½¢ä¿¡å·é‡å»ºå’Œæ°”å€™æ•°æ®æ’è¡¥ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œåœ¨è®¡ç®—æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒç«äº‰åŠ›çš„é‡å»ºè´¨é‡ï¼Œä¸ºä¸åŒé¢†åŸŸçš„åé—®é¢˜æä¾›å®æ—¶é«˜è´¨é‡è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥Amortized Posterior Samplingï¼ˆAPSï¼‰æ–¹æ³•ï¼Œä¸€ç§é’ˆå¯¹åé—®é¢˜ä¸­åé‡‡æ ·çš„æ–°å‹å˜åˆ†æ¨æ–­æ–¹æ³•ã€‚</li>
<li>é€šè¿‡è®­ç»ƒæ¡ä»¶æµæ¨¡å‹ï¼Œæœ€å°åŒ–å˜åˆ†åˆ†å¸ƒä¸æ‰©æ•£æ¨¡å‹åéªŒåˆ†å¸ƒä¹‹é—´çš„åˆ†æ­§ã€‚</li>
<li>APSæ–¹æ³•åˆ›å»ºä¸€ä¸ªå¼ºå¤§çš„ã€ä¸€æ¬¡æ€§é‡‡æ ·å™¨ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€ç¥ç»ç½‘ç»œå‡½æ•°è¯„ä¼°ä¸­ç”Ÿæˆå¤šæ ·åŒ–åéªŒæ ·æœ¬ã€‚</li>
<li>APSæ–¹æ³•å…·æœ‰é€šç”¨æ€§ï¼Œå¯åº”ç”¨äºå„ç§æµ‹é‡ã€å›¾åƒæ¢å¤ã€æµå½¢ä¿¡å·é‡å»ºå’Œæ°”å€™æ•°æ®æ’è¡¥ç­‰ä»»åŠ¡ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€ç›‘ç£ï¼Œæ— éœ€é…å¯¹è®­ç»ƒæ•°æ®ï¼Œé€‚ç”¨äºæ¬§å‡ é‡Œå¾—å’Œéæ¬§å‡ é‡Œå¾—é¢†åŸŸã€‚</li>
<li>APSåœ¨è®¡ç®—æ•ˆç‡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒç«äº‰åŠ›çš„é‡å»ºè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.17907">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1cd95b67ae60eaf394633e660e2bd7d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7206a1c059e5a0e9c2366da19d63da0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74b12f7c9f7dd85d8d4847cacef3cfab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdd6c230c35ee5e3a3fc72451aded3fc.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-15/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-15/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-15/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c047be575f3ba1e47cbc6be136cadd6a.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-15  Ensemble of Weak Spectral Total Variation Learners a PET-CT Case Study
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-15/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-71d52238ae8c73c5dfdd11b671f91485.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-15  RegGS Unposed Sparse Views Gaussian Splatting with 3DGS Registration
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30341.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
