<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  Uncertainty Estimation for Novel Views in Gaussian Splatting from   Primitive-Based Representations of Error and Visibility">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-e6853f3bb5b1f7bcfd4c0da8bf95a38c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-06-æ›´æ–°"><a href="#2025-08-06-æ›´æ–°" class="headerlink" title="2025-08-06 æ›´æ–°"></a>2025-08-06 æ›´æ–°</h1><h2 id="Uncertainty-Estimation-for-Novel-Views-in-Gaussian-Splatting-from-Primitive-Based-Representations-of-Error-and-Visibility"><a href="#Uncertainty-Estimation-for-Novel-Views-in-Gaussian-Splatting-from-Primitive-Based-Representations-of-Error-and-Visibility" class="headerlink" title="Uncertainty Estimation for Novel Views in Gaussian Splatting from   Primitive-Based Representations of Error and Visibility"></a>Uncertainty Estimation for Novel Views in Gaussian Splatting from   Primitive-Based Representations of Error and Visibility</h2><p><strong>Authors:Thomas Gottwald, Edgar Heinert, Matthias Rottmann</strong></p>
<p>In this work, we present a novel method for uncertainty estimation (UE) in Gaussian Splatting. UE is crucial for using Gaussian Splatting in critical applications such as robotics and medicine. Previous methods typically estimate the variance of Gaussian primitives and use the rendering process to obtain pixel-wise uncertainties. Our method establishes primitive representations of error and visibility of trainings views, which carries meaningful uncertainty information. This representation is obtained by projection of training error and visibility onto the primitives. Uncertainties of novel views are obtained by rendering the primitive representations of uncertainty for those novel views, yielding uncertainty feature maps. To aggregate these uncertainty feature maps of novel views, we perform a pixel-wise regression on holdout data. In our experiments, we analyze the different components of our method, investigating various combinations of uncertainty feature maps and regression models. Furthermore, we considered the effect of separating splatting into foreground and background. Our UEs show high correlations to true errors, outperforming state-of-the-art methods, especially on foreground objects. The trained regression models show generalization capabilities to new scenes, allowing uncertainty estimation without the need for holdout data. </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºé«˜æ–¯æ¶‚æŠ¹ï¼ˆGaussian Splattingï¼‰ä¸­çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ˆUEï¼‰çš„æ–°æ–¹æ³•ã€‚ä¸ç¡®å®šæ€§ä¼°è®¡æ˜¯ä½¿ç”¨é«˜æ–¯æ¶‚æŠ¹äºæœºå™¨äººå’ŒåŒ»å­¦ç­‰å…³é”®åº”ç”¨ä¸­çš„å…³é”®ã€‚ä»¥å‰çš„æ–¹æ³•é€šå¸¸ä¼°è®¡é«˜æ–¯åŸºå…ƒï¼ˆGaussian primitivesï¼‰çš„æ–¹å·®ï¼Œå¹¶ä½¿ç”¨æ¸²æŸ“è¿‡ç¨‹è·å¾—åƒç´ çº§çš„ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹äº†è®­ç»ƒè§†å›¾è¯¯å·®å’Œå¯è§æ€§çš„åŸºå…ƒè¡¨ç¤ºï¼Œå…¶ä¸­åŒ…å«æœ‰æ„ä¹‰çš„ä¸ç¡®å®šæ€§ä¿¡æ¯ã€‚è¿™ç§è¡¨ç¤ºæ˜¯é€šè¿‡å°†è®­ç»ƒè¯¯å·®å’Œå¯è§æ€§æŠ•å½±åˆ°åŸºå…ƒä¸Šè·å¾—çš„ã€‚æ–°å‹è§†å›¾çš„ä¸ç¡®å®šæ€§æ˜¯é€šè¿‡æ¸²æŸ“è¿™äº›æ–°å‹è§†å›¾çš„åŸºå…ƒä¸ç¡®å®šæ€§è¡¨ç¤ºè€Œè·å¾—çš„ï¼Œä»è€Œäº§ç”Ÿä¸ç¡®å®šæ€§ç‰¹å¾å›¾ã€‚ä¸ºäº†èšåˆè¿™äº›æ–°å‹è§†å›¾çš„ä¸ç¡®å®šæ€§ç‰¹å¾å›¾ï¼Œæˆ‘ä»¬åœ¨é¢„ç•™æ•°æ®ä¸Šè¿›è¡Œåƒç´ çº§å›å½’ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†æˆ‘ä»¬æ–¹æ³•çš„ä¸åŒç»„æˆéƒ¨åˆ†ï¼Œç ”ç©¶äº†ä¸ç¡®å®šæ€§ç‰¹å¾å›¾å’Œå›å½’æ¨¡å‹çš„å„ç§ç»„åˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è€ƒè™‘äº†å°†æ¶‚æŠ¹åˆ†ä¸ºå‰æ™¯å’ŒèƒŒæ™¯çš„å½±å“ã€‚æˆ‘ä»¬çš„ä¸ç¡®å®šæ€§ä¼°è®¡ä¸çœŸå®è¯¯å·®é«˜åº¦ç›¸å…³ï¼Œå°¤å…¶æ˜¯åœ¨å‰æ™¯å¯¹è±¡ä¸Šï¼Œä¼˜äºæœ€æ–°æ–¹æ³•ã€‚ç»è¿‡è®­ç»ƒçš„å›å½’æ¨¡å‹å¯¹æ–°åœºæ™¯å…·æœ‰é€šç”¨æ€§ï¼Œå…è®¸åœ¨æ²¡æœ‰é¢„ç•™æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02443v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºé«˜æ–¯æ··åˆæ¨¡å‹ä¸­çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ˆUEï¼‰çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¯¹äºåœ¨æœºå™¨äººå’ŒåŒ»å­¦ç­‰å…³é”®åº”ç”¨ä¸­ä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡å‹è‡³å…³é‡è¦ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹äº†è®­ç»ƒè§†å›¾çš„è¯¯å·®å’Œå¯è§æ€§çš„åŸå§‹è¡¨ç¤ºï¼ŒåŒ…å«æœ‰æ„ä¹‰çš„ä¸ç¡®å®šæ€§ä¿¡æ¯ã€‚é€šè¿‡æŠ•å½±è®­ç»ƒè¯¯å·®å’Œå¯è§æ€§æ¥è·å¾—è¿™ç§è¡¨ç¤ºã€‚é€šè¿‡æ¸²æŸ“è¿™äº›æ–°é¢–è§†å›¾çš„ä¸ç¡®å®šæ€§åŸå§‹è¡¨ç¤ºï¼Œæˆ‘ä»¬è·å¾—äº†ä¸ç¡®å®šæ€§ç‰¹å¾å›¾ã€‚ä¸ºäº†æ±‡æ€»è¿™äº›ä¸ç¡®å®šæ€§ç‰¹å¾å›¾ï¼Œæˆ‘ä»¬å¯¹ä¿ç•™æ•°æ®æ‰§è¡Œåƒç´ çº§å›å½’ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åˆ†æä¸åŒç»„ä»¶å’Œè€ƒè™‘å‰æ™¯ä¸èƒŒæ™¯çš„åˆ†ç¦»æ•ˆæœæ—¶è¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œä¸çœŸå®è¯¯å·®é«˜åº¦ç›¸å…³ï¼Œå¹¶ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å‰æ™¯å¯¹è±¡ä¸Šã€‚è®­ç»ƒçš„å›å½’æ¨¡å‹å…·æœ‰å¯¹æ–°åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ— éœ€ä¿ç•™æ•°æ®å³å¯è¿›è¡Œä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ç”¨äºé«˜æ–¯æ··åˆæ¨¡å‹ä¸­çš„ä¸ç¡®å®šæ€§ä¼°è®¡çš„æ–°æ–¹æ³•ã€‚</li>
<li>å»ºç«‹äº†è®­ç»ƒè§†å›¾çš„è¯¯å·®å’Œå¯è§æ€§çš„åŸå§‹è¡¨ç¤ºï¼ŒåŒ…å«æœ‰æ„ä¹‰çš„ä¸ç¡®å®šæ€§ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡æ¸²æŸ“æ–°é¢–è§†å›¾çš„ä¸ç¡®å®šæ€§åŸå§‹è¡¨ç¤ºè·å¾—ä¸ç¡®å®šæ€§ç‰¹å¾å›¾ã€‚</li>
<li>é€šè¿‡åƒç´ çº§å›å½’æ±‡æ€»ä¸ç¡®å®šæ€§ç‰¹å¾å›¾ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸çœŸå®è¯¯å·®é«˜åº¦ç›¸å…³ï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>è®­ç»ƒçš„å›å½’æ¨¡å‹å…·æœ‰å¯¹æ–°åœºæ™¯çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02443">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fd84d36eeb4452e09e6b2183b7fc4b5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41b718d8ec6ccf47cf2eae2867b3be1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a336803d49d8ae8c234514500fbbebf.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SplatSSC-Decoupled-Depth-Guided-Gaussian-Splatting-for-Semantic-Scene-Completion"><a href="#SplatSSC-Decoupled-Depth-Guided-Gaussian-Splatting-for-Semantic-Scene-Completion" class="headerlink" title="SplatSSC: Decoupled Depth-Guided Gaussian Splatting for Semantic Scene   Completion"></a>SplatSSC: Decoupled Depth-Guided Gaussian Splatting for Semantic Scene   Completion</h2><p><strong>Authors:Rui Qian, Haozhi Cao, Tianchen Deng, Shenghai Yuan, Lihua Xie</strong></p>
<p>Monocular 3D Semantic Scene Completion (SSC) is a challenging yet promising task that aims to infer dense geometric and semantic descriptions of a scene from a single image. While recent object-centric paradigms significantly improve efficiency by leveraging flexible 3D Gaussian primitives, they still rely heavily on a large number of randomly initialized primitives, which inevitably leads to 1) inefficient primitive initialization and 2) outlier primitives that introduce erroneous artifacts. In this paper, we propose SplatSSC, a novel framework that resolves these limitations with a depth-guided initialization strategy and a principled Gaussian aggregator. Instead of random initialization, SplatSSC utilizes a dedicated depth branch composed of a Group-wise Multi-scale Fusion (GMF) module, which integrates multi-scale image and depth features to generate a sparse yet representative set of initial Gaussian primitives. To mitigate noise from outlier primitives, we develop the Decoupled Gaussian Aggregator (DGA), which enhances robustness by decomposing geometric and semantic predictions during the Gaussian-to-voxel splatting process. Complemented with a specialized Probability Scale Loss, our method achieves state-of-the-art performance on the Occ-ScanNet dataset, outperforming prior approaches by over 6.3% in IoU and 4.1% in mIoU, while reducing both latency and memory consumption by more than 9.3%. The code will be released upon acceptance. </p>
<blockquote>
<p>å•ç›®3Dè¯­ä¹‰åœºæ™¯è¡¥å…¨ï¼ˆSSCï¼‰æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§ä½†å‰æ™¯å…‰æ˜çš„ä»»åŠ¡ï¼Œæ—¨åœ¨ä»å•å¼ å›¾åƒä¸­æ¨æ–­åœºæ™¯çš„å¯†é›†å‡ ä½•å’Œè¯­ä¹‰æè¿°ã€‚è™½ç„¶æœ€è¿‘çš„ä»¥å¯¹è±¡ä¸ºä¸­å¿ƒçš„æ¨¡å¼é€šè¿‡åˆ©ç”¨çµæ´»çš„3Dé«˜æ–¯åŸå§‹æ•°æ®æ˜¾è‘—æé«˜äº†æ•ˆç‡ï¼Œä½†å®ƒä»¬ä»ç„¶ä¸¥é‡ä¾èµ–äºå¤§é‡éšæœºåˆå§‹åŒ–çš„åŸå§‹æ•°æ®ï¼Œè¿™ä¸å¯é¿å…åœ°å¯¼è‡´1ï¼‰åŸå§‹æ•°æ®åˆå§‹åŒ–æ•ˆç‡ä½ä¸‹å’Œ2ï¼‰å¼‚å¸¸åŸå§‹æ•°æ®å¼•å…¥é”™è¯¯ä¼ªå½±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SplatSSCï¼Œè¿™æ˜¯ä¸€ç§è§£å†³è¿™äº›é™åˆ¶çš„æ–°é¢–æ¡†æ¶ï¼Œé‡‡ç”¨æ·±åº¦å¼•å¯¼åˆå§‹åŒ–ç­–ç•¥å’Œæœ‰åŸåˆ™çš„é«˜æ–¯èšåˆå™¨ã€‚SplatSSCä¸æ˜¯è¿›è¡Œéšæœºåˆå§‹åŒ–ï¼Œè€Œæ˜¯åˆ©ç”¨ç”±ç»„çº§å¤šå°ºåº¦èåˆï¼ˆGMFï¼‰æ¨¡å—ç»„æˆçš„ä¸“ç”¨æ·±åº¦åˆ†æ”¯ï¼Œè¯¥æ¨¡å—èåˆå¤šå°ºåº¦å›¾åƒå’Œæ·±åº¦ç‰¹å¾æ¥ç”Ÿæˆç¨€ç–ä½†æœ‰ä»£è¡¨æ€§çš„åˆå§‹é«˜æ–¯åŸå§‹æ•°æ®é›†ã€‚ä¸ºäº†å‡è½»å¼‚å¸¸åŸå§‹æ•°æ®å¸¦æ¥çš„å™ªå£°ï¼Œæˆ‘ä»¬å¼€å‘äº†å»è€¦é«˜æ–¯èšåˆå™¨ï¼ˆDGAï¼‰ï¼Œé€šè¿‡åœ¨é«˜æ–¯åˆ°ä½“ç´ è´´å›¾è¿‡ç¨‹ä¸­åˆ†è§£å‡ ä½•å’Œè¯­ä¹‰é¢„æµ‹ï¼Œæé«˜äº†æ–¹æ³•çš„ç¨³å¥æ€§ã€‚ç»“åˆä¸“ç”¨çš„æ¦‚ç‡å°ºåº¦æŸå¤±ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨Occ-ScanNetæ•°æ®é›†ä¸Šå®ç°äº†æœ€æ–°æ€§èƒ½ï¼Œåœ¨IoUå’ŒmIoUä¸Šåˆ†åˆ«è¶…è¶Šäº†å…ˆå‰çš„æ–¹æ³•6.3%å’Œ4.1%ï¼ŒåŒæ—¶é™ä½äº†è¶…è¿‡9.3%çš„å»¶è¿Ÿå’Œå†…å­˜æ¶ˆè€—ã€‚ä»£ç åœ¨æ¥å—åå°†äºˆä»¥å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02261v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSplatSSCçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºè§£å†³å•ç›®ä¸‰ç»´è¯­ä¹‰åœºæ™¯å®Œæˆï¼ˆSSCï¼‰ä»»åŠ¡ä¸­é‡åˆ°çš„åŸå§‹åˆå§‹åŒ–æ•ˆç‡ä½ä¸‹å’Œå¼‚å¸¸åŸå§‹å¼•å…¥é”™è¯¯ä¼ªå½±çš„é—®é¢˜ã€‚é€šè¿‡æ·±åº¦å¼•å¯¼çš„åˆå§‹åŒ–å’ŒåŸåˆ™æ€§çš„é«˜æ–¯èšé›†ç­–ç•¥ï¼ŒSplatSSCæ”¹å–„äº†ä¼ ç»Ÿéšæœºåˆå§‹åŒ–æ–¹æ³•ï¼Œå®ç°äº†é«˜æ•ˆä¸”å‡†ç¡®çš„åœºæ™¯æè¿°ã€‚åœ¨Occ-ScanNetæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæé«˜äº†äº¤å¹¶æ¯”ï¼ˆIoUï¼‰å’Œå¹³å‡äº¤å¹¶æ¯”ï¼ˆmIoUï¼‰è¶…è¿‡å…ˆå‰çš„æ–¹æ³•ï¼ŒåŒæ—¶é™ä½äº†å»¶è¿Ÿå’Œå†…å­˜æ¶ˆè€—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å•ç›®ä¸‰ç»´è¯­ä¹‰åœºæ™¯å®Œæˆï¼ˆSSCï¼‰æ—¨åœ¨ä»å•ä¸€å›¾åƒä¸­æ¨æ–­åœºæ™¯çš„å¯†é›†å‡ ä½•å’Œè¯­ä¹‰æè¿°ï¼Œæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜ä½†å‰æ™¯å¹¿é˜”çš„ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰å¯¹è±¡ä¸­å¿ƒèŒƒå¼è™½ç„¶é€šè¿‡çµæ´»çš„3Dé«˜æ–¯åŸå§‹æé«˜äº†æ•ˆç‡ï¼Œä½†ä»å­˜åœ¨åŸå§‹åˆå§‹åŒ–æ•ˆç‡ä½ä¸‹å’Œå¼‚å¸¸åŸå§‹å¼•å…¥é”™è¯¯ä¼ªå½±çš„é—®é¢˜ã€‚</li>
<li>SplatSSCæ¡†æ¶é€šè¿‡æ·±åº¦å¼•å¯¼çš„åˆå§‹åŒ–å’ŒåŸåˆ™æ€§çš„é«˜æ–¯èšé›†ç­–ç•¥è§£å†³äº†è¿™äº›é—®é¢˜ã€‚</li>
<li>SplatSSCä½¿ç”¨ä¸“é—¨çš„æ·±åº¦åˆ†æ”¯å’ŒGroup-wise Multi-scale Fusionï¼ˆGMFï¼‰æ¨¡å—ç”Ÿæˆç¨€ç–ä½†æœ‰ä»£è¡¨æ€§çš„åˆå§‹é«˜æ–¯åŸå§‹ã€‚</li>
<li>Decoupled Gaussian Aggregatorï¼ˆDGAï¼‰çš„å¼€å‘å‡è½»äº†å¼‚å¸¸åŸå§‹å¸¦æ¥çš„å™ªå£°ï¼Œé€šè¿‡åœ¨é«˜æ–¯åˆ°ä½“ç´ æº…å‡ºè¿‡ç¨‹ä¸­åˆ†è§£å‡ ä½•å’Œè¯­ä¹‰é¢„æµ‹å¢å¼ºäº†ç¨³å¥æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02261">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5b0df6296c83d2db5a9c659b10d60dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ac9a0a9d59a915a6f48e0b7e6c63dc6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a70d797503a71d46634136d87713c7c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f44a5c8f4291960b356295b4b35ea01d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b0149b2d046efa2741088a74368cd01.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="GaussianCross-Cross-modal-Self-supervised-3D-Representation-Learning-via-Gaussian-Splatting"><a href="#GaussianCross-Cross-modal-Self-supervised-3D-Representation-Learning-via-Gaussian-Splatting" class="headerlink" title="GaussianCross: Cross-modal Self-supervised 3D Representation Learning   via Gaussian Splatting"></a>GaussianCross: Cross-modal Self-supervised 3D Representation Learning   via Gaussian Splatting</h2><p><strong>Authors:Lei Yao, Yi Wang, Yi Zhang, Moyun Liu, Lap-Pui Chau</strong></p>
<p>The significance of informative and robust point representations has been widely acknowledged for 3D scene understanding. Despite existing self-supervised pre-training counterparts demonstrating promising performance, the model collapse and structural information deficiency remain prevalent due to insufficient point discrimination difficulty, yielding unreliable expressions and suboptimal performance. In this paper, we present GaussianCross, a novel cross-modal self-supervised 3D representation learning architecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques to address current challenges. GaussianCross seamlessly converts scale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian representation without missing details, enabling stable and generalizable pre-training. Subsequently, a tri-attribute adaptive distillation splatting module is incorporated to construct a 3D feature field, facilitating synergetic feature capturing of appearance, geometry, and semantic cues to maintain cross-modal consistency. To validate GaussianCross, we perform extensive evaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In particular, GaussianCross shows a prominent parameter and data efficiency, achieving superior performance through linear probing (&lt;0.1% parameters) and limited data training (1% of scenes) compared to state-of-the-art methods. Furthermore, GaussianCross demonstrates strong generalization capabilities, improving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on ScanNet200 semantic and instance segmentation tasks, respectively, supporting the effectiveness of our approach. The code, weights, and visualizations are publicly available at \href{<a target="_blank" rel="noopener" href="https://rayyoh.github.io/GaussianCross/%7D%7Bhttps://rayyoh.github.io/GaussianCross/%7D">https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}</a>. </p>
<blockquote>
<p>ç‚¹äº‘ä¿¡æ¯çš„è¡¨è¾¾å’Œç¨³å¥æ€§å¯¹äº3Dåœºæ™¯ç†è§£å…·æœ‰é‡è¦æ„ä¹‰ã€‚å°½ç®¡ç°æœ‰çš„è‡ªç›‘ç£é¢„è®­ç»ƒæ¨¡å‹è¡¨ç°å‡ºæœ‰å‰æ™¯çš„æ€§èƒ½ï¼Œä½†ç”±äºç‚¹äº‘è¾¨åˆ«éš¾åº¦ä¸è¶³ï¼Œæ¨¡å‹å´©æºƒå’Œç»“æ„ä¿¡æ¯ç¼ºå¤±çš„é—®é¢˜ä»ç„¶æ™®éå­˜åœ¨ï¼Œå¯¼è‡´è¡¨è¾¾ä¸å¯é å’Œæ€§èƒ½ä¸ä½³ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†GaussianCrossï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è·¨æ¨¡æ€è‡ªç›‘ç£3Dè¡¨ç¤ºå­¦ä¹ æ¶æ„ï¼Œå®ƒé›†æˆäº†å‰é¦ˆ3Dé«˜æ–¯å–·å°„ï¼ˆ3DGSï¼‰æŠ€æœ¯ã€‚GaussianCrossèƒ½å¤Ÿæ— ç¼åœ°å°†å°ºåº¦ä¸ä¸€è‡´çš„3Dç‚¹äº‘è½¬æ¢ä¸ºç»Ÿä¸€çš„ç«‹æ–¹ä½“å½’ä¸€åŒ–é«˜æ–¯è¡¨ç¤ºï¼Œè€Œä¸æŸå¤±ä»»ä½•ç»†èŠ‚ï¼Œä»è€Œå®ç°ç¨³å®šå’Œå¯æ³›åŒ–çš„é¢„è®­ç»ƒã€‚éšåï¼Œèå…¥äº†ä¸€ä¸ªä¸‰å±æ€§è‡ªé€‚åº”è’¸é¦å–·å°„æ¨¡å—ï¼Œä»¥æ„å»º3Dç‰¹å¾åœºï¼Œè¿™æœ‰åŠ©äºååŒæ•è·å¤–è§‚ã€å‡ ä½•å’Œè¯­ä¹‰çº¿ç´¢çš„ç‰¹å¾ï¼Œä»¥ä¿æŒè·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚ä¸ºäº†éªŒè¯GaussianCrossçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨å„ç§åŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼ŒåŒ…æ‹¬ScanNetã€ScanNet200å’ŒS3DISã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒGaussianCrossåœ¨å‚æ•°å’Œæ•°æ®æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œé€šè¿‡çº¿æ€§æ¢æµ‹ï¼ˆ&lt;0.1%çš„å‚æ•°ï¼‰å’Œæœ‰é™æ•°æ®è®­ç»ƒï¼ˆ1%çš„åœºæ™¯ï¼‰å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒGaussianCrossæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨ScanNet200çš„è¯­ä¹‰å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šï¼Œå…¨ç²¾ç»†è°ƒæ•´å‡†ç¡®åº¦æé«˜äº†9.3% mIoUå’Œ6.1% AP50ï¼Œè¿™æ”¯æŒäº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç ã€æƒé‡å’Œå¯è§†åŒ–ç»“æœå¯åœ¨[<a target="_blank" rel="noopener" href="https://rayyoh.github.io/GaussianCross/]%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://rayyoh.github.io/GaussianCross/]å…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02172v1">PDF</a> 14 pages, 8 figures, accepted by MMâ€™25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºGaussianCrossçš„æ–°å‹è·¨æ¨¡æ€è‡ªç›‘ç£ä¸‰ç»´è¡¨ç¤ºå­¦ä¹ æ¶æ„ï¼Œé›†æˆå‰é¦ˆä¸‰ç»´é«˜æ–¯å¡‘å½¢æŠ€æœ¯ï¼ˆ3DGSï¼‰ï¼Œè§£å†³ç°æœ‰æ¨¡å‹åœ¨ç‚¹äº‘å¤„ç†ä¸­çš„åå¡Œå’Œç»“æ„ä¿¡æ¯ç¼ºå¤±é—®é¢˜ã€‚GaussianCrosså¯å°†å°ºåº¦ä¸ä¸€è‡´çš„ä¸‰ç»´ç‚¹äº‘è½¬æ¢ä¸ºç»Ÿä¸€çš„ç«‹æ–¹ä½“å½’ä¸€åŒ–é«˜æ–¯è¡¨ç¤ºï¼ŒåŒæ—¶ä¿ç•™ç»†èŠ‚ï¼Œå®ç°ç¨³å®šå’Œå¯æ³›åŒ–çš„é¢„è®­ç»ƒã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥ä¸‰å±æ€§è‡ªé€‚åº”è’¸é¦å¡‘å½¢æ¨¡å—æ„å»ºä¸‰ç»´ç‰¹å¾åœºï¼Œèåˆå¤–è§‚ã€å‡ ä½•å’Œè¯­ä¹‰çº¿ç´¢ï¼Œä¿æŒè·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚åœ¨ScanNetã€ScanNet200å’ŒS3DISç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒGaussianCrosså±•ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œå‚æ•°ã€æ•°æ®æ•ˆç‡ã€‚å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›åœ¨ScanNet200è¯­ä¹‰å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šåˆ†åˆ«æé«˜äº†9.3% mIoUå’Œ6.1% AP50ã€‚ä»£ç ã€æƒé‡å’Œå¯è§†åŒ–å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GaussianCrossæ˜¯ä¸€ç§æ–°å‹è·¨æ¨¡æ€è‡ªç›‘ç£ä¸‰ç»´è¡¨ç¤ºå­¦ä¹ æ¶æ„ï¼Œé›†æˆ3DGSæŠ€æœ¯è§£å†³æ¨¡å‹åå¡Œå’Œä¿¡æ¯ç¼ºå¤±é—®é¢˜ã€‚</li>
<li>GaussianCrosså¯å°†å°ºåº¦ä¸ä¸€è‡´çš„ç‚¹äº‘è½¬æ¢ä¸ºç»Ÿä¸€çš„é«˜æ–¯è¡¨ç¤ºï¼Œå®ç°ç¨³å®šä¸”å¯æ³›åŒ–çš„é¢„è®­ç»ƒã€‚</li>
<li>ä¸‰å±æ€§è‡ªé€‚åº”è’¸é¦å¡‘å½¢æ¨¡å—ç”¨äºæ„å»ºä¸‰ç»´ç‰¹å¾åœºï¼Œèåˆå¤šç§çº¿ç´¢ï¼Œä¿æŒè·¨æ¨¡æ€ä¸€è‡´æ€§ã€‚</li>
<li>GaussianCrossåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå‚æ•°å’Œæ•°æ®æ•ˆç‡é«˜ã€‚</li>
<li>GaussianCrossåœ¨è¯­ä¹‰å’Œå®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä»£ç ã€æƒé‡å’Œå¯è§†åŒ–å·²å…¬å¼€ä¾›å…¬ä¼—è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02172">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d00798a2e5680ef7d323ba7e809bce2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21ca576e212c0afc364b1bcf1c751a69.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba9980705229881f3a518f41b85dc721.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ScrewSplat-An-End-to-End-Method-for-Articulated-Object-Recognition"><a href="#ScrewSplat-An-End-to-End-Method-for-Articulated-Object-Recognition" class="headerlink" title="ScrewSplat: An End-to-End Method for Articulated Object Recognition"></a>ScrewSplat: An End-to-End Method for Articulated Object Recognition</h2><p><strong>Authors:Seungyeon Kim, Junsu Ha, Young Hun Kim, Yonghyeon Lee, Frank C. Park</strong></p>
<p>Articulated object recognition â€“ the task of identifying both the geometry and kinematic joints of objects with movable parts â€“ is essential for enabling robots to interact with everyday objects such as doors and laptops. However, existing approaches often rely on strong assumptions, such as a known number of articulated parts; require additional inputs, such as depth images; or involve complex intermediate steps that can introduce potential errors â€“ limiting their practicality in real-world settings. In this paper, we introduce ScrewSplat, a simple end-to-end method that operates solely on RGB observations. Our approach begins by randomly initializing screw axes, which are then iteratively optimized to recover the objectâ€™s underlying kinematic structure. By integrating with Gaussian Splatting, we simultaneously reconstruct the 3D geometry and segment the object into rigid, movable parts. We demonstrate that our method achieves state-of-the-art recognition accuracy across a diverse set of articulated objects, and further enables zero-shot, text-guided manipulation using the recovered kinematic model. </p>
<blockquote>
<p>å…³èŠ‚å¯¹è±¡è¯†åˆ«â€”â€”è¯†åˆ«å…·æœ‰å¯åŠ¨éƒ¨ä»¶çš„å¯¹è±¡çš„å‡ ä½•å½¢çŠ¶å’Œè¿åŠ¨å…³èŠ‚çš„ä»»åŠ¡â€”â€”æ˜¯å®ç°æœºå™¨äººä¸é—¨ã€ç¬”è®°æœ¬ç”µè„‘ç­‰æ—¥å¸¸å¯¹è±¡äº¤äº’çš„å…³é”®ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå¼ºå¤§çš„å‡è®¾ï¼Œä¾‹å¦‚å·²çŸ¥å…³èŠ‚éƒ¨ä»¶çš„æ•°é‡ï¼›éœ€è¦é¢å¤–çš„è¾“å…¥ï¼Œå¦‚æ·±åº¦å›¾åƒï¼›æˆ–æ¶‰åŠå¯èƒ½å¼•å…¥æ½œåœ¨é”™è¯¯çš„å¤æ‚ä¸­é—´æ­¥éª¤â€”â€”è¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œçš„å®ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ScrewSplatï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„ç«¯åˆ°ç«¯æ–¹æ³•ï¼Œä»…ä½¿ç”¨RGBè§‚å¯Ÿç»“æœã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡éšæœºåˆå§‹åŒ–èºæ†è½´å¼€å§‹ï¼Œç„¶åå¯¹å…¶è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œä»¥æ¢å¤å¯¹è±¡çš„åº•å±‚è¿åŠ¨å­¦ç»“æ„ã€‚é€šè¿‡ä¸é«˜æ–¯æ‹¼è´´ç›¸ç»“åˆï¼Œæˆ‘ä»¬åŒæ—¶é‡å»ºäº†å¯¹è±¡çš„3Då‡ ä½•å½¢çŠ¶ï¼Œå¹¶å°†å…¶åˆ†å‰²æˆåˆšæ€§å¯ç§»åŠ¨éƒ¨ä»¶ã€‚æˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§å…³èŠ‚å¯¹è±¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è¯†åˆ«ç²¾åº¦ï¼Œå¹¶è¿›ä¸€æ­¥ä½¿ç”¨æ¢å¤çš„è¿åŠ¨å­¦æ¨¡å‹å®ç°äº†é›¶æ ·æœ¬ã€æ–‡æœ¬å¼•å¯¼çš„æ“æ§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02146v1">PDF</a> 26 pages, 12 figures, Conference on Robot Learning (CoRL) 2025</p>
<p><strong>æ‘˜è¦</strong><br>     æœºå™¨äººå¯¹å¸¦æœ‰æ´»åŠ¨éƒ¨ä»¶çš„æ—¥å¸¸ç‰©ä½“çš„äº’åŠ¨è¯†åˆ«è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬å¯¹ç‰©ä½“çš„å‡ ä½•å½¢çŠ¶å’Œæ´»åŠ¨å…³èŠ‚çš„è¯†åˆ«ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•å¾€å¾€ä¾èµ–äºå¼ºçƒˆçš„å‡è®¾ï¼Œå¦‚å·²çŸ¥çš„æ´»åŠ¨éƒ¨ä»¶æ•°é‡ï¼›éœ€è¦é¢å¤–çš„è¾“å…¥ï¼Œå¦‚æ·±åº¦å›¾åƒï¼›æˆ–è€…æ¶‰åŠå¯èƒ½å¼•å…¥é”™è¯¯çš„å¤æ‚ä¸­é—´æ­¥éª¤ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œçš„å®é™…åº”ç”¨ä¸­å—åˆ°é™åˆ¶ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºScrewSplatçš„ç®€å•ç«¯åˆ°ç«¯æ–¹æ³•ï¼Œå®ƒä»…ä½¿ç”¨RGBè§‚å¯Ÿç»“æœè¿›è¡Œæ“ä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡éšæœºåˆå§‹åŒ–èºä¸è½´å¼€å§‹ï¼Œç„¶åå¯¹å…¶è¿›è¡Œè¿­ä»£ä¼˜åŒ–ï¼Œä»¥æ¢å¤ç‰©ä½“çš„åº•å±‚è¿åŠ¨å­¦ç»“æ„ã€‚é€šè¿‡ä¸é«˜æ–¯è´´å›¾æŠ€æœ¯çš„ç»“åˆï¼Œæˆ‘ä»¬åŒæ—¶é‡å»ºäº†ç‰©ä½“çš„ä¸‰ç»´å‡ ä½•å½¢çŠ¶ï¼Œå¹¶å°†ç‰©ä½“åˆ†å‰²æˆåˆšæ€§ã€å¯ç§»åŠ¨çš„éƒ¨åˆ†ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§æ´»åŠ¨ç‰©ä½“ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è¯†åˆ«ç²¾åº¦ï¼Œå¹¶è¿›ä¸€æ­¥åˆ©ç”¨æ¢å¤çš„è¿åŠ¨å­¦æ¨¡å‹å®ç°äº†é›¶æ ·æœ¬æ–‡æœ¬æŒ‡å¯¼æ“ä½œã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æœºå™¨äººå¯¹æ—¥å¸¸ç‰©ä½“çš„äº’åŠ¨è¯†åˆ«éå¸¸é‡è¦ï¼Œæ¶‰åŠè¯†åˆ«ç‰©ä½“çš„å‡ ä½•å½¢çŠ¶å’Œæ´»åŠ¨å…³èŠ‚ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¦‚å¼ºçƒˆå‡è®¾ã€éœ€è¦é¢å¤–è¾“å…¥å’Œå¤æ‚ä¸­é—´æ­¥éª¤ã€‚</li>
<li>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºScrewSplatçš„ç®€æ´ç«¯åˆ°ç«¯æ–¹æ³•ï¼Œä»…ä½¿ç”¨RGBè§‚å¯Ÿç»“æœã€‚</li>
<li>ScrewSplaté€šè¿‡éšæœºåˆå§‹åŒ–èºä¸è½´å¹¶è¿­ä»£ä¼˜åŒ–ï¼Œä»¥æ¢å¤ç‰©ä½“çš„åº•å±‚è¿åŠ¨å­¦ç»“æ„ã€‚</li>
<li>ç»“åˆé«˜æ–¯è´´å›¾æŠ€æœ¯ï¼ŒScrewSplatåŒæ—¶é‡å»ºäº†ä¸‰ç»´å‡ ä½•å½¢çŠ¶å¹¶å°†ç‰©ä½“åˆ†å‰²æˆå¯ç§»åŠ¨éƒ¨åˆ†ã€‚</li>
<li>ScrewSplatåœ¨å¤šç§æ´»åŠ¨ç‰©ä½“ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„è¯†åˆ«ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02146">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6d34cf027991968dd0d1f22ef8f1567a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6853f3bb5b1f7bcfd4c0da8bf95a38c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02edcf145c54c18fc762f06d7c9e1815.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af1205b2939f71d240eba585368c444f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LT-Gaussian-Long-Term-Map-Update-Using-3D-Gaussian-Splatting-for-Autonomous-Driving"><a href="#LT-Gaussian-Long-Term-Map-Update-Using-3D-Gaussian-Splatting-for-Autonomous-Driving" class="headerlink" title="LT-Gaussian: Long-Term Map Update Using 3D Gaussian Splatting for   Autonomous Driving"></a>LT-Gaussian: Long-Term Map Update Using 3D Gaussian Splatting for   Autonomous Driving</h2><p><strong>Authors:Luqi Cheng, Zhangshuo Qi, Zijie Zhou, Chao Lu, Guangming Xiong</strong></p>
<p>Maps play an important role in autonomous driving systems. The recently proposed 3D Gaussian Splatting (3D-GS) produces rendering-quality explicit scene reconstruction results, demonstrating the potential for map construction in autonomous driving scenarios. However, because of the time and computational costs involved in generating Gaussian scenes, how to update the map becomes a significant challenge. In this paper, we propose LT-Gaussian, a map update method for 3D-GS-based maps. LT-Gaussian consists of three main components: Multimodal Gaussian Splatting, Structural Change Detection Module, and Gaussian-Map Update Module. Firstly, the Gaussian map of the old scene is generated using our proposed Multimodal Gaussian Splatting. Subsequently, during the map update process, we compare the outdated Gaussian map with the current LiDAR data stream to identify structural changes. Finally, we perform targeted updates to the Gaussian-map to generate an up-to-date map. We establish a benchmark for map updating on the nuScenes dataset to quantitatively evaluate our method. The experimental results show that LT-Gaussian can effectively and efficiently update the Gaussian-map, handling common environmental changes in autonomous driving scenarios. Furthermore, by taking full advantage of information from both new and old scenes, LT-Gaussian is able to produce higher quality reconstruction results compared to map update strategies that reconstruct maps from scratch. Our open-source code is available at <a target="_blank" rel="noopener" href="https://github.com/ChengLuqi/LT-gaussian">https://github.com/ChengLuqi/LT-gaussian</a>. </p>
<blockquote>
<p>åœ°å›¾åœ¨è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚æœ€è¿‘æå‡ºçš„3Dé«˜æ–¯å±•å¸ƒï¼ˆ3D-GSï¼‰æŠ€æœ¯èƒ½å¤Ÿäº§ç”Ÿé«˜è´¨é‡çš„æ¸²æŸ“æ•ˆæœï¼Œå±•ç°å‡ºåœ¨è‡ªåŠ¨é©¾é©¶åœºæ™¯æ„å»ºåœ°å›¾çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºç”Ÿæˆé«˜æ–¯åœºæ™¯æ‰€éœ€çš„æ—¶é—´å’Œè®¡ç®—æˆæœ¬ï¼Œå¦‚ä½•æ›´æ–°åœ°å›¾æˆä¸ºäº†ä¸€å¤§æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäº3D-GSåœ°å›¾çš„åœ°å›¾æ›´æ–°æ–¹æ³•LT-Gaussianã€‚LT-Gaussianä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªç»„æˆéƒ¨åˆ†ï¼šå¤šæ¨¡æ€é«˜æ–¯å±•å¸ƒã€ç»“æ„å˜åŒ–æ£€æµ‹æ¨¡å—å’Œé«˜æ–¯åœ°å›¾æ›´æ–°æ¨¡å—ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨æå‡ºçš„å¤šæ¨¡æ€é«˜æ–¯å±•å¸ƒç”Ÿæˆæ—§åœºæ™¯çš„é«˜æ–¯åœ°å›¾ã€‚éšåï¼Œåœ¨åœ°å›¾æ›´æ–°è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†è¿‡æ—¶çš„é«˜æ–¯åœ°å›¾ä¸å½“å‰çš„æ¿€å…‰é›·è¾¾æ•°æ®æµè¿›è¡Œæ¯”è¾ƒï¼Œä»¥è¯†åˆ«ç»“æ„å˜åŒ–ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹é«˜æ–¯åœ°å›¾è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„æ›´æ–°ï¼Œä»¥ç”Ÿæˆæœ€æ–°çš„åœ°å›¾ã€‚æˆ‘ä»¬åœ¨nuScenesæ•°æ®é›†ä¸Šå»ºç«‹äº†åœ°å›¾æ›´æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œä»¥å®šé‡è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLT-Gaussianèƒ½å¤Ÿæœ‰æ•ˆä¸”é«˜æ•ˆåœ°æ›´æ–°é«˜æ–¯åœ°å›¾ï¼Œå¤„ç†è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­å¸¸è§çš„ç¯å¢ƒå˜åŒ–ã€‚æ­¤å¤–ï¼Œé€šè¿‡å……åˆ†åˆ©ç”¨æ–°æ—§åœºæ™¯çš„ä¿¡æ¯ï¼ŒLT-Gaussianèƒ½å¤Ÿäº§ç”Ÿæ¯”é‚£äº›ä»å¤´å¼€å§‹é‡å»ºçš„åœ°å›¾æ›´æ–°ç­–ç•¥æ›´é«˜è´¨é‡çš„é‡å»ºç»“æœã€‚æˆ‘ä»¬çš„å¼€æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ChengLuqi/LT-gaussian%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ChengLuqi/LT-gaussianæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01704v1">PDF</a> Accepted by IV 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¸‰ç»´é«˜æ–¯æ˜ å°„çš„åœ°å›¾æ›´æ–°æ–¹æ³•å¯¹äºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿè‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºçš„LT-Gaussianç®—æ³•é’ˆå¯¹åœ°å›¾æ›´æ–°æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡å¼•å…¥ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€é«˜æ–¯æ‹¼è´´ã€ç»“æ„å˜åŒ–æ£€æµ‹æ¨¡å—å’Œé«˜æ–¯åœ°å›¾æ›´æ–°æ¨¡å—ï¼ŒLT-Gaussianèƒ½å¤Ÿé«˜æ•ˆä¸”æœ‰æ•ˆåœ°æ›´æ–°é«˜æ–¯åœ°å›¾ï¼Œå¹¶å¤„ç†è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­çš„å¸¸è§ç¯å¢ƒå˜åŒ–ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•åˆ©ç”¨æ–°æ—§åœºæ™¯çš„ä¿¡æ¯ä¼˜åŠ¿ï¼Œç”Ÿæˆé«˜è´¨é‡çš„é‡å»ºç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LT-Gaussiané’ˆå¯¹åŸºäºä¸‰ç»´é«˜æ–¯æ˜ å°„çš„åœ°å›¾æ›´æ–°æå‡ºäº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>LT-GaussianåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå¤šæ¨¡æ€é«˜æ–¯æ‹¼è´´ã€ç»“æ„å˜åŒ–æ£€æµ‹æ¨¡å—å’Œé«˜æ–¯åœ°å›¾æ›´æ–°æ¨¡å—ã€‚</li>
<li>å¤šæ¨¡æ€é«˜æ–¯æ‹¼è´´è¢«ç”¨æ¥ç”Ÿæˆæ—§çš„åœºæ™¯çš„é«˜æ–¯åœ°å›¾ã€‚</li>
<li>åœ¨åœ°å›¾æ›´æ–°è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡ä¸å½“å‰æ¿€å…‰é›·è¾¾æ•°æ®æµçš„å¯¹æ¯”ï¼Œæ£€æµ‹ç»“æ„å˜åŒ–ã€‚</li>
<li>é’ˆå¯¹å˜åŒ–éƒ¨åˆ†è¿›è¡Œçš„é«˜æ–¯åœ°å›¾æ›´æ–°èƒ½å¤Ÿç”Ÿæˆæ›´æ–°çš„åœ°å›¾ã€‚</li>
<li>åœ¨nuScenesæ•°æ®é›†ä¸Šå»ºç«‹çš„åœ°å›¾æ›´æ–°åŸºå‡†æµ‹è¯•è¯æ˜LT-Gaussianèƒ½æœ‰æ•ˆä¸”é«˜æ•ˆåœ°æ›´æ–°é«˜æ–¯åœ°å›¾ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01704">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-46e69ea7ba130f25abe1419d71eb1574.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2163f01c65cb8c76f7717e53237cf3a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b5e6b31a69782d7cd35317164709dd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e9ce1e4e8264dd49652444aecdea365.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c179231a9725a30ac661e30b8f908f73.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DisCo3D-Distilling-Multi-View-Consistency-for-3D-Scene-Editing"><a href="#DisCo3D-Distilling-Multi-View-Consistency-for-3D-Scene-Editing" class="headerlink" title="DisCo3D: Distilling Multi-View Consistency for 3D Scene Editing"></a>DisCo3D: Distilling Multi-View Consistency for 3D Scene Editing</h2><p><strong>Authors:Yufeng Chi, Huimin Ma, Kafeng Wang, Jianmin Li</strong></p>
<p>While diffusion models have demonstrated remarkable progress in 2D image generation and editing, extending these capabilities to 3D editing remains challenging, particularly in maintaining multi-view consistency. Classical approaches typically update 3D representations through iterative refinement based on a single editing view. However, these methods often suffer from slow convergence and blurry artifacts caused by cross-view inconsistencies. Recent methods improve efficiency by propagating 2D editing attention features, yet still exhibit fine-grained inconsistencies and failure modes in complex scenes due to insufficient constraints. To address this, we propose \textbf{DisCo3D}, a novel framework that distills 3D consistency priors into a 2D editor. Our method first fine-tunes a 3D generator using multi-view inputs for scene adaptation, then trains a 2D editor through consistency distillation. The edited multi-view outputs are finally optimized into 3D representations via Gaussian Splatting. Experimental results show DisCo3D achieves stable multi-view consistency and outperforms state-of-the-art methods in editing quality. </p>
<blockquote>
<p>è™½ç„¶æ‰©æ•£æ¨¡å‹åœ¨äºŒç»´å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å°†å…¶èƒ½åŠ›æ‰©å±•åˆ°ä¸‰ç»´ç¼–è¾‘ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿æŒå¤šè§†å›¾ä¸€è‡´æ€§æ–¹é¢ã€‚ä¼ ç»Ÿçš„æ–¹æ³•é€šå¸¸åŸºäºå•ä¸ªç¼–è¾‘è§†å›¾é€šè¿‡è¿­ä»£ä¼˜åŒ–æ¥æ›´æ–°ä¸‰ç»´è¡¨ç¤ºã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸é­å—æ…¢æ”¶æ•›å’Œç”±è·¨è§†å›¾ä¸ä¸€è‡´å¯¼è‡´çš„æ¨¡ç³Šä¼ªå½±çš„å›°æ‰°ã€‚æœ€è¿‘çš„æ–¹æ³•é€šè¿‡ä¼ æ’­äºŒç»´ç¼–è¾‘æ³¨æ„åŠ›ç‰¹å¾æ¥æé«˜æ•ˆç‡ï¼Œä½†ç”±äºçº¦æŸä¸è¶³ï¼Œåœ¨å¤æ‚åœºæ™¯ä¸­ä»ç„¶å­˜åœ¨ç»†å¾®çš„ä¸ä¸€è‡´æ€§å’Œå¤±è´¥æ¨¡å¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DisCo3Dè¿™ä¸€æ–°é¢–æ¡†æ¶ï¼Œå®ƒå°†ä¸‰ç»´ä¸€è‡´æ€§å…ˆéªŒçŸ¥è¯†è’¸é¦åˆ°äºŒç»´ç¼–è¾‘å™¨ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆä½¿ç”¨å¤šè§†å›¾è¾“å…¥å¯¹ä¸‰ç»´ç”Ÿæˆå™¨è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”åœºæ™¯ï¼Œç„¶åé€šè¿‡ä¸€è‡´æ€§è’¸é¦è®­ç»ƒäºŒç»´ç¼–è¾‘å™¨ã€‚æœ€åï¼Œå°†ç¼–è¾‘åçš„å¤šè§†å›¾è¾“å‡ºé€šè¿‡é«˜æ–¯å¹³é“ºä¼˜åŒ–ä¸ºä¸‰ç»´è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDisCo3Då®ç°äº†ç¨³å®šçš„å¤šè§†å›¾ä¸€è‡´æ€§ï¼Œå¹¶åœ¨ç¼–è¾‘è´¨é‡æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01684v1">PDF</a> 17 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äº3Dç¼–è¾‘çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ä¿æŒå¤šè§†è§’ä¸€è‡´æ€§æ–¹é¢ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶DisCo3Dï¼Œå®ƒé€šè¿‡è’¸é¦3Dä¸€è‡´æ€§å…ˆéªŒçŸ¥è¯†åˆ°2Dç¼–è¾‘å™¨ä¸­ï¼Œé€šè¿‡å¾®è°ƒ3Dç”Ÿæˆå™¨å¹¶ä½¿ç”¨å¤šè§†è§’è¾“å…¥è¿›è¡Œåœºæ™¯é€‚åº”ï¼Œç„¶åè®­ç»ƒ2Dç¼–è¾‘å™¨è¿›è¡Œä¸€è‡´æ€§è’¸é¦ã€‚ç¼–è¾‘åçš„å¤šè§†è§’è¾“å‡ºé€šè¿‡é«˜æ–¯æŠ•å½±ä¼˜åŒ–ä¸º3Dè¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDisCo3Då®ç°äº†ç¨³å®šçš„å¤šè§†è§’ä¸€è‡´æ€§ï¼Œå¹¶åœ¨ç¼–è¾‘è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨3Dç¼–è¾‘ä¸­é¢ä¸´å¤šè§†è§’ä¸€è‡´æ€§ç»´æŒçš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡å•ä¸€ç¼–è¾‘è§†è§’çš„è¿­ä»£ä¼˜åŒ–æ¥æ›´æ–°3Dè¡¨ç¤ºï¼Œä½†å­˜åœ¨é€Ÿåº¦æ…¢ã€æ˜“å‡ºç°æ¨¡ç³Šä¼ªå½±çš„é—®é¢˜ã€‚</li>
<li>æœ€è¿‘çš„æ–¹æ³•é€šè¿‡ä¼ æ’­2Dç¼–è¾‘æ³¨æ„åŠ›ç‰¹å¾æé«˜äº†æ•ˆç‡ï¼Œä½†ä»å­˜åœ¨ç»†å¾®ä¸ä¸€è‡´æ€§ä»¥åŠåœ¨å¤æ‚åœºæ™¯ä¸­çš„å¤±è´¥æ¨¡å¼ã€‚</li>
<li>DisCo3Dæ¡†æ¶é€šè¿‡è’¸é¦3Dä¸€è‡´æ€§å…ˆéªŒçŸ¥è¯†åˆ°2Dç¼–è¾‘å™¨ä¸­æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>DisCo3Dé¦–å…ˆé€šè¿‡å¤šè§†è§’è¾“å…¥å¾®è°ƒ3Dç”Ÿæˆå™¨è¿›è¡Œåœºæ™¯é€‚åº”ã€‚</li>
<li>ç„¶åè®­ç»ƒ2Dç¼–è¾‘å™¨è¿›è¡Œä¸€è‡´æ€§è’¸é¦ï¼Œç¼–è¾‘åçš„å¤šè§†è§’è¾“å‡ºé€šè¿‡é«˜æ–¯æŠ•å½±ä¼˜åŒ–ä¸º3Dè¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4a117273ec868473f031e8060cd8087f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b9873b5aec74e27324485962ade360a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-063a346421ade1f5bd884a5f90321bba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b2beefad62b673bb452dfaf939621967.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Can3Tok-Canonical-3D-Tokenization-and-Latent-Modeling-of-Scene-Level-3D-Gaussians"><a href="#Can3Tok-Canonical-3D-Tokenization-and-Latent-Modeling-of-Scene-Level-3D-Gaussians" class="headerlink" title="Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D   Gaussians"></a>Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D   Gaussians</h2><p><strong>Authors:Quankai Gao, Iliyan Georgiev, Tuanfeng Y. Wang, Krishna Kumar Singh, Ulrich Neumann, Jae Shin Yoon</strong></p>
<p>3D generation has made significant progress, however, it still largely remains at the object-level. Feedforward 3D scene-level generation has been rarely explored due to the lack of models capable of scaling-up latent representation learning on 3D scene-level data. Unlike object-level generative models, which are trained on well-labeled 3D data in a bounded canonical space, scene-level generations with 3D scenes represented by 3D Gaussian Splatting (3DGS) are unbounded and exhibit scale inconsistency across different scenes, making unified latent representation learning for generative purposes extremely challenging. In this paper, we introduce Can3Tok, the first 3D scene-level variational autoencoder (VAE) capable of encoding a large number of Gaussian primitives into a low-dimensional latent embedding, which effectively captures both semantic and spatial information of the inputs. Beyond model design, we propose a general pipeline for 3D scene data processing to address scale inconsistency issue. We validate our method on the recent scene-level 3D dataset DL3DV-10K, where we found that only Can3Tok successfully generalizes to novel 3D scenes, while compared methods fail to converge on even a few hundred scene inputs during training and exhibit zero generalization ability during inference. Finally, we demonstrate image-to-3DGS and text-to-3DGS generation as our applications to demonstrate its ability to facilitate downstream generation tasks. </p>
<blockquote>
<p>è™½ç„¶ä¸‰ç»´ç”ŸæˆæŠ€æœ¯å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»ç„¶ä¸»è¦åœç•™åœ¨å¯¹è±¡çº§åˆ«ã€‚ç”±äºç¼ºä¹èƒ½å¤Ÿåœ¨ä¸‰ç»´åœºæ™¯çº§åˆ«æ•°æ®ä¸Šæ‰©å±•æ½œåœ¨è¡¨ç¤ºå­¦ä¹ çš„æ¨¡å‹ï¼Œå‰é¦ˆä¸‰ç»´åœºæ™¯çº§åˆ«ç”Ÿæˆçš„ç ”ç©¶å¾ˆå°‘ã€‚ä¸åœ¨æœ‰é™è§„èŒƒç©ºé—´ä¸­è®­ç»ƒçš„å¯¹è±¡çº§ç”Ÿæˆæ¨¡å‹ä¸åŒï¼Œä»¥ä¸‰ç»´é«˜æ–¯è´´å›¾ï¼ˆ3DGSï¼‰è¡¨ç¤ºçš„ä¸‰ç»´åœºæ™¯çš„åœºæ™¯çº§ç”Ÿæˆæ˜¯æ— ç•Œçš„ï¼Œå¹¶ä¸”åœ¨ä¸åŒåœºæ™¯ä¹‹é—´è¡¨ç°å‡ºå°ºåº¦ä¸ä¸€è‡´æ€§ï¼Œè¿™ä½¿å¾—ç”¨äºç”Ÿæˆç›®çš„çš„ç»Ÿä¸€æ½œåœ¨è¡¨ç¤ºå­¦ä¹ æå…·æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Can3Tokï¼Œè¿™æ˜¯ä¸€æ¬¾é¦–æ¬¾èƒ½å¤Ÿåœ¨å¤§è§„æ¨¡çš„ä¸‰ç»´åœºæ™¯çº§åˆ«ä¸Šä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ç¼–ç å¤§é‡é«˜æ–¯åŸè¯­åˆ°ä¸€ä¸ªä½ç»´åº¦æ½œåœ¨åµŒå…¥ç‰©çš„ä¸‰ç»´åœºæ™¯çº§æ¨¡å‹ã€‚å®ƒèƒ½æœ‰æ•ˆåœ°æ•è·è¾“å…¥æ•°æ®çš„è¯­ä¹‰å’Œç©ºé—´ä¿¡æ¯ã€‚é™¤äº†æ¨¡å‹è®¾è®¡ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†é’ˆå¯¹ä¸‰ç»´åœºæ™¯æ•°æ®å¤„ç†çš„ä¸€èˆ¬æµç¨‹æ¥è§£å†³å°ºåº¦ä¸ä¸€è‡´çš„é—®é¢˜ã€‚æˆ‘ä»¬åœ¨æœ€æ–°çš„åœºæ™¯çº§ä¸‰ç»´æ•°æ®é›†DL3DV-10Kä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå‘ç°åªæœ‰Can3Tokèƒ½å¤ŸæˆåŠŸæ¨å¹¿åˆ°æ–°çš„ä¸‰ç»´åœºæ™¯ï¼Œè€Œç›¸æ¯”ä¹‹ä¸‹å…¶ä»–æ–¹æ³•ç”šè‡³åœ¨æ•°ç™¾ä¸ªåœºæ™¯è¾“å…¥çš„è®­ç»ƒè¿‡ç¨‹ä¸­æ— æ³•æ”¶æ•›ï¼Œå¹¶ä¸”åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ²¡æœ‰æ³›åŒ–èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†å›¾åƒåˆ°3DGSå’Œæ–‡æœ¬åˆ°3DGSçš„åº”ç”¨æ¥å±•ç¤ºå®ƒä¿ƒè¿›ä¸‹æ¸¸ç”Ÿæˆä»»åŠ¡çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01464v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Can3Tokæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ä¸‰ç»´åœºæ™¯æ•°æ®çš„åœºæ™¯çº§å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿç¼–ç å¤§é‡çš„é«˜æ–¯åŸºæœ¬ä½“åˆ°ä¸€ä¸ªä½ç»´åº¦çš„æ½œåœ¨åµŒå…¥ç©ºé—´ï¼Œæœ‰æ•ˆæ•æ‰è¾“å…¥çš„åœºæ™¯è¯­ä¹‰å’Œç©ºé—´ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³è§„æ¨¡ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œè¿˜æå‡ºäº†é’ˆå¯¹ä¸‰ç»´åœºæ™¯æ•°æ®å¤„ç†çš„é€šç”¨æµç¨‹ã€‚ç›¸è¾ƒäºå…¶ä»–æ–¹æ³•ï¼ŒCan3TokæˆåŠŸæ¨å¹¿åˆ°æ–°å‹ä¸‰ç»´åœºæ™¯ï¼Œå¹¶åœ¨DL3DV-10Kæ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œå±•ç¤ºäº†å›¾åƒåˆ°ä¸‰ç»´é«˜æ–¯å–·æº…ï¼ˆ3DGSï¼‰å’Œæ–‡å­—åˆ°ä¸‰ç»´é«˜æ–¯å–·æº…çš„ç”Ÿæˆåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3D generationç›®å‰ä¸»è¦åœç•™åœ¨ç‰©ä½“çº§åˆ«ï¼Œåœºæ™¯çº§åˆ«çš„ç”Ÿæˆç ”ç©¶è¾ƒå°‘ã€‚</li>
<li>ç¼ºä¹èƒ½å¤Ÿåœ¨ä¸‰ç»´åœºæ™¯çº§åˆ«æ•°æ®ä¸Šè¿›è¡Œè§„æ¨¡åŒ–æ½œåœ¨è¡¨ç¤ºå­¦ä¹ çš„æ¨¡å‹æ˜¯ä¸»è¦åŸå› ã€‚</li>
<li>Can3Tokæ˜¯é¦–ä¸ªé’ˆå¯¹ä¸‰ç»´åœºæ™¯æ•°æ®çš„åœºæ™¯çº§å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ã€‚</li>
<li>Can3Tokæ¨¡å‹èƒ½å°†å¤§é‡é«˜æ–¯åŸºæœ¬ä½“ç¼–ç è¿›ä½ç»´æ½œåœ¨åµŒå…¥ç©ºé—´ï¼Œæ•æ‰åœºæ™¯çš„è¯­ä¹‰å’Œç©ºé—´ä¿¡æ¯ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é’ˆå¯¹ä¸‰ç»´åœºæ™¯æ•°æ®å¤„ç†çš„é€šç”¨æµç¨‹æ¥è§£å†³è§„æ¨¡ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>åœ¨DL3DV-10Kæ•°æ®é›†ä¸ŠéªŒè¯äº†Can3Tokçš„æœ‰æ•ˆæ€§ï¼Œç›¸æ¯”å…¶ä»–æ–¹æ³•å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-189df16fb2529c9cc027e56a6e5eccc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68d6af8c2c0cbcb48ecb22e8dd4170e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-370854fdb2151a6ac9821cae90419dbb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6fd9c4ee44fd11c769f51c6d8882a71.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-22de937dd2d314234a537bf0c90332b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cfbc8ff32df1ca086a5afb945b9bdcc.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="MoGA-3D-Generative-Avatar-Prior-for-Monocular-Gaussian-Avatar-Reconstruction"><a href="#MoGA-3D-Generative-Avatar-Prior-for-Monocular-Gaussian-Avatar-Reconstruction" class="headerlink" title="MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar   Reconstruction"></a>MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar   Reconstruction</h2><p><strong>Authors:Zijian Dong, Longteng Duan, Jie Song, Michael J. Black, Andreas Geiger</strong></p>
<p>We present MoGA, a novel method to reconstruct high-fidelity 3D Gaussian avatars from a single-view image. The main challenge lies in inferring unseen appearance and geometric details while ensuring 3D consistency and realism. Most previous methods rely on 2D diffusion models to synthesize unseen views; however, these generated views are sparse and inconsistent, resulting in unrealistic 3D artifacts and blurred appearance. To address these limitations, we leverage a generative avatar model, that can generate diverse 3D avatars by sampling deformed Gaussians from a learned prior distribution. Due to limited 3D training data, such a 3D model alone cannot capture all image details of unseen identities. Consequently, we integrate it as a prior, ensuring 3D consistency by projecting input images into its latent space and enforcing additional 3D appearance and geometric constraints. Our novel approach formulates Gaussian avatar creation as model inversion by fitting the generative avatar to synthetic views from 2D diffusion models. The generative avatar provides an initialization for model fitting, enforces 3D regularization, and helps in refining pose. Experiments show that our method surpasses state-of-the-art techniques and generalizes well to real-world scenarios. Our Gaussian avatars are also inherently animatable. For code, see https:&#x2F;&#x2F; zj-dong.github.io&#x2F; MoGA&#x2F;. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†MoGAè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒèƒ½å¤Ÿä»å•è§†å›¾å›¾åƒé‡å»ºå‡ºé«˜ä¿çœŸåº¦çš„ä¸‰ç»´é«˜æ–¯å¤´åƒã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºåœ¨ä¿éšœä¸‰ç»´ä¸€è‡´æ€§å’ŒçœŸå®æ„Ÿçš„åŒæ—¶æ¨æ–­å‡ºçœ‹ä¸è§çš„å¤–è§‚å’Œå‡ ä½•ç»†èŠ‚ã€‚è¿‡å»çš„å¤§å¤šæ•°æ–¹æ³•éƒ½ä¾èµ–äºäºŒç»´æ‰©æ•£æ¨¡å‹æ¥åˆæˆçœ‹ä¸è§çš„è§†å›¾ï¼Œç„¶è€Œè¿™äº›ç”Ÿæˆçš„è§†å›¾ç¨€ç–ä¸”ä¸ä¸€è‡´ï¼Œå¯¼è‡´ä¸‰ç»´äººå·¥åˆ¶å“ä¸çœŸå®å’Œå¤–è§‚æ¨¡ç³Šã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™ï¼Œæˆ‘ä»¬åˆ©ç”¨ç”Ÿæˆå¤´åƒæ¨¡å‹ï¼Œé€šè¿‡ä»å­¦ä¹ åˆ°çš„å…ˆéªŒåˆ†å¸ƒä¸­é‡‡æ ·å˜å½¢çš„é«˜æ–¯å‡½æ•°æ¥ç”Ÿæˆå¤šæ ·åŒ–çš„ä¸‰ç»´å¤´åƒã€‚ç”±äºä¸‰ç»´è®­ç»ƒæ•°æ®æœ‰é™ï¼Œä»…ä½¿ç”¨è¿™æ ·çš„ä¸‰ç»´æ¨¡å‹æ— æ³•æ•è·æœªè§è¿‡èº«ä»½çš„æ‰€æœ‰å›¾åƒç»†èŠ‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å…¶æ•´åˆä¸ºå…ˆéªŒï¼Œé€šè¿‡å°†è¾“å…¥å›¾åƒæŠ•å½±åˆ°å…¶æ½œåœ¨ç©ºé—´å¹¶æ–½åŠ é¢å¤–çš„ä¸‰ç»´å¤–è§‚å’Œå‡ ä½•çº¦æŸæ¥ç¡®ä¿ä¸‰ç»´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„æ–°æ–¹æ³•å°†é«˜æ–¯å¤´åƒåˆ›å»ºå…¬å¼åŒ–ä¸ºæ¨¡å‹åæ¼”ï¼Œé€šè¿‡å°†ç”Ÿæˆçš„å¤´åƒæ‹Ÿåˆåˆ°äºŒç»´æ‰©æ•£æ¨¡å‹çš„åˆæˆè§†å›¾æ¥å®ç°ã€‚ç”Ÿæˆçš„å¤´åƒä¸ºæ¨¡å‹æ‹Ÿåˆæä¾›äº†åˆå§‹åŒ–ï¼Œå¼ºåˆ¶å®æ–½ä¸‰ç»´æ­£åˆ™åŒ–ï¼Œå¹¶æœ‰åŠ©äºç»†åŒ–å§¿æ€ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†æœ€å…ˆè¿›çš„æŠ€æœ¯å¹¶åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚æˆ‘ä»¬çš„é«˜æ–¯å¤´åƒæœ¬è´¨ä¸Šæ˜¯å¯åŠ¨ç”»çš„ã€‚æœ‰å…³ä»£ç ï¼Œè¯·å‚è§ï¼š[<a target="_blank" rel="noopener" href="https://zj-dong.github.io/MoGA/]">https://zj-dong.github.io/MoGA/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23597v2">PDF</a> ICCV 2025 (Highlight), Project Page: <a target="_blank" rel="noopener" href="https://zj-dong.github.io/MoGA/">https://zj-dong.github.io/MoGA/</a></p>
<p><strong>Summary</strong><br>     æå‡ºäº†ä¸€ç§åä¸ºMoGAçš„æ–°æ–¹æ³•ï¼Œå¯ä»å•è§†å›¾å›¾åƒé‡å»ºé«˜ä¿çœŸ3Dé«˜æ–¯åŒ–èº«ã€‚è¯¥æ–¹æ³•ä¸»è¦æŒ‘æˆ˜åœ¨äºæ¨æ–­å‡ºæœªè§çš„å¤–è§‚å’Œå‡ ä½•ç»†èŠ‚ï¼ŒåŒæ—¶ç¡®ä¿3Dä¸€è‡´æ€§å’Œé€¼çœŸæ€§ã€‚ä¸å¤§å¤šæ•°å…ˆå‰æ–¹æ³•ä¸åŒï¼ŒMoGAé‡‡ç”¨ç”ŸæˆåŒ–èº«æ¨¡å‹ï¼Œé€šè¿‡ä»å­¦ä¹ åˆ°çš„å…ˆéªŒåˆ†å¸ƒä¸­é‡‡æ ·å˜å½¢é«˜æ–¯æ¥ç”Ÿæˆå¤šæ ·åŒ–çš„3DåŒ–èº«ã€‚é€šè¿‡å°†ç”ŸæˆåŒ–èº«æ¨¡å‹ä½œä¸ºå…ˆéªŒï¼Œå¹¶å°†è¾“å…¥å›¾åƒæŠ•å½±åˆ°å…¶æ½œåœ¨ç©ºé—´å¹¶æ–½åŠ é¢å¤–çš„3Då¤–è§‚å’Œå‡ ä½•çº¦æŸæ¥ç¡®ä¿3Dä¸€è‡´æ€§ã€‚MoGAå°†é«˜æ–¯åŒ–èº«åˆ›å»ºå…¬å¼åŒ–ä¸ºæ¨¡å‹åæ¼”é—®é¢˜ï¼Œé€šè¿‡å°†ç”ŸæˆåŒ–èº«æ‹Ÿåˆåˆ°æ¥è‡ª2Dæ‰©æ•£æ¨¡å‹çš„åˆæˆè§†å›¾æ¥å®ç°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºæœ€æ–°æŠ€æœ¯å¹¶åœ¨çœŸå®åœºæ™¯ä¸­å…·æœ‰è‰¯å¥½æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MoGAæ˜¯ä¸€ç§ä»å•è§†å›¾å›¾åƒé‡å»ºé«˜ä¿çœŸ3Dé«˜æ–¯åŒ–èº«çš„æ–°æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸»è¦è§£å†³å¦‚ä½•æ¨æ–­æœªè§çš„å¤–è§‚å’Œå‡ ä½•ç»†èŠ‚ï¼Œå¹¶ç¡®ä¿3Dä¸€è‡´æ€§å’Œé€¼çœŸæ€§ã€‚</li>
<li>MoGAé‡‡ç”¨ç”ŸæˆåŒ–èº«æ¨¡å‹ï¼Œé€šè¿‡é‡‡æ ·å˜å½¢é«˜æ–¯æ¥ç”Ÿæˆå¤šæ ·åŒ–çš„3DåŒ–èº«ã€‚</li>
<li>ä¸ºäº†ç¡®ä¿3Dä¸€è‡´æ€§ï¼Œå°†ç”ŸæˆåŒ–èº«æ¨¡å‹ä½œä¸ºå…ˆéªŒï¼Œå¹¶æ–½åŠ é¢å¤–çš„çº¦æŸã€‚</li>
<li>MoGAé€šè¿‡å°†ç”ŸæˆåŒ–èº«æ‹Ÿåˆåˆ°æ¥è‡ª2Dæ‰©æ•£æ¨¡å‹çš„åˆæˆè§†å›¾æ¥å®ç°æ¨¡å‹åæ¼”ã€‚</li>
<li>å®éªŒè¯æ˜MoGAä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶åœ¨çœŸå®åœºæ™¯ä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0c663b898eadc9571c2253777a4e53b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-92c4cfb63cc8def479f57a52e048adf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4030795a97d1504b3de20f5c85aeca83.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GSCache-Real-Time-Radiance-Caching-for-Volume-Path-Tracing-using-3D-Gaussian-Splatting"><a href="#GSCache-Real-Time-Radiance-Caching-for-Volume-Path-Tracing-using-3D-Gaussian-Splatting" class="headerlink" title="GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D   Gaussian Splatting"></a>GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D   Gaussian Splatting</h2><p><strong>Authors:David Bauer, Qi Wu, Hamid Gadirov, Kwan-Liu Ma</strong></p>
<p>Real-time path tracing is rapidly becoming the standard for rendering in entertainment and professional applications. In scientific visualization, volume rendering plays a crucial role in helping researchers analyze and interpret complex 3D data. Recently, photorealistic rendering techniques have gained popularity in scientific visualization, yet they face significant challenges. One of the most prominent issues is slow rendering performance and high pixel variance caused by Monte Carlo integration. In this work, we introduce a novel radiance caching approach for path-traced volume rendering. Our method leverages advances in volumetric scene representation and adapts 3D Gaussian splatting to function as a multi-level, path-space radiance cache. This cache is designed to be trainable on the fly, dynamically adapting to changes in scene parameters such as lighting configurations and transfer functions. By incorporating our cache, we achieve less noisy, higher-quality images without increasing rendering costs. To evaluate our approach, we compare it against a baseline path tracer that supports uniform sampling and next-event estimation and the state-of-the-art for neural radiance caching. Through both quantitative and qualitative analyses, we demonstrate that our path-space radiance cache is a robust solution that is easy to integrate and significantly enhances the rendering quality of volumetric visualization applications while maintaining comparable computational efficiency. </p>
<blockquote>
<p>å®æ—¶è·¯å¾„è¿½è¸ªæ­£åœ¨è¿…é€Ÿæˆä¸ºå¨±ä¹å’Œä¸“ä¸šåº”ç”¨æ¸²æŸ“çš„æ ‡å‡†ã€‚åœ¨ç§‘å­¦å¯è§†åŒ–ä¸­ï¼Œä½“æ¸²æŸ“åœ¨ç ”ç©¶åˆ†æå¤æ‚ä¸‰ç»´æ•°æ®æ—¶æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚è¿‘å¹´æ¥ï¼Œè™½ç„¶å†™å®æ¸²æŸ“æŠ€æœ¯åœ¨ç§‘å­¦å¯è§†åŒ–ä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œä½†å®ƒä»¬é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ã€‚æœ€çªå‡ºçš„é—®é¢˜ä¹‹ä¸€æ˜¯Monte Carloç§¯åˆ†å¯¼è‡´çš„æ¸²æŸ“æ€§èƒ½ç¼“æ…¢å’Œé«˜åƒç´ æ–¹å·®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸ºè·¯å¾„è¿½è¸ªä½“ç§¯æ¸²æŸ“å¼•å…¥äº†ä¸€ç§æ–°å‹è¾å°„ç¼“å­˜æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä½“ç§¯åœºæ™¯è¡¨ç¤ºçš„è¿›å±•ï¼Œå¹¶é€‚åº”ä¸‰ç»´é«˜æ–¯æº…å°„ä½œä¸ºå¤šçº§è·¯å¾„ç©ºé—´è¾å°„ç¼“å­˜ã€‚è¯¥ç¼“å­˜è¢«è®¾è®¡ä¸ºå¯å³æ—¶è®­ç»ƒï¼ŒåŠ¨æ€é€‚åº”åœºæ™¯å‚æ•°çš„å˜åŒ–ï¼Œå¦‚ç…§æ˜é…ç½®å’Œä¼ è¾“å‡½æ•°ã€‚é€šè¿‡é‡‡ç”¨æˆ‘ä»¬çš„ç¼“å­˜ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¸å¢åŠ æ¸²æŸ“æˆæœ¬çš„æƒ…å†µä¸‹å®ç°å™ªå£°æ›´å°‘ã€è´¨é‡æ›´é«˜çš„å›¾åƒã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å°†å®ƒä¸æ”¯æŒå‡åŒ€é‡‡æ ·å’Œä¸‹ä¸€ä¸ªäº‹ä»¶ä¼°è®¡çš„åŸºæœ¬è·¯å¾„è¿½è¸ªå™¨ä»¥åŠæœ€æ–°çš„ç¥ç»è¾å°„ç¼“å­˜æŠ€æœ¯è¿›è¡Œäº†æ¯”è¾ƒã€‚é€šè¿‡å®šé‡å’Œå®šæ€§åˆ†æï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„è·¯å¾„ç©ºé—´è¾å°„ç¼“å­˜æ˜¯ä¸€ç§ç¨³å¥çš„è§£å†³æ–¹æ¡ˆï¼Œæ˜“äºé›†æˆï¼Œåœ¨ä¿æŒç›¸å½“çš„è®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†ä½“ç§¯å¯è§†åŒ–åº”ç”¨çš„æ¸²æŸ“è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19718v2">PDF</a> </p>
<p><strong>Summary</strong><br>å®æ—¶è·¯å¾„è¿½è¸ªæ­£åœ¨è¿…é€Ÿæˆä¸ºå¨±ä¹å’Œä¸“ä¸šåº”ç”¨ä¸­æ¸²æŸ“çš„æ ‡å‡†ã€‚åœ¨ç§‘å­¦å¯è§†åŒ–ä¸­ï¼Œä½“ç§¯æ¸²æŸ“æ‰®æ¼”ç€å¸®åŠ©ç ”ç©¶äººå‘˜åˆ†æå’Œè§£é‡Šå¤æ‚3Dæ•°æ®çš„é‡è¦è§’è‰²ã€‚å°½ç®¡æœ€è¿‘å…‰æ …åŒ–æ¸²æŸ“æŠ€æœ¯åœ¨ç§‘å­¦å¯è§†åŒ–ä¸­å—åˆ°æ¬¢è¿ï¼Œä½†å®ƒä»¬é¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ã€‚å…¶ä¸­æœ€çªå‡ºçš„é—®é¢˜æ˜¯æ¸²æŸ“æ€§èƒ½ç¼“æ…¢å’Œç”±è’™ç‰¹å¡æ´›ç§¯åˆ†å¼•èµ·çš„é«˜åƒç´ æ–¹å·®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„è¾å°„ç¼“å­˜è·¯å¾„è¿½è¸ªä½“ç§¯æ¸²æŸ“æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†ä½“ç§¯åœºæ™¯è¡¨ç¤ºçš„è¿›å±•ï¼Œå¹¶å°†ä¸‰ç»´é«˜æ–¯æ¶‚æŠ¹æŠ€æœ¯é€‚åº”äºä½œä¸ºå¤šå±‚æ¬¡è·¯å¾„ç©ºé—´è¾å°„ç¼“å­˜å·¥ä½œã€‚è¯¥ç¼“å­˜è®¾è®¡èƒ½å¤Ÿåœ¨é£è¡Œæ—¶è¿›è¡Œè®­ç»ƒï¼ŒåŠ¨æ€é€‚åº”åœºæ™¯å‚æ•°çš„å˜åŒ–ï¼Œå¦‚ç…§æ˜é…ç½®å’Œä¼ è¾“åŠŸèƒ½ã€‚é€šè¿‡ç»“åˆæˆ‘ä»¬çš„ç¼“å­˜ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¸å¢åŠ æ¸²æŸ“æˆæœ¬çš„æƒ…å†µä¸‹å®ç°æ›´å°‘çš„å™ªå£°å’Œæ›´é«˜è´¨é‡çš„å›¾åƒã€‚æˆ‘ä»¬é€šè¿‡å°†å…¶ä¸åŸºçº¿è·¯å¾„è¿½è¸ªå™¨å’Œæ”¯æŒå‡åŒ€é‡‡æ ·å’Œä¸‹ä¸€ä¸ªäº‹ä»¶ä¼°è®¡çš„æœ€å…ˆè¿›æŠ€æœ¯è¿›è¡Œæ¯”è¾ƒï¼Œæ¥è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚é€šè¿‡å®šé‡å’Œå®šæ€§åˆ†æï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„è·¯å¾„ç©ºé—´è¾å°„ç¼“å­˜æ˜¯ä¸€ç§å¯é çš„è§£å†³æ–¹æ¡ˆï¼Œæ˜“äºé›†æˆï¼Œå¹¶èƒ½æ˜¾è‘—æé«˜ä½“ç§¯å¯è§†åŒ–åº”ç”¨ç¨‹åºçš„æ¸²æŸ“è´¨é‡ï¼ŒåŒæ—¶ä¿æŒç›¸å½“çš„è®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®æ—¶è·¯å¾„è¿½è¸ªå·²æˆä¸ºå¨±ä¹å’Œä¸“ä¸šåº”ç”¨ä¸­æ¸²æŸ“çš„ä¸»æµæŠ€æœ¯ã€‚</li>
<li>ç§‘å­¦å¯è§†åŒ–ä¸­ä½“ç§¯æ¸²æŸ“åœ¨åˆ†æå¤æ‚3Dæ•°æ®æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚</li>
<li>è’™ç‰¹å¡æ´›ç§¯åˆ†åœ¨å…‰æ …åŒ–æ¸²æŸ“æŠ€æœ¯ä¸­å¼•å‘é«˜åƒç´ æ–¹å·®å’Œæ¸²æŸ“æ€§èƒ½ç¼“æ…¢çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹çš„è·¯å¾„ç©ºé—´è¾å°„ç¼“å­˜æ–¹æ³•ç”¨äºä½“ç§¯æ¸²æŸ“ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨ä½“ç§¯åœºæ™¯è¡¨ç¤ºçš„è¿›å±•ï¼Œå¹¶é‡‡ç”¨ä¸‰ç»´é«˜æ–¯æ¶‚æŠ¹æŠ€æœ¯ä½œä¸ºå¤šå±‚æ¬¡è·¯å¾„ç©ºé—´è¾å°„ç¼“å­˜ã€‚</li>
<li>ç¼“å­˜è®¾è®¡èƒ½åŠ¨æ€é€‚åº”åœºæ™¯å‚æ•°å˜åŒ–ï¼Œæé«˜æ¸²æŸ“è´¨é‡å¹¶å‡å°‘å™ªå£°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd8c347981b7bf8e3a3a2171c0c5754b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d63b06ef49ade5451ccfe1f6713ed3b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-099b61f6d92ca708e5ffd372b76ff432.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1baff9d6451ae49a6f73274ee56ffef5.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GS-Occ3D-Scaling-Vision-only-Occupancy-Reconstruction-with-Gaussian-Splatting"><a href="#GS-Occ3D-Scaling-Vision-only-Occupancy-Reconstruction-with-Gaussian-Splatting" class="headerlink" title="GS-Occ3D: Scaling Vision-only Occupancy Reconstruction with Gaussian   Splatting"></a>GS-Occ3D: Scaling Vision-only Occupancy Reconstruction with Gaussian   Splatting</h2><p><strong>Authors:Baijun Ye, Minghui Qin, Saining Zhang, Moonjun Gong, Shaoting Zhu, Zebang Shen, Luan Zhang, Lu Zhang, Hao Zhao, Hang Zhao</strong></p>
<p>Occupancy is crucial for autonomous driving, providing essential geometric priors for perception and planning. However, existing methods predominantly rely on LiDAR-based occupancy annotations, which limits scalability and prevents leveraging vast amounts of potential crowdsourced data for auto-labeling. To address this, we propose GS-Occ3D, a scalable vision-only framework that directly reconstructs occupancy. Vision-only occupancy reconstruction poses significant challenges due to sparse viewpoints, dynamic scene elements, severe occlusions, and long-horizon motion. Existing vision-based methods primarily rely on mesh representation, which suffer from incomplete geometry and additional post-processing, limiting scalability. To overcome these issues, GS-Occ3D optimizes an explicit occupancy representation using an Octree-based Gaussian Surfel formulation, ensuring efficiency and scalability. Additionally, we decompose scenes into static background, ground, and dynamic objects, enabling tailored modeling strategies: (1) Ground is explicitly reconstructed as a dominant structural element, significantly improving large-area consistency; (2) Dynamic vehicles are separately modeled to better capture motion-related occupancy patterns. Extensive experiments on the Waymo dataset demonstrate that GS-Occ3D achieves state-of-the-art geometry reconstruction results. By curating vision-only binary occupancy labels from diverse urban scenes, we show their effectiveness for downstream occupancy models on Occ3D-Waymo and superior zero-shot generalization on Occ3D-nuScenes. It highlights the potential of large-scale vision-based occupancy reconstruction as a new paradigm for scalable auto-labeling. Project Page: <a target="_blank" rel="noopener" href="https://gs-occ3d.github.io/">https://gs-occ3d.github.io/</a> </p>
<blockquote>
<p>å ç”¨ä¿¡æ¯å¯¹äºè‡ªåŠ¨é©¾é©¶è‡³å…³é‡è¦ï¼Œå®ƒä¸ºæ„ŸçŸ¥å’Œè§„åˆ’æä¾›äº†å¿…è¦çš„å‡ ä½•å…ˆéªŒçŸ¥è¯†ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºåŸºäºæ¿€å…‰é›·è¾¾çš„å ç”¨æ³¨é‡Šï¼Œè¿™é™åˆ¶äº†å¯æ‰©å±•æ€§ï¼Œå¹¶é˜»æ­¢äº†å¤§é‡æ½œåœ¨çš„ä¼—åŒ…æ•°æ®ç”¨äºè‡ªåŠ¨æ ‡æ³¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GS-Occ3Dï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„ä»…è§†è§‰æ¡†æ¶ï¼Œå¯ç›´æ¥é‡å»ºå ç”¨ä¿¡æ¯ã€‚ä»…ä½¿ç”¨è§†è§‰çš„å ç”¨ä¿¡æ¯é‡å»ºç”±äºç¨€ç–çš„è§†ç‚¹ã€åŠ¨æ€çš„åœºæ™¯å…ƒç´ ã€ä¸¥é‡çš„é®æŒ¡å’Œé•¿è·ç¦»è¿åŠ¨è€Œé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºäºè§†è§‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºç½‘æ ¼è¡¨ç¤ºï¼Œè¿™ä¼šå¯¼è‡´å‡ ä½•ä¸å®Œæ•´å’Œé¢å¤–çš„åå¤„ç†ï¼Œä»è€Œé™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼ŒGS-Occ3Dä½¿ç”¨åŸºäºå…«å‰æ ‘çš„é«˜æ–¯æ›²é¢ç‰‡ï¼ˆGaussian Surfelï¼‰ä¼˜åŒ–æ˜¾å¼å ç”¨è¡¨ç¤ºï¼Œç¡®ä¿æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†åœºæ™¯åˆ†è§£ä¸ºé™æ€èƒŒæ™¯ã€åœ°é¢å’ŒåŠ¨æ€ç‰©ä½“ï¼Œä»¥å®ç°å®šåˆ¶å»ºæ¨¡ç­–ç•¥ï¼šï¼ˆ1ï¼‰åœ°é¢è¢«æ˜ç¡®é‡å»ºä¸ºä¸»è¦çš„ç»“æ„å…ƒç´ ï¼Œè¿™æ˜¾è‘—æé«˜äº†å¤§é¢ç§¯çš„ä¸€è‡´æ€§ï¼›ï¼ˆ2ï¼‰åŠ¨æ€è½¦è¾†åˆ™è¿›è¡Œå•ç‹¬å»ºæ¨¡ï¼Œä»¥æ›´å¥½åœ°æ•æ‰ä¸è¿åŠ¨ç›¸å…³çš„å ç”¨æ¨¡å¼ã€‚åœ¨Waymoæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGS-Occ3Dè¾¾åˆ°äº†æœ€æ–°çš„å‡ ä½•é‡å»ºæ•ˆæœã€‚æˆ‘ä»¬ä»å„ç§åŸå¸‚åœºæ™¯ä¸­æ•´ç†å‡ºä»…ä½¿ç”¨è§†è§‰çš„äºŒè¿›åˆ¶å ç”¨æ ‡ç­¾ï¼Œå¹¶å±•ç¤ºäº†å®ƒä»¬åœ¨Occ3D-Waymoä¸Šçš„ä¸‹æ¸¸å ç”¨æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠåœ¨Occ3D-nuScenesä¸Šçš„å‡ºè‰²é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚è¿™å‡¸æ˜¾äº†å¤§è§„æ¨¡åŸºäºè§†è§‰çš„å ç”¨ä¿¡æ¯é‡å»ºä½œä¸ºæ–°çš„å¯æ‰©å±•è‡ªåŠ¨æ ‡æ³¨æ¨¡å¼æ½œåŠ›ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://gs-occ3d.github.io/">https://gs-occ3d.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19451v3">PDF</a> ICCV 2025. Project Page: <a target="_blank" rel="noopener" href="https://gs-occ3d.github.io/">https://gs-occ3d.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºGS-Occ3Dçš„ä»…è§†è§‰æ„ŸçŸ¥çš„å¯æ‰©å±•æ¡†æ¶ï¼Œç”¨äºç›´æ¥é‡å»ºå ç”¨ç©ºé—´ã€‚è¯¥æ¡†æ¶è§£å†³äº†ä»…ä¾é LiDARçš„å ç”¨æ ‡æ³¨æ–¹æ³•å­˜åœ¨çš„å¯æ‰©å±•æ€§é—®é¢˜ï¼Œå¹¶èƒ½å¤Ÿåˆ©ç”¨å¤§é‡çš„æ½œåœ¨ä¼—åŒ…æ•°æ®è¿›è¡Œè‡ªåŠ¨æ ‡æ³¨ã€‚GS-Occ3Dé‡‡ç”¨åŸºäºOctreeçš„Gaussian Surfelå…¬å¼ä¼˜åŒ–å ç”¨è¡¨ç¤ºï¼Œè§£å†³äº†ç°æœ‰è§†è§‰æ–¹æ³•åœ¨å¤„ç†ç¨€ç–è§†ç‚¹ã€åŠ¨æ€åœºæ™¯å…ƒç´ ã€ä¸¥é‡é®æŒ¡å’Œé•¿è·ç¦»è¿åŠ¨æ—¶çš„æŒ‘æˆ˜ã€‚é€šè¿‡åˆ†è§£åœºæ™¯ä¸ºé™æ€èƒŒæ™¯ã€åœ°é¢å’ŒåŠ¨æ€ç‰©ä½“ï¼Œå®ç°äº†é’ˆå¯¹æ€§çš„å»ºæ¨¡ç­–ç•¥ï¼Œæ˜¾è‘—æé«˜äº†å ç”¨é‡å»ºçš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGS-Occ3Dåœ¨Waymoæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„å‡ ä½•é‡å»ºæ•ˆæœï¼Œå¹¶å±•ç¤ºäº†å¤§è§„æ¨¡è§†è§‰æ„ŸçŸ¥å ç”¨é‡å»ºä½œä¸ºæ–°çš„å¯æ‰©å±•è‡ªåŠ¨æ ‡æ³¨æ–¹æ³•çš„å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GS-Occ3Dæ˜¯ä¸€ä¸ªä»…è§†è§‰æ„ŸçŸ¥çš„æ¡†æ¶ï¼Œç”¨äºç›´æ¥é‡å»ºå ç”¨ç©ºé—´ï¼Œè§£å†³äº†ä¾èµ–LiDARçš„å ç”¨æ ‡æ³¨æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿåˆ©ç”¨å¤§é‡çš„æ½œåœ¨ä¼—åŒ…æ•°æ®è¿›è¡Œè‡ªåŠ¨æ ‡æ³¨ï¼Œæé«˜äº†å¯æ‰©å±•æ€§ã€‚</li>
<li>GS-Occ3Dé‡‡ç”¨åŸºäºOctreeçš„Gaussian Surfelå…¬å¼ä¼˜åŒ–å ç”¨è¡¨ç¤ºï¼Œè§£å†³äº†ç°æœ‰è§†è§‰æ–¹æ³•åœ¨å¤„ç†å„ç§åœºæ™¯æŒ‘æˆ˜æ—¶çš„é—®é¢˜ã€‚</li>
<li>åœºæ™¯è¢«åˆ†è§£ä¸ºé™æ€èƒŒæ™¯ã€åœ°é¢å’ŒåŠ¨æ€ç‰©ä½“ï¼Œä»¥å®ç°é’ˆå¯¹æ€§çš„å»ºæ¨¡ç­–ç•¥ã€‚</li>
<li>åœ°é¢ä½œä¸ºä¸»å¯¼ç»“æ„å…ƒç´ è¢«æ˜¾å¼é‡å»ºï¼Œæé«˜äº†å¤§åŒºåŸŸçš„ä¸€è‡´æ€§ã€‚</li>
<li>åŠ¨æ€è½¦è¾†è¢«å•ç‹¬å»ºæ¨¡ï¼Œä»¥æ›´å¥½åœ°æ•æ‰è¿åŠ¨ç›¸å…³çš„å ç”¨æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19451">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f4a0cff5ebd538878c2f6676936d3c09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5337e1b374c7dc1047f328770ee538cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9874a664ee8860ec1f69c92d5e11e58a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Occlusion-Aware-Temporally-Consistent-Amodal-Completion-for-3D-Human-Object-Interaction-Reconstruction"><a href="#Occlusion-Aware-Temporally-Consistent-Amodal-Completion-for-3D-Human-Object-Interaction-Reconstruction" class="headerlink" title="Occlusion-Aware Temporally Consistent Amodal Completion for 3D   Human-Object Interaction Reconstruction"></a>Occlusion-Aware Temporally Consistent Amodal Completion for 3D   Human-Object Interaction Reconstruction</h2><p><strong>Authors:Hyungjun Doh, Dong In Lee, Seunggeun Chi, Pin-Hao Huang, Kwonjoon Lee, Sangpil Kim, Karthik Ramani</strong></p>
<p>We introduce a novel framework for reconstructing dynamic human-object interactions from monocular video that overcomes challenges associated with occlusions and temporal inconsistencies. Traditional 3D reconstruction methods typically assume static objects or full visibility of dynamic subjects, leading to degraded performance when these assumptions are violated-particularly in scenarios where mutual occlusions occur. To address this, our framework leverages amodal completion to infer the complete structure of partially obscured regions. Unlike conventional approaches that operate on individual frames, our method integrates temporal context, enforcing coherence across video sequences to incrementally refine and stabilize reconstructions. This template-free strategy adapts to varying conditions without relying on predefined models, significantly enhancing the recovery of intricate details in dynamic scenes. We validate our approach using 3D Gaussian Splatting on challenging monocular videos, demonstrating superior precision in handling occlusions and maintaining temporal stability compared to existing techniques. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„ä»å•ç›®è§†é¢‘ä¸­é‡å»ºåŠ¨æ€äººæœºäº¤äº’çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…‹æœäº†ä¸é®æŒ¡å’Œæ—¶é—´ä¸ä¸€è‡´ç›¸å…³çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„3Dé‡å»ºæ–¹æ³•é€šå¸¸å‡è®¾ç‰©ä½“æ˜¯é™æ€çš„æˆ–åŠ¨æ€ä¸»ä½“å®Œå…¨å¯è§ï¼Œå½“è¿™äº›å‡è®¾è¢«è¿åæ—¶ï¼Œç‰¹åˆ«æ˜¯åœ¨å‘ç”Ÿç›¸äº’é®æŒ¡çš„åœºæ™¯ä¸­ï¼Œä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åˆ©ç”¨æ¨¡æ€å®Œæˆæ¥æ¨æ–­éƒ¨åˆ†é®æŒ¡åŒºåŸŸçš„å®Œæ•´ç»“æ„ã€‚ä¸åœ¨å•ä¸ªå¸§ä¸Šè¿è¡Œçš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†æ—¶é—´ä¸Šä¸‹æ–‡ï¼Œå¼ºåˆ¶è§†é¢‘åºåˆ—ä¹‹é—´çš„è¿è´¯æ€§ï¼Œä»¥é€æ­¥ä¼˜åŒ–å’Œç¨³å®šé‡å»ºã€‚è¿™ç§æ— æ¨¡æ¿çš„ç­–ç•¥èƒ½å¤Ÿé€‚åº”ä¸åŒçš„æ¡ä»¶ï¼Œè€Œä¸ä¾èµ–äºé¢„å®šä¹‰çš„æ¨¡å‹ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†åŠ¨æ€åœºæ™¯ä¸­ç»†èŠ‚çš„æ¢å¤ã€‚æˆ‘ä»¬ä½¿ç”¨å•ç›®è§†é¢‘ä¸Šçš„ä¸‰ç»´é«˜æ–¯æ‹¼è´´æ³•éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†é®æŒ¡å’Œä¿æŒæ—¶é—´ç¨³å®šæ€§æ–¹é¢è¡¨ç°å‡ºè¾ƒé«˜çš„ç²¾åº¦ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08137v2">PDF</a> ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ä»å•ç›®è§†é¢‘ä¸­é‡å»ºåŠ¨æ€äºº-ç‰©ä½“äº¤äº’çš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…‹æœäº†ä¸é®æŒ¡å’Œæš‚æ—¶ä¸ä¸€è‡´ç›¸å…³çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿ3Dé‡å»ºæ–¹æ³•é€šå¸¸å‡è®¾ç‰©ä½“é™æ€æˆ–åŠ¨æ€ä¸»ä½“çš„å®Œå…¨å¯è§æ€§ï¼Œå½“è¿™äº›å‡è®¾ä¸æˆç«‹æ—¶ï¼Œç‰¹åˆ«æ˜¯åœ¨å‘ç”Ÿç›¸äº’é®æŒ¡çš„åœºæ™¯ä¸­ï¼Œæ€§èƒ½ä¼šä¸‹é™ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åˆ©ç”¨æ¨¡æ€å®Œæˆæ³•æ¨æ–­éƒ¨åˆ†é®æŒ¡åŒºåŸŸçš„å®Œæ•´ç»“æ„ã€‚ä¸ä¼ ç»Ÿåœ¨å•ä¸ªå¸§ä¸Šæ“ä½œçš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†æ—¶é—´ä¸Šä¸‹æ–‡ï¼Œå¼ºåˆ¶è§†é¢‘åºåˆ—ä¹‹é—´çš„è¿è´¯æ€§ï¼Œä»¥é€æ­¥ç²¾ç»†å’Œè°ƒæ•´é‡å»ºã€‚è¿™ç§æ— æ¨¡æ¿çš„ç­–ç•¥é€‚åº”äºå„ç§æ¡ä»¶ï¼Œè€Œä¸ä¾èµ–äºé¢„è®¾æ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†åœ¨åŠ¨æ€åœºæ™¯ä¸­æ¢å¤ç»†èŠ‚çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°å‹æ¡†æ¶èƒ½å¤Ÿé‡å»ºå•ç›®è§†é¢‘ä¸­çš„åŠ¨æ€äºº-ç‰©ä½“äº¤äº’ã€‚</li>
<li>è¯¥æ¡†æ¶å…‹æœäº†é®æŒ¡å’Œæš‚æ—¶ä¸ä¸€è‡´çš„æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿ3Dé‡å»ºæ–¹æ³•é€šå¸¸å‡è®¾ç‰©ä½“é™æ€æˆ–å®Œå…¨å¯è§ï¼Œä½†åœ¨å®é™…åœºæ™¯ä¸­è¿™äº›å‡è®¾å¯èƒ½ä¸æˆç«‹ã€‚</li>
<li>æ¡†æ¶åˆ©ç”¨æ¨¡æ€å®Œæˆæ³•æ¨æ–­éƒ¨åˆ†é®æŒ¡åŒºåŸŸçš„å®Œæ•´ç»“æ„ã€‚</li>
<li>ä¸åªåœ¨å•ä¸ªå¸§ä¸Šæ“ä½œçš„æ–¹æ³•ä¸åŒï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æ—¶é—´ä¸Šä¸‹æ–‡ï¼Œä¿æŒè§†é¢‘åºåˆ—ä¹‹é—´çš„è¿è´¯æ€§ã€‚</li>
<li>æ— æ¨¡æ¿ç­–ç•¥ä½¿å…¶é€‚åº”å„ç§æ¡ä»¶ï¼Œæ— éœ€ä¾èµ–é¢„è®¾æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08137">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-500aaec2355f11738d9faf3985db308f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b60ec09ad6c0d9ab5fb47c0db1aba86b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d51f4726f06cad14d2161db44429b26c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-008b3117a9208e938d4af5d2a105cadb.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SegmentDreamer-Towards-High-fidelity-Text-to-3D-Synthesis-with-Segmented-Consistency-Trajectory-Distillation"><a href="#SegmentDreamer-Towards-High-fidelity-Text-to-3D-Synthesis-with-Segmented-Consistency-Trajectory-Distillation" class="headerlink" title="SegmentDreamer: Towards High-fidelity Text-to-3D Synthesis with   Segmented Consistency Trajectory Distillation"></a>SegmentDreamer: Towards High-fidelity Text-to-3D Synthesis with   Segmented Consistency Trajectory Distillation</h2><p><strong>Authors:Jiahao Zhu, Zixuan Chen, Guangcong Wang, Xiaohua Xie, Yi Zhou</strong></p>
<p>Recent advancements in text-to-3D generation improve the visual quality of Score Distillation Sampling (SDS) and its variants by directly connecting Consistency Distillation (CD) to score distillation. However, due to the imbalance between self-consistency and cross-consistency, these CD-based methods inherently suffer from improper conditional guidance, leading to sub-optimal generation results. To address this issue, we present SegmentDreamer, a novel framework designed to fully unleash the potential of consistency models for high-fidelity text-to-3D generation. Specifically, we reformulate SDS through the proposed Segmented Consistency Trajectory Distillation (SCTD), effectively mitigating the imbalance issues by explicitly defining the relationship between self- and cross-consistency. Moreover, SCTD partitions the Probability Flow Ordinary Differential Equation (PF-ODE) trajectory into multiple sub-trajectories and ensures consistency within each segment, which can theoretically provide a significantly tighter upper bound on distillation error. Additionally, we propose a distillation pipeline for a more swift and stable generation. Extensive experiments demonstrate that our SegmentDreamer outperforms state-of-the-art methods in visual quality, enabling high-fidelity 3D asset creation through 3D Gaussian Splatting (3DGS). </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬åˆ°3Dç”Ÿæˆçš„è¿›å±•é€šè¿‡ç›´æ¥å°†ä¸€è‡´æ€§è’¸é¦ï¼ˆCDï¼‰ä¸å¾—åˆ†è’¸é¦ç›¸è”ç³»ï¼Œæé«˜äº†å¾—åˆ†è’¸é¦é‡‡æ ·ï¼ˆSDSï¼‰åŠå…¶å˜ä½“çš„è§†è§‰è´¨é‡ã€‚ç„¶è€Œï¼Œç”±äºè‡ªä¸€è‡´æ€§å’Œè·¨ä¸€è‡´æ€§ä¹‹é—´çš„ä¸å¹³è¡¡ï¼Œè¿™äº›åŸºäºCDçš„æ–¹æ³•ä¸å¯é¿å…åœ°å­˜åœ¨ä¸å½“çš„æ¡ä»¶æŒ‡å¯¼ï¼Œå¯¼è‡´ç”Ÿæˆç»“æœä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SegmentDreamerï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å……åˆ†é‡Šæ”¾ä¸€è‡´æ€§æ¨¡å‹æ½œåŠ›çš„é«˜ä¿çœŸæ–‡æœ¬åˆ°3Dç”Ÿæˆçš„æ–°å‹æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡æå‡ºçš„åˆ†æ®µä¸€è‡´æ€§è½¨è¿¹è’¸é¦ï¼ˆSCTDï¼‰é‡æ–°å®šä¹‰äº†SDSï¼Œé€šè¿‡æ˜ç¡®è‡ªä¸€è‡´æ€§å’Œè·¨ä¸€è‡´æ€§ä¹‹é—´çš„å…³ç³»ï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†ä¸å¹³è¡¡é—®é¢˜ã€‚æ­¤å¤–ï¼ŒSCTDå°†æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆPF-ODEï¼‰è½¨è¿¹åˆ’åˆ†ä¸ºå¤šä¸ªå­è½¨è¿¹ï¼Œç¡®ä¿æ¯ä¸ªæ®µå†…çš„ä¸€è‡´æ€§ï¼Œè¿™åœ¨ç†è®ºä¸Šå¯ä»¥ä¸ºè’¸é¦è¯¯å·®æä¾›æ›´ç´§å¯†çš„ä¸Šç•Œã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ›´å¿«é€Ÿç¨³å®šçš„è’¸é¦ç®¡é“ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SegmentDreameråœ¨è§†è§‰è´¨é‡ä¸Šè¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œé€šè¿‡3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰å®ç°äº†é«˜ä¿çœŸ3Dèµ„äº§åˆ›å»ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.05256v2">PDF</a> Accepted by ICCV 2025, project page:   <a target="_blank" rel="noopener" href="https://zjhjojo.github.io/segmentdreamer/">https://zjhjojo.github.io/segmentdreamer/</a></p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æè¿°äº†æœ€æ–°è¿›å±•åœ¨æ–‡æœ¬åˆ°ä¸‰ç»´ç”Ÿæˆçš„æŠ€æœ¯ï¼Œä»‹ç»äº†Score Distillation Samplingï¼ˆSDSï¼‰åŠå…¶å˜ä½“çš„è§†è§‰è´¨é‡æå‡æ–¹æ³•ã€‚æ–‡ç« æŒ‡å‡ºCDï¼ˆä¸€è‡´æ€§è’¸é¦ï¼‰æŠ€æœ¯ä¸å…¶ä»–æŠ€æœ¯ç›¸ç»“åˆçš„ä¼˜åŠ¿ï¼Œä½†ç”±äºè‡ªæˆ‘ä¸€è‡´æ€§å’Œè·¨ä¸€è‡´æ€§ä¹‹é—´çš„ä¸å¹³è¡¡é—®é¢˜ï¼Œè¿™äº›åŸºäºCDçš„æ–¹æ³•å­˜åœ¨æ½œåœ¨çš„æ€§èƒ½éšœç¢ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†SegmentDreameræ¡†æ¶ï¼Œé€šè¿‡åˆ†æ®µä¸€è‡´æ€§è½¨è¿¹è’¸é¦ï¼ˆSCTDï¼‰æŠ€æœ¯é‡æ–°å®šä¹‰äº†SDSï¼Œæœ‰æ•ˆè§£å†³äº†ä¸å¹³è¡¡é—®é¢˜ï¼Œå¹¶ä¼˜åŒ–äº†æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆPF-ODEï¼‰è½¨è¿¹çš„å¤šä¸ªå­è½¨è¿¹ï¼Œç¡®ä¿æ¯ä¸ªåˆ†æ®µå†…çš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æå‡ºäº†æ›´å¿«é€Ÿç¨³å®šçš„ç”Ÿæˆè’¸é¦ç®¡é“ã€‚å®éªŒè¯æ˜ï¼ŒSegmentDreameråœ¨è§†è§‰è´¨é‡ä¸Šè¶…è¿‡äº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶é€šè¿‡ä¸‰ç»´é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰å®ç°äº†é«˜ä¿çœŸä¸‰ç»´èµ„äº§åˆ›å»ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°ä¸‰ç»´ç”ŸæˆæŠ€æœ¯æœ‰æ‰€æå‡ï¼Œé€šè¿‡Score Distillation Samplingï¼ˆSDSï¼‰åŠå…¶å˜ä½“æ”¹è¿›è§†è§‰è´¨é‡ã€‚</li>
<li>ä¸€è‡´æ€§è’¸é¦ï¼ˆCDï¼‰æŠ€æœ¯åœ¨æå‡æ–‡æœ¬åˆ°ä¸‰ç»´ç”Ÿæˆæ•ˆæœæ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>CD-basedæ–¹æ³•å­˜åœ¨è‡ªæˆ‘ä¸€è‡´æ€§å’Œè·¨ä¸€è‡´æ€§ä¸å¹³è¡¡é—®é¢˜ï¼Œå¯¼è‡´æ¡ä»¶æŒ‡å¯¼ä¸å½“å’Œç”Ÿæˆç»“æœä¸ä½³ã€‚</li>
<li>SegmentDreameræ¡†æ¶é€šè¿‡åˆ†æ®µä¸€è‡´æ€§è½¨è¿¹è’¸é¦ï¼ˆSCTDï¼‰è§£å†³äº†CDæ–¹æ³•çš„ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>SCTDæŠ€æœ¯é‡æ–°å®šä¹‰äº†SDSï¼Œä¼˜åŒ–äº†æ¦‚ç‡æµå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆPF-ODEï¼‰è½¨è¿¹çš„å­è½¨è¿¹ã€‚</li>
<li>SegmentDreameræå‡ºæ›´å¿«é€Ÿç¨³å®šçš„ç”Ÿæˆè’¸é¦ç®¡é“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.05256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b35e4c2b6dd83884e355fd3f3e3f951b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abf0cfc2a5689de7a7784f9c2d6d4a95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-402f17b41318a68e0409c7186b25c0ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34b5118dd8f3dd8ef66aba5f1f56b1af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85115c22e044cae71a63fe33062374cc.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TSGS-Improving-Gaussian-Splatting-for-Transparent-Surface-Reconstruction-via-Normal-and-De-lighting-Priors"><a href="#TSGS-Improving-Gaussian-Splatting-for-Transparent-Surface-Reconstruction-via-Normal-and-De-lighting-Priors" class="headerlink" title="TSGS: Improving Gaussian Splatting for Transparent Surface   Reconstruction via Normal and De-lighting Priors"></a>TSGS: Improving Gaussian Splatting for Transparent Surface   Reconstruction via Normal and De-lighting Priors</h2><p><strong>Authors:Mingwei Li, Pu Pang, Hehe Fan, Hua Huang, Yi Yang</strong></p>
<p>Reconstructing transparent surfaces is essential for tasks such as robotic manipulation in labs, yet it poses a significant challenge for 3D reconstruction techniques like 3D Gaussian Splatting (3DGS). These methods often encounter a transparency-depth dilemma, where the pursuit of photorealistic rendering through standard $\alpha$-blending undermines geometric precision, resulting in considerable depth estimation errors for transparent materials. To address this issue, we introduce Transparent Surface Gaussian Splatting (TSGS), a new framework that separates geometry learning from appearance refinement. In the geometry learning stage, TSGS focuses on geometry by using specular-suppressed inputs to accurately represent surfaces. In the second stage, TSGS improves visual fidelity through anisotropic specular modeling, crucially maintaining the established opacity to ensure geometric accuracy. To enhance depth inference, TSGS employs a first-surface depth extraction method. This technique uses a sliding window over $\alpha$-blending weights to pinpoint the most likely surface location and calculates a robust weighted average depth. To evaluate the transparent surface reconstruction task under realistic conditions, we collect a TransLab dataset that includes complex transparent laboratory glassware. Extensive experiments on TransLab show that TSGS achieves accurate geometric reconstruction and realistic rendering of transparent objects simultaneously within the efficient 3DGS framework. Specifically, TSGS significantly surpasses current leading methods, achieving a 37.3% reduction in chamfer distance and an 8.0% improvement in F1 score compared to the top baseline. The code and dataset are available at <a target="_blank" rel="noopener" href="https://longxiang-ai.github.io/TSGS/">https://longxiang-ai.github.io/TSGS/</a>. </p>
<blockquote>
<p>é‡å»ºé€æ˜è¡¨é¢å¯¹äºå®éªŒå®¤çš„æœºå™¨äººæ“ä½œç­‰ä»»åŠ¡è‡³å…³é‡è¦ï¼Œç„¶è€Œï¼Œå®ƒä¸º3Dé‡å»ºæŠ€æœ¯ï¼ˆä¾‹å¦‚3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰ï¼‰å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚è¿™äº›æ–¹æ³•ç»å¸¸é¢ä¸´é€æ˜åº¦ä¸æ·±åº¦ä¹‹é—´çš„ä¸¤éš¾å›°å¢ƒï¼Œå…¶ä¸­é€šè¿‡æ ‡å‡†alphaæ··åˆè¿½æ±‚é€¼çœŸçš„æ¸²æŸ“ä¼šç ´åå‡ ä½•ç²¾åº¦ï¼Œå¯¼è‡´å¯¹é€æ˜ææ–™çš„æ·±åº¦ä¼°è®¡å‡ºç°é‡å¤§è¯¯å·®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†é€æ˜è¡¨é¢é«˜æ–¯æ‹¼è´´ï¼ˆTSGSï¼‰è¿™ä¸€æ–°æ¡†æ¶ï¼Œå®ƒå°†å‡ ä½•å­¦ä¹ ä¸å¤–è§‚ä¼˜åŒ–åˆ†ç¦»ã€‚åœ¨å‡ ä½•å­¦ä¹ é˜¶æ®µï¼ŒTSGSé€šè¿‡ä½¿ç”¨æŠ‘åˆ¶é•œé¢åå°„çš„è¾“å…¥æ¥å‡†ç¡®è¡¨ç¤ºè¡¨é¢ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼ŒTSGSé€šè¿‡å„å‘å¼‚æ€§é•œé¢å»ºæ¨¡æé«˜è§†è§‰ä¿çœŸåº¦ï¼Œå…³é”®çš„æ˜¯ä¿æŒå·²å»ºç«‹çš„é€æ˜åº¦ä»¥ç¡®ä¿å‡ ä½•ç²¾åº¦ã€‚ä¸ºäº†æé«˜æ·±åº¦æ¨æ–­èƒ½åŠ›ï¼ŒTSGSé‡‡ç”¨äº†ä¸€ç§åŸºäºé¦–è¡¨é¢æ·±åº¦æå–æ–¹æ³•çš„æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯é€šè¿‡alphaæ··åˆæƒé‡ä¸Šçš„æ»‘åŠ¨çª—å£æ¥ç²¾ç¡®å®šä½è¡¨é¢ä½ç½®ï¼Œå¹¶è®¡ç®—ä¸€ä¸ªç¨³å¥çš„åŠ æƒå¹³å‡æ·±åº¦ã€‚ä¸ºäº†åœ¨å®é™…æ¡ä»¶ä¸‹è¯„ä¼°é€æ˜è¡¨é¢é‡å»ºä»»åŠ¡ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªTransLabæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å¤æ‚çš„é€æ˜å®éªŒå®¤ç»ç’ƒå™¨çš¿ã€‚åœ¨TransLabä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒTSGSèƒ½å¤Ÿåœ¨é«˜æ•ˆçš„3DGSæ¡†æ¶å†…å®ç°é€æ˜ç‰©ä½“çš„å‡†ç¡®å‡ ä½•é‡å»ºå’Œé€¼çœŸæ¸²æŸ“ã€‚å…·ä½“æ¥è¯´ï¼ŒTSGSå¤§å¤§è¶…è¶Šäº†å½“å‰çš„ä¸»æµæ–¹æ³•ï¼Œä¸é¡¶çº§åŸºçº¿ç›¸æ¯”ï¼Œè½¦èº«è·ç¦»å‡å°‘äº†37.3%ï¼ŒF1åˆ†æ•°æé«˜äº†8.0%ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨[<a target="_blank" rel="noopener" href="https://longxiang-ai.github.io/TSGS/]%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://longxiang-ai.github.io/TSGS/]ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12799v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://longxiang-ai.github.io/TSGS/">https://longxiang-ai.github.io/TSGS/</a> . Accepted by ACM   MM 2025</p>
<p><strong>Summary</strong></p>
<p>é€æ˜è¡¨é¢é‡å»ºæ˜¯å®éªŒå®¤æœºå™¨äººæ“ä½œç­‰ä»»åŠ¡çš„å…³é”®ï¼Œä½†å¯¹äº3Dé‡å»ºæŠ€æœ¯ï¼ˆå¦‚3Dé«˜æ–¯è´´ç‰‡æŠ€æœ¯ï¼‰è€Œè¨€æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚æ ‡å‡†çš„èåˆæ–¹æ³•ä¼šæŸå®³å‡ ä½•ç²¾åº¦ï¼Œå¯¼è‡´æ·±åº¦ä¼°è®¡é”™è¯¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºé€æ˜è¡¨é¢é«˜æ–¯è´´ç‰‡æŠ€æœ¯ï¼ˆTSGSï¼‰ï¼Œè¯¥æŠ€æœ¯å°†å‡ ä½•å­¦ä¹ ä¸å¤–è§‚ä¼˜åŒ–åˆ†å¼€ã€‚é¦–å…ˆï¼ŒTSGSä½¿ç”¨æŠ‘åˆ¶é•œé¢åå°„çš„å›¾åƒæ¥å‡†ç¡®è¡¨ç¤ºè¡¨é¢å‡ ä½•ç»“æ„ï¼›æ¥ç€ï¼Œé€šè¿‡å„å‘å¼‚æ€§é•œé¢æ¨¡å‹æé«˜è§†è§‰ä¿çœŸåº¦å¹¶ç»´æŒé€æ˜åº¦ä¿è¯å‡ ä½•ç²¾åº¦ã€‚ä¸ºå¢å¼ºæ·±åº¦æ¨æ–­ï¼ŒTSGSé‡‡ç”¨é¦–è¡¨é¢æ·±åº¦æå–æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨å…·æœ‰å¤æ‚é€æ˜å®éªŒå®¤å™¨çš¿çš„TransLabæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼Œè¯æ˜TSGSåœ¨é€æ˜ç‰©ä½“å‡†ç¡®å‡ ä½•é‡å»ºå’ŒçœŸå®æ„Ÿæ¸²æŸ“æ–¹é¢å…·æœ‰å“è¶Šæ€§èƒ½ï¼Œç›¸è¾ƒäºç°æœ‰é¡¶å°–æ–¹æ³•æœ‰æ˜æ˜¾æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€æ˜è¡¨é¢é‡å»ºæ˜¯å®éªŒå®¤æœºå™¨äººæ“ä½œç­‰ä»»åŠ¡çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿ3Dé‡å»ºæŠ€æœ¯åœ¨å¤„ç†é€æ˜ç‰©ä½“æ—¶é¢ä¸´é€æ˜åº¦ä¸æ·±åº¦ä¼°è®¡çš„å›°å¢ƒã€‚</li>
<li>TSGSæŠ€æœ¯é€šè¿‡åˆ†ç¦»å‡ ä½•å­¦ä¹ ä¸å¤–è§‚ä¼˜åŒ–æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>TSGSä½¿ç”¨æŠ‘åˆ¶é•œé¢åå°„çš„å›¾åƒè¿›è¡Œå‡ ä½•å­¦ä¹ ï¼Œç¡®ä¿å‡†ç¡®è¡¨ç¤ºè¡¨é¢ç»“æ„ã€‚</li>
<li>TSGSé‡‡ç”¨å„å‘å¼‚æ€§é•œé¢æ¨¡å‹æé«˜è§†è§‰è´¨é‡å¹¶ä¿æŒé€æ˜åº¦ä»¥ç»´æŒå‡ ä½•ç²¾åº¦ã€‚</li>
<li>TSGSé‡‡ç”¨é¦–è¡¨é¢æ·±åº¦æå–æŠ€æœ¯ä»¥å¢å¼ºæ·±åº¦æ¨æ–­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12799">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c96dd672a9af70f0e311c11d631f306a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e5eb595d5d5a7026406088483b85581b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38dc100224377513c478bce2ec3f899c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-215b505a4bbae5e4a4c42db3f4487ba9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb9dd20b5b6a58525db2dead2a9108a6.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Embracing-Dynamics-Dynamics-aware-4D-Gaussian-Splatting-SLAM"><a href="#Embracing-Dynamics-Dynamics-aware-4D-Gaussian-Splatting-SLAM" class="headerlink" title="Embracing Dynamics: Dynamics-aware 4D Gaussian Splatting SLAM"></a>Embracing Dynamics: Dynamics-aware 4D Gaussian Splatting SLAM</h2><p><strong>Authors:Zhicong Sun, Jacqueline Lo, Jinxing Hu</strong></p>
<p>Simultaneous localization and mapping (SLAM) technology has recently achieved photorealistic mapping capabilities thanks to the real-time, high-fidelity rendering enabled by 3D Gaussian Splatting (3DGS). However, due to the static representation of scenes, current 3DGS-based SLAM encounters issues with pose drift and failure to reconstruct accurate maps in dynamic environments. To address this problem, we present D4DGS-SLAM, the first SLAM method based on 4DGS map representation for dynamic environments. By incorporating the temporal dimension into scene representation, D4DGS-SLAM enables high-quality reconstruction of dynamic scenes. Utilizing the dynamics-aware InfoModule, we can obtain the dynamics, visibility, and reliability of scene points, and filter out unstable dynamic points for tracking accordingly. When optimizing Gaussian points, we apply different isotropic regularization terms to Gaussians with varying dynamic characteristics. Experimental results on real-world dynamic scene datasets demonstrate that our method outperforms state-of-the-art approaches in both camera pose tracking and map quality. </p>
<blockquote>
<p>åŒæ—¶å®šä½ä¸åœ°å›¾æ„å»ºï¼ˆSLAMï¼‰æŠ€æœ¯æœ€è¿‘ç”±äº3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰æ‰€å®ç°çš„å®æ—¶é«˜ä¿çœŸæ¸²æŸ“è€Œè·å¾—äº†é€¼çœŸçš„æ˜ å°„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºåœºæ™¯é™æ€è¡¨ç¤ºï¼Œå½“å‰åŸºäº3DGSçš„SLAMåœ¨åŠ¨æ€ç¯å¢ƒä¸­é‡åˆ°äº†å§¿æ€æ¼‚ç§»å’Œæ— æ³•é‡å»ºå‡†ç¡®åœ°å›¾çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†D4DGS-SLAMï¼Œè¿™æ˜¯åŸºäº4DGSåœ°å›¾è¡¨ç¤ºçš„ç”¨äºåŠ¨æ€ç¯å¢ƒçš„ç¬¬ä¸€ä¸ªSLAMæ–¹æ³•ã€‚é€šè¿‡å°†æ—¶é—´ç»´åº¦èå…¥åœºæ™¯è¡¨ç¤ºï¼ŒD4DGS-SLAMèƒ½å¤Ÿå®ç°åŠ¨æ€åœºæ™¯çš„é«˜è´¨é‡é‡å»ºã€‚é€šè¿‡ä½¿ç”¨åŠ¨åŠ›å­¦æ„ŸçŸ¥çš„InfoModuleï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—åœºæ™¯ç‚¹çš„åŠ¨åŠ›å­¦ã€å¯è§æ€§å’Œå¯é æ€§ï¼Œå¹¶ç›¸åº”åœ°è¿‡æ»¤å‡ºä¸ç¨³å®šåŠ¨æ€ç‚¹è¿›è¡Œè·Ÿè¸ªã€‚åœ¨ä¼˜åŒ–é«˜æ–¯ç‚¹æ—¶ï¼Œæˆ‘ä»¬å¯¹å…·æœ‰ä¸åŒåŠ¨æ€ç‰¹å¾çš„é«˜æ–¯åº”ç”¨ä¸åŒçš„ç­‰è·æ­£åˆ™åŒ–é¡¹ã€‚åœ¨çœŸå®ä¸–ç•ŒåŠ¨æ€åœºæ™¯æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç›¸æœºå§¿æ€è¿½è¸ªå’Œåœ°å›¾è´¨é‡æ–¹é¢å‡ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.04844v2">PDF</a> This paper has been accepted by IROS 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¸‰ç»´é«˜æ–¯èåˆï¼ˆ3DGSï¼‰æŠ€æœ¯çš„å®æ—¶æ¸²æŸ“èƒ½åŠ›ï¼ŒåŒæ­¥å®šä½ä¸åœ°å›¾æ„å»ºï¼ˆSLAMï¼‰æŠ€æœ¯å·²å®ç°äº†é€¼çœŸçš„æ˜ å°„åŠŸèƒ½ã€‚ç„¶è€Œï¼Œç”±äºåœºæ™¯é™æ€è¡¨ç¤ºçš„é™åˆ¶ï¼Œå½“å‰åŸºäº3DGSçš„SLAMåœ¨åŠ¨æ€ç¯å¢ƒä¸­å­˜åœ¨å§¿æ€æ¼‚ç§»å’Œæ— æ³•å‡†ç¡®é‡å»ºåœ°å›¾çš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå››ç»´é«˜æ–¯èåˆï¼ˆ4DGSï¼‰åœ°å›¾è¡¨ç¤ºçš„SLAMæ–¹æ³•â€”â€”D4DGS-SLAMã€‚é€šè¿‡å¼•å…¥æ—¶é—´ç»´åº¦è¿›è¡Œåœºæ™¯è¡¨ç¤ºï¼ŒD4DGS-SLAMå¯å®ç°åŠ¨æ€åœºæ™¯çš„é«˜è´¨é‡é‡å»ºã€‚åˆ©ç”¨åŠ¨åŠ›å­¦æ„ŸçŸ¥çš„InfoModuleï¼Œæˆ‘ä»¬å¯ä»¥è·å–åœºæ™¯çš„åŠ¨æ€æ€§ã€å¯è§æ€§å’Œå¯é æ€§ä¿¡æ¯ï¼Œå¹¶æ®æ­¤è¿‡æ»¤å‡ºä¸ç¨³å®šåŠ¨æ€ç‚¹è¿›è¡Œè·Ÿè¸ªã€‚åœ¨ä¼˜åŒ–é«˜æ–¯ç‚¹æ—¶ï¼Œæˆ‘ä»¬å¯¹å…·æœ‰ä¸åŒåŠ¨æ€ç‰¹æ€§çš„é«˜æ–¯åº”ç”¨ä¸åŒçš„ç­‰è·æ­£åˆ™åŒ–é¡¹ã€‚åœ¨çœŸå®ä¸–ç•ŒåŠ¨æ€åœºæ™¯æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç›¸æœºå§¿æ€è·Ÿè¸ªå’Œåœ°å›¾è´¨é‡æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºä¸‰ç»´é«˜æ–¯èåˆï¼ˆ3DGSï¼‰æŠ€æœ¯çš„åŒæ­¥å®šä½ä¸åœ°å›¾æ„å»ºï¼ˆSLAMï¼‰æŠ€æœ¯å·²å®ç°äº†é€¼çœŸçš„æ˜ å°„åŠŸèƒ½ã€‚</li>
<li>å½“å‰åŸºäº3DGSçš„SLAMåœ¨åŠ¨æ€ç¯å¢ƒä¸­å­˜åœ¨å§¿æ€æ¼‚ç§»å’Œåœ°å›¾ç²¾åº¦é—®é¢˜ã€‚</li>
<li>ä¸ºè§£å†³åŠ¨æ€ç¯å¢ƒä¸‹çš„å§¿æ€æ¼‚ç§»é—®é¢˜ï¼Œé¦–æ¬¡æå‡ºäº†åŸºäºå››ç»´é«˜æ–¯èåˆï¼ˆ4DGSï¼‰åœ°å›¾è¡¨ç¤ºçš„SLAMæ–¹æ³•â€”â€”D4DGS-SLAMã€‚</li>
<li>D4DGS-SLAMé€šè¿‡å¼•å…¥æ—¶é—´ç»´åº¦ï¼Œå®ç°äº†åŠ¨æ€åœºæ™¯çš„é«˜è´¨é‡é‡å»ºã€‚</li>
<li>D4DGS-SLAMåˆ©ç”¨åŠ¨åŠ›å­¦æ„ŸçŸ¥çš„InfoModuleè¿‡æ»¤ä¸ç¨³å®šåŠ¨æ€ç‚¹ï¼Œå¹¶ä¼˜åŒ–é«˜æ–¯ç‚¹ä»¥æ”¹å–„æ€§èƒ½ã€‚</li>
<li>D4DGS-SLAMåœ¨ç›¸æœºå§¿æ€è·Ÿè¸ªå’Œåœ°å›¾è´¨é‡æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.04844">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-13b05007705666e76fd708ff9c72fa4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a08f69537ef4a6979b3534f5fa2e41e7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d42d0b8d2aebdf35382119bbda140b10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b7d02519756333f4917233046a79057e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1257604e10c63d4aefcfa2cb6be3823e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b626be3672932bd3e937cfeb894f6bf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04fbb9c58b035b46c97099238d9fd1f3.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="StableGS-A-Floater-Free-Framework-for-3D-Gaussian-Splatting"><a href="#StableGS-A-Floater-Free-Framework-for-3D-Gaussian-Splatting" class="headerlink" title="StableGS: A Floater-Free Framework for 3D Gaussian Splatting"></a>StableGS: A Floater-Free Framework for 3D Gaussian Splatting</h2><p><strong>Authors:Luchao Wang, Qian Ren, Kaimin Liao, Hua Wang, Zhi Chen, Yaohua Tang</strong></p>
<p>3D Gaussian Splatting (3DGS) reconstructions are plagued by stubborn <code>floater&quot; artifacts that degrade their geometric and visual fidelity. We are the first to reveal the root cause: a fundamental conflict in the 3DGS optimization process where the opacity gradients of floaters vanish when their blended color reaches a pseudo-equilibrium of canceling errors against the background, trapping them in a spurious local minimum. To resolve this, we propose StableGS, a novel framework that decouples geometric regularization from final appearance rendering. Its core is a Dual Opacity architecture that creates two separate rendering paths: a </code>Geometric Regularization Pathâ€ to bear strong depth-based constraints for structural correctness, and an &#96;&#96;Appearance Refinement Pathâ€ to generate high-fidelity details upon this stable foundation. We complement this with a synergistic set of geometric constraints: a self-supervised depth consistency loss and an external geometric prior enabled by our efficient global scale optimization algorithm. Experiments on multiple benchmarks show StableGS not only eliminates floaters but also resolves the common blur-artifact trade-off, achieving state-of-the-art geometric accuracy and visual quality. </p>
<blockquote>
<p>3DGSé‡å»ºè¿‡ç¨‹ä¸­å—åˆ°â€œæ¼‚æµ®ç‰©â€é—®é¢˜çš„å›°æ‰°ï¼Œè¿™ä¸ªé—®é¢˜ä¼šå¯¼è‡´å…¶å‡ ä½•å’Œè§†è§‰çœŸå®æ„Ÿé™ä½ã€‚æˆ‘ä»¬æ˜¯é¦–ä¸ªæ­ç¤ºå…¶åŸå› çš„å›¢é˜Ÿï¼šåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å­˜åœ¨ä¸€ä¸ªåŸºæœ¬å†²çªï¼Œå³æ¼‚æµ®ç‰©çš„ä¸é€æ˜åº¦æ¢¯åº¦åœ¨å…¶æ··åˆé¢œè‰²è¾¾åˆ°ä¸èƒŒæ™¯ç›¸æŠµæ¶ˆçš„ä¼ªå¹³è¡¡çŠ¶æ€æ—¶æ¶ˆå¤±ï¼Œä½¿å®ƒä»¬é™·å…¥ä¸€ä¸ªè™šå‡çš„å±€éƒ¨æœ€å°å€¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†StableGSï¼Œè¿™æ˜¯ä¸€ä¸ªå°†å‡ ä½•æ­£åˆ™åŒ–ä¸æœ€ç»ˆå¤–è§‚æ¸²æŸ“è§£è€¦çš„æ–°å‹æ¡†æ¶ã€‚å…¶æ ¸å¿ƒæ˜¯åŒé‡ä¸é€æ˜åº¦æ¶æ„ï¼Œè¯¥æ¶æ„åˆ›å»ºäº†ä¸¤æ¡å•ç‹¬çš„æ¸²æŸ“è·¯å¾„ï¼šâ€œå‡ ä½•æ­£åˆ™åŒ–è·¯å¾„â€ï¼Œç”¨äºæ‰¿å—åŸºäºæ·±åº¦çš„å¼ºå¤§ç»“æ„çº¦æŸï¼›â€œå¤–è§‚ç»†åŒ–è·¯å¾„â€ï¼Œåœ¨ç¨³å®šçš„åŸºå‡†ä¸Šç”Ÿæˆé«˜ä¿çœŸç»†èŠ‚ã€‚æˆ‘ä»¬ä»¥ä¸€ç»„ååŒå‡ ä½•çº¦æŸæ¥è¡¥å……å®ƒï¼šé€šè¿‡è‡ªæˆ‘ç›‘ç£çš„æ·±åº¦ä¸€è‡´æ€§æŸå¤±å’Œç”±æˆ‘ä»¬çš„é«˜æ•ˆå…¨å±€å°ºåº¦ä¼˜åŒ–ç®—æ³•å¯ç”¨çš„å¤–éƒ¨å‡ ä½•å…ˆéªŒã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒStableGSä¸ä»…æ¶ˆé™¤äº†æ¼‚æµ®ç‰©é—®é¢˜ï¼Œè¿˜è§£å†³äº†å¸¸è§çš„æ¨¡ç³Šä¼ªå½±æƒè¡¡é—®é¢˜ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„å‡ ä½•ç²¾åº¦å’Œè§†è§‰è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18458v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡º3D Gaussian Splattingï¼ˆ3DGSï¼‰é‡å»ºä¸­å­˜åœ¨çš„â€œæµ®å­â€ä¼ªå½±é—®é¢˜ï¼Œåˆ†æå…¶æ ¹æœ¬åŸå› ä¸ºä¼˜åŒ–è¿‡ç¨‹ä¸­çš„åŸºæœ¬å†²çªã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºStableGSæ¡†æ¶ï¼Œé‡‡ç”¨åŒé‡é€æ˜åº¦æ¶æ„ï¼Œåˆ›å»ºä¸¤ä¸ªç‹¬ç«‹æ¸²æŸ“è·¯å¾„ï¼Œåˆ†åˆ«è´Ÿè´£ç»“æ„æ­£ç¡®æ€§çš„å‡ ä½•æ­£åˆ™åŒ–å’Œåœ¨æ­¤åŸºç¡€ä¸Šç”Ÿæˆé«˜ä¿çœŸç»†èŠ‚çš„å¤–è§‚ä¼˜åŒ–è·¯å¾„ã€‚åŒæ—¶å¼•å…¥ä¸€ç³»åˆ—å‡ ä½•çº¦æŸï¼ŒåŒ…æ‹¬è‡ªç›‘ç£æ·±åº¦ä¸€è‡´æ€§æŸå¤±å’Œç”±é«˜æ•ˆå…¨å±€å°ºåº¦ä¼˜åŒ–ç®—æ³•å®ç°çš„å¤–éƒ¨å‡ ä½•å…ˆéªŒã€‚å®éªŒè¡¨æ˜ï¼ŒStableGSä¸ä»…èƒ½æ¶ˆé™¤æµ®å­ä¼ªå½±ï¼Œè¿˜èƒ½è§£å†³å¸¸è§çš„æ¨¡ç³Šä¼ªå½±æƒè¡¡é—®é¢˜ï¼Œå®ç°æœ€å…ˆè¿›çš„å‡ ä½•ç²¾åº¦å’Œè§†è§‰è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGSé‡å»ºå­˜åœ¨â€œæµ®å­â€ä¼ªå½±é—®é¢˜ï¼Œå½±å“å‡ ä½•å’Œè§†è§‰ä¿çœŸåº¦ã€‚</li>
<li>æµ®å­ä¼ªå½±çš„æ ¹æœ¬åŸå› æ˜¯ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„åŸºæœ¬å†²çªï¼Œå¯¼è‡´é€æ˜åº¦æ¢¯åº¦æ¶ˆå¤±ã€‚</li>
<li>StableGSæ¡†æ¶é‡‡ç”¨åŒé‡é€æ˜åº¦æ¶æ„ï¼Œåˆ›å»ºä¸¤ä¸ªç‹¬ç«‹æ¸²æŸ“è·¯å¾„ï¼šå‡ ä½•æ­£åˆ™åŒ–å’Œå¤–è§‚ä¼˜åŒ–è·¯å¾„ã€‚</li>
<li>StableGSå¼•å…¥è‡ªç›‘ç£æ·±åº¦ä¸€è‡´æ€§æŸå¤±å’Œå¤–éƒ¨å‡ ä½•å…ˆéªŒï¼Œæé«˜ç»“æ„æ­£ç¡®æ€§å’Œé«˜ä¿çœŸç»†èŠ‚ç”Ÿæˆã€‚</li>
<li>StableGSè§£å†³äº†æµ®å­ä¼ªå½±é—®é¢˜ï¼Œå®ç°äº†æ›´ç²¾ç¡®çš„å‡ ä½•é‡å»ºã€‚</li>
<li>StableGSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶å‡ ä½•ç²¾åº¦å’Œè§†è§‰è´¨é‡è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18458">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-22708a26b343f1d394e2fb1ae390c296.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ee5b04d4c8947f65f84f25ec2897d82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aee465b898244da28060f7fb911464f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aaaa2a65f45fa9b56a26b3cccbcf468b.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Surgical-Gaussian-Surfels-Highly-Accurate-Real-time-Surgical-Scene-Rendering-using-Gaussian-Surfels"><a href="#Surgical-Gaussian-Surfels-Highly-Accurate-Real-time-Surgical-Scene-Rendering-using-Gaussian-Surfels" class="headerlink" title="Surgical Gaussian Surfels: Highly Accurate Real-time Surgical Scene   Rendering using Gaussian Surfels"></a>Surgical Gaussian Surfels: Highly Accurate Real-time Surgical Scene   Rendering using Gaussian Surfels</h2><p><strong>Authors:Idris O. Sunmola, Zhenjun Zhao, Samuel Schmidgall, Yumeng Wang, Paul Maria Scheikl, Viet Pham, Axel Krieger</strong></p>
<p>Accurate geometric reconstruction of deformable tissues in monocular endoscopic video remains a fundamental challenge in robot-assisted minimally invasive surgery. Although recent volumetric and point primitive methods based on neural radiance fields (NeRF) and 3D Gaussian primitives have efficiently rendered surgical scenes, they still struggle with handling artifact-free tool occlusions and preserving fine anatomical details. These limitations stem from unrestricted Gaussian scaling and insufficient surface alignment constraints during reconstruction. To address these issues, we introduce Surgical Gaussian Surfels (SGS), which transform anisotropic point primitives into surface-aligned elliptical splats by constraining the scale component of the Gaussian covariance matrix along the view-aligned axis. We also introduce the Fully Fused Deformation Multilayer Perceptron (FFD-MLP), a lightweight Multi-Layer Perceptron (MLP) that predicts accurate surfel motion fields up to 5x faster than a standard MLP. This is coupled with locality constraints to handle complex tissue deformations. We use homodirectional view-space positional gradients to capture fine image details by splitting Gaussian Surfels in over-reconstructed regions. In addition, we define surface normals as the direction of the steepest density change within each Gaussian surfel primitive, enabling accurate normal estimation without requiring monocular normal priors. We evaluate our method on two in-vivo surgical datasets, where it outperforms current state-of-the-art methods in surface geometry, normal map quality, and rendering efficiency, while remaining competitive in real-time rendering performance. We make our code available at <a target="_blank" rel="noopener" href="https://github.com/aloma85/SurgicalGaussianSurfels">https://github.com/aloma85/SurgicalGaussianSurfels</a> </p>
<blockquote>
<p>åœ¨å•ç›®å†…çª¥é•œè§†é¢‘ä¸­ï¼Œå¯å˜å½¢ç»„ç»‡çš„ç²¾ç¡®å‡ ä½•é‡å»ºä»ç„¶æ˜¯æœºå™¨äººè¾…åŠ©å¾®åˆ›æ‰‹æœ¯ä¸­çš„ä¸€é¡¹åŸºæœ¬æŒ‘æˆ˜ã€‚å°½ç®¡æœ€è¿‘åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œ3Dé«˜æ–¯åŸå§‹ç‚¹çš„ä½“ç§¯å’Œç‚¹åŸå§‹æ–¹æ³•å·²ç»æœ‰æ•ˆåœ°å‘ˆç°äº†æ‰‹æœ¯åœºæ™¯ï¼Œä½†å®ƒä»¬ä»ç„¶åœ¨å¤„ç†æ— å·¥å…·é®æŒ¡ç‰©å’Œä¿ç•™ç²¾ç»†è§£å‰–ç»†èŠ‚æ–¹é¢å­˜åœ¨å›°éš¾ã€‚è¿™äº›å±€é™æ€§æºäºé«˜æ–¯ç¼©æ”¾çš„æ— é™åˆ¶æ€§å’Œé‡å»ºè¿‡ç¨‹ä¸­è¡¨é¢å¯¹é½çº¦æŸçš„ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ‰‹æœ¯é«˜æ–¯æ›²é¢å…ƒç´ ï¼ˆSurgical Gaussian Surfelsï¼ŒSGSï¼‰ï¼Œå®ƒå°†å„å‘å¼‚æ€§çš„ç‚¹åŸå§‹å…ƒç´ é€šè¿‡çº¦æŸé«˜æ–¯åæ–¹å·®çŸ©é˜µçš„å°ºåº¦æˆåˆ†åœ¨è§†å›¾å¯¹é½è½´ä¸Šè½¬æ¢ä¸ºè¡¨é¢å¯¹é½çš„æ¤­åœ†å¹³æ¿ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å…¨èåˆå˜å½¢å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆFully Fused Deformation Multilayer Perceptronï¼ŒFFD-MLPï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ï¼Œå®ƒèƒ½å¤Ÿä»¥æ¯”æ ‡å‡†MLPå¿«5å€çš„é€Ÿåº¦é¢„æµ‹å‡†ç¡®çš„æ›²é¢å…ƒç´ è¿åŠ¨åœºã€‚è¿™ä¸å±€éƒ¨çº¦æŸç›¸ç»“åˆï¼Œå¯ä»¥å¤„ç†å¤æ‚çš„ç»„ç»‡å˜å½¢ã€‚æˆ‘ä»¬é€šè¿‡å°†åŒæ–¹å‘è§†å›¾ç©ºé—´ä½ç½®æ¢¯åº¦ç”¨äºæ•è·å›¾åƒç»†èŠ‚ï¼Œå°†é«˜æ–¯æ›²é¢å…ƒç´ åœ¨è¿‡åº¦é‡å»ºåŒºåŸŸä¸­æ‹†åˆ†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†è¡¨é¢æ³•çº¿å®šä¹‰ä¸ºæ¯ä¸ªé«˜æ–¯æ›²é¢å…ƒç´ åŸå§‹å†…éƒ¨å¯†åº¦å˜åŒ–æœ€é™¡å³­çš„æ–¹å‘ï¼Œä»è€Œèƒ½å¤Ÿåœ¨ä¸éœ€è¦å•çœ¼æ­£å¸¸å…ˆéªŒçš„æƒ…å†µä¸‹è¿›è¡Œå‡†ç¡®çš„æ³•çº¿ä¼°è®¡ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä½“å†…æ‰‹æœ¯æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨è¡¨é¢å‡ ä½•ã€æ³•çº¿å›¾è´¨é‡å’Œæ¸²æŸ“æ•ˆç‡æ–¹é¢ä¼˜äºå½“å‰æœ€æ–°æŠ€æœ¯ï¼ŒåŒæ—¶åœ¨å®æ—¶æ¸²æŸ“æ€§èƒ½æ–¹é¢ä¿æŒç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/aloma85/SurgicalGaussianSurfels%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/aloma85/SurgicalGaussianSurfelsè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04079v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨æœºå™¨äººè¾…åŠ©çš„å¾®åˆ›æ‰‹æœ¯ä¸­ï¼ŒåŸºäºç¥ç»è¾å°„åœºå’Œ3Dé«˜æ–¯åŸè¯­çš„æ–¹æ³•åœ¨å¤„ç†å·¥å…·é®æŒ¡å’Œä¿ç•™ç²¾ç»†è§£å‰–ç»†èŠ‚æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†Surgical Gaussian Surfelsï¼ˆSGSï¼‰æ–¹æ³•ï¼Œé€šè¿‡å°†å„å‘å¼‚æ€§ç‚¹åŸå§‹è½¬æ¢ä¸ºè¡¨é¢å¯¹é½çš„æ¤­åœ†å¹³æ¿ï¼Œå¹¶çº¦æŸé«˜æ–¯åæ–¹å·®çŸ©é˜µçš„å°ºåº¦æˆåˆ†æ¥æ”¹è¿›é‡å»ºã€‚åŒæ—¶ï¼Œå¼•å…¥Fully Fused Deformation Multilayer Perceptronï¼ˆFFD-MLPï¼‰ç½‘ç»œï¼Œé¢„æµ‹å‡†ç¡®çš„surfelè¿åŠ¨åœºï¼Œé€Ÿåº¦æ¯”æ ‡å‡†MLPå¿«5å€ã€‚æ­¤å¤–ï¼Œé€šè¿‡åŒæ–¹å‘è§†å›¾ç©ºé—´ä½ç½®æ¢¯åº¦æ•æ‰å›¾åƒç»†èŠ‚ï¼Œå¹¶é€šè¿‡å®šä¹‰é«˜æ–¯surfelåŸè¯­å†…æœ€é™¡å³­å¯†åº¦å˜åŒ–æ–¹å‘ä¸ºè¡¨é¢æ³•çº¿ï¼Œå®ç°å‡†ç¡®æ³•çº¿ä¼°è®¡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¡¨é¢å‡ ä½•ã€æ³•çº¿å›¾è´¨é‡å’Œæ¸²æŸ“æ•ˆç‡æ–¹é¢ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ¡ˆï¼Œå¹¶åœ¨å®æ—¶æ¸²æŸ“æ€§èƒ½ä¸Šä¿æŒç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥å¾„è¾å°„åœºå’Œé«˜æ–¯åŸå§‹æ–¹æ³•åœ¨æ‰‹æœ¯åœºæ™¯é‡å»ºä¸­é¢ä¸´å·¥å…·é®æŒ¡å’Œè§£å‰–ç»†èŠ‚ä¿ç•™çš„æŒ‘æˆ˜ã€‚</li>
<li>Surgical Gaussian Surfelsï¼ˆSGSï¼‰é€šè¿‡çº¦æŸé«˜æ–¯åæ–¹å·®çŸ©é˜µçš„å°ºåº¦æˆåˆ†æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå¹¶å°†å„å‘å¼‚æ€§ç‚¹åŸå§‹è½¬æ¢ä¸ºè¡¨é¢å¯¹é½çš„æ¤­åœ†å¹³æ¿ã€‚</li>
<li>å¼•å…¥Fully Fused Deformation Multilayer Perceptronï¼ˆFFD-MLPï¼‰ç½‘ç»œï¼Œé¢„æµ‹å‡†ç¡®çš„surfelè¿åŠ¨åœºï¼Œé€Ÿåº¦æ›´å¿«ã€‚</li>
<li>é€šè¿‡åŒæ–¹å‘è§†å›¾ç©ºé—´ä½ç½®æ¢¯åº¦æ•æ‰å›¾åƒç»†èŠ‚ï¼Œå®ç°ç²¾ç»†å›¾åƒé‡å»ºã€‚</li>
<li>é€šè¿‡å®šä¹‰é«˜æ–¯surfelåŸè¯­å†…çš„è¡¨é¢æ³•çº¿ï¼Œå®ç°å‡†ç¡®æ³•çº¿ä¼°è®¡ï¼Œæ— éœ€å•çœ¼æ³•çº¿å…ˆéªŒã€‚</li>
<li>åœ¨ä¸¤ä¸ªä½“å†…æ‰‹æœ¯æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¡¨é¢å‡ ä½•ã€æ³•çº¿å›¾è´¨é‡å’Œæ¸²æŸ“æ•ˆç‡æ–¹é¢ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04079">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a00ed29a89b9c55709272ec166b41eed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5632a38c3c76e0367ef17cb99f19afa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e11699f2118b90a03ce7f350aa3d6822.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="AutoOcc-Automatic-Open-Ended-Semantic-Occupancy-Annotation-via-Vision-Language-Guided-Gaussian-Splatting"><a href="#AutoOcc-Automatic-Open-Ended-Semantic-Occupancy-Annotation-via-Vision-Language-Guided-Gaussian-Splatting" class="headerlink" title="AutoOcc: Automatic Open-Ended Semantic Occupancy Annotation via   Vision-Language Guided Gaussian Splatting"></a>AutoOcc: Automatic Open-Ended Semantic Occupancy Annotation via   Vision-Language Guided Gaussian Splatting</h2><p><strong>Authors:Xiaoyu Zhou, Jingqi Wang, Yongtao Wang, Yufei Wei, Nan Dong, Ming-Hsuan Yang</strong></p>
<p>Obtaining high-quality 3D semantic occupancy from raw sensor data remains an essential yet challenging task, often requiring extensive manual labeling. In this work, we propose AutoOcc, a vision-centric automated pipeline for open-ended semantic occupancy annotation that integrates differentiable Gaussian splatting guided by vision-language models. We formulate the open-ended semantic 3D occupancy reconstruction task to automatically generate scene occupancy by combining attention maps from vision-language models and foundation vision models. We devise semantic-aware Gaussians as intermediate geometric descriptors and propose a cumulative Gaussian-to-voxel splatting algorithm that enables effective and efficient occupancy annotation. Our framework outperforms existing automated occupancy annotation methods without human labels. AutoOcc also enables open-ended semantic occupancy auto-labeling, achieving robust performance in both static and dynamically complex scenarios. </p>
<blockquote>
<p>ä»åŸå§‹ä¼ æ„Ÿå™¨æ•°æ®ä¸­è·å–é«˜è´¨é‡çš„ä¸‰ç»´è¯­ä¹‰å ç”¨ä»ç„¶æ˜¯ä¸€é¡¹é‡è¦è€Œå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„æ‰‹åŠ¨æ ‡è®°ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AutoOccï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„å¼€æ”¾å¼è¯­ä¹‰å ç”¨æ³¨é‡Šçš„è‡ªåŠ¨åŒ–ç®¡é“ï¼Œå®ƒé›†æˆäº†ç”±è§†è§‰è¯­è¨€æ¨¡å‹å¼•å¯¼çš„å¯åŒºåˆ†çš„é«˜æ–¯æ‹¼è´´ã€‚æˆ‘ä»¬å°†å¼€æ”¾å¼è¯­ä¹‰ä¸‰ç»´å ç”¨é‡å»ºä»»åŠ¡è¡¨è¿°ä¸ºé€šè¿‡ç»“åˆè§†è§‰è¯­è¨€æ¨¡å‹å’ŒåŸºç¡€è§†è§‰æ¨¡å‹çš„æ³¨æ„åŠ›å›¾æ¥è‡ªåŠ¨ç”Ÿæˆåœºæ™¯å ç”¨ã€‚æˆ‘ä»¬è®¾è®¡è¯­ä¹‰æ„ŸçŸ¥é«˜æ–¯ä½œä¸ºä¸­é—´å‡ ä½•æè¿°ç¬¦ï¼Œå¹¶æå‡ºä¸€ç§ç´¯ç§¯çš„é«˜æ–¯åˆ°ä½“ç´ æ‹¼è´´ç®—æ³•ï¼Œä»¥å®ç°æœ‰æ•ˆå’Œé«˜æ•ˆçš„å ç”¨æ³¨é‡Šã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¸éœ€è¦äººå·¥æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œä¼˜äºç°æœ‰çš„è‡ªåŠ¨å ç”¨æ³¨é‡Šæ–¹æ³•ã€‚AutoOccè¿˜å®ç°äº†å¼€æ”¾å¼è¯­ä¹‰å ç”¨çš„è‡ªåŠ¨æ ‡è®°ï¼Œåœ¨é™æ€å’ŒåŠ¨æ€å¤æ‚çš„åœºæ™¯ä¸­éƒ½èƒ½å®ç°ç¨³å¥çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04981v3">PDF</a> ICCV 2025 Hightlight (main conference)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†AutoOccï¼Œä¸€ä¸ªé¢å‘å¼€æ”¾å¼è¯­ä¹‰å ç”¨çš„è‡ªåŠ¨åŒ–æ³¨é‡Šç®¡é“ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¯å¾®åˆ†çš„Gaussian splattingå’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆåœºæ™¯å ç”¨ã€‚é€šè¿‡è®¾è®¡è¯­ä¹‰æ„ŸçŸ¥é«˜æ–¯ä½œä¸ºä¸­é—´å‡ ä½•æè¿°ç¬¦ï¼Œå¹¶ç»“åˆç´¯ç§¯çš„é«˜æ–¯åˆ°ä½“ç´ splattingç®—æ³•ï¼Œå®ç°äº†é«˜æ•ˆä¸”æœ‰æ•ˆçš„å ç”¨æ³¨é‡Šã€‚è¯¥æ–¹æ³•è¶…è¶Šäº†ç°æœ‰æ— éœ€äººå·¥æ ‡ç­¾çš„å ç”¨æ³¨é‡Šæ–¹æ³•ï¼Œå¹¶å¯åœ¨é™æ€å’ŒåŠ¨æ€å¤æ‚åœºæ™¯ä¸­å®ç°ç¨³å¥æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AutoOccæ˜¯ä¸€ä¸ªé¢å‘å¼€æ”¾å¼è¯­ä¹‰å ç”¨çš„è‡ªåŠ¨åŒ–æ³¨é‡Šç®¡é“ï¼Œé›†æˆäº†å¯å¾®åˆ†çš„Gaussian splattingå’Œè§†è§‰è¯­è¨€æ¨¡å‹ã€‚</li>
<li>é€šè¿‡ç»“åˆæ³¨æ„åŠ›å›¾å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæå‡ºäº†åœºæ™¯å ç”¨çš„è‡ªåŠ¨ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>è®¾è®¡äº†è¯­ä¹‰æ„ŸçŸ¥é«˜æ–¯ä½œä¸ºä¸­é—´å‡ ä½•æè¿°ç¬¦ï¼Œä¸ºå ç”¨æ³¨é‡Šæä¾›äº†æœ‰æ•ˆæ‰‹æ®µã€‚</li>
<li>é‡‡ç”¨äº†ç´¯ç§¯çš„é«˜æ–¯åˆ°ä½“ç´ splattingç®—æ³•ï¼Œæé«˜äº†å ç”¨æ³¨é‡Šçš„æ•ˆç‡ã€‚</li>
<li>è¯¥æ–¹æ³•è¶…è¶Šäº†ç°æœ‰æ— éœ€äººå·¥æ ‡ç­¾çš„å ç”¨æ³¨é‡Šæ–¹æ³•ã€‚</li>
<li>AutoOccèƒ½å¤Ÿåœ¨å¤æ‚åœºæ™¯ï¼ŒåŒ…æ‹¬é™æ€å’ŒåŠ¨æ€åœºæ™¯ä¸­å®ç°ç¨³å¥çš„å ç”¨æ³¨é‡Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04981">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1c9e9ff01175ff981c31d8ba500317a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5afb72485466d4514b8db49e03131ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e0988260dc40d29b8f7236f7fbfd046.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a0a04d960e4b8d68d339c83ed86c8f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3411938e6f62d14de99d1ef63af8f1d.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Momentum-GS-Momentum-Gaussian-Self-Distillation-for-High-Quality-Large-Scene-Reconstruction"><a href="#Momentum-GS-Momentum-Gaussian-Self-Distillation-for-High-Quality-Large-Scene-Reconstruction" class="headerlink" title="Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large   Scene Reconstruction"></a>Momentum-GS: Momentum Gaussian Self-Distillation for High-Quality Large   Scene Reconstruction</h2><p><strong>Authors:Jixuan Fan, Wanhua Li, Yifei Han, Tianru Dai, Yansong Tang</strong></p>
<p>3D Gaussian Splatting has demonstrated notable success in large-scale scene reconstruction, but challenges persist due to high training memory consumption and storage overhead. Hybrid representations that integrate implicit and explicit features offer a way to mitigate these limitations. However, when applied in parallelized block-wise training, two critical issues arise since reconstruction accuracy deteriorates due to reduced data diversity when training each block independently, and parallel training restricts the number of divided blocks to the available number of GPUs. To address these issues, we propose Momentum-GS, a novel approach that leverages momentum-based self-distillation to promote consistency and accuracy across the blocks while decoupling the number of blocks from the physical GPU count. Our method maintains a teacher Gaussian decoder updated with momentum, ensuring a stable reference during training. This teacher provides each block with global guidance in a self-distillation manner, promoting spatial consistency in reconstruction. To further ensure consistency across the blocks, we incorporate block weighting, dynamically adjusting each blockâ€™s weight according to its reconstruction accuracy. Extensive experiments on large-scale scenes show that our method consistently outperforms existing techniques, achieving a 12.8% improvement in LPIPS over CityGaussian with much fewer divided blocks and establishing a new state of the art. Project page: <a target="_blank" rel="noopener" href="https://jixuan-fan.github.io/Momentum-GS_Page/">https://jixuan-fan.github.io/Momentum-GS_Page/</a> </p>
<blockquote>
<p>3Dé«˜æ–¯è´´å›¾åœ¨å¤§è§„æ¨¡åœºæ™¯é‡å»ºä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºå­˜åœ¨è¾ƒé«˜çš„è®­ç»ƒå†…å­˜æ¶ˆè€—å’Œå­˜å‚¨å¼€é”€ã€‚èåˆéšå¼å’Œæ˜¾å¼ç‰¹å¾çš„æ··åˆè¡¨ç¤ºæä¾›äº†ä¸€ç§ç¼“è§£è¿™äº›é™åˆ¶çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œåœ¨å¹¶è¡Œå—å¼è®­ç»ƒä¸­åº”ç”¨æ—¶ï¼Œä¼šå‡ºç°ä¸¤ä¸ªé—®é¢˜ï¼šä¸€æ˜¯é‡å»ºç²¾åº¦å› æ¯ä¸ªå—ç‹¬ç«‹è®­ç»ƒæ—¶æ•°æ®å¤šæ ·æ€§å‡å°‘è€Œä¸‹é™ï¼ŒäºŒæ˜¯å¹¶è¡Œè®­ç»ƒå°†å—çš„æ•°é‡é™åˆ¶ä¸ºå¯ç”¨çš„GPUæ•°é‡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Momentum-GSï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåŠ¨é‡è‡ªè’¸é¦æŠ€æœ¯çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ä¿ƒè¿›å—ä¹‹é—´çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ï¼ŒåŒæ—¶è§£é™¤å—æ•°é‡ä¸ç‰©ç†GPUæ•°é‡çš„è€¦åˆã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»´æŠ¤äº†ä¸€ä¸ªä½¿ç”¨åŠ¨é‡æ›´æ–°çš„æ•™å¸ˆé«˜æ–¯è§£ç å™¨ï¼Œä»¥ç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¨³å®šå‚è€ƒã€‚è¿™ä½æ•™å¸ˆä»¥è‡ªè’¸é¦çš„æ–¹å¼ä¸ºæ¯ä¸ªå—æä¾›å…¨å±€æŒ‡å¯¼ï¼Œä¿ƒè¿›é‡å»ºçš„ç©ºé—´ä¸€è‡´æ€§ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºå—ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†å—æƒé‡ï¼Œæ ¹æ®é‡å»ºç²¾åº¦åŠ¨æ€è°ƒæ•´æ¯ä¸ªå—çš„æƒé‡ã€‚åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œåœ¨CityGaussianä¸Šçš„LPIPSæé«˜äº†12.8%ï¼Œå¹¶ä¸”ä½¿ç”¨çš„å—æ›´å°‘ï¼Œåˆ›ä¸‹äº†æ–°çš„ä¸–ç•Œçºªå½•ã€‚é¡¹ç›®é¡µé¢ï¼š[<a target="_blank" rel="noopener" href="https://jixuan-fan.github.io/Momentum-GS_Page/]">https://jixuan-fan.github.io/Momentum-GS_Page/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04887v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§è§„æ¨¡åœºæ™¯é‡å»ºé¢†åŸŸï¼Œ3Dé«˜æ–¯æ¸²æŸ“æŠ€æœ¯å·²ç»å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»æœ‰å†…å­˜æ¶ˆè€—å¤§å’Œå­˜å‚¨å¼€é”€çš„é—®é¢˜å¾…è§£å†³ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºæ··åˆè¡¨ç¤ºæ³•ä»¥èåˆéšå¼å’Œæ˜¾å¼ç‰¹å¾æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ç„¶è€Œï¼Œåœ¨å¹¶è¡Œå—çº§è®­ç»ƒä¸­ï¼Œè¯¥æ–¹æ³•é¢ä¸´ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šé‡å»ºç²¾åº¦ä¸‹é™å’Œæ•°æ®å¤šæ ·æ€§å‡å°‘çš„é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›ç¼ºé™·ï¼Œç ”ç©¶è€…æå‡ºMomentum-GSæ–¹æ³•ï¼Œé‡‡ç”¨åŸºäºåŠ¨é‡çš„è‡ªæˆ‘è’¸é¦æŠ€æœ¯æ¥æé«˜å„å—ä¹‹é—´çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ï¼Œå¹¶å°†å—çš„æ•°ç›®ä¸å®é™…çš„GPUæ•°é‡åˆ†å¼€è®¡ç®—ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤§å‹åœºæ™¯ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œç›¸è¾ƒäºCityGaussianæé«˜äº†LPIPSå¾—åˆ†12.8%ï¼Œå¹¶å®ç°äº†æ›´å°‘çš„å—åˆ†å‰²ï¼Œæˆä¸ºä¸šç•Œæ–°çš„é¢†è·‘è€…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3Dé«˜æ–¯æ¸²æŸ“æŠ€æœ¯åœ¨å¤§å‹åœºæ™¯é‡å»ºä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´å†…å­˜æ¶ˆè€—å¤§å’Œå­˜å‚¨å¼€é”€çš„é—®é¢˜ã€‚</li>
<li>æ··åˆè¡¨ç¤ºæ³•è¯•å›¾é€šè¿‡ç»“åˆéšå¼å’Œæ˜¾å¼ç‰¹å¾æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚ä½†åœ¨å¹¶è¡Œå—çº§è®­ç»ƒä¸­é‡å»ºç²¾åº¦å’Œå¤šæ ·æ€§çš„é—®é¢˜é™åˆ¶äº†å…¶æ€§èƒ½ã€‚</li>
<li>Momentum-GSæ–¹æ³•é€šè¿‡å¼•å…¥åŸºäºåŠ¨é‡çš„è‡ªæˆ‘è’¸é¦æŠ€æœ¯æ¥æé«˜å—é—´çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>Momentum-GSè§£å†³äº†å¹¶è¡Œè®­ç»ƒä¸­çš„ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šé‡å»ºç²¾åº¦ä¸‹é™å’Œæ•°æ®å¤šæ ·æ€§å‡å°‘çš„é—®é¢˜ã€‚å®ƒé€šè¿‡å¼•å…¥æ•™å¸ˆé«˜æ–¯è§£ç å™¨å¹¶æä¾›å…¨å±€æŒ‡å¯¼æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†å—æƒé‡æœºåˆ¶æ¥ç¡®ä¿é‡å»ºçš„ä¸€è‡´æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04887">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c02707dbc6f7f334f08b77ce6443c21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-46f7ed7647aa71ead67b18f53075d002.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-728e43ce77c3b7e74455c88fef4c4960.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Sequential-Gaussian-Avatars-with-Hierarchical-Motion-Context"><a href="#Sequential-Gaussian-Avatars-with-Hierarchical-Motion-Context" class="headerlink" title="Sequential Gaussian Avatars with Hierarchical Motion Context"></a>Sequential Gaussian Avatars with Hierarchical Motion Context</h2><p><strong>Authors:Wangze Xu, Yifan Zhan, Zhihang Zhong, Xiao Sun</strong></p>
<p>The emergence of neural rendering has significantly advanced the rendering quality of 3D human avatars, with the recently popular 3DGS technique enabling real-time performance. However, SMPL-driven 3DGS human avatars still struggle to capture fine appearance details due to the complex mapping from pose to appearance during fitting. In this paper, we propose SeqAvatar, which excavates the explicit 3DGS representation to better model human avatars based on a hierarchical motion context. Specifically, we utilize a coarse-to-fine motion conditions that incorporate both the overall human skeleton and fine-grained vertex motions for non-rigid deformation. To enhance the robustness of the proposed motion conditions, we adopt a spatio-temporal multi-scale sampling strategy to hierarchically integrate more motion clues to model human avatars. Extensive experiments demonstrate that our method significantly outperforms 3DGS-based approaches and renders human avatars orders of magnitude faster than the latest NeRF-based models that incorporate temporal context, all while delivering performance that is at least comparable or even superior. Project page: <a target="_blank" rel="noopener" href="https://zezeaaa.github.io/projects/SeqAvatar/">https://zezeaaa.github.io/projects/SeqAvatar/</a> </p>
<blockquote>
<p>ç¥ç»æ¸²æŸ“çš„å‡ºç°æå¤§åœ°æé«˜äº†3Däººç±»è™šæ‹Ÿå½¢è±¡çš„æ¸²æŸ“è´¨é‡ï¼Œæœ€è¿‘æµè¡Œçš„3DGSæŠ€æœ¯èƒ½å¤Ÿå®ç°å®æ—¶æ€§èƒ½ã€‚ç„¶è€Œï¼ŒSMPLé©±åŠ¨çš„3DGSäººç±»è™šæ‹Ÿå½¢è±¡ä»ç„¶éš¾ä»¥æ•æ‰ç²¾ç»†çš„å¤–è§‚ç»†èŠ‚ï¼Œå› ä¸ºåœ¨æ‹Ÿåˆè¿‡ç¨‹ä¸­ä»å§¿åŠ¿åˆ°å¤–è§‚çš„æ˜ å°„éå¸¸å¤æ‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SeqAvatarï¼Œå®ƒæŒ–æ˜äº†æ˜ç¡®çš„3DGSè¡¨ç¤ºï¼Œä»¥åŸºäºåˆ†å±‚è¿åŠ¨ä¸Šä¸‹æ–‡æ›´å¥½åœ°å»ºæ¨¡äººç±»è™šæ‹Ÿå½¢è±¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨ä»ç²—åˆ°ç»†çš„è¿åŠ¨æ¡ä»¶ï¼Œç»“åˆæ•´ä½“äººç±»éª¨éª¼å’Œç²¾ç»†é¡¶ç‚¹è¿åŠ¨è¿›è¡Œéåˆšæ€§å˜å½¢ã€‚ä¸ºäº†æé«˜æ‰€æå‡ºè¿åŠ¨æ¡ä»¶çš„ç¨³å¥æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨æ—¶ç©ºå¤šå°ºåº¦é‡‡æ ·ç­–ç•¥ï¼Œåˆ†å±‚èåˆæ›´å¤šè¿åŠ¨çº¿ç´¢æ¥å»ºæ¨¡äººç±»è™šæ‹Ÿå½¢è±¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºäº3DGSçš„æ–¹æ³•ï¼Œå¹¶ä¸”ä¸é‡‡ç”¨æ—¶é—´ä¸Šä¸‹æ–‡çš„æœ€æ–°NeRFæ¨¡å‹ç›¸æ¯”ï¼Œæ¸²æŸ“äººç±»è™šæ‹Ÿå½¢è±¡çš„é€Ÿåº¦è¦å¿«å¾—å¤šï¼ŒåŒæ—¶æ€§èƒ½è‡³å°‘ç›¸å½“ç”šè‡³æ›´ä¼˜ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://zezeaaa.github.io/projects/SeqAvatar/">https://zezeaaa.github.io/projects/SeqAvatar/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16768v2">PDF</a> ICCV2025</p>
<p><strong>Summary</strong></p>
<p>ç¥ç»ç½‘ç»œæ¸²æŸ“æŠ€æœ¯çš„å‡ºç°æ˜¾è‘—æé«˜äº†3Däººç±»è§’è‰²çš„æ¸²æŸ“è´¨é‡ï¼Œç‰¹åˆ«æ˜¯æµè¡Œçš„3DGSæŠ€æœ¯èƒ½å¤Ÿå®ç°å®æ—¶æ€§èƒ½ã€‚ç„¶è€Œï¼ŒåŸºäºSMPLçš„3DGSäººç±»è§’è‰²åœ¨æ‹Ÿåˆè¿‡ç¨‹ä¸­ä»ç„¶éš¾ä»¥æ•æ‰ç²¾ç»†çš„å¤–è§‚ç»†èŠ‚ã€‚æœ¬æ–‡æå‡ºSeqAvatarï¼Œé€šè¿‡æŒ–æ˜æ˜ç¡®çš„3DGSè¡¨ç¤ºå¹¶åŸºäºåˆ†å±‚è¿åŠ¨ä¸Šä¸‹æ–‡å¯¹äººç‰©è§’è‰²è¿›è¡Œæ›´å¥½çš„å»ºæ¨¡æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒé‡‡ç”¨ä»ç²—åˆ°ç»†çš„è¿åŠ¨æ¡ä»¶ï¼Œç»“åˆæ•´ä½“äººç‰©éª¨éª¼å’Œç²¾ç»†é¡¶ç‚¹è¿åŠ¨æ¥å®ç°éåˆšä½“å˜å½¢ã€‚ä¸ºäº†å¢å¼ºè¿åŠ¨æ¡ä»¶çš„ç¨³å¥æ€§ï¼Œç ”ç©¶é‡‡ç”¨äº†æ—¶ç©ºå¤šå°ºåº¦é‡‡æ ·ç­–ç•¥æ¥åˆ†å±‚æ•´åˆæ›´å¤šè¿åŠ¨çº¿ç´¢ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºäº3DGSçš„æ–¹æ³•ï¼Œå¹¶ä¸”æ¸²æŸ“é€Ÿåº¦æ¯”æœ€æ–°ç»“åˆæ—¶é—´ä¸Šä¸‹æ–‡çš„NeRFæ¨¡å‹å¿«å¾—å¤šï¼ŒåŒæ—¶æ€§èƒ½è‡³å°‘ä¸ä¹‹ç›¸å½“æˆ–æ›´ä¼˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œæ¸²æŸ“æŠ€æœ¯æé«˜äº†3Däººç±»è§’è‰²çš„æ¸²æŸ“è´¨é‡ï¼Œç‰¹åˆ«æ˜¯3DGSæŠ€æœ¯å¯å®ç°å®æ—¶æ€§èƒ½ã€‚</li>
<li>åŸºäºSMPLçš„3DGSæ–¹æ³•åœ¨æ•æ‰è§’è‰²ç²¾ç»†å¤–è§‚ç»†èŠ‚æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>SeqAvataré€šè¿‡æŒ–æ˜æ˜ç¡®çš„3DGSè¡¨ç¤ºæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒåŸºäºåˆ†å±‚è¿åŠ¨ä¸Šä¸‹æ–‡å¯¹äººç‰©è§’è‰²è¿›è¡Œå»ºæ¨¡ã€‚</li>
<li>SeqAvataré‡‡ç”¨ä»ç²—åˆ°ç»†çš„è¿åŠ¨æ¡ä»¶ï¼Œç»“åˆæ•´ä½“äººç‰©éª¨éª¼å’Œç²¾ç»†é¡¶ç‚¹è¿åŠ¨æ¥å®ç°éåˆšä½“å˜å½¢ã€‚</li>
<li>ä¸ºäº†å¢å¼ºè¿åŠ¨æ¡ä»¶çš„ç¨³å¥æ€§ï¼Œé‡‡ç”¨äº†æ—¶ç©ºå¤šå°ºåº¦é‡‡æ ·ç­–ç•¥ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºSeqAvataræ˜¾è‘—ä¼˜äºåŸºäº3DGSçš„æ–¹æ³•ï¼Œå¹¶ä¸”æ¸²æŸ“é€Ÿåº¦æ›´å¿«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16768">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cb83d7e77d65ada5ff7333865bdea540.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d261ad94b4dae0a70a6749252521491a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15db0196306fea2d774c45d12abea292.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="RoboGSim-A-Real2Sim2Real-Robotic-Gaussian-Splatting-Simulator"><a href="#RoboGSim-A-Real2Sim2Real-Robotic-Gaussian-Splatting-Simulator" class="headerlink" title="RoboGSim: A Real2Sim2Real Robotic Gaussian Splatting Simulator"></a>RoboGSim: A Real2Sim2Real Robotic Gaussian Splatting Simulator</h2><p><strong>Authors:Xinhai Li, Jialin Li, Ziheng Zhang, Rui Zhang, Fan Jia, Tiancai Wang, Haoqiang Fan, Kuo-Kun Tseng, Ruiping Wang</strong></p>
<p>Efficient acquisition of real-world embodied data has been increasingly critical. However, large-scale demonstrations captured by remote operation tend to take extremely high costs and fail to scale up the data size in an efficient manner. Sampling the episodes under a simulated environment is a promising way for large-scale collection while existing simulators fail to high-fidelity modeling on texture and physics. To address these limitations, we introduce the RoboGSim, a real2sim2real robotic simulator, powered by 3D Gaussian Splatting and the physics engine. RoboGSim mainly includes four parts: Gaussian Reconstructor, Digital Twins Builder, Scene Composer, and Interactive Engine. It can synthesize the simulated data with novel views, objects, trajectories, and scenes. RoboGSim also provides an online, reproducible, and safe evaluation for different manipulation policies. The real2sim and sim2real transfer experiments show a high consistency in the texture and physics. We compared the test results of RoboGSim data and real robot data on both RoboGSim and real robot platforms. The experimental results show that the RoboGSim data model can achieve zero-shot performance on the real robot, with results comparable to real robot data. Additionally, in experiments with novel perspectives and novel scenes, the RoboGSim data model performed even better on the real robot than the real robot data model. This not only helps reduce the sim2real gap but also addresses the limitations of real robot data collection, such as its single-source and high cost. We hope RoboGSim serves as a closed-loop simulator for fair comparison on policy learning. More information can be found on our project page <a target="_blank" rel="noopener" href="https://robogsim.github.io/">https://robogsim.github.io/</a>. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œä½“æ„Ÿæ•°æ®çš„é«˜æ•ˆé‡‡é›†å˜å¾—è¶Šæ¥è¶Šå…³é”®ã€‚ç„¶è€Œï¼Œé€šè¿‡è¿œç¨‹æ“ä½œæ•è·çš„å¤§è§„æ¨¡æ¼”ç¤ºå¾€å¾€æˆæœ¬æé«˜ï¼Œä¸”æœªèƒ½ä»¥é«˜æ•ˆçš„æ–¹å¼æ‰©å¤§æ•°æ®é‡ã€‚åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸‹å¯¹æƒ…èŠ‚è¿›è¡Œé‡‡æ ·æ˜¯å¤§è§„æ¨¡é‡‡é›†çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹å¼ï¼Œä½†ç°æœ‰æ¨¡æ‹Ÿå™¨åœ¨çº¹ç†å’Œç‰©ç†æ–¹é¢çš„é«˜ä¿çœŸå»ºæ¨¡å­˜åœ¨ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†RoboGSimï¼Œè¿™æ˜¯ä¸€ä¸ªç”±3Dé«˜æ–¯æ‹¼è´´å’Œç‰©ç†å¼•æ“é©±åŠ¨çš„real2sim2realæœºå™¨äººæ¨¡æ‹Ÿå™¨ã€‚RoboGSimä¸»è¦åŒ…æ‹¬å››ä¸ªéƒ¨åˆ†ï¼šé«˜æ–¯é‡å»ºå™¨ã€æ•°å­—å­ªç”Ÿæ„å»ºå™¨ã€åœºæ™¯ä½œæ›²å®¶å’Œäº¤äº’å¼•æ“ã€‚å®ƒå¯ä»¥åˆæˆå…·æœ‰æ–°é¢–è§†è§’ã€ç‰©ä½“ã€è½¨è¿¹å’Œåœºæ™¯æ¨¡æ‹Ÿæ•°æ®ã€‚RoboGSimè¿˜ä¸ºä¸åŒçš„æ“ä½œç­–ç•¥æä¾›äº†åœ¨çº¿ã€å¯é‡å¤å’Œå®‰å…¨çš„è¯„ä¼°ã€‚çº¹ç†å’Œç‰©ç†çš„å®æ‹Ÿä¸æ‹Ÿå®è½¬ç§»å®éªŒè¡¨ç°å‡ºé«˜åº¦ä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨RoboGSimå¹³å°å’ŒçœŸå®æœºå™¨äººå¹³å°ä¸Šå¯¹RoboGSimæ•°æ®å’ŒçœŸå®æœºå™¨äººæ•°æ®è¿›è¡Œäº†æµ‹è¯•ç»“æœçš„æ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRoboGSimæ•°æ®æ¨¡å‹èƒ½å¤Ÿåœ¨çœŸå®æœºå™¨äººä¸Šå®ç°é›¶å°„æ•ˆæœï¼Œå…¶æ€§èƒ½ä¸çœŸå®æœºå™¨äººæ•°æ®ç›¸å½“ã€‚æ­¤å¤–ï¼Œåœ¨å…·æœ‰æ–°é¢–è§†è§’å’Œåœºæ™¯çš„å®éªŒä¸­ï¼ŒRoboGSimæ•°æ®æ¨¡å‹åœ¨çœŸå®æœºå™¨äººä¸Šçš„è¡¨ç°ç”šè‡³æ¯”çœŸå®æœºå™¨äººæ•°æ®æ¨¡å‹æ›´å¥½ã€‚è¿™ä¸ä»…æœ‰åŠ©äºç¼©å°æ¨¡æ‹Ÿä¸ç°å®çš„å·®è·ï¼Œè¿˜è§£å†³äº†çœŸå®æœºå™¨äººæ•°æ®é‡‡é›†çš„å±€é™æ€§ï¼Œå¦‚å•ä¸€æ¥æºå’Œé«˜æˆæœ¬çš„é—®é¢˜ã€‚æˆ‘ä»¬å¸Œæœ›RoboGSimèƒ½ä½œä¸ºé—­ç¯æ¨¡æ‹Ÿå™¨ï¼Œä¸ºç­–ç•¥å­¦ä¹ çš„å…¬å¹³æ¯”è¾ƒæä¾›æœåŠ¡ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢<a target="_blank" rel="noopener" href="https://robogsim.github.io/%E3%80%82">https://robogsim.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11839v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†RoboGSimæ¨¡æ‹Ÿå™¨ï¼Œé€šè¿‡åˆ©ç”¨3Dé«˜æ–¯Splattingå’Œç‰©ç†å¼•æ“æŠ€æœ¯è§£å†³äº†è¿œç¨‹æ“ä½œå¤§è§„æ¨¡æ¼”ç¤ºæ‰€é¢ä¸´çš„é—®é¢˜ã€‚è¯¥æ¨¡æ‹Ÿå™¨ä¸»è¦ç”±é«˜æ–¯é‡å»ºå™¨ã€æ•°å­—åŒèƒèƒå»ºé€ å™¨ã€åœºæ™¯ç»„åˆå™¨å’Œäº¤äº’å¼•æ“å››éƒ¨åˆ†ç»„æˆï¼Œå¯ä»¥åˆæˆæ¨¡æ‹Ÿæ•°æ®å¹¶å±•ç¤ºæ–°é¢–çš„è§†å›¾ã€ç‰©ä½“ã€è½¨è¿¹å’Œåœºæ™¯ã€‚æ­¤å¤–ï¼ŒRoboGSimæä¾›äº†åœ¨çº¿ã€å¯é‡å¤å’Œå®‰å…¨çš„è¯„ä¼°ä¸åŒæ“ä½œç­–ç•¥çš„å¹³å°ã€‚é€šè¿‡çœŸå®åˆ°æ¨¡æ‹Ÿå†åˆ°çœŸå®çš„è½¬ç§»å®éªŒï¼Œå±•ç°äº†å…¶åœ¨çº¹ç†å’Œç‰©ç†æ–¹é¢çš„é«˜åº¦ä¸€è‡´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRoboGSimæ•°æ®æ¨¡å‹èƒ½åœ¨çœŸå®æœºå™¨äººä¸Šå®ç°é›¶å°„å‡»æ€§èƒ½ï¼Œä¸çœŸå®æœºå™¨äººæ•°æ®ç»“æœç›¸å½“ã€‚å¹¶ä¸”åœ¨æ–°å‹è§†è§’å’Œåœºæ™¯çš„å®éªŒä¸­ï¼ŒRoboGSimæ•°æ®æ¨¡å‹ç”šè‡³è¡¨ç°æ›´ä½³ã€‚è¯¥æ¨¡æ‹Ÿå™¨ä¸ä»…æœ‰åŠ©äºç¼©å°æ¨¡æ‹Ÿä¸çœŸå®ä¹‹é—´çš„å·®è·ï¼Œè¿˜è§£å†³äº†çœŸå®æœºå™¨äººæ•°æ®é‡‡é›†çš„å±€é™æ€§ï¼Œå¦‚å•ä¸€æ¥æºå’Œé«˜æˆæœ¬é—®é¢˜ã€‚æœŸæœ›RoboGSimèƒ½ä½œä¸ºé—­ç¯æ¨¡æ‹Ÿå™¨ï¼Œä¸ºç­–ç•¥å­¦ä¹ æä¾›å…¬å¹³æ¯”è¾ƒçš„å¹³å°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>RoboGSimæ˜¯ä¸€ä¸ªåŸºäº3Dé«˜æ–¯Splattingå’Œç‰©ç†å¼•æ“æŠ€æœ¯çš„æ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡æ”¶é›†ç°å®æœºå™¨äººæ•°æ®çš„é«˜æˆæœ¬å’Œæ— æ³•æœ‰æ•ˆæ‰©å±•çš„é—®é¢˜ã€‚</li>
<li>RoboGSimåŒ…æ‹¬å››ä¸ªä¸»è¦éƒ¨åˆ†ï¼Œèƒ½å¤Ÿåˆæˆæ¨¡æ‹Ÿæ•°æ®å¹¶å±•ç¤ºä¸åŒçš„è§†å›¾ã€ç‰©ä½“ã€è½¨è¿¹å’Œåœºæ™¯ã€‚</li>
<li>è¯¥æ¨¡æ‹Ÿå™¨æä¾›äº†åœ¨çº¿ã€å¯é‡å¤å’Œå®‰å…¨çš„è¯„ä¼°å¹³å°ï¼Œç”¨äºä¸åŒæ“ä½œç­–ç•¥çš„è¯„ä»·ã€‚</li>
<li>RoboGSimåœ¨çœŸå®åˆ°æ¨¡æ‹Ÿå†åˆ°çœŸå®çš„è½¬ç§»å®éªŒä¸­å±•ç°äº†å…¶åœ¨çº¹ç†å’Œç‰©ç†æ–¹é¢çš„é«˜åº¦ä¸€è‡´æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒRoboGSimæ•°æ®æ¨¡å‹åœ¨çœŸå®æœºå™¨äººä¸Šçš„æ€§èƒ½ä¸çœŸå®æœºå™¨äººæ•°æ®ç›¸å½“ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹è¡¨ç°æ›´ä½³ã€‚</li>
<li>è¯¥æ¨¡æ‹Ÿå™¨æœ‰åŠ©äºç¼©å°æ¨¡æ‹Ÿä¸çœŸå®æœºå™¨äººä¹‹é—´çš„å·®è·ï¼Œå¹¶è§£å†³çœŸå®æœºå™¨äººæ•°æ®é‡‡é›†çš„å±€é™æ€§ã€‚</li>
<li>RoboGSimçš„é¢„æœŸä½œç”¨æ˜¯ä¸ºç­–ç•¥å­¦ä¹ æä¾›ä¸€ä¸ªå…¬å¹³æ¯”è¾ƒçš„å¹³å°ï¼Œä½œä¸ºä¸€ä¸ªé—­ç¯æ¨¡æ‹Ÿå™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11839">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8d37866108e7d5961faa547561960239.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c91135dc54fc8bcae5e560c8a86ae642.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a81a1b8aa96e99125104a5b6b2aa8d1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5681edf3d4b0cfbd92f580dad17a538d.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f41e461634289951c99dea6a22e2b222.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  ASDR Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant   Neural Rendering
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-823ccf16ce1a587fe983b141c93f6b52.jpg" class="responsive-img" alt="å…ƒå®‡å®™/è™šæ‹Ÿäºº">
                        
                        <span class="card-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  Is It Really You? Exploring Biometric Verification Scenarios in   Photorealistic Talking-Head Avatar Videos
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    å…ƒå®‡å®™/è™šæ‹Ÿäºº
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
