<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  InfoSyncNet Information Synchronization Temporal Convolutional Network   for Visual Speech Recognition">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-f838a1f256efd42a08f80f84afa34416.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    61 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-06-æ›´æ–°"><a href="#2025-08-06-æ›´æ–°" class="headerlink" title="2025-08-06 æ›´æ–°"></a>2025-08-06 æ›´æ–°</h1><h2 id="InfoSyncNet-Information-Synchronization-Temporal-Convolutional-Network-for-Visual-Speech-Recognition"><a href="#InfoSyncNet-Information-Synchronization-Temporal-Convolutional-Network-for-Visual-Speech-Recognition" class="headerlink" title="InfoSyncNet: Information Synchronization Temporal Convolutional Network   for Visual Speech Recognition"></a>InfoSyncNet: Information Synchronization Temporal Convolutional Network   for Visual Speech Recognition</h2><p><strong>Authors:Junxiao Xue, Xiaozhen Liu, Xuecheng Wu, Fei Yu, Jun Wang</strong></p>
<p>Estimating spoken content from silent videos is crucial for applications in Assistive Technology (AT) and Augmented Reality (AR). However, accurately mapping lip movement sequences in videos to words poses significant challenges due to variability across sequences and the uneven distribution of information within each sequence. To tackle this, we introduce InfoSyncNet, a non-uniform sequence modeling network enhanced by tailored data augmentation techniques. Central to InfoSyncNet is a non-uniform quantization module positioned between the encoder and decoder, enabling dynamic adjustment to the networkâ€™s focus and effectively handling the natural inconsistencies in visual speech data. Additionally, multiple training strategies are incorporated to enhance the modelâ€™s capability to handle variations in lighting and the speakerâ€™s orientation. Comprehensive experiments on the LRW and LRW1000 datasets confirm the superiority of InfoSyncNet, achieving new state-of-the-art accuracies of 92.0% and 60.7% Top-1 ACC. The code is available for download (see comments). </p>
<blockquote>
<p>ä»æ— å£°è§†é¢‘ä¸­ä¼°è®¡è¯­éŸ³å†…å®¹å¯¹äºè¾…åŠ©æŠ€æœ¯ï¼ˆATï¼‰å’Œå¢å¼ºç°å®ï¼ˆARï¼‰çš„åº”ç”¨è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºåºåˆ—ä¹‹é—´çš„å¯å˜æ€§å’Œåºåˆ—å†…ä¿¡æ¯çš„ä¸å‡åŒ€åˆ†å¸ƒï¼Œå‡†ç¡®åœ°å°†è§†é¢‘ä¸­çš„å”‡åŠ¨åºåˆ—æ˜ å°„åˆ°å•è¯ä¸Šå¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†InfoSyncNetï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å®šåˆ¶çš„æ•°æ®å¢å¼ºæŠ€æœ¯å¢å¼ºçš„éå‡åŒ€åºåˆ—å»ºæ¨¡ç½‘ç»œã€‚InfoSyncNetçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªä½äºç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´çš„éå‡åŒ€é‡åŒ–æ¨¡å—ï¼Œå®ƒèƒ½å¤Ÿå®ç°ç½‘ç»œå…³æ³¨çš„åŠ¨æ€è°ƒæ•´ï¼Œå¹¶æœ‰æ•ˆåœ°å¤„ç†è§†è§‰è¯­éŸ³æ•°æ®ä¸­çš„è‡ªç„¶ä¸ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œè¿˜èå…¥äº†å¤šç§è®­ç»ƒç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹å¤„ç†å…‰ç…§å’Œè¯´è¯äººæ–¹å‘å˜åŒ–çš„èƒ½åŠ›ã€‚åœ¨LRWå’ŒLRW1000æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¯å®äº†InfoSyncNetçš„ä¼˜è¶Šæ€§ï¼Œè¾¾åˆ°äº†æœ€æ–°çš„æœ€é«˜å‡†ç¡®ç‡ï¼ŒTop-1 ACCåˆ†åˆ«ä¸º92.0%å’Œ60.7%ã€‚ä»£ç å¯ä¸‹è½½ï¼ˆè¯¦è§æ³¨é‡Šï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02460v1">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/liuxiaozhen123/InfoSyncNet">https://github.com/liuxiaozhen123/InfoSyncNet</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ— å£°è§†é¢‘è¿›è¡Œå£è¯­å†…å®¹ä¼°ç®—å¯¹äºè¾…åŠ©æŠ€æœ¯å’Œå¢å¼ºç°å®åº”ç”¨è‡³å…³é‡è¦ã€‚é’ˆå¯¹è§†é¢‘ä¸­çš„å”‡åŠ¨åºåˆ—ä¸è¯è¯­çš„ç²¾å‡†æ˜ å°„æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒInfoSyncNeté‡‡ç”¨äº†éå‡åŒ€åºåˆ—å»ºæ¨¡ç½‘ç»œç»“åˆå®šåˆ¶åŒ–æ•°æ®å¢å¼ºæŠ€æœ¯æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚å…¶ä¸­éå‡åŒ€é‡åŒ–æ¨¡å—ä¸ºæ ¸å¿ƒç»„ä»¶ï¼Œå®ç°äº†ç½‘ç»œå…³æ³¨çš„åŠ¨æ€è°ƒæ•´å¹¶æœ‰æ•ˆåº”å¯¹è§†è§‰è¯­éŸ³æ•°æ®çš„è‡ªç„¶ä¸ä¸€è‡´æ€§ã€‚å®éªŒè¯æ˜InfoSyncNetåœ¨LRWå’ŒLRW1000æ•°æ®é›†ä¸Šè¡¨ç°å“è¶Šï¼Œè¾¾åˆ°æ–°çš„ä¸šç•Œé¡¶å°–å‡†ç¡®ç‡ï¼Œåˆ†åˆ«ä¸º92.0%å’Œ60.7%ã€‚ä»£ç å·²å…¬å¼€ä¸‹è½½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>InfoSyncNetç”¨äºä»æ— å£°è§†é¢‘ä¸­ä¼°ç®—å£è¯­å†…å®¹ï¼Œåœ¨è¾…åŠ©æŠ€æœ¯å’Œå¢å¼ºç°å®åº”ç”¨ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>å”‡åŠ¨åºåˆ—ä¸è¯è¯­çš„æ˜ å°„é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºåºåˆ—å˜åŒ–å¤šç«¯åŠä¿¡æ¯åˆ†å¸ƒä¸å‡ä¸€ã€‚</li>
<li>InfoSyncNeté€šè¿‡éå‡åŒ€åºåˆ—å»ºæ¨¡åº”å¯¹æŒ‘æˆ˜ï¼Œç‰¹åˆ«åŒ…æ‹¬éå‡åŒ€é‡åŒ–æ¨¡å—ï¼Œå®ç°ç½‘ç»œå…³æ³¨åŠ¨æ€è°ƒæ•´ã€‚</li>
<li>å®šåˆ¶åŒ–çš„æ•°æ®å¢å¼ºæŠ€æœ¯å’Œå¤šç§è®­ç»ƒç­–ç•¥æå‡äº†æ¨¡å‹å¤„ç†ä¸åŒå…‰ç…§æ¡ä»¶å’Œè¯´è¯äººæœå‘å˜åŒ–çš„èƒ½åŠ›ã€‚</li>
<li>åœ¨LRWå’ŒLRW1000æ•°æ®é›†ä¸Šçš„å®éªŒè¯å®InfoSyncNetçš„å“è¶Šæ€§èƒ½ï¼Œè¾¾åˆ°æ–°çš„ä¸šç•Œé¡¶å°–å‡†ç¡®ç‡ã€‚</li>
<li>InfoSyncNetçš„ä»£ç å·²å…¬å¼€ä¾›ä¸‹è½½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02460">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8cb39e29f0d4d806c2ecacf7eb3ce833.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89ee0c11a3dfffdfb0b8dd79fbf187df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09e694ca4ce7baa1c007826a74308e29.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b837941f14a8eb8bcc347a95737e045.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06d02d28771b5aa9553f45d4f266fcbe.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Charting-15-years-of-progress-in-deep-learning-for-speech-emotion-recognition-A-replication-study"><a href="#Charting-15-years-of-progress-in-deep-learning-for-speech-emotion-recognition-A-replication-study" class="headerlink" title="Charting 15 years of progress in deep learning for speech emotion   recognition: A replication study"></a>Charting 15 years of progress in deep learning for speech emotion   recognition: A replication study</h2><p><strong>Authors:Andreas Triantafyllopoulos, Anton Batliner, BjÃ¶rn W. Schuller</strong></p>
<p>Speech emotion recognition (SER) has long benefited from the adoption of deep learning methodologies. Deeper models â€“ with more layers and more trainable parameters â€“ are generally perceived as being &#96;betterâ€™ by the SER community. This raises the question â€“ \emph{how much better} are modern-era deep neural networks compared to their earlier iterations? Beyond that, the more important question of how to move forward remains as poignant as ever. SER is far from a solved problem; therefore, identifying the most prominent avenues of future research is of paramount importance. In the present contribution, we attempt a quantification of progress in the 15 years of research beginning with the introduction of the landmark 2009 INTERSPEECH Emotion Challenge. We conduct a large scale investigation of model architectures, spanning both audio-based models that rely on speech inputs and text-baed models that rely solely on transcriptions. Our results point towards diminishing returns and a plateau after the recent introduction of transformer architectures. Moreover, we demonstrate how perceptions of progress are conditioned on the particular selection of models that are compared. Our findings have important repercussions about the state-of-the-art in SER research and the paths forward </p>
<blockquote>
<p>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰é•¿ä¹…ä»¥æ¥å—ç›Šäºæ·±åº¦å­¦ä¹ æ–¹æ³•çš„é‡‡ç”¨ã€‚åœ¨SERç•Œï¼Œæ›´æ·±çš„æ¨¡å‹ï¼ˆå…·æœ‰æ›´å¤šå±‚å’Œæ›´å¤šå¯è®­ç»ƒå‚æ•°ï¼‰é€šå¸¸è¢«è®¤ä¸ºæ˜¯â€œæ›´å¥½â€çš„ã€‚è¿™å°±æå‡ºäº†ä¸€ä¸ªé—®é¢˜â€”â€”ä¸ç°ä»£æ—©æœŸçš„æ·±åº¦ç¥ç»ç½‘ç»œç›¸æ¯”ï¼Œç°ä»£æ·±åº¦ç¥ç»ç½‘ç»œâ€œæ›´å¥½â€åˆ°ä»€ä¹ˆåœ°æ­¥ï¼Ÿé™¤æ­¤ä¹‹å¤–ï¼Œå¦‚ä½•ç»§ç»­å‰è¿›è¿™ä¸€æ›´é‡è¦çš„é—®é¢˜ä»ç„¶è‡³å…³é‡è¦ã€‚SERè¿œéä¸€ä¸ªå·²è§£å†³çš„é—®é¢˜ï¼Œå› æ­¤ï¼Œç¡®å®šæœªæ¥ç ”ç©¶çš„æœ€é‡è¦é€”å¾„è‡³å…³é‡è¦ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å°è¯•é‡åŒ–è‡ª2009å¹´å…·æœ‰é‡Œç¨‹ç¢‘æ„ä¹‰çš„INTERSPEECHæƒ…æ„ŸæŒ‘æˆ˜èµ›å¼•å…¥ä»¥æ¥15å¹´ç ”ç©¶çš„è¿›å±•ã€‚æˆ‘ä»¬å¯¹æ¨¡å‹æ¶æ„è¿›è¡Œäº†å¤§è§„æ¨¡è°ƒæŸ¥ï¼Œæ—¢åŒ…æ‹¬åŸºäºéŸ³é¢‘çš„ä¾èµ–äºè¯­éŸ³è¾“å…¥çš„æ¨¡å‹ï¼Œä¹ŸåŒ…æ‹¬ä»…ä¾èµ–äºè½¬å½•çš„æ–‡æœ¬æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨æœ€è¿‘å¼•å…¥transformeræ¶æ„ä¹‹åï¼Œæ”¶ç›Šæ­£åœ¨å‡å°‘å¹¶ä¸”å‡ºç°é«˜åŸæ•ˆåº”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†äººä»¬å¯¹è¿›æ­¥çš„çœ‹æ³•æ˜¯å—æ‰€é€‰æ‹©çš„æ¯”è¾ƒæ¨¡å‹çš„å½±å“ã€‚æˆ‘ä»¬çš„å‘ç°å¯¹SERç ”ç©¶çš„æœ€æ–°è¿›å±•ä»¥åŠæœªæ¥çš„æ–¹å‘å…·æœ‰é‡è¦çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02448v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/CHI-TUM/ser-progress-replication">https://github.com/CHI-TUM/ser-progress-replication</a> Submitted   for review</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡æœ¬ä¸»è¦æ¢è®¨äº†è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰é¢†åŸŸçš„ç ”ç©¶è¿›å±•ï¼Œé€šè¿‡å¤§è§„æ¨¡çš„æ¨¡å‹æ¶æ„è°ƒæŸ¥ï¼Œå‘ç°éšç€æ·±åº¦å­¦ä¹ æ–¹æ³•çš„ä¸æ–­é‡‡ç”¨ï¼Œè¿›æ­¥å·²ç»è¶‹äºå¹³ç¨³å¹¶è¾¾åˆ°é¡¶å³°ã€‚ç ”ç©¶å‘ç°åœ¨å¼•å…¥Transformeræ¶æ„åï¼Œæ”¶ç›Šé€æ¸é€’å‡ã€‚æ­¤å¤–ï¼Œå¯¹äºè¿›æ­¥çš„è®¤è¯†å–å†³äºæ‰€é€‰æ‹©çš„æ¨¡å‹æ¯”è¾ƒã€‚è¿™ç¯‡æ–‡æœ¬å¯¹äºäº†è§£å½“å‰SERç ”ç©¶çš„æœ€æ–°è¿›å±•å’Œæœªæ¥çš„æ–¹å‘å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰é¢†åŸŸå—ç›Šäºæ·±åº¦å­¦ä¹ æ–¹æ³•çš„å¹¿æ³›åº”ç”¨ã€‚</li>
<li>æ›´æ·±å±‚æ¬¡çš„æ¨¡å‹é€šå¸¸è¢«è®¤ä¸ºæ˜¯æ›´å¥½çš„é€‰æ‹©ï¼Œä½†åœ¨å®é™…ä¸­å¯èƒ½å­˜åœ¨æ”¶ç›Šé€’å‡ç°è±¡ã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡çš„æ¨¡å‹æ¶æ„è°ƒæŸ¥ï¼Œç ”ç©¶å‘ç°è¿‘å¹´æ¥è¿›æ­¥é€æ¸æ”¾ç¼“å¹¶å¯èƒ½å·²ç»è¾¾åˆ°ä¸€ä¸ªå³°å€¼æˆ–å¹³ç¨³çŠ¶æ€ã€‚</li>
<li>åœ¨å¼•å…¥Transformeræ¶æ„åï¼Œæ¨¡å‹æ€§èƒ½çš„æå‡é€æ¸è¶‹äºå¹³ç¨³ã€‚</li>
<li>å¯¹è¿›æ­¥çš„è®¤è¯†å—åˆ°æ‰€é€‰æ¨¡å‹æ¯”è¾ƒçš„å½±å“ã€‚</li>
<li>å½“å‰SERç ”ç©¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦è¿›ä¸€æ­¥æ¢ç´¢å’Œç ”ç©¶æ–°çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ec2b0a6eb5805920ef3628e4ff5fc8ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad61e7a16f8e76f2b9b0c2a0f7d8c78a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d5db95dc6c661c5234f7c639ed5e638.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SpeechR-A-Benchmark-for-Speech-Reasoning-in-Large-Audio-Language-Models"><a href="#SpeechR-A-Benchmark-for-Speech-Reasoning-in-Large-Audio-Language-Models" class="headerlink" title="SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models"></a>SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models</h2><p><strong>Authors:Wanqi Yang, Yanda Li, Yunchao Wei, Meng Fang, Ling Chen</strong></p>
<p>Large audio-language models (LALMs) have achieved near-human performance in sentence-level transcription and emotion recognition. However, existing evaluations focus mainly on surface-level perception, leaving the capacity of models for contextual and inference-driven reasoning in speech-based scenarios insufficiently examined. To address this gap, we introduce SpeechR, a unified benchmark for evaluating reasoning over speech in large audio-language models. SpeechR evaluates models along three key dimensions: factual retrieval, procedural inference, and normative judgment. It includes three distinct evaluation formats. The multiple-choice version measures answer selection accuracy. The generative version assesses the coherence and logical consistency of reasoning chains. The acoustic-feature version investigates whether variations in stress and emotion affect reasoning performance. Evaluations on eleven state-of-the-art LALMs reveal that high transcription accuracy does not translate into strong reasoning capabilities. SpeechR establishes a structured benchmark for evaluating reasoning in spoken language, enabling more targeted analysis of model capabilities across diverse dialogue-based tasks. </p>
<blockquote>
<p>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMsï¼‰åœ¨å¥å­çº§è½¬å½•å’Œæƒ…ç»ªè¯†åˆ«æ–¹é¢å·²ç»è¾¾åˆ°äº†æ¥è¿‘äººç±»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨è¡¨é¢å±‚æ¬¡çš„æ„ŸçŸ¥ä¸Šï¼Œå¯¹æ¨¡å‹åœ¨åŸºäºè¯­éŸ³çš„åœºæ™¯ä¸­è¿›è¡Œä¸Šä¸‹æ–‡å’Œæ¨ç†é©±åŠ¨æ¨ç†çš„èƒ½åŠ›ç¼ºä¹è¶³å¤Ÿçš„è€ƒå¯Ÿã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpeechRï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ä¸­è¯­éŸ³æ¨ç†çš„ç»Ÿä¸€åŸºå‡†ã€‚SpeechRæ²¿ç€ä¸‰ä¸ªå…³é”®ç»´åº¦è¯„ä¼°æ¨¡å‹ï¼šäº‹å®æ£€ç´¢ã€ç¨‹åºæ¨ç†å’Œè§„èŒƒæ€§åˆ¤æ–­ã€‚å®ƒåŒ…æ‹¬ä¸‰ç§ä¸åŒçš„è¯„ä¼°æ ¼å¼ã€‚å¤šé¡¹é€‰æ‹©ç‰ˆæœ¬è¡¡é‡ç­”æ¡ˆé€‰æ‹©å‡†ç¡®æ€§ã€‚ç”Ÿæˆç‰ˆæœ¬è¯„ä¼°æ¨ç†é“¾çš„è¿è´¯æ€§å’Œé€»è¾‘ä¸€è‡´æ€§ã€‚å£°å­¦ç‰¹å¾ç‰ˆæœ¬ç ”ç©¶åº”åŠ›å’Œæƒ…ç»ªçš„å˜åŒ–æ˜¯å¦å½±å“æ¨ç†æ€§èƒ½ã€‚å¯¹åä¸€ç§æœ€å…ˆè¿›çš„LALMè¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼Œé«˜è½¬å½•å‡†ç¡®æ€§å¹¶ä¸ç­‰åŒäºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚SpeechRå»ºç«‹äº†ä¸€ä¸ªç»“æ„åŒ–çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å£è¯­ä¸­çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨å„ç§åŸºäºå¯¹è¯çš„ä»»åŠ¡ä¸Šæ›´é’ˆå¯¹æ€§åœ°åˆ†ææ¨¡å‹çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02018v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰åœ¨å¥å­çº§è½¬å½•å’Œæƒ…æ„Ÿè¯†åˆ«æ–¹é¢å·²è¾¾åˆ°æ¥è¿‘äººç±»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨è¡¨é¢æ„ŸçŸ¥ä¸Šï¼Œç¼ºä¹å¯¹è¯­éŸ³åœºæ™¯ä¸­æ¨¡å‹ä¸Šä¸‹æ–‡å’Œæ¨ç†é©±åŠ¨èƒ½åŠ›çš„å……åˆ†è€ƒå¯Ÿã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpeechRï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ä¸­è¯­éŸ³æ¨ç†çš„ç»Ÿä¸€åŸºå‡†ã€‚SpeechRé€šè¿‡ä¸‰ä¸ªå…³é”®ç»´åº¦ï¼šäº‹å®æ£€ç´¢ã€ç¨‹åºæ¨ç†å’Œè§„èŒƒåˆ¤æ–­æ¥è¯„ä¼°æ¨¡å‹ã€‚åŒ…æ‹¬ä¸‰ç§ä¸åŒçš„è¯„ä¼°æ ¼å¼ã€‚å¤šé¡¹é€‰æ‹©ç‰ˆè¡¡é‡ç­”æ¡ˆé€‰æ‹©å‡†ç¡®æ€§ã€‚ç”Ÿæˆç‰ˆè¯„ä¼°æ¨ç†é“¾çš„è¿è´¯æ€§å’Œé€»è¾‘ä¸€è‡´æ€§ã€‚å£°å­¦ç‰¹å¾ç‰ˆç ”ç©¶åº”åŠ›å’Œæƒ…æ„Ÿå˜åŒ–æ˜¯å¦å½±å“æ¨ç†æ€§èƒ½ã€‚å¯¹åä¸€ç§æœ€æ–°LALMçš„è¯„ä¼°æ˜¾ç¤ºï¼Œé«˜è½¬å½•å‡†ç¡®æ€§å¹¶ä¸ç­‰åŒäºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚SpeechRä¸ºè¯„ä¼°å£è¯­ä¸­çš„æ¨ç†èƒ½åŠ›å»ºç«‹äº†ç»“æ„åŒ–åŸºå‡†ï¼Œä½¿ä¸åŒå¯¹è¯ä»»åŠ¡çš„èƒ½åŠ›åˆ†ææ›´å…·é’ˆå¯¹æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰åœ¨è¯­éŸ³é¢†åŸŸå·²æ¥è¿‘äººç±»æ€§èƒ½æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨å¥å­çº§è½¬å½•å’Œæƒ…æ„Ÿè¯†åˆ«æ–¹é¢ã€‚</li>
<li>ç°æœ‰è¯„ä¼°ä¸»è¦å…³æ³¨è¡¨é¢æ„ŸçŸ¥ï¼Œç¼ºä¹æ¨¡å‹ä¸Šä¸‹æ–‡å’Œæ¨ç†é©±åŠ¨èƒ½åŠ›çš„è€ƒå¯Ÿã€‚</li>
<li>å¼•å…¥SpeechRç»Ÿä¸€åŸºå‡†ä»¥è¯„ä¼°æ¨¡å‹ä¸­è¯­éŸ³æ¨ç†çš„èƒ½åŠ›ã€‚</li>
<li>SpeechRæ¶µç›–ä¸‰ä¸ªå…³é”®ç»´åº¦ï¼šäº‹å®æ£€ç´¢ã€ç¨‹åºæ¨ç†å’Œè§„èŒƒåˆ¤æ–­ã€‚</li>
<li>æä¾›ä¸‰ç§è¯„ä¼°æ ¼å¼ï¼ŒåŒ…æ‹¬å¤šé¡¹é€‰æ‹©ã€ç”Ÿæˆå’ŒåŸºäºå£°å­¦ç‰¹å¾çš„è¯„ä¼°ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºé«˜è½¬å½•å‡†ç¡®æ€§å¹¶ä¸ç­‰åŒäºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02018">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-69abad54fe13ab2f6b0b805c1919d73e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd4b50e9cde15f6d28deef9dc6afd287.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69832ef1f8333bdd3829d8881ed0b807.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d32814cd4d6266a9617fcf178e6b54f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-03126bd423403e2a5933e7a29fe4b0a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df3471a682740dc6a2433704eb9050d7.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CSLRConformer-A-Data-Centric-Conformer-Approach-for-Continuous-Arabic-Sign-Language-Recognition-on-the-Isharah-Datase"><a href="#CSLRConformer-A-Data-Centric-Conformer-Approach-for-Continuous-Arabic-Sign-Language-Recognition-on-the-Isharah-Datase" class="headerlink" title="CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic   Sign Language Recognition on the Isharah Datase"></a>CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic   Sign Language Recognition on the Isharah Datase</h2><p><strong>Authors:Fatimah Mohamed Emad Elden</strong></p>
<p>The field of Continuous Sign Language Recognition (CSLR) poses substantial technical challenges, including fluid inter-sign transitions, the absence of temporal boundaries, and co-articulation effects. This paper, developed for the MSLR 2025 Workshop Challenge at ICCV 2025, addresses the critical challenge of signer-independent recognition to advance the generalization capabilities of CSLR systems across diverse signers. A data-centric methodology is proposed, centered on systematic feature engineering, a robust preprocessing pipeline, and an optimized model architecture. Key contributions include a principled feature selection process guided by Exploratory Data Analysis (EDA) to isolate communicative keypoints, a rigorous preprocessing pipeline incorporating DBSCAN-based outlier filtering and spatial normalization, and the novel CSLRConformer architecture. This architecture adapts the hybrid CNN-Transformer design of the Conformer model, leveraging its capacity to model local temporal dependencies and global sequence context; a characteristic uniquely suited for the spatio-temporal dynamics of sign language. The proposed methodology achieved a competitive performance, with a Word Error Rate (WER) of 5.60% on the development set and 12.01% on the test set, a result that secured a 3rd place ranking on the official competition platform. This research validates the efficacy of cross-domain architectural adaptation, demonstrating that the Conformer model, originally conceived for speech recognition, can be successfully repurposed to establish a new state-of-the-art performance in keypoint-based CSLR. </p>
<blockquote>
<p>è¿ç»­æ‰‹è¯­è¯†åˆ«ï¼ˆCSLRï¼‰é¢†åŸŸé¢ä¸´ç€å·¨å¤§çš„æŠ€æœ¯æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æµç•…çš„ç¬¦å·é—´è¿‡æ¸¡ã€æ— æ—¶é—´è¾¹ç•Œå’ŒååŒå‘éŸ³æ•ˆåº”ç­‰ã€‚æœ¬æ–‡æ˜¯ä¸ºICCV 2025ç ”è®¨ä¼šæŒ‘æˆ˜èµ›MSLR 2025è€Œå¼€å‘çš„ï¼Œè§£å†³äº†ç‹¬ç«‹äºç­¾åè€…çš„è¯†åˆ«è¿™ä¸€å…³é”®æŒ‘æˆ˜ï¼Œä»¥æ¨è¿›CSLRç³»ç»Ÿåœ¨ä¸åŒç­¾åè€…ä¹‹é—´çš„é€šç”¨åŒ–èƒ½åŠ›ã€‚æå‡ºäº†ä¸€ç§ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æ–¹æ³•ï¼Œä»¥ç³»ç»Ÿçš„ç‰¹å¾å·¥ç¨‹ã€ç¨³å¥çš„é¢„å¤„ç†ç®¡é“å’Œä¼˜åŒ–åçš„æ¨¡å‹æ¶æ„ä¸ºä¸­å¿ƒã€‚ä¸»è¦è´¡çŒ®åŒ…æ‹¬ä»¥æ¢ç´¢æ€§æ•°æ®åˆ†æï¼ˆEDAï¼‰ä¸ºæŒ‡å¯¼çš„åŸåˆ™æ€§ç‰¹å¾é€‰æ‹©è¿‡ç¨‹ï¼Œä»¥éš”ç¦»é€šä¿¡å…³é”®ç‚¹ï¼Œä¸¥æ ¼çš„é¢„å¤„ç†ç®¡é“ï¼Œé‡‡ç”¨åŸºäºDBSCANçš„å¼‚å¸¸å€¼è¿‡æ»¤å’Œç©ºé—´å½’ä¸€åŒ–ï¼Œä»¥åŠæ–°é¢–çš„CSLRConformeræ¶æ„ã€‚è¯¥æ¶æ„é‡‡ç”¨äº†Conformeræ¨¡å‹çš„æ··åˆCNN-Transformerè®¾è®¡ï¼Œè¯¥è®¾è®¡å…·æœ‰å¯¹å±€éƒ¨æ—¶é—´ä¾èµ–æ€§å’Œå…¨å±€åºåˆ—ä¸Šä¸‹æ–‡è¿›è¡Œå»ºæ¨¡çš„èƒ½åŠ›ï¼›è¿™ä¸€ç‰¹æ€§éå¸¸é€‚åˆæ‰‹è¯­çš„ç©ºé—´æ—¶é—´åŠ¨æ€ã€‚æ‰€æå‡ºçš„æ–¹æ³•å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œå¼€å‘é›†ä¸Šçš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸º5.60%ï¼Œæµ‹è¯•é›†ä¸Šä¸º12.01%ï¼Œè¿™ä¸€ç»“æœåœ¨å®˜æ–¹ç«èµ›å¹³å°ä¸Šè·å¾—äº†ç¬¬ä¸‰åã€‚è¿™é¡¹ç ”ç©¶éªŒè¯äº†è·¨åŸŸæ¶æ„é€‚åº”æ€§çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜åŸæœ¬ä¸ºè¯­éŸ³è¯†åˆ«è€Œè®¾è®¡çš„Conformeræ¨¡å‹å¯ä»¥æˆåŠŸåœ°è¿›è¡Œæ”¹é€ ï¼Œåœ¨åŸºäºå…³é”®ç‚¹çš„CSLRä¸­å»ºç«‹æœ€æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01791v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    é’ˆå¯¹è¿ç»­æ‰‹è¯­è¯†åˆ«ï¼ˆCSLRï¼‰é¢†åŸŸçš„æŠ€æœ¯æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ç‰¹å¾å·¥ç¨‹ã€é¢„å¤„ç†ç®¡é“å’Œä¼˜åŒ–çš„æ¨¡å‹æ¶æ„ã€‚é‡‡ç”¨åŸºäºæ¢ç´¢æ€§æ•°æ®åˆ†æï¼ˆEDAï¼‰çš„ç‰¹å¾é€‰æ‹©è¿‡ç¨‹ï¼Œç»“åˆDBSCANå¼‚å¸¸å€¼è¿‡æ»¤å’Œç©ºé—´å½’ä¸€åŒ–çš„ä¸¥æ ¼é¢„å¤„ç†ç®¡é“ï¼Œä»¥åŠæ–°é¢–çš„CSLRConformeræ¶æ„ã€‚è¯¥æ–¹æ³•å°†Conformeræ¨¡å‹çš„æ··åˆCNN-Transformerè®¾è®¡é€‚åº”åˆ°æ‰‹è¯­è¯†åˆ«ä¸­ï¼Œå®ç°å±€éƒ¨æ—¶é—´ä¾èµ–æ€§å’Œå…¨å±€åºåˆ—ä¸Šä¸‹æ–‡çš„å»ºæ¨¡ï¼Œéå¸¸é€‚åˆæ‰‹è¯­çš„æ—¶ç©ºåŠ¨æ€ç‰¹æ€§ã€‚åœ¨MSLR 2025 Workshop Challengeä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¼€å‘é›†ä¸Šçš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸º5.6%ï¼Œæµ‹è¯•é›†ä¸Šä¸º12%ï¼Œåœ¨å®˜æ–¹ç«èµ›å¹³å°ä¸Šè·å¾—ç¬¬ä¸‰åã€‚ç ”ç©¶éªŒè¯äº†è·¨åŸŸæ¶æ„é€‚åº”çš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜åŸæœ¬ä¸ºè¯­éŸ³è¯†åˆ«è®¾è®¡çš„Conformeræ¨¡å‹å¯ä»¥æˆåŠŸç”¨äºåŸºäºå…³é”®ç‚¹çš„CSLRï¼Œå¹¶å®ç°äº†æœ€æ–°æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä»‹ç»äº†è¿ç»­æ‰‹è¯­è¯†åˆ«ï¼ˆCSLRï¼‰çš„æŠ€æœ¯æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ‰‹è¯­é—´çš„æµç•…è¿‡æ¸¡ã€æ— æ—¶é—´è¾¹ç•Œå’ŒååŒå‘éŸ³æ•ˆåº”ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œä»¥è§£å†³è·¨ä¸åŒæ‰‹è¯­è€…çš„ç‹¬ç«‹è¯†åˆ«é—®é¢˜ï¼Œæ—¨åœ¨æé«˜CSLRç³»ç»Ÿçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç‰¹å¾å·¥ç¨‹ã€é¢„å¤„ç†ç®¡é“å’Œä¼˜åŒ–çš„æ¨¡å‹æ¶æ„ï¼Œæ„å»ºäº†ä¸€ç§æ–°çš„CSLRConformeræ¶æ„ã€‚</li>
<li>åˆ©ç”¨æ¢ç´¢æ€§æ•°æ®åˆ†æï¼ˆEDAï¼‰æŒ‡å¯¼ç‰¹å¾é€‰æ‹©ï¼Œä»¥è¯†åˆ«æ²Ÿé€šçš„å…³é”®ç‚¹ã€‚</li>
<li>å¼•å…¥äº†åŒ…æ‹¬DBSCANå¼‚å¸¸å€¼è¿‡æ»¤å’Œç©ºé—´å½’ä¸€åŒ–åœ¨å†…çš„ä¸¥æ ¼é¢„å¤„ç†ç®¡é“ã€‚</li>
<li>Conformeræ¨¡å‹çš„æ··åˆCNN-Transformerè®¾è®¡è¢«é€‚åº”åˆ°æ‰‹è¯­è¯†åˆ«ä¸­ï¼Œå®ç°äº†å¯¹å±€éƒ¨æ—¶é—´ä¾èµ–æ€§å’Œå…¨å±€åºåˆ—ä¸Šä¸‹æ–‡çš„å»ºæ¨¡ã€‚</li>
<li>åœ¨MSLR 2025 Workshop Challengeä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å–å¾—äº†æ˜¾è‘—æˆç»©ï¼ŒéªŒè¯äº†è·¨åŸŸæ¶æ„é€‚åº”çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01791">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-70bacc5b9fc89e77972222f0dda16902.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcface09caa4c4e64612010582a1f1e7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd74b2f2502fb21b06eb728eb747f41b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Voxlect-A-Speech-Foundation-Model-Benchmark-for-Modeling-Dialects-and-Regional-Languages-Around-the-Globe"><a href="#Voxlect-A-Speech-Foundation-Model-Benchmark-for-Modeling-Dialects-and-Regional-Languages-Around-the-Globe" class="headerlink" title="Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and   Regional Languages Around the Globe"></a>Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and   Regional Languages Around the Globe</h2><p><strong>Authors:Tiantian Feng, Kevin Huang, Anfeng Xu, Xuan Shi, Thanathai Lertpetchpun, Jihwan Lee, Yoonjeong Lee, Dani Byrd, Shrikanth Narayanan</strong></p>
<p>We present Voxlect, a novel benchmark for modeling dialects and regional languages worldwide using speech foundation models. Specifically, we report comprehensive benchmark evaluations on dialects and regional language varieties in English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai, Spanish, French, German, Brazilian Portuguese, and Italian. Our study used over 2 million training utterances from 30 publicly available speech corpora that are provided with dialectal information. We evaluate the performance of several widely used speech foundation models in classifying speech dialects. We assess the robustness of the dialectal models under noisy conditions and present an error analysis that highlights modeling results aligned with geographic continuity. In addition to benchmarking dialect classification, we demonstrate several downstream applications enabled by Voxlect. Specifically, we show that Voxlect can be applied to augment existing speech recognition datasets with dialect information, enabling a more detailed analysis of ASR performance across dialectal variations. Voxlect is also used as a tool to evaluate the performance of speech generation systems. Voxlect is publicly available with the license of the RAIL family at: <a target="_blank" rel="noopener" href="https://github.com/tiantiaf0627/voxlect">https://github.com/tiantiaf0627/voxlect</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Voxlectï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºä½¿ç”¨è¯­éŸ³åŸºç¡€æ¨¡å‹å¯¹å…¨çƒæ–¹è¨€å’ŒåŒºåŸŸè¯­è¨€è¿›è¡Œå»ºæ¨¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†è‹±è¯­ã€é˜¿æ‹‰ä¼¯è¯­ã€æ™®é€šè¯ã€ç²¤è¯­ã€è—è¯­ã€å°åº¦è¯­ã€æ³°è¯­ã€è¥¿ç­ç‰™è¯­ã€æ³•è¯­ã€å¾·è¯­ã€å·´è¥¿è‘¡è„ç‰™è¯­å’Œæ„å¤§åˆ©è¯­ç­‰æ–¹è¨€å’ŒåŒºåŸŸè¯­è¨€å˜ä½“çš„å…¨é¢åŸºå‡†æµ‹è¯•è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä½¿ç”¨äº†30ä¸ªå…¬å¼€å¯ç”¨çš„è¯­éŸ³è¯­æ–™åº“ä¸­çš„è¶…è¿‡200ä¸‡ä¸ªè®­ç»ƒç‰‡æ®µï¼Œè¿™äº›è¯­æ–™åº“éƒ½æä¾›äº†æ–¹è¨€ä¿¡æ¯ã€‚æˆ‘ä»¬è¯„ä¼°äº†å‡ ç§å¹¿æ³›ä½¿ç”¨çš„è¯­éŸ³åŸºç¡€æ¨¡å‹åœ¨åˆ†ç±»æ–¹è¨€æ–¹é¢çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¯„ä¼°äº†æ–¹è¨€æ¨¡å‹åœ¨å™ªå£°æ¡ä»¶ä¸‹çš„ç¨³å¥æ€§ï¼Œå¹¶è¿›è¡Œäº†è¯¯å·®åˆ†æï¼Œä»¥çªå‡ºä¸åœ°ç†è¿ç»­æ€§ä¸€è‡´çš„å»ºæ¨¡ç»“æœã€‚é™¤äº†åŸºå‡†æ–¹è¨€åˆ†ç±»å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†Voxlectæ”¯æŒçš„å‡ ä¸ªä¸‹æ¸¸åº”ç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å±•ç¤ºäº†Voxlectå¯ä»¥åº”ç”¨äºé€šè¿‡æ–¹è¨€ä¿¡æ¯å¢å¼ºç°æœ‰çš„è¯­éŸ³è¯†åˆ«æ•°æ®é›†ï¼Œä»è€Œæ›´è¯¦ç»†åœ°åˆ†æä¸åŒæ–¹è¨€çš„è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚Voxlectè¿˜ç”¨ä½œè¯„ä¼°è¯­éŸ³ç”Ÿæˆç³»ç»Ÿæ€§èƒ½çš„å·¥å…·æœ‰å…³Voxlectçš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬å…¶åœ¨RAILå®¶æ—ä¸‹çš„è®¸å¯è¯ï¼Œå¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/tiantiaf0627/voxlect%E3%80%82">https://github.com/tiantiaf0627/voxlectã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01691v1">PDF</a> </p>
<p><strong>Summary</strong><br>Voxlectæ˜¯ä¸€ä¸ªç”¨äºå»ºæ¨¡ä¸–ç•Œå„åœ°æ–¹è¨€å’ŒåŒºåŸŸè¯­è¨€çš„åŸºå‡†æµ‹è¯•å·¥å…·ã€‚è¯¥ç ”ç©¶å¯¹å¤šç§è¯­è¨€çš„æ–¹è¨€è¿›è¡Œäº†å…¨é¢åŸºå‡†æµ‹è¯•ï¼Œå¹¶ä½¿ç”¨äº†å¸¦æœ‰æ–¹è¨€ä¿¡æ¯çš„å…¬å¼€è¯­éŸ³è¯­æ–™åº“è¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜è¯„ä¼°äº†åˆ†ç±»æ–¹è¨€çš„è¯­éŸ³åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†Voxlectåœ¨å¤šä¸ªä¸‹æ¸¸åº”ç”¨ä¸­çš„å®ç”¨æ€§ï¼Œå¦‚å¢å¼ºç°æœ‰è¯­éŸ³è¯†åˆ«æ•°æ®é›†ã€è¯„ä¼°è¯­éŸ³ç”Ÿæˆç³»ç»Ÿæ€§èƒ½ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Voxlectæ˜¯ä¸€ä¸ªå…¨çƒæ–¹è¨€å’ŒåŒºåŸŸè¯­è¨€çš„è¯­éŸ³å»ºæ¨¡åŸºå‡†æµ‹è¯•å·¥å…·ã€‚</li>
<li>ç ”ç©¶æ¶µç›–äº†å¤šç§è¯­è¨€çš„æ–¹è¨€å…¨é¢åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬è‹±è¯­ã€é˜¿æ‹‰ä¼¯è¯­ã€æ™®é€šè¯ã€ç²¤è¯­ã€è—è¯­ã€å°åº¦è¯­è¨€ã€æ³°è¯­ã€è¥¿ç­ç‰™è¯­ã€æ³•è¯­ã€å¾·è¯­ã€å·´è¥¿è‘¡è„ç‰™è¯­å’Œæ„å¤§åˆ©è¯­ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†å¸¦æœ‰æ–¹è¨€ä¿¡æ¯çš„è¶…è¿‡2ç™¾ä¸‡è®­ç»ƒè¯­éŸ³æ•°æ®ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†å¤šä¸ªå¹¿æ³›ä½¿ç”¨çš„è¯­éŸ³åŸºç¡€æ¨¡å‹åœ¨åˆ†ç±»æ–¹è¨€æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>Voxlectèƒ½å¤Ÿå¢å¼ºç°æœ‰è¯­éŸ³è¯†åˆ«æ•°æ®é›†ï¼Œå¹¶æ›´è¯¦ç»†åœ°åˆ†æä¸åŒæ–¹è¨€çš„è¯­éŸ³è¯†åˆ«æ€§èƒ½ã€‚</li>
<li>Voxlectä¹Ÿå¯ç”¨äºè¯„ä¼°è¯­éŸ³ç”Ÿæˆç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01691">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1bfcab496dde16b25a7d41f03d04be62.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e79c25180f43901ca0d00e67d4af73d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40806bf76b3eacd5102411956218ec44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ae8c54b8daca48c36948d1ed3f1fd53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71c55bebc502d1227b0c05eecae4b2a0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PESTO-Real-Time-Pitch-Estimation-with-Self-supervised-Transposition-equivariant-Objective"><a href="#PESTO-Real-Time-Pitch-Estimation-with-Self-supervised-Transposition-equivariant-Objective" class="headerlink" title="PESTO: Real-Time Pitch Estimation with Self-supervised   Transposition-equivariant Objective"></a>PESTO: Real-Time Pitch Estimation with Self-supervised   Transposition-equivariant Objective</h2><p><strong>Authors:Alain Riou, Bernardo Torres, Ben Hayes, Stefan Lattner, GaÃ«tan Hadjeres, GaÃ«l Richard, Geoffroy Peeters</strong></p>
<p>In this paper, we introduce PESTO, a self-supervised learning approach for single-pitch estimation using a Siamese architecture. Our model processes individual frames of a Variable-$Q$ Transform (VQT) and predicts pitch distributions. The neural network is designed to be equivariant to translations, notably thanks to a Toeplitz fully-connected layer. In addition, we construct pitch-shifted pairs by translating and cropping the VQT frames and train our model with a novel class-based transposition-equivariant objective, eliminating the need for annotated data. Thanks to this architecture and training objective, our model achieves remarkable performances while being very lightweight ($130$k parameters). Evaluations on music and speech datasets (MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms self-supervised baselines but also competes with supervised methods, exhibiting superior cross-dataset generalization. Finally, we enhance PESTOâ€™s practical utility by developing a streamable VQT implementation using cached convolutions. Combined with our modelâ€™s low latency (less than 10 ms) and minimal parameter count, this makes PESTO particularly suitable for real-time applications. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†PESTOï¼Œè¿™æ˜¯ä¸€ç§åŸºäºSiameseæ¶æ„çš„å•éŸ³é«˜ä¼°è®¡è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¨¡å‹å¤„ç†å¯å˜Qå˜æ¢ï¼ˆVQTï¼‰çš„å•ä¸ªå¸§ï¼Œå¹¶é¢„æµ‹éŸ³é«˜åˆ†å¸ƒã€‚ç¥ç»ç½‘ç»œè¢«è®¾è®¡æˆå¯¹å¹³ç§»å…·æœ‰ç­‰å˜æ€§ï¼Œè¿™å¾—ç›Šäºæ‰˜æ™®åˆ©å…¹å…¨è¿æ¥å±‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å¹³ç§»å’Œè£å‰ªVQTå¸§æ¥æ„å»ºéŸ³é«˜å¯¹ï¼Œå¹¶ä½¿ç”¨æ–°å‹ç±»åˆ«å¹³ç§»ç­‰å˜ç›®æ ‡è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œä»è€Œæ— éœ€æ³¨é‡Šæ•°æ®ã€‚ç”±äºè¿™ç§æ¶æ„å’Œè®­ç»ƒç›®æ ‡ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å–å¾—å“è¶Šæ€§èƒ½çš„åŒæ—¶ï¼Œéå¸¸è½»é‡ï¼ˆä»…æœ‰13ä¸‡ä¸ªå‚æ•°ï¼‰ã€‚åœ¨éŸ³ä¹å’Œè¯­éŸ³æ•°æ®é›†ï¼ˆMIR-1Kã€MDB-stem-synthå’ŒPTDBï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒPESTOä¸ä»…è¶…è¶Šäº†è‡ªç›‘ç£åŸºçº¿ï¼Œè¿˜ä¸æœ‰ç›‘ç£æ–¹æ³•ç›¸ç«äº‰ï¼Œå±•ç°å‡ºå‡ºè‰²çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨ç¼“å­˜å·ç§¯å¼€å‘å¯æµå¼ä¼ è¾“çš„VQTå®ç°ï¼Œå¢å¼ºäº†PESTOçš„å®é™…æ•ˆç”¨ã€‚ç»“åˆæˆ‘ä»¬æ¨¡å‹çš„ä½å»¶è¿Ÿï¼ˆå°‘äº10æ¯«ç§’ï¼‰å’Œå°‘é‡çš„å‚æ•°æ•°é‡ï¼Œè¿™ä½¿å¾—PESTOç‰¹åˆ«é€‚åˆå®æ—¶åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01488v1">PDF</a> Accepted to the Transactions of the International Society for Music   Information Retrieval</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†PESTOï¼Œä¸€ç§åŸºäºè‡ªç›‘ç£å­¦ä¹ çš„å•éŸ³é«˜ä¼°è®¡æ–¹æ³•ï¼Œé‡‡ç”¨Siameseæ¶æ„ã€‚è¯¥æ¨¡å‹å¤„ç†å¯å˜Qå€¼å˜æ¢ï¼ˆVQTï¼‰çš„å•ä¸ªå¸§ï¼Œé¢„æµ‹éŸ³é«˜åˆ†å¸ƒã€‚ç¥ç»ç½‘ç»œè®¾è®¡ä¸ºå¯¹ç¿»è¯‘å…·æœ‰ç­‰ä»·æ€§ï¼Œå¾—ç›ŠäºToeplitzå…¨è¿æ¥å±‚ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç¿»è¯‘å’Œè£å‰ªVQTå¸§æ„å»ºéŸ³é«˜ç§»ä½å¯¹ï¼Œå¹¶ç”¨æ–°å‹ç±»åˆ«åŸºè½¬æ¢ç­‰ä»·ç›®æ ‡è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œæ— éœ€æ³¨é‡Šæ•°æ®ã€‚è¯¥æ¶æ„å’ŒåŸ¹è®­ç›®æ ‡ä½¿å¾—æ¨¡å‹åœ¨å…·æœ‰ä¼˜è‰¯æ€§èƒ½çš„åŒæ—¶éå¸¸è½»é‡çº§ï¼ˆä»…æœ‰$ $ç™¾1. æ•´ä½“æ€§èƒ½ä¼˜å¼‚ï¼Œé€‚ç”¨äºéŸ³ä¹ä¸è¯­éŸ³æ•°æ®é›†ï¼ˆMIR-1Kã€MDB-stem-synthå’ŒPTDBï¼‰ã€‚PESTOä¸ä»…è¶…è¶Šäº†è‡ªç›‘ç£åŸºçº¿ï¼Œè¿˜èƒ½ä¸ç›‘ç£æ–¹æ³•ç›¸ç«äº‰ï¼Œå±•ç°å‡ºå‡ºè‰²çš„è·¨æ•°æ®é›†æ³›åŒ–èƒ½åŠ›ã€‚æœ€åï¼Œå¼€å‘äº†ä¸€ç§ä½¿ç”¨ç¼“å­˜å·ç§¯çš„VQTå®æ—¶å®ç°ï¼Œå¢å¼ºäº†PESTOçš„å®é™…åº”ç”¨ä»·å€¼ã€‚å…¶ä½å»¶è¿Ÿï¼ˆå°äºç™¾æ¯«ç§’çº§ï¼‰å’Œå°‘é‡å‚æ•°ä½¿å¾—PESTOç‰¹åˆ«é€‚åˆå®æ—¶åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡ä¸­åˆ—å‡ºçš„å…³é”®è§è§£è¦ç‚¹ï¼š</p>
<ul>
<li>PESTOæ˜¯ä¸€ç§è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•çš„å•éŸ³é«˜ä¼°è®¡æ¨¡å‹ï¼Œåˆ©ç”¨Siameseæ¶æ„è¿›è¡ŒéŸ³é«˜é¢„æµ‹ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡å¤„ç†å¯å˜Qå€¼å˜æ¢çš„å¸§æ¥é¢„æµ‹éŸ³é«˜åˆ†å¸ƒã€‚</li>
<li>ç¥ç»ç½‘ç»œè®¾è®¡å…·æœ‰ç¿»è¯‘ç­‰ä»·æ€§ï¼Œé€šè¿‡Toeplitzå…¨è¿æ¥å±‚å®ç°ã€‚</li>
<li>åˆ©ç”¨éŸ³é«˜ç§»ä½å¯¹å’Œæ–°å‹ç±»åˆ«åŸºè½¬æ¢ç­‰ä»·ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€æ ‡æ³¨æ•°æ®ã€‚</li>
<li>PESTOæ¨¡å‹æ€§èƒ½ä¼˜è‰¯ä¸”è½»é‡çº§ï¼ˆä»…æœ‰$ $ç™¾kå‚æ•°ï¼‰ã€‚åœ¨éŸ³ä¹å’Œè¯­éŸ³æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºå…¶æ³›åŒ–èƒ½åŠ›å¼ºã€‚</li>
<li>PESTOä¸ä»…ä¼˜äºè‡ªç›‘ç£åŸºçº¿æ–¹æ³•ï¼Œè¿˜èƒ½ä¸ç›‘ç£æ–¹æ³•ç«äº‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01488">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d1f0c84083581839fb16473878371517.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04fec749a8c2fcd79819bd3cbce0eded.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-848f427c56b2cd90c18d0ee85fec7329.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Multi-Granularity-Adaptive-Time-Frequency-Attention-Framework-for-Audio-Deepfake-Detection-under-Real-World-Communication-Degradations"><a href="#Multi-Granularity-Adaptive-Time-Frequency-Attention-Framework-for-Audio-Deepfake-Detection-under-Real-World-Communication-Degradations" class="headerlink" title="Multi-Granularity Adaptive Time-Frequency Attention Framework for Audio   Deepfake Detection under Real-World Communication Degradations"></a>Multi-Granularity Adaptive Time-Frequency Attention Framework for Audio   Deepfake Detection under Real-World Communication Degradations</h2><p><strong>Authors:Haohan Shi, Xiyu Shi, Safak Dogan, Tianjin Huang, Yunxiao Zhang</strong></p>
<p>The rise of highly convincing synthetic speech poses a growing threat to audio communications. Although existing Audio Deepfake Detection (ADD) methods have demonstrated good performance under clean conditions, their effectiveness drops significantly under degradations such as packet losses and speech codec compression in real-world communication environments. In this work, we propose the first unified framework for robust ADD under such degradations, which is designed to effectively accommodate multiple types of Time-Frequency (TF) representations. The core of our framework is a novel Multi-Granularity Adaptive Attention (MGAA) architecture, which employs a set of customizable multi-scale attention heads to capture both global and local receptive fields across varying TF granularities. A novel adaptive fusion mechanism subsequently adjusts and fuses these attention branches based on the saliency of TF regions, allowing the model to dynamically reallocate its focus according to the characteristics of the degradation. This enables the effective localization and amplification of subtle forgery traces. Extensive experiments demonstrate that the proposed framework consistently outperforms state-of-the-art baselines across various real-world communication degradation scenarios, including six speech codecs and five levels of packet losses. In addition, comparative analysis reveals that the MGAA-enhanced features significantly improve separability between real and fake audio classes and sharpen decision boundaries. These results highlight the robustness and practical deployment potential of our framework in real-world communication environments. </p>
<blockquote>
<p>éšç€é«˜åº¦é€¼çœŸçš„åˆæˆè¯­éŸ³çš„å…´èµ·ï¼ŒéŸ³é¢‘é€šä¿¡é¢ä¸´ç€è¶Šæ¥è¶Šå¤§çš„å¨èƒã€‚å°½ç®¡ç°æœ‰çš„éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ï¼ˆADDï¼‰æ–¹æ³•åœ¨æ— å™ªå£°æ¡ä»¶ä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œä½†åœ¨çœŸå®é€šä¿¡ç¯å¢ƒä¸­çš„æ•°æ®åŒ…ä¸¢å¤±å’Œè¯­éŸ³ç¼–è§£ç å™¨å‹ç¼©ç­‰é€€åŒ–æ¡ä»¶ä¸‹ï¼Œå…¶æœ‰æ•ˆæ€§ä¼šæ˜¾è‘—é™ä½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªé’ˆå¯¹æ­¤ç±»é€€åŒ–æƒ…å†µä¸‹ç¨³å¥çš„ADDçš„ç»Ÿä¸€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨æœ‰æ•ˆé€‚åº”å¤šç§æ—¶é—´é¢‘ç‡ï¼ˆTFï¼‰è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ¡†æ¶çš„æ ¸å¿ƒæ˜¯ä¸€ç§æ–°å‹çš„å¤šç²’åº¦è‡ªé€‚åº”æ³¨æ„åŠ›ï¼ˆMGAAï¼‰æ¶æ„ï¼Œå®ƒé‡‡ç”¨ä¸€ç»„å¯å®šåˆ¶çš„å¤šå°ºåº¦æ³¨æ„åŠ›å¤´æ¥æ•è·ä¸åŒTFç²’åº¦ä¸‹çš„å…¨å±€å’Œå±€éƒ¨æ„Ÿå—é‡ã€‚éšåï¼Œä¸€ç§æ–°å‹è‡ªé€‚åº”èåˆæœºåˆ¶æ ¹æ®TFåŒºåŸŸçš„æ˜¾è‘—æ€§è°ƒæ•´å¹¶èåˆè¿™äº›æ³¨æ„åŠ›åˆ†æ”¯ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®é€€åŒ–çš„ç‰¹æ€§åŠ¨æ€é‡æ–°åˆ†é…å…¶ç„¦ç‚¹ã€‚è¿™èƒ½å¤Ÿå®ç°ç»†å¾®ä¼ªé€ ç—•è¿¹çš„æœ‰æ•ˆå®šä½å’Œæ”¾å¤§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ¡†æ¶åœ¨å„ç§çœŸå®é€šä¿¡é€€åŒ–åœºæ™¯ä¸‹å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿ï¼ŒåŒ…æ‹¬å…­ç§è¯­éŸ³ç¼–è§£ç å™¨å’Œäº”ä¸ªçº§åˆ«æ•°æ®åŒ…ä¸¢å¤±çš„åœºæ™¯ã€‚æ­¤å¤–ï¼Œå¯¹æ¯”åˆ†æè¡¨æ˜ï¼Œä½¿ç”¨MGAAå¢å¼ºçš„ç‰¹å¾æ˜¾è‘—æé«˜äº†çœŸå®å’Œä¼ªé€ éŸ³é¢‘ç±»åˆ«ä¹‹é—´çš„å¯åˆ†ç¦»æ€§ï¼Œå¹¶æ˜ç¡®äº†å†³ç­–è¾¹ç•Œã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†æˆ‘ä»¬çš„æ¡†æ¶åœ¨çœŸå®é€šä¿¡ç¯å¢ƒä¸­çš„ç¨³å¥æ€§å’Œå®é™…éƒ¨ç½²æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01467v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨åˆæˆè¯­éŸ³çš„å´›èµ·å¯¹éŸ³é¢‘é€šä¿¡å¸¦æ¥çš„å¨èƒã€‚ç°æœ‰éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹ï¼ˆADDï¼‰æ–¹æ³•åœ¨å®é™…é€šä¿¡ç¯å¢ƒä¸­å­˜åœ¨å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºé¦–ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨é€‚åº”å¤šç§æ—¶é—´é¢‘ç‡ï¼ˆTFï¼‰è¡¨ç¤ºï¼Œå¹¶å¼•å…¥å¤šç²’åº¦è‡ªé€‚åº”æ³¨æ„åŠ›ï¼ˆMGAAï¼‰æ¶æ„ï¼Œæœ‰æ•ˆåº”å¯¹å¤šç§é€šä¿¡ç¯å¢ƒä¸­çš„é€€åŒ–é—®é¢˜ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å„ç§çœŸå®é€šä¿¡é€€åŒ–åœºæ™¯ä¸­è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆæˆè¯­éŸ³çš„å´›èµ·å¯¹éŸ³é¢‘é€šä¿¡æ„æˆå¨èƒã€‚</li>
<li>ç°æœ‰ADDæ–¹æ³•åœ¨çœŸå®é€šä¿¡ç¯å¢ƒä¸‹æ€§èƒ½ä¸‹é™ã€‚</li>
<li>æå‡ºé¦–ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œé€‚åº”å¤šç§TFè¡¨ç¤ºã€‚</li>
<li>å¼•å…¥MGAAæ¶æ„ï¼Œæ•æ‰å…¨å±€å’Œå±€éƒ¨æ„Ÿå—é‡ã€‚</li>
<li>è‡ªé€‚åº”èåˆæœºåˆ¶æ ¹æ®TFåŒºåŸŸçš„æ˜¾è‘—æ€§è°ƒæ•´å¹¶èåˆæ³¨æ„åŠ›åˆ†æ”¯ã€‚</li>
<li>æ¡†æ¶åœ¨å¤šç§çœŸå®é€šä¿¡é€€åŒ–åœºæ™¯ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>MGAAå¢å¼ºç‰¹å¾æé«˜äº†çœŸå®å’Œä¼ªé€ éŸ³é¢‘ä¹‹é—´çš„å¯åˆ†æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad23a9ae2abb044eb2192b35165979ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c46503d4d8f4388b255abe79f88bcb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab2c980953deaf6adf58ee38ed9c9e53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75b81d97846fdb778196b0f366ea0548.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31c2640ee0974b54cd583b7ea94779ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-780214afc6ae4e109b2035a74c5adada.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Hearing-More-with-Less-Multi-Modal-Retrieval-and-Selection-Augmented-Conversational-LLM-Based-ASR"><a href="#Hearing-More-with-Less-Multi-Modal-Retrieval-and-Selection-Augmented-Conversational-LLM-Based-ASR" class="headerlink" title="Hearing More with Less: Multi-Modal Retrieval-and-Selection Augmented   Conversational LLM-Based ASR"></a>Hearing More with Less: Multi-Modal Retrieval-and-Selection Augmented   Conversational LLM-Based ASR</h2><p><strong>Authors:Bingshen Mu, Hexin Liu, Hongfei Xue, Kun Wei, Lei Xie</strong></p>
<p>Automatic Speech Recognition (ASR) aims to convert human speech content into corresponding text. In conversational scenarios, effectively utilizing context can enhance its accuracy. Large Language Modelsâ€™ (LLMs) exceptional long-context understanding and reasoning abilities enable LLM-based ASR (LLM-ASR) to leverage historical context for recognizing conversational speech, which has a high degree of contextual relevance. However, existing conversational LLM-ASR methods use a fixed number of preceding utterances or the entire conversation history as context, resulting in significant ASR confusion and computational costs due to massive irrelevant and redundant information. This paper proposes a multi-modal retrieval-and-selection method named MARS that augments conversational LLM-ASR by enabling it to retrieve and select the most relevant acoustic and textual historical context for the current utterance. Specifically, multi-modal retrieval obtains a set of candidate historical contexts, each exhibiting high acoustic or textual similarity to the current utterance. Multi-modal selection calculates the acoustic and textual similarities for each retrieved candidate historical context and, by employing our proposed near-ideal ranking method to consider both similarities, selects the best historical context. Evaluations on the Interspeech 2025 Multilingual Conversational Speech Language Model Challenge dataset show that the LLM-ASR, when trained on only 1.5K hours of data and equipped with the MARS, outperforms the state-of-the-art top-ranking system trained on 179K hours of data. </p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ—¨åœ¨å°†äººç±»è¯­éŸ³å†…å®¹è½¬æ¢ä¸ºç›¸åº”çš„æ–‡æœ¬ã€‚åœ¨å¯¹è¯åœºæ™¯ä¸­ï¼Œæœ‰æ•ˆåˆ©ç”¨ä¸Šä¸‹æ–‡å¯ä»¥æé«˜å…¶å‡†ç¡®æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‡ºè‰²çš„é•¿æ–‡æœ¬ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œä½¿å¾—åŸºäºLLMçš„ASRï¼ˆLLM-ASRï¼‰èƒ½å¤Ÿåˆ©ç”¨å†å²ä¸Šä¸‹æ–‡æ¥è¯†åˆ«å¯¹è¯è¯­éŸ³ï¼Œè¿™å¯¹é«˜åº¦ç›¸å…³çš„ä¸Šä¸‹æ–‡å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¯¹è¯å¼LLM-ASRæ–¹æ³•ä½¿ç”¨å›ºå®šæ•°é‡çš„å‰é¢çš„è¯è¯­æˆ–æ•´ä¸ªå¯¹è¯å†å²ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œè¿™ä¼šå¯¼è‡´å¤§é‡çš„æ— å…³å’Œå†—ä½™ä¿¡æ¯ï¼Œä»è€Œå¼•å‘æ˜¾è‘—çš„ASRæ··æ·†å’Œè®¡ç®—æˆæœ¬ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMARSçš„å¤šæ¨¡æ€æ£€ç´¢ä¸é€‰æ‹©æ–¹æ³•ï¼Œå®ƒé€šè¿‡å¢å¼ºå¯¹è¯å¼LLM-ASRçš„èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ£€ç´¢å¹¶é€‰æ‹©å½“å‰è¯è¯­ä¸­æœ€ç›¸å…³çš„éŸ³é¢‘å’Œæ–‡æœ¬å†å²ä¸Šä¸‹æ–‡ï¼Œä»è€Œæ‰©å±•äº†å¯¹è¯å¼LLM-ASRçš„åŠŸèƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œå¤šæ¨¡æ€æ£€ç´¢è·å¾—ä¸€ç»„å€™é€‰å†å²ä¸Šä¸‹æ–‡ï¼Œæ¯ä¸ªä¸Šä¸‹æ–‡åœ¨å£°éŸ³æˆ–æ–‡æœ¬ä¸Šä¸å½“å‰è¯è¯­è¡¨ç°å‡ºé«˜åº¦ç›¸ä¼¼æ€§ã€‚å¤šæ¨¡æ€é€‰æ‹©ä¼šè®¡ç®—æ¯ä¸ªæ£€ç´¢åˆ°çš„å€™é€‰å†å²ä¸Šä¸‹æ–‡çš„éŸ³é¢‘å’Œæ–‡æœ¬ç›¸ä¼¼æ€§ï¼Œå¹¶åˆ©ç”¨æˆ‘ä»¬æå‡ºçš„æ¥è¿‘ç†æƒ³æ’åºæ–¹æ³•æ¥è€ƒè™‘è¿™ä¸¤ä¸ªç›¸ä¼¼æ€§ï¼Œé€‰æ‹©æœ€ä½³å†å²ä¸Šä¸‹æ–‡ã€‚åœ¨InterSpeech 2025å¤šè¯­ç§å¯¹è¯è¯­éŸ³è¯­è¨€æ¨¡å‹æŒ‘æˆ˜èµ›æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œä»…ä½¿ç”¨1.5Kå°æ—¶æ•°æ®è®­ç»ƒçš„LLM-ASRï¼Œé…å¤‡MARSåï¼Œå…¶æ€§èƒ½ä¼˜äºä½¿ç”¨17.9ä¸‡å°æ—¶æ•°æ®è®­ç»ƒçš„æœ€æ–°é¡¶å°–ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01166v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯åˆ©ç”¨å¯¹è¯å†å²ä¸Šä¸‹æ–‡æ¥æé«˜è¯†åˆ«å‡†ç¡®æ€§ã€‚ç°æœ‰æ–¹æ³•å­˜åœ¨å›ºå®šä½¿ç”¨è‹¥å¹²å…ˆå‰è¯è¯­æˆ–æ•´ä¸ªå¯¹è¯å†å²ä½œä¸ºä¸Šä¸‹æ–‡çš„é—®é¢˜ï¼Œå¯¼è‡´ASRæ··æ·†å’Œè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMARSçš„å¤šæ¨¡æ€æ£€ç´¢ä¸é€‰æ‹©æ–¹æ³•ï¼Œèƒ½å¤Ÿæ£€ç´¢å¹¶é€‰æ‹©å½“å‰è¯è¯­æœ€ç›¸å…³çš„å£°éŸ³å’Œæ–‡å­—å†å²ä¸Šä¸‹æ–‡ã€‚åœ¨Interspeech 2025å¤šè¯­ç§å¯¹è¯è¯­éŸ³è¯­è¨€æ¨¡å‹æŒ‘æˆ˜èµ›æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä½¿ç”¨MARSçš„LLM-ASRæ€§èƒ½è¶…è¶Šäº†åŸºäºå¤§é‡æ•°æ®çš„å…ˆè¿›æ’åç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ASRä¸­å…·æœ‰å‡ºè‰²çš„é•¿æœŸä¸Šä¸‹æ–‡ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰ASRæ–¹æ³•åœ¨åˆ©ç”¨å¯¹è¯å†å²ä¸Šä¸‹æ–‡æ—¶å­˜åœ¨å†—ä½™ä¿¡æ¯å’Œè®¡ç®—æˆæœ¬é—®é¢˜ã€‚</li>
<li>MARSæ–¹æ³•é€šè¿‡å¤šæ¨¡æ€æ£€ç´¢ä¸é€‰æ‹©å¢å¼ºLLM-ASRæ€§èƒ½ã€‚</li>
<li>MARSèƒ½å¤Ÿæ£€ç´¢ä¸å½“å‰è¯è¯­æœ€ç›¸å…³çš„å£°éŸ³å’Œæ–‡å­—å†å²ä¸Šä¸‹æ–‡ã€‚</li>
<li>ä½¿ç”¨MARSçš„LLM-ASRåœ¨å°‘é‡æ•°æ®è®­ç»ƒä¸‹æ€§èƒ½è¶…è¶Šå¤§é‡æ•°æ®è®­ç»ƒçš„å…ˆè¿›ç³»ç»Ÿã€‚</li>
<li>MARSæ–¹æ³•é€šè¿‡å¹³è¡¡å£°éŸ³å’Œæ–‡å­—çš„ç›¸ä¼¼æ€§ï¼Œæé«˜äº†ASRçš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-39f2b5d2a756260c18f739f8fced3607.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd15bd7b01fe022378c664a43339df17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b84c5561a0f21176e123debd317380bf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9fd97d921c09ce8588c29e7cb6d626d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-606873d63a2b23067cd656c9e6b8ee1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cfbac33f568fb9caaa620737fc2ffb04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd4007d9b77ad6c1ad8d5c20a1553a8f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Fusion-of-Modulation-Spectrogram-and-SSL-with-Multi-head-Attention-for-Fake-Speech-Detection"><a href="#Fusion-of-Modulation-Spectrogram-and-SSL-with-Multi-head-Attention-for-Fake-Speech-Detection" class="headerlink" title="Fusion of Modulation Spectrogram and SSL with Multi-head Attention for   Fake Speech Detection"></a>Fusion of Modulation Spectrogram and SSL with Multi-head Attention for   Fake Speech Detection</h2><p><strong>Authors:Rishith Sadashiv T N, Abhishek Bedge, Saisha Suresh Bore, Jagabandhu Mishra, Mrinmoy Bhattacharjee, S R Mahadeva Prasanna</strong></p>
<p>Fake speech detection systems have become a necessity to combat against speech deepfakes. Current systems exhibit poor generalizability on out-of-domain speech samples due to lack to diverse training data. In this paper, we attempt to address domain generalization issue by proposing a novel speech representation using self-supervised (SSL) speech embeddings and the Modulation Spectrogram (MS) feature. A fusion strategy is used to combine both speech representations to introduce a new front-end for the classification task. The proposed SSL+MS fusion representation is passed to the AASIST back-end network. Experiments are conducted on monolingual and multilingual fake speech datasets to evaluate the efficacy of the proposed model architecture in cross-dataset and multilingual cases. The proposed model achieves a relative performance improvement of 37% and 20% on the ASVspoof 2019 and MLAAD datasets, respectively, in in-domain settings compared to the baseline. In the out-of-domain scenario, the model trained on ASVspoof 2019 shows a 36% relative improvement when evaluated on the MLAAD dataset. Across all evaluated languages, the proposed model consistently outperforms the baseline, indicating enhanced domain generalization. </p>
<blockquote>
<p>å‡è¯­éŸ³æ£€æµ‹ç³»ç»Ÿåœ¨åº”å¯¹è¯­éŸ³æ·±åº¦ä¼ªé€ æ—¶æˆä¸ºäº†å¿…è¦å·¥å…·ã€‚å½“å‰ç³»ç»Ÿç”±äºç¼ºå°‘å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®ï¼Œåœ¨åŸŸå¤–è¯­éŸ³æ ·æœ¬ä¸Šçš„é€šç”¨æ€§è¾ƒå·®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯•å›¾é€šè¿‡æå‡ºä¸€ç§æ–°å‹è¯­éŸ³è¡¨ç°æ¥è§£å†³é¢†åŸŸé€šç”¨æ€§é—®é¢˜ï¼Œè¯¥è¡¨ç°ä½¿ç”¨è‡ªç›‘ç£ï¼ˆSSLï¼‰è¯­éŸ³åµŒå…¥å’Œè°ƒåˆ¶å…‰è°±ï¼ˆMSï¼‰ç‰¹å¾ã€‚æˆ‘ä»¬é‡‡ç”¨èåˆç­–ç•¥ç»“åˆè¿™ä¸¤ç§è¯­éŸ³è¡¨ç°ï¼Œä¸ºåˆ†ç±»ä»»åŠ¡å¼•å…¥æ–°çš„å‰ç«¯ã€‚æå‡ºçš„SSL+MSèåˆè¡¨ç°ä¼ é€’ç»™AASISTåç«¯ç½‘ç»œã€‚å®éªŒåœ¨å•è¯­ç§å’Œå¤šè¯­ç§å‡è¯­éŸ³æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œä»¥è¯„ä¼°æ‰€ææ¨¡å‹æ¶æ„åœ¨è·¨æ•°æ®é›†å’Œå¤šè¯­ç§æƒ…å†µä¸‹çš„æœ‰æ•ˆæ€§ã€‚ä¸åŸºçº¿ç›¸æ¯”ï¼Œåœ¨æ‰€æè®®çš„æ¨¡å‹åœ¨åŸŸå†…è®¾ç½®ä¸‹ï¼Œåœ¨ASVspoof 2019å’ŒMLAADæ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†ç›¸å¯¹æ€§èƒ½æå‡37%å’Œ20%ã€‚åœ¨åŸŸå¤–åœºæ™¯ä¸‹ï¼Œè¯¥æ¨¡å‹åœ¨ASVspoof 2019æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨MLAADæ•°æ®é›†ä¸Šè¯„ä¼°æ—¶æ˜¾ç¤ºå‡ºç›¸å¯¹æ”¹å–„äº†36%ã€‚åœ¨æ‰€è¯„ä¼°çš„æ‰€æœ‰è¯­è¨€ä¸­ï¼Œæ‰€æå‡ºæ¨¡å‹çš„æ€§èƒ½å§‹ç»ˆä¼˜äºåŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºå¢å¼ºçš„é¢†åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01034v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹è¯­éŸ³æ·±åº¦ä¼ªé€ å¸¦æ¥çš„é—®é¢˜ï¼Œå‡è¯­éŸ³æ£€æµ‹ç³»ç»Ÿçš„ç ”ç©¶å˜å¾—è‡³å…³é‡è¦ã€‚å½“å‰ç³»ç»Ÿå› ç¼ºä¹å¤šæ ·åŒ–è®­ç»ƒæ•°æ®è€Œåœ¨è·¨åŸŸè¯­éŸ³æ ·æœ¬ä¸Šè¡¨ç°é€šç”¨æ€§ä¸è¶³ã€‚æœ¬æ–‡è¯•å›¾é€šè¿‡æå‡ºä¸€ç§æ–°å‹çš„è¯­éŸ³è¡¨ç°æ–¹å¼æ¥è§£å†³åŸŸæ³›åŒ–é—®é¢˜ï¼Œè¯¥æ–¹å¼ç»“åˆäº†è‡ªç›‘ç£ï¼ˆSSLï¼‰è¯­éŸ³åµŒå…¥å’Œè°ƒåˆ¶è°±ï¼ˆMSï¼‰ç‰¹å¾ã€‚é€šè¿‡èåˆç­–ç•¥ç»“åˆè¿™ä¸¤ç§è¯­éŸ³è¡¨ç°æ–¹å¼ï¼Œä¸ºåˆ†ç±»ä»»åŠ¡å¼•å…¥æ–°çš„å‰ç«¯ã€‚æ‰€æå‡ºçš„SSL+MSèåˆè¡¨ç°ä¼ é€’ç»™AASISTåç«¯ç½‘ç»œã€‚åœ¨å•è¯­ç§å’Œå¤šè¯­ç§å‡è¯­éŸ³æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒï¼ŒéªŒè¯äº†æ‰€æå‡ºæ¨¡å‹æ¶æ„åœ¨è·¨æ•°æ®é›†å’Œå¤šè¯­ç§æƒ…å†µä¸‹çš„æœ‰æ•ˆæ€§ã€‚ä¸åŸºçº¿ç›¸æ¯”ï¼Œæ‰€æå‡ºæ¨¡å‹åœ¨ASVspoof 2019å’ŒMLAADæ•°æ®é›†ä¸Šçš„åŸŸå†…è®¾ç½®åˆ†åˆ«å®ç°äº†37%å’Œ20%çš„ç›¸å¯¹æ€§èƒ½æå‡ã€‚åœ¨è·¨åŸŸåœºæ™¯ä¸­ï¼ŒåŸºäºASVspoof 2019è®­ç»ƒçš„æ¨¡å‹åœ¨MLAADæ•°æ®é›†ä¸Šæ˜¾ç¤ºäº†36%çš„ç›¸å¯¹æ”¹è¿›ã€‚åœ¨æ‰€æœ‰è¯„ä¼°çš„è¯­è¨€ä¸­ï¼Œæ‰€æå‡ºæ¨¡å‹å§‹ç»ˆä¼˜äºåŸºçº¿ï¼Œæ˜¾ç¤ºå‡ºå¢å¼ºçš„åŸŸæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å‡è¯­éŸ³æ£€æµ‹ç³»ç»Ÿå› è®­ç»ƒæ•°æ®ç¼ºä¹å¤šæ ·æ€§è€Œåœ¨è·¨åŸŸè¯­éŸ³æ ·æœ¬ä¸Šè¡¨ç°å‡ºæœ‰é™çš„é€šç”¨æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è¯­éŸ³è¡¨ç°æ–¹å¼ï¼Œç»“åˆè‡ªç›‘ç£ï¼ˆSSLï¼‰è¯­éŸ³åµŒå…¥å’Œè°ƒåˆ¶è°±ï¼ˆMSï¼‰ç‰¹å¾ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡èåˆç­–ç•¥ï¼Œå°†SSLå’ŒMSç‰¹å¾ç»“åˆï¼Œä¸ºåˆ†ç±»ä»»åŠ¡è®¾è®¡æ–°å‹å‰ç«¯ã€‚</li>
<li>æ‰€æå‡ºçš„æ¨¡å‹æ¶æ„åœ¨å•è¯­ç§å’Œå¤šè¯­ç§å‡è¯­éŸ³æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒéªŒè¯ï¼Œè¡¨ç°å‡ºä¼˜è‰¯çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>åœ¨åŸŸå†…è®¾ç½®ä¸‹ï¼Œç›¸å¯¹äºåŸºçº¿ï¼Œæ‰€æå‡ºæ¨¡å‹åœ¨ASVspoof 2019å’ŒMLAADæ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>åœ¨è·¨åŸŸåœºæ™¯ä¸‹ï¼Œæ¨¡å‹åœ¨MLAADæ•°æ®é›†ä¸Šçš„æ€§èƒ½æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01034">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20999ec8286979082ad2f4b299879b29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2275e8522857fde7e2801522cc5d153.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-406e127c1476f8596e5aa936ddd1eb36.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f838a1f256efd42a08f80f84afa34416.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ac75531b7f381b856a6e81d91eae245.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AudioGen-Omni-A-Unified-Multimodal-Diffusion-Transformer-for-Video-Synchronized-Audio-Speech-and-Song-Generation"><a href="#AudioGen-Omni-A-Unified-Multimodal-Diffusion-Transformer-for-Video-Synchronized-Audio-Speech-and-Song-Generation" class="headerlink" title="AudioGen-Omni: A Unified Multimodal Diffusion Transformer for   Video-Synchronized Audio, Speech, and Song Generation"></a>AudioGen-Omni: A Unified Multimodal Diffusion Transformer for   Video-Synchronized Audio, Speech, and Song Generation</h2><p><strong>Authors:Le Wang, Jun Wang, Feng Deng, Chen Zhang, Di Zhang, Kun Gai</strong></p>
<p>We present AudioGen-Omni - a unified approach based on multimodal diffusion transformers (MMDit), capable of generating high-fidelity audio, speech, and songs coherently synchronized with the input video. AudioGen-Omni introduces a novel joint training paradigm that seamlessly integrates large-scale video-text-audio corpora, enabling a model capable of generating semantically rich, acoustically diverse audio conditioned on multimodal inputs and adaptable to a wide range of audio generation tasks. AudioGen-Omni employs a unified lyrics-transcription encoder that encodes graphemes and phonemes from both sung and spoken inputs into dense frame-level representations. Dense frame-level representations are fused using an AdaLN-based joint attention mechanism enhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein RoPE is selectively applied to temporally structured modalities to ensure precise and robust cross-modal alignment. By unfreezing all modalities and masking missing inputs, AudioGen-Omni mitigates the semantic constraints of text-frozen paradigms, enabling effective cross-modal conditioning. This joint training approach enhances audio quality, semantic alignment, and lip-sync accuracy, while also achieving state-of-the-art results on Text-to-Audio&#x2F;Speech&#x2F;Song tasks. With an inference time of 1.91 seconds for 8 seconds of audio, it offers substantial improvements in both efficiency and generality. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†AudioGen-Omniâ€”â€”ä¸€ç§åŸºäºå¤šæ¨¡å¼æ‰©æ•£å˜å‹å™¨ï¼ˆMMDitï¼‰çš„ç»Ÿä¸€æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸è¾“å…¥è§†é¢‘åŒæ­¥çš„é«˜ä¿çœŸéŸ³é¢‘ã€è¯­éŸ³å’Œæ­Œæ›²ã€‚AudioGen-Omniå¼•å…¥äº†ä¸€ç§æ–°çš„è”åˆè®­ç»ƒèŒƒå¼ï¼Œæ— ç¼é›†æˆäº†å¤§è§„æ¨¡çš„è§†é¢‘-æ–‡æœ¬-éŸ³é¢‘è¯­æ–™åº“ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šæ¨¡å¼è¾“å…¥æ¡ä»¶ä¸‹ç”Ÿæˆè¯­ä¹‰ä¸°å¯Œã€å£°éŸ³å¤šæ ·çš„éŸ³é¢‘ï¼Œå¹¶é€‚åº”å„ç§éŸ³é¢‘ç”Ÿæˆä»»åŠ¡ã€‚AudioGen-Omnié‡‡ç”¨ç»Ÿä¸€çš„æ­Œè¯è½¬å½•ç¼–ç å™¨ï¼Œå°†å”±è¯å’Œå£è¯­è¾“å…¥ä¸­çš„å­—æ¯å’ŒéŸ³ç´ ç¼–ç æˆå¯†é›†çš„å¸§çº§è¡¨ç¤ºã€‚å¯†é›†çš„å¸§çº§è¡¨ç¤ºé€šè¿‡ä½¿ç”¨åŸºäºAdaLNçš„è”åˆæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œèåˆï¼Œå¢å¼ºäº†ç›¸ä½å¯¹é½çš„å¼‚å‘å®šä½æ³¨å…¥ï¼ˆPAAPIï¼‰ï¼Œå…¶ä¸­RoPEè¢«æœ‰é€‰æ‹©åœ°åº”ç”¨äºå…·æœ‰æ—¶é—´ç»“æ„çš„æ¨¡å¼ï¼Œä»¥ç¡®ä¿ç²¾ç¡®å’Œç¨³å®šçš„è·¨æ¨¡å¼å¯¹é½ã€‚é€šè¿‡è§£å†»æ‰€æœ‰æ¨¡å¼å¹¶æ©ç›–ç¼ºå¤±çš„è¾“å…¥ï¼ŒAudioGen-Omniå‡è½»äº†æ–‡æœ¬å†»ç»“èŒƒå¼çš„è¯­ä¹‰çº¦æŸï¼Œå®ç°äº†æœ‰æ•ˆçš„è·¨æ¨¡å¼æ¡ä»¶ã€‚è¿™ç§è”åˆè®­ç»ƒæ–¹æ³•æé«˜äº†éŸ³é¢‘è´¨é‡ã€è¯­ä¹‰å¯¹é½å’Œå”‡åŒæ­¥ç²¾åº¦ï¼ŒåŒæ—¶åœ¨æ–‡æœ¬åˆ°éŸ³é¢‘&#x2F;è¯­éŸ³&#x2F;æ­Œæ›²ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æˆæœã€‚å…¶æ¨ç†æ—¶é—´ä¸º1.91ç§’å¯ç”Ÿæˆ8ç§’çš„éŸ³é¢‘ï¼Œåœ¨æ•ˆç‡å’Œé€šç”¨æ€§æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00733v2">PDF</a> 12 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨ï¼ˆMMDitï¼‰çš„AudioGen-Omniç»Ÿä¸€æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸è¾“å…¥è§†é¢‘åŒæ­¥çš„é«˜ä¿çœŸéŸ³é¢‘ã€è¯­éŸ³å’Œæ­Œæ›²ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§æ–°å‹è”åˆè®­ç»ƒèŒƒå¼ï¼Œæ— ç¼é›†æˆå¤§è§„æ¨¡è§†é¢‘-æ–‡æœ¬-éŸ³é¢‘è¯­æ–™åº“ï¼Œå¯ç”Ÿæˆè¯­ä¹‰ä¸°å¯Œã€å£°éŸ³å¤šæ ·çš„éŸ³é¢‘ï¼Œå¹¶é€‚åº”å„ç§éŸ³é¢‘ç”Ÿæˆä»»åŠ¡ã€‚AudioGen-Omnié‡‡ç”¨ç»Ÿä¸€çš„æ­Œè¯-è½¬å½•ç¼–ç å™¨ï¼Œå¯¹æ­Œå”±å’Œå£è¯­è¾“å…¥ä¸­çš„å­—æ¯å’ŒéŸ³ç´ è¿›è¡Œå¯†é›†å¸§çº§ç¼–ç è¡¨ç¤ºã€‚é€šè¿‡AdaLNè”åˆæ³¨æ„åŠ›æœºåˆ¶çš„èåˆï¼Œç»“åˆç›¸ä½å¯¹é½çš„å¼‚æ„ä½ç½®æ³¨å…¥ï¼ˆPAAPIï¼‰ï¼Œå°†RoPEé€‰æ‹©æ€§åº”ç”¨äºæ—¶é—´ç»“æ„åŒ–æ¨¡å¼ï¼Œç¡®ä¿ç²¾ç¡®å’Œç¨³å®šçš„è·¨æ¨¡å¼å¯¹é½ã€‚è§£å†»æ‰€æœ‰æ¨¡å¼å¹¶å±è”½ç¼ºå¤±è¾“å…¥ï¼ŒAudioGen-Omniç¼“è§£äº†æ–‡æœ¬å†»ç»“èŒƒå¼çš„è¯­ä¹‰çº¦æŸï¼Œå®ç°äº†æœ‰æ•ˆçš„è·¨æ¨¡å¼æ¡ä»¶ã€‚è¯¥è”åˆè®­ç»ƒæ–¹æ³•æé«˜äº†éŸ³é¢‘è´¨é‡ã€è¯­ä¹‰å¯¹é½å’Œå”‡åŒæ­¥ç²¾åº¦ï¼Œå¹¶åœ¨æ–‡æœ¬åˆ°éŸ³é¢‘&#x2F;è¯­éŸ³&#x2F;æ­Œæ›²ä»»åŠ¡ä¸Šå®ç°äº†æœ€æ–°æˆæœã€‚å…¶æ¨ç†æ—¶é—´ä¸ºæ¯ç”Ÿæˆ8ç§’éŸ³é¢‘åªéœ€çº¦1.91ç§’ï¼Œæ•ˆç‡å’Œé€šç”¨æ€§å‡æœ‰æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AudioGen-Omniæ˜¯ä¸€ç§åŸºäºå¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨ï¼ˆMMDitï¼‰çš„ç»Ÿä¸€æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜ä¿çœŸéŸ³é¢‘ã€è¯­éŸ³å’Œæ­Œæ›²ï¼Œä¸”ä¸è¾“å…¥è§†é¢‘åŒæ­¥ã€‚</li>
<li>å¼•å…¥æ–°å‹è”åˆè®­ç»ƒèŒƒå¼ï¼Œé›†æˆè§†é¢‘-æ–‡æœ¬-éŸ³é¢‘è¯­æ–™åº“ã€‚</li>
<li>é‡‡ç”¨ç»Ÿä¸€çš„æ­Œè¯-è½¬å½•ç¼–ç å™¨ï¼Œè¿›è¡Œå¯†é›†å¸§çº§ç¼–ç è¡¨ç¤ºã€‚</li>
<li>ç»“åˆAdaLNè”åˆæ³¨æ„åŠ›æœºåˆ¶å’ŒPAAPIæŠ€æœ¯ï¼Œç¡®ä¿ç²¾ç¡®å’Œç¨³å®šçš„è·¨æ¨¡å¼å¯¹é½ã€‚</li>
<li>é€šè¿‡è§£å†»æ‰€æœ‰æ¨¡å¼å¹¶å±è”½ç¼ºå¤±è¾“å…¥ï¼Œå®ç°æœ‰æ•ˆè·¨æ¨¡æ€æ¡ä»¶ã€‚</li>
<li>è”åˆè®­ç»ƒæ–¹æ³•æé«˜äº†éŸ³é¢‘è´¨é‡ã€è¯­ä¹‰å¯¹é½å’Œå”‡åŒæ­¥ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00733">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e87e3a369212016a0417bc74b4c2db03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38e20067b86f9682aa8b7b2428f93191.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a86b4f217474a0d03ffcfc10bb25eabd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6b67d41d4c49685f7d28ce294f9f9f4.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Real-Time-Audio-Visual-Speech-Enhancement-Using-Pre-trained-Visual-Representations"><a href="#Real-Time-Audio-Visual-Speech-Enhancement-Using-Pre-trained-Visual-Representations" class="headerlink" title="Real-Time Audio-Visual Speech Enhancement Using Pre-trained Visual   Representations"></a>Real-Time Audio-Visual Speech Enhancement Using Pre-trained Visual   Representations</h2><p><strong>Authors:T. Aleksandra Ma, Sile Yin, Li-Chia Yang, Shuo Zhang</strong></p>
<p>Speech enhancement in audio-only settings remains challenging, particularly in the presence of interfering speakers. This paper presents a simple yet effective real-time audio-visual speech enhancement (AVSE) system, RAVEN, which isolates and enhances the on-screen target speaker while suppressing interfering speakers and background noise. We investigate how visual embeddings learned from audio-visual speech recognition (AVSR) and active speaker detection (ASD) contribute to AVSE across different SNR conditions and numbers of interfering speakers. Our results show concatenating embeddings from AVSR and ASD models provides the greatest improvement in low-SNR, multi-speaker environments, while AVSR embeddings alone perform best in noise-only scenarios. In addition, we develop a real-time streaming system that operates on a computer CPU and we provide a video demonstration and code repository. To our knowledge, this is the first open-source implementation of a real-time AVSE system. </p>
<blockquote>
<p>åœ¨åªæœ‰éŸ³é¢‘çš„ç¯å¢ƒä¸­ï¼Œè¯­éŸ³å¢å¼ºä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨å¹²æ‰°è¯´è¯è€…çš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å®æ—¶è§†å¬è¯­éŸ³å¢å¼ºï¼ˆAVSEï¼‰ç³»ç»Ÿï¼Œåä¸ºRAVENï¼Œè¯¥ç³»ç»Ÿå¯ä»¥éš”ç¦»å¹¶å¢å¼ºå±å¹•ä¸Šçš„ç›®æ ‡è¯´è¯è€…ï¼ŒåŒæ—¶æŠ‘åˆ¶å¹²æ‰°è¯´è¯è€…å’ŒèƒŒæ™¯å™ªå£°ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä»è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰å’Œä¸»åŠ¨è¯´è¯è€…æ£€æµ‹ï¼ˆASDï¼‰ä¸­å­¦åˆ°çš„è§†è§‰åµŒå…¥å¦‚ä½•åœ¨ä¸åŒä¿¡å™ªæ¯”æ¡ä»¶å’Œå¹²æ‰°è¯´è¯è€…æ•°é‡çš„æƒ…å†µä¸‹å¯¹AVSEåšå‡ºè´¡çŒ®ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨ä¿¡å™ªæ¯”ä½ã€å¤šè¯´è¯è€…çš„ç¯å¢ƒä¸­ï¼Œæ‹¼æ¥AVSRå’ŒASDæ¨¡å‹çš„åµŒå…¥ä¼šæä¾›æœ€å¤§çš„æ”¹è¿›ï¼Œè€Œä»…ä½¿ç”¨AVSRåµŒå…¥åœ¨åªæœ‰å™ªå£°çš„åœºæ™¯ä¸­è¡¨ç°æœ€ä½³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯åœ¨è®¡ç®—æœºCPUä¸Šè¿è¡Œçš„å®æ—¶æµåª’ä½“ç³»ç»Ÿï¼Œå¹¶æä¾›äº†è§†é¢‘æ¼”ç¤ºå’Œä»£ç ä»“åº“ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå®æ—¶AVSEç³»ç»Ÿçš„å¼€æºå®ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21448v2">PDF</a> Accepted into Interspeech 2025; corrected author name typo</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å®æ—¶è§†å¬è¯­éŸ³å¢å¼ºï¼ˆAVSEï¼‰ç³»ç»Ÿï¼Œåä¸ºRAVENã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿåœ¨æœ‰å¹²æ‰°è¯´è¯äººçš„æƒ…å†µä¸‹ï¼Œéš”ç¦»å¹¶å¢å¼ºå±å¹•ä¸Šçš„ç›®æ ‡è¯´è¯äººå£°éŸ³ï¼ŒåŒæ—¶æŠ‘åˆ¶å¹²æ‰°è¯´è¯äººå’ŒèƒŒæ™¯å™ªéŸ³ã€‚æ–‡ç« æ¢è®¨äº†å¦‚ä½•ä»è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰å’Œæ´»åŠ¨è¯´è¯äººæ£€æµ‹ï¼ˆASDï¼‰ä¸­å­¦ä¹ è§†è§‰åµŒå…¥ï¼Œä»¥åŠ©åŠ›AVSEåœ¨ä¸åŒä¿¡å™ªæ¯”æ¡ä»¶å’Œå¹²æ‰°è¯´è¯äººæ•°é‡çš„åœºæ™¯ä¸­çš„åº”ç”¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä¿¡å™ªæ¯”ä½ã€å­˜åœ¨å¤šåå¹²æ‰°è¯´è¯äººçš„ç¯å¢ƒä¸­ï¼ŒèåˆAVSRå’ŒASDæ¨¡å‹çš„åµŒå…¥ä¿¡æ¯å–å¾—çš„æ•ˆæœæœ€ä½³ï¼›è€Œä»…åœ¨å™ªå£°åœºæ™¯ä¸‹ï¼ŒAVSRåµŒå…¥ä¿¡æ¯è¡¨ç°æœ€å¥½ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜å¼€å‘äº†ä¸€ä¸ªå¯åœ¨è®¡ç®—æœºCPUä¸Šè¿è¡Œçš„å®æ—¶æµåª’ä½“ç³»ç»Ÿï¼Œå¹¶æä¾›äº†è§†é¢‘æ¼”ç¤ºå’Œä»£ç ä»“åº“ã€‚è¿™æ˜¯é¦–ä¸ªå¼€æºçš„å®æ—¶AVSEç³»ç»Ÿã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§å®æ—¶è§†å¬è¯­éŸ³å¢å¼ºï¼ˆAVSEï¼‰ç³»ç»ŸRAVENï¼Œèƒ½å¤Ÿåœ¨æœ‰å¹²æ‰°è¯´è¯äººçš„æƒ…å†µä¸‹å¢å¼ºç›®æ ‡è¯´è¯äººçš„è¯­éŸ³ã€‚</li>
<li>RAVENç³»ç»Ÿé€šè¿‡éš”ç¦»å’Œå¢å¼ºç›®æ ‡è¯´è¯äººå£°éŸ³ï¼ŒåŒæ—¶æŠ‘åˆ¶å¹²æ‰°è¯´è¯äººå’ŒèƒŒæ™¯å™ªéŸ³ï¼Œæ”¹å–„äº†è¯­éŸ³æ¸…æ™°åº¦ã€‚</li>
<li>è®ºæ–‡æ¢è®¨äº†è§†è§‰åµŒå…¥åœ¨AVSEä¸­çš„ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯ä»è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰å’Œæ´»åŠ¨è¯´è¯äººæ£€æµ‹ï¼ˆASDï¼‰ä¸­å­¦ä¹ å¾—åˆ°çš„è§†è§‰åµŒå…¥ã€‚</li>
<li>åœ¨ä¸åŒä¿¡å™ªæ¯”æ¡ä»¶å’Œå¹²æ‰°è¯´è¯äººæ•°é‡çš„åœºæ™¯ä¸­ï¼ŒèåˆAVSRå’ŒASDæ¨¡å‹çš„åµŒå…¥ä¿¡æ¯èƒ½å¤Ÿå–å¾—æœ€ä½³æ€§èƒ½ã€‚</li>
<li>AVSRåµŒå…¥ä¿¡æ¯åœ¨ä»…æœ‰å™ªå£°çš„åœºæ™¯ä¸­è¡¨ç°æœ€å¥½ã€‚</li>
<li>è®ºæ–‡å¼€å‘äº†ä¸€ä¸ªå¯åœ¨è®¡ç®—æœºCPUä¸Šè¿è¡Œçš„å®æ—¶æµåª’ä½“ç³»ç»Ÿï¼Œå®ç°äº†AVSEçš„å®æ—¶åº”ç”¨ã€‚</li>
<li>è¯¥ç³»ç»Ÿæ˜¯é¦–ä¸ªå¼€æºçš„å®æ—¶AVSEç³»ç»Ÿï¼Œæä¾›äº†è§†é¢‘æ¼”ç¤ºå’Œä»£ç ä»“åº“ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œè¿›ä¸€æ­¥å¼€å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4c4d77aa9e018e9a1fb3ab7c1a2d3665.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ccb7a4863ac98099675f2e8d020b1d26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52a76c793b5edeeafae6751fd28e31a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-027184c5e56c67cc641549bf7039f859.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Edge-ASR-Towards-Low-Bit-Quantization-of-Automatic-Speech-Recognition-Models"><a href="#Edge-ASR-Towards-Low-Bit-Quantization-of-Automatic-Speech-Recognition-Models" class="headerlink" title="Edge-ASR: Towards Low-Bit Quantization of Automatic Speech Recognition   Models"></a>Edge-ASR: Towards Low-Bit Quantization of Automatic Speech Recognition   Models</h2><p><strong>Authors:Chen Feng, Yicheng Lin, Shaojie Zhuo, Chenzheng Su, Ramchalam Kinattinkara Ramakrishnan, Zhaocong Yuan, Xiaopeng Zhang</strong></p>
<p>Recent advances in Automatic Speech Recognition (ASR) have demonstrated remarkable accuracy and robustness in diverse audio applications, such as live transcription and voice command processing. However, deploying these models on resource-constrained edge devices (e.g., IoT device, wearables) still presents substantial challenges due to strict limits on memory, compute and power. Quantization, particularly Post-Training Quantization (PTQ), offers an effective way to reduce model size and inference cost without retraining. Despite its importance, the performance implications of various advanced quantization methods and bit-width configurations on ASR models remain unclear. In this work, we present a comprehensive benchmark of eight state-of-the-art (SOTA) PTQ methods applied to two leading edge-ASR model families, Whisper and Moonshine. We systematically evaluate model performances (i.e., accuracy, memory I&#x2F;O and bit operations) across seven diverse datasets from the open ASR leader-board, analyzing the impact of quantization and various configurations on both weights and activations. Built on an extension of the LLM compression toolkit, our framework integrates edge-ASR models, diverse advanced quantization algorithms, a unified calibration and evaluation data pipeline, with detailed analysis tools. Our results characterize the trade-offs between efficiency and accuracy, demonstrating that even $3$-bit quantization can succeed on high capacity models when using advanced PTQ techniques. These findings provide valuable insights for optimizing ASR models on low-power, always-on edge devices. </p>
<blockquote>
<p>è¿‘æœŸè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯çš„è¿›å±•åœ¨å„ç§éŸ³é¢‘åº”ç”¨ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—å‡†ç¡®æ€§å’Œç¨³å¥æ€§ï¼Œä¾‹å¦‚å®æ—¶è½¬å½•å’Œè¯­éŸ³å‘½ä»¤å¤„ç†ã€‚ç„¶è€Œï¼Œå°†è¿™äº›æ¨¡å‹éƒ¨ç½²åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ï¼ˆä¾‹å¦‚ç‰©è”ç½‘è®¾å¤‡å’Œå¯ç©¿æˆ´è®¾å¤‡ï¼‰ä¸Šä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºå†…å­˜ã€è®¡ç®—å’ŒåŠŸç‡çš„ä¸¥æ ¼é™åˆ¶ã€‚é‡åŒ–ï¼Œå°¤å…¶æ˜¯è®­ç»ƒåé‡åŒ–ï¼ˆPTQï¼‰ï¼Œæä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹å¼æ¥å‡å°‘æ¨¡å‹å¤§å°å’Œæ¨ç†æˆæœ¬è€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚å°½ç®¡å…¶å¾ˆé‡è¦ï¼Œä½†å„ç§å…ˆè¿›çš„é‡åŒ–æ–¹æ³•å’Œä½å®½é…ç½®å¯¹ASRæ¨¡å‹æ€§èƒ½çš„å½±å“ä»ä¸æ¸…æ¥šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹åº”ç”¨äºä¸¤ä¸ªé¢†å…ˆè¾¹ç¼˜ASRæ¨¡å‹å®¶æ—Whisperå’ŒMoonshineçš„å…«ç§æœ€æ–°ï¼ˆSOTAï¼‰PTQæ–¹æ³•è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†æ¨¡å‹åœ¨ä¸ƒä¸ªå¼€æ”¾ASRæ’è¡Œæ¦œæ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼ˆå³å‡†ç¡®æ€§ã€å†…å­˜I&#x2F;Oå’Œä½æ“ä½œï¼‰ï¼Œåˆ†æäº†é‡åŒ–å’Œå„ç§é…ç½®å¯¹æƒé‡å’Œæ¿€æ´»çš„å½±å“ã€‚æˆ‘ä»¬çš„æ¡†æ¶åŸºäºLLMå‹ç¼©å·¥å…·åŒ…çš„æ‰©å±•ï¼Œé›†æˆäº†è¾¹ç¼˜ASRæ¨¡å‹ã€å„ç§å…ˆè¿›é‡åŒ–ç®—æ³•ã€ç»Ÿä¸€çš„æ ¡å‡†å’Œè¯„ä¼°æ•°æ®æµæ°´çº¿ä»¥åŠè¯¦ç»†çš„åˆ†æå·¥å…·ã€‚æˆ‘ä»¬çš„ç»“æœæè¿°äº†æ•ˆç‡å’Œå‡†ç¡®æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œè¡¨æ˜ä½¿ç”¨å…ˆè¿›çš„PTQæŠ€æœ¯æ—¶ï¼Œå³ä½¿3ä½é‡åŒ–ä¹Ÿå¯ä»¥åœ¨é«˜å®¹é‡æ¨¡å‹ä¸Šå–å¾—æˆåŠŸã€‚è¿™äº›å‘ç°å¯¹äºåœ¨ä½åŠŸè€—ã€å§‹ç»ˆå¼€å¯çš„è¾¹ç¼˜è®¾å¤‡ä¸Šä¼˜åŒ–ASRæ¨¡å‹æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07877v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯çš„è¿›å±•åœ¨å®æ—¶è½¬å½•å’Œè¯­éŸ³å‘½ä»¤å¤„ç†ç­‰åº”ç”¨ä¸­è¡¨ç°å‡ºæƒŠäººçš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œåœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²è¿™äº›æ¨¡å‹ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚å†…å­˜ã€è®¡ç®—å’Œç”µæºæ–¹é¢çš„é™åˆ¶ã€‚æœ¬ç ”ç©¶å¯¹ä¸¤æ¬¾å‰æ²¿çš„è¾¹ç¼˜ASRæ¨¡å‹å®¶æ—Whisperå’ŒMoonshineåº”ç”¨å…«ç§å…ˆè¿›çš„PTQé‡åŒ–æ–¹æ³•è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚é€šè¿‡è·¨ä¸ƒä¸ªå¼€æ”¾ASRæ’è¡Œæ¦œæ•°æ®é›†çš„ç³»ç»Ÿè¯„ä»·ï¼Œç ”ç©¶åˆ†æäº†é‡åŒ–æ–¹æ³•å’Œé…ç½®å¯¹æƒé‡å’Œæ¿€æ´»çš„å½±å“ã€‚åŸºäºLLMå‹ç¼©å·¥å…·åŒ…çš„æ‰©å±•ï¼Œæœ¬ç ”ç©¶é›†æˆäº†è¾¹ç¼˜ASRæ¨¡å‹ã€å¤šç§å…ˆè¿›çš„é‡åŒ–ç®—æ³•ã€ç»Ÿä¸€çš„æ ¡å‡†å’Œè¯„ä»·æ•°æ®ç®¡é“ä»¥åŠè¯¦ç»†çš„åˆ†æå·¥å…·ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å…ˆè¿›çš„PTQæŠ€æœ¯ï¼Œå³ä½¿3ä½é‡åŒ–ä¹Ÿèƒ½åœ¨é«˜å®¹é‡æ¨¡å‹ä¸­å–å¾—æˆåŠŸã€‚è¿™äº›å‘ç°å¯¹äºåœ¨ä½åŠŸè€—ã€å§‹ç»ˆå¼€å¯çš„è¾¹ç¼˜è®¾å¤‡ä¸Šä¼˜åŒ–ASRæ¨¡å‹æä¾›äº†å®è´µçš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯åœ¨å¤šç§åº”ç”¨ä¸­è¡¨ç°å‡ºé«˜å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ASRæ¨¡å‹é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦é‡åŒ–æŠ€æœ¯æ¥å‡å°æ¨¡å‹å¤§å°å’Œæ¨ç†æˆæœ¬ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†ä¸¤ç§å‰æ²¿ASRæ¨¡å‹å®¶æ—Whisperå’ŒMoonshineï¼Œä½¿ç”¨å…«ç§å…ˆè¿›çš„PTQé‡åŒ–æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶è·¨ä¸ƒä¸ªæ•°æ®é›†ç³»ç»Ÿè¯„ä»·äº†æ¨¡å‹æ€§èƒ½ï¼ŒåŒ…æ‹¬å‡†ç¡®æ€§ã€å†…å­˜I&#x2F;Oå’Œä½æ“ä½œã€‚</li>
<li>ç ”ç©¶é›†æˆäº†è¾¹ç¼˜ASRæ¨¡å‹ã€é‡åŒ–ç®—æ³•ã€æ ¡å‡†å’Œè¯„ä»·æ•°æ®ç®¡é“åŠåˆ†æå·¥å…·ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨å…ˆè¿›çš„PTQæŠ€æœ¯ï¼Œç”šè‡³3ä½é‡åŒ–ä¹Ÿèƒ½åœ¨é«˜å®¹é‡æ¨¡å‹ä¸­æˆåŠŸåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07877">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3cd6d6960f16845b051fea6548eb18ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2daa949f18e40bff8643453361b8af5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1e75300f6a64ef230f4b74b9297b8985.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8560a893995c249789e42dc370883551.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9a869f5d790b11a806353b7da1d61f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7e6446ea1ed1ad1783c9c1906def3a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f04140017f1979953c06d3a58bba3cb6.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Rectifying-Magnitude-Neglect-in-Linear-Attention"><a href="#Rectifying-Magnitude-Neglect-in-Linear-Attention" class="headerlink" title="Rectifying Magnitude Neglect in Linear Attention"></a>Rectifying Magnitude Neglect in Linear Attention</h2><p><strong>Authors:Qihang Fan, Huaibo Huang, Yuang Ai, Ran He</strong></p>
<p>As the core operator of Transformers, Softmax Attention exhibits excellent global modeling capabilities. However, its quadratic complexity limits its applicability to vision tasks. In contrast, Linear Attention shares a similar formulation with Softmax Attention while achieving linear complexity, enabling efficient global information modeling. Nevertheless, Linear Attention suffers from a significant performance degradation compared to standard Softmax Attention. In this paper, we analyze the underlying causes of this issue based on the formulation of Linear Attention. We find that, unlike Softmax Attention, Linear Attention entirely disregards the magnitude information of the Query. This prevents the attention score distribution from dynamically adapting as the Query scales. As a result, despite its structural similarity to Softmax Attention, Linear Attention exhibits a significantly different attention score distribution. Based on this observation, we propose Magnitude-Aware Linear Attention (MALA), which modifies the computation of Linear Attention to fully incorporate the Queryâ€™s magnitude. This adjustment allows MALA to generate an attention score distribution that closely resembles Softmax Attention while exhibiting a more well-balanced structure. We evaluate the effectiveness of MALA on multiple tasks, including image classification, object detection, instance segmentation, semantic segmentation, natural language processing, speech recognition, and image generation. Our MALA achieves strong results on all of these tasks. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/qhfan/MALA">https://github.com/qhfan/MALA</a> </p>
<blockquote>
<p>Transformerçš„æ ¸å¿ƒæ“ä½œå™¨Softmax Attentionå±•ç°å‡ºå‡ºè‰²çš„å…¨å±€å»ºæ¨¡èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶äºŒæ¬¡å¤æ‚åº¦é™åˆ¶äº†å…¶åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒLinear Attentionä¸Softmax Attentionå…·æœ‰ç›¸ä¼¼çš„å…¬å¼ï¼Œä½†å®ç°äº†çº¿æ€§å¤æ‚åº¦ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å…¨å±€ä¿¡æ¯å»ºæ¨¡ã€‚ç„¶è€Œï¼Œä¸æ ‡å‡†çš„Softmax Attentionç›¸æ¯”ï¼ŒLinear Attentionçš„æ€§èƒ½ä¸¥é‡ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åŸºäºLinear Attentionçš„å…¬å¼åˆ†ææ­¤é—®é¢˜çš„æ ¹æœ¬åŸå› ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸Softmax Attentionä¸åŒï¼ŒLinear Attentionå®Œå…¨å¿½ç•¥äº†Queryçš„å¹…åº¦ä¿¡æ¯ã€‚è¿™é˜»æ­¢äº†æ³¨æ„åŠ›å¾—åˆ†åˆ†å¸ƒéšç€Queryçš„ç¼©æ”¾è€ŒåŠ¨æ€é€‚åº”ã€‚å› æ­¤ï¼Œå°½ç®¡å®ƒä¸Softmax Attentionç»“æ„ç›¸ä¼¼ï¼Œä½†Linear Attentionçš„æ³¨æ„åŠ›å¾—åˆ†åˆ†å¸ƒå´å¤§ä¸ç›¸åŒã€‚åŸºäºæ­¤è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†å¹…åº¦æ„ŸçŸ¥çš„Linear Attentionï¼ˆMALAï¼‰ï¼Œå®ƒä¿®æ”¹äº†Linear Attentionçš„è®¡ç®—ä»¥å……åˆ†èå…¥Queryçš„å¹…åº¦ã€‚è¿™ä¸€è°ƒæ•´ä½¿MALAèƒ½å¤Ÿç”Ÿæˆä¸Softmax Attentionç›¸ä¼¼çš„æ³¨æ„åŠ›å¾—åˆ†åˆ†å¸ƒï¼ŒåŒæ—¶å±•ç°å‡ºæ›´ä¸ºå¹³è¡¡çš„ç»“æ„ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¯„ä¼°äº†MALAçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²ã€è¯­ä¹‰åˆ†å‰²ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«å’Œå›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬çš„MALAåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†å¼ºå¤§çš„ç»“æœã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/qhfan/MALA">https://github.com/qhfan/MALA</a>ä¸Šæä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00698v3">PDF</a> Accepted by ICCV2025, highlight paper</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Transformerä¸­çš„æ ¸å¿ƒæ“ä½œã€‚Softmax Attentionå…·æœ‰å‡ºè‰²çš„å…¨å±€å»ºæ¨¡èƒ½åŠ›ï¼Œä½†å…¶äºŒæ¬¡å¤æ‚åº¦é™åˆ¶äº†å…¶åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒLinear Attentionè™½ç„¶ä¸Softmax Attentionå…·æœ‰ç±»ä¼¼çš„å…¬å¼ï¼Œä½†å…·æœ‰çº¿æ€§å¤æ‚åº¦ï¼Œèƒ½å¤Ÿå®ç°æœ‰æ•ˆçš„å…¨å±€ä¿¡æ¯å»ºæ¨¡ã€‚ç„¶è€Œï¼Œå…¶æ€§èƒ½è¾ƒå·®ã€‚åŸºäºLinear Attentionçš„å…¬å¼åˆ†æï¼Œæˆ‘ä»¬å‘ç°Linear Attentionå¿½ç•¥äº†Queryçš„å¹…åº¦ä¿¡æ¯ï¼Œå¯¼è‡´æ³¨æ„åŠ›å¾—åˆ†åˆ†å¸ƒæ— æ³•éšQueryçš„å˜åŒ–è€ŒåŠ¨æ€è°ƒæ•´ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Magnitude-Aware Linear Attentionï¼ˆMALAï¼‰ï¼Œå®ƒé€šè¿‡ä¿®æ”¹Linear Attentionçš„è®¡ç®—æ¥å…¨é¢èå…¥Queryçš„å¹…åº¦ï¼Œç”Ÿæˆä¸Softmax Attentionç›¸ä¼¼çš„æ³¨æ„åŠ›å¾—åˆ†åˆ†å¸ƒï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸Šå–å¾—äº†è‰¯å¥½æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Softmax Attentionåœ¨Transformerä¸­å…·æœ‰ä¼˜ç§€çš„å…¨å±€å»ºæ¨¡èƒ½åŠ›ï¼Œä½†äºŒæ¬¡å¤æ‚åº¦é™åˆ¶äº†å…¶åœ¨è§†è§‰ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚</li>
<li>Linear Attentionè™½ç„¶å…·æœ‰çº¿æ€§å¤æ‚åº¦ï¼Œä½†ä¸Softmax Attentionç›¸æ¯”ï¼Œå…¶æ€§èƒ½æœ‰æ‰€é™ä½ã€‚</li>
<li>Linear Attentionå¿½ç•¥äº†Queryçš„å¹…åº¦ä¿¡æ¯ï¼Œå¯¼è‡´æ³¨æ„åŠ›å¾—åˆ†åˆ†å¸ƒå›ºå®šï¼Œæ— æ³•éšQueryçš„å˜åŒ–è€Œè°ƒæ•´ã€‚</li>
<li>MALAé€šè¿‡ä¿®æ”¹Linear Attentionçš„è®¡ç®—ï¼Œå…¨é¢èå…¥Queryçš„å¹…åº¦ä¿¡æ¯ã€‚</li>
<li>MALAç”Ÿæˆçš„æ³¨æ„åŠ›å¾—åˆ†åˆ†å¸ƒä¸Softmax Attentionç›¸ä¼¼ï¼Œå…·æœ‰æ›´å¹³è¡¡çš„ç»“æ„ã€‚</li>
<li>MALAåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€å¯¹è±¡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²ã€è¯­ä¹‰åˆ†å‰²ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³è¯†åˆ«å’Œå›¾åƒç”Ÿæˆï¼Œå¹¶å–å¾—äº†è‰¯å¥½æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6b6494763e1e0d31602e9349fcfbacd3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e9ae945a2ce34db84f97b69f636a7ad2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c67263aea0a851211e4b263070b438e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7d0fbfade3a2fae4267e44252804f06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb9855ff5403bcef39fb12ca47885498.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="FedMLAC-Mutual-Learning-Driven-Heterogeneous-Federated-Audio-Classification"><a href="#FedMLAC-Mutual-Learning-Driven-Heterogeneous-Federated-Audio-Classification" class="headerlink" title="FedMLAC: Mutual Learning Driven Heterogeneous Federated Audio   Classification"></a>FedMLAC: Mutual Learning Driven Heterogeneous Federated Audio   Classification</h2><p><strong>Authors:Jun Bai, Rajib Rana, Di Wu, Youyang Qu, Xiaohui Tao, Ji Zhang, Carlos Busso, Shivakumara Palaiahnakote</strong></p>
<p>Federated Learning (FL) offers a privacy-preserving framework for training audio classification (AC) models across decentralized clients without sharing raw data. However, Federated Audio Classification (FedAC) faces three major challenges: data heterogeneity, model heterogeneity, and data poisoning, which degrade performance in real-world settings. While existing methods often address these issues separately, a unified and robust solution remains underexplored. We propose FedMLAC, a mutual learning-based FL framework that tackles all three challenges simultaneously. Each client maintains a personalized local AC model and a lightweight, globally shared Plug-in model. These models interact via bidirectional knowledge distillation, enabling global knowledge sharing while adapting to local data distributions, thus addressing both data and model heterogeneity. To counter data poisoning, we introduce a Layer-wise Pruning Aggregation (LPA) strategy that filters anomalous Plug-in updates based on parameter deviations during aggregation. Extensive experiments on four diverse audio classification benchmarks, including both speech and non-speech tasks, show that FedMLAC consistently outperforms state-of-the-art baselines in classification accuracy and robustness to noisy data. </p>
<blockquote>
<p>è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä¸ºåœ¨åˆ†æ•£çš„å®¢æˆ·ç«¯ä¸Šè®­ç»ƒéŸ³é¢‘åˆ†ç±»ï¼ˆACï¼‰æ¨¡å‹æä¾›äº†ä¸€ç§ä¿æŠ¤éšç§çš„æ¡†æ¶ï¼Œæ— éœ€å…±äº«åŸå§‹æ•°æ®ã€‚ç„¶è€Œï¼Œè”é‚¦éŸ³é¢‘åˆ†ç±»ï¼ˆFedACï¼‰é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šæ•°æ®å¼‚è´¨æ€§ã€æ¨¡å‹å¼‚è´¨æ€§ï¼Œä»¥åŠæ•°æ®æ¯’åŒ–ï¼Œå®ƒä»¬åœ¨ç°å®ä¸–ç•Œçš„è®¾ç½®ä¸­ä¼šé™ä½æ€§èƒ½ã€‚è™½ç„¶ç°æœ‰æ–¹æ³•é€šå¸¸åˆ†åˆ«è§£å†³è¿™äº›é—®é¢˜ï¼Œä½†ç»Ÿä¸€ä¸”ç¨³å¥çš„è§£å†³æ–¹æ¡ˆä»ç„¶ç¼ºä¹æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºFedMLACï¼Œä¸€ä¸ªåŸºäºç›¸äº’å­¦ä¹ çš„è”é‚¦å­¦ä¹ æ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶åº”å¯¹æ‰€æœ‰ä¸‰ä¸ªæŒ‘æˆ˜ã€‚æ¯ä¸ªå®¢æˆ·ç«¯ç»´æŠ¤ä¸€ä¸ªä¸ªæ€§åŒ–çš„æœ¬åœ°ACæ¨¡å‹å’Œä¸€ä¸ªè½»é‡çº§çš„ã€å…¨å±€å…±äº«çš„æ’ä»¶æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹é€šè¿‡åŒå‘çŸ¥è¯†è’¸é¦è¿›è¡Œäº¤äº’ï¼Œèƒ½å¤Ÿå®ç°å…¨å±€çŸ¥è¯†å…±äº«ï¼ŒåŒæ—¶é€‚åº”æœ¬åœ°æ•°æ®åˆ†å¸ƒï¼Œä»è€Œè§£å†³æ•°æ®å’Œæ¨¡å‹å¼‚è´¨æ€§é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹æ•°æ®æ¯’åŒ–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ†å±‚å‰ªæèšåˆï¼ˆLPAï¼‰ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æ ¹æ®èšåˆè¿‡ç¨‹ä¸­çš„å‚æ•°åå·®æ¥è¿‡æ»¤å¼‚å¸¸çš„æ’ä»¶æ›´æ–°ã€‚åœ¨åŒ…æ‹¬è¯­éŸ³å’Œéè¯­éŸ³ä»»åŠ¡åœ¨å†…çš„å››ä¸ªä¸åŒçš„éŸ³é¢‘åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFedMLACåœ¨åˆ†ç±»å‡†ç¡®æ€§å’Œå¯¹å™ªå£°æ•°æ®çš„ç¨³å¥æ€§æ–¹é¢å‡ä¼˜äºæœ€æ–°åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10207v2">PDF</a> updated version for the first submission</p>
<p><strong>Summary</strong></p>
<p>è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä¸ºéŸ³é¢‘åˆ†ç±»ï¼ˆACï¼‰æ¨¡å‹åœ¨åˆ†æ•£å¼å®¢æˆ·ç«¯ä¸Šçš„è®­ç»ƒæä¾›äº†ä¸€ä¸ªéšç§ä¿æŠ¤æ¡†æ¶ï¼Œæ— éœ€å…±äº«åŸå§‹æ•°æ®ã€‚ç„¶è€Œï¼Œè”é‚¦éŸ³é¢‘åˆ†ç±»ï¼ˆFedACï¼‰é¢ä¸´æ•°æ®å¼‚è´¨æ€§ã€æ¨¡å‹å¼‚è´¨æ€§å’Œæ•°æ®ä¸­æ¯’ä¸‰å¤§æŒ‘æˆ˜ï¼Œè¿™äº›æŒ‘æˆ˜åœ¨ç°å®ä¸–ç•Œç¯å¢ƒä¸­ä¼šé™ä½æ€§èƒ½ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºFedMLACï¼Œä¸€ä¸ªåŸºäºäº’åŠ©å­¦ä¹ çš„è”é‚¦å­¦ä¹ æ¡†æ¶ï¼ŒåŒæ—¶è§£å†³è¿™ä¸‰ä¸ªæŒ‘æˆ˜ã€‚æ¯ä¸ªå®¢æˆ·ç«¯ç»´æŠ¤ä¸ªæ€§åŒ–çš„æœ¬åœ°ACæ¨¡å‹å’Œè½»é‡çº§çš„å…¨å±€å…±äº«æ’ä»¶æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹é€šè¿‡åŒå‘çŸ¥è¯†è’¸é¦è¿›è¡Œäº¤äº’ï¼Œå®ç°å…¨å±€çŸ¥è¯†å…±äº«ï¼ŒåŒæ—¶é€‚åº”æœ¬åœ°æ•°æ®åˆ†å¸ƒï¼Œä»è€Œè§£å†³æ•°æ®å’Œæ¨¡å‹å¼‚è´¨æ€§ã€‚ä¸ºäº†åº”å¯¹æ•°æ®ä¸­æ¯’ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ†å±‚å‰ªæèšåˆï¼ˆLPAï¼‰ç­–ç•¥ï¼Œæ ¹æ®èšåˆè¿‡ç¨‹ä¸­çš„å‚æ•°åå·®æ¥è¿‡æ»¤å¼‚å¸¸çš„æ’ä»¶æ›´æ–°ã€‚åœ¨å››ä¸ªä¸åŒçš„éŸ³é¢‘åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFedMLACåœ¨åˆ†ç±»å‡†ç¡®æ€§å’Œå¯¹å™ªå£°æ•°æ®çš„é²æ£’æ€§æ–¹é¢å‡ä¼˜äºæœ€æ–°åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è”é‚¦å­¦ä¹ ï¼ˆFLï¼‰ä¸ºéŸ³é¢‘åˆ†ç±»æ¨¡å‹çš„è®­ç»ƒæä¾›äº†éšç§ä¿æŠ¤æ¡†æ¶ã€‚</li>
<li>è”é‚¦éŸ³é¢‘åˆ†ç±»ï¼ˆFedACï¼‰é¢ä¸´æ•°æ®å¼‚è´¨æ€§ã€æ¨¡å‹å¼‚è´¨æ€§å’Œæ•°æ®ä¸­æ¯’ä¸‰å¤§æŒ‘æˆ˜ã€‚</li>
<li>FedMLACæ˜¯ä¸€ä¸ªåŸºäºäº’åŠ©å­¦ä¹ çš„è”é‚¦å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶è§£å†³ä¸Šè¿°ä¸‰å¤§æŒ‘æˆ˜ã€‚</li>
<li>FedMLACé€šè¿‡åŒå‘çŸ¥è¯†è’¸é¦è§£å†³æ•°æ®åˆ†å¸ƒå’Œæ¨¡å‹å¼‚è´¨æ€§é—®é¢˜ã€‚</li>
<li>FedMLACå¼•å…¥åˆ†å±‚å‰ªæèšåˆï¼ˆLPAï¼‰ç­–ç•¥æ¥åº”å¯¹æ•°æ®ä¸­æ¯’é—®é¢˜ã€‚</li>
<li>FedMLACåœ¨åˆ†ç±»å‡†ç¡®æ€§ä»¥åŠå¯¹å™ªå£°æ•°æ®çš„é²æ£’æ€§æ–¹é¢ä¼˜äºæœ€æ–°åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10207">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-53317f5e2d6a4f0aae2d022295dc1b93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73c361d5471a89624c7ec8b5ed062d7d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="GestureLSM-Latent-Shortcut-based-Co-Speech-Gesture-Generation-with-Spatial-Temporal-Modeling"><a href="#GestureLSM-Latent-Shortcut-based-Co-Speech-Gesture-Generation-with-Spatial-Temporal-Modeling" class="headerlink" title="GestureLSM: Latent Shortcut based Co-Speech Gesture Generation with   Spatial-Temporal Modeling"></a>GestureLSM: Latent Shortcut based Co-Speech Gesture Generation with   Spatial-Temporal Modeling</h2><p><strong>Authors:Pinxin Liu, Luchuan Song, Junhua Huang, Haiyang Liu, Chenliang Xu</strong></p>
<p>Generating full-body human gestures based on speech signals remains challenges on quality and speed. Existing approaches model different body regions such as body, legs and hands separately, which fail to capture the spatial interactions between them and result in unnatural and disjointed movements. Additionally, their autoregressive&#x2F;diffusion-based pipelines show slow generation speed due to dozens of inference steps. To address these two challenges, we propose GestureLSM, a flow-matching-based approach for Co-Speech Gesture Generation with spatial-temporal modeling. Our method i) explicitly model the interaction of tokenized body regions through spatial and temporal attention, for generating coherent full-body gestures. ii) introduce the flow matching to enable more efficient sampling by explicitly modeling the latent velocity space. To overcome the suboptimal performance of flow matching baseline, we propose latent shortcut learning and beta distribution time stamp sampling during training to enhance gesture synthesis quality and accelerate inference. Combining the spatial-temporal modeling and improved flow matching-based framework, GestureLSM achieves state-of-the-art performance on BEAT2 while significantly reducing inference time compared to existing methods, highlighting its potential for enhancing digital humans and embodied agents in real-world applications. Project Page: <a target="_blank" rel="noopener" href="https://andypinxinliu.github.io/GestureLSM">https://andypinxinliu.github.io/GestureLSM</a> </p>
<blockquote>
<p>åŸºäºè¯­éŸ³ä¿¡å·ç”Ÿæˆå…¨èº«äººä½“å§¿æ€åœ¨è´¨é‡å’Œé€Ÿåº¦æ–¹é¢ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•å°†èº«ä½“çš„ä¸åŒéƒ¨ä½ï¼ˆå¦‚èº«ä½“ã€è…¿å’Œæ‰‹ï¼‰åˆ†åˆ«å»ºæ¨¡ï¼Œæ— æ³•æ•æ‰å®ƒä»¬ä¹‹é—´çš„ç©ºé—´äº¤äº’ï¼Œå¯¼è‡´åŠ¨ä½œä¸è‡ªç„¶ã€ä¸è¿è´¯ã€‚æ­¤å¤–ï¼Œå®ƒä»¬çš„è‡ªå›å½’&#x2F;æ‰©æ•£åŸºç®¡é“ç”±äºéœ€è¦æ•°åæ­¥æ¨ç†è€Œæ˜¾ç¤ºå‡ºç¼“æ…¢çš„ç”Ÿæˆé€Ÿåº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæµåŒ¹é…çš„ååŒè¯­éŸ³å§¿æ€ç”Ÿæˆæ–¹æ³•GestureLSMï¼Œå¹¶è¿›è¡Œäº†æ—¶ç©ºå»ºæ¨¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸€ï¼‰é€šè¿‡ç©ºé—´å’Œæ—¶é—´æ³¨æ„åŠ›æ˜¾å¼åœ°æ¨¡æ‹Ÿæ ‡è®°èº«ä½“éƒ¨ä½çš„äº¤äº’ï¼Œä»¥ç”Ÿæˆè¿è´¯çš„å…¨èº«å§¿æ€ã€‚äºŒï¼‰å¼•å…¥æµåŒ¹é…ï¼Œé€šè¿‡æ˜¾å¼å»ºæ¨¡æ½œåœ¨é€Ÿåº¦ç©ºé—´ï¼Œä½¿é‡‡æ ·æ›´åŠ é«˜æ•ˆã€‚ä¸ºäº†å…‹æœæµåŒ¹é…åŸºçº¿æ€§èƒ½ä¸ä½³çš„é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æå‡ºäº†æ½œåœ¨å¿«æ·æ–¹å¼å­¦ä¹ å’Œbetaåˆ†å¸ƒæ—¶é—´æˆ³é‡‡æ ·ï¼Œä»¥æé«˜å§¿æ€åˆæˆè´¨é‡å’ŒåŠ é€Ÿæ¨ç†ã€‚ç»“åˆæ—¶ç©ºå»ºæ¨¡å’Œæ”¹è¿›çš„åŸºäºæµåŒ¹é…çš„æ¡†æ¶ï¼ŒGestureLSMåœ¨BEAT2ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”æ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ï¼Œå‡¸æ˜¾å…¶åœ¨å¢å¼ºæ•°å­—äººç±»å’Œå®ä½“ä»£ç†åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://andypinxinliu.github.io/GestureLSM">https://andypinxinliu.github.io/GestureLSM</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18898v3">PDF</a> Accepted to ICCV 2025. Project Page:   <a target="_blank" rel="noopener" href="https://andypinxinliu.github.io/GestureLSM">https://andypinxinliu.github.io/GestureLSM</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºè¯­éŸ³ä¿¡å·ç”Ÿæˆå…¨èº«äººç±»åŠ¨ä½œåœ¨è´¨é‡å’Œé€Ÿåº¦ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•åˆ†åˆ«å»ºæ¨¡èº«ä½“ã€è…¿å’Œæ‰‹éƒ¨ç­‰ä¸åŒçš„èº«ä½“åŒºåŸŸï¼Œå¿½ç•¥äº†å®ƒä»¬ä¹‹é—´çš„ç©ºé—´äº¤äº’ï¼Œå¯¼è‡´åŠ¨ä½œä¸è‡ªç„¶ã€ä¸è¿è´¯ã€‚æ­¤å¤–ï¼Œå®ƒä»¬çš„è‡ªå›å½’&#x2F;æ‰©æ•£ç”Ÿæˆæµç¨‹ç”±äºéœ€è¦å¤§é‡çš„æ¨ç†æ­¥éª¤ï¼Œç”Ÿæˆé€Ÿåº¦è¾ƒæ…¢ã€‚é’ˆå¯¹è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºGestureLSMï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæµåŒ¹é…çš„ååŒè¯­éŸ³åŠ¨ä½œç”Ÿæˆæ–¹æ³•ï¼Œå…·æœ‰æ—¶ç©ºå»ºæ¨¡åŠŸèƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸€ï¼‰é€šè¿‡ç©ºé—´å’Œæ—¶é—´æ³¨æ„åŠ›æ˜¾å¼åœ°æ¨¡æ‹Ÿäº†æ ‡è®°èº«ä½“åŒºåŸŸçš„äº¤äº’ï¼Œä»¥ç”Ÿæˆè¿è´¯çš„å…¨èº«åŠ¨ä½œã€‚äºŒï¼‰å¼•å…¥æµåŒ¹é…ï¼Œé€šè¿‡æ˜¾å¼å»ºæ¨¡æ½œåœ¨é€Ÿåº¦ç©ºé—´ï¼Œä½¿é‡‡æ ·æ›´åŠ é«˜æ•ˆã€‚ä¸ºäº†å…‹æœæµåŒ¹é…åŸºçº¿æ–¹æ³•çš„æ€§èƒ½ä¸ä½³é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æå‡ºäº†æ½œåœ¨æ·å¾„å­¦ä¹ å’ŒåŸºäºbetaåˆ†å¸ƒçš„æ—¶é—´æˆ³é‡‡æ ·ï¼Œä»¥æé«˜åŠ¨ä½œåˆæˆè´¨é‡å’ŒåŠ é€Ÿæ¨ç†ã€‚ç»“åˆäº†æ—¶ç©ºå»ºæ¨¡å’Œæ”¹è¿›çš„åŸºäºæµåŒ¹é…æ¡†æ¶çš„GestureLSMï¼Œåœ¨BEAT2ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ï¼Œçªæ˜¾å…¶åœ¨å¢å¼ºæ•°å­—äººç±»å’Œå®ä½“ä»£ç†å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ–¹æ³•æ¨¡æ‹Ÿä¸åŒèº«ä½“åŒºåŸŸå¯¼è‡´åŠ¨ä½œä¸è‡ªç„¶å’Œè”åˆæ€§ä¸è¶³ã€‚</li>
<li>ç°æœ‰æ–¹æ³•çš„è‡ªå›å½’&#x2F;æ‰©æ•£ç”Ÿæˆæµç¨‹å¯¼è‡´ç”Ÿæˆé€Ÿåº¦è¾ƒæ…¢ã€‚</li>
<li>GestureLSMæ–¹æ³•é€šè¿‡ç©ºé—´å’Œæ—¶é—´æ³¨æ„åŠ›æ˜¾å¼å»ºæ¨¡èº«ä½“åŒºåŸŸçš„äº¤äº’ã€‚</li>
<li>GestureLSMå¼•å…¥æµåŒ¹é…ä»¥æ˜¾å¼å»ºæ¨¡æ½œåœ¨é€Ÿåº¦ç©ºé—´å¹¶æé«˜é‡‡æ ·æ•ˆç‡ã€‚</li>
<li>é€šè¿‡æ½œåœ¨æ·å¾„å­¦ä¹ å’Œbetaåˆ†å¸ƒæ—¶é—´æˆ³é‡‡æ ·æ”¹è¿›æµåŒ¹é…ä»¥æé«˜åŠ¨ä½œåˆæˆè´¨é‡å’ŒåŠ é€Ÿæ¨ç†ã€‚</li>
<li>GestureLSMåœ¨BEAT2ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18898">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e000d41130bf5f31bb6c8805ac38f9f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec563ce563158330ef24f16c48e058e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f71aae53c617290800b2299a161b05d7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a24ea0831f0471568ee493e4851abd1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7489ddd80b112f39f0e1a8cb15f9aa8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7f257b9e64decf58357f02d79cba5777.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  NS-Net Decoupling CLIP Semantic Information through NULL-Space for   Generalizable AI-Generated Image Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-391d0438101f32b0daf729408f735270.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  Semi-Supervised Dual-Threshold Contrastive Learning for Ultrasound Image   Classification and Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29885.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
