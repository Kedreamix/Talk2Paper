<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-08-06  InfoSyncNet Information Synchronization Temporal Convolutional Network   for Visual Speech Recognition">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-f838a1f256efd42a08f80f84afa34416.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    15k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    61 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-06-更新"><a href="#2025-08-06-更新" class="headerlink" title="2025-08-06 更新"></a>2025-08-06 更新</h1><h2 id="InfoSyncNet-Information-Synchronization-Temporal-Convolutional-Network-for-Visual-Speech-Recognition"><a href="#InfoSyncNet-Information-Synchronization-Temporal-Convolutional-Network-for-Visual-Speech-Recognition" class="headerlink" title="InfoSyncNet: Information Synchronization Temporal Convolutional Network   for Visual Speech Recognition"></a>InfoSyncNet: Information Synchronization Temporal Convolutional Network   for Visual Speech Recognition</h2><p><strong>Authors:Junxiao Xue, Xiaozhen Liu, Xuecheng Wu, Fei Yu, Jun Wang</strong></p>
<p>Estimating spoken content from silent videos is crucial for applications in Assistive Technology (AT) and Augmented Reality (AR). However, accurately mapping lip movement sequences in videos to words poses significant challenges due to variability across sequences and the uneven distribution of information within each sequence. To tackle this, we introduce InfoSyncNet, a non-uniform sequence modeling network enhanced by tailored data augmentation techniques. Central to InfoSyncNet is a non-uniform quantization module positioned between the encoder and decoder, enabling dynamic adjustment to the network’s focus and effectively handling the natural inconsistencies in visual speech data. Additionally, multiple training strategies are incorporated to enhance the model’s capability to handle variations in lighting and the speaker’s orientation. Comprehensive experiments on the LRW and LRW1000 datasets confirm the superiority of InfoSyncNet, achieving new state-of-the-art accuracies of 92.0% and 60.7% Top-1 ACC. The code is available for download (see comments). </p>
<blockquote>
<p>从无声视频中估计语音内容对于辅助技术（AT）和增强现实（AR）的应用至关重要。然而，由于序列之间的可变性和序列内信息的不均匀分布，准确地将视频中的唇动序列映射到单词上带来了巨大的挑战。为了解决这一问题，我们引入了InfoSyncNet，这是一个通过定制的数据增强技术增强的非均匀序列建模网络。InfoSyncNet的核心是一个位于编码器和解码器之间的非均匀量化模块，它能够实现网络关注的动态调整，并有效地处理视觉语音数据中的自然不一致性。此外，还融入了多种训练策略，以提高模型处理光照和说话人方向变化的能力。在LRW和LRW1000数据集上的综合实验证实了InfoSyncNet的优越性，达到了最新的最高准确率，Top-1 ACC分别为92.0%和60.7%。代码可下载（详见注释）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02460v1">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/liuxiaozhen123/InfoSyncNet">https://github.com/liuxiaozhen123/InfoSyncNet</a></p>
<p><strong>Summary</strong></p>
<p>基于无声视频进行口语内容估算对于辅助技术和增强现实应用至关重要。针对视频中的唇动序列与词语的精准映射所面临的挑战，InfoSyncNet采用了非均匀序列建模网络结合定制化数据增强技术来解决这一问题。其中非均匀量化模块为核心组件，实现了网络关注的动态调整并有效应对视觉语音数据的自然不一致性。实验证明InfoSyncNet在LRW和LRW1000数据集上表现卓越，达到新的业界顶尖准确率，分别为92.0%和60.7%。代码已公开下载。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>InfoSyncNet用于从无声视频中估算口语内容，在辅助技术和增强现实应用中具有重要意义。</li>
<li>唇动序列与词语的映射面临重大挑战，主要由于序列变化多端及信息分布不均一。</li>
<li>InfoSyncNet通过非均匀序列建模应对挑战，特别包括非均匀量化模块，实现网络关注动态调整。</li>
<li>定制化的数据增强技术和多种训练策略提升了模型处理不同光照条件和说话人朝向变化的能力。</li>
<li>在LRW和LRW1000数据集上的实验证实InfoSyncNet的卓越性能，达到新的业界顶尖准确率。</li>
<li>InfoSyncNet的代码已公开供下载。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02460">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8cb39e29f0d4d806c2ecacf7eb3ce833.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89ee0c11a3dfffdfb0b8dd79fbf187df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09e694ca4ce7baa1c007826a74308e29.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1b837941f14a8eb8bcc347a95737e045.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06d02d28771b5aa9553f45d4f266fcbe.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Charting-15-years-of-progress-in-deep-learning-for-speech-emotion-recognition-A-replication-study"><a href="#Charting-15-years-of-progress-in-deep-learning-for-speech-emotion-recognition-A-replication-study" class="headerlink" title="Charting 15 years of progress in deep learning for speech emotion   recognition: A replication study"></a>Charting 15 years of progress in deep learning for speech emotion   recognition: A replication study</h2><p><strong>Authors:Andreas Triantafyllopoulos, Anton Batliner, Björn W. Schuller</strong></p>
<p>Speech emotion recognition (SER) has long benefited from the adoption of deep learning methodologies. Deeper models – with more layers and more trainable parameters – are generally perceived as being &#96;better’ by the SER community. This raises the question – \emph{how much better} are modern-era deep neural networks compared to their earlier iterations? Beyond that, the more important question of how to move forward remains as poignant as ever. SER is far from a solved problem; therefore, identifying the most prominent avenues of future research is of paramount importance. In the present contribution, we attempt a quantification of progress in the 15 years of research beginning with the introduction of the landmark 2009 INTERSPEECH Emotion Challenge. We conduct a large scale investigation of model architectures, spanning both audio-based models that rely on speech inputs and text-baed models that rely solely on transcriptions. Our results point towards diminishing returns and a plateau after the recent introduction of transformer architectures. Moreover, we demonstrate how perceptions of progress are conditioned on the particular selection of models that are compared. Our findings have important repercussions about the state-of-the-art in SER research and the paths forward </p>
<blockquote>
<p>语音情感识别（SER）长久以来受益于深度学习方法的采用。在SER界，更深的模型（具有更多层和更多可训练参数）通常被认为是“更好”的。这就提出了一个问题——与现代早期的深度神经网络相比，现代深度神经网络“更好”到什么地步？除此之外，如何继续前进这一更重要的问题仍然至关重要。SER远非一个已解决的问题，因此，确定未来研究的最重要途径至关重要。在本研究中，我们尝试量化自2009年具有里程碑意义的INTERSPEECH情感挑战赛引入以来15年研究的进展。我们对模型架构进行了大规模调查，既包括基于音频的依赖于语音输入的模型，也包括仅依赖于转录的文本模型。我们的结果表明，在最近引入transformer架构之后，收益正在减少并且出现高原效应。此外，我们还证明了人们对进步的看法是受所选择的比较模型的影响。我们的发现对SER研究的最新进展以及未来的方向具有重要的影响。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02448v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/CHI-TUM/ser-progress-replication">https://github.com/CHI-TUM/ser-progress-replication</a> Submitted   for review</p>
<p><strong>Summary</strong></p>
<p>这篇文本主要探讨了语音情感识别（SER）领域的研究进展，通过大规模的模型架构调查，发现随着深度学习方法的不断采用，进步已经趋于平稳并达到顶峰。研究发现在引入Transformer架构后，收益逐渐递减。此外，对于进步的认识取决于所选择的模型比较。这篇文本对于了解当前SER研究的最新进展和未来的方向具有重要意义。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音情感识别（SER）领域受益于深度学习方法的广泛应用。</li>
<li>更深层次的模型通常被认为是更好的选择，但在实际中可能存在收益递减现象。</li>
<li>通过大规模的模型架构调查，研究发现近年来进步逐渐放缓并可能已经达到一个峰值或平稳状态。</li>
<li>在引入Transformer架构后，模型性能的提升逐渐趋于平稳。</li>
<li>对进步的认识受到所选模型比较的影响。</li>
<li>当前SER研究仍面临挑战，需要进一步探索和研究新的方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02448">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ec2b0a6eb5805920ef3628e4ff5fc8ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad61e7a16f8e76f2b9b0c2a0f7d8c78a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d5db95dc6c661c5234f7c639ed5e638.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SpeechR-A-Benchmark-for-Speech-Reasoning-in-Large-Audio-Language-Models"><a href="#SpeechR-A-Benchmark-for-Speech-Reasoning-in-Large-Audio-Language-Models" class="headerlink" title="SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models"></a>SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models</h2><p><strong>Authors:Wanqi Yang, Yanda Li, Yunchao Wei, Meng Fang, Ling Chen</strong></p>
<p>Large audio-language models (LALMs) have achieved near-human performance in sentence-level transcription and emotion recognition. However, existing evaluations focus mainly on surface-level perception, leaving the capacity of models for contextual and inference-driven reasoning in speech-based scenarios insufficiently examined. To address this gap, we introduce SpeechR, a unified benchmark for evaluating reasoning over speech in large audio-language models. SpeechR evaluates models along three key dimensions: factual retrieval, procedural inference, and normative judgment. It includes three distinct evaluation formats. The multiple-choice version measures answer selection accuracy. The generative version assesses the coherence and logical consistency of reasoning chains. The acoustic-feature version investigates whether variations in stress and emotion affect reasoning performance. Evaluations on eleven state-of-the-art LALMs reveal that high transcription accuracy does not translate into strong reasoning capabilities. SpeechR establishes a structured benchmark for evaluating reasoning in spoken language, enabling more targeted analysis of model capabilities across diverse dialogue-based tasks. </p>
<blockquote>
<p>大型音频语言模型（LALMs）在句子级转录和情绪识别方面已经达到了接近人类的性能。然而，现有的评估主要集中在表面层次的感知上，对模型在基于语音的场景中进行上下文和推理驱动推理的能力缺乏足够的考察。为了弥补这一空白，我们引入了SpeechR，这是一个用于评估大型音频语言模型中语音推理的统一基准。SpeechR沿着三个关键维度评估模型：事实检索、程序推理和规范性判断。它包括三种不同的评估格式。多项选择版本衡量答案选择准确性。生成版本评估推理链的连贯性和逻辑一致性。声学特征版本研究应力和情绪的变化是否影响推理性能。对十一种最先进的LALM进行的评估表明，高转录准确性并不等同于强大的推理能力。SpeechR建立了一个结构化的基准，用于评估口语中的推理能力，能够在各种基于对话的任务上更针对性地分析模型的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02018v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型音频语言模型（LALM）在句子级转录和情感识别方面已达到接近人类的性能。然而，现有的评估主要集中在表面感知上，缺乏对语音场景中模型上下文和推理驱动能力的充分考察。为解决这一空白，我们引入了SpeechR，一个用于评估大型音频语言模型中语音推理的统一基准。SpeechR通过三个关键维度：事实检索、程序推理和规范判断来评估模型。包括三种不同的评估格式。多项选择版衡量答案选择准确性。生成版评估推理链的连贯性和逻辑一致性。声学特征版研究应力和情感变化是否影响推理性能。对十一种最新LALM的评估显示，高转录准确性并不等同于强大的推理能力。SpeechR为评估口语中的推理能力建立了结构化基准，使不同对话任务的能力分析更具针对性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型音频语言模型（LALM）在语音领域已接近人类性能水平，特别是在句子级转录和情感识别方面。</li>
<li>现有评估主要关注表面感知，缺乏模型上下文和推理驱动能力的考察。</li>
<li>引入SpeechR统一基准以评估模型中语音推理的能力。</li>
<li>SpeechR涵盖三个关键维度：事实检索、程序推理和规范判断。</li>
<li>提供三种评估格式，包括多项选择、生成和基于声学特征的评估。</li>
<li>评估结果显示高转录准确性并不等同于强大的推理能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02018">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-69abad54fe13ab2f6b0b805c1919d73e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd4b50e9cde15f6d28deef9dc6afd287.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69832ef1f8333bdd3829d8881ed0b807.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d32814cd4d6266a9617fcf178e6b54f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-03126bd423403e2a5933e7a29fe4b0a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df3471a682740dc6a2433704eb9050d7.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CSLRConformer-A-Data-Centric-Conformer-Approach-for-Continuous-Arabic-Sign-Language-Recognition-on-the-Isharah-Datase"><a href="#CSLRConformer-A-Data-Centric-Conformer-Approach-for-Continuous-Arabic-Sign-Language-Recognition-on-the-Isharah-Datase" class="headerlink" title="CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic   Sign Language Recognition on the Isharah Datase"></a>CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic   Sign Language Recognition on the Isharah Datase</h2><p><strong>Authors:Fatimah Mohamed Emad Elden</strong></p>
<p>The field of Continuous Sign Language Recognition (CSLR) poses substantial technical challenges, including fluid inter-sign transitions, the absence of temporal boundaries, and co-articulation effects. This paper, developed for the MSLR 2025 Workshop Challenge at ICCV 2025, addresses the critical challenge of signer-independent recognition to advance the generalization capabilities of CSLR systems across diverse signers. A data-centric methodology is proposed, centered on systematic feature engineering, a robust preprocessing pipeline, and an optimized model architecture. Key contributions include a principled feature selection process guided by Exploratory Data Analysis (EDA) to isolate communicative keypoints, a rigorous preprocessing pipeline incorporating DBSCAN-based outlier filtering and spatial normalization, and the novel CSLRConformer architecture. This architecture adapts the hybrid CNN-Transformer design of the Conformer model, leveraging its capacity to model local temporal dependencies and global sequence context; a characteristic uniquely suited for the spatio-temporal dynamics of sign language. The proposed methodology achieved a competitive performance, with a Word Error Rate (WER) of 5.60% on the development set and 12.01% on the test set, a result that secured a 3rd place ranking on the official competition platform. This research validates the efficacy of cross-domain architectural adaptation, demonstrating that the Conformer model, originally conceived for speech recognition, can be successfully repurposed to establish a new state-of-the-art performance in keypoint-based CSLR. </p>
<blockquote>
<p>连续手语识别（CSLR）领域面临着巨大的技术挑战，包括流畅的符号间过渡、无时间边界和协同发音效应等。本文是为ICCV 2025研讨会挑战赛MSLR 2025而开发的，解决了独立于签名者的识别这一关键挑战，以推进CSLR系统在不同签名者之间的通用化能力。提出了一种以数据为中心的方法，以系统的特征工程、稳健的预处理管道和优化后的模型架构为中心。主要贡献包括以探索性数据分析（EDA）为指导的原则性特征选择过程，以隔离通信关键点，严格的预处理管道，采用基于DBSCAN的异常值过滤和空间归一化，以及新颖的CSLRConformer架构。该架构采用了Conformer模型的混合CNN-Transformer设计，该设计具有对局部时间依赖性和全局序列上下文进行建模的能力；这一特性非常适合手语的空间时间动态。所提出的方法取得了具有竞争力的性能，开发集上的单词错误率（WER）为5.60%，测试集上为12.01%，这一结果在官方竞赛平台上获得了第三名。这项研究验证了跨域架构适应性的有效性，表明原本为语音识别而设计的Conformer模型可以成功地进行改造，在基于关键点的CSLR中建立最新的最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01791v1">PDF</a> </p>
<p><strong>摘要</strong><br>    针对连续手语识别（CSLR）领域的技术挑战，本文提出一种数据驱动的方法，包括特征工程、预处理管道和优化的模型架构。采用基于探索性数据分析（EDA）的特征选择过程，结合DBSCAN异常值过滤和空间归一化的严格预处理管道，以及新颖的CSLRConformer架构。该方法将Conformer模型的混合CNN-Transformer设计适应到手语识别中，实现局部时间依赖性和全局序列上下文的建模，非常适合手语的时空动态特性。在MSLR 2025 Workshop Challenge上的实验结果表明，该方法在开发集上的词错误率（WER）为5.6%，测试集上为12%，在官方竞赛平台上获得第三名。研究验证了跨域架构适应的有效性，表明原本为语音识别设计的Conformer模型可以成功用于基于关键点的CSLR，并实现了最新性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>介绍了连续手语识别（CSLR）的技术挑战，包括手语间的流畅过渡、无时间边界和协同发音效应。</li>
<li>提出了一种数据驱动的方法，以解决跨不同手语者的独立识别问题，旨在提高CSLR系统的泛化能力。</li>
<li>通过特征工程、预处理管道和优化的模型架构，构建了一种新的CSLRConformer架构。</li>
<li>利用探索性数据分析（EDA）指导特征选择，以识别沟通的关键点。</li>
<li>引入了包括DBSCAN异常值过滤和空间归一化在内的严格预处理管道。</li>
<li>Conformer模型的混合CNN-Transformer设计被适应到手语识别中，实现了对局部时间依赖性和全局序列上下文的建模。</li>
<li>在MSLR 2025 Workshop Challenge上的实验结果表明，该方法取得了显著成绩，验证了跨域架构适应的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01791">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-70bacc5b9fc89e77972222f0dda16902.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dcface09caa4c4e64612010582a1f1e7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd74b2f2502fb21b06eb728eb747f41b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Voxlect-A-Speech-Foundation-Model-Benchmark-for-Modeling-Dialects-and-Regional-Languages-Around-the-Globe"><a href="#Voxlect-A-Speech-Foundation-Model-Benchmark-for-Modeling-Dialects-and-Regional-Languages-Around-the-Globe" class="headerlink" title="Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and   Regional Languages Around the Globe"></a>Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and   Regional Languages Around the Globe</h2><p><strong>Authors:Tiantian Feng, Kevin Huang, Anfeng Xu, Xuan Shi, Thanathai Lertpetchpun, Jihwan Lee, Yoonjeong Lee, Dani Byrd, Shrikanth Narayanan</strong></p>
<p>We present Voxlect, a novel benchmark for modeling dialects and regional languages worldwide using speech foundation models. Specifically, we report comprehensive benchmark evaluations on dialects and regional language varieties in English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai, Spanish, French, German, Brazilian Portuguese, and Italian. Our study used over 2 million training utterances from 30 publicly available speech corpora that are provided with dialectal information. We evaluate the performance of several widely used speech foundation models in classifying speech dialects. We assess the robustness of the dialectal models under noisy conditions and present an error analysis that highlights modeling results aligned with geographic continuity. In addition to benchmarking dialect classification, we demonstrate several downstream applications enabled by Voxlect. Specifically, we show that Voxlect can be applied to augment existing speech recognition datasets with dialect information, enabling a more detailed analysis of ASR performance across dialectal variations. Voxlect is also used as a tool to evaluate the performance of speech generation systems. Voxlect is publicly available with the license of the RAIL family at: <a target="_blank" rel="noopener" href="https://github.com/tiantiaf0627/voxlect">https://github.com/tiantiaf0627/voxlect</a>. </p>
<blockquote>
<p>我们介绍了Voxlect，这是一个新的基准测试，用于使用语音基础模型对全球方言和区域语言进行建模。具体来说，我们报告了英语、阿拉伯语、普通话、粤语、藏语、印度语、泰语、西班牙语、法语、德语、巴西葡萄牙语和意大利语等方言和区域语言变体的全面基准测试评估。我们的研究使用了30个公开可用的语音语料库中的超过200万个训练片段，这些语料库都提供了方言信息。我们评估了几种广泛使用的语音基础模型在分类方言方面的性能。我们评估了方言模型在噪声条件下的稳健性，并进行了误差分析，以突出与地理连续性一致的建模结果。除了基准方言分类外，我们还展示了Voxlect支持的几个下游应用。具体来说，我们展示了Voxlect可以应用于通过方言信息增强现有的语音识别数据集，从而更详细地分析不同方言的语音识别性能。Voxlect还用作评估语音生成系统性能的工具有关Voxlect的详细信息，包括其在RAIL家族下的许可证，可访问：<a target="_blank" rel="noopener" href="https://github.com/tiantiaf0627/voxlect%E3%80%82">https://github.com/tiantiaf0627/voxlect。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01691v1">PDF</a> </p>
<p><strong>Summary</strong><br>Voxlect是一个用于建模世界各地方言和区域语言的基准测试工具。该研究对多种语言的方言进行了全面基准测试，并使用了带有方言信息的公开语音语料库进行训练。此外，该研究还评估了分类方言的语音基础模型的性能，展示了Voxlect在多个下游应用中的实用性，如增强现有语音识别数据集、评估语音生成系统性能等。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Voxlect是一个全球方言和区域语言的语音建模基准测试工具。</li>
<li>研究涵盖了多种语言的方言全面基准测试，包括英语、阿拉伯语、普通话、粤语、藏语、印度语言、泰语、西班牙语、法语、德语、巴西葡萄牙语和意大利语。</li>
<li>研究使用了带有方言信息的超过2百万训练语音数据。</li>
<li>研究评估了多个广泛使用的语音基础模型在分类方言方面的性能。</li>
<li>Voxlect能够增强现有语音识别数据集，并更详细地分析不同方言的语音识别性能。</li>
<li>Voxlect也可用于评估语音生成系统的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01691">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1bfcab496dde16b25a7d41f03d04be62.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e79c25180f43901ca0d00e67d4af73d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40806bf76b3eacd5102411956218ec44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ae8c54b8daca48c36948d1ed3f1fd53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71c55bebc502d1227b0c05eecae4b2a0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PESTO-Real-Time-Pitch-Estimation-with-Self-supervised-Transposition-equivariant-Objective"><a href="#PESTO-Real-Time-Pitch-Estimation-with-Self-supervised-Transposition-equivariant-Objective" class="headerlink" title="PESTO: Real-Time Pitch Estimation with Self-supervised   Transposition-equivariant Objective"></a>PESTO: Real-Time Pitch Estimation with Self-supervised   Transposition-equivariant Objective</h2><p><strong>Authors:Alain Riou, Bernardo Torres, Ben Hayes, Stefan Lattner, Gaëtan Hadjeres, Gaël Richard, Geoffroy Peeters</strong></p>
<p>In this paper, we introduce PESTO, a self-supervised learning approach for single-pitch estimation using a Siamese architecture. Our model processes individual frames of a Variable-$Q$ Transform (VQT) and predicts pitch distributions. The neural network is designed to be equivariant to translations, notably thanks to a Toeplitz fully-connected layer. In addition, we construct pitch-shifted pairs by translating and cropping the VQT frames and train our model with a novel class-based transposition-equivariant objective, eliminating the need for annotated data. Thanks to this architecture and training objective, our model achieves remarkable performances while being very lightweight ($130$k parameters). Evaluations on music and speech datasets (MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms self-supervised baselines but also competes with supervised methods, exhibiting superior cross-dataset generalization. Finally, we enhance PESTO’s practical utility by developing a streamable VQT implementation using cached convolutions. Combined with our model’s low latency (less than 10 ms) and minimal parameter count, this makes PESTO particularly suitable for real-time applications. </p>
<blockquote>
<p>本文介绍了PESTO，这是一种基于Siamese架构的单音高估计自监督学习方法。我们的模型处理可变Q变换（VQT）的单个帧，并预测音高分布。神经网络被设计成对平移具有等变性，这得益于托普利兹全连接层。此外，我们通过平移和裁剪VQT帧来构建音高对，并使用新型类别平移等变目标进行模型训练，从而无需注释数据。由于这种架构和训练目标，我们的模型在取得卓越性能的同时，非常轻量（仅有13万个参数）。在音乐和语音数据集（MIR-1K、MDB-stem-synth和PTDB）上的评估表明，PESTO不仅超越了自监督基线，还与有监督方法相竞争，展现出出色的跨数据集泛化能力。最后，我们通过使用缓存卷积开发可流式传输的VQT实现，增强了PESTO的实际效用。结合我们模型的低延迟（少于10毫秒）和少量的参数数量，这使得PESTO特别适合实时应用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01488v1">PDF</a> Accepted to the Transactions of the International Society for Music   Information Retrieval</p>
<p><strong>Summary</strong></p>
<p>本文介绍了PESTO，一种基于自监督学习的单音高估计方法，采用Siamese架构。该模型处理可变Q值变换（VQT）的单个帧，预测音高分布。神经网络设计为对翻译具有等价性，得益于Toeplitz全连接层。此外，通过翻译和裁剪VQT帧构建音高移位对，并用新型类别基转换等价目标进行模型训练，无需注释数据。该架构和培训目标使得模型在具有优良性能的同时非常轻量级（仅有$ $百1. 整体性能优异，适用于音乐与语音数据集（MIR-1K、MDB-stem-synth和PTDB）。PESTO不仅超越了自监督基线，还能与监督方法相竞争，展现出出色的跨数据集泛化能力。最后，开发了一种使用缓存卷积的VQT实时实现，增强了PESTO的实际应用价值。其低延迟（小于百毫秒级）和少量参数使得PESTO特别适合实时应用。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是文中列出的关键见解要点：</p>
<ul>
<li>PESTO是一种自监督学习方法的单音高估计模型，利用Siamese架构进行音高预测。</li>
<li>该模型通过处理可变Q值变换的帧来预测音高分布。</li>
<li>神经网络设计具有翻译等价性，通过Toeplitz全连接层实现。</li>
<li>利用音高移位对和新型类别基转换等价目标进行训练，无需标注数据。</li>
<li>PESTO模型性能优良且轻量级（仅有$ $百k参数）。在音乐和语音数据集上的评估显示其泛化能力强。</li>
<li>PESTO不仅优于自监督基线方法，还能与监督方法竞争。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01488">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d1f0c84083581839fb16473878371517.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04fec749a8c2fcd79819bd3cbce0eded.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-848f427c56b2cd90c18d0ee85fec7329.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Multi-Granularity-Adaptive-Time-Frequency-Attention-Framework-for-Audio-Deepfake-Detection-under-Real-World-Communication-Degradations"><a href="#Multi-Granularity-Adaptive-Time-Frequency-Attention-Framework-for-Audio-Deepfake-Detection-under-Real-World-Communication-Degradations" class="headerlink" title="Multi-Granularity Adaptive Time-Frequency Attention Framework for Audio   Deepfake Detection under Real-World Communication Degradations"></a>Multi-Granularity Adaptive Time-Frequency Attention Framework for Audio   Deepfake Detection under Real-World Communication Degradations</h2><p><strong>Authors:Haohan Shi, Xiyu Shi, Safak Dogan, Tianjin Huang, Yunxiao Zhang</strong></p>
<p>The rise of highly convincing synthetic speech poses a growing threat to audio communications. Although existing Audio Deepfake Detection (ADD) methods have demonstrated good performance under clean conditions, their effectiveness drops significantly under degradations such as packet losses and speech codec compression in real-world communication environments. In this work, we propose the first unified framework for robust ADD under such degradations, which is designed to effectively accommodate multiple types of Time-Frequency (TF) representations. The core of our framework is a novel Multi-Granularity Adaptive Attention (MGAA) architecture, which employs a set of customizable multi-scale attention heads to capture both global and local receptive fields across varying TF granularities. A novel adaptive fusion mechanism subsequently adjusts and fuses these attention branches based on the saliency of TF regions, allowing the model to dynamically reallocate its focus according to the characteristics of the degradation. This enables the effective localization and amplification of subtle forgery traces. Extensive experiments demonstrate that the proposed framework consistently outperforms state-of-the-art baselines across various real-world communication degradation scenarios, including six speech codecs and five levels of packet losses. In addition, comparative analysis reveals that the MGAA-enhanced features significantly improve separability between real and fake audio classes and sharpen decision boundaries. These results highlight the robustness and practical deployment potential of our framework in real-world communication environments. </p>
<blockquote>
<p>随着高度逼真的合成语音的兴起，音频通信面临着越来越大的威胁。尽管现有的音频深度伪造检测（ADD）方法在无噪声条件下表现出良好的性能，但在真实通信环境中的数据包丢失和语音编解码器压缩等退化条件下，其有效性会显著降低。在这项工作中，我们提出了首个针对此类退化情况下稳健的ADD的统一框架，该框架旨在有效适应多种时间频率（TF）表示。我们的框架的核心是一种新型的多粒度自适应注意力（MGAA）架构，它采用一组可定制的多尺度注意力头来捕获不同TF粒度下的全局和局部感受野。随后，一种新型自适应融合机制根据TF区域的显著性调整并融合这些注意力分支，使模型能够根据退化的特性动态重新分配其焦点。这能够实现细微伪造痕迹的有效定位和放大。大量实验表明，所提出的框架在各种真实通信退化场景下始终优于最先进的基线，包括六种语音编解码器和五个级别数据包丢失的场景。此外，对比分析表明，使用MGAA增强的特征显著提高了真实和伪造音频类别之间的可分离性，并明确了决策边界。这些结果凸显了我们的框架在真实通信环境中的稳健性和实际部署潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01467v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注合成语音的崛起对音频通信带来的威胁。现有音频深度伪造检测（ADD）方法在实际通信环境中存在局限性。为此，本文提出首个统一框架，该框架旨在适应多种时间频率（TF）表示，并引入多粒度自适应注意力（MGAA）架构，有效应对多种通信环境中的退化问题。实验证明，该框架在各种真实通信退化场景中表现优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>合成语音的崛起对音频通信构成威胁。</li>
<li>现有ADD方法在真实通信环境下性能下降。</li>
<li>提出首个统一框架，适应多种TF表示。</li>
<li>引入MGAA架构，捕捉全局和局部感受野。</li>
<li>自适应融合机制根据TF区域的显著性调整并融合注意力分支。</li>
<li>框架在多种真实通信退化场景中表现优越。</li>
<li>MGAA增强特征提高了真实和伪造音频之间的可分性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01467">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ad23a9ae2abb044eb2192b35165979ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c46503d4d8f4388b255abe79f88bcb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab2c980953deaf6adf58ee38ed9c9e53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75b81d97846fdb778196b0f366ea0548.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31c2640ee0974b54cd583b7ea94779ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-780214afc6ae4e109b2035a74c5adada.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Hearing-More-with-Less-Multi-Modal-Retrieval-and-Selection-Augmented-Conversational-LLM-Based-ASR"><a href="#Hearing-More-with-Less-Multi-Modal-Retrieval-and-Selection-Augmented-Conversational-LLM-Based-ASR" class="headerlink" title="Hearing More with Less: Multi-Modal Retrieval-and-Selection Augmented   Conversational LLM-Based ASR"></a>Hearing More with Less: Multi-Modal Retrieval-and-Selection Augmented   Conversational LLM-Based ASR</h2><p><strong>Authors:Bingshen Mu, Hexin Liu, Hongfei Xue, Kun Wei, Lei Xie</strong></p>
<p>Automatic Speech Recognition (ASR) aims to convert human speech content into corresponding text. In conversational scenarios, effectively utilizing context can enhance its accuracy. Large Language Models’ (LLMs) exceptional long-context understanding and reasoning abilities enable LLM-based ASR (LLM-ASR) to leverage historical context for recognizing conversational speech, which has a high degree of contextual relevance. However, existing conversational LLM-ASR methods use a fixed number of preceding utterances or the entire conversation history as context, resulting in significant ASR confusion and computational costs due to massive irrelevant and redundant information. This paper proposes a multi-modal retrieval-and-selection method named MARS that augments conversational LLM-ASR by enabling it to retrieve and select the most relevant acoustic and textual historical context for the current utterance. Specifically, multi-modal retrieval obtains a set of candidate historical contexts, each exhibiting high acoustic or textual similarity to the current utterance. Multi-modal selection calculates the acoustic and textual similarities for each retrieved candidate historical context and, by employing our proposed near-ideal ranking method to consider both similarities, selects the best historical context. Evaluations on the Interspeech 2025 Multilingual Conversational Speech Language Model Challenge dataset show that the LLM-ASR, when trained on only 1.5K hours of data and equipped with the MARS, outperforms the state-of-the-art top-ranking system trained on 179K hours of data. </p>
<blockquote>
<p>自动语音识别（ASR）旨在将人类语音内容转换为相应的文本。在对话场景中，有效利用上下文可以提高其准确性。大型语言模型（LLM）出色的长文本理解和推理能力，使得基于LLM的ASR（LLM-ASR）能够利用历史上下文来识别对话语音，这对高度相关的上下文具有重要意义。然而，现有的对话式LLM-ASR方法使用固定数量的前面的话语或整个对话历史作为上下文，这会导致大量的无关和冗余信息，从而引发显著的ASR混淆和计算成本。本文提出了一种名为MARS的多模态检索与选择方法，它通过增强对话式LLM-ASR的能力，使其能够检索并选择当前话语中最相关的音频和文本历史上下文，从而扩展了对话式LLM-ASR的功能。具体来说，多模态检索获得一组候选历史上下文，每个上下文在声音或文本上与当前话语表现出高度相似性。多模态选择会计算每个检索到的候选历史上下文的音频和文本相似性，并利用我们提出的接近理想排序方法来考虑这两个相似性，选择最佳历史上下文。在InterSpeech 2025多语种对话语音语言模型挑战赛数据集上的评估表明，仅使用1.5K小时数据训练的LLM-ASR，配备MARS后，其性能优于使用17.9万小时数据训练的最新顶尖系统。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01166v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型的自动语音识别（ASR）技术利用对话历史上下文来提高识别准确性。现有方法存在固定使用若干先前话语或整个对话历史作为上下文的问题，导致ASR混淆和计算成本高昂。本文提出了一种名为MARS的多模态检索与选择方法，能够检索并选择当前话语最相关的声音和文字历史上下文。在Interspeech 2025多语种对话语音语言模型挑战赛数据集上的评估显示，使用MARS的LLM-ASR性能超越了基于大量数据的先进排名系统。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在ASR中具有出色的长期上下文理解和推理能力。</li>
<li>现有ASR方法在利用对话历史上下文时存在冗余信息和计算成本问题。</li>
<li>MARS方法通过多模态检索与选择增强LLM-ASR性能。</li>
<li>MARS能够检索与当前话语最相关的声音和文字历史上下文。</li>
<li>使用MARS的LLM-ASR在少量数据训练下性能超越大量数据训练的先进系统。</li>
<li>MARS方法通过平衡声音和文字的相似性，提高了ASR的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01166">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-39f2b5d2a756260c18f739f8fced3607.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd15bd7b01fe022378c664a43339df17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b84c5561a0f21176e123debd317380bf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9fd97d921c09ce8588c29e7cb6d626d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-606873d63a2b23067cd656c9e6b8ee1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cfbac33f568fb9caaa620737fc2ffb04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd4007d9b77ad6c1ad8d5c20a1553a8f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Fusion-of-Modulation-Spectrogram-and-SSL-with-Multi-head-Attention-for-Fake-Speech-Detection"><a href="#Fusion-of-Modulation-Spectrogram-and-SSL-with-Multi-head-Attention-for-Fake-Speech-Detection" class="headerlink" title="Fusion of Modulation Spectrogram and SSL with Multi-head Attention for   Fake Speech Detection"></a>Fusion of Modulation Spectrogram and SSL with Multi-head Attention for   Fake Speech Detection</h2><p><strong>Authors:Rishith Sadashiv T N, Abhishek Bedge, Saisha Suresh Bore, Jagabandhu Mishra, Mrinmoy Bhattacharjee, S R Mahadeva Prasanna</strong></p>
<p>Fake speech detection systems have become a necessity to combat against speech deepfakes. Current systems exhibit poor generalizability on out-of-domain speech samples due to lack to diverse training data. In this paper, we attempt to address domain generalization issue by proposing a novel speech representation using self-supervised (SSL) speech embeddings and the Modulation Spectrogram (MS) feature. A fusion strategy is used to combine both speech representations to introduce a new front-end for the classification task. The proposed SSL+MS fusion representation is passed to the AASIST back-end network. Experiments are conducted on monolingual and multilingual fake speech datasets to evaluate the efficacy of the proposed model architecture in cross-dataset and multilingual cases. The proposed model achieves a relative performance improvement of 37% and 20% on the ASVspoof 2019 and MLAAD datasets, respectively, in in-domain settings compared to the baseline. In the out-of-domain scenario, the model trained on ASVspoof 2019 shows a 36% relative improvement when evaluated on the MLAAD dataset. Across all evaluated languages, the proposed model consistently outperforms the baseline, indicating enhanced domain generalization. </p>
<blockquote>
<p>假语音检测系统在应对语音深度伪造时成为了必要工具。当前系统由于缺少多样化的训练数据，在域外语音样本上的通用性较差。在本文中，我们试图通过提出一种新型语音表现来解决领域通用性问题，该表现使用自监督（SSL）语音嵌入和调制光谱（MS）特征。我们采用融合策略结合这两种语音表现，为分类任务引入新的前端。提出的SSL+MS融合表现传递给AASIST后端网络。实验在单语种和多语种假语音数据集上进行，以评估所提模型架构在跨数据集和多语种情况下的有效性。与基线相比，在所提议的模型在域内设置下，在ASVspoof 2019和MLAAD数据集上分别实现了相对性能提升37%和20%。在域外场景下，该模型在ASVspoof 2019数据集上进行训练，并在MLAAD数据集上评估时显示出相对改善了36%。在所评估的所有语言中，所提出模型的性能始终优于基线，显示出增强的领域泛化能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01034v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>针对语音深度伪造带来的问题，假语音检测系统的研究变得至关重要。当前系统因缺乏多样化训练数据而在跨域语音样本上表现通用性不足。本文试图通过提出一种新型的语音表现方式来解决域泛化问题，该方式结合了自监督（SSL）语音嵌入和调制谱（MS）特征。通过融合策略结合这两种语音表现方式，为分类任务引入新的前端。所提出的SSL+MS融合表现传递给AASIST后端网络。在单语种和多语种假语音数据集上进行的实验，验证了所提出模型架构在跨数据集和多语种情况下的有效性。与基线相比，所提出模型在ASVspoof 2019和MLAAD数据集上的域内设置分别实现了37%和20%的相对性能提升。在跨域场景中，基于ASVspoof 2019训练的模型在MLAAD数据集上显示了36%的相对改进。在所有评估的语言中，所提出模型始终优于基线，显示出增强的域泛化能力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>假语音检测系统因训练数据缺乏多样性而在跨域语音样本上表现出有限的通用性。</li>
<li>提出了一种新的语音表现方式，结合自监督（SSL）语音嵌入和调制谱（MS）特征，以提高模型的泛化能力。</li>
<li>通过融合策略，将SSL和MS特征结合，为分类任务设计新型前端。</li>
<li>所提出的模型架构在单语种和多语种假语音数据集上进行实验验证，表现出优良的有效性。</li>
<li>在域内设置下，相对于基线，所提出模型在ASVspoof 2019和MLAAD数据集上实现了显著的性能提升。</li>
<li>在跨域场景下，模型在MLAAD数据集上的性能有显著改善。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01034">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-20999ec8286979082ad2f4b299879b29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2275e8522857fde7e2801522cc5d153.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-406e127c1476f8596e5aa936ddd1eb36.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f838a1f256efd42a08f80f84afa34416.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ac75531b7f381b856a6e81d91eae245.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AudioGen-Omni-A-Unified-Multimodal-Diffusion-Transformer-for-Video-Synchronized-Audio-Speech-and-Song-Generation"><a href="#AudioGen-Omni-A-Unified-Multimodal-Diffusion-Transformer-for-Video-Synchronized-Audio-Speech-and-Song-Generation" class="headerlink" title="AudioGen-Omni: A Unified Multimodal Diffusion Transformer for   Video-Synchronized Audio, Speech, and Song Generation"></a>AudioGen-Omni: A Unified Multimodal Diffusion Transformer for   Video-Synchronized Audio, Speech, and Song Generation</h2><p><strong>Authors:Le Wang, Jun Wang, Feng Deng, Chen Zhang, Di Zhang, Kun Gai</strong></p>
<p>We present AudioGen-Omni - a unified approach based on multimodal diffusion transformers (MMDit), capable of generating high-fidelity audio, speech, and songs coherently synchronized with the input video. AudioGen-Omni introduces a novel joint training paradigm that seamlessly integrates large-scale video-text-audio corpora, enabling a model capable of generating semantically rich, acoustically diverse audio conditioned on multimodal inputs and adaptable to a wide range of audio generation tasks. AudioGen-Omni employs a unified lyrics-transcription encoder that encodes graphemes and phonemes from both sung and spoken inputs into dense frame-level representations. Dense frame-level representations are fused using an AdaLN-based joint attention mechanism enhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein RoPE is selectively applied to temporally structured modalities to ensure precise and robust cross-modal alignment. By unfreezing all modalities and masking missing inputs, AudioGen-Omni mitigates the semantic constraints of text-frozen paradigms, enabling effective cross-modal conditioning. This joint training approach enhances audio quality, semantic alignment, and lip-sync accuracy, while also achieving state-of-the-art results on Text-to-Audio&#x2F;Speech&#x2F;Song tasks. With an inference time of 1.91 seconds for 8 seconds of audio, it offers substantial improvements in both efficiency and generality. </p>
<blockquote>
<p>我们提出了AudioGen-Omni——一种基于多模式扩散变压器（MMDit）的统一方法，能够生成与输入视频同步的高保真音频、语音和歌曲。AudioGen-Omni引入了一种新的联合训练范式，无缝集成了大规模的视频-文本-音频语料库，使模型能够在多模式输入条件下生成语义丰富、声音多样的音频，并适应各种音频生成任务。AudioGen-Omni采用统一的歌词转录编码器，将唱词和口语输入中的字母和音素编码成密集的帧级表示。密集的帧级表示通过使用基于AdaLN的联合注意力机制进行融合，增强了相位对齐的异向定位注入（PAAPI），其中RoPE被有选择地应用于具有时间结构的模式，以确保精确和稳定的跨模式对齐。通过解冻所有模式并掩盖缺失的输入，AudioGen-Omni减轻了文本冻结范式的语义约束，实现了有效的跨模式条件。这种联合训练方法提高了音频质量、语义对齐和唇同步精度，同时在文本到音频&#x2F;语音&#x2F;歌曲任务上达到了最先进的成果。其推理时间为1.91秒可生成8秒的音频，在效率和通用性方面都有显著提高。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00733v2">PDF</a> 12 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>基于多模态扩散变压器（MMDit）的AudioGen-Omni统一方法，能够生成与输入视频同步的高保真音频、语音和歌曲。该方法引入了一种新型联合训练范式，无缝集成大规模视频-文本-音频语料库，可生成语义丰富、声音多样的音频，并适应各种音频生成任务。AudioGen-Omni采用统一的歌词-转录编码器，对歌唱和口语输入中的字母和音素进行密集帧级编码表示。通过AdaLN联合注意力机制的融合，结合相位对齐的异构位置注入（PAAPI），将RoPE选择性应用于时间结构化模式，确保精确和稳定的跨模式对齐。解冻所有模式并屏蔽缺失输入，AudioGen-Omni缓解了文本冻结范式的语义约束，实现了有效的跨模式条件。该联合训练方法提高了音频质量、语义对齐和唇同步精度，并在文本到音频&#x2F;语音&#x2F;歌曲任务上实现了最新成果。其推理时间为每生成8秒音频只需约1.91秒，效率和通用性均有显著提高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AudioGen-Omni是一种基于多模态扩散变压器（MMDit）的统一方法，能够生成高保真音频、语音和歌曲，且与输入视频同步。</li>
<li>引入新型联合训练范式，集成视频-文本-音频语料库。</li>
<li>采用统一的歌词-转录编码器，进行密集帧级编码表示。</li>
<li>结合AdaLN联合注意力机制和PAAPI技术，确保精确和稳定的跨模式对齐。</li>
<li>通过解冻所有模式并屏蔽缺失输入，实现有效跨模态条件。</li>
<li>联合训练方法提高了音频质量、语义对齐和唇同步精度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00733">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e87e3a369212016a0417bc74b4c2db03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38e20067b86f9682aa8b7b2428f93191.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a86b4f217474a0d03ffcfc10bb25eabd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6b67d41d4c49685f7d28ce294f9f9f4.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Real-Time-Audio-Visual-Speech-Enhancement-Using-Pre-trained-Visual-Representations"><a href="#Real-Time-Audio-Visual-Speech-Enhancement-Using-Pre-trained-Visual-Representations" class="headerlink" title="Real-Time Audio-Visual Speech Enhancement Using Pre-trained Visual   Representations"></a>Real-Time Audio-Visual Speech Enhancement Using Pre-trained Visual   Representations</h2><p><strong>Authors:T. Aleksandra Ma, Sile Yin, Li-Chia Yang, Shuo Zhang</strong></p>
<p>Speech enhancement in audio-only settings remains challenging, particularly in the presence of interfering speakers. This paper presents a simple yet effective real-time audio-visual speech enhancement (AVSE) system, RAVEN, which isolates and enhances the on-screen target speaker while suppressing interfering speakers and background noise. We investigate how visual embeddings learned from audio-visual speech recognition (AVSR) and active speaker detection (ASD) contribute to AVSE across different SNR conditions and numbers of interfering speakers. Our results show concatenating embeddings from AVSR and ASD models provides the greatest improvement in low-SNR, multi-speaker environments, while AVSR embeddings alone perform best in noise-only scenarios. In addition, we develop a real-time streaming system that operates on a computer CPU and we provide a video demonstration and code repository. To our knowledge, this is the first open-source implementation of a real-time AVSE system. </p>
<blockquote>
<p>在只有音频的环境中，语音增强仍然是一个挑战，特别是在存在干扰说话者的情况下。本文提出了一种简单有效的实时视听语音增强（AVSE）系统，名为RAVEN，该系统可以隔离并增强屏幕上的目标说话者，同时抑制干扰说话者和背景噪声。我们研究了从视听语音识别（AVSR）和主动说话者检测（ASD）中学到的视觉嵌入如何在不同信噪比条件和干扰说话者数量的情况下对AVSE做出贡献。我们的结果表明，在信噪比低、多说话者的环境中，拼接AVSR和ASD模型的嵌入会提供最大的改进，而仅使用AVSR嵌入在只有噪声的场景中表现最佳。此外，我们开发了一个可在计算机CPU上运行的实时流媒体系统，并提供了视频演示和代码仓库。据我们所知，这是第一个实时AVSE系统的开源实现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21448v2">PDF</a> Accepted into Interspeech 2025; corrected author name typo</p>
<p><strong>摘要</strong></p>
<p>该论文提出了一种简单有效的实时视听语音增强（AVSE）系统，名为RAVEN。该系统能够在有干扰说话人的情况下，隔离并增强屏幕上的目标说话人声音，同时抑制干扰说话人和背景噪音。文章探讨了如何从视听语音识别（AVSR）和活动说话人检测（ASD）中学习视觉嵌入，以助力AVSE在不同信噪比条件和干扰说话人数量的场景中的应用。实验结果显示，在信噪比低、存在多名干扰说话人的环境中，融合AVSR和ASD模型的嵌入信息取得的效果最佳；而仅在噪声场景下，AVSR嵌入信息表现最好。此外，论文还开发了一个可在计算机CPU上运行的实时流媒体系统，并提供了视频演示和代码仓库。这是首个开源的实时AVSE系统。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>论文提出了一种实时视听语音增强（AVSE）系统RAVEN，能够在有干扰说话人的情况下增强目标说话人的语音。</li>
<li>RAVEN系统通过隔离和增强目标说话人声音，同时抑制干扰说话人和背景噪音，改善了语音清晰度。</li>
<li>论文探讨了视觉嵌入在AVSE中的作用，特别是从视听语音识别（AVSR）和活动说话人检测（ASD）中学习得到的视觉嵌入。</li>
<li>在不同信噪比条件和干扰说话人数量的场景中，融合AVSR和ASD模型的嵌入信息能够取得最佳性能。</li>
<li>AVSR嵌入信息在仅有噪声的场景中表现最好。</li>
<li>论文开发了一个可在计算机CPU上运行的实时流媒体系统，实现了AVSE的实时应用。</li>
<li>该系统是首个开源的实时AVSE系统，提供了视频演示和代码仓库，便于其他研究者使用和进一步开发。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21448">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4c4d77aa9e018e9a1fb3ab7c1a2d3665.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ccb7a4863ac98099675f2e8d020b1d26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52a76c793b5edeeafae6751fd28e31a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-027184c5e56c67cc641549bf7039f859.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Edge-ASR-Towards-Low-Bit-Quantization-of-Automatic-Speech-Recognition-Models"><a href="#Edge-ASR-Towards-Low-Bit-Quantization-of-Automatic-Speech-Recognition-Models" class="headerlink" title="Edge-ASR: Towards Low-Bit Quantization of Automatic Speech Recognition   Models"></a>Edge-ASR: Towards Low-Bit Quantization of Automatic Speech Recognition   Models</h2><p><strong>Authors:Chen Feng, Yicheng Lin, Shaojie Zhuo, Chenzheng Su, Ramchalam Kinattinkara Ramakrishnan, Zhaocong Yuan, Xiaopeng Zhang</strong></p>
<p>Recent advances in Automatic Speech Recognition (ASR) have demonstrated remarkable accuracy and robustness in diverse audio applications, such as live transcription and voice command processing. However, deploying these models on resource-constrained edge devices (e.g., IoT device, wearables) still presents substantial challenges due to strict limits on memory, compute and power. Quantization, particularly Post-Training Quantization (PTQ), offers an effective way to reduce model size and inference cost without retraining. Despite its importance, the performance implications of various advanced quantization methods and bit-width configurations on ASR models remain unclear. In this work, we present a comprehensive benchmark of eight state-of-the-art (SOTA) PTQ methods applied to two leading edge-ASR model families, Whisper and Moonshine. We systematically evaluate model performances (i.e., accuracy, memory I&#x2F;O and bit operations) across seven diverse datasets from the open ASR leader-board, analyzing the impact of quantization and various configurations on both weights and activations. Built on an extension of the LLM compression toolkit, our framework integrates edge-ASR models, diverse advanced quantization algorithms, a unified calibration and evaluation data pipeline, with detailed analysis tools. Our results characterize the trade-offs between efficiency and accuracy, demonstrating that even $3$-bit quantization can succeed on high capacity models when using advanced PTQ techniques. These findings provide valuable insights for optimizing ASR models on low-power, always-on edge devices. </p>
<blockquote>
<p>近期自动语音识别（ASR）技术的进展在各种音频应用中表现出了显著准确性和稳健性，例如实时转录和语音命令处理。然而，将这些模型部署在资源受限的边缘设备（例如物联网设备和可穿戴设备）上仍然面临重大挑战，这主要是由于内存、计算和功率的严格限制。量化，尤其是训练后量化（PTQ），提供了一种有效的方式来减少模型大小和推理成本而无需重新训练。尽管其很重要，但各种先进的量化方法和位宽配置对ASR模型性能的影响仍不清楚。在这项工作中，我们对应用于两个领先边缘ASR模型家族Whisper和Moonshine的八种最新（SOTA）PTQ方法进行了全面评估。我们系统地评估了模型在七个开放ASR排行榜数据集上的性能（即准确性、内存I&#x2F;O和位操作），分析了量化和各种配置对权重和激活的影响。我们的框架基于LLM压缩工具包的扩展，集成了边缘ASR模型、各种先进量化算法、统一的校准和评估数据流水线以及详细的分析工具。我们的结果描述了效率和准确性之间的权衡，表明使用先进的PTQ技术时，即使3位量化也可以在高容量模型上取得成功。这些发现对于在低功耗、始终开启的边缘设备上优化ASR模型提供了宝贵的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07877v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>近期自动语音识别（ASR）技术的进展在实时转录和语音命令处理等应用中表现出惊人的准确性和稳健性。然而，在资源受限的边缘设备上部署这些模型仍存在挑战，如内存、计算和电源方面的限制。本研究对两款前沿的边缘ASR模型家族Whisper和Moonshine应用八种先进的PTQ量化方法进行了全面评估。通过跨七个开放ASR排行榜数据集的系统评价，研究分析了量化方法和配置对权重和激活的影响。基于LLM压缩工具包的扩展，本研究集成了边缘ASR模型、多种先进的量化算法、统一的校准和评价数据管道以及详细的分析工具。研究结果表明，使用先进的PTQ技术，即使3位量化也能在高容量模型中取得成功。这些发现对于在低功耗、始终开启的边缘设备上优化ASR模型提供了宝贵的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动语音识别（ASR）技术在多种应用中表现出高准确性和稳健性。</li>
<li>在资源受限的边缘设备上部署ASR模型面临挑战，需要量化技术来减小模型大小和推理成本。</li>
<li>研究评估了两种前沿ASR模型家族Whisper和Moonshine，使用八种先进的PTQ量化方法。</li>
<li>研究跨七个数据集系统评价了模型性能，包括准确性、内存I&#x2F;O和位操作。</li>
<li>研究集成了边缘ASR模型、量化算法、校准和评价数据管道及分析工具。</li>
<li>研究表明，使用先进的PTQ技术，甚至3位量化也能在高容量模型中成功应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07877">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3cd6d6960f16845b051fea6548eb18ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2daa949f18e40bff8643453361b8af5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1e75300f6a64ef230f4b74b9297b8985.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8560a893995c249789e42dc370883551.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9a869f5d790b11a806353b7da1d61f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7e6446ea1ed1ad1783c9c1906def3a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f04140017f1979953c06d3a58bba3cb6.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Rectifying-Magnitude-Neglect-in-Linear-Attention"><a href="#Rectifying-Magnitude-Neglect-in-Linear-Attention" class="headerlink" title="Rectifying Magnitude Neglect in Linear Attention"></a>Rectifying Magnitude Neglect in Linear Attention</h2><p><strong>Authors:Qihang Fan, Huaibo Huang, Yuang Ai, Ran He</strong></p>
<p>As the core operator of Transformers, Softmax Attention exhibits excellent global modeling capabilities. However, its quadratic complexity limits its applicability to vision tasks. In contrast, Linear Attention shares a similar formulation with Softmax Attention while achieving linear complexity, enabling efficient global information modeling. Nevertheless, Linear Attention suffers from a significant performance degradation compared to standard Softmax Attention. In this paper, we analyze the underlying causes of this issue based on the formulation of Linear Attention. We find that, unlike Softmax Attention, Linear Attention entirely disregards the magnitude information of the Query. This prevents the attention score distribution from dynamically adapting as the Query scales. As a result, despite its structural similarity to Softmax Attention, Linear Attention exhibits a significantly different attention score distribution. Based on this observation, we propose Magnitude-Aware Linear Attention (MALA), which modifies the computation of Linear Attention to fully incorporate the Query’s magnitude. This adjustment allows MALA to generate an attention score distribution that closely resembles Softmax Attention while exhibiting a more well-balanced structure. We evaluate the effectiveness of MALA on multiple tasks, including image classification, object detection, instance segmentation, semantic segmentation, natural language processing, speech recognition, and image generation. Our MALA achieves strong results on all of these tasks. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/qhfan/MALA">https://github.com/qhfan/MALA</a> </p>
<blockquote>
<p>Transformer的核心操作器Softmax Attention展现出出色的全局建模能力。然而，其二次复杂度限制了其在视觉任务中的应用。相比之下，Linear Attention与Softmax Attention具有相似的公式，但实现了线性复杂度，能够高效地全局信息建模。然而，与标准的Softmax Attention相比，Linear Attention的性能严重下降。在本文中，我们基于Linear Attention的公式分析此问题的根本原因。我们发现，与Softmax Attention不同，Linear Attention完全忽略了Query的幅度信息。这阻止了注意力得分分布随着Query的缩放而动态适应。因此，尽管它与Softmax Attention结构相似，但Linear Attention的注意力得分分布却大不相同。基于此观察，我们提出了幅度感知的Linear Attention（MALA），它修改了Linear Attention的计算以充分融入Query的幅度。这一调整使MALA能够生成与Softmax Attention相似的注意力得分分布，同时展现出更为平衡的结构。我们在多个任务上评估了MALA的有效性，包括图像分类、目标检测、实例分割、语义分割、自然语言处理、语音识别和图像生成。我们的MALA在所有任务上都取得了强大的结果。代码将在<a target="_blank" rel="noopener" href="https://github.com/qhfan/MALA">https://github.com/qhfan/MALA</a>上提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00698v3">PDF</a> Accepted by ICCV2025, highlight paper</p>
<p><strong>Summary</strong></p>
<p>本文探讨了Transformer中的核心操作。Softmax Attention具有出色的全局建模能力，但其二次复杂度限制了其在视觉任务中的应用。相比之下，Linear Attention虽然与Softmax Attention具有类似的公式，但具有线性复杂度，能够实现有效的全局信息建模。然而，其性能较差。基于Linear Attention的公式分析，我们发现Linear Attention忽略了Query的幅度信息，导致注意力得分分布无法随Query的变化而动态调整。因此，我们提出了Magnitude-Aware Linear Attention（MALA），它通过修改Linear Attention的计算来全面融入Query的幅度，生成与Softmax Attention相似的注意力得分分布，并在多个任务上取得了良好效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Softmax Attention在Transformer中具有优秀的全局建模能力，但二次复杂度限制了其在视觉任务中的应用。</li>
<li>Linear Attention虽然具有线性复杂度，但与Softmax Attention相比，其性能有所降低。</li>
<li>Linear Attention忽略了Query的幅度信息，导致注意力得分分布固定，无法随Query的变化而调整。</li>
<li>MALA通过修改Linear Attention的计算，全面融入Query的幅度信息。</li>
<li>MALA生成的注意力得分分布与Softmax Attention相似，具有更平衡的结构。</li>
<li>MALA在多个任务上进行了评估，包括图像分类、对象检测、实例分割、语义分割、自然语言处理、语音识别和图像生成，并取得了良好效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00698">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6b6494763e1e0d31602e9349fcfbacd3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e9ae945a2ce34db84f97b69f636a7ad2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c67263aea0a851211e4b263070b438e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7d0fbfade3a2fae4267e44252804f06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb9855ff5403bcef39fb12ca47885498.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="FedMLAC-Mutual-Learning-Driven-Heterogeneous-Federated-Audio-Classification"><a href="#FedMLAC-Mutual-Learning-Driven-Heterogeneous-Federated-Audio-Classification" class="headerlink" title="FedMLAC: Mutual Learning Driven Heterogeneous Federated Audio   Classification"></a>FedMLAC: Mutual Learning Driven Heterogeneous Federated Audio   Classification</h2><p><strong>Authors:Jun Bai, Rajib Rana, Di Wu, Youyang Qu, Xiaohui Tao, Ji Zhang, Carlos Busso, Shivakumara Palaiahnakote</strong></p>
<p>Federated Learning (FL) offers a privacy-preserving framework for training audio classification (AC) models across decentralized clients without sharing raw data. However, Federated Audio Classification (FedAC) faces three major challenges: data heterogeneity, model heterogeneity, and data poisoning, which degrade performance in real-world settings. While existing methods often address these issues separately, a unified and robust solution remains underexplored. We propose FedMLAC, a mutual learning-based FL framework that tackles all three challenges simultaneously. Each client maintains a personalized local AC model and a lightweight, globally shared Plug-in model. These models interact via bidirectional knowledge distillation, enabling global knowledge sharing while adapting to local data distributions, thus addressing both data and model heterogeneity. To counter data poisoning, we introduce a Layer-wise Pruning Aggregation (LPA) strategy that filters anomalous Plug-in updates based on parameter deviations during aggregation. Extensive experiments on four diverse audio classification benchmarks, including both speech and non-speech tasks, show that FedMLAC consistently outperforms state-of-the-art baselines in classification accuracy and robustness to noisy data. </p>
<blockquote>
<p>联邦学习（FL）为在分散的客户端上训练音频分类（AC）模型提供了一种保护隐私的框架，无需共享原始数据。然而，联邦音频分类（FedAC）面临三大挑战：数据异质性、模型异质性，以及数据毒化，它们在现实世界的设置中会降低性能。虽然现有方法通常分别解决这些问题，但统一且稳健的解决方案仍然缺乏探索。我们提出FedMLAC，一个基于相互学习的联邦学习框架，可以同时应对所有三个挑战。每个客户端维护一个个性化的本地AC模型和一个轻量级的、全局共享的插件模型。这些模型通过双向知识蒸馏进行交互，能够实现全局知识共享，同时适应本地数据分布，从而解决数据和模型异质性问题。为了应对数据毒化，我们引入了一种分层剪枝聚合（LPA）策略，该策略根据聚合过程中的参数偏差来过滤异常的插件更新。在包括语音和非语音任务在内的四个不同的音频分类基准测试上的大量实验表明，FedMLAC在分类准确性和对噪声数据的稳健性方面均优于最新基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10207v2">PDF</a> updated version for the first submission</p>
<p><strong>Summary</strong></p>
<p>联邦学习（FL）为音频分类（AC）模型在分散式客户端上的训练提供了一个隐私保护框架，无需共享原始数据。然而，联邦音频分类（FedAC）面临数据异质性、模型异质性和数据中毒三大挑战，这些挑战在现实世界环境中会降低性能。针对这些问题，我们提出FedMLAC，一个基于互助学习的联邦学习框架，同时解决这三个挑战。每个客户端维护个性化的本地AC模型和轻量级的全局共享插件模型。这些模型通过双向知识蒸馏进行交互，实现全局知识共享，同时适应本地数据分布，从而解决数据和模型异质性。为了应对数据中毒，我们引入了一种分层剪枝聚合（LPA）策略，根据聚合过程中的参数偏差来过滤异常的插件更新。在四个不同的音频分类基准测试上的广泛实验表明，FedMLAC在分类准确性和对噪声数据的鲁棒性方面均优于最新基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>联邦学习（FL）为音频分类模型的训练提供了隐私保护框架。</li>
<li>联邦音频分类（FedAC）面临数据异质性、模型异质性和数据中毒三大挑战。</li>
<li>FedMLAC是一个基于互助学习的联邦学习框架，能够同时解决上述三大挑战。</li>
<li>FedMLAC通过双向知识蒸馏解决数据分布和模型异质性问题。</li>
<li>FedMLAC引入分层剪枝聚合（LPA）策略来应对数据中毒问题。</li>
<li>FedMLAC在分类准确性以及对噪声数据的鲁棒性方面优于最新基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10207">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-53317f5e2d6a4f0aae2d022295dc1b93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73c361d5471a89624c7ec8b5ed062d7d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="GestureLSM-Latent-Shortcut-based-Co-Speech-Gesture-Generation-with-Spatial-Temporal-Modeling"><a href="#GestureLSM-Latent-Shortcut-based-Co-Speech-Gesture-Generation-with-Spatial-Temporal-Modeling" class="headerlink" title="GestureLSM: Latent Shortcut based Co-Speech Gesture Generation with   Spatial-Temporal Modeling"></a>GestureLSM: Latent Shortcut based Co-Speech Gesture Generation with   Spatial-Temporal Modeling</h2><p><strong>Authors:Pinxin Liu, Luchuan Song, Junhua Huang, Haiyang Liu, Chenliang Xu</strong></p>
<p>Generating full-body human gestures based on speech signals remains challenges on quality and speed. Existing approaches model different body regions such as body, legs and hands separately, which fail to capture the spatial interactions between them and result in unnatural and disjointed movements. Additionally, their autoregressive&#x2F;diffusion-based pipelines show slow generation speed due to dozens of inference steps. To address these two challenges, we propose GestureLSM, a flow-matching-based approach for Co-Speech Gesture Generation with spatial-temporal modeling. Our method i) explicitly model the interaction of tokenized body regions through spatial and temporal attention, for generating coherent full-body gestures. ii) introduce the flow matching to enable more efficient sampling by explicitly modeling the latent velocity space. To overcome the suboptimal performance of flow matching baseline, we propose latent shortcut learning and beta distribution time stamp sampling during training to enhance gesture synthesis quality and accelerate inference. Combining the spatial-temporal modeling and improved flow matching-based framework, GestureLSM achieves state-of-the-art performance on BEAT2 while significantly reducing inference time compared to existing methods, highlighting its potential for enhancing digital humans and embodied agents in real-world applications. Project Page: <a target="_blank" rel="noopener" href="https://andypinxinliu.github.io/GestureLSM">https://andypinxinliu.github.io/GestureLSM</a> </p>
<blockquote>
<p>基于语音信号生成全身人体姿态在质量和速度方面仍然存在挑战。现有方法将身体的不同部位（如身体、腿和手）分别建模，无法捕捉它们之间的空间交互，导致动作不自然、不连贯。此外，它们的自回归&#x2F;扩散基管道由于需要数十步推理而显示出缓慢的生成速度。为了解决这两个挑战，我们提出了基于流匹配的协同语音姿态生成方法GestureLSM，并进行了时空建模。我们的方法一）通过空间和时间注意力显式地模拟标记身体部位的交互，以生成连贯的全身姿态。二）引入流匹配，通过显式建模潜在速度空间，使采样更加高效。为了克服流匹配基线性能不佳的问题，我们在训练过程中提出了潜在快捷方式学习和beta分布时间戳采样，以提高姿态合成质量和加速推理。结合时空建模和改进的基于流匹配的框架，GestureLSM在BEAT2上实现了最先进的性能，同时与现有方法相比显著减少了推理时间，凸显其在增强数字人类和实体代理在现实世界应用中的潜力。项目页面：<a target="_blank" rel="noopener" href="https://andypinxinliu.github.io/GestureLSM">https://andypinxinliu.github.io/GestureLSM</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18898v3">PDF</a> Accepted to ICCV 2025. Project Page:   <a target="_blank" rel="noopener" href="https://andypinxinliu.github.io/GestureLSM">https://andypinxinliu.github.io/GestureLSM</a></p>
<p><strong>Summary</strong></p>
<p>基于语音信号生成全身人类动作在质量和速度上仍面临挑战。现有方法分别建模身体、腿和手部等不同的身体区域，忽略了它们之间的空间交互，导致动作不自然、不连贯。此外，它们的自回归&#x2F;扩散生成流程由于需要大量的推理步骤，生成速度较慢。针对这两个问题，我们提出GestureLSM，这是一种基于流匹配的协同语音动作生成方法，具有时空建模功能。我们的方法一）通过空间和时间注意力显式地模拟了标记身体区域的交互，以生成连贯的全身动作。二）引入流匹配，通过显式建模潜在速度空间，使采样更加高效。为了克服流匹配基线方法的性能不佳问题，我们在训练过程中提出了潜在捷径学习和基于beta分布的时间戳采样，以提高动作合成质量和加速推理。结合了时空建模和改进的基于流匹配框架的GestureLSM，在BEAT2上达到了最先进的性能，同时显著减少了推理时间，突显其在增强数字人类和实体代理实际应用中的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有方法模拟不同身体区域导致动作不自然和联合性不足。</li>
<li>现有方法的自回归&#x2F;扩散生成流程导致生成速度较慢。</li>
<li>GestureLSM方法通过空间和时间注意力显式建模身体区域的交互。</li>
<li>GestureLSM引入流匹配以显式建模潜在速度空间并提高采样效率。</li>
<li>通过潜在捷径学习和beta分布时间戳采样改进流匹配以提高动作合成质量和加速推理。</li>
<li>GestureLSM在BEAT2上达到了最先进的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18898">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e000d41130bf5f31bb6c8805ac38f9f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec563ce563158330ef24f16c48e058e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f71aae53c617290800b2299a161b05d7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a24ea0831f0471568ee493e4851abd1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7489ddd80b112f39f0e1a8cb15f9aa8.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7f257b9e64decf58357f02d79cba5777.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-08-06  NS-Net Decoupling CLIP Semantic Information through NULL-Space for   Generalizable AI-Generated Image Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-391d0438101f32b0daf729408f735270.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-08-06  Semi-Supervised Dual-Threshold Contrastive Learning for Ultrasound Image   Classification and Segmentation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29885.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
