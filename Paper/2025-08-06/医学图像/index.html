<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  RL-U$^2$Net A Dual-Branch UNet with Reinforcement Learning-Assisted   Multimodal Feature Fusion for Accurate 3D Whole-Heart Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01668v1/page_4_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-06-æ›´æ–°"><a href="#2025-08-06-æ›´æ–°" class="headerlink" title="2025-08-06 æ›´æ–°"></a>2025-08-06 æ›´æ–°</h1><h2 id="RL-U-2-Net-A-Dual-Branch-UNet-with-Reinforcement-Learning-Assisted-Multimodal-Feature-Fusion-for-Accurate-3D-Whole-Heart-Segmentation"><a href="#RL-U-2-Net-A-Dual-Branch-UNet-with-Reinforcement-Learning-Assisted-Multimodal-Feature-Fusion-for-Accurate-3D-Whole-Heart-Segmentation" class="headerlink" title="RL-U$^2$Net: A Dual-Branch UNet with Reinforcement Learning-Assisted   Multimodal Feature Fusion for Accurate 3D Whole-Heart Segmentation"></a>RL-U$^2$Net: A Dual-Branch UNet with Reinforcement Learning-Assisted   Multimodal Feature Fusion for Accurate 3D Whole-Heart Segmentation</h2><p><strong>Authors:Jierui Qu, Jianchun Zhao</strong></p>
<p>Accurate whole-heart segmentation is a critical component in the precise diagnosis and interventional planning of cardiovascular diseases. Integrating complementary information from modalities such as computed tomography (CT) and magnetic resonance imaging (MRI) can significantly enhance segmentation accuracy and robustness. However, existing multi-modal segmentation methods face several limitations: severe spatial inconsistency between modalities hinders effective feature fusion; fusion strategies are often static and lack adaptability; and the processes of feature alignment and segmentation are decoupled and inefficient. To address these challenges, we propose a dual-branch U-Net architecture enhanced by reinforcement learning for feature alignment, termed RL-U$^2$Net, designed for precise and efficient multi-modal 3D whole-heart segmentation. The model employs a dual-branch U-shaped network to process CT and MRI patches in parallel, and introduces a novel RL-XAlign module between the encoders. The module employs a cross-modal attention mechanism to capture semantic correspondences between modalities and a reinforcement-learning agent learns an optimal rotation strategy that consistently aligns anatomical pose and texture features. The aligned features are then reconstructed through their respective decoders. Finally, an ensemble-learning-based decision module integrates the predictions from individual patches to produce the final segmentation result. Experimental results on the publicly available MM-WHS 2017 dataset demonstrate that the proposed RL-U$^2$Net outperforms existing state-of-the-art methods, achieving Dice coefficients of 93.1% on CT and 87.0% on MRI, thereby validating the effectiveness and superiority of the proposed approach. </p>
<blockquote>
<p>ç²¾ç¡®çš„å…¨å¿ƒåˆ†å‰²æ˜¯å¿ƒè¡€ç®¡ç–¾ç—…çš„ç²¾ç¡®è¯Šæ–­å’Œä»‹å…¥è®¡åˆ’ä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚æ•´åˆè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å’Œç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ç­‰æ¨¡æ€çš„äº’è¡¥ä¿¡æ¯å¯ä»¥æ˜¾è‘—æé«˜åˆ†å‰²çš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤šæ¨¡æ€åˆ†å‰²æ–¹æ³•é¢ä¸´ä¸€äº›å±€é™æ€§ï¼šä¸åŒæ¨¡æ€ä¹‹é—´çš„ç©ºé—´ä¸ä¸€è‡´æ€§ä¸¥é‡é˜»ç¢äº†æœ‰æ•ˆçš„ç‰¹å¾èåˆï¼›èåˆç­–ç•¥é€šå¸¸æ˜¯é™æ€çš„ï¼Œç¼ºä¹é€‚åº”æ€§ï¼›ç‰¹å¾å¯¹é½å’Œåˆ†å‰²çš„è¿‡ç¨‹æ˜¯è§£è€¦çš„ä¸”æ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”±å¼ºåŒ–å­¦ä¹ å¢å¼ºçš„åŒåˆ†æ”¯U-Netæ¶æ„ï¼Œç”¨äºç‰¹å¾å¯¹é½ï¼Œç§°ä¸ºRL-U$^2$Netï¼Œæ—¨åœ¨å®ç°ç²¾ç¡®å’Œé«˜æ•ˆçš„å¤šæ¨¡æ€3Då…¨å¿ƒåˆ†å‰²ã€‚è¯¥æ¨¡å‹é‡‡ç”¨åŒåˆ†æ”¯Uå½¢ç½‘ç»œå¹¶è¡Œå¤„ç†CTå’ŒMRIæ–‘å—ï¼Œå¹¶åœ¨ç¼–ç å™¨ä¹‹é—´å¼•å…¥äº†ä¸€ä¸ªæ–°çš„RL-XAlignæ¨¡å—ã€‚è¯¥æ¨¡å—é‡‡ç”¨è·¨æ¨¡æ€æ³¨æ„æœºåˆ¶æ¥æ•è·æ¨¡æ€ä¹‹é—´çš„è¯­ä¹‰å¯¹åº”å…³ç³»ï¼Œå¼ºåŒ–å­¦ä¹ ä»£ç†å­¦ä¹ æœ€ä¼˜æ—‹è½¬ç­–ç•¥ï¼Œä»¥ä¸€è‡´åœ°å¯¹é½è§£å‰–å§¿åŠ¿å’Œçº¹ç†ç‰¹å¾ã€‚å¯¹é½çš„ç‰¹å¾ç„¶åé€šè¿‡å„è‡ªçš„è§£ç å™¨è¿›è¡Œé‡å»ºã€‚æœ€åï¼ŒåŸºäºé›†æˆå­¦ä¹ çš„å†³ç­–æ¨¡å—å°†å„ä¸ªæ–‘å—çš„é¢„æµ‹ç»“æœæ•´åˆèµ·æ¥ï¼Œç”Ÿæˆæœ€ç»ˆçš„åˆ†å‰²ç»“æœã€‚åœ¨å…¬å¼€çš„MM-WHS 2017æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„RL-U$^2$Netä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒCTä¸Šçš„Diceç³»æ•°ä¸º93.1%ï¼ŒMRIä¸Šçš„Diceç³»æ•°ä¸º87.0%ï¼Œä»è€ŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02557v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    æå‡ºä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å¤šæ¨¡æ€ä¸‰ç»´å¿ƒè„åˆ†å‰²æ–¹æ³•ï¼Œé‡‡ç”¨åŒåˆ†æ”¯U-Netæ¶æ„ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ è¿›è¡Œç‰¹å¾å¯¹é½ã€‚æ–¹æ³•åˆ©ç”¨CTå’ŒMRIæ•°æ®ï¼Œé€šè¿‡äº¤å‰æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶å’Œå¼ºåŒ–å­¦ä¹ å®ç°ç²¾å‡†å’Œé«˜æ•ˆçš„å¤šæ¨¡æ€å¿ƒè„åˆ†å‰²ã€‚åœ¨å…¬å¼€æ•°æ®é›†MM-WHS 2017ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ŒCTå’ŒMRIçš„Diceç³»æ•°åˆ†åˆ«ä¸º93.1%å’Œ87.0%ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€ä¿¡æ¯é›†æˆå¯¹äºæé«˜å¿ƒè¡€ç®¡ç–¾ç—…çš„è¯Šæ–­å’Œæ²»ç–—ç²¾åº¦è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰å¤šæ¨¡æ€åˆ†å‰²æ–¹æ³•å­˜åœ¨ç©ºé—´ä¸ä¸€è‡´æ€§ã€èåˆç­–ç•¥é™æ€ç¼ºä¹é€‚åº”æ€§ä»¥åŠç‰¹å¾å¯¹é½å’Œåˆ†å‰²è¿‡ç¨‹è§£è€¦ç­‰é—®é¢˜ã€‚</li>
<li>æå‡ºçš„RL-U$^2$Netæ¨¡å‹é‡‡ç”¨åŒåˆ†æ”¯U-Netæ¶æ„å¤„ç†CTå’ŒMRIå›¾åƒï¼Œå¹¶è¡Œå¤„ç†ä¸åŒæ¨¡æ€çš„å›¾åƒæ•°æ®ã€‚</li>
<li>RL-XAlignæ¨¡å—åˆ©ç”¨äº¤å‰æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶æ•æ‰ä¸åŒæ¨¡æ€ä¹‹é—´çš„è¯­ä¹‰å¯¹åº”å…³ç³»ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä»£ç†å­¦ä¹ æœ€ä¼˜æ—‹è½¬ç­–ç•¥ï¼Œå®ç°è§£å‰–å§¿åŠ¿å’Œçº¹ç†ç‰¹å¾çš„ä¸€è‡´å¯¹é½ã€‚</li>
<li>å¯¹é½çš„ç‰¹å¾é€šè¿‡å„è‡ªçš„è§£ç å™¨è¿›è¡Œé‡å»ºï¼Œå¹¶é€šè¿‡é›†æˆå­¦ä¹ å†³ç­–æ¨¡å—ç”Ÿæˆæœ€ç»ˆçš„åˆ†å‰²ç»“æœã€‚</li>
<li>åœ¨MM-WHS 2017æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å®ç°äº†é«˜åˆ†å‰²æ€§èƒ½ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02557">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02557v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02557v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02557v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02557v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Comprehensive-Analysis-of-X-ray-Sources-in-Terzan-5-Using-Chandra-Observations"><a href="#A-Comprehensive-Analysis-of-X-ray-Sources-in-Terzan-5-Using-Chandra-Observations" class="headerlink" title="A Comprehensive Analysis of X-ray Sources in Terzan 5 Using Chandra   Observations"></a>A Comprehensive Analysis of X-ray Sources in Terzan 5 Using Chandra   Observations</h2><p><strong>Authors:Gourav Kumawat, Craig O. Heinke, Jiaqi Zhao, Arash Bahramian, Haldan N. Cohn, Phyllis M. Lugger</strong></p>
<p>We analyze photometry, spectra, and variability of over 100 faint X-ray sources in the globular cluster Terzan 5, using 737 ks of Chandra data. X-ray colors and spectral fitting allow clear separation of foreground sources (with less extinction than the cluster), quiescent low-mass X-ray binaries (qLMXBs), and sources with harder spectra. We identify 22 candidate qLMXBs, over twice that found in any other cluster. This is consistent with Terzan 5â€™s stellar interaction rate, the highest among Galactic globular clusters. We do not see qLMXBs dominated by thermal emission below $L_X\sim10^{32}$ erg&#x2F;s, though qLMXBs with stronger nonthermal emission could be missed. We find that more than 50 % of the qLMXB sources have neutron star thermal component contributing over 80 % of the total luminosity. We report an unusual spectral feature around 1.75 keV in the combined spectrum of Ter 5 X-3. The concentration of the qLMXBs within the cluster is consistent with that of a population of mass $1.46 \pm 0.14$ M$_\odot$. We identify secure X-ray counterparts to millisecond pulsars Terzan 5 ar and Terzan 5 at, using positional coincidence and orbital X-ray light curves matching those expected for spider pulsars. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹çƒçŠ¶æ˜Ÿå›¢Terzan 5ä¸­çš„100å¤šä¸ªæš—Xå°„çº¿æºçš„å…‰åº¦å­¦ã€å…‰è°±å’Œå¯å˜æ€§è¿›è¡Œäº†åˆ†æï¼Œä½¿ç”¨äº†é•¿è¾¾737 ksçš„Chandraæ•°æ®ã€‚é€šè¿‡Xå°„çº¿é¢œè‰²å’Œå…‰è°±æ‹Ÿåˆï¼Œå¯ä»¥æ¸…æ¥šåœ°åˆ†è¾¨å‡ºå‰æ™¯æºï¼ˆæ¯”æ˜Ÿå›¢ä¸­çš„é®æŒ¡æ›´å°‘ï¼‰ã€é™æ€ä½è´¨é‡Xå°„çº¿åŒæ˜Ÿï¼ˆqLMXBsï¼‰å’Œå…‰è°±è¾ƒç¡¬çš„æºã€‚æˆ‘ä»¬ç¡®å®šäº†22ä¸ªå€™é€‰çš„qLMXBsï¼Œæ•°é‡æ˜¯å…¶ä»–ä»»ä½•æ˜Ÿå›¢çš„ä¸¤å€ã€‚è¿™ä¸Terzan 5çš„æ’æ˜Ÿäº¤äº’ç‡ç›¸ç¬¦ï¼Œå®ƒæ˜¯é“¶æ²³ç³»çƒçŠ¶æ˜Ÿå›¢ä¸­æœ€é«˜çš„ã€‚æˆ‘ä»¬æ²¡æœ‰çœ‹åˆ°çƒ­å‘å°„ä¸»å¯¼çš„qLMXBsåœ¨Lxçº¦å°äº~ 10^32erg&#x2F;sçš„æƒ…å†µä¸‹æ˜æ˜¾å­˜åœ¨ï¼Œå°½ç®¡éçƒ­å‘å°„è¾ƒå¼ºçš„qLMXBså¯èƒ½æœ‰æ‰€ç¼ºå¤±ã€‚æˆ‘ä»¬å‘ç°è¶…è¿‡ä¸€åŠçš„qLMXBæºçš„ä¸­å­æ˜Ÿçƒ­æˆåˆ†è´¡çŒ®äº†è¶…è¿‡æ€»å…‰åº¦çš„80%ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†åœ¨Ter 5 X-3çš„ç»¼åˆå…‰è°±ä¸­å‡ºç°äº†ç½•è§çš„çº¦1.75 keVç‰¹å¾è°±ã€‚qLMXBsåœ¨æ˜Ÿå›¢å†…çš„åˆ†å¸ƒä¸è´¨é‡ä¸ºåœ°çƒè´¨é‡çš„ä¸€è‡´ã€‚é€šè¿‡å®šä½å»åˆåº¦å’Œè½¨é“Xå°„çº¿å…‰å˜æ›²çº¿åŒ¹é…çš„æ–¹å¼ï¼Œæˆ‘ä»¬ç¡®å®šäº†æ¯«ç§’è„‰å†²æ˜ŸTerzan 5 arå’ŒTerzan 5 atçš„å¯é çš„Xå°„çº¿å¯¹åº”ç‰©ï¼Œè¿™ä¸èœ˜è››è„‰å†²æ˜Ÿçš„é¢„æœŸç›¸ç¬¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02526v1">PDF</a> Accepted for publication in The Astrophysical Journal (ApJ). 24   pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºé•¿è¾¾737åƒç§’çš„é’±å¾·æ‹‰æ•°æ®ï¼Œæˆ‘ä»¬å¯¹å¤©é¹…åº§5å·çƒçŠ¶æ˜Ÿå›¢ä¸­çš„è¶…è¿‡ä¸€ç™¾ä¸ªå¾®å¼±çš„Xå°„çº¿æºè¿›è¡Œäº†å…‰åº¦åˆ†æã€å…‰è°±åˆ†æå’Œå˜åŒ–æ€§åˆ†æã€‚é€šè¿‡Xå°„çº¿é¢œè‰²å’Œå…‰è°±æ‹Ÿåˆï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ¸…æ¥šåœ°åˆ†è¾¨å‡ºå‰æ™¯æºï¼ˆæ˜Ÿå›¢æ¶ˆå…‰è¾ƒå°‘çš„æºï¼‰ã€é™æ€ä½è´¨é‡Xå°„çº¿åŒæ˜Ÿï¼ˆqLMXBsï¼‰ä»¥åŠå…‰è°±æ›´ç¡¬çš„æºã€‚æˆ‘ä»¬å‘ç°è¯¥æ˜Ÿå›¢æ‹¥æœ‰é«˜è¾¾22ä¸ªqLMXBså€™é€‰ä½“ï¼Œè¿™ä¸€æ•°é‡æ˜¯å…¶å®ƒæ˜Ÿå›¢çš„ä¸¤å€ï¼Œä¸å¤©é¹…åº§5å·çƒçŠ¶æ˜Ÿå›¢å†…æ’æ˜Ÿç›¸äº’ä½œç”¨ç‡æœ€é«˜çš„æƒ…å†µç›¸ç¬¦ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸­å­æ˜Ÿçƒ­æˆåˆ†å¯¹è¶…è¿‡ä¸€åŠçš„qLMXBæºçš„è´¡çŒ®è¶…è¿‡æ€»å‘å…‰åº¦çš„ç™¾åˆ†ä¹‹å…«åä»¥ä¸Šã€‚è¿˜å‘ç°äº†ä¸€äº›ç‰¹æ®Šè°±ç‰¹å¾ã€‚å¦å¤–æˆ‘ä»¬ä¹Ÿç¡®å®šäº†æ¯«ç§’è„‰å†²æ˜Ÿçš„å¯é Xå°„çº¿å¯¹åº”ä½“ã€‚ç»¼åˆæ¥çœ‹ï¼Œæœ¬é¡¹ç ”ç©¶æ·±åŒ–äº†å¯¹å¤©é¹…åº§5å·çƒçŠ¶æ˜Ÿå›¢ä¸­Xå°„çº¿æºçš„äº†è§£ã€‚å°½ç®¡æŸäº›ç ”ç©¶æ¨æµ‹å­˜åœ¨ä¸€äº›ç–æ¼ä¹‹å¤„ï¼Œæ¯”å¦‚å¯¹è¾ƒå¼±çš„çƒ­å‘å°„å‹qLMXBsçš„é—æ¼ä»¥åŠå¯¹ç‰¹æ®Šè°±ç‰¹å¾çš„æ·±å…¥æ¢ç©¶ç­‰ï¼Œä½†æœ¬é¡¹ç ”ç©¶ä»ä¸ºæˆ‘ä»¬æä¾›äº†å®è´µçš„è§‚æµ‹æ•°æ®å’Œæ–°çš„è®¤è¯†ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°è¿™ä¸€å‘ç°å¯¹ç ”ç©¶çƒçŠ¶æ˜Ÿå›¢å†…æ’æ˜Ÿæ¼”åŒ–åŠç›¸äº’ä½œç”¨æœºåˆ¶å…·æœ‰é‡å¤§æ„ä¹‰ã€‚å¯¹äºæœªæ¥çš„ç ”ç©¶è€Œè¨€ï¼Œè¿™æœ‰åŠ©äºæˆ‘ä»¬æ›´æ·±å…¥åœ°ç†è§£çƒçŠ¶æ˜Ÿå›¢å†…éƒ¨å¤æ‚çš„å¤©ä½“ç‰©ç†è¿‡ç¨‹ä»¥åŠæ’æ˜Ÿå½¢æˆå’Œæ¼”åŒ–çš„ç»†èŠ‚ã€‚è¿™ä¹Ÿå¯èƒ½å¯¹å¼€å‘æ›´æœ‰æ•ˆçš„ç©ºé—´æ¢æµ‹æŠ€æœ¯å’Œæ•°æ®è§£ææ–¹æ³•æœ‰æ‰€å¯ç¤ºã€‚å¯¹äºè¿›ä¸€æ­¥çš„æ¢ç´¢å’Œåº”ç”¨è¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æœŸå¾…æœªæ¥çš„ç ”ç©¶èƒ½å¤Ÿå¸¦æ¥æ›´å¤šçš„çªç ´å’Œæ–°çš„å‘ç°ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™é¡¹ç ”ç©¶ä¸ºæˆ‘ä»¬æä¾›äº†å…³äºå¤©é¹…åº§äº”å·çƒçŠ¶æ˜Ÿå›¢ä¸­Xå°„çº¿æºçš„ä¸°å¯Œè€Œæ·±åˆ»çš„ä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹å¤©é¹…åº§5å·çƒçŠ¶æ˜Ÿå›¢å†…çš„è¶…è¿‡ä¸€ç™¾ä¸ªå¾®å¼±Xå°„çº¿æºè¿›è¡Œäº†è¯¦ç»†åˆ†æã€‚</li>
<li>é€šè¿‡Xå°„çº¿é¢œè‰²å’Œå…‰è°±æ‹ŸåˆæˆåŠŸåŒºåˆ†äº†å‰æ™¯æºã€é™æ€ä½è´¨é‡Xå°„çº¿åŒæ˜Ÿå’Œå…¶ä»–ç±»å‹çš„æºã€‚</li>
<li>å‘ç°è¯¥æ˜Ÿå›¢æ‹¥æœ‰å¤§é‡é™æ€ä½è´¨é‡Xå°„çº¿åŒæ˜Ÿå€™é€‰ä½“ï¼Œæ•°é‡æ˜¯å…¶ä»–æ˜Ÿå›¢çš„ä¸¤å€ã€‚</li>
<li>è§‚å¯Ÿåˆ°ä¸­å­æ˜Ÿçƒ­æˆåˆ†åœ¨å¤šæ•°é™æ€ä½è´¨é‡Xå°„çº¿åŒæ˜Ÿä¸­çš„é‡è¦ä½œç”¨ã€‚</li>
<li>å‘ç°ç‰¹æ®Šè°±ç‰¹å¾å’Œç½•è§çš„Xå°„çº¿å¯¹åº”ä½“ã€‚</li>
<li>ç ”ç©¶ç»“æœæ·±åŒ–äº†å¯¹å¤©é¹…åº§5å·çƒçŠ¶æ˜Ÿå›¢å†…Xå°„çº¿æºçš„äº†è§£ï¼Œå¯¹æ’æ˜Ÿæ¼”åŒ–åŠç›¸äº’ä½œç”¨æœºåˆ¶çš„ç ”ç©¶æœ‰é‡å¤§æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02526">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02526v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02526v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02526v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02526v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Accurate-and-Interpretable-Postmenstrual-Age-Prediction-via-Multimodal-Large-Language-Model"><a href="#Accurate-and-Interpretable-Postmenstrual-Age-Prediction-via-Multimodal-Large-Language-Model" class="headerlink" title="Accurate and Interpretable Postmenstrual Age Prediction via Multimodal   Large Language Model"></a>Accurate and Interpretable Postmenstrual Age Prediction via Multimodal   Large Language Model</h2><p><strong>Authors:Qifan Chen, Jin Cui, Cindy Duan, Yushuo Han, Yifei Shi</strong></p>
<p>Accurate estimation of postmenstrual age (PMA) at scan is crucial for assessing neonatal development and health. While deep learning models have achieved high accuracy in predicting PMA from brain MRI, they often function as black boxes, offering limited transparency and interpretability in clinical decision support. In this work, we address the dual challenge of accuracy and interpretability by adapting a multimodal large language model (MLLM) to perform both precise PMA prediction and clinically relevant explanation generation. We introduce a parameter-efficient fine-tuning (PEFT) strategy using instruction tuning and Low-Rank Adaptation (LoRA) applied to the Qwen2.5-VL-7B model. The model is trained on four 2D cortical surface projection maps derived from neonatal MRI scans. By employing distinct prompts for training and inference, our approach enables the MLLM to handle a regression task during training and generate clinically relevant explanations during inference. The fine-tuned model achieves a low prediction error with a 95 percent confidence interval of 0.78 to 1.52 weeks, while producing interpretable outputs grounded in developmental features, marking a significant step toward transparent and trustworthy AI systems in perinatal neuroscience. </p>
<blockquote>
<p>ç²¾ç¡®ä¼°è®¡æ‰«ææ—¶çš„èƒé¾„ï¼ˆPMAï¼‰å¯¹äºè¯„ä¼°æ–°ç”Ÿå„¿å‘è‚²å’Œå¥åº·è‡³å…³é‡è¦ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨é€šè¿‡è„‘éƒ¨MRIé¢„æµ‹èƒé¾„æ–¹é¢å·²ç»è¾¾åˆ°äº†å¾ˆé«˜çš„å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬é€šå¸¸åƒé»‘ç®±ä¸€æ ·è¿ä½œï¼Œåœ¨ä¸´åºŠå†³ç­–æ”¯æŒä¸­æä¾›æœ‰é™çš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡é€‚åº”å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥è§£å†³å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§çš„åŒé‡æŒ‘æˆ˜ï¼Œä»¥æ‰§è¡Œç²¾ç¡®çš„èƒé¾„é¢„æµ‹å’Œç›¸å…³ä¸´åºŠè§£é‡Šç”Ÿæˆã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ä½¿ç”¨æŒ‡ä»¤è°ƒæ•´å’ŒLow-Rank Adaptationï¼ˆLoRAï¼‰åº”ç”¨äºQwen2.5-VL-7Bæ¨¡å‹çš„å‚æ•°æœ‰æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ç­–ç•¥ã€‚è¯¥æ¨¡å‹åœ¨æ–°ç”Ÿå„¿MRIæ‰«ææ´¾ç”Ÿçš„å››ä¸ªäºŒç»´çš®å±‚è¡¨é¢æŠ•å½±å›¾ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚é€šè¿‡ä¸ºè®­ç»ƒå’Œæ¨ç†é‡‡ç”¨ä¸åŒçš„æç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿MLLMèƒ½å¤Ÿåœ¨è®­ç»ƒæœŸé—´å¤„ç†å›å½’ä»»åŠ¡ï¼Œå¹¶åœ¨æ¨ç†æœŸé—´ç”Ÿæˆä¸ä¸´åºŠç›¸å…³çš„è§£é‡Šã€‚ç»è¿‡å¾®è°ƒçš„æ¨¡å‹å®ç°äº†è¾ƒä½çš„é¢„æµ‹è¯¯å·®ï¼Œ95ï¼…çš„ç½®ä¿¡åŒºé—´ä¸º0.78è‡³1.52å‘¨ï¼ŒåŒæ—¶äº§ç”ŸåŸºäºå‘è‚²ç‰¹å¾çš„å¯è§£é‡Šè¾“å‡ºï¼Œæ ‡å¿—ç€æœç€é€æ˜å’Œå¯é çš„å›´ç”ŸæœŸç¥ç»ç§‘å­¦äººå·¥æ™ºèƒ½ç³»ç»Ÿè¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02525v1">PDF</a> Submitted to the NeurIPS 2025 Workshop GenAI4Health. Conference   website: <a target="_blank" rel="noopener" href="https://aihealth.ischool.utexas.edu/GenAI4HealthNeurips2025/">https://aihealth.ischool.utexas.edu/GenAI4HealthNeurips2025/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰å¯¹æ–°ç”Ÿå„¿è„‘MRIæ‰«æè¿›è¡Œç²¾ç¡®çš„åæ‰«æå¹´é¾„ï¼ˆPMAï¼‰é¢„æµ‹ï¼Œå¹¶ç”Ÿæˆä¸´åºŠç›¸å…³è§£é‡Šçš„æ–¹æ³•ã€‚è¯¥ç ”ç©¶é€šè¿‡å‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼ˆPEFTï¼‰ç­–ç•¥å’Œä½ç§©é€‚é…ï¼ˆLoRAï¼‰æŠ€æœ¯å®ç°äº†æ¨¡å‹çš„åº”ç”¨ï¼Œåœ¨æ–°ç”Ÿå„¿MRIæ‰«æçš„å››ä¸ªäºŒç»´çš®å±‚è¡¨é¢æŠ•å½±å›¾ä¸Šè®­ç»ƒæ¨¡å‹ã€‚è¯¥ç ”ç©¶ç»“åˆäº†è®­ç»ƒæ—¶çš„å›å½’ä»»åŠ¡å’Œæ¨ç†æ—¶çš„è§£é‡Šç”Ÿæˆï¼Œå®ç°äº†æ¨¡å‹çš„ç²¾ç»†è°ƒæ•´ï¼Œé¢„æµ‹è¯¯å·®ä½ï¼Œ95%ç½®ä¿¡åŒºé—´ä¸º0.78è‡³1.52å‘¨ï¼Œå¹¶ç”Ÿæˆäº†åŸºäºå‘è‚²ç‰¹å¾çš„å¯è§£é‡Šè¾“å‡ºï¼Œä¸ºå›´äº§æœŸç¥ç»ç§‘å­¦çš„é€æ˜å’Œå¯ä¿¡èµ–çš„AIç³»ç»Ÿè¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¼ºè°ƒäº†åœ¨è¯„ä¼°æ–°ç”Ÿå„¿å‘å±•å’Œå¥åº·æ—¶ï¼Œå‡†ç¡®ä¼°è®¡åæ‰«æå¹´é¾„ï¼ˆPMAï¼‰çš„é‡è¦æ€§ã€‚</li>
<li>ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¿›è¡Œç²¾ç¡®PMAé¢„æµ‹å’Œä¸´åºŠç›¸å…³è§£é‡Šç”Ÿæˆæ˜¯è§£å†³è¯¥é—®é¢˜çš„æ–°æ–¹æ³•ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨å‚æ•°é«˜æ•ˆçš„å¾®è°ƒï¼ˆPEFTï¼‰ç­–ç•¥å’Œä½ç§©é€‚é…ï¼ˆLoRAï¼‰æŠ€æœ¯ï¼Œå¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œä¼˜åŒ–ã€‚</li>
<li>æ¨¡å‹åœ¨æ–°ç”Ÿå„¿MRIæ‰«æçš„äºŒç»´çš®å±‚è¡¨é¢æŠ•å½±å›¾ä¸Šè®­ç»ƒï¼Œå®ç°äº†ç²¾ç»†è°ƒæ•´ã€‚</li>
<li>æ¨¡å‹é¢„æµ‹è¯¯å·®ä½ï¼Œ95%ç½®ä¿¡åŒºé—´ä¸º0.78è‡³1.52å‘¨ã€‚</li>
<li>æ¨¡å‹ç”Ÿæˆäº†åŸºäºå‘è‚²ç‰¹å¾çš„å¯è§£é‡Šè¾“å‡ºï¼Œå¢å¼ºäº†AIç³»ç»Ÿçš„é€æ˜åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02525">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02525v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02525v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02525v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02525v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02525v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02525v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Glioblastoma-Overall-Survival-Prediction-With-Vision-Transformers"><a href="#Glioblastoma-Overall-Survival-Prediction-With-Vision-Transformers" class="headerlink" title="Glioblastoma Overall Survival Prediction With Vision Transformers"></a>Glioblastoma Overall Survival Prediction With Vision Transformers</h2><p><strong>Authors:Yin Lin, iccardo Barbieri, Domenico Aquino, Giuseppe Lauria, Marina Grisoli, Elena De Momi, Alberto Redaelli, Simona Ferrante</strong></p>
<p>Glioblastoma is one of the most aggressive and common brain tumors, with a median survival of 10-15 months. Predicting Overall Survival (OS) is critical for personalizing treatment strategies and aligning clinical decisions with patient outcomes. In this study, we propose a novel Artificial Intelligence (AI) approach for OS prediction using Magnetic Resonance Imaging (MRI) images, exploiting Vision Transformers (ViTs) to extract hidden features directly from MRI images, eliminating the need of tumor segmentation. Unlike traditional approaches, our method simplifies the workflow and reduces computational resource requirements.   The proposed model was evaluated on the BRATS dataset, reaching an accuracy of 62.5% on the test set, comparable to the top-performing methods. Additionally, it demonstrated balanced performance across precision, recall, and F1 score, overcoming the best model in these metrics. The dataset size limits the generalization of the ViT which typically requires larger datasets compared to convolutional neural networks. This limitation in generalization is observed across all the cited studies. This work highlights the applicability of ViTs for downsampled medical imaging tasks and establishes a foundation for OS prediction models that are computationally efficient and do not rely on segmentation. </p>
<blockquote>
<p>èƒ¶è´¨æ¯ç»†èƒç˜¤æ˜¯æœ€å…·ä¾µè¢­æ€§å’Œæœ€å¸¸è§çš„è„‘è‚¿ç˜¤ä¹‹ä¸€ï¼Œä¸­ä½ç”Ÿå­˜æœŸä¸º10-15ä¸ªæœˆã€‚é¢„æµ‹æ€»ä½“ç”Ÿå­˜æœŸï¼ˆOSï¼‰å¯¹äºä¸ªæ€§åŒ–æ²»ç–—ç­–ç•¥ä»¥åŠä¸æ‚£è€…ç»“æœç›¸ç¬¦çš„ä¸´åºŠå†³ç­–è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ–¹æ³•ï¼Œç”¨äºä½¿ç”¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å›¾åƒè¿›è¡ŒOSé¢„æµ‹ï¼Œåˆ©ç”¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ç›´æ¥ä»MRIå›¾åƒä¸­æå–éšè—ç‰¹å¾ï¼Œæ— éœ€è¿›è¡Œè‚¿ç˜¤åˆ†å‰²ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç®€åŒ–äº†å·¥ä½œæµç¨‹å¹¶é™ä½äº†è®¡ç®—èµ„æºè¦æ±‚ã€‚æ‰€æå‡ºæ¨¡å‹åœ¨BRATSæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œåœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°62.5%ï¼Œä¸è¡¨ç°æœ€å¥½çš„æ–¹æ³•ç›¸å½“ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1å¾—åˆ†æ–¹é¢è¡¨ç°å‡ºå¹³è¡¡çš„ç»©æ•ˆï¼Œåœ¨è¿™äº›æŒ‡æ ‡ä¸Šè¶…è¶Šäº†æœ€ä½³æ¨¡å‹ã€‚æ•°æ®é›†çš„å¤§å°é™åˆ¶äº†ViTçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸å·ç§¯ç¥ç»ç½‘ç»œç›¸æ¯”ï¼ŒViTé€šå¸¸éœ€è¦æ›´å¤§çš„æ•°æ®é›†ã€‚æ‰€æœ‰å¼•ç”¨çš„ç ”ç©¶éƒ½è§‚å¯Ÿåˆ°äº†è¿™ç§æ³›åŒ–é™åˆ¶ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†ViTsåœ¨é™é‡‡æ ·åŒ»å­¦æˆåƒä»»åŠ¡ä¸­çš„åº”ç”¨æ€§ï¼Œå¹¶ä¸ºè®¡ç®—æ•ˆç‡é«˜ã€ä¸ä¾èµ–åˆ†å‰²çš„OSé¢„æµ‹æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02439v1">PDF</a> 4 pages, 4 figures, EMBC2025</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„æ€»ä½“ç”Ÿå­˜æœŸï¼ˆOSï¼‰é¢„æµ‹æ–°æ–¹æ³•ï¼Œåˆ©ç”¨ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å›¾åƒï¼Œé‡‡ç”¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ç›´æ¥ä»MRIå›¾åƒä¸­æå–éšè—ç‰¹å¾ï¼Œæ— éœ€è‚¿ç˜¤åˆ†å‰²ã€‚è¯¥æ–¹æ³•ç®€åŒ–äº†å·¥ä½œæµç¨‹ï¼Œé™ä½äº†è®¡ç®—èµ„æºè¦æ±‚ã€‚åœ¨BRATSæ•°æ®é›†ä¸Šè¯„ä¼°çš„æ¨¡å‹æµ‹è¯•é›†å‡†ç¡®ç‡ä¸º62.5%ï¼Œä¸ä¼ ç»Ÿé¡¶çº§æ–¹æ³•ç›¸å½“ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨ç²¾åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°æ–¹é¢è¡¨ç°å‡ºå¹³è¡¡çš„æ€§èƒ½ï¼Œåœ¨è¿™äº›æŒ‡æ ‡ä¸Šè¶…è¶Šäº†æœ€ä½³æ¨¡å‹ã€‚ä½†ç”±äºæ•°æ®é›†å¤§å°é™åˆ¶ï¼ŒViTçš„æ³›åŒ–èƒ½åŠ›å—é™ï¼Œé€šå¸¸éœ€è¦æ¯”å·ç§¯ç¥ç»ç½‘ç»œæ›´å¤§çš„æ•°æ®é›†ã€‚æœ¬ç ”ç©¶å¼ºè°ƒäº†ViTsåœ¨ä¸‹é‡‡æ ·åŒ»å­¦æˆåƒä»»åŠ¡ä¸­çš„åº”ç”¨æ€§ï¼Œå¹¶ä¸ºè®¡ç®—æ•ˆç‡é«˜ã€ä¸ä¾èµ–åˆ†å‰²çš„OSé¢„æµ‹æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ¬ç ”ç©¶åˆ©ç”¨AIå’ŒMRIå›¾åƒè¿›è¡Œèƒ¶è´¨æ¯ç»†èƒç˜¤çš„æ€»ä½“ç”Ÿå­˜æœŸé¢„æµ‹ã€‚</li>
<li>é‡‡ç”¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ç›´æ¥ä»MRIå›¾åƒä¸­æå–éšè—ç‰¹å¾ï¼Œç®€åŒ–äº†å·¥ä½œæµç¨‹å¹¶é™ä½äº†è®¡ç®—èµ„æºéœ€æ±‚ã€‚</li>
<li>åœ¨BRATSæ•°æ®é›†ä¸Šè¿›è¡Œçš„æ¨¡å‹è¯„ä¼°è¾¾åˆ°äº†62.5%çš„æµ‹è¯•é›†å‡†ç¡®ç‡ï¼Œä¸ä¼ ç»Ÿé¡¶çº§æ–¹æ³•ç›¸å½“ã€‚</li>
<li>æ¨¡å‹åœ¨ç²¾åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°æ–¹é¢è¡¨ç°å‡ºå¹³è¡¡çš„æ€§èƒ½ï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰æœ€ä½³æ¨¡å‹ã€‚</li>
<li>æ•°æ®é›†å¤§å°é™åˆ¶äº†ViTçš„æ³›åŒ–èƒ½åŠ›ï¼Œéœ€è¦æ›´å¤§çš„æ•°æ®é›†ä»¥æ”¹å–„å…¶æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†ViTsåœ¨ä¸‹é‡‡æ ·åŒ»å­¦æˆåƒä»»åŠ¡ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02439v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02439v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Identifying-actionable-driver-mutations-in-lung-cancer-using-an-efficient-Asymmetric-Transformer-Decoder"><a href="#Identifying-actionable-driver-mutations-in-lung-cancer-using-an-efficient-Asymmetric-Transformer-Decoder" class="headerlink" title="Identifying actionable driver mutations in lung cancer using an   efficient Asymmetric Transformer Decoder"></a>Identifying actionable driver mutations in lung cancer using an   efficient Asymmetric Transformer Decoder</h2><p><strong>Authors:Biagio Brattoli, Jack Shi, Jongchan Park, Taebum Lee, Donggeun Yoo, Sergio Pereira</strong></p>
<p>Identifying actionable driver mutations in non-small cell lung cancer (NSCLC) can impact treatment decisions and significantly improve patient outcomes. Despite guideline recommendations, broader adoption of genetic testing remains challenging due to limited availability and lengthy turnaround times. Machine Learning (ML) methods for Computational Pathology (CPath) offer a potential solution; however, research often focuses on only one or two common mutations, limiting the clinical value of these tools and the pool of patients who can benefit from them. This study evaluates various Multiple Instance Learning (MIL) techniques to detect six key actionable NSCLC driver mutations: ALK, BRAF, EGFR, ERBB2, KRAS, and MET ex14. Additionally, we introduce an Asymmetric Transformer Decoder model that employs queries and key-values of varying dimensions to maintain a low query dimensionality. This approach efficiently extracts information from patch embeddings and minimizes overfitting risks, proving highly adaptable to the MIL setting. Moreover, we present a method to directly utilize tissue type in the model, addressing a typical MIL limitation where either all regions or only some specific regions are analyzed, neglecting biological relevance. Our method outperforms top MIL models by an average of 3%, and over 4% when predicting rare mutations such as ERBB2 and BRAF, moving ML-based tests closer to being practical alternatives to standard genetic testing. </p>
<blockquote>
<p>è¯†åˆ«éå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰ä¸­çš„å¯æ“ä½œé©±åŠ¨åŸºå› çªå˜å¯ä»¥å¯¹æ²»ç–—å†³ç­–äº§ç”Ÿå½±å“ï¼Œå¹¶æ˜¾è‘—æ”¹å–„æ‚£è€…é¢„åã€‚å°½ç®¡æœ‰æŒ‡å—æ¨èï¼Œä½†ç”±äºæœ‰é™çš„å¯ç”¨æ€§å’Œæ¼«é•¿çš„å‘¨è½¬æ—¶é—´ï¼Œæ›´å¹¿æ³›åœ°é‡‡ç”¨åŸºå› æ£€æµ‹ä»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚è®¡ç®—ç—…ç†å­¦ï¼ˆCPathï¼‰ä¸­çš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ–¹æ³•æä¾›äº†ä¸€ç§æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼›ç„¶è€Œï¼Œç ”ç©¶é€šå¸¸åªå…³æ³¨ä¸€ç§æˆ–ä¸¤ç§å¸¸è§çªå˜ï¼Œè¿™é™åˆ¶äº†è¿™äº›å·¥å…·çš„ä¸´åºŠä»·å€¼ä»¥åŠèƒ½ä»è¿™äº›å·¥å…·ä¸­å—ç›Šçš„æ‚£è€…ç¾¤ä½“ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†å„ç§å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æŠ€æœ¯ï¼Œä»¥æ£€æµ‹å…­ç§å…³é”®å¯æ“ä½œNSCLCé©±åŠ¨çªå˜ï¼šALKã€BRAFã€EGFRã€ERBB2ã€KRASå’ŒMET ex14ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸å¯¹ç§°å˜å‹å™¨è§£ç å™¨æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨ä¸åŒç»´åº¦çš„æŸ¥è¯¢å’Œé”®å€¼æ¥ä¿æŒä½æŸ¥è¯¢ç»´åº¦ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°ä»è¡¥ä¸åµŒå…¥ä¸­æå–ä¿¡æ¯ï¼Œæœ€å°åŒ–è¿‡åº¦æ‹Ÿåˆé£é™©ï¼Œå¹¶èƒ½å¾ˆå¥½åœ°é€‚åº”MILç¯å¢ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç›´æ¥åœ¨æ¨¡å‹ä¸­ä½¿ç”¨ç»„ç»‡ç±»å‹çš„æ–¹æ³•ï¼Œè§£å†³äº†å…¸å‹çš„MILé™åˆ¶ï¼Œå³åˆ†ææ‰€æœ‰åŒºåŸŸæˆ–ä»…åˆ†ææŸäº›ç‰¹å®šåŒºåŸŸè€Œå¿½è§†ç”Ÿç‰©å­¦æ„ä¹‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¹³å‡æ¯”é¡¶çº§MILæ¨¡å‹çš„æ€§èƒ½é«˜å‡º3%ï¼Œåœ¨é¢„æµ‹å¦‚ERBB2å’ŒBRAFç­‰ç½•è§çªå˜æ—¶ï¼Œæ€§èƒ½æé«˜äº†è¶…è¿‡4%ï¼Œè¿™ä½¿å¾—åŸºäºMLçš„æµ‹è¯•æ›´æ¥è¿‘æˆä¸ºæ ‡å‡†é—ä¼ æµ‹è¯•çš„å®ç”¨æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02431v1">PDF</a> Accepted at MICCAI 2025 Workshop COMPAYL</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶åˆ©ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯æ£€æµ‹éå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰çš„å…­ç§å…³é”®å¯æ“ä½œçš„é©±åŠ¨åŸºå› çªå˜ï¼ŒåŒ…æ‹¬ALKã€BRAFã€EGFRã€ERBB2ã€KRASå’ŒMET ex14ã€‚ç ”ç©¶é‡‡ç”¨å¤šç§å®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æŠ€æœ¯ï¼Œå¹¶å¼•å…¥ä¸å¯¹ç§°è½¬æ¢å™¨è§£ç å™¨æ¨¡å‹ï¼Œè¯¥æ–¹æ³•å¯æœ‰æ•ˆæå–è¡¥ä¸åµŒå…¥ä¿¡æ¯ï¼Œæœ€å°åŒ–è¿‡åº¦æ‹Ÿåˆé£é™©ï¼Œå¹¶é«˜åº¦é€‚åº”MILç¯å¢ƒã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§ç›´æ¥åˆ©ç”¨ç»„ç»‡ç±»å‹çš„æ–¹æ³•ï¼Œè§£å†³äº†å…¸å‹çš„MILé™åˆ¶é—®é¢˜ï¼Œå³åœ¨åˆ†ææ—¶è¦ä¹ˆåˆ†ææ‰€æœ‰åŒºåŸŸï¼Œè¦ä¹ˆåªåˆ†ææŸäº›ç‰¹å®šåŒºåŸŸï¼Œå¿½ç•¥äº†ç”Ÿç‰©ç›¸å…³æ€§ã€‚æ­¤æ–¹æ³•ä¼˜äºé¡¶çº§MILæ¨¡å‹ï¼Œåœ¨é¢„æµ‹ç½•è§çªå˜å¦‚ERBB2å’ŒBRAFæ—¶è¡¨ç°æ›´ä¸ºçªå‡ºï¼Œä½¿åŸºäºMLçš„æµ‹è¯•æ›´æ¥è¿‘äºå®ç”¨çš„æ›¿ä»£æ ‡å‡†é—ä¼ æµ‹è¯•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯†åˆ«éå°ç»†èƒè‚ºç™Œï¼ˆNSCLCï¼‰çš„å¯æ“ä½œé©±åŠ¨åŸºå› çªå˜å¯¹æ²»ç–—å†³ç­–å’Œæ‚£è€…é¢„åæœ‰é‡è¦å½±å“ã€‚</li>
<li>æœºå™¨å­¦ä¹ åœ¨è®¡ç®—ç—…ç†å­¦ï¼ˆCPathï¼‰ä¸­çš„åº”ç”¨ä¸ºè§£å†³é—ä¼ æµ‹è¯•çš„é™åˆ¶æä¾›äº†æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨å¤šç§å®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æŠ€æœ¯æ£€æµ‹NSCLCçš„å…­ç§å…³é”®çªå˜ã€‚</li>
<li>å¼•å…¥ä¸å¯¹ç§°è½¬æ¢å™¨è§£ç å™¨æ¨¡å‹ï¼Œæœ‰æ•ˆæå–ä¿¡æ¯å¹¶æœ€å°åŒ–è¿‡åº¦æ‹Ÿåˆé£é™©ã€‚</li>
<li>æå‡ºä¸€ç§ç›´æ¥åˆ©ç”¨ç»„ç»‡ç±»å‹çš„æ–¹æ³•ï¼Œè§£å†³MILåœ¨åˆ†ææ—¶çš„å…¸å‹é™åˆ¶é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•ä¼˜äºå…¶ä»–MILæ¨¡å‹ï¼Œåœ¨é¢„æµ‹ç½•è§çªå˜æ—¶è¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02431">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02431v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02431v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02431v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02431v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02431v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Whole-body-Representation-Learning-For-Competing-Preclinical-Disease-Risk-Assessment"><a href="#Whole-body-Representation-Learning-For-Competing-Preclinical-Disease-Risk-Assessment" class="headerlink" title="Whole-body Representation Learning For Competing Preclinical Disease   Risk Assessment"></a>Whole-body Representation Learning For Competing Preclinical Disease   Risk Assessment</h2><p><strong>Authors:Dmitrii Seletkov, Sophie Starck, Ayhan Can Erdur, Yundi Zhang, Daniel Rueckert, Rickmer Braren</strong></p>
<p>Reliable preclinical disease risk assessment is essential to move public healthcare from reactive treatment to proactive identification and prevention. However, image-based risk prediction algorithms often consider one condition at a time and depend on hand-crafted features obtained through segmentation tools. We propose a whole-body self-supervised representation learning method for the preclinical disease risk assessment under a competing risk modeling. This approach outperforms whole-body radiomics in multiple diseases, including cardiovascular disease (CVD), type 2 diabetes (T2D), chronic obstructive pulmonary disease (COPD), and chronic kidney disease (CKD). Simulating a preclinical screening scenario and subsequently combining with cardiac MRI, it sharpens further the prediction for CVD subgroups: ischemic heart disease (IHD), hypertensive diseases (HD), and stroke. The results indicate the translational potential of whole-body representations as a standalone screening modality and as part of a multi-modal framework within clinical workflows for early personalized risk stratification. The code is available at <a target="_blank" rel="noopener" href="https://github.com/yayapa/WBRLforCR/">https://github.com/yayapa/WBRLforCR/</a> </p>
<blockquote>
<p>å¯é çš„æ—©æœŸç–¾ç—…é£é™©è¯„ä¼°å¯¹äºæ¨åŠ¨å…¬å…±åŒ»ç–—ä»è¢«åŠ¨æ²»ç–—è½¬å‘ä¸»åŠ¨è¯†åˆ«å’Œé¢„é˜²è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼ŒåŸºäºå›¾åƒçš„é¢„æµ‹ç®—æ³•é€šå¸¸ä¸€æ¬¡åªè€ƒè™‘ä¸€ç§æƒ…å†µï¼Œå¹¶ä¾èµ–äºé€šè¿‡åˆ†å‰²å·¥å…·è·å¾—çš„æ‰‹å·¥ç‰¹å¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨èº«è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œç”¨äºåœ¨ç«äº‰é£é™©å»ºæ¨¡ä¸‹è¿›è¡Œæ—©æœŸç–¾ç—…é£é™©è¯„ä¼°ã€‚è¯¥æ–¹æ³•åœ¨å¤šç–¾ç—…è¯„ä¼°ä¸­ä¼˜äºå…¨èº«æ”¾å°„ç»„å­¦æ–¹æ³•ï¼ŒåŒ…æ‹¬å¿ƒè¡€ç®¡ç–¾ç—…ï¼ˆCVDï¼‰ã€2å‹ç³–å°¿ç—…ï¼ˆT2Dï¼‰ã€æ…¢æ€§é˜»å¡æ€§è‚ºç–¾ç—…ï¼ˆCOPDï¼‰å’Œæ…¢æ€§è‚¾è„ç–¾ç—…ï¼ˆCKDï¼‰ã€‚æ¨¡æ‹Ÿæ—©æœŸç­›æŸ¥æƒ…æ™¯ï¼Œå¹¶ä¸å¿ƒè„MRIç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æé«˜äº†å¯¹å¿ƒè¡€ç®¡ç–¾ç—…äºšç»„çš„é¢„æµ‹èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç¼ºè¡€æ€§å¿ƒè„ç—…ï¼ˆIHDï¼‰ã€é«˜è¡€å‹ç–¾ç—…ï¼ˆHDï¼‰å’Œä¸­é£ã€‚ç»“æœè¡¨æ˜ï¼Œå…¨èº«è¡¨ç°å…·æœ‰ä½œä¸ºç‹¬ç«‹ç­›æŸ¥æ¨¡å¼çš„æ½œåŠ›ï¼Œä»¥åŠä½œä¸ºä¸´åºŠå·¥ä½œæµç¨‹å†…å¤šæ¨¡æ€æ¡†æ¶çš„ä¸€éƒ¨åˆ†è¿›è¡Œæ—©æœŸä¸ªæ€§åŒ–é£é™©åˆ†å±‚çš„èƒ½åŠ›ã€‚ç›¸å…³ä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/yayapa/WBRLforCR/%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yayapa/WBRLforCR/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02307v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºåŒ»å­¦å½±åƒçš„ç–¾ç—…é£é™©é¢„å…ˆè¯„ä¼°ä¸ºå…¬å…±å«ç”Ÿé¢†åŸŸä»è¢«åŠ¨æ²»ç–—è½¬å‘ä¸»åŠ¨è¯†åˆ«å’Œé¢„é˜²æä¾›äº†å¯èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å›¾åƒé¢„æµ‹ç®—æ³•å¸¸å¸¸åªé’ˆå¯¹å•ä¸€ç–¾ç—…è¿›è¡Œåˆ†æï¼Œå¹¶ä¾èµ–äºåˆ†å‰²å·¥å…·è·å¾—çš„æ‰‹åŠ¨ç‰¹å¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨èº«è‡ªæˆ‘ç›‘ç£å­¦ä¹ çš„æ–¹æ³•ç”¨äºç«äº‰é£é™©æ¨¡å‹ä¸‹çš„ç–¾ç—…é£é™©é¢„å…ˆè¯„ä¼°ã€‚è¯¥æ–¹æ³•åœ¨å¤šç§ç–¾ç—…ä¸Šçš„è¡¨ç°ä¼˜äºå…¨èº«æ”¾å°„ç»„å­¦æ–¹æ³•ï¼ŒåŒ…æ‹¬å¿ƒè¡€ç®¡ç–¾ç—…ã€äºŒå‹ç³–å°¿ç—…ã€æ…¢æ€§é˜»å¡æ€§è‚ºç—…å’Œæ…¢æ€§è‚¾è„ç–¾ç—…ã€‚æ¨¡æ‹Ÿé¢„å…ˆç­›æŸ¥æƒ…æ™¯å¹¶ç»“åˆå¿ƒè„MRIï¼Œè¯¥æ–¹æ³•èƒ½æ›´ç²¾ç¡®åœ°é¢„æµ‹å¿ƒè¡€ç®¡ç–¾ç—…çš„å­ç±»åˆ«ï¼šç¼ºè¡€æ€§å¿ƒè„ç—…ã€é«˜è¡€å‹ç–¾ç—…å’Œè„‘å’ä¸­ã€‚ç»“æœè¯æ˜äº†å…¨èº«è¡¨ç°ä½œä¸ºç‹¬ç«‹ç­›æŸ¥æ¨¡å¼å’Œä¸´åºŠå·¥ä½œæµç¨‹ä¸­çš„å¤šæ¨¡å¼æ¡†æ¶åœ¨æ—©æœŸä¸ªæ€§åŒ–é£é™©è¯„ä¼°ä¸­çš„è½¬åŒ–æ½œåŠ›ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yayapa/WBRLforCR/%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yayapa/WBRLforCR/æ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯é çš„ä¸´åºŠå‰ç–¾ç—…é£é™©è¯„ä¼°å¯¹äºå…¬å…±å«ç”Ÿé¢†åŸŸä»è¢«åŠ¨æ²»ç–—è½¬å‘ä¸»åŠ¨é¢„é˜²å’Œè¯†åˆ«è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰å›¾åƒé¢„æµ‹ç®—æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¸¸å¸¸ä»…é’ˆå¯¹å•ä¸€ç–¾ç—…è¿›è¡Œåˆ†æï¼Œå¹¶ä¾èµ–æ‰‹åŠ¨ç‰¹å¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å…¨èº«è‡ªæˆ‘ç›‘ç£å­¦ä¹ çš„æ–¹æ³•ç”¨äºç–¾ç—…é£é™©é¢„å…ˆè¯„ä¼°ï¼Œè¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¿ƒè¡€ç®¡ç–¾ç—…ã€äºŒå‹ç³–å°¿ç—…ã€æ…¢æ€§é˜»å¡æ€§è‚ºç—…å’Œæ…¢æ€§è‚¾è„ç–¾ç—…ç­‰å¤šç§ç–¾ç—…ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç»“åˆå¿ƒè„MRIè¿›è¡Œæ¨¡æ‹Ÿé¢„å…ˆç­›æŸ¥ï¼Œèƒ½æ›´ç²¾ç¡®åœ°é¢„æµ‹å¿ƒè¡€ç®¡ç–¾ç—…çš„å­ç±»åˆ«ã€‚</li>
<li>å…¨èº«è¡¨ç°ä½œä¸ºç‹¬ç«‹ç­›æŸ¥æ¨¡å¼å’Œå¤šæ¨¡å¼æ¡†æ¶åœ¨ä¸ªæ€§åŒ–é£é™©è¯„ä¼°ä¸­å…·æœ‰è½¬åŒ–æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02307">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02307v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02307v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02307v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Semi-Supervised-Dual-Threshold-Contrastive-Learning-for-Ultrasound-Image-Classification-and-Segmentation"><a href="#Semi-Supervised-Dual-Threshold-Contrastive-Learning-for-Ultrasound-Image-Classification-and-Segmentation" class="headerlink" title="Semi-Supervised Dual-Threshold Contrastive Learning for Ultrasound Image   Classification and Segmentation"></a>Semi-Supervised Dual-Threshold Contrastive Learning for Ultrasound Image   Classification and Segmentation</h2><p><strong>Authors:Peng Zhang, Zhihui Lai, Heng Kong</strong></p>
<p>Confidence-based pseudo-label selection usually generates overly confident yet incorrect predictions, due to the early misleadingness of model and overfitting inaccurate pseudo-labels in the learning process, which heavily degrades the performance of semi-supervised contrastive learning. Moreover, segmentation and classification tasks are treated independently and the affinity fails to be fully explored. To address these issues, we propose a novel semi-supervised dual-threshold contrastive learning strategy for ultrasound image classification and segmentation, named Hermes. This strategy combines the strengths of contrastive learning with semi-supervised learning, where the pseudo-labels assist contrastive learning by providing additional guidance. Specifically, an inter-task attention and saliency module is also developed to facilitate information sharing between the segmentation and classification tasks. Furthermore, an inter-task consistency learning strategy is designed to align tumor features across both tasks, avoiding negative transfer for reducing features discrepancy. To solve the lack of publicly available ultrasound datasets, we have collected the SZ-TUS dataset, a thyroid ultrasound image dataset. Extensive experiments on two public ultrasound datasets and one private dataset demonstrate that Hermes consistently outperforms several state-of-the-art methods across various semi-supervised settings. </p>
<blockquote>
<p>åŸºäºç½®ä¿¡åº¦çš„ä¼ªæ ‡ç­¾é€‰æ‹©é€šå¸¸ä¼šäº§ç”Ÿè¿‡äºè‡ªä¿¡ä½†é”™è¯¯çš„é¢„æµ‹ï¼Œè¿™æ˜¯ç”±äºæ¨¡å‹æ—©æœŸçš„è¯¯å¯¼ä»¥åŠåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å¯¹ä¸å‡†ç¡®ä¼ªæ ‡ç­¾çš„è¿‡æ‹Ÿåˆæ‰€å¯¼è‡´çš„ï¼Œè¿™ä¸¥é‡é™ä½äº†åŠç›‘ç£å¯¹æ¯”å­¦ä¹ çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡è¢«ç‹¬ç«‹å¤„ç†ï¼Œå¹¶ä¸”äº²å’ŒåŠ›æ²¡æœ‰å¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŠç›‘ç£åŒé˜ˆå€¼å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œç”¨äºè¶…å£°å›¾åƒçš„åˆ†ç±»å’Œåˆ†å‰²ï¼Œåä¸ºHermesã€‚è¯¥ç­–ç•¥ç»“åˆäº†å¯¹æ¯”å­¦ä¹ ä¸åŠç›‘ç£å­¦ä¹ çš„ä¼˜ç‚¹ï¼Œå…¶ä¸­ä¼ªæ ‡ç­¾é€šè¿‡æä¾›é¢å¤–æŒ‡å¯¼æ¥è¾…åŠ©å¯¹æ¯”å­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼Œè¿˜å¼€å‘äº†ä¸€ä¸ªè·¨ä»»åŠ¡æ³¨æ„åŠ›å’Œæ˜¾è‘—æ¨¡å—ï¼Œä»¥ä¿ƒè¿›åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡ä¹‹é—´çš„ä¿¡æ¯å…±äº«ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ç§è·¨ä»»åŠ¡ä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥ï¼Œä»¥å¯¹é½ä¸¤ä¸ªä»»åŠ¡ä¸­çš„è‚¿ç˜¤ç‰¹å¾ï¼Œé¿å…è´Ÿé¢è½¬ç§»ï¼Œå‡å°‘ç‰¹å¾å·®å¼‚ã€‚ä¸ºäº†è§£å†³ç¼ºä¹å…¬å¼€å¯ç”¨çš„è¶…å£°æ•°æ®é›†çš„é—®é¢˜ï¼Œæˆ‘ä»¬æ”¶é›†äº†SZ-TUSæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªç”²çŠ¶è…ºè¶…å£°å›¾åƒæ•°æ®é›†ã€‚åœ¨ä¸¤ä¸ªå…¬å¼€è¶…å£°æ•°æ®é›†å’Œä¸€ä¸ªç§æœ‰æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHermesåœ¨å„ç§åŠç›‘ç£è®¾ç½®ä¸‹å‡ä¼˜äºå‡ ç§æœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02265v1">PDF</a> Accepted in ECAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºåŠç›‘ç£å¯¹æ¯”å­¦ä¹ ä¸­ä¼ªæ ‡ç­¾é€‰æ‹©å­˜åœ¨çš„é—®é¢˜ï¼ŒåŒ…æ‹¬è¿‡æ—©çš„è¯¯å¯¼æ¨¡å‹å’Œä¼ªæ ‡ç­¾ä¸å‡†ç¡®å¯¼è‡´çš„è¿‡æ‹Ÿåˆç°è±¡ï¼Œä¸¥é‡å½±å“äº†åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡çš„æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹åŠç›‘ç£åŒé˜ˆå€¼å¯¹æ¯”å­¦ä¹ ç­–ç•¥ï¼Œåä¸ºHermesï¼Œç»“åˆäº†å¯¹æ¯”å­¦ä¹ ä¸åŠç›‘ç£å­¦ä¹ çš„ä¼˜åŠ¿ï¼Œåˆ©ç”¨ä¼ªæ ‡ç­¾ä¸ºå¯¹æ¯”å­¦ä¹ æä¾›é¢å¤–æŒ‡å¯¼ã€‚åŒæ—¶ï¼Œå¼€å‘äº†ä¸€ä¸ªè·¨ä»»åŠ¡æ³¨æ„åŠ›å’Œæ˜¾è‘—æ€§æ¨¡å—ï¼Œä¿ƒè¿›åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡ä¹‹é—´çš„ä¿¡æ¯å…±äº«ã€‚é€šè¿‡è®¾è®¡è·¨ä»»åŠ¡ä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥ï¼Œå¯¹è‚¿ç˜¤ç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œå‡å°‘ç‰¹å¾å·®å¼‚å¹¶é¿å…è´Ÿè¿ç§»ã€‚ä¸ºè§£å†³ç¼ºä¹å…¬å¼€å¯ç”¨çš„è¶…å£°æ•°æ®é›†çš„é—®é¢˜ï¼Œæ”¶é›†äº†SZ-TUSæ•°æ®é›†ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯Hermesçš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£å¯¹æ¯”å­¦ä¹ ä¸­ä¼ªæ ‡ç­¾é€‰æ‹©å­˜åœ¨è¿‡æ—©è¯¯å¯¼æ¨¡å‹å’Œä¼ªæ ‡ç­¾è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå½±å“æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹åŠç›‘ç£åŒé˜ˆå€¼å¯¹æ¯”å­¦ä¹ ç­–ç•¥Hermesï¼Œç»“åˆå¯¹æ¯”å­¦ä¹ ä¸åŠç›‘ç£å­¦ä¹ ä¼˜åŠ¿ã€‚</li>
<li>ä¼ªæ ‡ç­¾ä¸ºå¯¹æ¯”å­¦ä¹ æä¾›é¢å¤–æŒ‡å¯¼ã€‚</li>
<li>å¼€å‘è·¨ä»»åŠ¡æ³¨æ„åŠ›å’Œæ˜¾è‘—æ€§æ¨¡å—ï¼Œä¿ƒè¿›åˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡ä¹‹é—´çš„ä¿¡æ¯å…±äº«ã€‚</li>
<li>è®¾è®¡è·¨ä»»åŠ¡ä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥ï¼Œå¯¹è‚¿ç˜¤ç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œå‡å°‘ç‰¹å¾å·®å¼‚ã€‚</li>
<li>æ”¶é›†SZ-TUSæ•°æ®é›†ï¼Œè§£å†³ç¼ºä¹å…¬å¼€å¯ç”¨çš„è¶…å£°æ•°æ®é›†é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02265">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02265v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02265v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02265v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02265v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Deep-classification-algorithm-for-De-identification-of-DICOM-medical-images"><a href="#Deep-classification-algorithm-for-De-identification-of-DICOM-medical-images" class="headerlink" title="Deep classification algorithm for De-identification of DICOM medical   images"></a>Deep classification algorithm for De-identification of DICOM medical   images</h2><p><strong>Authors:Bufano Michele, Kotter Elmar</strong></p>
<p>Background : De-identification of DICOM (Digital Imaging and Communi-cations in Medicine) files is an essential component of medical image research. Personal Identifiable Information (PII) and&#x2F;or Personal Health Identifying Information (PHI) need to be hidden or removed due to legal reasons. According to the Health Insurance Portability and Accountability Act (HIPAA) and privacy rules, also full-face photographic images and any compa-rable images are direct identifiers and are considered protected health information that also need to be de-identified. Objective : The study aimed to implement a method that permit to de-identify the PII and PHI information present in the header and burned on the pixel data of DICOM. Methods : To execute the de-identification, we implemented an algorithm based on the safe harbor method, defined by HIPAA. Our algorithm uses input customizable parameter to classify and then possibly de-identify individual DICOM tags. Results : The most sensible information, like names, history, personal data and institution were successfully recognized. Conclusions : We developed a python algorithm that is able to classify infor-mation present in a DICOM file. The flexibility provided by the use of customi-zable input parameters, which allow the user to customize the entire process de-pending on the case (e.g., the language), makes the entire program very promis-ing for both everyday use and research purposes. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/rtdicomexplorer/deep_deidentification">https://github.com/rtdicomexplorer/deep_deidentification</a>. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šDICOMï¼ˆåŒ»å­¦æ•°å­—æˆåƒä¸é€šä¿¡ï¼‰æ–‡ä»¶çš„åŒ¿ååŒ–æ˜¯åŒ»å­¦å›¾åƒç ”ç©¶çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚ç”±äºæ³•å¾‹åŸå› ï¼Œéœ€è¦éšè—æˆ–åˆ é™¤ä¸ªäººå¯è¯†åˆ«ä¿¡æ¯ï¼ˆPIIï¼‰å’Œ&#x2F;æˆ–ä¸ªäººå¥åº·è¯†åˆ«ä¿¡æ¯ï¼ˆPHIï¼‰ã€‚æ ¹æ®å¥åº·ä¿é™©æµé€šä¸è´£ä»»æ³•æ¡ˆï¼ˆHIPAAï¼‰å’Œéšç§è§„åˆ™ï¼Œå…¨é¢é¢éƒ¨ç…§ç‰‡å’Œä»»ä½•å¯æ¯”å›¾åƒæ˜¯ç›´æ¥æ ‡è¯†ç¬¦ï¼Œä¹Ÿè¢«è§†ä¸ºå—ä¿æŠ¤çš„å¥åº·ä¿¡æ¯ï¼ŒåŒæ ·éœ€è¦åŒ¿ååŒ–ã€‚ç›®æ ‡ï¼šæœ¬ç ”ç©¶æ—¨åœ¨å®ç°ä¸€ç§æ–¹æ³•ï¼Œå…è®¸å¯¹DICOMçš„å¤´éƒ¨å’Œåƒç´ æ•°æ®ä¸­å­˜åœ¨çš„PIIå’ŒPHIä¿¡æ¯è¿›è¡ŒåŒ¿ååŒ–å¤„ç†ã€‚æ–¹æ³•ï¼šä¸ºäº†æ‰§è¡ŒåŒ¿ååŒ–å¤„ç†ï¼Œæˆ‘ä»¬åŸºäºHIPAAå®šä¹‰çš„å®‰å…¨æ¸¯æ–¹æ³•å®ç°äº†ä¸€ç§ç®—æ³•ã€‚æˆ‘ä»¬çš„ç®—æ³•ä½¿ç”¨å¯å®šåˆ¶çš„è¾“å…¥å‚æ•°å¯¹å•ä¸ªDICOMæ ‡ç­¾è¿›è¡Œåˆ†ç±»ï¼Œç„¶åå¯èƒ½è¿›è¡ŒåŒ¿ååŒ–å¤„ç†ã€‚ç»“æœï¼šæœ€æ•æ„Ÿçš„ä¿¡æ¯ï¼Œå¦‚å§“åã€ç—…å²ã€ä¸ªäººæ•°æ®å’Œæœºæ„ï¼Œå‡æˆåŠŸè¯†åˆ«ã€‚ç»“è®ºï¼šæˆ‘ä»¬å¼€å‘äº†ä¸€ç§Pythonç®—æ³•ï¼Œèƒ½å¤Ÿåˆ†ç±»å¤„ç†DICOMæ–‡ä»¶ä¸­çš„ä¿¡æ¯ã€‚é€šè¿‡ä½¿ç”¨å¯å®šåˆ¶çš„è¾“å…¥å‚æ•°æä¾›çš„çµæ´»æ€§ï¼Œå…è®¸ç”¨æˆ·æ ¹æ®å…·ä½“æƒ…å†µï¼ˆä¾‹å¦‚è¯­è¨€ï¼‰å¯¹æ•´ä¸ªæµç¨‹è¿›è¡Œå®šåˆ¶ï¼Œè¿™ä½¿å¾—è¯¥ç¨‹åºåœ¨æ—¥å¸¸ä½¿ç”¨å’Œç ”ç©¶æ–¹é¢éƒ½å¾ˆæœ‰å‰æ™¯ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/rtdicomexplorer/deep_deidentification%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/rtdicomexplorer/deep_deidentificationä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02177v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¯¹DICOMæ–‡ä»¶çš„å»æ ‡è¯†åŒ–ç ”ç©¶çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦å›¾åƒç ”ç©¶ä¸­ã€‚æ ¹æ®HIPAAç­‰ç›¸å…³æ³•å¾‹è¦æ±‚ï¼Œéœ€éšè—æˆ–åˆ é™¤ä¸ªäººä¿¡æ¯å’ŒåŒ»ç–—ä¿¡æ¯ã€‚è¯¥ç ”ç©¶çš„ç›®æ ‡æ˜¯å®æ–½ä¸€ç§å…è®¸ä»DICOMæ–‡ä»¶çš„æ ‡é¢˜å’Œåƒç´ æ•°æ®ä¸­è¯†åˆ«å’Œå»é™¤ä¸ªäººä¿¡æ¯çš„æ–¹æ³•ã€‚é‡‡ç”¨åŸºäºHIPAAå®‰å…¨æ¸¯æ–¹æ³•çš„ç®—æ³•å®ç°å»æ ‡è¯†åŒ–è¿‡ç¨‹ï¼Œå¹¶å¯æ ¹æ®è¾“å…¥çš„å¯å®šåˆ¶å‚æ•°å¯¹ä¸ªåˆ«DICOMæ ‡ç­¾è¿›è¡Œåˆ†ç±»å’Œå»æ ‡è¯†ã€‚æˆåŠŸè¯†åˆ«äº†å§“åã€ç—…å²ã€ä¸ªäººæ•°æ®å’Œæœºæ„ç­‰æ•æ„Ÿä¿¡æ¯ã€‚å¼€å‘äº†ä¸€ä¸ªPythonç®—æ³•ï¼Œå…·æœ‰çµæ´»æ€§å’Œå¯å®šåˆ¶æ€§ï¼Œå¯å¹¿æ³›åº”ç”¨äºæ—¥å¸¸ä½¿ç”¨å’ŒåŒ»å­¦ç ”ç©¶ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DICOMæ–‡ä»¶çš„å»æ ‡è¯†åŒ–æ˜¯åŒ»å­¦å›¾åƒç ”ç©¶ä¸­çš„å…³é”®æ­¥éª¤ï¼Œæ¶‰åŠéšè—æˆ–åˆ é™¤ä¸ªäººä¿¡æ¯å’ŒåŒ»ç–—ä¿¡æ¯ï¼Œç¬¦åˆHIPAAç­‰æ³•å¾‹è§„å®šçš„è¦æ±‚ã€‚</li>
<li>ç ”ç©¶ç›®æ ‡æ˜¯é€šè¿‡å®æ–½ä¸€ç§ç®—æ³•ï¼Œä»DICOMæ–‡ä»¶çš„æ ‡é¢˜å’Œåƒç´ æ•°æ®ä¸­è¯†åˆ«å’Œå»é™¤ä¸ªäººä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨åŸºäºHIPAAå®‰å…¨æ¸¯æ–¹æ³•çš„ç®—æ³•å®ç°å»æ ‡è¯†åŒ–è¿‡ç¨‹ã€‚è¯¥ç®—æ³•ä½¿ç”¨å¯å®šåˆ¶çš„è¾“å…¥å‚æ•°ï¼Œå¯å¯¹ä¸ªåˆ«DICOMæ ‡ç­¾è¿›è¡Œåˆ†ç±»å’Œå»æ ‡è¯†ã€‚</li>
<li>æˆåŠŸè¯†åˆ«å’Œå»é™¤DICOMæ–‡ä»¶ä¸­çš„æ•æ„Ÿä¿¡æ¯ï¼Œå¦‚å§“åã€ç—…å²ã€ä¸ªäººæ•°æ®å’Œæœºæ„ç­‰ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªPythonç®—æ³•ï¼Œå…·æœ‰çµæ´»æ€§å’Œå¯å®šåˆ¶æ€§ï¼Œé€‚ç”¨äºæ—¥å¸¸ä½¿ç”¨å’ŒåŒ»å­¦ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02177">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02177v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02177v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02177v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02177v1/page_3_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02177v1/page_3_3.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02177v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02177v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="REACT-KD-Region-Aware-Cross-modal-Topological-Knowledge-Distillation-for-Interpretable-Medical-Image-Classification"><a href="#REACT-KD-Region-Aware-Cross-modal-Topological-Knowledge-Distillation-for-Interpretable-Medical-Image-Classification" class="headerlink" title="REACT-KD: Region-Aware Cross-modal Topological Knowledge Distillation   for Interpretable Medical Image Classification"></a>REACT-KD: Region-Aware Cross-modal Topological Knowledge Distillation   for Interpretable Medical Image Classification</h2><p><strong>Authors:Hongzhao Chen, Hexiao Ding, Yufeng Jiang, Jing Lan, Ka Chun Li, Gerald W. Y. Cheng, Sam Ng, Chi Lai Ho, Jing Cai, Liang-ting Lin, Jung Sun Yoo</strong></p>
<p>Reliable and interpretable tumor classification from clinical imaging remains a core challenge due to heterogeneous modality quality, limited annotations, and the lack of structured anatomical guidance. We introduce REACT-KD, a Region-Aware Cross-modal Topological Knowledge Distillation framework that transfers rich supervision from high-fidelity multi-modal sources into a lightweight CT-based student model. The framework uses a dual teacher design: one branch captures structure-function relationships using dual-tracer PET&#x2F;CT, and the other models dose-aware features through synthetically degraded low-dose CT data. These branches jointly guide the student model through two complementary objectives. The first focuses on semantic alignment via logits distillation, while the second models anatomical topology using region graph distillation. A shared CBAM-3D module is employed to maintain consistent attention across modalities. To improve reliability for deployment, REACT-KD introduces modality dropout during training, allowing inference under partial or noisy inputs. The staging task for hepatocellular carcinoma (HCC) is conducted as a case study. REACT-KD achieves an average AUC of 93.4% on an internal PET&#x2F;CT cohort and maintains 76.6% to 81.5% AUC across varying dose levels in external CT testing. Decision curve analysis shows that REACT-KD consistently provides the highest clinical benefit across decision thresholds, supporting its potential in real-world diagnostics. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Kinetics-JOJO/REACT-KD">https://github.com/Kinetics-JOJO/REACT-KD</a>. </p>
<blockquote>
<p>ä»ä¸´åºŠå½±åƒå­¦è¿›è¡Œå¯é ä¸”å¯è§£é‡Šçš„è‚¿ç˜¤åˆ†ç±»ä»ç„¶æ˜¯ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼Œå…¶ä¸»è¦åŸå› åŒ…æ‹¬æ¨¡æ€è´¨é‡çš„å¼‚è´¨æ€§ã€æ³¨é‡Šæœ‰é™ä»¥åŠç¼ºä¹ç»“æ„åŒ–è§£å‰–æŒ‡å¯¼ã€‚æˆ‘ä»¬å¼•å…¥äº†REACT-KDï¼Œè¿™æ˜¯ä¸€ä¸ªåŒºåŸŸæ„ŸçŸ¥è·¨æ¨¡æ€æ‹“æ‰‘çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œå®ƒå¯ä»¥ä»é«˜ä¿çœŸå¤šæ¨¡æ€æºè½¬ç§»ä¸°å¯Œçš„ç›‘ç£ä¿¡æ¯åˆ°åŸºäºCTçš„è½»é‡çº§å­¦ç”Ÿæ¨¡å‹ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŒæ•™å¸ˆè®¾è®¡ï¼šä¸€ä¸ªåˆ†æ”¯ä½¿ç”¨åŒè¿½è¸ªPET&#x2F;CTæ•æ‰ç»“æ„-åŠŸèƒ½å…³ç³»ï¼Œå¦ä¸€ä¸ªåˆ†æ”¯é€šè¿‡åˆæˆé€€åŒ–çš„ä½å‰‚é‡CTæ•°æ®å»ºæ¨¡å‰‚é‡æ„ŸçŸ¥ç‰¹å¾ã€‚è¿™ä¸¤ä¸ªåˆ†æ”¯é€šè¿‡ä¸¤ä¸ªäº’è¡¥çš„ç›®æ ‡å…±åŒå¼•å¯¼å­¦ç”Ÿæ¨¡å‹ã€‚ç¬¬ä¸€ä¸ªç›®æ ‡æ˜¯é€šè¿‡logitsè’¸é¦å…³æ³¨è¯­ä¹‰å¯¹é½ï¼Œè€Œç¬¬äºŒä¸ªç›®æ ‡åˆ™æ˜¯é€šè¿‡åŒºåŸŸå›¾è’¸é¦å¯¹è§£å‰–æ‹“æ‰‘è¿›è¡Œå»ºæ¨¡ã€‚é‡‡ç”¨å…±äº«çš„CBAM-3Dæ¨¡å—ä»¥ç»´æŒå„æ¨¡æ€ä¹‹é—´çš„ä¸€è‡´æ³¨æ„åŠ›ã€‚ä¸ºäº†æé«˜éƒ¨ç½²çš„å¯é æ€§ï¼ŒREACT-KDåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†æ¨¡æ€ä¸¢å¼ƒï¼Œä½¿å¾—åœ¨éƒ¨åˆ†æˆ–å˜ˆæ‚çš„è¾“å…¥ä¸‹å¯ä»¥è¿›è¡Œæ¨æ–­ã€‚ä»¥è‚ç™Œåˆ†æœŸä»»åŠ¡ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼ŒREACT-KDåœ¨å†…éƒ¨PET&#x2F;CTé˜Ÿåˆ—ä¸Šå–å¾—äº†å¹³å‡AUCä¸º93.4%çš„æˆç»©ï¼Œå¹¶åœ¨å¤–éƒ¨CTæµ‹è¯•ä¸­ï¼Œåœ¨ä¸åŒå‰‚é‡æ°´å¹³ä¸‹ç»´æŒ76.6%è‡³81.5%çš„AUCã€‚å†³ç­–æ›²çº¿åˆ†ææ˜¾ç¤ºï¼ŒREACT-KDåœ¨å†³ç­–é˜ˆå€¼èŒƒå›´å†…å§‹ç»ˆæä¾›æœ€é«˜çš„ä¸´åºŠæ•ˆç›Šï¼Œè¿™æ”¯æŒå…¶åœ¨çœŸå®ä¸–ç•Œè¯Šæ–­ä¸­çš„æ½œåŠ›ã€‚ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/Kinetics-JOJO/REACT-KD%E3%80%82">https://github.com/Kinetics-JOJO/REACT-KDã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02104v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºREACT-KDçš„åŒºåŸŸæ„ŸçŸ¥è·¨æ¨¡æ€æ‹“æ‰‘çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œç”¨äºä»ä¸´åºŠæˆåƒä¸­å¯é ä¸”å¯è§£é‡Šåœ°è¿›è¡Œè‚¿ç˜¤åˆ†ç±»ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»é«˜è´¨é‡å¤šæ¨¡æ€æºè½¬ç§»ä¸°å¯Œç›‘ç£ä¿¡æ¯åˆ°è½»é‡çº§çš„CTåŸºç¡€å­¦ç”Ÿæ¨¡å‹ï¼Œè§£å†³äº†ç”±äºæ¨¡æ€è´¨é‡å¼‚è´¨æ€§ã€æ ‡æ³¨æœ‰é™ä»¥åŠç¼ºä¹ç»“æ„åŒ–è§£å‰–å­¦æŒ‡å¯¼æ‰€å¸¦æ¥çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚é€šè¿‡åŒæ•™å¸ˆè®¾è®¡ï¼Œä¸€ä¸ªåˆ†æ”¯åˆ©ç”¨åŒè¿½è¸ªPET&#x2F;CTæ•æ‰ç»“æ„-åŠŸèƒ½å…³ç³»ï¼Œå¦ä¸€ä¸ªåˆ†æ”¯é€šè¿‡åˆæˆé€€åŒ–ä½å‰‚é‡CTæ•°æ®å»ºæ¨¡å‰‚é‡æ„ŸçŸ¥ç‰¹å¾ã€‚ä¸¤ä¸ªåˆ†æ”¯é€šè¿‡ä¸¤ä¸ªäº’è¡¥çš„ç›®æ ‡å…±åŒå¼•å¯¼å­¦ç”Ÿæ¨¡å‹ï¼šä¸€ä¸ªä¾§é‡äºé€šè¿‡é€»è¾‘è’¸é¦å®ç°è¯­ä¹‰å¯¹é½ï¼Œå¦ä¸€ä¸ªåˆ™åˆ©ç”¨åŒºåŸŸå›¾è’¸é¦æ¨¡æ‹Ÿè§£å‰–å­¦æ‹“æ‰‘ã€‚ä¸ºæé«˜éƒ¨ç½²çš„å¯é æ€§ï¼ŒREACT-KDåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†æ¨¡æ€ä¸¢å¤±ï¼Œå¯åœ¨éƒ¨åˆ†æˆ–å˜ˆæ‚è¾“å…¥ä¸‹è¿›è¡Œæ¨ç†ã€‚ä»¥è‚ç™Œåˆ†æœŸä»»åŠ¡ä¸ºä¾‹è¿›è¡Œç ”ç©¶ï¼ŒREACT-KDåœ¨å†…éƒ¨PET&#x2F;CTé˜Ÿåˆ—ä¸­å¹³å‡AUCè¾¾åˆ°93.4%ï¼Œåœ¨ä¸åŒå‰‚é‡æ°´å¹³çš„å¤–éƒ¨CTæµ‹è¯•ä¸­ç»´æŒ76.6%è‡³81.5%çš„AUCã€‚å†³ç­–æ›²çº¿åˆ†ææ˜¾ç¤ºï¼ŒREACT-KDåœ¨å†³ç­–é˜ˆå€¼èŒƒå›´å†…å§‹ç»ˆæä¾›æœ€é«˜çš„ä¸´åºŠæ•ˆç›Šï¼Œè¯æ˜äº†å…¶åœ¨çœŸå®ä¸–ç•Œè¯Šæ–­ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>REACT-KDæ˜¯ä¸€ä¸ªç”¨äºè‚¿ç˜¤åˆ†ç±»çš„åŒºåŸŸæ„ŸçŸ¥è·¨æ¨¡æ€æ‹“æ‰‘çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œè§£å†³äº†ä¸´åºŠæˆåƒä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨åŒæ•™å¸ˆè®¾è®¡ï¼Œé€šè¿‡ç»“æ„-åŠŸèƒ½å…³ç³»å’Œå‰‚é‡æ„ŸçŸ¥ç‰¹å¾è¿›è¡Œå­¦ä¹ å’ŒæŒ‡å¯¼ã€‚</li>
<li>ä½¿ç”¨è¯­ä¹‰å¯¹é½å’Œè§£å‰–å­¦æ‹“æ‰‘æ¨¡æ‹Ÿçš„è”åˆç›®æ ‡æ¥å¼•å¯¼å­¦ç”Ÿæ¨¡å‹ã€‚</li>
<li>é€šè¿‡å…±äº«CBAM-3Dæ¨¡å—å®ç°è·¨æ¨¡æ€çš„ä¸€è‡´æ³¨æ„åŠ›ã€‚</li>
<li>REACT-KDé€šè¿‡å¼•å…¥æ¨¡æ€ä¸¢å¤±æé«˜éƒ¨ç½²å¯é æ€§ï¼Œæ”¯æŒéƒ¨åˆ†æˆ–å˜ˆæ‚è¾“å…¥ä¸‹çš„æ¨ç†ã€‚</li>
<li>ä»¥è‚ç™Œåˆ†æœŸä¸ºä¾‹ï¼ŒREACT-KDåœ¨å†…éƒ¨å’Œå¤–éƒ¨æµ‹è¯•ä¸­è¡¨ç°å‡ºé«˜AUCå’Œä¸´åºŠæ•ˆç›Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02104">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02104v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02104v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02104v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Conditional-Diffusion-Model-with-Anatomical-Dose-Dual-Constraints-for-End-to-End-Multi-Tumor-Dose-Prediction"><a href="#Conditional-Diffusion-Model-with-Anatomical-Dose-Dual-Constraints-for-End-to-End-Multi-Tumor-Dose-Prediction" class="headerlink" title="Conditional Diffusion Model with Anatomical-Dose Dual Constraints for   End-to-End Multi-Tumor Dose Prediction"></a>Conditional Diffusion Model with Anatomical-Dose Dual Constraints for   End-to-End Multi-Tumor Dose Prediction</h2><p><strong>Authors:Hui Xie, Haiqin Hu, Lijuan Ding, Qing Li, Yue Sun, Tao Tan</strong></p>
<p>Radiotherapy treatment planning often relies on time-consuming, trial-and-error adjustments that heavily depend on the expertise of specialists, while existing deep learning methods face limitations in generalization, prediction accuracy, and clinical applicability. To tackle these challenges, we propose ADDiff-Dose, an Anatomical-Dose Dual Constraints Conditional Diffusion Model for end-to-end multi-tumor dose prediction. The model employs LightweightVAE3D to compress high-dimensional CT data and integrates multimodal inputs, including target and organ-at-risk (OAR) masks and beam parameters, within a progressive noise addition and denoising framework. It incorporates conditional features via a multi-head attention mechanism and utilizes a composite loss function combining MSE, conditional terms, and KL divergence to ensure both dosimetric accuracy and compliance with clinical constraints. Evaluation on a large-scale public dataset (2,877 cases) and three external institutional cohorts (450 cases in total) demonstrates that ADDiff-Dose significantly outperforms traditional baselines, achieving an MAE of 0.101-0.154 (compared to 0.316 for UNet and 0.169 for GAN models), a DICE coefficient of 0.927 (a 6.8% improvement), and limiting spinal cord maximum dose error to within 0.1 Gy. The average plan generation time per case is reduced to 22 seconds. Ablation studies confirm that the structural encoder enhances compliance with clinical dose constraints by 28.5%. To our knowledge, this is the first study to introduce a conditional diffusion model framework for radiotherapy dose prediction, offering a generalizable and efficient solution for automated treatment planning across diverse tumor sites, with the potential to substantially reduce planning time and improve clinical workflow efficiency. </p>
<blockquote>
<p>æ”¾ç–—æ²»ç–—è®¡åˆ’å¾€å¾€ä¾èµ–äºè€—æ—¶ä¸”éœ€è¦é€šè¿‡åå¤è¯•éªŒå’Œè°ƒè¯•çš„ä¸“å®¶ä¸“ä¸šçŸ¥è¯†ï¼Œè€Œç°æœ‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•é¢ä¸´æ³›åŒ–èƒ½åŠ›ã€é¢„æµ‹ç²¾åº¦å’Œä¸´åºŠé€‚ç”¨æ€§çš„å±€é™ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ADDiff-Doseï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºç«¯åˆ°ç«¯å¤šè‚¿ç˜¤å‰‚é‡é¢„æµ‹çš„è§£å‰–å‰‚é‡åŒé‡çº¦æŸæ¡ä»¶æ‰©æ•£æ¨¡å‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨LightweightVAE3Då‹ç¼©é«˜ç»´CTæ•°æ®ï¼Œå¹¶åœ¨æ¸è¿›çš„å™ªå£°æ·»åŠ å’Œå»å™ªæ¡†æ¶å†…æ•´åˆäº†åŒ…æ‹¬ç›®æ ‡å’Œé£é™©å™¨å®˜ï¼ˆOARï¼‰æ©è†œå’Œå…‰æŸå‚æ•°åœ¨å†…çš„å¤šæ¨¡æ€è¾“å…¥ã€‚å®ƒé€šè¿‡å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶èå…¥æ¡ä»¶ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨ç»„åˆæŸå¤±å‡½æ•°ç»“åˆå‡æ–¹è¯¯å·®ã€æ¡ä»¶é¡¹å’ŒKLæ•£åº¦ï¼Œä»¥ç¡®ä¿å‰‚é‡å‡†ç¡®æ€§å’Œç¬¦åˆä¸´åºŠçº¦æŸã€‚åœ¨å¤§å‹å…¬å…±æ•°æ®é›†ï¼ˆ2877ä¾‹ï¼‰å’Œä¸‰ä¸ªå¤–éƒ¨æœºæ„é˜Ÿåˆ—ï¼ˆå…±450ä¾‹ï¼‰ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒADDiff-Doseæ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºçº¿ï¼Œå®ç°äº†å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ä¸º0.101-0.154ï¼ˆä¸UNetçš„0.316å’ŒGANæ¨¡å‹çš„0.169ç›¸æ¯”ï¼‰ï¼ŒDICEç³»æ•°ä¸º0.927ï¼ˆæé«˜äº†6.8%ï¼‰ï¼Œå¹¶å°†è„Šé«“æœ€å¤§å‰‚é‡è¯¯å·®é™åˆ¶åœ¨0.1 Gyä»¥å†…ã€‚æ¯ä¸ªç—…ä¾‹çš„å¹³å‡è®¡åˆ’ç”Ÿæˆæ—¶é—´ç¼©çŸ­è‡³22ç§’ã€‚æ¶ˆèç ”ç©¶è¯å®ï¼Œç»“æ„ç¼–ç å™¨é€šè¿‡å¢å¼ºä¸´åºŠå‰‚é‡çº¦æŸçš„éµå¾ªæ€§æé«˜äº†28.5%ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°†æ¡ä»¶æ‰©æ•£æ¨¡å‹æ¡†æ¶å¼•å…¥æ”¾ç–—å‰‚é‡é¢„æµ‹ç ”ç©¶ï¼Œä¸ºè·¨ä¸åŒè‚¿ç˜¤éƒ¨ä½çš„è‡ªåŠ¨åŒ–æ²»ç–—è®¡åˆ’æä¾›å¯æ¨å¹¿å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå…·æœ‰å¤§å¹…é™ä½è§„åˆ’æ—¶é—´å’Œæé«˜ä¸´åºŠå·¥ä½œæµç¨‹æ•ˆç‡çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02043v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºADDiff-Doseçš„è§£å‰–å‰‚é‡åŒçº¦æŸæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºç«¯åˆ°ç«¯å¤šè‚¿ç˜¤å‰‚é‡é¢„æµ‹ã€‚è¯¥æ¨¡å‹é‡‡ç”¨LightweightVAE3Då‹ç¼©é«˜ç»´CTæ•°æ®ï¼Œç»“åˆå¤šæ¨¡æ€è¾“å…¥ï¼ŒåŒ…æ‹¬ç›®æ ‡å’Œå±é™©å™¨å®˜æ©è†œä»¥åŠå…‰æŸå‚æ•°ï¼Œåœ¨é€æ­¥æ·»åŠ å™ªå£°å’Œå»å™ªå£°çš„æ¡†æ¶å†…å·¥ä½œã€‚é€šè¿‡å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å¼•å…¥æ¡ä»¶ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨ç»„åˆæŸå¤±å‡½æ•°ç¡®ä¿å‰‚é‡å‡†ç¡®æ€§å’Œç¬¦åˆä¸´åºŠçº¦æŸã€‚åœ¨å¤§å‹å…¬å…±æ•°æ®é›†å’Œä¸‰ä¸ªå¤–éƒ¨æœºæ„é˜Ÿåˆ—ä¸­çš„è¯„ä¼°è¡¨æ˜ï¼ŒADDiff-Doseæ˜¾è‘—ä¼˜äºä¼ ç»ŸåŸºçº¿ï¼Œè¾¾åˆ°å¹³å‡ç»å¯¹è¯¯å·®0.101-0.154ï¼ˆä¸UNetçš„0.316å’ŒGANæ¨¡å‹çš„0.169ç›¸æ¯”ï¼‰ï¼ŒDICEç³»æ•°è¾¾åˆ°0.927ï¼ˆæé«˜äº†6.8%ï¼‰ï¼Œå¹¶å°†è„Šé«“æœ€å¤§å‰‚é‡è¯¯å·®é™åˆ¶åœ¨0.1 Gyä»¥å†…ã€‚å¹³å‡æ¯ä¾‹è®¡åˆ’ç”Ÿæˆæ—¶é—´ç¼©çŸ­è‡³22ç§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ADDiff-Doseæ¨¡å‹é‡‡ç”¨LightweightVAE3DæŠ€æœ¯å‹ç¼©é«˜ç»´CTæ•°æ®ï¼Œæé«˜å¤„ç†æ•ˆç‡ã€‚</li>
<li>æ¨¡å‹ç»“åˆå¤šæ¨¡æ€è¾“å…¥ï¼ŒåŒ…æ‹¬ç›®æ ‡åŠå±é™©å™¨å®˜ä¿¡æ¯ã€å…‰æŸå‚æ•°ç­‰ï¼Œåœ¨é€æ­¥å»å™ªæ¡†æ¶å†…å·¥ä½œã€‚</li>
<li>é€šè¿‡å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å¼•å…¥æ¡ä»¶ç‰¹å¾ï¼Œå¢å¼ºæ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>å¤åˆæŸå¤±å‡½æ•°ç¡®ä¿äº†é¢„æµ‹å‰‚é‡çš„å‡†ç¡®æ€§å’Œç¬¦åˆä¸´åºŠçº¦æŸã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒADDiff-Doseåœ¨å‰‚é‡é¢„æµ‹æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤§å¹…åº¦å‡å°‘æ”¾ç–—è®¡åˆ’åˆ¶å®šæ—¶é—´ï¼Œæé«˜ä¸´åºŠå·¥ä½œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02043">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02043v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02043v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02043v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.02043v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Less-is-More-AMBER-AFNO-â€“-a-New-Benchmark-for-Lightweight-3D-Medical-Image-Segmentation"><a href="#Less-is-More-AMBER-AFNO-â€“-a-New-Benchmark-for-Lightweight-3D-Medical-Image-Segmentation" class="headerlink" title="Less is More: AMBER-AFNO â€“ a New Benchmark for Lightweight 3D Medical   Image Segmentation"></a>Less is More: AMBER-AFNO â€“ a New Benchmark for Lightweight 3D Medical   Image Segmentation</h2><p><strong>Authors:Andrea Dosi, Semanto Mondal, Rajib Chandra Ghosh, Massimo Brescia, Giuseppe Longo</strong></p>
<p>This work presents the results of a methodological transfer from remote sensing to healthcare, adapting AMBER â€“ a transformer-based model originally designed for multiband images, such as hyperspectral data â€“ to the task of 3D medical datacube segmentation. In this study, we use the AMBER architecture with Adaptive Fourier Neural Operators (AFNO) in place of the multi-head self-attention mechanism. While existing models rely on various forms of attention to capture global context, AMBER-AFNO achieves this through frequency-domain mixing, enabling a drastic reduction in model complexity. This design reduces the number of trainable parameters by over 80% compared to UNETR++, while maintaining a FLOPs count comparable to other state-of-the-art architectures. Model performance is evaluated on two benchmark 3D medical datasets â€“ ACDC and Synapse â€“ using standard metrics such as Dice Similarity Coefficient (DSC) and Hausdorff Distance (HD), demonstrating that AMBER-AFNO achieves competitive or superior accuracy with significant gains in training efficiency, inference speed, and memory usage. </p>
<blockquote>
<p>æœ¬æ–‡å±•ç¤ºäº†ä»é¥æ„Ÿå‘åŒ»ç–—é¢†åŸŸçš„æ–¹æ³•è¿ç§»ç»“æœï¼Œå°†AMBERâ€”â€”ä¸€ç§æœ€åˆä¸ºå¤šå…‰è°±å›¾åƒï¼ˆå¦‚è¶…å…‰è°±æ•°æ®ï¼‰è®¾è®¡çš„åŸºäºtransformerçš„æ¨¡å‹â€”â€”æ”¹ç¼–ç”¨äº3DåŒ»ç–—æ•°æ®ç«‹æ–¹ä½“åˆ†å‰²ä»»åŠ¡ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨AMBERæ¶æ„å¹¶ç”¨è‡ªé€‚åº”å‚…é‡Œå¶ç¥ç»ç½‘ç»œè¿ç®—ç¬¦ï¼ˆAFNOï¼‰ä»£æ›¿å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚å°½ç®¡ç°æœ‰æ¨¡å‹ä¾èµ–äºå„ç§å½¢å¼çš„æ³¨æ„åŠ›æ¥æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ï¼Œä½†AMBER-AFNOé€šè¿‡é¢‘åŸŸæ··åˆå®ç°äº†è¿™ä¸€ç‚¹ï¼Œä½¿å¾—æ¨¡å‹å¤æ‚åº¦å¤§å¹…é™ä½ã€‚è¿™ç§è®¾è®¡å°†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡å‡å°‘äº†8Nä»¥ä¸Šï¼ŒåŒæ—¶ä¿æŒä¸æœ€æ–°æ¶æ„ç›¸å½“çš„FLOPsè®¡æ•°ã€‚è¯¥æ¨¡å‹çš„æ€§èƒ½åœ¨ä¸¤ä¸ªåŸºå‡†çš„3DåŒ»ç–—æ•°æ®é›†ACDCå’ŒSynapseä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œä½¿ç”¨Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰å’ŒHausdorffè·ç¦»ï¼ˆHDï¼‰ç­‰æ ‡å‡†æŒ‡æ ‡ï¼Œè¯æ˜äº†AMBER-AFNOåœ¨è®­ç»ƒæ•ˆç‡ã€æ¨ç†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æˆ–æ›´é«˜çš„ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01941v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬å·¥ä½œå°†è¿œç¨‹ä¼ æ„Ÿçš„æ–¹æ³•å­¦åº”ç”¨äºåŒ»ç–—å¥åº·é¢†åŸŸï¼Œé€šè¿‡æ”¹è¿›AMBERæ¨¡å‹ä»¥é€‚åº”ä¸‰ç»´åŒ»å­¦æ•°æ®åˆ†å‰²ä»»åŠ¡ã€‚ç ”ç©¶ä¸­é‡‡ç”¨è‡ªé€‚åº”å‚…é‡Œå¶ç¥ç»ç½‘ç»œè¿ç®—ç¬¦ï¼ˆAFNOï¼‰æ›¿ä»£å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡é¢‘ç‡åŸŸæ··åˆå®ç°å…¨å±€ä¸Šä¸‹æ–‡æ•æ‰ï¼Œå¤§å¹…é™ä½æ¨¡å‹å¤æ‚åº¦ï¼Œå‚æ•°æ•°é‡å‡å°‘è¶…è¿‡80%ï¼ŒåŒæ—¶ä¿æŒä¸ç°æœ‰é¡¶å°–æ¶æ„ç›¸å½“çš„FLOPsè®¡æ•°ã€‚åœ¨ACDCå’ŒSynapseä¸¤ä¸ªåŸºå‡†ä¸‰ç»´åŒ»å­¦æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨Diceç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰å’ŒHausdorffè·ç¦»ï¼ˆHDï¼‰ç­‰æ ‡å‡†æŒ‡æ ‡è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œæ˜¾ç¤ºAMBER-AFNOåœ¨è®­ç»ƒæ•ˆç‡ã€æ¨ç†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AMBERæ¨¡å‹æˆåŠŸä»é¥æ„Ÿé¢†åŸŸè½¬ç§»åˆ°åŒ»ç–—å¥åº·é¢†åŸŸï¼Œå®ç°äº†åŒ»å­¦æ•°æ®åˆ†å‰²çš„ä»»åŠ¡é€‚åº”ã€‚</li>
<li>é‡‡ç”¨è‡ªé€‚åº”å‚…é‡Œå¶ç¥ç»ç½‘ç»œè¿ç®—ç¬¦ï¼ˆAFNOï¼‰æ›¿ä»£è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ç°äº†æ¨¡å‹å¤æ‚åº¦çš„æ˜¾è‘—é™ä½ã€‚</li>
<li>AMBER-AFNOæ¨¡å‹å¤æ‚åº¦é™ä½æ˜¾è‘—ï¼Œå‚æ•°æ•°é‡å‡å°‘äº†è¶…è¿‡80%ã€‚</li>
<li>AMBER-AFNOæ¨¡å‹ä¿æŒäº†ä¸é¡¶å°–æ¶æ„ç›¸å½“çš„FLOPsè®¡æ•°ã€‚</li>
<li>åœ¨ä¸¤ä¸ªåŸºå‡†ä¸‰ç»´åŒ»å­¦æ•°æ®é›†ä¸Šï¼ŒAMBER-AFNOæ¨¡å‹å®ç°äº†ç«äº‰æ€§æˆ–æ›´é«˜çš„å‡†ç¡®æ€§ã€‚</li>
<li>AMBER-AFNOæ¨¡å‹åœ¨è®­ç»ƒæ•ˆç‡ã€æ¨ç†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨æ–¹é¢æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01941">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01941v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01941v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01941v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01941v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="HAWC-VERITAS-Fermi-LAT-and-XMM-Newton-follow-up-observations-of-the-unidentified-ultra-high-energy-gamma-ray-source-LHAASO-J2108-5157"><a href="#HAWC-VERITAS-Fermi-LAT-and-XMM-Newton-follow-up-observations-of-the-unidentified-ultra-high-energy-gamma-ray-source-LHAASO-J2108-5157" class="headerlink" title="HAWC, VERITAS, Fermi-LAT and XMM-Newton follow-up observations of the   unidentified ultra-high-energy gamma-ray source LHAASO J2108+5157"></a>HAWC, VERITAS, Fermi-LAT and XMM-Newton follow-up observations of the   unidentified ultra-high-energy gamma-ray source LHAASO J2108+5157</h2><p><strong>Authors:Sajan Kumar</strong></p>
<p>We report observations of the ultra-high-energy gamma-ray source LHAASO J2108$+$5157, utilizing VERITAS, HAWC, \emph{Fermi}-LAT, and \textit{XMM-Newton}. VERITAS has collected $\sim$ 40 hours of data that we used to set ULs to the emission above 200 GeV. The HAWC data, collected over $\sim 2400$ days, reveal emission between 3 and 146 TeV, with a significance of $7.5~\sigma$, favoring an extended source model. The best-fit spectrum measured by HAWC is characterized by a simple power-law with a spectral index of $2.45\pm0.11_{stat}$. \emph{Fermi}-LAT analysis finds a point source with a very soft spectrum in the LHAASO J2108+5157 region, consistent with the 4FGL-DR3 catalog results. The \textit{XMM-Newton} analysis yields a null detection of the source in the 2 - 7 keV band. The broadband spectrum can be interpreted as a pulsar and a pulsar wind nebula system, where the GeV gamma-ray emission originates from an unidentified pulsar, and the X-ray and TeV emission is attributed to synchrotron radiation and inverse Compton scattering of electrons accelerated within a pulsar wind nebula. In this leptonic scenario, our X-ray upper limit provides a stringent constraint on the magnetic field, which is $\lesssim 1.5\ \mu$G. </p>
<blockquote>
<p>æˆ‘ä»¬æŠ¥å‘Šäº†å¯¹è¶…é«˜èƒ½ä¼½é©¬å°„çº¿æºLHAASO J2108$+$5157çš„è§‚å¯Ÿç»“æœï¼Œä½¿ç”¨çš„æ˜¯VERITASã€HAWCã€\emph{Fermi}-LAT å’Œ \textit{XMM-Newton}ã€‚VERITASæ”¶é›†äº†çº¦40å°æ—¶çš„æ•°æ®ï¼Œè¿™äº›æ•°æ®è¢«ç”¨æ¥è®¾å®šé«˜äº200å‰ç”µå­ä¼ç‰¹ï¼ˆGeVï¼‰çš„å‘å°„ä¸Šé™ã€‚HAWCæ”¶é›†çš„æ•°æ®è¦†ç›–çº¦2400å¤©ï¼Œæ­ç¤ºäº†ä»‹äº3å‰ç”µå­ä¼ç‰¹å’Œå¤ªç”µå­ä¼ç‰¹ä¹‹é—´çš„å‘å°„ç‰©ï¼Œå…·æœ‰7.5Ïƒçš„æ˜¾è‘—æ€§ï¼Œæ”¯æŒæ‰©å±•æºæ¨¡å‹ã€‚HAWCæµ‹é‡çš„æœ€ä½³æ‹Ÿåˆè°±å…·æœ‰ç®€å•çš„å¹‚å¾‹ç‰¹å¾ï¼Œè°±æŒ‡æ•°ä¸º$ 2.45\pm 0.11_{stat}$ã€‚\emph{Fermi}-LATåˆ†æå‘ç°åœ¨LHAASO J2108$+$åŒºåŸŸå†…å­˜åœ¨ä¸€ä¸ªè°±éå¸¸è½¯çš„ç‚¹æºï¼Œä¸æ¥è‡ªåˆ†ç±»ä¸ºå¤©ä½“è§†ç•Œçš„æé™æ ¸ä¸Šé€ƒé€¸å­çš„å…‰å­ç›´ç©¿å±‚çº§æè¿°è¡¨åˆ†æç»“æœä¸€è‡´ã€‚\textit{XMM-Newton}åˆ†æåœ¨æ³¢æ®µä¸º2è‡³7åƒç”µå­ä¼ç‰¹ï¼ˆkeVï¼‰çš„èŒƒå›´å†…æœªæ£€æµ‹åˆ°è¯¥æºã€‚å®½å¸¦é¢‘è°±å¯ä»¥è¢«è§£é‡Šä¸ºæ˜¯è„‰å†²æ˜Ÿå’Œè„‰å†²æ˜Ÿé£æ˜Ÿäº‘ç³»ç»Ÿçš„è¡¨ç°ï¼Œå…¶ä¸­åƒå…†ç”µå­ä¼ç‰¹ä¼½é©¬å°„çº¿å‘å°„æ¥è‡ªä¸€ä¸ªæœªè¯†åˆ«çš„è„‰å†²æ˜Ÿï¼Œè€ŒXå°„çº¿å’Œå¤ªç”µå­ä¼ç‰¹å‘å°„å½’å› äºè„‰å†²æ˜Ÿé£æ˜Ÿäº‘å†…çš„ç”µå­åŒæ­¥è¾å°„å’Œé€†åº·æ™®é¡¿æ•£å°„ã€‚åœ¨è¿™ç§è½»å­åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬çš„Xå°„çº¿ä¸Šé™ä¸ºç£åœºæä¾›äº†ä¸¥æ ¼çš„çº¦æŸï¼Œç£åœºå°äºç­‰äºçº¦ä¸ºç±³è§éŸ¦é«˜æ–¯æ´›æ³•èƒ½è®¡ç®—çš„æ”¾å¤§æ•°é‡å¯¹è¾“å…¥é¡¹é€šè¿‡æœ€å°åŒ–ä¸”è¿ç»“ä¸Šä¸€ä¸ªç‰¹å¾è§’Î¸æˆ–æ¥è¿‘Î¾å¤„æ ‡é‡çš„æ•°ä¹˜ä»¥Î³åœ¨è§’é¢‘ç‡ä¸ºÏ‰ç”µç£æ³¢çš„ç›¸å¯¹è¯¯å·®çš„ä¸Šé™ã€‚æ ¹æ®æˆ‘ä»¬çš„ç ”ç©¶æ¨æ–­è¯¥ç£åœºå¼ºåº¦åº”å°äºæˆ–ç­‰äºçº¦ä¸º 0.3å¾®ç‰¹æ–¯æ‹‰ ã€‚è¿™ä¸å…¶ä»–å¯¹å°„ç”µæºçš„åˆ†ç±»ç‰¹å¾ç¬¦åˆéå¸¸å¼ºçš„æŒ‡ç¤ºæˆ–åŒ…å«ä¸­å¿ƒèƒ½é‡æ’æ˜Ÿçš„ä¸€ç§èƒ½é‡æƒ…å½¢.ã€‚è¿™ä¸ä»…æ˜¾ç¤ºäº†å®ƒä»¬åœ¨ç‰¹å®šç±»å‹é«˜èƒ½ç²’å­é—®é¢˜ä¸­çš„é‡è¦æ€§è¿˜å¯¹æ–°å‹é¢„æµ‹ç†è®ºå’Œç°æœ‰çš„è®¡ç®—é—®é¢˜ä¸­éƒ½æœ‰ç€æ·±è¿œçš„å½±å“æˆ–å¯å‘æ€§æŒ‡å¼•è¿›ä¸€æ­¥å¸®åŠ©æˆ‘ä»¬åˆ©ç”¨æ¥è‡ªåŒä¸€æŠ€æœ¯çš„ç‰¹å¾å¹¶å°†å…¶ç”¨åœ¨åˆ†å­å…‰çº¿ç§»åŠ¨æ»¤é•œäº§ç”Ÿçš„å•å‘é˜€ä¸€èˆ¬ç‚¹ç­›é€‰æŠ˜å°„é¢å’Œä¼˜åŒ–çš„è”åˆæ‰©å¢æ‹‰æ›¼åå‘æ¯”è¾ƒæè¿°é¢†åŸŸä¸­ä¹Ÿå¯ä»¥åŸºäºæ­¤è®¾æ¨¡æ‹Ÿæ³•å¯¹æ¶µç›–æœ€ç»ˆå…‰è°±èƒ½é‡åˆ†å¸ƒä¸å…‰å˜æ›²çº¿ç­‰çš„ç‰©ç†æ¨¡å‹è¿›è¡Œæ„å»ºã€‚è¿™äº›å‘ç°æœ‰åŠ©äºæˆ‘ä»¬æ›´æ·±å…¥åœ°ç†è§£å®‡å®™ä¸­çš„é«˜èƒ½ç°è±¡å’Œç‰©ç†è¿‡ç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01934v1">PDF</a> 12 pages, 4 figures, submitted to The Astrophysical Journal</p>
<p><strong>Summary</strong><br>    æŠ¥å‘Šåˆ©ç”¨VERITASã€HAWCã€\emph{Fermi}-LATåŠ\textit{XMM-Newton}è§‚æµ‹è¶…é«˜èƒ½ä¼½é©¬å°„çº¿æºLHAASO J2108$+$5157çš„ç»“æœã€‚HAWCæ•°æ®æ˜¾ç¤ºè¯¥æºåœ¨3è‡³146TeVé—´å‘å°„æ˜¾è‘—ï¼Œæ”¯æŒæ‰©å±•æºæ¨¡å‹ï¼Œå…‰è°±æŒ‡æ•°çº¦ä¸º2.45ã€‚Fermi-LATåˆ†æå‘ç°è¯¥åŒºåŸŸæœ‰ä¸€ä¸ªéå¸¸è½¯è°±çš„ç‚¹æºã€‚XMM-Newtonåˆ†ææœªåœ¨2-7keVæ³¢æ®µæ£€æµ‹åˆ°è¯¥æºã€‚æ¨æµ‹å…¶ä¸ºä¸€ä¸ªè„‰å†²æ˜Ÿå’Œè„‰å†²æ˜Ÿé£æ˜Ÿäº‘ç³»ç»Ÿï¼Œä¼½é©¬å°„çº¿å‘å°„æºè‡ªæœªè¯†åˆ«è„‰å†²æ˜Ÿï¼ŒXå°„çº¿å’ŒTeVå‘å°„å½’å› äºåŒæ­¥è¾å°„å’Œé€†åº·æ™®é¡¿æ•£å°„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨å¤šç§æœ›è¿œé•œï¼ˆVERITASã€HAWCã€\emph{Fermi}-LATã€\textit{XMM-Newton}ï¼‰è§‚æµ‹è¶…é«˜èƒ½ä¼½é©¬å°„çº¿æºLHAASO J2108$+$5157ã€‚</li>
<li>HAWCæ•°æ®æ˜¾ç¤ºè¯¥æºåœ¨3è‡³146TeVé—´å‘å°„æ˜¾è‘—ï¼Œç¬¦åˆæ‰©å±•æºæ¨¡å‹ï¼Œå…‰è°±æŒ‡æ•°ä¸º2.45ã€‚</li>
<li>\emph{Fermi}-LATå‘ç°ä¸€ä¸ªè½¯è°±ç‚¹æºå­˜åœ¨äºLHAASO J2108$+$5157åŒºåŸŸã€‚</li>
<li>\textit{XMM-Newton}åœ¨2-7keVæ³¢æ®µæœªæ£€æµ‹åˆ°è¯¥æºã€‚</li>
<li>æºå¯èƒ½æ˜¯ä¸€ä¸ªè„‰å†²æ˜Ÿå’Œè„‰å†²æ˜Ÿé£æ˜Ÿäº‘ç³»ç»Ÿã€‚</li>
<li>ä¼½é©¬å°„çº¿å‘å°„å¯èƒ½æºè‡ªæœªè¯†åˆ«çš„è„‰å†²æ˜Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01934">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01934v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01934v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01934v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="IAUNet-Instance-Aware-U-Net"><a href="#IAUNet-Instance-Aware-U-Net" class="headerlink" title="IAUNet: Instance-Aware U-Net"></a>IAUNet: Instance-Aware U-Net</h2><p><strong>Authors:Yaroslav Prytula, Illia Tsiporenko, Ali Zeynalli, Dmytro Fishman</strong></p>
<p>Instance segmentation is critical in biomedical imaging to accurately distinguish individual objects like cells, which often overlap and vary in size. Recent query-based methods, where object queries guide segmentation, have shown strong performance. While U-Net has been a go-to architecture in medical image segmentation, its potential in query-based approaches remains largely unexplored. In this work, we present IAUNet, a novel query-based U-Net architecture. The core design features a full U-Net architecture, enhanced by a novel lightweight convolutional Pixel decoder, making the model more efficient and reducing the number of parameters. Additionally, we propose a Transformer decoder that refines object-specific features across multiple scales. Finally, we introduce the 2025 Revvity Full Cell Segmentation Dataset, a unique resource with detailed annotations of overlapping cell cytoplasm in brightfield images, setting a new benchmark for biomedical instance segmentation. Experiments on multiple public datasets and our own show that IAUNet outperforms most state-of-the-art fully convolutional, transformer-based, and query-based models and cell segmentation-specific models, setting a strong baseline for cell instance segmentation tasks. Code is available at <a target="_blank" rel="noopener" href="https://github.com/SlavkoPrytula/IAUNet">https://github.com/SlavkoPrytula/IAUNet</a> </p>
<blockquote>
<p>å®ä¾‹åˆ†å‰²åœ¨ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿå‡†ç¡®åŒºåˆ†å¦‚ç»†èƒç­‰é‡å ä¸”å¤§å°å„å¼‚çš„ä¸ªä½“å¯¹è±¡ã€‚åŸºäºæŸ¥è¯¢çš„è¿‘æœŸæ–¹æ³•è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå…¶ä¸­å¯¹è±¡æŸ¥è¯¢å¼•å¯¼åˆ†å‰²ã€‚è™½ç„¶U-Netåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å·²æˆä¸ºé¦–é€‰æ¶æ„ï¼Œä½†å…¶åŸºäºæŸ¥è¯¢çš„æ–¹æ³•çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†IAUNetï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŸºäºæŸ¥è¯¢çš„U-Netæ¶æ„ã€‚æ ¸å¿ƒè®¾è®¡ç‰¹ç‚¹æ˜¯ä¸€ä¸ªå®Œæ•´çš„U-Netæ¶æ„ï¼Œé€šè¿‡æ–°å‹è½»é‡çº§å·ç§¯åƒç´ è§£ç å™¨è¿›è¡Œå¢å¼ºï¼Œä½¿æ¨¡å‹æ›´é«˜æ•ˆå¹¶å‡å°‘å‚æ•°æ•°é‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªTransformerè§£ç å™¨ï¼Œè¯¥è§£ç å™¨å¯ä»¥åœ¨å¤šä¸ªå°ºåº¦ä¸Šä¼˜åŒ–ç‰¹å®šå¯¹è±¡çš„ç‰¹å¾ã€‚æœ€åï¼Œæˆ‘ä»¬æ¨å‡ºäº†2025å¹´Revvityå…¨ç»†èƒåˆ†å‰²æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªç‹¬ç‰¹çš„èµ„æºï¼Œå¯¹æ˜åœºå›¾åƒä¸­é‡å çš„ç»†èƒè´¨è¿›è¡Œäº†è¯¦ç»†çš„æ³¨é‡Šï¼Œä¸ºç”Ÿç‰©åŒ»å­¦å®ä¾‹åˆ†å‰²è®¾å®šäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ã€‚åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†å’Œæˆ‘ä»¬è‡ªå·±çš„æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒIAUNetåœ¨å¤§å¤šæ•°æœ€å…ˆè¿›çš„å…¨å·ç§¯ã€åŸºäºTransformerå’ŒåŸºäºæŸ¥è¯¢çš„æ¨¡å‹ä»¥åŠç»†èƒåˆ†å‰²ç‰¹å®šæ¨¡å‹ä¸­è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œä¸ºç»†èƒå®ä¾‹åˆ†å‰²ä»»åŠ¡è®¾å®šäº†å¼ºæœ‰åŠ›çš„åŸºå‡†ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/SlavkoPrytula/IAUNet">https://github.com/SlavkoPrytula/IAUNet</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01928v1">PDF</a> Published in CVPR Workshops (CVMI), 2025. Project   page&#x2F;code&#x2F;models&#x2F;dataset:   $\href{<a target="_blank" rel="noopener" href="https://slavkoprytula.github.io/IAUNet/%7D%7B/text%7Bthis">https://slavkoprytula.github.io/IAUNet/}{\text{this</a> https URL}}$</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä»‹ç»äº†ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­å®ä¾‹åˆ†å‰²çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯åœ¨åŒºåˆ†é‡å å’Œå¤§å°ä¸åŒçš„å¯¹è±¡ï¼ˆå¦‚ç»†èƒï¼‰æ—¶ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæŸ¥è¯¢çš„U-Netæ¶æ„IAUNetï¼Œå…·æœ‰å…¨æ–°çš„è½»é‡çº§å·ç§¯åƒç´ è§£ç å™¨å’ŒTransformerè§£ç å™¨ï¼Œä»¥æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ç‹¬ç‰¹çš„Revvityå…¨ç»†èƒåˆ†å‰²æ•°æ®é›†ï¼Œä¸ºç”Ÿç‰©åŒ»å­¦å®ä¾‹åˆ†å‰²è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚å®éªŒè¡¨æ˜ï¼ŒIAUNetåœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå¤§å¤šæ•°å…ˆè¿›çš„å…¨å·ç§¯ã€åŸºäºTransformerå’ŒåŸºäºæŸ¥è¯¢çš„æ¨¡å‹ä»¥åŠç»†èƒåˆ†å‰²ç‰¹å®šæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®ä¾‹åˆ†å‰²åœ¨ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒºåˆ†é‡å å’Œå¤§å°ä¸åŒçš„å¯¹è±¡ï¼ˆå¦‚ç»†èƒï¼‰æ—¶ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºæŸ¥è¯¢çš„U-Netæ¶æ„IAUNetã€‚</li>
<li>IAUNetå…·æœ‰è½»é‡çº§å·ç§¯åƒç´ è§£ç å™¨å’ŒTransformerè§£ç å™¨ï¼Œæé«˜äº†æ¨¡å‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥äº†ç‹¬ç‰¹çš„Revvityå…¨ç»†èƒåˆ†å‰²æ•°æ®é›†ï¼Œä¸ºç”Ÿç‰©åŒ»å­¦å®ä¾‹åˆ†å‰²æä¾›äº†æ–°åŸºå‡†ã€‚</li>
<li>IAUNetåœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/SlavkoPrytula/IAUNet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/SlavkoPrytula/IAUNetæ‰¾åˆ°ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01928">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01928v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01928v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01928v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01928v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01928v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Medical-Image-De-Identification-Resources-Synthetic-DICOM-Data-and-Tools-for-Validation"><a href="#Medical-Image-De-Identification-Resources-Synthetic-DICOM-Data-and-Tools-for-Validation" class="headerlink" title="Medical Image De-Identification Resources: Synthetic DICOM Data and   Tools for Validation"></a>Medical Image De-Identification Resources: Synthetic DICOM Data and   Tools for Validation</h2><p><strong>Authors:Michael W. Rutherford, Tracy Nolan, Linmin Pei, Ulrike Wagner, Qinyan Pan, Phillip Farmer, Kirk Smith, Benjamin Kopchick, Laura Opsahl-Ong, Granger Sutton, David Clunie, Keyvan Farahani, Fred Prior</strong></p>
<p>Medical imaging research increasingly depends on large-scale data sharing to promote reproducibility and train Artificial Intelligence (AI) models. Ensuring patient privacy remains a significant challenge for open-access data sharing. Digital Imaging and Communications in Medicine (DICOM), the global standard data format for medical imaging, encodes both essential clinical metadata and extensive protected health information (PHI) and personally identifiable information (PII). Effective de-identification must remove identifiers, preserve scientific utility, and maintain DICOM validity. Tools exist to perform de-identification, but few assess its effectiveness, and most rely on subjective reviews, limiting reproducibility and regulatory confidence. To address this gap, we developed an openly accessible DICOM dataset infused with synthetic PHI&#x2F;PII and an evaluation framework for benchmarking image de-identification workflows. The Medical Image de-identification (MIDI) dataset was built using publicly available de-identified data from The Cancer Imaging Archive (TCIA). It includes 538 subjects (216 for validation, 322 for testing), 605 studies, 708 series, and 53,581 DICOM image instances. These span multiple vendors, imaging modalities, and cancer types. Synthetic PHI and PII were embedded into structured data elements, plain text data elements, and pixel data to simulate real-world identity leaks encountered by TCIA curation teams. Accompanying evaluation tools include a Python script, answer keys (known truth), and mapping files that enable automated comparison of curated data against expected transformations. The framework is aligned with the HIPAA Privacy Rule â€œSafe Harborâ€ method, DICOM PS3.15 Confidentiality Profiles, and TCIA best practices. It supports objective, standards-driven evaluation of de-identification workflows, promoting safer and more consistent medical image sharing. </p>
<blockquote>
<p>åŒ»å­¦æˆåƒç ”ç©¶è¶Šæ¥è¶Šä¾èµ–äºå¤§è§„æ¨¡çš„æ•°æ®å…±äº«ï¼Œä»¥ä¿ƒè¿›ç»“æœçš„é‡å¤éªŒè¯å’Œè®­ç»ƒäººå·¥æ™ºèƒ½æ¨¡å‹ã€‚ç„¶è€Œï¼Œç¡®ä¿æ‚£è€…éšç§åœ¨å¼€æ”¾æ•°æ®å…±äº«ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚DICOMï¼ˆåŒ»å­¦æ•°å­—æˆåƒå’Œé€šä¿¡ï¼‰æ˜¯å…¨çƒåŒ»å­¦æˆåƒçš„æ ‡å‡†æ•°æ®æ ¼å¼ï¼Œå®ƒæ—¢åŒ…å«åŸºæœ¬çš„ä¸´åºŠå…ƒæ•°æ®ï¼Œä¹ŸåŒ…å«å¤§é‡çš„å—ä¿æŠ¤å¥åº·ä¿¡æ¯ï¼ˆPHIï¼‰å’Œä¸ªäººèº«ä»½ä¿¡æ¯ï¼ˆPIIï¼‰ã€‚æœ‰æ•ˆçš„åŒ¿åå¤„ç†å¿…é¡»æ¶ˆé™¤æ ‡è¯†ç¬¦ï¼ŒåŒæ—¶ä¿ç•™ç§‘å­¦æ•ˆç”¨å¹¶ç»´æŒDICOMçš„æœ‰æ•ˆæ€§ã€‚è™½ç„¶å­˜åœ¨è¿›è¡ŒåŒ¿åå¤„ç†çš„å·¥å…·ï¼Œä½†å¾ˆå°‘æœ‰å·¥å…·è¯„ä¼°å…¶æœ‰æ•ˆæ€§ï¼Œå¤§å¤šæ•°å·¥å…·éƒ½ä¾èµ–äºä¸»è§‚å®¡æŸ¥ï¼Œè¿™é™åˆ¶äº†ç»“æœçš„é‡å¤éªŒè¯å’Œç›‘ç®¡ä¿¡å¿ƒã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯å…¬å¼€è®¿é—®çš„DICOMæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ³¨å…¥äº†åˆæˆPHI&#x2F;PIIå’Œä¸€ä¸ªç”¨äºè¯„ä¼°å›¾åƒåŒ¿åå¤„ç†å·¥ä½œæµç¨‹çš„åŸºå‡†æ¡†æ¶ã€‚åŒ»å­¦å›¾åƒåŒ¿åå¤„ç†ï¼ˆMIDIï¼‰æ•°æ®é›†æ˜¯ä½¿ç”¨å¯ä»The Cancer Imaging Archiveï¼ˆTCIAï¼‰è·å¾—çš„å·²åŒ¿åå¤„ç†çš„æ•°æ®æ„å»ºçš„ã€‚å®ƒåŒ…å«538åå—è¯•è€…ï¼ˆ216åç”¨äºéªŒè¯ï¼Œ322åç”¨äºæµ‹è¯•ï¼‰ã€605é¡¹ç ”ç©¶ã€708ä¸ªç³»åˆ—å’Œ53581ä¸ªDICOMå›¾åƒå®ä¾‹ã€‚è¿™äº›å›¾åƒè·¨è¶Šäº†å¤šä¸ªä¾›åº”å•†ã€æˆåƒæ¨¡å¼å’Œç™Œç—‡ç±»å‹ã€‚åˆæˆPHIå’ŒPIIè¢«åµŒå…¥åˆ°ç»“æ„åŒ–æ•°æ®å…ƒç´ ã€çº¯æ–‡æœ¬æ•°æ®å…ƒç´ å’Œåƒç´ æ•°æ®ä¸­ï¼Œä»¥æ¨¡æ‹ŸTCIAç­–åˆ’å›¢é˜Ÿåœ¨ç°å®ä¸–ç•Œä¸­é‡åˆ°çš„èº«ä»½æ³„éœ²æƒ…å†µã€‚ä¼´éšçš„è¯„ä¼°å·¥å…·åŒ…æ‹¬Pythonè„šæœ¬ã€ç­”æ¡ˆå¯†é’¥ï¼ˆå·²çŸ¥äº‹å®ï¼‰å’Œæ˜ å°„æ–‡ä»¶ï¼Œè¿™äº›æ–‡ä»¶èƒ½å¤Ÿå®ç°å¯¹å·²ç­–åˆ’æ•°æ®ä¸é¢„æœŸè½¬æ¢çš„è‡ªåŠ¨æ¯”è¾ƒã€‚è¯¥æ¡†æ¶ä¸HIPAAéšç§è§„åˆ™çš„â€œå®‰å…¨æ¸¯â€æ–¹æ³•ã€DICOM PS3.15ä¿å¯†é…ç½®æ–‡ä»¶ä»¥åŠTCIAçš„æœ€ä½³å®è·µç›¸ä¸€è‡´ã€‚å®ƒæ”¯æŒå¯¹åŒ¿åå¤„ç†å·¥ä½œæµç¨‹è¿›è¡Œå®¢è§‚ã€ä»¥æ ‡å‡†é©±åŠ¨çš„è¯„ä»·ï¼Œä»è€Œä¿ƒè¿›æ›´å®‰å…¨ã€æ›´ä¸€è‡´çš„åŒ»å­¦å›¾åƒå…±äº«ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01889v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†åŒ»å­¦æˆåƒç ”ç©¶ä¸­å¯¹å¤§è§„æ¨¡æ•°æ®å…±äº«çš„éœ€æ±‚ï¼ŒåŒæ—¶ç¡®ä¿æ‚£è€…éšç§çš„å¼€æ”¾è®¿é—®æ•°æ®å…±äº«æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚ä¸ºè§£å†³ç°æœ‰å·¥å…·åœ¨è¯„ä¼°å»æ ‡è¯†åŒ–æ•ˆæœæ–¹é¢çš„ä¸è¶³ï¼Œæœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªå…¬å¼€å¯è®¿é—®çš„DICOMæ•°æ®é›†ï¼Œå¹¶æ³¨å…¥äº†åˆæˆä¸ªäººä¿¡æ¯å’Œèº«ä»½è¯†åˆ«ä¿¡æ¯ï¼ŒåŒæ—¶æ„å»ºäº†ä¸€ä¸ªè¯„ä¼°å»æ ‡è¯†åŒ–å·¥ä½œæµç¨‹çš„æ¡†æ¶ã€‚è¯¥æ•°æ®é›†ä½¿ç”¨TCIAçš„å…¬å¼€å»æ ‡è¯†æ•°æ®æ„å»ºï¼ŒåŒ…æ‹¬å¤šä¸ªä¾›åº”å•†ã€æˆåƒæ¨¡å¼å’Œç™Œç—‡ç±»å‹çš„å›¾åƒã€‚è¯¥æ¡†æ¶æ”¯æŒå¯¹å»æ ‡è¯†åŒ–å·¥ä½œæµç¨‹è¿›è¡Œå®¢è§‚ã€æ ‡å‡†åŒ–çš„è¯„ä¼°ï¼Œä¿ƒè¿›äº†æ›´å®‰å…¨ã€æ›´ä¸€è‡´çš„åŒ»å­¦å›¾åƒå…±äº«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒç ”ç©¶ä¾èµ–å¤§è§„æ¨¡æ•°æ®å…±äº«ä»¥ä¿ƒè¿›å¯é‡å¤æ€§å’Œäººå·¥æ™ºèƒ½æ¨¡å‹è®­ç»ƒã€‚</li>
<li>æ•°æ®å…±äº«ä¸­ç¡®ä¿æ‚£è€…éšç§æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>DICOMä½œä¸ºå…¨çƒåŒ»å­¦æˆåƒæ•°æ®æ ¼å¼æ ‡å‡†ï¼ŒåŒ…å«å…³é”®ä¸´åºŠå…ƒæ•°æ®å’Œå—ä¿æŠ¤çš„å¥åº·ä¿¡æ¯ï¼ˆPHIï¼‰åŠä¸ªäººè¯†åˆ«ä¿¡æ¯ï¼ˆPIIï¼‰ã€‚</li>
<li>å»æ ‡è¯†åŒ–å·¥å…·éœ€ç§»é™¤æ ‡è¯†ç¬¦ã€ä¿æŒç§‘å­¦æ•ˆç”¨å¹¶ç»´æŒDICOMæœ‰æ•ˆæ€§ã€‚</li>
<li>ç°æœ‰è¯„ä¼°å·¥å…·åœ¨è¯„ä¼°å»æ ‡è¯†åŒ–æ•ˆæœæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œæœ¬ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªå…¬å¼€çš„DICOMæ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ã€‚</li>
<li>MIDIæ•°æ®é›†ä½¿ç”¨TCIAçš„å…¬å¼€å»æ ‡è¯†æ•°æ®æ„å»ºï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­çš„èº«ä»½æ³„éœ²æƒ…å†µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01889">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01889v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01889v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01889v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01889v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01889v1/page_5_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01889v1/page_5_2.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Large-Kernel-MedNeXt-for-Breast-Tumor-Segmentation-and-Self-Normalizing-Network-for-pCR-Classification-in-Magnetic-Resonance-Images"><a href="#Large-Kernel-MedNeXt-for-Breast-Tumor-Segmentation-and-Self-Normalizing-Network-for-pCR-Classification-in-Magnetic-Resonance-Images" class="headerlink" title="Large Kernel MedNeXt for Breast Tumor Segmentation and Self-Normalizing   Network for pCR Classification in Magnetic Resonance Images"></a>Large Kernel MedNeXt for Breast Tumor Segmentation and Self-Normalizing   Network for pCR Classification in Magnetic Resonance Images</h2><p><strong>Authors:Toufiq Musah</strong></p>
<p>Accurate breast tumor segmentation in dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) is important for downstream tasks such as pathological complete response (pCR) assessment. In this work, we address both segmentation and pCR classification using the large-scale MAMA-MIA DCE-MRI dataset. We employ a large-kernel MedNeXt architecture with a two-stage training strategy that expands the receptive field from 3x3x3 to 5x5x5 kernels using the UpKern algorithm. This approach allows stable transfer of learned features to larger kernels, improving segmentation performance on the unseen validation set. An ensemble of large-kernel models achieved a Dice score of 0.67 and a normalized Hausdorff Distance (NormHD) of 0.24. For pCR classification, we trained a self-normalizing network (SNN) on radiomic features extracted from the predicted segmentations and first post-contrast DCE-MRI, reaching an average balanced accuracy of 57%, and up to 75% in some subgroups. Our findings highlight the benefits of combining larger receptive fields and radiomics-driven classification while motivating future work on advanced ensembling and the integration of clinical variables to further improve performance and generalization. Code: <a target="_blank" rel="noopener" href="https://github.com/toufiqmusah/caladan-mama-mia.git">https://github.com/toufiqmusah/caladan-mama-mia.git</a> </p>
<blockquote>
<p>åœ¨åŠ¨æ€å¯¹æ¯”å¢å¼ºç£å…±æŒ¯æˆåƒï¼ˆDCE-MRIï¼‰ä¸­è¿›è¡Œç²¾ç¡®çš„ä¹³è…ºè‚¿ç˜¤åˆ†å‰²å¯¹äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚ç—…ç†å®Œå…¨ååº”ï¼ˆpCRï¼‰è¯„ä¼°ï¼‰éå¸¸é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹åˆ†å‰²å’ŒpCRåˆ†ç±»ä½¿ç”¨å¤§è§„æ¨¡çš„MAMA-MIA DCE-MRIæ•°æ®é›†ã€‚æˆ‘ä»¬é‡‡ç”¨å¤§å†…æ ¸MedNeXtæ¶æ„å’Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä½¿ç”¨UpKernç®—æ³•å°†æ„Ÿå—é‡ä»3x3x3æ‰©å±•åˆ°5x5x5å†…æ ¸ã€‚è¿™ç§æ–¹æ³•å…è®¸å°†å­¦ä¹ åˆ°çš„ç‰¹å¾ç¨³å®šåœ°è½¬ç§»åˆ°æ›´å¤§çš„å†…æ ¸ï¼Œä»è€Œæé«˜äº†æœªè§éªŒè¯é›†çš„åˆ†å‰²æ€§èƒ½ã€‚å¤§å†…æ ¸æ¨¡å‹ç»„åˆè¾¾åˆ°äº†Diceåˆ†æ•°ä¸º0.67ï¼Œå½’ä¸€åŒ–Hausdorffè·ç¦»ï¼ˆNormHDï¼‰ä¸º0.24ã€‚å¯¹äºpCRåˆ†ç±»ï¼Œæˆ‘ä»¬åœ¨ä»é¢„æµ‹çš„åˆ†å‰²å’Œé¦–æ¬¡å¯¹æ¯”å¢å¼ºåçš„DCE-MRIä¸­æå–çš„æ”¾å°„å­¦ç‰¹å¾ä¸Šè®­ç»ƒäº†è‡ªå½’ä¸€åŒ–ç½‘ç»œï¼ˆSNNï¼‰ï¼Œè¾¾åˆ°å¹³å‡å¹³è¡¡ç²¾åº¦ä¸º57%ï¼ŒæŸäº›å­ç»„ç”šè‡³é«˜è¾¾75%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†ç»“åˆæ›´å¤§çš„æ„Ÿå—é‡å’Œæ”¾å°„å­¦é©±åŠ¨åˆ†ç±»çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶é¼“åŠ±æœªæ¥çš„å·¥ä½œåœ¨é«˜çº§é›†æˆå’Œä¸´åºŠå˜é‡çš„æ•´åˆæ–¹é¢è¿›ä¸€æ­¥æ”¹è¿›æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/toufiqmusah/caladan-mama-mia.git%E3%80%82">https://github.com/toufiqmusah/caladan-mama-mia.gitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01831v1">PDF</a> 8 pages, 2 figures, 2 tables, Accepted at MICCAI 2025 Deep-Breath   Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆ©ç”¨å¤§å‹æ ¸MedNeXtæ¶æ„ä¸UpKernç®—æ³•æå‡åŠ¨æ€å¢å¼ºç£å…±æŒ¯æˆåƒï¼ˆDCE-MRIï¼‰ä¸­çš„ä¹³è…ºè‚¿ç˜¤åˆ†å‰²å‡†ç¡®æ€§ã€‚ä½¿ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ‰©å¤§æ„Ÿå—é‡ä»3x3x3è‡³5x5x5æ ¸ï¼Œé€šè¿‡å¤§å‹æ ¸æ¨¡å‹ç»„åˆå®ç°Diceè¯„åˆ†0.67å’Œå½’ä¸€åŒ–Hausdorffè·ç¦»ï¼ˆNormHDï¼‰0.24çš„ä¼˜å¼‚åˆ†å‰²æ•ˆæœã€‚é’ˆå¯¹ç—…ç†å®Œå…¨ååº”ï¼ˆpCRï¼‰åˆ†ç±»ï¼Œåˆ©ç”¨é¢„æµ‹çš„åˆ†å‰²å’Œé¦–æ¬¡å¯¹æ¯”å¢å¼ºDCE-MRIæå–çš„æ”¾å°„å­¦ç‰¹å¾è®­ç»ƒè‡ªå½’ä¸€åŒ–ç½‘ç»œï¼ˆSNNï¼‰ï¼Œå¹³å‡å¹³è¡¡ç²¾åº¦è¾¾57%ï¼Œéƒ¨åˆ†å­ç»„é«˜è¾¾75%ã€‚ç ”ç©¶ç»“åˆäº†æ›´å¤§çš„æ„Ÿå—é‡å’Œæ”¾å°„å­¦ç‰¹å¾é©±åŠ¨çš„åˆ†ç±»ä¼˜åŠ¿ï¼Œå¹¶é¼“åŠ±æœªæ¥åœ¨æ›´é«˜çº§çš„é›†æˆå’Œä¸´åºŠå˜é‡æ•´åˆæ–¹é¢å¼€å±•è¿›ä¸€æ­¥ç ”ç©¶ä»¥æé«˜æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨å¤§å‹æ ¸MedNeXtæ¶æ„ä¸UpKernç®—æ³•æå‡DCE-MRIä¸­çš„ä¹³è…ºè‚¿ç˜¤åˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
<li>ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ‰©å¤§æ¨¡å‹æ„Ÿå—é‡ï¼Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>å¤§å‹æ ¸æ¨¡å‹ç»„åˆå®ç°è¾ƒé«˜çš„Diceè¯„åˆ†å’ŒNormHDå€¼ã€‚</li>
<li>åˆ©ç”¨é¢„æµ‹çš„åˆ†å‰²å’Œé¦–æ¬¡å¯¹æ¯”å¢å¼ºDCE-MRIæå–çš„æ”¾å°„å­¦ç‰¹å¾è¿›è¡ŒpCRåˆ†ç±»ã€‚</li>
<li>è‡ªå½’ä¸€åŒ–ç½‘ç»œï¼ˆSNNï¼‰åœ¨pCRåˆ†ç±»ä¸­è¡¨ç°è‰¯å¥½ï¼Œè¾¾åˆ°è¾ƒé«˜çš„å¹³è¡¡ç²¾åº¦ã€‚</li>
<li>ç»“åˆæ›´å¤§çš„æ„Ÿå—é‡å’Œæ”¾å°„å­¦ç‰¹å¾é©±åŠ¨çš„åˆ†ç±»èƒ½æé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01831">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01831v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01831v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01831v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01831v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01831v1/page_4_1.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="M-3-AD-Multi-task-Multi-gate-Mixture-of-Experts-for-Alzheimerâ€™s-Disease-Diagnosis-with-Conversion-Pattern-Modeling"><a href="#M-3-AD-Multi-task-Multi-gate-Mixture-of-Experts-for-Alzheimerâ€™s-Disease-Diagnosis-with-Conversion-Pattern-Modeling" class="headerlink" title="M$^3$AD: Multi-task Multi-gate Mixture of Experts for Alzheimerâ€™s   Disease Diagnosis with Conversion Pattern Modeling"></a>M$^3$AD: Multi-task Multi-gate Mixture of Experts for Alzheimerâ€™s   Disease Diagnosis with Conversion Pattern Modeling</h2><p><strong>Authors:Yufeng Jiang, Hexiao Ding, Hongzhao Chen, Jing Lan, Xinzhi Teng, Gerald W. Y. Cheng, Zongxi Li, Haoran Xie, Jung Sun Yoo, Jing Cai</strong></p>
<p>Alzheimerâ€™s disease (AD) progression follows a complex continuum from normal cognition (NC) through mild cognitive impairment (MCI) to dementia, yet most deep learning approaches oversimplify this into discrete classification tasks. This study introduces M$^3$AD, a novel multi-task multi-gate mixture of experts framework that jointly addresses diagnostic classification and cognitive transition modeling using structural MRI. We incorporate three key innovations: (1) an open-source T1-weighted sMRI preprocessing pipeline, (2) a unified learning framework capturing NC-MCI-AD transition patterns with demographic priors (age, gender, brain volume) for improved generalization, and (3) a customized multi-gate mixture of experts architecture enabling effective multi-task learning with structural MRI alone. The framework employs specialized expert networks for diagnosis-specific pathological patterns while shared experts model common structural features across the cognitive continuum. A two-stage training protocol combines SimMIM pretraining with multi-task fine-tuning for joint optimization. Comprehensive evaluation across six datasets comprising 12,037 T1-weighted sMRI scans demonstrates superior performance: 95.13% accuracy for three-class NC-MCI-AD classification and 99.15% for binary NC-AD classification, representing improvements of 4.69% and 0.55% over state-of-the-art approaches. The multi-task formulation simultaneously achieves 97.76% accuracy in predicting cognitive transition. Our framework outperforms existing methods using fewer modalities and offers a clinically practical solution for early intervention. Code: <a target="_blank" rel="noopener" href="https://github.com/csyfjiang/M3AD">https://github.com/csyfjiang/M3AD</a>. </p>
<blockquote>
<p>é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰çš„è¿›å±•ä»æ­£å¸¸è®¤çŸ¥ï¼ˆNCï¼‰åˆ°è½»åº¦è®¤çŸ¥éšœç¢ï¼ˆMCIï¼‰å†åˆ°ç—´å‘†æ˜¯ä¸€ä¸ªå¤æ‚çš„è¿‡ç¨‹ï¼Œä½†å¤§å¤šæ•°æ·±åº¦å­¦ä¹ çš„æ–¹æ³•éƒ½å°†å…¶ç®€åŒ–ä¸ºç‹¬ç«‹çš„åˆ†ç±»ä»»åŠ¡ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†M$^3$ADï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¤šä»»åŠ¡å¤šé—¨ä¸“å®¶æ··åˆæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ç»“æ„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰è”åˆè§£å†³è¯Šæ–­åˆ†ç±»å’Œè®¤çŸ¥è¿‡æ¸¡å»ºæ¨¡é—®é¢˜ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰å¼€æºçš„T1åŠ æƒsMRIé¢„å¤„ç†ç®¡é“ï¼Œï¼ˆ2ï¼‰ä¸€ä¸ªç»Ÿä¸€çš„å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäººå£ç»Ÿè®¡å­¦å…ˆéªŒä¿¡æ¯ï¼ˆå¹´é¾„ã€æ€§åˆ«ã€è„‘å®¹é‡ï¼‰æ¥æ•æ‰NC-MCI-ADè¿‡æ¸¡æ¨¡å¼ï¼Œä»¥æé«˜é€šç”¨æ€§ï¼Œï¼ˆ3ï¼‰å®šåˆ¶çš„å¤šé—¨ä¸“å®¶æ··åˆæ¶æ„ï¼Œèƒ½å¤Ÿåˆ©ç”¨ç»“æ„MRIè¿›è¡Œæœ‰æ•ˆçš„å¤šä»»åŠ¡å­¦ä¹ ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸“é—¨çš„ä¸“ä¸šç½‘ç»œæ¥é’ˆå¯¹è¯Šæ–­çš„ç‰¹å®šç—…ç†æ¨¡å¼ï¼Œè€Œå…±äº«ä¸“å®¶åˆ™å¯¹è®¤çŸ¥è¿ç»­ä½“çš„å¸¸è§ç»“æ„ç‰¹å¾è¿›è¡Œå»ºæ¨¡ã€‚ä¸¤é˜¶æ®µè®­ç»ƒåè®®å°†SimMIMé¢„è®­ç»ƒä¸å¤šä»»åŠ¡å¾®è°ƒç›¸ç»“åˆï¼Œä»¥å®ç°è”åˆä¼˜åŒ–ã€‚åœ¨åŒ…å«12037ä¸ªT1åŠ æƒsMRIæ‰«æçš„å…­ä¸ªæ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œå…¶æ€§èƒ½å“è¶Šï¼šå¯¹äºä¸‰ç±»NC-MCI-ADåˆ†ç±»çš„å‡†ç¡®ç‡ä¸º95.13%ï¼Œå¯¹äºäºŒå…ƒNC-ADåˆ†ç±»çš„å‡†ç¡®ç‡ä¸º99.15%ï¼Œç›¸è¾ƒäºæœ€æ–°æ–¹æ³•åˆ†åˆ«æé«˜äº†4.69%å’Œ0.55%ã€‚å¤šä»»åŠ¡å…¬å¼åŒæ—¶å®ç°äº†é¢„æµ‹è®¤çŸ¥è¿‡æ¸¡çš„97.76%å‡†ç¡®ç‡ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä½¿ç”¨è¾ƒå°‘çš„æ¨¡æ€è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸ºæ—©æœŸå¹²é¢„æä¾›äº†ä¸´åºŠå®ç”¨è§£å†³æ–¹æ¡ˆã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/csyfjiang/M3AD">https://github.com/csyfjiang/M3AD</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01819v1">PDF</a> 11 pages, 6 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºM$^3$ADçš„æ–°å‹å¤šä»»åŠ¡å¤šé—¨æ··åˆä¸“å®¶æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç»“æ„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰è”åˆè§£å†³è¯Šæ–­åˆ†ç±»å’Œè®¤çŸ¥è¿‡æ¸¡å»ºæ¨¡é—®é¢˜ã€‚è¯¥ç ”ç©¶åŒ…å«ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šå¼€æºT1åŠ æƒsMRIé¢„å¤„ç†ç®¡é“ã€ç»Ÿä¸€å­¦ä¹ æ¡†æ¶æ•æ‰NC-MCI-ADè¿‡æ¸¡æ¨¡å¼å¹¶è€ƒè™‘äººå£ç»Ÿè®¡å­¦å…ˆéªŒä¿¡æ¯ä»¥æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œä»¥åŠå®šåˆ¶çš„å¤šé—¨æ··åˆä¸“å®¶æ¶æ„ï¼Œå®ç°æœ‰æ•ˆçš„å¤šä»»åŠ¡å­¦ä¹ ã€‚è¯¥æ¡†æ¶åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸ºæ—©æœŸå¹²é¢„æä¾›äº†å®ç”¨çš„ä¸´åºŠè§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>M$^3$ADæ¡†æ¶é¦–æ¬¡ç»“åˆäº†ç»“æ„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰è¿›è¡Œè¯Šæ–­åˆ†ç±»å’Œè®¤çŸ¥è¿‡æ¸¡å»ºæ¨¡çš„å¤šä»»åŠ¡å­¦ä¹ ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªå¼€æ”¾æºä»£ç çš„T1åŠ æƒsMRIé¢„å¤„ç†ç®¡é“ä½œä¸ºæ•°æ®å¤„ç†åŸºç¡€ã€‚</li>
<li>é‡‡ç”¨ç»Ÿä¸€å­¦ä¹ æ¡†æ¶æ•æ‰NC-MCI-ADè®¤çŸ¥è¿‡æ¸¡æ¨¡å¼ï¼Œè€ƒè™‘äººå£ç»Ÿè®¡å­¦å…ˆéªŒä»¥æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¤šé—¨æ··åˆä¸“å®¶æ¶æ„ä¸“é—¨å¤„ç†è¯Šæ–­ç‰¹å¼‚æ€§ç—…ç†æ¨¡å¼ï¼ŒåŒæ—¶å…±äº«ä¸“å®¶å»ºæ¨¡è®¤çŸ¥è¿ç»­è°±çš„é€šç”¨ç»“æ„ç‰¹å¾ã€‚</li>
<li>é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒåè®®ç»“åˆSimMIMé¢„è®­ç»ƒå’Œå¤šä»»åŠ¡å¾®è°ƒè¿›è¡Œè”åˆä¼˜åŒ–ã€‚</li>
<li>åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºM$^3$ADæ¡†æ¶åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶æ˜¯åœ¨é¢„æµ‹è®¤çŸ¥è¿‡æ¸¡çš„å¤šä»»åŠ¡æ ¼å¼ä¸­ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01819v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01819v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01819v1/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01819v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Skip-priors-and-add-graph-based-anatomical-information-for-point-based-Couinaud-segmentation"><a href="#Skip-priors-and-add-graph-based-anatomical-information-for-point-based-Couinaud-segmentation" class="headerlink" title="Skip priors and add graph-based anatomical information, for point-based   Couinaud segmentation"></a>Skip priors and add graph-based anatomical information, for point-based   Couinaud segmentation</h2><p><strong>Authors:Xiaotong Zhang, Alexander Broersen, Gonnie CM van Erp, Silvia L. Pintea, Jouke Dijkstra</strong></p>
<p>The preoperative planning of liver surgery relies on Couinaud segmentation from computed tomography (CT) images, to reduce the risk of bleeding and guide the resection procedure. Using 3D point-based representations, rather than voxelizing the CT volume, has the benefit of preserving the physical resolution of the CT. However, point-based representations need prior knowledge of the liver vessel structure, which is time consuming to acquire. Here, we propose a point-based method for Couinaud segmentation, without explicitly providing the prior liver vessel structure. To allow the model to learn this anatomical liver vessel structure, we add a graph reasoning module on top of the point features. This adds implicit anatomical information to the model, by learning affinities across point neighborhoods. Our method is competitive on the MSD and LiTS public datasets in Dice coefficient and average surface distance scores compared to four pioneering point-based methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ZhangXiaotong015/GrPn">https://github.com/ZhangXiaotong015/GrPn</a>. </p>
<blockquote>
<p>è‚è„æ‰‹æœ¯çš„æœ¯å‰è§„åˆ’ä¾èµ–äºæ ¹æ®è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒçš„Couinaudåˆ†æ®µï¼Œä»¥å‡å°‘å‡ºè¡€é£é™©å¹¶æŒ‡å¯¼åˆ‡é™¤æ‰‹æœ¯ã€‚ä½¿ç”¨åŸºäº3Dç‚¹çš„è¡¨ç¤ºæ–¹æ³•ï¼ˆè€Œä¸æ˜¯å¯¹CTä½“ç§¯è¿›è¡Œä½“ç´ åŒ–ï¼‰çš„å¥½å¤„æ˜¯ä¿ç•™äº†CTçš„ç‰©ç†åˆ†è¾¨ç‡ã€‚ç„¶è€Œï¼ŒåŸºäºç‚¹çš„è¡¨ç¤ºæ–¹æ³•éœ€è¦å¯¹è‚è„è¡€ç®¡ç»“æ„æœ‰å…ˆéªŒçŸ¥è¯†ï¼Œè€Œè¿™éœ€è¦èŠ±è´¹å¤§é‡æ—¶é—´æ‰èƒ½è·å¾—ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç‚¹çš„Couinaudåˆ†æ®µæ–¹æ³•ï¼Œæ— éœ€æ˜ç¡®æä¾›å…ˆéªŒè‚è„è¡€ç®¡ç»“æ„ã€‚ä¸ºäº†è®©æ¨¡å‹å­¦ä¹ è¿™ç§è§£å‰–è‚è„è¡€ç®¡ç»“æ„ï¼Œæˆ‘ä»¬åœ¨ç‚¹ç‰¹å¾ä¹‹ä¸Šæ·»åŠ äº†ä¸€ä¸ªå›¾æ¨ç†æ¨¡å—ã€‚è¿™é€šè¿‡äº†è§£ç‚¹é‚»åŸŸä¹‹é—´çš„äº²å’ŒåŠ›ï¼Œä¸ºæ¨¡å‹å¢åŠ äº†éšå¼è§£å‰–ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨MSDå’ŒLiTSå…¬å…±æ•°æ®é›†ä¸Šçš„ç‹„å…‹ç³»æ•°å’Œå¹³å‡è¡¨é¢è·ç¦»å¾—åˆ†ä¸å››ç§å…ˆè¿›çš„ç‚¹æ–¹æ³•ç›¸å½“ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ZhangXiaotong015/GrPn%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ZhangXiaotong015/GrPnè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01785v1">PDF</a> Accepted at MICCAI 2025 GRAIL workshop</p>
<p><strong>Summary</strong><br>     åŸºäºè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å›¾åƒçš„Couinaudåˆ†æ®µæ˜¯è‚è„æ‰‹æœ¯æœ¯å‰è§„åˆ’çš„å…³é”®ï¼Œèƒ½é™ä½å‡ºè¡€é£é™©å¹¶æŒ‡å¯¼åˆ‡é™¤æ‰‹æœ¯ã€‚ç›¸è¾ƒäºä½“ç´ åŒ–CTä½“ç§¯çš„ä¼ ç»Ÿæ–¹æ³•ï¼Œé‡‡ç”¨åŸºäºç‚¹çš„ä¸‰ç»´è¡¨ç¤ºå¯ä¿ç•™CTçš„ç‰©ç†åˆ†è¾¨ç‡ã€‚ä½†è¿™ç§æ–¹æ³•éœ€è¦é¢„å…ˆäº†è§£è‚è„è¡€ç®¡ç»“æ„ï¼Œè¿™ä¸€æ­¥éª¤éå¸¸è€—æ—¶ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åŸºäºç‚¹çš„Couinaudåˆ†æ®µæ–¹æ³•ï¼Œæ— éœ€æ˜ç¡®æä¾›è‚è„è¡€ç®¡ç»“æ„çš„å…ˆéªŒçŸ¥è¯†ã€‚ä¸ºäº†ä»¤æ¨¡å‹å­¦ä¹ è¿™ç§è§£å‰–å­¦è‚è„è¡€ç®¡ç»“æ„ï¼Œæˆ‘ä»¬åœ¨ç‚¹ç‰¹å¾ä¹‹ä¸Šå¢åŠ äº†ä¸€ä¸ªå›¾æ¨ç†æ¨¡å—ã€‚è¯¥æ¨¡å—é€šè¿‡å­¦ä¹ ç‚¹é‚»åŸŸé—´çš„äº²å’ŒåŠ›ï¼Œä¸ºæ¨¡å‹å¢åŠ äº†éšå¼è§£å‰–å­¦ä¿¡æ¯ã€‚ç›¸è¾ƒäºå››ç§é¢†å…ˆçš„ç‚¹æ–¹æ³•ï¼Œæœ¬ç ”ç©¶çš„æ–¹æ³•åœ¨MSDå’ŒLiTSå…¬å¼€æ•°æ®é›†ä¸Šçš„Diceç³»æ•°å’Œå¹³å‡è¡¨é¢è·ç¦»å¾—åˆ†ä¸Šå…·æœ‰ç«äº‰åŠ›ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/ZhangXiaotong015/GrPn%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ZhangXiaotong015/GrPnè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Couinaudåˆ†æ®µæ˜¯è‚è„æ‰‹æœ¯æœ¯å‰è§„åˆ’çš„å…³é”®ï¼Œæœ‰åŠ©äºé™ä½æ‰‹æœ¯é£é™©ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•é‡‡ç”¨ä½“ç´ åŒ–CTå›¾åƒï¼Œå­˜åœ¨åˆ†è¾¨ç‡æŸå¤±ã€‚</li>
<li>åŸºäºç‚¹çš„è¡¨ç¤ºæ–¹æ³•èƒ½ä¿ç•™CTçš„ç‰©ç†åˆ†è¾¨ç‡ï¼Œä½†éœ€è¦é¢„å…ˆäº†è§£è‚è„è¡€ç®¡ç»“æ„ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç‚¹çš„Couinaudåˆ†æ®µæ–¹æ³•ï¼Œæ— éœ€æä¾›è‚è„è¡€ç®¡ç»“æ„çš„å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>é€šè¿‡å¢åŠ å›¾æ¨ç†æ¨¡å—ï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ è§£å‰–å­¦è‚è„è¡€ç®¡ç»“æ„ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–é¢†å…ˆæ–¹æ³•ã€‚</li>
<li>ç›¸å…³ä»£ç å¯é€šè¿‡æŒ‡å®šé“¾æ¥è·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01785">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01785v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01785v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01785v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01785v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Register-Anything-Estimating-â€œCorresponding-Promptsâ€-for-Segment-Anything-Model"><a href="#Register-Anything-Estimating-â€œCorresponding-Promptsâ€-for-Segment-Anything-Model" class="headerlink" title="Register Anything: Estimating â€œCorresponding Promptsâ€ for Segment   Anything Model"></a>Register Anything: Estimating â€œCorresponding Promptsâ€ for Segment   Anything Model</h2><p><strong>Authors:Shiqi Huang, Tingfa Xu, Wen Yan, Dean Barratt, Yipeng Hu</strong></p>
<p>Establishing pixel&#x2F;voxel-level or region-level correspondences is the core challenge in image registration. The latter, also known as region-based correspondence representation, leverages paired regions of interest (ROIs) to enable regional matching while preserving fine-grained capability at pixel&#x2F;voxel level. Traditionally, this representation is implemented via two steps: segmenting ROIs in each image then matching them between the two images. In this paper, we simplify this into one step by directly â€œsearching for corresponding promptsâ€, using extensively pre-trained segmentation models (e.g., SAM) for a training-free registration approach, PromptReg. Firstly, we introduce the â€œcorresponding prompt problemâ€, which aims to identify a corresponding Prompt Y in Image Y for any given visual Prompt X in Image X, such that the two respectively prompt-conditioned segmentations are a pair of corresponding ROIs from the two images. Secondly, we present an â€œinverse promptâ€ solution that generates primary and optionally auxiliary prompts, inverting Prompt X into the prompt space of Image Y. Thirdly, we propose a novel registration algorithm that identifies multiple paired corresponding ROIs by marginalizing the inverted Prompt X across both prompt and spatial dimensions. Comprehensive experiments are conducted on five applications of registering 3D prostate MR, 3D abdomen MR, 3D lung CT, 2D histopathology and, as a non-medical example, 2D aerial images. Based on metrics including Dice and target registration errors on anatomical structures, the proposed registration outperforms both intensity-based iterative algorithms and learning-based DDF-predicting networks, even yielding competitive performance with weakly-supervised approaches that require fully-segmented training data. </p>
<blockquote>
<p>åœ¨å›¾åƒé…å‡†ä¸­ï¼Œå»ºç«‹åƒç´ &#x2F;ä½“ç´ çº§åˆ«æˆ–åŒºåŸŸçº§åˆ«çš„å¯¹åº”å…³ç³»æ˜¯æ ¸å¿ƒæŒ‘æˆ˜ã€‚åè€…ä¹Ÿç§°ä¸ºåŸºäºåŒºåŸŸçš„å¯¹åº”å…³ç³»è¡¨ç¤ºï¼Œå®ƒåˆ©ç”¨æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰çš„é…å¯¹ï¼Œå®ç°åŒºåŸŸåŒ¹é…ï¼ŒåŒæ—¶åœ¨åƒç´ &#x2F;ä½“ç´ çº§åˆ«ä¿æŒç²¾ç»†èƒ½åŠ›ã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™ç§è¡¨ç¤ºé€šè¿‡ä¸¤ä¸ªæ­¥éª¤å®ç°ï¼šåœ¨æ¯ä¸ªå›¾åƒä¸­åˆ†å‰²ROIï¼Œç„¶ååœ¨ä¸¤ä¸ªå›¾åƒä¹‹é—´è¿›è¡ŒåŒ¹é…ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ç›´æ¥â€œæœç´¢å¯¹åº”æç¤ºâ€å°†è¿™ä¸€è¿‡ç¨‹ç®€åŒ–ä¸ºä¸€æ­¥ï¼Œåˆ©ç”¨å¹¿æ³›é¢„è®­ç»ƒçš„åˆ†å‰²æ¨¡å‹ï¼ˆä¾‹å¦‚SAMï¼‰å®ç°æ— éœ€è®­ç»ƒå³å¯è¿›è¡Œé…å‡†çš„æ–¹æ³•ï¼Œå³PromptRegã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥â€œå¯¹åº”æç¤ºé—®é¢˜â€ï¼Œæ—¨åœ¨é’ˆå¯¹å›¾åƒXä¸­çš„ä»»ä½•ç»™å®šè§†è§‰æç¤ºXï¼Œæ‰¾åˆ°å›¾åƒYä¸­çš„å¯¹åº”æç¤ºYï¼Œä½¿å¾—è¿™ä¸¤ä¸ªæç¤ºæ¡ä»¶ä¸‹çš„åˆ†å‰²æ˜¯è¿™ä¸¤ä¸ªå›¾åƒä¸­çš„ä¸€å¯¹ç›¸åº”ROIã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§â€œé€†æç¤ºâ€è§£å†³æ–¹æ¡ˆï¼Œç”Ÿæˆä¸»è¦æç¤ºå’Œå¯é€‰çš„è¾…åŠ©æç¤ºï¼Œå°†æç¤ºXåè½¬åˆ°å›¾åƒYçš„æç¤ºç©ºé—´ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é…å‡†ç®—æ³•ï¼Œé€šè¿‡æœ€å°åŒ–æç¤ºXåœ¨æç¤ºå’Œç©ºé—´ç»´åº¦ä¸Šçš„é€†æ˜ å°„ï¼Œæ¥è¯†åˆ«å¤šä¸ªé…å¯¹çš„ç›¸åº”ROIã€‚åœ¨äº”ä¸ªåº”ç”¨ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼ŒåŒ…æ‹¬3Då‰åˆ—è…ºMRã€3Dè…¹éƒ¨MRã€3Dè‚ºéƒ¨CTã€2Dç»„ç»‡ç—…ç†å­¦å’Œä½œä¸ºéåŒ»å­¦ç¤ºä¾‹çš„2Dèˆªç©ºå›¾åƒã€‚åŸºäºç‹„å…‹ç³»æ•°å’Œç›®æ ‡æ³¨å†Œè¯¯å·®ç­‰è§£å‰–å­¦ç»“æ„æŒ‡æ ‡ï¼Œæ‰€æå‡ºçš„é…å‡†æ–¹æ³•ä¼˜äºåŸºäºå¼ºåº¦çš„è¿­ä»£ç®—æ³•å’ŒåŸºäºå­¦ä¹ çš„DDFé¢„æµ‹ç½‘ç»œï¼Œç”šè‡³ä¸éœ€è¦å®Œå…¨åˆ†å‰²è®­ç»ƒæ•°æ®çš„å¼±ç›‘ç£æ–¹æ³•ç«äº‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01697v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹å›¾åƒæ³¨å†Œä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜â€”â€”åƒç´ &#x2F;ä½“ç´ çº§åˆ«æˆ–åŒºåŸŸçº§åˆ«çš„å¯¹åº”é—®é¢˜è¿›è¡Œäº†ç ”ç©¶ã€‚æå‡ºä¸€ç§ç®€åŒ–çš„æ–¹æ³•ï¼Œé€šè¿‡ç›´æ¥â€œå¯»æ‰¾å¯¹åº”çš„æç¤ºâ€æ¥å®ç°åœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹è¿›è¡Œå›¾åƒæ³¨å†Œï¼Œå¹¶ä»‹ç»äº†ä¸€ç§æ–°å‹çš„æ³¨å†Œç®—æ³•ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåº”ç”¨é¢†åŸŸå‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒæ³¨å†Œä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜æ˜¯å»ºç«‹åƒç´ &#x2F;ä½“ç´ çº§åˆ«æˆ–åŒºåŸŸçº§åˆ«çš„å¯¹åº”ã€‚</li>
<li>åŒºåŸŸåŸºäºçš„å¯¹åº”è¡¨ç¤ºåˆ©ç”¨æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰é…å¯¹æ¥å®ç°åŒºåŸŸåŒ¹é…ï¼ŒåŒæ—¶ä¿æŒåƒç´ &#x2F;ä½“ç´ çº§åˆ«çš„ç²¾ç»†èƒ½åŠ›ã€‚</li>
<li>ä¼ ç»Ÿå®ç°æ­¤è¡¨ç¤ºéœ€è¦ä¸¤æ­¥ï¼šåœ¨æ¯ä¸ªå›¾åƒä¸­åˆ†å‰²ROIï¼Œç„¶ååœ¨ä¸¤ä¸ªå›¾åƒä¹‹é—´è¿›è¡ŒåŒ¹é…ã€‚</li>
<li>æœ¬æ–‡ç®€åŒ–æ­¤è¿‡ç¨‹ä¸ºä¸€æ­¥ï¼Œé€šè¿‡ç›´æ¥â€œå¯»æ‰¾å¯¹åº”çš„æç¤ºâ€æ¥å®ç°è®­ç»ƒè‡ªç”±çš„æ³¨å†Œæ–¹æ³•ï¼Œç§°ä¸ºPromptRegã€‚</li>
<li>å¼•å…¥äº†â€œç›¸åº”æç¤ºé—®é¢˜â€ï¼Œæ—¨åœ¨è¯†åˆ«Image Yä¸­å¯¹åº”äºImage Xä¸­çš„ä»»ä½•è§†è§‰æç¤ºYçš„æç¤ºã€‚</li>
<li>æå‡ºäº†ä¸€ç§åå‘æç¤ºè§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡ç”Ÿæˆä¸»è¦å’Œè¾…åŠ©æç¤ºæ¥åè½¬æç¤ºXåˆ°Image Yçš„æç¤ºç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01697">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01697v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01697v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01697v1/page_2_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Measuring-and-Predicting-Where-and-When-Pathologists-Focus-their-Visual-Attention-while-Grading-Whole-Slide-Images-of-Cancer"><a href="#Measuring-and-Predicting-Where-and-When-Pathologists-Focus-their-Visual-Attention-while-Grading-Whole-Slide-Images-of-Cancer" class="headerlink" title="Measuring and Predicting Where and When Pathologists Focus their Visual   Attention while Grading Whole Slide Images of Cancer"></a>Measuring and Predicting Where and When Pathologists Focus their Visual   Attention while Grading Whole Slide Images of Cancer</h2><p><strong>Authors:Souradeep Chakraborty, Ruoyu Xue, Rajarsi Gupta, Oksana Yaskiv, Constantin Friedman, Natallia Sheuka, Dana Perez, Paul Friedman, Won-Tak Choi, Waqas Mahmud, Beatrice Knudsen, Gregory Zelinsky, Joel Saltz, Dimitris Samaras</strong></p>
<p>The ability to predict the attention of expert pathologists could lead to decision support systems for better pathology training. We developed methods to predict the spatio-temporal (where and when) movements of pathologistsâ€™ attention as they grade whole slide images (WSIs) of prostate cancer. We characterize a pathologistâ€™s attention trajectory by their x, y, and m (magnification) movements of a viewport as they navigate WSIs using a digital microscope. This information was obtained from 43 pathologists across 123 WSIs, and we consider the task of predicting the pathologist attention scanpaths constructed from the viewport centers. We introduce a fixation extraction algorithm that simplifies an attention trajectory by extracting fixations in the pathologistâ€™s viewing while preserving semantic information, and we use these pre-processed data to train and test a two-stage model to predict the dynamic (scanpath) allocation of attention during WSI reading via intermediate attention heatmap prediction. In the first stage, a transformer-based sub-network predicts the attention heatmaps (static attention) across different magnifications. In the second stage, we predict the attention scanpath by sequentially modeling the next fixation points in an autoregressive manner using a transformer-based approach, starting at the WSI center and leveraging multi-magnification feature representations from the first stage. Experimental results show that our scanpath prediction model outperforms chance and baseline models. Tools developed from this model could assist pathology trainees in learning to allocate their attention during WSI reading like an expert. </p>
<blockquote>
<p>é¢„æµ‹ä¸“å®¶ç—…ç†å­¦å®¶æ³¨æ„åŠ›çš„èƒ½åŠ›å¯ä»¥ä¸ºæ›´å¥½çš„ç—…ç†å­¦è®­ç»ƒæä¾›å†³ç­–æ”¯æŒç³»ç»Ÿã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥é¢„æµ‹ç—…ç†å­¦å®¶åœ¨è¯„ä¼°å‰åˆ—è…ºç™Œå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰æ—¶çš„æ—¶é—´å’Œç©ºé—´æ³¨æ„åŠ›è½¬ç§»æ–¹å‘ï¼ˆå³åœ¨å“ªé‡Œçœ‹ä»¥åŠä½•æ—¶çœ‹ï¼‰ã€‚æˆ‘ä»¬é€šè¿‡ç—…ç†å­¦å®¶ä½¿ç”¨æ•°å­—æ˜¾å¾®é•œæµè§ˆWSIæ—¶çš„xã€yå’Œmï¼ˆæ”¾å¤§å€æ•°ï¼‰è¿åŠ¨æ¥æç»˜ä»–ä»¬çš„æ³¨æ„åŠ›è½¨è¿¹ã€‚è¿™äº›ä¿¡æ¯æ¥æºäº43ä½ç—…ç†ä¸“å®¶å¯¹123å¼ WSIçš„æ•°æ®ï¼Œæˆ‘ä»¬è€ƒè™‘çš„ä»»åŠ¡æ˜¯é¢„æµ‹ç”±è§†å›¾ä¸­å¿ƒæ„å»ºçš„ç—…ç†å­¦å®¶æ³¨æ„åŠ›æ‰«æè·¯å¾„ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ³¨è§†ç‚¹æå–ç®—æ³•ï¼Œå®ƒé€šè¿‡æå–ç—…ç†å­¦å®¶æŸ¥çœ‹è¿‡ç¨‹ä¸­çš„æ³¨è§†ç‚¹æ¥ç®€åŒ–æ³¨æ„åŠ›è½¨è¿¹ï¼ŒåŒæ—¶ä¿ç•™è¯­ä¹‰ä¿¡æ¯ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™äº›é¢„å¤„ç†è¿‡çš„æ•°æ®æ¥è®­ç»ƒå’Œæµ‹è¯•ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ¨¡å‹ï¼Œé€šè¿‡é¢„æµ‹ä¸­é—´æ³¨æ„åŠ›çƒ­å›¾æ¥é¢„æµ‹åœ¨WSIé˜…è¯»è¿‡ç¨‹ä¸­åŠ¨æ€ï¼ˆæ‰«æè·¯å¾„ï¼‰çš„æ³¨æ„åŠ›åˆ†é…ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä¸€ä¸ªåŸºäºtransformerçš„å­ç½‘ç»œä¼šåœ¨ä¸åŒçš„æ”¾å¤§å€æ•°ä¸‹é¢„æµ‹æ³¨æ„åŠ›çƒ­å›¾ï¼ˆé™æ€æ³¨æ„åŠ›ï¼‰ã€‚åœ¨ç¬¬äºŒé˜¶æ®µä¸­ï¼Œæˆ‘ä»¬ä»¥è‡ªå›å½’çš„æ–¹å¼é€šè¿‡åŸºäºtransformerçš„æ–¹æ³•ä¾æ¬¡é¢„æµ‹ä¸‹ä¸€ä¸ªæ³¨è§†ç‚¹æ¥é¢„æµ‹æ³¨æ„åŠ›æ‰«æè·¯å¾„ï¼Œä»WSIä¸­å¿ƒå¼€å§‹ï¼Œå¹¶åˆ©ç”¨ç¬¬ä¸€é˜¶æ®µçš„å¤šç§æ”¾å¤§ç‰¹æ€§è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ‰«æè·¯å¾„é¢„æµ‹æ¨¡å‹ä¼˜äºéšæœºå’ŒåŸºçº¿æ¨¡å‹ã€‚ä»è¿™ä¸ªæ¨¡å‹å¼€å‘çš„å·¥å…·å¯ä»¥å¸®åŠ©ç—…ç†å­¦æ–°æ‰‹å­¦ä¹ å¦‚ä½•åƒä¸“å®¶ä¸€æ ·åˆ†é…ä»–ä»¬åœ¨é˜…è¯»WSIæ—¶çš„æ³¨æ„åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01668v1">PDF</a> Accepted to Medical Image Analysis (MEDIA), Elsevier, 2025. This is   the accepted manuscript version; the final published article link will be   updated when available</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶é¢„æµ‹ç—…ç†ä¸“å®¶æ³¨æ„åŠ›åœ¨å‰åˆ—è…ºç™Œç—‡å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰ä¸­çš„æ—¶ç©ºç§»åŠ¨ï¼Œä»¥æ”¯æŒå†³ç­–æ”¯æŒç³»ç»Ÿæ”¹å–„ç—…ç†è®­ç»ƒã€‚é€šè¿‡è®°å½•ç—…ç†ä¸“å®¶åœ¨å…¨åˆ‡ç‰‡å›¾åƒä¸Šçš„æµè§ˆè½¨è¿¹ï¼ˆå³æ³¨æ„åŠ›è½¨è¿¹ï¼‰ï¼Œå»ºç«‹é¢„æµ‹æ¨¡å‹ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ³¨è§†ç‚¹æå–ç®—æ³•ï¼Œç®€åŒ–æ³¨æ„åŠ›è½¨è¿¹å¹¶ä¿ç•™è¯­ä¹‰ä¿¡æ¯ã€‚é€šè¿‡ä¸¤é˜¶æ®µæ¨¡å‹é¢„æµ‹åŠ¨æ€æ³¨æ„åŠ›åˆ†é…ï¼šç¬¬ä¸€é˜¶æ®µé¢„æµ‹ä¸åŒæ”¾å¤§å€ç‡ä¸‹çš„é™æ€æ³¨æ„åŠ›å›¾ï¼Œç¬¬äºŒé˜¶æ®µåˆ©ç”¨å˜å‹å™¨æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªæ³¨è§†ç‚¹çš„æ³¨æ„åŠ›è½¨è¿¹ã€‚å®éªŒç»“æœè¯æ˜è¯¥æ¨¡å‹ä¼˜äºéšæœºå’ŒåŸºçº¿æ¨¡å‹ã€‚è¯¥å·¥å…·å¯å¸®åŠ©ç—…ç†è®­ç»ƒç”Ÿå­¦ä¹ å¦‚ä½•åƒä¸“å®¶ä¸€æ ·åˆ†é…æ³¨æ„åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ—¨åœ¨å¼€å‘é¢„æµ‹ç—…ç†ä¸“å®¶åœ¨å…¨åˆ‡ç‰‡å›¾åƒä¸Šæ³¨æ„åŠ›çš„å†³ç­–æ”¯æŒç³»ç»Ÿã€‚</li>
<li>é€šè¿‡æ”¶é›†ç—…ç†ä¸“å®¶çš„æ—¶ç©ºç§»åŠ¨æ•°æ®ï¼Œæ„å»ºäº†é¢„æµ‹æ¨¡å‹ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ³¨è§†ç‚¹æå–ç®—æ³•ï¼Œç®€åŒ–æ³¨æ„åŠ›è½¨è¿¹æ•°æ®ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µæ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼šé™æ€æ³¨æ„åŠ›å›¾é¢„æµ‹å’ŒåŠ¨æ€æ³¨æ„åŠ›è½¨è¿¹é¢„æµ‹ã€‚</li>
<li>æ¨¡å‹åˆ©ç”¨å˜å‹å™¨æ¶æ„è¿›è¡Œæ³¨æ„åŠ›æœºåˆ¶çš„æ·±åº¦å­¦ä¹ å’Œé¢„æµ‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹èƒ½æœ‰æ•ˆé¢„æµ‹ç—…ç†ä¸“å®¶çš„æ³¨æ„åŠ›è½¨è¿¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01668v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01668v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01668v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="LLaDA-MedV-Exploring-Large-Language-Diffusion-Models-for-Biomedical-Image-Understanding"><a href="#LLaDA-MedV-Exploring-Large-Language-Diffusion-Models-for-Biomedical-Image-Understanding" class="headerlink" title="LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical   Image Understanding"></a>LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical   Image Understanding</h2><p><strong>Authors:Xuanzhao Dong, Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Peijie Qiu, Shao Tang, Xin Li, Yalin Wang</strong></p>
<p>Autoregressive models (ARMs) have long dominated the landscape of biomedical vision-language models (VLMs). Recently, masked diffusion models such as LLaDA have emerged as promising alternatives, yet their application in the biomedical domain remains largely underexplored. To bridge this gap, we introduce \textbf{LLaDA-MedV}, the first large language diffusion model tailored for biomedical image understanding through vision instruction tuning. LLaDA-MedV achieves relative performance gains of 7.855% over LLaVA-Med and 1.867% over LLaDA-V in the open-ended biomedical visual conversation task, and sets new state-of-the-art accuracy on the closed-form subset of three VQA benchmarks: 84.93% on VQA-RAD, 92.31% on SLAKE, and 95.15% on PathVQA. Furthermore, a detailed comparison with LLaVA-Med suggests that LLaDA-MedV is capable of generating reasonably longer responses by explicitly controlling response length, which can lead to more informative outputs. We also conduct an in-depth analysis of both the training and inference stages, highlighting the critical roles of initialization weight selection, fine-tuning strategies, and the interplay between sampling steps and response repetition. The code and model weight is released at <a target="_blank" rel="noopener" href="https://github.com/LLM-VLM-GSL/LLaDA-MedV">https://github.com/LLM-VLM-GSL/LLaDA-MedV</a>. </p>
<blockquote>
<p>è‡ªåŠ¨å›å½’æ¨¡å‹ï¼ˆARMsï¼‰é•¿æœŸä»¥æ¥åœ¨ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é¢†åŸŸå æ®ä¸»å¯¼åœ°ä½ã€‚æœ€è¿‘ï¼Œå¦‚LLaDAç­‰é®ç½©æ‰©æ•£æ¨¡å‹ä½œä¸ºæœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆè€Œå‡ºç°ï¼Œä½†å®ƒä»¬åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„åº”ç”¨ä»è¢«å¤§å¤§å¿½è§†ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒç†è§£é€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´è€Œé‡èº«å®šåˆ¶çš„ç¬¬ä¸€ä¸ªå¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹LLaDA-MedVã€‚\LLaDA-MedVåœ¨å¼€æ”¾å¼çš„ç”Ÿç‰©åŒ»å­¦è§†è§‰å¯¹è¯ä»»åŠ¡ä¸Šç›¸å¯¹äºLLaVA-Medå’ŒLLaDA-Våˆ†åˆ«å®ç°äº†7.855%å’Œ1.867%çš„ç›¸å¯¹æ€§èƒ½æå‡ï¼Œå¹¶åœ¨ä¸‰ä¸ªVQAåŸºå‡†æµ‹è¯•é›†çš„å°é—­å½¢å¼å­é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼šåœ¨VQA-RADä¸Šè¾¾åˆ°84.93%ï¼Œåœ¨SLAKEä¸Šè¾¾åˆ°92.31%ï¼Œåœ¨PathVQAä¸Šè¾¾åˆ°95.15%ã€‚æ­¤å¤–ï¼Œä¸LLaVA-Medçš„è¯¦ç»†æ¯”è¾ƒè¡¨æ˜ï¼ŒLLaDA-MedVèƒ½å¤Ÿé€šè¿‡æ˜ç¡®æ§åˆ¶å“åº”é•¿åº¦æ¥ç”Ÿæˆç›¸å¯¹æ›´é•¿çš„å“åº”ï¼Œä»è€Œäº§ç”Ÿæ›´å…·ä¿¡æ¯é‡çš„è¾“å‡ºã€‚æˆ‘ä»¬è¿˜å¯¹è®­ç»ƒå’Œæ¨ç†é˜¶æ®µè¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œçªå‡ºäº†åˆå§‹åŒ–æƒé‡é€‰æ‹©ã€å¾®è°ƒç­–ç•¥ä»¥åŠé‡‡æ ·æ­¥éª¤å’Œå“åº”é‡å¤ä¹‹é—´çš„ç›¸äº’ä½œç”¨çš„å…³é”®ä½œç”¨ã€‚ä»£ç å’Œæ¨¡å‹æƒé‡å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/LLM-VLM-GSL/LLaDA-MedV%E3%80%82">https://github.com/LLM-VLM-GSL/LLaDA-MedVã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01617v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLaDA-MedVæ˜¯ä¸“ä¸ºç”Ÿç‰©åŒ»å­¦å›¾åƒç†è§£è®¾è®¡çš„å¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹ã€‚ç›¸è¾ƒäºå…¶ä»–æ¨¡å‹ï¼ŒLLaDA-MedVåœ¨å¼€æ”¾å‹ç”Ÿç‰©åŒ»å­¦è§†è§‰å¯¹è¯ä»»åŠ¡ä¸­å…·æœ‰æ›´é«˜çš„æ€§èƒ½ï¼Œä¸”åœ¨ä¸‰ä¸ªè§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•çš„å­é›†ä¸Šè¾¾åˆ°æœ€æ–°æ°´å¹³ï¼šVQA-RADçš„84.93%ï¼ŒSLAKEçš„92.31%ï¼ŒPathVQAçš„95.15%ã€‚å…¶ä¼˜åŠ¿åœ¨äºèƒ½å¤Ÿç”Ÿæˆæ›´é•¿çš„å“åº”ï¼Œå¹¶æ˜ç¡®æ§åˆ¶å“åº”é•¿åº¦ã€‚è¯¥æ¨¡å‹çš„å…³é”®ç¯èŠ‚åŒ…æ‹¬åˆå§‹åŒ–æƒé‡é€‰æ‹©ã€å¾®è°ƒç­–ç•¥ä»¥åŠé‡‡æ ·æ­¥éª¤å’Œå“åº”é‡å¤ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLaDA-MedVæ˜¯é¦–ä¸ªé’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒç†è§£çš„å¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>LLaDA-MedVåœ¨ç”Ÿç‰©åŒ»å­¦è§†è§‰å¯¹è¯ä»»åŠ¡ä¸Šç›¸å¯¹äºLLaVA-Medå’ŒLLaDA-Væœ‰æ€§èƒ½æå‡ã€‚</li>
<li>LLaDA-MedVåœ¨ä¸‰ä¸ªè§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡é«˜ã€‚</li>
<li>LLaDA-MedVèƒ½ç”Ÿæˆæ›´é•¿çš„å“åº”ï¼Œå¹¶æ˜ç¡®æ§åˆ¶å“åº”é•¿åº¦ã€‚</li>
<li>åˆå§‹åŒ–æƒé‡é€‰æ‹©ã€å¾®è°ƒç­–ç•¥ä»¥åŠé‡‡æ ·æ­¥éª¤å’Œå“åº”é‡å¤æ˜¯LLaDA-MedVçš„å…³é”®ç¯èŠ‚ã€‚</li>
<li>LLaDA-MedVçš„ä»£ç å’Œæ¨¡å‹æƒé‡å·²å…¬å¼€å‘å¸ƒã€‚</li>
<li>LLaDA-MedVå¯¹äºæ¨åŠ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒç†è§£é¢†åŸŸçš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01617v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01617v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01617v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01617v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_åŒ»å­¦å›¾åƒ/2508.01617v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/TTS/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_TTS/2508.02391v1/page_0_0.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  Inference-time Scaling for Diffusion-based Audio Super-resolution
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-08-06\./crop_Diffusion Models/2507.10340v3/page_5_1.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  From Pixels to Pathology Restoration Diffusion for   Diagnostic-Consistent Virtual IHC
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24801.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
