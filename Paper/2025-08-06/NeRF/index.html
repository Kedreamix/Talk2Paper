<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF 方向最新论文已更新，请持续关注 Update in 2025-08-06  ASDR Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant   Neural Rendering">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-f41e461634289951c99dea6a22e2b222.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    27 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-06-更新"><a href="#2025-08-06-更新" class="headerlink" title="2025-08-06 更新"></a>2025-08-06 更新</h1><h2 id="ASDR-Exploiting-Adaptive-Sampling-and-Data-Reuse-for-CIM-based-Instant-Neural-Rendering"><a href="#ASDR-Exploiting-Adaptive-Sampling-and-Data-Reuse-for-CIM-based-Instant-Neural-Rendering" class="headerlink" title="ASDR: Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant   Neural Rendering"></a>ASDR: Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant   Neural Rendering</h2><p><strong>Authors:Fangxin Liu, Haomin Li, Bowen Zhu, Zongwu Wang, Zhuoran Song, Habing Guan, Li Jiang</strong></p>
<p>Neural Radiance Fields (NeRF) offer significant promise for generating photorealistic images and videos. However, existing mainstream neural rendering models often fall short in meeting the demands for immediacy and power efficiency in practical applications. Specifically, these models frequently exhibit irregular access patterns and substantial computational overhead, leading to undesirable inference latency and high power consumption. Computing-in-memory (CIM), an emerging computational paradigm, has the potential to address these access bottlenecks and reduce the power consumption associated with model execution.   To bridge the gap between model performance and real-world scene requirements, we propose an algorithm-architecture co-design approach, abbreviated as ASDR, a CIM-based accelerator supporting efficient neural rendering. At the algorithmic level, we propose two rendering optimization schemes: (1) Dynamic sampling by online sensing of the rendering difficulty of different pixels, thus reducing access memory and computational overhead. (2) Reducing MLP overhead by decoupling and approximating the volume rendering of color and density. At the architecture level, we design an efficient ReRAM-based CIM architecture with efficient data mapping and reuse microarchitecture. Experiments demonstrate that our design can achieve up to $9.55\times$ and $69.75\times$ speedup over state-of-the-art NeRF accelerators and Xavier NX GPU in graphics rendering tasks with only $0.1$ PSNR loss. </p>
<blockquote>
<p>神经辐射场（NeRF）在生成真实感图像和视频方面显示出巨大的潜力。然而，现有的主流神经渲染模型往往在实际应用中难以满足即时性和能效的要求。具体来说，这些模型经常出现不规则的数据访问模式以及大量的计算开销，导致推理延迟和较高的功耗。作为新兴计算范式，计算内存（CIM）具有解决这些访问瓶颈和降低模型执行相关功耗的潜力。为了弥模型性能与现实场景需求之间的差距，我们提出了一种算法与架构协同设计的方法，简称为ASDR，这是一种基于CIM的加速器，支持高效的神经渲染。在算法层面，我们提出了两种渲染优化方案：（1）通过在线感知不同像素的渲染难度进行动态采样，从而减少内存访问和计算开销。（2）通过解耦并近似颜色和密度的体积渲染来减少MLP的开销。在架构层面，我们设计了一种基于ReRAM的高效CIM架构，具有高效的数据映射和重用微架构。实验表明，我们的设计在图形渲染任务中比最先进的NeRF加速器和Xavier NX GPU加速高达9.55倍和69.75倍，同时只有0.1的峰值信噪比损失。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02304v1">PDF</a> Accepted by the 2025 International Conference on Architectural   Support for Programming Languages and Operating Systems (ASPLOS 2025). The   paper will be presented at ASPLOS 2026</p>
<p><strong>摘要</strong></p>
<p>神经网络辐射场（NeRF）在生成真实感图像和视频方面展现出巨大潜力。然而，现有主流神经渲染模型在实际应用中难以满足即时性和能效需求。本文通过计算内存（CIM）技术来解决访问瓶颈，减少模型执行时的功耗，提出了一种基于CIM的加速器算法架构协同设计方法（简称ASDR），支持高效神经渲染。算法层面，我们提出两种渲染优化方案：动态采样，在线感知不同像素的渲染难度以降低内存访问和计算开销；分离并近似颜色和密度的体积渲染以减少MLP开销。架构层面，我们设计了一种高效的基于ReRAM的CIM架构，具有高效的数据映射和重用微架构。实验表明，我们的设计在图形渲染任务中相较于最新NeRF加速器和Xavier NX GPU，速度提升可达9.55倍和69.75倍，同时只有0.1 PSNR损失。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>神经网络辐射场（NeRF）在生成真实感图像和视频方面表现出卓越性能。</li>
<li>现有神经渲染模型存在即时性和能效问题，难以满足实际应用需求。</li>
<li>计算内存（CIM）技术有助于解决访问瓶颈，降低模型执行功耗。</li>
<li>提出一种基于CIM的加速器算法架构协同设计方法（ASDR），支持高效神经渲染。</li>
<li>在算法层面，通过动态采样和体积渲染的分离近似优化渲染性能。</li>
<li>在架构层面，设计高效的基于ReRAM的CIM架构，具有优秀的数据映射和重用特性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02304">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f41e461634289951c99dea6a22e2b222.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69f8f7ca8a04031aa91bd9e7c3510444.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-15971a16377d1d94aa56376fa89fe5b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-341ea3e3035d95773d97898d76be2f25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27e495ecf056a809418494ee29539504.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-abcc6df5059b3792d7d50a27ce655e84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9fc3c1ef72e6d75d9b3b1a54b531dcb.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Imbalance-Robust-and-Sampling-Efficient-Continuous-Conditional-GANs-via-Adaptive-Vicinity-and-Auxiliary-Regularization"><a href="#Imbalance-Robust-and-Sampling-Efficient-Continuous-Conditional-GANs-via-Adaptive-Vicinity-and-Auxiliary-Regularization" class="headerlink" title="Imbalance-Robust and Sampling-Efficient Continuous Conditional GANs via   Adaptive Vicinity and Auxiliary Regularization"></a>Imbalance-Robust and Sampling-Efficient Continuous Conditional GANs via   Adaptive Vicinity and Auxiliary Regularization</h2><p><strong>Authors:Xin Ding, Yun Chen, Yongwei Wang, Kao Zhang, Sen Zhang, Peibei Cao, Xiangxue Wang</strong></p>
<p>Recent advances in conditional generative modeling have introduced Continuous conditional Generative Adversarial Network (CcGAN) and Continuous Conditional Diffusion Model (CCDM) for estimating high-dimensional data distributions conditioned on scalar, continuous regression labels (e.g., angles, ages, or temperatures). However, these approaches face fundamental limitations: CcGAN suffers from data imbalance due to fixed-size vicinity constraints, while CCDM requires computationally expensive iterative sampling. We present CcGAN-AVAR, an enhanced CcGAN framework that addresses both challenges: (1) leveraging the GAN framework’s native one-step generation to overcome CCDMs’ sampling bottleneck (achieving 300x-2000x faster inference), while (2) two novel components specifically target data imbalance - an adaptive vicinity mechanism that dynamically adjusts vicinity’s size, and a multi-task discriminator that constructs two regularization terms (through auxiliary regression and density ratio estimation) to significantly improve generator training. Extensive experiments on four benchmark datasets (64x64 to 192x192 resolution) across eight challenging imbalanced settings demonstrate that CcGAN-AVAR achieves state-of-the-art generation quality while maintaining sampling efficiency. </p>
<blockquote>
<p>最近条件生成模型的进展引入了连续条件生成对抗网络（CcGAN）和连续条件扩散模型（CCDM），用于估计基于标量、连续回归标签的高维数据分布（例如角度、年龄或温度）。然而，这些方法面临根本性局限：CcGAN由于固定大小的邻近约束而受到数据不平衡的影响，而CCDM需要计算昂贵的迭代采样。我们提出了CcGAN-AVAR，这是一种增强的CcGAN框架，解决了这两个挑战：（1）利用GAN框架的一次性生成来克服CCDM的采样瓶颈（实现300倍至2000倍更快的推理速度），同时（2）两个新组件专门解决数据不平衡问题——一种自适应邻近机制，可动态调整邻近大小，以及一个多任务鉴别器，构建两个正则化项（通过辅助回归和密度比率估计），以显著改善生成器训练。在四个基准数据集（分辨率从64x64到192x192）进行的八个具有挑战性的不平衡设置的大量实验表明，CcGAN-AVAR在保持采样效率的同时实现了最先进的生成质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01725v1">PDF</a> </p>
<p><strong>Summary</strong><br>新一代连续条件生成模型如CcGAN-AVAR在解决高维数据分布估计问题上取得了显著进展，尤其适用于基于连续回归标签的条件生成任务。相较于传统方法如CcGAN和CCDM，CcGAN-AVAR通过改进框架设计克服了数据不平衡和计算成本高昂的问题。它采用一步生成法提升采样效率，同时引入自适应邻近机制和多任务鉴别器改善数据不平衡问题。实验证明，CcGAN-AVAR在多个数据集上实现了出色的生成质量和采样效率。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>连续条件生成模型（如CcGAN-AVAR）适用于基于连续回归标签的高维数据分布估计。</li>
<li>传统方法如CcGAN和CCDM存在数据不平衡和计算成本问题。</li>
<li>CcGAN-AVAR通过改进框架设计克服这些问题，包括一步生成法提升采样效率。</li>
<li>CcGAN-AVAR引入自适应邻近机制和多任务鉴别器改善数据不平衡问题。</li>
<li>CcGAN-AVAR在多个数据集上实现了出色的生成质量和采样效率。</li>
<li>自适应邻近机制能动态调整邻近大小以应对数据不平衡问题。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01725">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e9eadc45d6a0cec98ffc81bec447c949.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ebd74a861aa4147ce065a319c90a782.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01e865c2fedb2e0690efee02d7177957.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c82e9f34975b6c3da762ff12a1c61fe0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f9b9795b6837a72275bcbd02e115493.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d79a15f7261ab4da6f7e0bec5f4eb5bc.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Cooperative-Perception-A-Resource-Efficient-Framework-for-Multi-Drone-3D-Scene-Reconstruction-Using-Federated-Diffusion-and-NeRF"><a href="#Cooperative-Perception-A-Resource-Efficient-Framework-for-Multi-Drone-3D-Scene-Reconstruction-Using-Federated-Diffusion-and-NeRF" class="headerlink" title="Cooperative Perception: A Resource-Efficient Framework for Multi-Drone   3D Scene Reconstruction Using Federated Diffusion and NeRF"></a>Cooperative Perception: A Resource-Efficient Framework for Multi-Drone   3D Scene Reconstruction Using Federated Diffusion and NeRF</h2><p><strong>Authors:Massoud Pourmandi</strong></p>
<p>The proposal introduces an innovative drone swarm perception system that aims to solve problems related to computational limitations and low-bandwidth communication, and real-time scene reconstruction. The framework enables efficient multi-agent 3D&#x2F;4D scene synthesis through federated learning of shared diffusion model and YOLOv12 lightweight semantic extraction and local NeRF updates while maintaining privacy and scalability. The framework redesigns generative diffusion models for joint scene reconstruction, and improves cooperative scene understanding, while adding semantic-aware compression protocols. The approach can be validated through simulations and potential real-world deployment on drone testbeds, positioning it as a disruptive advancement in multi-agent AI for autonomous systems. </p>
<blockquote>
<p>提案介绍了一种创新的无人机群感知系统，旨在解决与计算限制、低带宽通信和实时场景重建相关的问题。该框架通过联合扩散模型的联邦学习、YOLOv12轻量级语义提取和局部NeRF更新，实现了高效的多智能体3D&#x2F;4D场景合成，同时保持了隐私和可扩展性。该框架重新设计了用于联合场景重建的生成性扩散模型，提高了合作场景理解的能力，并增加了语义感知压缩协议。该方法可通过模拟和无人机测试平台上的潜在实际部署进行验证，定位其为多智能体人工智能自主系统领域的突破性进展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00967v1">PDF</a> 15 pages, 3 figures, 1 table, 1 algorithm. Preprint based on NeurIPS   2024 template</p>
<p><strong>Summary</strong><br>     该提案提出了一种创新的无人机群感知系统，旨在解决计算限制、低带宽通信和实时场景重建等问题。该系统通过联合场景重建的生成性扩散模型、语义感知压缩协议、联邦学习共享扩散模型、YOLOv12轻量级语义提取和局部NeRF更新，实现了高效的多智能体3D&#x2F;4D场景合成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>无人机群感知系统旨在解决计算限制、低带宽通信和实时场景重建问题。</li>
<li>系统采用联邦学习、扩散模型和YOLOv12轻量级语义提取技术。</li>
<li>通过局部NeRF更新实现高效的多智能体3D&#x2F;4D场景合成。</li>
<li>系统具备隐私保护和可扩展性。</li>
<li>语义感知压缩协议用于提高合作场景理解。</li>
<li>该系统可通过模拟和无人机测试平台上的真实世界部署进行验证。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00967">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-186f7d279cd4e3264a015493c8fd46b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a568d7b7e6857ea220fc019b84d37c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c4169ef9011d40a3d93a2545fa67b2d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Surgical-Gaussian-Surfels-Highly-Accurate-Real-time-Surgical-Scene-Rendering-using-Gaussian-Surfels"><a href="#Surgical-Gaussian-Surfels-Highly-Accurate-Real-time-Surgical-Scene-Rendering-using-Gaussian-Surfels" class="headerlink" title="Surgical Gaussian Surfels: Highly Accurate Real-time Surgical Scene   Rendering using Gaussian Surfels"></a>Surgical Gaussian Surfels: Highly Accurate Real-time Surgical Scene   Rendering using Gaussian Surfels</h2><p><strong>Authors:Idris O. Sunmola, Zhenjun Zhao, Samuel Schmidgall, Yumeng Wang, Paul Maria Scheikl, Viet Pham, Axel Krieger</strong></p>
<p>Accurate geometric reconstruction of deformable tissues in monocular endoscopic video remains a fundamental challenge in robot-assisted minimally invasive surgery. Although recent volumetric and point primitive methods based on neural radiance fields (NeRF) and 3D Gaussian primitives have efficiently rendered surgical scenes, they still struggle with handling artifact-free tool occlusions and preserving fine anatomical details. These limitations stem from unrestricted Gaussian scaling and insufficient surface alignment constraints during reconstruction. To address these issues, we introduce Surgical Gaussian Surfels (SGS), which transform anisotropic point primitives into surface-aligned elliptical splats by constraining the scale component of the Gaussian covariance matrix along the view-aligned axis. We also introduce the Fully Fused Deformation Multilayer Perceptron (FFD-MLP), a lightweight Multi-Layer Perceptron (MLP) that predicts accurate surfel motion fields up to 5x faster than a standard MLP. This is coupled with locality constraints to handle complex tissue deformations. We use homodirectional view-space positional gradients to capture fine image details by splitting Gaussian Surfels in over-reconstructed regions. In addition, we define surface normals as the direction of the steepest density change within each Gaussian surfel primitive, enabling accurate normal estimation without requiring monocular normal priors. We evaluate our method on two in-vivo surgical datasets, where it outperforms current state-of-the-art methods in surface geometry, normal map quality, and rendering efficiency, while remaining competitive in real-time rendering performance. We make our code available at <a target="_blank" rel="noopener" href="https://github.com/aloma85/SurgicalGaussianSurfels">https://github.com/aloma85/SurgicalGaussianSurfels</a> </p>
<blockquote>
<p>在单目内窥镜视频中，对可变形组织的精确几何重建仍然是机器人辅助微创手术中的一项基本挑战。尽管最近基于神经辐射场（NeRF）和3D高斯原始点的体积和点原始方法已经有效地呈现了手术场景，但它们仍然在处理无工具遮挡物和保留精细解剖细节方面存在困难。这些局限性源于高斯缩放的无限制性和重建过程中表面对齐约束的不足。为了解决这些问题，我们引入了手术高斯曲面元素（Surgical Gaussian Surfels，SGS），通过将高斯协方差矩阵的尺度成分约束在视图对齐轴上，将各向异性点原始点转换为与表面对齐的椭圆平板。我们还引入了全融合变形多层感知器（FFD-MLP），这是一个轻量级的多层感知器（MLP），它能够在比标准MLP快5倍的速度下预测准确的曲面运动场。这与局部约束相结合，可以处理复杂的组织变形。我们通过将高斯曲面元素在过度重建区域中进行拆分来捕捉图像中的细微细节，使用同方向视图空间位置梯度。此外，我们将表面法线定义为每个高斯曲面元素内密度变化最陡峭的方向，从而能够准确估计法线，而无需单目法线先验。我们在两个体内手术数据集上评估了我们的方法，该方法在表面几何、法线图质量和渲染效率方面均优于当前最新技术，同时在实时渲染性能方面保持竞争力。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/aloma85/SurgicalGaussianSurfels%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/aloma85/SurgicalGaussianSurfels上获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04079v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>最新研究针对机器人辅助微创手术中内镜视频的可变形组织精准几何重建问题提出解决方案。引入Surgical Gaussian Surfels（SGS），通过约束高斯协方差矩阵尺度成分并沿视图对齐轴将点基元转化为表面对齐椭圆平面，解决了工具遮挡造成的伪影和解剖细节保留不足的问题。同时，提出Fully Fused Deformation Multilayer Perceptron（FFD-MLP），预测准确表面运动场，处理复杂组织变形。通过定义高斯surfel基元内密度变化最陡峭方向为表面法线，实现无需单眼法线先验的准确法线估计。在两个活体手术数据集上的评估表明，该方法在表面几何、法线图质量和渲染效率方面优于当前最先进方法，同时保持实时渲染性能竞争力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器人辅助微创手术中的内镜视频可变形组织几何重建仍是挑战。</li>
<li>当前基于NeRF和3D高斯基元的方法在处理工具遮挡和保留解剖细节方面存在局限性。</li>
<li>引入Surgical Gaussian Surfels（SGS）解决上述问题，通过约束高斯协方差矩阵实现更准确的重建。</li>
<li>提出Fully Fused Deformation Multilayer Perceptron（FFD-MLP）预测表面运动场，更快更准确地处理复杂组织变形。</li>
<li>利用密度变化定义表面法线，实现无需单眼法线先验的准确法线估计。</li>
<li>方法在多个数据集上表现优于现有方法，提高表面几何、法线图和渲染效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04079">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a00ed29a89b9c55709272ec166b41eed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5632a38c3c76e0367ef17cb99f19afa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e11699f2118b90a03ce7f350aa3d6822.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GAS-Generative-Avatar-Synthesis-from-a-Single-Image"><a href="#GAS-Generative-Avatar-Synthesis-from-a-Single-Image" class="headerlink" title="GAS: Generative Avatar Synthesis from a Single Image"></a>GAS: Generative Avatar Synthesis from a Single Image</h2><p><strong>Authors:Yixing Lu, Junting Dong, Youngjoong Kwon, Qin Zhao, Bo Dai, Fernando De la Torre</strong></p>
<p>We present a unified and generalizable framework for synthesizing view-consistent and temporally coherent avatars from a single image, addressing the challenging task of single-image avatar generation. Existing diffusion-based methods often condition on sparse human templates (e.g., depth or normal maps), which leads to multi-view and temporal inconsistencies due to the mismatch between these signals and the true appearance of the subject. Our approach bridges this gap by combining the reconstruction power of regression-based 3D human reconstruction with the generative capabilities of a diffusion model. In a first step, an initial 3D reconstructed human through a generalized NeRF provides comprehensive conditioning, ensuring high-quality synthesis faithful to the reference appearance and structure. Subsequently, the derived geometry and appearance from the generalized NeRF serve as input to a video-based diffusion model. This strategic integration is pivotal for enforcing both multi-view and temporal consistency throughout the avatar’s generation. Empirical results underscore the superior generalization ability of our proposed method, demonstrating its effectiveness across diverse in-domain and out-of-domain in-the-wild datasets. </p>
<blockquote>
<p>我们提出了一个统一且可推广的框架，用于从单张图像合成视角一致且时间连贯的虚拟角色，解决了单图像虚拟角色生成的挑战任务。现有的基于扩散的方法通常依赖于稀疏的人体模板（例如深度或法线图），这会导致这些信号与主体真实外观之间的不匹配，从而产生多视角和时间上的不一致性。我们的方法通过结合基于回归的3D人体重建的重建能力与扩散模型的生成能力来弥补这一差距。首先，通过广义NeRF进行初步3D重建人体，提供全面的条件，确保高质量合成忠于参考外观和结构。随后，从广义NeRF派生的几何形状和外观作为视频扩散模型的输入。这种战略性的整合对于在虚拟角色生成过程中强制实施多视角和时间一致性至关重要。经验结果突显了我们所提出方法的出色泛化能力，证明了它在各种领域内外、真实场景数据集中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06957v2">PDF</a> ICCV 2025; Project Page: <a target="_blank" rel="noopener" href="https://humansensinglab.github.io/GAS/">https://humansensinglab.github.io/GAS/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种统一且通用的框架，用于从单张图像合成视角一致、时间连贯的虚拟角色。该框架结合回归式三维人体重建的重建能力与扩散模型的生成能力，解决了现有扩散模型依赖于稀疏人体模板而导致的多视角和时间不一致性问题。首先，通过通用NeRF进行初始三维人体重建，提供全面的条件，确保合成的高质量且忠于参考的外观和结构。接着，从通用NeRF派生的几何和外观作为视频扩散模型的输入，这对于强制执行多视角和时间连贯性至关重要。经验结果表明，该方法具有出色的泛化能力，在多种领域内的和领域外的野外数据集上都表现出其有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种统一且通用的框架，用于从单张图像合成虚拟角色。</li>
<li>结合回归式三维人体重建与扩散模型的生成能力。</li>
<li>通过通用NeRF进行初始三维人体重建，提供全面的条件。</li>
<li>解决了现有扩散模型依赖于稀疏人体模板导致的问题。</li>
<li>通过视频扩散模型实现多视角和时间连贯性。</li>
<li>框架具有出色的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06957">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4051b2273865fc183efb9af8dd673d6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a35b0421bfdf7f0ec7680228e4b5ee9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61186df640852268c7e25db6405240d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3407eebe3546c090b7f8c0ca2900578e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Sequential-Gaussian-Avatars-with-Hierarchical-Motion-Context"><a href="#Sequential-Gaussian-Avatars-with-Hierarchical-Motion-Context" class="headerlink" title="Sequential Gaussian Avatars with Hierarchical Motion Context"></a>Sequential Gaussian Avatars with Hierarchical Motion Context</h2><p><strong>Authors:Wangze Xu, Yifan Zhan, Zhihang Zhong, Xiao Sun</strong></p>
<p>The emergence of neural rendering has significantly advanced the rendering quality of 3D human avatars, with the recently popular 3DGS technique enabling real-time performance. However, SMPL-driven 3DGS human avatars still struggle to capture fine appearance details due to the complex mapping from pose to appearance during fitting. In this paper, we propose SeqAvatar, which excavates the explicit 3DGS representation to better model human avatars based on a hierarchical motion context. Specifically, we utilize a coarse-to-fine motion conditions that incorporate both the overall human skeleton and fine-grained vertex motions for non-rigid deformation. To enhance the robustness of the proposed motion conditions, we adopt a spatio-temporal multi-scale sampling strategy to hierarchically integrate more motion clues to model human avatars. Extensive experiments demonstrate that our method significantly outperforms 3DGS-based approaches and renders human avatars orders of magnitude faster than the latest NeRF-based models that incorporate temporal context, all while delivering performance that is at least comparable or even superior. Project page: <a target="_blank" rel="noopener" href="https://zezeaaa.github.io/projects/SeqAvatar/">https://zezeaaa.github.io/projects/SeqAvatar/</a> </p>
<blockquote>
<p>神经渲染的出现极大地提高了3D人类角色模型的渲染质量，最近流行的3DGS技术甚至实现了实时性能。然而，由SMPL驱动的3DGS人类角色模型在拟合过程中仍然难以捕捉精细的外观细节，因为姿势到外观的复杂映射。在本文中，我们提出了SeqAvatar，它挖掘了明确的3DGS表示，基于分层运动上下文更好地建模人类角色。具体来说，我们利用从粗到细的运动条件，结合整体人类骨骼和精细顶点运动进行非刚体变形。为了提高所提出运动条件的稳健性，我们采用时空多尺度采样策略，分层融合更多运动线索来建模人类角色。大量实验表明，我们的方法显著优于基于3DGS的方法，并且与最新融入时间上下文的NeRF模型相比，渲染人类角色模型的速度要快得多，同时性能至少相当甚至更好。项目页面：<a target="_blank" rel="noopener" href="https://zezeaaa.github.io/projects/SeqAvatar/">https://zezeaaa.github.io/projects/SeqAvatar/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16768v2">PDF</a> ICCV2025</p>
<p><strong>Summary</strong><br>神经网络渲染技术的出现极大地提高了3D人类角色模型的渲染质量，流行的3DGS技术可实现实时性能。然而，基于SMPL驱动的3DGS人类角色模型在拟合过程中仍然难以捕捉精细的外观细节。本文提出SeqAvatar，通过挖掘明确的3DGS表示并基于分层运动上下文更好地建模人类角色模型。具体来说，我们利用从粗到细的运动条件，结合整体人类骨骼和精细顶点运动进行非刚性变形。为了提高所提运动条件的稳健性，我们采用时空多尺度采样策略，分层集成更多运动线索来建模人类角色。实验表明，我们的方法显著优于基于3DGS的方法，并且与人类角色建模中融入时序上下文的最新NeRF模型相比，速度要快得多，同时性能至少与之相当甚至更好。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经网络渲染提高了3D人类角色模型的渲染质量。</li>
<li>流行的3DGS技术可实现实时性能，但难以捕捉精细外观细节。</li>
<li>SeqAvatar通过挖掘明确的3DGS表示来改进模型。</li>
<li>SeqAvatar利用从粗到细的运动条件进行建模，结合整体人类骨骼和精细顶点运动。</li>
<li>采用时空多尺度采样策略提高运动条件的稳健性。</li>
<li>与基于3DGS的方法相比，SeqAvatar表现更优。</li>
<li>SeqAvatar的渲染速度比最新融入时序上下文的NeRF模型快，且性能至少相当或更好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16768">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cb83d7e77d65ada5ff7333865bdea540.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d261ad94b4dae0a70a6749252521491a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15db0196306fea2d774c45d12abea292.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="PLGS-Robust-Panoptic-Lifting-with-3D-Gaussian-Splatting"><a href="#PLGS-Robust-Panoptic-Lifting-with-3D-Gaussian-Splatting" class="headerlink" title="PLGS: Robust Panoptic Lifting with 3D Gaussian Splatting"></a>PLGS: Robust Panoptic Lifting with 3D Gaussian Splatting</h2><p><strong>Authors:Yu Wang, Xiaobao Wei, Ming Lu, Guoliang Kang</strong></p>
<p>Previous methods utilize the Neural Radiance Field (NeRF) for panoptic lifting, while their training and rendering speed are unsatisfactory. In contrast, 3D Gaussian Splatting (3DGS) has emerged as a prominent technique due to its rapid training and rendering speed. However, unlike NeRF, the conventional 3DGS may not satisfy the basic smoothness assumption as it does not rely on any parameterized structures to render (e.g., MLPs). Consequently, the conventional 3DGS is, in nature, more susceptible to noisy 2D mask supervision. In this paper, we propose a new method called PLGS that enables 3DGS to generate consistent panoptic segmentation masks from noisy 2D segmentation masks while maintaining superior efficiency compared to NeRF-based methods. Specifically, we build a panoptic-aware structured 3D Gaussian model to introduce smoothness and design effective noise reduction strategies. For the semantic field, instead of initialization with structure from motion, we construct reliable semantic anchor points to initialize the 3D Gaussians. We then use these anchor points as smooth regularization during training. Additionally, we present a self-training approach using pseudo labels generated by merging the rendered masks with the noisy masks to enhance the robustness of PLGS. For the instance field, we project the 2D instance masks into 3D space and match them with oriented bounding boxes to generate cross-view consistent instance masks for supervision. Experiments on various benchmarks demonstrate that our method outperforms previous state-of-the-art methods in terms of both segmentation quality and speed. </p>
<blockquote>
<p>之前的方法采用神经辐射场（NeRF）进行全景提升，但其训练和渲染速度并不令人满意。相比之下，3D高斯贴图（3DGS）由于快速的训练和渲染速度而成为了一项突出的技术。然而，与NeRF不同，传统的3DGS可能无法满足基本平滑假设，因为它不依赖任何参数化结构进行渲染（例如，多层感知机）。因此，传统的3DGS本质上更容易受到嘈杂的2D遮罩监督的影响。在本文中，我们提出了一种新的方法，称为PLGS，它使3DGS能够从嘈杂的2D分割遮罩中生成一致的全景分割遮罩，同时保持比NeRF基方法更高的效率。具体来说，我们建立了一个全景感知结构化3D高斯模型，以引入平滑度并设计有效的降噪策略。对于语义场，我们没有使用运动结构进行初始化，而是构建可靠语义锚点来初始化3D高斯。然后，我们在训练过程中使用这些锚点作为平滑正则化。此外，我们还采用了一种自训练方法，通过合并渲染的遮罩和嘈杂的遮罩来生成伪标签，以增强PLGS的稳健性。对于实例场，我们将2D实例遮罩投影到3D空间中，并与定向边界框进行匹配，以生成用于监督的跨视图一致实例遮罩。在各种基准测试上的实验表明，我们的方法在分割质量和速度方面都优于之前的最先进方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17505v2">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文提出一种名为PLGS的新方法，结合了3D高斯涂敷技术（3DGS）和神经网络辐射场（NeRF）的优势，旨在从噪声二维分割掩膜生成一致的全景分割掩膜。该方法建立了全景感知结构化三维高斯模型，引入平滑性并设计有效的降噪策略。语义场方面，采用可靠语义锚点初始化三维高斯模型，作为训练过程中的平滑正则化。此外，还提出了一种利用渲染掩膜与噪声掩膜合并生成伪标签的自我训练方法，以提高PLGS的稳健性。在实例场方面，将二维实例掩膜投影到三维空间，并与定向边界框匹配，生成跨视图一致性的实例掩膜进行监管。实验表明，该方法在分割质量和速度方面均优于先前的方法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>提出PLGS方法，结合了3DGS的快速训练和渲染速度与NeRF的优越性。</li>
<li>建立全景感知结构化三维高斯模型，引入平滑性并设计有效的降噪策略。</li>
<li>采用可靠语义锚点初始化三维高斯模型，用作训练过程中的平滑正则化。</li>
<li>提出利用渲染掩膜与噪声掩膜合并生成伪标签的自我训练方法。</li>
<li>将二维实例掩膜投影到三维空间，并与定向边界框匹配，确保跨视图的一致性。</li>
<li>实验表明，PLGS方法在分割质量和速度方面均优于先前技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17505">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2e91cb7bf8334b5e040351149f84b492.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84f3546d05868c157d890090cb970938.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2a0aa3e5439af40222b13a73f8d8ef4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d83ec065efe74d5430cca5c13e369575.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3efef1a659ef791352fd6decd7a2b92a.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-08-06  From Pixels to Pathology Restoration Diffusion for   Diagnostic-Consistent Virtual IHC
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e6853f3bb5b1f7bcfd4c0da8bf95a38c.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2025-08-06  Uncertainty Estimation for Novel Views in Gaussian Splatting from   Primitive-Based Representations of Error and Visibility
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29997.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
