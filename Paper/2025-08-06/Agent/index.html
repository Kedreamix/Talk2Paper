<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-08-06  Hierarchical Learning-Based Control for Multi-Agent Shepherding of   Stochastic Autonomous Agents">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-9667208415f3a6b2b2fd2f0880e5b1ab.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    16.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    66 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-06-更新"><a href="#2025-08-06-更新" class="headerlink" title="2025-08-06 更新"></a>2025-08-06 更新</h1><h2 id="Hierarchical-Learning-Based-Control-for-Multi-Agent-Shepherding-of-Stochastic-Autonomous-Agents"><a href="#Hierarchical-Learning-Based-Control-for-Multi-Agent-Shepherding-of-Stochastic-Autonomous-Agents" class="headerlink" title="Hierarchical Learning-Based Control for Multi-Agent Shepherding of   Stochastic Autonomous Agents"></a>Hierarchical Learning-Based Control for Multi-Agent Shepherding of   Stochastic Autonomous Agents</h2><p><strong>Authors:Italo Napolitano, Stefano Covone, Andrea Lama, Francesco De Lellis, Mario di Bernardo</strong></p>
<p>Multi-agent shepherding represents a challenging distributed control problem where herder agents must coordinate to guide independently moving targets to desired spatial configurations. Most existing control strategies assume cohesive target behavior, which frequently fails in practical applications where targets exhibit stochastic autonomous behavior. This paper presents a hierarchical learning-based control architecture that decomposes the shepherding problem into a high-level decision-making module and a low-level motion control component. The proposed distributed control system synthesizes effective control policies directly from closed-loop experience without requiring explicit inter-agent communication or prior knowledge of target dynamics. The decentralized architecture achieves cooperative control behavior through emergent coordination without centralized supervision. Experimental validation demonstrates superior closed-loop performance compared to state-of-the-art heuristic control methods, achieving 100% success rates with improved settling times and control efficiency. The control architecture scales beyond its design conditions, adapts to time-varying goal regions, and demonstrates practical implementation feasibility through real-time experiments on the Robotarium platform. </p>
<blockquote>
<p>多智能体放牧代表了一个具有挑战性的分布式控制问题，其中牧羊人智能体必须协调行动，以引导独立移动的目标达到期望的空间配置。大多数现有的控制策略都假设目标行为是协调一致的，这在目标表现出随机自主行为的应用场景中往往会失败。本文提出了一种基于层次学习的控制架构，它将放牧问题分解为高层决策模块和底层运动控制组件。所提出的分布式控制系统直接从闭环经验中综合有效的控制策略，无需明确的智能体间通信或对目标动力学的先验知识。去中心化的架构通过涌现协调实现了无集中监管的合作控制行为。实验验证表明，与传统的启发式控制方法相比，该架构具有优越的闭环性能，实现了100%的成功率，缩短了稳定时间和提高了控制效率。该控制架构在设计条件之外进行了扩展，适应了时变目标区域，并通过Robotarium平台上的实时实验证明了其实施的可行性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02632v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于分层学习控制架构的多智能体牧羊问题解决方案。该架构分为高层决策模块和低层运动控制组件，可从闭环经验中直接合成有效的控制策略，无需明确的智能体间通信或对目标动力学的先验知识。该分散式架构通过突发协调实现控制行为，无需中央监督。实验验证表明，与传统的启发式控制方法相比，该架构具有优异的闭环性能，实现了更高的成功率、更快的结算时间和更高的控制效率。此外，该控制架构能够适应时间变化的目标区域，在Robotarium平台上进行实时实验，证明了其实施的可行性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多智能体牧羊问题是一个分布式控制挑战，需要智能体协调引导独立移动的目标达到期望的空间配置。</li>
<li>现有控制策略大多假设目标行为具有凝聚力，但在目标表现出随机自主行为时，这些策略常常会失效。</li>
<li>本文提出了一种基于分层学习控制架构的解决方案，该架构分为高层决策和低层运动控制。</li>
<li>该架构能从闭环经验中合成有效的控制策略，无需智能体间的明确通信或对目标动力学的先验知识。</li>
<li>分散式架构通过突发协调实现控制行为，无需中央监督，并通过实验验证其性能优于启发式控制方法。</li>
<li>控制架构具有良好的适应性，能够应对时间变化的目标区域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02632">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e1bdc47b251e171961f4dd435421e70f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e23b7d44c0a91d73c4912a121ab67963.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f9afd92a4e0839470a2b3b870b2ea6e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7cc1721c64e082b5f39d3a6730a631d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94ae7f573bfeeff5d3545dd72d07b48b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a9e20c8acdc6d09eb969fa5a0100a2a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HyCodePolicy-Hybrid-Language-Controllers-for-Multimodal-Monitoring-and-Decision-in-Embodied-Agents"><a href="#HyCodePolicy-Hybrid-Language-Controllers-for-Multimodal-Monitoring-and-Decision-in-Embodied-Agents" class="headerlink" title="HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and   Decision in Embodied Agents"></a>HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and   Decision in Embodied Agents</h2><p><strong>Authors:Yibin Liu, Zhixuan Liang, Zanxin Chen, Tianxing Chen, Mengkang Hu, Wanxi Dong, Congsheng Xu, Zhaoming Han, Yusen Qin, Yao Mu</strong></p>
<p>Recent advances in multimodal large language models (MLLMs) have enabled richer perceptual grounding for code policy generation in embodied agents. However, most existing systems lack effective mechanisms to adaptively monitor policy execution and repair codes during task completion. In this work, we introduce HyCodePolicy, a hybrid language-based control framework that systematically integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair into a closed-loop programming cycle for embodied agents. Technically, given a natural language instruction, our system first decomposes it into subgoals and generates an initial executable program grounded in object-centric geometric primitives. The program is then executed in simulation, while a vision-language model (VLM) observes selected checkpoints to detect and localize execution failures and infer failure reasons. By fusing structured execution traces capturing program-level events with VLM-based perceptual feedback, HyCodePolicy infers failure causes and repairs programs. This hybrid dual feedback mechanism enables self-correcting program synthesis with minimal human supervision. Our results demonstrate that HyCodePolicy significantly improves the robustness and sample efficiency of robot manipulation policies, offering a scalable strategy for integrating multimodal reasoning into autonomous decision-making pipelines. </p>
<blockquote>
<p>最近的多模态大型语言模型（MLLM）的进步为实体代理中的代码策略生成提供了更丰富的感知基础。然而，大多数现有系统缺乏有效的机制来在任务完成过程中自适应地监视策略执行和修复代码。在这项工作中，我们介绍了HyCodePolicy，这是一个基于混合语言的控制框架，它系统地集成了代码合成、几何基础、感知监控和迭代修复，形成一个闭环编程周期，用于实体代理。从技术上讲，给定自然语言指令，我们的系统首先将其分解为子目标并生成基于对象为中心的几何原始数据的初始可执行程序。然后，该程序在仿真中执行，同时视觉语言模型（VLM）观察选定的检查点以检测和定位执行故障并推断故障原因。通过将捕获程序级事件的结构化执行轨迹与基于VLM的感知反馈相结合，HyCodePolicy可以推断出失败原因并修复程序。这种混合双重反馈机制实现了自我修正的程序合成，只需最少的人工监督。我们的结果表明，HyCodePolicy显著提高了机器人操作策略的稳健性和样本效率，为将多模态推理集成到自主决策管道中提供了可伸缩的策略。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02629v1">PDF</a> Accepted to ICCV 2025 Workshop on Multi-Modal Reasoning for Agentic   Intelligence</p>
<p><strong>Summary</strong></p>
<p>随着多模态大型语言模型（MLLMs）的近期发展，为实体代理中的代码策略生成提供了更丰富的感知基础。然而，大多数现有系统缺乏在任务完成过程中自适应地监控策略执行和修复代码的有效机制。在此工作中，我们引入了HyCodePolicy，这是一个基于语言的控制框架，系统地集成了代码合成、几何基础、感知监控和迭代修复，为实体代理提供了一个闭环编程周期。给定自然语言指令，我们的系统首先将其分解为子目标并生成基于对象为中心的几何原始数据的初始可执行程序。该程序在模拟中执行，同时视觉语言模型（VLM）会观察选定检查点以检测和定位执行故障并推断故障原因。通过融合捕捉程序级事件的结构化执行轨迹与基于VLM的感知反馈，HyCodePolicy能够推断故障原因并修复程序。这种混合双重反馈机制实现了自我修正的程序合成，几乎无需人工监督。我们的结果表明，HyCodePolicy显著提高了机器人操作策略的稳健性和样本效率，为自主决策管道中多模式推理的集成提供了可扩展的策略。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）为实体代理中的代码策略生成提供了丰富的感知基础。</li>
<li>当前系统缺乏在任务执行过程中自适应监控和调整代码的能力。</li>
<li>HyCodePolicy是一个基于语言的控制框架，集成了代码合成、几何基础、感知监控和迭代修复。</li>
<li>系统通过分解自然语言指令生成初始可执行程序，该程序基于对象中心的几何原始数据。</li>
<li>程序在模拟环境中执行，期间由视觉语言模型（VLM）观察并检测执行故障。</li>
<li>通过结合结构化执行轨迹和基于VLM的感知反馈，HyCodePolicy能推断故障原因并进行程序修复。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02629">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2f1b1063d79cac75e2b5970f38e7c1b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e00bea4a7fb9f8f4ccfb2822d131f8ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-071ec1194a132ef9a07dfbf35bf0f5f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b03b45c362657aa8047b34093867ff8a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45c0ca75878461436ff4182c63ab2bee.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="HealthFlow-A-Self-Evolving-AI-Agent-with-Meta-Planning-for-Autonomous-Healthcare-Research"><a href="#HealthFlow-A-Self-Evolving-AI-Agent-with-Meta-Planning-for-Autonomous-Healthcare-Research" class="headerlink" title="HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous   Healthcare Research"></a>HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous   Healthcare Research</h2><p><strong>Authors:Yinghao Zhu, Yifan Qi, Zixiang Wang, Lei Gu, Dehao Sui, Haoran Hu, Xichen Zhang, Ziyi He, Liantao Ma, Lequan Yu</strong></p>
<p>The efficacy of AI agents in healthcare research is hindered by their reliance on static, predefined strategies. This creates a critical limitation: agents can become better tool-users but cannot learn to become better strategic planners, a crucial skill for complex domains like healthcare. We introduce HealthFlow, a self-evolving AI agent that overcomes this limitation through a novel meta-level evolution mechanism. HealthFlow autonomously refines its own high-level problem-solving policies by distilling procedural successes and failures into a durable, strategic knowledge base. To anchor our research and facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark featuring complex, realistic health data analysis tasks derived from peer-reviewed clinical research. Our comprehensive experiments demonstrate that HealthFlow’s self-evolving approach significantly outperforms state-of-the-art agent frameworks. This work marks a necessary shift from building better tool-users to designing smarter, self-evolving task-managers, paving the way for more autonomous and effective AI for scientific discovery. </p>
<blockquote>
<p>人工智能代理在医疗领域研究的效用受到其依赖静态、预设策略的阻碍。这产生了一个关键的局限性：代理可以变得更好的工具使用者，但不能学会成为更好的策略规划者，这对于医疗等复杂领域是一项至关重要的技能。我们引入了HealthFlow，这是一种自我进化的AI代理，它通过一种新的元级进化机制克服了这一局限性。HealthFlow自主地提炼其自己的高级问题解决策略，将程序的成功和失败转化为持久、战略性的知识库。为了巩固我们的研究并促进可重复评估，我们推出了EHRFlowBench，这是一个新的基准测试平台，它包含从同行评审的临床研究中得出的复杂且真实健康数据分析任务。我们的综合实验表明，HealthFlow的自我进化方法显著优于最新的代理框架。这项工作标志着从构建更好的工具使用者到设计更智能、自我进化的任务管理器的必要转变，为更自主、有效的AI科学发现铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02621v1">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/yhzhu99/HealthFlow">https://github.com/yhzhu99/HealthFlow</a></p>
<p><strong>Summary</strong></p>
<p>AI代理在医疗健康研究中的效能受限于其依赖静态、预设策略的问题。为解决这一瓶颈，我们推出HealthFlow，一种自我进化的AI代理，通过新型元级进化机制突破此限制。HealthFlow可自主优化其高级问题解决策略，将程序成功与失败经验提炼成持久、战略性的知识库。为支持研究并促进可重复评估，我们推出EHRFlowBench，以复杂、现实化的健康数据分析任务为特点的新基准测试，这些任务源自经过同行评审的临床研究。我们的综合实验显示，HealthFlow的自我进化方法显著优于当前最先进的代理框架。这项工作标志着从构建更好的工具用户转向设计更智能的自我进化任务管理器，为更自主和有效的AI科学发现铺平了道路。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AI代理在医疗健康研究中的效能受限。</li>
<li>现有AI代理主要作为工具用户，缺乏战略规划能力。</li>
<li>HealthFlow通过自我进化机制突破此限制，可自主优化高级问题解决策略。</li>
<li>HealthFlow将经验提炼成持久、战略性的知识库。</li>
<li>推出EHRFlowBench作为新型基准测试，以支持研究和促进评估的可重复性。</li>
<li>综合实验显示HealthFlow显著优于当前最先进的代理框架。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02621">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6583329b7d83f21eaa2c818413e17718.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50065c8704aab9311ac2e750f13e2bad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47f1a7ae4a529cb58f8300807f1536e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0899982d81a337c9cae6476ecf5eedd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5285ac4c2f2cec3122944a66f69b276f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60fefc2a8d9f17c4a0dda6a3e7915c90.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Emergence-of-Fair-Leaders-via-Mediators-in-Multi-Agent-Reinforcement-Learning"><a href="#Emergence-of-Fair-Leaders-via-Mediators-in-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Emergence of Fair Leaders via Mediators in Multi-Agent Reinforcement   Learning"></a>Emergence of Fair Leaders via Mediators in Multi-Agent Reinforcement   Learning</h2><p><strong>Authors:Akshay Dodwadmath, Setareh Maghsudi</strong></p>
<p>Stackelberg games and their resulting equilibria have received increasing attention in the multi-agent reinforcement learning literature. Each stage of a traditional Stackelberg game involves a leader(s) acting first, followed by the followers. In situations where the roles of leader(s) and followers can be interchanged, the designated role can have considerable advantages, for example, in first-mover advantage settings. Then the question arises: Who should be the leader and when? A bias in the leader selection process can lead to unfair outcomes. This problem is aggravated if the agents are self-interested and care only about their goals and rewards. We formally define this leader selection problem and show its relation to fairness in agents’ returns. Furthermore, we propose a multi-agent reinforcement learning framework that maximizes fairness by integrating mediators. Mediators have previously been used in the simultaneous action setting with varying levels of control, such as directly performing agents’ actions or just recommending them. Our framework integrates mediators in the Stackelberg setting with minimal control (leader selection). We show that the presence of mediators leads to self-interested agents taking fair actions, resulting in higher overall fairness in agents’ returns. </p>
<blockquote>
<p>斯塔克尔伯格（Stackelberg）博弈及其产生的均衡状态在多智能体强化学习文献中受到了越来越多的关注。传统的斯塔克尔伯格博弈的每个阶段都涉及一个或多个领导者率先行动，随后是追随者。在领导者和追随者的角色可以互换的情况下，指定的角色可以带来巨大的优势，例如在先行优势环境中。然后会出现一个问题：谁应该是领导者，又应该在何时？领导者选择过程中的偏见可能导致不公平的结果。如果智能体是自私的，只关心他们的目标和回报，那么这个问题就会加剧。我们正式定义了这个问题并展示了它与智能体回报公平性的关系。此外，我们提出了一个多智能体强化学习框架，通过引入中介来最大化公平性。中介以前已被用于具有不同控制级别的同时行动设置，例如直接执行智能体的行动或仅仅是提出建议。我们的框架在斯塔克尔伯格背景下整合中介，控制力度最小（领导层选择）。我们表明，中介的存在导致自私的智能体采取公平的行动，从而提高了智能体回报的整体公平性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02421v1">PDF</a> Accepted to ECAI 2025</p>
<p><strong>Summary</strong><br>在多重代理强化学习文献中，斯塔克尔伯格博弈及其产生的均衡状态越来越受到关注。传统的斯塔克尔伯格博弈每个阶段都有领导者先行，随后是追随者。在领导者和追随者角色可以互换的情况下，指定角色可以带来巨大的优势，例如在先行优势设置中。因此产生了一个问题：谁应该成为领导者，何时成为领导者？领导选择过程中的偏见可能导致不公平的结果。如果代理人是自私的，只关心他们的目标和回报，这个问题会更加严重。我们正式定义了这个领导选择问题，并展示了它与代理回报公平性的关系。此外，我们提出了一种多代理强化学习框架，通过引入调解者来最大化公平性。调解者以前已被用于同时行动的环境中，具有不同程度的控制力，如直接执行代理的行动或仅提供建议。我们的框架将调解者纳入斯塔克尔伯格设置中，具有最小的控制力（领导选择）。我们表明，调解者的存在导致自私的代理人采取公平行动，从而提高代理回报的整体公平性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>斯塔克尔伯格博弈在多代理强化学习中受到关注。</li>
<li>领导者和追随者角色的互换在博弈中具有优势，特别是在先行优势场景中。</li>
<li>领导选择过程中的偏见可能导致不公平结果。</li>
<li>代理的自私性会加剧这一问题。</li>
<li>提出了领导选择问题的正式定义，并与代理回报的公平性相联系。</li>
<li>引入调解者的多代理强化学习框架可以最大化公平性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02421">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-628bc9a17eb86cf3afa8405888a44afb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6574dc55891d02c02e2eb8d917003421.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db5dfeb1a47e3a51c9b5f05f5d47cb1b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c3b8b34f5ff29d94777f24bb7c22528e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a31227162febfed18b1f178796d7036.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CellForge-Agentic-Design-of-Virtual-Cell-Models"><a href="#CellForge-Agentic-Design-of-Virtual-Cell-Models" class="headerlink" title="CellForge: Agentic Design of Virtual Cell Models"></a>CellForge: Agentic Design of Virtual Cell Models</h2><p><strong>Authors:Xiangru Tang, Zhuoyun Yu, Jiapeng Chen, Yan Cui, Daniel Shao, Weixu Wang, Fang Wu, Yuchen Zhuang, Wenqi Shi, Zhi Huang, Arman Cohan, Xihong Lin, Fabian Theis, Smita Krishnaswamy, Mark Gerstein</strong></p>
<p>Virtual cell modeling represents an emerging frontier at the intersection of artificial intelligence and biology, aiming to predict quantities such as responses to diverse perturbations quantitatively. However, autonomously building computational models for virtual cells is challenging due to the complexity of biological systems, the heterogeneity of data modalities, and the need for domain-specific expertise across multiple disciplines. Here, we introduce CellForge, an agentic system that leverages a multi-agent framework that transforms presented biological datasets and research objectives directly into optimized computational models for virtual cells. More specifically, given only raw single-cell multi-omics data and task descriptions as input, CellForge outputs both an optimized model architecture and executable code for training virtual cell models and inference. The framework integrates three core modules: Task Analysis for presented dataset characterization and relevant literature retrieval, Method Design, where specialized agents collaboratively develop optimized modeling strategies, and Experiment Execution for automated generation of code. The agents in the Design module are separated into experts with differing perspectives and a central moderator, and have to collaboratively exchange solutions until they achieve a reasonable consensus. We demonstrate CellForge’s capabilities in single-cell perturbation prediction, using six diverse datasets that encompass gene knockouts, drug treatments, and cytokine stimulations across multiple modalities. CellForge consistently outperforms task-specific state-of-the-art methods. Overall, CellForge demonstrates how iterative interaction between LLM agents with differing perspectives provides better solutions than directly addressing a modeling challenge. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/gersteinlab/CellForge">https://github.com/gersteinlab/CellForge</a>. </p>
<blockquote>
<p>虚拟细胞建模是人工智能和生物学交叉领域的新兴前沿，旨在定量预测对不同扰动的反应等数量。然而，由于生物系统的复杂性、数据模态的异质性和多学科领域专业知识的需求，自主构建虚拟细胞计算模型具有挑战性。在这里，我们介绍了CellForge，一个智能系统，它利用多智能体框架直接将呈现的生物学数据集和研究目标转化为优化的计算模型，用于虚拟细胞。更具体地说，CellForge仅接受原始的单细胞多组学数据和任务描述作为输入，输出的是优化后的模型架构和可用于训练虚拟细胞模型和进行推断的可执行代码。该框架集成了三个核心模块：任务分析，用于呈现数据集表征和相关文献检索；方法设计，其中专业智能体协同开发优化建模策略；实验执行，用于自动生成代码。设计模块中的智能体分为不同角度的专家和一个中央协调者，他们必须协同交流解决方案，直到达成合理共识。我们利用涵盖基因敲除、药物治疗和细胞因子刺激的六个不同数据集，展示了CellForge在单细胞扰动预测方面的能力。CellForge的性能始终优于特定任务的最先进方法。总的来说，CellForge展示了不同角度的LLM智能体之间的迭代交互如何提供比直接解决建模挑战更好的解决方案。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/gersteinlab/CellForge%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/gersteinlab/CellForge公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02276v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>虚拟细胞建模是人工智能与生物学交叉领域的新兴前沿，旨在定量预测对多种扰动的响应。然而，自主构建虚拟细胞的计算模型具有挑战性，因为生物系统的复杂性、数据模态的异质性和多学科领域专业知识的需求。这里介绍CellForge，一个利用多智能体框架的agentic系统，可直接将呈现的生物学数据集和研究目标转化为优化的计算模型。CellForge接受原始单细胞多组学数据和任务描述作为输入，输出优化后的模型架构和可执行代码，用于训练和推断虚拟细胞模型。通过任务分析、方法设计和实验执行三个核心模块，实现了自动化生成代码。在单细胞扰动预测中，CellForge使用六个包含基因敲除、药物治疗和细胞因子刺激的多模态数据集进行演示，并始终优于特定任务的最先进方法。总体而言，CellForge展示了在不同视角的大型语言模型智能体之间的迭代交互如何提供比直接解决建模挑战更好的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>虚拟细胞建模是人工智能与生物学结合的新兴领域，旨在预测生物系统对扰动的响应。</li>
<li>CellForge是一个多智能体系统，能将生物数据集和研究目标转化为虚拟细胞模型。</li>
<li>CellForge接受原始单细胞多组学数据和任务描述为输入，并输出优化后的模型架构和训练代码。</li>
<li>CellForge包括三个核心模块：任务分析、方法设计和实验执行。</li>
<li>在单细胞扰动预测中，CellForge表现出卓越的性能，优于现有方法。</li>
<li>CellForge通过智能体间的迭代交互，实现了更好的解决方案。</li>
<li>CellForge的代码已公开可用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02276">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cdf8a8a3e0512e40aa4e7b561da76174.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9b8d9c9aec41c575dde3b7e66cf1d18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19a7a03f2c36b1258440486dcd43b77c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8400162dd794e1f4c8c1437d1f3e4a2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9667208415f3a6b2b2fd2f0880e5b1ab.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Patho-AgenticRAG-Towards-Multimodal-Agentic-Retrieval-Augmented-Generation-for-Pathology-VLMs-via-Reinforcement-Learning"><a href="#Patho-AgenticRAG-Towards-Multimodal-Agentic-Retrieval-Augmented-Generation-for-Pathology-VLMs-via-Reinforcement-Learning" class="headerlink" title="Patho-AgenticRAG: Towards Multimodal Agentic Retrieval-Augmented   Generation for Pathology VLMs via Reinforcement Learning"></a>Patho-AgenticRAG: Towards Multimodal Agentic Retrieval-Augmented   Generation for Pathology VLMs via Reinforcement Learning</h2><p><strong>Authors:Wenchuan Zhang, Jingru Guo, Hengzhe Zhang, Penghao Zhang, Jie Chen, Shuwan Zhang, Zhang Zhang, Yuhao Yi, Hong Bu</strong></p>
<p>Although Vision Language Models (VLMs) have shown strong generalization in medical imaging, pathology presents unique challenges due to ultra-high resolution, complex tissue structures, and nuanced clinical semantics. These factors make pathology VLMs prone to hallucinations, i.e., generating outputs inconsistent with visual evidence, which undermines clinical trust. Existing RAG approaches in this domain largely depend on text-based knowledge bases, limiting their ability to leverage diagnostic visual cues. To address this, we propose Patho-AgenticRAG, a multimodal RAG framework with a database built on page-level embeddings from authoritative pathology textbooks. Unlike traditional text-only retrieval systems, it supports joint text-image search, enabling direct retrieval of textbook pages that contain both the queried text and relevant visual cues, thus avoiding the loss of critical image-based information. Patho-AgenticRAG also supports reasoning, task decomposition, and multi-turn search interactions, improving accuracy in complex diagnostic scenarios. Experiments show that Patho-AgenticRAG significantly outperforms existing multimodal models in complex pathology tasks like multiple-choice diagnosis and visual question answering. Our project is available at the Patho-AgenticRAG repository: <a target="_blank" rel="noopener" href="https://github.com/Wenchuan-Zhang/Patho-AgenticRAG">https://github.com/Wenchuan-Zhang/Patho-AgenticRAG</a>. </p>
<blockquote>
<p>尽管视觉语言模型（VLMs）在医学成像中表现出了强大的泛化能力，但由于超高分辨率、复杂的组织结构和微妙的临床语义，病理学还是面临着独特的挑战。这些因素使得病理学VLMs容易产生幻觉，即生成与视觉证据不一致的输出，这损害了临床信任。目前该领域的RAG方法大多依赖于基于文本的知识库，限制了它们利用诊断视觉线索的能力。为了解决这一问题，我们提出了Patho-AgenticRAG，这是一个多模态RAG框架，其数据库建立在权威病理学教科书页面级别的嵌入之上。与传统的仅文本检索系统不同，它支持联合文本图像搜索，能够直接检索包含查询文本和相关视觉线索的教科书页面，从而避免了基于关键图像信息丢失的问题。Patho-AgenticRAG还支持推理、任务分解和多轮搜索交互，提高了复杂诊断场景中的准确性。实验表明，Patho-AgenticRAG在多项选择题诊断和视觉问答等复杂病理学任务上显著优于现有多模态模型。我们的项目可在Patho-AgenticRAG仓库中找到：<a target="_blank" rel="noopener" href="https://github.com/Wenchuan-Zhang/Patho-AgenticRAG%E3%80%82">https://github.com/Wenchuan-Zhang/Patho-AgenticRAG。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02258v1">PDF</a> </p>
<p><strong>Summary</strong><br>病理视语言模型面临独特挑战，包括超高分辨率、复杂组织结构和细微临床语义。Patho-AgenticRAG是一个多模态RAG框架，支持联合文本图像搜索，避免丢失关键图像信息，提高复杂诊断场景的准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>病理视语言模型（VLMs）在医学成像中具有良好的泛化能力，但在病理学领域面临独特挑战。</li>
<li>病理学的高分辨率、复杂组织结构和细微临床语义使模型易产生幻觉输出。</li>
<li>现有RAG方法主要依赖文本知识库，难以利用诊断视觉线索。</li>
<li>Patho-AgenticRAG是一个多模态RAG框架，支持联合文本图像搜索，避免信息丢失。</li>
<li>Patho-AgenticRAG能进行推理、任务分解和多轮搜索交互，提高复杂诊断场景的准确性。</li>
<li>实验显示，Patho-AgenticRAG在复杂病理学任务（如多选诊断和视觉问答）上显著优于现有多模态模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02258">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-383714874adb8c283a72898d8cf40c20.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b7d3a6bc7da7e065fed1cada15982e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9eb62662ced5d199cf4632bae1ece74a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-014b4ea2844b0d4895915050978ca620.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d38304ea1fed64524c9f9465a9e2a1e4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3f8b8677788ced309e6357cc3f2a8729.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SE-Agent-Self-Evolution-Trajectory-Optimization-in-Multi-Step-Reasoning-with-LLM-Based-Agents"><a href="#SE-Agent-Self-Evolution-Trajectory-Optimization-in-Multi-Step-Reasoning-with-LLM-Based-Agents" class="headerlink" title="SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning   with LLM-Based Agents"></a>SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning   with LLM-Based Agents</h2><p><strong>Authors:Jiaye Lin, Yifu Guo, Yuzhen Han, Sen Hu, Ziyi Ni, Licheng Wang, Mingguang Chen, Daxin Jiang, Binxing Jiao, Chen Hu, Huacan Wang</strong></p>
<p>Large Language Model (LLM)-based agents have recently shown impressive capabilities in complex reasoning and tool use via multi-step interactions with their environments. While these agents have the potential to tackle complicated tasks, their problem-solving process, i.e., agents’ interaction trajectory leading to task completion, remains underexploited. These trajectories contain rich feedback that can navigate agents toward the right directions for solving problems correctly. Although prevailing approaches, such as Monte Carlo Tree Search (MCTS), can effectively balance exploration and exploitation, they ignore the interdependence among various trajectories and lack the diversity of search spaces, which leads to redundant reasoning and suboptimal outcomes. To address these challenges, we propose SE-Agent, a Self-Evolution framework that enables Agents to optimize their reasoning processes iteratively. Our approach revisits and enhances former pilot trajectories through three key operations: revision, recombination, and refinement. This evolutionary mechanism enables two critical advantages: (1) it expands the search space beyond local optima by intelligently exploring diverse solution paths guided by previous trajectories, and (2) it leverages cross-trajectory inspiration to efficiently enhance performance while mitigating the impact of suboptimal reasoning paths. Through these mechanisms, SE-Agent achieves continuous self-evolution that incrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench Verified to resolve real-world GitHub issues. Experimental results across five strong LLMs show that integrating SE-Agent delivers up to 55% relative improvement, achieving state-of-the-art performance among all open-source agents on SWE-bench Verified. Our code and demonstration materials are publicly available at <a target="_blank" rel="noopener" href="https://github.com/wanghuacan/SE-Agent">https://github.com/wanghuacan/SE-Agent</a>. </p>
<blockquote>
<p>基于大型语言模型（LLM）的代理最近表现出通过多步骤与环境互动进行复杂推理和工具使用的惊人能力。虽然这些代理有潜力处理复杂任务，但他们的解决问题过程，即代理完成任务的互动轨迹，仍未得到充分探索。这些轨迹包含丰富的反馈，可以引导代理朝着正确方向解决问题。虽然现有的方法，如蒙特卡洛树搜索（MCTS），可以有效地平衡探索和利用，但它们忽略了不同轨迹之间的相互依赖性，并且缺乏搜索空间的多样性，这导致冗余推理和次优结果。为了解决这些挑战，我们提出了SE-Agent，这是一种自我进化框架，使代理能够迭代优化他们的推理过程。我们的方法通过三个关键操作来重新审视和改进先前的轨迹：修订、重组和细化。这种进化机制带来了两个关键优势：（1）它通过智能地探索以前轨迹引导的多样化解决方案路径，扩大了搜索空间，超越了局部最优；（2）它利用跨轨迹的灵感来有效地提高性能，同时减轻次优推理路径的影响。通过这些机制，SE-Agent实现了持续的自我进化，逐步提高了推理质量。我们在SWE-bench Verified上评估了SE-Agent，以解决现实世界中的GitHub问题。在五个强大的LLM上的实验结果表明，集成SE-Agent带来了高达55%的相对改进，在SWE-bench Verified上的性能达到了开源代理中的最佳水平。我们的代码和演示材料可在<a target="_blank" rel="noopener" href="https://github.com/wanghuacan/SE-Agent%E5%85%AC%E5%BC%80%E9%83%BD%E5%8F%96%E3%80%82">https://github.com/wanghuacan/SE-Agent公开获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02085v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的代理在复杂推理和工具使用方面展现出强大的能力，通过多步骤与环境的交互来完成任务。然而，这些代理的问题解决过程，即代理完成任务的交互轨迹，尚未得到充分研究。这些轨迹包含丰富的反馈，可以引导代理正确解决问题。虽然现有的方法如蒙特卡洛树搜索（MCTS）可以有效地平衡探索和利用，但它们忽略了不同轨迹之间的依赖性并缺乏搜索空间的多样性，导致冗余推理和次优结果。为解决这些问题，我们提出了SE-Agent，一个自我进化框架，使代理能够迭代优化其推理过程。通过修订、重组和细化等关键操作，该机制扩大了搜索空间并提高了性能。在SWE-bench Verified上评估SE-Agent解决GitHub实际问题的能力时，相较于五个主流LLM，其实现了高达55%的相对改进率并达到先进性能水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based agents demonstrate impressive capabilities in complex reasoning and tool use via multi-step interactions with environments.</li>
<li>Agents’ problem-solving trajectories contain rich feedback that can guide them to solve problems correctly.</li>
<li>Existing methods like MCTS ignore the interdependence among trajectories and lack search space diversity, leading to redundant reasoning and suboptimal outcomes.</li>
<li>SE-Agent framework enables iterative optimization of reasoning processes for agents through three key operations: revision, recombination, and refinement.</li>
<li>SE-Agent expands search space beyond local optima and leverages cross-trajectory inspiration to enhance performance.</li>
<li>SE-Agent achieves continuous self-evolution, incrementally improving reasoning quality.</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02085">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0b7f2fbb98b7e8c38dc87889e0d9ce0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4bea939b287932cfc986fed5e11ab9cc.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Agent-Based-Feature-Generation-from-Clinical-Notes-for-Outcome-Prediction"><a href="#Agent-Based-Feature-Generation-from-Clinical-Notes-for-Outcome-Prediction" class="headerlink" title="Agent-Based Feature Generation from Clinical Notes for Outcome   Prediction"></a>Agent-Based Feature Generation from Clinical Notes for Outcome   Prediction</h2><p><strong>Authors:Jiayi Wang, Jacqueline Jil Vallon, Neil Panjwani, Xi Ling, Sushmita Vij, Sandy Srinivas, John Leppert, Mark K. Buyyounouski, Mohsen Bayati</strong></p>
<p>Electronic health records (EHRs) contain rich unstructured clinical notes that could enhance predictive modeling, yet extracting meaningful features from these notes remains challenging. Current approaches range from labor-intensive manual clinician feature generation (CFG) to fully automated representational feature generation (RFG) that lack interpretability and clinical relevance. Here we introduce SNOW (Scalable Note-to-Outcome Workflow), a modular multi-agent system powered by large language models (LLMs) that autonomously generates structured clinical features from unstructured notes without human intervention. We evaluated SNOW against manual CFG, clinician-guided LLM approaches, and RFG methods for predicting 5-year prostate cancer recurrence in 147 patients from Stanford Healthcare. While manual CFG achieved the highest performance (AUC-ROC: 0.771), SNOW matched this performance (0.761) without requiring any clinical expertise, significantly outperforming both baseline features alone (0.691) and all RFG approaches. The clinician-guided LLM method also performed well (0.732) but still required expert input. SNOW’s specialized agents handle feature discovery, extraction, validation, post-processing, and aggregation, creating interpretable features that capture complex clinical information typically accessible only through manual review. Our findings demonstrate that autonomous LLM systems can replicate expert-level feature engineering at scale, potentially transforming how clinical ML models leverage unstructured EHR data while maintaining the interpretability essential for clinical deployment. </p>
<blockquote>
<p>电子健康记录（EHRs）包含丰富的非结构化临床笔记，这些笔记可以增强预测模型的性能，然而从这些笔记中提取有意义的特征仍然是一个挑战。当前的方法范围从劳动密集型的手动临床医生特征生成（CFG）到缺乏可解释性和临床相关性的完全自动化的代表性特征生成（RFG）。在这里，我们介绍了SNOW（可扩展的笔记到结果工作流程），这是一个由大型语言模型（LLM）驱动的模块化多代理系统，能够无需人工干预，从非结构化笔记中自主生成结构化临床特征。我们在斯坦福医疗健康的147名患者中，用SNOW与手动CFG、临床医生指导的LLM方法和RFG方法预测前列腺癌5年复发率进行比较评估。虽然手动CFG表现最佳（AUC-ROC：0.771），但SNOW与之相匹配（0.761），无需任何临床经验，显著优于基本特征（0.691）和所有RFG方法。临床医生指导的LLM方法表现良好（0.732），但仍需专家输入。SNOW的专用代理处理特征发现、提取、验证、后处理和聚合，创建可解释的特征，这些特征能够捕获通常只能通过手动审查才能获得的复杂临床信息。我们的研究结果表明，自主化的LLM系统能够在大规模上复制专家级的特征工程，有可能改变临床机器学习模型如何利用非结构化EHR数据的方式，同时保持对于临床部署至关重要的可解释性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01956v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了名为SNOW的模块化多智能体系统，该系统利用大型语言模型（LLM）从非结构化病历记录中提取结构化临床特征，用于预测前列腺癌复发的风险。在斯坦福医疗机构的测试中，尽管手动CFG表现最佳（AUC-ROC：0.771），但SNOW系统无需临床专家参与即实现了与之相当的性能（AUC-ROC：0.761），显著优于基本特征（AUC-ROC：0.691）和全自动RFG方法。SNOW系统由多个智能体组成，负责特征发现、提取、验证、后处理和聚合，可生成易于理解的特征，捕捉复杂临床信息，这通常是只能通过人工审查才能获取的。这些发现证明，自主的大型语言模型系统在大规模特征工程上可模拟专家级的表现，可能为临床部署中的机器学习模型提供更高效的非结构化病历数据利用方式并保持必要的可解释性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SNOW系统利用大型语言模型（LLM）从非结构化病历记录中提取结构化临床特征。</li>
<li>SNOW系统的性能与手动CFG相当，但无需临床专家参与。</li>
<li>SNOW系统通过多个智能体进行特征发现、提取、验证、后处理和聚合。</li>
<li>SNOW系统能够生成易于理解的特征，捕捉复杂临床信息。</li>
<li>自主的大型语言模型系统在特征工程上可模拟专家级的表现。</li>
<li>SNOW系统的应用可提升临床机器学习模型对非结构化病历数据的利用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01956">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-804da4e86d3cf863013c2e5eae9ed990.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-395e1a0b1dff23b9c2e6ee51bf9b4fed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21be5355ff492c9450aeeb1a399a1029.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="StreamAgent-Towards-Anticipatory-Agents-for-Streaming-Video-Understanding"><a href="#StreamAgent-Towards-Anticipatory-Agents-for-Streaming-Video-Understanding" class="headerlink" title="StreamAgent: Towards Anticipatory Agents for Streaming Video   Understanding"></a>StreamAgent: Towards Anticipatory Agents for Streaming Video   Understanding</h2><p><strong>Authors:Haolin Yang, Feilong Tang, Linxiao Zhao, Xiang An, Ming Hu, Huifa Li, Xinlin Zhuang, Boqian Wang, Yifan Lu, Xiaofeng Zhang, Abdalla Swikir, Junjun He, Zongyuan Ge, Imran Razzak</strong></p>
<p>Real-time streaming video understanding in domains such as autonomous driving and intelligent surveillance poses challenges beyond conventional offline video processing, requiring continuous perception, proactive decision making, and responsive interaction based on dynamically evolving visual content. However, existing methods rely on alternating perception-reaction or asynchronous triggers, lacking task-driven planning and future anticipation, which limits their real-time responsiveness and proactive decision making in evolving video streams. To this end, we propose a StreamAgent that anticipates the temporal intervals and spatial regions expected to contain future task-relevant information to enable proactive and goal-driven responses. Specifically, we integrate question semantics and historical observations through prompting the anticipatory agent to anticipate the temporal progression of key events, align current observations with the expected future evidence, and subsequently adjust the perception action (e.g., attending to task-relevant regions or continuously tracking in subsequent frames). To enable efficient inference, we design a streaming KV-cache memory mechanism that constructs a hierarchical memory structure for selective recall of relevant tokens, enabling efficient semantic retrieval while reducing the overhead of storing all tokens in the traditional KV-cache. Extensive experiments on streaming and long video understanding tasks demonstrate that our method outperforms existing methods in response accuracy and real-time efficiency, highlighting its practical value for real-world streaming scenarios. </p>
<blockquote>
<p>在自动驾驶和智能监控等领域，实时流媒体视频理解面临着超越传统离线视频处理的挑战。这需要我们进行连续感知、主动决策，以及基于动态演化视觉内容的响应交互。然而，现有方法依赖于交替感知反应或异步触发，缺乏任务驱动的规划和未来预期，这限制了它们在实时流媒体中的实时响应能力和主动决策制定。为此，我们提出了StreamAgent，能够预测未来包含任务相关信息的时间间隔和空间区域，以实现以目标和主动的反应。具体来说，我们通过提示预测代理整合问题语义和历史观察，以预测关键事件的时间进展，将当前观察与预期的未来证据对齐，随后调整感知行为（例如，关注任务相关区域或在后续帧中持续跟踪）。为了实现高效推理，我们设计了一种流式KV缓存机制，构建了一种分层内存结构，用于选择性回忆相关令牌，实现在不存储所有令牌的传统KV缓存的同时进行高效语义检索。在流媒体和长视频理解任务上的大量实验表明，我们的方法在响应准确性和实时效率方面优于现有方法，凸显了其在真实世界流媒体场景中的实用价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01875v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>实时流媒体视频理解在自动驾驶和智能监控等领域面临传统离线视频处理无法应对的挑战，需要持续感知、主动决策和基于动态演化视觉内容的响应交互。然而，现有方法依赖于交替感知反应或异步触发，缺乏任务驱动的规划和未来预期，这限制了其在实时流媒体中的响应能力和主动决策。为此，我们提出了StreamAgent，可预测未来任务相关信息的预期时间间隔和空间区域，以实现目标驱动的响应。具体来说，我们通过整合问题语义和历史观察，促使预测代理预测关键事件的时间进展，将当前观察与预期的未来证据对齐，并据此调整感知行动（例如，关注任务相关区域或在后续帧中进行持续跟踪）。为提升推理效率，我们设计了一种流式KV缓存机制，构建分层内存结构以实现选择性召回相关令牌，从而在减少存储所有令牌的传统KV缓存开销的同时，实现高效语义检索。在流媒体和长视频理解任务上的大量实验表明，我们的方法在响应准确性和实时效率方面优于现有方法，突显其在真实世界流媒体场景中的实用价值。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>实时流媒体视频理解面临超越传统离线视频处理的挑战，需要持续感知、主动决策和响应交互。</li>
<li>现有方法缺乏任务驱动的规划和未来预期，限制了实时响应能力和主动决策。</li>
<li>提出的StreamAgent能够预测未来任务相关信息的预期时间间隔和空间区域。</li>
<li>通过整合问题语义和历史观察，StreamAgent可以促使预测代理预测关键事件的时间进展。</li>
<li>StreamAgent能够调整感知行动，例如关注任务相关区域或在后续帧中进行持续跟踪。</li>
<li>设计了流式KV缓存机制，实现高效语义检索和减少存储开销。</li>
<li>在流媒体和长视频理解任务上的实验表明，StreamAgent在响应准确性和实时效率方面优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01875">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f0a322e89a14c8913098a83056c4f167.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da8f345e4cb79d9109a343dcaf1df819.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1d428b2932a8049520c7e9dae36ca77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-336bd406f4175d31309dc8ecb170e644.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-990e8dddf959acfac63f7f3bf6a5df58.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Web-CogReasoner-Towards-Knowledge-Induced-Cognitive-Reasoning-for-Web-Agents"><a href="#Web-CogReasoner-Towards-Knowledge-Induced-Cognitive-Reasoning-for-Web-Agents" class="headerlink" title="Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web   Agents"></a>Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web   Agents</h2><p><strong>Authors:Yuhan Guo, Cong Guo, Aiwen Sun, Hongliang He, Xinyu Yang, Yue Lu, Yingji Zhang, Xuntao Guo, Dong Zhang, Jianzhuang Liu, Jiang Duan, Yijia Xiao, Liangjian Wen, Hai-Ming Xu, Yong Dai</strong></p>
<p>Multimodal large-scale models have significantly advanced the development of web agents, enabling perception and interaction with digital environments akin to human cognition. In this paper, we argue that web agents must first acquire sufficient knowledge to effectively engage in cognitive reasoning. Therefore, we decompose a web agent’s capabilities into two essential stages: knowledge content learning and cognitive processes. To formalize this, we propose Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and Procedural. In this framework, knowledge content learning corresponds to the agent’s processes of Memorizing and Understanding, which rely on the first two knowledge types, representing the “what” of learning. Conversely, cognitive processes correspond to Exploring, grounded in Procedural knowledge, defining the “how” of reasoning and action. To facilitate knowledge acquisition, we construct the Web-CogDataset, a structured resource curated from 14 real-world websites, designed to systematically instill core knowledge necessary for web agent. This dataset serves as the agent’s conceptual grounding-the “nouns” upon which comprehension is built-as well as the basis for learning how to reason and act. Building on this foundation, we operationalize these processes through a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing and training our proposed agent, the Web-CogReasoner. Extensive experimentation reveals its significant superiority over existing models, especially in generalizing to unseen tasks where structured knowledge is decisive. To enable rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation suite designed to assess and compare agent performance across the delineated knowledge domains and cognitive capabilities. Our code and data is open sourced at <a target="_blank" rel="noopener" href="https://github.com/Gnonymous/Web-CogReasoner">https://github.com/Gnonymous/Web-CogReasoner</a> </p>
<blockquote>
<p>多模态大规模模型已经显著推动了网络代理的发展，使其能够感知和与数字环境进行类似于人类认知的交互。在本文中，我们认为网络代理必须首先获得足够的知识以有效地参与认知推理。因此，我们将网络代理的能力分解为两个关键阶段：知识内容学习和认知过程。为了正式提出这一点，我们提出了Web-CogKnowledge框架，将知识分类为事实、概念和程序三类。在此框架中，知识内容学习对应于代理的记忆和理解过程，这依赖于前两种知识类型，代表学习的“内容是什么”。相反，认知过程对应于基于程序知识的探索，定义了推理和行动方式的“如何”。为了促进知识获取，我们构建了Web-Cog数据集，这是一个从14个真实网站精心策划的结构化资源，旨在系统地灌输网络代理所需的核心知识。该数据集作为代理的概念基础——构建理解的“名词”，以及学习如何推理和行动的基础。在此基础上，我们通过新的知识驱动的思考链（CoT）推理框架来实施这些过程，开发和训练我们提出的代理——Web-CogReasoner。大量实验表明，与现有模型相比，它具有显著的优势，特别是在处理未见过的任务时，结构化知识是起决定性的。为了进行严格的评估，我们引入了Web-CogBench，这是一套全面的评估工具，旨在评估和比较代理在不同划分的知识领域和认知能力方面的表现。我们的代码和数据在<a target="_blank" rel="noopener" href="https://github.com/Gnonymous/Web-CogReasoner%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/Gnonymous/Web-CogReasoner开源。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01858v1">PDF</a> Our code and data is open sourced at   <a target="_blank" rel="noopener" href="https://github.com/Gnonymous/Web-CogReasoner">https://github.com/Gnonymous/Web-CogReasoner</a></p>
<p><strong>Summary</strong><br>     本文主张Web代理需先获取充足知识再参与认知推理，提出Web-CogKnowledge Framework，将知识分为事实、概念与程序三类。为促使知识获取，构建Web-CogDataset，通过结构化资源教授核心知识。在此基础上提出Chain-of-Thought（CoT）推理框架并训练Web-CogReasoner代理。实验证明其在未见任务上的表现优于现有模型。同时推出Web-CogBench评估套件来评估代理性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Web代理在开发过程中，首先需要获取充足的知识来进行有效的认知推理。</li>
<li>提出Web-CogKnowledge Framework框架，将知识分为事实、概念和程序三类。</li>
<li>构建Web-CogDataset数据集，从真实世界的网站中挑选结构化资源，为代理提供核心知识的训练基础。</li>
<li>引入Chain-of-Thought（CoT）推理框架，使代理能在获取的知识基础上进行推理操作。</li>
<li>训练出Web-CogReasoner代理，并在一系列实验中证明了其性能显著优于现有模型。</li>
<li>推出Web-CogBench评估套件，用于评估代理在不同知识领域和认知能力方面的表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01858">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3f2482ff355f92c6997ee058a612d80d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef234964cf0f399bfb6ac0b23200ee88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-448ad4e4b5a7abfcea94db0e958d502b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="AGENTICT-2-S-Robust-Text-to-SPARQL-via-Agentic-Collaborative-Reasoning-over-Heterogeneous-Knowledge-Graphs-for-the-Circular-Economy"><a href="#AGENTICT-2-S-Robust-Text-to-SPARQL-via-Agentic-Collaborative-Reasoning-over-Heterogeneous-Knowledge-Graphs-for-the-Circular-Economy" class="headerlink" title="AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning   over Heterogeneous Knowledge Graphs for the Circular Economy"></a>AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning   over Heterogeneous Knowledge Graphs for the Circular Economy</h2><p><strong>Authors:Yang Zhao, Chengxiao Dai, Wei Zhuo, Tan Chuan Fu, Yue Xiu, Dusit Niyato, Jonathan Z. Low, Eugene Ho Hong Zhuang, Daren Zong Loong Tan</strong></p>
<p>Question answering over heterogeneous knowledge graphs (KGQA) involves reasoning across diverse schemas, incomplete alignments, and distributed data sources. Existing text-to-SPARQL approaches rely on large-scale domain-specific fine-tuning or operate within single-graph settings, limiting their generalizability in low-resource domains and their ability to handle queries spanning multiple graphs. These challenges are particularly relevant in domains such as the circular economy, where information about classifications, processes, and emissions is distributed across independently curated knowledge graphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes KGQA into subtasks managed by specialized agents responsible for retrieval, query generation, and verification. A scheduler assigns subgoals to different graphs using weak-to-strong alignment strategies. A two-stage verifier detects structurally invalid and semantically underspecified queries through symbolic validation and counterfactual consistency checks. Experiments on real-world circular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy by 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing the average prompt length by 46.4%. These results demonstrate the benefits of agent-based schema-aware reasoning for scalable KGQA and support decision-making in sustainability domains through robust cross-graph reasoning. </p>
<blockquote>
<p>问答系统在异质知识图谱（KGQA）中的应用涉及跨不同模式、不完整对齐和分布式数据源的推理。现有的文本到SPARQL的方法依赖于大规模的特定领域的微调，或者在单个图环境中运行，这在低资源领域中限制了其通用性，也限制了它们处理跨多个图的查询的能力。这些挑战在循环经济等领域尤为突出，循环经济中的分类、过程和排放信息分布在独立维护的知识图谱（KGs）中。我们提出了AgenticT$^2$S，这是一个模块化框架，它将KGQA分解成由专门代理负责检索、查询生成和验证的子任务。调度器使用弱到强的对齐策略将子目标分配给不同的图形。两阶段验证器通过符号验证和反向事实一致性检查来检测结构无效和语义未指定的查询。在现实世界循环经济知识图谱上的实验表明，与最佳基线相比，AgenticT$^2$S的执行精度提高了17.3%，三重F$_1$提高了25.4%，同时平均提示长度减少了46.4%。这些结果证明了基于代理的模式感知推理在可扩展的KGQA中的好处，并通过可靠的跨图推理支持可持续性领域的决策制定。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01815v1">PDF</a> </p>
<p><strong>Summary</strong><br>知识图谱问答（KGQA）面临跨不同模式、不完全对齐和分布式数据源的推理挑战。现有文本到SPARQL的转换方法依赖于大规模特定领域的微调或在单一图谱环境中运行，这限制了其在低资源领域中的通用性以及处理跨多图谱查询的能力。特别是在循环经济等领域，信息分散在独立编制的知识图谱中。我们提出了AgenticT$^2$S，一个模块化框架，将KGQA分解成由专门代理负责检索、查询生成和验证的子任务。调度器使用弱到强的对齐策略将子目标分配给不同的图谱。两阶段验证器通过符号验证和反事实一致性检查检测结构和语义上未指定的查询。在真实循环经济知识图谱上的实验表明，AgenticT$^2$S相较于最佳基线，执行精度提高17.3%，三元组级别F$_1$提高25.4%，平均提示长度减少46.4%。这表明基于代理的模式感知推理对于可扩展的KGQA和可持续性领域的决策支持具有优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>KGQA面临跨不同知识图谱的推理挑战，需处理多样模式、不完全对齐和分布式数据源。</li>
<li>现有文本到SPARQL的转换方法缺乏通用性，难以处理跨多图谱查询。</li>
<li>AgenticT$^2$S是一个模块化框架，通过专门代理处理KGQA的不同子任务，如检索、查询生成和验证。</li>
<li>调度器根据弱到强的对齐策略分配子目标到不同图谱。</li>
<li>两阶段验证器检测结构和语义上未指定的查询。</li>
<li>在真实循环经济知识图谱上的实验表明，AgenticT$^2$S较现有方法有明显性能提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01815">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-111e9a1826250d0fd9e91ab96e587750.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e5ab24816efd01d6f89af3b74c4ffcf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cffb252e83ecd909099ce46edca8a8a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3477f14a12cf50f32f6d714eccff55a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ce4780a1ad0b8421fad304f1bf3141c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9197d2868e37de21e55e3c519037ab06.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b122537bc2123cf8cea965ba0dfcf09.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Collaborative-Medical-Triage-under-Uncertainty-A-Multi-Agent-Dynamic-Matching-Approach"><a href="#Collaborative-Medical-Triage-under-Uncertainty-A-Multi-Agent-Dynamic-Matching-Approach" class="headerlink" title="Collaborative Medical Triage under Uncertainty: A Multi-Agent Dynamic   Matching Approach"></a>Collaborative Medical Triage under Uncertainty: A Multi-Agent Dynamic   Matching Approach</h2><p><strong>Authors:Hongyan Cheng, Chengzhang Yu, Yanshu Shi, Chiyue Wang, Cong Liu, Zhanpeng Jin</strong></p>
<p>The post-pandemic surge in healthcare demand, coupled with critical nursing shortages, has placed unprecedented pressure on medical triage systems, necessitating innovative AI-driven solutions. We present a multi-agent interactive intelligent system for medical triage that addresses three fundamental challenges in current AI-based triage systems: inadequate medical specialization leading to misclassification, heterogeneous department structures across healthcare institutions, and inefficient detail-oriented questioning that impedes rapid triage decisions. Our system employs three specialized agents–RecipientAgent, InquirerAgent, and DepartmentAgent–that collaborate through Inquiry Guidance mechanism and Classification Guidance Mechanism to transform unstructured patient symptoms into accurate department recommendations. To ensure robust evaluation, we constructed a comprehensive Chinese medical triage dataset from “Ai Ai Yi Medical Network”, comprising 3,360 real-world cases spanning 9 primary departments and 62 secondary departments. Experimental results demonstrate that our multi-agent system achieves 89.6% accuracy in primary department classification and 74.3% accuracy in secondary department classification after four rounds of patient interaction. The system’s dynamic matching based guidance mechanisms enable efficient adaptation to diverse hospital configurations while maintaining high triage accuracy. We successfully developed this multi-agent triage system that not only adapts to organizational heterogeneity across healthcare institutions but also ensures clinically sound decision-making. </p>
<blockquote>
<p>后疫情时代医疗需求的激增，加上护理人员的严重短缺，给医疗分流系统带来了前所未有的压力，急需创新的AI驱动解决方案。我们提出了一种用于医疗分流的多智能体交互式系统，解决了当前基于AI的分流系统中的三个基本挑战：医疗专业化不足导致误分类、医疗机构之间部门结构异质、以及效率低下、细节导向的提问阻碍快速分流决策。我们的系统采用三个专用智能体——RecipientAgent、InquirerAgent和DepartmentAgent，它们通过查询指导机制和分类指导机制进行协作，将非结构化的患者症状转化为准确的部门推荐。为确保稳健评估，我们从“爱爱医疗网”构建了一个全面的中文医疗分流数据集，包含3360个真实案例，涵盖9个主要部门和62个次要部门。实验结果表明，我们的多智能体系统在四轮患者互动后，在主要部门分类方面达到了89.6%的准确率，在次要部门分类方面达到了74.3%的准确率。该系统的动态匹配指导机制能够高效地适应不同的医院配置，同时保持较高的分流准确率。我们成功开发了这个多智能体分流系统，它不仅适应于医疗机构之间的组织异质性，而且确保临床决策的科学性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22504v2">PDF</a> 10 pages, 8 figures, 2 table</p>
<p><strong>总结</strong><br>    疫情后医疗保健需求的激增与护理人员的短缺给医疗分流系统带来了前所未有的压力，迫切需要创新的人工智能驱动解决方案。针对当前AI分流系统中的三个基本挑战，即医疗专业化不足导致的误分类、医疗机构之间部门结构差异大和不详尽的细节问题导致的分流决策效率低下等问题，我们提出了一种多智能体互动智能系统进行医疗分流。该系统采用三个专门代理：RecipientAgent、InquirerAgent和DepartmentAgent，通过询问指导和分类指导机制协同工作，将患者的不结构化症状转化为准确的科室推荐。为了确保有效的评估，我们从“爱爱医医疗网络”构建了一个全面的中文医疗分流数据集，包含涵盖9个主要科室和62个二级科室的3360个真实病例。实验结果显示，经过四轮的患者互动后，该多智能系统主要科室分类准确度达到了89.6%，二级科室分类准确度达到了74.3%。该系统的动态匹配指导机制能够实现高效的医院配置适应，同时保持较高的分流准确性。我们成功开发了这个多智能分流系统，不仅适应于医疗机构间的组织异质性，而且确保临床决策的科学性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>疫情后的医疗保健需求激增和护理短缺给医疗分流系统带来了压力，需要AI解决方案。</li>
<li>当前AI医疗分流系统面临三大挑战：医疗专业化不足、医疗机构部门结构差异大和效率问题。</li>
<li>提出了一种多智能体互动智能系统，包括三个专门代理：RecipientAgent、InquirerAgent和DepartmentAgent。</li>
<li>通过询问指导和分类指导机制，智能系统可将患者的不结构化症状转化为准确的科室推荐。</li>
<li>在包含真实病例的中文医疗分流数据集上进行评估，该系统表现良好，主要科室分类准确度高。</li>
<li>系统的动态匹配指导机制能适应不同的医院配置，同时保持较高的分流准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22504">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a3da58f9ad030263fb73e6ed308e5016.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20286432eadcdf5f23df851e01a78324.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-434b2667cfd235646a507b91ee78998e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb477c1378d6dff9bedc66ce3cca704f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbd808d8692665741ce01194e3a0cdd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b4d89f1ed8b412ec02431ee17b248c0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GEM-Gaussian-Embedding-Modeling-for-Out-of-Distribution-Detection-in-GUI-Agents"><a href="#GEM-Gaussian-Embedding-Modeling-for-Out-of-Distribution-Detection-in-GUI-Agents" class="headerlink" title="GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in   GUI Agents"></a>GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in   GUI Agents</h2><p><strong>Authors:Zheng Wu, Pengzhou Cheng, Zongru Wu, Lingzhong Dong, Zhuosheng Zhang</strong></p>
<p>Graphical user interface (GUI) agents have recently emerged as an intriguing paradigm for human-computer interaction, capable of automatically executing user instructions to operate intelligent terminal devices. However, when encountering out-of-distribution (OOD) instructions that violate environmental constraints or exceed the current capabilities of agents, GUI agents may suffer task breakdowns or even pose security threats. Therefore, effective OOD detection for GUI agents is essential. Traditional OOD detection methods perform suboptimally in this domain due to the complex embedding space and evolving GUI environments. In this work, we observe that the in-distribution input semantic space of GUI agents exhibits a clustering pattern with respect to the distance from the centroid. Based on the finding, we propose GEM, a novel method based on fitting a Gaussian mixture model over input embedding distances extracted from the GUI agent that reflect its capability boundary. Evaluated on eight datasets spanning smartphones, computers, and web browsers, our method achieves an average accuracy improvement of 23.70% over the best-performing baseline while only increasing training time by 4.9% and testing time by 6.5%. We also experimentally demonstrate that GEM can improve the step-wise success rate by 9.40% by requesting assistance from the cloud model when encountering OOD samples. Analysis verifies the generalization ability of our method through experiments on nine different backbones. The codes are available at <a target="_blank" rel="noopener" href="https://github.com/Wuzheng02/GEM-OODforGUIagents">https://github.com/Wuzheng02/GEM-OODforGUIagents</a>. </p>
<blockquote>
<p>图形用户界面（GUI）代理作为人机交互的一种引人入胜的模式最近已经出现，它能够自动执行用户指令来操作智能终端设备。然而，当遇到超出分布（OOD）的指令，这些指令可能违反环境约束或超出代理的当前能力时，GUI代理可能会发生任务故障，甚至可能构成安全威胁。因此，有效的GUI代理的OOD检测至关重要。由于嵌入空间的复杂性和GUI环境的不断变化，传统的OOD检测方法在这个领域表现不佳。在这项工作中，我们发现GUI代理的内置分布输入语义空间在距离质心方面表现出聚类模式。基于这一发现，我们提出了基于高斯混合模型拟合的GEM新方法，该方法对从GUI代理提取的输入嵌入距离进行建模，反映了其能力边界。在涵盖智能手机、计算机和网页浏览器的八个数据集上评估，我们的方法在最先进的基线方法上平均提高了23.70%的准确率，同时仅将训练时间增加4.9%，测试时间增加6.5%。我们还通过实验证明，当遇到OOD样本时，通过请求云模型协助，GEM可以提高步骤成功率9.4%。分析并通过在九个不同主干网络上进行的实验验证了我们的方法的泛化能力。相关代码可通过链接<a target="_blank" rel="noopener" href="https://github.com/Wuzheng02/GEM-OODforGUIagents%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Wuzheng02/GEM-OODforGUIagents访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12842v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨图形用户界面（GUI）代理在面临超出其能力范围或违反环境约束的未知指令时面临的挑战，并提出一种基于高斯混合模型的方法（GEM）进行异常检测。该方法能有效识别出GUI代理在智能手机、计算机和网页浏览器等不同平台上的异常输入，相较于现有最佳基线模型，其准确率平均提高23.70%，同时仅增加训练时间4.9%和测试时间6.5%。当遇到超出代理能力的样本时，通过请求云模型协助，可进一步提高步骤成功率9.40%。实验证明该方法具有良好的泛化能力，相关代码已公开于GitHub。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GUI代理在面临超出其能力范围或违反环境约束的未知指令时可能出现问题。</li>
<li>传统OOD检测方法在GUI环境中表现不佳，主要由于环境复杂和变化性大。</li>
<li>本文观察到GUI代理的输入语义空间具有特定的聚类模式，基于此提出了基于高斯混合模型的GEM方法。</li>
<li>GEM方法在不同平台上实现了较高的准确率提升，且对训练和测试时间的影响较小。</li>
<li>当遇到未知样本时，通过请求云模型协助，可进一步提高代理的性能。</li>
<li>实验证明GEM方法具有良好的泛化能力，并适用于多种不同的模型架构。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12842">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d23f1f13aa38d5cc6a37242728bd2846.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed0b213bd5f2f06b369a58934137a559.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93286aca50f6d7492bccd07f3cbdf121.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d11ed813242b14fec95f65eda28f1a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9476b1ac5c9690bc7535feebb29fb81f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7712fd477bee525baff1c5a7cfcb16fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f1323ab0da352e826c7ac45bab91788.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Interpreting-Multi-band-Galaxy-Observations-with-Large-Language-Model-Based-Agents"><a href="#Interpreting-Multi-band-Galaxy-Observations-with-Large-Language-Model-Based-Agents" class="headerlink" title="Interpreting Multi-band Galaxy Observations with Large Language   Model-Based Agents"></a>Interpreting Multi-band Galaxy Observations with Large Language   Model-Based Agents</h2><p><strong>Authors:Zechang Sun, Yuan-Sen Ting, Yaobo Liang, Nan Duan, Song Huang, Zheng Cai</strong></p>
<p>Astronomical research traditionally relies on extensive domain knowledge to interpret observations and narrow down hypotheses. We demonstrate that this process can be emulated using large language model-based agents to accelerate research workflows. We propose mephisto, a multi-agent collaboration framework that mimics human reasoning to interpret multi-band galaxy observations. mephisto interacts with the CIGALE codebase, which includes spectral energy distribution (SED) models to explain observations. In this open-world setting, mephisto learns from its self-play experience, performs tree search, and accumulates knowledge in a dynamically updated base. As a proof of concept, we apply mephisto to the latest data from the James Webb Space Telescope. mephisto attains near-human proficiency in reasoning about galaxies’ physical scenarios, even when dealing with a recently discovered population of “Little Red Dot” galaxies. This represents the first demonstration of agentic research in astronomy, advancing towards end-to-end research via LLM agents and potentially expediting astronomical discoveries. </p>
<blockquote>
<p>传统的天文研究依赖于广泛的领域知识来解释观测结果并缩小假设范围。我们证明，这个过程可以使用基于大型语言模型的代理来加速研究工作流程。我们提出了模仿人类推理来解释多波段星系观测数据的智能体合作框架“墨菲斯托”。墨菲斯托与CIGALE代码库进行交互，该代码库包含光谱能量分布（SED）模型来解释观测结果。在这个开放世界环境中，墨菲斯托通过自我玩耍经验学习，进行树搜索，并在动态更新的基础上积累知识。作为概念验证，我们将墨菲斯托应用于詹姆斯韦伯太空望远镜的最新数据。墨菲斯托在处理最近发现的“小红点”星系群体时，在理解星系物理场景方面达到了近乎人类的熟练程度。这标志着天文领域智能研究的首次展示，朝着通过大型语言模型代理进行端到端研究的方向迈进，并有可能加速天文学发现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.14807v2">PDF</a> Accepted at the NIPS ML4PS Workshop 2024. The journal version is in   preparation. Code and data will be fully made public following the journal   publication. We welcome any comments and feedback</p>
<p><strong>Summary</strong></p>
<p>基于大规模语言模型的代理能够模拟人类推理，加快天文研究流程。我们提出了一个名为mephisto的多代理协作框架，用于解释多波段星系观测结果。通过与包括光谱能量分布（SED）模型在内的CIGALE代码库互动，mephisto可以在开放世界环境中进行自我游戏经验学习，进行树状搜索并在动态更新的知识库中积累知识。在詹姆斯·韦伯太空望远镜的最新数据应用中，mephisto在关于星系物理场景的推理中达到了接近人类的熟练程度，即使在处理最新发现的“小红点”星系群体时也是如此。这标志着代理研究的首次在天文学领域的展示，朝着通过LLM代理进行端到端研究的方向迈进，并可能加速天文学发现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型代理能模拟人类推理以加快天文学研究。</li>
<li>提出名为mephisto的多代理协作框架，用以解释多波段星系观测。</li>
<li>mephisto与CIGALE代码库中的SED模型互动。</li>
<li>mephisto在开放世界环境中进行自我游戏经验学习、树状搜索和知识积累。</li>
<li>在詹姆斯·韦伯太空望远镜数据中，mephisto展现出接近人类的推理能力，特别是在解释星系物理场景方面。</li>
<li>这是首次展示在天文学领域使用代理进行研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.14807">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6425f9e89d79cb0f7af7c2868d3a0448.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-faaa9449101f0aa4b09028fe216e8c56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-869b32fe6a7ff5202c1da3366a1f7b9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13cb0cc4d0ced69b5bb07a8d318a2f48.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ME-IGM-Individual-Global-Max-in-Maximum-Entropy-Multi-Agent-Reinforcement-Learning"><a href="#ME-IGM-Individual-Global-Max-in-Maximum-Entropy-Multi-Agent-Reinforcement-Learning" class="headerlink" title="ME-IGM: Individual-Global-Max in Maximum Entropy Multi-Agent   Reinforcement Learning"></a>ME-IGM: Individual-Global-Max in Maximum Entropy Multi-Agent   Reinforcement Learning</h2><p><strong>Authors:Wen-Tse Chen, Yuxuan Li, Shiyu Huang, Jiayu Chen, Jeff Schneider</strong></p>
<p>Multi-agent credit assignment is a fundamental challenge for cooperative multi-agent reinforcement learning (MARL), where a team of agents learn from shared reward signals. The Individual-Global-Max (IGM) condition is a widely used principle for multi-agent credit assignment, requiring that the joint action determined by individual Q-functions maximizes the global Q-value. Meanwhile, the principle of maximum entropy has been leveraged to enhance exploration in MARL. However, we identify a critical limitation in existing maximum entropy MARL methods: a misalignment arises between local policies and the joint policy that maximizes the global Q-value, leading to violations of the IGM condition. To address this misalignment, we propose an order-preserving transformation. Building on it, we introduce ME-IGM, a novel maximum entropy MARL algorithm compatible with any credit assignment mechanism that satisfies the IGM condition while enjoying the benefits of maximum entropy exploration. We empirically evaluate two variants of ME-IGM: ME-QMIX and ME-QPLEX, in non-monotonic matrix games, and demonstrate their state-of-the-art performance across 17 scenarios in SMAC-v2 and Overcooked. </p>
<blockquote>
<p>多智能体信用分配是合作型多智能体强化学习（MARL）中的一个基本挑战，其中一组智能体从共享奖励信号中学习。个体-全局-最大化（IGM）条件是用于多智能体信用分配的一个广泛应用的原则，要求由个体Q函数决定的联合行动最大化全局Q值。同时，最大熵原则已被用于增强MARL中的探索。然而，我们发现了现有最大熵MARL方法的一个关键局限：局部政策和最大化全局Q值的联合政策之间出现不匹配，导致IGM条件的违反。为了解决这种不匹配，我们提出了一种保序变换。在此基础上，我们引入了ME-IGM，这是一种新型的最大熵MARL算法，与任何满足IGM条件的信用分配机制兼容，同时享受最大熵探索的优势。我们通过非单调矩阵游戏对ME-IGM的两个变体ME-QMIX和ME-QPLEX进行了实证评估，在SMAC-v2和Overcooked的17个场景中展示了它们的最先进性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.13930v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了多智能体强化学习中的信用分配问题，并指出了现有最大熵多智能体强化学习方法中的关键局限性。为了解决这一局限性，本文提出了ME-IGM算法，该算法兼容任何满足IGM条件的信用分配机制，并享受最大熵探索的优势。经过在非单调矩阵游戏中的实证研究，ME-IGM的两个变体ME-QMIX和ME-QPLEX在SMAC-v2和Overcooked的17个场景中展现了卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多智能体强化学习面临的核心挑战之一是信用分配问题，尤其在共享奖励信号的情境中尤为重要。</li>
<li>个体全局最大（IGM）条件广泛应用于多智能体信用分配原则中，它要求由个体Q函数决定的联合行动最大化全局Q值。</li>
<li>最大熵原则已用于增强多智能体强化学习中的探索能力。</li>
<li>然而，现有的最大熵多智能体强化学习方法存在一个关键局限：局部政策与最大化全局Q值的联合政策之间的不匹配导致IGM条件的违反。</li>
<li>为了解决这一不匹配问题，本文提出了一种顺序保留转换方法。</li>
<li>基于此转换方法，引入了ME-IGM算法，该算法与任何满足IGM条件的信用分配机制兼容，并享受最大熵探索的优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.13930">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-778bdeb0341b2c5b4a7a3f1ff8382cf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffba41638f2e8cb75c213513307453f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c49894d235ec3065295651d15529bf36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f97ec04e7a2e286f1a17a4604cb25b05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62c1ac08f122a5d693e2ceda80543bc5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-478183fd9210f6cb966afc43f0e42a53.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-098648614c4853e987444c1488b0fb78.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-08-06  MicroMix Efficient Mixed-Precision Quantization with Microscaling   Formats for Large Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-35e7d6f740610ead318746206df0c675.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-08-06  Raw Data Matters Enhancing Prompt Tuning by Internal Augmentation on   Vision-Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26548.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
