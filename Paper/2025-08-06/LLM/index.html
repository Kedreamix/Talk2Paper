<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  Raw Data Matters Enhancing Prompt Tuning by Internal Augmentation on   Vision-Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-35e7d6f740610ead318746206df0c675.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-06-æ›´æ–°"><a href="#2025-08-06-æ›´æ–°" class="headerlink" title="2025-08-06 æ›´æ–°"></a>2025-08-06 æ›´æ–°</h1><h2 id="Raw-Data-Matters-Enhancing-Prompt-Tuning-by-Internal-Augmentation-on-Vision-Language-Models"><a href="#Raw-Data-Matters-Enhancing-Prompt-Tuning-by-Internal-Augmentation-on-Vision-Language-Models" class="headerlink" title="Raw Data Matters: Enhancing Prompt Tuning by Internal Augmentation on   Vision-Language Models"></a>Raw Data Matters: Enhancing Prompt Tuning by Internal Augmentation on   Vision-Language Models</h2><p><strong>Authors:Haoyang Li, Liang Wang, Chao Wang, Siyu Zhou, Jing Jiang, Yan Peng, Guodong Long</strong></p>
<p>For CLIP-based prompt tuning, introducing more data as additional knowledge for enhancing fine-tuning process is proved to be an effective approach. Existing data amplification strategies for prompt tuning typically rely on external knowledge (e.g., large language models or pre-structured knowledge bases), resulting in higher costs for data collection and processing, while generally ignoring further utilization of features in image modality. To address this, we propose Augmentation-driven Prompt Tuning (AugPT), a self-contained distillation-based prompt tuning approach using only internal augmentation on raw dataset to better exploit known features. Specifically, AugPT employs self-supervised augmentation on unlabeled images in the training set, and introduces a novel gating mechanism based on consensus test, reusing the pre-trained prompt tuning backbone model to spontaneously filter noisy samples, further enhancing the quality of augmented views. Extensive experiments validate that AugPT simultaneously enhances model performance and generalization capability without using appended external knowledge. The code of AugPT is available at: <a target="_blank" rel="noopener" href="https://github.com/JREion/AugPT">https://github.com/JREion/AugPT</a> . </p>
<blockquote>
<p>å¯¹äºåŸºäºCLIPçš„æç¤ºè°ƒæ•´ï¼ˆprompt tuningï¼‰ï¼Œå¼•å…¥æ›´å¤šæ•°æ®ä½œä¸ºé¢å¤–çŸ¥è¯†ä»¥å¢å¼ºå¾®è°ƒè¿‡ç¨‹å·²è¢«è¯æ˜æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚ç°æœ‰çš„æ•°æ®å¢å¼ºç­–ç•¥é€šå¸¸ä¾èµ–äºå¤–éƒ¨çŸ¥è¯†ï¼ˆä¾‹å¦‚å¤§å‹è¯­è¨€æ¨¡å‹æˆ–é¢„ç»“æ„åŒ–çŸ¥è¯†åº“ï¼‰ï¼Œå¯¼è‡´æ•°æ®æ”¶é›†å’Œå¤„ç†æˆæœ¬è¾ƒé«˜ï¼ŒåŒæ—¶å¿½ç•¥äº†è¿›ä¸€æ­¥åˆ©ç”¨å›¾åƒæ¨¡æ€ä¸­çš„ç‰¹å¾ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè‡ªå¢å¼ºçš„æç¤ºè°ƒæ•´ï¼ˆAugPTï¼‰æ–¹æ³•ã€‚è¿™æ˜¯ä¸€ç§åŸºäºè‡ªæˆ‘è’¸é¦çš„æç¤ºè°ƒæ•´æ–¹æ³•ï¼Œä»…ä½¿ç”¨åŸå§‹æ•°æ®é›†çš„å†…éƒ¨å¢å¼ºæ¥æ›´å¥½åœ°åˆ©ç”¨å·²çŸ¥ç‰¹å¾ã€‚å…·ä½“æ¥è¯´ï¼ŒAugPTå¯¹è®­ç»ƒé›†ä¸­çš„æ— æ ‡ç­¾å›¾åƒè¿›è¡Œè‡ªç›‘ç£å¢å¼ºï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åŸºäºå…±è¯†æµ‹è¯•çš„æ–°å‹é—¨æ§æœºåˆ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æç¤ºè°ƒæ•´éª¨å¹²æ¨¡å‹è‡ªåŠ¨è¿‡æ»¤å™ªå£°æ ·æœ¬ï¼Œè¿›ä¸€æ­¥æé«˜å¢å¼ºè§†å›¾çš„å“è´¨ã€‚å¤§é‡å®éªŒéªŒè¯ï¼ŒAugPTåœ¨ä¸ä½¿ç”¨é™„åŠ å¤–éƒ¨çŸ¥è¯†çš„æƒ…å†µä¸‹ï¼ŒåŒæ—¶æé«˜äº†æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚AugPTçš„ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/JREion/AugPT">https://github.com/JREion/AugPT</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02671v1">PDF</a> 16 pages, 6 figures, 15 tables</p>
<p><strong>Summary</strong><br>æ–‡æœ¬æå‡ºäº†ä¸€ç§åŸºäºCLIPçš„æ–°æ–¹æ³•AugPTï¼Œè¯¥æ–¹æ³•ä½¿ç”¨ä»…å†…éƒ¨å¢å¼ºçš„åŸå§‹æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œåˆ©ç”¨æœªæ ‡è®°å›¾åƒä¸­çš„è‡ªç›‘ç£å¢å¼ºå’ŒåŸºäºå…±è¯†æµ‹è¯•çš„æ–°é—¨æ§æœºåˆ¶ï¼Œä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæ— éœ€ä½¿ç”¨é¢å¤–çš„å¤–éƒ¨çŸ¥è¯†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPèƒŒæ™¯ä¸‹çš„æç¤ºè°ƒæ•´æ–¹æ³•ä¸­ï¼Œå¼•å…¥æ›´å¤šæ•°æ®ä½œä¸ºé¢å¤–çš„çŸ¥è¯†ä»¥æ”¹è¿›å¾®è°ƒè¿‡ç¨‹æ˜¯ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ç°æœ‰çš„æ•°æ®å¢å¼ºç­–ç•¥é€šå¸¸ä¾èµ–äºå¤–éƒ¨çŸ¥è¯†ï¼ˆå¦‚å¤§å‹è¯­è¨€æ¨¡å‹æˆ–é¢„ç»“æ„åŒ–çŸ¥è¯†åº“ï¼‰ï¼Œå¯¼è‡´æ•°æ®é‡‡é›†å’Œå¤„ç†æˆæœ¬è¾ƒé«˜ï¼ŒåŒæ—¶å¿½ç•¥äº†å›¾åƒæ¨¡æ€ç‰¹å¾çš„è¿›ä¸€æ­¥åˆ©ç”¨ã€‚</li>
<li>AugPTæ˜¯ä¸€ç§åŸºäºè’¸é¦çš„æç¤ºè°ƒæ•´æ–¹æ³•ï¼Œä»…ä½¿ç”¨åŸå§‹æ•°æ®é›†çš„å†…éƒ¨å¢å¼ºæ¥æ›´å¥½åœ°åˆ©ç”¨å·²çŸ¥ç‰¹å¾ã€‚</li>
<li>AugPTåœ¨è®­ç»ƒé›†çš„éæ ‡è®°å›¾åƒä¸Šé‡‡ç”¨è‡ªç›‘ç£å¢å¼ºã€‚</li>
<li>AugPTå¼•å…¥äº†ä¸€ç§åŸºäºå…±è¯†æµ‹è¯•çš„æ–°é—¨æ§æœºåˆ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æç¤ºè°ƒæ•´éª¨å¹²æ¨¡å‹è‡ªåŠ¨è¿‡æ»¤å™ªå£°æ ·æœ¬ï¼Œè¿›ä¸€æ­¥æé«˜å¢å¼ºè§†å›¾çš„å“è´¨ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒAugPTåœ¨ä¸ä½¿ç”¨å¤–éƒ¨çŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œèƒ½åŒæ—¶æé«˜æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02671">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4cf3672b6047ab540ed9a8895ea6c020.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f67b6f477ff7bf8e6ac6344b4d7ca056.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7de9c10e1cd13012af8a0a4f10033a96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8dc81ff4b618d908ae79f8c29a49b89.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5c96ce70b1b4508fa5a5a93ec3b2653.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03249446815a0882d28c90258060d702.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LOST-Low-rank-and-Sparse-Pre-training-for-Large-Language-Models"><a href="#LOST-Low-rank-and-Sparse-Pre-training-for-Large-Language-Models" class="headerlink" title="LOST: Low-rank and Sparse Pre-training for Large Language Models"></a>LOST: Low-rank and Sparse Pre-training for Large Language Models</h2><p><strong>Authors:Jiaxi Li, Lu Yin, Li Shen, Jinjin Xu, Liwu Xu, Tianjin Huang, Wenwu Wang, Shiwei Liu, Xilu Wang</strong></p>
<p>While large language models (LLMs) have achieved remarkable performance across a wide range of tasks, their massive scale incurs prohibitive computational and memory costs for pre-training from scratch. Recent studies have investigated the use of low-rank parameterization as a means of reducing model size and training cost. In this context, sparsity is often employed as a complementary technique to recover important information lost in low-rank compression by capturing salient features in the residual space. However, existing approaches typically combine low-rank and sparse components in a simplistic or ad hoc manner, often resulting in undesirable performance degradation compared to full-rank training. In this paper, we propose \textbf{LO}w-rank and \textbf{S}parse pre-\textbf{T}raining (\textbf{LOST}) for LLMs, a novel method that ingeniously integrates low-rank and sparse structures to enable effective training of LLMs from scratch under strict efficiency constraints. LOST applies singular value decomposition to weight matrices, preserving the dominant low-rank components, while allocating the remaining singular values to construct channel-wise sparse components to complement the expressiveness of low-rank training. We evaluate LOST on LLM pretraining ranging from 60M to 7B parameters. Our experiments show that LOST achieves competitive or superior performance compared to full-rank models, while significantly reducing both memory and compute overhead. Moreover, Code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models%7D%7BLOST">https://github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models}{LOST</a> Repo} </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬çš„å¤§è§„æ¨¡å¯¼è‡´äº†ä»å¤´å¼€å§‹é¢„è®­ç»ƒçš„è®¡ç®—å’Œå†…å­˜æˆæœ¬é«˜æ˜‚ã€‚æœ€è¿‘çš„ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨ä½ç§©å‚æ•°åŒ–ä½œä¸ºå‡å°æ¨¡å‹å¤§å°å’Œé™ä½è®­ç»ƒæˆæœ¬çš„æ–¹æ³•ã€‚åœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­ï¼Œç¨€ç–æ€§é€šå¸¸è¢«ç”¨ä½œä¸€ç§è¾…åŠ©æŠ€æœ¯ï¼Œä»¥æ¢å¤åœ¨ä½ç§©å‹ç¼©ä¸­ä¸¢å¤±çš„é‡è¦ä¿¡æ¯ï¼Œé€šè¿‡æ•è·æ®‹å·®ç©ºé—´ä¸­çš„æ˜¾è‘—ç‰¹å¾æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•é€šå¸¸ä»¥ç®€å•æˆ–å³å…´çš„æ–¹å¼å°†ä½ç§©å’Œç¨€ç–ç»„ä»¶ç»„åˆåœ¨ä¸€èµ·ï¼Œä¸å…¨ç§©è®­ç»ƒç›¸æ¯”ï¼Œé€šå¸¸ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸ºLLMæå‡ºäº†<strong>LOW</strong>ç§©å’Œ<strong>ç¨€ç–</strong>é¢„<strong>è®­ç»ƒ</strong>ï¼ˆ<strong>LOST</strong>ï¼‰çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å·§å¦™åœ°ç»“åˆäº†ä½ç§©å’Œç¨€ç–ç»“æ„ï¼Œèƒ½å¤Ÿåœ¨ä¸¥æ ¼çš„æ•ˆç‡çº¦æŸä¸‹æœ‰æ•ˆåœ°ä»å¤´å¼€å§‹è®­ç»ƒLLMã€‚LOSTé€šè¿‡å¥‡å¼‚å€¼åˆ†è§£åº”ç”¨äºæƒé‡çŸ©é˜µï¼Œä¿ç•™ä¸»è¦ä½ç§©æˆåˆ†ï¼ŒåŒæ—¶å°†å‰©ä½™çš„å¥‡å¼‚å€¼åˆ†é…ç»™é€šé“ç¨€ç–æˆåˆ†ï¼Œä»¥è¡¥å……ä½ç§©è®­ç»ƒçš„è¡¨è¾¾æ€§ã€‚æˆ‘ä»¬å¯¹å‚æ•°èŒƒå›´ä»60Måˆ°7Bçš„LLMé¢„è®­ç»ƒè¿›è¡Œäº†LOSTè¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼ŒLOSTåœ¨æ€§èƒ½æ–¹é¢è¾¾åˆ°äº†ä¸å…¨ç§©æ¨¡å‹ç›¸å½“æˆ–æ›´ä¼˜çš„æ°´å¹³ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†å†…å­˜å’Œè®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models">LOSTä»“åº“</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02668v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¹¿æ³›çš„ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†å…¶åºå¤§çš„è§„æ¨¡å¸¦æ¥äº†é«˜æ˜‚çš„è®¡ç®—å’Œå†…å­˜æˆæœ¬ã€‚è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨ä½ç§©å‚æ•°åŒ–æ–¹æ³•å¯ä»¥å‡å°‘æ¨¡å‹å¤§å°å’Œè®­ç»ƒæˆæœ¬ã€‚ç„¶è€Œï¼Œç®€å•ç»“åˆä½ç§©å’Œç¨€ç–æˆåˆ†ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„ä½ç§©å’Œç¨€ç–é¢„è®­ç»ƒæ–¹æ³•ï¼ˆLOSTï¼‰ï¼Œè¯¥æ–¹æ³•å·§å¦™åœ°å°†ä½ç§©å’Œç¨€ç–ç»“æ„ç›¸ç»“åˆï¼Œåœ¨ä¸¥æ ¼çš„æ•ˆç‡çº¦æŸä¸‹å®ç°äº†LLMçš„æœ‰æ•ˆè®­ç»ƒã€‚é€šè¿‡å¯¹æƒé‡çŸ©é˜µè¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£ï¼ŒLOSTä¿ç•™äº†ä¸»è¦ä½ç§©æˆåˆ†ï¼Œå¹¶å°†å‰©ä½™å¥‡å¼‚å€¼åˆ†é…ç»™é€šé“ç¨€ç–æˆåˆ†ï¼Œä»¥è¡¥å……ä½ç§©è®­ç»ƒçš„è¡¨è¾¾èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒLOSTåœ¨LLMé¢„è®­ç»ƒä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½äº†å†…å­˜å’Œè®¡ç®—æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œå†…å­˜éœ€æ±‚çš„é—®é¢˜ã€‚</li>
<li>ä½ç§©å‚æ•°åŒ–æ–¹æ³•è¢«ç ”ç©¶ç”¨äºå‡å°‘æ¨¡å‹å¤§å°å’Œè®­ç»ƒæˆæœ¬ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç®€å•ç»“åˆä½ç§©å’Œç¨€ç–æˆåˆ†ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>æå‡ºçš„LOW-rankå’ŒSparse pre-Trainingï¼ˆLOSTï¼‰æ–¹æ³•ç»“åˆäº†ä½ç§©å’Œç¨€ç–ç»“æ„ã€‚</li>
<li>LOSTé€šè¿‡å¥‡å¼‚å€¼åˆ†è§£ä¿ç•™ä¸»è¦ä½ç§©æˆåˆ†ï¼Œå¹¶åˆ†é…å‰©ä½™å¥‡å¼‚å€¼å½¢æˆé€šé“ç¨€ç–æˆåˆ†ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒLOSTåœ¨LLMé¢„è®­ç»ƒä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”æ˜¾è‘—é™ä½å†…å­˜å’Œè®¡ç®—æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02668">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-61c7b2b355a28bc582daeb0ea18bdf63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5268f128a958213ce6ff05a8cf9fcc99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-beb261ac57fee753dcf46f102c69d54c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a7d7f79733c99f4d5a883f43766a908.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb75c898537360ab80d48b28e61027a6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Evaluating-Variance-in-Visual-Question-Answering-Benchmarks"><a href="#Evaluating-Variance-in-Visual-Question-Answering-Benchmarks" class="headerlink" title="Evaluating Variance in Visual Question Answering Benchmarks"></a>Evaluating Variance in Visual Question Answering Benchmarks</h2><p><strong>Authors:Nikitha SR</strong></p>
<p>Multimodal large language models (MLLMs) have emerged as powerful tools for visual question answering (VQA), enabling reasoning and contextual understanding across visual and textual modalities. Despite their advancements, the evaluation of MLLMs on VQA benchmarks often relies on point estimates, overlooking the significant variance in performance caused by factors such as stochastic model outputs, training seed sensitivity, and hyperparameter configurations. This paper critically examines these issues by analyzing variance across 14 widely used VQA benchmarks, covering diverse tasks such as visual reasoning, text understanding, and commonsense reasoning. We systematically study the impact of training seed, framework non-determinism, model scale, and extended instruction finetuning on performance variability. Additionally, we explore Cloze-style evaluation as an alternate assessment strategy, studying its effectiveness in reducing stochasticity and improving reliability across benchmarks. Our findings highlight the limitations of current evaluation practices and advocate for variance-aware methodologies to foster more robust and reliable development of MLLMs. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä½œä¸ºè§†è§‰é—®ç­”ï¼ˆVQAï¼‰çš„å¼ºå¤§å·¥å…·å·²ç»å‡ºç°ï¼Œèƒ½å¤Ÿåœ¨è§†è§‰å’Œæ–‡æœ¬æ¨¡å¼ä¹‹é—´è¿›è¡Œæ¨ç†å’Œä¸Šä¸‹æ–‡ç†è§£ã€‚å°½ç®¡å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨VQAåŸºå‡†æµ‹è¯•ä¸­å¯¹MLLMsçš„è¯„ä¼°å¾€å¾€ä¾èµ–äºç‚¹ä¼°è®¡ï¼Œä»è€Œå¿½è§†äº†ç”±äºéšæœºæ¨¡å‹è¾“å‡ºã€è®­ç»ƒç§å­æ•æ„Ÿæ€§å’Œè¶…å‚æ•°é…ç½®ç­‰å› ç´ å¯¼è‡´çš„æ€§èƒ½æ˜¾è‘—å·®å¼‚ã€‚æœ¬æ–‡é€šè¿‡åˆ†æ14ä¸ªå¹¿æ³›ä½¿ç”¨çš„VQAåŸºå‡†æµ‹è¯•çš„æ–¹å·®ï¼Œå¯¹è¿™äº›é—®é¢˜è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œæ¶µç›–äº†è§†è§‰æ¨ç†ã€æ–‡æœ¬ç†è§£å’Œå¸¸è¯†æ¨ç†ç­‰å¤šæ ·åŒ–ä»»åŠ¡ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†è®­ç»ƒç§å­ã€æ¡†æ¶éç¡®å®šæ€§ã€æ¨¡å‹è§„æ¨¡ä»¥åŠæ‰©å±•æŒ‡ä»¤å¾®è°ƒå¯¹æ€§èƒ½å˜åŒ–çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†ä»¥å¡«ç©ºå¼è¯„ä¼°ä½œä¸ºæ›¿ä»£è¯„ä¼°ç­–ç•¥ï¼Œç ”ç©¶äº†å…¶åœ¨å‡å°‘éšæœºæ€§å’Œæé«˜è·¨åŸºå‡†æµ‹è¯•å¯é æ€§æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜äº†å½“å‰è¯„ä¼°æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶æå€¡é‡‡ç”¨è€ƒè™‘å˜å¼‚æ€§çš„æ–¹æ³•æ¥ä¿ƒè¿›MLLMsæ›´ç¨³å¥å’Œå¯é çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02645v1">PDF</a> Accepted in ICCV 2025 Workshop on Whatâ€™s Next in Multimodal   Foundational Models</p>
<p><strong>Summary</strong></p>
<p>MLLMsåœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ–¹é¢çš„åº”ç”¨å±•ç°å‡ºå¼ºå¤§çš„è·¨è§†è§‰å’Œæ–‡æœ¬æ¨¡æ€çš„æ¨ç†å’Œä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–ç‚¹ä¼°è®¡ï¼Œå¿½ç•¥äº†æ¨¡å‹è¾“å‡ºéšæœºæ€§ã€è®­ç»ƒç§å­æ•æ„Ÿæ€§å’Œè¶…å‚æ•°é…ç½®ç­‰å› ç´ å¯¼è‡´çš„æ€§èƒ½æ˜¾è‘—å˜åŒ–ã€‚æœ¬æ–‡é€šè¿‡åˆ†æåœ¨14ä¸ªå¹¿æ³›ä½¿ç”¨çš„VQAåŸºå‡†æµ‹è¯•ä¸Šçš„æ–¹å·®ï¼Œç³»ç»Ÿç ”ç©¶äº†è®­ç»ƒç§å­ã€æ¡†æ¶éç¡®å®šæ€§ã€æ¨¡å‹è§„æ¨¡å’Œæ‰©å±•æŒ‡ä»¤å¾®è°ƒå¯¹æ€§èƒ½å˜åŒ–çš„å½±å“ï¼Œå¹¶æ¢è®¨äº†é—­å¼è¯„ä¼°ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶å‘ç°å½“å‰è¯„ä¼°æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶ä¸»å¼ é‡‡ç”¨è€ƒè™‘æ–¹å·®çš„è¯„ä¼°æ–¹æ³•æ¥ä¿ƒè¿›æ›´ç¨³å¥å’Œå¯é çš„LLMå‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä¸­å±•ç°å‡ºå¼ºå¤§çš„è·¨æ¨¡æ€èƒ½åŠ›ã€‚</li>
<li>å½“å‰è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾èµ–ç‚¹ä¼°è®¡ï¼Œå¿½ç•¥äº†æ¨¡å‹æ€§èƒ½çš„é‡å¤§å˜åŒ–ã€‚</li>
<li>è®­ç»ƒç§å­ã€æ¡†æ¶éç¡®å®šæ€§ã€æ¨¡å‹è§„æ¨¡å’Œå¾®è°ƒå½±å“æ€§èƒ½å˜åŒ–ã€‚</li>
<li>åŸºå‡†æµ‹è¯•ä¸Šçš„æ–¹å·®åˆ†ææ­ç¤ºäº†è¯„ä¼°æ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>è€ƒè™‘æ–¹å·®çš„è¯„ä¼°æ–¹æ³•å¯¹äºä¿ƒè¿›ç¨³å¥å’Œå¯é çš„LLMå‘å±•è‡³å…³é‡è¦ã€‚</li>
<li>æå‡ºä½¿ç”¨é—­å¼è¯„ä¼°ç­–ç•¥ä½œä¸ºæ›¿ä»£è¯„ä¼°æ–¹æ³•ï¼Œä»¥å‡å°‘éšæœºæ€§å¹¶æ”¹å–„åŸºå‡†æµ‹è¯•çš„å¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02645">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-45ad12c5b2a8a7fed798c9c66956da7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd505d10203ce5a9727527db701f509d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d86946452ce5cdc3b03aee1b0fef07e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52b0a34e710e53048f9ff7524f73f585.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4eae2e19842bf1d3141d677c3585921.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f588e7571a2eaefbf8018be15b1a36d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47bd7e11e24bb8428753eb2fb2c9655b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d39b0846295ddb7ecff57441c2ae14db.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Test-Set-Quality-in-Multilingual-LLM-Evaluation"><a href="#Test-Set-Quality-in-Multilingual-LLM-Evaluation" class="headerlink" title="Test Set Quality in Multilingual LLM Evaluation"></a>Test Set Quality in Multilingual LLM Evaluation</h2><p><strong>Authors:Kranti Chalamalasetti, Gabriel Bernier-Colborne, Yvan Gauthier, Sowmya Vajjala</strong></p>
<p>Several multilingual benchmark datasets have been developed in a semi-automatic manner in the recent past to measure progress and understand the state-of-the-art in the multilingual capabilities of Large Language Models. However, there is not a lot of attention paid to the quality of the datasets themselves, despite the existence of previous work in identifying errors in even fully human-annotated test sets. In this paper, we manually analyze recent multilingual evaluation sets in two languages - French and Telugu, identifying several errors in the process. We compare the performance difference across several LLMs with the original and revised versions of the datasets and identify large differences (almost 10% in some cases) in both languages). Based on these results, we argue that test sets should not be considered immutable and should be revisited, checked for correctness, and potentially versioned. We end with some recommendations for both the dataset creators as well as consumers on addressing the dataset quality issues. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œä¸ºäº†è¡¡é‡å’Œäº†è§£å¤šè¯­è¨€å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•å’ŒçŠ¶æ€ï¼Œå·²ç»ä»¥åŠè‡ªåŠ¨çš„æ–¹å¼å¼€å‘äº†å‡ ä¸ªå¤šè¯­è¨€åŸºå‡†æ•°æ®é›†ã€‚ç„¶è€Œï¼Œå°½ç®¡æœ‰å…ˆå‰çš„å·¥ä½œå·²ç»è¯†åˆ«å‡ºå³ä½¿æ˜¯å®Œå…¨äººå·¥æ³¨é‡Šçš„æµ‹è¯•é›†ä¸­ä¹Ÿå­˜åœ¨é”™è¯¯ï¼Œä½†äººä»¬å¯¹æ•°æ®é›†æœ¬èº«çš„å“è´¨å¹¶æ²¡æœ‰ç»™äºˆè¶³å¤Ÿçš„é‡è§†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ‰‹åŠ¨åˆ†æäº†æœ€è¿‘çš„ä¸¤é—¨è¯­è¨€â€”â€”æ³•è¯­å’Œæ³°å¢å›ºè¯­çš„å¤šè¯­è¨€è¯„ä¼°é›†ï¼Œå¹¶åœ¨è¿‡ç¨‹ä¸­å‘ç°äº†å¤šä¸ªé”™è¯¯ã€‚æˆ‘ä»¬å°†å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŸå§‹å’Œä¿®è®¢åçš„æ•°æ®é›†ä¸Šçš„æ€§èƒ½å·®å¼‚è¿›è¡Œäº†æ¯”è¾ƒï¼Œå‘ç°åœ¨ä¸¤ç§è¯­è¨€ä¸­éƒ½å­˜åœ¨å·¨å¤§å·®å¼‚ï¼ˆåœ¨æŸäº›æƒ…å†µä¸‹å‡ ä¹è¾¾åˆ°10%ï¼‰ã€‚åŸºäºè¿™äº›ç»“æœï¼Œæˆ‘ä»¬è®¤ä¸ºæµ‹è¯•é›†ä¸åº”è¢«è§†ä¸ºæ°¸æ’ä¸å˜çš„ï¼Œè€Œåº”è¯¥è¿›è¡Œå¤æŸ¥ä»¥æ£€æŸ¥å…¶å‡†ç¡®æ€§å¹¶å¯èƒ½å¯¹å…¶è¿›è¡Œç‰ˆæœ¬æ§åˆ¶ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹æ•°æ®é›†åˆ›å»ºè€…å’Œä½¿ç”¨è€…æå‡ºä¸€äº›è§£å†³æ•°æ®é›†è´¨é‡é—®é¢˜çš„å»ºè®®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02635v1">PDF</a> Accepted at the 1st Workshop on Multilingual Data Quality Signals,   COLM 2025, Short paper. 10 pages in total</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¼€å‘çš„å¤šè¯­ç§åŸºå‡†æ•°æ®é›†ä»¥åŠè‡ªåŠ¨æ–¹å¼è¡¡é‡å¤§è¯­è¨€æ¨¡å‹çš„è¿›æ­¥å¹¶äº†è§£å…¶æœ€æ–°æŠ€æœ¯çŠ¶æ€ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æµ‹è¯•é›†å³ä½¿ç”±å®Œå…¨äººå·¥æ³¨é‡Šï¼Œä¹Ÿä»ç„¶å­˜åœ¨é”™è¯¯ã€‚æœ¬æ–‡é€šè¿‡æ‰‹åŠ¨åˆ†æä¸¤ç§è¯­è¨€çš„æœ€è¿‘å¤šè¯­ç§è¯„ä¼°é›†ï¼ˆæ³•è¯­å’Œæ³°å¢å›ºè¯­ï¼‰ï¼Œå‘ç°äº†æ•°æ®é›†ä¸­çš„ä¸€äº›é”™è¯¯ã€‚åœ¨å¯¹æ¯”äº†åŸå§‹æ•°æ®é›†ä¸ä¿®è®¢æ•°æ®é›†çš„ä¸åŒæ€§èƒ½å·®å¼‚åï¼Œæœ¬æ–‡å¼ºè°ƒäº†æµ‹è¯•é›†å¹¶éä¸€æˆä¸å˜ï¼Œåº”è¯¥å¯¹å…¶è¿›è¡Œé‡æ–°è¯„ä¼°ï¼Œå¹¶é‡è§†è´¨é‡é—®é¢˜çš„è§£å†³ï¼Œå¯¹æ­¤æœ¬æ–‡æä¾›äº†å¯¹äºæ•°æ®é›†çš„åˆ›å»ºè€…å’Œæ¶ˆè´¹è€…ä»¬çš„è§£å†³å»ºè®®ã€‚åŒæ—¶ä¹Ÿè¯´æ˜äº†éœ€è¦è¿›è¡Œè´¨é‡æ§åˆ¶å¹¶å¯èƒ½éœ€è¦ä½¿ç”¨ä¸åŒç‰ˆæœ¬çš„ç®¡ç†åŠæ³•ï¼Œä»¥ä¿éšœæµ‹è¯•é›†çš„å¯é æ€§ã€‚è¿™ä¸ºLLMæ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°æä¾›äº†ä¸€ä¸ªå…¨æ–°çš„è§†è§’å’Œæ·±å…¥çš„æ€è€ƒã€‚é’ˆå¯¹ç°æœ‰çš„è¯„æµ‹æ•°æ®è´¨é‡é—®é¢˜è¿›è¡Œäº†æœ‰åŠ›çš„åé©³ä¸æ·±å…¥æ¢è®¨ï¼Œå¹¶ä¸”ä»ä¸¤ä¸ªå…·ä½“çš„å®ä¾‹å‡ºå‘è¿›è¡Œäº†å®è¯é˜è¿°ã€‚è¿™æ—¢æœ‰åŠ©äºç ”ç©¶è€…çš„å‚è€ƒå’Œå€Ÿé‰´ï¼Œä¹Ÿä¿ƒè¿›äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸç ”ç©¶çš„æ·±å…¥å‘å±•ã€‚ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„ç§‘ç ”æä¾›äº†æœ‰ä»·å€¼çš„å¯ç¤ºå’Œè§£å†³æ–¹æ¡ˆã€‚åŒæ—¶ï¼Œä¹Ÿæé†’äº†äººä»¬è¦é‡è§†æ•°æ®è´¨é‡çš„é—®é¢˜ï¼Œç¡®ä¿æ•°æ®é›†çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚å¯¹äºæœªæ¥çš„ç ”ç©¶è€Œè¨€ï¼Œæœ¬æ–‡å…·æœ‰é‡è¦çš„å‚è€ƒä»·å€¼å’Œå®è·µæŒ‡å¯¼æ„ä¹‰ã€‚åŒæ—¶å¼ºè°ƒäº†æ•°æ®é›†è´¨é‡çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºäº†æ”¹è¿›å»ºè®®ã€‚å¯¹äºæœªæ¥å¤šè¯­è¨€æ¨¡å‹çš„å‘å±•å’Œåº”ç”¨å…·æœ‰ç§¯æçš„æ¨åŠ¨ä½œç”¨ã€‚åŒæ—¶ä¹ŸæŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„å¯èƒ½æ–¹å‘å’ŒæŒ‘æˆ˜ã€‚æœ¬æ–‡å¼ºè°ƒäº†æ•°æ®é›†è´¨é‡çš„é‡è¦æ€§ä»¥åŠå…¶å¯¹è¯¥é¢†åŸŸæœªæ¥ç ”ç©¶çš„æ½œåœ¨å½±å“ã€‚<strong>Key Takeaways</strong></p>
<ul>
<li>å¤šè¯­ç§åŸºå‡†æ•°æ®é›†ç”¨ä»¥è¡¡é‡å¤§è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œè¿™äº›æ•°æ®é›†çš„è´¨é‡é—®é¢˜è¢«å¿½è§†ã€‚</li>
<li>æ‰‹åŠ¨åˆ†æå‘ç°å¤šè¯­ç§è¯„ä¼°é›†å­˜åœ¨é”™è¯¯ã€‚</li>
<li>æµ‹è¯•é›†å¹¶éä¸€æˆä¸å˜ï¼Œéœ€è¦å®šæœŸæ£€æŸ¥å’Œä¿®æ­£é”™è¯¯ã€‚</li>
<li>æµ‹è¯•é›†çš„è´¨é‡é—®é¢˜å¯¹LLMæ€§èƒ½è¯„ä¼°äº§ç”Ÿæ˜¾è‘—å½±å“ï¼ˆåœ¨æŸäº›æƒ…å†µä¸‹å·®å¼‚è¾¾10%ï¼‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5ccf5bcf730699f90b2c50592be93455.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21e8c06b0b0232d0a0f236aabbca0a8f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1926848d0ef6bfefeb88d49b9773ab01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bbc192ad23141f07887637b6bae23b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a99bb5a07370ae06dee8fb83547411d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a2a5fb27037f9419ea8018d3786c474.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="HyCodePolicy-Hybrid-Language-Controllers-for-Multimodal-Monitoring-and-Decision-in-Embodied-Agents"><a href="#HyCodePolicy-Hybrid-Language-Controllers-for-Multimodal-Monitoring-and-Decision-in-Embodied-Agents" class="headerlink" title="HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and   Decision in Embodied Agents"></a>HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and   Decision in Embodied Agents</h2><p><strong>Authors:Yibin Liu, Zhixuan Liang, Zanxin Chen, Tianxing Chen, Mengkang Hu, Wanxi Dong, Congsheng Xu, Zhaoming Han, Yusen Qin, Yao Mu</strong></p>
<p>Recent advances in multimodal large language models (MLLMs) have enabled richer perceptual grounding for code policy generation in embodied agents. However, most existing systems lack effective mechanisms to adaptively monitor policy execution and repair codes during task completion. In this work, we introduce HyCodePolicy, a hybrid language-based control framework that systematically integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair into a closed-loop programming cycle for embodied agents. Technically, given a natural language instruction, our system first decomposes it into subgoals and generates an initial executable program grounded in object-centric geometric primitives. The program is then executed in simulation, while a vision-language model (VLM) observes selected checkpoints to detect and localize execution failures and infer failure reasons. By fusing structured execution traces capturing program-level events with VLM-based perceptual feedback, HyCodePolicy infers failure causes and repairs programs. This hybrid dual feedback mechanism enables self-correcting program synthesis with minimal human supervision. Our results demonstrate that HyCodePolicy significantly improves the robustness and sample efficiency of robot manipulation policies, offering a scalable strategy for integrating multimodal reasoning into autonomous decision-making pipelines. </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥ä¸ºå®ä½“ä»£ç†ä¸­çš„ä»£ç ç­–ç•¥ç”Ÿæˆæä¾›äº†æ›´ä¸°å¯Œçš„æ„ŸçŸ¥åŸºç¡€ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰ç³»ç»Ÿç¼ºä¹æœ‰æ•ˆçš„æœºåˆ¶æ¥åœ¨ä»»åŠ¡å®Œæˆè¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°ç›‘è§†ç­–ç•¥æ‰§è¡Œå’Œä¿®å¤ä»£ç ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†HyCodePolicyï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ··åˆè¯­è¨€çš„æ§åˆ¶æ¡†æ¶ï¼Œå®ƒå°†ä»£ç åˆæˆã€å‡ ä½•åŸºç¡€ã€æ„ŸçŸ¥ç›‘æ§å’Œè¿­ä»£ä¿®å¤ç³»ç»Ÿåœ°é›†æˆåˆ°ä¸€ä¸ªé—­ç¯ç¼–ç¨‹å‘¨æœŸä¸­ï¼Œç”¨äºå®ä½“ä»£ç†ã€‚ä»æŠ€æœ¯ä¸Šè®²ï¼Œç»™å®šè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿé¦–å…ˆå°†å…¶åˆ†è§£ä¸ºå­ç›®æ ‡å¹¶ç”ŸæˆåŸºäºå¯¹è±¡ä¸ºä¸­å¿ƒçš„å‡ ä½•åŸå§‹æ•°æ®çš„åˆå§‹å¯æ‰§è¡Œç¨‹åºã€‚è¯¥ç¨‹åºç„¶ååœ¨ä»¿çœŸä¸­æ‰§è¡Œï¼ŒåŒæ—¶è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è§‚å¯Ÿé€‰å®šæ£€æŸ¥ç‚¹ä»¥æ£€æµ‹å’Œå®šä½æ‰§è¡Œå¤±è´¥å¹¶æ¨æ–­å¤±è´¥åŸå› ã€‚é€šè¿‡å°†æ•è·ç¨‹åºçº§äº‹ä»¶çš„ç»“æ„åŒ–æ‰§è¡Œè½¨è¿¹ä¸åŸºäºVLMçš„æ„ŸçŸ¥åé¦ˆç›¸èåˆï¼ŒHyCodePolicyæ¨æ–­å‡ºå¤±è´¥åŸå› å¹¶ä¿®å¤ç¨‹åºã€‚è¿™ç§æ··åˆåŒåé¦ˆæœºåˆ¶å®ç°äº†è‡ªæˆ‘ä¿®æ­£çš„ç¨‹åºåˆæˆï¼Œå‡ ä¹æ— éœ€äººå·¥ç›‘ç£ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒHyCodePolicyæ˜¾è‘—æé«˜äº†æœºå™¨äººæ“ä½œç­–ç•¥çš„ç¨³å¥æ€§å’Œæ ·æœ¬æ•ˆç‡ï¼Œä¸ºå°†å¤šæ¨¡æ€æ¨ç†é›†æˆåˆ°è‡ªä¸»å†³ç­–ç®¡é“ä¸­æä¾›äº†å¯æ‰©å±•çš„ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02629v1">PDF</a> Accepted to ICCV 2025 Workshop on Multi-Modal Reasoning for Agentic   Intelligence</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è¿›æ­¥ä¸ºå®ä½“ä»£ç†ä¸­çš„ä»£ç ç­–ç•¥ç”Ÿæˆæä¾›äº†æ›´ä¸°å¯Œçš„æ„ŸçŸ¥åŸºç¡€ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰ç³»ç»Ÿç¼ºä¹åœ¨ä»»åŠ¡å®Œæˆè¿‡ç¨‹ä¸­è‡ªé€‚åº”ç›‘æ§ç­–ç•¥æ‰§è¡Œå’Œä¿®å¤ä»£ç çš„æœ‰æ•ˆæœºåˆ¶ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†HyCodePolicyï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè¯­è¨€çš„æ§åˆ¶æ¡†æ¶ï¼Œç³»ç»Ÿåœ°é›†æˆäº†ä»£ç åˆæˆã€å‡ ä½•åŸºç¡€ã€æ„ŸçŸ¥ç›‘æ§å’Œè¿­ä»£ä¿®å¤ï¼Œä¸ºå®ä½“ä»£ç†å½¢æˆäº†ä¸€ä¸ªé—­ç¯ç¼–ç¨‹å‘¨æœŸã€‚ç»™å®šè‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œè¯¥ç³»ç»Ÿé¦–å…ˆå°†å…¶åˆ†è§£ä¸ºå­ç›®æ ‡å¹¶ç”ŸæˆåŸºäºå¯¹è±¡ä¸ºä¸­å¿ƒçš„å‡ ä½•åŸå§‹æ•°æ®çš„åˆå§‹å¯æ‰§è¡Œç¨‹åºã€‚ç„¶ååœ¨æ¨¡æ‹Ÿä¸­æ‰§è¡Œç¨‹åºï¼ŒåŒæ—¶è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è§‚å¯Ÿé€‰å®šæ£€æŸ¥ç‚¹ä»¥æ£€æµ‹å’Œå®šä½æ‰§è¡Œæ•…éšœå¹¶æ¨æ–­å¤±è´¥åŸå› ã€‚é€šè¿‡èåˆæ•è·ç¨‹åºçº§äº‹ä»¶çš„ç»“æ„åŒ–æ‰§è¡Œè½¨è¿¹å’ŒåŸºäºVLMçš„æ„ŸçŸ¥åé¦ˆï¼ŒHyCodePolicyå¯ä»¥æ¨æ–­æ•…éšœåŸå› å¹¶ä¿®å¤ç¨‹åºã€‚è¿™ç§æ··åˆåŒé‡åé¦ˆæœºåˆ¶å®ç°äº†åœ¨æå°‘äººç±»ç›‘ç£ä¸‹çš„è‡ªæˆ‘ä¿®æ­£ç¨‹åºåˆæˆã€‚ç»“æœè¡¨æ˜ï¼ŒHyCodePolicyæ˜¾è‘—æé«˜äº†æœºå™¨äººæ“ä½œç­–ç•¥çš„ç¨³å¥æ€§å’Œæ ·æœ¬æ•ˆç‡ï¼Œä¸ºè‡ªä¸»å†³ç­–ç®¡é“ä¸­æ•´åˆå¤šæ¨¡æ€æ¨ç†æä¾›äº†å¯ä¼¸ç¼©ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ä¸ºå®ä½“ä»£ç†ä¸­çš„ä»£ç ç­–ç•¥ç”Ÿæˆæä¾›äº†ä¸°å¯Œçš„æ„ŸçŸ¥åŸºç¡€ã€‚</li>
<li>ç°æœ‰ç³»ç»Ÿåœ¨ç›‘æ§å’Œä¿®å¤ä»£ç æ–¹é¢çš„è‡ªé€‚åº”èƒ½åŠ›æœ‰é™ã€‚</li>
<li>HyCodePolicyæ˜¯ä¸€ä¸ªåŸºäºè¯­è¨€çš„æ§åˆ¶æ¡†æ¶ï¼Œé›†æˆäº†ä»£ç åˆæˆã€å‡ ä½•åŸºç¡€ã€æ„ŸçŸ¥ç›‘æ§å’Œè¿­ä»£ä¿®å¤ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡åˆ†è§£è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆåˆå§‹å¯æ‰§è¡Œç¨‹åºã€‚</li>
<li>é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹è§‚å¯Ÿé€‰å®šæ£€æŸ¥ç‚¹æ¥æ£€æµ‹å¹¶å®šä½æ‰§è¡Œæ•…éšœã€‚</li>
<li>HyCodePolicyé€šè¿‡ç»“åˆç»“æ„åŒ–æ‰§è¡Œè½¨è¿¹å’Œæ„ŸçŸ¥åé¦ˆæ¥æ¨æ–­æ•…éšœåŸå› å¹¶è¿›è¡Œç¨‹åºä¿®å¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02629">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2f1b1063d79cac75e2b5970f38e7c1b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e00bea4a7fb9f8f4ccfb2822d131f8ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-071ec1194a132ef9a07dfbf35bf0f5f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b03b45c362657aa8047b34093867ff8a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45c0ca75878461436ff4182c63ab2bee.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Mitigating-Attention-Hacking-in-Preference-Based-Reward-Modeling-via-Interaction-Distillation"><a href="#Mitigating-Attention-Hacking-in-Preference-Based-Reward-Modeling-via-Interaction-Distillation" class="headerlink" title="Mitigating Attention Hacking in Preference-Based Reward Modeling via   Interaction Distillation"></a>Mitigating Attention Hacking in Preference-Based Reward Modeling via   Interaction Distillation</h2><p><strong>Authors:Jianxiang Zang, Meiling Ning, Shihan Dou, Jiazheng Zhang, Tao Gui, Qi Zhang, Xuanjing Huang</strong></p>
<p>The reward model (RM), as the core component of reinforcement learning from human feedback (RLHF) for large language models (LLMs), responsible for providing reward signals to generated responses. However, mainstream preference modeling in RM is inadequate in terms of token-level interaction, making its judgment signals vulnerable to being hacked by misallocated attention to context. This stems from two fundamental limitations: (1) Current preference modeling employs decoder-only architectures, where the unidirectional causal attention mechanism leads to forward-decaying intra-sequence attention within the prompt-response sequence. (2) The independent Siamese-encoding paradigm induces the absence of token-level inter-sequence attention between chosen and rejected sequences. To address this â€œattention hackingâ€, we propose â€œInteraction Distillationâ€, a novel training framework for more adequate preference modeling through attention-level optimization. The method introduces an interaction-based natural language understanding model as the teacher to provide sophisticated token interaction patterns via comprehensive attention, and guides the preference modeling to simulate teacher modelâ€™s interaction pattern through an attentional alignment objective. Through extensive experiments, interaction distillation has demonstrated its ability to provide more stable and generalizable reward signals compared to state-of-the-art RM optimization methods that target data noise, highlighting the attention hacking constitute a more fundamental limitation in RM. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ä¸ºç”Ÿæˆçš„å“åº”æä¾›å¥–åŠ±ä¿¡å·ã€‚ç„¶è€Œï¼ŒRMä¸­çš„ä¸»æµåå¥½å»ºæ¨¡åœ¨ä»¤ç‰Œçº§äº¤äº’æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´åˆ¤æ–­ä¿¡å·å®¹æ˜“å—åˆ°å¯¹ä¸Šä¸‹æ–‡åˆ†é…ä¸å½“çš„æ³¨æ„åŠ›çš„æ”»å‡»ã€‚è¿™æºäºä¸¤ä¸ªåŸºæœ¬å±€é™ï¼šï¼ˆ1ï¼‰å½“å‰çš„åå¥½å»ºæ¨¡ä»…é‡‡ç”¨è§£ç å™¨æ¶æ„ï¼Œå…¶ä¸­å•å‘å› æœæ³¨æ„åŠ›æœºåˆ¶å¯¼è‡´æç¤º-å“åº”åºåˆ—å†…çš„åºåˆ—å†…æ³¨æ„åŠ›å‘ˆç°å‰å‘è¡°å‡ã€‚ï¼ˆ2ï¼‰ç‹¬ç«‹çš„Siameseç¼–ç èŒƒå¼å¯¼è‡´æ‰€é€‰åºåˆ—å’Œæ‹’ç»åºåˆ—ä¹‹é—´ç¼ºå°‘ä»¤ç‰Œçº§åºåˆ—é—´æ³¨æ„åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ç§â€œæ³¨æ„åŠ›æ”»å‡»â€ï¼Œæˆ‘ä»¬æå‡ºäº†â€œäº¤äº’è’¸é¦â€ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æ³¨æ„åŠ›çº§åˆ«ä¼˜åŒ–è¿›è¡Œæ›´å……åˆ†åå¥½å»ºæ¨¡çš„æ–°å‹è®­ç»ƒæ¡†æ¶ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§åŸºäºäº¤äº’çš„è‡ªç„¶è¯­è¨€ç†è§£æ¨¡å‹ä½œä¸ºæ•™å¸ˆï¼Œé€šè¿‡å…¨é¢çš„æ³¨æ„åŠ›æä¾›ç²¾ç»†çš„ä»¤ç‰Œäº¤äº’æ¨¡å¼ï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›å¯¹é½ç›®æ ‡æŒ‡å¯¼åå¥½å»ºæ¨¡æ¥æ¨¡æ‹Ÿæ•™å¸ˆæ¨¡å‹çš„äº¤äº’æ¨¡å¼ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œäº¤äº’è’¸é¦æ˜¾ç¤ºå‡ºæä¾›æ¯”é’ˆå¯¹æ•°æ®å™ªå£°ä¼˜åŒ–çš„æœ€æ–°RMæ–¹æ³•æ›´ç¨³å®šå’Œå¯æ¨å¹¿çš„å¥–åŠ±ä¿¡å·çš„èƒ½åŠ›ï¼Œå¼ºè°ƒäº†æ³¨æ„åŠ›æ”»å‡»åœ¨RMä¸­æ„æˆæ›´åŸºæœ¬çš„å±€é™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02618v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ä½œä¸ºå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­çš„æ ¸å¿ƒç»„ä»¶ï¼Œç”¨äºä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆå“åº”æä¾›å¥–åŠ±ä¿¡å·ã€‚ç„¶è€Œï¼Œä¸»æµçš„åå¥½å»ºæ¨¡åœ¨RMä¸­å­˜åœ¨ç¬¦å·çº§äº¤äº’çš„ä¸è¶³ï¼Œä½¿å¾—å…¶åˆ¤æ–­ä¿¡å·å®¹æ˜“å—åˆ°å¯¹ä¸Šä¸‹æ–‡åˆ†é…ä¸å½“æ³¨æ„åŠ›çš„æ”»å‡»ã€‚è¿™æºäºä¸¤ä¸ªåŸºæœ¬å±€é™ï¼šä¸€æ˜¯å½“å‰åå¥½å»ºæ¨¡é‡‡ç”¨å•å‘å› æœæ³¨æ„åŠ›æœºåˆ¶ï¼›äºŒæ˜¯ç‹¬ç«‹å¼èµ›ç“¦ç¼–ç èŒƒå¼å¯¼è‡´äº†æ ‡è®°çº§åˆ«åºåˆ—é—´æ³¨æ„åŠ›çš„ç¼ºå¤±ã€‚ä¸ºè§£å†³æ³¨æ„åŠ›ç ´è§£é—®é¢˜ï¼Œæå‡ºäº†åä¸ºâ€œäº¤äº’è’¸é¦â€çš„æ–°å‹è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡æ³¨æ„åŠ›çº§åˆ«çš„ä¼˜åŒ–è¿›è¡Œæ›´å‡†ç¡®çš„åå¥½å»ºæ¨¡ã€‚è¯¥æ–¹æ³•å¼•å…¥åŸºäºäº¤äº’çš„è‡ªç„¶è¯­è¨€ç†è§£æ¨¡å‹ä½œä¸ºæ•™å¸ˆï¼Œé€šè¿‡å…¨é¢çš„æ³¨æ„åŠ›æä¾›å¤æ‚çš„æ ‡è®°äº¤äº’æ¨¡å¼ï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›å¯¹é½ç›®æ ‡å¼•å¯¼åå¥½å»ºæ¨¡æ¨¡æ‹Ÿæ•™å¸ˆæ¨¡å‹çš„äº¤äº’æ¨¡å¼ã€‚å®éªŒè¯æ˜ï¼Œäº¤äº’è’¸é¦æä¾›äº†æ›´ç¨³å®šå’Œå¯æ³›åŒ–çš„å¥–åŠ±ä¿¡å·ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰æ˜¯å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰çš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œè´Ÿè´£ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆå“åº”æä¾›å¥–åŠ±ä¿¡å·ã€‚</li>
<li>ä¸»æµåå¥½å»ºæ¨¡åœ¨RMä¸­å­˜åœ¨å±€é™æ€§ï¼Œæ˜“å—åˆ°æ³¨æ„åŠ›ç ´è§£æ”»å‡»ã€‚</li>
<li>å½“å‰åå¥½å»ºæ¨¡çš„å±€é™ä¸»è¦ä½“ç°åœ¨ä¸¤ä¸ªæ–¹é¢ï¼šä¸€æ˜¯å•å‘å› æœæ³¨æ„åŠ›æœºåˆ¶å¯¼è‡´çš„åºåˆ—å†…éƒ¨æ³¨æ„åŠ›è¡°å‡ï¼›äºŒæ˜¯åœ¨é€‰æ‹©åºåˆ—å’Œæ‹’ç»åºåˆ—ä¹‹é—´ç¼ºä¹æ ‡è®°çº§åˆ«çš„äº¤äº’æ³¨æ„åŠ›ã€‚</li>
<li>â€œäº¤äº’è’¸é¦â€æ˜¯ä¸€ç§æ–°å‹è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³æ³¨æ„åŠ›ç ´è§£é—®é¢˜ï¼Œé€šè¿‡æ³¨æ„åŠ›çº§åˆ«çš„ä¼˜åŒ–è¿›è¡Œæ›´å‡†ç¡®çš„åå¥½å»ºæ¨¡ã€‚</li>
<li>äº¤äº’è’¸é¦å¼•å…¥åŸºäºäº¤äº’çš„è‡ªç„¶è¯­è¨€ç†è§£æ¨¡å‹ä½œä¸ºæ•™å¸ˆï¼Œé€šè¿‡å…¨é¢çš„æ³¨æ„åŠ›æä¾›å¤æ‚çš„æ ‡è®°äº¤äº’æ¨¡å¼ã€‚</li>
<li>äº¤äº’è’¸é¦é€šè¿‡æ³¨æ„åŠ›å¯¹é½ç›®æ ‡æ¥å¼•å¯¼åå¥½å»ºæ¨¡æ¨¡æ‹Ÿæ•™å¸ˆæ¨¡å‹çš„äº¤äº’æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02618">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0aa758030e87e3261ee82f31b003c935.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6893c163fc7f47bae88b58b02984b63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9aa88decb8b2322586720b54f82c4e94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df1bfefaa73abf3ab37ed7b367788baf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79c624f9f637e5a058a01bf9c913aecf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffebb094f31a77d29f0db6cd31f80a29.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Meta-RAG-on-Large-Codebases-Using-Code-Summarization"><a href="#Meta-RAG-on-Large-Codebases-Using-Code-Summarization" class="headerlink" title="Meta-RAG on Large Codebases Using Code Summarization"></a>Meta-RAG on Large Codebases Using Code Summarization</h2><p><strong>Authors:Vali Tawosia, Salwa Alamir, Xiaomo Liu, Manuela Veloso</strong></p>
<p>Large Language Model (LLM) systems have been at the forefront of applied Artificial Intelligence (AI) research in a multitude of domains. One such domain is software development, where researchers have pushed the automation of a number of code tasks through LLM agents. Software development is a complex ecosystem, that stretches far beyond code implementation and well into the realm of code maintenance. In this paper, we propose a multi-agent system to localize bugs in large pre-existing codebases using information retrieval and LLMs. Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8%, into a compact, structured, natural language representation. We then use an LLM agent to determine which parts of the codebase are critical for bug resolution, i.e. bug localization. We demonstrate the usefulness of Meta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores 84.67 % and 53.0 % for file-level and function-level correct localization rates, respectively, achieving state-of-the-art performance. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»ç»Ÿåœ¨å¤šé¢†åŸŸåº”ç”¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç ”ç©¶ä¸­å¤„äºå‰æ²¿åœ°ä½ã€‚å…¶ä¸­ä¸€ä¸ªè¿™æ ·çš„é¢†åŸŸæ˜¯è½¯ä»¶å¼€å‘ï¼Œç ”ç©¶äººå‘˜é€šè¿‡LLMä»£ç†æ¨åŠ¨äº†å¤šä¸ªä»£ç ä»»åŠ¡çš„è‡ªåŠ¨åŒ–ã€‚è½¯ä»¶å¼€å‘æ˜¯ä¸€ä¸ªå¤æ‚çš„ç”Ÿæ€ç³»ç»Ÿï¼Œè¿œè¿œè¶…å‡ºä»£ç å®ç°ï¼Œæ·±å…¥åˆ°ä»£ç ç»´æŠ¤çš„é¢†åŸŸã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨ä¿¡æ¯æ£€ç´¢å’ŒLLMåœ¨å¤§å‹ç°æœ‰ä»£ç åº“ä¸­å®šä½é”™è¯¯çš„å¤šä»£ç†ç³»ç»Ÿã€‚æˆ‘ä»¬çš„ç³»ç»Ÿå¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ–¹æ³•ï¼Œå³Meta-RAGï¼Œæˆ‘ä»¬åˆ©ç”¨æ‘˜è¦å°†ä»£ç åº“å¹³å‡å‹ç¼©79.8%ï¼Œå‹ç¼©æˆç´§å‡‘ã€ç»“æ„åŒ–ã€è‡ªç„¶è¯­è¨€è¡¨ç¤ºçš„å½¢å¼ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨LLMä»£ç†æ¥ç¡®å®šå¯¹è§£å†³é”™è¯¯è‡³å…³é‡è¦çš„ä»£ç åº“éƒ¨åˆ†ï¼Œå³é”™è¯¯å®šä½ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨SWE-bench Liteæ•°æ®é›†å¯¹Meta-RAGè¿›è¡Œäº†è¯„ä¼°ï¼Œè¯æ˜äº†å…¶å®ç”¨æ€§ã€‚Meta-RAGçš„æ–‡ä»¶çº§å’Œå‡½æ•°çº§æ­£ç¡®å®šä½ç‡åˆ†åˆ«ä¸º84.67%å’Œ53.0%ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02611v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç³»ç»Ÿåœ¨å¤šä¸ªé¢†åŸŸçš„äººå·¥æ™ºèƒ½ç ”ç©¶å‰æ²¿ä¸­å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚åœ¨è½¯ä»¶å¼€å‘é¢†åŸŸï¼Œç ”ç©¶äººå‘˜é€šè¿‡LLMä»£ç†æ¨åŠ¨äº†ä»£ç ä»»åŠ¡çš„è‡ªåŠ¨åŒ–ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šä»£ç†ç³»ç»Ÿï¼Œåˆ©ç”¨ä¿¡æ¯æ£€ç´¢å’ŒLLMå®šä½å¤§å‹ç°æœ‰ä»£ç åº“ä¸­çš„é”™è¯¯ã€‚æˆ‘ä»¬ç³»ç»Ÿå¼•å…¥äº†æ–°å‹çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•Meta-RAGï¼Œé€šè¿‡æ‘˜è¦å°†ä»£ç åº“å¹³å‡å‹ç¼©79.8%ï¼Œè½¬åŒ–ä¸ºç´§å‡‘ã€ç»“æ„åŒ–çš„è‡ªç„¶è¯­è¨€è¡¨ç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨LLMä»£ç†ç¡®å®šå¯¹é”™è¯¯ä¿®å¤è‡³å…³é‡è¦çš„ä»£ç åº“éƒ¨åˆ†ï¼Œå³é”™è¯¯å®šä½ã€‚é€šè¿‡SWE-bench Liteæ•°æ®é›†è¯„ä¼°ï¼ŒMeta-RAGåœ¨æ–‡ä»¶çº§åˆ«å’Œå‡½æ•°çº§åˆ«çš„æ­£ç¡®å®šä½ç‡åˆ†åˆ«ä¸º84.67%å’Œ53.0%ï¼Œè¾¾åˆ°äº†ä¸šç•Œå…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMç³»ç»Ÿåœ¨å¤šä¸ªé¢†åŸŸçš„äººå·¥æ™ºèƒ½ç ”ç©¶ä¸­å¤„äºå‰æ²¿åœ°ä½ï¼ŒåŒ…æ‹¬è½¯ä»¶å¼€å‘ã€‚</li>
<li>åœ¨è½¯ä»¶å¼€å‘ä¸­ï¼ŒLLMä»£ç†æ¨åŠ¨äº†ä»£ç ä»»åŠ¡çš„è‡ªåŠ¨åŒ–ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¤šä»£ç†ç³»ç»Ÿï¼Œåˆ©ç”¨ä¿¡æ¯æ£€ç´¢å’ŒLLMæŠ€æœ¯å®šä½å¤§å‹ç°æœ‰ä»£ç åº“ä¸­çš„é”™è¯¯ã€‚</li>
<li>å¼•å…¥äº†æ–°å‹çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•Meta-RAGï¼Œé€šè¿‡æ‘˜è¦å°†ä»£ç åº“è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€è¡¨ç¤ºã€‚</li>
<li>Meta-RAGæ–¹æ³•èƒ½å¹³å‡å‹ç¼©ä»£ç åº“79.8%ï¼Œæé«˜é”™è¯¯å®šä½çš„æ•ˆç‡ã€‚</li>
<li>é€šè¿‡SWE-bench Liteæ•°æ®é›†è¯„ä¼°ï¼ŒMeta-RAGåœ¨æ–‡ä»¶çº§åˆ«å’Œå‡½æ•°çº§åˆ«çš„æ­£ç¡®å®šä½ç‡åˆ†åˆ«è¾¾åˆ°äº†84.67%å’Œ53.0%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02611">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2eda9320db29644ee67095f960cb88bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e781acf167585620641c75d72ec7801.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d2f65b18f7fbd86ffd31c0f947eb244.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-272591b4084aa1bf24f2e6c0c2fdc030.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c724ec70b9faf1132c6c2623be7299bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dba5ec36b97c1861f9df40f77d233e2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="StructSynth-Leveraging-LLMs-for-Structure-Aware-Tabular-Data-Synthesis-in-Low-Data-Regimes"><a href="#StructSynth-Leveraging-LLMs-for-Structure-Aware-Tabular-Data-Synthesis-in-Low-Data-Regimes" class="headerlink" title="StructSynth: Leveraging LLMs for Structure-Aware Tabular Data Synthesis   in Low-Data Regimes"></a>StructSynth: Leveraging LLMs for Structure-Aware Tabular Data Synthesis   in Low-Data Regimes</h2><p><strong>Authors:Siyi Liu, Yujia Zheng, Yongqi Zhang</strong></p>
<p>The application of machine learning on tabular data in specialized domains is severely limited by data scarcity. While generative models offer a solution, traditional methods falter in low-data regimes, and recent Large Language Models (LLMs) often ignore the explicit dependency structure of tabular data, leading to low-fidelity synthetics. To address these limitations, we introduce StructSynth, a novel framework that integrates the generative power of LLMs with robust structural control. StructSynth employs a two-stage architecture. First, it performs explicit structure discovery to learn a Directed Acyclic Graph (DAG) from the available data. Second, this learned structure serves as a high-fidelity blueprint to steer the LLMâ€™s generation process, forcing it to adhere to the learned feature dependencies and thereby ensuring the generated data respects the underlying structure by design. Our extensive experiments demonstrate that StructSynth produces synthetic data with significantly higher structural integrity and downstream utility than state-of-the-art methods. It proves especially effective in challenging low-data scenarios, successfully navigating the trade-off between privacy preservation and statistical fidelity. </p>
<blockquote>
<p>åœ¨ç‰¹å®šé¢†åŸŸï¼Œæœºå™¨å­¦ä¹ åœ¨è¡¨æ ¼æ•°æ®ä¸Šçš„åº”ç”¨å—åˆ°æ•°æ®ç¨€ç¼ºçš„ä¸¥é‡é™åˆ¶ã€‚è™½ç„¶ç”Ÿæˆæ¨¡å‹æä¾›äº†ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œä½†ä¼ ç»Ÿæ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ä¼šå¤±å»æ•ˆèƒ½ï¼Œè€Œä¸”æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾€å¾€ä¼šå¿½ç•¥è¡¨æ ¼æ•°æ®çš„æ˜¾å¼ä¾èµ–ç»“æ„ï¼Œå¯¼è‡´åˆæˆæ•°æ®çš„ä¿çœŸåº¦è¾ƒä½ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†StructSynthï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ä¸ç¨³å¥çš„ç»“æ„æ§åˆ¶ç›¸ç»“åˆã€‚StructSynthé‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ã€‚é¦–å…ˆï¼Œå®ƒæ‰§è¡Œæ˜¾å¼ç»“æ„å‘ç°ï¼Œä»å¯ç”¨æ•°æ®ä¸­å­¦ä¹ æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ã€‚å…¶æ¬¡ï¼Œè¿™ä¸€å­¦ä¹ åˆ°çš„ç»“æ„ä½œä¸ºä¸€ä¸ªé«˜ä¿çœŸè“å›¾ï¼Œç”¨äºå¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œè¿«ä½¿å®ƒéµå¾ªå­¦ä¹ åˆ°çš„ç‰¹å¾ä¾èµ–å…³ç³»ï¼Œä»è€Œç¡®ä¿ç”Ÿæˆçš„æ•°æ®åœ¨è®¾è®¡ä¸Šå°Šé‡åº•å±‚ç»“æ„ã€‚æˆ‘ä»¬çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒStructSynthç”Ÿæˆçš„æ•°æ®åœ¨ç»“æ„å®Œæ•´æ€§å’Œä¸‹æ¸¸å®ç”¨æ€§æ–¹é¢æ˜æ˜¾é«˜äºæœ€æ–°æ–¹æ³•ã€‚å®ƒåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä½æ•°æ®åœºæ™¯ä¸­è¡¨ç°å°¤å…¶å‡ºè‰²ï¼ŒæˆåŠŸåœ°åœ¨éšç§ä¿æŠ¤å’Œç»Ÿè®¡ä¿çœŸä¹‹é—´æ‰¾åˆ°äº†å¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02601v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç»“æ„åŒ–æ•°æ®åœ¨ç‰¹å®šé¢†åŸŸåº”ç”¨æœºå™¨å­¦ä¹ æ—¶ï¼Œæ•°æ®ç¨€ç¼ºæ€§é™åˆ¶äº†æ¨¡å‹çš„æ€§èƒ½ã€‚ç”Ÿæˆæ¨¡å‹ä¸ºè§£å†³æ­¤é—®é¢˜æä¾›äº†ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œä½†åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹ä¼ ç»Ÿæ–¹æ³•è¡¨ç°ä¸ä½³ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¿½ç•¥äº†ç»“æ„åŒ–æ•°æ®çš„æ˜¾å¼ä¾èµ–ç»“æ„ï¼Œå¯¼è‡´ç”Ÿæˆçš„åˆæˆæ•°æ®ä¿çœŸåº¦è¾ƒä½ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†StructSynthæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†LLMçš„ç”Ÿæˆèƒ½åŠ›ä¸ç¨³å¥çš„ç»“æ„æ§åˆ¶ã€‚StructSynthé‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ï¼Œé¦–å…ˆè¿›è¡Œæ˜¾å¼ç»“æ„å‘ç°ï¼Œä»ç°æœ‰æ•°æ®ä¸­å­¦ä¹ æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ã€‚å…¶æ¬¡ï¼Œä½¿ç”¨å­¦ä¹ çš„ç»“æ„ä½œä¸ºé«˜ä¿çœŸè“å›¾æ¥å¼•å¯¼LLMçš„ç”Ÿæˆè¿‡ç¨‹ï¼Œç¡®ä¿ç”Ÿæˆçš„æ•°æ®éµå¾ªè®¾è®¡çš„åº•å±‚ç»“æ„ã€‚å®éªŒè¯æ˜ï¼ŒStructSynthåœ¨ç»“æ„å®Œæ•´æ€§å’Œä¸‹æ¸¸å®ç”¨æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½æ•°æ®åœºæ™¯ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼ŒæˆåŠŸå®ç°äº†éšç§ä¿æŠ¤å’Œç»Ÿè®¡ä¿çœŸä¹‹é—´çš„å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®ç¨€ç¼ºæ€§æ˜¯æœºå™¨å­¦ä¹ åœ¨ç‰¹å®šé¢†åŸŸåº”ç”¨ç»“æ„åŒ–æ•°æ®çš„ç“¶é¢ˆã€‚</li>
<li>ç”Ÿæˆæ¨¡å‹æ˜¯è§£å†³æ•°æ®ç¨€ç¼ºæ€§çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹è¡¨ç°ä¸ä½³ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¿½ç•¥äº†ç»“æ„åŒ–æ•°æ®çš„æ˜¾å¼ä¾èµ–ç»“æ„ï¼Œå¯¼è‡´åˆæˆæ•°æ®ä¿çœŸåº¦ä½ã€‚</li>
<li>StructSynthæ¡†æ¶ç»“åˆäº†LLMçš„ç”Ÿæˆèƒ½åŠ›ä¸ç»“æ„æ§åˆ¶æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>StructSynthé‡‡ç”¨ä¸¤é˜¶æ®µæ¶æ„ï¼Œå…ˆè¿›è¡Œç»“æ„å‘ç°å­¦ä¹ ï¼Œå†åº”ç”¨å­¦åˆ°çš„ç»“æ„æŒ‡å¯¼LLMç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>å®éªŒè¡¨æ˜StructSynthç”Ÿæˆçš„åˆæˆæ•°æ®å…·æœ‰æ›´é«˜çš„ç»“æ„å®Œæ•´æ€§å’Œä¸‹æ¸¸å®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02601">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-51aeba23d4a0f3fb0932ba35f04124fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15668b17c81fa499f2ef8818ca4df8b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-341e80d94ea9ba2926a218687976c9e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-844b21418e43d29ff55e588a5f1e5308.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CAMA-Enhancing-Mathematical-Reasoning-in-Large-Language-Models-with-Causal-Knowledge"><a href="#CAMA-Enhancing-Mathematical-Reasoning-in-Large-Language-Models-with-Causal-Knowledge" class="headerlink" title="CAMA: Enhancing Mathematical Reasoning in Large Language Models with   Causal Knowledge"></a>CAMA: Enhancing Mathematical Reasoning in Large Language Models with   Causal Knowledge</h2><p><strong>Authors:Lei Zan, Keli Zhang, Ruichu Cai, Lujia Pan</strong></p>
<p>Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks, yet they still struggle with complex mathematical reasoning, a challenge fundamentally rooted in deep structural dependencies. To address this challenge, we propose \textbf{CA}usal \textbf{MA}thematician (\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit, reusable mathematical structure. In the learning stage, CAMA first constructs the \textbf{M}athematical \textbf{C}ausal \textbf{G}raph (\textbf{MCG}), a high-level representation of solution strategies, by combining LLM priors with causal discovery algorithms applied to a corpus of question-solution pairs. The resulting MCG encodes essential knowledge points and their causal dependencies. To better align the graph with downstream reasoning tasks, CAMA further refines the MCG through iterative feedback derived from a selected subset of the question-solution pairs. In the reasoning stage, given a new question, CAMA dynamically extracts a task-relevant subgraph from the MCG, conditioned on both the question content and the LLMâ€™s intermediate reasoning trace. This subgraph, which encodes the most pertinent knowledge points and their causal dependencies, is then injected back into the LLM to guide its reasoning process. Empirical results on real-world datasets show that CAMA significantly improves LLM performance on challenging mathematical problems. Furthermore, our experiments demonstrate that structured guidance consistently outperforms unstructured alternatives, and that incorporating asymmetric causal relationships yields greater improvements than using symmetric associations alone. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨å¤æ‚çš„æ•°å­¦æ¨ç†æ–¹é¢ä»å­˜åœ¨å›°éš¾ï¼Œè¿™ä¸€æŒ‘æˆ˜æ ¹æœ¬æºäºæ·±å±‚çš„ç»“æ„ä¾èµ–æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å› æœæ•°å­¦å®¶ï¼ˆ\textbf{CAMA}ï¼‰è¿™ä¸€ä¸¤é˜¶æ®µå› æœæ¡†æ¶ï¼Œä¸ºLLMé…å¤‡æ˜ç¡®çš„å¯é‡ç”¨æ•°å­¦ç»“æ„ã€‚åœ¨å­¦ä¹ é˜¶æ®µï¼ŒCAMAé¦–å…ˆæ„å»ºæ•°å­¦å› æœå›¾ï¼ˆMCGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è§£å†³æ–¹æ¡ˆç­–ç•¥çš„é«˜çº§è¡¨ç¤ºï¼Œé€šè¿‡ç»“åˆLLMçš„å…ˆéªŒçŸ¥è¯†å’Œåº”ç”¨äºé—®é¢˜è§£å†³æ–¹æ¡ˆå¯¹è¯­æ–™åº“çš„å› æœå‘ç°ç®—æ³•ã€‚ç»“æœäº§ç”Ÿçš„MCGç¼–ç äº†é‡è¦çš„çŸ¥è¯†ç‚¹åŠå…¶å› æœå…³ç³»ã€‚ä¸ºäº†æ›´å¥½åœ°ä¸ä¸‹æ¸¸æ¨ç†ä»»åŠ¡å¯¹é½ï¼ŒCAMAé€šè¿‡æ¥è‡ªé—®é¢˜è§£å†³æ–¹æ¡ˆå¯¹é€‰æ‹©å­é›†çš„è¿­ä»£åé¦ˆè¿›ä¸€æ­¥æ”¹è¿›äº†MCGã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œå¯¹äºæ–°é—®é¢˜ï¼ŒCAMAä¼šæ ¹æ®é—®é¢˜çš„å†…å®¹å’ŒLLMçš„ä¸­é—´æ¨ç†è½¨è¿¹åŠ¨æ€åœ°ä»MCGä¸­æå–å‡ºä¸ä»»åŠ¡ç›¸å…³çš„å­å›¾ã€‚è¿™ä¸ªå­å›¾ç¼–ç äº†æœ€ç›¸å…³çš„çŸ¥è¯†ç‚¹åŠå…¶å› æœå…³ç³»ï¼Œç„¶åé‡æ–°æ³¨å…¥LLMä»¥æŒ‡å¯¼å…¶æ¨ç†è¿‡ç¨‹ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒCAMAåœ¨è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦é—®é¢˜æ–¹é¢æ˜¾è‘—æé«˜äº†LLMçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œç»“æ„åŒ–çš„æŒ‡å¯¼å§‹ç»ˆä¼˜äºéç»“æ„åŒ–çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶ä¸”èå…¥ä¸å¯¹ç§°çš„å› æœå…³ç³»æ¯”ä»…ä½¿ç”¨å¯¹ç§°å…³è”å¸¦æ¥æ›´å¤§çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02583v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨å¤æ‚çš„æ•°å­¦æ¨ç†æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæå‡ºäº†å› æœæ•°å­¦å®¶ï¼ˆCAMAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼Œæ—¨åœ¨ä¸ºLLMæä¾›æ˜ç¡®çš„å¯é‡å¤ä½¿ç”¨çš„æ•°å­¦ç»“æ„ã€‚ç¬¬ä¸€é˜¶æ®µæ„å»ºæ•°å­¦å› æœå›¾ï¼ˆMCGï¼‰ï¼Œç»“åˆLLMå…ˆéªŒçŸ¥è¯†ä¸å› æœå‘ç°ç®—æ³•ï¼Œå¯¹é—®é¢˜è§£ç­”å¯¹è¿›è¡Œé«˜å±‚æ¬¡è¡¨ç¤ºã€‚ç¬¬äºŒé˜¶æ®µæ ¹æ®æ–°é—®é¢˜ä»MCGä¸­æå–ç›¸å…³å­å›¾ï¼Œå¹¶æ³¨å…¥LLMä»¥æŒ‡å¯¼å…¶æ¨ç†è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜ï¼ŒCAMAèƒ½æ˜¾è‘—æé«˜LLMè§£å†³æ•°å­¦é—®é¢˜çš„èƒ½åŠ›ï¼Œä¸”ç»“æ„åŒ–çš„æŒ‡å¯¼æ–¹å¼ä¼˜äºéç»“æ„åŒ–æ–¹å¼ï¼Œåˆ©ç”¨ä¸å¯¹ç§°å› æœå…³ç³»æ¯”ä»…ä½¿ç”¨å¯¹ç§°å…³è”æ•ˆæœæ›´å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤æ‚æ•°å­¦æ¨ç†ä¸Šä»æœ‰æŒ‘æˆ˜ï¼Œéœ€è¦æ–°çš„æ–¹æ³•æ¥è§£å†³ã€‚</li>
<li>CAMAæ¡†æ¶åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼šå­¦ä¹ é˜¶æ®µå’Œæ¨ç†é˜¶æ®µã€‚</li>
<li>å­¦ä¹ é˜¶æ®µé€šè¿‡æ„å»ºæ•°å­¦å› æœå›¾ï¼ˆMCGï¼‰æ¥ç»“åˆLLMçš„å…ˆéªŒçŸ¥è¯†ä¸å› æœå‘ç°ç®—æ³•ã€‚</li>
<li>CAMAé€šè¿‡è¿­ä»£åé¦ˆä¼˜åŒ–MCGï¼Œä½¿å…¶æ›´å¥½åœ°é€‚åº”ä¸‹æ¸¸æ¨ç†ä»»åŠ¡ã€‚</li>
<li>æ¨ç†é˜¶æ®µæ ¹æ®æ–°é—®é¢˜ä»MCGä¸­æå–ç›¸å…³å­å›¾ï¼Œå¹¶æ³¨å…¥LLMä»¥æŒ‡å¯¼å…¶æ¨ç†ã€‚</li>
<li>å®éªŒè¡¨æ˜CAMAèƒ½æ˜¾è‘—æé«˜LLMè§£å†³æ•°å­¦é—®é¢˜çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02583">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d1c28ab7f4a2f4f03183c8ab9c0e5c98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb36c79e159593473a6f104fb2645231.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef83bcc7ab4751853333c5463f3b0db0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Contextual-Graph-Transformer-A-Small-Language-Model-for-Enhanced-Engineering-Document-Information-Extraction"><a href="#Contextual-Graph-Transformer-A-Small-Language-Model-for-Enhanced-Engineering-Document-Information-Extraction" class="headerlink" title="Contextual Graph Transformer: A Small Language Model for Enhanced   Engineering Document Information Extraction"></a>Contextual Graph Transformer: A Small Language Model for Enhanced   Engineering Document Information Extraction</h2><p><strong>Authors:Karan Reddy, Mayukha Pal</strong></p>
<p>Standard transformer-based language models, while powerful for general text, often struggle with the fine-grained syntax and entity relationships in complex technical, engineering documents. To address this, we propose the Contextual Graph Transformer (CGT), a hybrid neural architecture that combines Graph Neural Networks (GNNs) and Transformers for domain-specific question answering. CGT constructs a dynamic graph over input tokens using sequential, skip-gram, and semantic similarity edges, which is processed by GATv2Conv layers for local structure learning. These enriched embeddings are then passed to a Transformer encoder to capture global dependencies. Unlike generic large models, technical domains often require specialized language models with stronger contextualization and structure awareness. CGT offers a parameter-efficient solution for such use cases. Integrated into a Retrieval-Augmented Generation (RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7% higher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems from CGTs ability to jointly model structural token interactions and long-range semantic coherence. The model is trained from scratch using a two-phase approach: pretraining on general text followed by fine-tuning on domain-specific manuals. This highlights CGTs adaptability to technical language, enabling better grounding, entity tracking, and retrieval-augmented responses in real-world applications. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„åŸºäºå˜å‹å™¨çš„è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ä¸€èˆ¬æ–‡æœ¬æ—¶è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨å¤„ç†å¤æ‚æŠ€æœ¯ã€å·¥ç¨‹æ–‡æ¡£ä¸­çš„ç»†å¾®è¯­æ³•å’Œå®ä½“å…³ç³»æ—¶å¾€å¾€é‡åˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸Šä¸‹æ–‡å›¾å˜å‹å™¨ï¼ˆCGTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ··åˆç¥ç»ç½‘ç»œæ¶æ„ï¼Œå®ƒç»“åˆäº†å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰å’Œå˜å‹å™¨ï¼Œç”¨äºç‰¹å®šé¢†åŸŸçš„é—®ç­”ã€‚CGTåœ¨è¾“å…¥æ ‡è®°ä¸Šæ„å»ºäº†ä¸€ä¸ªåŠ¨æ€å›¾ï¼Œè¯¥å›¾ä½¿ç”¨é¡ºåºã€è·³è¯å’Œè¯­ä¹‰ç›¸ä¼¼è¾¹ç¼˜è¿›è¡Œå¤„ç†ï¼Œé€šè¿‡GATv2Convå±‚è¿›è¡Œå±€éƒ¨ç»“æ„å­¦ä¹ ã€‚è¿™äº›ä¸°å¯Œçš„åµŒå…¥ç„¶åä¼ é€’ç»™å˜å‹å™¨ç¼–ç å™¨ä»¥æ•è·å…¨å±€ä¾èµ–å…³ç³»ã€‚ä¸é€šç”¨å¤§å‹æ¨¡å‹ä¸åŒï¼ŒæŠ€æœ¯é¢†åŸŸå¾€å¾€éœ€è¦å…·æœ‰æ›´å¼ºä¸Šä¸‹æ–‡åŒ–å’Œç»“æ„æ„è¯†çš„ä¸“é—¨è¯­è¨€æ¨¡å‹ã€‚CGTä¸ºè¿™ç§æƒ…å†µæä¾›äº†å‚æ•°é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚é›†æˆåˆ°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“ä¸­ï¼ŒCGTä¼˜äºGPT-2å’ŒBERTç­‰åŸºçº¿ï¼Œåœ¨ä»…æœ‰62.4%å‚æ•°çš„æƒ…å†µä¸‹ï¼Œæ¯”GPT-2é«˜å‡º24.7%çš„å‡†ç¡®ç‡ã€‚è¿™ä¸€æ”¶ç›Šæ¥æºäºCGTè”åˆå»ºæ¨¡ç»“æ„æ ‡è®°äº¤äº’å’Œé•¿è·ç¦»è¯­ä¹‰è¿è´¯æ€§çš„èƒ½åŠ›ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µæ–¹æ³•ä»å¤´å¼€å§‹è®­ç»ƒï¼šé¦–å…ˆåœ¨é€šç”¨æ–‡æœ¬ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨ç‰¹å®šé¢†åŸŸæ‰‹å†Œä¸Šè¿›è¡Œå¾®è°ƒã€‚è¿™çªå‡ºäº†CGTå¯¹æŠ€æœ¯è¯­è¨€çš„é€‚åº”æ€§ï¼Œå¯åœ¨å®é™…åº”ç”¨ç¨‹åºä¸­å®ç°æ›´å¥½çš„æ¥åœ°ã€å®ä½“è·Ÿè¸ªå’Œæ£€ç´¢å¢å¼ºå“åº”ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02532v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹æ ‡å‡†åŸºäºè½¬æ¢å™¨çš„è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚æŠ€æœ¯ã€å·¥ç¨‹æ–‡æ¡£ä¸­ç»†å¾®è¯­æ³•å’Œå®ä½“å…³ç³»æ—¶çš„å›°å¢ƒï¼Œæå‡ºä¸Šä¸‹æ–‡å›¾è½¬æ¢å™¨ï¼ˆCGTï¼‰è¿™ä¸€æ··åˆç¥ç»ç½‘ç»œæ¶æ„ã€‚CGTç»“åˆå›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰å’Œè½¬æ¢å™¨ï¼Œç”¨äºç‰¹å®šé¢†åŸŸçš„é—®ç­”ã€‚å®ƒé€šè¿‡æ„å»ºåŠ¨æ€å›¾ã€ä½¿ç”¨GATv2Convå±‚è¿›è¡Œæœ¬åœ°ç»“æ„å­¦ä¹ ã€å¢å¼ºåµŒå…¥ï¼Œå¹¶ä¼ é€’ç»™è½¬æ¢å™¨ç¼–ç å™¨ä»¥æ•è·å…¨å±€ä¾èµ–å…³ç³»ã€‚CGTä¸ºæŠ€æœ¯é¢†åŸŸçš„ç‰¹å®šç”¨ä¾‹æä¾›äº†å‚æ•°é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºGPT-2å’ŒBERTç­‰åŸºçº¿æ¨¡å‹æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ ‡å‡†è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚æŠ€æœ¯æ–‡æ¡£æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ä¸Šä¸‹æ–‡å›¾è½¬æ¢å™¨ï¼ˆCGTï¼‰æ˜¯ä¸€ç§æ··åˆç¥ç»ç½‘ç»œæ¶æ„ï¼Œç»“åˆäº†å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰å’Œè½¬æ¢å™¨æŠ€æœ¯ã€‚</li>
<li>CGTé€šè¿‡æ„å»ºåŠ¨æ€å›¾å’Œåˆ©ç”¨GATv2Convå±‚è¿›è¡Œæœ¬åœ°ç»“æ„å­¦ä¹ æ¥å¢å¼ºåµŒå…¥ã€‚</li>
<li>CGTåœ¨å…¨å±€èŒƒå›´å†…ä½¿ç”¨è½¬æ¢å™¨ç¼–ç å™¨æ•è·ä¾èµ–å…³ç³»ï¼Œå…·å¤‡æ›´å¼ºçš„ä¸Šä¸‹æ–‡å’Œç»“æ„æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>CGTåœ¨å‚æ•°æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œé€‚ç”¨äºæŠ€æœ¯é¢†åŸŸã€‚</li>
<li>åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç®¡é“ä¸­ï¼ŒCGTè¾ƒåŸºçº¿æ¨¡å‹æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02532">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-38153323ca9d638964707ff5115cbbe4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d098aa5fb485531b4034c9daebf6b2ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1fac99981ed73815b2f69d251466101.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12c18e48385330b634eab9c93e24a9e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6797694190246add4d89ba335e4c79b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49fad9045bf723423fc1a6279017e086.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Accurate-and-Interpretable-Postmenstrual-Age-Prediction-via-Multimodal-Large-Language-Model"><a href="#Accurate-and-Interpretable-Postmenstrual-Age-Prediction-via-Multimodal-Large-Language-Model" class="headerlink" title="Accurate and Interpretable Postmenstrual Age Prediction via Multimodal   Large Language Model"></a>Accurate and Interpretable Postmenstrual Age Prediction via Multimodal   Large Language Model</h2><p><strong>Authors:Qifan Chen, Jin Cui, Cindy Duan, Yushuo Han, Yifei Shi</strong></p>
<p>Accurate estimation of postmenstrual age (PMA) at scan is crucial for assessing neonatal development and health. While deep learning models have achieved high accuracy in predicting PMA from brain MRI, they often function as black boxes, offering limited transparency and interpretability in clinical decision support. In this work, we address the dual challenge of accuracy and interpretability by adapting a multimodal large language model (MLLM) to perform both precise PMA prediction and clinically relevant explanation generation. We introduce a parameter-efficient fine-tuning (PEFT) strategy using instruction tuning and Low-Rank Adaptation (LoRA) applied to the Qwen2.5-VL-7B model. The model is trained on four 2D cortical surface projection maps derived from neonatal MRI scans. By employing distinct prompts for training and inference, our approach enables the MLLM to handle a regression task during training and generate clinically relevant explanations during inference. The fine-tuned model achieves a low prediction error with a 95 percent confidence interval of 0.78 to 1.52 weeks, while producing interpretable outputs grounded in developmental features, marking a significant step toward transparent and trustworthy AI systems in perinatal neuroscience. </p>
<blockquote>
<p>äº§åå¹´é¾„ï¼ˆPMAï¼‰çš„å‡†ç¡®ä¼°è®¡æ˜¯è¯„ä¼°æ–°ç”Ÿå„¿å‘è‚²å’Œå¥åº·çš„å…³é”®ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨æ ¹æ®è„‘éƒ¨MRIé¢„æµ‹PMAæ–¹é¢å·²ç»è¾¾åˆ°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬é€šå¸¸åƒé»‘ç›’å­ä¸€æ ·è¿ä½œï¼Œåœ¨ä¸´åºŠå†³ç­–æ”¯æŒä¸­æä¾›æœ‰é™çš„é€æ˜åº¦å’Œè§£é‡Šæ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡é€‚åº”å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ¥è§£å†³å‡†ç¡®æ€§å’Œè§£é‡Šæ€§çš„åŒé‡æŒ‘æˆ˜ï¼Œä»¥æ‰§è¡Œç²¾ç¡®çš„PMAé¢„æµ‹å’Œç”Ÿæˆä¸ä¸´åºŠç›¸å…³çš„è§£é‡Šã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å‚æ•°æœ‰æ•ˆçš„å¾®è°ƒï¼ˆPEFTï¼‰ç­–ç•¥ï¼Œä½¿ç”¨æŒ‡ä»¤è°ƒæ•´å’Œåº”ç”¨äºQwen2.5-VL-7Bæ¨¡å‹çš„ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ã€‚è¯¥æ¨¡å‹åœ¨ç”±æ–°ç”Ÿå„¿MRIæ‰«ææ´¾ç”Ÿçš„å››ä¸ªäºŒç»´çš®è´¨è¡¨é¢æŠ•å½±å›¾ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚é€šè¿‡ä¸ºè®­ç»ƒå’Œæ¨ç†é‡‡ç”¨ä¸åŒçš„æç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿MLLMèƒ½å¤Ÿåœ¨è®­ç»ƒæ—¶å¤„ç†å›å½’ä»»åŠ¡ï¼Œå¹¶åœ¨æ¨ç†æ—¶ç”Ÿæˆä¸ä¸´åºŠç›¸å…³çš„è§£é‡Šã€‚ç»è¿‡å¾®è°ƒçš„æ¨¡å‹å®ç°äº†è¾ƒä½çš„é¢„æµ‹è¯¯å·®ï¼Œ95ï¼…ç½®ä¿¡åŒºé—´ä¸º0.78è‡³1.52å‘¨ï¼ŒåŒæ—¶äº§ç”Ÿäº†åŸºäºå‘è‚²ç‰¹å¾çš„å¯è§£é‡Šè¾“å‡ºï¼Œæœç€é€æ˜å’Œå¯ä¿¡èµ–çš„å›´äº§æœŸç¥ç»ç§‘å­¦äººå·¥æ™ºèƒ½ç³»ç»Ÿè¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02525v1">PDF</a> Submitted to the NeurIPS 2025 Workshop GenAI4Health. Conference   website: <a target="_blank" rel="noopener" href="https://aihealth.ischool.utexas.edu/GenAI4HealthNeurips2025/">https://aihealth.ischool.utexas.edu/GenAI4HealthNeurips2025/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨äº§åå¹´é¾„ï¼ˆPMAï¼‰çš„ç²¾ç¡®ä¼°ç®—ï¼Œè¿™å¯¹äºè¯„ä¼°æ–°ç”Ÿå„¿å‘è‚²å’Œå¥åº·è‡³å…³é‡è¦ã€‚ç ”ç©¶é‡‡ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¿›è¡Œç²¾ç¡®PMAé¢„æµ‹å’Œç”Ÿæˆä¸´åºŠç›¸å…³è§£é‡Šï¼Œè§£å†³å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§çš„åŒé‡æŒ‘æˆ˜ã€‚é€šè¿‡å‚æ•°æœ‰æ•ˆçš„å¾®è°ƒï¼ˆPEFTï¼‰ç­–ç•¥å’Œä½¿ç”¨æŒ‡ä»¤è°ƒæ•´åŠä½ç§©é€‚åº”ï¼ˆLoRAï¼‰åº”ç”¨äºQwen2.5-VL-7Bæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨æ–°ç”Ÿå„¿MRIæ‰«æçš„å››ä¸ª2Dçš®è´¨è¡¨é¢æŠ•å½±å›¾ä¸Šè®­ç»ƒã€‚é€šè¿‡ä¸ºè®­ç»ƒå’Œæ¨ç†é‡‡ç”¨ä¸åŒæç¤ºï¼ŒMLLMèƒ½å¤Ÿåœ¨è®­ç»ƒæ—¶å¤„ç†å›å½’ä»»åŠ¡å¹¶åœ¨æ¨ç†æ—¶ç”Ÿæˆä¸ä¸´åºŠç›¸å…³çš„è§£é‡Šã€‚å¾®è°ƒåçš„æ¨¡å‹é¢„æµ‹è¯¯å·®ä½ï¼Œ95%ç½®ä¿¡åŒºé—´ä¸º0.78è‡³1.52å‘¨ï¼Œä¸”è¾“å‡ºæœ‰å‘è‚²ç‰¹å¾ä½œä¸ºä¾æ®ï¼Œè¿™åœ¨å›´äº§ç¥ç»ç§‘å­¦çš„é€æ˜å’Œå¯ä¿¡èµ–çš„AIç³»ç»Ÿä¸­è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº§åå¹´é¾„ï¼ˆPMAï¼‰çš„ç²¾ç¡®ä¼°è®¡æ˜¯è¯„ä¼°æ–°ç”Ÿå„¿å‘è‚²å’Œå¥åº·çš„å…³é”®ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¢«ç”¨äºè§£å†³è¿™ä¸€ä»»åŠ¡ï¼Œä»¥æé«˜å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>å‚æ•°æœ‰æ•ˆçš„å¾®è°ƒï¼ˆPEFTï¼‰ç­–ç•¥ï¼ŒåŒ…æ‹¬æŒ‡ä»¤è°ƒæ•´å’Œä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ï¼Œè¢«åº”ç”¨äºMLLMæ¨¡å‹ä¼˜åŒ–ã€‚</li>
<li>æ¨¡å‹åœ¨æ–°ç”Ÿå„¿MRIæ‰«æçš„å››ä¸ª2Dçš®è´¨è¡¨é¢æŠ•å½±å›¾ä¸Šè®­ç»ƒï¼Œæé«˜æ¨¡å‹çš„æ•ˆèƒ½ã€‚</li>
<li>é€šè¿‡ä¸åŒçš„æç¤ºè¿›è¡Œè®­ç»ƒå’Œæ¨ç†ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†å›å½’ä»»åŠ¡å¹¶ç”Ÿæˆä¸´åºŠç›¸å…³çš„è§£é‡Šã€‚</li>
<li>ç²¾ç»†è°ƒæ•´çš„æ¨¡å‹å®ç°äº†ä½é¢„æµ‹è¯¯å·®ï¼Œ95%çš„ç½®ä¿¡åŒºé—´åœ¨ä¸€å‘¨å†…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02525">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-463742fec61412a4e429428f786293bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8569c3a93b7a4f8cbb242392faa6f99e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-389727ea9871d2840e7fcf43b717919e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3f0ac794bc49c3c8a5ebc52e1699456.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4af04618868f0b77e92e207d2fbc5ab6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65ebe8e95e4697d9fe392a2daadb43f3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Decomposed-Reasoning-with-Reinforcement-Learning-for-Relevance-Assessment-in-UGC-Platforms"><a href="#Decomposed-Reasoning-with-Reinforcement-Learning-for-Relevance-Assessment-in-UGC-Platforms" class="headerlink" title="Decomposed Reasoning with Reinforcement Learning for Relevance   Assessment in UGC Platforms"></a>Decomposed Reasoning with Reinforcement Learning for Relevance   Assessment in UGC Platforms</h2><p><strong>Authors:Xiaowei Yuan, Lei Jin, Haoxin Zhang, Yan Gao, Yi Wu, Yao Hu, Ziyang Huang, Jun Zhao, Kang Liu</strong></p>
<p>Retrieval-augmented generation (RAG) plays a critical role in user-generated content (UGC) platforms, but its effectiveness depends heavily on accurate relevance assessment of query-document pairs. Despite recent advances in applying large language models (LLMs) to relevance modeling, UGC platforms present unique challenges: 1) ambiguous user intent due to sparse user feedback in RAG scenarios, and 2) substantial noise introduced by informal and unstructured language. To address these issues, we propose the Reinforced Reasoning Model for Relevance Assessment (R3A), which introduces a decomposed reasoning framework over queries and candidate documents before scoring. R3A first leverages auxiliary high-ranked documents within the platform to infer latent query intent. It then performs verbatim fragment extraction to justify relevance decisions, thereby reducing errors caused by noisy UGC. Based on a reinforcement learning framework, R3A is optimized to mitigate distortions arising from ambiguous queries and unstructured content. Experimental results show that R3A significantly outperforms existing baseline methods in terms of relevance accuracy, across both offline benchmarks and online experiments. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨ç”¨æˆ·ç”Ÿæˆå†…å®¹ï¼ˆUGCï¼‰å¹³å°ä¸­æ‰®æ¼”ç€å…³é”®è§’è‰²ï¼Œä½†å…¶æœ‰æ•ˆæ€§å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæŸ¥è¯¢æ–‡æ¡£å¯¹çš„ç›¸å…³æ€§è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚å°½ç®¡æœ€è¿‘å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨äºç›¸å…³æ€§å»ºæ¨¡å–å¾—äº†è¿›å±•ï¼Œä½†UGCå¹³å°ä»ç„¶é¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼š1ï¼‰RAGåœºæ™¯ä¸­çš„ç”¨æˆ·åé¦ˆç¨€ç–å¯¼è‡´ç”¨æˆ·æ„å›¾æ¨¡ç³Šï¼Œä»¥åŠ2ï¼‰éæ­£å¼å’Œéç»“æ„åŒ–è¯­è¨€å¼•å…¥çš„å¤§é‡å™ªå£°ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç›¸å…³æ€§è¯„ä¼°çš„å¼ºåŒ–æ¨ç†æ¨¡å‹ï¼ˆR3Aï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨è¯„åˆ†ä¹‹å‰å¼•å…¥äº†é’ˆå¯¹æŸ¥è¯¢å’Œå€™é€‰æ–‡æ¡£çš„åˆ†è§£æ¨ç†æ¡†æ¶ã€‚R3Aé¦–å…ˆåˆ©ç”¨å¹³å°å†…è¾…åŠ©çš„é«˜æ’åæ–‡æ¡£æ¥æ¨æ–­æ½œåœ¨çš„æŸ¥è¯¢æ„å›¾ã€‚ç„¶åï¼Œå®ƒæ‰§è¡Œé€å­—ç‰‡æ®µæå–ä»¥è¯æ˜ç›¸å…³æ€§å†³ç­–ï¼Œä»è€Œå‡å°‘ç”±å˜ˆæ‚çš„UGCé€ æˆçš„é”™è¯¯ã€‚åŸºäºå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ŒR3Aè¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥å‡è½»ç”±æ¨¡ç³ŠæŸ¥è¯¢å’Œéç»“æ„åŒ–å†…å®¹å¼•èµ·çš„å¤±çœŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR3Aåœ¨ç¦»çº¿åŸºå‡†æµ‹è¯•å’Œåœ¨çº¿å®éªŒä¸­å‡æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œåœ¨ç›¸å…³æ€§å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02506v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>RAGåœ¨ç”¨æˆ·ç”Ÿæˆå†…å®¹ï¼ˆUGCï¼‰å¹³å°ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†å…¶æ•ˆæœå–å†³äºæŸ¥è¯¢æ–‡æ¡£å¯¹çš„ç›¸å…³æ€§è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚é’ˆå¯¹UGCå¹³å°çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œå¦‚ç¨€ç–åé¦ˆä¸‹çš„ç”¨æˆ·æ„å›¾æ¨¡ç³Šå’Œå¤§é‡éæ­£å¼ã€éç»“æ„åŒ–è¯­è¨€å¼•å…¥çš„å™ªå£°ï¼Œæˆ‘ä»¬æå‡ºäº†å¼ºåŒ–æ¨ç†æ¨¡å‹ï¼ˆReinforced Reasoning Model for Relevance Assessmentï¼Œç®€ç§°R3Aï¼‰ã€‚R3Aé‡‡ç”¨åˆ†è§£æ¨ç†æ¡†æ¶å¯¹æŸ¥è¯¢å’Œå€™é€‰æ–‡æ¡£è¿›è¡Œè¯„åˆ†å‰çš„å¤„ç†ã€‚å®ƒé¦–å…ˆåˆ©ç”¨å¹³å°å†…è¾…åŠ©çš„é«˜æ’åæ–‡æ¡£æ¥æ¨æ–­æ½œåœ¨çš„æŸ¥è¯¢æ„å›¾ï¼Œç„¶åé€šè¿‡æå–å­—é¢ç‰‡æ®µæ¥è¯æ˜ç›¸å…³æ€§å†³ç­–ï¼Œä»è€Œå‡å°‘ç”±å˜ˆæ‚çš„UGCå¼•èµ·çš„é”™è¯¯ã€‚åŸºäºå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ŒR3Aè¢«ä¼˜åŒ–ç”¨äºå‡è½»ç”±æ¨¡ç³ŠæŸ¥è¯¢å’Œéç»“æ„åŒ–å†…å®¹å¼•èµ·çš„å¤±çœŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR3Aåœ¨ç¦»çº¿åŸºå‡†æµ‹è¯•å’Œåœ¨çº¿å®éªŒä¸­ï¼Œåœ¨ç›¸å…³æ€§å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨UGCå¹³å°ä¸­å¾ˆé‡è¦ï¼Œä½†æŸ¥è¯¢æ–‡æ¡£å¯¹çš„ç›¸å…³æ€§è¯„ä¼°æ˜¯å…³é”®ã€‚</li>
<li>UGCå¹³å°é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šç¨€ç–åé¦ˆä¸‹çš„ç”¨æˆ·æ„å›¾æ¨¡ç³Šå’Œå¤§é‡éç»“æ„åŒ–è¯­è¨€çš„å™ªå£°ã€‚</li>
<li>æå‡ºå¼ºåŒ–æ¨ç†æ¨¡å‹ï¼ˆR3Aï¼‰ä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>R3Aåˆ©ç”¨è¾…åŠ©çš„é«˜æ’åæ–‡æ¡£æ¥æ¨æ–­æ½œåœ¨æŸ¥è¯¢æ„å›¾ã€‚</li>
<li>R3Aé€šè¿‡æå–å­—é¢ç‰‡æ®µæ¥è¯æ˜ç›¸å…³æ€§å†³ç­–ï¼Œå‡å°‘é”™è¯¯ã€‚</li>
<li>R3Aé‡‡ç”¨å¼ºåŒ–å­¦ä¹ æ¡†æ¶è¿›è¡Œä¼˜åŒ–ï¼Œä»¥å‡è½»æ¨¡ç³ŠæŸ¥è¯¢å’Œéç»“æ„åŒ–å†…å®¹çš„å¤±çœŸã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒR3Aåœ¨ç›¸å…³æ€§å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02506">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5a54a1809d6d0b68239865c66d5f7181.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34084d21a745d9ab7ef1b1a53a72be62.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9fcc1f1aa36031055970a60ad022a2a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ec9be659500bc40558794e0f1e8d711.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="OptiHive-Ensemble-Selection-for-LLM-Based-Optimization-via-Statistical-Modeling"><a href="#OptiHive-Ensemble-Selection-for-LLM-Based-Optimization-via-Statistical-Modeling" class="headerlink" title="OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical   Modeling"></a>OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical   Modeling</h2><p><strong>Authors:Maxime Bouscary, Saurabh Amin</strong></p>
<p>LLM-based solvers have emerged as a promising means of automating problem modeling and solving. However, they remain unreliable and often depend on iterative repair loops that result in significant latency. We introduce OptiHive, an LLM-based framework that produces high-quality solvers for optimization problems from natural-language descriptions without iterative self-correction. OptiHive uses a single batched LLM query to generate diverse components (solvers, problem instances, and validation tests) and filters out erroneous components to ensure fully interpretable outputs. Taking into account the imperfection of the generated components, we employ a statistical model to infer their true performance, enabling principled uncertainty quantification and solver selection. On tasks ranging from traditional optimization problems to challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive significantly outperforms baselines, increasing the optimality rate from 5% to 92% on the most complex problems. </p>
<blockquote>
<p>åŸºäºLLMçš„æ±‚è§£å™¨å·²ç»æˆä¸ºè‡ªåŠ¨åŒ–é—®é¢˜å»ºæ¨¡å’Œæ±‚è§£çš„ä¸€ç§æœ‰å‰é€”çš„æ‰‹æ®µã€‚ç„¶è€Œï¼Œå®ƒä»¬ä»ç„¶ä¸å¯é ï¼Œå¹¶ä¸”ç»å¸¸ä¾èµ–äºå¯¼è‡´æ˜¾è‘—å»¶è¿Ÿçš„è¿­ä»£ä¿®å¤å¾ªç¯ã€‚æˆ‘ä»¬å¼•å…¥äº†OptiHiveï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºLLMçš„æ¡†æ¶ï¼Œå®ƒå¯ä»¥ä»è‡ªç„¶è¯­è¨€æè¿°ä¸­ä¸ºä¼˜åŒ–é—®é¢˜ç”Ÿæˆé«˜è´¨é‡æ±‚è§£å™¨ï¼Œè€Œæ— éœ€è¿›è¡Œè¿­ä»£è‡ªæˆ‘æ ¡æ­£ã€‚OptiHiveä½¿ç”¨å•ä¸ªæ‰¹å¤„ç†LLMæŸ¥è¯¢æ¥ç”Ÿæˆå„ç§ç»„ä»¶ï¼ˆæ±‚è§£å™¨ã€é—®é¢˜å®ä¾‹å’ŒéªŒè¯æµ‹è¯•ï¼‰ï¼Œå¹¶è¿‡æ»¤æ‰é”™è¯¯çš„ç»„ä»¶ä»¥ç¡®ä¿å®Œå…¨å¯è§£é‡Šçš„è¾“å‡ºã€‚è€ƒè™‘åˆ°ç”Ÿæˆçš„ç»„ä»¶çš„ä¸å®Œå–„æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨ç»Ÿè®¡æ¨¡å‹æ¥æ¨æ–­å®ƒä»¬çš„çœŸå®æ€§èƒ½ï¼Œä»è€Œå®ç°æœ‰åŸåˆ™çš„ä¸ç¡®å®šæ€§é‡åŒ–å’Œæ±‚è§£å™¨é€‰æ‹©ã€‚åœ¨ä»ä¼ ç»Ÿä¼˜åŒ–é—®é¢˜åˆ°å¤šä»“åº“è½¦è¾†è·¯ç”±é—®é¢˜çš„æŒ‘æˆ˜æ€§å˜ä½“ç­‰ä»»åŠ¡ä¸­ï¼ŒOptiHiveæ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œåœ¨æœ€å¤æ‚çš„é—®é¢˜ä¸Šå°†æœ€ä¼˜ç‡ä»5%æé«˜åˆ°92%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02503v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLM-basedæ±‚è§£å™¨åœ¨è‡ªåŠ¨åŒ–é—®é¢˜å»ºæ¨¡å’Œæ±‚è§£æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†ä»å­˜åœ¨å¯é æ€§å’Œæ•ˆç‡é—®é¢˜ã€‚OptiHiveæ¡†æ¶åº”è¿è€Œç”Ÿï¼Œå¯ä»è‡ªç„¶è¯­è¨€æè¿°ä¸­ç”Ÿæˆé«˜è´¨é‡æ±‚è§£å™¨ï¼Œæ— éœ€è¿­ä»£è‡ªæˆ‘ä¿®æ­£ã€‚é€šè¿‡ä¸€æ¬¡æ€§æ‰¹é‡æŸ¥è¯¢LLMï¼Œç”Ÿæˆå¤šæ ·åŒ–ç»„ä»¶å¹¶è¿‡æ»¤é”™è¯¯ç»„ä»¶ï¼Œç¡®ä¿è¾“å‡ºå®Œå…¨å¯è§£é‡Šã€‚è€ƒè™‘åˆ°ç”Ÿæˆç»„ä»¶çš„ä¸å®Œç¾æ€§ï¼Œé‡‡ç”¨ç»Ÿè®¡æ¨¡å‹æ¨æ–­å…¶çœŸå®æ€§èƒ½ï¼Œå®ç°æœ‰åŸåˆ™çš„ä¸ç¡®å®šæ€§é‡åŒ–å’Œæ±‚è§£å™¨é€‰æ‹©ã€‚åœ¨ä»ä¼ ç»Ÿä¼˜åŒ–é—®é¢˜åˆ°å¤šä»“åº“è½¦è¾†è·¯å¾„é—®é¢˜çš„å„ç§ä»»åŠ¡ä¸­ï¼ŒOptiHiveæ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œåœ¨å¤æ‚é—®é¢˜ä¸Šå°†æœ€ä¼˜ç‡ä»5%æå‡è‡³92%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-basedæ±‚è§£å™¨åœ¨è‡ªåŠ¨åŒ–é—®é¢˜å»ºæ¨¡å’Œæ±‚è§£ä¸Šå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>OptiHiveæ¡†æ¶èƒ½å¤Ÿä»è‡ªç„¶è¯­è¨€æè¿°ä¸­ç”Ÿæˆé«˜è´¨é‡æ±‚è§£å™¨ï¼Œæ— éœ€è¿­ä»£è‡ªæˆ‘ä¿®æ­£ã€‚</li>
<li>OptiHiveé€šè¿‡ä¸€æ¬¡æ€§æ‰¹é‡æŸ¥è¯¢LLMç”Ÿæˆå¤šæ ·åŒ–ç»„ä»¶ï¼Œæé«˜äº†æ•ˆç‡ã€‚</li>
<li>OptiHiveèƒ½å¤Ÿè¿‡æ»¤é”™è¯¯ç»„ä»¶ï¼Œç¡®ä¿è¾“å‡ºå®Œå…¨å¯è§£é‡Šã€‚</li>
<li>ç”Ÿæˆç»„ä»¶å­˜åœ¨ä¸å®Œç¾æ€§ï¼Œé‡‡ç”¨ç»Ÿè®¡æ¨¡å‹æ¨æ–­å…¶çœŸå®æ€§èƒ½ã€‚</li>
<li>OptiHiveå®ç°äº†æœ‰åŸåˆ™çš„ä¸ç¡®å®šæ€§é‡åŒ–ï¼Œæœ‰åŠ©äºæ±‚è§£å™¨é€‰æ‹©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02503">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a0f6b226a943df314097878e896d12e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4419322f15b6d5f584e2d6349a420ee9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a1ca9171d808b9a5e2a901071a316ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35e7d6f740610ead318746206df0c675.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="The-SMeL-Test-A-simple-benchmark-for-media-literacy-in-language-models"><a href="#The-SMeL-Test-A-simple-benchmark-for-media-literacy-in-language-models" class="headerlink" title="The SMeL Test: A simple benchmark for media literacy in language models"></a>The SMeL Test: A simple benchmark for media literacy in language models</h2><p><strong>Authors:Gustaf Ahdritz, Anat Kleiman</strong></p>
<p>The internet is rife with unattributed, deliberately misleading, or otherwise untrustworthy content. Though large language models (LLMs) are often tasked with autonomous web browsing, the extent to which they have learned the simple heuristics human researchers use to navigate this noisy environment is not currently known. In this paper, we introduce the Synthetic Media Literacy Test (SMeL Test), a minimal benchmark that tests the ability of language models to actively filter out untrustworthy information in context. We benchmark a variety of commonly used instruction-tuned LLMs, including reasoning models, and find that no model consistently trusts more reliable sources; while reasoning in particular is associated with higher scores, even the best API model we test hallucinates up to 70% of the time. Remarkably, larger and more capable models do not necessarily outperform their smaller counterparts. We hope our work sheds more light on this important form of hallucination and guides the development of new methods to combat it. </p>
<blockquote>
<p>äº’è”ç½‘å……æ–¥ç€æœªç»è¯å®ã€æ•…æ„è¯¯å¯¼æˆ–å…¶ä»–ä¸å¯ä¿¡èµ–çš„å†…å®¹ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸è¢«èµ‹äºˆè‡ªä¸»æµè§ˆç½‘é¡µçš„ä»»åŠ¡ï¼Œä½†å®ƒä»¬æ˜¯å¦å·²ç»å­¦ä¼šäº†äººç±»ç ”ç©¶è€…ç”¨æ¥æµè§ˆè¿™ç§å˜ˆæ‚ç¯å¢ƒçš„ç®€å•å¯å‘å¼æŠ€æœ¯ï¼Œç›®å‰å°šä¸æ¸…æ¥šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åˆæˆåª’ä½“ç´ å…»æµ‹è¯•ï¼ˆSMeLæµ‹è¯•ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ€å°çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºæµ‹è¯•è¯­è¨€æ¨¡å‹åœ¨æƒ…å¢ƒä¸­ä¸»åŠ¨è¿‡æ»¤æ‰ä¸å¯é ä¿¡æ¯çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹å„ç§å¸¸ç”¨çš„æŒ‡ä»¤è°ƒæ•´å‹LLMè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬æ¨ç†æ¨¡å‹ï¼Œå¹¶å‘ç°æ²¡æœ‰ä»»ä½•æ¨¡å‹èƒ½å§‹ç»ˆä¿¡ä»»æ›´å¯é çš„æ¥æºï¼›è™½ç„¶æ¨ç†ä¸æ›´é«˜çš„åˆ†æ•°ç‰¹åˆ«ç›¸å…³ï¼Œä½†å³ä½¿æ˜¯æˆ‘ä»¬æµ‹è¯•çš„æœ€å¥½çš„APIæ¨¡å‹ï¼Œä¹Ÿæœ‰é«˜è¾¾70%çš„æ—¶é—´ä¼šå‡ºç°å¹»è§‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ›´å¤§ã€æ›´å…ˆè¿›çš„æ¨¡å‹å¹¶ä¸ä¸€å®šæ¯”å°å‹æ¨¡å‹è¡¨ç°æ›´å¥½ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œèƒ½è¿›ä¸€æ­¥æ­ç¤ºè¿™ç§é‡è¦çš„å¹»è§‰å½¢å¼ï¼Œå¹¶ä¸ºå¼€å‘æ–°çš„å¯¹æŠ—æ–¹æ³•æä¾›æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02074v1">PDF</a> </p>
<p><strong>Summary</strong><br>äº’è”ç½‘å……æ–¥ç€å¤§é‡æœªç»æˆæƒã€æ•…æ„è¯¯å¯¼æˆ–å…¶ä»–ä¸å¯é çš„å†…å®¹ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»å¸¸ç”¨äºè‡ªåŠ¨æµè§ˆç½‘é¡µï¼Œä½†å®ƒä»¬æ˜¯å¦æŒæ¡äº†äººç±»ç ”ç©¶è€…ç”¨æ¥åº”å¯¹è¿™ç§å™ªå£°ç¯å¢ƒçš„ç®€å•å¯å‘å¼æ–¹æ³•å°šä¸æ¸…æ¥šã€‚æœ¬æ–‡ä»‹ç»äº†åˆæˆåª’ä½“ç´ å…»æµ‹è¯•ï¼ˆSMeLæµ‹è¯•ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ€å°çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºæµ‹è¯•è¯­è¨€æ¨¡å‹åœ¨æƒ…å¢ƒä¸­ä¸»åŠ¨è¿‡æ»¤ä¸å¯é ä¿¡æ¯çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹ä¸€ç³»åˆ—å¸¸ç”¨çš„æŒ‡ä»¤è°ƒæ•´LLMè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬æ¨ç†æ¨¡å‹ï¼Œå‘ç°æ²¡æœ‰ä»»ä½•æ¨¡å‹å§‹ç»ˆä¿¡ä»»æ›´å¯é çš„æ¥æºï¼›å°¤å…¶æ˜¯æ¨ç†ä¸æ›´é«˜çš„åˆ†æ•°æœ‰å…³ï¼Œä½†å³ä½¿æˆ‘ä»¬æµ‹è¯•çš„æœ€ä½³APIæ¨¡å‹ä¹Ÿæœ‰é«˜è¾¾70%çš„å¹»æƒ³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ›´å¤§çš„ã€æ›´å¼ºå¤§çš„æ¨¡å‹å¹¶ä¸ä¸€å®šä¼˜äºè¾ƒå°çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œèƒ½æ›´å¤šåœ°æ­ç¤ºè¿™ç§å¹»è§‰çš„å½¢å¼ï¼Œå¹¶ä¸ºå¼€å‘æ–°çš„å¯¹æŠ—æ–¹æ³•æä¾›æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨äº’è”ç½‘ä¿¡æ¯ç­›é€‰æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>åˆæˆåª’ä½“ç´ å…»æµ‹è¯•ï¼ˆSMeLæµ‹è¯•ï¼‰ç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹è¿‡æ»¤ä¸å¯é ä¿¡æ¯çš„èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«å¯é ä¿¡æ¯æ¥æºæ–¹é¢è¡¨ç°ä¸ä¸€è‡´ã€‚</li>
<li>æ¨ç†èƒ½åŠ›è¾ƒå¼ºçš„è¯­è¨€æ¨¡å‹å¾—åˆ†è¾ƒé«˜ï¼Œä½†ä»å­˜åœ¨é«˜è¾¾70%çš„å¹»è§‰æƒ…å†µã€‚</li>
<li>æ¨¡å‹å¤§å°ä¸å…¶æ€§èƒ½ä¹‹é—´ä¸å­˜åœ¨å¿…ç„¶è”ç³»ã€‚</li>
<li>äº’è”ç½‘ä¸Šçš„ä¿¡æ¯ç»å¸¸æœªç»æˆæƒã€è¯¯å¯¼æˆ–ä¸å¯é ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-875443754a2917e78567871a9a18aa6c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="â€œEnergonâ€-Unveiling-Transformers-from-GPU-Power-and-Thermal-Side-Channels"><a href="#â€œEnergonâ€-Unveiling-Transformers-from-GPU-Power-and-Thermal-Side-Channels" class="headerlink" title="â€œEnergonâ€: Unveiling Transformers from GPU Power and Thermal   Side-Channels"></a>â€œEnergonâ€: Unveiling Transformers from GPU Power and Thermal   Side-Channels</h2><p><strong>Authors:Arunava Chaudhuri, Shubhi Shukla, Sarani Bhattacharya, Debdeep Mukhopadhyay</strong></p>
<p>Transformers have become the backbone of many Machine Learning (ML) applications, including language translation, summarization, and computer vision. As these models are increasingly deployed in shared Graphics Processing Unit (GPU) environments via Machine Learning as a Service (MLaaS), concerns around their security grow. In particular, the risk of side-channel attacks that reveal architectural details without physical access remains underexplored, despite the high value of the proprietary models they target. This work to the best of our knowledge is the first to investigate GPU power and thermal fluctuations as side-channels and further exploit them to extract information from pre-trained transformer models. The proposed analysis shows how these side channels can be exploited at user-privilege to reveal critical architectural details such as encoder&#x2F;decoder layer and attention head for both language and vision transformers. We demonstrate the practical impact by evaluating multiple language and vision pre-trained transformers which are publicly available. Through extensive experimental evaluations, we demonstrate that the attack model achieves a high accuracy of over 89% on average for model family identification and 100% for hyperparameter classification, in both single-process as well as noisy multi-process scenarios. Moreover, by leveraging the extracted architectural information, we demonstrate highly effective black-box transfer adversarial attacks with an average success rate exceeding 93%, underscoring the security risks posed by GPU side-channel leakage in deployed transformer models. </p>
<blockquote>
<p>Transformerå·²æˆä¸ºè®¸å¤šæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰åº”ç”¨çš„æ ¸å¿ƒï¼ŒåŒ…æ‹¬è¯­è¨€ç¿»è¯‘ã€æ‘˜è¦å’Œè®¡ç®—æœºè§†è§‰ã€‚éšç€è¿™äº›æ¨¡å‹è¶Šæ¥è¶Šå¤šåœ°é€šè¿‡æœºå™¨å­¦ä¹ æœåŠ¡ï¼ˆMLaaSï¼‰åœ¨å…±äº«å›¾å½¢å¤„ç†å•å…ƒï¼ˆGPUï¼‰ç¯å¢ƒä¸­éƒ¨ç½²ï¼Œäººä»¬å¯¹å…¶å®‰å…¨æ€§è¶Šæ¥è¶Šæ‹…å¿§ã€‚ç‰¹åˆ«æ˜¯ï¼Œå°½ç®¡é’ˆå¯¹çš„ç›®æ ‡æ¨¡å‹å…·æœ‰å¾ˆé«˜çš„ä¸“æœ‰ä»·å€¼ï¼Œä½†å…³äºåœ¨æ²¡æœ‰ç‰©ç†è®¿é—®çš„æƒ…å†µä¸‹æ­ç¤ºæ¶æ„ç»†èŠ‚çš„è¾¹ä¿¡é“æ”»å‡»é£é™©ä»ç„¶è¢«ä½ä¼°ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œé¦–æ¬¡ç ”ç©¶äº†GPUåŠŸç‡å’Œçƒ­æ³¢åŠ¨ä½œä¸ºè¾¹ä¿¡é“ï¼Œå¹¶è¿›ä¸€æ­¥åˆ©ç”¨å®ƒä»¬ä»é¢„è®­ç»ƒçš„Transformeræ¨¡å‹ä¸­æå–ä¿¡æ¯ã€‚æå‡ºçš„åˆ†æè¡¨æ˜ï¼Œå¦‚ä½•åˆ©ç”¨è¿™äº›è¾¹ä¿¡é“åœ¨ç”¨æˆ·ç‰¹æƒä¸‹æ­ç¤ºå…³é”®æ¶æ„ç»†èŠ‚ï¼Œå¦‚è¯­è¨€å’Œè§†è§‰Transformerçš„ç¼–ç å™¨&#x2F;è§£ç å™¨å±‚å’Œæ³¨æ„åŠ›å¤´ã€‚æˆ‘ä»¬é€šè¿‡è¯„ä¼°å¤šä¸ªå…¬å¼€å¯ç”¨çš„è¯­è¨€å’Œè§†è§‰é¢„è®­ç»ƒTransformeræ¥å±•ç¤ºå…¶å®è·µå½±å“ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒè¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜æ”»å‡»æ¨¡å‹åœ¨æ¨¡å‹å®¶æ—è¯†åˆ«æ–¹é¢å¹³å‡å‡†ç¡®ç‡è¶…è¿‡89%ï¼Œåœ¨è¶…å‚æ•°åˆ†ç±»æ–¹é¢è¾¾åˆ°100%ï¼Œæ— è®ºæ˜¯åœ¨å•è¿›ç¨‹è¿˜æ˜¯åœ¨å˜ˆæ‚çš„å¤šè¿›ç¨‹åœºæ™¯ä¸­éƒ½æ˜¯å¦‚æ­¤ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ©ç”¨æå–çš„æ¶æ„ä¿¡æ¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é«˜åº¦æœ‰æ•ˆçš„é»‘ç›’è½¬ç§»å¯¹æŠ—æ€§æ”»å‡»ï¼Œå¹³å‡æˆåŠŸç‡è¶…è¿‡93%ï¼Œè¿™çªæ˜¾äº†éƒ¨ç½²çš„Transformeræ¨¡å‹ä¸­GPUè¾¹ä¿¡é“æ³„æ¼æ‰€å¸¦æ¥çš„å®‰å…¨é£é™©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01768v1">PDF</a> Accepted at IEEE&#x2F;ACM International Conference on Computer-Aided   Design, 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡é¦–æ¬¡æ¢ç´¢åˆ©ç”¨GPUåŠŸè€—å’Œçƒ­æ³¢åŠ¨ä½œä¸ºä¾§é€šé“æ”»å‡»çš„æ–¹å¼ï¼Œä»¥æ­¤ä»é¢„è®­ç»ƒçš„transformeræ¨¡å‹ä¸­æå–ä¿¡æ¯ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™ç§ä¾§é€šé“æ”»å‡»å¯åœ¨ç”¨æˆ·æƒé™çº§åˆ«ä¸‹æ­ç¤ºè¯­è¨€å’Œè§†è§‰transformerçš„ç¼–ç å™¨&#x2F;è§£ç å™¨å±‚å’Œæ³¨æ„åŠ›å¤´ç­‰å…³é”®æ¶æ„ç»†èŠ‚ã€‚æ”»å‡»æ¨¡å‹çš„å‡†ç¡®ç‡é«˜è¾¾89%ä»¥ä¸Šï¼Œåœ¨å•è¿›ç¨‹å’Œå¤šè¿›ç¨‹å˜ˆæ‚åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºé«˜æ•ˆçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡æå–æ¶æ„ä¿¡æ¯è¿›è¡Œçš„é»‘ç®±è½¬ç§»å¯¹æŠ—æ€§æ”»å‡»çš„æˆåŠŸç‡å¹³å‡è¶…è¿‡93%ï¼Œå‡¸æ˜¾å‡ºéƒ¨ç½²çš„transformeræ¨¡å‹é¢ä¸´çš„GPUä¾§é€šé“æ³„æ¼é£é™©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é¦–æ¬¡ç ”ç©¶GPUåŠŸè€—å’Œçƒ­æ³¢åŠ¨ä½œä¸ºä¾§é€šé“æ”»å‡»åœ¨MLaaSä¸­çš„é¢„è®­ç»ƒTransformeræ¨¡å‹ã€‚</li>
<li>ä¾§é€šé“æ”»å‡»å¯æ­ç¤ºè¯­è¨€å’Œè§†è§‰Transformerçš„å…³é”®æ¶æ„ç»†èŠ‚ï¼Œå¦‚ç¼–ç å™¨&#x2F;è§£ç å™¨å±‚å’Œæ³¨æ„åŠ›å¤´ã€‚</li>
<li>æ”»å‡»æ¨¡å‹åœ¨æ¨¡å‹å®¶æ—è¯†åˆ«å’Œè¶…å‚æ•°åˆ†ç±»æ–¹é¢çš„å‡†ç¡®ç‡è¶…è¿‡89%ã€‚</li>
<li>ä¾§é€šé“æ”»å‡»å¯¹å•è¿›ç¨‹å’Œå¤šè¿›ç¨‹åœºæ™¯å‡æœ‰æ•ˆã€‚</li>
<li>åˆ©ç”¨æå–çš„æ¶æ„ä¿¡æ¯è¿›è¡Œé»‘ç®±è½¬ç§»å¯¹æŠ—æ€§æ”»å‡»ï¼ŒæˆåŠŸç‡è¶…è¿‡93%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01768">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-49bc192f6e55bb329f03a3a6d599d9c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-692fcf647275d94e44aa122fcd2b5540.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-12188dc7b105502fc8a4f13230f6821c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1bbeaa1a6f474106ce3e1e80de5e9ceb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1b65eb4ab7a0e8a314918c42da8c0f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-246a65542610025052f6bef65bc4ba19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d61588a492f1c08da10f1318cfdf37e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84344a33f2e30705deec8eabe031f46a.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Semantic-Encryption-Secure-and-Effective-Interaction-with-Cloud-based-Large-Language-Models-via-Semantic-Transformation"><a href="#Semantic-Encryption-Secure-and-Effective-Interaction-with-Cloud-based-Large-Language-Models-via-Semantic-Transformation" class="headerlink" title="Semantic Encryption: Secure and Effective Interaction with Cloud-based   Large Language Models via Semantic Transformation"></a>Semantic Encryption: Secure and Effective Interaction with Cloud-based   Large Language Models via Semantic Transformation</h2><p><strong>Authors:Dong Chen, Tong Yang, Feipeng Zhai, Pengpeng Ouyang, Qidong Liu, Yafei Li, Chong Fu, Mingliang Xu</strong></p>
<p>The increasing adoption of Cloud-based Large Language Models (CLLMs) has raised significant concerns regarding data privacy during user interactions. While existing approaches primarily focus on encrypting sensitive information, they often overlook the logical structure of user inputs. This oversight can lead to reduced data utility and degraded performance of CLLMs. To address these limitations and enable secure yet effective interactions, we propose Semantic Encryption (SE)-a plug-and-play framework designed to preserve both privacy and utility. SE consists of two key components: Semantic Encoding and Semantic Decoding. In the encoding phase, a lightweight local model transforms the original user input into an alternative semantic context that maintains the original intent and logical structure while obfuscating sensitive information. This transformed input is then processed by the CLLM, which generates a response based on the transformed semantic context. To maintain a seamless user experience, the decoding phase will reconstruct the CLLMâ€™s response back into the original semantic context by referencing the locally stored user input. Extensive experimental evaluations demonstrate that SE effectively protects data privacy without compromising data utility or user experience, offering a practical solution for secure interaction with CLLMs. Particularly, the proposed SE demonstrates a significant improvement over the state-of-the-art InferDPT, surpassing it across various evaluated metrics and datasets. </p>
<blockquote>
<p>éšç€åŸºäºäº‘çš„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆCLLMsï¼‰çš„æ—¥ç›Šæ™®åŠï¼Œç”¨æˆ·äº¤äº’è¿‡ç¨‹ä¸­çš„æ•°æ®éšç§é—®é¢˜å¼•å‘äº†é‡å¤§å…³æ³¨ã€‚è™½ç„¶ç°æœ‰çš„æ–¹æ³•ä¸»è¦å…³æ³¨åŠ å¯†æ•æ„Ÿä¿¡æ¯ï¼Œä½†å®ƒä»¬å¾€å¾€ä¼šå¿½ç•¥ç”¨æˆ·è¾“å…¥çš„é€»è¾‘ç»“æ„ã€‚è¿™ç§ç–å¿½å¯èƒ½å¯¼è‡´æ•°æ®æ•ˆç”¨é™ä½å’ŒCLLMæ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶å¹¶å®ç°å®‰å…¨è€Œæœ‰æ•ˆçš„äº¤äº’ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­ä¹‰åŠ å¯†ï¼ˆSEï¼‰â€”â€”ä¸€ä¸ªå³æ’å³ç”¨æ¡†æ¶ï¼Œæ—¨åœ¨ä¿æŠ¤éšç§å’Œæ•ˆç”¨ã€‚SEç”±ä¸¤ä¸ªå…³é”®ç»„ä»¶æ„æˆï¼šè¯­ä¹‰ç¼–ç å’Œè¯­ä¹‰è§£ç ã€‚åœ¨ç¼–ç é˜¶æ®µï¼Œä¸€ä¸ªè½»å‹çš„æœ¬åœ°æ¨¡å‹å°†åŸå§‹ç”¨æˆ·è¾“å…¥è½¬æ¢ä¸ºæ›¿ä»£çš„è¯­ä¹‰ä¸Šä¸‹æ–‡ï¼Œè¯¥è½¬æ¢ä¿æŒäº†åŸå§‹æ„å›¾å’Œé€»è¾‘ç»“æ„ï¼ŒåŒæ—¶æ©ç›–äº†æ•æ„Ÿä¿¡æ¯ã€‚ç„¶åï¼Œè¿™ä¸ªè½¬æ¢åçš„è¾“å…¥è¢«CLLMå¤„ç†ï¼ŒåŸºäºè½¬æ¢åçš„è¯­ä¹‰ä¸Šä¸‹æ–‡ç”Ÿæˆå“åº”ã€‚ä¸ºäº†ä¿æŒæ— ç¼çš„ç”¨æˆ·ä½“éªŒï¼Œè§£ç é˜¶æ®µå°†é€šè¿‡å¼•ç”¨æœ¬åœ°å­˜å‚¨çš„ç”¨æˆ·è¾“å…¥å°†CLLMçš„å“åº”é‡æ„å›åŸå§‹è¯­ä¹‰ä¸Šä¸‹æ–‡ã€‚å¹¿æ³›çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒSEåœ¨æœ‰æ•ˆä¿æŠ¤æ•°æ®éšç§çš„åŒæ—¶ï¼Œä¸æŸå®³æ•°æ®æ•ˆç”¨æˆ–ç”¨æˆ·ä½“éªŒï¼Œä¸ºä¸CLLMçš„å®‰å…¨äº¤äº’æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚ç‰¹åˆ«æ˜¯ï¼Œæ‰€æå‡ºçš„SEåœ¨å…ˆè¿›æŒ‡æ ‡å’Œæ•°æ®é›†ä¸Šæ˜¾è‘—æ”¹è¿›äº†InferDPTï¼Œè¶…è¶Šäº†å®ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01638v1">PDF</a> </p>
<p><strong>Summary</strong><br>äº‘æœåŠ¡ä¸­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆCLLMsï¼‰åœ¨äº¤äº’è¿‡ç¨‹ä¸­å¼•å‘æ•°æ®éšç§å…³æ³¨ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ•æ„Ÿä¿¡æ¯çš„åŠ å¯†ï¼Œä½†å¿½ç•¥äº†ç”¨æˆ·è¾“å…¥çš„å†…åœ¨é€»è¾‘ç»“æ„ï¼Œå¯èƒ½å¯¼è‡´æ•°æ®æ•ˆç”¨é™ä½å’ŒCLLMæ€§èƒ½ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºè¯­ä¹‰åŠ å¯†ï¼ˆSEï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨ä¿æŠ¤éšç§å’Œæé«˜å®ç”¨æ€§ã€‚å®ƒåŒ…å«è¯­ä¹‰ç¼–ç å’Œè¯­ä¹‰è§£ç ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼Œèƒ½ç»´æŒç”¨æˆ·åŸå§‹æ„å›¾å’Œé€»è¾‘ç»“æ„çš„åŒæ—¶ï¼Œéšè—æ•æ„Ÿä¿¡æ¯å¹¶è§£ç å›åº”ï¼Œä¿éšœç”¨æˆ·ä½“éªŒã€‚å®éªŒè¯æ˜ï¼ŒSEåœ¨ä¿æŠ¤æ•°æ®éšç§æ–¹é¢æ•ˆæœæ˜¾è‘—ï¼Œä¸”ä¸å½±å“æ•°æ®æ•ˆç”¨å’Œç”¨æˆ·ä½“éªŒï¼Œä¸ºä¸CLLMçš„å®‰å…¨äº¤äº’æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº‘æœåŠ¡ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆCLLMsï¼‰åœ¨äº¤äº’è¿‡ç¨‹ä¸­å­˜åœ¨æ•°æ®éšç§å…³åˆ‡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨æ•æ„Ÿä¿¡æ¯åŠ å¯†ï¼Œä½†å¿½ç•¥ç”¨æˆ·è¾“å…¥çš„é€»è¾‘ç»“æ„ã€‚</li>
<li>è¯­ä¹‰åŠ å¯†ï¼ˆSEï¼‰æ¡†æ¶æ—¨åœ¨åŒæ—¶ä¿æŠ¤éšç§å’Œæé«˜å®ç”¨æ€§ã€‚</li>
<li>SEåŒ…å«è¯­ä¹‰ç¼–ç å’Œè¯­ä¹‰è§£ç ä¸¤ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>è¯­ä¹‰ç¼–ç èƒ½ç»´æŒç”¨æˆ·åŸå§‹æ„å›¾å’Œé€»è¾‘ç»“æ„ï¼ŒåŒæ—¶éšè—æ•æ„Ÿä¿¡æ¯ã€‚</li>
<li>è¯­ä¹‰è§£ç èƒ½ä¿éšœCLLMçš„å›åº”ä¸åŸå§‹è¯­ä¹‰ä¸Šä¸‹æ–‡ä¸€è‡´ï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01638">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7998337db5a4317ba8621db6793d7962.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf52800f939c0787f276d659bdf70b38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1411b255b3bf6de35ff73771b170eb0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a3057d37792266355bec588cab0fc21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a5e7c41264169f9576dcb9954694886.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="OpenMed-NER-Open-Source-Domain-Adapted-State-of-the-Art-Transformers-for-Biomedical-NER-Across-12-Public-Datasets"><a href="#OpenMed-NER-Open-Source-Domain-Adapted-State-of-the-Art-Transformers-for-Biomedical-NER-Across-12-Public-Datasets" class="headerlink" title="OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers   for Biomedical NER Across 12 Public Datasets"></a>OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers   for Biomedical NER Across 12 Public Datasets</h2><p><strong>Authors:Maziyar Panahi</strong></p>
<p>Named-entity recognition (NER) is fundamental to extracting structured information from the &gt;80% of healthcare data that resides in unstructured clinical notes and biomedical literature. Despite recent advances with large language models, achieving state-of-the-art performance across diverse entity types while maintaining computational efficiency remains a significant challenge. We introduce OpenMed NER, a suite of open-source, domain-adapted transformer models that combine lightweight domain-adaptive pre-training (DAPT) with parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs cost-effective DAPT on a 350k-passage corpus compiled from ethically sourced, publicly available research repositories and de-identified clinical notes (PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA backbones. This is followed by task-specific fine-tuning with LoRA, which updates less than 1.5% of model parameters. We evaluate our models on 12 established biomedical NER benchmarks spanning chemicals, diseases, genes, and species. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of these 12 datasets, with substantial gains across diverse entity types. Our models advance the state-of-the-art on foundational disease and chemical benchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger improvements of over 5.3 and 9.7 percentage points on more specialized gene and clinical cell line corpora. This work demonstrates that strategically adapted open-source models can surpass closed-source solutions. This performance is achieved with remarkable efficiency: training completes in under 12 hours on a single GPU with a low carbon footprint (&lt; 1.2 kg CO2e), producing permissively licensed, open-source checkpoints designed to help practitioners facilitate compliance with emerging data protection and AI regulations, such as the EU AI Act. </p>
<blockquote>
<p>å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æ˜¯ä»è¶…è¿‡80%çš„é©»ç•™åœ¨éç»“æ„åŒ–ä¸´åºŠç¬”è®°å’Œç”Ÿç‰©åŒ»å­¦æ–‡çŒ®ä¸­çš„åŒ»ç–—æ•°æ®ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯çš„åŸºç¡€ã€‚å°½ç®¡æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹æœ‰æ‰€è¿›å±•ï¼Œä½†åœ¨ç»´æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œå®ç°è·¨å¤šç§å®ä½“ç±»å‹çš„æœ€å…ˆè¿›çš„æ€§èƒ½ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†OpenMed NERï¼Œè¿™æ˜¯ä¸€ç³»åˆ—å¼€æºçš„ã€é’ˆå¯¹é¢†åŸŸè¿›è¡Œé€‚åº”çš„è½¬æ¢å™¨æ¨¡å‹ï¼Œå®ƒç»“åˆäº†è½»é‡çº§çš„é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼ˆDAPTï¼‰å’Œå‚æ•°é«˜æ•ˆçš„ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥æˆæœ¬æ•ˆç›Šçš„æ–¹å¼åœ¨ç”±ä¼¦ç†æ¥æºçš„ã€å…¬å¼€å¯ç”¨çš„ç ”ç©¶ä»“åº“å’Œå»æ ‡è¯†çš„ä¸´åºŠç¬”è®°ï¼ˆPubMedã€arXivå’ŒMIMIC-IIIï¼‰ç¼–è¯‘çš„350kæ®µè½è¯­æ–™åº“ä¸Šè¿›è¡ŒDAPTï¼Œä½¿ç”¨DeBERTa-v3ã€PubMedBERTå’ŒBioELECTRAä½œä¸ºéª¨å¹²ã€‚æ¥ç€é€šè¿‡LoRAè¿›è¡Œé’ˆå¯¹ä»»åŠ¡çš„å¾®è°ƒï¼ŒLoRAæ›´æ–°çš„æ¨¡å‹å‚æ•°å°‘äº1.5%ã€‚æˆ‘ä»¬åœ¨åŒ…å«åŒ–å­¦ã€ç–¾ç—…ã€åŸºå› å’Œç‰©ç§çš„12ä¸ªæˆç†Ÿçš„ç”Ÿç‰©åŒ»å­¦NERåŸºå‡†æµ‹è¯•é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¨¡å‹ã€‚OpenMed NERåœ¨è¿™12ä¸ªæ•°æ®é›†çš„10ä¸ªä¸Šå–å¾—äº†æ–°çš„æœ€å…ˆè¿›çš„å¾®è§‚F1åˆ†æ•°ï¼Œåœ¨å¤šç§å®ä½“ç±»å‹ä¸Šå®ç°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨åŸºç¡€ç–¾ç—…å’ŒåŒ–å­¦åŸºå‡†æµ‹è¯•é›†ä¸Šå–å¾—äº†å…ˆè¿›çš„è¡¨ç°ï¼ˆä¾‹å¦‚ï¼ŒBC5CDR-Diseaseï¼Œ+2.7 ppï¼‰ï¼ŒåŒæ—¶åœ¨æ›´ä¸“ä¸šçš„åŸºå› å’Œä¸´åºŠç»†èƒæ ªè¯­æ–™åº“ä¸Šå®ç°äº†è¶…è¿‡5.3å’Œ9.7ä¸ªç™¾åˆ†ç‚¹çš„æ”¹è¿›ã€‚è¿™é¡¹å·¥ä½œè¯æ˜ï¼Œç»è¿‡æˆ˜ç•¥é€‚åº”çš„å¼€æºæ¨¡å‹å¯ä»¥è¶…è¶Šé—­æºè§£å†³æ–¹æ¡ˆã€‚è¿™ç§æ€§èƒ½çš„å®ç°å…·æœ‰æƒŠäººçš„æ•ˆç‡ï¼šè®­ç»ƒåœ¨å•ä¸ªGPUä¸Šä¸åˆ°12å°æ—¶å†…å®Œæˆï¼Œä½ç¢³è¶³è¿¹ä½äº1.2å…¬æ–¤äºŒæ°§åŒ–ç¢³å½“é‡ï¼Œäº§ç”Ÿå¼€æ”¾æºä»£ç çš„è®¸å¯æ£€æŸ¥ç‚¹ï¼Œæ—¨åœ¨å¸®åŠ©ä»ä¸šè€…ç¬¦åˆæ–°å…´çš„æ•°æ®ä¿æŠ¤å’Œäººå·¥æ™ºèƒ½æ³•è§„ï¼Œå¦‚æ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01630v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¼€æ”¾åŒ»ç–—å‘½åå®ä½“è¯†åˆ«ï¼ˆOpenMed NERï¼‰æ˜¯ä¸€å¥—å¼€æºçš„ã€é€‚åº”é¢†åŸŸçš„å˜å‹å™¨æ¨¡å‹ï¼Œç»“åˆäº†è½»é‡çº§é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼ˆDAPTï¼‰å’Œå‚æ•°é«˜æ•ˆçš„ä½é˜¶é€‚åº”ï¼ˆLoRAï¼‰ã€‚è¯¥æ¨¡å‹åœ¨ä¼¦ç†æ¥æºçš„å…¬å¼€ç ”ç©¶èµ„æºåº“ã€å»æ ‡è¯†åŒ–çš„ä¸´åºŠç¬”è®°ï¼ˆPubMedã€arXivå’ŒMIMIC-IIIï¼‰çš„350kæ®µè½è¯­æ–™åº“ä¸Šè¿›è¡Œç»æµå®æƒ çš„DAPTï¼Œå¹¶ä½¿ç”¨DeBERTa-v3ã€PubMedBERTå’ŒBioELECTRAä½œä¸ºåŸºç¡€ã€‚é€šè¿‡ç‰¹å®šä»»åŠ¡å¾®è°ƒLoRAï¼Œæ›´æ–°å°‘äº1.5%çš„æ¨¡å‹å‚æ•°ã€‚åœ¨12ä¸ªå·²å»ºç«‹çš„ç”Ÿç‰©åŒ»å­¦å‘½åå®ä½“è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°ï¼ŒOpenMed NERåœ¨10ä¸ªæ•°æ®é›†ä¸Šå®ç°æœ€æ–°å¾®F1åˆ†æ•°ï¼Œå¹¶åœ¨ä¸åŒå®ä½“ç±»å‹ä¸Šå–å¾—æ˜¾è‘—æ”¶ç›Šã€‚æ­¤å·¥ä½œè¯æ˜æˆ˜ç•¥é€‚åº”çš„å¼€æºæ¨¡å‹å¯ä»¥è¶…è¶Šå°é—­æºä»£ç è§£å†³æ–¹æ¡ˆï¼Œä¸”è¡¨ç°é«˜æ•ˆï¼šåœ¨å•ä¸ªGPUä¸Šä¸åˆ°12å°æ—¶å†…å®Œæˆè®­ç»ƒï¼Œç¢³æ’æ”¾è¶³è¿¹å°ï¼ˆ&lt; 1.2 kg CO2eï¼‰ï¼Œæä¾›ç¬¦åˆæ–°å…´æ•°æ®ä¿æŠ¤å’Œäººå·¥æ™ºèƒ½æ³•è§„çš„å¼€æ”¾è®¸å¯æ£€æŸ¥ç‚¹ï¼Œå¦‚æ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenMed NERæ˜¯å¼€æºçš„ã€é€‚åº”é¢†åŸŸçš„å˜å‹å™¨æ¨¡å‹ï¼Œç”¨äºä»éç»“æ„åŒ–ä¸´åºŠç¬”è®°å’Œç”Ÿç‰©åŒ»å­¦æ–‡çŒ®ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ã€‚</li>
<li>ç»“åˆè½»é‡çº§é¢†åŸŸè‡ªé€‚åº”é¢„è®­ç»ƒï¼ˆDAPTï¼‰å’Œå‚æ•°é«˜æ•ˆçš„ä½é˜¶é€‚åº”ï¼ˆLoRAï¼‰ã€‚</li>
<li>åœ¨ä¼¦ç†æ¥æºçš„å…¬å¼€èµ„æºåº“å’Œå»æ ‡è¯†åŒ–çš„ä¸´åºŠç¬”è®°ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>åœ¨å¤šä¸ªç”Ÿç‰©åŒ»å­¦å‘½åå®ä½“è¯†åˆ«åŸºå‡†æµ‹è¯•ä¸­å®ç°æœ€æ–°å¾®F1åˆ†æ•°ã€‚</li>
<li>åœ¨å¤šç§å®ä½“ç±»å‹ä¸Šå–å¾—æ˜¾è‘—æˆæ•ˆï¼Œç‰¹åˆ«æ˜¯åœ¨æ›´ä¸“ä¸šçš„åŸºå› å’Œä¸´åºŠç»†èƒçº¿è¯­æ–™åº“ä¸Šã€‚</li>
<li>æˆ˜ç•¥é€‚åº”çš„å¼€æºæ¨¡å‹è¡¨ç°ä¼˜äºå°é—­æºä»£ç è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-727b6b61f4c341b37d6a53d69e86bcb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51df8fab053061697a8691bfbfd79bf4.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="LLaDA-MedV-Exploring-Large-Language-Diffusion-Models-for-Biomedical-Image-Understanding"><a href="#LLaDA-MedV-Exploring-Large-Language-Diffusion-Models-for-Biomedical-Image-Understanding" class="headerlink" title="LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical   Image Understanding"></a>LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical   Image Understanding</h2><p><strong>Authors:Xuanzhao Dong, Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Peijie Qiu, Shao Tang, Xin Li, Yalin Wang</strong></p>
<p>Autoregressive models (ARMs) have long dominated the landscape of biomedical vision-language models (VLMs). Recently, masked diffusion models such as LLaDA have emerged as promising alternatives, yet their application in the biomedical domain remains largely underexplored. To bridge this gap, we introduce \textbf{LLaDA-MedV}, the first large language diffusion model tailored for biomedical image understanding through vision instruction tuning. LLaDA-MedV achieves relative performance gains of 7.855% over LLaVA-Med and 1.867% over LLaDA-V in the open-ended biomedical visual conversation task, and sets new state-of-the-art accuracy on the closed-form subset of three VQA benchmarks: 84.93% on VQA-RAD, 92.31% on SLAKE, and 95.15% on PathVQA. Furthermore, a detailed comparison with LLaVA-Med suggests that LLaDA-MedV is capable of generating reasonably longer responses by explicitly controlling response length, which can lead to more informative outputs. We also conduct an in-depth analysis of both the training and inference stages, highlighting the critical roles of initialization weight selection, fine-tuning strategies, and the interplay between sampling steps and response repetition. The code and model weight is released at <a target="_blank" rel="noopener" href="https://github.com/LLM-VLM-GSL/LLaDA-MedV">https://github.com/LLM-VLM-GSL/LLaDA-MedV</a>. </p>
<blockquote>
<p>è‡ªå›å½’æ¨¡å‹ï¼ˆARMsï¼‰é•¿æœŸä»¥æ¥åœ¨ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é¢†åŸŸå æ®ä¸»å¯¼åœ°ä½ã€‚æœ€è¿‘ï¼Œå¦‚LLaDAä¹‹ç±»çš„æ©æ¨¡æ‰©æ•£æ¨¡å‹çš„å‡ºç°æ˜¾ç¤ºå‡ºå…¶ä½œä¸ºæœ‰å‰é€”çš„æ›¿ä»£å“çš„æ½œåŠ›ï¼Œç„¶è€Œå®ƒä»¬åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„åº”ç”¨ä»è¢«å¤§å¤§å¿½è§†ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒç†è§£è®¾è®¡çš„é¦–ä¸ªå¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹LLaDA-MedVï¼Œé€šè¿‡è§†è§‰æŒ‡ä»¤å¾®è°ƒæ¥å®ç°ã€‚LLaDA-MedVåœ¨å¼€æ”¾å¼çš„ç”Ÿç‰©åŒ»å­¦è§†è§‰å¯¹è¯ä»»åŠ¡ä¸­ç›¸å¯¹äºLLaVA-Medå’ŒLLaDA-Vçš„æ€§èƒ½åˆ†åˆ«æå‡äº†7.855%å’Œ1.867%ï¼Œå¹¶åœ¨ä¸‰ä¸ªVQAåŸºå‡†æµ‹è¯•é›†çš„å°é—­å¼å­é›†ä¸Šè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„å‡†ç¡®åº¦ï¼šåœ¨VQA-RADä¸Šè¾¾åˆ°84.93%ï¼Œåœ¨SLAKEä¸Šè¾¾åˆ°92.31%ï¼Œåœ¨PathVQAä¸Šè¾¾åˆ°95.15%ã€‚æ­¤å¤–ï¼Œä¸LLaVA-Medçš„è¯¦ç»†æ¯”è¾ƒè¡¨æ˜ï¼ŒLLaDA-MedVèƒ½å¤Ÿé€šè¿‡æ˜ç¡®æ§åˆ¶å“åº”é•¿åº¦æ¥ç”Ÿæˆç›¸å¯¹æ›´é•¿çš„å“åº”ï¼Œä»è€Œå¯èƒ½å¯¼è‡´æ›´å…·ä¿¡æ¯é‡çš„è¾“å‡ºã€‚æˆ‘ä»¬è¿˜å¯¹è®­ç»ƒå’Œæ¨ç†é˜¶æ®µè¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œå¼ºè°ƒäº†åˆå§‹åŒ–æƒé‡é€‰æ‹©ã€å¾®è°ƒç­–ç•¥ä»¥åŠé‡‡æ ·æ­¥éª¤å’Œå“åº”é‡å¤ä¹‹é—´çš„ç›¸äº’ä½œç”¨çš„å…³é”®ä½œç”¨ã€‚ä»£ç å’Œæ¨¡å‹æƒé‡å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/LLM-VLM-GSL/LLaDA-MedV%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/LLM-VLM-GSL/LLaDA-MedVå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01617v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šLLaDA-MedVæ˜¯ä¸€æ¬¾é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒç†è§£çš„è¯­è¨€æ‰©æ•£æ¨¡å‹ï¼Œå®ƒé€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´å®ç°æ€§èƒ½ä¼˜åŒ–ã€‚ç›¸è¾ƒäºå…¶ä»–æ¨¡å‹ï¼ŒLLaDA-MedVåœ¨å¼€æ”¾å’Œå°é—­å½¢å¼çš„ç”Ÿç‰©åŒ»å­¦è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä½³ï¼Œå¹¶åœ¨ä¸‰ä¸ªVQAåŸºå‡†æµ‹è¯•ä¸­åˆ›é€ äº†æ–°çš„å‡†ç¡®ç‡è®°å½•ã€‚åŒæ—¶ï¼Œå®ƒèƒ½å¤Ÿç”Ÿæˆè¾ƒé•¿çš„å“åº”å¹¶æ§åˆ¶å“åº”é•¿åº¦ï¼Œæä¾›æ›´ä¸°å¯Œçš„è¾“å‡ºä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¯¹è®­ç»ƒå’Œæ¨ç†é˜¶æ®µè¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œæ¢è®¨äº†åˆå§‹åŒ–æƒé‡é€‰æ‹©ã€å¾®è°ƒç­–ç•¥ä»¥åŠé‡‡æ ·æ­¥éª¤å’Œå“åº”é‡å¤ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚æ¨¡å‹ä»£ç å’Œæƒé‡å·²å‘å¸ƒåœ¨æŒ‡å®šé“¾æ¥ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLaDA-MedVæ˜¯ä¸“ä¸ºç”Ÿç‰©åŒ»å­¦å›¾åƒç†è§£è®¾è®¡çš„è¯­è¨€æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>LLaDA-MedVé€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´å®ç°æ€§èƒ½ä¼˜åŒ–ã€‚</li>
<li>LLaDA-MedVåœ¨ç”Ÿç‰©åŒ»å­¦è§†è§‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>LLaDA-MedVåœ¨ä¸‰ä¸ªVQAåŸºå‡†æµ‹è¯•ä¸­åˆ›é€ äº†æ–°çš„å‡†ç¡®ç‡è®°å½•ã€‚</li>
<li>LLaDA-MedVèƒ½å¤Ÿç”Ÿæˆè¾ƒé•¿çš„å“åº”å¹¶æ§åˆ¶å“åº”é•¿åº¦ï¼Œæä¾›æ›´ä¸°å¯Œçš„è¾“å‡ºä¿¡æ¯ã€‚</li>
<li>ç ”ç©¶å¯¹LLaDA-MedVçš„è®­ç»ƒå’Œæ¨ç†é˜¶æ®µè¿›è¡Œäº†æ·±å…¥åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-28f50ce6fbe5010fdd218a258d6057b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8cdde17c0c7023c6476ca6684b57542b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bc842df081978fe43604c02fe95adac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c9c5b7226586101902537bd928e01f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e3a50d503fd1515247626c76e4676af3.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Are-All-Prompt-Components-Value-Neutral-Understanding-the-Heterogeneous-Adversarial-Robustness-of-Dissected-Prompt-in-Large-Language-Models"><a href="#Are-All-Prompt-Components-Value-Neutral-Understanding-the-Heterogeneous-Adversarial-Robustness-of-Dissected-Prompt-in-Large-Language-Models" class="headerlink" title="Are All Prompt Components Value-Neutral? Understanding the Heterogeneous   Adversarial Robustness of Dissected Prompt in Large Language Models"></a>Are All Prompt Components Value-Neutral? Understanding the Heterogeneous   Adversarial Robustness of Dissected Prompt in Large Language Models</h2><p><strong>Authors:Yujia Zheng, Tianhao Li, Haotian Huang, Tianyu Zeng, Jingyu Lu, Chuangxin Chu, Yuekai Huang, Ziyou Jiang, Qian Xiong, Yuyao Ge, Mingyang Li</strong></p>
<p>Prompt-based adversarial attacks have become an effective means to assess the robustness of large language models (LLMs). However, existing approaches often treat prompts as monolithic text, overlooking their structural heterogeneity-different prompt components contribute unequally to adversarial robustness. Prior works like PromptRobust assume prompts are value-neutral, but our analysis reveals that complex, domain-specific prompts with rich structures have components with differing vulnerabilities. To address this gap, we introduce PromptAnatomy, an automated framework that dissects prompts into functional components and generates diverse, interpretable adversarial examples by selectively perturbing each component using our proposed method, ComPerturb. To ensure linguistic plausibility and mitigate distribution shifts, we further incorporate a perplexity (PPL)-based filtering mechanism. As a complementary resource, we annotate four public instruction-tuning datasets using the PromptAnatomy framework, verified through human review. Extensive experiments across these datasets and five advanced LLMs demonstrate that ComPerturb achieves state-of-the-art attack success rates. Ablation studies validate the complementary benefits of prompt dissection and PPL filtering. Our results underscore the importance of prompt structure awareness and controlled perturbation for reliable adversarial robustness evaluation in LLMs. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/Yujiaaaaa/PACP">https://github.com/Yujiaaaaa/PACP</a>. </p>
<blockquote>
<p>åŸºäºæç¤ºçš„å¯¹æŠ—æ€§æ”»å‡»å·²æˆä¸ºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç¨³å¥æ€§çš„æœ‰æ•ˆæ‰‹æ®µã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸å°†æç¤ºè§†ä¸ºå•ä¸€æ–‡æœ¬ï¼Œå¿½ç•¥äº†å…¶ç»“æ„å¼‚è´¨æ€§â€”â€”ä¸åŒçš„æç¤ºç»„ä»¶å¯¹å¯¹æŠ—ç¨³å¥æ€§çš„è´¡çŒ®æ˜¯ä¸å¹³ç­‰çš„ã€‚åƒPromptRobustè¿™æ ·çš„å…ˆå‰å·¥ä½œå‡è®¾æç¤ºæ˜¯ä»·å€¼ä¸­ç«‹çš„ï¼Œä½†æˆ‘ä»¬çš„åˆ†æå‘ç°ï¼Œå…·æœ‰ä¸°å¯Œç»“æ„å’Œç‰¹å®šé¢†åŸŸçš„å¤æ‚æç¤ºå…·æœ‰ä¸åŒæ¼æ´çš„ç»„ä»¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†PromptAnatomyï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œå®ƒå°†æç¤ºåˆ†è§£æˆåŠŸèƒ½ç»„ä»¶ï¼Œå¹¶é€šè¿‡æˆ‘ä»¬æå‡ºçš„ComPerturbæ–¹æ³•é€‰æ‹©æ€§åœ°æ‰°åŠ¨æ¯ä¸ªç»„ä»¶æ¥ç”Ÿæˆå¤šæ ·ä¸”å¯è§£é‡Šçš„å¯¹æŠ—ç¤ºä¾‹ã€‚ä¸ºäº†ç¡®ä¿è¯­è¨€ä¸Šçš„åˆç†æ€§å’Œç¼“è§£åˆ†å¸ƒåç§»ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥èå…¥äº†ä¸€ä¸ªåŸºäºå›°æƒ‘åº¦ï¼ˆPPLï¼‰çš„è¿‡æ»¤æœºåˆ¶ã€‚ä½œä¸ºä¸€ä¸ªè¡¥å……èµ„æºï¼Œæˆ‘ä»¬ä½¿ç”¨PromptAnatomyæ¡†æ¶å¯¹å››ä¸ªå…¬å…±æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†è¿›è¡Œäº†æ ‡æ³¨ï¼Œå¹¶é€šè¿‡äººå·¥å®¡æŸ¥è¿›è¡Œäº†éªŒè¯ã€‚åœ¨è¿™äº›æ•°æ®é›†å’Œäº”ä¸ªå…ˆè¿›LLMä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒComPerturbè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ”»å‡»æˆåŠŸç‡ã€‚æ¶ˆèç ”ç©¶éªŒè¯äº†æç¤ºåˆ†è§£å’ŒPPLè¿‡æ»¤çš„äº’è¡¥æ•ˆç›Šã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†æ„è¯†åˆ°æç¤ºç»“æ„çš„é‡è¦æ€§å’Œå—æ§æ‰°åŠ¨ï¼Œè¿™å¯¹äºLLMçš„å¯é å¯¹æŠ—æ€§ç¨³å¥æ€§è¯„ä¼°è‡³å…³é‡è¦ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Yujiaaaaa/PACP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Yujiaaaaa/PACPæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01554v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºäºæç¤ºçš„å¯¹æŠ—æ€§æ”»å‡»çš„æœ‰æ•ˆæ‰‹æ®µï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰æ–¹æ³•çš„ä¸è¶³ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€å°†æç¤ºè§†ä¸ºå•ä¸€çš„æ–‡æœ¬ï¼Œå¿½ç•¥äº†å…¶ç»“æ„å¼‚è´¨æ€§ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†PromptAnatomyæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨åœ°å°†æç¤ºåˆ†è§£ä¸ºåŠŸèƒ½ç»„ä»¶ï¼Œå¹¶é€šè¿‡é€‰æ‹©æ€§æ‰°åŠ¨æ¯ä¸ªç»„ä»¶æ¥ç”Ÿæˆå¤šæ ·ä¸”å¯è§£é‡Šçš„å¯¹æŠ—æ ·æœ¬ã€‚åŒæ—¶ï¼Œä¸ºäº†ç¡®ä¿è¯­è¨€ä¸Šçš„åˆç†æ€§å’Œå‡è½»åˆ†å¸ƒåç§»ï¼Œç»“åˆå›°æƒ‘åº¦ï¼ˆPPLï¼‰è¿‡æ»¤æœºåˆ¶ã€‚åœ¨å››ä¸ªå…¬å¼€æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†å’Œäº”ä¸ªå…ˆè¿›LLMä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒComPerturbæ–¹æ³•å–å¾—äº†æœ€å…ˆè¿›çš„æ”»å‡»æˆåŠŸç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰è¯„ä¼°LLMç¨³å¥æ€§çš„æ–¹æ³•å¿½ç•¥äº†æç¤ºçš„ç»“æ„å¼‚è´¨æ€§ï¼Œå³ä¸åŒæç¤ºç»„ä»¶å¯¹å¯¹æŠ—ç¨³å¥æ€§çš„è´¡çŒ®ä¸å‡ç­‰ã€‚</li>
<li>PromptAnatomyæ¡†æ¶èƒ½å¤Ÿè‡ªåŠ¨åœ°å°†æç¤ºåˆ†è§£ä¸ºåŠŸèƒ½ç»„ä»¶ï¼Œç”Ÿæˆå¤šæ ·ä¸”å¯è§£é‡Šçš„å¯¹æŠ—æ ·æœ¬ã€‚</li>
<li>ComPerturbæ–¹æ³•é€šè¿‡é€‰æ‹©æ€§æ‰°åŠ¨æç¤ºçš„æ¯ä¸ªç»„ä»¶ï¼Œå¢å¼ºäº†å¯¹æŠ—æ€§æ”»å‡»çš„æ•ˆæœã€‚</li>
<li>å›°æƒ‘åº¦ï¼ˆPPLï¼‰è¿‡æ»¤æœºåˆ¶ç”¨äºç¡®ä¿è¯­è¨€ä¸Šçš„åˆç†æ€§å’Œå‡è½»åˆ†å¸ƒåç§»ã€‚</li>
<li>åœ¨å››ä¸ªå…¬å¼€æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†å’Œäº”ä¸ªå…ˆè¿›LLMä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒComPerturbæ–¹æ³•å–å¾—äº†æœ€å…ˆè¿›çš„æ”»å‡»æˆåŠŸç‡ã€‚</li>
<li>æ¶ˆèç ”ç©¶éªŒè¯äº†æç¤ºè§£å‰–å’ŒPPLè¿‡æ»¤çš„äº’è¡¥æ•ˆç›Šã€‚</li>
<li>ç»“æœå¼ºè°ƒäº†åœ¨è¿›è¡ŒLLMçš„å¯¹æŠ—æ€§ç¨³å¥æ€§è¯„ä¼°æ—¶ï¼Œéœ€è¦è€ƒè™‘æç¤ºç»“æ„æ„è¯†å’Œå—æ§æ‰°åŠ¨çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01554">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f2e327a4f3640da8834fad62edb4975f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2b28e94ac4a8767664c40af91a0b925.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbf68577785cb9b43f8da90577372207.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca4cf84e985c37577f50b00f2811d3b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01ee7e1e85c3ced0a7d82940674d166d.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Transformers-in-Pseudo-Random-Number-Generation-A-Dual-Perspective-on-Theory-and-Practice"><a href="#Transformers-in-Pseudo-Random-Number-Generation-A-Dual-Perspective-on-Theory-and-Practice" class="headerlink" title="Transformers in Pseudo-Random Number Generation: A Dual Perspective on   Theory and Practice"></a>Transformers in Pseudo-Random Number Generation: A Dual Perspective on   Theory and Practice</h2><p><strong>Authors:Ran Li, Lingshu Zeng</strong></p>
<p>Pseudo-random number generators (PRNGs) are high-nonlinear processes, and they are key blocks in optimization of Large language models. Transformers excel at processing complex nonlinear relationships. Thus it is reasonable to generate high-quality pseudo-random numbers based on transformers. In this paper, we explore this question from both theoretical and practical perspectives, highlighting the potential benefits and implications of Transformer in PRNGs. We theoretically demonstrate that decoder-only Transformer models with Chain-of-Thought can simulate both the Linear Congruential Generator (LCG) and Mersenne Twister (MT) PRNGs. Based on this, we conclude that the log-precision decoder-only Transformer can represent non-uniform $\text{AC}^0$. Our simulative theoretical findings are validated through experiments. The random numbers generated by Transformer-based PRNGs successfully pass the majority of NIST tests, whose heat maps exhibit clear statistical randomness. Finally, we assess their capability in prediction attacks. </p>
<blockquote>
<p>ä¼ªéšæœºæ•°ç”Ÿæˆå™¨ï¼ˆPRNGsï¼‰æ˜¯é«˜éçº¿æ€§è¿‡ç¨‹ï¼Œæ˜¯ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„å…³é”®ç¯èŠ‚ã€‚Transformeræ“…é•¿å¤„ç†å¤æ‚çš„éçº¿æ€§å…³ç³»ã€‚å› æ­¤ï¼ŒåŸºäºTransformerç”Ÿæˆé«˜è´¨é‡ä¼ªéšæœºæ•°æ˜¯æœ‰é“ç†çš„ã€‚æœ¬æ–‡å°†ä»ç†è®ºå’Œå®è·µä¸¤ä¸ªè§’åº¦æ¢è®¨è¿™ä¸ªé—®é¢˜ï¼Œé‡ç‚¹ä»‹ç»Transformeråœ¨PRNGsä¸­çš„æ½œåœ¨ä¼˜åŠ¿å’Œå½±å“ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜ï¼Œä»…è§£ç çš„Transformeræ¨¡å‹ç»“åˆæ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰å¯ä»¥æ¨¡æ‹Ÿçº¿æ€§åŒä½™ç”Ÿæˆå™¨ï¼ˆLCGï¼‰å’Œæ¢…æ£®æ—‹è½¬å™¨ï¼ˆMTï¼‰PRNGsã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œå¯¹æ•°ç²¾åº¦ä»…è§£ç çš„Transformerå¯ä»¥ä»£è¡¨éå‡åŒ€AC^0ã€‚æˆ‘ä»¬çš„æ¨¡æ‹Ÿç†è®ºå‘ç°å·²ç»é€šè¿‡å®éªŒå¾—åˆ°éªŒè¯ã€‚åŸºäºTransformerçš„PRNGç”Ÿæˆçš„éšæœºæ•°æˆåŠŸé€šè¿‡äº†NISTæµ‹è¯•çš„å¤§éƒ¨åˆ†æµ‹è¯•ï¼Œå…¶çƒ­å›¾æ˜¾ç¤ºå‡ºæ˜æ˜¾çš„ç»Ÿè®¡éšæœºæ€§ã€‚æœ€åï¼Œæˆ‘ä»¬è¯„ä¼°äº†å®ƒä»¬åœ¨é¢„æµ‹æ”»å‡»æ–¹é¢çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01134v1">PDF</a> 27 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºTransformeræ¨¡å‹çš„ä¼ªéšæœºæ•°ç”Ÿæˆå™¨ï¼ˆPRNGsï¼‰çš„æ½œåŠ›ä¸å½±å“ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä»…è§£ç å™¨å‹çš„Transformeræ¨¡å‹å¯ä»¥æ¨¡æ‹Ÿçº¿æ€§åŒä½™ç”Ÿæˆå™¨ï¼ˆLCGï¼‰å’Œæ¢…æ£®æ—‹èŠ±ï¼ˆMTï¼‰ç­‰PRNGsï¼Œå¹¶æˆåŠŸç”Ÿæˆé€šè¿‡äº†NISTæµ‹è¯•çš„ä¼ªéšæœºæ•°ã€‚æœ¬æ–‡éªŒè¯äº†åŸºäºTransformerçš„PRNGåœ¨æ¨¡æ‹Ÿéçº¿æ€§å…³ç³»æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¨¡å‹å¯ç”¨äºæ¨¡æ‹Ÿä¼ªéšæœºæ•°ç”Ÿæˆå™¨ï¼ˆPRNGsï¼‰ï¼Œå¦‚çº¿æ€§åŒä½™ç”Ÿæˆå™¨ï¼ˆLCGï¼‰å’Œæ¢…æ£®æ—‹èŠ±ï¼ˆMTï¼‰ã€‚</li>
<li>ä»…è§£ç å™¨å‹çš„Transformerèƒ½å¤Ÿå¤„ç†å¤æ‚çš„éçº¿æ€§å…³ç³»ï¼Œä¸ºé«˜è´¨é‡çš„ä¼ªéšæœºæ•°ç”Ÿæˆæä¾›äº†åŸºç¡€ã€‚</li>
<li>åŸºäºTransformerçš„PRNGç”Ÿæˆçš„ä¼ªéšæœºæ•°æˆåŠŸé€šè¿‡äº†NISTæµ‹è¯•ï¼Œè¡¨ç°å‡ºæ˜æ˜¾çš„ç»Ÿè®¡éšæœºæ€§ã€‚</li>
<li>Transformeræ¨¡å‹åœ¨æ¨¡æ‹Ÿéå‡åŒ€åˆ†å¸ƒæ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ã€‚</li>
<li>æœ¬æ–‡éªŒè¯äº†ç†è®ºæ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶å®é™…æ€§èƒ½ã€‚</li>
<li>åŸºäºTransformerçš„PRNGså…·æœ‰ä¸€å®šçš„é¢„æµ‹æ”»å‡»é˜²æŠ¤èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed69d6a0ad712b63f916fdeead7b18de.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9667208415f3a6b2b2fd2f0880e5b1ab.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  Hierarchical Learning-Based Control for Multi-Agent Shepherding of   Stochastic Autonomous Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-69abad54fe13ab2f6b0b805c1919d73e.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  MedVLThinker Simple Baselines for Multimodal Medical Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25879.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
