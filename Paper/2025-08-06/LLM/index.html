<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-08-06  Raw Data Matters Enhancing Prompt Tuning by Internal Augmentation on   Vision-Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-35e7d6f740610ead318746206df0c675.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    80 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-06-更新"><a href="#2025-08-06-更新" class="headerlink" title="2025-08-06 更新"></a>2025-08-06 更新</h1><h2 id="Raw-Data-Matters-Enhancing-Prompt-Tuning-by-Internal-Augmentation-on-Vision-Language-Models"><a href="#Raw-Data-Matters-Enhancing-Prompt-Tuning-by-Internal-Augmentation-on-Vision-Language-Models" class="headerlink" title="Raw Data Matters: Enhancing Prompt Tuning by Internal Augmentation on   Vision-Language Models"></a>Raw Data Matters: Enhancing Prompt Tuning by Internal Augmentation on   Vision-Language Models</h2><p><strong>Authors:Haoyang Li, Liang Wang, Chao Wang, Siyu Zhou, Jing Jiang, Yan Peng, Guodong Long</strong></p>
<p>For CLIP-based prompt tuning, introducing more data as additional knowledge for enhancing fine-tuning process is proved to be an effective approach. Existing data amplification strategies for prompt tuning typically rely on external knowledge (e.g., large language models or pre-structured knowledge bases), resulting in higher costs for data collection and processing, while generally ignoring further utilization of features in image modality. To address this, we propose Augmentation-driven Prompt Tuning (AugPT), a self-contained distillation-based prompt tuning approach using only internal augmentation on raw dataset to better exploit known features. Specifically, AugPT employs self-supervised augmentation on unlabeled images in the training set, and introduces a novel gating mechanism based on consensus test, reusing the pre-trained prompt tuning backbone model to spontaneously filter noisy samples, further enhancing the quality of augmented views. Extensive experiments validate that AugPT simultaneously enhances model performance and generalization capability without using appended external knowledge. The code of AugPT is available at: <a target="_blank" rel="noopener" href="https://github.com/JREion/AugPT">https://github.com/JREion/AugPT</a> . </p>
<blockquote>
<p>对于基于CLIP的提示调整（prompt tuning），引入更多数据作为额外知识以增强微调过程已被证明是一种有效的方法。现有的数据增强策略通常依赖于外部知识（例如大型语言模型或预结构化知识库），导致数据收集和处理成本较高，同时忽略了进一步利用图像模态中的特征。针对这一问题，我们提出了基于自增强的提示调整（AugPT）方法。这是一种基于自我蒸馏的提示调整方法，仅使用原始数据集的内部增强来更好地利用已知特征。具体来说，AugPT对训练集中的无标签图像进行自监督增强，并引入了一种基于共识测试的新型门控机制，利用预训练的提示调整骨干模型自动过滤噪声样本，进一步提高增强视图的品质。大量实验验证，AugPT在不使用附加外部知识的情况下，同时提高了模型的性能和泛化能力。AugPT的代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/JREion/AugPT">https://github.com/JREion/AugPT</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02671v1">PDF</a> 16 pages, 6 figures, 15 tables</p>
<p><strong>Summary</strong><br>文本提出了一种基于CLIP的新方法AugPT，该方法使用仅内部增强的原始数据集进行微调，利用未标记图像中的自监督增强和基于共识测试的新门控机制，以提高模型的性能和泛化能力，无需使用额外的外部知识。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIP背景下的提示调整方法中，引入更多数据作为额外的知识以改进微调过程是一种有效方法。</li>
<li>现有的数据增强策略通常依赖于外部知识（如大型语言模型或预结构化知识库），导致数据采集和处理成本较高，同时忽略了图像模态特征的进一步利用。</li>
<li>AugPT是一种基于蒸馏的提示调整方法，仅使用原始数据集的内部增强来更好地利用已知特征。</li>
<li>AugPT在训练集的非标记图像上采用自监督增强。</li>
<li>AugPT引入了一种基于共识测试的新门控机制，利用预训练的提示调整骨干模型自动过滤噪声样本，进一步提高增强视图的品质。</li>
<li>实验证明，AugPT在不使用外部知识的情况下，能同时提高模型的性能和泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02671">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4cf3672b6047ab540ed9a8895ea6c020.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f67b6f477ff7bf8e6ac6344b4d7ca056.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7de9c10e1cd13012af8a0a4f10033a96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a8dc81ff4b618d908ae79f8c29a49b89.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f5c96ce70b1b4508fa5a5a93ec3b2653.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03249446815a0882d28c90258060d702.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="LOST-Low-rank-and-Sparse-Pre-training-for-Large-Language-Models"><a href="#LOST-Low-rank-and-Sparse-Pre-training-for-Large-Language-Models" class="headerlink" title="LOST: Low-rank and Sparse Pre-training for Large Language Models"></a>LOST: Low-rank and Sparse Pre-training for Large Language Models</h2><p><strong>Authors:Jiaxi Li, Lu Yin, Li Shen, Jinjin Xu, Liwu Xu, Tianjin Huang, Wenwu Wang, Shiwei Liu, Xilu Wang</strong></p>
<p>While large language models (LLMs) have achieved remarkable performance across a wide range of tasks, their massive scale incurs prohibitive computational and memory costs for pre-training from scratch. Recent studies have investigated the use of low-rank parameterization as a means of reducing model size and training cost. In this context, sparsity is often employed as a complementary technique to recover important information lost in low-rank compression by capturing salient features in the residual space. However, existing approaches typically combine low-rank and sparse components in a simplistic or ad hoc manner, often resulting in undesirable performance degradation compared to full-rank training. In this paper, we propose \textbf{LO}w-rank and \textbf{S}parse pre-\textbf{T}raining (\textbf{LOST}) for LLMs, a novel method that ingeniously integrates low-rank and sparse structures to enable effective training of LLMs from scratch under strict efficiency constraints. LOST applies singular value decomposition to weight matrices, preserving the dominant low-rank components, while allocating the remaining singular values to construct channel-wise sparse components to complement the expressiveness of low-rank training. We evaluate LOST on LLM pretraining ranging from 60M to 7B parameters. Our experiments show that LOST achieves competitive or superior performance compared to full-rank models, while significantly reducing both memory and compute overhead. Moreover, Code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models%7D%7BLOST">https://github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models}{LOST</a> Repo} </p>
<blockquote>
<p>虽然大型语言模型（LLM）在多种任务中取得了显著的性能，但它们的大规模导致了从头开始预训练的计算和内存成本高昂。最近的研究探讨了使用低秩参数化作为减小模型大小和降低训练成本的方法。在此上下文中，稀疏性通常被用作一种辅助技术，以恢复在低秩压缩中丢失的重要信息，通过捕获残差空间中的显著特征来实现这一点。然而，现有的方法通常以简单或即兴的方式将低秩和稀疏组件组合在一起，与全秩训练相比，通常会导致性能下降。在本文中，我们为LLM提出了<strong>LOW</strong>秩和<strong>稀疏</strong>预<strong>训练</strong>（<strong>LOST</strong>）的新方法，该方法巧妙地结合了低秩和稀疏结构，能够在严格的效率约束下有效地从头开始训练LLM。LOST通过奇异值分解应用于权重矩阵，保留主要低秩成分，同时将剩余的奇异值分配给通道稀疏成分，以补充低秩训练的表达性。我们对参数范围从60M到7B的LLM预训练进行了LOST评估。实验表明，LOST在性能方面达到了与全秩模型相当或更优的水平，同时显著降低了内存和计算开销。此外，代码可在<a target="_blank" rel="noopener" href="https://github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models">LOST仓库</a>中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02668v1">PDF</a> </p>
<p><strong>Summary</strong><br>大语言模型（LLM）在广泛的任务中取得了显著的性能，但其庞大的规模带来了高昂的计算和内存成本。近期研究表明，使用低秩参数化方法可以减少模型大小和训练成本。然而，简单结合低秩和稀疏成分会导致性能下降。本文提出一种新颖的低秩和稀疏预训练方法（LOST），该方法巧妙地将低秩和稀疏结构相结合，在严格的效率约束下实现了LLM的有效训练。通过对权重矩阵进行奇异值分解，LOST保留了主要低秩成分，并将剩余奇异值分配给通道稀疏成分，以补充低秩训练的表达能力。实验表明，LOST在LLM预训练中表现优异，同时显著降低了内存和计算成本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大语言模型（LLM）面临高计算成本和内存需求的问题。</li>
<li>低秩参数化方法被研究用于减少模型大小和训练成本。</li>
<li>现有方法简单结合低秩和稀疏成分，导致性能下降。</li>
<li>提出的LOW-rank和Sparse pre-Training（LOST）方法结合了低秩和稀疏结构。</li>
<li>LOST通过奇异值分解保留主要低秩成分，并分配剩余奇异值形成通道稀疏成分。</li>
<li>实验表明，LOST在LLM预训练中表现优异，且显著降低内存和计算成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02668">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-61c7b2b355a28bc582daeb0ea18bdf63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5268f128a958213ce6ff05a8cf9fcc99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-beb261ac57fee753dcf46f102c69d54c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a7d7f79733c99f4d5a883f43766a908.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb75c898537360ab80d48b28e61027a6.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Evaluating-Variance-in-Visual-Question-Answering-Benchmarks"><a href="#Evaluating-Variance-in-Visual-Question-Answering-Benchmarks" class="headerlink" title="Evaluating Variance in Visual Question Answering Benchmarks"></a>Evaluating Variance in Visual Question Answering Benchmarks</h2><p><strong>Authors:Nikitha SR</strong></p>
<p>Multimodal large language models (MLLMs) have emerged as powerful tools for visual question answering (VQA), enabling reasoning and contextual understanding across visual and textual modalities. Despite their advancements, the evaluation of MLLMs on VQA benchmarks often relies on point estimates, overlooking the significant variance in performance caused by factors such as stochastic model outputs, training seed sensitivity, and hyperparameter configurations. This paper critically examines these issues by analyzing variance across 14 widely used VQA benchmarks, covering diverse tasks such as visual reasoning, text understanding, and commonsense reasoning. We systematically study the impact of training seed, framework non-determinism, model scale, and extended instruction finetuning on performance variability. Additionally, we explore Cloze-style evaluation as an alternate assessment strategy, studying its effectiveness in reducing stochasticity and improving reliability across benchmarks. Our findings highlight the limitations of current evaluation practices and advocate for variance-aware methodologies to foster more robust and reliable development of MLLMs. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）作为视觉问答（VQA）的强大工具已经出现，能够在视觉和文本模式之间进行推理和上下文理解。尽管取得了进展，但在VQA基准测试中对MLLMs的评估往往依赖于点估计，从而忽视了由于随机模型输出、训练种子敏感性和超参数配置等因素导致的性能显著差异。本文通过分析14个广泛使用的VQA基准测试的方差，对这些问题进行了深入研究，涵盖了视觉推理、文本理解和常识推理等多样化任务。我们系统地研究了训练种子、框架非确定性、模型规模以及扩展指令微调对性能变化的影响。此外，我们还探索了以填空式评估作为替代评估策略，研究了其在减少随机性和提高跨基准测试可靠性方面的有效性。我们的研究结果表明了当前评估方法的局限性，并提倡采用考虑变异性的方法来促进MLLMs更稳健和可靠的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02645v1">PDF</a> Accepted in ICCV 2025 Workshop on What’s Next in Multimodal   Foundational Models</p>
<p><strong>Summary</strong></p>
<p>MLLMs在视觉问答（VQA）方面的应用展现出强大的跨视觉和文本模态的推理和上下文理解能力。然而，现有评估方法主要依赖点估计，忽略了模型输出随机性、训练种子敏感性和超参数配置等因素导致的性能显著变化。本文通过分析在14个广泛使用的VQA基准测试上的方差，系统研究了训练种子、框架非确定性、模型规模和扩展指令微调对性能变化的影响，并探讨了闭式评估策略的有效性。研究发现当前评估方法的局限性，并主张采用考虑方差的评估方法来促进更稳健和可靠的LLM发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMs在视觉问答（VQA）中展现出强大的跨模态能力。</li>
<li>当前评估方法主要依赖点估计，忽略了模型性能的重大变化。</li>
<li>训练种子、框架非确定性、模型规模和微调影响性能变化。</li>
<li>基准测试上的方差分析揭示了评估方法的局限性。</li>
<li>考虑方差的评估方法对于促进稳健和可靠的LLM发展至关重要。</li>
<li>提出使用闭式评估策略作为替代评估方法，以减少随机性并改善基准测试的可靠性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02645">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-45ad12c5b2a8a7fed798c9c66956da7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd505d10203ce5a9727527db701f509d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d86946452ce5cdc3b03aee1b0fef07e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52b0a34e710e53048f9ff7524f73f585.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4eae2e19842bf1d3141d677c3585921.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f588e7571a2eaefbf8018be15b1a36d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47bd7e11e24bb8428753eb2fb2c9655b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d39b0846295ddb7ecff57441c2ae14db.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Test-Set-Quality-in-Multilingual-LLM-Evaluation"><a href="#Test-Set-Quality-in-Multilingual-LLM-Evaluation" class="headerlink" title="Test Set Quality in Multilingual LLM Evaluation"></a>Test Set Quality in Multilingual LLM Evaluation</h2><p><strong>Authors:Kranti Chalamalasetti, Gabriel Bernier-Colborne, Yvan Gauthier, Sowmya Vajjala</strong></p>
<p>Several multilingual benchmark datasets have been developed in a semi-automatic manner in the recent past to measure progress and understand the state-of-the-art in the multilingual capabilities of Large Language Models. However, there is not a lot of attention paid to the quality of the datasets themselves, despite the existence of previous work in identifying errors in even fully human-annotated test sets. In this paper, we manually analyze recent multilingual evaluation sets in two languages - French and Telugu, identifying several errors in the process. We compare the performance difference across several LLMs with the original and revised versions of the datasets and identify large differences (almost 10% in some cases) in both languages). Based on these results, we argue that test sets should not be considered immutable and should be revisited, checked for correctness, and potentially versioned. We end with some recommendations for both the dataset creators as well as consumers on addressing the dataset quality issues. </p>
<blockquote>
<p>近年来，为了衡量和了解多语言大型语言模型的最新进展和状态，已经以半自动的方式开发了几个多语言基准数据集。然而，尽管有先前的工作已经识别出即使是完全人工注释的测试集中也存在错误，但人们对数据集本身的品质并没有给予足够的重视。在本文中，我们手动分析了最近的两门语言——法语和泰卢固语的多语言评估集，并在过程中发现了多个错误。我们将多个大型语言模型在原始和修订后的数据集上的性能差异进行了比较，发现在两种语言中都存在巨大差异（在某些情况下几乎达到10%）。基于这些结果，我们认为测试集不应被视为永恒不变的，而应该进行复查以检查其准确性并可能对其进行版本控制。最后，我们对数据集创建者和使用者提出一些解决数据集质量问题的建议。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02635v1">PDF</a> Accepted at the 1st Workshop on Multilingual Data Quality Signals,   COLM 2025, Short paper. 10 pages in total</p>
<p><strong>Summary</strong></p>
<p>近期开发的多语种基准数据集以半自动方式衡量大语言模型的进步并了解其最新技术状态。然而，现有的测试集即使由完全人工注释，也仍然存在错误。本文通过手动分析两种语言的最近多语种评估集（法语和泰卢固语），发现了数据集中的一些错误。在对比了原始数据集与修订数据集的不同性能差异后，本文强调了测试集并非一成不变，应该对其进行重新评估，并重视质量问题的解决，对此本文提供了对于数据集的创建者和消费者们的解决建议。同时也说明了需要进行质量控制并可能需要使用不同版本的管理办法，以保障测试集的可靠性。这为LLM模型的训练和评估提供了一个全新的视角和深入的思考。针对现有的评测数据质量问题进行了有力的反驳与深入探讨，并且从两个具体的实例出发进行了实证阐述。这既有助于研究者的参考和借鉴，也促进了自然语言处理领域研究的深入发展。为自然语言处理领域的科研提供了有价值的启示和解决方案。同时，也提醒了人们要重视数据质量的问题，确保数据集的准确性和可靠性。对于未来的研究而言，本文具有重要的参考价值和实践指导意义。同时强调了数据集质量的重要性，并提出了改进建议。对于未来多语言模型的发展和应用具有积极的推动作用。同时也指出了未来研究的可能方向和挑战。本文强调了数据集质量的重要性以及其对该领域未来研究的潜在影响。<strong>Key Takeaways</strong></p>
<ul>
<li>多语种基准数据集用以衡量大语言模型的进步。然而，这些数据集的质量问题被忽视。</li>
<li>手动分析发现多语种评估集存在错误。</li>
<li>测试集并非一成不变，需要定期检查和修正错误。</li>
<li>测试集的质量问题对LLM性能评估产生显著影响（在某些情况下差异达10%）。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02635">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5ccf5bcf730699f90b2c50592be93455.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21e8c06b0b0232d0a0f236aabbca0a8f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1926848d0ef6bfefeb88d49b9773ab01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bbc192ad23141f07887637b6bae23b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a99bb5a07370ae06dee8fb83547411d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a2a5fb27037f9419ea8018d3786c474.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="HyCodePolicy-Hybrid-Language-Controllers-for-Multimodal-Monitoring-and-Decision-in-Embodied-Agents"><a href="#HyCodePolicy-Hybrid-Language-Controllers-for-Multimodal-Monitoring-and-Decision-in-Embodied-Agents" class="headerlink" title="HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and   Decision in Embodied Agents"></a>HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and   Decision in Embodied Agents</h2><p><strong>Authors:Yibin Liu, Zhixuan Liang, Zanxin Chen, Tianxing Chen, Mengkang Hu, Wanxi Dong, Congsheng Xu, Zhaoming Han, Yusen Qin, Yao Mu</strong></p>
<p>Recent advances in multimodal large language models (MLLMs) have enabled richer perceptual grounding for code policy generation in embodied agents. However, most existing systems lack effective mechanisms to adaptively monitor policy execution and repair codes during task completion. In this work, we introduce HyCodePolicy, a hybrid language-based control framework that systematically integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair into a closed-loop programming cycle for embodied agents. Technically, given a natural language instruction, our system first decomposes it into subgoals and generates an initial executable program grounded in object-centric geometric primitives. The program is then executed in simulation, while a vision-language model (VLM) observes selected checkpoints to detect and localize execution failures and infer failure reasons. By fusing structured execution traces capturing program-level events with VLM-based perceptual feedback, HyCodePolicy infers failure causes and repairs programs. This hybrid dual feedback mechanism enables self-correcting program synthesis with minimal human supervision. Our results demonstrate that HyCodePolicy significantly improves the robustness and sample efficiency of robot manipulation policies, offering a scalable strategy for integrating multimodal reasoning into autonomous decision-making pipelines. </p>
<blockquote>
<p>近期多模态大型语言模型（MLLMs）的进步为实体代理中的代码策略生成提供了更丰富的感知基础。然而，大多数现有系统缺乏有效的机制来在任务完成过程中自适应地监视策略执行和修复代码。在这项工作中，我们引入了HyCodePolicy，这是一个基于混合语言的控制框架，它将代码合成、几何基础、感知监控和迭代修复系统地集成到一个闭环编程周期中，用于实体代理。从技术上讲，给定自然语言指令，我们的系统首先将其分解为子目标并生成基于对象为中心的几何原始数据的初始可执行程序。该程序然后在仿真中执行，同时视觉语言模型（VLM）观察选定检查点以检测和定位执行失败并推断失败原因。通过将捕获程序级事件的结构化执行轨迹与基于VLM的感知反馈相融合，HyCodePolicy推断出失败原因并修复程序。这种混合双反馈机制实现了自我修正的程序合成，几乎无需人工监督。我们的结果表明，HyCodePolicy显著提高了机器人操作策略的稳健性和样本效率，为将多模态推理集成到自主决策管道中提供了可扩展的策略。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02629v1">PDF</a> Accepted to ICCV 2025 Workshop on Multi-Modal Reasoning for Agentic   Intelligence</p>
<p><strong>Summary</strong></p>
<p>近期多模态大型语言模型（MLLM）的进步为实体代理中的代码策略生成提供了更丰富的感知基础。然而，大多数现有系统缺乏在任务完成过程中自适应监控策略执行和修复代码的有效机制。本研究介绍了HyCodePolicy，这是一个基于语言的控制框架，系统地集成了代码合成、几何基础、感知监控和迭代修复，为实体代理形成了一个闭环编程周期。给定自然语言指令，该系统首先将其分解为子目标并生成基于对象为中心的几何原始数据的初始可执行程序。然后在模拟中执行程序，同时视觉语言模型（VLM）观察选定检查点以检测和定位执行故障并推断失败原因。通过融合捕获程序级事件的结构化执行轨迹和基于VLM的感知反馈，HyCodePolicy可以推断故障原因并修复程序。这种混合双重反馈机制实现了在极少人类监督下的自我修正程序合成。结果表明，HyCodePolicy显著提高了机器人操作策略的稳健性和样本效率，为自主决策管道中整合多模态推理提供了可伸缩策略。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型的进步为实体代理中的代码策略生成提供了丰富的感知基础。</li>
<li>现有系统在监控和修复代码方面的自适应能力有限。</li>
<li>HyCodePolicy是一个基于语言的控制框架，集成了代码合成、几何基础、感知监控和迭代修复。</li>
<li>系统通过分解自然语言指令生成初始可执行程序。</li>
<li>通过视觉语言模型观察选定检查点来检测并定位执行故障。</li>
<li>HyCodePolicy通过结合结构化执行轨迹和感知反馈来推断故障原因并进行程序修复。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02629">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2f1b1063d79cac75e2b5970f38e7c1b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e00bea4a7fb9f8f4ccfb2822d131f8ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-071ec1194a132ef9a07dfbf35bf0f5f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b03b45c362657aa8047b34093867ff8a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45c0ca75878461436ff4182c63ab2bee.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Mitigating-Attention-Hacking-in-Preference-Based-Reward-Modeling-via-Interaction-Distillation"><a href="#Mitigating-Attention-Hacking-in-Preference-Based-Reward-Modeling-via-Interaction-Distillation" class="headerlink" title="Mitigating Attention Hacking in Preference-Based Reward Modeling via   Interaction Distillation"></a>Mitigating Attention Hacking in Preference-Based Reward Modeling via   Interaction Distillation</h2><p><strong>Authors:Jianxiang Zang, Meiling Ning, Shihan Dou, Jiazheng Zhang, Tao Gui, Qi Zhang, Xuanjing Huang</strong></p>
<p>The reward model (RM), as the core component of reinforcement learning from human feedback (RLHF) for large language models (LLMs), responsible for providing reward signals to generated responses. However, mainstream preference modeling in RM is inadequate in terms of token-level interaction, making its judgment signals vulnerable to being hacked by misallocated attention to context. This stems from two fundamental limitations: (1) Current preference modeling employs decoder-only architectures, where the unidirectional causal attention mechanism leads to forward-decaying intra-sequence attention within the prompt-response sequence. (2) The independent Siamese-encoding paradigm induces the absence of token-level inter-sequence attention between chosen and rejected sequences. To address this “attention hacking”, we propose “Interaction Distillation”, a novel training framework for more adequate preference modeling through attention-level optimization. The method introduces an interaction-based natural language understanding model as the teacher to provide sophisticated token interaction patterns via comprehensive attention, and guides the preference modeling to simulate teacher model’s interaction pattern through an attentional alignment objective. Through extensive experiments, interaction distillation has demonstrated its ability to provide more stable and generalizable reward signals compared to state-of-the-art RM optimization methods that target data noise, highlighting the attention hacking constitute a more fundamental limitation in RM. </p>
<blockquote>
<p>奖励模型（RM）作为大型语言模型（LLM）中基于人类反馈的强化学习（RLHF）的核心组件，负责为生成的响应提供奖励信号。然而，RM中的主流偏好建模在令牌级交互方面存在不足，导致判断信号容易受到对上下文分配不当的注意力的攻击。这源于两个基本局限：（1）当前的偏好建模仅采用解码器架构，其中单向因果注意力机制导致提示-响应序列内的序列内注意力呈现前向衰减。（2）独立的Siamese编码范式导致所选序列和拒绝序列之间缺少令牌级序列间注意力。为了解决这种“注意力攻击”，我们提出了“交互蒸馏”，这是一种通过注意力级别优化进行更充分偏好建模的新型训练框架。该方法引入了一种基于交互的自然语言理解模型作为教师，通过全面的注意力提供精细的令牌交互模式，并通过注意力对齐目标指导偏好建模来模拟教师模型的交互模式。通过大量实验，交互蒸馏显示出提供比针对数据噪声优化的最新RM方法更稳定和可推广的奖励信号的能力，强调了注意力攻击在RM中构成更基本的局限。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02618v1">PDF</a> </p>
<p><strong>Summary</strong>：奖励模型（RM）作为强化学习从人类反馈（RLHF）中的核心组件，用于为大型语言模型（LLM）生成响应提供奖励信号。然而，主流的偏好建模在RM中存在符号级交互的不足，使得其判断信号容易受到对上下文分配不当注意力的攻击。这源于两个基本局限：一是当前偏好建模采用单向因果注意力机制；二是独立式赛瓦编码范式导致了标记级别序列间注意力的缺失。为解决注意力破解问题，提出了名为“交互蒸馏”的新型训练框架，通过注意力级别的优化进行更准确的偏好建模。该方法引入基于交互的自然语言理解模型作为教师，通过全面的注意力提供复杂的标记交互模式，并通过注意力对齐目标引导偏好建模模拟教师模型的交互模式。实验证明，交互蒸馏提供了更稳定和可泛化的奖励信号。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>奖励模型（RM）是强化学习从人类反馈（RLHF）的核心部分，负责为大型语言模型（LLM）生成响应提供奖励信号。</li>
<li>主流偏好建模在RM中存在局限性，易受到注意力破解攻击。</li>
<li>当前偏好建模的局限主要体现在两个方面：一是单向因果注意力机制导致的序列内部注意力衰减；二是在选择序列和拒绝序列之间缺乏标记级别的交互注意力。</li>
<li>“交互蒸馏”是一种新型训练框架，旨在解决注意力破解问题，通过注意力级别的优化进行更准确的偏好建模。</li>
<li>交互蒸馏引入基于交互的自然语言理解模型作为教师，通过全面的注意力提供复杂的标记交互模式。</li>
<li>交互蒸馏通过注意力对齐目标来引导偏好建模模拟教师模型的交互模式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02618">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0aa758030e87e3261ee82f31b003c935.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6893c163fc7f47bae88b58b02984b63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9aa88decb8b2322586720b54f82c4e94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df1bfefaa73abf3ab37ed7b367788baf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79c624f9f637e5a058a01bf9c913aecf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffebb094f31a77d29f0db6cd31f80a29.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Meta-RAG-on-Large-Codebases-Using-Code-Summarization"><a href="#Meta-RAG-on-Large-Codebases-Using-Code-Summarization" class="headerlink" title="Meta-RAG on Large Codebases Using Code Summarization"></a>Meta-RAG on Large Codebases Using Code Summarization</h2><p><strong>Authors:Vali Tawosia, Salwa Alamir, Xiaomo Liu, Manuela Veloso</strong></p>
<p>Large Language Model (LLM) systems have been at the forefront of applied Artificial Intelligence (AI) research in a multitude of domains. One such domain is software development, where researchers have pushed the automation of a number of code tasks through LLM agents. Software development is a complex ecosystem, that stretches far beyond code implementation and well into the realm of code maintenance. In this paper, we propose a multi-agent system to localize bugs in large pre-existing codebases using information retrieval and LLMs. Our system introduces a novel Retrieval Augmented Generation (RAG) approach, Meta-RAG, where we utilize summaries to condense codebases by an average of 79.8%, into a compact, structured, natural language representation. We then use an LLM agent to determine which parts of the codebase are critical for bug resolution, i.e. bug localization. We demonstrate the usefulness of Meta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores 84.67 % and 53.0 % for file-level and function-level correct localization rates, respectively, achieving state-of-the-art performance. </p>
<blockquote>
<p>大型语言模型（LLM）系统在多领域应用人工智能（AI）研究中处于前沿地位。其中一个这样的领域是软件开发，研究人员通过LLM代理推动了多个代码任务的自动化。软件开发是一个复杂的生态系统，远远超出代码实现，深入到代码维护的领域。在本文中，我们提出了一种使用信息检索和LLM在大型现有代码库中定位错误的多代理系统。我们的系统引入了一种新型的基于检索增强生成（RAG）的方法，即Meta-RAG，我们利用摘要将代码库平均压缩79.8%，压缩成紧凑、结构化、自然语言表示的形式。然后，我们使用LLM代理来确定对解决错误至关重要的代码库部分，即错误定位。我们通过使用SWE-bench Lite数据集对Meta-RAG进行了评估，证明了其实用性。Meta-RAG的文件级和函数级正确定位率分别为84.67%和53.0%，达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02611v1">PDF</a> </p>
<p><strong>Summary</strong>：大型语言模型（LLM）系统在多个领域的人工智能研究前沿中发挥了重要作用。在软件开发领域，研究人员通过LLM代理推动了代码任务的自动化。本文提出了一种多代理系统，利用信息检索和LLM定位大型现有代码库中的错误。我们系统引入了新型的检索增强生成（RAG）方法Meta-RAG，通过摘要将代码库平均压缩79.8%，转化为紧凑、结构化的自然语言表示。然后，我们使用LLM代理确定对错误修复至关重要的代码库部分，即错误定位。通过SWE-bench Lite数据集评估，Meta-RAG在文件级别和函数级别的正确定位率分别为84.67%和53.0%，达到了业界先进水平。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>LLM系统在多个领域的人工智能研究中处于前沿地位，包括软件开发。</li>
<li>在软件开发中，LLM代理推动了代码任务的自动化。</li>
<li>本文提出了一个多代理系统，利用信息检索和LLM技术定位大型现有代码库中的错误。</li>
<li>引入了新型的检索增强生成（RAG）方法Meta-RAG，通过摘要将代码库转化为自然语言表示。</li>
<li>Meta-RAG方法能平均压缩代码库79.8%，提高错误定位的效率。</li>
<li>通过SWE-bench Lite数据集评估，Meta-RAG在文件级别和函数级别的正确定位率分别达到了84.67%和53.0%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02611">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2eda9320db29644ee67095f960cb88bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e781acf167585620641c75d72ec7801.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d2f65b18f7fbd86ffd31c0f947eb244.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-272591b4084aa1bf24f2e6c0c2fdc030.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c724ec70b9faf1132c6c2623be7299bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dba5ec36b97c1861f9df40f77d233e2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="StructSynth-Leveraging-LLMs-for-Structure-Aware-Tabular-Data-Synthesis-in-Low-Data-Regimes"><a href="#StructSynth-Leveraging-LLMs-for-Structure-Aware-Tabular-Data-Synthesis-in-Low-Data-Regimes" class="headerlink" title="StructSynth: Leveraging LLMs for Structure-Aware Tabular Data Synthesis   in Low-Data Regimes"></a>StructSynth: Leveraging LLMs for Structure-Aware Tabular Data Synthesis   in Low-Data Regimes</h2><p><strong>Authors:Siyi Liu, Yujia Zheng, Yongqi Zhang</strong></p>
<p>The application of machine learning on tabular data in specialized domains is severely limited by data scarcity. While generative models offer a solution, traditional methods falter in low-data regimes, and recent Large Language Models (LLMs) often ignore the explicit dependency structure of tabular data, leading to low-fidelity synthetics. To address these limitations, we introduce StructSynth, a novel framework that integrates the generative power of LLMs with robust structural control. StructSynth employs a two-stage architecture. First, it performs explicit structure discovery to learn a Directed Acyclic Graph (DAG) from the available data. Second, this learned structure serves as a high-fidelity blueprint to steer the LLM’s generation process, forcing it to adhere to the learned feature dependencies and thereby ensuring the generated data respects the underlying structure by design. Our extensive experiments demonstrate that StructSynth produces synthetic data with significantly higher structural integrity and downstream utility than state-of-the-art methods. It proves especially effective in challenging low-data scenarios, successfully navigating the trade-off between privacy preservation and statistical fidelity. </p>
<blockquote>
<p>在特定领域，机器学习在表格数据上的应用受到数据稀缺的严重限制。虽然生成模型提供了一种解决方案，但传统方法在数据稀缺的情况下会失去效能，而且最近的大型语言模型（LLM）往往会忽略表格数据的显式依赖结构，导致合成数据的保真度较低。为了解决这些限制，我们引入了StructSynth，这是一个新型框架，它将大型语言模型的生成能力与稳健的结构控制相结合。StructSynth采用两阶段架构。首先，它执行显式结构发现，从可用数据中学习有向无环图（DAG）。其次，这一学习到的结构作为一个高保真蓝图，用于引导大型语言模型的生成过程，迫使它遵循学习到的特征依赖关系，从而确保生成的数据在设计上尊重底层结构。我们的大量实验表明，StructSynth生成的数据在结构完整性和下游实用性方面明显高于最新方法。它在具有挑战性的低数据场景中表现尤其出色，成功地在隐私保护和统计保真之间找到了平衡。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02601v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于结构化数据在特定领域应用机器学习时，数据稀缺性限制了模型的性能。生成模型为解决此问题提供了一种解决方案，但在低数据环境下传统方法表现不佳。大型语言模型（LLM）忽略了结构化数据的显式依赖结构，导致生成的合成数据保真度较低。为解决这些问题，我们引入了StructSynth框架，该框架结合了LLM的生成能力与稳健的结构控制。StructSynth采用两阶段架构，首先进行显式结构发现，从现有数据中学习有向无环图（DAG）。其次，使用学习的结构作为高保真蓝图来引导LLM的生成过程，确保生成的数据遵循设计的底层结构。实验证明，StructSynth在结构完整性和下游实用性方面优于现有方法，特别是在低数据场景中表现尤为出色，成功实现了隐私保护和统计保真之间的平衡。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据稀缺性是机器学习在特定领域应用结构化数据的瓶颈。</li>
<li>生成模型是解决数据稀缺性的有效方法，但在低数据环境下表现不佳。</li>
<li>大型语言模型（LLM）忽略了结构化数据的显式依赖结构，导致合成数据保真度低。</li>
<li>StructSynth框架结合了LLM的生成能力与结构控制来解决上述问题。</li>
<li>StructSynth采用两阶段架构，先进行结构发现学习，再应用学到的结构指导LLM生成过程。</li>
<li>实验表明StructSynth生成的合成数据具有更高的结构完整性和下游实用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02601">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-51aeba23d4a0f3fb0932ba35f04124fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15668b17c81fa499f2ef8818ca4df8b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-341e80d94ea9ba2926a218687976c9e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-844b21418e43d29ff55e588a5f1e5308.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CAMA-Enhancing-Mathematical-Reasoning-in-Large-Language-Models-with-Causal-Knowledge"><a href="#CAMA-Enhancing-Mathematical-Reasoning-in-Large-Language-Models-with-Causal-Knowledge" class="headerlink" title="CAMA: Enhancing Mathematical Reasoning in Large Language Models with   Causal Knowledge"></a>CAMA: Enhancing Mathematical Reasoning in Large Language Models with   Causal Knowledge</h2><p><strong>Authors:Lei Zan, Keli Zhang, Ruichu Cai, Lujia Pan</strong></p>
<p>Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks, yet they still struggle with complex mathematical reasoning, a challenge fundamentally rooted in deep structural dependencies. To address this challenge, we propose \textbf{CA}usal \textbf{MA}thematician (\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit, reusable mathematical structure. In the learning stage, CAMA first constructs the \textbf{M}athematical \textbf{C}ausal \textbf{G}raph (\textbf{MCG}), a high-level representation of solution strategies, by combining LLM priors with causal discovery algorithms applied to a corpus of question-solution pairs. The resulting MCG encodes essential knowledge points and their causal dependencies. To better align the graph with downstream reasoning tasks, CAMA further refines the MCG through iterative feedback derived from a selected subset of the question-solution pairs. In the reasoning stage, given a new question, CAMA dynamically extracts a task-relevant subgraph from the MCG, conditioned on both the question content and the LLM’s intermediate reasoning trace. This subgraph, which encodes the most pertinent knowledge points and their causal dependencies, is then injected back into the LLM to guide its reasoning process. Empirical results on real-world datasets show that CAMA significantly improves LLM performance on challenging mathematical problems. Furthermore, our experiments demonstrate that structured guidance consistently outperforms unstructured alternatives, and that incorporating asymmetric causal relationships yields greater improvements than using symmetric associations alone. </p>
<blockquote>
<p>大型语言模型（LLM）在多种任务中表现出强大的性能，但在复杂的数学推理方面仍存在困难，这一挑战根本源于深层的结构依赖性。为了解决这一挑战，我们提出了因果数学家（\textbf{CAMA}）这一两阶段因果框架，为LLM配备明确的可重用数学结构。在学习阶段，CAMA首先构建数学因果图（MCG），这是一种解决方案策略的高级表示，通过结合LLM的先验知识和应用于问题解决方案对语料库的因果发现算法。结果产生的MCG编码了重要的知识点及其因果关系。为了更好地与下游推理任务对齐，CAMA通过来自问题解决方案对选择子集的迭代反馈进一步改进了MCG。在推理阶段，对于新问题，CAMA会根据问题的内容和LLM的中间推理轨迹动态地从MCG中提取出与任务相关的子图。这个子图编码了最相关的知识点及其因果关系，然后重新注入LLM以指导其推理过程。在真实数据集上的实证结果表明，CAMA在解决具有挑战性的数学问题方面显著提高了LLM的性能。此外，我们的实验表明，结构化的指导始终优于非结构化的替代方案，并且融入不对称的因果关系比仅使用对称关联带来更大的改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02583v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在多种任务上表现出强大的性能，但在复杂的数学推理方面仍存在挑战。为解决这一挑战，提出了因果数学家（CAMA）框架，该框架包含两个阶段，旨在为LLM提供明确的可重复使用的数学结构。第一阶段构建数学因果图（MCG），结合LLM先验知识与因果发现算法，对问题解答对进行高层次表示。第二阶段根据新问题从MCG中提取相关子图，并注入LLM以指导其推理过程。实验表明，CAMA能显著提高LLM解决数学问题的能力，且结构化的指导方式优于非结构化方式，利用不对称因果关系比仅使用对称关联效果更好。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在复杂数学推理上仍有挑战，需要新的方法来解决。</li>
<li>CAMA框架包含两个阶段：学习阶段和推理阶段。</li>
<li>学习阶段通过构建数学因果图（MCG）来结合LLM的先验知识与因果发现算法。</li>
<li>CAMA通过迭代反馈优化MCG，使其更好地适应下游推理任务。</li>
<li>推理阶段根据新问题从MCG中提取相关子图，并注入LLM以指导其推理。</li>
<li>实验表明CAMA能显著提高LLM解决数学问题的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02583">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d1c28ab7f4a2f4f03183c8ab9c0e5c98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb36c79e159593473a6f104fb2645231.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef83bcc7ab4751853333c5463f3b0db0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Contextual-Graph-Transformer-A-Small-Language-Model-for-Enhanced-Engineering-Document-Information-Extraction"><a href="#Contextual-Graph-Transformer-A-Small-Language-Model-for-Enhanced-Engineering-Document-Information-Extraction" class="headerlink" title="Contextual Graph Transformer: A Small Language Model for Enhanced   Engineering Document Information Extraction"></a>Contextual Graph Transformer: A Small Language Model for Enhanced   Engineering Document Information Extraction</h2><p><strong>Authors:Karan Reddy, Mayukha Pal</strong></p>
<p>Standard transformer-based language models, while powerful for general text, often struggle with the fine-grained syntax and entity relationships in complex technical, engineering documents. To address this, we propose the Contextual Graph Transformer (CGT), a hybrid neural architecture that combines Graph Neural Networks (GNNs) and Transformers for domain-specific question answering. CGT constructs a dynamic graph over input tokens using sequential, skip-gram, and semantic similarity edges, which is processed by GATv2Conv layers for local structure learning. These enriched embeddings are then passed to a Transformer encoder to capture global dependencies. Unlike generic large models, technical domains often require specialized language models with stronger contextualization and structure awareness. CGT offers a parameter-efficient solution for such use cases. Integrated into a Retrieval-Augmented Generation (RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7% higher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems from CGTs ability to jointly model structural token interactions and long-range semantic coherence. The model is trained from scratch using a two-phase approach: pretraining on general text followed by fine-tuning on domain-specific manuals. This highlights CGTs adaptability to technical language, enabling better grounding, entity tracking, and retrieval-augmented responses in real-world applications. </p>
<blockquote>
<p>传统的基于变压器的语言模型在处理一般文本时表现出强大的能力，但在处理复杂技术、工程文档中的细微语法和实体关系时往往遇到困难。为了解决这个问题，我们提出了上下文图变压器（CGT），这是一种混合神经网络架构，它结合了图神经网络（GNNs）和变压器，用于特定领域的问答。CGT在输入标记上构建了一个动态图，该图使用顺序、跳词和语义相似边缘进行处理，通过GATv2Conv层进行局部结构学习。这些丰富的嵌入然后传递给变压器编码器以捕获全局依赖关系。与通用大型模型不同，技术领域往往需要具有更强上下文化和结构意识的专门语言模型。CGT为这种情况提供了参数高效的解决方案。集成到检索增强生成（RAG）管道中，CGT优于GPT-2和BERT等基线，在仅有62.4%参数的情况下，比GPT-2高出24.7%的准确率。这一收益来源于CGT联合建模结构标记交互和长距离语义连贯性的能力。该模型采用两阶段方法从头开始训练：首先在通用文本上进行预训练，然后在特定领域手册上进行微调。这突出了CGT对技术语言的适应性，可在实际应用程序中实现更好的接地、实体跟踪和检索增强响应。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02532v1">PDF</a> </p>
<p><strong>Summary</strong><br>     针对标准基于转换器的语言模型在处理复杂技术、工程文档中细微语法和实体关系时的困境，提出上下文图转换器（CGT）这一混合神经网络架构。CGT结合图神经网络（GNNs）和转换器，用于特定领域的问答。它通过构建动态图、使用GATv2Conv层进行本地结构学习、增强嵌入，并传递给转换器编码器以捕获全局依赖关系。CGT为技术领域的特定用例提供了参数高效的解决方案，并在检索增强生成（RAG）管道中表现优异，相较于GPT-2和BERT等基线模型有更高的准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>标准语言模型在处理复杂技术文档时面临挑战，需要针对特定领域的解决方案。</li>
<li>上下文图转换器（CGT）是一种混合神经网络架构，结合了图神经网络（GNNs）和转换器技术。</li>
<li>CGT通过构建动态图和利用GATv2Conv层进行本地结构学习来增强嵌入。</li>
<li>CGT在全局范围内使用转换器编码器捕获依赖关系，具备更强的上下文和结构感知能力。</li>
<li>CGT在参数效率方面表现出优势，适用于技术领域。</li>
<li>在检索增强生成（RAG）管道中，CGT较基线模型有更高的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02532">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-38153323ca9d638964707ff5115cbbe4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d098aa5fb485531b4034c9daebf6b2ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1fac99981ed73815b2f69d251466101.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12c18e48385330b634eab9c93e24a9e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6797694190246add4d89ba335e4c79b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49fad9045bf723423fc1a6279017e086.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Accurate-and-Interpretable-Postmenstrual-Age-Prediction-via-Multimodal-Large-Language-Model"><a href="#Accurate-and-Interpretable-Postmenstrual-Age-Prediction-via-Multimodal-Large-Language-Model" class="headerlink" title="Accurate and Interpretable Postmenstrual Age Prediction via Multimodal   Large Language Model"></a>Accurate and Interpretable Postmenstrual Age Prediction via Multimodal   Large Language Model</h2><p><strong>Authors:Qifan Chen, Jin Cui, Cindy Duan, Yushuo Han, Yifei Shi</strong></p>
<p>Accurate estimation of postmenstrual age (PMA) at scan is crucial for assessing neonatal development and health. While deep learning models have achieved high accuracy in predicting PMA from brain MRI, they often function as black boxes, offering limited transparency and interpretability in clinical decision support. In this work, we address the dual challenge of accuracy and interpretability by adapting a multimodal large language model (MLLM) to perform both precise PMA prediction and clinically relevant explanation generation. We introduce a parameter-efficient fine-tuning (PEFT) strategy using instruction tuning and Low-Rank Adaptation (LoRA) applied to the Qwen2.5-VL-7B model. The model is trained on four 2D cortical surface projection maps derived from neonatal MRI scans. By employing distinct prompts for training and inference, our approach enables the MLLM to handle a regression task during training and generate clinically relevant explanations during inference. The fine-tuned model achieves a low prediction error with a 95 percent confidence interval of 0.78 to 1.52 weeks, while producing interpretable outputs grounded in developmental features, marking a significant step toward transparent and trustworthy AI systems in perinatal neuroscience. </p>
<blockquote>
<p>产后年龄（PMA）的准确估计是评估新生儿发育和健康的关键。虽然深度学习模型在根据脑部MRI预测PMA方面已经达到了较高的准确性，但它们通常像黑盒子一样运作，在临床决策支持中提供有限的透明度和解释性。在这项工作中，我们通过适应多模态大型语言模型（MLLM）来解决准确性和解释性的双重挑战，以执行精确的PMA预测和生成与临床相关的解释。我们引入了一种参数有效的微调（PEFT）策略，使用指令调整和应用于Qwen2.5-VL-7B模型的低秩适应（LoRA）。该模型在由新生儿MRI扫描派生的四个二维皮质表面投影图上进行了训练。通过为训练和推理采用不同的提示，我们的方法使MLLM能够在训练时处理回归任务，并在推理时生成与临床相关的解释。经过微调的模型实现了较低的预测误差，95％置信区间为0.78至1.52周，同时产生了基于发育特征的可解释输出，朝着透明和可信赖的围产期神经科学人工智能系统迈出了重要一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02525v1">PDF</a> Submitted to the NeurIPS 2025 Workshop GenAI4Health. Conference   website: <a target="_blank" rel="noopener" href="https://aihealth.ischool.utexas.edu/GenAI4HealthNeurips2025/">https://aihealth.ischool.utexas.edu/GenAI4HealthNeurips2025/</a></p>
<p><strong>Summary</strong></p>
<p>本文关注产后年龄（PMA）的精确估算，这对于评估新生儿发育和健康至关重要。研究采用多模态大型语言模型（MLLM）进行精确PMA预测和生成临床相关解释，解决准确性和可解释性的双重挑战。通过参数有效的微调（PEFT）策略和使用指令调整及低秩适应（LoRA）应用于Qwen2.5-VL-7B模型，该模型在新生儿MRI扫描的四个2D皮质表面投影图上训练。通过为训练和推理采用不同提示，MLLM能够在训练时处理回归任务并在推理时生成与临床相关的解释。微调后的模型预测误差低，95%置信区间为0.78至1.52周，且输出有发育特征作为依据，这在围产神经科学的透明和可信赖的AI系统中迈出了重要一步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>产后年龄（PMA）的精确估计是评估新生儿发育和健康的关键。</li>
<li>多模态大型语言模型（MLLM）被用于解决这一任务，以提高准确性和可解释性。</li>
<li>参数有效的微调（PEFT）策略，包括指令调整和低秩适应（LoRA），被应用于MLLM模型优化。</li>
<li>模型在新生儿MRI扫描的四个2D皮质表面投影图上训练，提高模型的效能。</li>
<li>通过不同的提示进行训练和推理，使模型能够处理回归任务并生成临床相关的解释。</li>
<li>精细调整的模型实现了低预测误差，95%的置信区间在一周内。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02525">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-463742fec61412a4e429428f786293bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8569c3a93b7a4f8cbb242392faa6f99e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-389727ea9871d2840e7fcf43b717919e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3f0ac794bc49c3c8a5ebc52e1699456.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4af04618868f0b77e92e207d2fbc5ab6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65ebe8e95e4697d9fe392a2daadb43f3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Decomposed-Reasoning-with-Reinforcement-Learning-for-Relevance-Assessment-in-UGC-Platforms"><a href="#Decomposed-Reasoning-with-Reinforcement-Learning-for-Relevance-Assessment-in-UGC-Platforms" class="headerlink" title="Decomposed Reasoning with Reinforcement Learning for Relevance   Assessment in UGC Platforms"></a>Decomposed Reasoning with Reinforcement Learning for Relevance   Assessment in UGC Platforms</h2><p><strong>Authors:Xiaowei Yuan, Lei Jin, Haoxin Zhang, Yan Gao, Yi Wu, Yao Hu, Ziyang Huang, Jun Zhao, Kang Liu</strong></p>
<p>Retrieval-augmented generation (RAG) plays a critical role in user-generated content (UGC) platforms, but its effectiveness depends heavily on accurate relevance assessment of query-document pairs. Despite recent advances in applying large language models (LLMs) to relevance modeling, UGC platforms present unique challenges: 1) ambiguous user intent due to sparse user feedback in RAG scenarios, and 2) substantial noise introduced by informal and unstructured language. To address these issues, we propose the Reinforced Reasoning Model for Relevance Assessment (R3A), which introduces a decomposed reasoning framework over queries and candidate documents before scoring. R3A first leverages auxiliary high-ranked documents within the platform to infer latent query intent. It then performs verbatim fragment extraction to justify relevance decisions, thereby reducing errors caused by noisy UGC. Based on a reinforcement learning framework, R3A is optimized to mitigate distortions arising from ambiguous queries and unstructured content. Experimental results show that R3A significantly outperforms existing baseline methods in terms of relevance accuracy, across both offline benchmarks and online experiments. </p>
<blockquote>
<p>检索增强生成（RAG）在用户生成内容（UGC）平台中扮演着关键角色，但其有效性很大程度上取决于查询文档对的相关性评估的准确性。尽管最近将大型语言模型（LLM）应用于相关性建模取得了进展，但UGC平台仍然面临着独特的挑战：1）RAG场景中的用户反馈稀疏导致用户意图模糊，以及2）非正式和非结构化语言引入的大量噪声。为了解决这些问题，我们提出了相关性评估的强化推理模型（R3A），该模型在评分之前引入了针对查询和候选文档的分解推理框架。R3A首先利用平台内辅助的高排名文档来推断潜在的查询意图。然后，它执行逐字片段提取以证明相关性决策，从而减少由嘈杂的UGC造成的错误。基于强化学习框架，R3A进行了优化，以减轻由模糊查询和非结构化内容引起的失真。实验结果表明，R3A在离线基准测试和在线实验中均显著优于现有基线方法，在相关性准确性方面表现出色。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02506v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>RAG在用户生成内容（UGC）平台中扮演重要角色，但其效果取决于查询文档对的相关性评估的准确性。针对UGC平台的独特挑战，如稀疏反馈下的用户意图模糊和大量非正式、非结构化语言引入的噪声，我们提出了强化推理模型（Reinforced Reasoning Model for Relevance Assessment，简称R3A）。R3A采用分解推理框架对查询和候选文档进行评分前的处理。它首先利用平台内辅助的高排名文档来推断潜在的查询意图，然后通过提取字面片段来证明相关性决策，从而减少由嘈杂的UGC引起的错误。基于强化学习框架，R3A被优化用于减轻由模糊查询和非结构化内容引起的失真。实验结果表明，R3A在离线基准测试和在线实验中，在相关性准确性方面显著优于现有基线方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>检索增强生成（RAG）在UGC平台中很重要，但查询文档对的相关性评估是关键。</li>
<li>UGC平台面临两大挑战：稀疏反馈下的用户意图模糊和大量非结构化语言的噪声。</li>
<li>提出强化推理模型（R3A）以应对这些挑战。</li>
<li>R3A利用辅助的高排名文档来推断潜在查询意图。</li>
<li>R3A通过提取字面片段来证明相关性决策，减少错误。</li>
<li>R3A采用强化学习框架进行优化，以减轻模糊查询和非结构化内容的失真。</li>
<li>实验证明，R3A在相关性准确性方面显著优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02506">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5a54a1809d6d0b68239865c66d5f7181.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34084d21a745d9ab7ef1b1a53a72be62.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9fcc1f1aa36031055970a60ad022a2a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ec9be659500bc40558794e0f1e8d711.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="OptiHive-Ensemble-Selection-for-LLM-Based-Optimization-via-Statistical-Modeling"><a href="#OptiHive-Ensemble-Selection-for-LLM-Based-Optimization-via-Statistical-Modeling" class="headerlink" title="OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical   Modeling"></a>OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical   Modeling</h2><p><strong>Authors:Maxime Bouscary, Saurabh Amin</strong></p>
<p>LLM-based solvers have emerged as a promising means of automating problem modeling and solving. However, they remain unreliable and often depend on iterative repair loops that result in significant latency. We introduce OptiHive, an LLM-based framework that produces high-quality solvers for optimization problems from natural-language descriptions without iterative self-correction. OptiHive uses a single batched LLM query to generate diverse components (solvers, problem instances, and validation tests) and filters out erroneous components to ensure fully interpretable outputs. Taking into account the imperfection of the generated components, we employ a statistical model to infer their true performance, enabling principled uncertainty quantification and solver selection. On tasks ranging from traditional optimization problems to challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive significantly outperforms baselines, increasing the optimality rate from 5% to 92% on the most complex problems. </p>
<blockquote>
<p>基于LLM的求解器已经成为自动化问题建模和求解的一种有前途的手段。然而，它们仍然不可靠，并且经常依赖于导致显著延迟的迭代修复循环。我们引入了OptiHive，这是一个基于LLM的框架，它可以从自然语言描述中为优化问题生成高质量求解器，而无需进行迭代自我校正。OptiHive使用单个批处理LLM查询来生成各种组件（求解器、问题实例和验证测试），并过滤掉错误的组件以确保完全可解释的输出。考虑到生成的组件的不完善性，我们采用统计模型来推断它们的真实性能，从而实现有原则的不确定性量化和求解器选择。在从传统优化问题到多仓库车辆路由问题的挑战性变体等任务中，OptiHive显著优于基线，在最复杂的问题上将最优率从5%提高到92%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02503v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLM-based求解器在自动化问题建模和求解方面展现出巨大潜力，但仍存在可靠性和效率问题。OptiHive框架应运而生，可从自然语言描述中生成高质量求解器，无需迭代自我修正。通过一次性批量查询LLM，生成多样化组件并过滤错误组件，确保输出完全可解释。考虑到生成组件的不完美性，采用统计模型推断其真实性能，实现有原则的不确定性量化和求解器选择。在从传统优化问题到多仓库车辆路径问题的各种任务中，OptiHive显著优于基线方法，在复杂问题上将最优率从5%提升至92%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-based求解器在自动化问题建模和求解上具有巨大潜力。</li>
<li>OptiHive框架能够从自然语言描述中生成高质量求解器，无需迭代自我修正。</li>
<li>OptiHive通过一次性批量查询LLM生成多样化组件，提高了效率。</li>
<li>OptiHive能够过滤错误组件，确保输出完全可解释。</li>
<li>生成组件存在不完美性，采用统计模型推断其真实性能。</li>
<li>OptiHive实现了有原则的不确定性量化，有助于求解器选择。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02503">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4a0f6b226a943df314097878e896d12e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4419322f15b6d5f584e2d6349a420ee9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a1ca9171d808b9a5e2a901071a316ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35e7d6f740610ead318746206df0c675.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="The-SMeL-Test-A-simple-benchmark-for-media-literacy-in-language-models"><a href="#The-SMeL-Test-A-simple-benchmark-for-media-literacy-in-language-models" class="headerlink" title="The SMeL Test: A simple benchmark for media literacy in language models"></a>The SMeL Test: A simple benchmark for media literacy in language models</h2><p><strong>Authors:Gustaf Ahdritz, Anat Kleiman</strong></p>
<p>The internet is rife with unattributed, deliberately misleading, or otherwise untrustworthy content. Though large language models (LLMs) are often tasked with autonomous web browsing, the extent to which they have learned the simple heuristics human researchers use to navigate this noisy environment is not currently known. In this paper, we introduce the Synthetic Media Literacy Test (SMeL Test), a minimal benchmark that tests the ability of language models to actively filter out untrustworthy information in context. We benchmark a variety of commonly used instruction-tuned LLMs, including reasoning models, and find that no model consistently trusts more reliable sources; while reasoning in particular is associated with higher scores, even the best API model we test hallucinates up to 70% of the time. Remarkably, larger and more capable models do not necessarily outperform their smaller counterparts. We hope our work sheds more light on this important form of hallucination and guides the development of new methods to combat it. </p>
<blockquote>
<p>互联网充斥着未经证实、故意误导或其他不可信赖的内容。虽然大型语言模型（LLM）通常被赋予自主浏览网页的任务，但它们是否已经学会了人类研究者用来浏览这种嘈杂环境的简单启发式技术，目前尚不清楚。在本文中，我们介绍了合成媒体素养测试（SMeL测试），这是一个最小的基准测试，用于测试语言模型在情境中主动过滤掉不可靠信息的能力。我们对各种常用的指令调整型LLM进行了基准测试，包括推理模型，并发现没有任何模型能始终信任更可靠的来源；虽然推理与更高的分数特别相关，但即使是我们测试的最好的API模型，也有高达70%的时间会出现幻觉。值得注意的是，更大、更先进的模型并不一定比小型模型表现更好。我们希望我们的工作能进一步揭示这种重要的幻觉形式，并为开发新的对抗方法提供指导。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02074v1">PDF</a> </p>
<p><strong>Summary</strong><br>互联网充斥着大量未经授权、故意误导或其他不可靠的内容。大型语言模型（LLM）经常用于自动浏览网页，但它们是否掌握了人类研究者用来应对这种噪声环境的简单启发式方法尚不清楚。本文介绍了合成媒体素养测试（SMeL测试），这是一个最小的基准测试，用于测试语言模型在情境中主动过滤不可靠信息的能力。我们对一系列常用的指令调整LLM进行了基准测试，包括推理模型，发现没有任何模型始终信任更可靠的来源；尤其是推理与更高的分数有关，但即使我们测试的最佳API模型也有高达70%的幻想。值得注意的是，更大的、更强大的模型并不一定优于较小的模型。我们希望我们的工作能更多地揭示这种幻觉的形式，并为开发新的对抗方法提供指导。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在互联网信息筛选方面存在挑战。</li>
<li>合成媒体素养测试（SMeL测试）用于评估语言模型过滤不可靠信息的能力。</li>
<li>现有语言模型在识别可靠信息来源方面表现不一致。</li>
<li>推理能力较强的语言模型得分较高，但仍存在高达70%的幻觉情况。</li>
<li>模型大小与其性能之间不存在必然联系。</li>
<li>互联网上的信息经常未经授权、误导或不可靠。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02074">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-875443754a2917e78567871a9a18aa6c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="“Energon”-Unveiling-Transformers-from-GPU-Power-and-Thermal-Side-Channels"><a href="#“Energon”-Unveiling-Transformers-from-GPU-Power-and-Thermal-Side-Channels" class="headerlink" title="“Energon”: Unveiling Transformers from GPU Power and Thermal   Side-Channels"></a>“Energon”: Unveiling Transformers from GPU Power and Thermal   Side-Channels</h2><p><strong>Authors:Arunava Chaudhuri, Shubhi Shukla, Sarani Bhattacharya, Debdeep Mukhopadhyay</strong></p>
<p>Transformers have become the backbone of many Machine Learning (ML) applications, including language translation, summarization, and computer vision. As these models are increasingly deployed in shared Graphics Processing Unit (GPU) environments via Machine Learning as a Service (MLaaS), concerns around their security grow. In particular, the risk of side-channel attacks that reveal architectural details without physical access remains underexplored, despite the high value of the proprietary models they target. This work to the best of our knowledge is the first to investigate GPU power and thermal fluctuations as side-channels and further exploit them to extract information from pre-trained transformer models. The proposed analysis shows how these side channels can be exploited at user-privilege to reveal critical architectural details such as encoder&#x2F;decoder layer and attention head for both language and vision transformers. We demonstrate the practical impact by evaluating multiple language and vision pre-trained transformers which are publicly available. Through extensive experimental evaluations, we demonstrate that the attack model achieves a high accuracy of over 89% on average for model family identification and 100% for hyperparameter classification, in both single-process as well as noisy multi-process scenarios. Moreover, by leveraging the extracted architectural information, we demonstrate highly effective black-box transfer adversarial attacks with an average success rate exceeding 93%, underscoring the security risks posed by GPU side-channel leakage in deployed transformer models. </p>
<blockquote>
<p>Transformer已成为许多机器学习（ML）应用的核心，包括语言翻译、摘要和计算机视觉。随着这些模型越来越多地通过机器学习服务（MLaaS）在共享图形处理单元（GPU）环境中部署，人们对其安全性越来越担忧。特别是，尽管针对的目标模型具有很高的专有价值，但关于在没有物理访问的情况下揭示架构细节的边信道攻击风险仍然被低估。据我们所知，这项工作首次研究了GPU功率和热波动作为边信道，并进一步利用它们从预训练的Transformer模型中提取信息。提出的分析表明，如何利用这些边信道在用户特权下揭示关键架构细节，如语言和视觉Transformer的编码器&#x2F;解码器层和注意力头。我们通过评估多个公开可用的语言和视觉预训练Transformer来展示其实践影响。通过广泛的实验评估，我们证明攻击模型在模型家族识别方面平均准确率超过89%，在超参数分类方面达到100%，无论是在单进程还是在嘈杂的多进程场景中都是如此。此外，通过利用提取的架构信息，我们展示了高度有效的黑盒转移对抗性攻击，平均成功率超过93%，这突显了部署的Transformer模型中GPU边信道泄漏所带来的安全风险。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01768v1">PDF</a> Accepted at IEEE&#x2F;ACM International Conference on Computer-Aided   Design, 2025</p>
<p><strong>Summary</strong></p>
<p>本文首次探索利用GPU功耗和热波动作为侧通道攻击的方式，以此从预训练的transformer模型中提取信息。研究表明，这种侧通道攻击可在用户权限级别下揭示语言和视觉transformer的编码器&#x2F;解码器层和注意力头等关键架构细节。攻击模型的准确率高达89%以上，在单进程和多进程嘈杂场景下均表现出高效的性能。此外，通过提取架构信息进行的黑箱转移对抗性攻击的成功率平均超过93%，凸显出部署的transformer模型面临的GPU侧通道泄漏风险。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>首次研究GPU功耗和热波动作为侧通道攻击在MLaaS中的预训练Transformer模型。</li>
<li>侧通道攻击可揭示语言和视觉Transformer的关键架构细节，如编码器&#x2F;解码器层和注意力头。</li>
<li>攻击模型在模型家族识别和超参数分类方面的准确率超过89%。</li>
<li>侧通道攻击对单进程和多进程场景均有效。</li>
<li>利用提取的架构信息进行黑箱转移对抗性攻击，成功率超过93%。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01768">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-49bc192f6e55bb329f03a3a6d599d9c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-692fcf647275d94e44aa122fcd2b5540.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-12188dc7b105502fc8a4f13230f6821c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1bbeaa1a6f474106ce3e1e80de5e9ceb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1b65eb4ab7a0e8a314918c42da8c0f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-246a65542610025052f6bef65bc4ba19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d61588a492f1c08da10f1318cfdf37e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84344a33f2e30705deec8eabe031f46a.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Semantic-Encryption-Secure-and-Effective-Interaction-with-Cloud-based-Large-Language-Models-via-Semantic-Transformation"><a href="#Semantic-Encryption-Secure-and-Effective-Interaction-with-Cloud-based-Large-Language-Models-via-Semantic-Transformation" class="headerlink" title="Semantic Encryption: Secure and Effective Interaction with Cloud-based   Large Language Models via Semantic Transformation"></a>Semantic Encryption: Secure and Effective Interaction with Cloud-based   Large Language Models via Semantic Transformation</h2><p><strong>Authors:Dong Chen, Tong Yang, Feipeng Zhai, Pengpeng Ouyang, Qidong Liu, Yafei Li, Chong Fu, Mingliang Xu</strong></p>
<p>The increasing adoption of Cloud-based Large Language Models (CLLMs) has raised significant concerns regarding data privacy during user interactions. While existing approaches primarily focus on encrypting sensitive information, they often overlook the logical structure of user inputs. This oversight can lead to reduced data utility and degraded performance of CLLMs. To address these limitations and enable secure yet effective interactions, we propose Semantic Encryption (SE)-a plug-and-play framework designed to preserve both privacy and utility. SE consists of two key components: Semantic Encoding and Semantic Decoding. In the encoding phase, a lightweight local model transforms the original user input into an alternative semantic context that maintains the original intent and logical structure while obfuscating sensitive information. This transformed input is then processed by the CLLM, which generates a response based on the transformed semantic context. To maintain a seamless user experience, the decoding phase will reconstruct the CLLM’s response back into the original semantic context by referencing the locally stored user input. Extensive experimental evaluations demonstrate that SE effectively protects data privacy without compromising data utility or user experience, offering a practical solution for secure interaction with CLLMs. Particularly, the proposed SE demonstrates a significant improvement over the state-of-the-art InferDPT, surpassing it across various evaluated metrics and datasets. </p>
<blockquote>
<p>随着基于云的的大型语言模型（CLLMs）的日益普及，用户交互过程中的数据隐私问题引发了重大关注。虽然现有的方法主要关注加密敏感信息，但它们往往会忽略用户输入的逻辑结构。这种疏忽可能导致数据效用降低和CLLM性能下降。为了解决这些限制并实现安全而有效的交互，我们提出了语义加密（SE）——一个即插即用框架，旨在保护隐私和效用。SE由两个关键组件构成：语义编码和语义解码。在编码阶段，一个轻型的本地模型将原始用户输入转换为替代的语义上下文，该转换保持了原始意图和逻辑结构，同时掩盖了敏感信息。然后，这个转换后的输入被CLLM处理，基于转换后的语义上下文生成响应。为了保持无缝的用户体验，解码阶段将通过引用本地存储的用户输入将CLLM的响应重构回原始语义上下文。广泛的实验评估表明，SE在有效保护数据隐私的同时，不损害数据效用或用户体验，为与CLLM的安全交互提供了实用解决方案。特别是，所提出的SE在先进指标和数据集上显著改进了InferDPT，超越了它。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01638v1">PDF</a> </p>
<p><strong>Summary</strong><br>云服务中大型语言模型（CLLMs）在交互过程中引发数据隐私关注。现有方法主要关注敏感信息的加密，但忽略了用户输入的内在逻辑结构，可能导致数据效用降低和CLLM性能下降。为此，我们提出语义加密（SE）框架，旨在保护隐私和提高实用性。它包含语义编码和语义解码两个关键组件，能维持用户原始意图和逻辑结构的同时，隐藏敏感信息并解码回应，保障用户体验。实验证明，SE在保护数据隐私方面效果显著，且不影响数据效用和用户体验，为与CLLM的安全交互提供了实用解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>云服务中的大型语言模型（CLLMs）在交互过程中存在数据隐私关切。</li>
<li>现有方法主要关注敏感信息加密，但忽略用户输入的逻辑结构。</li>
<li>语义加密（SE）框架旨在同时保护隐私和提高实用性。</li>
<li>SE包含语义编码和语义解码两个关键组件。</li>
<li>语义编码能维持用户原始意图和逻辑结构，同时隐藏敏感信息。</li>
<li>语义解码能保障CLLM的回应与原始语义上下文一致，提升用户体验。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01638">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7998337db5a4317ba8621db6793d7962.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf52800f939c0787f276d659bdf70b38.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1411b255b3bf6de35ff73771b170eb0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a3057d37792266355bec588cab0fc21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a5e7c41264169f9576dcb9954694886.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="OpenMed-NER-Open-Source-Domain-Adapted-State-of-the-Art-Transformers-for-Biomedical-NER-Across-12-Public-Datasets"><a href="#OpenMed-NER-Open-Source-Domain-Adapted-State-of-the-Art-Transformers-for-Biomedical-NER-Across-12-Public-Datasets" class="headerlink" title="OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers   for Biomedical NER Across 12 Public Datasets"></a>OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers   for Biomedical NER Across 12 Public Datasets</h2><p><strong>Authors:Maziyar Panahi</strong></p>
<p>Named-entity recognition (NER) is fundamental to extracting structured information from the &gt;80% of healthcare data that resides in unstructured clinical notes and biomedical literature. Despite recent advances with large language models, achieving state-of-the-art performance across diverse entity types while maintaining computational efficiency remains a significant challenge. We introduce OpenMed NER, a suite of open-source, domain-adapted transformer models that combine lightweight domain-adaptive pre-training (DAPT) with parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs cost-effective DAPT on a 350k-passage corpus compiled from ethically sourced, publicly available research repositories and de-identified clinical notes (PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA backbones. This is followed by task-specific fine-tuning with LoRA, which updates less than 1.5% of model parameters. We evaluate our models on 12 established biomedical NER benchmarks spanning chemicals, diseases, genes, and species. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of these 12 datasets, with substantial gains across diverse entity types. Our models advance the state-of-the-art on foundational disease and chemical benchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger improvements of over 5.3 and 9.7 percentage points on more specialized gene and clinical cell line corpora. This work demonstrates that strategically adapted open-source models can surpass closed-source solutions. This performance is achieved with remarkable efficiency: training completes in under 12 hours on a single GPU with a low carbon footprint (&lt; 1.2 kg CO2e), producing permissively licensed, open-source checkpoints designed to help practitioners facilitate compliance with emerging data protection and AI regulations, such as the EU AI Act. </p>
<blockquote>
<p>命名实体识别（NER）是从超过80%的驻留在非结构化临床笔记和生物医学文献中的医疗数据中提取结构化信息的基础。尽管最近的大型语言模型有所进展，但在维持计算效率的同时，实现跨多种实体类型的最先进的性能仍然是一个重大挑战。我们引入了OpenMed NER，这是一系列开源的、针对领域进行适应的转换器模型，它结合了轻量级的领域自适应预训练（DAPT）和参数高效的低秩适应（LoRA）。我们的方法以成本效益的方式在由伦理来源的、公开可用的研究仓库和去标识的临床笔记（PubMed、arXiv和MIMIC-III）编译的350k段落语料库上进行DAPT，使用DeBERTa-v3、PubMedBERT和BioELECTRA作为骨干。接着通过LoRA进行针对任务的微调，LoRA更新的模型参数少于1.5%。我们在包含化学、疾病、基因和物种的12个成熟的生物医学NER基准测试集上评估了我们的模型。OpenMed NER在这12个数据集的10个上取得了新的最先进的微观F1分数，在多种实体类型上实现了显著的改进。我们的模型在基础疾病和化学基准测试集上取得了先进的表现（例如，BC5CDR-Disease，+2.7 pp），同时在更专业的基因和临床细胞株语料库上实现了超过5.3和9.7个百分点的改进。这项工作证明，经过战略适应的开源模型可以超越闭源解决方案。这种性能的实现具有惊人的效率：训练在单个GPU上不到12小时内完成，低碳足迹低于1.2公斤二氧化碳当量，产生开放源代码的许可检查点，旨在帮助从业者符合新兴的数据保护和人工智能法规，如欧盟人工智能法案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01630v1">PDF</a> </p>
<p><strong>Summary</strong><br>     开放医疗命名实体识别（OpenMed NER）是一套开源的、适应领域的变压器模型，结合了轻量级领域自适应预训练（DAPT）和参数高效的低阶适应（LoRA）。该模型在伦理来源的公开研究资源库、去标识化的临床笔记（PubMed、arXiv和MIMIC-III）的350k段落语料库上进行经济实惠的DAPT，并使用DeBERTa-v3、PubMedBERT和BioELECTRA作为基础。通过特定任务微调LoRA，更新少于1.5%的模型参数。在12个已建立的生物医学命名实体识别基准测试上评估，OpenMed NER在10个数据集上实现最新微F1分数，并在不同实体类型上取得显著收益。此工作证明战略适应的开源模型可以超越封闭源代码解决方案，且表现高效：在单个GPU上不到12小时内完成训练，碳排放足迹小（&lt; 1.2 kg CO2e），提供符合新兴数据保护和人工智能法规的开放许可检查点，如欧盟人工智能法案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OpenMed NER是开源的、适应领域的变压器模型，用于从非结构化临床笔记和生物医学文献中提取结构化信息。</li>
<li>结合轻量级领域自适应预训练（DAPT）和参数高效的低阶适应（LoRA）。</li>
<li>在伦理来源的公开资源库和去标识化的临床笔记上进行训练。</li>
<li>在多个生物医学命名实体识别基准测试中实现最新微F1分数。</li>
<li>在多种实体类型上取得显著成效，特别是在更专业的基因和临床细胞线语料库上。</li>
<li>战略适应的开源模型表现优于封闭源代码解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01630">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-727b6b61f4c341b37d6a53d69e86bcb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51df8fab053061697a8691bfbfd79bf4.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="LLaDA-MedV-Exploring-Large-Language-Diffusion-Models-for-Biomedical-Image-Understanding"><a href="#LLaDA-MedV-Exploring-Large-Language-Diffusion-Models-for-Biomedical-Image-Understanding" class="headerlink" title="LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical   Image Understanding"></a>LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical   Image Understanding</h2><p><strong>Authors:Xuanzhao Dong, Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Peijie Qiu, Shao Tang, Xin Li, Yalin Wang</strong></p>
<p>Autoregressive models (ARMs) have long dominated the landscape of biomedical vision-language models (VLMs). Recently, masked diffusion models such as LLaDA have emerged as promising alternatives, yet their application in the biomedical domain remains largely underexplored. To bridge this gap, we introduce \textbf{LLaDA-MedV}, the first large language diffusion model tailored for biomedical image understanding through vision instruction tuning. LLaDA-MedV achieves relative performance gains of 7.855% over LLaVA-Med and 1.867% over LLaDA-V in the open-ended biomedical visual conversation task, and sets new state-of-the-art accuracy on the closed-form subset of three VQA benchmarks: 84.93% on VQA-RAD, 92.31% on SLAKE, and 95.15% on PathVQA. Furthermore, a detailed comparison with LLaVA-Med suggests that LLaDA-MedV is capable of generating reasonably longer responses by explicitly controlling response length, which can lead to more informative outputs. We also conduct an in-depth analysis of both the training and inference stages, highlighting the critical roles of initialization weight selection, fine-tuning strategies, and the interplay between sampling steps and response repetition. The code and model weight is released at <a target="_blank" rel="noopener" href="https://github.com/LLM-VLM-GSL/LLaDA-MedV">https://github.com/LLM-VLM-GSL/LLaDA-MedV</a>. </p>
<blockquote>
<p>自回归模型（ARMs）长期以来在生物医学视觉语言模型（VLMs）领域占据主导地位。最近，如LLaDA之类的掩模扩散模型的出现显示出其作为有前途的替代品的潜力，然而它们在生物医学领域的应用仍被大大忽视。为了填补这一空白，我们引入了针对生物医学图像理解设计的首个大型语言扩散模型LLaDA-MedV，通过视觉指令微调来实现。LLaDA-MedV在开放式的生物医学视觉对话任务中相对于LLaVA-Med和LLaDA-V的性能分别提升了7.855%和1.867%，并在三个VQA基准测试集的封闭式子集上达到了新的最先进的准确度：在VQA-RAD上达到84.93%，在SLAKE上达到92.31%，在PathVQA上达到95.15%。此外，与LLaVA-Med的详细比较表明，LLaDA-MedV能够通过明确控制响应长度来生成相对更长的响应，从而可能导致更具信息量的输出。我们还对训练和推理阶段进行了深入分析，强调了初始化权重选择、微调策略以及采样步骤和响应重复之间的相互作用的关键作用。代码和模型权重已在<a target="_blank" rel="noopener" href="https://github.com/LLM-VLM-GSL/LLaDA-MedV%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/LLM-VLM-GSL/LLaDA-MedV发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01617v1">PDF</a> </p>
<p><strong>Summary</strong>：LLaDA-MedV是一款针对生物医学图像理解的语言扩散模型，它通过视觉指令调整实现性能优化。相较于其他模型，LLaDA-MedV在开放和封闭形式的生物医学视觉任务上表现更佳，并在三个VQA基准测试中创造了新的准确率记录。同时，它能够生成较长的响应并控制响应长度，提供更丰富的输出信息。此外，该研究还对训练和推理阶段进行了深入分析，探讨了初始化权重选择、微调策略以及采样步骤和响应重复之间的相互作用。模型代码和权重已发布在指定链接上。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>LLaDA-MedV是专为生物医学图像理解设计的语言扩散模型。</li>
<li>LLaDA-MedV通过视觉指令调整实现性能优化。</li>
<li>LLaDA-MedV在生物医学视觉任务上的表现优于其他模型。</li>
<li>LLaDA-MedV在三个VQA基准测试中创造了新的准确率记录。</li>
<li>LLaDA-MedV能够生成较长的响应并控制响应长度，提供更丰富的输出信息。</li>
<li>研究对LLaDA-MedV的训练和推理阶段进行了深入分析。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01617">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-28f50ce6fbe5010fdd218a258d6057b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8cdde17c0c7023c6476ca6684b57542b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bc842df081978fe43604c02fe95adac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c9c5b7226586101902537bd928e01f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e3a50d503fd1515247626c76e4676af3.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Are-All-Prompt-Components-Value-Neutral-Understanding-the-Heterogeneous-Adversarial-Robustness-of-Dissected-Prompt-in-Large-Language-Models"><a href="#Are-All-Prompt-Components-Value-Neutral-Understanding-the-Heterogeneous-Adversarial-Robustness-of-Dissected-Prompt-in-Large-Language-Models" class="headerlink" title="Are All Prompt Components Value-Neutral? Understanding the Heterogeneous   Adversarial Robustness of Dissected Prompt in Large Language Models"></a>Are All Prompt Components Value-Neutral? Understanding the Heterogeneous   Adversarial Robustness of Dissected Prompt in Large Language Models</h2><p><strong>Authors:Yujia Zheng, Tianhao Li, Haotian Huang, Tianyu Zeng, Jingyu Lu, Chuangxin Chu, Yuekai Huang, Ziyou Jiang, Qian Xiong, Yuyao Ge, Mingyang Li</strong></p>
<p>Prompt-based adversarial attacks have become an effective means to assess the robustness of large language models (LLMs). However, existing approaches often treat prompts as monolithic text, overlooking their structural heterogeneity-different prompt components contribute unequally to adversarial robustness. Prior works like PromptRobust assume prompts are value-neutral, but our analysis reveals that complex, domain-specific prompts with rich structures have components with differing vulnerabilities. To address this gap, we introduce PromptAnatomy, an automated framework that dissects prompts into functional components and generates diverse, interpretable adversarial examples by selectively perturbing each component using our proposed method, ComPerturb. To ensure linguistic plausibility and mitigate distribution shifts, we further incorporate a perplexity (PPL)-based filtering mechanism. As a complementary resource, we annotate four public instruction-tuning datasets using the PromptAnatomy framework, verified through human review. Extensive experiments across these datasets and five advanced LLMs demonstrate that ComPerturb achieves state-of-the-art attack success rates. Ablation studies validate the complementary benefits of prompt dissection and PPL filtering. Our results underscore the importance of prompt structure awareness and controlled perturbation for reliable adversarial robustness evaluation in LLMs. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/Yujiaaaaa/PACP">https://github.com/Yujiaaaaa/PACP</a>. </p>
<blockquote>
<p>基于提示的对抗性攻击已成为评估大型语言模型（LLM）稳健性的有效手段。然而，现有方法通常将提示视为单一文本，忽略了其结构异质性——不同的提示组件对对抗稳健性的贡献是不平等的。像PromptRobust这样的先前工作假设提示是价值中立的，但我们的分析发现，具有丰富结构和特定领域的复杂提示具有不同漏洞的组件。为了解决这一差距，我们引入了PromptAnatomy，这是一个自动化框架，它将提示分解成功能组件，并通过我们提出的ComPerturb方法选择性地扰动每个组件来生成多样且可解释的对抗示例。为了确保语言上的合理性和缓解分布偏移，我们进一步融入了一个基于困惑度（PPL）的过滤机制。作为一个补充资源，我们使用PromptAnatomy框架对四个公共指令调整数据集进行了标注，并通过人工审查进行了验证。在这些数据集和五个先进LLM上进行的大量实验表明，ComPerturb达到了最先进的攻击成功率。消融研究验证了提示分解和PPL过滤的互补效益。我们的结果强调了意识到提示结构的重要性和受控扰动，这对于LLM的可靠对抗性稳健性评估至关重要。代码和数据可在<a target="_blank" rel="noopener" href="https://github.com/Yujiaaaaa/PACP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Yujiaaaaa/PACP找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01554v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对大型语言模型（LLM）的基于提示的对抗性攻击的有效手段，并指出了现有方法的不足。现有方法往往将提示视为单一的文本，忽略了其结构异质性。针对这一问题，本文提出了PromptAnatomy框架，该框架能够自动地将提示分解为功能组件，并通过选择性扰动每个组件来生成多样且可解释的对抗样本。同时，为了确保语言上的合理性和减轻分布偏移，结合困惑度（PPL）过滤机制。在四个公开指令调整数据集和五个先进LLM上的广泛实验表明，ComPerturb方法取得了最先进的攻击成功率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有评估LLM稳健性的方法忽略了提示的结构异质性，即不同提示组件对对抗稳健性的贡献不均等。</li>
<li>PromptAnatomy框架能够自动地将提示分解为功能组件，生成多样且可解释的对抗样本。</li>
<li>ComPerturb方法通过选择性扰动提示的每个组件，增强了对抗性攻击的效果。</li>
<li>困惑度（PPL）过滤机制用于确保语言上的合理性和减轻分布偏移。</li>
<li>在四个公开指令调整数据集和五个先进LLM上的广泛实验表明，ComPerturb方法取得了最先进的攻击成功率。</li>
<li>消融研究验证了提示解剖和PPL过滤的互补效益。</li>
<li>结果强调了在进行LLM的对抗性稳健性评估时，需要考虑提示结构意识和受控扰动的重要性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01554">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f2e327a4f3640da8834fad62edb4975f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e2b28e94ac4a8767664c40af91a0b925.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbf68577785cb9b43f8da90577372207.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca4cf84e985c37577f50b00f2811d3b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01ee7e1e85c3ced0a7d82940674d166d.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Transformers-in-Pseudo-Random-Number-Generation-A-Dual-Perspective-on-Theory-and-Practice"><a href="#Transformers-in-Pseudo-Random-Number-Generation-A-Dual-Perspective-on-Theory-and-Practice" class="headerlink" title="Transformers in Pseudo-Random Number Generation: A Dual Perspective on   Theory and Practice"></a>Transformers in Pseudo-Random Number Generation: A Dual Perspective on   Theory and Practice</h2><p><strong>Authors:Ran Li, Lingshu Zeng</strong></p>
<p>Pseudo-random number generators (PRNGs) are high-nonlinear processes, and they are key blocks in optimization of Large language models. Transformers excel at processing complex nonlinear relationships. Thus it is reasonable to generate high-quality pseudo-random numbers based on transformers. In this paper, we explore this question from both theoretical and practical perspectives, highlighting the potential benefits and implications of Transformer in PRNGs. We theoretically demonstrate that decoder-only Transformer models with Chain-of-Thought can simulate both the Linear Congruential Generator (LCG) and Mersenne Twister (MT) PRNGs. Based on this, we conclude that the log-precision decoder-only Transformer can represent non-uniform $\text{AC}^0$. Our simulative theoretical findings are validated through experiments. The random numbers generated by Transformer-based PRNGs successfully pass the majority of NIST tests, whose heat maps exhibit clear statistical randomness. Finally, we assess their capability in prediction attacks. </p>
<blockquote>
<p>伪随机数生成器（PRNGs）是高非线性过程，是优化大型语言模型的关键环节。Transformer擅长处理复杂的非线性关系。因此，基于Transformer生成高质量伪随机数是有道理的。本文将从理论和实践两个角度探讨这个问题，重点介绍Transformer在PRNGs中的潜在优势和影响。我们从理论上证明，仅解码的Transformer模型结合思维链（Chain-of-Thought）可以模拟线性同余生成器（LCG）和梅森旋转器（MT）PRNGs。基于此，我们得出结论，对数精度仅解码的Transformer可以代表非均匀AC^0。我们的模拟理论发现已经通过实验得到验证。基于Transformer的PRNG生成的随机数成功通过了NIST测试的大部分测试，其热图显示出明显的统计随机性。最后，我们评估了它们在预测攻击方面的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01134v1">PDF</a> 27 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>本文探讨了基于Transformer模型的伪随机数生成器（PRNGs）的潜力与影响。研究表明，仅解码器型的Transformer模型可以模拟线性同余生成器（LCG）和梅森旋花（MT）等PRNGs，并成功生成通过了NIST测试的伪随机数。本文验证了基于Transformer的PRNG在模拟非线性关系方面的优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer模型可用于模拟伪随机数生成器（PRNGs），如线性同余生成器（LCG）和梅森旋花（MT）。</li>
<li>仅解码器型的Transformer能够处理复杂的非线性关系，为高质量的伪随机数生成提供了基础。</li>
<li>基于Transformer的PRNG生成的伪随机数成功通过了NIST测试，表现出明显的统计随机性。</li>
<li>Transformer模型在模拟非均匀分布方面表现出潜力。</li>
<li>本文验证了理论模型的有效性，并通过实验验证了其实际性能。</li>
<li>基于Transformer的PRNGs具有一定的预测攻击防护能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01134">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ed69d6a0ad712b63f916fdeead7b18de.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9667208415f3a6b2b2fd2f0880e5b1ab.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-08-06  Hierarchical Learning-Based Control for Multi-Agent Shepherding of   Stochastic Autonomous Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-69abad54fe13ab2f6b0b805c1919d73e.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-08-06  MedVLThinker Simple Baselines for Multimodal Medical Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25879.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
