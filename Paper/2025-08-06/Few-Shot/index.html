<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  MicroMix Efficient Mixed-Precision Quantization with Microscaling   Formats for Large Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-098648614c4853e987444c1488b0fb78.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    67 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-06-æ›´æ–°"><a href="#2025-08-06-æ›´æ–°" class="headerlink" title="2025-08-06 æ›´æ–°"></a>2025-08-06 æ›´æ–°</h1><h2 id="MicroMix-Efficient-Mixed-Precision-Quantization-with-Microscaling-Formats-for-Large-Language-Models"><a href="#MicroMix-Efficient-Mixed-Precision-Quantization-with-Microscaling-Formats-for-Large-Language-Models" class="headerlink" title="MicroMix: Efficient Mixed-Precision Quantization with Microscaling   Formats for Large Language Models"></a>MicroMix: Efficient Mixed-Precision Quantization with Microscaling   Formats for Large Language Models</h2><p><strong>Authors:Wenyuan Liu, Haoqian Meng, Yilun Luo, Peng Zhang, Xindian Ma</strong></p>
<p>Quantization significantly accelerates inference in large language models (LLMs) by replacing original high-precision matrices with low-precision counterparts. Recent advances in weight-activation quantization have primarily focused on mapping both weights and activations to the INT4 format. Although the new FP4 Tensor Cores in NVIDIAâ€™s Blackwell architecture offer up to 4x speedup over FP16, existing INT4-based kernels fail to fully exploit this capability due to mismatched data formats. To bridge this gap, we propose MicroMix, a co-designed mixed-precision quantization algorithm and matrix multiplication kernel based on Microscaling (MX) data formats. Tailored for the Blackwell architecture, the MicroMix kernel supports arbitrary combinations of MXFP4, MXFP6, and MXFP8 channels, and produces BFloat16 outputs. To achieve a favorable trade-off between accuracy and efficiency for each linear layer, we introduce quantization thresholds that identify activation elements where lower-precision formats (MXFP4 or MXFP6) incur excessive quantization error. Our algorithm selectively allocates higher-precision channels to preserve accuracy while maintaining compute efficiency. MicroMix achieves competitive or superior performance across diverse downstream tasks, including zero-shot and few-shot learning, language modeling, code generation, and mathematical reasoning. On both consumer-grade (RTX 5070Ti laptop) and server-grade (RTX 5090) GPUs, our kernel delivers at least 20% faster execution than TensorRT-FP8. Furthermore, when applied to various Llama and Qwen models, MicroMix consistently improves prefill latency and memory efficiency across a range of batch sizes compared to TensorRT baselines. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/lwy2020/MicroMix">https://github.com/lwy2020/MicroMix</a>. </p>
<blockquote>
<p>é‡åŒ–é€šè¿‡ç”¨ä½ç²¾åº¦çŸ©é˜µæ›¿æ¢åŸå§‹çš„é«˜ç²¾åº¦çŸ©é˜µï¼Œä»è€Œæ˜¾è‘—åŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨æ–­ã€‚æƒé‡æ¿€æ´»é‡åŒ–çš„æœ€æ–°è¿›å±•ä¸»è¦é›†ä¸­åœ¨å°†æƒé‡å’Œæ¿€æ´»æ˜ å°„åˆ°INT4æ ¼å¼ä¸Šã€‚å°½ç®¡NVIDIA Blackwellæ¶æ„ä¸­çš„æ–°å‹FP4 Tensor Coresæ¯”FP16æä¾›é«˜è¾¾4å€çš„åŠ é€Ÿï¼Œä½†ç”±äºæ•°æ®æ ¼å¼ä¸åŒ¹é…ï¼Œç°æœ‰çš„åŸºäºINT4çš„æ ¸æ— æ³•å……åˆ†åˆ©ç”¨è¿™ä¸€åŠŸèƒ½ã€‚ä¸ºäº†å¼¥åˆè¿™ä¸€é¸¿æ²Ÿï¼Œæˆ‘ä»¬æå‡ºäº†MicroMixï¼Œè¿™æ˜¯ä¸€ç§å…±åŒè®¾è®¡çš„æ··åˆç²¾åº¦é‡åŒ–ç®—æ³•å’ŒåŸºäºMicroscalingï¼ˆMXï¼‰æ•°æ®æ ¼å¼çš„çŸ©é˜µä¹˜æ³•æ ¸ã€‚é’ˆå¯¹Blackwellæ¶æ„è¿›è¡Œå®šåˆ¶ï¼ŒMicroMixæ ¸æ”¯æŒMXFP4ã€MXFP6å’ŒMXFP8é€šé“çš„ä»»æ„ç»„åˆï¼Œå¹¶äº§ç”ŸBFloat16è¾“å‡ºã€‚ä¸ºäº†åœ¨æ¯ä¸ªçº¿æ€§å±‚ä¹‹é—´å®ç°å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†é‡åŒ–é˜ˆå€¼ï¼Œä»¥ç¡®å®šåœ¨æ¿€æ´»å…ƒç´ ä¸­ï¼Œä½¿ç”¨è¾ƒä½ç²¾åº¦æ ¼å¼ï¼ˆMXFP4æˆ–MXFP6ï¼‰ä¼šå¯¼è‡´è¿‡å¤šçš„é‡åŒ–è¯¯å·®çš„ä½ç½®ã€‚æˆ‘ä»¬çš„ç®—æ³•æœ‰é€‰æ‹©åœ°åˆ†é…è¾ƒé«˜ç²¾åº¦çš„é€šé“ä»¥ä¿ç•™å‡†ç¡®æ€§å¹¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚MicroMixåœ¨åŒ…æ‹¬é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ã€è¯­è¨€å»ºæ¨¡ã€ä»£ç ç”Ÿæˆå’Œæ•°å­¦æ¨ç†åœ¨å†…çš„å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›æˆ–å‡ºè‰²çš„æ€§èƒ½ã€‚æ— è®ºæ˜¯åœ¨æ¶ˆè´¹è€…çº§ï¼ˆRTX 5070Tiç¬”è®°æœ¬ç”µè„‘ï¼‰è¿˜æ˜¯æœåŠ¡å™¨çº§ï¼ˆRTX 5090ï¼‰GPUä¸Šï¼Œæˆ‘ä»¬çš„æ ¸çš„æ‰§è¡Œé€Ÿåº¦è‡³å°‘æ¯”TensorRT-FP8å¿«20%ã€‚æ­¤å¤–ï¼Œå½“åº”ç”¨äºå„ç§Llamaå’ŒQwenæ¨¡å‹æ—¶ï¼Œä¸TensorRTåŸºå‡†ç›¸æ¯”ï¼ŒMicroMixåœ¨å„ç§æ‰¹æ¬¡å¤§å°çš„æƒ…å†µä¸‹å§‹ç»ˆæ”¹è¿›äº†é¢„å¡«å……å»¶è¿Ÿå’Œå†…å­˜æ•ˆç‡ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lwy2020/MicroMix%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/lwy2020/MicroMixæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02343v1">PDF</a> 12 pages</p>
<p><strong>æ‘˜è¦</strong><br>é‡åŒ–é€šè¿‡ä½¿ç”¨ä½ç²¾åº¦çŸ©é˜µæ›¿ä»£åŸå§‹çš„é«˜ç²¾åº¦çŸ©é˜µï¼Œæ˜¾è‘—åŠ é€Ÿäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†é€Ÿåº¦ã€‚æœ€è¿‘å…³äºæƒé‡æ¿€æ´»é‡åŒ–çš„è¿›å±•ä¸»è¦é›†ä¸­åœ¨å°†æƒé‡å’Œæ¿€æ´»æ˜ å°„åˆ°INT4æ ¼å¼ä¸Šã€‚å°½ç®¡NVIDIAçš„Blackwellæ¶æ„ä¸­çš„æ–°å‹FP4 Tensor Coresæ¯”FP16æä¾›äº†é«˜è¾¾4å€çš„é€Ÿåº¦æå‡ï¼Œä½†ç”±äºæ•°æ®æ ¼å¼ä¸åŒ¹é…ï¼Œç°æœ‰çš„INT4å†…æ ¸æ— æ³•å……åˆ†åˆ©ç”¨æ­¤åŠŸèƒ½ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†MicroMixï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹Blackwellæ¶æ„çš„æ··åˆç²¾åº¦é‡åŒ–ç®—æ³•å’ŒåŸºäºMicroscalingï¼ˆMXï¼‰æ•°æ®æ ¼å¼çš„çŸ©é˜µä¹˜æ³•å†…æ ¸ã€‚MicroMixå†…æ ¸æ”¯æŒMXFP4ã€MXFP6å’ŒMXFP8é€šé“çš„ä»»æ„ç»„åˆï¼Œå¹¶äº§ç”ŸBFloat16è¾“å‡ºã€‚ä¸ºäº†é’ˆå¯¹æ¯ä¸ªçº¿æ€§å±‚å®ç°å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´çš„å¹³è¡¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†é‡åŒ–é˜ˆå€¼æ¥è¯†åˆ«åœ¨è¾ƒä½ç²¾åº¦æ ¼å¼ï¼ˆMXFP4æˆ–MXFP6ï¼‰ä¸‹å‡ºç°è¿‡å¤§é‡åŒ–è¯¯å·®çš„æ¿€æ´»å…ƒç´ ã€‚æˆ‘ä»¬çš„ç®—æ³•æœ‰é€‰æ‹©åœ°ä¸ºä¿ç•™å‡†ç¡®æ€§åˆ†é…è¾ƒé«˜ç²¾åº¦é€šé“çš„åŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚MicroMixåœ¨åŒ…æ‹¬é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ã€è¯­è¨€å»ºæ¨¡ã€ä»£ç ç”Ÿæˆå’Œæ•°å­¦æ¨ç†ç­‰ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ä¸­å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ç”šè‡³æ›´å‡ºè‰²çš„æ€§èƒ½ã€‚æ— è®ºæ˜¯åœ¨æ¶ˆè´¹è€…çº§ï¼ˆRTX 5070Tiç¬”è®°æœ¬ç”µè„‘ï¼‰è¿˜æ˜¯æœåŠ¡å™¨çº§ï¼ˆRTX 5090ï¼‰GPUä¸Šï¼Œæˆ‘ä»¬çš„å†…æ ¸çš„æ‰§è¡Œé€Ÿåº¦éƒ½æ¯”TensorRT-FP8è‡³å°‘å¿«20ï¼…ã€‚æ­¤å¤–ï¼Œå½“åº”ç”¨äºå„ç§Llamaå’ŒQwenæ¨¡å‹æ—¶ï¼Œä¸TensorRTåŸºçº¿ç›¸æ¯”ï¼ŒMicroMixåœ¨è·¨ä¸åŒæ‰¹æ¬¡å¤§å°çš„æƒ…å†µä¸‹å§‹ç»ˆæé«˜äº†é¢„å¡«å……å»¶è¿Ÿå’Œå†…å­˜æ•ˆç‡ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lwy2020/MicroMix">https://github.com/lwy2020/MicroMix</a>æ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é‡åŒ–èƒ½æ˜¾è‘—åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>å½“å‰INT4å†…æ ¸æ— æ³•å®Œå…¨åˆ©ç”¨NVIDIA Blackwellæ¶æ„ä¸­çš„FP4 Tensor Coresçš„ä¼˜åŠ¿ã€‚</li>
<li>MicroMixæ˜¯ä¸€ç§é’ˆå¯¹Blackwellæ¶æ„çš„æ··åˆç²¾åº¦é‡åŒ–ç®—æ³•å’ŒçŸ©é˜µä¹˜æ³•å†…æ ¸ã€‚</li>
<li>MicroMixæ”¯æŒå¤šç§æ•°æ®æ ¼å¼ç»„åˆï¼Œå¹¶äº§ç”ŸBFloat16è¾“å‡ºã€‚</li>
<li>é€šè¿‡å¼•å…¥é‡åŒ–é˜ˆå€¼ï¼ŒMicroMixå®ç°äº†çº¿æ€§å±‚ä¹‹é—´çš„å‡†ç¡®æ€§å’Œæ•ˆç‡å¹³è¡¡ã€‚</li>
<li>MicroMixåœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬å­¦ä¹ ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02343">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0352e83eb99eec9db5231dc61bddd9c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc21568d6526675d64b4ac3d10a1bbc6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7f12776d26f5d5ac777f49452ea38ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31dbe936fdf0b371d274d024d962fb53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f27bda080ab0fc5ef751fcfd6af1ad1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7360e2b617b61fef9dfb225ecccea4ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93f39324a3b203da343ba88514935b38.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-247388e77a5b109494dbb8b0593f2d68.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-411ba0a5d1fed03ad250619325c46d3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9eb887bbe18ed64f1bb7445815560e94.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="OccamVTS-Distilling-Vision-Models-to-1-Parameters-for-Time-Series-Forecasting"><a href="#OccamVTS-Distilling-Vision-Models-to-1-Parameters-for-Time-Series-Forecasting" class="headerlink" title="OccamVTS: Distilling Vision Models to 1% Parameters for Time Series   Forecasting"></a>OccamVTS: Distilling Vision Models to 1% Parameters for Time Series   Forecasting</h2><p><strong>Authors:Sisuo Lyu, Siru Zhong, Weilin Ruan, Qingxiang Liu, Qingsong Wen, Hui Xiong, Yuxuan Liang</strong></p>
<p>Time series forecasting is fundamental to diverse applications, with recent approaches leverage large vision models (LVMs) to capture temporal patterns through visual representations. We reveal that while vision models enhance forecasting performance, 99% of their parameters are unnecessary for time series tasks. Through cross-modal analysis, we find that time series align with low-level textural features but not high-level semantics, which can impair forecasting accuracy. We propose OccamVTS, a knowledge distillation framework that extracts only the essential 1% of predictive information from LVMs into lightweight networks. Using pre-trained LVMs as privileged teachers, OccamVTS employs pyramid-style feature alignment combined with correlation and feature distillation to transfer beneficial patterns while filtering out semantic noise. Counterintuitively, this aggressive parameter reduction improves accuracy by eliminating overfitting to irrelevant visual features while preserving essential temporal patterns. Extensive experiments across multiple benchmark datasets demonstrate that OccamVTS consistently achieves state-of-the-art performance with only 1% of the original parameters, particularly excelling in few-shot and zero-shot scenarios. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—é¢„æµ‹åœ¨å¤šç§åº”ç”¨ä¸­å…·æœ‰æ ¹æœ¬é‡è¦æ€§ã€‚æœ€è¿‘çš„æ–¹æ³•åˆ©ç”¨å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMsï¼‰é€šè¿‡è§†è§‰è¡¨ç¤ºæ¥æ•æ‰æ—¶é—´åºåˆ—çš„æ¨¡å¼ã€‚æˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶è§†è§‰æ¨¡å‹æé«˜äº†é¢„æµ‹æ€§èƒ½ï¼Œä½†å…¶99%çš„å‚æ•°å¯¹æ—¶é—´åºåˆ—ä»»åŠ¡æ˜¯ä¸å¿…è¦çš„ã€‚é€šè¿‡è·¨æ¨¡æ€åˆ†æï¼Œæˆ‘ä»¬å‘ç°æ—¶é—´åºåˆ—ä¸ä½çº§åˆ«çš„çº¹ç†ç‰¹å¾ç›¸ç¬¦ï¼Œè€Œä¸é«˜çº§åˆ«çš„è¯­ä¹‰ç‰¹å¾ä¸ç¬¦ï¼Œè¿™å¯èƒ½ä¼šæŸå®³é¢„æµ‹å‡†ç¡®æ€§ã€‚æˆ‘ä»¬æå‡ºäº†OccamVTSï¼Œè¿™æ˜¯ä¸€ä¸ªçŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œä»…ä»å¤§å‹è§†è§‰æ¨¡å‹ä¸­æå–å…³é”®çš„1%é¢„æµ‹ä¿¡æ¯åˆ°è½»é‡çº§ç½‘ç»œä¸­ã€‚ä½¿ç”¨é¢„è®­ç»ƒçš„å¤§å‹è§†è§‰æ¨¡å‹ä½œä¸ºç‰¹æƒæ•™å¸ˆï¼ŒOccamVTSé‡‡ç”¨é‡‘å­—å¡”é£æ ¼çš„ç‰¹å¾å¯¹é½ï¼Œç»“åˆç›¸å…³æ€§å’Œç‰¹å¾è’¸é¦æ¥è½¬ç§»æœ‰ç›Šçš„æ¨¡å¼ï¼ŒåŒæ—¶è¿‡æ»¤æ‰è¯­ä¹‰å™ªå£°ã€‚è¿™ç§å‡å°‘å‚æ•°çš„æ¿€è¿›åšæ³•å‡ºäººæ„æ–™åœ°æé«˜äº†å‡†ç¡®æ€§ï¼Œå®ƒé€šè¿‡æ¶ˆé™¤å¯¹æ— å…³è§†è§‰ç‰¹å¾çš„è¿‡åº¦æ‹ŸåˆåŒæ—¶ä¿ç•™å…³é”®çš„æ—¶é—´æ¨¡å¼æ¥æé«˜å‡†ç¡®æ€§ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒOccamVTSå§‹ç»ˆè¾¾åˆ°äº†æœ€æ–°çš„æŠ€æœ¯æ°´å¹³ï¼Œä»…ä½¿ç”¨åŸå§‹å‚æ•°çš„ç™¾åˆ†ä¹‹ä¸€ï¼Œå°¤å…¶åœ¨å°æ ·å’Œé›¶æ ·æœ¬åœºæ™¯ä¸­è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01727v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘æœŸæ–¹æ³•åˆ©ç”¨å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMsï¼‰æ•æ‰æ—¶é—´åºåˆ—çš„ä¸´æ—¶æ¨¡å¼æ¥è¿›è¡Œé¢„æµ‹ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶è§†è§‰æ¨¡å‹èƒ½æé«˜é¢„æµ‹æ€§èƒ½ï¼Œä½†å…¶99%çš„å‚æ•°å¯¹æ—¶é—´åºåˆ—ä»»åŠ¡æ¥è¯´æ˜¯å†—ä½™çš„ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºOccamVTSçš„çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½ä»LVMsä¸­æå–ä»…1%çš„é¢„æµ‹ä¿¡æ¯åˆ°è½»é‡çº§ç½‘ç»œä¸­ã€‚å€ŸåŠ©é¢„è®­ç»ƒçš„LVMä½œä¸ºç‰¹æƒæ•™å¸ˆç½‘ç»œï¼ŒOccamVTSé‡‡ç”¨é‡‘å­—å¡”é£æ ¼çš„ç‰¹å¾å¯¹é½ç»“åˆç›¸å…³æ€§å’Œç‰¹å¾è’¸é¦æ¥è½¬ç§»æœ‰ç›Šæ¨¡å¼ï¼ŒåŒæ—¶è¿‡æ»¤æ‰è¯­ä¹‰å™ªå£°ã€‚é€šè¿‡å‡å°‘å‚æ•°ï¼ŒOccamVTSæé«˜äº†å‡†ç¡®æ€§ï¼Œå› ä¸ºå»é™¤äº†å¯¹æ— å…³è§†è§‰ç‰¹å¾çš„è¿‡åº¦æ‹Ÿåˆï¼ŒåŒæ—¶ä¿ç•™äº†å…³é”®çš„æ—¶é—´æ¨¡å¼ã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒOccamVTSåœ¨ä»…ä½¿ç”¨åŸå§‹å‚æ•°çš„1%çš„æƒ…å†µä¸‹æŒç»­å–å¾—å“è¶Šæ€§èƒ½ï¼Œå°¤å…¶åœ¨å°æ ·å°ºå¯¸å’Œé›¶æ ·æœ¬åœºæ™¯ä¸­è¡¨ç°çªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMsï¼‰èƒ½æœ‰æ•ˆæ•æ‰æ—¶é—´åºåˆ—çš„ä¸´æ—¶æ¨¡å¼ï¼Œä½†å¤§éƒ¨åˆ†å‚æ•°å¯¹æ—¶é—´åºåˆ—ä»»åŠ¡æ˜¯å†—ä½™çš„ã€‚</li>
<li>çŸ¥è¯†è’¸é¦æ¡†æ¶OccamVTSèƒ½ä»LVMä¸­æå–å…³é”®é¢„æµ‹ä¿¡æ¯åˆ°è½»é‡çº§ç½‘ç»œã€‚</li>
<li>OccamVTSé‡‡ç”¨é‡‘å­—å¡”é£æ ¼çš„ç‰¹å¾å¯¹é½ç»“åˆç›¸å…³æ€§å’Œç‰¹å¾è’¸é¦æ¥è½¬ç§»æœ‰ç›Šæ¨¡å¼å¹¶è¿‡æ»¤è¯­ä¹‰å™ªå£°ã€‚</li>
<li>é€šè¿‡å‡å°‘å‚æ•°ï¼ŒOccamVTSæé«˜äº†é¢„æµ‹å‡†ç¡®æ€§ï¼Œé€šè¿‡æ¶ˆé™¤å¯¹æ— å…³ç‰¹å¾çš„è¿‡åº¦æ‹Ÿåˆå¹¶ä¿ç•™å…³é”®æ—¶é—´æ¨¡å¼ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒOccamVTSåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†å“è¶Šæ€§èƒ½ï¼Œå°¤å…¶æ˜¯å°æ ·å°ºå¯¸å’Œé›¶æ ·æœ¬åœºæ™¯ä¸‹çš„è¡¨ç°çªå‡ºã€‚</li>
<li>ç ”ç©¶å‘ç°æ—¶é—´åºåˆ—ä¸ä½çº§åˆ«çº¹ç†ç‰¹å¾æœ‰å…³ï¼Œè€Œä¸é«˜çº§è¯­ä¹‰å…³è”ä¸å¤§ï¼Œåè€…å¯èƒ½æŸå®³é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01727">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-366ca7bcd9342141d392493a24a6dc51.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e8e061c4ddb4f1c870afb59f9ee9391a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc81af021ecafe6e8dc2ad8fa290940a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d448278136b7b149bed2cf489457e6c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6948da3a59de6c4bf1bde7db46d7ee57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d73031c12c28a3f5df85f92727845e15.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="EvoVLMA-Evolutionary-Vision-Language-Model-Adaptation"><a href="#EvoVLMA-Evolutionary-Vision-Language-Model-Adaptation" class="headerlink" title="EvoVLMA: Evolutionary Vision-Language Model Adaptation"></a>EvoVLMA: Evolutionary Vision-Language Model Adaptation</h2><p><strong>Authors:Kun Ding, Ying Wang, Shiming Xiang</strong></p>
<p>Pre-trained Vision-Language Models (VLMs) have been exploited in various Computer Vision tasks (e.g., few-shot recognition) via model adaptation, such as prompt tuning and adapters. However, existing adaptation methods are designed by human experts, requiring significant time cost and experience. Inspired by recent advances in Large Language Models (LLMs) based code generation, we propose an Evolutionary Vision-Language Model Adaptation (EvoVLMA) method to automatically search training-free efficient adaptation algorithms for VLMs. We recognize feature selection and logits computation as the key functions in training-free VLM adaptation, and propose a two-stage LLM-assisted evolutionary algorithm for optimizing these parts in a sequential manner, effectively addressing the challenge posed by the expansive search space through a divide-and-conquer strategy. Besides, to enhance the stability and efficiency of searching process, we propose low-precision code conversion, web based code execution and process monitoring, leading to a highly effective automatic algorithm design system. Extensive experiments demonstrate that the algorithms found by EvoVLMA can obtain promising results compared to previous manually-designed ones. More specifically, in the 8-shot image classification setting, the classical APE algorithm can be improved by 1.91 points in recognition accuracy. This research opens new possibilities for automating the optimization of adaptation algorithms of pre-trained multimodal models. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/kding1225/EvoVLMA">https://github.com/kding1225/EvoVLMA</a> </p>
<blockquote>
<p>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é€šè¿‡å„ç§æ¨¡å‹é€‚é…æ–¹æ³•å·²åœ¨å„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œå°æ ·æœ¬è¯†åˆ«ï¼‰ä¸­å¾—åˆ°åº”ç”¨ï¼Œå¦‚æç¤ºè°ƒæ•´å’Œé€‚é…å™¨ç­‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é€‚é…æ–¹æ³•æ˜¯ç”±äººç±»ä¸“å®¶è®¾è®¡çš„ï¼Œéœ€è¦å¤§é‡çš„æ—¶é—´å’Œç»éªŒã€‚å—åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç ç”Ÿæˆçš„æœ€æ–°è¿›å±•çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¿›åŒ–è§†è§‰è¯­è¨€æ¨¡å‹é€‚é…ï¼ˆEvoVLMAï¼‰æ–¹æ³•ï¼Œå¯è‡ªåŠ¨æœç´¢æ— éœ€è®­ç»ƒçš„VLMsçš„é«˜æ•ˆé€‚é…ç®—æ³•ã€‚æˆ‘ä»¬è®¤è¯†åˆ°ç‰¹å¾é€‰æ‹©å’Œé€»è¾‘è®¡ç®—æ˜¯æ— éœ€è®­ç»ƒçš„VLMé€‚é…ä¸­çš„å…³é”®åŠŸèƒ½ï¼Œå¹¶æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„LLMè¾…åŠ©è¿›åŒ–ç®—æ³•ï¼ŒæŒ‰åºä¼˜åŒ–è¿™äº›éƒ¨åˆ†ï¼Œé€šè¿‡åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥æœ‰æ•ˆåœ°è§£å†³äº†ç”±å·¨å¤§çš„æœç´¢ç©ºé—´å¸¦æ¥çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜æœç´¢è¿‡ç¨‹çš„ç¨³å®šæ€§å’Œæ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä½ç²¾åº¦ä»£ç è½¬æ¢ã€åŸºäºç½‘ç»œçš„ä»£ç æ‰§è¡Œå’Œè¿›ç¨‹ç›‘æ§ï¼Œä»è€Œå»ºç«‹äº†ä¸€ä¸ªé«˜æ•ˆè‡ªåŠ¨çš„ç®—æ³•è®¾è®¡ç³»ç»Ÿã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸ä»¥å‰çš„æ‰‹åŠ¨è®¾è®¡ç®—æ³•ç›¸æ¯”ï¼ŒEvoVLMAæ‰¾åˆ°çš„ç®—æ³•å¯ä»¥è·å¾—æœ‰å‰æ™¯çš„ç»“æœã€‚æ›´å…·ä½“åœ°è¯´ï¼Œåœ¨8æ¬¡å°„å‡»çš„å›¾åƒåˆ†ç±»è®¾ç½®ä¸­ï¼Œç»å…¸APEç®—æ³•çš„è¯†åˆ«å‡†ç¡®ç‡å¯ä»¥æé«˜1.91ä¸ªç‚¹ã€‚è¿™é¡¹ç ”ç©¶ä¸ºè‡ªåŠ¨ä¼˜åŒ–é¢„è®­ç»ƒå¤šæ¨¡æ€æ¨¡å‹çš„é€‚é…ç®—æ³•å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/kding1225/EvoVLMA">https://github.com/kding1225/EvoVLMA</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01558v1">PDF</a> This paper has been accepted by ACM Multimedia 2025 (ACM MM 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºè¿›åŒ–ç®—æ³•çš„è‡ªåŠ¨è§†è§‰è¯­è¨€æ¨¡å‹é€‚åº”æ–¹æ³•ï¼ˆEvoVLMAï¼‰ï¼Œç”¨äºä¸ºé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹è¿›è¡Œé«˜æ•ˆçš„æ¨¡å‹é€‚åº”ã€‚EvoVLMAè¯†åˆ«äº†ç‰¹å¾é€‰æ‹©å’Œè®¡ç®—logitsä½œä¸ºå…³é”®ä»»åŠ¡ï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µLLMè¾…åŠ©è¿›åŒ–ç®—æ³•æ¥ä¼˜åŒ–è¿™äº›éƒ¨åˆ†ã€‚é€šè¿‡ä»£ç è½¬æ¢ã€åŸºäºç½‘ç»œçš„ä»£ç æ‰§è¡Œå’Œè¿‡ç¨‹ç›‘æ§ç­‰æŠ€æœ¯ï¼Œæé«˜äº†æœç´¢è¿‡ç¨‹çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç›¸å¯¹äºæ‰‹åŠ¨è®¾è®¡çš„æ–¹æ³•è·å¾—è¾ƒå¥½æ•ˆæœï¼Œå¦‚é€šè¿‡è¿›åŒ–ç®—æ³•æ”¹è¿›çš„APEç®—æ³•åœ¨å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„å‡†ç¡®åº¦æå‡è¾¾åˆ°äº†çº¦è¿‘äºŒåä¸ªç‚¹ã€‚è¿™å¼€åˆ›äº†è‡ªåŠ¨ä¼˜åŒ–é¢„è®­ç»ƒå¤šæ¨¡æ€æ¨¡å‹è‡ªé€‚åº”ç®—æ³•çš„æ–°å¯èƒ½ã€‚æ›´å¤šç»†èŠ‚è¯·å‚è§æ–‡ä¸­æè¿°åŠGitHubä»“åº“åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/kding1225/EvoVLMA%E3%80%82">https://github.com/kding1225/EvoVLMAã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EvoVLMAæ˜¯ä¸€ç§åŸºäºè¿›åŒ–ç®—æ³•çš„è‡ªåŠ¨è§†è§‰è¯­è¨€æ¨¡å‹é€‚åº”æ–¹æ³•ï¼Œé€‚ç”¨äºé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„è‡ªé€‚åº”é—®é¢˜ã€‚</li>
<li>EvoVLMAé€šè¿‡è¯†åˆ«ç‰¹å¾é€‰æ‹©å’Œè®¡ç®—logitsä½œä¸ºå…³é”®ä»»åŠ¡å¹¶é‡‡ç”¨ä¸¤é˜¶æ®µLLMè¾…åŠ©è¿›åŒ–ç®—æ³•è¿›è¡Œä¼˜åŒ–ï¼Œæœ‰æ•ˆè§£å†³äº†è®­ç»ƒè‡ªé€‚åº”æ¨¡å‹çš„æœç´¢ç©ºé—´é—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†ä½ç²¾åº¦ä»£ç è½¬æ¢ã€åŸºäºç½‘ç»œçš„ä»£ç æ‰§è¡Œå’Œè¿‡ç¨‹ç›‘æ§ç­‰æŠ€æœ¯æ¥æé«˜æœç´¢è¿‡ç¨‹çš„ç¨³å®šæ€§å’Œæ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01558">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8ff81b06b129122b00069e072ed44b3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8087750705c975860b581a3e1a82a8b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12dfe979ef4c330de6793f451318cc36.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c202189e14c16161ba58cc6a1a09a85c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ccd65fb29e52a734f530e3faf12ab9b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Instruction-based-Time-Series-Editing"><a href="#Instruction-based-Time-Series-Editing" class="headerlink" title="Instruction-based Time Series Editing"></a>Instruction-based Time Series Editing</h2><p><strong>Authors:Jiaxing Qiu, Dongliang Guo, Brynne Sullivan, Teague R. Henry, Tom Hartvigsen</strong></p>
<p>In time series editing, we aim to modify some properties of a given time series without altering others. For example, when analyzing a hospital patientâ€™s blood pressure, we may add a sudden early drop and observe how it impacts their future while preserving other conditions. Existing diffusion-based editors rely on rigid, predefined attribute vectors as conditions and produce all-or-nothing edits through sampling. This attribute- and sampling-based approach limits flexibility in condition format and lacks customizable control over editing strength. To overcome these limitations, we introduce Instruction-based Time Series Editing, where users specify intended edits using natural language. This allows users to express a wider range of edits in a more accessible format. We then introduce InstructTime, the first instruction-based time series editor. InstructTime takes in time series and instructions, embeds them into a shared multi-modal representation space, then decodes their embeddings to generate edited time series. By learning a structured multi-modal representation space, we can easily interpolate between embeddings to achieve varying degrees of edit. To handle local and global edits together, we propose multi-resolution encoders. In our experiments, we use synthetic and real datasets and find that InstructTime is a state-of-the-art time series editor: InstructTime achieves high-quality edits with controllable strength, can generalize to unseen instructions, and can be easily adapted to unseen conditions through few-shot learning. </p>
<blockquote>
<p>åœ¨æ—¶é—´åºåˆ—ç¼–è¾‘ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¯¹ç»™å®šçš„æ—¶é—´åºåˆ—çš„æŸäº›å±æ€§è¿›è¡Œä¿®æ”¹ï¼Œè€Œä¸æ”¹å˜å…¶ä»–å±æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨åˆ†æåŒ»é™¢æ‚£è€…çš„è¡€å‹æ—¶ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå¢åŠ ä¸€ä¸ªçªç„¶çš„æ—©æœŸä¸‹é™ï¼Œå¹¶è§‚å¯Ÿå®ƒå¦‚ä½•å½±å“æœªæ¥ï¼ŒåŒæ—¶ä¿æŒå…¶ä»–çŠ¶å†µä¸å˜ã€‚ç°æœ‰çš„åŸºäºæ‰©æ•£çš„ç¼–è¾‘å™¨ä¾èµ–äºåƒµç¡¬çš„é¢„å®šä¹‰å±æ€§å‘é‡ä½œä¸ºæ¡ä»¶ï¼Œå¹¶é€šè¿‡é‡‡æ ·è¿›è¡Œå…¨æœ‰æˆ–å…¨æ— çš„ç¼–è¾‘ã€‚è¿™ç§åŸºäºå±æ€§å’Œé‡‡æ ·çš„æ–¹æ³•é™åˆ¶äº†æ¡ä»¶æ ¼å¼çµæ´»æ€§ï¼Œå¹¶ç¼ºä¹å¯¹ç¼–è¾‘å¼ºåº¦çš„å¯å®šåˆ¶æ§åˆ¶ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæŒ‡ä»¤çš„æ—¶é—´åºåˆ—ç¼–è¾‘ï¼Œç”¨æˆ·å¯ä»¥ä½¿ç”¨è‡ªç„¶è¯­è¨€æŒ‡å®šé¢„æœŸçš„ç¼–è¾‘å†…å®¹ã€‚è¿™å…è®¸ç”¨æˆ·ä»¥æ›´å¯è®¿é—®çš„æ ¼å¼è¡¨è¾¾æ›´å¹¿æ³›çš„ç¼–è¾‘å†…å®¹ã€‚ç„¶åæˆ‘ä»¬æ¨å‡ºäº†InstructTimeï¼Œç¬¬ä¸€ä¸ªåŸºäºæŒ‡ä»¤çš„æ—¶é—´åºåˆ—ç¼–è¾‘å™¨ã€‚InstructTimeæ¥æ”¶æ—¶é—´åºåˆ—å’ŒæŒ‡ä»¤ï¼Œå°†å®ƒä»¬åµŒå…¥åˆ°å…±äº«çš„å¤šæ¨¡å¼è¡¨ç¤ºç©ºé—´ä¸­ï¼Œç„¶åè§£ç å®ƒä»¬çš„åµŒå…¥ä»¥ç”Ÿæˆç¼–è¾‘åçš„æ—¶é—´åºåˆ—ã€‚é€šè¿‡å­¦ä¹ ç»“æ„åŒ–çš„å¤šæ¨¡å¼è¡¨ç¤ºç©ºé—´ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°åœ¨åµŒå…¥ä¹‹é—´è¿›è¡Œæ’å€¼ä»¥å®ç°ä¸åŒç¨‹åº¦çš„ç¼–è¾‘ã€‚ä¸ºäº†åŒæ—¶å¤„ç†å±€éƒ¨å’Œå…¨å±€ç¼–è¾‘ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šåˆ†è¾¨ç‡ç¼–ç å™¨ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨åˆæˆå’ŒçœŸå®æ•°æ®é›†å‘ç°InstructTimeæ˜¯å…ˆè¿›çš„æ—¶é—´åºåˆ—ç¼–è¾‘å™¨ï¼šInstructTimeèƒ½å¤Ÿå®ç°é«˜è´¨é‡ä¸”å¯æ§å¼ºåº¦çš„ç¼–è¾‘ï¼Œå¯ä»¥æ¨å¹¿åˆ°æœªè§è¿‡çš„æŒ‡ä»¤ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡å°æ ·æœ¬å­¦ä¹ è½»æ¾é€‚åº”æœªè§è¿‡çš„æ¡ä»¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01504v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ—¶é—´åºåˆ—ç¼–è¾‘æ—¨åœ¨ä¿®æ”¹æ—¶é—´åºåˆ—çš„æŸäº›å±æ€§è€Œä¸æ”¹å˜å…¶ä»–å±æ€§ã€‚ç°æœ‰åŸºäºæ‰©æ•£çš„ç¼–è¾‘å™¨ä¾èµ–äºé¢„è®¾çš„å±æ€§å‘é‡ä½œä¸ºæ¡ä»¶ï¼Œå¹¶é€šè¿‡é‡‡æ ·è¿›è¡Œå…¨æœ‰æˆ–å…¨æ— çš„ç¼–è¾‘ï¼Œè¿™é™åˆ¶äº†æ¡ä»¶æ ¼å¼çš„çµæ´»æ€§å’Œç¼–è¾‘å¼ºåº¦çš„å¯å®šåˆ¶æ§åˆ¶ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºæŒ‡ä»¤çš„æ—¶é—´åºåˆ—ç¼–è¾‘æ–¹æ³•ï¼Œç”¨æˆ·å¯ä»¥ä½¿ç”¨è‡ªç„¶è¯­è¨€æŒ‡å®šé¢„æœŸçš„ç¼–è¾‘ã€‚æˆ‘ä»¬ç„¶åä»‹ç»äº†InstructTimeï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæŒ‡ä»¤çš„æ—¶é—´åºåˆ—ç¼–è¾‘å™¨ã€‚å®ƒé‡‡ç”¨æ—¶é—´åºåˆ—å’ŒæŒ‡ä»¤ï¼Œå°†å®ƒä»¬åµŒå…¥åˆ°å…±äº«çš„å¤šæ¨¡å¼è¡¨ç¤ºç©ºé—´ä¸­ï¼Œç„¶åè§£ç å®ƒä»¬çš„åµŒå…¥ä»¥ç”Ÿæˆç¼–è¾‘åçš„æ—¶é—´åºåˆ—ã€‚é€šè¿‡å­¦ä¹ ç»“æ„åŒ–çš„å¤šæ¨¡å¼è¡¨ç¤ºç©ºé—´ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°åœ¨åµŒå…¥ä¹‹é—´è¿›è¡Œæ’å€¼ä»¥å®ç°ä¸åŒç¨‹åº¦çš„ç¼–è¾‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶é—´åºåˆ—ç¼–è¾‘æ—¨åœ¨ä¿®æ”¹æ—¶é—´åºåˆ—çš„ç‰¹å®šå±æ€§ï¼ŒåŒæ—¶ä¿æŒå…¶ä»–å±æ€§ä¸å˜ã€‚</li>
<li>ç°æœ‰åŸºäºæ‰©æ•£çš„ç¼–è¾‘å™¨æœ‰å±€é™æ€§ï¼Œå¦‚æ¡ä»¶æ ¼å¼ä¸çµæ´»å’Œç¼–è¾‘å¼ºåº¦ä¸å¯å®šåˆ¶ã€‚</li>
<li>å¼•å…¥åŸºäºæŒ‡ä»¤çš„æ—¶é—´åºåˆ—ç¼–è¾‘æ–¹æ³•ï¼Œå…è®¸ç”¨æˆ·ä»¥è‡ªç„¶è¯­è¨€æŒ‡å®šé¢„æœŸçš„ç¼–è¾‘ã€‚</li>
<li>InstructTimeæ˜¯é¦–ä¸ªåŸºäºæŒ‡ä»¤çš„æ—¶é—´åºåˆ—ç¼–è¾‘å™¨ï¼Œé‡‡ç”¨å¤šæ¨¡å¼è¡¨ç¤ºç©ºé—´è¿›è¡Œç¼–è¾‘ã€‚</li>
<li>InstructTimeé€šè¿‡åµŒå…¥å’Œè§£ç æœºåˆ¶å®ç°é«˜è´¨é‡ä¸”å¯æ§çš„ç¼–è¾‘å¼ºåº¦ã€‚</li>
<li>InstructTimeèƒ½å¤Ÿå¤„ç†å±€éƒ¨å’Œå…¨å±€ç¼–è¾‘çš„ç»“åˆï¼Œæå‡ºäº†å¤šåˆ†è¾¨ç‡ç¼–ç å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01504">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8291ba2e937f2f74952b3734e81ff602.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdaeee13e317fee0eac73846d7a294bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db3f1117a728ce9acdcc7f2fe470da26.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Multi-Operator-Few-Shot-Learning-for-Generalization-Across-PDE-Families"><a href="#Multi-Operator-Few-Shot-Learning-for-Generalization-Across-PDE-Families" class="headerlink" title="Multi-Operator Few-Shot Learning for Generalization Across PDE Families"></a>Multi-Operator Few-Shot Learning for Generalization Across PDE Families</h2><p><strong>Authors:Yile Li, Shandian Zhe</strong></p>
<p>Learning solution operators for partial differential equations (PDEs) has become a foundational task in scientific machine learning. However, existing neural operator methods require abundant training data for each specific PDE and lack the ability to generalize across PDE families. In this work, we propose MOFS: a unified multimodal framework for multi-operator few-shot learning, which aims to generalize to unseen PDE operators using only a few demonstration examples. Our method integrates three key components: (i) multi-task self-supervised pretraining of a shared Fourier Neural Operator (FNO) encoder to reconstruct masked spatial fields and predict frequency spectra, (ii) text-conditioned operator embeddings derived from statistical summaries of input-output fields, and (iii) memory-augmented multimodal prompting with gated fusion and cross-modal gradient-based attention. We adopt a two-stage training paradigm that first learns prompt-conditioned inference on seen operators and then applies end-to-end contrastive fine-tuning to align latent representations across vision, frequency, and text modalities. Experiments on PDE benchmarks, including Darcy Flow and Navier Stokes variants, demonstrate that our model outperforms existing operator learning baselines in few-shot generalization. Extensive ablations validate the contributions of each modality and training component. Our approach offers a new foundation for universal and data-efficient operator learning across scientific domains. </p>
<blockquote>
<p>å­¦ä¹ åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEsï¼‰çš„è§£å†³æ–¹æ¡ˆç®—å­å·²æˆä¸ºç§‘å­¦æœºå™¨å­¦ä¹ ä¸­çš„ä¸€é¡¹åŸºç¡€ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç¥ç»ç®—å­æ–¹æ³•é’ˆå¯¹æ¯ç§ç‰¹å®šåå¾®åˆ†æ–¹ç¨‹éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®ï¼Œä¸”ç¼ºä¹è·¨åå¾®åˆ†æ–¹ç¨‹å®¶æ—çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MOFSï¼šä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œç”¨äºå¤šç®—å­çš„å°‘æ ·æœ¬å­¦ä¹ ï¼Œæ—¨åœ¨ä»…ä½¿ç”¨å°‘æ•°å‡ ä¸ªç¤ºä¾‹æ¥æ³›åŒ–åˆ°çœ‹ä¸è§çš„åå¾®åˆ†æ–¹ç¨‹ç®—å­ã€‚æˆ‘ä»¬çš„æ–¹æ³•é›†æˆäº†ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆiï¼‰å¤šä»»åŠ¡è‡ªç›‘ç£é¢„è®­ç»ƒå…±äº«å‚…é‡Œå¶ç¥ç»ç®—å­ï¼ˆFNOï¼‰ç¼–ç å™¨ï¼Œä»¥é‡å»ºæ©è”½çš„ç©ºé—´åœºå¹¶é¢„æµ‹é¢‘è°±ï¼›ï¼ˆiiï¼‰ä»è¾“å…¥-è¾“å‡ºåœºçš„ç»Ÿè®¡æ‘˜è¦ä¸­å¾—å‡ºçš„æ–‡æœ¬æ¡ä»¶ç®—å­åµŒå…¥ï¼›ï¼ˆiiiï¼‰å¸¦æœ‰é—¨æ§èåˆå’Œè·¨æ¨¡æ€åŸºäºæ¢¯åº¦çš„æ³¨æ„åŠ›çš„è®°å¿†å¢å¼ºå¤šæ¨¡æ€æç¤ºã€‚æˆ‘ä»¬é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼ï¼Œé¦–å…ˆå­¦ä¹ å¯¹å¯è§ç®—å­çš„æç¤ºæ¡ä»¶æ¨ç†ï¼Œç„¶ååº”ç”¨ç«¯åˆ°ç«¯å¯¹æ¯”å¾®è°ƒï¼Œä»¥å¯¹é½è§†è§‰ã€é¢‘ç‡å’Œæ–‡æœ¬æ¨¡æ€çš„æ½œåœ¨è¡¨ç¤ºã€‚åœ¨åŒ…æ‹¬è¾¾è¥¿æµå’ŒNavier-Stokeså˜ä½“çš„PDEåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å°‘æ ·æœ¬æ³›åŒ–æ–¹é¢ä¼˜äºç°æœ‰çš„ç®—å­å­¦ä¹ åŸºå‡†æµ‹è¯•ã€‚å¹¿æ³›çš„æ¶ˆèç ”ç©¶éªŒè¯äº†æ¯ç§æ¨¡æ€å’ŒåŸ¹è®­ç»„ä»¶çš„è´¡çŒ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºè·¨ç§‘å­¦é¢†åŸŸçš„é€šç”¨å’Œæ•°æ®é«˜æ•ˆç®—å­å­¦ä¹ æä¾›äº†æ–°çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01211v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸€ä¸ªç ”ç©¶å›¢é˜Ÿæå‡ºäº†MOFSï¼šä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€å°‘æ ·æœ¬å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°‘é‡ç¤ºä¾‹å®ç°å¯¹æœªè§è¿‡çš„PDEç®—ç¬¦è¿›è¡Œæ³›åŒ–ã€‚è¯¥ç ”ç©¶é›†æˆäº†ä¸‰é¡¹å…³é”®æŠ€æœ¯ï¼ŒåŒ…æ‹¬å¤šä»»åŠ¡è‡ªç›‘ç£é¢„è®­ç»ƒå…±äº«å‚…é‡Œå¶ç¥ç»ç½‘ç»œï¼ˆFNOï¼‰ç¼–ç å™¨ã€åŸºäºæ–‡æœ¬æ¡ä»¶çš„ç®—ç¬¦åµŒå…¥ä»¥åŠå¢å¼ºè®°å¿†çš„å¤šæ¨¡æ€æç¤ºä¸é—¨æ§èåˆå’Œè·¨æ¨¡æ€æ¢¯åº¦æ³¨æ„åŠ›æœºåˆ¶ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨PDEåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰ç®—ç¬¦å­¦ä¹ åŸºå‡†ï¼Œå®ç°äº†è·¨ç§‘å­¦é¢†åŸŸçš„é€šç”¨å’Œé«˜æ•ˆç®—ç¬¦å­¦ä¹ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†MOFSæ¡†æ¶ï¼Œå®ç°äº†å¯¹PDEç®—ç¬¦çš„å°‘æ ·æœ¬å­¦ä¹ æ³›åŒ–ã€‚</li>
<li>MOFSæ¡†æ¶é›†æˆäº†å¤šä»»åŠ¡è‡ªç›‘ç£é¢„è®­ç»ƒçš„å‚…é‡Œå¶ç¥ç»ç½‘ç»œç¼–ç å™¨ï¼Œç”¨äºé‡å»ºæ©è†œç©ºé—´åœºå’Œé¢„æµ‹é¢‘è°±ã€‚</li>
<li>ç®—ç¬¦åµŒå…¥é€šè¿‡åŸºäºæ–‡æœ¬æ¡ä»¶çš„ç»Ÿè®¡æ‘˜è¦å®ç°ï¼Œå°†è¾“å…¥-è¾“å‡ºå­—æ®µçš„ä¿¡æ¯èå…¥æ¨¡å‹ã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨å¢å¼ºè®°å¿†çš„å¤šæ¨¡æ€æç¤ºæŠ€æœ¯ï¼ŒåŒ…æ‹¬é—¨æ§èåˆå’Œè·¨æ¨¡æ€æ¢¯åº¦æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œå…ˆå­¦ä¹ å·²çŸ¥ç®—ç¬¦çš„æç¤ºæ¡ä»¶æ¨ç†ï¼Œç„¶åé€šè¿‡ç«¯åˆ°ç«¯å¯¹æ¯”å¾®è°ƒå®ç°è·¨è§†è§‰ã€é¢‘ç‡å’Œæ–‡æœ¬æ¨¡æ€çš„æ½œåœ¨è¡¨ç¤ºå¯¹é½ã€‚</li>
<li>å®éªŒè¯æ˜MOFSåœ¨PDEåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºç°æœ‰ç®—ç¬¦å­¦ä¹ åŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01211">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d170694efdf79d1c4d0949b0d6878a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e6409579618512487f8ea6f0dc75129.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe3c2a3e164262eeeee551ca09999d30.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="COLLAGE-Adaptive-Fusion-based-Retrieval-for-Augmented-Policy-Learning"><a href="#COLLAGE-Adaptive-Fusion-based-Retrieval-for-Augmented-Policy-Learning" class="headerlink" title="COLLAGE: Adaptive Fusion-based Retrieval for Augmented Policy Learning"></a>COLLAGE: Adaptive Fusion-based Retrieval for Augmented Policy Learning</h2><p><strong>Authors:Sateesh Kumar, Shivin Dass, Georgios Pavlakos, Roberto MartÃ­n-MartÃ­n</strong></p>
<p>In this work, we study the problem of data retrieval for few-shot imitation learning: selecting data from a large dataset to train a performant policy for a specific task, given only a few target demonstrations. Prior methods retrieve data using a single-feature distance heuristic, assuming that the best demonstrations are those that most closely resemble the target examples in visual, semantic, or motion space. However, this approach captures only a subset of the relevant information and can introduce detrimental demonstrations, e.g., retrieving data from unrelated tasks due to similar scene layouts, or selecting similar motions from tasks with divergent goals. We present COLLAGE, a method for COLLective data AGgrEgation in few-shot imitation learning that uses an adaptive late fusion mechanism to guide the selection of relevant demonstrations based on a task-specific combination of multiple cues. COLLAGE follows a simple, flexible, and efficient recipe: it assigns weights to subsets of the dataset that are pre-selected using a single feature (e.g., appearance, shape, or language similarity), based on how well a policy trained on each subset predicts actions in the target demonstrations. These weights are then used to perform importance sampling during policy training, sampling data more densely or sparsely according to estimated relevance. COLLAGE is general and feature-agnostic, allowing it to combine any number of subsets selected by any retrieval heuristic, and to identify which subsets provide the greatest benefit for the target task. In extensive experiments, COLLAGE outperforms state-of-the-art retrieval and multi-task learning approaches by 5.1% in simulation across 10 tasks, and by 16.6% in the real world across 6 tasks, where we perform retrieval from the large-scale DROID dataset. More information at <a target="_blank" rel="noopener" href="https://robin-lab.cs.utexas.edu/COLLAGE">https://robin-lab.cs.utexas.edu/COLLAGE</a> . </p>
<blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å°æ ·æ¨¡ä»¿å­¦ä¹ çš„æ•°æ®æ£€ç´¢é—®é¢˜ï¼šä»å¤§å‹æ•°æ®é›†ä¸­é€‰æ‹©æ•°æ®æ¥è®­ç»ƒé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ç­–ç•¥ï¼Œè€Œåªæœ‰å°‘æ•°ç›®æ ‡æ¼”ç¤ºæ ·æœ¬ã€‚ä»¥å‰çš„æ–¹æ³•ä½¿ç”¨å•ä¸€ç‰¹å¾è·ç¦»å¯å‘å¼è¿›è¡Œæ•°æ®å­˜å‚¨æ£€ç´¢ï¼Œå‡è®¾æœ€ä½³çš„æ¼”ç¤ºæ˜¯é‚£äº›åœ¨è§†è§‰ã€è¯­ä¹‰æˆ–è¿åŠ¨ç©ºé—´ä¸­æœ€æ¥è¿‘ç›®æ ‡ç¤ºä¾‹çš„æ¼”ç¤ºã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•åªæ•è·äº†éƒ¨åˆ†ç›¸å…³ä¿¡æ¯ï¼Œå¹¶å¯èƒ½å¼•å…¥æœ‰å®³çš„æ¼”ç¤ºï¼Œä¾‹å¦‚ç”±äºåœºæ™¯å¸ƒå±€ç›¸ä¼¼è€Œä»æ— å…³ä»»åŠ¡ä¸­æ£€ç´¢æ•°æ®ï¼Œæˆ–ä»ç›®æ ‡ä¸åŒçš„ä»»åŠ¡ä¸­é€‰æ‹©ç›¸ä¼¼çš„åŠ¨ä½œã€‚æˆ‘ä»¬æå‡ºäº†COLLAGEï¼Œè¿™æ˜¯ä¸€ç§å°æ ·æ¨¡ä»¿å­¦ä¹ ä¸­çš„é›†ä½“æ•°æ®èšåˆæ–¹æ³•ã€‚å®ƒä½¿ç”¨è‡ªé€‚åº”çš„åæœŸèåˆæœºåˆ¶ï¼Œæ ¹æ®é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å¤šä¸ªçº¿ç´¢ç»„åˆæ¥æŒ‡å¯¼ç›¸å…³æ¼”ç¤ºçš„é€‰æ‹©ã€‚COLLAGEéµå¾ªç®€å•ã€çµæ´»å’Œé«˜æ•ˆçš„é…æ–¹ï¼šå®ƒä¸ºä½¿ç”¨å•ä¸€ç‰¹å¾ï¼ˆä¾‹å¦‚å¤–è§‚ã€å½¢çŠ¶æˆ–è¯­è¨€ç›¸ä¼¼æ€§ï¼‰é¢„å…ˆé€‰æ‹©çš„æ•°æ®é›†å­é›†åˆ†é…æƒé‡ï¼ŒåŸºäºåœ¨æ¯ä¸ªå­é›†ä¸Šè®­ç»ƒçš„ç­–ç•¥å¯¹ç›®æ ‡æ¼”ç¤ºçš„é¢„æµ‹æ•ˆæœã€‚ç„¶åï¼Œè¿™äº›æƒé‡è¢«ç”¨äºåœ¨ç­–ç•¥è®­ç»ƒæœŸé—´æ‰§è¡Œé‡è¦æ€§é‡‡æ ·ï¼Œæ ¹æ®ä¼°è®¡çš„ç›¸å…³æ€§æ›´å¯†é›†æˆ–æ›´ç¨€ç–åœ°é‡‡æ ·æ•°æ®ã€‚COLLAGEæ˜¯é€šç”¨çš„ï¼Œä¸ç‰¹å¾æ— å…³ï¼Œå¯ä»¥ç»„åˆä»»ä½•æ•°é‡ç”±ä»»ä½•æ£€ç´¢å¯å‘å¼é€‰æ‹©çš„å­é›†ï¼Œå¹¶ç¡®å®šå“ªäº›å­é›†å¯¹ç›®æ ‡ä»»åŠ¡æä¾›æœ€å¤§çš„å¥½å¤„ã€‚åœ¨å¹¿æ³›çš„å®éªŒä¸­ï¼Œä¸æœ€æ–°æ£€ç´¢å’Œå¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒCOLLAGEåœ¨æ¨¡æ‹Ÿçš„10ä¸ªä»»åŠ¡ä¸Šæé«˜äº†5.1%çš„æ€§èƒ½ï¼Œåœ¨ç°å®ä¸–ç•Œçš„6ä¸ªä»»åŠ¡ä¸Šæé«˜äº†16.6%çš„æ€§èƒ½ï¼Œæˆ‘ä»¬ä»å¤§è§„æ¨¡DROIDæ•°æ®é›†ä¸­è¿›è¡Œæ£€ç´¢ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://robin-lab.cs.utexas.edu/COLLAGE%E3%80%82%EF%BC%88%E6%B3%A8%EF%BC%9A%E8%AF%A5%E7%BD%91%E5%9D%80%E5%8F%AF%E8%83%BD%E4%B8%8D%E8%83%BD%E7%9B%B4%E6%8E%A5%E8%AE%BF%E9%97%AE%EF%BC%89">https://robin-lab.cs.utexas.edu/COLLAGEã€‚ï¼ˆæ³¨ï¼šè¯¥ç½‘å€å¯èƒ½æ— æ³•ç›´æ¥è®¿é—®ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01131v1">PDF</a> Accepted at the Conference on Robot Learning (CoRL), 2025. Project   page: <a target="_blank" rel="noopener" href="https://robin-lab.cs.utexas.edu/COLLAGE">https://robin-lab.cs.utexas.edu/COLLAGE</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å°‘æ ·æœ¬æ¨¡ä»¿å­¦ä¹ ä¸­çš„æ•°æ®æ£€ç´¢é—®é¢˜ï¼Œå³å¦‚ä½•ä»å¤§è§„æ¨¡æ•°æ®é›†ä¸­é€‰æ‹©æ•°æ®æ¥è®­ç»ƒé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ç­–ç•¥ï¼Œä»…ç»™å‡ºå°‘é‡çš„ç›®æ ‡æ¼”ç¤ºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•COLLAGEï¼Œä½¿ç”¨è‡ªé€‚åº”çš„åæœŸèåˆæœºåˆ¶ï¼ŒåŸºäºç‰¹å®šä»»åŠ¡çš„å¤šä¸ªçº¿ç´¢ç»„åˆæ¥æŒ‡å¯¼ç›¸å…³æ¼”ç¤ºçš„é€‰æ‹©ã€‚COLLAGEç®€å•ã€çµæ´»ã€é«˜æ•ˆï¼Œä¸ºæ•°æ®é›†å­é›†åˆ†é…æƒé‡ï¼ŒåŸºäºæ¯ä¸ªå­é›†è®­ç»ƒçš„ç­–ç•¥å¯¹ç›®æ ‡æ¼”ç¤ºçš„é¢„æµ‹æ•ˆæœã€‚è¿™äº›æƒé‡ç”¨äºåœ¨ç­–ç•¥è®­ç»ƒæœŸé—´æ‰§è¡Œé‡è¦æ€§é‡‡æ ·ï¼Œæ ¹æ®ä¼°è®¡çš„ç›¸å…³æ€§æ›´å¯†é›†æˆ–æ›´ç¨€ç–åœ°é‡‡æ ·æ•°æ®ã€‚åœ¨ä¸æœ€å…ˆè¿›çš„æ£€ç´¢å’Œå¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•è¿›è¡Œçš„å¹¿æ³›å®éªŒä¸­ï¼ŒCOLLAGEåœ¨æ¨¡æ‹Ÿçš„10ä¸ªä»»åŠ¡ä¸­æé«˜äº†5.1%çš„æ€§èƒ½ï¼Œåœ¨ç°å®ä¸–ç•Œçš„6ä¸ªä»»åŠ¡ä¸­æé«˜äº†16.6%çš„æ€§èƒ½ï¼Œè¿™äº›å®éªŒå‡ä»å¤§è§„æ¨¡DROISæ•°æ®é›†ä¸­è¿›è¡Œæ£€ç´¢ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç ”ç©¶äº†å°‘æ ·æœ¬æ¨¡ä»¿å­¦ä¹ ä¸­çš„æ•°æ®æ£€ç´¢é—®é¢˜ï¼Œå…³æ³¨äºä»å¤§è§„æ¨¡æ•°æ®é›†ä¸­é€‰æ‹©ç›¸å…³æ•°æ®æ¥è®­ç»ƒä»»åŠ¡ç‰¹å®šç­–ç•¥ã€‚</li>
<li>æå‡ºäº†COLLAGEæ–¹æ³•ï¼Œé‡‡ç”¨è‡ªé€‚åº”åæœŸèåˆæœºåˆ¶ï¼Œç»“åˆå¤šç§çº¿ç´¢æ¥é€‰æ‹©ç›¸å…³çš„æ¼”ç¤ºæ•°æ®ã€‚</li>
<li>COLLAGEç®€å•ã€çµæ´»ã€é«˜æ•ˆï¼Œèƒ½æ ¹æ®å­é›†å¯¹ç›®æ ‡æ¼”ç¤ºçš„é¢„æµ‹æ€§èƒ½ä¸ºæ•°æ®é›†å­é›†åˆ†é…æƒé‡ã€‚</li>
<li>COLLAGEé‡‡ç”¨é‡è¦æ€§é‡‡æ ·ï¼Œæ ¹æ®ä¼°è®¡çš„ç›¸å…³æ€§è°ƒæ•´æ•°æ®é‡‡æ ·å¯†åº¦ã€‚</li>
<li>COLLAGEå¯ä¸å…¶ä»–ä»»ä½•æ£€ç´¢å¯å‘å¼æ–¹æ³•ç»“åˆï¼Œå¯è¯†åˆ«å¯¹ç›®æ ‡ä»»åŠ¡æœ€æœ‰ç›Šçš„å­é›†ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿçš„10ä¸ªä»»åŠ¡å’Œç°å®ä¸–ç•Œçš„6ä¸ªä»»åŠ¡å®éªŒä¸­ï¼ŒCOLLAGEæ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>COLLAGEæ–¹æ³•ä»å¤§è§„æ¨¡DROISæ•°æ®é›†ä¸­è¿›è¡Œæ£€ç´¢ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01131">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5256b7bf248d597a43d592e5a6e26b27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-942d57c5878cc451b94f15c0adf21259.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8028dbfb0effbe83dadfa36fc5c12d0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01f4e1cfce8220cc4ba841348976f8f1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VAULT-Vigilant-Adversarial-Updates-via-LLM-Driven-Retrieval-Augmented-Generation-for-NLI"><a href="#VAULT-Vigilant-Adversarial-Updates-via-LLM-Driven-Retrieval-Augmented-Generation-for-NLI" class="headerlink" title="VAULT: Vigilant Adversarial Updates via LLM-Driven Retrieval-Augmented   Generation for NLI"></a>VAULT: Vigilant Adversarial Updates via LLM-Driven Retrieval-Augmented   Generation for NLI</h2><p><strong>Authors:Roie Kazoom, Ofir Cohen, Rami Puzis, Asaf Shabtai, Ofer Hadar</strong></p>
<p>We introduce VAULT, a fully automated adversarial RAG pipeline that systematically uncovers and remedies weaknesses in NLI models through three stages: retrieval, adversarial generation, and iterative retraining. First, we perform balanced few-shot retrieval by embedding premises with both semantic (BGE) and lexical (BM25) similarity. Next, we assemble these contexts into LLM prompts to generate adversarial hypotheses, which are then validated by an LLM ensemble for label fidelity. Finally, the validated adversarial examples are injected back into the training set at increasing mixing ratios, progressively fortifying a zero-shot RoBERTa-base model.On standard benchmarks, VAULT elevates RoBERTa-base accuracy from 88.48% to 92.60% on SNLI +4.12%, from 75.04% to 80.95% on ANLI +5.91%, and from 54.67% to 71.99% on MultiNLI +17.32%. It also consistently outperforms prior in-context adversarial methods by up to 2.0% across datasets. By automating high-quality adversarial data curation at scale, VAULT enables rapid, human-independent robustness improvements in NLI inference tasks. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†VAULTï¼Œè¿™æ˜¯ä¸€ç§å…¨è‡ªåŠ¨å¯¹æŠ—æ€§RAGç®¡é“ï¼Œå®ƒé€šè¿‡ä¸‰ä¸ªé˜¶æ®µç³»ç»Ÿåœ°å‘ç°å’Œè§£å†³NLIæ¨¡å‹çš„å¼±ç‚¹ï¼šæ£€ç´¢ã€å¯¹æŠ—æ€§ç”Ÿæˆå’Œè¿­ä»£é‡è®­ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡åµŒå…¥å‰æï¼Œå®ç°è¯­ä¹‰ï¼ˆBGEï¼‰å’Œè¯æ±‡ï¼ˆBM25ï¼‰ç›¸ä¼¼æ€§å¹³è¡¡è¿›è¡Œå°‘é‡æ£€ç´¢ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¿™äº›ä¸Šä¸‹æ–‡ç»„åˆæˆLLMæç¤ºï¼Œç”Ÿæˆå¯¹æŠ—æ€§å‡è®¾ï¼Œç„¶åé€šè¿‡LLMé›†åˆéªŒè¯æ ‡ç­¾çš„å¿ å®åº¦ã€‚æœ€åï¼Œç»è¿‡éªŒè¯çš„å¯¹æŠ—æ€§å®ä¾‹ä»¥ä¸æ–­å¢åŠ çš„æ··åˆæ¯”ä¾‹æ³¨å…¥è®­ç»ƒé›†ï¼Œé€æ­¥åŠ å¼ºé›¶æ ·æœ¬RoBERTaåŸºç¡€æ¨¡å‹ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVAULTå°†RoBERTaåŸºç¡€å‡†ç¡®ç‡ä»SNLIçš„88.48%æé«˜åˆ°92.6%ï¼ŒANLIçš„å‡†ç¡®ç‡ä»75.04%æé«˜åˆ°80.95%ï¼ŒMultiNLIçš„å‡†ç¡®ç‡ä»54.67%æé«˜åˆ°71.99%ã€‚å®ƒè¿˜æŒç»­ä¼˜äºå…ˆå‰çš„ä¸Šä¸‹æ–‡å¯¹æŠ—æ–¹æ³•ï¼Œè·¨æ•°æ®é›†æœ€é«˜è¾¾2.0%ã€‚é€šè¿‡å¤§è§„æ¨¡è‡ªåŠ¨åŒ–é«˜è´¨é‡å¯¹æŠ—æ•°æ®æ”¶é›†ï¼ŒVAULTèƒ½å¤Ÿåœ¨NLIæ¨æ–­ä»»åŠ¡ä¸­å®ç°å¿«é€Ÿã€æ— éœ€äººå·¥å‚ä¸çš„ç¨³å¥æ€§æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00965v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæˆ‘ä»¬æå‡ºäº†VAULTè¿™ä¸€å…¨è‡ªåŠ¨çš„å¯¹æŠ—æ€§RAGæµç¨‹ï¼Œç”¨äºç³»ç»Ÿå‘ç°å’Œæ”¹è¿›è‡ªç„¶è¯­è¨€ç†è§£æ¨¡å‹ï¼ˆNLIï¼‰ä¸­çš„å¼±ç‚¹ã€‚VAULTåŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šæ£€ç´¢ã€å¯¹æŠ—æ€§ç”Ÿæˆå’Œè¿­ä»£é‡è®­ç»ƒã€‚é€šè¿‡åµŒå…¥å‰æå®ç°å¹³è¡¡å°‘é‡æ£€ç´¢ï¼Œåˆ©ç”¨è¯­ä¹‰å’Œè¯æ±‡ç›¸ä¼¼æ€§ç”ŸæˆLLMæç¤ºä»¥äº§ç”Ÿå¯¹æŠ—æ€§å‡è®¾ï¼Œé€šè¿‡LLMé›†åˆéªŒè¯æ ‡ç­¾å¯ä¿¡åº¦ã€‚æœ€åï¼Œå°†éªŒè¯è¿‡çš„å¯¹æŠ—æ€§å®ä¾‹ä»¥ä¸åŒæ··åˆæ¯”ä¾‹æ³¨å…¥è®­ç»ƒé›†ï¼Œé€æ­¥å¼ºåŒ–é›¶æ ·æœ¬RoBERTaåŸºç¡€æ¨¡å‹ã€‚åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒVAULTæé«˜äº†RoBERTaåŸºç¡€æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼ŒVAULTèƒ½å¤Ÿè‡ªåŠ¨åŒ–å¤§è§„æ¨¡é«˜è´¨é‡å¯¹æŠ—æ•°æ®çš„æ•´ç†ï¼Œå®ç°æ— éœ€äººå·¥å‚ä¸çš„NLIæ¨ç†ä»»åŠ¡ç¨³å¥æ€§å¿«é€Ÿæ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>VAULTæ˜¯ä¸€ä¸ªå…¨è‡ªåŠ¨çš„å¯¹æŠ—æ€§RAGæµç¨‹ï¼Œç”¨äºæå‡è‡ªç„¶è¯­è¨€ç†è§£æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>VAULTåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼šæ£€ç´¢ã€å¯¹æŠ—æ€§ç”Ÿæˆå’Œè¿­ä»£é‡è®­ç»ƒã€‚</li>
<li>VAULTé€šè¿‡åµŒå…¥å‰æå®ç°å¹³è¡¡å°‘é‡æ£€ç´¢ï¼Œå¹¶åˆ©ç”¨è¯­ä¹‰å’Œè¯æ±‡ç›¸ä¼¼æ€§ç”Ÿæˆå¯¹æŠ—æ€§å‡è®¾ã€‚</li>
<li>LLMé›†åˆè¢«ç”¨äºéªŒè¯æ ‡ç­¾å¯ä¿¡åº¦ã€‚</li>
<li>VAULTå°†éªŒè¯è¿‡çš„å¯¹æŠ—æ€§å®ä¾‹æ³¨å…¥è®­ç»ƒé›†ï¼Œé€æ­¥å¼ºåŒ–æ¨¡å‹ã€‚</li>
<li>VAULTåœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½æå‡æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00965">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-098648614c4853e987444c1488b0fb78.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4087bcd3335cbc4378383a3ba5f4b624.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a155249f3418395ca5596c3c983aaf6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad5868d46fc18bca66ab84b54958332f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af439fd802d9320263032a3f44a849c5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Brain-Graph-Foundation-Model-Pre-Training-and-Prompt-Tuning-for-Any-Atlas-and-Disorder"><a href="#A-Brain-Graph-Foundation-Model-Pre-Training-and-Prompt-Tuning-for-Any-Atlas-and-Disorder" class="headerlink" title="A Brain Graph Foundation Model: Pre-Training and Prompt-Tuning for Any   Atlas and Disorder"></a>A Brain Graph Foundation Model: Pre-Training and Prompt-Tuning for Any   Atlas and Disorder</h2><p><strong>Authors:Xinxu Wei, Kanhao Zhao, Yong Jiao, Lifang He, Yu Zhang</strong></p>
<p>As large language models (LLMs) continue to revolutionize AI research, there is a growing interest in building large-scale brain foundation models to advance neuroscience. While most existing brain foundation models are pre-trained on time-series signals or connectome features, we propose a novel graph-based pre-training paradigm for constructing a brain graph foundation model. In this paper, we introduce the Brain Graph Foundation Model, termed BrainGFM, a unified framework that leverages graph contrastive learning and graph masked autoencoders for large-scale fMRI-based pre-training. BrainGFM is pre-trained on a diverse mixture of brain atlases with varying parcellations, significantly expanding the pre-training corpus and enhancing the modelâ€™s ability to generalize across heterogeneous fMRI-derived brain representations. To support efficient and versatile downstream transfer, we integrate both graph prompts and language prompts into the model design, enabling BrainGFM to flexibly adapt to a wide range of atlases, neurological and psychiatric disorders, and task settings. Furthermore, we employ meta-learning to optimize the graph prompts, facilitating strong generalization to previously unseen disorders under both few-shot and zero-shot learning conditions via language-guided prompting. BrainGFM is pre-trained on 27 neuroimaging datasets spanning 25 common neurological and psychiatric disorders, encompassing 2 types of brain atlases (functional and anatomical) across 8 widely-used parcellations, and covering over 25,000 subjects, 60,000 fMRI scans, and a total of 400,000 graph samples aggregated across all atlases and parcellations. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/weixinxu666/BrainGFM">https://github.com/weixinxu666/BrainGFM</a> </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»§ç»­å¼•é¢†äººå·¥æ™ºèƒ½ç ”ç©¶çš„é©å‘½ï¼Œå»ºç«‹å¤§è§„æ¨¡è„‘åŸºç¡€æ¨¡å‹ä»¥æ¨åŠ¨ç¥ç»ç§‘å­¦å‘å±•çš„å…´è¶£æ—¥ç›Šæµ“åšã€‚å°½ç®¡å¤§å¤šæ•°ç°æœ‰çš„è„‘åŸºç¡€æ¨¡å‹éƒ½æ˜¯åŸºäºæ—¶é—´åºåˆ—ä¿¡å·æˆ–è¿æ¥ç»„ç‰¹å¾è¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œä½†æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå›¾çš„æ–°å‹é¢„è®­ç»ƒèŒƒå¼æ¥æ„å»ºè„‘å›¾åŸºç¡€æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Brain Graph Foundation Modelï¼ˆBrainGFMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œåˆ©ç”¨å›¾å¯¹æ¯”å­¦ä¹ å’Œå›¾æ©ç è‡ªåŠ¨ç¼–ç å™¨è¿›è¡Œå¤§è§„æ¨¡åŸºäºfMRIçš„é¢„è®­ç»ƒã€‚BrainGFMåœ¨å…·æœ‰ä¸åŒåˆ†å‰²çš„å¤šç§è„‘å›¾è°±æ··åˆæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ˜¾è‘—æ‰©å±•äº†é¢„è®­ç»ƒè¯­æ–™åº“ï¼Œæé«˜äº†æ¨¡å‹åœ¨ä¸åŒå¼‚æ„å›¾MRIè¡ç”Ÿçš„è„‘è¡¨ç¤ºä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†æ”¯æŒé«˜æ•ˆä¸”é€šç”¨çš„ä¸‹æ¸¸è¿ç§»ï¼Œæˆ‘ä»¬å°†å›¾æç¤ºå’Œè¯­è¨€æç¤ºé›†æˆåˆ°æ¨¡å‹è®¾è®¡ä¸­ï¼Œä½¿BrainGFMèƒ½å¤Ÿçµæ´»åœ°é€‚åº”å„ç§å›¾è°±ã€ç¥ç»å’Œç²¾ç¥ç–¾ç—…ä»¥åŠä»»åŠ¡è®¾ç½®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨å…ƒå­¦ä¹ æ¥ä¼˜åŒ–å›¾æç¤ºï¼Œé€šè¿‡è¯­è¨€å¼•å¯¼æç¤ºï¼Œåœ¨å°‘æ ·æœ¬å­¦ä¹ å’Œé›¶æ ·æœ¬å­¦ä¹ æ¡ä»¶ä¸‹å®ç°å¯¹ä»¥å‰æœªè§è¿‡çš„ç–¾ç—…çš„å¼ºå¤§æ³›åŒ–ã€‚BrainGFMåœ¨27ä¸ªç¥ç»æˆåƒæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¶µç›–25ç§å¸¸è§çš„ç¥ç»å’Œç²¾ç¥ç–¾ç—…ï¼ŒåŒ…æ‹¬ä¸¤ç§ç±»å‹çš„è„‘å›¾è°±ï¼ˆåŠŸèƒ½å’Œç»“æ„ï¼‰è·¨è¶Š8ç§å¹¿æ³›ä½¿ç”¨çš„åˆ†å‰²æ–¹æ³•ï¼Œæ¶‰åŠè¶…è¿‡25,000åå—è¯•è€…ã€60,000æ¬¡fMRIæ‰«æå’Œæ€»è®¡40ä¸‡å¼ è·¨æ‰€æœ‰å›¾è°±å’Œåˆ†å‰²æ–¹æ³•çš„å›¾æ ·æœ¬ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/weixinxu666/BrainGFM">https://github.com/weixinxu666/BrainGFM</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.02044v2">PDF</a> 30pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†BrainGFMæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå›¾å¯¹æ¯”å­¦ä¹ å’Œå›¾æ©ç è‡ªç¼–ç å™¨çš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºå¤§è§„æ¨¡åŸºäºfMRIçš„é¢„è®­ç»ƒã€‚BrainGFMåˆ©ç”¨å¤šç§è„‘å›¾è°±å’Œåˆ†å‰²æ–¹æ³•ï¼Œæ‰©å±•äº†é¢„è®­ç»ƒè¯­æ–™åº“ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹å¼‚æ„å›¾åƒçš„æ³›åŒ–èƒ½åŠ›ã€‚æ¨¡å‹ç»“åˆäº†å›¾æç¤ºå’Œè¯­è¨€æç¤ºï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒçš„è„‘å›¾è°±ã€ç¥ç»å’Œç²¾ç¥éšœç¢ä»¥åŠä»»åŠ¡è®¾ç½®ã€‚é€šè¿‡å…ƒå­¦ä¹ ä¼˜åŒ–å›¾æç¤ºï¼ŒBrainGFMåœ¨å°‘æ ·æœ¬å’Œæ— æ ·æœ¬å­¦ä¹ æ¡ä»¶ä¸‹å¯¹æœªè§è¿‡çš„ç–¾ç—…è¿›è¡Œè¯­è¨€å¼•å¯¼æç¤ºï¼Œå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¨¡å‹åœ¨æ¶µç›–å¤šç§å¸¸è§ç¥ç»å’Œç²¾ç¥éšœç¢çš„27ä¸ªç¥ç»æˆåƒæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BrainGFMæ˜¯ä¸€ä¸ªåŸºäºå›¾å¯¹æ¯”å­¦ä¹ å’Œå›¾æ©ç è‡ªç¼–ç å™¨çš„è„‘å›¾åŸºç¡€æ¨¡å‹ã€‚</li>
<li>å®ƒä½¿ç”¨å¤šæ ·åŒ–çš„è„‘å›¾è°±è¿›è¡Œå¤§è§„æ¨¡åŸºäºfMRIçš„é¢„è®­ç»ƒã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿé€‚åº”å¤šç§ä¸åŒçš„è„‘å›¾è°±å’Œåˆ†å‰²æ–¹æ³•ï¼Œå¢å¼ºå…¶åœ¨å¼‚æ„å›¾åƒä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç»“åˆäº†å›¾æç¤ºå’Œè¯­è¨€æç¤ºï¼ŒBrainGFMå¯çµæ´»åº”ç”¨äºå„ç§è„‘å›¾è°±ã€ç¥ç»å’Œç²¾ç¥éšœç¢ä»¥åŠä»»åŠ¡è®¾ç½®ã€‚</li>
<li>é€šè¿‡å…ƒå­¦ä¹ ä¼˜åŒ–å›¾æç¤ºï¼ŒBrainGFMåœ¨å°‘æ ·æœ¬å’Œæ— æ ·æœ¬å­¦ä¹ æ¡ä»¶ä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨æ¶µç›–å¤šç§å¸¸è§ç¥ç»å’Œç²¾ç¥éšœç¢çš„å¤šä¸ªç¥ç»æˆåƒæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.02044">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8240473e7d2d92280f4a898008345d07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5df40297031134afb3c984c140b88e2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-881d3f5feb000646c35e1f5864541f68.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Think-Reflect-Create-Metacognitive-Learning-for-Zero-Shot-Robotic-Planning-with-LLMs"><a href="#Think-Reflect-Create-Metacognitive-Learning-for-Zero-Shot-Robotic-Planning-with-LLMs" class="headerlink" title="Think, Reflect, Create: Metacognitive Learning for Zero-Shot Robotic   Planning with LLMs"></a>Think, Reflect, Create: Metacognitive Learning for Zero-Shot Robotic   Planning with LLMs</h2><p><strong>Authors:Wenjie Lin, Jin Wei-Kocsis, Jiansong Zhang, Byung-Cheol Min, Dongming Gan, Paul Asunda, Ragu Athinarayanan</strong></p>
<p>While large language models (LLMs) have shown great potential across various domains, their applications in robotics remain largely limited to static prompt-based behaviors and still face challenges in complex tasks under zero-shot or few-shot settings. Inspired by human metacognitive learning and creative problem-solving, we address this limitation by exploring a fundamental question: Can LLMs be empowered with metacognitive capabilities to reason, reflect, and create, thereby enhancing their ability to perform robotic tasks with minimal demonstrations? In this paper, we present a framework that integrates metacognitive learning into LLM-powered multi-robot collaboration. The system equips the LLM-powered robotic agents with a skill decomposition and self-reflection mechanism that identifies modular skills from prior tasks, reflects on failures in unseen task scenarios, and synthesizes effective new solutions. We propose a more challenging robotic benchmark task and evaluate our framework on the existing benchmark and the novel task. Experimental results show that our metacognitive learning framework significantly outperforms existing baselines. Moreover, we observe that the framework can generate solutions that differ from the ground truth yet still successfully complete the tasks. These findings support our hypothesis that metacognitive learning can foster creativity in robotic planning. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ä¸ªé¢†åŸŸè¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨æœºå™¨äººé¢†åŸŸçš„åº”ç”¨ä»ç„¶ä¸»è¦å±€é™äºåŸºäºé™æ€æç¤ºçš„è¡Œä¸ºï¼Œå¹¶ä¸”åœ¨é›¶æ ·æœ¬æˆ–å°æ ·ä¾‹è®¾ç½®ä¸‹æ‰§è¡Œå¤æ‚ä»»åŠ¡æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬å—åˆ°äººç±»å…ƒè®¤çŸ¥å­¦ä¹ å’Œåˆ›é€ æ€§è§£å†³é—®é¢˜çš„å¯å‘ï¼Œé€šè¿‡æ¢ç´¢ä¸€ä¸ªåŸºæœ¬é—®é¢˜æ¥è§£å†³è¿™ä¸€å±€é™æ€§ï¼šèƒ½å¦ä¸ºLLMèµ‹äºˆå…ƒè®¤çŸ¥èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ¨ç†ã€åæ€å’Œåˆ›é€ ï¼Œä»è€Œæé«˜å®ƒä»¬åœ¨å°‘é‡æ¼”ç¤ºæƒ…å†µä¸‹æ‰§è¡Œæœºå™¨äººä»»åŠ¡çš„èƒ½åŠ›ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå°†å…ƒè®¤çŸ¥å­¦ä¹ èå…¥LLMé©±åŠ¨çš„å¤šæœºå™¨äººåä½œçš„æ¡†æ¶ã€‚è¯¥ç³»ç»Ÿä¸ºLLMé©±åŠ¨çš„æœºå™¨äººä»£ç†é…å¤‡äº†æŠ€èƒ½åˆ†è§£å’Œè‡ªæˆ‘åæ€æœºåˆ¶ï¼Œè¯¥æœºåˆ¶èƒ½å¤Ÿä»å…ˆå‰ä»»åŠ¡ä¸­è¯†åˆ«æ¨¡å—åŒ–æŠ€èƒ½ï¼Œåæ€æœªè§ä»»åŠ¡åœºæ™¯ä¸­çš„å¤±è´¥ï¼Œå¹¶åˆæˆæœ‰æ•ˆçš„æ–°è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„æœºå™¨äººåŸºå‡†ä»»åŠ¡ï¼Œå¹¶åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•å’Œæ–°å‹ä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¡†æ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å…ƒè®¤çŸ¥å­¦ä¹ æ¡†æ¶æ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æµ‹è¯•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¯¥æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆä¸æ ‡å‡†ç­”æ¡ˆä¸åŒä½†ä»èƒ½æˆåŠŸå®Œæˆä»»åŠ¡çš„è§£å†³æ–¹æ¡ˆã€‚è¿™äº›å‘ç°æ”¯æŒæˆ‘ä»¬çš„å‡è®¾ï¼Œå³å…ƒè®¤çŸ¥å­¦ä¹ å¯ä»¥ä¿ƒè¿›æœºå™¨äººè§„åˆ’ä¸­çš„åˆ›é€ åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14899v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹åœ¨æœºå™¨äººé¢†åŸŸçš„åº”ç”¨ä»ç„¶å±€é™äºé™æ€æç¤ºè¡Œä¸ºï¼Œå¹¶åœ¨é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬ç¯å¢ƒä¸‹çš„å¤æ‚ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡å—äººç±»å…ƒè®¤çŸ¥å­¦ä¹ å’Œåˆ›é€ æ€§è§£å†³é—®é¢˜çš„å¯å‘ï¼Œæ¢ç´¢äº†ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼šèƒ½å¦ä¸ºå¤§è¯­è¨€æ¨¡å‹èµ‹äºˆå…ƒè®¤çŸ¥èƒ½åŠ›è¿›è¡Œæ¨ç†ã€åæ€å’Œåˆ›æ–°ï¼Œä»è€Œæå‡å…¶åœ¨å°‘é‡æ¼”ç¤ºä¸‹çš„æœºå™¨äººä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ï¼Ÿæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå°†å…ƒè®¤çŸ¥å­¦ä¹ æ•´åˆåˆ°è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æœºå™¨äººåä½œæ¡†æ¶ä¸­ã€‚è¯¥æ¡†æ¶ä¸ºè¯­è¨€æ¨¡å‹é©±åŠ¨çš„æœºå™¨äººé…å¤‡äº†æŠ€èƒ½åˆ†è§£å’Œè‡ªæˆ‘åæ€æœºåˆ¶ï¼Œå¯ä»¥ä»å…ˆå‰çš„ä»»åŠ¡ä¸­è¯†åˆ«æ¨¡å—åŒ–æŠ€èƒ½ï¼Œåæ€åœ¨æœªè§çš„ä»»åŠ¡åœºæ™¯ä¸­çš„å¤±è´¥ï¼Œå¹¶åˆæˆæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥å…ƒè®¤çŸ¥å­¦ä¹ æ¡†æ¶æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå¹¶èƒ½ç”Ÿæˆä¸çœŸå®ç­”æ¡ˆä¸åŒä½†ä»èƒ½æˆåŠŸå®Œæˆä»»åŠ¡çš„è§£å†³æ–¹æ¡ˆã€‚è¿™æ”¯æŒäº†æˆ‘ä»¬çš„å‡è®¾ï¼šå…ƒè®¤çŸ¥å­¦ä¹ å¯ä»¥ä¿ƒè¿›æœºå™¨äººè§„åˆ’ä¸­çš„åˆ›é€ åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹åœ¨æœºå™¨äººé¢†åŸŸçš„åº”ç”¨å—é™äºé™æ€æç¤ºè¡Œä¸ºï¼Œå°¤å…¶åœ¨å¤æ‚ä»»åŠ¡çš„é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬ç¯å¢ƒä¸‹å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æ¢ç´¢äº†ä½¿å¤§è¯­è¨€æ¨¡å‹å…·å¤‡å…ƒè®¤çŸ¥èƒ½åŠ›çš„å¯èƒ½æ€§ï¼Œä»¥è¿›è¡Œæ¨ç†ã€åæ€å’Œåˆ›æ–°ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶å°†å…ƒè®¤çŸ¥å­¦ä¹ æ•´åˆåˆ°è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æœºå™¨äººåä½œä¸­ã€‚</li>
<li>æ¡†æ¶é…å¤‡äº†æŠ€èƒ½åˆ†è§£å’Œè‡ªæˆ‘åæ€æœºåˆ¶ï¼Œè¯†åˆ«æ¨¡å—åŒ–æŠ€èƒ½ï¼Œåæ€å¤±è´¥å¹¶åˆæˆæ–°è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥å…ƒè®¤çŸ¥å­¦ä¹ æ¡†æ¶ä¼˜äºç°æœ‰åŸºçº¿ã€‚</li>
<li>æ¡†æ¶èƒ½å¤Ÿç”Ÿæˆä¸çœŸå®ç­”æ¡ˆä¸åŒä½†ä»èƒ½æˆåŠŸå®Œæˆä»»åŠ¡çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14899">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5219eb1c3700af63ad984272886f678e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-155864565114538d3857a4cd3604e306.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e2997b58d2603ed1a95808156a52f7c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e400e29548236647f1af7fcd70dbf461.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6109be5e5b57fd8f71093d6dbf91d16e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0977947316dc0e0f2f7420a1dc985367.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39395c8cf9dfb07a2342e8617c9b473d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dc3fdd091d9a6d1c00fad37845735f20.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GateLens-A-Reasoning-Enhanced-LLM-Agent-for-Automotive-Software-Release-Analytics"><a href="#GateLens-A-Reasoning-Enhanced-LLM-Agent-for-Automotive-Software-Release-Analytics" class="headerlink" title="GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release   Analytics"></a>GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release   Analytics</h2><p><strong>Authors:Arsham Gholamzadeh Khoee, Shuai Wang, Yinan Yu, Robert Feldt, Dhasarathy Parthasarathy</strong></p>
<p>Ensuring reliable software release decisions is critical in safety-critical domains such as automotive manufacturing. Release validation relies on large tabular datasets, yet manual analysis is slow, costly, and error-prone. While Large Language Models (LLMs) offer promising automation potential, they face challenges in analytical reasoning, structured data handling, and ambiguity resolution. This paper introduces GateLens, an LLM-based system for analyzing tabular data in the automotive domain. GateLens translates natural language queries into Relational Algebra (RA) expressions and generates optimized Python code. Unlike traditional multi-agent or planning-based systems that can be slow, opaque, and costly to maintain, GateLens emphasizes speed, transparency, and reliability. Experimental results show that GateLens outperforms the existing Chain-of-Thought (CoT) + Self-Consistency (SC) based system on real-world datasets, particularly in handling complex and ambiguous queries. Ablation studies confirm the essential role of the RA layer. Industrial deployment shows over 80% reduction in analysis time while maintaining high accuracy across test result interpretation, impact assessment, and release candidate evaluation. GateLens operates effectively in zero-shot settings without requiring few-shot examples or agent orchestration. This work advances deployable LLM system design by identifying key architectural features-intermediate formal representations, execution efficiency, and low configuration overhead-crucial for safety-critical industrial applications. </p>
<blockquote>
<p>åœ¨æ±½è½¦è¡Œä¸šç­‰å®‰å…¨å…³é”®é¢†åŸŸï¼Œç¡®ä¿å¯é çš„è½¯ä»¶å‘å¸ƒå†³ç­–è‡³å…³é‡è¦ã€‚å‘å¸ƒéªŒè¯ä¾èµ–äºå¤§å‹è¡¨æ ¼æ•°æ®é›†ï¼Œä½†æ‰‹åŠ¨åˆ†æé€Ÿåº¦ç¼“æ…¢ã€æˆæœ¬é«˜æ˜‚ä¸”æ˜“å‡ºé”™ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›äº†æœ‰å‰æ™¯çš„è‡ªåŠ¨åŒ–æ½œåŠ›ï¼Œä½†åœ¨åˆ†ææ¨ç†ã€ç»“æ„åŒ–æ•°æ®å¤„ç†å’Œæ­§ä¹‰è§£å†³æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†GateLensï¼Œä¸€ä¸ªåŸºäºLLMç”¨äºåˆ†ææ±½è½¦é¢†åŸŸè¡¨æ ¼æ•°æ®çš„ç³»ç»Ÿã€‚GateLenså°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç¿»è¯‘æˆå…³ç³»ä»£æ•°ï¼ˆRAï¼‰è¡¨è¾¾å¼ï¼Œå¹¶ç”Ÿæˆä¼˜åŒ–çš„Pythonä»£ç ã€‚ä¸ä¼ ç»Ÿçš„å¤šä»£ç†æˆ–åŸºäºè§„åˆ’çš„ç³»ç»Ÿç›¸æ¯”ï¼Œè¿™äº›ç³»ç»Ÿå¯èƒ½é€Ÿåº¦è¾ƒæ…¢ã€ä¸é€æ˜ä¸”ç»´æŠ¤æˆæœ¬é«˜æ˜‚ï¼ŒGateLenså¼ºè°ƒé€Ÿåº¦ã€é€æ˜åº¦å’Œå¯é æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨çœŸå®æ•°æ®é›†ä¸Šï¼ŒGateLensä¼˜äºç°æœ‰çš„åŸºäºæ€ç»´é“¾ï¼ˆCoTï¼‰å’Œè‡ªæˆ‘ä¸€è‡´æ€§ï¼ˆSCï¼‰çš„ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚å’Œæ¨¡ç³ŠæŸ¥è¯¢æ–¹é¢ã€‚æ¶ˆèç ”ç©¶è¯å®äº†RAå±‚çš„å…³é”®ä½œç”¨ã€‚å·¥ä¸šéƒ¨ç½²æ˜¾ç¤ºï¼Œåœ¨åˆ†ææ—¶é—´æ–¹é¢å‡å°‘äº†80%ä»¥ä¸Šï¼ŒåŒæ—¶åœ¨æµ‹è¯•ç»“æœè§£è¯»ã€å½±å“è¯„ä¼°å’Œå‘å¸ƒå€™é€‰è¯„ä¼°ç­‰æ–¹é¢ä¿æŒäº†é«˜å‡†ç¡®æ€§ã€‚GateLensåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹è¿è¡Œæœ‰æ•ˆï¼Œæ— éœ€å°‘é‡æ ·æœ¬æˆ–ä»£ç†ç¼–æ’ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡è¯†åˆ«å…³é”®æ¶æ„ç‰¹å¾â€”â€”ä¸­é—´æ­£å¼è¡¨ç¤ºã€æ‰§è¡Œæ•ˆç‡å’Œä½é…ç½®å¼€é”€ï¼Œæ¥æ¨è¿›å¯éƒ¨ç½²çš„LLMç³»ç»Ÿè®¾è®¡ï¼Œå¯¹äºå®‰å…¨å…³é”®çš„å·¥ä¸šåº”ç”¨è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.21735v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†GateLensç³»ç»Ÿï¼Œä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ±½è½¦é¢†åŸŸè¡¨æ ¼æ•°æ®åˆ†æç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿå°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬åŒ–ä¸ºå…³ç³»ä»£æ•°è¡¨è¾¾å¼å¹¶ç”Ÿæˆä¼˜åŒ–çš„Pythonä»£ç ï¼Œå…·æœ‰å¿«é€Ÿã€é€æ˜å’Œå¯é çš„ç‰¹ç‚¹ã€‚ç›¸è¾ƒäºä¼ ç»Ÿç³»ç»Ÿï¼Œå®ƒåœ¨å¤„ç†å¤æ‚å’Œæ¨¡ç³ŠæŸ¥è¯¢æ—¶è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œå¹¶èƒ½åœ¨å·¥ä¸šéƒ¨ç½²ä¸­å¤§å¹…å‡å°‘åˆ†ææ—¶é—´åŒæ—¶ä¿æŒé«˜ç²¾åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ±½è½¦åˆ¶é€ ä¸šä¸­ï¼Œè½¯ä»¶å‘å¸ƒçš„å¯é æ€§å†³ç­–è‡³å…³é‡è¦ã€‚</li>
<li>é‡Šæ”¾éªŒè¯ä¾èµ–äºå¤§å‹è¡¨æ ¼æ•°æ®é›†ï¼Œä½†æ‰‹åŠ¨åˆ†ææ˜¯ç¼“æ…¢ã€æ˜‚è´µå’Œæ˜“å‡ºé”™çš„ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–æ½œåŠ›æ–¹é¢å…·æœ‰å‰æ™¯ï¼Œä½†åœ¨åˆ†ææ¨ç†ã€ç»“æ„æ•°æ®å¤„ç†å’Œæ­§ä¹‰è§£å†³æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>GateLensç³»ç»Ÿä»‹ç»ï¼šåˆ©ç”¨LLMåˆ†ææ±½è½¦é¢†åŸŸçš„è¡¨æ ¼æ•°æ®ï¼Œå°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬åŒ–ä¸ºå…³ç³»ä»£æ•°è¡¨è¾¾å¼ï¼Œå¹¶ç”Ÿæˆä¼˜åŒ–çš„Pythonä»£ç ã€‚</li>
<li>GateLenså¼ºè°ƒé€Ÿåº¦ã€é€æ˜åº¦å’Œå¯é æ€§ï¼Œä¼˜äºä¼ ç»Ÿå¤šä»£ç†æˆ–åŸºäºè§„åˆ’çš„ç³»ç»Ÿã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒGateLensåœ¨çœŸå®æ•°æ®é›†ä¸Šä¼˜äºç°æœ‰çš„Chain-of-Thoughtï¼ˆCoTï¼‰+ Self-Consistencyï¼ˆSCï¼‰ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚å’Œæ¨¡ç³ŠæŸ¥è¯¢æ–¹é¢ã€‚</li>
<li>å·¥ä¸šåŒ–éƒ¨ç½²æ˜¾ç¤ºï¼Œåˆ†ææ—¶é—´å‡å°‘äº†80%ä»¥ä¸Šï¼ŒåŒæ—¶åœ¨æµ‹è¯•ç»“æœè§£è¯»ã€å½±å“è¯„ä¼°å’Œå‘å¸ƒå€™é€‰è¯„ä¼°æ–¹é¢ä¿æŒäº†é«˜ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.21735">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c51b8c69be5f43f4eb9dcdb0afa4b681.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b5ec0619f840603efe3c0f1ba349e1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d091d497ea76aee67d1aeebee0484915.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-61d0f7e7fb5f585249d7e75ffc04c86c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="PRIMAL-Physically-Reactive-and-Interactive-Motor-Model-for-Avatar-Learning"><a href="#PRIMAL-Physically-Reactive-and-Interactive-Motor-Model-for-Avatar-Learning" class="headerlink" title="PRIMAL: Physically Reactive and Interactive Motor Model for Avatar   Learning"></a>PRIMAL: Physically Reactive and Interactive Motor Model for Avatar   Learning</h2><p><strong>Authors:Yan Zhang, Yao Feng, AlpÃ¡r Cseke, Nitin Saini, Nathan Bajandas, Nicolas Heron, Michael J. Black</strong></p>
<p>We formulate the motor system of an interactive avatar as a generative motion model that can drive the body to move through 3D space in a perpetual, realistic, controllable, and responsive manner. Although human motion generation has been extensively studied, many existing methods lack the responsiveness and realism of real human movements. Inspired by recent advances in foundation models, we propose PRIMAL, which is learned with a two-stage paradigm. In the pretraining stage, the model learns body movements from a large number of sub-second motion segments, providing a generative foundation from which more complex motions are built. This training is fully unsupervised without annotations. Given a single-frame initial state during inference, the pretrained model not only generates unbounded, realistic, and controllable motion, but also enables the avatar to be responsive to induced impulses in real time. In the adaptation phase, we employ a novel ControlNet-like adaptor to fine-tune the base model efficiently, adapting it to new tasks such as few-shot personalized action generation and spatial target reaching. Evaluations show that our proposed method outperforms state-of-the-art baselines. We leverage the model to create a real-time character animation system in Unreal Engine that feels highly responsive and natural. Code, models, and more results are available at: <a target="_blank" rel="noopener" href="https://yz-cnsdqz.github.io/eigenmotion/PRIMAL">https://yz-cnsdqz.github.io/eigenmotion/PRIMAL</a> </p>
<blockquote>
<p>æˆ‘ä»¬å°†äº¤äº’å¼è§’è‰²çš„è¿åŠ¨ç³»ç»Ÿå»ºæ¨¡ä¸ºä¸€ä¸ªç”Ÿæˆè¿åŠ¨æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿé©±åŠ¨è§’è‰²åœ¨ä¸‰ç»´ç©ºé—´ä¸­ä»¥æŒç»­ã€çœŸå®ã€å¯æ§å’Œå“åº”è¿…é€Ÿçš„æ–¹å¼ç§»åŠ¨ã€‚å°½ç®¡äººç±»è¿åŠ¨ç”Ÿæˆå·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†è®¸å¤šç°æœ‰æ–¹æ³•ç¼ºä¹çœŸå®äººç±»è¿åŠ¨çš„å“åº”æ€§å’ŒçœŸå®æ€§ã€‚å—åŸºç¡€æ¨¡å‹æœ€æ–°è¿›å±•çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†PRIMALæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µå­¦ä¹ èŒƒå¼ã€‚åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼Œæ¨¡å‹ä»å¤§é‡äºšç§’çº§è¿åŠ¨ç‰‡æ®µä¸­å­¦ä¹ èº«ä½“åŠ¨ä½œï¼Œä¸ºæ„å»ºæ›´å¤æ‚çš„åŠ¨ä½œæä¾›äº†ä¸€ä¸ªç”ŸæˆåŸºç¡€ã€‚è¿™ç§è®­ç»ƒæ˜¯å®Œå…¨æ— ç›‘ç£çš„ï¼Œæ— éœ€æ³¨é‡Šã€‚ç»™å®šæ¨ç†è¿‡ç¨‹ä¸­çš„å•å¸§åˆå§‹çŠ¶æ€ï¼Œé¢„è®­ç»ƒæ¨¡å‹ä¸ä»…ç”Ÿæˆæ— ç•Œé™ã€çœŸå®ã€å¯æ§çš„è¿åŠ¨ï¼Œè¿˜ä½¿è§’è‰²èƒ½å¤Ÿå®æ—¶å“åº”äº§ç”Ÿçš„è„‰å†²ã€‚åœ¨é€‚åº”é˜¶æ®µï¼Œæˆ‘ä»¬é‡‡ç”¨ç±»ä¼¼ControlNetçš„é€‚é…å™¨å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶é€‚åº”æ–°ä»»åŠ¡ï¼Œå¦‚å°æ ·æœ¬äººæ ¼åŒ–åŠ¨ä½œç”Ÿæˆå’Œç©ºé—´ç›®æ ‡è¾¾æˆã€‚è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¼˜äºæœ€æ–°åŸºçº¿ã€‚æˆ‘ä»¬åˆ©ç”¨è¯¥æ¨¡å‹åœ¨Unreal Engineä¸­åˆ›å»ºäº†ä¸€ä¸ªå®æ—¶è§’è‰²åŠ¨ç”»ç³»ç»Ÿï¼Œæ„Ÿè§‰éå¸¸å“åº”ä¸”è‡ªç„¶ã€‚ä»£ç ã€æ¨¡å‹å’Œæ›´å¤šç»“æœè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://yz-cnsdqz.github.io/eigenmotion/PRIMAL">https://yz-cnsdqz.github.io/eigenmotion/PRIMAL</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17544v2">PDF</a> ICCVâ€™25 camera ready; main paper and appendix; 19 pages in total</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§äº¤äº’å¼è§’è‰²çš„è¿åŠ¨ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé‡‡ç”¨ç”Ÿæˆè¿åŠ¨æ¨¡å‹çš„æ–¹å¼ï¼Œä½¿è§’è‰²èƒ½å¤Ÿåœ¨ä¸‰ç»´ç©ºé—´ä¸­å®ç°æŒä¹…ã€çœŸå®ã€å¯æ§å’Œå“åº”è¿…é€Ÿçš„åŠ¨ä½œã€‚ç³»ç»Ÿé‡‡ç”¨ä¸¤é˜¶æ®µå­¦ä¹ æ³•ï¼Œé¦–å…ˆåœ¨å¤§é‡æ— æ ‡æ³¨çš„å­ç§’çº§è¿åŠ¨ç‰‡æ®µä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä¸ºå¤æ‚åŠ¨ä½œç”Ÿæˆæä¾›åŸºç¡€ï¼›ç„¶ååœ¨æ¨ç†é˜¶æ®µé€šè¿‡æ§åˆ¶ç½‘ç»œé€‚é…å™¨è¿›è¡Œå¾®è°ƒï¼Œé€‚åº”æ–°ä»»åŠ¡å¦‚ä¸ªæ€§åŒ–åŠ¨ä½œç”Ÿæˆå’Œç©ºé—´ç›®æ ‡è¾¾åˆ°ç­‰ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¹¶åœ¨Unreal Engineä¸­å®ç°äº†å®æ—¶å“åº”å’Œè‡ªç„¶åŠ¨ç”»çš„è§’è‰²ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº¤äº’å¼è§’è‰²çš„è¿åŠ¨ç³»ç»Ÿé€šè¿‡ç”Ÿæˆè¿åŠ¨æ¨¡å‹å®ç°è§’è‰²çš„æŒä¹…ã€çœŸå®ã€å¯æ§å’Œå“åº”è¿…é€Ÿçš„åŠ¨ä½œã€‚</li>
<li>ç³»ç»Ÿé‡‡ç”¨ä¸¤é˜¶æ®µå­¦ä¹ æ³•ï¼Œå…ˆè¿›è¡Œé¢„è®­ç»ƒå­¦ä¹ åŸºç¡€åŠ¨ä½œï¼Œç„¶åè¿›è¡Œå¾®è°ƒé€‚åº”æ–°ä»»åŠ¡ã€‚</li>
<li>é¢„è®­ç»ƒé˜¶æ®µä½¿ç”¨å¤§é‡æ— æ ‡æ³¨çš„å­ç§’çº§è¿åŠ¨ç‰‡æ®µï¼Œä¸ºå¤æ‚åŠ¨ä½œç”Ÿæˆæä¾›åŸºç¡€ã€‚</li>
<li>æ§åˆ¶ç½‘ç»œé€‚é…å™¨ç”¨äºåœ¨æ¨ç†é˜¶æ®µè¿›è¡Œå¾®è°ƒï¼Œé€‚åº”ä¸ªæ€§åŒ–åŠ¨ä½œç”Ÿæˆå’Œç©ºé—´ç›®æ ‡è¾¾åˆ°ç­‰ä»»åŠ¡ã€‚</li>
<li>è¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæä¾›äº†æ›´çœŸå®çš„è§’è‰²è¿åŠ¨ç”Ÿæˆã€‚</li>
<li>ç³»ç»Ÿåœ¨Unreal Engineä¸­å®ç°äº†å®æ—¶å“åº”å’Œè‡ªç„¶åŠ¨ç”»çš„è§’è‰²ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17544">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55ff1ebe5e5ce5169c42ff2cebc0dcf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-529d645fda8a944e9b3a0d78858355df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a40f2a72bc54eed874291e26ebc48a9.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Beyond-Scaling-Measuring-and-Predicting-the-Upper-Bound-of-Knowledge-Retention-in-Language-Model-Pre-Training"><a href="#Beyond-Scaling-Measuring-and-Predicting-the-Upper-Bound-of-Knowledge-Retention-in-Language-Model-Pre-Training" class="headerlink" title="Beyond Scaling: Measuring and Predicting the Upper Bound of Knowledge   Retention in Language Model Pre-Training"></a>Beyond Scaling: Measuring and Predicting the Upper Bound of Knowledge   Retention in Language Model Pre-Training</h2><p><strong>Authors:Changhao Jiang, Ming Zhang, Junjie Ye, Xiaoran Fan, Yifei Cao, Jiajun Sun, Zhiheng Xi, Shihan Dou, Yi Dong, Yujiong Shen, Jingqi Tong, Baoyu Fan, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Qi Zhang, Tao Gui, Xuanjing Huang</strong></p>
<p>The GPT-4 technical report highlights the possibility of predicting model performance on downstream tasks using only pre-training signals, though detailed methodologies are absent. Such predictive capabilities are essential for resource-efficient pre-training and the construction of task-aligned datasets. In this paper, we aim to predict performance in closed-book question answering (QA), a vital downstream task that directly reflects a modelâ€™s internalized knowledge without the help of external tools. We address three primary challenges: (1) limited access to and understanding of pre-training corpora, (2) limitations of current evaluation methods for pre-trained models, and (3) limitations of frequency-based metrics in predicting model performance. In response, we conduct large-scale retrieval and semantic analysis across the pre-training corpora of 21 publicly available and 3 custom-trained large language models. We then develop a multi-template QA evaluation framework incorporating paraphrased question variants. Building on these foundations, we propose Size-dependent Mutual Information (SMI), an information-theoretic metric that linearly correlates pre-training data characteristics, model size, and QA accuracy, without requiring additional training. Experimental results show that SMI outperforms co-occurrence-based baselines, achieving $R^2 &gt; 0.75$ on models with over one billion parameters. Theoretical analysis further suggests an upper bound of around 80% QA accuracy under optimal pre-training, reflecting intrinsic memory limitations and motivating the use of retrieval or few-shot methods in later stages. </p>
<blockquote>
<p>GPT-4æŠ€æœ¯æŠ¥å‘Šå¼ºè°ƒäº†ä»…ä½¿ç”¨é¢„è®­ç»ƒä¿¡å·é¢„æµ‹æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šæ€§èƒ½çš„å¯è¡Œæ€§ï¼Œè™½ç„¶ç¼ºä¹è¯¦ç»†çš„æ–¹æ³•è®ºã€‚è¿™ç§é¢„æµ‹èƒ½åŠ›å¯¹äºèµ„æºé«˜æ•ˆçš„é¢„è®­ç»ƒå’Œä»»åŠ¡å¯¹é½æ•°æ®é›†çš„æ„å»ºè‡³å…³é‡è¦ã€‚æœ¬æ–‡æ—¨åœ¨é¢„æµ‹å°é—­å¼é—®ç­”ï¼ˆQAï¼‰ä¸­çš„æ€§èƒ½ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œèƒ½å¤Ÿç›´æ¥åæ˜ æ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†ï¼Œæ— éœ€å¤–éƒ¨å·¥å…·çš„å¸®åŠ©ã€‚æˆ‘ä»¬è§£å†³äº†ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰å¯¹é¢„è®­ç»ƒè¯­æ–™åº“çš„æœ‰é™è®¿é—®å’Œäº†è§£ï¼Œï¼ˆ2ï¼‰å½“å‰é¢„è®­ç»ƒæ¨¡å‹è¯„ä¼°æ–¹æ³•çš„å±€é™æ€§ï¼Œï¼ˆ3ï¼‰åŸºäºé¢‘ç‡çš„æŒ‡æ ‡åœ¨é¢„æµ‹æ¨¡å‹æ€§èƒ½æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¹21ä¸ªå…¬å¼€å¯ç”¨çš„å’Œ3ä¸ªè‡ªå®šä¹‰è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒè¯­æ–™åº“è¿›è¡Œäº†å¤§è§„æ¨¡æ£€ç´¢å’Œè¯­ä¹‰åˆ†æã€‚ç„¶åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤šæ¨¡æ¿QAè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æ”¹è¿°é—®é¢˜å˜ä½“ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†Size-dependent Mutual Informationï¼ˆSMIï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¿¡æ¯ç†è®ºæŒ‡æ ‡ï¼Œå®ƒçº¿æ€§åœ°å…³è”é¢„è®­ç»ƒæ•°æ®ç‰¹æ€§ã€æ¨¡å‹å¤§å°ä¸é—®ç­”å‡†ç¡®æ€§ï¼Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSMIä¼˜äºåŸºäºå…±ç°çš„åŸºçº¿ï¼Œåœ¨å…·æœ‰è¶…è¿‡åäº¿å‚æ•°çš„æ¨¡å‹ä¸Šå®ç°äº†RÂ²&gt; 0.75ã€‚ç†è®ºåˆ†æè¿›ä¸€æ­¥è¡¨æ˜ï¼Œåœ¨æœ€ä½³é¢„è®­ç»ƒæƒ…å†µä¸‹ï¼ŒQAå‡†ç¡®ç‡çš„ä¸Šé™çº¦ä¸º80%ï¼Œè¿™åæ˜ äº†å†…åœ¨çš„å­˜å‚¨é™åˆ¶ï¼Œå¹¶æ¿€åŠ±åœ¨åæœŸä½¿ç”¨æ£€ç´¢æˆ–å°æ ·æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04066v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨é¢„æµ‹å¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨å°é—­é—®ç­”ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¯¹å¤šä¸ªé¢„è®­ç»ƒè¯­æ–™åº“çš„è¯­ä¹‰åˆ†æï¼Œæå‡ºäº†åŸºäºä¿¡æ¯ç†è®ºçš„åº¦é‡æ ‡å‡†â€”â€”Size-dependent Mutual Informationï¼ˆSMIï¼‰ã€‚è¯¥åº¦é‡æ ‡å‡†èƒ½å¤Ÿåæ˜ é¢„è®­ç»ƒæ•°æ®ç‰¹æ€§ã€æ¨¡å‹å¤§å°ä¸é—®ç­”å‡†ç¡®åº¦çš„å…³ç³»ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSMIç›¸è¾ƒäºåŸºäºå…±ç°çš„åŸºçº¿æ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œåœ¨ç™¾äº¿å‚æ•°ä»¥ä¸Šçš„æ¨¡å‹ä¸Šè¾¾åˆ°RÂ²å¤§äº0.75çš„æ•ˆæœã€‚æ­¤å¤–ï¼Œç†è®ºåˆ†æç»“æœæŒ‡å‡ºï¼Œåœ¨æœ€ä½³é¢„è®­ç»ƒæ¡ä»¶ä¸‹ï¼ŒQAå‡†ç¡®ç‡çš„ä¸Šé™çº¦ä¸º80%ï¼Œè¿™åæ˜ äº†å†…åœ¨çš„è®°å¿†é™åˆ¶å¹¶é¼“åŠ±åç»­é˜¶æ®µä½¿ç”¨æ£€ç´¢æˆ–å°æ ·æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4æŠ€æœ¯æŠ¥å‘Šå¼ºè°ƒäº†ä»…ä½¿ç”¨é¢„è®­ç»ƒä¿¡å·é¢„æµ‹æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­æ€§èƒ½çš„å¯èƒ½æ€§ã€‚</li>
<li>é’ˆå¯¹èµ„æºé«˜æ•ˆçš„é¢„è®­ç»ƒå’Œä»»åŠ¡å¯¹é½æ•°æ®é›†çš„å»ºè®¾ï¼Œé¢„æµ‹èƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>åœ¨å°é—­é—®ç­”ä»»åŠ¡ä¸­é¢„æµ‹æ¨¡å‹æ€§èƒ½æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œè¯¥ä»»åŠ¡ç›´æ¥åæ˜ æ¨¡å‹çš„å†…éƒ¨çŸ¥è¯†ä¸”æ— éœ€å¤–éƒ¨å·¥å…·å¸®åŠ©ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿé€šè¿‡å¤§è§„æ¨¡æ£€ç´¢å’Œè¯­ä¹‰åˆ†æå¤šä¸ªé¢„è®­ç»ƒè¯­æ–™åº“æ¥åº”å¯¹æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†Size-dependent Mutual Informationï¼ˆSMIï¼‰è¿™ä¸€ä¿¡æ¯ç†è®ºåº¦é‡æ ‡å‡†ï¼Œèƒ½å¤Ÿçº¿æ€§å…³è”é¢„è®­ç»ƒæ•°æ®ç‰¹æ€§ã€æ¨¡å‹å¤§å°ä¸é—®ç­”å‡†ç¡®åº¦ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºSMIç›¸è¾ƒäºå…±ç°åŸºçº¿æœ‰æ›´å¥½çš„è¡¨ç°ï¼Œå¹¶åœ¨ç™¾äº¿å‚æ•°ä»¥ä¸Šçš„æ¨¡å‹ä¸Šå–å¾—è¾ƒé«˜å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04066">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9d0134f46fff689697c639919c07d7e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0264b678e108df6d83390f4d0e80b7c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a03edec00f314858be33645c298981e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="UoMo-A-Foundation-Model-for-Mobile-Traffic-Forecasting-with-Diffusion-Model"><a href="#UoMo-A-Foundation-Model-for-Mobile-Traffic-Forecasting-with-Diffusion-Model" class="headerlink" title="UoMo: A Foundation Model for Mobile Traffic Forecasting with Diffusion   Model"></a>UoMo: A Foundation Model for Mobile Traffic Forecasting with Diffusion   Model</h2><p><strong>Authors:Haoye Chai, Shiyuan Zhang, Xiaoqian Qi, Baohua Qiu, Yong Li</strong></p>
<p>Mobile traffic forecasting allows operators to anticipate network dynamics and performance in advance, offering substantial potential for enhancing service quality and improving user experience. However, existing models are often task-oriented and are trained with tailored data, which limits their effectiveness in diverse mobile network tasks of Base Station (BS) deployment, resource allocation, energy optimization, etc. and hinders generalization across different urban environments. Foundation models have made remarkable strides across various domains of NLP and CV due to their multi-tasking adaption and zero&#x2F;few-shot learning capabilities. In this paper, we propose an innovative Foundation model for Mo}bile traffic forecasting (FoMo), aiming to handle diverse forecasting tasks of short&#x2F;long-term predictions and distribution generation across multiple cities to support network planning and optimization. FoMo combines diffusion models and transformers, where various spatio-temporal masks are proposed to enable FoMo to learn intrinsic features of different tasks, and a contrastive learning strategy is developed to capture the correlations between mobile traffic and urban contexts, thereby improving its transfer learning capability. Extensive experiments on 9 real-world datasets demonstrate that FoMo outperforms current models concerning diverse forecasting tasks and zero&#x2F;few-shot learning, showcasing a strong universality. </p>
<blockquote>
<p>ç§»åŠ¨æµé‡é¢„æµ‹ä½¿è¿è¥å•†èƒ½å¤Ÿæå‰é¢„æµ‹ç½‘ç»œåŠ¨æ€å’Œæ€§èƒ½ï¼Œä¸ºæé«˜æœåŠ¡è´¨é‡å’Œæ”¹å–„ç”¨æˆ·ä½“éªŒæä¾›äº†å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹é€šå¸¸æ˜¯é¢å‘ä»»åŠ¡çš„ï¼Œå¹¶ä¸”ä½¿ç”¨å®šåˆ¶æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨åŸºç«™éƒ¨ç½²ã€èµ„æºé…ç½®ã€èƒ½æºä¼˜åŒ–ç­‰å¤šæ ·åŒ–çš„ç§»åŠ¨ç½‘ç»œä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶é˜»ç¢äº†å®ƒä»¬åœ¨ä¸åŒåŸå¸‚ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ç”±äºå…¶åœ¨å¤šä»»åŠ¡é€‚åº”å’Œé›¶&#x2F;å°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›æ–¹é¢çš„ä¼˜åŠ¿ï¼ŒåŸºç¡€æ¨¡å‹åœ¨NLPå’Œè®¡ç®—æœºè§†è§‰çš„ä¸åŒé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„ç§»åŠ¨æµé‡é¢„æµ‹åŸºç¡€æ¨¡å‹ï¼ˆFoMoï¼‰ï¼Œæ—¨åœ¨å¤„ç†çŸ­æœŸ&#x2F;é•¿æœŸé¢„æµ‹å’Œè·¨å¤šä¸ªåŸå¸‚è¿›è¡Œåˆ†å¸ƒç”Ÿæˆçš„å¤šæ ·åŒ–é¢„æµ‹ä»»åŠ¡ï¼Œä»¥æ”¯æŒç½‘ç»œè§„åˆ’å’Œä¼˜åŒ–ã€‚FoMoç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œå˜å‹å™¨ï¼Œæå‡ºäº†å„ç§æ—¶ç©ºæ©ç ï¼Œä½¿FoMoèƒ½å¤Ÿå­¦ä¹ ä¸åŒä»»åŠ¡çš„å†…è•´ç‰¹å¾ï¼Œå¹¶å¼€å‘äº†ä¸€ç§å¯¹æ¯”å­¦ä¹ ç­–ç•¥æ¥æ•æ‰ç§»åŠ¨æµé‡å’ŒåŸå¸‚ä¸Šä¸‹æ–‡ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»è€Œæé«˜å…¶è¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚åœ¨9ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFoMoåœ¨å¤šæ ·åŒ–çš„é¢„æµ‹ä»»åŠ¡å’Œé›¶&#x2F;å°‘æ ·æœ¬å­¦ä¹ ä¸Šä¼˜äºå½“å‰æ¨¡å‹ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„é€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15322v3">PDF</a> 2025 ACM SIGKDD International Conference on Knowledge Discovery and   Data Mining, KDD 2025</p>
<p><strong>Summary</strong></p>
<p>ç§»åŠ¨æµé‡é¢„æµ‹æœ‰åŠ©äºè¿è¥å•†æå‰é¢„æµ‹ç½‘ç»œåŠ¨æ€å’Œæ€§èƒ½ï¼Œå¯¹æé«˜æœåŠ¡è´¨é‡å’Œç”¨æˆ·ä½“éªŒæœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹å¾€å¾€é’ˆå¯¹ç‰¹å®šä»»åŠ¡å¹¶ä½¿ç”¨ç‰¹å®šæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œè¿™åœ¨å¤šæ ·åŒ–çš„ç§»åŠ¨ç½‘ç»œä»»åŠ¡ï¼ˆå¦‚åŸºç«™éƒ¨ç½²ã€èµ„æºåˆ†é…ã€èƒ½æºä¼˜åŒ–ç­‰ï¼‰ä¸­é™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶é˜»ç¢äº†å…¶åœ¨ä¸åŒåŸå¸‚ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„ç§»åŠ¨æµé‡é¢„æµ‹åŸºç¡€æ¨¡å‹ï¼ˆFoMoï¼‰ï¼Œæ—¨åœ¨å¤„ç†çŸ­æœŸ&#x2F;é•¿æœŸé¢„æµ‹å’Œè·¨å¤šä¸ªåŸå¸‚çš„åˆ†å¸ƒç”Ÿæˆçš„å¤šæ ·åŒ–é¢„æµ‹ä»»åŠ¡ï¼Œä»¥æ”¯æŒç½‘ç»œè§„åˆ’å’Œä¼˜åŒ–ã€‚FoMoç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œè½¬æ¢å™¨ï¼Œé€šè¿‡æå‡ºå„ç§æ—¶ç©ºæ©ç æ¥ä½¿æ¨¡å‹å­¦ä¹ ä¸åŒä»»åŠ¡çš„å†…è•´ç‰¹å¾ï¼Œå¹¶å¼€å‘äº†ä¸€ç§å¯¹æ¯”å­¦ä¹ ç­–ç•¥æ¥æ•æ‰ç§»åŠ¨æµé‡å’ŒåŸå¸‚ç¯å¢ƒä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä»è€Œæé«˜äº†å…¶è¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚åœ¨9ä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒFoMoåœ¨å¤šæ ·åŒ–çš„é¢„æµ‹ä»»åŠ¡å’Œé›¶&#x2F;å°‘æ ·æœ¬å­¦ä¹ ä¸Šä¼˜äºå½“å‰æ¨¡å‹ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§»åŠ¨æµé‡é¢„æµ‹å¯¹äºç½‘ç»œè¿è¥å•†æ¥è¯´éå¸¸é‡è¦ï¼Œæœ‰åŠ©äºä»–ä»¬æå‰äº†è§£ç½‘ç»œåŠ¨æ€å’Œæ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨å¤šæ ·åŒ–çš„ç§»åŠ¨ç½‘ç»œä»»åŠ¡ä¸­å—é™äºç‰¹å®šä»»åŠ¡å’Œæ•°æ®çš„è®­ç»ƒï¼Œå½±å“äº†å…¶åœ¨ä¸åŒåŸå¸‚ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„ç§»åŠ¨æµé‡é¢„æµ‹åŸºç¡€æ¨¡å‹ï¼ˆFoMoï¼‰ï¼Œæ—¨åœ¨å¤„ç†å¤šç§é¢„æµ‹ä»»åŠ¡ï¼ŒåŒ…æ‹¬çŸ­æœŸå’Œé•¿æœŸé¢„æµ‹ï¼Œä»¥åŠè·¨å¤šä¸ªåŸå¸‚çš„åˆ†å¸ƒç”Ÿæˆã€‚</li>
<li>FoMoç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œè½¬æ¢å™¨ï¼Œé€šè¿‡æ—¶ç©ºæ©ç å’Œå¯¹æ¯”å­¦ä¹ ç­–ç•¥æ¥æ•æ‰ç§»åŠ¨æµé‡å’ŒåŸå¸‚ç¯å¢ƒä¹‹é—´çš„ç›¸å…³æ€§ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ ç­–ç•¥æœ‰åŠ©äºæé«˜æ¨¡å‹çš„è¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>åœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFoMoåœ¨å¤šæ ·åŒ–çš„é¢„æµ‹ä»»åŠ¡å’Œé›¶&#x2F;å°‘æ ·æœ¬å­¦ä¹ ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„é€šç”¨æ€§å’Œä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15322">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d14267085979ee01c739af38fefea32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abc6db5748181dee09b3d6b3846c321c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cdc1424af55d5a06aa3f34f84b9b64c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c0938987dfb55e57889a2ea28bc0ebb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-056f6c29e55df18d472809f403355ddc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ee8b9f1d756354259d901fdabdca45b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f6e46ae053b430c5104de225f1c4430.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="THREAD-Thinking-Deeper-with-Recursive-Spawning"><a href="#THREAD-Thinking-Deeper-with-Recursive-Spawning" class="headerlink" title="THREAD: Thinking Deeper with Recursive Spawning"></a>THREAD: Thinking Deeper with Recursive Spawning</h2><p><strong>Authors:Philip Schroeder, Nathaniel Morgan, Hongyin Luo, James Glass</strong></p>
<p>Large language models (LLMs) have shown impressive capabilities across diverse settings, but still struggle as the length and complexity of the context increases. To address this challenge, we propose Thinking Recursively and Dynamically (ThReaD). THREAD frames model generation as a thread of execution that, based on the context, can run to completion or dynamically spawn new threads. By spawning, threads can offload work (e.g., thinking, retrieving information) to child threads, which only return tokens needed for the parent thread to do its work. In effect, this enables the model to adapt, as needed, the amount of intermediate work used to produce tokens. We apply THREAD in the settings of LLM task solving and question answering, where the dynamic threading allows the model to recursively decompose the given task or question into progressively simpler sub-problems that can be solved by separate child threads. We test THREAD, implemented using a few-shot learning approach, on diverse benchmarks for agent tasks and data-grounded question answering. THREAD achieves state-of-the-art performance with GPT-4 and GPT-3.5 on these benchmarks, including ALFWorld, TextCraft, and WebShop, along with two new benchmarks, DataCommons QA and MIMIC-III ICU QA. In addition, THREAD outperforms existing frameworks by 10% to 50% absolute points with smaller models, including Llama-3-8b and CodeLlama-7b. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§åœºæ™¯ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œä½†éšç€ä¸Šä¸‹æ–‡é•¿åº¦å’Œå¤æ‚æ€§çš„å¢åŠ ï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é€’å½’åŠ¨æ€æ€è€ƒï¼ˆThreadï¼‰ã€‚Threadå°†æ¨¡å‹ç”Ÿæˆæ„å»ºä¸ºæ‰§è¡Œçº¿ç¨‹ï¼Œå¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡è¿è¡Œåˆ°å®Œæˆæˆ–åŠ¨æ€ç”Ÿæˆæ–°çº¿ç¨‹ã€‚é€šè¿‡ç”Ÿæˆçº¿ç¨‹ï¼Œå¯ä»¥å°†å·¥ä½œï¼ˆå¦‚æ€è€ƒã€æ£€ç´¢ä¿¡æ¯ï¼‰å§”æ´¾ç»™å­çº¿ç¨‹ï¼Œå­çº¿ç¨‹åªè¿”å›çˆ¶çº¿ç¨‹å®Œæˆå·¥ä½œæ‰€éœ€çš„ä»¤ç‰Œã€‚å®é™…ä¸Šï¼Œè¿™å…è®¸æ¨¡å‹æ ¹æ®éœ€è¦é€‚åº”ç”¨äºäº§ç”Ÿä»¤ç‰Œçš„ä¸­é—´å·¥ä½œé‡ã€‚æˆ‘ä»¬åœ¨LLMä»»åŠ¡è§£å†³å’Œé—®ç­”åœºæ™¯ä¸­åº”ç”¨äº†Threadï¼Œå…¶ä¸­åŠ¨æ€çº¿ç¨‹å…è®¸æ¨¡å‹å°†ç»™å®šçš„ä»»åŠ¡æˆ–é—®é¢˜é€’å½’åœ°åˆ†è§£ä¸ºå¯ä»¥å•ç‹¬ç”±å­çº¿ç¨‹è§£å†³çš„è¶Šæ¥è¶Šç®€å•çš„å­é—®é¢˜ã€‚æˆ‘ä»¬ä½¿ç”¨å°‘é‡æ ·æœ¬å­¦ä¹ çš„æ–¹æ³•å®ç°äº†Threadï¼Œå¹¶åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸Šå¯¹ä»£ç†ä»»åŠ¡å’ŒåŸºäºæ•°æ®çš„é—®é¢˜å›ç­”è¿›è¡Œäº†æµ‹è¯•ã€‚Threadåœ¨åŒ…æ‹¬ALFWorldã€TextCraftã€WebShopä»¥åŠä¸¤ä¸ªæ–°åŸºå‡†æµ‹è¯•DataCommons QAå’ŒMIMIC-III ICU QAåœ¨å†…çš„å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸GPT-4å’ŒGPT-3.5çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¯¹äºè¾ƒå°çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬Llama-3-8bå’ŒCodeLlama-7bç­‰ï¼ŒThreadåœ¨ç»å¯¹å¾—åˆ†ä¸Šè¾ƒç°æœ‰æ¡†æ¶é«˜å‡º10%è‡³50%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.17402v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§åœºæ™¯ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨å¤„ç†å¤æ‚å’Œé•¿æ–‡æœ¬æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†é€’å½’åŠ¨æ€æ€è€ƒï¼ˆThinking Recursively and Dynamicallyï¼Œç®€ç§°ThReaDï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†æ¨¡å‹ç”Ÿæˆè§†ä¸ºæ‰§è¡Œçº¿ç¨‹ï¼Œèƒ½å¤Ÿæ ¹æ®ä¸Šä¸‹æ–‡è¿è¡Œè‡³å®Œæˆæˆ–åŠ¨æ€ç”Ÿæˆæ–°çº¿ç¨‹ã€‚æ–°çº¿ç¨‹çš„ç”Ÿæˆå¯ä»¥å¸ä¸‹å·¥ä½œï¼ˆå¦‚æ€è€ƒã€è·å–ä¿¡æ¯ï¼‰å¹¶å°†å…¶åˆ†é…ç»™å­çº¿ç¨‹ï¼Œä»…è¿”å›çˆ¶çº¿ç¨‹å®Œæˆå·¥ä½œæ‰€éœ€çš„æ ‡è®°ã€‚åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä»»åŠ¡è§£å†³å’Œé—®ç­”åœºæ™¯ä¸­ï¼ŒåŠ¨æ€çº¿ç¨‹ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿé€’å½’åœ°å°†ç»™å®šä»»åŠ¡æˆ–é—®é¢˜åˆ†è§£ä¸ºæ›´ç®€å•çš„å­é—®é¢˜ï¼Œè¿™äº›å­é—®é¢˜å¯ä»¥ç”±å•ç‹¬çš„å­çº¿ç¨‹è§£å†³ã€‚æœ¬ç ”ç©¶åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†Threadæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ä»£ç†ä»»åŠ¡å’ŒåŸºäºæ•°æ®çš„é—®é¢˜å›ç­”ã€‚Threadæ¡†æ¶å®ç°äº†åœ¨GPT-4å’ŒGPT-3.5ä¸Šçš„æœ€æ–°æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šç°æœ‰æ¡†æ¶ï¼Œç»å¯¹ç‚¹æ•°æé«˜äº†10%è‡³50%ã€‚å³ä½¿æ˜¯è¾ƒå°çš„æ¨¡å‹ï¼Œå¦‚Llama-3-8bå’ŒCodeLlama-7bä¹Ÿèƒ½å–å¾—æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚å’Œé•¿æ–‡æœ¬å¤„ç†æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ThReaDæ¡†æ¶å°†æ¨¡å‹ç”Ÿæˆè§†ä¸ºæ‰§è¡Œçº¿ç¨‹ï¼Œèƒ½åŠ¨æ€è°ƒæ•´å·¥ä½œè´Ÿè½½ã€‚</li>
<li>ThReaDé€šè¿‡åŠ¨æ€ç”Ÿæˆå­çº¿ç¨‹æ¥åˆ†è§£å¤æ‚ä»»åŠ¡æˆ–é—®é¢˜ã€‚</li>
<li>ThReaDæ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬ALFWorldã€TextCraftã€WebShopç­‰ã€‚</li>
<li>ThReaDå®ç°äº†åœ¨GPT-4å’ŒGPT-3.5ä¸Šçš„æœ€æ–°æ€§èƒ½ã€‚</li>
<li>ThReaDæ¡†æ¶åœ¨å°å‹æ¨¡å‹ä¸Šä¹Ÿæœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.17402">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5aa44080634543f14adb64409f14431a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed8b95fd851239b03d85304a78d8dd99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a5d15e33150e8cdf96e0dd18b9d8946.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1052b4987f43e6498994df003acadc2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8cee0c28bd33c04973229fbedbfe2b2d.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Continual-Adversarial-Defense"><a href="#Continual-Adversarial-Defense" class="headerlink" title="Continual Adversarial Defense"></a>Continual Adversarial Defense</h2><p><strong>Authors:Qian Wang, Hefei Ling, Yingwei Li, Qihao Liu, Ruoxi Jia, Ning Yu</strong></p>
<p>In response to the rapidly evolving nature of adversarial attacks against visual classifiers, numerous defenses have been proposed to generalize against as many known attacks as possible. However, designing a defense method that generalizes to all types of attacks is unrealistic, as the environment in which the defense system operates is dynamic. Over time, new attacks inevitably emerge that exploit the vulnerabilities of existing defenses and bypass them. Therefore, we propose a continual defense strategy under a practical threat model and, for the first time, introduce the Continual Adversarial Defense (CAD) framework. CAD continuously collects adversarial data online and adapts to evolving attack sequences, while adhering to four practical principles: (1) continual adaptation to new attacks without catastrophic forgetting, (2) few-shot adaptation, (3) memory-efficient adaptation, and (4) high classification accuracy on both clean and adversarial data. We explore and integrate cutting-edge techniques from continual learning, few-shot learning, and ensemble learning to fulfill the principles. Extensive experiments validate the effectiveness of our approach against multi-stage adversarial attacks and demonstrate significant improvements over a wide range of baseline methods. We further observe that CADâ€™s defense performance tends to saturate as the number of attacks increases, indicating its potential as a persistent defense once adapted to a sufficiently diverse set of attacks. Our research sheds light on a brand-new paradigm for continual defense adaptation against dynamic and evolving attacks. </p>
<blockquote>
<p>é’ˆå¯¹è§†è§‰åˆ†ç±»å™¨æ‰€é¢ä¸´çš„ä¸æ–­è¿›åŒ–çš„å¯¹æŠ—æ€§æ”»å‡»ï¼Œå·²ç»æå‡ºäº†è®¸å¤šé˜²å¾¡ç­–ç•¥æ¥åº”å¯¹å°½å¯èƒ½å¤šçš„å·²çŸ¥æ”»å‡»ã€‚ç„¶è€Œï¼Œè®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿåº”å¯¹æ‰€æœ‰ç±»å‹æ”»å‡»çš„é˜²å¾¡æ–¹æ³•æ˜¯ä¸ç°å®çš„ï¼Œå› ä¸ºé˜²å¾¡ç³»ç»Ÿæ‰€å¤„çš„ç¯å¢ƒæ˜¯åŠ¨æ€çš„ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œæ–°çš„æ”»å‡»ä¸å¯é¿å…åœ°ä¼šå‡ºç°ï¼Œåˆ©ç”¨ç°æœ‰é˜²å¾¡çš„æ¼æ´å¹¶ç»•è¿‡å®ƒä»¬ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨å®ç”¨çš„å¨èƒæ¨¡å‹ä¸‹æå‡ºäº†æŒç»­çš„é˜²å¾¡ç­–ç•¥ï¼Œå¹¶é¦–æ¬¡å¼•å…¥äº†æŒç»­å¯¹æŠ—æ€§é˜²å¾¡ï¼ˆCADï¼‰æ¡†æ¶ã€‚CADæŒç»­åœ¨çº¿æ”¶é›†å¯¹æŠ—æ€§æ•°æ®ï¼Œå¹¶é€‚åº”ä¸æ–­å˜åŒ–çš„æ”»å‡»åºåˆ—ï¼ŒåŒæ—¶éµå¾ªå››ä¸ªå®ç”¨åŸåˆ™ï¼šï¼ˆ1ï¼‰ä¸æ–­é€‚åº”æ–°æ”»å‡»è€Œä¸ä¼šå‘ç”Ÿç¾éš¾æ€§é—å¿˜ï¼Œï¼ˆ2ï¼‰å¿«é€Ÿé€‚åº”ï¼Œï¼ˆ3ï¼‰å†…å­˜é«˜æ•ˆé€‚åº”ï¼Œä»¥åŠï¼ˆ4ï¼‰åœ¨å¹²å‡€å’Œå¯¹æŠ—æ€§æ•°æ®ä¸Šéƒ½å…·æœ‰é«˜åˆ†ç±»å‡†ç¡®ç‡ã€‚æˆ‘ä»¬æ¢ç´¢å¹¶æ•´åˆäº†ç»ˆèº«å­¦ä¹ ã€å¿«é€Ÿå­¦ä¹ å’Œé›†æˆå­¦ä¹ çš„å‰æ²¿æŠ€æœ¯æ¥å®ç°è¿™äº›åŸåˆ™ã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬å¯¹å¤šé˜¶æ®µå¯¹æŠ—æ€§æ”»å‡»çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨ä¸€ç³»åˆ—åŸºå‡†æ–¹æ³•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è§‚å¯Ÿåˆ°ï¼Œéšç€æ”»å‡»æ•°é‡çš„å¢åŠ ï¼ŒCADçš„é˜²å¾¡æ€§èƒ½è¶‹äºé¥±å’Œï¼Œè¿™è¡¨æ˜ä¸€æ—¦é€‚åº”è¶³å¤Ÿå¤šæ ·åŒ–çš„æ”»å‡»é›†ï¼Œå…¶ä½œä¸ºæŒä¹…é˜²å¾¡çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºåº”å¯¹åŠ¨æ€å’Œä¸æ–­æ¼”å˜çš„æ”»å‡»çš„æŒç»­é˜²å¾¡é€‚åº”æä¾›äº†æ–°çš„èŒƒä¾‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.09481v6">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¢å¯¹è§†è§‰åˆ†ç±»å™¨ä¸æ–­æ¼”å˜çš„å¯¹æŠ—æ€§æ”»å‡»ï¼Œå°½ç®¡å·²æœ‰å¤šç§é˜²å¾¡ç­–ç•¥æå‡ºï¼Œæ—¨åœ¨åº”å¯¹å°½å¯èƒ½å¤šçš„å·²çŸ¥æ”»å‡»ï¼Œä½†è®¾è®¡ä¸€ç§èƒ½åº”å¯¹æ‰€æœ‰ç±»å‹æ”»å‡»çš„é˜²å¾¡æ–¹æ³•æ˜¯ä¸å¯è¡Œçš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå®é™…çš„å¨èƒæ¨¡å‹ä¸‹çš„æŒç»­é˜²å¾¡ç­–ç•¥ï¼Œå¹¶é¦–æ¬¡å¼•å…¥äº†æŒç»­å¯¹æŠ—æ€§é˜²å¾¡ï¼ˆCADï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶èƒ½å¤ŸæŒç»­åœ¨çº¿æ”¶é›†å¯¹æŠ—æ€§æ•°æ®ï¼Œå¹¶é€‚åº”ä¸æ–­å˜åŒ–çš„æ”»å‡»åºåˆ—ï¼ŒåŒæ—¶éµå¾ªå››ä¸ªå®ç”¨åŸåˆ™ï¼š1ï¼‰é€‚åº”æ–°æ”»å‡»è€Œä¸é—å¿˜æ—§çŸ¥è¯†ï¼›2ï¼‰å°æ ·æœ¬é€‚åº”ï¼›3ï¼‰å†…å­˜é«˜æ•ˆé€‚åº”ï¼›4ï¼‰åœ¨å¹²å‡€å’Œå¯¹æŠ—æ€§æ•°æ®ä¸Šå®ç°é«˜åˆ†ç±»å‡†ç¡®ç‡ã€‚æˆ‘ä»¬é€šè¿‡é›†æˆæœ€æ–°æŠ€æœ¯ï¼Œå¦‚æŒç»­å­¦ä¹ ã€å°æ ·æœ¬å­¦ä¹ å’Œé›†æˆå­¦ä¹ æ¥å®ç°è¿™äº›åŸåˆ™ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½æœ‰æ•ˆåº”å¯¹å¤šé˜¶æ®µå¯¹æŠ—æ€§æ”»å‡»ï¼Œå¹¶åœ¨å¤šç§åŸºçº¿æ–¹æ³•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚éšç€æ”»å‡»æ•°é‡çš„å¢åŠ ï¼ŒCADçš„é˜²å¾¡æ€§èƒ½è¶‹äºé¥±å’Œï¼Œè¿™è¡¨æ˜å…¶åœ¨é€‚åº”è¶³å¤Ÿå¤šæ ·åŒ–çš„æ”»å‡»åå…·æœ‰æŒä¹…é˜²å¾¡çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æŠ—æ”»å‡»çš„ä¸æ–­æ¼”å˜ä½¿å¾—è®¾è®¡ä¸€ç§èƒ½åº”å¯¹æ‰€æœ‰æ”»å‡»çš„é˜²å¾¡æ–¹æ³•å˜å¾—ä¸åˆ‡å®é™…ã€‚</li>
<li>å¼•å…¥æŒç»­å¯¹æŠ—æ€§é˜²å¾¡ï¼ˆCADï¼‰æ¡†æ¶æ¥åº”å¯¹åŠ¨æ€å˜åŒ–çš„æ”»å‡»ã€‚</li>
<li>CADæ¡†æ¶èƒ½æŒç»­åœ¨çº¿æ”¶é›†å¯¹æŠ—æ€§æ•°æ®å¹¶é€‚åº”æ”»å‡»åºåˆ—çš„å˜åŒ–ã€‚</li>
<li>CADéµå¾ªå››ä¸ªå®ç”¨åŸåˆ™ï¼šé€‚åº”æ–°æ”»å‡»ã€å°æ ·æœ¬é€‚åº”ã€å†…å­˜é«˜æ•ˆé€‚åº”å’Œé«˜åˆ†ç±»å‡†ç¡®ç‡ã€‚</li>
<li>é€šè¿‡é›†æˆæŒç»­å­¦ä¹ ã€å°æ ·æœ¬å­¦ä¹ å’Œé›†æˆå­¦ä¹ çš„æœ€æ–°æŠ€æœ¯æ¥å®ç°è¿™äº›åŸåˆ™ã€‚</li>
<li>å®éªŒè¯æ˜CADèƒ½æœ‰æ•ˆåº”å¯¹å¤šé˜¶æ®µå¯¹æŠ—æ€§æ”»å‡»å¹¶æ˜¾è‘—æ”¹è¿›åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.09481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eab87b569468bd376d0816b37301b3f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4c8e7c429901128b124e8134e93f831.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3bbadd8758d6130a2788743eedbe2531.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d50867f355f7fc85648a92968ee0c519.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  From Pixels to Pathology Restoration Diffusion for   Diagnostic-Consistent Virtual IHC
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9667208415f3a6b2b2fd2f0880e5b1ab.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  Hierarchical Learning-Based Control for Multi-Agent Shepherding of   Stochastic Autonomous Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
