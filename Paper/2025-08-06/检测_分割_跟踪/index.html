<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="检测/分割/跟踪">
    <meta name="description" content="检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-08-06  Enhancing Object Discovery for Unsupervised Instance Segmentation and   Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>检测/分割/跟踪 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6c014146585de1fbe87fd8be9413e80e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">检测/分割/跟踪</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">检测/分割/跟踪</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                检测/分割/跟踪
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    31 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-06-更新"><a href="#2025-08-06-更新" class="headerlink" title="2025-08-06 更新"></a>2025-08-06 更新</h1><h2 id="Enhancing-Object-Discovery-for-Unsupervised-Instance-Segmentation-and-Object-Detection"><a href="#Enhancing-Object-Discovery-for-Unsupervised-Instance-Segmentation-and-Object-Detection" class="headerlink" title="Enhancing Object Discovery for Unsupervised Instance Segmentation and   Object Detection"></a>Enhancing Object Discovery for Unsupervised Instance Segmentation and   Object Detection</h2><p><strong>Authors:Xingyu Feng, Hebei Gao, Hong Li</strong></p>
<p>We propose Cut-Once-and-LEaRn (COLER), a simple approach for unsupervised instance segmentation and object detection. COLER first uses our developed CutOnce to generate coarse pseudo labels, then enables the detector to learn from these masks. CutOnce applies Normalized Cut only once and does not rely on any clustering methods, but it can generate multiple object masks in an image. We have designed several novel yet simple modules that not only allow CutOnce to fully leverage the object discovery capabilities of self-supervised models, but also free it from reliance on mask post-processing. During training, COLER achieves strong performance without requiring specially designed loss functions for pseudo labels, and its performance is further improved through self-training. COLER is a zero-shot unsupervised model that outperforms previous state-of-the-art methods on multiple benchmarks.We believe our method can help advance the field of unsupervised object localization. </p>
<blockquote>
<p>我们提出了Cut-Once-and-LEaRn（COLER）方法，这是一种用于无监督实例分割和对象检测的简单方法。COLER首先使用我们开发的CutOnce生成粗略的伪标签，然后让检测器从这些掩膜中学习。CutOnce只应用一次归一化切割，不依赖任何聚类方法，但可以在图像中生成多个对象掩膜。我们设计了几种新颖而简单的模块，这些模块不仅允许CutOnce充分利用自监督模型的对象发现能力，而且还使其摆脱对掩膜后处理的依赖。在训练过程中，COLER在不针对伪标签设计特殊损失函数的情况下实现了强大的性能，并且其性能通过自训练得到了进一步提高。COLER是一种零样本无监督模型，在多个基准测试上超过了以前的最先进方法。我们相信我们的方法可以帮助推动无监督对象定位领域的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02386v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文提出了Cut-Once-and-LEaRn（COLER）方法，这是一种用于无监督实例分割和对象检测的简单方法。COLER首先使用开发的CutOnce生成粗略伪标签，然后使检测器从这些标签中学习。CutOnce只应用一次标准化切割，无需依赖任何聚类方法，但可以生成图像中的多个对象掩模。通过几个新的简单模块，CutOnce不仅能够充分利用自监督模型的对象发现能力，而且摆脱了伪标签的依赖。在训练过程中，COLER无需针对伪标签设计特殊设计的损失函数，并且可以通过自训练进一步提高性能。COLER是一个零样本无监督模型，在多个基准测试中超过了最新的技术方法。本文认为该方法有助于推动无监督对象定位领域的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>COLER是一种用于无监督实例分割和对象检测的简单方法。</li>
<li>CutOnce生成伪标签，用于训练检测器。</li>
<li>CutOnce通过应用一次标准化切割生成多个对象掩模，无需聚类方法。</li>
<li>通过新模块的应用，CutOnce能充分利用自监督模型的对象发现能力，并摆脱对伪标签的依赖。</li>
<li>COLER在训练过程中无需特殊设计的损失函数，可通过自训练进一步提高性能。</li>
<li>COLER是一个零样本无监督模型，性能优于现有的最新技术方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02386">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3f06bef4c49a0bb56d1d39decac8a97e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ed7b22051b465bab036c28466ea6c3c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5f4fb6faeef55a77621f9a09d5d31ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-69fa25226b76408575b66135099502ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-635e11c3ae63ecfcbba54b6ff802a3e8.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Semi-Supervised-Semantic-Segmentation-via-Derivative-Label-Propagation"><a href="#Semi-Supervised-Semantic-Segmentation-via-Derivative-Label-Propagation" class="headerlink" title="Semi-Supervised Semantic Segmentation via Derivative Label Propagation"></a>Semi-Supervised Semantic Segmentation via Derivative Label Propagation</h2><p><strong>Authors:Yuanbin Fu, Xiaojie Guo</strong></p>
<p>Semi-supervised semantic segmentation, which leverages a limited set of labeled images, helps to relieve the heavy annotation burden. While pseudo-labeling strategies yield promising results, there is still room for enhancing the reliability of pseudo-labels. Hence, we develop a semi-supervised framework, namely DerProp, equipped with a novel derivative label propagation to rectify imperfect pseudo-labels. Our label propagation method imposes discrete derivative operations on pixel-wise feature vectors as additional regularization, thereby generating strictly regularized similarity metrics. Doing so effectively alleviates the ill-posed problem that identical similarities correspond to different features, through constraining the solution space. Extensive experiments are conducted to verify the rationality of our design, and demonstrate our superiority over other methods. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/ForawardStar/DerProp/">https://github.com/ForawardStar/DerProp/</a>. </p>
<blockquote>
<p>半监督语义分割利用有限的标记图像，有助于减轻繁重的标注负担。虽然伪标签策略取得了令人鼓舞的结果，但提高伪标签的可靠性仍有空间。因此，我们开发了一个名为DerProp的半监督框架，配备了一种新型衍生标签传播方法，以修正不完美的伪标签。我们的标签传播方法将离散导数运算施加于像素级特征向量作为额外的正则化，从而产生严格正则化的相似度度量。这样做可以有效地缓解不适定问题，即相同的相似度对应于不同的特征，通过约束解空间来实现这一点。我们进行了大量实验来验证设计的合理性，并证明我们的方法在性能上优于其他方法。代码可通过<a target="_blank" rel="noopener" href="https://github.com/ForawardStar/DerProp/%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ForawardStar/DerProp/获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02254v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于有限标记图像集的新型半监督语义分割框架DerProp。该方法引入了一种新型标签传播策略来纠正不完美的伪标签，对像素级特征向量进行离散导数操作，生成严格的规则化相似性度量，有效解决相似特征对应的伪标签错误问题，并在实验上证明了其设计的合理性和超越其他方法的优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种基于有限标记图像集的新型半监督语义分割框架DerProp。</li>
<li>采用新型标签传播策略来纠正伪标签。</li>
<li>对像素级特征向量进行离散导数操作，生成规则化的相似性度量。</li>
<li>解决相似特征对应的伪标签错误问题。</li>
<li>实验验证了设计的合理性以及DerProp相对于其他方法的优越性。</li>
<li>代码已经开源并可在网上获取。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02254">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-362fdbe9b3c7230d282b64bd9f25978e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51d4e090bec061e8e10e97d9fc9ee490.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c03b5a34caabc95e7c189eee6d5c4482.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f089ad40274a2410ca39dda72eabe5f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Unified-Category-Level-Object-Detection-and-Pose-Estimation-from-RGB-Images-using-3D-Prototypes"><a href="#Unified-Category-Level-Object-Detection-and-Pose-Estimation-from-RGB-Images-using-3D-Prototypes" class="headerlink" title="Unified Category-Level Object Detection and Pose Estimation from RGB   Images using 3D Prototypes"></a>Unified Category-Level Object Detection and Pose Estimation from RGB   Images using 3D Prototypes</h2><p><strong>Authors:Tom Fischer, Xiaojie Zhang, Eddy Ilg</strong></p>
<p>Recognizing objects in images is a fundamental problem in computer vision. Although detecting objects in 2D images is common, many applications require determining their pose in 3D space. Traditional category-level methods rely on RGB-D inputs, which may not always be available, or employ two-stage approaches that use separate models and representations for detection and pose estimation. For the first time, we introduce a unified model that integrates detection and pose estimation into a single framework for RGB images by leveraging neural mesh models with learned features and multi-model RANSAC. Our approach achieves state-of-the-art results for RGB category-level pose estimation on REAL275, improving on the current state-of-the-art by 22.9% averaged across all scale-agnostic metrics. Finally, we demonstrate that our unified method exhibits greater robustness compared to single-stage baselines. Our code and models are available at <a target="_blank" rel="noopener" href="https://github.com/Fischer-Tom/unified-detection-and-pose-estimation">https://github.com/Fischer-Tom/unified-detection-and-pose-estimation</a>. </p>
<blockquote>
<p>识别图像中的物体是计算机视觉中的一个基本问题。虽然检测二维图像中的物体很常见，但许多应用需要确定其在三维空间中的姿态。传统的类别级别的方法依赖于RGB-D输入，这可能并不总是可用，或者采用两阶段的方法，使用独立的模型和表示为检测和姿态估计建模。首次，我们引入了一个统一模型，该模型利用神经网格模型学习的特征和基于多模型的RANSAC将检测和姿态估计集成到一个单一的RGB图像框架中。我们的方法在REAL275的RGB类别级别的姿态估计上取得了最新的结果，在所有尺度无关指标上的平均表现较当前最佳水平提高了22.9%。最后，我们证明了我们的统一方法比单阶段基线展现出更大的稳健性。我们的代码和模型在<a target="_blank" rel="noopener" href="https://github.com/Fischer-Tom/unified-detection-and-pose-estimation%E4%B8%8A%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/Fischer-Tom/unified-detection-and-pose-estimation上可用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02157v1">PDF</a> Published at ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新的统一模型，该模型利用神经网络模型和学习的特征以及多模态RANSAC，将物体检测与姿态估计整合到一个框架中，仅使用RGB图像即可实现。该方法在RGB类别级别的姿态估计方面达到了最先进的成果，并表现出较高的稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文解决了计算机视觉中的物体识别问题，并重点关注在图像中的物体检测及其在三维空间中的姿态确定。</li>
<li>传统的方法通常依赖于RGB-D输入或两阶段的方法，需要使用独立的模型和表示进行检测和姿态估计。</li>
<li>本文首次引入了一种统一模型，该模型将检测和姿态估计整合到一个框架中，仅使用RGB图像。</li>
<li>该方法利用神经网络模型和学习的特征，以及多模态RANSAC来实现。</li>
<li>该方法在RGB类别级别的姿态估计方面达到了最先进的成果，平均改进了当前最先进技术的22.9%。</li>
<li>与单阶段基线相比，该方法表现出更高的稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02157">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-da2bfd6067af96f3329bb82cc0e384d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2e63c652391abe5a02ce1f5f361e11f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-caca239ff317f5769b8d2bb9db054288.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f07e49deb17a67d0b0b375ab1f6cac13.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Beyond-RGB-and-Events-Enhancing-Object-Detection-under-Adverse-Lighting-with-Monocular-Normal-Maps"><a href="#Beyond-RGB-and-Events-Enhancing-Object-Detection-under-Adverse-Lighting-with-Monocular-Normal-Maps" class="headerlink" title="Beyond RGB and Events: Enhancing Object Detection under Adverse Lighting   with Monocular Normal Maps"></a>Beyond RGB and Events: Enhancing Object Detection under Adverse Lighting   with Monocular Normal Maps</h2><p><strong>Authors:Mingjie Liu, Hanqing Liu, Chuang Zhu</strong></p>
<p>Accurate object detection under adverse lighting conditions is critical for real-world applications such as autonomous driving. Although neuromorphic event cameras have been introduced to handle these scenarios, adverse lighting often induces distracting reflections from tunnel walls or road surfaces, which frequently lead to false obstacle detections. However, neither RGB nor event data alone is robust enough to address these complexities, and mitigating these issues without additional sensors remains underexplored. To overcome these challenges, we propose leveraging normal maps, directly predicted from monocular RGB images, as robust geometric cues to suppress false positives and enhance detection accuracy. We introduce NRE-Net, a novel multi-modal detection framework that effectively fuses three complementary modalities: monocularly predicted surface normal maps, RGB images, and event streams. To optimize the fusion process, our framework incorporates two key modules: the Adaptive Dual-stream Fusion Module (ADFM), which integrates RGB and normal map features, and the Event-modality Aware Fusion Module (EAFM), which adapts to the high dynamic range characteristics of event data. Extensive evaluations on the DSEC-Det-sub and PKU-DAVIS-SOD datasets demonstrate that NRE-Net significantly outperforms state-of-the-art methods. Our approach achieves mAP50 improvements of 7.9% and 6.1% over frame-based approaches (e.g., YOLOX), while surpassing the fusion-based SFNet by 2.7% on the DSEC-Det-sub dataset and SODFormer by 7.1% on the PKU-DAVIS-SOD dataset. </p>
<blockquote>
<p>在恶劣照明条件下准确的目标检测对于自动驾驶等实际应用至关重要。尽管神经形态事件相机已经被引入来处理这些场景，但恶劣的照明条件往往会引起隧道墙壁或路面表面的反射干扰，这经常导致错误的障碍物检测。然而，无论是RGB还是单独的事件数据，都无法足够稳健地应对这些复杂性，而且不依赖额外的传感器来缓解这些问题仍然缺乏足够的探索。为了克服这些挑战，我们提出利用从单目RGB图像直接预测得到的法线图作为稳健的几何线索来抑制误报并增强检测准确性。我们引入了NRE-Net，这是一种新型的多模态检测框架，能够有效地融合三种互补模态：从单目中预测的法线图、RGB图像和事件流。为了优化融合过程，我们的框架采用了两个关键模块：自适应双流融合模块（ADFM），它融合了RGB和法图特征；事件模态感知融合模块（EAFM），它适应了事件数据的高动态范围特性。在DSEC-Det-sub和PKU-DAVIS-SOD数据集上的广泛评估表明，NRE-Net显著优于最先进的方法。我们的方法在基于帧的方法（例如YOLOX）的基础上提高了mAP50的准确度，达到7.9%和6.1%，而在基于融合的方法中，相较于SFNet在DSEC-Det-sub数据集上提高了2.7%，相较于SODFormer在PKU-DAVIS-SOD数据集上提高了7.1%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02127v1">PDF</a> </p>
<p><strong>摘要</strong><br>在恶劣照明条件下准确的目标检测对于自动驾驶等实际应用至关重要。尽管神经形态事件相机已被引入处理这些情况，但恶劣的照明条件经常在隧道墙壁或路面表面引起干扰反射，这经常导致错误的障碍检测。然而，无论是RGB还是事件数据本身都不足以应对这些复杂性，而且不依赖额外传感器来减轻这些问题仍然是一个尚未充分研究的问题。为了克服这些挑战，本文提出了利用从单目RGB图像直接预测得到的法线图作为可靠的几何线索来抑制误报并提高检测准确性。本文介绍了NRE-Net，这是一种新型的多模态检测框架，它有效地融合了三种互补模态：从单目图像预测得到的表面法线图、RGB图像和事件流。为了优化融合过程，我们的框架采用了两个关键模块：自适应双流融合模块（ADFM），它融合了RGB和法线图特征；事件模态感知融合模块（EAFM），它适应了事件数据的高动态范围特性。在DSEC-Det-sub和PKU-DAVIS-SOD数据集上的广泛评估表明，NRE-Net显著优于最先进的方法。我们的方法在DSEC-Det-sub数据集上较基于帧的方法（例如YOLOX）提高了mAP50得分7.9％和6.1％，并在PKU-DAVIS-SOD数据集上较融合方法的SODFormer高出7.1％。</p>
<p><strong>关键见解</strong></p>
<ul>
<li>在恶劣照明条件下，目标检测面临干扰反射问题，导致误报。</li>
<li>仅依赖RGB或事件数据不足以解决这些复杂性。</li>
<li>提出利用从单目RGB图像直接预测得到的法线图作为几何线索来提高检测准确性。</li>
<li>介绍NRE-Net多模态检测框架，融合了RGB图像、事件流和法线图三种模态。</li>
<li>框架包含两个关键模块：ADFM和EAFM，分别用于特征融合和适应事件数据的高动态范围特性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02127">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5c18e59acb0c65b2b42f0ace7e34efd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00cd4bee9f04fcd035d3e3f0f180aae3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4f172d00655c3ce761b85432f912f7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c014146585de1fbe87fd8be9413e80e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Self-Supervised-YOLO-Leveraging-Contrastive-Learning-for-Label-Efficient-Object-Detection"><a href="#Self-Supervised-YOLO-Leveraging-Contrastive-Learning-for-Label-Efficient-Object-Detection" class="headerlink" title="Self-Supervised YOLO: Leveraging Contrastive Learning for   Label-Efficient Object Detection"></a>Self-Supervised YOLO: Leveraging Contrastive Learning for   Label-Efficient Object Detection</h2><p><strong>Authors:Manikanta Kotthapalli, Reshma Bhatia, Nainsi Jain</strong></p>
<p>One-stage object detectors such as the YOLO family achieve state-of-the-art performance in real-time vision applications but remain heavily reliant on large-scale labeled datasets for training. In this work, we present a systematic study of contrastive self-supervised learning (SSL) as a means to reduce this dependency by pretraining YOLOv5 and YOLOv8 backbones on unlabeled images using the SimCLR framework. Our approach introduces a simple yet effective pipeline that adapts YOLO’s convolutional backbones as encoders, employs global pooling and projection heads, and optimizes a contrastive loss using augmentations of the COCO unlabeled dataset (120k images). The pretrained backbones are then fine-tuned on a cyclist detection task with limited labeled data. Experimental results show that SSL pretraining leads to consistently higher mAP, faster convergence, and improved precision-recall performance, especially in low-label regimes. For example, our SimCLR-pretrained YOLOv8 achieves a mAP@50:95 of 0.7663, outperforming its supervised counterpart despite using no annotations during pretraining. These findings establish a strong baseline for applying contrastive SSL to one-stage detectors and highlight the potential of unlabeled data as a scalable resource for label-efficient object detection. </p>
<blockquote>
<p>单阶段目标检测器，如YOLO系列，在实时视觉应用中实现了最先进的性能，但仍严重依赖于大规模标注数据集进行训练。在这项工作中，我们对对比自监督学习（SSL）进行了系统研究，以减少对标注数据的依赖。我们通过使用SimCLR框架，对YOLOv5和YOLOv8的主干网络进行预训练，利用无标签图像来实现这一目标。我们的方法引入了一个简单有效的流程，将YOLO的卷积主干网络作为编码器，采用全局池化和投影头，并使用COCO无标签数据集（12万张图像）的增强版来优化对比损失。然后，在有限的标注数据上，对预训练的主干网络进行微调，以执行骑行者检测任务。实验结果表明，SSL预训练能持续提高mAP、加快收敛速度并改善精度-召回率性能，特别是在标注数据较少的情况下。例如，我们采用SimCLR预训练的YOLOv8在mAP@50:95上达到了0.7663，尽管在预训练过程中没有使用任何注释，仍优于其有监督的同类模型。这些发现为将对比SSL应用于单阶段检测器建立了强大的基线，并突出了无标签数据作为标签高效目标检测的可扩展资源的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01966v1">PDF</a> 11 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>基于YOLO系列的一阶段目标检测器在实时视觉应用中取得了最先进的性能，但它们在训练过程中严重依赖于大规模标签数据集。本文系统性地研究了对比型自监督学习（SSL）技术，以减小对这种依赖程度。通过在未标记的图像上采用SimCLR框架预训练YOLOv5和YOLOv8的backbone部分，研究引入了简单有效的管道线。此方法对YOLO的卷积主干进行适应性调整，将其作为编码器，并利用全局池化和投影头来优化对比损失，并利用COCO无标签数据集的扩充进行优化训练。之后使用有限的标记数据对预训练的backbone进行微调，用于骑行者检测任务。实验结果表明，SSL预训练持续提高了mAP值、加快了收敛速度并改善了精度召回性能，特别是在标签稀缺的情况下。例如，使用SimCLR预训练的YOLOv8在mAP@50:95指标上达到了0.7663，尽管在预训练阶段未使用任何注释，但超越了其有监督训练下的性能水平。这一发现对于将对比型SSL应用在一阶段检测器上具有重要的里程碑意义，并突出了未标记数据作为标签高效目标检测的潜在可扩展资源。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>一阶段目标检测器如YOLO系列在实时视觉应用中表现优异，但对大规模标签数据集依赖性强。</li>
<li>对比型自监督学习（SSL）用于减少这种依赖。</li>
<li>采用SimCLR框架进行预训练，使用未标记图像对YOLOv5和YOLOv8的backbone进行训练。</li>
<li>预训练模型在骑行者检测任务上通过有限的标记数据进行微调。</li>
<li>SSL预训练提高了mAP值、加快了收敛速度并改善了精度召回性能。</li>
<li>在低标签数据情况下，SSL预训练尤为重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01966">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8eb0be628f43ece6eae4efb8b100f75d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf3cf53c8fba3233712024eac7a93bc0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73bf5ced1bae2b59aeefb91d948fd39f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Referring-Remote-Sensing-Image-Segmentation-with-Cross-view-Semantics-Interaction-Network"><a href="#Referring-Remote-Sensing-Image-Segmentation-with-Cross-view-Semantics-Interaction-Network" class="headerlink" title="Referring Remote Sensing Image Segmentation with Cross-view Semantics   Interaction Network"></a>Referring Remote Sensing Image Segmentation with Cross-view Semantics   Interaction Network</h2><p><strong>Authors:Jiaxing Yang, Lihe Zhang, Huchuan Lu</strong></p>
<p>Recently, Referring Remote Sensing Image Segmentation (RRSIS) has aroused wide attention. To handle drastic scale variation of remote targets, existing methods only use the full image as input and nest the saliency-preferring techniques of cross-scale information interaction into traditional single-view structure. Although effective for visually salient targets, they still struggle in handling tiny, ambiguous ones in lots of real scenarios. In this work, we instead propose a paralleled yet unified segmentation framework Cross-view Semantics Interaction Network (CSINet) to solve the limitations. Motivated by human behavior in observing targets of interest, the network orchestrates visual cues from remote and close distances to conduct synergistic prediction. In its every encoding stage, a Cross-View Window-attention module (CVWin) is utilized to supplement global and local semantics into close-view and remote-view branch features, finally promoting the unified representation of feature in every encoding stage. In addition, we develop a Collaboratively Dilated Attention enhanced Decoder (CDAD) to mine the orientation property of target and meanwhile integrate cross-view multiscale features. The proposed network seamlessly enhances the exploitation of global and local semantics, achieving significant improvements over others while maintaining satisfactory speed. </p>
<blockquote>
<p>最近，遥感图像分割（RRSIS）已引起了广泛关注。在处理遥感目标的尺度剧变时，现有方法仅使用全图作为输入，并将跨尺度信息交互的显著性优先技术嵌入到传统的单视图结构中。尽管对于视觉上显著的目标这些方法很有效，但在处理现实中大量的小型、模糊的目标时，它们仍然存在问题。在这项工作中，我们提出了一种并行但统一的分割框架——跨视图语义交互网络（CSINet），以解决这些限制。受人类观察感兴趣目标的行为的启发，该网络协调来自远近距离的视觉线索来进行协同预测。在其每个编码阶段，使用跨视图窗口注意力模块（CVWin）将全局和局部语义补充到近视图和远视图分支特征中，最终促进每个编码阶段特征的统一表示。此外，我们开发了一种协同扩张注意力增强解码器（CDAD），以挖掘目标的方向属性，同时集成跨视图的多尺度特征。所提出的网络无缝地增强了全局和局部语义的利用，在保持令人满意的运行速度的同时，实现了对他人成果的显著改进。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01331v1">PDF</a> </p>
<p><strong>Summary</strong><br>远程遥感图像分割（RRSIS）领域中的现有方法在处理尺度变化剧烈的遥感目标时存在局限性。为此，本文提出了一种并行统一的分割框架——跨视图语义交互网络（CSINet）。该网络模仿人类观察目标的行为，协调远程和近距离的视觉线索进行协同预测。在每个编码阶段，使用跨视图窗口注意力模块（CVWin）将全局和局部语义信息补充到近距离和远距离分支特征中，促进了每个编码阶段特征的统一表示。此外，还开发了一种协同扩张注意力增强解码器（CDAD），以挖掘目标的定向属性并同时整合跨视图的多尺度特征。所提出的网络无缝地提高了全局和局部语义的利用，在保持令人满意的速度的同时，实现了对其他人显著的提升。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>RRSIS领域中现有方法在处理尺度变化剧烈的遥感目标时存在挑战。</li>
<li>提出了一种新的并行统一的分割框架CSINet来解决这一问题。</li>
<li>CSINet模仿人类观察目标的行为，融合远程和近距离的视觉线索进行协同预测。</li>
<li>跨视图窗口注意力模块（CVWin）在每个编码阶段融合全局和局部语义信息。</li>
<li>开发了一种协同扩张注意力增强解码器（CDAD）以挖掘目标的定向属性并整合多尺度特征。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01331">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-31ddd378e0580946a1c7f0a8ae5a482b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54c2f43bc467354492179196034ec705.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8866187f47e0807a8496d1ce1241242.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ODOV-Towards-Open-Domain-Open-Vocabulary-Object-Detection"><a href="#ODOV-Towards-Open-Domain-Open-Vocabulary-Object-Detection" class="headerlink" title="ODOV: Towards Open-Domain Open-Vocabulary Object Detection"></a>ODOV: Towards Open-Domain Open-Vocabulary Object Detection</h2><p><strong>Authors:Yupeng Zhang, Ruize Han, Fangnan Zhou, Song Wang, Wei Feng, Liang Wan</strong></p>
<p>In this work, we handle a new problem of Open-Domain Open-Vocabulary (ODOV) object detection, which considers the detection model’s adaptability to the real world including both domain and category shifts. For this problem, we first construct a new benchmark OD-LVIS, which includes 46,949 images, covers 18 complex real-world domains and 1,203 categories, and provides a comprehensive dataset for evaluating real-world object detection. Besides, we develop a novel baseline method for ODOV detection.The proposed method first leverages large language models to generate the domain-agnostic text prompts for category embedding. It further learns the domain embedding from the given image, which, during testing, can be integrated into the category embedding to form the customized domain-specific category embedding for each test image. We provide sufficient benchmark evaluations for the proposed ODOV detection task and report the results, which verify the rationale of ODOV detection, the usefulness of our benchmark, and the superiority of the proposed method. </p>
<blockquote>
<p>在这项工作中，我们解决了一个全新的开放域开放词汇表（ODOV）目标检测问题。这个问题考虑了检测模型对现实世界适应性的问题，包括领域和类别迁移。针对此问题，我们首先构建了一个新的基准测试OD-LVIS，其中包括46949张图像，涵盖18个复杂的现实世界领域和1203个类别，为评估现实世界目标检测提供了综合数据集。此外，我们还为ODOV检测开发了一种新的基线方法。该方法首先利用大型语言模型生成与领域无关的文字提示来进行类别嵌入。它进一步从给定的图像中学习领域嵌入，在测试期间，可以将其集成到类别嵌入中，以形成针对每个测试图像的自定义领域特定类别嵌入。我们对提出的ODOV检测任务进行了充分的基准评估并报告了结果，验证了ODOV检测的原理、我们基准测试的实用性和所提出方法的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01253v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文章探讨了开放域开放词汇（ODOV）物体检测的新问题，关注检测模型在现实世界中的适应性，包括领域和类别迁移。为此，构建了新的基准数据集OD-LVIS，包含46,949张图像、涵盖18个复杂现实世界领域和1,203个类别，为评估现实世界物体检测提供了综合数据集。此外，开发了一种针对ODOV检测的基线方法，该方法利用大型语言模型生成与领域无关的文字提示来进行类别嵌入，并从给定图像中学习领域嵌入。在测试阶段，可以将领域嵌入集成到类别嵌入中，形成针对每个测试图像的自定义领域特定类别嵌入。文章提供了对提出的ODOV检测任务的充足基准评估，并报告了结果，验证了ODOV检测的原理、基准数据集的有用性以及所提出方法的优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章解决了开放域开放词汇（ODOV）物体检测的新问题，关注模型在现实世界的适应性。</li>
<li>构建了新的基准数据集OD-LVIS，包含多种领域的图像和丰富的类别，为评估现实世界物体检测提供了综合数据集。</li>
<li>提出了一种针对ODOV检测的基线方法，利用大型语言模型生成文字提示进行类别嵌入。</li>
<li>方法通过结合领域嵌入和类别嵌入，形成针对每个测试图像的自定义领域特定类别嵌入。</li>
<li>文章进行了充分的实验评估，验证了ODOV检测任务的重要性、基准数据集的有用性以及所提出方法的有效性。</li>
<li>该方法对于处理现实世界中复杂领域的物体检测问题具有潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01253">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f2987aa6026c28caa7c668789434b826.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cac428d7fec5530177de3ced2567252f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35088e73a5b0138b81cf3b6cc9c2588b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de9f0a60722392bc732dbfd2edacaeec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0287a626534b7741ae7f8e334b9e5ed.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Taming-SAM-for-Underwater-Instance-Segmentation-and-Beyond"><a href="#Taming-SAM-for-Underwater-Instance-Segmentation-and-Beyond" class="headerlink" title="Taming SAM for Underwater Instance Segmentation and Beyond"></a>Taming SAM for Underwater Instance Segmentation and Beyond</h2><p><strong>Authors:Hua Li, Shijie Lian, Zhiyuan Li, Runmin Cong, Chongyi Li</strong></p>
<p>With recent breakthroughs in large-scale modeling, the Segment Anything Model (SAM) has demonstrated significant potential in a variety of visual applications. However, due to the lack of underwater domain expertise, SAM and its variants face performance limitations in end-to-end underwater instance segmentation tasks, while their higher computational requirements further hinder their application in underwater scenarios. To address this challenge, we propose a large-scale underwater instance segmentation dataset, UIIS10K, which includes 10,048 images with pixel-level annotations for 10 categories. Then, we introduce UWSAM, an efficient model designed for automatic and accurate segmentation of underwater instances. UWSAM efficiently distills knowledge from the SAM ViT-Huge image encoder into the smaller ViT-Small image encoder via the Mask GAT-based Underwater Knowledge Distillation (MG-UKD) method for effective visual representation learning. Furthermore, we design an End-to-end Underwater Prompt Generator (EUPG) for UWSAM, which automatically generates underwater prompts instead of explicitly providing foreground points or boxes as prompts, thus enabling the network to locate underwater instances accurately for efficient segmentation. Comprehensive experimental results show that our model is effective, achieving significant performance improvements over state-of-the-art methods on multiple underwater instance datasets. Datasets and codes are available at <a target="_blank" rel="noopener" href="https://github.com/LiamLian0727/UIIS10K">https://github.com/LiamLian0727/UIIS10K</a>. </p>
<blockquote>
<p>随着大规模建模的最新突破，Segment Anything Model（SAM）在各种视觉应用中表现出了巨大的潜力。然而，由于缺乏水下领域的专业知识，SAM及其变体在端到端的水下实例分割任务中面临性能局限，而它们较高的计算要求进一步阻碍了在水下场景中的应用。为了应对这一挑战，我们提出了大规模水下实例分割数据集UIIS10K，其中包括10,048张具有像素级注释的10类图像。接着，我们介绍了专为自动和精确水下实例分割而设计的高效模型UWSAM。UWSAM通过基于Mask GAT的水下知识蒸馏（MG-UKD）方法，有效地从SAM的ViT-Huge图像编码器提炼知识，并应用到较小的ViT-Small图像编码器上，以实现有效的视觉表示学习。此外，我们为UWSAM设计了端到端的水下提示生成器（EUPG），它会自动生成水下提示，而不是显式提供前景点或框作为提示，从而使网络能够准确定位水下实例，实现高效分割。综合实验结果表明，我们的模型是有效的，在多个水下实例数据集上实现了对最先进方法的重要性能改进。数据集和代码可在[<a target="_blank" rel="noopener" href="https://github.com/LiamLian0">https://github.com/LiamLian0</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15581v2">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>随着大规模建模技术的突破，Segment Anything Model（SAM）在多种视觉应用中展现出巨大潜力。然而，由于缺乏水下领域的专业知识，SAM及其变体在端到端的水下实例分割任务中的性能受到限制。为此，我们提出了大规模水下实例分割数据集UIIS10K，包含10,048张带有像素级注释的10类图像。同时，我们引入了面向水下实例自动准确分割的UWSAM模型。它通过Mask GAT基础水下知识蒸馏（MG-UKD）方法，有效地从SAM的ViT-Huge图像编码器中提炼知识，用于小型ViT-Small图像编码器的有效视觉表示学习。此外，我们为UWSAM设计了端到端水下提示生成器（EUPG），能够自动生成水下提示，使网络能够准确定位水下实例，实现高效分割。实验结果表明，我们的模型在多个水下实例数据集上实现了显著的性能提升。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>Segment Anything Model（SAM）在视觉应用中有巨大潜力，但在水下实例分割方面存在性能限制。</li>
<li>缺乏水下领域的专业知识是SAM及其变体在水下应用中的一大挑战。</li>
<li>提出了大规模水下实例分割数据集UIIS10K，包含10,048张带有像素级注释的图像。</li>
<li>引入了面向水下实例分割的UWSAM模型，具有高效的知识提炼和视觉表示学习能力。</li>
<li>UWSAM通过Mask GAT基础水下知识蒸馏（MG-UKD）方法提升性能。</li>
<li>介绍了端到端水下提示生成器（EUPG），能够自动生成水下提示，提高网络定位水下实例的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15581">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6ecb05a47af4b85db1bb146fb2f8a00a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cefa54b5cff8ecd0b6e01a97f424d5df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-523ebe3b707ca0a28da3138c83e7ba28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01897eabea2660f7b94d2515def72534.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a8503ef79ba6f37bebdd25898b8e026.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">检测/分割/跟踪</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8560fdbacc0a01f8ec2b9d23c9246927.jpg" class="responsive-img" alt="人脸相关">
                        
                        <span class="card-title">人脸相关</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            人脸相关 方向最新论文已更新，请持续关注 Update in 2025-08-06  DiffusionFF Face Forgery Detection via Diffusion-based Artifact   Localization
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    人脸相关
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">人脸相关</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-fb4281b718235c06a29087ee9685816d.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-08-06  Glioblastoma Overall Survival Prediction With Vision Transformers
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30666.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
