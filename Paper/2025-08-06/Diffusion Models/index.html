<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  From Pixels to Pathology Restoration Diffusion for   Diagnostic-Consistent Virtual IHC">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-3efef1a659ef791352fd6decd7a2b92a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-06-æ›´æ–°"><a href="#2025-08-06-æ›´æ–°" class="headerlink" title="2025-08-06 æ›´æ–°"></a>2025-08-06 æ›´æ–°</h1><h2 id="From-Pixels-to-Pathology-Restoration-Diffusion-for-Diagnostic-Consistent-Virtual-IHC"><a href="#From-Pixels-to-Pathology-Restoration-Diffusion-for-Diagnostic-Consistent-Virtual-IHC" class="headerlink" title="From Pixels to Pathology: Restoration Diffusion for   Diagnostic-Consistent Virtual IHC"></a>From Pixels to Pathology: Restoration Diffusion for   Diagnostic-Consistent Virtual IHC</h2><p><strong>Authors:Jingsong Liu, Xiaofeng Deng, Han Li, Azar Kazemi, Christian Grashei, Gesa Wilkens, Xin You, Tanja Groll, Nassir Navab, Carolin Mogler, Peter J. SchÃ¼ffler</strong></p>
<p>Hematoxylin and eosin (H&amp;E) staining is the clinical standard for assessing tissue morphology, but it lacks molecular-level diagnostic information. In contrast, immunohistochemistry (IHC) provides crucial insights into biomarker expression, such as HER2 status for breast cancer grading, but remains costly and time-consuming, limiting its use in time-sensitive clinical workflows. To address this gap, virtual staining from H&amp;E to IHC has emerged as a promising alternative, yet faces two core challenges: (1) Lack of fair evaluation of synthetic images against misaligned IHC ground truths, and (2) preserving structural integrity and biological variability during translation. To this end, we present an end-to-end framework encompassing both generation and evaluation in this work. We introduce Star-Diff, a structure-aware staining restoration diffusion model that reformulates virtual staining as an image restoration task. By combining residual and noise-based generation pathways, Star-Diff maintains tissue structure while modeling realistic biomarker variability. To evaluate the diagnostic consistency of the generated IHC patches, we propose the Semantic Fidelity Score (SFS), a clinical-grading-task-driven metric that quantifies class-wise semantic degradation based on biomarker classification accuracy. Unlike pixel-level metrics such as SSIM and PSNR, SFS remains robust under spatial misalignment and classifier uncertainty. Experiments on the BCI dataset demonstrate that Star-Diff achieves state-of-the-art (SOTA) performance in both visual fidelity and diagnostic relevance. With rapid inference and strong clinical alignment,it presents a practical solution for applications such as intraoperative virtual IHC synthesis. </p>
<blockquote>
<p>è‹æœ¨ç²¾å’Œä¼Šçº¢ï¼ˆH&amp;Eï¼‰æŸ“è‰²æ˜¯è¯„ä¼°ç»„ç»‡å½¢æ€çš„ä¸´åºŠæ ‡å‡†ï¼Œä½†å®ƒç¼ºä¹åˆ†å­æ°´å¹³çš„è¯Šæ–­ä¿¡æ¯ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå…ç–«ç»„ç»‡åŒ–å­¦ï¼ˆIHCï¼‰ä¸ºç”Ÿç‰©æ ‡å¿—ç‰©è¡¨è¾¾æä¾›äº†å…³é”®è§è§£ï¼Œå¦‚ä¹³è…ºç™Œåˆ†çº§ä¸­çš„HER2çŠ¶æ€ï¼Œä½†å…¶æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ï¼Œé™åˆ¶äº†å…¶åœ¨æ—¶é—´æ•æ„Ÿçš„ä¸´åºŠå·¥ä½œæµç¨‹ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œä»H&amp;Eåˆ°IHCçš„è™šæ‹ŸæŸ“è‰²å·²æˆä¸ºä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†é¢ä¸´ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰åˆæˆå›¾åƒä¸é”™é…çš„IHCçœŸå®å€¼ä¹‹é—´çš„å…¬å¹³è¯„ä¼°ç¼ºå¤±ï¼›ï¼ˆ2ï¼‰åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­ä¿æŒç»“æ„å®Œæ•´æ€§å’Œç”Ÿç‰©å˜å¼‚æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨å·¥ä½œä¸­æå‡ºäº†ä¸€ä¸ªæ¶µç›–ç”Ÿæˆå’Œè¯„ä¼°çš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚æˆ‘ä»¬ä»‹ç»äº†Star-Diffï¼Œè¿™æ˜¯ä¸€ä¸ªç»“æ„æ„ŸçŸ¥çš„æŸ“è‰²æ¢å¤æ‰©æ•£æ¨¡å‹ï¼Œå®ƒå°†è™šæ‹ŸæŸ“è‰²é‡æ–°å®šä¹‰ä¸ºå›¾åƒæ¢å¤ä»»åŠ¡ã€‚é€šè¿‡ç»“åˆæ®‹å·®å’ŒåŸºäºå™ªå£°çš„ç”Ÿæˆè·¯å¾„ï¼ŒStar-Diffåœ¨æ¨¡æ‹Ÿç°å®ç”Ÿç‰©æ ‡å¿—ç‰©å˜å¼‚æ€§çš„åŒæ—¶ä¿æŒäº†ç»„ç»‡ç»“æ„ã€‚ä¸ºäº†è¯„ä¼°ç”ŸæˆIHCè¡¥ä¸çš„è¯Šæ–­ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­ä¹‰ä¿çœŸåº¦è¯„åˆ†ï¼ˆSFSï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºä¸´åºŠåˆ†çº§ä»»åŠ¡çš„æŒ‡æ ‡ï¼Œå®ƒæ ¹æ®ç”Ÿç‰©æ ‡å¿—ç‰©åˆ†ç±»å‡†ç¡®æ€§å¯¹ç±»çº§è¯­ä¹‰é™è§£è¿›è¡Œé‡åŒ–ã€‚ä¸SSIMå’ŒPSNRç­‰åƒç´ çº§æŒ‡æ ‡ä¸åŒï¼ŒSFSåœ¨ç©ºé—´é”™ä½å’Œåˆ†ç±»å™¨ä¸ç¡®å®šæ€§ä¸‹ä¿æŒç¨³å¥ã€‚åœ¨BCIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒStar-Diffåœ¨è§†è§‰ä¿çœŸåº¦å’Œè¯Šæ–­ç›¸å…³æ€§æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚å…·æœ‰å¿«é€Ÿæ¨ç†å’Œå¼ºå¤§çš„ä¸´åºŠå¯¹é½èƒ½åŠ›ï¼Œå®ƒä¸ºæœ¯ä¸­è™šæ‹ŸIHCåˆæˆç­‰åº”ç”¨æä¾›äº†å®é™…è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02528v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>H&amp;EæŸ“è‰²æ˜¯è¯„ä¼°ç»„ç»‡å½¢æ€çš„ä¸´åºŠæ ‡å‡†ï¼Œä½†ç¼ºä¹åˆ†å­æ°´å¹³çš„è¯Šæ–­ä¿¡æ¯ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå…ç–«ç»„ç»‡åŒ–å­¦ï¼ˆIHCï¼‰ä¸ºç”Ÿç‰©æ ‡å¿—ç‰©è¡¨è¾¾æä¾›å…³é”®è§è§£ï¼Œå¦‚ä¹³è…ºç™Œåˆ†çº§ä¸­çš„HER2çŠ¶æ€ï¼Œä½†æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ï¼Œé™åˆ¶äº†å…¶åœ¨æ—¶é—´æ•æ„Ÿçš„ä¸´åºŠå·¥ä½œæµç¨‹ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œä»H&amp;Eåˆ°IHCçš„è™šæ‹ŸæŸ“è‰²å·²æˆä¸ºä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†é¢ä¸´ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰åˆæˆå›¾åƒä¸é”™ä½çš„IHCçœŸå®å€¼ä¹‹é—´çš„å…¬å¹³è¯„ä¼°ä¸è¶³ï¼›ï¼ˆ2ï¼‰åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­ä¿æŒç»“æ„å®Œæ•´æ€§å’Œç”Ÿç‰©å˜å¼‚æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨å·¥ä½œä¸­æå‡ºäº†æ¶µç›–ç”Ÿæˆå’Œè¯„ä¼°çš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚æˆ‘ä»¬ä»‹ç»äº†Star-Diffï¼Œè¿™æ˜¯ä¸€ç§ç»“æ„æ„ŸçŸ¥æŸ“è‰²æ¢å¤æ‰©æ•£æ¨¡å‹ï¼Œå°†è™šæ‹ŸæŸ“è‰²é‡æ–°å®šä¹‰ä¸ºå›¾åƒæ¢å¤ä»»åŠ¡ã€‚é€šè¿‡ç»“åˆæ®‹å·®å’ŒåŸºäºå™ªå£°çš„ç”Ÿæˆè·¯å¾„ï¼ŒStar-Diffåœ¨æ¨¡æ‹Ÿç”Ÿç‰©æ ‡å¿—ç‰©å˜å¼‚æ€§çš„åŒæ—¶ä¿æŒäº†ç»„ç»‡ç»“æ„ã€‚ä¸ºäº†è¯„ä¼°ç”ŸæˆIHCæ–‘å—çš„è¯Šæ–­ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†è¯­ä¹‰ä¿çœŸåº¦è¯„åˆ†ï¼ˆSFSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºä¸´åºŠåˆ†çº§ä»»åŠ¡çš„æŒ‡æ ‡ï¼Œæ ¹æ®ç”Ÿç‰©æ ‡å¿—ç‰©åˆ†ç±»å‡†ç¡®æ€§é‡åŒ–ç±»çº§åˆ«çš„è¯­ä¹‰é™è§£ã€‚ä¸SSIMå’ŒPSNRç­‰åƒç´ çº§æŒ‡æ ‡ä¸åŒï¼ŒSFSåœ¨ç©ºé—´é”™ä½å’Œåˆ†ç±»å™¨ä¸ç¡®å®šæ€§ä¸‹ä¿æŒç¨³å¥ã€‚åœ¨BCIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒStar-Diffåœ¨è§†è§‰ä¿çœŸåº¦å’Œè¯Šæ–­ç›¸å…³æ€§æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼ˆSOTAï¼‰ã€‚å…¶å¿«é€Ÿæ¨ç†å’Œå¼ºå¤§çš„ä¸´åºŠå¯¹é½èƒ½åŠ›ä¸ºæœ¯ä¸­è™šæ‹ŸIHCåˆæˆç­‰åº”ç”¨æä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>H&amp;EæŸ“è‰²åœ¨è¯„ä¼°ç»„ç»‡å½¢æ€ä¸Šæ˜¯ä¸´åºŠæ ‡å‡†ï¼Œä½†ç¼ºä¹åˆ†å­æ°´å¹³çš„è¯Šæ–­ä¿¡æ¯ã€‚</li>
<li>å…ç–«ç»„ç»‡åŒ–å­¦ï¼ˆIHCï¼‰æä¾›æœ‰å…³ç”Ÿç‰©æ ‡å¿—ç‰©è¡¨è¾¾çš„è§è§£ï¼Œä½†æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚</li>
<li>è™šæ‹ŸæŸ“è‰²æŠ€æœ¯ä½œä¸ºä»H&amp;Eåˆ°IHCçš„æ›¿ä»£æ–¹æ¡ˆé¢ä¸´ä¸¤ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ï¼šåˆæˆå›¾åƒä¸çœŸå®IHCçš„è¯„ä¼°åŠç»“æ„å®Œæ•´æ€§å’Œç”Ÿç‰©å˜å¼‚æ€§çš„ä¿æŒã€‚</li>
<li>Star-Diffæ˜¯ä¸€ç§ç»“æ„æ„ŸçŸ¥æŸ“è‰²æ¢å¤æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å°†è™šæ‹ŸæŸ“è‰²ä½œä¸ºå›¾åƒæ¢å¤ä»»åŠ¡æ¥å¤„ç†ã€‚</li>
<li>Star-Diffç»“åˆäº†æ®‹å·®å’ŒåŸºäºå™ªå£°çš„ç”Ÿæˆè·¯å¾„ï¼Œä»¥æ¨¡æ‹Ÿç”Ÿç‰©æ ‡å¿—ç‰©å˜å¼‚æ€§å’Œä¿æŒç»„ç»‡ç»“æ„ã€‚</li>
<li>è¯­ä¹‰ä¿çœŸåº¦è¯„åˆ†ï¼ˆSFSï¼‰æ˜¯ä¸€ç§æ–°çš„è¯„ä»·æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆçš„IHCå›¾åƒçš„è¯Šæ–­ä¸€è‡´æ€§ï¼Œå®ƒåŸºäºä¸´åºŠåˆ†çº§ä»»åŠ¡å¹¶è€ƒè™‘ç”Ÿç‰©æ ‡å¿—ç‰©çš„åˆ†ç±»å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02528">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-665af69cf482ef14026c7a36af1f5287.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-911bd2225eb2f8fe2d65c1d9c728e40d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c141b691349995552d3893c00183d361.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Dream-to-Recon-Monocular-3D-Reconstruction-with-Diffusion-Depth-Distillation-from-Single-Images"><a href="#Dream-to-Recon-Monocular-3D-Reconstruction-with-Diffusion-Depth-Distillation-from-Single-Images" class="headerlink" title="Dream-to-Recon: Monocular 3D Reconstruction with Diffusion-Depth   Distillation from Single Images"></a>Dream-to-Recon: Monocular 3D Reconstruction with Diffusion-Depth   Distillation from Single Images</h2><p><strong>Authors:Philipp Wulff, Felix Wimbauer, Dominik Muhle, Daniel Cremers</strong></p>
<p>Volumetric scene reconstruction from a single image is crucial for a broad range of applications like autonomous driving and robotics. Recent volumetric reconstruction methods achieve impressive results, but generally require expensive 3D ground truth or multi-view supervision. We propose to leverage pre-trained 2D diffusion models and depth prediction models to generate synthetic scene geometry from a single image. This can then be used to distill a feed-forward scene reconstruction model. Our experiments on the challenging KITTI-360 and Waymo datasets demonstrate that our method matches or outperforms state-of-the-art baselines that use multi-view supervision, and offers unique advantages, for example regarding dynamic scenes. </p>
<blockquote>
<p>ä»å•å¼ å›¾åƒè¿›è¡Œåœºæ™¯ä½“ç§¯é‡å»ºå¯¹äºè‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººæŠ€æœ¯ç­‰å¹¿æ³›åº”ç”¨é¢†åŸŸè‡³å…³é‡è¦ã€‚è™½ç„¶æœ€è¿‘çš„ä½“ç§¯é‡å»ºæ–¹æ³•å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦æ˜‚è´µçš„3DçœŸå®æ•°æ®æˆ–å¤šè§†è§’ç›‘ç£ã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨é¢„è®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹å’Œæ·±åº¦é¢„æµ‹æ¨¡å‹æ¥ä»å•å¼ å›¾åƒç”Ÿæˆåˆæˆåœºæ™¯å‡ ä½•ç»“æ„ï¼Œç„¶åå¯ä»¥ç”¨äºæç‚¼å‰é¦ˆåœºæ™¯é‡å»ºæ¨¡å‹ã€‚æˆ‘ä»¬åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„KITTI-360å’ŒWaymoæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä½¿ç”¨å¤šè§†è§’ç›‘ç£çš„å…ˆè¿›æŠ€æœ¯ç›¸æ¯”ï¼Œæ€§èƒ½ç›¸åŒ¹é…æˆ–æ›´èƒœä¸€ç­¹ï¼Œå¹¶å…·æœ‰ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œä¾‹å¦‚åœ¨åŠ¨æ€åœºæ™¯æ–¹é¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02323v1">PDF</a> ICCV 2025. Website: <a target="_blank" rel="noopener" href="https://philippwulff.github.io/dream-to-recon">https://philippwulff.github.io/dream-to-recon</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†åˆ©ç”¨é¢„è®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹å’Œæ·±åº¦é¢„æµ‹æ¨¡å‹ï¼Œä»å•å¼ å›¾åƒç”Ÿæˆåˆæˆåœºæ™¯å‡ ä½•çš„æ–¹æ³•ï¼Œç”¨äºè’¸é¦å‰é¦ˆåœºæ™¯é‡å»ºæ¨¡å‹ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„KITTI-360å’ŒWaymoæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åŒ¹é…æˆ–è¶…è¶Šäº†ä½¿ç”¨å¤šè§†è§’ç›‘ç£çš„åŸºçº¿ï¼Œå¹¶å…·æœ‰åŠ¨æ€åœºæ™¯çš„ç‹¬ç‰¹ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« è®¨è®ºäº†ä»å•å¼ å›¾åƒè¿›è¡Œä½“ç§¯åœºæ™¯é‡å»ºçš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºå…¶åœ¨è‡ªåŠ¨é©¾é©¶å’Œæœºå™¨äººæŠ€æœ¯ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚</li>
<li>ç°æœ‰ä½“ç§¯é‡å»ºæ–¹æ³•è™½æ•ˆæœæ˜¾è‘—ï¼Œä½†éœ€è¦æ˜‚è´µçš„3DçœŸå®æ•°æ®æˆ–å¤šè§†è§’ç›‘ç£ã€‚</li>
<li>æ–‡ç« æå‡ºäº†åˆ©ç”¨é¢„è®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹å’Œæ·±åº¦é¢„æµ‹æ¨¡å‹çš„æ–¹æ³•ï¼Œä»å•å¼ å›¾åƒç”Ÿæˆåˆæˆåœºæ™¯å‡ ä½•ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿè’¸é¦å‡ºå‰é¦ˆåœºæ™¯é‡å»ºæ¨¡å‹ã€‚</li>
<li>åœ¨KITTI-360å’ŒWaymoæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸å¤šè§†è§’ç›‘ç£çš„åŸºçº¿ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨åŠ¨æ€åœºæ™¯ä¸Šå…·æœ‰ç‹¬ç‰¹ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02323">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c6ed5e977aa03bf1cd37affd00e8b192.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e736ca1c28a5766a4c325b58c6bb261.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5fdc7a2331b06b5adea35c6da680d36.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Devil-is-in-the-Detail-Towards-Injecting-Fine-Details-of-Image-Prompt-in-Image-Generation-via-Conflict-free-Guidance-and-Stratified-Attention"><a href="#Devil-is-in-the-Detail-Towards-Injecting-Fine-Details-of-Image-Prompt-in-Image-Generation-via-Conflict-free-Guidance-and-Stratified-Attention" class="headerlink" title="Devil is in the Detail: Towards Injecting Fine Details of Image Prompt   in Image Generation via Conflict-free Guidance and Stratified Attention"></a>Devil is in the Detail: Towards Injecting Fine Details of Image Prompt   in Image Generation via Conflict-free Guidance and Stratified Attention</h2><p><strong>Authors:Kyungmin Jo, Jooyeol Yun, Jaegul Choo</strong></p>
<p>While large-scale text-to-image diffusion models enable the generation of high-quality, diverse images from text prompts, these prompts struggle to capture intricate details, such as textures, preventing the user intent from being reflected. This limitation has led to efforts to generate images conditioned on user-provided images, referred to as image prompts. Recent work modifies the self-attention mechanism to impose image conditions in generated images by replacing or concatenating the keys and values from the image prompt. This enables the self-attention layer to work like a cross-attention layer, generally used to incorporate text prompts. In this paper, we identify two common issues in existing methods of modifying self-attention to generate images that reflect the details of image prompts. First, existing approaches neglect the importance of image prompts in classifier-free guidance. Specifically, current methods use image prompts as both desired and undesired conditions in classifier-free guidance, causing conflicting signals. To resolve this, we propose conflict-free guidance by using image prompts only as desired conditions, ensuring that the generated image faithfully reflects the image prompt. In addition, we observe that the two most common self-attention modifications involve a trade-off between the realism of the generated image and alignment with the image prompt. Specifically, selecting more keys and values from the image prompt improves alignment, while selecting more from the generated image enhances realism. To balance both, we propose an new self-attention modification method, Stratified Attention to jointly use keys and values from both images rather than selecting between them. Through extensive experiments across three image generation tasks, we show that the proposed method outperforms existing image-prompting models in faithfully reflecting the image prompt. </p>
<blockquote>
<p>è™½ç„¶å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿä»æ–‡æœ¬æç¤ºä¸­ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å›¾åƒï¼Œä½†è¿™äº›æç¤ºå¾ˆéš¾æ•æ‰å¤æ‚çš„ç»†èŠ‚ï¼Œå¦‚çº¹ç†ï¼Œæ— æ³•åæ˜ ç”¨æˆ·çš„æ„å›¾ã€‚è¿™ä¸€å±€é™æ€§ä¿ƒä½¿äººä»¬åŠªåŠ›æ ¹æ®ç”¨æˆ·æä¾›çš„å›¾åƒç”Ÿæˆå›¾åƒï¼Œç§°ä¸ºå›¾åƒæç¤ºã€‚æœ€è¿‘çš„å·¥ä½œé€šè¿‡ä¿®æ”¹è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡åœ¨ç”Ÿæˆçš„å›¾åƒä¸­æ›¿æ¢æˆ–è¿æ¥å›¾åƒæç¤ºçš„é”®å’Œå€¼æ¥æ–½åŠ å›¾åƒæ¡ä»¶ã€‚è¿™ä½¿å¾—è‡ªæ³¨æ„åŠ›å±‚èƒ½å¤Ÿåƒç”¨äºèåˆæ–‡æœ¬æç¤ºçš„äº¤å‰æ³¨æ„åŠ›å±‚é‚£æ ·å·¥ä½œã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸¤ç§åœ¨ç°æœ‰æ–¹æ³•ä¸­ä¿®æ”¹è‡ªæ³¨æ„åŠ›ä»¥ç”Ÿæˆåæ˜ å›¾åƒæç¤ºç»†èŠ‚çš„å›¾åƒæ—¶å­˜åœ¨çš„å¸¸è§é—®é¢˜ã€‚é¦–å…ˆï¼Œç°æœ‰æ–¹æ³•å¿½è§†äº†å›¾åƒæç¤ºåœ¨åˆ†ç±»å™¨è‡ªç”±æŒ‡å¯¼ä¸­çš„é‡è¦æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œå½“å‰çš„æ–¹æ³•å°†å›¾åƒæç¤ºæ—¢ç”¨ä½œæ‰€éœ€æ¡ä»¶åˆç”¨ä½œéæ‰€éœ€æ¡ä»¶ï¼Œäº§ç”Ÿç›¸äº’çŸ›ç›¾çš„ä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡ä»…ä½¿ç”¨å›¾åƒæç¤ºä½œä¸ºæ‰€éœ€æ¡ä»¶ï¼Œæå‡ºäº†æ— å†²çªæŒ‡å¯¼æ–¹æ³•ï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒå¿ å®åæ˜ å›¾åƒæç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°æœ€å¸¸è§çš„ä¸¤ç§è‡ªæ³¨æ„åŠ›ä¿®æ”¹æ¶‰åŠç”Ÿæˆå›¾åƒçš„é€¼çœŸåº¦å’Œä¸å›¾åƒæç¤ºå¯¹é½ä¹‹é—´çš„æƒè¡¡ã€‚å…·ä½“æ¥è¯´ï¼Œä»å›¾åƒæç¤ºä¸­é€‰æ‹©æ›´å¤šçš„é”®å’Œå€¼å¯ä»¥æé«˜å¯¹é½æ€§ï¼Œè€Œä»ç”Ÿæˆçš„å›¾åƒä¸­é€‰æ‹©æ›´å¤šåˆ™å¢å¼ºé€¼çœŸåº¦ã€‚ä¸ºäº†å¹³è¡¡è¿™ä¸¤è€…ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è‡ªæ³¨æ„åŠ›ä¿®æ”¹æ–¹æ³•â€”â€”åˆ†å±‚æ³¨æ„åŠ›ï¼Œè¯¥æ–¹æ³•èƒ½å¤ŸåŒæ—¶ä½¿ç”¨æ¥è‡ªä¸¤è€…çš„é”®å’Œå€¼ï¼Œè€Œä¸æ˜¯åœ¨å®ƒä»¬ä¹‹é—´åšå‡ºé€‰æ‹©ã€‚é€šè¿‡ä¸‰é¡¹å›¾åƒç”Ÿæˆä»»åŠ¡çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æ‰€æå‡ºçš„æ–¹æ³•åœ¨å¿ å®åæ˜ å›¾åƒæç¤ºæ–¹é¢ä¼˜äºç°æœ‰çš„å›¾åƒæç¤ºæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02004v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒæ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå³æ— æ³•æ•æ‰ç”¨æˆ·æä¾›çš„å›¾åƒæç¤ºä¸­çš„ç»†èŠ‚ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶äººå‘˜æ”¹è¿›äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶ä»¥å¼•å…¥å›¾åƒæ¡ä»¶ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰æ–¹æ³•ä¸­çš„ä¸¤ä¸ªé—®é¢˜ã€‚é¦–å…ˆï¼Œç°æœ‰æ–¹æ³•å¿½è§†äº†å›¾åƒæç¤ºåœ¨åˆ†ç±»å™¨å…è´¹æŒ‡å¯¼ä¸­çš„é‡è¦æ€§ï¼Œå¯¼è‡´ç”Ÿæˆçš„å›¾åƒæ— æ³•å¿ å®åæ˜ å›¾åƒæç¤ºã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†æ— å†²çªæŒ‡å¯¼æ–¹æ³•ï¼Œä»…å°†å›¾åƒæç¤ºä½œä¸ºæœŸæœ›æ¡ä»¶ã€‚å…¶æ¬¡ï¼Œç°æœ‰è‡ªæ³¨æ„åŠ›ä¿®æ”¹æ–¹æ³•é¢ä¸´çœŸå®æ„Ÿå’Œä¸å›¾åƒæç¤ºå¯¹é½ä¹‹é—´çš„æƒè¡¡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åˆ†å±‚æ³¨æ„åŠ›æ–¹æ³•ï¼Œç»“åˆæ¥è‡ªå›¾åƒå’Œç”Ÿæˆå›¾åƒçš„é”®å’Œå€¼ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¿ å®åæ˜ å›¾åƒæç¤ºæ–¹é¢ä¼˜äºç°æœ‰å›¾åƒæç¤ºæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹è™½èƒ½ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å›¾åƒï¼Œä½†éš¾ä»¥æ•æ‰ç”¨æˆ·æä¾›çš„å›¾åƒæç¤ºä¸­çš„ç»†èŠ‚ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½è§†äº†å›¾åƒæç¤ºåœ¨åˆ†ç±»å™¨å…è´¹æŒ‡å¯¼ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>æå‡ºçš„æ— å†²çªæŒ‡å¯¼æ–¹æ³•ä»…å°†å›¾åƒæç¤ºä½œä¸ºæœŸæœ›æ¡ä»¶ï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒå¿ å®åæ˜ å›¾åƒæç¤ºã€‚</li>
<li>ç°æœ‰è‡ªæ³¨æ„åŠ›ä¿®æ”¹æ–¹æ³•åœ¨çœŸå®æ„Ÿå’Œä¸å›¾åƒæç¤ºå¯¹é½ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚</li>
<li>æå‡ºçš„åˆ†å±‚æ³¨æ„åŠ›æ–¹æ³•ç»“åˆæ¥è‡ªå›¾åƒå’Œç”Ÿæˆå›¾åƒçš„é”®å’Œå€¼ï¼Œä»¥å¹³è¡¡çœŸå®æ€§å’Œå¯¹é½ã€‚</li>
<li>é€šè¿‡ä¸‰ä¸ªå›¾åƒç”Ÿæˆä»»åŠ¡çš„å¹¿æ³›å®éªŒï¼Œæ–°æ–¹æ³•åœ¨å¿ å®åæ˜ å›¾åƒæç¤ºæ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02004">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5b286ef525cb614dc8e725aba515bb75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efab99c3b65dde5230f53b39ffbba4ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-290823b79e337f7cf379f797d02e94d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d364ee80cada1551e0752f1a20763831.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6038a75c19b999e2b3bb58c51cda8c02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5077066eae218b9e58fae3bee1976ff1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DiffusionFF-Face-Forgery-Detection-via-Diffusion-based-Artifact-Localization"><a href="#DiffusionFF-Face-Forgery-Detection-via-Diffusion-based-Artifact-Localization" class="headerlink" title="DiffusionFF: Face Forgery Detection via Diffusion-based Artifact   Localization"></a>DiffusionFF: Face Forgery Detection via Diffusion-based Artifact   Localization</h2><p><strong>Authors:Siran Peng, Haoyuan Zhang, Li Gao, Tianshuo Zhang, Bao Li, Zhen Lei</strong></p>
<p>The rapid evolution of deepfake generation techniques demands robust and accurate face forgery detection algorithms. While determining whether an image has been manipulated remains essential, the ability to precisely localize forgery artifacts has become increasingly important for improving model explainability and fostering user trust. To address this challenge, we propose DiffusionFF, a novel framework that enhances face forgery detection through diffusion-based artifact localization. Our method utilizes a denoising diffusion model to generate high-quality Structural Dissimilarity (DSSIM) maps, which effectively capture subtle traces of manipulation. These DSSIM maps are then fused with high-level semantic features extracted by a pretrained forgery detector, leading to significant improvements in detection accuracy. Extensive experiments on both cross-dataset and intra-dataset benchmarks demonstrate that DiffusionFF not only achieves superior detection performance but also offers precise and fine-grained artifact localization, highlighting its overall effectiveness. </p>
<blockquote>
<p>æ·±åº¦ä¼ªé€ ç”ŸæˆæŠ€æœ¯çš„å¿«é€Ÿå‘å±•è¦æ±‚æœ‰ç¨³å¥ä¸”å‡†ç¡®çš„äººè„¸ä¼ªé€ æ£€æµ‹ç®—æ³•ã€‚ç¡®å®šå›¾åƒæ˜¯å¦è¢«ç¯¡æ”¹ä»ç„¶è‡³å…³é‡è¦ï¼Œä½†ç²¾ç¡®å®šä½ä¼ªé€ ç—•è¿¹çš„èƒ½åŠ›åœ¨æé«˜æ¨¡å‹è§£é‡Šæ€§å’Œä¿ƒè¿›ç”¨æˆ·ä¿¡ä»»æ–¹é¢å˜å¾—æ—¥ç›Šé‡è¦ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DiffusionFFè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡åŸºäºæ‰©æ•£çš„ç—•è¿¹å®šä½æŠ€æœ¯æ¥æé«˜äººè„¸ä¼ªé€ æ£€æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å»å™ªæ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„ç»“æ„å·®å¼‚ï¼ˆDSSIMï¼‰å›¾ï¼Œæœ‰æ•ˆæ•æ‰ç»†å¾®çš„ç¯¡æ”¹ç—•è¿¹ã€‚è¿™äº›DSSIMå›¾ç„¶åä¸ç”±é¢„è®­ç»ƒä¼ªé€ æ£€æµ‹å™¨æå–çš„é«˜çº§è¯­ä¹‰ç‰¹å¾ç›¸èåˆï¼Œæ˜¾è‘—æé«˜äº†æ£€æµ‹ç²¾åº¦ã€‚è·¨æ•°æ®é›†å’Œå†…éƒ¨æ•°æ®é›†çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDiffusionFFä¸ä»…å®ç°äº†å“è¶Šçš„æ£€æµ‹æ€§èƒ½ï¼Œè€Œä¸”æä¾›äº†ç²¾ç¡®å’Œç²¾ç»†çš„ç—•è¿¹å®šä½ï¼Œå‡¸æ˜¾äº†å…¶æ•´ä½“æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01873v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦ä¼ªé€ ç”ŸæˆæŠ€æœ¯çš„å¿«é€Ÿå‘å±•è¦æ±‚æœ‰ç¨³å¥å‡†ç¡®çš„é¢éƒ¨ä¼ªé€ æ£€æµ‹ç®—æ³•ã€‚é™¤äº†åˆ¤æ–­å›¾åƒæ˜¯å¦è¢«æ“çºµå¤–ï¼Œèƒ½å¤Ÿç²¾ç¡®å®šä½ä¼ªé€ ç—•è¿¹çš„èƒ½åŠ›å¯¹äºæé«˜æ¨¡å‹è§£é‡Šæ€§å’Œå¢å¼ºç”¨æˆ·ä¿¡ä»»åº¦ä¹Ÿå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DiffusionFFè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡åŸºäºæ‰©æ•£çš„ç—•è¿¹å®šä½æŠ€æœ¯æ¥æå‡é¢éƒ¨ä¼ªé€ æ£€æµ‹æ•ˆæœã€‚è¯¥æ–¹æ³•åˆ©ç”¨å»å™ªæ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„ç»“æ„å·®å¼‚ï¼ˆDSSIMï¼‰å›¾ï¼Œæœ‰æ•ˆæ•æ‰å¾®å¦™çš„æ“ä½œç—•è¿¹ã€‚è¿™äº›DSSIMå›¾éšåä¸é¢„è®­ç»ƒä¼ªé€ æ£€æµ‹å™¨æå–çš„é«˜çº§è¯­ä¹‰ç‰¹å¾èåˆï¼Œæå¤§åœ°æé«˜äº†æ£€æµ‹å‡†ç¡®ç‡ã€‚åœ¨è·¨æ•°æ®é›†å’Œå†…éƒ¨æ•°æ®é›†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDiffusionFFä¸ä»…å®ç°äº†å‡ºè‰²çš„æ£€æµ‹æ€§èƒ½ï¼Œè€Œä¸”æä¾›äº†ç²¾ç¡®å’Œç²¾ç»†çš„ç—•è¿¹å®šä½ï¼Œå‡¸æ˜¾äº†å…¶æ•´ä½“æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ä¼ªé€ ç”ŸæˆæŠ€æœ¯çš„å¿«é€Ÿå‘å±•è¦æ±‚æœ‰ç¨³å¥å‡†ç¡®çš„é¢éƒ¨ä¼ªé€ æ£€æµ‹ç®—æ³•ã€‚</li>
<li>é™¤äº†åˆ¤æ–­å›¾åƒæ˜¯å¦è¢«æ“çºµå¤–ï¼Œå®šä½ä¼ªé€ ç—•è¿¹çš„èƒ½åŠ›æ—¥ç›Šé‡è¦ï¼Œä»¥æé«˜æ¨¡å‹è§£é‡Šæ€§å’Œç”¨æˆ·ä¿¡ä»»åº¦ã€‚</li>
<li>DiffusionFFæ¡†æ¶åˆ©ç”¨å»å™ªæ‰©æ•£æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„ç»“æ„å·®å¼‚ï¼ˆDSSIMï¼‰å›¾ï¼Œä»¥æ•æ‰æ“ä½œç—•è¿¹ã€‚</li>
<li>DSSIMå›¾ä¸é¢„è®­ç»ƒä¼ªé€ æ£€æµ‹å™¨çš„é«˜çº§è¯­ä¹‰ç‰¹å¾èåˆï¼Œæé«˜äº†æ£€æµ‹å‡†ç¡®ç‡ã€‚</li>
<li>DiffusionFFå®ç°äº†è·¨æ•°æ®é›†å’Œå†…éƒ¨æ•°æ®é›†çš„ä¼˜å¼‚æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>DiffusionFFæä¾›äº†ç²¾ç¡®å’Œç²¾ç»†çš„ä¼ªé€ ç—•è¿¹å®šä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01873">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-066c16821967d4265f79871679349e11.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ce46e6b1f5326aad001a145a0fbf643.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94419728732b608cd95b9b637bdc5550.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42ad975732e1be8f58e5c1759f0002d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2bd4275f69a04a8609f50774c2643265.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00971867626d3b87c5371b6365befe0d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Diffusion-based-3D-Hand-Motion-Recovery-with-Intuitive-Physics"><a href="#Diffusion-based-3D-Hand-Motion-Recovery-with-Intuitive-Physics" class="headerlink" title="Diffusion-based 3D Hand Motion Recovery with Intuitive Physics"></a>Diffusion-based 3D Hand Motion Recovery with Intuitive Physics</h2><p><strong>Authors:Yufei Zhang, Zijun Cui, Jeffrey O. Kephart, Qiang Ji</strong></p>
<p>While 3D hand reconstruction from monocular images has made significant progress, generating accurate and temporally coherent motion estimates from videos remains challenging, particularly during hand-object interactions. In this paper, we present a novel 3D hand motion recovery framework that enhances image-based reconstructions through a diffusion-based and physics-augmented motion refinement model. Our model captures the distribution of refined motion estimates conditioned on initial ones, generating improved sequences through an iterative denoising process. Instead of relying on scarce annotated video data, we train our model only using motion capture data without images. We identify valuable intuitive physics knowledge during hand-object interactions, including key motion states and their associated motion constraints. We effectively integrate these physical insights into our diffusion model to improve its performance. Extensive experiments demonstrate that our approach significantly improves various frame-wise reconstruction methods, achieving state-of-the-art (SOTA) performance on existing benchmarks. </p>
<blockquote>
<p>è™½ç„¶ä»å•ç›®å›¾åƒä¸­é‡å»º3Dæ‰‹éƒ¨å·²ç»å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»è§†é¢‘ä¸­ç”Ÿæˆå‡†ç¡®ä¸”æ—¶é—´è¿è´¯çš„è¿åŠ¨ä¼°è®¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ‰‹éƒ¨ä¸ç‰©ä½“äº¤äº’æ—¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„3Dæ‰‹éƒ¨è¿åŠ¨æ¢å¤æ¡†æ¶ï¼Œå®ƒé€šè¿‡åŸºäºæ‰©æ•£å’Œç‰©ç†å¢å¼ºçš„è¿åŠ¨ä¼˜åŒ–æ¨¡å‹æ¥å¢å¼ºåŸºäºå›¾åƒçš„é‡å»ºã€‚æˆ‘ä»¬çš„æ¨¡å‹æ•è·äº†åŸºäºåˆå§‹ä¼°è®¡çš„ç²¾ç»†è¿åŠ¨ä¼°è®¡çš„åˆ†å¸ƒï¼Œå¹¶é€šè¿‡è¿­ä»£å»å™ªè¿‡ç¨‹ç”Ÿæˆäº†æ”¹è¿›åºåˆ—ã€‚æˆ‘ä»¬å¹¶æ²¡æœ‰ä¾èµ–äºç¨€ç¼ºçš„æ ‡æ³¨è§†é¢‘æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ï¼Œè€Œæ˜¯ä»…ä½¿ç”¨è¿åŠ¨æ•è·æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸ä½¿ç”¨å›¾åƒã€‚æˆ‘ä»¬ç¡®å®šäº†åœ¨æ‰‹éƒ¨ä¸ç‰©ä½“äº¤äº’è¿‡ç¨‹ä¸­çš„æœ‰ä»·å€¼çš„ç›´è§‚ç‰©ç†çŸ¥è¯†ï¼ŒåŒ…æ‹¬å…³é”®è¿åŠ¨çŠ¶æ€åŠå…¶ç›¸å…³çš„è¿åŠ¨çº¦æŸã€‚æˆ‘ä»¬æœ‰æ•ˆåœ°å°†è¿™äº›ç‰©ç†è§è§£é›†æˆåˆ°æˆ‘ä»¬çš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»¥æé«˜å…¶æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æ”¹è¿›äº†å„ç§å¸§çº§é‡å»ºæ–¹æ³•ï¼Œåœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01835v1">PDF</a> ICCV2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„3Dæ‰‹éƒ¨è¿åŠ¨æ¢å¤æ¡†æ¶ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹å’Œç‰©ç†å¢å¼ºè¿åŠ¨ä¼˜åŒ–æ¨¡å‹æå‡åŸºäºå›¾åƒçš„é‡å»ºæ•ˆæœã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿæ•æ‰åˆå§‹è¿åŠ¨ä¼°è®¡çš„åˆ†å¸ƒæƒ…å†µï¼Œå¹¶é€šè¿‡è¿­ä»£å»å™ªè¿‡ç¨‹ç”Ÿæˆæ”¹è¿›åºåˆ—ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨è¿åŠ¨æ•æ‰æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€å›¾åƒæ•°æ®ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å°†æ‰‹éƒ¨ä¸ç‰©ä½“äº¤äº’è¿‡ç¨‹ä¸­çš„ç›´è§‰ç‰©ç†çŸ¥è¯†èå…¥æ‰©æ•£æ¨¡å‹ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œæ˜¾è‘—æé«˜å¸§é‡å»ºæ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„3Dæ‰‹éƒ¨è¿åŠ¨æ¢å¤æ¡†æ¶ï¼Œç»“åˆäº†æ‰©æ•£æ¨¡å‹å’Œç‰©ç†å¢å¼ºæŠ€æœ¯ï¼Œæé«˜äº†åŸºäºå›¾åƒçš„é‡å»ºæ•ˆæœã€‚</li>
<li>é€šè¿‡æ•æ‰åˆå§‹è¿åŠ¨ä¼°è®¡çš„åˆ†å¸ƒæƒ…å†µï¼Œå¹¶è¿­ä»£å»å™ªï¼Œç”Ÿæˆæ”¹è¿›çš„è¿åŠ¨åºåˆ—ã€‚</li>
<li>ä»…åˆ©ç”¨è¿åŠ¨æ•æ‰æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€å›¾åƒæ•°æ®ã€‚</li>
<li>èå…¥ç›´è§‰ç‰©ç†çŸ¥è¯†ï¼ŒåŒ…æ‹¬å…³é”®è¿åŠ¨çŠ¶æ€å’Œå…³è”çš„è¿åŠ¨çº¦æŸï¼Œæå‡æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¾è‘—æé«˜å¸§é‡å»ºæ•ˆæœï¼Œå®ç°äº†å¯¹ç°æœ‰æŠ€æœ¯çš„è¶…è¶Šã€‚</li>
<li>æå‡ºçš„æ¨¡å‹åœ¨ç°æœ‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01835">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3c551d727303d9c5a35a456988f008eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8402cebb6ad0346899d7b1fbb7e725af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0d3842ea4749725586e26543e02ca77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0b9c5e71adde779b6ee199136ec02ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8fc93dae40350ae654866c9959188303.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DisCo3D-Distilling-Multi-View-Consistency-for-3D-Scene-Editing"><a href="#DisCo3D-Distilling-Multi-View-Consistency-for-3D-Scene-Editing" class="headerlink" title="DisCo3D: Distilling Multi-View Consistency for 3D Scene Editing"></a>DisCo3D: Distilling Multi-View Consistency for 3D Scene Editing</h2><p><strong>Authors:Yufeng Chi, Huimin Ma, Kafeng Wang, Jianmin Li</strong></p>
<p>While diffusion models have demonstrated remarkable progress in 2D image generation and editing, extending these capabilities to 3D editing remains challenging, particularly in maintaining multi-view consistency. Classical approaches typically update 3D representations through iterative refinement based on a single editing view. However, these methods often suffer from slow convergence and blurry artifacts caused by cross-view inconsistencies. Recent methods improve efficiency by propagating 2D editing attention features, yet still exhibit fine-grained inconsistencies and failure modes in complex scenes due to insufficient constraints. To address this, we propose \textbf{DisCo3D}, a novel framework that distills 3D consistency priors into a 2D editor. Our method first fine-tunes a 3D generator using multi-view inputs for scene adaptation, then trains a 2D editor through consistency distillation. The edited multi-view outputs are finally optimized into 3D representations via Gaussian Splatting. Experimental results show DisCo3D achieves stable multi-view consistency and outperforms state-of-the-art methods in editing quality. </p>
<blockquote>
<p>è™½ç„¶æ‰©æ•£æ¨¡å‹åœ¨äºŒç»´å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å°†è¿™äº›èƒ½åŠ›æ‰©å±•åˆ°ä¸‰ç»´ç¼–è¾‘ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ä¿æŒå¤šè§†å›¾ä¸€è‡´æ€§æ–¹é¢ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸åŸºäºå•ä¸ªç¼–è¾‘è§†å›¾é€šè¿‡è¿­ä»£ä¼˜åŒ–æ¥æ›´æ–°ä¸‰ç»´è¡¨ç¤ºã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸å­˜åœ¨æ”¶æ•›é€Ÿåº¦æ…¢ã€ç”±äºè·¨è§†å›¾ä¸ä¸€è‡´å¯¼è‡´å›¾åƒæ¨¡ç³Šç­‰ç¼ºé™·ã€‚æœ€è¿‘çš„æ–¹æ³•é€šè¿‡ä¼ æ’­äºŒç»´ç¼–è¾‘æ³¨æ„åŠ›ç‰¹å¾æ¥æé«˜æ•ˆç‡ï¼Œä½†ç”±äºçº¦æŸä¸è¶³ï¼Œåœ¨å¤æ‚åœºæ™¯ä¸­ä»ç„¶å­˜åœ¨ç»†å¾®çš„ä¸ä¸€è‡´æ€§å’Œå¤±è´¥æ¨¡å¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°å‹æ¡†æ¶DisCo3Dï¼Œå®ƒé€šè¿‡è’¸é¦ä¸‰ç»´ä¸€è‡´æ€§å…ˆéªŒçŸ¥è¯†æ¥è¾…åŠ©äºŒç»´ç¼–è¾‘å™¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆä½¿ç”¨å¤šè§†å›¾è¾“å…¥å¯¹ä¸‰ç»´ç”Ÿæˆå™¨è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”åœºæ™¯ï¼Œç„¶åé€šè¿‡ä¸€è‡´æ€§è’¸é¦æ¥è®­ç»ƒäºŒç»´ç¼–è¾‘å™¨ã€‚æœ€åï¼Œç¼–è¾‘åçš„å¤šè§†å›¾è¾“å‡ºé€šè¿‡é«˜æ–¯æ‹¼è´´ä¼˜åŒ–ä¸ºä¸‰ç»´è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDisCo3Då®ç°äº†ç¨³å®šçš„å¤šè§†å›¾ä¸€è‡´æ€§ï¼Œå¹¶åœ¨ç¼–è¾‘è´¨é‡æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01684v1">PDF</a> 17 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDisCo3Dçš„æ–°æ¡†æ¶ï¼Œå®ƒé€šè¿‡è’¸é¦3Dä¸€è‡´æ€§å…ˆéªŒçŸ¥è¯†æ¥æ”¹è¿›äºŒç»´ç¼–è¾‘å™¨åœ¨ä¸‰ç»´ç¼–è¾‘ä¸­çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡å¾®è°ƒä¸‰ç»´ç”Ÿæˆå™¨ä»¥é€‚åº”åœºæ™¯ï¼Œç„¶åè®­ç»ƒäºŒç»´ç¼–è¾‘å™¨ä»¥ä¿è¯ä¸€è‡´æ€§ã€‚æœ€åï¼Œé€šè¿‡é«˜æ–¯æ˜ å°„ä¼˜åŒ–ç¼–è¾‘åçš„å¤šè§†è§’è¾“å‡ºåˆ°ä¸‰ç»´è¡¨ç¤ºã€‚DisCo3Då®ç°äº†ç¨³å®šçš„å¤šè§†è§’ä¸€è‡´æ€§ï¼Œå¹¶åœ¨ç¼–è¾‘è´¨é‡ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨äºŒç»´å›¾åƒç”Ÿæˆå’Œç¼–è¾‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ä¸‰ç»´ç¼–è¾‘æ–¹é¢çš„æ‰©å±•ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°¤å…¶æ˜¯åœ¨ä¿æŒå¤šè§†è§’ä¸€è‡´æ€§æ–¹é¢ã€‚</li>
<li>ç»å…¸æ–¹æ³•é€šå¸¸é€šè¿‡åŸºäºå•ä¸€ç¼–è¾‘è§†è§’çš„è¿­ä»£ä¼˜åŒ–æ¥æ›´æ–°ä¸‰ç»´è¡¨ç¤ºï¼Œä½†å¸¸å¸¸é¢ä¸´æ”¶æ•›é€Ÿåº¦æ…¢å’Œè·¨è§†è§’ä¸ä¸€è‡´å¯¼è‡´çš„æ¨¡ç³Šä¼ªå½±é—®é¢˜ã€‚</li>
<li>æœ€è¿‘çš„æ–¹æ³•é€šè¿‡ä¼ æ’­äºŒç»´ç¼–è¾‘æ³¨æ„åŠ›ç‰¹å¾æé«˜äº†æ•ˆç‡ï¼Œä½†ç”±äºçº¦æŸä¸è¶³ï¼Œåœ¨å¤æ‚åœºæ™¯ä¸­ä»ç„¶å­˜åœ¨ç»†å¾®çš„ä¸ä¸€è‡´æ€§å’Œå¤±è´¥æ¨¡å¼ã€‚</li>
<li>DisCo3Dæ¡†æ¶é€šè¿‡è’¸é¦3Dä¸€è‡´æ€§å…ˆéªŒçŸ¥è¯†åˆ°äºŒç»´ç¼–è¾‘å™¨æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>DisCo3Dé¦–å…ˆä½¿ç”¨å¤šè§†è§’è¾“å…¥å¾®è°ƒä¸‰ç»´ç”Ÿæˆå™¨ä»¥é€‚åº”åœºæ™¯ã€‚</li>
<li>ç„¶åï¼Œå®ƒè®­ç»ƒäºŒç»´ç¼–è¾‘å™¨ä»¥ä¿è¯ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a117273ec868473f031e8060cd8087f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b9873b5aec74e27324485962ade360a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-063a346421ade1f5bd884a5f90321bba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b2beefad62b673bb452dfaf939621967.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="StrandDesigner-Towards-Practical-Strand-Generation-with-Sketch-Guidance"><a href="#StrandDesigner-Towards-Practical-Strand-Generation-with-Sketch-Guidance" class="headerlink" title="StrandDesigner: Towards Practical Strand Generation with Sketch Guidance"></a>StrandDesigner: Towards Practical Strand Generation with Sketch Guidance</h2><p><strong>Authors:Na Zhang, Moran Li, Chengming Xu, Han Feng, Xiaobin Hu, Jiangning Zhang, Weijian Cao, Chengjie Wang, Yanwei Fu</strong></p>
<p>Realistic hair strand generation is crucial for applications like computer graphics and virtual reality. While diffusion models can generate hairstyles from text or images, these inputs lack precision and user-friendliness. Instead, we propose the first sketch-based strand generation model, which offers finer control while remaining user-friendly. Our framework tackles key challenges, such as modeling complex strand interactions and diverse sketch patterns, through two main innovations: a learnable strand upsampling strategy that encodes 3D strands into multi-scale latent spaces, and a multi-scale adaptive conditioning mechanism using a transformer with diffusion heads to ensure consistency across granularity levels. Experiments on several benchmark datasets show our method outperforms existing approaches in realism and precision. Qualitative results further confirm its effectiveness. Code will be released at <a target="_blank" rel="noopener" href="https://github.com/fighting-Zhang/StrandDesigner">GitHub</a>. </p>
<blockquote>
<p>çœŸå®æ„Ÿçš„å‘ä¸ç”Ÿæˆå¯¹äºè®¡ç®—æœºå›¾å½¢å’Œè™šæ‹Ÿç°å®ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹å¯ä»¥ä»æ–‡æœ¬æˆ–å›¾åƒç”Ÿæˆå‘å‹ï¼Œä½†è¿™äº›è¾“å…¥ç¼ºä¹ç²¾ç¡®æ€§å’Œç”¨æˆ·å‹å¥½æ€§ã€‚ç›¸åï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè‰å›¾çš„é¦–ä¸ªå‘ä¸ç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒç”¨æˆ·å‹å¥½çš„åŒæ—¶æä¾›äº†æ›´ç²¾ç»†çš„æ§åˆ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡ä¸¤ä¸ªä¸»è¦åˆ›æ–°è§£å†³äº†å»ºæ¨¡å¤æ‚å‘ä¸äº¤äº’å’Œå¤šæ ·è‰å›¾æ¨¡å¼ç­‰å…³é”®æŒ‘æˆ˜ï¼šä¸€ç§å¯å­¦ä¹ çš„å‘ä¸ä¸Šé‡‡æ ·ç­–ç•¥ï¼Œå°†3Då‘ä¸ç¼–ç åˆ°å¤šå°ºåº¦æ½œåœ¨ç©ºé—´ä¸­ï¼›ä»¥åŠä½¿ç”¨å¸¦æœ‰æ‰©æ•£å¤´çš„å˜å‹å™¨ç¡®ä¿è·¨ç²’åº¦å±‚æ¬¡ä¸€è‡´æ€§çš„å¤šå°ºåº¦è‡ªé€‚åº”è°ƒèŠ‚æœºåˆ¶ã€‚åœ¨å‡ ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨çœŸå®æ„Ÿå’Œç²¾ç¡®åº¦æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å®šæ€§ç»“æœè¿›ä¸€æ­¥è¯å®äº†å…¶æœ‰æ•ˆæ€§ã€‚ä»£ç å°†åœ¨GitHubä¸Šå‘å¸ƒï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/fighting-Zhang/StrandDesigner">GitHubé“¾æ¥</a>ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01650v1">PDF</a> Accepted to ACM Multimedia 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè‰å›¾æŠ€æœ¯çš„å‘ä¸ç”Ÿæˆæ¨¡å‹ï¼Œè§£å†³äº†ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ç¼ºä¹ç²¾åº¦å’Œç”¨æˆ·å‹å¥½æ€§çš„é—®é¢˜ã€‚é€šè¿‡é‡‡ç”¨å­¦ä¹ å‘ä¸ä¸Šé‡‡æ ·ç­–ç•¥å’Œè‡ªé€‚åº”è°ƒèŠ‚æœºåˆ¶ç­‰æŠ€æœ¯æ‰‹æ®µï¼Œå®ç°äº†å‘ä¸æ¨¡å‹çš„ç²¾ç»†æ§åˆ¶å’Œé€¼çœŸç”Ÿæˆã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®æ„Ÿå’Œç²¾ç¡®åº¦ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå¯å¹¿æ³›åº”ç”¨äºè®¡ç®—æœºå›¾å½¢å’Œè™šæ‹Ÿç°å®é¢†åŸŸã€‚ä»£ç å·²ä¸Šä¼ è‡³GitHubä»“åº“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†é¦–ä¸ªåŸºäºè‰å›¾æŠ€æœ¯çš„å‘ä¸ç”Ÿæˆæ¨¡å‹ï¼Œå®ç°äº†ç²¾ç»†æ§åˆ¶å’Œç”¨æˆ·å‹å¥½æ€§ã€‚</li>
<li>é€šè¿‡å­¦ä¹ å‘ä¸ä¸Šé‡‡æ ·ç­–ç•¥å’Œå¤šå°ºåº¦è‡ªé€‚åº”è°ƒèŠ‚æœºåˆ¶ï¼Œè§£å†³äº†å¤æ‚å‘ä¸äº¤äº’å’Œå¤šæ ·è‰å›¾æ¨¡å¼å»ºæ¨¡çš„æŒ‘æˆ˜ã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†å‘ä¸æ¨¡å‹çš„é€¼çœŸç”Ÿæˆå’Œç²¾ç»†æ§åˆ¶ã€‚</li>
<li>æ¨¡å‹å…·å¤‡è‡ªé€‚åº”è°ƒèŠ‚æœºåˆ¶ï¼Œç¡®ä¿ä¸åŒç²’åº¦çº§åˆ«çš„ä¸€è‡´æ€§ã€‚</li>
<li>å®šæ€§ç»“æœè¯å®äº†è¯¥æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä»£ç å·²å…¬å¼€ï¼Œæ–¹ä¾¿ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01650">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9bb06eb8aacb4378292c8a0208510227.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e150646394efe3edea03f14a2ae9aa51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e763132e3f96ab47efe421894d80d949.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LLaDA-MedV-Exploring-Large-Language-Diffusion-Models-for-Biomedical-Image-Understanding"><a href="#LLaDA-MedV-Exploring-Large-Language-Diffusion-Models-for-Biomedical-Image-Understanding" class="headerlink" title="LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical   Image Understanding"></a>LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical   Image Understanding</h2><p><strong>Authors:Xuanzhao Dong, Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Peijie Qiu, Shao Tang, Xin Li, Yalin Wang</strong></p>
<p>Autoregressive models (ARMs) have long dominated the landscape of biomedical vision-language models (VLMs). Recently, masked diffusion models such as LLaDA have emerged as promising alternatives, yet their application in the biomedical domain remains largely underexplored. To bridge this gap, we introduce \textbf{LLaDA-MedV}, the first large language diffusion model tailored for biomedical image understanding through vision instruction tuning. LLaDA-MedV achieves relative performance gains of 7.855% over LLaVA-Med and 1.867% over LLaDA-V in the open-ended biomedical visual conversation task, and sets new state-of-the-art accuracy on the closed-form subset of three VQA benchmarks: 84.93% on VQA-RAD, 92.31% on SLAKE, and 95.15% on PathVQA. Furthermore, a detailed comparison with LLaVA-Med suggests that LLaDA-MedV is capable of generating reasonably longer responses by explicitly controlling response length, which can lead to more informative outputs. We also conduct an in-depth analysis of both the training and inference stages, highlighting the critical roles of initialization weight selection, fine-tuning strategies, and the interplay between sampling steps and response repetition. The code and model weight is released at <a target="_blank" rel="noopener" href="https://github.com/LLM-VLM-GSL/LLaDA-MedV">https://github.com/LLM-VLM-GSL/LLaDA-MedV</a>. </p>
<blockquote>
<p>è‡ªå›å½’æ¨¡å‹ï¼ˆARMsï¼‰é•¿æœŸä»¥æ¥åœ¨ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰é¢†åŸŸå æ®ä¸»å¯¼åœ°ä½ã€‚æœ€è¿‘ï¼Œå¦‚LLaDAä¹‹ç±»çš„æ©æ¨¡æ‰©æ•£æ¨¡å‹å·²æˆä¸ºæœ‰å‰æ™¯çš„æ›¿ä»£å“ï¼Œç„¶è€Œå®ƒä»¬åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„åº”ç”¨ä»è¢«å¤§å¤§å¿½è§†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†é’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒç†è§£è®¾è®¡çš„è¯­è¨€æ‰©æ•£æ¨¡å‹LLaDA-MedVã€‚LLaDA-MedVåœ¨å¼€æ”¾å¼ç”Ÿç‰©åŒ»å­¦è§†è§‰å¯¹è¯ä»»åŠ¡ä¸Šç›¸å¯¹äºLLaVA-Medçš„æ€§èƒ½æå‡äº†7.855%ï¼Œç›¸å¯¹äºLLaDA-Vçš„æ€§èƒ½æå‡äº†1.867%ï¼Œå¹¶åœ¨ä¸‰ä¸ªé—®ç­”åŸºå‡†æµ‹è¯•é›†çš„å°é—­å¼å­é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„å‡†ç¡®æ€§ï¼šåœ¨VQA-RADä¸Šä¸º84.93%ï¼Œåœ¨SLAKEä¸Šä¸º92.31%ï¼Œåœ¨PathVQAä¸Šä¸º95.15%ã€‚æ­¤å¤–ï¼Œä¸LLaVA-Medçš„è¯¦ç»†æ¯”è¾ƒè¡¨æ˜ï¼ŒLLaDA-MedVèƒ½å¤Ÿé€šè¿‡æ˜ç¡®æ§åˆ¶å“åº”é•¿åº¦æ¥ç”Ÿæˆæ›´é•¿çš„åˆç†å“åº”ï¼Œä»è€Œç”Ÿæˆæ›´å…·ä¿¡æ¯é‡çš„è¾“å‡ºã€‚æˆ‘ä»¬è¿˜æ·±å…¥åˆ†æäº†è®­ç»ƒå’Œæ¨ç†é˜¶æ®µï¼Œå¼ºè°ƒäº†åˆå§‹åŒ–æƒé‡é€‰æ‹©ã€å¾®è°ƒç­–ç•¥ä»¥åŠé‡‡æ ·æ­¥éª¤ä¸å“åº”é‡å¤ä¹‹é—´çš„ç›¸äº’ä½œç”¨çš„é‡è¦æ€§ã€‚ä»£ç å’Œæ¨¡å‹æƒé‡å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/LLM-VLM-GSL/LLaDA-MedV%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/LLM-VLM-GSL/LLaDA-MedVå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01617v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿‘æœŸï¼Œé’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒç†è§£çš„è‡ªåŠ¨è¯­è¨€æ‰©æ•£æ¨¡å‹LLaDA-MedVè¢«æå‡ºã€‚ç›¸è¾ƒäºå…¶ä»–æ¨¡å‹ï¼ŒLLaDA-MedVåœ¨å¼€æ”¾å‹ç”Ÿç‰©åŒ»å­¦è§†è§‰å¯¹è¯ä»»åŠ¡ä¸­å…·æœ‰æ›´é«˜çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒæ—¶åœ¨ä¸‰ä¸ªè§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸­çš„å‡†ç¡®ç‡è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚æ¨¡å‹å¯ä»¥ç”Ÿæˆè¾ƒé•¿çš„å›åº”å¹¶æ§åˆ¶å“åº”é•¿åº¦ï¼Œæä¾›æ›´å¤šä¿¡æ¯è¾“å‡ºã€‚æ¨¡å‹çš„å…³é”®è®­ç»ƒä¸æ¨ç†é˜¶æ®µä¹Ÿè¢«æ·±å…¥åˆ†æã€‚ä»£ç å’Œæ¨¡å‹æƒé‡å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLaDA-MedVæ˜¯é¦–ä¸ªé’ˆå¯¹ç”Ÿç‰©åŒ»å­¦å›¾åƒç†è§£çš„å¤§å‹è¯­è¨€æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡è§†è§‰æŒ‡ä»¤å¾®è°ƒå®ç°ã€‚</li>
<li>LLaDA-MedVåœ¨å¼€æ”¾å‹ç”Ÿç‰©åŒ»å­¦è§†è§‰å¯¹è¯ä»»åŠ¡ä¸­ç›¸å¯¹äºLLaVA-Medå’ŒLLaDA-Væœ‰æ›´é«˜çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>LLaDA-MedVåœ¨ä¸‰ä¸ªè§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå…·ä½“æ•°æ®ä¸ºï¼šVQA-RADçš„84.93%ï¼ŒSLAKEçš„92.31%ï¼ŒPathVQAçš„95.15%ã€‚</li>
<li>LLaDA-MedVèƒ½å¤Ÿç”Ÿæˆè¾ƒé•¿çš„å›åº”å¹¶æ§åˆ¶å“åº”é•¿åº¦ï¼Œæä¾›æ›´å¤šä¿¡æ¯è¾“å‡ºã€‚</li>
<li>æ¨¡å‹çš„å…³é”®è®­ç»ƒé˜¶æ®µåŒ…æ‹¬åˆå§‹åŒ–æƒé‡é€‰æ‹©ã€å¾®è°ƒç­–ç•¥ç­‰è¢«è¯¦ç»†åˆ†æã€‚</li>
<li>æ¨¡å‹æ¨ç†é˜¶æ®µçš„é‡‡æ ·æ­¥éª¤å’Œå“åº”é‡å¤ä¹‹é—´çš„ç›¸äº’ä½œç”¨è¢«æ·±å…¥æ¢è®¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-28f50ce6fbe5010fdd218a258d6057b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cdde17c0c7023c6476ca6684b57542b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1bc842df081978fe43604c02fe95adac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c9c5b7226586101902537bd928e01f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3a50d503fd1515247626c76e4676af3.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SDMatte-Grafting-Diffusion-Models-for-Interactive-Matting"><a href="#SDMatte-Grafting-Diffusion-Models-for-Interactive-Matting" class="headerlink" title="SDMatte: Grafting Diffusion Models for Interactive Matting"></a>SDMatte: Grafting Diffusion Models for Interactive Matting</h2><p><strong>Authors:Longfei Huang, Yu Liang, Hao Zhang, Jinwei Chen, Wei Dong, Lunde Chen, Wanyu Liu, Bo Li, Peng-Tao Jiang</strong></p>
<p>Recent interactive matting methods have shown satisfactory performance in capturing the primary regions of objects, but they fall short in extracting fine-grained details in edge regions. Diffusion models trained on billions of image-text pairs, demonstrate exceptional capability in modeling highly complex data distributions and synthesizing realistic texture details, while exhibiting robust text-driven interaction capabilities, making them an attractive solution for interactive matting. To this end, we propose SDMatte, a diffusion-driven interactive matting model, with three key contributions. First, we exploit the powerful priors of diffusion models and transform the text-driven interaction capability into visual prompt-driven interaction capability to enable interactive matting. Second, we integrate coordinate embeddings of visual prompts and opacity embeddings of target objects into U-Net, enhancing SDMatteâ€™s sensitivity to spatial position information and opacity information. Third, we propose a masked self-attention mechanism that enables the model to focus on areas specified by visual prompts, leading to better performance. Extensive experiments on multiple datasets demonstrate the superior performance of our method, validating its effectiveness in interactive matting. Our code and model are available at <a target="_blank" rel="noopener" href="https://github.com/vivoCameraResearch/SDMatte">https://github.com/vivoCameraResearch/SDMatte</a>. </p>
<blockquote>
<p>è¿‘æœŸäº¤äº’å¼æŠ å›¾æ–¹æ³•å·²ç»å±•ç°å‡ºæ•æ‰ç‰©ä½“ä¸»è¦åŒºåŸŸçš„æ»¡æ„æ€§èƒ½ï¼Œä½†åœ¨è¾¹ç¼˜åŒºåŸŸæå–ç²¾ç»†ç»†èŠ‚æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚æ‰©æ•£æ¨¡å‹ç»è¿‡æ•°åäº¿å›¾åƒæ–‡æœ¬å¯¹çš„è®­ç»ƒï¼Œåœ¨å»ºæ¨¡é«˜åº¦å¤æ‚çš„æ•°æ®åˆ†å¸ƒå’Œåˆæˆé€¼çœŸçš„çº¹ç†ç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼ŒåŒæ—¶å±•ç°å‡ºç¨³å¥çš„æ–‡æœ¬é©±åŠ¨äº¤äº’èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºäº¤äº’å¼æŠ å›¾çš„ç†æƒ³è§£å†³æ–¹æ¡ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºSDMatteï¼Œä¸€ç§åŸºäºæ‰©æ•£çš„äº¤äº’å¼æŠ å›¾æ¨¡å‹ï¼Œæœ‰ä¸‰ä¸ªä¸»è¦è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„æœ‰åŠ›å…ˆéªŒï¼Œå°†æ–‡æœ¬é©±åŠ¨çš„äº¤äº’èƒ½åŠ›è½¬åŒ–ä¸ºè§†è§‰æç¤ºé©±åŠ¨çš„äº¤äº’èƒ½åŠ›ï¼Œä»¥å®ç°äº¤äº’å¼æŠ å›¾ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†è§†è§‰æç¤ºçš„åæ ‡åµŒå…¥å’Œç›®æ ‡å¯¹è±¡çš„é€æ˜åº¦åµŒå…¥é›†æˆåˆ°U-Netä¸­ï¼Œæé«˜SDMatteå¯¹ç©ºé—´ä½ç½®ä¿¡æ¯å’Œé€æ˜åº¦ä¿¡æ¯çš„æ•æ„Ÿåº¦ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ©è†œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…³æ³¨è§†è§‰æç¤ºæŒ‡å®šçš„åŒºåŸŸï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ï¼ŒéªŒè¯äº†å…¶åœ¨äº¤äº’å¼æŠ å›¾ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/vivocameraresearch/SDMatte%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/vivoCameraResearch/SDMatteä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00443v2">PDF</a> Accepted at ICCV 2025, 11 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æå‡ºäº†ä¸€ç§åä¸ºSDMatteçš„åŸºäºæ‰©æ•£æ¨¡å‹çš„äº¤äº’å¼æŠ å›¾æ–¹æ³•ï¼Œå…·æœ‰ä¸‰å¤§è´¡çŒ®ã€‚é¦–å…ˆï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¼ºå¤§å…ˆéªŒçŸ¥è¯†ï¼Œå°†æ–‡æœ¬é©±åŠ¨äº¤äº’èƒ½åŠ›è½¬åŒ–ä¸ºè§†è§‰æç¤ºé©±åŠ¨äº¤äº’èƒ½åŠ›ï¼Œå®ç°äº¤äº’å¼æŠ å›¾ã€‚å…¶æ¬¡ï¼Œå°†è§†è§‰æç¤ºçš„åæ ‡åµŒå…¥å’Œç›®æ ‡å¯¹è±¡çš„é€æ˜åº¦åµŒå…¥æ•´åˆåˆ°U-Netä¸­ï¼Œæé«˜SDMatteå¯¹ç©ºé—´ä½ç½®ä¿¡æ¯å’Œé€æ˜åº¦ä¿¡æ¯çš„æ•æ„Ÿæ€§ã€‚æœ€åï¼Œæå‡ºäº†ä¸€ç§æ©è†œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…³æ³¨è§†è§‰æç¤ºæŒ‡å®šçš„åŒºåŸŸï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†å…¶ä¼˜è¶Šæ€§ï¼Œå±•ç¤ºäº†åœ¨äº¤äº’å¼æŠ å›¾ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å…·æœ‰å»ºæ¨¡å¤æ‚æ•°æ®åˆ†å¸ƒå’Œåˆæˆé€¼çœŸçº¹ç†ç»†èŠ‚çš„èƒ½åŠ›ï¼Œå±•ç°å‡ºåœ¨äº¤äº’å¼æŠ å›¾é¢†åŸŸçš„æ½œåŠ›ã€‚</li>
<li>SDMatteæ¨¡å‹åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œå®ç°æ–‡æœ¬é©±åŠ¨äº¤äº’èƒ½åŠ›åˆ°è§†è§‰æç¤ºé©±åŠ¨äº¤äº’èƒ½åŠ›çš„è½¬åŒ–ã€‚</li>
<li>é€šè¿‡æ•´åˆè§†è§‰æç¤ºçš„åæ ‡åµŒå…¥å’Œç›®æ ‡å¯¹è±¡çš„é€æ˜åº¦åµŒå…¥åˆ°U-Netä¸­ï¼ŒSDMatteæé«˜äº†å¯¹ç©ºé—´ä½ç½®ä¿¡æ¯å’Œé€æ˜åº¦ä¿¡æ¯çš„æ•æ„Ÿæ€§ã€‚</li>
<li>SDMatteæå‡ºçš„æ©è†œè‡ªæ³¨æ„åŠ›æœºåˆ¶èƒ½å…³æ³¨è§†è§‰æç¤ºæŒ‡å®šçš„åŒºåŸŸï¼Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</li>
<li>ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒSDMatteåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>SDMatteæ¨¡å‹çš„æœ‰æ•ˆæ€§å·²åœ¨å®è·µä¸­å¾—åˆ°éªŒè¯ï¼Œå¯åº”ç”¨äºäº¤äº’å¼æŠ å›¾ä»»åŠ¡ã€‚</li>
<li>SDMatteæ¨¡å‹çš„ä»£ç å’Œæ¨¡å‹å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/vivoCameraResearch/SDMatte%E3%80%82">https://github.com/vivoCameraResearch/SDMatteã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00443">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5c299ac451cfd72b5f217f2b256dac14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd6f9f4704e803c5d52b6bbbf3772fac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88feef2fd2525d079f43b631a707aabb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfeaf2427b1a6fbff11e3f4d65328765.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Inversion-DPO-Precise-and-Efficient-Post-Training-for-Diffusion-Models"><a href="#Inversion-DPO-Precise-and-Efficient-Post-Training-for-Diffusion-Models" class="headerlink" title="Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models"></a>Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models</h2><p><strong>Authors:Zejian Li, Yize Li, Chenye Meng, Zhongni Liu, Yang Ling, Shengyuan Zhang, Guang Yang, Changyuan Yang, Zhiyuan Yang, Lingyun Sun</strong></p>
<p>Recent advancements in diffusion models (DMs) have been propelled by alignment methods that post-train models to better conform to human preferences. However, these approaches typically require computation-intensive training of a base model and a reward model, which not only incurs substantial computational overhead but may also compromise model accuracy and training efficiency. To address these limitations, we propose Inversion-DPO, a novel alignment framework that circumvents reward modeling by reformulating Direct Preference Optimization (DPO) with DDIM inversion for DMs. Our method conducts intractable posterior sampling in Diffusion-DPO with the deterministic inversion from winning and losing samples to noise and thus derive a new post-training paradigm. This paradigm eliminates the need for auxiliary reward models or inaccurate appromixation, significantly enhancing both precision and efficiency of training. We apply Inversion-DPO to a basic task of text-to-image generation and a challenging task of compositional image generation. Extensive experiments show substantial performance improvements achieved by Inversion-DPO compared to existing post-training methods and highlight the ability of the trained generative models to generate high-fidelity compositionally coherent images. For the post-training of compostitional image geneation, we curate a paired dataset consisting of 11,140 images with complex structural annotations and comprehensive scores, designed to enhance the compositional capabilities of generative models. Inversion-DPO explores a new avenue for efficient, high-precision alignment in diffusion models, advancing their applicability to complex realistic generation tasks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/MIGHTYEZ/Inversion-DPO">https://github.com/MIGHTYEZ/Inversion-DPO</a> </p>
<blockquote>
<p>è¿‘æœŸæ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„è¿›å±•å¾—ç›Šäºé€šè¿‡å¯¹æ¨¡å‹è¿›è¡Œåè®­ç»ƒä»¥æ›´å¥½ç¬¦åˆäººç±»åå¥½çš„å¯¹é½æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦è®¡ç®—å¯†é›†çš„åŸºç¡€æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹çš„è®­ç»ƒï¼Œè¿™ä¸ä»…äº§ç”Ÿäº†å¤§é‡çš„è®¡ç®—å¼€é”€ï¼Œè¿˜å¯èƒ½å½±å“æ¨¡å‹çš„å‡†ç¡®æ€§å’Œè®­ç»ƒæ•ˆç‡ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Inversion-DPOï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¯¹é½æ¡†æ¶ï¼Œå®ƒé€šè¿‡é‡æ–°åˆ¶å®šæ‰©æ•£æ¨¡å‹çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä¸DDIMåè½¬æ¥è§„é¿å¥–åŠ±å»ºæ¨¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä»è·èƒœå’Œå¤±è´¥æ ·æœ¬åˆ°å™ªå£°çš„ç¡®å®šæ€§åè½¬æ¥è¿›è¡Œéš¾ä»¥æ‰æ‘¸çš„åéªŒé‡‡æ ·ï¼Œä»è€Œå¼€åˆ›äº†æ–°çš„åè®­ç»ƒèŒƒå¼ã€‚è¿™ç§æ–¹æ³•æ¶ˆé™¤äº†å¯¹è¾…åŠ©å¥–åŠ±æ¨¡å‹æˆ–ä¸ç²¾ç¡®è¿‘ä¼¼è®¡ç®—çš„éœ€æ±‚ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬å°†Inversion-DPOåº”ç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„åŸºæœ¬ä»»åŠ¡ä»¥åŠå…·æœ‰æŒ‘æˆ˜æ€§çš„ç»„åˆå›¾åƒç”Ÿæˆä»»åŠ¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„åè®­ç»ƒæ–¹æ³•ç›¸æ¯”ï¼ŒInversion-DPOå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶å±•ç¤ºäº†è®­ç»ƒå¥½çš„ç”Ÿæˆæ¨¡å‹åœ¨ç”Ÿæˆé«˜ä¿çœŸç»„åˆè¿è´¯å›¾åƒæ–¹é¢çš„èƒ½åŠ›ã€‚å¯¹äºç»„åˆå›¾åƒç”Ÿæˆçš„åè®­ç»ƒï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªåŒ…å«11,140å¼ å¸¦æœ‰å¤æ‚ç»“æ„æ³¨é‡Šå’Œå…¨é¢åˆ†æ•°çš„å›¾åƒé…å¯¹æ•°æ®é›†ï¼Œæ—¨åœ¨æé«˜ç”Ÿæˆæ¨¡å‹çš„ç»„åˆèƒ½åŠ›ã€‚Inversion-DPOä¸ºæ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆé«˜ç²¾åº¦å¯¹é½æ¢ç´¢äº†ä¸€æ¡æ–°é€”å¾„ï¼Œæé«˜äº†å…¶åœ¨å¤æ‚ç°å®ç”Ÿæˆä»»åŠ¡ä¸­çš„é€‚ç”¨æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MIGHTYEZ/Inversion-DPO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/MIGHTYEZ/Inversion-DPOæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11554v4">PDF</a> Accepted by ACM MM25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹çš„æ–°è¿›å±•ï¼Œæå‡ºäº†ä¸€ç§åä¸ºInversion-DPOçš„æ–°å‹å¯¹é½æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ”¹é©Direct Preference Optimization (DPO)ä¸DDIMåæ¼”æ–¹æ³•ï¼Œè§„é¿äº†å¥–åŠ±å»ºæ¨¡ï¼Œä»è€Œå®ç°äº†æ‰©æ•£æ¨¡å‹çš„åè®­ç»ƒã€‚æ–°æ–¹æ³•é€šè¿‡ä»è·èƒœå’Œå¤±è´¥æ ·æœ¬ä¸­ç¡®å®šæ€§åœ°åæ¼”å™ªå£°æ¥è¿›è¡Œä¸å¯è¡Œåé‡‡æ ·ï¼Œå¼€åˆ›äº†æ— éœ€è¾…åŠ©å¥–åŠ±æ¨¡å‹æˆ–ä¸ç²¾ç¡®è¿‘ä¼¼çš„æ–°åè®­ç»ƒèŒƒå¼ã€‚æ­¤èŒƒå¼æé«˜äº†è®­ç»ƒå’Œç²¾åº¦æ•ˆç‡ã€‚åº”ç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’ŒåŸºæœ¬ä»»åŠ¡ä»¥åŠæ›´å…·æŒ‘æˆ˜æ€§çš„ç»„åˆå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼Œå®éªŒç»“æœæ˜¾ç¤ºInversion-DPOç›¸è¾ƒäºç°æœ‰åè®­ç»ƒæ–¹æ³•å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶å±•ç¤ºäº†ç”Ÿæˆæ¨¡å‹ç”Ÿæˆé«˜ä¿çœŸç»„åˆå›¾åƒçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸ºåè®­ç»ƒç»„åˆå›¾åƒç”Ÿæˆï¼Œç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†ä¸€ä¸ªåŒ…å«å¤æ‚ç»“æ„æ³¨é‡Šå’Œè¯„åˆ†çš„ä¸€å¯¹ä¸€å›¾åƒæ•°æ®é›†ï¼Œæ—¨åœ¨å¢å¼ºç”Ÿæˆæ¨¡å‹çš„ç»„åˆèƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºæ‰©æ•£æ¨¡å‹çš„é«˜æ•ˆé«˜ç²¾åº¦å¯¹é½æ¢ç´¢äº†æ–°é€”å¾„ï¼Œæé«˜äº†å…¶åœ¨å¤æ‚ç°å®ç”Ÿæˆä»»åŠ¡ä¸­çš„åº”ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰çš„æœ€æ–°è¿›å±•é€šè¿‡å¯¹é½æ–¹æ³•æ¥æ›´å¥½åœ°ç¬¦åˆäººç±»åå¥½ã€‚</li>
<li>å½“å‰æ–¹æ³•éœ€è¦è®¡ç®—å¯†é›†çš„è®­ç»ƒåŸºç¡€æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹ï¼Œå¸¦æ¥è®¡ç®—å¼€é”€å¹¶å¯èƒ½å½±å“å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹å¯¹é½æ¡†æ¶Inversion-DPOï¼Œé€šè¿‡æ”¹é©Direct Preference Optimization (DPO)ä¸DDIMåæ¼”è§„é¿å¥–åŠ±å»ºæ¨¡ã€‚</li>
<li>Inversion-DPOå®ç°äº†æ— éœ€è¾…åŠ©å¥–åŠ±æ¨¡å‹æˆ–ä¸ç²¾ç¡®è¿‘ä¼¼çš„æ–°åè®­ç»ƒèŒƒå¼ã€‚</li>
<li>è¯¥æ–¹æ³•åº”ç”¨äºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’ŒåŸºæœ¬ä»»åŠ¡ä»¥åŠæ›´å…·æŒ‘æˆ˜æ€§çš„ç»„åˆå›¾åƒç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>Inversion-DPOæ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†ç”Ÿæˆé«˜ä¿çœŸç»„åˆå›¾åƒçš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11554">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-abe37bd42027c69ae7d649919520a765.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cea410f71a40b6a96c766d9d8f9c525c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-197a30615846b520486ff827a583fc56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-516aae4842a1f258e5d171d9ef798f74.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Text-Embedding-Knows-How-to-Quantize-Text-Guided-Diffusion-Models"><a href="#Text-Embedding-Knows-How-to-Quantize-Text-Guided-Diffusion-Models" class="headerlink" title="Text Embedding Knows How to Quantize Text-Guided Diffusion Models"></a>Text Embedding Knows How to Quantize Text-Guided Diffusion Models</h2><p><strong>Authors:Hongjae Lee, Myungjun Son, Dongjea Kang, Seung-Won Jung</strong></p>
<p>Despite the success of diffusion models in image generation tasks such as text-to-image, the enormous computational complexity of diffusion models limits their use in resource-constrained environments. To address this, network quantization has emerged as a promising solution for designing efficient diffusion models. However, existing diffusion model quantization methods do not consider input conditions, such as text prompts, as an essential source of information for quantization. In this paper, we propose a novel quantization method dubbed Quantization of Language-to-Image diffusion models using text Prompts (QLIP). QLIP leverages text prompts to guide the selection of bit precision for every layer at each time step. In addition, QLIP can be seamlessly integrated into existing quantization methods to enhance quantization efficiency. Our extensive experiments demonstrate the effectiveness of QLIP in reducing computational complexity and improving the quality of the generated images across various datasets. </p>
<blockquote>
<p>å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç­‰å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å…¶å·¨å¤§çš„è®¡ç®—å¤æ‚åº¦é™åˆ¶äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç½‘ç»œé‡åŒ–ä½œä¸ºè®¾è®¡é«˜æ•ˆæ‰©æ•£æ¨¡å‹çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆè€Œå‡ºç°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ‰©æ•£æ¨¡å‹é‡åŒ–æ–¹æ³•æ²¡æœ‰è€ƒè™‘è¾“å…¥æ¡ä»¶ï¼Œå¦‚æ–‡æœ¬æç¤ºï¼Œä½œä¸ºé‡åŒ–çš„é‡è¦ä¿¡æ¯æ¥æºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é‡åŒ–æ–¹æ³•ï¼Œç§°ä¸ºä½¿ç”¨æ–‡æœ¬æç¤ºçš„è¯­è¨€åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹é‡åŒ–ï¼ˆQLIPï¼‰ã€‚QLIPåˆ©ç”¨æ–‡æœ¬æç¤ºæ¥æŒ‡å¯¼æ¯ä¸ªæ—¶é—´æ­¥é•¿æ¯ä¸€å±‚çš„ä½ç²¾åº¦çš„é€‰æ‹©ã€‚æ­¤å¤–ï¼ŒQLIPå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„é‡åŒ–æ–¹æ³•ä¸­ï¼Œä»¥æé«˜é‡åŒ–çš„æ•ˆç‡ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒQLIPåœ¨é™ä½è®¡ç®—å¤æ‚åº¦å’Œæé«˜å„ç§æ•°æ®é›†ç”Ÿæˆå›¾åƒçš„è´¨é‡æ–¹é¢éå¸¸æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10340v3">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨ç½‘ç»œé‡åŒ–æŠ€æœ¯çš„å¸®åŠ©ä¸‹ï¼Œåœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œå°¤å…¶æ˜¯åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ‰©æ•£æ¨¡å‹é‡åŒ–æ–¹æ³•å¾€å¾€å¿½ç•¥äº†è¾“å…¥æ¡ä»¶ï¼Œå¦‚æ–‡æœ¬æç¤ºï¼Œä½œä¸ºé‡åŒ–çš„é‡è¦ä¿¡æ¯æ¥æºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é‡åŒ–æ–¹æ³•â€”â€”åˆ©ç”¨æ–‡æœ¬æç¤ºçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„é‡åŒ–ï¼ˆQLIPï¼‰ã€‚QLIPåˆ©ç”¨æ–‡æœ¬æç¤ºæŒ‡å¯¼æ¯ä¸€å±‚æ¯ä¸€æ­¥çš„ä½ç²¾åº¦é€‰æ‹©ï¼Œå¹¶å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„é‡åŒ–æ–¹æ³•ä¸­ä»¥æé«˜é‡åŒ–æ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼ŒQLIPåœ¨é™ä½è®¡ç®—å¤æ‚åº¦å’Œæé«˜ç”Ÿæˆå›¾åƒè´¨é‡æ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸï¼Œç‰¹åˆ«æ˜¯åœ¨æ–‡æœ¬åˆ°å›¾åƒçš„ä»»åŠ¡ä¸­ã€‚</li>
<li>ç½‘ç»œé‡åŒ–æ˜¯è§£å†³æ‰©æ•£æ¨¡å‹åœ¨èµ„æºå—é™ç¯å¢ƒä¸­ä½¿ç”¨çš„é‡è¦æ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ¨¡å‹é‡åŒ–æ–¹æ³•å¿½ç•¥äº†æ–‡æœ¬æç¤ºç­‰è¾“å…¥æ¡ä»¶ã€‚</li>
<li>QLIPæ˜¯ä¸€ç§æ–°çš„é‡åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨æ–‡æœ¬æç¤ºæŒ‡å¯¼æ¯ä¸€å±‚æ¯ä¸€æ­¥çš„ä½ç²¾åº¦é€‰æ‹©ã€‚</li>
<li>QLIPå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„é‡åŒ–æ–¹æ³•ä¸­ä»¥æé«˜é‡åŒ–æ•ˆç‡ã€‚</li>
<li>QLIPåœ¨é™ä½è®¡ç®—å¤æ‚åº¦å’Œæé«˜ç”Ÿæˆå›¾åƒè´¨é‡æ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10340">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe771c8e544f4bf2be8acbd1f0795ed4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e64aa29e95b9721956b28ec3eaf96ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2aa3b77751ac59de3a65f3cd6e077e15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74ec4d790da943e1abb6c91946f18c89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7270cdb683832881bd87b1ecbc6665f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3efef1a659ef791352fd6decd7a2b92a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Subjective-Camera-0-1-Bridging-Human-Cognition-and-Visual-Reconstruction-through-Sequence-Aware-Sketch-Guided-Diffusion"><a href="#Subjective-Camera-0-1-Bridging-Human-Cognition-and-Visual-Reconstruction-through-Sequence-Aware-Sketch-Guided-Diffusion" class="headerlink" title="Subjective Camera 0.1: Bridging Human Cognition and Visual   Reconstruction through Sequence-Aware Sketch-Guided Diffusion"></a>Subjective Camera 0.1: Bridging Human Cognition and Visual   Reconstruction through Sequence-Aware Sketch-Guided Diffusion</h2><p><strong>Authors:Haoyang Chen, Dongfang Sun, Caoyuan Ma, Shiqin Wang, Kewei Zhang, Zheng Wang, Zhixiang Wang</strong></p>
<p>We introduce the concept of a subjective camera to reconstruct meaningful moments that physical cameras fail to capture. We propose Subjective Camera 0.1, a framework for reconstructing real-world scenes from readily accessible subjective readouts, i.e., textual descriptions and progressively drawn rough sketches. Built on optimization-based alignment of diffusion models, our approach avoids large-scale paired training data and mitigates generalization issues. To address the challenge of integrating multiple abstract concepts in real-world scenarios, we design a Sequence-Aware Sketch-Guided Diffusion framework with three loss terms for concept-wise sequential optimization, following the natural order of subjective readouts. Experiments on two datasets demonstrate that our method achieves state-of-the-art performance in image quality as well as spatial and semantic alignment with target scenes. User studies with 40 participants further confirm that our approach is consistently preferred.Our project page is at: subjective-camera.github.io </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥ä¸»è§‚ç›¸æœºçš„æ¦‚å¿µï¼Œä»¥é‡å»ºç‰©ç†ç›¸æœºæœªèƒ½æ•æ‰çš„æœ‰æ„ä¹‰æ—¶åˆ»ã€‚æˆ‘ä»¬æå‡ºä¸»è§‚ç›¸æœº0.1ï¼Œè¿™æ˜¯ä¸€ä¸ªä»æ˜“äºè·å–çš„ä¸»è§‚è¯»æ•°ï¼ˆå¦‚æ–‡æœ¬æè¿°å’Œé€æ­¥ç»˜åˆ¶çš„ç²—ç•¥è‰å›¾ï¼‰é‡å»ºç°å®åœºæ™¯çš„æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹åœ¨åŸºäºä¼˜åŒ–çš„æ‰©æ•£æ¨¡å‹å¯¹é½ä¹‹ä¸Šï¼Œé¿å…äº†å¤§è§„æ¨¡é…å¯¹è®­ç»ƒæ•°æ®ï¼Œå¹¶ç¼“è§£äº†æ³›åŒ–é—®é¢˜ã€‚ä¸ºäº†è§£å†³åœ¨ç°å®åœºæ™¯ä¸­æ•´åˆå¤šä¸ªæŠ½è±¡æ¦‚å¿µçš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåºåˆ—æ„ŸçŸ¥è‰å›¾å¼•å¯¼æ‰©æ•£æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªç”¨äºæ¦‚å¿µçº§é¡ºåºä¼˜åŒ–çš„æŸå¤±é¡¹ï¼Œéµå¾ªä¸»è§‚è¯»æ•°çš„è‡ªç„¶é¡ºåºã€‚åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒè´¨é‡ä»¥åŠç©ºé—´å’Œæ—¶é—´ä¸ç›®æ ‡çš„åœºæ™¯è¯­ä¹‰å¯¹é½æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æœ‰40åå‚ä¸è€…å‚ä¸çš„ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æ˜¯ï¼š<a target="_blank" rel="noopener" href="http://subjective-camera.github.io/">ä¸»è§‚ç›¸æœºå®˜ç½‘</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23711v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸»è§‚ç›¸æœºæ¦‚å¿µï¼Œç”¨ä»¥é‡å»ºç‰©ç†ç›¸æœºæ— æ³•æ•æ‰çš„æœ‰æ„ä¹‰æ—¶åˆ»ã€‚æˆ‘ä»¬æå‡ºäº†ä¸»è§‚ç›¸æœº0.1æ¡†æ¶ï¼Œé€šè¿‡ä¼˜åŒ–åŸºäºæ‰©æ•£æ¨¡å‹çš„å¯¹é½æ–¹å¼ï¼Œä»æ˜“è·å–çš„ä¸»è§‚è¯»æ•°ï¼ˆå¦‚æ–‡æœ¬æè¿°å’Œç²—ç•¥è‰å›¾ï¼‰é‡å»ºç°å®åœºæ™¯ã€‚ä¸ºè§£å†³ç°å®åœºæ™¯ä¸­æ•´åˆå¤šä¸ªæŠ½è±¡æ¦‚å¿µçš„é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†åºåˆ—æ„ŸçŸ¥è‰å›¾å¼•å¯¼æ‰©æ•£æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªç”¨äºæ¦‚å¿µçº§é¡ºåºä¼˜åŒ–çš„æŸå¤±é¡¹ï¼Œéµå¾ªä¸»è§‚è¯»æ•°çš„è‡ªç„¶é¡ºåºã€‚åœ¨ä¸¤é¡¹æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒè´¨é‡ä»¥åŠç©ºé—´ä¸è¯­ä¹‰å¯¹é½ç›®æ ‡åœºæ™¯æ–¹é¢è¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚40åå‚ä¸è€…çš„ç”¨æˆ·ç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥ä¸»è§‚ç›¸æœºæ¦‚å¿µï¼Œæ—¨åœ¨æ•æ‰ç‰©ç†ç›¸æœºæ— æ³•æ•æ‰çš„æœ‰æ„ä¹‰æ—¶åˆ»ã€‚</li>
<li>æå‡ºSubjective Camera 0.1æ¡†æ¶ï¼Œåˆ©ç”¨æ–‡æœ¬æè¿°å’Œç²—ç•¥è‰å›¾é‡å»ºç°å®åœºæ™¯ã€‚</li>
<li>åŸºäºä¼˜åŒ–å¯¹é½çš„æ‰©æ•£æ¨¡å‹ï¼Œæ— éœ€å¤§è§„æ¨¡é…å¯¹è®­ç»ƒæ•°æ®ï¼Œå‡è½»æ³›åŒ–é—®é¢˜ã€‚</li>
<li>è®¾è®¡åºåˆ—æ„ŸçŸ¥è‰å›¾å¼•å¯¼æ‰©æ•£æ¡†æ¶ï¼Œé€šè¿‡ä¸‰ä¸ªæŸå¤±é¡¹è¿›è¡Œæ¦‚å¿µçº§é¡ºåºä¼˜åŒ–ã€‚</li>
<li>æ¡†æ¶éµå¾ªä¸»è§‚è¯»æ•°çš„è‡ªç„¶é¡ºåºï¼Œèƒ½æ›´å‡†ç¡®åœ°é‡å»ºåœºæ™¯ã€‚</li>
<li>åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒè´¨é‡åŠç©ºé—´ã€è¯­ä¹‰å¯¹é½æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ç”¨æˆ·ç ”ç©¶è¯å®è¯¥æ–¹æ³•çš„ä¸€è‡´ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23711">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-46c533685f4ea60e686461646b060e85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-273edf3397d44f1bd6368dabdd328f5b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05946393eab0194835ba32079040353e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7541b8eec6eaf4332230c5985404242c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5786d8cc3d1b7ddced0faa54d6838377.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DIP-Unsupervised-Dense-In-Context-Post-training-of-Visual-Representations"><a href="#DIP-Unsupervised-Dense-In-Context-Post-training-of-Visual-Representations" class="headerlink" title="DIP: Unsupervised Dense In-Context Post-training of Visual   Representations"></a>DIP: Unsupervised Dense In-Context Post-training of Visual   Representations</h2><p><strong>Authors:Sophia Sirko-Galouchenko, Spyros Gidaris, Antonin Vobecky, Andrei Bursuc, Nicolas Thome</strong></p>
<p>We introduce DIP, a novel unsupervised post-training method designed to enhance dense image representations in large-scale pretrained vision encoders for in-context scene understanding. Unlike prior approaches that rely on complex self-distillation architectures, our method trains the vision encoder using pseudo-tasks that explicitly simulate downstream in-context scenarios, inspired by meta-learning principles. To enable post-training on unlabeled data, we propose an automatic mechanism for generating in-context tasks that combines a pretrained diffusion model and the vision encoder itself. DIP is simple, unsupervised, and computationally efficient, requiring less than 9 hours on a single A100 GPU. By learning dense representations through pseudo in-context tasks, it achieves strong performance across a wide variety of downstream real-world in-context scene understanding tasks. It outperforms both the initial vision encoder and prior methods, offering a practical and effective solution for improving dense representations. Code available here: <a target="_blank" rel="noopener" href="https://github.com/sirkosophia/DIP">https://github.com/sirkosophia/DIP</a> </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†DIPï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ— ç›‘ç£åè®­ç»ƒæ³•ï¼Œæ—¨åœ¨å¢å¼ºå¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨çš„å¯†é›†å›¾åƒè¡¨ç¤ºï¼Œä»¥è¿›è¡Œä¸Šä¸‹æ–‡åœºæ™¯ç†è§£ã€‚ä¸åŒäºä¾èµ–å¤æ‚è‡ªæˆ‘è’¸é¦æ¶æ„çš„å…ˆå‰æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨ä¼ªä»»åŠ¡æ¥æ˜ç¡®æ¨¡æ‹Ÿä¸‹æ¸¸ä¸Šä¸‹æ–‡åœºæ™¯ï¼Œè¿™æ˜¯å—å…ƒå­¦ä¹ åŸç†çš„å¯å‘ã€‚ä¸ºäº†åœ¨æ— æ ‡ç­¾æ•°æ®ä¸Šè¿›è¡Œåè®­ç»ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”Ÿæˆä¸Šä¸‹æ–‡ä»»åŠ¡çš„è‡ªåŠ¨æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ç»“åˆäº†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’Œè§†è§‰ç¼–ç å™¨æœ¬èº«ã€‚DIPç®€å•ã€æ— ç›‘ç£ä¸”è®¡ç®—é«˜æ•ˆï¼Œåœ¨å•ä¸ªA100 GPUä¸Šä¸åˆ°9å°æ—¶å³å¯å®Œæˆã€‚å®ƒé€šè¿‡ä¼ªä¸Šä¸‹æ–‡ä»»åŠ¡å­¦ä¹ å¯†é›†è¡¨ç¤ºï¼Œåœ¨å¤šç§ä¸‹æ¸¸ç°å®ä¸–ç•Œä¸Šä¸‹æ–‡åœºæ™¯ç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚å®ƒä¼˜äºåˆå§‹çš„è§†è§‰ç¼–ç å™¨å’Œå…ˆå‰çš„æ–¹æ³•ï¼Œä¸ºæ”¹è¿›å¯†é›†è¡¨ç¤ºæä¾›äº†å®ç”¨æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sirkosophia/DIP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sirkosophia/DIPæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18463v2">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†DIPï¼Œä¸€ç§æ–°å‹æ— ç›‘ç£çš„åè®­ç»ƒæŠ€æœ¯ï¼Œæ—¨åœ¨å¢å¼ºå¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨çš„å¯†é›†å›¾åƒè¡¨ç¤ºï¼Œä»¥è¿›è¡Œä¸Šä¸‹æ–‡åœºæ™¯ç†è§£ã€‚DIPé‡‡ç”¨ä¼ªä»»åŠ¡æ¨¡æ‹Ÿä¸‹æ¸¸ä¸Šä¸‹æ–‡åœºæ™¯ï¼Œå—å…ƒå­¦ä¹ åŸç†å¯å‘ã€‚é€šè¿‡é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’Œè§†è§‰ç¼–ç å™¨æœ¬èº«ç”Ÿæˆä¸Šä¸‹æ–‡ä»»åŠ¡ï¼Œå®ç°æ— ç›‘ç£æ•°æ®åè®­ç»ƒã€‚DIPç®€å•ã€é«˜æ•ˆï¼Œå¯åœ¨å•ä¸ªA100 GPUä¸Šä¸åˆ°9å°æ—¶å†…å®Œæˆã€‚é€šè¿‡ä¼ªä¸Šä¸‹æ–‡ä»»åŠ¡å­¦ä¹ å¯†é›†è¡¨ç¤ºï¼Œåœ¨å¤šç§ä¸‹æ¸¸ç°å®åœºæ™¯ç†è§£ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DIPæ˜¯ä¸€ç§æ— ç›‘ç£åè®­ç»ƒæŠ€æœ¯ï¼Œç”¨äºå¢å¼ºé¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨çš„å›¾åƒè¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>DIPé€šè¿‡æ¨¡æ‹Ÿä¸‹æ¸¸ä¸Šä¸‹æ–‡åœºæ™¯ç”Ÿæˆä¼ªä»»åŠ¡ï¼Œä»è€Œå¢å¼ºè§†è§‰ç¼–ç å™¨çš„æ€§èƒ½ã€‚</li>
<li>DIPé‡‡ç”¨å…ƒå­¦ä¹ åŸç†ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’Œè§†è§‰ç¼–ç å™¨ç”Ÿæˆä¸Šä¸‹æ–‡ä»»åŠ¡ã€‚</li>
<li>DIPåœ¨å•ä¸ªA100 GPUä¸Šè®¡ç®—æ•ˆç‡é«˜ï¼Œè®­ç»ƒæ—¶é—´çŸ­ã€‚</li>
<li>DIPåœ¨å¤šç§ä¸‹æ¸¸ç°å®åœºæ™¯ç†è§£ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºåˆå§‹è§†è§‰ç¼–ç å™¨å’Œå…ˆå‰æ–¹æ³•ã€‚</li>
<li>DIPé€‚ç”¨äºæ”¹å–„å¯†é›†è¡¨ç¤ºï¼Œä¸ºåœºæ™¯ç†è§£æä¾›äº†ä¸€ç§å®ç”¨æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18463">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-686437810a21a2d926ca209b4c8c2986.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecda420d1beeeb84656ca8879d4dab5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c444de8bbf8e99e5a3990aec6a54c98e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9170775b531b262f2d0192e97ac782e2.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DanceGRPO-Unleashing-GRPO-on-Visual-Generation"><a href="#DanceGRPO-Unleashing-GRPO-on-Visual-Generation" class="headerlink" title="DanceGRPO: Unleashing GRPO on Visual Generation"></a>DanceGRPO: Unleashing GRPO on Visual Generation</h2><p><strong>Authors:Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, Ping Luo</strong></p>
<p>Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face critical limitations: incompatibility with modern Ordinary Differential Equations (ODEs)-based sampling paradigms, instability in large-scale training, and lack of validation for video generation. This paper introduces DanceGRPO, the first unified framework to adapt Group Relative Policy Optimization (GRPO) to visual generation paradigms, unleashing one unified RL algorithm across two generative paradigms (diffusion models and rectified flows), three tasks (text-to-image, text-to-video, image-to-video), four foundation models (Stable Diffusion, HunyuanVideo, FLUX, SkyReels-I2V), and five reward models (image&#x2F;video aesthetics, text-image alignment, video motion quality, and binary reward). To our knowledge, DanceGRPO is the first RL-based unified framework capable of seamless adaptation across diverse generative paradigms, tasks, foundational models, and reward models. DanceGRPO demonstrates consistent and substantial improvements, which outperform baselines by up to 181% on benchmarks such as HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can stabilize policy optimization for complex video generation, but also enables generative policy to better capture denoising trajectories for Best-of-N inference scaling and learn from sparse binary feedback. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis. The code will be released. </p>
<blockquote>
<p>è¿‘æœŸç”Ÿæˆæ¨¡å‹â€”â€”å°¤å…¶æ˜¯æ‰©æ•£æ¨¡å‹å’Œä¿®æ­£æµâ€”â€”çš„çªç ´ä¸ºè§†è§‰å†…å®¹åˆ›ä½œå¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ï¼Œä½†å¦‚ä½•ä½¿æ¨¡å‹è¾“å‡ºç¬¦åˆäººç±»åå¥½ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è§†è§‰ç”Ÿæˆæ–¹æ³•é¢ä¸´é‡è¦å±€é™ï¼šä¸ç°ä»£åŸºäºå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰çš„é‡‡æ ·èŒƒå¼ä¸å…¼å®¹ã€å¤§è§„æ¨¡è®­ç»ƒä¸ç¨³å®šã€ä»¥åŠè§†é¢‘ç”Ÿæˆçš„éªŒè¯ç¼ºä¹ã€‚æœ¬æ–‡ä»‹ç»äº†DanceGRPOï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå°†é›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰é€‚åº”äºè§†è§‰ç”ŸæˆèŒƒå¼çš„ç»Ÿä¸€æ¡†æ¶ï¼Œé‡Šæ”¾äº†ä¸€ç§ç»Ÿä¸€çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè¯¥ç®—æ³•æ¶µç›–äº†ä¸¤ç§ç”ŸæˆèŒƒå¼ï¼ˆæ‰©æ•£æ¨¡å‹å’Œä¿®æ­£æµï¼‰ã€ä¸‰ç§ä»»åŠ¡ï¼ˆæ–‡æœ¬åˆ°å›¾åƒã€æ–‡æœ¬åˆ°è§†é¢‘ã€å›¾åƒåˆ°è§†é¢‘ï¼‰ã€å››ç§åŸºç¡€æ¨¡å‹ï¼ˆStable Diffusionã€HuYuanVideoã€FLUXã€SkyReels-I2Vï¼‰ï¼Œä»¥åŠäº”ç§å¥–åŠ±æ¨¡å‹ï¼ˆå›¾åƒ&#x2F;è§†é¢‘ç¾å­¦ã€æ–‡æœ¬-å›¾åƒå¯¹é½ã€è§†é¢‘è¿åŠ¨è´¨é‡ã€äºŒå…ƒå¥–åŠ±ï¼‰ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒDanceGRPOæ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿåœ¨å¤šç§ç”ŸæˆèŒƒå¼ã€ä»»åŠ¡ã€åŸºç¡€æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹ä¹‹é—´æ— ç¼é€‚åº”çš„åŸºäºå¼ºåŒ–å­¦ä¹ çš„ç»Ÿä¸€æ¡†æ¶ã€‚DanceGRPOè¡¨ç°å‡ºæŒç»­ä¸”æ˜¾è‘—çš„æ”¹è¿›ï¼Œåœ¨HPS-v2.1ã€CLIP Scoreã€VideoAlignå’ŒGenEvalç­‰åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ä¼˜äºåŸºçº¿é«˜è¾¾181%ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒDanceGRPOä¸ä»…èƒ½å¤Ÿç¨³å®šå¤æ‚è§†é¢‘ç”Ÿæˆçš„ç­–ç•¥ä¼˜åŒ–ï¼Œè¿˜èƒ½å¤Ÿä½¿ç”Ÿæˆç­–ç•¥æ›´å¥½åœ°æ•æ‰å»å™ªè½¨è¿¹ï¼Œç”¨äºBest-of-Næ¨ç†æ‰©å±•ï¼Œå¹¶ä»ç¨€ç–çš„äºŒå…ƒåé¦ˆä¸­å­¦ä¹ ã€‚æˆ‘ä»¬çš„ç»“æœç¡®ç«‹äº†DanceGRPOåœ¨è§†è§‰ç”Ÿæˆä¸­æ‰©å±•å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä»»åŠ¡çš„ç¨³å¥æ€§å’Œé€šç”¨æ€§è§£å†³æ–¹æ¡ˆï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ä¸åˆæˆè§†è§‰çš„å’Œè°èåˆæä¾›äº†æ–°çš„è§è§£ã€‚ä»£ç å°†è¢«å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07818v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://dancegrpo.github.io/">https://dancegrpo.github.io/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¿‘æœŸç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹å’Œæ ¡æ­£æµï¼‰çš„çªç ´ä¸ºè§†è§‰å†…å®¹åˆ›ä½œå¸¦æ¥é©å‘½æ€§å˜åŒ–ï¼Œä½†å¦‚ä½•ä½¿æ¨¡å‹è¾“å‡ºä¸äººç±»åå¥½å¯¹é½ä»æ˜¯å…³é”®æŒ‘æˆ˜ã€‚ç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„è§†è§‰ç”Ÿæˆæ–¹æ³•å­˜åœ¨ä¸ç°ä»£åŸºäºå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEsï¼‰çš„é‡‡æ ·èŒƒå¼ä¸å…¼å®¹ã€å¤§è§„æ¨¡è®­ç»ƒä¸ç¨³å®šã€è§†é¢‘ç”ŸæˆéªŒè¯ç¼ºä¹ç­‰é—®é¢˜ã€‚æœ¬æ–‡å¼•å…¥DanceGRPOæ¡†æ¶ï¼Œé¦–æ¬¡å°†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰é€‚åº”äºè§†è§‰ç”ŸæˆèŒƒå¼ï¼Œå®ç°ç»Ÿä¸€RLç®—æ³•åœ¨ä¸¤ç§ç”ŸæˆèŒƒå¼ï¼ˆæ‰©æ•£æ¨¡å‹å’Œæ ¡æ­£æµï¼‰ã€ä¸‰ç§ä»»åŠ¡ï¼ˆæ–‡æœ¬è½¬å›¾åƒã€æ–‡æœ¬è½¬è§†é¢‘ã€å›¾åƒè½¬è§†é¢‘ï¼‰ã€å››ç§åŸºç¡€æ¨¡å‹ï¼ˆStable Diffusionã€HunyuanVideoç­‰ï¼‰å’Œäº”ç§å¥–åŠ±æ¨¡å‹ä¸­çš„çµæ´»åº”ç”¨ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒDanceGRPOæ˜¯é¦–ä¸ªèƒ½å¤Ÿæ— ç¼é€‚åº”å¤šç§ç”ŸæˆèŒƒå¼ã€ä»»åŠ¡ã€åŸºç¡€æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹çš„RLç»Ÿä¸€æ¡†æ¶ã€‚DanceGRPOåœ¨HPS-v2.1ç­‰åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸åŸºçº¿ç›¸æ¯”æœ€é«˜è¾¾181%çš„æŒç»­å’Œå®è´¨æ€§æ”¹è¿›ï¼ŒåŒæ—¶æ˜¾è‘—æå‡äº†è§†é¢‘ç”Ÿæˆçš„ç­–ç•¥ä¼˜åŒ–ç¨³å®šæ€§ï¼Œå¹¶èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å»å™ªè½¨è¿¹ç”¨äºBest-of-Næ¨ç†æ‰©å±•å’Œä»ç¨€ç–äºŒå…ƒåé¦ˆä¸­å­¦ä¹ ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒDanceGRPOä¸ºå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ä»»åŠ¡åœ¨è§†è§‰ç”Ÿæˆæ–¹é¢çš„åº”ç”¨æä¾›äº†ç¨³å¥ä¸”é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºå¼ºåŒ–å­¦ä¹ ä¸è§†è§‰åˆæˆçš„å’Œè°èåˆæä¾›äº†æ–°çš„è§è§£ã€‚ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>çªç ´æ€§çš„ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹å’Œæ ¡æ­£æµï¼‰åœ¨è§†è§‰å†…å®¹åˆ›ä½œé¢†åŸŸå…·æœ‰é©å‘½æ€§å½±å“ã€‚</li>
<li>æ¨¡å‹è¾“å‡ºä¸äººç±»åå¥½å¯¹é½ä»æ˜¯å½“å‰çš„é‡è¦æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰åŸºäºå¼ºåŒ–å­¦ä¹ çš„è§†è§‰ç”Ÿæˆæ–¹æ³•é¢ä¸´å¤šç§æŒ‘æˆ˜ï¼Œå¦‚ä¸ç°ä»£é‡‡æ ·èŒƒå¼ä¸å…¼å®¹ã€è®­ç»ƒä¸ç¨³å®šã€è§†é¢‘ç”ŸæˆéªŒè¯ç¼ºä¹ç­‰ã€‚</li>
<li>å¼•å…¥DanceGRPOæ¡†æ¶ï¼Œé¦–æ¬¡å®ç°ç»Ÿä¸€RLç®—æ³•åœ¨å¤šç§ç”ŸæˆèŒƒå¼ã€ä»»åŠ¡ã€åŸºç¡€æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚</li>
<li>DanceGRPOå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œå¹¶åœ¨è§†é¢‘ç”Ÿæˆçš„æ”¿ç­–ä¼˜åŒ–ç¨³å®šæ€§æ–¹é¢æœ‰æ‰€æå‡ã€‚</li>
<li>DanceGRPOèƒ½å¤Ÿæ•æ‰å»å™ªè½¨è¿¹ç”¨äºBest-of-Næ¨ç†æ‰©å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07818">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-54a33747f280eac8850e1f5b661270a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8281f31d36f5e8180d43b3673a688594.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf339ab349d00a486a97d8d64e6ec85c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DRC-Enhancing-Personalized-Image-Generation-via-Disentangled-Representation-Composition"><a href="#DRC-Enhancing-Personalized-Image-Generation-via-Disentangled-Representation-Composition" class="headerlink" title="DRC: Enhancing Personalized Image Generation via Disentangled   Representation Composition"></a>DRC: Enhancing Personalized Image Generation via Disentangled   Representation Composition</h2><p><strong>Authors:Yiyan Xu, Wuqiang Zheng, Wenjie Wang, Fengbin Zhu, Xinting Hu, Yang Zhang, Fuli Feng, Tat-Seng Chua</strong></p>
<p>Personalized image generation has emerged as a promising direction in multimodal content creation. It aims to synthesize images tailored to individual style preferences (e.g., color schemes, character appearances, layout) and semantic intentions (e.g., emotion, action, scene contexts) by leveraging user-interacted history images and multimodal instructions. Despite notable progress, existing methods â€“ whether based on diffusion models, large language models, or Large Multimodal Models (LMMs) â€“ struggle to accurately capture and fuse user style preferences and semantic intentions. In particular, the state-of-the-art LMM-based method suffers from the entanglement of visual features, leading to Guidance Collapse, where the generated images fail to preserve user-preferred styles or reflect the specified semantics.   To address these limitations, we introduce DRC, a novel personalized image generation framework that enhances LMMs through Disentangled Representation Composition. DRC explicitly extracts user style preferences and semantic intentions from history images and the reference image, respectively, to form user-specific latent instructions that guide image generation within LMMs. Specifically, it involves two critical learning stages: 1) Disentanglement learning, which employs a dual-tower disentangler to explicitly separate style and semantic features, optimized via a reconstruction-driven paradigm with difficulty-aware importance sampling; and 2) Personalized modeling, which applies semantic-preserving augmentations to effectively adapt the disentangled representations for robust personalized generation. Extensive experiments on two benchmarks demonstrate that DRC shows competitive performance while effectively mitigating the guidance collapse issue, underscoring the importance of disentangled representation learning for controllable and effective personalized image generation. </p>
<blockquote>
<p>ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆå·²æˆä¸ºå¤šåª’ä½“å†…å®¹åˆ›å»ºçš„ä¸€ä¸ªå‰æ™¯å¹¿é˜”çš„æ–¹å‘ã€‚å®ƒæ—¨åœ¨é€šè¿‡åˆ©ç”¨ç”¨æˆ·äº¤äº’çš„å†å²å›¾åƒå’Œå¤šæ¨¡æ€æŒ‡ä»¤ï¼Œåˆæˆç¬¦åˆä¸ªäººé£æ ¼åå¥½ï¼ˆå¦‚è‰²å½©æ–¹æ¡ˆã€è§’è‰²å¤–è§‚ã€å¸ƒå±€ï¼‰å’Œè¯­ä¹‰æ„å›¾ï¼ˆå¦‚æƒ…æ„Ÿã€åŠ¨ä½œã€åœºæ™¯ä¸Šä¸‹æ–‡ï¼‰çš„å›¾åƒã€‚å°½ç®¡å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•â€”â€”æ— è®ºæ˜¯åŸºäºæ‰©æ•£æ¨¡å‹ã€å¤§å‹è¯­è¨€æ¨¡å‹è¿˜æ˜¯å¤§å‹å¤šåª’ä½“æ¨¡å‹ï¼ˆLMMï¼‰â€”â€”åœ¨å‡†ç¡®æ•æ‰å’Œèåˆç”¨æˆ·é£æ ¼åå¥½å’Œè¯­ä¹‰æ„å›¾æ–¹é¢éƒ½å­˜åœ¨å›°éš¾ã€‚ç‰¹åˆ«æ˜¯æœ€å…ˆè¿›çš„åŸºäºLMMçš„æ–¹æ³•å—åˆ°è§†è§‰ç‰¹å¾çº ç¼ çš„å½±å“ï¼Œå¯¼è‡´å‡ºç°â€œæŒ‡å¯¼å´©æºƒâ€ï¼Œç”Ÿæˆçš„å›¾åƒæ— æ³•ä¿æŒç”¨æˆ·åå¥½çš„é£æ ¼æˆ–åæ˜ æŒ‡å®šçš„è¯­ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17349v2">PDF</a> Accepted for publication in ACM MMâ€™25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆé¢†åŸŸçš„ç ”ç©¶è¿›å±•ï¼Œæå‡ºä¸€ç§åŸºäºè§£çº ç¼ è¡¨ç¤ºç»„åˆï¼ˆDRCï¼‰çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆæ–¹é¢çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡æ˜ç¡®æå–ç”¨æˆ·é£æ ¼åå¥½å’Œè¯­ä¹‰æ„å›¾ï¼Œå½¢æˆç”¨æˆ·ç‰¹å®šçš„æ½œåœ¨æŒ‡ä»¤ï¼ŒæŒ‡å¯¼å›¾åƒåœ¨LMMså†…çš„ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸ªäººåŒ–å›¾åƒç”Ÿæˆæˆä¸ºå¤šæ¨¡æ€å†…å®¹åˆ›å»ºçš„æœ‰å‰é€”çš„æ–¹å‘ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾ˆéš¾æ•æ‰å’Œèåˆç”¨æˆ·çš„é£æ ¼åå¥½å’Œè¯­ä¹‰æ„å›¾ã€‚</li>
<li>å½“å‰é¢†å…ˆçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰é¢ä¸´è§†è§‰ç‰¹å¾çº ç¼ çš„é—®é¢˜ï¼Œå¯¼è‡´ç”Ÿæˆçš„å›¾åƒæ— æ³•ä¿æŒç”¨æˆ·åå¥½çš„é£æ ¼æˆ–åæ˜ æŒ‡å®šçš„è¯­ä¹‰ã€‚</li>
<li>DRCæ¡†æ¶æ—¨åœ¨é€šè¿‡è§£çº ç¼ è¡¨ç¤ºç»„åˆå¢å¼ºLMMsåœ¨ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆæ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>DRCæ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªå…³é”®å­¦ä¹ é˜¶æ®µï¼šè§£çº ç¼ å­¦ä¹ å’Œä¸ªæ€§åŒ–å»ºæ¨¡ã€‚</li>
<li>è§£çº ç¼ å­¦ä¹ é˜¶æ®µé‡‡ç”¨åŒå¡”è§£ç¼ å™¨æ˜ç¡®åˆ†ç¦»é£æ ¼ä¸è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶é€šè¿‡é‡å»ºé©±åŠ¨èŒƒå¼å’Œä¼˜åŒ–éš¾åº¦æ„ŸçŸ¥é‡è¦æ€§é‡‡æ ·è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>ä¸ªæ€§åŒ–å»ºæ¨¡é˜¶æ®µåº”ç”¨è¯­ä¹‰ä¿ç•™å¢å¼ºï¼Œæœ‰æ•ˆé€‚åº”è§£ç¼ è¡¨ç¤ºï¼Œä»¥å®ç°ç¨³å¥çš„ä¸ªæ€§åŒ–ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17349">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e5b91d6fce2cd7856556e893d811b646.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30503c65c3aa70b9d75ba96538a9ddc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b607c4390285eae583691cc79a5e4413.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6be1655f3388411430c87c11e02910a7.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Efficient-Input-level-Backdoor-Defense-on-Text-to-Image-Synthesis-via-Neuron-Activation-Variation"><a href="#Efficient-Input-level-Backdoor-Defense-on-Text-to-Image-Synthesis-via-Neuron-Activation-Variation" class="headerlink" title="Efficient Input-level Backdoor Defense on Text-to-Image Synthesis via   Neuron Activation Variation"></a>Efficient Input-level Backdoor Defense on Text-to-Image Synthesis via   Neuron Activation Variation</h2><p><strong>Authors:Shengfang Zhai, Jiajun Li, Yue Liu, Huanran Chen, Zhihua Tian, Wenjie Qu, Qingni Shen, Ruoxi Jia, Yinpeng Dong, Jiaheng Zhang</strong></p>
<p>In recent years, text-to-image (T2I) diffusion models have gained significant attention for their ability to generate high quality images reflecting text prompts. However, their growing popularity has also led to the emergence of backdoor threats, posing substantial risks. Currently, effective defense strategies against such threats are lacking due to the diversity of backdoor targets in T2I synthesis. In this paper, we propose NaviT2I, an efficient input-level backdoor defense framework against diverse T2I backdoors. Our approach is based on the new observation that trigger tokens tend to induce significant neuron activation variation in the early stage of the diffusion generation process, a phenomenon we term Early-step Activation Variation. Leveraging this insight, NaviT2I navigates T2I models to prevent malicious inputs by analyzing Neuron activation variations caused by input tokens. Extensive experiments show that NaviT2I significantly outperforms the baselines in both effectiveness and efficiency across diverse datasets, various T2I backdoors, and different model architectures including UNet and DiT. Furthermore, we show that our method remains effective under potential adaptive attacks. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹å› å…¶èƒ½å¤Ÿç”Ÿæˆåæ˜ æ–‡æœ¬æç¤ºçš„é«˜è´¨é‡å›¾åƒè€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œå…¶æ—¥ç›Šæ™®åŠä¹Ÿå¸¦æ¥äº†åé—¨å¨èƒï¼Œå¸¦æ¥äº†ç›¸å½“å¤§çš„é£é™©ã€‚ç”±äºT2Iåˆæˆä¸­çš„åé—¨ç›®æ ‡å¤šæ ·æ€§ï¼Œç›®å‰ç¼ºä¹é’ˆå¯¹æ­¤ç±»å¨èƒçš„æœ‰æ•ˆé˜²å¾¡ç­–ç•¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†NaviT2Iï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹å¤šç§T2Iåé—¨çš„é«˜æ•ˆè¾“å…¥çº§åé—¨é˜²å¾¡æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºä¸€ä¸ªæ–°çš„è§‚å¯Ÿç»“æœï¼Œå³è§¦å‘ä»¤ç‰Œå¾€å¾€ä¼šåœ¨æ‰©æ•£ç”Ÿæˆè¿‡ç¨‹çš„æ—©æœŸé˜¶æ®µå¼•èµ·ç¥ç»å…ƒæ¿€æ´»çš„æ˜¾è‘—å˜åŒ–ï¼Œæˆ‘ä»¬å°†è¿™ä¸€ç°è±¡ç§°ä¸ºâ€œæ—©æœŸæ­¥éª¤æ¿€æ´»å˜åŒ–â€ã€‚åˆ©ç”¨è¿™ä¸€è§è§£ï¼ŒNaviT2Ié€šè¿‡åˆ†æè¾“å…¥ä»¤ç‰Œå¼•èµ·çš„ç¥ç»å…ƒæ¿€æ´»å˜åŒ–ï¼Œå¼•å¯¼T2Iæ¨¡å‹é˜²æ­¢æ¶æ„è¾“å…¥ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒNaviT2Iåœ¨å¤šä¸ªæ•°æ®é›†ã€å¤šç§T2Iåé—¨å’Œä¸åŒæ¨¡å‹æ¶æ„ï¼ˆåŒ…æ‹¬UNetå’ŒDiTï¼‰æ–¹é¢ï¼Œåœ¨æœ‰æ•ˆæ€§å’Œæ•ˆç‡æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºåŸºçº¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ½œåœ¨çš„é€‚åº”æ€§æ”»å‡»ä¸‹ä»ç„¶æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06453v2">PDF</a> 20 pages. ICCV 2025 (Highlight)</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œä½†è¿‘æœŸå‡ºç°çš„åé—¨å¨èƒç»™å…¶å¸¦æ¥äº†å·¨å¤§é£é™©ã€‚ç”±äºç¼ºä¹é’ˆå¯¹T2Iåˆæˆä¸­åé—¨ç›®æ ‡çš„é˜²å¾¡ç­–ç•¥ï¼Œæœ¬æ–‡æå‡ºäº†NaviT2Iï¼Œä¸€ä¸ªé«˜æ•ˆçš„è¾“å…¥çº§åé—¨é˜²å¾¡æ¡†æ¶ã€‚NaviT2IåŸºäºæ–°è§‚å¯Ÿï¼Œè§¦å‘ä»¤ç‰Œåœ¨æ‰©æ•£ç”Ÿæˆè¿‡ç¨‹çš„æ—©æœŸé˜¶æ®µä¼šå¼•èµ·ç¥ç»å…ƒæ¿€æ´»å˜åŒ–æ˜¾è‘—ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºæ—©æœŸæ¿€æ´»å˜åŒ–ç°è±¡ã€‚åˆ©ç”¨è¿™ä¸€æ´å¯Ÿï¼ŒNaviT2Ié€šè¿‡åˆ†æè¾“å…¥ä»¤ç‰Œå¼•èµ·çš„ç¥ç»å…ƒæ¿€æ´»å˜åŒ–æ¥é˜²æ­¢æ¶æ„è¾“å…¥ã€‚å®éªŒè¡¨æ˜ï¼ŒNaviT2Iåœ¨å¤šä¸ªæ•°æ®é›†ã€å„ç§T2Iåé—¨ä»¥åŠä¸åŒæ¨¡å‹æ¶æ„ï¼ˆåŒ…æ‹¬UNetå’ŒDiTï¼‰ä¸Šï¼Œåœ¨æœ‰æ•ˆæ€§å’Œæ•ˆç‡æ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ½œåœ¨çš„è‡ªé€‚åº”æ”»å‡»ä¸‹ä»ç„¶ä¿æŒæœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2Iæ‰©æ•£æ¨¡å‹å› èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆé«˜è´¨é‡å›¾åƒè€Œå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†ä¹Ÿå­˜åœ¨åé—¨å¨èƒé£é™©ã€‚</li>
<li>ç¼ºä¹é’ˆå¯¹T2Iåˆæˆä¸­å¤šæ ·åé—¨ç›®æ ‡çš„é˜²å¾¡ç­–ç•¥ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†NaviT2Iï¼Œä¸€ä¸ªé«˜æ•ˆçš„è¾“å…¥çº§åé—¨é˜²å¾¡æ¡†æ¶ï¼Œé’ˆå¯¹T2Iæ¨¡å‹è¿›è¡Œé˜²å¾¡ã€‚</li>
<li>NaviT2IåŸºäºæ–°è§‚å¯Ÿï¼šè§¦å‘ä»¤ç‰Œåœ¨æ‰©æ•£æ¨¡å‹çš„æ—©æœŸé˜¶æ®µä¼šå¼•èµ·ç¥ç»å…ƒæ¿€æ´»æ˜¾è‘—å˜åŒ–ã€‚</li>
<li>NaviT2Ié€šè¿‡åˆ†æè¾“å…¥ä»¤ç‰Œå¼•èµ·çš„ç¥ç»å…ƒæ¿€æ´»å˜åŒ–æ¥é˜²æ­¢æ¶æ„è¾“å…¥ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒNaviT2Iåœ¨å¤šä¸ªæ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼ŒåŒ…æ‹¬æ•°æ®é›†ã€T2Iåé—¨ç±»å‹ã€æ¨¡å‹æ¶æ„ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06453">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0add7bafaf95b3c486493ce0482f9b8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c612872c4cc38bbd8d54e5775cf2b4aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb8441ab50526dbaa1a9d5ff1b629b41.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GAS-Generative-Avatar-Synthesis-from-a-Single-Image"><a href="#GAS-Generative-Avatar-Synthesis-from-a-Single-Image" class="headerlink" title="GAS: Generative Avatar Synthesis from a Single Image"></a>GAS: Generative Avatar Synthesis from a Single Image</h2><p><strong>Authors:Yixing Lu, Junting Dong, Youngjoong Kwon, Qin Zhao, Bo Dai, Fernando De la Torre</strong></p>
<p>We present a unified and generalizable framework for synthesizing view-consistent and temporally coherent avatars from a single image, addressing the challenging task of single-image avatar generation. Existing diffusion-based methods often condition on sparse human templates (e.g., depth or normal maps), which leads to multi-view and temporal inconsistencies due to the mismatch between these signals and the true appearance of the subject. Our approach bridges this gap by combining the reconstruction power of regression-based 3D human reconstruction with the generative capabilities of a diffusion model. In a first step, an initial 3D reconstructed human through a generalized NeRF provides comprehensive conditioning, ensuring high-quality synthesis faithful to the reference appearance and structure. Subsequently, the derived geometry and appearance from the generalized NeRF serve as input to a video-based diffusion model. This strategic integration is pivotal for enforcing both multi-view and temporal consistency throughout the avatarâ€™s generation. Empirical results underscore the superior generalization ability of our proposed method, demonstrating its effectiveness across diverse in-domain and out-of-domain in-the-wild datasets. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€ä¸”å¯æ¨å¹¿çš„æ¡†æ¶ï¼Œå¯ä»å•å¼ å›¾åƒä¸­åˆæˆè§†è§’ä¸€è‡´ä¸”æ—¶é—´è¿è´¯çš„è™šæ‹Ÿè§’è‰²ï¼Œä»¥è§£å†³å•å›¾åƒè§’è‰²ç”Ÿæˆè¿™ä¸€å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç°æœ‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•é€šå¸¸ä¾èµ–äºç¨€ç–çš„äººä½“æ¨¡æ¿ï¼ˆä¾‹å¦‚æ·±åº¦æˆ–æ³•çº¿å›¾ï¼‰ï¼Œè¿™ä¼šå¯¼è‡´è¿™äº›ä¿¡å·ä¸ä¸»ä½“çš„çœŸå®å¤–è§‚ä¹‹é—´çš„ä¸åŒ¹é…ï¼Œè¿›è€Œäº§ç”Ÿå¤šè§†è§’å’Œæ—¶é—´ä¸Šçš„ä¸ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†åŸºäºå›å½’çš„3Däººä½“é‡å»ºçš„é‡å»ºèƒ½åŠ›ä¸æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä»¥ç¼©å°è¿™ä¸€å·®è·ã€‚é¦–å…ˆï¼Œé€šè¿‡é€šç”¨çš„NeRFç”Ÿæˆåˆå§‹çš„3Dé‡å»ºäººä½“ï¼Œæä¾›å…¨é¢çš„æ¡ä»¶ï¼Œç¡®ä¿é«˜è´¨é‡ä¸”å¿ äºå‚è€ƒå¤–è§‚å’Œç»“æ„çš„åˆæˆã€‚éšåï¼Œä»é€šç”¨çš„NeRFä¸­å¾—å‡ºçš„å‡ ä½•å½¢çŠ¶å’Œå¤–è§‚ä½œä¸ºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„è¾“å…¥ã€‚è¿™ç§æˆ˜ç•¥æ€§çš„æ•´åˆå¯¹äºåœ¨è§’è‰²ç”Ÿæˆè¿‡ç¨‹ä¸­å¼ºåˆ¶æ‰§è¡Œå¤šè§†è§’å’Œæ—¶é—´ä¸€è‡´æ€§è‡³å…³é‡è¦ã€‚ç»éªŒç»“æœå¼ºè°ƒäº†æˆ‘ä»¬æå‡ºæ–¹æ³•çš„å“è¶Šæ³›åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†å®ƒåœ¨å„ç§é¢†åŸŸå†…å¤–ã€é‡ç”Ÿæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06957v2">PDF</a> ICCV 2025; Project Page: <a target="_blank" rel="noopener" href="https://humansensinglab.github.io/GAS/">https://humansensinglab.github.io/GAS/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€ä¸”é€šç”¨çš„æ¡†æ¶ï¼Œç”¨äºä»å•å¼ å›¾åƒåˆæˆè§†è§’ä¸€è‡´ã€æ—¶é—´è¿è´¯çš„è™šæ‹Ÿè§’è‰²ã€‚è¯¥æ¡†æ¶è§£å†³äº†åŸºäºæ‰©æ•£çš„å•å›¾åƒè™šæ‹Ÿè§’è‰²ç”Ÿæˆä¸­çš„ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œé€šè¿‡ç»“åˆå›å½’å‹ä¸‰ç»´äººä½“é‡å»ºçš„é‡å»ºèƒ½åŠ›ä¸æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œç¡®ä¿äº†é«˜è´¨é‡ä¸”å¿ äºåŸå§‹å¤–è§‚å’Œç»“æ„çš„åˆæˆã€‚é¦–å…ˆï¼Œé€šè¿‡é€šç”¨NeRFè¿›è¡Œä¸‰ç»´äººä½“é‡å»ºï¼Œä¸ºåç»­åˆæˆæä¾›å…¨é¢çš„æ¡ä»¶ã€‚æ¥ç€ï¼Œå°†ä»NeRFä¸­å¾—å‡ºçš„å‡ ä½•å’Œå¤–è§‚ä¿¡æ¯ä½œä¸ºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„è¾“å…¥ï¼Œå¼ºåˆ¶å®ç°å¤šè§†è§’å’Œæ—¶é—´ä¸Šçš„è¿è´¯æ€§ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨ä¸åŒé¢†åŸŸå’Œé‡å¤–æ•°æ®é›†ä¸Šéƒ½è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºä»å•å¼ å›¾åƒåˆæˆè§†è§’ä¸€è‡´ã€æ—¶é—´è¿è´¯çš„è™šæ‹Ÿè§’è‰²ã€‚</li>
<li>è§£å†³äº†ç°æœ‰æ‰©æ•£æ¨¡å‹åœ¨è™šæ‹Ÿè§’è‰²ç”Ÿæˆä¸­çš„å¤šè§†è§’å’Œæ—¶é—´ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>ç»“åˆäº†å›å½’å‹ä¸‰ç»´äººä½“é‡å»ºçš„é‡å»ºèƒ½åŠ›ä¸æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨é€šç”¨NeRFè¿›è¡Œåˆå§‹ä¸‰ç»´äººä½“é‡å»ºï¼Œä¸ºåç»­åˆæˆæä¾›å…¨é¢æ¡ä»¶ã€‚</li>
<li>ä»NeRFä¸­å¾—å‡ºçš„å‡ ä½•å’Œå¤–è§‚ä¿¡æ¯ä½œä¸ºè§†é¢‘æ‰©æ•£æ¨¡å‹çš„è¾“å…¥ã€‚</li>
<li>é€šè¿‡ç­–ç•¥æ€§æ•´åˆå®ç°äº†å¤šè§†è§’å’Œæ—¶é—´è¿è´¯æ€§çš„å¼ºåˆ¶å®æ–½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4051b2273865fc183efb9af8dd673d6b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a35b0421bfdf7f0ec7680228e4b5ee9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61186df640852268c7e25db6405240d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3407eebe3546c090b7f8c0ca2900578e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Individual-Content-and-Motion-Dynamics-Preserved-Pruning-for-Video-Diffusion-Models"><a href="#Individual-Content-and-Motion-Dynamics-Preserved-Pruning-for-Video-Diffusion-Models" class="headerlink" title="Individual Content and Motion Dynamics Preserved Pruning for Video   Diffusion Models"></a>Individual Content and Motion Dynamics Preserved Pruning for Video   Diffusion Models</h2><p><strong>Authors:Yiming Wu, Huan Wang, Zhenghao Chen, Dong Xu</strong></p>
<p>The high computational cost and slow inference time are major obstacles to deploying Video Diffusion Models (VDMs). To overcome this, we introduce a new Video Diffusion Model Compression approach using individual content and motion dynamics preserved pruning and consistency loss. First, we empirically observe that deeper VDM layers are crucial for maintaining the quality of \textbf{motion dynamics} (\textit{e.g.,} coherence of the entire video), while shallower layers are more focused on \textbf{individual content} (\textit{e.g.,} individual frames). Therefore, we prune redundant blocks from the shallower layers while preserving more of the deeper layers, resulting in a lightweight VDM variant called VDMini. Moreover, we propose an \textbf{Individual Content and Motion Dynamics (ICMD)} Consistency Loss to gain comparable generation performance as larger VDM to VDMini. In particular, we first use the Individual Content Distillation (ICD) Loss to preserve the consistency in the features of each generated frame between the teacher and student models. Next, we introduce a Multi-frame Content Adversarial (MCA) Loss to enhance the motion dynamics across the generated video as a whole. This method significantly accelerates inference time while maintaining high-quality video generation. Extensive experiments demonstrate the effectiveness of our VDMini on two important video generation tasks, Text-to-Video (T2V) and Image-to-Video (I2V), where we respectively achieve an average 2.5 $\times$, 1.4 $\times$, and 1.25 $\times$ speed up for the I2V method SF-V, the T2V method T2V-Turbo-v2, and the T2V method HunyuanVideo, while maintaining the quality of the generated videos on several benchmarks including UCF101, VBench-T2V, and VBench-I2V. </p>
<blockquote>
<p>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰çš„é«˜è®¡ç®—æˆæœ¬å’Œç¼“æ…¢çš„æ¨ç†æ—¶é—´æ˜¯å…¶ä¸»è¦éƒ¨ç½²éšœç¢ã€‚ä¸ºäº†å…‹æœè¿™ä¸€éšœç¢ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„è§†é¢‘æ‰©æ•£æ¨¡å‹å‹ç¼©æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¿ç•™äº†ä¸ªäººå†…å®¹å’Œè¿åŠ¨åŠ¨åŠ›å­¦çš„ä¿®å‰ªå’Œä¸€è‡´æ€§æŸå¤±ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç»éªŒæ€§åœ°è§‚å¯Ÿåˆ°ï¼Œè¾ƒæ·±çš„VDMå±‚å¯¹äºç»´æŒè¿åŠ¨åŠ¨åŠ›å­¦çš„è´¨é‡ï¼ˆä¾‹å¦‚æ•´ä¸ªè§†é¢‘çš„ä¸€è‡´æ€§ï¼‰è‡³å…³é‡è¦ï¼Œè€Œè¾ƒæµ…çš„å±‚åˆ™æ›´ä¸“æ³¨äºä¸ªäººå†…å®¹ï¼ˆä¾‹å¦‚å•ä¸ªå¸§ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»è¾ƒæµ…çš„å±‚ä¸­ä¿®å‰ªæ‰å¤šä½™çš„éƒ¨åˆ†ï¼ŒåŒæ—¶ä¿ç•™æ›´å¤šçš„æ·±å±‚ç»“æ„ï¼Œä»è€Œå¾—åˆ°ä¸€ç§è½»é‡çº§çš„VDMå˜ä½“ï¼Œç§°ä¸ºVDMiniã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸ªäººå†…å®¹å’Œè¿åŠ¨åŠ¨åŠ›å­¦ï¼ˆICMDï¼‰ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥åœ¨å¤§å‹VDMä¸VDMiniä¹‹é—´è·å¾—ç›¸å½“çš„ç”Ÿæˆæ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ä¸ªäººå†…å®¹è’¸é¦ï¼ˆICDï¼‰æŸå¤±æ¥ä¿æŒæ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´æ¯ä¸ªç”Ÿæˆå¸§çš„ç‰¹å¾ä¸€è‡´æ€§ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šå¸§å†…å®¹å¯¹æŠ—ï¼ˆMCAï¼‰æŸå¤±ï¼Œä»¥æé«˜æ•´ä¸ªç”Ÿæˆè§†é¢‘çš„åŠ¨æ€è¿åŠ¨æ•ˆæœã€‚è¯¥æ–¹æ³•æ˜¾è‘—åŠ é€Ÿäº†æ¨ç†æ—¶é—´ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„è§†é¢‘ç”Ÿæˆã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„VDMiniåœ¨ä¸¤é¡¹é‡è¦çš„è§†é¢‘ç”Ÿæˆä»»åŠ¡â€”â€”æ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰å’Œå›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰ä¸Šæ•ˆæœæ˜¾è‘—ã€‚åœ¨I2Væ–¹æ³•SF-Vã€T2Væ–¹æ³•T2V-Turbo-v2ä»¥åŠT2Væ–¹æ³•HunyuanVideoä¸Šï¼Œæˆ‘ä»¬åˆ†åˆ«å®ç°äº†å¹³å‡2.5å€ã€1.4å€å’Œ1.25å€çš„åŠ é€Ÿï¼ŒåŒæ—¶åœ¨åŒ…æ‹¬UCF101ã€VBench-T2Vå’ŒVBench-I2Vç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šä¿æŒäº†ç”Ÿæˆè§†é¢‘çš„è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18375v2">PDF</a> ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰é¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œæ…¢æ¨ç†æ—¶é—´çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„è§†é¢‘æ‰©æ•£æ¨¡å‹å‹ç¼©æ–¹æ³•ï¼Œé‡‡ç”¨ä¸ªä½“å†…å®¹å’Œè¿åŠ¨åŠ¨æ€ä¿ç•™çš„ä¿®å‰ªå’Œä¸€è‡´æ€§æŸå¤±ã€‚é€šè¿‡å®è¯è§‚å¯Ÿï¼Œæˆ‘ä»¬å‘ç°è¾ƒæ·±çš„VDMå±‚å¯¹äºä¿æŒè¿åŠ¨åŠ¨æ€è´¨é‡è‡³å…³é‡è¦ï¼Œè€Œè¾ƒæµ…çš„å±‚æ›´ä¸“æ³¨äºä¸ªä½“å†…å®¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä»è¾ƒæµ…çš„å±‚ä¿®å‰ªå†—ä½™å—ï¼ŒåŒæ—¶ä¿ç•™æ›´å¤šçš„æ·±å±‚ï¼Œå½¢æˆè½»é‡çº§çš„VDMå˜ä½“VDMiniã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸ªä½“å†…å®¹å’Œè¿åŠ¨åŠ¨æ€ï¼ˆICMDï¼‰ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥åœ¨è¾ƒå¤§çš„VDMä¸VDMiniä¹‹é—´è·å¾—ç›¸å½“çš„ç”Ÿäº§æ€§èƒ½ã€‚é€šè¿‡ä¸ªä½“å†…å®¹è’¸é¦ï¼ˆICDï¼‰æŸå¤±ä¿æŒæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´æ¯ä¸ªç”Ÿæˆå¸§çš„ç‰¹å¾ä¸€è‡´æ€§ï¼Œå¹¶å¼•å…¥å¤šå¸§å†…å®¹å¯¹æŠ—ï¼ˆMCAï¼‰æŸå¤±ä»¥å¢å¼ºæ•´ä¸ªç”Ÿæˆè§†é¢‘çš„è¿åŠ¨åŠ¨æ€ã€‚æ­¤æ–¹æ³•åœ¨ç»´æŒé«˜è´¨é‡è§†é¢‘ç”Ÿæˆçš„åŒæ—¶ï¼Œæ˜¾è‘—åŠ é€Ÿäº†æ¨ç†æ—¶é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰é¢ä¸´è®¡ç®—æˆæœ¬é«˜å’Œæ¨ç†æ—¶é—´æ…¢çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„VDMå‹ç¼©æ–¹æ³•ï¼Œç»“åˆä¸ªä½“å†…å®¹å’Œè¿åŠ¨åŠ¨æ€ä¿ç•™çš„ä¿®å‰ªå’Œä¸€è‡´æ€§æŸå¤±ã€‚</li>
<li>è¾ƒæ·±çš„VDMå±‚å¯¹ä¿æŒè¿åŠ¨åŠ¨æ€è´¨é‡è‡³å…³é‡è¦ï¼Œè€Œè¾ƒæµ…çš„å±‚æ›´å…³æ³¨ä¸ªä½“å†…å®¹ã€‚</li>
<li>æå‡ºäº†VDMiniï¼Œé€šè¿‡ä¿®å‰ªæµ…å±‚å†—ä½™å—å¹¶ä¿ç•™æ›´å¤šæ·±å±‚ï¼Œå½¢æˆè½»é‡çº§çš„VDMå˜ä½“ã€‚</li>
<li>å¼•å…¥äº†ICMDä¸€è‡´æ€§æŸå¤±ï¼Œä»¥åœ¨å¤§å‹VDMå’ŒVDMiniä¹‹é—´å®ç°ç›¸å½“çš„ç”Ÿäº§æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨ICDæŸå¤±ä¿æŒç‰¹å¾ä¸€è‡´æ€§ï¼Œå¹¶ä½¿ç”¨MCAæŸå¤±å¢å¼ºæ•´ä¸ªç”Ÿæˆè§†é¢‘çš„è¿åŠ¨åŠ¨æ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18375">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e0490970681aff5244e9d128cbbfc62d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39099762fc947c51230a54f797664409.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30d2b9d1b66601b800572b268202e5ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-191d67e34a36365ada83c07d08399370.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3bcd9fe6a7e5d62e56b37d42c3f6d47e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="TKG-DM-Training-free-Chroma-Key-Content-Generation-Diffusion-Model"><a href="#TKG-DM-Training-free-Chroma-Key-Content-Generation-Diffusion-Model" class="headerlink" title="TKG-DM: Training-free Chroma Key Content Generation Diffusion Model"></a>TKG-DM: Training-free Chroma Key Content Generation Diffusion Model</h2><p><strong>Authors:Ryugo Morita, Stanislav Frolov, Brian Bernhard Moser, Takahiro Shirakawa, Ko Watanabe, Andreas Dengel, Jinjia Zhou</strong></p>
<p>Diffusion models have enabled the generation of high-quality images with a strong focus on realism and textual fidelity. Yet, large-scale text-to-image models, such as Stable Diffusion, struggle to generate images where foreground objects are placed over a chroma key background, limiting their ability to separate foreground and background elements without fine-tuning. To address this limitation, we present a novel Training-Free Chroma Key Content Generation Diffusion Model (TKG-DM), which optimizes the initial random noise to produce images with foreground objects on a specifiable color background. Our proposed method is the first to explore the manipulation of the color aspects in initial noise for controlled background generation, enabling precise separation of foreground and background without fine-tuning. Extensive experiments demonstrate that our training-free method outperforms existing methods in both qualitative and quantitative evaluations, matching or surpassing fine-tuned models. Finally, we successfully extend it to other tasks (e.g., consistency models and text-to-video), highlighting its transformative potential across various generative applications where independent control of foreground and background is crucial. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²ç»èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾ç‰‡ï¼Œå¹¶å¼ºçƒˆä¸“æ³¨äºç°å®æ„Ÿå’Œæ–‡æœ¬å¿ å®åº¦ã€‚ç„¶è€Œï¼Œå¤§è§„æ¨¡çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œå¦‚Stable Diffusionï¼Œåœ¨ç”Ÿæˆå‰æ™¯ç‰©ä½“æ”¾ç½®åœ¨è‰²é”®èƒŒæ™¯ä¸Šçš„å›¾åƒæ—¶é‡åˆ°å›°éš¾ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨æœªç»å¾®è°ƒçš„æƒ…å†µä¸‹åˆ†ç¦»å‰æ™¯å’ŒèƒŒæ™¯å…ƒç´ çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨æ–°çš„æ— è®­ç»ƒè‰²é”®å†…å®¹ç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼ˆTKG-DMï¼‰ï¼Œå®ƒé€šè¿‡ä¼˜åŒ–åˆå§‹éšæœºå™ªå£°æ¥ç”Ÿæˆå…·æœ‰æŒ‡å®šé¢œè‰²èƒŒæ™¯çš„å›¾åƒã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•æ˜¯ç¬¬ä¸€ä¸ªæ¢ç´¢åˆå§‹å™ªå£°ä¸­é¢œè‰²æ–¹é¢çš„æ§åˆ¶ä»¥ç”¨äºèƒŒæ™¯ç”Ÿæˆçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸å¾®è°ƒçš„æƒ…å†µä¸‹å®ç°å‰æ™¯å’ŒèƒŒæ™¯çš„ç²¾ç¡®åˆ†ç¦»ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ— è®­ç»ƒæ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”šè‡³è¶…è¿‡äº†å¾®è°ƒæ¨¡å‹ã€‚æœ€åï¼Œæˆ‘ä»¬æˆåŠŸåœ°å°†å…¶æ‰©å±•åˆ°å…¶ä»–ä»»åŠ¡ï¼ˆå¦‚ä¸€è‡´æ€§æ¨¡å‹å’Œæ–‡æœ¬åˆ°è§†é¢‘ï¼‰ï¼Œçªæ˜¾å…¶åœ¨å„ç§ç”Ÿæˆåº”ç”¨ç¨‹åºä¸­çš„å˜é©æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç‹¬ç«‹æ§åˆ¶å‰æ™¯å’ŒèƒŒæ™¯çš„æƒ…å†µä¸‹è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15580v3">PDF</a> Accepted to CVPR2025(Highlight). Code at:   <a target="_blank" rel="noopener" href="https://github.com/ryugo417/TKG-DM">https://github.com/ryugo417/TKG-DM</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Diffusionæ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨çœŸå®æ€§å’Œæ–‡æœ¬å¿ å®åº¦æ–¹é¢çš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œå¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰åœ¨ç”Ÿæˆå‰æ™¯ç‰©ä½“æ”¾ç½®åœ¨è‰²é”®èƒŒæ™¯ä¸Šçš„å›¾åƒæ—¶å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–°å‹è‰²é”®å†…å®¹ç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼ˆTKG-DMï¼‰ï¼Œå®ƒé€šè¿‡ä¼˜åŒ–åˆå§‹éšæœºå™ªå£°æ¥ç”Ÿæˆå…·æœ‰æŒ‡å®šèƒŒæ™¯è‰²çš„å‰æ™¯ç‰©ä½“å›¾åƒã€‚è¯¥æ–¹æ³•é¦–æ¬¡æ¢ç´¢äº†åˆå§‹å™ªå£°ä¸­é¢œè‰²æ–¹é¢çš„æ“æ§ï¼Œå®ç°äº†æ— éœ€ç²¾ç»†è°ƒæ•´çš„ç²¾ç¡®å‰æ™¯ä¸èƒŒæ™¯åˆ†ç¦»ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç”šè‡³ä¸ç²¾ç»†è°ƒæ•´çš„æ¨¡å‹ç›¸åŒ¹é…æˆ–è¶…è¶Šã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•çš„æ½œåŠ›å·¨å¤§ï¼Œå¯å¹¿æ³›åº”ç”¨äºå„ç§ç”Ÿæˆä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç‹¬ç«‹æ§åˆ¶å‰æ™¯å’ŒèƒŒæ™¯çš„ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusionæ¨¡å‹èƒ½ç”Ÿæˆé«˜è´¨é‡ã€é€¼çœŸçš„å›¾åƒã€‚</li>
<li>å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨ç”Ÿæˆç‰¹å®šèƒŒæ™¯ï¼ˆå¦‚è‰²é”®èƒŒæ™¯ï¼‰ä¸Šçš„å‰æ™¯ç‰©ä½“å›¾åƒæ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„Training-Free Chroma Key Content Generation Diffusion Modelï¼ˆTKG-DMï¼‰èƒ½ä¼˜åŒ–åˆå§‹éšæœºå™ªå£°ï¼Œç”Ÿæˆå…·æœ‰æŒ‡å®šèƒŒæ™¯è‰²çš„å‰æ™¯ç‰©ä½“å›¾åƒã€‚</li>
<li>TKG-DMé¦–æ¬¡æ¢ç´¢äº†åˆå§‹å™ªå£°ä¸­é¢œè‰²æ–¹é¢çš„æ“æ§ï¼Œå®ç°å‰æ™¯å’ŒèƒŒæ™¯çš„ç²¾ç¡®åˆ†ç¦»ï¼Œæ— éœ€ç²¾ç»†è°ƒæ•´ã€‚</li>
<li>TKG-DMåœ¨å®šæ€§å’Œå®šé‡è¯„ä¼°ä¸­éƒ½è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šäº†æŸäº›ç»è¿‡ç²¾ç»†è°ƒæ•´çš„æ¨¡å‹ã€‚</li>
<li>TKG-DMå¯å¹¿æ³›åº”ç”¨äºå„ç§ç”Ÿæˆä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç‹¬ç«‹æ§åˆ¶å‰æ™¯å’ŒèƒŒæ™¯çš„ä»»åŠ¡ä¸­è¡¨ç°çªå‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15580">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b941ec7903645158dc6ae9e24b1972e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14dee72935c06c364738a92e092cb21e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-565d8b32e55dc29992b2ae70d5de0562.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-309e6068f65d0a50d169a815e66014c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24d2645586e89df64ad5eee5f6fa42c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-371059e35c21a291b8a5e9ce814bed1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1e41ffb9360ba2ad669276447598043.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="DiffSSC-Semantic-LiDAR-Scan-Completion-using-Denoising-Diffusion-Probabilistic-Models"><a href="#DiffSSC-Semantic-LiDAR-Scan-Completion-using-Denoising-Diffusion-Probabilistic-Models" class="headerlink" title="DiffSSC: Semantic LiDAR Scan Completion using Denoising Diffusion   Probabilistic Models"></a>DiffSSC: Semantic LiDAR Scan Completion using Denoising Diffusion   Probabilistic Models</h2><p><strong>Authors:Helin Cao, Sven Behnke</strong></p>
<p>Perception systems play a crucial role in autonomous driving, incorporating multiple sensors and corresponding computer vision algorithms. 3D LiDAR sensors are widely used to capture sparse point clouds of the vehicleâ€™s surroundings. However, such systems struggle to perceive occluded areas and gaps in the scene due to the sparsity of these point clouds and their lack of semantics. To address these challenges, Semantic Scene Completion (SSC) jointly predicts unobserved geometry and semantics in the scene given raw LiDAR measurements, aiming for a more complete scene representation. Building on promising results of diffusion models in image generation and super-resolution tasks, we propose their extension to SSC by implementing the noising and denoising diffusion processes in the point and semantic spaces individually. To control the generation, we employ semantic LiDAR point clouds as conditional input and design local and global regularization losses to stabilize the denoising process. We evaluate our approach on autonomous driving datasets, and it achieves state-of-the-art performance for SSC, surpassing most existing methods. </p>
<blockquote>
<p>æ„ŸçŸ¥ç³»ç»Ÿåœ¨è‡ªåŠ¨é©¾é©¶ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå®ƒèåˆäº†å¤šç§ä¼ æ„Ÿå™¨å’Œç›¸åº”çš„è®¡ç®—æœºè§†è§‰ç®—æ³•ã€‚3Dæ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨è¢«å¹¿æ³›åº”ç”¨äºæ•æ‰è½¦è¾†å‘¨å›´ç¨€ç–çš„ç‚¹äº‘ã€‚ç„¶è€Œï¼Œç”±äºç‚¹äº‘çš„ç¨€ç–æ€§å’Œç¼ºä¹è¯­ä¹‰ä¿¡æ¯ï¼Œè¿™æ ·çš„ç³»ç»Ÿåœ¨æ„ŸçŸ¥é®æŒ¡åŒºåŸŸå’Œåœºæ™¯ä¸­çš„é—´éš™æ—¶é¢ä¸´å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¯­ä¹‰åœºæ™¯è¡¥å…¨ï¼ˆSSCï¼‰èƒ½å¤Ÿæ ¹æ®åŸå§‹çš„æ¿€å…‰é›·è¾¾æµ‹é‡è”åˆé¢„æµ‹åœºæ™¯ä¸­æœªè§‚å¯Ÿåˆ°çš„å‡ ä½•å½¢çŠ¶å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œæ—¨åœ¨å®ç°æ›´å®Œæ•´çš„åœºæ™¯è¡¨ç¤ºã€‚åŸºäºæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸­çš„å‡ºè‰²è¡¨ç°ï¼Œæˆ‘ä»¬å°†å…¶æ‰©å±•åˆ°SSCï¼Œé€šè¿‡åœ¨ç‚¹å’Œè¯­ä¹‰ç©ºé—´ä¸Šåˆ†åˆ«å®ç°å™ªå£°å’Œå»å™ªå£°æ‰©æ•£è¿‡ç¨‹ã€‚ä¸ºäº†æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ï¼Œæˆ‘ä»¬ä½¿ç”¨è¯­ä¹‰æ¿€å…‰é›·è¾¾ç‚¹äº‘ä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œå¹¶è®¾è®¡å±€éƒ¨å’Œå…¨å±€æ­£åˆ™åŒ–æŸå¤±æ¥ç¨³å®šå»å™ªå£°è¿‡ç¨‹ã€‚æˆ‘ä»¬åœ¨è‡ªåŠ¨é©¾é©¶æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå®ƒå®ç°äº†æœ€å…ˆè¿›çš„SSCæ€§èƒ½ï¼Œè¶…è¶Šäº†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18092v3">PDF</a> 2025 IEEE&#x2F;RSJ International Conference on Intelligent Robots and   Systems (IROS 2025), Hangzhou, China, Oct 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ„ŸçŸ¥ç³»ç»Ÿåœ¨è‡ªåŠ¨é©¾é©¶ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œèåˆäº†å¤šç§ä¼ æ„Ÿå™¨å’Œè®¡ç®—æœºè§†è§‰ç®—æ³•ã€‚3Dæ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨å¹¿æ³›åº”ç”¨äºæ•æ‰è½¦è¾†å‘¨å›´ç¨€ç–çš„ç‚¹äº‘æ•°æ®ã€‚ç„¶è€Œï¼Œç”±äºç‚¹äº‘æ•°æ®çš„ç¨€ç–æ€§å’Œç¼ºä¹è¯­ä¹‰ä¿¡æ¯ï¼Œè¿™ç±»ç³»ç»Ÿåœ¨æ„ŸçŸ¥é®æŒ¡åŒºåŸŸå’Œåœºæ™¯ä¸­çš„é—´éš™æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¯­ä¹‰åœºæ™¯è¡¥å…¨ï¼ˆSSCï¼‰æ—¨åœ¨æ ¹æ®åŸå§‹æ¿€å…‰é›·è¾¾æµ‹é‡è”åˆé¢„æµ‹æœªè§‚å¯Ÿåˆ°çš„åœºæ™¯å‡ ä½•å’Œè¯­ä¹‰ï¼Œä»¥å®ç°æ›´å®Œæ•´çš„åœºæ™¯è¡¨ç¤ºã€‚åŸºäºæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸­çš„ä¼˜å¼‚è¡¨ç°ï¼Œæˆ‘ä»¬å°†å…¶æ‰©å±•åˆ°SSCï¼Œé€šè¿‡åœ¨ç‚¹å’Œè¯­ä¹‰ç©ºé—´åˆ†åˆ«å®ç°å™ªå£°å’Œå»å™ªå£°æ‰©æ•£è¿‡ç¨‹ã€‚ä¸ºäº†æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ï¼Œæˆ‘ä»¬ä½¿ç”¨è¯­ä¹‰æ¿€å…‰é›·è¾¾ç‚¹äº‘ä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œå¹¶è®¾è®¡å±€éƒ¨å’Œå…¨å±€æ­£åˆ™åŒ–æŸå¤±ä»¥ç¨³å®šå»å™ªå£°è¿‡ç¨‹ã€‚æˆ‘ä»¬åœ¨è‡ªåŠ¨é©¾é©¶æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå®ƒåœ¨SSCæ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ„ŸçŸ¥ç³»ç»Ÿåœ¨è‡ªåŠ¨é©¾é©¶ä¸­èµ·å…³é”®ä½œç”¨ï¼Œèåˆå¤šç§ä¼ æ„Ÿå™¨å’Œè®¡ç®—æœºè§†è§‰ç®—æ³•ã€‚</li>
<li>3Dæ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨å¹¿æ³›åº”ç”¨äºæ•æ‰è½¦è¾†å‘¨å›´çš„ç¨€ç–ç‚¹äº‘æ•°æ®ï¼Œä½†å­˜åœ¨æ„ŸçŸ¥é®æŒ¡åŒºåŸŸå’Œåœºæ™¯é—´éš™çš„æŒ‘æˆ˜ã€‚</li>
<li>è¯­ä¹‰åœºæ™¯è¡¥å…¨ï¼ˆSSCï¼‰å¯è”åˆé¢„æµ‹æœªè§‚å¯Ÿåˆ°çš„åœºæ™¯å‡ ä½•å’Œè¯­ä¹‰ï¼Œä»¥å®ç°æ›´å®Œæ•´çš„åœºæ™¯è¡¨ç¤ºã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆå’Œè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¯æ‰©å±•åˆ°SSCã€‚</li>
<li>åœ¨ç‚¹å’Œè¯­ä¹‰ç©ºé—´åˆ†åˆ«å®ç°å™ªå£°å’Œå»å™ªå£°æ‰©æ•£è¿‡ç¨‹ã€‚</li>
<li>ä½¿ç”¨è¯­ä¹‰æ¿€å…‰é›·è¾¾ç‚¹äº‘ä½œä¸ºæ¡ä»¶è¾“å…¥æ¥æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18092">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-147711aaa91c692131f67ba35cec7154.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c790ebb1619c781ff45fb42b27a9a6f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0680bf6bbf76c038bb6bf1e4b6b409a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-513b9e274003db340e8685c891ac80aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50c4cc43d42ff5ff80c5625db369720b.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5fd13196327de12b5517ebc5e254b84c.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  RL-U$^2$Net A Dual-Branch UNet with Reinforcement Learning-Assisted   Multimodal Feature Fusion for Accurate 3D Whole-Heart Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f41e461634289951c99dea6a22e2b222.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  ASDR Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant   Neural Rendering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26633.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
