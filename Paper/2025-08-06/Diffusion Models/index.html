<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-08-06  From Pixels to Pathology Restoration Diffusion for   Diagnostic-Consistent Virtual IHC">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-3efef1a659ef791352fd6decd7a2b92a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    79 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-06-更新"><a href="#2025-08-06-更新" class="headerlink" title="2025-08-06 更新"></a>2025-08-06 更新</h1><h2 id="From-Pixels-to-Pathology-Restoration-Diffusion-for-Diagnostic-Consistent-Virtual-IHC"><a href="#From-Pixels-to-Pathology-Restoration-Diffusion-for-Diagnostic-Consistent-Virtual-IHC" class="headerlink" title="From Pixels to Pathology: Restoration Diffusion for   Diagnostic-Consistent Virtual IHC"></a>From Pixels to Pathology: Restoration Diffusion for   Diagnostic-Consistent Virtual IHC</h2><p><strong>Authors:Jingsong Liu, Xiaofeng Deng, Han Li, Azar Kazemi, Christian Grashei, Gesa Wilkens, Xin You, Tanja Groll, Nassir Navab, Carolin Mogler, Peter J. Schüffler</strong></p>
<p>Hematoxylin and eosin (H&amp;E) staining is the clinical standard for assessing tissue morphology, but it lacks molecular-level diagnostic information. In contrast, immunohistochemistry (IHC) provides crucial insights into biomarker expression, such as HER2 status for breast cancer grading, but remains costly and time-consuming, limiting its use in time-sensitive clinical workflows. To address this gap, virtual staining from H&amp;E to IHC has emerged as a promising alternative, yet faces two core challenges: (1) Lack of fair evaluation of synthetic images against misaligned IHC ground truths, and (2) preserving structural integrity and biological variability during translation. To this end, we present an end-to-end framework encompassing both generation and evaluation in this work. We introduce Star-Diff, a structure-aware staining restoration diffusion model that reformulates virtual staining as an image restoration task. By combining residual and noise-based generation pathways, Star-Diff maintains tissue structure while modeling realistic biomarker variability. To evaluate the diagnostic consistency of the generated IHC patches, we propose the Semantic Fidelity Score (SFS), a clinical-grading-task-driven metric that quantifies class-wise semantic degradation based on biomarker classification accuracy. Unlike pixel-level metrics such as SSIM and PSNR, SFS remains robust under spatial misalignment and classifier uncertainty. Experiments on the BCI dataset demonstrate that Star-Diff achieves state-of-the-art (SOTA) performance in both visual fidelity and diagnostic relevance. With rapid inference and strong clinical alignment,it presents a practical solution for applications such as intraoperative virtual IHC synthesis. </p>
<blockquote>
<p>苏木精和伊红（H&amp;E）染色是评估组织形态的临床标准，但它缺乏分子水平的诊断信息。相比之下，免疫组织化学（IHC）为生物标志物表达提供了关键见解，如乳腺癌分级中的HER2状态，但其成本高昂且耗时，限制了其在时间敏感的临床工作流程中的应用。为了解决这一差距，从H&amp;E到IHC的虚拟染色已成为一种有前途的替代方案，但面临两个核心挑战：（1）合成图像与错配的IHC真实值之间的公平评估缺失；（2）在翻译过程中保持结构完整性和生物变异性。为此，我们在工作中提出了一个涵盖生成和评估的端到端框架。我们介绍了Star-Diff，这是一个结构感知的染色恢复扩散模型，它将虚拟染色重新定义为图像恢复任务。通过结合残差和基于噪声的生成路径，Star-Diff在模拟现实生物标志物变异性的同时保持了组织结构。为了评估生成IHC补丁的诊断一致性，我们提出了语义保真度评分（SFS），这是一个基于临床分级任务的指标，它根据生物标志物分类准确性对类级语义降解进行量化。与SSIM和PSNR等像素级指标不同，SFS在空间错位和分类器不确定性下保持稳健。在BCI数据集上的实验表明，Star-Diff在视觉保真度和诊断相关性方面达到了最新水平。具有快速推理和强大的临床对齐能力，它为术中虚拟IHC合成等应用提供了实际解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02528v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>H&amp;E染色是评估组织形态的临床标准，但缺乏分子水平的诊断信息。相比之下，免疫组织化学（IHC）为生物标志物表达提供关键见解，如乳腺癌分级中的HER2状态，但成本高昂且耗时，限制了其在时间敏感的临床工作流程中的应用。为了解决这一差距，从H&amp;E到IHC的虚拟染色已成为一种有前途的替代方案，但面临两个核心挑战：（1）合成图像与错位的IHC真实值之间的公平评估不足；（2）在翻译过程中保持结构完整性和生物变异性。因此，我们在工作中提出了涵盖生成和评估的端到端框架。我们介绍了Star-Diff，这是一种结构感知染色恢复扩散模型，将虚拟染色重新定义为图像恢复任务。通过结合残差和基于噪声的生成路径，Star-Diff在模拟生物标志物变异性的同时保持了组织结构。为了评估生成IHC斑块的诊断一致性，我们提出了语义保真度评分（SFS），这是一种基于临床分级任务的指标，根据生物标志物分类准确性量化类级别的语义降解。与SSIM和PSNR等像素级指标不同，SFS在空间错位和分类器不确定性下保持稳健。在BCI数据集上的实验表明，Star-Diff在视觉保真度和诊断相关性方面达到了最新水平（SOTA）。其快速推理和强大的临床对齐能力为术中虚拟IHC合成等应用提供了切实可行的解决方案。</p>
<p><strong>要点</strong></p>
<ol>
<li>H&amp;E染色在评估组织形态上是临床标准，但缺乏分子水平的诊断信息。</li>
<li>免疫组织化学（IHC）提供有关生物标志物表达的见解，但成本高昂且耗时。</li>
<li>虚拟染色技术作为从H&amp;E到IHC的替代方案面临两个核心挑战：合成图像与真实IHC的评估及结构完整性和生物变异性的保持。</li>
<li>Star-Diff是一种结构感知染色恢复扩散模型，能将虚拟染色作为图像恢复任务来处理。</li>
<li>Star-Diff结合了残差和基于噪声的生成路径，以模拟生物标志物变异性和保持组织结构。</li>
<li>语义保真度评分（SFS）是一种新的评价指标，用于评估生成的IHC图像的诊断一致性，它基于临床分级任务并考虑生物标志物的分类准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02528">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-665af69cf482ef14026c7a36af1f5287.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-911bd2225eb2f8fe2d65c1d9c728e40d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c141b691349995552d3893c00183d361.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Dream-to-Recon-Monocular-3D-Reconstruction-with-Diffusion-Depth-Distillation-from-Single-Images"><a href="#Dream-to-Recon-Monocular-3D-Reconstruction-with-Diffusion-Depth-Distillation-from-Single-Images" class="headerlink" title="Dream-to-Recon: Monocular 3D Reconstruction with Diffusion-Depth   Distillation from Single Images"></a>Dream-to-Recon: Monocular 3D Reconstruction with Diffusion-Depth   Distillation from Single Images</h2><p><strong>Authors:Philipp Wulff, Felix Wimbauer, Dominik Muhle, Daniel Cremers</strong></p>
<p>Volumetric scene reconstruction from a single image is crucial for a broad range of applications like autonomous driving and robotics. Recent volumetric reconstruction methods achieve impressive results, but generally require expensive 3D ground truth or multi-view supervision. We propose to leverage pre-trained 2D diffusion models and depth prediction models to generate synthetic scene geometry from a single image. This can then be used to distill a feed-forward scene reconstruction model. Our experiments on the challenging KITTI-360 and Waymo datasets demonstrate that our method matches or outperforms state-of-the-art baselines that use multi-view supervision, and offers unique advantages, for example regarding dynamic scenes. </p>
<blockquote>
<p>从单张图像进行场景体积重建对于自动驾驶和机器人技术等广泛应用领域至关重要。虽然最近的体积重建方法取得了令人印象深刻的结果，但它们通常需要昂贵的3D真实数据或多视角监督。我们建议使用预训练的2D扩散模型和深度预测模型来从单张图像生成合成场景几何结构，然后可以用于提炼前馈场景重建模型。我们在具有挑战性的KITTI-360和Waymo数据集上的实验表明，我们的方法与使用多视角监督的先进技术相比，性能相匹配或更胜一筹，并具有独特的优势，例如在动态场景方面。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02323v1">PDF</a> ICCV 2025. Website: <a target="_blank" rel="noopener" href="https://philippwulff.github.io/dream-to-recon">https://philippwulff.github.io/dream-to-recon</a></p>
<p><strong>Summary</strong></p>
<p>该文提出了利用预训练的2D扩散模型和深度预测模型，从单张图像生成合成场景几何的方法，用于蒸馏前馈场景重建模型。在具有挑战性的KITTI-360和Waymo数据集上的实验表明，该方法匹配或超越了使用多视角监督的基线，并具有动态场景的独特优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章讨论了从单张图像进行体积场景重建的重要性，并指出其在自动驾驶和机器人技术等领域的应用。</li>
<li>现有体积重建方法虽效果显著，但需要昂贵的3D真实数据或多视角监督。</li>
<li>文章提出了利用预训练的2D扩散模型和深度预测模型的方法，从单张图像生成合成场景几何。</li>
<li>该方法能够蒸馏出前馈场景重建模型。</li>
<li>在KITTI-360和Waymo数据集上的实验证明了该方法的有效性，与多视角监督的基线相比具有竞争力。</li>
<li>该方法在动态场景上具有独特优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02323">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c6ed5e977aa03bf1cd37affd00e8b192.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e736ca1c28a5766a4c325b58c6bb261.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5fdc7a2331b06b5adea35c6da680d36.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Devil-is-in-the-Detail-Towards-Injecting-Fine-Details-of-Image-Prompt-in-Image-Generation-via-Conflict-free-Guidance-and-Stratified-Attention"><a href="#Devil-is-in-the-Detail-Towards-Injecting-Fine-Details-of-Image-Prompt-in-Image-Generation-via-Conflict-free-Guidance-and-Stratified-Attention" class="headerlink" title="Devil is in the Detail: Towards Injecting Fine Details of Image Prompt   in Image Generation via Conflict-free Guidance and Stratified Attention"></a>Devil is in the Detail: Towards Injecting Fine Details of Image Prompt   in Image Generation via Conflict-free Guidance and Stratified Attention</h2><p><strong>Authors:Kyungmin Jo, Jooyeol Yun, Jaegul Choo</strong></p>
<p>While large-scale text-to-image diffusion models enable the generation of high-quality, diverse images from text prompts, these prompts struggle to capture intricate details, such as textures, preventing the user intent from being reflected. This limitation has led to efforts to generate images conditioned on user-provided images, referred to as image prompts. Recent work modifies the self-attention mechanism to impose image conditions in generated images by replacing or concatenating the keys and values from the image prompt. This enables the self-attention layer to work like a cross-attention layer, generally used to incorporate text prompts. In this paper, we identify two common issues in existing methods of modifying self-attention to generate images that reflect the details of image prompts. First, existing approaches neglect the importance of image prompts in classifier-free guidance. Specifically, current methods use image prompts as both desired and undesired conditions in classifier-free guidance, causing conflicting signals. To resolve this, we propose conflict-free guidance by using image prompts only as desired conditions, ensuring that the generated image faithfully reflects the image prompt. In addition, we observe that the two most common self-attention modifications involve a trade-off between the realism of the generated image and alignment with the image prompt. Specifically, selecting more keys and values from the image prompt improves alignment, while selecting more from the generated image enhances realism. To balance both, we propose an new self-attention modification method, Stratified Attention to jointly use keys and values from both images rather than selecting between them. Through extensive experiments across three image generation tasks, we show that the proposed method outperforms existing image-prompting models in faithfully reflecting the image prompt. </p>
<blockquote>
<p>虽然大规模文本到图像的扩散模型能够从文本提示中生成高质量、多样化的图像，但这些提示很难捕捉复杂的细节，如纹理，无法反映用户的意图。这一局限性促使人们努力根据用户提供的图像生成图像，称为图像提示。最近的工作通过修改自注意力机制，通过在生成的图像中替换或连接图像提示的键和值来施加图像条件。这使得自注意力层能够像用于融合文本提示的交叉注意力层那样工作。在本文中，我们发现了两种在现有方法中修改自注意力以生成反映图像提示细节的图像时存在的常见问题。首先，现有方法忽视了图像提示在分类器自由指导中的重要性。具体来说，当前的方法将图像提示既用作所需条件又用作非所需条件，产生相互矛盾的信息。为解决这一问题，我们通过仅使用图像提示作为所需条件，提出了无冲突指导方法，确保生成的图像忠实反映图像提示。此外，我们观察到最常见的两种自注意力修改涉及生成图像的逼真度和与图像提示对齐之间的权衡。具体来说，从图像提示中选择更多的键和值可以提高对齐性，而从生成的图像中选择更多则增强逼真度。为了平衡这两者，我们提出了一种新的自注意力修改方法——分层注意力，该方法能够同时使用来自两者的键和值，而不是在它们之间做出选择。通过三项图像生成任务的广泛实验，我们证明了所提出的方法在忠实反映图像提示方面优于现有的图像提示模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02004v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了大规模文本到图像的扩散模型在生成图像时面临的挑战，即无法捕捉用户提供的图像提示中的细节。为此，研究人员改进了自注意力机制以引入图像条件，并指出了现有方法中的两个问题。首先，现有方法忽视了图像提示在分类器免费指导中的重要性，导致生成的图像无法忠实反映图像提示。为解决这一问题，本文提出了无冲突指导方法，仅将图像提示作为期望条件。其次，现有自注意力修改方法面临真实感和与图像提示对齐之间的权衡。为解决这一问题，本文提出了分层注意力方法，结合来自图像和生成图像的键和值。实验表明，该方法在忠实反映图像提示方面优于现有图像提示模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本到图像的扩散模型虽能生成高质量、多样化的图像，但难以捕捉用户提供的图像提示中的细节。</li>
<li>现有方法忽视了图像提示在分类器免费指导中的重要性。</li>
<li>提出的无冲突指导方法仅将图像提示作为期望条件，确保生成的图像忠实反映图像提示。</li>
<li>现有自注意力修改方法在真实感和与图像提示对齐之间存在权衡。</li>
<li>提出的分层注意力方法结合来自图像和生成图像的键和值，以平衡真实性和对齐。</li>
<li>通过三个图像生成任务的广泛实验，新方法在忠实反映图像提示方面表现出优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02004">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5b286ef525cb614dc8e725aba515bb75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efab99c3b65dde5230f53b39ffbba4ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-290823b79e337f7cf379f797d02e94d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d364ee80cada1551e0752f1a20763831.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6038a75c19b999e2b3bb58c51cda8c02.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5077066eae218b9e58fae3bee1976ff1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DiffusionFF-Face-Forgery-Detection-via-Diffusion-based-Artifact-Localization"><a href="#DiffusionFF-Face-Forgery-Detection-via-Diffusion-based-Artifact-Localization" class="headerlink" title="DiffusionFF: Face Forgery Detection via Diffusion-based Artifact   Localization"></a>DiffusionFF: Face Forgery Detection via Diffusion-based Artifact   Localization</h2><p><strong>Authors:Siran Peng, Haoyuan Zhang, Li Gao, Tianshuo Zhang, Bao Li, Zhen Lei</strong></p>
<p>The rapid evolution of deepfake generation techniques demands robust and accurate face forgery detection algorithms. While determining whether an image has been manipulated remains essential, the ability to precisely localize forgery artifacts has become increasingly important for improving model explainability and fostering user trust. To address this challenge, we propose DiffusionFF, a novel framework that enhances face forgery detection through diffusion-based artifact localization. Our method utilizes a denoising diffusion model to generate high-quality Structural Dissimilarity (DSSIM) maps, which effectively capture subtle traces of manipulation. These DSSIM maps are then fused with high-level semantic features extracted by a pretrained forgery detector, leading to significant improvements in detection accuracy. Extensive experiments on both cross-dataset and intra-dataset benchmarks demonstrate that DiffusionFF not only achieves superior detection performance but also offers precise and fine-grained artifact localization, highlighting its overall effectiveness. </p>
<blockquote>
<p>深度伪造生成技术的快速发展要求有稳健且准确的人脸伪造检测算法。确定图像是否被篡改仍然至关重要，但精确定位伪造痕迹的能力在提高模型解释性和促进用户信任方面变得日益重要。为了应对这一挑战，我们提出了DiffusionFF这一新型框架，它通过基于扩散的痕迹定位技术来提高人脸伪造检测能力。我们的方法利用去噪扩散模型生成高质量的结构差异（DSSIM）图，有效捕捉细微的篡改痕迹。这些DSSIM图然后与由预训练伪造检测器提取的高级语义特征相融合，显著提高了检测精度。跨数据集和内部数据集的广泛实验表明，DiffusionFF不仅实现了卓越的检测性能，而且提供了精确和精细的痕迹定位，凸显了其整体有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01873v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>深度伪造生成技术的快速发展要求有稳健准确的面部伪造检测算法。除了判断图像是否被操纵外，能够精确定位伪造痕迹的能力对于提高模型解释性和增强用户信任度也变得越来越重要。为解决这一挑战，我们提出了DiffusionFF这一新型框架，它通过基于扩散的痕迹定位技术来提升面部伪造检测效果。该方法利用去噪扩散模型生成高质量的结构差异（DSSIM）图，有效捕捉微妙的操作痕迹。这些DSSIM图随后与预训练伪造检测器提取的高级语义特征融合，极大地提高了检测准确率。在跨数据集和内部数据集基准测试上的广泛实验表明，DiffusionFF不仅实现了出色的检测性能，而且提供了精确和精细的痕迹定位，凸显了其整体有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度伪造生成技术的快速发展要求有稳健准确的面部伪造检测算法。</li>
<li>除了判断图像是否被操纵外，定位伪造痕迹的能力日益重要，以提高模型解释性和用户信任度。</li>
<li>DiffusionFF框架利用去噪扩散模型生成高质量的结构差异（DSSIM）图，以捕捉操作痕迹。</li>
<li>DSSIM图与预训练伪造检测器的高级语义特征融合，提高了检测准确率。</li>
<li>DiffusionFF实现了跨数据集和内部数据集的优异检测性能。</li>
<li>DiffusionFF提供了精确和精细的伪造痕迹定位。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01873">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-066c16821967d4265f79871679349e11.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ce46e6b1f5326aad001a145a0fbf643.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-94419728732b608cd95b9b637bdc5550.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42ad975732e1be8f58e5c1759f0002d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2bd4275f69a04a8609f50774c2643265.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00971867626d3b87c5371b6365befe0d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Diffusion-based-3D-Hand-Motion-Recovery-with-Intuitive-Physics"><a href="#Diffusion-based-3D-Hand-Motion-Recovery-with-Intuitive-Physics" class="headerlink" title="Diffusion-based 3D Hand Motion Recovery with Intuitive Physics"></a>Diffusion-based 3D Hand Motion Recovery with Intuitive Physics</h2><p><strong>Authors:Yufei Zhang, Zijun Cui, Jeffrey O. Kephart, Qiang Ji</strong></p>
<p>While 3D hand reconstruction from monocular images has made significant progress, generating accurate and temporally coherent motion estimates from videos remains challenging, particularly during hand-object interactions. In this paper, we present a novel 3D hand motion recovery framework that enhances image-based reconstructions through a diffusion-based and physics-augmented motion refinement model. Our model captures the distribution of refined motion estimates conditioned on initial ones, generating improved sequences through an iterative denoising process. Instead of relying on scarce annotated video data, we train our model only using motion capture data without images. We identify valuable intuitive physics knowledge during hand-object interactions, including key motion states and their associated motion constraints. We effectively integrate these physical insights into our diffusion model to improve its performance. Extensive experiments demonstrate that our approach significantly improves various frame-wise reconstruction methods, achieving state-of-the-art (SOTA) performance on existing benchmarks. </p>
<blockquote>
<p>虽然从单目图像中重建3D手部已经取得了显著进展，但从视频中生成准确且时间连贯的运动估计仍然具有挑战性，特别是在手部与物体交互时。在本文中，我们提出了一种新颖的3D手部运动恢复框架，它通过基于扩散和物理增强的运动优化模型来增强基于图像的重建。我们的模型捕获了基于初始估计的精细运动估计的分布，并通过迭代去噪过程生成了改进序列。我们并没有依赖于稀缺的标注视频数据来训练模型，而是仅使用运动捕获数据进行训练，并不使用图像。我们确定了在手部与物体交互过程中的有价值的直观物理知识，包括关键运动状态及其相关的运动约束。我们有效地将这些物理见解集成到我们的扩散模型中，以提高其性能。大量实验表明，我们的方法显著改进了各种帧级重建方法，在现有基准测试中实现了最佳性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01835v1">PDF</a> ICCV2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新颖的3D手部运动恢复框架，通过扩散模型和物理增强运动优化模型提升基于图像的重建效果。该模型能够捕捉初始运动估计的分布情况，并通过迭代去噪过程生成改进序列。研究团队利用运动捕捉数据进行训练，无需图像数据。此外，该研究还将手部与物体交互过程中的直觉物理知识融入扩散模型，提升模型性能。实验证明，该方法在现有基准测试中表现卓越，显著提高帧重建效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新颖的3D手部运动恢复框架，结合了扩散模型和物理增强技术，提高了基于图像的重建效果。</li>
<li>通过捕捉初始运动估计的分布情况，并迭代去噪，生成改进的运动序列。</li>
<li>仅利用运动捕捉数据进行训练，无需图像数据。</li>
<li>融入直觉物理知识，包括关键运动状态和关联的运动约束，提升扩散模型的性能。</li>
<li>该方法显著提高帧重建效果，实现了对现有技术的超越。</li>
<li>提出的模型在现有基准测试中表现卓越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01835">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3c551d727303d9c5a35a456988f008eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8402cebb6ad0346899d7b1fbb7e725af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a0d3842ea4749725586e26543e02ca77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0b9c5e71adde779b6ee199136ec02ef.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8fc93dae40350ae654866c9959188303.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DisCo3D-Distilling-Multi-View-Consistency-for-3D-Scene-Editing"><a href="#DisCo3D-Distilling-Multi-View-Consistency-for-3D-Scene-Editing" class="headerlink" title="DisCo3D: Distilling Multi-View Consistency for 3D Scene Editing"></a>DisCo3D: Distilling Multi-View Consistency for 3D Scene Editing</h2><p><strong>Authors:Yufeng Chi, Huimin Ma, Kafeng Wang, Jianmin Li</strong></p>
<p>While diffusion models have demonstrated remarkable progress in 2D image generation and editing, extending these capabilities to 3D editing remains challenging, particularly in maintaining multi-view consistency. Classical approaches typically update 3D representations through iterative refinement based on a single editing view. However, these methods often suffer from slow convergence and blurry artifacts caused by cross-view inconsistencies. Recent methods improve efficiency by propagating 2D editing attention features, yet still exhibit fine-grained inconsistencies and failure modes in complex scenes due to insufficient constraints. To address this, we propose \textbf{DisCo3D}, a novel framework that distills 3D consistency priors into a 2D editor. Our method first fine-tunes a 3D generator using multi-view inputs for scene adaptation, then trains a 2D editor through consistency distillation. The edited multi-view outputs are finally optimized into 3D representations via Gaussian Splatting. Experimental results show DisCo3D achieves stable multi-view consistency and outperforms state-of-the-art methods in editing quality. </p>
<blockquote>
<p>虽然扩散模型在二维图像生成和编辑方面取得了显著的进步，但将这些能力扩展到三维编辑仍然具有挑战性，尤其是在保持多视图一致性方面。传统方法通常基于单个编辑视图通过迭代优化来更新三维表示。然而，这些方法通常存在收敛速度慢、由于跨视图不一致导致图像模糊等缺陷。最近的方法通过传播二维编辑注意力特征来提高效率，但由于约束不足，在复杂场景中仍然存在细微的不一致性和失败模式。为了解决这一问题，我们提出了新型框架DisCo3D，它通过蒸馏三维一致性先验知识来辅助二维编辑器。我们的方法首先使用多视图输入对三维生成器进行微调，以适应场景，然后通过一致性蒸馏来训练二维编辑器。最后，编辑后的多视图输出通过高斯拼贴优化为三维表示。实验结果表明，DisCo3D实现了稳定的多视图一致性，并在编辑质量方面优于最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01684v1">PDF</a> 17 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为DisCo3D的新框架，它通过蒸馏3D一致性先验知识来改进二维编辑器在三维编辑中的性能。该框架通过微调三维生成器以适应场景，然后训练二维编辑器以保证一致性。最后，通过高斯映射优化编辑后的多视角输出到三维表示。DisCo3D实现了稳定的多视角一致性，并在编辑质量上超越了现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在二维图像生成和编辑方面取得了显著进展，但在三维编辑方面的扩展仍然具有挑战性，尤其是在保持多视角一致性方面。</li>
<li>经典方法通常通过基于单一编辑视角的迭代优化来更新三维表示，但常常面临收敛速度慢和跨视角不一致导致的模糊伪影问题。</li>
<li>最近的方法通过传播二维编辑注意力特征提高了效率，但由于约束不足，在复杂场景中仍然存在细微的不一致性和失败模式。</li>
<li>DisCo3D框架通过蒸馏3D一致性先验知识到二维编辑器来解决这些问题。</li>
<li>DisCo3D首先使用多视角输入微调三维生成器以适应场景。</li>
<li>然后，它训练二维编辑器以保证一致性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01684">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4a117273ec868473f031e8060cd8087f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3b9873b5aec74e27324485962ade360a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-063a346421ade1f5bd884a5f90321bba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b2beefad62b673bb452dfaf939621967.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="StrandDesigner-Towards-Practical-Strand-Generation-with-Sketch-Guidance"><a href="#StrandDesigner-Towards-Practical-Strand-Generation-with-Sketch-Guidance" class="headerlink" title="StrandDesigner: Towards Practical Strand Generation with Sketch Guidance"></a>StrandDesigner: Towards Practical Strand Generation with Sketch Guidance</h2><p><strong>Authors:Na Zhang, Moran Li, Chengming Xu, Han Feng, Xiaobin Hu, Jiangning Zhang, Weijian Cao, Chengjie Wang, Yanwei Fu</strong></p>
<p>Realistic hair strand generation is crucial for applications like computer graphics and virtual reality. While diffusion models can generate hairstyles from text or images, these inputs lack precision and user-friendliness. Instead, we propose the first sketch-based strand generation model, which offers finer control while remaining user-friendly. Our framework tackles key challenges, such as modeling complex strand interactions and diverse sketch patterns, through two main innovations: a learnable strand upsampling strategy that encodes 3D strands into multi-scale latent spaces, and a multi-scale adaptive conditioning mechanism using a transformer with diffusion heads to ensure consistency across granularity levels. Experiments on several benchmark datasets show our method outperforms existing approaches in realism and precision. Qualitative results further confirm its effectiveness. Code will be released at <a target="_blank" rel="noopener" href="https://github.com/fighting-Zhang/StrandDesigner">GitHub</a>. </p>
<blockquote>
<p>真实感的发丝生成对于计算机图形和虚拟现实等应用至关重要。虽然扩散模型可以从文本或图像生成发型，但这些输入缺乏精确性和用户友好性。相反，我们提出了基于草图的首个发丝生成模型，该模型在保持用户友好的同时提供了更精细的控制。我们的框架通过两个主要创新解决了建模复杂发丝交互和多样草图模式等关键挑战：一种可学习的发丝上采样策略，将3D发丝编码到多尺度潜在空间中；以及使用带有扩散头的变压器确保跨粒度层次一致性的多尺度自适应调节机制。在几个基准数据集上的实验表明，我们的方法在真实感和精确度方面优于现有方法。定性结果进一步证实了其有效性。代码将在GitHub上发布（<a target="_blank" rel="noopener" href="https://github.com/fighting-Zhang/StrandDesigner">GitHub链接</a>）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01650v1">PDF</a> Accepted to ACM Multimedia 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于草图技术的发丝生成模型，解决了传统扩散模型缺乏精度和用户友好性的问题。通过采用学习发丝上采样策略和自适应调节机制等技术手段，实现了发丝模型的精细控制和逼真生成。实验证明，该方法在真实感和精确度上超越了现有方法，可广泛应用于计算机图形和虚拟现实领域。代码已上传至GitHub仓库。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文提出了首个基于草图技术的发丝生成模型，实现了精细控制和用户友好性。</li>
<li>通过学习发丝上采样策略和多尺度自适应调节机制，解决了复杂发丝交互和多样草图模式建模的挑战。</li>
<li>模型在多个基准数据集上的实验表现优异，实现了发丝模型的逼真生成和精细控制。</li>
<li>模型具备自适应调节机制，确保不同粒度级别的一致性。</li>
<li>定性结果证实了该模型的有效性。</li>
<li>代码已公开，方便研究者使用和改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01650">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9bb06eb8aacb4378292c8a0208510227.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e150646394efe3edea03f14a2ae9aa51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e763132e3f96ab47efe421894d80d949.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LLaDA-MedV-Exploring-Large-Language-Diffusion-Models-for-Biomedical-Image-Understanding"><a href="#LLaDA-MedV-Exploring-Large-Language-Diffusion-Models-for-Biomedical-Image-Understanding" class="headerlink" title="LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical   Image Understanding"></a>LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical   Image Understanding</h2><p><strong>Authors:Xuanzhao Dong, Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Peijie Qiu, Shao Tang, Xin Li, Yalin Wang</strong></p>
<p>Autoregressive models (ARMs) have long dominated the landscape of biomedical vision-language models (VLMs). Recently, masked diffusion models such as LLaDA have emerged as promising alternatives, yet their application in the biomedical domain remains largely underexplored. To bridge this gap, we introduce \textbf{LLaDA-MedV}, the first large language diffusion model tailored for biomedical image understanding through vision instruction tuning. LLaDA-MedV achieves relative performance gains of 7.855% over LLaVA-Med and 1.867% over LLaDA-V in the open-ended biomedical visual conversation task, and sets new state-of-the-art accuracy on the closed-form subset of three VQA benchmarks: 84.93% on VQA-RAD, 92.31% on SLAKE, and 95.15% on PathVQA. Furthermore, a detailed comparison with LLaVA-Med suggests that LLaDA-MedV is capable of generating reasonably longer responses by explicitly controlling response length, which can lead to more informative outputs. We also conduct an in-depth analysis of both the training and inference stages, highlighting the critical roles of initialization weight selection, fine-tuning strategies, and the interplay between sampling steps and response repetition. The code and model weight is released at <a target="_blank" rel="noopener" href="https://github.com/LLM-VLM-GSL/LLaDA-MedV">https://github.com/LLM-VLM-GSL/LLaDA-MedV</a>. </p>
<blockquote>
<p>自回归模型（ARMs）长期以来在生物医学视觉语言模型（VLMs）领域占据主导地位。最近，如LLaDA之类的掩模扩散模型已成为有前景的替代品，然而它们在生物医学领域的应用仍被大大忽视。为了弥补这一空白，我们引入了针对生物医学图像理解设计的语言扩散模型LLaDA-MedV。LLaDA-MedV在开放式生物医学视觉对话任务上相对于LLaVA-Med的性能提升了7.855%，相对于LLaDA-V的性能提升了1.867%，并在三个问答基准测试集的封闭式子集上达到了最新水平的准确性：在VQA-RAD上为84.93%，在SLAKE上为92.31%，在PathVQA上为95.15%。此外，与LLaVA-Med的详细比较表明，LLaDA-MedV能够通过明确控制响应长度来生成更长的合理响应，从而生成更具信息量的输出。我们还深入分析了训练和推理阶段，强调了初始化权重选择、微调策略以及采样步骤与响应重复之间的相互作用的重要性。代码和模型权重已在<a target="_blank" rel="noopener" href="https://github.com/LLM-VLM-GSL/LLaDA-MedV%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/LLM-VLM-GSL/LLaDA-MedV发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01617v1">PDF</a> </p>
<p><strong>Summary</strong><br>     近期，针对生物医学图像理解的自动语言扩散模型LLaDA-MedV被提出。相较于其他模型，LLaDA-MedV在开放型生物医学视觉对话任务中具有更高的性能表现，同时在三个视觉问答基准测试中的准确率达到了最新水平。模型可以生成较长的回应并控制响应长度，提供更多信息输出。模型的关键训练与推理阶段也被深入分析。代码和模型权重已发布在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLaDA-MedV是首个针对生物医学图像理解的大型语言扩散模型，通过视觉指令微调实现。</li>
<li>LLaDA-MedV在开放型生物医学视觉对话任务中相对于LLaVA-Med和LLaDA-V有更高的性能表现。</li>
<li>LLaDA-MedV在三个视觉问答基准测试上的准确率达到了最新水平，具体数据为：VQA-RAD的84.93%，SLAKE的92.31%，PathVQA的95.15%。</li>
<li>LLaDA-MedV能够生成较长的回应并控制响应长度，提供更多信息输出。</li>
<li>模型的关键训练阶段包括初始化权重选择、微调策略等被详细分析。</li>
<li>模型推理阶段的采样步骤和响应重复之间的相互作用被深入探讨。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01617">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-28f50ce6fbe5010fdd218a258d6057b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cdde17c0c7023c6476ca6684b57542b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1bc842df081978fe43604c02fe95adac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c9c5b7226586101902537bd928e01f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e3a50d503fd1515247626c76e4676af3.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SDMatte-Grafting-Diffusion-Models-for-Interactive-Matting"><a href="#SDMatte-Grafting-Diffusion-Models-for-Interactive-Matting" class="headerlink" title="SDMatte: Grafting Diffusion Models for Interactive Matting"></a>SDMatte: Grafting Diffusion Models for Interactive Matting</h2><p><strong>Authors:Longfei Huang, Yu Liang, Hao Zhang, Jinwei Chen, Wei Dong, Lunde Chen, Wanyu Liu, Bo Li, Peng-Tao Jiang</strong></p>
<p>Recent interactive matting methods have shown satisfactory performance in capturing the primary regions of objects, but they fall short in extracting fine-grained details in edge regions. Diffusion models trained on billions of image-text pairs, demonstrate exceptional capability in modeling highly complex data distributions and synthesizing realistic texture details, while exhibiting robust text-driven interaction capabilities, making them an attractive solution for interactive matting. To this end, we propose SDMatte, a diffusion-driven interactive matting model, with three key contributions. First, we exploit the powerful priors of diffusion models and transform the text-driven interaction capability into visual prompt-driven interaction capability to enable interactive matting. Second, we integrate coordinate embeddings of visual prompts and opacity embeddings of target objects into U-Net, enhancing SDMatte’s sensitivity to spatial position information and opacity information. Third, we propose a masked self-attention mechanism that enables the model to focus on areas specified by visual prompts, leading to better performance. Extensive experiments on multiple datasets demonstrate the superior performance of our method, validating its effectiveness in interactive matting. Our code and model are available at <a target="_blank" rel="noopener" href="https://github.com/vivoCameraResearch/SDMatte">https://github.com/vivoCameraResearch/SDMatte</a>. </p>
<blockquote>
<p>近期交互式抠图方法已经展现出捕捉物体主要区域的满意性能，但在边缘区域提取精细细节方面存在不足。扩散模型经过数十亿图像文本对的训练，在建模高度复杂的数据分布和合成逼真的纹理细节方面表现出卓越的能力，同时展现出稳健的文本驱动交互能力，使其成为交互式抠图的理想解决方案。为此，我们提出SDMatte，一种基于扩散的交互式抠图模型，有三个主要贡献。首先，我们利用扩散模型的有力先验，将文本驱动的交互能力转化为视觉提示驱动的交互能力，以实现交互式抠图。其次，我们将视觉提示的坐标嵌入和目标对象的透明度嵌入集成到U-Net中，提高SDMatte对空间位置信息和透明度信息的敏感度。第三，我们提出了一种掩膜自注意力机制，使模型能够关注视觉提示指定的区域，从而提高性能。在多个数据集上的广泛实验证明了我们方法的优越性，验证了其在交互式抠图中的有效性。我们的代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/vivocameraresearch/SDMatte%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/vivoCameraResearch/SDMatte上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.00443v2">PDF</a> Accepted at ICCV 2025, 11 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>文本提出了一种名为SDMatte的基于扩散模型的交互式抠图方法，具有三大贡献。首先，利用扩散模型的强大先验知识，将文本驱动交互能力转化为视觉提示驱动交互能力，实现交互式抠图。其次，将视觉提示的坐标嵌入和目标对象的透明度嵌入整合到U-Net中，提高SDMatte对空间位置信息和透明度信息的敏感性。最后，提出了一种掩膜自注意力机制，使模型能够关注视觉提示指定的区域，从而提高性能。该方法在多个数据集上的实验验证了其优越性，展示了在交互式抠图任务中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型具有建模复杂数据分布和合成逼真纹理细节的能力，展现出在交互式抠图领域的潜力。</li>
<li>SDMatte模型利用扩散模型的先验知识，实现文本驱动交互能力到视觉提示驱动交互能力的转化。</li>
<li>通过整合视觉提示的坐标嵌入和目标对象的透明度嵌入到U-Net中，SDMatte提高了对空间位置信息和透明度信息的敏感性。</li>
<li>SDMatte提出的掩膜自注意力机制能关注视觉提示指定的区域，进一步提高性能。</li>
<li>相较于传统方法，SDMatte在多个数据集上的实验表现出卓越性能。</li>
<li>SDMatte模型的有效性已在实践中得到验证，可应用于交互式抠图任务。</li>
<li>SDMatte模型的代码和模型已公开在<a target="_blank" rel="noopener" href="https://github.com/vivoCameraResearch/SDMatte%E3%80%82">https://github.com/vivoCameraResearch/SDMatte。</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.00443">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5c299ac451cfd72b5f217f2b256dac14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd6f9f4704e803c5d52b6bbbf3772fac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88feef2fd2525d079f43b631a707aabb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfeaf2427b1a6fbff11e3f4d65328765.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Inversion-DPO-Precise-and-Efficient-Post-Training-for-Diffusion-Models"><a href="#Inversion-DPO-Precise-and-Efficient-Post-Training-for-Diffusion-Models" class="headerlink" title="Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models"></a>Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models</h2><p><strong>Authors:Zejian Li, Yize Li, Chenye Meng, Zhongni Liu, Yang Ling, Shengyuan Zhang, Guang Yang, Changyuan Yang, Zhiyuan Yang, Lingyun Sun</strong></p>
<p>Recent advancements in diffusion models (DMs) have been propelled by alignment methods that post-train models to better conform to human preferences. However, these approaches typically require computation-intensive training of a base model and a reward model, which not only incurs substantial computational overhead but may also compromise model accuracy and training efficiency. To address these limitations, we propose Inversion-DPO, a novel alignment framework that circumvents reward modeling by reformulating Direct Preference Optimization (DPO) with DDIM inversion for DMs. Our method conducts intractable posterior sampling in Diffusion-DPO with the deterministic inversion from winning and losing samples to noise and thus derive a new post-training paradigm. This paradigm eliminates the need for auxiliary reward models or inaccurate appromixation, significantly enhancing both precision and efficiency of training. We apply Inversion-DPO to a basic task of text-to-image generation and a challenging task of compositional image generation. Extensive experiments show substantial performance improvements achieved by Inversion-DPO compared to existing post-training methods and highlight the ability of the trained generative models to generate high-fidelity compositionally coherent images. For the post-training of compostitional image geneation, we curate a paired dataset consisting of 11,140 images with complex structural annotations and comprehensive scores, designed to enhance the compositional capabilities of generative models. Inversion-DPO explores a new avenue for efficient, high-precision alignment in diffusion models, advancing their applicability to complex realistic generation tasks. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/MIGHTYEZ/Inversion-DPO">https://github.com/MIGHTYEZ/Inversion-DPO</a> </p>
<blockquote>
<p>近期扩散模型（DMs）的进展得益于通过对模型进行后训练以更好符合人类偏好的对齐方法。然而，这些方法通常需要计算密集的基础模型和奖励模型的训练，这不仅产生了大量的计算开销，还可能影响模型的准确性和训练效率。为了解决这些局限性，我们提出了Inversion-DPO，这是一种新的对齐框架，它通过重新制定扩散模型的直接偏好优化（DPO）与DDIM反转来规避奖励建模。我们的方法通过从获胜和失败样本到噪声的确定性反转来进行难以捉摸的后验采样，从而开创了新的后训练范式。这种方法消除了对辅助奖励模型或不精确近似计算的需求，显著提高了训练的准确性和效率。我们将Inversion-DPO应用于文本到图像生成的基本任务以及具有挑战性的组合图像生成任务。大量实验表明，与现有的后训练方法相比，Inversion-DPO取得了显著的性能提升，并展示了训练好的生成模型在生成高保真组合连贯图像方面的能力。对于组合图像生成的后训练，我们整理了一个包含11,140张带有复杂结构注释和全面分数的图像配对数据集，旨在提高生成模型的组合能力。Inversion-DPO为扩散模型的高效高精度对齐探索了一条新途径，提高了其在复杂现实生成任务中的适用性。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/MIGHTYEZ/Inversion-DPO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/MIGHTYEZ/Inversion-DPO找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11554v4">PDF</a> Accepted by ACM MM25</p>
<p><strong>Summary</strong></p>
<p>本文介绍了扩散模型的新进展，提出了一种名为Inversion-DPO的新型对齐框架，该框架通过改革Direct Preference Optimization (DPO)与DDIM反演方法，规避了奖励建模，从而实现了扩散模型的后训练。新方法通过从获胜和失败样本中确定性地反演噪声来进行不可行后采样，开创了无需辅助奖励模型或不精确近似的新后训练范式。此范式提高了训练和精度效率。应用于文本到图像生成和基本任务以及更具挑战性的组合图像生成任务中，实验结果显示Inversion-DPO相较于现有后训练方法实现了显著的性能提升，并展示了生成模型生成高保真组合图像的能力。此外，为后训练组合图像生成，研究团队创建了一个包含复杂结构注释和评分的一对一图像数据集，旨在增强生成模型的组合能力。该研究为扩散模型的高效高精度对齐探索了新途径，提高了其在复杂现实生成任务中的应用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型（DMs）的最新进展通过对齐方法来更好地符合人类偏好。</li>
<li>当前方法需要计算密集的训练基础模型和奖励模型，带来计算开销并可能影响准确性和效率。</li>
<li>提出了一种新型对齐框架Inversion-DPO，通过改革Direct Preference Optimization (DPO)与DDIM反演规避奖励建模。</li>
<li>Inversion-DPO实现了无需辅助奖励模型或不精确近似的新后训练范式。</li>
<li>该方法应用于文本到图像生成和基本任务以及更具挑战性的组合图像生成任务。</li>
<li>Inversion-DPO显著提高了性能，并展示了生成高保真组合图像的能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11554">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-abe37bd42027c69ae7d649919520a765.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cea410f71a40b6a96c766d9d8f9c525c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-197a30615846b520486ff827a583fc56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-516aae4842a1f258e5d171d9ef798f74.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Text-Embedding-Knows-How-to-Quantize-Text-Guided-Diffusion-Models"><a href="#Text-Embedding-Knows-How-to-Quantize-Text-Guided-Diffusion-Models" class="headerlink" title="Text Embedding Knows How to Quantize Text-Guided Diffusion Models"></a>Text Embedding Knows How to Quantize Text-Guided Diffusion Models</h2><p><strong>Authors:Hongjae Lee, Myungjun Son, Dongjea Kang, Seung-Won Jung</strong></p>
<p>Despite the success of diffusion models in image generation tasks such as text-to-image, the enormous computational complexity of diffusion models limits their use in resource-constrained environments. To address this, network quantization has emerged as a promising solution for designing efficient diffusion models. However, existing diffusion model quantization methods do not consider input conditions, such as text prompts, as an essential source of information for quantization. In this paper, we propose a novel quantization method dubbed Quantization of Language-to-Image diffusion models using text Prompts (QLIP). QLIP leverages text prompts to guide the selection of bit precision for every layer at each time step. In addition, QLIP can be seamlessly integrated into existing quantization methods to enhance quantization efficiency. Our extensive experiments demonstrate the effectiveness of QLIP in reducing computational complexity and improving the quality of the generated images across various datasets. </p>
<blockquote>
<p>尽管扩散模型在文本到图像等图像生成任务中取得了成功，但其巨大的计算复杂度限制了其在资源受限环境中的应用。为了解决这一问题，网络量化作为设计高效扩散模型的有前途的解决方案而出现。然而，现有的扩散模型量化方法没有考虑输入条件，如文本提示，作为量化的重要信息来源。在本文中，我们提出了一种新的量化方法，称为使用文本提示的语言到图像扩散模型量化（QLIP）。QLIP利用文本提示来指导每个时间步长每一层的位精度的选择。此外，QLIP可以无缝集成到现有的量化方法中，以提高量化的效率。我们的广泛实验表明，QLIP在降低计算复杂度和提高各种数据集生成图像的质量方面非常有效。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10340v3">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>扩散模型在网络量化技术的帮助下，在图像生成任务中取得了显著的成功，尤其是在文本到图像的任务中。然而，现有的扩散模型量化方法往往忽略了输入条件，如文本提示，作为量化的重要信息来源。本文提出了一种新的量化方法——利用文本提示的文本到图像扩散模型的量化（QLIP）。QLIP利用文本提示指导每一层每一步的位精度选择，并可以无缝集成到现有的量化方法中以提高量化效率。实验证明，QLIP在降低计算复杂度和提高生成图像质量方面效果显著。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像生成任务中取得了成功，特别是在文本到图像的任务中。</li>
<li>网络量化是解决扩散模型在资源受限环境中使用的重要方法。</li>
<li>现有扩散模型量化方法忽略了文本提示等输入条件。</li>
<li>QLIP是一种新的量化方法，利用文本提示指导每一层每一步的位精度选择。</li>
<li>QLIP可以无缝集成到现有的量化方法中以提高量化效率。</li>
<li>QLIP在降低计算复杂度和提高生成图像质量方面效果显著。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10340">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fe771c8e544f4bf2be8acbd1f0795ed4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e64aa29e95b9721956b28ec3eaf96ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2aa3b77751ac59de3a65f3cd6e077e15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74ec4d790da943e1abb6c91946f18c89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7270cdb683832881bd87b1ecbc6665f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3efef1a659ef791352fd6decd7a2b92a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Subjective-Camera-0-1-Bridging-Human-Cognition-and-Visual-Reconstruction-through-Sequence-Aware-Sketch-Guided-Diffusion"><a href="#Subjective-Camera-0-1-Bridging-Human-Cognition-and-Visual-Reconstruction-through-Sequence-Aware-Sketch-Guided-Diffusion" class="headerlink" title="Subjective Camera 0.1: Bridging Human Cognition and Visual   Reconstruction through Sequence-Aware Sketch-Guided Diffusion"></a>Subjective Camera 0.1: Bridging Human Cognition and Visual   Reconstruction through Sequence-Aware Sketch-Guided Diffusion</h2><p><strong>Authors:Haoyang Chen, Dongfang Sun, Caoyuan Ma, Shiqin Wang, Kewei Zhang, Zheng Wang, Zhixiang Wang</strong></p>
<p>We introduce the concept of a subjective camera to reconstruct meaningful moments that physical cameras fail to capture. We propose Subjective Camera 0.1, a framework for reconstructing real-world scenes from readily accessible subjective readouts, i.e., textual descriptions and progressively drawn rough sketches. Built on optimization-based alignment of diffusion models, our approach avoids large-scale paired training data and mitigates generalization issues. To address the challenge of integrating multiple abstract concepts in real-world scenarios, we design a Sequence-Aware Sketch-Guided Diffusion framework with three loss terms for concept-wise sequential optimization, following the natural order of subjective readouts. Experiments on two datasets demonstrate that our method achieves state-of-the-art performance in image quality as well as spatial and semantic alignment with target scenes. User studies with 40 participants further confirm that our approach is consistently preferred.Our project page is at: subjective-camera.github.io </p>
<blockquote>
<p>我们引入主观相机的概念，以重建物理相机未能捕捉的有意义时刻。我们提出主观相机0.1，这是一个从易于获取的主观读数（如文本描述和逐步绘制的粗略草图）重建现实场景的框架。我们的方法建立在基于优化的扩散模型对齐之上，避免了大规模配对训练数据，并缓解了泛化问题。为了解决在现实场景中整合多个抽象概念的挑战，我们设计了一个序列感知草图引导扩散框架，包含三个用于概念级顺序优化的损失项，遵循主观读数的自然顺序。在两个数据集上的实验表明，我们的方法在图像质量以及空间和时间与目标的场景语义对齐方面达到了最新技术水平。有40名参与者参与的用户研究进一步证实了我们方法的优越性。我们的项目页面是：<a target="_blank" rel="noopener" href="http://subjective-camera.github.io/">主观相机官网</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23711v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究提出了一种主观相机概念，用以重建物理相机无法捕捉的有意义时刻。我们提出了主观相机0.1框架，通过优化基于扩散模型的对齐方式，从易获取的主观读数（如文本描述和粗略草图）重建现实场景。为解决现实场景中整合多个抽象概念的问题，我们设计了序列感知草图引导扩散框架，包含三个用于概念级顺序优化的损失项，遵循主观读数的自然顺序。在两项数据集上的实验表明，我们的方法在图像质量以及空间与语义对齐目标场景方面达到领先水平。40名参与者的用户研究进一步证实了我们方法的优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入主观相机概念，旨在捕捉物理相机无法捕捉的有意义时刻。</li>
<li>提出Subjective Camera 0.1框架，利用文本描述和粗略草图重建现实场景。</li>
<li>基于优化对齐的扩散模型，无需大规模配对训练数据，减轻泛化问题。</li>
<li>设计序列感知草图引导扩散框架，通过三个损失项进行概念级顺序优化。</li>
<li>框架遵循主观读数的自然顺序，能更准确地重建场景。</li>
<li>在两个数据集上的实验证明，该方法在图像质量及空间、语义对齐方面表现优异。</li>
<li>用户研究证实该方法的一致优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23711">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-46c533685f4ea60e686461646b060e85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-273edf3397d44f1bd6368dabdd328f5b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-05946393eab0194835ba32079040353e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7541b8eec6eaf4332230c5985404242c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5786d8cc3d1b7ddced0faa54d6838377.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DIP-Unsupervised-Dense-In-Context-Post-training-of-Visual-Representations"><a href="#DIP-Unsupervised-Dense-In-Context-Post-training-of-Visual-Representations" class="headerlink" title="DIP: Unsupervised Dense In-Context Post-training of Visual   Representations"></a>DIP: Unsupervised Dense In-Context Post-training of Visual   Representations</h2><p><strong>Authors:Sophia Sirko-Galouchenko, Spyros Gidaris, Antonin Vobecky, Andrei Bursuc, Nicolas Thome</strong></p>
<p>We introduce DIP, a novel unsupervised post-training method designed to enhance dense image representations in large-scale pretrained vision encoders for in-context scene understanding. Unlike prior approaches that rely on complex self-distillation architectures, our method trains the vision encoder using pseudo-tasks that explicitly simulate downstream in-context scenarios, inspired by meta-learning principles. To enable post-training on unlabeled data, we propose an automatic mechanism for generating in-context tasks that combines a pretrained diffusion model and the vision encoder itself. DIP is simple, unsupervised, and computationally efficient, requiring less than 9 hours on a single A100 GPU. By learning dense representations through pseudo in-context tasks, it achieves strong performance across a wide variety of downstream real-world in-context scene understanding tasks. It outperforms both the initial vision encoder and prior methods, offering a practical and effective solution for improving dense representations. Code available here: <a target="_blank" rel="noopener" href="https://github.com/sirkosophia/DIP">https://github.com/sirkosophia/DIP</a> </p>
<blockquote>
<p>我们介绍了DIP，这是一种新型的无监督后训练法，旨在增强大规模预训练视觉编码器的密集图像表示，以进行上下文场景理解。不同于依赖复杂自我蒸馏架构的先前方法，我们的方法使用伪任务来明确模拟下游上下文场景，这是受元学习原理的启发。为了在无标签数据上进行后训练，我们提出了一种生成上下文任务的自动机制，该机制结合了预训练的扩散模型和视觉编码器本身。DIP简单、无监督且计算高效，在单个A100 GPU上不到9小时即可完成。它通过伪上下文任务学习密集表示，在多种下游现实世界上下文场景理解任务中表现出强大的性能。它优于初始的视觉编码器和先前的方法，为改进密集表示提供了实用有效的解决方案。代码可在<a target="_blank" rel="noopener" href="https://github.com/sirkosophia/DIP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sirkosophia/DIP找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18463v2">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong><br>     本文提出了DIP，一种新型无监督的后训练技术，旨在增强大规模预训练视觉编码器的密集图像表示，以进行上下文场景理解。DIP采用伪任务模拟下游上下文场景，受元学习原理启发。通过预训练的扩散模型和视觉编码器本身生成上下文任务，实现无监督数据后训练。DIP简单、高效，可在单个A100 GPU上不到9小时内完成。通过伪上下文任务学习密集表示，在多种下游现实场景理解任务中表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DIP是一种无监督后训练技术，用于增强预训练视觉编码器的图像表示能力。</li>
<li>DIP通过模拟下游上下文场景生成伪任务，从而增强视觉编码器的性能。</li>
<li>DIP采用元学习原理，利用预训练的扩散模型和视觉编码器生成上下文任务。</li>
<li>DIP在单个A100 GPU上计算效率高，训练时间短。</li>
<li>DIP在多种下游现实场景理解任务中表现优异，优于初始视觉编码器和先前方法。</li>
<li>DIP适用于改善密集表示，为场景理解提供了一种实用有效的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18463">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-686437810a21a2d926ca209b4c8c2986.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecda420d1beeeb84656ca8879d4dab5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c444de8bbf8e99e5a3990aec6a54c98e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9170775b531b262f2d0192e97ac782e2.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DanceGRPO-Unleashing-GRPO-on-Visual-Generation"><a href="#DanceGRPO-Unleashing-GRPO-on-Visual-Generation" class="headerlink" title="DanceGRPO: Unleashing GRPO on Visual Generation"></a>DanceGRPO: Unleashing GRPO on Visual Generation</h2><p><strong>Authors:Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, Ping Luo</strong></p>
<p>Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face critical limitations: incompatibility with modern Ordinary Differential Equations (ODEs)-based sampling paradigms, instability in large-scale training, and lack of validation for video generation. This paper introduces DanceGRPO, the first unified framework to adapt Group Relative Policy Optimization (GRPO) to visual generation paradigms, unleashing one unified RL algorithm across two generative paradigms (diffusion models and rectified flows), three tasks (text-to-image, text-to-video, image-to-video), four foundation models (Stable Diffusion, HunyuanVideo, FLUX, SkyReels-I2V), and five reward models (image&#x2F;video aesthetics, text-image alignment, video motion quality, and binary reward). To our knowledge, DanceGRPO is the first RL-based unified framework capable of seamless adaptation across diverse generative paradigms, tasks, foundational models, and reward models. DanceGRPO demonstrates consistent and substantial improvements, which outperform baselines by up to 181% on benchmarks such as HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can stabilize policy optimization for complex video generation, but also enables generative policy to better capture denoising trajectories for Best-of-N inference scaling and learn from sparse binary feedback. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis. The code will be released. </p>
<blockquote>
<p>近期生成模型——尤其是扩散模型和修正流——的突破为视觉内容创作带来了革命性的变化，但如何使模型输出符合人类偏好仍然是一个关键挑战。现有的基于强化学习（RL）的视觉生成方法面临重要局限：与现代基于常微分方程（ODE）的采样范式不兼容、大规模训练不稳定、以及视频生成的验证缺乏。本文介绍了DanceGRPO，这是第一个将集团相对政策优化（GRPO）适应于视觉生成范式的统一框架，释放了一种统一的强化学习算法，该算法涵盖了两种生成范式（扩散模型和修正流）、三种任务（文本到图像、文本到视频、图像到视频）、四种基础模型（Stable Diffusion、HuYuanVideo、FLUX、SkyReels-I2V），以及五种奖励模型（图像&#x2F;视频美学、文本-图像对齐、视频运动质量、二元奖励）。据我们所知，DanceGRPO是第一个能够在多种生成范式、任务、基础模型和奖励模型之间无缝适应的基于强化学习的统一框架。DanceGRPO表现出持续且显著的改进，在HPS-v2.1、CLIP Score、VideoAlign和GenEval等基准测试上的表现优于基线高达181%。值得一提的是，DanceGRPO不仅能够稳定复杂视频生成的策略优化，还能够使生成策略更好地捕捉去噪轨迹，用于Best-of-N推理扩展，并从稀疏的二元反馈中学习。我们的结果确立了DanceGRPO在视觉生成中扩展强化学习从人类反馈（RLHF）任务的稳健性和通用性解决方案，为强化学习与合成视觉的和谐融合提供了新的见解。代码将被发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07818v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://dancegrpo.github.io/">https://dancegrpo.github.io/</a></p>
<p><strong>摘要</strong></p>
<p>近期生成模型（如扩散模型和校正流）的突破为视觉内容创作带来革命性变化，但如何使模型输出与人类偏好对齐仍是关键挑战。现有基于强化学习（RL）的视觉生成方法存在与现代基于常微分方程（ODEs）的采样范式不兼容、大规模训练不稳定、视频生成验证缺乏等问题。本文引入DanceGRPO框架，首次将组相对策略优化（GRPO）适应于视觉生成范式，实现统一RL算法在两种生成范式（扩散模型和校正流）、三种任务（文本转图像、文本转视频、图像转视频）、四种基础模型（Stable Diffusion、HunyuanVideo等）和五种奖励模型中的灵活应用。据我们所知，DanceGRPO是首个能够无缝适应多种生成范式、任务、基础模型和奖励模型的RL统一框架。DanceGRPO在HPS-v2.1等基准测试中实现了与基线相比最高达181%的持续和实质性改进，同时显著提升了视频生成的策略优化稳定性，并能够更好地捕捉去噪轨迹用于Best-of-N推理扩展和从稀疏二元反馈中学习。我们的研究结果表明，DanceGRPO为强化学习从人类反馈（RLHF）任务在视觉生成方面的应用提供了稳健且通用的解决方案，为强化学习与视觉合成的和谐融合提供了新的见解。代码将公开发布。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>突破性的生成模型（如扩散模型和校正流）在视觉内容创作领域具有革命性影响。</li>
<li>模型输出与人类偏好对齐仍是当前的重要挑战。</li>
<li>现有基于强化学习的视觉生成方法面临多种挑战，如与现代采样范式不兼容、训练不稳定、视频生成验证缺乏等。</li>
<li>引入DanceGRPO框架，首次实现统一RL算法在多种生成范式、任务、基础模型和奖励模型中的应用。</li>
<li>DanceGRPO实现了显著的性能改进，并在视频生成的政策优化稳定性方面有所提升。</li>
<li>DanceGRPO能够捕捉去噪轨迹用于Best-of-N推理扩展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07818">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-54a33747f280eac8850e1f5b661270a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8281f31d36f5e8180d43b3673a688594.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf339ab349d00a486a97d8d64e6ec85c.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DRC-Enhancing-Personalized-Image-Generation-via-Disentangled-Representation-Composition"><a href="#DRC-Enhancing-Personalized-Image-Generation-via-Disentangled-Representation-Composition" class="headerlink" title="DRC: Enhancing Personalized Image Generation via Disentangled   Representation Composition"></a>DRC: Enhancing Personalized Image Generation via Disentangled   Representation Composition</h2><p><strong>Authors:Yiyan Xu, Wuqiang Zheng, Wenjie Wang, Fengbin Zhu, Xinting Hu, Yang Zhang, Fuli Feng, Tat-Seng Chua</strong></p>
<p>Personalized image generation has emerged as a promising direction in multimodal content creation. It aims to synthesize images tailored to individual style preferences (e.g., color schemes, character appearances, layout) and semantic intentions (e.g., emotion, action, scene contexts) by leveraging user-interacted history images and multimodal instructions. Despite notable progress, existing methods – whether based on diffusion models, large language models, or Large Multimodal Models (LMMs) – struggle to accurately capture and fuse user style preferences and semantic intentions. In particular, the state-of-the-art LMM-based method suffers from the entanglement of visual features, leading to Guidance Collapse, where the generated images fail to preserve user-preferred styles or reflect the specified semantics.   To address these limitations, we introduce DRC, a novel personalized image generation framework that enhances LMMs through Disentangled Representation Composition. DRC explicitly extracts user style preferences and semantic intentions from history images and the reference image, respectively, to form user-specific latent instructions that guide image generation within LMMs. Specifically, it involves two critical learning stages: 1) Disentanglement learning, which employs a dual-tower disentangler to explicitly separate style and semantic features, optimized via a reconstruction-driven paradigm with difficulty-aware importance sampling; and 2) Personalized modeling, which applies semantic-preserving augmentations to effectively adapt the disentangled representations for robust personalized generation. Extensive experiments on two benchmarks demonstrate that DRC shows competitive performance while effectively mitigating the guidance collapse issue, underscoring the importance of disentangled representation learning for controllable and effective personalized image generation. </p>
<blockquote>
<p>个性化图像生成已成为多媒体内容创建的一个前景广阔的方向。它旨在通过利用用户交互的历史图像和多模态指令，合成符合个人风格偏好（如色彩方案、角色外观、布局）和语义意图（如情感、动作、场景上下文）的图像。尽管取得了显著的进展，但现有方法——无论是基于扩散模型、大型语言模型还是大型多媒体模型（LMM）——在准确捕捉和融合用户风格偏好和语义意图方面都存在困难。特别是最先进的基于LMM的方法受到视觉特征纠缠的影响，导致出现“指导崩溃”，生成的图像无法保持用户偏好的风格或反映指定的语义。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.17349v2">PDF</a> Accepted for publication in ACM MM’25</p>
<p><strong>Summary</strong></p>
<p>本文探讨个性化图像生成领域的研究进展，提出一种基于解纠缠表示组合（DRC）的新框架，旨在提高大型多模态模型（LMMs）在个性化图像生成方面的性能。该框架通过明确提取用户风格偏好和语义意图，形成用户特定的潜在指令，指导图像在LMMs内的生成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>个人化图像生成成为多模态内容创建的有前途的方向。</li>
<li>现有方法很难捕捉和融合用户的风格偏好和语义意图。</li>
<li>当前领先的大型多模态模型（LMMs）面临视觉特征纠缠的问题，导致生成的图像无法保持用户偏好的风格或反映指定的语义。</li>
<li>DRC框架旨在通过解纠缠表示组合增强LMMs在个性化图像生成方面的性能。</li>
<li>DRC框架包括两个关键学习阶段：解纠缠学习和个性化建模。</li>
<li>解纠缠学习阶段采用双塔解缠器明确分离风格与语义特征，并通过重建驱动范式和优化难度感知重要性采样进行优化。</li>
<li>个性化建模阶段应用语义保留增强，有效适应解缠表示，以实现稳健的个性化生成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.17349">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e5b91d6fce2cd7856556e893d811b646.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30503c65c3aa70b9d75ba96538a9ddc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b607c4390285eae583691cc79a5e4413.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6be1655f3388411430c87c11e02910a7.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Efficient-Input-level-Backdoor-Defense-on-Text-to-Image-Synthesis-via-Neuron-Activation-Variation"><a href="#Efficient-Input-level-Backdoor-Defense-on-Text-to-Image-Synthesis-via-Neuron-Activation-Variation" class="headerlink" title="Efficient Input-level Backdoor Defense on Text-to-Image Synthesis via   Neuron Activation Variation"></a>Efficient Input-level Backdoor Defense on Text-to-Image Synthesis via   Neuron Activation Variation</h2><p><strong>Authors:Shengfang Zhai, Jiajun Li, Yue Liu, Huanran Chen, Zhihua Tian, Wenjie Qu, Qingni Shen, Ruoxi Jia, Yinpeng Dong, Jiaheng Zhang</strong></p>
<p>In recent years, text-to-image (T2I) diffusion models have gained significant attention for their ability to generate high quality images reflecting text prompts. However, their growing popularity has also led to the emergence of backdoor threats, posing substantial risks. Currently, effective defense strategies against such threats are lacking due to the diversity of backdoor targets in T2I synthesis. In this paper, we propose NaviT2I, an efficient input-level backdoor defense framework against diverse T2I backdoors. Our approach is based on the new observation that trigger tokens tend to induce significant neuron activation variation in the early stage of the diffusion generation process, a phenomenon we term Early-step Activation Variation. Leveraging this insight, NaviT2I navigates T2I models to prevent malicious inputs by analyzing Neuron activation variations caused by input tokens. Extensive experiments show that NaviT2I significantly outperforms the baselines in both effectiveness and efficiency across diverse datasets, various T2I backdoors, and different model architectures including UNet and DiT. Furthermore, we show that our method remains effective under potential adaptive attacks. </p>
<blockquote>
<p>近年来，文本到图像（T2I）扩散模型因其能够生成反映文本提示的高质量图像而备受关注。然而，其日益普及也带来了后门威胁，带来了相当大的风险。由于T2I合成中的后门目标多样性，目前缺乏针对此类威胁的有效防御策略。在本文中，我们提出了NaviT2I，这是一种针对多种T2I后门的高效输入级后门防御框架。我们的方法基于一个新的观察结果，即触发令牌往往会在扩散生成过程的早期阶段引起神经元激活的显著变化，我们将这一现象称为“早期步骤激活变化”。利用这一见解，NaviT2I通过分析输入令牌引起的神经元激活变化，引导T2I模型防止恶意输入。大量实验表明，NaviT2I在多个数据集、多种T2I后门和不同模型架构（包括UNet和DiT）方面，在有效性和效率方面都显著优于基线。此外，我们还表明，我们的方法在潜在的适应性攻击下仍然有效。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06453v2">PDF</a> 20 pages. ICCV 2025 (Highlight)</p>
<p><strong>Summary</strong></p>
<p>文本到图像（T2I）扩散模型能够根据文本提示生成高质量图像，但近期出现的后门威胁给其带来了巨大风险。由于缺乏针对T2I合成中后门目标的防御策略，本文提出了NaviT2I，一个高效的输入级后门防御框架。NaviT2I基于新观察，触发令牌在扩散生成过程的早期阶段会引起神经元激活变化显著，我们称之为早期激活变化现象。利用这一洞察，NaviT2I通过分析输入令牌引起的神经元激活变化来防止恶意输入。实验表明，NaviT2I在多个数据集、各种T2I后门以及不同模型架构（包括UNet和DiT）上，在有效性和效率方面都显著优于基线方法，并且在潜在的自适应攻击下仍然保持有效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T2I扩散模型因能够根据文本提示生成高质量图像而受到广泛关注，但也存在后门威胁风险。</li>
<li>缺乏针对T2I合成中多样后门目标的防御策略。</li>
<li>本文提出了NaviT2I，一个高效的输入级后门防御框架，针对T2I模型进行防御。</li>
<li>NaviT2I基于新观察：触发令牌在扩散模型的早期阶段会引起神经元激活显著变化。</li>
<li>NaviT2I通过分析输入令牌引起的神经元激活变化来防止恶意输入。</li>
<li>实验表明，NaviT2I在多个方面显著优于基线方法，包括数据集、T2I后门类型、模型架构等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06453">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0add7bafaf95b3c486493ce0482f9b8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c612872c4cc38bbd8d54e5775cf2b4aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb8441ab50526dbaa1a9d5ff1b629b41.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="GAS-Generative-Avatar-Synthesis-from-a-Single-Image"><a href="#GAS-Generative-Avatar-Synthesis-from-a-Single-Image" class="headerlink" title="GAS: Generative Avatar Synthesis from a Single Image"></a>GAS: Generative Avatar Synthesis from a Single Image</h2><p><strong>Authors:Yixing Lu, Junting Dong, Youngjoong Kwon, Qin Zhao, Bo Dai, Fernando De la Torre</strong></p>
<p>We present a unified and generalizable framework for synthesizing view-consistent and temporally coherent avatars from a single image, addressing the challenging task of single-image avatar generation. Existing diffusion-based methods often condition on sparse human templates (e.g., depth or normal maps), which leads to multi-view and temporal inconsistencies due to the mismatch between these signals and the true appearance of the subject. Our approach bridges this gap by combining the reconstruction power of regression-based 3D human reconstruction with the generative capabilities of a diffusion model. In a first step, an initial 3D reconstructed human through a generalized NeRF provides comprehensive conditioning, ensuring high-quality synthesis faithful to the reference appearance and structure. Subsequently, the derived geometry and appearance from the generalized NeRF serve as input to a video-based diffusion model. This strategic integration is pivotal for enforcing both multi-view and temporal consistency throughout the avatar’s generation. Empirical results underscore the superior generalization ability of our proposed method, demonstrating its effectiveness across diverse in-domain and out-of-domain in-the-wild datasets. </p>
<blockquote>
<p>我们提出了一种统一且可推广的框架，可从单张图像中合成视角一致且时间连贯的虚拟角色，以解决单图像角色生成这一具有挑战性的任务。现有的基于扩散的方法通常依赖于稀疏的人体模板（例如深度或法线图），这会导致这些信号与主体的真实外观之间的不匹配，进而产生多视角和时间上的不一致性。我们的方法结合了基于回归的3D人体重建的重建能力与扩散模型的生成能力，以缩小这一差距。首先，通过通用的NeRF生成初始的3D重建人体，提供全面的条件，确保高质量且忠于参考外观和结构的合成。随后，从通用的NeRF中得出的几何形状和外观作为视频扩散模型的输入。这种战略性的整合对于在角色生成过程中强制执行多视角和时间一致性至关重要。经验结果强调了我们提出方法的卓越泛化能力，证明了它在各种领域内外、野生数据集上的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.06957v2">PDF</a> ICCV 2025; Project Page: <a target="_blank" rel="noopener" href="https://humansensinglab.github.io/GAS/">https://humansensinglab.github.io/GAS/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种统一且通用的框架，用于从单张图像合成视角一致、时间连贯的虚拟角色。该框架解决了基于扩散的单图像虚拟角色生成中的不一致性问题，通过结合回归型三维人体重建的重建能力与扩散模型的生成能力，确保了高质量且忠于原始外观和结构的合成。首先，通过通用NeRF进行三维人体重建，为后续合成提供全面的条件。接着，将从NeRF中得出的几何和外观信息作为视频扩散模型的输入，强制实现多视角和时间上的连贯性。经验结果表明，该方法具有出色的泛化能力，在不同领域和野外数据集上都表现出色。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种统一框架，用于从单张图像合成视角一致、时间连贯的虚拟角色。</li>
<li>解决了现有扩散模型在虚拟角色生成中的多视角和时间不一致性问题。</li>
<li>结合了回归型三维人体重建的重建能力与扩散模型的生成能力。</li>
<li>使用通用NeRF进行初始三维人体重建，为后续合成提供全面条件。</li>
<li>从NeRF中得出的几何和外观信息作为视频扩散模型的输入。</li>
<li>通过策略性整合实现了多视角和时间连贯性的强制实施。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.06957">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4051b2273865fc183efb9af8dd673d6b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7a35b0421bfdf7f0ec7680228e4b5ee9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61186df640852268c7e25db6405240d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3407eebe3546c090b7f8c0ca2900578e.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Individual-Content-and-Motion-Dynamics-Preserved-Pruning-for-Video-Diffusion-Models"><a href="#Individual-Content-and-Motion-Dynamics-Preserved-Pruning-for-Video-Diffusion-Models" class="headerlink" title="Individual Content and Motion Dynamics Preserved Pruning for Video   Diffusion Models"></a>Individual Content and Motion Dynamics Preserved Pruning for Video   Diffusion Models</h2><p><strong>Authors:Yiming Wu, Huan Wang, Zhenghao Chen, Dong Xu</strong></p>
<p>The high computational cost and slow inference time are major obstacles to deploying Video Diffusion Models (VDMs). To overcome this, we introduce a new Video Diffusion Model Compression approach using individual content and motion dynamics preserved pruning and consistency loss. First, we empirically observe that deeper VDM layers are crucial for maintaining the quality of \textbf{motion dynamics} (\textit{e.g.,} coherence of the entire video), while shallower layers are more focused on \textbf{individual content} (\textit{e.g.,} individual frames). Therefore, we prune redundant blocks from the shallower layers while preserving more of the deeper layers, resulting in a lightweight VDM variant called VDMini. Moreover, we propose an \textbf{Individual Content and Motion Dynamics (ICMD)} Consistency Loss to gain comparable generation performance as larger VDM to VDMini. In particular, we first use the Individual Content Distillation (ICD) Loss to preserve the consistency in the features of each generated frame between the teacher and student models. Next, we introduce a Multi-frame Content Adversarial (MCA) Loss to enhance the motion dynamics across the generated video as a whole. This method significantly accelerates inference time while maintaining high-quality video generation. Extensive experiments demonstrate the effectiveness of our VDMini on two important video generation tasks, Text-to-Video (T2V) and Image-to-Video (I2V), where we respectively achieve an average 2.5 $\times$, 1.4 $\times$, and 1.25 $\times$ speed up for the I2V method SF-V, the T2V method T2V-Turbo-v2, and the T2V method HunyuanVideo, while maintaining the quality of the generated videos on several benchmarks including UCF101, VBench-T2V, and VBench-I2V. </p>
<blockquote>
<p>视频扩散模型（VDM）的高计算成本和缓慢的推理时间是其主要部署障碍。为了克服这一障碍，我们引入了一种新的视频扩散模型压缩方法，该方法保留了个人内容和运动动力学的修剪和一致性损失。首先，我们经验性地观察到，较深的VDM层对于维持运动动力学的质量（例如整个视频的一致性）至关重要，而较浅的层则更专注于个人内容（例如单个帧）。因此，我们从较浅的层中修剪掉多余的部分，同时保留更多的深层结构，从而得到一种轻量级的VDM变体，称为VDMini。此外，我们提出了一种个人内容和运动动力学（ICMD）一致性损失，以在大型VDM与VDMini之间获得相当的生成性能。具体来说，我们首先使用个人内容蒸馏（ICD）损失来保持教师模型和学生模型之间每个生成帧的特征一致性。接下来，我们引入了一种多帧内容对抗（MCA）损失，以提高整个生成视频的动态运动效果。该方法显著加速了推理时间，同时保持了高质量的视频生成。大量实验证明，我们的VDMini在两项重要的视频生成任务——文本到视频（T2V）和图像到视频（I2V）上效果显著。在I2V方法SF-V、T2V方法T2V-Turbo-v2以及T2V方法HunyuanVideo上，我们分别实现了平均2.5倍、1.4倍和1.25倍的加速，同时在包括UCF101、VBench-T2V和VBench-I2V等多个基准测试上保持了生成视频的质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.18375v2">PDF</a> ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>视频扩散模型（VDM）面临高计算成本和慢推理时间的挑战。为此，我们引入了一种新的视频扩散模型压缩方法，采用个体内容和运动动态保留的修剪和一致性损失。通过实证观察，我们发现较深的VDM层对于保持运动动态质量至关重要，而较浅的层更专注于个体内容。因此，我们从较浅的层修剪冗余块，同时保留更多的深层，形成轻量级的VDM变体VDMini。此外，我们提出了个体内容和运动动态（ICMD）一致性损失，以在较大的VDM与VDMini之间获得相当的生产性能。通过个体内容蒸馏（ICD）损失保持教师和学生模型之间每个生成帧的特征一致性，并引入多帧内容对抗（MCA）损失以增强整个生成视频的运动动态。此方法在维持高质量视频生成的同时，显著加速了推理时间。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视频扩散模型（VDM）面临计算成本高和推理时间慢的问题。</li>
<li>引入了一种新的VDM压缩方法，结合个体内容和运动动态保留的修剪和一致性损失。</li>
<li>较深的VDM层对保持运动动态质量至关重要，而较浅的层更关注个体内容。</li>
<li>提出了VDMini，通过修剪浅层冗余块并保留更多深层，形成轻量级的VDM变体。</li>
<li>引入了ICMD一致性损失，以在大型VDM和VDMini之间实现相当的生产性能。</li>
<li>使用ICD损失保持特征一致性，并使用MCA损失增强整个生成视频的运动动态。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.18375">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e0490970681aff5244e9d128cbbfc62d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39099762fc947c51230a54f797664409.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30d2b9d1b66601b800572b268202e5ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-191d67e34a36365ada83c07d08399370.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3bcd9fe6a7e5d62e56b37d42c3f6d47e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="TKG-DM-Training-free-Chroma-Key-Content-Generation-Diffusion-Model"><a href="#TKG-DM-Training-free-Chroma-Key-Content-Generation-Diffusion-Model" class="headerlink" title="TKG-DM: Training-free Chroma Key Content Generation Diffusion Model"></a>TKG-DM: Training-free Chroma Key Content Generation Diffusion Model</h2><p><strong>Authors:Ryugo Morita, Stanislav Frolov, Brian Bernhard Moser, Takahiro Shirakawa, Ko Watanabe, Andreas Dengel, Jinjia Zhou</strong></p>
<p>Diffusion models have enabled the generation of high-quality images with a strong focus on realism and textual fidelity. Yet, large-scale text-to-image models, such as Stable Diffusion, struggle to generate images where foreground objects are placed over a chroma key background, limiting their ability to separate foreground and background elements without fine-tuning. To address this limitation, we present a novel Training-Free Chroma Key Content Generation Diffusion Model (TKG-DM), which optimizes the initial random noise to produce images with foreground objects on a specifiable color background. Our proposed method is the first to explore the manipulation of the color aspects in initial noise for controlled background generation, enabling precise separation of foreground and background without fine-tuning. Extensive experiments demonstrate that our training-free method outperforms existing methods in both qualitative and quantitative evaluations, matching or surpassing fine-tuned models. Finally, we successfully extend it to other tasks (e.g., consistency models and text-to-video), highlighting its transformative potential across various generative applications where independent control of foreground and background is crucial. </p>
<blockquote>
<p>扩散模型已经能够生成高质量的图片，并强烈专注于现实感和文本忠实度。然而，大规模的文本到图像模型，如Stable Diffusion，在生成前景物体放置在色键背景上的图像时遇到困难，这限制了它们在未经微调的情况下分离前景和背景元素的能力。为了解决这一局限性，我们提出了一种全新的无训练色键内容生成扩散模型（TKG-DM），它通过优化初始随机噪声来生成具有指定颜色背景的图像。我们提出的方法是第一个探索初始噪声中颜色方面的控制以用于背景生成的方法，能够在不微调的情况下实现前景和背景的精确分离。大量实验表明，我们的无训练方法在定性和定量评估中都优于现有方法，甚至超过了微调模型。最后，我们成功地将其扩展到其他任务（如一致性模型和文本到视频），突显其在各种生成应用程序中的变革潜力，尤其是在需要独立控制前景和背景的情况下至关重要。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15580v3">PDF</a> Accepted to CVPR2025(Highlight). Code at:   <a target="_blank" rel="noopener" href="https://github.com/ryugo417/TKG-DM">https://github.com/ryugo417/TKG-DM</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了Diffusion模型在生成高质量图像方面的应用，特别是在真实性和文本忠实度方面的优势。然而，大型文本到图像模型（如Stable Diffusion）在生成前景物体放置在色键背景上的图像时存在困难。为了解决这个问题，提出了一种无需训练的新型色键内容生成扩散模型（TKG-DM），它通过优化初始随机噪声来生成具有指定背景色的前景物体图像。该方法首次探索了初始噪声中颜色方面的操控，实现了无需精细调整的精确前景与背景分离。实验证明，该方法在定性和定量评估中都优于现有方法，甚至与精细调整的模型相匹配或超越。此外，该方法的潜力巨大，可广泛应用于各种生成任务，特别是在需要独立控制前景和背景的任务中表现突出。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusion模型能生成高质量、逼真的图像。</li>
<li>大型文本到图像模型在生成特定背景（如色键背景）上的前景物体图像时存在挑战。</li>
<li>提出的Training-Free Chroma Key Content Generation Diffusion Model（TKG-DM）能优化初始随机噪声，生成具有指定背景色的前景物体图像。</li>
<li>TKG-DM首次探索了初始噪声中颜色方面的操控，实现前景和背景的精确分离，无需精细调整。</li>
<li>TKG-DM在定性和定量评估中都表现出优异的性能，甚至超越了某些经过精细调整的模型。</li>
<li>TKG-DM可广泛应用于各种生成任务，特别是在需要独立控制前景和背景的任务中表现突出。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15580">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b941ec7903645158dc6ae9e24b1972e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14dee72935c06c364738a92e092cb21e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-565d8b32e55dc29992b2ae70d5de0562.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-309e6068f65d0a50d169a815e66014c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24d2645586e89df64ad5eee5f6fa42c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-371059e35c21a291b8a5e9ce814bed1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1e41ffb9360ba2ad669276447598043.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="DiffSSC-Semantic-LiDAR-Scan-Completion-using-Denoising-Diffusion-Probabilistic-Models"><a href="#DiffSSC-Semantic-LiDAR-Scan-Completion-using-Denoising-Diffusion-Probabilistic-Models" class="headerlink" title="DiffSSC: Semantic LiDAR Scan Completion using Denoising Diffusion   Probabilistic Models"></a>DiffSSC: Semantic LiDAR Scan Completion using Denoising Diffusion   Probabilistic Models</h2><p><strong>Authors:Helin Cao, Sven Behnke</strong></p>
<p>Perception systems play a crucial role in autonomous driving, incorporating multiple sensors and corresponding computer vision algorithms. 3D LiDAR sensors are widely used to capture sparse point clouds of the vehicle’s surroundings. However, such systems struggle to perceive occluded areas and gaps in the scene due to the sparsity of these point clouds and their lack of semantics. To address these challenges, Semantic Scene Completion (SSC) jointly predicts unobserved geometry and semantics in the scene given raw LiDAR measurements, aiming for a more complete scene representation. Building on promising results of diffusion models in image generation and super-resolution tasks, we propose their extension to SSC by implementing the noising and denoising diffusion processes in the point and semantic spaces individually. To control the generation, we employ semantic LiDAR point clouds as conditional input and design local and global regularization losses to stabilize the denoising process. We evaluate our approach on autonomous driving datasets, and it achieves state-of-the-art performance for SSC, surpassing most existing methods. </p>
<blockquote>
<p>感知系统在自动驾驶中扮演着至关重要的角色，它融合了多种传感器和相应的计算机视觉算法。3D激光雷达传感器被广泛应用于捕捉车辆周围稀疏的点云。然而，由于点云的稀疏性和缺乏语义信息，这样的系统在感知遮挡区域和场景中的间隙时面临困难。为了解决这些挑战，语义场景补全（SSC）能够根据原始的激光雷达测量联合预测场景中未观察到的几何形状和语义信息，旨在实现更完整的场景表示。基于扩散模型在图像生成和超分辨率任务中的出色表现，我们将其扩展到SSC，通过在点和语义空间上分别实现噪声和去噪声扩散过程。为了控制生成过程，我们使用语义激光雷达点云作为条件输入，并设计局部和全局正则化损失来稳定去噪声过程。我们在自动驾驶数据集上评估了我们的方法，它实现了最先进的SSC性能，超越了大多数现有方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.18092v3">PDF</a> 2025 IEEE&#x2F;RSJ International Conference on Intelligent Robots and   Systems (IROS 2025), Hangzhou, China, Oct 2025</p>
<p><strong>摘要</strong></p>
<p>感知系统在自动驾驶中扮演着至关重要的角色，融合了多种传感器和计算机视觉算法。3D激光雷达传感器广泛应用于捕捉车辆周围稀疏的点云数据。然而，由于点云数据的稀疏性和缺乏语义信息，这类系统在感知遮挡区域和场景中的间隙时面临挑战。为了解决这些挑战，语义场景补全（SSC）旨在根据原始激光雷达测量联合预测未观察到的场景几何和语义，以实现更完整的场景表示。基于扩散模型在图像生成和超分辨率任务中的优异表现，我们将其扩展到SSC，通过在点和语义空间分别实现噪声和去噪声扩散过程。为了控制生成过程，我们使用语义激光雷达点云作为条件输入，并设计局部和全局正则化损失以稳定去噪声过程。我们在自动驾驶数据集上评估了我们的方法，它在SSC方面实现了最先进的性能，超越了大多数现有方法。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>感知系统在自动驾驶中起关键作用，融合多种传感器和计算机视觉算法。</li>
<li>3D激光雷达传感器广泛应用于捕捉车辆周围的稀疏点云数据，但存在感知遮挡区域和场景间隙的挑战。</li>
<li>语义场景补全（SSC）可联合预测未观察到的场景几何和语义，以实现更完整的场景表示。</li>
<li>扩散模型在图像生成和超分辨率任务中表现优异，可扩展到SSC。</li>
<li>在点和语义空间分别实现噪声和去噪声扩散过程。</li>
<li>使用语义激光雷达点云作为条件输入来控制生成过程。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.18092">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-147711aaa91c692131f67ba35cec7154.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c790ebb1619c781ff45fb42b27a9a6f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0680bf6bbf76c038bb6bf1e4b6b409a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-513b9e274003db340e8685c891ac80aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50c4cc43d42ff5ff80c5625db369720b.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5fd13196327de12b5517ebc5e254b84c.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-08-06  RL-U$^2$Net A Dual-Branch UNet with Reinforcement Learning-Assisted   Multimodal Feature Fusion for Accurate 3D Whole-Heart Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f41e461634289951c99dea6a22e2b222.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-08-06  ASDR Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant   Neural Rendering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26633.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
