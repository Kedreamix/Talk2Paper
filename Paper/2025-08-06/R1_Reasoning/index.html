<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  MedVLThinker Simple Baselines for Multimodal Medical Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-69abad54fe13ab2f6b0b805c1919d73e.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-06-æ›´æ–°"><a href="#2025-08-06-æ›´æ–°" class="headerlink" title="2025-08-06 æ›´æ–°"></a>2025-08-06 æ›´æ–°</h1><h2 id="MedVLThinker-Simple-Baselines-for-Multimodal-Medical-Reasoning"><a href="#MedVLThinker-Simple-Baselines-for-Multimodal-Medical-Reasoning" class="headerlink" title="MedVLThinker: Simple Baselines for Multimodal Medical Reasoning"></a>MedVLThinker: Simple Baselines for Multimodal Medical Reasoning</h2><p><strong>Authors:Xiaoke Huang, Juncheng Wu, Hui Liu, Xianfeng Tang, Yuyin Zhou</strong></p>
<p>Large Reasoning Models (LRMs) have introduced a new paradigm in AI by enabling models to &#96;&#96;think before respondingâ€ via chain-of-thought reasoning. However, the absence of open and reproducible recipes for building reasoning-centric medical LMMs hinders community-wide research, analysis, and comparison. In this paper, we present MedVLThinker, a suite of simple yet strong baselines. Our fully open recipe consists of: (1) systematic data curation for both text-only and image-text medical data, filtered according to varying levels of reasoning difficulty, and (2) two training paradigms: Supervised Fine-Tuning (SFT) on distilled reasoning traces and Reinforcement Learning with Verifiable Rewards (RLVR) based on final answer correctness. Across extensive experiments on the Qwen2.5-VL model family (3B, 7B) and six medical QA benchmarks, we find that RLVR consistently and significantly outperforms SFT. Additionally, under the RLVR framework, a key, counter-intuitive finding is that training on our curated text-only reasoning data provides a more substantial performance boost than training on multimodal image-text data. Our best open 7B model, trained using the RLVR recipe on text-only data, establishes a new state-of-the-art on existing public VQA benchmarks, surpassing all previous open-source medical LMMs. Furthermore, scaling our model to 32B achieves performance on par with the proprietary GPT-4o. We release all curated data, models, and code to provide the community with a strong, open foundation for future research in multimodal medical reasoning. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡åŸºäºæ€ç»´é“¾çš„æ¨ç†ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å›ç­”ä¹‹å‰è¿›è¡Œâ€œæ€è€ƒâ€ï¼Œä»è€Œåœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå¼•å…¥äº†ä¸€ç§æ–°çš„èŒƒå¼ã€‚ç„¶è€Œï¼Œç¼ºä¹æ„å»ºä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„åŒ»å­¦LMMsçš„å¼€æ”¾å’Œå¯é‡å¤ä½¿ç”¨çš„é£Ÿè°±ï¼Œé˜»ç¢äº†ç¤¾åŒºèŒƒå›´å†…çš„ç ”ç©¶ã€åˆ†æå’Œæ¯”è¾ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†MedVLThinkerï¼Œè¿™æ˜¯ä¸€å¥—ç®€å•è€Œå¼ºå¤§çš„åŸºçº¿ã€‚æˆ‘ä»¬å®Œå…¨å¼€æ”¾çš„é£Ÿè°±åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é’ˆå¯¹çº¯æ–‡æœ¬å’Œå›¾åƒæ–‡æœ¬åŒ»ç–—æ•°æ®çš„æœ‰ç³»ç»Ÿæ•°æ®æ”¶é›†ï¼Œæ ¹æ®ä¸åŒç¨‹åº¦çš„æ¨ç†éš¾åº¦è¿›è¡Œè¿‡æ»¤ï¼›ï¼ˆ2ï¼‰ä¸¤ç§è®­ç»ƒèŒƒå¼ï¼šåŸºäºè’¸é¦æ¨ç†ç—•è¿¹çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒåŸºäºæœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§çš„å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ã€‚åœ¨Qwen2.5-VLæ¨¡å‹å®¶æ—ï¼ˆ3Bã€7Bï¼‰å’Œå…­ä¸ªåŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒRLVRå§‹ç»ˆæ˜¾è‘—ä¼˜äºSFTã€‚æ­¤å¤–ï¼Œåœ¨RLVRæ¡†æ¶ä¸‹ï¼Œä¸€ä¸ªå…³é”®ä¸”åç›´è§‰çš„å‘ç°æ˜¯ï¼Œåœ¨æˆ‘ä»¬æ•´ç†å¥½çš„çº¯æ–‡æœ¬æ¨ç†æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¯”åœ¨å¤šæ¨¡æ€å›¾åƒæ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒæ›´èƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬æœ€å¥½çš„å¼€æº7Bæ¨¡å‹ï¼Œä½¿ç”¨RLVRé…æ–¹åœ¨çº¯æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨ç°æœ‰çš„å…¬å…±è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•é›†ä¸Šå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¹‹å‰æ‰€æœ‰çš„å¼€æºåŒ»ç–—LMMsã€‚æ­¤å¤–ï¼Œå°†æˆ‘ä»¬çš„æ¨¡å‹æ‰©å±•åˆ°32Bï¼Œå…¶æ€§èƒ½ä¸ä¸“æœ‰GPT-4oç›¸å½“ã€‚æˆ‘ä»¬å‘å¸ƒæ‰€æœ‰æ•´ç†å¥½çš„æ•°æ®ã€æ¨¡å‹å’Œä»£ç ï¼Œä¸ºç¤¾åŒºæä¾›ä¸€ä¸ªå¼ºå¤§çš„å¼€æ”¾åŸºç¡€ï¼Œä¾›æœªæ¥åœ¨å¤šåª’ä½“åŒ»ç–—æ¨ç†é¢†åŸŸè¿›è¡Œç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02669v1">PDF</a> Project page and code: <a target="_blank" rel="noopener" href="https://ucsc-vlaa.github.io/MedVLThinker/">https://ucsc-vlaa.github.io/MedVLThinker/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡å¼•å…¥é“¾å¼æ€ç»´æ¨ç†èƒ½åŠ›ï¼Œä¸ºäººå·¥æ™ºèƒ½å¸¦æ¥äº†å…¨æ–°çš„æ¨¡å¼ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹æ„å»ºä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„åŒ»å­¦å¤§å‹æ¨¡å‹çš„å¼€æ”¾å’Œå¯å¤åˆ¶é…æ–¹ï¼Œé˜»ç¢äº†ç¤¾åŒºèŒƒå›´å†…çš„ç ”ç©¶ã€åˆ†æå’Œæ¯”è¾ƒã€‚æœ¬æ–‡æå‡ºäº†MedVLThinkerï¼Œå®ƒåŒ…å«ç®€å•è€Œå¼ºå¤§çš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„å®Œå…¨å¼€æ”¾é…æ–¹åŒ…æ‹¬ï¼šï¼ˆ1ï¼‰é’ˆå¯¹ä¸åŒç¨‹åº¦çš„æ¨ç†éš¾åº¦è¿›è¡Œç³»ç»ŸåŒ–æ•°æ®æ”¶é›†ï¼Œæ¶µç›–çº¯æ–‡æœ¬å’Œå›¾åƒæ–‡æœ¬åŒ»ç–—æ•°æ®ï¼›ï¼ˆ2ï¼‰ä¸¤ç§è®­ç»ƒèŒƒå¼ï¼šåŸºäºè’¸é¦æ¨ç†ç—•è¿¹çš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’ŒåŸºäºæœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§çš„å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒå’Œå¯¹Qwen2.5-VLæ¨¡å‹å®¶æ—ï¼ˆ3Bã€7Bï¼‰ä»¥åŠå…­ä¸ªåŒ»ç–—é—®ç­”åŸºå‡†çš„æµ‹è¯•ï¼Œæˆ‘ä»¬å‘ç°RLVRæŒç»­ä¸”æ˜¾è‘—ä¼˜äºSFTã€‚æ­¤å¤–ï¼Œåœ¨RLVRæ¡†æ¶ä¸‹ï¼Œä¸€ä¸ªå…³é”®çš„ã€åç›´è§‰çš„å‘ç°æ˜¯ï¼Œåœ¨æˆ‘ä»¬ç²¾é€‰çš„çº¯æ–‡æœ¬æ¨ç†æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒæ¯”åœ¨å¤šæ¨¡æ€å›¾åƒæ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒèƒ½æä¾›æ›´å¤§å¹…åº¦çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬æœ€å¥½çš„å¼€æ”¾7Bæ¨¡å‹ï¼Œä½¿ç”¨RLVRé…æ–¹åœ¨çº¯æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåœ¨ç°æœ‰çš„å…¬å…±è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æ–°çš„æ°´å¹³ï¼Œè¶…è¶Šäº†ä¹‹å‰æ‰€æœ‰çš„å¼€æºåŒ»ç–—å¤§å‹æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå°†æˆ‘ä»¬çš„æ¨¡å‹æ‰©å±•åˆ°32Bï¼Œå…¶æ€§èƒ½è¾¾åˆ°äº†ä¸ä¸“æœ‰GPT-4oç›¸å½“çš„æ°´å¹³ã€‚æˆ‘ä»¬å‘å¸ƒæ‰€æœ‰ç²¾é€‰æ•°æ®ã€æ¨¡å‹å’Œä»£ç ï¼Œä¸ºç¤¾åŒºæä¾›ä¸€ä¸ªå¼ºå¤§çš„å¼€æ”¾åŸºç¡€ï¼Œä»¥ä¾›æœªæ¥åœ¨å¤šåª’ä½“åŒ»ç–—æ¨ç†é¢†åŸŸè¿›è¡Œç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰é€šè¿‡å¼•å…¥é“¾å¼æ€ç»´æ¨ç†èƒ½åŠ›ä¸ºAIå¸¦æ¥æ–°èŒƒå¼ã€‚</li>
<li>MedVLThinkeræä¾›ç®€å•è€Œå¼ºå¤§çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«æ•°æ®æ”¶é›†å’Œä¸¤ç§è®­ç»ƒèŒƒå¼ã€‚</li>
<li>RLVRè®­ç»ƒèŒƒå¼åœ¨åŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºSFTã€‚</li>
<li>åœ¨RLVRæ¡†æ¶ä¸‹ï¼Œçº¯æ–‡æœ¬æ¨ç†æ•°æ®è®­ç»ƒæ¯”å¤šæ¨¡æ€å›¾åƒæ–‡æœ¬æ•°æ®è®­ç»ƒæ›´èƒ½æå‡æ€§èƒ½ã€‚</li>
<li>7Bæ¨¡å‹åœ¨å…¬å…±è§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šå…ˆå‰å¼€æºåŒ»ç–—å¤§å‹æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹æ‰©å±•åˆ°32Bæ—¶ï¼Œæ€§èƒ½ä¸ä¸“æœ‰GPT-4oç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02669">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e53d8a17a822b509bfc8fdcf05a5a13c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9d9a4099ab9852efc2ad52017e556232.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-102a37e4375ff4737b16c9a22b2bedb2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Mitigating-Attention-Hacking-in-Preference-Based-Reward-Modeling-via-Interaction-Distillation"><a href="#Mitigating-Attention-Hacking-in-Preference-Based-Reward-Modeling-via-Interaction-Distillation" class="headerlink" title="Mitigating Attention Hacking in Preference-Based Reward Modeling via   Interaction Distillation"></a>Mitigating Attention Hacking in Preference-Based Reward Modeling via   Interaction Distillation</h2><p><strong>Authors:Jianxiang Zang, Meiling Ning, Shihan Dou, Jiazheng Zhang, Tao Gui, Qi Zhang, Xuanjing Huang</strong></p>
<p>The reward model (RM), as the core component of reinforcement learning from human feedback (RLHF) for large language models (LLMs), responsible for providing reward signals to generated responses. However, mainstream preference modeling in RM is inadequate in terms of token-level interaction, making its judgment signals vulnerable to being hacked by misallocated attention to context. This stems from two fundamental limitations: (1) Current preference modeling employs decoder-only architectures, where the unidirectional causal attention mechanism leads to forward-decaying intra-sequence attention within the prompt-response sequence. (2) The independent Siamese-encoding paradigm induces the absence of token-level inter-sequence attention between chosen and rejected sequences. To address this â€œattention hackingâ€, we propose â€œInteraction Distillationâ€, a novel training framework for more adequate preference modeling through attention-level optimization. The method introduces an interaction-based natural language understanding model as the teacher to provide sophisticated token interaction patterns via comprehensive attention, and guides the preference modeling to simulate teacher modelâ€™s interaction pattern through an attentional alignment objective. Through extensive experiments, interaction distillation has demonstrated its ability to provide more stable and generalizable reward signals compared to state-of-the-art RM optimization methods that target data noise, highlighting the attention hacking constitute a more fundamental limitation in RM. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ä¸ºç”Ÿæˆçš„å“åº”æä¾›å¥–åŠ±ä¿¡å·ã€‚ç„¶è€Œï¼ŒRMä¸­çš„ä¸»æµåå¥½å»ºæ¨¡åœ¨ä»¤ç‰Œçº§äº¤äº’æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´åˆ¤æ–­ä¿¡å·å®¹æ˜“å—åˆ°å¯¹ä¸Šä¸‹æ–‡åˆ†é…ä¸å½“çš„æ³¨æ„åŠ›çš„æ”»å‡»ã€‚è¿™æºäºä¸¤ä¸ªåŸºæœ¬å±€é™ï¼šï¼ˆ1ï¼‰å½“å‰çš„åå¥½å»ºæ¨¡ä»…é‡‡ç”¨è§£ç å™¨æ¶æ„ï¼Œå…¶ä¸­å•å‘å› æœæ³¨æ„åŠ›æœºåˆ¶å¯¼è‡´æç¤º-å“åº”åºåˆ—å†…çš„åºåˆ—å†…æ³¨æ„åŠ›å‘ˆç°å‰å‘è¡°å‡ã€‚ï¼ˆ2ï¼‰ç‹¬ç«‹çš„Siameseç¼–ç èŒƒå¼å¯¼è‡´æ‰€é€‰åºåˆ—å’Œæ‹’ç»åºåˆ—ä¹‹é—´ç¼ºä¹ä»¤ç‰Œçº§åºåˆ—é—´æ³¨æ„åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ç§â€œæ³¨æ„åŠ›æ”»å‡»â€ï¼Œæˆ‘ä»¬æå‡ºäº†â€œäº¤äº’è’¸é¦â€ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æ³¨æ„åŠ›å±‚é¢ä¼˜åŒ–æ¥è¿›è¡Œæ›´å……åˆ†åå¥½å»ºæ¨¡çš„æ–°å‹è®­ç»ƒæ¡†æ¶ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§åŸºäºäº¤äº’çš„è‡ªç„¶è¯­è¨€ç†è§£æ¨¡å‹ä½œä¸ºæ•™å¸ˆï¼Œé€šè¿‡å…¨é¢çš„æ³¨æ„åŠ›æä¾›ç²¾ç»†çš„ä»¤ç‰Œäº¤äº’æ¨¡å¼ï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›å¯¹é½ç›®æ ‡æŒ‡å¯¼åå¥½å»ºæ¨¡æ¥æ¨¡æ‹Ÿæ•™å¸ˆæ¨¡å‹çš„äº¤äº’æ¨¡å¼ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œäº¤äº’è’¸é¦æ˜¾ç¤ºå‡ºå…¶æä¾›æ¯”é’ˆå¯¹æ•°æ®å™ªå£°çš„æœ€å…ˆè¿›RMä¼˜åŒ–æ–¹æ³•æ›´ç¨³å®šå’Œå¯æ¨å¹¿çš„å¥–åŠ±ä¿¡å·çš„èƒ½åŠ›ï¼Œå¼ºè°ƒäº†æ³¨æ„åŠ›æ”»å‡»åœ¨RMä¸­æ„æˆäº†ä¸€ä¸ªæ›´æ ¹æœ¬çš„å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02618v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ é¢†åŸŸä¸­ï¼Œå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ä½œä¸ºæ ¸å¿ƒç»„ä»¶å¯¹äººç±»åé¦ˆï¼ˆRLHFï¼‰è¿›è¡Œå»ºæ¨¡ã€‚ä¸»æµRMåå¥½å»ºæ¨¡ä¸è¶³ä½“ç°åœ¨ä»¤ç‰Œçº§åˆ«äº¤äº’æ–¹é¢ï¼Œè¿™ä½¿å…¶å®¹æ˜“å—åˆ°å¿½è§†ã€‚æœ¬æ–‡é€šè¿‡æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œäº¤äº’è’¸é¦â€çš„æ–°è®­ç»ƒæ¡†æ¶æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ³¨æ„åŠ›çº§åˆ«çš„ä¼˜åŒ–è¿›è¡Œæ›´å……åˆ†çš„åå¥½å»ºæ¨¡ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼Œäº¤äº’è’¸é¦èƒ½å¤Ÿæä¾›æ›´ç¨³å®šå’Œå¯æ³›åŒ–çš„å¥–åŠ±ä¿¡å·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰åœ¨å¼ºåŒ–å­¦ä¹ é¢†åŸŸä¸­èµ·ç€æ ¸å¿ƒä½œç”¨ï¼Œè´Ÿè´£ä¸ºç”Ÿæˆå“åº”æä¾›å¥–åŠ±ä¿¡å·ã€‚</li>
<li>å½“å‰ä¸»æµçš„RMåå¥½å»ºæ¨¡åœ¨ä»¤ç‰Œçº§åˆ«äº¤äº’æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå®¹æ˜“å—åˆ°ä¸Šä¸‹æ–‡ä¸­çš„è¯¯æ³¨æ„åŠ›åˆ†é…æ”»å‡»ï¼ˆattention hackingï¼‰ã€‚</li>
<li>â€œattention hackingâ€é—®é¢˜çš„æ ¹æœ¬åŸå› æ˜¯ç°æœ‰çš„RMæ¶æ„çš„ä¸¤å¤§å±€é™æ€§ï¼šä¸€æ˜¯å•å‘å› æœæ³¨æ„åŠ›æœºåˆ¶å¯¼è‡´åºåˆ—å†…çš„å‰å‘è¡°å‡æ³¨æ„åŠ›ï¼›äºŒæ˜¯ç‹¬ç«‹Siameseç¼–ç èŒƒå¼å¯¼è‡´æ‰€é€‰åºåˆ—å’Œæ‹’ç»åºåˆ—ä¹‹é—´ç¼ºå°‘ä»¤ç‰Œçº§åˆ«çš„åºåˆ—é—´æ³¨æ„åŠ›ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†åä¸ºâ€œäº¤äº’è’¸é¦â€çš„æ–°è®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ³¨æ„åŠ›çº§åˆ«çš„ä¼˜åŒ–è¿›è¡Œæ›´å……åˆ†çš„åå¥½å»ºæ¨¡ã€‚</li>
<li>äº¤äº’è’¸é¦å¼•å…¥äº†ä¸€ä¸ªåŸºäºäº¤äº’çš„è‡ªç„¶è¯­è¨€ç†è§£æ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œé€šè¿‡å…¨é¢çš„æ³¨æ„åŠ›æä¾›å¤æ‚çš„ä»¤ç‰Œäº¤äº’æ¨¡å¼ã€‚</li>
<li>äº¤äº’è’¸é¦é€šè¿‡æ³¨æ„åŠ›å¯¹é½ç›®æ ‡æŒ‡å¯¼åå¥½å»ºæ¨¡æ¨¡æ‹Ÿæ•™å¸ˆæ¨¡å‹çš„äº¤äº’æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02618">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0aa758030e87e3261ee82f31b003c935.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f6893c163fc7f47bae88b58b02984b63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9aa88decb8b2322586720b54f82c4e94.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df1bfefaa73abf3ab37ed7b367788baf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79c624f9f637e5a058a01bf9c913aecf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ffebb094f31a77d29f0db6cd31f80a29.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="CAMA-Enhancing-Mathematical-Reasoning-in-Large-Language-Models-with-Causal-Knowledge"><a href="#CAMA-Enhancing-Mathematical-Reasoning-in-Large-Language-Models-with-Causal-Knowledge" class="headerlink" title="CAMA: Enhancing Mathematical Reasoning in Large Language Models with   Causal Knowledge"></a>CAMA: Enhancing Mathematical Reasoning in Large Language Models with   Causal Knowledge</h2><p><strong>Authors:Lei Zan, Keli Zhang, Ruichu Cai, Lujia Pan</strong></p>
<p>Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks, yet they still struggle with complex mathematical reasoning, a challenge fundamentally rooted in deep structural dependencies. To address this challenge, we propose \textbf{CA}usal \textbf{MA}thematician (\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit, reusable mathematical structure. In the learning stage, CAMA first constructs the \textbf{M}athematical \textbf{C}ausal \textbf{G}raph (\textbf{MCG}), a high-level representation of solution strategies, by combining LLM priors with causal discovery algorithms applied to a corpus of question-solution pairs. The resulting MCG encodes essential knowledge points and their causal dependencies. To better align the graph with downstream reasoning tasks, CAMA further refines the MCG through iterative feedback derived from a selected subset of the question-solution pairs. In the reasoning stage, given a new question, CAMA dynamically extracts a task-relevant subgraph from the MCG, conditioned on both the question content and the LLMâ€™s intermediate reasoning trace. This subgraph, which encodes the most pertinent knowledge points and their causal dependencies, is then injected back into the LLM to guide its reasoning process. Empirical results on real-world datasets show that CAMA significantly improves LLM performance on challenging mathematical problems. Furthermore, our experiments demonstrate that structured guidance consistently outperforms unstructured alternatives, and that incorporating asymmetric causal relationships yields greater improvements than using symmetric associations alone. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨å¤æ‚çš„æ•°å­¦æ¨ç†æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™ä¸€æŒ‘æˆ˜æ ¹æœ¬æºäºæ·±å±‚çš„ç»“æ„ä¾èµ–æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é…å¤‡LLMä»¥æ˜ç¡®ã€å¯é‡å¤ä½¿ç”¨çš„æ•°å­¦ç»“æ„çš„ä¸¤é˜¶æ®µå› æœæ¡†æ¶ï¼Œåä¸º<strong>CAMA</strong>ï¼ˆå› æœæ•°å­¦å®¶ï¼‰ã€‚åœ¨å­¦ä¹ é˜¶æ®µï¼ŒCAMAé¦–å…ˆé€šè¿‡ç»“åˆLLMå…ˆéªŒçŸ¥è¯†å’Œåº”ç”¨äºé—®é¢˜è§£å†³æ–¹æ¡ˆå¯¹è¯­æ–™åº“çš„å› æœå‘ç°ç®—æ³•ï¼Œæ„å»ºæ•°å­¦å› æœå›¾ï¼ˆMCGï¼‰ï¼Œè¿™æ˜¯è§£å†³æ–¹æ¡ˆç­–ç•¥çš„é«˜çº§è¡¨ç¤ºã€‚ç”Ÿæˆçš„MCGç¼–ç äº†å¿…è¦çš„çŸ¥è¯†ç‚¹å’Œå®ƒä»¬çš„å› æœä¾èµ–å…³ç³»ã€‚ä¸ºäº†æ›´å¥½åœ°ä¸ä¸‹æ¸¸æ¨ç†ä»»åŠ¡å¯¹é½ï¼ŒCAMAè¿›ä¸€æ­¥é€šè¿‡æ¥è‡ªé—®é¢˜è§£å†³æ–¹æ¡ˆå¯¹æ‰€é€‰å­é›†çš„è¿­ä»£åé¦ˆæ¥ä¼˜åŒ–MCGã€‚åœ¨æ¨ç†é˜¶æ®µï¼Œå¯¹äºæ–°é—®é¢˜ï¼ŒCAMAä¼šæ ¹æ®é—®é¢˜çš„å†…å®¹å’ŒLLMçš„ä¸­é—´æ¨ç†è½¨è¿¹ï¼Œä»MCGä¸­åŠ¨æ€æå–ä¸ä»»åŠ¡ç›¸å…³çš„å­å›¾ã€‚è¿™ä¸ªå­å›¾ç¼–ç äº†æœ€ç›¸å…³çš„çŸ¥è¯†ç‚¹å’Œå®ƒä»¬çš„å› æœä¾èµ–å…³ç³»ï¼Œç„¶åé‡æ–°æ³¨å…¥LLMä»¥æŒ‡å¯¼å…¶æ¨ç†è¿‡ç¨‹ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒCAMAèƒ½æ˜¾è‘—æé«˜LLMè§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦é—®é¢˜çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¿˜è¡¨æ˜ï¼Œç»“æ„åŒ–çš„æŒ‡å¯¼å§‹ç»ˆä¼˜äºéç»“æ„åŒ–çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶ä¸”èå…¥ä¸å¯¹ç§°çš„å› æœå…³ç³»æ¯”ä»…ä½¿ç”¨å¯¹ç§°å…³è”å¸¦æ¥æ›´å¤§çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02583v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¹¿æ³›çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨å¤æ‚çš„æ•°å­¦æ¨ç†æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæå‡ºäº†å› æœæ•°å­¦å®¶ï¼ˆCAMAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šå­¦ä¹ é˜¶æ®µå’Œæ¨ç†é˜¶æ®µã€‚åœ¨å­¦ä¹ é˜¶æ®µï¼ŒCAMAæ„å»ºæ•°å­¦å› æœå›¾ï¼ˆMCGï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é«˜çº§è§£å†³æ–¹æ¡ˆç­–ç•¥è¡¨ç¤ºã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒCAMAæ ¹æ®é—®é¢˜å’ŒLLMçš„ä¸­é—´æ¨ç†è½¨è¿¹ä»MCGä¸­æå–ç›¸å…³å­å›¾ï¼Œå¹¶æ³¨å…¥LLMä»¥æŒ‡å¯¼å…¶æ¨ç†è¿‡ç¨‹ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒCAMAèƒ½æ˜¾è‘—æé«˜LLMè§£å†³å¤æ‚æ•°å­¦é—®é¢˜çš„èƒ½åŠ›ï¼Œä¸”ç»“æ„åŒ–çš„æŒ‡å¯¼æ–¹å¼ä¼˜äºéç»“æ„åŒ–æ–¹å¼ï¼Œè€ƒè™‘ä¸å¯¹ç§°å› æœå…³ç³»æ¯”ä»…ä½¿ç”¨å¯¹ç§°å…³è”æ•ˆæœæ›´å¥½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMsè™½åœ¨å¤šç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤æ‚æ•°å­¦æ¨ç†æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>CAMAæ¡†æ¶åˆ†ä¸ºå­¦ä¹ é˜¶æ®µå’Œæ¨ç†é˜¶æ®µï¼Œæ—¨åœ¨è§£å†³LLMsåœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚</li>
<li>åœ¨å­¦ä¹ é˜¶æ®µï¼ŒCAMAæ„å»ºæ•°å­¦å› æœå›¾ï¼ˆMCGï¼‰ï¼ŒèåˆLLMå…ˆéªŒçŸ¥è¯†ä¸å› æœå‘ç°ç®—æ³•ï¼Œå½¢æˆé«˜çº§è§£å†³æ–¹æ¡ˆç­–ç•¥è¡¨ç¤ºã€‚</li>
<li>CAMAé€šè¿‡è¿­ä»£åé¦ˆä¼˜åŒ–MCGï¼Œä½¿å…¶æ›´å¥½åœ°ä¸ä¸‹æ¸¸æ¨ç†ä»»åŠ¡å¯¹é½ã€‚</li>
<li>åœ¨æ¨ç†é˜¶æ®µï¼ŒCAMAæ ¹æ®é—®é¢˜å’ŒLLMçš„æ¨ç†è½¨è¿¹ä»MCGä¸­æå–ç›¸å…³å­å›¾ï¼ŒæŒ‡å¯¼LLMçš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>å®è¯ç»“æœè¡¨æ˜ï¼ŒCAMAèƒ½æ˜¾è‘—æé«˜LLMè§£å†³æ•°å­¦é—®é¢˜çš„èƒ½åŠ›ï¼Œä¸”ç»“æ„åŒ–çš„æŒ‡å¯¼æ–¹å¼æ•ˆæœæ›´ä½³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02583">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d1c28ab7f4a2f4f03183c8ab9c0e5c98.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb36c79e159593473a6f104fb2645231.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef83bcc7ab4751853333c5463f3b0db0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Decomposed-Reasoning-with-Reinforcement-Learning-for-Relevance-Assessment-in-UGC-Platforms"><a href="#Decomposed-Reasoning-with-Reinforcement-Learning-for-Relevance-Assessment-in-UGC-Platforms" class="headerlink" title="Decomposed Reasoning with Reinforcement Learning for Relevance   Assessment in UGC Platforms"></a>Decomposed Reasoning with Reinforcement Learning for Relevance   Assessment in UGC Platforms</h2><p><strong>Authors:Xiaowei Yuan, Lei Jin, Haoxin Zhang, Yan Gao, Yi Wu, Yao Hu, Ziyang Huang, Jun Zhao, Kang Liu</strong></p>
<p>Retrieval-augmented generation (RAG) plays a critical role in user-generated content (UGC) platforms, but its effectiveness depends heavily on accurate relevance assessment of query-document pairs. Despite recent advances in applying large language models (LLMs) to relevance modeling, UGC platforms present unique challenges: 1) ambiguous user intent due to sparse user feedback in RAG scenarios, and 2) substantial noise introduced by informal and unstructured language. To address these issues, we propose the Reinforced Reasoning Model for Relevance Assessment (R3A), which introduces a decomposed reasoning framework over queries and candidate documents before scoring. R3A first leverages auxiliary high-ranked documents within the platform to infer latent query intent. It then performs verbatim fragment extraction to justify relevance decisions, thereby reducing errors caused by noisy UGC. Based on a reinforcement learning framework, R3A is optimized to mitigate distortions arising from ambiguous queries and unstructured content. Experimental results show that R3A significantly outperforms existing baseline methods in terms of relevance accuracy, across both offline benchmarks and online experiments. </p>
<blockquote>
<p>å¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰åœ¨ç”¨æˆ·ç”Ÿæˆå†…å®¹ï¼ˆUGCï¼‰å¹³å°ä¸­æ‰®æ¼”ç€å…³é”®è§’è‰²ï¼Œä½†å…¶æœ‰æ•ˆæ€§å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæŸ¥è¯¢æ–‡æ¡£å¯¹çš„ç›¸å…³æ€§è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚å°½ç®¡æœ€è¿‘å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨äºç›¸å…³æ€§å»ºæ¨¡å–å¾—äº†è¿›å±•ï¼Œä½†UGCå¹³å°ä»ç„¶é¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼š1ï¼‰RAGåœºæ™¯ä¸­çš„ç”¨æˆ·åé¦ˆç¨€å°‘å¯¼è‡´ç”¨æˆ·æ„å›¾æ¨¡ç³Šï¼Œä»¥åŠ2ï¼‰éæ­£å¼å’Œéç»“æ„åŒ–è¯­è¨€å¸¦æ¥çš„å¤§é‡å™ªéŸ³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç›¸å…³æ€§è¯„ä¼°å¼ºåŒ–æ¨ç†æ¨¡å‹ï¼ˆR3Aï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨è¯„åˆ†ä¹‹å‰å¼•å…¥äº†é’ˆå¯¹æŸ¥è¯¢å’Œå€™é€‰æ–‡æ¡£çš„åˆ†è§£æ¨ç†æ¡†æ¶ã€‚R3Aé¦–å…ˆåˆ©ç”¨å¹³å°å†…è¾…åŠ©çš„é«˜æ’åæ–‡æ¡£æ¥æ¨æ–­æ½œåœ¨çš„æŸ¥è¯¢æ„å›¾ã€‚ç„¶åï¼Œå®ƒæ‰§è¡Œé€å­—ç‰‡æ®µæå–ä»¥è¯æ˜ç›¸å…³æ€§å†³ç­–ï¼Œä»è€Œå‡å°‘ç”±å˜ˆæ‚çš„UGCé€ æˆçš„é”™è¯¯ã€‚åŸºäºå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼ŒR3Aç»è¿‡ä¼˜åŒ–ï¼Œå‡è½»äº†ç”±æ¨¡ç³ŠæŸ¥è¯¢å’Œéç»“æ„åŒ–å†…å®¹å¼•èµ·çš„å¤±çœŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR3Aåœ¨ç¦»çº¿åŸºå‡†æµ‹è¯•å’Œåœ¨çº¿å®éªŒä¸­ï¼Œåœ¨ç›¸å…³æ€§å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02506v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†åœ¨ç”¨æˆ·ç”Ÿæˆå†…å®¹ï¼ˆUGCï¼‰å¹³å°ä¸­ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„é‡è¦æ€§åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å¼ºåŒ–æ¨ç†æ¨¡å‹ç”¨äºç›¸å…³æ€§è¯„ä¼°ï¼ˆR3Aï¼‰ã€‚R3Aé€šè¿‡ä¸€ä¸ªåˆ†è§£æ¨ç†æ¡†æ¶å¯¹æŸ¥è¯¢å’Œå€™é€‰æ–‡æ¡£è¿›è¡Œè¯„åˆ†å‰çš„å¤„ç†ï¼Œåˆ©ç”¨å¹³å°ä¸­çš„è¾…åŠ©é«˜æ’åæ–‡æ¡£æ¥æ¨æ–­æ½œåœ¨æŸ¥è¯¢æ„å›¾ï¼Œå¹¶è¿›è¡Œé€å­—ç‰‡æ®µæå–ä»¥è¯æ˜ç›¸å…³æ€§å†³ç­–ã€‚R3AåŸºäºå¼ºåŒ–å­¦ä¹ æ¡†æ¶è¿›è¡Œä¼˜åŒ–ï¼Œä»¥å‡è½»æ¨¡ç³ŠæŸ¥è¯¢å’Œéç»“æ„åŒ–å†…å®¹å¼•èµ·çš„å¤±çœŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR3Aåœ¨ç¦»çº¿åŸºå‡†æµ‹è¯•å’Œåœ¨çº¿å®éªŒä¸­ï¼Œåœ¨ç›¸å…³æ€§å‡†ç¡®æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åœ¨ç”¨æˆ·ç”Ÿæˆå†…å®¹ï¼ˆUGCï¼‰å¹³å°ä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œå…¶æœ‰æ•ˆæ€§å–å†³äºæŸ¥è¯¢æ–‡æ¡£å¯¹çš„ç›¸å…³æ€§è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚</li>
<li>UGCå¹³å°åœ¨RAGåœºæ™¯ä¸­å­˜åœ¨ä¸¤ä¸ªç‹¬ç‰¹æŒ‘æˆ˜ï¼šç”±äºç¨€ç–çš„ç”¨æˆ·åé¦ˆå¯¼è‡´çš„æ¨¡ç³Šç”¨æˆ·æ„å›¾ï¼Œä»¥åŠç”±éæ­£å¼å’Œéç»“æ„åŒ–è¯­è¨€å¼•å…¥çš„å¤§é‡å™ªéŸ³ã€‚</li>
<li>æå‡ºçš„å¼ºåŒ–æ¨ç†æ¨¡å‹ç”¨äºç›¸å…³æ€§è¯„ä¼°ï¼ˆR3Aï¼‰å¼•å…¥äº†ä¸€ä¸ªåˆ†è§£æ¨ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯¹æŸ¥è¯¢å’Œå€™é€‰æ–‡æ¡£è¿›è¡Œè¯„åˆ†å‰çš„å¤„ç†ã€‚</li>
<li>R3Aåˆ©ç”¨è¾…åŠ©é«˜æ’åæ–‡æ¡£æ¥æ¨æ–­æ½œåœ¨æŸ¥è¯¢æ„å›¾ï¼Œå¹¶è¿›è¡Œé€å­—ç‰‡æ®µæå–ï¼Œä»¥è¯æ˜ç›¸å…³æ€§å†³ç­–ï¼Œä»è€Œå‡å°‘ç”±å˜ˆæ‚çš„UGCå¼•èµ·çš„é”™è¯¯ã€‚</li>
<li>R3AåŸºäºå¼ºåŒ–å­¦ä¹ æ¡†æ¶è¿›è¡Œä¼˜åŒ–ï¼Œæ—¨åœ¨å‡è½»æ¨¡ç³ŠæŸ¥è¯¢å’Œéç»“æ„åŒ–å†…å®¹å¯¼è‡´çš„å¤±çœŸã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒR3Aåœ¨ç¦»çº¿åŸºå‡†æµ‹è¯•å’Œåœ¨çº¿å®éªŒä¸­ï¼Œæ˜¾è‘—æé«˜äº†ç›¸å…³æ€§è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02506">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5a54a1809d6d0b68239865c66d5f7181.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34084d21a745d9ab7ef1b1a53a72be62.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9fcc1f1aa36031055970a60ad022a2a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ec9be659500bc40558794e0f1e8d711.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Traffic-R1-Reinforced-LLMs-Bring-Human-Like-Reasoning-to-Traffic-Signal-Control-Systems"><a href="#Traffic-R1-Reinforced-LLMs-Bring-Human-Like-Reasoning-to-Traffic-Signal-Control-Systems" class="headerlink" title="Traffic-R1: Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal   Control Systems"></a>Traffic-R1: Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal   Control Systems</h2><p><strong>Authors:Xingchen Zou, Yuhao Yang, Zheng Chen, Xixuan Hao, Yiqi Chen, Chao Huang, Yuxuan Liang</strong></p>
<p>Traffic signal control (TSC) is vital for mitigating congestion and sustaining urban mobility. In this paper, we introduce Traffic-R1, a foundation model with human-like reasoning for TSC systems. Our model is developed through self-exploration and iteration of reinforced large language models (LLMs) with expert guidance in a simulated traffic environment. Compared to traditional reinforcement learning (RL) and recent LLM-based methods, Traffic-R1 offers three significant advantages. First, Traffic-R1 delivers zero-shot generalisation, transferring unchanged to new road networks and out-of-distribution incidents by utilizing its internal traffic control policies and human-like reasoning. Second, its 3B-parameter architecture is lightweight enough for real-time inference on mobile-class chips, enabling large-scale edge deployment. Third, Traffic-R1 provides an explainable TSC process and facilitates multi-intersection communication through its self-iteration and a new synchronous communication network. Extensive benchmarks demonstrate that Traffic-R1 sets a new state of the art, outperforming strong baselines and training-intensive RL controllers. In practice, the model now manages signals for more than 55,000 drivers daily, shortening average queues by over 5% and halving operator workload. Our checkpoint is available at <a target="_blank" rel="noopener" href="https://huggingface.co/Season998/Traffic-R1">https://huggingface.co/Season998/Traffic-R1</a>. </p>
<blockquote>
<p>äº¤é€šä¿¡å·æ§åˆ¶ï¼ˆTSCï¼‰å¯¹äºç¼“è§£äº¤é€šæ‹¥å µå’Œç»´æŒåŸå¸‚æµåŠ¨æ€§è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Traffic-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰äººç±»æ€ç»´æ¨ç†èƒ½åŠ›çš„TSCç³»ç»ŸåŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ˜¯é€šè¿‡åœ¨æ¨¡æ‹Ÿäº¤é€šç¯å¢ƒä¸­è‡ªæˆ‘æ¢ç´¢å’Œè¿­ä»£å¼ºåŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¹¶åœ¨ä¸“å®¶æŒ‡å¯¼ä¸‹å¼€å‘å‡ºæ¥çš„ã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæœ€æ–°çš„LLMæ–¹æ³•ç›¸æ¯”ï¼ŒTraffic-R1å…·æœ‰ä¸‰å¤§ä¼˜åŠ¿ã€‚é¦–å…ˆï¼ŒTraffic-R1å®ç°äº†é›¶å°„å‡»æ³›åŒ–ï¼Œé€šè¿‡åˆ©ç”¨å…¶å†…éƒ¨çš„äº¤é€šæ§åˆ¶æ”¿ç­–å’Œäººç±»æ€ç»´æ¨ç†ï¼Œæ— éœ€ä»»ä½•ä¿®æ”¹å³å¯é€‚åº”æ–°çš„é“è·¯ç½‘ç»œå’Œç¦»ç¾¤åˆ†å¸ƒäº‹ä»¶ã€‚å…¶æ¬¡ï¼Œå…¶3Bå‚æ•°çš„æ¶æ„è¶³å¤Ÿè½»ä¾¿ï¼Œå¯åœ¨ç§»åŠ¨èŠ¯ç‰‡ä¸Šè¿›è¡Œå®æ—¶æ¨ç†ï¼Œæ”¯æŒå¤§è§„æ¨¡è¾¹ç¼˜éƒ¨ç½²ã€‚ç¬¬ä¸‰ï¼ŒTraffic-R1æä¾›äº†ä¸€ä¸ªå¯è§£é‡Šçš„TSCè¿‡ç¨‹ï¼Œå¹¶é€šè¿‡å…¶è‡ªæˆ‘è¿­ä»£å’Œæ–°çš„åŒæ­¥é€šä¿¡ç½‘ç»œä¿ƒè¿›äº†å¤šäº¤å‰è·¯å£é€šä¿¡ã€‚å¹¿æ³›çš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼ŒTraffic-R1åˆ›é€ äº†æ–°çš„æŠ€æœ¯é¡¶å°–æ°´å¹³ï¼Œè¶…è¶Šäº†å¼ºå¤§çš„åŸºå‡†æµ‹è¯•å’Œè®­ç»ƒå¯†é›†çš„RLæ§åˆ¶å™¨ã€‚åœ¨å®è·µä¸­ï¼Œè¯¥æ¨¡å‹ç°åœ¨æ¯å¤©ä¸ºè¶…è¿‡55000åå¸æœºç®¡ç†ä¿¡å·ï¼Œç¼©çŸ­äº†å¹³å‡é˜Ÿåˆ—è¶…è¿‡5%ï¼Œå¹¶å°†æ“ä½œäººå‘˜çš„å·¥ä½œé‡å‡åŠã€‚æˆ‘ä»¬çš„æ£€æŸ¥ç‚¹ä½äº[<a target="_blank" rel="noopener" href="https://huggingface.co/Season998/Traffic-R1%E3%80%82]%EF%BC%88%E6%B3%A8%EF%BC%9A%E6%AD%A4%E9%93%BE%E6%8E%A5%E6%97%A0%E6%B3%95%E7%9B%B4%E6%8E%A5%E8%AE%BF%E9%97%AE%E4%BB%A5%E9%AA%8C%E8%AF%81%E5%85%B6%E6%9C%89%E6%95%88%E6%80%A7%EF%BC%89">https://huggingface.co/Season998/Traffic-R1ã€‚]ï¼ˆæ³¨ï¼šæ­¤é“¾æ¥æ— æ³•ç›´æ¥è®¿é—®ä»¥éªŒè¯å…¶æœ‰æ•ˆæ€§ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02344v1">PDF</a> </p>
<p><strong>Summary</strong><br>     äº¤é€šä¿¡å·æ§åˆ¶ï¼ˆTSCï¼‰å¯¹äºç¼“è§£äº¤é€šæ‹¥å µå’Œç»´æŒåŸå¸‚æµåŠ¨æ€§è‡³å…³é‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†Traffic-R1ï¼Œä¸€ç§å…·æœ‰äººç±»æ€ç»´æ¨ç†èƒ½åŠ›çš„TSCç³»ç»ŸåŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹é€šè¿‡åœ¨æ¨¡æ‹Ÿäº¤é€šç¯å¢ƒä¸­å¯¹å¼ºåŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªæˆ‘æ¢ç´¢ä¸è¿­ä»£ï¼Œå¹¶è¾…ä»¥ä¸“å®¶æŒ‡å¯¼è¿›è¡Œå¼€å‘ã€‚ä¸ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæœ€è¿‘çš„LLMæ–¹æ³•ç›¸æ¯”ï¼ŒTraffic-R1æä¾›äº†ä¸‰å¤§ä¼˜åŠ¿ï¼šé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¯è½»æ¾é€‚åº”æ–°é“è·¯ç½‘ç»œå’Œç¦»ç¾¤äº‹ä»¶ï¼›å…¶3Bå‚æ•°æ¶æ„é€‚åˆåœ¨ç§»åŠ¨çº§èŠ¯ç‰‡ä¸Šè¿›è¡Œå®æ—¶æ¨æ–­ï¼Œæ”¯æŒå¤§è§„æ¨¡è¾¹ç¼˜éƒ¨ç½²ï¼›æä¾›å¯è§£é‡Šçš„TSCæµç¨‹ï¼Œå¹¶é€šè¿‡è‡ªæˆ‘è¿­ä»£å’Œæ–°åŒæ­¥é€šä¿¡ç½‘ç»œä¿ƒè¿›å¤šè·¯å£é€šä¿¡ã€‚å®è·µè¡¨æ˜ï¼Œè¯¥æ¨¡å‹ç°å·²ç®¡ç†è¶…è¿‡5ä¸‡äº”åƒåå¸æœºçš„ä¿¡å·ï¼Œå¹³å‡ç¼©çŸ­æ’é˜Ÿæ—¶é—´è¶…è¿‡5%ï¼Œå‡åŠæ“ä½œå·¥ä½œé‡ã€‚æˆ‘ä»¬çš„æ£€æŸ¥ç‚¹ä½äºï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/Season998/Traffic-R1%E3%80%82">https://huggingface.co/Season998/Traffic-R1ã€‚</a> </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Traffic-R1æ˜¯ä¸€ç§å…·æœ‰äººç±»æ€ç»´æ¨ç†èƒ½åŠ›çš„äº¤é€šä¿¡å·æ§åˆ¶ï¼ˆTSCï¼‰ç³»ç»ŸåŸºç¡€æ¨¡å‹ã€‚</li>
<li>Traffic-R1é€šè¿‡è‡ªæˆ‘æ¢ç´¢ä¸è¿­ä»£å¼ºåŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯ï¼Œç»“åˆä¸“å®¶æŒ‡å¯¼è¿›è¡Œå¼€å‘ã€‚</li>
<li>Traffic-R1å…·å¤‡ä¸‰å¤§ä¼˜åŠ¿ï¼šé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€è½»é‡çº§å®æ—¶æ¨æ–­èƒ½åŠ›åŠæä¾›å¯è§£é‡Šçš„TSCæµç¨‹ã€‚</li>
<li>Traffic-R1æ¶æ„æ”¯æŒåœ¨æ–°é“è·¯ç½‘ç»œå’Œç¦»ç¾¤äº‹ä»¶ä¸­çš„å¿«é€Ÿé€‚åº”ã€‚</li>
<li>æ¨¡å‹å¯åœ¨ç§»åŠ¨çº§èŠ¯ç‰‡ä¸Šå®ç°å®æ—¶æ¨æ–­ï¼Œæœ‰åˆ©äºå¤§è§„æ¨¡è¾¹ç¼˜éƒ¨ç½²ã€‚</li>
<li>Traffic-R1èƒ½å¤Ÿä¿ƒè¿›å¤šè·¯å£é€šä¿¡å¹¶å…·å¤‡è‡ªæˆ‘è¿­ä»£èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02344">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-36dd9119fab11882bb8395663aab68e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b2f6daf08c2d16593479e39bd6176c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-176fccc2adb7dd36ef745172cc281bcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-737ddbfdab08ab140c1b0288c708dbd1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0c3e71021b149b58f01e8ddf18bee787.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e1fa0def084d76ac2280eec10a34f10.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Zero-shot-Compositional-Action-Recognition-with-Neural-Logic-Constraints"><a href="#Zero-shot-Compositional-Action-Recognition-with-Neural-Logic-Constraints" class="headerlink" title="Zero-shot Compositional Action Recognition with Neural Logic Constraints"></a>Zero-shot Compositional Action Recognition with Neural Logic Constraints</h2><p><strong>Authors:Gefan Ye, Lin Li, Kexin Li, Jun Xiao, Long chen</strong></p>
<p>Zero-shot compositional action recognition (ZS-CAR) aims to identify unseen verb-object compositions in the videos by exploiting the learned knowledge of verb and object primitives during training. Despite compositional learningâ€™s progress in ZS-CAR, two critical challenges persist: 1) Missing compositional structure constraint, leading to spurious correlations between primitives; 2) Neglecting semantic hierarchy constraint, leading to semantic ambiguity and impairing the training process. In this paper, we argue that human-like symbolic reasoning offers a principled solution to these challenges by explicitly modeling compositional and hierarchical structured abstraction. To this end, we propose a logic-driven ZS-CAR framework LogicCAR that integrates dual symbolic constraints: Explicit Compositional Logic and Hierarchical Primitive Logic. Specifically, the former models the restrictions within the compositions, enhancing the compositional reasoning ability of our model. The latter investigates the semantical dependencies among different primitives, empowering the models with fine-to-coarse reasoning capacity. By formalizing these constraints in first-order logic and embedding them into neural network architectures, LogicCAR systematically bridges the gap between symbolic abstraction and existing models. Extensive experiments on the Sth-com dataset demonstrate that our LogicCAR outperforms existing baseline methods, proving the effectiveness of our logic-driven constraints. </p>
<blockquote>
<p>é›¶æ ·æœ¬ç»„åˆåŠ¨ä½œè¯†åˆ«ï¼ˆZS-CARï¼‰æ—¨åœ¨åˆ©ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¹ å¾—çš„åŠ¨è¯å’ŒåŸå§‹å¯¹è±¡çš„è®¤çŸ¥æ¥è¯†åˆ«è§†é¢‘ä¸­æœªè§è¿‡çš„åŠ¨è¯-å¯¹è±¡ç»„åˆã€‚å°½ç®¡ZS-CARåœ¨ç»„åˆå­¦ä¹ æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ä»ç„¶å­˜åœ¨ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼š1ï¼‰ç¼ºå°‘ç»„åˆç»“æ„çº¦æŸï¼Œå¯¼è‡´åŸå§‹å…ƒç´ ä¹‹é—´çš„è™šå‡å…³è”ï¼›2ï¼‰å¿½ç•¥äº†è¯­ä¹‰å±‚æ¬¡çº¦æŸï¼Œå¯¼è‡´è¯­ä¹‰æ¨¡ç³Šå¹¶å½±å“è®­ç»ƒè¿‡ç¨‹ã€‚æœ¬æ–‡è®¤ä¸ºï¼Œé€šè¿‡æ˜¾å¼å»ºæ¨¡ç»„åˆå’Œå±‚æ¬¡ç»“æ„æŠ½è±¡ï¼Œäººç±»ç±»ä¼¼çš„ç¬¦å·æ¨ç†ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜æä¾›äº†åŸåˆ™æ€§çš„è§£å†³æ–¹æ¡ˆã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé€»è¾‘é©±åŠ¨çš„ZS-CARæ¡†æ¶LogicCARï¼Œè¯¥æ¡†æ¶é›†æˆäº†åŒé‡ç¬¦å·çº¦æŸï¼šæ˜¾å¼ç»„åˆé€»è¾‘å’Œåˆ†å±‚åŸå§‹é€»è¾‘ã€‚å…·ä½“è€Œè¨€ï¼Œå‰è€…å¯¹ç»„åˆå†…çš„é™åˆ¶è¿›è¡Œå»ºæ¨¡ï¼Œå¢å¼ºäº†æˆ‘ä»¬çš„æ¨¡å‹çš„ç»„åˆæ¨ç†èƒ½åŠ›ã€‚åè€…åˆ™ç ”ç©¶ä¸åŒåŸå§‹å…ƒç´ ä¹‹é—´çš„è¯­ä¹‰ä¾èµ–å…³ç³»ï¼Œèµ‹äºˆæ¨¡å‹ç²¾ç»†åˆ°ç²—ç•¥çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ä¸€é˜¶é€»è¾‘å½¢å¼åŒ–è¿™äº›çº¦æŸå¹¶å°†å…¶åµŒå…¥ç¥ç»ç½‘ç»œæ¶æ„ä¸­ï¼ŒLogicCARç³»ç»Ÿåœ°å¡«è¡¥äº†ç¬¦å·æŠ½è±¡å’Œç°æœ‰æ¨¡å‹ä¹‹é—´çš„é¸¿æ²Ÿã€‚åœ¨Sth-comæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„LogicCARä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„é€»è¾‘é©±åŠ¨çº¦æŸçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02320v1">PDF</a> 14 pages, 6 figures; Accepted by ACM MM2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºé›¶æ ·æœ¬ç»„åˆåŠ¨ä½œè¯†åˆ«ï¼ˆZS-CARï¼‰ä¸­çš„æŒ‘æˆ˜ï¼Œå¦‚ç¼ºä¹ç»„åˆç»“æ„çº¦æŸå’Œå¿½è§†è¯­ä¹‰å±‚æ¬¡çº¦æŸï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é€»è¾‘é©±åŠ¨çš„ZS-CARæ¡†æ¶LogicCARã€‚è¯¥æ¡†æ¶é›†æˆäº†åŒé‡ç¬¦å·çº¦æŸï¼ŒåŒ…æ‹¬æ˜ç¡®çš„ç»„åˆé€»è¾‘å’Œå±‚æ¬¡åŸå§‹é€»è¾‘ï¼Œä»¥å¼¥è¡¥ç¬¦å·æŠ½è±¡å’Œç°æœ‰æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚åœ¨Sth-comæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒLogicCARæ¡†æ¶ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ZS-CARé¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šç¼ºä¹ç»„åˆç»“æ„çº¦æŸå’Œå¿½è§†è¯­ä¹‰å±‚æ¬¡çº¦æŸã€‚</li>
<li>äººç±»ç¬¦å·æ¨ç†ä¸ºè§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜æä¾›äº†åŸåˆ™æ€§è§£å†³æ–¹æ¡ˆã€‚</li>
<li>LogicCARæ¡†æ¶é›†æˆäº†åŒé‡ç¬¦å·çº¦æŸï¼šæ˜ç¡®çš„ç»„åˆé€»è¾‘å’Œå±‚æ¬¡åŸå§‹é€»è¾‘ã€‚</li>
<li>ç»„åˆé€»è¾‘æ—¨åœ¨å»ºç«‹ä¸åŒæˆåˆ†ä¹‹é—´çš„é€»è¾‘å…³ç³»ä»¥å¢å¼ºæ¨¡å‹å¯¹ç»„åˆçš„ç†è§£ã€‚</li>
<li>å±‚æ¬¡åŸå§‹é€»è¾‘æ¢ç©¶ä¸åŒæˆåˆ†é—´çš„è¯­ä¹‰ä¾èµ–å…³ç³»ï¼Œå¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>LogicCARé€šè¿‡å½¢å¼åŒ–è¿™äº›çº¦æŸå¹¶å°†å…¶åµŒå…¥ç¥ç»ç½‘ç»œæ¶æ„ä¸­ï¼ŒæˆåŠŸå¡«è¡¥äº†ç¬¦å·æŠ½è±¡ä¸ç°æœ‰æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02320">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7765db0308b40d2df8a1ea31282a6247.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-900b2f8863b8eec58594e79a5c764747.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0428cd03f2dcc07f50d4534a25cba43f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb95e74e0ddd0c49466f91a64f9666dd.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="CAPO-Towards-Enhancing-LLM-Reasoning-through-Verifiable-Generative-Credit-Assignment"><a href="#CAPO-Towards-Enhancing-LLM-Reasoning-through-Verifiable-Generative-Credit-Assignment" class="headerlink" title="CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative   Credit Assignment"></a>CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative   Credit Assignment</h2><p><strong>Authors:Guofu Xie, Yunsheng Shi, Hongtao Tian, Ting Yao, Xiao Zhang</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has improved the reasoning abilities of Large Language Models (LLMs) by using rule-based binary feedback, helping to mitigate reward hacking. However, current RLVR methods typically treat whole responses as single actions, assigning the same reward to every token. This coarse-grained feedback hampers precise credit assignment, making it hard for models to identify which reasoning steps lead to success or failure, and often results in suboptimal policies and inefficient learning. Methods like PPO provide credit assignment through value estimation, but often yield inaccurate and unverifiable signals due to limited sampling. On the other hand, methods using Process Reward Models can provide step-by-step judgments for each reasoning step, but they require high-quality process supervision labels and are time-consuming when applied in online reinforcement learning (RL). To overcome these limitations, we introduce a simple but efficient method Credit Assignment Policy Optimization (CAPO). Given a reasoning response rollout from the policy model, CAPO directly leverages an off-the-shelf, general-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to generate all step-wise critique by one pass, thereby providing verifiable token-level rewards to refine the tokens that were originally assigned identical rule-based rewards. This enables more fine-grained credit assignment in an effective way. Furthermore, to enhance the accuracy and robustness of CAPO, we employ voting mechanisms that scale with the number of generated critiques. Extensive experiments using different backbones like Llama and Qwen models and in different sizes show that CAPO consistently outperforms supervised learning-based and RL-based fine-tuning methods across six challenging mathematical benchmarks and three out-of-domain benchmarks. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰é€šè¿‡ä½¿ç”¨åŸºäºè§„åˆ™çš„äºŒå…ƒåé¦ˆæé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œæœ‰åŠ©äºç¼“è§£å¥–åŠ±é»‘å®¢è¡Œä¸ºã€‚ç„¶è€Œï¼Œå½“å‰çš„RLVRæ–¹æ³•é€šå¸¸å°†æ•´ä¸ªå“åº”è§†ä¸ºå•ä¸ªåŠ¨ä½œï¼Œä¸ºæ¯ä¸ªä»¤ç‰Œåˆ†é…ç›¸åŒçš„å¥–åŠ±ã€‚è¿™ç§ç²—ç²’åº¦çš„åé¦ˆé˜»ç¢äº†ç²¾ç¡®çš„ä¿¡åº¦åˆ†é…ï¼Œä½¿æ¨¡å‹éš¾ä»¥è¯†åˆ«å“ªäº›æ¨ç†æ­¥éª¤å¯¼è‡´æˆåŠŸæˆ–å¤±è´¥ï¼Œå¹¶ä¸”é€šå¸¸å¯¼è‡´æ¬¡ä¼˜ç­–ç•¥å’Œå­¦ä¹ æ•ˆç‡ä½ä¸‹ã€‚åƒPPOè¿™æ ·çš„æ–¹æ³•é€šè¿‡ä»·å€¼ä¼°è®¡æä¾›ä¿¡ç”¨åˆ†é…ï¼Œä½†ç”±äºæœ‰é™çš„é‡‡æ ·ï¼Œé€šå¸¸ä¼šäº§ç”Ÿä¸å‡†ç¡®å’Œä¸å¯éªŒè¯çš„ä¿¡å·ã€‚å¦ä¸€æ–¹é¢ï¼Œä½¿ç”¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„æ–¹æ³•å¯ä»¥ä¸ºæ¯ä¸ªæ¨ç†æ­¥éª¤æä¾›é€æ­¥åˆ¤æ–­ï¼Œä½†å®ƒä»¬éœ€è¦é«˜è´¨é‡çš„è¿‡ç¨‹ç›‘ç£æ ‡ç­¾ï¼Œå¹¶ä¸”åœ¨åº”ç”¨äºåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ—¶å¾ˆè€—æ—¶ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•è€Œé«˜æ•ˆçš„æ–¹æ³•â€”â€”ä¿¡ç”¨åˆ†é…ç­–ç•¥ä¼˜åŒ–ï¼ˆCAPOï¼‰ã€‚ç»™å®šç­–ç•¥æ¨¡å‹çš„æ¨ç†å“åº”å±•å¼€ï¼ŒCAPOç›´æ¥åˆ©ç”¨ç°æˆçš„é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆLLM-as-GenPRMï¼‰ï¼Œé€šè¿‡ä¸€æ¬¡ä¼ é€’ç”Ÿæˆæ‰€æœ‰é€æ­¥çš„è¯„è®ºï¼Œä»è€Œä¸ºåŸæœ¬è¢«èµ‹äºˆç›¸åŒè§„åˆ™å¥–åŠ±çš„ä»¤ç‰Œæä¾›å¯éªŒè¯çš„ä»¤ç‰Œçº§å¥–åŠ±ï¼Œä»¥æ”¹è¿›è¿™äº›ä»¤ç‰Œã€‚è¿™ä½¿å¾—èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¿›è¡Œæ›´ç²¾ç»†çš„ç²’åº¦ä¿¡ç”¨åˆ†é…ã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜CAPOçš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†éšç”Ÿæˆè¯„è®ºæ•°é‡æ‰©å±•çš„æŠ•ç¥¨æœºåˆ¶ã€‚ä½¿ç”¨ä¸åŒæ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œä¸åŒè§„æ¨¡çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCAPOåœ¨å…­ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦åŸºå‡†æµ‹è¯•å’Œä¸‰ä¸ªè·¨åŸŸåŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºåŸºäºç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„å¾®è°ƒæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02298v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong><br>    RLVRå·²æå‡LLMçš„æ¨ç†èƒ½åŠ›ï¼Œä½†å­˜åœ¨å¥–åŠ±åˆ†é…ä¸ç²¾ç»†çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºä¸€ç§æ–°çš„æ–¹æ³•CAPOï¼Œåˆ©ç”¨é€šç”¨LLMä½œä¸ºç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆLLM-as-GenPRMï¼‰ï¼Œåœ¨ä¸€æ¬¡ä¼ é€’ä¸­ç”Ÿæˆæ‰€æœ‰æ­¥éª¤çš„è¯„åˆ¤ï¼Œæä¾›å¯éªŒè¯çš„æ ‡è®°çº§åˆ«å¥–åŠ±ï¼Œæ›´æœ‰æ•ˆåœ°è¿›è¡Œç²¾ç»†å¥–åŠ±åˆ†é…ã€‚é€šè¿‡æŠ•ç¥¨æœºåˆ¶æé«˜CAPOçš„å‡†ç¡®æ€§å’Œç¨³å¥æ€§ï¼Œå®éªŒè¡¨æ˜CAPOåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLVRå·²å¢å¼ºLLMçš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡è§„åˆ™åé¦ˆå‡å°‘å¥–åŠ±æ¬ºéª—ã€‚</li>
<li>å½“å‰RLVRæ–¹æ³•å­˜åœ¨å¥–åŠ±åˆ†é…è¿‡äºç²—ç³™çš„é—®é¢˜ï¼Œéš¾ä»¥è¯†åˆ«æˆåŠŸæˆ–å¤±è´¥çš„æ¨ç†æ­¥éª¤ã€‚</li>
<li>CAPOæ–¹æ³•åˆ©ç”¨é€šç”¨LLMä½œä¸ºç”Ÿæˆè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆLLM-as-GenPRMï¼‰ï¼Œæä¾›å¯éªŒè¯çš„æ ‡è®°çº§åˆ«å¥–åŠ±ã€‚</li>
<li>CAPOé€šè¿‡ä¸€æ¬¡ä¼ é€’ç”Ÿæˆæ‰€æœ‰æ­¥éª¤çš„è¯„åˆ¤ï¼Œå®ç°æ›´æœ‰æ•ˆçš„ç²¾ç»†å¥–åŠ±åˆ†é…ã€‚</li>
<li>CAPOé‡‡ç”¨æŠ•ç¥¨æœºåˆ¶ï¼Œéšç€ç”Ÿæˆçš„è¯„è®ºæ•°é‡å¢åŠ ï¼Œå…¶å‡†ç¡®æ€§å’Œç¨³å¥æ€§å¾—åˆ°æé«˜ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒCAPOåœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•å’Œè·¨åŸŸåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜äºåŸºäºç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02298">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5c99df7ea8bb2947b79bf180a26af0d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd762febb499bf5b662cbd6fd2da8321.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc3f2a2b93bcca504af9fb9db39f96c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a37ca4e63909f8f97aa0b35e2e9feb08.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e4804cbc9b4a25e2541a4dd72ddf56f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b9fa710940cfc1047492bcb4aa7a08bd.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FinWorld-An-All-in-One-Open-Source-Platform-for-End-to-End-Financial-AI-Research-and-Deployment"><a href="#FinWorld-An-All-in-One-Open-Source-Platform-for-End-to-End-Financial-AI-Research-and-Deployment" class="headerlink" title="FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI   Research and Deployment"></a>FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI   Research and Deployment</h2><p><strong>Authors:Wentao Zhang, Yilei Zhao, Chuqiao Zong, Xinrun Wang, Bo An</strong></p>
<p>Financial AI holds great promise for transforming modern finance, with the potential to support a wide range of tasks such as market forecasting, portfolio management, quantitative trading, and automated analysis. However, existing platforms remain limited in task coverage, lack robust multimodal data integration, and offer insufficient support for the training and deployment of large language models (LLMs). In response to these limitations, we present FinWorld, an all-in-one open-source platform that provides end-to-end support for the entire financial AI workflow, from data acquisition to experimentation and deployment. FinWorld distinguishes itself through native integration of heterogeneous financial data, unified support for diverse AI paradigms, and advanced agent automation, enabling seamless development and deployment. Leveraging data from 2 representative markets, 4 stock pools, and over 800 million financial data points, we conduct comprehensive experiments on 4 key financial AI tasks. These experiments systematically evaluate deep learning and reinforcement learning algorithms, with particular emphasis on RL-based finetuning for LLMs and LLM Agents. The empirical results demonstrate that FinWorld significantly enhances reproducibility, supports transparent benchmarking, and streamlines deployment, thereby providing a strong foundation for future research and real-world applications. Code is available at Github~\footnote{<a target="_blank" rel="noopener" href="https://github.com/DVampire/FinWorld%7D">https://github.com/DVampire/FinWorld}</a>. </p>
<blockquote>
<p>é‡‘èäººå·¥æ™ºèƒ½åœ¨ç°ä»£é‡‘èé¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œèƒ½å¤Ÿæ”¯æŒå¸‚åœºé¢„æµ‹ã€æŠ•èµ„ç»„åˆç®¡ç†ã€é‡åŒ–äº¤æ˜“å’Œè‡ªåŠ¨åŒ–åˆ†æç­‰å¤šç§ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰å¹³å°åœ¨ä»»åŠ¡è¦†ç›–ã€ç¼ºä¹ç¨³å¥çš„å¤šæ¨¡å¼æ•°æ®é›†æˆä»¥åŠå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒå’Œéƒ¨ç½²çš„æ”¯æŒä¸è¶³ç­‰æ–¹é¢ä»å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†FinWorldï¼Œè¿™æ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„å¼€æºå¹³å°ï¼Œä¸ºé‡‘èäººå·¥æ™ºèƒ½çš„æ•´ä¸ªå·¥ä½œæµç¨‹æä¾›ç«¯åˆ°ç«¯çš„æ”¯æŒï¼Œä»æ•°æ®é‡‡é›†åˆ°å®éªŒå’Œéƒ¨ç½²ã€‚FinWorldé€šè¿‡åŸç”Ÿé›†æˆå¼‚æ„é‡‘èæ•°æ®ã€ç»Ÿä¸€æ”¯æŒå¤šæ ·åŒ–çš„AIèŒƒå¼å’Œå…ˆè¿›çš„ä»£ç†è‡ªåŠ¨åŒ–ç­‰åŠŸèƒ½ï¼Œå®ç°äº†æ— ç¼çš„å¼€å‘å’Œéƒ¨ç½²ã€‚æˆ‘ä»¬åˆ©ç”¨æ¥è‡ªä¸¤ä¸ªä»£è¡¨æ€§å¸‚åœºã€å››ä¸ªè‚¡ç¥¨æ± å’Œè¶…è¿‡8äº¿ä¸ªé‡‘èæ•°æ®ç‚¹çš„æ•°æ®ï¼Œå¯¹é‡‘èäººå·¥æ™ºèƒ½çš„å››ä¸ªå…³é”®ä»»åŠ¡è¿›è¡Œäº†å…¨é¢çš„å®éªŒã€‚è¿™äº›å®éªŒç³»ç»Ÿåœ°è¯„ä¼°äº†æ·±åº¦å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¹¶ç‰¹åˆ«å¼ºè°ƒäº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„LLMå¾®è°ƒä»¥åŠLLMä»£ç†ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒFinWorldæ˜¾è‘—æé«˜äº†å¯é‡å¤æ€§ï¼Œæ”¯æŒé€æ˜çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶ç®€åŒ–äº†éƒ¨ç½²ï¼Œä»è€Œä¸ºæœªæ¥çš„ç ”ç©¶å’Œå®é™…åº”ç”¨æä¾›äº†åšå®çš„åŸºç¡€ã€‚ä»£ç å¯åœ¨Githubä¸Šæ‰¾åˆ°ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/DVampire/FinWorld%EF%BC%89%E3%80%82">https://github.com/DVampire/FinWorldï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02292v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é‡‘èAIå¯¹ç°ä»£é‡‘èçš„å˜é©å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¯æ”¯æŒå¸‚åœºé¢„æµ‹ã€æŠ•èµ„ç»„åˆç®¡ç†ã€é‡åŒ–äº¤æ˜“å’Œè‡ªåŠ¨åŒ–åˆ†æç­‰å¤šç§ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰å¹³å°å­˜åœ¨ä»»åŠ¡è¦†ç›–é¢æœ‰é™ã€ç¼ºä¹é²æ£’çš„å¤šæ¨¡å¼æ•°æ®é›†æˆä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è®­ç»ƒå’Œéƒ¨ç½²æ”¯æŒä¸è¶³ç­‰é—®é¢˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†FinWorldï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨æ–¹ä½å¼€æºå¹³å°ï¼Œä¸ºé‡‘èAIå·¥ä½œæµæä¾›ä»æ•°æ®é‡‡é›†åˆ°å®éªŒå’Œéƒ¨ç½²çš„ç«¯åˆ°ç«¯æ”¯æŒã€‚é€šè¿‡æ•´åˆå¼‚æ„é‡‘èæ•°æ®ã€ç»Ÿä¸€æ”¯æŒå¤šæ ·åŒ–çš„AIèŒƒå¼ä»¥åŠå…ˆè¿›çš„ä»£ç†è‡ªåŠ¨åŒ–ï¼ŒFinWorldå¯å®ç°æ— ç¼å¼€å‘ä¸éƒ¨ç½²ã€‚åˆ©ç”¨æ¥è‡ªä¸¤ä¸ªä»£è¡¨æ€§å¸‚åœºã€å››ä¸ªè‚¡ç¥¨æ± å’Œè¶…è¿‡8äº¿ä¸ªé‡‘èæ•°æ®ç‚¹çš„æ•°æ®ï¼Œæˆ‘ä»¬åœ¨å››ä¸ªå…³é”®é‡‘èAIä»»åŠ¡ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒã€‚å®éªŒç»“æœè¯æ˜ï¼ŒFinWorldæ˜¾è‘—æé«˜äº†å¯é‡å¤æ€§ã€æ”¯æŒé€æ˜åŸºå‡†æµ‹è¯•å¹¶ç®€åŒ–äº†éƒ¨ç½²ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å’Œå®é™…åº”ç”¨æä¾›äº†åšå®çš„åŸºç¡€ã€‚ä»£ç å·²ä¸Šä¼ è‡³GitHubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡‘èAIå…·æœ‰å˜é©ç°ä»£é‡‘èçš„å·¨å¤§æ½œåŠ›ï¼Œæ¶µç›–å¸‚åœºé¢„æµ‹ã€æŠ•èµ„ç»„åˆç®¡ç†ç­‰å¤šé¡¹ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰é‡‘èAIå¹³å°å­˜åœ¨ä»»åŠ¡è¦†ç›–é¢æœ‰é™ã€æ•°æ®é›†æˆä¸è¶³ç­‰é—®é¢˜ã€‚</li>
<li>FinWorldæ˜¯ä¸€ä¸ªå¼€æºå¹³å°ï¼Œæä¾›é‡‘èAIå·¥ä½œæµçš„ç«¯åˆ°ç«¯æ”¯æŒï¼ŒåŒ…æ‹¬æ•°æ®é‡‡é›†ã€å®éªŒå’Œéƒ¨ç½²ã€‚</li>
<li>FinWorldé€šè¿‡æ•´åˆå¼‚æ„é‡‘èæ•°æ®ã€æ”¯æŒå¤šç§AIèŒƒå¼å’Œä»£ç†è‡ªåŠ¨åŒ–æ¥åŒºåˆ†è‡ªå·±ã€‚</li>
<li>FinWorldé€šè¿‡å®è¯å®éªŒè¯æ˜äº†å…¶æé«˜å¯é‡å¤æ€§ã€æ”¯æŒé€æ˜åŸºå‡†æµ‹è¯•å¹¶ç®€åŒ–éƒ¨ç½²çš„èƒ½åŠ›ã€‚</li>
<li>FinWorldä¸ºæœªæ¥çš„ç ”ç©¶å’Œå®é™…åº”ç”¨æä¾›äº†åšå®çš„åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02292">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3241f6e13b46beff25051a3889f67792.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-22b04d0325cb648261d3f80545763dfd.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Beyond-the-Trade-off-Self-Supervised-Reinforcement-Learning-for-Reasoning-Modelsâ€™-Instruction-Following"><a href="#Beyond-the-Trade-off-Self-Supervised-Reinforcement-Learning-for-Reasoning-Modelsâ€™-Instruction-Following" class="headerlink" title="Beyond the Trade-off: Self-Supervised Reinforcement Learning for   Reasoning Modelsâ€™ Instruction Following"></a>Beyond the Trade-off: Self-Supervised Reinforcement Learning for   Reasoning Modelsâ€™ Instruction Following</h2><p><strong>Authors:Qingyu Ren, Qianyu He, Bowei Zhang, Jie Zeng, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, Fei Yu</strong></p>
<p>Reasoning models excel in complex problem solving but exhibit a concerning trade off between reasoning capabilities and instruction following abilities. Existing approaches for improving instruction following rely on stronger external models, creating methodological bottlenecks and practical limitations including increased costs and accessibility constraints. We propose a self-supervised RL framework that leverages reasoning modelsâ€™ own internal signals to improve instruction following capabilities without external supervision. Extensive experiments demonstrate that our framework significantly improves instruction following capabilities while maintaining reasoning performance, offering a scalable and cost-effective approach to enhance instruction following in reasoning models. The data and code are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Rainier-rq/verl-if">https://github.com/Rainier-rq/verl-if</a>. </p>
<blockquote>
<p>æ¨ç†æ¨¡å‹åœ¨è§£å†³å¤æ‚é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ¨ç†èƒ½åŠ›å’Œæ‰§è¡ŒæŒ‡ä»¤èƒ½åŠ›ä¹‹é—´è¡¨ç°å‡ºä»¤äººæ‹…å¿§çš„æƒè¡¡ã€‚ç°æœ‰æ”¹å–„æ‰§è¡ŒæŒ‡ä»¤çš„æ–¹æ³•ä¾èµ–äºæ›´å¼ºå¤§çš„å¤–éƒ¨æ¨¡å‹ï¼Œè¿™é€ æˆäº†æ–¹æ³•ä¸Šçš„ç“¶é¢ˆå’Œå®é™…é™åˆ¶ï¼ŒåŒ…æ‹¬æˆæœ¬å¢åŠ å’Œå¯è®¿é—®æ€§çº¦æŸã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªæˆ‘ç›‘ç£çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨æ¨ç†æ¨¡å‹æœ¬èº«çš„å†…éƒ¨ä¿¡å·æ¥æé«˜æ‰§è¡ŒæŒ‡ä»¤çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€å¤–éƒ¨ç›‘ç£ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨ä¿æŒæ¨ç†æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†æ‰§è¡ŒæŒ‡ä»¤çš„èƒ½åŠ›ï¼Œä¸ºå¢å¼ºæ¨ç†æ¨¡å‹ä¸­çš„æŒ‡ä»¤æ‰§è¡Œæä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”ç»æµé«˜æ•ˆçš„æ–¹æ³•ã€‚æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Rainier-rq/verl-if">https://github.com/Rainier-rq/verl-if</a>å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02150v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡ä¸­æŒ‡å‡ºï¼Œè™½ç„¶æ¨ç†æ¨¡å‹åœ¨è§£å†³å¤æ‚é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ‰§è¡ŒæŒ‡ä»¤æ–¹é¢å­˜åœ¨å±€é™ã€‚ä¼ ç»Ÿçš„æ”¹è¿›æ–¹æ³•ä¾èµ–äºå¼ºå¤§çš„å¤–éƒ¨æ¨¡å‹ï¼Œå¯¼è‡´äº†æ–¹æ³•ä¸Šçš„ç“¶é¢ˆå’Œå®é™…åº”ç”¨ä¸­çš„é™åˆ¶ï¼Œå¦‚æˆæœ¬å¢åŠ å’Œå¯è®¿é—®æ€§å—é™ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨æ¨ç†æ¨¡å‹å†…éƒ¨ä¿¡å·è¿›è¡Œè‡ªæˆ‘ç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ¡†æ¶ï¼Œä»¥æé«˜æ‰§è¡ŒæŒ‡ä»¤çš„èƒ½åŠ›ï¼Œæ— éœ€å¤–éƒ¨ç›‘ç£ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æå‡æŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›çš„åŒæ—¶ï¼Œä¿æŒäº†æ¨ç†æ€§èƒ½ï¼Œä¸ºå¢å¼ºæ¨ç†æ¨¡å‹çš„æŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›æä¾›äº†å¯æ‰©å±•ä¸”ç»æµé«˜æ•ˆçš„æ–¹æ³•ã€‚ç›¸å…³æ•°æ®å’Œä»£ç å·²å…¬å¼€äº<a target="_blank" rel="noopener" href="https://github.com/Rainier-rq/verl-if%E3%80%82">https://github.com/Rainier-rq/verl-ifã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†æ¨¡å‹åœ¨è§£å†³å¤æ‚é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ‰§è¡ŒæŒ‡ä»¤æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>ä¼ ç»Ÿæ”¹è¿›æ–¹æ³•ä¾èµ–å¤–éƒ¨æ¨¡å‹ï¼Œå­˜åœ¨æ–¹æ³•è®ºä¸Šçš„ç“¶é¢ˆå’Œå®é™…åº”ç”¨ä¸­çš„é™åˆ¶ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‡ªæˆ‘ç›‘ç£å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨æ¨ç†æ¨¡å‹å†…éƒ¨ä¿¡å·æé«˜æ‰§è¡ŒæŒ‡ä»¤çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶æ— éœ€å¤–éƒ¨ç›‘ç£ï¼Œèƒ½æœ‰æ•ˆæé«˜æŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›å¹¶ç»´æŒæ¨ç†æ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ¡†æ¶æ•ˆæœæ˜¾è‘—ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰å¯æ‰©å±•æ€§å’Œç»æµé«˜æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02150">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dc8f64890471136d00c3d877fc51b473.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f10a4d74f6c897f215e5cb20ed99f653.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf2a13d4c57b3abcac51aed24ac6c388.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ada4f73ab12ff60b477ee32f9d435a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6434394249fe44fe2cd16e43fafe3cfd.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="AURORA-Augmented-Understanding-via-Structured-Reasoning-and-Reinforcement-Learning-for-Reference-Audio-Visual-Segmentation"><a href="#AURORA-Augmented-Understanding-via-Structured-Reasoning-and-Reinforcement-Learning-for-Reference-Audio-Visual-Segmentation" class="headerlink" title="AURORA: Augmented Understanding via Structured Reasoning and   Reinforcement Learning for Reference Audio-Visual Segmentation"></a>AURORA: Augmented Understanding via Structured Reasoning and   Reinforcement Learning for Reference Audio-Visual Segmentation</h2><p><strong>Authors:Ziyang Luo, Nian Liu, Fahad Shahbaz Khan, Junwei Han</strong></p>
<p>Reference Audio-Visual Segmentation (Ref-AVS) tasks challenge models to precisely locate sounding objects by integrating visual, auditory, and textual cues. Existing methods often lack genuine semantic understanding, tending to memorize fixed reasoning patterns. Furthermore, jointly training for reasoning and segmentation can compromise pixel-level precision. To address these issues, we introduce AURORA, a novel framework designed to enhance genuine reasoning and language comprehension in reference audio-visual segmentation. We employ a structured Chain-of-Thought (CoT) prompting mechanism to guide the model through a step-by-step reasoning process and introduce a novel segmentation feature distillation loss to effectively integrate these reasoning abilities without sacrificing segmentation performance. To further cultivate the modelâ€™s genuine reasoning capabilities, we devise a further two-stage training strategy: first, a &#96;&#96;corrective reflective-style trainingâ€ stage utilizes self-correction to enhance the quality of reasoning paths, followed by reinforcement learning via Group Reward Policy Optimization (GRPO) to bolster robustness in challenging scenarios. Experiments demonstrate that AURORA achieves state-of-the-art performance on Ref-AVS benchmarks and generalizes effectively to unreferenced segmentation. </p>
<blockquote>
<p>å‚è€ƒéŸ³é¢‘è§†è§‰åˆ†å‰²ï¼ˆRef-AVSï¼‰ä»»åŠ¡é€šè¿‡æ•´åˆè§†è§‰ã€å¬è§‰å’Œæ–‡æœ¬çº¿ç´¢ï¼ŒæŒ‘æˆ˜æ¨¡å‹ç²¾ç¡®å®šä½å‘å£°ç‰©ä½“ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹çœŸæ­£çš„è¯­ä¹‰ç†è§£ï¼Œå€¾å‘äºè®°å¿†å›ºå®šçš„æ¨ç†æ¨¡å¼ã€‚æ­¤å¤–ï¼Œè”åˆè®­ç»ƒå’Œæ¨ç†å’Œåˆ†å‰²ä¼šå½±å“åƒç´ çº§çš„ç²¾ç¡®åº¦ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AURORAï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å¢å¼ºçœŸå®æ¨ç†å’Œè¯­è¨€ç†è§£çš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºå‚è€ƒéŸ³é¢‘è§†è§‰åˆ†å‰²ã€‚æˆ‘ä»¬é‡‡ç”¨ç»“æ„åŒ–çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæœºåˆ¶ï¼Œé€šè¿‡é€æ­¥æ¨ç†è¿‡ç¨‹å¼•å¯¼æ¨¡å‹ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„åˆ†å‰²ç‰¹å¾è’¸é¦æŸå¤±ï¼Œä»¥æœ‰æ•ˆåœ°æ•´åˆè¿™äº›æ¨ç†èƒ½åŠ›ï¼Œè€Œä¸ç‰ºç‰²åˆ†å‰²æ€§èƒ½ã€‚ä¸ºäº†è¿›ä¸€æ­¥åŸ¹å…»æ¨¡å‹çš„çœŸå®æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè¿›ä¸€æ­¥çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šé¦–å…ˆï¼Œâ€œçŸ«æ­£åæ€å¼è®­ç»ƒâ€é˜¶æ®µåˆ©ç”¨è‡ªæˆ‘çŸ«æ­£æ¥æé«˜æ¨ç†è·¯å¾„çš„è´¨é‡ï¼Œç„¶åé€šè¿‡ç¾¤ä½“å¥–åŠ±æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æé«˜åœ¨å¤æ‚åœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒAURORAåœ¨Ref-AVSåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°æ¨å¹¿åˆ°æ— å‚è€ƒåˆ†å‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02149v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§é’ˆå¯¹éŸ³é¢‘è§†è§‰åˆ†å‰²ä»»åŠ¡çš„æ–°å‹æ¡†æ¶AURORAï¼Œæ—¨åœ¨å¢å¼ºçœŸå®æ¨ç†å’Œè¯­è¨€ç†è§£èƒ½åŠ›ã€‚é‡‡ç”¨ç»“æ„åŒ–æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæœºåˆ¶å¼•å¯¼æ¨¡å‹é€æ­¥æ¨ç†ï¼Œå¹¶å¼•å…¥æ–°çš„åˆ†å‰²ç‰¹å¾è’¸é¦æŸå¤±ï¼Œä»¥åœ¨ä¸å½±å“åˆ†å‰²æ€§èƒ½çš„æƒ…å†µä¸‹æ•´åˆè¿™äº›æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„çœŸå®æ¨ç†èƒ½åŠ›ï¼Œè®¾è®¡äº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šé¦–å…ˆï¼Œåˆ©ç”¨è‡ªæˆ‘ä¿®æ­£çš„â€œçº æ­£åæ€å¼è®­ç»ƒâ€é˜¶æ®µæé«˜æ¨ç†è·¯å¾„çš„è´¨é‡ï¼Œç„¶åé€šè¿‡ç¾¤ä½“å¥–åŠ±æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰å¼ºåŒ–å­¦ä¹ æ¥æå‡åœ¨å¤æ‚åœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒAURORAåœ¨Ref-AVSåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶æœ‰æ•ˆåœ°æ¨å¹¿åˆ°äº†æ— å‚è€ƒåˆ†å‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AURORAæ˜¯ä¸€ä¸ªé’ˆå¯¹éŸ³é¢‘è§†è§‰åˆ†å‰²ä»»åŠ¡çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜çœŸå®æ¨ç†å’Œè¯­è¨€ç†è§£èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨ç»“æ„åŒ–æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæœºåˆ¶æ¥å¼•å¯¼æ¨¡å‹è¿›è¡Œé€æ­¥æ¨ç†ã€‚</li>
<li>å¼•å…¥åˆ†å‰²ç‰¹å¾è’¸é¦æŸå¤±ï¼Œæœ‰æ•ˆæ•´åˆæ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¸ç‰ºç‰²åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨äº†ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥æ¥è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„çœŸå®æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µä¸ºçº æ­£åæ€å¼è®­ç»ƒï¼Œåˆ©ç”¨è‡ªæˆ‘ä¿®æ­£æé«˜æ¨ç†è·¯å¾„çš„è´¨é‡ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé‡‡ç”¨ç¾¤ä½“å¥–åŠ±æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰å¼ºåŒ–å­¦ä¹ ï¼Œæå‡æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼ŒAURORAåœ¨Ref-AVSåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ï¼Œå¹¶èƒ½æœ‰æ•ˆæ¨å¹¿è‡³æ— å‚è€ƒåˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02149">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-29e1c144b65fedf373f5979819c92895.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71fba0b5c7dc5102d14cee34af5a6b5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44fb6eca40b0ef8b2978ee823dea42ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82b949a2a4c113f366b85e1ef57f8229.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9de8e8b04f09597c88714d3d816a1d43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21bf469853796eb7e31f1cfb6060d8a2.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Donâ€™t-Overthink-It-A-Survey-of-Efficient-R1-style-Large-Reasoning-Models"><a href="#Donâ€™t-Overthink-It-A-Survey-of-Efficient-R1-style-Large-Reasoning-Models" class="headerlink" title="Donâ€™t Overthink It: A Survey of Efficient R1-style Large Reasoning   Models"></a>Donâ€™t Overthink It: A Survey of Efficient R1-style Large Reasoning   Models</h2><p><strong>Authors:Linan Yue, Yichao Du, Yizhi Wang, Weibo Gao, Fangzhou Yao, Li Wang, Ye Liu, Ziyu Xu, Qi Liu, Shimin Di, Min-Ling Zhang</strong></p>
<p>Recently, Large Reasoning Models (LRMs) have gradually become a research hotspot due to their outstanding performance in handling complex tasks. Among them, DeepSeek R1 has garnered significant attention for its exceptional performance and open-source nature, driving advancements in the research of R1-style LRMs. Unlike traditional Large Language Models (LLMs), these models enhance logical deduction and decision-making capabilities during reasoning by incorporating mechanisms such as long chain-of-thought and self-reflection through reinforcement learning. However, with the widespread application of these models, the problem of overthinking has gradually emerged. Specifically, when generating answers, these models often construct excessively long reasoning chains with redundant or repetitive steps, which leads to reduced reasoning efficiency and may affect the accuracy of the final answer. To this end, various efficient reasoning methods have been proposed, aiming to reduce the length of reasoning paths without compromising model performance and reasoning capability. By reviewing the current research advancements in the field of efficient reasoning methods systematically, we categorize existing works into two main directions based on the lens of single-model optimization versus model collaboration: (1) Efficient Reasoning with Single Model, which focuses on improving the reasoning efficiency of individual models; and (2) Efficient Reasoning with Model Collaboration, which explores optimizing reasoning paths through collaboration among multiple models. Besides, we maintain a public GitHub repository that tracks the latest progress in efficient reasoning methods. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰ç”±äºå…¶å¤„ç†å¤æ‚ä»»åŠ¡çš„å‡ºè‰²æ€§èƒ½é€æ¸æˆä¸ºç ”ç©¶çƒ­ç‚¹ã€‚å…¶ä¸­ï¼ŒDeepSeek R1å› å…¶å“è¶Šçš„æ€§èƒ½å’Œå¼€æºæ€§è´¨å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œæ¨åŠ¨äº†R1é£æ ¼LRMsçš„ç ”ç©¶è¿›å±•ã€‚ä¸ä¼ ç»Ÿçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸åŒï¼Œè¿™äº›æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ ç­‰æœºåˆ¶æé«˜æ¨ç†è¿‡ç¨‹ä¸­çš„é€»è¾‘æ¨å¯¼å’Œå†³ç­–èƒ½åŠ›ï¼Œå¦‚é•¿é“¾æ€ç»´å’Œè‡ªæˆ‘åæ€ã€‚ç„¶è€Œï¼Œéšç€è¿™äº›æ¨¡å‹çš„å¹¿æ³›åº”ç”¨ï¼Œè¿‡åº¦æ€è€ƒçš„é—®é¢˜é€æ¸å‡ºç°ã€‚å…·ä½“åœ°è¯´ï¼Œåœ¨ç”Ÿæˆç­”æ¡ˆæ—¶ï¼Œè¿™äº›æ¨¡å‹å¾€å¾€ä¼šæ„å»ºè¿‡é•¿ä¸”å†—ä½™æˆ–é‡å¤çš„æ¨ç†é“¾ï¼Œå¯¼è‡´æ¨ç†æ•ˆç‡é™ä½ï¼Œå¹¶å¯èƒ½å½±å“æœ€ç»ˆç­”æ¡ˆçš„å‡†ç¡®æ€§ã€‚ä¸ºæ­¤ï¼Œå·²ç»æå‡ºäº†å„ç§é«˜æ•ˆçš„æ¨ç†æ–¹æ³•ï¼Œæ—¨åœ¨ç¼©çŸ­æ¨ç†è·¯å¾„ï¼ŒåŒæ—¶ä¸æŸå®³æ¨¡å‹æ€§èƒ½å’Œæ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°å›é¡¾äº†å½“å‰é«˜æ•ˆæ¨ç†æ–¹æ³•é¢†åŸŸçš„ç ”ç©¶è¿›å±•ï¼Œä»å•ä¸€æ¨¡å‹ä¼˜åŒ–å’Œæ¨¡å‹åä½œçš„è§’åº¦ï¼Œå°†ç°æœ‰å·¥ä½œåˆ†ä¸ºä¸¤ä¸ªä¸»è¦æ–¹å‘ï¼šï¼ˆ1ï¼‰åŸºäºå•ä¸€æ¨¡å‹çš„é«˜æ•ˆæ¨ç†ï¼Œä¸“æ³¨äºæé«˜å•ä¸ªæ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼›ï¼ˆ2ï¼‰åŸºäºæ¨¡å‹åä½œçš„é«˜æ•ˆæ¨ç†ï¼Œæ¢ç´¢é€šè¿‡å¤šä¸ªæ¨¡å‹çš„åä½œæ¥ä¼˜åŒ–æ¨ç†è·¯å¾„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç»´æŠ¤ä¸€ä¸ªå…¬å…±GitHubä»“åº“ï¼Œè·Ÿè¸ªé«˜æ•ˆæ¨ç†æ–¹æ³•çš„æœ€æ–°è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02120v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å°¤å…¶æ˜¯DeepSeek R1å› å…¶å“è¶Šæ€§èƒ½å’Œå¼€æºç‰¹æ€§è€Œå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œæ¨åŠ¨äº†R1é£æ ¼LRMsçš„ç ”ç©¶è¿›å±•ã€‚è¿™äº›æ¨¡å‹é€šè¿‡èå…¥é•¿é“¾æ€ç»´ã€è‡ªæˆ‘åæ€ç­‰æœºåˆ¶ï¼Œå¢å¼ºäº†é€»è¾‘æ¨æ–­å’Œå†³ç­–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿‡æ€é—®é¢˜çš„å‡ºç°é€æ¸æ˜¾ç°ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å¸¸å¸¸æ„å»ºå†—é•¿ã€é‡å¤æ¨ç†é“¾ï¼Œå½±å“æ¨ç†æ•ˆç‡å’Œç­”æ¡ˆå‡†ç¡®æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºé«˜æ•ˆæ¨ç†æ–¹æ³•ï¼Œæ—¨åœ¨ç¼©çŸ­æ¨ç†è·¯å¾„è€Œä¸æŸå®³æ¨¡å‹æ€§èƒ½å’Œæ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡ä»å•ä¸€æ¨¡å‹ä¼˜åŒ–å’Œæ¨¡å‹åä½œä¸¤ä¸ªè§’åº¦ç³»ç»Ÿè¯„è¿°äº†å½“å‰é«˜æ•ˆæ¨ç†æ–¹æ³•çš„ç ”ç©¶è¿›å±•ï¼Œå¹¶ç»´æŠ¤äº†ä¸€ä¸ªå…¬å…±GitHubä»“åº“ä»¥è¿½è¸ªæœ€æ–°è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LRMsï¼Œç‰¹åˆ«æ˜¯DeepSeek R1ï¼Œå› å“è¶Šæ€§èƒ½å’Œå¼€æºç‰¹æ€§å¤‡å—å…³æ³¨ï¼Œæ¨åŠ¨R1é£æ ¼LRMsç ”ç©¶ã€‚</li>
<li>LRMsé€šè¿‡èå…¥é•¿é“¾æ€ç»´ã€è‡ªæˆ‘åæ€ç­‰æœºåˆ¶å¢å¼ºé€»è¾‘æ¨æ–­å’Œå†³ç­–èƒ½åŠ›ã€‚</li>
<li>è¿‡æ€é—®é¢˜é€æ¸æ˜¾ç°ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶æ„å»ºå†—é•¿ã€é‡å¤æ¨ç†é“¾ï¼Œå½±å“æ¨ç†æ•ˆç‡å’Œç­”æ¡ˆå‡†ç¡®æ€§ã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºé«˜æ•ˆæ¨ç†æ–¹æ³•ï¼Œæ—¨åœ¨ç¼©çŸ­æ¨ç†è·¯å¾„è€Œä¸æŸå®³æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ç³»ç»Ÿè¯„è¿°äº†å•ä¸€æ¨¡å‹ä¼˜åŒ–å’Œæ¨¡å‹åä½œä¸¤ä¸ªè§’åº¦çš„é«˜æ•ˆæ¨ç†æ–¹æ³•ç ”ç©¶è¿›å±•ã€‚</li>
<li>å•ä¸€æ¨¡å‹ä¼˜åŒ–æ–¹æ³•è‡´åŠ›äºæå‡å•ä¸ªæ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02120">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5bbb23e5d2482d76ead944098de1ab88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87a433b165018f2f455505e407c5b26b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c07a11142b8f7dad670777aa6a00a62.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SE-Agent-Self-Evolution-Trajectory-Optimization-in-Multi-Step-Reasoning-with-LLM-Based-Agents"><a href="#SE-Agent-Self-Evolution-Trajectory-Optimization-in-Multi-Step-Reasoning-with-LLM-Based-Agents" class="headerlink" title="SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning   with LLM-Based Agents"></a>SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning   with LLM-Based Agents</h2><p><strong>Authors:Jiaye Lin, Yifu Guo, Yuzhen Han, Sen Hu, Ziyi Ni, Licheng Wang, Mingguang Chen, Daxin Jiang, Binxing Jiao, Chen Hu, Huacan Wang</strong></p>
<p>Large Language Model (LLM)-based agents have recently shown impressive capabilities in complex reasoning and tool use via multi-step interactions with their environments. While these agents have the potential to tackle complicated tasks, their problem-solving process, i.e., agentsâ€™ interaction trajectory leading to task completion, remains underexploited. These trajectories contain rich feedback that can navigate agents toward the right directions for solving problems correctly. Although prevailing approaches, such as Monte Carlo Tree Search (MCTS), can effectively balance exploration and exploitation, they ignore the interdependence among various trajectories and lack the diversity of search spaces, which leads to redundant reasoning and suboptimal outcomes. To address these challenges, we propose SE-Agent, a Self-Evolution framework that enables Agents to optimize their reasoning processes iteratively. Our approach revisits and enhances former pilot trajectories through three key operations: revision, recombination, and refinement. This evolutionary mechanism enables two critical advantages: (1) it expands the search space beyond local optima by intelligently exploring diverse solution paths guided by previous trajectories, and (2) it leverages cross-trajectory inspiration to efficiently enhance performance while mitigating the impact of suboptimal reasoning paths. Through these mechanisms, SE-Agent achieves continuous self-evolution that incrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench Verified to resolve real-world GitHub issues. Experimental results across five strong LLMs show that integrating SE-Agent delivers up to 55% relative improvement, achieving state-of-the-art performance among all open-source agents on SWE-bench Verified. Our code and demonstration materials are publicly available at <a target="_blank" rel="noopener" href="https://github.com/wanghuacan/SE-Agent">https://github.com/wanghuacan/SE-Agent</a>. </p>
<blockquote>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç†æœ€è¿‘è¡¨ç°å‡ºåœ¨å¤æ‚æ¨ç†å’Œå·¥å…·ä½¿ç”¨æ–¹é¢çš„ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œè¿™æ˜¯é€šè¿‡ä¸ç¯å¢ƒçš„å¤šæ­¥éª¤äº¤äº’å®ç°çš„ã€‚è™½ç„¶è¿™äº›ä»£ç†æœ‰æ½œåŠ›å¤„ç†å¤æ‚ä»»åŠ¡ï¼Œä½†å®ƒä»¬çš„è§£å†³é—®é¢˜è¿‡ç¨‹ï¼Œå³ä»£ç†å®Œæˆä»»åŠ¡çš„äº¤äº’è½¨è¿¹ï¼Œä»è¢«ä½ä¼°ã€‚è¿™äº›è½¨è¿¹åŒ…å«ä¸°å¯Œçš„åé¦ˆï¼Œå¯ä»¥å¼•å¯¼ä»£ç†æœç€æ­£ç¡®çš„æ–¹å‘è§£å†³é—®é¢˜ã€‚å°½ç®¡ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨ï¼Œä½†å®ƒä»¬å¿½ç•¥äº†å„ç§è½¨è¿¹ä¹‹é—´çš„ç›¸äº’ä¾èµ–æ€§ï¼Œå¹¶ä¸”ç¼ºä¹æœç´¢ç©ºé—´çš„å¤šæ ·æ€§ï¼Œè¿™å¯¼è‡´äº†å†—ä½™æ¨ç†å’Œæ¬¡ä¼˜ç»“æœã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SE-Agentï¼Œè¿™æ˜¯ä¸€ç§è‡ªæˆ‘è¿›åŒ–æ¡†æ¶ï¼Œä½¿ä»£ç†èƒ½å¤Ÿè¿­ä»£åœ°ä¼˜åŒ–ä»–ä»¬çš„æ¨ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä¸‰ä¸ªå…³é”®æ“ä½œï¼šä¿®è®¢ã€é‡ç»„å’Œç²¾ç‚¼ï¼Œé‡æ–°è®¿é—®å¹¶å¢å¼ºå…ˆå‰çš„è½¨è¿¹ã€‚è¿™ç§è¿›åŒ–æœºåˆ¶å¸¦æ¥äº†ä¸¤ä¸ªå…³é”®ä¼˜åŠ¿ï¼šï¼ˆ1ï¼‰å®ƒé€šè¿‡æ™ºèƒ½åœ°æ¢ç´¢ç”±å…ˆå‰è½¨è¿¹å¼•å¯¼çš„å¤šæ ·åŒ–è§£å†³æ–¹æ¡ˆè·¯å¾„ï¼Œæ‰©å¤§äº†æœç´¢ç©ºé—´ï¼Œè¶…è¶Šäº†å±€éƒ¨æœ€ä¼˜ï¼›ï¼ˆ2ï¼‰å®ƒåˆ©ç”¨è·¨è½¨è¿¹çš„çµæ„Ÿæ¥æœ‰æ•ˆåœ°æé«˜æ€§èƒ½ï¼ŒåŒæ—¶å‡è½»æ¬¡ä¼˜æ¨ç†è·¯å¾„çš„å½±å“ã€‚é€šè¿‡è¿™äº›æœºåˆ¶ï¼ŒSE-Agentå®ç°äº†è¿ç»­çš„è‡ªæˆ‘è¿›åŒ–ï¼Œé€æ­¥æé«˜äº†æ¨ç†è´¨é‡ã€‚æˆ‘ä»¬åœ¨SWE-bench Verifiedä¸Šè¯„ä¼°äº†SE-Agentï¼Œä»¥è§£å†³ç°å®ä¸–ç•Œä¸­çš„GitHubé—®é¢˜ã€‚åœ¨äº”ä¸ªå¼ºå¤§çš„LLMä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œé›†æˆSE-Agentå¸¦æ¥äº†é«˜è¾¾55%çš„ç›¸å¯¹æ”¹è¿›ï¼Œåœ¨SWE-bench Verifiedä¸Šçš„æ€§èƒ½è¾¾åˆ°äº†å¼€æºä»£ç†ä¸­çš„æœ€æ–°æ°´å¹³ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¼”ç¤ºææ–™å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wanghuacan/SE-Agent">https://github.com/wanghuacan/SE-Agent</a>å…¬å¼€è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02085v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤æ‚æ¨ç†å’Œå·¥å…·ä½¿ç”¨æ–¹é¢å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œé€šè¿‡ä¸ç¯å¢ƒçš„å¤šæ­¥äº¤äº’æ¥å®Œæˆä»»åŠ¡ã€‚ç„¶è€Œï¼ŒLLMçš„é—®é¢˜è§£å†³è¿‡ç¨‹ï¼ˆå³å®Œæˆä»»åŠ¡æ—¶ä»£ç†çš„äº¤äº’è½¨è¿¹ï¼‰å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚æœ¬æ–‡æå‡ºSE-Agentï¼Œä¸€ä¸ªè‡ªæˆ‘è¿›åŒ–æ¡†æ¶ï¼Œé€šè¿‡ä¿®è®¢ã€é‡ç»„å’Œç»†åŒ–å…ˆå‰çš„è½¨è¿¹ï¼Œä½¿ä»£ç†èƒ½å¤Ÿä¼˜åŒ–å…¶æ¨ç†è¿‡ç¨‹ã€‚è¿™ç§è¿›åŒ–æœºåˆ¶æœ‰åŠ©äºæ™ºèƒ½æ¢ç´¢å¤šç§è§£å†³æ–¹æ¡ˆè·¯å¾„ï¼Œå¹¶åˆ©ç”¨è·¨è½¨è¿¹çµæ„Ÿæé«˜æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSE-Agentåœ¨è§£å†³ç°å®ä¸–ç•ŒGitHubé—®é¢˜ä¸Šå®ç°äº†é«˜è¾¾55%çš„ç›¸å¯¹æ”¹è¿›ï¼Œè¾¾åˆ°äº†å¼€æºä»£ç†ä¸­çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤æ‚æ¨ç†å’Œå·¥å…·ä½¿ç”¨æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>LLMçš„é—®é¢˜è§£å†³è¿‡ç¨‹ï¼ˆäº¤äº’è½¨è¿¹ï¼‰å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚</li>
<li>SE-Agentæ˜¯ä¸€ä¸ªè‡ªæˆ‘è¿›åŒ–çš„æ¡†æ¶ï¼Œé€šè¿‡ä¿®è®¢ã€é‡ç»„å’Œç»†åŒ–ä¹‹å‰çš„è½¨è¿¹æ¥ä¼˜åŒ–ä»£ç†çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>SE-Agentæ‰©å¤§äº†æœç´¢ç©ºé—´ï¼Œé€šè¿‡æ™ºèƒ½æ¢ç´¢å¤šç§è§£å†³æ–¹æ¡ˆè·¯å¾„æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>SE-Agentåˆ©ç”¨è·¨è½¨è¿¹çµæ„Ÿï¼Œæé«˜äº†æ€§èƒ½å¹¶å‡è½»äº†æ¬¡ä¼˜æ¨ç†è·¯å¾„çš„å½±å“ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSE-Agentåœ¨è§£å†³ç°å®ä¸–ç•Œé—®é¢˜ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0b7f2fbb98b7e8c38dc87889e0d9ce0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bea939b287932cfc986fed5e11ab9cc.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="The-SMeL-Test-A-simple-benchmark-for-media-literacy-in-language-models"><a href="#The-SMeL-Test-A-simple-benchmark-for-media-literacy-in-language-models" class="headerlink" title="The SMeL Test: A simple benchmark for media literacy in language models"></a>The SMeL Test: A simple benchmark for media literacy in language models</h2><p><strong>Authors:Gustaf Ahdritz, Anat Kleiman</strong></p>
<p>The internet is rife with unattributed, deliberately misleading, or otherwise untrustworthy content. Though large language models (LLMs) are often tasked with autonomous web browsing, the extent to which they have learned the simple heuristics human researchers use to navigate this noisy environment is not currently known. In this paper, we introduce the Synthetic Media Literacy Test (SMeL Test), a minimal benchmark that tests the ability of language models to actively filter out untrustworthy information in context. We benchmark a variety of commonly used instruction-tuned LLMs, including reasoning models, and find that no model consistently trusts more reliable sources; while reasoning in particular is associated with higher scores, even the best API model we test hallucinates up to 70% of the time. Remarkably, larger and more capable models do not necessarily outperform their smaller counterparts. We hope our work sheds more light on this important form of hallucination and guides the development of new methods to combat it. </p>
<blockquote>
<p>äº’è”ç½‘å……æ–¥ç€æœªç»è¯å®ã€æ•…æ„è¯¯å¯¼æˆ–å…¶ä»–ä¸å¯ä¿¡èµ–çš„å†…å®¹ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸è¢«èµ‹äºˆè‡ªä¸»ç½‘é¡µæµè§ˆçš„ä»»åŠ¡ï¼Œä½†å®ƒä»¬å­¦ä¹ äººç±»ç ”ç©¶è€…ç”¨äºæµè§ˆè¿™ç§å˜ˆæ‚ç¯å¢ƒçš„ç®€å•å¯å‘å¼ç­–ç•¥çš„ç¨‹åº¦å°šä¸æ¸…æ¥šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åˆæˆåª’ä½“ç´ å…»æµ‹è¯•ï¼ˆSMeLæµ‹è¯•ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ€å°çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æµ‹è¯•è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šæƒ…å¢ƒä¸­ä¸»åŠ¨è¿‡æ»¤æ‰ä¸å¯é ä¿¡æ¯çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹å„ç§å¸¸ç”¨çš„æŒ‡ä»¤è°ƒæ•´å‹LLMè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬æ¨ç†æ¨¡å‹ï¼Œå‘ç°æ²¡æœ‰ä»»ä½•æ¨¡å‹èƒ½å¤Ÿå§‹ç»ˆå¦‚ä¸€åœ°ä¿¡ä»»æ›´å¯é çš„æ¥æºï¼›è™½ç„¶æ¨ç†ä¸æ›´é«˜çš„åˆ†æ•°ç‰¹åˆ«ç›¸å…³ï¼Œä½†å³ä½¿æ˜¯æˆ‘ä»¬æµ‹è¯•çš„æœ€ä½³APIæ¨¡å‹ï¼Œä¹Ÿæœ‰é«˜è¾¾70%çš„æ—¶é—´ä¼šå‡ºç°å¹»è§‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ›´å¤§ã€æ›´å¼ºå¤§çš„æ¨¡å‹å¹¶ä¸ä¸€å®šæ¯”å°å‹æ¨¡å‹è¡¨ç°æ›´å¥½ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œèƒ½è¿›ä¸€æ­¥æ­ç¤ºè¿™ç§é‡è¦çš„å¹»è§‰å½¢å¼ï¼Œå¹¶ä¸ºå¼€å‘æ–°çš„å¯¹æŠ—æ–¹æ³•æä¾›æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02074v1">PDF</a> </p>
<p><strong>Summary</strong><br>äº’è”ç½‘å……æ–¥ç€æœªç»è¯å®ã€æ•…æ„è¯¯å¯¼æˆ–å…¶ä»–ä¸å¯é çš„å†…å®¹ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¸¸è¢«ç”¨äºè‡ªä¸»æµè§ˆç½‘é¡µï¼Œä½†å®ƒä»¬æ˜¯å¦æŒæ¡äº†äººç±»ç ”ç©¶è€…ç”¨äºå¯¼èˆªè¿™ç§å˜ˆæ‚ç¯å¢ƒçš„ç®€å•å¯å‘å¼æŠ€æœ¯å°šä¸æ¸…æ¥šã€‚æœ¬æ–‡ä»‹ç»äº†åˆæˆåª’ä»‹ç´ å…»æµ‹è¯•ï¼ˆSMeLæµ‹è¯•ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ€å°çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºæµ‹è¯•è¯­è¨€æ¨¡å‹åœ¨æƒ…å¢ƒä¸­ä¸»åŠ¨è¿‡æ»¤æ‰ä¸å¯é ä¿¡æ¯çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹ä¸€ç³»åˆ—å¸¸ç”¨çš„æŒ‡ä»¤è°ƒæ•´å‹LLMsè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬æ¨ç†æ¨¡å‹ï¼Œå‘ç°æ²¡æœ‰ä»»ä½•æ¨¡å‹å§‹ç»ˆä¿¡ä»»æ›´å¯é çš„æ¥æºï¼›è™½ç„¶æ¨ç†ä¸æ›´é«˜çš„åˆ†æ•°æœ‰å…³ï¼Œä½†å³ä½¿æˆ‘ä»¬æµ‹è¯•çš„æœ€ä½³APIæ¨¡å‹ä¹Ÿä¼šè¾¾åˆ°70%çš„å¹»è§‰ç°è±¡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ›´å¤§ã€æ›´å¼ºå¤§çš„æ¨¡å‹å¹¶ä¸ä¸€å®šä¼˜äºè¾ƒå°çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç ”ç©¶èƒ½æ›´å¤šåœ°å…³æ³¨è¿™ç§å¹»è§‰ç°è±¡çš„å‘å±•å¹¶å¼•å¯¼å¼€å‘æ–°æ–¹æ³•åº”å¯¹å¹»è§‰ç°è±¡çš„å‡ºç°ã€‚å¸Œæœ›è¿™ä¸€æµ‹è¯•ä¸ºæœªæ¥è®¾è®¡æœ‰æ•ˆçš„ç­–ç•¥ä»¥æé«˜LLMsè¾¨åˆ«çœŸå®ä¸å‡ä¿¡æ¯çš„èƒ½åŠ›æä¾›äº†å¯èƒ½çš„æ–¹å‘ã€‚æ­¤å¤–æˆ‘ä»¬ä¹ŸæŒ‡å‡ºè¿™å¯ä»¥ä½œä¸ºè¯„ä»·AIç½‘ç»œç´ å…»çš„ä¸€ä¸ªé‡è¦æ ‡å‡†ä¹‹ä¸€ï¼Œå…·æœ‰é‡è¦çš„åº”ç”¨ä»·å€¼å’Œå®é™…æ„ä¹‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¯ä»¥ä¸ºå¼€å‘æ–°å‹çš„äººå·¥æ™ºèƒ½åª’ä½“ç´ å…»è¯„ä¼°å·¥å…·æä¾›å‚è€ƒå’Œå€Ÿé‰´ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡è¿™é¡¹ç ”ç©¶å¼•å¯¼ç ”ç©¶äººå‘˜å’Œè¡Œä¸šä»ä¸šè€…åœ¨å¼€å‘å¤§å‹è¯­è¨€æ¨¡å‹æ—¶å…³æ³¨å…¶åœ¨è¿‡æ»¤ç½‘ç»œä¸­çš„ä¸å®ä¿¡æ¯èƒ½åŠ›ã€‚ä¸ºæ­¤æˆ‘ä»¬çš„å·¥ä½œæœªæ¥ä»æœ‰å¾…æ·±åŒ–å’Œå®Œå–„ï¼Œé€šè¿‡ä¸æ–­æ”¹è¿›æ¨¡å‹å’Œç®—æ³•æ¥å¢å¼ºæ¨¡å‹çš„è¯†åˆ«èƒ½åŠ›å¹¶æ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•ã€‚æœªæ¥æˆ‘ä»¬ä¹ŸæœŸå¾…æœ‰æ›´å¤šç›¸å…³ç ”ç©¶æ¥éªŒè¯å’Œæ”¹è¿›åˆæˆåª’ä»‹ç´ å…»æµ‹è¯•çš„æ•ˆæœå’Œé€‚ç”¨æ€§ï¼Œä»è€Œä¸ºæ¨åŠ¨AIç½‘ç»œç´ å…»çš„æé«˜åšå‡ºæ›´å¤§çš„è´¡çŒ®ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„å·¥ä½œå¯¹äºæ¨åŠ¨äººå·¥æ™ºèƒ½çš„å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚æˆ‘ä»¬ç›¸ä¿¡é€šè¿‡ä¸æ–­çš„åŠªåŠ›å’Œç ”ç©¶æˆ‘ä»¬å¯ä»¥æé«˜äººå·¥æ™ºèƒ½çš„åª’ä½“ç´ å…»æ°´å¹³ä»è€Œæ›´å¥½åœ°åº”å¯¹ç½‘ç»œä¿¡æ¯çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬ç›¸ä¿¡äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•å°†æ›´åŠ æ³¨é‡å…¶åª’ä½“ç´ å…»çš„æå‡å¹¶å¸¦æ¥æ›´åŠ ç§¯æçš„å½±å“å’Œè´¡çŒ®ã€‚æˆ‘ä»¬ç›¸ä¿¡æˆ‘ä»¬çš„ç ”ç©¶å°†æ¨åŠ¨äººå·¥æ™ºèƒ½åœ¨çœŸå®ä¸–ç•Œä¸­çš„æœ‰æ•ˆåº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åº”å¯¹æ—¥ç›Šå¢é•¿çš„è™šå‡ä¿¡æ¯é—®é¢˜ã€‚è¿™å°†ä¸ºäººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•å¼€è¾Ÿæ–°çš„æ–¹å‘å¹¶è§£å†³ç°å®é—®é¢˜å®ç°ç§‘æŠ€çš„å®é™…åº”ç”¨å’Œç¤¾ä¼šæ•ˆç›Šçš„ç»Ÿä¸€å’Œæå‡åˆ›é€ å·¨å¤§çš„æ½œåŠ›ã€‚ã€‚åœ¨æœªæ¥äººå·¥æ™ºèƒ½å‘å±•ä¸­èƒ½å¤Ÿå¼•é¢†æ–°æŠ€æœ¯ç ”å‘åŠ©åŠ›ç§‘å­¦è¿›æ­¥ã€‚ä¸ºæ­¤æˆ‘ä»¬å°†æŒç»­å¼€å±•ç ”ç©¶è‡´åŠ›äºè§£å†³ç›¸å…³é¢†åŸŸçš„éš¾é¢˜æ¨åŠ¨æŠ€æœ¯çš„è¿›æ­¥å’Œå‘å±•ä»¥æ›´å¥½åœ°æœåŠ¡ç¤¾ä¼šå’Œé€ ç¦äººç±»ã€‚ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½å¤Ÿå¸®åŠ©äººä»¬æ›´å¥½åœ°ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ä½•æ“ä½œå¹¶å°†å…¶ç”¨äºæ”¹è¿›çœŸå®çš„è‡ªç„¶è¯­è¨€ç†è§£å’Œå¯¹è¯æŠ€æœ¯æœåŠ¡äºç”¨æˆ·çš„å‘å±•éœ€è¦å¹¶é€šè¿‡ç§‘å­¦æ–¹æ³•å¼€å‘è¯­è¨€ç›¸å…³çš„å…¶ä»–æ½œåŠ›æ›´å¤§èŒƒå›´çš„å°†å…¶æ¨å¹¿åˆ°å„ç±»ç¤¾ä¼šå·¥ä½œä¸­ä»¥æœŸæé«˜æ•´ä¸ªç¤¾ä¼šçš„è¯­è¨€ç´ å…»å’Œæ°´å¹³æ¨åŠ¨ç¤¾ä¼šçš„å…¨é¢è¿›æ­¥å’Œå‘å±•ã€‚ã€‚æˆ‘ä»¬ç›¸ä¿¡è¿™é¡¹å·¥ä½œå°†ä¸ºäººå·¥æ™ºèƒ½é¢†åŸŸå¸¦æ¥é‡è¦çš„è´¡çŒ®å’Œå¯ç¤ºæ¨åŠ¨å…¶ä¸æ–­è¿›æ­¥å’Œå‘å±•å¹¶é€ ç¦äºäººç±»ã€‚ã€‚æˆ‘ä»¬å°†åœ¨åç»­çš„ç ”ç©¶å·¥ä½œä¸­è¿›ä¸€æ­¥å®Œå–„å’Œæå‡ç›¸å…³ç ”ç©¶æ€è·¯å’Œæ‰‹æ®µä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯†åˆ«å’Œåˆ¤æ–­èƒ½åŠ›è´¡çŒ®æ›´å¤šæ›´å¥½çš„ç§‘ç ”æˆæœå’Œç ”ç©¶æˆæœä»¥è§£å†³ç¤¾ä¼šä¸Šçš„å„ç§é—®é¢˜å¹¶æä¾›æ›´æœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘å’ŒæœåŠ¡ä¿éšœä»¥å®ç°ç¤¾ä¼šçš„å…¨é¢è¿›æ­¥å’Œå‘å±•ã€‚ã€‚æ€»çš„æ¥è¯´æˆ‘ä»¬å¸Œæœ›é€šè¿‡æˆ‘ä»¬çš„ç ”ç©¶èƒ½å¤Ÿä¸ºäººå·¥æ™ºèƒ½é¢†åŸŸçš„å‘å±•åšå‡ºæ›´å¤§çš„è´¡çŒ®å’Œå¯ç¤ºä¸ºäººç±»çš„ç§‘æŠ€è¿›æ­¥å’Œå‘å±•åšå‡ºæ›´å¤šçš„è´¡çŒ®å’ŒåŠªåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äº’è”ç½‘ä¿¡æ¯ç­›é€‰æ–¹é¢çš„èƒ½åŠ›å°šä¸æ¸…æ¥šã€‚</li>
<li>åˆæˆåª’ä»‹ç´ å…»æµ‹è¯•ï¼ˆSMeLæµ‹è¯•ï¼‰è¢«å¼•å…¥ä½œä¸ºè¯„ä¼°LLMsè¿‡æ»¤ä¸çœŸå®ä¿¡æ¯èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æ¨ç†æ¨¡å‹è™½ç„¶åœ¨æµ‹è¯•ä¸­è¡¨ç°è¾ƒå¥½ï¼Œä½†ä»å­˜åœ¨é«˜è¾¾70%çš„å¹»è§‰ç°è±¡ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å¹¶ä¸ä¸€å®šä¼˜äºè¾ƒå°çš„æ¨¡å‹ã€‚</li>
<li>å½“å‰ç ”ç©¶æŒ‡å‡ºäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç½‘ç»œä¿¡æ¯æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶å’Œæ”¹è¿›æä¾›äº†æ–¹å‘ã€‚</li>
<li>è¯¥ç ”ç©¶å¼ºè°ƒäº†åª’ä½“ç´ å…»åœ¨äººå·¥æ™ºèƒ½å‘å±•ä¸­çš„é‡è¦æ€§ï¼Œå¹¶å»ºè®®ç ”ç©¶äººå‘˜å…³æ³¨æ­¤é¢†åŸŸçš„ç ”ç©¶ä¸å¼€å‘å·¥ä½œä»¥æå‡AIåº”å¯¹ç½‘ç»œä¿¡æ¯æŒ‘æˆ˜çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02074">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-875443754a2917e78567871a9a18aa6c.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MolReasoner-Toward-Effective-and-Interpretable-Reasoning-for-Molecular-LLMs"><a href="#MolReasoner-Toward-Effective-and-Interpretable-Reasoning-for-Molecular-LLMs" class="headerlink" title="MolReasoner: Toward Effective and Interpretable Reasoning for Molecular   LLMs"></a>MolReasoner: Toward Effective and Interpretable Reasoning for Molecular   LLMs</h2><p><strong>Authors:Guojiang Zhao, Sihang Li, Zixiang Lu, Zheng Cheng, Haitao Lin, Lirong Wu, Hanchen Xia, Hengxing Cai, Wentao Guo, Hongshuai Wang, Mingjun Xu, Siyu Zhu, Guolin Ke, Linfeng Zhang, Zhifeng Gao</strong></p>
<p>Large Language Models(LLMs) have demonstrated remarkable performance across various domains, yet their capabilities in molecular reasoning remain insufficiently explored. Current approaches tend to rely heavily on general-purpose prompting, which lacks domain-specific molecular semantics, while those that use fine-tuning strategies often face challenges with interpretability and reasoning depth. To address these issues, we introduce MolReasoner, a two-stage framework designed to transition LLMs from memorization towards chemical reasoning. First, we propose Mol-SFT, which initializes the modelâ€™s reasoning abilities via synthetic Chain-of-Thought(CoT) samples generated by GPT-4o and verified for chemical accuracy. Subsequently, Mol-RL applies reinforcement learning with specialized reward functions designed explicitly to align chemical structures with linguistic descriptions, thereby enhancing molecular reasoning capabilities. Our approach notably enhances interpretability, improving the model â€˜s molecular understanding and enabling better generalization. Extensive experiments demonstrate that MolReasoner outperforms existing methods, and marking a significant shift from memorization-based outputs to robust chemical reasoning. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å„ä¸ªé¢†åŸŸéƒ½è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨åˆ†å­æ¨ç†æ–¹é¢çš„èƒ½åŠ›å´å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚å½“å‰çš„æ–¹æ³•å¾€å¾€è¿‡äºä¾èµ–é€šç”¨æç¤ºï¼Œç¼ºä¹é¢†åŸŸç‰¹å®šçš„åˆ†å­è¯­ä¹‰ï¼Œè€Œä½¿ç”¨å¾®è°ƒç­–ç•¥çš„æ–¹æ³•åˆ™å¸¸å¸¸é¢ä¸´å¯è§£é‡Šæ€§å’Œæ¨ç†æ·±åº¦æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†MolReasonerï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿LLMsä»è®°å¿†è½¬å‘åŒ–å­¦æ¨ç†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†Mol-SFTï¼Œé€šè¿‡GPT-4oç”Ÿæˆå¹¶ç»åŒ–å­¦å‡†ç¡®æ€§éªŒè¯çš„åˆæˆæ€ç»´é“¾ï¼ˆCoTï¼‰æ ·æœ¬å¯¹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›è¿›è¡Œåˆå§‹åŒ–ã€‚éšåï¼ŒMol-RLåº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œå¹¶è®¾è®¡ä¸“é—¨çš„å¥–åŠ±å‡½æ•°ï¼Œä»¥å°†åŒ–å­¦ç»“æ„ä¸è¯­è¨€æè¿°å¯¹é½ï¼Œä»è€Œæé«˜åˆ†å­æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†å¯è§£é‡Šæ€§ï¼Œæé«˜äº†æ¨¡å‹å¯¹åˆ†å­çš„ç†è§£ï¼Œå¹¶å®ç°äº†æ›´å¥½çš„æ³›åŒ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMolReasonerä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ ‡å¿—ç€ä»åŸºäºè®°å¿†çš„è¾“å‡ºæ¥åˆ°ç¨³å¥çš„åŒ–å­¦æ¨ç†çš„é‡å¤§è½¬å˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02066v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šé¢†åŸŸè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†åœ¨åˆ†å­æ¨ç†æ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–é€šç”¨æç¤ºï¼Œç¼ºä¹é¢†åŸŸç‰¹å®šçš„åˆ†å­è¯­ä¹‰ï¼Œè€Œé‡‡ç”¨å¾®è°ƒç­–ç•¥çš„æ–¹æ³•åˆ™å¸¸é¢ä¸´å¯è§£é‡Šæ€§å’Œæ¨ç†æ·±åº¦æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºMolReasonerï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿LLMsä»è®°å¿†è¿‡æ¸¡åˆ°åŒ–å­¦æ¨ç†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºMol-SFTï¼Œé€šè¿‡åˆæˆç”Ÿæˆç”±GPT-4oéªŒè¯åŒ–å­¦å‡†ç¡®æ€§çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ ·æœ¬ï¼Œåˆæ­¥æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚æ¥ç€ï¼ŒMol-RLåº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œå¹¶è®¾è®¡ä¸“é—¨é’ˆå¯¹åŒ–å­¦ç»“æ„ä¸è¯­è¨€æè¿°å¯¹é½çš„å¥–åŠ±å‡½æ•°ï¼Œä»è€Œæé«˜åˆ†å­æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†å¯è§£é‡Šæ€§ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹åˆ†å­çš„ç†è§£ï¼Œå®ç°äº†æ›´å¥½çš„æ³›åŒ–ã€‚å®éªŒè¯æ˜ï¼ŒMolReasonerä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ ‡å¿—ç€ä»åŸºäºè®°å¿†çš„è¾“å‡ºåˆ°ç¨³å¥åŒ–å­¦æ¨ç†çš„é‡è¦è½¬å˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ†å­æ¨ç†æ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–é€šç”¨æç¤ºï¼Œç¼ºä¹é¢†åŸŸç‰¹å®šçš„åˆ†å­è¯­ä¹‰ã€‚</li>
<li>MolReasoneræ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ¡†æ¶ï¼Œæ—¨åœ¨ä½¿LLMsä»è®°å¿†è¿‡æ¸¡åˆ°åŒ–å­¦æ¨ç†ã€‚</li>
<li>Mol-SFTé€šè¿‡åˆæˆç”Ÿæˆå¹¶éªŒè¯åŒ–å­¦å‡†ç¡®çš„é“¾å¼æ€ç»´æ ·æœ¬ï¼Œåˆæ­¥æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Mol-RLåº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œè®¾è®¡ä¸“é—¨é’ˆå¯¹åŒ–å­¦ç»“æ„ä¸è¯­è¨€æè¿°å¯¹é½çš„å¥–åŠ±å‡½æ•°ã€‚</li>
<li>MolReasoneræé«˜äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œå¢å¼ºäº†åˆ†å­ç†è§£ï¼Œå®ç°äº†æ›´å¥½çš„æ³›åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02066">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-99338bd8653710d14f1e9a892064c74c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28430c1398fb021737f98840e7595064.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12a7fef5a418a8904616a2eba3ced53a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c53923e08ff9c335824b53efa66537c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-99820c0527ad9db4a165bcfd10948ba1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Mapillary-Vistas-Validation-for-Fine-Grained-Traffic-Signs-A-Benchmark-Revealing-Vision-Language-Model-Limitations"><a href="#Mapillary-Vistas-Validation-for-Fine-Grained-Traffic-Signs-A-Benchmark-Revealing-Vision-Language-Model-Limitations" class="headerlink" title="Mapillary Vistas Validation for Fine-Grained Traffic Signs: A Benchmark   Revealing Vision-Language Model Limitations"></a>Mapillary Vistas Validation for Fine-Grained Traffic Signs: A Benchmark   Revealing Vision-Language Model Limitations</h2><p><strong>Authors:Sparsh Garg, Abhishek Aich</strong></p>
<p>Obtaining high-quality fine-grained annotations for traffic signs is critical for accurate and safe decision-making in autonomous driving. Widely used datasets, such as Mapillary, often provide only coarse-grained labels - without distinguishing semantically important types such as stop signs or speed limit signs. To this end, we present a new validation set for traffic signs derived from the Mapillary dataset called Mapillary Vistas Validation for Traffic Signs (MVV), where we decompose composite traffic signs into granular, semantically meaningful categories. The dataset includes pixel-level instance masks and has been manually annotated by expert annotators to ensure label fidelity. Further, we benchmark several state-of-the-art VLMs against the self-supervised DINOv2 model on this dataset and show that DINOv2 consistently outperforms all VLM baselines-not only on traffic sign recognition, but also on heavily represented categories like vehicles and humans. Our analysis reveals significant limitations in current vision-language models for fine-grained visual understanding and establishes DINOv2 as a strong baseline for dense semantic matching in autonomous driving scenarios. This dataset and evaluation framework pave the way for more reliable, interpretable, and scalable perception systems.   Code and data are available at: <a target="_blank" rel="noopener" href="https://github.com/nec-labs-ma/relabeling">https://github.com/nec-labs-ma/relabeling</a> </p>
<blockquote>
<p>è·å–é«˜è´¨é‡çš„äº¤é€šæ ‡å¿—ç²¾ç»†ç²’åº¦æ³¨é‡Šå¯¹äºè‡ªåŠ¨é©¾é©¶ä¸­çš„å‡†ç¡®å’Œå®‰å…¨å†³ç­–è‡³å…³é‡è¦ã€‚å¹¿æ³›ä½¿ç”¨çš„æ•°æ®é›†ï¼Œå¦‚Mapillaryï¼Œé€šå¸¸åªæä¾›ç²—ç•¥çš„æ ‡ç­¾ï¼Œæ— æ³•åŒºåˆ†è¯­ä¹‰ä¸Šé‡è¦çš„ç±»å‹ï¼Œå¦‚åœè½¦æ ‡å¿—æˆ–é™é€Ÿæ ‡å¿—ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸€ä¸ªä»Mapillaryæ•°æ®é›†æ´¾ç”Ÿçš„æ–°çš„äº¤é€šæ ‡å¿—éªŒè¯é›†ï¼Œç§°ä¸ºMapillary Vistasäº¤é€šæ ‡å¿—éªŒè¯é›†ï¼ˆMVVï¼‰ï¼Œæˆ‘ä»¬å°†å¤åˆäº¤é€šæ ‡å¿—åˆ†è§£ä¸ºé¢—ç²’çŠ¶çš„ã€è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„ç±»åˆ«ã€‚è¯¥æ•°æ®é›†åŒ…æ‹¬åƒç´ çº§çš„å®ä¾‹æ©æ¨¡ï¼Œå¹¶ç”±ä¸“ä¸šæ³¨é‡Šè€…è¿›è¡Œæ‰‹åŠ¨æ³¨é‡Šï¼Œä»¥ç¡®ä¿æ ‡ç­¾çš„ä¿çœŸåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨è¯¥æ•°æ®é›†ä¸Šå¯¹æ¯”äº†å‡ ç§æœ€å…ˆè¿›çš„VLMä¸è‡ªç›‘ç£çš„DINOv2æ¨¡å‹ï¼Œå¹¶æ˜¾ç¤ºDINOv2åœ¨æ­¤æ•°æ®é›†ä¸Šä¸ä»…å§‹ç»ˆåœ¨äº¤é€šæ ‡å¿—è¯†åˆ«æ–¹é¢ä¼˜äºæ‰€æœ‰VLMåŸºå‡†æµ‹è¯•ï¼Œè€Œä¸”åœ¨è½¦è¾†å’Œäººç±»ç­‰é‡åº¦è¡¨ç¤ºç±»åˆ«æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç²¾ç»†ç²’åº¦è§†è§‰ç†è§£æ–¹é¢çš„é‡å¤§å±€é™æ€§ï¼Œå¹¶å°†DINOv2ç¡®ç«‹ä¸ºè‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­å¯†é›†è¯­ä¹‰åŒ¹é…çš„å¼ºå¤§åŸºå‡†æµ‹è¯•ã€‚è¯¥æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ä¸ºæ›´å¯é ã€å¯è§£é‡Šå’Œå¯æ‰©å±•çš„æ„ŸçŸ¥ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚æœ‰å…³ä»£ç å’Œæ•°æ®å¯ä»¥åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/nec-labs-ma/relabeling%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/nec-labs-ma/relabelingæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02047v1">PDF</a> Accepted to ICCV 2025 Workshop (4th DataCV Workshop and Challenge)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºMapillaryæ•°æ®é›†æ„å»ºçš„Mapillary Vistas Validation for Traffic Signsï¼ˆMVVï¼‰æ•°æ®é›†ï¼Œä¸ºäº¤é€šæ ‡å¿—çš„ç²¾ç»†ç²’åº¦æ ‡æ³¨æä¾›äº†é«˜è´¨é‡èµ„æºã€‚è¯¥æ•°æ®é›†å°†å¤åˆäº¤é€šæ ‡å¿—åˆ†è§£ä¸ºç²’çŠ¶ã€è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„ç±»åˆ«ï¼Œå¹¶åŒ…æ‹¬åƒç´ çº§çš„å®ä¾‹æ©è†œã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¯¹æ¯”äº†æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸è‡ªæˆ‘ç›‘ç£çš„DINOv2æ¨¡å‹åœ¨è¯¥æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œç»“æœæ˜¾ç¤ºDINOv2ä¸ä»…äº¤é€šæ ‡å¿—è¯†åˆ«è¡¨ç°å‡ºè‰²ï¼Œå¯¹è½¦è¾†å’Œè¡Œäººç­‰å¸¸è§ç±»åˆ«çš„è¯†åˆ«ä¹Ÿè¡¨ç°ä¼˜ç§€ã€‚æœ¬ç ”ç©¶æ­ç¤ºäº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç²¾ç»†ç²’åº¦è§†è§‰ç†è§£æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶ç¡®ç«‹äº†DINOv2åœ¨è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸­çš„å¯†é›†è¯­ä¹‰åŒ¹é…èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MVVæ•°æ®é›†åŸºäºMapillaryæ•°æ®é›†æ„å»ºï¼Œä¸“ä¸ºäº¤é€šæ ‡å¿—çš„ç²¾ç»†ç²’åº¦æ ‡æ³¨è®¾è®¡ã€‚</li>
<li>MVVæ•°æ®é›†å°†å¤åˆäº¤é€šæ ‡å¿—åˆ†è§£ä¸ºç²’çŠ¶ã€è¯­ä¹‰ä¸Šæ„ä¹‰æ˜ç¡®çš„ç±»åˆ«ã€‚</li>
<li>æ•°æ®é›†åŒ…å«åƒç´ çº§çš„å®ä¾‹æ©è†œï¼Œç¡®ä¿æ ‡æ³¨çš„å‡†ç¡®æ€§ã€‚</li>
<li>å¯¹æ¯”äº†å¤šç§å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨MVVæ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚</li>
<li>DINOv2æ¨¡å‹åœ¨äº¤é€šæ ‡å¿—è¯†åˆ«åŠå…¶ä»–å¸¸è§ç±»åˆ«è¯†åˆ«ä¸Šè¡¨ç°ä¼˜ç§€ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç²¾ç»†ç²’åº¦è§†è§‰ç†è§£æ–¹é¢çš„å±€é™æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02047">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a2b8aba7beb62e2d1e23d9fea842ba5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31fba22e30f421f4ec81559e26a0d108.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be012fdbdcedcc993ded54604daa3b06.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-936b60c79b57c2611561e38c7e2395e0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-da40f8b0b662a1217e0eadb273c26200.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4d1a8df333deec2a2f0b13b207864b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d29d18e180a65d01703693dcd4bb4594.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Bench2ADVLM-A-Closed-Loop-Benchmark-for-Vision-language-Models-in-Autonomous-Driving"><a href="#Bench2ADVLM-A-Closed-Loop-Benchmark-for-Vision-language-Models-in-Autonomous-Driving" class="headerlink" title="Bench2ADVLM: A Closed-Loop Benchmark for Vision-language Models in   Autonomous Driving"></a>Bench2ADVLM: A Closed-Loop Benchmark for Vision-language Models in   Autonomous Driving</h2><p><strong>Authors:Tianyuan Zhang, Ting Jin, Lu Wang, Jiangfan Liu, Siyuan Liang, Mingchuan Zhang, Aishan Liu, Xianglong Liu</strong></p>
<p>Vision-Language Models (VLMs) have recently emerged as a promising paradigm in autonomous driving (AD). However, current performance evaluation protocols for VLM-based AD systems (ADVLMs) are predominantly confined to open-loop settings with static inputs, neglecting the more realistic and informative closed-loop setting that captures interactive behavior, feedback resilience, and real-world safety. To address this, we introduce Bench2ADVLM, a unified hierarchical closed-loop evaluation framework for real-time, interactive assessment of ADVLMs across both simulation and physical platforms. Inspired by dual-process theories of cognition, we first adapt diverse ADVLMs to simulation environments via a dual-system adaptation architecture. In this design, heterogeneous high-level driving commands generated by target ADVLMs (fast system) are interpreted by a general-purpose VLM (slow system) into standardized mid-level control actions suitable for execution in simulation. To bridge the gap between simulation and reality, we design a physical control abstraction layer that translates these mid-level actions into low-level actuation signals, enabling, for the first time, closed-loop testing of ADVLMs on physical vehicles. To enable more comprehensive evaluation, Bench2ADVLM introduces a self-reflective scenario generation module that automatically explores model behavior and uncovers potential failure modes for safety-critical scenario generation. Overall, Bench2ADVLM establishes a hierarchical evaluation pipeline that seamlessly integrates high-level abstract reasoning, mid-level simulation actions, and low-level real-world execution. Experiments on diverse scenarios across multiple state-of-the-art ADVLMs and physical platforms validate the diagnostic strength of our framework, revealing that existing ADVLMs still exhibit limited performance under closed-loop conditions. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æœ€è¿‘ä½œä¸ºè‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰é¢†åŸŸçš„ä¸€ç§æœ‰å‰é€”çš„èŒƒå¼è€Œå‡ºç°ã€‚ç„¶è€Œï¼Œå½“å‰åŸºäºVLMçš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼ˆADVLMï¼‰çš„æ€§èƒ½è¯„ä¼°åè®®ä¸»è¦å±€é™äºå¼€æ”¾å¾ªç¯ç¯å¢ƒï¼Œå…·æœ‰é™æ€è¾“å…¥ï¼Œå¿½ç•¥äº†æ›´çœŸå®ã€æ›´å…·ä¿¡æ¯é‡çš„é—­ç¯è®¾ç½®ï¼Œè¯¥è®¾ç½®å¯ä»¥æ•æ‰äº¤äº’è¡Œä¸ºã€åé¦ˆå¼¹æ€§å’ŒçœŸå®ä¸–ç•Œå®‰å…¨æ€§ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Bench2ADVLMï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åˆ†å±‚é—­ç¯è¯„ä¼°æ¡†æ¶ï¼Œå¯ç”¨äºåœ¨ä»¿çœŸå’Œç‰©ç†å¹³å°ä¸Šå¯¹ADVLMè¿›è¡Œå®æ—¶ã€äº¤äº’å¼çš„è¯„ä¼°ã€‚å—åˆ°è®¤çŸ¥åŒè¿‡ç¨‹ç†è®ºçš„å¯å‘ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡åŒç³»ç»Ÿé€‚åº”æ¶æ„å°†å„ç§ADVLMé€‚åº”åˆ°ä»¿çœŸç¯å¢ƒã€‚åœ¨æ­¤è®¾è®¡ä¸­ï¼Œç”±ç›®æ ‡ADVLMï¼ˆå¿«é€Ÿç³»ç»Ÿï¼‰ç”Ÿæˆçš„ä¸åŒé«˜çº§é©¾é©¶å‘½ä»¤è¢«é€šç”¨VLMï¼ˆæ…¢é€Ÿç³»ç»Ÿï¼‰è§£é‡Šä¸ºæ ‡å‡†åŒ–çš„ä¸­çº§æ§åˆ¶åŠ¨ä½œï¼Œé€‚ç”¨äºä»¿çœŸä¸­çš„æ‰§è¡Œã€‚ä¸ºäº†å¼¥ä»¿çœŸä¸çœŸå®ä¹‹é—´çš„å·®è·ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç‰©ç†æ§åˆ¶æŠ½è±¡å±‚ï¼Œå°†è¿™äº›ä¸­çº§åŠ¨ä½œè½¬åŒ–ä¸ºä½çº§æ‰§è¡Œä¿¡å·ï¼Œä»è€Œé¦–æ¬¡å®ç°å¯¹ç‰©ç†è½¦è¾†ä¸ŠADVLMçš„é—­ç¯æµ‹è¯•ã€‚ä¸ºäº†è¿›è¡Œæ›´å…¨é¢çš„è¯„ä¼°ï¼ŒBench2ADVLMå¼•å…¥äº†ä¸€ä¸ªè‡ªæˆ‘åæ€çš„åœºæ™¯ç”Ÿæˆæ¨¡å—ï¼Œè¯¥æ¨¡å—è‡ªåŠ¨æ¢ç´¢æ¨¡å‹è¡Œä¸ºå¹¶æ­ç¤ºæ½œåœ¨æ•…éšœæ¨¡å¼ï¼Œç”¨äºç”Ÿæˆå…³é”®å®‰å…¨åœºæ™¯ã€‚æ€»çš„æ¥è¯´ï¼ŒBench2ADVLMå»ºç«‹äº†ä¸€ä¸ªåˆ†å±‚çš„è¯„ä¼°æµç¨‹ï¼Œæ— ç¼é›†æˆäº†é«˜çº§æŠ½è±¡æ¨ç†ã€ä¸­çº§ä»¿çœŸåŠ¨ä½œå’Œä½çº§çœŸå®ä¸–ç•Œæ‰§è¡Œã€‚åœ¨å¤šä¸ªå…ˆè¿›çš„ADVLMå’Œç‰©ç†å¹³å°ä¸Šçš„å„ç§åœºæ™¯çš„å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶çš„è¯Šæ–­èƒ½åŠ›ï¼Œè¡¨æ˜ç°æœ‰çš„ADVLMåœ¨é—­ç¯æ¡ä»¶ä¸‹ä»è¡¨ç°å‡ºæœ‰é™æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02028v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰é¢†åŸŸä¸­æ–°å…´çš„æœ‰å‰é€”çš„èŒƒå¼â€”â€”è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚ç„¶è€Œï¼Œå½“å‰é’ˆå¯¹åŸºäºVLMçš„ADç³»ç»Ÿï¼ˆADVLMsï¼‰çš„æ€§èƒ½è¯„ä¼°åè®®ä¸»è¦å±€é™äºå¼€æ”¾å¾ªç¯è®¾ç½®å’Œé™æ€è¾“å…¥ï¼Œå¿½ç•¥äº†æ›´ç°å®å’Œæ›´å…·ä¿¡æ¯å«é‡çš„é—­ç¯è®¾ç½®ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Bench2ADVLMï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åˆ†å±‚é—­ç¯è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºå®æ—¶ã€äº¤äº’å¼åœ°è¯„ä¼°ADVLMsåœ¨æ¨¡æ‹Ÿå’Œç‰©ç†å¹³å°ä¸Šçš„è¡¨ç°ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†è‡ªæˆ‘åæ€çš„åœºæ™¯ç”Ÿæˆæ¨¡å—ï¼Œå¹¶å»ºç«‹äº†åˆ†å±‚è¯„ä¼°ç®¡é“ï¼Œæ— ç¼é›†æˆäº†é«˜çº§æŠ½è±¡æ¨ç†ã€ä¸­çº§æ¨¡æ‹ŸåŠ¨ä½œå’Œä½çº§ç°å®ä¸–ç•Œæ‰§è¡Œã€‚å®éªŒè¡¨æ˜ï¼Œç°æœ‰ADVLMsåœ¨é—­ç¯æ¡ä»¶ä¸‹ä»è¡¨ç°æœ‰é™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰é¢†åŸŸä¸­å…·æœ‰å‰æ™¯ã€‚</li>
<li>å½“å‰ADVLMsçš„æ€§èƒ½è¯„ä¼°ä¸»è¦å±€é™äºå¼€æ”¾å¾ªç¯è®¾ç½®ï¼Œç¼ºä¹ç°å®æ€§å’Œäº’åŠ¨æ€§ã€‚</li>
<li>Bench2ADVLMæ˜¯ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºåœ¨æ¨¡æ‹Ÿå’Œç‰©ç†å¹³å°ä¸Šè¿›è¡Œå®æ—¶ã€äº¤äº’å¼è¯„ä¼°ADVLMsã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨åˆ†å±‚è®¾è®¡ï¼ŒåŒ…æ‹¬é«˜çº§æŠ½è±¡æ¨ç†ã€ä¸­çº§æ¨¡æ‹ŸåŠ¨ä½œå’Œä½çº§ç°å®ä¸–ç•Œæ‰§è¡Œã€‚</li>
<li>æ¡†æ¶ä¸­çš„è‡ªæˆ‘åæ€åœºæ™¯ç”Ÿæˆæ¨¡å—å¯è‡ªåŠ¨æ¢ç´¢æ¨¡å‹è¡Œä¸ºå¹¶æ­ç¤ºæ½œåœ¨å¤±è´¥æ¨¡å¼ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰ADVLMsåœ¨é—­ç¯æ¡ä»¶ä¸‹çš„æ€§èƒ½è¡¨ç°æœ‰é™ã€‚</li>
<li>Bench2ADVLMçš„æ¨å‡ºå¡«è¡¥äº†ADVLMsåœ¨é—­ç¯è¯„ä¼°æ–¹é¢çš„ç©ºç™½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02028">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-12365ace7c8fe141eadab9eaa830c37f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea3dfef142fa2728c1fcd29cc68ebe9d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-649c0b0613857b0168b9195a0689228c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5396d5c4a641dc785b80a9e9eb9f5edf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b607d9ab38243530bd8bc93716148409.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SpeechR-A-Benchmark-for-Speech-Reasoning-in-Large-Audio-Language-Models"><a href="#SpeechR-A-Benchmark-for-Speech-Reasoning-in-Large-Audio-Language-Models" class="headerlink" title="SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models"></a>SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models</h2><p><strong>Authors:Wanqi Yang, Yanda Li, Yunchao Wei, Meng Fang, Ling Chen</strong></p>
<p>Large audio-language models (LALMs) have achieved near-human performance in sentence-level transcription and emotion recognition. However, existing evaluations focus mainly on surface-level perception, leaving the capacity of models for contextual and inference-driven reasoning in speech-based scenarios insufficiently examined. To address this gap, we introduce SpeechR, a unified benchmark for evaluating reasoning over speech in large audio-language models. SpeechR evaluates models along three key dimensions: factual retrieval, procedural inference, and normative judgment. It includes three distinct evaluation formats. The multiple-choice version measures answer selection accuracy. The generative version assesses the coherence and logical consistency of reasoning chains. The acoustic-feature version investigates whether variations in stress and emotion affect reasoning performance. Evaluations on eleven state-of-the-art LALMs reveal that high transcription accuracy does not translate into strong reasoning capabilities. SpeechR establishes a structured benchmark for evaluating reasoning in spoken language, enabling more targeted analysis of model capabilities across diverse dialogue-based tasks. </p>
<blockquote>
<p>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰åœ¨å¥å­çº§è½¬å½•å’Œæƒ…æ„Ÿè¯†åˆ«æ–¹é¢å·²æ¥è¿‘äººç±»æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨è¡¨é¢å±‚æ¬¡çš„æ„ŸçŸ¥ï¼Œå¯¹æ¨¡å‹åœ¨åŸºäºè¯­éŸ³åœºæ™¯ä¸­çš„ä¸Šä¸‹æ–‡å’Œæ¨ç†é©±åŠ¨çš„æ¨ç†èƒ½åŠ›å°šæœªè¿›è¡Œå……åˆ†ç ”ç©¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpeechRï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ä¸­è¯­éŸ³æ¨ç†çš„ç»Ÿä¸€åŸºå‡†æµ‹è¯•ã€‚SpeechRä»ä¸‰ä¸ªå…³é”®ç»´åº¦è¯„ä¼°æ¨¡å‹ï¼šäº‹å®æ£€ç´¢ã€ç¨‹åºæ¨ç†å’Œè§„èŒƒæ€§åˆ¤æ–­ã€‚å®ƒåŒ…æ‹¬ä¸‰ç§ä¸åŒçš„è¯„ä¼°æ ¼å¼ã€‚å¤šé¡¹é€‰æ‹©ç‰ˆæœ¬è¡¡é‡ç­”æ¡ˆé€‰æ‹©çš„å‡†ç¡®æ€§ã€‚ç”Ÿæˆç‰ˆæœ¬è¯„ä¼°æ¨ç†é“¾çš„è¿è´¯æ€§å’Œé€»è¾‘ä¸€è‡´æ€§ã€‚å£°å­¦ç‰¹å¾ç‰ˆæœ¬ç ”ç©¶è¯­éŸ³ä¸­çš„é‡éŸ³å’Œæƒ…æ„Ÿå˜åŒ–æ˜¯å¦ä¼šå½±å“æ¨ç†æ€§èƒ½ã€‚å¯¹åä¸€ç§æœ€æ–°çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„è¯„ä¼°è¡¨æ˜ï¼Œé«˜è½¬å½•å‡†ç¡®æ€§å¹¶ä¸ç­‰åŒäºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚SpeechRå»ºç«‹äº†ä¸€ä¸ªè¯„ä¼°å£è¯­æ¨ç†çš„ç»“æ„åŒ–åŸºå‡†æµ‹è¯•ï¼Œèƒ½å¤Ÿæ›´å…·ä½“åœ°åˆ†æä¸åŒå¯¹è¯ä»»åŠ¡ä¸­çš„æ¨¡å‹èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02018v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†ä¸ºè¯„ä¼°å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ä¸­è¯­éŸ³åŸºç¡€ä¸Šçš„æƒ…å¢ƒæ¨ç†å’Œæ¨ç†é©±åŠ¨èƒ½åŠ›è€Œæå‡ºçš„SpeechRåŸºå‡†æµ‹è¯•ã€‚SpeechRé€šè¿‡ä¸‰ä¸ªå…³é”®ç»´åº¦ï¼šäº‹å®æ£€ç´¢ã€ç¨‹åºæ¨ç†å’Œè§„èŒƒæ€§åˆ¤æ–­ï¼Œä»¥åŠä¸‰ç§ç‹¬ç‰¹çš„è¯„ä¼°æ ¼å¼æ¥å…¨é¢è¯„ä¼°æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œé«˜è½¬å½•å‡†ç¡®æ€§å¹¶ä¸ç­‰åŒäºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚SpeechRä¸ºè¯„ä¼°å£è¯­ä¸­çš„æ¨ç†èƒ½åŠ›å»ºç«‹äº†ç»“æ„åŒ–åŸºå‡†æµ‹è¯•ï¼Œä½¿é’ˆå¯¹ä¸åŒå¯¹è¯ä»»åŠ¡çš„æ¨¡å‹èƒ½åŠ›åˆ†ææ›´åŠ ç²¾å‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Large audio-language models (LALMs) å·²åœ¨å¥å­çº§è½¬å½•å’Œæƒ…æ„Ÿè¯†åˆ«æ–¹é¢è¾¾åˆ°æ¥è¿‘äººç±»çš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰è¯„ä¼°ä¸»è¦å…³æ³¨è¡¨é¢å±‚æ¬¡çš„æ„ŸçŸ¥ï¼Œå¯¹æ¨¡å‹åœ¨è¯­éŸ³åœºæ™¯ä¸­çš„æƒ…å¢ƒå’Œæ¨ç†é©±åŠ¨èƒ½åŠ›è€ƒå¯Ÿä¸è¶³ã€‚</li>
<li>ä¸ºè§£å†³æ­¤å·®è·ï¼Œæå‡ºäº†SpeechRåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°LALMåœ¨è¯­éŸ³ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>SpeechRåŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»´åº¦ï¼šäº‹å®æ£€ç´¢ã€ç¨‹åºæ¨ç†å’Œè§„èŒƒæ€§åˆ¤æ–­ã€‚</li>
<li>å®ƒæä¾›äº†ä¸‰ç§è¯„ä¼°æ ¼å¼ï¼šå¤šé¡¹é€‰æ‹©ã€ç”Ÿæˆè¯„ä¼°å’ŒåŸºäºå£°éŸ³çš„è¯„ä¼°æ ¼å¼ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œé«˜è½¬å½•å‡†ç¡®æ€§å¹¶ä¸ç­‰åŒäºæ¨¡å‹å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02018">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69abad54fe13ab2f6b0b805c1919d73e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd4b50e9cde15f6d28deef9dc6afd287.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-69832ef1f8333bdd3829d8881ed0b807.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9d32814cd4d6266a9617fcf178e6b54f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03126bd423403e2a5933e7a29fe4b0a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df3471a682740dc6a2433704eb9050d7.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="IMoRe-Implicit-Program-Guided-Reasoning-for-Human-Motion-Q-A"><a href="#IMoRe-Implicit-Program-Guided-Reasoning-for-Human-Motion-Q-A" class="headerlink" title="IMoRe: Implicit Program-Guided Reasoning for Human Motion Q&amp;A"></a>IMoRe: Implicit Program-Guided Reasoning for Human Motion Q&amp;A</h2><p><strong>Authors:Chen Li, Chinthani Sugandhika, Yeo Keat Ee, Eric Peh, Hao Zhang, Hong Yang, Deepu Rajan, Basura Fernando</strong></p>
<p>Existing human motion Q&amp;A methods rely on explicit program execution, where the requirement for manually defined functional modules may limit the scalability and adaptability. To overcome this, we propose an implicit program-guided motion reasoning (IMoRe) framework that unifies reasoning across multiple query types without manually designed modules. Unlike existing implicit reasoning approaches that infer reasoning operations from question words, our model directly conditions on structured program functions, ensuring a more precise execution of reasoning steps. Additionally, we introduce a program-guided reading mechanism, which dynamically selects multi-level motion representations from a pretrained motion Vision Transformer (ViT), capturing both high-level semantics and fine-grained motion cues. The reasoning module iteratively refines memory representations, leveraging structured program functions to extract relevant information for different query types. Our model achieves state-of-the-art performance on Babel-QA and generalizes to a newly constructed motion Q&amp;A dataset based on HuMMan, demonstrating its adaptability across different motion reasoning datasets. Code and dataset are available at: <a target="_blank" rel="noopener" href="https://github.com/LUNAProject22/IMoRe">https://github.com/LUNAProject22/IMoRe</a>. </p>
<blockquote>
<p>ç°æœ‰çš„äººç±»è¿åŠ¨é—®ç­”æ–¹æ³•ä¾èµ–äºæ˜ç¡®çš„ç¨‹åºæ‰§è¡Œï¼Œå…¶ä¸­å¯¹æ‰‹åŠ¨å®šä¹‰åŠŸèƒ½æ¨¡å—çš„éœ€æ±‚å¯èƒ½ä¼šé™åˆ¶å…¶å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç¼ºé™·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§éšå¼ç¨‹åºå¼•å¯¼çš„è¿åŠ¨æ¨ç†ï¼ˆIMoReï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç»Ÿä¸€å¤šç§æŸ¥è¯¢ç±»å‹çš„æ¨ç†ï¼Œè€Œæ— éœ€æ‰‹åŠ¨è®¾è®¡æ¨¡å—ã€‚ä¸åŒäºç°æœ‰ä»é—®é¢˜è¯ä¸­æ¨æ–­æ¨ç†æ“ä½œçš„éšå¼æ¨ç†æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ç›´æ¥ä»¥ç»“æ„åŒ–çš„ç¨‹åºåŠŸèƒ½ä¸ºæ¡ä»¶ï¼Œç¡®ä¿æ¨ç†æ­¥éª¤çš„æ›´ç²¾ç¡®æ‰§è¡Œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç¨‹åºå¼•å¯¼çš„é˜…è¯»æœºåˆ¶ï¼Œè¯¥æœºåˆ¶ä»é¢„è®­ç»ƒçš„è¿åŠ¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ä¸­åŠ¨æ€é€‰æ‹©å¤šçº§è¿åŠ¨è¡¨ç¤ºï¼Œæ•æ‰é«˜çº§è¯­ä¹‰å’Œç²¾ç»†çš„è¿åŠ¨çº¿ç´¢ã€‚æ¨ç†æ¨¡å—é€šè¿‡è¿­ä»£ä¼˜åŒ–å†…å­˜è¡¨ç¤ºï¼Œåˆ©ç”¨ç»“æ„åŒ–çš„ç¨‹åºåŠŸèƒ½æå–ä¸åŒç±»å‹æŸ¥è¯¢çš„ç›¸å…³ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨Babel-QAä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œå¹¶æ¨å¹¿åˆ°äº†åŸºäºHuMManæ–°æ„å»ºçš„è¿åŠ¨é—®ç­”æ•°æ®é›†ä¸Šï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒè¿åŠ¨æ¨ç†æ•°æ®é›†ä¸Šçš„é€‚åº”æ€§ã€‚ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/LUNAProject22/IMoRe%E3%80%82">https://github.com/LUNAProject22/IMoReã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01984v1">PDF</a> *Equal contribution. Accepted by the International Conference on   Computer Vision (ICCV 2025)</p>
<p><strong>Summary</strong><br>åŸºäºç°æœ‰çš„äººç±»è¿åŠ¨é—®ç­”æ–¹æ³•ä¾èµ–äºæ˜¾å¼ç¨‹åºæ‰§è¡Œï¼Œå­˜åœ¨å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§å—é™çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§éšå¼ç¨‹åºå¼•å¯¼çš„è¿åŠ¨æ¨ç†ï¼ˆIMoReï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ— éœ€æ‰‹åŠ¨è®¾è®¡æ¨¡å—ï¼Œå°±èƒ½ç»Ÿä¸€å¤šç§æŸ¥è¯¢ç±»å‹çš„æ¨ç†è¿‡ç¨‹ã€‚ä¸å…¶ä»–éšå¼æ¨ç†æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹ç›´æ¥ä¾èµ–äºç»“æ„åŒ–çš„ç¨‹åºåŠŸèƒ½ï¼Œç¡®ä¿æ¨ç†æ­¥éª¤æ›´ç²¾ç¡®çš„æ‰§è¡Œã€‚åŒæ—¶å¼•å…¥ç¨‹åºå¼•å¯¼çš„é˜…è¯»æœºåˆ¶ï¼ŒåŠ¨æ€åœ°ä»é¢„è®­ç»ƒçš„è¿åŠ¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ä¸­é€‰æ‹©å¤šçº§è¿åŠ¨è¡¨ç¤ºï¼Œæ•æ‰é«˜çº§è¯­ä¹‰å’Œç²¾ç»†çš„è¿åŠ¨çº¿ç´¢ã€‚è¯¥æ¨¡å‹åœ¨Babel-QAä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨åŸºäºHuMManæ„å»ºçš„æ–°è¿åŠ¨é—®ç­”æ•°æ®é›†ä¸Šè¿›è¡Œäº†æ¨å¹¿ï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒè¿åŠ¨æ¨ç†æ•°æ®é›†ä¸Šçš„é€‚åº”æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰çš„äººç±»è¿åŠ¨é—®ç­”æ–¹æ³•ä¸»è¦ä¾èµ–äºæ˜¾å¼ç¨‹åºæ‰§è¡Œï¼Œå­˜åœ¨å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§é™åˆ¶ã€‚</li>
<li>æå‡ºçš„éšå¼ç¨‹åºå¼•å¯¼çš„è¿åŠ¨æ¨ç†ï¼ˆIMoReï¼‰æ¡†æ¶èƒ½ç»Ÿä¸€å¤šç§æŸ¥è¯¢ç±»å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œæ— éœ€æ‰‹åŠ¨è®¾è®¡æ¨¡å—ã€‚</li>
<li>ä¸å…¶ä»–éšå¼æ¨ç†æ–¹æ³•ä¸åŒï¼ŒIMoReæ¨¡å‹ç›´æ¥ä¾èµ–äºç»“æ„åŒ–çš„ç¨‹åºåŠŸèƒ½ï¼Œç¡®ä¿æ¨ç†æ­¥éª¤æ›´ç²¾ç¡®çš„æ‰§è¡Œã€‚</li>
<li>IMoReæ¡†æ¶å¼•å…¥äº†ç¨‹åºå¼•å¯¼çš„é˜…è¯»æœºåˆ¶ï¼Œèƒ½å¤Ÿä»é¢„è®­ç»ƒçš„è¿åŠ¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ä¸­é€‰æ‹©å¤šçº§è¿åŠ¨è¡¨ç¤ºã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿæ•æ‰é«˜çº§è¯­ä¹‰å’Œç²¾ç»†çš„è¿åŠ¨çº¿ç´¢ã€‚</li>
<li>IMoReæ¡†æ¶åœ¨Babel-QAä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</li>
<li>IMoReæ¡†æ¶åœ¨åŸºäºHuMManæ„å»ºçš„æ–°è¿åŠ¨é—®ç­”æ•°æ®é›†ä¸Šå…·æœ‰è‰¯å¥½çš„æ¨å¹¿æ€§ï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒè¿åŠ¨æ¨ç†æ•°æ®é›†ä¸Šçš„é€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01984">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dbf2ef2886008878e9232cab2e298bb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a83b8e6a13af1972ce8d970f78f93386.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="TIBSTC-CoT-A-Multi-Domain-Instruction-Dataset-for-Chain-of-Thought-Reasoning-in-Language-Models"><a href="#TIBSTC-CoT-A-Multi-Domain-Instruction-Dataset-for-Chain-of-Thought-Reasoning-in-Language-Models" class="headerlink" title="TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought   Reasoning in Language Models"></a>TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought   Reasoning in Language Models</h2><p><strong>Authors:Fan Gao, Cheng Huang, Nyima Tashi, Yutong Liu, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, Renzeg Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Xiao Feng, Hao Wang, Yongbin Yu</strong></p>
<p>To address the severe data scarcity in Tibetan, a low-resource language spoken by over six million people, we introduce TIBSTC-CoT, the large-scale, multi-domain Tibetan dataset automatically constructed via chain-of-thought prompting with large language models (LLMs). TIBSTC-CoT establishes a scalable and reproducible framework for dataset creation in low-resource settings, covering diverse domains and reasoning patterns essential for language understanding and generation. Building on this dataset, we develop the Sunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with chain-of-thought capabilities. Trained entirely on TIBSTC-CoT, Sunshine-thinking has demonstrated strong reasoning and generation performance, comparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a significant step toward inclusive AI by enabling high-quality Tibetan language processing through both resource creation and model innovation. All data are available: <a target="_blank" rel="noopener" href="https://github.com/Vicentvankor/sun-shine">https://github.com/Vicentvankor/sun-shine</a>. </p>
<blockquote>
<p>é’ˆå¯¹è¶…è¿‡å…­ç™¾ä¸‡äººæ‰€ä½¿ç”¨çš„è—è¯­è¿™ç§èµ„æºåŒ®ä¹çš„è¯­è¨€ä¸­æ•°æ®æåº¦ç¨€ç¼ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†TIBSTC-CoTã€‚è¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šé¢†åŸŸçš„è—è¯­æ•°æ®é›†ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é“¾å¼æ€ç»´æç¤ºè‡ªåŠ¨æ„å»ºã€‚TIBSTC-CoTä¸ºèµ„æºåŒ®ä¹ç¯å¢ƒä¸‹çš„æ•°æ®é›†åˆ›å»ºå»ºç«‹äº†ä¸€ä¸ªå¯æ‰©å±•å’Œå¯å¤åˆ¶çš„æ¡†æ¶ï¼Œæ¶µç›–äº†å¯¹è¯­è¨€ç†è§£å’Œç”Ÿæˆè‡³å…³é‡è¦çš„ä¸åŒé¢†åŸŸå’Œæ€ç»´æ¨¡å¼ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç³»åˆ—ä»¥è—è¯­ä¸ºä¸­å¿ƒçš„LLMå®¶æ—â€”â€”Sunshine-thinkingã€‚è¿™ä¸€ç³»åˆ—æ¨¡å‹é…å¤‡äº†é“¾å¼æ€ç»´èƒ½åŠ›ï¼Œå®Œå…¨åœ¨TIBSTC-CoTä¸Šè¿›è¡Œè®­ç»ƒï¼Œå…¶æ¨ç†å’Œç”Ÿæˆæ€§èƒ½è¡¨ç°å‡ºè‰²ï¼Œä¸æœ€æ–°çš„å¤šè¯­ç§LLMç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡èµ„æºåˆ›å»ºå’Œæ¨¡å‹åˆ›æ–°å®ç°äº†é«˜è´¨é‡è—è¯­å¤„ç†ï¼Œæœç€åŒ…å®¹æ€§äººå·¥æ™ºèƒ½è¿ˆå‡ºäº†ä¸€å¤§æ­¥ã€‚æ‰€æœ‰æ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Vicentvankor/sun-shine%E5%A4%84%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Vicentvankor/sun-shineå¤„è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01977v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>TIBSTC-CoTæ˜¯ä¸“ä¸ºè§£å†³è—è¯­æ•°æ®ç¨€ç¼ºé—®é¢˜è€Œæ„å»ºçš„å¤§å‹å¤šé¢†åŸŸè—è¯­æ•°æ®é›†ï¼Œé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹çš„é“¾å¼æ€ç»´æç¤ºæŠ€æœ¯è‡ªåŠ¨åˆ›å»ºã€‚å®ƒå»ºç«‹äº†ä¸€ä¸ªåœ¨èµ„æºåŒ®ä¹ç¯å¢ƒä¸­åˆ›å»ºæ•°æ®é›†çš„å¯æ‰©å±•å’Œå¯å¤åˆ¶æ¡†æ¶ï¼Œæ¶µç›–å¯¹è¯­è¨€ç†è§£å’Œç”Ÿæˆè‡³å…³é‡è¦çš„å¤šç§é¢†åŸŸå’Œæ€ç»´æ¨¡å¼ã€‚åŸºäºTIBSTC-CoTæ•°æ®é›†å¼€å‘çš„Sunshine-thinkingç³»åˆ—è—è¯­ä¸­å¿ƒåŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°äº†å¼ºå¤§çš„æ¨ç†å’Œç”Ÿæˆæ€§èƒ½ï¼Œä¸å…ˆè¿›çš„å¤šè¯­ç§å¤§å‹è¯­è¨€æ¨¡å‹ç›¸å½“ã€‚æ­¤å·¥ä½œé€šè¿‡èµ„æºåˆ›å»ºå’Œæ¨¡å‹åˆ›æ–°å®ç°äº†é«˜è´¨é‡çš„è—è¯­å¤„ç†ï¼Œæœç€åŒ…å®¹æ€§äººå·¥æ™ºèƒ½è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TIBSTC-CoTæ•°æ®é›†ä¸“ä¸ºè§£å†³è—è¯­æ•°æ®ç¨€ç¼ºé—®é¢˜è€Œè®¾è®¡ï¼Œæ˜¯ä¸€ä¸ªå¤§å‹ã€å¤šé¢†åŸŸçš„è—è¯­æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹çš„é“¾å¼æ€ç»´æç¤ºæŠ€æœ¯è‡ªåŠ¨åˆ›å»ºTIBSTC-CoTæ•°æ®é›†ï¼Œå®ç°å¯æ‰©å±•å’Œå¯å¤åˆ¶çš„æ•°æ®é›†åˆ›å»ºæ¡†æ¶ã€‚</li>
<li>Sunshine-thinkingç³»åˆ—è—è¯­ä¸­å¿ƒåŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹æ€§èƒ½ä¸å…ˆè¿›çš„å¤šè¯­ç§å¤§å‹è¯­è¨€æ¨¡å‹ç›¸å½“ã€‚</li>
<li>æ­¤å·¥ä½œä¸ä»…åœ¨èµ„æºåˆ›å»ºæ–¹é¢å–å¾—è¿›å±•ï¼Œè¿˜åœ¨æ¨¡å‹åˆ›æ–°æ–¹é¢å®ç°äº†çªç ´ã€‚</li>
<li>TIBSTC-CoTæ•°æ®é›†å’Œå¤šé¢†åŸŸå¤§å‹è¯­è¨€æ¨¡å‹çš„ç»“åˆæœ‰åŠ©äºæ¨åŠ¨åŒ…å®¹æ€§äººå·¥æ™ºèƒ½çš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01977">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8f5fb40a4b911331ee140d9241b5b8d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0d639f5feb4243d5d303a5b8e5c23de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5311605453523e1978b771096184b92.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f67634563159247e274cf3eabc73b9a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76f86343bd1c0d97b0e224d032704c3f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f76810ae743c6ff2fe5a462f45b3634.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dae7429f55c53fa0770d889c96c1d3a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-faab9ba5700e1bb672c89ce94a69a286.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Stochastic-Encodings-for-Active-Feature-Acquisition"><a href="#Stochastic-Encodings-for-Active-Feature-Acquisition" class="headerlink" title="Stochastic Encodings for Active Feature Acquisition"></a>Stochastic Encodings for Active Feature Acquisition</h2><p><strong>Authors:Alexander Norcliffe, Changhee Lee, Fergus Imrie, Mihaela van der Schaar, Pietro Lio</strong></p>
<p>Active Feature Acquisition is an instance-wise, sequential decision making problem. The aim is to dynamically select which feature to measure based on current observations, independently for each test instance. Common approaches either use Reinforcement Learning, which experiences training difficulties, or greedily maximize the conditional mutual information of the label and unobserved features, which makes myopic acquisitions. To address these shortcomings, we introduce a latent variable model, trained in a supervised manner. Acquisitions are made by reasoning about the features across many possible unobserved realizations in a stochastic latent space. Extensive evaluation on a large range of synthetic and real datasets demonstrates that our approach reliably outperforms a diverse set of baselines. </p>
<blockquote>
<p>ä¸»åŠ¨ç‰¹å¾è·å–æ˜¯ä¸€ä¸ªåŸºäºå®ä¾‹çš„ã€é¡ºåºå†³ç­–é—®é¢˜ã€‚å…¶ç›®æ ‡æ˜¯åŸºäºå½“å‰è§‚å¯ŸåŠ¨æ€é€‰æ‹©è¦æµ‹é‡çš„ç‰¹å¾ï¼Œé’ˆå¯¹æ¯ä¸ªæµ‹è¯•å®ä¾‹ç‹¬ç«‹è¿›è¡Œã€‚å¸¸è§çš„æ–¹æ³•è¦ä¹ˆä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œè¿™ä¼šå¯¼è‡´è®­ç»ƒå›°éš¾ï¼Œè¦ä¹ˆè´ªå©ªåœ°æœ€å¤§åŒ–æ ‡ç­¾å’Œæœªè§‚å¯Ÿç‰¹å¾çš„æ¡ä»¶äº’ä¿¡æ¯ï¼Œä»è€Œå¯¼è‡´çŸ­è§†è·å–ã€‚ä¸ºäº†è§£å†³è¿™äº›ç¼ºç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ½œåœ¨å˜é‡æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨ç›‘ç£æ–¹å¼è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡åœ¨ä¸€ä¸ªéšæœºæ½œåœ¨ç©ºé—´ä¸­æ¨ç†å¤šä¸ªå¯èƒ½çš„æœªè§‚å¯Ÿå®ç°çš„ç‰¹å¾æ¥è¿›è¡Œé‡‡é›†ã€‚åœ¨å¤§é‡åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯é åœ°ä¼˜äºä¸€ç»„å¤šæ ·åŒ–çš„åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01957v1">PDF</a> 31 pages, 15 figures, 17 tables, published at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸»åŠ¨ç‰¹å¾è·å–ï¼ˆActive Feature Acquisitionï¼‰ä½œä¸ºä¸€ç§å®ä¾‹çº§çš„åºè´¯å†³ç­–é—®é¢˜ã€‚ç›®æ ‡æ˜¯åŸºäºå½“å‰è§‚æµ‹åŠ¨æ€é€‰æ‹©å¯¹æ¯ä¸ªæµ‹è¯•å®ä¾‹è¿›è¡Œå“ªäº›ç‰¹å¾çš„æµ‹é‡ã€‚å¸¸è§æ–¹æ³•å­˜åœ¨ä¸è¶³ï¼Œå¦‚ä½¿ç”¨å¼ºåŒ–å­¦ä¹ é¢ä¸´è®­ç»ƒå›°éš¾ï¼Œæˆ–è¿‡åˆ†è¿½æ±‚æœ€å¤§åŒ–æ ‡ç­¾ä¸æœªè§‚æµ‹ç‰¹å¾çš„æ¡ä»¶äº’ä¿¡æ¯è€Œå¯¼è‡´çŸ­è§†è·å–ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§åŸºäºç›‘ç£å­¦ä¹ çš„æ½œåœ¨å˜é‡æ¨¡å‹ã€‚é€šè¿‡åœ¨ä¸€ä¸ªéšæœºæ½œåœ¨ç©ºé—´ä¸­å¯¹å¤šä¸ªå¯èƒ½çš„æœªè§‚æµ‹å®ç°è¿›è¡Œç‰¹å¾æ¨ç†æ¥è¿›è¡Œè·å–ã€‚åœ¨å¤šç§åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯é åœ°ä¼˜äºä¸€ç³»åˆ—åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸»åŠ¨ç‰¹å¾è·å–æ˜¯ä¸€ç§åŸºäºå®ä¾‹çš„åºè´¯å†³ç­–è¿‡ç¨‹ï¼Œç›®æ ‡æ˜¯åŠ¨æ€é€‰æ‹©å¯¹æ¯ä¸ªæµ‹è¯•å®ä¾‹åº”æµ‹é‡çš„ç‰¹å¾ã€‚</li>
<li>å¸¸è§æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å’Œæœ€å¤§åŒ–æ ‡ç­¾ä¸æœªè§‚æµ‹ç‰¹å¾çš„æ¡ä»¶äº’ä¿¡æ¯ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ æ–¹æ³•é¢ä¸´è®­ç»ƒå›°éš¾ï¼Œè€ŒåŸºäºæ¡ä»¶äº’ä¿¡æ¯çš„æ–¹æ³•å¯èƒ½å¯¼è‡´çŸ­è§†è·å–ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ç§åŸºäºç›‘ç£å­¦ä¹ çš„æ½œåœ¨å˜é‡æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡åœ¨ä¸€ä¸ªéšæœºæ½œåœ¨ç©ºé—´ä¸­å¯¹å¤šä¸ªå¯èƒ½çš„æœªè§‚æµ‹å®ç°è¿›è¡Œç‰¹å¾æ¨ç†æ¥è¿›è¡Œè·å–å†³ç­–ã€‚</li>
<li>å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4d3d26cfa344bccc6659381bc2ff5839.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98f6500545ace2bd366d22b31ad66e9b.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-06/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-35e7d6f740610ead318746206df0c675.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  Raw Data Matters Enhancing Prompt Tuning by Internal Augmentation on   Vision-Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-06/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-d4797d8c9b9b6c9e205b28907d6e1b11.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-06  Text2Lip Progressive Lip-Synced Talking Face Generation from Text via   Viseme-Guided Rendering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26548.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
