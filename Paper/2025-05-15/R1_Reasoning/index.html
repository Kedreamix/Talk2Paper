<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-15  AC-Reason Towards Theory-Guided Actual Causality Reasoning with Large   Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0e8d199658f9832bcaa9874e192322f2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    90 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-15-æ›´æ–°"><a href="#2025-05-15-æ›´æ–°" class="headerlink" title="2025-05-15 æ›´æ–°"></a>2025-05-15 æ›´æ–°</h1><h2 id="AC-Reason-Towards-Theory-Guided-Actual-Causality-Reasoning-with-Large-Language-Models"><a href="#AC-Reason-Towards-Theory-Guided-Actual-Causality-Reasoning-with-Large-Language-Models" class="headerlink" title="AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large   Language Models"></a>AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large   Language Models</h2><p><strong>Authors:Yanxi Zhang, Xin Cong, Zhong Zhang, Xiao Liu, Dongyan Zhao, Yesai Wu</strong></p>
<p>Actual causality (AC), a fundamental aspect of causal reasoning (CR), is responsible for attribution and responsibility assignment in real-world scenarios. However, existing LLM-based methods lack grounding in formal AC theory, resulting in limited interpretability. Therefore, we propose AC-Reason, a semi-formal reasoning framework that identifies causally relevant events within an AC scenario, infers the values of their formal causal factors (e.g., sufficiency, necessity, and normality), and answers AC queries via a theory-guided algorithm with explanations. While AC-Reason does not explicitly construct a causal graph, it operates over variables in the underlying causal structure to support principled reasoning. To enable comprehensive evaluation, we introduce AC-Bench, a new benchmark built upon and substantially extending Big-Bench Hard Causal Judgment (BBH-CJ). AC-Bench comprises ~1K carefully annotated samples, each with detailed reasoning steps and focuses solely on actual causation. The case study shows that synthesized samples in AC-Bench present greater challenges for LLMs. Extensive experiments on BBH-CJ and AC-Bench show that AC-Reason consistently improves LLM performance over baselines. On BBH-CJ, all tested LLMs surpass the average human rater accuracy of 69.60%, with GPT-4 + AC-Reason achieving 75.04%. On AC-Bench, GPT-4 + AC-Reason again achieves the highest accuracy of 71.82%. AC-Bench further enables fine-grained analysis of reasoning faithfulness, revealing that only Qwen-2.5-72B-Instruct, Claude-3.5-Sonnet, and GPT-4o exhibit faithful reasoning, whereas GPT-4 tends to exploit shortcuts. Finally, our ablation study proves that integrating AC theory into LLMs is highly effective, with the proposed algorithm contributing the most significant performance gains. </p>
<blockquote>
<p>å®é™…å› æœå…³ç³»ï¼ˆACï¼‰æ˜¯å› æœæ¨ç†ï¼ˆCRï¼‰çš„ä¸€ä¸ªåŸºæœ¬æ–¹é¢ï¼Œè´Ÿè´£ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„å½’å±å’Œè´£ä»»åˆ†é…ã€‚ç„¶è€Œï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç°æœ‰æ–¹æ³•ç¼ºä¹æ­£å¼çš„ACç†è®ºåŸºç¡€ï¼Œå¯¼è‡´è§£é‡Šæ€§æœ‰é™ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†AC-Reasonï¼Œè¿™æ˜¯ä¸€ä¸ªåŠæ­£å¼çš„æ¨ç†æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿåœ¨ACåœºæ™¯ä¸­è¯†åˆ«å› æœç›¸å…³çš„äº‹ä»¶ï¼Œæ¨æ–­å®ƒä»¬çš„å½¢å¼å› æœå› ç´ ï¼ˆä¾‹å¦‚ï¼Œå……åˆ†æ€§ã€å¿…è¦æ€§å’Œæ­£å¸¸æ€§ï¼‰çš„å€¼ï¼Œå¹¶é€šè¿‡å¸¦æœ‰è§£é‡Šçš„ç†è®ºæŒ‡å¯¼ç®—æ³•å›ç­”ACæŸ¥è¯¢ã€‚è™½ç„¶AC-Reasonæ²¡æœ‰æ˜ç¡®æ„å»ºå› æœå›¾ï¼Œä½†å®ƒæ“ä½œåº•å±‚å› æœç»“æ„ä¸­çš„å˜é‡ä»¥æ”¯æŒæœ‰åŸåˆ™çš„æ¨ç†ã€‚ä¸ºäº†è¿›è¡Œå…¨é¢çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å¼•å…¥äº†AC-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºBig-Bench Hard Causal Judgmentï¼ˆBBH-CJï¼‰å¹¶å¯¹å…¶è¿›è¡Œå®è´¨æ€§æ‰©å±•çš„æ–°åŸºå‡†æµ‹è¯•ã€‚AC-BenchåŒ…å«çº¦1000ä¸ªç²¾å¿ƒæ³¨é‡Šçš„æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½æœ‰è¯¦ç»†çš„æ¨ç†æ­¥éª¤ï¼Œå¹¶ä¸”åªä¸“æ³¨äºå®é™…å› æœå…³ç³»ã€‚æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜ï¼ŒAC-Benchä¸­çš„åˆæˆæ ·æœ¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ„æˆäº†æ›´å¤§çš„æŒ‘æˆ˜ã€‚åœ¨BBH-CJå’ŒAC-Benchä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒAC-ReasonæŒç»­æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ç›¸å¯¹äºåŸºçº¿ã€‚åœ¨BBH-CJä¸Šï¼Œæ‰€æœ‰æµ‹è¯•çš„å¤§å‹è¯­è¨€æ¨¡å‹éƒ½è¶…è¿‡äº†äººç±»è¯„åˆ†è€…çš„å¹³å‡å‡†ç¡®ç‡69.60%ï¼Œå…¶ä¸­GPT-4 + AC-Reasonè¾¾åˆ°äº†75.04%ã€‚åœ¨AC-Benchä¸Šï¼ŒGPT-4 + AC-Reasonå†æ¬¡è·å¾—äº†æœ€é«˜çš„å‡†ç¡®ç‡71.82%ã€‚AC-Benchè¿˜è¿›ä¸€æ­¥å®ç°äº†å¯¹æ¨ç†å¿ å®åº¦çš„ç»†è‡´åˆ†æï¼Œç»“æœè¡¨æ˜åªæœ‰Qwen-2.5-72B-Instructã€Claude-3.5-Sonnetå’ŒGPT-4å±•ç°å‡ºå¿ å®çš„æ¨ç†èƒ½åŠ›ï¼Œè€ŒGPT-4å€¾å‘äºä½¿ç”¨æ·å¾„ã€‚æœ€åï¼Œæˆ‘ä»¬çš„æ¶ˆèç ”ç©¶è¯æ˜å°†ACç†è®ºæ•´åˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ˜¯éå¸¸æœ‰æ•ˆçš„ï¼Œå…¶ä¸­æˆ‘ä»¬æå‡ºçš„ç®—æ³•è´¡çŒ®äº†å¯¹æ€§èƒ½çš„æœ€å¤§æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08750v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä»‹ç»äº†å®é™…å› æœå…³ç³»ï¼ˆACï¼‰åœ¨å› æœæ¨ç†ï¼ˆCRï¼‰ä¸­çš„é‡è¦æ€§åŠå…¶åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­çš„å½’å› å’Œè´£ä»»åˆ†é…ä½œç”¨ã€‚ç°æœ‰åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•ç¼ºä¹å½¢å¼åŒ–çš„ACç†è®ºæŒ‡å¯¼ï¼Œå¯¼è‡´è§£é‡Šæ€§æœ‰é™ã€‚å› æ­¤ï¼Œæå‡ºäº†AC-ReasonåŠå½¢å¼åŒ–æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡è¯†åˆ«ACåœºæ™¯ä¸­çš„å› æœç›¸å…³äº‹ä»¶ã€æ¨æ–­å…¶å½¢å¼åŒ–å› æœå› ç´ å€¼ï¼ˆå¦‚å……åˆ†æ€§ã€å¿…è¦æ€§å’Œæ­£å¸¸æ€§ï¼‰ï¼Œå¹¶é€šè¿‡ç†è®ºå¼•å¯¼ç®—æ³•ä»¥è§£é‡Šæ–¹å¼å›ç­”ACæŸ¥è¯¢ã€‚å¼•å…¥AC-BenchåŸºå‡†æµ‹è¯•ï¼Œåœ¨Big-Bench Hard Causal Judgmentï¼ˆBBH-CJï¼‰åŸºç¡€ä¸Šæ„å»ºå¹¶å¤§å¹…æ‰©å±•ï¼ŒåŒ…å«çº¦1000ä¸ªç²¾å¿ƒæ ‡æ³¨çš„æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½æœ‰è¯¦ç»†çš„æ¨ç†æ­¥éª¤ï¼Œå¹¶ä¸“æ³¨äºå®é™…å› æœå…³ç³»ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒAC-Reasonæ¡†æ¶èƒ½æŒç»­æé«˜LLMçš„æ€§èƒ½ï¼Œåœ¨BBH-CJå’ŒAC-Benchä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGPT-4 + AC-Reasonå‡†ç¡®ç‡æœ€é«˜è¾¾åˆ°75.04%ã€‚æœ€åï¼Œæ¶ˆèç ”ç©¶è¯æ˜å°†ACç†è®ºæ•´åˆåˆ°LLMä¸­æ˜¯æœ‰æ•ˆçš„ï¼Œå…¶ä¸­æå‡ºçš„ç®—æ³•è´¡çŒ®æœ€å¤§æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®é™…å› æœå…³ç³»ï¼ˆACï¼‰æ˜¯å› æœæ¨ç†ï¼ˆCRï¼‰çš„æ ¸å¿ƒï¼Œå¯¹äºç°å®ä¸–ç•Œçš„å½’å› å’Œè´£ä»»åˆ†é…è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•åœ¨å› æœæ¨ç†æ–¹é¢ç¼ºä¹å½¢å¼åŒ–ç†è®ºæ”¯æ’‘ï¼Œå¯¼è‡´è§£é‡Šæ€§ä¸è¶³ã€‚</li>
<li>AC-Reasonæ¡†æ¶é€šè¿‡è¯†åˆ«ACåœºæ™¯ä¸­çš„å› æœç›¸å…³äº‹ä»¶å’Œæ¨æ–­å…¶å› æœå› ç´ å€¼ï¼Œæ”¯æŒåŠå½¢å¼åŒ–çš„æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>AC-BenchåŸºå‡†æµ‹è¯•æ‰©å±•è‡ªBBH-CJï¼ŒåŒ…å«ç²¾å¿ƒæ ‡æ³¨çš„æ ·æœ¬ï¼Œä¸“æ³¨äºè¯„ä¼°å®é™…å› æœå…³ç³»æ¨ç†èƒ½åŠ›ã€‚</li>
<li>AC-Reasonèƒ½æé«˜LLMåœ¨BBH-CJå’ŒAC-Benchä¸Šçš„æ€§èƒ½ï¼ŒGPT-4 + AC-Reasonè¡¨ç°æœ€ä½³ã€‚</li>
<li>ç²¾ç»†ç²’åº¦çš„åˆ†ææ˜¾ç¤ºæŸäº›LLMï¼ˆå¦‚GPT-4ï¼‰å­˜åœ¨æ¨ç†å¿ å®æ€§é—®é¢˜ï¼Œå€¾å‘äºé‡‡å–æ·å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08750">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1bc52ecbb8643f5dd31f1d80853ee0d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34da65f61c196c5e6d5a5c47d23309d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-336b99bd6296968d3f0965542ac9c9e3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="TRAIL-Trace-Reasoning-and-Agentic-Issue-Localization"><a href="#TRAIL-Trace-Reasoning-and-Agentic-Issue-Localization" class="headerlink" title="TRAIL: Trace Reasoning and Agentic Issue Localization"></a>TRAIL: Trace Reasoning and Agentic Issue Localization</h2><p><strong>Authors:Darshan Deshpande, Varun Gangal, Hersh Mehta, Jitin Krishnan, Anand Kannappan, Rebecca Qian</strong></p>
<p>The increasing adoption of agentic workflows across diverse domains brings a critical need to scalably and systematically evaluate the complex traces these systems generate. Current evaluation methods depend on manual, domain-specific human analysis of lengthy workflow traces - an approach that does not scale with the growing complexity and volume of agentic outputs. Error analysis in these settings is further complicated by the interplay of external tool outputs and language model reasoning, making it more challenging than traditional software debugging. In this work, we (1) articulate the need for robust and dynamic evaluation methods for agentic workflow traces, (2) introduce a formal taxonomy of error types encountered in agentic systems, and (3) present a set of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and grounded in established agentic benchmarks. To ensure ecological validity, we curate traces from both single and multi-agent systems, focusing on real-world applications such as software engineering and open-world information retrieval. Our evaluations reveal that modern long context LLMs perform poorly at trace debugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our dataset and code are made publicly available to support and accelerate future research in scalable evaluation for agentic workflows. </p>
<blockquote>
<p>éšç€ä»£ç†å·¥ä½œæµç¨‹åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨æ—¥ç›Šå¹¿æ³›ï¼Œå¯¹å¯ä¼¸ç¼©ã€ç³»ç»Ÿåœ°è¯„ä¼°è¿™äº›ç³»ç»Ÿäº§ç”Ÿçš„å¤æ‚è·Ÿè¸ªä¿¡æ¯çš„éœ€æ±‚ä¹Ÿæ—¥ç›Šè¿«åˆ‡ã€‚å½“å‰çš„è¯„ä¼°æ–¹æ³•ä¾èµ–äºå¯¹å†—é•¿çš„å·¥ä½œæµè·Ÿè¸ªè¿›è¡Œæ‰‹åŠ¨ã€ç‰¹å®šé¢†åŸŸçš„äººç±»åˆ†æâ€”â€”è¿™ç§æ–¹æ³•æ— æ³•éšç€ä»£ç†è¾“å‡ºçš„å¤æ‚æ€§å’Œæ•°é‡çš„å¢é•¿è€Œæ‰©å±•ã€‚è¿™äº›ç¯å¢ƒä¸­çš„é”™è¯¯åˆ†æè¿›ä¸€æ­¥å—åˆ°å¤–éƒ¨å·¥å…·è¾“å‡ºå’Œè¯­è¨€æ¨¡å‹æ¨ç†çš„ç›¸äº’ä½œç”¨çš„å½±å“ï¼Œä½¿å…¶æ¯”ä¼ ç»Ÿè½¯ä»¶è°ƒè¯•æ›´å…·æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ï¼ˆ1ï¼‰æå‡ºäº†å¯¹ä»£ç†å·¥ä½œæµç¨‹è·Ÿè¸ªè¿›è¡Œç¨³å¥å’ŒåŠ¨æ€è¯„ä¼°çš„éœ€æ±‚ï¼Œï¼ˆ2ï¼‰ä»‹ç»äº†åœ¨ä»£ç†ç³»ç»Ÿä¸­é‡åˆ°é”™è¯¯ç±»å‹çš„æ­£å¼åˆ†ç±»ï¼Œä»¥åŠï¼ˆ3ï¼‰æ ¹æ®è¿™ä¸€åˆ†ç±»å­¦å¹¶åŸºäºå·²å»ºç«‹çš„ä»£ç†åŸºå‡†æµ‹è¯•ï¼Œå±•ç¤ºäº†ä¸€å¥—ç”±äººç±»æ ‡æ³¨çš„148ä¸ªå¤§å‹è·Ÿè¸ªé›†ï¼ˆTRAILï¼‰ã€‚ä¸ºä¿è¯ç”Ÿæ€æ•ˆåº¦ï¼Œæˆ‘ä»¬ä»å•ä»£ç†ç³»ç»Ÿå’Œå¤šä»£ç†ç³»ç»Ÿä¸­æŒ‘é€‰è·Ÿè¸ªé›†ï¼Œé‡ç‚¹å…³æ³¨ç°å®ä¸–ç•Œçš„åº”ç”¨ï¼Œå¦‚è½¯ä»¶å·¥ç¨‹å’Œå¼€æ”¾ä¸–ç•Œä¿¡æ¯æ£€ç´¢ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è·Ÿè¸ªè°ƒè¯•æ–¹é¢çš„è¡¨ç°ä¸ä½³ï¼Œæœ€ä½³Gemini-2.5-proæ¨¡å‹çš„TRAILå¾—åˆ†ä»…ä¸º11%ã€‚æˆ‘ä»¬å…¬å¼€æä¾›æ•°æ®é›†å’Œä»£ç ï¼Œä»¥æ”¯æŒå’ŒåŠ é€Ÿæœªæ¥å¯¹ä»£ç†å·¥ä½œæµç¨‹çš„å¯æ‰©å±•è¯„ä¼°çš„ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08638v1">PDF</a> Dataset link: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/PatronusAI/TRAIL">https://huggingface.co/datasets/PatronusAI/TRAIL</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†éšç€agenticå·¥ä½œæµç¨‹åœ¨å¤šä¸ªé¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå¯¹å¯æ‰©å±•å’Œç³»ç»Ÿæ€§è¯„ä»·å…¶ç”Ÿæˆçš„å¤æ‚è·Ÿè¸ªä¿¡æ¯çš„éœ€æ±‚æ—¥ç›Šè¿«åˆ‡ã€‚å½“å‰çš„è¯„ä»·æ–¹æ³•ä¾èµ–äºå¯¹å†—é•¿çš„å·¥ä½œæµè·Ÿè¸ªè¿›è¡Œæ‰‹åŠ¨ã€ç‰¹å®šé¢†åŸŸçš„åˆ†æï¼Œè¿™ç§æ–¹æ³•æ— æ³•é€‚åº”agenticè¾“å‡ºçš„æ—¥ç›Šå¤æ‚å’Œå¤§é‡å¢é•¿ã€‚æœ¬æ–‡ä¸­ï¼Œä½œè€…å¼ºè°ƒäº†éœ€è¦é’ˆå¯¹agenticå·¥ä½œæµè·Ÿè¸ªè¿›è¡Œç¨³å¥å’ŒåŠ¨æ€çš„è¯„ä»·æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†åœ¨agenticç³»ç»Ÿä¸­é‡åˆ°çš„é”™è¯¯ç±»å‹çš„æ­£å¼åˆ†ç±»ã€‚æ­¤å¤–ï¼Œè¿˜åˆ©ç”¨æ­¤åˆ†ç±»å’ŒåŸºäºå…¬è®¤çš„agenticåŸºå‡†æµ‹è¯•æ„å»ºäº†ä¸€å¥—åŒ…å«å¤§å‹äººä¸ºæ³¨é‡Šçš„è·Ÿè¸ªæ•°æ®ï¼ˆTRAILï¼‰ã€‚è¿™äº›è·Ÿè¸ªæ•°æ®æ¥æºäºå•æ™ºèƒ½ä½“å’Œå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œä¸“æ³¨äºè½¯ä»¶å·¥ç¨‹å’Œå¼€æ”¾ä¸–ç•Œä¿¡æ¯æ£€ç´¢ç­‰å®é™…åº”ç”¨åœºæ™¯ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œç°ä»£é•¿æ–‡æœ¬LLMæ¨¡å‹åœ¨è·Ÿè¸ªè°ƒè¯•æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œæ€§èƒ½æœ€ä½³çš„Gemini-2.5ä¸“ä¸šæ¨¡å‹åœ¨TRAILä¸Šä»…å¾—åˆ†ç™¾åˆ†ä¹‹åä¸€ã€‚ä¸ºäº†æ–¹ä¾¿å’ŒåŠ é€Ÿæœªæ¥çš„ç ”ç©¶å·¥ä½œï¼Œæ•°æ®é›†å’Œä»£ç éƒ½å·²å…¬å¼€å‘å¸ƒã€‚<br> ç²¾ç®€åçš„ä¸­æ–‡ç‰ˆæ¦‚è¿°ä¸è¶…å­—æ•°é™åˆ¶ï¼ˆçº¦ä¸€ç™¾å­—ï¼‰ï¼šæœ¬æ–‡ä¸»è¦è®¨è®ºagenticå·¥ä½œæµç¨‹è¯„ä»·çš„é—®é¢˜ï¼Œæå‡ºéœ€è¦æ–°çš„è¯„ä»·æ–¹æ³•å’Œæ•°æ®é›†æ¥åº”å¯¹å¤æ‚æ€§æå‡å’Œå·¥ä½œé‡å¢é•¿çš„é—®é¢˜ã€‚æ„å»ºäº†ä¸€ç§æ–°æ•°æ®é›†â€”â€”TRAILç”¨äºæµ‹è¯•å’Œæ¯”è¾ƒä¸åŒçš„agenticå·¥ä½œæµç¨‹æ€§èƒ½è¯„ä»·çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæ—¨åœ¨å¸®åŠ©æ›´å¥½åœ°åº”å¯¹è¯¯å·®æ£€æµ‹å’Œè¿½è¸ªé”™è¯¯åˆ†æç­‰ç°å®éœ€æ±‚åœºæ™¯åº”ç”¨ï¼›ä½†åŒæ—¶æ³¨æ„åˆ°æ€§èƒ½è¯„ä»·é¢ä¸´å›°éš¾çš„é—®é¢˜å’ŒæŒ‘æˆ˜ã€‚é€šè¿‡æ•°æ®é›†çš„å…¬å¼€åˆ†äº«å’Œå…±äº«ç ”ç©¶æˆæœæ¨åŠ¨ç›¸å…³é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶å’Œå‘å±•ã€‚  æ€»ä½“è€Œè¨€æ˜¯æä¾›äº†ä¸€äº›å…·æœ‰å¯å‘æ€§çš„ç ”ç©¶è§†è§’å’Œæ€è€ƒï¼Œä½†ä¹Ÿå¸¦æ¥äº†æ€§èƒ½å’Œå¯é æ€§çš„æŒ‘æˆ˜éœ€è¦è¿›ä¸€æ­¥æ¢è®¨å’Œç ”ç©¶çš„é—®é¢˜æ„è¯†å’Œæ–¹æ³•æŒ‡å¯¼æ–¹é¢çš„åæ€ç ”ç©¶é—®é¢˜å’Œæ¨è¿›æŒ‘æˆ˜æœªæ¥æ–¹å‘æ€§ä»·å€¼çš„æ–‡ç« å†…å®¹æ€»ç»“å’Œè¡¨è¿°è¡¨è¾¾é‡è¦ä¿¡æ¯å’Œå…³é”®ç‚¹ï¼›åŸºäºå¤§é‡æ•°æ®çš„çœŸå®å®éªŒæµ‹è¯•åˆ†æç»“æœæ€»ç»“åŠå…·ä½“é—®é¢˜çš„è§£å†³æ–¹å‘ä»¥åŠåç»­çš„å¯¹ç­–é€‰æ‹©æŒ‡å‘æ½œåœ¨è¿›ä¸€æ­¥å¯èƒ½åº”ç”¨çš„æ¡ˆä¾‹å»ºè®®å’Œä¸‹ä¸€æ­¥å·¥ä½œè®¡åˆ’å‰ç»æ€§å¯å‘ç†è§£å‘å±•çš„äº®ç‚¹<br>ä»¥ä¸ŠsummaryåŒæ—¶å……åˆ†æ³¨æ„åˆ°äº†æœ¬æ–‡ä½œè€…ç ”ç©¶ä¸­é‡åˆ°çš„éš¾ç‚¹ä»¥åŠé‡‡ç”¨çš„åˆ›æ–°è§£å†³æ€è·¯å¦‚åŸºäºç°ä»£ç§‘æŠ€çš„æŠ€æœ¯æ€§ç‰¹è‰²ï¼šæå‡ºå¹¶å¼€å‘ä¸€ç§æ–°çš„å…¬å¼€å¯ç”¨çš„æ•°æ®é›†å¹¶å°è¯•è§£å†³äº†ç°å®åº”ç”¨ä¸­æ€§èƒ½è¯„ä»·çš„éš¾ç‚¹å’Œå¤æ‚åº¦çš„é—®é¢˜åœ¨æ•´ä½“é¢†åŸŸä¸­é€šè¿‡å°è¯•è§£å†³æ–¹æ¡ˆå¼•å‡ºäº†ä¸€äº›æœ‰å¯å‘æ€§æœªæ¥è¿›ä¸€æ­¥æ·±å…¥ç ”ç©¶å¯èƒ½æ€§çš„é—®é¢˜ï¼Œå› æ­¤æ˜¯å¯¹å…¨æ–‡å¾ˆå¥½çš„ä¸€ä¸ªç®€æ˜æ‰¼è¦çš„æ¦‚è¿°è¯´æ˜æƒ…å†µå¯¹äºå…¶ä»–åŒè¡Œæœ‰å¼•å¯¼å­¦ä¹ å’Œæ·±åŒ–æ¢ç´¢å®è·µçš„æ–¹å‘å¯å‘ä»·å€¼å’Œå¯å‘æ€§æ€ç»´çš„å¼€æ‹“æ•ˆæœè¾ƒçªå‡ºè¯¥è®ºæ–‡æœªæ¥é¢†åŸŸçš„å‰æ™¯æ¨å¹¿åº”ç”¨æ‹“å±•å°†æœ‰è‰¯å¥½çš„åŠ©åŠ›ä½œç”¨å’Œå¯è¡Œæ€§ä½œç”¨ä¸å¸®åŠ©è¾ƒå¤§é€šè¿‡æé«˜æŠ€æœ¯åˆ›æ–°åˆ›æ–°æ¨åŠ¨è§£å†³å…³é”®éš¾ç‚¹çš„æ–¹å¼å‘ˆç°å‡ºå¯¹äºç§‘æŠ€ç¤¾ä¼šä»¥åŠç§‘æŠ€è¿›æ­¥çš„åº”ç”¨åœºæ™¯ä¸­æ‰€èƒ½äº§ç”Ÿçš„å®è·µæ„ä¹‰å’Œæ•ˆç›Šçš„åº”ç”¨ç ”ç©¶æœ‰ç€æ·±åˆ»çš„åˆ†æåŒæ—¶æœ‰ä¸€å®šå¯¹ç­–æè®®ã€å¯¹æœªæ¥çš„å¯è¡Œæ€§è€ƒé‡ç­‰å¤šç»´åº¦å±•æœ›çš„ç°å®å¯æ“ä½œæ€§æä¾›äº†è¾ƒå¤šçš„åº”ç”¨ä»·å€¼å…¼å…·åˆç†æ€§ä¸å…¨é¢æ€§åŠ©åŠ›äºæ›´åŠ å…·æœ‰å¯æ“ä½œæ€§æ‰©å±•è®¤çŸ¥æœ‰æ•ˆæ„ä¹‰æ”¯æŒç±»ä¼¼å®è·µæ¨å¹¿è¾ƒä¸ºæœ‰æ•ˆæœ‰åŠ›çš„ç§‘æŠ€æ¨å¹¿åº”ç”¨è¯„ä»·ç±»æ‘˜è¦çš„æ ¸å¿ƒä¿¡æ¯æ±‡æ€»ä¼ é€’æˆåŠŸè§£å†³äº†ç°å®ä¸­ç§‘æŠ€é¢†åŸŸçš„éš¾é¢˜å¹¶ä¸”å±•æœ›æœªæ¥è¯¥é¢†åŸŸçš„åº”ç”¨å‰æ™¯</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08638">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6b0c3f4a2143ce7f7e1a2587125e64c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-259552e95ae5cbde9b08b614767e7a25.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="D-Hammer-Efficient-Equational-Reasoning-for-Labelled-Dirac-Notation"><a href="#D-Hammer-Efficient-Equational-Reasoning-for-Labelled-Dirac-Notation" class="headerlink" title="D-Hammer: Efficient Equational Reasoning for Labelled Dirac Notation"></a>D-Hammer: Efficient Equational Reasoning for Labelled Dirac Notation</h2><p><strong>Authors:Yingte Xu, Li Zhou, Gilles Barthe</strong></p>
<p>Labelled Dirac notation is a formalism commonly used by physicists to represent many-body quantum systems and by computer scientists to assert properties of quantum programs. It is supported by a rich equational theory for proving equality between expressions in the language. These proofs are typically carried on pen-and-paper, and can be exceedingly long and error-prone. We introduce D-Hammer, the first tool to support automated equational proof for labelled Dirac notation. The salient features of D-Hammer include: an expressive, higher-order, dependently-typed language for labelled Dirac notation; an efficient normalization algorithm; and an optimized C++ implementation. We evaluate the implementation on representative examples from both plain and labelled Dirac notation. In the case of plain Dirac notation, we show that our implementation significantly outperforms DiracDec. </p>
<blockquote>
<p>æ ‡è®°Diracç¬¦å·æ˜¯ä¸€ç§å½¢å¼ä¸»ä¹‰ï¼Œç‰©ç†å­¦å®¶å¸¸ç”¨å®ƒæ¥è¡¨ç¤ºå¤šä½“é‡å­ç³»ç»Ÿï¼Œè®¡ç®—æœºç§‘å­¦å®¶ç”¨å®ƒæ¥æ–­è¨€é‡å­ç¨‹åºçš„å±æ€§ã€‚å®ƒæ”¯æŒä¸°å¯Œçš„ç­‰å¼ç†è®ºï¼Œç”¨äºè¯æ˜è¯­è¨€ä¸­è¡¨è¾¾å¼ä¹‹é—´çš„ç­‰å¼å…³ç³»ã€‚è¿™äº›è¯æ˜é€šå¸¸ç”¨ç¬”å’Œçº¸è¿›è¡Œï¼Œå¯èƒ½éå¸¸å†—é•¿ä¸”å®¹æ˜“å‡ºé”™ã€‚æˆ‘ä»¬ä»‹ç»äº†D-Hammerï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ”¯æŒæ ‡è®°Diracç¬¦å·çš„è‡ªåŠ¨åŒ–ç­‰å¼è¯æ˜çš„å·¥å…·ã€‚D-Hammerçš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼šç”¨äºæ ‡è®°Diracç¬¦å·çš„è¡¨è¾¾åŠ›å¼ºã€é«˜é˜¶ã€ä¾èµ–ç±»å‹è¯­è¨€ï¼›é«˜æ•ˆçš„æ­£è§„åŒ–ç®—æ³•ï¼›ä»¥åŠä¼˜åŒ–çš„C++å®ç°ã€‚æˆ‘ä»¬å¯¹æ¥è‡ªæ™®é€šDiracç¬¦å·å’Œæ ‡è®°Diracç¬¦å·çš„ä»£è¡¨æ€§ç¤ºä¾‹è¿›è¡Œäº†å®æ–½è¯„ä¼°ã€‚åœ¨æ™®é€šDiracç¬¦å·çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¡¨æ˜æˆ‘ä»¬çš„å®ç°æ˜¾è‘—ä¼˜äºDiracDecã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08633v1">PDF</a> This version of the contribution has been accepted for publication,   after peer review but is not the Version of Record and does not reflect   post-acceptance improvements, or any corrections. 47 pages (with appendix)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç”¨äºæ ‡è®°Diracç¬¦å·çš„å½¢å¼ä¸»ä¹‰ï¼Œå®ƒåœ¨ç‰©ç†å­¦å®¶ä¸­å¸¸ç”¨äºè¡¨ç¤ºå¤šä½“é‡å­ç³»ç»Ÿï¼Œåœ¨è®¡ç®—æœºç§‘å­¦å®¶ä¸­ç”¨äºæ–­è¨€é‡å­ç¨‹åºçš„å±æ€§ã€‚ç„¶è€Œï¼Œè¯æ˜è¡¨è¾¾å¼ä¹‹é—´ç­‰ä»·çš„è¯æ˜é€šå¸¸éœ€è¦æ‰‹åŠ¨è¿›è¡Œï¼Œå¯èƒ½ä¼šéå¸¸å†—é•¿å’Œå®¹æ˜“å‡ºé”™ã€‚å› æ­¤ï¼Œæœ¬æ–‡å¼•å…¥äº†D-Hammerå·¥å…·ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªæ”¯æŒæ ‡è®°Diracç¬¦å·çš„è‡ªåŠ¨åŒ–ç­‰å¼è¯æ˜çš„å·¥å…·ã€‚D-Hammerå…·æœ‰è¡¨è¾¾æ€§é«˜ã€é¡ºåºä¾èµ–ç±»å‹åŒ–çš„è¯­è¨€ç‰¹ç‚¹ï¼Œæ‹¥æœ‰é«˜æ•ˆçš„æ­£è§„åŒ–ç®—æ³•å’Œä¼˜åŒ–åçš„C++å®ç°ã€‚åœ¨æ™®é€šå’Œæ ‡è®°Diracç¬¦å·çš„ä»£è¡¨ä¾‹å­ä¸Šè¿›è¡Œçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå¯¹äºæ™®é€šDiracç¬¦å·ï¼Œæˆ‘ä»¬çš„å®ç°æ˜¾è‘—ä¼˜äºDiracDecã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ ‡ç¤ºDiracç¬¦å·æ˜¯ä¸€ç§å¸¸ç”¨äºç‰©ç†å’Œè®¡ç®—æœºé¢†åŸŸçš„è¡¨ç¤ºå½¢å¼ï¼Œå°¤å…¶åœ¨é‡å­è®¡ç®—å’Œé‡å­ç¨‹åºå±æ€§æ–¹é¢å°¤ä¸ºé‡è¦ã€‚</li>
<li>ç›®å‰å¯¹æ ‡è®°Diracç¬¦å·çš„ç­‰å¼è¯æ˜é€šå¸¸ä¾èµ–äºæ‰‹åŠ¨æ“ä½œï¼Œè¿‡ç¨‹å†—é•¿ä¸”æ˜“å‡ºé”™ã€‚</li>
<li>D-Hammerå·¥å…·é¦–æ¬¡å®ç°äº†å¯¹æ ‡è®°Diracç¬¦å·çš„è‡ªåŠ¨åŒ–ç­‰å¼è¯æ˜æ”¯æŒã€‚</li>
<li>D-Hammerå·¥å…·å…·å¤‡é«˜çº§ã€ä¾èµ–ç±»å‹åŒ–çš„è¯­è¨€ç‰¹ç‚¹ã€‚</li>
<li>D-Hammerå·¥å…·å…·å¤‡é«˜æ•ˆçš„æ­£è§„åŒ–ç®—æ³•å’ŒC++ä¼˜åŒ–å®ç°ã€‚</li>
<li>å¯¹äºæ™®é€šDiracç¬¦å·çš„ä»£è¡¨ä¾‹å­ï¼ŒD-Hammerçš„å®ç°æ€§èƒ½æ˜¾è‘—ä¼˜äºDiracDecã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-02ca1c59aef095d54981cc6d02e60ef5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="OpenThinkIMG-Learning-to-Think-with-Images-via-Visual-Tool-Reinforcement-Learning"><a href="#OpenThinkIMG-Learning-to-Think-with-Images-via-Visual-Tool-Reinforcement-Learning" class="headerlink" title="OpenThinkIMG: Learning to Think with Images via Visual Tool   Reinforcement Learning"></a>OpenThinkIMG: Learning to Think with Images via Visual Tool   Reinforcement Learning</h2><p><strong>Authors:Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, Yu Cheng</strong></p>
<p>While humans can flexibly leverage interactive visual cognition for complex problem-solving, enabling Large Vision-Language Models (LVLMs) to learn similarly adaptive behaviors with visual tools remains challenging. A significant hurdle is the current lack of standardized infrastructure, which hinders integrating diverse tools, generating rich interaction data, and training robust agents effectively. To address these gaps, we introduce OpenThinkIMG, the first open-source, comprehensive end-to-end framework for tool-augmented LVLMs. It features standardized vision tool interfaces, scalable trajectory generation for policy initialization, and a flexible training environment. Furthermore, considering supervised fine-tuning (SFT) on static demonstrations offers limited policy generalization for dynamic tool invocation, we propose a novel reinforcement learning (RL) framework V-ToolRL to train LVLMs to learn adaptive policies for invoking external vision tools. V-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies by directly optimizing for task success using feedback from tool interactions. We empirically validate V-ToolRL on challenging chart reasoning tasks. Our RL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its SFT-initialized counterpart (+28.83 points) and surpasses established supervised tool-learning baselines like Taco and CogCom by an average of +12.7 points. Notably, it also surpasses prominent closed-source models like GPT-4.1 by +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational framework for advancing dynamic, tool-augmented visual reasoning, helping the community develop AI agents that can genuinely â€œthink with imagesâ€. </p>
<blockquote>
<p>äººç±»åœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶ï¼Œå¯ä»¥çµæ´»åˆ©ç”¨äº¤äº’å¼è§†è§‰è®¤çŸ¥ã€‚ç„¶è€Œï¼Œè®©å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å­¦ä¹ ç±»ä¼¼è‡ªé€‚åº”è¡Œä¸ºå¹¶ä½¿ç”¨è§†è§‰å·¥å…·ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸€ä¸ªä¸»è¦çš„éšœç¢æ˜¯ç¼ºä¹æ ‡å‡†åŒ–åŸºç¡€è®¾æ–½ï¼Œè¿™é˜»ç¢äº†æ•´åˆå„ç§å·¥å…·ã€ç”Ÿæˆä¸°å¯Œçš„äº¤äº’æ•°æ®ä»¥åŠæœ‰æ•ˆåœ°è®­ç»ƒç¨³å¥çš„ä»£ç†ã€‚ä¸ºäº†è§£å†³è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†OpenThinkIMGï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºå·¥å…·å¢å¼ºLVLMsçš„å¼€æºã€å…¨é¢ç«¯åˆ°ç«¯çš„æ¡†æ¶ã€‚å®ƒå…·å¤‡æ ‡å‡†åŒ–è§†è§‰å·¥å…·æ¥å£ã€å¯æ‰©å±•çš„è½¨è¿¹ç”Ÿæˆä»¥è¿›è¡Œç­–ç•¥åˆå§‹åŒ–ï¼Œä»¥åŠçµæ´»çš„è®­ç»ƒç¯å¢ƒã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°å¯¹é™æ€æ¼”ç¤ºçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¯¹äºåŠ¨æ€å·¥å…·è°ƒç”¨çš„ç­–ç•¥æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶V-ToolRLï¼Œç”¨äºè®­ç»ƒLVLMså­¦ä¹ è‡ªé€‚åº”ç­–ç•¥ä»¥è°ƒç”¨å¤–éƒ¨è§†è§‰å·¥å…·ã€‚V-ToolRLä½¿LVLMsèƒ½å¤Ÿé€šè¿‡ç›´æ¥ä¼˜åŒ–ä»»åŠ¡æˆåŠŸåé¦ˆæ¥è‡ªä¸»å‘ç°æœ€ä½³å·¥å…·ä½¿ç”¨ç­–ç•¥ã€‚æˆ‘ä»¬é€šè¿‡å…·æœ‰æŒ‘æˆ˜æ€§çš„å›¾è¡¨æ¨ç†ä»»åŠ¡å¯¹V-ToolRLè¿›è¡Œäº†å®è¯éªŒè¯ã€‚æˆ‘ä»¬çš„åŸºäºQwen2-VL-2Bçš„RLè®­ç»ƒä»£ç†æ˜¾è‘—ä¼˜äºå…¶SFTåˆå§‹åŒ–çš„å¯¹åº”ç‰©ï¼ˆ+28.83åˆ†ï¼‰ï¼Œå¹¶è¶…è¶Šäº†Tacoå’ŒCogComç­‰æ—¢å®šçš„ç›‘ç£å·¥å…·å­¦ä¹ åŸºå‡†çº¿ï¼Œå¹³å‡é«˜å‡º+12.7åˆ†ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒè¿˜è¶…è¶Šäº†é—­æºæ¨¡å‹å¦‚GPT-4.1ï¼Œå‡†ç¡®ç‡æé«˜äº†+8.68åˆ†ã€‚æˆ‘ä»¬å¸Œæœ›OpenThinkIMGå¯ä»¥ä½œä¸ºæ¨è¿›åŠ¨æ€ã€å·¥å…·å¢å¼ºçš„è§†è§‰æ¨ç†çš„åŸºç¡€æ¡†æ¶ï¼Œå¸®åŠ©ç¤¾åŒºå¼€å‘èƒ½å¤ŸçœŸæ­£â€œä»¥å›¾åƒæ€è€ƒâ€çš„AIä»£ç†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08617v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong><br>è¿™æ˜¯ä¸€ç¯‡å…³äºåˆ©ç”¨å¼€æºæ¡†æ¶OpenThinkIMGå®ç°å·¥å…·å¢å¼ºçš„è§†è§‰è®¤çŸ¥æ¨¡å‹çš„ç ”ç©¶ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆå¼ºåŒ–å­¦ä¹ çš„æ–°å‹è®­ç»ƒæ–¹æ³•ï¼Œä»¥æ”¯æŒæ¨¡å‹è‡ªä¸»å‘ç°æœ€ä½³å·¥å…·ä½¿ç”¨ç­–ç•¥ï¼Œä»è€Œæå‡åœ¨å›¾è¡¨ç†è§£ç­‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ç»è¿‡å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å›¾è¡¨æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•å’Œå…¶ä»–å…ˆè¿›çš„å·¥å…·å­¦ä¹ æ¨¡å‹ã€‚è¿™æ ‡å¿—ç€AIå›¾åƒç†è§£çš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨åˆ©ç”¨è§†è§‰å·¥å…·è¿›è¡Œè‡ªé€‚åº”è¡Œä¸ºå­¦ä¹ æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç¼ºä¹æ ‡å‡†åŒ–åŸºç¡€è®¾æ–½æ˜¯é˜»ç¢å› ç´ ä¹‹ä¸€ï¼Œå½±å“å·¥å…·æ•´åˆã€ä¸°å¯Œäº¤äº’æ•°æ®çš„æœ‰æ•ˆè®­ç»ƒã€‚</li>
<li>OpenThinkIMGæ˜¯ä¸€ä¸ªå¼€æºçš„ã€å…¨é¢çš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œç”¨äºå·¥å…·å¢å¼ºçš„LVLMsï¼Œå…·æœ‰æ ‡å‡†åŒ–è§†è§‰å·¥å…·æ¥å£ã€å¯æ‰©å±•çš„è½¨è¿¹ç”Ÿæˆå’Œçµæ´»çš„è®­ç»ƒç¯å¢ƒã€‚</li>
<li>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åœ¨åŠ¨æ€å·¥å…·è°ƒç”¨ä¸Šçš„ç­–ç•¥æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œå› æ­¤æå‡ºäº†ä¸€ç§æ–°å‹çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶V-ToolRLæ¥è®­ç»ƒLVLMsã€‚</li>
<li>V-ToolRLä½¿LVLMsèƒ½å¤Ÿé€šè¿‡ç›´æ¥ä¼˜åŒ–ä»»åŠ¡æˆåŠŸåé¦ˆæ¥è‡ªä¸»å‘ç°æœ€ä½³å·¥å…·ä½¿ç”¨ç­–ç•¥ã€‚</li>
<li>åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å›¾è¡¨æ¨ç†ä»»åŠ¡ä¸Šï¼ŒRLè®­ç»ƒçš„æ¨¡å‹æ˜¾è‘—ä¼˜äºSFTåˆå§‹åŒ–çš„æ¨¡å‹å’Œå…¶ä»–å…ˆè¿›çš„ç›‘ç£å·¥å…·å­¦ä¹ æ¨¡å‹ï¼Œå¦‚Tacoå’ŒCogComã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a04168629f9da424f91fb5c17ebb1912.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aeee57b4659334f11bd7588a548eab73.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-567555b432b20a71770f8bee7e654764.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="From-Seeing-to-Doing-Bridging-Reasoning-and-Decision-for-Robotic-Manipulation"><a href="#From-Seeing-to-Doing-Bridging-Reasoning-and-Decision-for-Robotic-Manipulation" class="headerlink" title="From Seeing to Doing: Bridging Reasoning and Decision for Robotic   Manipulation"></a>From Seeing to Doing: Bridging Reasoning and Decision for Robotic   Manipulation</h2><p><strong>Authors:Yifu Yuan, Haiqin Cui, Yibin Chen, Zibin Dong, Fei Ni, Longxin Kou, Jinyi Liu, Pengyi Li, Yan Zheng, Jianye Hao</strong></p>
<p>Achieving generalization in robotic manipulation remains a critical challenge, particularly for unseen scenarios and novel tasks. Current Vision-Language-Action (VLA) models, while building on top of general Vision-Language Models (VLMs), still fall short of achieving robust zero-shot performance due to the scarcity and heterogeneity prevalent in embodied datasets. To address these limitations, we propose FSD (From Seeing to Doing), a novel vision-language model that generates intermediate representations through spatial relationship reasoning, providing fine-grained guidance for robotic manipulation. Our approach combines a hierarchical data pipeline for training with a self-consistency mechanism that aligns spatial coordinates with visual signals. Through extensive experiments, we comprehensively validated FSDâ€™s capabilities in both â€œseeingâ€ and â€œdoing,â€ achieving outstanding performance across 8 benchmarks for general spatial reasoning and embodied reference abilities, as well as on our proposed more challenging benchmark VABench. We also verified zero-shot capabilities in robot manipulation, demonstrating significant performance improvements over baseline methods in both SimplerEnv and real robot settings. Experimental results show that FSD achieves 54.1% success rate in SimplerEnv and 72% success rate across 8 real-world tasks, outperforming the strongest baseline by 30%. </p>
<blockquote>
<p>å®ç°æœºå™¨äººåœ¨æ“ä½œä¸­çš„é€šç”¨æ€§ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªè§è¿‡çš„åœºæ™¯å’Œæ–°å‹ä»»åŠ¡ä¸­ã€‚å°½ç®¡å½“å‰çš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹å»ºç«‹åœ¨é€šç”¨çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¹‹ä¸Šï¼Œä½†ç”±äºä½“ç°æ•°æ®é›†çš„ç¨€ç¼ºæ€§å’Œå¼‚è´¨æ€§ï¼Œå®ƒä»¬ä»ç„¶éš¾ä»¥å®ç°ç¨³å¥çš„é›¶å°„å‡»æ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†FSDï¼ˆä»çœ‹åˆ°åˆ°è¡ŒåŠ¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå®ƒé€šè¿‡ç©ºé—´å…³ç³»æ¨ç†ç”Ÿæˆä¸­é—´è¡¨ç¤ºï¼Œä¸ºæœºå™¨äººæ“ä½œæä¾›ç²¾ç»†çš„æŒ‡å¯¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ç”¨äºè®­ç»ƒçš„åˆ†å±‚æ¬¡æ•°æ®ç®¡é“å’Œè‡ªä¸€è‡´æ€§æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å°†ç©ºé—´åæ ‡ä¸è§†è§‰ä¿¡å·å¯¹é½ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬å…¨é¢éªŒè¯äº†FSDåœ¨â€œçœ‹åˆ°â€å’Œâ€œè¡ŒåŠ¨â€æ–¹é¢çš„èƒ½åŠ›ï¼Œåœ¨é€šç”¨çš„ç©ºé—´æ¨ç†å’Œä½“ç°å‚è€ƒèƒ½åŠ›çš„8ä¸ªåŸºå‡†æµ‹è¯•ä»¥åŠæˆ‘ä»¬æå‡ºçš„æ›´å…·æŒ‘æˆ˜æ€§çš„VABenchåŸºå‡†æµ‹è¯•ä¸Šéƒ½å–å¾—äº†å‡ºè‰²çš„è¡¨ç°ã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†æœºå™¨äººåœ¨æ“ä½œä¸­çš„é›¶å°„å‡»èƒ½åŠ›ï¼Œåœ¨SimplerEnvå’ŒçœŸå®æœºå™¨äººç¯å¢ƒä¸­ï¼Œç›¸å¯¹äºåŸºå‡†æ–¹æ³•çš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFSDåœ¨SimplerEnvä¸­çš„æˆåŠŸç‡ä¸º54.1%ï¼Œåœ¨8ä¸ªçœŸå®ä¸–ç•Œä»»åŠ¡ä¸­çš„æˆåŠŸç‡ä¸º72%ï¼Œä¼˜äºæœ€å¼ºåŸºå‡†æ–¹æ³•3è‰ºæ•¢çµæ¨èƒ¶ç»Ÿå°‘çŠ¯å®‰0%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08548v1">PDF</a> Early version</p>
<p><strong>Summary</strong></p>
<p>æ–°ä¸€ä»£è§†è§‰-è¯­è¨€æ¨¡å‹FSDé€šè¿‡ç©ºé—´å…³ç³»æ¨ç†ç”Ÿæˆä¸­é—´è¡¨ç¤ºï¼Œä¸ºæœºå™¨äººæ“ä½œæä¾›ç²¾ç»†æŒ‡å¯¼ï¼Œè§£å†³æœªè§åœºæ™¯å’Œæ–°ä»»åŠ¡çš„é€šç”¨åŒ–æŒ‘æˆ˜ã€‚åœ¨å…«ä¸ªé€šç”¨ç©ºé—´æ¨ç†å’Œå…·èº«å‚è€ƒèƒ½åŠ›åŸºå‡†æµ‹è¯•ä¸­ï¼Œä»¥åŠæ›´å…·æŒ‘æˆ˜æ€§çš„VABenchåŸºå‡†æµ‹è¯•ä¸­ï¼ŒFSDè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚åœ¨SimplerEnvç¯å¢ƒä¸­ï¼ŒFSDæˆåŠŸç‡ä¸º54.1%ï¼Œå¹¶åœ¨çœŸå®æœºå™¨äººä»»åŠ¡çš„å…«ä¸ªåœºæ™¯ä¸­å®ç°72%çš„æˆåŠŸç‡ï¼Œæ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®ç°æœºå™¨äººåœ¨æœªè§åœºæ™¯å’Œæ–°ä»»åŠ¡ä¸­çš„é€šç”¨åŒ–æ“ä½œæ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰Vision-Language-Actionï¼ˆVLAï¼‰æ¨¡å‹åœ¨è§£å†³æ­¤ç±»æŒ‘æˆ˜æ—¶ä»å­˜åœ¨å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨é›¶æ ·æœ¬æ€§èƒ½ä¸Šã€‚</li>
<li>FSDæ¨¡å‹é€šè¿‡ç©ºé—´å…³ç³»æ¨ç†ç”Ÿæˆä¸­é—´è¡¨ç¤ºï¼Œä¸ºæœºå™¨äººæ“ä½œæä¾›ç²¾ç»†æŒ‡å¯¼ã€‚</li>
<li>FSDé‡‡ç”¨åˆ†å±‚æ•°æ®ç®¡é“å’Œè‡ªä¸€è‡´æ€§æœºåˆ¶æ¥è®­ç»ƒæ¨¡å‹ï¼Œå®ç°äº†è§†è§‰ä¿¡å·çš„ç©ºé—´åæ ‡å¯¹é½ã€‚</li>
<li>FSDåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬é€šç”¨ç©ºé—´æ¨ç†å’Œå…·èº«å‚è€ƒèƒ½åŠ›æµ‹è¯•ã€‚</li>
<li>åœ¨SimplerEnvç¯å¢ƒä¸­ï¼ŒFSDæˆåŠŸç‡ä¸º54.1%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08548">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a309dac19b70c491d1a0cb7ff5704281.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8ea8f4500eab0b8ab7be88961f5a393.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ecdf734fc28a7865e218a16ca3831019.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-538366f168afd5bd54aad0707ba8a6a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e74435e0d59b6cc9d741e16865d8fd8a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Judging-the-Judges-Can-Large-Vision-Language-Models-Fairly-Evaluate-Chart-Comprehension-and-Reasoning"><a href="#Judging-the-Judges-Can-Large-Vision-Language-Models-Fairly-Evaluate-Chart-Comprehension-and-Reasoning" class="headerlink" title="Judging the Judges: Can Large Vision-Language Models Fairly Evaluate   Chart Comprehension and Reasoning?"></a>Judging the Judges: Can Large Vision-Language Models Fairly Evaluate   Chart Comprehension and Reasoning?</h2><p><strong>Authors:Md Tahmid Rahman Laskar, Mohammed Saidul Islam, Ridwan Mahbub, Ahmed Masry, Mizanur Rahman, Amran Bhuiyan, Mir Tafseer Nayeem, Shafiq Joty, Enamul Hoque, Jimmy Huang</strong></p>
<p>Charts are ubiquitous as they help people understand and reason with data. Recently, various downstream tasks, such as chart question answering, chart2text, and fact-checking, have emerged. Large Vision-Language Models (LVLMs) show promise in tackling these tasks, but their evaluation is costly and time-consuming, limiting real-world deployment. While using LVLMs as judges to assess the chart comprehension capabilities of other LVLMs could streamline evaluation processes, challenges like proprietary datasets, restricted access to powerful models, and evaluation costs hinder their adoption in industrial settings. To this end, we present a comprehensive evaluation of 13 open-source LVLMs as judges for diverse chart comprehension and reasoning tasks. We design both pairwise and pointwise evaluation tasks covering criteria like factual correctness, informativeness, and relevancy. Additionally, we analyze LVLM judges based on format adherence, positional consistency, length bias, and instruction-following. We focus on cost-effective LVLMs (&lt;10B parameters) suitable for both research and commercial use, following a standardized evaluation protocol and rubric to measure the LVLM judgeâ€™s accuracy. Experimental results reveal notable variability: while some open LVLM judges achieve GPT-4-level evaluation performance (about 80% agreement with GPT-4 judgments), others struggle (below ~10% agreement). Our findings highlight that state-of-the-art open-source LVLMs can serve as cost-effective automatic evaluators for chart-related tasks, though biases such as positional preference and length bias persist. </p>
<blockquote>
<p>å›¾è¡¨æ— å¤„ä¸åœ¨ï¼Œå› ä¸ºå®ƒä»¬æœ‰åŠ©äºäººä»¬ç†è§£å’Œåˆ†ææ•°æ®ã€‚æœ€è¿‘ï¼Œå‡ºç°äº†å„ç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚å›¾è¡¨é—®ç­”ã€å›¾è¡¨è½¬æ–‡æœ¬å’ŒæŸ¥è¯ç­‰ã€‚å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨åº”å¯¹è¿™äº›ä»»åŠ¡æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å…¶è¯„ä¼°æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²ã€‚è™½ç„¶ä½¿ç”¨LVLMsä½œä¸ºåˆ¤æ–­è€…å¯¹å…¶ä»–LVLMsçš„å›¾è¡¨ç†è§£èƒ½åŠ›è¿›è¡Œè¯„ä¼°å¯ä»¥ç®€åŒ–è¯„ä¼°æµç¨‹ï¼Œä½†ä¸“æœ‰æ•°æ®é›†ã€å¯¹å¼ºå¤§æ¨¡å‹çš„è®¿é—®å—é™ä»¥åŠè¯„ä¼°æˆæœ¬ç­‰æŒ‘æˆ˜é˜»ç¢äº†å…¶åœ¨å·¥ä¸šç¯å¢ƒä¸­çš„é‡‡ç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¹13ä¸ªå¼€æºLVLMsä½œä¸ºå¤šç§å›¾è¡¨ç†è§£å’Œæ¨ç†ä»»åŠ¡çš„åˆ¤æ–­è€…è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬è®¾è®¡äº†æˆå¯¹çš„å’Œä¸ªåˆ«çš„è¯„ä¼°ä»»åŠ¡ï¼Œæ¶µç›–äº‹å®æ­£ç¡®æ€§ã€ä¿¡æ¯ä¸°å¯Œæ€§å’Œç›¸å…³æ€§ç­‰æ ‡å‡†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ ¹æ®æ ¼å¼éµå®ˆã€ä½ç½®ä¸€è‡´æ€§ã€é•¿åº¦åå·®å’ŒæŒ‡ä»¤éµå¾ªç­‰æ ‡å‡†å¯¹LVLMåˆ¤æ–­è€…è¿›è¡Œäº†åˆ†æã€‚æˆ‘ä»¬ä¸“æ³¨äºæˆæœ¬æ•ˆç›Šé«˜çš„LVLMsï¼ˆå‚æ•°å°äº10Bï¼‰ï¼Œé€‚åˆç ”ç©¶å’Œå•†ä¸šä½¿ç”¨ï¼Œå¹¶æŒ‰ç…§æ ‡å‡†åŒ–çš„è¯„ä¼°åè®®å’Œè¯„åˆ†è¡¨æ¥è¡¡é‡LVLMåˆ¤æ–­è€…çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼šä¸€äº›å¼€æºLVLMåˆ¤æ–­è€…è¾¾åˆ°äº†GPT-4çº§çš„è¯„ä¼°æ€§èƒ½ï¼ˆä¸GPT-4åˆ¤æ–­çš„å…±è¯†çº¦ä¸º80%ï¼‰ï¼Œè€Œå…¶ä»–åˆ¤æ–­è€…åˆ™è¡¨ç°æŒ£æ‰ï¼ˆå…±è¯†ä½äºçº¦10%ï¼‰ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„å¼€æºLVLMså¯ä»¥ä½œä¸ºæˆæœ¬æ•ˆç›Šé«˜çš„å›¾è¡¨ç›¸å…³ä»»åŠ¡çš„è‡ªåŠ¨è¯„ä¼°è€…ï¼Œå°½ç®¡ä»ç„¶å­˜åœ¨ä½ç½®åå¥½å’Œé•¿åº¦åå·®ç­‰åè§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08468v1">PDF</a> Accepted at ACL 2025 Industry Track</p>
<p><strong>Summary</strong>ï¼š<br>è¿‘æœŸå‡ºç°è®¸å¤šå›¾è¡¨ç›¸å…³çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚å›¾è¡¨é—®ç­”ã€å›¾è¡¨è½¬æ–‡æœ¬å’ŒæŸ¥æ ¸äº‹å®ç­‰ã€‚å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è¿™äº›ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†è¯„ä¼°æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§ä½¿ç”¨å¼€æºLVLMsä½œä¸ºè¯„åˆ¤è€…å¯¹å›¾è¡¨ç†è§£ä»»åŠ¡è¿›è¡Œè¯„ä¼°çš„å…¨é¢è¯„ä¼°æ–¹æ³•ï¼ŒåŒ…æ‹¬æˆå¯¹å’Œç‚¹çŠ¶çš„è¯„ä¼°ä»»åŠ¡ï¼Œå¹¶åŸºäºäº‹å®æ­£ç¡®æ€§ã€ä¿¡æ¯ä¸°å¯Œæ€§å’Œç›¸å…³æ€§ç­‰æ ‡å‡†è¿›è¡Œè¯„ä»·ã€‚ç ”ç©¶å…³æ³¨æˆæœ¬æ•ˆç›Šé«˜çš„LVLMsï¼Œé€‚åˆç§‘ç ”å’Œå•†ä¸šä½¿ç”¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸€äº›å¼€æºLVLMè¯„åˆ¤è€…çš„è¯„ä¼°æ€§èƒ½è¾¾åˆ°äº†GPT-4æ°´å¹³ï¼Œä½†ä¹Ÿå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè™½ç„¶å­˜åœ¨ä½ç½®åå¥½å’Œé•¿åº¦åè§ç­‰åè§ï¼Œä½†å…ˆè¿›å¼€æºLVLMså¯ä»¥ä½œä¸ºå›¾è¡¨ç›¸å…³ä»»åŠ¡çš„ä½æˆæœ¬è‡ªåŠ¨è¯„ä¼°å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å›¾è¡¨ç†è§£ä»»åŠ¡æ—¥ç›Šå—åˆ°å…³æ³¨ï¼ŒåŒ…æ‹¬å›¾è¡¨é—®ç­”ã€å›¾è¡¨è½¬æ–‡æœ¬å’ŒæŸ¥æ ¸äº‹å®ç­‰ã€‚</li>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨å›¾è¡¨ç†è§£ä»»åŠ¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†è¯„ä¼°å›°éš¾ä¸”æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>æå‡ºä½¿ç”¨å¼€æºLVLMsä½œä¸ºè¯„åˆ¤è€…å¯¹å›¾è¡¨ç†è§£ä»»åŠ¡è¿›è¡Œç»¼åˆè¯„ä»·çš„æ–¹æ³•ã€‚</li>
<li>è¯„ä¼°ä»»åŠ¡åŒ…æ‹¬æˆå¯¹å’Œç‚¹çŠ¶çš„è¯„ä¼°ä»»åŠ¡ï¼Œå¹¶åŸºäºäº‹å®æ­£ç¡®æ€§ã€ä¿¡æ¯ä¸°å¯Œæ€§å’Œç›¸å…³æ€§ç­‰æ ‡å‡†è¿›è¡Œè¯„ä»·ã€‚</li>
<li>å…³æ³¨æˆæœ¬æ•ˆç›Šé«˜çš„LVLMsï¼Œé€‚åˆç§‘ç ”å’Œå•†ä¸šä½¿ç”¨ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒæŸäº›å¼€æºLVLMè¯„åˆ¤è€…æ€§èƒ½æ¥è¿‘GPT-4ï¼Œä½†ä¹Ÿå­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08468">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4763bcbcb0ef48de048cc586ef99209a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2d9f0b8be33d75096944e72a9d1bcad5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c56b9fcff386ea5c74c8cd6d984958c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af29e46b681e8325e6051b3b91bee87c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-470dc5815fd1e315ce85d69c02b407c9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VCRBench-Exploring-Long-form-Causal-Reasoning-Capabilities-of-Large-Video-Language-Models"><a href="#VCRBench-Exploring-Long-form-Causal-Reasoning-Capabilities-of-Large-Video-Language-Models" class="headerlink" title="VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large   Video Language Models"></a>VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large   Video Language Models</h2><p><strong>Authors:Pritam Sarkar, Ali Etemad</strong></p>
<p>Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for evaluating causal reasoning in visually grounded and goal-driven settings. To fill this gap, we introduce a novel benchmark named Video-based long-form Causal Reasoning (VCRBench). We create VCRBench using procedural videos of simple everyday activities, where the steps are deliberately shuffled with each clip capturing a key causal event, to test whether LVLMs can identify, reason about, and correctly sequence the events needed to accomplish a specific goal. Moreover, the benchmark is carefully designed to prevent LVLMs from exploiting linguistic shortcuts, as seen in multiple-choice or binary QA formats, while also avoiding the challenges associated with evaluating open-ended QA. Our evaluation of state-of-the-art LVLMs on VCRBench suggests that these models struggle with video-based long-form causal reasoning, primarily due to their difficulty in modeling long-range causal dependencies directly from visual observations. As a simple step toward enabling such capabilities, we propose Recognition-Reasoning Decomposition (RRD), a modular approach that breaks video-based causal reasoning into two sub-tasks of video recognition and causal reasoning. Our experiments on VCRBench show that RRD significantly boosts accuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis reveals interesting insights, for instance, that LVLMs primarily rely on language knowledge for complex video-based long-form causal reasoning tasks. </p>
<blockquote>
<p>å°½ç®¡è§†é¢‘ç†è§£é¢†åŸŸæœ€è¿‘æœ‰æ‰€è¿›å±•ï¼Œä½†å¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨åŸºäºè§†é¢‘å› æœå…³ç³»æ¨ç†æ–¹é¢çš„èƒ½åŠ›ä»é²œä¸ºäººçŸ¥ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºç¼ºä¹ç”¨äºè¯„ä¼°è§†è§‰åŒ–åŠç›®æ ‡é©±åŠ¨ç¯å¢ƒä¸­å› æœæ¨ç†èƒ½åŠ›çš„ç›¸å…³åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸€ç§åä¸ºVideo-basedé•¿å½¢å¼å› æœæ¨ç†ï¼ˆVCRBenchï¼‰çš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬ä½¿ç”¨æ—¥å¸¸ç®€å•æ´»åŠ¨çš„ç¨‹åºåŒ–è§†é¢‘æ¥åˆ›å»ºVCRBenchï¼Œå…¶ä¸­æ­¥éª¤è¢«æ•…æ„æ‰“ä¹±ï¼Œæ¯ä¸ªç‰‡æ®µéƒ½æ•æ‰åˆ°ä¸€ä¸ªå…³é”®çš„å› æœäº‹ä»¶ï¼Œä»¥æµ‹è¯•LVLMsæ˜¯å¦èƒ½å¤Ÿè¯†åˆ«ã€æ¨ç†å’Œæ­£ç¡®æ’åºå®Œæˆç‰¹å®šç›®æ ‡æ‰€éœ€çš„äº‹ä»¶ã€‚æ­¤å¤–ï¼Œè¯¥åŸºå‡†æµ‹è¯•ç²¾å¿ƒè®¾è®¡ï¼Œæ—¨åœ¨é˜²æ­¢LVLMsåˆ©ç”¨å¤šé€‰æˆ–äºŒå…ƒé—®ç­”æ ¼å¼ä¸­çš„è¯­è¨€æ·å¾„ï¼ŒåŒæ—¶é¿å…ä¸å¼€æ”¾å¼é—®ç­”è¯„ä¼°ç›¸å…³çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¯¹å…ˆè¿›LVLMsåœ¨VCRBenchä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¿™äº›æ¨¡å‹åœ¨åŸºäºè§†é¢‘çš„é•¿æœŸå› æœæ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬éš¾ä»¥ç›´æ¥ä»è§†è§‰è§‚å¯Ÿä¸­å»ºç«‹é•¿æœŸå› æœå…³ç³»ä¾èµ–ã€‚ä½œä¸ºæœç€å…·å¤‡è¿™ç§èƒ½åŠ›çš„ç®€å•æ­¥éª¤ï¼Œæˆ‘ä»¬æå‡ºäº†è¯†åˆ«æ¨ç†åˆ†è§£ï¼ˆRRDï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ¨¡å—åŒ–æ–¹æ³•ï¼Œå°†åŸºäºè§†é¢‘çš„å› æœæ¨ç†åˆ†ä¸ºè§†é¢‘è¯†åˆ«å’Œå› æœæ¨ç†ä¸¤ä¸ªå­ä»»åŠ¡ã€‚æˆ‘ä»¬åœ¨VCRBenchä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRRDåœ¨VCRBenchä¸Šçš„å‡†ç¡®ç‡æœ‰äº†æ˜¾è‘—æé«˜ï¼Œæé«˜äº†é«˜è¾¾25.2%ã€‚æœ€åï¼Œæˆ‘ä»¬çš„æ·±å…¥åˆ†ææ­ç¤ºäº†ä¸€äº›æœ‰è¶£è§è§£ï¼Œä¾‹å¦‚LVLMsä¸»è¦ä¾èµ–è¯­è¨€çŸ¥è¯†è¿›è¡Œå¤æ‚çš„åŸºäºè§†é¢‘çš„é•¿æœŸå› æœæ¨ç†ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08455v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€é¡¹æ–°çš„è§†é¢‘åŸºå‡†æµ‹è¯•â€”â€”åŸºäºè§†é¢‘çš„é•¿å½¢å¼å› æœæ¨ç†åŸºå‡†æµ‹è¯•ï¼ˆVCRBenchï¼‰ï¼Œä»¥è¯„ä¼°å¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†è§‰åŒ–åœºæ™¯ä¸­çš„å› æœæ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•é€šè¿‡æ‰“ä¹±æ—¥å¸¸æ´»åŠ¨çš„æ­¥éª¤é¡ºåºåˆ¶ä½œç¨‹åºåŒ–è§†é¢‘ï¼Œæ¯ä¸ªè§†é¢‘ç‰‡æ®µæ•æ‰å…³é”®å› æœäº‹ä»¶ï¼Œä»¥æµ‹è¯•LVLMsæ˜¯å¦å…·å¤‡è¯†åˆ«ã€æ¨ç†å’Œæ­£ç¡®æ’åºå®Œæˆç‰¹å®šç›®æ ‡æ‰€éœ€çš„äº‹ä»¶çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥åŸºå‡†æµ‹è¯•æ—¨åœ¨é¿å…LVLMsåˆ©ç”¨è¯­è¨€æ·å¾„å’Œåœ¨å¼€æ”¾å¼é—®ç­”è¯„ä¼°ä¸­å‡ºç°çš„æŒ‘æˆ˜ã€‚å¯¹å…ˆè¿›LVLMsçš„è¯„ä¼°è¡¨æ˜ï¼Œå®ƒä»¬åœ¨åŸºäºè§†é¢‘çš„é•¿å½¢å¼å› æœæ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä¸»è¦éš¾ä»¥ç›´æ¥ä»è§†è§‰è§‚å¯Ÿä¸­å»ºç«‹é•¿æœŸå› æœå…³ç³»ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„è§£å†³æ–¹æ¡ˆâ€”â€”è¯†åˆ«æ¨ç†åˆ†è§£ï¼ˆRRDï¼‰ï¼Œå°†è§†é¢‘åŸºäºçš„å› æœæ¨ç†åˆ†è§£ä¸ºè§†é¢‘è¯†åˆ«å’Œå› æœæ¨ç†ä¸¤ä¸ªå­ä»»åŠ¡ã€‚åœ¨VCRBenchä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRRDåœ¨VCRBenchä¸Šçš„å‡†ç¡®ç‡æ˜¾è‘—æé«˜ï¼Œæé«˜äº†é«˜è¾¾25.2%ã€‚æœ€åï¼Œæˆ‘ä»¬çš„æ·±å…¥åˆ†ææ­ç¤ºäº†æœ‰è¶£çš„è§è§£ï¼Œå¦‚LVLMsä¸»è¦ä¾èµ–è¯­è¨€çŸ¥è¯†è¿›è¡Œå¤æ‚è§†é¢‘é•¿å½¢å¼çš„å› æœæ¨ç†ä»»åŠ¡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—è¿›å±•ï¼Œä½†åœ¨åŸºäºè§†é¢‘çš„å› æœæ¨ç†æ–¹é¢çš„èƒ½åŠ›ä»å¾…æ¢ç´¢ã€‚</li>
<li>ç¼ºä¹ç”¨äºè¯„ä¼°è§†è§‰åŒ–ç¯å¢ƒä¸­ç›®æ ‡é©±åŠ¨è®¾ç½®çš„å› æœæ¨ç†çš„ç›¸å…³åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æ–°åŸºå‡†æµ‹è¯•VCRBenché€šè¿‡ä½¿ç”¨ç¨‹åºåŒ–è§†é¢‘æ¥æµ‹è¯•LVLMsçš„å› æœæ¨ç†èƒ½åŠ›ï¼Œè¿™äº›è§†é¢‘åŒ…å«æ—¥å¸¸æ´»åŠ¨çš„å…³é”®å› æœäº‹ä»¶ã€‚</li>
<li>VCRBenchè®¾è®¡æ—¨åœ¨é¿å…è¯­è¨€æ·å¾„å’Œå¼€æ”¾å¼é—®ç­”è¯„ä¼°çš„æŒ‘æˆ˜ã€‚</li>
<li>LVLMsåœ¨åŸºäºè§†é¢‘çš„é•¿å½¢å¼å› æœæ¨ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œéš¾ä»¥ç›´æ¥ä»è§†è§‰è§‚å¯Ÿä¸­å»ºç«‹é•¿æœŸå› æœå…³ç³»ã€‚</li>
<li>è¯†åˆ«æ¨ç†åˆ†è§£ï¼ˆRRDï¼‰æ˜¯ä¸€ç§æ¨¡å—åŒ–æ–¹æ³•ï¼Œå°†è§†é¢‘åŸºäºçš„å› æœæ¨ç†åˆ†è§£ä¸ºä¸¤ä¸ªå­ä»»åŠ¡ï¼Œå³è§†é¢‘è¯†åˆ«å’Œå› æœæ¨ç†ï¼Œæ˜¾è‘—æé«˜å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08455">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb482b8f92d1d115dc7b7103a5844231.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2f2f48fc69fdf363cfcb250a49fc731.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f9fb805e72683cffcf758897e7c8b92e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-520770a4566aa20f1885dd2a04840bd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a90e824467fc2375a914cc712318965.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bf0239dedcf6a0e1df86977bd947e4f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Scalable-UAV-Multi-Hop-Networking-via-Multi-Agent-Reinforcement-Learning-with-Large-Language-Models"><a href="#Scalable-UAV-Multi-Hop-Networking-via-Multi-Agent-Reinforcement-Learning-with-Large-Language-Models" class="headerlink" title="Scalable UAV Multi-Hop Networking via Multi-Agent Reinforcement Learning   with Large Language Models"></a>Scalable UAV Multi-Hop Networking via Multi-Agent Reinforcement Learning   with Large Language Models</h2><p><strong>Authors:Yanggang Xu, Weijie Hong, Jirong Zha, Geng Chen, Jianfeng Zheng, Chen-Chun Hsia, Xinlei Chen</strong></p>
<p>In disaster scenarios, establishing robust emergency communication networks is critical, and unmanned aerial vehicles (UAVs) offer a promising solution to rapidly restore connectivity. However, organizing UAVs to form multi-hop networks in large-scale dynamic environments presents significant challenges, including limitations in algorithmic scalability and the vast exploration space required for coordinated decision-making. To address these issues, we propose MRLMN, a novel framework that integrates multi-agent reinforcement learning (MARL) and large language models (LLMs) to jointly optimize UAV agents toward achieving optimal networking performance. The framework incorporates a grouping strategy with reward decomposition to enhance algorithmic scalability and balance decision-making across UAVs. In addition, behavioral constraints are applied to selected key UAVs to improve the robustness of the network. Furthermore, the framework integrates LLM agents, leveraging knowledge distillation to transfer their high-level decision-making capabilities to MARL agents. This enhances both the efficiency of exploration and the overall training process. In the distillation module, a Hungarian algorithm-based matching scheme is applied to align the decision outputs of the LLM and MARL agents and define the distillation loss. Extensive simulation results validate the effectiveness of our approach, demonstrating significant improvements in network performance, including enhanced coverage and communication quality. </p>
<blockquote>
<p>åœ¨ç¾éš¾åœºæ™¯ä¸­ï¼Œå»ºç«‹ç¨³å¥çš„ç´§æ€¥é€šä¿¡ç½‘ç»œè‡³å…³é‡è¦ï¼Œæ— äººæœºï¼ˆUAVsï¼‰ä¸ºå¿«é€Ÿæ¢å¤è¿é€šæ€§æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œåœ¨å¤§è§„æ¨¡åŠ¨æ€ç¯å¢ƒä¸­ç»„ç»‡æ— äººæœºå½¢æˆå¤šè·³ç½‘ç»œé¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç®—æ³•å¯æ‰©å±•æ€§çš„é™åˆ¶å’Œåè°ƒå†³ç­–æ‰€éœ€çš„å¤§é‡æ¢ç´¢ç©ºé—´ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MRLMNï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œå…±åŒä¼˜åŒ–æ— äººæœºä»£ç†ï¼Œä»¥å®ç°æœ€ä½³çš„ç½‘ç»œæ€§èƒ½ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åˆ†ç»„ç­–ç•¥å’Œå¥–åŠ±åˆ†è§£ï¼Œä»¥æé«˜ç®—æ³•çš„å¯æ‰©å±•æ€§ï¼Œå¹¶åœ¨æ— äººæœºä¹‹é—´å¹³è¡¡å†³ç­–ã€‚æ­¤å¤–ï¼Œå¯¹é€‰å®šçš„é‡è¦æ— äººæœºåº”ç”¨äº†è¡Œä¸ºçº¦æŸï¼Œä»¥æé«˜ç½‘ç»œçš„ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ•´åˆäº†LLMä»£ç†ï¼Œåˆ©ç”¨çŸ¥è¯†è’¸é¦è½¬ç§»å…¶é«˜çº§å†³ç­–èƒ½åŠ›ç»™MARLä»£ç†ã€‚è¿™æé«˜äº†æ¢ç´¢æ•ˆç‡å’Œæ•´ä½“è®­ç»ƒè¿‡ç¨‹ã€‚åœ¨è’¸é¦æ¨¡å—ä¸­ï¼Œåº”ç”¨åŸºäºåŒˆç‰™åˆ©ç®—æ³•çš„åŒ¹é…æ–¹æ¡ˆï¼Œä»¥å¯¹é½LLMå’ŒMARLä»£ç†çš„å†³ç­–è¾“å‡ºå¹¶å®šä¹‰è’¸é¦æŸå¤±ã€‚å¹¿æ³›çš„ä»¿çœŸç»“æœéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºç½‘ç»œæ€§èƒ½çš„æ˜¾è‘—æé«˜ï¼ŒåŒ…æ‹¬è¦†ç›–èŒƒå›´å’Œé€šä¿¡è´¨é‡çš„å¢å¼ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08448v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ— äººæœºåœ¨ç¾éš¾åœºæ™¯ä¸­å»ºç«‹åº”æ€¥é€šä¿¡ç½‘ç»œå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œåœ¨å¤§è§„æ¨¡åŠ¨æ€ç¯å¢ƒä¸­ç»„ç»‡æ— äººæœºå½¢æˆå¤šè·³ç½‘ç»œé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºMRLMNæ¡†æ¶ï¼Œç»“åˆå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä¼˜åŒ–æ— äººæœºä»¥å®ç°æœ€ä½³ç½‘ç»œæ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†ç»„ç­–ç•¥å’Œå¥–åŠ±åˆ†è§£æé«˜ç®—æ³•å¯æ‰©å±•æ€§ï¼Œå¹¶å¹³è¡¡æ— äººæœºé—´çš„å†³ç­–ã€‚æ­¤å¤–ï¼Œå¯¹å…³é”®æ— äººæœºçš„è¡Œä¸ºçº¦æŸæé«˜äº†ç½‘ç»œçš„ç¨³å¥æ€§ã€‚é›†æˆLLMæ™ºèƒ½ä½“ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦æå‡æ¢ç´¢æ•ˆç‡å’Œæ•´ä½“è®­ç»ƒè¿‡ç¨‹ã€‚ä»¿çœŸå®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆæé«˜ç½‘ç»œæ€§èƒ½ï¼ŒåŒ…æ‹¬è¦†ç›–èŒƒå›´å’Œé€šä¿¡è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åœ¨ç¾éš¾åœºæ™¯ä¸­ï¼Œå»ºç«‹æ— äººæœºåº”æ€¥é€šä¿¡ç½‘ç»œè‡³å…³é‡è¦ã€‚</li>
<li>ç»„ç»‡æ— äººæœºå½¢æˆå¤šè·³ç½‘ç»œé¢ä¸´ç®—æ³•å¯æ‰©å±•æ€§å’Œå†³ç­–åè°ƒçš„æŒ‘æˆ˜ã€‚</li>
<li>MRLMNæ¡†æ¶ç»“åˆMARLå’ŒLLMï¼Œä¼˜åŒ–æ— äººæœºä»¥å®ç°æœ€ä½³ç½‘ç»œæ€§èƒ½ã€‚</li>
<li>åˆ†ç»„ç­–ç•¥å’Œå¥–åŠ±åˆ†è§£æé«˜ç®—æ³•å¯æ‰©å±•æ€§ï¼Œå¹³è¡¡æ— äººæœºå†³ç­–ã€‚</li>
<li>å¯¹å…³é”®æ— äººæœºçš„è¡Œä¸ºçº¦æŸæé«˜ç½‘ç»œç¨³å¥æ€§ã€‚</li>
<li>LLMæ™ºèƒ½ä½“çš„é›†æˆé€šè¿‡çŸ¥è¯†è’¸é¦æå‡æ¢ç´¢æ•ˆç‡å’Œè®­ç»ƒè¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-527c88256cd7e73c01aaf21e2ff280f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17d80ef69447c72ce41fdaea9e7cbff0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Accelerating-Chain-of-Thought-Reasoning-When-Goal-Gradient-Importance-Meets-Dynamic-Skipping"><a href="#Accelerating-Chain-of-Thought-Reasoning-When-Goal-Gradient-Importance-Meets-Dynamic-Skipping" class="headerlink" title="Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance   Meets Dynamic Skipping"></a>Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance   Meets Dynamic Skipping</h2><p><strong>Authors:Ren Zhuang, Ben Wang, Shuifa Sun</strong></p>
<p>Large Language Models leverage Chain-of-Thought (CoT) prompting for complex tasks, but their reasoning traces are often excessively verbose and inefficient, leading to significant computational costs and latency. Current CoT compression techniques typically rely on generic importance metrics and static compression rates, which may inadvertently remove functionally critical tokens or fail to adapt to varying reasoning complexity. To overcome these limitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic CoT compression via supervised fine-tuning. This approach introduces two synergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric accurately identifying functionally relevant tokens by measuring the gradient influence of their intermediate representations on the final answer loss, and (2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the compression rate based on runtime model uncertainty while ensuring local coherence through an adaptive N-token constraint. To our knowledge, this is the first work unifying a goal-oriented, gradient-based importance metric with dynamic, uncertainty-aware skipping for CoT compression. Trained on compressed MATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization across diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It achieves substantial efficiency gains - reducing CoT token counts by over 45% on average and delivering 1.6-2.0 times inference speedups - while maintaining high reasoning accuracy. Notably, it significantly outperforms existing baselines by preserving accuracy even at high effective compression rates, advancing the state of the art in the CoT reasoning efficiency-accuracy trade-off. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åˆ©ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ¥å®Œæˆå¤æ‚ä»»åŠ¡ï¼Œä½†å®ƒä»¬çš„æ¨ç†è¿‡ç¨‹é€šå¸¸è¿‡äºå†—é•¿ä¸”æ•ˆç‡ä½ä¸‹ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬å¢åŠ å’Œå»¶è¿Ÿã€‚å½“å‰çš„CoTå‹ç¼©æŠ€æœ¯é€šå¸¸ä¾èµ–äºé€šç”¨é‡è¦æ€§æŒ‡æ ‡å’Œé™æ€å‹ç¼©ç‡ï¼Œè¿™å¯èƒ½ä¼šæ— æ„ä¸­åˆ é™¤åŠŸèƒ½å…³é”®ä»¤ç‰Œæˆ–æ— æ³•é€‚åº”ä¸åŒçš„æ¨ç†å¤æ‚æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”GoGI-Skipï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡ç›‘ç£å¾®è°ƒå­¦ä¹ åŠ¨æ€CoTå‹ç¼©çš„æ–°å‹æ¡†æ¶ã€‚è¿™ç§æ–¹æ³•å¼•å…¥äº†ä¸¤ç§ååŒåˆ›æ–°ï¼šï¼ˆ1ï¼‰ç›®æ ‡æ¢¯åº¦é‡è¦æ€§ï¼ˆGoGIï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æŒ‡æ ‡ï¼Œé€šè¿‡æµ‹é‡ä¸­é—´è¡¨ç¤ºå¯¹æœ€ç»ˆç­”æ¡ˆæŸå¤±çš„æ¢¯åº¦å½±å“æ¥å‡†ç¡®è¯†åˆ«åŠŸèƒ½ç›¸å…³ä»¤ç‰Œï¼›ï¼ˆ2ï¼‰è‡ªé€‚åº”åŠ¨æ€è·³è¿‡ï¼ˆADSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æœºåˆ¶ï¼Œæ ¹æ®è¿è¡Œæ—¶æ¨¡å‹çš„ä¸ç¡®å®šæ€§åŠ¨æ€è°ƒæ•´å‹ç¼©ç‡ï¼ŒåŒæ—¶é€šè¿‡è‡ªé€‚åº”Nä»¤ç‰Œçº¦æŸç¡®ä¿å±€éƒ¨è¿è´¯æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹å°†ç›®æ ‡å¯¼å‘ã€åŸºäºæ¢¯åº¦çš„é‡è¦æ€§æŒ‡æ ‡ä¸ç”¨äºCoTå‹ç¼©çš„åŠ¨æ€ã€ä¸ç¡®å®šæ€§æ„ŸçŸ¥è·³è¿‡ç›¸ç»“åˆçš„å·¥ä½œã€‚åœ¨å‹ç¼©MATHæ•°æ®ä¸Šè®­ç»ƒçš„Adaptive GoGI-Skipå±•ç¤ºäº†å¼ºå¤§çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œåœ¨å„ç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ï¼ŒåŒ…æ‹¬AIMEã€GPQAå’ŒGSM8Kã€‚å®ƒå®ç°äº†å®è´¨æ€§çš„æ•ˆç‡æå‡â€”â€”å¹³å‡å‡å°‘CoTä»¤ç‰Œè®¡æ•°è¶…è¿‡45%ï¼Œå¹¶æä¾›1.6-2.0å€çš„æ¨ç†é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒé«˜æ¨ç†å‡†ç¡®æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨ä¿æŒé«˜å‹ç¼©ç‡çš„åŒæ—¶ï¼Œä»èƒ½ä¿æŒå‡†ç¡®æ€§ï¼Œåœ¨æ¨ç†æ•ˆç‡ä¸å‡†ç¡®æ€§ä¹‹é—´å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08392v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹åˆ©ç”¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºè¿›è¡Œå¤æ‚ä»»åŠ¡ï¼Œä½†å…¶æ¨ç†è¿‡ç¨‹å¾€å¾€è¿‡äºå†—é•¿å’Œæ•ˆç‡ä½ä¸‹ï¼Œå¸¦æ¥é‡å¤§è®¡ç®—æˆæœ¬å’Œå»¶è¿Ÿã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†è‡ªé€‚åº”GoGI-Skipæ¡†æ¶ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒå®ç°åŠ¨æ€CoTå‹ç¼©ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ç›®æ ‡å¯¼å‘çš„æ¢¯åº¦é‡è¦æ€§ï¼ˆGoGIï¼‰å’Œè‡ªé€‚åº”åŠ¨æ€è·³è¿‡ï¼ˆADSï¼‰ä¸¤ä¸ªåˆ›æ–°æœºåˆ¶ï¼Œå‡†ç¡®è¯†åˆ«åŠŸèƒ½ç›¸å…³ç¬¦å·å¹¶åŠ¨æ€è°ƒæ•´å‹ç¼©ç‡ä»¥é€‚åº”è¿è¡Œæ—¶æ¨¡å‹çš„ä¸ç¡®å®šæ€§ã€‚è¯¥æ¡†æ¶åœ¨å‹ç¼©MATHæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨AIMEã€GPQAå’ŒGSM8Kç­‰å¤šæ ·åŒ–æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„è·¨åŸŸæ³›åŒ–èƒ½åŠ›ã€‚å®ƒå®ç°äº†æ˜¾è‘—çš„æ•ˆç‡æå‡ï¼Œå¹³å‡å‡å°‘CoTç¬¦å·è®¡æ•°è¶…è¿‡45%ï¼Œæä¾›1.6-2.0å€çš„æ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒé«˜æ¨ç†å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå¯¹äºå¤æ‚ä»»åŠ¡å¾ˆé‡è¦ï¼Œä½†å­˜åœ¨å†—é•¿å’Œæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚</li>
<li>å½“å‰CoTå‹ç¼©æŠ€æœ¯å¯èƒ½ç§»é™¤å…³é”®ç¬¦å·æˆ–æ— æ³•é€‚åº”ä¸åŒçš„æ¨ç†å¤æ‚æ€§ã€‚</li>
<li>æå‡ºäº†è‡ªé€‚åº”GoGI-Skipæ¡†æ¶ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒå®ç°åŠ¨æ€CoTå‹ç¼©ã€‚</li>
<li>æ¡†æ¶åŒ…å«ä¸¤ä¸ªåˆ›æ–°æœºåˆ¶ï¼šç›®æ ‡å¯¼å‘çš„æ¢¯åº¦é‡è¦æ€§ï¼ˆGoGIï¼‰å’Œè‡ªé€‚åº”åŠ¨æ€è·³è¿‡ï¼ˆADSï¼‰ã€‚</li>
<li>GoGIèƒ½å‡†ç¡®è¯†åˆ«åŠŸèƒ½ç›¸å…³ç¬¦å·ï¼Œè€ŒADSèƒ½åŠ¨æ€è°ƒæ•´å‹ç¼©ç‡ä»¥é€‚åº”æ¨¡å‹çš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å¤šç§æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œæ˜¾è‘—æ•ˆç‡æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08392">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ecbff0bc91bc400e61ab38b61dedbc7a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b302cbac24c8ca1d2f3425ee8646cdb1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59ab252b964a73558ab3eb990de81e8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-734a775bdd3195cfc418999183716de8.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Modeling-Unseen-Environments-with-Language-guided-Composable-Causal-Components-in-Reinforcement-Learning"><a href="#Modeling-Unseen-Environments-with-Language-guided-Composable-Causal-Components-in-Reinforcement-Learning" class="headerlink" title="Modeling Unseen Environments with Language-guided Composable Causal   Components in Reinforcement Learning"></a>Modeling Unseen Environments with Language-guided Composable Causal   Components in Reinforcement Learning</h2><p><strong>Authors:Xinyue Wang, Biwei Huang</strong></p>
<p>Generalization in reinforcement learning (RL) remains a significant challenge, especially when agents encounter novel environments with unseen dynamics. Drawing inspiration from human compositional reasoning â€“ where known components are reconfigured to handle new situations â€“ we introduce World Modeling with Compositional Causal Components (WM3C). This novel framework enhances RL generalization by learning and leveraging compositional causal components. Unlike previous approaches focusing on invariant representation learning or meta-learning, WM3C identifies and utilizes causal dynamics among composable elements, facilitating robust adaptation to new tasks. Our approach integrates language as a compositional modality to decompose the latent space into meaningful components and provides theoretical guarantees for their unique identification under mild assumptions. Our practical implementation uses a masked autoencoder with mutual information constraints and adaptive sparsity regularization to capture high-level semantic information and effectively disentangle transition dynamics. Experiments on numerical simulations and real-world robotic manipulation tasks demonstrate that WM3C significantly outperforms existing methods in identifying latent processes, improving policy learning, and generalizing to unseen tasks. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„æ³›åŒ–ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å½“æ™ºèƒ½ä½“é‡åˆ°å…·æœ‰æœªè§åŠ¨æ€æ€§çš„æ–°ç¯å¢ƒæ—¶ã€‚æˆ‘ä»¬ä»äººç±»çš„ç»„åˆæ¨ç†ä¸­æ±²å–çµæ„Ÿâ€”â€”å·²çŸ¥ç»„ä»¶é‡æ–°é…ç½®ä»¥åº”å¯¹æ–°æƒ…å†µâ€”â€”æˆ‘ä»¬å¼•å…¥äº†åŸºäºç»„åˆå› æœç»„ä»¶çš„ä¸–ç•Œå»ºæ¨¡ï¼ˆWM3Cï¼‰ã€‚è¿™ä¸€æ–°é¢–æ¡†æ¶é€šè¿‡å­¦ä¹ å’Œåˆ©ç”¨ç»„åˆå› æœç»„ä»¶æ¥å¢å¼ºRLçš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ä»¥å¾€ä¸“æ³¨äºä¸å˜è¡¨ç¤ºå­¦ä¹ æˆ–å…ƒå­¦ä¹ çš„æ–¹æ³•ä¸åŒï¼ŒWM3Cèƒ½å¤Ÿè¯†åˆ«å’Œåˆ©ç”¨å¯ç»„åˆå…ƒç´ ä¹‹é—´çš„å› æœåŠ¨æ€å…³ç³»ï¼Œä¿ƒè¿›å¯¹æ–°ä»»åŠ¡çš„ç¨³å¥é€‚åº”ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†è¯­è¨€ä½œä¸ºä¸€ç§ç»„åˆæ¨¡å¼è¿›è¡Œé›†æˆï¼Œå°†æ½œåœ¨ç©ºé—´åˆ†è§£æˆæœ‰æ„ä¹‰çš„ç»„ä»¶ï¼Œå¹¶åœ¨æ¸©å’Œçš„å‡è®¾ä¸‹ä¸ºå®ƒä»¬çš„å”¯ä¸€è¯†åˆ«æä¾›ç†è®ºä¿è¯ã€‚æˆ‘ä»¬çš„å®é™…åº”ç”¨ä½¿ç”¨å¸¦æœ‰äº’ä¿¡æ¯çº¦æŸå’Œè‡ªé€‚åº”ç¨€ç–æ­£åˆ™åŒ–çš„æ©ç è‡ªåŠ¨ç¼–ç å™¨æ¥æ•è·é«˜çº§è¯­ä¹‰ä¿¡æ¯å¹¶æœ‰æ•ˆåœ°è§£å¼€è½¬æ¢åŠ¨æ€ã€‚åœ¨æ•°å€¼æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œæœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒWM3Cåœ¨è¯†åˆ«æ½œåœ¨è¿‡ç¨‹ã€æ”¹è¿›ç­–ç•¥å­¦ä¹ å’Œæ³›åŒ–åˆ°æœªè§ä»»åŠ¡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08361v1">PDF</a> Published as a conference paper at ICLR 2025</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä¸­çš„æ³›åŒ–æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ä»£ç†é‡åˆ°å…·æœ‰æœªè§åŠ¨æ€æ€§çš„æ–°ç¯å¢ƒæ—¶ã€‚æˆ‘ä»¬å—åˆ°äººç±»ç»„åˆæ¨ç†çš„å¯å‘ï¼Œå¼•å…¥äº†ç»„åˆå› æœç»„ä»¶çš„ä¸–ç•Œå»ºæ¨¡ï¼ˆWM3Cï¼‰è¿™ä¸€æ–°é¢–æ¡†æ¶ï¼Œä»¥æé«˜å¼ºåŒ–å­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›ã€‚WM3Cé€šè¿‡å­¦ä¹ å’Œåˆ©ç”¨ç»„åˆå› æœç»„ä»¶æ¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œä¸åŒäºä¸“æ³¨äºä¸å˜è¡¨ç¤ºå­¦ä¹ æˆ–å…ƒå­¦ä¹ çš„æ–¹æ³•ï¼ŒWM3Cèƒ½å¤Ÿè¯†åˆ«å’Œåˆ©ç”¨å¯ç»„åˆå…ƒç´ ä¹‹é—´çš„å› æœåŠ¨æ€å…³ç³»ï¼Œä¿ƒè¿›å¯¹æ–°ä»»åŠ¡çš„ç¨³å¥é€‚åº”ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å°†è¯­è¨€ä½œä¸ºç»„åˆæ¨¡å¼æ¥åˆ†è§£æ½œåœ¨ç©ºé—´ï¼Œå¹¶åœ¨æ¸©å’Œå‡è®¾ä¸‹ä¸ºå®ƒä»¬çš„å”¯ä¸€è¯†åˆ«æä¾›ç†è®ºä¿è¯ã€‚é€šè¿‡å¸¦æœ‰äº’ä¿¡æ¯çº¦æŸçš„è‡ªé€‚åº”ç¨€ç–æ­£åˆ™åŒ–çš„æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼Œæˆ‘ä»¬çš„å®é™…å®ç°å¯ä»¥æ•è·é«˜çº§è¯­ä¹‰ä¿¡æ¯å¹¶æœ‰æ•ˆåœ°è§£å¼€è½¬ç§»åŠ¨æ€ã€‚æ•°å€¼æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œæœºå™¨äººæ“ä½œä»»åŠ¡çš„å®éªŒè¡¨æ˜ï¼ŒWM3Cåœ¨è¯†åˆ«æ½œåœ¨è¿‡ç¨‹ã€æ”¹è¿›ç­–ç•¥å­¦ä¹ å’Œæ³›åŒ–åˆ°æœªè§ä»»åŠ¡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¸­çš„æ³›åŒ–é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢å¯¹æ–°ç¯å¢ƒæ—¶ã€‚</li>
<li>WM3Cæ¡†æ¶é€šè¿‡å­¦ä¹ å’Œåˆ©ç”¨ç»„åˆå› æœç»„ä»¶æé«˜å¼ºåŒ–å­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>WM3Cä¸åŒäºä¼ ç»Ÿæ–¹æ³•ï¼Œèƒ½è¯†åˆ«å’Œé€‚åº”å¯ç»„åˆå…ƒç´ é—´çš„å› æœåŠ¨æ€å…³ç³»ã€‚</li>
<li>WM3Cåˆ©ç”¨è¯­è¨€ä½œä¸ºåˆ†è§£æ½œåœ¨ç©ºé—´çš„ç»„åˆæ¨¡å¼ï¼Œæä¾›ç†è®ºä¿è¯ã€‚</li>
<li>æ©ç è‡ªåŠ¨ç¼–ç å™¨ç»“åˆäº’ä¿¡æ¯çº¦æŸå’Œè‡ªé€‚åº”ç¨€ç–æ­£åˆ™åŒ–ç”¨äºæ•æ‰é«˜çº§è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>WM3Cåœ¨è¯†åˆ«æ½œåœ¨è¿‡ç¨‹ã€ç­–ç•¥å­¦ä¹ å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08361">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-313b0d3d3d6d7639668f2c6fd17f1d2e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-80be6d42525446817f8eca40d30bba68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04ef9809bb185390ae0d7f9d6f827323.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="An-Identifiable-Cost-Aware-Causal-Decision-Making-Framework-Using-Counterfactual-Reasoning"><a href="#An-Identifiable-Cost-Aware-Causal-Decision-Making-Framework-Using-Counterfactual-Reasoning" class="headerlink" title="An Identifiable Cost-Aware Causal Decision-Making Framework Using   Counterfactual Reasoning"></a>An Identifiable Cost-Aware Causal Decision-Making Framework Using   Counterfactual Reasoning</h2><p><strong>Authors:Ruichu Cai, Xi Chen, Jie Qiao, Zijian Li, Yuequn Liu, Wei Chen, Keli Zhang, Jiale Zheng</strong></p>
<p>Decision making under abnormal conditions is a critical process that involves evaluating the current state and determining the optimal action to restore the system to a normal state at an acceptable cost. However, in such scenarios, existing decision-making frameworks highly rely on reinforcement learning or root cause analysis, resulting in them frequently neglecting the cost of the actions or failing to incorporate causal mechanisms adequately. By relaxing the existing causal decision framework to solve the necessary cause, we propose a minimum-cost causal decision (MiCCD) framework via counterfactual reasoning to address the above challenges. Emphasis is placed on making counterfactual reasoning processes identifiable in the presence of a large amount of mixed anomaly data, as well as finding the optimal intervention state in a continuous decision space. Specifically, it formulates a surrogate model based on causal graphs, using abnormal pattern clustering labels as supervisory signals. This enables the approximation of the structural causal model among the variables and lays a foundation for identifiable counterfactual reasoning. With the causal structure approximated, we then established an optimization model based on counterfactual estimation. The Sequential Least Squares Programming (SLSQP) algorithm is further employed to optimize intervention strategies while taking costs into account. Experimental evaluations on both synthetic and real-world datasets reveal that MiCCD outperforms conventional methods across multiple metrics, including F1-score, cost efficiency, and ranking quality(nDCG@k values), thus validating its efficacy and broad applicability. </p>
<blockquote>
<p>åœ¨å¼‚å¸¸æ¡ä»¶ä¸‹çš„å†³ç­–åˆ¶å®šæ˜¯ä¸€ä¸ªå…³é”®è¿‡ç¨‹ï¼Œéœ€è¦è¯„ä¼°å½“å‰çŠ¶æ€ï¼Œå¹¶ä»¥å¯æ¥å—çš„æˆæœ¬ç¡®å®šå°†ç³»ç»Ÿæ¢å¤åˆ°æ­£å¸¸çŠ¶æ€çš„æœ€ä½³è¡ŒåŠ¨ã€‚ç„¶è€Œï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç°æœ‰çš„å†³ç­–æ¡†æ¶é«˜åº¦ä¾èµ–äºå¼ºåŒ–å­¦ä¹ æˆ–æ ¹æœ¬åŸå› åˆ†æï¼Œå¯¼è‡´å®ƒä»¬ç»å¸¸å¿½è§†è¡ŒåŠ¨çš„æˆæœ¬æˆ–æœªèƒ½å……åˆ†èå…¥å› æœæœºåˆ¶ã€‚æˆ‘ä»¬é€šè¿‡è§£å†³å¿…è¦çš„å› æœå…³ç³»æ¥æ”¾æ¾ç°æœ‰çš„å› æœå†³ç­–æ¡†æ¶ï¼Œæå‡ºäº†ä¸€ç§åŸºäºåäº‹å®æ¨ç†çš„æœ€ä½æˆæœ¬å› æœå†³ç­–ï¼ˆMiCCDï¼‰æ¡†æ¶ï¼Œä»¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ã€‚é‡ç‚¹æ”¾åœ¨äº†åœ¨å¤§é‡çš„æ··åˆå¼‚å¸¸æ•°æ®å­˜åœ¨çš„æƒ…å†µä¸‹ï¼Œä½¿åäº‹å®æ¨ç†è¿‡ç¨‹å¯è¯†åˆ«ï¼Œä»¥åŠåœ¨è¿ç»­å†³ç­–ç©ºé—´ä¸­æ‰¾åˆ°æœ€ä½³å¹²é¢„çŠ¶æ€ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåŸºäºå› æœå›¾å»ºç«‹äº†ä¸€ä¸ªæ›¿ä»£æ¨¡å‹ï¼Œä½¿ç”¨å¼‚å¸¸æ¨¡å¼èšç±»æ ‡ç­¾ä½œä¸ºç›‘ç£ä¿¡å·ã€‚è¿™èƒ½å¤Ÿå®ç°å˜é‡é—´ç»“æ„å› æœæ¨¡å‹çš„è¿‘ä¼¼ï¼Œä¸ºå¯è¯†åˆ«çš„åäº‹å®æ¨ç†å¥ å®šäº†åŸºç¡€ã€‚åœ¨è¿‘ä¼¼äº†å› æœç»“æ„åï¼Œæˆ‘ä»¬å»ºç«‹äº†åŸºäºåäº‹å®ä¼°è®¡çš„ä¼˜åŒ–æ¨¡å‹ã€‚è¿›ä¸€æ­¥é‡‡ç”¨åºè´¯æœ€å°äºŒä¹˜è§„åˆ’ï¼ˆSLSQPï¼‰ç®—æ³•æ¥ä¼˜åŒ–å¹²é¢„ç­–ç•¥ï¼ŒåŒæ—¶è€ƒè™‘åˆ°æˆæœ¬å› ç´ ã€‚åœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒMiCCDåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼ŒåŒ…æ‹¬F1å¾—åˆ†ã€æˆæœ¬æ•ˆç‡å’Œæ’åè´¨é‡ï¼ˆnDCG@kå€¼ï¼‰ï¼Œä»è€ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå¹¿æ³›çš„é€‚ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08343v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>åœ¨å¼‚å¸¸æ¡ä»¶ä¸‹è¿›è¡Œå†³ç­–æ˜¯ä¸€ä¸ªæ¶‰åŠè¯„ä¼°å½“å‰çŠ¶æ€å¹¶ç¡®å®šä»¥å¯æ¥å—çš„ä»£ä»·å°†ç³»ç»Ÿæ¢å¤åˆ°æ­£å¸¸çŠ¶æ€çš„æœ€ä¼˜è¡ŒåŠ¨çš„è¿‡ç¨‹ã€‚é’ˆå¯¹ç°æœ‰å†³ç­–æ¡†æ¶åœ¨å¼‚å¸¸æƒ…å†µä¸‹å¸¸å¿½ç•¥è¡ŒåŠ¨æˆæœ¬æˆ–æœªèƒ½å……åˆ†èå…¥å› æœæœºåˆ¶çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºåäº‹å®æ¨ç†çš„æœ€ä½æˆæœ¬å› æœå†³ç­–ï¼ˆMiCCDï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºåŸºäºå› æœå›¾çš„æ›¿ä»£æ¨¡å‹ï¼Œä½¿ç”¨å¼‚å¸¸æ¨¡å¼èšç±»æ ‡ç­¾ä½œä¸ºç›‘ç£ä¿¡å·ï¼Œä»¥è¿‘ä¼¼å˜é‡é—´çš„ç»“æ„å› æœæ¨¡å‹ï¼Œå¹¶ä¸ºåäº‹å®æ¨ç†çš„è¯†åˆ«å¥ å®šåŸºç¡€ã€‚åœ¨ä¼˜åŒ–æ¨¡å‹æ–¹é¢ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†è€ƒè™‘æˆæœ¬çš„åäº‹å®ä¼°è®¡æ–¹æ³•ï¼Œå¹¶åˆ©ç”¨åºåˆ—æœ€å°äºŒä¹˜è§„åˆ’ï¼ˆSLSQPï¼‰ç®—æ³•ä¼˜åŒ–å¹²é¢„ç­–ç•¥ã€‚åœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒMiCCDåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼ŒåŒ…æ‹¬F1åˆ†æ•°ã€æˆæœ¬æ•ˆç‡å’Œæ’åè´¨é‡ï¼ˆnDCG@kå€¼ï¼‰ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå¹¿æ³›çš„åº”ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å†³ç­–åˆ¶å®šåœ¨å¼‚å¸¸æ¡ä»¶ä¸‹è‡³å…³é‡è¦ï¼Œæ¶‰åŠè¯„ä¼°å½“å‰çŠ¶æ€å¹¶ç¡®å®šæ¢å¤ç³»ç»Ÿçš„æœ€ä¼˜è¡ŒåŠ¨ã€‚</li>
<li>ç°æœ‰å†³ç­–æ¡†æ¶åœ¨å¼‚å¸¸æƒ…å†µä¸‹å¸¸å¿½ç•¥è¡ŒåŠ¨æˆæœ¬æˆ–æœªå……åˆ†èå…¥å› æœæœºåˆ¶ã€‚</li>
<li>æå‡ºæœ€ä½æˆæœ¬å› æœå†³ç­–ï¼ˆMiCCDï¼‰æ¡†æ¶ï¼Œç»“åˆåäº‹å®æ¨ç†å¤„ç†å¼‚å¸¸æ•°æ®ã€‚</li>
<li>MiCCDæ„å»ºåŸºäºå› æœå›¾çš„æ›¿ä»£æ¨¡å‹ï¼Œä½¿ç”¨å¼‚å¸¸æ¨¡å¼èšç±»æ ‡ç­¾ä½œä¸ºç›‘ç£ä¿¡å·ã€‚</li>
<li>MiCCDèƒ½å¤Ÿè¿‘ä¼¼å˜é‡é—´çš„ç»“æ„å› æœæ¨¡å‹ï¼Œä¸ºåäº‹å®æ¨ç†çš„è¯†åˆ«æä¾›åŸºç¡€ã€‚</li>
<li>é‡‡ç”¨è€ƒè™‘æˆæœ¬çš„åäº‹å®ä¼°è®¡æ–¹æ³•å’Œåºåˆ—æœ€å°äºŒä¹˜è§„åˆ’ï¼ˆSLSQPï¼‰ç®—æ³•ä¼˜åŒ–å¹²é¢„ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08343">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf2ab0eefa8ae1189e3656cc96e74194.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d7a95b56fe7af8d5f007cd8c50137ec.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="AM-Thinking-v1-Advancing-the-Frontier-of-Reasoning-at-32B-Scale"><a href="#AM-Thinking-v1-Advancing-the-Frontier-of-Reasoning-at-32B-Scale" class="headerlink" title="AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale"></a>AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale</h2><p><strong>Authors:Yunjie Ji, Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Han Zhao, Xiangang Li</strong></p>
<p>We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale.   Built entirely from the open-source Qwen2.5-32B base model and publicly available queries, AM-Thinking-v1 leverages a meticulously crafted post-training pipeline - combining supervised fine-tuning and reinforcement learning - to deliver exceptional reasoning capabilities. This work demonstrates that the open-source community can achieve high performance at the 32B scale, a practical sweet spot for deployment and fine-tuning. By striking a balance between top-tier performance and real-world usability, we hope AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale models, pushing reasoning boundaries while keeping accessibility at the core of innovation. We have open-sourced our model on \href{<a target="_blank" rel="noopener" href="https://huggingface.co/a-m-team/AM-Thinking-v1%7D%7BHugging">https://huggingface.co/a-m-team/AM-Thinking-v1}{Hugging</a> Face}. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†AM-Thinking-v1ï¼Œè¿™æ˜¯ä¸€ä¸ª32Bçš„å¯†é›†è¯­è¨€æ¨¡å‹ï¼Œåœ¨æ¨ç†æ–¹é¢å¤„äºå‰æ²¿åœ°ä½ï¼Œä½“ç°äº†å¼€æºåˆ›æ–°çš„åä½œç²¾ç¥ã€‚AM-Thinking-v1è¶…è¶Šäº†DeepSeek-R1ï¼Œå¹¶ä¸é¢†å…ˆçš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ï¼ˆå¦‚Qwen3-235B-A22Bå’ŒSeed1.5-Thinkingï¼‰ç›¸ç«äº‰ã€‚åœ¨AIME 2024ä¸Šå–å¾—äº†85.3çš„æƒŠäººæˆç»©ï¼Œåœ¨AIME 2025ä¸Šå–å¾—äº†74.4çš„æˆç»©ï¼Œåœ¨LiveCodeBenchä¸Šå–å¾—äº†70.3çš„æˆç»©ï¼Œå±•ç¤ºäº†åŒç±»è§„æ¨¡å¼€æºæ¨¡å‹ä¸­çš„æœ€å…ˆè¿›çš„æ•°å­¦å’Œç¼–ç èƒ½åŠ›ã€‚AM-Thinking-v1å®Œå…¨åŸºäºå¼€æºçš„Qwen2.5-32BåŸºç¡€æ¨¡å‹å’Œå…¬å¼€æŸ¥è¯¢æ„å»ºï¼Œåˆ©ç”¨ç²¾å¿ƒè®¾è®¡çš„åè®­ç»ƒç®¡é“ï¼ˆç»“åˆç›‘ç£å¾®è°ƒå¼ºåŒ–å­¦ä¹ ï¼‰æ¥å®ç°å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†å¼€æºç¤¾åŒºå¯ä»¥åœ¨32Bè§„æ¨¡ä¸Šå®ç°é«˜æ€§èƒ½ï¼Œè¿™æ˜¯éƒ¨ç½²å’Œå¾®è°ƒçš„å®é™…ç†æƒ³ç‚¹ã€‚æˆ‘ä»¬åœ¨ä¸¤è€…ä¹‹é—´è¾¾åˆ°äº†å¹³è¡¡ï¼šä¸€æµçš„æ€§èƒ½å’Œç°å®ä¸–ç•Œçš„ä½¿ç”¨æ€§ï¼Œæˆ‘ä»¬å¸Œæœ›AM-Thinking-v1èƒ½æ¿€å‘è¿›ä¸€æ­¥åä½œåŠªåŠ›ï¼Œåˆ©ç”¨ä¸­å‹æ¨¡å‹ï¼Œæ¨åŠ¨æ¨ç†è¾¹ç•Œï¼ŒåŒæ—¶ä¿æŒå¯è®¿é—®æ€§æ˜¯åˆ›æ–°çš„æ ¸å¿ƒã€‚æˆ‘ä»¬çš„æ¨¡å‹å·²ç»å¼€æºåœ¨Hugging Faceï¼ˆ<a target="_blank" rel="noopener" href="https://huggingface.co/a-m-team/AM-Thinking-v1%EF%BC%89%E3%80%82">https://huggingface.co/a-m-team/AM-Thinking-v1ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08311v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>AM-Thinking-v1æ˜¯ä¸€æ¬¾å…·å¤‡å…ˆè¿›æ¨ç†èƒ½åŠ›çš„32Bå¯†é›†è¯­è¨€æ¨¡å‹ï¼Œå±•ç°äº†å¼€æºåˆ›æ–°çš„åä½œç²¾ç¥ã€‚è¯¥æ¨¡å‹åœ¨AIME 2024ä¸Šå–å¾—85.3çš„é«˜åˆ†ï¼Œåœ¨AIME 2025ä¸Šå–å¾—74.4åˆ†ï¼Œåœ¨LiveCodeBenchä¸Šå–å¾—70.3åˆ†ï¼Œå±•ç¤ºäº†åŒç±»è§„æ¨¡å¼€æºæ¨¡å‹ä¸­çš„å“è¶Šæ•°å­¦å’Œç¼–ç èƒ½åŠ›ã€‚è¯¥æ¨¡å‹ç”±å¼€æºçš„Qwen2.5-32BåŸºç¡€æ¨¡å‹å’Œå…¬å¼€æŸ¥è¯¢æ„å»ºï¼Œé‡‡ç”¨ç²¾å¿ƒè®¾è®¡çš„åè®­ç»ƒç®¡é“ï¼Œç»“åˆç›‘ç£å¾®è°ƒå¼ºåŒ–å­¦ä¹ ï¼Œå®ç°å‡ºè‰²çš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å·¥ä½œè¯æ˜äº†å¼€æºç¤¾åŒºå¯ä»¥åœ¨32Bè§„æ¨¡ä¸Šå®ç°é«˜æ€§èƒ½ï¼Œä¸ºéƒ¨ç½²å’Œå¾®è°ƒæä¾›äº†å®ç”¨ç”œåŒºã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡AM-Thinking-v1æ¿€å‘è¿›ä¸€æ­¥åä½œåŠªåŠ›ï¼Œåˆ©ç”¨ä¸­å‹æ¨¡å‹æé«˜æ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒåˆ›æ–°çš„æ ¸å¿ƒå¯åŠæ€§ã€‚æ¨¡å‹å·²å¼€æºåœ¨Hugging Faceä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AM-Thinking-v1æ˜¯ä¸€æ¬¾å…·å¤‡å…ˆè¿›æ¨ç†èƒ½åŠ›çš„è¯­è¨€æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>AM-Thinking-v1åŸºäºå¼€æºçš„Qwen2.5-32BåŸºç¡€æ¨¡å‹å’Œå…¬å¼€æŸ¥è¯¢æ„å»ºã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨äº†ç»“åˆç›‘ç£å¾®è°ƒå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒç®¡é“ã€‚</li>
<li>è¯¥æ¨¡å‹è¯æ˜äº†å¼€æºç¤¾åŒºèƒ½åœ¨32Bè§„æ¨¡ä¸Šå®ç°é«˜æ€§èƒ½ã€‚</li>
<li>AM-Thinking-v1æ—¨åœ¨æ¿€å‘è¿›ä¸€æ­¥åä½œåŠªåŠ›ï¼Œåˆ©ç”¨ä¸­å‹æ¨¡å‹æé«˜æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08311">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a7c0b2456f775529d5e7cfffeb26f82e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5b13c9183d9c8d0f16fb56f1b1a32de.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e13b39454be13707f0268bd1302eaa48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-773f3d7eab29c23efc5cfd7ace8e74dd.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Training-Strategies-for-Efficient-Embodied-Reasoning"><a href="#Training-Strategies-for-Efficient-Embodied-Reasoning" class="headerlink" title="Training Strategies for Efficient Embodied Reasoning"></a>Training Strategies for Efficient Embodied Reasoning</h2><p><strong>Authors:William Chen, Suneel Belkhale, Suvir Mirchandani, Oier Mees, Danny Driess, Karl Pertsch, Sergey Levine</strong></p>
<p>Robot chain-of-thought reasoning (CoT) â€“ wherein a model predicts helpful intermediate representations before choosing actions â€“ provides an effective method for improving the generalization and performance of robot policies, especially vision-language-action models (VLAs). While such approaches have been shown to improve performance and generalization, they suffer from core limitations, like needing specialized robot reasoning data and slow inference speeds. To design new robot reasoning approaches that address these issues, a more complete characterization of why reasoning helps policy performance is critical. We hypothesize several mechanisms by which robot reasoning improves policies â€“ (1) better representation learning, (2) improved learning curricularization, and (3) increased expressivity â€“ then devise simple variants of robot CoT reasoning to isolate and test each one. We find that learning to generate reasonings does lead to better VLA representations, while attending to the reasonings aids in actually leveraging these features for improved action prediction. Our results provide us with a better understanding of why CoT reasoning helps VLAs, which we use to introduce two simple and lightweight alternative recipes for robot reasoning. Our proposed approaches achieve significant performance gains over non-reasoning policies, state-of-the-art results on the LIBERO-90 benchmark, and a 3x inference speedup compared to standard robot reasoning. </p>
<blockquote>
<p>æœºå™¨äººæ€ç»´é“¾æ¨ç†ï¼ˆCoTï¼‰â€”â€”å…¶ä¸­æ¨¡å‹åœ¨é‡‡å–è¡ŒåŠ¨å‰é¢„æµ‹æœ‰ç”¨çš„ä¸­é—´è¡¨ç¤ºâ€”â€”ä¸ºæå‡æœºå™¨äººç­–ç•¥ï¼ˆç‰¹åˆ«æ˜¯è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAsï¼‰ï¼‰çš„æ³›åŒ–èƒ½åŠ›å’Œæ€§èƒ½æä¾›äº†ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚è™½ç„¶å·²æœ‰ç ”ç©¶æ˜¾ç¤ºè¿™ç±»æ–¹æ³•èƒ½å¤Ÿæå‡æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä»å­˜åœ¨æ ¸å¿ƒå±€é™ï¼Œå¦‚éœ€è¦ä¸“é—¨çš„æœºå™¨äººæ¨ç†æ•°æ®ä»¥åŠæ¨ç†é€Ÿåº¦æ…¢ã€‚ä¸ºäº†è®¾è®¡è§£å†³è¿™äº›é—®é¢˜çš„æ–°çš„æœºå™¨äººæ¨ç†æ–¹æ³•ï¼Œå¯¹æ¨ç†ä¸ºä½•èƒ½å¸®åŠ©ç­–ç•¥æ€§èƒ½è¿›è¡Œæ›´å®Œæ•´çš„è¡¨å¾è‡³å…³é‡è¦ã€‚æˆ‘ä»¬å‡è®¾æœºå™¨äººæ¨ç†æ”¹å–„ç­–ç•¥çš„å‡ ä¸ªæœºåˆ¶â€”â€”ï¼ˆ1ï¼‰æ›´å¥½çš„è¡¨ç¤ºå­¦ä¹ ï¼Œï¼ˆ2ï¼‰æ”¹è¿›çš„å­¦ä¹ è¯¾ç¨‹åŒ–ï¼Œï¼ˆ3ï¼‰å¢åŠ è¡¨ç°åŠ›â€”â€”ç„¶åè®¾è®¡äº†ç®€å•çš„æœºå™¨äººCoTæ¨ç†å˜ä½“æ¥éš”ç¦»å¹¶æµ‹è¯•æ¯ä¸€ä¸ªå‡è®¾ã€‚æˆ‘ä»¬å‘ç°å­¦ä¹ ç”Ÿæˆæ¨ç†ç¡®å®ä¼šå¯¼è‡´æ›´å¥½çš„VLAè¡¨ç¤ºï¼Œè€Œå…³æ³¨è¿™äº›æ¨ç†æœ‰åŠ©äºåˆ©ç”¨è¿™äº›ç‰¹å¾æ¥æ”¹å–„åŠ¨ä½œé¢„æµ‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœä¸ºæˆ‘ä»¬æä¾›äº†CoTæ¨ç†ä¸ºä½•èƒ½å¸®åŠ©VLAsçš„æ›´å¥½ç†è§£ï¼Œæˆ‘ä»¬æ®æ­¤å¼•å…¥äº†ä¸¤ç§ç®€å•è½»é‡çº§çš„æœºå™¨äººæ¨ç†æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ç›¸è¾ƒäºéæ¨ç†ç­–ç•¥å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨LIBERO-90åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå¹¶ä¸”ç›¸è¾ƒäºæ ‡å‡†æœºå™¨äººæ¨ç†å®ç°äº†3å€çš„æ¨ç†é€Ÿåº¦æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08243v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœºå™¨äººæ€ç»´é“¾æ¨ç†ï¼ˆCoTï¼‰é€šè¿‡é¢„æµ‹è¡ŒåŠ¨å‰çš„ä¸­é—´è¡¨ç¤ºæ¥æ”¹å–„æœºå™¨äººç­–ç•¥çš„æ³›åŒ–æ€§å’Œæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹ï¼ˆVLAsï¼‰ä¸­æ•ˆæœæ˜¾è‘—ã€‚è™½ç„¶æ­¤æ–¹æ³•èƒ½æé«˜æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½†å®ƒå­˜åœ¨æ ¸å¿ƒå±€é™ï¼Œå¦‚éœ€è¦ç‰¹å®šçš„æœºå™¨äººæ¨ç†æ•°æ®å’Œæ¨ç†é€Ÿåº¦æ…¢ã€‚ä¸ºäº†è®¾è®¡è§£å†³è¿™äº›é—®é¢˜çš„æ–°çš„æœºå™¨äººæ¨ç†æ–¹æ³•ï¼Œæ›´å…¨é¢åœ°æè¿°æ¨ç†æ˜¯å¦‚ä½•å¸®åŠ©ç­–ç•¥æ€§èƒ½è‡³å…³é‡è¦ã€‚æœ¬æ–‡å‡è®¾æœºå™¨äººæ¨ç†æ”¹å–„ç­–ç•¥çš„å‡ ä¸ªæœºåˆ¶â€”â€”åŒ…æ‹¬æ›´å¥½çš„è¡¨ç¤ºå­¦ä¹ ã€æ”¹è¿›çš„å­¦ä¹ è¯¾ç¨‹åŒ–å’Œæé«˜çš„è¡¨è¾¾åŠ›ï¼Œç„¶åè®¾è®¡äº†ç®€å•çš„æœºå™¨äººCoTæ¨ç†å˜ä½“æ¥å­¤ç«‹å’Œæµ‹è¯•æ¯ä¸ªå‡è®¾ã€‚æˆ‘ä»¬å‘ç°å­¦ä¹ ç”Ÿæˆæ¨ç†ç¡®å®ä¼šå¯¼è‡´æ›´å¥½çš„VLAè¡¨ç¤ºï¼Œè€Œå…³æ³¨æ¨ç†æœ‰åŠ©äºåˆ©ç”¨è¿™äº›ç‰¹å¾æ¥æ”¹å–„è¡ŒåŠ¨é¢„æµ‹ã€‚æœ¬æ–‡çš„ç»“æœä¸ºæˆ‘ä»¬æä¾›äº†æ›´å¥½åœ°ç†è§£CoTæ¨ç†ä¸ºä½•èƒ½å¸®åŠ©VLAsçš„ä¾æ®ï¼Œå¹¶æ®æ­¤æå‡ºäº†ç®€å•è½»é‡çº§çš„æœºå™¨äººæ¨ç†æ–¹æ³•ã€‚ä¸æ— æ¨ç†ç­–ç•¥ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨LIBERO-90åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶ä¸”æ¨ç†é€Ÿåº¦æé«˜äº†ä¸‰å€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœºå™¨äººæ€ç»´é“¾æ¨ç†ï¼ˆCoTï¼‰èƒ½æœ‰æ•ˆæé«˜æœºå™¨äººç­–ç•¥çš„æ³›åŒ–æ€§å’Œæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹ï¼ˆVLAsï¼‰ä¸­ã€‚</li>
<li>CoTæ¨ç†çš„æ ¸å¿ƒé™åˆ¶åŒ…æ‹¬éœ€è¦ç‰¹å®šçš„æœºå™¨äººæ¨ç†æ•°æ®å’Œæ¨ç†é€Ÿåº¦æ…¢ã€‚</li>
<li>é€šè¿‡å‡è®¾å‡ ä¸ªæœºåˆ¶æ¥è§£é‡ŠCoTæ¨ç†å¦‚ä½•æé«˜ç­–ç•¥æ€§èƒ½ï¼ŒåŒ…æ‹¬æ›´å¥½çš„è¡¨ç¤ºå­¦ä¹ ã€æ”¹è¿›çš„å­¦ä¹ è¯¾ç¨‹åŒ–å’Œæé«˜çš„è¡¨è¾¾åŠ›ã€‚</li>
<li>å­¦ä¹ ç”Ÿæˆæ¨ç†æœ‰åŠ©äºè·å¾—æ›´å¥½çš„VLAè¡¨ç¤ºï¼Œè€Œå…³æ³¨æ¨ç†æœ‰åŠ©äºåˆ©ç”¨è¿™äº›ç‰¹å¾æ”¹å–„è¡ŒåŠ¨é¢„æµ‹ã€‚</li>
<li>æå‡ºç®€å•è½»é‡çº§çš„æœºå™¨äººæ¨ç†æ–¹æ³•ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹¶åœ¨LIBERO-90åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†ä¼˜å¼‚ç»“æœã€‚</li>
<li>ä¸ä¼ ç»Ÿæœºå™¨äººæ¨ç†ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å…·æœ‰æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</li>
<li>æœ¬æ–‡ä¸ºæˆ‘ä»¬æä¾›äº†æ›´å¥½åœ°ç†è§£CoTæ¨ç†å¦‚ä½•å¸®åŠ©VLAsçš„ä¾æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08243">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7de2af0c1b4c19a5dc547e6696b5a3c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e842e1c8dee0e302e371ade74837785b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e8d199658f9832bcaa9874e192322f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d792006688f4088a4be39b45f7da8c9.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DSADF-Thinking-Fast-and-Slow-for-Decision-Making"><a href="#DSADF-Thinking-Fast-and-Slow-for-Decision-Making" class="headerlink" title="DSADF: Thinking Fast and Slow for Decision Making"></a>DSADF: Thinking Fast and Slow for Decision Making</h2><p><strong>Authors:Alex Zhihao Dou, Dongfei Cui, Jun Yan, Weida Wang, Benteng Chen, Haoming Wang, Zeke Xie, Shufei Zhang</strong></p>
<p>Although Reinforcement Learning (RL) agents are effective in well-defined environments, they often struggle to generalize their learned policies to dynamic settings due to their reliance on trial-and-error interactions. Recent work has explored applying Large Language Models (LLMs) or Vision Language Models (VLMs) to boost the generalization of RL agents through policy optimization guidance or prior knowledge. However, these approaches often lack seamless coordination between the RL agent and the foundation model, leading to unreasonable decision-making in unfamiliar environments and efficiency bottlenecks. Making full use of the inferential capabilities of foundation models and the rapid response capabilities of RL agents and enhancing the interaction between the two to form a dual system is still a lingering scientific question. To address this problem, we draw inspiration from Kahnemanâ€™s theory of fast thinking (System 1) and slow thinking (System 2), demonstrating that balancing intuition and deep reasoning can achieve nimble decision-making in a complex world. In this study, we propose a Dual-System Adaptive Decision Framework (DSADF), integrating two complementary modules: System 1, comprising an RL agent and a memory space for fast and intuitive decision making, and System 2, driven by a VLM for deep and analytical reasoning. DSADF facilitates efficient and adaptive decision-making by combining the strengths of both systems. The empirical study in the video game environment: Crafter and Housekeep demonstrates the effectiveness of our proposed method, showing significant improvements in decision abilities for both unseen and known tasks. </p>
<blockquote>
<p>å°½ç®¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†åœ¨æ˜ç¡®ç¯å¢ƒä¸­æ˜¯æœ‰æ•ˆçš„ï¼Œä½†ç”±äºå®ƒä»¬ä¾èµ–äºè¯•é”™äº¤äº’ï¼Œå› æ­¤åœ¨å°†å­¦ä¹ åˆ°çš„ç­–ç•¥æ¨å¹¿åˆ°åŠ¨æ€è®¾ç½®æ—¶å¾€å¾€ä¼šé‡åˆ°å›°éš¾ã€‚æœ€è¿‘çš„ç ”ç©¶æ¢ç´¢äº†å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆ–è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åº”ç”¨äºé€šè¿‡ç­–ç•¥ä¼˜åŒ–æŒ‡å¯¼æˆ–å…ˆéªŒçŸ¥è¯†æ¥æå‡RLä»£ç†çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€ç¼ºä¹RLä»£ç†å’ŒåŸºç¡€æ¨¡å‹ä¹‹é—´çš„æ— ç¼åä½œï¼Œå¯¼è‡´åœ¨ä¸ç†Ÿæ‚‰çš„ç¯å¢ƒä¸­åšå‡ºä¸åˆç†çš„å†³ç­–å’Œæ•ˆç‡ç“¶é¢ˆã€‚å……åˆ†åˆ©ç”¨åŸºç¡€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’ŒRLä»£ç†çš„å¿«é€Ÿååº”èƒ½åŠ›ï¼Œå¹¶å¢å¼ºä¸¤è€…ä¹‹é—´çš„äº¤äº’ä»¥å½¢æˆåŒç³»ç»Ÿï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„ç§‘å­¦é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ä»å¡å°¼æ›¼çš„å¿«æ€è€ƒï¼ˆç³»ç»Ÿ1ï¼‰å’Œæ…¢æ€è€ƒï¼ˆç³»ç»Ÿ2ï¼‰ç†è®ºä¸­è·å¾—çµæ„Ÿï¼Œè¡¨æ˜å¹³è¡¡ç›´è§‰å’Œæ·±åº¦æ¨ç†å¯ä»¥åœ¨å¤æ‚ä¸–ç•Œä¸­å®ç°æ•æ·å†³ç­–ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŒç³»ç»Ÿè‡ªé€‚åº”å†³ç­–æ¡†æ¶ï¼ˆDSADFï¼‰ï¼Œå®ƒé›†æˆäº†ä¸¤ä¸ªäº’è¡¥çš„æ¨¡å—ï¼šç³»ç»Ÿ1ç”±RLä»£ç†å’Œç”¨äºå¿«é€Ÿç›´è§‰å†³ç­–çš„è®°å¿†ç©ºé—´ç»„æˆï¼›ç³»ç»Ÿ2ç”±VLMé©±åŠ¨è¿›è¡Œæ·±åº¦åˆ†ææ¨ç†ã€‚DSADFé€šè¿‡ç»“åˆä¸¤ä¸ªç³»ç»Ÿçš„ä¼˜åŠ¿ï¼Œå®ç°äº†é«˜æ•ˆè‡ªé€‚åº”å†³ç­–ã€‚åœ¨ç”µå­æ¸¸æˆç¯å¢ƒâ€œå·¥åŒ ä¸å®¶æ”¿â€ä¸­çš„å®è¯ç ”ç©¶è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæ— è®ºæ˜¯åœ¨æœªè§è¿‡çš„ä»»åŠ¡è¿˜æ˜¯å·²çŸ¥çš„ä»»åŠ¡ä¸Šéƒ½æ˜¾è‘—æé«˜äº†å†³ç­–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08189v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å®šä¹‰æ˜ç¡®çš„ç¯å¢ƒä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†åœ¨åŠ¨æ€ç¯å¢ƒä¸­å¸¸å¸¸æ— æ³•å°†å…¶ç­–ç•¥è¿›è¡Œæ³›åŒ–ã€‚ä¸ºæ­¤ï¼Œä¸€äº›æœ€æ–°çš„ç ”ç©¶å°è¯•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆ–è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥æå‡RLç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨RL agentå’ŒåŸºå‡†æ¨¡å‹ä¹‹é—´çš„åè°ƒæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´ä¸ç†Ÿæ‚‰ç¯å¢ƒä¸‹çš„å†³ç­–ä¸åˆç†ä»¥åŠæ•ˆç‡ç“¶é¢ˆã€‚å€Ÿé‰´Kahnemançš„â€œç³»ç»Ÿä¸€â€å’Œâ€œç³»ç»ŸäºŒâ€ç†è®ºï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŒç³»ç»Ÿè‡ªé€‚åº”å†³ç­–æ¡†æ¶ï¼ˆDSADFï¼‰ï¼Œç»“åˆäº†ç›´è§‰å’Œæ·±åº¦æ¨ç†çš„ä¼˜åŠ¿ï¼Œä»¥å®ç°å¿«é€Ÿå†³ç­–ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªäº’è¡¥æ¨¡å—ï¼šç³»ç»Ÿä¸€ç”±RL agentå’Œè®°å¿†ç©ºé—´ç»„æˆï¼Œç”¨äºå¿«é€Ÿç›´è§‰å†³ç­–ï¼›ç³»ç»ŸäºŒç”±VLMé©±åŠ¨è¿›è¡Œæ·±åº¦åˆ†ææ¨ç†ã€‚åœ¨æ¨¡æ‹Ÿæ¸¸æˆç¯å¢ƒCrafter and Housekeepä¸­çš„å®è¯ç ”ç©¶è¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—æé«˜äº†æœªçŸ¥ä»»åŠ¡å’Œå·²çŸ¥ä»»åŠ¡çš„å†³ç­–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨å¤„ç†åŠ¨æ€ç¯å¢ƒæ—¶å­˜åœ¨ç­–ç•¥æ³›åŒ–çš„é—®é¢˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹å¯ç”¨æ¥æé«˜å¼ºåŒ–å­¦ä¹ ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨å¤„ç†å¼ºåŒ–å­¦ä¹ agentå’ŒåŸºå‡†æ¨¡å‹ä¹‹é—´çš„åè°ƒæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>DSADFæ¡†æ¶ç»“åˆç›´è§‰å’Œæ·±åº¦æ¨ç†ï¼Œæ—¨åœ¨å®ç°å¿«é€Ÿä¸”è‡ªé€‚åº”çš„å†³ç­–ã€‚</li>
<li>DSADFæ¡†æ¶åŒ…å«ä¸¤ä¸ªäº’è¡¥æ¨¡å—ï¼šç³»ç»Ÿä¸€ç”¨äºå¿«é€Ÿç›´è§‰å†³ç­–ï¼Œç³»ç»ŸäºŒç”¨äºæ·±åº¦åˆ†ææ¨ç†ã€‚</li>
<li>å€Ÿé‰´äº†Kahnemançš„ç³»ç»Ÿä¸€å’Œç³»ç»ŸäºŒç†è®ºï¼Œæ—¨åœ¨å¹³è¡¡ç›´è§‰å’Œæ·±åº¦æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08189">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4cdf2d16d4a1056e6de882fd3cf8c46.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86c28df59b321c9f8a2f9e3fbbccf712.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac1156d9c1e54e3503804df81e40aa3a.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Visually-Interpretable-Subtask-Reasoning-for-Visual-Question-Answering"><a href="#Visually-Interpretable-Subtask-Reasoning-for-Visual-Question-Answering" class="headerlink" title="Visually Interpretable Subtask Reasoning for Visual Question Answering"></a>Visually Interpretable Subtask Reasoning for Visual Question Answering</h2><p><strong>Authors:Yu Cheng, Arushi Goel, Hakan Bilen</strong></p>
<p>Answering complex visual questions like &#96;Which red furniture can be used for sitting?â€™ requires multi-step reasoning, including object recognition, attribute filtering, and relational understanding. Recent work improves interpretability in multimodal large language models (MLLMs) by decomposing tasks into sub-task programs, but these methods are computationally expensive and less accurate due to poor adaptation to target data. To address this, we introduce VISTAR (Visually Interpretable Subtask-Aware Reasoning Model), a subtask-driven training framework that enhances both interpretability and reasoning by generating textual and visual explanations within MLLMs. Instead of relying on external models, VISTAR fine-tunes MLLMs to produce structured Subtask-of-Thought rationales (step-by-step reasoning sequences). Experiments on two benchmarks show that VISTAR consistently improves reasoning accuracy while maintaining interpretability. Our code and dataset will be available at <a target="_blank" rel="noopener" href="https://github.com/ChengJade/VISTAR">https://github.com/ChengJade/VISTAR</a>. </p>
<blockquote>
<p>å›ç­”åƒâ€œå“ªäº›çº¢è‰²å®¶å…·å¯ä»¥ç”¨äºåç€ï¼Ÿâ€è¿™æ ·çš„å¤æ‚è§†è§‰é—®é¢˜ï¼Œéœ€è¦è¿›è¡Œå¤šæ­¥éª¤æ¨ç†ï¼ŒåŒ…æ‹¬å¯¹è±¡è¯†åˆ«ã€å±æ€§è¿‡æ»¤å’Œå…³ç³»ç†è§£ã€‚æœ€è¿‘çš„å·¥ä½œé€šè¿‡å°†ä»»åŠ¡åˆ†è§£æˆå­ä»»åŠ¡ç¨‹åºæ¥æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å¯è§£é‡Šæ€§ï¼Œä½†è¿™äº›æ–¹æ³•è®¡ç®—é‡å¤§ï¼Œç”±äºä¸é€‚åº”ç›®æ ‡æ•°æ®è€Œç²¾åº¦è¾ƒä½ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VISTARï¼ˆè§†è§‰å¯è§£é‡Šçš„å­ä»»åŠ¡æ„ŸçŸ¥æ¨ç†æ¨¡å‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå­ä»»åŠ¡é©±åŠ¨çš„è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡åœ¨MLLMå†…éƒ¨ç”Ÿæˆæ–‡æœ¬å’Œè§†è§‰è§£é‡Šæ¥å¢å¼ºå¯è§£é‡Šæ€§å’Œæ¨ç†èƒ½åŠ›ã€‚VISTARä¸éœ€è¦ä¾èµ–å¤–éƒ¨æ¨¡å‹ï¼Œè€Œæ˜¯å¯¹MLLMè¿›è¡Œå¾®è°ƒä»¥äº§ç”Ÿç»“æ„åŒ–æ€è€ƒçš„å­ä»»åŠ¡ï¼ˆé€æ­¥æ¨ç†åºåˆ—ï¼‰ã€‚åœ¨ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVISTARåœ¨ä¿æŒå¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œä¸æ–­æé«˜æ¨ç†çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/ChengJade/VISTAR%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/ChengJade/VISTARä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08084v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä»‹ç»äº†å¤æ‚è§†è§‰é—®ç­”éœ€è¦å¤šæ­¥éª¤æ¨ç†ï¼ŒåŒ…æ‹¬ç›®æ ‡è¯†åˆ«ã€å±æ€§è¿‡æ»¤å’Œå…³ç³»ç†è§£ç­‰ã€‚ä¸ºè§£å†³ç°æœ‰æ–¹æ³•çš„è®¡ç®—æˆæœ¬é«˜å’Œå‡†ç¡®æ€§å·®çš„é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†åä¸ºVISTARçš„å­ä»»åŠ¡é©±åŠ¨è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­ç”Ÿæˆæ–‡æœ¬å’Œè§†è§‰è§£é‡Šæ¥å¢å¼ºå¯è§£é‡Šæ€§å’Œæ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVISTARåœ¨ä¿æŒå¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œæé«˜äº†æ¨ç†çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤æ‚è§†è§‰é—®ç­”éœ€è¦è¿›è¡Œå¤šæ­¥éª¤æ¨ç†ï¼ŒåŒ…æ‹¬ç›®æ ‡è¯†åˆ«ã€å±æ€§è¿‡æ»¤å’Œå…³ç³»ç†è§£ç­‰ã€‚</li>
<li>å½“å‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å¯è§£é‡Šæ€§æå‡æ–¹æ³•æ˜¯é€šè¿‡ä»»åŠ¡åˆ†è§£æˆå­ä»»åŠ¡ç¨‹åºå®ç°çš„ã€‚</li>
<li>å°½ç®¡è¿™äº›æ–¹æ³•å¯ä»¥æå‡å¯è§£é‡Šæ€§ï¼Œä½†ç”±äºè®¡ç®—æˆæœ¬é«˜å’Œå¯¹ç›®æ ‡æ•°æ®é€‚åº”æ€§å·®ï¼Œå…¶å‡†ç¡®æ€§å’Œæ•ˆç‡æœ‰å¾…æé«˜ã€‚</li>
<li>VISTARæ˜¯ä¸€ä¸ªå­ä»»åŠ¡é©±åŠ¨çš„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸­ç”Ÿæˆæ–‡æœ¬å’Œè§†è§‰è§£é‡Šæ¥å¢å¼ºå¯è§£é‡Šæ€§å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>VISTARé€šè¿‡å¾®è°ƒMLLMsæ¥äº§ç”Ÿç»“æ„åŒ–çš„å­ä»»åŠ¡æ€ç»´ä¾æ®ï¼ˆé€æ­¥æ¨ç†åºåˆ—ï¼‰ï¼Œè€Œä¸æ˜¯ä¾èµ–å¤–éƒ¨æ¨¡å‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒVISTARåœ¨ä¿æŒå¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œæé«˜äº†æ¨ç†çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08084">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e69e4a0a9e9be5a0b4a356803a4e9db8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-50304a1a4333493f421d5df1426f65b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3a6e9c3864f8d1d4aa4829945b39924.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7681c90a5761fd0b368815122e8cfec7.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="FalseReject-A-Resource-for-Improving-Contextual-Safety-and-Mitigating-Over-Refusals-in-LLMs-via-Structured-Reasoning"><a href="#FalseReject-A-Resource-for-Improving-Contextual-Safety-and-Mitigating-Over-Refusals-in-LLMs-via-Structured-Reasoning" class="headerlink" title="FalseReject: A Resource for Improving Contextual Safety and Mitigating   Over-Refusals in LLMs via Structured Reasoning"></a>FalseReject: A Resource for Improving Contextual Safety and Mitigating   Over-Refusals in LLMs via Structured Reasoning</h2><p><strong>Authors:Zhehao Zhang, Weijie Xu, Fanyou Wu, Chandan K. Reddy</strong></p>
<p>Safety alignment approaches in large language models (LLMs) often lead to the over-refusal of benign queries, significantly diminishing their utility in sensitive scenarios. To address this challenge, we introduce FalseReject, a comprehensive resource containing 16k seemingly toxic queries accompanied by structured responses across 44 safety-related categories. We propose a graph-informed adversarial multi-agent interaction framework to generate diverse and complex prompts, while structuring responses with explicit reasoning to aid models in accurately distinguishing safe from unsafe contexts. FalseReject includes training datasets tailored for both standard instruction-tuned models and reasoning-oriented models, as well as a human-annotated benchmark test set. Our extensive benchmarking on 29 state-of-the-art (SOTA) LLMs reveals persistent over-refusal challenges. Empirical results demonstrate that supervised finetuning with FalseReject substantially reduces unnecessary refusals without compromising overall safety or general language capabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®‰å…¨å¯¹é½æ–¹æ³•å¾€å¾€å¯¼è‡´è‰¯æ€§æŸ¥è¯¢è¢«æ‹’ç»çš„æƒ…å†µè¿‡å¤šï¼Œè¿™åœ¨æ•æ„Ÿåœºæ™¯ä¸­æ˜¾è‘—é™ä½äº†å…¶æ•ˆç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†FalseRejectï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«16000ä¸ªçœ‹ä¼¼æœ‰æ¯’æŸ¥è¯¢çš„ç»¼åˆèµ„æºï¼Œå¹¶æ¶µç›–äº†ä¸è¿™ç›¸å…³çš„å®‰å…¨æ€§ç›¸å…³çš„44ä¸ªç±»åˆ«çš„ç»“æ„åŒ–å“åº”ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºå›¾å¯¹æŠ—çš„å¤šæ™ºèƒ½ä½“äº¤äº’æ¡†æ¶ï¼Œç”Ÿæˆå¤šæ ·ä¸”å¤æ‚çš„æç¤ºï¼ŒåŒæ—¶åˆ©ç”¨ç»“æ„åŒ–å“åº”å’Œæ˜ç¡®æ¨ç†æ¥å¸®åŠ©æ¨¡å‹å‡†ç¡®åŒºåˆ†å®‰å…¨å’Œä¸å®‰å…¨ä¸Šä¸‹æ–‡ã€‚FalseRejectæ—¢åŒ…å«é€‚åˆæ ‡å‡†æŒ‡ä»¤è®­ç»ƒæ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†ï¼Œä¹ŸåŒ…å«é¢å‘æ¨ç†æ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†ï¼Œè¿˜åŒ…æ‹¬ä¸€ä¸ªäººå·¥æ ‡æ³¨çš„åŸºå‡†æµ‹è¯•é›†ã€‚æˆ‘ä»¬å¯¹æœ€æ–°å‰æ²¿çš„29ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ï¼Œå‘ç°æŒç»­å­˜åœ¨è¿‡åº¦æ‹’ç»çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨FalseRejectè¿›è¡Œç›‘ç£å¾®è°ƒèƒ½å¤§å¹…å‡å°‘ä¸å¿…è¦çš„æ‹’ç»æƒ…å†µï¼ŒåŒæ—¶ä¸ä¼šæŸå®³æ•´ä½“å®‰å…¨æ€§æˆ–é€šç”¨è¯­è¨€åŠŸèƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08054v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®‰å…¨å¯¹é½æ–¹æ³•å¾€å¾€ä¼šå¯¼è‡´è‰¯æ€§æŸ¥è¯¢è¢«æ‹’ç»ï¼Œä¸¥é‡å½±å“äº†å…¶åœ¨æ•æ„Ÿåœºæ™¯ä¸­çš„å®ç”¨æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºFalseRejectèµ„æºï¼ŒåŒ…å«çœ‹ä¼¼æœ‰æ¯’çš„æŸ¥è¯¢å’Œç»“æ„åŒ–å“åº”ã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªåŸºäºå›¾ä¿¡æ¯å¯¹æŠ—å¤šæ™ºèƒ½ä½“äº¤äº’æ¡†æ¶ç”Ÿæˆæç¤ºå¹¶ç»“æ„å›åº”ä»¥æé«˜æ¨¡å‹çš„å‡†ç¡®åˆ¤æ–­èƒ½åŠ›ã€‚æ­¤å¤–è¿˜åŒ…æ‹¬å®šåˆ¶è®­ç»ƒé›†ï¼Œè¦†ç›–æ ‡å‡†æŒ‡ä»¤è°ƒæ•´æ¨¡å‹å’Œæ¨ç†å¯¼å‘æ¨¡å‹ï¼Œä»¥åŠäººç±»æ³¨é‡ŠåŸºå‡†æµ‹è¯•é›†ã€‚æˆ‘ä»¬å¯¹29ä¸ªæœ€å…ˆè¿›çš„LLMè¿›è¡Œäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ï¼Œå‘ç°ä»å­˜åœ¨è¿‡åº¦æ‹’ç»çš„æŒ‘æˆ˜ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨FalseRejectè¿›è¡Œç›‘ç£å¾®è°ƒå¯ä»¥å¤§å¤§å‡å°‘ä¸å¿…è¦çš„æ‹’ç»ï¼ŒåŒæ—¶ä¸æŸå®³æ•´ä½“å®‰å…¨æ€§æˆ–é€šç”¨è¯­è¨€åŠŸèƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨å¯¹é½æ–¹æ³•å­˜åœ¨è¿‡åº¦æ‹’ç»è‰¯æ€§æŸ¥è¯¢çš„é—®é¢˜ã€‚</li>
<li>FalseRejectèµ„æºåŒ…å«çœ‹ä¼¼æœ‰æ¯’çš„æŸ¥è¯¢å’Œç»“æ„åŒ–å“åº”ï¼Œç”¨äºåº”å¯¹æ­¤é—®é¢˜ã€‚</li>
<li>æå‡ºåŸºäºå›¾ä¿¡æ¯å¯¹æŠ—å¤šæ™ºèƒ½ä½“äº¤äº’æ¡†æ¶ï¼Œæé«˜æ¨¡å‹åŒºåˆ†å®‰å…¨ä¸éå®‰å…¨è¯­å¢ƒçš„å‡†ç¡®æ€§ã€‚</li>
<li>FalseRejectåŒ…å«å®šåˆ¶çš„è®­ç»ƒé›†ï¼Œè¦†ç›–æ ‡å‡†æŒ‡ä»¤è°ƒæ•´æ¨¡å‹å’Œæ¨ç†å¯¼å‘æ¨¡å‹çš„éœ€æ±‚ã€‚</li>
<li>FalseRejectåŒ…å«äººç±»æ³¨é‡ŠåŸºå‡†æµ‹è¯•é›†ï¼Œä¾¿äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„LLMä¹Ÿå­˜åœ¨è¿‡åº¦æ‹’ç»çš„æŒ‘æˆ˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08054">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9292cf0d974df1daf85c9f0927366477.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e30d06503d16d92647ed39cd50e9b32f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf51fa8fb6bbe4cbf990b3ff6cba40d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09207769e0dc2c7bfc68a74c02269e4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-950900a64b16aee64105510420762eb7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf7eb6fa3749bb4faa784ec987c430c1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MilChat-Introducing-Chain-of-Thought-Reasoning-and-GRPO-to-a-Multimodal-Small-Language-Model-for-Remote-Sensing"><a href="#MilChat-Introducing-Chain-of-Thought-Reasoning-and-GRPO-to-a-Multimodal-Small-Language-Model-for-Remote-Sensing" class="headerlink" title="MilChat: Introducing Chain of Thought Reasoning and GRPO to a Multimodal   Small Language Model for Remote Sensing"></a>MilChat: Introducing Chain of Thought Reasoning and GRPO to a Multimodal   Small Language Model for Remote Sensing</h2><p><strong>Authors:Aybora Koksal, A. Aydin Alatan</strong></p>
<p>Remarkable capabilities in understanding and generating text-image content have been demonstrated by recent advancements in multimodal large language models (MLLMs). However, their effectiveness in specialized domains-particularly those requiring resource-efficient and domain-specific adaptations-has remained limited. In this work, a lightweight multimodal language model termed MilChat is introduced, specifically adapted to analyze remote sensing imagery in secluded areas, including challenging missile launch sites. A new dataset, MilData, was compiled by verifying hundreds of aerial images through expert review, and subtle military installations were highlighted via detailed captions. Supervised fine-tuning on a 2B-parameter open-source MLLM with chain-of-thought (CoT) reasoning annotations was performed, enabling more accurate and interpretable explanations. Additionally, Group Relative Policy Optimization (GRPO) was leveraged to enhance the modelâ€™s ability to detect critical domain-specific cues-such as defensive layouts and key military structures-while minimizing false positives on civilian scenes. Through empirical evaluations, it has been shown that MilChat significantly outperforms both larger, general-purpose multimodal models and existing remote sensing-adapted approaches on open-ended captioning and classification metrics. Over 80% recall and 98% precision were achieved on the newly proposed MilData benchmark, underscoring the potency of targeted fine-tuning and reinforcement learning in specialized real-world applications. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•å±•ç¤ºå‡ºäº†åœ¨ç†è§£å’Œç”Ÿæˆæ–‡æœ¬å›¾åƒå†…å®¹æ–¹é¢çš„å“è¶Šèƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç‰¹å®šé¢†åŸŸçš„æœ‰æ•ˆæ€§ï¼Œå°¤å…¶æ˜¯é‚£äº›éœ€è¦èµ„æºé«˜æ•ˆå’Œé¢†åŸŸç‰¹å®šçš„é€‚åº”æ€§çš„é¢†åŸŸï¼Œä»ç„¶æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œå¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼Œç§°ä¸ºMilChatï¼Œç‰¹åˆ«é€‚åº”äºåˆ†æåè¿œåœ°åŒºçš„é¥æ„Ÿå›¾åƒï¼ŒåŒ…æ‹¬å…·æœ‰æŒ‘æˆ˜æ€§çš„å¯¼å¼¹å‘å°„åœºã€‚ä¸€ä¸ªæ–°çš„æ•°æ®é›†MilDataæ˜¯é€šè¿‡ä¸“å®¶å®¡æŸ¥éªŒè¯çš„æ•°ç™¾å¼ èˆªç©ºå›¾åƒç¼–è¯‘è€Œæˆçš„ï¼Œé€šè¿‡è¯¦ç»†çš„æ ‡é¢˜çªå‡ºäº†å¾®å¦™çš„å†›äº‹è®¾æ–½ã€‚åœ¨å…·æœ‰æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æ³¨é‡Šçš„2Bå‚æ•°å¼€æºMLLMä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œä»è€Œæä¾›æ›´å‡†ç¡®å’Œå¯è§£é‡Šçš„è§£é‡Šã€‚æ­¤å¤–ï¼Œåˆ©ç”¨é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¢å¼ºæ¨¡å‹æ£€æµ‹å…³é”®é¢†åŸŸçº¿ç´¢çš„èƒ½åŠ›ï¼Œå¦‚é˜²å¾¡å¸ƒå±€å’Œå…³é”®å†›äº‹ç»“æ„ï¼ŒåŒæ—¶å°½é‡å‡å°‘åœ¨æ°‘ç”¨åœºæ™¯ä¸­çš„è¯¯æŠ¥ã€‚é€šè¿‡å®è¯è¯„ä¼°ï¼ŒMilChatåœ¨å¼€æ”¾å¼æ ‡é¢˜å’Œåˆ†ç±»æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºæ›´å¤§ã€é€šç”¨çš„å¤šæ¨¡æ€æ¨¡å‹ä»¥åŠç°æœ‰çš„é¥æ„Ÿé€‚åº”æ–¹æ³•ã€‚åœ¨æ–°æå‡ºçš„MilDataåŸºå‡†æµ‹è¯•ä¸Šï¼Œå¬å›ç‡è¶…è¿‡80%ï¼Œå‡†ç¡®ç‡è¾¾åˆ°98%ï¼Œè¿™çªæ˜¾äº†é’ˆå¯¹ç‰¹å®šç°å®ä¸–ç•Œåº”ç”¨çš„ç²¾ç»†è°ƒæ•´å’Œå¼ºåŒ–å­¦ä¹ çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07984v1">PDF</a> Submitted to JSTARS on April 2, 2025. Code and dataset will be   available upon acceptance</p>
<p><strong>Summary</strong>ï¼šè¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç†è§£å’Œç”Ÿæˆæ–‡æœ¬å›¾åƒå†…å®¹æ–¹é¢å±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ç‰¹å®šé¢†åŸŸçš„åº”ç”¨æ•ˆæœæœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦èµ„æºé«˜æ•ˆå’Œé¢†åŸŸç‰¹å®šçš„é€‚åº”åœºæ™¯ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†ä¸€ç§è½»é‡çº§å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹MilChatï¼Œä¸“é—¨é€‚åº”äºåˆ†æé¥æ„Ÿå›¾åƒï¼ŒåŒ…æ‹¬å…·æœ‰æŒ‘æˆ˜æ€§çš„å¯¼å¼¹å‘å°„åœ°ç‚¹ç­‰é¢†åŸŸã€‚é€šè¿‡ä¸“å®¶å®¡æŸ¥éªŒè¯æ•°ç™¾å¼ ç©ºä¸­å›¾åƒï¼Œåˆ›å»ºäº†æ–°çš„æ•°æ®é›†MilDataï¼Œå¹¶é€šè¿‡è¯¦ç»†å­—å¹•çªå‡ºå¾®å¦™çš„å†›äº‹è®¾æ–½ã€‚é€šè¿‡åœ¨å¸¦æœ‰æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æ³¨é‡Šçš„å¼€æºMLLMä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œå®ç°äº†æ›´å‡†ç¡®å’Œå¯è§£é‡Šçš„è§£é‡Šã€‚æ­¤å¤–ï¼Œåˆ©ç”¨é›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰æé«˜äº†æ¨¡å‹æ£€æµ‹å…³é”®é¢†åŸŸçº¿ç´¢çš„èƒ½åŠ›ï¼Œå¦‚é˜²å¾¡å¸ƒå±€å’Œå…³é”®å†›äº‹ç»“æ„ï¼ŒåŒæ—¶å‡å°‘äº†å¯¹æ°‘ç”¨åœºæ™¯çš„è¯¯æŠ¥ã€‚ç»éªŒè¯„ä¼°è¡¨æ˜ï¼ŒMilChatåœ¨å¼€æ”¾å­—å¹•å’Œåˆ†ç±»æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºå¤§å‹é€šç”¨å¤šæ¨¡æ€æ¨¡å‹å’Œç°æœ‰çš„é¥æ„Ÿé€‚åº”æ–¹æ³•ã€‚åœ¨æ–°æå‡ºçš„MilDataåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¬å›ç‡è¶…è¿‡80%ï¼Œå‡†ç¡®ç‡é«˜è¾¾98%ï¼Œçªæ˜¾äº†é’ˆå¯¹ç‰¹å®šç°å®ä¸–ç•Œåº”ç”¨çš„ç²¾ç»†è°ƒæ•´å’Œå¼ºåŒ–å­¦ä¹ çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ–‡æœ¬å’Œå›¾åƒç†è§£æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>åœ¨ç‰¹å®šé¢†åŸŸï¼Œå¦‚é¥æ„Ÿå›¾åƒåˆ†æï¼Œéœ€è¦èµ„æºé«˜æ•ˆå’Œé¢†åŸŸç‰¹å®šçš„æ¨¡å‹é€‚åº”ã€‚</li>
<li>å¼•å…¥è½»é‡çº§å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹MilChatï¼Œç‰¹åˆ«é€‚ç”¨äºåˆ†æé¥æ„Ÿå›¾åƒï¼ŒåŒ…æ‹¬å¯¼å¼¹å‘å°„åœ°ç‚¹ç­‰æŒ‘æˆ˜åœºæ™¯ã€‚</li>
<li>é€šè¿‡ä¸“å®¶å®¡æŸ¥åˆ›å»ºæ–°çš„æ•°æ®é›†MilDataï¼Œé€šè¿‡è¯¦ç»†å­—å¹•çªå‡ºæ˜¾ç¤ºå¾®å¦™çš„å†›äº‹è®¾æ–½ã€‚</li>
<li>ç›‘ç£å¾®è°ƒç»“åˆæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æ³¨é‡Šæé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>åˆ©ç”¨é›†å›¢ç›¸å¯¹æ”¿ç­–ä¼˜åŒ–ï¼ˆGRPOï¼‰æé«˜æ¨¡å‹åœ¨æ£€æµ‹å…³é”®é¢†åŸŸçº¿ç´¢æ–¹é¢çš„èƒ½åŠ›ï¼Œå¦‚é˜²å¾¡å¸ƒå±€å’Œå†›äº‹ç»“æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07984">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4785ee4a03c3926893d1026c635dd433.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91f360f1acc522e32e79eba4eaeb86f3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-455921f023280d771fcd5cfe7f180f51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6144f994d10d94f8260988369808006.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59376c88e55b74bbcc162483ecc21325.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Practical-Reasoning-Interruption-Attacks-on-Reasoning-Large-Language-Models"><a href="#Practical-Reasoning-Interruption-Attacks-on-Reasoning-Large-Language-Models" class="headerlink" title="Practical Reasoning Interruption Attacks on Reasoning Large Language   Models"></a>Practical Reasoning Interruption Attacks on Reasoning Large Language   Models</h2><p><strong>Authors:Yu Cui, Cong Zuo</strong></p>
<p>Reasoning large language models (RLLMs) have demonstrated outstanding performance across a variety of tasks, yet they also expose numerous security vulnerabilities. Most of these vulnerabilities have centered on the generation of unsafe content. However, recent work has identified a distinct â€œthinking-stoppedâ€ vulnerability in DeepSeek-R1: under adversarial prompts, the modelâ€™s reasoning process ceases at the system level and produces an empty final answer. Building upon this vulnerability, researchers developed a novel prompt injection attack, termed reasoning interruption attack, and also offered an initial analysis of its root cause. Through extensive experiments, we verify the previous analyses, correct key errors based on three experimental findings, and present a more rigorous explanation of the fundamental causes driving the vulnerability. Moreover, existing attacks typically require over 2,000 tokens, impose significant overhead, reduce practicality, and are easily detected. To overcome these limitations, we propose the first practical reasoning interruption attack. It succeeds with just 109 tokens by exploiting our newly uncovered â€œreasoning token overflowâ€ (RTO) effect to overwrite the modelâ€™s final answer, forcing it to return an invalid response. Experimental results demonstrate that our proposed attack is highly effective. Furthermore, we discover that the method for triggering RTO differs between the official DeepSeek-R1 release and common unofficial deployments. As a broadened application of RTO, we also construct a novel jailbreak attack that enables the transfer of unsafe content within the reasoning tokens into final answer, thereby exposing it to the user. Our work carries significant implications for enhancing the security of RLLMs. </p>
<blockquote>
<p>æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆRLLMsï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œä½†åŒæ—¶ä¹Ÿæš´éœ²å‡ºäº†è®¸å¤šå®‰å…¨æ¼æ´ã€‚å…¶ä¸­å¤§å¤šæ•°æ¼æ´ä¸»è¦é›†ä¸­åœ¨ç”Ÿæˆä¸å®‰å…¨å†…å®¹ä¸Šã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„å·¥ä½œåœ¨DeepSeek-R1ä¸­è¯†åˆ«å‡ºäº†ä¸€ç§ç‹¬ç‰¹çš„â€œæ€ç»´åœæ»â€æ¼æ´ï¼šåœ¨å¯¹æŠ—æç¤ºä¸‹ï¼Œæ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ä¼šåœ¨ç³»ç»Ÿå±‚é¢åœæ­¢ï¼Œå¹¶äº§ç”Ÿç©ºçš„æœ€ç»ˆç­”æ¡ˆã€‚åŸºäºæ­¤æ¼æ´ï¼Œç ”ç©¶äººå‘˜å¼€å‘äº†ä¸€ç§åä¸ºæ¨ç†ä¸­æ–­æ”»å‡»çš„æ–°å‹æç¤ºæ³¨å…¥æ”»å‡»ï¼Œå¹¶å¯¹å…¶åŸå› è¿›è¡Œäº†åˆæ­¥åˆ†æã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬éªŒè¯äº†ä¹‹å‰çš„åˆ†æï¼Œæ ¹æ®ä¸‰ä¸ªå®éªŒå‘ç°çº æ­£äº†å…³é”®é”™è¯¯ï¼Œå¹¶å¯¹é©±åŠ¨è¯¥æ¼æ´çš„æ ¹æœ¬åŸå› æä¾›äº†æ›´ä¸¥æ ¼çš„è§£é‡Šã€‚è€Œä¸”ï¼Œç°æœ‰çš„æ”»å‡»é€šå¸¸éœ€è¦è¶…è¿‡2000ä¸ªä»¤ç‰Œï¼Œä¼šå¸¦æ¥æ˜¾è‘—çš„å¼€é”€ï¼Œé™ä½å®ç”¨æ€§ï¼Œå¹¶ä¸”å¾ˆå®¹æ˜“è¢«æ£€æµ‹å‡ºæ¥ã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ç§å®ç”¨çš„æ¨ç†ä¸­æ–­æ”»å‡»ã€‚å®ƒä»…ä½¿ç”¨109ä¸ªä»¤ç‰Œå°±èƒ½æˆåŠŸï¼Œé€šè¿‡åˆ©ç”¨æˆ‘ä»¬æ–°å‘ç°çš„â€œæ¨ç†ä»¤ç‰Œæº¢å‡ºâ€ï¼ˆRTOï¼‰æ•ˆåº”æ¥è¦†ç›–æ¨¡å‹çš„æœ€ç»ˆç­”æ¡ˆï¼Œè¿«ä½¿å…¶è¿”å›æ— æ•ˆå“åº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ”»å‡»éå¸¸æœ‰æ•ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°è§¦å‘RTOçš„æ–¹æ³•åœ¨å®˜æ–¹DeepSeek-R1å‘å¸ƒå’Œå¸¸è§çš„éå®˜æ–¹éƒ¨ç½²ä¹‹é—´æ˜¯ä¸åŒçš„ã€‚ä½œä¸ºRTOçš„æ‰©å±•åº”ç”¨ï¼Œæˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ç§æ–°å‹è¶Šç‹±æ”»å‡»ï¼Œä½¿ä¸å®‰å…¨å†…å®¹èƒ½å¤Ÿåœ¨æ¨ç†ä»¤ç‰Œå†…è½¬ç§»åˆ°æœ€ç»ˆç­”æ¡ˆä¸­ï¼Œä»è€Œæš´éœ²ç»™ç”¨æˆ·ã€‚æˆ‘ä»¬çš„å·¥ä½œå¯¹å¢å¼ºRLLMsçš„å®‰å…¨æ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06643v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆRLLMsï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†åŒæ—¶ä¹Ÿæš´éœ²å‡ºè®¸å¤šå®‰å…¨æ¼æ´ã€‚æœ€è¿‘çš„å·¥ä½œå‘ç°äº†DeepSeek-R1ä¸­ç‹¬ç‰¹çš„â€œæ€ç»´åœæ»â€æ¼æ´ï¼šåœ¨æ•Œå¯¹æç¤ºä¸‹ï¼Œè¯¥æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ä¼šåœ¨ç³»ç»Ÿå±‚é¢åœæ­¢ï¼Œå¹¶äº§ç”Ÿç©ºç™½çš„æœ€ç»ˆç­”æ¡ˆã€‚åŸºäºæ­¤æ¼æ´ï¼Œç ”ç©¶è€…å¼€å‘äº†ä¸€ç§åä¸ºâ€œæ¨ç†ä¸­æ–­æ”»å‡»â€çš„æ–°å‹æç¤ºæ³¨å…¥æ”»å‡»ï¼Œå¹¶å¯¹æ ¹æœ¬åŸå› è¿›è¡Œäº†åˆæ­¥åˆ†æã€‚æˆ‘ä»¬é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯äº†ä¹‹å‰çš„åˆ†æï¼ŒåŸºäºä¸‰ä¸ªå®éªŒå‘ç°çº æ­£äº†å…³é”®é”™è¯¯ï¼Œå¹¶å¯¹é©±åŠ¨è¯¥æ¼æ´çš„æ ¹æœ¬åŸå› æä¾›äº†æ›´ä¸¥æ ¼çš„è§£é‡Šã€‚æ­¤å¤–ï¼Œç°æœ‰çš„æ”»å‡»é€šå¸¸éœ€è¦è¶…è¿‡2000ä¸ªä»¤ç‰Œï¼Œä¼šå¸¦æ¥æ˜¾è‘—çš„å¼€é”€ï¼Œé™ä½å®ç”¨æ€§ï¼Œå¹¶ä¸”å®¹æ˜“è¢«æ£€æµ‹ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ç§å®ç”¨çš„æ¨ç†ä¸­æ–­æ”»å‡»ï¼Œä»…ä½¿ç”¨109ä¸ªä»¤ç‰Œå°±æˆåŠŸåˆ©ç”¨äº†æˆ‘ä»¬æ–°å‘ç°çš„â€œæ¨ç†ä»¤ç‰Œæº¢å‡ºâ€ï¼ˆRTOï¼‰æ•ˆåº”æ¥è¦†ç›–æ¨¡å‹çš„æœ€ç»ˆç­”æ¡ˆï¼Œè¿«ä½¿å…¶è¿”å›æ— æ•ˆå“åº”ã€‚å®éªŒç»“æœè¯æ˜æˆ‘ä»¬çš„æ”»å‡»éå¸¸æœ‰æ•ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°è§¦å‘RTOçš„æ–¹æ³•åœ¨å®˜æ–¹DeepSeek-R1å‘è¡Œç‰ˆå’Œå¸¸è§çš„éå®˜æ–¹éƒ¨ç½²ä¹‹é—´æœ‰æ‰€ä¸åŒã€‚ä½œä¸ºRTOçš„æ‰©å±•åº”ç”¨ï¼Œæˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ç§æ–°å‹çš„è¶Šç‹±æ”»å‡»ï¼Œèƒ½å¤Ÿå°†æ¨ç†ä»¤ç‰Œä¸­çš„ä¸å®‰å…¨å†…å®¹è½¬ç§»åˆ°æœ€ç»ˆç­”æ¡ˆä¸­ï¼Œä»è€Œå°†å…¶æš´éœ²ç»™ç”¨æˆ·ã€‚æˆ‘ä»¬çš„å·¥ä½œå¯¹å¢å¼ºRLLMsçš„å®‰å…¨æ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>DeepSeek-R1å­˜åœ¨ä¸€ç§åä¸ºâ€œæ€ç»´åœæ»â€çš„ç‹¬ç‰¹æ¼æ´ï¼Œå³åœ¨æ•Œå¯¹æç¤ºä¸‹æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¼šåœæ­¢ã€‚</li>
<li>ç ”ç©¶è€…åˆ©ç”¨æ­¤æ¼æ´å¼€å‘äº†ä¸€ç§åä¸ºâ€œæ¨ç†ä¸­æ–­æ”»å‡»â€çš„æ–°å‹æç¤ºæ³¨å…¥æ”»å‡»æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å¹¿æ³›å®éªŒéªŒè¯ï¼Œæˆ‘ä»¬å‘ç°äº†å…³äºè¯¥æ¼æ´æ›´ä¸¥æ ¼çš„è§£é‡Šå’Œå…ˆå‰åˆ†æçš„é”™è¯¯çº æ­£ã€‚</li>
<li>æˆ‘ä»¬æå‡ºäº†æ›´é«˜æ•ˆçš„æ¨ç†ä¸­æ–­æ”»å‡»æ–¹æ³•ï¼Œä»…ä½¿ç”¨å°‘é‡ä»¤ç‰Œå°±èƒ½æˆåŠŸæ”»å‡»æ¨¡å‹ã€‚</li>
<li>è§¦å‘æ¨ç†ä»¤ç‰Œæº¢å‡ºï¼ˆRTOï¼‰çš„æ–¹æ³•åœ¨ä¸åŒç‰ˆæœ¬çš„DeepSeek-R1ä¹‹é—´å­˜åœ¨å·®å¼‚ã€‚</li>
<li>åŸºäºRTOæ•ˆåº”ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ç§æ–°å‹çš„è¶Šç‹±æ”»å‡»ï¼Œèƒ½å¤Ÿæš´éœ²ä¸å®‰å…¨å†…å®¹ç»™ç”¨æˆ·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06643">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c665b515175dfd0a7c5ddcc5427f1984.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-710150ca26b5bc23397a8d1549589531.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-806b65bd11d0a432800fac5c3ffe1379.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-74c86a16dbde2a6e042bc04bcc97cfce.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="A-New-DAPO-Algorithm-for-Stock-Trading"><a href="#A-New-DAPO-Algorithm-for-Stock-Trading" class="headerlink" title="A New DAPO Algorithm for Stock Trading"></a>A New DAPO Algorithm for Stock Trading</h2><p><strong>Authors:Ruijian Zha, Bojun Liu</strong></p>
<p>Recent advances in reinforcement learning, such as Dynamic Sampling Policy Optimization (DAPO), show strong performance when paired with large language models (LLMs). Motivated by this success, we ask whether similar gains can be realized in financial trading. We design a trading agent that combines an improved Group Relative Policy Optimization (GRPO) algorithm, augmented with ideas from DAPO, with LLM-based risk and sentiment signals extracted from financial news. On the NASDAQ-100 index (FNSPID dataset), our agent attains a cumulative return of 230.49 percent and an information ratio of 0.37, outperforming the CPPO-DeepSeek baseline. It also cuts training time from about 8 hours to 2.5 hours over 100 epochs while markedly reducing RAM usage. The proposed RL-LLM framework offers a scalable path toward data-efficient trading agents. Code: <a target="_blank" rel="noopener" href="https://github.com/Ruijian-Zha/FinRL-DAPO-SR/">https://github.com/Ruijian-Zha/FinRL-DAPO-SR/</a> </p>
<blockquote>
<p>è¿‘æœŸå¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„è¿›å±•ï¼Œå¦‚åŠ¨æ€é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼ˆDAPOï¼‰ï¼Œåœ¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆæ—¶è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚å—æ­¤æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æƒ³çŸ¥é“æ˜¯å¦èƒ½åœ¨é‡‘èäº¤æ˜“ä¸­å®ç°ç±»ä¼¼çš„æ”¶ç›Šã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªäº¤æ˜“ä»£ç†ï¼Œå®ƒç»“åˆäº†æ”¹è¿›çš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ï¼Œèåˆäº†DAPOçš„ç†å¿µï¼Œä»¥åŠåŸºäºLLMçš„é£é™©å’Œæƒ…ç»ªä¿¡å·ï¼Œè¿™äº›ä¿¡å·æ˜¯ä»é‡‘èæ–°é—»ä¸­æå–çš„ã€‚åœ¨çº³æ–¯è¾¾å…‹100æŒ‡æ•°ï¼ˆFNSPIDæ•°æ®é›†ï¼‰ä¸Šï¼Œæˆ‘ä»¬çš„ä»£ç†å®ç°äº†230.49%çš„ç´¯è®¡å›æŠ¥ç‡å’Œ0.37çš„ä¿¡æ¯æ¯”ç‡ï¼Œè¶…è¿‡äº†CPPO-DeepSeekåŸºçº¿ã€‚æ­¤å¤–ï¼Œå®ƒå°†è®­ç»ƒæ—¶é—´ä»çº¦8å°æ—¶ç¼©çŸ­è‡³2.5å°æ—¶ï¼ˆ100ä¸ªå‘¨æœŸï¼‰ï¼Œå¹¶æ˜¾è‘—é™ä½äº†RAMä½¿ç”¨é‡ã€‚æ‰€æå‡ºçš„RL-LLMæ¡†æ¶ä¸ºæ•°æ®é«˜æ•ˆäº¤æ˜“ä»£ç†æä¾›äº†å¯æ‰©å±•çš„è·¯å¾„ã€‚ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/Ruijian-Zha/FinRL-DAPO-SR/">https://github.com/Ruijian-Zha/FinRL-DAPO-SR/</a> </p>
</blockquote>
<p><strong>ç®€åŒ–ç‰ˆç¿»è¯‘</strong>ï¼š</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.06408v1">PDF</a> Accepted to IEEE IDS 2025 Special Track: Financial Reinforcement   Learning and Foundation Models (FinRLFM). 3 pages, 2 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¼ºåŒ–å­¦ä¹ ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆçš„ç ”ç©¶è¿›å±•ï¼Œå¦‚åŠ¨æ€é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼ˆDAPOï¼‰ï¼Œå±•ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚æœ¬ç ”ç©¶å—æ­¤å¯å‘ï¼Œæ¢ç´¢å°†æ”¹è¿›åçš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç®—æ³•ä¸LLMæå–çš„é‡‘èæ–°é—»é£é™©ä¸æƒ…ç»ªä¿¡å·ç»“åˆï¼Œåº”ç”¨äºé‡‘èäº¤æ˜“é¢†åŸŸã€‚åœ¨çº³æ–¯è¾¾å…‹100æŒ‡æ•°ï¼ˆFNSPIDæ•°æ®é›†ï¼‰ä¸Šï¼Œè¯¥äº¤æ˜“ä»£ç†å®ç°äº†ç´¯è®¡å›æŠ¥ç‡230.49%ï¼Œä¿¡æ¯æ¯”ç‡ä¸º0.37ï¼Œä¼˜äºCPPO-DeepSeekåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—ç¼©çŸ­äº†è®­ç»ƒæ—¶é—´å¹¶é™ä½äº†å†…å­˜ä½¿ç”¨ï¼Œä¸ºæ„å»ºæ•°æ®é«˜æ•ˆäº¤æ˜“ä»£ç†æä¾›äº†å¯æ‰©å±•è·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»“åˆåœ¨è¿‘æœŸå±•ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶æ¢ç´¢äº†å°†æ”¹è¿›åçš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•åº”ç”¨äºé‡‘èäº¤æ˜“é¢†åŸŸã€‚</li>
<li>ç»“åˆDAPOæ€æƒ³çš„GRPOç®—æ³•ä¸LLMæå–çš„é‡‘èæ–°é—»é£é™©åŠæƒ…ç»ªä¿¡å·ï¼Œåœ¨çº³æ–¯è¾¾å…‹100æŒ‡æ•°ä¸Šå®ç°äº†é«˜å›æŠ¥ç‡ã€‚</li>
<li>è¯¥äº¤æ˜“ä»£ç†ä¼˜äºCPPO-DeepSeekåŸºçº¿æ¨¡å‹ï¼Œå®ç°äº†ç´¯è®¡å›æŠ¥ç‡230.49%ï¼Œä¿¡æ¯æ¯”ç‡ä¸º0.37ã€‚</li>
<li>è¯¥æ¡†æ¶ç¼©çŸ­äº†è®­ç»ƒæ—¶é—´ï¼Œä»çº¦8å°æ—¶å‡å°‘åˆ°2.5å°æ—¶ã€‚</li>
<li>è¯¥æ¡†æ¶é™ä½äº†å†…å­˜ä½¿ç”¨ï¼Œä¸ºæ„å»ºæ•°æ®é«˜æ•ˆäº¤æ˜“ä»£ç†æä¾›äº†å¯æ‰©å±•è·¯å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.06408">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e0bedee654414cfb7774067e9e0178e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-209d127e7f6530b6c8d8bc2fa43b50e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1f204e25689f35ef7acf0d2f132bde3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b146c0ac93d9c16dc8202b78c50c7a9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e5b10a279e45d742097ecaac9d43d8a.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="APOLLO-Automated-LLM-and-Lean-Collaboration-for-Advanced-Formal-Reasoning"><a href="#APOLLO-Automated-LLM-and-Lean-Collaboration-for-Advanced-Formal-Reasoning" class="headerlink" title="APOLLO: Automated LLM and Lean Collaboration for Advanced Formal   Reasoning"></a>APOLLO: Automated LLM and Lean Collaboration for Advanced Formal   Reasoning</h2><p><strong>Authors:Azim Ospanov, Farzan Farnia, Roozbeh Yousefzadeh</strong></p>
<p>Formal reasoning and automated theorem proving constitute a challenging subfield of machine learning, in which machines are tasked with proving mathematical theorems using formal languages like Lean. A formal verification system can check whether a formal proof is correct or not almost instantaneously, but generating a completely correct formal proof with large language models (LLMs) remains a formidable task. The usual approach in the literature is to prompt the LLM many times (up to several thousands) until one of the generated proofs passes the verification system. In this work, we present APOLLO (Automated PrOof repair via LLM and Lean cOllaboration), a modular, model-agnostic pipeline that combines the strengths of the Lean compiler with an LLMâ€™s reasoning abilities to achieve better proof-generation results at a low sampling budget. Apollo directs a fully automated process in which the LLM generates proofs for theorems, a set of agents analyze the proofs, fix the syntax errors, identify the mistakes in the proofs using Lean, isolate failing sub-lemmas, utilize automated solvers, and invoke an LLM on each remaining goal with a low top-K budget. The repaired sub-proofs are recombined and reverified, iterating up to a user-controlled maximum number of attempts. On the miniF2F benchmark, we establish a new state-of-the-art accuracy of 75.0% among 7B-parameter models while keeping the sampling budget below one thousand. Moreover, Apollo raises the state-of-the-art accuracy for Goedel-Prover-SFT to 65.6% while cutting sample complexity from 25,600 to a few hundred. General-purpose models (o3-mini, o4-mini) jump from 3-7% to over 40% accuracy. Our results demonstrate that targeted, compiler-guided repair of LLM outputs yields dramatic gains in both efficiency and correctness, suggesting a general paradigm for scalable automated theorem proving. </p>
<blockquote>
<p>å½¢å¼æ¨ç†å’Œè‡ªåŠ¨åŒ–å®šç†è¯æ˜æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å­é¢†åŸŸï¼Œè¯¥é¢†åŸŸä¸­ï¼Œæœºå™¨ä½¿ç”¨å¦‚Leanç­‰æ­£å¼è¯­è¨€æ¥å®Œæˆæ•°å­¦å®šç†çš„è¯æ˜ã€‚å½¢å¼åŒ–éªŒè¯ç³»ç»Ÿå‡ ä¹å¯ä»¥ç«‹å³æ£€æŸ¥å½¢å¼åŒ–è¯æ˜æ˜¯å¦æ­£ç¡®ï¼Œä½†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆå®Œå…¨æ­£ç¡®çš„å½¢å¼åŒ–è¯æ˜ä»ç„¶æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ã€‚æ–‡çŒ®ä¸­çš„é€šå¸¸åšæ³•æ˜¯å¤šæ¬¡ï¼ˆé«˜è¾¾æ•°åƒæ¬¡ï¼‰æç¤ºLLMï¼Œç›´åˆ°ç”Ÿæˆçš„è¯æ˜ä¹‹ä¸€é€šè¿‡éªŒè¯ç³»ç»Ÿã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†APOLLOï¼ˆé€šè¿‡LLMå’ŒLeanåä½œçš„è‡ªåŠ¨åŒ–è¯æ˜ä¿®å¤ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–çš„ã€æ¨¡å‹æ— å…³çš„æµç¨‹ï¼Œå®ƒå°†Leanç¼–è¯‘å™¨çš„ä¼˜åŠ¿ä¸LLMçš„æ¨ç†èƒ½åŠ›ç›¸ç»“åˆï¼Œä»¥è¾ƒä½çš„é‡‡æ ·é¢„ç®—å®ç°æ›´å¥½çš„è¯æ˜ç”Ÿæˆç»“æœã€‚Apolloå¼•å¯¼ä¸€ä¸ªå®Œå…¨è‡ªåŠ¨åŒ–çš„è¿‡ç¨‹ï¼Œå…¶ä¸­LLMä¸ºå®šç†ç”Ÿæˆè¯æ˜ï¼Œä¸€ç»„æ™ºèƒ½ä½“åˆ†æè¯æ˜ï¼Œä¿®å¤è¯­æ³•é”™è¯¯ï¼Œä½¿ç”¨Leanè¯†åˆ«è¯æ˜ä¸­çš„é”™è¯¯ï¼Œéš”ç¦»å¤±è´¥çš„å­å¼•ç†ï¼Œåˆ©ç”¨è‡ªåŠ¨åŒ–æ±‚è§£å™¨ï¼Œå¹¶åœ¨å‰©ä½™çš„æ¯ä¸ªç›®æ ‡ä¸Šä½¿ç”¨é¢„ç®—è¾ƒä½çš„top-Kè°ƒç”¨LLMã€‚ä¿®å¤çš„äºšè¯æ˜è¢«é‡æ–°ç»„åˆå¹¶é‡æ–°éªŒè¯ï¼Œè¿­ä»£ç›´è‡³ç”¨æˆ·æ§åˆ¶çš„æœ€å¤§å°è¯•æ¬¡æ•°ã€‚åœ¨miniF2FåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬åœ¨7Bå‚æ•°æ¨¡å‹ä¸­å»ºç«‹äº†æœ€æ–°æœ€å…ˆè¿›çš„75.0%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä¿æŒé‡‡æ ·é¢„ç®—ä½äºä¸€åƒã€‚æ­¤å¤–ï¼ŒApolloå°†Goedel-Prover-SFTçš„ç°æœ‰å‡†ç¡®ç‡æé«˜åˆ°65.6%ï¼ŒåŒæ—¶å°†æ ·æœ¬å¤æ‚åº¦ä»25,600é™ä½åˆ°å‡ ç™¾ã€‚é€šç”¨æ¨¡å‹ï¼ˆo3-miniã€o4-miniï¼‰çš„å‡†ç¡®ç‡ä»3-7%è·ƒå‡è‡³è¶…è¿‡40%ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé’ˆå¯¹LLMè¾“å‡ºçš„ç¼–è¯‘å™¨å¼•å¯¼ä¿®å¤åœ¨æé«˜æ•ˆç‡å’Œæ­£ç¡®æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œè¿™ä¸ºè¿›ä¸€æ­¥å¯æ‰©å±•çš„è‡ªåŠ¨åŒ–å®šç†è¯æ˜æä¾›äº†é€šç”¨çš„èŒƒä¾‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05758v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†æœºå™¨å­¦ä¹ ä¸­æŒ‘æˆ˜çš„ä¸€ä¸ªå­é¢†åŸŸâ€”â€”å½¢å¼æ¨ç†å’Œè‡ªåŠ¨å®šç†è¯æ˜ã€‚æ–‡ä¸­æåˆ°æœºå™¨éœ€è¦åˆ©ç”¨å½¢å¼è¯­è¨€å¦‚Leanæ¥è¯æ˜æ•°å­¦å®šç†ã€‚è™½ç„¶å½¢å¼éªŒè¯ç³»ç»Ÿå¯ä»¥å‡ ä¹ç¬é—´éªŒè¯å½¢å¼è¯æ˜çš„æ­£ç¡®æ€§ï¼Œä½†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆå®Œå…¨æ­£ç¡®çš„å½¢å¼è¯æ˜ä»ç„¶æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§ç»“åˆLeanç¼–è¯‘å™¨ä¸LLMæ¨ç†èƒ½åŠ›çš„æ¨¡å—åŒ–ã€æ¨¡å‹æ— å…³çš„ç®¡é“Apolloï¼Œä»¥åœ¨ä½é‡‡æ ·é¢„ç®—å†…å®ç°æ›´å¥½çš„è¯æ˜ç”Ÿæˆç»“æœã€‚Apolloé€šè¿‡è‡ªåŠ¨åŒ–è¿‡ç¨‹ç”Ÿæˆå®šç†è¯æ˜ï¼Œä½¿ç”¨ä¸€ç»„ä»£ç†åˆ†æã€ä¿®å¤è¯­æ³•é”™è¯¯ï¼Œå¹¶ä½¿ç”¨Leanè¯†åˆ«è¯æ˜ä¸­çš„é”™è¯¯ã€‚ä¿®å¤åçš„å­è¯æ˜è¢«é‡æ–°ç»„åˆå¹¶é‡æ–°éªŒè¯ï¼Œè¿­ä»£ç›´è‡³ç”¨æˆ·æ§åˆ¶çš„æœ€å¤§å°è¯•æ¬¡æ•°ã€‚åœ¨miniF2FåŸºå‡†æµ‹è¯•ä¸­ï¼ŒApolloåœ¨7Bå‚æ•°æ¨¡å‹ä¸­è¾¾åˆ°äº†75.0%çš„æœ€æ–°å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä¿æŒé‡‡æ ·é¢„ç®—ä½äºä¸€åƒã€‚æ­¤å¤–ï¼ŒApolloæé«˜äº†Goedel-Prover-SFTçš„å‡†ç¡®ç‡è‡³65.6%ï¼Œå¹¶å°†æ ·æœ¬å¤æ‚åº¦ä»25600é™ä½åˆ°å‡ ç™¾ã€‚é€šç”¨æ¨¡å‹ï¼ˆo3-miniã€o4-miniï¼‰çš„å‡†ç¡®ç‡ä»3-7%è·ƒå‡è‡³è¶…è¿‡40%ã€‚ç»“æœè¯æ˜äº†é’ˆå¯¹LLMè¾“å‡ºçš„ç¼–è¯‘å™¨å¼•å¯¼ä¿®å¤åœ¨æé«˜æ•ˆç‡å’Œæ­£ç¡®æ€§æ–¹é¢çš„æ˜¾è‘—æ”¶ç›Šï¼Œä¸ºå¯æ‰©å±•çš„è‡ªåŠ¨å®šç†è¯æ˜æå‡ºäº†é€šç”¨èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½¢å¼æ¨ç†å’Œè‡ªåŠ¨å®šç†è¯æ˜æ˜¯æœºå™¨å­¦ä¹ ä¸­ä¸€ä¸ªæœ‰æŒ‘æˆ˜çš„å­é¢†åŸŸï¼Œæ¶‰åŠä½¿ç”¨å¦‚Leançš„å½¢å¼è¯­è¨€æ¥ç”Ÿæˆæ•°å­¦å®šç†çš„è¯æ˜ã€‚</li>
<li>å½“å‰æ–‡çŒ®ä¸­çš„é€šå¸¸åšæ³•æ˜¯é€šè¿‡å¤šæ¬¡æç¤ºLLMï¼ˆæœ€å¤šè¾¾æ•°åƒæ¬¡ï¼‰æ¥ç”Ÿæˆé€šè¿‡éªŒè¯ç³»ç»Ÿçš„è¯æ˜ã€‚</li>
<li>Apolloæ˜¯ä¸€ä¸ªç»“åˆLeanç¼–è¯‘å™¨ä¸LLMæ¨ç†èƒ½åŠ›çš„ç®¡é“ï¼Œæ—¨åœ¨æé«˜è¯æ˜ç”Ÿæˆçš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>Apolloé€šè¿‡è‡ªåŠ¨åŒ–è¿‡ç¨‹ç”Ÿæˆå®šç†è¯æ˜ï¼Œå¹¶åŒ…æ‹¬è¯æ˜åˆ†æã€é”™è¯¯ä¿®å¤ã€å­ç›®æ ‡æ±‚è§£ç­‰æ­¥éª¤ã€‚</li>
<li>Apolloåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°çš„é«˜å‡†ç¡®ç‡ï¼Œè¯æ˜äº†å…¶åœ¨æé«˜æ•ˆç‡å’Œæ­£ç¡®æ€§æ–¹é¢çš„æ˜¾è‘—æ”¶ç›Šã€‚</li>
<li>Apolloçš„æ–¹æ³•ä¸ºå¯æ‰©å±•çš„è‡ªåŠ¨å®šç†è¯æ˜æå‡ºäº†é€šç”¨èŒƒå¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05758">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f6d11ec1dca796624ef747b32e7b29fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-34dd0c453eb3590e3d40764bfc406e5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3244e076a20ac951dd63271d466f290.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1087f5bbd0549a7b725842c081530138.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-15/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-15/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-15/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-9292cf0d974df1daf85c9f0927366477.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-15  CodePDE An Inference Framework for LLM-driven PDE Solver Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-18/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-03-18\./crop_åŒ»å­¦å›¾åƒ/2503.12094v1/page_2_0.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-14  U2AD Uncertainty-based Unsupervised Anomaly Detection Framework for   Detecting T2 Hyperintensity in MRI Spinal Cord
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28879.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
