<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-15  CodePDE An Inference Framework for LLM-driven PDE Solver Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-9292cf0d974df1daf85c9f0927366477.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-15-æ›´æ–°"><a href="#2025-05-15-æ›´æ–°" class="headerlink" title="2025-05-15 æ›´æ–°"></a>2025-05-15 æ›´æ–°</h1><h2 id="CodePDE-An-Inference-Framework-for-LLM-driven-PDE-Solver-Generation"><a href="#CodePDE-An-Inference-Framework-for-LLM-driven-PDE-Solver-Generation" class="headerlink" title="CodePDE: An Inference Framework for LLM-driven PDE Solver Generation"></a>CodePDE: An Inference Framework for LLM-driven PDE Solver Generation</h2><p><strong>Authors:Shanda Li, Tanya Marwah, Junhong Shen, Weiwei Sun, Andrej Risteski, Yiming Yang, Ameet Talwalkar</strong></p>
<p>Partial differential equations (PDEs) are fundamental to modeling physical systems, yet solving them remains a complex challenge. Traditional numerical solvers rely on expert knowledge to implement and are computationally expensive, while neural-network-based solvers require large training datasets and often lack interpretability. In this work, we frame PDE solving as a code generation task and introduce CodePDE, the first inference framework for generating PDE solvers using large language models (LLMs). Leveraging advanced inference-time algorithms and scaling strategies, CodePDE unlocks critical capacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and test-time scaling â€“ all without task-specific tuning. CodePDE achieves superhuman performance across a range of representative PDE problems. We also present a systematic empirical analysis of LLM generated solvers, analyzing their accuracy, efficiency, and numerical scheme choices. Our findings highlight the promise and the current limitations of LLMs in PDE solving, offering a new perspective on solver design and opportunities for future model development. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/LithiumDA/CodePDE">https://github.com/LithiumDA/CodePDE</a>. </p>
<blockquote>
<p>åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEï¼‰æ˜¯ç‰©ç†ç³»ç»Ÿå»ºæ¨¡çš„åŸºç¡€ï¼Œä½†å…¶æ±‚è§£ä»ç„¶æ˜¯ä¸€é¡¹å¤æ‚çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„æ•°å€¼æ±‚è§£å™¨ä¾èµ–äºä¸“å®¶çŸ¥è¯†è¿›è¡Œå®æ–½ï¼Œè®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œè€ŒåŸºäºç¥ç»ç½‘ç»œçš„æ±‚è§£å™¨åˆ™éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶ä¸”å¾€å¾€ç¼ºä¹å¯è§£é‡Šæ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†PDEæ±‚è§£çœ‹ä½œæ˜¯ä¸€ç§ä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†CodePDEï¼Œè¿™æ˜¯ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”ŸæˆPDEæ±‚è§£å™¨çš„ç¬¬ä¸€ä¸ªæ¨ç†æ¡†æ¶ã€‚åˆ©ç”¨å…ˆè¿›çš„æ¨ç†æ—¶ç®—æ³•å’Œæ‰©å±•ç­–ç•¥ï¼ŒCodePDEè§£é”äº†LLMç”¨äºPDEæ±‚è§£çš„å…³é”®èƒ½åŠ›ï¼šæ¨ç†ã€è°ƒè¯•ã€è‡ªæˆ‘å®Œå–„å’Œæµ‹è¯•æ—¶é—´æ‰©å±•â€”â€”æ‰€æœ‰è¿™ä¸€åˆ‡éƒ½ä¸éœ€è¦è¿›è¡Œç‰¹å®šä»»åŠ¡çš„è°ƒæ•´ã€‚CodePDEåœ¨ä¸€ç³»åˆ—å…·æœ‰ä»£è¡¨æ€§çš„PDEé—®é¢˜ä¸Šå®ç°äº†è¶…äººç±»çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å¯¹LLMç”Ÿæˆçš„æ±‚è§£å™¨è¿›è¡Œäº†ç³»ç»Ÿçš„å®è¯åˆ†æï¼Œåˆ†æäº†å®ƒä»¬çš„å‡†ç¡®æ€§ã€æ•ˆç‡å’Œæ•°å€¼æ–¹æ¡ˆçš„é€‰æ‹©ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†LLMåœ¨PDEæ±‚è§£ä¸­çš„æ½œåŠ›åŠå½“å‰å±€é™ï¼Œä¸ºæ±‚è§£å™¨è®¾è®¡å’Œæœªæ¥æ¨¡å‹å¼€å‘æä¾›äº†æ–°è§†è§’å’Œæœºä¼šã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LithiumDA/CodePDE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LithiumDA/CodePDEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08783v1">PDF</a> </p>
<p><strong>Summary</strong><br>PDEæ±‚è§£æ˜¯ç‰©ç†ç³»ç»Ÿå»ºæ¨¡çš„åŸºç¡€ï¼Œä½†æ±‚è§£ä»ç„¶æ˜¯ä¸€é¡¹å¤æ‚çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ•°å€¼æ±‚è§£å™¨ä¾èµ–äºä¸“ä¸šçŸ¥è¯†å®æ–½ä¸”è®¡ç®—æˆæœ¬é«˜ï¼Œè€ŒåŸºäºç¥ç»ç½‘ç»œçš„æ±‚è§£å™¨éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®é›†ä¸”ç¼ºä¹å¯è§£é‡Šæ€§ã€‚æœ¬ç ”ç©¶å°†PDEæ±‚è§£æ¡†æ¶åŒ–ä¸ºä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œå¹¶å¼•å…¥CodePDEï¼Œè¿™æ˜¯ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”ŸæˆPDEæ±‚è§£å™¨çš„é¦–ä¸ªæ¨ç†æ¡†æ¶ã€‚CodePDEåˆ©ç”¨å…ˆè¿›çš„æ¨ç†æ—¶é—´ç®—æ³•å’Œæ‰©å±•ç­–ç•¥ï¼Œè§£é”äº†LLMç”¨äºPDEæ±‚è§£çš„å…³é”®èƒ½åŠ›ï¼šæ¨ç†ã€è°ƒè¯•ã€è‡ªæˆ‘å®Œå–„å’Œæµ‹è¯•æ—¶é—´æ‰©å±•ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è°ƒæ•´ã€‚CodePDEåœ¨ä»£è¡¨æ€§PDEé—®é¢˜ä¸Šå®ç°äº†è¶…äººç±»æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å¯¹LLMç”Ÿæˆçš„æ±‚è§£å™¨è¿›è¡Œäº†ç³»ç»Ÿçš„å®è¯åˆ†æï¼Œåˆ†æå…¶å‡†ç¡®æ€§ã€æ•ˆç‡å’Œæ•°å€¼æ–¹æ¡ˆé€‰æ‹©ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†LLMåœ¨PDEæ±‚è§£ä¸­çš„æ½œåŠ›ä¸å½“å‰å±€é™æ€§ï¼Œä¸ºæ±‚è§£å™¨è®¾è®¡å’Œæœªæ¥æ¨¡å‹å¼€å‘æä¾›äº†æ–°çš„è§†è§’å’Œæœºä¼šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PDEæ±‚è§£æ˜¯ç‰©ç†ç³»ç»Ÿå»ºæ¨¡çš„åŸºç¡€ï¼Œä½†æ±‚è§£è¿‡ç¨‹å¤æ‚ä¸”ä¼ ç»Ÿæ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶å°†PDEæ±‚è§£æ¡†æ¶åŒ–ä¸ºä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œå¼•å…¥CodePDEï¼Œè¿™æ˜¯é¦–ä¸ªä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ¡†æ¶ã€‚</li>
<li>CodePDEèƒ½å¤Ÿåˆ©ç”¨LLMè¿›è¡Œæ¨ç†ã€è°ƒè¯•ã€è‡ªæˆ‘å®Œå–„å’Œæµ‹è¯•æ—¶é—´æ‰©å±•ï¼Œæ— éœ€ç‰¹å®šä»»åŠ¡è°ƒæ•´ã€‚</li>
<li>CodePDEåœ¨ä»£è¡¨æ€§PDEé—®é¢˜ä¸Šå®ç°äº†è¶…äººç±»æ€§èƒ½ã€‚</li>
<li>LLMç”Ÿæˆçš„æ±‚è§£å™¨åœ¨å‡†ç¡®æ€§ã€æ•ˆç‡å’Œæ•°å€¼æ–¹æ¡ˆé€‰æ‹©æ–¹é¢è¿›è¡Œäº†ç³»ç»Ÿçš„å®è¯åˆ†æã€‚</li>
<li>ç ”ç©¶çªå‡ºäº†LLMåœ¨PDEæ±‚è§£ä¸­çš„æ½œåŠ›ä¸å½“å‰å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08783">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-59f94e49f3ceecea570f09ce9cf18cd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca936bba47215748a7ef10030463030c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85b6963e6c07e8706fbd2b26d1b6c1bf.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="HealthBench-Evaluating-Large-Language-Models-Towards-Improved-Human-Health"><a href="#HealthBench-Evaluating-Large-Language-Models-Towards-Improved-Human-Health" class="headerlink" title="HealthBench: Evaluating Large Language Models Towards Improved Human   Health"></a>HealthBench: Evaluating Large Language Models Towards Improved Human   Health</h2><p><strong>Authors:Rahul K. Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin QuiÃ±onero-Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, Johannes Heidecke, Karan Singhal</strong></p>
<p>We present HealthBench, an open-source benchmark measuring the performance and safety of large language models in healthcare. HealthBench consists of 5,000 multi-turn conversations between a model and an individual user or healthcare professional. Responses are evaluated using conversation-specific rubrics created by 262 physicians. Unlike previous multiple-choice or short-answer benchmarks, HealthBench enables realistic, open-ended evaluation through 48,562 unique rubric criteria spanning several health contexts (e.g., emergencies, transforming clinical data, global health) and behavioral dimensions (e.g., accuracy, instruction following, communication). HealthBench performance over the last two years reflects steady initial progress (compare GPT-3.5 Turboâ€™s 16% to GPT-4oâ€™s 32%) and more rapid recent improvements (o3 scores 60%). Smaller models have especially improved: GPT-4.1 nano outperforms GPT-4o and is 25 times cheaper. We additionally release two HealthBench variations: HealthBench Consensus, which includes 34 particularly important dimensions of model behavior validated via physician consensus, and HealthBench Hard, where the current top score is 32%. We hope that HealthBench grounds progress towards model development and applications that benefit human health. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºHealthBenchï¼Œè¿™æ˜¯ä¸€ä¸ªè¡¡é‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸæ€§èƒ½å’Œå®‰å…¨çš„å¼€æºåŸºå‡†æµ‹è¯•ã€‚HealthBenchåŒ…å«5000æ¬¡å¤šè½®å¯¹è¯ï¼Œå¯¹è¯å†…å®¹æ˜¯åœ¨æ¨¡å‹å’Œä¸ªäººç”¨æˆ·æˆ–åŒ»ç–—ä¸“ä¸šäººå£«ä¹‹é—´è¿›è¡Œçš„ã€‚å“åº”çš„è¯„ä»·ä½¿ç”¨ç”±262ååŒ»ç”Ÿåˆ›å»ºçš„ç‰¹å®šå¯¹è¯è¯„åˆ†æ ‡å‡†ã€‚ä¸å¥åº·æŠ¤ç†ä¹‹å‰çš„å¤šé¡¹é€‰æ‹©æˆ–ç®€çŸ­ç­”æ¡ˆåŸºå‡†æµ‹è¯•ä¸åŒï¼ŒHealthBenchèƒ½å¤Ÿé€šè¿‡æ¶µç›–å¤šä¸ªå¥åº·èƒŒæ™¯ï¼ˆä¾‹å¦‚ç´§æ€¥æƒ…å†µã€è½¬æ¢ä¸´åºŠæ•°æ®ã€å…¨çƒå¥åº·ï¼‰å’Œè¡Œä¸ºç»´åº¦ï¼ˆä¾‹å¦‚å‡†ç¡®æ€§ã€éµå¾ªæŒ‡ä»¤ã€æ²Ÿé€šï¼‰çš„48562ä¸ªç‹¬ç‰¹è¯„åˆ†æ ‡å‡†ï¼Œè¿›è¡Œç°å®ã€å¼€æ”¾çš„è¯„ä¼°ã€‚è¿‡å»ä¸¤å¹´çš„HealthBenchæ€§èƒ½åæ˜ äº†ç¨³å®šçš„åˆæ­¥è¿›å±•ï¼ˆæ¯”è¾ƒGPT-3.5 Turboçš„16%åˆ°GPT-4oçš„32%ï¼‰ï¼Œä»¥åŠæ›´å¿«çš„è¿‘æœŸæ”¹è¿›ï¼ˆo3å¾—åˆ†ä¸º60%ï¼‰ã€‚å°å‹æ¨¡å‹å°¤å…¶å–å¾—äº†è¿›æ­¥ï¼šGPT-4.1 nanoä¼˜äºGPT-4oï¼Œå¹¶ä¸”æˆæœ¬é™ä½äº†25å€ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸¤ä¸ªHealthBenchå˜ä½“ï¼šHealthBench Consensusï¼ŒåŒ…æ‹¬é€šè¿‡åŒ»ç”Ÿå…±è¯†éªŒè¯çš„æ¨¡å‹è¡Œä¸ºçš„34ä¸ªç‰¹åˆ«é‡è¦çš„ç»´åº¦ï¼›ä»¥åŠHealthBench Hardï¼Œå½“å‰æœ€é«˜å¾—åˆ†ä¸º32%ã€‚æˆ‘ä»¬å¸Œæœ›HealthBenchèƒ½ä¸ºæ¨¡å‹å‘å±•å’Œæœ‰ç›Šäºäººç±»å¥åº·çš„åº”ç”¨å¥ å®šåŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08775v1">PDF</a> Blog: <a target="_blank" rel="noopener" href="https://openai.com/index/healthbench/">https://openai.com/index/healthbench/</a> Code:   <a target="_blank" rel="noopener" href="https://github.com/openai/simple-evals">https://github.com/openai/simple-evals</a></p>
<p><strong>Summary</strong></p>
<p>å¥åº·åŸºå‡†ï¼ˆHealthBenchï¼‰æ˜¯ä¸€ä¸ªè¡¡é‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—é¢†åŸŸæ€§èƒ½å’Œå®‰å…¨çš„å¼€æºåŸºå‡†æµ‹è¯•ã€‚å®ƒåŒ…å«5000æ¬¡æ¨¡å‹ä¸ç”¨æˆ·æˆ–åŒ»ç–—ä¸“ä¸šäººå£«ä¹‹é—´çš„å¤šè½®å¯¹è¯ã€‚å“åº”è¯„ä»·é‡‡ç”¨ç”±262ååŒ»å¸ˆåˆ›å»ºçš„ç‰¹å®šå¯¹è¯æ ‡å‡†çš„è¯„åˆ†ç³»ç»Ÿã€‚ä¸åŒäºä¹‹å‰çš„å¤šé¡¹é€‰æ‹©æˆ–ç®€çŸ­ç­”æ¡ˆåŸºå‡†æµ‹è¯•ï¼Œå¥åº·åŸºå‡†æµ‹è¯•é€šè¿‡æ¶µç›–å¤šä¸ªå¥åº·èƒŒæ™¯å’Œè¡Œä¸ºç»´åº¦çš„48562ä¸ªç‹¬ç‰¹æ ‡å‡†ï¼Œå®ç°äº†ç°å®ã€å¼€æ”¾å¼çš„è¯„ä¼°ã€‚åœ¨è¿‡å»çš„ä¸¤å¹´é‡Œï¼Œéšç€æ¨¡å‹çš„å‘å±•ï¼Œå¥åº·åŸºå‡†æµ‹è¯•çš„å¾—åˆ†å‘ˆç°å‡ºç¨³æ­¥ä¸Šå‡çš„è¶‹åŠ¿ã€‚å°¤å…¶æ˜¯å°å‹æ¨¡å‹çš„è¿›æ­¥æ˜¾è‘—ï¼šGPT-4.1 nanoçš„è¡¨ç°è¶…è¿‡äº†GPT-4oï¼Œå¹¶ä¸”æˆæœ¬ä»…ä¸ºåè€…çš„äºŒåäº”åˆ†ä¹‹ä¸€ã€‚æ­¤å¤–ï¼Œè¿˜æ¨å‡ºäº†å¥åº·åŸºå‡†å…±è¯†å’Œå¥åº·åŸºå‡†éš¾é¢˜ä¸¤ä¸ªç‰ˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥åº·åŸºå‡†ï¼ˆHealthBenchï¼‰æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—é¢†åŸŸçš„æ€§èƒ½å’Œå®‰å…¨çš„å¼€æºåŸºå‡†æµ‹è¯•ã€‚</li>
<li>å®ƒåŒ…å«5000æ¬¡å¤šè½®å¯¹è¯ï¼Œå¯¹è¯å†…å®¹æ¨¡æ‹Ÿäº†æ¨¡å‹ä¸ç”¨æˆ·æˆ–åŒ»ç–—ä¸“ä¸šäººå£«çš„äº’åŠ¨ã€‚</li>
<li>å“åº”è¯„ä»·é‡‡ç”¨ç‰¹å®šå¯¹è¯æ ‡å‡†çš„è¯„åˆ†ç³»ç»Ÿï¼Œç”±262ååŒ»å¸ˆå…±åŒåˆ¶å®šã€‚</li>
<li>å¥åº·åŸºå‡†æµ‹è¯•å®ç°äº†ç°å®ã€å¼€æ”¾å¼çš„è¯„ä¼°ï¼Œæ¶µç›–å¤šä¸ªå¥åº·èƒŒæ™¯å’Œè¡Œä¸ºç»´åº¦ï¼ŒåŒ…æ‹¬ç´§æ€¥æƒ…å†µã€ä¸´åºŠæ•°æ®è½¬æ¢ã€å…¨çƒå¥åº·ç­‰ã€‚</li>
<li>è¿‡å»ä¸¤å¹´é‡Œï¼Œè¯­è¨€æ¨¡å‹åœ¨å¥åº·åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°é€æ¸æé«˜ï¼Œå°¤å…¶æ˜¯å°å‹æ¨¡å‹çš„è¿›æ­¥æ˜¾è‘—ã€‚</li>
<li>GPT-4.1 nanoæ¨¡å‹è¡¨ç°ä¼˜ç§€ï¼Œä¸ä»…è¶…è¶Šäº†GPT-4oï¼Œè€Œä¸”æˆæœ¬æ›´ä¸ºä½å»‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08775">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-139c44d0f0f58e3b983f8182fede2621.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17ca3b2318a585c0f50713ab2356323f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce29c4b483fa0c4f5a4866250d8faccc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28237f2e021ec539f5bee1b0b33238ab.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Towards-Autonomous-UAV-Visual-Object-Search-in-City-Space-Benchmark-and-Agentic-Methodology"><a href="#Towards-Autonomous-UAV-Visual-Object-Search-in-City-Space-Benchmark-and-Agentic-Methodology" class="headerlink" title="Towards Autonomous UAV Visual Object Search in City Space: Benchmark and   Agentic Methodology"></a>Towards Autonomous UAV Visual Object Search in City Space: Benchmark and   Agentic Methodology</h2><p><strong>Authors:Yatai Ji, Zhengqiu Zhu, Yong Zhao, Beidan Liu, Chen Gao, Yihao Zhao, Sihang Qiu, Yue Hu, Quanjun Yin, Yong Li</strong></p>
<p>Aerial Visual Object Search (AVOS) tasks in urban environments require Unmanned Aerial Vehicles (UAVs) to autonomously search for and identify target objects using visual and textual cues without external guidance. Existing approaches struggle in complex urban environments due to redundant semantic processing, similar object distinction, and the exploration-exploitation dilemma. To bridge this gap and support the AVOS task, we introduce CityAVOS, the first benchmark dataset for autonomous search of common urban objects. This dataset comprises 2,420 tasks across six object categories with varying difficulty levels, enabling comprehensive evaluation of UAV agentsâ€™ search capabilities. To solve the AVOS tasks, we also propose PRPSearcher (Perception-Reasoning-Planning Searcher), a novel agentic method powered by multi-modal large language models (MLLMs) that mimics human three-tier cognition. Specifically, PRPSearcher constructs three specialized maps: an object-centric dynamic semantic map enhancing spatial perception, a 3D cognitive map based on semantic attraction values for target reasoning, and a 3D uncertainty map for balanced exploration-exploitation search. Also, our approach incorporates a denoising mechanism to mitigate interference from similar objects and utilizes an Inspiration Promote Thought (IPT) prompting mechanism for adaptive action planning. Experimental results on CityAVOS demonstrate that PRPSearcher surpasses existing baselines in both success rate and search efficiency (on average: +37.69% SR, +28.96% SPL, -30.69% MSS, and -46.40% NE). While promising, the performance gap compared to humans highlights the need for better semantic reasoning and spatial exploration capabilities in AVOS tasks. This work establishes a foundation for future advances in embodied target search. Dataset and source code are available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/CityAVOS-3DF8">https://anonymous.4open.science/r/CityAVOS-3DF8</a>. </p>
<blockquote>
<p>æ— äººæœºåœ¨åŸå¸‚ç¯å¢ƒä¸­çš„ç©ºä¸­è§†è§‰ç›®æ ‡æœç´¢ï¼ˆAVOSï¼‰ä»»åŠ¡è¦æ±‚æ— äººæœºåˆ©ç”¨è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢è‡ªä¸»æœç´¢å¹¶è¯†åˆ«ç›®æ ‡å¯¹è±¡ï¼Œè€Œæ— éœ€å¤–éƒ¨æŒ‡å¯¼ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤æ‚çš„åŸå¸‚ç¯å¢ƒä¸­é¢ä¸´å†—ä½™è¯­ä¹‰å¤„ç†ã€ç›¸ä¼¼å¯¹è±¡åŒºåˆ†ä»¥åŠæ¢ç´¢ä¸åˆ©ç”¨å›°å¢ƒç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·å¹¶æ”¯æŒAVOSä»»åŠ¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†CityAVOSï¼Œè¿™æ˜¯ç”¨äºåŸå¸‚å¸¸è§ç›®æ ‡è‡ªä¸»æœç´¢çš„ç¬¬ä¸€ä¸ªåŸºå‡†æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«å…­ä¸ªç›®æ ‡ç±»åˆ«çš„2,420ä¸ªä»»åŠ¡ï¼Œéš¾åº¦çº§åˆ«å„ä¸ç›¸åŒï¼Œèƒ½å¤Ÿå…¨é¢è¯„ä¼°æ— äººæœºä»£ç†çš„æœç´¢èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³AVOSä»»åŠ¡ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†PRPSearcherï¼ˆæ„ŸçŸ¥-æ¨ç†-è§„åˆ’æœç´¢å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”±å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é©±åŠ¨çš„æ–°å‹ä»£ç†æ–¹æ³•ï¼Œå¯æ¨¡ä»¿äººç±»çš„ä¸‰å±‚è®¤çŸ¥ã€‚å…·ä½“æ¥è¯´ï¼ŒPRPSearcheræ„å»ºäº†ä¸‰å¼ ä¸“é—¨åœ°å›¾ï¼šä¸€å¼ å¢å¼ºç©ºé—´æ„ŸçŸ¥çš„å¯¹è±¡ä¸­å¿ƒåŠ¨æ€è¯­ä¹‰åœ°å›¾ï¼Œä¸€å¼ åŸºäºç›®æ ‡æ¨ç†çš„è¯­ä¹‰å¸å¼•åŠ›å€¼çš„3Dè®¤çŸ¥åœ°å›¾ï¼Œä»¥åŠä¸€å¼ å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨çš„3Dä¸ç¡®å®šæ€§åœ°å›¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜èå…¥äº†ä¸€ç§é™å™ªæœºåˆ¶ï¼Œä»¥å‡è½»ç›¸ä¼¼å¯¹è±¡çš„å¹²æ‰°ï¼Œå¹¶é‡‡ç”¨äº†çµæ„Ÿä¿ƒè¿›æ€è€ƒï¼ˆIPTï¼‰çš„æç¤ºæœºåˆ¶æ¥è¿›è¡Œè‡ªé€‚åº”è¡ŒåŠ¨è§„åˆ’ã€‚åœ¨CityAVOSä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPRPSearcheråœ¨æˆåŠŸç‡å’Œæœç´¢æ•ˆç‡ä¸Šå‡è¶…è¶Šäº†ç°æœ‰åŸºçº¿ï¼ˆå¹³å‡æˆåŠŸç‡+37.69%ï¼ŒSPL+28.96%ï¼ŒMSS-30.69%ï¼ŒNE-46.40%ï¼‰ã€‚å°½ç®¡å¾ˆæœ‰å¸Œæœ›ï¼Œä½†ä¸äººç±»çš„æ€§èƒ½å·®è·è¡¨æ˜ï¼ŒAVOSä»»åŠ¡éœ€è¦æ›´å¥½çš„è¯­ä¹‰æ¨ç†å’Œç©ºé—´æ¢ç´¢èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œä¸ºæœªæ¥çš„åµŒå…¥å¼ç›®æ ‡æœç´¢è¿›å±•å¥ å®šäº†åŸºç¡€ã€‚æ•°æ®é›†å’Œæºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/CityAVOS-3DF8%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/CityAVOS-3DF8ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08765v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨åŸå¸‚ç¯å¢ƒä¸­çš„ç©ºä¸­è§†è§‰ç›®æ ‡æœç´¢ï¼ˆAVOSï¼‰ä»»åŠ¡ä¸­ï¼Œæ— äººæœºéœ€è¦è‡ªä¸»é€šè¿‡è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢æœç´¢å¹¶è¯†åˆ«ç›®æ ‡ç‰©ä½“ï¼Œè€Œæ— éœ€å¤–éƒ¨æŒ‡å¯¼ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚åŸå¸‚ç¯å¢ƒæ—¶é¢ä¸´å†—ä½™è¯­ä¹‰å¤„ç†ã€ç›¸ä¼¼ç‰©ä½“åŒºåˆ†ä»¥åŠæ¢ç´¢ä¸åˆ©ç”¨å›°å¢ƒç­‰é—®é¢˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·å¹¶æ”¯æŒAVOSä»»åŠ¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†CityAVOSæ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹åŸå¸‚å¸¸è§ç›®æ ‡è‡ªä¸»æœç´¢çš„åŸºå‡†æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«å…­ä¸ªä¸åŒéš¾åº¦çº§åˆ«çš„å¯¹è±¡ç±»åˆ«çš„2420ä¸ªä»»åŠ¡ï¼Œèƒ½å¤Ÿå…¨é¢è¯„ä¼°æ— äººæœºæœç´¢èƒ½åŠ›ã€‚ä¸ºè§£å†³AVOSä»»åŠ¡ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†PRPSearcherï¼ˆæ„ŸçŸ¥-æ¨ç†-è§„åˆ’æœç´¢å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”±å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„æ–°å‹ä»£ç†æ–¹æ³•ï¼Œå¯æ¨¡ä»¿äººç±»çš„ä¸‰å±‚è®¤çŸ¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPRPSearcheråœ¨æˆåŠŸç‡å’Œæœç´¢æ•ˆç‡ä¸Šå‡è¶…è¶Šäº†ç°æœ‰åŸºçº¿æ–¹æ³•ã€‚å°½ç®¡ç»“æœå…·æœ‰å‰æ™¯ï¼Œä½†ä¸äººç±»ç›¸æ¯”çš„æ€§èƒ½å·®è·ä»çªæ˜¾äº†AVOSä»»åŠ¡ä¸­æ›´å¥½è¯­ä¹‰æ¨ç†å’Œç©ºé—´æ¢ç´¢èƒ½åŠ›çš„å¿…è¦æ€§ã€‚æœ¬ç ”ç©¶ä¸ºæœªæ¥çš„è¿›æ­¥å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/CityAVOS-3DF8">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸå¸‚ç¯å¢ƒä¸­çš„ç©ºä¸­è§†è§‰ç›®æ ‡æœç´¢ï¼ˆAVOSï¼‰ä»»åŠ¡éœ€è¦æ— äººæœºè‡ªä¸»å®Œæˆç›®æ ‡ç‰©ä½“çš„è§†è§‰å’Œæ–‡æœ¬çº¿ç´¢æœç´¢å’Œè¯†åˆ«ã€‚</li>
<li>å½“å‰æ–¹æ³•åœ¨å¤„ç†å¤æ‚åŸå¸‚ç¯å¢ƒæ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå¦‚å†—ä½™è¯­ä¹‰å¤„ç†ã€ç›¸ä¼¼ç‰©ä½“åŒºåˆ†å’Œæ¢ç´¢ä¸åˆ©ç”¨çš„çŸ›ç›¾ã€‚</li>
<li>ä¸ºæ”¯æŒAVOSä»»åŠ¡ï¼Œå¼•å…¥äº†CityAVOSæ•°æ®é›†ï¼ŒåŒ…å«å…­ä¸ªå¯¹è±¡ç±»åˆ«çš„ä¸åŒéš¾åº¦çº§åˆ«çš„ä»»åŠ¡ï¼Œç”¨äºå…¨é¢è¯„ä¼°æ— äººæœºçš„æœç´¢èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ä»£ç†æ–¹æ³•PRPSearcherï¼Œé€šè¿‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¨¡ä»¿äººç±»çš„ä¸‰å±‚è®¤çŸ¥ç»“æ„æ¥è§£å†³AVOSä»»åŠ¡ã€‚</li>
<li>PRPSearcheré€šè¿‡æ„å»ºä¸‰ç§ç‰¹æ®Šåœ°å›¾ï¼ˆå¯¹è±¡ä¸ºä¸­å¿ƒåŠ¨æ€è¯­ä¹‰å›¾ã€åŸºäºè¯­ä¹‰å¸å¼•åŠ›çš„ä¸‰ç»´è®¤çŸ¥å›¾å’Œä¸‰ç»´ä¸ç¡®å®šæ€§å›¾ï¼‰ä»¥åŠå»å™ªæœºåˆ¶å’Œè‡ªé€‚åº”è¡ŒåŠ¨è§„åˆ’æœºåˆ¶æ¥æé«˜æœç´¢æ•ˆç‡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒPRPSearcheråœ¨æˆåŠŸç‡å’Œæœç´¢æ•ˆç‡ä¸Šè¶…è¶Šäº†ç°æœ‰åŸºçº¿æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08765">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cd9894b728eac030206ff6c4ced6f2b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5cc0ee99a46af69f1baa3b016a60e4b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-01c303c423bf90655c4783f236acb38f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1205d5964b94191687db731b04a6f88f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DeepMath-Creative-A-Benchmark-for-Evaluating-Mathematical-Creativity-of-Large-Language-Models"><a href="#DeepMath-Creative-A-Benchmark-for-Evaluating-Mathematical-Creativity-of-Large-Language-Models" class="headerlink" title="DeepMath-Creative: A Benchmark for Evaluating Mathematical Creativity of   Large Language Models"></a>DeepMath-Creative: A Benchmark for Evaluating Mathematical Creativity of   Large Language Models</h2><p><strong>Authors:Xiaoyang Chen, Xinan Dai, Yu Du, Qian Feng, Naixu Guo, Tingshuo Gu, Yuting Gao, Yingyi Gao, Xudong Han, Xiang Jiang, Yilin Jin, Hongyi Lin, Shisheng Lin, Xiangnan Li, Yuante Li, Yixing Li, Zhentao Lai, Zilu Ma, Yingrong Peng, Jiacheng Qian, Hao-Yu Sun, Jianbo Sun, Zirui Wang, Siwei Wu, Zian Wang, Bin Xu, Jianghao Xu, Yiyang Yu, Zichuan Yang, Hongji Zha, Ruichong Zhang</strong></p>
<p>To advance the mathematical proficiency of large language models (LLMs), the DeepMath team has launched an open-source initiative aimed at developing an open mathematical LLM and systematically evaluating its mathematical creativity. This paper represents the initial contribution of this initiative. While recent developments in mathematical LLMs have predominantly emphasized reasoning skills, as evidenced by benchmarks on elementary to undergraduate-level mathematical tasks, the creative capabilities of these models have received comparatively little attention, and evaluation datasets remain scarce. To address this gap, we propose an evaluation criteria for mathematical creativity and introduce DeepMath-Creative, a novel, high-quality benchmark comprising constructive problems across algebra, geometry, analysis, and other domains. We conduct a systematic evaluation of mainstream LLMsâ€™ creative problem-solving abilities using this dataset. Experimental results show that even under lenient scoring criteria â€“ emphasizing core solution components and disregarding minor inaccuracies, such as small logical gaps, incomplete justifications, or redundant explanations â€“ the best-performing model, O3 Mini, achieves merely 70% accuracy, primarily on basic undergraduate-level constructive tasks. Performance declines sharply on more complex problems, with models failing to provide substantive strategies for open problems. These findings suggest that, although current LLMs display a degree of constructive proficiency on familiar and lower-difficulty problems, such performance is likely attributable to the recombination of memorized patterns rather than authentic creative insight or novel synthesis. </p>
<blockquote>
<p>ä¸ºäº†æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•°å­¦èƒ½åŠ›ï¼ŒDeepMathå›¢é˜Ÿå‘èµ·äº†ä¸€é¡¹å¼€æºå€¡è®®ï¼Œæ—¨åœ¨å¼€å‘ä¸€ä¸ªå¼€æºçš„æ•°å­¦LLMï¼Œå¹¶ç³»ç»Ÿåœ°è¯„ä¼°å…¶æ•°å­¦åˆ›é€ åŠ›ã€‚æœ¬æ–‡ä»£è¡¨äº†è¿™ä¸ªå€¡è®®çš„åˆæ­¥è´¡çŒ®ã€‚è™½ç„¶æœ€è¿‘çš„æ•°å­¦LLMå‘å±•ä¸»è¦å¼ºè°ƒæ¨ç†èƒ½åŠ›ï¼Œæ­£å¦‚åœ¨åŸºç¡€åˆ°å¤§å­¦æ°´å¹³çš„æ•°å­¦ä»»åŠ¡åŸºå‡†æµ‹è¯•ä¸­æ‰€è¯æ˜çš„é‚£æ ·ï¼Œä½†è¿™äº›æ¨¡å‹çš„åˆ›é€ æ€§èƒ½åŠ›ç›¸æ¯”ä¹‹ä¸‹å—åˆ°çš„å…³æ³¨è¾ƒå°‘ï¼Œè¯„ä¼°æ•°æ®é›†ä»ç„¶ç¨€ç¼ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†æ•°å­¦åˆ›é€ åŠ›çš„è¯„ä¼°æ ‡å‡†ï¼Œå¹¶ä»‹ç»äº†DeepMath-Creativeï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„é«˜è´¨é‡åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä»£æ•°ã€å‡ ä½•ã€åˆ†æå’Œå…¶ä»–é¢†åŸŸçš„æ„é€ æ€§é—®é¢˜ã€‚æˆ‘ä»¬ä½¿ç”¨æ­¤æ•°æ®é›†ç³»ç»Ÿåœ°è¯„ä¼°ä¸»æµLLMçš„åˆ›é€ æ€§é—®é¢˜è§£å†³èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿åœ¨å®½æ¾çš„è¯„åˆ†æ ‡å‡†ä¸‹â€”â€”å¼ºè°ƒæ ¸å¿ƒè§£å†³æ–¹æ¡ˆæˆåˆ†ï¼Œå¹¶å¿½ç•¥å°çš„ä¸å‡†ç¡®ä¹‹å¤„ï¼Œå¦‚å°çš„é€»è¾‘å·®è·ã€ä¸å®Œæ•´ç†ç”±æˆ–å†—ä½™è§£é‡Šâ€”â€”è¡¨ç°æœ€ä½³çš„æ¨¡å‹O3 Miniä¹Ÿä»…è¾¾åˆ°70%çš„å‡†ç¡®ç‡ï¼Œä¸»è¦æ˜¯åœ¨åŸºæœ¬çš„æœ¬ç§‘æ°´å¹³æ„é€ æ€§ä»»åŠ¡ä¸Šã€‚åœ¨æ›´å¤æ‚çš„é—®é¢˜ä¸Šï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™ï¼Œæ¨¡å‹æ— æ³•ä¸ºå¼€æ”¾é—®é¢˜æä¾›å®è´¨æ€§çš„ç­–ç•¥ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå°½ç®¡å½“å‰LLMåœ¨ç†Ÿæ‚‰å’Œè¾ƒä½éš¾åº¦çš„é—®é¢˜ä¸Šæ˜¾ç¤ºå‡ºä¸€å®šç¨‹åº¦çš„æ„é€ æ€§èƒ½åŠ›ï¼Œä½†è¿™ç§è¡¨ç°å¾ˆå¯èƒ½å½’å› äºè®°å¿†æ¨¡å¼çš„é‡ç»„ï¼Œè€Œä¸æ˜¯çœŸæ­£çš„åˆ›é€ åŠ›æˆ–æ–°é¢–çš„ç»¼åˆèƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08744v1">PDF</a> 14 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>DeepMathå›¢é˜Ÿæ¨å‡ºäº†ä¸€é¡¹å¼€æºå€¡è®®ï¼Œæ—¨åœ¨å¼€å‘ä¸€ä¸ªå¼€æ”¾çš„æ•°å­¦å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¹¶ç³»ç»Ÿåœ°è¯„ä¼°å…¶æ•°å­¦åˆ›é€ åŠ›ã€‚ä¸ºè§£å†³å½“å‰æ•°å­¦LLMåœ¨åˆ›é€ åŠ›è¯„ä»·æ–¹é¢çš„ä¸è¶³ï¼Œå›¢é˜Ÿæå‡ºäº†æ•°å­¦åˆ›é€ åŠ›çš„è¯„ä»·æ ‡å‡†å’Œæ–°å‹é«˜è´¨é‡åŸºå‡†æµ‹è¯•DeepMath-Creativeã€‚ä½¿ç”¨æ­¤æ•°æ®é›†å¯¹ä¸»æµLLMçš„åˆ›é€ æ€§è§£å†³é—®é¢˜çš„èƒ½åŠ›è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿åœ¨å®½æ¾è¯„åˆ†æ ‡å‡†ä¸‹ï¼Œç°æœ‰æ¨¡å‹çš„æ€§èƒ½ä»æœ‰é™ï¼Œé¢ä¸´å¤æ‚é—®é¢˜æ—¶æ— æ³•æä¾›å®è´¨æ€§è§£å†³æ–¹æ¡ˆã€‚è¿™è¡¨æ˜å½“å‰LLMçš„è¡¨ç°æ›´å¤šä¾èµ–äºå¯¹è®°å¿†æ¨¡å¼çš„é‡ç»„ï¼Œè€ŒéçœŸæ­£çš„åˆ›é€ æ€§æ´å¯Ÿæˆ–æ–°é¢–çš„ç»¼åˆèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepMathå›¢é˜Ÿæå‡ºäº†ä¸€ä¸ªæ—¨åœ¨å¼€å‘æ•°å­¦LLMå¹¶è¯„ä¼°å…¶æ•°å­¦åˆ›é€ åŠ›çš„å¼€æºå€¡è®®ã€‚</li>
<li>å½“å‰æ•°å­¦LLMä¸»è¦ä¾§é‡äºæ¨ç†èƒ½åŠ›ï¼Œè€Œå¯¹å…¶åˆ›é€ æ€§èƒ½åŠ›å…³æ³¨è¾ƒå°‘ã€‚</li>
<li>å›¢é˜Ÿå¼•å…¥äº†æ•°å­¦åˆ›é€ åŠ›çš„è¯„ä»·æ ‡å‡†å’Œæ–°å‹åŸºå‡†æµ‹è¯•DeepMath-Creativeã€‚</li>
<li>ä½¿ç”¨DeepMath-Creativeè¿›è¡Œçš„ç³»ç»Ÿè¯„ä¼°æ˜¾ç¤ºï¼Œä¸»æµLLMåœ¨åˆ›é€ æ€§è§£å†³é—®é¢˜æ–¹é¢çš„èƒ½åŠ›æœ‰é™ã€‚</li>
<li>åœ¨å®½æ¾è¯„åˆ†æ ‡å‡†ä¸‹ï¼Œæœ€ä½³æ¨¡å‹O3 Miniåœ¨åŸºæœ¬æœ¬ç§‘ç”Ÿæ°´å¹³çš„æ„é€ ä»»åŠ¡ä¸Šä»…è¾¾åˆ°70%çš„å‡†ç¡®ç‡ã€‚</li>
<li>åœ¨å¤æ‚é—®é¢˜ä¸Šï¼ŒLLMçš„å®è´¨æ€§ç­–ç•¥ç¼ºå¤±ï¼Œéš¾ä»¥æä¾›æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08744">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-de6f7f352be92412972b8a507d6b14f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-989f5f605315e84f4e3ff59c84dad96c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Securing-RAG-A-Risk-Assessment-and-Mitigation-Framework"><a href="#Securing-RAG-A-Risk-Assessment-and-Mitigation-Framework" class="headerlink" title="Securing RAG: A Risk Assessment and Mitigation Framework"></a>Securing RAG: A Risk Assessment and Mitigation Framework</h2><p><strong>Authors:Lukas Ammann, Sara Ott, Christoph R. Landolt, Marco P. Lehmann</strong></p>
<p>Retrieval Augmented Generation (RAG) has emerged as the de facto industry standard for user-facing NLP applications, offering the ability to integrate data without re-training or fine-tuning Large Language Models (LLMs). This capability enhances the quality and accuracy of responses but also introduces novel security and privacy challenges, particularly when sensitive data is integrated. With the rapid adoption of RAG, securing data and services has become a critical priority. This paper first reviews the vulnerabilities of RAG pipelines, and outlines the attack surface from data pre-processing and data storage management to integration with LLMs. The identified risks are then paired with corresponding mitigations in a structured overview. In a second step, the paper develops a framework that combines RAG-specific security considerations, with existing general security guidelines, industry standards, and best practices. The proposed framework aims to guide the implementation of robust, compliant, secure, and trustworthy RAG systems. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å·²æˆä¸ºé¢å‘ç”¨æˆ·çš„NLPåº”ç”¨çš„è¡Œä¸šå®é™…æ ‡å‡†ï¼Œå®ƒæä¾›äº†åœ¨æ— éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æƒ…å†µä¸‹æ•´åˆæ•°æ®çš„èƒ½åŠ›ã€‚è¿™ä¸€åŠŸèƒ½æé«˜äº†å“åº”çš„è´¨é‡å’Œå‡†ç¡®æ€§ï¼Œä½†ä¹Ÿå¸¦æ¥äº†æ–°çš„å®‰å…¨å’Œéšç§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ•´åˆæ•æ„Ÿæ•°æ®æ—¶ã€‚éšç€RAGçš„å¿«é€Ÿé‡‡ç”¨ï¼Œä¿éšœæ•°æ®å’ŒæœåŠ¡çš„å®‰å…¨å·²æˆä¸ºè‡³å…³é‡è¦çš„ä¼˜å…ˆäº‹é¡¹ã€‚æœ¬æ–‡é¦–å…ˆå›é¡¾äº†RAGç®¡é“çš„å®‰å…¨æ¼æ´ï¼Œå¹¶æ¦‚è¿°äº†ä»æ•°æ®é¢„å¤„ç†å’Œæ•°æ®å­˜å‚¨ç®¡ç†åˆ°ä¸LLMé›†æˆçš„æ”»å‡»é¢ã€‚ç„¶åï¼Œå°†ç¡®å®šçš„é£é™©ä¸ç›¸åº”çš„ç¼“è§£æªæ–½åœ¨ç»“æ„åŒ–æ¦‚è¿°ä¸­é…å¯¹ã€‚åœ¨ç¬¬äºŒæ­¥ä¸­ï¼Œæœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†RAGç‰¹å®šçš„å®‰å…¨è€ƒè™‘å› ç´ ã€ç°æœ‰çš„é€šç”¨å®‰å…¨å‡†åˆ™ã€è¡Œä¸šæ ‡å‡†å’Œæœ€ä½³å®è·µã€‚æ‰€æå‡ºçš„æ¡†æ¶æ—¨åœ¨æŒ‡å¯¼å®æ–½ç¨³å¥ã€åˆè§„ã€å®‰å…¨å’Œå¯ä¿¡èµ–çš„RAGç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08728v1">PDF</a> 8 pages, 3 figures, Sara Ott and Lukas Ammann contributed equally</p>
<p><strong>Summary</strong><br>å¤§è¦æ¨¡èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰é›†æˆä¸­çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å·²æˆä¸ºé¢å‘ç”¨æˆ·çš„NLPåº”ç”¨ç¨‹åºçš„è¡Œä¸šæ ‡å‡†ï¼Œå¯åœ¨ä¸é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹çš„æƒ…å†µä¸‹æ•´åˆæ•°æ®ã€‚è¿™é¡¹åŠŸèƒ½å¢å¼ºäº†å“åº”çš„è´¨é‡å’Œå‡†ç¡®æ€§ï¼Œä½†ä¹Ÿå¸¦æ¥äº†æ–°çš„å®‰å…¨å’Œéšç§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨é›†æˆæ•æ„Ÿæ•°æ®æ—¶ã€‚æœ¬ç»¼è¿°ä»‹ç»äº†RAGç®¡é“çš„å®‰å…¨æ¼æ´ï¼Œå¹¶æ¦‚è¿°äº†ä»æ•°æ®é¢„å¤„ç†å’Œå­˜å‚¨ç®¡ç†åˆ°ä¸LLMé›†æˆçš„æ”»å‡»é¢ã€‚æ‰€æå‡ºçš„é£é™©ä¸ç›¸åº”çš„ç¼“è§£æªæ–½è¢«ç»“æ„åŒ–æ¦‚è¿°ã€‚æœ¬æ–‡æ¥ç€å¼€å‘äº†ä¸€ä¸ªç»“åˆäº†RAGç‰¹æœ‰çš„å®‰å…¨è€ƒè™‘å› ç´ ä»¥åŠç°æœ‰çš„ä¸€èˆ¬å®‰å…¨å‡†åˆ™ã€è¡Œä¸šæ ‡å‡†å’Œæœ€ä½³å®è·µçš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨æŒ‡å¯¼å®æ–½ç¨³å¥ã€åˆè§„ã€å®‰å…¨å’Œå¯é çš„RAGç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAGå·²æˆä¸ºNLPåº”ç”¨çš„è¡Œä¸šæ ‡å‡†ï¼Œå¯ä»¥é›†æˆæ•°æ®è€Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–å¾®è°ƒLLMï¼Œæé«˜äº†å“åº”è´¨é‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>RAGå¼•å…¥æ–°çš„å®‰å…¨å’Œéšç§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ•æ„Ÿæ•°æ®æ—¶ã€‚</li>
<li>æœ¬æ–‡è¯¦ç»†å›é¡¾äº†RAGçš„å®‰å…¨æ¼æ´ï¼Œå¹¶æè¿°äº†ä»æ•°æ®é¢„å¤„ç†åˆ°LLMé›†æˆçš„æ”»å‡»é¢ã€‚</li>
<li>é’ˆå¯¹RAGçš„å®‰å…¨é£é™©ï¼Œæå‡ºäº†ç›¸åº”çš„ç¼“è§£æªæ–½ã€‚</li>
<li>ç»“åˆRAGç‰¹æœ‰çš„å®‰å…¨è€ƒè™‘å› ç´ ï¼Œæœ¬ç»¼è¿°å¼€å‘äº†ç»¼åˆæ€§çš„å®‰å…¨æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†ç°æœ‰çš„ä¸€èˆ¬å®‰å…¨å‡†åˆ™ã€è¡Œä¸šæ ‡å‡†å’Œæœ€ä½³å®è·µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08728">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-701a5e98b32cb779e325ec29167a4652.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fbe2e6f0120a29dee495a82d44d8095.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7eb10079c0a035beb4b3a744a5f29f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-32ac657133189ecaa9a7bc82386a813f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LLM-based-Prompt-Ensemble-for-Reliable-Medical-Entity-Recognition-from-EHRs"><a href="#LLM-based-Prompt-Ensemble-for-Reliable-Medical-Entity-Recognition-from-EHRs" class="headerlink" title="LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from   EHRs"></a>LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from   EHRs</h2><p><strong>Authors:K M Sajjadul Islam, Ayesha Siddika Nipu, Jiawei Wu, Praveen Madiraju</strong></p>
<p>Electronic Health Records (EHRs) are digital records of patient information, often containing unstructured clinical text. Named Entity Recognition (NER) is essential in EHRs for extracting key medical entities like problems, tests, and treatments to support downstream clinical applications. This paper explores prompt-based medical entity recognition using large language models (LLMs), specifically GPT-4o and DeepSeek-R1, guided by various prompt engineering techniques, including zero-shot, few-shot, and an ensemble approach. Among all strategies, GPT-4o with prompt ensemble achieved the highest classification performance with an F1-score of 0.95 and recall of 0.98, outperforming DeepSeek-R1 on the task. The ensemble method improved reliability by aggregating outputs through embedding-based similarity and majority voting. </p>
<blockquote>
<p>ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰æ˜¯æ‚£è€…ä¿¡æ¯çš„æ•°å­—è®°å½•ï¼Œé€šå¸¸åŒ…å«éç»“æ„çš„ä¸´åºŠæ–‡æœ¬ã€‚åœ¨ç”µå­å¥åº·è®°å½•ä¸­ï¼Œå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰å¯¹äºæå–å…³é”®åŒ»ç–—å®ä½“è‡³å…³é‡è¦ï¼Œå¦‚é—®é¢˜ã€æµ‹è¯•å’Œæ²»ç–—æ–¹æ³•ï¼Œä»¥æ”¯æŒä¸‹æ¸¸çš„ä¸´åºŠåº”ç”¨ã€‚æœ¬æ–‡æ¢è®¨äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åŒ»ç–—å®ä½“è¯†åˆ«ï¼ˆLLMï¼‰ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨GPT-4oå’ŒDeepSeek-R1æ¨¡å‹è¿›è¡Œæ¢ç´¢ï¼Œè¿™äº›æ¨¡å‹ç”±å„ç§æç¤ºå·¥ç¨‹æŠ€æœ¯å¼•å¯¼ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œé›†æˆæ–¹æ³•ã€‚åœ¨æ‰€æœ‰ç­–ç•¥ä¸­ï¼Œä½¿ç”¨æç¤ºé›†æˆçš„GPT-4oåœ¨F1åˆ†æ•°ä¸Šè¾¾åˆ°äº†æœ€é«˜çš„åˆ†ç±»æ€§èƒ½ä¸º0.95å’Œå¬å›ç‡ä¸º0.98ã€‚åœ¨æ­¤ä»»åŠ¡ä¸Šè¶…è¿‡äº†DeepSeek-R1æ¨¡å‹çš„è¡¨ç°ã€‚é›†æˆæ–¹æ³•é€šè¿‡åŸºäºåµŒå…¥çš„ç›¸ä¼¼æ€§å’Œå¤šæ•°æŠ•ç¥¨æ¥èšåˆè¾“å‡ºï¼Œæé«˜äº†å¯é æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08704v1">PDF</a> IEEE 26th International Conference on Information Reuse and   Integration for Data Science (IRI 2025), San Jose, CA, USA</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰åœ¨ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶é‡‡ç”¨äº†GPT-4oå’ŒDeepSeek-R1æ¨¡å‹ï¼Œå¹¶è¿ç”¨äº†å¤šç§æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œå¦‚é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œé›†æˆæ–¹æ³•ã€‚å…¶ä¸­ï¼ŒGPT-4oä¸æç¤ºé›†æˆç­–ç•¥å–å¾—äº†æœ€é«˜çš„åˆ†ç±»æ€§èƒ½ï¼ŒF1åˆ†æ•°è¾¾åˆ°0.95ï¼Œå¬å›ç‡è¾¾åˆ°0.98ï¼Œåœ¨ä»»åŠ¡ä¸Šä¼˜äºDeepSeek-R1ã€‚é›†æˆæ–¹æ³•é€šè¿‡åŸºäºåµŒå…¥çš„ç›¸ä¼¼æ€§å’Œå¤šæ•°æŠ•ç¥¨æ¥èšåˆè¾“å‡ºï¼Œæé«˜äº†å¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰åŒ…å«æ‚£è€…ä¿¡æ¯çš„æ•°å­—è®°å½•ï¼Œå…¶ä¸­æ¶‰åŠå¤§é‡çš„éç»“æ„åŒ–ä¸´åºŠæ–‡æœ¬ã€‚</li>
<li>å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰åœ¨EHRsä¸­éå¸¸é‡è¦ï¼Œå¯æå–å…³é”®åŒ»ç–—å®ä½“å¦‚ç–¾ç—…ã€æµ‹è¯•å’Œæ²»ç–—æ–¹æ³•ï¼Œä»¥æ”¯æŒä¸‹æ¸¸ä¸´åºŠåº”ç”¨ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå¦‚GPT-4oå’ŒDeepSeek-R1ï¼Œè¿›è¡ŒåŸºäºæç¤ºçš„åŒ»å­¦å®ä½“è¯†åˆ«ã€‚</li>
<li>æç¤ºå·¥ç¨‹æŠ€æœ¯åŒ…æ‹¬é›¶æ ·æœ¬ã€å°‘æ ·æœ¬æ–¹æ³•è¢«è¿ç”¨äºæ¨¡å‹è®­ç»ƒä¸­ã€‚</li>
<li>GPT-4oä¸æç¤ºé›†æˆç­–ç•¥è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼ŒF1åˆ†æ•°é«˜è¾¾0.95ï¼Œå¬å›ç‡ä¸º0.98ã€‚</li>
<li>é›†æˆæ–¹æ³•é€šè¿‡åŸºäºåµŒå…¥çš„ç›¸ä¼¼æ€§å’Œå¤šæ•°æŠ•ç¥¨æœºåˆ¶æé«˜äº†æ€§èƒ½å¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08704">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f6117f0837716de1ed4f7266a0a73f21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-629d509339aac6aaa5a0276c0c40c6b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e61795bda67a7a018dcf609069aa2c3a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3921432b3d1e2a445e857c03a2917396.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee8a0816c818be8f3d8ad48739197dc9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2ea5b898bc1b1646a2ede2a83cab71e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f977aab0ec910fe8a2200e0df2a0b50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64e88b2adba0df149f380f5a976a0583.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Granite-speech-open-source-speech-aware-LLMs-with-strong-English-ASR-capabilities"><a href="#Granite-speech-open-source-speech-aware-LLMs-with-strong-English-ASR-capabilities" class="headerlink" title="Granite-speech: open-source speech-aware LLMs with strong English ASR   capabilities"></a>Granite-speech: open-source speech-aware LLMs with strong English ASR   capabilities</h2><p><strong>Authors:George Saon, Avihu Dekel, Alexander Brooks, Tohru Nagano, Abraham Daniels, Aharon Satt, Ashish Mittal, Brian Kingsbury, David Haws, Edmilson Morais, Gakuto Kurata, Hagai Aronowitz, Ibrahim Ibrahim, Jeff Kuo, Kate Soule, Luis Lastras, Masayuki Suzuki, Ron Hoory, Samuel Thomas, Sashi Novitasari, Takashi Fukuda, Vishal Sunder, Xiaodong Cui, Zvi Kons</strong></p>
<p>Granite-speech LLMs are compact and efficient speech language models specifically designed for English ASR and automatic speech translation (AST). The models were trained by modality aligning the 2B and 8B parameter variants of granite-3.3-instruct to speech on publicly available open-source corpora containing audio inputs and text targets consisting of either human transcripts for ASR or automatically generated translations for AST. Comprehensive benchmarking shows that on English ASR, which was our primary focus, they outperform several competitorsâ€™ models that were trained on orders of magnitude more proprietary data, and they keep pace on English-to-X AST for major European languages, Japanese, and Chinese. The speech-specific components are: a conformer acoustic encoder using block attention and self-conditioning trained with connectionist temporal classification, a windowed query-transformer speech modality adapter used to do temporal downsampling of the acoustic embeddings and map them to the LLM text embedding space, and LoRA adapters to further fine-tune the text LLM. Granite-speech-3.3 operates in two modes: in speech mode, it performs ASR and AST by activating the encoder, projector, and LoRA adapters; in text mode, it calls the underlying granite-3.3-instruct model directly (without LoRA), essentially preserving all the text LLM capabilities and safety. Both models are freely available on HuggingFace (<a target="_blank" rel="noopener" href="https://huggingface.co/ibm-granite/granite-speech-3.3-2b">https://huggingface.co/ibm-granite/granite-speech-3.3-2b</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/ibm-granite/granite-speech-3.3-8b">https://huggingface.co/ibm-granite/granite-speech-3.3-8b</a>) and can be used for both research and commercial purposes under a permissive Apache 2.0 license. </p>
<blockquote>
<p>èŠ±å²—å²©è¯­éŸ³LLMæ˜¯ä¸ºè‹±è¯­è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè‡ªåŠ¨è¯­éŸ³ç¿»è¯‘ï¼ˆASTï¼‰ä¸“é—¨è®¾è®¡çš„å°å‹ã€é«˜æ•ˆçš„è¯­éŸ³è¯­è¨€æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹é€šè¿‡æ¨¡æ€å¯¹é½å¯¹èŠ±å²—å²©3.3-instructçš„2Bå’Œ8Bå‚æ•°å˜ä½“è¿›è¡Œè®­ç»ƒï¼Œåœ¨å…¬ä¼—å¯è®¿é—®çš„å¼€æºè¯­æ–™åº“ä¸Šè¿›è¡Œè¯­éŸ³è®­ç»ƒï¼Œè¿™äº›è¯­æ–™åº“åŒ…å«ç”±äººç±»è½¬å½•çš„ASRéŸ³é¢‘è¾“å…¥æˆ–è‡ªåŠ¨ç”Ÿæˆçš„ç¿»è¯‘ASTæ–‡æœ¬ç›®æ ‡ã€‚å…¨é¢çš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„ä¸»è¦å…³æ³¨ç‚¹è‹±è¯­ASRä¸Šï¼Œå®ƒä»¬çš„æ€§èƒ½è¶…è¿‡äº†ç”¨å¤§é‡ä¸“æœ‰æ•°æ®è®­ç»ƒçš„ç«äº‰å¯¹æ‰‹çš„æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨ä¸»è¦çš„æ¬§æ´²è¯­è¨€ã€æ—¥è¯­å’Œä¸­æ–‡çš„è‹±è¯­åˆ°Xçš„ASTä¸Šä¹Ÿèƒ½ä¸ä¹‹ç«äº‰ã€‚è¯­éŸ³ç‰¹å®šç»„ä»¶åŒ…æ‹¬ï¼šä½¿ç”¨å—æ³¨æ„åŠ›å’Œè‡ªè°ƒèŠ‚è®­ç»ƒçš„è¿æ¥ä¸»ä¹‰è€…æ—¶é—´åˆ†ç±»æ³•çš„conformerå£°éŸ³ç¼–ç å™¨ï¼Œç”¨äºå¯¹å£°éŸ³åµŒå…¥è¿›è¡Œæ—¶é—´ä¸‹é‡‡æ ·å¹¶å°†å…¶æ˜ å°„åˆ°LLMæ–‡æœ¬åµŒå…¥ç©ºé—´çš„çª—å£æŸ¥è¯¢å˜å‹å™¨è¯­éŸ³æ¨¡æ€é€‚é…å™¨ï¼Œä»¥åŠLoRAé€‚é…å™¨æ¥è¿›ä¸€æ­¥å¾®è°ƒæ–‡æœ¬LLMã€‚èŠ±å²—å²©è¯­éŸ³3.3æœ‰ä¸¤ç§æ¨¡å¼ï¼šåœ¨è¯­éŸ³æ¨¡å¼ä¸‹ï¼Œå®ƒé€šè¿‡æ¿€æ´»ç¼–ç å™¨ã€æŠ•å½±ä»ªå’ŒLoRAé€‚é…å™¨æ¥æ‰§è¡ŒASRå’ŒASTï¼›åœ¨æ–‡æœ¬æ¨¡å¼ä¸‹ï¼Œå®ƒç›´æ¥è°ƒç”¨åº•å±‚çš„èŠ±å²—å²©3.3-instructæ¨¡å‹ï¼ˆä¸ä½¿ç”¨LoRAï¼‰ï¼ŒåŸºæœ¬ä¸Šä¿ç•™äº†æ‰€æœ‰æ–‡æœ¬LLMçš„åŠŸèƒ½å’Œå®‰å…¨ã€‚è¿™ä¸¤ä¸ªæ¨¡å‹éƒ½å¯ä»¥åœ¨HuggingFaceä¸Šå…è´¹è·å¾—ï¼ˆ<a target="_blank" rel="noopener" href="https://huggingface.co/ibm-granite/granite-speech-3.3-2b">https://huggingface.co/ibm-granite/granite-speech-3.3-2b</a> å’Œ <a target="_blank" rel="noopener" href="https://huggingface.co/ibm-granite/granite-speech-3.3-8b%EF%BC%89%EF%BC%8C%E5%B9%B6%E4%B8%94%E5%8F%AF%E4%BB%A5%E5%9C%A8%E7%A0%94%E7%A9%B6%E5%92%8C%E5%95%86%E4%B8%9A%E7%94%A8%E9%80%94%E4%B8%8B%E6%A0%B9%E6%8D%AEApache">https://huggingface.co/ibm-granite/granite-speech-3.3-8bï¼‰ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ç ”ç©¶å’Œå•†ä¸šç”¨é€”ä¸‹æ ¹æ®Apache</a> 2.0è®¸å¯è¯ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08699v1">PDF</a> 7 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ç¯‡å…³äºGranite-speech LLMæ¨¡å‹çš„ä»‹ç»ã€‚è¯¥æ¨¡å‹æ˜¯ä¸ºè‹±è¯­è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè‡ªåŠ¨è¯­éŸ³ç¿»è¯‘ï¼ˆASTï¼‰è®¾è®¡çš„ç´§å‡‘é«˜æ•ˆçš„è¯­éŸ³è¯­è¨€æ¨¡å‹ã€‚æ¨¡å‹é€šè¿‡æ¨¡æ€å¯¹é½è®­ç»ƒï¼Œå¯åœ¨å…¬å¼€å¯ç”¨çš„å¼€æºè¯­æ–™åº“ä¸Šè¿›è¡Œè‹±è¯­ASRå’Œä¸»è¦æ¬§æ´²è¯­è¨€ã€æ—¥è¯­å’Œä¸­æ–‡çš„è‹±è¯­è‡³Xçš„ASTä»»åŠ¡ã€‚å®ƒåŒ…å«è¯­éŸ³ç‰¹å®šçš„ç»„ä»¶ï¼Œå¦‚ä½¿ç”¨å—æ³¨æ„åŠ›å’Œè‡ªè°ƒèŠ‚è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œå£°å­¦ç¼–ç å™¨ï¼Œç”¨äºæ‰§è¡Œæ—¶åºåˆ†ç±»çš„çª—å£æŸ¥è¯¢è½¬æ¢å™¨è¯­éŸ³æ¨¡æ€é€‚é…å™¨å’ŒLoRAé€‚é…å™¨ã€‚Granite-speechæ¨¡å‹æœ‰ä¸¤ç§æ¨¡å¼ï¼šè¯­éŸ³æ¨¡å¼ä¸‹æ‰§è¡ŒASRå’ŒASTï¼Œæ–‡æœ¬æ¨¡å¼ä¸‹åˆ™ç›´æ¥è°ƒç”¨åº•å±‚çš„granite-3.3-instructæ¨¡å‹ï¼Œä¿ç•™æ‰€æœ‰æ–‡æœ¬LLMåŠŸèƒ½å’Œå®‰å…¨æ€§ã€‚è¿™äº›æ¨¡å‹å¯åœ¨Hugging Faceä¸Šå…è´¹ä½¿ç”¨ï¼Œå¹¶å¯ç”¨äºç ”ç©¶å’Œå•†ä¸šç›®çš„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Granite-speech LLMsæ˜¯ä¸“ä¸ºè‹±è¯­ASRå’ŒASTè®¾è®¡çš„ç´§å‡‘é«˜æ•ˆçš„è¯­è¨€æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹é€šè¿‡æ¨¡æ€å¯¹é½è®­ç»ƒï¼Œå¯åœ¨å…¬å¼€å¯ç”¨çš„å¼€æºè¯­æ–™åº“ä¸Šè¿›è¡Œè‹±è¯­ASRä»»åŠ¡ã€‚</li>
<li>å®ƒåœ¨è‹±è¯­ASRæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç”šè‡³è¶…è¶Šäº†ä½¿ç”¨å¤§é‡ä¸“æœ‰æ•°æ®è®­ç»ƒçš„ç«äº‰å¯¹æ‰‹æ¨¡å‹ã€‚</li>
<li>æ¨¡å‹å¯¹äºä¸»è¦æ¬§æ´²è¯­è¨€ã€æ—¥è¯­å’Œä¸­æ–‡çš„è‹±è¯­è‡³Xçš„ASTä»»åŠ¡ä¹Ÿæœ‰è‰¯å¥½è¡¨ç°ã€‚</li>
<li>Granite-speechæ¨¡å‹åŒ…å«è¯­éŸ³ç‰¹å®šçš„ç»„ä»¶ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œå£°å­¦ç¼–ç å™¨å’Œè¯­éŸ³æ¨¡æ€é€‚é…å™¨ã€‚</li>
<li>æ¨¡å‹å…·æœ‰ä¸¤ç§æ¨¡å¼ï¼šè¯­éŸ³æ¨¡å¼å’Œæ–‡æœ¬æ¨¡å¼ï¼Œå¯ä»¥åœ¨ä¸åŒåœºæ™¯ä¸‹çµæ´»ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08699">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fc72260ffcc1219216a5194cd398632a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e632cac9145f67384950bf69edee463a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbb6eef0de4be212ea422f418e10c8c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-439237ceb1f6ab4a666fb18a2645e68f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45ea1134d9964a4d1866894b947ebb75.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4cc28e88215bff331be9c263d84a9718.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09bf101b8358322e1a14cfb8d4ef8d31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c05cec89983769300b561da0351ef84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-70e92b64284c5a773aaaaecb6d4551d7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Revealing-economic-facts-LLMs-know-more-than-they-say"><a href="#Revealing-economic-facts-LLMs-know-more-than-they-say" class="headerlink" title="Revealing economic facts: LLMs know more than they say"></a>Revealing economic facts: LLMs know more than they say</h2><p><strong>Authors:Marcus Buckmann, Quynh Anh Nguyen, Edward Hill</strong></p>
<p>We investigate whether the hidden states of large language models (LLMs) can be used to estimate and impute economic and financial statistics. Focusing on county-level (e.g. unemployment) and firm-level (e.g. total assets) variables, we show that a simple linear model trained on the hidden states of open-source LLMs outperforms the modelsâ€™ text outputs. This suggests that hidden states capture richer economic information than the responses of the LLMs reveal directly. A learning curve analysis indicates that only a few dozen labelled examples are sufficient for training. We also propose a transfer learning method that improves estimation accuracy without requiring any labelled data for the target variable. Finally, we demonstrate the practical utility of hidden-state representations in super-resolution and data imputation tasks. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„éšè—çŠ¶æ€æ˜¯å¦å¯ä»¥ç”¨äºä¼°è®¡å’Œå¡«è¡¥ç»æµå’Œé‡‘èç»Ÿè®¡æ•°æ®ã€‚æˆ‘ä»¬ä¸“æ³¨äºå¿çº§ï¼ˆå¦‚å¤±ä¸šç‡ï¼‰å’Œä¼ä¸šçº§ï¼ˆå¦‚æ€»èµ„äº§ï¼‰çš„å˜é‡ï¼Œå‘ç°åŸºäºå¼€æºLLMéšè—çŠ¶æ€è®­ç»ƒçš„ç®€å•çº¿æ€§æ¨¡å‹è¡¨ç°ä¼˜äºLLMçš„æ–‡æœ¬è¾“å‡ºã€‚è¿™è¡¨æ˜éšè—çŠ¶æ€æ•æ‰åˆ°çš„ç»æµä¿¡æ¯æ¯”LLMçš„ç›´æ¥å›åº”æ›´ä¸°å¯Œã€‚å­¦ä¹ æ›²çº¿åˆ†æè¡¨æ˜ï¼Œåªéœ€å‡ åä¸ªæ ‡è®°æ ·æœ¬å°±è¶³ä»¥è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸éœ€è¦ç›®æ ‡å˜é‡çš„æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹æé«˜ä¼°è®¡ç²¾åº¦ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºäº†éšè—çŠ¶æ€è¡¨ç¤ºåœ¨è¶…åˆ†è¾¨ç‡å’Œæ•°æ®å¡«è¡¥ä»»åŠ¡ä¸­çš„å®é™…æ•ˆç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08662v1">PDF</a> 34 pages, 17 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„éšè—çŠ¶æ€å¯ç”¨äºä¼°è®¡å’Œå¡«è¡¥ç»æµä¸è´¢åŠ¡ç»Ÿè®¡æ•°æ®ã€‚ç ”ç©¶å…³æ³¨å¿çº§ï¼ˆå¦‚å¤±ä¸šç‡ï¼‰å’Œä¼ä¸šçº§ï¼ˆå¦‚æ€»èµ„äº§ï¼‰å˜é‡ï¼Œæ˜¾ç¤ºåŸºäºå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹éšè—çŠ¶æ€è®­ç»ƒçš„ç®€å•çº¿æ€§æ¨¡å‹ä¼˜äºæ¨¡å‹æ–‡æœ¬è¾“å‡ºã€‚è¿™è¡¨æ˜éšè—çŠ¶æ€æ•æ‰çš„ç»æµä¿¡æ¯æ¯”æ¨¡å‹ç›´æ¥æ­ç¤ºçš„æ›´ä¸ºä¸°å¯Œã€‚å­¦ä¹ æ›²çº¿åˆ†ææ˜¾ç¤ºåªéœ€å‡ åä¸ªæ ‡ç­¾æ ·æœ¬å³å¯è¿›è¡Œæœ‰æ•ˆè®­ç»ƒã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€ç›®æ ‡å˜é‡æ ‡ç­¾æ•°æ®çš„è¿ç§»å­¦ä¹ æ–¹æ³•æ¥æé«˜ä¼°è®¡ç²¾åº¦ã€‚æœ€åï¼Œå±•ç¤ºäº†éšè—çŠ¶æ€è¡¨ç¤ºåœ¨è¶…åˆ†è¾¨ç‡å’Œæ•°æ®å¡«è¡¥ä»»åŠ¡ä¸­çš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„éšè—çŠ¶æ€å¯ç”¨äºç»æµé‡‘èç»Ÿè®¡æ•°æ®çš„ä¼°è®¡å’Œå¡«è¡¥ã€‚</li>
<li>éšè—çŠ¶æ€åŒ…å«æ¯”æ¨¡å‹ç›´æ¥è¾“å‡ºæ›´ä¸°å¯Œç»æµä¿¡æ¯çš„å¯èƒ½æ€§ã€‚</li>
<li>åªéœ€å°‘é‡æ ‡ç­¾æ ·æœ¬å³å¯è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>æå‡ºä¸€ç§æ— éœ€ç›®æ ‡å˜é‡æ ‡ç­¾æ•°æ®çš„è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œæé«˜ä¼°è®¡å‡†ç¡®æ€§ã€‚</li>
<li>éšè—çŠ¶æ€è¡¨ç¤ºåœ¨è¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸­å…·æœ‰å®ç”¨æ€§ã€‚</li>
<li>éšè—çŠ¶æ€åœ¨æ•°æ®å¡«è¡¥ä»»åŠ¡ä¸­å…·æœ‰å®ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08662">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58f526af65795cccc48dd7954fcb117d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-587bd668a9308a0ecaedf5008b36fd06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfa3e169528d272fdaa2233de6738500.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Visually-Guided-Decoding-Gradient-Free-Hard-Prompt-Inversion-with-Language-Models"><a href="#Visually-Guided-Decoding-Gradient-Free-Hard-Prompt-Inversion-with-Language-Models" class="headerlink" title="Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with   Language Models"></a>Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with   Language Models</h2><p><strong>Authors:Donghoon Kim, Minji Bae, Kyuhong Shim, Byonghyo Shim</strong></p>
<p>Text-to-image generative models like DALL-E and Stable Diffusion have revolutionized visual content creation across various applications, including advertising, personalized media, and design prototyping. However, crafting effective textual prompts to guide these models remains challenging, often requiring extensive trial and error. Existing prompt inversion approaches, such as soft and hard prompt techniques, are not so effective due to the limited interpretability and incoherent prompt generation. To address these issues, we propose Visually Guided Decoding (VGD), a gradient-free approach that leverages large language models (LLMs) and CLIP-based guidance to generate coherent and semantically aligned prompts. In essence, VGD utilizes the robust text generation capabilities of LLMs to produce human-readable prompts. Further, by employing CLIP scores to ensure alignment with user-specified visual concepts, VGD enhances the interpretability, generalization, and flexibility of prompt generation without the need for additional training. Our experiments demonstrate that VGD outperforms existing prompt inversion techniques in generating understandable and contextually relevant prompts, facilitating more intuitive and controllable interactions with text-to-image models. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆæ¨¡å‹ï¼Œå¦‚DALL-Eå’ŒStable Diffusionï¼Œå·²ç»å½»åº•æ”¹å˜äº†åŒ…æ‹¬å¹¿å‘Šã€ä¸ªæ€§åŒ–åª’ä½“å’Œè®¾è®¡åŸå‹åœ¨å†…çš„å„ç§åº”ç”¨ä¸­çš„è§†è§‰å†…å®¹åˆ›ä½œã€‚ç„¶è€Œï¼Œä¸ºå¼•å¯¼è¿™äº›æ¨¡å‹è€Œæ’°å†™æœ‰æ•ˆçš„æ–‡æœ¬æç¤ºä»å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„è¯•é”™ã€‚ç°æœ‰çš„æç¤ºåè½¬æ–¹æ³•ï¼Œå¦‚è½¯æç¤ºå’Œç¡¬æç¤ºæŠ€æœ¯ï¼Œç”±äºè§£é‡Šæ€§å’Œè¿è´¯æ€§æç¤ºç”Ÿæˆèƒ½åŠ›æœ‰é™ï¼Œæ•ˆæœå¹¶ä¸ç†æƒ³ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œè§†è§‰å¼•å¯¼è§£ç â€ï¼ˆVGDï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€æ¢¯åº¦çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒåŸºäºCLIPçš„æŒ‡å¯¼æ¥ç”Ÿæˆè¿è´¯ä¸”è¯­ä¹‰å¯¹é½çš„æç¤ºã€‚æœ¬è´¨ä¸Šï¼ŒVGDåˆ©ç”¨LLMå¼ºå¤§çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›æ¥äº§ç”Ÿäººç±»å¯è¯»çš„æç¤ºã€‚æ­¤å¤–ï¼Œé€šè¿‡ä½¿ç”¨CLIPè¯„åˆ†æ¥ç¡®ä¿ä¸ç”¨æˆ·æŒ‡å®šçš„è§†è§‰æ¦‚å¿µå¯¹é½ï¼ŒVGDæé«˜äº†æç¤ºç”Ÿæˆçš„è§£é‡Šæ€§ã€é€šç”¨æ€§å’Œçµæ´»æ€§ï¼Œè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨ç”Ÿæˆæ˜“äºç†è§£å’Œä¸Šä¸‹æ–‡ç›¸å…³çš„æç¤ºæ–¹é¢ï¼ŒVGDä¼˜äºç°æœ‰çš„æç¤ºåè½¬æŠ€æœ¯ï¼Œæ›´èƒ½ä¿ƒè¿›ä¸æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç›´è§‚å’Œå¯æ§äº¤äº’ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08622v1">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong><br>æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆæ¨¡å‹å¦‚DALL-Eå’ŒStable Diffusionå·²å¹¿æ³›åº”ç”¨äºå¹¿å‘Šã€ä¸ªæ€§åŒ–åª’ä½“å’Œè®¾è®¡åŸå‹ç­‰å„ä¸ªé¢†åŸŸã€‚ç„¶è€Œï¼Œå¦‚ä½•ä¸ºè¿™äº›æ¨¡å‹æä¾›æœ‰æ•ˆçš„æ–‡æœ¬æç¤ºä»æ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œé€šå¸¸éœ€è¦å¤§é‡è¯•é”™ã€‚ç°æœ‰æç¤ºåè½¬æ–¹æ³•å¦‚è½¯æç¤ºå’Œç¡¬æç¤ºæŠ€æœ¯å› ç¼ºä¹å¯è§£é‡Šæ€§å’Œä¸è¿è´¯çš„æç¤ºç”Ÿæˆè€Œæ•ˆæœä¸ä½³ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ— éœ€æ¢¯åº¦çš„è§†è§‰å¼•å¯¼è§£ç ï¼ˆVGDï¼‰æ–¹æ³•ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒCLIPæŒ‡å¯¼ç”Ÿæˆè¿è´¯ä¸”è¯­ä¹‰å¯¹é½çš„æç¤ºã€‚VGDåˆ©ç”¨LLMçš„å¥å£®æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›äº§ç”Ÿäººç±»å¯è¯»çš„æç¤ºï¼Œå¹¶é€šè¿‡CLIPè¯„åˆ†ç¡®ä¿ä¸ç”¨æˆ·æŒ‡å®šçš„è§†è§‰æ¦‚å¿µå¯¹é½ï¼Œæé«˜äº†æç¤ºç”Ÿæˆçš„å¯è§£é‡Šæ€§ã€é€šç”¨æ€§å’Œçµæ´»æ€§ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒã€‚å®éªŒè¯æ˜ï¼ŒVGDåœ¨ç”Ÿæˆå¯ç†è§£å’Œä¸Šä¸‹æ–‡ç›¸å…³çš„æç¤ºæ–¹é¢ä¼˜äºç°æœ‰æç¤ºåè½¬æŠ€æœ¯ï¼Œä½¿æ–‡æœ¬è½¬å›¾åƒæ¨¡å‹çš„äº¤äº’æ›´åŠ ç›´è§‚å’Œå¯æ§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¹¿å‘Šã€ä¸ªæ€§åŒ–åª’ä½“å’Œè®¾è®¡åŸå‹ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>å½“å‰æ¨¡å‹é¢ä¸´å¦‚ä½•æä¾›æœ‰æ•ˆæ–‡æœ¬æç¤ºçš„æŒ‘æˆ˜ï¼Œéœ€è¦æ›´å¤šå¯è§£é‡Šå’Œè¿è´¯çš„æç¤ºç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>ç°æœ‰æç¤ºåè½¬æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¦‚ç¼ºä¹å¯è§£é‡Šæ€§å’Œä¸è¿è´¯çš„æç¤ºç”Ÿæˆã€‚</li>
<li>VGDæ–¹æ³•ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’ŒCLIPæŒ‡å¯¼ï¼Œæ— éœ€æ¢¯åº¦ç”Ÿæˆè¿è´¯ä¸”è¯­ä¹‰å¯¹é½çš„æç¤ºã€‚</li>
<li>VGDåˆ©ç”¨LLMçš„å¥å£®æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œäº§ç”Ÿäººç±»å¯è¯»çš„æç¤ºã€‚</li>
<li>CLIPè¯„åˆ†ç¡®ä¿ç”¨æˆ·æŒ‡å®šçš„è§†è§‰æ¦‚å¿µä¸æç¤ºå¯¹é½ï¼Œæé«˜æç¤ºç”Ÿæˆçš„å¯è§£é‡Šæ€§ã€é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08622">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ac843aa234a6828d93515d17706b5071.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e18b4132b4414e77bd35bb63cb43008.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac19e513bf87ed33cdda4fb2c8efa550.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9cbd16a06eb744e4543d456fe50e8d0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Small-but-Significant-On-the-Promise-of-Small-Language-Models-for-Accessible-AIED"><a href="#Small-but-Significant-On-the-Promise-of-Small-Language-Models-for-Accessible-AIED" class="headerlink" title="Small but Significant: On the Promise of Small Language Models for   Accessible AIED"></a>Small but Significant: On the Promise of Small Language Models for   Accessible AIED</h2><p><strong>Authors:Yumou Wei, Paulo Carvalho, John Stamper</strong></p>
<p>GPT has become nearly synonymous with large language models (LLMs), an increasingly popular term in AIED proceedings. A simple keyword-based search reveals that 61% of the 76 long and short papers presented at AIED 2024 describe novel solutions using LLMs to address some of the long-standing challenges in education, and 43% specifically mention GPT. Although LLMs pioneered by GPT create exciting opportunities to strengthen the impact of AI on education, we argue that the fieldâ€™s predominant focus on GPT and other resource-intensive LLMs (with more than 10B parameters) risks neglecting the potential impact that small language models (SLMs) can make in providing resource-constrained institutions with equitable and affordable access to high-quality AI tools. Supported by positive results on knowledge component (KC) discovery, a critical challenge in AIED, we demonstrate that SLMs such as Phi-2 can produce an effective solution without elaborate prompting strategies. Hence, we call for more attention to developing SLM-based AIED approaches. </p>
<blockquote>
<p>GPTå·²ç»æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£åè¯ï¼Œè¿™æ˜¯äººå·¥æ™ºèƒ½æ•™è‚²ç¨‹åºä¸­çš„ä¸€ä¸ªæ—¥ç›Šæµè¡Œçš„æœ¯è¯­ã€‚ä¸€ä¸ªç®€å•çš„åŸºäºå…³é”®è¯çš„æœç´¢ç»“æœæ˜¾ç¤ºï¼Œåœ¨AIED 2024ä¼šè®®ä¸Šå±•ç¤ºçš„76ç¯‡é•¿çŸ­è®ºæ–‡ä¸­ï¼Œæœ‰61%çš„è®ºæ–‡æè¿°äº†ä½¿ç”¨LLMè§£å†³æ•™è‚²ä¸­ä¸€äº›é•¿æœŸå­˜åœ¨æŒ‘æˆ˜çš„æ–°è§£å†³æ–¹æ¡ˆï¼Œå…¶ä¸­43%ç‰¹åˆ«æåˆ°äº†GPTã€‚è™½ç„¶ç”±GPTå¼€åˆ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºäººå·¥æ™ºèƒ½å¯¹æ•™è‚²çš„å½±å“åˆ›é€ äº†æ¿€åŠ¨äººå¿ƒçš„æœºä¼šï¼Œæˆ‘ä»¬è®¤ä¸ºï¼Œè¯¥é¢†åŸŸå¯¹GPTå’Œå…¶ä»–èµ„æºå¯†é›†å‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå…·æœ‰è¶…è¿‡10äº¿ä¸ªå‚æ•°ï¼‰çš„è¿‡åº¦å…³æ³¨ï¼Œå¯èƒ½å¿½è§†äº†å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰åœ¨èµ„æºå—é™æœºæ„ä¸­å®ç°å…¬å¹³å’Œè´Ÿæ‹…å¾—èµ·çš„ä¼˜è´¨äººå·¥æ™ºèƒ½å·¥å…·è®¿é—®çš„æ½œåœ¨å½±å“ã€‚é€šè¿‡å¯¹äººå·¥æ™ºèƒ½æ•™è‚²ä¸­çš„å…³é”®æŒ‘æˆ˜â€”â€”çŸ¥è¯†ç»„ä»¶ï¼ˆKCï¼‰å‘ç°çš„ç§¯æç»“æœçš„æ”¯æŒï¼Œæˆ‘ä»¬è¯æ˜äº†Phi-2ç­‰å°å‹è¯­è¨€æ¨¡å‹å¯ä»¥åœ¨ä¸éœ€è¦å¤æ‚æç¤ºç­–ç•¥çš„æƒ…å†µä¸‹äº§ç”Ÿæœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬å‘¼åæ›´å¤šåœ°å…³æ³¨åŸºäºå°å‹è¯­è¨€æ¨¡å‹çš„äººå·¥æ™ºèƒ½æ•™è‚²æ–¹æ³•çš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08588v1">PDF</a> This vision paper advocates using small language models (e.g., Phi-2)   in AI for education (AIED)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²æˆä¸ºAIæ•™è‚²ç ”è®¨ä¼šä¸Šçš„çƒ­é—¨è¯æ±‡ï¼ŒGPTå°¤ä¸ºçªå‡ºã€‚å°½ç®¡GPTå¼•é¢†çš„LLMä¸ºAIåœ¨æ•™è‚²é¢†åŸŸå¸¦æ¥äº†æœºä¼šï¼Œä½†è¿‡åˆ†å…³æ³¨å¯èƒ½ä¼šå¿½ç•¥å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰åœ¨èµ„æºå—é™æœºæ„ä¸­çš„æ½œåŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒSLMå¦‚Phi-2åœ¨çŸ¥è¯†ç»„ä»¶ï¼ˆKCï¼‰å‘ç°ç­‰å…³é”®æŒ‘æˆ˜ä¸Šè¡¨ç°å‡ºç§¯æç»“æœï¼Œæ— éœ€å¤æ‚æç¤ºç­–ç•¥å³å¯æä¾›æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚å› æ­¤ï¼Œå‘¼åæ›´å¤šå…³æ³¨åŸºäºSLMçš„AIæ•™è‚²æ–¹æ³•çš„å¼€å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPTå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨AIæ•™è‚²é¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ã€‚</li>
<li>61%çš„AIED 2024è®ºæ–‡æè¿°äº†ä½¿ç”¨LLMè§£å†³æ•™è‚²é•¿æœŸæŒ‘æˆ˜çš„æ–°æ–¹æ³•ã€‚</li>
<li>43%çš„è®ºæ–‡ç‰¹åˆ«æåˆ°äº†GPTã€‚</li>
<li>å¯¹GPTå’Œå…¶ä»–èµ„æºå¯†é›†å‹LLMçš„è¿‡åº¦å…³æ³¨å¯èƒ½å¿½è§†äº†å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰çš„æ½œåŠ›ã€‚</li>
<li>SLMåœ¨çŸ¥è¯†ç»„ä»¶ï¼ˆKCï¼‰å‘ç°ç­‰å…³é”®æŒ‘æˆ˜ä¸Šå±•ç°å‡ºç§¯æç»“æœã€‚</li>
<li>SLMå¦‚Phi-2æ— éœ€å¤æ‚çš„æç¤ºç­–ç•¥å³å¯æä¾›æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08588">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5dfa5fbf3c8cc599aceb674ac2db7bff.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FalseReject-A-Resource-for-Improving-Contextual-Safety-and-Mitigating-Over-Refusals-in-LLMs-via-Structured-Reasoning"><a href="#FalseReject-A-Resource-for-Improving-Contextual-Safety-and-Mitigating-Over-Refusals-in-LLMs-via-Structured-Reasoning" class="headerlink" title="FalseReject: A Resource for Improving Contextual Safety and Mitigating   Over-Refusals in LLMs via Structured Reasoning"></a>FalseReject: A Resource for Improving Contextual Safety and Mitigating   Over-Refusals in LLMs via Structured Reasoning</h2><p><strong>Authors:Zhehao Zhang, Weijie Xu, Fanyou Wu, Chandan K. Reddy</strong></p>
<p>Safety alignment approaches in large language models (LLMs) often lead to the over-refusal of benign queries, significantly diminishing their utility in sensitive scenarios. To address this challenge, we introduce FalseReject, a comprehensive resource containing 16k seemingly toxic queries accompanied by structured responses across 44 safety-related categories. We propose a graph-informed adversarial multi-agent interaction framework to generate diverse and complex prompts, while structuring responses with explicit reasoning to aid models in accurately distinguishing safe from unsafe contexts. FalseReject includes training datasets tailored for both standard instruction-tuned models and reasoning-oriented models, as well as a human-annotated benchmark test set. Our extensive benchmarking on 29 state-of-the-art (SOTA) LLMs reveals persistent over-refusal challenges. Empirical results demonstrate that supervised finetuning with FalseReject substantially reduces unnecessary refusals without compromising overall safety or general language capabilities. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å®‰å…¨å¯¹é½æ–¹æ³•é€šå¸¸ä¼šå¯¼è‡´è‰¯æ€§æŸ¥è¯¢è¢«æ‹’ç»ï¼Œè¿™åœ¨æ•æ„Ÿåœºæ™¯ä¸­å¤§å¤§é™ä½äº†å…¶æ•ˆç”¨ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†FalseRejectï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«16,000ä¸ªçœ‹ä¼¼æœ‰æ¯’æŸ¥è¯¢çš„ç»¼åˆèµ„æºï¼Œå¹¶æä¾›äº†è·¨44ä¸ªå®‰å…¨ç›¸å…³ç±»åˆ«çš„ç»“æ„åŒ–å“åº”ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºå›¾ä¿¡æ¯çš„å¯¹æŠ—æ€§å¤šæ™ºèƒ½ä½“äº¤äº’æ¡†æ¶ï¼Œä»¥ç”Ÿæˆå¤šæ ·åŒ–å’Œå¤æ‚çš„æç¤ºï¼ŒåŒæ—¶é€šè¿‡ç»“æ„åŒ–å“åº”æä¾›æ˜ç¡®æ¨ç†æ¥å¸®åŠ©æ¨¡å‹å‡†ç¡®åŒºåˆ†å®‰å…¨å’Œä¸å®‰å…¨çš„ä¸Šä¸‹æ–‡ã€‚FalseRejectæ—¢åŒ…å«é’ˆå¯¹æ ‡å‡†æŒ‡ä»¤è°ƒæ•´æ¨¡å‹å’Œæ¨ç†å¯¼å‘æ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†ï¼Œä¹ŸåŒ…å«ä¸€ä¸ªäººå·¥æ³¨é‡Šçš„åŸºå‡†æµ‹è¯•é›†ã€‚æˆ‘ä»¬å¯¹29ä¸ªæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†æŒç»­çš„è¿‡åº¦æ‹’ç»æŒ‘æˆ˜ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨FalseRejectè¿›è¡Œç›‘ç£å¾®è°ƒå¯ä»¥å¤§å¤§å‡å°‘ä¸å¿…è¦çš„æ‹’ç»ï¼ŒåŒæ—¶ä¸ä¼šæŸå®³æ•´ä½“å®‰å…¨æ€§æˆ–é€šç”¨è¯­è¨€åŠŸèƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08054v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®‰å…¨å¯¹é½æ–¹æ³•å¸¸å¸¸å¯¼è‡´è‰¯æ€§æŸ¥è¯¢è¢«æ‹’ç»ï¼Œä¸¥é‡å½±å“äº†å…¶åœ¨æ•æ„Ÿåœºæ™¯ä¸­çš„å®ç”¨æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºFalseRejectèµ„æºï¼ŒåŒ…å«16ä¸‡ä¸ªçœ‹ä¼¼æœ‰æ¯’çš„æŸ¥è¯¢ä»¥åŠç»“æ„åŒ–çš„å›å¤è·¨è¶Š44ä¸ªå®‰å…¨ç±»åˆ«ã€‚æˆ‘ä»¬æå‡ºä¸€ä¸ªå›¾é©±åŠ¨çš„å¯¹æŠ—å¤šæ™ºèƒ½ä½“äº¤äº’æ¡†æ¶æ¥ç”Ÿæˆå¤šæ ·ä¸”å¤æ‚çš„æç¤ºï¼ŒåŒæ—¶ç»“æ„åŒ–å›å¤ä»¥æ˜ç¡®æ¨ç†å¸®åŠ©æ¨¡å‹å‡†ç¡®åŒºåˆ†å®‰å…¨å’Œä¸å®‰å…¨ä¸Šä¸‹æ–‡ã€‚FalseRejectåŒ…æ‹¬é’ˆå¯¹æ ‡å‡†æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹å’Œæ¨ç†å¯¼å‘æ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†ï¼Œä»¥åŠä¸€ä¸ªäººå·¥æ³¨é‡Šçš„åŸºå‡†æµ‹è¯•é›†ã€‚æˆ‘ä»¬å¯¹29ä¸ªæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ï¼Œå‘ç°æŒç»­å­˜åœ¨çš„è¿‡åº¦æ‹’ç»æŒ‘æˆ˜ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨FalseRejectè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒèƒ½æ˜¾è‘—å‡å°‘ä¸å¿…è¦çš„æ‹’ç»ï¼ŒåŒæ—¶ä¸æŸå®³æ•´ä½“å®‰å…¨æ€§æˆ–é€šç”¨è¯­è¨€èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å®‰å…¨æ€§æ–¹é¢å­˜åœ¨è¿‡åº¦æ‹’ç»è‰¯æ€§æŸ¥è¯¢çš„é—®é¢˜ã€‚</li>
<li>FalseRejectèµ„æºåŒ…å«çœ‹ä¼¼æœ‰æ¯’çš„æŸ¥è¯¢å’Œç»“æ„åŒ–å›å¤ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>æ¨å‡ºå›¾é©±åŠ¨çš„å¯¹æŠ—å¤šæ™ºèƒ½ä½“äº¤äº’æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå¤æ‚å¤šæ ·çš„æç¤ºã€‚</li>
<li>FalseRejectèµ„æºåŒ…å«é’ˆå¯¹ä¸åŒç±»å‹çš„è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†ã€‚</li>
<li>å¹¿æ³›çš„åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼Œç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒºåˆ†å®‰å…¨å’Œä¸å®‰å…¨ä¸Šä¸‹æ–‡æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ä½¿ç”¨FalseRejectè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒèƒ½å‡å°‘ä¸å¿…è¦çš„æ‹’ç»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08054">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9292cf0d974df1daf85c9f0927366477.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e30d06503d16d92647ed39cd50e9b32f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf51fa8fb6bbe4cbf990b3ff6cba40d9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-09207769e0dc2c7bfc68a74c02269e4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-950900a64b16aee64105510420762eb7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf7eb6fa3749bb4faa784ec987c430c1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Ophora-A-Large-Scale-Data-Driven-Text-Guided-Ophthalmic-Surgical-Video-Generation-Model"><a href="#Ophora-A-Large-Scale-Data-Driven-Text-Guided-Ophthalmic-Surgical-Video-Generation-Model" class="headerlink" title="Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video   Generation Model"></a>Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video   Generation Model</h2><p><strong>Authors:Wei Li, Ming Hu, Guoan Wang, Lihao Liu, Kaijin Zhou, Junzhi Ning, Xin Guo, Zongyuan Ge, Lixu Gu, Junjun He</strong></p>
<p>In ophthalmic surgery, developing an AI system capable of interpreting surgical videos and predicting subsequent operations requires numerous ophthalmic surgical videos with high-quality annotations, which are difficult to collect due to privacy concerns and labor consumption. Text-guided video generation (T2V) emerges as a promising solution to overcome this issue by generating ophthalmic surgical videos based on surgeon instructions. In this paper, we present Ophora, a pioneering model that can generate ophthalmic surgical videos following natural language instructions. To construct Ophora, we first propose a Comprehensive Data Curation pipeline to convert narrative ophthalmic surgical videos into a large-scale, high-quality dataset comprising over 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive Video-Instruction Tuning scheme to transfer rich spatial-temporal knowledge from a T2V model pre-trained on natural video-text datasets for privacy-preserved ophthalmic surgical video generation based on Ophora-160K. Experiments on video quality evaluation via quantitative analysis and ophthalmologist feedback demonstrate that Ophora can generate realistic and reliable ophthalmic surgical videos based on surgeon instructions. We also validate the capability of Ophora for empowering downstream tasks of ophthalmic surgical workflow understanding. Code is available at <a target="_blank" rel="noopener" href="https://github.com/mar-cry/Ophora">https://github.com/mar-cry/Ophora</a>. </p>
<blockquote>
<p>åœ¨çœ¼ç§‘æ‰‹æœ¯ä¸­ï¼Œå¼€å‘ä¸€ä¸ªèƒ½å¤Ÿè§£è¯»æ‰‹æœ¯è§†é¢‘å¹¶é¢„æµ‹åç»­æ“ä½œçš„AIç³»ç»Ÿéœ€è¦å¤§é‡çš„å¸¦æœ‰é«˜è´¨é‡æ³¨é‡Šçš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚ç”±äºéšç§é—®é¢˜å’ŒåŠ³åŠ¨æ¶ˆè€—ï¼Œè¿™äº›è§†é¢‘çš„æ”¶é›†éå¸¸å›°éš¾ã€‚æ–‡æœ¬å¼•å¯¼çš„è§†é¢‘ç”Ÿæˆï¼ˆT2Vï¼‰ä½œä¸ºä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡åŸºäºå¤–ç§‘åŒ»ç”Ÿçš„æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…ˆè¿›çš„æ¨¡å‹â€”â€”å¥¥èŠ™æ‹‰ï¼ˆOphoraï¼‰ï¼Œå®ƒå¯ä»¥æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚ä¸ºäº†æ„å»ºå¥¥èŠ™æ‹‰ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ä¸ªå…¨é¢çš„æ•°æ®æ•´ç†ç®¡é“ï¼Œå°†å™è¿°æ€§çœ¼ç§‘æ‰‹æœ¯è§†é¢‘è½¬åŒ–ä¸ºå¤§è§„æ¨¡çš„é«˜è´¨é‡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡16ä¸‡å¯¹è§†é¢‘æŒ‡ä»¤å¯¹ï¼Œå³å¥¥èŠ™æ‹‰-16ä¸‡æ•°æ®é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¸è¿›çš„è§†é¢‘æŒ‡ä»¤è°ƒæ•´æ–¹æ¡ˆï¼Œä»ä¸€ä¸ªåœ¨å¤©ç„¶è§†é¢‘æ–‡æœ¬æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„T2Væ¨¡å‹ä¸­è½¬ç§»ä¸°å¯Œçš„æ—¶ç©ºçŸ¥è¯†ï¼Œç”¨äºåŸºäºå¥¥èŠ™æ‹‰-16ä¸‡æ•°æ®é›†çš„éšç§ä¿æŠ¤çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ç”Ÿæˆã€‚é€šè¿‡å¯¹è§†é¢‘è´¨é‡çš„å®šé‡åˆ†æå’Œçœ¼ç§‘åŒ»ç”Ÿçš„åé¦ˆè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œå¥¥èŠ™æ‹‰å¯ä»¥æ ¹æ®å¤–ç§‘åŒ»ç”Ÿçš„æŒ‡ä»¤ç”Ÿæˆç°å®å’Œå¯é çš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†å¥¥èŠ™æ‹‰åœ¨æ‰§è¡Œçœ¼ç§‘æ‰‹æœ¯å·¥ä½œæµç¨‹ç†è§£ç­‰ä¸‹æ¸¸ä»»åŠ¡æ–¹é¢çš„èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mar-cry/Ophora">https://github.com/mar-cry/Ophora</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07449v2">PDF</a> Early accepted in MICCAI25</p>
<p><strong>Summary</strong></p>
<p>åœ¨çœ¼ç§‘æ‰‹æœ¯é¢†åŸŸï¼Œå¼€å‘èƒ½å¤Ÿè§£è¯»æ‰‹æœ¯è§†é¢‘å¹¶é¢„æµ‹åç»­æ“ä½œçš„AIç³»ç»Ÿéœ€è¦å¤§é‡å¸¦æœ‰é«˜è´¨é‡æ³¨é‡Šçš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚ç”±äºéšç§é—®é¢˜å’ŒåŠ³åŠ¨æ¶ˆè€—ï¼Œè¿™äº›è§†é¢‘çš„æ”¶é›†å¾ˆå›°éš¾ã€‚æ–‡æœ¬å¼•å¯¼çš„è§†é¢‘ç”Ÿæˆï¼ˆT2Vï¼‰æŠ€æœ¯ä½œä¸ºä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ ¹æ®å¤–ç§‘åŒ»ç”Ÿçš„æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºOphoraçš„å¼€åˆ›æ€§æ¨¡å‹ï¼Œå®ƒèƒ½æ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚ä¸ºæ„å»ºOphoraï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§å…¨é¢çš„æ•°æ®æ•´ç†ç®¡é“ï¼Œå°†å™äº‹çœ¼ç§‘æ‰‹æœ¯è§†é¢‘è½¬åŒ–ä¸ºå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†Ophora-160Kï¼ŒåŒ…å«è¶…è¿‡16ä¸‡å¯¹è§†é¢‘æŒ‡ä»¤ã€‚æ¥ç€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¸è¿›å¼è§†é¢‘æŒ‡ä»¤è°ƒæ•´æ–¹æ¡ˆï¼Œä»ä¸€ä¸ªåœ¨å¤©ç„¶è§†é¢‘æ–‡æœ¬æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„T2Væ¨¡å‹è½¬ç§»ä¸°å¯Œçš„æ—¶ç©ºçŸ¥è¯†ï¼Œç”¨äºåŸºäºOphora-160Kçš„éšç§ä¿æŠ¤çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ç”Ÿæˆã€‚é€šè¿‡å®šé‡åˆ†æå’Œçœ¼ç§‘åŒ»ç”Ÿåé¦ˆè¿›è¡Œçš„è§†é¢‘è´¨é‡è¯„ä¼°å®éªŒè¡¨æ˜ï¼ŒOphoraå¯ä»¥æ ¹æ®å¤–ç§‘åŒ»ç”Ÿçš„æŒ‡ä»¤ç”ŸæˆçœŸå®å¯é çš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚æˆ‘ä»¬è¿˜éªŒè¯äº†Ophoraåœ¨çœ¼ç§‘æ‰‹æœ¯å·¥ä½œæµç¨‹ç†è§£ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„èƒ½åŠ›ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/mar-cry/Ophora">é“¾æ¥</a>è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çœ¼ç§‘æ‰‹æœ¯AIç³»ç»Ÿçš„å¼€å‘é¢ä¸´æ”¶é›†é«˜è´¨é‡æ ‡æ³¨è§†é¢‘æ•°æ®çš„æŒ‘æˆ˜ï¼Œå› ä¸ºè¿™äº›æ•°æ®æ¶‰åŠéšç§é—®é¢˜å’ŒåŠ³åŠ¨å¯†é›†åº¦é«˜çš„éš¾é¢˜ã€‚</li>
<li>æ–‡æœ¬å¼•å¯¼çš„è§†é¢‘ç”Ÿæˆï¼ˆT2Vï¼‰æŠ€æœ¯ä¸ºè§£å†³æ­¤é—®é¢˜æä¾›äº†å¸Œæœ›ï¼Œèƒ½å¤Ÿæ ¹æ®å¤–ç§‘åŒ»ç”Ÿçš„æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§åä¸ºOphoraçš„æ¨¡å‹ï¼Œèƒ½å¤ŸåŸºäºè‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆçœ¼ç§‘æ‰‹æœ¯è§†é¢‘ï¼Œå¹¶å…·æœ‰å¼ºå¤§çš„èƒ½åŠ›æ”¯æŒä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚çœ¼ç§‘æ‰‹æœ¯å·¥ä½œæµç¨‹çš„ç†è§£ã€‚</li>
<li>Ophoraçš„æ„å»ºåŒ…æ‹¬ä¸¤ä¸ªå…³é”®æ­¥éª¤ï¼šä¸€æ˜¯ä½¿ç”¨ç»¼åˆæ•°æ®æ•´ç†ç®¡é“åˆ›å»ºå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†Ophora-160Kï¼›äºŒæ˜¯é‡‡ç”¨æ¸è¿›å¼è§†é¢‘æŒ‡ä»¤è°ƒæ•´æ–¹æ¡ˆï¼Œä»é¢„è®­ç»ƒçš„T2Væ¨¡å‹è½¬ç§»çŸ¥è¯†ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒOphoraç”Ÿæˆçš„çœ¼ç§‘æ‰‹æœ¯è§†é¢‘æ—¢çœŸå®åˆå¯é ï¼Œèƒ½å¤Ÿæ ¹æ®å¤–ç§‘åŒ»ç”Ÿçš„æŒ‡ä»¤è¿›è¡Œç”Ÿæˆã€‚</li>
<li>ä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œä¾¿äºå…¶ä»–äººä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07449">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d54feaa03e6ba76811d7f69a801bc9a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4ec3b71a8c06c8f6d824bb3db7290c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cca6f327a9e0a5dece7366cde8a1f565.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="FMNV-A-Dataset-of-Media-Published-News-Videos-for-Fake-News-Detection"><a href="#FMNV-A-Dataset-of-Media-Published-News-Videos-for-Fake-News-Detection" class="headerlink" title="FMNV: A Dataset of Media-Published News Videos for Fake News Detection"></a>FMNV: A Dataset of Media-Published News Videos for Fake News Detection</h2><p><strong>Authors:Yihao Wang, Zhong Qian, Peifeng Li</strong></p>
<p>News media, particularly video-based platforms, have become deeply embed-ded in daily life, concurrently amplifying the risks of misinformation dissem-ination. Consequently, multimodal fake news detection has garnered signifi-cant research attention. However, existing datasets predominantly comprise user-generated videos characterized by crude editing and limited public en-gagement, whereas professionally crafted fake news videos disseminated by media outlets-often politically or virally motivated-pose substantially greater societal harm. To address this gap, we construct FMNV, a novel da-taset exclusively composed of news videos published by media organizations. Through empirical analysis of existing datasets and our curated collection, we categorize fake news videos into four distinct types. Building upon this taxonomy, we employ Large Language Models (LLMs) to automatically generate deceptive content by manipulating authentic media-published news videos. Furthermore, we propose FMNVD, a baseline model featuring a dual-stream architecture that integrates spatio-temporal motion features from a 3D ResNeXt-101 backbone and static visual semantics from CLIP. The two streams are fused via an attention-based mechanism, while co-attention modules refine the visual, textual, and audio features for effective multi-modal aggregation. Comparative experiments demonstrate both the generali-zation capability of FMNV across multiple baselines and the superior detec-tion efficacy of FMNVD. This work establishes critical benchmarks for de-tecting high-impact fake news in media ecosystems while advancing meth-odologies for cross-modal inconsistency analysis. Our dataset is available in <a target="_blank" rel="noopener" href="https://github.com/DennisIW/FMNV">https://github.com/DennisIW/FMNV</a>. </p>
<blockquote>
<p>æ–°é—»åª’ä½“çš„æ™®åŠï¼Œç‰¹åˆ«æ˜¯è§†é¢‘å¹³å°ï¼Œå·²ç»æ·±å…¥åˆ°æ—¥å¸¸ç”Ÿæ´»ä¸­ï¼ŒåŒæ—¶ä¹ŸåŠ å‰§äº†å‡ä¿¡æ¯ä¼ æ’­çš„æ½œåœ¨é£é™©ã€‚å› æ­¤ï¼Œå¤šæ¨¡æ€å‡æ–°é—»æ£€æµ‹å¼•èµ·äº†æ˜¾è‘—çš„ç ”ç©¶å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰æ•°æ®é›†ä¸»è¦ç”±ç”¨æˆ·ç”Ÿæˆçš„è§†é¢‘ç»„æˆï¼Œè¿™äº›è§†é¢‘ç¼–è¾‘ç²—ç³™ä¸”å…¬ä¼—å‚ä¸åº¦æœ‰é™ï¼Œè€Œç”±åª’ä½“å‘å¸ƒçš„ä¸“ä¸šåˆ¶ä½œçš„å‡æ–°é—»è§†é¢‘â€”é€šå¸¸å¸¦æœ‰æ”¿æ²»æˆ–ç—…æ¯’å¼è¥é”€åŠ¨æœºâ€”å¯¹ç¤¾ä¼šé€ æˆäº†æ›´å¤§çš„å±å®³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ„å»ºäº†FMNVæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä»…åŒ…å«ç”±åª’ä½“ç»„ç»‡å‘å¸ƒçš„æ–°é—»è§†é¢‘ã€‚é€šè¿‡å¯¹ç°æœ‰æ•°æ®é›†å’Œæˆ‘ä»¬çš„ç²¾é€‰é›†åˆè¿›è¡Œå®è¯åˆ†æï¼Œæˆ‘ä»¬å°†å‡æ–°é—»è§†é¢‘åˆ†ä¸ºå››ç§ç±»å‹ã€‚åŸºäºè¿™ä¸€åˆ†ç±»ï¼Œæˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æ“çºµçœŸå®çš„åª’ä½“å‘å¸ƒæ–°é—»è§†é¢‘è‡ªåŠ¨ç”Ÿæˆæ¬ºéª—æ€§å†…å®¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºFMNVDåŸºçº¿æ¨¡å‹ï¼Œé‡‡ç”¨åŒæµæ¶æ„ï¼Œèåˆäº†æ¥è‡ª3D ResNeXt-101éª¨å¹²çš„æ—¶ç©ºè¿åŠ¨ç‰¹å¾å’ŒCLIPçš„é™æ€è§†è§‰è¯­ä¹‰ã€‚ä¸¤ä¸ªæµé€šè¿‡åŸºäºæ³¨æ„åŠ›çš„æœºåˆ¶èåˆï¼ŒåŒæ—¶å…±æ³¨æ„åŠ›æ¨¡å—å¯¹è§†è§‰ã€æ–‡æœ¬å’ŒéŸ³é¢‘ç‰¹å¾è¿›è¡Œç»†åŒ–ï¼Œä»¥å®ç°æœ‰æ•ˆçš„å¤šæ¨¡æ€èšåˆã€‚å¯¹æ¯”å®éªŒè¡¨æ˜ï¼ŒFMNVåœ¨å¤šä¸ªåŸºçº¿ä¸Šçš„é€šç”¨åŒ–èƒ½åŠ›å’ŒFMNVDçš„æ£€æµ‹æ•ˆèƒ½å‡è¡¨ç°å‡ºè‰²ã€‚è¿™é¡¹å·¥ä½œä¸ºæ£€æµ‹åª’ä½“ç”Ÿæ€ç³»ç»Ÿä¸­é«˜å½±å“åŠ›çš„å‡æ–°é—»å»ºç«‹äº†å…³é”®åŸºå‡†ï¼Œå¹¶æ¨åŠ¨äº†è·¨æ¨¡æ€ä¸ä¸€è‡´æ€§åˆ†æçš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/DennisIW/FMNV%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/DennisIW/FMNVä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07687v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–°é—»åª’ä½“çš„æ™®åŠå¸¦æ¥äº†å‡æ–°é—»ä¼ æ’­çš„é£é™©ï¼Œç‰¹åˆ«æ˜¯åœ¨è§†é¢‘å¹³å°ä¸Šã€‚ç°æœ‰çš„å‡æ–°é—»æ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨ç”¨æˆ·ç”Ÿæˆçš„è§†é¢‘ä¸Šï¼Œç¼ºä¹åª’ä½“å‘å¸ƒçš„ç»è¿‡ä¸“ä¸šåˆ¶ä½œçš„å‡æ–°é—»è§†é¢‘çš„ç ”ç©¶ã€‚æœ¬æ–‡æ„å»ºäº†ä¸€ä¸ªç”±åª’ä½“ç»„ç»‡å‘å¸ƒçš„æ–°é—»è§†é¢‘ç»„æˆçš„æ–°æ•°æ®é›†FMNVï¼Œå¹¶å¯¹å‡æ–°é—»è§†é¢‘è¿›è¡Œåˆ†ç±»ã€‚åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨ç”Ÿæˆæ¬ºéª—å†…å®¹ï¼Œå¹¶æå‡ºåŸºäºåŒæµæ¶æ„çš„FMNVDåŸºçº¿æ¨¡å‹ï¼Œå®ç°è·¨æ¨¡æ€å‡æ–°é—»æ£€æµ‹ã€‚è¯¥æ¨¡å‹èåˆæ—¶ç©ºè¿åŠ¨ç‰¹å¾å’Œé™æ€è§†è§‰è¯­ä¹‰ï¼Œé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œå¤šæ¨¡æ€èšåˆï¼Œæé«˜æ£€æµ‹æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°é—»åª’ä½“çš„æ™®åŠå¢åŠ äº†å‡æ–°é—»ä¼ æ’­çš„é£é™©ï¼Œç‰¹åˆ«æ˜¯è§†é¢‘å¹³å°ä¸Šçš„å‡æ–°é—»ã€‚</li>
<li>ç°æœ‰å‡æ–°é—»æ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨ç”¨æˆ·ç”Ÿæˆçš„è§†é¢‘ä¸Šï¼Œç¼ºä¹åª’ä½“å‘å¸ƒçš„ç»è¿‡ä¸“ä¸šåˆ¶ä½œçš„å‡æ–°é—»è§†é¢‘ç ”ç©¶ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªç”±åª’ä½“ç»„ç»‡å‘å¸ƒçš„æ–°é—»è§†é¢‘ç»„æˆçš„æ–°æ•°æ®é›†FMNVã€‚</li>
<li>å‡æ–°é—»è§†é¢‘è¢«åˆ†ç±»ä¸ºå››ç§ç±»å‹ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨ç”Ÿæˆæ¬ºéª—å†…å®¹ã€‚</li>
<li>æå‡ºäº†åŸºäºåŒæµæ¶æ„çš„FMNVDåŸºçº¿æ¨¡å‹ï¼Œå®ç°è·¨æ¨¡æ€å‡æ–°é—»æ£€æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3ce02f68ec63d1b1980c0bc42e1937a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-892322b26f499a4ff89dece3de94d689.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04dc08ee3cc9e1d91e8d14e319b2723c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3e6a3d74c2aea758a3ab5ddbb54c42c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1de53fa6629a62ab3637c0b3173dbed6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88988ef867b6da3d90ac888b773a5ce0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AI-Hiring-with-LLMs-A-Context-Aware-and-Explainable-Multi-Agent-Framework-for-Resume-Screening"><a href="#AI-Hiring-with-LLMs-A-Context-Aware-and-Explainable-Multi-Agent-Framework-for-Resume-Screening" class="headerlink" title="AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent   Framework for Resume Screening"></a>AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent   Framework for Resume Screening</h2><p><strong>Authors:Frank P. -W. Lo, Jianing Qiu, Zeyu Wang, Haibao Yu, Yeming Chen, Gao Zhang, Benny Lo</strong></p>
<p>Resume screening is a critical yet time-intensive process in talent acquisition, requiring recruiters to analyze vast volume of job applications while remaining objective, accurate, and fair. With the advancements in Large Language Models (LLMs), their reasoning capabilities and extensive knowledge bases demonstrate new opportunities to streamline and automate recruitment workflows. In this work, we propose a multi-agent framework for resume screening using LLMs to systematically process and evaluate resumes. The framework consists of four core agents, including a resume extractor, an evaluator, a summarizer, and a score formatter. To enhance the contextual relevance of candidate assessments, we integrate Retrieval-Augmented Generation (RAG) within the resume evaluator, allowing incorporation of external knowledge sources, such as industry-specific expertise, professional certifications, university rankings, and company-specific hiring criteria. This dynamic adaptation enables personalized recruitment, bridging the gap between AI automation and talent acquisition. We assess the effectiveness of our approach by comparing AI-generated scores with ratings provided by HR professionals on a dataset of anonymized online resumes. The findings highlight the potential of multi-agent RAG-LLM systems in automating resume screening, enabling more efficient and scalable hiring workflows. </p>
<blockquote>
<p>ç®€å†ç­›é€‰æ˜¯äººæ‰æ‹›è˜ä¸­ä¸€ä¸ªè‡³å…³é‡è¦ä½†åˆè€—è´¹æ—¶é—´çš„ç¯èŠ‚ï¼Œæ‹›è˜äººå‘˜éœ€è¦åˆ†æå¤§é‡çš„æ±‚èŒç”³è¯·ï¼ŒåŒæ—¶ä¿æŒå®¢è§‚ã€å‡†ç¡®å’Œå…¬æ­£ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ï¼Œå…¶æ¨ç†èƒ½åŠ›å’Œå¹¿æ³›çš„çŸ¥è¯†åº“ä¸ºæ‹›è˜æµç¨‹çš„ç®€åŒ–å’Œè‡ªåŠ¨åŒ–æä¾›äº†æ–°çš„æœºä¼šã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä½¿ç”¨LLMè¿›è¡Œç®€å†ç­›é€‰çš„å¤šä»£ç†æ¡†æ¶ï¼Œç³»ç»Ÿåœ°å¤„ç†å’Œè¯„ä¼°ç®€å†ã€‚è¯¥æ¡†æ¶åŒ…å«å››ä¸ªæ ¸å¿ƒä»£ç†ï¼ŒåŒ…æ‹¬ç®€å†æå–å™¨ã€è¯„ä¼°å™¨ã€æ‘˜è¦å™¨å’Œè®¡åˆ†æ ¼å¼åŒ–å™¨ã€‚ä¸ºäº†æé«˜å€™é€‰äººè¯„ä¼°çš„ä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼Œæˆ‘ä»¬åœ¨ç®€å†è¯„ä¼°å™¨ä¸­é›†æˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Œå…è®¸èå…¥å¤–éƒ¨çŸ¥è¯†æºï¼Œå¦‚è¡Œä¸šä¸“ä¸šçŸ¥è¯†ã€ä¸“ä¸šè®¤è¯ã€å¤§å­¦æ’åå’Œå…¬å¸ç‰¹å®šçš„æ‹›è˜æ ‡å‡†ã€‚è¿™ç§åŠ¨æ€é€‚åº”ä½¿ä¸ªæ€§åŒ–æ‹›è˜æˆä¸ºå¯èƒ½ï¼Œç¼©å°äº†äººå·¥æ™ºèƒ½è‡ªåŠ¨åŒ–å’Œäººæ‰æ‹›è˜ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬é€šè¿‡æ¯”è¾ƒAIç”Ÿæˆçš„åˆ†æ•°å’ŒäººåŠ›èµ„æºä¸“ä¸šäººå£«åœ¨åŒ¿ååœ¨çº¿ç®€å†æ•°æ®é›†ä¸Šæä¾›çš„è¯„åˆ†æ¥è¯„ä¼°æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¤šä»£ç†RAG-LLMç³»ç»Ÿåœ¨è‡ªåŠ¨åŒ–ç®€å†ç­›é€‰æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œèƒ½å¤Ÿå®ç°æ›´é«˜æ•ˆã€å¯æ‰©å±•çš„æ‹›è˜æµç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.02870v2">PDF</a> Accepted by CVPR 2025 Workshop</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç®€å†ç­›é€‰å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨ç³»ç»ŸåŒ–å¤„ç†å’Œè¯„ä¼°ç®€å†ï¼Œæå‡æ‹›è˜æµç¨‹æ•ˆç‡å’Œä¸ªæ€§åŒ–ç¨‹åº¦ã€‚é€šè¿‡ç®€å†æå–å™¨ã€è¯„ä¼°å™¨ã€æ‘˜è¦å™¨å’Œåˆ†æ•°æ ¼å¼åŒ–å™¨ç­‰å››ä¸ªæ ¸å¿ƒæ™ºèƒ½ä½“çš„ååŒå·¥ä½œï¼Œç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œèå…¥å¤–éƒ¨çŸ¥è¯†æºï¼Œå®ç°ä¸ªæ€§åŒ–æ‹›è˜ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨è‡ªåŠ¨åŒ–ç®€å†ç­›é€‰æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œæœ‰åŠ©äºæé«˜æ‹›è˜æµç¨‹çš„æ•ˆç‡å’Œè§„æ¨¡æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨æ‹›è˜æµç¨‹ä¸­å±•ç°è‡ªåŠ¨åŒ–æ½œåŠ›ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºLLMçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ç”¨äºç®€å†ç­›é€‰ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬ç®€å†æå–å™¨ã€è¯„ä¼°å™¨ã€æ‘˜è¦å™¨å’Œåˆ†æ•°æ ¼å¼åŒ–å™¨ã€‚</li>
<li>æ•´åˆRAGæŠ€æœ¯ä»¥æå‡è¯„ä¼°çš„ä¸Šä¸‹æ–‡ç›¸å…³æ€§ã€‚</li>
<li>èƒ½èå…¥å¤–éƒ¨çŸ¥è¯†æºï¼Œå¦‚è¡Œä¸šä¸“é•¿ã€ä¸“ä¸šè®¤è¯ã€å¤§å­¦æ’åå’Œå…¬å¸æ‹›è˜æ ‡å‡†ã€‚</li>
<li>å®ç°ä¸ªæ€§åŒ–æ‹›è˜ï¼Œç¼©å°AIè‡ªåŠ¨åŒ–ä¸äººæ‰æ‹›è˜ä¹‹é—´çš„å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.02870">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8b7eb2ead8ab676b05c616421e982dcb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e53ae138b6827cf1c1fb40a1dac3fc71.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-312ba8c7016724988110ea64ac6c0ee8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88e4b53fc3daff1dbe67359f6d0b8e0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bc42d765a5a7d4e65d4c6ced45b3ed8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffefd10cc973422131ce40ccabfa42d9.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="SMI-An-Information-Theoretic-Metric-for-Predicting-Model-Knowledge-Solely-from-Pre-Training-Signals"><a href="#SMI-An-Information-Theoretic-Metric-for-Predicting-Model-Knowledge-Solely-from-Pre-Training-Signals" class="headerlink" title="SMI: An Information-Theoretic Metric for Predicting Model Knowledge   Solely from Pre-Training Signals"></a>SMI: An Information-Theoretic Metric for Predicting Model Knowledge   Solely from Pre-Training Signals</h2><p><strong>Authors:Changhao Jiang, Ming Zhang, Junjie Ye, Xiaoran Fan, Yifei Cao, Jiajun Sun, Zhiheng Xi, Shihan Dou, Yi Dong, Yujiong Shen, Jingqi Tong, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Qi Zhang, Tao Gui, Xuanjing Huang</strong></p>
<p>The GPT-4 technical report highlights the possibility of predicting model performance on downstream tasks using only pre-training signals, though detailed methodologies are absent. Such predictive capabilities are essential for resource-efficient pre-training and the construction of task-aligned datasets. In this paper, we aim to predict performance in closed-book question answering (QA), a vital downstream task indicative of a modelâ€™s internal knowledge. We address three primary challenges: (1) limited access to and understanding of pre-training corpora, (2) limitations of current evaluation methods for pre-trained models, and (3) limitations of frequency-based metrics in predicting model performance. In response to these challenges, we conduct large-scale retrieval and semantic analysis across the pre-training corpora of 21 publicly available and 3 custom-trained large language models. Subsequently, we develop a multi-template QA evaluation framework incorporating paraphrased question variants. Building on these foundations, we propose Size-dependent Mutual Information (SMI), an information-theoretic metric that linearly correlates pre-training data characteristics, model size, and QA accuracy, without requiring any additional training. The experimental results demonstrate that SMI outperforms co-occurrence-based baselines, achieving $R^2$ &gt; 0.75 on models with over one billion parameters. Theoretical analysis further reveals the marginal benefits of scaling model size and optimizing data, indicating that the upper limit of specific QA task accuracy is approximately 80%. Our project is available at <a target="_blank" rel="noopener" href="https://github.com/yuhui1038/SMI">https://github.com/yuhui1038/SMI</a>. </p>
<blockquote>
<p>GPT-4æŠ€æœ¯æŠ¥å‘Šå¼ºè°ƒäº†ä»…ä½¿ç”¨é¢„è®­ç»ƒä¿¡å·é¢„æµ‹æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½çš„å¯èƒ½æ€§ï¼Œå°½ç®¡ç¼ºä¹è¯¦ç»†çš„æ–¹æ³•è®ºã€‚è¿™ç§é¢„æµ‹èƒ½åŠ›å¯¹äºèµ„æºé«˜æ•ˆçš„é¢„è®­ç»ƒå’Œä»»åŠ¡å¯¹é½æ•°æ®é›†çš„å»ºè®¾è‡³å…³é‡è¦ã€‚æœ¬æ–‡æ—¨åœ¨é¢„æµ‹å°é—­å¼é—®ç­”ï¼ˆQAï¼‰ä¸­çš„æ€§èƒ½ï¼Œè¿™æ˜¯æŒ‡ç¤ºæ¨¡å‹å†…éƒ¨çŸ¥è¯†çš„é‡è¦ä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬é¢ä¸´ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰å¯¹é¢„è®­ç»ƒè¯­æ–™åº“çš„æœ‰é™è®¿é—®å’Œäº†è§£ï¼Œï¼ˆ2ï¼‰å½“å‰é¢„è®­ç»ƒæ¨¡å‹è¯„ä¼°æ–¹æ³•çš„å±€é™æ€§ï¼Œä»¥åŠï¼ˆ3ï¼‰åŸºäºé¢‘ç‡çš„æŒ‡æ ‡åœ¨é¢„æµ‹æ¨¡å‹æ€§èƒ½æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¯¹21ä¸ªå…¬å¼€å¯ç”¨å’Œ3ä¸ªè‡ªå®šä¹‰è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒè¯­æ–™åº“è¿›è¡Œäº†å¤§è§„æ¨¡æ£€ç´¢å’Œè¯­ä¹‰åˆ†æã€‚éšåï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤šæ¨¡æ¿QAè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æ”¹è¿°é—®é¢˜å˜ä½“ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†Size-dependent Mutual Informationï¼ˆSMIï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¿¡æ¯ç†è®ºåº¦é‡æ ‡å‡†ï¼Œå®ƒçº¿æ€§åœ°å…³è”é¢„è®­ç»ƒæ•°æ®ç‰¹æ€§ã€æ¨¡å‹å¤§å°ä¸é—®ç­”å‡†ç¡®æ€§ï¼Œè€Œæ— éœ€ä»»ä½•é¢å¤–çš„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSMIä¼˜äºåŸºäºå…±ç°çš„åŸºçº¿ï¼Œåœ¨å…·æœ‰è¶…è¿‡äº¿çº§å‚æ•°çš„æ¨¡å‹ä¸Šå®ç°RÂ²&gt; 0.75ã€‚ç†è®ºåˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†æ‰©å¤§æ¨¡å‹è§„æ¨¡å’Œä¼˜åŒ–æ•°æ®çš„è¾¹é™…æ•ˆç›Šï¼Œè¡¨æ˜ç‰¹å®šQAä»»åŠ¡å‡†ç¡®åº¦çš„ä¸Šé™å¤§çº¦ä¸º80%ã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yuhui1038/SMI%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yuhui1038/SMIä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04066v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨é¢„æµ‹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°é—­é—®ç­”ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œè§£å†³ç°æœ‰æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜ã€‚é€šè¿‡å¯¹å¤šä¸ªé¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­æ–™åº“è¿›è¡Œå¤§è§„æ¨¡æ£€ç´¢å’Œè¯­ä¹‰åˆ†æï¼Œæå‡ºä¸€ç§æ–°çš„ä¿¡æ¯ç†è®ºåº¦é‡æ ‡å‡†â€”â€”Size-dependent Mutual Information (SMI)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSMIèƒ½æœ‰æ•ˆé¢„æµ‹æ¨¡å‹åœ¨é—®ç­”ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œä¼˜äºåŸºäºå…±ç°çš„åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ—¨åœ¨é¢„æµ‹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°é—­é—®ç­”ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œè§£å†³é¢„è®­ç»ƒè¯­æ–™åº“çš„æœ‰é™è®¿é—®å’Œç†è§£ã€å½“å‰é¢„è®­ç»ƒæ¨¡å‹è¯„ä¼°æ–¹æ³•çš„å±€é™æ€§ä»¥åŠé¢‘ç‡åŸºç¡€æŒ‡æ ‡åœ¨é¢„æµ‹æ¨¡å‹æ€§èƒ½æ–¹é¢çš„å±€é™æ€§ç­‰ä¸‰å¤§æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å¯¹å¤šä¸ªé¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­æ–™åº“è¿›è¡Œå¤§è§„æ¨¡æ£€ç´¢å’Œè¯­ä¹‰åˆ†æï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ä¿¡æ¯ç†è®ºåº¦é‡æ ‡å‡†â€”â€”Size-dependent Mutual Information (SMI)ã€‚</li>
<li>SMIçº¿æ€§ç›¸å…³é¢„è®­ç»ƒæ•°æ®ç‰¹æ€§ã€æ¨¡å‹å¤§å°ä¸é—®ç­”å‡†ç¡®æ€§ï¼Œä¸”æ— éœ€ä»»ä½•é¢å¤–çš„è®­ç»ƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSMIåœ¨é¢„æµ‹æ¨¡å‹åœ¨é—®ç­”ä»»åŠ¡ä¸­çš„æ€§èƒ½æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºåŸºäºå…±ç°çš„åŸºçº¿æ–¹æ³•ï¼Œå…·æœ‰é«˜è¾¾RÂ²&gt; 0.75çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>ç†è®ºåˆ†ææ­ç¤ºäº†æ‰©å¤§æ¨¡å‹è§„æ¨¡å’Œä¼˜åŒ–æ•°æ®çš„è¾¹é™…æ•ˆç›Šï¼Œå¹¶æŒ‡å‡ºç‰¹å®šé—®ç­”ä»»åŠ¡å‡†ç¡®åº¦çš„ä¸Šé™çº¦ä¸º80%ã€‚</li>
<li>ç ”ç©¶æˆæœå·²å…¬å¼€åœ¨GitHubä¸Šï¼Œå¯ä¾›å…¬ä¼—æŸ¥é˜…å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04066">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8e95760cf01b690639ec31ebd5cd8f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1306d6eef2aa0b8b9183465d6fed4ebe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0f5799ec680cec99df6624580ea2259.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CursorCore-Assist-Programming-through-Aligning-Anything"><a href="#CursorCore-Assist-Programming-through-Aligning-Anything" class="headerlink" title="CursorCore: Assist Programming through Aligning Anything"></a>CursorCore: Assist Programming through Aligning Anything</h2><p><strong>Authors:Hao Jiang, Qi Liu, Rui Li, Shengyu Ye, Shijin Wang</strong></p>
<p>Large language models have been successfully applied to programming assistance tasks, such as code completion, code insertion, and instructional code editing. However, these applications remain insufficiently automated and struggle to effectively integrate various types of information during the programming process, including coding history, current code, and user instructions. In this work, we propose a new conversational framework that comprehensively integrates these information sources, collect data to train our models and evaluate their performance. Firstly, to thoroughly evaluate how well models align with different types of information and the quality of their outputs, we introduce a new benchmark, APEval (Assist Programming Eval), to comprehensively assess the performance of models in programming assistance tasks. Then, for data collection, we develop a data generation pipeline, Programming-Instruct, which synthesizes training data from diverse sources, such as GitHub and online judge platforms. This pipeline can automatically generate various types of messages throughout the programming process. Finally, using this pipeline, we generate 219K samples, fine-tune multiple models, and develop the CursorCore series. We show that CursorCore outperforms other models of comparable size. This framework unifies applications such as inline chat and automated editing, contributes to the advancement of coding assistants. Code, models and data are freely available at <a target="_blank" rel="noopener" href="https://github.com/TechxGenus/CursorCore">https://github.com/TechxGenus/CursorCore</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å·²æˆåŠŸåº”ç”¨äºç¼–ç¨‹è¾…åŠ©ä»»åŠ¡ï¼Œå¦‚ä»£ç è¡¥å…¨ã€ä»£ç æ’å…¥å’ŒæŒ‡ä»¤ä»£ç ç¼–è¾‘ã€‚ç„¶è€Œï¼Œè¿™äº›åº”ç”¨è‡ªåŠ¨åŒ–ç¨‹åº¦ä»ç„¶ä¸è¶³ï¼Œåœ¨ç¼–ç¨‹è¿‡ç¨‹ä¸­éš¾ä»¥æœ‰æ•ˆåœ°æ•´åˆå„ç§ç±»å‹çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç¼–ç å†å²ã€å½“å‰ä»£ç å’Œç”¨æˆ·æŒ‡ä»¤ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¯¹è¯æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…¨é¢æ•´åˆäº†è¿™äº›ä¿¡æ¯æºï¼Œå¹¶æ”¶é›†æ•°æ®ä»¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹å¹¶è¯„ä¼°å…¶æ€§èƒ½ã€‚é¦–å…ˆï¼Œä¸ºäº†å½»åº•è¯„ä¼°æ¨¡å‹å¯¹ä¸åŒç±»å‹ä¿¡æ¯çš„å¥‘åˆç¨‹åº¦åŠå…¶è¾“å‡ºçš„è´¨é‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•APEvalï¼ˆè¾…åŠ©ç¼–ç¨‹è¯„ä¼°ï¼‰ï¼Œä»¥å…¨é¢è¯„ä¼°æ¨¡å‹åœ¨ç¼–ç¨‹è¾…åŠ©ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚å…¶æ¬¡ï¼Œå¯¹äºæ•°æ®æ”¶é›†ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ•°æ®ç”Ÿæˆç®¡é“Programming-Instructï¼Œè¯¥ç®¡é“å¯ä»¥ä»GitHubå’Œåœ¨çº¿è£åˆ¤å¹³å°ç­‰ä¸åŒçš„æ¥æºåˆæˆè®­ç»ƒæ•°æ®ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆç¼–ç¨‹è¿‡ç¨‹ä¸­çš„å„ç§ç±»å‹çš„ä¿¡æ¯ã€‚æœ€åï¼Œä½¿ç”¨è¿™ä¸ªç®¡é“ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†21.9ä¸‡ä¸ªæ ·æœ¬ï¼Œå¯¹å¤šä¸ªæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶å¼€å‘äº†CursorCoreç³»åˆ—ã€‚æˆ‘ä»¬è¯æ˜CursorCoreåœ¨åŒç±»æ¨¡å‹ä¸­è¡¨ç°ä¼˜è¶Šã€‚è¿™ä¸€æ¡†æ¶ç»Ÿä¸€äº†å¦‚å³æ—¶èŠå¤©å’Œè‡ªåŠ¨ç¼–è¾‘ç­‰åº”ç”¨ï¼Œä¸ºç¼–ç åŠ©æ‰‹çš„è¿›æ­¥åšå‡ºäº†è´¡çŒ®ã€‚ç›¸å…³ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/TechxGenus/CursorCore%E4%B8%8A%E5%85%8D%E8%B4%B9%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/TechxGenus/CursorCoreä¸Šå…è´¹è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07002v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç¼–ç¨‹è¾…åŠ©ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„åº”ç”¨å‰æ™¯ï¼Œä½†ä»é¢ä¸´è‡ªåŠ¨åŒ–ç¨‹åº¦ä¸è¶³å’Œéš¾ä»¥æ•´åˆå¤šç§ä¿¡æ¯çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å¯¹è¯æ¡†æ¶ï¼Œæ—¨åœ¨å…¨é¢æ•´åˆç¼–ç¨‹è¿‡ç¨‹ä¸­çš„å„ç§ä¿¡æ¯ï¼Œå¹¶å¼•å…¥æ–°çš„è¯„ä¼°æ ‡å‡†å’Œæ•°æ®ç”Ÿæˆç®¡é“æ¥è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ã€‚æœ€ç»ˆå¼€å‘çš„CursorCoreç³»åˆ—æ¨¡å‹åœ¨ç¼–ç¨‹è¾…åŠ©ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œç»Ÿä¸€äº†å³æ—¶èŠå¤©å’Œè‡ªåŠ¨ç¼–è¾‘ç­‰åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å·²æˆåŠŸåº”ç”¨äºç¼–ç¨‹è¾…åŠ©ä»»åŠ¡ï¼Œå¦‚ä»£ç è¡¥å…¨ã€ä»£ç æ’å…¥å’ŒæŒ‡ä»¤ä»£ç ç¼–è¾‘ã€‚</li>
<li>ç°æœ‰åº”ç”¨è‡ªåŠ¨åŒ–ç¨‹åº¦ä¸è¶³ï¼Œéš¾ä»¥æ•´åˆç¼–ç¨‹è¿‡ç¨‹ä¸­çš„å¤šç§ä¿¡æ¯ï¼Œå¦‚ç¼–ç å†å²ã€å½“å‰ä»£ç å’Œç”¨æˆ·æŒ‡ä»¤ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å¯¹è¯æ¡†æ¶ï¼Œæ—¨åœ¨å…¨é¢æ•´åˆè¿™äº›ä¿¡æ¯æºã€‚</li>
<li>å¼•å…¥äº†æ–°çš„è¯„ä¼°æ ‡å‡†APEvalï¼Œä»¥å…¨é¢è¯„ä¼°æ¨¡å‹åœ¨ç¼–ç¨‹è¾…åŠ©ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>å¼€å‘äº†æ•°æ®ç”Ÿæˆç®¡é“Programming-Instructï¼Œå¯ä»GitHubå’Œåœ¨çº¿è£åˆ¤å¹³å°ç­‰æ¥æºåˆæˆè®­ç»ƒæ•°æ®ã€‚</li>
<li>ä½¿ç”¨è¯¥ç®¡é“ç”Ÿæˆäº†219Kä¸ªæ ·æœ¬ï¼Œå¯¹å¤šä¸ªæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶å¼€å‘äº†CursorCoreç³»åˆ—æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.07002">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7f2d945b601dda9ab9b5d6876d059702.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e76c0aefc82640a3f988eb05ee1ed6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1791000b0ca802989cc426e516ee647.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfadc98ef5d949f65e0c0025ca35f93c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7848c04e983886cbc5cd5dc0afa9d8e5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a4ede4a82af4c3affe4616ecbadf5f2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24c5b800dede338a9603e7ea1678fa4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-980b107ddb618b23ffa8a5d1038aa99e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f0b6a6794e150f60064da936c9ef8a48.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Round-and-Round-We-Go-What-makes-Rotary-Positional-Encodings-useful"><a href="#Round-and-Round-We-Go-What-makes-Rotary-Positional-Encodings-useful" class="headerlink" title="Round and Round We Go! What makes Rotary Positional Encodings useful?"></a>Round and Round We Go! What makes Rotary Positional Encodings useful?</h2><p><strong>Authors:Federico Barbero, Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, Petar VeliÄkoviÄ‡</strong></p>
<p>Positional Encodings (PEs) are a critical component of Transformer-based Large Language Models (LLMs), providing the attention mechanism with important sequence-position information. One of the most popular types of encoding used today in LLMs are Rotary Positional Encodings (RoPE), that rotate the queries and keys based on their relative distance. A common belief is that RoPE is useful because it helps to decay token dependency as relative distance increases. In this work, we argue that this is unlikely to be the core reason. We study the internals of a trained Gemma 7B model to understand how RoPE is being used at a mechanical level. We find that Gemma learns to use RoPE to construct robust â€œpositionalâ€ attention patterns by exploiting the highest frequencies. We also find that, in general, Gemma greatly prefers to use the lowest frequencies of RoPE, which we suspect are used to carry semantic information. We mathematically prove interesting behaviours of RoPE and conduct experiments to verify our findings, proposing a modification of RoPE that fixes some highlighted issues and improves performance. We believe that this work represents an interesting step in better understanding PEs in LLMs, which we believe holds crucial value for scaling LLMs to large sizes and context lengths. </p>
<blockquote>
<p>ä½ç½®ç¼–ç ï¼ˆPEsï¼‰æ˜¯åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä¸ºæ³¨æ„åŠ›æœºåˆ¶æä¾›é‡è¦çš„åºåˆ—ä½ç½®ä¿¡æ¯ã€‚ç›®å‰LLMä¸­æœ€æµè¡Œçš„ç¼–ç ç±»å‹ä¹‹ä¸€æ˜¯æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ï¼Œå®ƒæ ¹æ®æŸ¥è¯¢å’Œé”®ä¹‹é—´çš„ç›¸å¯¹è·ç¦»è¿›è¡Œæ—‹è½¬ã€‚äººä»¬æ™®éè®¤ä¸ºRoPEæ˜¯æœ‰ç”¨çš„ï¼Œå› ä¸ºå®ƒæœ‰åŠ©äºéšç€ç›¸å¯¹è·ç¦»çš„å¢åŠ è€Œè¡°å‡ä»¤ç‰Œä¾èµ–æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™ä¸å¤ªå¯èƒ½æˆä¸ºæ ¸å¿ƒåŸå› ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸€ä¸ªè®­ç»ƒè¿‡çš„Gemma 7Bæ¨¡å‹çš„å†…éƒ¨ï¼Œä»¥äº†è§£RoPEåœ¨æœºæ¢°çº§åˆ«æ˜¯å¦‚ä½•è¢«ä½¿ç”¨çš„ã€‚æˆ‘ä»¬å‘ç°Gemmaå­¦ä¼šäº†åˆ©ç”¨æœ€é«˜é¢‘ç‡ä½¿ç”¨RoPEæ¥æ„å»ºç¨³å¥çš„â€œä½ç½®â€æ³¨æ„åŠ›æ¨¡å¼ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œæ€»çš„æ¥è¯´ï¼ŒGemmaæ›´å–œæ¬¢ä½¿ç”¨RoPEçš„æœ€ä½é¢‘ç‡ï¼Œæˆ‘ä»¬æ€€ç–‘è¿™äº›é¢‘ç‡ç”¨äºä¼ é€’è¯­ä¹‰ä¿¡æ¯ã€‚æˆ‘ä»¬æ•°å­¦åœ°è¯æ˜äº†RoPEçš„æœ‰è¶£è¡Œä¸ºï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„å‘ç°ï¼Œæå‡ºäº†å¯¹RoPEçš„ä¿®æ”¹ï¼Œè§£å†³äº†æŸäº›çªå‡ºçš„é—®é¢˜å¹¶æé«˜äº†æ€§èƒ½ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™é¡¹å·¥ä½œåœ¨æ›´å¥½åœ°ç†è§£LLMä¸­çš„PEsæ–¹é¢ä»£è¡¨äº†æœ‰è¶£çš„ä¸€æ­¥ï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™å¯¹äºå°†LLMæ‰©å±•åˆ°å¤§è§„æ¨¡å’Œä¸Šä¸‹æ–‡é•¿åº¦å…·æœ‰å…³é”®ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.06205v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¯¥ç ”ç©¶æ¢è®¨äº†ä½ç½®ç¼–ç ï¼ˆPEï¼‰åœ¨åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å…³é”®ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ã€‚ç ”ç©¶å‘ç°ï¼ŒRoPEå¹¶éä¸»è¦ç”¨äºè¡°å‡ä»¤ç‰Œä¾èµ–å…³ç³»ï¼Œè€Œæ˜¯ç”¨äºæ„å»ºç¨³å¥çš„â€œä½ç½®â€æ³¨æ„åŠ›æ¨¡å¼ï¼Œå¹¶åˆ©ç”¨æœ€é«˜é¢‘ç‡è¿›è¡Œåˆ©ç”¨ã€‚æ­¤å¤–ï¼Œæ¨¡å‹æ›´å€¾å‘äºä½¿ç”¨RoPEçš„æœ€ä½é¢‘ç‡éƒ¨åˆ†ï¼Œå¯èƒ½ç”¨äºä¼ é€’è¯­ä¹‰ä¿¡æ¯ã€‚ç ”ç©¶å¯¹RoPEçš„æ•°å­¦è¡Œä¸ºè¿›è¡Œäº†éªŒè¯å¹¶æå‡ºäº†æ”¹è¿›æ–¹æ¡ˆï¼Œä¸ºè§£å†³ä¸€äº›å…³é”®é—®é¢˜å¹¶æé«˜æ€§èƒ½å¥ å®šäº†åŸºç¡€ã€‚è¯¥å·¥ä½œå¯¹äºç†è§£LLMä¸­çš„ä½ç½®ç¼–ç å…·æœ‰é‡å¤§æ„ä¹‰ï¼Œå¯¹äºæ‰©å¤§LLMè§„æ¨¡å’Œä¸Šä¸‹æ–‡é•¿åº¦è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä½ç½®ç¼–ç ï¼ˆPEï¼‰æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å…³é”®ç»„ä»¶ï¼Œä¸ºæ³¨æ„åŠ›æœºåˆ¶æä¾›é‡è¦çš„åºåˆ—ä½ç½®ä¿¡æ¯ã€‚</li>
<li>æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰è¢«ç”¨äºæ„å»ºç¨³å¥çš„â€œä½ç½®â€æ³¨æ„åŠ›æ¨¡å¼ã€‚</li>
<li>RoPEå¹¶éä¸»è¦ç”¨äºè¡°å‡ä»¤ç‰Œä¾èµ–å…³ç³»ï¼Œè€Œæ˜¯é€šè¿‡åˆ©ç”¨æœ€é«˜é¢‘ç‡æ¥å·¥ä½œã€‚</li>
<li>æ¨¡å‹å€¾å‘äºä½¿ç”¨RoPEçš„æœ€ä½é¢‘ç‡éƒ¨åˆ†ï¼Œè¿™éƒ¨åˆ†å¯èƒ½æ¶‰åŠä¼ é€’è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>ç ”ç©¶å¯¹RoPEçš„æ•°å­¦è¡Œä¸ºè¿›è¡Œäº†éªŒè¯ï¼Œæå‡ºäº†ä¸€äº›æœ‰è¶£çš„å‘ç°ã€‚</li>
<li>å¯¹RoPEè¿›è¡Œäº†æ”¹è¿›ï¼Œè§£å†³äº†æŸäº›å…³é”®é—®é¢˜ï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.06205">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ce9e94a395af72c48cd106fc310d393f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6cb0c5f171adab1901d953dd99263450.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea01e6940b0add56929d498e66785805.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de5e79902f3d7877da766c9cc3a68d15.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Exploring-the-Trade-Offs-Quantization-Methods-Task-Difficulty-and-Model-Size-in-Large-Language-Models-From-Edge-to-Giant"><a href="#Exploring-the-Trade-Offs-Quantization-Methods-Task-Difficulty-and-Model-Size-in-Large-Language-Models-From-Edge-to-Giant" class="headerlink" title="Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and   Model Size in Large Language Models From Edge to Giant"></a>Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and   Model Size in Large Language Models From Edge to Giant</h2><p><strong>Authors:Jemin Lee, Sihyeong Park, Jinse Kwon, Jihun Oh, Yongin Kwon</strong></p>
<p>Quantization has gained attention as a promising solution for the cost-effective deployment of large and small language models. However, most prior work has been limited to perplexity or basic knowledge tasks and lacks a comprehensive evaluation of recent models like Llama-3.3. In this paper, we conduct a comprehensive evaluation of instruction-tuned models spanning 1B to 405B parameters, applying four quantization methods across 13 datasets. Our findings reveal that (1) quantized models generally surpass smaller FP16 baselines, yet they often struggle with instruction-following and hallucination detection; (2) FP8 consistently emerges as the most robust option across tasks, and AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller models can suffer severe accuracy drops at 4-bit quantization, while 70B-scale models maintain stable performance; (4) notably, \textit{hard} tasks do not always experience the largest accuracy losses, indicating that quantization magnifies a modelâ€™s inherent weaknesses rather than simply correlating with task difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant performance declines in Coding and STEM tasks, though it occasionally reports improvements in reasoning. </p>
<blockquote>
<p>é‡åŒ–æŠ€æœ¯ä½œä¸ºéƒ¨ç½²å¤§å°è¯­è¨€æ¨¡å‹çš„ä¸€ç§å…·æœ‰æˆæœ¬æ•ˆç›Šçš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆè€Œå¤‡å—å…³æ³¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ—©æœŸçš„å·¥ä½œä»…é™äºå›°æƒ‘åº¦æˆ–åŸºç¡€çŸ¥è¯†ä»»åŠ¡ï¼Œç¼ºä¹å¯¹è¯¸å¦‚Llama-3.3ç­‰æœ€æ–°æ¨¡å‹çš„å…¨é¢è¯„ä¼°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œè¿™äº›æ¨¡å‹è·¨è¶Šäº†1Båˆ°405Bçš„å‚æ•°èŒƒå›´ï¼Œåœ¨13ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨äº†å››ç§é‡åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼šï¼ˆ1ï¼‰é‡åŒ–æ¨¡å‹æ€»ä½“ä¸Šè¶…è¿‡äº†è¾ƒå°çš„FP16åŸºå‡†æ¨¡å‹ï¼Œä½†åœ¨éµå¾ªæŒ‡ä»¤å’Œå¹»è§‰æ£€æµ‹æ–¹é¢å¾€å¾€å­˜åœ¨å›°éš¾ï¼›ï¼ˆ2ï¼‰FP8åœ¨å„é¡¹ä»»åŠ¡ä¸­å§‹ç»ˆè¡¨ç°å‡ºæœ€ç¨³å¥çš„é€‰æ‹©ï¼ŒAWQåœ¨ä»…æƒé‡é‡åŒ–æ–¹é¢å¾€å¾€ä¼˜äºGPTQï¼›ï¼ˆ3ï¼‰è¾ƒå°æ¨¡å‹åœ¨4ä½é‡åŒ–æ—¶å¯èƒ½ä¼šé­å—ä¸¥é‡çš„ç²¾åº¦ä¸‹é™ï¼Œè€Œ70Bè§„æ¨¡çš„æ¨¡å‹åˆ™èƒ½ä¿æŒç¨³å®šçš„æ€§èƒ½ï¼›ï¼ˆ4ï¼‰å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œéš¾åº¦å¤§çš„ä»»åŠ¡å¹¶ä¸æ€»æ˜¯ç»å†æœ€å¤§çš„ç²¾åº¦æŸå¤±ï¼Œè¿™è¡¨æ˜é‡åŒ–æ”¾å¤§äº†ä¸€ä¸ªæ¨¡å‹çš„å›ºæœ‰å¼±ç‚¹ï¼Œè€Œä¸æ˜¯ç®€å•åœ°ä¸ä»»åŠ¡éš¾åº¦ç›¸å…³ï¼›ï¼ˆ5ï¼‰åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„åˆ¤æ–­ï¼ˆMT-Benchï¼‰çªå‡ºäº†ç¼–ç å’ŒSTEMä»»åŠ¡çš„æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œå°½ç®¡å®ƒæœ‰æ—¶ä¼šæŠ¥å‘Šæ¨ç†æ–¹é¢çš„æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11055v4">PDF</a> Accepted in IJCAI 2025, 21 pages, 2 figure</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†æŒ‡ä»¤å¾®è°ƒæ¨¡å‹çš„é‡åŒ–æ•ˆæœï¼Œæ¶‰åŠä»1Båˆ°405Bå‚æ•°çš„æ¨¡å‹èŒƒå›´ã€‚å®éªŒé‡‡ç”¨äº†å››ç§é‡åŒ–æ–¹æ³•ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé‡åŒ–æ¨¡å‹åœ¨æŸäº›æ–¹é¢è¶…è¶Šäº†è¾ƒå°æ¨¡å‹çš„åŸºçº¿æ°´å¹³ï¼Œä½†åœ¨æŒ‡ä»¤éµå¾ªå’Œå¹»è§‰æ£€æµ‹æ–¹é¢è¡¨ç°æ¬ ä½³ã€‚å…¶ä¸­ï¼ŒFP8åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­æœ€å…·ç¨³å¥æ€§ï¼ŒAWQåœ¨é‡é‡çº§é‡åŒ–æ–¹é¢ä¼˜äºGPTQã€‚è¾ƒå°çš„æ¨¡å‹åœ¨å››æ¯”ç‰¹é‡åŒ–æ—¶ä¼šé‡åˆ°å‡†ç¡®åº¦çš„ä¸¥é‡ä¸‹é™ï¼Œè€Œå¤§å‹æ¨¡å‹åˆ™èƒ½ä¿æŒç¨³å®šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé‡åŒ–ä¼šæ”¾å¤§æ¨¡å‹çš„å›ºæœ‰å¼±ç‚¹ï¼Œè€Œéå•çº¯ä¸ä»»åŠ¡éš¾åº¦ç›¸å…³ã€‚LLMåˆ¤å®šï¼ˆMT-Benchï¼‰æ˜¾ç¤ºç¼–ç å’ŒSTEMä»»åŠ¡çš„æ€§èƒ½æ˜æ˜¾ä¸‹é™ï¼Œä½†åœ¨æ¨ç†ä»»åŠ¡ä¸­æœ‰æ—¶ä¼šæœ‰æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é‡åŒ–ä½œä¸ºéƒ¨ç½²å¤§å°è¯­è¨€æ¨¡å‹çš„ç»æµè§£å†³æ–¹æ¡ˆå·²å¼•èµ·å…³æ³¨ã€‚</li>
<li>æœ¬ç ”ç©¶å…¨é¢è¯„ä¼°äº†æŒ‡ä»¤å¾®è°ƒæ¨¡å‹çš„é‡åŒ–æ•ˆæœï¼Œæ¶‰åŠå¤šç§å‚æ•°è§„æ¨¡çš„æ¨¡å‹ã€‚</li>
<li>é‡åŒ–æ¨¡å‹åœ¨æŸäº›æ–¹é¢è¶…è¶Šè¾ƒå°æ¨¡å‹çš„åŸºçº¿æ°´å¹³ï¼Œä½†åœ¨æŒ‡ä»¤éµå¾ªå’Œå¹»è§‰æ£€æµ‹æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>FP8åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­æœ€å…·ç¨³å¥æ€§ï¼ŒAWQåœ¨é‡é‡çº§é‡åŒ–ä¸Šè¡¨ç°è¾ƒå¥½ã€‚</li>
<li>è¾ƒå°æ¨¡å‹åœ¨å››æ¯”ç‰¹é‡åŒ–æ—¶å‡†ç¡®åº¦ä¸‹é™ä¸¥é‡ï¼Œè€Œå¤§å‹æ¨¡å‹æ€§èƒ½ç¨³å®šã€‚</li>
<li>é‡åŒ–ä¼šæ”¾å¤§æ¨¡å‹çš„å›ºæœ‰å¼±ç‚¹ï¼Œå¹¶éä»…ä¸ä»»åŠ¡éš¾åº¦ç›¸å…³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-83adab20b6e7b2a4016c73f69ce45fc2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c6953c0fd643e48a949e3db7843ca22.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f76cc5736fac1d2ddc5d3debffb65997.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6748fa4dc153505de77db20a110c062f.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Training-Ultra-Long-Context-Language-Model-with-Fully-Pipelined-Distributed-Transformer"><a href="#Training-Ultra-Long-Context-Language-Model-with-Fully-Pipelined-Distributed-Transformer" class="headerlink" title="Training Ultra Long Context Language Model with Fully Pipelined   Distributed Transformer"></a>Training Ultra Long Context Language Model with Fully Pipelined   Distributed Transformer</h2><p><strong>Authors:Jinghan Yao, Sam Ade Jacobs, Masahiro Tanaka, Olatunji Ruwase, Hari Subramoni, Dhabaleswar K. Panda</strong></p>
<p>Large Language Models (LLMs) with long context capabilities are integral to complex tasks in natural language processing and computational biology, such as text generation and protein sequence analysis. However, training LLMs directly on extremely long contexts demands considerable GPU resources and increased memory, leading to higher costs and greater complexity. Alternative approaches that introduce long context capabilities via downstream finetuning or adaptations impose significant design limitations. In this paper, we propose Fully Pipelined Distributed Transformer (FPDT) for efficiently training long-context LLMs with extreme hardware efficiency. For GPT and Llama models, we achieve a 16x increase in sequence length that can be trained on the same hardware compared to current state-of-the-art solutions. With our dedicated sequence chunk pipeline design, we can now train 8B LLM with 2 million sequence length on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed FPDT is agnostic to existing training techniques and is proven to work efficiently across different LLM models. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›ï¼Œå¯¹äºè‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—ç”Ÿç‰©å­¦ä¸­çš„å¤æ‚ä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬ç”Ÿæˆå’Œè›‹ç™½è´¨åºåˆ—åˆ†æï¼‰è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç›´æ¥åœ¨æé•¿çš„æ–‡æœ¬ä¸Šè®­ç»ƒLLMéœ€è¦å¤§é‡çš„GPUèµ„æºå’Œå¢åŠ çš„å†…å­˜ï¼Œå¯¼è‡´æˆæœ¬å’Œå¤æ‚æ€§å¢åŠ ã€‚é€šè¿‡ä¸‹æ¸¸å¾®è°ƒæˆ–é€‚åº”å¼•å…¥é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›çš„æ›¿ä»£æ–¹æ³•å¸¦æ¥äº†æ˜¾è‘—çš„è®¾è®¡é™åˆ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å…¨ç®¡é“åˆ†å¸ƒå¼è½¬æ¢å™¨ï¼ˆFPDTï¼‰ï¼Œä»¥æé«˜çš„ç¡¬ä»¶æ•ˆç‡æœ‰æ•ˆåœ°è®­ç»ƒå…·æœ‰é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚å¯¹äºGPTå’Œéª†é©¬æ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨ç›¸åŒçš„ç¡¬ä»¶ä¸Šå®ç°äº†åºåˆ—é•¿åº¦çš„16å€å¢åŠ ï¼Œè¿™è¶…è¿‡äº†å½“å‰æœ€å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡æˆ‘ä»¬ä¸“é—¨çš„åºåˆ—å—ç®¡é“è®¾è®¡ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥åœ¨ä»…ä½¿ç”¨4ä¸ªGPUçš„æƒ…å†µä¸‹è®­ç»ƒå…·æœ‰2ç™¾ä¸‡åºåˆ—é•¿åº¦çš„8Bå¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒè¶…è¿‡55%çš„æœ€å¤§æµ®ç‚¹å•å…ƒä½¿ç”¨ç‡ã€‚æˆ‘ä»¬æå‡ºçš„FPDTä¸ç°æœ‰çš„è®­ç»ƒæŠ€æœ¯æ— å…³ï¼Œå¹¶å·²è¯æ˜åœ¨ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­éƒ½èƒ½æœ‰æ•ˆå·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.16978v2">PDF</a> The Eighth Annual Conference on Machine Learning and Systems   (MLSysâ€™25)</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Fully Pipelined Distributed Transformerï¼ˆFPDTï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨é«˜æ•ˆè®­ç»ƒå…·æœ‰é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼ŒåŒæ—¶ä¿æŒæé«˜çš„ç¡¬ä»¶æ•ˆç‡ã€‚ä¸ç°æœ‰è§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼ŒFPDTå¯å®ç°åºåˆ—é•¿åº¦è¾¾å½“å‰å…ˆè¿›æ°´å¹³çš„16å€å¢é•¿ï¼ŒåŒæ—¶åœ¨ç›¸åŒç¡¬ä»¶ä¸Šè¿›è¡Œè®­ç»ƒã€‚é€šè¿‡ä¸“ç”¨çš„åºåˆ—å—æµæ°´çº¿è®¾è®¡ï¼Œç°åœ¨åªéœ€ä½¿ç”¨4ä¸ªGPUå°±å¯ä»¥è®­ç»ƒæ‹¥æœ‰è¶…è¿‡2ç™¾ä¸‡åºåˆ—é•¿åº¦çš„æ¨¡å‹ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒFPDTé€‚ç”¨äºä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡ç»“åˆå…ˆå‰çš„æŠ€æœ¯å’Œé«˜æ•ˆæ¶æ„åˆ›æ–°æ–¹æ³•å®ç°å¼ºå¤§ä¸”æ€§èƒ½å‡ºä¼—çš„è¯­è¨€æ¨¡å‹æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚æˆ‘ä»¬åªéœ€å‡ ä¸ªæœåŠ¡å™¨èŠ‚ç‚¹å³å¯å®ç°å¤§è§„æ¨¡æ¨¡å‹çš„è®­ç»ƒï¼Œè¿™æå¤§åœ°é™ä½äº†è®­ç»ƒæˆæœ¬å¹¶æé«˜äº†æ•ˆç‡ã€‚æˆ‘ä»¬ç›¸ä¿¡FPDTå°†æå¤§åœ°æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œå¹¶ä¿ƒè¿›å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—ç”Ÿç‰©å­¦ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹äºå¤„ç†å¤æ‚çš„è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—ç”Ÿç‰©å­¦ä»»åŠ¡è‡³å…³é‡è¦ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆå’Œè›‹ç™½è´¨åºåˆ—åˆ†æã€‚ç„¶è€Œï¼Œè®­ç»ƒå…·æœ‰é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›çš„LLMséœ€è¦å¤§é‡çš„GPUèµ„æºå’Œå†…å­˜ï¼Œå¯¼è‡´æˆæœ¬å¢åŠ å’Œå¤æ‚æ€§æé«˜ã€‚</li>
<li>å½“å‰ç ”ç©¶ä¸­æå‡ºäº†ä¸€ç§åä¸ºFully Pipelined Distributed Transformerï¼ˆFPDTï¼‰çš„æ–°æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡ä¸“é—¨çš„åºåˆ—å—æµæ°´çº¿è®¾è®¡æé«˜äº†è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ã€‚è¿™ä¸€æ–¹æ³•åœ¨ä¿æŒé«˜ç¡¬ä»¶æ•ˆç‡çš„åŒæ—¶å®ç°äº†å¯¹è¶…é•¿åºåˆ—çš„è®­ç»ƒèƒ½åŠ›å¢é•¿ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•å¯¹ç°æœ‰çš„è®­ç»ƒæŠ€æœ¯å…·æœ‰ä¸­ç«‹æ€§ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒçš„LLMæ¨¡å‹ä¸­æœ‰æ•ˆè¿è¡Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.16978">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bce393bfb0bbc20f522dde95d5bd8e65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fc2bc76d26bfd5b6e283d9d79a2e075.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41907849e3c1b4fbebd2ca792ce6f2a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d28caf71c49e7fbb665be8c8f592985d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60f4cf27d8aea18ec33b6cad0c8d9cea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-885e9a0fc5adfc89ee805077e92dce01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e91be00527a787a21347cefdf0b097e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a04296cc208c42aa8bf40d100bf4d51.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="TrackFormers-In-Search-of-Transformer-Based-Particle-Tracking-for-the-High-Luminosity-LHC-Era"><a href="#TrackFormers-In-Search-of-Transformer-Based-Particle-Tracking-for-the-High-Luminosity-LHC-Era" class="headerlink" title="TrackFormers: In Search of Transformer-Based Particle Tracking for the   High-Luminosity LHC Era"></a>TrackFormers: In Search of Transformer-Based Particle Tracking for the   High-Luminosity LHC Era</h2><p><strong>Authors:Sascha Caron, Nadezhda Dobreva, Antonio Ferrer SÃ¡nchez, JosÃ© D. MartÃ­n-Guerrero, Uraz Odyurt, Roberto Ruiz de Austri Bazan, Zef Wolffs, Yue Zhao</strong></p>
<p>High-Energy Physics experiments are facing a multi-fold data increase with every new iteration. This is certainly the case for the upcoming High-Luminosity LHC upgrade. Such increased data processing requirements forces revisions to almost every step of the data processing pipeline. One such step in need of an overhaul is the task of particle track reconstruction, a.k.a., tracking. A Machine Learning-assisted solution is expected to provide significant improvements, since the most time-consuming step in tracking is the assignment of hits to particles or track candidates. This is the topic of this paper.   We take inspiration from large language models. As such, we consider two approaches: the prediction of the next word in a sentence (next hit point in a track), as well as the one-shot prediction of all hits within an event. In an extensive design effort, we have experimented with three models based on the Transformer architecture and one model based on the U-Net architecture, performing track association predictions for collision event hit points. In our evaluation, we consider a spectrum of simple to complex representations of the problem, eliminating designs with lower metrics early on. We report extensive results, covering both prediction accuracy (score) and computational performance. We have made use of the REDVID simulation framework, as well as reductions applied to the TrackML data set, to compose five data sets from simple to complex, for our experiments. The results highlight distinct advantages among different designs in terms of prediction accuracy and computational performance, demonstrating the efficiency of our methodology. Most importantly, the results show the viability of a one-shot encoder-classifier based Transformer solution as a practical approach for the task of tracking. </p>
<blockquote>
<p>é«˜èƒ½ç‰©ç†å®éªŒé¢ä¸´ç€æ¯ä¸€æ¬¡è¿­ä»£éƒ½å‘ˆç°å¤šé‡æ•°æ®å¢é•¿çš„æƒ…å†µã€‚å³å°†åˆ°æ¥çš„é«˜äº®åº¦LHCå‡çº§ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™ç§å¢åŠ çš„æ•°æ®å¤„ç†è¦æ±‚å‡ ä¹è¿«ä½¿æ•°æ®å¤„ç†æµç¨‹çš„æ¯ä¸€æ­¥éƒ½è¿›è¡Œä¿®æ”¹ã€‚éœ€è¦å…¨é¢æ£€ä¿®çš„æ­¥éª¤ä¹‹ä¸€æ˜¯ç²’å­è½¨è¿¹é‡å»ºçš„ä»»åŠ¡ï¼Œä¹Ÿç§°ä¸ºè·Ÿè¸ªã€‚ç”±äºè·Ÿè¸ªä¸­æœ€è€—æ—¶çš„æ­¥éª¤æ˜¯å°†å‘½ä¸­ç‚¹åˆ†é…ç»™ç²’å­æˆ–è½¨è¿¹å€™é€‰è€…ï¼Œå› æ­¤é¢„è®¡æœºå™¨å­¦ä¹ è¾…åŠ©è§£å†³æ–¹æ¡ˆå°†å¸¦æ¥é‡å¤§æ”¹è¿›ã€‚è¿™æ˜¯æœ¬æ–‡çš„ä¸»é¢˜ã€‚æˆ‘ä»¬ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ±²å–çµæ„Ÿã€‚å› æ­¤ï¼Œæˆ‘ä»¬è€ƒè™‘ä¸¤ç§æ–¹æ³•ï¼šé¢„æµ‹å¥å­ä¸­çš„ä¸‹ä¸€ä¸ªè¯ï¼ˆè½¨è¿¹ä¸­çš„ä¸‹ä¸€ä¸ªå‘½ä¸­ç‚¹ï¼‰ï¼Œä»¥åŠäº‹ä»¶å†…æ‰€æœ‰å‘½ä¸­ç‚¹çš„ä¸€æ¬¡æ€§é¢„æµ‹ã€‚åœ¨å¹¿æ³›çš„è®¾è®¡å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°è¯•äº†ä¸‰åŸºäºTransformeræ¶æ„çš„æ¨¡å‹å’Œä¸€ç§åŸºäºU-Netæ¶æ„çš„æ¨¡å‹ï¼Œå¯¹ç¢°æ’äº‹ä»¶å‘½ä¸­ç‚¹è¿›è¡Œè½¨è¿¹å…³è”é¢„æµ‹ã€‚åœ¨æˆ‘ä»¬çš„è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘äº†é—®é¢˜çš„ç®€å•åˆ°å¤æ‚è¡¨ç¤ºå½¢å¼ï¼Œå¹¶å°½æ—©æ·˜æ±°äº†æŒ‡æ ‡è¾ƒä½çš„æ–¹æ¡ˆã€‚æˆ‘ä»¬æŠ¥å‘Šäº†å¹¿æ³›çš„ç»“æœï¼Œæ¶µç›–äº†é¢„æµ‹ç²¾åº¦ï¼ˆå¾—åˆ†ï¼‰å’Œè®¡ç®—æ€§èƒ½ã€‚æˆ‘ä»¬åˆ©ç”¨REDVIDæ¨¡æ‹Ÿæ¡†æ¶ä»¥åŠå¯¹TrackMLæ•°æ®é›†çš„ç®€åŒ–ï¼Œä¸ºæˆ‘ä»¬çš„å®éªŒä»ç®€å•åˆ°å¤æ‚åœ°åˆ›å»ºäº†äº”ä¸ªæ•°æ®é›†ã€‚ç»“æœçªå‡ºäº†ä¸åŒè®¾è®¡åœ¨é¢„æµ‹ç²¾åº¦å’Œè®¡ç®—æ€§èƒ½æ–¹é¢çš„ä¼˜åŠ¿ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œç»“æœè¡¨æ˜åŸºäºä¸€æ¬¡æ€§ç¼–ç å™¨åˆ†ç±»å™¨è§£å†³æ–¹æ¡ˆçš„Transformeræ˜¯ä¸€ç§ç”¨äºè·Ÿè¸ªä»»åŠ¡çš„å®ç”¨æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.07179v3">PDF</a> </p>
<p><strong>Summary</strong><br>é«˜èƒ½é‡ç‰©ç†å®éªŒé¢ä¸´æ•°æ®é‡å¤§å¢çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å³å°†å‡çº§çš„é«˜äº®åº¦LHCã€‚æ•°æ®å¤„ç†çš„æ¯ä¸ªç¯èŠ‚éƒ½éœ€è¦é‡æ–°å®¡è§†å’Œä¼˜åŒ–ã€‚è®ºæ–‡ä¸­æ¢è®¨äº†åˆ©ç”¨æœºå™¨å­¦ä¹ è¾…åŠ©çš„è§£å†³æ–¹æ¡ˆæ¥å¤„ç†ç²’å­è½¨è¿¹é‡å»ºé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºTransformeræ¶æ„å’ŒU-Netæ¶æ„çš„æ¨¡å‹ï¼Œå®ç°äº†ç¢°æ’äº‹ä»¶ä¸­çš„è½¨è¿¹å…³è”é¢„æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŒè®¾è®¡åœ¨é¢„æµ‹ç²¾åº¦å’Œè®¡ç®—æ€§èƒ½ä¸Šå„æœ‰ä¼˜åŠ¿ï¼Œå…¶ä¸­åŸºäºç¼–ç å™¨åˆ†ç±»å™¨çš„ä¸€ç«™å¼Transformerè§£å†³æ–¹æ¡ˆè¡¨ç°å‡ºè¾ƒé«˜çš„æ•ˆç‡å’Œå¯è¡Œæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é«˜èƒ½é‡ç‰©ç†å®éªŒé¢ä¸´æ•°æ®å¢é•¿é—®é¢˜ï¼Œæ¯ä¸ªè¿­ä»£ä¸­æ•°æ®é‡æ€¥å‰§å¢åŠ ã€‚</li>
<li>é«˜äº®åº¦LHCå‡çº§ä¹Ÿé¢ä¸´ç±»ä¼¼é—®é¢˜ï¼Œéœ€è¦ä¼˜åŒ–æ•°æ®å¤„ç†æµç¨‹ã€‚</li>
<li>ç²’å­è½¨è¿¹é‡å»ºæ˜¯æ•°æ®å¤„ç†ä¸­çš„ä¸€ä¸ªé‡è¦ç¯èŠ‚ï¼Œæœºå™¨å­¦ä¹ è¾…åŠ©çš„è§£å†³æ–¹æ¡ˆæœ‰æœ›æ˜¾è‘—æé«˜æ•ˆç‡ã€‚</li>
<li>è®ºæ–‡æ¢è®¨äº†åŸºäºTransformeræ¶æ„å’ŒU-Netæ¶æ„çš„æ¨¡å‹åœ¨è½¨è¿¹å…³è”é¢„æµ‹ä¸­çš„åº”ç”¨ã€‚</li>
<li>å®éªŒç»“æœå±•ç¤ºäº†ä¸åŒè®¾è®¡çš„ä¼˜åŠ¿ï¼ŒåŒ…æ‹¬é¢„æµ‹ç²¾åº¦å’Œè®¡ç®—æ€§èƒ½æ–¹é¢çš„å·®å¼‚ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.07179">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58db4760138c59f794575b611edebfca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-17669adfa0ca8445d0907be9f76752ee.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-15/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-15/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-15/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-01c303c423bf90655c4783f236acb38f.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-15  Towards Autonomous UAV Visual Object Search in City Space Benchmark and   Agentic Methodology
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-15/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0e8d199658f9832bcaa9874e192322f2.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-15  AC-Reason Towards Theory-Guided Actual Causality Reasoning with Large   Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27927k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
