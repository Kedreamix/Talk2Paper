<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-05-15  LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from   EHRs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-35e69c7c5f8a6822b6618c947b303201.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    33 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-05-15-更新"><a href="#2025-05-15-更新" class="headerlink" title="2025-05-15 更新"></a>2025-05-15 更新</h1><h2 id="LLM-based-Prompt-Ensemble-for-Reliable-Medical-Entity-Recognition-from-EHRs"><a href="#LLM-based-Prompt-Ensemble-for-Reliable-Medical-Entity-Recognition-from-EHRs" class="headerlink" title="LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from   EHRs"></a>LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from   EHRs</h2><p><strong>Authors:K M Sajjadul Islam, Ayesha Siddika Nipu, Jiawei Wu, Praveen Madiraju</strong></p>
<p>Electronic Health Records (EHRs) are digital records of patient information, often containing unstructured clinical text. Named Entity Recognition (NER) is essential in EHRs for extracting key medical entities like problems, tests, and treatments to support downstream clinical applications. This paper explores prompt-based medical entity recognition using large language models (LLMs), specifically GPT-4o and DeepSeek-R1, guided by various prompt engineering techniques, including zero-shot, few-shot, and an ensemble approach. Among all strategies, GPT-4o with prompt ensemble achieved the highest classification performance with an F1-score of 0.95 and recall of 0.98, outperforming DeepSeek-R1 on the task. The ensemble method improved reliability by aggregating outputs through embedding-based similarity and majority voting. </p>
<blockquote>
<p>电子健康记录（EHRs）是患者信息的数字记录，通常包含非结构化的临床文本。在电子健康记录中，命名实体识别（NER）对于提取关键医疗实体至关重要，如问题、测试和治疗方法，以支持下游的临床应用。本文探讨了基于提示的医疗实体识别，使用大型语言模型（LLM），特别是GPT-4o和DeepSeek-R1，在各种提示工程技术指导下，包括零样本、少样本和集成方法。在所有策略中，GPT-4o通过提示集成取得了最高的分类性能，F1分数为0.95，召回率为0.98，在该任务上优于DeepSeek-R1。集成方法通过基于嵌入的相似性和多数投票来聚合输出，提高了可靠性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08704v1">PDF</a> IEEE 26th International Conference on Information Reuse and   Integration for Data Science (IRI 2025), San Jose, CA, USA</p>
<p><strong>Summary</strong></p>
<p>本文主要探讨了基于大型语言模型的命名实体识别在电子健康记录（EHRs）中的应用。通过运用多种提示工程技术，包括零样本、少样本和集成方法，使用GPT-4o和DeepSeek-R1进行医学实体识别。其中，GPT-4o与提示集成策略取得了最高的分类性能，F1分数为0.95，召回率为0.98，在该任务上优于DeepSeek-R1。集成方法通过基于嵌入的相似性和多数投票法聚合输出，提高了可靠性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>电子健康记录（EHRs）中的命名实体识别（NER）是关键技术，可提取问题、测试和治疗等关键医疗实体，支持下游临床应用。</li>
<li>本文探讨了基于大型语言模型（LLMs）的提示工程技术在医学实体识别中的应用。</li>
<li>GPT-4o和DeepSeek-R1被用于此任务，其中GPT-4o与提示集成策略表现出最佳性能。</li>
<li>集成方法通过嵌入相似性度和多数投票法提高可靠性。</li>
<li>GPT-4o在F1分数和召回率方面表现出较高的分类性能。</li>
<li>提示工程技术，如零样本和少样本方法，也被用于医学实体识别任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08704">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f6117f0837716de1ed4f7266a0a73f21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-629d509339aac6aaa5a0276c0c40c6b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e61795bda67a7a018dcf609069aa2c3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3921432b3d1e2a445e857c03a2917396.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee8a0816c818be8f3d8ad48739197dc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ea5b898bc1b1646a2ede2a83cab71e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f977aab0ec910fe8a2200e0df2a0b50.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64e88b2adba0df149f380f5a976a0583.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FAD-Frequency-Adaptation-and-Diversion-for-Cross-domain-Few-shot-Learning"><a href="#FAD-Frequency-Adaptation-and-Diversion-for-Cross-domain-Few-shot-Learning" class="headerlink" title="FAD: Frequency Adaptation and Diversion for Cross-domain Few-shot   Learning"></a>FAD: Frequency Adaptation and Diversion for Cross-domain Few-shot   Learning</h2><p><strong>Authors:Ruixiao Shi, Fu Feng, Yucheng Xie, Jing Wang, Xin Geng</strong></p>
<p>Cross-domain few-shot learning (CD-FSL) requires models to generalize from limited labeled samples under significant distribution shifts. While recent methods enhance adaptability through lightweight task-specific modules, they operate solely in the spatial domain and overlook frequency-specific variations that are often critical for robust transfer. We observe that spatially similar images across domains can differ substantially in their spectral representations, with low and high frequencies capturing complementary semantic information at coarse and fine levels. This indicates that uniform spatial adaptation may overlook these spectral distinctions, thus constraining generalization. To address this, we introduce Frequency Adaptation and Diversion (FAD), a frequency-aware framework that explicitly models and modulates spectral components. At its core is the Frequency Diversion Adapter, which transforms intermediate features into the frequency domain using the discrete Fourier transform (DFT), partitions them into low, mid, and high-frequency bands via radial masks, and reconstructs each band using inverse DFT (IDFT). Each frequency band is then adapted using a dedicated convolutional branch with a kernel size tailored to its spectral scale, enabling targeted and disentangled adaptation across frequencies. Extensive experiments on the Meta-Dataset benchmark demonstrate that FAD consistently outperforms state-of-the-art methods on both seen and unseen domains, validating the utility of frequency-domain representations and band-wise adaptation for improving generalization in CD-FSL. </p>
<blockquote>
<p>跨域小样本学习（CD-FSL）需要模型在显著分布偏移的情况下从有限的标记样本中进行泛化。虽然最近的方法通过轻量级的任务特定模块增强了适应性，但它们仅在空间域中操作，并忽略了频率特定变化，这些变化对于稳健的迁移通常至关重要。我们观察到，跨域的图像在空间上可能非常相似，但在其光谱表示中可能存在显著差异，低频和高频在粗细级别上捕获互补的语义信息。这表明，统一的适应策略可能会忽略这些光谱差异，从而限制泛化能力。为了解决这个问题，我们引入了频率适应和分流（FAD），这是一个频率感知框架，可以显式地建模和调制光谱成分。其核心是频率分流适配器，它将中间特征转换为频率域使用离散傅里叶变换（DFT），通过径向掩膜将它们划分为低频、中频和高频波段，并使用逆DFT（IDFT）重建每个波段。然后，每个频率波段都使用专门的卷积分支进行适应，卷积核大小根据其光谱尺度量身定制，从而实现有针对性的解耦频率适应。在Meta-Dataset基准测试上的大量实验表明，FAD在可见和不可见域上均优于最新方法，验证了频率域表示和波段适应对于提高CD-FSL中的泛化能力的实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08349v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该研究探讨了跨域小样本学习（CD-FSL）中模型在有限标注样本下的泛化能力问题。针对现有方法忽略频率特定变化，该文提出了频率适应和偏离（FAD）框架，该框架显式地建模和调制光谱成分。通过离散傅里叶变换（DFT）将中间特征转换到频率域，并通过径向掩膜将频率分成低、中、高频带，再进行逆DFT（IDFT）重建每个频带。针对每个频带，使用专门的卷积分支进行适应，其内核大小适应于其光谱尺度，从而实现有针对性的解耦频率适应。在Meta-Dataset基准测试上的实验表明，FAD在可见和不可见域上均优于现有方法，验证了频率域表示和频带适应在提高CD-FSL泛化能力方面的实用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CD-FSL面临模型在有限标注样本下的泛化难题。</li>
<li>现有方法主要关注空间域适应，忽略了频率特定变化的重要性。</li>
<li>文中提出频率适应和偏离（FAD）框架，该框架显式建模和调制光谱成分。</li>
<li>FAD使用DFT转换中间特征到频率域，并分区为不同频率带。</li>
<li>每个频率带通过专门的卷积分支进行适应，实现有针对性的解耦频率适应。</li>
<li>在Meta-Dataset上的实验表明FAD在跨域小样本学习上的优越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08349">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d060eff9cb8c5680247801164fa2e3b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35e69c7c5f8a6822b6618c947b303201.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c6b2982f0326560e15a3b28629c8940.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-be6d2d03b03b385f989ac787f04ba7dd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LLM-Based-Detection-of-Tangled-Code-Changes-for-Higher-Quality-Method-Level-Bug-Datasets"><a href="#LLM-Based-Detection-of-Tangled-Code-Changes-for-Higher-Quality-Method-Level-Bug-Datasets" class="headerlink" title="LLM-Based Detection of Tangled Code Changes for Higher-Quality   Method-Level Bug Datasets"></a>LLM-Based Detection of Tangled Code Changes for Higher-Quality   Method-Level Bug Datasets</h2><p><strong>Authors:Md Nahidul Islam Opu, Shaowei Wang, Shaiful Chowdhury</strong></p>
<p>Tangled code changes-commits that conflate unrelated modifications such as bug fixes, refactorings, and enhancements-introduce significant noise into bug datasets and adversely affect the performance of bug prediction models. Addressing this issue at a fine-grained, method-level granularity remains underexplored. This is critical to address, as recent bug prediction models, driven by practitioner demand, are increasingly focusing on finer granularity rather than traditional class- or file-level predictions. This study investigates the utility of Large Language Models (LLMs) for detecting tangled code changes by leveraging both commit messages and method-level code diffs. We formulate the problem as a binary classification task and evaluate multiple prompting strategies, including zero-shot, few-shot, and chain-of-thought prompting, using state-of-the-art proprietary LLMs such as GPT-4o and Gemini-2.0-Flash.   Our results demonstrate that combining commit messages with code diffs significantly enhances model performance, with the combined few-shot and chain-of-thought prompting achieving an F1-score of 0.88. Additionally, we explore embedding-based machine learning models trained on LLM-generated embeddings, where a multi-layer perceptron classifier achieves superior performance (F1-score: 0.906, MCC: 0.807). These findings are encouraging for the research community, as method-level bug prediction remains an open research problem, largely due to the lack of noise-free bug datasets. This research not only contributes a novel method-level perspective to the untangling problem but also highlights practical avenues for enhancing automated software quality assessment tools. </p>
<blockquote>
<p>混杂的代码更改（将无关修改混杂在一起，如错误修复、重构和增强功能）为错误数据集引入了重大噪声，并对错误预测模型的性能产生了不利影响。在精细粒度的代码级别上解决这一问题尚未得到充分的探索和研究。这是亟需解决的问题，因为由从业者需求的推动，最新的错误预测模型越来越关注更精细的粒度，而非传统的类或文件级别的预测。本研究旨在通过利用提交消息和方法级代码差异来检测混杂的代码更改，从而探究大型语言模型（LLM）的效用。我们将该问题表述为二分类任务，并评估了多种提示策略，包括零样本、小样本和思维链提示策略，使用了最先进的专有大型语言模型如GPT-4o和Gemini-2.0-Flash。结果显示，结合提交消息和代码差异可以显著提高模型性能，其中结合了小样本和思维链提示策略的模型达到了0.88的F1分数。此外，我们还探索了基于嵌入的机器学习任务学习模型，该模型使用大型语言模型生成的嵌入进行训练，其中多层感知器分类器取得了出色的性能（F1分数：0.906，马修斯相关系数：0.807）。这些发现对研究界具有鼓舞人心的意义，因为方法级错误预测仍然是一个开放的研究问题，很大程度上是由于缺乏无噪声的错误数据集。本研究不仅为解决混淆问题提供了一个新颖的方法级视角，而且也为提高自动化软件质量评估工具提供了实际的改进途径。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08263v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本探讨了如何运用大型语言模型（LLMs）检测混乱的代码更改的问题。研究通过结合提交信息和代码差异，利用多种提示策略（包括零样本、小样本和思维链提示）来识别混乱的代码更改。结合使用小样本和思维链提示的方法取得了较高的F1分数。此外，该研究还探索了基于嵌入的机器学习方法，其中多层感知器分类器取得了最佳性能。该研究为方法级别的bug预测提供了新的视角，并强调了增强自动化软件质量评估工具的实际途径。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>混乱的代码更改会对bug数据集引入大量噪声，影响bug预测模型的性能。</li>
<li>方法级别的精细粒度处理是解决此问题的关键，但当前对此的研究仍然不足。</li>
<li>大型语言模型（LLMs）可用于检测混乱的代码更改，通过结合提交信息和代码差异显著提高模型性能。</li>
<li>研究评估了多种提示策略，包括小样本和思维链提示，取得了较高的F1分数。</li>
<li>基于LLM生成的嵌入的嵌入式机器学习方法表现出最佳性能。</li>
<li>该研究为方法级别的bug预测提供了新的视角。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08263">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4d8c8038d27447d024b2c0e470b0651a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-348577eb57373b76bb2f850ccf51ffb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d129821e8c9ac3382ab0f516ad35cf0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09684685178591f79c648e2e21790077.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Exploiting-Text-Semantics-for-Few-and-Zero-Shot-Node-Classification-on-Text-attributed-Graph"><a href="#Exploiting-Text-Semantics-for-Few-and-Zero-Shot-Node-Classification-on-Text-attributed-Graph" class="headerlink" title="Exploiting Text Semantics for Few and Zero Shot Node Classification on   Text-attributed Graph"></a>Exploiting Text Semantics for Few and Zero Shot Node Classification on   Text-attributed Graph</h2><p><strong>Authors:Yuxiang Wang, Xiao Yan, Shiyu Jin, Quanqing Xu, Chuang Hu, Yuanyuan Zhu, Bo Du, Jia Wu, Jiawei Jiang</strong></p>
<p>Text-attributed graph (TAG) provides a text description for each graph node, and few- and zero-shot node classification on TAGs have many applications in fields such as academia and social networks. Existing work utilizes various graph-based augmentation techniques to train the node and text embeddings, while text-based augmentations are largely unexplored. In this paper, we propose Text Semantics Augmentation (TSA) to improve accuracy by introducing more text semantic supervision signals. Specifically, we design two augmentation techniques, i.e., positive semantics matching and negative semantics contrast, to provide more reference texts for each graph node or text description. Positive semantic matching retrieves texts with similar embeddings to match with a graph node. Negative semantic contrast adds a negative prompt to construct a text description with the opposite semantics, which is contrasted with the original node and text. We evaluate TSA on 5 datasets and compare with 13 state-of-the-art baselines. The results show that TSA consistently outperforms all baselines, and its accuracy improvements over the best-performing baseline are usually over 5%. </p>
<blockquote>
<p>文本属性图（TAG）为每个图节点提供了文本描述，而TAG上的小样本和零样本节点分类在学术和社会网络等领域有许多应用。现有工作利用各种基于图的增强技术来训练节点和文本嵌入，而基于文本的增强技术则很少被探索。在本文中，我们提出文本语义增强（TSA），通过引入更多的文本语义监督信号来提高准确性。具体来说，我们设计了两种增强技术，即正向语义匹配和负向语义对比，为每个图节点或文本描述提供更多的参考文本。正向语义匹配检索具有相似嵌入的文本以与图节点匹配。负向语义对比则通过添加负提示来构建一个具有相反语义的文本描述，该描述与原始节点和文本形成对比。我们在5个数据集上评估了TSA，并与1 3个最新基线进行了比较。结果表明，TSA始终优于所有基线，并且相对于表现最佳的基线，其准确率提高通常超过5%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08168v1">PDF</a> </p>
<p><strong>Summary</strong><br>     文本语义增强（TSA）方法通过引入更多的文本语义监督信号，提高了文本节点分类的准确性。具体设计了两种增强技术：正向语义匹配和负向语义对比，为每个图节点或文本描述提供更多的参考文本。在五个数据集上评估TSA，与13个先进基线相比，TSA表现优越，准确率提高通常超过5%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Text-attributed graph (TAG) 为图节点提供文本描述，这在学术和社会网络等领域有着广泛的应用。</li>
<li>尽管现有工作使用了各种基于图的增强技术来训练节点和文本嵌入，但基于文本的增强方法仍被大量忽视。</li>
<li>文本语义增强（TSA）通过引入更多的文本语义监督信号提高了节点分类的准确性。</li>
<li>TSA设计了两种增强技术：正向语义匹配和负向语义对比，为图节点或文本描述提供更多的参考文本。</li>
<li>正向语义匹配通过检索与图节点嵌入相似的文本进行匹配。</li>
<li>负向语义对比通过添加负提示构建与原始节点具有相反语义的文本描述。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08168">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a4083d87399d541a15349c2308b60ce2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2142652ecd6e58f67f7e547e24bc639c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87649acb522996f65a6590eed79259e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-233119bbaf3b88b9093366a44e506a43.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PLHF-Prompt-Optimization-with-Few-Shot-Human-Feedback"><a href="#PLHF-Prompt-Optimization-with-Few-Shot-Human-Feedback" class="headerlink" title="PLHF: Prompt Optimization with Few-Shot Human Feedback"></a>PLHF: Prompt Optimization with Few-Shot Human Feedback</h2><p><strong>Authors:Chun-Pai Yang, Kan Zheng, Shou-De Lin</strong></p>
<p>Automatic prompt optimization frameworks are developed to obtain suitable prompts for large language models (LLMs) with respect to desired output quality metrics. Although existing approaches can handle conventional tasks such as fixed-solution question answering, defining the metric becomes complicated when the output quality cannot be easily assessed by comparisons with standard golden samples. Consequently, optimizing the prompts effectively and efficiently without a clear metric becomes a critical challenge. To address the issue, we present PLHF (which stands for “P”rompt “L”earning with “H”uman “F”eedback), a few-shot prompt optimization framework inspired by the well-known RLHF technique. Different from naive strategies, PLHF employs a specific evaluator module acting as the metric to estimate the output quality. PLHF requires only a single round of human feedback to complete the entire prompt optimization process. Empirical results on both public and industrial datasets show that PLHF outperforms prior output grading strategies for LLM prompt optimizations. </p>
<blockquote>
<p>自动提示优化框架是针对期望的输出质量指标为大语言模型（LLM）获得合适提示而开发的。尽管现有方法能够处理诸如固定解决方案问答之类的常规任务，但是当输出质量不能通过与标准金样样本的比较来轻松评估时，定义度量指标会变得复杂。因此，在没有明确指标的情况下有效和高效地优化提示成为一个关键挑战。为了解决这一问题，我们提出了PLHF（代表“P”rompt“L”earning with “H”uman “F”eedback），这是一个受广受欢迎的RLHF技术启发的少提示优化框架。不同于简单策略，PLHF采用特定评估模块作为度量指标来估计输出质量。PLHF仅需要一轮人类反馈即可完成整个提示优化过程。在公共和工业数据集上的经验结果表明，对于LLM提示优化，PLHF优于先前的输出评分策略。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07886v1">PDF</a> </p>
<p><strong>Summary</strong><br>自动提示优化框架旨在针对大型语言模型（LLM）获得合适的提示，以达到期望的输出质量指标。当输出质量无法通过与标准金样样本的比较进行评估时，定义指标变得复杂。PLHF是一个基于RLHF技术的few-shot提示优化框架，它使用一个特定的评估器模块作为指标来估计输出质量。PLHF只需一轮人类反馈即可完成整个提示优化过程，并在公共和工业数据集上的实证结果都表明其在LLM提示优化方面优于先前的输出评分策略。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自动提示优化框架旨在帮助大型语言模型获得合适的提示以提高输出质量。</li>
<li>在输出质量难以通过标准金样样本评估时，定义合适的评估指标是关键挑战。</li>
<li>PLHF是一个基于RLHF技术的few-shot提示优化框架，能有效解决这一挑战。</li>
<li>PLHF使用一个特定的评估器模块来估计输出质量，作为优化的指标。</li>
<li>PLHF只需一轮人类反馈即可完成整个提示优化过程，提高了效率。</li>
<li>实证结果表明，PLHF在LLM提示优化方面优于先前的输出评分策略。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07886">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-545a84aa853e3822d278c4cfd692769b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-525a528d453f85efdfa48ab66fdb9402.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02b0c1190eed6fa22e3e3abb2c4be894.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ed666cf169fe1a938e3a01be3923692f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LLMSR-XLLM25-Less-is-More-Enhancing-Structured-Multi-Agent-Reasoning-via-Quality-Guided-Distillation"><a href="#LLMSR-XLLM25-Less-is-More-Enhancing-Structured-Multi-Agent-Reasoning-via-Quality-Guided-Distillation" class="headerlink" title="LLMSR@XLLM25: Less is More: Enhancing Structured Multi-Agent Reasoning   via Quality-Guided Distillation"></a>LLMSR@XLLM25: Less is More: Enhancing Structured Multi-Agent Reasoning   via Quality-Guided Distillation</h2><p><strong>Authors:Jiahao Yuan, Xingzhe Sun, Xing Yu, Jingwen Wang, Dehui Du, Zhiqing Cui, Zixiang Di</strong></p>
<p>The LLMSR@XLLM25 formulates a low-resource structural reasoning task that challenges LLMs to generate interpretable, step-by-step rationales with minimal labeled data. We present Less is More, the third-place winning approach in the LLMSR@XLLM25, which focuses on structured reasoning from only 24 labeled examples. Our approach leverages a multi-agent framework with reverse-prompt induction, retrieval-augmented reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to distill high-quality supervision across three subtasks: question parsing, CoT parsing, and step-level verification. All modules are fine-tuned from Meta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure validation with reward filtering across few-shot and zero-shot prompts, our pipeline consistently improves structure reasoning quality. These results underscore the value of controllable data distillation in enhancing structured inference under low-resource constraints. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/JhCircle/Less-is-More">https://github.com/JhCircle/Less-is-More</a>. </p>
<blockquote>
<p>LLMSR@XLLM25提出了一个低资源结构推理任务，该任务挑战大型语言模型在少量标注数据的情况下生成可解释的、逐步的推理过程。我们提出了“少即是多”的方法，这是LLMSR@XLLM25竞赛中的第三名获奖方案，它专注于仅使用24个标注样本进行结构化推理。我们的方法利用多智能体框架进行逆向提示归纳，通过GPT-4o增强推理合成，以及两阶段奖励引导过滤，以提炼高质量监督信息，涵盖三个子任务：问题解析、思维链解析和步骤级验证。所有模块均采用统一的LoRA+设置，在Meta-Llama-3-8B-Instruct上进行微调。通过结合结构验证和奖励过滤在少量样本和零样本提示之间进行过滤，我们的管道不断提高了结构推理质量。这些结果强调了可控数据蒸馏在增强低资源约束下的结构化推理中的价值。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/JhCircle/Less-is-More%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/JhCircle/Less-is-More找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.16408v2">PDF</a> XLLM @ ACL 2025 Shared Task-III: LLM for Structural Reasoning   (LLM-SR)</p>
<p><strong>Summary</strong><br>少量标注数据的低资源结构化推理任务LLMSR@XLLM25提出挑战，Less is More方法凭借仅使用24个标注示例的结构化推理获得第三名。该方法采用多智能体框架结合逆向提示归纳、基于GPT-4o的检索增强推理合成以及双阶段奖励引导过滤技术，进行高质量监督学习，优化任务包括问题解析、CoT解析和步骤级别验证等模块。借助结构化验证与奖励过滤的结合，此方法能在少量甚至无示例情况下改善结构化推理质量，展示了可控数据蒸馏在增强低资源条件下的结构化推断价值。代码已公开于GitHub。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMSR@XLLM25提出了一个低资源结构化推理任务，挑战LLMs在少量标注数据下生成可解释的逐步推理。</li>
<li>Less is More方法在低资源条件下通过结构化推理获得优异表现，仅使用24个标注示例。</li>
<li>该方法采用多智能体框架结合逆向提示归纳技术，实现高质量监督学习。</li>
<li>方法结合了检索增强推理合成技术，通过GPT-4o支持。</li>
<li>方法包含三个子任务：问题解析、CoT解析和步骤级别验证，通过双阶段奖励引导过滤技术进行改进。</li>
<li>通过结合结构验证与奖励过滤技术，方法提高了结构化推理的质量，展现了可控数据蒸馏在低资源条件下的重要性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.16408">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-45fa3721211ca0166e2f8cc95d755a29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b97882cbb7b9fbc0ac0055206c0963c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef1b09856cac2793e13f551851db0c50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f487aeb04334e3c132482e50f60105f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68fde7c534620d519384ca30b68ab1fb.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Are-Transformers-Able-to-Reason-by-Connecting-Separated-Knowledge-in-Training-Data"><a href="#Are-Transformers-Able-to-Reason-by-Connecting-Separated-Knowledge-in-Training-Data" class="headerlink" title="Are Transformers Able to Reason by Connecting Separated Knowledge in   Training Data?"></a>Are Transformers Able to Reason by Connecting Separated Knowledge in   Training Data?</h2><p><strong>Authors:Yutong Yin, Zhaoran Wang</strong></p>
<p>Humans exhibit remarkable compositional reasoning by integrating knowledge from various sources. For example, if someone learns ( B &#x3D; f(A) ) from one source and ( C &#x3D; g(B) ) from another, they can deduce ( C&#x3D;g(B)&#x3D;g(f(A)) ) even without encountering ( ABC ) together, showcasing the generalization ability of human intelligence. In this paper, we introduce a synthetic learning task, “FTCT” (Fragmented at Training, Chained at Testing), to validate the potential of Transformers in replicating this skill and interpret its inner mechanism. In the training phase, data consist of separated knowledge fragments from an overall causal graph. During testing, Transformers must infer complete causal graph traces by integrating these fragments. Our findings demonstrate that few-shot Chain-of-Thought prompting enables Transformers to perform compositional reasoning on FTCT by revealing correct combinations of fragments, even if such combinations were absent in the training data. Furthermore, the emergence of compositional reasoning ability is strongly correlated with the model complexity and training-testing data similarity. We propose, both theoretically and empirically, that Transformers learn an underlying generalizable program from training, enabling effective compositional reasoning during testing. </p>
<blockquote>
<p>人类能够通过整合来自不同来源的知识展现出惊人的组合推理能力。例如，如果有人从某一来源学习到（B&#x3D;f（A））并从另一来源学习到（C&#x3D;g（B）），即使没有同时遇到（ABC），他们也能推断出（C&#x3D;g（B）&#x3D;g（f（A））），展示了人类智力的推广能力。在本文中，我们引入了一项合成学习任务——“FTCT”（训练时分散，测试时链接），以验证Transformer复制这种技能的潜力并解释其内在机制。在训练阶段，数据由来自整体因果图的分离知识片段组成。在测试期间，Transformer必须通过整合这些片段来推断完整的因果图轨迹。我们的研究发现，通过揭示正确的片段组合，即使在训练数据中不存在这样的组合，少数链式思维提示也能使Transformer在FTCT上表现出组合推理能力。此外，组合推理能力的出现与模型复杂度和训练-测试数据相似性之间存在强烈相关性。我们从理论和实践两方面提出，Transformer从训练中学习了一个通用的基础程序，从而在测试期间能够进行有效的组合推理。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.15857v5">PDF</a> Accepted by ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了人类展现出的组合推理能力，通过整合不同来源的知识进行推理。为验证Transformer模型是否具有这种能力并解析其内在机制，提出了一种名为“FTCT”（训练时片段化，测试时连锁）的合成学习任务。在训练阶段，数据由来自整体因果图的知识片段组成。在测试阶段，Transformer必须通过这些片段推断完整的因果图轨迹。研究发现，少数链式思维提示（Chain-of-Thought）能够使Transformer在FTCT上执行组合推理，正确组合知识片段，即使这些组合在训练数据中不存在。此外，模型复杂性和训练测试数据相似性对组合推理能力的出现有重要影响。本文提出，Transformer从训练中学习了一个通用的底层程序，能够在测试时进行有效的组合推理。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人类能整合不同来源的知识进行组合推理。</li>
<li>提出一种名为“FTCT”的合成学习任务来验证Transformer模型的组合推理能力。</li>
<li>在训练阶段，数据由因果图的知识片段组成；测试时，Transformer需整合这些片段来推断完整的因果图轨迹。</li>
<li>少数链式思维提示（Chain-of-Thought）使Transformer能执行组合推理，正确组合知识片段。</li>
<li>模型复杂性和训练测试数据相似性对组合推理能力有显著影响。</li>
<li>Transformer从训练中学习了一个通用的底层程序，能在测试时进行组合推理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.15857">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b4d2eab2371f776c086b57e643d3406c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-331113244c212d41a5e43ffe7ba53214.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="2-5-Years-in-Class-A-Multimodal-Textbook-for-Vision-Language-Pretraining"><a href="#2-5-Years-in-Class-A-Multimodal-Textbook-for-Vision-Language-Pretraining" class="headerlink" title="2.5 Years in Class: A Multimodal Textbook for Vision-Language   Pretraining"></a>2.5 Years in Class: A Multimodal Textbook for Vision-Language   Pretraining</h2><p><strong>Authors:Wenqi Zhang, Hang Zhang, Xin Li, Jiashuo Sun, Yongliang Shen, Weiming Lu, Deli Zhao, Yueting Zhuang, Lidong Bing</strong></p>
<p>Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality \textbf{multimodal textbook} corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving. Our code are available at <a target="_blank" rel="noopener" href="https://github.com/DAMO-NLP-SG/multimodal_textbook">https://github.com/DAMO-NLP-SG/multimodal_textbook</a>. </p>
<blockquote>
<p>与图像文本配对数据相比，交错语料库使视觉语言模型（VLMs）能够像人类一样更自然地理解世界。然而，这些现有的数据集是从网页上爬取的，面临着知识密度低、图像文本关系松散、图像之间逻辑连贯性差等挑战。另一方面，互联网上有大量教学视频（如在线几何课程），人类广泛用来学习基础学科，但这些宝贵资源在VLM训练中仍被忽视。在本文中，我们引入了一种高质量的多模式教科书语料库，具有丰富的基础知识，用于VLM预训练。它收集了超过2.5年的教学视频，总计2.2万节课时。我们首先使用大型语言模型（LLM）提出的分类法来系统地收集教学视频。然后，我们从视频中逐步提取和精炼视觉（关键帧）、音频（ASR）和文本知识（OCR），并按时间顺序组织成图像文本交错语料库。与其他语料库相比，我们的以视频为中心的教学资料提供了更连贯的上下文、更丰富的知识和更好的图像文本对齐。实验证明其预训练性能卓越，特别是在知识密集和推理密集的任务（如ScienceQA和MathVista）中。此外，在我们教科书上进行预训练的VLMs表现出出色的交错上下文意识，利用视觉和文本线索解决少量上下文的任务。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/DAMO-NLP-SG/multimodal_textbook%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/DAMO-NLP-SG/multimodal_textbook中找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00958v4">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>本文介绍了为视觉语言模型（VLM）预训练设计的高质量多模态教科书语料库。该语料库通过收集超过2.5年的教学视频，以图像和文本交错的方式组织，提供丰富的基础知识。实验表明，该语料库在知识密集和推理密集型任务上的预训练性能出色，如ScienceQA和MathVista。此外，在此语料库上训练的VLM具有出色的交错上下文意识，能够在少量情境下利用视觉和文本线索解决问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态教科书语料库结合了教学视频、图像和文本，为视觉语言模型（VLM）预训练提供了丰富资源。</li>
<li>该语料库通过收集超过2.5年的教学视频，包含总计22,000课时，具有更丰富的基础知识和连贯的上下文。</li>
<li>相比其他数据集，该语料库在知识密集和推理密集型任务上的预训练性能优越。</li>
<li>该语料库通过图像和文本的交错组织方式，提高了图像和文本的对齐性。</li>
<li>在此语料库上训练的VLM具有出色的上下文意识，能在少量情境下利用视觉和文本线索解决问题。</li>
<li>该研究提供了一个公开可用的代码库，方便其他人使用和研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00958">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7170d747bbb6f9ca30b1f51660f0ec55.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-13e7421e8ee6711909dc0bcf6b92c2ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9ad43a10ebfc11a2ed1edf5a99a6bc4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7cf76536efe3ec3e379acb807abb9863.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Clickbait-Detection-via-Large-Language-Models"><a href="#Clickbait-Detection-via-Large-Language-Models" class="headerlink" title="Clickbait Detection via Large Language Models"></a>Clickbait Detection via Large Language Models</h2><p><strong>Authors:Han Wang, Yi Zhu, Ye Wang, Yun Li, Yunhao Yuan, Jipeng Qiang</strong></p>
<p>Clickbait, which aims to induce users with some surprising and even thrilling headlines for increasing click-through rates, permeates almost all online content publishers, such as news portals and social media. Recently, Large Language Models (LLMs) have emerged as a powerful instrument and achieved tremendous success in a series of NLP downstream tasks. However, it is not yet known whether LLMs can be served as a high-quality clickbait detection system. In this paper, we analyze the performance of LLMs in the few-shot and zero-shot scenarios on several English and Chinese benchmark datasets. Experimental results show that LLMs cannot achieve the best results compared to the state-of-the-art deep and fine-tuning PLMs methods. Different from human intuition, the experiments demonstrated that LLMs cannot make satisfied clickbait detection just by the headlines. </p>
<blockquote>
<p>标题诱导（Clickbait）通过设计一些令人惊讶甚至兴奋的标题来增加点击率，几乎渗透到了所有的在线内容发布者，如新闻门户和社交媒体。近期，大型语言模型（LLMs）作为一种强大的工具，在一系列NLP下游任务中取得了巨大的成功。然而，尚不清楚LLMs是否可以作为高质量的标题诱导检测系统进行应用。在本文中，我们在少量样本甚至零样本场景下分析了LLMs在多个英文和中文基准数据集上的表现。实验结果表明，与目前先进的深度学习和微调预训练语言模型（PLMs）方法相比，LLMs无法取得最佳结果。不同于人类的直觉，实验证明LLMs仅通过标题无法进行有效的标题诱导检测。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.09597v4">PDF</a> 10 pages, 4 figures</p>
<p><strong>Summary</strong>:<br>大型语言模型（LLMs）用于检测点击诱饵的效果分析。研究发现，在有限的几个场景中，LLMs相较于深度与微调的语言模型方法并未达到最佳效果。实验表明，LLMs仅依靠标题无法有效检测点击诱饵，与人类直觉不同。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>LLMs被尝试用于点击诱饵检测，但实验结果并不理想。</li>
<li>LLMs在少数场景和零场景下的表现不如深度与微调的语言模型方法。</li>
<li>仅依靠标题，LLMs无法有效检测点击诱饵。</li>
<li>实验结果与人类直觉存在差异。</li>
<li>目前尚不清楚LLMs是否能作为高质量的点击诱饵检测系统。</li>
<li>该研究涉及到自然语言处理下游任务中的大型语言模型应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2306.09597">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6f0d346189ac2be773833ff9e75086e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0565ab5cf704a75261fdb49c2ef21c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ba3cad607b4313f432354648d3685d5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9733315ced82e7f249c4164db3048c79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a58ce065fdfe8bde38b8f25c71f8b12.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-15/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-15/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-15/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-55fc01089e004c871fd0fbfc742fc591.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-05-15  PrePrompt Predictive prompting for class incremental learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-15/MMT/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-a3a758e46ffc9f547ea59124167c7ee0.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-05-15  Aya Vision Advancing the Frontier of Multilingual Multimodality
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26633.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
