<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-15  VIViT Variable-Input Vision Transformer Framework for 3D MR Image   Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-986e0875fc30a20aaf1fe57a2d1500ca.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-05-15-æ›´æ–°"><a href="#2025-05-15-æ›´æ–°" class="headerlink" title="2025-05-15 æ›´æ–°"></a>2025-05-15 æ›´æ–°</h1><h2 id="VIViT-Variable-Input-Vision-Transformer-Framework-for-3D-MR-Image-Segmentation"><a href="#VIViT-Variable-Input-Vision-Transformer-Framework-for-3D-MR-Image-Segmentation" class="headerlink" title="VIViT: Variable-Input Vision Transformer Framework for 3D MR Image   Segmentation"></a>VIViT: Variable-Input Vision Transformer Framework for 3D MR Image   Segmentation</h2><p><strong>Authors:Badhan Kumar Das, Ajay Singh, Gengyan Zhao, Han Liu, Thomas J. Re, Dorin Comaniciu, Eli Gibson, Andreas Maier</strong></p>
<p>Self-supervised pretrain techniques have been widely used to improve the downstream tasksâ€™ performance. However, real-world magnetic resonance (MR) studies usually consist of different sets of contrasts due to different acquisition protocols, which poses challenges for the current deep learning methods on large-scale pretrain and different downstream tasks with different input requirements, since these methods typically require a fixed set of input modalities or, contrasts. To address this challenge, we propose variable-input ViT (VIViT), a transformer-based framework designed for self-supervised pretraining and segmentation finetuning for variable contrasts in each study. With this ability, our approach can maximize the data availability in pretrain, and can transfer the learned knowledge from pretrain to downstream tasks despite variations in input requirements. We validate our method on brain infarct and brain tumor segmentation, where our method outperforms current CNN and ViT-based models with a mean Dice score of 0.624 and 0.883 respectively. These results highlight the efficacy of our design for better adaptability and performance on tasks with real-world heterogeneous MR data. </p>
<blockquote>
<p>è‡ªç›‘ç£é¢„è®­ç»ƒæŠ€æœ¯åœ¨æé«˜ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æ–¹é¢å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œç”±äºä¸åŒçš„é‡‡é›†åè®®ï¼Œç°å®ä¸–ç•Œä¸­çš„ç£å…±æŒ¯ï¼ˆMRï¼‰ç ”ç©¶é€šå¸¸ç”±ä¸åŒçš„å¯¹æ¯”ç»„åˆæ„æˆï¼Œè¿™ç»™å½“å‰æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨å¤§å‹é¢„è®­ç»ƒå’Œä¸åŒä¸‹æ¸¸ä»»åŠ¡ä¸Šå¸¦æ¥äº†æŒ‘æˆ˜ï¼Œè¿™äº›ä¸‹æ¸¸ä»»åŠ¡æœ‰ä¸åŒçš„è¾“å…¥è¦æ±‚ï¼Œå› ä¸ºè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦ä¸€ç»„å›ºå®šçš„è¾“å…¥æ¨¡å¼æˆ–å¯¹æ¯”ç»„åˆã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¯å˜è¾“å…¥ViTï¼ˆVIViTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºtransformerçš„æ¡†æ¶ï¼Œç”¨äºé’ˆå¯¹æ¯é¡¹ç ”ç©¶ä¸­çš„å¯å˜å¯¹æ¯”è¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒå’Œåˆ†å‰²å¾®è°ƒã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨é¢„è®­ç»ƒæœŸé—´æœ€å¤§åŒ–æ•°æ®å¯ç”¨æ€§ï¼Œå¹¶ä¸”å³ä½¿è¾“å…¥è¦æ±‚å‘ç”Ÿå˜åŒ–ï¼Œä¹Ÿèƒ½å°†ä»é¢„è®­ç»ƒä¸­å­¦ä¹ çš„çŸ¥è¯†è½¬ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬åœ¨è„‘æ¢—æ­»å’Œè„‘è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹³å‡Diceå¾—åˆ†ä¸Šè¶…è¿‡äº†å½“å‰çš„CNNå’ŒViTæ¨¡å‹ï¼Œåˆ†åˆ«è¾¾åˆ°äº†0.624å’Œ0.883ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†æˆ‘ä»¬çš„è®¾è®¡åœ¨é€‚åº”å’Œè§£å†³å…·æœ‰ç°å®ä¸–ç•Œä¸­å¼‚è´¨MRæ•°æ®ä»»åŠ¡æ—¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08693v1">PDF</a> 9 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå˜å‹å™¨çš„å¯å˜è¾“å…¥è‡ªç›‘ç£é¢„è®­ç»ƒä¸åˆ†å‰²å¾®è°ƒæ¡†æ¶ï¼ˆVIViTï¼‰ï¼Œæ—¨åœ¨è§£å†³ç£å…±æŒ¯æˆåƒç ”ç©¶ä¸­ä¸åŒå¯¹æ¯”åº¦æ•°æ®å¸¦æ¥çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å¯é€‚åº”ä¸åŒè¾“å…¥è¦æ±‚ï¼Œæé«˜æ•°æ®åˆ©ç”¨ç‡ï¼Œå¹¶å°†é¢„è®­ç»ƒä¸­çš„çŸ¥è¯†è½¬ç§»åˆ°ä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨è„‘æ¢—æ­»å’Œè„‘è‚¿ç˜¤åˆ†å‰²éªŒè¯ä¸­ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰CNNå’ŒViTæ¨¡å‹ï¼Œæ˜¾ç¤ºå‡ºæ›´å¥½çš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VIViTæ¡†æ¶æ˜¯åŸºäºå˜å‹å™¨çš„å¯å˜è¾“å…¥è‡ªç›‘ç£é¢„è®­ç»ƒè®¾è®¡ï¼Œé€‚ç”¨äºç£å…±æŒ¯æˆåƒç ”ç©¶ä¸­çš„ä¸åŒå¯¹æ¯”åº¦æ•°æ®ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿæœ€å¤§åŒ–é¢„è®­ç»ƒä¸­çš„æ•°æ®å¯ç”¨æ€§ï¼Œå¹¶é€‚åº”ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡è¾“å…¥è¦æ±‚ã€‚</li>
<li>VIViTæ¡†æ¶é€šè¿‡è‡ªç›‘ç£é¢„è®­ç»ƒä¸åˆ†å‰²å¾®è°ƒï¼Œæé«˜äº†æ¨¡å‹å¯¹çœŸå®ä¸–ç•Œå¼‚è´¨ç£å…±æŒ¯æ•°æ®çš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚</li>
<li>åœ¨è„‘æ¢—æ­»å’Œè„‘è‚¿ç˜¤åˆ†å‰²éªŒè¯ä¸­ï¼ŒVIViTæ–¹æ³•è¡¨ç°å‡ºä¼˜äºç°æœ‰CNNå’ŒViTæ¨¡å‹çš„æ€§èƒ½ï¼Œå¹³å‡Diceå¾—åˆ†åˆ†åˆ«ä¸º0.624å’Œ0.883ã€‚</li>
<li>VIViTæ¡†æ¶è§£å†³äº†å½“å‰æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡é¢„è®­ç»ƒå’Œä¸åŒä¸‹æ¸¸ä»»åŠ¡æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œè¿™äº›ä»»åŠ¡å…·æœ‰ä¸åŒçš„è¾“å…¥è¦æ±‚ã€‚</li>
<li>VIViTæ¡†æ¶çš„è®¾è®¡æé«˜äº†æ¨¡å‹çš„æ•ˆèƒ½ï¼Œä½¿å…¶æ›´é€‚ç”¨äºå¤„ç†çœŸå®ä¸–ç•Œçš„å¼‚è´¨æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08693">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-285ee3b86364731def87bdbd42d1fe20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f1dcc2a1b03fa1bc20da97cb58d5f69.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-Machine-Learning-Pipeline-for-Molecular-Property-Prediction-using-ChemXploreML"><a href="#A-Machine-Learning-Pipeline-for-Molecular-Property-Prediction-using-ChemXploreML" class="headerlink" title="A Machine Learning Pipeline for Molecular Property Prediction using   ChemXploreML"></a>A Machine Learning Pipeline for Molecular Property Prediction using   ChemXploreML</h2><p><strong>Authors:Aravindh Nivas Marimuthu, Brett A. McGuire</strong></p>
<p>We present ChemXploreML, a modular desktop application designed for machine learning-based molecular property prediction. The frameworkâ€™s flexible architecture allows integration of any molecular embedding technique with modern machine learning algorithms, enabling researchers to customize their prediction pipelines without extensive programming expertise. To demonstrate the frameworkâ€™s capabilities, we implement and evaluate two molecular embedding approaches - Mol2Vec and VICGAE (Variance-Invariance-Covariance regularized GRU Auto-Encoder) - combined with state-of-the-art tree-based ensemble methods (Gradient Boosting Regression, XGBoost, CatBoost, and LightGBM). Using five fundamental molecular properties as test cases - melting point (MP), boiling point (BP), vapor pressure (VP), critical temperature (CT), and critical pressure (CP) - we validate our framework on a dataset from the CRC Handbook of Chemistry and Physics. The models achieve excellent performance for well-distributed properties, with R$^2$ values up to 0.93 for critical temperature predictions. Notably, while Mol2Vec embeddings (300 dimensions) delivered slightly higher accuracy, VICGAE embeddings (32 dimensions) exhibited comparable performance yet offered significantly improved computational efficiency. ChemXploreMLâ€™s modular design facilitates easy integration of new embedding techniques and machine learning algorithms, providing a flexible platform for customized property prediction tasks. The application automates chemical data preprocessing (including UMAP-based exploration of molecular space), model optimization, and performance analysis through an intuitive interface, making sophisticated machine learning techniques accessible while maintaining extensibility for advanced cheminformatics users. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºChemXploreMLï¼Œè¿™æ˜¯ä¸€æ¬¾åŸºäºæœºå™¨å­¦ä¹ çš„æ¨¡å—åŒ–æ¡Œé¢åº”ç”¨ç¨‹åºï¼Œç”¨äºåˆ†å­å±æ€§é¢„æµ‹ã€‚è¯¥æ¡†æ¶çµæ´»çš„æ¶æ„å…è®¸å°†ä»»ä½•åˆ†å­åµŒå…¥æŠ€æœ¯ä¸ç°ä»£æœºå™¨å­¦ä¹ ç®—æ³•ç›¸ç»“åˆï¼Œä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿæ ¹æ®è‡ªå·±çš„é¢„æµ‹ç®¡é“å®šåˆ¶æ— éœ€æ·±åšçš„ç¼–ç¨‹æŠ€èƒ½ã€‚ä¸ºäº†å±•ç¤ºæ¡†æ¶çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬å®ç°äº†ä¸¤ç§åˆ†å­åµŒå…¥æ–¹æ³•å¹¶è¿›è¡Œè¯„ä¼°ï¼Œåˆ†åˆ«æ˜¯Mol2Vecå’ŒVICGAEï¼ˆæ–¹å·®-åæ–¹å·®æ­£åˆ™åŒ–GRUè‡ªç¼–ç å™¨ï¼‰ã€‚æˆ‘ä»¬å°†è¿™äº›æ–¹æ³•ä¸ç°ä»£æ ‘å½¢é›†æˆæ–¹æ³•ï¼ˆæ¢¯åº¦å¢å¼ºå›å½’ã€XGBoostã€CatBoostå’ŒLightGBMï¼‰ç›¸ç»“åˆã€‚ä»¥äº”ç§åŸºæœ¬åˆ†å­å±æ€§ä½œä¸ºæµ‹è¯•æ¡ˆä¾‹ï¼šç†”ç‚¹ï¼ˆMPï¼‰ã€æ²¸ç‚¹ï¼ˆBPï¼‰ã€è’¸æ±½å‹ï¼ˆVPï¼‰ã€ä¸´ç•Œæ¸©åº¦ï¼ˆCTï¼‰å’Œä¸´ç•Œå‹åŠ›ï¼ˆCPï¼‰ã€‚æˆ‘ä»¬åœ¨CRCåŒ–å­¦ç‰©ç†æ‰‹å†Œçš„æ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶ã€‚å¯¹äºåˆ†å¸ƒè‰¯å¥½çš„å±æ€§ï¼Œæ¨¡å‹å–å¾—äº†å‡ºè‰²çš„æ€§èƒ½è¡¨ç°ï¼Œä¸´ç•Œæ¸©åº¦é¢„æµ‹çš„RÂ²å€¼é«˜è¾¾0.93ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶Mol2VecåµŒå…¥ï¼ˆ300ç»´ï¼‰ç•¥å¾®æé«˜äº†å‡†ç¡®æ€§ï¼Œä½†VICGAEåµŒå…¥ï¼ˆ32ç»´ï¼‰è¡¨ç°å‡ºç›¸å½“çš„æ€§èƒ½å¹¶å¤§å¤§æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚ChemXploreMLçš„æ¨¡å—åŒ–è®¾è®¡ä¿ƒè¿›äº†æ–°åµŒå…¥æŠ€æœ¯å’Œæœºå™¨å­¦ä¹ ç®—æ³•çš„è½»æ¾é›†æˆï¼Œä¸ºå®šåˆ¶å±æ€§é¢„æµ‹ä»»åŠ¡æä¾›äº†çµæ´»çš„å¹³å°ã€‚è¯¥åº”ç”¨ç¨‹åºé€šè¿‡ç›´è§‚ç•Œé¢è‡ªåŠ¨æ‰§è¡ŒåŒ–å­¦æ•°æ®é¢„å¤„ç†ï¼ˆåŒ…æ‹¬åŸºäºUMAPçš„åˆ†å­ç©ºé—´æ¢ç´¢ï¼‰ã€æ¨¡å‹ä¼˜åŒ–å’Œæ€§èƒ½åˆ†æï¼Œä½¿å¾—å¤æ‚çš„æœºå™¨å­¦ä¹ æŠ€æœ¯å˜å¾—æ˜“äºè®¿é—®ï¼ŒåŒæ—¶ä¿æŒäº†é«˜çº§åŒ–å­¦ä¿¡æ¯å­¦ç”¨æˆ·çš„å¯æ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08688v1">PDF</a> 17 pages, 7 figures, accepted in Journal of Chemical Information and   Modeling</p>
<p><strong>Summary</strong></p>
<p>ChemXploreMLæ˜¯ä¸€ä¸ªæ¨¡å—åŒ–æ¡Œé¢åº”ç”¨ç¨‹åºï¼Œç”¨äºåŸºäºæœºå™¨å­¦ä¹ çš„åˆ†å­å±æ€§é¢„æµ‹ã€‚è¯¥ç¨‹åºæ¶æ„çµæ´»ï¼Œå¯é›†æˆä»»ä½•åˆ†å­åµŒå…¥æŠ€æœ¯å’Œç°ä»£æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæ— éœ€æ·±åšç¼–ç¨‹ç»éªŒå³å¯å®šåˆ¶é¢„æµ‹æµç¨‹ã€‚é€šè¿‡å¯¹Mol2Vecå’ŒVICGAEä¸¤ç§åˆ†å­åµŒå…¥æ–¹æ³•ä¸æœ€æ–°çš„æ ‘é›†æˆæ–¹æ³•è¿›è¡Œç»“åˆåº”ç”¨ï¼Œå¯¹ç†”ç‚¹ã€æ²¸ç‚¹ã€è’¸æ±½å‹ã€ä¸´ç•Œæ¸©åº¦å’Œä¸´ç•Œå‹åŠ›ç­‰äº”ç§åŸºæœ¬åˆ†å­å±æ€§è¿›è¡Œé¢„æµ‹éªŒè¯ï¼Œå–å¾—äº†è‰¯å¥½æ•ˆæœã€‚å…¶ä¸­ï¼ŒMol2VecåµŒå…¥è™½ç²¾åº¦ç¨é«˜ï¼Œä½†VICGAEåµŒå…¥åœ¨è®¡ç®—æ•ˆç‡ä¸Šè¡¨ç°æ›´ä¼˜ç§€ã€‚ChemXploreMLçš„æ¨¡å—åŒ–è®¾è®¡ä¾¿äºé›†æˆæ–°çš„åµŒå…¥æŠ€æœ¯å’Œæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œä¸ºå®šåˆ¶å±æ€§é¢„æµ‹ä»»åŠ¡æä¾›äº†çµæ´»å¹³å°ï¼ŒåŒæ—¶è‡ªåŠ¨åŒ–åŒ–å­¦æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹ä¼˜åŒ–å’Œæ€§èƒ½åˆ†æï¼Œé€šè¿‡ç›´è§‚ç•Œé¢ä½¿é«˜çº§åŒ–å­¦ä¿¡æ¯å­¦ç”¨æˆ·æ›´æ˜“è®¿é—®å’Œä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ChemXploreMLæ˜¯ä¸€ä¸ªæ¨¡å—åŒ–æ¡Œé¢åº”ç”¨ç¨‹åºï¼Œç”¨äºåŸºäºæœºå™¨å­¦ä¹ çš„åˆ†å­å±æ€§é¢„æµ‹ã€‚</li>
<li>æ¡†æ¶å…è®¸çµæ´»é›†æˆä¸åŒçš„åˆ†å­åµŒå…¥æŠ€æœ¯å’Œæœºå™¨å­¦ä¹ ç®—æ³•ã€‚</li>
<li>å±•ç¤ºäº†ä½¿ç”¨Mol2Vecå’ŒVICGAEåµŒå…¥ä¸æ ‘é›†æˆæ–¹æ³•ç»“åˆé¢„æµ‹åˆ†å­å±æ€§çš„èƒ½åŠ›ã€‚</li>
<li>æ¡†æ¶åœ¨CRCåŒ–å­¦ç‰©ç†æ‰‹å†Œæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œå¯¹åˆ†å¸ƒè‰¯å¥½çš„å±æ€§è¡¨ç°ä¼˜å¼‚ï¼ŒRÂ²å€¼é«˜è¾¾0.93ã€‚</li>
<li>Mol2VecåµŒå…¥ç²¾åº¦é«˜ä½†ç»´åº¦è¾ƒå¤§ï¼Œè€ŒVICGAEåµŒå…¥åœ¨è®¡ç®—æ•ˆç‡ä¸Šæœ‰ä¼˜åŠ¿ã€‚</li>
<li>ChemXploreMLè‡ªåŠ¨åŒ–æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹ä¼˜åŒ–å’Œæ€§èƒ½åˆ†æï¼Œå¹¶é€šè¿‡ç›´è§‚ç•Œé¢æä¾›ç”¨æˆ·å‹å¥½ä½“éªŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08688">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-051bfed32c13e10c5493e400546958c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-82e9ae975c4a789a86006a5cb3ef93ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b09f403419d7ba3ee518a063359949a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0a104cd83e6a357d03587ba554c7461.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Calibration-and-Uncertainty-for-multiRater-Volume-Assessment-in-multiorgan-Segmentation-CURVAS-challenge-results"><a href="#Calibration-and-Uncertainty-for-multiRater-Volume-Assessment-in-multiorgan-Segmentation-CURVAS-challenge-results" class="headerlink" title="Calibration and Uncertainty for multiRater Volume Assessment in   multiorgan Segmentation (CURVAS) challenge results"></a>Calibration and Uncertainty for multiRater Volume Assessment in   multiorgan Segmentation (CURVAS) challenge results</h2><p><strong>Authors:Meritxell Riera-Marin, Sikha O K, Julia Rodriguez-Comas, Matthias Stefan May, Zhaohong Pan, Xiang Zhou, Xiaokun Liang, Franciskus Xaverius Erick, Andrea Prenner, Cedric Hemon, Valentin Boussot, Jean-Louis Dillenseger, Jean-Claude Nunes, Abdul Qayyum, Moona Mazher, Steven A Niederer, Kaisar Kushibar, Carlos Martin-Isla, Petia Radeva, Karim Lekadir, Theodore Barfoot, Luis C. Garcia Peraza Herrera, Ben Glocker, Tom Vercauteren, Lucas Gago, Justin Englemann, Joy-Marie Kleiss, Anton Aubanell, Andreu Antolin, Javier Garcia-Lopez, Miguel A. Gonzalez Ballester, Adrian Galdran</strong></p>
<p>Deep learning (DL) has become the dominant approach for medical image segmentation, yet ensuring the reliability and clinical applicability of these models requires addressing key challenges such as annotation variability, calibration, and uncertainty estimation. This is why we created the Calibration and Uncertainty for multiRater Volume Assessment in multiorgan Segmentation (CURVAS), which highlights the critical role of multiple annotators in establishing a more comprehensive ground truth, emphasizing that segmentation is inherently subjective and that leveraging inter-annotator variability is essential for robust model evaluation. Seven teams participated in the challenge, submitting a variety of DL models evaluated using metrics such as Dice Similarity Coefficient (DSC), Expected Calibration Error (ECE), and Continuous Ranked Probability Score (CRPS). By incorporating consensus and dissensus ground truth, we assess how DL models handle uncertainty and whether their confidence estimates align with true segmentation performance. Our findings reinforce the importance of well-calibrated models, as better calibration is strongly correlated with the quality of the results. Furthermore, we demonstrate that segmentation models trained on diverse datasets and enriched with pre-trained knowledge exhibit greater robustness, particularly in cases deviating from standard anatomical structures. Notably, the best-performing models achieved high DSC and well-calibrated uncertainty estimates. This work underscores the need for multi-annotator ground truth, thorough calibration assessments, and uncertainty-aware evaluations to develop trustworthy and clinically reliable DL-based medical image segmentation models. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰å·²æˆä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²çš„ä¸»æµæ–¹æ³•ï¼Œä½†è¦ç¡®ä¿è¿™äº›æ¨¡å‹å¯é æ€§å’Œä¸´åºŠé€‚ç”¨æ€§ï¼Œéœ€è¦è§£å†³æ ‡æ³¨å˜å¼‚æ€§ã€æ ¡å‡†å’Œä¸ç¡®å®šæ€§ä¼°è®¡ç­‰å…³é”®æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†å¤šè¯„ä¼°è€…ä½“ç§¯è¯„ä¼°ä¸­çš„æ ¡å‡†å’Œä¸ç¡®å®šæ€§ï¼ˆCURVASï¼‰ï¼Œå®ƒå¼ºè°ƒäº†å¤šä¸ªæ ‡æ³¨è€…åœ¨å»ºç«‹æ›´å…¨é¢çœŸå®æƒ…å†µä¸­çš„å…³é”®ä½œç”¨ï¼Œå¹¶å¼ºè°ƒåˆ†å‰²æœ¬è´¨ä¸Šæ˜¯ä¸»è§‚çš„ï¼Œåˆ©ç”¨æ ‡æ³¨è€…ä¹‹é—´çš„å˜å¼‚æ€§å¯¹äºç¨³å¥çš„æ¨¡å‹è¯„ä¼°è‡³å…³é‡è¦ã€‚ä¸ƒä¸ªå›¢é˜Ÿå‚ä¸äº†æ­¤æ¬¡æŒ‘æˆ˜ï¼Œæäº¤äº†å¤šç§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé‡‡ç”¨ç‹„å…‹ç›¸ä¼¼ç³»æ•°ï¼ˆDSCï¼‰ã€æœŸæœ›æ ¡å‡†è¯¯å·®ï¼ˆECEï¼‰å’Œè¿ç»­æ’åæ¦‚ç‡åˆ†æ•°ï¼ˆCRPSï¼‰ç­‰æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚é€šè¿‡çº³å…¥å…±è¯†å’Œéå…±è¯†çœŸå®æƒ…å†µï¼Œæˆ‘ä»¬è¯„ä¼°æ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚ä½•å¤„ç†ä¸ç¡®å®šæ€§ï¼Œä»¥åŠå®ƒä»¬çš„ç½®ä¿¡åº¦ä¼°è®¡æ˜¯å¦ä¸çœŸæ­£çš„åˆ†å‰²æ€§èƒ½ç›¸ç¬¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå†æ¬¡å¼ºè°ƒäº†è‰¯å¥½æ ¡å‡†æ¨¡å‹çš„é‡è¦æ€§ï¼Œå› ä¸ºæ›´å¥½çš„æ ¡å‡†ä¸ç»“æœè´¨é‡å¯†åˆ‡ç›¸å…³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜åœ¨å¤šæ ·åŒ–æ•°æ®é›†ä¸Šè®­ç»ƒå¹¶ä¸°å¯Œé¢„è®­ç»ƒçŸ¥è¯†çš„åˆ†å‰²æ¨¡å‹è¡¨ç°å‡ºæ›´å¤§çš„ç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨åç¦»æ ‡å‡†è§£å‰–ç»“æ„çš„æƒ…å†µä¸‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¡¨ç°æœ€ä½³çš„æ¨¡å‹å®ç°äº†é«˜DSCå’Œè‰¯å¥½çš„æ ¡å‡†ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†å¤šè¯„ä¼°è€…çœŸå®æƒ…å†µã€å…¨é¢çš„æ ¡å‡†è¯„ä¼°å’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥è¯„ä¼°çš„éœ€æ±‚ï¼Œä»¥å¼€å‘å¯ä¿¡å’Œä¸´åºŠå¯é çš„åŸºäºæ·±åº¦å­¦ä¹ çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08685v1">PDF</a> This challenge was hosted in MICCAI 2024</p>
<p><strong>Summary</strong><br>     æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å æ®ä¸»å¯¼åœ°ä½ï¼Œä½†ç¡®ä¿æ¨¡å‹çš„å¯é æ€§å’Œä¸´åºŠé€‚ç”¨æ€§éœ€è¦è§£å†³æ ‡æ³¨å˜å¼‚æ€§ã€æ ¡å‡†å’Œä¸ç¡®å®šæ€§ä¼°è®¡ç­‰å…³é”®æŒ‘æˆ˜ã€‚CURVASç ”ç©¶å¼ºè°ƒå¤šæ ‡æ³¨è€…åœ¨å»ºç«‹æ›´å…¨é¢çœŸå®æ ‡ç­¾ä¸­çš„å…³é”®ä½œç”¨ï¼ŒæŒ‡å‡ºåˆ†å‰²æœ¬è´¨ä¸Šæ˜¯ä¸»è§‚çš„ï¼Œåˆ©ç”¨æ ‡æ³¨è€…é—´å˜å¼‚æ€§å¯¹äºç¨³å¥æ¨¡å‹è¯„ä¼°è‡³å…³é‡è¦ã€‚ç ”ç©¶é€šè¿‡èå…¥å…±è¯†å’Œåˆ†æ­§çœŸå®æ ‡ç­¾ï¼Œè¯„ä¼°æ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚ä½•å¤„ç†ä¸ç¡®å®šæ€§å’Œå…¶ç½®ä¿¡åº¦ä¼°è®¡æ˜¯å¦ä¸çœŸå®åˆ†å‰²æ€§èƒ½ç›¸ç¬¦ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè‰¯å¥½æ ¡å‡†çš„æ¨¡å‹éå¸¸é‡è¦ï¼Œä¸”åœ¨å¤šæ ·åŒ–æ•°æ®é›†ä¸Šè®­ç»ƒçš„ã€ç”¨é¢„è®­ç»ƒçŸ¥è¯†ä¸°å¯Œçš„åˆ†å‰²æ¨¡å‹æ›´ç¨³å¥ã€‚æœ€ä½³æ¨¡å‹å®ç°é«˜DSCå’Œè‰¯å¥½æ ¡å‡†çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚å¼ºè°ƒéœ€è¦å¤šæ ‡æ³¨è€…çœŸå®æ ‡ç­¾ã€å…¨é¢çš„æ ¡å‡†è¯„ä¼°å’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥è¯„ä¼°ï¼Œä»¥å¼€å‘å¯ä¿¡ä¸”ä¸´åºŠå¯é çš„åŸºäºæ·±åº¦å­¦ä¹ çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ˜¯åŒ»å­¦å›¾åƒåˆ†å‰²çš„ä¸»å¯¼æ–¹æ³•ï¼Œä½†ç¡®ä¿æ¨¡å‹çš„å¯é æ€§å’Œä¸´åºŠé€‚ç”¨æ€§é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>CURVASç ”ç©¶å¼ºè°ƒå¤šæ ‡æ³¨è€…åœ¨å»ºç«‹æ›´å…¨é¢çœŸå®æ ‡ç­¾ä¸­çš„å…³é”®ä½œç”¨ã€‚</li>
<li>åˆ†å‰²æœ¬è´¨ä¸Šæ˜¯ä¸»è§‚çš„ï¼Œåˆ©ç”¨æ ‡æ³¨è€…é—´å˜å¼‚æ€§å¯¹ç¨³å¥æ¨¡å‹è¯„ä¼°è‡³å…³é‡è¦ã€‚</li>
<li>ç ”ç©¶é€šè¿‡èå…¥å…±è¯†å’Œåˆ†æ­§çœŸå®æ ‡ç­¾ï¼Œè¯„ä¼°æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ä¸ç¡®å®šæ€§å¤„ç†ã€‚</li>
<li>è‰¯å¥½æ ¡å‡†çš„æ¨¡å‹éå¸¸é‡è¦ï¼Œä¸”ä¸åˆ†å‰²è´¨é‡å¼ºç›¸å…³ã€‚</li>
<li>åœ¨å¤šæ ·åŒ–æ•°æ®é›†ä¸Šè®­ç»ƒçš„ã€ç”¨é¢„è®­ç»ƒçŸ¥è¯†ä¸°å¯Œçš„åˆ†å‰²æ¨¡å‹æ›´ç¨³å¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08685">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f3ba1318e3907202706508a5c3cdd7f1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Out-of-Distribution-Detection-in-Medical-Imaging-Using-Multi-Exit-Class-Activation-Maps-and-Feature-Masking"><a href="#Unsupervised-Out-of-Distribution-Detection-in-Medical-Imaging-Using-Multi-Exit-Class-Activation-Maps-and-Feature-Masking" class="headerlink" title="Unsupervised Out-of-Distribution Detection in Medical Imaging Using   Multi-Exit Class Activation Maps and Feature Masking"></a>Unsupervised Out-of-Distribution Detection in Medical Imaging Using   Multi-Exit Class Activation Maps and Feature Masking</h2><p><strong>Authors:Yu-Jen Chen, Xueyang Li, Yiyu Shi, Tsung-Yi Ho</strong></p>
<p>Out-of-distribution (OOD) detection is essential for ensuring the reliability of deep learning models in medical imaging applications. This work is motivated by the observation that class activation maps (CAMs) for in-distribution (ID) data typically emphasize regions that are highly relevant to the modelâ€™s predictions, whereas OOD data often lacks such focused activations. By masking input images with inverted CAMs, the feature representations of ID data undergo more substantial changes compared to those of OOD data, offering a robust criterion for differentiation. In this paper, we introduce a novel unsupervised OOD detection framework, Multi-Exit Class Activation Map (MECAM), which leverages multi-exit CAMs and feature masking. By utilizing mult-exit networks that combine CAMs from varying resolutions and depths, our method captures both global and local feature representations, thereby enhancing the robustness of OOD detection. We evaluate MECAM on multiple ID datasets, including ISIC19 and PathMNIST, and test its performance against three medical OOD datasets, RSNA Pneumonia, COVID-19, and HeadCT, and one natural image OOD dataset, iSUN. Comprehensive comparisons with state-of-the-art OOD detection methods validate the effectiveness of our approach. Our findings emphasize the potential of multi-exit networks and feature masking for advancing unsupervised OOD detection in medical imaging, paving the way for more reliable and interpretable models in clinical practice. </p>
<blockquote>
<p>åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ£€æµ‹å¯¹äºç¡®ä¿åŒ»ç–—æˆåƒåº”ç”¨ä¸­æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¯é æ€§è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶å·¥ä½œå—åˆ°ä»¥ä¸‹è§‚å¯Ÿç»“æœçš„å¯å‘ï¼šå¯¹äºå†…éƒ¨åˆ†å¸ƒï¼ˆIDï¼‰æ•°æ®çš„ç±»åˆ«æ¿€æ´»å›¾ï¼ˆCAMsï¼‰é€šå¸¸å¼ºè°ƒä¸æ¨¡å‹é¢„æµ‹é«˜åº¦ç›¸å…³çš„åŒºåŸŸï¼Œè€ŒOODæ•°æ®é€šå¸¸ç¼ºä¹è¿™ç§æœ‰é’ˆå¯¹æ€§çš„æ¿€æ´»ã€‚é€šè¿‡ç”¨åå‘CAMsé®æŒ¡è¾“å…¥å›¾åƒï¼ŒIDæ•°æ®çš„ç‰¹å¾è¡¨ç¤ºç»å†äº†æ¯”OODæ•°æ®æ›´å¤§çš„å˜åŒ–ï¼Œä¸ºå·®å¼‚åŒ–æä¾›äº†ç¨³å¥çš„æ ‡å‡†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ— ç›‘ç£OODæ£€æµ‹æ¡†æ¶â€”â€”å¤šå‡ºå£ç±»åˆ«æ¿€æ´»å›¾ï¼ˆMECAMï¼‰ï¼Œå®ƒåˆ©ç”¨å¤šå‡ºå£CAMså’Œç‰¹å¾é®è”½æŠ€æœ¯ã€‚é€šè¿‡åˆ©ç”¨ç»“åˆä¸åŒåˆ†è¾¨ç‡å’Œæ·±åº¦CAMsçš„å¤šå‡ºå£ç½‘ç»œï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ•æ‰å…¨å±€å’Œå±€éƒ¨ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œæé«˜äº†OODæ£€æµ‹çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªIDæ•°æ®é›†ä¸Šè¯„ä¼°äº†MECAMï¼ŒåŒ…æ‹¬ISIC19å’ŒPathMNISTï¼Œå¹¶åœ¨ä¸‰ä¸ªåŒ»å­¦OODæ•°æ®é›†ï¼ˆRSNAè‚ºç‚ã€COVID-19å’ŒHeadCTï¼‰å’Œä¸€ä¸ªè‡ªç„¶å›¾åƒOODæ•°æ®é›†ï¼ˆiSUNï¼‰ä¸Šæµ‹è¯•äº†å…¶æ€§èƒ½ã€‚ä¸æœ€å…ˆè¿›çš„OODæ£€æµ‹æ–¹æ³•çš„å…¨é¢æ¯”è¾ƒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜äº†å¤šå‡ºå£ç½‘ç»œå’Œç‰¹å¾é®è”½åœ¨æ¨è¿›åŒ»å­¦æˆåƒä¸­çš„æ— ç›‘ç£OODæ£€æµ‹æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºä¸´åºŠå®è·µä¸­çš„æ›´å¯é å’Œå¯è§£é‡Šçš„æ¨¡å‹é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08604v1">PDF</a> 10 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šå‡ºå£ç±»æ¿€æ´»æ˜ å°„ï¼ˆMECAMï¼‰çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹æ¡†æ¶ï¼Œç”¨äºåŒ»å­¦å›¾åƒä¸­çš„ç¦»ç¾¤å€¼æ£€æµ‹ã€‚é€šè¿‡åˆ©ç”¨å¤šå‡ºå£ç½‘ç»œç»“åˆä¸åŒåˆ†è¾¨ç‡å’Œæ·±åº¦çš„CAMsï¼Œç»“åˆç‰¹å¾é®è”½æŠ€æœ¯ï¼ŒMECAMèƒ½æœ‰æ•ˆæ•æ‰å›¾åƒçš„å…¨å±€å’Œå±€éƒ¨ç‰¹å¾è¡¨ç¤ºï¼Œæé«˜ç¦»ç¾¤å€¼æ£€æµ‹çš„ç¨³å¥æ€§ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†MECAMçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OODæ£€æµ‹åœ¨åŒ»å­¦æˆåƒåº”ç”¨ä¸­å¯¹äºç¡®ä¿æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¯é æ€§è‡³å…³é‡è¦ã€‚</li>
<li>ç±»æ¿€æ´»æ˜ å°„ï¼ˆCAMsï¼‰å¯¹äºåŒºåˆ†IDæ•°æ®å’ŒOODæ•°æ®éå¸¸æœ‰ç”¨ï¼ŒIDæ•°æ®çš„CAMsé€šå¸¸å¼ºè°ƒä¸æ¨¡å‹é¢„æµ‹é«˜åº¦ç›¸å…³çš„åŒºåŸŸï¼Œè€ŒOODæ•°æ®åˆ™ç¼ºä¹è¿™ç§èšç„¦çš„æ¿€æ´»ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨å¤šå‡ºå£ç½‘ç»œç»“åˆä¸åŒåˆ†è¾¨ç‡å’Œæ·±åº¦çš„CAMsï¼Œå¯ä»¥æé«˜OODæ£€æµ‹çš„ç¨³å¥æ€§ã€‚</li>
<li>MECAMæ¡†æ¶åˆ©ç”¨ç‰¹å¾é®è”½æŠ€æœ¯ï¼Œé€šè¿‡æ©è”½è¾“å…¥å›¾åƒæ¥åŒºåˆ†IDæ•°æ®å’ŒOODæ•°æ®çš„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†MECAMçš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ISIC19ã€PathMNISTã€RSNA Pneumoniaã€COVID-19ã€HeadCTä»¥åŠè‡ªç„¶å›¾åƒOODæ•°æ®é›†iSUNã€‚</li>
<li>ä¸æœ€æ–°çš„OODæ£€æµ‹æ–¹æ³•ç›¸æ¯”ï¼ŒMECAMå…·æœ‰ä¼˜è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3ea5504092a6dd180a484a13669a0cee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ce4a8019305f084d5241fc0c39a7dc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc52b8e410311486affca31b7e0475b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-991ac2f4dbd3124a527b79de06890dbc.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="An-incremental-algorithm-for-non-convex-AI-enhanced-medical-image-processing"><a href="#An-incremental-algorithm-for-non-convex-AI-enhanced-medical-image-processing" class="headerlink" title="An incremental algorithm for non-convex AI-enhanced medical image   processing"></a>An incremental algorithm for non-convex AI-enhanced medical image   processing</h2><p><strong>Authors:Elena Morotti</strong></p>
<p>Solving non-convex regularized inverse problems is challenging due to their complex optimization landscapes and multiple local minima. However, these models remain widely studied as they often yield high-quality, task-oriented solutions, particularly in medical imaging, where the goal is to enhance clinically relevant features rather than merely minimizing global error. We propose incDG, a hybrid framework that integrates deep learning with incremental model-based optimization to efficiently approximate the $\ell_0$-optimal solution of imaging inverse problems. Built on the Deep Guess strategy, incDG exploits a deep neural network to generate effective initializations for a non-convex variational solver, which refines the reconstruction through regularized incremental iterations. This design combines the efficiency of Artificial Intelligence (AI) tools with the theoretical guarantees of model-based optimization, ensuring robustness and stability. We validate incDG on TpV-regularized optimization tasks, demonstrating its effectiveness in medical image deblurring and tomographic reconstruction across diverse datasets, including synthetic images, brain CT slices, and chest-abdomen scans. Results show that incDG outperforms both conventional iterative solvers and deep learning-based methods, achieving superior accuracy and stability. Moreover, we confirm that training incDG without ground truth does not significantly degrade performance, making it a practical and powerful tool for solving non-convex inverse problems in imaging and beyond. </p>
<blockquote>
<p>è§£å†³éå‡¸æ­£åˆ™åŒ–åé—®é¢˜å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰å¤æ‚çš„ä¼˜åŒ–æ™¯è§‚å’Œå¤šä¸ªå±€éƒ¨æœ€å°å€¼ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»è¢«å¹¿æ³›ç ”ç©¶ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸ä¼šäº§ç”Ÿé«˜è´¨é‡çš„ä»»åŠ¡å¯¼å‘è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»å­¦æˆåƒä¸­ï¼Œç›®æ ‡æ˜¯å¢å¼ºä¸ä¸´åºŠç›¸å…³çš„ç‰¹å¾ï¼Œè€Œä¸ä»…ä»…æ˜¯å‡å°‘å…¨å±€é”™è¯¯ã€‚æˆ‘ä»¬æå‡ºäº†incDGï¼Œè¿™æ˜¯ä¸€ä¸ªæ··åˆæ¡†æ¶ï¼Œå®ƒå°†æ·±åº¦å­¦ä¹ ä¸å¢é‡æ¨¡å‹ä¼˜åŒ–ç›¸ç»“åˆï¼Œä»¥æœ‰æ•ˆåœ°é€¼è¿‘æˆåƒåé—®é¢˜çš„l_0æœ€ä¼˜è§£ã€‚incDGå»ºç«‹åœ¨Deep Guessç­–ç•¥ä¹‹ä¸Šï¼Œåˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œä¸ºéçº¿æ€§å˜åˆ†æ±‚è§£å™¨ç”Ÿæˆæœ‰æ•ˆçš„åˆå§‹åŒ–ï¼Œé€šè¿‡æ­£åˆ™åŒ–çš„å¢é‡è¿­ä»£æ¥å®Œå–„é‡å»ºã€‚è¿™ç§è®¾è®¡ç»“åˆäº†äººå·¥æ™ºèƒ½å·¥å…·çš„æ•ˆç‡å’Œæ¨¡å‹ä¼˜åŒ–çš„ç†è®ºä¿è¯ï¼Œç¡®ä¿äº†ç¨³å¥æ€§å’Œç¨³å®šæ€§ã€‚æˆ‘ä»¬åœ¨TpVæ­£åˆ™åŒ–ä¼˜åŒ–ä»»åŠ¡ä¸ŠéªŒè¯äº†incDGçš„æœ‰æ•ˆæ€§ï¼Œå®ƒåœ¨åŒ»å­¦å›¾åƒå»æ¨¡ç³Šå’Œæ–­å±‚é‡å»ºæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ¶‰åŠå¤šç§æ•°æ®é›†ï¼ŒåŒ…æ‹¬åˆæˆå›¾åƒã€è„‘éƒ¨CTåˆ‡ç‰‡å’Œè…¹éƒ¨æ‰«æã€‚ç»“æœè¡¨æ˜ï¼ŒincDGåœ¨å‡†ç¡®æ€§å’Œç¨³å®šæ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„è¿­ä»£æ±‚è§£å™¨å’ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç¡®è®¤åœ¨æ²¡æœ‰çœŸå®å€¼çš„æƒ…å†µä¸‹è®­ç»ƒincDGä¸ä¼šæ˜¾è‘—é™ä½å…¶æ€§èƒ½ï¼Œä½¿å…¶æˆä¸ºè§£å†³æˆåƒå’Œéå‡¸åé—®é¢˜é¢†åŸŸçš„å®ç”¨ä¸”å¼ºå¤§çš„å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08324v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹éå‡¸æ­£åˆ™åŒ–åé—®é¢˜çš„æ±‚è§£ï¼Œç”±äºå…¶å¤æ‚çš„ä¼˜åŒ–æ™¯è§‚å’Œå¤šä¸ªå±€éƒ¨æœ€å°å€¼ï¼Œå¸¦æ¥å¾ˆå¤§æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å› å¸¸å¸¸èƒ½é’ˆå¯¹ç‰¹å®šä»»åŠ¡æä¾›é«˜è´¨é‡è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—æˆåƒé¢†åŸŸï¼Œå…¶ç›®æ ‡åœ¨äºå¢å¼ºä¸´åºŠç›¸å…³ç‰¹å¾ï¼Œè€Œéä»…ä»…å‡å°‘å…¨å±€é”™è¯¯ï¼Œå› æ­¤ä»å—åˆ°å¹¿æ³›ç ”ç©¶ã€‚æœ¬æ–‡æå‡ºincDGè¿™ä¸€æ··åˆæ¡†æ¶ï¼Œå®ƒæ•´åˆæ·±åº¦å­¦ä¹ åŠå¢é‡æ¨¡å‹ä¼˜åŒ–ï¼Œä»¥é«˜æ•ˆè¿‘ä¼¼æˆåƒåé—®é¢˜çš„l_0æœ€ä¼˜è§£ã€‚incDGåŸºäºæ·±åº¦çŒœæµ‹ç­–ç•¥ï¼Œåˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œä¸ºéçº¿æ€§å˜åˆ†æ±‚è§£å™¨ç”Ÿæˆæœ‰æ•ˆåˆå§‹åŒ–ï¼Œé€šè¿‡æ­£åˆ™åŒ–å¢é‡è¿­ä»£ä¼˜åŒ–é‡å»ºã€‚æ­¤è®¾è®¡ç»“åˆäººå·¥æ™ºèƒ½å·¥å…·çš„é«˜æ•ˆæ€§ä¸æ¨¡å‹åŸºç¡€ä¼˜åŒ–çš„ç†è®ºä¿éšœï¼Œç¡®ä¿äº†ç¨³å¥æ€§å’Œç¨³å®šæ€§ã€‚æœ¬æ–‡åœ¨TpVæ­£åˆ™ä¼˜åŒ–ä»»åŠ¡ä¸ŠéªŒè¯äº†incDGçš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºå…¶åœ¨åŒ»ç–—å›¾åƒå»æ¨¡ç³Šå’Œå±‚æé‡å»ºä¸­çš„ä¼˜åŠ¿ï¼Œæ¶‰åŠåˆæˆå›¾åƒã€è„‘éƒ¨CTåˆ‡ç‰‡å’Œè…¹éƒ¨æ‰«æç­‰å¤šç§æ•°æ®é›†ã€‚ç»“æœæ˜¾ç¤ºï¼ŒincDGç›¸è¾ƒäºä¼ ç»Ÿè¿­ä»£æ±‚è§£å™¨å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œæ— éœ€çœŸå®å€¼è®­ç»ƒincDGå¹¶ä¸ä¼šæ˜¾è‘—å½±å“æ€§èƒ½ï¼Œä½¿å…¶æˆä¸ºè§£å†³æˆåƒå’Œéå‡¸åé—®é¢˜å®ç”¨ä¸”å¼ºå¤§çš„å·¥å…·ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>éå‡¸æ­£åˆ™åŒ–åé—®é¢˜ç”±äºå¤æ‚çš„ä¼˜åŒ–æ™¯è§‚å’Œå¤šä¸ªå±€éƒ¨æœ€å°å€¼è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>incDGæ¡†æ¶ç»“åˆäº†æ·±åº¦å­¦ä¹ ä¸å¢é‡æ¨¡å‹ä¼˜åŒ–ï¼Œä»¥é«˜æ•ˆæ±‚è§£æˆåƒåé—®é¢˜çš„l_0æœ€ä¼˜è§£ã€‚</li>
<li>incDGåŸºäºæ·±åº¦çŒœæµ‹ç­–ç•¥ï¼Œåˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œåˆå§‹åŒ–éçº¿æ€§å˜åˆ†æ±‚è§£å™¨ã€‚</li>
<li>incDGåœ¨åŒ»ç–—å›¾åƒå»æ¨¡ç³Šå’Œå±‚æé‡å»ºä»»åŠ¡ä¸Šå±•ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>incDGåœ¨å¤šç§æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿè¿­ä»£æ±‚è§£å™¨å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>incDGçš„è®¾è®¡ç»“åˆäº†äººå·¥æ™ºèƒ½å·¥å…·çš„é«˜æ•ˆæ€§å’Œæ¨¡å‹åŸºç¡€ä¼˜åŒ–çš„ç†è®ºä¿éšœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08324">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cabc3f67b16edab1eafa3cc9546cd936.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-303457bef0d7fa2987fdf3ce88011895.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-789cca0064470f5450324de3bf96f544.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Skeleton-Guided-Diffusion-Model-for-Accurate-Foot-X-ray-Synthesis-in-Hallux-Valgus-Diagnosis"><a href="#Skeleton-Guided-Diffusion-Model-for-Accurate-Foot-X-ray-Synthesis-in-Hallux-Valgus-Diagnosis" class="headerlink" title="Skeleton-Guided Diffusion Model for Accurate Foot X-ray Synthesis in   Hallux Valgus Diagnosis"></a>Skeleton-Guided Diffusion Model for Accurate Foot X-ray Synthesis in   Hallux Valgus Diagnosis</h2><p><strong>Authors:Midi Wan, Pengfei Li, Yizhuo Liang, Di Wu, Yushan Pan, Guangzhen Zhu, Hao Wang</strong></p>
<p>Medical image synthesis plays a crucial role in providing anatomically accurate images for diagnosis and treatment. Hallux valgus, which affects approximately 19% of the global population, requires frequent weight-bearing X-rays for assessment, placing additional strain on both patients and healthcare providers. Existing X-ray models often struggle to balance image fidelity, skeletal consistency, and physical constraints, particularly in diffusion-based methods that lack skeletal guidance. We propose the Skeletal-Constrained Conditional Diffusion Model (SCCDM) and introduce KCC, a foot evaluation method utilizing skeletal landmarks. SCCDM incorporates multi-scale feature extraction and attention mechanisms, improving the Structural Similarity Index (SSIM) by 5.72% (0.794) and Peak Signal-to-Noise Ratio (PSNR) by 18.34% (21.40 dB). When combined with KCC, the model achieves an average score of 0.85, demonstrating strong clinical applicability. The code is available at <a target="_blank" rel="noopener" href="https://github.com/midisec/SCCDM">https://github.com/midisec/SCCDM</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆæˆåœ¨æä¾›ç”¨äºè¯Šæ–­å’Œæ²»ç–—è§£å‰–å‡†ç¡®çš„å›¾åƒæ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚å¤§çº¦19%çš„å…¨çƒäººå£å—åˆ°æ‹‡è¶¾å¤–ç¿»çš„å½±å“ï¼Œè¿™éœ€è¦ç»å¸¸è¿›è¡Œè´Ÿé‡Xå°„çº¿æ£€æŸ¥è¿›è¡Œè¯„ä¼°ï¼Œç»™æ‚£è€…å’ŒåŒ»ç–—æœåŠ¡æä¾›è€…å¸¦æ¥é¢å¤–çš„å‹åŠ›ã€‚ç°æœ‰çš„Xå°„çº¿æ¨¡å‹å¾€å¾€éš¾ä»¥å¹³è¡¡å›¾åƒä¿çœŸåº¦ã€éª¨éª¼ä¸€è‡´æ€§å’Œç‰©ç†çº¦æŸï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹éª¨éª¼æŒ‡å¯¼çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ä¸­æ›´æ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬æå‡ºäº†éª¨éª¼çº¦æŸæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆSCCDMï¼‰ï¼Œå¹¶å¼•å…¥äº†åˆ©ç”¨éª¨éª¼æ ‡å¿—çš„è¶³éƒ¨è¯„ä¼°æ–¹æ³•KCCã€‚SCCDMç»“åˆäº†å¤šå°ºåº¦ç‰¹å¾æå–å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜äº†ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰5.72%ï¼ˆå¾—åˆ†ä¸º0.794ï¼‰ï¼Œå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æé«˜äº†18.34%ï¼ˆå¾—åˆ†ä¸º21.40åˆ†è´ï¼‰ã€‚ä¸KCCç›¸ç»“åˆåï¼Œæ¨¡å‹å¾—åˆ†å¹³å‡ä¸º0.85ï¼Œæ˜¾ç¤ºå‡ºå¾ˆå¼ºçš„ä¸´åºŠé€‚ç”¨æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/midisec/SCCDM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/midisec/SCCDMè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08247v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒåˆæˆåœ¨è¯Šæ–­å’Œæ²»ç–—ä¸­æä¾›è§£å‰–å‡†ç¡®çš„å›¾åƒï¼Œèµ·åˆ°å…³é”®ä½œç”¨ã€‚é’ˆå¯¹è¶³æ‹‡è¶¾å¤–ç¿»ï¼ˆå½±å“å…¨çƒçº¦19%äººå£ï¼‰çš„è¯„ä¼°ï¼Œç°æœ‰Xå°„çº¿æ¨¡å‹åœ¨å›¾åƒä¿çœŸåº¦ã€éª¨éª¼ä¸€è‡´æ€§å’Œç‰©ç†çº¦æŸä¹‹é—´éš¾ä»¥å¹³è¡¡ã€‚æˆ‘ä»¬æå‡ºäº†éª¨éª¼çº¦æŸæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆSCCDMï¼‰ï¼Œå¹¶ç»“åˆè¶³éƒ¨è¯„ä¼°æ–¹æ³•KCCï¼Œåˆ©ç”¨éª¨éª¼åœ°æ ‡å®ç°å¤šå°ºåº¦ç‰¹å¾æå–å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œæé«˜äº†ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰å’Œå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ï¼Œå±•ç°å‡ºå¼ºå¤§çš„ä¸´åºŠé€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆæˆåœ¨è¯Šæ–­å’Œæ²»ç–—ä¸­è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿæä¾›è§£å‰–å‡†ç¡®çš„å›¾åƒã€‚</li>
<li>è¶³æ‹‡è¶¾å¤–ç¿»æ˜¯ä¸€ç§å¸¸è§çš„ç—…ç—‡ï¼Œéœ€è¦é¢‘ç¹è¿›è¡Œè´Ÿé‡Xå°„çº¿æ£€æŸ¥ï¼Œå¯¹ç—…äººå’ŒåŒ»ç–—æä¾›è€…éƒ½å¢åŠ äº†è´Ÿæ‹…ã€‚</li>
<li>ç°æœ‰Xå°„çº¿æ¨¡å‹åœ¨å¹³è¡¡å›¾åƒè´¨é‡æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ‰©æ•£æ¨¡å‹ä¸­ç¼ºä¹éª¨éª¼æŒ‡å¯¼ã€‚</li>
<li>æå‡ºçš„éª¨éª¼çº¦æŸæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼ˆSCCDMï¼‰ç»“åˆäº†å¤šå°ºåº¦ç‰¹å¾æå–å’Œæ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>SCCDMé€šè¿‡ç»“åˆKCCæ–¹æ³•ï¼Œæé«˜äº†ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰å’Œå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ï¼Œå±•ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>SCCDMæ¨¡å‹çš„ä¸´åºŠé€‚ç”¨æ€§å¾—åˆ°äº†éªŒè¯ï¼Œå¹³å‡å¾—åˆ†ä¸º0.85ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08247">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fa628ffd908654f6b3abcbc00580857f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-68d4c8f9542aaf67cfd32cc5d7b9fc75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00f2a6685402e1d840976b04fbb02756.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-261b4361989aa5c5a81211a63f83bd6f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="EventDiff-A-Unified-and-Efficient-Diffusion-Model-Framework-for-Event-based-Video-Frame-Interpolation"><a href="#EventDiff-A-Unified-and-Efficient-Diffusion-Model-Framework-for-Event-based-Video-Frame-Interpolation" class="headerlink" title="EventDiff: A Unified and Efficient Diffusion Model Framework for   Event-based Video Frame Interpolation"></a>EventDiff: A Unified and Efficient Diffusion Model Framework for   Event-based Video Frame Interpolation</h2><p><strong>Authors:Hanle Zheng, Xujie Han, Zegang Peng, Shangbin Zhang, Guangxun Du, Zhuo Zou, Xilin Wang, Jibin Wu, Hao Guo, Lei Deng</strong></p>
<p>Video Frame Interpolation (VFI) is a fundamental yet challenging task in computer vision, particularly under conditions involving large motion, occlusion, and lighting variation. Recent advancements in event cameras have opened up new opportunities for addressing these challenges. While existing event-based VFI methods have succeeded in recovering large and complex motions by leveraging handcrafted intermediate representations such as optical flow, these designs often compromise high-fidelity image reconstruction under subtle motion scenarios due to their reliance on explicit motion modeling. Meanwhile, diffusion models provide a promising alternative for VFI by reconstructing frames through a denoising process, eliminating the need for explicit motion estimation or warping operations. In this work, we propose EventDiff, a unified and efficient event-based diffusion model framework for VFI. EventDiff features a novel Event-Frame Hybrid AutoEncoder (HAE) equipped with a lightweight Spatial-Temporal Cross Attention (STCA) module that effectively fuses dynamic event streams with static frames. Unlike previous event-based VFI methods, EventDiff performs interpolation directly in the latent space via a denoising diffusion process, making it more robust across diverse and challenging VFI scenarios. Through a two-stage training strategy that first pretrains the HAE and then jointly optimizes it with the diffusion model, our method achieves state-of-the-art performance across multiple synthetic and real-world event VFI datasets. The proposed method outperforms existing state-of-the-art event-based VFI methods by up to 1.98dB in PSNR on Vimeo90K-Triplet and shows superior performance in SNU-FILM tasks with multiple difficulty levels. Compared to the emerging diffusion-based VFI approach, our method achieves up to 5.72dB PSNR gain on Vimeo90K-Triplet and 4.24X faster inference. </p>
<blockquote>
<p>è§†é¢‘å¸§æ’å€¼ï¼ˆVFIï¼‰æ˜¯è®¡ç®—æœºè§†è§‰çš„ä¸€é¡¹åŸºæœ¬ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠå¤§è¿åŠ¨ã€é®æŒ¡å’Œå…‰ç…§å˜åŒ–çš„æƒ…å†µä¸‹ã€‚äº‹ä»¶ç›¸æœºçš„æœ€æ–°è¿›å±•ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜æä¾›äº†æ–°çš„æœºé‡ã€‚è™½ç„¶ç°æœ‰çš„åŸºäºäº‹ä»¶é©±åŠ¨çš„VFIæ–¹æ³•å·²ç»èƒ½å¤Ÿé€šè¿‡åˆ©ç”¨æ‰‹å·¥åˆ¶ä½œçš„ä¸­é—´è¡¨ç¤ºï¼ˆå¦‚å…‰æµï¼‰æ¥æ¢å¤å¤æ‚çš„å¤§è¿åŠ¨ï¼Œä½†è¿™äº›è®¾è®¡åœ¨ç»†å¾®è¿åŠ¨åœºæ™¯ä¸‹å¾€å¾€ç‰ºç‰²äº†é«˜ä¿çœŸå›¾åƒé‡å»ºï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºæ˜¾å¼è¿åŠ¨å»ºæ¨¡ã€‚ä¸æ­¤åŒæ—¶ï¼Œæ‰©æ•£æ¨¡å‹é€šè¿‡å»å™ªè¿‡ç¨‹é‡å»ºå¸§ï¼Œæ— éœ€æ˜¾å¼è¿åŠ¨ä¼°è®¡æˆ–æ‰­æ›²æ“ä½œï¼Œä¸ºVFIæä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºäº‹ä»¶çš„æ‰©æ•£æ¨¡å‹æ¡†æ¶EventDiffï¼Œç”¨äºVFIã€‚EventDiffçš„ç‰¹ç‚¹æ˜¯ä¸€ç§æ–°å‹çš„äº‹ä»¶å¸§æ··åˆè‡ªåŠ¨ç¼–ç å™¨ï¼ˆHAEï¼‰ï¼Œé…å¤‡äº†ä¸€ä¸ªè½»é‡çº§çš„æ—¶ç©ºäº¤å‰æ³¨æ„åŠ›ï¼ˆSTCAï¼‰æ¨¡å—ï¼Œå¯ä»¥æœ‰æ•ˆåœ°èåˆåŠ¨æ€äº‹ä»¶æµå’Œé™æ€å¸§ã€‚ä¸ä»¥å‰çš„åŸºäºäº‹ä»¶çš„VFIæ–¹æ³•ä¸åŒï¼ŒEventDiffé€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œå»å™ªæ‰©æ•£è¿‡ç¨‹ç›´æ¥æ‰§è¡Œæ’å€¼ï¼Œä½¿å…¶åœ¨å¤šæ ·ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„VFIåœºæ™¯ä¸­æ›´åŠ ç¨³å¥ã€‚é€šè¿‡é¦–å…ˆé¢„è®­ç»ƒHAEï¼Œç„¶åä¸å…¶æ‰©æ•£æ¨¡å‹è”åˆä¼˜åŒ–çš„ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåˆæˆå’Œç°å®ä¸–ç•Œçš„äº‹ä»¶VFIæ•°æ®é›†ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨Vimeo90K-Tripletä¸Šçš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æ¯”ç°æœ‰çš„åŸºäºäº‹ä»¶çš„VFIæ–¹æ³•é«˜å‡º1.98dBï¼Œåœ¨å…·æœ‰å¤šä¸ªéš¾åº¦çº§åˆ«çš„SNU-FILMä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä¸æ–°å…´çš„åŸºäºæ‰©æ•£çš„VFIæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨Vimeo90K-Tripletä¸Šå®ç°äº†é«˜è¾¾5.72dBçš„PSNRå¢ç›Šï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†4.24å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08235v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºäº‹ä»¶æ‰©æ•£æ¨¡å‹çš„è§†é¢‘å¸§æ’å€¼ï¼ˆVFIï¼‰æ¡†æ¶ï¼Œåä¸ºEventDiffã€‚å®ƒé€šè¿‡ç»“åˆäº‹ä»¶æµä¸é™æ€å¸§ï¼Œåˆ©ç”¨äº‹ä»¶å¸§æ··åˆè‡ªåŠ¨ç¼–ç å™¨ï¼ˆHAEï¼‰å’Œæ—¶ç©ºäº¤å‰æ³¨æ„åŠ›ï¼ˆSTCAï¼‰æ¨¡å—ï¼Œå®ç°äº†åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„ç›´æ¥æ’å€¼ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºäº‹ä»¶çš„VFIæ–¹æ³•ç›¸æ¯”ï¼ŒEventDiffæ›´åŠ ç¨³å¥ï¼Œé€‚ç”¨äºå„ç§æŒ‘æˆ˜æ€§çš„VFIåœºæ™¯ã€‚é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåˆæˆå’ŒçœŸå®ä¸–ç•Œäº‹ä»¶VFIæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨Vimeo90K-Tripletä¸Šï¼Œè¯¥æ–¹æ³•è¾ƒç°æœ‰æŠ€æœ¯æé«˜äº†1.98dBçš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ï¼Œåœ¨SNU-FILMä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸”æ¨ç†é€Ÿåº¦æ¯”æ–°å…´çš„æ‰©æ•£VFIæ–¹æ³•å¿«4.24å€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘å¸§æ’å€¼ï¼ˆVFIï¼‰æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„ä¸€é¡¹åŸºæœ¬è€Œå…·æœ‰æŒ‘æˆ˜çš„ä»»åŠ¡ï¼Œå°¤å…¶åœ¨æ¶‰åŠå¤§è¿åŠ¨ã€é®æŒ¡å’Œå…‰ç…§å˜åŒ–çš„æƒ…å†µä¸‹ã€‚</li>
<li>äº‹ä»¶ç›¸æœºçš„å‘å±•ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜æä¾›äº†æ–°çš„æœºä¼šã€‚</li>
<li>ç°æœ‰çš„åŸºäºäº‹ä»¶çš„æ–¹æ³•è™½ç„¶èƒ½å¤Ÿé€šè¿‡åˆ©ç”¨å…‰å­¦æµç­‰ä¸­é—´è¡¨ç¤ºæ¥æ¢å¤å¤§è¿åŠ¨ï¼Œä½†åœ¨ç»†å¾®è¿åŠ¨åœºæ™¯ä¸‹éš¾ä»¥å®ç°é«˜ä¿çœŸå›¾åƒé‡å»ºã€‚</li>
<li>æ‰©æ•£æ¨¡å‹é€šè¿‡å»å™ªè¿‡ç¨‹é‡å»ºå¸§ï¼Œæ— éœ€æ˜¾å¼è¿åŠ¨ä¼°è®¡æˆ–æ‰­æ›²æ“ä½œï¼Œä¸ºVFIæä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>EventDiffæ˜¯ä¸€ä¸ªåŸºäºäº‹ä»¶çš„æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œé€šè¿‡äº‹ä»¶å¸§æ··åˆè‡ªåŠ¨ç¼–ç å™¨å’Œæ—¶ç©ºäº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œå®ç°äº†åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„ç›´æ¥æ’å€¼ã€‚</li>
<li>EventDiffé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå…ˆåœ¨é™æ€æ•°æ®é›†ä¸Šè®­ç»ƒè‡ªåŠ¨ç¼–ç å™¨ï¼Œç„¶åå°†å…¶ä¸ä¼˜åŒ–æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œè¾¾åˆ°æœ€ä¼˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08235">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7f6282d30bdba5df60d4bdb664f8c90d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-947a8d18dceabff7cf49a4e21ed9a7e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b830bd0a6cfebe68ea3132abb2b9fb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9123eb7d17a9edc71f0644f05bb016f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-986e0875fc30a20aaf1fe57a2d1500ca.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="JSover-Joint-Spectrum-Estimation-and-Multi-Material-Decomposition-from-Single-Energy-CT-Projections"><a href="#JSover-Joint-Spectrum-Estimation-and-Multi-Material-Decomposition-from-Single-Energy-CT-Projections" class="headerlink" title="JSover: Joint Spectrum Estimation and Multi-Material Decomposition from   Single-Energy CT Projections"></a>JSover: Joint Spectrum Estimation and Multi-Material Decomposition from   Single-Energy CT Projections</h2><p><strong>Authors:Qing Wu, Hongjiang Wei, Jingyi Yu, S. Kevin Zhou, Yuyao Zhang</strong></p>
<p>Multi-material decomposition (MMD) enables quantitative reconstruction of tissue compositions in the human body, supporting a wide range of clinical applications. However, traditional MMD typically requires spectral CT scanners and pre-measured X-ray energy spectra, significantly limiting clinical applicability. To this end, various methods have been developed to perform MMD using conventional (i.e., single-energy, SE) CT systems, commonly referred to as SEMMD. Despite promising progress, most SEMMD methods follow a two-step image decomposition pipeline, which first reconstructs monochromatic CT images using algorithms such as FBP, and then performs decomposition on these images. The initial reconstruction step, however, neglects the energy-dependent attenuation of human tissues, introducing severe nonlinear beam hardening artifacts and noise into the subsequent decomposition. This paper proposes JSover, a fundamentally reformulated one-step SEMMD framework that jointly reconstructs multi-material compositions and estimates the energy spectrum directly from SECT projections. By explicitly incorporating physics-informed spectral priors into the SEMMD process, JSover accurately simulates a virtual spectral CT system from SE acquisitions, thereby improving the reliability and accuracy of decomposition. Furthermore, we introduce implicit neural representation (INR) as an unsupervised deep learning solver for representing the underlying material maps. The inductive bias of INR toward continuous image patterns constrains the solution space and further enhances estimation quality. Extensive experiments on both simulated and real CT datasets show that JSover outperforms state-of-the-art SEMMD methods in accuracy and computational efficiency. </p>
<blockquote>
<p>å¤šææ–™åˆ†è§£ï¼ˆMMDï¼‰æŠ€æœ¯èƒ½å¤Ÿå®šé‡é‡å»ºäººä½“ç»„ç»‡æˆåˆ†ï¼Œæ”¯æŒå¹¿æ³›çš„åº”ç”¨äºä¸´åºŠã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„MMDé€šå¸¸éœ€è¦å…‰è°±CTæ‰«æä»ªå’Œé¢„æµ‹é‡çš„Xå°„çº¿èƒ½é‡å…‰è°±ï¼Œè¿™æå¤§åœ°é™åˆ¶äº†å…¶åœ¨ä¸´åºŠä¸Šçš„é€‚ç”¨æ€§ã€‚ä¸ºæ­¤ï¼Œå·²ç»å¼€å‘äº†å„ç§ä½¿ç”¨å¸¸è§„ï¼ˆå³å•èƒ½é‡ï¼ŒSEï¼‰CTç³»ç»Ÿçš„MMDæ–¹æ³•ï¼Œé€šå¸¸è¢«ç§°ä¸ºSEMMDã€‚å°½ç®¡å‰æ™¯å……æ»¡å¸Œæœ›ï¼Œä½†å¤§å¤šæ•°SEMMDæ–¹æ³•éµå¾ªä¸¤æ­¥å›¾åƒåˆ†è§£æµç¨‹ï¼Œé¦–å…ˆä½¿ç”¨FBPç­‰ç®—æ³•é‡å»ºå•è‰²CTå›¾åƒï¼Œç„¶åå¯¹è¿™äº›å›¾åƒè¿›è¡Œåˆ†è§£ã€‚ç„¶è€Œï¼Œæœ€åˆçš„é‡å»ºæ­¥éª¤å¿½è§†äº†äººä½“ç»„ç»‡çš„èƒ½é‡ä¾èµ–è¡°å‡ï¼Œå¼•å…¥äº†ä¸¥é‡çš„éçº¿æ€§æŸç¡¬åŒ–ä¼ªå½±å’Œå™ªå£°ï¼Œå½±å“åç»­çš„åˆ†è§£ã€‚æœ¬æ–‡æå‡ºäº†JSoverï¼Œè¿™æ˜¯ä¸€ä¸ªä»æ ¹æœ¬ä¸Šé‡æ–°åˆ¶å®šçš„ä¸€æ­¥å¼SEMMDæ¡†æ¶ï¼Œå®ƒè”åˆé‡å»ºå¤šææ–™æˆåˆ†å¹¶ç›´æ¥ä»SECTæŠ•å½±ä¼°è®¡èƒ½é‡å…‰è°±ã€‚é€šè¿‡æ˜¾å¼åœ°å°†ç‰©ç†ä¿¡æ¯å…‰è°±å…ˆéªŒçŸ¥è¯†çº³å…¥SEMMDè¿‡ç¨‹ï¼ŒJSoverèƒ½å¤Ÿæ¨¡æ‹Ÿä»SEé‡‡é›†ä¸­çš„è™šæ‹Ÿå…‰è°±CTç³»ç»Ÿï¼Œä»è€Œæé«˜åˆ†è§£çš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰ä½œä¸ºä¸€ç§æ— ç›‘ç£æ·±åº¦å­¦ä¹ æ±‚è§£å™¨ï¼Œç”¨äºè¡¨ç¤ºåº•å±‚ææ–™å›¾ã€‚INRå¯¹è¿ç»­å›¾åƒæ¨¡å¼çš„å½’çº³åè§çº¦æŸäº†è§£ç©ºé—´ï¼Œè¿›ä¸€æ­¥æé«˜äº†ä¼°è®¡è´¨é‡ã€‚åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®CTæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒJSoveråœ¨å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¸Šä¼˜äºæœ€å…ˆè¿›çš„SEMMDæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08123v1">PDF</a> 11 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šææ–™åˆ†è§£ï¼ˆMMDï¼‰åœ¨äººä½“ç»„ç»‡æˆåˆ†å®šé‡é‡å»ºä¸­çš„åº”ç”¨ï¼Œå¹¶æ”¯æŒå¹¿æ³›çš„åº”ç”¨äºä¸´åºŠã€‚ä¼ ç»ŸMMDéœ€è¦å…‰è°±CTæ‰«æä»ªå’Œé¢„æµ‹Xå°„çº¿èƒ½é‡å…‰è°±ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠçš„é€‚ç”¨æ€§ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…å¼€å‘äº†ä¸€ç§ä½¿ç”¨å¸¸è§„CTç³»ç»Ÿçš„SEMMDæ–¹æ³•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºJSoverçš„ä¸€ç«™å¼SEMMDæ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥ä»SECTæŠ•å½±è”åˆé‡å»ºå¤šææ–™æˆåˆ†å¹¶ä¼°è®¡èƒ½é‡å…‰è°±ã€‚è¯¥æ¡†æ¶é€šè¿‡èå…¥ç‰©ç†å…‰è°±å…ˆéªŒä¿¡æ¯ï¼Œæ¨¡æ‹Ÿå‡ºè™šæ‹Ÿå…‰è°±CTç³»ç»Ÿï¼Œæé«˜äº†åˆ†è§£çš„å¯é æ€§å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œå¼•å…¥éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰ä½œä¸ºæ— ç›‘ç£æ·±åº¦å­¦ä¹ æ±‚è§£å™¨ï¼Œç”¨äºè¡¨ç¤ºåŸºç¡€ææ–™å›¾ã€‚å®éªŒè¯æ˜ï¼ŒJSoveråœ¨å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¸Šä¼˜äºç°æœ‰SEMMDæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MMDæŠ€æœ¯ç”¨äºäººä½“ç»„ç»‡æˆåˆ†çš„å®šé‡é‡å»ºï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨äºä¸´åºŠçš„æ½œåŠ›ã€‚</li>
<li>ä¼ ç»ŸMMDæ–¹æ³•éœ€è¦å…‰è°±CTæ‰«æä»ªå’Œé¢„æµ‹Xå°„çº¿èƒ½é‡å…‰è°±ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠçš„é€‚ç”¨æ€§ã€‚</li>
<li>JSoveræ˜¯ä¸€ç§æ–°å‹çš„SEMMDæ¡†æ¶ï¼Œèƒ½åœ¨ä¸€æ­¥å†…ç›´æ¥ä»SECTæŠ•å½±è¿›è¡Œå¤šææ–™æˆåˆ†çš„é‡å»ºå’Œèƒ½é‡å…‰è°±çš„ä¼°è®¡ã€‚</li>
<li>JSoveré€šè¿‡èå…¥ç‰©ç†å…‰è°±å…ˆéªŒä¿¡æ¯ï¼Œæœ‰æ•ˆåœ°æ¨¡æ‹Ÿå‡ºè™šæ‹Ÿå…‰è°±CTç³»ç»Ÿã€‚</li>
<li>éšå¼ç¥ç»è¡¨ç¤ºï¼ˆINRï¼‰è¢«å¼•å…¥ä½œä¸ºæ— ç›‘ç£æ·±åº¦å­¦ä¹ æ±‚è§£å™¨ï¼Œç”¨äºè¡¨ç¤ºåŸºç¡€ææ–™å›¾ï¼Œæé«˜äº†ä¼°è®¡è´¨é‡ã€‚</li>
<li>JSoveråœ¨å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ä¸Šè¶…è¶Šäº†ç°æœ‰çš„SEMMDæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08123">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6a0ddc163a54f8bf2a630f85fc8a28c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-858fdbaf8b437b407bfd3f5977f85337.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e610cdb750e79ac3b5335725b6c3fc7e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dea237e39dc5b47782fac3cdd26ecf9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c5ffd863978e7f56ae44bc285e1c55b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e99a41d4c047362124d6d35083822ff.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Probabilistic-approach-to-longitudinal-response-prediction-application-to-radiomics-from-brain-cancer-imaging"><a href="#Probabilistic-approach-to-longitudinal-response-prediction-application-to-radiomics-from-brain-cancer-imaging" class="headerlink" title="Probabilistic approach to longitudinal response prediction: application   to radiomics from brain cancer imaging"></a>Probabilistic approach to longitudinal response prediction: application   to radiomics from brain cancer imaging</h2><p><strong>Authors:Isabella Cama, Michele Piana, Cristina Campi, Sara Garbarino</strong></p>
<p>Longitudinal imaging analysis tracks disease progression and treatment response over time, providing dynamic insights into treatment efficacy and disease evolution. Radiomic features extracted from medical imaging can support the study of disease progression and facilitate longitudinal prediction of clinical outcomes. This study presents a probabilistic model for longitudinal response prediction, integrating baseline features with intermediate follow-ups. The probabilistic nature of the model naturally allows to handle the instrinsic uncertainty of the longitudinal prediction of disease progression. We evaluate the proposed model against state-of-the-art disease progression models in both a synthetic scenario and using a brain cancer dataset. Results demonstrate that the approach is competitive against existing methods while uniquely accounting for uncertainty and controlling the growth of problem dimensionality, eliminating the need for data from intermediate follow-ups. </p>
<blockquote>
<p>çºµå‘æˆåƒåˆ†æå¯ä»¥è¿½è¸ªç–¾ç—…çš„è¿›å±•å’Œæ²»ç–—æ•ˆæœéšæ—¶é—´çš„å˜åŒ–ï¼Œä¸ºæ²»ç–—æ•ˆæœå’Œç–¾ç—…æ¼”å˜æä¾›åŠ¨æ€è§è§£ã€‚ä»åŒ»å­¦å›¾åƒä¸­æå–çš„æ”¾å°„å­¦ç‰¹å¾å¯ä»¥æ”¯æŒå¯¹ç–¾ç—…è¿›å±•çš„ç ”ç©¶ï¼Œå¹¶ä¿ƒè¿›å¯¹ä¸´åºŠç»“æœçš„çºµå‘é¢„æµ‹ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ¦‚ç‡çš„çºµå‘ååº”é¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†åŸºçº¿ç‰¹å¾ä¸ä¸­é—´éšè®¿ç»“æœç›¸ç»“åˆã€‚è¯¥æ¨¡å‹çš„æ¦‚ç‡æ€§è´¨è‡ªç„¶åœ°å¤„ç†äº†ç–¾ç—…è¿›å±•çºµå‘é¢„æµ‹çš„å†…åœ¨ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬åœ¨åˆæˆåœºæ™¯å’Œè„‘ç™Œæ•°æ®é›†ä¸Šå¯¹æ‰€æå‡ºçš„æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶ç‹¬ç‰¹åœ°è€ƒè™‘äº†ä¸ç¡®å®šæ€§å¹¶æ§åˆ¶äº†é—®é¢˜ç»´åº¦çš„å¢é•¿ï¼Œä»è€Œæ— éœ€ä¸­é—´éšè®¿æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07973v1">PDF</a> 21 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>çºµå‘æˆåƒåˆ†æèƒ½éšæ—¶é—´è¿½è¸ªç–¾ç—…è¿›å±•å’Œæ²»ç–—æ•ˆæœï¼Œä¸ºæ²»ç–—æ•ˆæœå’Œç–¾ç—…æ¼”å˜æä¾›åŠ¨æ€è§è§£ã€‚ä»åŒ»å­¦å½±åƒä¸­æå–çš„æ”¾å°„ç»„å­¦ç‰¹å¾æœ‰åŠ©äºç ”ç©¶ç–¾ç—…è¿›å±•ï¼Œå¹¶ä¿ƒè¿›å¯¹ä¸´åºŠç»“æœçš„çºµå‘é¢„æµ‹ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ¦‚ç‡çš„çºµå‘ååº”é¢„æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†åŸºçº¿ç‰¹å¾ä¸ä¸­é—´éšè®¿ç›¸ç»“åˆã€‚è¯¥æ¨¡å‹çš„æ¦‚ç‡æ€§è‡ªç„¶åœ°å¤„ç†äº†ç–¾ç—…è¿›å±•çºµå‘é¢„æµ‹çš„å†…åœ¨ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬åœ¨åˆæˆåœºæ™¯å’Œè„‘ç™Œæ•°æ®é›†ä¸­è¯„ä¼°äº†æ‰€æå‡ºçš„æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åº”å¯¹ä¸ç¡®å®šæ€§åŠæ§åˆ¶é—®é¢˜ç»´åº¦å¢é•¿æ–¹é¢å…·æœ‰ç«äº‰åŠ›ï¼Œä¸”æ— éœ€ä¸­é—´éšè®¿æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çºµå‘æˆåƒåˆ†æèƒ½è¿½è¸ªç–¾ç—…è¿›å±•å’Œæ²»ç–—æ•ˆæœã€‚</li>
<li>åŒ»å­¦å½±åƒä¸­çš„æ”¾å°„ç»„å­¦ç‰¹å¾æœ‰åŠ©äºç ”ç©¶ç–¾ç—…è¿›å±•ã€‚</li>
<li>æå‡ºçš„åŸºäºæ¦‚ç‡çš„çºµå‘ååº”é¢„æµ‹æ¨¡å‹ç»“åˆäº†åŸºçº¿ç‰¹å¾ä¸ä¸­é—´éšè®¿ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤„ç†ç–¾ç—…è¿›å±•é¢„æµ‹ä¸­çš„å†…åœ¨ä¸ç¡®å®šæ€§ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹åœ¨ç«äº‰æ€§ä¸å¤„ç†ä¸ç¡®å®šæ€§æ–¹é¢è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>æ¨¡å‹èƒ½æœ‰æ•ˆæ§åˆ¶é—®é¢˜ç»´åº¦çš„å¢é•¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07973">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3b94f5cdb6a8074885ca090daaf8d5a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7eb4b3e3e91e4acbd04ec50b6768dcf2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Computationally-Efficient-Diffusion-Models-in-Medical-Imaging-A-Comprehensive-Review"><a href="#Computationally-Efficient-Diffusion-Models-in-Medical-Imaging-A-Comprehensive-Review" class="headerlink" title="Computationally Efficient Diffusion Models in Medical Imaging: A   Comprehensive Review"></a>Computationally Efficient Diffusion Models in Medical Imaging: A   Comprehensive Review</h2><p><strong>Authors: Abdullah, Tao Huang, Ickjai Lee, Euijoon Ahn</strong></p>
<p>The diffusion model has recently emerged as a potent approach in computer vision, demonstrating remarkable performances in the field of generative artificial intelligence. Capable of producing high-quality synthetic images, diffusion models have been successfully applied across a range of applications. However, a significant challenge remains with the high computational cost associated with training and generating these models. This study focuses on the efficiency and inference time of diffusion-based generative models, highlighting their applications in both natural and medical imaging. We present the most recent advances in diffusion models by categorizing them into three key models: the Denoising Diffusion Probabilistic Model (DDPM), the Latent Diffusion Model (LDM), and the Wavelet Diffusion Model (WDM). These models play a crucial role in medical imaging, where producing fast, reliable, and high-quality medical images is essential for accurate analysis of abnormalities and disease diagnosis. We first investigate the general framework of DDPM, LDM, and WDM and discuss the computational complexity gap filled by these models in natural and medical imaging. We then discuss the current limitations of these models as well as the opportunities and future research directions in medical imaging. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹æœ€è¿‘ä½œä¸ºè®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ç§å¼ºå¤§æ–¹æ³•è€Œå‡ºç°ï¼Œåœ¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½é¢†åŸŸè¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ€§èƒ½ã€‚æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿäº§ç”Ÿé«˜è´¨é‡çš„åˆæˆå›¾åƒï¼Œå¹¶å·²æˆåŠŸåº”ç”¨äºå¤šç§åº”ç”¨ç¨‹åºã€‚ç„¶è€Œï¼Œä¸è®­ç»ƒå’Œç”Ÿæˆè¿™äº›æ¨¡å‹ç›¸å…³çš„é«˜è®¡ç®—æˆæœ¬ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶é‡ç‚¹å…³æ³¨åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ¨¡å‹çš„æ•ˆç‡å’Œæ¨ç†æ—¶é—´ï¼Œå¼ºè°ƒå®ƒä»¬åœ¨è‡ªç„¶å’ŒåŒ»å­¦å½±åƒä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬é€šè¿‡å°†æ‰©æ•£æ¨¡å‹åˆ†ç±»ä¸ºä¸‰ä¸ªå…³é”®æ¨¡å‹ï¼šå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ã€æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰å’Œå°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰ï¼Œæ¥ä»‹ç»å…¶æœ€æ–°è¿›å±•ã€‚è¿™äº›æ¨¡å‹åœ¨åŒ»å­¦å½±åƒä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œå…¶ä¸­å¿«é€Ÿã€å¯é ã€é«˜è´¨é‡çš„åŒ»å­¦å½±åƒå¯¹äºå¼‚å¸¸åˆ†æå’Œç–¾ç—…è¯Šæ–­è‡³å…³é‡è¦ã€‚æˆ‘ä»¬é¦–å…ˆç ”ç©¶DDPMã€LDMå’ŒWDMçš„ä¸€èˆ¬æ¡†æ¶ï¼Œå¹¶è®¨è®ºè¿™äº›æ¨¡å‹åœ¨è‡ªç„¶å’ŒåŒ»å­¦å½±åƒä¸­å¡«è¡¥çš„è®¡ç®—å¤æ‚æ€§å·®è·ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¨è®ºäº†è¿™äº›æ¨¡å‹çš„å½“å‰å±€é™æ€§ä»¥åŠæœªæ¥åœ¨åŒ»å­¦å½±åƒä¸­çš„æœºé‡å’Œç ”ç©¶æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.07866v1">PDF</a> pages 36, 6 figures</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå´­éœ²å¤´è§’ï¼Œå°¤å…¶åœ¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½é¢†åŸŸè¡¨ç°çªå‡ºã€‚å…¶èƒ½ç”Ÿæˆé«˜è´¨é‡åˆæˆå›¾åƒï¼Œå¹¶æˆåŠŸåº”ç”¨äºå¤šä¸ªé¢†åŸŸã€‚ç„¶è€Œï¼Œè®­ç»ƒä¸ç”Ÿæˆæ¨¡å‹çš„è®¡ç®—æˆæœ¬é«˜æ˜‚ä»æ˜¯é‡å¤§æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å…³æ³¨æ‰©æ•£å¼ç”Ÿæˆæ¨¡å‹çš„æ•ˆç‡ä¸æ¨ç†æ—¶é—´ï¼Œå¹¶å¼ºè°ƒå…¶åœ¨è‡ªç„¶ä¸åŒ»å­¦å½±åƒä¸­çš„åº”ç”¨ã€‚å°†æ‰©æ•£æ¨¡å‹åˆ†ä¸ºä¸‰å¤§å…³é”®æ¨¡å‹ï¼šå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ã€æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰å’Œå°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDMï¼‰ã€‚è¿™äº›æ¨¡å‹åœ¨åŒ»å­¦å½±åƒä¸­æ‰®æ¼”å…³é”®è§’è‰²ï¼Œèƒ½å¿«é€Ÿã€å¯é ã€é«˜è´¨é‡ç”ŸæˆåŒ»ç–—å›¾åƒï¼Œå¯¹å¼‚å¸¸åˆ†æä¸ç–¾ç—…è¯Šæ–­è‡³å…³é‡è¦ã€‚ç ”ç©¶é¦–å…ˆæ¢è®¨DDPMã€LDMå’ŒWDMçš„ä¸€èˆ¬æ¡†æ¶åŠå…¶åœ¨å½±åƒé¢†åŸŸçš„è®¡ç®—å¤æ‚åº¦å·®è·ï¼Œæ¥ç€è®¨è®ºå½“å‰æ¨¡å‹çš„å±€é™ä»¥åŠæœªæ¥åŒ»å­¦å½±åƒçš„ç ”ç©¶æœºé‡å’Œæ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸè¡¨ç°çªå‡ºï¼Œå°¤å…¶åœ¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ–¹é¢ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹èƒ½ç”Ÿæˆé«˜è´¨é‡åˆæˆå›¾åƒï¼Œå¹¶æˆåŠŸåº”ç”¨äºå¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬åŒ»å­¦å½±åƒã€‚</li>
<li>æ‰©æ•£æ¨¡å‹é¢ä¸´è®¡ç®—æˆæœ¬é«˜æ˜‚çš„æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶å…³æ³¨æ‰©æ•£æ¨¡å‹çš„æ•ˆç‡ä¸æ¨ç†æ—¶é—´ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åˆ†ä¸ºä¸‰å¤§å…³é”®æ¨¡å‹ï¼šDDPMã€LDMå’ŒWDMã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨åŒ»å­¦å½±åƒä¸­å…·æœ‰é‡è¦åº”ç”¨ï¼Œèƒ½å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œæœ‰åŠ©äºå¼‚å¸¸åˆ†æå’Œç–¾ç—…è¯Šæ–­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.07866">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a47b85fa74127ddd7bb58070642985e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91abd5b967da853886396e9b6504a2cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc655ae612d359c2bcdb9e691e8e2fb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e84c1aa533a7010578d4235c8731e28e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Automatic-quality-control-in-multi-centric-fetal-brain-MRI-super-resolution-reconstruction"><a href="#Automatic-quality-control-in-multi-centric-fetal-brain-MRI-super-resolution-reconstruction" class="headerlink" title="Automatic quality control in multi-centric fetal brain MRI   super-resolution reconstruction"></a>Automatic quality control in multi-centric fetal brain MRI   super-resolution reconstruction</h2><p><strong>Authors:Thomas Sanchez, Vladyslav Zalevskyi, Angeline Mihailov, Gerard MartÃ­-Juan, Elisenda Eixarch, Andras Jakab, Vincent Dunet, MÃ©riam Koob, Guillaume Auzias, Meritxell Bach Cuadra</strong></p>
<p>Quality control (QC) has long been considered essential to guarantee the reliability of neuroimaging studies. It is particularly important for fetal brain MRI, where acquisitions and image processing techniques are less standardized than in adult imaging. In this work, we focus on automated quality control of super-resolution reconstruction (SRR) volumes of fetal brain MRI, an important processing step where multiple stacks of thick 2D slices are registered together and combined to build a single, isotropic and artifact-free T2 weighted volume. We propose FetMRQC$<em>{SR}$, a machine-learning method that extracts more than 100 image quality metrics to predict image quality scores using a random forest model. This approach is well suited to a problem that is high dimensional, with highly heterogeneous data and small datasets. We validate FetMRQC$</em>{SR}$ in an out-of-domain (OOD) setting and report high performance (ROC AUC &#x3D; 0.89), even when faced with data from an unknown site or SRR method. We also investigate failure cases and show that they occur in $45%$ of the images due to ambiguous configurations for which the rating from the expert is arguable. These results are encouraging and illustrate how a non deep learning-based method like FetMRQC$_{SR}$ is well suited to this multifaceted problem. Our tool, along with all the code used to generate, train and evaluate the model are available at <a target="_blank" rel="noopener" href="https://github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr/">https://github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr/</a> . </p>
<blockquote>
<p>è´¨é‡æ§åˆ¶ï¼ˆQCï¼‰å¯¹äºç¥ç»å½±åƒå­¦ç ”ç©¶çš„å¯é æ€§è‡³å…³é‡è¦ï¼Œè¿™ä¸€ç‚¹é•¿æœŸä»¥æ¥å¤‡å—å…³æ³¨ã€‚å¯¹äºèƒå„¿è„‘éƒ¨MRIï¼ˆç£å…±æŒ¯æˆåƒï¼‰è€Œè¨€å°¤å…¶å¦‚æ­¤ï¼Œå› ä¸ºä¸æˆäººæˆåƒç›¸æ¯”ï¼Œèƒå„¿è„‘éƒ¨MRIçš„é‡‡é›†å’Œå›¾åƒå¤„ç†æŠ€æœ¯æ ‡å‡†åŒ–ç¨‹åº¦è¾ƒä½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºèƒå„¿è„‘éƒ¨MRIçš„è¶…åˆ†è¾¨ç‡é‡å»ºï¼ˆSRRï¼‰ä½“ç§¯çš„è‡ªåŠ¨è´¨é‡æ§åˆ¶ã€‚è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„å¤„ç†æ­¥éª¤ï¼Œéœ€è¦å°†å¤šä¸ªåšçš„äºŒç»´åˆ‡ç‰‡å †å èµ·æ¥ï¼Œå¹¶ç»„åˆæˆä¸€ä¸ªå•ä¸€çš„ã€å„å‘åŒæ€§çš„ã€æ— ä¼ªå½±çš„T2åŠ æƒä½“ç§¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºFetMRQC_{SR}çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æå–äº†è¶…è¿‡100é¡¹å›¾åƒè´¨é‡æŒ‡æ ‡ï¼Œå¹¶ä½¿ç”¨éšæœºæ£®æ—æ¨¡å‹é¢„æµ‹å›¾åƒè´¨é‡è¯„åˆ†ã€‚è¿™ç§æ–¹æ³•éå¸¸é€‚åˆé«˜ç»´ã€æ•°æ®é«˜åº¦å¼‚è´¨ä¸”æ•°æ®é›†è¾ƒå°çš„é—®é¢˜ã€‚æˆ‘ä»¬åœ¨åŸŸå¤–ï¼ˆOODï¼‰ç¯å¢ƒä¸­éªŒè¯äº†FetMRQC_{SR}ï¼Œå¹¶æŠ¥å‘Šäº†å‡ºè‰²çš„æ€§èƒ½ï¼ˆROC AUC &#x3D; 0.89ï¼‰ï¼Œå³ä½¿åœ¨é¢å¯¹æ¥è‡ªæœªçŸ¥ç«™ç‚¹æˆ–SRRæ–¹æ³•çš„æ•°æ®æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬è¿˜è°ƒæŸ¥äº†å¤±è´¥çš„æƒ…å†µï¼Œå¹¶è¡¨æ˜è¿™äº›å¤±è´¥å‘ç”Ÿåœ¨45%çš„å›¾åƒä¸­ï¼Œæ˜¯ç”±äºæ¨¡ç³Šé…ç½®å¯¼è‡´çš„ï¼Œä¸“å®¶å¯¹æ­¤çš„è¯„åˆ†å…·æœ‰äº‰è®®ã€‚è¿™äº›ç»“æœä»¤äººé¼“èˆï¼Œå¹¶è¯´æ˜äº†åƒFetMRQC_{SR}è¿™æ ·çš„éæ·±åº¦å­¦ä¹ æ–¹æ³•å¦‚ä½•é€‚åº”è¿™ç§å¤šé¢é—®é¢˜ã€‚æˆ‘ä»¬çš„å·¥å…·ä»¥åŠç”¨äºç”Ÿæˆã€è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹çš„æ‰€æœ‰ä»£ç å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr/%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10156v3">PDF</a> 11 pages, 3 figures</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡å…³æ³¨èƒå„¿è„‘éƒ¨MRIçš„è¶…åˆ†è¾¨ç‡é‡å»ºï¼ˆSRRï¼‰ä½“ç§¯çš„è‡ªåŠ¨åŒ–è´¨é‡æ§åˆ¶ã€‚æå‡ºä¸€ç§åä¸ºFetMRQC_{SR}çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æå–è¶…è¿‡100ä¸ªå›¾åƒè´¨é‡æŒ‡æ ‡ï¼Œä½¿ç”¨éšæœºæ£®æ—æ¨¡å‹é¢„æµ‹å›¾åƒè´¨é‡åˆ†æ•°ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºé«˜ç»´ã€æ•°æ®é«˜åº¦å¼‚è´¨ä¸”æ•°æ®é›†è¾ƒå°çš„é—®é¢˜ã€‚åœ¨é¢†åŸŸå¤–ï¼ˆOODï¼‰ç¯å¢ƒä¸‹éªŒè¯äº†FetMRQC_{SR}çš„æ€§èƒ½ï¼Œå³ä½¿é¢å¯¹æ¥è‡ªæœªçŸ¥ç«™ç‚¹æˆ–SRRæ–¹æ³•çš„æ•°æ®ï¼Œä¹Ÿè¡¨ç°å‡ºé«˜æ€§èƒ½ï¼ˆROC AUC &#x3D; 0.89ï¼‰ã€‚è°ƒæŸ¥å¤±è´¥æ¡ˆä¾‹å‘ç°ï¼Œ45%çš„å›¾åƒç”±äºæ¨¡ç³Šé…ç½®å¯¼è‡´ä¸“å®¶è¯„åˆ†æœ‰äº‰è®®ã€‚ç»“æœé¼“åŠ±äººå¿ƒï¼Œè¯´æ˜éæ·±åº¦å­¦ä¹ æ–¹æ³•å¦‚FetMRQC_{SR}é€‚åˆè¿™ç§å¤šé¢é—®é¢˜ã€‚ç›¸å…³å·¥å…·å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr/%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr/è·å–ã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>æœ¬ç ”ç©¶å…³æ³¨èƒå„¿è„‘éƒ¨MRIçš„è¶…åˆ†è¾¨ç‡é‡å»ºï¼ˆSRRï¼‰çš„è´¨é‡æ§åˆ¶ã€‚</li>
<li>æå‡ºä¸€ç§æœºå™¨å­¦ä¹ æ–¹æ³•FetMRQC_{SR}ï¼Œé€šè¿‡æå–è¶…è¿‡100ä¸ªå›¾åƒè´¨é‡æŒ‡æ ‡é¢„æµ‹å›¾åƒè´¨é‡åˆ†æ•°ã€‚</li>
<li>è¯¥æ–¹æ³•é€‚åº”äºé«˜ç»´ã€æ•°æ®é«˜åº¦å¼‚è´¨ä¸”æ•°æ®é›†å°çš„åœºæ™¯ã€‚</li>
<li>åœ¨é¢†åŸŸå¤–ç¯å¢ƒä¸‹éªŒè¯äº†FetMRQC_{SR}çš„é«˜æ€§èƒ½ã€‚</li>
<li>45%çš„å›¾åƒè´¨é‡è¯„åˆ†å­˜åœ¨äº‰è®®ï¼Œä¸»è¦ç”±äºå›¾åƒæ¨¡ç³Šé…ç½®å¯¼è‡´ã€‚</li>
<li>éæ·±åº¦å­¦ä¹ æ–¹æ³•å¦‚FetMRQC_{SR}é€‚åˆè§£å†³æ­¤ç±»å¤šé¢é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10156">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f5018279e140310583219913201d5184.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-95eb2a9b36bde144774b751fc34f44a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc8153f3671aa11c8cb7d3223f13b24b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GBT-SAM-Adapting-a-Foundational-Deep-Learning-Model-for-Generalizable-Brain-Tumor-Segmentation-via-Efficient-Integration-of-Multi-Parametric-MRI-Data"><a href="#GBT-SAM-Adapting-a-Foundational-Deep-Learning-Model-for-Generalizable-Brain-Tumor-Segmentation-via-Efficient-Integration-of-Multi-Parametric-MRI-Data" class="headerlink" title="GBT-SAM: Adapting a Foundational Deep Learning Model for Generalizable   Brain Tumor Segmentation via Efficient Integration of Multi-Parametric MRI   Data"></a>GBT-SAM: Adapting a Foundational Deep Learning Model for Generalizable   Brain Tumor Segmentation via Efficient Integration of Multi-Parametric MRI   Data</h2><p><strong>Authors:Cecilia Diana-Albelda, Roberto Alcover-Couso, Ãlvaro GarcÃ­a-MartÃ­n, Jesus Bescos, Marcos Escudero-ViÃ±olo</strong></p>
<p>Gliomas are aggressive brain tumors that require accurate imaging-based diagnosis, with segmentation playing a critical role in evaluating morphology and treatment decisions. Manual delineation of gliomas is time-consuming and prone to variability, motivating the use of deep learning to improve consistency and alleviate clinical workload. However, existing methods often fail to fully exploit the information available in multi-parametric MRI (mp-MRI), particularly inter-slice contextual features, and typically require considerable computational resources while lacking robustness across tumor type variations. We present GBT-SAM, a parameter-efficient deep learning framework that adapts the Segment Anything Model (SAM), a large-scale vision model, to volumetric mp-MRI data. GBT-SAM reduces input complexity by selecting fewer than 2.6% of slices per scan while incorporating all four MRI modalities, preserving essential tumor-related information with minimal cost. Furthermore, our model is trained by a two-step fine-tuning strategy that incorporates a depth-aware module to capture inter-slice correlations and lightweight adaptation layers, resulting in just 6.5M trainable parameters, which is the lowest among SAM-based approaches. GBT-SAM achieves a Dice Score of 93.54 on the BraTS Adult Glioma dataset and demonstrates robust performance on Meningioma, Pediatric Glioma, and Sub-Saharan Glioma datasets. These results highlight GBT-SAMâ€™s potential as a computationally efficient and domain-robust framework for brain tumor segmentation using mp-MRI. Our code and models are available at <a target="_blank" rel="noopener" href="https://github.com/vpulab/med-sam-brain">https://github.com/vpulab/med-sam-brain</a> . </p>
<blockquote>
<p>èƒ¶è´¨ç˜¤æ˜¯ä¾µè¢­æ€§çš„è„‘è‚¿ç˜¤ï¼Œéœ€è¦åŸºäºå‡†ç¡®çš„æˆåƒè¿›è¡Œè¯Šæ–­ï¼Œå…¶ä¸­åˆ†å‰²åœ¨è¯„ä¼°å½¢æ€å’Œæ²»æ²»ç–—å†³ç­–ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚æ‰‹åŠ¨æç»˜èƒ¶è´¨ç˜¤æ—¢è€—æ—¶åˆå®¹æ˜“äº§ç”Ÿåå·®ï¼Œå› æ­¤ä¿ƒä½¿ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¥æé«˜ä¸€è‡´æ€§å¹¶å‡è½»ä¸´åºŠå·¥ä½œé‡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æœªèƒ½å……åˆ†åˆ©ç”¨å¤šå‚æ•°MRIï¼ˆmp-MRIï¼‰ä¸­çš„å¯ç”¨ä¿¡æ¯ï¼Œå°¤å…¶æ˜¯åˆ‡ç‰‡é—´çš„ä¸Šä¸‹æ–‡ç‰¹å¾ï¼Œå¹¶ä¸”é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œè€Œä¸”åœ¨è‚¿ç˜¤ç±»å‹å˜åŒ–æ–¹é¢ç¼ºä¹ç¨³å¥æ€§ã€‚æˆ‘ä»¬æå‡ºäº†GBT-SAMï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé€‚åº”äº†å¤§è§„æ¨¡è§†è§‰æ¨¡å‹çš„Segment Anything Modelï¼ˆSAMï¼‰åˆ°ä½“ç§¯mp-MRIæ•°æ®ã€‚GBT-SAMé€šè¿‡é€‰æ‹©æ¯æ‰«æå°‘äº2.6%çš„åˆ‡ç‰‡æ¥é™ä½è¾“å…¥å¤æ‚æ€§ï¼ŒåŒæ—¶ç»“åˆæ‰€æœ‰å››ç§MRIæ¨¡å¼ï¼Œä»¥æœ€ä½çš„æˆæœ¬ä¿ç•™ä¸è‚¿ç˜¤ç›¸å…³çš„åŸºæœ¬ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é€šè¿‡ä¸¤æ­¥å¾®è°ƒç­–ç•¥è¿›è¡Œè®­ç»ƒï¼Œè¯¥ç­–ç•¥ç»“åˆäº†æ·±åº¦æ„ŸçŸ¥æ¨¡å—æ¥æ•è·åˆ‡ç‰‡é—´çš„ç›¸å…³æ€§ä»¥åŠè½»é‡çº§é€‚åº”å±‚ï¼Œä½¿å¾—å¯è®­ç»ƒçš„å‚æ•°ä»…ä¸º650ä¸‡ï¼Œæ˜¯SAMæ–¹æ³•ä¸­å‚æ•°æœ€å°‘çš„ä¸€ç§ã€‚GBT-SAMåœ¨BraTSæˆäººèƒ¶è´¨ç˜¤æ•°æ®é›†ä¸Šå–å¾—äº†93.54çš„Diceå¾—åˆ†ï¼Œå¹¶åœ¨è„‘è†œç˜¤ã€å„¿ç«¥èƒ¶è´¨ç˜¤å’Œæ’’å“ˆæ‹‰ä»¥å—èƒ¶è´¨ç˜¤æ•°æ®é›†ä¸Šè¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœçªå‡ºäº†GBT-SAMä½œä¸ºä½¿ç”¨mp-MRIè¿›è¡Œè„‘è‚¿ç˜¤åˆ†å‰²çš„è®¡ç®—é«˜æ•ˆä¸”ç¨³å¥çš„æ¡†æ¶çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/vpulab/med-sam-brain%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/vpulab/med-sam-brainä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04325v3">PDF</a> </p>
<p><strong>Summary</strong><br>    è¯¥ç ”ç©¶æå‡ºä¸€ç§å‚æ•°æ•ˆç‡é«˜çš„æ·±åº¦å­¦ä¹ æ¡†æ¶GBT-SAMï¼ŒåŸºäºSegment Anything Modelï¼ˆSAMï¼‰ï¼Œåº”ç”¨äºä½“ç§¯å‹å¤šå‚æ•°MRIï¼ˆmp-MRIï¼‰æ•°æ®ã€‚GBT-SAMå‡å°‘æ‰«æåˆ‡ç‰‡æ•°é‡è‡³æ¯æ‰«æå°‘äº2.6%ï¼ŒåŒæ—¶ç»“åˆæ‰€æœ‰å››ç§MRIæ¨¡å¼ï¼Œä»¥æœ€å°çš„æˆæœ¬ä¿ç•™å…³é”®çš„è‚¿ç˜¤ç›¸å…³ä¿¡æ¯ã€‚é€šè¿‡ä¸¤æ­¥å¾®è°ƒç­–ç•¥å’Œæ·±åº¦æ„ŸçŸ¥æ¨¡å—ï¼ŒGBT-SAMå®ç°äº†åœ¨BraTSæˆäººèƒ¶è´¨ç˜¤æ•°æ®é›†ä¸Šçš„Dice Scoreä¸º93.54%ï¼Œå¹¶åœ¨è„‘è†œç˜¤ã€å„¿ç«¥èƒ¶è´¨ç˜¤å’Œæ’’å“ˆæ‹‰ä»¥å—èƒ¶è´¨ç˜¤æ•°æ®é›†ä¸Šè¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>1.Gliomaséœ€è¦åŸºäºæˆåƒçš„ç²¾ç¡®è¯Šæ–­ï¼Œåˆ†å‰²åœ¨è¯„ä¼°å½¢æ€å’Œæ²»ç–—å†³ç­–ä¸­èµ·å…³é”®ä½œç”¨ã€‚<br>2.æ‰‹åŠ¨å‹¾å‹’èƒ¶è´¨ç˜¤è€—æ—¶ä¸”æ˜“å‡ºé”™ï¼Œæ·±åº¦å­¦ä¹ å¯æ”¹å–„ä¸€è‡´æ€§å’Œå‡è½»ä¸´åºŠå·¥ä½œé‡ã€‚<br>3.ç°æœ‰æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨å¤šå‚æ•°MRIï¼ˆmp-MRIï¼‰çš„ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ‡ç‰‡é—´çš„ä¸Šä¸‹æ–‡ç‰¹å¾æ–¹é¢ã€‚<br>4.GBT-SAMæ˜¯ä¸€ä¸ªå‚æ•°é«˜æ•ˆçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ŒåŸºäºSAMæ¨¡å‹åº”ç”¨äºä½“ç§¯å‹mp-MRIæ•°æ®ã€‚<br>5.GBT-SAMå‡å°‘æ‰«æåˆ‡ç‰‡æ•°é‡ï¼ŒåŒæ—¶ç»“åˆæ‰€æœ‰MRIæ¨¡å¼ä»¥ä¿ç•™å…³é”®è‚¿ç˜¤ä¿¡æ¯ã€‚<br>6.GBT-SAMé€šè¿‡ä¸¤æ­¥å¾®è°ƒç­–ç•¥å’Œæ·±åº¦æ„ŸçŸ¥æ¨¡å—å®ç°é«˜æ€§èƒ½ï¼Œåœ¨BraTSæˆäººèƒ¶è´¨ç˜¤æ•°æ®é›†ä¸Šçš„Dice Scoreä¸º93.54%ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04325">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-31911c8daac1bf42013659e9e7966c34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-80d8e3e52d9b1bd8ba1610f2c0eee38a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-839d4291f409975364b8e3f52af976ec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5644b0de7c5ac5d343ef027f8235d41f.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Ptychographic-Image-Reconstruction-from-Limited-Data-via-Score-Based-Diffusion-Models-with-Physics-Guidance"><a href="#Ptychographic-Image-Reconstruction-from-Limited-Data-via-Score-Based-Diffusion-Models-with-Physics-Guidance" class="headerlink" title="Ptychographic Image Reconstruction from Limited Data via Score-Based   Diffusion Models with Physics-Guidance"></a>Ptychographic Image Reconstruction from Limited Data via Score-Based   Diffusion Models with Physics-Guidance</h2><p><strong>Authors:Refik Mert Cam, Junjing Deng, Rajkumar Kettimuthu, Mathew J. Cherukara, Tekin Bicer</strong></p>
<p>Ptychography is a data-intensive computational imaging technique that achieves high spatial resolution over large fields of view. The technique involves scanning a coherent beam across overlapping regions and recording diffraction patterns. Conventional reconstruction algorithms require substantial overlap, increasing data volume and experimental time, reaching PiB-scale experimental data and weeks to month-long data acquisition times. To address this, we propose a reconstruction method employing a physics-guided score-based diffusion model. Our approach trains a diffusion model on representative object images to learn an object distribution prior. During reconstruction, we modify the reverse diffusion process to enforce data consistency, guiding reverse diffusion toward a physically plausible solution. This method requires a single pretraining phase, allowing it to generalize across varying scan overlap ratios and positions. Our results demonstrate that the proposed method achieves high-fidelity reconstructions with only a 20% overlap, while the widely employed rPIE method requires a 62% overlap to achieve similar accuracy. This represents a significant reduction in data requirements, offering an alternative to conventional techniques. </p>
<blockquote>
<p>Ptychographyæ˜¯ä¸€ç§æ•°æ®å¯†é›†å‹çš„è®¡ç®—æˆåƒæŠ€æœ¯ï¼Œå¯ä»¥åœ¨å¤§è§†é‡èŒƒå›´å†…å®ç°é«˜ç©ºé—´åˆ†è¾¨ç‡ã€‚è¯¥æŠ€æœ¯æ¶‰åŠåœ¨é‡å åŒºåŸŸæ‰«æç›¸å¹²å…‰æŸå¹¶è®°å½•è¡å°„å›¾æ¡ˆã€‚ä¼ ç»Ÿçš„é‡å»ºç®—æ³•éœ€è¦å¤§é‡çš„é‡å ï¼Œè¿™å¢åŠ äº†æ•°æ®é‡å’Œå®éªŒæ—¶é—´ï¼Œè¾¾åˆ°PiBè§„æ¨¡çš„å®éªŒæ•°æ®å’Œé•¿è¾¾æ•°å‘¨è‡³æ•°æœˆçš„æ•°æ®é‡‡é›†æ—¶é—´ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é‡‡ç”¨ç‰©ç†å¼•å¯¼è¯„åˆ†æ‰©æ•£æ¨¡å‹çš„é‡å»ºæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åœ¨å…·æœ‰ä»£è¡¨æ€§çš„å¯¹è±¡å›¾åƒä¸Šè®­ç»ƒæ‰©æ•£æ¨¡å‹æ¥å­¦ä¹ å¯¹è±¡åˆ†å¸ƒå…ˆéªŒã€‚åœ¨é‡å»ºè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¿®æ”¹åå‘æ‰©æ•£è¿‡ç¨‹ä»¥å¼ºåˆ¶æ‰§è¡Œæ•°æ®ä¸€è‡´æ€§ï¼Œå¼•å¯¼åå‘æ‰©æ•£æœç€ç‰©ç†å¯è¡Œè§£è¿›è¡Œã€‚è¯¥æ–¹æ³•ä»…éœ€ä¸€ä¸ªé¢„è®­ç»ƒé˜¶æ®µï¼Œå¯é’ˆå¯¹å„ç§æ‰«æé‡å ç‡å’Œä½ç½®è¿›è¡Œæ¨å¹¿ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨ä»…20%çš„é‡å æƒ…å†µä¸‹å®ç°äº†é«˜ä¿çœŸåº¦çš„é‡å»ºï¼Œè€Œå¹¿æ³›ä½¿ç”¨çš„rPIEæ–¹æ³•åˆ™éœ€è¦62%çš„é‡å æ‰èƒ½è¾¾åˆ°ç±»ä¼¼çš„å‡†ç¡®æ€§ã€‚è¿™ä»£è¡¨äº†æ•°æ®éœ€æ±‚çš„æ˜¾è‘—å‡å°‘ï¼Œä¸ºä¼ ç»ŸæŠ€æœ¯æä¾›äº†æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18767v2">PDF</a> Preprint submitted to IEEE MLSP 2025</p>
<p><strong>Summary</strong><br>     åŸºäºç‰©ç†æ¨¡å‹çš„æ‰©æ•£æ¨¡å‹åœ¨å‡å°‘æ‰«æé‡å åº¦çš„è¦æ±‚ä¸‹å®ç°äº†é«˜æ•ˆçš„Ptychographyæˆåƒé‡å»ºã€‚é€šè¿‡è®­ç»ƒæ‰©æ•£æ¨¡å‹å­¦ä¹ ç‰©ä½“åˆ†å¸ƒå…ˆéªŒï¼Œåˆ©ç”¨åå‘æ‰©æ•£è¿‡ç¨‹è¾¾åˆ°é«˜ä¿çœŸé‡å»ºçš„æ•ˆæœã€‚ä¸éœ€è¦æ›´é«˜é‡å åº¦çš„ä¼ ç»ŸæŠ€æœ¯ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•æ›´åŠ é«˜æ•ˆï¼Œå¤§å¹…é™ä½å®éªŒæ—¶é—´å’Œæ•°æ®é‡éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ptychographyæ˜¯ä¸€ç§è®¡ç®—æˆåƒæŠ€æœ¯ï¼Œå…·æœ‰é«˜ç©ºé—´åˆ†è¾¨ç‡å’Œå¤§è§†é‡çš„ç‰¹ç‚¹ã€‚</li>
<li>ä¼ ç»Ÿé‡å»ºç®—æ³•éœ€è¦å¤§é‡é‡å åŒºåŸŸï¼Œå¢åŠ äº†å®éªŒæ—¶é—´å’Œæ•°æ®é‡éœ€æ±‚ã€‚</li>
<li>åŸºäºç‰©ç†æ¨¡å‹çš„æ‰©æ•£æ¨¡å‹è¢«åº”ç”¨äºPtychographyæˆåƒé‡å»ºä¸­ã€‚</li>
<li>è®­ç»ƒæ‰©æ•£æ¨¡å‹å­¦ä¹ ç‰©ä½“åˆ†å¸ƒå…ˆéªŒä¿¡æ¯ä»¥ä¼˜åŒ–é‡å»ºè¿‡ç¨‹ã€‚</li>
<li>ä¿®æ”¹åå‘æ‰©æ•£è¿‡ç¨‹ä»¥å®ç°æ•°æ®ä¸€è‡´æ€§ï¼Œå‘ç‰©ç†ä¸Šåˆç†çš„ç»“æœå¼•å¯¼ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•ä»…éœ€è¦è¾ƒä½çš„é‡å åº¦ï¼ˆ20%ï¼‰å³å¯å®ç°é«˜ä¿çœŸé‡å»ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18767">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-26d4d96e9e9d4a382003bfa6e3761ea1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef4eeb1bbbbc7f1933c5275369ba5304.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73247140ab45f0453ba26a05692744ae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-18ffe276aaa83bb9cbcbd91e50198bd7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3a898b4cdf5c37b519be10fc401c15c.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Can-open-source-large-language-models-be-used-for-tumor-documentation-in-Germany-â€“-An-evaluation-on-urological-doctorsâ€™-notes"><a href="#Can-open-source-large-language-models-be-used-for-tumor-documentation-in-Germany-â€“-An-evaluation-on-urological-doctorsâ€™-notes" class="headerlink" title="Can open source large language models be used for tumor documentation in   Germany? â€“ An evaluation on urological doctorsâ€™ notes"></a>Can open source large language models be used for tumor documentation in   Germany? â€“ An evaluation on urological doctorsâ€™ notes</h2><p><strong>Authors:Stefan Lenz, Arsenij Ustjanzew, Marco Jeray, Meike Ressing, Torsten Panholzer</strong></p>
<p>Tumor documentation in Germany is largely done manually, requiring reading patient records and entering data into structured databases. Large language models (LLMs) could potentially enhance this process by improving efficiency and reliability. This evaluation tests eleven different open source LLMs with sizes ranging from 1-70 billion model parameters on three basic tasks of the tumor documentation process: identifying tumor diagnoses, assigning ICD-10 codes, and extracting the date of first diagnosis. For evaluating the LLMs on these tasks, a dataset of annotated text snippets based on anonymized doctorsâ€™ notes from urology was prepared. Different prompting strategies were used to investigate the effect of the number of examples in few-shot prompting and to explore the capabilities of the LLMs in general. The models Llama 3.1 8B, Mistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks. Models with less extensive training data or having fewer than 7 billion parameters showed notably lower performance, while larger models did not display performance gains. Examples from a different medical domain than urology could also improve the outcome in few-shot prompting, which demonstrates the ability of LLMs to handle tasks needed for tumor documentation. Open source LLMs show a strong potential for automating tumor documentation. Models from 7-12 billion parameters could offer an optimal balance between performance and resource efficiency. With tailored fine-tuning and well-designed prompting, these models might become important tools for clinical documentation in the future. The code for the evaluation is available from <a target="_blank" rel="noopener" href="https://github.com/stefan-m-lenz/UroLlmEval">https://github.com/stefan-m-lenz/UroLlmEval</a>. We also release the dataset as a new valuable resource that addresses the shortage of authentic and easily accessible benchmarks in German-language medical NLP. </p>
<blockquote>
<p>åœ¨å¾·å›½ï¼Œè‚¿ç˜¤è®°å½•å·¥ä½œå¤§å¤šä»¥æ‰‹åŠ¨æ–¹å¼è¿›è¡Œï¼Œéœ€è¦é˜…è¯»æ‚£è€…ç—…å†å¹¶å°†æ•°æ®è¾“å…¥ç»“æ„åŒ–æ•°æ®åº“ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ‰æœ›é€šè¿‡æé«˜æ•ˆç‡å¯é æ€§å¢å¼ºè¿™ä¸€è¿‡ç¨‹ã€‚æœ¬æ¬¡è¯„ä¼°å¯¹ä¸‰ç§è‚¿ç˜¤è®°å½•åŸºæœ¬ä»»åŠ¡ï¼ˆè¯†åˆ«è‚¿ç˜¤è¯Šæ–­ã€åˆ†é…ICD-10ä»£ç å’Œæå–é¦–æ¬¡è¯Šæ–­æ—¥æœŸï¼‰ä¸Šï¼Œæµ‹è¯•äº†è§„æ¨¡åœ¨1äº¿è‡³70äº¿æ¨¡å‹å‚æ•°ä¹‹é—´çš„11ç§ä¸åŒå¼€æºLLMsã€‚ä¸ºäº†è¯„ä¼°è¿™äº›LLMsåœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œæˆ‘ä»¬å‡†å¤‡äº†ä¸€ä»½åŸºäºæ³Œå°¿ç§‘åŒ¿ååŒ»ç”Ÿç¬”è®°çš„æ ‡æ³¨æ–‡æœ¬ç‰‡æ®µæ•°æ®é›†ã€‚ä½¿ç”¨äº†ä¸åŒçš„æç¤ºç­–ç•¥æ¥æ¢ç©¶å°‘é‡æç¤ºä¸­ç¤ºä¾‹æ•°é‡å¯¹LLMsèƒ½åŠ›çš„å½±å“ï¼Œå¹¶æ¢ç´¢LLMsçš„ä¸€èˆ¬èƒ½åŠ›ã€‚Llama 3.1 8Bã€Mistral 7Bå’ŒMistral NeMo 12Bç­‰æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚å…·æœ‰è¾ƒå°‘è®­ç»ƒæ•°æ®æˆ–å‚æ•°å°‘äº7äº¿çš„æ¨¡å‹è¡¨ç°æ˜æ˜¾è¾ƒå·®ï¼Œè€Œå¤§å‹æ¨¡å‹å¹¶æœªæ˜¾ç¤ºå‡ºæ€§èƒ½æå‡ã€‚æ¥è‡ªä¸åŒäºæ³Œå°¿ç§‘çš„åŒ»å­¦é¢†åŸŸçš„ä¾‹å­ä¹Ÿèƒ½åœ¨å°‘é‡æç¤ºä¸­æ”¹å–„ç»“æœï¼Œè¿™è¯æ˜äº†LLMså¤„ç†è‚¿ç˜¤è®°å½•æ‰€éœ€ä»»åŠ¡çš„èƒ½åŠ›ã€‚å¼€æºLLMsåœ¨è‡ªåŠ¨è‚¿ç˜¤è®°å½•æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚å…·æœ‰7-12äº¿å‚æ•°çš„æ¨¡å‹å¯èƒ½åœ¨æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¹‹é—´æä¾›æœ€ä½³å¹³è¡¡ã€‚é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒå’Œè‰¯å¥½çš„æç¤ºè®¾è®¡ï¼Œè¿™äº›æ¨¡å‹å¯èƒ½æˆä¸ºæœªæ¥ä¸´åºŠè®°å½•çš„é‡è¦å·¥å…·ã€‚è¯„ä¼°çš„ä»£ç å¯ä»<a target="_blank" rel="noopener" href="https://github.com/stefan-m-lenz/UroLlmEval%E8%8E%B7%E5%8F%96%E3%80%82%E6%88%91%E4%BB%AC%E8%BF%98%E5%85%AC%E5%BC%80%E4%BA%86%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E4%BD%9C%E4%B8%BA%E8%A7%A3%E5%86%B3%E5%BE%B7%E5%9B%BD%E5%8C%BB%E5%AD%A6NLP%E9%A2%86%E5%9F%9F%E4%B8%AD%E7%9C%9F%E5%AE%9E%E3%80%81%E6%98%93%E4%BA%8E%E8%AE%BF%E9%97%AE%E7%9A%84%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E8%B5%84%E6%BA%90%E7%9F%AD%E7%BC%BA%E7%9A%84%E6%96%B0%E6%9C%89%E4%BB%B7%E5%80%BC%E8%B5%84%E6%BA%90%E3%80%82">https://github.com/stefan-m-lenz/UroLlmEvalè·å–ã€‚æˆ‘ä»¬è¿˜å…¬å¼€äº†æ•°æ®é›†ï¼Œä½œä¸ºè§£å†³å¾·å›½åŒ»å­¦NLPé¢†åŸŸä¸­çœŸå®ã€æ˜“äºè®¿é—®çš„åŸºå‡†æµ‹è¯•èµ„æºçŸ­ç¼ºçš„æ–°æœ‰ä»·å€¼èµ„æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12106v3">PDF</a> 53 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æœ¬ä»‹ç»äº†å¾·å›½è‚¿ç˜¤è®°å½•çš„ç°çŠ¶ï¼Œä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨åŒ–è‚¿ç˜¤è®°å½•è¿‡ç¨‹ä¸­çš„æ½œåŠ›ã€‚é€šè¿‡å¯¹11ç§ä¸åŒè§„æ¨¡ï¼ˆä»1äº¿åˆ°70äº¿å‚æ•°ï¼‰çš„å¼€æºLLMsè¿›è¡Œè¯„ä»·ï¼Œå‘ç°å®ƒä»¬åœ¨è‚¿ç˜¤è¯Šæ–­è¯†åˆ«ã€ICD-10ä»£ç åˆ†é…å’Œé¦–æ¬¡è¯Šæ–­æ—¥æœŸæå–ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚æ¨¡å‹å‚æ•°åœ¨7-12äº¿ä¹‹é—´çš„æ¨¡å‹åœ¨æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†å¹³è¡¡ã€‚é€šè¿‡ç²¾ç»†è°ƒæ•´å’Œç²¾å¿ƒè®¾è®¡æç¤ºï¼Œè¿™äº›æ¨¡å‹æœ‰æœ›åœ¨æœªæ¥æˆä¸ºä¸´åºŠæ–‡æ¡£ç®¡ç†çš„é‡è¦å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‚¿ç˜¤è®°å½•åœ¨å¾·å›½ä¸»è¦ä¾èµ–æ‰‹åŠ¨å¤„ç†ï¼Œéœ€è¦é˜…è¯»æ‚£è€…è®°å½•å’Œå°†æ•°æ®è¾“å…¥ç»“æ„åŒ–æ•°æ®åº“ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ‰æ½œåŠ›æé«˜è‚¿ç˜¤è®°å½•çš„æ•ˆç‡å’Œå¯é æ€§ã€‚</li>
<li>åœ¨ä¸‰é¡¹åŸºæœ¬ä»»åŠ¡ï¼ˆè¯†åˆ«è‚¿ç˜¤è¯Šæ–­ã€åˆ†é…ICD-10ä»£ç å’Œæå–é¦–æ¬¡è¯Šæ–­æ—¥æœŸï¼‰ä¸­è¯„ä¼°äº†11ç§ä¸åŒè§„æ¨¡å’Œç±»å‹çš„å¼€æºLLMsã€‚</li>
<li>Llama 3.1 8Bã€Mistral 7Bå’ŒMistral NeMo 12Bç­‰æ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
<li>æ¨¡å‹å‚æ•°åœ¨7-12äº¿ä¹‹é—´çš„æ¨¡å‹åœ¨æ€§èƒ½å’Œèµ„æºæ•ˆç‡ä¹‹é—´è¾¾åˆ°å¹³è¡¡ã€‚</li>
<li>é€šè¿‡ç²¾ç»†è°ƒæ•´å’Œç²¾å¿ƒè®¾è®¡æç¤ºï¼Œè¿™äº›æ¨¡å‹å¯ä»¥åœ¨æœªæ¥çš„ä¸´åºŠæ–‡æ¡£ç®¡ç†ä¸­å‘æŒ¥é‡è¦ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12106">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f5b2171a7c546253d3782efe165a317.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec1b13367880e573c14047c7e811034b.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="A-Classification-Benchmark-for-Artificial-Intelligence-Detection-of-Laryngeal-Cancer-from-Patient-Voice"><a href="#A-Classification-Benchmark-for-Artificial-Intelligence-Detection-of-Laryngeal-Cancer-from-Patient-Voice" class="headerlink" title="A Classification Benchmark for Artificial Intelligence Detection of   Laryngeal Cancer from Patient Voice"></a>A Classification Benchmark for Artificial Intelligence Detection of   Laryngeal Cancer from Patient Voice</h2><p><strong>Authors:Mary Paterson, James Moor, Luisa Cutillo</strong></p>
<p>Cases of laryngeal cancer are predicted to rise significantly in the coming years. Current diagnostic pathways are inefficient, putting undue stress on both patients and the medical system. Artificial intelligence offers a promising solution by enabling non-invasive detection of laryngeal cancer from patient voice, which could help prioritise referrals more effectively. A major barrier in this field is the lack of reproducible methods. Our work addresses this challenge by introducing a benchmark suite comprising 36 models trained and evaluated on open-source datasets. These models classify patients with benign and malignant voice pathologies. All models are accessible in a public repository, providing a foundation for future research. We evaluate three algorithms and three audio feature sets, including both audio-only inputs and multimodal inputs incorporating demographic and symptom data. Our best model achieves a balanced accuracy of 83.7%, sensitivity of 84.0%, specificity of 83.3%, and AUROC of 91.8%. </p>
<blockquote>
<p>å–‰ç™Œç—…ä¾‹é¢„è®¡å°†åœ¨æœªæ¥å‡ å¹´æ˜¾è‘—å¢åŠ ã€‚å½“å‰çš„è¯Šæ–­é€”å¾„æ•ˆç‡ä½ä¸‹ï¼Œç»™æ‚£è€…å’ŒåŒ»ç–—ç³»ç»Ÿå¸¦æ¥äº†ä¸å¿…è¦çš„å‹åŠ›ã€‚äººå·¥æ™ºèƒ½é€šè¿‡å®ç°é€šè¿‡æ‚£è€…å£°éŸ³è¿›è¡Œå–‰ç™Œæ— åˆ›æ£€æµ‹ï¼Œä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œè¿™æœ‰åŠ©äºæ›´æœ‰æ•ˆåœ°ä¼˜å…ˆå¤„ç†è½¬è¯Šäº‹å®œã€‚è¯¥é¢†åŸŸçš„ä¸€ä¸ªä¸»è¦éšœç¢æ˜¯ç¼ºä¹å¯é‡å¤çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„å·¥ä½œé€šè¿‡å¼•å…¥ä¸€ä¸ªåŒ…å«36ä¸ªæ¨¡å‹çš„åŸºå‡†æµ‹è¯•å¥—ä»¶æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¿™äº›æ¨¡å‹åœ¨å¼€æºæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚è¿™äº›æ¨¡å‹å¯¹è‰¯æ€§æˆ–æ¶æ€§è¯­éŸ³ç—…ç†æ‚£è€…è¿›è¡Œåˆ†ç±»ã€‚æ‰€æœ‰æ¨¡å‹éƒ½å¯åœ¨å…¬å…±å­˜å‚¨åº“ä¸­è®¿é—®ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†åŸºç¡€ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸‰ç§ç®—æ³•å’Œä¸‰ç§éŸ³é¢‘ç‰¹å¾é›†ï¼ŒåŒ…æ‹¬ä»…ä½¿ç”¨éŸ³é¢‘è¾“å…¥å’Œç»“åˆäººå£ç»Ÿè®¡æ•°æ®å’Œç—‡çŠ¶æ•°æ®çš„å¤šåª’ä½“è¾“å…¥ã€‚æˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹å®ç°äº†83.7%çš„å¹³è¡¡å‡†ç¡®ç‡ã€84.0%çš„çµæ•åº¦ã€83.3%çš„ç‰¹å¼‚æ€§å’Œ91.8%çš„AUROCå€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16267v2">PDF</a> 16 pages, 6 figures, 10 tables</p>
<p><strong>Summary</strong><br>     æœªæ¥å–‰ç™Œç—…ä¾‹é¢„è®¡æ˜¾è‘—ä¸Šå‡ï¼Œå½“å‰è¯Šæ–­æµç¨‹ä½æ•ˆï¼Œç»™æ‚£è€…å’ŒåŒ»ç–—ç³»ç»Ÿå¸¦æ¥å‹åŠ›ã€‚äººå·¥æ™ºèƒ½å¯é€šè¿‡æ‚£è€…å£°éŸ³éä¾µå…¥æ€§æ£€æµ‹å–‰ç™Œï¼Œæœ‰æ•ˆä¼˜åŒ–è½¬è¯Šä¼˜å…ˆçº§ã€‚æœ¬ç ”ç©¶è§£å†³äº†é¢†åŸŸå†…çš„é‡å¤§æŒ‘æˆ˜ï¼Œå³ç¼ºä¹å¯é‡å¤çš„æ–¹æ³•ã€‚æˆ‘ä»¬æ¨å‡ºäº†ä¸€å¥—åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼ŒåŒ…å«36ä¸ªåœ¨å¼€æºæ•°æ®é›†ä¸Šè®­ç»ƒå’Œè¯„ä¼°çš„æ¨¡å‹ï¼Œç”¨äºåˆ†ç±»è‰¯æ€§å’Œæ¶æ€§è¯­éŸ³ç—…ç†æ‚£è€…ã€‚æ‰€æœ‰æ¨¡å‹å‡å¯åœ¨å…¬å…±ä»“åº“ä¸­è®¿é—®ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æ‰“ä¸‹åŸºç¡€ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸‰ç§ç®—æ³•å’Œä¸‰ç§éŸ³é¢‘ç‰¹å¾é›†ï¼ŒåŒ…æ‹¬ä»…éŸ³é¢‘è¾“å…¥å’Œç»“åˆäººå£ç»Ÿè®¡å­¦å’Œç—‡çŠ¶æ•°æ®çš„å¤šæ¨¡å¼è¾“å…¥ã€‚æœ€ä½³æ¨¡å‹è¾¾åˆ°83.7%çš„å¹³è¡¡å‡†ç¡®ç‡ï¼Œæ•æ„Ÿåº¦ä¸º84.0%ï¼Œç‰¹å¼‚åº¦ä¸º83.3%ï¼ŒAUROCä¸º91.8%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å–‰ç™Œç—…ä¾‹é¢„è®¡åœ¨æœªæ¥å‡ å¹´æ˜¾è‘—ä¸Šå‡ã€‚</li>
<li>å½“å‰è¯Šæ–­å–‰ç™Œçš„æ–¹æ³•å­˜åœ¨æ•ˆç‡é—®é¢˜ã€‚</li>
<li>äººå·¥æ™ºèƒ½ä¸ºå–‰ç™Œçš„éä¾µå…¥æ€§æ£€æµ‹æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å…¬å…±ä»“åº“ä¸­æä¾›äº†ä¸€å¥—åŒ…å«36ä¸ªæ¨¡å‹çš„åŸºå‡†æµ‹è¯•å¥—ä»¶ã€‚</li>
<li>è¿™äº›æ¨¡å‹ç”¨äºåˆ†ç±»è‰¯æ€§å’Œæ¶æ€§è¯­éŸ³ç—…ç†æ‚£è€…ã€‚</li>
<li>æœ€ä½³æ¨¡å‹çš„å‡†ç¡®ç‡ã€æ•æ„Ÿåº¦å’Œç‰¹å¼‚åº¦è¡¨ç°è‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16267">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d22d5adb18c5cc7502e47b9a03668f9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1ede9b85e5588865e10c791dceb9da8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd376f7f74f5a771d56a967b29d28059.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-add15a7a831f94ed2b381b8d35bc5453.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c494d2945776a7a9359987bc0c736ffd.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Breast-Cancer-Histopathology-Classification-using-CBAM-EfficientNetV2-with-Transfer-Learning"><a href="#Breast-Cancer-Histopathology-Classification-using-CBAM-EfficientNetV2-with-Transfer-Learning" class="headerlink" title="Breast Cancer Histopathology Classification using CBAM-EfficientNetV2   with Transfer Learning"></a>Breast Cancer Histopathology Classification using CBAM-EfficientNetV2   with Transfer Learning</h2><p><strong>Authors:Naren Sengodan</strong></p>
<p>Breast cancer histopathology image classification is critical for early detection and improved patient outcomes. 1 This study introduces a novel approach leveraging EfficientNetV2 models, to improve feature extraction and focus on relevant tissue regions. The proposed models were evaluated on the BreakHis dataset across multiple magnification scales (40X, 100X, 200X, and 400X). 2 Among them, the EfficientNetV2-XL with CBAM achieved outstanding performance, reaching a peak accuracy of 99.01 percent and an F1-score of 98.31 percent at 400X magnification, outperforming state-of-the-art methods. 3 By integrating Contrast Limited Adaptive Histogram Equalization (CLAHE) for preprocessing and optimizing computational efficiency, this method demonstrates its suitability for real-time clinical deployment. 3 The results underscore the potential of attention-enhanced scalable architectures in advancing diagnostic precision for breast cancer detection. </p>
<blockquote>
<p>ä¹³è…ºç™Œç»„ç»‡ç—…ç†å­¦å›¾åƒåˆ†ç±»å¯¹æ—©æœŸæ£€æµ‹å’Œæ”¹å–„æ‚£è€…é¢„åè‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§åˆ©ç”¨EfficientNetV2æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œä»¥æé«˜ç‰¹å¾æå–å¹¶å…³æ³¨ç›¸å…³ç»„ç»‡åŒºåŸŸã€‚æ‰€æå‡ºçš„æ¨¡å‹åœ¨BreakHisæ•°æ®é›†ä¸Šçš„å¤šä¸ªæ”¾å¤§å€æ•°ï¼ˆ40Xã€100Xã€200Xå’Œ400Xï¼‰ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å…¶ä¸­ï¼Œå¸¦æœ‰CBAMçš„EfficientNetV2-XLè¡¨ç°çªå‡ºï¼Œåœ¨400Xæ”¾å¤§å€æ•°ä¸‹è¾¾åˆ°äº†99.01%çš„å³°å€¼å‡†ç¡®ç‡å’Œ98.31%çš„F1åˆ†æ•°ï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚é€šè¿‡é›†æˆå¯¹æ¯”åº¦å—é™è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–ï¼ˆCLAHEï¼‰è¿›è¡Œé¢„å¤„ç†å¹¶ä¼˜åŒ–è®¡ç®—æ•ˆç‡ï¼Œè¯¥æ–¹æ³•æ˜¾ç¤ºå‡ºé€‚ç”¨äºå®æ—¶ä¸´åºŠéƒ¨ç½²çš„é€‚ç”¨æ€§ã€‚ç»“æœå¼ºè°ƒäº†æ³¨æ„åŠ›å¢å¼ºå¯æ‰©å±•æ¶æ„åœ¨æé«˜ä¹³è…ºç™Œæ£€æµ‹è¯Šæ–­ç²¾åº¦æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.22392v6">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨EfficientNetV2æ¨¡å‹è¿›è¡Œä¹³è…ºç™Œç»„ç»‡ç—…ç†å­¦å›¾åƒåˆ†ç±»çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨BreakHisæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤šå°ºåº¦è¯„ä¼°ï¼Œè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚é€šè¿‡ç»“åˆCLAHEé¢„å¤„ç†æŠ€æœ¯å’Œä¼˜åŒ–è®¡ç®—æ•ˆç‡ï¼Œè¯¥æ–¹æ³•é€‚åˆå®æ—¶ä¸´åºŠéƒ¨ç½²ï¼Œæœ‰æœ›æé«˜ä¹³è…ºç™Œè¯Šæ–­çš„ç²¾ç¡®åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EfficientNetV2æ¨¡å‹è¢«ç”¨äºä¹³è…ºç™Œç»„ç»‡ç—…ç†å­¦å›¾åƒåˆ†ç±»ï¼Œä»¥æé«˜ç‰¹å¾æå–å’Œå…³æ³¨ç›¸å…³ç»„ç»‡åŒºåŸŸã€‚</li>
<li>åœ¨BreakHisæ•°æ®é›†ä¸Šï¼ŒEfficientNetV2-XLä¸CBAMç»“åˆçš„æ–¹æ³•è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨400Xæ”¾å¤§ç‡ä¸‹è¾¾åˆ°99.01%çš„å‡†ç¡®ç‡å’Œ98.31%çš„F1åˆ†æ•°ï¼Œä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>é€šè¿‡ç»“åˆCLAHEè¿›è¡Œé¢„å¤„ç†å’Œä¼˜åŒ–è®¡ç®—æ•ˆç‡ï¼Œè¯¥æ–¹æ³•é€‚åˆå®æ—¶ä¸´åºŠéƒ¨ç½²ã€‚</li>
<li>è¯¥ç ”ç©¶å¼ºè°ƒäº†æ³¨æ„å¢å¼ºå‹å¯æ‰©å±•æ¶æ„åœ¨æ”¹è¿›ä¹³è…ºç™Œæ£€æµ‹è¯Šæ–­ç²¾åº¦æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•æœ‰åŠ©äºæé«˜ä¹³è…ºç™Œçš„æ—©æœŸæ£€æµ‹ç‡ï¼Œè¿›è€Œæ”¹å–„æ‚£è€…çš„é¢„åã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.22392">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0cb40bc1e2a3da1de73d93f2a4d912f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4e7b750d428441307b3c85fed352d4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c710e2d98987b13516fc9ab4523e2c93.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ab3121ab55083d06844962c52d38a0ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-645632704fd97bb7bce146c077e7a60b.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="DEFT-Efficient-Fine-Tuning-of-Diffusion-Models-by-Learning-the-Generalised-h-transform"><a href="#DEFT-Efficient-Fine-Tuning-of-Diffusion-Models-by-Learning-the-Generalised-h-transform" class="headerlink" title="DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the   Generalised $h$-transform"></a>DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the   Generalised $h$-transform</h2><p><strong>Authors:Alexander Denker, Francisco Vargas, Shreyas Padhy, Kieran Didi, Simon Mathis, Vincent Dutordoir, Riccardo Barbano, Emile Mathieu, Urszula Julia Komorowska, Pietro Lio</strong></p>
<p>Generative modelling paradigms based on denoising diffusion processes have emerged as a leading candidate for conditional sampling in inverse problems. In many real-world applications, we often have access to large, expensively trained unconditional diffusion models, which we aim to exploit for improving conditional sampling. Most recent approaches are motivated heuristically and lack a unifying framework, obscuring connections between them. Further, they often suffer from issues such as being very sensitive to hyperparameters, being expensive to train or needing access to weights hidden behind a closed API. In this work, we unify conditional training and sampling using the mathematically well-understood Doobâ€™s h-transform. This new perspective allows us to unify many existing methods under a common umbrella. Under this framework, we propose DEFT (Doobâ€™s h-transform Efficient FineTuning), a new approach for conditional generation that simply fine-tunes a very small network to quickly learn the conditional $h$-transform, while keeping the larger unconditional network unchanged. DEFT is much faster than existing baselines while achieving state-of-the-art performance across a variety of linear and non-linear benchmarks. On image reconstruction tasks, we achieve speedups of up to 1.6$\times$, while having the best perceptual quality on natural images and reconstruction performance on medical images. Further, we also provide initial experiments on protein motif scaffolding and outperform reconstruction guidance methods. </p>
<blockquote>
<p>åŸºäºå»å™ªæ‰©æ•£è¿‡ç¨‹çš„ç”Ÿæˆå»ºæ¨¡èŒƒå¼å·²æˆä¸ºé€†é—®é¢˜ä¸­æ¡ä»¶é‡‡æ ·çš„é¢†å…ˆå€™é€‰æ–¹æ³•ã€‚åœ¨è®¸å¤šå®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬å¸¸å¸¸èƒ½å¤Ÿè®¿é—®å¤§å‹ä¸”è®­ç»ƒæˆæœ¬é«˜æ˜‚çš„æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬æ—¨åœ¨åˆ©ç”¨è¿™äº›æ¨¡å‹æ¥æ”¹å–„æ¡ä»¶é‡‡æ ·ã€‚è™½ç„¶æœ€è¿‘çš„æ–¹æ³•å¤§å¤šå—åˆ°å¯å‘å¹¶ç¼ºä¹ç»Ÿä¸€æ¡†æ¶çš„æ¨åŠ¨ï¼Œå¯¼è‡´å®ƒä»¬ä¹‹é—´çš„è”ç³»å˜å¾—æ¨¡ç³Šã€‚æ­¤å¤–ï¼Œå®ƒä»¬ç»å¸¸é¢ä¸´è¯¸å¦‚å¯¹è¶…å‚æ•°éå¸¸æ•æ„Ÿã€è®­ç»ƒæˆæœ¬é«˜æ˜‚æˆ–éœ€è¦è®¿é—®éšè—åœ¨å°é—­APIä¹‹åçš„æƒé‡ç­‰é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ•°å­¦ä¸Šç†è§£è‰¯å¥½çš„Doobçš„hå˜æ¢æ¥ç»Ÿä¸€æ¡ä»¶è®­ç»ƒå’Œé‡‡æ ·ã€‚è¿™ä¸ªæ–°è§†è§’å…è®¸æˆ‘ä»¬åœ¨ä¸€ä¸ªé€šç”¨æ¡†æ¶ä¸‹ç»Ÿä¸€è®¸å¤šç°æœ‰æ–¹æ³•ã€‚åœ¨è¯¥æ¡†æ¶ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†DEFTï¼ˆDoobçš„hå˜æ¢é«˜æ•ˆå¾®è°ƒï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¡ä»¶ç”Ÿæˆæ–¹æ³•ï¼Œå®ƒåªéœ€å¾®è°ƒä¸€ä¸ªéå¸¸å°çš„ç½‘ç»œæ¥å¿«é€Ÿå­¦ä¹ æ¡ä»¶hå˜æ¢ï¼ŒåŒæ—¶ä¿æŒè¾ƒå¤§çš„æ— æ¡ä»¶ç½‘ç»œä¸å˜ã€‚DEFTç›¸è¾ƒäºç°æœ‰åŸºå‡†æµ‹è¯•é€Ÿåº¦æ›´å¿«ï¼ŒåŒæ—¶åœ¨å„ç§çº¿æ€§å’Œéçº¿æ€§åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚åœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸Šï¼Œæˆ‘ä»¬å®ç°äº†é«˜è¾¾1.6å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶åœ¨è‡ªç„¶å›¾åƒä¸Šå…·æœ‰æœ€ä½³çš„æ„ŸçŸ¥è´¨é‡å’Œåœ¨åŒ»å­¦å›¾åƒä¸Šçš„é‡å»ºæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹è›‹ç™½è´¨åŠ¨æœºè„šæ‰‹æ¶è¿›è¡Œäº†åˆæ­¥å®éªŒå¹¶è¶…è¶Šäº†é‡å»ºæŒ‡å¯¼æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.01781v4">PDF</a> arXiv admin note: text overlap with arXiv:2312.09236</p>
<p><strong>Summary</strong><br>     åŸºäºå»å™ªæ‰©æ•£è¿‡ç¨‹çš„ç”Ÿæˆå»ºæ¨¡èŒƒå¼å·²æˆä¸ºè§£å†³åé—®é¢˜ä¸­æ¡ä»¶é‡‡æ ·çš„é¢†å…ˆå€™é€‰æ–¹æ³•ã€‚æœ¬æ–‡åˆ©ç”¨æ•°å­¦ä¸Šç†è§£è‰¯å¥½çš„Doobçš„h-å˜æ¢ç»Ÿä¸€äº†æ¡ä»¶è®­ç»ƒå’Œé‡‡æ ·ã€‚åœ¨æ­¤æ¡†æ¶ä¸‹ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡ä»¶ç”Ÿæˆæ–¹æ³•DEFTï¼ˆDoobçš„h-å˜æ¢é«˜æ•ˆå¾®è°ƒï¼‰ï¼Œå®ƒåªéœ€å¾®è°ƒä¸€ä¸ªéå¸¸å°çš„ç½‘ç»œå³å¯å¿«é€Ÿå­¦ä¹ æ¡ä»¶h-å˜æ¢ï¼ŒåŒæ—¶ä¿æŒè¾ƒå¤§çš„æ— æ¡ä»¶ç½‘ç»œä¸å˜ã€‚DEFTåœ¨å¤šç§çº¿æ€§å’Œéçº¿æ€§åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸Šå®ç°äº†æœ€é«˜è¾¾1.6å€çš„åŠ é€Ÿï¼ŒåŒæ—¶åœ¨è‡ªç„¶å›¾åƒä¸Šå…·æœ‰æœ€ä½³çš„æ„ŸçŸ¥è´¨é‡å’ŒåŒ»å­¦å›¾åƒçš„é‡å»ºæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå»ºæ¨¡èŒƒå¼åœ¨è§£å†³åé—®é¢˜çš„æ¡ä»¶é‡‡æ ·æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>åŸºäºå»å™ªæ‰©æ•£è¿‡ç¨‹çš„æ¨¡å‹åœ¨è®¸å¤šçœŸå®ä¸–ç•Œåº”ç”¨ä¸­ç”¨äºæ”¹å–„æ¡ä»¶é‡‡æ ·ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç¼ºä¹ç»Ÿä¸€æ¡†æ¶ï¼Œä¸”ç»å¸¸é¢ä¸´è¶…å‚æ•°æ•æ„Ÿã€è®­ç»ƒæˆæœ¬é«˜æˆ–ä¾èµ–å°é—­APIç­‰é—®é¢˜ã€‚</li>
<li>Doobçš„h-å˜æ¢æä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’æ¥ç»Ÿä¸€æ¡ä»¶è®­ç»ƒå’Œé‡‡æ ·ã€‚</li>
<li>DEFTï¼ˆDoobçš„h-å˜æ¢é«˜æ•ˆå¾®è°ƒï¼‰æ˜¯ä¸€ç§æ–°çš„æ¡ä»¶ç”Ÿæˆæ–¹æ³•ï¼Œå®ƒé€šè¿‡å¾®è°ƒå°ç½‘ç»œå¿«é€Ÿå­¦ä¹ æ¡ä»¶h-å˜æ¢ï¼ŒåŒæ—¶ä¿æŒå¤§ç½‘ç»œä¸å˜ã€‚</li>
<li>DEFTåœ¨å›¾åƒé‡å»ºä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡å’Œå…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.01781">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a35a35ae04ee635d3ed7ae7f09c5167e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-089cd1947b9623c22b4d8ad4cb1c4339.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="How-to-build-the-best-medical-image-segmentation-algorithm-using-foundation-models-a-comprehensive-empirical-study-with-Segment-Anything-Model"><a href="#How-to-build-the-best-medical-image-segmentation-algorithm-using-foundation-models-a-comprehensive-empirical-study-with-Segment-Anything-Model" class="headerlink" title="How to build the best medical image segmentation algorithm using   foundation models: a comprehensive empirical study with Segment Anything   Model"></a>How to build the best medical image segmentation algorithm using   foundation models: a comprehensive empirical study with Segment Anything   Model</h2><p><strong>Authors:Hanxue Gu, Haoyu Dong, Jichen Yang, Maciej A. Mazurowski</strong></p>
<p>Automated segmentation is a fundamental medical image analysis task, which enjoys significant advances due to the advent of deep learning. While foundation models have been useful in natural language processing and some vision tasks for some time, the foundation model developed with image segmentation in mind - Segment Anything Model (SAM) - has been developed only recently and has shown similar promise. However, there are still no systematic analyses or â€œbest-practiceâ€ guidelines for optimal fine-tuning of SAM for medical image segmentation. This work summarizes existing fine-tuning strategies with various backbone architectures, model components, and fine-tuning algorithms across 18 combinations, and evaluates them on 17 datasets covering all common radiology modalities. Our study reveals that (1) fine-tuning SAM leads to slightly better performance than previous segmentation methods, (2) fine-tuning strategies that use parameter-efficient learning in both the encoder and decoder are superior to other strategies, (3) network architecture has a small impact on final performance, (4) further training SAM with self-supervised learning can improve final model performance. We also demonstrate the ineffectiveness of some methods popular in the literature and further expand our experiments into few-shot and prompt-based settings. Lastly, we released our code and MRI-specific fine-tuned weights, which consistently obtained superior performance over the original SAM, at <a target="_blank" rel="noopener" href="https://github.com/mazurowski-lab/finetune-SAM">https://github.com/mazurowski-lab/finetune-SAM</a>. </p>
<blockquote>
<p>è‡ªåŠ¨åˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒåˆ†æä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œéšç€æ·±åº¦å­¦ä¹ çš„å‡ºç°ï¼Œå®ƒå–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚è™½ç„¶åŸºç¡€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’ŒæŸäº›è§†è§‰ä»»åŠ¡ä¸­å·²æœ‰ä¸€æ®µæ—¶é—´çš„åº”ç”¨ï¼Œä½†ä»¥å›¾åƒåˆ†å‰²ä¸ºä¸­å¿ƒå¼€å‘çš„åŸºç¡€æ¨¡å‹â€”â€”ä»»æ„åˆ†å‰²æ¨¡å‹ï¼ˆSAMï¼‰çš„å¼€å‘å´æ˜¯åœ¨æœ€è¿‘æ‰å®Œæˆï¼Œå¹¶æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¯¹äºSAMåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æœ€ä½³å¾®è°ƒï¼Œä»ç„¶æ²¡æœ‰ç³»ç»Ÿçš„åˆ†ææˆ–â€œæœ€ä½³å®è·µâ€æŒ‡å—ã€‚è¿™é¡¹å·¥ä½œæ€»ç»“äº†ä½¿ç”¨å„ç§ä¸»å¹²æ¶æ„ã€æ¨¡å‹ç»„ä»¶å’Œå¾®è°ƒç®—æ³•çš„ç°æœ‰å¾®è°ƒç­–ç•¥ï¼Œå…±æ¶‰åŠ18ç§ç»„åˆï¼Œå¹¶åœ¨æ¶µç›–æ‰€æœ‰å¸¸è§æ”¾å°„å­¦æ¨¡æ€çš„17ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼šï¼ˆ1ï¼‰å¯¹SAMè¿›è¡Œå¾®è°ƒç•¥ä¼˜äºä»¥å‰çš„åˆ†å‰²æ–¹æ³•ï¼›ï¼ˆ2ï¼‰åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¸­éƒ½ä½¿ç”¨å‚æ•°æœ‰æ•ˆå­¦ä¹ çš„å¾®è°ƒç­–ç•¥ä¼˜äºå…¶ä»–ç­–ç•¥ï¼›ï¼ˆ3ï¼‰ç½‘ç»œæ¶æ„å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“å¾ˆå°ï¼›ï¼ˆ4ï¼‰ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ è¿›ä¸€æ­¥è®­ç»ƒSAMå¯ä»¥æé«˜æœ€ç»ˆæ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†æ–‡çŒ®ä¸­ä¸€äº›æµè¡Œæ–¹æ³•çš„æ— æ•ˆæ€§ï¼Œå¹¶å°†æˆ‘ä»¬çš„å®éªŒè¿›ä¸€æ­¥æ‰©å±•åˆ°å°æ ·æœ¬æ–‡æœ¬å’ŒåŸºäºæç¤ºçš„è®¾ç½®ä¸­ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/mazurowski-lab/finetune-SAM%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E9%92%88%E5%AF%B9MRI%E7%9A%84%E7%89%B9%E5%AE%9A%E5%BE%AE%E8%B0%83%E6%9D%83%E9%87%8D%EF%BC%8C%E8%BF%99%E4%BA%9B%E6%9D%83%E9%87%8D%E5%9C%A8%E5%8E%9F%E5%A7%8BSAM%E4%B8%8A%E5%A7%8B%E7%BB%88%E8%8E%B7%E5%BE%97%E4%BA%86%E4%BC%98%E8%B6%8A%E7%9A%84%E6%80%A7%E8%83%BD%E3%80%82">https://github.com/mazurowski-lab/finetune-SAMä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç å’Œé’ˆå¯¹MRIçš„ç‰¹å®šå¾®è°ƒæƒé‡ï¼Œè¿™äº›æƒé‡åœ¨åŸå§‹SAMä¸Šå§‹ç»ˆè·å¾—äº†ä¼˜è¶Šçš„æ€§èƒ½ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.09957v3">PDF</a> Accepted for publication at the Journal of Machine Learning for   Biomedical Imaging (MELBA)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªåŠ¨åŒ–åˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒåˆ†æçš„åŸºæœ¬ä»»åŠ¡ä¹‹ä¸€ã€‚æ–°å¼€å‘çš„é’ˆå¯¹å›¾åƒåˆ†å‰²çš„foundationæ¨¡å‹Segment Anything Modelï¼ˆSAMï¼‰æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†ç›®å‰ä»ç¼ºä¹é’ˆå¯¹SAMä¼˜åŒ–å¾®è°ƒçš„ç³»ç»Ÿåˆ†æå’Œæœ€ä½³å®è·µæŒ‡å—ã€‚æœ¬ç ”ç©¶æ€»ç»“äº†ä½¿ç”¨ä¸åŒéª¨å¹²æ¶æ„ã€æ¨¡å‹ç»„ä»¶å’Œå¾®è°ƒç®—æ³•çš„ç°æœ‰å¾®è°ƒç­–ç•¥ï¼Œå¹¶åœ¨æ¶µç›–æ‰€æœ‰å¸¸è§æ”¾å°„å­¦æ¨¡æ€çš„17ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œå¾®è°ƒSAMç•¥ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œä½¿ç”¨å‚æ•°æœ‰æ•ˆå­¦ä¹ çš„å¾®è°ƒç­–ç•¥ä¼˜äºå…¶ä»–ç­–ç•¥ï¼Œç½‘ç»œæ¶æ„å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“è¾ƒå°ï¼Œè€Œä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ è¿›ä¸€æ­¥è®­ç»ƒSAMå¯ä»¥æé«˜æœ€ç»ˆæ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¾®è°ƒSAMåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸Šçš„æ€§èƒ½ç•¥ä¼˜äºå‰æœŸæ–¹æ³•ã€‚</li>
<li>ä½¿ç”¨å‚æ•°æœ‰æ•ˆå­¦ä¹ çš„å¾®è°ƒç­–ç•¥ï¼Œåœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¸­éƒ½è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
<li>ç½‘ç»œæ¶æ„å¯¹æœ€ç»ˆæ€§èƒ½çš„å½±å“è¾ƒå°ã€‚</li>
<li>é€šè¿‡è‡ªç›‘ç£å­¦ä¹ è¿›ä¸€æ­¥è®­ç»ƒSAMï¼Œå¯ä»¥æé«˜æœ€ç»ˆæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶è¯å®äº†éƒ¨åˆ†æ–‡çŒ®ä¸­æµè¡Œçš„æ–¹æ³•å¹¶ä¸æœ‰æ•ˆã€‚</li>
<li>å®éªŒè¿›ä¸€æ­¥æ‰©å±•åˆ°äº†å°æ ·æœ¬å’ŒåŸºäºæç¤ºçš„è®¾ç½®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.09957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0ef979f362d30c91768edb3cb9cfcbb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0395a23f624f16d17e20aa8bcbbf93a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7b10e1ba15649dc14ded6d0704408d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56e928361467f47b8e5e5155b445b95b.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="RT-GAN-Recurrent-Temporal-GAN-for-Adding-Lightweight-Temporal-Consistency-to-Frame-Based-Domain-Translation-Approaches"><a href="#RT-GAN-Recurrent-Temporal-GAN-for-Adding-Lightweight-Temporal-Consistency-to-Frame-Based-Domain-Translation-Approaches" class="headerlink" title="RT-GAN: Recurrent Temporal GAN for Adding Lightweight Temporal   Consistency to Frame-Based Domain Translation Approaches"></a>RT-GAN: Recurrent Temporal GAN for Adding Lightweight Temporal   Consistency to Frame-Based Domain Translation Approaches</h2><p><strong>Authors:Shawn Mathew, Saad Nadeem, Alvin C. Goh, Arie Kaufman</strong></p>
<p>Fourteen million colonoscopies are performed annually just in the U.S. However, the videos from these colonoscopies are not saved due to storage constraints (each video from a high-definition colonoscope camera can be in tens of gigabytes). Instead, a few relevant individual frames are saved for documentation&#x2F;reporting purposes and these are the frames on which most current colonoscopy AI models are trained on. While developing new unsupervised domain translation methods for colonoscopy (e.g. to translate between real optical and virtual&#x2F;CT colonoscopy), it is thus typical to start with approaches that initially work for individual frames without temporal consistency. Once an individual-frame model has been finalized, additional contiguous frames are added with a modified deep learning architecture to train a new model from scratch for temporal consistency. This transition to temporally-consistent deep learning models, however, requires significantly more computational and memory resources for training. In this paper, we present a lightweight solution with a tunable temporal parameter, RT-GAN (Recurrent Temporal GAN), for adding temporal consistency to individual frame-based approaches that reduces training requirements by a factor of 5. We demonstrate the effectiveness of our approach on two challenging use cases in colonoscopy: haustral fold segmentation (indicative of missed surface) and realistic colonoscopy simulator video generation. We also release a first-of-its kind temporal dataset for colonoscopy for the above use cases. The datasets, accompanying code, and pretrained models will be made available on our Computational Endoscopy Platform GitHub (<a target="_blank" rel="noopener" href="https://github.com/nadeemlab/CEP">https://github.com/nadeemlab/CEP</a>). The supplementary video is available at <a target="_blank" rel="noopener" href="https://youtu.be/UMVP-uIXwWk">https://youtu.be/UMVP-uIXwWk</a>. </p>
<blockquote>
<p>æ¯å¹´ä»…åœ¨ç¾å›˜å›½å°±æœ‰å¤šè¾¾ä¸€åƒå››ç™¾ä¸‡æ¬¡ç»“è‚ é•œæ£€æŸ¥ã€‚ç„¶è€Œï¼Œç”±äºå­˜å‚¨é™åˆ¶ï¼ˆæ¯ä¸ªé«˜æ¸…ç»“è‚ é•œæ‘„åƒæœºæ‹æ‘„çš„è§†é¢‘å¯èƒ½è¾¾åˆ°æ•°ååƒå…†ï¼‰ï¼Œè¿™äº›ç»“è‚ é•œæ£€æŸ¥çš„è§†é¢‘å¹¶æœªä¿å­˜ã€‚ç›¸åï¼Œä¸ºäº†è®°å½•æˆ–æŠ¥å‘Šç›®çš„ï¼Œä»…ä¿å­˜äº†ä¸€äº›ç›¸å…³çš„å•ç‹¬å¸§ï¼Œè€Œè¿™äº›å¸§ä¹Ÿæ˜¯å½“å‰å¤§å¤šæ•°ç»“è‚ é•œæ£€æŸ¥AIæ¨¡å‹çš„è®­ç»ƒåŸºç¡€ã€‚åœ¨å¼€å‘ç”¨äºç»“è‚ é•œæ£€æŸ¥çš„æ–°å‹æ— ç›‘ç£åŸŸè½¬æ¢æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œç”¨äºçœŸå®å…‰å­¦ç»“è‚ é•œå’Œè™šæ‹Ÿ&#x2F;CTç»“è‚ é•œä¹‹é—´çš„è½¬æ¢ï¼‰æ—¶ï¼Œå› æ­¤é€šå¸¸ä¼šé‡‡ç”¨é¦–å…ˆé’ˆå¯¹å•ç‹¬å¸§è¿›è¡Œå·¥ä½œçš„æ–¹æ³•ï¼Œè€Œæ— éœ€è€ƒè™‘æ—¶é—´ä¸€è‡´æ€§ã€‚ä¸€æ—¦å®Œæˆå•ä¸ªå¸§æ¨¡å‹çš„æœ€ç»ˆç¡®å®šï¼Œå°†ä½¿ç”¨ç»è¿‡ä¿®æ”¹çš„æ·±åº¦å­¦ä¹ æ¶æ„æ·»åŠ è¿ç»­çš„ç›¸é‚»å¸§ï¼Œç„¶åä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªæ–°æ¨¡å‹ä»¥å®ç°æ—¶é—´ä¸€è‡´æ€§ã€‚ç„¶è€Œï¼Œè¿™ç§å‘æ—¶é—´ä¸€è‡´çš„æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è¿‡æ¸¡éœ€è¦æ›´å¤šçš„è®¡ç®—å’Œå†…å­˜èµ„æºè¿›è¡Œè®­ç»ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…·æœ‰å¯è°ƒæ—¶é—´å‚æ•°çš„è½»é‡çº§è§£å†³æ–¹æ¡ˆRT-GANï¼ˆé€’å½’æ—¶é—´GANï¼‰ï¼Œè¯¥æ–¹æ¡ˆå¯ä¸ºåŸºäºå•ä¸ªå¸§çš„æ–¹æ³•æä¾›æ—¶é—´ä¸€è‡´æ€§ï¼Œé€šè¿‡å°†è®­ç»ƒè¦æ±‚é™ä½äº”åˆ†ä¹‹ä¸€æ¥å‡å°‘è®­ç»ƒéœ€æ±‚ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ç»“è‚ é•œæ£€æŸ¥ä¸­çš„ä¸¤ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç”¨ä¾‹æ¥è¯æ˜æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼šç»“è‚ è¢‹åˆ†å‰²ï¼ˆæŒ‡ç¤ºä¸¢å¤±çš„è¡¨é¢ï¼‰å’Œé€¼çœŸçš„ç»“è‚ é•œæ£€æŸ¥æ¨¡æ‹Ÿå™¨è§†é¢‘ç”Ÿæˆã€‚æˆ‘ä»¬è¿˜é’ˆå¯¹ä¸Šè¿°ç”¨ä¾‹å‘å¸ƒäº†é¦–ä¸ªç»“è‚ é•œæ£€æŸ¥æ—¶é—´åºåˆ—æ•°æ®é›†ã€‚æ•°æ®é›†ã€é…å¥—ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å°†å‘å¸ƒåœ¨æˆ‘ä»¬çš„è®¡ç®—å†…çª¥é•œå¹³å°GitHubä¸Šï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/nadeemlab/CEP%EF%BC%89%E3%80%82%E8%A1%A5%E5%85%85%E8%A7%86%E9%A2%91%E5%8F%AF%E9%80%9A%E8%BF%87%E4%BB%A5%E4%B8%8B%E9%93%BE%E6%8E%A5%E6%9F%A5%E7%9C%8B%EF%BC%9A[%E8%A7%86%E9%A2%91%E9%93%BE%E6%8E%A5](https://youtu.be/UMVP-uIXwWk)%E3%80%82">https://github.com/nadeemlab/CEPï¼‰ã€‚è¡¥å……è§†é¢‘å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥æŸ¥çœ‹ï¼š[è§†é¢‘é“¾æ¥](https://youtu.be/UMVP-uIXwWk)ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00868v2">PDF</a> MICCAI 2025 Early Accept. First two authors contributed equally</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºå¯è°ƒæ•´æ—¶é—´å‚æ•°çš„è½»é‡çº§è§£å†³æ–¹æ¡ˆRT-GANï¼Œç”¨äºä¸ºå•å¸§æ–¹æ³•å¢åŠ æ—¶é—´ä¸€è‡´æ€§ï¼Œå‡å°‘åŸ¹è®­è¦æ±‚ï¼Œå¹¶åœ¨ç»“è‚ é•œæ£€æŸ¥çš„ä¸¤ä¸ªç”¨ä¾‹ä¸ŠéªŒè¯å…¶æœ‰æ•ˆæ€§ï¼šç»“è‚ çš±è¥åˆ†å‰²å’Œé€¼çœŸçš„ç»“è‚ é•œæ£€æŸ¥æ¨¡æ‹Ÿå™¨è§†é¢‘ç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¯å¹´åœ¨ç¾å›½è¿›è¡Œ14ä¸‡æ¬¡ç»“è‚ é•œæ£€æŸ¥ï¼Œä½†ç”±äºå­˜å‚¨é™åˆ¶ï¼Œè¿™äº›æ£€æŸ¥çš„è§†é¢‘å¹¶æœªä¿å­˜ã€‚</li>
<li>å½“å‰ç»“è‚ é•œæ£€æŸ¥AIæ¨¡å‹ä¸»è¦åœ¨ä¿å­˜çš„ä¸ªåˆ«å¸§ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>åœ¨å¼€å‘ç»“è‚ é•œæ£€æŸ¥çš„è·¨åŸŸç¿»è¯‘æ–¹æ³•æ—¶ï¼Œé€šå¸¸é¦–å…ˆå…³æ³¨å•å¸§æ–¹æ³•ï¼Œç„¶åé€æ­¥å¼•å…¥æ—¶é—´ä¸€è‡´æ€§ã€‚</li>
<li>å®ç°æ—¶é—´ä¸€è‡´æ€§çš„æ·±åº¦å­¦ä¹ æ¨¡å‹éœ€è¦æ›´å¤šçš„è®¡ç®—å’Œå†…å­˜èµ„æºè¿›è¡Œè®­ç»ƒã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¯è°ƒæ•´æ—¶é—´å‚æ•°çš„è½»é‡çº§è§£å†³æ–¹æ¡ˆRT-GANï¼Œä¸ºå•å¸§æ–¹æ³•å¢åŠ æ—¶é—´ä¸€è‡´æ€§ï¼Œé™ä½è®­ç»ƒè¦æ±‚ã€‚</li>
<li>RT-GANåœ¨ç»“è‚ çš±è¥åˆ†å‰²å’Œé€¼çœŸçš„ç»“è‚ é•œæ£€æŸ¥æ¨¡æ‹Ÿå™¨è§†é¢‘ç”Ÿæˆä¸¤ä¸ªç”¨ä¾‹ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.00868">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-adf15c8401c614e304f9b972a1dabe01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d732cdb9730690481115522e7ce59722.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ce3f24992d0f3ae50295c40dc7f8423.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5283dcb59376ba0e8c11c4bb68a5d9fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-780b0c2f055f4c8ee47ecdd27a1f7351.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-05-15/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-05-15/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-15/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2c891b48b0a22237410a191f712b7654.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-15  TT-DF A Large-Scale Diffusion-Based Dataset and Benchmark for Human   Body Forgery Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-05-15/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-44b8b4993fc331ade1ede53e2e9933eb.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-05-15  Boosting Zero-shot Stereo Matching using Large-scale Mixed Images   Sources in the Real World
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-15
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27685.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
