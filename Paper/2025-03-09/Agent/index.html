<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-03-09  Multi-Agent Inverse Q-Learning from Demonstrations">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a6d19e10e57fe6b34da29c78421a0ded.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    40 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-09-更新"><a href="#2025-03-09-更新" class="headerlink" title="2025-03-09 更新"></a>2025-03-09 更新</h1><h2 id="Multi-Agent-Inverse-Q-Learning-from-Demonstrations"><a href="#Multi-Agent-Inverse-Q-Learning-from-Demonstrations" class="headerlink" title="Multi-Agent Inverse Q-Learning from Demonstrations"></a>Multi-Agent Inverse Q-Learning from Demonstrations</h2><p><strong>Authors:Nathaniel Haynam, Adam Khoja, Dhruv Kumar, Vivek Myers, Erdem Bıyık</strong></p>
<p>When reward functions are hand-designed, deep reinforcement learning algorithms often suffer from reward misspecification, causing them to learn suboptimal policies in terms of the intended task objectives. In the single-agent case, inverse reinforcement learning (IRL) techniques attempt to address this issue by inferring the reward function from expert demonstrations. However, in multi-agent problems, misalignment between the learned and true objectives is exacerbated due to increased environment non-stationarity and variance that scales with multiple agents. As such, in multi-agent general-sum games, multi-agent IRL algorithms have difficulty balancing cooperative and competitive objectives. To address these issues, we propose Multi-Agent Marginal Q-Learning from Demonstrations (MAMQL), a novel sample-efficient framework for multi-agent IRL. For each agent, MAMQL learns a critic marginalized over the other agents’ policies, allowing for a well-motivated use of Boltzmann policies in the multi-agent context. We identify a connection between optimal marginalized critics and single-agent soft-Q IRL, allowing us to apply a direct, simple optimization criterion from the single-agent domain. Across our experiments on three different simulated domains, MAMQL significantly outperforms previous multi-agent methods in average reward, sample efficiency, and reward recovery by often more than 2-5x. We make our code available at <a target="_blank" rel="noopener" href="https://sites.google.com/view/mamql">https://sites.google.com/view/mamql</a> . </p>
<blockquote>
<p>当奖励函数是手动设计时，深度强化学习算法往往会受到奖励误指定的困扰，导致它们在预期的任务目标方面学习到的策略是次优的。在单代理情况下，逆向强化学习（IRL）技术试图通过从专家演示中推断奖励函数来解决这个问题。然而，在多代理问题中，由于环境非平稳性和随着多个代理而增加的方差，学习目标和真实目标之间的不一致性会加剧。因此，在多代理总收益游戏中，多代理IRL算法很难平衡合作和竞争目标。为了解决这些问题，我们提出了多代理边际Q学习（MAMQL），这是一种用于多代理IRL的新型样本高效框架。对于每个代理，MAMQL学习其他代理策略的边际评论家，这允许在多代理背景下使用有充分依据的玻尔兹曼策略。我们发现了最优边际评论家和单代理软Q IRL之间的联系，使我们能够应用来自单代理领域的直接、简单的优化标准。在我们对三个不同模拟领域的实验测试中，MAMQL在平均奖励、样本效率和奖励恢复方面显著优于之前的多代理方法，通常超过2-5倍。我们的代码可在<a target="_blank" rel="noopener" href="https://sites.google.com/view/mamql%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://sites.google.com/view/mamql上获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04679v1">PDF</a> 8 pages, 4 figures, 2 tables. Published at the International   Conference on Robotics and Automation (ICRA) 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为MAMQL的多智能体边际Q学习算法，用于解决多智能体强化学习中的奖励函数误指定问题。该算法通过边际化其他智能体的策略来为每个智能体学习一个评论家，并应用Boltzmann策略。MAMQL在模拟域的测试中显著优于其他多智能体方法，提高了平均奖励、样本效率和奖励恢复能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MAMQL是一种针对多智能体强化学习中的奖励函数误指定问题的新型算法。</li>
<li>MAMQL通过为每个智能体学习一个边际评论家来解决多智能体环境中的合作与竞争目标平衡问题。</li>
<li>MAMQL利用Boltzmann策略，在多智能体环境中表现出良好的适应性。</li>
<li>该算法在模拟域的测试中显著优于其他多智能体方法。</li>
<li>MAMQL提高了多智能体系统的平均奖励、样本效率和奖励恢复能力。</li>
<li>MAMQL算法与单智能体的软Q-IRL之间存在联系，这使其能够应用简单的优化标准。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04679">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-451b70416b6429da5493f427b60dbb6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f06e6e485397199611cf33298e8fd4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de889533c827fe811783856d6f66d857.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c85c70f3f1af36f364f831349b634a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-258ff4ae2bb15a91fc2d2766dd02e20a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65ed09a73519b363a353bca9953a685f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DVM-SLAM-Decentralized-Visual-Monocular-Simultaneous-Localization-and-Mapping-for-Multi-Agent-Systems"><a href="#DVM-SLAM-Decentralized-Visual-Monocular-Simultaneous-Localization-and-Mapping-for-Multi-Agent-Systems" class="headerlink" title="DVM-SLAM: Decentralized Visual Monocular Simultaneous Localization and   Mapping for Multi-Agent Systems"></a>DVM-SLAM: Decentralized Visual Monocular Simultaneous Localization and   Mapping for Multi-Agent Systems</h2><p><strong>Authors:Joshua Bird, Jan Blumenkamp, Amanda Prorok</strong></p>
<p>Cooperative Simultaneous Localization and Mapping (C-SLAM) enables multiple agents to work together in mapping unknown environments while simultaneously estimating their own positions. This approach enhances robustness, scalability, and accuracy by sharing information between agents, reducing drift, and enabling collective exploration of larger areas. In this paper, we present Decentralized Visual Monocular SLAM (DVM-SLAM), the first open-source decentralized monocular C-SLAM system. By only utilizing low-cost and light-weight monocular vision sensors, our system is well suited for small robots and micro aerial vehicles (MAVs). DVM-SLAM’s real-world applicability is validated on physical robots with a custom collision avoidance framework, showcasing its potential in real-time multi-agent autonomous navigation scenarios. We also demonstrate comparable accuracy to state-of-the-art centralized monocular C-SLAM systems. We open-source our code and provide supplementary material online. </p>
<blockquote>
<p>协同定位与地图构建（C-SLAM）允许多个智能体在未知环境中协同工作，同时估计它们自身的位置。通过共享信息、减少漂移以及进行更大区域的集体探索，该方法增强了系统的稳健性、可扩展性和准确性。本文提出了第一个开源的去中心化单目协同视觉同步定位与地图构建系统——分散式视觉单目SLAM（DVM-SLAM）。该系统仅使用低成本、轻量级的单目视觉传感器，非常适合小型机器人和微型飞行器（MAVs）。通过物理机器人上的自定义避障框架验证了DVM-SLAM在现实世界的适用性，展示了其在实时多智能体自主导航场景中的潜力。我们还证明了其精度与最先进的集中式单目C-SLAM系统相当。我们公开了源代码并提供了补充材料。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04126v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>C-SLAM技术通过多个智能体协同工作，在未知环境中进行地图构建并估计各自位置。本文介绍了一种名为DVM-SLAM的开源去中心化单目视觉SLAM系统，适用于小型机器人和微型飞行器。该系统通过利用低成本、轻量级的单目视觉传感器，具有良好的实时多智能体自主导航潜力，并通过实物机器人验证了其实用性，与最新的集中化单目视觉C-SLAM系统相比表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>C-SLAM技术允许多个智能体协同工作，在未知环境中进行地图构建和位置估计。</li>
<li>DVM-SLAM是首个开源的去中心化单目视觉SLAM系统。</li>
<li>DVM-SLAM适用于低成本、轻量级的单目视觉传感器，特别适用于小型机器人和微型飞行器。</li>
<li>DVM-SLAM具有实时多智能体自主导航的潜力。</li>
<li>通过实物机器人验证了DVM-SLAM系统的实用性。</li>
<li>DVM-SLAM与最新的集中化单目视觉C-SLAM系统相比表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04126">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-98818a404bafddd913680bb85e7458ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7888655cd73fcb449cd673de492b6170.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f130b0d3e51547c6ba092cefe4b142bd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c2cac3e4ea1eb32d80f15972b2cfb2b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a362c6d2379ed4d0a966e5f9cdd9adf4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-905bf18689f0de570b40e519803d24a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-407b25b8bc7e96b44cdd27065bf3cdf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14b85cd9fc23ad6b7f68315b263cfee3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-079ab572218e68e2519d6b0502c8d18b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PokeChamp-an-Expert-level-Minimax-Language-Agent"><a href="#PokeChamp-an-Expert-level-Minimax-Language-Agent" class="headerlink" title="PokéChamp: an Expert-level Minimax Language Agent"></a>PokéChamp: an Expert-level Minimax Language Agent</h2><p><strong>Authors:Seth Karten, Andy Luu Nguyen, Chi Jin</strong></p>
<p>We introduce Pok&#39;eChamp, a minimax agent powered by Large Language Models (LLMs) for Pok&#39;emon battles. Built on a general framework for two-player competitive games, Pok&#39;eChamp leverages the generalist capabilities of LLMs to enhance minimax tree search. Specifically, LLMs replace three key modules: (1) player action sampling, (2) opponent modeling, and (3) value function estimation, enabling the agent to effectively utilize gameplay history and human knowledge to reduce the search space and address partial observability. Notably, our framework requires no additional LLM training. We evaluate Pok&#39;eChamp in the popular Gen 9 OU format. When powered by GPT-4o, it achieves a win rate of 76% against the best existing LLM-based bot and 84% against the strongest rule-based bot, demonstrating its superior performance. Even with an open-source 8-billion-parameter Llama 3.1 model, Pok&#39;eChamp consistently outperforms the previous best LLM-based bot, Pok&#39;ellmon powered by GPT-4o, with a 64% win rate. Pok&#39;eChamp attains a projected Elo of 1300-1500 on the Pok&#39;emon Showdown online ladder, placing it among the top 30%-10% of human players. In addition, this work compiles the largest real-player Pok&#39;emon battle dataset, featuring over 3 million games, including more than 500k high-Elo matches. Based on this dataset, we establish a series of battle benchmarks and puzzles to evaluate specific battling skills. We further provide key updates to the local game engine. We hope this work fosters further research that leverage Pok&#39;emon battle as benchmark to integrate LLM technologies with game-theoretic algorithms addressing general multiagent problems. Videos, code, and dataset available at <a target="_blank" rel="noopener" href="https://sites.google.com/view/pokechamp-llm">https://sites.google.com/view/pokechamp-llm</a>. </p>
<blockquote>
<p>我们介绍了Pok’eChamp，这是一个由大型语言模型（LLMs）驱动的宝可梦战斗的极小极大值代理人。基于两人竞技游戏的通用框架，Pok’eChamp利用LLMs的通用能力来增强极小极大值树搜索。具体来说，LLMs替换了三个关键模块：（1）玩家动作采样，（2）对手建模和（3）值函数估计，这使得代理人能够有效地利用游戏历史和人类知识来减少搜索空间并解决部分可观察性。值得注意的是，我们的框架不需要额外的LLM训练。我们在流行的Gen 9 OU格式中对Pok’eChamp进行了评估。当它由GPT-4o驱动时，与现有的最佳LLM机器人相比，它的胜率为76%，与最强的规则机器人相比，胜率为84%，这证明了其卓越性能。即使使用开源的8亿参数Llama 3.1模型，Pok’eChamp也始终优于之前最好的基于LLM的机器人Pok’ellmon，胜率为64%。Pok’eChamp在宝可梦对决在线排行榜上的预估Elo得分为1300-1500，跻身人类玩家前30%-10%。此外，这项工作还编译了最大的真实玩家宝可梦战斗数据集，包含超过300万场比赛，其中包括超过50万场高Elo比赛。基于此数据集，我们建立了一系列战斗基准测试和谜题来评估特定的战斗技能。我们还对本地游戏引擎进行了关键更新。我们希望这项工作能够激发更多的研究，利用宝可梦战斗作为基准测试，将LLM技术与解决一般多智能体问题的博弈论算法相结合。视频、代码和数据集可在<a target="_blank" rel="noopener" href="https://sites.google.com/view/pokechamp-llm">https://sites.google.com/view/pokechamp-llm</a>上找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04094v1">PDF</a> 24 pages, 13 figures</p>
<p><strong>Summary</strong></p>
<p>Pok’eChamp是一个利用大型语言模型（LLMs）驱动的minimax对战代理，用于进行Pokémon对战。该代理采用通用两玩家竞技游戏框架，通过LLMs增强minimax树搜索能力。LLMs替换关键模块，包括玩家行动采样、对手建模和价值函数估计，使代理能够利用游戏历史和人类知识来减少搜索空间并解决部分可观测性问题。Pok’eChamp在Gen 9 OU格式中的评估表现优异，使用GPT-4o时，在对抗最佳LLM基机器人和最强规则基机器人时，胜率分别达到了76%和84%。即使是使用开源的8亿参数Llama 3.1模型，Pok’eChamp也表现出超越之前最佳LLM基机器人Pok’ellmon的64%胜率。该工作还编译了最大的真实玩家Pokémon战斗数据集，包含超过300万场比赛，以及超过50万场高Elo比赛。基于此数据集，该工作建立了一系列战斗基准测试和谜题来评估特定的战斗技能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Pok’eChamp是一个基于大型语言模型（LLMs）的minimax对战代理，专门用于Pokémon对战。</li>
<li>LLMs在Pok’eChamp中用于替换关键模块，如玩家行动采样、对手建模和价值函数估计，提高了代理的游戏性能。</li>
<li>Pok’eChamp在Gen 9 OU格式中的评估表现优异，使用不同模型时表现出高胜率。</li>
<li>Pok’eChamp使用开源模型Llama 3.1也表现出优越性能，超过了之前的最佳LLM基机器人。</li>
<li>该工作编译了最大的真实玩家Pokémon战斗数据集，包含大量比赛数据。</li>
<li>基于此数据集，建立了战斗基准测试和谜题，以评估特定的战斗技能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04094">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1deda35faf4ca989b188296d95e40c5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b67b3b1187b48067c5637c57d3db0818.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a03d43b74f1ebdf6bbaa083f194fe5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f532c26fc3948f27a31474788c969c8f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Multi-Agent-Systems-Powered-by-Large-Language-Models-Applications-in-Swarm-Intelligence"><a href="#Multi-Agent-Systems-Powered-by-Large-Language-Models-Applications-in-Swarm-Intelligence" class="headerlink" title="Multi-Agent Systems Powered by Large Language Models: Applications in   Swarm Intelligence"></a>Multi-Agent Systems Powered by Large Language Models: Applications in   Swarm Intelligence</h2><p><strong>Authors:Cristian Jimenez-Romero, Alper Yegenoglu, Christian Blum</strong></p>
<p>This work examines the integration of large language models (LLMs) into multi-agent simulations by replacing the hard-coded programs of agents with LLM-driven prompts. The proposed approach is showcased in the context of two examples of complex systems from the field of swarm intelligence: ant colony foraging and bird flocking. Central to this study is a toolchain that integrates LLMs with the NetLogo simulation platform, leveraging its Python extension to enable communication with GPT-4o via the OpenAI API. This toolchain facilitates prompt-driven behavior generation, allowing agents to respond adaptively to environmental data. For both example applications mentioned above, we employ both structured, rule-based prompts and autonomous, knowledge-driven prompts. Our work demonstrates how this toolchain enables LLMs to study self-organizing processes and induce emergent behaviors within multi-agent environments, paving the way for new approaches to exploring intelligent systems and modeling swarm intelligence inspired by natural phenomena. We provide the code, including simulation files and data at <a target="_blank" rel="noopener" href="https://github.com/crjimene/swarm_gpt">https://github.com/crjimene/swarm_gpt</a>. </p>
<blockquote>
<p>本文研究了大型语言模型（LLMs）在多智能体仿真中的集成方法，通过用LLM驱动的提示替换智能体的硬编码程序来实现。所提出的方法在群智能领域的两个复杂系统示例中得到了展示：蚂蚁群体觅食和鸟类集群迁徙。本研究的核心是一个将LLMs与NetLogo仿真平台集成的工具链，它利用NetLogo的Python扩展，通过OpenAI API与GPT-4o进行通信。该工具链促进了提示驱动的行为生成，使智能体能够自适应地响应环境数据。对于上述两个示例应用程序，我们既采用结构化的、基于规则的提示，也采用自主的、知识驱动的提示。我们的工作展示了该工具链如何使LLMs能够研究自组织过程并在多智能体环境中引发新兴行为，为探索智能系统和建立受自然现象启发的群智能模型开辟了新的途径。我们提供的代码，包括仿真文件和资料可以在<a target="_blank" rel="noopener" href="https://github.com/crjimene/swarm_gpt%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/crjimene/swarm_gpt找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03800v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）被集成到多智能体模拟中，通过LLM驱动的提示替换硬编码的智能体程序。本研究以群体智能领域的两个复杂系统——蚁群觅食和鸟群迁移为例，展示了这一方法。通过整合LLM与NetLogo模拟平台并借助Python扩展与GPT-4o进行通信的工具链，促进了提示驱动的行为生成，使智能体能根据环境数据自适应反应。我们的工作证明了这一工具链在研究自我组织过程和在多智能体环境中诱发新兴行为的能力，并为探索智能系统和模仿自然现象的群体智能提供了新的方法。代码及相关资料已上传至GitHub仓库。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）被用于多智能体模拟中，取代了传统的硬编码程序。</li>
<li>研究聚焦于如何利用LLM驱动的提示引导智能体的行为。</li>
<li>通过蚁群觅食和鸟群迁移两个例子展示了该方法的实际应用。</li>
<li>利用工具链整合了LLM与NetLogo模拟平台，并借助Python扩展实现与GPT-4o的通信。</li>
<li>提示驱动的行为生成使得智能体能根据环境数据自适应反应。</li>
<li>该方法展现了自我组织过程和新兴行为在多智能体环境中的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03800">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bd4b01eeda7d7767113ffcc904991d02.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-895af913db01b2911fe8c0dd616a6fa8.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="METAL-A-Multi-Agent-Framework-for-Chart-Generation-with-Test-Time-Scaling"><a href="#METAL-A-Multi-Agent-Framework-for-Chart-Generation-with-Test-Time-Scaling" class="headerlink" title="METAL: A Multi-Agent Framework for Chart Generation with Test-Time   Scaling"></a>METAL: A Multi-Agent Framework for Chart Generation with Test-Time   Scaling</h2><p><strong>Authors:Bingxuan Li, Yiwei Wang, Jiuxiang Gu, Kai-Wei Chang, Nanyun Peng</strong></p>
<p>Chart generation aims to generate code to produce charts satisfying the desired visual properties, e.g., texts, layout, color, and type. It has great potential to empower the automatic professional report generation in financial analysis, research presentation, education, and healthcare. In this work, we build a vision-language model (VLM) based multi-agent framework for effective automatic chart generation. Generating high-quality charts requires both strong visual design skills and precise coding capabilities that embed the desired visual properties into code. Such a complex multi-modal reasoning process is difficult for direct prompting of VLMs. To resolve these challenges, we propose METAL, a multi-agent framework that decomposes the task of chart generation into the iterative collaboration among specialized agents. METAL achieves 5.2% improvement over the current best result in the chart generation task. The METAL framework exhibits the phenomenon of test-time scaling: its performance increases monotonically as the logarithmic computational budget grows from 512 to 8192 tokens. In addition, we find that separating different modalities during the critique process of METAL boosts the self-correction capability of VLMs in the multimodal context. </p>
<blockquote>
<p>图表生成旨在生成代码，以产生满足所需视觉属性的图表，例如文本、布局、颜色和类型。它在金融分析、研究报告、教育和医疗的自动专业报告生成方面具有巨大的潜力。在这项工作中，我们建立了一个基于视觉语言模型（VLM）的多智能体框架，用于有效的自动图表生成。生成高质量的图表需要强大的视觉设计技能和精确的编码能力，将所需的视觉属性嵌入代码中。这样的复杂多模态推理过程对于直接提示VLM来说很难。为了解决这些挑战，我们提出了METAL，这是一个多智能体框架，将图表生成任务分解为专业智能体之间的迭代协作。METAL在图表生成任务上实现了比当前最佳结果高出5.2%的改进。METAL框架表现出测试时缩放现象：随着对数计算预算从512增长到8192令牌，其性能单调增加。此外，我们发现，在METAL的批判过程中分离不同的模态提升了多模态上下文中VLM的自我修正能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17651v3">PDF</a> </p>
<p><strong>Summary</strong><br>图表生成旨在生成代码以生成满足所需视觉属性的图表，如文本、布局、颜色和类型等。该研究建立了一个基于视觉语言模型（VLM）的多代理框架，以实现有效的自动图表生成。高质量图表生成需要强大的视觉设计技能和精确的编码能力，将所需的视觉属性嵌入代码中。这种复杂的跨模态推理过程难以通过直接提示VLMs实现。为了解决这些挑战，我们提出了METAL多代理框架，该框架将图表生成任务分解为专业代理之间的迭代协作。METAL在图表生成任务上实现了对当前最佳结果的5.2%的提升。此外，我们发现METAL的评审过程中不同模态的分离增强了VLMs在多模态环境中的自我修正能力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>图表生成旨在自动生成满足特定视觉属性的代码。</li>
<li>建立了一个基于视觉语言模型（VLM）的多代理框架进行自动图表生成。</li>
<li>高质量图表生成需要强大的视觉设计技能和精确的编码能力。</li>
<li>METAL多代理框架将图表生成任务分解为专业代理之间的迭代协作。</li>
<li>METAL在图表生成任务上实现了显著的性能提升。</li>
<li>METAL框架展现出测试时缩放现象，性能随计算预算的对数增长而提高。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17651">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a758790a0c3e6384cb15626c46f81ee7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-031f2c64ef8042aaa9b617c2eedcb36b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56da3ece4457353dc64a394365069757.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e929cb3ebe13c63d860ee8d80918e69.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="An-LLM-based-Agent-for-Reliable-Docker-Environment-Configuration"><a href="#An-LLM-based-Agent-for-Reliable-Docker-Environment-Configuration" class="headerlink" title="An LLM-based Agent for Reliable Docker Environment Configuration"></a>An LLM-based Agent for Reliable Docker Environment Configuration</h2><p><strong>Authors:Ruida Hu, Chao Peng, Xinchen Wang, Cuiyun Gao</strong></p>
<p>Environment configuration is a critical yet time-consuming step in software development, especially when dealing with unfamiliar code repositories. While Large Language Models (LLMs) demonstrate the potential to accomplish software engineering tasks, existing methods for environment configuration often rely on manual efforts or fragile scripts, leading to inefficiencies and unreliable outcomes. We introduce Repo2Run, the first LLM-based agent designed to fully automate environment configuration and generate executable Dockerfiles for arbitrary Python repositories. We address two major challenges: (1) enabling the LLM agent to configure environments within isolated Docker containers, and (2) ensuring the successful configuration process is recorded and accurately transferred to a Dockerfile without error. To achieve this, we propose atomic configuration synthesis, featuring a dual-environment architecture (internal and external environment) with a rollback mechanism to prevent environment “pollution” from failed commands, guaranteeing atomic execution (execute fully or not at all) and a Dockerfile generator to transfer successful configuration steps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark of 420 recent Python repositories with unit tests, where it achieves an 86.0% success rate, outperforming the best baseline by 63.9%. Repo2Run is available at <a target="_blank" rel="noopener" href="https://github.com/bytedance/Repo2Run">https://github.com/bytedance/Repo2Run</a>. </p>
<blockquote>
<p>环境配置是软件开发中的一个关键且耗时的步骤，尤其是在处理不熟悉的代码仓库时。虽然大型语言模型（LLM）显示出完成软件工程任务的潜力，但现有的环境配置方法通常依赖于手动操作或易出错的脚本，导致效率低下和结果不可靠。我们介绍了Repo2Run，这是基于LLM的第一个完全自动化环境配置的代理，并为任意Python仓库生成可执行的Dockerfile。我们解决了两个主要挑战：（1）使LLM代理能够在隔离的Docker容器内配置环境；（2）确保成功的配置过程被记录并准确地转移到Dockerfile中而不出错。为了实现这一点，我们提出了原子配置合成，它采用双环境架构（内部和外部环境）并带有回滚机制，以防止因命令失败而导致的环境“污染”，保证原子执行（完全执行或不执行）以及Dockerfile生成器，将成功的配置步骤转移到可运行的Dockerfile中。我们在由单位测试组成的420个最新Python仓库的基准测试上对Repo2Run进行了评估，其成功率为86.0%，优于最佳基线63.9%。Repo2Run可在<a target="_blank" rel="noopener" href="https://github.com/bytedance/Repo2Run%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/bytedance/Repo2Run上获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13681v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>环境配置是软件开发中的一个重要且耗时的步骤，尤其是在处理不熟悉的代码仓库时。现有环境配置方法依赖手动或脆弱的脚本，导致效率低下和结果不可靠。我们引入了Repo2Run，这是第一个基于大型语言模型（LLM）的代理，旨在完全自动化环境配置并为任意Python仓库生成可执行Dockerfile。通过原子配置合成和双环境架构等关键技术，Repo2Run能够在Docker容器中配置环境并确保成功配置过程被准确记录并转移到Dockerfile中。我们在包含单位测试的基准测试上评估了Repo2Run，它在成功率上实现了超过其他基准模型的显著优势。更多信息可访问其GitHub仓库链接：<a target="_blank" rel="noopener" href="https://github.com/bytedance/Repo2Run">链接地址</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>环境配置在软件开发中是关键且耗时的步骤，特别是面对未知代码仓库时。</li>
<li>当前环境配置方法存在效率低下和结果不可靠的问题。</li>
<li>Repo2Run是首个基于LLM的自动化环境配置代理，可为任意Python仓库生成可执行Dockerfile。</li>
<li>Repo2Run解决了在Docker容器中配置环境的难题，并确保了成功配置过程被准确记录并转移到Dockerfile中。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13681">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cd73d7b58306acb5080fbed96f8789ff.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1c43555db8816a6d00350b740edfe65a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0403d13ba342e95775a92da9eb9b0cb5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17bee8276e0a918929ed1d21bb6ce2e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f9f710832a3d655e55ddeb679144336.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="R2-KG-General-Purpose-Dual-Agent-Framework-for-Reliable-Reasoning-on-Knowledge-Graphs"><a href="#R2-KG-General-Purpose-Dual-Agent-Framework-for-Reliable-Reasoning-on-Knowledge-Graphs" class="headerlink" title="R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on   Knowledge Graphs"></a>R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on   Knowledge Graphs</h2><p><strong>Authors:Sumin Jo, Junseong Choi, Jiho Kim, Edward Choi</strong></p>
<p>Recent studies have combined Large Language Models (LLMs) with Knowledge Graphs (KGs) to enhance reasoning, improving inference accuracy without additional training while mitigating hallucination. However, existing frameworks are often rigid, struggling to adapt to KG or task changes. They also rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning. To address this, We introduce R2-KG, a plug-and-play, dual-agent framework that separates reasoning into two roles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor (a high-capacity LLM) that makes final judgments. This design is cost-efficient for LLM inference while still maintaining strong reasoning accuracy. Additionally, R2-KG employs an Abstention mechanism, generating answers only when sufficient evidence is collected from KG, which significantly enhances reliability. Experiments across multiple KG-based reasoning tasks show that R2-KG consistently outperforms baselines in both accuracy and reliability, regardless of the inherent capability of LLMs used as the Operator. Further experiments reveal that the single-agent version of R2-KG, equipped with a strict self-consistency strategy, achieves significantly higher-than-baseline reliability while reducing inference cost. However, it also leads to a higher abstention rate in complex KGs. Our findings establish R2-KG as a flexible and cost-effective solution for KG-based reasoning. It reduces reliance on high-capacity LLMs while ensuring trustworthy inference. </p>
<blockquote>
<p>近期研究已将大型语言模型（LLM）与知识图谱（KG）相结合，以提升推理能力，可在不额外训练的情况下提高推理准确性，同时减轻虚构现象。然而，现有框架通常较为僵化，难以适应知识图谱或任务变更。它们还严重依赖强大的LLM进行可靠（即值得信赖）的推理。为解决这一问题，我们推出了R2-KG，一个即插即用的双代理框架，将推理分为两个角色：一个负责收集证据的运营商（低容量LLM）和一个进行最终判定的监督者（高容量LLM）。这种设计在LLM推理方面是成本效益较高的，同时仍能保持强大的推理准确性。此外，R2-KG采用了一种弃权机制，仅在从知识图谱收集到足够证据时才生成答案，这显著提高了可靠性。在多个基于知识图谱的推理任务上的实验表明，R2-KG在准确性和可靠性方面始终优于基准线，无论使用作为操作员（Operator）的LLM的固有能力如何。进一步的实验表明，配备有严格自我一致性策略的单一代理版本的R2-KG在可靠性方面大大超过基线水平，同时降低了推理成本。然而，这也导致了在复杂知识图谱中的弃权率更高。我们的发现确立了R2-KG作为基于知识图谱推理的灵活且经济的解决方案。它降低了对高容量LLM的依赖，同时确保可信推断。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.12767v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）与知识图谱（KG）的结合提升了推理能力，提高了推理的准确性并缓解了虚构的问题。然而，现有框架缺乏灵活性，难以适应知识图谱或任务的变化，且高度依赖强大的LLM进行可靠推理。为解决这一问题，我们推出了R2-KG，一个即插即用的双代理框架，将推理分为两个角色：进行证据搜集的操作员（低容量LLM）和做出最终判断的监督员（高容量LLM）。此设计在保持强大的推理准确性的同时，降低了LLM的推理成本。此外，R2-KG采用拒绝机制，仅在从知识图谱收集到足够证据时才生成答案，显著提高了可靠性。在多个基于知识图谱的推理任务上的实验表明，R2-KG在准确性和可靠性方面始终优于基线，无论使用的操作员LLM的能力如何。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>R2-KG结合了大型语言模型（LLMs）和知识图谱（KGs），提升了推理能力。</li>
<li>现有框架存在缺乏灵活性和适应性的挑战。</li>
<li>R2-KG通过分离推理角色为操作员和监督员来解决这些问题，提高推理准确性并降低成本。</li>
<li>R2-KG采用拒绝机制，仅在有足够证据时生成答案，增强了可靠性。</li>
<li>实验表明，R2-KG在多个基于知识图谱的推理任务上表现优于基线。</li>
<li>R2-KG在复杂知识图谱中的拒绝率相对较高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.12767">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a6d19e10e57fe6b34da29c78421a0ded.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-390db9da421728a359b8163248d907a2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-393642f767d54684c51feb71db96f357.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ACC-Collab-An-Actor-Critic-Approach-to-Multi-Agent-LLM-Collaboration"><a href="#ACC-Collab-An-Actor-Critic-Approach-to-Multi-Agent-LLM-Collaboration" class="headerlink" title="ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration"></a>ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration</h2><p><strong>Authors:Andrew Estornell, Jean-Francois Ton, Yuanshun Yao, Yang Liu</strong></p>
<p>Large language models (LLMs) have demonstrated a remarkable ability to serve as general-purpose tools for various language-based tasks. Recent works have demonstrated that the efficacy of such models can be improved through iterative dialog between multiple models. While these paradigms show promise in improving model efficacy, most works in this area treat collaboration as an emergent behavior, rather than a learned behavior. In doing so, current multi-agent frameworks rely on collaborative behaviors to have been sufficiently trained into off-the-shelf models. To address this limitation, we propose ACC-Collab, an Actor-Critic based learning framework to produce a two-agent team (an actor-agent and a critic-agent) specialized in collaboration. We demonstrate that ACC-Collab outperforms SotA multi-agent techniques on a wide array of benchmarks. </p>
<blockquote>
<p>大型语言模型（LLM）已经表现出作为各种语言基础任务的通用工具的显著能力。最近的研究表明，通过多个模型之间的迭代对话可以提高此类模型的有效性。虽然这些范式在提高模型有效性方面显示出潜力，但该领域的大多数工作都将协作视为一种突发行为，而非学习行为。因此，当前的多智能体框架依赖于协作行为已经充分训练为现成的模型。为了解决这个问题，我们提出了ACC-Collab，这是一种基于Actor-Critic的学习框架，用于生成一个专门从事协作的两智能体团队（一个Actor智能体和一个Critic智能体）。我们证明，在广泛的基准测试中，ACC-Collab的表现优于最新多智能体技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00053v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）具有作为多种语言任务通用工具的能力。最新研究表明，通过多个模型之间的迭代对话可以提高模型的效率。然而，当前的多代理框架依赖于协作行为的充分训练，而协作被视为一种新兴行为而非学习行为。为解决这一局限性，我们提出了基于Actor-Critic学习的ACC-Collab框架，构建了一支由两个代理组成的团队（一个演员代理和一个评论家代理），并擅长协作。我们证明了ACC-Collab在广泛的基准测试中优于现有先进的多代理技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）具有多种语言任务的通用工具能力。</li>
<li>通过多个模型之间的迭代对话可以提高模型的效率。</li>
<li>当前的多代理框架依赖协作行为的充分训练，但协作被视为新兴行为而非学习行为。</li>
<li>提出了基于Actor-Critic学习的ACC-Collab框架来解决这一局限性。</li>
<li>ACC-Collab框架由两个代理组成：一个演员代理和一个评论家代理。</li>
<li>演员代理和评论家代理组成的团队擅长协作。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.00053">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c7eb8933593370723397907a2b0cfb45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd6f88554a948be927d3394cd6f6e7c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f17130aa9ebabb6ef12a9e0e8c4572e8.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="How-Far-Are-We-on-the-Decision-Making-of-LLMs-Evaluating-LLMs’-Gaming-Ability-in-Multi-Agent-Environments"><a href="#How-Far-Are-We-on-the-Decision-Making-of-LLMs-Evaluating-LLMs’-Gaming-Ability-in-Multi-Agent-Environments" class="headerlink" title="How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming   Ability in Multi-Agent Environments"></a>How Far Are We on the Decision-Making of LLMs? Evaluating LLMs’ Gaming   Ability in Multi-Agent Environments</h2><p><strong>Authors:Jen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Michael R. Lyu</strong></p>
<p>Decision-making is a complex process requiring diverse abilities, making it an excellent framework for evaluating Large Language Models (LLMs). Researchers have examined LLMs’ decision-making through the lens of Game Theory. However, existing evaluation mainly focus on two-player scenarios where an LLM competes against another. Additionally, previous benchmarks suffer from test set leakage due to their static design. We introduce GAMA($\gamma$)-Bench, a new framework for evaluating LLMs’ Gaming Ability in Multi-Agent environments. It includes eight classical game theory scenarios and a dynamic scoring scheme specially designed to quantitatively assess LLMs’ performance. $\gamma$-Bench allows flexible game settings and adapts the scoring system to different game parameters, enabling comprehensive evaluation of robustness, generalizability, and strategies for improvement. Our results indicate that GPT-3.5 demonstrates strong robustness but limited generalizability, which can be enhanced using methods like Chain-of-Thought. We also evaluate 13 LLMs from 6 model families, including GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2. Gemini-1.5-Pro outperforms others, scoring of $69.8$ out of $100$, followed by LLaMA-3.1-70B ($65.9$) and Mixtral-8x22B ($62.4$). Our code and experimental results are publicly available at <a target="_blank" rel="noopener" href="https://github.com/CUHK-ARISE/GAMABench">https://github.com/CUHK-ARISE/GAMABench</a>. </p>
<blockquote>
<p>决策是一个需要多种能力的复杂过程，因此它是评估大型语言模型（LLM）的绝佳框架。研究人员已经通过博弈论的角度研究了LLM的决策制定。然而，现有的评估主要集中在两人场景中，即LLM与其他LLM之间的竞争。此外，先前的基准测试由于其静态设计而遭受测试集泄露的问题。我们推出了GAMA($\gamma$)-Bench，这是一个新的评估LLM在多智能体环境中的游戏能力的框架。它包括八个经典的游戏理论场景和一个专门设计的动态评分方案，以定量评估LLM的性能。$\gamma$-Bench允许灵活的游戏设置，并适应不同的游戏参数来调整评分系统，从而全面评估LLM的稳健性、泛化能力和改进策略。我们的结果表明，GPT-3.5表现出强大的稳健性，但泛化能力有限，可以通过如“思维链”等方法加以增强。我们还评估了来自6个模型家族的13个LLM，包括GPT-3.5、GPT-4、双子座、LLaMA-3.1、Mixtral和Qwen-2。其中，双子座-专业版表现最佳，得分为满分一百中的69.8分，其次是LLaMA-3.1（得分65.9）和Mixtral（得分62.4）。我们的代码和实验结果可在<a target="_blank" rel="noopener" href="https://github.com/CUHK-ARISE/GAMABench%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/CUHK-ARISE/GAMABench上公开获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.11807v7">PDF</a> Accepted to ICLR 2025; 11 pages of main text; 26 pages of appendices;   Included models: GPT-3.5-{0613, 1106, 0125}, GPT-4-0125, GPT-4o-0806,   Gemini-{1.0, 1.5)-Pro, LLaMA-3.1-{7, 70, 405}B, Mixtral-8x{7, 22}B,   Qwen-2-72B</p>
<p><strong>Summary</strong><br>决策制定是一个需要多种能力的复杂过程，因此成为评估大型语言模型（LLM）的理想框架。研究者通过博弈论的角度研究LLM的决策制定能力。现有评估主要集中在两人场景中，即LLM与另一个智能体的竞争情境。但先前的评价指标因设计静态而易出现测试集泄露问题。为此，我们推出了GAMA($\gamma$)-Bench新框架，用于在多变体环境中评估LLM的游戏能力。它包含八个经典博弈论场景和特别设计的动态评分系统，定量评估LLM的表现。$\gamma$-Bench框架允许灵活的游戏设置，并能适应不同的游戏参数调整评分系统，全面评估LLM的稳健性、通用性和改进策略。研究发现GPT-3.5展现出强大的稳健性但通用性有限，而通过使用链思维等方法可以增强其通用性。我们还评估了来自六个模型家族的13个LLM的性能排名，并公开了相关代码和实验结果。<a target="_blank" rel="noopener" href="https://github.com/CUHK-ARISE/GAMABench%E3%80%82">https://github.com/CUHK-ARISE/GAMABench。</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型（LLM）的决策制定能力通过博弈论角度进行研究。</li>
<li>现有评估框架主要集中在两人场景中，存在测试集泄露问题。</li>
<li>引入GAMA($\gamma$)-Bench新框架用于多变体环境中评估LLM的游戏能力。</li>
<li>$\gamma$-Bench包含多个经典博弈论场景和动态评分系统，可全面评估LLM的稳健性、通用性和策略改进。</li>
<li>GPT-3.5展现出强大的稳健性但通用性有限，可通过特定方法如链思维增强通用性。</li>
<li>评估了多个LLM的性能排名，包括GPT系列和其他模型家族。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.11807">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-32615f9c7bf0fd0e69ac299ee7e77cda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8efba2ecd89df792cbb3223412e25fd3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3747aa23e21f63b5f58d1f6fc8f8982.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DoraemonGPT-Toward-Understanding-Dynamic-Scenes-with-Large-Language-Models-Exemplified-as-A-Video-Agent"><a href="#DoraemonGPT-Toward-Understanding-Dynamic-Scenes-with-Large-Language-Models-Exemplified-as-A-Video-Agent" class="headerlink" title="DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language   Models (Exemplified as A Video Agent)"></a>DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language   Models (Exemplified as A Video Agent)</h2><p><strong>Authors:Zongxin Yang, Guikun Chen, Xiaodi Li, Wenguan Wang, Yi Yang</strong></p>
<p>Recent LLM-driven visual agents mainly focus on solving image-based tasks, which limits their ability to understand dynamic scenes, making it far from real-life applications like guiding students in laboratory experiments and identifying their mistakes. Hence, this paper explores DoraemonGPT, a comprehensive and conceptually elegant system driven by LLMs to understand dynamic scenes. Considering the video modality better reflects the ever-changing nature of real-world scenarios, we exemplify DoraemonGPT as a video agent. Given a video with a question&#x2F;task, DoraemonGPT begins by converting the input video into a symbolic memory that stores task-related attributes. This structured representation allows for spatial-temporal querying and reasoning by well-designed sub-task tools, resulting in concise intermediate results. Recognizing that LLMs have limited internal knowledge when it comes to specialized domains (e.g., analyzing the scientific principles underlying experiments), we incorporate plug-and-play tools to assess external knowledge and address tasks across different domains. Moreover, a novel LLM-driven planner based on Monte Carlo Tree Search is introduced to explore the large planning space for scheduling various tools. The planner iteratively finds feasible solutions by backpropagating the result’s reward, and multiple solutions can be summarized into an improved final answer. We extensively evaluate DoraemonGPT’s effectiveness on three benchmarks and several in-the-wild scenarios. The code will be released at <a target="_blank" rel="noopener" href="https://github.com/z-x-yang/DoraemonGPT">https://github.com/z-x-yang/DoraemonGPT</a>. </p>
<blockquote>
<p>最近的LLM驱动视觉代理主要专注于解决基于图像的任务，这限制了它们对动态场景的理解能力，因此难以应用于指导实验室实验和学生纠错等现实生活场景。因此，本文探讨了DoraemonGPT这一由LLM驱动的全面而概念优雅的系统，以理解动态场景。考虑到视频模式能更好地反映真实场景的不断变化性质，我们将DoraemonGPT作为视频代理进行示例。对于包含问题或任务的视频，DoraemonGPT首先将其转换为存储任务相关属性的符号记忆。这种结构化表示允许通过精心设计的工作表进行时空查询和推理，从而得到简洁的中间结果。我们认识到，在特定领域（例如分析实验背后的科学原理），LLM的内部知识是有限的，因此我们引入了即插即用工具来评估外部知识并解决不同领域的任务。此外，还介绍了一种基于蒙特卡洛树搜索的LLM驱动规划器，用于探索大量规划空间以调度各种工具。规划器通过反向传播结果的奖励来迭代寻找可行解决方案，并将多个解决方案总结为改进的最终答案。我们在三个基准测试和若干野外场景中广泛评估了DoraemonGPT的有效性。代码将在<a target="_blank" rel="noopener" href="https://github.com/z-x-yang/DoraemonGPT%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/z-x-yang/DoraemonGPT上发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.08392v4">PDF</a> </p>
<p><strong>Summary</strong><br>大热的自然语言驱动模型多应用于处理静态图像任务，对动态场景理解尚显不足，距离应用于真实世界场景如学生实验指导等存在较大差距。本研究以全新的LLM驱动的动态场景理解系统DoraemonGPT为切入，结合视频载体特点开展研究。通过转化视频为符号记忆，存储任务相关属性，利用子任务工具进行时空查询和推理，产生精准中间结果。发现特定领域知识受限于模型本身知识广度的问题后，本研究利用全新插拔式工具评估外部知识，拓宽模型应用领域。同时引入基于蒙特卡洛树搜索的LLM驱动规划器进行任务调度规划，利用结果反馈优化求解路径。通过三项基准测试及现实场景验证证明模型的有效性。模型代码已上传至GitHub。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM驱动的视觉代理模型在处理动态场景理解方面存在巨大潜力。</li>
<li>DoraemonGPT系统结合视频模态处理动态场景，具有强大的时空查询和推理能力。</li>
<li>通过将视频转化为符号记忆，DoraemonGPT能够存储任务相关属性并产生精准中间结果。</li>
<li>利用子任务工具集以完成特定的分析推理工作，从而提高效率并增加答案的全面性。</li>
<li>LLMS的知识范围受限于特定领域的问题得到解决，通过引入插拔式工具来评估外部知识并应用于不同领域任务。</li>
<li>利用蒙特卡洛树搜索技术的LLM驱动规划器有效解决了大规模任务规划问题。该规划器能够通过结果反馈进行迭代优化求解路径。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.08392">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ee02e491cc6155641a65e955be34b07b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7c2292768cc604cf99e9f03d987eaac1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a57e6fa4116078d4faeda552e88d23dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f172a8427a618d9920f886acd3582869.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9270b9eee5a705be400add8098fb3398.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2d5c304c76f86e7fd0d91f82559552ae.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-74fdbea47417467763438c3c007c1516.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-03-09  Compositional Translation A Novel LLM-based Approach for Low-resource   Machine Translation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-c7eb8933593370723397907a2b0cfb45.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-03-09  LLMVoX Autoregressive Streaming Text-to-Speech Model for Any LLM
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">18181.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
