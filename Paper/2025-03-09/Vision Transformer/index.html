<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-09  ViT-VS On the Applicability of Pretrained Vision Transformer Features   for Generalizable Visual Servoing">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-1cb6b2a7e3e81f209d843b99170518b1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    39 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-09-æ›´æ–°"><a href="#2025-03-09-æ›´æ–°" class="headerlink" title="2025-03-09 æ›´æ–°"></a>2025-03-09 æ›´æ–°</h1><h2 id="ViT-VS-On-the-Applicability-of-Pretrained-Vision-Transformer-Features-for-Generalizable-Visual-Servoing"><a href="#ViT-VS-On-the-Applicability-of-Pretrained-Vision-Transformer-Features-for-Generalizable-Visual-Servoing" class="headerlink" title="ViT-VS: On the Applicability of Pretrained Vision Transformer Features   for Generalizable Visual Servoing"></a>ViT-VS: On the Applicability of Pretrained Vision Transformer Features   for Generalizable Visual Servoing</h2><p><strong>Authors:Alessandro Scherl, Stefan Thalhammer, Bernhard Neuberger, Wilfried WÃ¶ber, JosÃ© GracÃ­a-RodrÃ­guez</strong></p>
<p>Visual servoing enables robots to precisely position their end-effector relative to a target object. While classical methods rely on hand-crafted features and thus are universally applicable without task-specific training, they often struggle with occlusions and environmental variations, whereas learning-based approaches improve robustness but typically require extensive training. We present a visual servoing approach that leverages pretrained vision transformers for semantic feature extraction, combining the advantages of both paradigms while also being able to generalize beyond the provided sample. Our approach achieves full convergence in unperturbed scenarios and surpasses classical image-based visual servoing by up to 31.2% relative improvement in perturbed scenarios. Even the convergence rates of learning-based methods are matched despite requiring no task- or object-specific training. Real-world evaluations confirm robust performance in end-effector positioning, industrial box manipulation, and grasping of unseen objects using only a reference from the same category. Our code and simulation environment are available at: <a target="_blank" rel="noopener" href="https://alessandroscherl.github.io/ViT-VS/">https://alessandroscherl.github.io/ViT-VS/</a> </p>
<blockquote>
<p>è§†è§‰ä¼ºæœæŠ€æœ¯ä½¿æœºå™¨äººèƒ½å¤Ÿç²¾ç¡®åœ°å°†å…¶æœ«ç«¯æ‰§è¡Œå™¨å®šä½åˆ°ç›®æ ‡å¯¹è±¡ä¸Šã€‚è™½ç„¶ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºæ‰‹å·¥ç‰¹å¾ï¼Œå› æ­¤æ— éœ€ç‰¹å®šä»»åŠ¡è®­ç»ƒå³å¯æ™®éé€‚ç”¨ï¼Œä½†å®ƒä»¬é€šå¸¸éš¾ä»¥å¤„ç†é®æŒ¡å’Œç¯å¢ƒå˜åŒ–ï¼Œè€ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•æé«˜äº†ç¨³å¥æ€§ï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡è®­ç»ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰å˜æ¢å™¨è¿›è¡Œè¯­ä¹‰ç‰¹å¾æå–çš„è§†è§‰ä¼ºæœæ–¹æ³•ï¼Œç»“åˆäº†ä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹ï¼ŒåŒæ—¶èƒ½å¤Ÿæ¨å¹¿åˆ°æ ·æœ¬ä¹‹å¤–ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ— å¹²æ‰°åœºæ™¯ä¸­å®ç°äº†å®Œå…¨æ”¶æ•›ï¼Œåœ¨å¹²æ‰°åœºæ™¯ä¸­ç›¸å¯¹äºä¼ ç»Ÿçš„åŸºäºå›¾åƒçš„è§†è§‰ä¼ºæœæ–¹æ³•æé«˜äº†é«˜è¾¾3 1.2%çš„æ”¹è¿›ã€‚å³ä½¿æ²¡æœ‰ç‰¹å®šä»»åŠ¡æˆ–å¯¹è±¡çš„è®­ç»ƒï¼Œä¹Ÿèƒ½è¾¾åˆ°åŸºäºå­¦ä¹ æ–¹æ³•ç›¸å½“çš„æ”¶æ•›ç‡ã€‚ç°å®ä¸–ç•Œçš„è¯„ä¼°è¯å®äº†åœ¨æœ«ç«¯æ‰§è¡Œå™¨å®šä½ã€å·¥ä¸šç®±ä½“æ“ä½œå’Œä»…ä½¿ç”¨åŒä¸€ç±»åˆ«å‚è€ƒè¿›è¡ŒæœªçŸ¥ç‰©ä½“æŠ“å–æ–¹é¢çš„ç¨³å¥æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å’Œä»¿çœŸç¯å¢ƒå¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://alessandroscherl.github.io/ViT-VS/">https://alessandroscherl.github.io/ViT-VS/</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04545v1">PDF</a> </p>
<p><strong>Summary</strong><br>è§†è§‰ä¼ºæœæŠ€æœ¯ä½¿æœºå™¨äººèƒ½å¤Ÿç²¾ç¡®åœ°å°†æœ«ç«¯æ‰§è¡Œå™¨å®šä½åˆ°ç›®æ ‡å¯¹è±¡ã€‚ç»“åˆé¢„è®­ç»ƒçš„è§†è§‰å˜å‹å™¨è¿›è¡Œè¯­ä¹‰ç‰¹å¾æå–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†ç»å…¸æ–¹æ³•å’ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œæ— éœ€ç‰¹å®šçš„ä»»åŠ¡æˆ–å¯¹è±¡è®­ç»ƒå°±èƒ½å®ç°è¶…è¶Šæ ·æœ¬çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å—å¹²æ‰°åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸å¯¹äºä¼ ç»Ÿçš„å›¾åƒè§†è§‰ä¼ºæœæŠ€æœ¯æé«˜äº†é«˜è¾¾31.2%çš„æ€§èƒ½ã€‚åœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­ï¼Œå®ƒåœ¨æœ«ç«¯æ‰§è¡Œå™¨å®šä½ã€å·¥ä¸šç®±æ“ä½œå’ŒåŒä¸€ç±»åˆ«å¯¹è±¡çš„æŠ“å–ä¸­è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§ç»“åˆé¢„è®­ç»ƒè§†è§‰å˜å‹å™¨è¿›è¡Œè¯­ä¹‰ç‰¹å¾æå–çš„è§†è§‰ä¼ºæœæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†ç»å…¸æ–¹æ³•å’ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œæ— éœ€ç‰¹å®šçš„ä»»åŠ¡æˆ–å¯¹è±¡è®­ç»ƒå°±èƒ½å®ç°æ³›åŒ–ã€‚</li>
<li>åœ¨æœªå—å¹²æ‰°çš„åœºæ™¯ä¸­ï¼Œè¯¥æ–¹æ³•å®ç°äº†å®Œå…¨çš„æ”¶æ•›ã€‚</li>
<li>åœ¨å—å¹²æ‰°åœºæ™¯ä¸­ï¼Œè¯¥æ–¹æ³•ç›¸å¯¹äºä¼ ç»Ÿå›¾åƒè§†è§‰ä¼ºæœæŠ€æœ¯æé«˜äº†é«˜è¾¾31.2%çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æœºå™¨äººæœ«ç«¯æ‰§è¡Œå™¨å®šä½ã€å·¥ä¸šç®±æ“ä½œå’ŒåŒä¸€ç±»åˆ«å¯¹è±¡çš„æŠ“å–ä¸­è¿›è¡Œäº†ç°å®ä¸–ç•Œçš„è¯„ä¼°ï¼Œè¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†å„ç§ç¯å¢ƒå’Œå¯¹è±¡çš„å˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04545">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0abd72b4a293ba70387bea2b1adcc27d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6670b19de82be581c00e2c74a33ece4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6a802202d81b54270a90307d8df3228.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20bcb62dc6aed8ec150ad6ed4e639cf2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="GBT-SAM-A-Parameter-Efficient-Depth-Aware-Model-for-Generalizable-Brain-tumour-Segmentation-on-mp-MRI"><a href="#GBT-SAM-A-Parameter-Efficient-Depth-Aware-Model-for-Generalizable-Brain-tumour-Segmentation-on-mp-MRI" class="headerlink" title="GBT-SAM: A Parameter-Efficient Depth-Aware Model for Generalizable Brain   tumour Segmentation on mp-MRI"></a>GBT-SAM: A Parameter-Efficient Depth-Aware Model for Generalizable Brain   tumour Segmentation on mp-MRI</h2><p><strong>Authors:Cecilia Diana-Albelda, Roberto Alcover-Couso, Ãlvaro GarcÃ­a-MartÃ­n, Jesus Bescos, Marcos Escudero-ViÃ±olo</strong></p>
<p>Gliomas are brain tumours that stand out for their highly lethal and aggressive nature, which demands a precise approach in their diagnosis. Medical image segmentation plays a crucial role in the evaluation and follow-up of these tumours, allowing specialists to analyse their morphology. However, existing methods for automatic glioma segmentation often lack generalization capability across other brain tumour domains, require extensive computational resources, or fail to fully utilize the multi-parametric MRI (mp-MRI) data used to delineate them. In this work, we introduce GBT-SAM, a novel Generalizable Brain Tumour (GBT) framework that extends the Segment Anything Model (SAM) to brain tumour segmentation tasks. Our method employs a two-step training protocol: first, fine-tuning the patch embedding layer to process the entire mp-MRI modalities, and second, incorporating parameter-efficient LoRA blocks and a Depth-Condition block into the Vision Transformer (ViT) to capture inter-slice correlations. GBT-SAM achieves state-of-the-art performance on the Adult Glioma dataset (Dice Score of $93.54$) while demonstrating robust generalization across Meningioma, Pediatric Glioma, and Sub-Saharan Glioma datasets. Furthermore, GBT-SAM uses less than 6.5M trainable parameters, thus offering an efficient solution for brain tumour segmentation. \ Our code and models are available at <a target="_blank" rel="noopener" href="https://github.com/vpulab/med-sam-brain">https://github.com/vpulab/med-sam-brain</a> . </p>
<blockquote>
<p>èƒ¶è´¨ç˜¤æ˜¯æ¶æ€§ç¨‹åº¦é«˜ã€ä¾µè¢­æ€§å¼ºçš„è„‘è‚¿ç˜¤ï¼Œå¯¹å…¶è¯Šæ–­éœ€è¦ç²¾ç¡®çš„æ–¹æ³•ã€‚åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨è¯„ä¼°å’Œæ²»ç–—è¿™äº›è‚¿ç˜¤ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œå…è®¸ä¸“å®¶åˆ†æå®ƒä»¬çš„å½¢æ€ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è‡ªåŠ¨èƒ¶è´¨ç˜¤åˆ†å‰²æ–¹æ³•å¾€å¾€ç¼ºä¹åœ¨å…¶ä»–è„‘è‚¿ç˜¤é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œæˆ–è€…æœªèƒ½å……åˆ†åˆ©ç”¨ç”¨äºç•Œå®šè‚¿ç˜¤çš„å¤šå‚æ•°MRIï¼ˆmp-MRIï¼‰æ•°æ®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†GBT-SAMï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ³›åŒ–çš„è„‘è‚¿ç˜¤ï¼ˆGBTï¼‰æ¡†æ¶ï¼Œå®ƒå°†Segment Anything Modelï¼ˆSAMï¼‰æ‰©å±•åˆ°è„‘è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä¸¤æ­¥è®­ç»ƒåè®®ï¼šé¦–å…ˆï¼Œå¾®è°ƒè¡¥ä¸åµŒå…¥å±‚ä»¥å¤„ç†æ•´ä¸ªmp-MRIæ¨¡å¼ï¼›å…¶æ¬¡ï¼Œåœ¨è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ä¸­èå…¥å‚æ•°é«˜æ•ˆçš„LoRAå—å’Œæ·±åº¦æ¡ä»¶å—ï¼Œä»¥æ•æ‰åˆ‡ç‰‡é—´çš„ç›¸å…³æ€§ã€‚GBT-SAMåœ¨æˆäººèƒ¶è´¨ç˜¤æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆDiceå¾—åˆ†ä¸º93.54%ï¼‰ï¼ŒåŒæ—¶åœ¨è„‘è†œç˜¤ã€å„¿ç«¥èƒ¶è´¨ç˜¤å’Œæ’’å“ˆæ‹‰ä»¥å—èƒ¶è´¨ç˜¤æ•°æ®é›†ä¸Šå±•ç¤ºäº†ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒGBT-SAMä½¿ç”¨çš„å¯è®­ç»ƒå‚æ•°å°‘äº650ä¸‡ï¼Œå› æ­¤ä¸ºè„‘è‚¿ç˜¤åˆ†å‰²æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/vpulab/med-sam-brain%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/vpulab/med-sam-brainä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04325v1">PDF</a> </p>
<p><strong>Summary</strong><br>èƒ¶è´¨æ¯ç»†èƒç˜¤æ˜¯æ¶æ€§ç¨‹åº¦é«˜ã€ä¾µè¢­æ€§å¼ºçš„è„‘è‚¿ç˜¤ï¼Œå¯¹å…¶è¯Šæ–­éœ€è¦ç²¾å‡†çš„æ–¹æ³•ã€‚åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨è¯„ä¼°ä¸è¿½è¸ªè¿™äº›è‚¿ç˜¤ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œä½¿ä¸“å®¶èƒ½å¤Ÿåˆ†æå®ƒä»¬çš„å½¢æ€ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è‡ªåŠ¨èƒ¶è´¨æ¯ç»†èƒç˜¤åˆ†å‰²æ–¹æ³•å¾€å¾€ç¼ºä¹è·¨å…¶ä»–è„‘è‚¿ç˜¤é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ï¼Œéœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºï¼Œæˆ–æœªèƒ½å……åˆ†åˆ©ç”¨å¤šå‚æ•°MRIæ•°æ®è¿›è¡Œåˆ†å‰²ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§æ–°å‹é€šç”¨è„‘è‚¿ç˜¤åˆ†å‰²æ¡†æ¶GBT-SAMï¼Œè¯¥æ¡†æ¶æ‰©å±•äº†SAMç”¨äºè„‘è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤æ­¥è®­ç»ƒåè®®ï¼Œé¦–å…ˆå¾®è°ƒè¡¥ä¸åµŒå…¥å±‚ä»¥å¤„ç†æ•´ä¸ªå¤šå‚æ•°MRIæ¨¡æ€æ•°æ®ï¼Œç„¶åèå…¥å‚æ•°é«˜æ•ˆçš„LoRAå—å’Œæ·±åº¦æ¡ä»¶å—åˆ°è§†è§‰è½¬æ¢å™¨ä¸­ï¼Œä»¥æ•æ‰åˆ‡ç‰‡é—´çš„ç›¸å…³æ€§ã€‚GBT-SAMåœ¨æˆäººèƒ¶è´¨æ¯ç»†èƒç˜¤æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆDiceåˆ†æ•°ä¸º93.54%ï¼‰ï¼Œå¹¶åœ¨è„‘è†œç˜¤ã€å„¿ç«¥èƒ¶è´¨æ¯ç»†èƒç˜¤å’Œæ’’å“ˆæ‹‰ä»¥å—èƒ¶è´¨æ¯ç»†èƒç˜¤æ•°æ®é›†ä¸Šè¡¨ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒGBT-SAMä½¿ç”¨çš„å¯è®­ç»ƒå‚æ•°å°‘äº650ä¸‡ï¼Œä¸ºè„‘è‚¿ç˜¤åˆ†å‰²æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èƒ¶è´¨æ¯ç»†èƒç˜¤æ˜¯æ¶æ€§ç¨‹åº¦é«˜ã€ä¾µè¢­æ€§å¼ºçš„è„‘è‚¿ç˜¤ï¼Œéœ€è¦ç²¾å‡†è¯Šæ–­ã€‚</li>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨è¯„ä¼°ä¸è¿½è¸ªè„‘è‚¿ç˜¤ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>ç°æœ‰è‡ªåŠ¨èƒ¶è´¨æ¯ç»†èƒç˜¤åˆ†å‰²æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œå¦‚ç¼ºä¹æ³›åŒ–èƒ½åŠ›ã€è®¡ç®—èµ„æºéœ€æ±‚å¤§ï¼Œä»¥åŠæœªèƒ½å……åˆ†åˆ©ç”¨å¤šå‚æ•°MRIæ•°æ®ã€‚</li>
<li>GBT-SAMæ˜¯ä¸€ç§æ–°å‹è„‘è‚¿ç˜¤åˆ†å‰²æ¡†æ¶ï¼Œæ‰©å±•äº†SAMç”¨äºè„‘è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>GBT-SAMé‡‡ç”¨ä¸¤æ­¥è®­ç»ƒåè®®ï¼ŒåŒ…æ‹¬å¾®è°ƒè¡¥ä¸åµŒå…¥å±‚å’Œèå…¥å‚æ•°é«˜æ•ˆçš„LoRAå—å’Œæ·±åº¦æ¡ä»¶å—åˆ°è§†è§‰è½¬æ¢å™¨ä¸­ã€‚</li>
<li>GBT-SAMåœ¨å¤šä¸ªè„‘è‚¿ç˜¤æ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜è¶Šæ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04325">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94f7bc94ad4d039e0a1b7810c53f0fe1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bf322f8050f3b09629c75e5bf0791fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-750f89e2323cf1001dcd372ff31c1a77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5966b058c28bc67abfd53731528eb601.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Synthetic-Data-is-an-Elegant-GIFT-for-Continual-Vision-Language-Models"><a href="#Synthetic-Data-is-an-Elegant-GIFT-for-Continual-Vision-Language-Models" class="headerlink" title="Synthetic Data is an Elegant GIFT for Continual Vision-Language Models"></a>Synthetic Data is an Elegant GIFT for Continual Vision-Language Models</h2><p><strong>Authors:Bin Wu, Wuxuan Shi, Jinqiao Wang, Mang Ye</strong></p>
<p>Pre-trained Vision-Language Models (VLMs) require Continual Learning (CL) to efficiently update their knowledge and adapt to various downstream tasks without retraining from scratch. However, for VLMs, in addition to the loss of knowledge previously learned from downstream tasks, pre-training knowledge is also corrupted during continual fine-tuning. This issue is exacerbated by the unavailability of original pre-training data, leaving VLMâ€™s generalization ability degrading. In this paper, we propose GIFT, a novel continual fine-tuning approach that utilizes synthetic data to overcome catastrophic forgetting in VLMs. Taking advantage of recent advances in text-to-image synthesis, we employ a pre-trained diffusion model to recreate both pre-training and learned downstream task data. In this way, the VLM can revisit previous knowledge through distillation on matching diffusion-generated images and corresponding text prompts. Leveraging the broad distribution and high alignment between synthetic image-text pairs in VLMâ€™s feature space, we propose a contrastive distillation loss along with an image-text alignment constraint. To further combat in-distribution overfitting and enhance distillation performance with limited amount of generated data, we incorporate adaptive weight consolidation, utilizing Fisher information from these synthetic image-text pairs and achieving a better stability-plasticity balance. Extensive experiments demonstrate that our method consistently outperforms previous state-of-the-art approaches across various settings. </p>
<blockquote>
<p>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰éœ€è¦æŒç»­å­¦ä¹ ï¼ˆCLï¼‰æ¥æœ‰æ•ˆåœ°æ›´æ–°å…¶çŸ¥è¯†ï¼Œå¹¶é€‚åº”å„ç§ä¸‹æ¸¸ä»»åŠ¡è€Œæ— éœ€ä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒã€‚ç„¶è€Œï¼Œå¯¹äºVLMsè€Œè¨€ï¼Œé™¤äº†ä¹‹å‰ä»ä¸‹æ¸¸ä»»åŠ¡ä¸­å­¦åˆ°çš„çŸ¥è¯†ä¸¢å¤±ä¹‹å¤–ï¼ŒæŒç»­å¾®è°ƒè¿‡ç¨‹ä¸­è¿˜ä¼šç ´åé¢„è®­ç»ƒçŸ¥è¯†ã€‚è¿™ä¸€é—®é¢˜å› æ— æ³•è·å¾—åŸå§‹é¢„è®­ç»ƒæ•°æ®è€ŒåŠ å‰§ï¼Œå¯¼è‡´VLMçš„æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†GIFTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æŒç»­å¾®è°ƒæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨åˆæˆæ•°æ®æ¥å…‹æœVLMä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚æˆ‘ä»¬åˆ©ç”¨æœ€æ–°çš„æ–‡æœ¬åˆ°å›¾åƒåˆæˆçš„è¿›å±•ï¼Œé‡‡ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ¥é‡æ–°åˆ›å»ºé¢„è®­ç»ƒå’Œå·²å­¦ä¹ çš„ä¸‹æ¸¸ä»»åŠ¡æ•°æ®ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒVLMå¯ä»¥é€šè¿‡å¯¹åŒ¹é…çš„æ‰©æ•£ç”Ÿæˆå›¾åƒå’Œç›¸åº”çš„æ–‡æœ¬æç¤ºè¿›è¡Œè’¸é¦æ¥é‡æ¸©ä»¥å‰çš„çŸ¥è¯†ã€‚æˆ‘ä»¬å‡­å€Ÿåˆæˆå›¾åƒ-æ–‡æœ¬å¯¹åœ¨VLMç‰¹å¾ç©ºé—´ä¸­çš„å¹¿æ³›åˆ†å¸ƒå’Œé«˜å¯¹é½åº¦ï¼Œæå‡ºäº†å¯¹æ¯”è’¸é¦æŸå¤±å’Œå›¾åƒ-æ–‡æœ¬å¯¹é½çº¦æŸã€‚ä¸ºäº†è¿›ä¸€æ­¥å¯¹æŠ—å†…éƒ¨åˆ†å¸ƒè¿‡æ‹Ÿåˆé—®é¢˜å¹¶å¢å¼ºåœ¨æœ‰é™ç”Ÿæˆæ•°æ®ä¸‹çš„è’¸é¦æ€§èƒ½ï¼Œæˆ‘ä»¬ç»“åˆäº†è‡ªé€‚åº”æƒé‡æ•´åˆï¼Œåˆ©ç”¨è¿™äº›åˆæˆå›¾åƒ-æ–‡æœ¬å¯¹çš„Fisherä¿¡æ¯ï¼Œå®ç°äº†æ›´å¥½çš„ç¨³å®šæ€§-å¯å¡‘æ€§å¹³è¡¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§è®¾ç½®ä¸‹å‡ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04229v1">PDF</a> This work is accepted by CVPR 2025. Modifications may be performed</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æ¢è®¨äº†åœ¨ç¼ºä¹åŸå§‹é¢„è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œé¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æŒç»­å­¦ä¹ ï¼ˆCLï¼‰è¿‡ç¨‹ä¸­é¢ä¸´çš„çŸ¥è¯†ä¸¢å¤±å’Œé¢„è®­ç»ƒçŸ¥è¯†è¢«è…èš€çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æŒç»­å¾®è°ƒæ–¹æ³•GIFTï¼Œåˆ©ç”¨åˆæˆæ•°æ®å…‹æœVLMsä¸­çš„ç¾éš¾æ€§é—å¿˜ã€‚é€šè¿‡åˆ©ç”¨æœ€æ–°çš„æ–‡æœ¬åˆ°å›¾åƒåˆæˆæŠ€æœ¯ï¼Œä»¥åŠä¸€ä¸ªé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œæ¥é‡æ–°åˆ›å»ºé¢„è®­ç»ƒæ•°æ®å’Œå·²å­¦ä¹ çš„ä¸‹æ¸¸ä»»åŠ¡æ•°æ®ã€‚VLMå¯ä»¥é€šè¿‡å¯¹åŒ¹é…çš„æ‰©æ•£ç”Ÿæˆå›¾åƒå’Œç›¸åº”çš„æ–‡æœ¬æç¤ºè¿›è¡Œè’¸é¦ï¼Œé‡æ–°è·å¾—ä¹‹å‰çš„çŸ¥è¯†ã€‚åŒæ—¶ï¼Œæœ¬æ–‡æå‡ºäº†å¯¹æ¯”è’¸é¦æŸå¤±å’Œå›¾åƒæ–‡æœ¬å¯¹é½çº¦æŸï¼Œå¹¶åˆ©ç”¨åˆæˆå›¾åƒæ–‡æœ¬å¯¹çš„å¹¿æ³›åˆ†å¸ƒå’Œé«˜åº¦å¯¹é½ç‰¹æ€§ã€‚ä¸ºè¿›ä¸€æ­¥å¯¹æŠ—å†…éƒ¨åˆ†å¸ƒè¿‡æ‹Ÿåˆé—®é¢˜å¹¶å¢å¼ºæœ‰é™ç”Ÿæˆæ•°æ®çš„è’¸é¦æ€§èƒ½ï¼Œæœ¬æ–‡ç»“åˆäº†è‡ªé€‚åº”æƒé‡æ•´åˆï¼Œåˆ©ç”¨è¿™äº›åˆæˆå›¾åƒæ–‡æœ¬å¯¹çš„Fisherä¿¡æ¯ï¼Œå®ç°äº†æ›´å¥½çš„ç¨³å®šæ€§å¯å¡‘æ€§å¹³è¡¡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§è®¾ç½®ä¸‹å‡ä¼˜äºä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æŒç»­å­¦ä¹ ï¼ˆCLï¼‰è¿‡ç¨‹ä¸­é¢ä¸´çŸ¥è¯†ä¸¢å¤±çš„é—®é¢˜ã€‚</li>
<li>åœ¨ç¼ºä¹åŸå§‹é¢„è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œé¢„è®­ç»ƒçŸ¥è¯†ä¹Ÿä¼šè¢«è…èš€ã€‚</li>
<li>GIFTæ˜¯ä¸€ç§æ–°çš„æŒç»­å¾®è°ƒæ–¹æ³•ï¼Œåˆ©ç”¨åˆæˆæ•°æ®å…‹æœVLMsä¸­çš„ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹é‡æ–°åˆ›å»ºé¢„è®­ç»ƒæ•°æ®å’Œä¸‹æ¸¸ä»»åŠ¡æ•°æ®ã€‚</li>
<li>VLMå¯ä»¥é€šè¿‡å¯¹åŒ¹é…çš„æ‰©æ•£ç”Ÿæˆå›¾åƒå’Œç›¸åº”çš„æ–‡æœ¬æç¤ºè¿›è¡Œè’¸é¦ï¼Œé‡æ–°è·å¾—ä¹‹å‰çš„çŸ¥è¯†ã€‚</li>
<li>æå‡ºäº†å¯¹æ¯”è’¸é¦æŸå¤±å’Œå›¾åƒæ–‡æœ¬å¯¹é½çº¦æŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04229">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a9ef1f14fb63f74c76c82b4f3a845606.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f654ad0a8b76d9af538784bfadb15e64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ece8f619e0c4d9183827ae8a59829a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72a2591551c4ddc7a2b590ef2818d860.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="WeakMedSAM-Weakly-Supervised-Medical-Image-Segmentation-via-SAM-with-Sub-Class-Exploration-and-Prompt-Affinity-Mining"><a href="#WeakMedSAM-Weakly-Supervised-Medical-Image-Segmentation-via-SAM-with-Sub-Class-Exploration-and-Prompt-Affinity-Mining" class="headerlink" title="WeakMedSAM: Weakly-Supervised Medical Image Segmentation via SAM with   Sub-Class Exploration and Prompt Affinity Mining"></a>WeakMedSAM: Weakly-Supervised Medical Image Segmentation via SAM with   Sub-Class Exploration and Prompt Affinity Mining</h2><p><strong>Authors:Haoran Wang, Lian Huai, Wenbin Li, Lei Qi, Xingqun Jiang, Yinghuan Shi</strong></p>
<p>We have witnessed remarkable progress in foundation models in vision tasks. Currently, several recent works have utilized the segmenting anything model (SAM) to boost the segmentation performance in medical images, where most of them focus on training an adaptor for fine-tuning a large amount of pixel-wise annotated medical images following a fully supervised manner. In this paper, to reduce the labeling cost, we investigate a novel weakly-supervised SAM-based segmentation model, namely WeakMedSAM. Specifically, our proposed WeakMedSAM contains two modules: 1) to mitigate severe co-occurrence in medical images, a sub-class exploration module is introduced to learn accurate feature representations. 2) to improve the quality of the class activation maps, our prompt affinity mining module utilizes the prompt capability of SAM to obtain an affinity map for random-walk refinement. Our method can be applied to any SAM-like backbone, and we conduct experiments with SAMUS and EfficientSAM. The experimental results on three popularly-used benchmark datasets, i.e., BraTS 2019, AbdomenCT-1K, and MSD Cardiac dataset, show the promising results of our proposed WeakMedSAM. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/wanghr64/WeakMedSAM">https://github.com/wanghr64/WeakMedSAM</a>. </p>
<blockquote>
<p>åœ¨è§†è§‰ä»»åŠ¡çš„åŸºç¡€æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç›®å‰ï¼Œä¸€äº›æœ€æ–°çš„å·¥ä½œå·²ç»åˆ©ç”¨ä»»ä½•äº‹ç‰©åˆ†å‰²æ¨¡å‹ï¼ˆSAMï¼‰æ¥æé«˜åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ€§èƒ½ï¼Œå…¶ä¸­å¤§å¤šæ•°å·¥ä½œä¸»è¦é›†ä¸­åœ¨ä»¥å®Œå…¨ç›‘ç£çš„æ–¹å¼è®­ç»ƒé€‚é…å™¨ï¼Œå¯¹å¤§é‡çš„åƒç´ çº§æ³¨é‡ŠåŒ»å­¦å›¾åƒè¿›è¡Œå¾®è°ƒã€‚ä¸ºäº†é™ä½æ ‡æ³¨æˆæœ¬ï¼Œæœ¬æ–‡ç ”ç©¶äº†ä¸€ç§æ–°å‹çš„å¼±ç›‘ç£SAMåˆ†å‰²æ¨¡å‹ï¼Œå³WeakMedSAMã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºçš„WeakMedSAMåŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼š1ï¼‰ä¸ºäº†å‡å°‘åŒ»å­¦å›¾åƒä¸­çš„ä¸¥é‡å…±å‘ç”Ÿç°è±¡ï¼Œå¼•å…¥äº†å­ç±»æ¢ç´¢æ¨¡å—æ¥å­¦ä¹ ç²¾ç¡®çš„ç‰¹å¾è¡¨ç¤ºã€‚2ï¼‰ä¸ºäº†æé«˜ç±»æ¿€æ´»å›¾çš„è´¨é‡ï¼Œæˆ‘ä»¬çš„æç¤ºäº²å’ŒåŠ›æŒ–æ˜æ¨¡å—åˆ©ç”¨SAMçš„æç¤ºèƒ½åŠ›æ¥è·å¾—ç”¨äºéšæœºæ¸¸èµ°ç²¾åŒ–çš„äº²å’ŒåŠ›å›¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åº”ç”¨äºä»»ä½•SAMç±»ä¼¼çš„éª¨å¹²ç½‘ï¼Œæˆ‘ä»¬åœ¨SAMUSå’ŒEfficientSAMä¸Šè¿›è¡Œäº†å®éªŒã€‚åœ¨ä¸‰ä¸ªå¸¸ç”¨çš„åŸºå‡†æ•°æ®é›†ï¼Œå³BraTS 2019ã€AbdomenCT-1Kå’ŒMSDå¿ƒè„æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„WeakMedSAMå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wanghr64/WeakMedSAM%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wanghr64/WeakMedSAMæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04106v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼±ç›‘ç£çš„åˆ†å‰²æ¨¡å‹WeakMedSAMï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚è¯¥æ¨¡å‹åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šå­ç±»åˆ«æ¢ç´¢æ¨¡å—ç”¨äºå­¦ä¹ å‡†ç¡®çš„ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶å‡å°‘åŒ»å­¦å›¾åƒä¸­çš„ä¸¥é‡å…±ç°é—®é¢˜ï¼›æç¤ºäº²å’ŒåŠ›æŒ–æ˜æ¨¡å—åˆ©ç”¨åˆ†å‰²ä»»ä½•äº‹ç‰©æ¨¡å‹ï¼ˆSAMï¼‰çš„æç¤ºèƒ½åŠ›è·å¾—äº²å’ŒåŠ›å›¾è¿›è¡Œéšæœºæ¸¸èµ°ç»†åŒ–ï¼Œä»¥æé«˜ç±»åˆ«æ¿€æ´»å›¾çš„è´¨é‡ã€‚å®éªŒç»“æœè¯æ˜äº†WeakMedSAMåœ¨ä¸‰ä¸ªå¸¸ç”¨åŸºå‡†æ•°æ®é›†ä¸Šçš„ä¼˜å¼‚è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å¼±ç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹WeakMedSAMã€‚</li>
<li>WeakMedSAMåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šå­ç±»åˆ«æ¢ç´¢æ¨¡å—å’Œæç¤ºäº²å’ŒåŠ›æŒ–æ˜æ¨¡å—ã€‚</li>
<li>å­ç±»åˆ«æ¢ç´¢æ¨¡å—é€šè¿‡å­¦ä¹ å‡†ç¡®ç‰¹å¾è¡¨ç¤ºæ¥ç¼“è§£åŒ»å­¦å›¾åƒä¸­çš„ä¸¥é‡å…±ç°é—®é¢˜ã€‚</li>
<li>æç¤ºäº²å’ŒåŠ›æŒ–æ˜æ¨¡å—åˆ©ç”¨SAMçš„æç¤ºèƒ½åŠ›è·å¾—äº²å’ŒåŠ›å›¾ï¼Œä»¥æé«˜ç±»åˆ«æ¿€æ´»å›¾çš„è´¨é‡ã€‚</li>
<li>WeakMedSAMå¯åº”ç”¨äºä»»ä½•SAMç±»ä¼¼çš„éª¨å¹²ç½‘ï¼Œå¹¶åœ¨å®éªŒä¸­ä¸SAMUSå’ŒEfficientSAMè¿›è¡Œäº†å¯¹æ¯”ã€‚</li>
<li>åœ¨ä¸‰ä¸ªå¸¸ç”¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒWeakMedSAMè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04106">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e323683c49116e8cbcd4bc5b1325e1a2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1cb6b2a7e3e81f209d843b99170518b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e1124f1221980b5421e9988bc7b69e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b78c7413bf5961246376766049392672.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f10930621d4d62f1ed08890a401efa3f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Pruning-Deep-Neural-Networks-via-a-Combination-of-the-Marchenko-Pastur-Distribution-and-Regularization"><a href="#Pruning-Deep-Neural-Networks-via-a-Combination-of-the-Marchenko-Pastur-Distribution-and-Regularization" class="headerlink" title="Pruning Deep Neural Networks via a Combination of the Marchenko-Pastur   Distribution and Regularization"></a>Pruning Deep Neural Networks via a Combination of the Marchenko-Pastur   Distribution and Regularization</h2><p><strong>Authors:Leonid Berlyand, Theo Bourdais, Houman Owhadi, Yitzchak Shmalo</strong></p>
<p>Deep neural networks (DNNs) have brought significant advancements in various applications in recent years, such as image recognition, speech recognition, and natural language processing. In particular, Vision Transformers (ViTs) have emerged as a powerful class of models in the field of deep learning for image classification. In this work, we propose a novel Random Matrix Theory (RMT)-based method for pruning pre-trained DNNs, based on the sparsification of weights and singular vectors, and apply it to ViTs. RMT provides a robust framework to analyze the statistical properties of large matrices, which has been shown to be crucial for understanding and optimizing the performance of DNNs. We demonstrate that our RMT-based pruning can be used to reduce the number of parameters of ViT models (trained on ImageNet) by 30-50% with less than 1% loss in accuracy. To our knowledge, this represents the state-of-the-art in pruning for these ViT models. Furthermore, we provide a rigorous mathematical underpinning of the above numerical studies, namely we proved a theorem for fully connected DNNs, and other more general DNN structures, describing how the randomness in the weight matrices of a DNN decreases as the weights approach a local or global minimum (during training). We verify this theorem through numerical experiments on fully connected DNNs, providing empirical support for our theoretical findings. Moreover, we prove a theorem that describes how DNN loss decreases as we remove randomness in the weight layers, and show a monotone dependence of the decrease in loss with the amount of randomness that we remove. Our results also provide significant RMT-based insights into the role of regularization during training and pruning. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰å„ä¸ªåº”ç”¨é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç‰¹åˆ«æ˜¯åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸï¼ŒVision Transformersï¼ˆViTsï¼‰ä½œä¸ºå›¾åƒåˆ†ç±»æ¨¡å‹çš„ä¸€ç§å¼ºå¤§ç±»åˆ«è€Œå´­éœ²å¤´è§’ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºéšæœºçŸ©é˜µç†è®ºï¼ˆRMTï¼‰çš„é¢„è®­ç»ƒDNNå‰ªææ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºæƒé‡å’Œå¥‡å¼‚å‘é‡çš„ç¨€ç–åŒ–ï¼Œå¹¶é€‚ç”¨äºViTsã€‚RMTæä¾›äº†ä¸€ä¸ªåˆ†æå¤§å‹çŸ©é˜µç»Ÿè®¡ç‰¹æ€§çš„ç¨³å¥æ¡†æ¶ï¼Œå·²è¢«è¯æ˜å¯¹äºç†è§£å’Œä¼˜åŒ–DNNçš„æ€§èƒ½è‡³å…³é‡è¦ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„åŸºäºRMTçš„å‰ªæå¯ç”¨äºå°†ImageNetè®­ç»ƒçš„ViTæ¨¡å‹çš„å‚æ•°æ•°é‡å‡å°‘30%~50%ï¼ŒåŒæ—¶ç²¾åº¦æŸå¤±å°äº1%ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯è¿™äº›ViTæ¨¡å‹å‰ªæçš„æœ€æ–°æˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹ä¸Šè¿°æ•°å€¼ç ”ç©¶è¿›è¡Œäº†ä¸¥è°¨çš„æ•°å­¦è®ºè¯ï¼Œå³æˆ‘ä»¬ä¸ºå…¨è¿æ¥DNNå’Œå…¶ä»–æ›´ä¸€èˆ¬çš„DNNç»“æ„è¯æ˜äº†ä¸€ä¸ªå®šç†ï¼Œæè¿°äº†DNNæƒé‡çŸ©é˜µä¸­çš„éšæœºæ€§å¦‚ä½•éšç€æƒé‡æ¥è¿‘å±€éƒ¨æˆ–å…¨å±€æœ€å°å€¼ï¼ˆåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼‰è€Œå‡å°‘ã€‚æˆ‘ä»¬é€šè¿‡å…¨è¿æ¥DNNçš„æ•°å€¼å®éªŒéªŒè¯äº†è¿™ä¸€å®šç†ï¼Œä¸ºæˆ‘ä»¬çš„ç†è®ºå‘ç°æä¾›äº†å®è¯æ”¯æŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†å¦ä¸€ä¸ªå®šç†ï¼Œæè¿°äº†å½“æˆ‘ä»¬ç§»é™¤æƒé‡å±‚ä¸­çš„éšæœºæ€§æ—¶ï¼ŒDNNçš„æŸå¤±å¦‚ä½•å‡å°‘ï¼Œå¹¶å±•ç¤ºäº†æŸå¤±å‡å°‘ä¸æˆ‘ä»¬ç§»é™¤çš„éšæœºæ€§çš„é‡ä¹‹é—´çš„å•è°ƒä¾èµ–æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¿˜æä¾›äº†åŸºäºRMTçš„å¯¹è®­ç»ƒè¿‡ç¨‹ä¸­æ­£åˆ™åŒ–ä½œç”¨çš„æ·±åˆ»è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01922v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºéšæœºçŸ©é˜µç†è®ºï¼ˆRMTï¼‰çš„é¢„è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰å‰ªææ–¹æ³•ï¼Œå¹¶åº”ç”¨äºVision Transformersï¼ˆViTsï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡æƒé‡å’Œå¥‡å¼‚å‘é‡çš„ç¨€ç–åŒ–å®ç°å‰ªæï¼Œå¯å‡å°ViTæ¨¡å‹ï¼ˆåœ¨ImageNetä¸Šè®­ç»ƒï¼‰çš„å‚æ•°æ•°é‡è¾¾30-50%ï¼Œä¸”å‡†ç¡®åº¦æŸå¤±å°äº1%ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æä¾›äº†ä¸¥æ ¼çš„æ•°å­¦ç†è®ºæ”¯æŒï¼ŒåŒ…æ‹¬æè¿°DNNæƒé‡çŸ©é˜µéšæœºæ€§å¦‚ä½•éšè®­ç»ƒè¿‡ç¨‹ä¸­æƒé‡æ¥è¿‘å±€éƒ¨æˆ–å…¨å±€æœ€å°å€¼è€Œå‡å°‘çš„å®šç†ã€‚é€šè¿‡æ•°å€¼å®éªŒéªŒè¯äº†è¯¥å®šç†ï¼Œå¹¶ä¸ºç†è®ºå‘ç°æä¾›äº†å®è¯æ”¯æŒã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜è¯æ˜äº†æè¿°éšç€æƒé‡å±‚éšæœºæ€§çš„æ¶ˆé™¤ï¼ŒDNNæŸå¤±å¦‚ä½•å‡å°‘çš„å®šç†ï¼Œå±•ç¤ºäº†æŸå¤±å‡å°‘ä¸æ¶ˆé™¤çš„éšæœºæ€§çš„å•è°ƒä¾èµ–å…³ç³»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºéšæœºçŸ©é˜µç†è®ºï¼ˆRMTï¼‰çš„é¢„è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰å‰ªææ–°æ–¹æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºVision Transformersï¼ˆViTsï¼‰ã€‚</li>
<li>é€šè¿‡æƒé‡å’Œå¥‡å¼‚å‘é‡çš„ç¨€ç–åŒ–ï¼Œæ˜¾è‘—å‡å°‘äº†ViTæ¨¡å‹çš„å‚æ•°æ•°é‡ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„å‡†ç¡®åº¦ã€‚</li>
<li>æä¾›äº†ä¸¥æ ¼çš„æ•°å­¦ç†è®ºæ”¯æŒï¼ŒåŒ…æ‹¬æè¿°DNNæƒé‡çŸ©é˜µéšæœºæ€§å˜åŒ–çš„å®šç†ï¼Œå¹¶é€šè¿‡æ•°å€¼å®éªŒéªŒè¯äº†è¯¥å®šç†ã€‚</li>
<li>è¯æ˜äº†æè¿°DNNæŸå¤±éšæƒé‡å±‚éšæœºæ€§å‡å°‘è€Œå‡å°‘çš„å®šç†ï¼Œå±•ç¤ºäº†æŸå¤±å‡å°‘ä¸éšæœºæ€§æ¶ˆé™¤ä¹‹é—´çš„å•è°ƒå…³ç³»ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥å®ç°é«˜è¾¾30-50%çš„å‚æ•°ç¼©å‡ï¼Œä»£è¡¨äº†ViTæ¨¡å‹çš„æœ€æ–°å‰ªææˆæœã€‚</li>
<li>ç»“æœå¯¹è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ­£åˆ™åŒ–ä½œç”¨æä¾›äº†é‡è¦çš„RMTè§è§£ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºæ·±åº¦ç¥ç»ç½‘ç»œæ€§èƒ½ä¼˜åŒ–å’Œç†è®ºåˆ†ææä¾›äº†æ–°çš„è§†è§’å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01922">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c5813982102f754e4ac159ad35ca9b0b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-18678ad6b7f893f31ed1928aca7f7c8c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MedUnifier-Unifying-Vision-and-Language-Pre-training-on-Medical-Data-with-Vision-Generation-Task-using-Discrete-Visual-Representations"><a href="#MedUnifier-Unifying-Vision-and-Language-Pre-training-on-Medical-Data-with-Vision-Generation-Task-using-Discrete-Visual-Representations" class="headerlink" title="MedUnifier: Unifying Vision-and-Language Pre-training on Medical Data   with Vision Generation Task using Discrete Visual Representations"></a>MedUnifier: Unifying Vision-and-Language Pre-training on Medical Data   with Vision Generation Task using Discrete Visual Representations</h2><p><strong>Authors:Ziyang Zhang, Yang Yu, Yucheng Chen, Xulei Yang, Si Yong Yeo</strong></p>
<p>Despite significant progress in Vision-Language Pre-training (VLP), current approaches predominantly emphasize feature extraction and cross-modal comprehension, with limited attention to generating or transforming visual content. This gap hinders the modelâ€™s ability to synthesize coherent and novel visual representations from textual prompts, thereby reducing the effectiveness of multi-modal learning. In this work, we propose MedUnifier, a unified VLP framework tailored for medical data. MedUnifier seamlessly integrates text-grounded image generation capabilities with multi-modal learning strategies, including image-text contrastive alignment, image-text matching and image-grounded text generation. Unlike traditional methods that reply on continuous visual representations, our approach employs visual vector quantization, which not only facilitates a more cohesive learning strategy for cross-modal understanding but also enhances multi-modal generation quality by effectively leveraging discrete representations. Our frameworkâ€™s effectiveness is evidenced by the experiments on established benchmarks, including uni-modal tasks (supervised fine-tuning), cross-modal tasks (image-text retrieval and zero-shot image classification), and multi-modal tasks (medical report generation, image synthesis), where it achieves state-of-the-art performance across various tasks. MedUnifier also offers a highly adaptable tool for a wide range of language and vision tasks in healthcare, marking advancement toward the development of a generalizable AI model for medical applications. </p>
<blockquote>
<p>å°½ç®¡è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å½“å‰çš„æ–¹æ³•ä¸»è¦ä¾§é‡äºç‰¹å¾æå–å’Œè·¨æ¨¡æ€ç†è§£ï¼Œå¯¹ç”Ÿæˆæˆ–è½¬æ¢è§†è§‰å†…å®¹çš„å…³æ³¨æœ‰é™ã€‚è¿™ä¸€å·®è·é˜»ç¢äº†æ¨¡å‹ä»æ–‡æœ¬æç¤ºä¸­åˆæˆè¿è´¯ä¸”æ–°é¢–çš„è§†è§‰è¡¨ç¤ºçš„èƒ½åŠ›ï¼Œä»è€Œé™ä½äº†å¤šæ¨¡æ€å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹åŒ»ç–—æ•°æ®çš„ç»Ÿä¸€VLPæ¡†æ¶MedUnifierã€‚MedUnifieræ— ç¼é›†æˆäº†åŸºäºæ–‡æœ¬çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ä¸å¤šæ¨¡æ€å­¦ä¹ ç­–ç•¥ï¼ŒåŒ…æ‹¬å›¾åƒæ–‡æœ¬å¯¹æ¯”å¯¹é½ã€å›¾åƒæ–‡æœ¬åŒ¹é…å’ŒåŸºäºå›¾åƒçš„æ–‡æœ¬ç”Ÿæˆã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–äºè¿ç»­è§†è§‰è¡¨ç¤ºçš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨è§†è§‰å‘é‡é‡åŒ–ï¼Œè¿™ä¸ä»…æœ‰åŠ©äºæ›´è¿è´¯çš„è·¨æ¨¡æ€ç†è§£å­¦ä¹ ç­–ç•¥ï¼Œè€Œä¸”é€šè¿‡æœ‰æ•ˆåˆ©ç”¨ç¦»æ•£è¡¨ç¤ºï¼Œæé«˜äº†å¤šæ¨¡æ€ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å…¬è®¤çš„æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬å•æ¨¡æ€ä»»åŠ¡ï¼ˆç›‘ç£å¾®è°ƒï¼‰ã€è·¨æ¨¡æ€ä»»åŠ¡ï¼ˆå›¾åƒæ–‡æœ¬æ£€ç´¢å’Œé›¶æ ·æœ¬å›¾åƒåˆ†ç±»ï¼‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ï¼ˆåŒ»ç–—æŠ¥å‘Šç”Ÿæˆã€å›¾åƒåˆæˆï¼‰ã€‚å®ƒåœ¨å„ç§ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚MedUnifierè¿˜ä¸ºåŒ»ç–—é¢†åŸŸä¸­çš„å¤šç§è¯­è¨€å’Œè§†è§‰ä»»åŠ¡æä¾›äº†é«˜åº¦é€‚åº”çš„å·¥å…·ï¼Œæ ‡å¿—ç€æœç€å¼€å‘ç”¨äºåŒ»ç–—åº”ç”¨çš„å¯æ¨å¹¿äººå·¥æ™ºèƒ½æ¨¡å‹çš„å‘å±•è¿›æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01019v2">PDF</a> To be pubilshed in CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹åŒ»ç–—æ•°æ®çš„ç»Ÿä¸€è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¡†æ¶MedUnifierã€‚è¯¥æ¡†æ¶èåˆäº†æ–‡æœ¬é©±åŠ¨å›¾åƒç”Ÿæˆèƒ½åŠ›ä¸å¤šæ¨¡æ€å­¦ä¹ ç­–ç•¥ï¼ŒåŒ…æ‹¬å›¾åƒæ–‡æœ¬å¯¹æ¯”å¯¹é½ã€å›¾åƒæ–‡æœ¬åŒ¹é…å’Œå›¾åƒé©±åŠ¨æ–‡æœ¬ç”Ÿæˆã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–äºè¿ç»­è§†è§‰è¡¨å¾çš„æ–¹æ³•ä¸åŒï¼ŒMedUnifieré‡‡ç”¨è§†è§‰å‘é‡é‡åŒ–ï¼Œä¸ä»…ä¿ƒè¿›äº†è·¨æ¨¡æ€ç†è§£çš„æ›´è¿è´¯å­¦ä¹ ç­–ç•¥ï¼Œè€Œä¸”é€šè¿‡æœ‰æ•ˆåˆ©ç”¨ç¦»æ•£è¡¨å¾æé«˜äº†å¤šæ¨¡æ€ç”Ÿæˆè´¨é‡ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒMedUnifierå®ç°äº†å„ç§ä»»åŠ¡çš„æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸ºåŒ»ç–—å¥åº·é¢†åŸŸä¸­çš„è¯­è¨€å’Œè§†è§‰ä»»åŠ¡æä¾›äº†é«˜åº¦å¯é€‚åº”çš„å·¥å…·ï¼Œæ ‡å¿—ç€åŒ»ç–—åº”ç”¨é€šç”¨äººå·¥æ™ºèƒ½æ¨¡å‹å¼€å‘çš„è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è§†è§‰è¯­è¨€é¢„è®­ç»ƒï¼ˆVLPï¼‰ä¸»è¦å…³æ³¨ç‰¹å¾æå–å’Œè·¨æ¨¡æ€ç†è§£ï¼Œå¿½è§†äº†è§†è§‰å†…å®¹çš„ç”Ÿæˆæˆ–è½¬æ¢ã€‚</li>
<li>MedUnifieræ¡†æ¶å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œèåˆäº†æ–‡æœ¬é©±åŠ¨çš„å›¾åƒç”Ÿæˆä¸å¤šæ¨¡æ€å­¦ä¹ ç­–ç•¥ã€‚</li>
<li>MedUnifieré‡‡ç”¨è§†è§‰å‘é‡é‡åŒ–ï¼Œæœ‰æ•ˆä¿ƒè¿›è·¨æ¨¡æ€ç†è§£çš„è¿è´¯å­¦ä¹ ç­–ç•¥åŠå¤šæ¨¡æ€ç”Ÿæˆè´¨é‡çš„æé«˜ã€‚</li>
<li>ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒMedUnifieråœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸Šå®ç°æœ€ä½³æ€§èƒ½ï¼ŒåŒ…æ‹¬å•æ¨¡æ€ã€è·¨æ¨¡æ€å’Œå¤šæ¨¡æ€ä»»åŠ¡ã€‚</li>
<li>MedUnifieræ¡†æ¶å…·æœ‰é«˜åº¦çš„é€‚åº”æ€§ï¼Œå¯å¹¿æ³›åº”ç”¨äºåŒ»ç–—å¥åº·é¢†åŸŸçš„è¯­è¨€å’Œè§†è§‰ä»»åŠ¡ã€‚</li>
<li>MedUnifierä¸ºåŒ»ç–—åº”ç”¨é€šç”¨äººå·¥æ™ºèƒ½æ¨¡å‹çš„å¼€å‘æä¾›äº†é‡è¦è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01019">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16dbe29a8fce97a028eaee3f6b1ee1ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-166e1f719fc8616a59f877d53d7d8886.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5371ea9168a0ba20e591780e5157317c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MobileViM-A-Light-weight-and-Dimension-independent-Vision-Mamba-for-3D-Medical-Image-Analysis"><a href="#MobileViM-A-Light-weight-and-Dimension-independent-Vision-Mamba-for-3D-Medical-Image-Analysis" class="headerlink" title="MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D   Medical Image Analysis"></a>MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D   Medical Image Analysis</h2><p><strong>Authors:Wei Dai, Jun Liu</strong></p>
<p>Efficient evaluation of three-dimensional (3D) medical images is crucial for diagnostic and therapeutic practices in healthcare. Recent years have seen a substantial uptake in applying deep learning and computer vision to analyse and interpret medical images. Traditional approaches, such as convolutional neural networks (CNNs) and vision transformers (ViTs), face significant computational challenges, prompting the need for architectural advancements. Recent efforts have led to the introduction of novel architectures like the &#96;&#96;Mambaâ€™â€™ model as alternative solutions to traditional CNNs or ViTs. The Mamba model excels in the linear processing of one-dimensional data with low computational demands. However, Mambaâ€™s potential for 3D medical image analysis remains underexplored and could face significant computational challenges as the dimension increases. This manuscript presents MobileViM, a streamlined architecture for efficient segmentation of 3D medical images. In the MobileViM network, we invent a new dimension-independent mechanism and a dual-direction traversing approach to incorporate with a vision-Mamba-based framework. MobileViM also features a cross-scale bridging technique to improve efficiency and accuracy across various medical imaging modalities. With these enhancements, MobileViM achieves segmentation speeds exceeding 90 frames per second (FPS) on a single graphics processing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster than the state-of-the-art deep learning models for processing 3D images with the same computational resources. In addition, experimental evaluations demonstrate that MobileViM delivers superior performance, with Dice similarity scores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024, ATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses existing models. </p>
<blockquote>
<p>é«˜æ•ˆè¯„ä¼°ä¸‰ç»´ï¼ˆ3Dï¼‰åŒ»å­¦å½±åƒå¯¹äºåŒ»ç–—è¯Šæ–­å’Œæ²»ç–—å®è·µè‡³å…³é‡è¦ã€‚è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰åœ¨åŒ»å­¦å›¾åƒåˆ†æå’Œè§£é‡Šæ–¹é¢çš„åº”ç”¨å¾—åˆ°äº†å¹¿æ³›æ¥çº³ã€‚ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰é¢ä¸´é‡å¤§çš„è®¡ç®—æŒ‘æˆ˜ï¼Œè¿™ä¿ƒä½¿äº†æ¶æ„å‘å±•çš„å¿…è¦æ€§ã€‚æœ€è¿‘çš„åŠªåŠ›å¯¼è‡´äº†ç±»ä¼¼â€œMambaâ€æ¨¡å‹ç­‰æ–°æ¶æ„çš„å¼•å…¥ï¼Œä½œä¸ºå¯¹ä¼ ç»ŸCNNæˆ–ViTçš„æ›¿ä»£è§£å†³æ–¹æ¡ˆã€‚Mambaæ¨¡å‹åœ¨å¤„ç†ä¸€ç»´æ•°æ®çš„çº¿æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè®¡ç®—éœ€æ±‚è¾ƒä½ã€‚ç„¶è€Œï¼ŒMambaåœ¨3DåŒ»å­¦å›¾åƒåˆ†ææ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œéšç€ç»´åº¦çš„å¢åŠ ï¼Œå¯èƒ½ä¼šé¢ä¸´é‡å¤§çš„è®¡ç®—æŒ‘æˆ˜ã€‚æœ¬æ‰‹ç¨¿æå‡ºäº†MobileViMï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé«˜æ•ˆåˆ†å‰²3DåŒ»å­¦å½±åƒçš„ç²¾ç®€æ¶æ„ã€‚åœ¨MobileViMç½‘ç»œä¸­ï¼Œæˆ‘ä»¬åˆ›é€ äº†ä¸€ç§æ–°çš„ç»´åº¦ç‹¬ç«‹æœºåˆ¶å’Œä¸€ç§åŒå‘éå†æ–¹æ³•ä¸åŸºäºè§†è§‰Mambaçš„æ¡†æ¶ç›¸ç»“åˆã€‚MobileViMè¿˜é‡‡ç”¨è·¨å°ºåº¦æ¡¥æ¥æŠ€æœ¯ï¼Œä»¥æé«˜ä¸åŒåŒ»å­¦å½±åƒæ¨¡æ€çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡è¿™äº›å¢å¼ºåŠŸèƒ½ï¼ŒMobileViMåœ¨å•ä¸ªå›¾å½¢å¤„ç†å•å…ƒï¼ˆå³NVIDIA RTX 4090ï¼‰ä¸Šå®ç°äº†è¶…è¿‡æ¯ç§’90å¸§ï¼ˆFPSï¼‰çš„åˆ†å‰²é€Ÿåº¦ã€‚æ­¤æ€§èƒ½æ¯”ä½¿ç”¨ç›¸åŒè®¡ç®—èµ„æºçš„ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹å¤„ç†3Då›¾åƒçš„é€Ÿåº¦å¿«24 FPSä»¥ä¸Šã€‚æ­¤å¤–ï¼Œå®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒMobileViMçš„æ€§èƒ½å“è¶Šï¼Œåœ¨PENGWINã€BraTS2024ã€ATLASå’ŒToothfairy2æ•°æ®é›†ä¸Šçš„Diceç›¸ä¼¼åº¦å¾—åˆ†åˆ†åˆ«è¾¾åˆ°äº†92.72%ã€86.69%ã€80.46%å’Œ77.43%ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13524v4">PDF</a> The corresponding author disagrees with the manuscript submitted to   arXiv</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºäº†MobileViMæ¶æ„ï¼Œç”¨äºé«˜æ•ˆå¤„ç†ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚è¯¥æ¶æ„ç»“åˆäº†Mambaæ¨¡å‹çš„ä¼˜ç‚¹ï¼Œé‡‡ç”¨æ–°çš„ç»´åº¦ç‹¬ç«‹æœºåˆ¶å’ŒåŒå‘éå†æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†è·¨å°ºåº¦æ¡¥æ¥æŠ€æœ¯ã€‚MobileViMå®ç°äº†è¶…è¿‡æ¯ç§’90å¸§çš„é€Ÿåº¦ï¼Œè¶…è¶Šäº†ç°æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºå…¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„ä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MobileViMæ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²çš„é«˜æ•ˆæ¶æ„ã€‚</li>
<li>ç»“åˆäº†Mambaæ¨¡å‹çš„ä¼˜ç‚¹ï¼Œå¹¶é‡‡ç”¨æ–°çš„ç»´åº¦ç‹¬ç«‹æœºåˆ¶å’ŒåŒå‘éå†æ–¹æ³•ã€‚</li>
<li>å¼•å…¥äº†è·¨å°ºåº¦æ¡¥æ¥æŠ€æœ¯ä»¥æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>MobileViMå®ç°äº†æ¯ç§’è¶…è¿‡90å¸§çš„é€Ÿåº¦ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ¨¡å‹ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†ä¼˜å¼‚çš„æ€§èƒ½ï¼ŒDiceç›¸ä¼¼åº¦å¾—åˆ†é«˜ã€‚</li>
<li>MobileViMæ¶æ„é€‚ç”¨äºå¤šç§åŒ»å­¦æˆåƒæ¨¡æ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13524">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a19da823a39016902a1c548a5c88342b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b8d590b505813b80d6e6b5e2bad344e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5082a48f077c477e48276a57b6fe17b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d40734e73903ff1d635e972ebebfdebd.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Modulating-CNN-Features-with-Pre-Trained-ViT-Representations-for-Open-Vocabulary-Object-Detection"><a href="#Modulating-CNN-Features-with-Pre-Trained-ViT-Representations-for-Open-Vocabulary-Object-Detection" class="headerlink" title="Modulating CNN Features with Pre-Trained ViT Representations for   Open-Vocabulary Object Detection"></a>Modulating CNN Features with Pre-Trained ViT Representations for   Open-Vocabulary Object Detection</h2><p><strong>Authors:Xiangyu Gao, Yu Dai, Benliu Qiu, Lanxiao Wang, Heqian Qiu, Hongliang Li</strong></p>
<p>Owing to large-scale image-text contrastive training, pre-trained vision language model (VLM) like CLIP shows superior open-vocabulary recognition ability. Most existing open-vocabulary object detectors attempt to utilize the pre-trained VLMs to attain generalized representation. F-ViT uses the pre-trained visual encoder as the backbone network and freezes it during training. However, its frozen backbone doesnâ€™t benefit from the labeled data to strengthen the representation for detection. Therefore, we propose a novel two-branch backbone network, named as \textbf{V}iT-Feature-\textbf{M}odulated Multi-Scale \textbf{C}onvolutional Network (VMCNet), which consists of a trainable convolutional branch, a frozen pre-trained ViT branch and a VMC module. The trainable CNN branch could be optimized with labeled data while the frozen pre-trained ViT branch could keep the representation ability derived from large-scale pre-training. Then, the proposed VMC module could modulate the multi-scale CNN features with the representations from ViT branch. With this proposed mixed structure, the detector is more likely to discover objects of novel categories. Evaluated on two popular benchmarks, our method boosts the detection performance on novel category and outperforms state-of-the-art methods. On OV-COCO, the proposed method achieves 44.3 AP$<em>{50}^{\mathrm{novel}}$ with ViT-B&#x2F;16 and 48.5 AP$</em>{50}^{\mathrm{novel}}$ with ViT-L&#x2F;14. On OV-LVIS, VMCNet with ViT-B&#x2F;16 and ViT-L&#x2F;14 reaches 27.8 and 38.4 mAP$_{r}$. </p>
<blockquote>
<p>ç”±äºå¤§è§„æ¨¡å›¾æ–‡å¯¹æ¯”è®­ç»ƒï¼Œé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰è¡¨ç°å‡ºå“è¶Šçš„å¼€è¯æ±‡è¯†åˆ«èƒ½åŠ›ã€‚ç°æœ‰çš„å¤§å¤šæ•°å¼€æ”¾è¯æ±‡å¯¹è±¡æ£€æµ‹å™¨éƒ½è¯•å›¾åˆ©ç”¨é¢„è®­ç»ƒçš„VLMsæ¥è·å¾—é€šç”¨è¡¨ç¤ºã€‚F-ViTä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ä½œä¸ºä¸»å¹²ç½‘ç»œï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å†»ç»“å®ƒã€‚ç„¶è€Œï¼Œå…¶å†»ç»“çš„ä¸»å¹²ç½‘ç»œæ— æ³•ä»æ ‡è®°æ•°æ®ä¸­å—ç›Šï¼Œä»¥å¢å¼ºæ£€æµ‹è¡¨ç¤ºã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŒåˆ†æ”¯ä¸»å¹²ç½‘ç»œï¼Œåä¸ºViTç‰¹å¾è°ƒåˆ¶å¤šå°ºåº¦å·ç§¯ç½‘ç»œï¼ˆVMCNetï¼‰ï¼Œå®ƒç”±ä¸€ä¸ªå¯è®­ç»ƒå·ç§¯åˆ†æ”¯ã€ä¸€ä¸ªå†»ç»“çš„é¢„è®­ç»ƒViTåˆ†æ”¯å’ŒVMCæ¨¡å—ç»„æˆã€‚å¯è®­ç»ƒçš„CNNåˆ†æ”¯å¯ä»¥åˆ©ç”¨æ ‡è®°æ•°æ®è¿›è¡Œä¼˜åŒ–ï¼Œè€Œå†»ç»“çš„é¢„è®­ç»ƒViTåˆ†æ”¯å¯ä»¥ä¿æŒä»å¤§è§„æ¨¡é¢„è®­ç»ƒä¸­å­¦åˆ°çš„è¡¨ç¤ºèƒ½åŠ›ã€‚ç„¶åï¼Œæ‰€æå‡ºçš„VMCæ¨¡å—å¯ä»¥è°ƒåˆ¶æ¥è‡ªViTåˆ†æ”¯çš„å¤šå°ºåº¦CNNç‰¹å¾ã€‚é€šè¿‡è¿™ç§æ··åˆç»“æ„ï¼Œæ£€æµ‹å™¨æ›´æœ‰å¯èƒ½å‘ç°æ–°å‹ç±»åˆ«çš„å¯¹è±¡ã€‚åœ¨ä¸¤ä¸ªæµè¡Œçš„åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ–°å‹ç±»åˆ«æ£€æµ‹æ€§èƒ½ä¸Šæœ‰æ‰€æå‡ï¼Œå¹¶è¶…è¶Šäº†æœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚åœ¨OV-COCOä¸Šï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä½¿ç”¨ViT-B&#x2F;16å®ç°äº†44.3çš„AP50novelï¼Œä½¿ç”¨ViT-L&#x2F;14å®ç°äº†48.5çš„AP50novelã€‚åœ¨OV-LVISä¸Šï¼ŒVMCNetä¸ViT-B&#x2F;16å’ŒViT-L&#x2F;14åˆ†åˆ«è¾¾åˆ°äº†27.8å’Œ38.4çš„mAPrã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16981v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºVMCNetçš„æ–°å‹ä¸¤åˆ†æ”¯éª¨å¹²ç½‘ç»œï¼Œç”¨äºå¼€æ”¾è¯æ±‡è¡¨å¯¹è±¡æ£€æµ‹ã€‚è¯¥ç½‘ç»œç»“åˆäº†é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„ä¼˜åŠ¿ï¼Œé€šè¿‡æ··åˆç»“æ„æé«˜æ£€æµ‹æ€§èƒ½ã€‚åœ¨å¤§å‹é¢„è®­ç»ƒçš„åŸºç¡€ä¸Šï¼ŒVMCNetèƒ½å¤Ÿåœ¨æ–°å‹ç±»åˆ«å¯¹è±¡æ£€æµ‹ä»»åŠ¡ä¸­å–å¾—ä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨å¤§è§„æ¨¡å›¾åƒæ–‡æœ¬å¯¹æ¯”è®­ç»ƒï¼Œé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¦‚CLIPå±•ç°å‡ºä¼˜è¶Šçš„å¼€æ”¾è¯æ±‡è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>F-ViTä½¿ç”¨é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨ä½œä¸ºéª¨å¹²ç½‘ç»œï¼Œä½†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å†»ç»“è¯¥ç½‘ç»œï¼Œæ— æ³•ä»æ ‡è®°æ•°æ®ä¸­è·ç›Šä»¥å¢å¼ºæ£€æµ‹è¡¨ç¤ºã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸¤åˆ†æ”¯éª¨å¹²ç½‘ç»œVMCNetï¼ŒåŒ…æ‹¬å¯è®­ç»ƒçš„å·ç§¯åˆ†æ”¯ã€å†»ç»“çš„é¢„è®­ç»ƒViTåˆ†æ”¯å’ŒVMCæ¨¡å—ã€‚</li>
<li>å¯è®­ç»ƒçš„CNNåˆ†æ”¯å¯ä¼˜åŒ–æ ‡è®°æ•°æ®ï¼Œè€Œå†»ç»“çš„é¢„è®­ç»ƒViTåˆ†æ”¯åˆ™ä¿æŒä»å¤§è§„æ¨¡é¢„è®­ç»ƒå¾—åˆ°çš„è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>VMCæ¨¡å—èƒ½å¤Ÿè°ƒåˆ¶å¤šå°ºåº¦CNNç‰¹å¾ä¸ViTåˆ†æ”¯çš„è¡¨ç¤ºï¼Œä½¿æ£€æµ‹å™¨æ›´å¯èƒ½å‘ç°æ–°å‹ç±»åˆ«çš„å¯¹è±¡ã€‚</li>
<li>åœ¨ä¸¤ä¸ªæµè¡ŒåŸºå‡†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ–°å‹ç±»åˆ«æ£€æµ‹æ€§èƒ½æ–¹é¢æœ‰æ‰€æå‡ï¼Œå¹¶ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16981">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b2b1b4e49a4d020943c637aa14151fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4630519de5801d9c8607f2d66dce399d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3a9d9caae2d8aa0cf3ceb26d1aaf30e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e23b9055f1cd38a68b8a6859ec6604ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43c1194db2eb80c83d993c1f586b79dc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Pathfinder-for-Low-altitude-Aircraft-with-Binary-Neural-Network"><a href="#Pathfinder-for-Low-altitude-Aircraft-with-Binary-Neural-Network" class="headerlink" title="Pathfinder for Low-altitude Aircraft with Binary Neural Network"></a>Pathfinder for Low-altitude Aircraft with Binary Neural Network</h2><p><strong>Authors:Kaijie Yin, Tian Gao, Hui Kong</strong></p>
<p>A prior global topological map (e.g., the OpenStreetMap, OSM) can boost the performance of autonomous mapping by a ground mobile robot. However, the prior map is usually incomplete due to lacking labeling in partial paths. To solve this problem, this paper proposes an OSM maker using airborne sensors carried by low-altitude aircraft, where the core of the OSM maker is a novel efficient pathfinder approach based on LiDAR and camera data, i.e., a binary dual-stream road segmentation model. Specifically, a multi-scale feature extraction based on the UNet architecture is implemented for images and point clouds. To reduce the effect caused by the sparsity of point cloud, an attention-guided gated block is designed to integrate image and point-cloud features. To optimize the model for edge deployment that significantly reduces storage footprint and computational demands, we propose a binarization streamline to each model component, including a variant of vision transformer (ViT) architecture as the encoder of the image branch, and new focal and perception losses to optimize the model training. The experimental results on two datasets demonstrate that our pathfinder method achieves SOTA accuracy with high efficiency in finding paths from the low-level airborne sensors, and we can create complete OSM prior maps based on the segmented road skeletons. Code and data are available at: \href{<a target="_blank" rel="noopener" href="https://github.com/IMRL/Pathfinder%7D%7Bhttps://github.com/IMRL/Pathfinder%7D">https://github.com/IMRL/Pathfinder}{https://github.com/IMRL/Pathfinder}</a>. </p>
<blockquote>
<p>å…ˆå‰çš„å…¨å±€æ‹“æ‰‘åœ°å›¾ï¼ˆä¾‹å¦‚ï¼ŒOpenStreetMapï¼ŒOSMï¼‰å¯ä»¥é€šè¿‡åœ°é¢ç§»åŠ¨æœºå™¨äººæå‡è‡ªä¸»æµ‹ç»˜çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºéƒ¨åˆ†è·¯å¾„ç¼ºä¹æ ‡æ³¨ï¼Œå…ˆå‰çš„åœ°å›¾é€šå¸¸æ˜¯ä¸å®Œæ•´çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨ä½ç©ºé£æœºæºå¸¦çš„æœºè½½ä¼ æ„Ÿå™¨çš„OSMåˆ¶ä½œæ–¹æ³•ã€‚è¯¥OSMåˆ¶ä½œçš„æ ¸å¿ƒæ˜¯ä¸€ç§åŸºäºæ¿€å…‰é›·è¾¾å’Œç›¸æœºæ•°æ®çš„æ–°å‹é«˜æ•ˆè·¯å¾„æŸ¥æ‰¾æ–¹æ³•ï¼Œå³äºŒè¿›åˆ¶åŒæµé“è·¯åˆ†å‰²æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼ŒåŸºäºUNetæ¶æ„å®ç°äº†å¤šå°ºåº¦ç‰¹å¾æå–ï¼Œç”¨äºå›¾åƒå’Œç‚¹äº‘ã€‚ä¸ºäº†å‡å°‘ç‚¹äº‘ç¨€ç–é€ æˆçš„å½±å“ï¼Œè®¾è®¡äº†ä¸€ä¸ªæ³¨æ„åŠ›å¼•å¯¼çš„é—¨æ§å—æ¥æ•´åˆå›¾åƒå’Œç‚¹äº‘ç‰¹å¾ã€‚ä¸ºäº†ä¼˜åŒ–æ¨¡å‹åœ¨è¾¹ç¼˜éƒ¨ç½²ï¼Œæ˜¾è‘—é™ä½å­˜å‚¨ç©ºé—´å’Œè®¡ç®—éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹çš„æ¯ä¸ªç»„ä»¶éƒ½è¿›è¡Œäº†äºŒå€¼åŒ–å¤„ç†ï¼ŒåŒ…æ‹¬ä½¿ç”¨å˜ç§è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¶æ„ä½œä¸ºå›¾åƒåˆ†æ”¯çš„ç¼–ç å™¨ï¼Œä»¥åŠæ–°çš„ç„¦ç‚¹å’Œæ„ŸçŸ¥æŸå¤±æ¥ä¼˜åŒ–æ¨¡å‹è®­ç»ƒã€‚åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è·¯å¾„æŸ¥æ‰¾æ–¹æ³•å®ç°äº†é«˜æ•ˆä¸”é«˜ç²¾åº¦çš„è·¯å¾„æŸ¥æ‰¾ï¼Œä»ä½çº§åˆ«æœºè½½ä¼ æ„Ÿå™¨å‡ºå‘ï¼Œå¹¶å¯ä»¥åŸºäºåˆ†å‰²çš„é“è·¯éª¨æ¶æ„å»ºå®Œæ•´çš„OSMå…ˆéªŒåœ°å›¾ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/IMRL/Pathfinder">https://github.com/IMRL/Pathfinder</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08824v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå…¨çƒæ‹“æ‰‘åœ°å›¾ï¼ˆå¦‚OpenStreetMapï¼ŒOSMï¼‰å¯ä»¥æ˜¾è‘—æå‡åœ°é¢ç§»åŠ¨æœºå™¨äººçš„è‡ªä¸»ç»˜å›¾æ€§èƒ½ã€‚ä½†å…ˆå‰åœ°å›¾å¸¸å¸¸å› ä¸ºéƒ¨åˆ†è·¯å¾„ç¼ºä¹æ ‡æ³¨è€Œä¸å®Œæ•´ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨ä½ç©ºé£æœºæºå¸¦çš„æœºè½½ä¼ æ„Ÿå™¨çš„OSMåˆ¶ä½œæ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ˜¯ä¸€ç§åŸºäºæ¿€å…‰é›·è¾¾å’Œç›¸æœºæ•°æ®çš„æ–°å‹é«˜æ•ˆè·¯å¾„æŸ¥æ‰¾æ–¹æ³•ï¼Œå³äºŒè¿›åˆ¶åŒæµé“è·¯åˆ†å‰²æ¨¡å‹ã€‚è¯¥ç ”ç©¶å®ç°äº†åŸºäºUNetæ¶æ„çš„å¤šå°ºåº¦ç‰¹å¾æå–ç”¨äºå›¾åƒå’Œç‚¹äº‘ã€‚ä¸ºé™ä½ç‚¹äº‘ç¨€ç–é€ æˆçš„å½±å“ï¼Œè®¾è®¡äº†æ³¨æ„åŠ›å¼•å¯¼é—¨æ§å—æ¥æ•´åˆå›¾åƒå’Œç‚¹äº‘ç‰¹å¾ã€‚ä¸ºä¼˜åŒ–æ¨¡å‹åœ¨è¾¹ç¼˜éƒ¨ç½²æ—¶çš„å­˜å‚¨å’Œè®¡ç®—éœ€æ±‚ï¼Œè¯¥ç ”ç©¶å¯¹æ¯ä¸ªæ¨¡å‹ç»„ä»¶è¿›è¡ŒäºŒè¿›åˆ¶åŒ–æµçº¿å¤„ç†ï¼ŒåŒ…æ‹¬ä½¿ç”¨è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰æ¶æ„ä½œä¸ºå›¾åƒåˆ†æ”¯çš„ç¼–ç å™¨ï¼Œä»¥åŠæ–°çš„ç„¦ç‚¹å’Œæ„ŸçŸ¥æŸå¤±æ¥ä¼˜åŒ–æ¨¡å‹è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¯»æ‰¾è·¯å¾„æ–¹é¢è¾¾åˆ°äº†é«˜å‡†ç¡®ç‡å’Œé«˜æ•ˆç‡ï¼Œå¹¶å¯æ ¹æ®åˆ†å‰²çš„é“è·¯éª¨æ¶ç”Ÿæˆå®Œæ•´çš„OSMå…ˆéªŒåœ°å›¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¨çƒæ‹“æ‰‘åœ°å›¾ï¼ˆå¦‚OpenStreetMapï¼‰èƒ½å¢å¼ºè‡ªä¸»ç»˜å›¾æ€§èƒ½ã€‚</li>
<li>ç°æœ‰åœ°å›¾å› éƒ¨åˆ†è·¯å¾„ç¼ºä¹æ ‡æ³¨è€Œä¸å®Œæ•´ã€‚</li>
<li>æå‡ºä½¿ç”¨ä½ç©ºé£æœºæºå¸¦çš„æœºè½½ä¼ æ„Ÿå™¨åˆ¶ä½œOSMåœ°å›¾ã€‚</li>
<li>é‡‡ç”¨åŸºäºLiDARå’Œç›¸æœºæ•°æ®çš„æ–°å‹é«˜æ•ˆè·¯å¾„æŸ¥æ‰¾æ–¹æ³•ã€‚</li>
<li>å®ç°å¤šå°ºåº¦ç‰¹å¾æå–ï¼Œæ•´åˆå›¾åƒå’Œç‚¹äº‘ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡æ³¨æ„åŠ›å¼•å¯¼é—¨æ§å—è§£å†³ç‚¹äº‘ç¨€ç–é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08824">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6eba96e53c31f955e96a739273811178.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce73b9af3e93857e119f615bf970bbe0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b51915a2d0d1a1c1b9e02e7573133e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15170cda63cb67b485ce0ddd4e305722.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acaa6f0f1eb6936f511fd3c3f2c2f446.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-384377cf15acae8bc5884e3cd8cc3af6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-169f52d7cc889ff6ea44d286d9c5edcf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d6573c40edfc1a18b8c15af80b8f232.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73e65d6530f5d2b2bbc2fe243b83f657.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-b477433883d23a14325a3ee3fd471e9b.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-09  Omnidirectional Multi-Object Tracking
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9270b9eee5a705be400add8098fb3398.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-09  Token-Efficient Long Video Understanding for Multimodal LLMs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">15444.6k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
