<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-09  L1 Controlling How Long A Reasoning Model Thinks With Reinforcement   Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b58eda2b46e7a065a1bfb7629fee2d02.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-09-æ›´æ–°"><a href="#2025-03-09-æ›´æ–°" class="headerlink" title="2025-03-09 æ›´æ–°"></a>2025-03-09 æ›´æ–°</h1><h2 id="L1-Controlling-How-Long-A-Reasoning-Model-Thinks-With-Reinforcement-Learning"><a href="#L1-Controlling-How-Long-A-Reasoning-Model-Thinks-With-Reinforcement-Learning" class="headerlink" title="L1: Controlling How Long A Reasoning Model Thinks With Reinforcement   Learning"></a>L1: Controlling How Long A Reasoning Model Thinks With Reinforcement   Learning</h2><p><strong>Authors:Pranjal Aggarwal, Sean Welleck</strong></p>
<p>Reasoning language models have shown an uncanny ability to improve performance at test-time by &#96;&#96;thinking longerâ€™â€™-that is, by generating longer chain-of-thought sequences and hence using more compute. However, the length of their chain-of-thought reasoning is not controllable, making it impossible to allocate test-time compute to achieve a desired level of performance. We introduce Length Controlled Policy Optimization (LCPO), a simple reinforcement learning method that optimizes for accuracy and adherence to user-specified length constraints. We use LCPO to train L1, a reasoning language model that produces outputs satisfying a length constraint given in its prompt. L1â€™s length control allows for smoothly trading off computational cost and accuracy on a wide range of tasks, and outperforms the state-of-the-art S1 method for length control. Furthermore, we uncover an unexpected short chain-of-thought capability in models trained with LCPO. For instance, our 1.5B L1 model surpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise control over reasoning length, allowing for fine-grained allocation of test-time compute and accuracy. We release code and models at <a target="_blank" rel="noopener" href="https://www.cmu-l3.github.io/l1">https://www.cmu-l3.github.io/l1</a> </p>
<blockquote>
<p>æ¨ç†è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºä¸€ç§ä»¤äººéš¾ä»¥ç½®ä¿¡çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶é€šè¿‡â€œæ€è€ƒå¾—æ›´ä¹…â€æ¥æé«˜æ€§èƒ½â€”â€”ä¹Ÿå°±æ˜¯è¯´ï¼Œé€šè¿‡ç”Ÿæˆæ›´é•¿çš„æ€ç»´é“¾åºåˆ—ï¼Œä»è€Œä½¿ç”¨æ›´å¤šçš„è®¡ç®—èµ„æºã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ€ç»´é“¾æ¨ç†é•¿åº¦æ˜¯ä¸å¯æ§åˆ¶çš„ï¼Œè¿™ä½¿å¾—æ— æ³•åˆ†é…æµ‹è¯•æ—¶çš„è®¡ç®—èµ„æºæ¥å®ç°æ‰€éœ€çº§åˆ«çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¼•å…¥äº†é•¿åº¦æ§åˆ¶ç­–ç•¥ä¼˜åŒ–ï¼ˆLCPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–å‡†ç¡®æ€§å’Œå¯¹ç”¨æˆ·æŒ‡å®šé•¿åº¦çº¦æŸçš„éµå¾ªã€‚æˆ‘ä»¬ä½¿ç”¨LCPOæ¥è®­ç»ƒL1ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨ç†è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®æç¤ºäº§ç”Ÿæ»¡è¶³é•¿åº¦çº¦æŸçš„è¾“å‡ºã€‚L1çš„é•¿åº¦æ§åˆ¶å¯ä»¥åœ¨å¹¿æ³›çš„ä»»åŠ¡ä¸Šå¹³ç¨³åœ°æƒè¡¡è®¡ç®—æˆæœ¬å’Œå‡†ç¡®æ€§ï¼Œå¹¶ä¼˜äºæœ€æ–°çš„S1æ–¹æ³•è¿›è¡Œé•¿åº¦æ§åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨LCPOè®­ç»ƒçš„æ¨¡å‹ä¸­å‘ç°äº†æ„å¤–çš„çŸ­æ€ç»´é“¾èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬1.5Bçš„L1æ¨¡å‹åœ¨åŒç­‰æ¨ç†é•¿åº¦ä¸‹è¶…è¶Šäº†GPT-4oã€‚æ€»çš„æ¥è¯´ï¼ŒLCPOå®ç°å¯¹æ¨ç†é•¿åº¦çš„ç²¾ç¡®æ§åˆ¶ï¼Œå…è®¸å¯¹æµ‹è¯•æ—¶çš„è®¡ç®—å’Œå‡†ç¡®æ€§è¿›è¡Œç²¾ç»†åˆ†é…ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://www.cmu-l3.github.io/l1">https://www.cmu-l3.github.io/l1</a>ä¸Šå‘å¸ƒä»£ç å’Œæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04697v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§åä¸ºLCPOï¼ˆé•¿åº¦æ§åˆ¶ç­–ç•¥ä¼˜åŒ–ï¼‰çš„æ–¹æ³•è¢«å¼•å…¥ï¼Œç”¨äºä¼˜åŒ–æ¨ç†è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å¹¶æ§åˆ¶å…¶æ¨ç†é•¿åº¦ã€‚é€šè¿‡è¯¥æ–¹æ³•ï¼Œç”¨æˆ·å¯ä»¥æŒ‡å®šè¾“å‡ºé•¿åº¦çº¦æŸï¼Œå®ç°åœ¨æµ‹è¯•æ—¶ç²¾ç¡®åˆ†é…è®¡ç®—èµ„æºå’Œè°ƒæ•´æ€§èƒ½ã€‚L1æ¨¡å‹ä½œä¸ºåº”ç”¨LCPOçš„å®ä¾‹ï¼Œèƒ½å¤Ÿåœ¨å¤šç§ä»»åŠ¡ä¸Šå®ç°é•¿åº¦æ§åˆ¶ï¼Œå¹¶åœ¨åŒç­‰æ¨ç†é•¿åº¦ä¸‹è¶…è¶Šç°æœ‰æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒLCPOè¿˜æ„å¤–åœ°å‘ç°äº†æ¨¡å‹åœ¨çŸ­é“¾æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚æ€»ä½“è€Œè¨€ï¼ŒLCPOä¸ºç²¾ç¡®æ§åˆ¶æ¨ç†é•¿åº¦æä¾›äº†è§£å†³æ–¹æ¡ˆï¼Œå…è®¸æ›´ç²¾ç»†åœ°åˆ†é…æµ‹è¯•æ—¶çš„è®¡ç®—èµ„æºå’Œæé«˜å‡†ç¡®æ€§ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å·²å‘å¸ƒåœ¨[ç½‘ç«™é“¾æ¥]ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LCPOæ–¹æ³•èƒ½å¤Ÿä¼˜åŒ–æ¨ç†è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶æ§åˆ¶å…¶æ¨ç†é•¿åº¦ã€‚</li>
<li>ç”¨æˆ·å¯ä»¥é€šè¿‡æŒ‡å®šé•¿åº¦çº¦æŸï¼Œåœ¨æµ‹è¯•æ—¶ç²¾ç¡®åˆ†é…è®¡ç®—èµ„æºã€‚</li>
<li>L1æ¨¡å‹å®ç°äº†é•¿åº¦æ§åˆ¶ï¼Œå¯åœ¨å¤šç§ä»»åŠ¡ä¸Šè°ƒæ•´è®¡ç®—èµ„æºå’Œæ€§èƒ½ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>L1æ¨¡å‹åœ¨åŒç­‰æ¨ç†é•¿åº¦ä¸‹è¡¨ç°å‡ºè¶…è¶Šç°æœ‰æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>LCPOæ„å¤–å‘ç°æ¨¡å‹åœ¨çŸ­é“¾æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>LCPOæœ‰åŠ©äºå®ç°æ¨ç†é•¿åº¦çš„ç²¾ç¡®æ§åˆ¶ï¼Œä¸ºæµ‹è¯•æ—¶çš„è®¡ç®—èµ„æºå’Œå‡†ç¡®æ€§åˆ†é…æä¾›æ›´ç²¾ç»†çš„è°ƒèŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04697">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c0c3839fe5584fd74cc673843d52a776.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb7f4d8e5ee64252f1d36a6116d09d75.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Quantifying-the-Reasoning-Abilities-of-LLMs-on-Real-world-Clinical-Cases"><a href="#Quantifying-the-Reasoning-Abilities-of-LLMs-on-Real-world-Clinical-Cases" class="headerlink" title="Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases"></a>Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases</h2><p><strong>Authors:Pengcheng Qiu, Chaoyi Wu, Shuyu Liu, Weike Zhao, Ya Zhang, Yanfeng Wang, Weidi Xie</strong></p>
<p>The latest reasoning-enhanced large language models (reasoning LLMs), such as DeepSeek-R1 and OpenAI-o3, have demonstrated remarkable success. However, the application of such reasoning enhancements to the highly professional medical domain has not been clearly evaluated, particularly regarding with not only assessing the final generation but also examining the quality of their reasoning processes. In this study, we present MedR-Bench, a reasoning-focused medical evaluation benchmark comprising 1,453 structured patient cases with reasoning references mined from case reports. Our benchmark spans 13 body systems and 10 specialty disorders, encompassing both common and rare diseases. In our evaluation, we introduce a versatile framework consisting of three critical clinical stages: assessment recommendation, diagnostic decision-making, and treatment planning, comprehensively capturing the LLMsâ€™ performance across the entire patient journey in healthcare. For metrics, we propose a novel agentic system, Reasoning Evaluator, designed to automate and objectively quantify free-text reasoning responses in a scalable manner from the perspectives of efficiency, factuality, and completeness by dynamically searching and performing cross-referencing checks. As a result, we assess five state-of-the-art reasoning LLMs, including DeepSeek-R1, OpenAI-o3-mini, and others. Our results reveal that current LLMs can handle relatively simple diagnostic tasks with sufficient critical assessment results, achieving accuracy generally over 85%. However, they still struggle with more complex tasks, such as assessment recommendation and treatment planning. In reasoning, their reasoning processes are generally reliable, with factuality scores exceeding 90%, though they often omit critical reasoning steps. Our study clearly reveals further development directions for current clinical LLMs. </p>
<blockquote>
<p>æœ€æ–°å¢å¼ºæ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆæ¨ç†LLMï¼‰ï¼Œå¦‚DeepSeek-R1å’ŒOpenAI-o3ï¼Œå·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚ç„¶è€Œï¼Œå°†è¿™ç±»æ¨ç†å¢å¼ºåº”ç”¨äºé«˜åº¦ä¸“ä¸šçš„åŒ»ç–—é¢†åŸŸå°šæœªå¾—åˆ°æ˜ç¡®è¯„ä¼°ï¼Œå°¤å…¶æ˜¯ä¸ä»…è¯„ä¼°æœ€ç»ˆç”Ÿæˆçš„ç»“æœï¼Œè¿˜è€ƒå¯Ÿå…¶æ¨ç†è¿‡ç¨‹çš„è´¨é‡ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MedR-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥æ¨ç†ä¸ºé‡ç‚¹çš„åŒ»ç–—è¯„ä¼°åŸºå‡†ï¼ŒåŒ…å«1453ä¸ªç»“æ„åŒ–ç—…ä¾‹å’Œä»ç—…ä¾‹æŠ¥å‘Šä¸­æŒ–æ˜çš„æ¨ç†å‚è€ƒã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ¶µç›–äº†13ä¸ªèº«ä½“ç³»ç»Ÿå’Œ10ç§ä¸“ä¸šç–¾ç—…ï¼ŒåŒ…æ‹¬å¸¸è§å’Œç½•è§ç–¾ç—…ã€‚åœ¨è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªå…³é”®ä¸´åºŠé˜¶æ®µçš„é€šç”¨æ¡†æ¶ï¼šè¯„ä¼°å»ºè®®ã€è¯Šæ–­å†³ç­–å’Œæ²»ç–—è®¡åˆ’ï¼Œå…¨é¢æ•æ‰LLMåœ¨åŒ»ç–—ä¿å¥çš„æ•´ä¸ªæ‚£è€…æ—…ç¨‹ä¸­çš„è¡¨ç°ã€‚åœ¨æŒ‡æ ‡æ–¹é¢ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ä»£ç†ç³»ç»Ÿâ€”â€”æ¨ç†è¯„ä¼°å™¨ï¼Œæ—¨åœ¨ä»¥è‡ªåŠ¨åŒ–å’Œå®¢è§‚çš„æ–¹å¼ä»æ•ˆç‡ã€çœŸå®æ€§å’Œå®Œæ•´æ€§ç­‰è§’åº¦å¯¹è‡ªç”±æ–‡æœ¬æ¨ç†å“åº”è¿›è¡Œå¯æ‰©å±•çš„é‡åŒ–è¯„ä¼°ï¼Œå¹¶é€šè¿‡åŠ¨æ€æœç´¢å’Œäº¤å‰å¼•ç”¨æ£€æŸ¥æ¥å®ç°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹äº”æ¬¾æœ€å…ˆè¿›çš„æ¨ç†LLMè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬DeepSeek-R1ã€OpenAI-o3-miniç­‰ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰LLMå¯ä»¥å¤„ç†ç›¸å¯¹ç®€å•çš„è¯Šæ–­ä»»åŠ¡ï¼Œå¹¶ä¸”æœ‰è¶³å¤Ÿçš„æ‰¹åˆ¤æ€§è¯„ä¼°ç»“æœï¼Œå‡†ç¡®ç‡ä¸€èˆ¬è¶…è¿‡85%ã€‚ä½†åœ¨æ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¦‚è¯„ä¼°å»ºè®®å’Œæ²»ç–—è®¡åˆ’æ–¹é¢ï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨æ¨ç†æ–¹é¢ï¼Œå®ƒä»¬çš„æ¨ç†è¿‡ç¨‹é€šå¸¸å¯é ï¼ŒçœŸå®æ€§åˆ†æ•°è¶…è¿‡90%ï¼Œä¸è¿‡å®ƒä»¬å¾€å¾€ä¼šé—æ¼å…³é”®çš„æ¨ç†æ­¥éª¤ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ˜ç¡®äº†å½“å‰ä¸´åºŠLLMçš„è¿›ä¸€æ­¥å‘å±•æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04691v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡æœ¬ä¸»è¦ä»‹ç»äº†æœ€æ–°æ¨ç†å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆreasoning LLMsï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨è¯„ä¼°ã€‚ç ”ç©¶ä¸­æå‡ºäº†MedR-Benchï¼Œä¸€ä¸ªä¸“æ³¨äºæ¨ç†çš„åŒ»å­¦è¯„ä¼°åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä»ç—…ä¾‹æŠ¥å‘Šä¸­æŒ–æ˜çš„1453ä¸ªç»“æ„åŒ–ç—…ä¾‹çš„æ¨ç†å‚è€ƒã€‚è¯„ä¼°æ¡†æ¶åŒ…æ‹¬è¯„ä¼°æ¨èã€è¯Šæ–­å†³ç­–å’Œæ²»ç–—è§„åˆ’ä¸‰ä¸ªé˜¶æ®µï¼Œå…¨é¢æ•æ‰LLMsåœ¨åŒ»ç–—ä¿å¥ä¸­çš„æ‚£è€…æ—…ç¨‹ä¸­çš„è¡¨ç°ã€‚ä¸ºåº¦é‡æ¨ç†å“åº”ï¼Œè®¾è®¡äº†ä¸€ç§æ–°å‹ä»£ç†ç³»ç»ŸReasoning Evaluatorï¼Œèƒ½è‡ªåŠ¨ã€å®¢è§‚åœ°é‡åŒ–è‡ªç”±æ–‡æœ¬æ¨ç†å“åº”ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå½“å‰LLMså¯ä»¥å¤„ç†ç›¸å¯¹ç®€å•çš„è¯Šæ–­ä»»åŠ¡ï¼Œä½†åœ¨æ›´å¤æ‚ä»»åŠ¡ä¸Šä»æœ‰æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†å¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆreasoning LLMsï¼‰å¦‚DeepSeek-R1å’ŒOpenAI-o3åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨è¯„ä¼°å°šä¸æ¸…æ¥šã€‚</li>
<li>MedR-Benchæ˜¯ä¸€ä¸ªä¸“æ³¨äºæ¨ç†çš„åŒ»å­¦è¯„ä¼°åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«1453ä¸ªç»“æ„åŒ–ç—…ä¾‹çš„æ¨ç†å‚è€ƒã€‚</li>
<li>è¯„ä¼°æ¡†æ¶åŒ…æ‹¬è¯„ä¼°æ¨èã€è¯Šæ–­å†³ç­–å’Œæ²»ç–—è§„åˆ’ä¸‰ä¸ªé˜¶æ®µã€‚</li>
<li>Reasoning Evaluatorä½œä¸ºä¸€ç§æ–°å‹ä»£ç†ç³»ç»Ÿï¼Œèƒ½è‡ªåŠ¨é‡åŒ–LLMsçš„æ¨ç†å“åº”ã€‚</li>
<li>å½“å‰LLMså¯ä»¥å¤„ç†ç®€å•çš„è¯Šæ–­ä»»åŠ¡ï¼Œä½†é¢å¯¹æ›´å¤æ‚ä»»åŠ¡ä»æœ‰æŒ‘æˆ˜ã€‚</li>
<li>LLMsçš„æ¨ç†è¿‡ç¨‹ä¸€èˆ¬å¯é ï¼Œä½†æœ‰æ—¶ä¼šé—æ¼å…³é”®æ¨ç†æ­¥éª¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04691">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-18d32da47588c4de33be991e39cd5cd5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f84f980e94d6644142887e9735efd997.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="START-Self-taught-Reasoner-with-Tools"><a href="#START-Self-taught-Reasoner-with-Tools" class="headerlink" title="START: Self-taught Reasoner with Tools"></a>START: Self-taught Reasoner with Tools</h2><p><strong>Authors:Chengpeng Li, Mingfeng Xue, Zhenru Zhang, Jiaxi Yang, Beichen Zhang, Xiang Wang, Bowen Yu, Binyuan Hui, Junyang Lin, Dayiheng Liu</strong></p>
<p>Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-of-thought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes. In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing the limitations of LRMs. The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., &#96;&#96;Wait, maybe using Python here is a good idea.â€™â€™) during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by fine-tuning the LRM. Through this framework, we have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å¦‚OpenAI-o1å’ŒDeepSeek-R1ï¼Œå·²ç»å±•ç°å‡ºåœ¨å¤æ‚æ¨ç†ä»»åŠ¡æ–¹é¢çš„å“è¶Šèƒ½åŠ›ï¼Œé€šè¿‡è¿ç”¨é•¿çš„æ€ç»´é“¾ï¼ˆCoTï¼‰ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¾€å¾€å­˜åœ¨äº§ç”Ÿå¹»è§‰å’Œä¸é«˜æ•ˆçš„é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬ä»…ä¾èµ–äºå†…éƒ¨æ¨ç†è¿‡ç¨‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†STARTï¼ˆå¸¦æœ‰å·¥å…·çš„è‡ªæ•™å¯¼æ¨ç†å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å·¥å…·é›†æˆé•¿CoTæ¨ç†LLMï¼Œå®ƒé€šè¿‡åˆ©ç”¨å¤–éƒ¨å·¥å…·æ¥æ˜¾è‘—å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ä»£ç æ‰§è¡Œï¼ŒSTARTèƒ½å¤Ÿæ‰§è¡Œå¤æ‚çš„è®¡ç®—ã€è‡ªæˆ‘æ£€æŸ¥ã€æ¢ç´¢å¤šæ ·åŒ–çš„æ–¹æ³•å’Œè‡ªæˆ‘è°ƒè¯•ï¼Œä»è€Œè§£å†³LRMçš„å±€é™æ€§ã€‚STARTçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶è‡ªå­¦ä¹ æ¡†æ¶ï¼Œå®ƒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®æŠ€æœ¯ï¼š1ï¼‰æç¤ºæ¨æ–­ï¼šæˆ‘ä»¬è¯æ˜åœ¨LRMçš„æ¨ç†è¿‡ç¨‹ä¸­æ’å…¥äººå·¥è®¾è®¡çš„æç¤ºï¼ˆä¾‹å¦‚ï¼Œâ€œç­‰ç­‰ï¼Œä¹Ÿè®¸åœ¨è¿™é‡Œä½¿ç”¨Pythonæ˜¯ä¸ªå¥½ä¸»æ„ã€‚â€ï¼‰æœ‰æ•ˆåœ°åˆºæ¿€äº†å…¶åˆ©ç”¨å¤–éƒ¨å·¥å…·çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€ä»»ä½•æ¼”ç¤ºæ•°æ®ã€‚æç¤ºæ¨æ–­è¿˜å¯ä»¥ä½œä¸ºä¸€ç§ç®€å•æœ‰æ•ˆçš„åºåˆ—æµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹æ³•ï¼›2ï¼‰æç¤ºæ‹’ç»é‡‡æ ·å¾®è°ƒï¼ˆHint-RFTï¼‰ï¼šHint-RFTç»“åˆäº†æç¤ºæ¨æ–­å’Œæ‹’ç»é‡‡æ ·å¾®è°ƒæŠ€æœ¯ï¼Œé€šè¿‡å¯¹ç”±LRMé€šè¿‡æç¤ºæ¨æ–­äº§ç”Ÿçš„å¸¦æœ‰å·¥å…·è°ƒç”¨çš„æ¨ç†è½¨è¿¹è¿›è¡Œè¯„åˆ†ã€è¿‡æ»¤å’Œä¿®æ”¹ï¼Œç„¶åå¯¹LRMè¿›è¡Œå¾®è°ƒã€‚é€šè¿‡è¿™ä¸ªæ¡†æ¶ï¼Œæˆ‘ä»¬å¯¹QwQ-32Bæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œå®ç°äº†STARTã€‚åœ¨åšå£«çº§ç§‘å­¦é—®ç­”ï¼ˆGPQAï¼‰ã€ç«èµ›çº§æ•°å­¦åŸºå‡†æµ‹è¯•ï¼ˆAMC23ã€AIME24ã€AIME25ï¼‰å’Œç«èµ›çº§ä»£ç åŸºå‡†æµ‹è¯•ï¼ˆLiveCodeBenchï¼‰ä¸­ï¼ŒSTARTçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º63.6%ã€95.0%ã€66.7%ã€47.1%å’Œ47.3%ã€‚å®ƒæ˜¾è‘—ä¼˜äºåŸºç¡€QwQ-32Bæ¨¡å‹ï¼Œå¹¶å®ç°äº†ä¸æœ€æ–°å¼€æºæ¨¡å‹R1-Distill-Qwen-32Bå’Œä¸“æœ‰æ¨¡å‹o1-Previewç›¸å½“çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>ç®€åŒ–ç¿»è¯‘</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04625v1">PDF</a> 38 pages, 5 figures and 6 tables</p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å¦‚OpenAI-o1å’ŒDeepSeek-R1ä¸­ï¼Œè™½ç„¶å®ƒä»¬é€šè¿‡é•¿æœŸæ€ç»´é“¾ï¼ˆCoTï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å­˜åœ¨å¹»è§†å’Œæ•ˆç‡ä¸é«˜çš„é—®é¢˜ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°çš„å·¥å…·é›†æˆé•¿æœŸæ€ç»´é“¾æ¨ç†LLMâ€”â€”STARTï¼Œå®ƒé€šè¿‡æ‰§è¡Œä»£ç ã€è‡ªæˆ‘æ£€æŸ¥ã€æ¢ç´¢å¤šæ ·æ–¹æ³•å’Œè‡ªæˆ‘è°ƒè¯•ï¼Œæ˜¾è‘—æé«˜äº†æ¨ç†èƒ½åŠ›ã€‚STARTçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶è‡ªæˆ‘å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬Hint-inferå’ŒHint Rejection Sampling Fine-Tuning (Hint-RFT)ä¸¤ä¸ªå…³é”®æŠ€æœ¯ã€‚åœ¨PhDçº§ç§‘å­¦é—®ç­”ã€ç«èµ›çº§æ•°å­¦åŸºå‡†æµ‹è¯•ä»¥åŠç«èµ›çº§ä»£ç åŸºå‡†æµ‹è¯•ä¸­ï¼ŒSTARTè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºåŸºç¡€æ¨¡å‹QwQ-32Bï¼Œå¹¶ä¸å¼€æºå…ˆè¿›æ¨¡å‹R1-Distill-Qwen-32Bå’Œä¸“æœ‰æ¨¡å‹o1-Previewç›¸å½“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹å¦‚OpenAI-o1å’ŒDeepSeek-R1åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å­˜åœ¨å¹»è§†å’Œæ•ˆç‡é—®é¢˜ã€‚</li>
<li>STARTæ˜¯ä¸€ç§æ–°å‹å·¥å…·é›†æˆé•¿æœŸæ€ç»´é“¾æ¨ç†LLMï¼Œé€šè¿‡æ‰§è¡Œä»£ç ã€è‡ªæˆ‘æ£€æŸ¥ç­‰æé«˜æ¨ç†èƒ½åŠ›ã€‚</li>
<li>STARTçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå…¶è‡ªæˆ‘å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬Hint-inferå’ŒHint-RFTä¸¤ä¸ªå…³é”®æŠ€æœ¯ã€‚</li>
<li>Hint-inferèƒ½åœ¨æ¨ç†è¿‡ç¨‹ä¸­åˆºæ¿€æ¨¡å‹åˆ©ç”¨å¤–éƒ¨å·¥å…·çš„èƒ½åŠ›ï¼Œä½œä¸ºç®€å•çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚</li>
<li>Hint-RFTç»“åˆHint-inferå’ŒRFTï¼Œå¯¹å·¥å…·è°ƒç”¨çš„æ¨ç†è½¨è¿¹è¿›è¡Œè¯„åˆ†ã€è¿‡æ»¤å’Œä¿®æ”¹ï¼Œç„¶åå¾®è°ƒæ¨¡å‹ã€‚</li>
<li>STARTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œæ˜¾è‘—ä¼˜äºåŸºç¡€æ¨¡å‹QwQ-32Bã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04625">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b9016a06e7cdff43140707c320d8473e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8584e5b3cd5d2545d74fb340690ba353.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bae8042537e82a54c3d6d86204b9e028.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b58eda2b46e7a065a1bfb7629fee2d02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4789acbe11f95faf3cc142bf2bbcd51.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d65ce562e6222939aa449cb6a2e3a03f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="A-Benchmark-for-Multi-Lingual-Vision-Language-Learning-in-Remote-Sensing-Image-Captioning"><a href="#A-Benchmark-for-Multi-Lingual-Vision-Language-Learning-in-Remote-Sensing-Image-Captioning" class="headerlink" title="A Benchmark for Multi-Lingual Vision-Language Learning in Remote Sensing   Image Captioning"></a>A Benchmark for Multi-Lingual Vision-Language Learning in Remote Sensing   Image Captioning</h2><p><strong>Authors:Qing Zhou, Tao Yang, Junyu Gao, Weiping Ni, Junzheng Wu, Qi Wang</strong></p>
<p>Remote Sensing Image Captioning (RSIC) is a cross-modal field bridging vision and language, aimed at automatically generating natural language descriptions of features and scenes in remote sensing imagery. Despite significant advances in developing sophisticated methods and large-scale datasets for training vision-language models (VLMs), two critical challenges persist: the scarcity of non-English descriptive datasets and the lack of multilingual capability evaluation for models. These limitations fundamentally impede the progress and practical deployment of RSIC, particularly in the era of large VLMs. To address these challenges, this paper presents several significant contributions to the field. First, we introduce and analyze BRSIC (Bilingual Remote Sensing Image Captioning), a comprehensive bilingual dataset that enriches three established English RSIC datasets with Chinese descriptions, encompassing 13,634 images paired with 68,170 bilingual captions. Building upon this foundation, we develop a systematic evaluation framework that addresses the prevalent inconsistency in evaluation protocols, enabling rigorous assessment of model performance through standardized retraining procedures on BRSIC. Furthermore, we present an extensive empirical study of eight state-of-the-art large vision-language models (LVLMs), examining their capabilities across multiple paradigms including zero-shot inference, supervised fine-tuning, and multi-lingual training. This comprehensive evaluation provides crucial insights into the strengths and limitations of current LVLMs in handling multilingual remote sensing tasks. Additionally, our cross-dataset transfer experiments reveal interesting findings. The code and data will be available at <a target="_blank" rel="noopener" href="https://github.com/mrazhou/BRSIC">https://github.com/mrazhou/BRSIC</a>. </p>
<blockquote>
<p>é¥æ„Ÿå›¾åƒæ ‡æ³¨ï¼ˆRSICï¼‰æ˜¯ä¸€ä¸ªè·¨æ¨¡æ€é¢†åŸŸï¼Œæ—¨åœ¨å¼¥åˆè§†è§‰å’Œè¯­è¨€ä¹‹é—´çš„é¸¿æ²Ÿï¼Œç›®æ ‡æ˜¯è‡ªåŠ¨å¯¹é¥æ„Ÿå›¾åƒä¸­çš„ç‰¹å¾å’Œåœºæ™¯ç”Ÿæˆè‡ªç„¶è¯­è¨€æè¿°ã€‚å°½ç®¡åœ¨ä¸ºè®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å¼€å‘é«˜çº§æ–¹æ³•å’Œå¤§è§„æ¨¡æ•°æ®é›†æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ä»ç„¶å­˜åœ¨ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šéè‹±è¯­æè¿°æ€§æ•°æ®é›†çš„ç¨€ç¼ºå’Œæ¨¡å‹ç¼ºä¹å¤šè¯­è¨€èƒ½åŠ›è¯„ä¼°ã€‚è¿™äº›å±€é™æ€§ä»æ ¹æœ¬ä¸Šé˜»ç¢äº†RSICçš„è¿›æ­¥å’Œå®é™…éƒ¨ç½²ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹VLMæ—¶ä»£ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡ä¸ºè¯¥é¢†åŸŸåšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»‹ç»å¹¶åˆ†æäº†BRSICï¼ˆåŒè¯­é¥æ„Ÿå›¾åƒæ ‡æ³¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»¼åˆåŒè¯­æ•°æ®é›†ï¼Œä¸°å¯Œäº†ä¸‰ä¸ªç°æœ‰çš„è‹±è¯­RSICæ•°æ®é›†ï¼Œå¢åŠ äº†ä¸­æ–‡æè¿°ï¼ŒåŒ…å«13634å¼ å›¾åƒå’Œé…å¯¹çš„ä¸­è‹±æ–‡åŒè¯­æè¿°å…±68170æ¡ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªç³»ç»Ÿè¯„ä¼°æ¡†æ¶ï¼Œè§£å†³äº†è¯„ä¼°åè®®ä¸­æ™®éå­˜åœ¨çš„ä¸ä¸€è‡´é—®é¢˜ï¼Œèƒ½å¤Ÿé€šè¿‡åœ¨BRSICä¸Šè¿›è¡Œæ ‡å‡†åŒ–çš„é‡æ–°è®­ç»ƒç¨‹åºï¼Œå¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œä¸¥æ ¼çš„è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹å…«ç§æœ€å…ˆè¿›çš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰è¿›è¡Œäº†å¹¿æ³›çš„å®è¯ç ”ç©¶ï¼Œè€ƒå¯Ÿäº†å®ƒä»¬åœ¨é›¶æ ·æœ¬æ¨ç†ã€ç›‘ç£å¾®è°ƒå’Œå¤šè¯­è¨€è®­ç»ƒç­‰å¤šç§èŒƒå¼ä¸‹çš„èƒ½åŠ›ã€‚è¿™ä¸€ç»¼åˆè¯„ä¼°æä¾›äº†å…³äºå½“å‰LVLMåœ¨å¤„ç†å¤šè¯­è¨€é¥æ„Ÿä»»åŠ¡æ–¹é¢çš„ä¼˜åŠ¿å’Œå±€é™æ€§çš„å…³é”®è§è§£ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„è·¨æ•°æ®é›†è½¬ç§»å®éªŒè¿˜æ­ç¤ºäº†ä¸€äº›æœ‰è¶£çš„å‘ç°ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å°†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/mrazhou/BRSIC%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/mrazhou/BRSICè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04592v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬è®ºæ–‡é’ˆå¯¹é¥æ„Ÿå›¾åƒè‡ªåŠ¨æè¿°ç”Ÿæˆé¢†åŸŸå­˜åœ¨çš„éè‹±è¯­æè¿°æ•°æ®é›†ç¨€ç¼ºå’Œæ¨¡å‹ç¼ºä¹å¤šè¯­è¨€èƒ½åŠ›è¯„ä¼°çš„é—®é¢˜ï¼Œæå‡ºäº†BRSICåŒè¯­é¥æ„Ÿå›¾åƒæè¿°æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«ä¸­æ–‡æè¿°ï¼Œä¸°å¯Œäº†ä¸‰ä¸ªå·²æœ‰çš„è‹±è¯­RSICæ•°æ®é›†ã€‚åŒæ—¶ï¼Œè®ºæ–‡å»ºç«‹äº†ç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶ï¼Œè§£å†³äº†è¯„ä¼°åè®®çš„ä¸ä¸€è‡´æ€§ï¼Œå¹¶è¿›è¡Œäº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¹¿æ³›å®è¯ç ”ç©¶ï¼Œæ¢ç´¢äº†é›¶æ ·æœ¬æ¨ç†ã€ç›‘ç£å¾®è°ƒåŠå¤šè¯­è¨€è®­ç»ƒç­‰å¤šç§èŒƒå¼ä¸‹çš„æ¨¡å‹èƒ½åŠ›ã€‚è¿™ä¸ºå½“å‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å¤„ç†å¤šè¯­è¨€é¥æ„Ÿä»»åŠ¡æä¾›äº†å…³é”®è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BRSICæ•°æ®é›†ä»‹ç»ï¼šå¼•å…¥å¹¶åˆ†æBRSICåŒè¯­é¥æ„Ÿå›¾åƒæè¿°æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä¸°å¯Œäº†å·²æœ‰çš„è‹±è¯­RSICæ•°æ®é›†ï¼ŒåŒ…å«ä¸­æ–‡æè¿°ã€‚</li>
<li>ç³»ç»Ÿè¯„ä¼°æ¡†æ¶çš„å»ºç«‹ï¼šä¸ºè§£å†³è¯„ä¼°åè®®çš„ä¸ä¸€è‡´æ€§ï¼Œè®ºæ–‡å»ºç«‹äº†ç³»ç»Ÿçš„è¯„ä¼°æ¡†æ¶ï¼Œä¸ºæ¨¡å‹æ€§èƒ½æä¾›äº†æ ‡å‡†åŒ–çš„é‡æ–°è®­ç»ƒç¨‹åºã€‚</li>
<li>å¤šèŒƒå¼ä¸‹çš„æ¨¡å‹èƒ½åŠ›è¯„ä¼°ï¼šè®ºæ–‡è¿›è¡Œäº†å¹¿æ³›çš„å®è¯ç ”ç©¶ï¼Œæ¢ç´¢äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬æ¨ç†ã€ç›‘ç£å¾®è°ƒåŠå¤šè¯­è¨€è®­ç»ƒç­‰å¤šç§èŒƒå¼ä¸‹çš„èƒ½åŠ›ã€‚</li>
<li>å½“å‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„ä¼˜ç¼ºç‚¹ï¼šé€šè¿‡å¯¹BRSICæ•°æ®é›†çš„å®è¯ç ”ç©¶ï¼Œæ­ç¤ºäº†å½“å‰å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤šè¯­è¨€é¥æ„Ÿä»»åŠ¡æ—¶çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚</li>
<li>è·¨æ•°æ®é›†è½¬ç§»å®éªŒçš„å‘ç°ï¼šé€šè¿‡è·¨æ•°æ®é›†çš„è½¬ç§»å®éªŒï¼Œå¾—å‡ºäº†æœ‰å…³ä¸åŒæ¨¡å‹å’Œæ–¹æ³•çš„æœ‰è¶£å‘ç°ã€‚</li>
<li>æ•°æ®å’Œä»£ç å…¬å¼€å¯ç”¨ï¼šè®ºæ–‡æ˜ç¡®æŒ‡å‡ºï¼Œç›¸å…³æ•°æ®å’Œä»£ç å¯åœ¨æŒ‡å®šç½‘ç«™ä¸‹è½½ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3db24c7e8d9080806be41cb768133536.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6c978a0c6e95f11ae9d8b9d27913602.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54c36bf8bc91b9390173939062d6a25c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a23c3409fbb1b20b591e555de992f791.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39f2e5059b9bf6505bceb190dc1d47e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0be7b1417f0bbf62f5121c7057cb8082.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="An-Empirical-Study-on-Eliciting-and-Improving-R1-like-Reasoning-Models"><a href="#An-Empirical-Study-on-Eliciting-and-Improving-R1-like-Reasoning-Models" class="headerlink" title="An Empirical Study on Eliciting and Improving R1-like Reasoning Models"></a>An Empirical Study on Eliciting and Improving R1-like Reasoning Models</h2><p><strong>Authors:Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, Lei Fang, Zhongyuan Wang, Ji-Rong Wen</strong></p>
<p>In this report, we present the third technical report on the development of slow-thinking models as part of the STILL project. As the technical pathway becomes clearer, scaling RL training has become a central technique for implementing such reasoning models. We systematically experiment with and document the effects of various factors influencing RL training, conducting experiments on both base models and fine-tuned models. Specifically, we demonstrate that our RL training approach consistently improves the Qwen2.5-32B base models, enhancing both response length and test accuracy. Furthermore, we show that even when a model like DeepSeek-R1-Distill-Qwen-1.5B has already achieved a high performance level, it can be further refined through RL training, reaching an accuracy of 39.33% on AIME 2024. Beyond RL training, we also explore the use of tool manipulation, finding that it significantly boosts the reasoning performance of large reasoning models. This approach achieves a remarkable accuracy of 86.67% with greedy search on AIME 2024, underscoring its effectiveness in enhancing model capabilities. We release our resources at the STILL project website: <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Slow_Thinking_with_LLMs">https://github.com/RUCAIBox/Slow_Thinking_with_LLMs</a>. </p>
<blockquote>
<p>åœ¨è¿™ä»½æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä½œä¸ºSTILLé¡¹ç›®ä¸€éƒ¨åˆ†çš„æ…¢æ€è€ƒæ¨¡å‹å‘å±•çš„ç¬¬ä¸‰ä¸ªæŠ€æœ¯æŠ¥å‘Šã€‚éšç€æŠ€æœ¯è·¯å¾„çš„æ˜ç¡®ï¼Œè§„æ¨¡åŒ–å¼ºåŒ–å­¦ä¹ è®­ç»ƒå·²æˆä¸ºå®ç°è¿™ç§æ¨ç†æ¨¡å‹çš„æ ¸å¿ƒæŠ€æœ¯ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°å®éªŒå¹¶è®°å½•äº†å½±å“å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„å„ç§å› ç´ ï¼Œå¯¹åŸºç¡€æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹éƒ½è¿›è¡Œäº†å®éªŒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ³•æŒç»­æ”¹å–„äº†Qwen2.5-32BåŸºç¡€æ¨¡å‹ï¼Œæé«˜äº†å“åº”é•¿åº¦å’Œæµ‹è¯•å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œå³ä½¿åƒDeepSeek-R1-Distill-Qwen-1.5Bè¿™æ ·çš„æ¨¡å‹å·²ç»è¾¾åˆ°äº†é«˜æ€§èƒ½æ°´å¹³ï¼Œä¹Ÿå¯ä»¥é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿›ä¸€æ­¥æ”¹è¿›ï¼Œåœ¨AIME 2024ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†39.33%ã€‚é™¤äº†å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†å·¥å…·æ“ä½œçš„ä½¿ç”¨ï¼Œå‘ç°å®ƒæ˜¾è‘—æé«˜äº†å¤§å‹æ¨ç†æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•åœ¨AIME 2024ä¸Šé€šè¿‡è´ªå¿ƒæœç´¢å®ç°äº†86.67%çš„å‡†ç¡®ç‡ï¼Œå‡¸æ˜¾äº†å…¶åœ¨æé«˜æ¨¡å‹èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬åœ¨STILLé¡¹ç›®ç½‘ç«™ä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„èµ„æºï¼š<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Slow_Thinking_with_LLMs%E3%80%82">https://github.com/RUCAIBox/Slow_Thinking_with_LLMsã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04548v1">PDF</a> Technical Report on Slow Thinking with LLMs: Part III</p>
<p><strong>Summary</strong><br>åœ¨æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä½œä¸ºSTILé¡¹ç›®ä¸€éƒ¨åˆ†çš„æ…¢æ€è€ƒæ¨¡å‹å‘å±•çš„ç¬¬ä¸‰ä¸ªæŠ€æœ¯æŠ¥å‘Šã€‚éšç€æŠ€æœ¯è·¯å¾„çš„æ¸…æ™°ï¼Œè§„æ¨¡åŒ–å¼ºåŒ–å­¦ä¹ è®­ç»ƒå·²æˆä¸ºå®ç°æ­¤ç±»æ¨ç†æ¨¡å‹çš„å…³é”®æŠ€æœ¯ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°å®éªŒå¹¶è®°å½•äº†å½±å“å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„å¤šç§å› ç´ ï¼Œå¹¶åœ¨åŸºç¡€æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒã€‚æˆ‘ä»¬è¯æ˜ï¼Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ³•èƒ½å¤ŸæŒç»­æé«˜Qwen2.5-32BåŸºç¡€æ¨¡å‹çš„å“åº”é•¿åº¦å’Œæµ‹è¯•ç²¾åº¦ã€‚å³ä½¿åƒDeepSeek-R1-Distill-Qwen-1.5Bè¿™æ ·çš„æ¨¡å‹å·²ç»å¤„äºé«˜æ°´å¹³æ€§èƒ½çŠ¶æ€ï¼Œä¹Ÿèƒ½é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿›ä¸€æ­¥ç²¾è¿›ï¼Œåœ¨AIME 2024ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†39.33%ã€‚é™¤äº†å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†å·¥å…·æ“çºµçš„ä½¿ç”¨ï¼Œå‘ç°å®ƒèƒ½æ˜¾è‘—æé«˜å¤§å‹æ¨ç†æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨AIME 2024ä¸Šé€šè¿‡è´ªå¿ƒæœç´¢å–å¾—äº†86.67%çš„æƒŠäººå‡†ç¡®ç‡ï¼Œçªæ˜¾å…¶åœ¨å¢å¼ºæ¨¡å‹èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å·²å°†èµ„æºå‘å¸ƒåœ¨STILé¡¹ç›®ç½‘ç«™ä¸Šï¼š<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/Slow_Thinking_with_LLMs">https://github.com/RUCAIBox/Slow_Thinking_with_LLMs</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŠ¥å‘Šä»‹ç»äº†æ…¢æ€è€ƒæ¨¡å‹å‘å±•çš„ç¬¬ä¸‰ä¸ªæŠ€æœ¯æŠ¥å‘Šï¼Œä½œä¸ºSTILé¡¹ç›®çš„ä¸€éƒ¨åˆ†ã€‚</li>
<li>è§„æ¨¡åŒ–å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ˜¯å®ç°æ­¤ç±»æ¨ç†æ¨¡å‹çš„å…³é”®æŠ€æœ¯ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ³•èƒ½æŒç»­æé«˜Qwen2.5-32Bæ¨¡å‹çš„å“åº”é•¿åº¦å’Œæµ‹è¯•ç²¾åº¦ã€‚</li>
<li>å·²ç»é«˜æ€§èƒ½çš„æ¨¡å‹å¦‚DeepSeek-R1-Distill-Qwen-1.5Bå¯é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ç²¾è¿›ã€‚</li>
<li>åœ¨AIME 2024ä¸Šï¼Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒåçš„æ¨¡å‹å‡†ç¡®ç‡è¾¾åˆ°äº†39.33%ã€‚</li>
<li>é™¤äº†å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæ¢ç´¢äº†å·¥å…·æ“çºµçš„ä½¿ç”¨ï¼Œæ˜¾è‘—æé«˜äº†å¤§å‹æ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04548">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-70226c64439aeb02ced53e2c5bb11a10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99af079be8e800a6b119fa378c1b6079.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b318a9f229f8822e4cdda3d0680e1974.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SOLAR-Scalable-Optimization-of-Large-scale-Architecture-for-Reasoning"><a href="#SOLAR-Scalable-Optimization-of-Large-scale-Architecture-for-Reasoning" class="headerlink" title="SOLAR: Scalable Optimization of Large-scale Architecture for Reasoning"></a>SOLAR: Scalable Optimization of Large-scale Architecture for Reasoning</h2><p><strong>Authors:Chen Li, Yinyi Luo, Anudeep Bolimera, Marios Savvides</strong></p>
<p>Large Language Models (LLMs) excel in reasoning but remain constrained by their Chain-of-Thought (CoT) approach, which struggles with complex tasks requiring more nuanced topological reasoning. We introduce SOLAR, Scalable Optimization of Large-scale Architecture for Reasoning, a framework that dynamically optimizes various reasoning topologies to enhance accuracy and efficiency.   Our Topological Annotation Generation (TAG) system automates topological dataset creation and segmentation, improving post-training and evaluation. Additionally, we propose Topological-Scaling, a reward-driven framework that aligns training and inference scaling, equipping LLMs with adaptive, task-aware reasoning.   SOLAR achieves substantial gains on MATH and GSM8K: +5% accuracy with Topological Tuning, +9% with Topological Reward, and +10.02% with Hybrid Scaling. It also reduces response length by over 5% for complex problems, lowering inference latency.   To foster the reward system, we train a multi-task Topological Reward Model (M-TRM), which autonomously selects the best reasoning topology and answer in a single pass, eliminating the need for training and inference on multiple single-task TRMs (S-TRMs), thus reducing both training cost and inference latency. In addition, in terms of performance, M-TRM surpasses all S-TRMs, improving accuracy by +10% and rank correlation by +9%.   To the best of our knowledge, SOLAR sets a new benchmark for scalable, high-precision LLM reasoning while introducing an automated annotation process and a dynamic reasoning topology competition mechanism. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä»å—åˆ°å…¶æ€ç»´é“¾ï¼ˆCoTï¼‰æ–¹æ³•çš„é™åˆ¶ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†éœ€è¦æ›´å¾®å¦™æ‹“æ‰‘æ¨ç†çš„å¤æ‚ä»»åŠ¡æ—¶è¡¨ç°æŒ£æ‰ã€‚æˆ‘ä»¬å¼•å…¥äº†SOLARï¼ˆç”¨äºæ¨ç†çš„å¤§è§„æ¨¡æ¶æ„çš„å¯æ‰©å±•ä¼˜åŒ–ï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥åŠ¨æ€ä¼˜åŒ–å„ç§æ¨ç†æ‹“æ‰‘ï¼Œä»¥æé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬çš„æ‹“æ‰‘æ ‡æ³¨ç”Ÿæˆï¼ˆTAGï¼‰ç³»ç»Ÿå¯ä»¥è‡ªåŠ¨åˆ›å»ºæ‹“æ‰‘æ•°æ®é›†å¹¶è¿›è¡Œåˆ†å‰²ï¼Œæ”¹è¿›äº†è®­ç»ƒåçš„æ€§èƒ½å’Œè¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æ‹“æ‰‘ç¼©æ”¾ï¼Œè¿™æ˜¯ä¸€ç§ä»¥å¥–åŠ±ä¸ºé©±åŠ¨åŠ›çš„æ¡†æ¶ï¼Œå¯ä»¥å®ç°å¯¹è®­ç»ƒå’Œæ¨ç†ç¼©æ”¾çš„åŒ¹é…ï¼Œä½¿LLMå…·å¤‡è‡ªé€‚åº”çš„ä»»åŠ¡æ„ŸçŸ¥æ¨ç†èƒ½åŠ›ã€‚SOLARåœ¨MATHå’ŒGSM8Kä¸Šå®ç°äº†é‡å¤§æ”¶ç›Šï¼šé€šè¿‡æ‹“æ‰‘è°ƒæ•´æé«˜5%çš„å‡†ç¡®ç‡ï¼Œé€šè¿‡æ‹“æ‰‘å¥–åŠ±æé«˜9%ï¼Œé€šè¿‡æ··åˆç¼©æ”¾æé«˜10.02%ã€‚å®ƒè¿˜å°†å¤æ‚é—®é¢˜çš„å“åº”é•¿åº¦ç¼©çŸ­äº†è¶…è¿‡5%ï¼Œé™ä½äº†æ¨ç†å»¶è¿Ÿã€‚ä¸ºäº†ä¿ƒè¿›å¥–åŠ±ç³»ç»Ÿçš„å»ºç«‹ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå¤šä»»åŠ¡æ‹“æ‰‘å¥–åŠ±æ¨¡å‹ï¼ˆM-TRMï¼‰ï¼Œè¯¥æ¨¡å‹å¯ä»¥åœ¨ä¸€æ¬¡ä¼ é€’ä¸­è‡ªä¸»é€‰æ‹©æœ€ä½³çš„æ¨ç†æ‹“æ‰‘å’Œç­”æ¡ˆï¼Œä»è€Œæ— éœ€åœ¨å¤šä¸ªå•ä»»åŠ¡TRMï¼ˆS-TRMï¼‰ä¸Šè¿›è¡Œè®­ç»ƒå’Œæ¨ç†ï¼Œé™ä½äº†è®­ç»ƒæˆæœ¬å’Œæ¨ç†å»¶è¿Ÿã€‚æ­¤å¤–ï¼Œå°±æ€§èƒ½è€Œè¨€ï¼ŒM-TRMè¶…è¶Šäº†æ‰€æœ‰S-TRMï¼Œæé«˜äº†10%çš„å‡†ç¡®ç‡å’Œ9%çš„æ’åç›¸å…³æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒSOLARä¸ºå¯æ‰©å±•ã€é«˜ç²¾åº¦çš„LLMæ¨ç†è®¾å®šäº†æ–°çš„åŸºå‡†ï¼ŒåŒæ—¶å¼•å…¥äº†è‡ªåŠ¨åŒ–çš„æ³¨é‡Šè¿‡ç¨‹å’ŒåŠ¨æ€çš„æ¨ç†æ‹“æ‰‘ç«äº‰æœºåˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04530v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä»å—åˆ°é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ–¹æ³•çš„é™åˆ¶ï¼Œéš¾ä»¥åº”å¯¹éœ€è¦æ›´ç²¾ç»†æ‹“æ‰‘æ¨ç†çš„å¤æ‚ä»»åŠ¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SOLARæ¡†æ¶ï¼Œèƒ½å¤ŸåŠ¨æ€ä¼˜åŒ–å„ç§æ¨ç†æ‹“æ‰‘ç»“æ„ï¼Œæé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æˆ‘ä»¬çš„æ‹“æ‰‘æ ‡æ³¨ç”Ÿæˆï¼ˆTAGï¼‰ç³»ç»Ÿå¯è‡ªåŠ¨åˆ›å»ºæ‹“æ‰‘æ•°æ®é›†å¹¶è¿›è¡Œåˆ†å‰²ï¼Œæ”¹è¿›äº†åè®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†æ‹“æ‰‘ç¼©æ”¾å¥–åŠ±é©±åŠ¨æ¡†æ¶ï¼Œä½¿è®­ç»ƒä¸æ¨ç†ç¼©æ”¾ç›¸åè°ƒï¼Œèµ‹äºˆLLMè‡ªé€‚åº”çš„ä»»åŠ¡æ„ŸçŸ¥æ¨ç†èƒ½åŠ›ã€‚SOLARåœ¨MATHå’ŒGSM8Kä¸Šå®ç°äº†æ˜¾è‘—çš„æå‡ï¼Œé€šè¿‡æ‹“æ‰‘è°ƒæ•´ã€æ‹“æ‰‘å¥–åŠ±å’Œæ··åˆç¼©æ”¾ç­‰æ–¹æ³•åˆ†åˆ«æé«˜äº†5%ã€9%å’Œ10.02%çš„å‡†ç¡®ç‡ã€‚åŒæ—¶ï¼Œå¯¹äºå¤æ‚é—®é¢˜ï¼Œå®ƒç¼©çŸ­äº†å“åº”é•¿åº¦è¶…è¿‡5%ï¼Œé™ä½äº†æ¨ç†å»¶è¿Ÿã€‚ä¸ºäº†ä¿ƒè¿›å¥–åŠ±ç³»ç»Ÿçš„å‘å±•ï¼Œæˆ‘ä»¬è®­ç»ƒäº†å¤šä»»åŠ¡æ‹“æ‰‘å¥–åŠ±æ¨¡å‹ï¼ˆM-TRMï¼‰ï¼Œèƒ½å¤Ÿè‡ªä¸»é€‰æ‹©æœ€ä½³æ¨ç†æ‹“æ‰‘å’Œç­”æ¡ˆï¼Œæ¶ˆé™¤äº†å¯¹å¤šä¸ªå•ä»»åŠ¡TRMï¼ˆS-TRMï¼‰çš„è®­ç»ƒå’Œæ¨ç†éœ€æ±‚ï¼Œé™ä½äº†è®­ç»ƒæˆæœ¬å’Œæ¨ç†å»¶è¿Ÿã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒSOLARä¸ºå¯æ‰©å±•çš„é«˜ç²¾åº¦LLMæ¨ç†è®¾å®šäº†æ–°çš„åŸºå‡†ï¼ŒåŒæ—¶å¼•å…¥äº†è‡ªåŠ¨åŒ–çš„æ ‡æ³¨è¿‡ç¨‹å’ŒåŠ¨æ€çš„æ¨ç†æ‹“æ‰‘ç«äº‰æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsè™½ç„¶æ“…é•¿æ¨ç†ï¼Œä½†å—é™äºChain-of-Thoughtï¼ˆCoTï¼‰æ–¹æ³•ï¼Œéš¾ä»¥å¤„ç†å¤æ‚ä»»åŠ¡çš„ç²¾ç»†æ‹“æ‰‘æ¨ç†ã€‚</li>
<li>SOLARæ¡†æ¶è¢«å¼•å…¥ï¼Œä»¥åŠ¨æ€ä¼˜åŒ–å„ç§æ¨ç†æ‹“æ‰‘ç»“æ„ï¼Œæé«˜LLMçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>TAGç³»ç»Ÿè‡ªåŠ¨åŒ–æ‹“æ‰‘æ•°æ®é›†åˆ›å»ºå’Œåˆ†å‰²ï¼Œæ”¹è¿›äº†è®­ç»ƒåçš„è¯„ä¼°å’Œè¿‡ç¨‹ã€‚</li>
<li>æå‡ºäº†æ‹“æ‰‘ç¼©æ”¾å¥–åŠ±é©±åŠ¨æ¡†æ¶ï¼Œä½¿LLMå…·å¤‡è‡ªé€‚åº”ä»»åŠ¡æ„ŸçŸ¥æ¨ç†èƒ½åŠ›ã€‚</li>
<li>SOLARåœ¨ç‰¹å®šä»»åŠ¡ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬æé«˜å‡†ç¡®ç‡å’Œé™ä½å“åº”é•¿åº¦ä¸æ¨ç†å»¶è¿Ÿã€‚</li>
<li>M-TRMæ¨¡å‹è¢«è®­ç»ƒå‡ºæ¥ï¼Œèƒ½å¤Ÿè‡ªä¸»é€‰æ‹©æœ€ä½³æ¨ç†æ‹“æ‰‘å’Œç­”æ¡ˆï¼Œé™ä½äº†è®­ç»ƒæˆæœ¬å’Œæ¨ç†æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04530">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3eadce84ca3a7e1ce3b98823121839b9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c3f1774af46b63be70d2fd4be52849a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Modular-Reasoning-about-Error-Bounds-for-Concurrent-Probabilistic-Programs"><a href="#Modular-Reasoning-about-Error-Bounds-for-Concurrent-Probabilistic-Programs" class="headerlink" title="Modular Reasoning about Error Bounds for Concurrent Probabilistic   Programs"></a>Modular Reasoning about Error Bounds for Concurrent Probabilistic   Programs</h2><p><strong>Authors:Kwing Hei Li, Alejandro Aguirre, Simon Oddershede Gregersen, Philipp G. Haselwarter, Joseph Tassarotti, Lars Birkedal</strong></p>
<p>We present Coneris, the first higher-order concurrent separation logic for reasoning about error probability bounds of higher-order concurrent probabilistic programs with higher-order state. To support modular reasoning about concurrent (non-probabilistic) program modules, state-of-the-art program logics internalize the classic notion of linearizability within the logic through the concept of logical atomicity.   Coneris extends this idea to probabilistic concurrent program modules. Thus Coneris supports modular reasoning about probabilistic concurrent modules by capturing a novel notion of randomized logical atomicity within the logic. To do so, Coneris utilizes presampling tapes and a novel probabilistic update modality to describe how state is changed probabilistically at linearization points. We demonstrate this approach by means of smaller synthetic examples and larger case studies.   All of the presented results, including the meta-theory, have been mechanized in the Rocq proof assistant and the Iris separation logic framework </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†Conerisï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªç”¨äºæ¨ç†é«˜é˜¶å¹¶å‘æ¦‚ç‡ç¨‹åºçš„é”™è¯¯æ¦‚ç‡ç•Œé™çš„é«˜é˜¶å¹¶å‘åˆ†ç¦»é€»è¾‘ï¼Œè¿™ç±»ç¨‹åºå…·æœ‰é«˜é˜¶çŠ¶æ€ã€‚ä¸ºäº†æ”¯æŒå¯¹å¹¶å‘ï¼ˆéæ¦‚ç‡ï¼‰ç¨‹åºæ¨¡å—çš„æ¨¡å—åŒ–æ¨ç†ï¼Œæœ€å…ˆè¿›çš„ç¨‹åºé€»è¾‘é€šè¿‡é€»è¾‘åŸå­æ€§çš„æ¦‚å¿µï¼Œåœ¨é€»è¾‘å†…éƒ¨åŒ–äº†çº¿æ€§åŒ–çš„ç»å…¸æ¦‚å¿µã€‚Coneriså°†è¿™ä¸€ç†å¿µæ‰©å±•åˆ°æ¦‚ç‡å¹¶å‘ç¨‹åºæ¨¡å—ã€‚å› æ­¤ï¼ŒConerisé€šè¿‡æ•æ‰é€»è¾‘ä¸­çš„éšæœºé€»è¾‘åŸå­æ€§çš„æ–°æ¦‚å¿µï¼Œæ”¯æŒå¯¹æ¦‚ç‡å¹¶å‘æ¨¡å—çš„æ¨¡å—åŒ–æ¨ç†ã€‚ä¸ºæ­¤ï¼ŒConerisåˆ©ç”¨é¢„é‡‡æ ·ç£å¸¦å’Œæ–°é¢–çš„æ¦‚ç‡æ›´æ–°æ¨¡å¼æ¥æè¿°çº¿æ€§åŒ–ç‚¹å¤„çŠ¶æ€å¦‚ä½•ä»¥æ¦‚ç‡æ–¹å¼å‘ç”Ÿå˜åŒ–ã€‚æˆ‘ä»¬é€šè¿‡è¾ƒå°çš„åˆæˆç¤ºä¾‹å’Œè¾ƒå¤§çš„æ¡ˆä¾‹ç ”ç©¶æ¥è¯æ˜è¿™ç§æ–¹æ³•ã€‚æ‰€æœ‰ç»“æœï¼ŒåŒ…æ‹¬å…ƒç†è®ºï¼Œéƒ½åœ¨Rocqè¯æ˜åŠ©æ‰‹å’ŒIrisåˆ†ç¦»é€»è¾‘æ¡†æ¶ä¸­æœºæ¢°åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04512v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨æ¦‚ç‡å¹¶å‘ç¨‹åºçš„ä¸Šä¸‹æ–‡ä¸­ï¼ŒConerisæ˜¯é¦–ä¸ªç”¨äºæ¨ç†é«˜é˜¶å¹¶å‘åˆ†ç¦»é€»è¾‘çš„é€»è¾‘æ€ç»´å·¥å…·ã€‚å®ƒæ”¯æŒæ¨¡å—åŒ–æ¨ç†å…³äºå¹¶å‘ï¼ˆéæ¦‚ç‡ï¼‰ç¨‹åºæ¨¡å—ï¼Œå¹¶é€šè¿‡é€»è¾‘åŸå­æ€§çš„æ¦‚å¿µå°†çº¿æ€§åŒ–ç†å¿µå†…åŒ–äºé€»è¾‘ä¸­ã€‚Coneriså°†æ­¤ç†å¿µæ‰©å±•è‡³æ¦‚ç‡å¹¶å‘ç¨‹åºæ¨¡å—ï¼Œé€šè¿‡åœ¨é€»è¾‘ä¸­æ•æ‰éšæœºé€»è¾‘åŸå­æ€§çš„æ–°é¢–æ¦‚å¿µæ¥æ”¯æŒæ¨¡å—åŒ–æ¨ç†å…³äºæ¦‚ç‡å¹¶å‘æ¨¡å—ã€‚Conerisä½¿ç”¨é¢„é‡‡æ ·ç£å¸¦å’Œæ–°é¢–çš„æ¦‚ç‡æ›´æ–°æ¨¡å¼æ¥æè¿°çº¿æ€§åŒ–ç‚¹ä¸ŠçŠ¶æ€å¦‚ä½•ä»¥æ¦‚ç‡æ–¹å¼å‘ç”Ÿå˜åŒ–ã€‚é€šè¿‡å°å‹åˆæˆç¤ºä¾‹å’Œå¤§å‹æ¡ˆä¾‹ç ”ç©¶è¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æ‰€æœ‰ç»“æœï¼ŒåŒ…æ‹¬å…ƒç†è®ºï¼Œéƒ½åœ¨Rocqè¯æ˜åŠ©ç†å’ŒIrisåˆ†ç¦»é€»è¾‘æ¡†æ¶ä¸­æœºæ¢°åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Conerisæ˜¯é¦–ä¸ªç”¨äºæ¨ç†å…³äºé«˜é˜¶å¹¶å‘æ¦‚ç‡ç¨‹åºçš„é”™è¯¯æ¦‚ç‡ç•Œé™çš„é«˜é˜¶å¹¶å‘åˆ†ç¦»é€»è¾‘ã€‚</li>
<li>å®ƒæ”¯æŒæ¨¡å—åŒ–æ¨ç†å…³äºå¹¶å‘ï¼ˆéæ¦‚ç‡ï¼‰ç¨‹åºæ¨¡å—ï¼Œé€šè¿‡é€»è¾‘åŸå­æ€§çš„æ¦‚å¿µå†…åŒ–çº¿æ€§åŒ–ç†å¿µã€‚</li>
<li>Coneriså°†è¿™ä¸€ç†å¿µæ‰©å±•åˆ°æ¦‚ç‡å¹¶å‘ç¨‹åºæ¨¡å—ï¼Œå¼•å…¥éšæœºé€»è¾‘åŸå­æ€§çš„æ¦‚å¿µã€‚</li>
<li>Conerisä½¿ç”¨é¢„é‡‡æ ·ç£å¸¦å’Œæ¦‚ç‡æ›´æ–°æ¨¡å¼æ¥æè¿°çŠ¶æ€åœ¨çº¿æ€§åŒ–ç‚¹ä¸Šçš„æ¦‚ç‡å˜åŒ–ã€‚</li>
<li>Conerisé€šè¿‡å°å‹å’Œå¤§å‹æ¡ˆä¾‹ç ”ç©¶è¯æ˜äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ‰€æœ‰ç»“æœï¼ŒåŒ…æ‹¬å…ƒç†è®ºï¼Œéƒ½åœ¨Rocqè¯æ˜åŠ©ç†ä¸­æœºæ¢°åŒ–ï¼Œè¯¥å·¥å…·ç”¨äºéªŒè¯é€»è¾‘å’Œæ¨ç†çš„æ­£ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04512">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-fe2086baedeb99732e0ef4876e00f297.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ceadb2ae05f3918f84767c11c67b7bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ad7deb7dc23d577058b0c4d294339d3.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="TRACT-Regression-Aware-Fine-tuning-Meets-Chain-of-Thought-Reasoning-for-LLM-as-a-Judge"><a href="#TRACT-Regression-Aware-Fine-tuning-Meets-Chain-of-Thought-Reasoning-for-LLM-as-a-Judge" class="headerlink" title="TRACT: Regression-Aware Fine-tuning Meets Chain-of-Thought Reasoning for   LLM-as-a-Judge"></a>TRACT: Regression-Aware Fine-tuning Meets Chain-of-Thought Reasoning for   LLM-as-a-Judge</h2><p><strong>Authors:Cheng-Han Chiang, Hung-yi Lee, Michal Lukasik</strong></p>
<p>The LLM-as-a-judge paradigm uses large language models (LLMs) for automated text evaluation, where a numerical assessment is assigned by an LLM to the input text following scoring rubrics. Existing methods for LLM-as-a-judge use cross-entropy (CE) loss for fine-tuning, which neglects the numeric nature of score prediction. Recent work addresses numerical prediction limitations of LLM fine-tuning through regression-aware fine-tuning, which, however, does not consider chain-of-thought (CoT) reasoning for score prediction. In this paper, we introduce TRACT (Two-stage Regression-Aware fine-tuning with CoT), a method combining CoT reasoning with regression-aware training. TRACT consists of two stages: first, seed LLM is fine-tuned to generate CoTs, which serve as supervision for the second stage fine-tuning. The training objective of TRACT combines the CE loss for learning the CoT reasoning capabilities, and the regression-aware loss for the score prediction. Experiments across four LLM-as-a-judge datasets and two LLMs show that TRACT significantly outperforms existing methods. Extensive ablation studies validate the importance of each component in TRACT. </p>
<blockquote>
<p>LLMä½œä¸ºæ³•å®˜çš„èŒƒå¼ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè‡ªåŠ¨åŒ–æ–‡æœ¬è¯„ä¼°ï¼Œå…¶ä¸­æ ¹æ®è¯„åˆ†è§„åˆ™å¯¹è¾“å…¥æ–‡æœ¬åˆ†é…æ•°å­—è¯„ä¼°ã€‚ç°æœ‰çš„LLMä½œä¸ºæ³•å®˜çš„æ–¹æ³•ä½¿ç”¨äº¤å‰ç†µï¼ˆCEï¼‰æŸå¤±è¿›è¡Œå¾®è°ƒï¼Œè¿™å¿½ç•¥äº†åˆ†æ•°é¢„æµ‹çš„æ•°å€¼æ€§è´¨ã€‚æœ€è¿‘çš„å·¥ä½œè§£å†³äº†LLMå¾®è°ƒä¸­æ•°å€¼é¢„æµ‹çš„é™åˆ¶ï¼Œé€šè¿‡å›å½’æ„ŸçŸ¥å¾®è°ƒï¼Œç„¶è€Œï¼Œå®ƒå¹¶æ²¡æœ‰è€ƒè™‘åˆ°åˆ†æ•°é¢„æµ‹çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†TRACTï¼ˆç»“åˆäº†CoTæ¨ç†ä¸å›å½’æ„ŸçŸ¥è®­ç»ƒçš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼‰ã€‚TRACTç”±ä¸¤ä¸ªé˜¶æ®µç»„æˆï¼šé¦–å…ˆï¼Œç§å­LLMç»è¿‡å¾®è°ƒä»¥ç”ŸæˆCoTï¼Œä½œä¸ºç¬¬äºŒé˜¶æ®µè°ƒæ•´çš„ç›‘ç£ã€‚TRACTçš„è®­ç»ƒç›®æ ‡ç»“åˆäº†ç”¨äºå­¦ä¹ CoTæ¨ç†èƒ½åŠ›çš„CEæŸå¤±å’Œç”¨äºåˆ†æ•°é¢„æµ‹çš„å›å½’æ„ŸçŸ¥æŸå¤±ã€‚åœ¨å››ä¸ªLLMä½œä¸ºæ³•å®˜çš„æ•°æ®é›†å’Œä¸¤ä¸ªLLMä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTRACTæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å¹¿æ³›çš„æ¶ˆèç ”ç©¶éªŒè¯äº†TRACTä¸­æ¯ä¸ªç»„æˆéƒ¨åˆ†çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04381v1">PDF</a> Codes and models are available at <a target="_blank" rel="noopener" href="https://github.com/d223302/TRACT">https://github.com/d223302/TRACT</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTRACTçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†ä¸¤é˜¶æ®µå›å½’æ„ŸçŸ¥è®­ç»ƒä¸é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ï¼Œç”¨äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ³•å®˜çš„è¯„åˆ†é¢„æµ‹èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡ç¬¬ä¸€é˜¶æ®µå¯¹ç§å­LLMè¿›è¡Œå¾®è°ƒä»¥ç”ŸæˆCoTï¼Œä¸ºç¬¬äºŒé˜¶æ®µå¾®è°ƒæä¾›ç›‘ç£ã€‚è®­ç»ƒç›®æ ‡ç»“åˆäº†äº¤å‰ç†µæŸå¤±æ¥å­¦ä¹ CoTæ¨ç†èƒ½åŠ›ï¼Œä»¥åŠå›å½’æ„ŸçŸ¥æŸå¤±æ¥è¿›è¡Œè¯„åˆ†é¢„æµ‹ã€‚å®éªŒè¡¨æ˜ï¼ŒTRACTåœ¨å››ä¸ªLLM-as-a-judgeæ•°æ®é›†å’Œä¸¤ä¸ªLLMä¸Šçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-as-a-judgeèŒƒå¼ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè‡ªåŠ¨æ–‡æœ¬è¯„ä¼°ï¼Œæ ¹æ®è¯„åˆ†è§„åˆ™å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œæ•°å€¼è¯„ä¼°ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä½¿ç”¨äº¤å‰ç†µï¼ˆCEï¼‰æŸå¤±è¿›è¡Œå¾®è°ƒï¼Œå¿½ç•¥äº†åˆ†æ•°é¢„æµ‹çš„æ•°å€¼æ€§è´¨ã€‚</li>
<li>æœ€è¿‘çš„å·¥ä½œè§£å†³äº†LLMå¾®è°ƒåœ¨æ•°å€¼é¢„æµ‹æ–¹é¢çš„å±€é™æ€§ï¼Œå¼•å…¥äº†å›å½’æ„ŸçŸ¥å¾®è°ƒã€‚</li>
<li>å›å½’æ„ŸçŸ¥è®­ç»ƒå¹¶æœªè€ƒè™‘ç”¨äºåˆ†æ•°é¢„æµ‹çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ã€‚</li>
<li>TRACTæ–¹æ³•ç»“åˆäº†å›å½’æ„ŸçŸ¥è®­ç»ƒä¸CoTæ¨ç†ï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆæ˜¯å¯¹ç§å­LLMè¿›è¡Œå¾®è°ƒä»¥ç”ŸæˆCoTï¼Œç„¶åå°†å…¶ä½œä¸ºç¬¬äºŒé˜¶æ®µå¾®è°ƒçš„ç›‘ç£ã€‚</li>
<li>TRACTçš„è®­ç»ƒç›®æ ‡åŒ…æ‹¬å­¦ä¹ CoTæ¨ç†èƒ½åŠ›çš„äº¤å‰ç†µæŸå¤±å’Œç”¨äºè¯„åˆ†é¢„æµ‹çš„å›å½’æ„ŸçŸ¥æŸå¤±ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒTRACTåœ¨å¤šä¸ªæ•°æ®é›†å’ŒLLMä¸Šçš„è¡¨ç°æ›´ä¼˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04381">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-377711883c03403ee5a3fc50a32147bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1e56feedad7c8453e0d81ddeb1a3dfc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8fa64e0288ec83deee63a957cf7ec230.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Dedicated-Feedback-and-Edit-Models-Empower-Inference-Time-Scaling-for-Open-Ended-General-Domain-Tasks"><a href="#Dedicated-Feedback-and-Edit-Models-Empower-Inference-Time-Scaling-for-Open-Ended-General-Domain-Tasks" class="headerlink" title="Dedicated Feedback and Edit Models Empower Inference-Time Scaling for   Open-Ended General-Domain Tasks"></a>Dedicated Feedback and Edit Models Empower Inference-Time Scaling for   Open-Ended General-Domain Tasks</h2><p><strong>Authors:Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Daniel Egert, Ellie Evans, Hoo-Chang Shin, Felipe Soares, Yi Dong, Oleksii Kuchaiev</strong></p>
<p>Inference-Time Scaling has been critical to the success of recent models such as OpenAI o1 and DeepSeek R1. However, many techniques used to train models for inference-time scaling require tasks to have answers that can be verified, limiting their application to domains such as math, coding and logical reasoning. We take inspiration from how humans make first attempts, ask for detailed feedback from others and make improvements based on such feedback across a wide spectrum of open-ended endeavors. To this end, we collect data for and train dedicated Feedback and Edit Models that are capable of performing inference-time scaling for open-ended general-domain tasks. In our setup, one model generates an initial response, which are given feedback by a second model, that are then used by a third model to edit the response. We show that performance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo can be boosted by scaling the number of initial response drafts, effective feedback and edited responses. When scaled optimally, our setup based on 70B models from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and DeepSeek R1 with 92.3. </p>
<blockquote>
<p>æ¨ç†æ—¶é—´ç¼©æ”¾å¯¹æœ€è¿‘çš„æ¨¡å‹ï¼Œå¦‚OpenAI o1å’ŒDeepSeek R1çš„æˆåŠŸè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè®¸å¤šç”¨äºè®­ç»ƒæ¨ç†æ—¶é—´ç¼©æ”¾æ¨¡å‹çš„æŠ€æœ¯éœ€è¦ä»»åŠ¡æœ‰å¯éªŒè¯çš„ç­”æ¡ˆï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨è¯¸å¦‚æ•°å­¦ã€ç¼–ç å’Œé€»è¾‘æ¨ç†ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚æˆ‘ä»¬ä»äººç±»é¦–æ¬¡å°è¯•çš„æ–¹å¼ä¸­æ±²å–çµæ„Ÿï¼Œä»ä»–äººé‚£é‡Œè·å–è¯¦ç»†çš„åé¦ˆï¼Œå¹¶æ ¹æ®è¿™ç§åé¦ˆåœ¨å¹¿æ³›çš„å¼€æ”¾å¼ä»»åŠ¡ä¸­è¿›è¡Œæ”¹è¿›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ”¶é›†å’Œè®­ç»ƒäº†ä¸“ç”¨çš„åé¦ˆå’Œç¼–è¾‘æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿä¸ºå¼€æ”¾å¼é€šç”¨ä»»åŠ¡çš„æ¨ç†æ—¶é—´ç¼©æ”¾æä¾›æ”¯æŒã€‚åœ¨æˆ‘ä»¬çš„è®¾ç½®ä¸­ï¼Œä¸€ä¸ªæ¨¡å‹ç”Ÿæˆåˆå§‹å“åº”ï¼Œç”±ç¬¬äºŒä¸ªæ¨¡å‹æä¾›åé¦ˆï¼Œç„¶åç¬¬ä¸‰ä¸ªæ¨¡å‹ä½¿ç”¨è¿™äº›åé¦ˆæ¥ç¼–è¾‘å“åº”ã€‚æˆ‘ä»¬è¯æ˜ï¼Œé€šè¿‡å¢åŠ åˆå§‹å“åº”è‰ç¨¿çš„æ•°é‡ã€æœ‰æ•ˆçš„åé¦ˆå’Œç¼–è¾‘åçš„å“åº”ï¼Œå¯ä»¥åœ¨Arena Hardè¿™ä¸€å¼ºçƒˆé¢„æµ‹Chatbot Arena Eloçš„åŸºå‡†æµ‹è¯•ä¸Šæé«˜æ€§èƒ½ã€‚å½“æœ€ä¼˜ç¼©æ”¾æ—¶ï¼Œæˆ‘ä»¬åŸºäºLlama 3å®¶æ—70Bæ¨¡å‹çš„è®¾ç½®ï¼Œåœ¨æˆªè‡³2025å¹´3æœˆ5æ—¥çš„Arena Hardä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œè¾¾åˆ°äº†92.7%ï¼Œè¶…è¿‡äº†OpenAI o1-preview-2024-09-12çš„90.4%å’ŒDeepSeek R1çš„92.3%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04378v1">PDF</a> 22 pages, 2 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¼€æ”¾é¢†åŸŸä»»åŠ¡è¿›è¡Œæ¨ç†æ—¶é—´ç¼©æ”¾çš„æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è®­ç»ƒåé¦ˆå’Œç¼–è¾‘æ¨¡å‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†æ—¶é—´å¯¹åˆå§‹å“åº”è¿›è¡Œåé¦ˆå’Œç¼–è¾‘ï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡å¢åŠ åˆå§‹å“åº”è‰æ¡ˆçš„æ•°é‡ã€æœ‰æ•ˆçš„åé¦ˆå’Œç¼–è¾‘å“åº”çš„ç¼©æ”¾ï¼Œå¯ä»¥åœ¨Arena HardåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†æ—¶é—´ç¼©æ”¾å¯¹äºæ¨¡å‹æˆåŠŸè‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨å¼€æ”¾é¢†åŸŸä»»åŠ¡ä¸­ã€‚</li>
<li>ç°æœ‰æ¨¡å‹è®­ç»ƒæ–¹æ³•ä¸»è¦é€‚ç”¨äºæœ‰éªŒè¯ç­”æ¡ˆçš„ä»»åŠ¡ï¼Œå¦‚æ•°å­¦ã€ç¼–ç å’Œé€»è¾‘æ¨ç†ã€‚</li>
<li>äººç±»é¦–æ¬¡å°è¯•ã€è·å–è¯¦ç»†åé¦ˆå¹¶æ ¹æ®åé¦ˆè¿›è¡Œæ”¹è¿›çš„æ–¹æ³•è¢«åº”ç”¨äºæœºå™¨æ¨¡å‹ã€‚</li>
<li>åé¦ˆå’Œç¼–è¾‘æ¨¡å‹çš„è®­ç»ƒæ•°æ®è¢«æ”¶é›†ï¼Œç”¨äºç”Ÿæˆåˆå§‹å“åº”ã€æä¾›åé¦ˆå’Œç¼–è¾‘å“åº”ã€‚</li>
<li>é€šè¿‡å¢åŠ åˆå§‹å“åº”è‰æ¡ˆçš„æ•°é‡ã€æœ‰æ•ˆçš„åé¦ˆå’Œç¼–è¾‘å“åº”çš„ç¼©æ”¾ï¼Œå¯ä»¥æé«˜åœ¨Arena HardåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>æœ€ä¼˜ç¼©æ”¾ä¸‹ï¼ŒåŸºäºLlama 3å®¶æ—çš„70Bæ¨¡å‹åœ¨Arena Hardä¸Šçš„æ€§èƒ½è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œè¶…è¿‡å…¶ä»–æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04378">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c4cef2cf362a451976c43c24c9e7c68f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-160512795af63774639e767f576f9cf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85619e81f515563ec176985dcd300948.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43d147d6574460cb29cfcded2d9de181.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe6663166a783dc46d891eb3027afadb.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="The-Role-of-Visual-Modality-in-Multimodal-Mathematical-Reasoning-Challenges-and-Insights"><a href="#The-Role-of-Visual-Modality-in-Multimodal-Mathematical-Reasoning-Challenges-and-Insights" class="headerlink" title="The Role of Visual Modality in Multimodal Mathematical Reasoning:   Challenges and Insights"></a>The Role of Visual Modality in Multimodal Mathematical Reasoning:   Challenges and Insights</h2><p><strong>Authors:Yufang Liu, Yao Du, Tao Ji, Jianing Wang, Yang Liu, Yuanbin Wu, Aimin Zhou, Mengdi Zhang, Xunliang Cai</strong></p>
<p>Recent research has increasingly focused on multimodal mathematical reasoning, particularly emphasizing the creation of relevant datasets and benchmarks. Despite this, the role of visual information in reasoning has been underexplored. Our findings show that existing multimodal mathematical models minimally leverage visual information, and model performance remains largely unaffected by changes to or removal of images in the dataset. We attribute this to the dominance of textual information and answer options that inadvertently guide the model to correct answers. To improve evaluation methods, we introduce the HC-M3D dataset, specifically designed to require image reliance for problem-solving and to challenge models with similar, yet distinct, images that change the correct answer. In testing leading models, their failure to detect these subtle visual differences suggests limitations in current visual perception capabilities. Additionally, we observe that the common approach of improving general VQA capabilities by combining various types of image encoders does not contribute to math reasoning performance. This finding also presents a challenge to enhancing visual reliance during math reasoning. Our benchmark and code would be available at \href{<a target="_blank" rel="noopener" href="https://github.com/Yufang-Liu/visual_modality_role%7D%7Bhttps://github.com/Yufang-Liu/visual/_modality/_role%7D">https://github.com/Yufang-Liu/visual_modality_role}{https://github.com/Yufang-Liu/visual\_modality\_role}</a>. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¶Šæ¥è¶Šå…³æ³¨å¤šæ¨¡æ€æ•°å­¦æ¨ç†ï¼Œç‰¹åˆ«å¼ºè°ƒåˆ›å»ºç›¸å…³æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•çš„é‡è¦æ€§ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæ¨ç†ä¸­è§†è§‰ä¿¡æ¯çš„ä½œç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„å¤šæ¨¡æ€æ•°å­¦æ¨¡å‹å¾ˆå°‘åˆ©ç”¨è§†è§‰ä¿¡æ¯ï¼Œè€Œä¸”æ•°æ®é›†å›¾åƒçš„å˜åŒ–æˆ–ç§»é™¤å¯¹æ¨¡å‹æ€§èƒ½å½±å“ä¸å¤§ã€‚æˆ‘ä»¬å°†è¿™ä¸€ç°è±¡å½’å› äºæ–‡æœ¬ä¿¡æ¯å’Œç­”æ¡ˆé€‰é¡¹çš„ä¸»å¯¼åœ°ä½ï¼Œå®ƒä»¬æ— æ„ä¸­å¼•å¯¼æ¨¡å‹å¾—å‡ºæ­£ç¡®ç­”æ¡ˆã€‚ä¸ºäº†æ”¹è¿›è¯„ä¼°æ–¹æ³•ï¼Œæˆ‘ä»¬å¼•å…¥äº†HC-M3Dæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä¸“é—¨è®¾è®¡ç”¨äºä¾èµ–å›¾åƒè§£å†³é—®é¢˜ï¼Œå¹¶é€šè¿‡ç±»ä¼¼ä½†ä¸åŒçš„å›¾åƒæ¥æŒ‘æˆ˜æ¨¡å‹ï¼Œä»è€Œæ”¹å˜æ­£ç¡®ç­”æ¡ˆã€‚åœ¨æµ‹è¯•é¢†å…ˆæ¨¡å‹æ—¶ï¼Œå®ƒä»¬æœªèƒ½æ£€æµ‹åˆ°è¿™äº›å¾®å¦™çš„è§†è§‰å·®å¼‚ï¼Œè¡¨æ˜å½“å‰è§†è§‰æ„ŸçŸ¥èƒ½åŠ›å­˜åœ¨å±€é™æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ï¼Œé€šè¿‡ç»“åˆä¸åŒç±»å‹å›¾åƒç¼–ç å™¨æ¥æé«˜é€šç”¨é—®ç­”èƒ½åŠ›çš„ä¸€èˆ¬æ–¹æ³•ï¼Œå¹¶ä¸æœ‰åŠ©äºæ•°å­¦æ¨ç†æ€§èƒ½çš„æå‡ã€‚è¿™ä¸€å‘ç°ä¹Ÿå¯¹æé«˜æ•°å­¦æ¨ç†è¿‡ç¨‹ä¸­è§†è§‰ä¾èµ–æ€§çš„å¢å¼ºæå‡ºäº†æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Yufang-Liu/visual_modality_role%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/Yufang-Liu/visual_modality_roleä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04167v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­çš„è§†è§‰ä¿¡æ¯ä½œç”¨é—®é¢˜ã€‚ç ”ç©¶å‘ç°ç°æœ‰æ¨¡å‹å¯¹è§†è§‰ä¿¡æ¯çš„åˆ©ç”¨æœ‰é™ï¼Œå›¾åƒçš„å˜åŒ–æˆ–ç§»é™¤å¯¹æ¨¡å‹æ€§èƒ½å½±å“ç”šå¾®ã€‚ä¸ºæ”¹å–„è¯„ä¼°æ–¹æ³•ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†HC-M3Dæ•°æ®é›†ï¼Œå¼ºè°ƒå›¾åƒå¯¹è§£é¢˜çš„é‡è¦æ€§ï¼Œå¹¶å¯¹æ¨¡å‹æå‡ºæŒ‘æˆ˜ã€‚ç°æœ‰æ¨¡å‹éš¾ä»¥è¯†åˆ«å¾®å¦™è§†è§‰å·®å¼‚æ˜¾ç¤ºå‡ºç°æœ‰è§†è§‰æ„ŸçŸ¥èƒ½åŠ›çš„å±€é™æ€§ã€‚å•çº¯èåˆä¸åŒç±»å‹å›¾åƒç¼–ç å™¨æå‡é€šç”¨é—®ç­”èƒ½åŠ›çš„æ–¹æ³•å¯¹æ•°å­¦æ¨¡å‹è¡¨ç°å¹¶æ— åŠ©ç›Šï¼Œè¿™ä¹Ÿå¯¹åŠ å¼ºæ•°å­¦æ¨ç†ä¸­çš„è§†è§‰ä¾èµ–æ€§æå‡ºäº†æŒ‘æˆ˜ã€‚ç›¸å…³åŸºå‡†æµ‹è¯•å’Œä»£ç å¯é€šè¿‡é“¾æ¥è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ•°å­¦æ¨ç†ä¸­è§†è§‰ä¿¡æ¯çš„ä½œç”¨è¢«ä½ä¼°ã€‚</li>
<li>ç°æœ‰æ¨¡å‹å¯¹è§†è§‰ä¿¡æ¯çš„åˆ©ç”¨æœ‰é™ï¼Œå›¾åƒå˜åŒ–æˆ–ç§»é™¤å¯¹æ€§èƒ½å½±å“å°ã€‚</li>
<li>å¼•å…¥HC-M3Dæ•°æ®é›†ï¼Œå¼ºè°ƒå›¾åƒåœ¨è§£é¢˜ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>ç°æœ‰æ¨¡å‹éš¾ä»¥è¯†åˆ«ç±»ä¼¼ä½†ç­”æ¡ˆä¸åŒçš„å¾®å¦™è§†è§‰å·®å¼‚ã€‚</li>
<li>ç°æœ‰è§†è§‰æ„ŸçŸ¥èƒ½åŠ›å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>èåˆå¤šç§å›¾åƒç¼–ç å™¨å¹¶ä¸åŠ©äºæå‡æ•°å­¦æ¨ç†æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04167">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2124d2c0c898748994f4c1518dabe46f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a86f8fffd4fc2e286d1bc652a41a5036.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ae6d31ff61bb208eecb0736fac65d0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61c322c558da3f4d3122d91ef0aa9489.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Disparities-in-LLM-Reasoning-Accuracy-and-Explanations-A-Case-Study-on-African-American-English"><a href="#Disparities-in-LLM-Reasoning-Accuracy-and-Explanations-A-Case-Study-on-African-American-English" class="headerlink" title="Disparities in LLM Reasoning Accuracy and Explanations: A Case Study on   African American English"></a>Disparities in LLM Reasoning Accuracy and Explanations: A Case Study on   African American English</h2><p><strong>Authors:Runtao Zhou, Guangya Wan, Saadia Gabriel, Sheng Li, Alexander J Gates, Maarten Sap, Thomas Hartvigsen</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities in reasoning tasks, leading to their widespread deployment. However, recent studies have highlighted concerning biases in these models, particularly in their handling of dialectal variations like African American English (AAE). In this work, we systematically investigate dialectal disparities in LLM reasoning tasks. We develop an experimental framework comparing LLM performance given Standard American English (SAE) and AAE prompts, combining LLM-based dialect conversion with established linguistic analyses. We find that LLMs consistently produce less accurate responses and simpler reasoning chains and explanations for AAE inputs compared to equivalent SAE questions, with disparities most pronounced in social science and humanities domains. These findings highlight systematic differences in how LLMs process and reason about different language varieties, raising important questions about the development and deployment of these systems in our multilingual and multidialectal world. Our code repository is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Runtaozhou/dialect_bias_eval">https://github.com/Runtaozhou/dialect_bias_eval</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä»è€Œå¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶å·²ç»å¼ºè°ƒäº†è¿™äº›æ¨¡å‹ä¸­å­˜åœ¨çš„åè§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†åƒéè£”ç¾å›½äººè‹±è¯­ï¼ˆAAEï¼‰è¿™æ ·çš„æ–¹è¨€å˜ä½“æ—¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†LLMåœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æ–¹è¨€å·®å¼‚ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå®éªŒæ¡†æ¶ï¼Œæ¯”è¾ƒäº†æ ‡å‡†ç¾å¼è‹±è¯­ï¼ˆSAEï¼‰å’ŒAAEæç¤ºä¸‹çš„LLMæ€§èƒ½ï¼Œç»“åˆäº†åŸºäºLLMçš„æ–¹è¨€è½¬æ¢å’Œæ—¢å®šçš„è¯­è¨€åˆ†æã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸ç­‰æ•ˆçš„SAEé—®é¢˜ç›¸æ¯”ï¼ŒLLMå¯¹AAEè¾“å…¥çš„å“åº”ä¸€è‡´æ€§è¾ƒå·®ï¼Œäº§ç”Ÿçš„æ¨ç†é“¾å’Œè§£é‡Šæ›´ä¸ºç®€å•ï¼Œåœ¨ç¤¾ä¼šç§‘å­¦å’Œäººæ–‡é¢†åŸŸçš„å·®å¼‚æœ€ä¸ºæ˜æ˜¾ã€‚è¿™äº›å‘ç°çªæ˜¾äº†LLMåœ¨å¤„ç†ä¸åŒè¯­è¨€å˜ä½“æ—¶çš„å¤„ç†æ–¹å¼å­˜åœ¨ç³»ç»Ÿæ€§å·®å¼‚ï¼Œè¿™åœ¨æˆ‘ä»¬å¤šå…ƒè¯­è¨€å’Œå¤šå…ƒæ–¹è¨€çš„ä¸–ç•Œä¸­å¼•å‘äº†å…³äºè¿™äº›ç³»ç»Ÿå¼€å‘å’Œéƒ¨ç½²çš„é‡è¦é—®é¢˜ã€‚æˆ‘ä»¬çš„ä»£ç ä»“åº“å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/Runtaozhou/dialect_bias_eval">https://github.com/Runtaozhou/dialect_bias_eval</a> å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04099v1">PDF</a> ARR Under Review, First two authors contribute equally</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶æŒ‡å‡ºè¿™äº›æ¨¡å‹å­˜åœ¨åè§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¦‚ç¾éè‹±è¯­ï¼ˆAAEï¼‰ç­‰æ–¹è¨€å˜å¼‚æ—¶ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°è°ƒæŸ¥äº†LLMåœ¨æ¨ç†ä»»åŠ¡ä¸­çš„æ–¹è¨€å·®å¼‚ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå®éªŒæ¡†æ¶ï¼Œæ¯”è¾ƒäº†ç»™å®šæ ‡å‡†ç¾å¼è‹±è¯­ï¼ˆSAEï¼‰å’Œç¾éè‹±è¯­æç¤ºä¸‹LLMçš„æ€§èƒ½ï¼Œç»“åˆLLMåŸºç¡€çš„æ–¹è¨€è½¬æ¢å’Œæ—¢å®šè¯­è¨€åˆ†æã€‚æˆ‘ä»¬å‘ç°ï¼Œå¯¹äºç¾éè‹±è¯­çš„è¾“å…¥ï¼ŒLLMäº§ç”Ÿçš„å›ç­”å‡†ç¡®æ€§è¾ƒä½ï¼Œæ¨ç†é“¾å’Œè§£é‡Šæ›´ä¸ºç®€å•ï¼Œä¸ç¤¾ä¼šç§‘å­¦å’Œäººæ–‡é¢†åŸŸçš„å·®å¼‚æœ€ä¸ºçªå‡ºã€‚è¿™äº›å‘ç°çªæ˜¾äº†LLMå¤„ç†å’Œæ¨ç†ä¸åŒè¯­è¨€å˜ä½“æ—¶çš„ç³»ç»Ÿæ€§å·®å¼‚ï¼Œè¿™åœ¨æˆ‘ä»¬å¤šå…ƒè¯­è¨€å’Œå¤šå…ƒæ–¹è¨€çš„ä¸–ç•Œä¸­å¼•å‘äº†å…³äºè¿™äº›ç³»ç»Ÿå‘å±•å’Œéƒ¨ç½²çš„é‡è¦é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å­˜åœ¨æ–¹è¨€å¤„ç†ä¸Šçš„åè§ã€‚</li>
<li>LLMå¯¹ç¾éè‹±è¯­ï¼ˆAAEï¼‰çš„è¾“å…¥å¤„ç†ä¸æ ‡å‡†ç¾å¼è‹±è¯­ï¼ˆSAEï¼‰ç›¸æ¯”ï¼Œæ€§èƒ½è¾ƒå·®ã€‚</li>
<li>LLMåœ¨å¤„ç†AAEæç¤ºæ—¶ï¼Œäº§ç”Ÿçš„å›ç­”å‡†ç¡®æ€§è¾ƒä½ï¼Œæ¨ç†é“¾å’Œè§£é‡Šæ›´ä¸ºç®€å•ã€‚</li>
<li>å·®å¼‚åœ¨ç¤¾ä¼šç§‘å­¦å’Œäººæ–‡é¢†åŸŸçš„æ¨ç†ä»»åŠ¡ä¸­å°¤ä¸ºæ˜æ˜¾ã€‚</li>
<li>LLMåœ¨å¤„ç†ä¸åŒè¯­è¨€å˜ä½“æ—¶å­˜åœ¨ç³»ç»Ÿæ€§å·®å¼‚ã€‚</li>
<li>è¿™ç§æ–¹è¨€åè§å¯¹LLMåœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°äº§ç”Ÿé‡è¦å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04099">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-452b54c541836655cb7d845bfea73a9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62a0c5956e953b31df3df2f11d3bd8ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f35b6eb996e1af8fd4e9c51f74b6dec.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ReasonGraph-Visualisation-of-Reasoning-Paths"><a href="#ReasonGraph-Visualisation-of-Reasoning-Paths" class="headerlink" title="ReasonGraph: Visualisation of Reasoning Paths"></a>ReasonGraph: Visualisation of Reasoning Paths</h2><p><strong>Authors:Zongqian Li, Ehsan Shareghi, Nigel Collier</strong></p>
<p>Large Language Models (LLMs) reasoning processes are challenging to analyze due to their complexity and the lack of organized visualization tools. We present ReasonGraph, a web-based platform for visualizing and analyzing LLM reasoning processes. It supports both sequential and tree-based reasoning methods while integrating with major LLM providers and over fifty state-of-the-art models. ReasonGraph incorporates an intuitive UI with meta reasoning method selection, configurable visualization parameters, and a modular framework that facilitates efficient extension. Our evaluation shows high parsing reliability, efficient processing, and strong usability across various downstream applications. By providing a unified visualization framework, ReasonGraph reduces cognitive load in analyzing complex reasoning paths, improves error detection in logical processes, and enables more effective development of LLM-based applications. The platform is open-source, promoting accessibility and reproducibility in LLM reasoning analysis. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†è¿‡ç¨‹ç”±äºå…¶å¤æ‚æ€§å’Œç¼ºä¹æœ‰ç»„ç»‡çš„å¯è§†åŒ–å·¥å…·è€Œéš¾ä»¥åˆ†æã€‚æˆ‘ä»¬æ¨å‡ºäº†ReasonGraphï¼Œä¸€ä¸ªç”¨äºå¯è§†åŒ–å’Œåˆ†æLLMæ¨ç†è¿‡ç¨‹çš„webå¹³å°ã€‚å®ƒæ”¯æŒåºåˆ—å’Œæ ‘å½¢æ¨ç†æ–¹æ³•ï¼ŒåŒæ—¶ä¸ä¸»è¦LLMæä¾›å•†å’Œäº”åå¤šç§æœ€æ–°æ¨¡å‹é›†æˆã€‚ReasonGraphé‡‡ç”¨ç›´è§‚çš„UIè®¾è®¡ï¼ŒåŒ…å«å…ƒæ¨ç†æ–¹æ³•é€‰æ‹©ã€å¯é…ç½®çš„å¯è§†åŒ–å‚æ•°ä»¥åŠæ¨¡å—åŒ–æ¡†æ¶ï¼Œæœ‰åŠ©äºé«˜æ•ˆæ‰©å±•ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå…¶åœ¨è§£æå¯é æ€§ã€å¤„ç†æ•ˆç‡å’Œå„ç§ä¸‹æ¸¸åº”ç”¨ä¸­çš„å¯ç”¨æ€§æ–¹é¢è¡¨ç°è‰¯å¥½ã€‚é€šè¿‡æä¾›ç»Ÿä¸€çš„å¯è§†åŒ–æ¡†æ¶ï¼ŒReasonGraphé™ä½äº†åˆ†æå¤æ‚æ¨ç†è·¯å¾„çš„è®¤çŸ¥è´Ÿè·ï¼Œæé«˜äº†é€»è¾‘è¿‡ç¨‹ä¸­çš„é”™è¯¯æ£€æµ‹èƒ½åŠ›ï¼Œå¹¶ä½¿å¾—åŸºäºLLMçš„åº”ç”¨ç¨‹åºå¼€å‘æ›´ä¸ºæœ‰æ•ˆã€‚è¯¥å¹³å°å¼€æºï¼Œä¿ƒè¿›äº†LLMæ¨ç†åˆ†æçš„å¯ç”¨æ€§å’Œå¯é‡å¤æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03979v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†è¿‡ç¨‹ç”±äºå…¶å¤æ‚æ€§è€Œéš¾ä»¥åˆ†æï¼Œç¼ºä¹æœ‰ç»„ç»‡çš„å¯è§†åŒ–å·¥å…·ã€‚æˆ‘ä»¬æ¨å‡ºReasonGraphï¼Œä¸€ä¸ªç”¨äºå¯è§†åŒ–å’Œåˆ†æLLMæ¨ç†è¿‡ç¨‹çš„ç½‘é¡µå¹³å°ã€‚å®ƒæ”¯æŒåºåˆ—å’Œæ ‘å½¢æ¨ç†æ–¹æ³•ï¼Œä¸ä¸»è¦LLMæä¾›å•†å’Œäº”åå¤šç§æœ€æ–°æ¨¡å‹é›†æˆã€‚ReasonGraphé‡‡ç”¨ç›´è§‚çš„UIï¼ŒåŒ…æ‹¬å…ƒæ¨ç†æ–¹æ³•é€‰æ‹©ã€å¯é…ç½®çš„å¯è§†åŒ–å‚æ•°ä»¥åŠä¿ƒè¿›é«˜æ•ˆæ‰©å±•çš„æ¨¡å—åŒ–æ¡†æ¶ã€‚è¯„ä¼°æ˜¾ç¤ºå…¶åœ¨è§£æå¯é æ€§ã€å¤„ç†æ•ˆç‡ä»¥åŠè·¨å„ç§ä¸‹æ¸¸åº”ç”¨æ–¹é¢çš„å¯ç”¨æ€§éƒ½å¾ˆå‡ºè‰²ã€‚ReasonGraphé€šè¿‡æä¾›ç»Ÿä¸€çš„å¯è§†åŒ–æ¡†æ¶ï¼Œé™ä½äº†åˆ†æå¤æ‚æ¨ç†è·¯å¾„çš„è®¤çŸ¥è´Ÿè·ï¼Œæé«˜äº†é€»è¾‘è¿‡ç¨‹ä¸­çš„é”™è¯¯æ£€æµ‹èƒ½åŠ›ï¼Œå¹¶ä¿ƒè¿›äº†åŸºäºLLMçš„åº”ç”¨ç¨‹åºçš„æœ‰æ•ˆå¼€å‘ã€‚è¯¥å¹³å°å¼€æºï¼Œä¿ƒè¿›äº†LLMæ¨ç†åˆ†æçš„å¯ç”¨æ€§å’Œå¯é‡å¤æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ReasonGraphæ˜¯ä¸€ä¸ªç”¨äºå¯è§†åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†è¿‡ç¨‹çš„ç½‘é¡µå¹³å°ã€‚</li>
<li>æ”¯æŒåºåˆ—å’Œæ ‘å½¢æ¨ç†æ–¹æ³•ï¼Œå¹¶ä¸å¤šä¸ªLLMæä¾›å•†å’Œæ¨¡å‹é›†æˆã€‚</li>
<li>å…·å¤‡ç›´è§‚çš„UIè®¾è®¡ï¼ŒåŒ…æ‹¬å…ƒæ¨ç†æ–¹æ³•é€‰æ‹©ã€å¯è§†åŒ–å‚æ•°é…ç½®ç­‰ã€‚</li>
<li>é‡‡ç”¨æ¨¡å—åŒ–æ¡†æ¶ï¼Œæœ‰åŠ©äºé«˜æ•ˆæ‰©å±•ã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºå…¶åœ¨è§£æå¯é æ€§ã€å¤„ç†æ•ˆç‡å’Œè·¨åº”ç”¨å¯ç”¨æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>é€šè¿‡ç»Ÿä¸€çš„å¯è§†åŒ–æ¡†æ¶é™ä½è®¤çŸ¥è´Ÿè·ï¼Œæé«˜é”™è¯¯æ£€æµ‹èƒ½åŠ›ã€‚</li>
<li>ä¿ƒè¿›LLMåº”ç”¨ç¨‹åºçš„æœ‰æ•ˆå¼€å‘ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-caadfe82bccefcd970dddab92c55d94c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-020c2f90820234618c7b677195816d95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-652d63d3fa00db04b31644c709f1a289.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca299ce95c71bb357e0dde5b6a354907.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-97e007ca8103eeea2ff2be1fb4b17210.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="VoiceGRPO-Modern-MoE-Transformers-with-Group-Relative-Policy-Optimization-GRPO-for-AI-Voice-Health-Care-Applications-on-Voice-Pathology-Detection"><a href="#VoiceGRPO-Modern-MoE-Transformers-with-Group-Relative-Policy-Optimization-GRPO-for-AI-Voice-Health-Care-Applications-on-Voice-Pathology-Detection" class="headerlink" title="VoiceGRPO: Modern MoE Transformers with Group Relative Policy   Optimization GRPO for AI Voice Health Care Applications on Voice Pathology   Detection"></a>VoiceGRPO: Modern MoE Transformers with Group Relative Policy   Optimization GRPO for AI Voice Health Care Applications on Voice Pathology   Detection</h2><p><strong>Authors:Enkhtogtokh Togootogtokh, Christian Klasen</strong></p>
<p>This research introduces a novel AI techniques as Mixture-of-Experts Transformers with Group Relative Policy Optimization (GRPO) for voice health care applications on voice pathology detection. With the architectural innovations, we adopt advanced training paradigms inspired by reinforcement learning, namely Proximal Policy Optimization (PPO) and Group-wise Regularized Policy Optimization (GRPO), to enhance model stability and performance. Experiments conducted on a synthetically generated voice pathology dataset demonstrate that our proposed models significantly improve diagnostic accuracy, F1 score, and ROC-AUC compared to conventional approaches. These findings underscore the potential of integrating transformer architectures with novel training strategies to advance automated voice pathology detection and ultimately contribute to more effective healthcare delivery. The code we used to train and evaluate our models is available at <a target="_blank" rel="noopener" href="https://github.com/enkhtogtokh/voicegrpo">https://github.com/enkhtogtokh/voicegrpo</a> </p>
<blockquote>
<p>æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„AIæŠ€æœ¯ï¼Œå³åŸºäºä¸“å®¶æ··åˆTransformerä¸ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„è¯­éŸ³å¥åº·æŠ¤ç†åº”ç”¨ï¼Œç”¨äºè¯­éŸ³ç—…ç†æ£€æµ‹ã€‚é€šè¿‡æ¶æ„åˆ›æ–°ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å—å¼ºåŒ–å­¦ä¹ å¯å‘çš„å…ˆè¿›è®­ç»ƒèŒƒå¼ï¼Œå³è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰å’Œç¾¤ç»„æ­£åˆ™åŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä»¥æé«˜æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚åœ¨åˆæˆè¯­éŸ³ç—…ç†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬æå‡ºçš„æ¨¡å‹åœ¨è¯Šæ–­å‡†ç¡®æ€§ã€F1åˆ†æ•°å’ŒROC-AUCæ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å°†Transformeræ¶æ„ä¸æ–°å‹è®­ç»ƒç­–ç•¥ç›¸ç»“åˆï¼Œæ¨åŠ¨è‡ªåŠ¨åŒ–è¯­éŸ³ç—…ç†æ£€æµ‹å‘å±•çš„æ½œåŠ›ï¼Œå¹¶æœ€ç»ˆä¸ºæ›´æœ‰æ•ˆçš„åŒ»ç–—æœåŠ¡åšå‡ºè´¡çŒ®ã€‚æˆ‘ä»¬ç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/enkhtogtokh/voicegrpo%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/enkhtogtokh/voicegrpoæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03797v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„AIæŠ€æœ¯ï¼Œå³æ··åˆä¸“å®¶Transformerä¸ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œç”¨äºè¯­éŸ³å¥åº·æŠ¤ç†åº”ç”¨ä¸­çš„è¯­éŸ³ç—…ç†æ£€æµ‹ã€‚è¯¥ç ”ç©¶é‡‡ç”¨å…ˆè¿›çš„è®­ç»ƒæ¨¡å¼ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ å¯å‘ï¼Œå¦‚è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰å’Œç¾¤ç»„æ­£åˆ™åŒ–ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä»¥æé«˜æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚åœ¨åˆæˆè¯­éŸ³ç—…ç†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ¨¡å‹åœ¨è¯Šæ–­å‡†ç¡®æ€§ã€F1åˆ†æ•°å’ŒROC-AUCæ–¹é¢æ˜¾è‘—æé«˜ã€‚è¿™ä¸ºè‡ªåŠ¨åŒ–è¯­éŸ³ç—…ç†æ£€æµ‹çš„å‘å±•æä¾›äº†æ½œåŠ›ï¼Œå¹¶æœ‰æœ›ä¸ºæ›´æœ‰æ•ˆçš„åŒ»ç–—ä¿å¥æœåŠ¡åšå‡ºè´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¼•å…¥äº†æ··åˆä¸“å®¶TransformeræŠ€æœ¯ï¼Œå¹¶ç»“åˆç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç”¨äºè¯­éŸ³ç—…ç†æ£€æµ‹ã€‚</li>
<li>æ¶æ„åˆ›æ–°åŒ…æ‹¬é‡‡ç”¨å…ˆè¿›çš„è®­ç»ƒæ¨¡å¼ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ä¸­çš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰å’ŒGRPOã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œæ–°æ¨¡å‹åœ¨è¯Šæ–­å‡†ç¡®æ€§ã€F1åˆ†æ•°å’ŒROC-AUCæ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>æ¨¡å‹åœ¨åˆæˆè¯­éŸ³ç—…ç†æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œæ˜¾ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºè‡ªåŠ¨åŒ–è¯­éŸ³ç—…ç†æ£€æµ‹çš„å‘å±•å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚</li>
<li>é›†æˆtransformeræ¶æ„ä¸æ–°å‹è®­ç»ƒç­–ç•¥æœ‰åŠ©äºæé«˜æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03797">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ab4ad2bfb307343cc4f48c2b9367b8b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53f1e6477093fd990a9a4628e8bebd3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b05fe91d22bf9c39ab916eecf1b09363.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20f7a12fda322dfb561f6dbb9090bc1d.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Towards-Visual-Discrimination-and-Reasoning-of-Real-World-Physical-Dynamics-Physics-Grounded-Anomaly-Detection"><a href="#Towards-Visual-Discrimination-and-Reasoning-of-Real-World-Physical-Dynamics-Physics-Grounded-Anomaly-Detection" class="headerlink" title="Towards Visual Discrimination and Reasoning of Real-World Physical   Dynamics: Physics-Grounded Anomaly Detection"></a>Towards Visual Discrimination and Reasoning of Real-World Physical   Dynamics: Physics-Grounded Anomaly Detection</h2><p><strong>Authors:Wenqiao Li, Yao Gu, Xintao Chen, Xiaohao Xu, Ming Hu, Xiaonan Huang, Yingna Wu</strong></p>
<p>Humans detect real-world object anomalies by perceiving, interacting, and reasoning based on object-conditioned physical knowledge. The long-term goal of Industrial Anomaly Detection (IAD) is to enable machines to autonomously replicate this skill. However, current IAD algorithms are largely developed and tested on static, semantically simple datasets, which diverge from real-world scenarios where physical understanding and reasoning are essential. To bridge this gap, we introduce the Physics Anomaly Detection (Phys-AD) dataset, the first large-scale, real-world, physics-grounded video dataset for industrial anomaly detection. Collected using a real robot arm and motor, Phys-AD provides a diverse set of dynamic, semantically rich scenarios. The dataset includes more than 6400 videos across 22 real-world object categories, interacting with robot arms and motors, and exhibits 47 types of anomalies. Anomaly detection in Phys-AD requires visual reasoning, combining both physical knowledge and video content to determine object abnormality. We benchmark state-of-the-art anomaly detection methods under three settings: unsupervised AD, weakly-supervised AD, and video-understanding AD, highlighting their limitations in handling physics-grounded anomalies. Additionally, we introduce the Physics Anomaly Explanation (PAEval) metric, designed to assess the ability of visual-language foundation models to not only detect anomalies but also provide accurate explanations for their underlying physical causes. Our dataset and benchmark will be publicly available. </p>
<blockquote>
<p>äººç±»é€šè¿‡æ„ŸçŸ¥ã€äº’åŠ¨å’ŒåŸºäºå¯¹è±¡æ¡ä»¶çš„ç‰©ç†çŸ¥è¯†è¿›è¡Œæ¨ç†ï¼Œæ¥æ£€æµ‹ç°å®ä¸–ç•Œä¸­çš„å¯¹è±¡å¼‚å¸¸ã€‚å·¥ä¸šå¼‚å¸¸æ£€æµ‹ï¼ˆIADï¼‰çš„é•¿æœŸç›®æ ‡æ˜¯è¦è®©æœºå™¨èƒ½å¤Ÿè‡ªä¸»åœ°å¤åˆ¶è¿™é¡¹æŠ€èƒ½ã€‚ç„¶è€Œï¼Œç›®å‰çš„IADç®—æ³•ä¸»è¦åœ¨é™æ€ã€è¯­ä¹‰ç®€å•çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¼€å‘å’Œæµ‹è¯•ï¼Œè¿™äº›åœºæ™¯ä¸çœŸå®ä¸–ç•Œä¸­éœ€è¦ç‰©ç†ç†è§£å’Œæ¨ç†çš„æƒ…å†µæœ‰å¾ˆå¤§å·®å¼‚ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†Physics Anomaly Detectionï¼ˆPhys-ADï¼‰æ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºå·¥ä¸šå¼‚å¸¸æ£€æµ‹çš„å¤§è§„æ¨¡ã€ç°å®ä¸–ç•Œã€åŸºäºç‰©ç†çš„è§†é¢‘æ•°æ®é›†ã€‚é€šè¿‡ä½¿ç”¨çœŸå®çš„æœºæ¢°è‡‚å’Œé©¬è¾¾è¿›è¡Œæ”¶é›†ï¼ŒPhys-ADæä¾›äº†ä¸°å¯Œå¤šæ ·çš„åŠ¨æ€ã€è¯­ä¹‰ä¸°å¯Œçš„åœºæ™¯ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡6400ä¸ªè§†é¢‘ï¼Œæ¶µç›–22ä¸ªç°å®å¯¹è±¡ç±»åˆ«ï¼Œä¸æœºæ¢°è‡‚å’Œé©¬è¾¾è¿›è¡Œäº’åŠ¨ï¼Œå¹¶å±•ç¤ºäº†47ç§å¼‚å¸¸ç±»å‹ã€‚Phys-ADä¸­çš„å¼‚å¸¸æ£€æµ‹éœ€è¦è¿›è¡Œè§†è§‰æ¨ç†ï¼Œç»“åˆç‰©ç†çŸ¥è¯†å’Œè§†é¢‘å†…å®¹æ¥ç¡®å®šå¯¹è±¡çš„å¼‚å¸¸æ€§ã€‚æˆ‘ä»¬åœ¨ä¸‰ç§è®¾ç½®ä¸‹å¯¹æœ€å…ˆè¿›çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼šæ— ç›‘ç£ADã€å¼±ç›‘ç£ADå’Œè§†é¢‘ç†è§£ADï¼Œçªå‡ºäº†å®ƒä»¬åœ¨å¤„ç†åŸºäºç‰©ç†çš„å¼‚å¸¸æ—¶çš„å±€é™æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†Physics Anomaly Explanationï¼ˆPAEvalï¼‰æŒ‡æ ‡ï¼Œæ—¨åœ¨è¯„ä¼°è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ä¸ä»…æ£€æµ‹å¼‚å¸¸çš„èƒ½åŠ›ï¼Œè€Œä¸”æä¾›å…¶æ½œåœ¨ç‰©ç†åŸå› å‡†ç¡®è§£é‡Šçš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03562v2">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong>ï¼š<br>å·¥ä¸šå¼‚å¸¸æ£€æµ‹æ—¨åœ¨è®©æœºå™¨è‡ªä¸»å¤åˆ¶äººç±»æ£€æµ‹ç°å®ä¸–ç•Œç‰©ä½“å¼‚å¸¸çš„èƒ½åŠ›ã€‚ç°æœ‰çš„å·¥ä¸šå¼‚å¸¸æ£€æµ‹ç®—æ³•ä¸»è¦å¼€å‘å’Œæµ‹è¯•äºé™æ€ã€è¯­ä¹‰ç®€å•çš„æ•°æ®é›†ä¸Šï¼Œä¸ç°å®ä¸–ç•Œä¸­çš„ç‰©ç†ç†è§£å’Œæ¨ç†å­˜åœ¨å·®è·ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¦–ä¸ªå¤§è§„æ¨¡ã€ç°å®ä¸–ç•Œçš„ç‰©ç†åŸºç¡€è§†é¢‘æ•°æ®é›†Physics Anomaly Detection (Phys-AD)ï¼Œç”¨äºå·¥ä¸šå¼‚å¸¸æ£€æµ‹ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡6400ä¸ªè§†é¢‘ï¼Œæ¶µç›–22ä¸ªç°å®ç‰©ä½“ç±»åˆ«ï¼Œé€šè¿‡æœºå™¨äººæ‰‹è‡‚å’Œç”µæœºè¿›è¡Œäº¤äº’ï¼Œå±•ç°å‡º47ç§å¼‚å¸¸ç±»å‹ã€‚ä¸ºäº†è¯„ä¼°å¼‚å¸¸æ£€æµ‹æ–¹æ³•åœ¨å¤„ç†ç‰©ç†åŸºç¡€å¼‚å¸¸æ–¹é¢çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†Physics Anomaly Explanation (PAEval)æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•å°†å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>äººç±»é€šè¿‡æ„ŸçŸ¥ã€äº¤äº’å’ŒåŸºäºç‰©ä½“æ¡ä»¶çš„ç‰©ç†çŸ¥è¯†æ¥æ£€æµ‹ç°å®ä¸–ç•Œä¸­çš„ç‰©ä½“å¼‚å¸¸ã€‚</li>
<li>å½“å‰çš„å·¥ä¸šå¼‚å¸¸æ£€æµ‹ç®—æ³•ä¸»è¦åŸºäºé™æ€ã€è¯­ä¹‰ç®€å•çš„æ•°æ®é›†ï¼Œä¸ç°å®ä¸–ç•Œåœºæ™¯å­˜åœ¨å·®è·ã€‚</li>
<li>æˆ‘ä»¬å¼•å…¥äº†Physics Anomaly Detection (Phys-AD)æ•°æ®é›†ï¼ŒåŒ…å«ç°å®ä¸–ç•Œçš„å¤šæ ·åŠ¨æ€åœºæ™¯ï¼Œæ¶µç›–è¶…è¿‡6400ä¸ªè§†é¢‘å’Œå¤šç§ç‰©ä½“ç±»åˆ«åŠå¼‚å¸¸ç±»å‹ã€‚</li>
<li>å¼‚å¸¸æ£€æµ‹éœ€è¦è§†è§‰æ¨ç†ï¼Œç»“åˆç‰©ç†çŸ¥è¯†å’Œè§†é¢‘å†…å®¹æ¥åˆ¤æ–­ç‰©ä½“å¼‚å¸¸ã€‚</li>
<li>æˆ‘ä»¬è¯„ä¼°äº†ä¸åŒå¼‚å¸¸æ£€æµ‹æ–¹æ³•åœ¨å¤„ç†ç‰©ç†åŸºç¡€å¼‚å¸¸æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶æŒ‡å‡ºäº†å®ƒä»¬çš„å±€é™æ€§ã€‚</li>
<li>å¼•å…¥äº†Physics Anomaly Explanation (PAEval)æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹åœ¨æ£€æµ‹å¼‚å¸¸çš„åŒæ—¶ï¼Œèƒ½å¦æä¾›å‡†ç¡®çš„ç‰©ç†åŸå› è§£é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03562">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ef261f7343a9fc60975c899a3dda14da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef56af999e0fa66b19ff8b4bc0890fc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f97d4895f62f85ebb523df64d14f8202.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4787ddd8020d30c63adce184f36bfae7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5a95becc8f3f9ed3e3004a01d6972b3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e9026d3d0740622131e4ab68c7a5d20.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e78c0322915cab3e9f8b37d8ccb9b3e.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="HoT-Highlighted-Chain-of-Thought-for-Referencing-Supporting-Facts-from-Inputs"><a href="#HoT-Highlighted-Chain-of-Thought-for-Referencing-Supporting-Facts-from-Inputs" class="headerlink" title="HoT: Highlighted Chain of Thought for Referencing Supporting Facts from   Inputs"></a>HoT: Highlighted Chain of Thought for Referencing Supporting Facts from   Inputs</h2><p><strong>Authors:Tin Nguyen, Logan Bolton, Mohammad Reza Taesiri, Anh Totti Nguyen</strong></p>
<p>An Achilles heel of Large Language Models (LLMs) is their tendency to hallucinate non-factual statements. A response mixed of factual and non-factual statements poses a challenge for humans to verify and accurately base their decisions on. To combat this problem, we propose Highlighted Chain-of-Thought Prompting (HoT), a technique for prompting LLMs to generate responses with XML tags that ground facts to those provided in the query. That is, given an input question, LLMs would first re-format the question to add XML tags highlighting key facts, and then, generate a response with highlights over the facts referenced from the input. Interestingly, in few-shot settings, HoT outperforms vanilla chain of thought prompting (CoT) on a wide range of 17 tasks from arithmetic, reading comprehension to logical reasoning. When asking humans to verify LLM responses, highlights help time-limited participants to more accurately and efficiently recognize when LLMs are correct. Yet, surprisingly, when LLMs are wrong, HoTs tend to make users believe that an answer is correct. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸€ä¸ªå¼±ç‚¹æ˜¯å®ƒä»¬å€¾å‘äºäº§ç”Ÿéäº‹å®æ€§çš„é™ˆè¿°ã€‚ç”±äº‹å®å’Œå¹»è§‰éäº‹å®æ€§é™ˆè¿°æ„æˆçš„å›åº”ç»™äººç±»å¸¦æ¥äº†éªŒè¯å¹¶å‡†ç¡®åšå‡ºå†³ç­–çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜äº®åŒ–æ€ç»´é“¾æç¤ºï¼ˆHoTï¼‰æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§æç¤ºLLMç”Ÿæˆå¸¦æœ‰XMLæ ‡ç­¾çš„å“åº”çš„æ–¹æ³•ï¼Œè¿™äº›æ ‡ç­¾åŸºäºæŸ¥è¯¢ä¸­æä¾›çš„äº‹å®ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç»™å®šä¸€ä¸ªè¾“å…¥é—®é¢˜ï¼ŒLLMä¼šé¦–å…ˆé‡æ–°æ ¼å¼åŒ–é—®é¢˜ï¼Œæ·»åŠ çªå‡ºå…³é”®äº‹å®çš„XMLæ ‡ç­¾ï¼Œç„¶åç”ŸæˆåŒ…å«ä»è¾“å…¥ä¸­å¼•ç”¨çš„é‡ç‚¹äº‹å®çš„å“åº”ã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨å°‘æ•°æƒ…å†µä¸‹ï¼ŒHoTåœ¨ç®—æœ¯ã€é˜…è¯»ç†è§£åˆ°é€»è¾‘æ¨ç†çš„å¹¿æ³›ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºåŸºæœ¬çš„æ€ç»´é“¾æç¤ºï¼ˆCoTï¼‰ã€‚å½“è¦æ±‚äººç±»éªŒè¯LLMçš„å“åº”æ—¶ï¼Œé‡ç‚¹æœ‰åŠ©äºæ—¶é—´æœ‰é™çš„å‚ä¸è€…æ›´å‡†ç¡®ã€é«˜æ•ˆåœ°è¯†åˆ«LLMæ˜¯å¦æ­£ç¡®ã€‚ç„¶è€Œï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå½“LLMé”™è¯¯æ—¶ï¼ŒHoTå¾€å¾€ä½¿ç”¨æˆ·è®¤ä¸ºç­”æ¡ˆæ˜¯æ­£ç¡®çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02003v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸€ä¸ªå¼±ç‚¹æ˜¯å®ƒä»¬å€¾å‘äºäº§ç”Ÿéäº‹å®æ€§çš„é™ˆè¿°ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜äº®åŒ–æ€ç»´é“¾æç¤ºï¼ˆHoTï¼‰æŠ€æœ¯ï¼Œé€šè¿‡ä¸ºLLMç”Ÿæˆå“åº”æ·»åŠ XMLæ ‡ç­¾æ¥ç¡®ç«‹äº‹å®ä¸æŸ¥è¯¢ä¸­æä¾›çš„äº‹å®ä¹‹é—´çš„è”ç³»ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹ï¼ŒHoTåœ¨ç®—æœ¯ã€é˜…è¯»ç†è§£åˆ°é€»è¾‘æ¨ç†ç­‰17é¡¹ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„æ€ç»´é“¾æç¤ºï¼ˆCoTï¼‰ã€‚äººç±»éªŒè¯LLMå“åº”æ—¶ï¼Œé«˜äº®æœ‰åŠ©äºå‚ä¸è€…æ›´ç²¾ç¡®å¿«é€Ÿåœ°åˆ¤æ–­LLMçš„æ­£ç¡®æ€§ã€‚ç„¶è€Œï¼Œå½“LLMå‡ºé”™æ—¶ï¼ŒHoTæŠ€æœ¯å´å¯èƒ½ä½¿ç”¨æˆ·è¯¯ä»¥ä¸ºç­”æ¡ˆæ˜¯æ­£ç¡®çš„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å­˜åœ¨äº§ç”Ÿéäº‹å®æ€§é™ˆè¿°çš„é—®é¢˜ã€‚</li>
<li>Highlighted Chain-of-Thought Promptingï¼ˆHoTï¼‰æŠ€æœ¯é€šè¿‡æ·»åŠ XMLæ ‡ç­¾æ¥ç¡®ç«‹äº‹å®ä¸æŸ¥è¯¢ä¹‹é—´çš„è”ç³»ã€‚</li>
<li>åœ¨å°‘æ ·æœ¬æƒ…å†µä¸‹ï¼ŒHoTåœ¨å¤šç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ€ç»´é“¾æç¤ºï¼ˆCoTï¼‰ã€‚</li>
<li>é«˜äº®æç¤ºæœ‰åŠ©äºäººç±»å¿«é€Ÿå‡†ç¡®åœ°åˆ¤æ–­LLMçš„æ­£ç¡®æ€§ã€‚</li>
<li>HoTæŠ€æœ¯å­˜åœ¨æ½œåœ¨é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´ç”¨æˆ·åœ¨LLMå‡ºé”™æ—¶è¯¯åˆ¤ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02003">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-25005df8ffe12bfbb16bac1cb2a6b9d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5cfcafada0c2a49fc221f1b52a1c2ff2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4dba5a47ecc8d2d91ac10a67354f1552.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c33430d017f31e144895205bd76d8dd0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2abd555d82805b0a8a39aed90a1a1284.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-596c5aa2e0a70fece1ad798fa9b7c3b2.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MapExRL-Human-Inspired-Indoor-Exploration-with-Predicted-Environment-Context-and-Reinforcement-Learning"><a href="#MapExRL-Human-Inspired-Indoor-Exploration-with-Predicted-Environment-Context-and-Reinforcement-Learning" class="headerlink" title="MapExRL: Human-Inspired Indoor Exploration with Predicted Environment   Context and Reinforcement Learning"></a>MapExRL: Human-Inspired Indoor Exploration with Predicted Environment   Context and Reinforcement Learning</h2><p><strong>Authors:Narek Harutyunyan, Brady Moon, Seungchan Kim, Cherie Ho, Adam Hung, Sebastian Scherer</strong></p>
<p>Path planning for robotic exploration is challenging, requiring reasoning over unknown spaces and anticipating future observations. Efficient exploration requires selecting budget-constrained paths that maximize information gain. Despite advances in autonomous exploration, existing algorithms still fall short of human performance, particularly in structured environments where predictive cues exist but are underutilized. Guided by insights from our user study, we introduce MapExRL, which improves robot exploration efficiency in structured indoor environments by enabling longer-horizon planning through reinforcement learning (RL) and global map predictions. Unlike many RL-based exploration methods that use motion primitives as the action space, our approach leverages frontiers for more efficient model learning and longer horizon reasoning. Our framework generates global map predictions from the observed map, which our policy utilizes, along with the prediction uncertainty, estimated sensor coverage, frontier distance, and remaining distance budget, to assess the strategic long-term value of frontiers. By leveraging multiple frontier scoring methods and additional context, our policy makes more informed decisions at each stage of the exploration. We evaluate our framework on a real-world indoor map dataset, achieving up to an 18.8% improvement over the strongest state-of-the-art baseline, with even greater gains compared to conventional frontier-based algorithms. </p>
<blockquote>
<p>æœºå™¨äººæ¢ç´¢çš„è·¯å¾„è§„åˆ’å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œéœ€è¦åœ¨æœªçŸ¥ç©ºé—´è¿›è¡Œæ¨ç†å¹¶é¢„æµ‹æœªæ¥è§‚æµ‹ã€‚é«˜æ•ˆçš„æ¢ç´¢éœ€è¦é€‰æ‹©é¢„ç®—é™åˆ¶å†…çš„è·¯å¾„ï¼Œä»¥æœ€å¤§åŒ–ä¿¡æ¯å¢ç›Šã€‚å°½ç®¡è‡ªä¸»æ¢ç´¢å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰ç®—æ³•ä»è¾¾ä¸åˆ°äººç±»çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»“æ„åŒ–ç¯å¢ƒä¸­ï¼Œé¢„æµ‹çº¿ç´¢å­˜åœ¨ä½†åˆ©ç”¨ä¸è¶³ã€‚å—ç”¨æˆ·ç ”ç©¶çš„å¯å‘ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MapExRLï¼Œå®ƒé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå…¨å±€åœ°å›¾é¢„æµ‹ï¼Œæé«˜äº†æœºå™¨äººåœ¨ç»“æ„åŒ–å®¤å†…ç¯å¢ƒä¸­çš„æ¢ç´¢æ•ˆç‡ã€‚ä¸è®¸å¤šåŸºäºRLçš„æ¢ç´¢æ–¹æ³•ä½¿ç”¨åŠ¨ä½œåŸè¯­ä½œä¸ºåŠ¨ä½œç©ºé—´ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨è¾¹ç•Œè¿›è¡Œæ›´æœ‰æ•ˆçš„æ¨¡å‹å­¦ä¹ å’Œæ›´é•¿çš„è§†é‡æ¨ç†ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä»è§‚å¯Ÿåˆ°çš„åœ°å›¾ç”Ÿæˆå…¨å±€åœ°å›¾é¢„æµ‹ï¼Œæˆ‘ä»¬çš„ç­–ç•¥åˆ©ç”¨è¿™äº›é¢„æµ‹ï¼Œä»¥åŠé¢„æµ‹çš„ä¸ç¡®å®šæ€§ã€ä¼°è®¡çš„ä¼ æ„Ÿå™¨è¦†ç›–èŒƒå›´ã€è¾¹ç•Œè·ç¦»å’Œå‰©ä½™è·ç¦»é¢„ç®—ï¼Œæ¥è¯„ä¼°è¾¹ç•Œçš„æˆ˜ç•¥é•¿æœŸä»·å€¼ã€‚é€šè¿‡åˆ©ç”¨å¤šç§è¾¹ç•Œè¯„åˆ†æ–¹æ³•å’Œé¢å¤–çš„ä¸Šä¸‹æ–‡ï¼Œæˆ‘ä»¬çš„ç­–ç•¥åœ¨æ¯ä¸ªæ¢ç´¢é˜¶æ®µéƒ½èƒ½åšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚æˆ‘ä»¬åœ¨çœŸå®çš„å®¤å†…åœ°å›¾æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œä¸æœ€å…ˆè¿›çš„åŸºçº¿ç›¸æ¯”ï¼Œæœ€å¤šæé«˜äº†18.8%ï¼Œä¸ä¼ ç»ŸåŸºäºè¾¹ç•Œçš„ç®—æ³•ç›¸æ¯”ï¼Œç”šè‡³è·å¾—äº†æ›´å¤§çš„æ”¶ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01548v1">PDF</a> 8 pages, 6 figures</p>
<p><strong>æ‘˜è¦</strong><br>æœºå™¨äººè·¯å¾„è§„åˆ’ç”¨äºæ¢ç´¢æœªçŸ¥ç©ºé—´å¹¶é¢„æµ‹æœªæ¥è§‚æµ‹ç»“æœï¼Œæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚æœ‰æ•ˆçš„æ¢ç´¢éœ€è¦é€‰æ‹©é¢„ç®—æœ‰é™çš„è·¯å¾„ï¼Œä»¥æœ€å¤§åŒ–ä¿¡æ¯æ”¶ç›Šã€‚å°½ç®¡è‡ªä¸»æ¢ç´¢é¢†åŸŸå–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰ç®—æ³•ä»ç„¶è¾¾ä¸åˆ°äººç±»æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»“æ„åŒ–ç¯å¢ƒä¸­ï¼Œå­˜åœ¨é¢„æµ‹çº¿ç´¢ä½†åˆ©ç”¨ä¸è¶³ã€‚å—ç”¨æˆ·ç ”ç©¶çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†MapExRLï¼Œå®ƒé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå…¨å±€åœ°å›¾é¢„æµ‹ï¼Œæé«˜äº†æœºå™¨äººåœ¨ç»“æ„åŒ–å®¤å†…ç¯å¢ƒä¸­çš„æ¢ç´¢æ•ˆç‡ã€‚ä¸å…¶ä»–åŸºäºRLçš„æ¢ç´¢æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨å‰æ²¿æŠ€æœ¯æ¥å®ç°æ›´æœ‰æ•ˆçš„æ¨¡å‹å­¦ä¹ å’Œæ›´é•¿è¿œçš„æ¨ç†ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä»è§‚å¯Ÿåˆ°çš„åœ°å›¾ç”Ÿæˆå…¨å±€åœ°å›¾é¢„æµ‹ï¼Œæˆ‘ä»¬çš„ç­–ç•¥åˆ©ç”¨é¢„æµ‹ä¸ç¡®å®šæ€§ã€ä¼°è®¡çš„ä¼ æ„Ÿå™¨è¦†ç›–èŒƒå›´ã€å‰æ²¿è·ç¦»å’Œå‰©ä½™è·ç¦»é¢„ç®—æ¥è¯„ä¼°å‰æ²¿çš„æˆ˜ç•¥é•¿æœŸä»·å€¼ã€‚é€šè¿‡åˆ©ç”¨å¤šç§å‰æ²¿è¯„åˆ†æ–¹æ³•å’Œé¢å¤–çš„ä¸Šä¸‹æ–‡ï¼Œæˆ‘ä»¬çš„ç­–ç•¥èƒ½å¤Ÿåœ¨æ¢ç´¢çš„æ¯ä¸ªé˜¶æ®µåšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚æˆ‘ä»¬åœ¨çœŸå®çš„å®¤å†…åœ°å›¾æ•°æ®é›†ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„åŸºç¡€ç®—æ³•å®ç°äº†æœ€é«˜è¾¾18.8%çš„æ”¹è¿›ï¼Œç›¸æ¯”äºä¼ ç»Ÿçš„å‰æ²¿ç®—æ³•æ•ˆæœæå‡æ›´å¤§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœºå™¨äººæ¢ç´¢è·¯å¾„è§„åˆ’éœ€è¦å¤„ç†æœªçŸ¥ç©ºé—´å’Œé¢„æµ‹æœªæ¥è§‚æµ‹ç»“æœã€‚</li>
<li>æœ‰æ•ˆæ¢ç´¢éœ€è¦é€‰æ‹©é¢„ç®—æœ‰é™çš„è·¯å¾„ä»¥æœ€å¤§åŒ–ä¿¡æ¯æ”¶ç›Šã€‚</li>
<li>åœ¨ç»“æ„åŒ–ç¯å¢ƒä¸­ï¼Œç°æœ‰ç®—æ³•çš„è‡ªä¸»æ€§èƒ½ä»ä½äºäººç±»è¡¨ç°ã€‚</li>
<li>MapExRLé€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œå…¨å±€åœ°å›¾é¢„æµ‹æé«˜äº†æœºå™¨äººåœ¨ç»“æ„åŒ–å®¤å†…ç¯å¢ƒçš„æ¢ç´¢æ•ˆç‡ã€‚</li>
<li>ä¸å…¶ä»–åŸºäºRLçš„æ¢ç´¢æ–¹æ³•ä¸åŒï¼ŒMapExRLä½¿ç”¨å‰æ²¿æŠ€æœ¯å®ç°æ›´æœ‰æ•ˆçš„æ¨¡å‹å­¦ä¹ å’Œæ›´é•¿è¿œçš„æ¨ç†ã€‚</li>
<li>MapExRLæ¡†æ¶è€ƒè™‘äº†é¢„æµ‹ä¸ç¡®å®šæ€§ã€ä¼ æ„Ÿå™¨è¦†ç›–èŒƒå›´ã€å‰æ²¿è·ç¦»å’Œå‰©ä½™è·ç¦»é¢„ç®—ç­‰å¤šä¸ªå› ç´ æ¥è¯„ä¼°è·¯å¾„é€‰æ‹©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01548">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7ceb442c2de2e6b751229f08961f60f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4df336a19f6b6981c1393fd454ad9073.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b5d48fc22aab8a4fe1ebcb376c3bf792.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-360b9ce58f1e875c6da762d7f0ad911e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fca31b15e5744a1b4f8845b5046bd55.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Llama-3-1-Sherkala-8B-Chat-An-Open-Large-Language-Model-for-Kazakh"><a href="#Llama-3-1-Sherkala-8B-Chat-An-Open-Large-Language-Model-for-Kazakh" class="headerlink" title="Llama-3.1-Sherkala-8B-Chat: An Open Large Language Model for Kazakh"></a>Llama-3.1-Sherkala-8B-Chat: An Open Large Language Model for Kazakh</h2><p><strong>Authors:Fajri Koto, Rituraj Joshi, Nurdaulet Mukhituly, Yuxia Wang, Zhuohan Xie, Rahul Pal, Daniil Orel, Parvez Mullah, Diana Turmakhan, Maiya Goloburda, Mohammed Kamran, Samujjwal Ghosh, Bokang Jia, Jonibek Mansurov, Mukhammed Togmanov, Debopriyo Banerjee, Nurkhan Laiyk, Akhmed Sakip, Xudong Han, Ekaterina Kochmar, Alham Fikri Aji, Aaryamonvikram Singh, Alok Anil Jadhav, Satheesh Katipomu, Samta Kamboj, Monojit Choudhury, Gurpreet Gosal, Gokul Ramakrishnan, Biswajit Mishra, Sarath Chandran, Avraham Sheinin, Natalia Vassilieva, Neha Sengupta, Larry Murray, Preslav Nakov</strong></p>
<p>Llama-3.1-Sherkala-8B-Chat, or Sherkala-Chat (8B) for short, is a state-of-the-art instruction-tuned open generative large language model (LLM) designed for Kazakh. Sherkala-Chat (8B) aims to enhance the inclusivity of LLM advancements for Kazakh speakers. Adapted from the LLaMA-3.1-8B model, Sherkala-Chat (8B) is trained on 45.3B tokens across Kazakh, English, Russian, and Turkish. With 8 billion parameters, it demonstrates strong knowledge and reasoning abilities in Kazakh, significantly outperforming existing open Kazakh and multilingual models of similar scale while achieving competitive performance in English. We release Sherkala-Chat (8B) as an open-weight instruction-tuned model and provide a detailed overview of its training, fine-tuning, safety alignment, and evaluation, aiming to advance research and support diverse real-world applications. </p>
<blockquote>
<p>Llama-3.1-Sherkala-8B-Chatï¼Œç®€ç§°Sherkala-Chatï¼ˆ8Bï¼‰ï¼Œæ˜¯ä¸€æ¬¾é’ˆå¯¹å“ˆè¨å…‹è¯­è®¾è®¡çš„æœ€å…ˆè¿›çš„æŒ‡ä»¤ä¼˜åŒ–å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚Sherkala-Chatï¼ˆ8Bï¼‰æ—¨åœ¨æé«˜å“ˆè¨å…‹è¯­ä½¿ç”¨è€…å¯¹LLMè¿›æ­¥çš„åŒ…å®¹æ€§ã€‚è¯¥æ¨¡å‹åŸºäºLLaMA-3.1-8Bæ¨¡å‹æ”¹ç¼–ï¼Œç»è¿‡å“ˆè¨å…‹è¯­ã€è‹±è¯­ã€ä¿„è¯­å’ŒåœŸè€³å…¶è¯­çš„45.3Bæ ‡è®°è®­ç»ƒã€‚æ‹¥æœ‰8äº¿å‚æ•°ï¼Œå®ƒåœ¨å“ˆè¨å…‹è¯­æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼€æºå“ˆè¨å…‹è¯­å’Œå¤šè¯­è¨€æ¨¡å‹ï¼ŒåŒæ—¶åœ¨è‹±è¯­æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ã€‚æˆ‘ä»¬å‘å¸ƒå¼€æºæƒé‡æŒ‡ä»¤ä¼˜åŒ–æ¨¡å‹Sherkala-Chatï¼ˆ8Bï¼‰ï¼Œå¹¶å¯¹å…¶è®­ç»ƒã€å¾®è°ƒã€å®‰å…¨å¯¹é½å’Œè¯„ä¼°æä¾›äº†è¯¦ç»†çš„æ¦‚è¿°ï¼Œæ—¨åœ¨æ¨åŠ¨ç ”ç©¶å¹¶æ”¯æŒå„ç§å®é™…åº”ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01493v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>LLaMA-3.1-Sherkala-8B-Chatæ˜¯ä¸€æ¬¾é’ˆå¯¹å“ˆè¨å…‹è¯­ä¼˜åŒ–çš„ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ—¨åœ¨æé«˜å“ˆè¨å…‹è¯­äººç¾¤çš„è¯­è¨€æ¨¡å‹åº”ç”¨åŒ…å®¹æ€§ã€‚åŸºäºLLaMA-3.1-8Bæ¨¡å‹æ”¹ç¼–ï¼Œç»è¿‡å“ˆè¨å…‹è¯­ã€è‹±è¯­ã€ä¿„è¯­å’ŒåœŸè€³å…¶è¯­çš„è®­ç»ƒï¼Œå±•ç°å‡ºå¼ºå¤§çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œåœ¨å“ˆè¨å…‹è¯­ä¸Šæ€§èƒ½å‡ºä¼—ã€‚å…¬å¼€ä¸ºæƒé‡å‹æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ï¼Œå¹¶ä¸ºç ”ç©¶è€…å’Œå¼€å‘äººå‘˜æä¾›äº†è®­ç»ƒã€å¾®è°ƒã€å®‰å…¨å¯¹é½å’Œè¯„ä¼°çš„è¯¦ç»†æ¦‚è¿°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Sherkala-Chat (8B)æ˜¯ä¸€æ¬¾é’ˆå¯¹å“ˆè¨å…‹è¯­çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚</li>
<li>è¯¥æ¨¡å‹æ˜¯åŸºäºLLaMA-3.1-8Bæ¨¡å‹æ”¹ç¼–è€Œæ¥ã€‚</li>
<li>Sherkala-Chat (8B)ç»è¿‡å¤šç§è¯­è¨€çš„è®­ç»ƒï¼ŒåŒ…æ‹¬å“ˆè¨å…‹è¯­ã€è‹±è¯­ã€ä¿„è¯­å’ŒåœŸè€³å…¶è¯­ã€‚</li>
<li>å®ƒæ‹¥æœ‰å¼ºå¤§çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œåœ¨å“ˆè¨å…‹è¯­æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼€æ”¾å“ˆè¨å…‹å’Œå¤šè¯­ç§æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹è¢«å…¬å¼€ä¸ºæƒé‡å‹æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ã€‚</li>
<li>æä¾›äº†å…³äºæ¨¡å‹çš„è®­ç»ƒã€å¾®è°ƒã€å®‰å…¨å¯¹é½å’Œè¯„ä¼°çš„è¯¦ç»†æ¦‚è¿°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01493">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cfba592207a0d460738bcd4a2d20b455.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5926d3a7771c32f1af7cd1e650c440af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8124f74e64b7e93f9a59efae592c7717.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Group-Relative-Policy-Optimization-for-Image-Captioning"><a href="#Group-Relative-Policy-Optimization-for-Image-Captioning" class="headerlink" title="Group Relative Policy Optimization for Image Captioning"></a>Group Relative Policy Optimization for Image Captioning</h2><p><strong>Authors:Xu Liang</strong></p>
<p>Image captioning tasks usually use two-stage training to complete model optimization. The first stage uses cross-entropy as the loss function for optimization, and the second stage uses self-critical sequence training (SCST) for reinforcement learning optimization. However, the SCST algorithm has certain defects. SCST relies only on a single greedy decoding result as a baseline. If the model itself is not stable enough, the greedy decoding result may be relatively worst, which will lead to a high variance of advantage estimation, further leading to unstable policy updates. In addition, SCST only compares one sampling result with the greedy decoding result, and the generation diversity is limited, which may fall into a local optimum. In this paper, we propose using the latest Group Relative Policy Optimization (GRPO) reinforcement learning algorithm as an optimization solution for the second stage. GRPO generates multiple candidate captions for the input image and then continuously optimizes the model through intragroup comparison. By constraining the amplitude of policy updates and KL divergence, the stability of the model during training is greatly guaranteed. In addition, compared to SCST, which only samples one answer, GRPO samples and generates multiple answers. Multiple candidate answers in the group cover a wider solution space. Combined with KL divergence constraints, GRPO can improve diversity while ensuring model stability. The code for this article is available at <a target="_blank" rel="noopener" href="https://github.com/liangxu-one/ms-models/tree/image_caption_grpo/research/arxiv_papers/Image_Caption_GRPO">https://github.com/liangxu-one/ms-models/tree/image_caption_grpo/research/arxiv_papers/Image_Caption_GRPO</a>. </p>
<blockquote>
<p>å›¾åƒæè¿°ä»»åŠ¡é€šå¸¸ä½¿ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¥å®Œæˆæ¨¡å‹ä¼˜åŒ–ã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨äº¤å‰ç†µä½œä¸ºä¼˜åŒ–æŸå¤±å‡½æ•°ï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨è‡ªæˆ‘æ‰¹åˆ¤åºåˆ—è®­ç»ƒï¼ˆSCSTï¼‰è¿›è¡Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ã€‚ç„¶è€Œï¼ŒSCSTç®—æ³•å­˜åœ¨ä¸€äº›ç¼ºé™·ã€‚SCSTä»…ä¾èµ–äºå•ä¸ªè´ªå©ªè§£ç ç»“æœä½œä¸ºåŸºçº¿ã€‚å¦‚æœæ¨¡å‹æœ¬èº«ä¸å¤Ÿç¨³å®šï¼Œè´ªå©ªè§£ç çš„ç»“æœå¯èƒ½ç›¸å¯¹è¾ƒå·®ï¼Œè¿™å°†å¯¼è‡´ä¼˜åŠ¿ä¼°è®¡çš„æ–¹å·®è¾ƒé«˜ï¼Œè¿›è€Œé€ æˆç­–ç•¥æ›´æ–°ä¸ç¨³å®šã€‚æ­¤å¤–ï¼ŒSCSTä»…å°†ä¸€æ¬¡é‡‡æ ·ç»“æœä¸è´ªå©ªè§£ç ç»“æœè¿›è¡Œæ¯”è¾ƒï¼Œç”Ÿæˆå¤šæ ·æ€§å—é™ï¼Œå¯èƒ½ä¼šé™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨æœ€æ–°çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¼ºåŒ–å­¦ä¹ ç®—æ³•ä½œä¸ºç¬¬äºŒé˜¶æ®µçš„ä¼˜åŒ–è§£å†³æ–¹æ¡ˆã€‚GRPOä¸ºè¾“å…¥å›¾åƒç”Ÿæˆå¤šä¸ªå€™é€‰æè¿°ï¼Œç„¶åé€šè¿‡ç»„å†…æ¯”è¾ƒä¸æ–­å¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚é€šè¿‡çº¦æŸç­–ç•¥æ›´æ–°çš„å¹…åº¦å’ŒKLæ•£åº¦ï¼Œå¯ä»¥æå¤§åœ°ä¿è¯æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œä¸åªé‡‡æ ·ä¸€ä¸ªç­”æ¡ˆçš„SCSTç›¸æ¯”ï¼ŒGRPOå¯ä»¥é‡‡æ ·å¹¶ç”Ÿæˆå¤šä¸ªç­”æ¡ˆã€‚ç»„ä¸­çš„å¤šä¸ªå€™é€‰ç­”æ¡ˆå¯ä»¥è¦†ç›–æ›´å¹¿æ³›çš„è§£ç©ºé—´ã€‚ç»“åˆKLæ•£åº¦çº¦æŸï¼ŒGRPOå¯ä»¥åœ¨ç¡®ä¿æ¨¡å‹ç¨³å®šæ€§çš„åŒæ—¶æé«˜å¤šæ ·æ€§ã€‚æœ¬æ–‡çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/liangxu-one/ms-models/tree/image_caption_grpo/research/arxiv_papers/Image_Caption_GRPO">https://github.com/liangxu-one/ms-models/tree/image_caption_grpo/research/arxiv_papers/Image_Caption_GRPO</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01333v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å›¾åƒæè¿°ä»»åŠ¡é€šå¸¸é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒå®Œæˆæ¨¡å‹ä¼˜åŒ–ã€‚ç¬¬ä¸€é˜¶æ®µä½¿ç”¨äº¤å‰ç†µä½œä¸ºæŸå¤±å‡½æ•°è¿›è¡Œä¼˜åŒ–ï¼Œç¬¬äºŒé˜¶æ®µåˆ™ä½¿ç”¨è‡ªæˆ‘æ‰¹åˆ¤åºåˆ—è®­ç»ƒï¼ˆSCSTï¼‰è¿›è¡Œå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ã€‚ä½†SCSTç®—æ³•å­˜åœ¨ç¼ºé™·ï¼Œæœ¬æ–‡æå‡ºä½¿ç”¨æœ€æ–°çš„Group Relative Policy Optimizationï¼ˆGRPOï¼‰å¼ºåŒ–å­¦ä¹ ç®—æ³•ä½œä¸ºç¬¬äºŒé˜¶æ®µçš„ä¼˜åŒ–è§£å†³æ–¹æ¡ˆã€‚GRPOé€šè¿‡ç»„å†…æ¯”è¾ƒæŒç»­ä¼˜åŒ–æ¨¡å‹ï¼Œä¿è¯è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡å‹çš„ç¨³å®šæ€§ï¼Œå¹¶æé«˜ç­”æ¡ˆçš„å¤šæ ·æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒæè¿°ä»»åŠ¡é€šå¸¸ä½¿ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼Œç¬¬ä¸€é˜¶æ®µä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè‡ªæˆ‘æ‰¹åˆ¤åºåˆ—è®­ç»ƒï¼ˆSCSTï¼‰ã€‚</li>
<li>SCSTç®—æ³•å­˜åœ¨ç¼ºé™·ï¼Œä¸»è¦è¡¨ç°åœ¨å¯¹æ¨¡å‹ç¨³å®šæ€§çš„ä¾èµ–ä»¥åŠç”Ÿæˆç­”æ¡ˆçš„å¤šæ ·æ€§å—é™ã€‚</li>
<li>GRPOç®—æ³•è¢«æå‡ºä½œä¸ºSCSTçš„ä¼˜åŒ–æ›¿ä»£ï¼Œé€šè¿‡ç”Ÿæˆå¤šä¸ªå€™é€‰æè¿°æ¥ä¼˜åŒ–æ¨¡å‹ã€‚</li>
<li>GRPOé€šè¿‡ç»„å†…æ¯”è¾ƒæ¥æŒç»­ä¼˜åŒ–æ¨¡å‹ï¼Œä¿è¯è®­ç»ƒç¨³å®šæ€§ã€‚</li>
<li>GRPOé€šè¿‡çº¦æŸç­–ç•¥æ›´æ–°çš„å¹…åº¦å’ŒKLæ•£åº¦ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¨³å®šæ€§ã€‚</li>
<li>ä¸SCSTç›¸æ¯”ï¼ŒGRPOèƒ½å¤Ÿé‡‡æ ·å¹¶ç”Ÿæˆå¤šä¸ªç­”æ¡ˆï¼Œè¦†ç›–æ›´å¹¿æ³›çš„è§£å†³æ–¹æ¡ˆç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01333">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-68ab5416c6a35f84e54cdbeecded1847.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c27971d2d64a5b251fd3234dd26a187.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acc8f521a414343252ac722e1586cd74.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="CE-U-Cross-Entropy-Unlearning"><a href="#CE-U-Cross-Entropy-Unlearning" class="headerlink" title="CE-U: Cross Entropy Unlearning"></a>CE-U: Cross Entropy Unlearning</h2><p><strong>Authors:Bo Yang</strong></p>
<p>Large language models (LLMs) inadvertently memorize sensitive data from their massive pretraining corpora \cite{jang2022knowledge}. In this work, we propose CE-U (Cross Entropy Unlearning), a novel loss function designed specifically for unlearning tasks. CE-U addresses fundamental limitations of gradient ascent approaches which suffer from instability due to vanishing gradients when model confidence is high and gradient exploding when confidence is low. We also unify standard cross entropy supervision and cross entropy unlearning into a single framework. Notably, on the TOFU benchmark for unlearning \cite{maini2024tofu}, CE-U achieves state-of-the-art results on LLaMA2-7B with 1% and 5% forgetting, even without the use of any extra reference model or additional positive samples. Our theoretical analysis further reveals that the gradient instability issues also exist in popular reinforcement learning algorithms like DPO \cite{rafailov2023direct} and GRPO\cite{Shao2024DeepSeekMath}, as they include a gradient ascent component. This suggests that applying CE-U principles to reinforcement learning could be a promising direction for improving stability and convergence. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¼šæ— æ„ä¸­ä»å…¶åºå¤§çš„é¢„è®­ç»ƒè¯­æ–™åº“ä¸­è®°å¿†æ•æ„Ÿæ•°æ®\cite{jang2022knowledge}ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CE-Uï¼ˆäº¤å‰ç†µé—å¿˜ï¼‰ï¼ˆCross Entropy Unlearningï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºé—å¿˜ä»»åŠ¡è®¾è®¡çš„å…¨æ–°æŸå¤±å‡½æ•°ã€‚CE-Uè§£å†³äº†æ¢¯åº¦ä¸Šå‡æ–¹æ³•çš„åŸºæœ¬å±€é™æ€§ï¼Œè¿™äº›æ–¹æ³•åœ¨é«˜æ¨¡å‹ç½®ä¿¡åº¦æ—¶å—åˆ°æ¢¯åº¦æ¶ˆå¤±çš„å½±å“ï¼Œè€Œåœ¨ä½ç½®ä¿¡åº¦æ—¶åˆ™å—åˆ°æ¢¯åº¦çˆ†ç‚¸çš„å½±å“ã€‚æˆ‘ä»¬è¿˜ç»Ÿä¸€äº†æ ‡å‡†äº¤å‰ç†µç›‘ç£å’Œäº¤å‰ç†µé—å¿˜åˆ°ä¸€ä¸ªå•ä¸€æ¡†æ¶ä¸­ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨é—å¿˜çš„TOFUåŸºå‡†æµ‹è¯•\cite{maini2024tofu}ä¸Šï¼ŒCE-Uåœ¨LLaMA2-7Bçš„1%å’Œ5%é—å¿˜ç‡ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå³ä½¿ä¸ä½¿ç”¨ä»»ä½•é¢å¤–çš„å‚è€ƒæ¨¡å‹æˆ–é¢å¤–çš„æ­£æ ·æœ¬ã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æè¿›ä¸€æ­¥æ­ç¤ºäº†æ¢¯åº¦ä¸ç¨³å®šé—®é¢˜ä¹Ÿå­˜åœ¨äºå¦‚DPO\cite{rafailov2023direct}å’ŒGRPO\cite{Shao2024DeepSeekMath}ç­‰æµè¡Œçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸­ï¼Œå› ä¸ºå®ƒä»¬åŒ…å«æ¢¯åº¦ä¸Šå‡çš„æˆåˆ†ã€‚è¿™è¡¨æ˜å°†CE-UåŸåˆ™åº”ç”¨äºå¼ºåŒ–å­¦ä¹ å¯èƒ½æ˜¯æé«˜ç¨³å®šæ€§å’Œæ”¶æ•›æ€§çš„ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01224v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæ— æ„ä¸­è®°å¿†æ•æ„Ÿæ•°æ®ã€‚æœ¬ç ”ç©¶æå‡ºCE-Uï¼ˆäº¤å‰ç†µé—å¿˜ï¼‰æŸå¤±å‡½æ•°ï¼Œä¸“é—¨ç”¨äºé—å¿˜ä»»åŠ¡ã€‚CE-Uè§£å†³äº†æ¢¯åº¦ä¸Šå‡æ–¹æ³•çš„åŸºæœ¬å±€é™ï¼ŒåŒ…æ‹¬æ¢¯åº¦æ¶ˆå¤±ä¸çˆ†ç‚¸çš„é—®é¢˜ã€‚åœ¨TOFUé—å¿˜åŸºå‡†æµ‹è¯•ä¸­ï¼ŒCE-Uåœ¨LLaMA2-7Bæ¨¡å‹ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é—å¿˜æ•ˆæœï¼Œå³ä½¿ä¸ä½¿ç”¨ä»»ä½•é¢å¤–çš„å‚è€ƒæ¨¡å‹æˆ–é¢å¤–çš„æ­£æ ·æœ¬ã€‚ç†è®ºåˆ†æè¿˜æ˜¾ç¤ºï¼Œæ¢¯åº¦ä¸ç¨³å®šé—®é¢˜ä¹Ÿå­˜åœ¨äºæµè¡Œçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸­ï¼Œå¦‚DPOå’ŒGRPOã€‚è¿™è¡¨æ˜å°†CE-UåŸç†åº”ç”¨äºå¼ºåŒ–å­¦ä¹ å¯èƒ½æ˜¯æé«˜ç¨³å®šæ€§å’Œæ”¶æ•›æ€§çš„æœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½æ— æ„è¯†åœ°è®°å¿†æ•æ„Ÿæ•°æ®ã€‚</li>
<li>CE-Uæ˜¯ä¸€ç§é’ˆå¯¹é—å¿˜ä»»åŠ¡çš„æ–°çš„æŸå¤±å‡½æ•°ï¼Œè§£å†³äº†æ¢¯åº¦ä¸Šå‡æ–¹æ³•ä¸­çš„åŸºç¡€å±€é™æ€§ã€‚</li>
<li>CE-Uèƒ½æœ‰æ•ˆè§£å†³æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜ã€‚</li>
<li>åœ¨TOFUåŸºå‡†æµ‹è¯•ä¸­ï¼ŒCE-Uåœ¨LLaMA2-7Bæ¨¡å‹ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é—å¿˜æ•ˆæœã€‚</li>
<li>CE-Uä¸ä½¿ç”¨é¢å¤–çš„å‚è€ƒæ¨¡å‹æˆ–é¢å¤–çš„æ­£æ ·æœ¬ã€‚</li>
<li>ç†è®ºåˆ†ææ˜¾ç¤ºæ¢¯åº¦ä¸ç¨³å®šé—®é¢˜ä¹Ÿå­˜åœ¨äºæŸäº›å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01224">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e692990bc0b7f064c750f7b2b328808.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a600ecbd251bed9b7a598f512dd2b6fc.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-c7eb8933593370723397907a2b0cfb45.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-09  LLMVoX Autoregressive Streaming Text-to-Speech Model for Any LLM
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-07/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-fc875c028575d0acf2e5c2a22d107c18.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-07  Look, Listen, and Answer Overcoming Biases for Audio-Visual Question   Answering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23251k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
