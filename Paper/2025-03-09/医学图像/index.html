<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-10  RadIR A Scalable Framework for Multi-Grained Medical Image Retrieval   via Radiology Report Mining">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-de552bc34893907d1e9500a7bc2f586a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-10-æ›´æ–°"><a href="#2025-03-10-æ›´æ–°" class="headerlink" title="2025-03-10 æ›´æ–°"></a>2025-03-10 æ›´æ–°</h1><h2 id="RadIR-A-Scalable-Framework-for-Multi-Grained-Medical-Image-Retrieval-via-Radiology-Report-Mining"><a href="#RadIR-A-Scalable-Framework-for-Multi-Grained-Medical-Image-Retrieval-via-Radiology-Report-Mining" class="headerlink" title="RadIR: A Scalable Framework for Multi-Grained Medical Image Retrieval   via Radiology Report Mining"></a>RadIR: A Scalable Framework for Multi-Grained Medical Image Retrieval   via Radiology Report Mining</h2><p><strong>Authors:Tengfei Zhang, Ziheng Zhao, Chaoyi Wu, Xiao Zhou, Ya Zhang, Yangfeng Wang, Weidi Xie</strong></p>
<p>Developing advanced medical imaging retrieval systems is challenging due to the varying definitions of &#96;similar imagesâ€™ across different medical contexts. This challenge is compounded by the lack of large-scale, high-quality medical imaging retrieval datasets and benchmarks. In this paper, we propose a novel methodology that leverages dense radiology reports to define image-wise similarity ordering at multiple granularities in a scalable and fully automatic manner. Using this approach, we construct two comprehensive medical imaging retrieval datasets: MIMIC-IR for Chest X-rays and CTRATE-IR for CT scans, providing detailed image-image ranking annotations conditioned on diverse anatomical structures. Furthermore, we develop two retrieval systems, RadIR-CXR and model-ChestCT, which demonstrate superior performance in traditional image-image and image-report retrieval tasks. These systems also enable flexible, effective image retrieval conditioned on specific anatomical structures described in text, achieving state-of-the-art results on 77 out of 78 metrics. </p>
<blockquote>
<p>å¼€å‘å…ˆè¿›çš„åŒ»å­¦å›¾åƒæ£€ç´¢ç³»ç»Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºâ€œç›¸ä¼¼å›¾åƒâ€çš„å®šä¹‰åœ¨ä¸åŒçš„åŒ»å­¦è¯­å¢ƒä¸­å„ä¸ç›¸åŒã€‚è¿™ä¸€æŒ‘æˆ˜ç”±äºç¼ºä¹å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„åŒ»å­¦å›¾åƒæ£€ç´¢æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•è€ŒåŠ å‰§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨å¯†é›†çš„æ”¾å°„å­¦æŠ¥å‘Šæ¥å®šä¹‰å›¾åƒå±‚é¢çš„ç›¸ä¼¼æ€§æ’åºï¼Œä»¥å¯ä¼¸ç¼©å’Œå…¨è‡ªåŠ¨çš„æ–¹å¼åœ¨å¤šä¸ªç²’åº¦çº§åˆ«ä¸Šå®ç°ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸¤ä¸ªå…¨é¢çš„åŒ»å­¦å›¾åƒæ£€ç´¢æ•°æ®é›†ï¼šç”¨äºèƒ¸éƒ¨Xå°„çº¿çš„MIMIC-IRå’Œç”¨äºCTæ‰«æçš„CTRATE-IRï¼Œå®ƒä»¬æ ¹æ®å¤šç§è§£å‰–ç»“æ„æä¾›äº†è¯¦ç»†çš„å›¾åƒ-å›¾åƒæ’åæ³¨é‡Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸¤ä¸ªæ£€ç´¢ç³»ç»ŸRadIR-CXRå’Œmodel-ChestCTï¼Œåœ¨ä¼ ç»Ÿçš„å›¾åƒ-å›¾åƒå’Œå›¾åƒ-æŠ¥å‘Šæ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚è¿™äº›ç³»ç»Ÿè¿˜èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬ä¸­æè¿°çš„ç‰¹å®šè§£å‰–ç»“æ„è¿›è¡Œçµæ´»æœ‰æ•ˆçš„å›¾åƒæ£€ç´¢ï¼Œåœ¨78ä¸ªæŒ‡æ ‡ä¸­çš„77ä¸ªä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04653v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åŒ»å­¦å›¾åƒæ£€ç´¢ç³»ç»Ÿé¢ä¸´å¤šç§æŒ‘æˆ˜ï¼Œå…¶ä¸­åŒ…æ‹¬åœ¨ä¸åŒåŒ»å­¦è¯­å¢ƒä¸‹å¯¹â€œç›¸ä¼¼å›¾åƒâ€å®šä¹‰çš„å·®å¼‚ã€ç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡åŒ»å­¦å›¾åƒæ£€ç´¢æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ç­‰ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡å¯†é›†çš„æ”¾å°„å­¦æŠ¥å‘Šæ¥å®šä¹‰å›¾åƒçº§ç›¸ä¼¼æ€§æ’åºçš„å¤šç§ç²’åº¦ï¼Œå¹¶ä»¥å¯ä¼¸ç¼©å’Œå…¨è‡ªåŠ¨çš„æ–¹å¼è¿›è¡Œã€‚åŸºäºæ­¤æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸¤ä¸ªå…¨é¢çš„åŒ»å­¦å›¾åƒæ£€ç´¢æ•°æ®é›†ï¼šMIMIC-IRç”¨äºèƒ¸éƒ¨Xå…‰æ£€æŸ¥å’ŒCTRATE-IRç”¨äºCTæ‰«æï¼Œä¸ºåŸºäºä¸åŒè§£å‰–ç»“æ„çš„å›¾åƒå›¾åƒæ’åæä¾›äº†è¯¦ç»†çš„æ³¨é‡Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸¤ä¸ªæ£€ç´¢ç³»ç»ŸRadIR-CXRå’Œmodel-ChestCTï¼Œåœ¨ä¼ ç»Ÿçš„å›¾åƒå›¾åƒå’Œå›¾åƒæŠ¥å‘Šæ£€ç´¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚è¿™äº›ç³»ç»Ÿè¿˜å¯ä»¥æ ¹æ®æ–‡æœ¬ä¸­æè¿°çš„ç‰¹å®šè§£å‰–ç»“æ„å®ç°çµæ´»æœ‰æ•ˆçš„å›¾åƒæ£€ç´¢ï¼Œåœ¨78ä¸ªæŒ‡æ ‡ä¸­çš„77ä¸ªä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒæ£€ç´¢ç³»ç»Ÿé¢ä¸´å®šä¹‰â€œç›¸ä¼¼å›¾åƒâ€çš„æŒ‘æˆ˜ï¼Œè¿™ä¸€æŒ‘æˆ˜å› ç¼ºä¹å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•è€ŒåŠ å‰§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¯†é›†çš„æ”¾å°„å­¦æŠ¥å‘Šæ¥å®šä¹‰å›¾åƒçº§ç›¸ä¼¼æ€§æ’åºçš„å¤šç§ç²’åº¦ï¼Œå¹¶å…¨è‡ªåŠ¨æ‰§è¡Œã€‚</li>
<li>æ„å»ºäº†ä¸¤ä¸ªå…¨é¢çš„åŒ»å­¦å›¾åƒæ£€ç´¢æ•°æ®é›†ï¼šMIMIC-IRå’ŒCTRATE-IRï¼Œä¸ºåŸºäºä¸åŒè§£å‰–ç»“æ„çš„å›¾åƒæä¾›è¯¦ç»†æ’åæ³¨é‡Šã€‚</li>
<li>å¼€å‘äº†ä¸¤ä¸ªé«˜æ€§èƒ½çš„åŒ»å­¦å›¾åƒæ£€ç´¢ç³»ç»Ÿï¼šRadIR-CXRå’Œmodel-ChestCTã€‚</li>
<li>è¿™äº›ç³»ç»Ÿå¯ä»¥çµæ´»æœ‰æ•ˆåœ°æ ¹æ®æ–‡æœ¬æè¿°çš„ç‰¹å®šè§£å‰–ç»“æ„è¿›è¡Œå›¾åƒæ£€ç´¢ã€‚</li>
<li>åœ¨å¤§éƒ¨åˆ†è¯„ä¼°æŒ‡æ ‡ä¸Šï¼Œè¿™äº›ç³»ç»Ÿè¾¾åˆ°äº†æœ€å…ˆè¿›çš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04653">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-25a4bb9994b7c0940a51bae0f3a42015.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f224d9a4bf8b6bb121438fb713a6fb68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d102e426dec969321b536faa30f3d3d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5318e60868c7b8569cd9479e4966cf73.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Adaptive-Prototype-Learning-for-Multimodal-Cancer-Survival-Analysis"><a href="#Adaptive-Prototype-Learning-for-Multimodal-Cancer-Survival-Analysis" class="headerlink" title="Adaptive Prototype Learning for Multimodal Cancer Survival Analysis"></a>Adaptive Prototype Learning for Multimodal Cancer Survival Analysis</h2><p><strong>Authors:Hong Liu, Haosen Yang, Federica Eduati, Josien P. W. Pluim, Mitko Veta</strong></p>
<p>Leveraging multimodal data, particularly the integration of whole-slide histology images (WSIs) and transcriptomic profiles, holds great promise for improving cancer survival prediction. However, excessive redundancy in multimodal data can degrade model performance. In this paper, we propose Adaptive Prototype Learning (APL), a novel and effective approach for multimodal cancer survival analysis. APL adaptively learns representative prototypes in a data-driven manner, reducing redundancy while preserving critical information. Our method employs two sets of learnable query vectors that serve as a bridge between high-dimensional representations and survival prediction, capturing task-relevant features. Additionally, we introduce a multimodal mixed self-attention mechanism to enable cross-modal interactions, further enhancing information fusion. Extensive experiments on five benchmark cancer datasets demonstrate the superiority of our approach over existing methods. The code is available at <a target="_blank" rel="noopener" href="https://github.com/HongLiuuuuu/APL">https://github.com/HongLiuuuuu/APL</a>. </p>
<blockquote>
<p>åˆ©ç”¨å¤šæ¨¡æ€æ•°æ®ï¼Œç‰¹åˆ«æ˜¯æ•´åˆå…¨åˆ‡ç‰‡ç»„ç»‡å›¾åƒï¼ˆWSIsï¼‰å’Œè½¬å½•ç»„å›¾è°±ï¼Œåœ¨ç™Œç—‡ç”Ÿå­˜é¢„æµ‹æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¤šæ¨¡æ€æ•°æ®ä¸­çš„è¿‡åº¦å†—ä½™å¯èƒ½ä¼šé™ä½æ¨¡å‹æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”åŸå‹å­¦ä¹ ï¼ˆAPLï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå¤šæ¨¡æ€ç™Œç—‡ç”Ÿå­˜åˆ†æçš„æ–°å‹æœ‰æ•ˆæ–¹æ³•ã€‚APLä»¥æ•°æ®é©±åŠ¨çš„æ–¹å¼è‡ªé€‚åº”åœ°å­¦ä¹ ä»£è¡¨æ€§åŸå‹ï¼Œå‡å°‘å†—ä½™çš„åŒæ—¶ä¿ç•™å…³é”®ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä¸¤ç»„å¯å­¦ä¹ çš„æŸ¥è¯¢å‘é‡ï¼Œä½œä¸ºé«˜ç»´è¡¨ç¤ºå’Œç”Ÿå­˜é¢„æµ‹ä¹‹é—´çš„æ¡¥æ¢ï¼Œæ•æ‰ä»»åŠ¡ç›¸å…³ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šæ¨¡æ€æ··åˆè‡ªæ³¨æ„æœºåˆ¶ï¼Œä»¥å®ç°è·¨æ¨¡æ€äº¤äº’ï¼Œè¿›ä¸€æ­¥å¢å¼ºä¿¡æ¯èåˆã€‚åœ¨äº”ä¸ªåŸºå‡†ç™Œç—‡æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HongLiuuuuu/APL">https://github.com/HongLiuuuuu/APL</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04643v1">PDF</a> 10 pages, 3 figures</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé¢†åŸŸç ”ç©¶äººå‘˜é€šè¿‡åˆ©ç”¨å¤šæ¨¡æ€æ•°æ®ï¼Œç‰¹åˆ«æ˜¯æ•´åˆå…¨åˆ‡ç‰‡ç»„ç»‡å›¾åƒï¼ˆWSIsï¼‰å’Œè½¬å½•ç»„å›¾è°±ï¼Œä¸ºæ”¹å–„ç™Œç—‡ç”Ÿå­˜é¢„æµ‹å¸¦æ¥äº†å¸Œæœ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è‡ªé€‚åº”åŸå‹å­¦ä¹ ï¼ˆAPLï¼‰çš„æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šæ¨¡æ€ç™Œç—‡ç”Ÿå­˜åˆ†æä¸­çš„æ•°æ®å†—ä½™é—®é¢˜ï¼Œè‡ªé€‚åº”åœ°å­¦ä¹ ä»£è¡¨æ€§åŸå‹å¹¶ä¿ç•™å…³é”®ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†è·¨æ¨¡æ€äº¤äº’çš„è‡ªæ³¨æ„æœºåˆ¶å¢å¼ºä¿¡æ¯èåˆèƒ½åŠ›ã€‚ä»£ç å·²åœ¨GitHubä¸Šå‘å¸ƒã€‚è¯¥æ–¹æ³•å·²åœ¨äº”ä¸ªç™Œç—‡æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒéªŒè¯ã€‚é€šè¿‡å¤šç»´æ¯”è¾ƒè¯æ˜äº†è¯¥æ–¹æ³•å…·æœ‰ä¼˜å¼‚çš„é¢„æµ‹æ•ˆèƒ½å’Œé€šç”¨æ€§ã€‚é‡‡ç”¨å¤šä»»åŠ¡è”åˆå­¦ä¹ å¢å¼ºç‰¹å¾é€‰æ‹©èƒ½åŠ›çš„åç»­å·¥ä½œå€¼å¾—å…³æ³¨ã€‚ä¸åŒåŒ»å­¦åˆ†æ”¯ç–¾ç—…è¯Šç–—ä½“ç³»çš„ç›¸ä¼¼æ€§å’Œå·®å¼‚éœ€ç»¼åˆè€ƒè™‘ï¼Œä»¥å®ç°æ›´å¥½çš„ç–¾ç—…è¯Šç–—èåˆç ”ç©¶ã€‚å¯¹äºèåˆåŒ»å­¦ä¸­çš„ä¸åŒä¸“ä¸šæ–¹å‘ç ”ç©¶æ–¹æ³•å’Œç­–ç•¥ä¹Ÿæœ‰å¾…è¿›ä¸€æ­¥æ¢ç´¢å’Œæ”¹è¿›ã€‚æœªæ¥çš„ç ”ç©¶éœ€è¦è¿›ä¸€æ­¥åŠ å¼ºä¸åŒç–¾ç—…é¢†åŸŸä¹‹é—´çš„äº¤æµå’Œåˆä½œï¼Œä»¥ä¿ƒè¿›åŒ»å­¦çš„å…¨é¢å‘å±•ã€‚åŸºäºèåˆåŒ»å­¦ç†å¿µçš„äº¤å‰å­¦ç§‘ç ”ç©¶ï¼Œå¯¹ç–¾ç—…çš„ç»¼åˆè¯Šç–—æ–¹æ¡ˆæœ‰ç€å·¨å¤§æ½œåŠ›ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œå€ŸåŠ©å¤§æ•°æ®ä¸äººå·¥æ™ºèƒ½çš„åŠ æŒæ¨è¿›ç›¸å…³ç ”ç©¶æ›´å…·é‡è¦æ„ä¹‰ã€‚æœ¬æ–‡ä¸»è¦è®¨è®ºäº†ç™Œç—‡ç”Ÿå­˜é¢„æµ‹æŠ€æœ¯çš„ç°çŠ¶å’Œæœªæ¥å‘å±•å‰æ™¯ä»¥åŠæ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä»‹ç»äº†è‡ªå·±çš„åˆ›æ–°æ€§è§£å†³æ–¹æ¡ˆä»¥åŠå®è·µä¸­çš„ä¼˜ç‚¹å’ŒæŒ‘æˆ˜æ€§ç‰¹ç‚¹ã€‚<strong>Key Takeaways</strong>:</p>
<ol>
<li>åˆ©ç”¨å¤šæ¨¡æ€æ•°æ®ï¼ˆå…¨åˆ‡ç‰‡ç»„ç»‡å›¾åƒå’Œè½¬å½•ç»„å›¾è°±ï¼‰è¿›è¡Œç™Œç—‡ç”Ÿå­˜é¢„æµ‹å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>è‡ªé€‚åº”åŸå‹å­¦ä¹ ï¼ˆAPLï¼‰æ–¹æ³•èƒ½æœ‰æ•ˆå¤„ç†å¤šæ¨¡æ€æ•°æ®ä¸­çš„å†—ä½™ä¿¡æ¯ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>APLæ–¹æ³•å¼•å…¥è·¨æ¨¡æ€äº¤äº’çš„è‡ªæ³¨æ„æœºåˆ¶ä»¥å¢å¼ºä¿¡æ¯èåˆèƒ½åŠ›ã€‚</li>
<li>å®éªŒéªŒè¯æ˜¾ç¤ºAPLæ–¹æ³•åœ¨äº”ä¸ªç™Œç—‡æ•°æ®é›†ä¸Šå…·æœ‰ä¼˜å¼‚æ€§èƒ½ã€‚</li>
<li>æœªæ¥ç ”ç©¶æ–¹å‘åŒ…æ‹¬å¤šä»»åŠ¡è”åˆå­¦ä¹ å¢å¼ºç‰¹å¾é€‰æ‹©èƒ½åŠ›ã€ä¸åŒåŒ»å­¦åˆ†æ”¯çš„ç–¾ç—…è¯Šç–—ä½“ç³»èåˆç ”ç©¶ç­‰ã€‚</li>
<li>èåˆåŒ»å­¦ç†å¿µä¸‹çš„äº¤å‰å­¦ç§‘ç ”ç©¶å¯¹ç–¾ç—…ç»¼åˆè¯Šç–—æ–¹æ¡ˆå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚å¤§æ•°æ®å’Œäººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨è¯¥é¢†åŸŸçš„åº”ç”¨å—åˆ°é‡è§†ã€‚å½“å‰å·¥ä½œä¸»è¦è®¨è®ºç™Œç—‡ç”Ÿå­˜é¢„æµ‹æŠ€æœ¯çš„æŒ‘æˆ˜å’Œæœªæ¥å‘å±•è¶‹åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04643">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-de552bc34893907d1e9500a7bc2f586a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f57bae72938ff745de4b1071cf59aebb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dd913f1cdffe2a4003bec1ec73e4762.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f9dcc190fb2b938eb0a2820332f01d56.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Enhancing-SAM-with-Efficient-Prompting-and-Preference-Optimization-for-Semi-supervised-Medical-Image-Segmentation"><a href="#Enhancing-SAM-with-Efficient-Prompting-and-Preference-Optimization-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="Enhancing SAM with Efficient Prompting and Preference Optimization for   Semi-supervised Medical Image Segmentation"></a>Enhancing SAM with Efficient Prompting and Preference Optimization for   Semi-supervised Medical Image Segmentation</h2><p><strong>Authors:Aishik Konwer, Zhijian Yang, Erhan Bas, Cao Xiao, Prateek Prasanna, Parminder Bhatia, Taha Kass-Hout</strong></p>
<p>Foundational models such as the Segment Anything Model (SAM) are gaining traction in medical imaging segmentation, supporting multiple downstream tasks. However, such models are supervised in nature, still relying on large annotated datasets or prompts supplied by experts. Conventional techniques such as active learning to alleviate such limitations are limited in scope and still necessitate continuous human involvement and complex domain knowledge for label refinement or establishing reward ground truth. To address these challenges, we propose an enhanced Segment Anything Model (SAM) framework that utilizes annotation-efficient prompts generated in a fully unsupervised fashion, while still capturing essential semantic, location, and shape information through contrastive language-image pretraining and visual question answering. We adopt the direct preference optimization technique to design an optimal policy that enables the model to generate high-fidelity segmentations with simple ratings or rankings provided by a virtual annotator simulating the human annotation process. State-of-the-art performance of our framework in tasks such as lung segmentation, breast tumor segmentation, and organ segmentation across various modalities, including X-ray, ultrasound, and abdominal CT, justifies its effectiveness in low-annotation data scenarios. </p>
<blockquote>
<p>åƒSegment Anything Modelï¼ˆSAMï¼‰è¿™æ ·çš„åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œæ”¯æŒå¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œæ­¤ç±»æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯ç›‘ç£çš„ï¼Œä»ç„¶ä¾èµ–äºä¸“å®¶æä¾›çš„å¤§é‡æ ‡æ³¨æ•°æ®é›†æˆ–æç¤ºã€‚ä¸ºäº†ç¼“è§£è¿™äº›é™åˆ¶ï¼Œé‡‡ç”¨ä¸»åŠ¨å­¦ä¹ çš„ä¼ ç»ŸæŠ€æœ¯èŒƒå›´æœ‰é™ï¼Œä»éœ€è¦æŒç»­çš„äººåŠ›æŠ•å…¥å’Œå¤æ‚çš„é¢†åŸŸçŸ¥è¯†æ¥è¿›è¡Œæ ‡ç­¾ç»†åŒ–æˆ–å»ºç«‹å¥–åŠ±åŸºå‡†çœŸå®å€¼ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºçš„Segment Anything Modelï¼ˆSAMï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å®Œå…¨æ— ç›‘ç£ç”Ÿæˆçš„æ ‡æ³¨æœ‰æ•ˆæç¤ºï¼ŒåŒæ—¶é€šè¿‡å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒå’Œè§†è§‰é—®ç­”æ•è·å…³é”®çš„è¯­ä¹‰ã€ä½ç½®å’Œå½¢çŠ¶ä¿¡æ¯ã€‚æˆ‘ä»¬é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–æŠ€æœ¯æ¥è®¾è®¡æœ€ä½³ç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå€ŸåŠ©æ¨¡æ‹Ÿäººç±»æ³¨é‡Šè¿‡ç¨‹çš„è™šæ‹Ÿæ³¨é‡Šå™¨æä¾›çš„ç®€å•è¯„åˆ†æˆ–æ’åç”Ÿæˆé«˜ä¿çœŸåˆ†å‰²ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨è‚ºåˆ†å‰²ã€ä¹³è…ºè‚¿ç˜¤åˆ†å‰²å’Œå™¨å®˜åˆ†å‰²ç­‰ä»»åŠ¡ä¸­çš„æœ€æ–°æ€§èƒ½è¡¨ç°ï¼Œä»¥åŠåœ¨Xå°„çº¿ã€è¶…å£°å’Œè…¹éƒ¨CTç­‰å„ç§æ¨¡æ€çš„åº”ç”¨ï¼Œè¯æ˜äº†å®ƒåœ¨ä½æ ‡æ³¨æ•°æ®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04639v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºSegment Anything Modelï¼ˆSAMï¼‰çš„åŸºç¡€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­é€æ¸å—åˆ°é‡è§†ï¼Œæ”¯æŒå¤šä»»åŠ¡å¤„ç†ã€‚ç„¶è€Œï¼Œæ­¤ç±»æ¨¡å‹ä»ä¾èµ–äºå¤§é‡æ ‡æ³¨æ•°æ®é›†æˆ–ä¸“å®¶æä¾›çš„æç¤ºï¼Œå­˜åœ¨ç›‘ç£å­¦ä¹ çš„å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºçš„SAMæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»¥å®Œå…¨æ— ç›‘ç£çš„æ–¹å¼ç”Ÿæˆæ ‡æ³¨æœ‰æ•ˆçš„æç¤ºï¼ŒåŒæ—¶ä»èƒ½é€šè¿‡å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒå’Œè§†è§‰é—®ç­”æ•è·å…³é”®çš„è¯­ä¹‰ã€ä½ç½®å’Œå½¢çŠ¶ä¿¡æ¯ã€‚é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–æŠ€æœ¯è®¾è®¡æœ€ä½³ç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå€ŸåŠ©æ¨¡æ‹Ÿäººç±»æ³¨é‡Šè¿‡ç¨‹çš„è™šæ‹Ÿæ³¨é‡Šå™¨æä¾›çš„ç®€å•è¯„åˆ†æˆ–æ’åç”Ÿæˆé«˜ä¿çœŸåˆ†å‰²ã€‚åœ¨è‚ºåˆ†å‰²ã€ä¹³è…ºè‚¿ç˜¤åˆ†å‰²å’Œå™¨å®˜åˆ†å‰²ç­‰ä»»åŠ¡ä¸­ï¼Œè¯¥æ¡†æ¶çš„å…ˆè¿›æ€§èƒ½è¯æ˜äº†å…¶åœ¨ä½æ ‡æ³¨æ•°æ®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Segment Anything Model (SAM) åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å—åˆ°é‡è§†ï¼Œæ”¯æŒå¤šä»»åŠ¡å¤„ç†ã€‚</li>
<li>SAMä»ä¾èµ–äºå¤§é‡æ ‡æ³¨æ•°æ®å’Œä¸“å®¶æç¤ºï¼Œå­˜åœ¨ç›‘ç£å­¦ä¹ çš„å±€é™æ€§ã€‚</li>
<li>æå‡ºçš„å¢å¼ºSAMæ¡†æ¶ä»¥å®Œå…¨æ— ç›‘ç£çš„æ–¹å¼ç”Ÿæˆæ ‡æ³¨æœ‰æ•ˆçš„æç¤ºã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒå’Œè§†è§‰é—®ç­”æ•è·å…³é”®çš„è¯­ä¹‰ã€ä½ç½®å’Œå½¢çŠ¶ä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨ç›´æ¥åå¥½ä¼˜åŒ–æŠ€æœ¯è®¾è®¡æœ€ä½³ç­–ç•¥ï¼Œç”Ÿæˆé«˜ä¿çœŸåˆ†å‰²ã€‚</li>
<li>æ¡†æ¶åœ¨è‚ºåˆ†å‰²ã€ä¹³è…ºè‚¿ç˜¤åˆ†å‰²å’Œå™¨å®˜åˆ†å‰²ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºå…ˆè¿›æ€§èƒ½ã€‚</li>
<li>æ¡†æ¶åœ¨ä½æ ‡æ³¨æ•°æ®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å¾—åˆ°è¯æ˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04639">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-948d1250612ec8d4f62580fdc376390f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81c346094d28e8e8e81faeb17f7ac82d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d3f8857441271c8dc2beabc0fce351c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="PathoPainter-Augmenting-Histopathology-Segmentation-via-Tumor-aware-Inpainting"><a href="#PathoPainter-Augmenting-Histopathology-Segmentation-via-Tumor-aware-Inpainting" class="headerlink" title="PathoPainter: Augmenting Histopathology Segmentation via Tumor-aware   Inpainting"></a>PathoPainter: Augmenting Histopathology Segmentation via Tumor-aware   Inpainting</h2><p><strong>Authors:Hong Liu, Haosen Yang, Evi M. C. Huijben, Mark Schuiveling, Ruisheng Su, Josien P. W. Pluim, Mitko Veta</strong></p>
<p>Tumor segmentation plays a critical role in histopathology, but it requires costly, fine-grained image-mask pairs annotated by pathologists. Thus, synthesizing histopathology data to expand the dataset is highly desirable. Previous works suffer from inaccuracies and limited diversity in image-mask pairs, both of which affect training segmentation, particularly in small-scale datasets and the inherently complex nature of histopathology images. To address this challenge, we propose PathoPainter, which reformulates image-mask pair generation as a tumor inpainting task. Specifically, our approach preserves the background while inpainting the tumor region, ensuring precise alignment between the generated image and its corresponding mask. To enhance dataset diversity while maintaining biological plausibility, we incorporate a sampling mechanism that conditions tumor inpainting on regional embeddings from a different image. Additionally, we introduce a filtering strategy to exclude uncertain synthetic regions, further improving the quality of the generated data. Our comprehensive evaluation spans multiple datasets featuring diverse tumor types and various training data scales. As a result, segmentation improved significantly with our synthetic data, surpassing existing segmentation data synthesis approaches, e.g., 75.69% -&gt; 77.69% on CAMELYON16. The code is available at <a target="_blank" rel="noopener" href="https://github.com/HongLiuuuuu/PathoPainter">https://github.com/HongLiuuuuu/PathoPainter</a>. </p>
<blockquote>
<p>è‚¿ç˜¤åˆ†å‰²åœ¨ç—…ç†å­¦ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œä½†å®ƒéœ€è¦ç—…ç†å­¦å®¶æ ‡æ³¨çš„æ˜‚è´µã€ç²¾ç»†çš„å›¾åƒ-æ©è†œå¯¹ã€‚å› æ­¤ï¼Œåˆæˆç—…ç†æ•°æ®ä»¥æ‰©å¤§æ•°æ®é›†æ˜¯éå¸¸ç†æƒ³çš„ã€‚ä¹‹å‰çš„å·¥ä½œå­˜åœ¨å›¾åƒ-æ©è†œå¯¹çš„ä¸å‡†ç¡®æ€§å’Œå¤šæ ·æ€§æœ‰é™çš„é—®é¢˜ï¼Œè¿™ä¸¤è€…éƒ½ä¼šå½±å“åˆ†å‰²è®­ç»ƒï¼Œç‰¹åˆ«æ˜¯åœ¨å°è§„æ¨¡æ•°æ®é›†å’Œç—…ç†å›¾åƒå›ºæœ‰çš„å¤æ‚æ€§æ–¹é¢ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†PathoPainterï¼Œå®ƒå°†å›¾åƒ-æ©è†œå¯¹çš„ç”Ÿæˆé‡æ–°åˆ¶å®šä¸ºä¸€ä¸ªè‚¿ç˜¤ä¿®å¤ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿®å¤è‚¿ç˜¤åŒºåŸŸçš„åŒæ—¶ä¿ç•™èƒŒæ™¯ï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒä¸å…¶å¯¹åº”çš„æ©è†œä¹‹é—´ç²¾ç¡®å¯¹é½ã€‚ä¸ºäº†åœ¨ä¿æŒç”Ÿç‰©åˆç†æ€§çš„åŒæ—¶æé«˜æ•°æ®é›†çš„å¤šæ ·æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é‡‡æ ·æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ ¹æ®æ¥è‡ªä¸åŒå›¾åƒçš„å±€éƒ¨åµŒå…¥æ¥è¿›è¡Œè‚¿ç˜¤ä¿®å¤ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨äº†ä¸€ç§è¿‡æ»¤ç­–ç•¥ï¼Œä»¥æ’é™¤ä¸ç¡®å®šçš„åˆæˆåŒºåŸŸï¼Œè¿›ä¸€æ­¥æé«˜ç”Ÿæˆæ•°æ®çš„è´¨é‡ã€‚æˆ‘ä»¬çš„ç»¼åˆè¯„ä¼°æ¶µç›–äº†å¤šä¸ªåŒ…å«å„ç§è‚¿ç˜¤ç±»å‹å’Œä¸åŒè®­ç»ƒæ•°æ®è§„æ¨¡çš„æ•°æ®é›†ã€‚å› æ­¤ï¼Œä½¿ç”¨æˆ‘ä»¬çš„åˆæˆæ•°æ®ï¼Œåˆ†å‰²æ•ˆæœå¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œè¶…è¶Šäº†ç°æœ‰çš„åˆ†å‰²æ•°æ®åˆæˆæ–¹æ³•ï¼Œä¾‹å¦‚åœ¨CAMELYON16ä¸Šçš„æˆç»©ä»75.69%æé«˜åˆ°äº†77.69%ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/HongLiuuuuu/PathoPainter%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/HongLiuuuuu/PathoPainterè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04634v1">PDF</a> 10 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè‚¿ç˜¤ä¿®å¤æŠ€æœ¯ï¼ˆPathoPainterï¼‰çš„æ•°æ®é›†åˆæˆæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è‚¿ç˜¤åˆ†å‰²é¢†åŸŸä¸­å¯¹ç—…ç†å­¦ä¸“å®¶ç²¾ç»†æ ‡æ³¨çš„å›¾åƒ-æ©è†œå¯¹æ•°æ®çš„ä¾èµ–é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡è‚¿ç˜¤åŒºåŸŸä¿®å¤æŠ€æœ¯ç”Ÿæˆå›¾åƒå’Œæ©è†œï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒä¸æ©è†œç²¾ç¡®å¯¹é½ï¼ŒåŒæ—¶å¼•å…¥é‡‡æ ·æœºåˆ¶å¢åŠ æ•°æ®é›†å¤šæ ·æ€§å¹¶ä¿æŒç”Ÿç‰©å­¦åˆç†æ€§ã€‚é€šè¿‡è¿‡æ»¤ä¸ç¡®å®šçš„åˆæˆåˆ†å‰²åŒºåŸŸï¼Œæé«˜ç”Ÿæˆæ•°æ®è´¨é‡ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä½¿ç”¨åˆæˆæ•°æ®å¯æ˜¾è‘—æé«˜åˆ†å‰²æ€§èƒ½ï¼Œè¶…è¶Šç°æœ‰åˆ†å‰²æ•°æ®åˆæˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PathoPainteræ˜¯ä¸€ç§ç”¨äºåˆæˆè‚¿ç˜¤åˆ†å‰²æ•°æ®é›†çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ•°æ®è·å–æˆæœ¬é«˜çš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡è‚¿ç˜¤ä¿®å¤æŠ€æœ¯ç”Ÿæˆå›¾åƒå’Œæ©è†œï¼Œç¡®ä¿ç²¾ç¡®å¯¹é½ã€‚</li>
<li>é‡‡ç”¨é‡‡æ ·æœºåˆ¶å¢åŠ æ•°æ®é›†å¤šæ ·æ€§å¹¶ç»´æŒç”Ÿç‰©å­¦åˆç†æ€§ã€‚</li>
<li>å¼•å…¥è¿‡æ»¤ç­–ç•¥ä»¥æ’é™¤ä¸ç¡®å®šçš„åˆæˆåˆ†å‰²åŒºåŸŸã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä½¿ç”¨PathoPainteråˆæˆçš„æ•°æ®èƒ½æ˜¾è‘—æé«˜åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>è¯¥æ–¹æ³•æ€§èƒ½ä¼˜äºç°æœ‰çš„åˆ†å‰²æ•°æ®åˆæˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04634">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ccb4446698d15c64f82160da280b57be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc7c3214532b2d5bde5942a8396e4364.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85dbfd7c7891b121bd5562e03d9959b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3652baf2602ad177fddf7c4a195d2167.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Maestro-A-302-GFLOPS-W-and-19-8GFLOPS-RISC-V-Vector-Tensor-Architecture-for-Wearable-Ultrasound-Edge-Computing"><a href="#Maestro-A-302-GFLOPS-W-and-19-8GFLOPS-RISC-V-Vector-Tensor-Architecture-for-Wearable-Ultrasound-Edge-Computing" class="headerlink" title="Maestro: A 302 GFLOPS&#x2F;W and 19.8GFLOPS RISC-V Vector-Tensor Architecture   for Wearable Ultrasound Edge Computing"></a>Maestro: A 302 GFLOPS&#x2F;W and 19.8GFLOPS RISC-V Vector-Tensor Architecture   for Wearable Ultrasound Edge Computing</h2><p><strong>Authors:Mattia Sinigaglia, Amirhossein Kiamarzi, Marco Bertuletti, Luigi Ghionda, Mattia Orlandi, Riccardo Tedeschi, Aurora Di Giampietro, Yvan Tortorella, Luca Bertaccini, Simone Benatti, Giuseppe Tagliavini, Luca Benini, Francesco Conti, Davide Rossi</strong></p>
<p>Most Wearable Ultrasound (WUS) devices lack the computational power to process signals at the edge, instead relying on remote offload, which introduces latency, high power consumption, and privacy concerns. We present Maestro, a RISC-V SoC with unified Vector-Tensor Unit (VTU) and memory-coupled Fast Fourier Transform (FFT) accelerators targeting edge processing for wearable ultrasound devices, fabricated using low-cost TSMC 65nm CMOS technology. The VTU achieves peak 302GFLOPS&#x2F;W and 19.8GFLOPS at FP16, while the multi-precision 16&#x2F;32-bit floating-point FFT accelerator delivers peak 60.6GFLOPS&#x2F;W and 3.6GFLOPS at FP16, We evaluate Maestro on a US-based gesture recognition task, achieving 1.62GFLOPS in signal processing at 26.68GFLOPS&#x2F;W, and 19.52GFLOPS in Convolutional Neural Network (CNN) workloads at 298.03GFLOPS&#x2F;W. Compared to a state-of-the-art SoC with a similar mission profile, Maestro achieves a 5x speedup while consuming only 12mW, with an energy consumption of 2.5mJ in a wearable US channel preprocessing and ML-based postprocessing pipeline. </p>
<blockquote>
<p>å¤§å¤šæ•°å¯ç©¿æˆ´è¶…å£°ï¼ˆWUSï¼‰è®¾å¤‡ç¼ºä¹åœ¨è¾¹ç¼˜å¤„ç†ä¿¡å·çš„è®¡ç®—èƒ½åŠ›ï¼Œè€Œæ˜¯ä¾èµ–äºè¿œç¨‹å¸è½½ï¼Œè¿™å¼•å…¥äº†å»¶è¿Ÿã€é«˜åŠŸè€—å’Œéšç§æ‹…å¿§ã€‚æˆ‘ä»¬æ¨å‡ºäº†Maestroï¼Œè¿™æ˜¯ä¸€æ¬¾é’ˆå¯¹å¯ç©¿æˆ´è¶…å£°è®¾å¤‡çš„è¾¹ç¼˜å¤„ç†è€Œè®¾è®¡çš„RISC-Vç³»ç»ŸèŠ¯ç‰‡ï¼ˆSoCï¼‰ï¼Œé…å¤‡äº†ç»Ÿä¸€çš„å‘é‡å¼ é‡å•å…ƒï¼ˆVTUï¼‰å’Œå†…å­˜è€¦åˆå¿«é€Ÿå‚…é‡Œå¶å˜æ¢ï¼ˆFFTï¼‰åŠ é€Ÿå™¨ï¼Œé‡‡ç”¨ä½æˆæœ¬TSMC 65nm CMOSæŠ€æœ¯åˆ¶é€ ã€‚VTUåœ¨FP16ä¸‹è¾¾åˆ°å³°å€¼302GFLOPS&#x2F;Wå’Œ19.8GFLOPSï¼Œè€Œå¤šç²¾åº¦16&#x2F;32ä½æµ®ç‚¹FFTåŠ é€Ÿå™¨åœ¨FP16ä¸‹è¾¾åˆ°å³°å€¼60.6GFLOPS&#x2F;Wå’Œ3.6GFLOPSã€‚æˆ‘ä»¬è¯„ä¼°äº†Maestroåœ¨åŸºäºç¾å›½çš„å§¿æ€è¯†åˆ«ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œåœ¨ä¿¡å·å¤„ç†çš„GFLOPSè¾¾åˆ°26.68æ—¶å®ç°äº†1.62GFLOPSçš„å¤„ç†é€Ÿåº¦ï¼Œåœ¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å·¥ä½œè´Ÿè½½çš„GFLOPSè¾¾åˆ°298.03æ—¶å®ç°äº†19.52GFLOPSçš„å¤„ç†é€Ÿåº¦ã€‚ä¸å…·æœ‰ç±»ä¼¼ä»»åŠ¡é…ç½®çš„æœ€æ–°SoCç›¸æ¯”ï¼ŒMaestroå®ç°äº†5å€çš„åŠ é€Ÿï¼Œä»…æ¶ˆè€—12mWçš„åŠŸè€—ï¼Œåœ¨å¯ç©¿æˆ´è¶…å£°é€šé“é¢„å¤„ç†å’ŒåŸºäºæœºå™¨å­¦ä¹ çš„åå¤„ç†ç®¡é“ä¸­çš„èƒ½è€—ä¸º2.5mJã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04581v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§æ–°å‹çš„ç©¿æˆ´å¼è¶…å£°å¤„ç†ç³»ç»ŸMaestroè¢«ç ”å‘å‡ºæ¥ï¼Œå®ƒå…·å¤‡è¾¹ç¼˜å¤„ç†èƒ½åŠ›ï¼Œè§£å†³äº†ç°æœ‰ç©¿æˆ´å¼è¶…å£°è®¾å¤‡ä¾èµ–è¿œç¨‹å¤„ç†å¯¼è‡´çš„å»¶è¿Ÿã€é«˜åŠŸè€—å’Œéšç§ç­‰é—®é¢˜ã€‚Maestroé‡‡ç”¨RISC-V SoCï¼Œé…å¤‡äº†ç»Ÿä¸€çš„å‘é‡å¼ é‡å•å…ƒï¼ˆVTUï¼‰å’Œå†…å­˜è€¦åˆå¿«é€Ÿå‚…é‡Œå¶å˜æ¢ï¼ˆFFTï¼‰åŠ é€Ÿå™¨ï¼Œå®ç°äº†ä½åŠŸè€—å¤„ç†ã€‚å…¶åœ¨æ‰‹åŠ¿è¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸æ¯”ç°æœ‰æœ€å…ˆè¿›çš„ç±»ä¼¼ä»»åŠ¡SoCï¼ŒMaestroå®ç°äº†5å€çš„åŠ é€Ÿï¼ŒåŒæ—¶åŠŸè€—ä»…ä¸º12mWã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Maestroæ˜¯ä¸€ç§æ–°å‹çš„ç©¿æˆ´å¼è¶…å£°å¤„ç†ç³»ç»Ÿï¼Œå…·å¤‡è¾¹ç¼˜å¤„ç†èƒ½åŠ›ã€‚</li>
<li>Maestroè§£å†³äº†ç°æœ‰ç©¿æˆ´å¼è¶…å£°è®¾å¤‡ä¾èµ–è¿œç¨‹å¤„ç†çš„é—®é¢˜ï¼Œå‡å°‘äº†å»¶è¿Ÿã€é«˜åŠŸè€—å’Œéšç§æ‹…å¿§ã€‚</li>
<li>Maestroé‡‡ç”¨RISC-V SoCï¼Œé…å¤‡äº†VTUå’ŒFFTåŠ é€Ÿå™¨ï¼Œå®ç°äº†ä½åŠŸè€—å¤„ç†ã€‚</li>
<li>VTUå®ç°äº†å³°å€¼302GFLOPS&#x2F;Wå’Œ19.8GFLOPSçš„è®¡ç®—èƒ½åŠ›ã€‚</li>
<li>FFTåŠ é€Ÿå™¨æä¾›äº†å¤šç²¾åº¦è®¡ç®—ï¼Œå³°å€¼è¾¾åˆ°60.6GFLOPS&#x2F;Wã€‚</li>
<li>åœ¨æ‰‹åŠ¿è¯†åˆ«ä»»åŠ¡ä¸­ï¼ŒMaestroç›¸æ¯”ç°æœ‰æœ€å…ˆè¿›çš„SoCå®ç°äº†5å€çš„åŠ é€Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c118c454c7f69c04d5553a5723127fae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55ec51910c5dffab79c932a9afcb3f97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f33abdea64d4cb4d4fb9ff5158afa83.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3a9b49e976151a68a94724e9a38c0072.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="In-Context-Reverse-Classification-Accuracy-Efficient-Estimation-of-Segmentation-Quality-without-Ground-Truth"><a href="#In-Context-Reverse-Classification-Accuracy-Efficient-Estimation-of-Segmentation-Quality-without-Ground-Truth" class="headerlink" title="In-Context Reverse Classification Accuracy: Efficient Estimation of   Segmentation Quality without Ground-Truth"></a>In-Context Reverse Classification Accuracy: Efficient Estimation of   Segmentation Quality without Ground-Truth</h2><p><strong>Authors:Matias Cosarinsky, Ramiro Billot, Lucas Mansilla, Gabriel Gimenez, Nicolas GaggiÃ³n, Guanghui Fu, Enzo Ferrante</strong></p>
<p>Assessing the quality of automatic image segmentation is crucial in clinical practice, but often very challenging due to the limited availability of ground truth annotations. In this paper, we introduce In-Context Reverse Classification Accuracy (In-Context RCA), a novel framework for automatically estimating segmentation quality in the absence of ground-truth annotations. By leveraging recent in-context learning segmentation models and incorporating retrieval-augmentation techniques to select the most relevant reference images, our approach enables efficient quality estimation with minimal reference data. Validated across diverse medical imaging modalities, our method demonstrates robust performance and computational efficiency, offering a promising solution for automated quality control in clinical workflows, where fast and reliable segmentation assessment is essential. The code is available at <a target="_blank" rel="noopener" href="https://github.com/mcosarinsky/In-Context-RCA">https://github.com/mcosarinsky/In-Context-RCA</a>. </p>
<blockquote>
<p>åœ¨åŒ»ç–—å®è·µä¸­ï¼Œè¯„ä¼°è‡ªåŠ¨å›¾åƒåˆ†å‰²çš„è´¨é‡è‡³å…³é‡è¦ï¼Œä½†ç”±äºçœŸå®æ ‡æ³¨æ•°æ®çš„æœ‰é™æ€§ï¼Œè¿™é€šå¸¸æå…·æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºä¸Šä¸‹æ–‡åå‘åˆ†ç±»å‡†ç¡®ç‡ï¼ˆIn-Context RCAï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œæ— éœ€çœŸå®æ ‡æ³¨æ•°æ®å³å¯è‡ªåŠ¨ä¼°è®¡åˆ†å‰²è´¨é‡ã€‚é€šè¿‡åˆ©ç”¨æœ€æ–°çš„ä¸Šä¸‹æ–‡å­¦ä¹ åˆ†å‰²æ¨¡å‹å’Œæ£€ç´¢å¢å¼ºæŠ€æœ¯æ¥é€‰æ‹©æœ€ç›¸å…³çš„å‚è€ƒå›¾åƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨æœ€å°‘çš„å‚è€ƒæ•°æ®ä¸‹å®ç°é«˜æ•ˆçš„è´¨é‡è¯„ä¼°ã€‚ç»è¿‡å¤šç§åŒ»å­¦å½±åƒæ¨¡æ€çš„éªŒè¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºäº†ç¨³å¥çš„æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ï¼Œä¸ºä¸´åºŠå·¥ä½œæµç¨‹ä¸­çš„è‡ªåŠ¨åŒ–è´¨é‡æ§åˆ¶æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨å¿«é€Ÿå¯é çš„åˆ†å‰²è¯„ä¼°è‡³å…³é‡è¦çš„é¢†åŸŸã€‚ç›¸å…³ä»£ç å¯è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/mcosarinsky/In-Context-RCA%E3%80%82">https://github.com/mcosarinsky/In-Context-RCAã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04522v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒè‡ªåŠ¨åˆ†å‰²è´¨é‡è¯„ä¼°åœ¨ä¸´åºŠå®è·µä¸­è‡³å…³é‡è¦ï¼Œä½†ç¼ºä¹çœŸå®æ ‡æ³¨å¯¼è‡´è¯„ä¼°å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹æ¡†æ¶In-Context Reverse Classification Accuracyï¼ˆIn-Context RCAï¼‰ï¼Œæ— éœ€çœŸå®æ ‡æ³¨å³å¯è‡ªåŠ¨ä¼°è®¡åˆ†å‰²è´¨é‡ã€‚è¯¥æ¡†æ¶ç»“åˆæœ€æ–°çš„ä¸Šä¸‹æ–‡å­¦ä¹ åˆ†å‰²æ¨¡å‹å’Œæ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼Œé€‰æ‹©æœ€ç›¸å…³çš„å‚è€ƒå›¾åƒï¼Œä»¥æœ€å°çš„å‚è€ƒæ•°æ®å®ç°é«˜æ•ˆçš„è´¨é‡è¯„ä¼°ã€‚ç»è¿‡å¤šç§åŒ»å­¦æˆåƒæ¨¡æ€çš„éªŒè¯ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ï¼Œä¸ºä¸´åºŠå·¥ä½œæµç¨‹ä¸­çš„è‡ªåŠ¨åŒ–è´¨é‡æ§åˆ¶æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œå°¤å…¶åœ¨éœ€è¦å¿«é€Ÿå¯é åˆ†å‰²è¯„ä¼°çš„æƒ…å†µä¸‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒè‡ªåŠ¨åˆ†å‰²è´¨é‡è¯„ä¼°çš„é‡è¦æ€§åŠæŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶In-Context RCAï¼Œæ— éœ€çœŸå®æ ‡æ³¨å³å¯è¯„ä¼°åˆ†å‰²è´¨é‡ã€‚</li>
<li>åˆ©ç”¨æœ€æ–°çš„ä¸Šä¸‹æ–‡å­¦ä¹ åˆ†å‰²æ¨¡å‹ã€‚</li>
<li>å¼•å…¥æ£€ç´¢å¢å¼ºæŠ€æœ¯ï¼Œé€‰æ‹©æœ€ç›¸å…³çš„å‚è€ƒå›¾åƒã€‚</li>
<li>æ–¹æ³•åœ¨å¤šç§åŒ»å­¦æˆåƒæ¨¡æ€ä¸‹è¡¨ç°å‡ºç¨³å¥æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚</li>
<li>å¯¹ä¸´åºŠå·¥ä½œæµç¨‹çš„è‡ªåŠ¨åŒ–è´¨é‡æ§åˆ¶å…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04522">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d5ea157a85e0c84e4e6ff0dcb3a6044.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6cf549f285c8c228a0ae93ce9677ac6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a401d6c006d09b9dbeb9e458a8f26a97.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Spatial-regularisation-for-improved-accuracy-and-interpretability-in-keypoint-based-registration"><a href="#Spatial-regularisation-for-improved-accuracy-and-interpretability-in-keypoint-based-registration" class="headerlink" title="Spatial regularisation for improved accuracy and interpretability in   keypoint-based registration"></a>Spatial regularisation for improved accuracy and interpretability in   keypoint-based registration</h2><p><strong>Authors:Benjamin Billot, Ramya Muthukrishnan, Esra Abaci-Turk, Ellen P. Grant, Nicholas Ayache, HervÃ© Delingette, Polina Golland</strong></p>
<p>Unsupervised registration strategies bypass requirements in ground truth transforms or segmentations by optimising similarity metrics between fixed and moved volumes. Among these methods, a recent subclass of approaches based on unsupervised keypoint detection stand out as very promising for interpretability. Specifically, these methods train a network to predict feature maps for fixed and moving images, from which explainable centres of mass are computed to obtain point clouds, that are then aligned in closed-form. However, the features returned by the network often yield spatially diffuse patterns that are hard to interpret, thus undermining the purpose of keypoint-based registration. Here, we propose a three-fold loss to regularise the spatial distribution of the features. First, we use the KL divergence to model features as point spread functions that we interpret as probabilistic keypoints. Then, we sharpen the spatial distributions of these features to increase the precision of the detected landmarks. Finally, we introduce a new repulsive loss across keypoints to encourage spatial diversity. Overall, our loss considerably improves the interpretability of the features, which now correspond to precise and anatomically meaningful landmarks. We demonstrate our three-fold loss in foetal rigid motion tracking and brain MRI affine registration tasks, where it not only outperforms state-of-the-art unsupervised strategies, but also bridges the gap with state-of-the-art supervised methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/BenBillot/spatial_regularisation">https://github.com/BenBillot/spatial_regularisation</a>. </p>
<blockquote>
<p>æ— ç›‘ç£çš„æ³¨å†Œç­–ç•¥é€šè¿‡ä¼˜åŒ–å›ºå®šä½“ç§¯å’Œç§»åŠ¨ä½“ç§¯ä¹‹é—´çš„ç›¸ä¼¼æ€§åº¦é‡æ¥ç»•è¿‡å¯¹çœŸå®å˜æ¢æˆ–åˆ†å‰²çš„è¦æ±‚ã€‚åœ¨è¿™äº›æ–¹æ³•ä¸­ï¼Œæœ€è¿‘å‡ºç°çš„ä¸€ç§åŸºäºæ— ç›‘ç£å…³é”®ç‚¹æ£€æµ‹çš„æ–¹æ³•ä½œä¸ºè§£é‡Šæ€§æ–¹é¢éå¸¸æœ‰å‰é€”çš„ä¸€ä¸ªå­ç±»è€Œè„±é¢–è€Œå‡ºã€‚å…·ä½“æ¥è¯´ï¼Œè¿™äº›æ–¹æ³•è®­ç»ƒç½‘ç»œä¸ºå›ºå®šå›¾åƒå’Œç§»åŠ¨å›¾åƒé¢„æµ‹ç‰¹å¾å›¾ï¼Œä»ä¸­è®¡ç®—å‡ºå¯è§£é‡Šçš„è´¨é‡ä¸­å¿ƒï¼Œä»¥è·å¾—ç‚¹äº‘ï¼Œç„¶åè¿™äº›ç‚¹äº‘ä»¥å°é—­å½¢å¼å¯¹é½ã€‚ç„¶è€Œï¼Œç½‘ç»œè¿”å›çš„ç‰¹å¾é€šå¸¸ä¼šäº§ç”Ÿéš¾ä»¥è§£é‡Šçš„ç©ºé—´æ‰©æ•£æ¨¡å¼ï¼Œä»è€Œç ´åäº†åŸºäºå…³é”®ç‚¹çš„æ³¨å†Œç›®çš„ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸‰é‡çš„æŸå¤±æ¥æ­£è§„åŒ–ç‰¹å¾çš„ç©ºé—´åˆ†å¸ƒã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨KLæ•£åº¦æ¥æ¨¡æ‹Ÿç‰¹å¾ä½œä¸ºç‚¹æ‰©æ•£å‡½æ•°ï¼Œæˆ‘ä»¬å°†å…¶è§£é‡Šä¸ºæ¦‚ç‡æ€§çš„å…³é”®ç‚¹ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿è¿™äº›ç‰¹å¾çš„ç©ºé—´åˆ†å¸ƒæ›´åŠ æ¸…æ™°ï¼Œä»¥æé«˜æ£€æµ‹åˆ°çš„åœ°æ ‡çš„ç²¾åº¦ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨å…³é”®ç‚¹ä¹‹é—´å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ’æ–¥æŸå¤±æ¥é¼“åŠ±ç©ºé—´å¤šæ ·æ€§ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„æŸå¤±å¤§å¤§æé«˜äº†ç‰¹å¾çš„å¯è§£é‡Šæ€§ï¼Œè¿™äº›ç‰¹å¾ç°åœ¨å¯¹åº”äºç²¾ç¡®ä¸”è§£å‰–ä¸Šæœ‰æ„ä¹‰çš„åœ°æ ‡ã€‚æˆ‘ä»¬åœ¨èƒå„¿åˆšæ€§è¿åŠ¨è·Ÿè¸ªå’Œè„‘MRIä»¿å°„æ³¨å†Œä»»åŠ¡ä¸­å±•ç¤ºäº†æˆ‘ä»¬çš„ä¸‰é‡æŸå¤±ï¼Œå®ƒä¸ä»…ä¼˜äºæœ€æ–°çš„æ— ç›‘ç£ç­–ç•¥ï¼Œè€Œä¸”ç¼©å°äº†ä¸æœ€æ–°ç›‘ç£æ–¹æ³•çš„å·®è·ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BenBillot/spatial_regularisation">https://github.com/BenBillot/spatial_regularisation</a> ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04499v1">PDF</a> under review</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ— ç›‘ç£å…³é”®ç‚¹æ£€æµ‹çš„æ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–å›ºå®šå›¾åƒå’Œç§»åŠ¨å›¾åƒä¹‹é—´çš„ç›¸ä¼¼æ€§åº¦é‡æ¥å®ç°å›¾åƒé…å‡†ï¼ŒåŒæ—¶æå‡ºäº†ä¸€ç§æ–°çš„ä¸‰é‡æŸå¤±å‡½æ•°å¯¹ç‰¹å¾çš„ç©ºé—´åˆ†å¸ƒè¿›è¡Œæ­£åˆ™åŒ–ï¼Œæé«˜äº†è§£é‡Šæ€§å…³é”®ç‚¹æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…ä¼˜äºç°æœ‰çš„æ— ç›‘ç£ç­–ç•¥ï¼Œè€Œä¸”ç¼©å°äº†ä¸æœ€æ–°ç›‘ç£æ–¹æ³•çš„å·®è·ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>å…³é”®å‘ç°ç‚¹</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ— ç›‘ç£å…³é”®ç‚¹æ£€æµ‹çš„æ— ç›‘ç£é…å‡†æ–¹æ³•ã€‚</li>
<li>é€šè¿‡è®­ç»ƒç½‘ç»œé¢„æµ‹å›ºå®šå’Œç§»åŠ¨å›¾åƒçš„ç‰¹å¾æ˜ å°„ï¼Œå¹¶è®¡ç®—å¯è§£é‡Šçš„ä¸­å¿ƒè´¨é‡æ¥è·å–ç‚¹äº‘è¿›è¡Œå¯¹é½ã€‚</li>
<li>é€šè¿‡KLæ•£åº¦å°†ç‰¹å¾å»ºæ¨¡ä¸ºç‚¹æ‰©æ•£å‡½æ•°ï¼Œè§£é‡Šä¸ºæ¦‚ç‡å…³é”®ç‚¹ã€‚</li>
<li>å¯¹ç‰¹å¾çš„ç©ºé—´åˆ†å¸ƒè¿›è¡Œé”åŒ–ä»¥æé«˜æ£€æµ‹åˆ°çš„åœ°æ ‡ç²¾åº¦ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„æ’æ–¥æŸå¤±æ¥é¼“åŠ±å…³é”®ç‚¹çš„ç©ºé—´å¤šæ ·æ€§ã€‚</li>
<li>ä¸‰é‡æŸå¤±å‡½æ•°æ˜¾è‘—æé«˜äº†ç‰¹å¾çš„è§£é‡Šæ€§ï¼Œè¿™äº›ç‰¹å¾ç°åœ¨å¯¹åº”äºç²¾ç¡®ä¸”è§£å‰–ä¸Šæ„ä¹‰çš„åœ°æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04499">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-77573c23bb63dea746cc03a4805043a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc71c82a2932ed132b853769d83c9338.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-61f4271b1952ce09e7b0ed0f08304acb.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Semantic-Alignment-of-Unimodal-Medical-Text-and-Vision-Representations"><a href="#Semantic-Alignment-of-Unimodal-Medical-Text-and-Vision-Representations" class="headerlink" title="Semantic Alignment of Unimodal Medical Text and Vision Representations"></a>Semantic Alignment of Unimodal Medical Text and Vision Representations</h2><p><strong>Authors:Maxime Di Folco, Emily Chan, Marta Hasny, Cosmin I. Bercea, Julia A. Schnabel</strong></p>
<p>General-purpose AI models, particularly those designed for text and vision, demonstrate impressive versatility across a wide range of deep-learning tasks. However, they often underperform in specialised domains like medical imaging, where domain-specific solutions or alternative knowledge transfer approaches are typically required. Recent studies have noted that general-purpose models can exhibit similar latent spaces when processing semantically related data, although this alignment does not occur naturally. Building on this insight, it has been shown that applying a simple transformation - at most affine - estimated from a subset of semantically corresponding samples, known as anchors, enables model stitching across diverse training paradigms, architectures, and modalities. In this paper, we explore how semantic alignment - estimating transformations between anchors - can bridge general-purpose AI with specialised medical knowledge. Using multiple public chest X-ray datasets, we demonstrate that model stitching across model architectures allows general models to integrate domain-specific knowledge without additional training, leading to improved performance on medical tasks. Furthermore, we introduce a novel zero-shot classification approach for unimodal vision encoders that leverages semantic alignment across modalities. Our results show that our method not only outperforms general multimodal models but also approaches the performance levels of fully trained, medical-specific multimodal solutions </p>
<blockquote>
<p>é€šç”¨äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œå°¤å…¶æ˜¯é‚£äº›ä¸ºæ–‡æœ¬å’Œè§†è§‰è®¾è®¡çš„æ¨¡å‹ï¼Œåœ¨å¹¿æ³›çš„æ·±åº¦å­¦ä¹ ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„é€šç”¨æ€§ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨ä¸“ä¸šé¢†åŸŸå¦‚åŒ»å­¦å½±åƒä¸­å¸¸å¸¸è¡¨ç°ä¸ä½³ï¼Œè¿™é€šå¸¸éœ€è¦é¢†åŸŸç‰¹å®šçš„è§£å†³æ–¹æ¡ˆæˆ–æ›¿ä»£çŸ¥è¯†è¿ç§»æ–¹æ³•ã€‚è¿‘æœŸçš„ç ”ç©¶å‘ç°ï¼Œå½“å¤„ç†è¯­ä¹‰ç›¸å…³æ•°æ®æ—¶ï¼Œé€šç”¨æ¨¡å‹å¯ä»¥å±•ç°å‡ºç±»ä¼¼çš„æ½œåœ¨ç©ºé—´ï¼Œå°½ç®¡è¿™ç§å¯¹é½å¹¶ä¸ä¼šè‡ªç„¶å‘ç”Ÿã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œå·²ç»è¯æ˜åº”ç”¨ä¸€ç§ç®€å•çš„è½¬æ¢â€”â€”æœ€å¤šæ˜¯ä»¿å°„å˜æ¢â€”â€”é€šè¿‡ä»è¯­ä¹‰å¯¹åº”çš„æ ·æœ¬å­é›†ä¼°è®¡å‡ºæ¥ï¼Œè¿™äº›æ ·æœ¬è¢«ç§°ä¸ºé”šç‚¹ï¼Œå¯ä»¥åœ¨ä¸åŒçš„è®­ç»ƒæ¨¡å¼ã€æ¶æ„å’Œæ¨¡æ€ä¹‹é—´è¿›è¡Œæ¨¡å‹æ‹¼æ¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†è¯­ä¹‰å¯¹é½å¦‚ä½•ä¼°ç®—é”šç‚¹ä¹‹é—´çš„è½¬æ¢â€”â€”èƒ½å¤Ÿæ¶èµ·é€šç”¨äººå·¥æ™ºèƒ½ä¸ç‰¹æ®ŠåŒ»å­¦çŸ¥è¯†ä¹‹é—´çš„æ¡¥æ¢ã€‚é€šè¿‡ä½¿ç”¨å¤šä¸ªå…¬å…±èƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ï¼Œæˆ‘ä»¬è¯æ˜äº†è·¨æ¨¡å‹æ¶æ„çš„æ¨¡å‹æ‹¼æ¥å…è®¸é€šç”¨æ¨¡å‹æ•´åˆç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†è€Œæ— éœ€é¢å¤–è®­ç»ƒï¼Œä»è€Œæé«˜äº†åŒ»å­¦ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°å‹çš„é›¶æ ·æœ¬åˆ†ç±»æ–¹æ³•ï¼Œç”¨äºå•æ¨¡æ€è§†è§‰ç¼–ç å™¨ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è·¨æ¨¡æ€çš„è¯­ä¹‰å¯¹é½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…ä¼˜äºä¸€èˆ¬çš„è·¨æ¨¡æ€æ¨¡å‹ï¼Œè€Œä¸”æ¥è¿‘ç»è¿‡å……åˆ†è®­ç»ƒçš„ç‰¹å®šåŒ»å­¦è·¨æ¨¡æ€è§£å†³æ–¹æ¡ˆçš„æ€§èƒ½æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04478v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†é€šç”¨äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨åŒ»å­¦æˆåƒç­‰ç‰¹å®šé¢†åŸŸçš„åº”ç”¨æŒ‘æˆ˜ã€‚é€šè¿‡è¯­ä¹‰å¯¹é½æŠ€æœ¯ï¼Œå®ç°åœ¨ä¸åŒè®­ç»ƒèŒƒå¼ã€æ¶æ„å’Œæ¨¡æ€ä¹‹é—´çš„æ¨¡å‹æ‹¼æ¥ï¼Œä½¿é€šç”¨æ¨¡å‹èƒ½å¤Ÿæ•´åˆç‰¹å®šé¢†åŸŸçŸ¥è¯†ï¼Œæé«˜åœ¨åŒ»ç–—ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè¯­ä¹‰å¯¹é½çš„é›¶å°„å‡»åˆ†ç±»æ–¹æ³•ï¼Œå¯¹äºå•æ¨¡æ€è§†è§‰ç¼–ç å™¨æ€§èƒ½æœ‰æ˜æ˜¾æå‡ï¼Œæ¥è¿‘å…¨è®­ç»ƒçš„åŒ»å­¦ä¸“ç”¨å¤šæ¨¡æ€è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€šç”¨äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨åŒ»å­¦æˆåƒç­‰ç‰¹å®šé¢†åŸŸå­˜åœ¨æ€§èƒ½ç“¶é¢ˆï¼Œéœ€è¦ç‰¹å®šé¢†åŸŸçš„è§£å†³æ–¹æ¡ˆæˆ–çŸ¥è¯†è¿ç§»æ–¹æ³•ã€‚</li>
<li>è¯­ä¹‰å¯¹é½æŠ€æœ¯èƒ½å¤Ÿå®ç°æ¨¡å‹æ‹¼æ¥ï¼Œä½¿å¾—é€šç”¨æ¨¡å‹æ•´åˆç‰¹å®šé¢†åŸŸçŸ¥è¯†æˆä¸ºå¯èƒ½ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨é”šç‚¹è¿›è¡Œè¯­ä¹‰å¯¹é½ä¼°è®¡ï¼Œå¯ä»¥å®ç°ç®€å•çš„æ¨¡å‹è½¬æ¢ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªå…¬å…±èƒ¸éƒ¨Xå…‰æ•°æ®é›†ä¸Šï¼Œæ¨¡å‹æ¶æ„ä¹‹é—´çš„æ‹¼æ¥ä½¿å¾—é€šç”¨æ¨¡å‹èƒ½å¤Ÿèå…¥ç‰¹å®šé¢†åŸŸçŸ¥è¯†ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„é›¶å°„å‡»åˆ†ç±»æ–¹æ³•ï¼Œåˆ©ç”¨è·¨æ¨¡æ€çš„è¯­ä¹‰å¯¹é½ï¼Œå¯¹å•æ¨¡æ€è§†è§‰ç¼–ç å™¨çš„æ€§èƒ½æœ‰æ˜¾è‘—æå‡ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…ä¼˜äºä¸€èˆ¬çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œè€Œä¸”æ¥è¿‘å®Œå…¨è®­ç»ƒçš„åŒ»å­¦ä¸“ç”¨å¤šæ¨¡æ€è§£å†³æ–¹æ¡ˆçš„æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-46acb79099111d1259c27a2fdbc24cac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8be69c489418b1c109b7d587246c74fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d30e0cea070b97065bfb7f96a55ab6ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ee62e832746a87830e12d196be37547.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GBT-SAM-A-Parameter-Efficient-Depth-Aware-Model-for-Generalizable-Brain-tumour-Segmentation-on-mp-MRI"><a href="#GBT-SAM-A-Parameter-Efficient-Depth-Aware-Model-for-Generalizable-Brain-tumour-Segmentation-on-mp-MRI" class="headerlink" title="GBT-SAM: A Parameter-Efficient Depth-Aware Model for Generalizable Brain   tumour Segmentation on mp-MRI"></a>GBT-SAM: A Parameter-Efficient Depth-Aware Model for Generalizable Brain   tumour Segmentation on mp-MRI</h2><p><strong>Authors:Cecilia Diana-Albelda, Roberto Alcover-Couso, Ãlvaro GarcÃ­a-MartÃ­n, Jesus Bescos, Marcos Escudero-ViÃ±olo</strong></p>
<p>Gliomas are brain tumours that stand out for their highly lethal and aggressive nature, which demands a precise approach in their diagnosis. Medical image segmentation plays a crucial role in the evaluation and follow-up of these tumours, allowing specialists to analyse their morphology. However, existing methods for automatic glioma segmentation often lack generalization capability across other brain tumour domains, require extensive computational resources, or fail to fully utilize the multi-parametric MRI (mp-MRI) data used to delineate them. In this work, we introduce GBT-SAM, a novel Generalizable Brain Tumour (GBT) framework that extends the Segment Anything Model (SAM) to brain tumour segmentation tasks. Our method employs a two-step training protocol: first, fine-tuning the patch embedding layer to process the entire mp-MRI modalities, and second, incorporating parameter-efficient LoRA blocks and a Depth-Condition block into the Vision Transformer (ViT) to capture inter-slice correlations. GBT-SAM achieves state-of-the-art performance on the Adult Glioma dataset (Dice Score of $93.54$) while demonstrating robust generalization across Meningioma, Pediatric Glioma, and Sub-Saharan Glioma datasets. Furthermore, GBT-SAM uses less than 6.5M trainable parameters, thus offering an efficient solution for brain tumour segmentation. \ Our code and models are available at <a target="_blank" rel="noopener" href="https://github.com/vpulab/med-sam-brain">https://github.com/vpulab/med-sam-brain</a> . </p>
<blockquote>
<p>èƒ¶è´¨ç˜¤æ˜¯çªå‡ºçš„è„‘è‚¿ç˜¤ï¼Œå…·æœ‰é«˜åº¦çš„è‡´æ­»æ€§å’Œä¾µè¢­æ€§ï¼Œè¿™å°±è¦æ±‚åœ¨è¯Šæ–­æ—¶éœ€è¦é‡‡å–ç²¾ç¡®çš„æ–¹æ³•ã€‚åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨è¯„ä¼°å’Œéšè®¿è¿™äº›è‚¿ç˜¤ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå…è®¸ä¸“å®¶åˆ†æå®ƒä»¬çš„å½¢æ€ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è‡ªåŠ¨èƒ¶è´¨ç˜¤åˆ†å‰²æ–¹æ³•å¾€å¾€ç¼ºä¹åœ¨å…¶ä»–è„‘è‚¿ç˜¤é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ï¼Œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œæˆ–è€…æœªèƒ½å……åˆ†åˆ©ç”¨ç”¨äºç•Œå®šè‚¿ç˜¤çš„å¤šå‚æ•°MRIï¼ˆmp-MRIï¼‰æ•°æ®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†GBT-SAMï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ³›åŒ–è„‘è‚¿ç˜¤ï¼ˆGBTï¼‰çš„æ–°æ¡†æ¶ï¼Œå®ƒæ‰©å±•äº†åˆ†æ®µä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰ä»¥ç”¨äºè„‘è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä¸¤æ­¥è®­ç»ƒåè®®ï¼šé¦–å…ˆï¼Œå¾®è°ƒè¡¥ä¸åµŒå…¥å±‚ä»¥å¤„ç†æ•´ä¸ªmp-MRIæ¨¡å¼ï¼›å…¶æ¬¡ï¼Œåœ¨è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ä¸­èå…¥å‚æ•°é«˜æ•ˆçš„LoRAå—å’Œæ·±åº¦æ¡ä»¶å—ï¼Œä»¥æ•æ‰åˆ‡ç‰‡é—´çš„ç›¸å…³æ€§ã€‚GBT-SAMåœ¨æˆäººèƒ¶è´¨ç˜¤æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ˆDiceå¾—åˆ†ä¸º93.54%ï¼‰ï¼ŒåŒæ—¶åœ¨è„‘è†œç˜¤ã€å„¿ç«¥èƒ¶è´¨ç˜¤å’Œæ’’å“ˆæ‹‰ä»¥å—èƒ¶è´¨ç˜¤æ•°æ®é›†ä¸Šè¡¨ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒGBT-SAMä½¿ç”¨çš„å¯è®­ç»ƒå‚æ•°å°‘äº650ä¸‡ï¼Œå› æ­¤ä¸ºè„‘è‚¿ç˜¤åˆ†å‰²æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/vpulab/med-sam-brain%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/vpulab/med-sam-brainä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04325v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºGBT-SAMçš„æ–°å‹é€šç”¨è„‘è‚¿ç˜¤ï¼ˆGBTï¼‰åˆ†å‰²æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ‰©å±•äº†Segment Anything Modelï¼ˆSAMï¼‰ä»¥åº”ç”¨äºè„‘è‚¿ç˜¤åˆ†å‰²ä»»åŠ¡ã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§ä¸¤æ­¥è®­ç»ƒåè®®ï¼Œé€šè¿‡åœ¨PatchåµŒå…¥å±‚å¤„ç†æ•´ä¸ªå¤šå‚æ•°MRIï¼ˆmp-MRIï¼‰æ¨¡å¼å¹¶è¿›è¡Œç²¾ç»†è°ƒæ•´ï¼Œä»¥åŠå°†å‚æ•°é«˜æ•ˆçš„LoRAå—å’Œæ·±åº¦æ¡ä»¶å—èå…¥è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ä»¥æ•è·åˆ‡ç‰‡é—´çš„ç›¸å…³æ€§ã€‚GBT-SAMåœ¨æˆäººèƒ¶è´¨ç˜¤æ•°æ®é›†ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ï¼ˆDiceåˆ†æ•°ä¸º93.54%ï¼‰ï¼Œå¹¶åœ¨è„‘è†œç˜¤ã€å„¿ç«¥èƒ¶è´¨ç˜¤å’Œæ’’å“ˆæ‹‰ä»¥å—èƒ¶è´¨ç˜¤æ•°æ®é›†ä¸Šè¡¨ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒGBT-SAMä½¿ç”¨çš„å¯è®­ç»ƒå‚æ•°å°‘äº650ä¸‡ï¼Œä¸ºè„‘è‚¿ç˜¤åˆ†å‰²æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GBT-SAMæ˜¯ä¸€ä¸ªé’ˆå¯¹è„‘è‚¿ç˜¤åˆ†å‰²çš„æ–°å‹æ¡†æ¶ï¼ŒåŸºäºSegment Anything Modelï¼ˆSAMï¼‰ã€‚</li>
<li>é‡‡ç”¨äº†ä¸¤æ­¥è®­ç»ƒåè®®ï¼šé¦–å…ˆå¾®è°ƒpatchåµŒå…¥å±‚å¤„ç†æ•´ä¸ªå¤šå‚æ•°MRIæ•°æ®ï¼Œç„¶åèå…¥è§†è§‰è½¬æ¢å™¨ä»¥æ•è·åˆ‡ç‰‡é—´çš„ç›¸å…³æ€§ã€‚</li>
<li>GBT-SAMåœ¨æˆäººèƒ¶è´¨ç˜¤æ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼ŒDiceåˆ†æ•°ä¸º93.54%ã€‚</li>
<li>GBT-SAMå±•ç°å‡ºå¯¹è„‘è†œç˜¤ã€å„¿ç«¥èƒ¶è´¨ç˜¤å’Œæ’’å“ˆæ‹‰ä»¥å—èƒ¶è´¨ç˜¤æ•°æ®é›†çš„ç¨³å¥æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>GBT-SAMä½¿ç”¨çš„å¯è®­ç»ƒå‚æ•°å°‘äº650ä¸‡ï¼Œå…·æœ‰é«˜æ•ˆæ€§ã€‚</li>
<li>GBT-SAMçš„æ–¹æ³•å¯ä»¥åœ¨å…¬å¼€ä»£ç å’Œæ¨¡å‹ä¸­æ‰¾åˆ°ï¼Œç½‘å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/vpulab/med-sam-brain%E3%80%82">https://github.com/vpulab/med-sam-brainã€‚</a></li>
<li>GBT-SAMæ¡†æ¶çš„å¼•å…¥ä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸæä¾›äº†ä¸€ç§æ–°çš„ã€é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04325">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-94f7bc94ad4d039e0a1b7810c53f0fe1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bf322f8050f3b09629c75e5bf0791fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-750f89e2323cf1001dcd372ff31c1a77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5966b058c28bc67abfd53731528eb601.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="WeakMedSAM-Weakly-Supervised-Medical-Image-Segmentation-via-SAM-with-Sub-Class-Exploration-and-Prompt-Affinity-Mining"><a href="#WeakMedSAM-Weakly-Supervised-Medical-Image-Segmentation-via-SAM-with-Sub-Class-Exploration-and-Prompt-Affinity-Mining" class="headerlink" title="WeakMedSAM: Weakly-Supervised Medical Image Segmentation via SAM with   Sub-Class Exploration and Prompt Affinity Mining"></a>WeakMedSAM: Weakly-Supervised Medical Image Segmentation via SAM with   Sub-Class Exploration and Prompt Affinity Mining</h2><p><strong>Authors:Haoran Wang, Lian Huai, Wenbin Li, Lei Qi, Xingqun Jiang, Yinghuan Shi</strong></p>
<p>We have witnessed remarkable progress in foundation models in vision tasks. Currently, several recent works have utilized the segmenting anything model (SAM) to boost the segmentation performance in medical images, where most of them focus on training an adaptor for fine-tuning a large amount of pixel-wise annotated medical images following a fully supervised manner. In this paper, to reduce the labeling cost, we investigate a novel weakly-supervised SAM-based segmentation model, namely WeakMedSAM. Specifically, our proposed WeakMedSAM contains two modules: 1) to mitigate severe co-occurrence in medical images, a sub-class exploration module is introduced to learn accurate feature representations. 2) to improve the quality of the class activation maps, our prompt affinity mining module utilizes the prompt capability of SAM to obtain an affinity map for random-walk refinement. Our method can be applied to any SAM-like backbone, and we conduct experiments with SAMUS and EfficientSAM. The experimental results on three popularly-used benchmark datasets, i.e., BraTS 2019, AbdomenCT-1K, and MSD Cardiac dataset, show the promising results of our proposed WeakMedSAM. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/wanghr64/WeakMedSAM">https://github.com/wanghr64/WeakMedSAM</a>. </p>
<blockquote>
<p>åœ¨è§†è§‰ä»»åŠ¡çš„åŸºç¡€æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç›®å‰ï¼Œä¸€äº›æœ€æ–°çš„å·¥ä½œåˆ©ç”¨åˆ†å‰²ä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰æ¥æé«˜åŒ»å­¦å›¾åƒçš„åˆ†å‰²æ€§èƒ½ï¼Œå…¶ä¸­å¤§å¤šæ•°å·¥ä½œéƒ½é›†ä¸­åœ¨ä»¥å®Œå…¨ç›‘ç£çš„æ–¹å¼è®­ç»ƒé€‚é…å™¨ï¼Œå¯¹å¤§é‡åƒç´ çº§æ³¨é‡Šçš„åŒ»å­¦å›¾åƒè¿›è¡Œå¾®è°ƒã€‚ä¸ºäº†é™ä½æ ‡æ³¨æˆæœ¬ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­ç ”ç©¶äº†ä¸€ç§æ–°å‹çš„åŸºäºå¼±ç›‘ç£çš„SAMåˆ†å‰²æ¨¡å‹ï¼Œå³WeakMedSAMã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºçš„WeakMedSAMåŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼š1ï¼‰ä¸ºäº†ç¼“è§£åŒ»å­¦å›¾åƒä¸­ä¸¥é‡çš„å…±å‘ç”Ÿé—®é¢˜ï¼Œå¼•å…¥äº†å­ç±»æ¢ç´¢æ¨¡å—æ¥å­¦ä¹ å‡†ç¡®çš„ç‰¹å¾è¡¨ç¤ºã€‚2ï¼‰ä¸ºäº†æé«˜ç±»æ¿€æ´»å›¾çš„è´¨é‡ï¼Œæˆ‘ä»¬çš„æç¤ºäº²å’ŒåŠ›æŒ–æ˜æ¨¡å—åˆ©ç”¨SAMçš„æç¤ºèƒ½åŠ›æ¥è·å¾—ç”¨äºéšæœºæ¸¸èµ°ç»†åŒ–çš„äº²å’ŒåŠ›å›¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åº”ç”¨äºä»»ä½•SAMç±»ä¼¼çš„backboneï¼Œæˆ‘ä»¬åœ¨SAMUSå’ŒEfficientSAMä¸Šè¿›è¡Œäº†å®éªŒã€‚åœ¨ä¸‰ä¸ªå¸¸ç”¨çš„åŸºå‡†æ•°æ®é›†ï¼ˆå³BraTS 2019ã€AbdomenCT-1Kå’ŒMSD Cardiacæ•°æ®é›†ï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„WeakMedSAMå…·æœ‰å¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚æˆ‘ä»¬çš„ä»£ç å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/wanghr64/WeakMedSAM%E3%80%82">https://github.com/wanghr64/WeakMedSAMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04106v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¼±ç›‘ç£çš„SAMï¼ˆåˆ†æ®µä»»ä½•æ¨¡å‹ï¼‰åŒ»ç–—å›¾åƒåˆ†å‰²æ¨¡å‹â€”â€”WeakMedSAMï¼Œä»¥é™ä½æ ‡æ³¨æˆæœ¬ã€‚å®ƒé€šè¿‡å­ç±»åˆ«æ¢ç´¢æ¨¡å—å­¦ä¹ å‡†ç¡®çš„ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶æ”¹è¿›äº†ç±»åˆ«æ¿€æ´»å›¾çš„å‡†ç¡®æ€§ã€‚åœ¨SAMUSå’ŒEfficientSAMç­‰æ¨¡å‹çš„å®éªŒè¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚è¯¥æ¨¡å‹åœ¨BraTS 2019ã€AbdomenCT-1Kå’ŒMSDå¿ƒè„æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†å…¶æ½œåŠ›ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯¥è®ºæ–‡æå‡ºä¸€ç§åŸºäºå¼±ç›‘ç£çš„SAMåŒ»ç–—å›¾åƒåˆ†å‰²æ¨¡å‹WeakMedSAMï¼Œæ—¨åœ¨é™ä½æ ‡æ³¨æˆæœ¬ã€‚</li>
<li>WeakMedSAMåŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šå­ç±»æ¢ç´¢æ¨¡å—ç”¨äºå­¦ä¹ å‡†ç¡®çš„ç‰¹å¾è¡¨ç¤ºï¼Œå¹¶æ”¹è¿›ç±»åˆ«æ¿€æ´»å›¾çš„å‡†ç¡®æ€§ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨éšæœºæ¸¸èµ°ç»†åŒ–æŠ€æœ¯ï¼Œé€šè¿‡SAMçš„æç¤ºèƒ½åŠ›è·å–äº²å’ŒåŠ›å›¾ã€‚</li>
<li>WeakMedSAMå¯åº”ç”¨äºä»»ä½•SAMç±»ä¼¼çš„ä¸»å¹²ç½‘ç»œï¼Œå¦‚SAMUSå’ŒEfficientSamã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†WeakMedSAMåœ¨BraTS 2019ã€AbdomenCT-1Kå’ŒMSDå¿ƒè„æ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04106">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e323683c49116e8cbcd4bc5b1325e1a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cb6b2a7e3e81f209d843b99170518b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e1124f1221980b5421e9988bc7b69e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b78c7413bf5961246376766049392672.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f10930621d4d62f1ed08890a401efa3f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Image-Based-Relocalization-and-Alignment-for-Long-Term-Monitoring-of-Dynamic-Underwater-Environments"><a href="#Image-Based-Relocalization-and-Alignment-for-Long-Term-Monitoring-of-Dynamic-Underwater-Environments" class="headerlink" title="Image-Based Relocalization and Alignment for Long-Term Monitoring of   Dynamic Underwater Environments"></a>Image-Based Relocalization and Alignment for Long-Term Monitoring of   Dynamic Underwater Environments</h2><p><strong>Authors:Beverley Gorry, Tobias Fischer, Michael Milford, Alejandro Fontan</strong></p>
<p>Effective monitoring of underwater ecosystems is crucial for tracking environmental changes, guiding conservation efforts, and ensuring long-term ecosystem health. However, automating underwater ecosystem management with robotic platforms remains challenging due to the complexities of underwater imagery, which pose significant difficulties for traditional visual localization methods. We propose an integrated pipeline that combines Visual Place Recognition (VPR), feature matching, and image segmentation on video-derived images. This method enables robust identification of revisited areas, estimation of rigid transformations, and downstream analysis of ecosystem changes. Furthermore, we introduce the SQUIDLE+ VPR Benchmark-the first large-scale underwater VPR benchmark designed to leverage an extensive collection of unstructured data from multiple robotic platforms, spanning time intervals from days to years. The dataset encompasses diverse trajectories, arbitrary overlap and diverse seafloor types captured under varying environmental conditions, including differences in depth, lighting, and turbidity. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/bev-gorry/underloc">https://github.com/bev-gorry/underloc</a> </p>
<blockquote>
<p>å¯¹æ°´ä¸‹ç”Ÿæ€ç³»ç»Ÿè¿›è¡Œæœ‰æ•ˆç›‘æµ‹å¯¹äºè·Ÿè¸ªç¯å¢ƒå˜åŒ–ã€æŒ‡å¯¼ä¿æŠ¤å·¥ä½œå’Œç¡®ä¿ç”Ÿæ€ç³»ç»Ÿé•¿æœŸå¥åº·è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºæ°´ä¸‹å›¾åƒçš„å¤æ‚æ€§ï¼Œä½¿ç”¨æœºå™¨äººå¹³å°è‡ªåŠ¨ç®¡ç†æ°´ä¸‹ç”Ÿæ€ç³»ç»Ÿä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™ç»™ä¼ ç»Ÿçš„è§†è§‰å®šä½æ–¹æ³•å¸¦æ¥äº†å·¨å¤§çš„å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆè§†è§‰å®šä½è¯†åˆ«ï¼ˆVPRï¼‰ã€ç‰¹å¾åŒ¹é…å’ŒåŸºäºè§†é¢‘å›¾åƒåˆ†å‰²æ–¹æ³•çš„ç»¼åˆæµç¨‹ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°ç¨³å¥çš„å›è®¿åŒºåŸŸè¯†åˆ«ã€åˆšæ€§è½¬æ¢ä¼°è®¡ä»¥åŠç”Ÿæ€ç³»ç»Ÿå˜åŒ–çš„ä¸‹æ¸¸åˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†SQUIDLE+ VPR Benchmarkâ€”â€”è¿™æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡æ°´ä¸‹VPRåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨åˆ©ç”¨æ¥è‡ªå¤šä¸ªæœºå™¨äººå¹³å°çš„æµ·é‡éç»“æ„åŒ–æ•°æ®é›†åˆï¼Œæ—¶é—´é—´éš”ä»å‡ å¤©åˆ°å‡ å¹´ä¸ç­‰ã€‚æ•°æ®é›†åŒ…å«å¤šç§è½¨è¿¹ã€ä»»æ„é‡å å’Œä¸åŒç¯å¢ƒæ¡ä»¶ä¸‹çš„å„ç§æµ·åº•ç±»å‹ï¼ˆåŒ…æ‹¬æ·±åº¦ã€å…‰ç…§å’ŒæµŠåº¦çš„å·®å¼‚ï¼‰ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/bev-gorry/underloc">https://github.com/bev-gorry/underloc</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04096v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆè§†è§‰å®šä½è¯†åˆ«ï¼ˆVPRï¼‰ã€ç‰¹å¾åŒ¹é…å’Œè§†é¢‘å›¾åƒåˆ†å‰²æŠ€æœ¯çš„æ–¹æ³•ï¼Œå®ç°å¯¹æ°´ä¸‹ç”Ÿæ€ç³»ç»Ÿçš„è‡ªåŠ¨åŒ–ç›‘æµ‹å’Œç®¡ç†ã€‚é€šè¿‡è¯¥æ–¹æ³•çš„å®æ–½ï¼Œèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«å›è®¿åŒºåŸŸã€ä¼°ç®—åˆšæ€§å˜æ¢ä»¥åŠåˆ†æç”Ÿæ€ç³»ç»Ÿå˜åŒ–ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†é¦–ä¸ªå¤§è§„æ¨¡æ°´ä¸‹VPRåŸºå‡†æµ‹è¯•é›†â€”â€”SQUIDLE+ VPR Benchmarkï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªå¤šä¸ªæœºå™¨äººå¹³å°çš„ä¸°å¯Œéç»“æ„åŒ–æ•°æ®ï¼Œè·¨è¶Šæ•°æ—¥ä¹ƒè‡³æ•°å¹´çš„æ—¶é—´æ®µã€‚æ•°æ®é›†æ¶µç›–äº†å¤šç§è½¨è¿¹ã€ä»»æ„é‡å å’Œä¸åŒæµ·åº•ç±»å‹ç­‰ç¯å¢ƒæ¡ä»¶ä¸‹çš„æ•°æ®å·®å¼‚ï¼ŒåŒ…æ‹¬æ·±åº¦ã€å…‰ç…§å’ŒæµŠåº¦ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ‰æ•ˆç›‘æµ‹æ°´ä¸‹ç”Ÿæ€ç³»ç»Ÿå¯¹äºè·Ÿè¸ªç¯å¢ƒå˜åŒ–ã€æŒ‡å¯¼ä¿æŠ¤å·¥ä½œå’Œç¡®ä¿ç”Ÿæ€ç³»ç»Ÿé•¿æœŸå¥åº·è‡³å…³é‡è¦ã€‚</li>
<li>æ°´ä¸‹æˆåƒçš„å¤æ‚æ€§ç»™ä¼ ç»Ÿè§†è§‰å®šä½æ–¹æ³•å¸¦æ¥äº†æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆè§†è§‰å®šä½è¯†åˆ«ï¼ˆVPRï¼‰ã€ç‰¹å¾åŒ¹é…å’Œè§†é¢‘å›¾åƒåˆ†å‰²çš„é›†æˆæ–¹æ³•ï¼Œç”¨äºç¨³å¥åœ°è¯†åˆ«å›è®¿åŒºåŸŸã€ä¼°ç®—åˆšæ€§å˜æ¢å’Œåˆ†æç”Ÿæ€ç³»ç»Ÿå˜åŒ–ã€‚</li>
<li>ä»‹ç»äº†é¦–ä¸ªå¤§è§„æ¨¡æ°´ä¸‹VPRåŸºå‡†æµ‹è¯•é›†â€”â€”SQUIDLE+ VPR Benchmarkã€‚</li>
<li>è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªå¤šä¸ªæœºå™¨äººå¹³å°çš„ä¸°å¯Œéç»“æ„åŒ–æ•°æ®ï¼Œè·¨è¶Šä¸åŒç¯å¢ƒæ¡ä»¶ä¸‹çš„æ•°æ®å·®å¼‚ã€‚</li>
<li>æ•°æ®é›†æ¶µç›–å¤šç§è½¨è¿¹å’Œä»»æ„é‡å æƒ…å†µï¼Œä¸ºæ°´ä¸‹ç”Ÿæ€ç³»ç»Ÿç®¡ç†çš„è‡ªåŠ¨åŒ–æä¾›äº†é‡è¦èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04096">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-73c910a13045336e0d8ac4a60f173263.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-75347b420fcad5c10858df46546e64e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37ffc2b4677fc2f6832132ec80f56de9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-871c583940c3e6c69475a449205531e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8b53d0cf04e1c917344744de915327a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d78cec8cbf5f857d1c84ac3f71048216.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ReRAW-RGB-to-RAW-Image-Reconstruction-via-Stratified-Sampling-for-Efficient-Object-Detection-on-the-Edge"><a href="#ReRAW-RGB-to-RAW-Image-Reconstruction-via-Stratified-Sampling-for-Efficient-Object-Detection-on-the-Edge" class="headerlink" title="ReRAW: RGB-to-RAW Image Reconstruction via Stratified Sampling for   Efficient Object Detection on the Edge"></a>ReRAW: RGB-to-RAW Image Reconstruction via Stratified Sampling for   Efficient Object Detection on the Edge</h2><p><strong>Authors:Radu Berdan, Beril Besbinar, Christoph Reinders, Junji Otsuka, Daisuke Iso</strong></p>
<p>Edge-based computer vision models running on compact, resource-limited devices benefit greatly from using unprocessed, detail-rich RAW sensor data instead of processed RGB images. Training these models, however, necessitates large labeled RAW datasets, which are costly and often impractical to obtain. Thus, converting existing labeled RGB datasets into sensor-specific RAW images becomes crucial for effective model training. In this paper, we introduce ReRAW, an RGB-to-RAW conversion model that achieves state-of-the-art reconstruction performance across five diverse RAW datasets. This is accomplished through ReRAWâ€™s novel multi-head architecture predicting RAW image candidates in gamma space. The performance is further boosted by a stratified sampling-based training data selection heuristic, which helps the model better reconstruct brighter RAW pixels. We finally demonstrate that pretraining compact models on a combination of high-quality synthetic RAW datasets (such as generated by ReRAW) and ground-truth RAW images for downstream tasks like object detection, outperforms both standard RGB pipelines, and RAW fine-tuning of RGB-pretrained models for the same task. </p>
<blockquote>
<p>åŸºäºè¾¹ç¼˜çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹åœ¨ç´§å‡‘ã€èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿è¡Œï¼Œä»ä½¿ç”¨æœªç»å¤„ç†ã€ç»†èŠ‚ä¸°å¯Œçš„RAWä¼ æ„Ÿå™¨æ•°æ®è€Œä¸æ˜¯ç»è¿‡å¤„ç†çš„RGBå›¾åƒä¸­è·ç›ŠåŒªæµ…ã€‚ç„¶è€Œï¼Œè®­ç»ƒè¿™äº›æ¨¡å‹éœ€è¦å¤§é‡çš„æ ‡è®°RAWæ•°æ®é›†ï¼Œè¿™æ—¢æ˜‚è´µåˆå¾€å¾€ä¸åˆ‡å®é™…ã€‚å› æ­¤ï¼Œå°†ç°æœ‰çš„æ ‡è®°RGBæ•°æ®é›†è½¬æ¢ä¸ºç‰¹å®šä¼ æ„Ÿå™¨RAWå›¾åƒå¯¹äºæœ‰æ•ˆæ¨¡å‹è®­ç»ƒå˜å¾—è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ReRAWï¼Œä¸€ç§RGBåˆ°RAWçš„è½¬æ¢æ¨¡å‹ï¼Œå®ƒåœ¨äº”ä¸ªä¸åŒçš„RAWæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºæ€§èƒ½ã€‚è¿™æ˜¯é€šè¿‡ReRAWçš„æ–°å‹å¤šå¤´æ¶æ„åœ¨ä¼½é©¬ç©ºé—´ä¸­é¢„æµ‹RAWå›¾åƒå€™é€‰ç‰©æ¥å®ç°çš„ã€‚é€šè¿‡åˆ†å±‚é‡‡æ ·ä¸ºåŸºç¡€çš„è®­ç»ƒæ•°æ®é€‰æ‹©å¯å‘å¼æ–¹æ³•ï¼Œæ€§èƒ½å¾—åˆ°äº†è¿›ä¸€æ­¥æå‡ï¼Œè¿™æœ‰åŠ©äºæ¨¡å‹æ›´å¥½åœ°é‡å»ºè¾ƒäº®çš„RAWåƒç´ ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜åœ¨é«˜è´¨é‡åˆæˆRAWæ•°æ®é›†ï¼ˆå¦‚ç”±ReRAWç”Ÿæˆï¼‰å’ŒçœŸå®RAWå›¾åƒçš„ç»„åˆä¸Šé¢„è®­ç»ƒç´§å‡‘æ¨¡å‹ï¼Œç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚å¯¹è±¡æ£€æµ‹ï¼‰ï¼Œä¼˜äºæ ‡å‡†çš„RGBç®¡é“å’Œç›¸åŒä»»åŠ¡çš„RAWå¾®è°ƒRGBé¢„è®­ç»ƒæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03782v1">PDF</a> Accepted at CVPR 2025</p>
<p><strong>Summary</strong><br>     é‡‡ç”¨æ— å¤„ç†ã€ç»†èŠ‚ä¸°å¯Œçš„RAWä¼ æ„Ÿå™¨æ•°æ®ï¼Œåœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šè¿è¡Œçš„è¾¹ç¼˜è®¡ç®—è§†è§‰æ¨¡å‹ç›¸è¾ƒäºä½¿ç”¨å·²å¤„ç†çš„RGBå›¾åƒå—ç›ŠåŒªæµ…ã€‚ç„¶è€Œï¼Œè®­ç»ƒè¿™äº›æ¨¡å‹éœ€è¦å¤§é‡æ ‡è®°çš„RAWæ•°æ®é›†ï¼Œè¿™æˆæœ¬é«˜æ˜‚å¹¶ä¸åˆ‡å®é™…ã€‚å› æ­¤ï¼Œå°†ç°æœ‰æ ‡è®°çš„RGBæ•°æ®é›†è½¬æ¢ä¸ºç‰¹å®šä¼ æ„Ÿå™¨RAWå›¾åƒå˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§RGBè½¬RAWè½¬æ¢æ¨¡å‹ReRAWï¼Œå…¶åœ¨äº”ä¸ªä¸åŒçš„RAWæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºæ€§èƒ½ã€‚å…¶åˆ›æ–°çš„å¤šå¤´æ¶æ„èƒ½å¤Ÿåœ¨ä¼½é©¬ç©ºé—´é¢„æµ‹RAWå›¾åƒå€™é€‰è€…ï¼Œä»è€Œå®ç°è¿™ä¸€ç›®æ ‡ã€‚æ­¤å¤–ï¼ŒåŸºäºåˆ†å±‚é‡‡æ ·çš„è®­ç»ƒæ•°æ®é€‰æ‹©å¯å‘å¼ç­–ç•¥è¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°é‡å»ºè¾ƒäº®çš„RAWåƒç´ ã€‚æœ€åè¯æ˜ï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡å¦‚ç›®æ ‡æ£€æµ‹ä¸Šï¼Œä½¿ç”¨é«˜è´¨é‡åˆæˆRAWæ•°æ®é›†ï¼ˆå¦‚ç”±ReRAWç”Ÿæˆï¼‰å’ŒçœŸå®RAWå›¾åƒé¢„è®­ç»ƒç´§å‡‘æ¨¡å‹ï¼Œç›¸è¾ƒäºæ ‡å‡†RGBç®¡é“å’ŒRGBé¢„è®­ç»ƒæ¨¡å‹çš„RAWå¾®è°ƒï¼Œè¡¨ç°æ›´ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨æœªå¤„ç†çš„RAWä¼ æ„Ÿå™¨æ•°æ®å¯¹è¾¹ç¼˜è®¡ç®—è§†è§‰æ¨¡å‹è¿›è¡Œè®­ç»ƒåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>è®­ç»ƒè¿™ç±»æ¨¡å‹éœ€è¦å¤§é‡æ ‡è®°çš„RAWæ•°æ®é›†ï¼Œä½†å…¶è·å–æˆæœ¬é«˜æ˜‚ä¸”å®é™…æ“ä½œå›°éš¾ã€‚</li>
<li>ReRAWæ¨¡å‹é€šè¿‡RGBè½¬RAWè½¬æ¢å®ç°äº†å…ˆè¿›çš„é‡å»ºæ€§èƒ½ï¼Œå€ŸåŠ©å…¶å¤šå¤´æ¶æ„åœ¨ä¼½é©¬ç©ºé—´é¢„æµ‹RAWå›¾åƒã€‚</li>
<li>åˆ†å±‚é‡‡æ ·å¯å‘å¼çš„è®­ç»ƒæ•°æ®é€‰æ‹©ç­–ç•¥å¢å¼ºäº†æ¨¡å‹çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é‡å»ºäº®åƒç´ æ–¹é¢ã€‚</li>
<li>é«˜è´¨é‡åˆæˆRAWæ•°æ®é›†ä¸çœŸå®RAWå›¾åƒçš„ç»“åˆä½¿ç”¨å¯¹äºé¢„è®­ç»ƒç´§å‡‘æ¨¡å‹è‡³å…³é‡è¦ã€‚</li>
<li>é¢„è®­ç»ƒæ¨¡å‹åœ¨ç›®æ ‡æ£€æµ‹ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¶Šäº†æ ‡å‡†RGBç®¡é“å’ŒRGBé¢„è®­ç»ƒæ¨¡å‹çš„RAWå¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cf3029351e4800ddc30b3010d180d1eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e105fa7405f6284a05e325d016176b9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44a668ce55002cb8a37b9e6a934163c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e611587f5de14e8db7a4fb624f86d4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eeaacc3a442e979394532071ca1a4259.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Deep-unrolling-for-learning-optimal-spatially-varying-regularisation-parameters-for-Total-Generalised-Variation"><a href="#Deep-unrolling-for-learning-optimal-spatially-varying-regularisation-parameters-for-Total-Generalised-Variation" class="headerlink" title="Deep unrolling for learning optimal spatially varying regularisation   parameters for Total Generalised Variation"></a>Deep unrolling for learning optimal spatially varying regularisation   parameters for Total Generalised Variation</h2><p><strong>Authors:Thanh Trung Vu, Andreas Kofler, Kostas Papafitsoros</strong></p>
<p>We extend a recently introduced deep unrolling framework for learning spatially varying regularisation parameters in inverse imaging problems to the case of Total Generalised Variation (TGV). The framework combines a deep convolutional neural network (CNN) inferring the two spatially varying TGV parameters with an unrolled algorithmic scheme that solves the corresponding variational problem. The two subnetworks are jointly trained end-to-end in a supervised fashion and as such the CNN learns to compute those parameters that drive the reconstructed images as close to the ground truth as possible. Numerical results in image denoising and MRI reconstruction show a significant qualitative and quantitative improvement compared to the best TGV scalar parameter case as well as to other approaches employing spatially varying parameters computed by unsupervised methods. We also observe that the inferred spatially varying parameter maps have a consistent structure near the image edges, asking for further theoretical investigations. In particular, the parameter that weighs the first-order TGV term has a triple-edge structure with alternating high-low-high values whereas the one that weighs the second-order term attains small values in a large neighbourhood around the edges. </p>
<blockquote>
<p>æˆ‘ä»¬å°†æœ€è¿‘å¼•å…¥çš„æ·±åº¦å±•å¼€æ¡†æ¶æ‰©å±•åˆ°æ€»å¹¿ä¹‰å˜åŒ–ï¼ˆTGVï¼‰çš„æƒ…å†µï¼Œç”¨äºå­¦ä¹ åæˆåƒé—®é¢˜ä¸­çš„ç©ºé—´å˜åŒ–æ­£åˆ™åŒ–å‚æ•°ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œç”¨äºæ¨æ–­ä¸¤ä¸ªç©ºé—´å˜åŒ–çš„TGVå‚æ•°ï¼Œä»¥åŠä¸€ä¸ªè§£å†³ç›¸åº”å˜åˆ†é—®é¢˜çš„å±•å¼€ç®—æ³•æ–¹æ¡ˆã€‚ä¸¤ä¸ªå­ç½‘ç»œä»¥ç›‘ç£æ–¹å¼ç«¯å¯¹ç«¯è”åˆè®­ç»ƒï¼Œå› æ­¤CNNå­¦ä¹ è®¡ç®—é‚£äº›å‚æ•°ï¼Œä»¥å°½å¯èƒ½æ¥è¿‘çœŸå®æƒ…å†µçš„æ–¹å¼é‡å»ºå›¾åƒã€‚åœ¨å›¾åƒå»å™ªå’ŒMRIé‡å»ºæ–¹é¢çš„æ•°å€¼ç»“æœè¡¨æ˜ï¼Œä¸æœ€ä½³TGVæ ‡é‡å‚æ•°æƒ…å†µä»¥åŠå…¶ä»–é‡‡ç”¨ç©ºé—´å˜åŒ–å‚æ•°çš„æ–¹æ³•ç›¸æ¯”ï¼Œæ— è®ºåœ¨å®šæ€§å’Œå®šé‡ä¸Šå‡æœ‰æ˜¾è‘—æ”¹å–„ã€‚è¿™äº›ç©ºé—´å˜åŒ–çš„å‚æ•°å›¾åœ¨å›¾åƒè¾¹ç¼˜é™„è¿‘å…·æœ‰ä¸€è‡´çš„ç»“æ„ï¼Œéœ€è¦è¿›ä¸€æ­¥çš„ç†è®ºç ”ç©¶ã€‚ç‰¹åˆ«æ˜¯ï¼Œæƒé‡ä¸€é˜¶TGVé¡¹çš„å‚æ•°å­—æ˜ å°„å…·æœ‰é«˜ä½é«˜çš„ä¸‰é‡è¾¹ç¼˜ç»“æ„ï¼Œè€Œæƒé‡äºŒé˜¶é¡¹çš„å‚æ•°åœ¨è¾¹ç¼˜é™„è¿‘çš„å¤§èŒƒå›´å†…è¾¾åˆ°è¾ƒå°å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16532v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡å°†æ·±åº¦å±•å¼€æ¡†æ¶æ‰©å±•åˆ°æ€»å¹¿ä¹‰å˜åŒ–ï¼ˆTGVï¼‰çš„æƒ…å†µï¼Œç»“åˆæ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨æ–­ä¸¤ä¸ªç©ºé—´å˜åŒ–çš„TGVå‚æ•°ä¸ä¸€ä¸ªè§£å†³ç›¸åº”å˜åˆ†é—®é¢˜çš„å±•å¼€ç®—æ³•æ–¹æ¡ˆã€‚ä¸¤ä¸ªå­ç½‘ç»œä»¥ç›‘ç£æ–¹å¼è¿›è¡Œç«¯åˆ°ç«¯çš„è”åˆè®­ç»ƒï¼Œä½¿å¾—CNNå­¦ä¹ è®¡ç®—ä½¿é‡å»ºå›¾åƒå°½å¯èƒ½æ¥è¿‘çœŸå®å€¼çš„å‚æ•°ã€‚å›¾åƒå»å™ªå’ŒMRIé‡å»ºçš„æ•°å€¼ç»“æœæ˜¾ç¤ºï¼Œä¸æœ€ä½³TGVæ ‡é‡å‚æ•°æƒ…å†µä»¥åŠå…¶ä»–é‡‡ç”¨ç©ºé—´å˜åŒ–å‚æ•°çš„è®¡ç®—æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æ–¹æ³•æœ‰æ˜¾è‘—çš„å®šæ€§å’Œå®šé‡æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å±•å¼€æ¡†æ¶è¢«æ‰©å±•åˆ°æ€»å¹¿ä¹‰å˜åŒ–ï¼ˆTGVï¼‰çš„æƒ…å†µã€‚</li>
<li>ç»“åˆCNNæ¨æ–­ç©ºé—´å˜åŒ–çš„TGVå‚æ•°ä¸å±•å¼€ç®—æ³•æ–¹æ¡ˆã€‚</li>
<li>ä¸¤ä¸ªå­ç½‘ç»œä»¥ç›‘ç£æ–¹å¼è¿›è¡Œç«¯åˆ°ç«¯çš„è”åˆè®­ç»ƒã€‚</li>
<li>CNNå­¦ä¹ è®¡ç®—ä½¿é‡å»ºå›¾åƒå°½å¯èƒ½æ¥è¿‘çœŸå®å€¼çš„å‚æ•°ã€‚</li>
<li>æ•°å€¼ç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å›¾åƒå»å™ªå’ŒMRIé‡å»ºä¸Šæœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>æ¨æ–­å‡ºçš„ç©ºé—´å˜åŒ–å‚æ•°åœ°å›¾åœ¨å›¾åƒè¾¹ç¼˜é™„è¿‘æœ‰ä¸€è‡´çš„ç»“æ„ã€‚</li>
<li>ä¸€é˜¶TGVå‚æ•°çš„æƒé‡å…·æœ‰ä¸‰è¾¹ç»“æ„ï¼Œè€ŒäºŒé˜¶TGVå‚æ•°çš„æƒé‡åœ¨è¾¹ç¼˜å‘¨å›´è¾¾åˆ°å°å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16532">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1d5594023d1ecc0534277e7d086c1d5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a4a7020a1b09c080179c2efdb7b35e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a12572c9996081246446bc259d4dea8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MobileViM-A-Light-weight-and-Dimension-independent-Vision-Mamba-for-3D-Medical-Image-Analysis"><a href="#MobileViM-A-Light-weight-and-Dimension-independent-Vision-Mamba-for-3D-Medical-Image-Analysis" class="headerlink" title="MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D   Medical Image Analysis"></a>MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D   Medical Image Analysis</h2><p><strong>Authors:Wei Dai, Jun Liu</strong></p>
<p>Efficient evaluation of three-dimensional (3D) medical images is crucial for diagnostic and therapeutic practices in healthcare. Recent years have seen a substantial uptake in applying deep learning and computer vision to analyse and interpret medical images. Traditional approaches, such as convolutional neural networks (CNNs) and vision transformers (ViTs), face significant computational challenges, prompting the need for architectural advancements. Recent efforts have led to the introduction of novel architectures like the &#96;&#96;Mambaâ€™â€™ model as alternative solutions to traditional CNNs or ViTs. The Mamba model excels in the linear processing of one-dimensional data with low computational demands. However, Mambaâ€™s potential for 3D medical image analysis remains underexplored and could face significant computational challenges as the dimension increases. This manuscript presents MobileViM, a streamlined architecture for efficient segmentation of 3D medical images. In the MobileViM network, we invent a new dimension-independent mechanism and a dual-direction traversing approach to incorporate with a vision-Mamba-based framework. MobileViM also features a cross-scale bridging technique to improve efficiency and accuracy across various medical imaging modalities. With these enhancements, MobileViM achieves segmentation speeds exceeding 90 frames per second (FPS) on a single graphics processing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster than the state-of-the-art deep learning models for processing 3D images with the same computational resources. In addition, experimental evaluations demonstrate that MobileViM delivers superior performance, with Dice similarity scores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024, ATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses existing models. </p>
<blockquote>
<p>åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸï¼Œå¯¹ä¸‰ç»´ï¼ˆ3Dï¼‰åŒ»å­¦å›¾åƒçš„æœ‰æ•ˆè¯„ä¼°å¯¹äºè¯Šæ–­å’Œæ²»ç–—å®è·µè‡³å…³é‡è¦ã€‚è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰åœ¨åŒ»å­¦å›¾åƒåˆ†æå’Œè§£é‡Šæ–¹é¢çš„åº”ç”¨å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ä¼ ç»Ÿæ–¹æ³•ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ï¼Œé¢ä¸´ç€é‡å¤§çš„è®¡ç®—æŒ‘æˆ˜ï¼Œè¿™ä¿ƒä½¿äº†æ¶æ„å‘å±•çš„å¿…è¦æ€§ã€‚æœ€è¿‘çš„åŠªåŠ›å¯¼è‡´äº†è¯¸å¦‚â€œMambaâ€æ¨¡å‹ç­‰æ–°æ¶æ„çš„å¼•å…¥ï¼Œä½œä¸ºä¼ ç»ŸCNNæˆ–ViTsçš„æ›¿ä»£è§£å†³æ–¹æ¡ˆã€‚Mambaæ¨¡å‹åœ¨å¤„ç†ä¸€ç»´æ•°æ®æ—¶å…·æœ‰å‡ºè‰²çš„çº¿æ€§å¤„ç†èƒ½åŠ›ä¸”è®¡ç®—éœ€æ±‚è¾ƒä½ã€‚ç„¶è€Œï¼ŒMambaåœ¨ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†ææ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œéšç€ç»´åº¦çš„å¢åŠ ï¼Œå¯èƒ½ä¼šé¢ä¸´é‡å¤§çš„è®¡ç®—æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†MobileViMï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé«˜æ•ˆåˆ†å‰²ä¸‰ç»´åŒ»å­¦å›¾åƒçš„ç®€åŒ–æ¶æ„ã€‚åœ¨MobileViMç½‘ç»œä¸­ï¼Œæˆ‘ä»¬å‘æ˜äº†ä¸€ç§æ–°çš„ä¸ç»´åº¦æ— å…³çš„æœºåˆ¶ä»¥åŠä¸€ç§åŒå‘éå†æ–¹æ³•ä¸åŸºäºè§†è§‰Mambaçš„æ¡†æ¶ç›¸ç»“åˆã€‚MobileViMè¿˜é‡‡ç”¨è·¨å°ºåº¦æ¡¥æ¢æŠ€æœ¯ï¼Œä»¥æé«˜å„ç§åŒ»å­¦æˆåƒæ¨¡å¼çš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§ã€‚é€šè¿‡è¿™äº›å¢å¼ºåŠŸèƒ½ï¼ŒMobileViMåœ¨å•ä¸ªå›¾å½¢å¤„ç†å•å…ƒï¼ˆå³NVIDIA RTX 4090ï¼‰ä¸Šå®ç°äº†è¶…è¿‡æ¯ç§’90å¸§ï¼ˆFPSï¼‰çš„åˆ†å‰²é€Ÿåº¦ã€‚è¿™ä¸€æ€§èƒ½æ¯”ä½¿ç”¨ç›¸åŒè®¡ç®—èµ„æºçš„å¤„ç†ä¸‰ç»´å›¾åƒçš„æœ€å…ˆè¿›æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¸§ç‡å¿«24 FPSä»¥ä¸Šã€‚æ­¤å¤–ï¼Œå®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒMobileViMå…·æœ‰å“è¶Šçš„æ€§èƒ½ï¼Œåœ¨PENGWINã€BraTS2024ã€ATLASå’ŒToothfairy2æ•°æ®é›†ä¸Šçš„Diceç›¸ä¼¼åº¦å¾—åˆ†åˆ†åˆ«è¾¾åˆ°92.72%ã€86.69%ã€80.46%å’Œ77.43%ï¼Œæ˜¾è‘—è¶…è¿‡äº†ç°æœ‰æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13524v4">PDF</a> The corresponding author disagrees with the manuscript submitted to   arXiv</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†æçš„æ–°å‹é«˜æ•ˆæ¶æ„MobileViMã€‚å®ƒé€šè¿‡å¼•å…¥ç»´åº¦ç‹¬ç«‹æœºåˆ¶ã€åŒå‘éå†æ–¹æ³•å’Œè·¨å°ºåº¦æ¡¥æ¥æŠ€æœ¯ï¼Œå®ç°äº†å¿«é€Ÿè€Œç²¾ç¡®çš„åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚ç›¸è¾ƒäºå…¶ä»–é¡¶å°–æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒMobileViMåœ¨å•ä¸€GPUä¸Šçš„è¿è¡Œé€Ÿåº¦æå‡äº†è¶…è¿‡24FPSï¼Œå¹¶ä¸”åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„Diceç›¸ä¼¼åº¦å¾—åˆ†è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MobileViMæ˜¯ä¸€ç§é’ˆå¯¹ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†æçš„é«˜æ•ˆæ¶æ„ã€‚</li>
<li>MobileViMå¼•å…¥äº†ç»´åº¦ç‹¬ç«‹æœºåˆ¶ï¼Œé€‚åº”ä¸åŒç»´åº¦çš„æ•°æ®å¤„ç†ã€‚</li>
<li>é€šè¿‡åŒå‘éå†æ–¹æ³•ï¼ŒMobileViMæå‡äº†å›¾åƒå¤„ç†çš„æ•ˆç‡ã€‚</li>
<li>è·¨å°ºåº¦æ¡¥æ¥æŠ€æœ¯æé«˜äº†MobileViMåœ¨ä¸åŒåŒ»å­¦å½±åƒæ¨¡æ€ä¸‹çš„æ€§èƒ½å’Œå‡†ç¡®æ€§ã€‚</li>
<li>MobileViMå®ç°äº†è¶…è¿‡90FPSçš„åˆ†å‰²é€Ÿåº¦ï¼Œåœ¨å•ä¸€GPUä¸Šçš„æ€§èƒ½è¶…è¶Šç°æœ‰æ¨¡å‹ã€‚</li>
<li>MobileViMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„Diceç›¸ä¼¼åº¦å¾—åˆ†è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13524">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a19da823a39016902a1c548a5c88342b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b8d590b505813b80d6e6b5e2bad344e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5082a48f077c477e48276a57b6fe17b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d40734e73903ff1d635e972ebebfdebd.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Game-Theoretic-Defenses-for-Robust-Conformal-Prediction-Against-Adversarial-Attacks-in-Medical-Imaging"><a href="#Game-Theoretic-Defenses-for-Robust-Conformal-Prediction-Against-Adversarial-Attacks-in-Medical-Imaging" class="headerlink" title="Game-Theoretic Defenses for Robust Conformal Prediction Against   Adversarial Attacks in Medical Imaging"></a>Game-Theoretic Defenses for Robust Conformal Prediction Against   Adversarial Attacks in Medical Imaging</h2><p><strong>Authors:Rui Luo, Jie Bao, Zhixin Zhou, Chuangyin Dang</strong></p>
<p>Adversarial attacks pose significant threats to the reliability and safety of deep learning models, especially in critical domains such as medical imaging. This paper introduces a novel framework that integrates conformal prediction with game-theoretic defensive strategies to enhance model robustness against both known and unknown adversarial perturbations. We address three primary research questions: constructing valid and efficient conformal prediction sets under known attacks (RQ1), ensuring coverage under unknown attacks through conservative thresholding (RQ2), and determining optimal defensive strategies within a zero-sum game framework (RQ3). Our methodology involves training specialized defensive models against specific attack types and employing maximum and minimum classifiers to aggregate defenses effectively. Extensive experiments conducted on the MedMNIST datasets, including PathMNIST, OrganAMNIST, and TissueMNIST, demonstrate that our approach maintains high coverage guarantees while minimizing prediction set sizes. The game-theoretic analysis reveals that the optimal defensive strategy often converges to a singular robust model, outperforming uniform and simple strategies across all evaluated datasets. This work advances the state-of-the-art in uncertainty quantification and adversarial robustness, providing a reliable mechanism for deploying deep learning models in adversarial environments. </p>
<blockquote>
<p>å¯¹æŠ—æ€§æ”»å‡»å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¯é æ€§å’Œå®‰å…¨æ€§æ„æˆé‡å¤§å¨èƒï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—æˆåƒç­‰å…³é”®é¢†åŸŸã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†åˆè§„æ€§é¢„æµ‹ä¸åšå¼ˆç†è®ºé˜²å¾¡ç­–ç•¥ç›¸ç»“åˆï¼Œä»¥æé«˜æ¨¡å‹å¯¹å·²çŸ¥å’ŒæœªçŸ¥å¯¹æŠ—æ€§æ‰°åŠ¨çš„é²æ£’æ€§ã€‚æˆ‘ä»¬è§£å†³äº†ä¸‰ä¸ªä¸»è¦çš„ç ”ç©¶é—®é¢˜ï¼šåœ¨å·²çŸ¥æ”»å‡»ä¸‹æ„å»ºæœ‰æ•ˆä¸”é«˜æ•ˆçš„åˆè§„æ€§é¢„æµ‹é›†ï¼ˆRQ1ï¼‰ï¼Œé€šè¿‡ä¿å®ˆé˜ˆå€¼åŒ–ç¡®ä¿åœ¨æœªçŸ¥æ”»å‡»ä¸‹çš„è¦†ç›–ç‡ï¼ˆRQ2ï¼‰ï¼Œä»¥åŠåœ¨é›¶å’Œåšå¼ˆæ¡†æ¶å†…ç¡®å®šæœ€ä½³é˜²å¾¡ç­–ç•¥ï¼ˆRQ3ï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬é’ˆå¯¹ç‰¹å®šæ”»å‡»ç±»å‹è®­ç»ƒä¸“é—¨çš„é˜²å¾¡æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨æœ€å¤§å’Œæœ€å°åˆ†ç±»å™¨æœ‰æ•ˆåœ°èšåˆé˜²å¾¡æªæ–½ã€‚åœ¨MedMNISTæ•°æ®é›†ï¼ˆåŒ…æ‹¬PathMNISTã€OrganAMNISTå’ŒTissueMNISTï¼‰ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒé«˜è¦†ç›–ç‡ä¿è¯çš„åŒæ—¶ï¼Œæœ€å°åŒ–äº†é¢„æµ‹é›†çš„å¤§å°ã€‚åšå¼ˆç†è®ºåˆ†æè¡¨æ˜ï¼Œæœ€ä½³é˜²å¾¡ç­–ç•¥é€šå¸¸ä¼šæ”¶æ•›åˆ°ä¸€ä¸ªå•ä¸€çš„ç¨³å¥æ¨¡å‹ï¼Œåœ¨æ‰€æœ‰è¯„ä¼°çš„æ•°æ®é›†ä¸Šï¼Œå…¶æ€§èƒ½éƒ½ä¼˜äºå‡åŒ€å’Œç®€å•ç­–ç•¥ã€‚è¿™é¡¹å·¥ä½œåœ¨ä¸ç¡®å®šæ€§çš„é‡åŒ–å’Œå¯¹æŠ—ç¨³å¥æ€§æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œä¸ºåœ¨æ•Œå¯¹ç¯å¢ƒä¸­éƒ¨ç½²æ·±åº¦å­¦ä¹ æ¨¡å‹æä¾›äº†å¯é çš„æœºåˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04376v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬è®ºæ–‡æå‡ºä¸€ç§æ–°å‹æ¡†æ¶ï¼Œç»“åˆä¿å½¢é¢„æµ‹å’Œæ¸¸æˆç†è®ºé˜²å¾¡ç­–ç•¥ï¼Œæé«˜æ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹æŠ—å·²çŸ¥å’ŒæœªçŸ¥å¯¹æŠ—æ€§æ‰°åŠ¨çš„ç¨³å¥æ€§ã€‚ç ”ç©¶è§£å†³ä¸‰ä¸ªä¸»è¦é—®é¢˜ï¼šåœ¨å·²çŸ¥æ”»å‡»ä¸‹æ„å»ºæœ‰æ•ˆä¸”é«˜æ•ˆçš„ä¿å½¢é¢„æµ‹é›†ã€é€šè¿‡ä¿å®ˆé˜ˆå€¼åŒ–ç¡®ä¿æœªçŸ¥æ”»å‡»ä¸‹çš„è¦†ç›–ï¼Œä»¥åŠåœ¨é›¶å’Œåšå¼ˆæ¡†æ¶å†…ç¡®å®šæœ€ä½³é˜²å¾¡ç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜è¦†ç›–ä¿è¯çš„åŒæ—¶å‡å°äº†é¢„æµ‹é›†å¤§å°ã€‚æ¸¸æˆç†è®ºåˆ†ææ˜¾ç¤ºï¼Œæœ€ä½³é˜²å¾¡ç­–ç•¥é€šå¸¸æ”¶æ•›äºå•ä¸€ç¨³å¥æ¨¡å‹ï¼Œåœ¨æ‰€æœ‰è¯„ä¼°æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºå‡åŒ€å’Œç®€å•ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æŠ—æ”»å‡»å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¯é æ€§å’Œå®‰å…¨æ€§æ„æˆé‡å¤§å¨èƒï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—æˆåƒç­‰å…³é”®é¢†åŸŸã€‚</li>
<li>è®ºæ–‡æå‡ºç»“åˆä¿å½¢é¢„æµ‹å’Œæ¸¸æˆç†è®ºé˜²å¾¡ç­–ç•¥çš„æ–°å‹æ¡†æ¶ï¼Œå¢å¼ºæ¨¡å‹å¯¹æŠ—å·²çŸ¥å’ŒæœªçŸ¥å¯¹æŠ—æ€§æ‰°åŠ¨çš„ç¨³å¥æ€§ã€‚</li>
<li>ç ”ç©¶è§£å†³ä¸‰ä¸ªä¸»è¦é—®é¢˜ï¼šæ„å»ºä¿å½¢é¢„æµ‹é›†ã€ç¡®ä¿æœªçŸ¥æ”»å‡»ä¸‹çš„è¦†ç›–ä»¥åŠç¡®å®šæœ€ä½³é˜²å¾¡ç­–ç•¥ã€‚</li>
<li>é€šè¿‡åœ¨MedMNISTæ•°æ®é›†ï¼ˆåŒ…æ‹¬PathMNISTã€OrganAMNISTå’ŒTissueMNISTï¼‰ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œè¯æ˜è¯¥æ–¹æ³•åœ¨ä¿æŒé«˜è¦†ç›–ä¿è¯çš„åŒæ—¶å‡å°é¢„æµ‹é›†å¤§å°ã€‚</li>
<li>æ¸¸æˆç†è®ºåˆ†ææ˜¾ç¤ºï¼Œæœ€ä½³é˜²å¾¡ç­–ç•¥é€šå¸¸æ”¶æ•›äºå•ä¸€ç¨³å¥æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå‡åŒ€å’Œç®€å•ç­–ç•¥ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†ä¸ç¡®å®šæ€§é‡åŒ–çš„æ°´å¹³ï¼Œå¹¶å¢å¼ºäº†å¯¹æŠ—æ€§ç¯å¢ƒä¸‹çš„æ¨¡å‹ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04376">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-88cfc01902b9e0d46e421500f63c4a7b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-68cabaa989d4abdac45042663f3d77ba.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LDPM-Towards-undersampled-MRI-reconstruction-with-MR-VAE-and-Latent-Diffusion-Prior"><a href="#LDPM-Towards-undersampled-MRI-reconstruction-with-MR-VAE-and-Latent-Diffusion-Prior" class="headerlink" title="LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent   Diffusion Prior"></a>LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent   Diffusion Prior</h2><p><strong>Authors:Xingjian Tang, Jingwei Guan, Linge Li, Ran Shi, Youmei Zhang, Mengye Lyu, Li Yan</strong></p>
<p>Diffusion models, as powerful generative models, have found a wide range of applications and shown great potential in solving image reconstruction problems. Some works attempted to solve MRI reconstruction with diffusion models, but these methods operate directly in pixel space, leading to higher computational costs for optimization and inference. Latent diffusion models, pre-trained on natural images with rich visual priors, are expected to solve the high computational cost problem in MRI reconstruction by operating in a lower-dimensional latent space. However, direct application to MRI reconstruction faces three key challenges: (1) absence of explicit control mechanisms for medical fidelity, (2) domain gap between natural images and MR physics, and (3) undefined data consistency in latent space. To address these challenges, a novel Latent Diffusion Prior-based undersampled MRI reconstruction (LDPM) method is proposed. Our LDPM framework addresses these challenges by: (1) a sketch-guided pipeline with a two-step reconstruction strategy, which balances perceptual quality and anatomical fidelity, (2) an MRI-optimized VAE (MR-VAE), which achieves an improvement of approximately 3.92 dB in PSNR for undersampled MRI reconstruction compared to that with SD-VAE \cite{sd}, and (3) Dual-Stage Sampler, a modified version of spaced DDPM sampler, which enforces high-fidelity reconstruction in the latent space. Experiments on the fastMRI dataset\cite{fastmri} demonstrate the state-of-the-art performance of the proposed method and its robustness across various scenarios. The effectiveness of each module is also verified through ablation experiments. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ä½œä¸ºå¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨å›¾åƒé‡å»ºé—®é¢˜ä¸­å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ï¼Œå¹¶æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ä¸€äº›ç ”ç©¶å°è¯•ä½¿ç”¨æ‰©æ•£æ¨¡å‹è§£å†³MRIé‡å»ºé—®é¢˜ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨åƒç´ ç©ºé—´ç›´æ¥æ“ä½œï¼Œå¯¼è‡´ä¼˜åŒ–å’Œæ¨ç†çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå…·æœ‰ä¸°å¯Œçš„è§†è§‰å…ˆéªŒï¼Œæœ‰æœ›é€šè¿‡ä½ç»´æ½œåœ¨ç©ºé—´è§£å†³MRIé‡å»ºä¸­çš„é«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚ç„¶è€Œï¼Œç›´æ¥åº”ç”¨äºMRIé‡å»ºé¢ä¸´ä¸‰ä¸ªå…³é”®æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰åŒ»å­¦ä¿çœŸåº¦çš„æ§åˆ¶æœºåˆ¶ç¼ºå¤±ï¼Œï¼ˆ2ï¼‰è‡ªç„¶å›¾åƒä¸MRç‰©ç†ä¹‹é—´çš„é¢†åŸŸå·®è·ï¼Œï¼ˆ3ï¼‰æ½œåœ¨ç©ºé—´ä¸­çš„æ•°æ®ä¸€è‡´æ€§æœªå®šä¹‰ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£å…ˆéªŒçš„æ¬ é‡‡æ ·MRIé‡å»ºï¼ˆLDPMï¼‰æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„LDPMæ¡†æ¶é€šè¿‡ä»¥ä¸‹æ–¹å¼åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰ä¸€ä¸ªè‰å›¾å¼•å¯¼çš„æµç¨‹ï¼Œé‡‡ç”¨ä¸¤æ­¥é‡å»ºç­–ç•¥ï¼Œå¹³è¡¡æ„ŸçŸ¥è´¨é‡å’Œè§£å‰–ä¿çœŸåº¦ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªé’ˆå¯¹MRIä¼˜åŒ–çš„VAEï¼ˆMR-VAEï¼‰ï¼Œåœ¨æ¬ é‡‡æ ·MRIé‡å»ºçš„PSNRä¸Šç›¸æ¯”SD-VAE \cite{sd}æé«˜äº†çº¦3.92 dBï¼›ï¼ˆ3ï¼‰åŒé˜¶æ®µé‡‡æ ·å™¨ï¼Œè¿™æ˜¯ä¸€ä¸ªæ”¹è¿›çš„é—´éš”DDPMé‡‡æ ·å™¨ï¼Œå®ƒåœ¨æ½œåœ¨ç©ºé—´ä¸­å¼ºåˆ¶é«˜ä¿çœŸé‡å»ºã€‚åœ¨fastMRIæ•°æ®é›†\cite{fastmri}ä¸Šçš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„å…ˆè¿›æ€§èƒ½å’Œåœ¨å„ç§åœºæ™¯ä¸­çš„ç¨³å¥æ€§ã€‚æ¯ä¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ä¹Ÿé€šè¿‡æ¶ˆèå®éªŒå¾—åˆ°äº†éªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02951v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨è§£å†³MRIé‡å»ºé—®é¢˜æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ä»é¢ä¸´æŒ‘æˆ˜ã€‚æ–°å‹LDPMæ–¹æ³•é€šè¿‡è‰å›¾å¼•å¯¼ç®¡é“ã€MRIä¼˜åŒ–çš„VAEå’ŒåŒé˜¶æ®µé‡‡æ ·å™¨ç­‰åˆ›æ–°æ‰‹æ®µï¼Œè§£å†³äº†è¿™äº›é—®é¢˜ï¼Œå¹¶åœ¨fastMRIæ•°æ®é›†ä¸Šå±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒé‡å»ºé—®é¢˜ä¸­å±•ç°äº†å·¨å¤§çš„æ½œåŠ›ã€‚</li>
<li>ç›´æ¥åœ¨åƒç´ ç©ºé—´æ“ä½œMRIé‡å»ºå¯¼è‡´é«˜è®¡ç®—æˆæœ¬ã€‚</li>
<li>æ½œåœ¨æ‰©æ•£æ¨¡å‹æœŸæœ›é€šè¿‡åœ¨ä½ç»´æ½œåœ¨ç©ºé—´æ“ä½œæ¥è§£å†³é«˜è®¡ç®—æˆæœ¬é—®é¢˜ã€‚</li>
<li>LDPMæ–¹æ³•é€šè¿‡è‰å›¾å¼•å¯¼ç®¡é“ã€MRIä¼˜åŒ–çš„VAEå’ŒåŒé˜¶æ®µé‡‡æ ·å™¨è§£å†³æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨MRIé‡å»ºä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>LDPMæ–¹æ³•åœ¨fastMRIæ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†åœ¨å„ç§åœºæ™¯ä¸‹çš„ç¨³å¥æ€§ã€‚</li>
<li>LDPMæ–¹æ³•é€šè¿‡å¹³è¡¡æ„ŸçŸ¥è´¨é‡å’Œè§£å‰–ä¿çœŸåº¦æ¥æé«˜MRIé‡å»ºçš„è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.02951">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e22c7fa8d8fb4f5ff7f28d3dda1da45e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5144b1cf28ed6a854c2c4fbbdfb72cea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5789dfc0f7479c776ca018ae0a47c6e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50cdb7f71b9ee820a4008700bf2bd88c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ac9a4d23757bfcf5918478f08c6c34a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0251eac9ec15658da969639324cfbb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7d307b2c4d7b4c57391a62c837dee76.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Enhancing-Multimodal-Medical-Image-Classification-using-Cross-Graph-Modal-Contrastive-Learning"><a href="#Enhancing-Multimodal-Medical-Image-Classification-using-Cross-Graph-Modal-Contrastive-Learning" class="headerlink" title="Enhancing Multimodal Medical Image Classification using Cross-Graph   Modal Contrastive Learning"></a>Enhancing Multimodal Medical Image Classification using Cross-Graph   Modal Contrastive Learning</h2><p><strong>Authors:Jun-En Ding, Chien-Chin Hsu, Chi-Hsiang Chu, Shuqiang Wang, Feng Liu</strong></p>
<p>The classification of medical images is a pivotal aspect of disease diagnosis, often enhanced by deep learning techniques. However, traditional approaches typically focus on unimodal medical image data, neglecting the integration of diverse non-image patient data. This paper proposes a novel Cross-Graph Modal Contrastive Learning (CGMCL) framework for multimodal structured data from different data domains to improve medical image classification. The model effectively integrates both image and non-image data by constructing cross-modality graphs and leveraging contrastive learning to align multimodal features in a shared latent space. An inter-modality feature scaling module further optimizes the representation learning process by reducing the gap between heterogeneous modalities. The proposed approach is evaluated on two datasets: a Parkinsonâ€™s disease (PD) dataset and a public melanoma dataset. Results demonstrate that CGMCL outperforms conventional unimodal methods in accuracy, interpretability, and early disease prediction. Additionally, the method shows superior performance in multi-class melanoma classification. The CGMCL framework provides valuable insights into medical image classification while offering improved disease interpretability and predictive capabilities. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†ç±»æ˜¯ç–¾ç—…è¯Šæ–­çš„å…³é”®ç¯èŠ‚ï¼Œé€šå¸¸å¯ä»¥é€šè¿‡æ·±åº¦å­¦ä¹ æŠ€æœ¯å¾—åˆ°å¢å¼ºã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ–¹æ³•ä¸»è¦å…³æ³¨å•æ¨¡æ€åŒ»å­¦å›¾åƒæ•°æ®ï¼Œå¿½ç•¥äº†ä¸åŒéå›¾åƒæ‚£è€…æ•°æ®çš„æ•´åˆã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è·¨å›¾æ¨¡æ€å¯¹æ¯”å­¦ä¹ ï¼ˆCGMCLï¼‰æ¡†æ¶ï¼Œç”¨äºä»ä¸åŒæ•°æ®åŸŸçš„å¤šæ¨¡æ€ç»“æ„åŒ–æ•°æ®ï¼Œä»¥æé«˜åŒ»å­¦å›¾åƒåˆ†ç±»çš„æ•ˆæœã€‚è¯¥æ¨¡å‹é€šè¿‡æ„å»ºè·¨æ¨¡æ€å›¾å¹¶åˆ©ç”¨å¯¹æ¯”å­¦ä¹ ï¼Œæœ‰æ•ˆåœ°æ•´åˆäº†å›¾åƒå’Œéå›¾åƒæ•°æ®ï¼Œåœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­å¯¹é½å¤šæ¨¡æ€ç‰¹å¾ã€‚è·¨æ¨¡æ€ç‰¹å¾ç¼©æ”¾æ¨¡å—è¿›ä¸€æ­¥ä¼˜åŒ–äº†è¡¨ç¤ºå­¦ä¹ è¿‡ç¨‹ï¼Œç¼©å°äº†ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼šå¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰æ•°æ®é›†å’Œå…¬å…±é»‘è‰²ç´ ç˜¤æ•°æ®é›†ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œæ—©æœŸç–¾ç—…é¢„æµ‹æ–¹é¢ï¼ŒCGMCLä¼˜äºä¼ ç»Ÿçš„å•æ¨¡æ€æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç±»é»‘è‰²ç´ ç˜¤åˆ†ç±»æ–¹é¢ä¹Ÿè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚CGMCLæ¡†æ¶ä¸ºåŒ»å­¦å›¾åƒåˆ†ç±»æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼ŒåŒæ—¶æé«˜äº†ç–¾ç—…å¯è§£é‡Šæ€§å’Œé¢„æµ‹èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17494v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è·¨å›¾æ¨¡æ€å¯¹æ¯”å­¦ä¹ ï¼ˆCGMCLï¼‰æ¡†æ¶ï¼Œç”¨äºå¤šæ¨¡æ€ç»“æ„åŒ–æ•°æ®çš„è·¨åŸŸèåˆï¼Œä»¥æé«˜åŒ»å­¦å›¾åƒåˆ†ç±»çš„å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºè·¨æ¨¡æ€å›¾å¹¶åˆ©ç”¨å¯¹æ¯”å­¦ä¹ ï¼Œåœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­å¯¹é½å¤šæ¨¡æ€ç‰¹å¾ï¼Œå®ç°å›¾åƒä¸éå›¾åƒæ•°æ®çš„èåˆã€‚è¯¥æ–¹æ³•ä¼˜åŒ–äº†å¤šæ¨¡æ€ç‰¹å¾çš„è¡¨ç¤ºå­¦ä¹ è¿‡ç¨‹ï¼Œå‡å°‘äº†ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¯æ˜ï¼Œä¸å¸¸è§„çš„å•æ¨¡æ€æ–¹æ³•ç›¸æ¯”ï¼ŒCGmclåœ¨å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œæ—©æœŸç–¾ç—…é¢„æµ‹æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤šç±»é»‘è‰²ç´ ç˜¤åˆ†ç±»æ–¹é¢ï¼ŒCGmclæ¡†æ¶æä¾›äº†å®è´µçš„åŒ»å­¦å›¾åƒåˆ†ç±»æ´å¯ŸåŠ›ï¼Œæé«˜äº†ç–¾ç—…å¯è§£é‡Šæ€§å’Œé¢„æµ‹èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†ç±»æ˜¯ç–¾ç—…è¯Šæ–­çš„é‡è¦ç¯èŠ‚ï¼Œè€Œæ·±åº¦å­¦ä¹ æŠ€æœ¯èƒ½å¢å¼ºå…¶å‡†ç¡®æ€§ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦å…³æ³¨å•æ¨¡æ€åŒ»å­¦å›¾åƒæ•°æ®ï¼Œå¿½ç•¥äº†ä¸åŒéå›¾åƒæ‚£è€…æ•°æ®çš„æ•´åˆã€‚</li>
<li>æå‡ºçš„Cross-Graph Modal Contrastive Learning (CGMCL)æ¡†æ¶æ—¨åœ¨æ•´åˆå¤šæ¨¡æ€ç»“æ„åŒ–æ•°æ®ï¼Œæé«˜åŒ»å­¦å›¾åƒåˆ†ç±»æ•ˆæœã€‚</li>
<li>CGMCLæ¡†æ¶é€šè¿‡æ„å»ºè·¨æ¨¡æ€å›¾ï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ å¯¹é½å¤šæ¨¡æ€ç‰¹å¾ï¼Œå®ç°å›¾åƒä¸éå›¾åƒæ•°æ®çš„èåˆã€‚</li>
<li>CGMCLæ¡†æ¶ä¼˜åŒ–äº†å¤šæ¨¡æ€ç‰¹å¾çš„è¡¨ç¤ºå­¦ä¹ è¿‡ç¨‹ï¼Œç¼©å°äº†ä¸åŒæ¨¡æ€é—´çš„å·®è·ã€‚</li>
<li>åœ¨å¸•é‡‘æ£®æ°ç—…å’Œé»‘è‰²ç´ ç˜¤æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒCGmclåœ¨å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œæ—©æœŸç–¾ç—…é¢„æµ‹æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17494">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b730c93802a67649f6e4688c9e9e6745.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97222ee56ba028819d49009fa2c7ef35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6d9de4484218baee277721f07a825ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd6986fc293845fd61bc9ba737bf16d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f99f055faccc7f1c93c993d5ac326652.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Self-supervised-pre-training-with-diffusion-model-for-few-shot-landmark-detection-in-x-ray-images"><a href="#Self-supervised-pre-training-with-diffusion-model-for-few-shot-landmark-detection-in-x-ray-images" class="headerlink" title="Self-supervised pre-training with diffusion model for few-shot landmark   detection in x-ray images"></a>Self-supervised pre-training with diffusion model for few-shot landmark   detection in x-ray images</h2><p><strong>Authors:Roberto Di Via, Francesca Odone, Vito Paolo Pastore</strong></p>
<p>Deep neural networks have been extensively applied in the medical domain for various tasks, including image classification, segmentation, and landmark detection. However, their application is often hindered by data scarcity, both in terms of available annotations and images. This study introduces a novel application of denoising diffusion probabilistic models (DDPMs) to the landmark detection task, specifically addressing the challenge of limited annotated data in x-ray imaging. Our key innovation lies in leveraging DDPMs for self-supervised pre-training in landmark detection, a previously unexplored approach in this domain. This method enables accurate landmark detection with minimal annotated training data (as few as 50 images), surpassing both ImageNet supervised pre-training and traditional self-supervised techniques across three popular x-ray benchmark datasets. To our knowledge, this work represents the first application of diffusion models for self-supervised learning in landmark detection, which may offer a valuable pre-training approach in few-shot regimes, for mitigating data scarcity. </p>
<blockquote>
<p>æ·±åº¦ç¥ç»ç½‘ç»œå·²åœ¨åŒ»ç–—é¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œç”¨äºå„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€åˆ†å‰²å’Œåœ°æ ‡æ£€æµ‹ã€‚ç„¶è€Œï¼Œå…¶åº”ç”¨å¾€å¾€å—åˆ°æ•°æ®å’Œæ³¨é‡Šå¯ç”¨æ€§çš„é™åˆ¶ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰çš„æ–°åº”ç”¨ï¼Œä¸“é—¨è§£å†³Xå°„çº¿æˆåƒä¸­æ³¨é‡Šæ•°æ®æœ‰é™æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ä¸»è¦åˆ›æ–°ä¹‹å¤„åœ¨äºï¼Œåˆ©ç”¨DDPMsè¿›è¡Œåœ°æ ‡æ£€æµ‹çš„è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œè¿™æ˜¯è¯¥é¢†åŸŸä¹‹å‰æœªè¢«æ¢ç´¢çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æå°‘çš„æ³¨é‡Šè®­ç»ƒæ•°æ®ï¼ˆä»…50å¼ å›¾åƒï¼‰çš„æƒ…å†µä¸‹å®ç°å‡†ç¡®çš„åœ°æ ‡æ£€æµ‹ï¼Œè¶…è¶Šäº†ImageNetç›‘ç£é¢„è®­ç»ƒå’Œä¼ ç»Ÿè‡ªç›‘ç£æŠ€æœ¯åœ¨ä¸‰ä¸ªæµè¡Œçš„Xå°„çº¿åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œä»£è¡¨äº†æ‰©æ•£æ¨¡å‹åœ¨åœ°æ ‡æ£€æµ‹è‡ªç›‘ç£å­¦ä¹ ä¸­çš„é¦–æ¬¡åº”ç”¨ï¼Œè¿™å¯èƒ½ä¸ºç¼“è§£æ•°æ®ç¨€ç¼ºçš„å°‘é‡æ¨¡å¼æä¾›æœ‰ä»·å€¼çš„é¢„è®­ç»ƒæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.18125v3">PDF</a> Accepted at WACV 2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé¢†åŸŸä¸­æ·±åº¦ç¥ç»ç½‘ç»œçš„åº”ç”¨å¹¿æ³›ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€åˆ†å‰²å’Œå…³é”®ç‚¹æ£€æµ‹ç­‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œæ•°æ®ç¨€ç¼ºï¼ˆåŒ…æ‹¬å¯ç”¨çš„æ³¨é‡Šå’Œå›¾åƒï¼‰å¸¸å¸¸é™åˆ¶å…¶åº”ç”¨ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å°†å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰åº”ç”¨äºåŒ»å­¦å›¾åƒä¸­çš„å…³é”®ç‚¹æ£€æµ‹ä»»åŠ¡ï¼Œè§£å†³æ ‡æ³¨æ•°æ®æœ‰é™çš„é—®é¢˜ã€‚ç ”ç©¶åˆ›æ–°ä¹‹å¤„åœ¨äºåˆ©ç”¨DDPMsè¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œæ­¤æ–¹æ³•åœ¨å°‘é‡æ ‡æ³¨æ•°æ®ï¼ˆä»…50å¼ å›¾åƒï¼‰çš„æƒ…å†µä¸‹å³å¯å®ç°å‡†ç¡®çš„å…³é”®ç‚¹æ£€æµ‹ï¼Œå¹¶åœ¨ä¸‰ä¸ªæµè¡Œçš„Xå°„çº¿åŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶Šäº†ImageNetç›‘ç£é¢„è®­ç»ƒå’Œä¼ ç»Ÿè‡ªç›‘ç£æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å°†å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰åº”ç”¨äºåŒ»å­¦å›¾åƒä¸­çš„å…³é”®ç‚¹æ£€æµ‹ä»»åŠ¡ã€‚</li>
<li>DDPMsèƒ½æœ‰æ•ˆè§£å†³åŒ»å­¦å›¾åƒä¸­æ ‡æ³¨æ•°æ®æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨DDPMsåœ¨å°‘é‡æ ‡æ³¨æ•°æ®ä¸‹å®ç°å‡†ç¡®çš„å…³é”®ç‚¹æ£€æµ‹ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªæµè¡Œçš„Xå°„çº¿åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„é¢„è®­ç»ƒæŠ€æœ¯ã€‚</li>
<li>æ­¤ç ”ç©¶æ˜¯é¦–æ¬¡å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºåŒ»å­¦å›¾åƒå…³é”®ç‚¹æ£€æµ‹ä¸­çš„è‡ªç›‘ç£å­¦ä¹ ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºæœªæ¥è§£å†³åŒ»å­¦å›¾åƒæ•°æ®ç¨€ç¼ºé—®é¢˜æä¾›äº†æ–°çš„æ€è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.18125">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4fe8ec52f131a12305c13150eef9066.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bdf8299076cafa7e4723193ef80f93d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a569e2988b76382c74c267bebcb93089.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e059e518c4b2016e7c6a59a420b98309.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="A-Survey-of-Deep-Learning-based-Radiology-Report-Generation-Using-Multimodal-Data"><a href="#A-Survey-of-Deep-Learning-based-Radiology-Report-Generation-Using-Multimodal-Data" class="headerlink" title="A Survey of Deep Learning-based Radiology Report Generation Using   Multimodal Data"></a>A Survey of Deep Learning-based Radiology Report Generation Using   Multimodal Data</h2><p><strong>Authors:Xinyi Wang, Grazziela Figueredo, Ruizhe Li, Wei Emma Zhang, Weitong Chen, Xin Chen</strong></p>
<p>Automatic radiology report generation can alleviate the workload for physicians and minimize regional disparities in medical resources, therefore becoming an important topic in the medical image analysis field. It is a challenging task, as the computational model needs to mimic physicians to obtain information from multi-modal input data (i.e., medical images, clinical information, medical knowledge, etc.), and produce comprehensive and accurate reports. Recently, numerous works have emerged to address this issue using deep-learning-based methods, such as transformers, contrastive learning, and knowledge-base construction. This survey summarizes the key techniques developed in the most recent works and proposes a general workflow for deep-learning-based report generation with five main components, including multi-modality data acquisition, data preparation, feature learning, feature fusion and interaction, and report generation. The state-of-the-art methods for each of these components are highlighted. Additionally, we summarize the latest developments in large model-based methods and model explainability, along with public datasets, evaluation methods, current challenges, and future directions in this field. We have also conducted a quantitative comparison between different methods in the same experimental setting. This is the most up-to-date survey that focuses on multi-modality inputs and data fusion for radiology report generation. The aim is to provide comprehensive and rich information for researchers interested in automatic clinical report generation and medical image analysis, especially when using multimodal inputs, and to assist them in developing new algorithms to advance the field. </p>
<blockquote>
<p>è‡ªåŠ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆèƒ½å¤Ÿå‡è½»åŒ»ç”Ÿçš„å·¥ä½œé‡ï¼Œå¹¶æœ€å°åŒ–åŒ»ç–—èµ„æºçš„åŒºåŸŸå·®å¼‚ï¼Œå› æ­¤æˆä¸ºåŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„é‡è¦è¯¾é¢˜ã€‚è¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºè®¡ç®—æ¨¡å‹éœ€è¦æ¨¡ä»¿åŒ»ç”Ÿä»å¤šæ¨¡æ€è¾“å…¥æ•°æ®ï¼ˆä¾‹å¦‚åŒ»å­¦å›¾åƒã€ä¸´åºŠä¿¡æ¯ã€åŒ»å­¦çŸ¥è¯†ç­‰ï¼‰ä¸­è·å¾—ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆå…¨é¢å‡†ç¡®çš„æŠ¥å‘Šã€‚æœ€è¿‘ï¼Œå‡ºç°äº†è®¸å¤šåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä¾‹å¦‚å˜å‹å™¨ã€å¯¹æ¯”å­¦ä¹ å’ŒçŸ¥è¯†åº“æ„å»ºã€‚è¿™ç¯‡ç»¼è¿°æ€»ç»“äº†æœ€è¿‘ä½œå“å¼€å‘çš„å…³é”®æŠ€æœ¯ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„æŠ¥å‘Šç”Ÿæˆçš„ä¸€èˆ¬å·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬äº”ä¸ªä¸»è¦ç»„ä»¶ï¼šå¤šæ¨¡æ€æ•°æ®é‡‡é›†ã€æ•°æ®å‡†å¤‡ã€ç‰¹å¾å­¦ä¹ ã€ç‰¹å¾èåˆå’Œäº¤äº’ä»¥åŠæŠ¥å‘Šç”Ÿæˆã€‚æœ¬æ–‡å¼ºè°ƒäº†è¿™äº›ç»„ä»¶çš„æœ€æ–°å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ€»ç»“äº†åŸºäºå¤§å‹æ¨¡å‹çš„æœ€æ–°å‘å±•å’Œæ¨¡å‹è§£é‡Šæ€§ï¼Œä»¥åŠå…¬å¼€æ•°æ®é›†ã€è¯„ä¼°æ–¹æ³•ã€å½“å‰æŒ‘æˆ˜å’Œè¯¥é¢†åŸŸçš„æœªæ¥å‘å±•æ–¹å‘ã€‚æˆ‘ä»¬è¿˜åœ¨ç›¸åŒçš„å®éªŒç¯å¢ƒä¸­å¯¹ä¸åŒæ–¹æ³•è¿›è¡Œäº†å®šé‡æ¯”è¾ƒã€‚è¿™æ˜¯æœ€æ–°ä¸“æ³¨äºå¤šæ¨¡æ€è¾“å…¥å’Œæ•°æ®èåˆç”¨äºæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„ç»¼è¿°ã€‚æ—¨åœ¨ä¸ºå¯¹è‡ªåŠ¨ä¸´åºŠæŠ¥å‘Šç”Ÿæˆå’ŒåŒ»å­¦å›¾åƒåˆ†ææ„Ÿå…´è¶£çš„ç ”ç©¶äººå‘˜æä¾›å…¨é¢ä¸°å¯Œçš„ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨å¤šæ¨¡æ€è¾“å…¥æ—¶ï¼Œå¹¶å¸®åŠ©ä»–ä»¬å¼€å‘æ–°ç®—æ³•ä»¥æ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.12833v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‡ªåŠ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„é‡è¦æ€§ï¼ŒåŒ…æ‹¬å…¶åœ¨å‡è½»åŒ»ç”Ÿå·¥ä½œé‡ã€ç¼©å°åŒ»ç–—èµ„æºåŒºåŸŸå·®è·æ–¹é¢çš„ä½œç”¨ã€‚æ–‡ç« æŒ‡å‡ºè¿™æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ä»»åŠ¡ï¼Œéœ€è¦è®¡ç®—æ¨¡å‹ä»å¤šæ¨¡æ€è¾“å…¥æ•°æ®ä¸­è·å–ä¿¡æ¯å¹¶ç”Ÿæˆå…¨é¢å‡†ç¡®çš„æŠ¥å‘Šã€‚æœ¬æ–‡æ€»ç»“äº†æœ€è¿‘åˆ©ç”¨æ·±åº¦å­¦ä¹ çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†åŸºäºæ·±åº¦å­¦ä¹ æŠ¥å‘Šçš„é€šç”¨å·¥ä½œæµç¨‹ï¼Œå¹¶è¯¦ç»†ä»‹ç»äº†å„éƒ¨åˆ†çš„å…³é”®æŠ€æœ¯ã€‚åŒæ—¶è¿›è¡Œäº†æœ€æ–°å‘å±•çš„å¤§å‹æ¨¡å‹æ–¹æ³•ã€æ¨¡å‹è§£é‡Šæ€§ç­‰æ–¹é¢çš„æ€»ç»“ã€‚é€šè¿‡åŒä¸€å®éªŒç¯å¢ƒä¸‹ä¸åŒæ–¹æ³•çš„å®šé‡æ¯”è¾ƒï¼Œæœ¬æ–‡æ˜¯æœ€æ–°çš„å…³æ³¨å¤šæ¨¡æ€è¾“å…¥å’Œæ•°æ®èåˆçš„æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„ç»¼è¿°ã€‚æ—¨åœ¨ä¸ºå¯¹è‡ªåŠ¨ä¸´åºŠæŠ¥å‘Šç”Ÿæˆå’ŒåŒ»å­¦å›¾åƒåˆ†ææ„Ÿå…´è¶£çš„å­¦è€…æä¾›å…¨é¢ä¸°å¯Œçš„ä¿¡æ¯ï¼Œå¹¶å¸®åŠ©å¼€å‘æ–°ç®—æ³•æ¨åŠ¨è¯¥é¢†åŸŸçš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è‡ªåŠ¨æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆæ˜¯åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„é‡è¦è¯é¢˜ï¼Œæœ‰åŠ©äºå‡è½»åŒ»ç”Ÿå·¥ä½œé‡å¹¶ç¼©å°åŒ»ç–—èµ„æºå·®è·ã€‚</li>
<li>è¯¥ä»»åŠ¡éœ€è¦è®¡ç®—æ¨¡å‹æ¨¡ä»¿åŒ»ç”Ÿä»å¤šæ¨¡æ€è¾“å…¥æ•°æ®ä¸­è·å–ä¿¡æ¯å¹¶ç”Ÿæˆå‡†ç¡®å…¨é¢çš„æŠ¥å‘Šã€‚</li>
<li>æœ€è¿‘åˆ©ç”¨æ·±åº¦å­¦ä¹ çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒåŒ…æ‹¬transformerã€å¯¹æ¯”å­¦ä¹ å’ŒçŸ¥è¯†åº“æ„å»ºç­‰ã€‚</li>
<li>åŸºäºæ·±åº¦å­¦ä¹ çš„æŠ¥å‘Šç”ŸæˆåŒ…å«äº”ä¸ªä¸»è¦ç»„ä»¶ï¼šå¤šæ¨¡æ€æ•°æ®é‡‡é›†ã€æ•°æ®å‡†å¤‡ã€ç‰¹å¾å­¦ä¹ ã€ç‰¹å¾èåˆä¸äº¤äº’å’ŒæŠ¥å‘Šç”Ÿæˆã€‚</li>
<li>æ€»ç»“äº†å„éƒ¨åˆ†çš„å…³é”®æŠ€æœ¯å’Œæœ€æ–°å‘å±•çš„å¤§å‹æ¨¡å‹æ–¹æ³•ã€æ¨¡å‹è§£é‡Šæ€§ç­‰æ–¹é¢çš„å†…å®¹ã€‚</li>
<li>æ–‡ç« é€šè¿‡å®šé‡æ¯”è¾ƒä¸åŒæ–¹æ³•åœ¨åŒä¸€å®éªŒç¯å¢ƒä¸‹çš„è¡¨ç°ï¼Œæä¾›äº†å…¨é¢çš„ç»¼è¿°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.12833">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bcdf60fbe109ae80517754ec4a52d394.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-989320cdea82334ad73e7b06b76aba87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cf04ac312f75f780d1f425cbaa0b56f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5eb06d594b723fee0065905f8bc415f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Implantable-Adaptive-Cells-A-Novel-Enhancement-for-Pre-Trained-U-Nets-in-Medical-Image-Segmentation"><a href="#Implantable-Adaptive-Cells-A-Novel-Enhancement-for-Pre-Trained-U-Nets-in-Medical-Image-Segmentation" class="headerlink" title="Implantable Adaptive Cells: A Novel Enhancement for Pre-Trained U-Nets   in Medical Image Segmentation"></a>Implantable Adaptive Cells: A Novel Enhancement for Pre-Trained U-Nets   in Medical Image Segmentation</h2><p><strong>Authors:Emil Benedykciuk, Marcin Denkowski, Grzegorz WÃ³jcik</strong></p>
<p>This paper introduces a novel approach to enhance the performance of pre-trained neural networks in medical image segmentation using gradient-based Neural Architecture Search (NAS) methods. We present the concept of Implantable Adaptive Cell (IAC), small modules identified through Partially-Connected DARTS based approach, designed to be injected into the skip connections of an existing and already trained U-shaped model. Unlike traditional NAS methods, our approach refines existing architectures without full retraining. Experiments on four medical datasets with MRI and CT images show consistent accuracy improvements on various U-Net configurations, with segmentation accuracy gain by approximately 5 percentage points across all validation datasets, with improvements reaching up to 11%pt in the best-performing cases. The findings of this study not only offer a cost-effective alternative to the complete overhaul of complex models for performance upgrades but also indicate the potential applicability of our method to other architectures and problem domains. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨åŸºäºæ¢¯åº¦çš„ç¥ç»ç½‘ç»œç»“æ„æœç´¢ï¼ˆNASï¼‰æ–¹æ³•æé«˜é¢„è®­ç»ƒç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ€§èƒ½çš„æ–°å‹æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†å¯æ¤å…¥è‡ªé€‚åº”å•å…ƒï¼ˆIACï¼‰çš„æ¦‚å¿µï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡éƒ¨åˆ†è¿æ¥DARTSæ–¹æ³•è¯†åˆ«çš„å°æ¨¡å—ï¼Œæ—¨åœ¨æ³¨å…¥åˆ°å·²è®­ç»ƒå’Œå­˜åœ¨çš„Uå½¢æ¨¡å‹çš„è·³è¿‡è¿æ¥ä¸­ã€‚ä¸ä¼ ç»Ÿçš„NASæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸è¿›è¡Œå…¨é¢å†è®­ç»ƒçš„æƒ…å†µä¸‹ä¼˜åŒ–ç°æœ‰æ¶æ„ã€‚åœ¨å››ä¸ªåŒ…å«MRIå’ŒCTå›¾åƒçš„åŒ»å­¦æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œåœ¨å„ç§U-Neté…ç½®ä¸Šï¼Œå‡†ç¡®ç‡å¾—åˆ°æŒç»­æé«˜ï¼Œåœ¨æ‰€æœ‰éªŒè¯æ•°æ®é›†ä¸Šçš„åˆ†å‰²å‡†ç¡®ç‡æé«˜çº¦5ä¸ªç™¾åˆ†ç‚¹ï¼Œåœ¨è¡¨ç°æœ€ä½³çš„æ¡ˆä¾‹ä¸­ï¼Œæé«˜å¹…åº¦é«˜è¾¾11ä¸ªç™¾åˆ†ç‚¹ã€‚æœ¬ç ”ç©¶çš„ç»“æœä¸ä»…ä¸ºé€šè¿‡å½»åº•æ”¹é©å¤æ‚æ¨¡å‹æ¥æé«˜æ€§èƒ½æä¾›äº†æˆæœ¬æ•ˆç›Šé«˜çš„æ›¿ä»£æ–¹æ¡ˆï¼Œè¿˜è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨å…¶ä»–æ¶æ„å’Œé—®é¢˜é¢†åŸŸå…·æœ‰æ½œåœ¨é€‚ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.03420v2">PDF</a> </p>
<p><strong>Summary</strong><br>è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨åŸºäºæ¢¯åº¦çš„ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰æ–¹æ³•æé«˜é¢„è®­ç»ƒç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ€§èƒ½çš„æ–°æ–¹æ³•ã€‚é€šè¿‡éƒ¨åˆ†è¿æ¥çš„DARTSæ–¹æ³•ï¼Œè®¾è®¡äº†ä¸€ç§åä¸ºæ¤å…¥å¼è‡ªé€‚åº”å•å…ƒï¼ˆIACï¼‰çš„å°æ¨¡å—ï¼Œå¯æ³¨å…¥å·²è®­ç»ƒçš„Uå½¢æ¨¡å‹çš„è·³è·ƒè¿æ¥ä¸­ã€‚è¯¥æ–¹æ³•åœ¨å››ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨å„ç§U-Neté…ç½®ä¸Šå‡å®ç°äº†ç¨³å®šçš„å‡†ç¡®æ€§æé«˜ï¼Œåˆ†å‰²å‡†ç¡®æ€§å¹³å‡æå‡çº¦5ä¸ªç™¾åˆ†ç‚¹ï¼Œæœ€ä½³æƒ…å†µä¸‹æå‡è¾¾11ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤ç ”ç©¶ä¸ä»…ä¸ºå‡çº§æ€§èƒ½æä¾›äº†å…¨é¢æ¨ç¿»å¤æ‚æ¨¡å‹çš„ä½æˆæœ¬æ›¿ä»£æ–¹æ¡ˆï¼Œè€Œä¸”è¿˜è¡¨æ˜è¯¥æ–¹æ³•åœ¨å…¶ä»–æ¶æ„å’Œé—®é¢˜é¢†åŸŸå…·æœ‰æ½œåœ¨é€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ¢¯åº¦çš„æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„æœç´¢æ–¹æ³•ç”¨äºæ”¹è¿›é¢„è®­ç»ƒç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨åä¸ºæ¤å…¥å¼è‡ªé€‚åº”å•å…ƒï¼ˆIACï¼‰çš„å°æ¨¡å—ï¼Œé€šè¿‡éƒ¨åˆ†è¿æ¥çš„DARTSæ–¹æ³•è®¾è®¡ã€‚</li>
<li>IACæ¨¡å—è¢«æ³¨å…¥åˆ°å·²è®­ç»ƒçš„Uå½¢æ¨¡å‹çš„è·³è·ƒè¿æ¥ä¸­ï¼Œä»¥ä¼˜åŒ–ç°æœ‰æ¶æ„ï¼Œæ— éœ€å®Œå…¨é‡æ–°è®­ç»ƒã€‚</li>
<li>åœ¨å››ä¸ªåŒ»å­¦æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§U-Neté…ç½®ä¸Šéƒ½å®ç°äº†åˆ†å‰²å‡†ç¡®æ€§çš„æ˜¾è‘—æé«˜ã€‚</li>
<li>ä¸ä¼ ç»Ÿçš„NASæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•æ›´ä¸ºé«˜æ•ˆï¼Œæˆæœ¬æ›´ä½ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä»…é€‚ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œè¿˜æœ‰æ½œåŠ›åº”ç”¨äºå…¶ä»–æ¶æ„å’Œé—®é¢˜é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.03420">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f798f474e03f9388e98124e4c83426e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7abc510c8352ce0b3b76c5a9663c443.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a63e3cb301ee86e7620010c4a970d3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-947061309be54a4523eea97c66a07311.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eb2b3fbfc1e8405d2fee2cdf598c5623.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a22b7b93f83c516704e346e177be86ed.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a25fdae49c198c9f5bb31f3c5c34aa9b.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-10  LLMVoX Autoregressive Streaming Text-to-Speech Model for Any LLM
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-716fefef52104778416315e065200599.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-10  Synthetic Data is an Elegant GIFT for Continual Vision-Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29058.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
