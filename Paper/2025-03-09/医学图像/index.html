<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-03-10  RadIR A Scalable Framework for Multi-Grained Medical Image Retrieval   via Radiology Report Mining">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-de552bc34893907d1e9500a7bc2f586a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    78 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-10-更新"><a href="#2025-03-10-更新" class="headerlink" title="2025-03-10 更新"></a>2025-03-10 更新</h1><h2 id="RadIR-A-Scalable-Framework-for-Multi-Grained-Medical-Image-Retrieval-via-Radiology-Report-Mining"><a href="#RadIR-A-Scalable-Framework-for-Multi-Grained-Medical-Image-Retrieval-via-Radiology-Report-Mining" class="headerlink" title="RadIR: A Scalable Framework for Multi-Grained Medical Image Retrieval   via Radiology Report Mining"></a>RadIR: A Scalable Framework for Multi-Grained Medical Image Retrieval   via Radiology Report Mining</h2><p><strong>Authors:Tengfei Zhang, Ziheng Zhao, Chaoyi Wu, Xiao Zhou, Ya Zhang, Yangfeng Wang, Weidi Xie</strong></p>
<p>Developing advanced medical imaging retrieval systems is challenging due to the varying definitions of &#96;similar images’ across different medical contexts. This challenge is compounded by the lack of large-scale, high-quality medical imaging retrieval datasets and benchmarks. In this paper, we propose a novel methodology that leverages dense radiology reports to define image-wise similarity ordering at multiple granularities in a scalable and fully automatic manner. Using this approach, we construct two comprehensive medical imaging retrieval datasets: MIMIC-IR for Chest X-rays and CTRATE-IR for CT scans, providing detailed image-image ranking annotations conditioned on diverse anatomical structures. Furthermore, we develop two retrieval systems, RadIR-CXR and model-ChestCT, which demonstrate superior performance in traditional image-image and image-report retrieval tasks. These systems also enable flexible, effective image retrieval conditioned on specific anatomical structures described in text, achieving state-of-the-art results on 77 out of 78 metrics. </p>
<blockquote>
<p>开发先进的医学图像检索系统是一个挑战，因为“相似图像”的定义在不同的医学语境中各不相同。这一挑战由于缺乏大规模、高质量的医学图像检索数据集和基准测试而加剧。在本文中，我们提出了一种新方法，该方法利用密集的放射学报告来定义图像层面的相似性排序，以可伸缩和全自动的方式在多个粒度级别上实现。通过这种方法，我们构建了两个全面的医学图像检索数据集：用于胸部X射线的MIMIC-IR和用于CT扫描的CTRATE-IR，它们根据多种解剖结构提供了详细的图像-图像排名注释。此外，我们开发了两个检索系统RadIR-CXR和model-ChestCT，在传统的图像-图像和图像-报告检索任务中表现出卓越的性能。这些系统还能够根据文本中描述的特定解剖结构进行灵活有效的图像检索，在78个指标中的77个上取得了最先进的成果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04653v1">PDF</a> </p>
<p><strong>摘要</strong><br>医学图像检索系统面临多种挑战，其中包括在不同医学语境下对“相似图像”定义的差异、缺乏大规模高质量医学图像检索数据集和基准测试等。本文提出了一种新的方法，通过密集的放射学报告来定义图像级相似性排序的多种粒度，并以可伸缩和全自动的方式进行。基于此方法，我们构建了两个全面的医学图像检索数据集：MIMIC-IR用于胸部X光检查和CTRATE-IR用于CT扫描，为基于不同解剖结构的图像图像排名提供了详细的注释。此外，我们开发了两个检索系统RadIR-CXR和model-ChestCT，在传统的图像图像和图像报告检索任务中表现出卓越的性能。这些系统还可以根据文本中描述的特定解剖结构实现灵活有效的图像检索，在78个指标中的77个上取得了最先进的成果。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>医学图像检索系统面临定义“相似图像”的挑战，这一挑战因缺乏大规模高质量数据集和基准测试而加剧。</li>
<li>提出了一种新的方法，利用密集的放射学报告来定义图像级相似性排序的多种粒度，并全自动执行。</li>
<li>构建了两个全面的医学图像检索数据集：MIMIC-IR和CTRATE-IR，为基于不同解剖结构的图像提供详细排名注释。</li>
<li>开发了两个高性能的医学图像检索系统：RadIR-CXR和model-ChestCT。</li>
<li>这些系统可以灵活有效地根据文本描述的特定解剖结构进行图像检索。</li>
<li>在大部分评估指标上，这些系统达到了最先进的表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04653">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-25a4bb9994b7c0940a51bae0f3a42015.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f224d9a4bf8b6bb121438fb713a6fb68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d102e426dec969321b536faa30f3d3d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5318e60868c7b8569cd9479e4966cf73.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Adaptive-Prototype-Learning-for-Multimodal-Cancer-Survival-Analysis"><a href="#Adaptive-Prototype-Learning-for-Multimodal-Cancer-Survival-Analysis" class="headerlink" title="Adaptive Prototype Learning for Multimodal Cancer Survival Analysis"></a>Adaptive Prototype Learning for Multimodal Cancer Survival Analysis</h2><p><strong>Authors:Hong Liu, Haosen Yang, Federica Eduati, Josien P. W. Pluim, Mitko Veta</strong></p>
<p>Leveraging multimodal data, particularly the integration of whole-slide histology images (WSIs) and transcriptomic profiles, holds great promise for improving cancer survival prediction. However, excessive redundancy in multimodal data can degrade model performance. In this paper, we propose Adaptive Prototype Learning (APL), a novel and effective approach for multimodal cancer survival analysis. APL adaptively learns representative prototypes in a data-driven manner, reducing redundancy while preserving critical information. Our method employs two sets of learnable query vectors that serve as a bridge between high-dimensional representations and survival prediction, capturing task-relevant features. Additionally, we introduce a multimodal mixed self-attention mechanism to enable cross-modal interactions, further enhancing information fusion. Extensive experiments on five benchmark cancer datasets demonstrate the superiority of our approach over existing methods. The code is available at <a target="_blank" rel="noopener" href="https://github.com/HongLiuuuuu/APL">https://github.com/HongLiuuuuu/APL</a>. </p>
<blockquote>
<p>利用多模态数据，特别是整合全切片组织图像（WSIs）和转录组图谱，在癌症生存预测方面展现出巨大潜力。然而，多模态数据中的过度冗余可能会降低模型性能。在本文中，我们提出了自适应原型学习（APL），这是一种用于多模态癌症生存分析的新型有效方法。APL以数据驱动的方式自适应地学习代表性原型，减少冗余的同时保留关键信息。我们的方法采用两组可学习的查询向量，作为高维表示和生存预测之间的桥梁，捕捉任务相关特征。此外，我们引入了一种多模态混合自注意机制，以实现跨模态交互，进一步增强信息融合。在五个基准癌症数据集上的广泛实验表明，我们的方法优于现有方法。代码可在<a target="_blank" rel="noopener" href="https://github.com/HongLiuuuuu/APL">https://github.com/HongLiuuuuu/APL</a>获得。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04643v1">PDF</a> 10 pages, 3 figures</p>
<p><strong>Summary</strong><br>医学图像领域研究人员通过利用多模态数据，特别是整合全切片组织图像（WSIs）和转录组图谱，为改善癌症生存预测带来了希望。本文提出了一种自适应原型学习（APL）的新方法，能够有效处理多模态癌症生存分析中的数据冗余问题，自适应地学习代表性原型并保留关键信息。此外，引入了跨模态交互的自注意机制增强信息融合能力。代码已在GitHub上发布。该方法已在五个癌症数据集上进行实验验证。通过多维比较证明了该方法具有优异的预测效能和通用性。采用多任务联合学习增强特征选择能力的后续工作值得关注。不同医学分支疾病诊疗体系的相似性和差异需综合考虑，以实现更好的疾病诊疗融合研究。对于融合医学中的不同专业方向研究方法和策略也有待进一步探索和改进。未来的研究需要进一步加强不同疾病领域之间的交流和合作，以促进医学的全面发展。基于融合医学理念的交叉学科研究，对疾病的综合诊疗方案有着巨大潜力。在此背景下，借助大数据与人工智能的加持推进相关研究更具重要意义。本文主要讨论了癌症生存预测技术的现状和未来发展前景以及所面临的挑战，介绍了自己的创新性解决方案以及实践中的优点和挑战性特点。<strong>Key Takeaways</strong>:</p>
<ol>
<li>利用多模态数据（全切片组织图像和转录组图谱）进行癌症生存预测具有巨大潜力。</li>
<li>自适应原型学习（APL）方法能有效处理多模态数据中的冗余信息，提高模型性能。</li>
<li>APL方法引入跨模态交互的自注意机制以增强信息融合能力。</li>
<li>实验验证显示APL方法在五个癌症数据集上具有优异性能。</li>
<li>未来研究方向包括多任务联合学习增强特征选择能力、不同医学分支的疾病诊疗体系融合研究等。</li>
<li>融合医学理念下的交叉学科研究对疾病综合诊疗方案具有巨大潜力。大数据和人工智能技术在该领域的应用受到重视。当前工作主要讨论癌症生存预测技术的挑战和未来发展趋势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04643">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-de552bc34893907d1e9500a7bc2f586a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f57bae72938ff745de4b1071cf59aebb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dd913f1cdffe2a4003bec1ec73e4762.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f9dcc190fb2b938eb0a2820332f01d56.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Enhancing-SAM-with-Efficient-Prompting-and-Preference-Optimization-for-Semi-supervised-Medical-Image-Segmentation"><a href="#Enhancing-SAM-with-Efficient-Prompting-and-Preference-Optimization-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="Enhancing SAM with Efficient Prompting and Preference Optimization for   Semi-supervised Medical Image Segmentation"></a>Enhancing SAM with Efficient Prompting and Preference Optimization for   Semi-supervised Medical Image Segmentation</h2><p><strong>Authors:Aishik Konwer, Zhijian Yang, Erhan Bas, Cao Xiao, Prateek Prasanna, Parminder Bhatia, Taha Kass-Hout</strong></p>
<p>Foundational models such as the Segment Anything Model (SAM) are gaining traction in medical imaging segmentation, supporting multiple downstream tasks. However, such models are supervised in nature, still relying on large annotated datasets or prompts supplied by experts. Conventional techniques such as active learning to alleviate such limitations are limited in scope and still necessitate continuous human involvement and complex domain knowledge for label refinement or establishing reward ground truth. To address these challenges, we propose an enhanced Segment Anything Model (SAM) framework that utilizes annotation-efficient prompts generated in a fully unsupervised fashion, while still capturing essential semantic, location, and shape information through contrastive language-image pretraining and visual question answering. We adopt the direct preference optimization technique to design an optimal policy that enables the model to generate high-fidelity segmentations with simple ratings or rankings provided by a virtual annotator simulating the human annotation process. State-of-the-art performance of our framework in tasks such as lung segmentation, breast tumor segmentation, and organ segmentation across various modalities, including X-ray, ultrasound, and abdominal CT, justifies its effectiveness in low-annotation data scenarios. </p>
<blockquote>
<p>像Segment Anything Model（SAM）这样的基础模型在医学图像分割中越来越受欢迎，支持多个下游任务。然而，此类模型本质上是监督的，仍然依赖于专家提供的大量标注数据集或提示。为了缓解这些限制，采用主动学习的传统技术范围有限，仍需要持续的人力投入和复杂的领域知识来进行标签细化或建立奖励基准真实值。为了解决这些挑战，我们提出了一种增强的Segment Anything Model（SAM）框架，该框架利用完全无监督生成的标注有效提示，同时通过对比语言图像预训练和视觉问答捕获关键的语义、位置和形状信息。我们采用直接偏好优化技术来设计最佳策略，使模型能够借助模拟人类注释过程的虚拟注释器提供的简单评分或排名生成高保真分割。我们的框架在肺分割、乳腺肿瘤分割和器官分割等任务中的最新性能表现，以及在X射线、超声和腹部CT等各种模态的应用，证明了它在低标注数据场景中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04639v1">PDF</a> Accepted to CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>基于Segment Anything Model（SAM）的基础模型在医学图像分割中逐渐受到重视，支持多任务处理。然而，此类模型仍依赖于大量标注数据集或专家提供的提示，存在监督学习的局限性。为解决这些问题，我们提出了一种增强的SAM框架，该框架以完全无监督的方式生成标注有效的提示，同时仍能通过对比语言图像预训练和视觉问答捕获关键的语义、位置和形状信息。采用直接偏好优化技术设计最佳策略，使模型能够借助模拟人类注释过程的虚拟注释器提供的简单评分或排名生成高保真分割。在肺分割、乳腺肿瘤分割和器官分割等任务中，该框架的先进性能证明了其在低标注数据场景中的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Segment Anything Model (SAM) 在医学图像分割中受到重视，支持多任务处理。</li>
<li>SAM仍依赖于大量标注数据和专家提示，存在监督学习的局限性。</li>
<li>提出的增强SAM框架以完全无监督的方式生成标注有效的提示。</li>
<li>该框架通过对比语言图像预训练和视觉问答捕获关键的语义、位置和形状信息。</li>
<li>采用直接偏好优化技术设计最佳策略，生成高保真分割。</li>
<li>框架在肺分割、乳腺肿瘤分割和器官分割等任务中表现出先进性能。</li>
<li>框架在低标注数据场景中的有效性得到证明。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04639">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-948d1250612ec8d4f62580fdc376390f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81c346094d28e8e8e81faeb17f7ac82d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d3f8857441271c8dc2beabc0fce351c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="PathoPainter-Augmenting-Histopathology-Segmentation-via-Tumor-aware-Inpainting"><a href="#PathoPainter-Augmenting-Histopathology-Segmentation-via-Tumor-aware-Inpainting" class="headerlink" title="PathoPainter: Augmenting Histopathology Segmentation via Tumor-aware   Inpainting"></a>PathoPainter: Augmenting Histopathology Segmentation via Tumor-aware   Inpainting</h2><p><strong>Authors:Hong Liu, Haosen Yang, Evi M. C. Huijben, Mark Schuiveling, Ruisheng Su, Josien P. W. Pluim, Mitko Veta</strong></p>
<p>Tumor segmentation plays a critical role in histopathology, but it requires costly, fine-grained image-mask pairs annotated by pathologists. Thus, synthesizing histopathology data to expand the dataset is highly desirable. Previous works suffer from inaccuracies and limited diversity in image-mask pairs, both of which affect training segmentation, particularly in small-scale datasets and the inherently complex nature of histopathology images. To address this challenge, we propose PathoPainter, which reformulates image-mask pair generation as a tumor inpainting task. Specifically, our approach preserves the background while inpainting the tumor region, ensuring precise alignment between the generated image and its corresponding mask. To enhance dataset diversity while maintaining biological plausibility, we incorporate a sampling mechanism that conditions tumor inpainting on regional embeddings from a different image. Additionally, we introduce a filtering strategy to exclude uncertain synthetic regions, further improving the quality of the generated data. Our comprehensive evaluation spans multiple datasets featuring diverse tumor types and various training data scales. As a result, segmentation improved significantly with our synthetic data, surpassing existing segmentation data synthesis approaches, e.g., 75.69% -&gt; 77.69% on CAMELYON16. The code is available at <a target="_blank" rel="noopener" href="https://github.com/HongLiuuuuu/PathoPainter">https://github.com/HongLiuuuuu/PathoPainter</a>. </p>
<blockquote>
<p>肿瘤分割在病理学中扮演着至关重要的角色，但它需要病理学家标注的昂贵、精细的图像-掩膜对。因此，合成病理数据以扩大数据集是非常理想的。之前的工作存在图像-掩膜对的不准确性和多样性有限的问题，这两者都会影响分割训练，特别是在小规模数据集和病理图像固有的复杂性方面。为了应对这一挑战，我们提出了PathoPainter，它将图像-掩膜对的生成重新制定为一个肿瘤修复任务。具体来说，我们的方法在修复肿瘤区域的同时保留背景，确保生成的图像与其对应的掩膜之间精确对齐。为了在保持生物合理性的同时提高数据集的多样性，我们引入了一种采样机制，该机制根据来自不同图像的局部嵌入来进行肿瘤修复。此外，我们还采用了一种过滤策略，以排除不确定的合成区域，进一步提高生成数据的质量。我们的综合评估涵盖了多个包含各种肿瘤类型和不同训练数据规模的数据集。因此，使用我们的合成数据，分割效果得到了显著提高，超越了现有的分割数据合成方法，例如在CAMELYON16上的成绩从75.69%提高到了77.69%。代码可通过<a target="_blank" rel="noopener" href="https://github.com/HongLiuuuuu/PathoPainter%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/HongLiuuuuu/PathoPainter获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04634v1">PDF</a> 10 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于肿瘤修复技术（PathoPainter）的数据集合成方法，旨在解决肿瘤分割领域中对病理学专家精细标注的图像-掩膜对数据的依赖问题。该方法通过肿瘤区域修复技术生成图像和掩膜，确保生成的图像与掩膜精确对齐，同时引入采样机制增加数据集多样性并保持生物学合理性。通过过滤不确定的合成分割区域，提高生成数据质量。在多个数据集上的评估显示，使用合成数据可显著提高分割性能，超越现有分割数据合成方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PathoPainter是一种用于合成肿瘤分割数据集的方法，旨在解决数据获取成本高的问题。</li>
<li>通过肿瘤修复技术生成图像和掩膜，确保精确对齐。</li>
<li>采用采样机制增加数据集多样性并维持生物学合理性。</li>
<li>引入过滤策略以排除不确定的合成分割区域。</li>
<li>在多个数据集上的评估显示，使用PathoPainter合成的数据能显著提高分割性能。</li>
<li>该方法性能优于现有的分割数据合成方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04634">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ccb4446698d15c64f82160da280b57be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc7c3214532b2d5bde5942a8396e4364.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85dbfd7c7891b121bd5562e03d9959b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3652baf2602ad177fddf7c4a195d2167.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Maestro-A-302-GFLOPS-W-and-19-8GFLOPS-RISC-V-Vector-Tensor-Architecture-for-Wearable-Ultrasound-Edge-Computing"><a href="#Maestro-A-302-GFLOPS-W-and-19-8GFLOPS-RISC-V-Vector-Tensor-Architecture-for-Wearable-Ultrasound-Edge-Computing" class="headerlink" title="Maestro: A 302 GFLOPS&#x2F;W and 19.8GFLOPS RISC-V Vector-Tensor Architecture   for Wearable Ultrasound Edge Computing"></a>Maestro: A 302 GFLOPS&#x2F;W and 19.8GFLOPS RISC-V Vector-Tensor Architecture   for Wearable Ultrasound Edge Computing</h2><p><strong>Authors:Mattia Sinigaglia, Amirhossein Kiamarzi, Marco Bertuletti, Luigi Ghionda, Mattia Orlandi, Riccardo Tedeschi, Aurora Di Giampietro, Yvan Tortorella, Luca Bertaccini, Simone Benatti, Giuseppe Tagliavini, Luca Benini, Francesco Conti, Davide Rossi</strong></p>
<p>Most Wearable Ultrasound (WUS) devices lack the computational power to process signals at the edge, instead relying on remote offload, which introduces latency, high power consumption, and privacy concerns. We present Maestro, a RISC-V SoC with unified Vector-Tensor Unit (VTU) and memory-coupled Fast Fourier Transform (FFT) accelerators targeting edge processing for wearable ultrasound devices, fabricated using low-cost TSMC 65nm CMOS technology. The VTU achieves peak 302GFLOPS&#x2F;W and 19.8GFLOPS at FP16, while the multi-precision 16&#x2F;32-bit floating-point FFT accelerator delivers peak 60.6GFLOPS&#x2F;W and 3.6GFLOPS at FP16, We evaluate Maestro on a US-based gesture recognition task, achieving 1.62GFLOPS in signal processing at 26.68GFLOPS&#x2F;W, and 19.52GFLOPS in Convolutional Neural Network (CNN) workloads at 298.03GFLOPS&#x2F;W. Compared to a state-of-the-art SoC with a similar mission profile, Maestro achieves a 5x speedup while consuming only 12mW, with an energy consumption of 2.5mJ in a wearable US channel preprocessing and ML-based postprocessing pipeline. </p>
<blockquote>
<p>大多数可穿戴超声（WUS）设备缺乏在边缘处理信号的计算能力，而是依赖于远程卸载，这引入了延迟、高功耗和隐私担忧。我们推出了Maestro，这是一款针对可穿戴超声设备的边缘处理而设计的RISC-V系统芯片（SoC），配备了统一的向量张量单元（VTU）和内存耦合快速傅里叶变换（FFT）加速器，采用低成本TSMC 65nm CMOS技术制造。VTU在FP16下达到峰值302GFLOPS&#x2F;W和19.8GFLOPS，而多精度16&#x2F;32位浮点FFT加速器在FP16下达到峰值60.6GFLOPS&#x2F;W和3.6GFLOPS。我们评估了Maestro在基于美国的姿态识别任务上的表现，在信号处理的GFLOPS达到26.68时实现了1.62GFLOPS的处理速度，在卷积神经网络（CNN）工作负载的GFLOPS达到298.03时实现了19.52GFLOPS的处理速度。与具有类似任务配置的最新SoC相比，Maestro实现了5倍的加速，仅消耗12mW的功耗，在可穿戴超声通道预处理和基于机器学习的后处理管道中的能耗为2.5mJ。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04581v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>一种新型的穿戴式超声处理系统Maestro被研发出来，它具备边缘处理能力，解决了现有穿戴式超声设备依赖远程处理导致的延迟、高功耗和隐私等问题。Maestro采用RISC-V SoC，配备了统一的向量张量单元（VTU）和内存耦合快速傅里叶变换（FFT）加速器，实现了低功耗处理。其在手势识别任务中表现优异，相比现有最先进的类似任务SoC，Maestro实现了5倍的加速，同时功耗仅为12mW。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Maestro是一种新型的穿戴式超声处理系统，具备边缘处理能力。</li>
<li>Maestro解决了现有穿戴式超声设备依赖远程处理的问题，减少了延迟、高功耗和隐私担忧。</li>
<li>Maestro采用RISC-V SoC，配备了VTU和FFT加速器，实现了低功耗处理。</li>
<li>VTU实现了峰值302GFLOPS&#x2F;W和19.8GFLOPS的计算能力。</li>
<li>FFT加速器提供了多精度计算，峰值达到60.6GFLOPS&#x2F;W。</li>
<li>在手势识别任务中，Maestro相比现有最先进的SoC实现了5倍的加速。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04581">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c118c454c7f69c04d5553a5723127fae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-55ec51910c5dffab79c932a9afcb3f97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f33abdea64d4cb4d4fb9ff5158afa83.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3a9b49e976151a68a94724e9a38c0072.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="In-Context-Reverse-Classification-Accuracy-Efficient-Estimation-of-Segmentation-Quality-without-Ground-Truth"><a href="#In-Context-Reverse-Classification-Accuracy-Efficient-Estimation-of-Segmentation-Quality-without-Ground-Truth" class="headerlink" title="In-Context Reverse Classification Accuracy: Efficient Estimation of   Segmentation Quality without Ground-Truth"></a>In-Context Reverse Classification Accuracy: Efficient Estimation of   Segmentation Quality without Ground-Truth</h2><p><strong>Authors:Matias Cosarinsky, Ramiro Billot, Lucas Mansilla, Gabriel Gimenez, Nicolas Gaggión, Guanghui Fu, Enzo Ferrante</strong></p>
<p>Assessing the quality of automatic image segmentation is crucial in clinical practice, but often very challenging due to the limited availability of ground truth annotations. In this paper, we introduce In-Context Reverse Classification Accuracy (In-Context RCA), a novel framework for automatically estimating segmentation quality in the absence of ground-truth annotations. By leveraging recent in-context learning segmentation models and incorporating retrieval-augmentation techniques to select the most relevant reference images, our approach enables efficient quality estimation with minimal reference data. Validated across diverse medical imaging modalities, our method demonstrates robust performance and computational efficiency, offering a promising solution for automated quality control in clinical workflows, where fast and reliable segmentation assessment is essential. The code is available at <a target="_blank" rel="noopener" href="https://github.com/mcosarinsky/In-Context-RCA">https://github.com/mcosarinsky/In-Context-RCA</a>. </p>
<blockquote>
<p>在医疗实践中，评估自动图像分割的质量至关重要，但由于真实标注数据的有限性，这通常极具挑战性。在本文中，我们引入了基于上下文反向分类准确率（In-Context RCA）这一新型框架，无需真实标注数据即可自动估计分割质量。通过利用最新的上下文学习分割模型和检索增强技术来选择最相关的参考图像，我们的方法能够在最少的参考数据下实现高效的质量评估。经过多种医学影像模态的验证，我们的方法表现出了稳健的性能和计算效率，为临床工作流程中的自动化质量控制提供了有前景的解决方案，特别是在快速可靠的分割评估至关重要的领域。相关代码可访问<a target="_blank" rel="noopener" href="https://github.com/mcosarinsky/In-Context-RCA%E3%80%82">https://github.com/mcosarinsky/In-Context-RCA。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04522v1">PDF</a> </p>
<p><strong>Summary</strong><br>医学图像自动分割质量评估在临床实践中至关重要，但缺乏真实标注导致评估具有挑战性。本文提出一种新型框架In-Context Reverse Classification Accuracy（In-Context RCA），无需真实标注即可自动估计分割质量。该框架结合最新的上下文学习分割模型和检索增强技术，选择最相关的参考图像，以最小的参考数据实现高效的质量评估。经过多种医学成像模态的验证，该方法表现出稳健的性能和计算效率，为临床工作流程中的自动化质量控制提供了有前景的解决方案，尤其在需要快速可靠分割评估的情况下。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像自动分割质量评估的重要性及挑战。</li>
<li>提出了一种新型框架In-Context RCA，无需真实标注即可评估分割质量。</li>
<li>利用最新的上下文学习分割模型。</li>
<li>引入检索增强技术，选择最相关的参考图像。</li>
<li>方法在多种医学成像模态下表现出稳健性和计算效率。</li>
<li>对临床工作流程的自动化质量控制具有潜在的应用价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04522">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4d5ea157a85e0c84e4e6ff0dcb3a6044.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6cf549f285c8c228a0ae93ce9677ac6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a401d6c006d09b9dbeb9e458a8f26a97.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Spatial-regularisation-for-improved-accuracy-and-interpretability-in-keypoint-based-registration"><a href="#Spatial-regularisation-for-improved-accuracy-and-interpretability-in-keypoint-based-registration" class="headerlink" title="Spatial regularisation for improved accuracy and interpretability in   keypoint-based registration"></a>Spatial regularisation for improved accuracy and interpretability in   keypoint-based registration</h2><p><strong>Authors:Benjamin Billot, Ramya Muthukrishnan, Esra Abaci-Turk, Ellen P. Grant, Nicholas Ayache, Hervé Delingette, Polina Golland</strong></p>
<p>Unsupervised registration strategies bypass requirements in ground truth transforms or segmentations by optimising similarity metrics between fixed and moved volumes. Among these methods, a recent subclass of approaches based on unsupervised keypoint detection stand out as very promising for interpretability. Specifically, these methods train a network to predict feature maps for fixed and moving images, from which explainable centres of mass are computed to obtain point clouds, that are then aligned in closed-form. However, the features returned by the network often yield spatially diffuse patterns that are hard to interpret, thus undermining the purpose of keypoint-based registration. Here, we propose a three-fold loss to regularise the spatial distribution of the features. First, we use the KL divergence to model features as point spread functions that we interpret as probabilistic keypoints. Then, we sharpen the spatial distributions of these features to increase the precision of the detected landmarks. Finally, we introduce a new repulsive loss across keypoints to encourage spatial diversity. Overall, our loss considerably improves the interpretability of the features, which now correspond to precise and anatomically meaningful landmarks. We demonstrate our three-fold loss in foetal rigid motion tracking and brain MRI affine registration tasks, where it not only outperforms state-of-the-art unsupervised strategies, but also bridges the gap with state-of-the-art supervised methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/BenBillot/spatial_regularisation">https://github.com/BenBillot/spatial_regularisation</a>. </p>
<blockquote>
<p>无监督的注册策略通过优化固定体积和移动体积之间的相似性度量来绕过对真实变换或分割的要求。在这些方法中，最近出现的一种基于无监督关键点检测的方法作为解释性方面非常有前途的一个子类而脱颖而出。具体来说，这些方法训练网络为固定图像和移动图像预测特征图，从中计算出可解释的质量中心，以获得点云，然后这些点云以封闭形式对齐。然而，网络返回的特征通常会产生难以解释的空间扩散模式，从而破坏了基于关键点的注册目的。在这里，我们提出了一个三重的损失来正规化特征的空间分布。首先，我们使用KL散度来模拟特征作为点扩散函数，我们将其解释为概率性的关键点。然后，我们使这些特征的空间分布更加清晰，以提高检测到的地标的精度。最后，我们在关键点之间引入了一个新的排斥损失来鼓励空间多样性。总的来说，我们的损失大大提高了特征的可解释性，这些特征现在对应于精确且解剖上有意义的地标。我们在胎儿刚性运动跟踪和脑MRI仿射注册任务中展示了我们的三重损失，它不仅优于最新的无监督策略，而且缩小了与最新监督方法的差距。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/BenBillot/spatial_regularisation">https://github.com/BenBillot/spatial_regularisation</a> 中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04499v1">PDF</a> under review</p>
<p><strong>摘要</strong><br>    本文提出一种基于无监督关键点检测的方法，通过优化固定图像和移动图像之间的相似性度量来实现图像配准，同时提出了一种新的三重损失函数对特征的空间分布进行正则化，提高了解释性关键点检测的准确性。实验结果表明，该方法不仅优于现有的无监督策略，而且缩小了与最新监督方法的差距。代码已公开。</p>
<p><strong>关键发现点</strong></p>
<ol>
<li>提出了一种基于无监督关键点检测的无监督配准方法。</li>
<li>通过训练网络预测固定和移动图像的特征映射，并计算可解释的中心质量来获取点云进行对齐。</li>
<li>通过KL散度将特征建模为点扩散函数，解释为概率关键点。</li>
<li>对特征的空间分布进行锐化以提高检测到的地标精度。</li>
<li>引入了一种新的排斥损失来鼓励关键点的空间多样性。</li>
<li>三重损失函数显著提高了特征的解释性，这些特征现在对应于精确且解剖上意义的地标。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04499">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-77573c23bb63dea746cc03a4805043a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc71c82a2932ed132b853769d83c9338.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-61f4271b1952ce09e7b0ed0f08304acb.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Semantic-Alignment-of-Unimodal-Medical-Text-and-Vision-Representations"><a href="#Semantic-Alignment-of-Unimodal-Medical-Text-and-Vision-Representations" class="headerlink" title="Semantic Alignment of Unimodal Medical Text and Vision Representations"></a>Semantic Alignment of Unimodal Medical Text and Vision Representations</h2><p><strong>Authors:Maxime Di Folco, Emily Chan, Marta Hasny, Cosmin I. Bercea, Julia A. Schnabel</strong></p>
<p>General-purpose AI models, particularly those designed for text and vision, demonstrate impressive versatility across a wide range of deep-learning tasks. However, they often underperform in specialised domains like medical imaging, where domain-specific solutions or alternative knowledge transfer approaches are typically required. Recent studies have noted that general-purpose models can exhibit similar latent spaces when processing semantically related data, although this alignment does not occur naturally. Building on this insight, it has been shown that applying a simple transformation - at most affine - estimated from a subset of semantically corresponding samples, known as anchors, enables model stitching across diverse training paradigms, architectures, and modalities. In this paper, we explore how semantic alignment - estimating transformations between anchors - can bridge general-purpose AI with specialised medical knowledge. Using multiple public chest X-ray datasets, we demonstrate that model stitching across model architectures allows general models to integrate domain-specific knowledge without additional training, leading to improved performance on medical tasks. Furthermore, we introduce a novel zero-shot classification approach for unimodal vision encoders that leverages semantic alignment across modalities. Our results show that our method not only outperforms general multimodal models but also approaches the performance levels of fully trained, medical-specific multimodal solutions </p>
<blockquote>
<p>通用人工智能模型，尤其是那些为文本和视觉设计的模型，在广泛的深度学习任务中表现出了令人印象深刻的通用性。然而，它们在专业领域如医学影像中常常表现不佳，这通常需要领域特定的解决方案或替代知识迁移方法。近期的研究发现，当处理语义相关数据时，通用模型可以展现出类似的潜在空间，尽管这种对齐并不会自然发生。基于这一见解，已经证明应用一种简单的转换——最多是仿射变换——通过从语义对应的样本子集估计出来，这些样本被称为锚点，可以在不同的训练模式、架构和模态之间进行模型拼接。在本文中，我们探讨了语义对齐如何估算锚点之间的转换——能够架起通用人工智能与特殊医学知识之间的桥梁。通过使用多个公共胸部X射线数据集，我们证明了跨模型架构的模型拼接允许通用模型整合特定领域的知识而无需额外训练，从而提高了医学任务上的性能。此外，我们介绍了一种新型的零样本分类方法，用于单模态视觉编码器，该方法利用跨模态的语义对齐。我们的结果表明，我们的方法不仅优于一般的跨模态模型，而且接近经过充分训练的特定医学跨模态解决方案的性能水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04478v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了通用人工智能模型在医学成像等特定领域的应用挑战。通过语义对齐技术，实现在不同训练范式、架构和模态之间的模型拼接，使通用模型能够整合特定领域知识，提高在医疗任务上的性能。同时，提出了一种基于语义对齐的零射击分类方法，对于单模态视觉编码器性能有明显提升，接近全训练的医学专用多模态解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>通用人工智能模型在医学成像等特定领域存在性能瓶颈，需要特定领域的解决方案或知识迁移方法。</li>
<li>语义对齐技术能够实现模型拼接，使得通用模型整合特定领域知识成为可能。</li>
<li>通过使用锚点进行语义对齐估计，可以实现简单的模型转换，提高模型性能。</li>
<li>在多个公共胸部X光数据集上，模型架构之间的拼接使得通用模型能够融入特定领域知识，无需额外训练。</li>
<li>提出了一种新型的零射击分类方法，利用跨模态的语义对齐，对单模态视觉编码器的性能有显著提升。</li>
<li>该方法不仅优于一般的多模态模型，而且接近完全训练的医学专用多模态解决方案的性能水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04478">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-46acb79099111d1259c27a2fdbc24cac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8be69c489418b1c109b7d587246c74fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d30e0cea070b97065bfb7f96a55ab6ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ee62e832746a87830e12d196be37547.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GBT-SAM-A-Parameter-Efficient-Depth-Aware-Model-for-Generalizable-Brain-tumour-Segmentation-on-mp-MRI"><a href="#GBT-SAM-A-Parameter-Efficient-Depth-Aware-Model-for-Generalizable-Brain-tumour-Segmentation-on-mp-MRI" class="headerlink" title="GBT-SAM: A Parameter-Efficient Depth-Aware Model for Generalizable Brain   tumour Segmentation on mp-MRI"></a>GBT-SAM: A Parameter-Efficient Depth-Aware Model for Generalizable Brain   tumour Segmentation on mp-MRI</h2><p><strong>Authors:Cecilia Diana-Albelda, Roberto Alcover-Couso, Álvaro García-Martín, Jesus Bescos, Marcos Escudero-Viñolo</strong></p>
<p>Gliomas are brain tumours that stand out for their highly lethal and aggressive nature, which demands a precise approach in their diagnosis. Medical image segmentation plays a crucial role in the evaluation and follow-up of these tumours, allowing specialists to analyse their morphology. However, existing methods for automatic glioma segmentation often lack generalization capability across other brain tumour domains, require extensive computational resources, or fail to fully utilize the multi-parametric MRI (mp-MRI) data used to delineate them. In this work, we introduce GBT-SAM, a novel Generalizable Brain Tumour (GBT) framework that extends the Segment Anything Model (SAM) to brain tumour segmentation tasks. Our method employs a two-step training protocol: first, fine-tuning the patch embedding layer to process the entire mp-MRI modalities, and second, incorporating parameter-efficient LoRA blocks and a Depth-Condition block into the Vision Transformer (ViT) to capture inter-slice correlations. GBT-SAM achieves state-of-the-art performance on the Adult Glioma dataset (Dice Score of $93.54$) while demonstrating robust generalization across Meningioma, Pediatric Glioma, and Sub-Saharan Glioma datasets. Furthermore, GBT-SAM uses less than 6.5M trainable parameters, thus offering an efficient solution for brain tumour segmentation. \ Our code and models are available at <a target="_blank" rel="noopener" href="https://github.com/vpulab/med-sam-brain">https://github.com/vpulab/med-sam-brain</a> . </p>
<blockquote>
<p>胶质瘤是突出的脑肿瘤，具有高度的致死性和侵袭性，这就要求在诊断时需要采取精确的方法。医学图像分割在评估和随访这些肿瘤中起着至关重要的作用，允许专家分析它们的形态。然而，现有的自动胶质瘤分割方法往往缺乏在其他脑肿瘤领域的泛化能力，需要大量的计算资源，或者未能充分利用用于界定肿瘤的多参数MRI（mp-MRI）数据。在这项工作中，我们介绍了GBT-SAM，这是一个可泛化脑肿瘤（GBT）的新框架，它扩展了分段任何模型（SAM）以用于脑肿瘤分割任务。我们的方法采用两步训练协议：首先，微调补丁嵌入层以处理整个mp-MRI模式；其次，在视觉变压器（ViT）中融入参数高效的LoRA块和深度条件块，以捕捉切片间的相关性。GBT-SAM在成人胶质瘤数据集上实现了最先进的性能（Dice得分为93.54%），同时在脑膜瘤、儿童胶质瘤和撒哈拉以南胶质瘤数据集上表现出稳健的泛化能力。此外，GBT-SAM使用的可训练参数少于650万，因此为脑肿瘤分割提供了有效的解决方案。我们的代码和模型可在<a target="_blank" rel="noopener" href="https://github.com/vpulab/med-sam-brain%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/vpulab/med-sam-brain上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04325v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种名为GBT-SAM的新型通用脑肿瘤（GBT）分割框架，该框架扩展了Segment Anything Model（SAM）以应用于脑肿瘤分割任务。它采用了一种两步训练协议，通过在Patch嵌入层处理整个多参数MRI（mp-MRI）模式并进行精细调整，以及将参数高效的LoRA块和深度条件块融入视觉转换器（ViT）以捕获切片间的相关性。GBT-SAM在成人胶质瘤数据集上实现了最佳性能（Dice分数为93.54%），并在脑膜瘤、儿童胶质瘤和撒哈拉以南胶质瘤数据集上表现出稳健的泛化能力。此外，GBT-SAM使用的可训练参数少于650万，为脑肿瘤分割提供了高效的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GBT-SAM是一个针对脑肿瘤分割的新型框架，基于Segment Anything Model（SAM）。</li>
<li>采用了两步训练协议：首先微调patch嵌入层处理整个多参数MRI数据，然后融入视觉转换器以捕获切片间的相关性。</li>
<li>GBT-SAM在成人胶质瘤数据集上取得了最佳性能，Dice分数为93.54%。</li>
<li>GBT-SAM展现出对脑膜瘤、儿童胶质瘤和撒哈拉以南胶质瘤数据集的稳健泛化能力。</li>
<li>GBT-SAM使用的可训练参数少于650万，具有高效性。</li>
<li>GBT-SAM的方法可以在公开代码和模型中找到，网址为：<a target="_blank" rel="noopener" href="https://github.com/vpulab/med-sam-brain%E3%80%82">https://github.com/vpulab/med-sam-brain。</a></li>
<li>GBT-SAM框架的引入为医学图像分割领域提供了一种新的、高效的解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04325">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-94f7bc94ad4d039e0a1b7810c53f0fe1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bf322f8050f3b09629c75e5bf0791fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-750f89e2323cf1001dcd372ff31c1a77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5966b058c28bc67abfd53731528eb601.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="WeakMedSAM-Weakly-Supervised-Medical-Image-Segmentation-via-SAM-with-Sub-Class-Exploration-and-Prompt-Affinity-Mining"><a href="#WeakMedSAM-Weakly-Supervised-Medical-Image-Segmentation-via-SAM-with-Sub-Class-Exploration-and-Prompt-Affinity-Mining" class="headerlink" title="WeakMedSAM: Weakly-Supervised Medical Image Segmentation via SAM with   Sub-Class Exploration and Prompt Affinity Mining"></a>WeakMedSAM: Weakly-Supervised Medical Image Segmentation via SAM with   Sub-Class Exploration and Prompt Affinity Mining</h2><p><strong>Authors:Haoran Wang, Lian Huai, Wenbin Li, Lei Qi, Xingqun Jiang, Yinghuan Shi</strong></p>
<p>We have witnessed remarkable progress in foundation models in vision tasks. Currently, several recent works have utilized the segmenting anything model (SAM) to boost the segmentation performance in medical images, where most of them focus on training an adaptor for fine-tuning a large amount of pixel-wise annotated medical images following a fully supervised manner. In this paper, to reduce the labeling cost, we investigate a novel weakly-supervised SAM-based segmentation model, namely WeakMedSAM. Specifically, our proposed WeakMedSAM contains two modules: 1) to mitigate severe co-occurrence in medical images, a sub-class exploration module is introduced to learn accurate feature representations. 2) to improve the quality of the class activation maps, our prompt affinity mining module utilizes the prompt capability of SAM to obtain an affinity map for random-walk refinement. Our method can be applied to any SAM-like backbone, and we conduct experiments with SAMUS and EfficientSAM. The experimental results on three popularly-used benchmark datasets, i.e., BraTS 2019, AbdomenCT-1K, and MSD Cardiac dataset, show the promising results of our proposed WeakMedSAM. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/wanghr64/WeakMedSAM">https://github.com/wanghr64/WeakMedSAM</a>. </p>
<blockquote>
<p>在视觉任务的基础模型中，我们看到了显著的进步。目前，一些最新的工作利用分割任何模型（SAM）来提高医学图像的分割性能，其中大多数工作都集中在以完全监督的方式训练适配器，对大量像素级注释的医学图像进行微调。为了降低标注成本，我们在本文中研究了一种新型的基于弱监督的SAM分割模型，即WeakMedSAM。具体来说，我们提出的WeakMedSAM包含两个模块：1）为了缓解医学图像中严重的共发生问题，引入了子类探索模块来学习准确的特征表示。2）为了提高类激活图的质量，我们的提示亲和力挖掘模块利用SAM的提示能力来获得用于随机游走细化的亲和力图。我们的方法可以应用于任何SAM类似的backbone，我们在SAMUS和EfficientSAM上进行了实验。在三个常用的基准数据集（即BraTS 2019、AbdomenCT-1K和MSD Cardiac数据集）上的实验结果表明，我们提出的WeakMedSAM具有广阔的应用前景。我们的代码可访问于：<a target="_blank" rel="noopener" href="https://github.com/wanghr64/WeakMedSAM%E3%80%82">https://github.com/wanghr64/WeakMedSAM。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04106v1">PDF</a> </p>
<p><strong>Summary</strong><br>     该论文提出了一种基于弱监督的SAM（分段任何模型）医疗图像分割模型——WeakMedSAM，以降低标注成本。它通过子类别探索模块学习准确的特征表示，并改进了类别激活图的准确性。在SAMUS和EfficientSAM等模型的实验证明其有效性。该模型在BraTS 2019、AbdomenCT-1K和MSD心脏数据集上的实验结果证明了其潜力。代码已公开在GitHub上。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>该论文提出一种基于弱监督的SAM医疗图像分割模型WeakMedSAM，旨在降低标注成本。</li>
<li>WeakMedSAM包含两个模块：子类探索模块用于学习准确的特征表示，并改进类别激活图的准确性。</li>
<li>该模型采用随机游走细化技术，通过SAM的提示能力获取亲和力图。</li>
<li>WeakMedSAM可应用于任何SAM类似的主干网络，如SAMUS和EfficientSam。</li>
<li>实验结果证明了WeakMedSAM在BraTS 2019、AbdomenCT-1K和MSD心脏数据集上的有效性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04106">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e323683c49116e8cbcd4bc5b1325e1a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1cb6b2a7e3e81f209d843b99170518b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7e1124f1221980b5421e9988bc7b69e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b78c7413bf5961246376766049392672.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f10930621d4d62f1ed08890a401efa3f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Image-Based-Relocalization-and-Alignment-for-Long-Term-Monitoring-of-Dynamic-Underwater-Environments"><a href="#Image-Based-Relocalization-and-Alignment-for-Long-Term-Monitoring-of-Dynamic-Underwater-Environments" class="headerlink" title="Image-Based Relocalization and Alignment for Long-Term Monitoring of   Dynamic Underwater Environments"></a>Image-Based Relocalization and Alignment for Long-Term Monitoring of   Dynamic Underwater Environments</h2><p><strong>Authors:Beverley Gorry, Tobias Fischer, Michael Milford, Alejandro Fontan</strong></p>
<p>Effective monitoring of underwater ecosystems is crucial for tracking environmental changes, guiding conservation efforts, and ensuring long-term ecosystem health. However, automating underwater ecosystem management with robotic platforms remains challenging due to the complexities of underwater imagery, which pose significant difficulties for traditional visual localization methods. We propose an integrated pipeline that combines Visual Place Recognition (VPR), feature matching, and image segmentation on video-derived images. This method enables robust identification of revisited areas, estimation of rigid transformations, and downstream analysis of ecosystem changes. Furthermore, we introduce the SQUIDLE+ VPR Benchmark-the first large-scale underwater VPR benchmark designed to leverage an extensive collection of unstructured data from multiple robotic platforms, spanning time intervals from days to years. The dataset encompasses diverse trajectories, arbitrary overlap and diverse seafloor types captured under varying environmental conditions, including differences in depth, lighting, and turbidity. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/bev-gorry/underloc">https://github.com/bev-gorry/underloc</a> </p>
<blockquote>
<p>对水下生态系统进行有效监测对于跟踪环境变化、指导保护工作和确保生态系统长期健康至关重要。然而，由于水下图像的复杂性，使用机器人平台自动管理水下生态系统仍然是一个挑战，这给传统的视觉定位方法带来了巨大的困难。我们提出了一种结合视觉定位识别（VPR）、特征匹配和基于视频图像分割方法的综合流程。该方法能够实现稳健的回访区域识别、刚性转换估计以及生态系统变化的下游分析。此外，我们还推出了SQUIDLE+ VPR Benchmark——这是第一个大规模水下VPR基准测试，旨在利用来自多个机器人平台的海量非结构化数据集合，时间间隔从几天到几年不等。数据集包含多种轨迹、任意重叠和不同环境条件下的各种海底类型（包括深度、光照和浊度的差异）。我们的代码可从以下网址获取：<a target="_blank" rel="noopener" href="https://github.com/bev-gorry/underloc">https://github.com/bev-gorry/underloc</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04096v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种结合视觉定位识别（VPR）、特征匹配和视频图像分割技术的方法，实现对水下生态系统的自动化监测和管理。通过该方法的实施，能够准确识别回访区域、估算刚性变换以及分析生态系统变化。此外，本文还介绍了首个大规模水下VPR基准测试集——SQUIDLE+ VPR Benchmark，该数据集包含来自多个机器人平台的丰富非结构化数据，跨越数日乃至数年的时间段。数据集涵盖了多种轨迹、任意重叠和不同海底类型等环境条件下的数据差异，包括深度、光照和浊度等。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>有效监测水下生态系统对于跟踪环境变化、指导保护工作和确保生态系统长期健康至关重要。</li>
<li>水下成像的复杂性给传统视觉定位方法带来了挑战。</li>
<li>提出了一种结合视觉定位识别（VPR）、特征匹配和视频图像分割的集成方法，用于稳健地识别回访区域、估算刚性变换和分析生态系统变化。</li>
<li>介绍了首个大规模水下VPR基准测试集——SQUIDLE+ VPR Benchmark。</li>
<li>该数据集包含来自多个机器人平台的丰富非结构化数据，跨越不同环境条件下的数据差异。</li>
<li>数据集涵盖多种轨迹和任意重叠情况，为水下生态系统管理的自动化提供了重要资源。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04096">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-73c910a13045336e0d8ac4a60f173263.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-75347b420fcad5c10858df46546e64e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37ffc2b4677fc2f6832132ec80f56de9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-871c583940c3e6c69475a449205531e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8b53d0cf04e1c917344744de915327a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d78cec8cbf5f857d1c84ac3f71048216.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ReRAW-RGB-to-RAW-Image-Reconstruction-via-Stratified-Sampling-for-Efficient-Object-Detection-on-the-Edge"><a href="#ReRAW-RGB-to-RAW-Image-Reconstruction-via-Stratified-Sampling-for-Efficient-Object-Detection-on-the-Edge" class="headerlink" title="ReRAW: RGB-to-RAW Image Reconstruction via Stratified Sampling for   Efficient Object Detection on the Edge"></a>ReRAW: RGB-to-RAW Image Reconstruction via Stratified Sampling for   Efficient Object Detection on the Edge</h2><p><strong>Authors:Radu Berdan, Beril Besbinar, Christoph Reinders, Junji Otsuka, Daisuke Iso</strong></p>
<p>Edge-based computer vision models running on compact, resource-limited devices benefit greatly from using unprocessed, detail-rich RAW sensor data instead of processed RGB images. Training these models, however, necessitates large labeled RAW datasets, which are costly and often impractical to obtain. Thus, converting existing labeled RGB datasets into sensor-specific RAW images becomes crucial for effective model training. In this paper, we introduce ReRAW, an RGB-to-RAW conversion model that achieves state-of-the-art reconstruction performance across five diverse RAW datasets. This is accomplished through ReRAW’s novel multi-head architecture predicting RAW image candidates in gamma space. The performance is further boosted by a stratified sampling-based training data selection heuristic, which helps the model better reconstruct brighter RAW pixels. We finally demonstrate that pretraining compact models on a combination of high-quality synthetic RAW datasets (such as generated by ReRAW) and ground-truth RAW images for downstream tasks like object detection, outperforms both standard RGB pipelines, and RAW fine-tuning of RGB-pretrained models for the same task. </p>
<blockquote>
<p>基于边缘的计算机视觉模型在紧凑、资源受限的设备上运行，从使用未经处理、细节丰富的RAW传感器数据而不是经过处理的RGB图像中获益匪浅。然而，训练这些模型需要大量的标记RAW数据集，这既昂贵又往往不切实际。因此，将现有的标记RGB数据集转换为特定传感器RAW图像对于有效模型训练变得至关重要。在本文中，我们介绍了ReRAW，一种RGB到RAW的转换模型，它在五个不同的RAW数据集上实现了最先进的重建性能。这是通过ReRAW的新型多头架构在伽马空间中预测RAW图像候选物来实现的。通过分层采样为基础的训练数据选择启发式方法，性能得到了进一步提升，这有助于模型更好地重建较亮的RAW像素。最后，我们证明在高质量合成RAW数据集（如由ReRAW生成）和真实RAW图像的组合上预训练紧凑模型，用于下游任务（如对象检测），优于标准的RGB管道和相同任务的RAW微调RGB预训练模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03782v1">PDF</a> Accepted at CVPR 2025</p>
<p><strong>Summary</strong><br>     采用无处理、细节丰富的RAW传感器数据，在资源有限的设备上运行的边缘计算视觉模型相较于使用已处理的RGB图像受益匪浅。然而，训练这些模型需要大量标记的RAW数据集，这成本高昂并不切实际。因此，将现有标记的RGB数据集转换为特定传感器RAW图像变得至关重要。本文介绍了一种RGB转RAW转换模型ReRAW，其在五个不同的RAW数据集上实现了最先进的重建性能。其创新的多头架构能够在伽马空间预测RAW图像候选者，从而实现这一目标。此外，基于分层采样的训练数据选择启发式策略进一步提高了性能，帮助模型更好地重建较亮的RAW像素。最后证明，在下游任务如目标检测上，使用高质量合成RAW数据集（如由ReRAW生成）和真实RAW图像预训练紧凑模型，相较于标准RGB管道和RGB预训练模型的RAW微调，表现更优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>使用未处理的RAW传感器数据对边缘计算视觉模型进行训练在资源受限的设备上具有显著优势。</li>
<li>训练这类模型需要大量标记的RAW数据集，但其获取成本高昂且实际操作困难。</li>
<li>ReRAW模型通过RGB转RAW转换实现了先进的重建性能，借助其多头架构在伽马空间预测RAW图像。</li>
<li>分层采样启发式的训练数据选择策略增强了模型的性能，特别是在重建亮像素方面。</li>
<li>高质量合成RAW数据集与真实RAW图像的结合使用对于预训练紧凑模型至关重要。</li>
<li>预训练模型在目标检测等下游任务上的表现超越了标准RGB管道和RGB预训练模型的RAW微调。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03782">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cf3029351e4800ddc30b3010d180d1eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e105fa7405f6284a05e325d016176b9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44a668ce55002cb8a37b9e6a934163c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7e611587f5de14e8db7a4fb624f86d4d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eeaacc3a442e979394532071ca1a4259.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Deep-unrolling-for-learning-optimal-spatially-varying-regularisation-parameters-for-Total-Generalised-Variation"><a href="#Deep-unrolling-for-learning-optimal-spatially-varying-regularisation-parameters-for-Total-Generalised-Variation" class="headerlink" title="Deep unrolling for learning optimal spatially varying regularisation   parameters for Total Generalised Variation"></a>Deep unrolling for learning optimal spatially varying regularisation   parameters for Total Generalised Variation</h2><p><strong>Authors:Thanh Trung Vu, Andreas Kofler, Kostas Papafitsoros</strong></p>
<p>We extend a recently introduced deep unrolling framework for learning spatially varying regularisation parameters in inverse imaging problems to the case of Total Generalised Variation (TGV). The framework combines a deep convolutional neural network (CNN) inferring the two spatially varying TGV parameters with an unrolled algorithmic scheme that solves the corresponding variational problem. The two subnetworks are jointly trained end-to-end in a supervised fashion and as such the CNN learns to compute those parameters that drive the reconstructed images as close to the ground truth as possible. Numerical results in image denoising and MRI reconstruction show a significant qualitative and quantitative improvement compared to the best TGV scalar parameter case as well as to other approaches employing spatially varying parameters computed by unsupervised methods. We also observe that the inferred spatially varying parameter maps have a consistent structure near the image edges, asking for further theoretical investigations. In particular, the parameter that weighs the first-order TGV term has a triple-edge structure with alternating high-low-high values whereas the one that weighs the second-order term attains small values in a large neighbourhood around the edges. </p>
<blockquote>
<p>我们将最近引入的深度展开框架扩展到总广义变化（TGV）的情况，用于学习反成像问题中的空间变化正则化参数。该框架结合了深度卷积神经网络（CNN），用于推断两个空间变化的TGV参数，以及一个解决相应变分问题的展开算法方案。两个子网络以监督方式端对端联合训练，因此CNN学习计算那些参数，以尽可能接近真实情况的方式重建图像。在图像去噪和MRI重建方面的数值结果表明，与最佳TGV标量参数情况以及其他采用空间变化参数的方法相比，无论在定性和定量上均有显著改善。这些空间变化的参数图在图像边缘附近具有一致的结构，需要进一步的理论研究。特别是，权重一阶TGV项的参数字映射具有高低高的三重边缘结构，而权重二阶项的参数在边缘附近的大范围内达到较小值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16532v2">PDF</a> </p>
<p><strong>Summary</strong><br>     本文将深度展开框架扩展到总广义变化（TGV）的情况，结合深度卷积神经网络（CNN）推断两个空间变化的TGV参数与一个解决相应变分问题的展开算法方案。两个子网络以监督方式进行端到端的联合训练，使得CNN学习计算使重建图像尽可能接近真实值的参数。图像去噪和MRI重建的数值结果显示，与最佳TGV标量参数情况以及其他采用空间变化参数的计算方法相比，本文方法有显著的定性和定量改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度展开框架被扩展到总广义变化（TGV）的情况。</li>
<li>结合CNN推断空间变化的TGV参数与展开算法方案。</li>
<li>两个子网络以监督方式进行端到端的联合训练。</li>
<li>CNN学习计算使重建图像尽可能接近真实值的参数。</li>
<li>数值结果显示该方法在图像去噪和MRI重建上有显著改进。</li>
<li>推断出的空间变化参数地图在图像边缘附近有一致的结构。</li>
<li>一阶TGV参数的权重具有三边结构，而二阶TGV参数的权重在边缘周围达到小值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16532">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1d5594023d1ecc0534277e7d086c1d5e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9a4a7020a1b09c080179c2efdb7b35e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a12572c9996081246446bc259d4dea8.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MobileViM-A-Light-weight-and-Dimension-independent-Vision-Mamba-for-3D-Medical-Image-Analysis"><a href="#MobileViM-A-Light-weight-and-Dimension-independent-Vision-Mamba-for-3D-Medical-Image-Analysis" class="headerlink" title="MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D   Medical Image Analysis"></a>MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D   Medical Image Analysis</h2><p><strong>Authors:Wei Dai, Jun Liu</strong></p>
<p>Efficient evaluation of three-dimensional (3D) medical images is crucial for diagnostic and therapeutic practices in healthcare. Recent years have seen a substantial uptake in applying deep learning and computer vision to analyse and interpret medical images. Traditional approaches, such as convolutional neural networks (CNNs) and vision transformers (ViTs), face significant computational challenges, prompting the need for architectural advancements. Recent efforts have led to the introduction of novel architectures like the &#96;&#96;Mamba’’ model as alternative solutions to traditional CNNs or ViTs. The Mamba model excels in the linear processing of one-dimensional data with low computational demands. However, Mamba’s potential for 3D medical image analysis remains underexplored and could face significant computational challenges as the dimension increases. This manuscript presents MobileViM, a streamlined architecture for efficient segmentation of 3D medical images. In the MobileViM network, we invent a new dimension-independent mechanism and a dual-direction traversing approach to incorporate with a vision-Mamba-based framework. MobileViM also features a cross-scale bridging technique to improve efficiency and accuracy across various medical imaging modalities. With these enhancements, MobileViM achieves segmentation speeds exceeding 90 frames per second (FPS) on a single graphics processing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster than the state-of-the-art deep learning models for processing 3D images with the same computational resources. In addition, experimental evaluations demonstrate that MobileViM delivers superior performance, with Dice similarity scores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024, ATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses existing models. </p>
<blockquote>
<p>在医疗保健领域，对三维（3D）医学图像的有效评估对于诊断和治疗实践至关重要。近年来，深度学习和计算机视觉在医学图像分析和解释方面的应用取得了显著进展。传统方法，如卷积神经网络（CNNs）和视觉转换器（ViTs），面临着重大的计算挑战，这促使了架构发展的必要性。最近的努力导致了诸如“Mamba”模型等新架构的引入，作为传统CNN或ViTs的替代解决方案。Mamba模型在处理一维数据时具有出色的线性处理能力且计算需求较低。然而，Mamba在三维医学图像分析方面的潜力尚未得到充分探索，随着维度的增加，可能会面临重大的计算挑战。本文提出了MobileViM，这是一个用于高效分割三维医学图像的简化架构。在MobileViM网络中，我们发明了一种新的与维度无关的机制以及一种双向遍历方法与基于视觉Mamba的框架相结合。MobileViM还采用跨尺度桥梁技术，以提高各种医学成像模式的有效性和准确性。通过这些增强功能，MobileViM在单个图形处理单元（即NVIDIA RTX 4090）上实现了超过每秒90帧（FPS）的分割速度。这一性能比使用相同计算资源的处理三维图像的最先进深度学习模型的帧率快24 FPS以上。此外，实验评估表明，MobileViM具有卓越的性能，在PENGWIN、BraTS2024、ATLAS和Toothfairy2数据集上的Dice相似度得分分别达到92.72%、86.69%、80.46%和77.43%，显著超过了现有模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.13524v4">PDF</a> The corresponding author disagrees with the manuscript submitted to   arXiv</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种针对三维医学图像分析的新型高效架构MobileViM。它通过引入维度独立机制、双向遍历方法和跨尺度桥接技术，实现了快速而精确的医学图像分割。相较于其他顶尖深度学习模型，MobileViM在单一GPU上的运行速度提升了超过24FPS，并且在多个数据集上的Dice相似度得分表现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MobileViM是一种针对三维医学图像分析的高效架构。</li>
<li>MobileViM引入了维度独立机制，适应不同维度的数据处理。</li>
<li>通过双向遍历方法，MobileViM提升了图像处理的效率。</li>
<li>跨尺度桥接技术提高了MobileViM在不同医学影像模态下的性能和准确性。</li>
<li>MobileViM实现了超过90FPS的分割速度，在单一GPU上的性能超越现有模型。</li>
<li>MobileViM在多个数据集上的Dice相似度得分表现出卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.13524">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a19da823a39016902a1c548a5c88342b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b8d590b505813b80d6e6b5e2bad344e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5082a48f077c477e48276a57b6fe17b7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d40734e73903ff1d635e972ebebfdebd.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Game-Theoretic-Defenses-for-Robust-Conformal-Prediction-Against-Adversarial-Attacks-in-Medical-Imaging"><a href="#Game-Theoretic-Defenses-for-Robust-Conformal-Prediction-Against-Adversarial-Attacks-in-Medical-Imaging" class="headerlink" title="Game-Theoretic Defenses for Robust Conformal Prediction Against   Adversarial Attacks in Medical Imaging"></a>Game-Theoretic Defenses for Robust Conformal Prediction Against   Adversarial Attacks in Medical Imaging</h2><p><strong>Authors:Rui Luo, Jie Bao, Zhixin Zhou, Chuangyin Dang</strong></p>
<p>Adversarial attacks pose significant threats to the reliability and safety of deep learning models, especially in critical domains such as medical imaging. This paper introduces a novel framework that integrates conformal prediction with game-theoretic defensive strategies to enhance model robustness against both known and unknown adversarial perturbations. We address three primary research questions: constructing valid and efficient conformal prediction sets under known attacks (RQ1), ensuring coverage under unknown attacks through conservative thresholding (RQ2), and determining optimal defensive strategies within a zero-sum game framework (RQ3). Our methodology involves training specialized defensive models against specific attack types and employing maximum and minimum classifiers to aggregate defenses effectively. Extensive experiments conducted on the MedMNIST datasets, including PathMNIST, OrganAMNIST, and TissueMNIST, demonstrate that our approach maintains high coverage guarantees while minimizing prediction set sizes. The game-theoretic analysis reveals that the optimal defensive strategy often converges to a singular robust model, outperforming uniform and simple strategies across all evaluated datasets. This work advances the state-of-the-art in uncertainty quantification and adversarial robustness, providing a reliable mechanism for deploying deep learning models in adversarial environments. </p>
<blockquote>
<p>对抗性攻击对深度学习模型的可靠性和安全性构成重大威胁，特别是在医疗成像等关键领域。本文介绍了一个新型框架，该框架将合规性预测与博弈理论防御策略相结合，以提高模型对已知和未知对抗性扰动的鲁棒性。我们解决了三个主要的研究问题：在已知攻击下构建有效且高效的合规性预测集（RQ1），通过保守阈值化确保在未知攻击下的覆盖率（RQ2），以及在零和博弈框架内确定最佳防御策略（RQ3）。我们的方法包括针对特定攻击类型训练专门的防御模型，并使用最大和最小分类器有效地聚合防御措施。在MedMNIST数据集（包括PathMNIST、OrganAMNIST和TissueMNIST）上进行的广泛实验表明，我们的方法在保持高覆盖率保证的同时，最小化了预测集的大小。博弈理论分析表明，最佳防御策略通常会收敛到一个单一的稳健模型，在所有评估的数据集上，其性能都优于均匀和简单策略。这项工作在不确定性的量化和对抗稳健性方面达到了最新水平，为在敌对环境中部署深度学习模型提供了可靠的机制。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04376v2">PDF</a> </p>
<p><strong>Summary</strong><br>     本论文提出一种新型框架，结合保形预测和游戏理论防御策略，提高深度学习模型对抗已知和未知对抗性扰动的稳健性。研究解决三个主要问题：在已知攻击下构建有效且高效的保形预测集、通过保守阈值化确保未知攻击下的覆盖，以及在零和博弈框架内确定最佳防御策略。实验证明，该方法在保持高覆盖保证的同时减小了预测集大小。游戏理论分析显示，最佳防御策略通常收敛于单一稳健模型，在所有评估数据集上表现优于均匀和简单策略。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>对抗攻击对深度学习模型的可靠性和安全性构成重大威胁，特别是在医疗成像等关键领域。</li>
<li>论文提出结合保形预测和游戏理论防御策略的新型框架，增强模型对抗已知和未知对抗性扰动的稳健性。</li>
<li>研究解决三个主要问题：构建保形预测集、确保未知攻击下的覆盖以及确定最佳防御策略。</li>
<li>通过在MedMNIST数据集（包括PathMNIST、OrganAMNIST和TissueMNIST）上进行广泛实验，证明该方法在保持高覆盖保证的同时减小预测集大小。</li>
<li>游戏理论分析显示，最佳防御策略通常收敛于单一稳健模型，该模型在多个数据集上的性能优于均匀和简单策略。</li>
<li>该方法提高了不确定性量化的水平，并增强了对抗性环境下的模型稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04376">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-88cfc01902b9e0d46e421500f63c4a7b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-68cabaa989d4abdac45042663f3d77ba.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LDPM-Towards-undersampled-MRI-reconstruction-with-MR-VAE-and-Latent-Diffusion-Prior"><a href="#LDPM-Towards-undersampled-MRI-reconstruction-with-MR-VAE-and-Latent-Diffusion-Prior" class="headerlink" title="LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent   Diffusion Prior"></a>LDPM: Towards undersampled MRI reconstruction with MR-VAE and Latent   Diffusion Prior</h2><p><strong>Authors:Xingjian Tang, Jingwei Guan, Linge Li, Ran Shi, Youmei Zhang, Mengye Lyu, Li Yan</strong></p>
<p>Diffusion models, as powerful generative models, have found a wide range of applications and shown great potential in solving image reconstruction problems. Some works attempted to solve MRI reconstruction with diffusion models, but these methods operate directly in pixel space, leading to higher computational costs for optimization and inference. Latent diffusion models, pre-trained on natural images with rich visual priors, are expected to solve the high computational cost problem in MRI reconstruction by operating in a lower-dimensional latent space. However, direct application to MRI reconstruction faces three key challenges: (1) absence of explicit control mechanisms for medical fidelity, (2) domain gap between natural images and MR physics, and (3) undefined data consistency in latent space. To address these challenges, a novel Latent Diffusion Prior-based undersampled MRI reconstruction (LDPM) method is proposed. Our LDPM framework addresses these challenges by: (1) a sketch-guided pipeline with a two-step reconstruction strategy, which balances perceptual quality and anatomical fidelity, (2) an MRI-optimized VAE (MR-VAE), which achieves an improvement of approximately 3.92 dB in PSNR for undersampled MRI reconstruction compared to that with SD-VAE \cite{sd}, and (3) Dual-Stage Sampler, a modified version of spaced DDPM sampler, which enforces high-fidelity reconstruction in the latent space. Experiments on the fastMRI dataset\cite{fastmri} demonstrate the state-of-the-art performance of the proposed method and its robustness across various scenarios. The effectiveness of each module is also verified through ablation experiments. </p>
<blockquote>
<p>扩散模型作为强大的生成模型，在图像重建问题中得到了广泛的应用，并显示出巨大的潜力。一些研究尝试使用扩散模型解决MRI重建问题，但这些方法在像素空间直接操作，导致优化和推理的计算成本较高。潜在扩散模型在自然图像上进行预训练，具有丰富的视觉先验，有望通过低维潜在空间解决MRI重建中的高计算成本问题。然而，直接应用于MRI重建面临三个关键挑战：（1）医学保真度的控制机制缺失，（2）自然图像与MR物理之间的领域差距，（3）潜在空间中的数据一致性未定义。为了解决这些挑战，提出了一种基于潜在扩散先验的欠采样MRI重建（LDPM）新方法。我们的LDPM框架通过以下方式应对这些挑战：（1）一个草图引导的流程，采用两步重建策略，平衡感知质量和解剖保真度；（2）一个针对MRI优化的VAE（MR-VAE），在欠采样MRI重建的PSNR上相比SD-VAE \cite{sd}提高了约3.92 dB；（3）双阶段采样器，这是一个改进的间隔DDPM采样器，它在潜在空间中强制高保真重建。在fastMRI数据集\cite{fastmri}上的实验证明了该方法的先进性能和在各种场景中的稳健性。每个模块的有效性也通过消融实验得到了验证。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.02951v2">PDF</a> </p>
<p><strong>Summary</strong><br>     潜在扩散模型在解决MRI重建问题方面具有巨大潜力，但仍面临挑战。新型LDPM方法通过草图引导管道、MRI优化的VAE和双阶段采样器等创新手段，解决了这些问题，并在fastMRI数据集上展现出卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型在图像重建问题中展现了巨大的潜力。</li>
<li>直接在像素空间操作MRI重建导致高计算成本。</li>
<li>潜在扩散模型期望通过在低维潜在空间操作来解决高计算成本问题。</li>
<li>LDPM方法通过草图引导管道、MRI优化的VAE和双阶段采样器解决潜在扩散模型在MRI重建中的挑战。</li>
<li>LDPM方法在fastMRI数据集上实现了先进性能，并展示了在各种场景下的稳健性。</li>
<li>LDPM方法通过平衡感知质量和解剖保真度来提高MRI重建的质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.02951">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e22c7fa8d8fb4f5ff7f28d3dda1da45e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5144b1cf28ed6a854c2c4fbbdfb72cea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5789dfc0f7479c776ca018ae0a47c6e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50cdb7f71b9ee820a4008700bf2bd88c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7ac9a4d23757bfcf5918478f08c6c34a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0251eac9ec15658da969639324cfbb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7d307b2c4d7b4c57391a62c837dee76.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Enhancing-Multimodal-Medical-Image-Classification-using-Cross-Graph-Modal-Contrastive-Learning"><a href="#Enhancing-Multimodal-Medical-Image-Classification-using-Cross-Graph-Modal-Contrastive-Learning" class="headerlink" title="Enhancing Multimodal Medical Image Classification using Cross-Graph   Modal Contrastive Learning"></a>Enhancing Multimodal Medical Image Classification using Cross-Graph   Modal Contrastive Learning</h2><p><strong>Authors:Jun-En Ding, Chien-Chin Hsu, Chi-Hsiang Chu, Shuqiang Wang, Feng Liu</strong></p>
<p>The classification of medical images is a pivotal aspect of disease diagnosis, often enhanced by deep learning techniques. However, traditional approaches typically focus on unimodal medical image data, neglecting the integration of diverse non-image patient data. This paper proposes a novel Cross-Graph Modal Contrastive Learning (CGMCL) framework for multimodal structured data from different data domains to improve medical image classification. The model effectively integrates both image and non-image data by constructing cross-modality graphs and leveraging contrastive learning to align multimodal features in a shared latent space. An inter-modality feature scaling module further optimizes the representation learning process by reducing the gap between heterogeneous modalities. The proposed approach is evaluated on two datasets: a Parkinson’s disease (PD) dataset and a public melanoma dataset. Results demonstrate that CGMCL outperforms conventional unimodal methods in accuracy, interpretability, and early disease prediction. Additionally, the method shows superior performance in multi-class melanoma classification. The CGMCL framework provides valuable insights into medical image classification while offering improved disease interpretability and predictive capabilities. </p>
<blockquote>
<p>医学图像分类是疾病诊断的关键环节，通常可以通过深度学习技术得到增强。然而，传统方法主要关注单模态医学图像数据，忽略了不同非图像患者数据的整合。本文提出了一种新型的跨图模态对比学习（CGMCL）框架，用于从不同数据域的多模态结构化数据，以提高医学图像分类的效果。该模型通过构建跨模态图并利用对比学习，有效地整合了图像和非图像数据，在共享潜在空间中对齐多模态特征。跨模态特征缩放模块进一步优化了表示学习过程，缩小了不同模态之间的差距。所提出的方法在两个数据集上进行了评估：帕金森病（PD）数据集和公共黑色素瘤数据集。结果表明，在准确性、可解释性和早期疾病预测方面，CGMCL优于传统的单模态方法。此外，该方法在多类黑色素瘤分类方面也表现出卓越的性能。CGMCL框架为医学图像分类提供了有价值的见解，同时提高了疾病可解释性和预测能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17494v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新型的跨图模态对比学习（CGMCL）框架，用于多模态结构化数据的跨域融合，以提高医学图像分类的准确性。该框架通过构建跨模态图并利用对比学习，在共享潜在空间中对齐多模态特征，实现图像与非图像数据的融合。该方法优化了多模态特征的表示学习过程，减少了不同模态之间的差距。在两个数据集上的评估结果证明，与常规的单模态方法相比，CGmcl在准确性、可解释性和早期疾病预测方面表现出优越的性能。特别是在多类黑色素瘤分类方面，CGmcl框架提供了宝贵的医学图像分类洞察力，提高了疾病可解释性和预测能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分类是疾病诊断的重要环节，而深度学习技术能增强其准确性。</li>
<li>传统方法主要关注单模态医学图像数据，忽略了不同非图像患者数据的整合。</li>
<li>提出的Cross-Graph Modal Contrastive Learning (CGMCL)框架旨在整合多模态结构化数据，提高医学图像分类效果。</li>
<li>CGMCL框架通过构建跨模态图，利用对比学习对齐多模态特征，实现图像与非图像数据的融合。</li>
<li>CGMCL框架优化了多模态特征的表示学习过程，缩小了不同模态间的差距。</li>
<li>在帕金森氏病和黑色素瘤数据集上的评估显示，CGmcl在准确性、可解释性和早期疾病预测方面优于传统方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17494">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b730c93802a67649f6e4688c9e9e6745.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97222ee56ba028819d49009fa2c7ef35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6d9de4484218baee277721f07a825ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd6986fc293845fd61bc9ba737bf16d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f99f055faccc7f1c93c993d5ac326652.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Self-supervised-pre-training-with-diffusion-model-for-few-shot-landmark-detection-in-x-ray-images"><a href="#Self-supervised-pre-training-with-diffusion-model-for-few-shot-landmark-detection-in-x-ray-images" class="headerlink" title="Self-supervised pre-training with diffusion model for few-shot landmark   detection in x-ray images"></a>Self-supervised pre-training with diffusion model for few-shot landmark   detection in x-ray images</h2><p><strong>Authors:Roberto Di Via, Francesca Odone, Vito Paolo Pastore</strong></p>
<p>Deep neural networks have been extensively applied in the medical domain for various tasks, including image classification, segmentation, and landmark detection. However, their application is often hindered by data scarcity, both in terms of available annotations and images. This study introduces a novel application of denoising diffusion probabilistic models (DDPMs) to the landmark detection task, specifically addressing the challenge of limited annotated data in x-ray imaging. Our key innovation lies in leveraging DDPMs for self-supervised pre-training in landmark detection, a previously unexplored approach in this domain. This method enables accurate landmark detection with minimal annotated training data (as few as 50 images), surpassing both ImageNet supervised pre-training and traditional self-supervised techniques across three popular x-ray benchmark datasets. To our knowledge, this work represents the first application of diffusion models for self-supervised learning in landmark detection, which may offer a valuable pre-training approach in few-shot regimes, for mitigating data scarcity. </p>
<blockquote>
<p>深度神经网络已在医疗领域得到广泛应用，用于各种任务，包括图像分类、分割和地标检测。然而，其应用往往受到数据和注释可用性的限制。本研究引入了去噪扩散概率模型（DDPMs）的新应用，专门解决X射线成像中注释数据有限所带来的挑战。我们的主要创新之处在于，利用DDPMs进行地标检测的自监督预训练，这是该领域之前未被探索的方法。该方法能够在极少的注释训练数据（仅50张图像）的情况下实现准确的地标检测，超越了ImageNet监督预训练和传统自监督技术在三个流行的X射线基准数据集上的表现。据我们所知，这项工作代表了扩散模型在地标检测自监督学习中的首次应用，这可能为缓解数据稀缺的少量模式提供有价值的预训练方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.18125v3">PDF</a> Accepted at WACV 2025</p>
<p><strong>Summary</strong><br>医学图像领域中深度神经网络的应用广泛，包括图像分类、分割和关键点检测等任务。然而，数据稀缺（包括可用的注释和图像）常常限制其应用。本研究首次将去噪扩散概率模型（DDPMs）应用于医学图像中的关键点检测任务，解决标注数据有限的问题。研究创新之处在于利用DDPMs进行自监督预训练，此方法在少量标注数据（仅50张图像）的情况下即可实现准确的关键点检测，并在三个流行的X射线基准数据集上超越了ImageNet监督预训练和传统自监督技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究将去噪扩散概率模型（DDPMs）应用于医学图像中的关键点检测任务。</li>
<li>DDPMs能有效解决医学图像中标注数据有限的问题。</li>
<li>研究采用自监督预训练方法，利用DDPMs在少量标注数据下实现准确的关键点检测。</li>
<li>该方法在三个流行的X射线基准数据集上表现优异，超越了传统的预训练技术。</li>
<li>此研究是首次将扩散模型应用于医学图像关键点检测中的自监督学习。</li>
<li>该方法为未来解决医学图像数据稀缺问题提供了新的思路。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.18125">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c4fe8ec52f131a12305c13150eef9066.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bdf8299076cafa7e4723193ef80f93d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a569e2988b76382c74c267bebcb93089.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e059e518c4b2016e7c6a59a420b98309.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="A-Survey-of-Deep-Learning-based-Radiology-Report-Generation-Using-Multimodal-Data"><a href="#A-Survey-of-Deep-Learning-based-Radiology-Report-Generation-Using-Multimodal-Data" class="headerlink" title="A Survey of Deep Learning-based Radiology Report Generation Using   Multimodal Data"></a>A Survey of Deep Learning-based Radiology Report Generation Using   Multimodal Data</h2><p><strong>Authors:Xinyi Wang, Grazziela Figueredo, Ruizhe Li, Wei Emma Zhang, Weitong Chen, Xin Chen</strong></p>
<p>Automatic radiology report generation can alleviate the workload for physicians and minimize regional disparities in medical resources, therefore becoming an important topic in the medical image analysis field. It is a challenging task, as the computational model needs to mimic physicians to obtain information from multi-modal input data (i.e., medical images, clinical information, medical knowledge, etc.), and produce comprehensive and accurate reports. Recently, numerous works have emerged to address this issue using deep-learning-based methods, such as transformers, contrastive learning, and knowledge-base construction. This survey summarizes the key techniques developed in the most recent works and proposes a general workflow for deep-learning-based report generation with five main components, including multi-modality data acquisition, data preparation, feature learning, feature fusion and interaction, and report generation. The state-of-the-art methods for each of these components are highlighted. Additionally, we summarize the latest developments in large model-based methods and model explainability, along with public datasets, evaluation methods, current challenges, and future directions in this field. We have also conducted a quantitative comparison between different methods in the same experimental setting. This is the most up-to-date survey that focuses on multi-modality inputs and data fusion for radiology report generation. The aim is to provide comprehensive and rich information for researchers interested in automatic clinical report generation and medical image analysis, especially when using multimodal inputs, and to assist them in developing new algorithms to advance the field. </p>
<blockquote>
<p>自动放射学报告生成能够减轻医生的工作量，并最小化医疗资源的区域差异，因此成为医学图像分析领域的重要课题。这是一个具有挑战性的任务，因为计算模型需要模仿医生从多模态输入数据（例如医学图像、临床信息、医学知识等）中获得信息，并生成全面准确的报告。最近，出现了许多基于深度学习的方法来解决这个问题，例如变压器、对比学习和知识库构建。这篇综述总结了最近作品开发的关键技术，并提出了一个基于深度学习的报告生成的一般工作流程，包括五个主要组件：多模态数据采集、数据准备、特征学习、特征融合和交互以及报告生成。本文强调了这些组件的最新先进方法。此外，我们还总结了基于大型模型的最新发展和模型解释性，以及公开数据集、评估方法、当前挑战和该领域的未来发展方向。我们还在相同的实验环境中对不同方法进行了定量比较。这是最新专注于多模态输入和数据融合用于放射学报告生成的综述。旨在为对自动临床报告生成和医学图像分析感兴趣的研究人员提供全面丰富的信息，特别是在使用多模态输入时，并帮助他们开发新算法以推动该领域的发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.12833v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了自动放射学报告生成的重要性，包括其在减轻医生工作量、缩小医疗资源区域差距方面的作用。文章指出这是一个挑战任务，需要计算模型从多模态输入数据中获取信息并生成全面准确的报告。本文总结了最近利用深度学习的方法来解决这个问题，提出了基于深度学习报告的通用工作流程，并详细介绍了各部分的关键技术。同时进行了最新发展的大型模型方法、模型解释性等方面的总结。通过同一实验环境下不同方法的定量比较，本文是最新的关注多模态输入和数据融合的放射学报告生成的综述。旨在为对自动临床报告生成和医学图像分析感兴趣的学者提供全面丰富的信息，并帮助开发新算法推动该领域的发展。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>自动放射学报告生成是医学图像分析领域的重要话题，有助于减轻医生工作量并缩小医疗资源差距。</li>
<li>该任务需要计算模型模仿医生从多模态输入数据中获取信息并生成准确全面的报告。</li>
<li>最近利用深度学习的方法来解决这个问题，包括transformer、对比学习和知识库构建等。</li>
<li>基于深度学习的报告生成包含五个主要组件：多模态数据采集、数据准备、特征学习、特征融合与交互和报告生成。</li>
<li>总结了各部分的关键技术和最新发展的大型模型方法、模型解释性等方面的内容。</li>
<li>文章通过定量比较不同方法在同一实验环境下的表现，提供了全面的综述。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.12833">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bcdf60fbe109ae80517754ec4a52d394.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-989320cdea82334ad73e7b06b76aba87.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5cf04ac312f75f780d1f425cbaa0b56f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5eb06d594b723fee0065905f8bc415f.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Implantable-Adaptive-Cells-A-Novel-Enhancement-for-Pre-Trained-U-Nets-in-Medical-Image-Segmentation"><a href="#Implantable-Adaptive-Cells-A-Novel-Enhancement-for-Pre-Trained-U-Nets-in-Medical-Image-Segmentation" class="headerlink" title="Implantable Adaptive Cells: A Novel Enhancement for Pre-Trained U-Nets   in Medical Image Segmentation"></a>Implantable Adaptive Cells: A Novel Enhancement for Pre-Trained U-Nets   in Medical Image Segmentation</h2><p><strong>Authors:Emil Benedykciuk, Marcin Denkowski, Grzegorz Wójcik</strong></p>
<p>This paper introduces a novel approach to enhance the performance of pre-trained neural networks in medical image segmentation using gradient-based Neural Architecture Search (NAS) methods. We present the concept of Implantable Adaptive Cell (IAC), small modules identified through Partially-Connected DARTS based approach, designed to be injected into the skip connections of an existing and already trained U-shaped model. Unlike traditional NAS methods, our approach refines existing architectures without full retraining. Experiments on four medical datasets with MRI and CT images show consistent accuracy improvements on various U-Net configurations, with segmentation accuracy gain by approximately 5 percentage points across all validation datasets, with improvements reaching up to 11%pt in the best-performing cases. The findings of this study not only offer a cost-effective alternative to the complete overhaul of complex models for performance upgrades but also indicate the potential applicability of our method to other architectures and problem domains. </p>
<blockquote>
<p>本文介绍了一种利用基于梯度的神经网络结构搜索（NAS）方法提高预训练神经网络在医学图像分割性能的新型方法。我们提出了可植入自适应单元（IAC）的概念，这是一种通过部分连接DARTS方法识别的小模块，旨在注入到已训练和存在的U形模型的跳过连接中。与传统的NAS方法不同，我们的方法能够在不进行全面再训练的情况下优化现有架构。在四个包含MRI和CT图像的医学数据集上进行的实验表明，在各种U-Net配置上，准确率得到持续提高，在所有验证数据集上的分割准确率提高约5个百分点，在表现最佳的案例中，提高幅度高达11个百分点。本研究的结果不仅为通过彻底改革复杂模型来提高性能提供了成本效益高的替代方案，还表明我们的方法在其他架构和问题领域具有潜在适用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.03420v2">PDF</a> </p>
<p><strong>Summary</strong><br>论文提出了一种利用基于梯度的神经网络架构搜索（NAS）方法提高预训练神经网络在医学图像分割性能的新方法。通过部分连接的DARTS方法，设计了一种名为植入式自适应单元（IAC）的小模块，可注入已训练的U形模型的跳跃连接中。该方法在四个医学数据集上的实验表明，在各种U-Net配置上均实现了稳定的准确性提高，分割准确性平均提升约5个百分点，最佳情况下提升达11个百分点。此研究不仅为升级性能提供了全面推翻复杂模型的低成本替代方案，而且还表明该方法在其他架构和问题领域具有潜在适用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文提出了一种基于梯度的新型神经网络架构搜索方法用于改进预训练神经网络在医学图像分割的性能。</li>
<li>采用名为植入式自适应单元（IAC）的小模块，通过部分连接的DARTS方法设计。</li>
<li>IAC模块被注入到已训练的U形模型的跳跃连接中，以优化现有架构，无需完全重新训练。</li>
<li>在四个医学数据集上的实验表明，该方法在各种U-Net配置上都实现了分割准确性的显著提高。</li>
<li>与传统的NAS方法相比，该方法更为高效，成本更低。</li>
<li>该方法不仅适用于医学图像分割，还有潜力应用于其他架构和问题领域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.03420">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f798f474e03f9388e98124e4c83426e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7abc510c8352ce0b3b76c5a9663c443.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a63e3cb301ee86e7620010c4a970d3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-947061309be54a4523eea97c66a07311.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eb2b3fbfc1e8405d2fee2cdf598c5623.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a22b7b93f83c516704e346e177be86ed.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a25fdae49c198c9f5bb31f3c5c34aa9b.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-03-10  LLMVoX Autoregressive Streaming Text-to-Speech Model for Any LLM
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-716fefef52104778416315e065200599.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-03-10  Synthetic Data is an Elegant GIFT for Continual Vision-Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29058.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
