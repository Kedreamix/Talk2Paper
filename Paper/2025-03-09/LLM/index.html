<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-09  LLMVoX Autoregressive Streaming Text-to-Speech Model for Any LLM">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-c7eb8933593370723397907a2b0cfb45.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    78 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-09-æ›´æ–°"><a href="#2025-03-09-æ›´æ–°" class="headerlink" title="2025-03-09 æ›´æ–°"></a>2025-03-09 æ›´æ–°</h1><h2 id="LLMVoX-Autoregressive-Streaming-Text-to-Speech-Model-for-Any-LLM"><a href="#LLMVoX-Autoregressive-Streaming-Text-to-Speech-Model-for-Any-LLM" class="headerlink" title="LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM"></a>LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM</h2><p><strong>Authors:Sambal Shikhar, Mohammed Irfan Kurpath, Sahal Shaji Mullappilly, Jean Lahoud, Fahad Khan, Rao Muhammad Anwer, Salman Khan, Hisham Cholakkal</strong></p>
<p>Recent advancements in speech-to-speech dialogue systems leverage LLMs for multimodal interactions, yet they remain hindered by fine-tuning requirements, high computational overhead, and text-speech misalignment. Existing speech-enabled LLMs often degrade conversational quality by modifying the LLM, thereby compromising its linguistic capabilities. In contrast, we propose LLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS system that generates high-quality speech with low latency, while fully preserving the capabilities of the base LLM. Our approach achieves a significantly lower Word Error Rate compared to speech-enabled LLMs, while operating at comparable latency and UTMOS score. By decoupling speech synthesis from LLM processing via a multi-queue token streaming system, LLMVoX supports seamless, infinite-length dialogues. Its plug-and-play design also facilitates extension to various tasks with different backbones. Furthermore, LLMVoX generalizes to new languages with only dataset adaptation, attaining a low Character Error Rate on an Arabic speech task. Additionally, we have integrated LLMVoX with a Vision-Language Model to create an omni-model with speech, text, and vision capabilities, without requiring additional multimodal training. Our code base and project page is available at <a target="_blank" rel="noopener" href="https://mbzuai-oryx.github.io/LLMVoX">https://mbzuai-oryx.github.io/LLMVoX</a> . </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè¯­éŸ³åˆ°è¯­éŸ³å¯¹è¯ç³»ç»Ÿçš„è¿›å±•å……åˆ†åˆ©ç”¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¤šæ¨¡å¼äº¤äº’ï¼Œä½†å®ƒä»¬ä»ç„¶å—åˆ°ç²¾ç»†è°ƒæ•´è¦æ±‚ã€é«˜è®¡ç®—å¼€é”€å’Œæ–‡æœ¬-è¯­éŸ³ä¸å¯¹é½çš„é™åˆ¶ã€‚ç°æœ‰çš„è¯­éŸ³èµ‹èƒ½çš„å¤§å‹è¯­è¨€æ¨¡å‹å¾€å¾€ä¼šé€šè¿‡ä¿®æ”¹å¤§å‹è¯­è¨€æ¨¡å‹æ¥é™ä½å¯¹è¯è´¨é‡ï¼Œä»è€ŒæŸå®³å…¶è¯­è¨€åŠŸèƒ½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†LLMVoXï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ã€å‚æ•°è§„æ¨¡ä¸º30Mçš„å¤§å‹è¯­è¨€æ¨¡å‹æ— å…³çš„ã€è‡ªå›å½’æµå¼æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼Œå®ƒå¯ä»¥åœ¨ä¿æŒåŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹åŠŸèƒ½çš„åŒæ—¶ï¼Œä»¥ä½å»¶è¿Ÿç”Ÿæˆé«˜è´¨é‡è¯­éŸ³ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸è¯­éŸ³èµ‹èƒ½çš„å¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼Œæ˜¾è‘—é™ä½äº†å•è¯é”™è¯¯ç‡ï¼ŒåŒæ—¶åœ¨å»¶è¿Ÿå’ŒUTMOSåˆ†æ•°æ–¹é¢è¡¨ç°å‡ºå¯æ¯”æ€§ã€‚é€šè¿‡å¤šé˜Ÿåˆ—ä»¤ç‰Œæµç³»ç»Ÿï¼Œå°†è¯­éŸ³åˆæˆä¸å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†è§£è€¦ï¼ŒLLMVoXæ”¯æŒæ— ç¼ã€æ— é™é•¿åº¦çš„å¯¹è¯ã€‚å…¶å³æ’å³ç”¨è®¾è®¡ä¹Ÿä¾¿äºæ‰©å±•åˆ°å…·æœ‰ä¸åŒä¸»å¹²çš„å„ç§ä»»åŠ¡ã€‚æ­¤å¤–ï¼ŒLLMVoXä»…é€šè¿‡æ•°æ®é›†é€‚åº”å°±èƒ½é€‚åº”æ–°è¯­è¨€ï¼Œåœ¨é˜¿æ‹‰ä¼¯è¯­éŸ³ä»»åŠ¡ä¸Šè¾¾åˆ°äº†è¾ƒä½çš„å­—ç¬¦é”™è¯¯ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†LLMVoXä¸è§†è§‰è¯­è¨€æ¨¡å‹é›†æˆï¼Œåˆ›å»ºäº†ä¸€ä¸ªå…·æœ‰è¯­éŸ³ã€æ–‡æœ¬å’Œè§†è§‰åŠŸèƒ½çš„ä¸‡èƒ½æ¨¡å‹ï¼Œæ— éœ€é¢å¤–çš„å¤šæ¨¡å¼è®­ç»ƒã€‚æˆ‘ä»¬çš„ä»£ç åº“å’Œé¡¹ç›®é¡µé¢å¯åœ¨<a target="_blank" rel="noopener" href="https://mbzuai-oryx.github.io/LLMVoX%E8%AE%BF%E9%97%AE%E3%80%82">https://mbzuai-oryx.github.io/LLMVoXè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04724v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMVoXæ˜¯ä¸€ç§è½»é‡çº§çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼Œå…·æœ‰é«˜è´¨é‡çš„è¯­éŸ³ç”Ÿæˆèƒ½åŠ›ï¼Œæ”¯æŒæµç•…æ— é™çš„å¯¹è¯ï¼ŒåŒæ—¶ä¿ç•™åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…¨éƒ¨åŠŸèƒ½ã€‚å®ƒé€šè¿‡è§£è€¦è¯­éŸ³åˆæˆä¸LLMå¤„ç†è¿‡ç¨‹ï¼Œé‡‡ç”¨å¤šé˜Ÿåˆ—ä»¤ç‰Œæµå¼ä¼ è¾“ç³»ç»Ÿå®ç°ä½å»¶è¿Ÿå’Œé«˜æ€§èƒ½ã€‚LLMVoXæ˜“äºé›†æˆåˆ°å…¶ä»–ä»»åŠ¡ä¸­ï¼Œå¹¶èƒ½é€‚åº”ä¸åŒçš„è¯­è¨€æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜ä¸è§†è§‰è¯­è¨€æ¨¡å‹é›†æˆï¼Œå½¢æˆå…·æœ‰è¯­éŸ³ã€æ–‡æœ¬å’Œè§†è§‰èƒ½åŠ›çš„å…¨èƒ½æ¨¡å‹ã€‚è¯¦æƒ…è®¿é—®å…¶å®˜ç½‘äº†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMVoXæ˜¯ä¸€ä¸ªè½»é‡çº§çš„æ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿï¼Œç”¨äºå¢å¼ºå¯¹è¯ç³»ç»Ÿçš„è¯­éŸ³åŠŸèƒ½ã€‚</li>
<li>å®ƒä¿ç•™äº†åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ‰€æœ‰åŠŸèƒ½ï¼ŒåŒæ—¶å®ç°äº†é«˜è´¨é‡çš„è¯­éŸ³ç”Ÿæˆã€‚</li>
<li>LLMVoXåˆ©ç”¨å¤šé˜Ÿåˆ—ä»¤ç‰Œæµå¼ä¼ è¾“ç³»ç»Ÿå®ç°è¯­éŸ³åˆæˆä¸LLMå¤„ç†çš„è§£è€¦ã€‚</li>
<li>ç³»ç»Ÿå…·æœ‰ä½å»¶è¿Ÿã€é«˜è´¨é‡è¯­éŸ³å’Œæ— ç¼å¯¹è¯çš„èƒ½åŠ›ã€‚</li>
<li>LLMVoXæ˜“äºé›†æˆåˆ°å…¶ä»–ä»»åŠ¡ä¸­ï¼Œå¹¶èƒ½å¤Ÿæ‰©å±•åˆ°å„ç§ä¸åŒçš„åº”ç”¨åœºæ™¯ã€‚</li>
<li>è¯¥ç³»ç»Ÿå…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯ä»¥é€šè¿‡ä»…é€‚åº”æ•°æ®é›†æ¥é€‚åº”æ–°è¯­è¨€ã€‚</li>
<li>LLMVoXä¸è§†è§‰è¯­è¨€æ¨¡å‹é›†æˆï¼Œå½¢æˆå…·æœ‰å¤šç§èƒ½åŠ›çš„å…¨èƒ½æ¨¡å‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04724">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7ec9b4bd289d72db89e0e73f89690d75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cf534253bb878e35cd0a0e33ab798ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a25fdae49c198c9f5bb31f3c5c34aa9b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Predictable-Scale-Part-I-â€“-Optimal-Hyperparameter-Scaling-Law-in-Large-Language-Model-Pretraining"><a href="#Predictable-Scale-Part-I-â€“-Optimal-Hyperparameter-Scaling-Law-in-Large-Language-Model-Pretraining" class="headerlink" title="Predictable Scale: Part I â€“ Optimal Hyperparameter Scaling Law in Large   Language Model Pretraining"></a>Predictable Scale: Part I â€“ Optimal Hyperparameter Scaling Law in Large   Language Model Pretraining</h2><p><strong>Authors:Houyi Li, Wenzheng Zheng, Jingcheng Hu, Qiufeng Wang, Hanshan Zhang, Zili Wang, Yangshijie Xu, Shuigeng Zhou, Xiangyu Zhang, Daxin Jiang</strong></p>
<p>The impressive capabilities of Large Language Models (LLMs) across diverse tasks are now well-established, yet their effective deployment necessitates careful hyperparameter optimization. Through extensive empirical studies involving grid searches across diverse configurations, we discover universal scaling laws governing these hyperparameters: optimal learning rate follows a power-law relationship with both model parameters and data sizes, while optimal batch size scales primarily with data sizes. Our analysis reveals a convex optimization landscape for hyperparameters under fixed models and data size conditions. This convexity implies an optimal hyperparameter plateau. We contribute a universal, plug-and-play optimal hyperparameter tool for the community. Its estimated values on the test set are merely 0.07% away from the globally optimal LLM performance found via an exhaustive search. These laws demonstrate remarkable robustness across variations in model sparsity, training data distribution, and model shape. To our best known, this is the first work that unifies different model shapes and structures, such as Mixture-of-Experts models and dense transformers, as well as establishes optimal hyperparameter scaling laws across diverse data distributions. This exhaustive optimization process demands substantial computational resources, utilizing nearly one million NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and hyperparameters from scratch and consuming approximately 100 trillion tokens in total. To facilitate reproducibility and further research, we will progressively release all loss measurements and model checkpoints through our designated repository <a target="_blank" rel="noopener" href="https://step-law.github.io/">https://step-law.github.io/</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„é¡¹ä»»åŠ¡ä¸­çš„å‡ºè‰²èƒ½åŠ›ç°å·²å¾—åˆ°å¹¿æ³›è®¤å¯ï¼Œä½†å…¶æœ‰æ•ˆéƒ¨ç½²éœ€è¦è¿›è¡Œè°¨æ…çš„è¶…å‚æ•°ä¼˜åŒ–ã€‚æˆ‘ä»¬é€šè¿‡æ¶‰åŠå¤šç§é…ç½®çš„ç½‘æ ¼æœç´¢çš„å¹¿æ³›å®è¯ç ”ç©¶ï¼Œå‘ç°äº†è¿™äº›è¶…å‚æ•°çš„é€šç”¨ç¼©æ”¾å®šå¾‹ï¼šæœ€ä½³å­¦ä¹ ç‡ä¸æ¨¡å‹å‚æ•°å’Œæ•°æ®å¤§å°ä¹‹é—´å­˜åœ¨å¹‚å¾‹å…³ç³»ï¼Œè€Œæœ€ä½³æ‰¹å¤„ç†å¤§å°ä¸»è¦éšæ•°æ®å¤§å°è¿›è¡Œç¼©æ”¾ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†å›ºå®šæ¨¡å‹å’Œæ•°æ®å¤§å°æ¡ä»¶ä¸‹è¶…å‚æ•°çš„å‡¸ä¼˜åŒ–æ™¯è§‚ã€‚è¿™ç§å‡¸æ€§æ„å‘³ç€å­˜åœ¨ä¸€ä¸ªæœ€ä½³è¶…å‚æ•°å¹³å°ã€‚æˆ‘ä»¬ä¸ºç¤¾åŒºè´¡çŒ®äº†ä¸€ä¸ªé€šç”¨ã€å³æ’å³ç”¨çš„æœ€ä½³è¶…å‚æ•°å·¥å…·ã€‚å…¶åœ¨æµ‹è¯•é›†ä¸Šçš„ä¼°è®¡å€¼ä¸é€šè¿‡è¯¦å°½æœç´¢æ‰¾åˆ°çš„å…¨å±€æœ€ä½³LLMæ€§èƒ½ä»…ç›¸å·®0.07%ã€‚è¿™äº›å®šå¾‹åœ¨æ¨¡å‹ç¨€ç–æ€§ã€è®­ç»ƒæ•°æ®åˆ†å¸ƒå’Œæ¨¡å‹å½¢çŠ¶çš„å˜åŒ–ä¸­è¡¨ç°å‡ºæƒŠäººçš„ç¨³å¥æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹ç»Ÿä¸€ä¸åŒæ¨¡å‹å½¢çŠ¶å’Œç»“æ„çš„å·¥ä½œï¼Œå¦‚ä¸“å®¶æ··åˆæ¨¡å‹å’Œå¯†é›†è½¬æ¢å™¨ï¼Œå¹¶å»ºç«‹äº†è·¨ä¸åŒæ•°æ®åˆ†å¸ƒçš„æœ€ä½³è¶…å‚æ•°ç¼©æ”¾å®šå¾‹ã€‚è¿™ä¸€è¯¦å°½çš„ä¼˜åŒ–è¿‡ç¨‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œä½¿ç”¨è¿‘ç™¾ä¸‡NVIDIA H800 GPUå°æ—¶ä»å¤´å¼€å§‹è®­ç»ƒå„ç§å¤§å°å’Œè¶…å‚æ•°çš„3700ä¸ªLLMï¼Œæ€»å…±æ¶ˆè€—çº¦10ä¸‡äº¿ä¸ªä»¤ç‰Œã€‚ä¸ºäº†ä¾¿äºå¤åˆ¶å’Œè¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å°†é€æ­¥é‡Šæ”¾æ‰€æœ‰æŸå¤±æµ‹é‡å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œé€šè¿‡æˆ‘ä»¬çš„æŒ‡å®šä»“åº“[<a target="_blank" rel="noopener" href="https://step-law.github.io/]%E8%BF%9B%E8%A1%8C%E5%85%AC%E5%BC%80%E3%80%82">https://step-law.github.io/]è¿›è¡Œå…¬å¼€ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04715v1">PDF</a> 19 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„é¡¹ä»»åŠ¡ä¸­çš„å¼ºå¤§èƒ½åŠ›å·²å¹¿ä¸ºäººçŸ¥ï¼Œä½†å…¶æœ‰æ•ˆéƒ¨ç½²éœ€è¦è°¨æ…çš„è¶…å‚æ•°ä¼˜åŒ–ã€‚é€šè¿‡å¹¿æ³›çš„å®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°äº†é€šç”¨è¶…å‚æ•°ç¼©æ”¾å®šå¾‹ï¼šæœ€ä½³å­¦ä¹ ç‡ä¸æ¨¡å‹å‚æ•°å’Œæ•°æ®å¤§å°å‘ˆå¹‚å¾‹å…³ç³»ï¼Œè€Œæœ€ä½³æ‰¹æ¬¡å¤§å°ä¸»è¦éšæ•°æ®å¤§å°è€Œæ‰©å±•ã€‚åˆ†æè¡¨æ˜ï¼Œåœ¨å›ºå®šæ¨¡å‹å’Œæ•°æ®å¤§å°æ¡ä»¶ä¸‹ï¼Œè¶…å‚æ•°ä¼˜åŒ–æ™¯è§‚å‘ˆç°å‡¸æ€§ï¼Œæš—ç¤ºå­˜åœ¨ä¸€ä¸ªæœ€ä½³è¶…å‚æ•°å¹³å°ã€‚æˆ‘ä»¬ä¸ºç¤¾åŒºè´¡çŒ®äº†ä¸€ä¸ªé€šç”¨ã€å³æ’å³ç”¨çš„æœ€ä½³è¶…å‚æ•°å·¥å…·ã€‚å…¶åœ¨æµ‹è¯•é›†ä¸Šçš„ä¼°è®¡å€¼ä¸é€šè¿‡å…¨é¢æœç´¢æ‰¾åˆ°çš„å…¨å±€æœ€ä½³LLMæ€§èƒ½ä»…ç›¸å·®0.07%ã€‚è¿™äº›å®šå¾‹åœ¨æ¨¡å‹ç¨€ç–æ€§ã€è®­ç»ƒæ•°æ®åˆ†å¸ƒå’Œæ¨¡å‹å½¢çŠ¶çš„å˜åŒ–æ–¹é¢è¡¨ç°å‡ºæƒŠäººçš„ç¨³å¥æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œç»Ÿä¸€äº†ä¸åŒçš„æ¨¡å‹å½¢çŠ¶å’Œç»“æ„ï¼Œå¦‚Mixture-of-Expertsæ¨¡å‹å’Œå¯†é›†è½¬æ¢å™¨ï¼Œå¹¶å»ºç«‹äº†è·¨ä¸åŒæ•°æ®åˆ†å¸ƒçš„æœ€ä½³è¶…å‚æ•°ç¼©æ”¾å®šå¾‹ã€‚æœ¬ä¼˜åŒ–è¿‡ç¨‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œåˆ©ç”¨è¿‘ç™¾ä¸‡ä¸ªNVIDIA H800 GPUå°æ—¶ä»å¤´å¼€å§‹è®­ç»ƒ3700ä¸ªä¸åŒå¤§å°å’Œè¶…å‚æ•°çš„LLMï¼Œå¹¶æ€»å…±æ¶ˆè€—çº¦åä¸‡äº¿ä¸ªä»¤ç‰Œã€‚ä¸ºäº†ä¾¿äºå¤åˆ¶å’Œè¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å°†é€æ­¥å‘å¸ƒæ‰€æœ‰æŸå¤±æµ‹é‡å’Œæ¨¡å‹æ£€æŸ¥ç‚¹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å…¶éƒ¨ç½²éœ€è¦å¯¹è¶…å‚æ•°è¿›è¡Œç²¾å¿ƒä¼˜åŒ–ã€‚</li>
<li>å‘ç°è¶…å‚æ•°ç¼©æ”¾å®šå¾‹ï¼šæœ€ä½³å­¦ä¹ ç‡ä¸æ¨¡å‹å‚æ•°å’Œæ•°æ®å¤§å°å‘ˆå¹‚å¾‹å…³ç³»ï¼Œè€Œæœ€ä½³æ‰¹æ¬¡å¤§å°éšæ•°æ®å¤§å°æ‰©å±•ã€‚</li>
<li>åœ¨å›ºå®šæ¨¡å‹å’Œæ•°æ®å¤§å°æ¡ä»¶ä¸‹ï¼Œè¶…å‚æ•°ä¼˜åŒ–å‘ˆç°å‡¸æ€§ï¼Œå­˜åœ¨æœ€ä½³è¶…å‚æ•°å¹³å°ã€‚</li>
<li>æå‡ºé€šç”¨ã€å³æ’å³ç”¨çš„æœ€ä½³è¶…å‚æ•°å·¥å…·ï¼Œå…¶æ€§èƒ½æ¥è¿‘å…¨å±€æœ€ä¼˜ã€‚</li>
<li>è¿™äº›å®šå¾‹åœ¨æ¨¡å‹ç¨€ç–æ€§ã€æ•°æ®åˆ†å¸ƒå’Œæ¨¡å‹å½¢çŠ¶å˜åŒ–æ–¹é¢è¡¨ç°å‡ºç¨³å¥æ€§ã€‚</li>
<li>ç»Ÿä¸€äº†ä¸åŒçš„æ¨¡å‹å½¢çŠ¶å’Œç»“æ„ï¼Œå¦‚Mixture-of-Expertsæ¨¡å‹å’Œå¯†é›†è½¬æ¢å™¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04715">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-06a3c1006b6d5ac25becce852a58adab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9b8411ee116b56bb17a6e87e319ade8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f01b5392c37709fae7c479f2a7aaf82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0a97518e3e5afeac3fe933267199386.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73d2e510691da61526bc11fa0295f9cd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Universality-of-Layer-Level-Entropy-Weighted-Quantization-Beyond-Model-Architecture-and-Size"><a href="#Universality-of-Layer-Level-Entropy-Weighted-Quantization-Beyond-Model-Architecture-and-Size" class="headerlink" title="Universality of Layer-Level Entropy-Weighted Quantization Beyond Model   Architecture and Size"></a>Universality of Layer-Level Entropy-Weighted Quantization Beyond Model   Architecture and Size</h2><p><strong>Authors:Alireza Behtash, Marijan Fofonjka, Ethan Baird, Tyler Mauer, Hossein Moghimifam, David Stout, Joel Dennison</strong></p>
<p>We present a novel approach to selective model quantization that transcends the limitations of architecture-specific and size-dependent compression methods for Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ). By analyzing the entropy distribution across transformer blocks, EWQ determines which blocks can be safely quantized without causing significant performance degradation, independent of model architecture or size. Our method outperforms uniform quantization approaches, maintaining Massive Multitask Language Understanding (MMLU) accuracy scores within 0.5% of unquantized models while reducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ across multiple architectures-from 1.6B to 70B parameters-showcasing consistent improvements in the quality-compression trade-off regardless of model scale or architectural design. A surprising finding of EWQ is its ability to reduce perplexity compared to unquantized models, suggesting the presence of beneficial regularization through selective precision reduction. This improvement holds across different model families, indicating a fundamental relationship between layer-level entropy and optimal precision requirements. Additionally, we introduce FastEWQ, a rapid method for entropy distribution analysis that eliminates the need for loading model weights. This technique leverages universal characteristics of entropy distribution that persist across various architectures and scales, enabling near-instantaneous quantization decisions while maintaining 80% classification accuracy with full entropy analysis. Our results demonstrate that effective quantization strategies can be developed independently of specific architectural choices or model sizes, opening new possibilities for efficient LLM deployment. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç†µåŠ æƒé‡åŒ–ï¼ˆEWQï¼‰çš„é€‰æ‹©æ€§æ¨¡å‹é‡åŒ–æ–°æ–¹æ³•ï¼Œå®ƒè¶…è¶Šäº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç‰¹å®šæ¶æ„å’Œå¤§å°ä¾èµ–å‹ç¼©æ–¹æ³•çš„å±€é™ã€‚é€šè¿‡åˆ†æå˜å‹å™¨å—ä¹‹é—´çš„ç†µåˆ†å¸ƒï¼ŒEWQèƒ½å¤Ÿç¡®å®šå“ªäº›å—å¯ä»¥å®‰å…¨é‡åŒ–ï¼Œè€Œä¸ä¼šå¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè¿™ç‹¬ç«‹äºæ¨¡å‹æ¶æ„æˆ–å¤§å°ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç»Ÿä¸€é‡åŒ–æ–¹æ³•ä¸Šè¡¨ç°æ›´å¥½ï¼Œåœ¨ä¿æŒå¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼ˆMMLUï¼‰å‡†ç¡®åº¦å¾—åˆ†ä¸æœªé‡åŒ–æ¨¡å‹ç›¸å·®ä¸åˆ°0.5%çš„åŒæ—¶ï¼Œå‡å°‘äº†é«˜è¾¾18%çš„å†…å­˜ä½¿ç”¨ã€‚æˆ‘ä»¬è¯æ˜äº†EWQåœ¨å¤šæ¶æ„ï¼ˆä»1.6Båˆ°70Bå‚æ•°ï¼‰ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå±•ç¤ºäº†åœ¨è´¨é‡å‹ç¼©æƒè¡¡æ–¹é¢çš„ä¸€è‡´æ”¹è¿›ï¼Œæ— è®ºæ¨¡å‹è§„æ¨¡æˆ–æ¶æ„è®¾è®¡å¦‚ä½•ã€‚EWQçš„ä¸€ä¸ªæ„å¤–å‘ç°æ˜¯å®ƒèƒ½å¤Ÿå‡å°‘å›°æƒ‘åº¦ä¸æœªé‡åŒ–æ¨¡å‹ç›¸æ¯”ï¼Œè¿™è¡¨æ˜é€šè¿‡é€‰æ‹©æ€§åœ°å‡å°‘ç²¾åº¦ï¼Œå­˜åœ¨æœ‰ç›Šçš„æ­£è§„åŒ–ã€‚è¿™ä¸€æ”¹è¿›åœ¨ä¸åŒçš„æ¨¡å‹å®¶æ—ä¸­æ™®éå­˜åœ¨ï¼Œè¡¨æ˜å±‚çº§ç†µå’Œæœ€ä½³ç²¾åº¦è¦æ±‚ä¹‹é—´å­˜åœ¨æ ¹æœ¬å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†FastEWQï¼Œè¿™æ˜¯ä¸€ç§å¿«é€Ÿåˆ†æç†µåˆ†å¸ƒçš„æ–¹æ³•ï¼Œæ— éœ€åŠ è½½æ¨¡å‹æƒé‡ã€‚è¯¥æŠ€æœ¯åˆ©ç”¨ç†µåˆ†å¸ƒçš„é€šç”¨ç‰¹æ€§ï¼Œè¿™äº›ç‰¹æ€§åœ¨å„ç§æ¶æ„å’Œè§„æ¨¡ä¸­æŒç»­å­˜åœ¨ï¼Œå¯ä»¥åœ¨è¿›è¡Œè¿‘ä¹å³æ—¶çš„é‡åŒ–å†³ç­–æ—¶ä¿æŒ80%çš„åˆ†ç±»ç²¾åº¦ï¼ŒåŒæ—¶è¿›è¡Œå…¨é¢ç†µåˆ†æã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæœ‰æ•ˆçš„é‡åŒ–ç­–ç•¥å¯ä»¥ç‹¬ç«‹äºç‰¹å®šçš„æ¶æ„é€‰æ‹©æˆ–æ¨¡å‹å¤§å°è€Œå‘å±•ï¼Œä¸ºé«˜æ•ˆéƒ¨ç½²LLMæä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04704v1">PDF</a> 29 pages, 7 figures, 14 tables; Comments are welcome</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç†µåŠ æƒé‡åŒ–ï¼ˆEWQï¼‰çš„é€‰æ‹©æ€§æ¨¡å‹é‡åŒ–æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•çªç ´äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¶æ„ç‰¹å®šå’Œå¤§å°ä¾èµ–çš„å‹ç¼©æ–¹æ³•çš„å±€é™ã€‚é€šè¿‡åˆ†æå˜å‹å™¨å—çš„ç†µåˆ†å¸ƒï¼ŒEWQèƒ½å¤Ÿç¡®å®šå“ªäº›å—å¯ä»¥åœ¨ä¸å½±å“æ€§èƒ½æ˜¾è‘—é€€åŒ–çš„æƒ…å†µä¸‹è¿›è¡Œå®‰å…¨é‡åŒ–ï¼Œç‹¬ç«‹äºæ¨¡å‹æ¶æ„æˆ–å¤§å°ã€‚è¯¥æ–¹æ³•ä¼˜äºå‡åŒ€é‡åŒ–æ–¹æ³•ï¼Œåœ¨ä¿æŒå¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼ˆMMLUï¼‰å‡†ç¡®ç‡å¾—åˆ†ä¸æœªé‡åŒ–æ¨¡å‹ç›¸å·®ä¸åˆ°0.5%çš„åŒæ—¶ï¼Œå‡å°‘äº†é«˜è¾¾18%çš„å†…å­˜ä½¿ç”¨ã€‚å®éªŒè¯æ˜ï¼ŒEWQåœ¨å¤šä¸ªæ¶æ„ï¼ˆä»1.6Båˆ°70Bå‚æ•°ï¼‰ä¸­å‡æœ‰æ•ˆï¼Œåœ¨æ¨¡å‹è§„æ¨¡æˆ–æ¶æ„è®¾è®¡æ–¹é¢æ˜¾ç¤ºå‡ºä¸€è‡´çš„æ”¹è¿›è´¨é‡å’Œå‹ç¼©æƒè¡¡ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼ŒEWQèƒ½å¤Ÿå‡å°‘ä¸æœªé‡åŒ–æ¨¡å‹ç›¸æ¯”çš„å›°æƒ‘åº¦ï¼Œè¿™è¡¨æ˜é€šè¿‡é€‰æ‹©æ€§ç²¾åº¦é™ä½å­˜åœ¨æœ‰ç›Šçš„æ­£è§„åŒ–ã€‚è¿™ç§æ”¹è¿›åœ¨ä¸åŒæ¨¡å‹å®¶æ—ä¸­æ™®éå­˜åœ¨ï¼Œè¡¨æ˜å±‚çº§ç†µä¸æœ€ä½³ç²¾åº¦è¦æ±‚ä¹‹é—´å­˜åœ¨æ ¹æœ¬å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†FastEWQï¼Œè¿™æ˜¯ä¸€ç§å¿«é€Ÿåˆ†æç†µåˆ†å¸ƒçš„æ–¹æ³•ï¼Œæ— éœ€åŠ è½½æ¨¡å‹æƒé‡ã€‚è¯¥æŠ€æœ¯åˆ©ç”¨ç†µåˆ†å¸ƒçš„æ™®éç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾åœ¨å„ç§æ¶æ„å’Œè§„æ¨¡ä¸ŠæŒç»­å­˜åœ¨ï¼Œå¯ä»¥åœ¨ä¿æŒ80%åˆ†ç±»å‡†ç¡®åº¦çš„åŒæ—¶è¿›è¡Œå³æ—¶é‡åŒ–å†³ç­–ï¼ŒåŒæ—¶è¿›è¡Œå…¨é¢ç†µåˆ†æã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæœ‰æ•ˆçš„é‡åŒ–ç­–ç•¥å¯ä»¥ç‹¬ç«‹äºç‰¹å®šçš„æ¶æ„é€‰æ‹©æˆ–æ¨¡å‹å¤§å°è€Œå‘å±•ï¼Œä¸ºé«˜æ•ˆéƒ¨ç½²LLMå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºç†µåŠ æƒé‡åŒ–ï¼ˆEWQï¼‰çš„æ–°å‹é€‰æ‹©æ€§æ¨¡å‹é‡åŒ–æ–¹æ³•ã€‚</li>
<li>é€šè¿‡åˆ†æç†µåˆ†å¸ƒç¡®å®šå¯é‡åŒ–çš„æ¨¡å‹å—ï¼Œå®ç°ç‹¬ç«‹äºæ¶æ„å’Œå¤§å°çš„æ¨¡å‹é‡åŒ–ã€‚</li>
<li>EWQåœ¨ç»´æŒMMLUå‡†ç¡®ç‡æŸå¤±æå°çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—é™ä½äº†å†…å­˜ä½¿ç”¨ã€‚</li>
<li>EWQåœ¨å¤šä¸ªæ¨¡å‹å’Œæ¶æ„ä¸Šå‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬åœ¨æ¨¡å‹å¤§å°å’Œå®¶æ—æ–¹é¢çš„æ”¹è¿›ã€‚</li>
<li>EWQèƒ½å‡å°‘æœªé‡åŒ–æ¨¡å‹çš„å›°æƒ‘åº¦ï¼Œæš—ç¤ºäº†ç²¾åº¦é™ä½å¸¦æ¥çš„æ½œåœ¨æ­£åˆ™åŒ–æ•ˆæœã€‚</li>
<li>ä»‹ç»äº†FastEWQæ–¹æ³•ï¼Œèƒ½å¿«é€Ÿåˆ†æç†µåˆ†å¸ƒè€Œæ— éœ€åŠ è½½æ¨¡å‹æƒé‡ï¼Œå®ç°è¿‘å³æ—¶é‡åŒ–å†³ç­–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04704">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d249d9d7789c1b489d7e5a32aef7fe8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11b8d2c534618d7ab78286680144ce10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afe3a52aa4ff10c566b9bcee71c2f29e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Quantifying-the-Reasoning-Abilities-of-LLMs-on-Real-world-Clinical-Cases"><a href="#Quantifying-the-Reasoning-Abilities-of-LLMs-on-Real-world-Clinical-Cases" class="headerlink" title="Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases"></a>Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases</h2><p><strong>Authors:Pengcheng Qiu, Chaoyi Wu, Shuyu Liu, Weike Zhao, Ya Zhang, Yanfeng Wang, Weidi Xie</strong></p>
<p>The latest reasoning-enhanced large language models (reasoning LLMs), such as DeepSeek-R1 and OpenAI-o3, have demonstrated remarkable success. However, the application of such reasoning enhancements to the highly professional medical domain has not been clearly evaluated, particularly regarding with not only assessing the final generation but also examining the quality of their reasoning processes. In this study, we present MedR-Bench, a reasoning-focused medical evaluation benchmark comprising 1,453 structured patient cases with reasoning references mined from case reports. Our benchmark spans 13 body systems and 10 specialty disorders, encompassing both common and rare diseases. In our evaluation, we introduce a versatile framework consisting of three critical clinical stages: assessment recommendation, diagnostic decision-making, and treatment planning, comprehensively capturing the LLMsâ€™ performance across the entire patient journey in healthcare. For metrics, we propose a novel agentic system, Reasoning Evaluator, designed to automate and objectively quantify free-text reasoning responses in a scalable manner from the perspectives of efficiency, factuality, and completeness by dynamically searching and performing cross-referencing checks. As a result, we assess five state-of-the-art reasoning LLMs, including DeepSeek-R1, OpenAI-o3-mini, and others. Our results reveal that current LLMs can handle relatively simple diagnostic tasks with sufficient critical assessment results, achieving accuracy generally over 85%. However, they still struggle with more complex tasks, such as assessment recommendation and treatment planning. In reasoning, their reasoning processes are generally reliable, with factuality scores exceeding 90%, though they often omit critical reasoning steps. Our study clearly reveals further development directions for current clinical LLMs. </p>
<blockquote>
<p>æœ€æ–°å¢å¼ºæ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆæ¨ç†LLMï¼‰ï¼Œå¦‚DeepSeek-R1å’ŒOpenAI-o3ï¼Œå·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚ç„¶è€Œï¼Œå°†è¿™ç§æ¨ç†å¢å¼ºåº”ç”¨äºé«˜åº¦ä¸“ä¸šçš„åŒ»ç–—é¢†åŸŸå°šæœªå¾—åˆ°æ˜ç¡®è¯„ä¼°ï¼Œå°¤å…¶æ˜¯ä¸ä»…è¦è¯„ä¼°æœ€ç»ˆç”Ÿæˆçš„ç»“æœï¼Œè¿˜è¦æ£€æŸ¥å…¶æ¨ç†è¿‡ç¨‹çš„è´¨é‡ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MedR-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªä»¥æ¨ç†ä¸ºé‡ç‚¹çš„åŒ»ç–—è¯„ä¼°åŸºå‡†ï¼ŒåŒ…å«1453ä¸ªç»“æ„åŒ–æ‚£è€…ç—…ä¾‹ï¼Œä»¥åŠä»ç—…ä¾‹æŠ¥å‘Šä¸­æŒ–æ˜çš„æ¨ç†å‚è€ƒã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ¶µç›–äº†13ä¸ªèº«ä½“ç³»ç»Ÿå’Œ10ç§ä¸“ä¸šç–¾ç—…ï¼ŒåŒ…æ‹¬å¸¸è§å’Œç½•è§ç–¾ç—…ã€‚åœ¨è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé€šç”¨çš„æ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªå…³é”®ä¸´åºŠé˜¶æ®µï¼šè¯„ä¼°æ¨èã€è¯Šæ–­å†³ç­–å’Œæ²»ç–—è®¡åˆ’ï¼Œå…¨é¢æ•æ‰LLMåœ¨åŒ»ç–—ä¿å¥çš„æ•´ä¸ªæ‚£è€…æ—…ç¨‹ä¸­çš„è¡¨ç°ã€‚å¯¹äºæŒ‡æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä»£ç†ç³»ç»Ÿï¼Œå³æ¨ç†è¯„ä¼°å™¨ï¼Œæ—¨åœ¨ä»¥åŠ¨æ€æœç´¢å’Œäº¤å‰å¼•ç”¨çš„æ–¹å¼è‡ªåŠ¨å’Œå®¢è§‚åœ°é‡åŒ–è‡ªç”±æ–‡æœ¬æ¨ç†çš„å“åº”ï¼Œä»æ•ˆç‡ã€çœŸå®æ€§å’Œå®Œæ•´æ€§ä¸‰ä¸ªæ–¹é¢è¿›è¡Œè§„æ¨¡åŒ–è¯„ä¼°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹äº”æ¬¾æœ€å…ˆè¿›çš„æ¨ç†LLMè¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬DeepSeek-R1ã€OpenAI-o3-miniç­‰ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰LLMå¯ä»¥å¤„ç†ç›¸å¯¹ç®€å•çš„è¯Šæ–­ä»»åŠ¡ï¼Œå¹¶è·å¾—è¶³å¤Ÿçš„æ‰¹åˆ¤æ€§è¯„ä¼°ç»“æœï¼Œå‡†ç¡®ç‡æ™®éè¶…è¿‡85%ã€‚ä½†åœ¨æ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¦‚è¯„ä¼°æ¨èå’Œæ²»ç–—è®¡åˆ’æ–¹é¢ï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚åœ¨æ¨ç†æ–¹é¢ï¼Œå®ƒä»¬çš„æ¨ç†è¿‡ç¨‹é€šå¸¸å¯é ï¼ŒçœŸå®æ€§å¾—åˆ†è¶…è¿‡90%ï¼Œå°½ç®¡å®ƒä»¬ç»å¸¸çœç•¥å…³é”®çš„æ¨ç†æ­¥éª¤ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ¸…æ¥šåœ°æŒ‡å‡ºäº†å½“å‰ä¸´åºŠLLMçš„è¿›ä¸€æ­¥å‘å±•æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04691v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ€æ–°çš„å¢å¼ºæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚DeepSeek-R1å’ŒOpenAI-o3ï¼‰å·²ç»å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚ç„¶è€Œï¼Œè¿™äº›æ¨ç†å¢å¼ºåœ¨é«˜åº¦ä¸“ä¸šçš„åŒ»ç–—é¢†åŸŸçš„åº”ç”¨å°šæœªå¾—åˆ°æ˜ç¡®è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯„ä¼°æœ€ç»ˆä¸€ä»£çš„åŒæ—¶ï¼Œä¹Ÿè¦è€ƒå¯Ÿå…¶æ¨ç†è¿‡ç¨‹çš„è´¨é‡ã€‚æœ¬ç ”ç©¶æå‡ºäº†MedR-Benchï¼Œä¸€ä¸ªä»¥æ¨ç†ä¸ºé‡ç‚¹çš„åŒ»ç–—è¯„ä¼°åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä»ç—…ä¾‹æŠ¥å‘Šä¸­æŒ–æ˜çš„1453ä¸ªç»“æ„åŒ–æ‚£è€…ç—…ä¾‹çš„æ¨ç†å‚è€ƒã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ¶µç›–äº†13ä¸ªèº«ä½“ç³»ç»Ÿå’Œ10ç§ä¸“ä¸šç–¾ç—…ï¼ŒåŒ…æ‹¬å¸¸è§å’Œç½•è§ç–¾ç—…ã€‚åœ¨è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªå…³é”®ä¸´åºŠé˜¶æ®µçš„é€šç”¨æ¡†æ¶ï¼šè¯„ä¼°å»ºè®®ã€è¯Šæ–­å†³ç­–å’Œæ²»ç–—è®¡åˆ’ï¼Œå…¨é¢æ•æ‰LLMsåœ¨åŒ»ç–—ä¿å¥ä¸­çš„æ•´ä¸ªæ‚£è€…æ—…ç¨‹ä¸­çš„è¡¨ç°ã€‚ä¸ºäº†è¡¡é‡æŒ‡æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ™ºèƒ½ç³»ç»Ÿâ€”â€”æ¨ç†è¯„ä¼°å™¨ï¼Œæ—¨åœ¨è‡ªåŠ¨å’Œå®¢è§‚åœ°ä»¥å¯ä¼¸ç¼©çš„æ–¹å¼ä»æ•ˆç‡ã€äº‹å®å’Œå®Œæ•´æ€§ç­‰è§’åº¦å¯¹è‡ªç”±æ–‡æœ¬æ¨ç†å›ç­”è¿›è¡Œé‡åŒ–è¯„ä¼°ï¼Œå¹¶è¿›è¡ŒåŠ¨æ€äº¤å‰å¼•ç”¨æ£€æŸ¥ã€‚ç»“æœè¯„ä¼°äº†äº”ç§æœ€æ–°æ¨ç†LLMï¼ŒåŒ…æ‹¬DeepSeek-R1ã€OpenAI-o3-miniç­‰ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰LLMså¯ä»¥å¤„ç†ç›¸å¯¹ç®€å•çš„è¯Šæ–­ä»»åŠ¡ï¼Œå¹¶å–å¾—è¶…è¿‡85%çš„å‡†ç¡®ç‡ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ›´å¤æ‚çš„ä»»åŠ¡ï¼ˆå¦‚è¯„ä¼°å»ºè®®å’Œæ²»ç–—è®¡åˆ’ï¼‰æ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚åœ¨æ¨ç†æ–¹é¢ï¼Œå®ƒä»¬çš„æ¨ç†è¿‡ç¨‹é€šå¸¸å¯é ï¼Œäº‹å®å¾—åˆ†è¶…è¿‡90%ï¼Œä½†å®ƒä»¬ç»å¸¸çœç•¥å…³é”®çš„æ¨ç†æ­¥éª¤ã€‚æœ¬ç ”ç©¶æ˜ç¡®äº†å½“å‰ä¸´åºŠLLMçš„å‘å±•æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ€æ–°æ¨ç†å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»ç–—é¢†åŸŸåº”ç”¨å°šæœªå……åˆ†è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯Šæ–­ã€æ²»ç–—å’Œè¯„ä¼°çš„æ•´ä¸ªè¿‡ç¨‹ä¸­ã€‚</li>
<li>MedR-BenchåŸºå‡†æµ‹è¯•åŒ…å«ç»“æ„åŒ–æ‚£è€…ç—…ä¾‹çš„æ¨ç†å‚è€ƒï¼Œæ¶µç›–å¤šä¸ªèº«ä½“ç³»ç»Ÿå’Œç–¾ç—…ç±»å‹ã€‚</li>
<li>æ¨ç†LLMsåœ¨ç®€å•çš„è¯Šæ–­ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œå‡†ç¡®ç‡è¶…è¿‡85%ï¼Œä½†åœ¨å¤æ‚çš„è¯„ä¼°å»ºè®®å’Œæ²»ç–—è®¡åˆ’ä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æ¨ç†è¿‡ç¨‹æ€»ä½“å¯é ï¼Œäº‹å®å¾—åˆ†é«˜ï¼Œä½†æœ‰æ—¶ä¼šçœç•¥å…³é”®æ¨ç†æ­¥éª¤ã€‚</li>
<li>å½“å‰LLMåœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ä»æœ‰è¿›ä¸€æ­¥å‘å±•çš„ç©ºé—´ï¼Œéœ€è¦æ”¹è¿›å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°å’Œæ¨ç†è¿‡ç¨‹çš„å®Œæ•´æ€§ã€‚</li>
<li>æå‡ºçš„æ¨ç†è¯„ä¼°å·¥å…·èƒ½å¤Ÿä¸ºLLMçš„è¯„ä¼°å’Œæ”¹è‰¯æä¾›æœ‰ä»·å€¼çš„å‚è€ƒã€‚</li>
<li>ç»“åˆåŒ»ç–—é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†å’ŒLLMçš„æŠ€æœ¯ä¼˜åŠ¿ï¼Œæœ‰æœ›ä¸ºåŒ»ç–—è¯Šæ–­ã€æ²»ç–—å’Œè¯„ä¼°å¸¦æ¥åˆ›æ–°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04691">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-18d32da47588c4de33be991e39cd5cd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f84f980e94d6644142887e9735efd997.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LLM-guided-Plan-and-Retrieval-A-Strategic-Alignment-for-Interpretable-User-Satisfaction-Estimation-in-Dialogue"><a href="#LLM-guided-Plan-and-Retrieval-A-Strategic-Alignment-for-Interpretable-User-Satisfaction-Estimation-in-Dialogue" class="headerlink" title="LLM-guided Plan and Retrieval: A Strategic Alignment for Interpretable   User Satisfaction Estimation in Dialogue"></a>LLM-guided Plan and Retrieval: A Strategic Alignment for Interpretable   User Satisfaction Estimation in Dialogue</h2><p><strong>Authors:Sangyeop Kim, Sohhyung Park, Jaewon Jung, Jinseok Kim, Sungzoon Cho</strong></p>
<p>Understanding user satisfaction with conversational systems, known as User Satisfaction Estimation (USE), is essential for assessing dialogue quality and enhancing user experiences. However, existing methods for USE face challenges due to limited understanding of underlying reasons for user dissatisfaction and the high costs of annotating user intentions. To address these challenges, we propose PRAISE (Plan and Retrieval Alignment for Interpretable Satisfaction Estimation), an interpretable framework for effective user satisfaction prediction. PRAISE operates through three key modules. The Strategy Planner develops strategies, which are natural language criteria for classifying user satisfaction. The Feature Retriever then incorporates knowledge on user satisfaction from Large Language Models (LLMs) and retrieves relevance features from utterances. Finally, the Score Analyzer evaluates strategy predictions and classifies user satisfaction. Experimental results demonstrate that PRAISE achieves state-of-the-art performance on three benchmarks for the USE task. Beyond its superior performance, PRAISE offers additional benefits. It enhances interpretability by providing instance-level explanations through effective alignment of utterances with strategies. Moreover, PRAISE operates more efficiently than existing approaches by eliminating the need for LLMs during the inference phase. </p>
<blockquote>
<p>ç†è§£ç”¨æˆ·å¯¹å¯¹è¯ç³»ç»Ÿçš„æ»¡æ„åº¦ï¼Œä¹Ÿç§°ä¸ºç”¨æˆ·æ»¡æ„åº¦ä¼°ç®—ï¼ˆUSEï¼‰ï¼Œå¯¹äºè¯„ä¼°å¯¹è¯è´¨é‡å’Œæå‡ç”¨æˆ·ä½“éªŒè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„USEæ–¹æ³•é¢ä¸´ç€æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬å¯¹å¯¼è‡´ç”¨æˆ·ä¸æ»¡çš„æ½œåœ¨åŸå› ç†è§£æœ‰é™ï¼Œå¹¶ä¸”æ ‡æ³¨ç”¨æˆ·æ„å›¾çš„æˆæœ¬å¾ˆé«˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†PRAISEï¼ˆé¢å‘å¯è§£é‡Šæ»¡æ„åº¦ä¼°è®¡çš„è®¡åˆ’ä¸æ£€ç´¢å¯¹é½ï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æœ‰æ•ˆé¢„æµ‹ç”¨æˆ·æ»¡æ„åº¦çš„å¯è§£é‡Šæ¡†æ¶ã€‚PRAISEé€šè¿‡ä¸‰ä¸ªä¸»è¦æ¨¡å—è¿›è¡Œæ“ä½œã€‚ç­–ç•¥è§„åˆ’å™¨åˆ¶å®šåˆ†ç±»ç”¨æˆ·æ»¡æ„åº¦çš„è‡ªç„¶è¯­è¨€æ ‡å‡†ã€‚ç‰¹å¾æ£€ç´¢å™¨ç„¶åç»“åˆæ¥è‡ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”¨æˆ·æ»¡æ„åº¦çŸ¥è¯†ï¼Œå¹¶ä»å‘è¨€ä¸­æ£€ç´¢ç›¸å…³ç‰¹å¾ã€‚æœ€åï¼Œåˆ†æ•°åˆ†æå™¨è¯„ä¼°ç­–ç•¥é¢„æµ‹å¹¶åˆ†ç±»ç”¨æˆ·æ»¡æ„åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPRAISEåœ¨USEä»»åŠ¡çš„ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚é™¤äº†å“è¶Šçš„æ€§èƒ½å¤–ï¼ŒPRAISEè¿˜æä¾›äº†å…¶ä»–ä¼˜åŠ¿ã€‚å®ƒé€šè¿‡æœ‰æ•ˆåœ°å°†å‘è¨€ä¸ç­–ç•¥å¯¹é½ï¼Œæä¾›äº†å®ä¾‹çº§çš„è§£é‡Šï¼Œå¢å¼ºäº†å¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼ŒPRAISEåœ¨æ¨ç†é˜¶æ®µä¸éœ€è¦ä½¿ç”¨LLMï¼Œå› æ­¤æ¯”ç°æœ‰æ–¹æ³•è¿è¡Œå¾—æ›´é«˜æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04675v1">PDF</a> Accepted by NAACL 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¯¹è¯ç³»ç»Ÿçš„ç”¨æˆ·æ»¡æ„åº¦è¯„ä¼°å¯¹äºè¡¡é‡å¯¹è¯è´¨é‡å’Œæå‡ç”¨æˆ·ä½“éªŒè‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´ç”¨æˆ·ä¸æ»¡æ„æ·±å±‚åŸå› ç†è§£å’Œæ ‡æ³¨ç”¨æˆ·æ„å›¾çš„é«˜æˆæœ¬ç­‰æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†PRAISEæ¡†æ¶ï¼Œé€šè¿‡ç­–ç•¥è§„åˆ’ã€ç‰¹å¾æ£€ç´¢å’Œè¯„åˆ†åˆ†æä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—ï¼Œå®ç°æœ‰æ•ˆçš„ç”¨æˆ·æ»¡æ„åº¦é¢„æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPRAISEåœ¨ä¸‰ä¸ªç”¨æˆ·æ»¡æ„åº¦è¯„ä¼°åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚åŒæ—¶ï¼ŒPRAISEå¢å¼ºäº†å¯è§£é‡Šæ€§ï¼Œé€šè¿‡å¯¹å¯¹è¯çš„ç­–ç•¥è§„åˆ’å’Œå¯¹ç­–ç•¥çš„æœ‰æ•ˆå¯¹é½ï¼Œæä¾›å®ä¾‹å±‚é¢çš„è§£é‡Šã€‚æ­¤å¤–ï¼ŒPRAISEåœ¨æ¨ç†é˜¶æ®µä¸éœ€è¦å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ›´åŠ é«˜æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”¨æˆ·æ»¡æ„åº¦è¯„ä¼°å¯¹äºå¯¹è¯ç³»ç»Ÿè‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ç”¨æˆ·æ»¡æ„åº¦è¯„ä¼°æ–¹æ³•é¢ä¸´æ·±å±‚åŸå› ç†è§£å’Œé«˜æˆæœ¬ç­‰æŒ‘æˆ˜ã€‚</li>
<li>PRAISEæ¡†æ¶é€šè¿‡ç­–ç•¥è§„åˆ’ã€ç‰¹å¾æ£€ç´¢å’Œè¯„åˆ†åˆ†æä¸‰ä¸ªæ ¸å¿ƒæ¨¡å—å®ç°æœ‰æ•ˆç”¨æˆ·æ»¡æ„åº¦é¢„æµ‹ã€‚</li>
<li>PRAISEæ¡†æ¶åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>PRAISEå¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œæä¾›å®ä¾‹å±‚é¢çš„è§£é‡Šã€‚</li>
<li>PRAISEé€šè¿‡æœ‰æ•ˆå¯¹é½ç­–ç•¥ï¼Œæå‡äº†ç”¨æˆ·æ»¡æ„åº¦è¯„ä¼°çš„ç²¾ç¡®åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04675">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-67929cfc575f0aab400d29f256936957.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a27f016e2c1ac3a1138fb7ddfd69a0fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0eaa91936d1e3b0e562e5366e7e9dbf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2f6ca55850c12403f4da139bad89a12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4bcec0ddb3e5c09f06946dc96232fc83.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b84c6fceeece5308929225cd1e9bf66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4786273dccb6b4ed979fca35d8045251.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8012a9af1350da656ca5baf8860eca3e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Implicit-Cross-Lingual-Rewarding-for-Efficient-Multilingual-Preference-Alignment"><a href="#Implicit-Cross-Lingual-Rewarding-for-Efficient-Multilingual-Preference-Alignment" class="headerlink" title="Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference   Alignment"></a>Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference   Alignment</h2><p><strong>Authors:Wen Yang, Junhong Wu, Chen Wang, Chengqing Zong, Jiajun Zhang</strong></p>
<p>Direct Preference Optimization (DPO) has become a prominent method for aligning Large Language Models (LLMs) with human preferences. While DPO has enabled significant progress in aligning English LLMs, multilingual preference alignment is hampered by data scarcity. To address this, we propose a novel approach that $\textit{captures}$ learned preferences from well-aligned English models by implicit rewards and $\textit{transfers}$ them to other languages through iterative training. Specifically, we derive an implicit reward model from the logits of an English DPO-aligned model and its corresponding reference model. This reward model is then leveraged to annotate preference relations in cross-lingual instruction-following pairs, using English instructions to evaluate multilingual responses. The annotated data is subsequently used for multilingual DPO fine-tuning, facilitating preference knowledge transfer from English to other languages. Fine-tuning Llama3 for two iterations resulted in a 12.72% average improvement in Win Rate and a 5.97% increase in Length Control Win Rate across all training languages on the X-AlpacaEval leaderboard. Our findings demonstrate that leveraging existing English-aligned models can enable efficient and effective multilingual preference alignment, significantly reducing the need for extensive multilingual preference data. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding">https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding</a> </p>
<blockquote>
<p>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å·²æˆä¸ºä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½ä¸€è‡´çš„é‡è¦æ–¹æ³•ã€‚è™½ç„¶DPOåœ¨è‹±è¯­LLMçš„å¯¹é½ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å¤šè¯­è¨€åå¥½å¯¹é½å´å—åˆ°æ•°æ®ç¨€ç¼ºçš„é˜»ç¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡éšæ€§å¥–åŠ±ä»å·²å¯¹é½çš„è‹±è¯­æ¨¡å‹ä¸­æ•è·å­¦ä¹ åˆ°çš„åå¥½ï¼Œå¹¶é€šè¿‡è¿­ä»£è®­ç»ƒå°†å®ƒä»¬è½¬ç§»åˆ°å…¶ä»–è¯­è¨€ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»è‹±è¯­DPOå¯¹é½æ¨¡å‹åŠå…¶ç›¸åº”å‚è€ƒæ¨¡å‹çš„é€»è¾‘æ¦‚ç‡ä¸­æ¨å¯¼å‡ºéšæ€§å¥–åŠ±æ¨¡å‹ã€‚ç„¶åï¼Œè¯¥å¥–åŠ±æ¨¡å‹è¢«ç”¨äºæ ‡æ³¨è·¨è¯­è¨€æŒ‡ä»¤å¯¹ä¸­çš„åå¥½å…³ç³»ï¼Œä½¿ç”¨è‹±è¯­æŒ‡ä»¤æ¥è¯„ä¼°å¤šè¯­è¨€å“åº”ã€‚éšåä½¿ç”¨è¿™äº›æ ‡æ³¨æ•°æ®è¿›è¡Œå¤šè¯­è¨€DPOå¾®è°ƒï¼Œä¿ƒè¿›ä»è‹±è¯­åˆ°å…¶ä»–è¯­è¨€çš„åå¥½çŸ¥è¯†è½¬ç§»ã€‚å¯¹Llama3è¿›è¡Œä¸¤æ¬¡è¿­ä»£å¾®è°ƒåï¼Œå…¶åœ¨X-AlpacaEvalæ’è¡Œæ¦œä¸Šæ‰€æœ‰è®­ç»ƒè¯­è¨€çš„èƒœç‡å¹³å‡æé«˜äº†12.72%ï¼Œé•¿åº¦æ§åˆ¶èƒœç‡æé«˜äº†5.97%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨ç°æœ‰çš„è‹±è¯­å¯¹é½æ¨¡å‹å¯ä»¥å®ç°é«˜æ•ˆçš„å¤šè¯­è¨€åå¥½å¯¹é½ï¼Œä»è€Œå¤§å¤§å‡å°‘äº†å¯¹å¤§é‡å¤šè¯­è¨€åå¥½æ•°æ®çš„éœ€æ±‚ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ZNLP/Implicit-Cross-Lingual-Rewardingæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04647v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ä»‹ç»äº†Direct Preference Optimizationï¼ˆDPOï¼‰æ–¹æ³•åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹çš„åº”ç”¨æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•é€šè¿‡ä»å·²å¯¹é½çš„è‹±æ–‡æ¨¡å‹ä¸­æ•è·å­¦ä¹ åå¥½ï¼Œå¹¶åˆ©ç”¨è¿­ä»£è®­ç»ƒå°†å…¶è½¬ç§»åˆ°å…¶ä»–è¯­è¨€ã€‚å…·ä½“åœ°ï¼Œæˆ‘ä»¬ä»è‹±æ–‡DPOå¯¹é½æ¨¡å‹çš„logitsåŠå…¶å¯¹åº”çš„å‚è€ƒæ¨¡å‹ä¸­æ¨å¯¼å‡ºéšå¼å¥–åŠ±æ¨¡å‹ï¼Œç”¨äºæ ‡æ³¨è·¨è¯­è¨€æŒ‡ä»¤å¯¹ä¸­çš„åå¥½å…³ç³»ã€‚åˆ©ç”¨è‹±æ–‡æŒ‡ä»¤è¯„ä¼°å¤šè¯­è¨€å“åº”ï¼Œå¹¶ä½¿ç”¨æ ‡æ³¨æ•°æ®è¿›è¡Œå¤šè¯­è¨€DPOå¾®è°ƒï¼Œå®ç°äº†ä»è‹±è¯­åˆ°å…¶ä»–è¯­è¨€çš„åå¥½çŸ¥è¯†è½¬ç§»ã€‚å¯¹Llama3è¿›è¡Œä¸¤æ¬¡è¿­ä»£å¾®è°ƒåï¼Œåœ¨X-AlpacaEvalæ’è¡Œæ¦œä¸Šå¹³å‡èƒœç‡æé«˜äº†12.72%ï¼Œé•¿åº¦æ§åˆ¶èƒœç‡æé«˜äº†5.97%ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨ç°æœ‰è‹±è¯­å¯¹é½æ¨¡å‹å¯å®ç°é«˜æ•ˆçš„å¤šè¯­è¨€åå¥½å¯¹é½ï¼Œæ˜¾è‘—é™ä½å¯¹å¤§é‡å¤šè¯­è¨€åå¥½æ•°æ®çš„éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DPOå·²æˆä¸ºä¸äººç±»åå¥½å¯¹é½çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ˜¾è‘—æ–¹æ³•ï¼Œä½†åœ¨å¤šè¯­è¨€ç¯å¢ƒä¸‹å­˜åœ¨æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡ä»å·²å¯¹é½çš„è‹±æ–‡æ¨¡å‹ä¸­æ•è·å­¦ä¹ åå¥½ï¼Œå¹¶å°†å…¶è½¬ç§»åˆ°å…¶ä»–è¯­è¨€ã€‚</li>
<li>åˆ©ç”¨éšå¼å¥–åŠ±æ¨¡å‹æ ‡æ³¨è·¨è¯­è¨€æŒ‡ä»¤å¯¹ä¸­çš„åå¥½å…³ç³»ï¼Œä½¿ç”¨è‹±æ–‡æŒ‡ä»¤è¯„ä¼°å¤šè¯­è¨€å“åº”ã€‚</li>
<li>ä½¿ç”¨æ ‡æ³¨æ•°æ®è¿›è¡Œå¤šè¯­è¨€DPOå¾®è°ƒï¼Œå®ç°åå¥½çŸ¥è¯†ä»è‹±è¯­åˆ°å…¶ä»–è¯­è¨€çš„è½¬ç§»ã€‚</li>
<li>å¯¹Llama3è¿›è¡Œå¾®è°ƒåï¼Œåœ¨X-AlpacaEvalæ’è¡Œæ¦œä¸Šçš„è¡¨ç°æœ‰æ˜¾è‘—æ”¹å–„ï¼Œå¹³å‡èƒœç‡æé«˜12.72%ï¼Œé•¿åº¦æ§åˆ¶èƒœç‡æé«˜5.97%ã€‚</li>
<li>åˆ©ç”¨ç°æœ‰è‹±è¯­å¯¹é½æ¨¡å‹å¯å®ç°å¤šè¯­è¨€åå¥½å¯¹é½ï¼Œé™ä½å¯¹å¤§é‡å¤šè¯­è¨€åå¥½æ•°æ®çš„éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04647">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-700304422a455e7afe041ff75e0050db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-502e6dc05de37c1cb8eef4a9af676c9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23c6cc6ae33513e893716a660240ae42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9983bbb5c61546a7ca149f798d8b5660.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Mark-Your-LLM-Detecting-the-Misuse-of-Open-Source-Large-Language-Models-via-Watermarking"><a href="#Mark-Your-LLM-Detecting-the-Misuse-of-Open-Source-Large-Language-Models-via-Watermarking" class="headerlink" title="Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models   via Watermarking"></a>Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models   via Watermarking</h2><p><strong>Authors:Yijie Xu, Aiwei Liu, Xuming Hu, Lijie Wen, Hui Xiong</strong></p>
<p>As open-source large language models (LLMs) like Llama3 become more capable, it is crucial to develop watermarking techniques to detect their potential misuse. Existing watermarking methods either add watermarks during LLM inference, which is unsuitable for open-source LLMs, or primarily target classification LLMs rather than recent generative LLMs. Adapting these watermarks to open-source LLMs for misuse detection remains an open challenge. This work defines two misuse scenarios for open-source LLMs: intellectual property (IP) violation and LLM Usage Violation. Then, we explore the application of inference-time watermark distillation and backdoor watermarking in these contexts. We propose comprehensive evaluation methods to assess the impact of various real-world further fine-tuning scenarios on watermarks and the effect of these watermarks on LLM performance. Our experiments reveal that backdoor watermarking could effectively detect IP Violation, while inference-time watermark distillation is applicable in both scenarios but less robust to further fine-tuning and has a more significant impact on LLM performance compared to backdoor watermarking. Exploring more advanced watermarking methods for open-source LLMs to detect their misuse should be an important future direction. </p>
<blockquote>
<p>éšç€åƒLlama3è¿™æ ·çš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›è¶Šæ¥è¶Šå¼ºï¼Œå¼€å‘æ°´å°æŠ€æœ¯æ¥æ£€æµ‹å…¶æ½œåœ¨æ»¥ç”¨æƒ…å†µå˜å¾—è‡³å…³é‡è¦ã€‚ç°æœ‰çš„æ°´å°æ–¹æ³•è¦ä¹ˆåœ¨LLMæ¨ç†è¿‡ç¨‹ä¸­æ·»åŠ æ°´å°ï¼Œè¿™ä¸é€‚ç”¨äºå¼€æºLLMï¼Œè¦ä¹ˆä¸»è¦é’ˆå¯¹åˆ†ç±»LLMè€Œéæœ€æ–°çš„ç”Ÿæˆå¼LLMã€‚å°†è¿™äº›æ°´å°é€‚åº”äºå¼€æºLLMä»¥è¿›è¡Œæ»¥ç”¨æ£€æµ‹ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾æ€§çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡å®šä¹‰äº†å¼€æºLLMçš„ä¸¤ç§æ»¥ç”¨åœºæ™¯ï¼šçŸ¥è¯†äº§æƒï¼ˆIPï¼‰ä¾µçŠ¯å’ŒLLMä½¿ç”¨è¿è§„ã€‚ç„¶åï¼Œæˆ‘ä»¬æ¢è®¨äº†æ¨ç†æ—¶é—´æ°´å°è’¸é¦å’Œåé—¨æ°´å°åœ¨è¿™äº›ä¸Šä¸‹æ–‡ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†å…¨é¢çš„è¯„ä¼°æ–¹æ³•ï¼Œä»¥è¯„ä¼°å„ç§ç°å®ä¸–ç•Œä¸­çš„è¿›ä¸€æ­¥å¾®è°ƒåœºæ™¯å¯¹æ°´å°çš„å½±å“ä»¥åŠè¿™äº›æ°´å°å¯¹LLMæ€§èƒ½çš„å½±å“ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåé—¨æ°´å°æŠ€æœ¯å¯ä»¥æœ‰æ•ˆæ£€æµ‹IPä¾µçŠ¯ï¼Œè€Œæ¨ç†æ—¶é—´æ°´å°è’¸é¦åœ¨ä¸¤ç§åœºæ™¯ä¸­éƒ½å¯åº”ç”¨ï¼Œä½†å¯¹æŠ—è¿›ä¸€æ­¥çš„å¾®è°ƒèƒ½åŠ›è¾ƒå¼±ï¼Œå¹¶ä¸”å¯¹LLMæ€§èƒ½çš„å½±å“æ¯”åé—¨æ°´å°æŠ€æœ¯æ›´ä¸ºæ˜¾è‘—ã€‚æ¢ç´¢æ›´å…ˆè¿›çš„æ°´å°æ–¹æ³•ç”¨äºå¼€æºLLMä»¥æ£€æµ‹å…¶æ»¥ç”¨æƒ…å†µï¼Œåº”æˆä¸ºæœªæ¥é‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04636v1">PDF</a> Accepted by the 1st Workshop on GenAI Watermarking, collocated with   ICLR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚Llama3çš„èƒ½åŠ›ä¸æ–­å¢å¼ºï¼Œå¼€å‘æ°´å°æŠ€æœ¯æ¥æ£€æµ‹å…¶æ½œåœ¨è¯¯ç”¨è‡³å…³é‡è¦ã€‚ç°æœ‰æ°´å°æ–¹æ³•è¦ä¹ˆåœ¨LLMæ¨ç†è¿‡ç¨‹ä¸­æ·»åŠ æ°´å°ï¼Œè¿™ä¸é€‚ç”¨äºå¼€æºLLMï¼Œè¦ä¹ˆä¸»è¦é’ˆå¯¹åˆ†ç±»LLMè€Œéæœ€æ–°çš„ç”Ÿæˆå¼LLMã€‚å°†è¿™äº›æ°´å°é€‚åº”äºå¼€æºLLMä»¥è¿›è¡Œè¯¯æ£€æµ‹ä»æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚æœ¬æ–‡å®šä¹‰äº†å¼€æºLLMçš„ä¸¤ç§è¯¯ç”¨åœºæ™¯ï¼šçŸ¥è¯†äº§æƒï¼ˆIPï¼‰ä¾µçŠ¯å’ŒLLMä½¿ç”¨è¿è§„ã€‚ç„¶åï¼Œæˆ‘ä»¬æ¢ç´¢äº†æ¨ç†æ—¶é—´æ°´å°è’¸é¦å’Œåé—¨æ°´å°åœ¨è¿™äº›åœºæ™¯ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†å…¨é¢çš„è¯„ä¼°æ–¹æ³•æ¥è¯„ä¼°å„ç§ç°å®ä¸–ç•Œè¿›ä¸€æ­¥çš„å¾®è°ƒåœºæ™¯å¯¹æ°´å°çš„å½±å“ä»¥åŠè¿™äº›æ°´å°å¯¹LLMæ€§èƒ½çš„å½±å“ã€‚å®éªŒè¡¨æ˜ï¼Œåé—¨æ°´å°æŠ€æœ¯èƒ½æœ‰æ•ˆæ£€æµ‹IPä¾µçŠ¯ï¼Œè€Œæ¨ç†æ—¶é—´æ°´å°è’¸é¦åœ¨ä¸¤ç§åœºæ™¯ä¸­å‡å¯åº”ç”¨ï¼Œä½†å¯¹è¿›ä¸€æ­¥å¾®è°ƒä¸å¤ªç¨³å¥ï¼Œå¹¶ä¸”å¯¹LLMæ€§èƒ½çš„å½±å“æ¯”åé—¨æ°´å°æŠ€æœ¯æ›´å¤§ã€‚æ¢ç´¢æ›´å…ˆè¿›çš„æ°´å°æ–¹æ³•ç”¨äºå¼€æºLLMä»¥æ£€æµ‹å…¶è¯¯ç”¨æ˜¯ä¸€ä¸ªé‡è¦çš„æœªæ¥æ–¹å‘ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>çŸ¥è¯†äº§æƒï¼ˆIPï¼‰ä¾µçŠ¯å’ŒLLMä½¿ç”¨è¿è§„æ˜¯å¼€æºLLMçš„ä¸¤ç§ä¸»è¦è¯¯ç”¨åœºæ™¯ã€‚</li>
<li>åé—¨æ°´å°æŠ€æœ¯èƒ½æœ‰æ•ˆæ£€æµ‹IPä¾µçŠ¯ã€‚</li>
<li>æ¨ç†æ—¶é—´æ°´å°è’¸é¦é€‚ç”¨äºä¸¤ç§è¯¯ç”¨åœºæ™¯ï¼Œä½†å¯¹è¿›ä¸€æ­¥å¾®è°ƒä¸å¤ªç¨³å¥ã€‚</li>
<li>æ¨ç†æ—¶é—´æ°´å°è’¸é¦å¯¹LLMæ€§èƒ½çš„å½±å“å¤§äºåé—¨æ°´å°æŠ€æœ¯ã€‚</li>
<li>å½“å‰æ°´å°æŠ€æœ¯é¢ä¸´é€‚åº”å¼€æºLLMçš„æŒ‘æˆ˜ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶å’Œæ”¹è¿›ã€‚</li>
<li>æ°´å°æŠ€æœ¯å¯¹äºæ£€æµ‹LLMè¯¯ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04636">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b91da5e7ec3578e7243beeb0f082961e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ed1d42d622fa93219583cf09e35b7a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-536f70c9b09407a462b773c879cad08b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f621152c7bcbdb31001dbeb01e58dc4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Better-Process-Supervision-with-Bi-directional-Rewarding-Signals"><a href="#Better-Process-Supervision-with-Bi-directional-Rewarding-Signals" class="headerlink" title="Better Process Supervision with Bi-directional Rewarding Signals"></a>Better Process Supervision with Bi-directional Rewarding Signals</h2><p><strong>Authors:Wenxiang Chen, Wei He, Zhiheng Xi, Honglin Guo, Boyang Hong, Jiazheng Zhang, Rui Zheng, Nijun Li, Tao Gui, Yun Li, Qi Zhang, Xuanjing Huang</strong></p>
<p>Process supervision, i.e., evaluating each step, is critical for complex large language model (LLM) reasoning and test-time searching with increased inference compute. Existing approaches, represented by process reward models (PRMs), primarily focus on rewarding signals up to the current step, exhibiting a one-directional nature and lacking a mechanism to model the distance to the final target. To address this problem, we draw inspiration from the A* algorithm, which states that an effective supervisory signal should simultaneously consider the incurred cost and the estimated cost for reaching the target. Building on this key insight, we introduce BiRM, a novel process supervision model that not only evaluates the correctness of previous steps but also models the probability of future success. We conduct extensive experiments on mathematical reasoning tasks and demonstrate that BiRM provides more precise evaluations of LLM reasoning steps, achieving an improvement of 3.1% on Gaokao2023 over PRM under the Best-of-N sampling method. Besides, in search-based strategies, BiRM provides more comprehensive guidance and outperforms ORM by 5.0% and PRM by 3.8% respectively on MATH-500. </p>
<blockquote>
<p>è¿‡ç¨‹ç›‘ç£ï¼Œå³è¯„ä¼°æ¯ä¸€æ­¥ï¼Œå¯¹äºå¤æ‚çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†å’Œæµ‹è¯•æ—¶çš„æœç´¢ä»¥åŠå¢åŠ çš„æ¨ç†è®¡ç®—æ¥è¯´è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•ä»¥è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ä¸ºä»£è¡¨ï¼Œä¸»è¦å…³æ³¨å½“å‰æ­¥éª¤çš„å¥–åŠ±ä¿¡å·ï¼Œå‘ˆç°å‡ºå•å‘æ€§ï¼Œç¼ºä¹æ¨¡æ‹Ÿæœ€ç»ˆç›®æ ‡ä¸å½“å‰çŠ¶æ€ä¹‹é—´è·ç¦»çš„æœºåˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä»A*ç®—æ³•ä¸­æ±²å–çµæ„Ÿï¼Œè¯¥ç®—æ³•æŒ‡å‡ºæœ‰æ•ˆçš„ç›‘ç£ä¿¡å·åº”åŒæ—¶è€ƒè™‘äº§ç”Ÿçš„æˆæœ¬å’Œåˆ°è¾¾ç›®æ ‡çš„é¢„ä¼°æˆæœ¬ã€‚åŸºäºè¿™ä¸€å…³é”®è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†BiRMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è¿‡ç¨‹ç›‘ç£æ¨¡å‹ï¼Œå®ƒä¸ä»…è¯„ä¼°å…ˆå‰æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œè¿˜æ¨¡æ‹Ÿæœªæ¥æˆåŠŸçš„æ¦‚ç‡ã€‚æˆ‘ä»¬åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯æ˜BiRMå¯¹LLMæ¨ç†æ­¥éª¤çš„è¯„ä»·æ›´ä¸ºç²¾ç¡®ï¼Œåœ¨æœ€ä½³Né‡‡æ ·æ–¹æ³•ä¸‹ï¼Œç›¸å¯¹äºPRMåœ¨Gaokao2023ä¸Šçš„æ”¹è¿›ç‡ä¸º3.1%ã€‚æ­¤å¤–ï¼Œåœ¨åŸºäºæœç´¢çš„ç­–ç•¥ä¸­ï¼ŒBiRMæä¾›æ›´å…¨é¢çš„æŒ‡å¯¼ï¼Œå¹¶åœ¨MATH-500ä¸Šç›¸å¯¹äºORMå’ŒPRMåˆ†åˆ«æé«˜äº†5.0%å’Œ3.8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04618v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºA*ç®—æ³•çš„ç†å¿µï¼Œæ–‡ç« æå‡ºäº†BiRMï¼Œä¸€ç§æ–°å‹çš„è¿‡ç¨‹ç›‘ç£æ¨¡å‹ã€‚è¯¥æ¨¡å‹ä¸ä»…èƒ½è¯„ä¼°å…ˆå‰æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œè¿˜èƒ½é¢„æµ‹æœªæ¥çš„æˆåŠŸæ¦‚ç‡ã€‚åœ¨å¤æ‚çš„æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼ŒBiRMæ¯”ä¼ ç»Ÿçš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨é«˜è€ƒæ•°å­¦å’ŒMATH-500çš„æœç´¢ç­–ç•¥æµ‹è¯•ä¸­ï¼ŒBiRMæ˜¾è‘—æå‡äº†å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ–‡ç« å¼ºè°ƒäº†è¿‡ç¨‹ç›‘ç£åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¢åŠ æ¨ç†è®¡ç®—çš„æƒ…å†µä¸‹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å½“å‰æ­¥éª¤çš„å¥–åŠ±ä¿¡å·ï¼Œç¼ºä¹å¯¹æœªæ¥ç›®æ ‡çš„å»ºæ¨¡æœºåˆ¶ã€‚</li>
<li>BiRMæ¨¡å‹ç»“åˆäº†A*ç®—æ³•çš„ç†å¿µï¼ŒåŒæ—¶è€ƒè™‘å·²äº§ç”Ÿçš„æˆæœ¬å’Œåˆ°è¾¾ç›®æ ‡çš„é¢„ä¼°æˆæœ¬æ¥æä¾›æœ‰æ•ˆçš„ç›‘ç£ä¿¡å·ã€‚</li>
<li>BiRMä¸ä»…è¯„ä¼°å…ˆå‰æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œè¿˜é¢„æµ‹æœªæ¥çš„æˆåŠŸæ¦‚ç‡ã€‚</li>
<li>åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼ŒBiRMæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„PRMæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯é‡‡ç”¨Best-of-Né‡‡æ ·æ–¹æ³•çš„æµ‹è¯•æ˜¾ç¤ºï¼Œåœ¨Gaokao2023ä¸ŠBiRMè¾ƒPRMæœ‰3.1%çš„æå‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04618">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-30b27ae87e01dfbca5ce817601f04067.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19cfaaf1814dbc5b05292775c379478e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1718239711f8c0cf50ebd7c12bfbb8d3.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="HalluCounter-Reference-free-LLM-Hallucination-Detection-in-the-Wild"><a href="#HalluCounter-Reference-free-LLM-Hallucination-Detection-in-the-Wild" class="headerlink" title="HalluCounter: Reference-free LLM Hallucination Detection in the Wild!"></a>HalluCounter: Reference-free LLM Hallucination Detection in the Wild!</h2><p><strong>Authors:Ashok Urlana, Gopichand Kanumolu, Charaka Vinayak Kumar, Bala Mallikarjunarao Garlapati, Rahul Mishra</strong></p>
<p>Response consistency-based, reference-free hallucination detection (RFHD) methods do not depend on internal model states, such as generation probabilities or gradients, which Grey-box models typically rely on but are inaccessible in closed-source LLMs. However, their inability to capture query-response alignment patterns often results in lower detection accuracy. Additionally, the lack of large-scale benchmark datasets spanning diverse domains remains a challenge, as most existing datasets are limited in size and scope. To this end, we propose HalluCounter, a novel reference-free hallucination detection method that utilizes both response-response and query-response consistency and alignment patterns. This enables the training of a classifier that detects hallucinations and provides a confidence score and an optimal response for user queries. Furthermore, we introduce HalluCounterEval, a benchmark dataset comprising both synthetically generated and human-curated samples across multiple domains. Our method outperforms state-of-the-art approaches by a significant margin, achieving over 90% average confidence in hallucination detection across datasets. </p>
<blockquote>
<p>åŸºäºå“åº”ä¸€è‡´æ€§çš„æ— å‚è€ƒå¹»è§‰æ£€æµ‹ï¼ˆRFHDï¼‰æ–¹æ³•ä¸ä¾èµ–äºå†…éƒ¨æ¨¡å‹çŠ¶æ€ï¼Œå¦‚ç”Ÿæˆæ¦‚ç‡æˆ–æ¢¯åº¦ï¼Œè€Œç°ç›’æ¨¡å‹é€šå¸¸ä¾èµ–äºè¿™äº›åœ¨å°é—­æºä»£ç çš„LLMä¸­ä¸å¯è®¿é—®çš„ä¿¡æ¯ã€‚ç„¶è€Œï¼Œå®ƒä»¬æ— æ³•æ•æ‰æŸ¥è¯¢-å“åº”å¯¹é½æ¨¡å¼ï¼Œè¿™å¾€å¾€å¯¼è‡´æ£€æµ‹å‡†ç¡®ç‡é™ä½ã€‚æ­¤å¤–ï¼Œç¼ºä¹æ¶µç›–å¤šä¸ªé¢†åŸŸçš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºå¤§å¤šæ•°ç°æœ‰æ•°æ®é›†åœ¨è§„æ¨¡å’ŒèŒƒå›´ä¸Šéƒ½æ˜¯æœ‰é™çš„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†HalluCounterï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ— å‚è€ƒå¹»è§‰æ£€æµ‹æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å“åº”-å“åº”å’ŒæŸ¥è¯¢-å“åº”çš„ä¸€è‡´æ€§å’Œå¯¹é½æ¨¡å¼ã€‚è¿™èƒ½å¤Ÿè®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨æ¥æ£€æµ‹å¹»è§‰ï¼Œå¹¶æä¾›ç”¨æˆ·æŸ¥è¯¢çš„ä¿¡å¿ƒè¯„åˆ†å’Œæœ€ä½³å“åº”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†HalluCounterEvalï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«å¤šä¸ªé¢†åŸŸçš„äººå·¥åˆæˆå’Œäººå·¥ç²¾é€‰æ ·æœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹»è§‰æ£€æµ‹å¹³å‡ç½®ä¿¡åº¦è¶…è¿‡99%ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04615v1">PDF</a> 30 pages, 4 figures</p>
<p><strong>Summary</strong><br>å¤§æ¨¡å‹å“åº”ä¸€è‡´æ€§æ— å‚è€ƒçš„å¹»è§‰æ£€æµ‹ï¼ˆRFHDï¼‰æ–¹æ³•ä¸ä¾èµ–äºå†…éƒ¨æ¨¡å‹çŠ¶æ€ï¼Œå¦‚ç”Ÿæˆæ¦‚ç‡æˆ–æ¢¯åº¦ç­‰ï¼Œä½†æ— æ³•æ•æ‰æŸ¥è¯¢å“åº”å¯¹é½æ¨¡å¼ï¼Œå¯¼è‡´æ£€æµ‹ç²¾åº¦è¾ƒä½ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºHalluCounteræ–¹æ³•ï¼Œåˆ©ç”¨å“åº”é—´ä¸€è‡´æ€§åŠæŸ¥è¯¢å“åº”å¯¹é½æ¨¡å¼è¿›è¡Œå¹»è§‰æ£€æµ‹ï¼Œå¹¶å¼•å…¥HalluCounterEvalåŸºå‡†æ•°æ®é›†ã€‚è¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹»è§‰æ£€æµ‹å¹³å‡ç½®ä¿¡åº¦è¶…è¿‡90%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>RFHDæ–¹æ³•ä¸ä¾èµ–å†…éƒ¨æ¨¡å‹çŠ¶æ€ï¼Œå¦‚ç”Ÿæˆæ¦‚ç‡æˆ–æ¢¯åº¦ã€‚</li>
<li>ç°æœ‰RFHDæ–¹æ³•å› æ— æ³•æ•æ‰æŸ¥è¯¢å“åº”å¯¹é½æ¨¡å¼è€Œå¯¼è‡´æ£€æµ‹ç²¾åº¦è¾ƒä½ã€‚</li>
<li>HalluCounteræ˜¯ä¸€ç§æ–°çš„RFHDæ–¹æ³•ï¼Œåˆ©ç”¨å“åº”é—´ä¸€è‡´æ€§åŠæŸ¥è¯¢å“åº”å¯¹é½æ¨¡å¼ã€‚</li>
<li>HalluCounterEvalåŸºå‡†æ•°æ®é›†åŒ…å«åˆæˆå’Œäººç±»ç²¾é€‰æ ·æœ¬ï¼Œè¦†ç›–å¤šä¸ªé¢†åŸŸã€‚</li>
<li>HalluCounteræ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>HalluCounteråœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹»è§‰æ£€æµ‹å¹³å‡ç½®ä¿¡åº¦è¶…è¿‡90%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d3cde69c878f090f1a1709fec005f0dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3e7864e39e04def0662bd5ebfb3ab414.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a747c051bff61dd8f9e8ff17bdf55190.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90e406a6701caf42861ff7a06e0c9c41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-932c6fc2ac71a59c9527efbddcfb6511.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f4ee130b802e8ba53e6273ef69a755b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-197c992ed3ff5054b792713a53d5a6a0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="HybridNorm-Towards-Stable-and-Efficient-Transformer-Training-via-Hybrid-Normalization"><a href="#HybridNorm-Towards-Stable-and-Efficient-Transformer-Training-via-Hybrid-Normalization" class="headerlink" title="HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid   Normalization"></a>HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid   Normalization</h2><p><strong>Authors:Zhijian Zhuo, Yutao Zeng, Ya Wang, Sijun Zhang, Jian Yang, Xiaoqing Li, Xun Zhou, Jinwen Ma</strong></p>
<p>Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, challenges remain in training deep transformer networks, especially regarding the location of layer normalization. While Pre-Norm structures facilitate easier training due to their more prominent identity path, they often yield suboptimal performance compared to Post-Norm. In this paper, we propose $\textbf{HybridNorm}$, a straightforward yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. This design not only stabilizes training but also enhances performance, particularly in the context of LLMs. Comprehensive experiments in both dense and sparse architectures show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches, achieving state-of-the-art results across various benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. %Code will be made publicly available. Code is available at <a target="_blank" rel="noopener" href="https://github.com/BryceZhuo/HybridNorm">https://github.com/BryceZhuo/HybridNorm</a>. </p>
<blockquote>
<p>Transformeræ¶æ„åœ¨ä¼—å¤šæœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­å·²æˆä¸ºé»˜è®¤é€‰æ‹©ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ã€‚å°½ç®¡å®ƒä»¬è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è®­ç»ƒæ·±åº¦Transformerç½‘ç»œæ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å±‚å½’ä¸€åŒ–çš„ä½ç½®é—®é¢˜ä¸Šã€‚ç”±äºPre-Normç»“æ„å…·æœ‰æ›´çªå‡ºçš„èº«ä»½è·¯å¾„ï¼Œä»è€Œä¾¿äºè®­ç»ƒï¼Œä½†å…¶æ€§èƒ½å¾€å¾€ä¸å¦‚Post-Normã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºHybridNormçš„æ··åˆå½’ä¸€åŒ–ç­–ç•¥ï¼Œè¿™æ˜¯ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ··åˆç­–ç•¥ï¼Œç»“åˆäº†Pre-Normå’ŒPost-Normæ–¹æ³•å„è‡ªçš„ä¼˜ç‚¹ã€‚å…·ä½“æ¥è¯´ï¼ŒHybridNormåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­é‡‡ç”¨QKVå½’ä¸€åŒ–ï¼Œå¹¶åœ¨æ¯ä¸ªTransformerå—çš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ä¸­ä½¿ç”¨Post-Normã€‚è¿™ç§è®¾è®¡ä¸ä»…ç¨³å®šäº†è®­ç»ƒè¿‡ç¨‹ï¼Œè¿˜æé«˜äº†æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒŒæ™¯ä¸‹ã€‚åœ¨å¯†é›†å’Œç¨€ç–æ¶æ„ä¸­çš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒHybridNormå§‹ç»ˆä¼˜äºPre-Normå’ŒPost-Normæ–¹æ³•ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æˆæœã€‚è¿™äº›å‘ç°çªæ˜¾äº†HybridNormä½œä¸ºæ›´ç¨³å®šã€æ›´æœ‰æ•ˆçš„æŠ€æœ¯ï¼Œæœ‰æ½œåŠ›æ”¹è¿›æ·±åº¦Transformeræ¨¡å‹çš„è®­ç»ƒæ€§èƒ½ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/BryceZhuo/HybridNorm">https://github.com/BryceZhuo/HybridNorm</a>ä¸Šå…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04598v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºHybridNormçš„æ··åˆå½’ä¸€åŒ–ç­–ç•¥ï¼Œç»“åˆäº†Pre-Normå’ŒPost-Normçš„ä¼˜åŠ¿ã€‚è¯¥ç­–ç•¥åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­é‡‡ç”¨QKVå½’ä¸€åŒ–ï¼Œå¹¶åœ¨æ¯ä¸ªtransformerå—çš„feed-forwardç½‘ç»œï¼ˆFFNï¼‰ä¸­ä½¿ç”¨Post-Normã€‚è¿™ç§è®¾è®¡ä¸ä»…ç¨³å®šè®­ç»ƒï¼Œè¿˜æé«˜äº†æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ã€‚å®éªŒè¡¨æ˜ï¼ŒHybridNormåœ¨å¯†é›†å’Œç¨€ç–æ¶æ„ä¸­éƒ½è¡¨ç°ä¼˜ç§€ï¼Œç›¸è¾ƒäºPre-Normå’ŒPost-Normæœ‰æ›´å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformerså·²æˆä¸ºå¤šç§æœºå™¨å­¦ä¹ ä»»åŠ¡çš„é»˜è®¤æ¶æ„ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ã€‚</li>
<li>å°½ç®¡Transformerè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨è®­ç»ƒæ·±åº¦ç½‘ç»œæ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå…³äºå±‚å½’ä¸€åŒ–çš„ä½ç½®æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚</li>
<li>Pre-Normç»“æ„å› æ›´çªå‡ºçš„èº«ä»½è·¯å¾„è€Œæ˜“äºè®­ç»ƒï¼Œä½†æ€§èƒ½å¾€å¾€ä¸å¦‚Post-Normã€‚</li>
<li>æœ¬æ–‡æå‡ºHybridNormï¼Œä¸€ä¸ªç»“åˆPre-Normå’ŒPost-Normä¼˜åŠ¿çš„æ··åˆå½’ä¸€åŒ–ç­–ç•¥ã€‚</li>
<li>HybridNormåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­é‡‡ç”¨QKVå½’ä¸€åŒ–ï¼Œå¹¶åœ¨FFNä¸­ä½¿ç”¨Post-Normã€‚</li>
<li>HybridNormä¸ä»…ç¨³å®šè®­ç»ƒï¼Œè¿˜æé«˜æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨LLMsä¸­ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒHybridNormåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ï¼Œå…·æœ‰æ›´ç¨³å®šå’Œæœ‰æ•ˆçš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04598">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-662e63eba070bcd8ae199df61eca965a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38768c5a780053b49471795c3bc0d1bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-265decdf333aa45e77cfaae4f052a9f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ac5a213858b29cdb5ec390d5578e1e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6432e672fb670e7d62dc6f17abe9430.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="RetinalGPT-A-Retinal-Clinical-Preference-Conversational-Assistant-Powered-by-Large-Vision-Language-Models"><a href="#RetinalGPT-A-Retinal-Clinical-Preference-Conversational-Assistant-Powered-by-Large-Vision-Language-Models" class="headerlink" title="RetinalGPT: A Retinal Clinical Preference Conversational Assistant   Powered by Large Vision-Language Models"></a>RetinalGPT: A Retinal Clinical Preference Conversational Assistant   Powered by Large Vision-Language Models</h2><p><strong>Authors:Wenhui Zhu, Xin Li, Xiwen Chen, Peijie Qiu, Vamsi Krishna Vasa, Xuanzhao Dong, Yanxi Chen, Natasha Lepore, Oana Dumitrascu, Yi Su, Yalin Wang</strong></p>
<p>Recently, Multimodal Large Language Models (MLLMs) have gained significant attention for their remarkable ability to process and analyze non-textual data, such as images, videos, and audio. Notably, several adaptations of general-domain MLLMs to the medical field have been explored, including LLaVA-Med. However, these medical adaptations remain insufficiently advanced in understanding and interpreting retinal images. In contrast, medical experts emphasize the importance of quantitative analyses for disease detection and interpretation. This underscores a gap between general-domain and medical-domain MLLMs: while general-domain MLLMs excel in broad applications, they lack the specialized knowledge necessary for precise diagnostic and interpretative tasks in the medical field. To address these challenges, we introduce \textit{RetinalGPT}, a multimodal conversational assistant for clinically preferred quantitative analysis of retinal images. Specifically, we achieve this by compiling a large retinal image dataset, developing a novel data pipeline, and employing customized visual instruction tuning to enhance both retinal analysis and enrich medical knowledge. In particular, RetinalGPT outperforms MLLM in the generic domain by a large margin in the diagnosis of retinal diseases in 8 benchmark retinal datasets. Beyond disease diagnosis, RetinalGPT features quantitative analyses and lesion localization, representing a pioneering step in leveraging LLMs for an interpretable and end-to-end clinical research framework. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Retinal-Research/RetinalGPT">https://github.com/Retinal-Research/RetinalGPT</a> </p>
<blockquote>
<p>è¿‘æœŸï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å› å…¶å¤„ç†å’Œåˆ†æéæ–‡æœ¬æ•°æ®ï¼ˆå¦‚å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ï¼‰çš„æ˜¾è‘—èƒ½åŠ›è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå·²ç»æ¢ç´¢äº†å°†é€šç”¨é¢†åŸŸçš„MLLMsé€‚åº”äºåŒ»ç–—é¢†åŸŸçš„å‡ ä¸ªé€‚åº”ç‰ˆæœ¬ï¼ŒåŒ…æ‹¬LLaVA-Medã€‚ç„¶è€Œï¼Œè¿™äº›åŒ»ç–—é€‚åº”ç‰ˆæœ¬åœ¨ç†è§£å’Œè§£é‡Šè§†ç½‘è†œå›¾åƒæ–¹é¢ä»ç„¶å­˜åœ¨ä¸è¶³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŒ»å­¦ä¸“å®¶å¼ºè°ƒå®šé‡åˆ†æåœ¨ç–¾ç—…æ£€æµ‹å’Œè§£é‡Šä¸­çš„é‡è¦æ€§ã€‚è¿™çªå‡ºäº†é€šç”¨é¢†åŸŸå’ŒåŒ»ç–—é¢†åŸŸMLLMsä¹‹é—´çš„é¸¿æ²Ÿï¼šè™½ç„¶é€šç”¨é¢†åŸŸçš„MLLMsåœ¨å¹¿æ³›åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ç¼ºä¹åŒ»ç–—é¢†åŸŸä¸­ç²¾ç¡®è¯Šæ–­å’Œè§£é‡Šä»»åŠ¡æ‰€éœ€çš„ä¸“ä¸šçŸ¥è¯†ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RetinalGPTï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¯¹è¯åŠ©æ‰‹ï¼Œç”¨äºä¸´åºŠä¸Šé¦–é€‰çš„è§†ç½‘è†œå›¾åƒå®šé‡åˆ†æã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡ç¼–è¯‘å¤§è§„æ¨¡çš„è§†ç½‘è†œå›¾åƒæ•°æ®é›†ã€å¼€å‘æ–°å‹æ•°æ®ç®¡é“å’Œé‡‡ç”¨å®šåˆ¶çš„è§†è§‰æŒ‡ä»¤è°ƒæ•´æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œä»¥å¢å¼ºè§†ç½‘è†œåˆ†æå’Œä¸°å¯ŒåŒ»å­¦çŸ¥è¯†ã€‚ç‰¹åˆ«æ˜¯åœ¨8ä¸ªåŸºå‡†è§†ç½‘è†œæ•°æ®é›†ä¸­ï¼ŒRetinalGPTåœ¨è§†ç½‘è†œç–¾ç—…çš„è¯Šæ–­æ–¹é¢å¤§å¤§è¶…è¿‡äº†é€šç”¨é¢†åŸŸçš„MLLMã€‚é™¤äº†ç–¾ç—…è¯Šæ–­å¤–ï¼ŒRetinalGPTè¿˜å…·å¤‡å®šé‡åˆ†æå’Œç—…ç¶å®šä½åŠŸèƒ½ï¼Œä»£è¡¨äº†åœ¨åˆ©ç”¨LLMsè¿›è¡Œå¯è§£é‡Šå’Œç«¯åˆ°ç«¯çš„ä¸´åºŠç ”ç©¶æ¡†æ¶æ–¹é¢çš„å¼€åˆ›æ€§æ­¥éª¤ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/Retinal-Research/RetinalGPT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Retinal-Research/RetinalGPTè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03987v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è§†ç½‘è†œå›¾åƒè¿›è¡Œå®šé‡åˆ†æçš„å¤šåª’ä½“å¯¹è¯åŠ©æ‰‹RetinalGPTã€‚é€šè¿‡æ„å»ºå¤§å‹è§†ç½‘è†œå›¾åƒæ•°æ®é›†ã€å¼€å‘æ–°å‹æ•°æ®æµç¨‹ç®¡é“å’Œå®šåˆ¶è§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼ŒRetinalGPTæé«˜äº†è§†ç½‘è†œåˆ†æçš„èƒ½åŠ›å¹¶ä¸°å¯Œäº†åŒ»å­¦çŸ¥è¯†ã€‚ç›¸è¾ƒäºé€šç”¨é¢†åŸŸçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒRetinalGPTåœ¨è§†ç½‘è†œç–¾ç—…çš„è¯Šæ–­ä¸Šå¤§å¹…è¶…è¶Šï¼Œä¸ä»…é™äºç–¾ç—…è¯Šæ–­ï¼Œè¿˜å…·å¤‡å®šé‡åˆ†æå’Œç—…ç¶å®šä½åŠŸèƒ½ï¼Œä¸ºä¸´åºŠç ”ç©¶å’Œè¯Šæ–­æä¾›äº†ä¸€ä¸ªå¯è§£é‡Šã€ç«¯åˆ°ç«¯çš„æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½å¤„ç†éæ–‡æœ¬æ•°æ®ï¼Œå¦‚å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ã€‚</li>
<li>MLLMsåœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ä»é¢ä¸´ç†è§£å’Œè§£é‡Šè§†ç½‘è†œå›¾åƒæ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>åŒ»å­¦ä¸“å®¶å¼ºè°ƒå®šé‡åˆ†ææ³•åœ¨ç–¾ç—…æ£€æµ‹å’Œè§£è¯»ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>RetinalGPTæ˜¯ä¸€ä¸ªå¤šåª’ä½“å¯¹è¯åŠ©æ‰‹ï¼Œä¸“ä¸ºä¸´åºŠæ‰€éœ€çš„è§†ç½‘è†œå›¾åƒå®šé‡åˆ†æè®¾è®¡ã€‚</li>
<li>RetinalGPTé€šè¿‡æ„å»ºå¤§å‹è§†ç½‘è†œå›¾åƒæ•°æ®é›†ã€å¼€å‘æ•°æ®æµç¨‹ç®¡é“å’Œå®šåˆ¶è§†è§‰æŒ‡ä»¤è°ƒæ•´æ¥å¢å¼ºæ€§èƒ½ã€‚</li>
<li>RetinalGPTåœ¨è§†ç½‘è†œç–¾ç—…è¯Šæ–­ä¸Šå¤§å¹…è¶…è¶Šé€šç”¨é¢†åŸŸçš„MLLMsã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03987">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-feaa19088fdb1259fe63852144c9eac6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-085d87585a318a307c4f68800a95faec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42d2a468956c315689d3dab658a5884b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RAAD-LLM-Adaptive-Anomaly-Detection-Using-LLMs-and-RAG-Integration"><a href="#RAAD-LLM-Adaptive-Anomaly-Detection-Using-LLMs-and-RAG-Integration" class="headerlink" title="RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration"></a>RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration</h2><p><strong>Authors:Alicia Russell-Gilbert, Sudip Mittal, Shahram Rahimi, Maria Seale, Joseph Jabour, Thomas Arnold, Joshua Church</strong></p>
<p>Anomaly detection in complex industrial environments poses unique challenges, particularly in contexts characterized by data sparsity and evolving operational conditions. Predictive maintenance (PdM) in such settings demands methodologies that are adaptive, transferable, and capable of integrating domain-specific knowledge. In this paper, we present RAAD-LLM, a novel framework for adaptive anomaly detection, leveraging large language models (LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach addresses the aforementioned PdM challenges. By effectively utilizing domain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time series data without requiring fine-tuning on specific datasets. The frameworkâ€™s adaptability mechanism enables it to adjust its understanding of normal operating conditions dynamically, thus increasing detection accuracy. We validate this methodology through a real-world application for a plastics manufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show significant improvements over our previous model with an accuracy increase from 70.7% to 89.1% on the real-world dataset. By allowing for the enriching of input series data with semantics, RAAD-LLM incorporates multimodal capabilities that facilitate more collaborative decision-making between the model and plant operators. Overall, our findings support RAAD-LLMâ€™s ability to revolutionize anomaly detection methodologies in PdM, potentially leading to a paradigm shift in how anomaly detection is implemented across various industries. </p>
<blockquote>
<p>åœ¨å¤æ‚çš„å·¥ä¸šç¯å¢ƒä¸­è¿›è¡Œå¼‚å¸¸æ£€æµ‹é¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç–å’Œè¿è¥æ¡ä»¶ä¸æ–­å˜åŒ–çš„æƒ…å†µä¸‹ã€‚æ­¤ç±»ç¯å¢ƒä¸­çš„é¢„æµ‹æ€§ç»´æŠ¤ï¼ˆPdMï¼‰éœ€è¦å…·æœ‰é€‚åº”æ€§ã€å¯ç§»æ¤æ€§å¹¶èƒ½å¤Ÿæ•´åˆç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„æ–¹æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†RAAD-LLMï¼Œè¿™æ˜¯ä¸€ç§è‡ªé€‚åº”å¼‚å¸¸æ£€æµ‹çš„æ–°æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç›¸ç»“åˆã€‚è¯¥æ–¹æ³•è§£å†³äº†ä¸Šè¿°PdMæŒ‘æˆ˜ã€‚é€šè¿‡æœ‰æ•ˆåˆ©ç”¨ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ï¼ŒRAAD-LLMå¢å¼ºäº†æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„å¼‚å¸¸æ£€æµ‹ï¼Œè€Œæ— éœ€åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒã€‚è¯¥æ¡†æ¶çš„é€‚åº”æ€§æœºåˆ¶ä½¿å…¶èƒ½å¤ŸåŠ¨æ€åœ°è°ƒæ•´å¯¹æ­£å¸¸æ“ä½œæ¡ä»¶çš„ç†è§£ï¼Œä»è€Œæé«˜æ£€æµ‹å‡†ç¡®æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¡‘æ–™åˆ¶é€ å‚çš„å®é™…åº”ç”¨å’ŒSkoltechå¼‚å¸¸åŸºå‡†æµ‹è¯•ï¼ˆSKABï¼‰éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸ä¹‹å‰çš„æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨ç°å®æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ä»70.7%æé«˜åˆ°äº†89.1%ã€‚é€šè¿‡å…è®¸ç”¨è¯­ä¹‰ä¿¡æ¯ä¸°å¯Œè¾“å…¥åºåˆ—æ•°æ®ï¼ŒRAAD-LLMç»“åˆäº†å¤šæ¨¡å¼åŠŸèƒ½ï¼Œä¿ƒè¿›äº†æ¨¡å‹å’Œå·¥å‚æ“ä½œå‘˜ä¹‹é—´çš„æ›´åä½œå†³ç­–ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ”¯æŒRAAD-LLMåœ¨PdMä¸­å¼‚å¸¸æ£€æµ‹æ–¹æ³•çš„é©å‘½æ€§å˜é©ï¼Œæœ‰å¯èƒ½å¯¼è‡´å„è¡Œä¸šå¼‚å¸¸æ£€æµ‹å®æ–½æ–¹å¼çš„èŒƒå¼è½¬å˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02800v2">PDF</a> arXiv admin note: substantial text overlap with arXiv:2411.00914</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„RAADæ¡†æ¶èƒ½æœ‰æ•ˆåº”å¯¹å¤æ‚å·¥ä¸šç¯å¢ƒä¸­çš„å¼‚å¸¸æ£€æµ‹æŒ‘æˆ˜ã€‚ç‰¹åˆ«æ˜¯é¢ä¸´æ•°æ®ç¨€ç–å’Œå¤šå˜æ“ä½œæ¡ä»¶æ—¶ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é¢†åŸŸç‰¹å®šçŸ¥è¯†æå‡äº†æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„å¼‚å¸¸æ£€æµ‹æ€§èƒ½ã€‚è¯¥æ¡†æ¶çš„åŠ¨æ€é€‚åº”æœºåˆ¶èƒ½å¤Ÿå®æ—¶è°ƒæ•´æ­£å¸¸æ“ä½œæ¡ä»¶çš„ç†è§£ï¼Œä»è€Œæé«˜æ£€æµ‹å‡†ç¡®æ€§ã€‚å…¶åœ¨å¡‘æ–™åˆ¶é€ å·¥å‚å’ŒSkoltechå¼‚å¸¸åŸºå‡†æµ‹è¯•ï¼ˆSKABï¼‰ä¸­çš„å®é™…åº”ç”¨éªŒè¯è¡¨æ˜äº†æ˜¾è‘—çš„æ•ˆæœæå‡ã€‚RAADæ¡†æ¶ç»“åˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œå¢å¼ºäº†è¾“å…¥åºåˆ—æ•°æ®çš„è¯­ä¹‰å†…æ¶µï¼Œå¼•å…¥äº†å¤šæ¨¡å¼èƒ½åŠ›ï¼Œä¸ºæ¨¡å‹å’Œå·¥å‚æ“ä½œè€…ä¹‹é—´æä¾›äº†æ›´åä½œçš„å†³ç­–æ”¯æŒã€‚æ•´ä½“æ¥çœ‹ï¼Œè¯¥ç ”ç©¶è¡¨æ˜RAADæ¡†æ¶èƒ½å¤Ÿå¼•é¢†å·¥ä¸šé¢†åŸŸå¼‚å¸¸æ£€æµ‹æ–¹æ³•çš„é©å‘½æ€§å˜é©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>RAAD-LLMæ¡†æ¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œç”¨äºè‡ªé€‚åº”å¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½æœ‰æ•ˆåº”å¯¹æ•°æ®ç¨€ç–å’Œå¤šå˜æ“ä½œæ¡ä»¶ä¸‹çš„å¼‚å¸¸æ£€æµ‹æŒ‘æˆ˜ã€‚</li>
<li>RAAD-LLMé€šè¿‡åˆ©ç”¨é¢†åŸŸç‰¹å®šçŸ¥è¯†æå‡äº†æ—¶é—´åºåˆ—æ•°æ®ä¸­çš„å¼‚å¸¸æ£€æµ‹æ€§èƒ½ã€‚</li>
<li>åŠ¨æ€é€‚åº”æœºåˆ¶èƒ½å®æ—¶è°ƒæ•´æ­£å¸¸æ“ä½œæ¡ä»¶çš„ç†è§£ï¼Œæé«˜æ£€æµ‹å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨å¡‘æ–™åˆ¶é€ å·¥å‚å’ŒSkoltechå¼‚å¸¸åŸºå‡†æµ‹è¯•ä¸­çš„å®é™…åº”ç”¨éªŒè¯è¡¨æ˜äº†æ˜¾è‘—çš„æ•ˆæœæå‡ã€‚</li>
<li>ä¸å…ˆå‰çš„æ¨¡å‹ç›¸æ¯”ï¼ŒRAAD-LLMçš„å‡†ç¡®æ€§ä»70.7%æé«˜åˆ°äº†89.1%ã€‚</li>
<li>è¯¥æ¡†æ¶å¼•å…¥äº†å¤šæ¨¡å¼èƒ½åŠ›ï¼Œä¸ºæ¨¡å‹å’Œå·¥å‚æ“ä½œè€…ä¹‹é—´æä¾›äº†æ›´åä½œçš„å†³ç­–æ”¯æŒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-25c35bdd9189e45412ce35818f0bab90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a10eba53167cf88dc695912d62a98e3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8df499ffca869658fc8861b56ff84458.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Union-of-Experts-Adapting-Hierarchical-Routing-to-Equivalently-Decomposed-Transformer"><a href="#Union-of-Experts-Adapting-Hierarchical-Routing-to-Equivalently-Decomposed-Transformer" class="headerlink" title="Union of Experts: Adapting Hierarchical Routing to Equivalently   Decomposed Transformer"></a>Union of Experts: Adapting Hierarchical Routing to Equivalently   Decomposed Transformer</h2><p><strong>Authors:Yujiao Yang, Jing Lian, Linhui Li</strong></p>
<p>We propose Union-of-Experts (UoE), which decomposes transformer into an equitant group of experts, and then implement selective routing on input data and experts. Our approach advances MoE design with four key innovations: (1) We conducted equitant expert decomposition on both MLP blocks and attention blocks based on matrix partition in tensor parallelism. (2) We developed two routing paradigms: patch-wise data selection and expert selection, to apply routing across different levels. (3) We design the architecture of UoE model, including Selective Multi-Head Attention (SMHA) and Union-of-MLP-Experts (UoME). (4) We develop parallel implementation of UoEâ€™s routing and computation operation, and optimize efficiency based on the hardware processing analysis. The experiments demonstrate that the UoE model surpass Full Attention, state-of-art MoEs and efficient transformers (including the model architecture of recently proposed DeepSeek-V3) in several tasks across image and natural language domains. In language modeling tasks, we achieve an average reduction of 2.38 in perplexity compared to the best-performed MoE method with an average of 76% FLOPs. In Long Range Arena benchmark, we recorded an average score that is at least 0.68% higher than all comparison models including Full Attention, MoEs, and transformer variants, with only 50% FLOPs of the best MoE method. In image classification, our model yielded an average accuracy improvement of 1.75% than the best model while maintaining comparable FLOPs. The source codes are available at <a target="_blank" rel="noopener" href="https://github.com/YujiaoYang-work/UoE">https://github.com/YujiaoYang-work/UoE</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸“å®¶è”ç›Ÿï¼ˆUoEï¼‰çš„æ–¹æ³•ï¼Œå®ƒå°†å˜å‹å™¨åˆ†è§£ä¸ºç­‰ä»·çš„ä¸“å®¶ç»„ï¼Œç„¶ååœ¨è¾“å…¥æ•°æ®å’Œä¸“å®¶ä¹‹é—´å®ç°é€‰æ‹©æ€§è·¯ç”±ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»¥å››é¡¹å…³é”®åˆ›æ–°æ¨åŠ¨äº†MoEè®¾è®¡çš„å‘å±•ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬åœ¨MLPå—å’Œæ³¨æ„åŠ›å—ä¸Šè¿›è¡Œäº†ç­‰ä»·çš„ä¸“å®¶åˆ†è§£ï¼Œè¿™æ˜¯åŸºäºå¼ é‡å¹¶è¡Œæ€§çš„çŸ©é˜µåˆ†åŒºã€‚ï¼ˆ2ï¼‰æˆ‘ä»¬å¼€å‘äº†ä¸¤ç§è·¯ç”±èŒƒå¼ï¼šåŸºäºè¡¥ä¸çš„æ•°æ®é€‰æ‹©å’Œä¸“å®¶é€‰æ‹©ï¼Œä»¥åº”ç”¨äºä¸åŒçš„å±‚æ¬¡ã€‚ï¼ˆ3ï¼‰æˆ‘ä»¬è®¾è®¡äº†UoEæ¨¡å‹çš„æ¶æ„ï¼ŒåŒ…æ‹¬é€‰æ‹©æ€§å¤šå¤´æ³¨æ„åŠ›ï¼ˆSMHAï¼‰å’Œè”ç›ŸMLPä¸“å®¶ï¼ˆUoMEï¼‰ã€‚ï¼ˆ4ï¼‰æˆ‘ä»¬å¹¶è¡Œå®ç°äº†UoEçš„è·¯ç”±å’Œè®¡ç®—æ“ä½œï¼Œå¹¶åŸºäºç¡¬ä»¶å¤„ç†åˆ†æè¿›è¡Œäº†æ•ˆç‡ä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒUoEæ¨¡å‹åœ¨å›¾åƒå’Œè‡ªç„¶è¯­è¨€é¢†åŸŸçš„å¤šä¸ªä»»åŠ¡ä¸­è¶…è¶Šäº†å…¨æ³¨æ„åŠ›ã€å…ˆè¿›çš„MoEå’Œé«˜æ•ˆå˜å‹å™¨ï¼ˆåŒ…æ‹¬æœ€è¿‘æå‡ºçš„DeepSeek-V3çš„æ¨¡å‹æ¶æ„ï¼‰ã€‚åœ¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­ï¼Œä¸æ€§èƒ½æœ€ä½³çš„MoEæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å°†å›°æƒ‘åº¦å¹³å‡é™ä½äº†2.38ï¼Œå¹³å‡æµ®ç‚¹è¿ç®—å‡å°‘äº†76%ã€‚åœ¨Long Range ArenaåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¹³å‡å¾—åˆ†è‡³å°‘é«˜äºæ‰€æœ‰å¯¹æ¯”æ¨¡å‹ï¼ˆåŒ…æ‹¬å…¨æ³¨æ„åŠ›ã€MoEå’Œå˜å‹å™¨å˜ä½“ï¼‰ï¼Œä¸”åªä½¿ç”¨äº†æœ€ä½³MoEæ–¹æ³•ä¸€åŠçš„æµ®ç‚¹è¿ç®—ã€‚åœ¨å›¾åƒåˆ†ç±»æ–¹é¢ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¿æŒç›¸å½“æµ®ç‚¹è¿ç®—é‡çš„æƒ…å†µä¸‹ï¼Œå¹³å‡å‡†ç¡®ç‡æ¯”æœ€ä½³æ¨¡å‹æé«˜äº†1.75%ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/YujiaoYang-work/UoE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YujiaoYang-work/UoEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02495v2">PDF</a> 17 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Union-of-Expertsï¼ˆUoEï¼‰æ¨¡å‹ï¼Œå®ƒå°†Transformeråˆ†è§£ä¸ºç­‰æ•ˆçš„ä¸“å®¶ç»„ï¼Œå¹¶å¯¹è¾“å…¥æ•°æ®å’Œä¸“å®¶è¿›è¡Œé€‰æ‹©æ€§è·¯ç”±ã€‚è¯¥æ¨¡å‹æœ‰å››ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šä¸€æ˜¯åŸºäºå¼ é‡å¹¶è¡Œæ€§çš„çŸ©é˜µåˆ†åŒºï¼Œå¯¹MLPå—å’Œæ³¨æ„åŠ›å—è¿›è¡Œç­‰æ•ˆä¸“å®¶åˆ†è§£ï¼›äºŒæ˜¯å¼€å‘ä¸¤ç§è·¯ç”±èŒƒå¼ï¼šè·¨ä¸åŒå±‚æ¬¡çš„æ–‘å—å¼æ•°æ®é€‰æ‹©å’Œä¸“å®¶é€‰æ‹©ï¼›ä¸‰æ˜¯è®¾è®¡UoEæ¨¡å‹æ¶æ„ï¼ŒåŒ…æ‹¬é€‰æ‹©æ€§å¤šå¤´æ³¨æ„åŠ›å’Œè”åˆçš„MLPä¸“å®¶ï¼›å››æ˜¯å®ç°UoEçš„è·¯ç”±å’Œè®¡ç®—æ“ä½œçš„å¹¶è¡Œå®ç°ï¼Œå¹¶åŸºäºç¡¬ä»¶å¤„ç†åˆ†æä¼˜åŒ–æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼ŒUoEæ¨¡å‹åœ¨å›¾åƒå’Œè‡ªç„¶è¯­è¨€é¢†åŸŸçš„å¤šä¸ªä»»åŠ¡ä¸­è¶…è¶Šäº†å…¨æ³¨æ„åŠ›ã€çŠ¶æ€æœ€ä¼˜çš„MoEå’Œé«˜æ•ˆçš„Transformerï¼ˆåŒ…æ‹¬æœ€è¿‘æå‡ºçš„DeepSeek-V3æ¨¡å‹æ¶æ„ï¼‰ã€‚åœ¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡ä¸­ï¼Œä¸è¡¨ç°æœ€ä½³çš„MoEæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å°†å›°æƒ‘åº¦å¹³å‡é™ä½äº†2.38ã€‚åœ¨Long Range ArenaåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¹³å‡å¾—åˆ†è‡³å°‘é«˜å‡ºæ‰€æœ‰å¯¹æ¯”æ¨¡å‹ï¼ˆåŒ…æ‹¬å…¨æ³¨æ„åŠ›ã€MoEå’ŒTransformerå˜ä½“ï¼‰ï¼Œä¸”ä»…ä½¿ç”¨æœ€ä½³MoEæ–¹æ³•çš„50%çš„æµ®ç‚¹è¿ç®—é‡ã€‚åœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¹³å‡ç²¾åº¦æ¯”æœ€ä½³æ¨¡å‹æé«˜äº†1.75%ï¼ŒåŒæ—¶ä¿æŒç›¸å½“çš„æµ®ç‚¹è¿ç®—é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UoEæ¨¡å‹å°†Transformeråˆ†è§£ä¸ºç­‰æ•ˆä¸“å®¶ç»„ï¼Œå®ç°é€‰æ‹©æ€§è·¯ç”±ã€‚</li>
<li>UoEæ¨¡å‹åœ¨MLPå—å’Œæ³¨æ„åŠ›å—ä¸Šè¿›è¡Œäº†åŸºäºçŸ©é˜µåˆ†åŒºçš„ä¸“å®¶åˆ†è§£ã€‚</li>
<li>å¼€å‘äº†ä¸¤ç§è·¯ç”±èŒƒå¼ï¼šæ–‘å—å¼æ•°æ®é€‰æ‹©å’Œä¸“å®¶é€‰æ‹©ï¼Œåº”ç”¨äºä¸åŒå±‚çº§ã€‚</li>
<li>UoEæ¨¡å‹æ¶æ„è®¾è®¡åŒ…æ‹¬é€‰æ‹©æ€§å¤šå¤´æ³¨æ„åŠ›å’Œè”åˆçš„MLPä¸“å®¶ã€‚</li>
<li>UoEæ¨¡å‹å®ç°äº†è·¯ç”±å’Œè®¡ç®—æ“ä½œçš„å¹¶è¡Œå¤„ç†ï¼Œå¹¶ä¼˜åŒ–äº†ç¡¬ä»¶å¤„ç†æ•ˆç‡ã€‚</li>
<li>åœ¨è¯­è¨€å»ºæ¨¡ã€Long Range ArenaåŸºå‡†æµ‹è¯•å’Œå›¾åƒåˆ†ç±»ç­‰ä»»åŠ¡ä¸­ï¼ŒUoEæ¨¡å‹è¡¨ç°è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
<li>UoEæ¨¡å‹çš„æºä»£ç å·²å…¬å¼€å‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02495">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-19cf89005b183c08b0106d0ad6d1bf2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19bba9e71a1326b4bda7584d14faac17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8be494f32600280076484d34d638802.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11699562dda5bc611776b8c0548bc2be.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Protein-Large-Language-Models-A-Comprehensive-Survey"><a href="#Protein-Large-Language-Models-A-Comprehensive-Survey" class="headerlink" title="Protein Large Language Models: A Comprehensive Survey"></a>Protein Large Language Models: A Comprehensive Survey</h2><p><strong>Authors:Yijia Xiao, Wanjia Zhao, Junkai Zhang, Yiqiao Jin, Han Zhang, Zhicheng Ren, Renliang Sun, Haixin Wang, Guancheng Wan, Pan Lu, Xiao Luo, Yu Zhang, James Zou, Yizhou Sun, Wei Wang</strong></p>
<p>Protein-specific large language models (Protein LLMs) are revolutionizing protein science by enabling more efficient protein structure prediction, function annotation, and design. While existing surveys focus on specific aspects or applications, this work provides the first comprehensive overview of Protein LLMs, covering their architectures, training datasets, evaluation metrics, and diverse applications. Through a systematic analysis of over 100 articles, we propose a structured taxonomy of state-of-the-art Protein LLMs, analyze how they leverage large-scale protein sequence data for improved accuracy, and explore their potential in advancing protein engineering and biomedical research. Additionally, we discuss key challenges and future directions, positioning Protein LLMs as essential tools for scientific discovery in protein science. Resources are maintained at <a target="_blank" rel="noopener" href="https://github.com/Yijia-Xiao/Protein-LLM-Survey">https://github.com/Yijia-Xiao/Protein-LLM-Survey</a>. </p>
<blockquote>
<p>è›‹ç™½è´¨ç‰¹å¼‚æ€§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆProtein LLMsï¼‰é€šè¿‡å®ç°æ›´é«˜æ•ˆçš„è›‹ç™½è´¨ç»“æ„é¢„æµ‹ã€åŠŸèƒ½æ³¨é‡Šå’Œè®¾è®¡ï¼Œæ­£åœ¨æ¨åŠ¨è›‹ç™½è´¨ç§‘å­¦çš„é©å‘½ã€‚è™½ç„¶ç°æœ‰çš„è°ƒæŸ¥ä¾§é‡äºç‰¹å®šæ–¹é¢æˆ–åº”ç”¨ï¼Œä½†è¿™é¡¹å·¥ä½œæä¾›äº†å¯¹Protein LLMsçš„é¦–ä¸ªå…¨é¢æ¦‚è¿°ï¼Œæ¶µç›–äº†å…¶æ¶æ„ã€è®­ç»ƒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡å’Œå¤šæ ·åŒ–çš„åº”ç”¨ã€‚é€šè¿‡å¯¹è¶…è¿‡100ç¯‡æ–‡ç« çš„ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†æœ€æ–°Protein LLMsçš„ç»“æ„åˆ†ç±»æ³•ï¼Œåˆ†æäº†å®ƒä»¬å¦‚ä½•åˆ©ç”¨å¤§è§„æ¨¡è›‹ç™½è´¨åºåˆ—æ•°æ®æé«˜å‡†ç¡®æ€§ï¼Œå¹¶æ¢ç´¢äº†å®ƒä»¬åœ¨æ¨åŠ¨è›‹ç™½è´¨å·¥ç¨‹å’Œç”Ÿç‰©åŒ»å­¦ç ”ç©¶æ–¹é¢çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†å…³é”®æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ–¹å‘ï¼Œå°†Protein LLMså®šä½ä¸ºè›‹ç™½è´¨ç§‘å­¦ç ”ç©¶ä¸­ä¸å¯æˆ–ç¼ºçš„å‘ç°å·¥å…·ã€‚ç›¸å…³èµ„æºå¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/Yijia-Xiao/Protein-LLM-Survey%E3%80%82">https://github.com/Yijia-Xiao/Protein-LLM-Surveyã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17504v2">PDF</a> 24 pages, 4 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>è›‹ç™½è´¨ç‰¹å®šçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆProtein LLMsï¼‰æ­£åœ¨é€šè¿‡æ›´é«˜æ•ˆçš„è›‹ç™½è´¨ç»“æ„é¢„æµ‹ã€åŠŸèƒ½æ³¨é‡Šå’Œè®¾è®¡ï¼Œæ¨åŠ¨è›‹ç™½è´¨ç§‘å­¦çš„é©å‘½ã€‚æœ¬æ–‡é¦–æ¬¡å…¨é¢æ¦‚è¿°äº†Protein LLMsï¼Œè¯¦ç»†ä»‹ç»äº†å…¶æ¶æ„ã€è®­ç»ƒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡å’Œå¤šç§åº”ç”¨ã€‚é€šè¿‡å¯¹100å¤šç¯‡æ–‡ç« çš„çš„ç³»ç»Ÿåˆ†æï¼Œæå‡ºäº†å…ˆè¿›çš„Protein LLMsçš„ç»“æ„åŒ–åˆ†ç±»ï¼Œåˆ†æäº†å®ƒä»¬å¦‚ä½•åˆ©ç”¨å¤§è§„æ¨¡è›‹ç™½è´¨åºåˆ—æ•°æ®æé«˜å‡†ç¡®æ€§ï¼Œå¹¶æ¢è®¨äº†å®ƒä»¬åœ¨æ¨è¿›è›‹ç™½è´¨å·¥ç¨‹å’Œç”Ÿç‰©åŒ»å­¦ç ”ç©¶æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è›‹ç™½è´¨ç‰¹å®šçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆProtein LLMsï¼‰åœ¨è›‹ç™½è´¨ç§‘å­¦é¢†åŸŸå‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œèƒ½å¤Ÿæé«˜è›‹ç™½è´¨ç»“æ„é¢„æµ‹ã€åŠŸèƒ½æ³¨é‡Šå’Œè®¾è®¡çš„æ•ˆç‡ã€‚</li>
<li>ç°æœ‰è°ƒæŸ¥ä¸»è¦å…³æ³¨Protein LLMsçš„ç‰¹å®šæ–¹é¢æˆ–åº”ç”¨ï¼Œè€Œæœ¬æ–‡é¦–æ¬¡æä¾›äº†å…¨é¢çš„ç»¼è¿°ï¼Œæ¶µç›–äº†æ¶æ„ã€è®­ç»ƒæ•°æ®é›†ã€è¯„ä¼°æŒ‡æ ‡å’Œå¤šç§åº”ç”¨ã€‚</li>
<li>é€šè¿‡ç³»ç»Ÿåˆ†æ100å¤šç¯‡ç›¸å…³è®ºæ–‡ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„Protein LLMsçš„ç»“æ„åŒ–åˆ†ç±»æ–¹æ³•ã€‚</li>
<li>Protein LLMsåˆ©ç”¨å¤§è§„æ¨¡è›‹ç™½è´¨åºåˆ—æ•°æ®æé«˜å‡†ç¡®æ€§ï¼Œå±•ç¤ºäº†å…¶åœ¨è›‹ç™½è´¨å·¥ç¨‹å’Œç”Ÿç‰©åŒ»å­¦ç ”ç©¶ä¸­çš„æ½œåŠ›ã€‚</li>
<li>è¯¥ç ”ç©¶è¿˜è®¨è®ºäº†Protein LLMsé¢ä¸´çš„å…³é”®æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚</li>
<li>Protein LLMså·²æˆä¸ºè›‹ç™½è´¨ç§‘å­¦ä¸­é‡è¦çš„ç§‘å­¦å‘ç°å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17504">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d58eba1da1a2db728155ba22ee0fd560.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-342ae809c9ea19ca9cc57ddb06cdc83d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18c17ab1e83fae6b495b6430c248c8f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13a9af5bb456551e288dc95bfeeaf34f.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="AdaptBot-Combining-LLM-with-Knowledge-Graphs-and-Human-Input-for-Generic-to-Specific-Task-Decomposition-and-Knowledge-Refinement"><a href="#AdaptBot-Combining-LLM-with-Knowledge-Graphs-and-Human-Input-for-Generic-to-Specific-Task-Decomposition-and-Knowledge-Refinement" class="headerlink" title="AdaptBot: Combining LLM with Knowledge Graphs and Human Input for   Generic-to-Specific Task Decomposition and Knowledge Refinement"></a>AdaptBot: Combining LLM with Knowledge Graphs and Human Input for   Generic-to-Specific Task Decomposition and Knowledge Refinement</h2><p><strong>Authors:Shivam Singh, Karthik Swaminathan, Nabanita Dash, Ramandeep Singh, Snehasis Banerjee, Mohan Sridharan, Madhava Krishna</strong></p>
<p>An embodied agent assisting humans is often asked to complete new tasks, and there may not be sufficient time or labeled examples to train the agent to perform these new tasks. Large Language Models (LLMs) trained on considerable knowledge across many domains can be used to predict a sequence of abstract actions for completing such tasks, although the agent may not be able to execute this sequence due to task-, agent-, or domain-specific constraints. Our framework addresses these challenges by leveraging the generic predictions provided by LLM and the prior domain knowledge encoded in a Knowledge Graph (KG), enabling an agent to quickly adapt to new tasks. The robot also solicits and uses human input as needed to refine its existing knowledge. Based on experimental evaluation in the context of cooking and cleaning tasks in simulation domains, we demonstrate that the interplay between LLM, KG, and human input leads to substantial performance gains compared with just using the LLM. Project website{\S}: <a target="_blank" rel="noopener" href="https://sssshivvvv.github.io/adaptbot/">https://sssshivvvv.github.io/adaptbot/</a> </p>
<blockquote>
<p>ä¸€ä¸ªå®ä½“ä»£ç†è¾…åŠ©äººç±»ç»å¸¸éœ€è¦å®Œæˆæ–°çš„ä»»åŠ¡ï¼Œå¹¶ä¸”å¯èƒ½æ²¡æœ‰è¶³å¤Ÿçš„æ—¶é—´æˆ–æ ‡è®°ç¤ºä¾‹æ¥è®­ç»ƒä»£ç†æ‰§è¡Œè¿™äº›æ–°ä»»åŠ¡ã€‚è™½ç„¶ä»£ç†å¯èƒ½ç”±äºä»»åŠ¡ã€ä»£ç†æˆ–é¢†åŸŸç‰¹å®šçš„çº¦æŸè€Œæ— æ³•æ‰§è¡Œæ­¤åºåˆ—ï¼Œä½†æ˜¯åŸºäºå¤šåŸŸå¹¿æ³›çŸ¥è¯†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ç”¨äºé¢„æµ‹å®Œæˆè¿™äº›ä»»åŠ¡çš„ä¸€ç³»åˆ—æŠ½è±¡åŠ¨ä½œã€‚æˆ‘ä»¬çš„æ¡†æ¶é€šè¿‡åˆ©ç”¨LLMæä¾›çš„é€šç”¨é¢„æµ‹å’Œç¼–ç åœ¨çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸­çš„å…ˆéªŒé¢†åŸŸçŸ¥è¯†æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œä»è€Œä½¿ä»£ç†èƒ½å¤Ÿè¿…é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚æœºå™¨äººè¿˜æ ¹æ®éœ€è¦å¾æ±‚å¹¶ä½¿ç”¨äººç±»è¾“å…¥æ¥å®Œå–„å…¶ç°æœ‰çŸ¥è¯†ã€‚åŸºäºæ¨¡æ‹Ÿé¢†åŸŸçš„çƒ¹é¥ªå’Œæ¸…æ´ä»»åŠ¡çš„å®éªŒè¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†LLMã€KGå’Œäººç±»è¾“å…¥ä¹‹é—´çš„ç›¸äº’ä½œç”¨ä¸ä»…ä½¿ç”¨LLMç›¸æ¯”ï¼Œå¯ä»¥å¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://sssshivvvv.github.io/adaptbot/">https://sssshivvvv.github.io/adaptbot/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02067v2">PDF</a> Accepted to IEEE International Conference on Robotics and Automation   (ICRA) 2025</p>
<p><strong>Summary</strong><br>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’ŒçŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰çš„æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨é€šç”¨é¢„æµ‹å’Œå…ˆéªŒé¢†åŸŸçŸ¥è¯†ï¼Œä½¿å®ä½“ä»£ç†èƒ½å¤Ÿè¿…é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚ä»£ç†åœ¨å¿…è¦æ—¶è¿˜ä¼šå¾æ±‚å¹¶åˆ©ç”¨äººç±»è¾“å…¥æ¥ä¼˜åŒ–ç°æœ‰çŸ¥è¯†ã€‚åœ¨æ¨¡æ‹ŸåŸŸçš„çƒ¹é¥ªå’Œæ¸…æ´ä»»åŠ¡å®éªŒè¯„ä¼°ä¸­ï¼Œä¸ä»…ä½¿ç”¨LLMç›¸æ¯”ï¼ŒLLMã€KGå’Œäººç±»è¾“å…¥ä¹‹é—´çš„ç›¸äº’ä½œç”¨å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMså¯é¢„æµ‹å®Œæˆæ–°ä»»åŠ¡çš„æŠ½è±¡åŠ¨ä½œåºåˆ—ã€‚</li>
<li>å®ä½“ä»£ç†å¯èƒ½å› ä»»åŠ¡ã€è‡ªèº«æˆ–é¢†åŸŸç‰¹å®šçº¦æŸï¼Œæ— æ³•æ‰§è¡Œè¯¥åºåˆ—ã€‚</li>
<li>æ¡†æ¶ç»“åˆLLMçš„é€šç”¨é¢„æµ‹å’ŒKGçš„å…ˆéªŒé¢†åŸŸçŸ¥è¯†ï¼ŒåŠ©åŠ›ä»£ç†å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚</li>
<li>ä»£ç†èƒ½å¤Ÿå¾æ±‚å¹¶åˆ©ç”¨äººç±»è¾“å…¥æ¥ä¼˜åŒ–ç°æœ‰çŸ¥è¯†ã€‚</li>
<li>åœ¨çƒ¹é¥ªå’Œæ¸…æ´ä»»åŠ¡çš„æ¨¡æ‹ŸåŸŸå®éªŒè¯„ä¼°ä¸­ï¼Œç»“åˆLLMã€KGå’Œäººç±»è¾“å…¥çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºä»…ä½¿ç”¨LLMã€‚</li>
<li>é¡¹ç›®ç½‘ç«™æä¾›äº†æ›´å¤šå…³äºè¯¥æ¡†æ¶çš„ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02067">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e80340b0a8d5a68accc7f31bfb904209.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cde7e507a16305ed26c812b942ca88f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffb9dd5648e68aa81510c95738047337.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-943ae4f8d5911880e45035f72349c68c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af1011ba4bc1188ed7609557bdd3e7a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25cf31987a1148572676f5811e29d25e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Human-Motion-Instruction-Tuning"><a href="#Human-Motion-Instruction-Tuning" class="headerlink" title="Human Motion Instruction Tuning"></a>Human Motion Instruction Tuning</h2><p><strong>Authors:Lei Li, Sen Jia, Wang Jianhao, Zhongyu Jiang, Feng Zhou, Ju Dai, Tianfang Zhang, Wu Zongkai, Jenq-Neng Hwang</strong></p>
<p>This paper presents LLaMo (Large Language and Human Motion Assistant), a multimodal framework for human motion instruction tuning. In contrast to conventional instruction-tuning approaches that convert non-linguistic inputs, such as video or motion sequences, into language tokens, LLaMo retains motion in its native form for instruction tuning. This method preserves motion-specific details that are often diminished in tokenization, thereby improving the modelâ€™s ability to interpret complex human behaviors. By processing both video and motion data alongside textual inputs, LLaMo enables a flexible, human-centric analysis. Experimental evaluations across high-complexity domains, including human behaviors and professional activities, indicate that LLaMo effectively captures domain-specific knowledge, enhancing comprehension and prediction in motion-intensive scenarios. We hope LLaMo offers a foundation for future multimodal AI systems with broad applications, from sports analytics to behavioral prediction. Our code and models are available on the project website: <a target="_blank" rel="noopener" href="https://github.com/ILGLJ/LLaMo">https://github.com/ILGLJ/LLaMo</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†LLaMoï¼ˆå¤§å‹è¯­è¨€å’Œäººç±»è¿åŠ¨è¾…åŠ©ç³»ç»Ÿï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºäººç±»è¿åŠ¨æŒ‡ä»¤è°ƒæ•´çš„å¤šæ¨¡å¼æ¡†æ¶ã€‚ä¸ä¼ ç»Ÿçš„å°†éè¯­è¨€è¾“å…¥ï¼ˆå¦‚è§†é¢‘æˆ–è¿åŠ¨åºåˆ—ï¼‰è½¬æ¢ä¸ºè¯­è¨€æ ‡è®°çš„æŒ‡ä»¤è°ƒæ•´æ–¹æ³•ä¸åŒï¼ŒLLaMoä¿ç•™è¿åŠ¨çš„æœ¬æœ‰å½¢å¼æ¥è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ã€‚è¿™ç§æ–¹æ³•ä¿ç•™äº†åŠ¨ä½œç‰¹å®šçš„ç»†èŠ‚ï¼Œè¿™äº›ç»†èŠ‚åœ¨ä»¤ç‰ŒåŒ–æ—¶é€šå¸¸ä¼šå‡å°‘ï¼Œä»è€Œæé«˜äº†æ¨¡å‹è§£é‡Šå¤æ‚äººç±»è¡Œä¸ºçš„èƒ½åŠ›ã€‚LLaMoèƒ½å¤Ÿå¤„ç†è§†é¢‘å’Œè¿åŠ¨æ•°æ®ä»¥åŠæ–‡æœ¬è¾“å…¥ï¼Œä»è€Œå®ç°çµæ´»ã€ä»¥äººä¸ºä¸­å¿ƒçš„åˆ†æã€‚åœ¨äººç±»è¡Œä¸ºå’Œä¸“ä¸šæ´»åŠ¨ç­‰é«˜å¤æ‚åº¦é¢†åŸŸçš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒLLaMoå¯ä»¥æœ‰æ•ˆåœ°æ•è·ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ï¼Œåœ¨è¿åŠ¨å¯†é›†å‹åœºæ™¯ä¸­æé«˜ç†è§£å’Œé¢„æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬å¸Œæœ›LLaMoèƒ½ä¸ºæœªæ¥å¤šæ¨¡å¼AIç³»ç»Ÿæä¾›åŸºç¡€ï¼Œå¹¿æ³›åº”ç”¨äºè¿åŠ¨åˆ†æåˆ°è¡Œä¸ºé¢„æµ‹ç­‰é¢†åŸŸã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨é¡¹ç›®ç½‘ç«™ä¸Šæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/ILGLJ/LLaMo%E3%80%82">https://github.com/ILGLJ/LLaMoã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16805v3">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong><br>LLaMoæ¡†æ¶ä»¥äººç±»è¿åŠ¨æŒ‡ä»¤è°ƒä¼˜ä¸ºä¸­å¿ƒï¼Œé€šè¿‡ä¿ç•™åŸå§‹åŠ¨ä½œå½¢å¼è€Œéå°†éè¯­è¨€è¾“å…¥è½¬åŒ–ä¸ºè¯­è¨€æ ‡è®°çš„æ–¹å¼è¿›è¡Œå¤„ç†ã€‚å®ƒç»“åˆè§†é¢‘å’Œè¿åŠ¨æ•°æ®ä¸æ–‡æœ¬è¾“å…¥ï¼Œæä¾›çµæ´»ä¸”ä»¥äººç±»ä¸ºä¸­å¿ƒçš„åˆ†ææ–¹å¼ã€‚è¯¥æ¡†æ¶å¯æ•è·é«˜å¤æ‚æ€§é¢†åŸŸçš„ç‰¹å®šçŸ¥è¯†ï¼Œæé«˜åœ¨åŠ¨ä½œå¯†é›†å‹åœºæ™¯ä¸­çš„ç†è§£å’Œé¢„æµ‹èƒ½åŠ›ã€‚LLaMoæ¡†æ¶æœ‰æœ›ä¸ºä»ä½“è‚²åˆ†æåˆ°è¡Œä¸ºé¢„æµ‹çš„å¤šæ¨¡å¼AIç³»ç»Ÿæä¾›åŸºç¡€ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®ç½‘ç«™ï¼š<a target="_blank" rel="noopener" href="https://github.com/ILGLJ/LLaMo">https://github.com/ILGLJ/LLaMo</a>ã€‚ </p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLaMoæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¡†æ¶ï¼Œç”¨äºäººç±»è¿åŠ¨æŒ‡ä»¤è°ƒä¼˜ã€‚</li>
<li>å®ƒä¿ç•™äº†åŸå§‹åŠ¨ä½œå½¢å¼ï¼Œé¿å…äº†å°†éè¯­è¨€è¾“å…¥è½¬åŒ–ä¸ºè¯­è¨€æ ‡è®°å¯¼è‡´çš„ç»†èŠ‚æŸå¤±ã€‚</li>
<li>LLaMoç»“åˆè§†é¢‘å’Œè¿åŠ¨æ•°æ®ä»¥åŠæ–‡æœ¬è¾“å…¥è¿›è¡Œå¤„ç†ï¼Œæä¾›äº†çµæ´»ä¸”ä»¥äººç±»ä¸ºä¸­å¿ƒçš„åˆ†ææ–¹å¼ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å¤æ‚é¢†åŸŸè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨åŠ¨ä½œå¯†é›†å‹åœºæ™¯ä¸­ã€‚</li>
<li>LLaMoæ¡†æ¶èƒ½å¤Ÿæ•è·ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Œæœ‰åŠ©äºæå‡ç†è§£å’Œé¢„æµ‹èƒ½åŠ›ã€‚</li>
<li>LLaMoçš„åº”ç”¨å‰æ™¯å¹¿æ³›ï¼Œå¯ä¸ºä½“è‚²åˆ†æã€è¡Œä¸ºé¢„æµ‹ç­‰å¤šæ¨¡æ€AIç³»ç»Ÿæ‰“ä¸‹åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16805">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e7a91a53c916cd827ae9911b61215b2e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60888dd67b71d0ef182a138fe8a1de30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62a8e81f246781b589809b8f31019850.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="LaVin-DiT-Large-Vision-Diffusion-Transformer"><a href="#LaVin-DiT-Large-Vision-Diffusion-Transformer" class="headerlink" title="LaVin-DiT: Large Vision Diffusion Transformer"></a>LaVin-DiT: Large Vision Diffusion Transformer</h2><p><strong>Authors:Zhaoqing Wang, Xiaobo Xia, Runnan Chen, Dongdong Yu, Changhu Wang, Mingming Gong, Tongliang Liu</strong></p>
<p>This paper presents the Large Vision Diffusion Transformer (LaVin-DiT), a scalable and unified foundation model designed to tackle over 20 computer vision tasks in a generative framework. Unlike existing large vision models directly adapted from natural language processing architectures, which rely on less efficient autoregressive techniques and disrupt spatial relationships essential for vision data, LaVin-DiT introduces key innovations to optimize generative performance for vision tasks. First, to address the high dimensionality of visual data, we incorporate a spatial-temporal variational autoencoder that encodes data into a continuous latent space. Second, for generative modeling, we develop a joint diffusion transformer that progressively produces vision outputs. Third, for unified multi-task training, in-context learning is implemented. Input-target pairs serve as task context, which guides the diffusion transformer to align outputs with specific tasks within the latent space. During inference, a task-specific context set and test data as queries allow LaVin-DiT to generalize across tasks without fine-tuning. Trained on extensive vision datasets, the model is scaled from 0.1B to 3.4B parameters, demonstrating substantial scalability and state-of-the-art performance across diverse vision tasks. This work introduces a novel pathway for large vision foundation models, underscoring the promising potential of diffusion transformers. The code and models are available. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è§†è§‰æ‰©æ•£è½¬æ¢å™¨ï¼ˆLaVin-DiTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¯æ‰©å±•çš„ç»Ÿä¸€åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨åœ¨ä¸€ä¸ªç”Ÿæˆæ¡†æ¶ä¸­è§£å†³è¶…è¿‡20é¡¹è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚ä¸ç°æœ‰çš„ç›´æ¥ä»è‡ªç„¶è¯­è¨€å¤„ç†æ¶æ„æ”¹ç¼–çš„å¤§å‹è§†è§‰æ¨¡å‹ä¸åŒï¼Œè¿™äº›æ¨¡å‹ä¾èµ–äºæ•ˆç‡è¾ƒä½çš„è‡ªåŠ¨å›å½’æŠ€æœ¯ï¼Œå¹¶ç ´åäº†å¯¹è§†è§‰æ•°æ®è‡³å…³é‡è¦çš„ç©ºé—´å…³ç³»ã€‚LaVin-DiTå¼•å…¥äº†å…³é”®çš„åˆ›æ–°ç‚¹ï¼Œä»¥ä¼˜åŒ–è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„ç”Ÿæˆæ€§èƒ½ã€‚é¦–å…ˆï¼Œé’ˆå¯¹è§†è§‰æ•°æ®çš„é«˜ç»´æ€§ï¼Œæˆ‘ä»¬èå…¥æ—¶ç©ºå˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼Œå°†æ•°æ®ç¼–ç ä¸ºè¿ç»­æ½œåœ¨ç©ºé—´ã€‚å…¶æ¬¡ï¼Œä¸ºäº†è¿›è¡Œç”Ÿæˆå»ºæ¨¡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§è”åˆæ‰©æ•£è½¬æ¢å™¨ï¼Œå…¶é€æ­¥ç”Ÿæˆè§†è§‰è¾“å‡ºã€‚ç¬¬ä¸‰ï¼Œä¸ºäº†è¿›è¡Œç»Ÿä¸€çš„å¤šä»»åŠ¡è®­ç»ƒï¼Œå®æ–½äº†ä¸Šä¸‹æ–‡å†…å­¦ä¹ ã€‚è¾“å…¥-ç›®æ ‡å¯¹ä½œä¸ºä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ŒæŒ‡å¯¼æ‰©æ•£è½¬æ¢å™¨åœ¨æ½œåœ¨ç©ºé—´å†…å°†è¾“å‡ºä¸ç‰¹å®šä»»åŠ¡å¯¹é½ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œç‰¹å®šä»»åŠ¡ä¸Šä¸‹æ–‡é›†å’Œæµ‹è¯•æ•°æ®ä½œä¸ºæŸ¥è¯¢ï¼Œä½¿LaVin-DiTèƒ½å¤Ÿåœ¨æ— éœ€å¾®è°ƒçš„æƒ…å†µä¸‹è·¨ä»»åŠ¡è¿›è¡Œæ¨å¹¿ã€‚è¯¥æ¨¡å‹ç»è¿‡å¹¿æ³›çš„è§†è§‰æ•°æ®é›†è®­ç»ƒï¼Œè§„æ¨¡ä»0.1Bæ‰©å±•åˆ°3.4Bå‚æ•°ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„å¯æ‰©å±•æ€§å’Œåœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸Šçš„å“è¶Šæ€§èƒ½ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹å¼•å…¥äº†ä¸€æ¡æ–°çš„é€”å¾„ï¼Œçªæ˜¾äº†æ‰©æ•£è½¬æ¢å™¨çš„å·¨å¤§æ½œåŠ›ã€‚ä»£ç å’Œæ¨¡å‹å‡å¯ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11505v4">PDF</a> 37 pages, 30 figures, 4 tables. Accepted by CVPR 2025</p>
<p><strong>Summary</strong>ï¼šæ­¤è®ºæ–‡ä»‹ç»äº†ä¸€ç§å¤§å‹è§†è§‰æ‰©æ•£å˜æ¢æ¨¡å‹ï¼ˆLaVin-DiTï¼‰ï¼Œå®ƒæ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„ç»Ÿä¸€åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨åœ¨ä¸€ä¸ªç”Ÿæˆæ¡†æ¶ä¸­è§£å†³è¶…è¿‡20é¡¹è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†ä¸€ç³»åˆ—åˆ›æ–°æŠ€æœ¯ï¼Œä»¥ä¼˜åŒ–è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„ç”Ÿæˆæ€§èƒ½ã€‚åŒ…æ‹¬ä½¿ç”¨æ—¶ç©ºå˜åˆ†è‡ªç¼–ç å™¨è§£å†³è§†è§‰æ•°æ®çš„é«˜ç»´é—®é¢˜ï¼Œå¼€å‘è”åˆæ‰©æ•£å˜æ¢å™¨è¿›è¡Œç”Ÿæˆå»ºæ¨¡ï¼Œä»¥åŠå®ç°ç»Ÿä¸€å¤šä»»åŠ¡è®­ç»ƒçš„ä¸Šä¸‹æ–‡å†…å­¦ä¹ ã€‚è¯¥æ¨¡å‹åœ¨å¹¿æ³›çš„è§†è§‰æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå‚æ•°è§„æ¨¡ä»0.1Båˆ°3.4Bï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„æ‰©å±•æ€§å’Œè·¨å„ç§è§†è§‰ä»»åŠ¡çš„å“è¶Šæ€§èƒ½ã€‚æ­¤å·¥ä½œä¸ºå¤§å‹è§†è§‰åŸºç¡€æ¨¡å‹å¼€è¾Ÿäº†ä¸€æ¡æ–°é€”å¾„ï¼Œå±•ç¤ºäº†æ‰©æ•£å˜æ¢å™¨çš„å¹¿é˜”å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LaVin-DiTæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åŸºç¡€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚</li>
<li>æ¨¡å‹å¼•å…¥äº†æ—¶ç©ºå˜åˆ†è‡ªç¼–ç å™¨ï¼Œä»¥ä¼˜åŒ–è§†è§‰æ•°æ®çš„å¤„ç†ã€‚</li>
<li>é€šè¿‡è”åˆæ‰©æ•£å˜æ¢å™¨è¿›è¡Œç”Ÿæˆå»ºæ¨¡ï¼Œé€æ­¥äº§ç”Ÿè§†è§‰è¾“å‡ºã€‚</li>
<li>é‡‡ç”¨ä¸Šä¸‹æ–‡å†…å­¦ä¹ å®ç°ç»Ÿä¸€å¤šä»»åŠ¡è®­ç»ƒã€‚</li>
<li>æ¨¡å‹å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œå‚æ•°è§„æ¨¡ä»0.1Båˆ°3.4Bã€‚</li>
<li>LaVin-DiTåœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11505">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c25b45182f1a02abbe6f45329380cbff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-637ffd24f99bfd8d8d5a3477f63c5bd4.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ACC-Collab-An-Actor-Critic-Approach-to-Multi-Agent-LLM-Collaboration"><a href="#ACC-Collab-An-Actor-Critic-Approach-to-Multi-Agent-LLM-Collaboration" class="headerlink" title="ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration"></a>ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration</h2><p><strong>Authors:Andrew Estornell, Jean-Francois Ton, Yuanshun Yao, Yang Liu</strong></p>
<p>Large language models (LLMs) have demonstrated a remarkable ability to serve as general-purpose tools for various language-based tasks. Recent works have demonstrated that the efficacy of such models can be improved through iterative dialog between multiple models. While these paradigms show promise in improving model efficacy, most works in this area treat collaboration as an emergent behavior, rather than a learned behavior. In doing so, current multi-agent frameworks rely on collaborative behaviors to have been sufficiently trained into off-the-shelf models. To address this limitation, we propose ACC-Collab, an Actor-Critic based learning framework to produce a two-agent team (an actor-agent and a critic-agent) specialized in collaboration. We demonstrate that ACC-Collab outperforms SotA multi-agent techniques on a wide array of benchmarks. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å±•ç°å‡ºä½œä¸ºå„ç§è¯­è¨€åŸºç¡€ä»»åŠ¡çš„é€šç”¨å·¥å…·çš„å‡ºè‰²èƒ½åŠ›ã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œè¡¨æ˜ï¼Œé€šè¿‡å¤šä¸ªæ¨¡å‹ä¹‹é—´çš„è¿­ä»£å¯¹è¯å¯ä»¥æé«˜æ­¤ç±»æ¨¡å‹çš„æ•ˆç‡ã€‚è™½ç„¶è¿™äº›èŒƒå¼åœ¨æé«˜æ¨¡å‹æ•ˆç‡æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†è¯¥é¢†åŸŸçš„å¤§å¤šæ•°å·¥ä½œéƒ½å°†åä½œè§†ä¸ºä¸€ç§çªå‘è¡Œä¸ºï¼Œè€Œéå­¦ä¹ è¡Œä¸ºã€‚å› æ­¤ï¼Œå½“å‰çš„å¤šä»£ç†æ¡†æ¶ä¾èµ–äºåä½œè¡Œä¸ºå·²ç»å……åˆ†è®­ç»ƒæˆå³å¸­æ¨¡å‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ACC-Collabï¼Œè¿™æ˜¯ä¸€ç§åŸºäºActor-Criticçš„å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆä¸€ä¸ªä¸“é—¨åä½œçš„ä¸¤ä»£ç†å›¢é˜Ÿï¼ˆä¸€ä¸ªè¡ŒåŠ¨ä»£ç†å’Œä¸€ä¸ªæ‰¹è¯„ä»£ç†ï¼‰ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒACC-Collabçš„è¡¨ç°ä¼˜äºå½“å‰æœ€ä½³çš„å¤šä»£ç†æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00053v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºå¼ºå¤§çš„é€šç”¨å·¥å…·èƒ½åŠ›ï¼Œå¯é€šè¿‡å¤šæ¬¡è¿­ä»£å¯¹è¯æé«˜æ¨¡å‹æ•ˆèƒ½ã€‚å½“å‰å¤šä»£ç†æ¡†æ¶ä¾èµ–æ¨¡å‹é¢„å…ˆè®­ç»ƒå¥½çš„åä½œè¡Œä¸ºï¼Œè€Œåä½œèƒ½åŠ›è¢«è§†ä¸ºä¸€ç§æ¶Œç°ç‰¹æ€§è€Œéå­¦ä¹ è¡Œä¸ºã€‚ä¸ºè§£å†³æ­¤å±€é™ï¼Œæå‡ºACC-Collabæ¡†æ¶ï¼Œæ„å»ºä¸“é—¨çš„æ¼”å‘˜ä»£ç†å’Œè¯„è®ºå®¶ä»£ç†å›¢é˜Ÿã€‚å®éªŒè¯æ˜ï¼ŒACC-Collabåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå±•ç°å‡ºå¼ºå¤§çš„é€šç”¨å·¥å…·èƒ½åŠ›ï¼Œå¹¶èƒ½é€šè¿‡å¤šæ¬¡è¿­ä»£å¯¹è¯æé«˜æ¨¡å‹æ•ˆèƒ½ã€‚</li>
<li>å½“å‰å¤šä»£ç†æ¡†æ¶åœ¨å¤„ç†LLMåä½œæ—¶å­˜åœ¨å±€é™æ€§ï¼Œä¾èµ–æ¨¡å‹é¢„å…ˆè®­ç»ƒå¥½çš„åä½œè¡Œä¸ºã€‚</li>
<li>åä½œèƒ½åŠ›è¢«è§†ä¸ºä¸€ç§æ¶Œç°ç‰¹æ€§è€Œéå­¦ä¹ è¡Œä¸ºã€‚</li>
<li>æå‡ºACC-Collabæ¡†æ¶æ¥è§£å†³ä¸Šè¿°å±€é™ï¼Œæ„å»ºä¸“é—¨çš„æ¼”å‘˜ä»£ç†å’Œè¯„è®ºå®¶ä»£ç†å›¢é˜Ÿã€‚</li>
<li>æ¼”å‘˜ä»£ç†å’Œè¯„è®ºå®¶ä»£ç†åœ¨ACC-Collabæ¡†æ¶ä¸­åˆ†åˆ«æ‰¿æ‹…ä¸åŒçš„è§’è‰²ï¼Œå…±åŒå®ç°é«˜æ•ˆçš„åä½œã€‚</li>
<li>ACC-Collabåœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.00053">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c7eb8933593370723397907a2b0cfb45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd6f88554a948be927d3394cd6f6e7c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f17130aa9ebabb6ef12a9e0e8c4572e8.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="RAG-DDR-Optimizing-Retrieval-Augmented-Generation-Using-Differentiable-Data-Rewards"><a href="#RAG-DDR-Optimizing-Retrieval-Augmented-Generation-Using-Differentiable-Data-Rewards" class="headerlink" title="RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable   Data Rewards"></a>RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable   Data Rewards</h2><p><strong>Authors:Xinze Li, Sen Mei, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Hao Chen, Ge Yu, Zhiyuan Liu, Maosong Sun, Chenyan Xiong</strong></p>
<p>Retrieval-Augmented Generation (RAG) has proven its effectiveness in mitigating hallucinations in Large Language Models (LLMs) by retrieving knowledge from external resources. To adapt LLMs for the RAG systems, current approaches use instruction tuning to optimize LLMs, improving their ability to utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses on equipping LLMs to handle diverse RAG tasks using different instructions. However, it trains RAG modules to overfit training signals and overlooks the varying data preferences among agents within the RAG system. In this paper, we propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG systems by aligning data preferences between different RAG modules. DDR works by collecting the rewards to optimize each agent in the RAG system with the rollout method, which prompts agents to sample some potential responses as perturbations, evaluates the impact of these perturbations on the whole RAG system, and subsequently optimizes the agent to produce outputs that improve the performance of the RAG system. Our experiments on various knowledge-intensive tasks demonstrate that DDR significantly outperforms the SFT method, particularly for LLMs with smaller-scale parameters that depend more on the retrieved knowledge. Additionally, DDR exhibits a stronger capability to align the data preference between RAG modules. The DDR method makes the generation module more effective in extracting key information from documents and mitigating conflicts between parametric memory and external knowledge. All codes are available at <a target="_blank" rel="noopener" href="https://github.com/OpenMatch/RAG-DDR">https://github.com/OpenMatch/RAG-DDR</a>. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡ä»å¤–éƒ¨èµ„æºæ£€ç´¢çŸ¥è¯†ï¼Œåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­è¯æ˜äº†å…¶å‡è½»å¹»è§‰çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†é€‚åº”RAGç³»ç»Ÿçš„LLMï¼Œå½“å‰çš„æ–¹æ³•ä½¿ç”¨æŒ‡ä»¤è°ƒæ•´æ¥ä¼˜åŒ–LLMï¼Œæé«˜å…¶åˆ©ç”¨æ£€ç´¢çŸ¥è¯†çš„èƒ½åŠ›ã€‚è¿™ç§æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•ä¾§é‡äºåˆ©ç”¨ä¸åŒæŒ‡ä»¤ä½¿LLMèƒ½å¤Ÿå¤„ç†å„ç§RAGä»»åŠ¡ã€‚ç„¶è€Œï¼Œå®ƒä½¿RAGæ¨¡å—è¿‡åº¦é€‚åº”è®­ç»ƒä¿¡å·ï¼Œå¹¶å¿½ç•¥äº†RAGç³»ç»Ÿå†…ä»£ç†ä¹‹é—´çš„æ•°æ®åå¥½å·®å¼‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯å¾®åˆ†æ•°æ®å¥–åŠ±ï¼ˆDDRï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ç«¯åˆ°ç«¯è®­ç»ƒRAGç³»ç»Ÿï¼Œå¯¹é½ä¸åŒRAGæ¨¡å—ä¹‹é—´çš„æ•°æ®åå¥½ã€‚DDRé€šè¿‡æ”¶é›†å¥–åŠ±æ¥ä¼˜åŒ–RAGç³»ç»Ÿä¸­æ¯ä¸ªä»£ç†çš„æ»šåŠ¨æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é¼“åŠ±ä»£ç†å¯¹ä¸€äº›æ½œåœ¨å“åº”è¿›è¡Œé‡‡æ ·ä½œä¸ºæ‰°åŠ¨ï¼Œè¯„ä¼°è¿™äº›æ‰°åŠ¨å¯¹æ•´ä¸ªRAGç³»ç»Ÿçš„å½±å“ï¼Œç„¶åä¼˜åŒ–ä»£ç†ä»¥äº§ç”Ÿæé«˜RAGç³»ç»Ÿæ€§èƒ½çš„è¾“å‡ºæ¥ã€‚æˆ‘ä»¬åœ¨å„ç§çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDDRæ˜¾è‘—ä¼˜äºSFTæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ›´ä¾èµ–æ£€ç´¢çŸ¥è¯†çš„å°è§„æ¨¡å‚æ•°LLMã€‚æ­¤å¤–ï¼ŒDDRåœ¨å¯¹é½RAGæ¨¡å—ä¹‹é—´çš„æ•°æ®åå¥½æ–¹é¢è¡¨ç°å‡ºæ›´å¼ºçš„èƒ½åŠ›ã€‚DDRæ–¹æ³•ä½¿ç”Ÿæˆæ¨¡å—æ›´æœ‰æ•ˆåœ°ä»æ–‡æ¡£ä¸­æå–å…³é”®ä¿¡æ¯ï¼Œå¹¶å‡è½»å‚æ•°å†…å­˜å’Œå¤–éƒ¨çŸ¥è¯†ä¹‹é—´çš„å†²çªã€‚æ‰€æœ‰ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/OpenMatch/RAG-DDR%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/OpenMatch/RAG-DDRä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13509v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é€šè¿‡ä»å¤–éƒ¨èµ„æºæ£€ç´¢çŸ¥è¯†ï¼Œæœ‰æ•ˆå‡è½»äº†æ¨¡å‹è™šæ„ï¼ˆhallucinationï¼‰çš„é—®é¢˜ã€‚å½“å‰çš„æ–¹æ³•é‡‡ç”¨æŒ‡ä»¤è°ƒæ•´ï¼ˆinstruction tuningï¼‰æ¥ä¼˜åŒ–LLMsï¼Œä½¿å…¶æ›´å¥½åœ°åˆ©ç”¨æ£€ç´¢åˆ°çš„çŸ¥è¯†ã€‚ç„¶è€Œï¼Œè¿™ç§ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•æ³¨é‡é€šè¿‡ä¸åŒæŒ‡ä»¤å¤„ç†å¤šæ ·åŒ–çš„RAGä»»åŠ¡ï¼Œå´å®¹æ˜“å¯¼è‡´RAGæ¨¡å—è¿‡åº¦é€‚åº”è®­ç»ƒä¿¡å·ï¼Œå¹¶å¿½ç•¥äº†RAGç³»ç»Ÿå†…å„ä»£ç†çš„æ•°æ®åå¥½å·®å¼‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¯å¾®åˆ†æ•°æ®å¥–åŠ±ï¼ˆDDRï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç«¯å¯¹ç«¯åœ°è®­ç»ƒRAGç³»ç»Ÿï¼Œå¯¹é½ä¸åŒRAGæ¨¡å—çš„æ•°æ®åå¥½ã€‚DDRé€šè¿‡æ”¶é›†å¥–åŠ±æ¥ä¼˜åŒ–RAGç³»ç»Ÿä¸­çš„æ¯ä¸ªä»£ç†ï¼Œé‡‡ç”¨rolloutæ–¹æ³•ä¿ƒä½¿ä»£ç†ç”Ÿæˆä¸€äº›å¯èƒ½çš„å“åº”ä½œä¸ºæ‰°åŠ¨ï¼Œè¯„ä¼°è¿™äº›æ‰°åŠ¨å¯¹æ•´ä¸ªRAGç³»ç»Ÿçš„å½±å“ï¼Œä»è€Œä¼˜åŒ–ä»£ç†ä»¥äº§ç”Ÿæé«˜RAGç³»ç»Ÿæ€§èƒ½çš„è¾“å‡ºã€‚å®éªŒè¡¨æ˜ï¼ŒDDRæ–¹æ³•åœ¨å¤šç§çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºSFTæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¾èµ–æ£€ç´¢çŸ¥è¯†çš„å°è§„æ¨¡å‚æ•°LLMsä¸Šã€‚DDRæ–¹æ³•å…·æœ‰æ›´å¼ºçš„å¯¹é½RAGæ¨¡å—æ•°æ®åå¥½çš„èƒ½åŠ›ï¼Œæé«˜äº†ç”Ÿæˆæ¨¡å—ä»æ–‡æ¡£ä¸­æå–å…³é”®ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå¹¶å‡è½»äº†å‚æ•°å†…å­˜å’Œå¤–éƒ¨çŸ¥è¯†ä¹‹é—´çš„å†²çªã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAGé€šè¿‡ä»å¤–éƒ¨èµ„æºæ£€ç´¢çŸ¥è¯†ï¼Œæœ‰æ•ˆå‡è½»LLMä¸­çš„è™šæ„é—®é¢˜ã€‚</li>
<li>å½“å‰LLMä¼˜åŒ–æ–¹æ³•å¤šé‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä½†è¿™ç§æ–¹æ³•å¯èƒ½å¯¼è‡´è¿‡åº¦é€‚åº”è®­ç»ƒä¿¡å·ã€‚</li>
<li>DDRæ–¹æ³•é€šè¿‡ç«¯å¯¹ç«¯è®­ç»ƒRAGç³»ç»Ÿï¼Œå¯¹é½ä¸åŒæ¨¡å—çš„æ•°æ®åå¥½ã€‚</li>
<li>DDRé‡‡ç”¨rolloutæ–¹æ³•ä¼˜åŒ–ä»£ç†ï¼Œé€šè¿‡è¯„ä¼°æ‰°åŠ¨å½±å“æ¥æ”¹è¿›ä»£ç†è¾“å‡ºã€‚</li>
<li>DDRåœ¨å¤šç§çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºSFTæ–¹æ³•ã€‚</li>
<li>DDRå¯¹å°è§„æ¨¡å‚æ•°LLMså°¤å…¶æœ‰æ•ˆï¼Œè¿™äº›æ¨¡å‹æ›´ä¾èµ–æ£€ç´¢çŸ¥è¯†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13509">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5296383301ff52d9b70263961f943f18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36d8d5da773870cd606d197f31d6c05c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10063682d00eff7850542f5d244c7e5c.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a6d19e10e57fe6b34da29c78421a0ded.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-09  Multi-Agent Inverse Q-Learning from Demonstrations
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b58eda2b46e7a065a1bfb7629fee2d02.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-09  L1 Controlling How Long A Reasoning Model Thinks With Reinforcement   Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17665k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
