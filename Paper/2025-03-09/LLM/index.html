<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-03-09  LLMVoX Autoregressive Streaming Text-to-Speech Model for Any LLM">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-c7eb8933593370723397907a2b0cfb45.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    78 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-09-更新"><a href="#2025-03-09-更新" class="headerlink" title="2025-03-09 更新"></a>2025-03-09 更新</h1><h2 id="LLMVoX-Autoregressive-Streaming-Text-to-Speech-Model-for-Any-LLM"><a href="#LLMVoX-Autoregressive-Streaming-Text-to-Speech-Model-for-Any-LLM" class="headerlink" title="LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM"></a>LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM</h2><p><strong>Authors:Sambal Shikhar, Mohammed Irfan Kurpath, Sahal Shaji Mullappilly, Jean Lahoud, Fahad Khan, Rao Muhammad Anwer, Salman Khan, Hisham Cholakkal</strong></p>
<p>Recent advancements in speech-to-speech dialogue systems leverage LLMs for multimodal interactions, yet they remain hindered by fine-tuning requirements, high computational overhead, and text-speech misalignment. Existing speech-enabled LLMs often degrade conversational quality by modifying the LLM, thereby compromising its linguistic capabilities. In contrast, we propose LLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS system that generates high-quality speech with low latency, while fully preserving the capabilities of the base LLM. Our approach achieves a significantly lower Word Error Rate compared to speech-enabled LLMs, while operating at comparable latency and UTMOS score. By decoupling speech synthesis from LLM processing via a multi-queue token streaming system, LLMVoX supports seamless, infinite-length dialogues. Its plug-and-play design also facilitates extension to various tasks with different backbones. Furthermore, LLMVoX generalizes to new languages with only dataset adaptation, attaining a low Character Error Rate on an Arabic speech task. Additionally, we have integrated LLMVoX with a Vision-Language Model to create an omni-model with speech, text, and vision capabilities, without requiring additional multimodal training. Our code base and project page is available at <a target="_blank" rel="noopener" href="https://mbzuai-oryx.github.io/LLMVoX">https://mbzuai-oryx.github.io/LLMVoX</a> . </p>
<blockquote>
<p>最近，语音到语音对话系统的进展充分利用了大型语言模型（LLM）进行多模式交互，但它们仍然受到精细调整要求、高计算开销和文本-语音不对齐的限制。现有的语音赋能的大型语言模型往往会通过修改大型语言模型来降低对话质量，从而损害其语言功能。相比之下，我们提出了LLMVoX，这是一个轻量级的、参数规模为30M的大型语言模型无关的、自回归流式文本到语音（TTS）系统，它可以在保持基础大型语言模型功能的同时，以低延迟生成高质量语音。我们的方法与语音赋能的大型语言模型相比，显著降低了单词错误率，同时在延迟和UTMOS分数方面表现出可比性。通过多队列令牌流系统，将语音合成与大型语言模型处理解耦，LLMVoX支持无缝、无限长度的对话。其即插即用设计也便于扩展到具有不同主干的各种任务。此外，LLMVoX仅通过数据集适应就能适应新语言，在阿拉伯语音任务上达到了较低的字符错误率。此外，我们将LLMVoX与视觉语言模型集成，创建了一个具有语音、文本和视觉功能的万能模型，无需额外的多模式训练。我们的代码库和项目页面可在<a target="_blank" rel="noopener" href="https://mbzuai-oryx.github.io/LLMVoX%E8%AE%BF%E9%97%AE%E3%80%82">https://mbzuai-oryx.github.io/LLMVoX访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04724v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMVoX是一种轻量级的文本转语音（TTS）系统，具有高质量的语音生成能力，支持流畅无限的对话，同时保留基础大型语言模型（LLM）的全部功能。它通过解耦语音合成与LLM处理过程，采用多队列令牌流式传输系统实现低延迟和高性能。LLMVoX易于集成到其他任务中，并能适应不同的语言模型。此外，它还与视觉语言模型集成，形成具有语音、文本和视觉能力的全能模型。详情访问其官网了解。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMVoX是一个轻量级的文本转语音系统，用于增强对话系统的语音功能。</li>
<li>它保留了基础大型语言模型的所有功能，同时实现了高质量的语音生成。</li>
<li>LLMVoX利用多队列令牌流式传输系统实现语音合成与LLM处理的解耦。</li>
<li>系统具有低延迟、高质量语音和无缝对话的能力。</li>
<li>LLMVoX易于集成到其他任务中，并能够扩展到各种不同的应用场景。</li>
<li>该系统具有良好的泛化能力，可以通过仅适应数据集来适应新语言。</li>
<li>LLMVoX与视觉语言模型集成，形成具有多种能力的全能模型。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04724">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7ec9b4bd289d72db89e0e73f89690d75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cf534253bb878e35cd0a0e33ab798ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a25fdae49c198c9f5bb31f3c5c34aa9b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Predictable-Scale-Part-I-–-Optimal-Hyperparameter-Scaling-Law-in-Large-Language-Model-Pretraining"><a href="#Predictable-Scale-Part-I-–-Optimal-Hyperparameter-Scaling-Law-in-Large-Language-Model-Pretraining" class="headerlink" title="Predictable Scale: Part I – Optimal Hyperparameter Scaling Law in Large   Language Model Pretraining"></a>Predictable Scale: Part I – Optimal Hyperparameter Scaling Law in Large   Language Model Pretraining</h2><p><strong>Authors:Houyi Li, Wenzheng Zheng, Jingcheng Hu, Qiufeng Wang, Hanshan Zhang, Zili Wang, Yangshijie Xu, Shuigeng Zhou, Xiangyu Zhang, Daxin Jiang</strong></p>
<p>The impressive capabilities of Large Language Models (LLMs) across diverse tasks are now well-established, yet their effective deployment necessitates careful hyperparameter optimization. Through extensive empirical studies involving grid searches across diverse configurations, we discover universal scaling laws governing these hyperparameters: optimal learning rate follows a power-law relationship with both model parameters and data sizes, while optimal batch size scales primarily with data sizes. Our analysis reveals a convex optimization landscape for hyperparameters under fixed models and data size conditions. This convexity implies an optimal hyperparameter plateau. We contribute a universal, plug-and-play optimal hyperparameter tool for the community. Its estimated values on the test set are merely 0.07% away from the globally optimal LLM performance found via an exhaustive search. These laws demonstrate remarkable robustness across variations in model sparsity, training data distribution, and model shape. To our best known, this is the first work that unifies different model shapes and structures, such as Mixture-of-Experts models and dense transformers, as well as establishes optimal hyperparameter scaling laws across diverse data distributions. This exhaustive optimization process demands substantial computational resources, utilizing nearly one million NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and hyperparameters from scratch and consuming approximately 100 trillion tokens in total. To facilitate reproducibility and further research, we will progressively release all loss measurements and model checkpoints through our designated repository <a target="_blank" rel="noopener" href="https://step-law.github.io/">https://step-law.github.io/</a> </p>
<blockquote>
<p>大型语言模型（LLM）在各项任务中的出色能力现已得到广泛认可，但其有效部署需要进行谨慎的超参数优化。我们通过涉及多种配置的网格搜索的广泛实证研究，发现了这些超参数的通用缩放定律：最佳学习率与模型参数和数据大小之间存在幂律关系，而最佳批处理大小主要随数据大小进行缩放。我们的分析揭示了固定模型和数据大小条件下超参数的凸优化景观。这种凸性意味着存在一个最佳超参数平台。我们为社区贡献了一个通用、即插即用的最佳超参数工具。其在测试集上的估计值与通过详尽搜索找到的全局最佳LLM性能仅相差0.07%。这些定律在模型稀疏性、训练数据分布和模型形状的变化中表现出惊人的稳健性。据我们所知，这是第一项统一不同模型形状和结构的工作，如专家混合模型和密集转换器，并建立了跨不同数据分布的最佳超参数缩放定律。这一详尽的优化过程需要大量的计算资源，使用近百万NVIDIA H800 GPU小时从头开始训练各种大小和超参数的3700个LLM，总共消耗约10万亿个令牌。为了便于复制和进一步研究，我们将逐步释放所有损失测量和模型检查点，通过我们的指定仓库[<a target="_blank" rel="noopener" href="https://step-law.github.io/]%E8%BF%9B%E8%A1%8C%E5%85%AC%E5%BC%80%E3%80%82">https://step-law.github.io/]进行公开。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04715v1">PDF</a> 19 pages</p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLM）在各项任务中的强大能力已广为人知，但其有效部署需要谨慎的超参数优化。通过广泛的实证研究，我们发现了通用超参数缩放定律：最佳学习率与模型参数和数据大小呈幂律关系，而最佳批次大小主要随数据大小而扩展。分析表明，在固定模型和数据大小条件下，超参数优化景观呈现凸性，暗示存在一个最佳超参数平台。我们为社区贡献了一个通用、即插即用的最佳超参数工具。其在测试集上的估计值与通过全面搜索找到的全局最佳LLM性能仅相差0.07%。这些定律在模型稀疏性、训练数据分布和模型形状的变化方面表现出惊人的稳健性。据我们所知，这项工作统一了不同的模型形状和结构，如Mixture-of-Experts模型和密集转换器，并建立了跨不同数据分布的最佳超参数缩放定律。本优化过程需要大量的计算资源，利用近百万个NVIDIA H800 GPU小时从头开始训练3700个不同大小和超参数的LLM，并总共消耗约十万亿个令牌。为了便于复制和进一步研究，我们将逐步发布所有损失测量和模型检查点。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型（LLM）在不同任务中展现出强大的能力，但其部署需要对超参数进行精心优化。</li>
<li>发现超参数缩放定律：最佳学习率与模型参数和数据大小呈幂律关系，而最佳批次大小随数据大小扩展。</li>
<li>在固定模型和数据大小条件下，超参数优化呈现凸性，存在最佳超参数平台。</li>
<li>提出通用、即插即用的最佳超参数工具，其性能接近全局最优。</li>
<li>这些定律在模型稀疏性、数据分布和模型形状变化方面表现出稳健性。</li>
<li>统一了不同的模型形状和结构，如Mixture-of-Experts模型和密集转换器。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04715">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-06a3c1006b6d5ac25becce852a58adab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9b8411ee116b56bb17a6e87e319ade8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6f01b5392c37709fae7c479f2a7aaf82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0a97518e3e5afeac3fe933267199386.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73d2e510691da61526bc11fa0295f9cd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Universality-of-Layer-Level-Entropy-Weighted-Quantization-Beyond-Model-Architecture-and-Size"><a href="#Universality-of-Layer-Level-Entropy-Weighted-Quantization-Beyond-Model-Architecture-and-Size" class="headerlink" title="Universality of Layer-Level Entropy-Weighted Quantization Beyond Model   Architecture and Size"></a>Universality of Layer-Level Entropy-Weighted Quantization Beyond Model   Architecture and Size</h2><p><strong>Authors:Alireza Behtash, Marijan Fofonjka, Ethan Baird, Tyler Mauer, Hossein Moghimifam, David Stout, Joel Dennison</strong></p>
<p>We present a novel approach to selective model quantization that transcends the limitations of architecture-specific and size-dependent compression methods for Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ). By analyzing the entropy distribution across transformer blocks, EWQ determines which blocks can be safely quantized without causing significant performance degradation, independent of model architecture or size. Our method outperforms uniform quantization approaches, maintaining Massive Multitask Language Understanding (MMLU) accuracy scores within 0.5% of unquantized models while reducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ across multiple architectures-from 1.6B to 70B parameters-showcasing consistent improvements in the quality-compression trade-off regardless of model scale or architectural design. A surprising finding of EWQ is its ability to reduce perplexity compared to unquantized models, suggesting the presence of beneficial regularization through selective precision reduction. This improvement holds across different model families, indicating a fundamental relationship between layer-level entropy and optimal precision requirements. Additionally, we introduce FastEWQ, a rapid method for entropy distribution analysis that eliminates the need for loading model weights. This technique leverages universal characteristics of entropy distribution that persist across various architectures and scales, enabling near-instantaneous quantization decisions while maintaining 80% classification accuracy with full entropy analysis. Our results demonstrate that effective quantization strategies can be developed independently of specific architectural choices or model sizes, opening new possibilities for efficient LLM deployment. </p>
<blockquote>
<p>我们提出了一种基于熵加权量化（EWQ）的选择性模型量化新方法，它超越了针对大型语言模型（LLM）的特定架构和大小依赖压缩方法的局限。通过分析变压器块之间的熵分布，EWQ能够确定哪些块可以安全量化，而不会导致性能显著下降，这独立于模型架构或大小。我们的方法在统一量化方法上表现更好，在保持大规模多任务语言理解（MMLU）准确度得分与未量化模型相差不到0.5%的同时，减少了高达18%的内存使用。我们证明了EWQ在多架构（从1.6B到70B参数）中的有效性，展示了在质量压缩权衡方面的一致改进，无论模型规模或架构设计如何。EWQ的一个意外发现是它能够减少困惑度与未量化模型相比，这表明通过选择性地减少精度，存在有益的正规化。这一改进在不同的模型家族中普遍存在，表明层级熵和最佳精度要求之间存在根本关系。此外，我们引入了FastEWQ，这是一种快速分析熵分布的方法，无需加载模型权重。该技术利用熵分布的通用特性，这些特性在各种架构和规模中持续存在，可以在进行近乎即时的量化决策时保持80%的分类精度，同时进行全面熵分析。我们的结果表明，有效的量化策略可以独立于特定的架构选择或模型大小而发展，为高效部署LLM提供了新的可能性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04704v1">PDF</a> 29 pages, 7 figures, 14 tables; Comments are welcome</p>
<p><strong>摘要</strong></p>
<p>本文提出了一种基于熵加权量化（EWQ）的选择性模型量化新方法，该方法突破了针对大型语言模型（LLM）的架构特定和大小依赖的压缩方法的局限。通过分析变压器块的熵分布，EWQ能够确定哪些块可以在不影响性能显著退化的情况下进行安全量化，独立于模型架构或大小。该方法优于均匀量化方法，在保持大规模多任务语言理解（MMLU）准确率得分与未量化模型相差不到0.5%的同时，减少了高达18%的内存使用。实验证明，EWQ在多个架构（从1.6B到70B参数）中均有效，在模型规模或架构设计方面显示出一致的改进质量和压缩权衡。令人惊讶的是，EWQ能够减少与未量化模型相比的困惑度，这表明通过选择性精度降低存在有益的正规化。这种改进在不同模型家族中普遍存在，表明层级熵与最佳精度要求之间存在根本关系。此外，我们还引入了FastEWQ，这是一种快速分析熵分布的方法，无需加载模型权重。该技术利用熵分布的普遍特征，这些特征在各种架构和规模上持续存在，可以在保持80%分类准确度的同时进行即时量化决策，同时进行全面熵分析。研究结果表明，有效的量化策略可以独立于特定的架构选择或模型大小而发展，为高效部署LLM开辟了新途径。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>提出了一种基于熵加权量化（EWQ）的新型选择性模型量化方法。</li>
<li>通过分析熵分布确定可量化的模型块，实现独立于架构和大小的模型量化。</li>
<li>EWQ在维持MMLU准确率损失极小的情况下，显著降低了内存使用。</li>
<li>EWQ在多个模型和架构上均表现出优异的性能，包括在模型大小和家族方面的改进。</li>
<li>EWQ能减少未量化模型的困惑度，暗示了精度降低带来的潜在正则化效果。</li>
<li>介绍了FastEWQ方法，能快速分析熵分布而无需加载模型权重，实现近即时量化决策。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04704">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2d249d9d7789c1b489d7e5a32aef7fe8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11b8d2c534618d7ab78286680144ce10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afe3a52aa4ff10c566b9bcee71c2f29e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Quantifying-the-Reasoning-Abilities-of-LLMs-on-Real-world-Clinical-Cases"><a href="#Quantifying-the-Reasoning-Abilities-of-LLMs-on-Real-world-Clinical-Cases" class="headerlink" title="Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases"></a>Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases</h2><p><strong>Authors:Pengcheng Qiu, Chaoyi Wu, Shuyu Liu, Weike Zhao, Ya Zhang, Yanfeng Wang, Weidi Xie</strong></p>
<p>The latest reasoning-enhanced large language models (reasoning LLMs), such as DeepSeek-R1 and OpenAI-o3, have demonstrated remarkable success. However, the application of such reasoning enhancements to the highly professional medical domain has not been clearly evaluated, particularly regarding with not only assessing the final generation but also examining the quality of their reasoning processes. In this study, we present MedR-Bench, a reasoning-focused medical evaluation benchmark comprising 1,453 structured patient cases with reasoning references mined from case reports. Our benchmark spans 13 body systems and 10 specialty disorders, encompassing both common and rare diseases. In our evaluation, we introduce a versatile framework consisting of three critical clinical stages: assessment recommendation, diagnostic decision-making, and treatment planning, comprehensively capturing the LLMs’ performance across the entire patient journey in healthcare. For metrics, we propose a novel agentic system, Reasoning Evaluator, designed to automate and objectively quantify free-text reasoning responses in a scalable manner from the perspectives of efficiency, factuality, and completeness by dynamically searching and performing cross-referencing checks. As a result, we assess five state-of-the-art reasoning LLMs, including DeepSeek-R1, OpenAI-o3-mini, and others. Our results reveal that current LLMs can handle relatively simple diagnostic tasks with sufficient critical assessment results, achieving accuracy generally over 85%. However, they still struggle with more complex tasks, such as assessment recommendation and treatment planning. In reasoning, their reasoning processes are generally reliable, with factuality scores exceeding 90%, though they often omit critical reasoning steps. Our study clearly reveals further development directions for current clinical LLMs. </p>
<blockquote>
<p>最新增强推理的大型语言模型（推理LLM），如DeepSeek-R1和OpenAI-o3，已经取得了显著的成果。然而，将这种推理增强应用于高度专业的医疗领域尚未得到明确评估，尤其是不仅要评估最终生成的结果，还要检查其推理过程的质量。在本研究中，我们提出了MedR-Bench，这是一个以推理为重点的医疗评估基准，包含1453个结构化患者病例，以及从病例报告中挖掘的推理参考。我们的基准测试涵盖了13个身体系统和10种专业疾病，包括常见和罕见疾病。在评估中，我们引入了一个通用的框架，包含三个关键临床阶段：评估推荐、诊断决策和治疗计划，全面捕捉LLM在医疗保健的整个患者旅程中的表现。对于指标，我们提出了一种新的代理系统，即推理评估器，旨在以动态搜索和交叉引用的方式自动和客观地量化自由文本推理的响应，从效率、真实性和完整性三个方面进行规模化评估。因此，我们对五款最先进的推理LLM进行了评估，包括DeepSeek-R1、OpenAI-o3-mini等。结果表明，当前LLM可以处理相对简单的诊断任务，并获得足够的批判性评估结果，准确率普遍超过85%。但在更复杂的任务，如评估推荐和治疗计划方面，它们仍然面临挑战。在推理方面，它们的推理过程通常可靠，真实性得分超过90%，尽管它们经常省略关键的推理步骤。我们的研究清楚地指出了当前临床LLM的进一步发展方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04691v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>最新的增强推理大型语言模型（如DeepSeek-R1和OpenAI-o3）已经取得了显著的成果。然而，这些推理增强在高度专业的医疗领域的应用尚未得到明确评估，特别是在评估最终一代的同时，也要考察其推理过程的质量。本研究提出了MedR-Bench，一个以推理为重点的医疗评估基准测试，包含从病例报告中挖掘的1453个结构化患者病例的推理参考。我们的基准测试涵盖了13个身体系统和10种专业疾病，包括常见和罕见疾病。在评估中，我们引入了一个包含三个关键临床阶段的通用框架：评估建议、诊断决策和治疗计划，全面捕捉LLMs在医疗保健中的整个患者旅程中的表现。为了衡量指标，我们提出了一个新的智能系统——推理评估器，旨在自动和客观地以可伸缩的方式从效率、事实和完整性等角度对自由文本推理回答进行量化评估，并进行动态交叉引用检查。结果评估了五种最新推理LLM，包括DeepSeek-R1、OpenAI-o3-mini等。结果表明，当前LLMs可以处理相对简单的诊断任务，并取得超过85%的准确率。然而，它们在更复杂的任务（如评估建议和治疗计划）方面仍存在困难。在推理方面，它们的推理过程通常可靠，事实得分超过90%，但它们经常省略关键的推理步骤。本研究明确了当前临床LLM的发展方向。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>最新推理增强大型语言模型在医疗领域应用尚未充分评估，特别是在诊断、治疗和评估的整个过程中。</li>
<li>MedR-Bench基准测试包含结构化患者病例的推理参考，涵盖多个身体系统和疾病类型。</li>
<li>推理LLMs在简单的诊断任务中表现良好，准确率超过85%，但在复杂的评估建议和治疗计划任务中面临挑战。</li>
<li>推理过程总体可靠，事实得分高，但有时会省略关键推理步骤。</li>
<li>当前LLM在医疗领域的应用仍有进一步发展的空间，需要改进其在复杂任务中的表现和推理过程的完整性。</li>
<li>提出的推理评估工具能够为LLM的评估和改良提供有价值的参考。</li>
<li>结合医疗领域的专业知识和LLM的技术优势，有望为医疗诊断、治疗和评估带来创新。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04691">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-18d32da47588c4de33be991e39cd5cd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f84f980e94d6644142887e9735efd997.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LLM-guided-Plan-and-Retrieval-A-Strategic-Alignment-for-Interpretable-User-Satisfaction-Estimation-in-Dialogue"><a href="#LLM-guided-Plan-and-Retrieval-A-Strategic-Alignment-for-Interpretable-User-Satisfaction-Estimation-in-Dialogue" class="headerlink" title="LLM-guided Plan and Retrieval: A Strategic Alignment for Interpretable   User Satisfaction Estimation in Dialogue"></a>LLM-guided Plan and Retrieval: A Strategic Alignment for Interpretable   User Satisfaction Estimation in Dialogue</h2><p><strong>Authors:Sangyeop Kim, Sohhyung Park, Jaewon Jung, Jinseok Kim, Sungzoon Cho</strong></p>
<p>Understanding user satisfaction with conversational systems, known as User Satisfaction Estimation (USE), is essential for assessing dialogue quality and enhancing user experiences. However, existing methods for USE face challenges due to limited understanding of underlying reasons for user dissatisfaction and the high costs of annotating user intentions. To address these challenges, we propose PRAISE (Plan and Retrieval Alignment for Interpretable Satisfaction Estimation), an interpretable framework for effective user satisfaction prediction. PRAISE operates through three key modules. The Strategy Planner develops strategies, which are natural language criteria for classifying user satisfaction. The Feature Retriever then incorporates knowledge on user satisfaction from Large Language Models (LLMs) and retrieves relevance features from utterances. Finally, the Score Analyzer evaluates strategy predictions and classifies user satisfaction. Experimental results demonstrate that PRAISE achieves state-of-the-art performance on three benchmarks for the USE task. Beyond its superior performance, PRAISE offers additional benefits. It enhances interpretability by providing instance-level explanations through effective alignment of utterances with strategies. Moreover, PRAISE operates more efficiently than existing approaches by eliminating the need for LLMs during the inference phase. </p>
<blockquote>
<p>理解用户对对话系统的满意度，也称为用户满意度估算（USE），对于评估对话质量和提升用户体验至关重要。然而，现有的USE方法面临着挑战，因为它们对导致用户不满的潜在原因理解有限，并且标注用户意图的成本很高。为了解决这些挑战，我们提出了PRAISE（面向可解释满意度估计的计划与检索对齐）框架，这是一个可有效预测用户满意度的可解释框架。PRAISE通过三个主要模块进行操作。策略规划器制定分类用户满意度的自然语言标准。特征检索器然后结合来自大型语言模型（LLM）的用户满意度知识，并从发言中检索相关特征。最后，分数分析器评估策略预测并分类用户满意度。实验结果表明，PRAISE在USE任务的三个基准测试中达到了最新技术水平。除了卓越的性能外，PRAISE还提供了其他优势。它通过有效地将发言与策略对齐，提供了实例级的解释，增强了可解释性。此外，PRAISE在推理阶段不需要使用LLM，因此比现有方法运行得更高效。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04675v1">PDF</a> Accepted by NAACL 2025</p>
<p><strong>Summary</strong></p>
<p>基于对话系统的用户满意度评估对于衡量对话质量和提升用户体验至关重要。现有方法面临用户不满意深层原因理解和标注用户意图的高成本等挑战。为应对这些挑战，本文提出了PRAISE框架，通过策略规划、特征检索和评分分析三个核心模块，实现有效的用户满意度预测。实验结果表明，PRAISE在三个用户满意度评估基准测试中达到了最新技术水平。同时，PRAISE增强了可解释性，通过对对话的策略规划和对策略的有效对齐，提供实例层面的解释。此外，PRAISE在推理阶段不需要大型语言模型，更加高效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>用户满意度评估对于对话系统至关重要。</li>
<li>现有用户满意度评估方法面临深层原因理解和高成本等挑战。</li>
<li>PRAISE框架通过策略规划、特征检索和评分分析三个核心模块实现有效用户满意度预测。</li>
<li>PRAISE框架在三个基准测试中达到了最新技术水平。</li>
<li>PRAISE增强了模型的可解释性，提供实例层面的解释。</li>
<li>PRAISE通过有效对齐策略，提升了用户满意度评估的精确度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04675">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-67929cfc575f0aab400d29f256936957.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a27f016e2c1ac3a1138fb7ddfd69a0fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0eaa91936d1e3b0e562e5366e7e9dbf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2f6ca55850c12403f4da139bad89a12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4bcec0ddb3e5c09f06946dc96232fc83.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b84c6fceeece5308929225cd1e9bf66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4786273dccb6b4ed979fca35d8045251.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8012a9af1350da656ca5baf8860eca3e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Implicit-Cross-Lingual-Rewarding-for-Efficient-Multilingual-Preference-Alignment"><a href="#Implicit-Cross-Lingual-Rewarding-for-Efficient-Multilingual-Preference-Alignment" class="headerlink" title="Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference   Alignment"></a>Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference   Alignment</h2><p><strong>Authors:Wen Yang, Junhong Wu, Chen Wang, Chengqing Zong, Jiajun Zhang</strong></p>
<p>Direct Preference Optimization (DPO) has become a prominent method for aligning Large Language Models (LLMs) with human preferences. While DPO has enabled significant progress in aligning English LLMs, multilingual preference alignment is hampered by data scarcity. To address this, we propose a novel approach that $\textit{captures}$ learned preferences from well-aligned English models by implicit rewards and $\textit{transfers}$ them to other languages through iterative training. Specifically, we derive an implicit reward model from the logits of an English DPO-aligned model and its corresponding reference model. This reward model is then leveraged to annotate preference relations in cross-lingual instruction-following pairs, using English instructions to evaluate multilingual responses. The annotated data is subsequently used for multilingual DPO fine-tuning, facilitating preference knowledge transfer from English to other languages. Fine-tuning Llama3 for two iterations resulted in a 12.72% average improvement in Win Rate and a 5.97% increase in Length Control Win Rate across all training languages on the X-AlpacaEval leaderboard. Our findings demonstrate that leveraging existing English-aligned models can enable efficient and effective multilingual preference alignment, significantly reducing the need for extensive multilingual preference data. The code is available at <a target="_blank" rel="noopener" href="https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding">https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding</a> </p>
<blockquote>
<p>直接偏好优化（DPO）已成为使大型语言模型（LLM）与人类偏好一致的重要方法。虽然DPO在英语LLM的对齐中取得了显著的进步，但多语言偏好对齐却受到数据稀缺的阻碍。为了解决这一问题，我们提出了一种新方法，通过隐性奖励从已对齐的英语模型中捕获学习到的偏好，并通过迭代训练将它们转移到其他语言。具体来说，我们从英语DPO对齐模型及其相应参考模型的逻辑概率中推导出隐性奖励模型。然后，该奖励模型被用于标注跨语言指令对中的偏好关系，使用英语指令来评估多语言响应。随后使用这些标注数据进行多语言DPO微调，促进从英语到其他语言的偏好知识转移。对Llama3进行两次迭代微调后，其在X-AlpacaEval排行榜上所有训练语言的胜率平均提高了12.72%，长度控制胜率提高了5.97%。我们的研究结果表明，利用现有的英语对齐模型可以实现高效的多语言偏好对齐，从而大大减少了对大量多语言偏好数据的需求。代码可在<a target="_blank" rel="noopener" href="https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04647v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>本摘要介绍了Direct Preference Optimization（DPO）方法在多语言环境下的应用挑战，并提出了一种新的解决方案。该方法通过从已对齐的英文模型中捕获学习偏好，并利用迭代训练将其转移到其他语言。具体地，我们从英文DPO对齐模型的logits及其对应的参考模型中推导出隐式奖励模型，用于标注跨语言指令对中的偏好关系。利用英文指令评估多语言响应，并使用标注数据进行多语言DPO微调，实现了从英语到其他语言的偏好知识转移。对Llama3进行两次迭代微调后，在X-AlpacaEval排行榜上平均胜率提高了12.72%，长度控制胜率提高了5.97%。研究结果表明，利用现有英语对齐模型可实现高效的多语言偏好对齐，显著降低对大量多语言偏好数据的需求。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DPO已成为与人类偏好对齐的大型语言模型（LLM）的显著方法，但在多语言环境下存在数据稀缺的挑战。</li>
<li>提出了一种新的方法，通过从已对齐的英文模型中捕获学习偏好，并将其转移到其他语言。</li>
<li>利用隐式奖励模型标注跨语言指令对中的偏好关系，使用英文指令评估多语言响应。</li>
<li>使用标注数据进行多语言DPO微调，实现偏好知识从英语到其他语言的转移。</li>
<li>对Llama3进行微调后，在X-AlpacaEval排行榜上的表现有显著改善，平均胜率提高12.72%，长度控制胜率提高5.97%。</li>
<li>利用现有英语对齐模型可实现多语言偏好对齐，降低对大量多语言偏好数据的需求。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04647">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-700304422a455e7afe041ff75e0050db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-502e6dc05de37c1cb8eef4a9af676c9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23c6cc6ae33513e893716a660240ae42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9983bbb5c61546a7ca149f798d8b5660.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Mark-Your-LLM-Detecting-the-Misuse-of-Open-Source-Large-Language-Models-via-Watermarking"><a href="#Mark-Your-LLM-Detecting-the-Misuse-of-Open-Source-Large-Language-Models-via-Watermarking" class="headerlink" title="Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models   via Watermarking"></a>Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models   via Watermarking</h2><p><strong>Authors:Yijie Xu, Aiwei Liu, Xuming Hu, Lijie Wen, Hui Xiong</strong></p>
<p>As open-source large language models (LLMs) like Llama3 become more capable, it is crucial to develop watermarking techniques to detect their potential misuse. Existing watermarking methods either add watermarks during LLM inference, which is unsuitable for open-source LLMs, or primarily target classification LLMs rather than recent generative LLMs. Adapting these watermarks to open-source LLMs for misuse detection remains an open challenge. This work defines two misuse scenarios for open-source LLMs: intellectual property (IP) violation and LLM Usage Violation. Then, we explore the application of inference-time watermark distillation and backdoor watermarking in these contexts. We propose comprehensive evaluation methods to assess the impact of various real-world further fine-tuning scenarios on watermarks and the effect of these watermarks on LLM performance. Our experiments reveal that backdoor watermarking could effectively detect IP Violation, while inference-time watermark distillation is applicable in both scenarios but less robust to further fine-tuning and has a more significant impact on LLM performance compared to backdoor watermarking. Exploring more advanced watermarking methods for open-source LLMs to detect their misuse should be an important future direction. </p>
<blockquote>
<p>随着像Llama3这样的开源大型语言模型（LLM）的能力越来越强，开发水印技术来检测其潜在滥用情况变得至关重要。现有的水印方法要么在LLM推理过程中添加水印，这不适用于开源LLM，要么主要针对分类LLM而非最新的生成式LLM。将这些水印适应于开源LLM以进行滥用检测仍然是一个开放性的挑战。本文定义了开源LLM的两种滥用场景：知识产权（IP）侵犯和LLM使用违规。然后，我们探讨了推理时间水印蒸馏和后门水印在这些上下文中的应用。我们提出了全面的评估方法，以评估各种现实世界中的进一步微调场景对水印的影响以及这些水印对LLM性能的影响。我们的实验表明，后门水印技术可以有效检测IP侵犯，而推理时间水印蒸馏在两种场景中都可应用，但对抗进一步的微调能力较弱，并且对LLM性能的影响比后门水印技术更为显著。探索更先进的水印方法用于开源LLM以检测其滥用情况，应成为未来重要的研究方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04636v1">PDF</a> Accepted by the 1st Workshop on GenAI Watermarking, collocated with   ICLR 2025</p>
<p><strong>摘要</strong></p>
<p>随着开源大型语言模型（LLM）如Llama3的能力不断增强，开发水印技术来检测其潜在误用至关重要。现有水印方法要么在LLM推理过程中添加水印，这不适用于开源LLM，要么主要针对分类LLM而非最新的生成式LLM。将这些水印适应于开源LLM以进行误检测仍是一项挑战。本文定义了开源LLM的两种误用场景：知识产权（IP）侵犯和LLM使用违规。然后，我们探索了推理时间水印蒸馏和后门水印在这些场景中的应用。我们提出了全面的评估方法来评估各种现实世界进一步的微调场景对水印的影响以及这些水印对LLM性能的影响。实验表明，后门水印技术能有效检测IP侵犯，而推理时间水印蒸馏在两种场景中均可应用，但对进一步微调不太稳健，并且对LLM性能的影响比后门水印技术更大。探索更先进的水印方法用于开源LLM以检测其误用是一个重要的未来方向。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>知识产权（IP）侵犯和LLM使用违规是开源LLM的两种主要误用场景。</li>
<li>后门水印技术能有效检测IP侵犯。</li>
<li>推理时间水印蒸馏适用于两种误用场景，但对进一步微调不太稳健。</li>
<li>推理时间水印蒸馏对LLM性能的影响大于后门水印技术。</li>
<li>当前水印技术面临适应开源LLM的挑战，需要进一步研究和改进。</li>
<li>水印技术对于检测LLM误用具有重要意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04636">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b91da5e7ec3578e7243beeb0f082961e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ed1d42d622fa93219583cf09e35b7a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-536f70c9b09407a462b773c879cad08b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f621152c7bcbdb31001dbeb01e58dc4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Better-Process-Supervision-with-Bi-directional-Rewarding-Signals"><a href="#Better-Process-Supervision-with-Bi-directional-Rewarding-Signals" class="headerlink" title="Better Process Supervision with Bi-directional Rewarding Signals"></a>Better Process Supervision with Bi-directional Rewarding Signals</h2><p><strong>Authors:Wenxiang Chen, Wei He, Zhiheng Xi, Honglin Guo, Boyang Hong, Jiazheng Zhang, Rui Zheng, Nijun Li, Tao Gui, Yun Li, Qi Zhang, Xuanjing Huang</strong></p>
<p>Process supervision, i.e., evaluating each step, is critical for complex large language model (LLM) reasoning and test-time searching with increased inference compute. Existing approaches, represented by process reward models (PRMs), primarily focus on rewarding signals up to the current step, exhibiting a one-directional nature and lacking a mechanism to model the distance to the final target. To address this problem, we draw inspiration from the A* algorithm, which states that an effective supervisory signal should simultaneously consider the incurred cost and the estimated cost for reaching the target. Building on this key insight, we introduce BiRM, a novel process supervision model that not only evaluates the correctness of previous steps but also models the probability of future success. We conduct extensive experiments on mathematical reasoning tasks and demonstrate that BiRM provides more precise evaluations of LLM reasoning steps, achieving an improvement of 3.1% on Gaokao2023 over PRM under the Best-of-N sampling method. Besides, in search-based strategies, BiRM provides more comprehensive guidance and outperforms ORM by 5.0% and PRM by 3.8% respectively on MATH-500. </p>
<blockquote>
<p>过程监督，即评估每一步，对于复杂的大型语言模型（LLM）推理和测试时的搜索以及增加的推理计算来说至关重要。现有方法以过程奖励模型（PRM）为代表，主要关注当前步骤的奖励信号，呈现出单向性，缺乏模拟最终目标与当前状态之间距离的机制。为了解决这个问题，我们从A*算法中汲取灵感，该算法指出有效的监督信号应同时考虑产生的成本和到达目标的预估成本。基于这一关键见解，我们引入了BiRM，这是一种新型的过程监督模型，它不仅评估先前步骤的正确性，还模拟未来成功的概率。我们在数学推理任务上进行了大量实验，证明BiRM对LLM推理步骤的评价更为精确，在最佳N采样方法下，相对于PRM在Gaokao2023上的改进率为3.1%。此外，在基于搜索的策略中，BiRM提供更全面的指导，并在MATH-500上相对于ORM和PRM分别提高了5.0%和3.8%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04618v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于A*算法的理念，文章提出了BiRM，一种新型的过程监督模型。该模型不仅能评估先前步骤的正确性，还能预测未来的成功概率。在复杂的数学推理任务中，BiRM比传统的过程奖励模型表现出更高的性能。特别是在高考数学和MATH-500的搜索策略测试中，BiRM显著提升了准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>文章强调了过程监督在大型语言模型（LLM）推理中的重要性，特别是在增加推理计算的情况下。</li>
<li>现有方法主要关注当前步骤的奖励信号，缺乏对未来目标的建模机制。</li>
<li>BiRM模型结合了A*算法的理念，同时考虑已产生的成本和到达目标的预估成本来提供有效的监督信号。</li>
<li>BiRM不仅评估先前步骤的正确性，还预测未来的成功概率。</li>
<li>在数学推理任务上，BiRM显著优于传统的PRM模型，特别是采用Best-of-N采样方法的测试显示，在Gaokao2023上BiRM较PRM有3.1%的提升。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04618">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-30b27ae87e01dfbca5ce817601f04067.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19cfaaf1814dbc5b05292775c379478e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1718239711f8c0cf50ebd7c12bfbb8d3.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="HalluCounter-Reference-free-LLM-Hallucination-Detection-in-the-Wild"><a href="#HalluCounter-Reference-free-LLM-Hallucination-Detection-in-the-Wild" class="headerlink" title="HalluCounter: Reference-free LLM Hallucination Detection in the Wild!"></a>HalluCounter: Reference-free LLM Hallucination Detection in the Wild!</h2><p><strong>Authors:Ashok Urlana, Gopichand Kanumolu, Charaka Vinayak Kumar, Bala Mallikarjunarao Garlapati, Rahul Mishra</strong></p>
<p>Response consistency-based, reference-free hallucination detection (RFHD) methods do not depend on internal model states, such as generation probabilities or gradients, which Grey-box models typically rely on but are inaccessible in closed-source LLMs. However, their inability to capture query-response alignment patterns often results in lower detection accuracy. Additionally, the lack of large-scale benchmark datasets spanning diverse domains remains a challenge, as most existing datasets are limited in size and scope. To this end, we propose HalluCounter, a novel reference-free hallucination detection method that utilizes both response-response and query-response consistency and alignment patterns. This enables the training of a classifier that detects hallucinations and provides a confidence score and an optimal response for user queries. Furthermore, we introduce HalluCounterEval, a benchmark dataset comprising both synthetically generated and human-curated samples across multiple domains. Our method outperforms state-of-the-art approaches by a significant margin, achieving over 90% average confidence in hallucination detection across datasets. </p>
<blockquote>
<p>基于响应一致性的无参考幻觉检测（RFHD）方法不依赖于内部模型状态，如生成概率或梯度，而灰盒模型通常依赖于这些在封闭源代码的LLM中不可访问的信息。然而，它们无法捕捉查询-响应对齐模式，这往往导致检测准确率降低。此外，缺乏涵盖多个领域的大规模基准数据集仍然是一个挑战，因为大多数现有数据集在规模和范围上都是有限的。为此，我们提出了HalluCounter，这是一种新型的无参考幻觉检测方法，它利用响应-响应和查询-响应的一致性和对齐模式。这能够训练一个分类器来检测幻觉，并提供用户查询的信心评分和最佳响应。此外，我们还介绍了HalluCounterEval，这是一个基准数据集，包含多个领域的人工合成和人工精选样本。我们的方法在多个数据集上的幻觉检测平均置信度超过99%，显著优于现有技术方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04615v1">PDF</a> 30 pages, 4 figures</p>
<p><strong>Summary</strong><br>大模型响应一致性无参考的幻觉检测（RFHD）方法不依赖于内部模型状态，如生成概率或梯度等，但无法捕捉查询响应对齐模式，导致检测精度较低。为解决此问题，我们提出HalluCounter方法，利用响应间一致性及查询响应对齐模式进行幻觉检测，并引入HalluCounterEval基准数据集。该方法显著优于现有技术，在多个数据集上的幻觉检测平均置信度超过90%。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>RFHD方法不依赖内部模型状态，如生成概率或梯度。</li>
<li>现有RFHD方法因无法捕捉查询响应对齐模式而导致检测精度较低。</li>
<li>HalluCounter是一种新的RFHD方法，利用响应间一致性及查询响应对齐模式。</li>
<li>HalluCounterEval基准数据集包含合成和人类精选样本，覆盖多个领域。</li>
<li>HalluCounter方法显著优于现有技术。</li>
<li>HalluCounter在多个数据集上的幻觉检测平均置信度超过90%。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04615">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d3cde69c878f090f1a1709fec005f0dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3e7864e39e04def0662bd5ebfb3ab414.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a747c051bff61dd8f9e8ff17bdf55190.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90e406a6701caf42861ff7a06e0c9c41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-932c6fc2ac71a59c9527efbddcfb6511.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f4ee130b802e8ba53e6273ef69a755b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-197c992ed3ff5054b792713a53d5a6a0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="HybridNorm-Towards-Stable-and-Efficient-Transformer-Training-via-Hybrid-Normalization"><a href="#HybridNorm-Towards-Stable-and-Efficient-Transformer-Training-via-Hybrid-Normalization" class="headerlink" title="HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid   Normalization"></a>HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid   Normalization</h2><p><strong>Authors:Zhijian Zhuo, Yutao Zeng, Ya Wang, Sijun Zhang, Jian Yang, Xiaoqing Li, Xun Zhou, Jinwen Ma</strong></p>
<p>Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, challenges remain in training deep transformer networks, especially regarding the location of layer normalization. While Pre-Norm structures facilitate easier training due to their more prominent identity path, they often yield suboptimal performance compared to Post-Norm. In this paper, we propose $\textbf{HybridNorm}$, a straightforward yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. This design not only stabilizes training but also enhances performance, particularly in the context of LLMs. Comprehensive experiments in both dense and sparse architectures show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches, achieving state-of-the-art results across various benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. %Code will be made publicly available. Code is available at <a target="_blank" rel="noopener" href="https://github.com/BryceZhuo/HybridNorm">https://github.com/BryceZhuo/HybridNorm</a>. </p>
<blockquote>
<p>Transformer架构在众多机器学习任务中已成为默认选择，特别是在大型语言模型（LLM）中。尽管它们表现出色，但在训练深度Transformer网络时仍面临挑战，尤其是在层归一化的位置问题上。由于Pre-Norm结构具有更突出的身份路径，从而便于训练，但其性能往往不如Post-Norm。在本文中，我们提出了一种名为HybridNorm的混合归一化策略，这是一种简单有效的混合策略，结合了Pre-Norm和Post-Norm方法各自的优点。具体来说，HybridNorm在注意力机制中采用QKV归一化，并在每个Transformer块的前馈网络（FFN）中使用Post-Norm。这种设计不仅稳定了训练过程，还提高了性能，特别是在大型语言模型的背景下。在密集和稀疏架构中的综合实验表明，HybridNorm始终优于Pre-Norm和Post-Norm方法，在各种基准测试中取得了最新成果。这些发现突显了HybridNorm作为更稳定、更有效的技术，有潜力改进深度Transformer模型的训练性能。代码将在<a target="_blank" rel="noopener" href="https://github.com/BryceZhuo/HybridNorm">https://github.com/BryceZhuo/HybridNorm</a>上公开提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04598v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为HybridNorm的混合归一化策略，结合了Pre-Norm和Post-Norm的优势。该策略在注意力机制中采用QKV归一化，并在每个transformer块的feed-forward网络（FFN）中使用Post-Norm。这种设计不仅稳定训练，还提高了性能，特别是在大型语言模型（LLMs）中。实验表明，HybridNorm在密集和稀疏架构中都表现优秀，相较于Pre-Norm和Post-Norm有更好的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformers已成为多种机器学习任务的默认架构，特别是在大型语言模型（LLMs）中。</li>
<li>尽管Transformer表现优异，但在训练深度网络时仍面临挑战，关于层归一化的位置是一个关键问题。</li>
<li>Pre-Norm结构因更突出的身份路径而易于训练，但性能往往不如Post-Norm。</li>
<li>本文提出HybridNorm，一个结合Pre-Norm和Post-Norm优势的混合归一化策略。</li>
<li>HybridNorm在注意力机制中采用QKV归一化，并在FFN中使用Post-Norm。</li>
<li>HybridNorm不仅稳定训练，还提高性能，特别是在LLMs中。</li>
<li>实验表明，HybridNorm在多种基准测试中表现最佳，具有更稳定和有效的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04598">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-662e63eba070bcd8ae199df61eca965a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38768c5a780053b49471795c3bc0d1bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-265decdf333aa45e77cfaae4f052a9f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ac5a213858b29cdb5ec390d5578e1e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d6432e672fb670e7d62dc6f17abe9430.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="RetinalGPT-A-Retinal-Clinical-Preference-Conversational-Assistant-Powered-by-Large-Vision-Language-Models"><a href="#RetinalGPT-A-Retinal-Clinical-Preference-Conversational-Assistant-Powered-by-Large-Vision-Language-Models" class="headerlink" title="RetinalGPT: A Retinal Clinical Preference Conversational Assistant   Powered by Large Vision-Language Models"></a>RetinalGPT: A Retinal Clinical Preference Conversational Assistant   Powered by Large Vision-Language Models</h2><p><strong>Authors:Wenhui Zhu, Xin Li, Xiwen Chen, Peijie Qiu, Vamsi Krishna Vasa, Xuanzhao Dong, Yanxi Chen, Natasha Lepore, Oana Dumitrascu, Yi Su, Yalin Wang</strong></p>
<p>Recently, Multimodal Large Language Models (MLLMs) have gained significant attention for their remarkable ability to process and analyze non-textual data, such as images, videos, and audio. Notably, several adaptations of general-domain MLLMs to the medical field have been explored, including LLaVA-Med. However, these medical adaptations remain insufficiently advanced in understanding and interpreting retinal images. In contrast, medical experts emphasize the importance of quantitative analyses for disease detection and interpretation. This underscores a gap between general-domain and medical-domain MLLMs: while general-domain MLLMs excel in broad applications, they lack the specialized knowledge necessary for precise diagnostic and interpretative tasks in the medical field. To address these challenges, we introduce \textit{RetinalGPT}, a multimodal conversational assistant for clinically preferred quantitative analysis of retinal images. Specifically, we achieve this by compiling a large retinal image dataset, developing a novel data pipeline, and employing customized visual instruction tuning to enhance both retinal analysis and enrich medical knowledge. In particular, RetinalGPT outperforms MLLM in the generic domain by a large margin in the diagnosis of retinal diseases in 8 benchmark retinal datasets. Beyond disease diagnosis, RetinalGPT features quantitative analyses and lesion localization, representing a pioneering step in leveraging LLMs for an interpretable and end-to-end clinical research framework. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Retinal-Research/RetinalGPT">https://github.com/Retinal-Research/RetinalGPT</a> </p>
<blockquote>
<p>近期，多模态大型语言模型（MLLMs）因其处理和分析非文本数据（如图像、视频和音频）的显著能力而受到广泛关注。值得注意的是，已经探索了将通用领域的MLLMs适应于医疗领域的几个适应版本，包括LLaVA-Med。然而，这些医疗适应版本在理解和解释视网膜图像方面仍然存在不足。相比之下，医学专家强调定量分析在疾病检测和解释中的重要性。这突出了通用领域和医疗领域MLLMs之间的鸿沟：虽然通用领域的MLLMs在广泛应用中表现出色，但它们缺乏医疗领域中精确诊断和解释任务所需的专业知识。为了解决这些挑战，我们引入了RetinalGPT，这是一个多模态对话助手，用于临床上首选的视网膜图像定量分析。具体来说，我们通过编译大规模的视网膜图像数据集、开发新型数据管道和采用定制的视觉指令调整来实现这一点，以增强视网膜分析和丰富医学知识。特别是在8个基准视网膜数据集中，RetinalGPT在视网膜疾病的诊断方面大大超过了通用领域的MLLM。除了疾病诊断外，RetinalGPT还具备定量分析和病灶定位功能，代表了在利用LLMs进行可解释和端到端的临床研究框架方面的开创性步骤。相关代码可通过<a target="_blank" rel="noopener" href="https://github.com/Retinal-Research/RetinalGPT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Retinal-Research/RetinalGPT获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03987v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对视网膜图像进行定量分析的多媒体对话助手RetinalGPT。通过构建大型视网膜图像数据集、开发新型数据流程管道和定制视觉指令调整，RetinalGPT提高了视网膜分析的能力并丰富了医学知识。相较于通用领域的多模态大型语言模型，RetinalGPT在视网膜疾病的诊断上大幅超越，不仅限于疾病诊断，还具备定量分析和病灶定位功能，为临床研究和诊断提供了一个可解释、端到端的框架。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）能处理非文本数据，如图像、视频和音频。</li>
<li>MLLMs在医疗领域的应用仍面临理解和解释视网膜图像方面的挑战。</li>
<li>医学专家强调定量分析法在疾病检测和解读中的重要性。</li>
<li>RetinalGPT是一个多媒体对话助手，专为临床所需的视网膜图像定量分析设计。</li>
<li>RetinalGPT通过构建大型视网膜图像数据集、开发数据流程管道和定制视觉指令调整来增强性能。</li>
<li>RetinalGPT在视网膜疾病诊断上大幅超越通用领域的MLLMs。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03987">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-feaa19088fdb1259fe63852144c9eac6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-085d87585a318a307c4f68800a95faec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42d2a468956c315689d3dab658a5884b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="RAAD-LLM-Adaptive-Anomaly-Detection-Using-LLMs-and-RAG-Integration"><a href="#RAAD-LLM-Adaptive-Anomaly-Detection-Using-LLMs-and-RAG-Integration" class="headerlink" title="RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration"></a>RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration</h2><p><strong>Authors:Alicia Russell-Gilbert, Sudip Mittal, Shahram Rahimi, Maria Seale, Joseph Jabour, Thomas Arnold, Joshua Church</strong></p>
<p>Anomaly detection in complex industrial environments poses unique challenges, particularly in contexts characterized by data sparsity and evolving operational conditions. Predictive maintenance (PdM) in such settings demands methodologies that are adaptive, transferable, and capable of integrating domain-specific knowledge. In this paper, we present RAAD-LLM, a novel framework for adaptive anomaly detection, leveraging large language models (LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach addresses the aforementioned PdM challenges. By effectively utilizing domain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time series data without requiring fine-tuning on specific datasets. The framework’s adaptability mechanism enables it to adjust its understanding of normal operating conditions dynamically, thus increasing detection accuracy. We validate this methodology through a real-world application for a plastics manufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show significant improvements over our previous model with an accuracy increase from 70.7% to 89.1% on the real-world dataset. By allowing for the enriching of input series data with semantics, RAAD-LLM incorporates multimodal capabilities that facilitate more collaborative decision-making between the model and plant operators. Overall, our findings support RAAD-LLM’s ability to revolutionize anomaly detection methodologies in PdM, potentially leading to a paradigm shift in how anomaly detection is implemented across various industries. </p>
<blockquote>
<p>在复杂的工业环境中进行异常检测面临着独特的挑战，特别是在数据稀疏和运营条件不断变化的情况下。此类环境中的预测性维护（PdM）需要具有适应性、可移植性并能够整合特定领域知识的方法。在本文中，我们提出了RAAD-LLM，这是一种自适应异常检测的新框架，它利用大型语言模型（LLM）与检索增强生成（RAG）相结合。该方法解决了上述PdM挑战。通过有效利用特定领域的知识，RAAD-LLM增强了时间序列数据中的异常检测，而无需在特定数据集上进行微调。该框架的适应性机制使其能够动态地调整对正常操作条件的理解，从而提高检测准确性。我们通过塑料制造厂的实际应用和Skoltech异常基准测试（SKAB）验证了该方法的有效性。结果显示，与之前的模型相比，我们在现实数据集上的准确率从70.7%提高到了89.1%。通过允许用语义信息丰富输入序列数据，RAAD-LLM结合了多模式功能，促进了模型和工厂操作员之间的更协作决策。总的来说，我们的研究支持RAAD-LLM在PdM中异常检测方法的革命性变革，有可能导致各行业异常检测实施方式的范式转变。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02800v2">PDF</a> arXiv admin note: substantial text overlap with arXiv:2411.00914</p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的RAAD框架能有效应对复杂工业环境中的异常检测挑战。特别是面临数据稀疏和多变操作条件时，该方法利用领域特定知识提升了时间序列数据中的异常检测性能。该框架的动态适应机制能够实时调整正常操作条件的理解，从而提高检测准确性。其在塑料制造工厂和Skoltech异常基准测试（SKAB）中的实际应用验证表明了显著的效果提升。RAAD框架结合了检索增强生成（RAG）技术，增强了输入序列数据的语义内涵，引入了多模式能力，为模型和工厂操作者之间提供了更协作的决策支持。整体来看，该研究表明RAAD框架能够引领工业领域异常检测方法的革命性变革。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>RAAD-LLM框架结合了大型语言模型（LLM）和检索增强生成（RAG）技术，用于自适应异常检测。</li>
<li>该框架能有效应对数据稀疏和多变操作条件下的异常检测挑战。</li>
<li>RAAD-LLM通过利用领域特定知识提升了时间序列数据中的异常检测性能。</li>
<li>动态适应机制能实时调整正常操作条件的理解，提高检测准确性。</li>
<li>在塑料制造工厂和Skoltech异常基准测试中的实际应用验证表明了显著的效果提升。</li>
<li>与先前的模型相比，RAAD-LLM的准确性从70.7%提高到了89.1%。</li>
<li>该框架引入了多模式能力，为模型和工厂操作者之间提供了更协作的决策支持。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02800">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-25c35bdd9189e45412ce35818f0bab90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a10eba53167cf88dc695912d62a98e3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8df499ffca869658fc8861b56ff84458.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Union-of-Experts-Adapting-Hierarchical-Routing-to-Equivalently-Decomposed-Transformer"><a href="#Union-of-Experts-Adapting-Hierarchical-Routing-to-Equivalently-Decomposed-Transformer" class="headerlink" title="Union of Experts: Adapting Hierarchical Routing to Equivalently   Decomposed Transformer"></a>Union of Experts: Adapting Hierarchical Routing to Equivalently   Decomposed Transformer</h2><p><strong>Authors:Yujiao Yang, Jing Lian, Linhui Li</strong></p>
<p>We propose Union-of-Experts (UoE), which decomposes transformer into an equitant group of experts, and then implement selective routing on input data and experts. Our approach advances MoE design with four key innovations: (1) We conducted equitant expert decomposition on both MLP blocks and attention blocks based on matrix partition in tensor parallelism. (2) We developed two routing paradigms: patch-wise data selection and expert selection, to apply routing across different levels. (3) We design the architecture of UoE model, including Selective Multi-Head Attention (SMHA) and Union-of-MLP-Experts (UoME). (4) We develop parallel implementation of UoE’s routing and computation operation, and optimize efficiency based on the hardware processing analysis. The experiments demonstrate that the UoE model surpass Full Attention, state-of-art MoEs and efficient transformers (including the model architecture of recently proposed DeepSeek-V3) in several tasks across image and natural language domains. In language modeling tasks, we achieve an average reduction of 2.38 in perplexity compared to the best-performed MoE method with an average of 76% FLOPs. In Long Range Arena benchmark, we recorded an average score that is at least 0.68% higher than all comparison models including Full Attention, MoEs, and transformer variants, with only 50% FLOPs of the best MoE method. In image classification, our model yielded an average accuracy improvement of 1.75% than the best model while maintaining comparable FLOPs. The source codes are available at <a target="_blank" rel="noopener" href="https://github.com/YujiaoYang-work/UoE">https://github.com/YujiaoYang-work/UoE</a>. </p>
<blockquote>
<p>我们提出了专家联盟（UoE）的方法，它将变压器分解为等价的专家组，然后在输入数据和专家之间实现选择性路由。我们的方法以四项关键创新推动了MoE设计的发展：（1）我们在MLP块和注意力块上进行了等价的专家分解，这是基于张量并行性的矩阵分区。（2）我们开发了两种路由范式：基于补丁的数据选择和专家选择，以应用于不同的层次。（3）我们设计了UoE模型的架构，包括选择性多头注意力（SMHA）和联盟MLP专家（UoME）。（4）我们并行实现了UoE的路由和计算操作，并基于硬件处理分析进行了效率优化。实验表明，UoE模型在图像和自然语言领域的多个任务中超越了全注意力、先进的MoE和高效变压器（包括最近提出的DeepSeek-V3的模型架构）。在语言建模任务中，与性能最佳的MoE方法相比，我们的模型将困惑度平均降低了2.38，平均浮点运算减少了76%。在Long Range Arena基准测试中，我们的模型平均得分至少高于所有对比模型（包括全注意力、MoE和变压器变体），且只使用了最佳MoE方法一半的浮点运算。在图像分类方面，我们的模型在保持相当浮点运算量的情况下，平均准确率比最佳模型提高了1.75%。源代码可在<a target="_blank" rel="noopener" href="https://github.com/YujiaoYang-work/UoE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YujiaoYang-work/UoE找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02495v2">PDF</a> 17 pages</p>
<p><strong>Summary</strong></p>
<p>本文提出了Union-of-Experts（UoE）模型，它将Transformer分解为等效的专家组，并对输入数据和专家进行选择性路由。该模型有四个关键创新点：一是基于张量并行性的矩阵分区，对MLP块和注意力块进行等效专家分解；二是开发两种路由范式：跨不同层次的斑块式数据选择和专家选择；三是设计UoE模型架构，包括选择性多头注意力和联合的MLP专家；四是实现UoE的路由和计算操作的并行实现，并基于硬件处理分析优化效率。实验表明，UoE模型在图像和自然语言领域的多个任务中超越了全注意力、状态最优的MoE和高效的Transformer（包括最近提出的DeepSeek-V3模型架构）。在语言建模任务中，与表现最佳的MoE方法相比，我们的模型将困惑度平均降低了2.38。在Long Range Arena基准测试中，我们的模型平均得分至少高出所有对比模型（包括全注意力、MoE和Transformer变体），且仅使用最佳MoE方法的50%的浮点运算量。在图像分类任务中，我们的模型平均精度比最佳模型提高了1.75%，同时保持相当的浮点运算量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UoE模型将Transformer分解为等效专家组，实现选择性路由。</li>
<li>UoE模型在MLP块和注意力块上进行了基于矩阵分区的专家分解。</li>
<li>开发了两种路由范式：斑块式数据选择和专家选择，应用于不同层级。</li>
<li>UoE模型架构设计包括选择性多头注意力和联合的MLP专家。</li>
<li>UoE模型实现了路由和计算操作的并行处理，并优化了硬件处理效率。</li>
<li>在语言建模、Long Range Arena基准测试和图像分类等任务中，UoE模型表现超越现有方法。</li>
<li>UoE模型的源代码已公开发布。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02495">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-19cf89005b183c08b0106d0ad6d1bf2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19bba9e71a1326b4bda7584d14faac17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8be494f32600280076484d34d638802.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11699562dda5bc611776b8c0548bc2be.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Protein-Large-Language-Models-A-Comprehensive-Survey"><a href="#Protein-Large-Language-Models-A-Comprehensive-Survey" class="headerlink" title="Protein Large Language Models: A Comprehensive Survey"></a>Protein Large Language Models: A Comprehensive Survey</h2><p><strong>Authors:Yijia Xiao, Wanjia Zhao, Junkai Zhang, Yiqiao Jin, Han Zhang, Zhicheng Ren, Renliang Sun, Haixin Wang, Guancheng Wan, Pan Lu, Xiao Luo, Yu Zhang, James Zou, Yizhou Sun, Wei Wang</strong></p>
<p>Protein-specific large language models (Protein LLMs) are revolutionizing protein science by enabling more efficient protein structure prediction, function annotation, and design. While existing surveys focus on specific aspects or applications, this work provides the first comprehensive overview of Protein LLMs, covering their architectures, training datasets, evaluation metrics, and diverse applications. Through a systematic analysis of over 100 articles, we propose a structured taxonomy of state-of-the-art Protein LLMs, analyze how they leverage large-scale protein sequence data for improved accuracy, and explore their potential in advancing protein engineering and biomedical research. Additionally, we discuss key challenges and future directions, positioning Protein LLMs as essential tools for scientific discovery in protein science. Resources are maintained at <a target="_blank" rel="noopener" href="https://github.com/Yijia-Xiao/Protein-LLM-Survey">https://github.com/Yijia-Xiao/Protein-LLM-Survey</a>. </p>
<blockquote>
<p>蛋白质特异性大型语言模型（Protein LLMs）通过实现更高效的蛋白质结构预测、功能注释和设计，正在推动蛋白质科学的革命。虽然现有的调查侧重于特定方面或应用，但这项工作提供了对Protein LLMs的首个全面概述，涵盖了其架构、训练数据集、评估指标和多样化的应用。通过对超过100篇文章的系统分析，我们提出了最新Protein LLMs的结构分类法，分析了它们如何利用大规模蛋白质序列数据提高准确性，并探索了它们在推动蛋白质工程和生物医学研究方面的潜力。此外，我们还讨论了关键挑战和未来发展方向，将Protein LLMs定位为蛋白质科学研究中不可或缺的发现工具。相关资源可访问：<a target="_blank" rel="noopener" href="https://github.com/Yijia-Xiao/Protein-LLM-Survey%E3%80%82">https://github.com/Yijia-Xiao/Protein-LLM-Survey。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.17504v2">PDF</a> 24 pages, 4 figures, 5 tables</p>
<p><strong>Summary</strong></p>
<p>蛋白质特定的大型语言模型（Protein LLMs）正在通过更高效的蛋白质结构预测、功能注释和设计，推动蛋白质科学的革命。本文首次全面概述了Protein LLMs，详细介绍了其架构、训练数据集、评估指标和多种应用。通过对100多篇文章的的系统分析，提出了先进的Protein LLMs的结构化分类，分析了它们如何利用大规模蛋白质序列数据提高准确性，并探讨了它们在推进蛋白质工程和生物医学研究方面的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>蛋白质特定的大型语言模型（Protein LLMs）在蛋白质科学领域发挥着重要作用，能够提高蛋白质结构预测、功能注释和设计的效率。</li>
<li>现有调查主要关注Protein LLMs的特定方面或应用，而本文首次提供了全面的综述，涵盖了架构、训练数据集、评估指标和多种应用。</li>
<li>通过系统分析100多篇相关论文，提出了一种新的Protein LLMs的结构化分类方法。</li>
<li>Protein LLMs利用大规模蛋白质序列数据提高准确性，展示了其在蛋白质工程和生物医学研究中的潜力。</li>
<li>该研究还讨论了Protein LLMs面临的关键挑战和未来发展方向。</li>
<li>Protein LLMs已成为蛋白质科学中重要的科学发现工具。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.17504">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d58eba1da1a2db728155ba22ee0fd560.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-342ae809c9ea19ca9cc57ddb06cdc83d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18c17ab1e83fae6b495b6430c248c8f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13a9af5bb456551e288dc95bfeeaf34f.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="AdaptBot-Combining-LLM-with-Knowledge-Graphs-and-Human-Input-for-Generic-to-Specific-Task-Decomposition-and-Knowledge-Refinement"><a href="#AdaptBot-Combining-LLM-with-Knowledge-Graphs-and-Human-Input-for-Generic-to-Specific-Task-Decomposition-and-Knowledge-Refinement" class="headerlink" title="AdaptBot: Combining LLM with Knowledge Graphs and Human Input for   Generic-to-Specific Task Decomposition and Knowledge Refinement"></a>AdaptBot: Combining LLM with Knowledge Graphs and Human Input for   Generic-to-Specific Task Decomposition and Knowledge Refinement</h2><p><strong>Authors:Shivam Singh, Karthik Swaminathan, Nabanita Dash, Ramandeep Singh, Snehasis Banerjee, Mohan Sridharan, Madhava Krishna</strong></p>
<p>An embodied agent assisting humans is often asked to complete new tasks, and there may not be sufficient time or labeled examples to train the agent to perform these new tasks. Large Language Models (LLMs) trained on considerable knowledge across many domains can be used to predict a sequence of abstract actions for completing such tasks, although the agent may not be able to execute this sequence due to task-, agent-, or domain-specific constraints. Our framework addresses these challenges by leveraging the generic predictions provided by LLM and the prior domain knowledge encoded in a Knowledge Graph (KG), enabling an agent to quickly adapt to new tasks. The robot also solicits and uses human input as needed to refine its existing knowledge. Based on experimental evaluation in the context of cooking and cleaning tasks in simulation domains, we demonstrate that the interplay between LLM, KG, and human input leads to substantial performance gains compared with just using the LLM. Project website{\S}: <a target="_blank" rel="noopener" href="https://sssshivvvv.github.io/adaptbot/">https://sssshivvvv.github.io/adaptbot/</a> </p>
<blockquote>
<p>一个实体代理辅助人类经常需要完成新的任务，并且可能没有足够的时间或标记示例来训练代理执行这些新任务。虽然代理可能由于任务、代理或领域特定的约束而无法执行此序列，但是基于多域广泛知识的大型语言模型（LLM）可用于预测完成这些任务的一系列抽象动作。我们的框架通过利用LLM提供的通用预测和编码在知识图谱（KG）中的先验领域知识来解决这些挑战，从而使代理能够迅速适应新任务。机器人还根据需要征求并使用人类输入来完善其现有知识。基于模拟领域的烹饪和清洁任务的实验评估，我们证明了LLM、KG和人类输入之间的相互作用与仅使用LLM相比，可以带来显著的性能提升。项目网站：<a target="_blank" rel="noopener" href="https://sssshivvvv.github.io/adaptbot/">https://sssshivvvv.github.io/adaptbot/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.02067v2">PDF</a> Accepted to IEEE International Conference on Robotics and Automation   (ICRA) 2025</p>
<p><strong>Summary</strong><br>基于大型语言模型（LLM）和知识图谱（KG）的框架，通过利用通用预测和先验领域知识，使实体代理能够迅速适应新任务。代理在必要时还会征求并利用人类输入来优化现有知识。在模拟域的烹饪和清洁任务实验评估中，与仅使用LLM相比，LLM、KG和人类输入之间的相互作用实现了显著的性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs可预测完成新任务的抽象动作序列。</li>
<li>实体代理可能因任务、自身或领域特定约束，无法执行该序列。</li>
<li>框架结合LLM的通用预测和KG的先验领域知识，助力代理快速适应新任务。</li>
<li>代理能够征求并利用人类输入来优化现有知识。</li>
<li>在烹饪和清洁任务的模拟域实验评估中，结合LLM、KG和人类输入的方法显著优于仅使用LLM。</li>
<li>项目网站提供了更多关于该框架的信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.02067">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e80340b0a8d5a68accc7f31bfb904209.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cde7e507a16305ed26c812b942ca88f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffb9dd5648e68aa81510c95738047337.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-943ae4f8d5911880e45035f72349c68c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af1011ba4bc1188ed7609557bdd3e7a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25cf31987a1148572676f5811e29d25e.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Human-Motion-Instruction-Tuning"><a href="#Human-Motion-Instruction-Tuning" class="headerlink" title="Human Motion Instruction Tuning"></a>Human Motion Instruction Tuning</h2><p><strong>Authors:Lei Li, Sen Jia, Wang Jianhao, Zhongyu Jiang, Feng Zhou, Ju Dai, Tianfang Zhang, Wu Zongkai, Jenq-Neng Hwang</strong></p>
<p>This paper presents LLaMo (Large Language and Human Motion Assistant), a multimodal framework for human motion instruction tuning. In contrast to conventional instruction-tuning approaches that convert non-linguistic inputs, such as video or motion sequences, into language tokens, LLaMo retains motion in its native form for instruction tuning. This method preserves motion-specific details that are often diminished in tokenization, thereby improving the model’s ability to interpret complex human behaviors. By processing both video and motion data alongside textual inputs, LLaMo enables a flexible, human-centric analysis. Experimental evaluations across high-complexity domains, including human behaviors and professional activities, indicate that LLaMo effectively captures domain-specific knowledge, enhancing comprehension and prediction in motion-intensive scenarios. We hope LLaMo offers a foundation for future multimodal AI systems with broad applications, from sports analytics to behavioral prediction. Our code and models are available on the project website: <a target="_blank" rel="noopener" href="https://github.com/ILGLJ/LLaMo">https://github.com/ILGLJ/LLaMo</a>. </p>
<blockquote>
<p>本文介绍了LLaMo（大型语言和人类运动辅助系统），这是一种用于人类运动指令调整的多模式框架。与传统的将非语言输入（如视频或运动序列）转换为语言标记的指令调整方法不同，LLaMo保留运动的本有形式来进行指令调整。这种方法保留了动作特定的细节，这些细节在令牌化时通常会减少，从而提高了模型解释复杂人类行为的能力。LLaMo能够处理视频和运动数据以及文本输入，从而实现灵活、以人为中心的分析。在人类行为和专业活动等高复杂度领域的实验评估表明，LLaMo可以有效地捕获特定领域的知识，在运动密集型场景中提高理解和预测能力。我们希望LLaMo能为未来多模式AI系统提供基础，广泛应用于运动分析到行为预测等领域。我们的代码和模型可在项目网站上找到：<a target="_blank" rel="noopener" href="https://github.com/ILGLJ/LLaMo%E3%80%82">https://github.com/ILGLJ/LLaMo。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16805v3">PDF</a> Accepted by CVPR 2025</p>
<p><strong>Summary</strong><br>LLaMo框架以人类运动指令调优为中心，通过保留原始动作形式而非将非语言输入转化为语言标记的方式进行处理。它结合视频和运动数据与文本输入，提供灵活且以人类为中心的分析方式。该框架可捕获高复杂性领域的特定知识，提高在动作密集型场景中的理解和预测能力。LLaMo框架有望为从体育分析到行为预测的多模式AI系统提供基础。更多信息请访问我们的项目网站：<a target="_blank" rel="noopener" href="https://github.com/ILGLJ/LLaMo">https://github.com/ILGLJ/LLaMo</a>。 </p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>LLaMo是一个多模态框架，用于人类运动指令调优。</li>
<li>它保留了原始动作形式，避免了将非语言输入转化为语言标记导致的细节损失。</li>
<li>LLaMo结合视频和运动数据以及文本输入进行处理，提供了灵活且以人类为中心的分析方式。</li>
<li>该框架在复杂领域表现出良好的性能，特别是在动作密集型场景中。</li>
<li>LLaMo框架能够捕获特定领域的专业知识，有助于提升理解和预测能力。</li>
<li>LLaMo的应用前景广泛，可为体育分析、行为预测等多模态AI系统打下基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16805">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e7a91a53c916cd827ae9911b61215b2e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60888dd67b71d0ef182a138fe8a1de30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62a8e81f246781b589809b8f31019850.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="LaVin-DiT-Large-Vision-Diffusion-Transformer"><a href="#LaVin-DiT-Large-Vision-Diffusion-Transformer" class="headerlink" title="LaVin-DiT: Large Vision Diffusion Transformer"></a>LaVin-DiT: Large Vision Diffusion Transformer</h2><p><strong>Authors:Zhaoqing Wang, Xiaobo Xia, Runnan Chen, Dongdong Yu, Changhu Wang, Mingming Gong, Tongliang Liu</strong></p>
<p>This paper presents the Large Vision Diffusion Transformer (LaVin-DiT), a scalable and unified foundation model designed to tackle over 20 computer vision tasks in a generative framework. Unlike existing large vision models directly adapted from natural language processing architectures, which rely on less efficient autoregressive techniques and disrupt spatial relationships essential for vision data, LaVin-DiT introduces key innovations to optimize generative performance for vision tasks. First, to address the high dimensionality of visual data, we incorporate a spatial-temporal variational autoencoder that encodes data into a continuous latent space. Second, for generative modeling, we develop a joint diffusion transformer that progressively produces vision outputs. Third, for unified multi-task training, in-context learning is implemented. Input-target pairs serve as task context, which guides the diffusion transformer to align outputs with specific tasks within the latent space. During inference, a task-specific context set and test data as queries allow LaVin-DiT to generalize across tasks without fine-tuning. Trained on extensive vision datasets, the model is scaled from 0.1B to 3.4B parameters, demonstrating substantial scalability and state-of-the-art performance across diverse vision tasks. This work introduces a novel pathway for large vision foundation models, underscoring the promising potential of diffusion transformers. The code and models are available. </p>
<blockquote>
<p>本文介绍了大型视觉扩散转换器（LaVin-DiT），这是一种可扩展的统一基础模型，旨在在一个生成框架中解决超过20项计算机视觉任务。与现有的直接从自然语言处理架构改编的大型视觉模型不同，这些模型依赖于效率较低的自动回归技术，并破坏了对视觉数据至关重要的空间关系。LaVin-DiT引入了关键的创新点，以优化计算机视觉任务的生成性能。首先，针对视觉数据的高维性，我们融入时空变分自动编码器，将数据编码为连续潜在空间。其次，为了进行生成建模，我们开发了一种联合扩散转换器，其逐步生成视觉输出。第三，为了进行统一的多任务训练，实施了上下文内学习。输入-目标对作为任务上下文，指导扩散转换器在潜在空间内将输出与特定任务对齐。在推理过程中，特定任务上下文集和测试数据作为查询，使LaVin-DiT能够在无需微调的情况下跨任务进行推广。该模型经过广泛的视觉数据集训练，规模从0.1B扩展到3.4B参数，表现出显著的可扩展性和在各种视觉任务上的卓越性能。这项工作为大型视觉基础模型引入了一条新的途径，突显了扩散转换器的巨大潜力。代码和模型均可使用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.11505v4">PDF</a> 37 pages, 30 figures, 4 tables. Accepted by CVPR 2025</p>
<p><strong>Summary</strong>：此论文介绍了一种大型视觉扩散变换模型（LaVin-DiT），它是一个可扩展的统一基础模型，旨在在一个生成框架中解决超过20项计算机视觉任务。该模型引入了一系列创新技术，以优化计算机视觉任务的生成性能。包括使用时空变分自编码器解决视觉数据的高维问题，开发联合扩散变换器进行生成建模，以及实现统一多任务训练的上下文内学习。该模型在广泛的视觉数据集上进行训练，参数规模从0.1B到3.4B，表现出显著的扩展性和跨各种视觉任务的卓越性能。此工作为大型视觉基础模型开辟了一条新途径，展示了扩散变换器的广阔前景。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>LaVin-DiT是一个统一的基础模型，能够处理多种计算机视觉任务。</li>
<li>模型引入了时空变分自编码器，以优化视觉数据的处理。</li>
<li>通过联合扩散变换器进行生成建模，逐步产生视觉输出。</li>
<li>采用上下文内学习实现统一多任务训练。</li>
<li>模型具有良好的可扩展性，参数规模从0.1B到3.4B。</li>
<li>LaVin-DiT在多种视觉任务上表现出卓越的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.11505">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c25b45182f1a02abbe6f45329380cbff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-637ffd24f99bfd8d8d5a3477f63c5bd4.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ACC-Collab-An-Actor-Critic-Approach-to-Multi-Agent-LLM-Collaboration"><a href="#ACC-Collab-An-Actor-Critic-Approach-to-Multi-Agent-LLM-Collaboration" class="headerlink" title="ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration"></a>ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration</h2><p><strong>Authors:Andrew Estornell, Jean-Francois Ton, Yuanshun Yao, Yang Liu</strong></p>
<p>Large language models (LLMs) have demonstrated a remarkable ability to serve as general-purpose tools for various language-based tasks. Recent works have demonstrated that the efficacy of such models can be improved through iterative dialog between multiple models. While these paradigms show promise in improving model efficacy, most works in this area treat collaboration as an emergent behavior, rather than a learned behavior. In doing so, current multi-agent frameworks rely on collaborative behaviors to have been sufficiently trained into off-the-shelf models. To address this limitation, we propose ACC-Collab, an Actor-Critic based learning framework to produce a two-agent team (an actor-agent and a critic-agent) specialized in collaboration. We demonstrate that ACC-Collab outperforms SotA multi-agent techniques on a wide array of benchmarks. </p>
<blockquote>
<p>大型语言模型（LLM）已展现出作为各种语言基础任务的通用工具的出色能力。近期的研究工作表明，通过多个模型之间的迭代对话可以提高此类模型的效率。虽然这些范式在提高模型效率方面显示出潜力，但该领域的大多数工作都将协作视为一种突发行为，而非学习行为。因此，当前的多代理框架依赖于协作行为已经充分训练成即席模型。为了解决这一局限性，我们提出了ACC-Collab，这是一种基于Actor-Critic的学习框架，用于生成一个专门协作的两代理团队（一个行动代理和一个批评代理）。我们证明，在广泛的基准测试中，ACC-Collab的表现优于当前最佳的多代理技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.00053v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）展现出强大的通用工具能力，可通过多次迭代对话提高模型效能。当前多代理框架依赖模型预先训练好的协作行为，而协作能力被视为一种涌现特性而非学习行为。为解决此局限，提出ACC-Collab框架，构建专门的演员代理和评论家代理团队。实验证明，ACC-Collab在多种基准测试中优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM展现出强大的通用工具能力，并能通过多次迭代对话提高模型效能。</li>
<li>当前多代理框架在处理LLM协作时存在局限性，依赖模型预先训练好的协作行为。</li>
<li>协作能力被视为一种涌现特性而非学习行为。</li>
<li>提出ACC-Collab框架来解决上述局限，构建专门的演员代理和评论家代理团队。</li>
<li>演员代理和评论家代理在ACC-Collab框架中分别承担不同的角色，共同实现高效的协作。</li>
<li>ACC-Collab在多种基准测试中表现出优异的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.00053">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c7eb8933593370723397907a2b0cfb45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd6f88554a948be927d3394cd6f6e7c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f17130aa9ebabb6ef12a9e0e8c4572e8.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="RAG-DDR-Optimizing-Retrieval-Augmented-Generation-Using-Differentiable-Data-Rewards"><a href="#RAG-DDR-Optimizing-Retrieval-Augmented-Generation-Using-Differentiable-Data-Rewards" class="headerlink" title="RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable   Data Rewards"></a>RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable   Data Rewards</h2><p><strong>Authors:Xinze Li, Sen Mei, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Hao Chen, Ge Yu, Zhiyuan Liu, Maosong Sun, Chenyan Xiong</strong></p>
<p>Retrieval-Augmented Generation (RAG) has proven its effectiveness in mitigating hallucinations in Large Language Models (LLMs) by retrieving knowledge from external resources. To adapt LLMs for the RAG systems, current approaches use instruction tuning to optimize LLMs, improving their ability to utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses on equipping LLMs to handle diverse RAG tasks using different instructions. However, it trains RAG modules to overfit training signals and overlooks the varying data preferences among agents within the RAG system. In this paper, we propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG systems by aligning data preferences between different RAG modules. DDR works by collecting the rewards to optimize each agent in the RAG system with the rollout method, which prompts agents to sample some potential responses as perturbations, evaluates the impact of these perturbations on the whole RAG system, and subsequently optimizes the agent to produce outputs that improve the performance of the RAG system. Our experiments on various knowledge-intensive tasks demonstrate that DDR significantly outperforms the SFT method, particularly for LLMs with smaller-scale parameters that depend more on the retrieved knowledge. Additionally, DDR exhibits a stronger capability to align the data preference between RAG modules. The DDR method makes the generation module more effective in extracting key information from documents and mitigating conflicts between parametric memory and external knowledge. All codes are available at <a target="_blank" rel="noopener" href="https://github.com/OpenMatch/RAG-DDR">https://github.com/OpenMatch/RAG-DDR</a>. </p>
<blockquote>
<p>检索增强生成（RAG）通过从外部资源检索知识，在大型语言模型（LLM）中证明了其减轻幻觉的有效性。为了适应RAG系统的LLM，当前的方法使用指令调整来优化LLM，提高其利用检索知识的能力。这种有监督微调（SFT）方法侧重于利用不同指令使LLM能够处理各种RAG任务。然而，它使RAG模块过度适应训练信号，并忽略了RAG系统内代理之间的数据偏好差异。在本文中，我们提出了一种可微分数据奖励（DDR）方法，该方法通过端到端训练RAG系统，对齐不同RAG模块之间的数据偏好。DDR通过收集奖励来优化RAG系统中每个代理的滚动方法，该方法鼓励代理对一些潜在响应进行采样作为扰动，评估这些扰动对整个RAG系统的影响，然后优化代理以产生提高RAG系统性能的输出来。我们在各种知识密集型任务上的实验表明，DDR显著优于SFT方法，特别是对于更依赖检索知识的小规模参数LLM。此外，DDR在对齐RAG模块之间的数据偏好方面表现出更强的能力。DDR方法使生成模块更有效地从文档中提取关键信息，并减轻参数内存和外部知识之间的冲突。所有代码可在<a target="_blank" rel="noopener" href="https://github.com/OpenMatch/RAG-DDR%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/OpenMatch/RAG-DDR上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.13509v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在大型语言模型（LLM）中，检索增强生成（RAG）通过从外部资源检索知识，有效减轻了模型虚构（hallucination）的问题。当前的方法采用指令调整（instruction tuning）来优化LLMs，使其更好地利用检索到的知识。然而，这种监督微调（SFT）方法注重通过不同指令处理多样化的RAG任务，却容易导致RAG模块过度适应训练信号，并忽略了RAG系统内各代理的数据偏好差异。本文提出了一种可微分数据奖励（DDR）方法，通过端对端地训练RAG系统，对齐不同RAG模块的数据偏好。DDR通过收集奖励来优化RAG系统中的每个代理，采用rollout方法促使代理生成一些可能的响应作为扰动，评估这些扰动对整个RAG系统的影响，从而优化代理以产生提高RAG系统性能的输出。实验表明，DDR方法在多种知识密集型任务上显著优于SFT方法，特别是在依赖检索知识的小规模参数LLMs上。DDR方法具有更强的对齐RAG模块数据偏好的能力，提高了生成模块从文档中提取关键信息的能力，并减轻了参数内存和外部知识之间的冲突。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAG通过从外部资源检索知识，有效减轻LLM中的虚构问题。</li>
<li>当前LLM优化方法多采用监督微调（SFT），但这种方法可能导致过度适应训练信号。</li>
<li>DDR方法通过端对端训练RAG系统，对齐不同模块的数据偏好。</li>
<li>DDR采用rollout方法优化代理，通过评估扰动影响来改进代理输出。</li>
<li>DDR在多种知识密集型任务上表现优于SFT方法。</li>
<li>DDR对小规模参数LLMs尤其有效，这些模型更依赖检索知识。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.13509">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5296383301ff52d9b70263961f943f18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36d8d5da773870cd606d197f31d6c05c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10063682d00eff7850542f5d244c7e5c.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a6d19e10e57fe6b34da29c78421a0ded.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-03-09  Multi-Agent Inverse Q-Learning from Demonstrations
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b58eda2b46e7a065a1bfb7629fee2d02.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-03-09  L1 Controlling How Long A Reasoning Model Thinks With Reinforcement   Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">17665k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
