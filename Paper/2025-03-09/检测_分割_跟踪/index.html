<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="检测/分割/跟踪">
    <meta name="description" content="检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-03-09  Omnidirectional Multi-Object Tracking">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>检测/分割/跟踪 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-b477433883d23a14325a3ee3fd471e9b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">检测/分割/跟踪</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">检测/分割/跟踪</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                检测/分割/跟踪
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    25 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-09-更新"><a href="#2025-03-09-更新" class="headerlink" title="2025-03-09 更新"></a>2025-03-09 更新</h1><h2 id="Omnidirectional-Multi-Object-Tracking"><a href="#Omnidirectional-Multi-Object-Tracking" class="headerlink" title="Omnidirectional Multi-Object Tracking"></a>Omnidirectional Multi-Object Tracking</h2><p><strong>Authors:Kai Luo, Hao Shi, Sheng Wu, Fei Teng, Mengfei Duan, Chang Huang, Yuhang Wang, Kaiwei Wang, Kailun Yang</strong></p>
<p>Panoramic imagery, with its 360{\deg} field of view, offers comprehensive information to support Multi-Object Tracking (MOT) in capturing spatial and temporal relationships of surrounding objects. However, most MOT algorithms are tailored for pinhole images with limited views, impairing their effectiveness in panoramic settings. Additionally, panoramic image distortions, such as resolution loss, geometric deformation, and uneven lighting, hinder direct adaptation of existing MOT methods, leading to significant performance degradation. To address these challenges, we propose OmniTrack, an omnidirectional MOT framework that incorporates Tracklet Management to introduce temporal cues, FlexiTrack Instances for object localization and association, and the CircularStatE Module to alleviate image and geometric distortions. This integration enables tracking in large field-of-view scenarios, even under rapid sensor motion. To mitigate the lack of panoramic MOT datasets, we introduce the QuadTrack dataset–a comprehensive panoramic dataset collected by a quadruped robot, featuring diverse challenges such as wide fields of view, intense motion, and complex environments. Extensive experiments on the public JRDB dataset and the newly introduced QuadTrack benchmark demonstrate the state-of-the-art performance of the proposed framework. OmniTrack achieves a HOTA score of 26.92% on JRDB, representing an improvement of 3.43%, and further achieves 23.45% on QuadTrack, surpassing the baseline by 6.81%. The dataset and code will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/xifen523/OmniTrack">https://github.com/xifen523/OmniTrack</a>. </p>
<blockquote>
<p>全景影像以其360°的视野，为捕捉周围物体的空间和时间关系以支持多目标跟踪（MOT）提供了全面的信息。然而，大多数MOT算法都是针对视野有限的针孔图像定制的，在全景环境中会降低其有效性。此外，全景图像的失真，如分辨率损失、几何变形和光线不均，阻碍了现有MOT方法的直接应用，导致性能显著下降。为了解决这些挑战，我们提出了OmniTrack，一个全向MOT框架，它结合了轨迹管理以引入时间线索、FlexiTrack实例进行目标定位和关联，以及CircularStatE模块来缓解图像和几何失真。这种整合使得在大视野场景中，即使在传感器快速运动的情况下也能进行追踪。为了缓解全景MOT数据集的缺乏，我们引入了QuadTrack数据集——一个由四足机器人收集的全方位全景数据集，具有广阔的视野、强烈的运动和复杂环境等多样挑战。在公共JRDB数据集和新引入的QuadTrack基准测试上的大量实验表明，所提出的框架具有最先进的性能。OmniTrack在JRDB上的HOTA得分为26.92%，提高了3.42%，并在QuadTrack上实现了23.45%，超过了基准线6.81%。数据集和代码将在<a target="_blank" rel="noopener" href="https://github.com/xifen523/OmniTrack">https://github.com/xifen523/OmniTrack</a>上公开发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04565v1">PDF</a> Accepted to CVPR 2025. The dataset and code will be made publicly   available at <a target="_blank" rel="noopener" href="https://github.com/xifen523/OmniTrack">https://github.com/xifen523/OmniTrack</a></p>
<p><strong>Summary</strong>：全景成像技术可为多目标跟踪（MOT）提供丰富的空间和时间信息，但在全景环境下，大多数MOT算法受限于视角范围，性能受限。此外，全景图像的畸变（如分辨率损失、几何变形和光照不均）给现有MOT方法的直接应用带来困难。为此，我们提出OmniTrack框架，结合轨迹管理引入时间线索、FlexiTrack进行目标定位和关联，以及CircularStatE模块缓解图像和几何畸变。该框架可在大视野场景下实现跟踪，甚至在快速传感器运动下也能保持性能。此外，我们还引入了QuadTrack数据集，该数据集由四足机器人收集，具有宽视野、强烈运动和复杂环境等多重挑战。实验表明，OmniTrack在公开JRDB数据集和新引入的QuadTrack基准测试上均表现优异。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>全景成像技术提供丰富的空间和时间信息支持多目标跟踪（MOT）。</li>
<li>大多数现有的MOT算法在全景环境下性能受限，需要新的算法来解决挑战。</li>
<li>OmniTrack框架结合轨迹管理、FlexiTrack和CircularStatE模块，实现全景环境下有效的多目标跟踪。</li>
<li>OmniTrack框架可在大视野场景下运行，甚至在快速传感器运动下也能保持性能。</li>
<li>引入QuadTrack数据集，具有宽视野、强烈运动和复杂环境等多重挑战。</li>
<li>实验表明OmniTrack在公开数据集和QuadTrack基准测试上表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04565">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-13c0a6dff440c31444c19a6a0fd83650.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efb9017e99c506d08094965c54dfe6d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50cd7fc9df3ff572d39f488038a36803.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98886acfcb08d945a5ada671fa223fd3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DSV-LFS-Unifying-LLM-Driven-Semantic-Cues-with-Visual-Features-for-Robust-Few-Shot-Segmentation"><a href="#DSV-LFS-Unifying-LLM-Driven-Semantic-Cues-with-Visual-Features-for-Robust-Few-Shot-Segmentation" class="headerlink" title="DSV-LFS: Unifying LLM-Driven Semantic Cues with Visual Features for   Robust Few-Shot Segmentation"></a>DSV-LFS: Unifying LLM-Driven Semantic Cues with Visual Features for   Robust Few-Shot Segmentation</h2><p><strong>Authors:Amin Karimi, Charalambos Poullis</strong></p>
<p>Few-shot semantic segmentation (FSS) aims to enable models to segment novel&#x2F;unseen object classes using only a limited number of labeled examples. However, current FSS methods frequently struggle with generalization due to incomplete and biased feature representations, especially when support images do not capture the full appearance variability of the target class. To improve the FSS pipeline, we propose a novel framework that utilizes large language models (LLMs) to adapt general class semantic information to the query image. Furthermore, the framework employs dense pixel-wise matching to identify similarities between query and support images, resulting in enhanced FSS performance. Inspired by reasoning-based segmentation frameworks, our method, named DSV-LFS, introduces an additional token into the LLM vocabulary, allowing a multimodal LLM to generate a “semantic prompt” from class descriptions. In parallel, a dense matching module identifies visual similarities between the query and support images, generating a “visual prompt”. These prompts are then jointly employed to guide the prompt-based decoder for accurate segmentation of the query image. Comprehensive experiments on the benchmark datasets Pascal-$5^{i}$ and COCO-$20^{i}$ demonstrate that our framework achieves state-of-the-art performance-by a significant margin-demonstrating superior generalization to novel classes and robustness across diverse scenarios. The source code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/aminpdik/DSV-LFS%7D%7Bhttps://github.com/aminpdik/DSV-LFS%7D">https://github.com/aminpdik/DSV-LFS}{https://github.com/aminpdik/DSV-LFS}</a> </p>
<blockquote>
<p>少量语义分割（FSS）旨在使模型能够仅使用有限数量的标记示例来对新型&#x2F;未见过的对象类别进行分割。然而，当前的FSS方法由于特征表示不完整和存在偏见，在推广时经常遇到困难，特别是当支持图像没有捕捉到目标类别的完整外观变化时。为了改进FSS管道，我们提出了一种利用大型语言模型（LLM）来适应通用类别语义信息到查询图像的新框架。此外，该框架采用密集像素级匹配来识别查询图像和支持图像之间的相似性，从而提高了FSS的性能。我们的方法受到基于推理的分割框架的启发，被称为DSV-LFS，它引入了LLM词汇中的一个附加令牌，允许多模态LLM从类别描述生成“语义提示”。同时，密集匹配模块识别查询图像和支持图像之间的视觉相似性，生成“视觉提示”。然后这些提示被共同用来引导基于提示的解码器，对查询图像进行准确的分割。在Pascal-$5^{i}$和COCO-$20^{i}$基准数据集上的综合实验表明，我们的框架实现了最先进的性能，并显示出对新型类别的优越推广能力和在不同场景中的稳健性。源代码可在<a target="_blank" rel="noopener" href="https://github.com/aminpdik/DSV-LFS">https://github.com/aminpdik/DSV-LFS</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04006v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种新的少样本语义分割（FSS）框架，该框架利用大型语言模型（LLM）来适应通用类语义信息到查询图像，并采用密集像素级匹配来识别查询图像和支持图像之间的相似性，从而提高FSS性能。该方法通过引入语义提示和视觉提示，实现了对查询图像的准确分割，并在基准数据集上取得了最先进的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新的少样本语义分割（FSS）框架，旨在使用有限数量的标记示例对新型&#x2F;未见过的对象类别进行分割。</li>
<li>利用大型语言模型（LLM）适应通用类语义信息到查询图像，改善特征表示的不完整和偏见问题。</li>
<li>通过密集像素级匹配识别查询图像和支持图像之间的相似性，提高FSS性能。</li>
<li>引入语义提示和视觉提示，通过模态LLM生成语义提示，密集匹配模块生成视觉提示。</li>
<li>联合使用语义和视觉提示，引导基于提示的解码器对查询图像进行准确分割。</li>
<li>在Pascal-$5^{i}$和COCO-$20^{i}$基准数据集上的综合实验表明，该框架实现了显著的先进性能，对新型类具有优越泛化能力，并在各种场景中表现出稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04006">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-55d3527522cd7c1f9aa9d6b2e802e467.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-81e3fb45d20f9e4ebe9f4022ffc636ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11f23573eb586dceefce36ed3d2a2d7f.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="ReRAW-RGB-to-RAW-Image-Reconstruction-via-Stratified-Sampling-for-Efficient-Object-Detection-on-the-Edge"><a href="#ReRAW-RGB-to-RAW-Image-Reconstruction-via-Stratified-Sampling-for-Efficient-Object-Detection-on-the-Edge" class="headerlink" title="ReRAW: RGB-to-RAW Image Reconstruction via Stratified Sampling for   Efficient Object Detection on the Edge"></a>ReRAW: RGB-to-RAW Image Reconstruction via Stratified Sampling for   Efficient Object Detection on the Edge</h2><p><strong>Authors:Radu Berdan, Beril Besbinar, Christoph Reinders, Junji Otsuka, Daisuke Iso</strong></p>
<p>Edge-based computer vision models running on compact, resource-limited devices benefit greatly from using unprocessed, detail-rich RAW sensor data instead of processed RGB images. Training these models, however, necessitates large labeled RAW datasets, which are costly and often impractical to obtain. Thus, converting existing labeled RGB datasets into sensor-specific RAW images becomes crucial for effective model training. In this paper, we introduce ReRAW, an RGB-to-RAW conversion model that achieves state-of-the-art reconstruction performance across five diverse RAW datasets. This is accomplished through ReRAW’s novel multi-head architecture predicting RAW image candidates in gamma space. The performance is further boosted by a stratified sampling-based training data selection heuristic, which helps the model better reconstruct brighter RAW pixels. We finally demonstrate that pretraining compact models on a combination of high-quality synthetic RAW datasets (such as generated by ReRAW) and ground-truth RAW images for downstream tasks like object detection, outperforms both standard RGB pipelines, and RAW fine-tuning of RGB-pretrained models for the same task. </p>
<blockquote>
<p>基于边缘的计算机视觉模型在紧凑、资源受限的设备上运行，与使用经过处理的RGB图像相比，使用未加工、细节丰富的原始传感器数据（RAW数据）将大有裨益。然而，训练这些模型需要大型标记原始数据集，这些数据集获取成本高昂且往往不切实际。因此，将现有的标记RGB数据集转换为特定传感器所需的RAW图像对于有效模型训练至关重要。在本文中，我们介绍了ReRAW，一种RGB到RAW的转换模型，它在五个不同的RAW数据集上实现了最先进的重建性能。这是通过ReRAW的新型多头架构实现的，该架构在伽马空间中预测RAW图像候选对象。通过分层采样基础上的训练数据选择启发式策略，进一步提高了性能，这有助于模型更好地重建明亮的RAW像素。最后，我们证明在高质量合成RAW数据集（如由ReRAW生成）和地面真实RAW图像上预训练紧凑模型，用于对象检测等下游任务，既优于标准的RGB管道，也优于对同一任务使用RGB预训练模型的RAW微调。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03782v1">PDF</a> Accepted at CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>在资源受限的设备上运行的基于边缘的计算机视觉模型，使用未加工的、细节丰富的原始传感器数据代替经过处理的RGB图像，可以大大提高性能。然而，训练这些模型需要庞大的标记原始数据集，这既昂贵又不切实际。因此，将现有的标记RGB数据集转换为传感器特定的原始图像对于有效的模型训练至关重要。本文介绍了ReRAW，一种RGB到RAW的转换模型，它通过在伽马空间预测RAW图像候选者，实现了跨越五个不同原始数据集的最新重建性能。性能的提升还得益于分层抽样基础上的训练数据选择策略，有助于模型更好地重建明亮的原始像素。最后证明，在高质量合成原始数据集（如由ReRAW生成）和地面真实原始图像的组合上预训练紧凑模型，用于执行对象检测等下游任务，优于标准的RGB管道以及相同任务的RGB预训练模型的原始微调。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>使用原始传感器数据代替RGB图像能提高边缘计算机视觉模型的性能。</li>
<li>庞大的标记原始数据集对于模型训练至关重要，但其获取成本高昂且不切实际。</li>
<li>ReRAW模型能够通过RGB到RAW转换，实现优秀的图像重建性能。</li>
<li>ReRAW模型采用多头架构预测原始图像候选者，并在伽马空间完成此操作。</li>
<li>分层抽样策略有助于模型更好地重建明亮的原始像素。</li>
<li>结合高质量合成原始数据集和真实原始图像预训练模型，能在下游任务上获得最佳性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03782">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cf3029351e4800ddc30b3010d180d1eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e105fa7405f6284a05e325d016176b9f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44a668ce55002cb8a37b9e6a934163c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e611587f5de14e8db7a4fb624f86d4d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eeaacc3a442e979394532071ca1a4259.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MIAdapt-Source-free-Few-shot-Domain-Adaptive-Object-Detection-for-Microscopic-Images"><a href="#MIAdapt-Source-free-Few-shot-Domain-Adaptive-Object-Detection-for-Microscopic-Images" class="headerlink" title="MIAdapt: Source-free Few-shot Domain Adaptive Object Detection for   Microscopic Images"></a>MIAdapt: Source-free Few-shot Domain Adaptive Object Detection for   Microscopic Images</h2><p><strong>Authors:Nimra Dilawar, Sara Nadeem, Javed Iqbal, Waqas Sultani, Mohsen Ali</strong></p>
<p>Existing generic unsupervised domain adaptation approaches require access to both a large labeled source dataset and a sufficient unlabeled target dataset during adaptation. However, collecting a large dataset, even if unlabeled, is a challenging and expensive endeavor, especially in medical imaging. In addition, constraints such as privacy issues can result in cases where source data is unavailable. Taking in consideration these challenges, we propose MIAdapt, an adaptive approach for Microscopic Imagery Adaptation as a solution for Source-free Few-shot Domain Adaptive Object detection (SF-FSDA). We also define two competitive baselines (1) Faster-FreeShot and (2) MT-FreeShot. Extensive experiments on the challenging M5-Malaria and Raabin-WBC datasets validate the effectiveness of MIAdapt. Without using any image from the source domain MIAdapt surpasses state-of-the-art source-free UDA (SF-UDA) methods by +21.3% mAP and few-shot domain adaptation (FSDA) approaches by +4.7% mAP on Raabin-WBC. Our code and models will be publicly available. </p>
<blockquote>
<p>现有的通用无监督域适应方法要求在适应过程中同时访问大量有标签的源数据集和足够的无标签的目标数据集。然而，在医疗成像等领域，即使是无标签的数据集，收集也是一个具有挑战性和成本高昂的任务。此外，隐私问题等约束条件可能导致无法使用源数据。考虑到这些挑战，我们提出了MIAdapt，这是一种用于显微镜图像适应的自适应方法，作为无源少镜头域自适应目标检测（SF-FSDA）的解决方案。我们还定义了两个有竞争力的基线方法：（1）Faster-FreeShot和（2）MT-FreeShot。在具有挑战性的M5疟疾和Raabin-WBC数据集上的大量实验验证了MIAdapt的有效性。MIAdapt在不使用任何源域图像的情况下，超过了最新的无源UDA（SF-UDA）方法的mAP+21.3%，并在Raabin-WBC数据集上超过了少镜头域适应（FSDA）方法的mAP+4.7%。我们的代码和模型将公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03370v2">PDF</a> 6 pages, 5 figures</p>
<p><strong>Summary</strong><br>本文提出一种针对显微镜图像自适应的MIAdapt方法，用于解决源数据缺失下的少量样本域自适应目标检测问题。在M5-Malaria和Raabin-WBC数据集上的实验验证了MIAdapt的有效性，相较于现有的源数据缺失下的无监督域自适应方法和少量样本域自适应方法，MIAdapt分别提高了21.3%和4.7%的平均精度。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>现有通用无监督域自适应方法需要大量标注的源数据集和足够的未标注目标数据集进行适应，这在医学成像中尤其具有挑战性和成本高昂。</li>
<li>针对这些挑战，提出了MIAdapt方法，用于显微镜图像的域适应。</li>
<li>MIAdapt是一种针对源数据缺失下的少量样本域自适应目标检测（SF-FSDA）的解决方案。</li>
<li>在M5-Malaria和Raabin-WBC数据集上进行了广泛的实验验证，证明了MIAdapt的有效性。</li>
<li>MIAdapt相较于现有方法显著提高了平均精度（mAP）。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03370">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cf50c1293384b7b9b24831761f296a4e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-15bae6af356674b32e943cf399db7933.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36c11912e3ba825a78e7f84a4e48e2c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-58276572fd7ef9418bb2a247dae84426.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-28e881fce8314b446a3cb04432c63a0e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b477433883d23a14325a3ee3fd471e9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f5f5852f7e12fab744a57b670e0630d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="FSPGD-Rethinking-Black-box-Attacks-on-Semantic-Segmentation"><a href="#FSPGD-Rethinking-Black-box-Attacks-on-Semantic-Segmentation" class="headerlink" title="FSPGD: Rethinking Black-box Attacks on Semantic Segmentation"></a>FSPGD: Rethinking Black-box Attacks on Semantic Segmentation</h2><p><strong>Authors:Eun-Sol Park, MiSo Park, Seung Park, Yong-Goo Shin</strong></p>
<p>Transferability, the ability of adversarial examples crafted for one model to deceive other models, is crucial for black-box attacks. Despite advancements in attack methods for semantic segmentation, transferability remains limited, reducing their effectiveness in real-world applications. To address this, we introduce the Feature Similarity Projected Gradient Descent (FSPGD) attack, a novel black-box approach that enhances both attack performance and transferability. Unlike conventional segmentation attacks that rely on output predictions for gradient calculation, FSPGD computes gradients from intermediate layer features. Specifically, our method introduces a loss function that targets local information by comparing features between clean images and adversarial examples, while also disrupting contextual information by accounting for spatial relationships between objects. Experiments on Pascal VOC 2012 and Cityscapes datasets demonstrate that FSPGD achieves superior transferability and attack performance, establishing a new state-of-the-art benchmark. Code is available at <a target="_blank" rel="noopener" href="https://github.com/KU-AIVS/FSPGD">https://github.com/KU-AIVS/FSPGD</a>. </p>
<blockquote>
<p>迁移性是指为某一模型制作的对抗样本欺骗其他模型的能力，对于黑盒攻击至关重要。尽管语义分割的攻击方法已经有所发展，但迁移性仍然有限，降低了其在现实世界应用中的有效性。为了解决这一问题，我们引入了特征相似性投影梯度下降（FSPGD）攻击，这是一种新型的黑盒攻击方法，既能提高攻击性能，又能增强迁移性。与传统的依赖输出预测进行梯度计算的分割攻击不同，FSPGD从中间层特征计算梯度。具体来说，我们的方法引入了一个损失函数，该函数通过比较干净图像和对抗样本之间的特征来定位局部信息，同时通过考虑对象之间的空间关系来破坏上下文信息。在Pascal VOC 2012和Cityscapes数据集上的实验表明，FSPGD在迁移性和攻击性能上达到了领先水平，树立了新的基准。代码可通过<a target="_blank" rel="noopener" href="https://github.com/KU-AIVS/FSPGD%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/KU-AIVS/FSPGD获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01262v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本研究指出在语义分割领域的攻击方法中转移性的重要性及其对现实应用的影响。为提高攻击性能和转移性，研究者提出了特征相似性投影梯度下降（FSPGD）攻击方法。该方法通过引入损失函数，针对局部信息和空间关系进行梯度计算，实现了对中间层特征的利用。实验证明，FSPGD在Pascal VOC 2012和Cityscapes数据集上取得了更高的转移性和攻击性能，达到业界最新水平。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>转移性是黑盒攻击中对抗性实例的关键能力，对于语义分割领域的攻击方法尤为重要。</li>
<li>当前攻击方法在转移性方面存在局限性，限制了其在现实应用中的有效性。</li>
<li>FSPGD攻击是一种新型黑盒方法，旨在提高攻击性能和转移性。</li>
<li>FSPGD通过引入损失函数并利用中间层特征来计算梯度，特别关注局部信息和空间关系。</li>
<li>实验证明，FSPGD在Pascal VOC 2012和Cityscapes数据集上的攻击性能和转移性均表现优异。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01262">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-21da0541afbc049ab593524b91bf5bd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c849e5f2a33c1dadd4fd0b49484cdb16.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d12629c49f9a7bc5fe29e5bff8d9b26f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f76bb3338fe0bfb424a829fdcf27ec3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c428c8703996d42bfa22681a2e80841a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Modulating-CNN-Features-with-Pre-Trained-ViT-Representations-for-Open-Vocabulary-Object-Detection"><a href="#Modulating-CNN-Features-with-Pre-Trained-ViT-Representations-for-Open-Vocabulary-Object-Detection" class="headerlink" title="Modulating CNN Features with Pre-Trained ViT Representations for   Open-Vocabulary Object Detection"></a>Modulating CNN Features with Pre-Trained ViT Representations for   Open-Vocabulary Object Detection</h2><p><strong>Authors:Xiangyu Gao, Yu Dai, Benliu Qiu, Lanxiao Wang, Heqian Qiu, Hongliang Li</strong></p>
<p>Owing to large-scale image-text contrastive training, pre-trained vision language model (VLM) like CLIP shows superior open-vocabulary recognition ability. Most existing open-vocabulary object detectors attempt to utilize the pre-trained VLMs to attain generalized representation. F-ViT uses the pre-trained visual encoder as the backbone network and freezes it during training. However, its frozen backbone doesn’t benefit from the labeled data to strengthen the representation for detection. Therefore, we propose a novel two-branch backbone network, named as \textbf{V}iT-Feature-\textbf{M}odulated Multi-Scale \textbf{C}onvolutional Network (VMCNet), which consists of a trainable convolutional branch, a frozen pre-trained ViT branch and a VMC module. The trainable CNN branch could be optimized with labeled data while the frozen pre-trained ViT branch could keep the representation ability derived from large-scale pre-training. Then, the proposed VMC module could modulate the multi-scale CNN features with the representations from ViT branch. With this proposed mixed structure, the detector is more likely to discover objects of novel categories. Evaluated on two popular benchmarks, our method boosts the detection performance on novel category and outperforms state-of-the-art methods. On OV-COCO, the proposed method achieves 44.3 AP$<em>{50}^{\mathrm{novel}}$ with ViT-B&#x2F;16 and 48.5 AP$</em>{50}^{\mathrm{novel}}$ with ViT-L&#x2F;14. On OV-LVIS, VMCNet with ViT-B&#x2F;16 and ViT-L&#x2F;14 reaches 27.8 and 38.4 mAP$_{r}$. </p>
<blockquote>
<p>由于大规模图像文本对比训练，像CLIP这样的预训练视觉语言模型（VLM）表现出卓越的开放词汇识别能力。大多数现有的开放词汇对象检测器试图利用预训练的VLMs来获得通用表示。F-ViT使用预训练的视觉编码器作为主干网络，并在训练期间冻结它。然而，其冻结的主干网络并不能从标记数据中受益，以增强检测表示。因此，我们提出了一种新型的两分支主干网络，名为ViT特征调制多尺度卷积网络（VMCNet），它由可训练的卷积分支、冻结的预训练ViT分支和VMC模块组成。可训练的CNN分支可以利用标记数据进行优化，而冻结的预训练ViT分支可以保持从大规模预训练中学到的表示能力。然后，所提出的VMC模块可以调制来自ViT分支的多尺度CNN特征。通过这种混合结构，检测器更有可能发现新型类别的对象。在两个流行的基准测试上进行评估，我们的方法在新型类别检测性能上有所提升，并超越了最先进的方法。在OV-COCO上，所提方法使用ViT-B&#x2F;16达到44.3 AP50novel，使用ViT-L&#x2F;14达到48.5 AP50novel。在OV-LVIS上，VMCNet与ViT-B&#x2F;16和ViT-L&#x2F; 点石即成金赋予了更强的开放性语意描述和更大的信息量上做出了明显的提升。通过使用我们提出的VMCNet结构，不仅提高了检测性能，而且显著提升了模型对新型类别对象的检测能力。该论文的主要贡献在于结合预训练视觉语言模型和卷积神经网络的优势，提出了一种混合结构的主干网络，实现了开放词汇下的高效对象检测。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16981v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于大规模图像文本对比训练，预训练视觉语言模型（如CLIP）展现出卓越的开箱词汇识别能力。现有开箱词汇目标检测器大多尝试利用预训练视觉语言模型获得通用表示。F-ViT使用预训练的视觉编码器作为骨干网络并在训练期间冻结它。然而，其冻结骨干网并未受益于标注数据来强化检测表示。因此，我们提出了一种新型的两分支骨干网络，名为ViT特征调制多尺度卷积网络（VMCNet），包括一个可训练卷积分支、一个冻结的预训练ViT分支和VMC模块。可训练的CNN分支可以利用标注数据进行优化，而冻结的预训练ViT分支则能够保持从大规模预训练中获得的表示能力。随后，所提出的VMC模块可以调制来自ViT分支的表示与多尺度CNN特征。通过这种混合结构，检测器更有可能发现新型类别的目标。在流行的基准测试上评估，我们的方法在新型类别检测性能上有所提升并超越了最先进的方法。在OV-COCO上，所提方法使用ViT-B&#x2F;16达到44.3 AP50novel，使用ViT-L&#x2F;14达到48.5 AP50novel。在OV-LVIS上，VMCNet使用ViT-B&#x2F;16和ViT-L&#x2F;14分别达到了27.8和38.4的mAPr。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大规模图像文本对比训练使预训练视觉语言模型（如CLIP）具有卓越的开箱词汇识别能力。</li>
<li>现有开箱词汇目标检测器倾向于利用预训练视觉语言模型获取通用表示。</li>
<li>F-ViT利用预训练视觉编码器作为骨干网络，但在训练过程中无法充分利用标注数据。</li>
<li>提出的VMCNet通过结合可训练的卷积分支和冻结的预训练ViT分支，旨在优化检测性能。</li>
<li>VMC模块能够调制来自ViT分支和CNN分支的特征，增强检测器对新型类别目标的发现能力。</li>
<li>在OV-COCO和OV-LVIS基准测试上，VMCNet显著提升了检测性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16981">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-9b2b1b4e49a4d020943c637aa14151fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4630519de5801d9c8607f2d66dce399d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3a9d9caae2d8aa0cf3ceb26d1aaf30e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e23b9055f1cd38a68b8a6859ec6604ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43c1194db2eb80c83d993c1f586b79dc.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">检测/分割/跟踪</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-838acf3222ef9c7da8966a4c1ef33bb7.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-03-09  Enhancing Multimodal Medical Image Classification using Cross-Graph   Modal Contrastive Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-09
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-1cb6b2a7e3e81f209d843b99170518b1.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-03-09  ViT-VS On the Applicability of Pretrained Vision Transformer Features   for Generalizable Visual Servoing
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-09
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27663.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
