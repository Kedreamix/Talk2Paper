<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-10  Synthetic Data is an Elegant GIFT for Continual Vision-Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-716fefef52104778416315e065200599.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    42 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-10-æ›´æ–°"><a href="#2025-03-10-æ›´æ–°" class="headerlink" title="2025-03-10 æ›´æ–°"></a>2025-03-10 æ›´æ–°</h1><h2 id="Synthetic-Data-is-an-Elegant-GIFT-for-Continual-Vision-Language-Models"><a href="#Synthetic-Data-is-an-Elegant-GIFT-for-Continual-Vision-Language-Models" class="headerlink" title="Synthetic Data is an Elegant GIFT for Continual Vision-Language Models"></a>Synthetic Data is an Elegant GIFT for Continual Vision-Language Models</h2><p><strong>Authors:Bin Wu, Wuxuan Shi, Jinqiao Wang, Mang Ye</strong></p>
<p>Pre-trained Vision-Language Models (VLMs) require Continual Learning (CL) to efficiently update their knowledge and adapt to various downstream tasks without retraining from scratch. However, for VLMs, in addition to the loss of knowledge previously learned from downstream tasks, pre-training knowledge is also corrupted during continual fine-tuning. This issue is exacerbated by the unavailability of original pre-training data, leaving VLMâ€™s generalization ability degrading. In this paper, we propose GIFT, a novel continual fine-tuning approach that utilizes synthetic data to overcome catastrophic forgetting in VLMs. Taking advantage of recent advances in text-to-image synthesis, we employ a pre-trained diffusion model to recreate both pre-training and learned downstream task data. In this way, the VLM can revisit previous knowledge through distillation on matching diffusion-generated images and corresponding text prompts. Leveraging the broad distribution and high alignment between synthetic image-text pairs in VLMâ€™s feature space, we propose a contrastive distillation loss along with an image-text alignment constraint. To further combat in-distribution overfitting and enhance distillation performance with limited amount of generated data, we incorporate adaptive weight consolidation, utilizing Fisher information from these synthetic image-text pairs and achieving a better stability-plasticity balance. Extensive experiments demonstrate that our method consistently outperforms previous state-of-the-art approaches across various settings. </p>
<blockquote>
<p>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰éœ€è¦æŒç»­å­¦ä¹ ï¼ˆCLï¼‰æ¥æœ‰æ•ˆåœ°æ›´æ–°å…¶çŸ¥è¯†ï¼Œå¹¶é€‚åº”å„ç§ä¸‹æ¸¸ä»»åŠ¡ï¼Œè€Œæ— éœ€ä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒã€‚ç„¶è€Œï¼Œå¯¹äºVLMsè€Œè¨€ï¼Œé™¤äº†ä»ä¸‹æ¸¸ä»»åŠ¡ä¸­å­¦åˆ°çš„çŸ¥è¯†ä¸¢å¤±ä¹‹å¤–ï¼Œé¢„è®­ç»ƒçŸ¥è¯†åœ¨æŒç»­å¾®è°ƒè¿‡ç¨‹ä¸­ä¹Ÿä¼šè¢«ç ´åã€‚ç”±äºåŸå§‹é¢„è®­ç»ƒæ•°æ®ä¸å¯ç”¨ï¼Œè¿™ä¸€é—®é¢˜æ›´åŠ ä¸¥é‡ï¼Œå¯¼è‡´VLMçš„æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æŒç»­å¾®è°ƒæ–¹æ³•GIFTï¼Œå®ƒåˆ©ç”¨åˆæˆæ•°æ®æ¥å…‹æœVLMä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚æˆ‘ä»¬å€ŸåŠ©æœ€æ–°çš„æ–‡æœ¬åˆ°å›¾åƒåˆæˆæŠ€æœ¯ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹é‡æ–°åˆ›å»ºé¢„è®­ç»ƒå’Œå·²å­¦ä¹ çš„ä¸‹æ¸¸ä»»åŠ¡æ•°æ®ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒVLMå¯ä»¥é€šè¿‡å¯¹åŒ¹é…çš„æ‰©æ•£ç”Ÿæˆå›¾åƒå’Œç›¸åº”çš„æ–‡æœ¬æç¤ºè¿›è¡Œè’¸é¦æ¥å›é¡¾ä»¥å‰çš„çŸ¥è¯†ã€‚å‡­å€Ÿåˆæˆå›¾åƒ-æ–‡æœ¬å¯¹åœ¨VLMç‰¹å¾ç©ºé—´ä¸­çš„å¹¿æ³›åˆ†å¸ƒå’Œé«˜å¯¹é½åº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¯¹æ¯”è’¸é¦æŸå¤±ä»¥åŠå›¾åƒ-æ–‡æœ¬å¯¹é½çº¦æŸã€‚ä¸ºäº†è¿›ä¸€æ­¥å¯¹æŠ—å†…éƒ¨åˆ†å¸ƒè¿‡æ‹Ÿåˆå¹¶å¢å¼ºæœ‰é™ç”Ÿæˆæ•°æ®çš„è’¸é¦æ€§èƒ½ï¼Œæˆ‘ä»¬ç»“åˆäº†è‡ªé€‚åº”æƒé‡æ•´åˆï¼Œåˆ©ç”¨è¿™äº›åˆæˆå›¾åƒ-æ–‡æœ¬å¯¹çš„Fisherä¿¡æ¯ï¼Œå®ç°äº†æ›´å¥½çš„ç¨³å®šæ€§-å¯å¡‘æ€§å¹³è¡¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§è®¾ç½®ä¸‹å§‹ç»ˆä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04229v1">PDF</a> This work is accepted by CVPR 2025. Modifications may be performed</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„æŒç»­å¾®è°ƒæ–¹æ³•GIFTï¼Œåˆ©ç”¨åˆæˆæ•°æ®å…‹æœè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨æŒç»­å­¦ä¹ ä¸­çš„ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹é‡æ–°åˆ›å»ºé¢„è®­ç»ƒåŠå·²å­¦ä¹ çš„ä¸‹æ¸¸ä»»åŠ¡æ•°æ®ï¼Œä½¿VLMèƒ½å¤Ÿé€šè¿‡åŒ¹é…ç”Ÿæˆçš„æ‰©æ•£å›¾åƒå’Œç›¸åº”çš„æ–‡æœ¬æç¤ºè¿›è¡ŒçŸ¥è¯†è’¸é¦ã€‚åŒæ—¶ï¼Œåˆ©ç”¨åˆæˆå›¾åƒ-æ–‡æœ¬å¯¹åœ¨VLMç‰¹å¾ç©ºé—´ä¸­çš„å¹¿æ³›åˆ†å¸ƒå’Œé«˜åº¦å¯¹é½ç‰¹æ€§ï¼Œæå‡ºäº†å¯¹æ¯”è’¸é¦æŸå¤±å’Œå›¾åƒ-æ–‡æœ¬å¯¹é½çº¦æŸã€‚ä¸ºè¿›ä¸€æ­¥è§£å†³å†…éƒ¨åˆ†å¸ƒè¿‡æ‹Ÿåˆé—®é¢˜å¹¶å¢å¼ºæœ‰é™ç”Ÿæˆæ•°æ®çš„è’¸é¦æ€§èƒ½ï¼Œèå…¥è‡ªé€‚åº”æƒé‡æ•´åˆç­–ç•¥ï¼Œåˆ©ç”¨è¿™äº›åˆæˆå›¾åƒ-æ–‡æœ¬å¯¹çš„Fisherä¿¡æ¯ï¼Œå®ç°æ›´å¥½çš„ç¨³å®š-å¯å¡‘æ€§å¹³è¡¡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§è®¾ç½®ä¸‹å‡è¡¨ç°ä¼˜äºå…ˆå‰çš„ä¸»æµæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GIFTæ˜¯ä¸€ç§é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æŒç»­å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹é‡æ–°åˆ›å»ºé¢„è®­ç»ƒåŠä¸‹æ¸¸ä»»åŠ¡æ•°æ®ï¼Œä½¿VLMèƒ½å¤Ÿé€šè¿‡çŸ¥è¯†è’¸é¦å¤ä¹ æ—§çŸ¥è¯†ã€‚</li>
<li>é€šè¿‡åˆæˆå›¾åƒ-æ–‡æœ¬å¯¹åœ¨VLMç‰¹å¾ç©ºé—´ä¸­çš„å¹¿æ³›åˆ†å¸ƒå’Œé«˜åº¦å¯¹é½ç‰¹æ€§ï¼Œå¼•å…¥å¯¹æ¯”è’¸é¦æŸå¤±å’Œå›¾åƒ-æ–‡æœ¬å¯¹é½çº¦æŸã€‚</li>
<li>ä¸ºè§£å†³å†…éƒ¨åˆ†å¸ƒè¿‡æ‹Ÿåˆé—®é¢˜å¹¶å¢å¼ºæœ‰é™ç”Ÿæˆæ•°æ®çš„æ€§èƒ½ï¼Œé‡‡ç”¨è‡ªé€‚åº”æƒé‡æ•´åˆç­–ç•¥ã€‚</li>
<li>ç»“åˆåˆæˆå›¾åƒ-æ–‡æœ¬å¯¹çš„Fisherä¿¡æ¯ï¼Œå®ç°ç¨³å®š-å¯å¡‘æ€§ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒGIFTæ–¹æ³•åœ¨å„ç§è®¾ç½®ä¸‹å‡ä¼˜äºå…ˆå‰çš„ä¸»æµæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04229">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a9ef1f14fb63f74c76c82b4f3a845606.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f654ad0a8b76d9af538784bfadb15e64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ece8f619e0c4d9183827ae8a59829a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72a2591551c4ddc7a2b590ef2818d860.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Underlying-Semantic-Diffusion-for-Effective-and-Efficient-In-Context-Learning"><a href="#Underlying-Semantic-Diffusion-for-Effective-and-Efficient-In-Context-Learning" class="headerlink" title="Underlying Semantic Diffusion for Effective and Efficient In-Context   Learning"></a>Underlying Semantic Diffusion for Effective and Efficient In-Context   Learning</h2><p><strong>Authors:Zhong Ji, Weilong Cao, Yan Zhang, Yanwei Pang, Jungong Han, Xuelong Li</strong></p>
<p>Diffusion models has emerged as a powerful framework for tasks like image controllable generation and dense prediction. However, existing models often struggle to capture underlying semantics (e.g., edges, textures, shapes) and effectively utilize in-context learning, limiting their contextual understanding and image generation quality. Additionally, high computational costs and slow inference speeds hinder their real-time applicability. To address these challenges, we propose Underlying Semantic Diffusion (US-Diffusion), an enhanced diffusion model that boosts underlying semantics learning, computational efficiency, and in-context learning capabilities on multi-task scenarios. We introduce Separate &amp; Gather Adapter (SGA), which decouples input conditions for different tasks while sharing the architecture, enabling better in-context learning and generalization across diverse visual domains. We also present a Feedback-Aided Learning (FAL) framework, which leverages feedback signals to guide the model in capturing semantic details and dynamically adapting to task-specific contextual cues. Furthermore, we propose a plug-and-play Efficient Sampling Strategy (ESS) for dense sampling at time steps with high-noise levels, which aims at optimizing training and inference efficiency while maintaining strong in-context learning performance. Experimental results demonstrate that US-Diffusion outperforms the state-of-the-art method, achieving an average reduction of 7.47 in FID on Map2Image tasks and an average reduction of 0.026 in RMSE on Image2Map tasks, while achieving approximately 9.45 times faster inference speed. Our method also demonstrates superior training efficiency and in-context learning capabilities, excelling in new datasets and tasks, highlighting its robustness and adaptability across diverse visual domains. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²ç»æˆä¸ºå›¾åƒå¯æ§ç”Ÿæˆå’Œå¯†é›†é¢„æµ‹ç­‰ä»»åŠ¡çš„é‡è¦æ¡†æ¶ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹å¾€å¾€éš¾ä»¥æ•æ‰åº•å±‚è¯­ä¹‰ï¼ˆå¦‚è¾¹ç¼˜ã€çº¹ç†ã€å½¢çŠ¶ï¼‰ï¼Œå¹¶æœ‰æ•ˆåœ°åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œä»è€Œé™åˆ¶äº†å…¶ä¸Šä¸‹æ–‡ç†è§£å’Œå›¾åƒç”Ÿæˆè´¨é‡ã€‚æ­¤å¤–ï¼Œè¾ƒé«˜çš„è®¡ç®—æˆæœ¬å’Œç¼“æ…¢çš„æ¨ç†é€Ÿåº¦é˜»ç¢äº†å…¶åœ¨å®æ—¶åº”ç”¨ä¸­çš„é€‚ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åº•å±‚è¯­ä¹‰æ‰©æ•£ï¼ˆUS-Diffusionï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¢å¼ºçš„æ‰©æ•£æ¨¡å‹ï¼Œå¯ä»¥åœ¨å¤šä»»åŠ¡åœºæ™¯ä¸‹æé«˜åº•å±‚è¯­ä¹‰å­¦ä¹ ã€è®¡ç®—æ•ˆç‡å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†åˆ†ç¦»ä¸èšåˆé€‚é…å™¨ï¼ˆSGAï¼‰ï¼Œå®ƒè§£è€¦äº†ä¸åŒä»»åŠ¡çš„è¾“å…¥æ¡ä»¶ï¼ŒåŒæ—¶å…±äº«æ¶æ„ï¼Œä»è€Œå®ç°äº†æ›´å¥½çš„ä¸Šä¸‹æ–‡å­¦ä¹ å’Œè·¨ä¸åŒè§†è§‰é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†åé¦ˆè¾…åŠ©å­¦ä¹ ï¼ˆFALï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨åé¦ˆä¿¡å·æ¥æŒ‡å¯¼æ¨¡å‹æ•æ‰è¯­ä¹‰ç»†èŠ‚å¹¶æ ¹æ®ä»»åŠ¡ç‰¹å®šçš„ä¸Šä¸‹æ–‡çº¿ç´¢è¿›è¡ŒåŠ¨æ€é€‚åº”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å³æ’å³ç”¨çš„é«˜æ•ˆé‡‡æ ·ç­–ç•¥ï¼ˆESSï¼‰ï¼Œç”¨äºåœ¨è¾ƒé«˜å™ªå£°æ°´å¹³çš„æ—¶é—´æ­¥é•¿ä¸Šè¿›è¡Œå¯†é›†é‡‡æ ·ï¼Œæ—¨åœ¨ä¼˜åŒ–è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUS-Diffusionåœ¨Map2Imageä»»åŠ¡ä¸Šçš„FIDå¹³å‡é™ä½äº†7.47ï¼Œåœ¨Image2Mapä»»åŠ¡ä¸Šçš„RMSEå¹³å‡é™ä½äº†0.026ï¼ŒåŒæ—¶æ¨ç†é€Ÿåº¦æé«˜äº†çº¦9.45å€ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜å±•ç¤ºäº†å“è¶Šçš„è®­ç»ƒæ•ˆç‡å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œåœ¨æ–°çš„æ•°æ®é›†å’Œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œçªæ˜¾å…¶åœ¨ä¸åŒè§†è§‰é¢†åŸŸçš„ç¨³å¥æ€§å’Œé€‚åº”æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04050v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºå›¾åƒå¯æ§ç”Ÿæˆå’Œå¯†é›†é¢„æµ‹ç­‰ä»»åŠ¡ä¸­çš„å¼ºå¤§æ¡†æ¶ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹åœ¨æ•æ‰åº•å±‚è¯­ä¹‰ï¼ˆå¦‚è¾¹ç¼˜ã€çº¹ç†ã€å½¢çŠ¶ï¼‰å’Œæœ‰æ•ˆåˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œé™åˆ¶äº†å…¶ä¸Šä¸‹æ–‡ç†è§£å’Œå›¾åƒç”Ÿæˆè´¨é‡ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºUnderlying Semantic Diffusionï¼ˆUS-Diffusionï¼‰æ‰©æ•£æ¨¡å‹å¢å¼ºåº•å±‚è¯­ä¹‰å­¦ä¹ ã€è®¡ç®—æ•ˆç‡å’Œå¤šä»»åŠ¡çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚å¼•å…¥åˆ†ç¦»èšé›†é€‚é…å™¨ï¼ˆSGAï¼‰ï¼Œå®ƒå…è®¸å¯¹ä¸åŒä»»åŠ¡è¿›è¡Œè§£è€¦è¾“å…¥æ¡ä»¶ï¼ŒåŒæ—¶å…±äº«æ¶æ„ï¼Œå®ç°äº†æ›´å¥½çš„ä¸Šä¸‹æ–‡å­¦ä¹ å’Œè·¨ä¸åŒè§†è§‰é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†åé¦ˆè¾…åŠ©å­¦ä¹ ï¼ˆFALï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨åé¦ˆä¿¡å·æŒ‡å¯¼æ¨¡å‹æ•æ‰è¯­ä¹‰ç»†èŠ‚å¹¶æ ¹æ®ä»»åŠ¡ç‰¹å®šä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å³æ’å³ç”¨çš„é«˜æ•ˆé‡‡æ ·ç­–ç•¥ï¼ˆESSï¼‰ï¼Œé’ˆå¯¹é«˜å™ªå£°æ°´å¹³çš„æ—¶é—´æ­¥é•¿è¿›è¡Œå¯†é›†é‡‡æ ·ï¼Œæ—¨åœ¨ä¼˜åŒ–è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUS-Diffusionåœ¨Map2Imageä»»åŠ¡ä¸Šå¹³å‡å‡å°‘äº†7.47çš„FIDåˆ†æ•°ï¼Œåœ¨Image2Mapä»»åŠ¡ä¸Šå¹³å‡å‡å°‘äº†0.026çš„RMSEåˆ†æ•°ï¼ŒåŒæ—¶æ¨ç†é€Ÿåº¦æé«˜äº†çº¦9.45å€ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜å±•ç¤ºäº†å‡ºè‰²çš„è®­ç»ƒæ•ˆç‡å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œåœ¨æ–°æ•°æ®é›†å’Œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå‡¸æ˜¾å…¶åœ¨ä¸åŒè§†è§‰é¢†åŸŸçš„ç¨³å¥æ€§å’Œé€‚åº”æ€§ã€‚</p>
<p><strong>è¦ç‚¹åˆ†æ</strong></p>
<ul>
<li>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºå›¾åƒç”Ÿæˆå’Œé¢„æµ‹ä»»åŠ¡çš„é‡è¦å·¥å…·ã€‚ä½†ç°æœ‰æ¨¡å‹å­˜åœ¨è¯­ä¹‰æ•æ‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ çš„å±€é™æ€§ã€‚</li>
<li>US-Diffusionæ¨¡å‹æ—¨åœ¨è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œé€šè¿‡å¢å¼ºåº•å±‚è¯­ä¹‰å­¦ä¹ ã€è®¡ç®—æ•ˆç‡å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥åˆ†ç¦»èšé›†é€‚é…å™¨ï¼ˆSGAï¼‰ä»¥è§£è€¦ä¸åŒä»»åŠ¡çš„è¾“å…¥æ¡ä»¶å¹¶å…±äº«æ¶æ„ï¼Œå®ç°æ›´å¥½çš„ä¸Šä¸‹æ–‡å­¦ä¹ å’Œè·¨é¢†åŸŸæ³›åŒ–ã€‚</li>
<li>åé¦ˆè¾…åŠ©å­¦ä¹ ï¼ˆFALï¼‰æ¡†æ¶åˆ©ç”¨åé¦ˆä¿¡å·æ¥æŒ‡å¯¼æ¨¡å‹æ•æ‰è¯­ä¹‰ç»†èŠ‚å¹¶é€‚åº”ä»»åŠ¡ç‰¹å®šçš„ä¸Šä¸‹æ–‡ã€‚</li>
<li>æå‡ºé«˜æ•ˆé‡‡æ ·ç­–ç•¥ï¼ˆESSï¼‰ï¼Œæ—¨åœ¨ä¼˜åŒ–è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0febfdbe65da464440b045c6a534a9f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ccbfecfbb3e0729f06b36743f23d3c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a6b13ee51196819e727ceb99ede0536.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TextDoctor-Unified-Document-Image-Inpainting-via-Patch-Pyramid-Diffusion-Models"><a href="#TextDoctor-Unified-Document-Image-Inpainting-via-Patch-Pyramid-Diffusion-Models" class="headerlink" title="TextDoctor: Unified Document Image Inpainting via Patch Pyramid   Diffusion Models"></a>TextDoctor: Unified Document Image Inpainting via Patch Pyramid   Diffusion Models</h2><p><strong>Authors:Wanglong Lu, Lingming Su, Jingjing Zheng, VinÃ­cius Veloso de Melo, Farzaneh Shoeleh, John Hawkin, Terrence Tricco, Hanli Zhao, Xianta Jiang</strong></p>
<p>Digital versions of real-world text documents often suffer from issues like environmental corrosion of the original document, low-quality scanning, or human interference. Existing document restoration and inpainting methods typically struggle with generalizing to unseen document styles and handling high-resolution images. To address these challenges, we introduce TextDoctor, a novel unified document image inpainting method. Inspired by human reading behavior, TextDoctor restores fundamental text elements from patches and then applies diffusion models to entire document images instead of training models on specific document types. To handle varying text sizes and avoid out-of-memory issues, common in high-resolution documents, we propose using structure pyramid prediction and patch pyramid diffusion models. These techniques leverage multiscale inputs and pyramid patches to enhance the quality of inpainting both globally and locally. Extensive qualitative and quantitative experiments on seven public datasets validated that TextDoctor outperforms state-of-the-art methods in restoring various types of high-resolution document images. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œæ–‡æœ¬æ–‡ä»¶çš„æ•°å­—ç‰ˆæœ¬é€šå¸¸é¢ä¸´åŸå§‹æ–‡ä»¶ç¯å¢ƒè…èš€ã€æ‰«æè´¨é‡ä½æˆ–äººä¸ºå¹²æ‰°ç­‰é—®é¢˜ã€‚ç°æœ‰çš„æ–‡æ¡£æ¢å¤å’Œä¿®å¤æ–¹æ³•é€šå¸¸åœ¨æ¨å¹¿åˆ°æœªè§è¿‡çš„æ–‡æ¡£é£æ ¼å’Œå¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ—¶é‡åˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†TextDoctorï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ç»Ÿä¸€æ–‡æ¡£å›¾åƒä¿®å¤æ–¹æ³•ã€‚TextDoctorå—äººç±»é˜…è¯»è¡Œä¸ºçš„å¯å‘ï¼Œé¦–å…ˆæ¢å¤æ–‡æœ¬çš„åŸºæœ¬å…ƒç´ ï¼Œç„¶åå¯¹æ•´ä¸ªæ–‡æ¡£å›¾åƒåº”ç”¨æ‰©æ•£æ¨¡å‹ï¼Œè€Œä¸æ˜¯åœ¨ç‰¹å®šæ–‡æ¡£ç±»å‹ä¸Šè®­ç»ƒæ¨¡å‹ã€‚ä¸ºäº†å¤„ç†ä¸åŒçš„æ–‡æœ¬å¤§å°å¹¶é¿å…é«˜åˆ†è¾¨ç‡æ–‡æ¡£ä¸­å¸¸è§çš„å†…å­˜ä¸è¶³é—®é¢˜ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ç»“æ„é‡‘å­—å¡”é¢„æµ‹å’Œè¡¥ä¸é‡‘å­—å¡”æ‰©æ•£æ¨¡å‹ã€‚è¿™äº›æŠ€æœ¯åˆ©ç”¨å¤šå°ºåº¦è¾“å…¥å’Œé‡‘å­—å¡”è¡¥ä¸æ¥æé«˜å…¨å±€å’Œå±€éƒ¨ä¿®å¤çš„è´¨é‡ã€‚åœ¨ä¸ƒä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®šæ€§å’Œå®šé‡å®éªŒéªŒè¯äº†TextDoctoråœ¨æ¢å¤å„ç§ç±»å‹çš„é«˜åˆ†è¾¨ç‡æ–‡æ¡£å›¾åƒæ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04021v1">PDF</a> 28 pages, 25 figures</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬DocumentDoctoré’ˆå¯¹æ•°å­—ç‰ˆç°å®ä¸–ç•Œæ–‡æœ¬æ–‡ä»¶å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚åŸå§‹æ–‡æ¡£çš„ç¯å¢ƒè…èš€ã€æ‰«æè´¨é‡ä½æˆ–äººä¸ºå¹²æ‰°ç­‰ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç»Ÿä¸€æ–‡æ¡£å›¾åƒä¿®å¤æ–¹æ³•ã€‚è¯¥æ–¹æ³•å—åˆ°äººç±»é˜…è¯»è¡Œä¸ºçš„å¯å‘ï¼Œä»è¡¥ä¸ä¸­æ¢å¤åŸºæœ¬æ–‡æœ¬å…ƒç´ ï¼Œç„¶ååº”ç”¨æ‰©æ•£æ¨¡å‹äºæ•´ä¸ªæ–‡æ¡£å›¾åƒï¼Œè€Œä¸æ˜¯é’ˆå¯¹ç‰¹å®šæ–‡æ¡£ç±»å‹è®­ç»ƒæ¨¡å‹ã€‚é’ˆå¯¹é«˜åˆ†è¾¨ç‡æ–‡æ¡£ä¸­å¸¸è§çš„æ–‡æœ¬å¤§å°ä¸åŒå’Œå†…å­˜ä¸è¶³é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç»“æ„é‡‘å­—å¡”é¢„æµ‹å’Œè¡¥ä¸é‡‘å­—å¡”æ‰©æ•£æ¨¡å‹ç­‰è§£å†³æ–¹æ¡ˆã€‚è¿™äº›æ–¹æ³•åˆ©ç”¨å¤šå°ºåº¦è¾“å…¥å’Œé‡‘å­—å¡”è¡¥ä¸ï¼Œæé«˜äº†å…¨å±€å’Œå±€éƒ¨ä¿®å¤çš„è´¨é‡ã€‚åœ¨ä¸ƒä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®šæ€§å’Œå®šé‡å®éªŒéªŒè¯äº†TextDoctoråœ¨æ¢å¤å„ç§ç±»å‹çš„é«˜åˆ†è¾¨ç‡æ–‡æ¡£å›¾åƒæ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TextDoctoræ˜¯ä¸€ç§æ–°çš„æ–‡æ¡£å›¾åƒä¿®å¤æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ•°å­—ç‰ˆç°å®ä¸–ç•Œæ–‡æœ¬æ–‡ä»¶å­˜åœ¨çš„é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿä»è¡¥ä¸ä¸­æ¢å¤åŸºæœ¬æ–‡æœ¬å…ƒç´ ï¼Œå¹¶åº”ç”¨æ‰©æ•£æ¨¡å‹äºæ•´ä¸ªæ–‡æ¡£å›¾åƒã€‚</li>
<li>TextDoctorçš„çµæ„Ÿæ¥æºäºäººç±»é˜…è¯»è¡Œä¸ºï¼Œèƒ½å¤Ÿå¤„ç†ä¸åŒçš„æ–‡æ¡£ç±»å‹ã€‚</li>
<li>ä¸ºäº†åº”å¯¹é«˜åˆ†è¾¨ç‡æ–‡æ¡£ä¸­å¸¸è§çš„æ–‡æœ¬å¤§å°ä¸åŒå’Œå†…å­˜ä¸è¶³é—®é¢˜ï¼ŒTextDoctoré‡‡ç”¨äº†ç»“æ„é‡‘å­—å¡”é¢„æµ‹å’Œè¡¥ä¸é‡‘å­—å¡”æ‰©æ•£æ¨¡å‹ç­‰æŠ€æœ¯ã€‚</li>
<li>è¿™äº›æŠ€æœ¯åˆ©ç”¨å¤šå°ºåº¦è¾“å…¥å’Œé‡‘å­—å¡”è¡¥ä¸ï¼Œæé«˜äº†ä¿®å¤è´¨é‡ï¼Œæ—¢å…¨å±€åˆå±€éƒ¨ã€‚</li>
<li>TextDoctoråœ¨ä¸ƒä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº†å…¶åœ¨æ¢å¤é«˜åˆ†è¾¨ç‡æ–‡æ¡£å›¾åƒæ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04021">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b65f96fbb7e74c9ed8226df787f0f15f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3b1d62e180c2fed98efedc741ecd88e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b7c99730e335994411760dbd58e7fa0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fec45272ccc6ebae38a1327271af480.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Positive-Unlabeled-Diffusion-Models-for-Preventing-Sensitive-Data-Generation"><a href="#Positive-Unlabeled-Diffusion-Models-for-Preventing-Sensitive-Data-Generation" class="headerlink" title="Positive-Unlabeled Diffusion Models for Preventing Sensitive Data   Generation"></a>Positive-Unlabeled Diffusion Models for Preventing Sensitive Data   Generation</h2><p><strong>Authors:Hiroshi Takahashi, Tomoharu Iwata, Atsutoshi Kumagai, Yuuki Yamanaka, Tomoya Yamashita</strong></p>
<p>Diffusion models are powerful generative models but often generate sensitive data that are unwanted by users, mainly because the unlabeled training data frequently contain such sensitive data. Since labeling all sensitive data in the large-scale unlabeled training data is impractical, we address this problem by using a small amount of labeled sensitive data. In this paper, we propose positive-unlabeled diffusion models, which prevent the generation of sensitive data using unlabeled and sensitive data. Our approach can approximate the evidence lower bound (ELBO) for normal (negative) data using only unlabeled and sensitive (positive) data. Therefore, even without labeled normal data, we can maximize the ELBO for normal data and minimize it for labeled sensitive data, ensuring the generation of only normal data. Through experiments across various datasets and settings, we demonstrated that our approach can prevent the generation of sensitive images without compromising image quality. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§å¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œä½†ç»å¸¸ä¼šç”Ÿæˆç”¨æˆ·ä¸éœ€è¦çš„æ•æ„Ÿæ•°æ®ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºæœªæ ‡è®°çš„è®­ç»ƒæ•°æ®ç»å¸¸åŒ…å«è¿™ç§æ•æ„Ÿæ•°æ®ã€‚ç”±äºåœ¨å¤§è§„æ¨¡æœªæ ‡è®°çš„è®­ç»ƒæ•°æ®ä¸­æ ‡è®°æ‰€æœ‰æ•æ„Ÿæ•°æ®æ˜¯ä¸åˆ‡å®é™…çš„ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨å°‘é‡æ ‡è®°çš„æ•æ„Ÿæ•°æ®æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ­£è´Ÿæœªæ ‡è®°æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨æœªæ ‡è®°å’Œæ•æ„Ÿæ•°æ®æ¥é˜²æ­¢æ•æ„Ÿæ•°æ®çš„ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä½¿ç”¨ä»…æœªæ ‡è®°çš„ï¼ˆæ­£å¸¸ï¼‰ï¼ˆé˜´æ€§ï¼‰æ•°æ®æ¥è¿‘ä¼¼æ­£å¸¸æ•°æ®çš„è¯æ®ä¸‹é™ï¼ˆELBOï¼‰ï¼Œå› æ­¤ï¼Œå³ä½¿æ²¡æœ‰æ ‡è®°çš„æ­£å¸¸æ•°æ®ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æœ€å¤§åŒ–æ­£å¸¸æ•°æ®çš„ELBOå¹¶æœ€å°åŒ–æ ‡è®°æ•æ„Ÿæ•°æ®çš„ELBOï¼Œç¡®ä¿åªç”Ÿæˆæ­£å¸¸æ•°æ®ã€‚é€šè¿‡åœ¨ä¸åŒæ•°æ®é›†å’Œè®¾ç½®ä¸Šçš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥é˜²æ­¢ç”Ÿæˆæ•æ„Ÿå›¾åƒï¼ŒåŒæ—¶ä¸ä¼šæŸå®³å›¾åƒè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03789v1">PDF</a> Accepted at ICLR2025. Code is available at   <a target="_blank" rel="noopener" href="https://github.com/takahashihiroshi/pudm">https://github.com/takahashihiroshi/pudm</a></p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹é€šè¿‡å°‘é‡æ ‡è®°æ•æ„Ÿæ•°æ®ç”Ÿæˆæ­£å‘æœªæ ‡è®°æ‰©æ•£æ¨¡å‹ï¼Œé˜²æ­¢ç”Ÿæˆæ•æ„Ÿæ•°æ®ã€‚æ­¤æ–¹æ³•æ— éœ€æ ‡è®°æ­£å¸¸æ•°æ®ï¼Œå³å¯æœ€å¤§åŒ–æ­£å¸¸æ•°æ®çš„è¯æ®ä¸‹é™ï¼ˆELBOï¼‰ï¼Œæœ€å°åŒ–æ ‡è®°æ•æ„Ÿæ•°æ®çš„ELBOï¼Œç¡®ä¿ä»…ç”Ÿæˆæ­£å¸¸æ•°æ®ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆé˜²æ­¢ç”Ÿæˆæ•æ„Ÿå›¾åƒï¼ŒåŒæ—¶ä¿è¯å›¾åƒè´¨é‡ä¸å—å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œä½†å¯èƒ½ä¼šç”Ÿæˆç”¨æˆ·ä¸éœ€è¦çš„æ•æ„Ÿæ•°æ®ã€‚</li>
<li>æ•æ„Ÿæ•°æ®ç»å¸¸å‡ºç°åœ¨æœªæ ‡è®°çš„è®­ç»ƒæ•°æ®ä¸­ã€‚</li>
<li>ä½¿ç”¨å°‘é‡æ ‡è®°æ•æ„Ÿæ•°æ®æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>æå‡ºæ­£å‘æœªæ ‡è®°æ‰©æ•£æ¨¡å‹ï¼Œåˆ©ç”¨æœªæ ‡è®°å’Œæ•æ„Ÿæ•°æ®é˜²æ­¢ç”Ÿæˆæ•æ„Ÿæ•°æ®ã€‚</li>
<li>é€šè¿‡æœ€å¤§åŒ–æ­£å¸¸æ•°æ®çš„è¯æ®ä¸‹é™ï¼ˆELBOï¼‰å’Œæœ€å°åŒ–æ ‡è®°æ•æ„Ÿæ•°æ®çš„ELBOæ¥å®ç°ã€‚</li>
<li>æ— éœ€æ ‡è®°æ­£å¸¸æ•°æ®å³å¯å®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03789">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b6c2323f04d89b448c90249a5aa61750.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a6436213afdb231c3be6c2d5fb2671d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Tackling-Few-Shot-Segmentation-in-Remote-Sensing-via-Inpainting-Diffusion-Model"><a href="#Tackling-Few-Shot-Segmentation-in-Remote-Sensing-via-Inpainting-Diffusion-Model" class="headerlink" title="Tackling Few-Shot Segmentation in Remote Sensing via Inpainting   Diffusion Model"></a>Tackling Few-Shot Segmentation in Remote Sensing via Inpainting   Diffusion Model</h2><p><strong>Authors:Steve Andreas Immanuel, Woojin Cho, Junhyuk Heo, Darongsae Kwon</strong></p>
<p>Limited data is a common problem in remote sensing due to the high cost of obtaining annotated samples. In the few-shot segmentation task, models are typically trained on base classes with abundant annotations and later adapted to novel classes with limited examples. However, this often necessitates specialized model architectures or complex training strategies. Instead, we propose a simple approach that leverages diffusion models to generate diverse variations of novel-class objects within a given scene, conditioned by the limited examples of the novel classes. By framing the problem as an image inpainting task, we synthesize plausible instances of novel classes under various environments, effectively increasing the number of samples for the novel classes and mitigating overfitting. The generated samples are then assessed using a cosine similarity metric to ensure semantic consistency with the novel classes. Additionally, we employ Segment Anything Model (SAM) to segment the generated samples and obtain precise annotations. By using high-quality synthetic data, we can directly fine-tune off-the-shelf segmentation models. Experimental results demonstrate that our method significantly enhances segmentation performance in low-data regimes, highlighting its potential for real-world remote sensing applications. </p>
<blockquote>
<p>åœ¨é¥æ„Ÿé¢†åŸŸï¼Œç”±äºè·å–æ ‡æ³¨æ ·æœ¬çš„æˆæœ¬é«˜æ˜‚ï¼Œæ•°æ®æœ‰é™æ˜¯ä¸€ä¸ªå¸¸è§é—®é¢˜ã€‚åœ¨å°‘é‡åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹é€šå¸¸ä¼šåœ¨åŸºç¡€ç±»åˆ«ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™äº›åŸºç¡€ç±»åˆ«æ‹¥æœ‰ä¸°å¯Œçš„æ ‡æ³¨ï¼Œç„¶åé€‚åº”å…·æœ‰æœ‰é™ç¤ºä¾‹çš„æ–°ç±»åˆ«ã€‚ç„¶è€Œï¼Œè¿™é€šå¸¸éœ€è¦ä¸“é—¨çš„æ¨¡å‹æ¶æ„æˆ–å¤æ‚çš„è®­ç»ƒç­–ç•¥ã€‚ç›¸åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹åœ¨ç»™å®šåœºæ™¯å†…ç”Ÿæˆå¤šç§æ–°å‹ç±»åˆ«å¯¹è±¡çš„å˜ä½“ï¼Œè¿™äº›å˜ä½“ä»¥æ–°ç±»åˆ«çš„æœ‰é™ç¤ºä¾‹ä¸ºæ¡ä»¶ã€‚é€šè¿‡å°†é—®é¢˜æ„é€ æˆå›¾åƒä¿®å¤ä»»åŠ¡ï¼Œæˆ‘ä»¬åœ¨å„ç§ç¯å¢ƒä¸‹åˆæˆæ–°ç±»åˆ«çš„åˆç†å®ä¾‹ï¼Œæœ‰æ•ˆåœ°å¢åŠ äº†æ–°ç±»åˆ«çš„æ ·æœ¬æ•°é‡ï¼Œå¹¶å‡è½»äº†è¿‡æ‹Ÿåˆã€‚ç”Ÿæˆçš„æ ·æœ¬ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦åº¦é‡è¿›è¡Œè¯„ä¼°ï¼Œä»¥ç¡®ä¿ä¸æ–°ç±»åˆ«è¯­ä¹‰ä¸€è‡´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨ä»»æ„åˆ†å‰²æ¨¡å‹ï¼ˆSAMï¼‰å¯¹ç”Ÿæˆçš„æ ·æœ¬è¿›è¡Œåˆ†å‰²ï¼Œä»¥è·å¾—ç²¾ç¡®æ ‡æ³¨ã€‚é€šè¿‡ä½¿ç”¨é«˜è´¨é‡çš„åˆæˆæ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥å¯¹ç°æˆçš„åˆ†å‰²æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•°æ®ç¨€å°‘çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜äº†åˆ†å‰²æ€§èƒ½ï¼Œçªæ˜¾å…¶åœ¨ç°å®ä¸–ç•Œé¥æ„Ÿåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03785v1">PDF</a> Accepted to ICLRW 2025 (Oral)</p>
<p><strong>Summary</strong></p>
<p>åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼Œé’ˆå¯¹é¥æ„Ÿé¢†åŸŸæœ‰é™æ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç®€å•çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆåœºæ™¯ä¸­æ–°å‹ç‰©ä½“å¤šæ ·åŒ–çš„å˜ä½“ï¼Œä»¥æœ‰é™çš„æ ·æœ¬ä¸ºæ¡ä»¶ï¼Œè§£å†³äº†æ ·æœ¬é‡è¾ƒå°‘çš„é—®é¢˜ã€‚é€šè¿‡å°†é—®é¢˜è½¬åŒ–ä¸ºå›¾åƒè¡¥å…¨ä»»åŠ¡ï¼Œåˆæˆå„ç§ç¯å¢ƒä¸‹æ–°å‹ç‰©ä½“çš„åˆç†å®ä¾‹ï¼Œå¢åŠ äº†æ–°å‹æ ·æœ¬çš„æ•°é‡ï¼Œæœ‰æ•ˆç¼“è§£äº†è¿‡æ‹Ÿåˆç°è±¡ã€‚ç”Ÿæˆçš„æ ·æœ¬ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼æ€§åº¦é‡è¿›è¡Œè¯„ä¼°ï¼Œç¡®ä¿ä¸æ–°å‹ç±»è¯­ä¹‰çš„ä¸€è‡´æ€§ã€‚åŒæ—¶ï¼Œé‡‡ç”¨åˆ†æ®µæ¨¡å‹å¯¹ç”Ÿæˆæ ·æœ¬è¿›è¡Œç²¾ç¡®æ ‡æ³¨ï¼Œå¹¶å¯ç›´æ¥å¯¹è´§æ¶ä¸Šçš„åˆ†å‰²æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹æ˜¾è‘—æé«˜åˆ†å‰²æ€§èƒ½ï¼Œçªæ˜¾å…¶åœ¨ç°å®é¥æ„Ÿåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºç”Ÿæˆæ–°å‹ç‰©ä½“å¤šæ ·åŒ–çš„å˜ä½“ï¼Œè§£å†³é¥æ„Ÿé¢†åŸŸæœ‰é™æ•°æ®é—®é¢˜ã€‚</li>
<li>æ–¹æ³•å°†é—®é¢˜è½¬åŒ–ä¸ºå›¾åƒè¡¥å…¨ä»»åŠ¡ï¼Œåˆæˆå„ç§ç¯å¢ƒä¸‹çš„æ–°å‹ç‰©ä½“å®ä¾‹ã€‚</li>
<li>ç”Ÿæˆæ ·æœ¬é€šè¿‡ä½™å¼¦ç›¸ä¼¼æ€§åº¦é‡è¯„ä¼°ï¼Œç¡®ä¿ä¸æ–°å‹ç±»çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>é‡‡ç”¨Segment Anything Modelï¼ˆSAMï¼‰å¯¹ç”Ÿæˆæ ·æœ¬è¿›è¡Œç²¾ç¡®æ ‡æ³¨ã€‚</li>
<li>ä½¿ç”¨é«˜è´¨é‡åˆæˆæ•°æ®å¯ç›´æ¥å¾®è°ƒè´§æ¶ä¸Šçš„åˆ†å‰²æ¨¡å‹ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜è¯¥æ–¹æ³•åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹æ˜¾è‘—æé«˜åˆ†å‰²æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03785">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6713c5cf68538c4917bcdaf08a2892d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c2f1999084763db57955f04b7650749.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe2bb63831237921c2d9b431240e513a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-203caff4fab9bda8d4835d4d9a088b3c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Efficient-Diversity-Preserving-Diffusion-Alignment-via-Gradient-Informed-GFlowNets"><a href="#Efficient-Diversity-Preserving-Diffusion-Alignment-via-Gradient-Informed-GFlowNets" class="headerlink" title="Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed   GFlowNets"></a>Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed   GFlowNets</h2><p><strong>Authors:Zhen Liu, Tim Z. Xiao, Weiyang Liu, Yoshua Bengio, Dinghuai Zhang</strong></p>
<p>While one commonly trains large diffusion models by collecting datasets on target downstream tasks, it is often desired to align and finetune pretrained diffusion models with some reward functions that are either designed by experts or learned from small-scale datasets. Existing post-training methods for reward finetuning of diffusion models typically suffer from lack of diversity in generated samples, lack of prior preservation, and&#x2F;or slow convergence in finetuning. Inspired by recent successes in generative flow networks (GFlowNets), a class of probabilistic models that sample with the unnormalized density of a reward function, we propose a novel GFlowNet method dubbed Nabla-GFlowNet (abbreviated as \methodname), the first GFlowNet method that leverages the rich signal in reward gradients, together with an objective called \graddb plus its variant \resgraddb designed for prior-preserving diffusion finetuning. We show that our proposed method achieves fast yet diversity- and prior-preserving finetuning of Stable Diffusion, a large-scale text-conditioned image diffusion model, on different realistic reward functions. </p>
<blockquote>
<p>é€šå¸¸ï¼Œäººä»¬é€šè¿‡æ”¶é›†ç›®æ ‡ä¸‹æ¸¸ä»»åŠ¡çš„æ•°æ®é›†æ¥è®­ç»ƒå¤§å‹æ‰©æ•£æ¨¡å‹ï¼Œä½†äººä»¬å¸¸å¸¸å¸Œæœ›é€šè¿‡ä¸ä¸“å®¶è®¾è®¡æˆ–ä»å°å‹æ•°æ®é›†ä¸­å­¦ä¹ å¾—åˆ°çš„å¥–åŠ±å‡½æ•°å¯¹é½å¹¶å¾®è°ƒé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ã€‚ç°æœ‰çš„ç”¨äºæ‰©æ•£æ¨¡å‹å¥–åŠ±å¾®è°ƒçš„åæœŸè®­ç»ƒæ–¹æ³•é€šå¸¸å­˜åœ¨ç”Ÿæˆæ ·æœ¬ç¼ºä¹å¤šæ ·æ€§ã€ç¼ºä¹å…ˆéªŒä¿ç•™ä»¥åŠå¾®è°ƒæ”¶æ•›ç¼“æ…¢ç­‰é—®é¢˜ã€‚å—æœ€è¿‘ç”Ÿæˆæµç½‘ç»œï¼ˆGFlowNetsï¼‰æˆåŠŸçš„å¯å‘ï¼ŒGFlowNetsæ˜¯ä¸€ç±»ä»¥å¥–åŠ±å‡½æ•°çš„æœªå½’ä¸€åŒ–å¯†åº¦è¿›è¡Œé‡‡æ ·çš„æ¦‚ç‡æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„GFlowNetæ–¹æ³•ï¼Œç§°ä¸ºNabla-GFlowNetï¼ˆç®€ç§°\methodnameï¼‰ï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªåˆ©ç”¨å¥–åŠ±æ¢¯åº¦ä¸°å¯Œä¿¡å·çš„GFlowNetæ–¹æ³•ï¼Œä»¥åŠä¸€ä¸ªç§°ä¸º\graddbçš„ç›®æ ‡åŠå…¶å˜ä½“\resgraddbï¼Œä¸“ä¸ºä¿ç•™å…ˆéªŒçš„æ‰©æ•£å¾®è°ƒè®¾è®¡ã€‚æˆ‘ä»¬å±•ç¤ºï¼Œæ‰€ææ–¹æ³•èƒ½å¤Ÿåœ¨ä¸åŒçš„ç°å®å¥–åŠ±å‡½æ•°ä¸Šå¿«é€Ÿä¸”å¤šæ ·ã€ä¿ç•™å…ˆéªŒåœ°å¯¹å¤§è§„æ¨¡æ–‡æœ¬æ¡ä»¶å›¾åƒæ‰©æ•£æ¨¡å‹Stable Diffusionè¿›è¡Œå¾®è°ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07775v2">PDF</a> Technical Report (35 pages, 31 figures), Accepted at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å¯ä»¥é€šè¿‡ä¸ä¸“å®¶è®¾è®¡æˆ–ä»å°è§„æ¨¡æ•°æ®é›†ä¸­å­¦ä¹ çš„å¥–åŠ±å‡½æ•°è¿›è¡Œå¯¹é½å’Œå¾®è°ƒï¼Œæ¥æé«˜å…¶æ€§èƒ½ã€‚ç°æœ‰çš„æ‰©æ•£æ¨¡å‹å¥–åŠ±å¾®è°ƒçš„åè®­ç»ƒæ–¹æ³•é€šå¸¸å­˜åœ¨ç”Ÿæˆæ ·æœ¬ç¼ºä¹å¤šæ ·æ€§ã€ç¼ºä¹å…ˆéªŒçŸ¥è¯†ä¿ç•™ä»¥åŠå¾®è°ƒæ”¶æ•›ç¼“æ…¢çš„é—®é¢˜ã€‚å—ç”Ÿæˆæµç½‘ç»œï¼ˆGFlowNetsï¼‰è¿‘æœŸæˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„GFlowNetæ–¹æ³•ï¼Œåä¸ºNabla-GFlowNetï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªåˆ©ç”¨å¥–åŠ±æ¢¯åº¦ä¸°å¯Œä¿¡å·çš„GFlowNetæ–¹æ³•ï¼Œé…åˆç§°ä¸º\graddbçš„ç›®æ ‡åŠå…¶å˜ä½“\resgraddbï¼Œç”¨äºå…ˆéªŒä¿ç•™çš„æ‰©æ•£å¾®è°ƒã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€ææ–¹æ³•åœ¨ä¸åŒçš„çœŸå®å¥–åŠ±å‡½æ•°ä¸Šï¼Œå®ç°äº†å¯¹å¤§è§„æ¨¡æ–‡æœ¬æ¡ä»¶å›¾åƒæ‰©æ•£æ¨¡å‹Stable Diffusionçš„å¿«é€Ÿã€å¤šæ ·æ€§å’Œå…ˆéªŒçŸ¥è¯†ä¿ç•™çš„å¾®è°ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å¯ä»¥é€šè¿‡ä¸å¥–åŠ±å‡½æ•°å¯¹é½å’Œå¾®è°ƒæ¥æé«˜æ€§èƒ½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨ç”Ÿæˆæ ·æœ¬ç¼ºä¹å¤šæ ·æ€§å’Œå…ˆéªŒçŸ¥è¯†ä¿ç•™çš„é—®é¢˜ã€‚</li>
<li>Nabla-GFlowNetæ˜¯ä¸€ç§æ–°çš„GFlowNetæ–¹æ³•ï¼Œåˆ©ç”¨å¥–åŠ±æ¢¯åº¦ä¸°å¯Œä¿¡å·ã€‚</li>
<li>Nabla-GFlowNeté…åˆ\graddbåŠå…¶å˜ä½“\resgraddbï¼Œç”¨äºå…ˆéªŒä¿ç•™çš„æ‰©æ•£å¾®è°ƒã€‚</li>
<li>æ‰€ææ–¹æ³•åœ¨ä¸åŒçœŸå®å¥–åŠ±å‡½æ•°ä¸Šå®ç°äº†å¯¹Stable Diffusionæ¨¡å‹çš„å¿«é€Ÿã€å¤šæ ·æ€§å’Œå…ˆéªŒçŸ¥è¯†ä¿ç•™çš„å¾®è°ƒã€‚</li>
<li>Stable Diffusionæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡æ–‡æœ¬æ¡ä»¶å›¾åƒæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>Nabla-GFlowNetæ–¹æ³•å…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ï¼Œå¯æ¨å¹¿åˆ°å…¶ä»–é¢†åŸŸçš„æ‰©æ•£æ¨¡å‹å¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07775">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-566ef6fcce08e6ec39393b0d7128a0c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d874aa872307bde9e8a53820468e24f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="UniMLVG-Unified-Framework-for-Multi-view-Long-Video-Generation-with-Comprehensive-Control-Capabilities-for-Autonomous-Driving"><a href="#UniMLVG-Unified-Framework-for-Multi-view-Long-Video-Generation-with-Comprehensive-Control-Capabilities-for-Autonomous-Driving" class="headerlink" title="UniMLVG: Unified Framework for Multi-view Long Video Generation with   Comprehensive Control Capabilities for Autonomous Driving"></a>UniMLVG: Unified Framework for Multi-view Long Video Generation with   Comprehensive Control Capabilities for Autonomous Driving</h2><p><strong>Authors:Rui Chen, Zehuan Wu, Yichen Liu, Yuxin Guo, Jingcheng Ni, Haifeng Xia, Siyu Xia</strong></p>
<p>The creation of diverse and realistic driving scenarios has become essential to enhance perception and planning capabilities of the autonomous driving system. However, generating long-duration, surround-view consistent driving videos remains a significant challenge. To address this, we present UniMLVG, a unified framework designed to generate extended street multi-perspective videos under precise control. By integrating single- and multi-view driving videos into the training data, our approach updates a DiT-based diffusion model equipped with cross-frame and cross-view modules across three stages with multi training objectives, substantially boosting the diversity and quality of generated visual content. Importantly, we propose an innovative explicit viewpoint modeling approach for multi-view video generation to effectively improve motion transition consistency. Capable of handling various input reference formats (e.g., text, images, or video), our UniMLVG generates high-quality multi-view videos according to the corresponding condition constraints such as 3D bounding boxes or frame-level text descriptions. Compared to the best models with similar capabilities, our framework achieves improvements of 48.2% in FID and 35.2% in FVD. </p>
<blockquote>
<p>åˆ›å»ºå¤šæ ·ä¸”é€¼çœŸçš„é©¾é©¶åœºæ™¯å¯¹äºæå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ„ŸçŸ¥å’Œè§„åˆ’èƒ½åŠ›è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”Ÿæˆé•¿æ—¶é—´ã€å…¨æ™¯ä¸€è‡´çš„é©¾é©¶è§†é¢‘ä»ç„¶æ˜¯ä¸€é¡¹é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†UniMLVGï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨åœ¨ç²¾ç¡®æ§åˆ¶ä¸‹ç”Ÿæˆæ‰©å±•è¡—é“å¤šè§†è§’è§†é¢‘ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å°†å•è§†è§’å’Œå¤šè§†è§’é©¾é©¶è§†é¢‘èå…¥è®­ç»ƒæ•°æ®ï¼Œæ›´æ–°äº†ä¸€ä¸ªé…å¤‡è·¨å¸§å’Œè·¨è§†è§’æ¨¡å—çš„ä¸‰é˜¶æ®µDiTæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡å¤šè®­ç»ƒç›®æ ‡å¤§å¹…æå‡äº†ç”Ÿæˆè§†è§‰å†…å®¹çš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„å¤šè§†è§’è§†é¢‘ç”Ÿæˆçš„æ˜¾å¼è§†ç‚¹å»ºæ¨¡æ–¹æ³•ï¼Œæœ‰æ•ˆæé«˜è¿åŠ¨è¿‡æ¸¡çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„UniMLVGèƒ½å¤Ÿå¤„ç†å„ç§è¾“å…¥å‚è€ƒæ ¼å¼ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒæˆ–è§†é¢‘ï¼‰ï¼Œæ ¹æ®ç›¸åº”çš„æ¡ä»¶çº¦æŸï¼ˆå¦‚3Dè¾¹ç•Œæ¡†æˆ–å¸§çº§æ–‡æœ¬æè¿°ï¼‰ç”Ÿæˆé«˜è´¨é‡çš„å¤šè§†è§’è§†é¢‘ã€‚ä¸å…·æœ‰ç±»ä¼¼èƒ½åŠ›çš„æœ€ä½³æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨FIDä¸Šæé«˜äº†48.2%ï¼Œåœ¨FVDä¸Šæé«˜äº†35.2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04842v3">PDF</a> </p>
<p><strong>Summary</strong><br>è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ„ŸçŸ¥å’Œè§„åˆ’èƒ½åŠ›çš„æå‡ç¦»ä¸å¼€å¤šæ ·åŒ–ä¸”çœŸå®çš„é©¾é©¶åœºæ™¯çš„ç”Ÿæˆã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æ¨å‡ºUniMLVGç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨ç”Ÿæˆæ‰©å±•è¡—é“å¤šè§†è§’è§†é¢‘ã€‚è¯¥æ¡†æ¶é›†æˆå•è§†è§’å’Œå¤šè§†è§’é©¾é©¶è§†é¢‘åˆ°è®­ç»ƒæ•°æ®ä¸­ï¼Œé€šè¿‡æ›´æ–°åŸºäºDiTçš„æ‰©æ•£æ¨¡å‹å¹¶é…å¤‡è·¨å¸§å’Œè·¨è§†æ¨¡å—ï¼Œåœ¨ä¸‰ä¸ªé˜¶æ®µè¿›è¡Œå¤šä»»åŠ¡è®­ç»ƒï¼Œæ˜¾è‘—æé«˜ç”Ÿæˆè§†é¢‘å†…å®¹çš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚æ­¤å¤–ï¼Œæå‡ºæ˜ç¡®çš„å¤šè§†è§’è§†é¢‘ç”Ÿæˆè§†ç‚¹å»ºæ¨¡æ–¹æ³•ï¼Œæœ‰æ•ˆæ”¹å–„åŠ¨ä½œè¿‡æ¸¡ä¸€è‡´æ€§ã€‚UniMLVGèƒ½å¤Ÿå¤„ç†å¤šç§è¾“å…¥å‚è€ƒæ ¼å¼ï¼Œå¦‚æ–‡æœ¬ã€å›¾åƒæˆ–è§†é¢‘ç­‰ï¼Œå¹¶æ ¹æ®æ¡ä»¶çº¦æŸç”Ÿæˆé«˜è´¨é‡çš„å¤šè§†è§’è§†é¢‘ã€‚ç›¸è¾ƒäºå…¶ä»–ç±»ä¼¼èƒ½åŠ›æ¨¡å‹ï¼Œè¯¥æ¡†æ¶åœ¨FIDå’ŒFVDæŒ‡æ ‡ä¸Šåˆ†åˆ«æå‡äº†48.2%å’Œ35.2%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¤šæ ·åŒ–ä¸”çœŸå®çš„é©¾é©¶åœºæ™¯å¯¹æå‡è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„æ„ŸçŸ¥å’Œè§„åˆ’èƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>UniMLVGæ¡†æ¶æ—¨åœ¨ç”Ÿæˆæ‰©å±•è¡—é“å¤šè§†è§’è§†é¢‘ï¼Œé›†æˆå•è§†è§’å’Œå¤šè§†è§’é©¾é©¶è§†é¢‘åˆ°è®­ç»ƒæ•°æ®ä¸­ã€‚</li>
<li>é€šè¿‡æ›´æ–°åŸºäºDiTçš„æ‰©æ•£æ¨¡å‹å¹¶é…å¤‡è·¨å¸§å’Œè·¨è§†æ¨¡å—ï¼Œæé«˜ç”Ÿæˆè§†é¢‘å†…å®¹çš„å¤šæ ·æ€§å’Œè´¨é‡ã€‚</li>
<li>UniMLVGæ¡†æ¶å…·å¤‡å¤„ç†å¤šç§è¾“å…¥å‚è€ƒæ ¼å¼çš„èƒ½åŠ›ï¼Œå¦‚æ–‡æœ¬ã€å›¾åƒæˆ–è§†é¢‘ç­‰ã€‚</li>
<li>åˆ›æ–°çš„æ˜ç¡®è§†ç‚¹å»ºæ¨¡æ–¹æ³•ç”¨äºå¤šè§†è§’è§†é¢‘ç”Ÿæˆï¼Œæ”¹å–„åŠ¨ä½œè¿‡æ¸¡ä¸€è‡´æ€§ã€‚</li>
<li>UniMLVGæ¡†æ¶èƒ½å¤Ÿæ ¹æ®æ¡ä»¶çº¦æŸç”Ÿæˆé«˜è´¨é‡çš„å¤šè§†è§’è§†é¢‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04842">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b4913200bfc127d729e3c05cae3a95d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2048f20e96d3f85ea60e525c0dc0e152.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e295be432ca5085af6d9aba09bc1c0cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ae96a0df37a5baac06537d9c9e3c66e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c9de545656bd881ff5f400bf9149d14.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VISION-XL-High-Definition-Video-Inverse-Problem-Solver-using-Latent-Image-Diffusion-Models"><a href="#VISION-XL-High-Definition-Video-Inverse-Problem-Solver-using-Latent-Image-Diffusion-Models" class="headerlink" title="VISION-XL: High Definition Video Inverse Problem Solver using Latent   Image Diffusion Models"></a>VISION-XL: High Definition Video Inverse Problem Solver using Latent   Image Diffusion Models</h2><p><strong>Authors:Taesung Kwon, Jong Chul Ye</strong></p>
<p>In this paper, we propose a novel framework for solving high-definition video inverse problems using latent image diffusion models. Building on recent advancements in spatio-temporal optimization for video inverse problems using image diffusion models, our approach leverages latent-space diffusion models to achieve enhanced video quality and resolution. To address the high computational demands of processing high-resolution frames, we introduce a pseudo-batch consistent sampling strategy, allowing efficient operation on a single GPU. Additionally, to improve temporal consistency, we present pseudo-batch inversion, an initialization technique that incorporates informative latents from the measurement. By integrating with SDXL, our framework achieves state-of-the-art video reconstruction across a wide range of spatio-temporal inverse problems, including complex combinations of frame averaging and various spatial degradations, such as deblurring, super-resolution, and inpainting. Unlike previous methods, our approach supports multiple aspect ratios (landscape, vertical, and square) and delivers HD-resolution reconstructions (exceeding 1280x720) in under 6 seconds per frame on a single NVIDIA 4090 GPU. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨æ½œåœ¨å›¾åƒæ‰©æ•£æ¨¡å‹è§£å†³é«˜æ¸…è§†é¢‘åé—®é¢˜çš„æ–°å‹æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å»ºç«‹åœ¨æœ€è¿‘åˆ©ç”¨å›¾åƒæ‰©æ•£æ¨¡å‹è§£å†³è§†é¢‘åé—®é¢˜çš„æ—¶ç©ºä¼˜åŒ–è¿›å±•ä¹‹ä¸Šï¼Œé€šè¿‡åˆ©ç”¨æ½œåœ¨ç©ºé—´æ‰©æ•£æ¨¡å‹æ¥æé«˜è§†é¢‘è´¨é‡å’Œåˆ†è¾¨ç‡ã€‚ä¸ºäº†è§£å†³å¤„ç†é«˜åˆ†è¾¨ç‡å¸§çš„é«˜è®¡ç®—éœ€æ±‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¼ªæ‰¹é‡ä¸€è‡´é‡‡æ ·ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯åœ¨å•ä¸ªGPUä¸Šå®ç°é«˜æ•ˆæ“ä½œã€‚æ­¤å¤–ï¼Œä¸ºäº†æé«˜æ—¶é—´ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¼ªæ‰¹é‡åè½¬åˆå§‹åŒ–æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ç»“åˆäº†æµ‹é‡ä¸­çš„ä¿¡æ¯æ½œåœ¨å› ç´ ã€‚é€šè¿‡ä¸SDXLé›†æˆï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å¹¿æ³›çš„æ—¶ç©ºåé—®é¢˜ä¸­å®ç°äº†æœ€å…ˆè¿›çš„è§†é¢‘é‡å»ºï¼ŒåŒ…æ‹¬å¸§å¹³å‡å€¼çš„å¤æ‚ç»„åˆå’Œå„ç§ç©ºé—´é€€åŒ–ï¼Œå¦‚å»æ¨¡ç³Šã€è¶…åˆ†è¾¨ç‡å’Œå›¾åƒä¿®å¤ã€‚ä¸åŒäºä»¥å‰çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ”¯æŒå¤šç§çºµæ¨ªæ¯”ï¼ˆæ¨ªå±ã€ç«–å±å’Œæ­£æ–¹å½¢ï¼‰ï¼Œå¹¶åœ¨å•ä¸ªNVIDIA 4090 GPUä¸Šä»¥æ¯ç§’ä¸åˆ°6å¸§çš„é€Ÿåº¦å®ç°é«˜æ¸…åˆ†è¾¨ç‡é‡å»ºï¼ˆè¶…è¿‡1280x720ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00156v3">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://vision-xl.github.io/">https://vision-xl.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºå›¾åƒæ‰©æ•£æ¨¡å‹çš„æ—¶ç©ºä¼˜åŒ–è¿›å±•ï¼Œæå‡ºä¸€ç§è§£å†³é«˜æ¸…è§†é¢‘é€†é—®é¢˜çš„æ–°å‹æ¡†æ¶ã€‚åˆ©ç”¨æ½œåœ¨ç©ºé—´æ‰©æ•£æ¨¡å‹æé«˜äº†è§†é¢‘è´¨é‡å’Œåˆ†è¾¨ç‡ï¼Œå¹¶å¼•å…¥ä¼ªæ‰¹æ¬¡ä¸€è‡´é‡‡æ ·ç­–ç•¥ï¼Œå®ç°äº†åœ¨å•ä¸ªGPUä¸Šçš„é«˜æ•ˆæ“ä½œã€‚ç»“åˆä¼ªæ‰¹æ¬¡åè½¬åˆå§‹åŒ–æŠ€æœ¯ï¼Œæé«˜äº†æ—¶é—´ä¸€è‡´æ€§ã€‚ç»“åˆSDXLæŠ€æœ¯ï¼Œè¯¥æ¡†æ¶åœ¨å¤šç§æ—¶ç©ºé€†é—®é¢˜ä¸­å®ç°äº†è§†é¢‘é‡å»ºçš„æœ€ä½³æ•ˆæœã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œè¯¥æ–¹æ³•æ”¯æŒå¤šç§çºµæ¨ªæ¯”ï¼Œå¹¶åœ¨å•ä¸ªNVIDIA 4090 GPUä¸Šå®ç°äº†è¶…è¿‡1280x720åˆ†è¾¨ç‡çš„é‡å»ºï¼Œæ¯å¸§å¤„ç†æ—¶é—´ä¸åˆ°6ç§’ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨æ½œåœ¨å›¾åƒæ‰©æ•£æ¨¡å‹è§£å†³é«˜æ¸…è§†é¢‘é€†é—®é¢˜ã€‚</li>
<li>å¼•å…¥ä¼ªæ‰¹æ¬¡ä¸€è‡´é‡‡æ ·ç­–ç•¥ï¼Œå®ç°é«˜æ•ˆå•GPUæ“ä½œã€‚</li>
<li>æå‡ºä¼ªæ‰¹æ¬¡åè½¬åˆå§‹åŒ–æŠ€æœ¯ï¼Œæé«˜æ—¶é—´ä¸€è‡´æ€§ã€‚</li>
<li>ç»“åˆSDXLæŠ€æœ¯å®ç°å…ˆè¿›è§†é¢‘é‡å»ºã€‚</li>
<li>æ”¯æŒå¤šç§çºµæ¨ªæ¯”çš„è§†é¢‘é‡å»ºã€‚</li>
<li>åœ¨å•ä¸ªNVIDIA 4090 GPUä¸Šå®ç°å¿«é€Ÿé«˜æ¸…åˆ†è¾¨ç‡é‡å»ºï¼ˆè¶…è¿‡1280x720ï¼‰ã€‚</li>
<li>æ¯å¸§å¤„ç†æ—¶é—´å°‘äº6ç§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00156">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1c475e92cd99161407e48b33d0641857.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8871c27cdf71b815704de01a55da047e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c167c42cc1be2a5bcb26cc2118e66a5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-716fefef52104778416315e065200599.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2542f9a161a5efa509201afd5b327014.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MVGenMaster-Scaling-Multi-View-Generation-from-Any-Image-via-3D-Priors-Enhanced-Diffusion-Model"><a href="#MVGenMaster-Scaling-Multi-View-Generation-from-Any-Image-via-3D-Priors-Enhanced-Diffusion-Model" class="headerlink" title="MVGenMaster: Scaling Multi-View Generation from Any Image via 3D Priors   Enhanced Diffusion Model"></a>MVGenMaster: Scaling Multi-View Generation from Any Image via 3D Priors   Enhanced Diffusion Model</h2><p><strong>Authors:Chenjie Cao, Chaohui Yu, Shang Liu, Fan Wang, Xiangyang Xue, Yanwei Fu</strong></p>
<p>We introduce MVGenMaster, a multi-view diffusion model enhanced with 3D priors to address versatile Novel View Synthesis (NVS) tasks. MVGenMaster leverages 3D priors that are warped using metric depth and camera poses, significantly enhancing both generalization and 3D consistency in NVS. Our model features a simple yet effective pipeline that can generate up to 100 novel views conditioned on variable reference views and camera poses with a single forward process. Additionally, we have developed a comprehensive large-scale multi-view image dataset called MvD-1M, comprising up to 1.6 million scenes, equipped with well-aligned metric depth to train MVGenMaster. Moreover, we present several training and model modifications to strengthen the model with scaled-up datasets. Extensive evaluations across in- and out-of-domain benchmarks demonstrate the effectiveness of our proposed method and data formulation. Models and codes will be released at <a target="_blank" rel="noopener" href="https://github.com/ewrfcas/MVGenMaster/">https://github.com/ewrfcas/MVGenMaster/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†MVGenMasterï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šè§†å›¾æ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡åŠ å…¥3Då…ˆéªŒçŸ¥è¯†æ¥åº”å¯¹å¤šæ ·åŒ–çš„æ–°å‹è§†å›¾åˆæˆï¼ˆNVSï¼‰ä»»åŠ¡ã€‚MVGenMasteråˆ©ç”¨é€šè¿‡åº¦é‡æ·±åº¦å’Œç›¸æœºå§¿æ€è¿›è¡Œå˜å½¢çš„3Då…ˆéªŒçŸ¥è¯†ï¼Œæ˜¾è‘—æé«˜äº†NVSä¸­çš„é€šç”¨æ€§å’Œ3Dä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹é‡‡ç”¨ç®€å•æœ‰æ•ˆçš„ç®¡é“ï¼Œåªéœ€ä¸€æ¬¡å‰å‘è¿‡ç¨‹ï¼Œå°±å¯ä»¥æ ¹æ®å¯å˜çš„å‚è€ƒè§†å›¾å’Œç›¸æœºå§¿æ€ç”Ÿæˆå¤šè¾¾100ä¸ªæ–°å‹è§†å›¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªåä¸ºMvD-1Mçš„å¤§è§„æ¨¡å¤šè§†å›¾å›¾åƒæ•°æ®é›†ï¼ŒåŒ…å«é«˜è¾¾160ä¸‡ä¸ªåœºæ™¯ï¼Œé…å¤‡å¯¹é½è‰¯å¥½çš„åº¦é‡æ·±åº¦ä»¥è®­ç»ƒMVGenMasterã€‚è€Œä¸”ï¼Œæˆ‘ä»¬å¯¹è®­ç»ƒå’Œæ¨¡å‹è¿›è¡Œäº†å‡ æ¬¡ä¿®æ”¹ï¼Œä»¥é€šè¿‡æ‰©å±•æ•°æ®é›†æ¥åŠ å¼ºæ¨¡å‹ã€‚åœ¨å†…éƒ¨å’Œå¤–éƒ¨åŸºå‡†æµ‹è¯•çš„å¤§é‡è¯„ä¼°è¡¨æ˜äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•å’Œæ•°æ®åˆ¶å®šçš„æœ‰æ•ˆæ€§ã€‚æ¨¡å‹å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/ewrfcas/MVGenMaster/">https://github.com/ewrfcas/MVGenMaster/</a>å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16157v3">PDF</a> Accepted by CVPR2025. Models and codes will be released at   <a target="_blank" rel="noopener" href="https://github.com/ewrfcas/MVGenMaster/">https://github.com/ewrfcas/MVGenMaster/</a>. The project page is at   <a target="_blank" rel="noopener" href="https://ewrfcas.github.io/MVGenMaster/">https://ewrfcas.github.io/MVGenMaster/</a></p>
<p><strong>Summary</strong></p>
<p>MVGenMasteræ˜¯ä¸€æ¬¾åˆ©ç”¨å¤šè§†è§’æ‰©æ•£æ¨¡å‹ç»“åˆ3Då…ˆéªŒæŠ€æœ¯çš„é€šç”¨å‹æ–°è§†è§’åˆæˆï¼ˆNVSï¼‰è§£å†³æ–¹æ¡ˆã€‚å®ƒé€šè¿‡åˆ©ç”¨åŸºäºåº¦é‡æ·±åº¦å’Œç›¸æœºå§¿æ€çš„3Då…ˆéªŒçŸ¥è¯†ï¼Œæ˜¾è‘—æé«˜äº†NVSçš„é€šç”¨æ€§å’Œ3Dä¸€è‡´æ€§ã€‚MVGenMasteræ‹¥æœ‰ç®€æ´é«˜æ•ˆçš„æµç¨‹ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€å‰å‘è¿‡ç¨‹ä¸­ï¼Œæ ¹æ®å¤šç§å‚è€ƒè§†è§’å’Œç›¸æœºå§¿æ€ç”Ÿæˆå¤šè¾¾100ä¸ªæ–°è§†è§’ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼€å‘äº†ä¸€ä¸ªåä¸ºMvD-1Mçš„å¤§è§„æ¨¡å¤šè§†è§’å›¾åƒæ•°æ®é›†ï¼ŒåŒ…å«160ä¸‡åœºæ™¯ï¼Œé…å¤‡å¯¹é½çš„åº¦é‡æ·±åº¦ä»¥è®­ç»ƒMVGenMasteræ¨¡å‹ã€‚è¯¥è®ºæ–‡ä¹Ÿä»‹ç»äº†ä¸€äº›é€šè¿‡æ‰©å±•æ•°æ®é›†åŠ å¼ºæ¨¡å‹æ€§èƒ½çš„è®­ç»ƒå’Œæ¨¡å‹ä¿®æ”¹æ–¹æ³•ã€‚è¯„ä¼°å’Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åŠæ•°æ®æ„å»ºæ–¹å¼éå¸¸æœ‰æ•ˆã€‚æ¨¡å‹å’Œä»£ç å°†å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/ewrfcas/MVGenMaster/">é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MVGenMasteræ˜¯ä¸€æ¬¾å¤šè§†è§’æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆäº†3Då…ˆéªŒæŠ€æœ¯ç”¨äºæ–°è§†è§’åˆæˆï¼ˆNVSï¼‰ã€‚</li>
<li>åˆ©ç”¨äº†åŸºäºåº¦é‡æ·±åº¦å’Œç›¸æœºå§¿æ€çš„3Då…ˆéªŒçŸ¥è¯†ï¼Œå¢å¼ºäº†æ¨¡å‹çš„é€šç”¨æ€§å’Œ3Dä¸€è‡´æ€§ã€‚</li>
<li>MVGenMasterå¯ä»¥åŸºäºå¤šç§å‚è€ƒè§†è§’å’Œç›¸æœºå§¿æ€ç”Ÿæˆå¤šä¸ªæ–°è§†è§’ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šè§†è§’å›¾åƒæ•°æ®é›†MvD-1Mï¼Œç”¨äºè®­ç»ƒMVGenMasteræ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹å¯ä»¥é€šè¿‡æ‰©å±•æ•°æ®é›†æ¥åŠ å¼ºæ€§èƒ½ï¼ŒåŒ…æ‹¬è®­ç»ƒå’Œæ¨¡å‹çš„ä¿®æ”¹æ–¹æ³•ã€‚</li>
<li>MVGenMasterç»è¿‡å¹¿æ³›è¯„ä¼°å’Œå®éªŒéªŒè¯ï¼Œè¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d48c4968a8b2b02ec517efb3e2c495da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c275c0b529da432422bfd9ca559becbc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d315a6d9e096f61d2aff7c2161218d49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1bb85f145dbd061760d16c7da28768a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e885603cbfe1aa9e3a4f37f0d4c2172e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SymmetricDiffusers-Learning-Discrete-Diffusion-on-Finite-Symmetric-Groups"><a href="#SymmetricDiffusers-Learning-Discrete-Diffusion-on-Finite-Symmetric-Groups" class="headerlink" title="SymmetricDiffusers: Learning Discrete Diffusion on Finite Symmetric   Groups"></a>SymmetricDiffusers: Learning Discrete Diffusion on Finite Symmetric   Groups</h2><p><strong>Authors:Yongxing Zhang, Donglin Yang, Renjie Liao</strong></p>
<p>Finite symmetric groups $S_n$ are essential in fields such as combinatorics, physics, and chemistry. However, learning a probability distribution over $S_n$ poses significant challenges due to its intractable size and discrete nature. In this paper, we introduce SymmetricDiffusers, a novel discrete diffusion model that simplifies the task of learning a complicated distribution over $S_n$ by decomposing it into learning simpler transitions of the reverse diffusion using deep neural networks. We identify the riffle shuffle as an effective forward transition and provide empirical guidelines for selecting the diffusion length based on the theory of random walks on finite groups. Additionally, we propose a generalized Plackett-Luce (PL) distribution for the reverse transition, which is provably more expressive than the PL distribution. We further introduce a theoretically grounded â€œdenoising scheduleâ€ to improve sampling and learning efficiency. Extensive experiments show that our model achieves state-of-the-art or comparable performances on solving tasks including sorting 4-digit MNIST images, jigsaw puzzles, and traveling salesman problems. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/DSL-Lab/SymmetricDiffusers">https://github.com/DSL-Lab/SymmetricDiffusers</a>. </p>
<blockquote>
<p>æœ‰é™å¯¹ç§°ç¾¤$S_n$åœ¨ç»„åˆå­¦ã€ç‰©ç†å­¦å’ŒåŒ–å­¦ç­‰é¢†åŸŸä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç„¶è€Œï¼Œå­¦ä¹ $S_n$ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒé¢ä¸´ç€å·¨å¤§çš„æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºå…¶éš¾ä»¥å¤„ç†çš„å¤§å°å’Œç¦»æ•£æ€§è´¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SymmetricDiffusersï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œå®ƒé€šè¿‡åˆ†è§£å­¦ä¹ åå‘æ‰©æ•£çš„æ›´ç®€å•è½¬æ¢æ¥ç®€åŒ–å­¦ä¹ $S_n$ä¸Šå¤æ‚åˆ†å¸ƒçš„ä»»åŠ¡ï¼Œè¿™äº›è½¬æ¢ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œå®Œæˆã€‚æˆ‘ä»¬ç¡®å®šäº†riffleæ´—ç‰Œä½œä¸ºä¸€ç§æœ‰æ•ˆçš„æ­£å‘è½¬æ¢ï¼Œå¹¶æ ¹æ®æœ‰é™ç¾¤ä¸Šçš„éšæœºæ¸¸èµ°ç†è®ºæä¾›äº†é€‰æ‹©æ‰©æ•£é•¿åº¦çš„ç»éªŒæŒ‡å—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¹¿ä¹‰çš„Plackett-Luceï¼ˆPLï¼‰åˆ†å¸ƒç”¨äºåå‘è½¬æ¢ï¼Œè¯¥åˆ†å¸ƒè¢«è¯æ˜æ¯”PLåˆ†å¸ƒæ›´å…·è¡¨ç°åŠ›ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªç†è®ºåŸºç¡€çš„â€œé™å™ªæ—¶é—´è¡¨â€ï¼Œä»¥æé«˜é‡‡æ ·å’Œå­¦ä¹ æ•ˆç‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è§£å†³åŒ…æ‹¬æ’åº4ä½MNISTå›¾åƒã€æ‹¼å›¾å’Œæ—…è¡Œæ¨é”€å‘˜é—®é¢˜åœ¨å†…çš„ä»»åŠ¡æ—¶è¾¾åˆ°äº†æœ€æ–°æˆ–ç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/DSL-Lab/SymmetricDiffusers%E3%80%82">https://github.com/DSL-Lab/SymmetricDiffusersã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02942v2">PDF</a> ICLR 2025 Oral</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„ç¦»æ•£æ‰©æ•£æ¨¡å‹â€”â€”SymmetricDiffusersï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿç®€åŒ–å­¦ä¹ æœ‰é™å¯¹ç§°ç¾¤$S_n$ä¸Šçš„å¤æ‚åˆ†å¸ƒçš„ä»»åŠ¡ã€‚å®ƒé€šè¿‡åˆ†è§£å¤æ‚çš„åˆ†å¸ƒå­¦ä¹ è¿‡ç¨‹ä¸ºæ›´ç®€å•åå‘æ‰©æ•£è¿‡æ¸¡æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œåˆ©ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œåå‘è¿‡æ¸¡å­¦ä¹ ã€‚ç ”ç©¶è¿‡ç¨‹ä¸­é‡‡ç”¨äº†æœ‰æ•ˆçš„æ­£å‘è¿‡æ¸¡æ–¹æ³•â€”â€”æ´—ç‰Œï¼Œå¹¶åŸºäºéšæœºæœ‰é™ç¾¤ç†è®ºæä¾›äº†é€‰æ‹©æ‰©æ•£é•¿åº¦çš„ç»éªŒæŒ‡å—ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§å¹¿ä¹‰çš„Plackett-Luceåˆ†å¸ƒç”¨äºåå‘è¿‡æ¸¡ï¼Œæé«˜äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚åŒæ—¶å¼•å…¥äº†ç†è®ºåŸºç¡€çš„â€œå»å™ªè®¡åˆ’â€ä»¥æé«˜é‡‡æ ·å’Œå­¦ä¹ æ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ’åºã€æ‹¼å›¾å’Œæ—…è¡Œå•†é—®é¢˜ç­‰å¤šé¡¹ä»»åŠ¡ä¸Šå–å¾—äº†é¢†å…ˆæ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SymmetricDiffusersæ˜¯ä¸€ç§é’ˆå¯¹æœ‰é™å¯¹ç§°ç¾¤$S_n$ä¸Šçš„å¤æ‚åˆ†å¸ƒçš„ç¦»æ•£æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>å®ƒé€šè¿‡åˆ†è§£å¤æ‚åˆ†å¸ƒå­¦ä¹ è¿‡ç¨‹ä¸ºæ›´ç®€å•åå‘æ‰©æ•£è¿‡æ¸¡æ¥å®ç°å­¦ä¹ ã€‚</li>
<li>æ·±åº¦ç¥ç»ç½‘ç»œç”¨äºè¿›è¡Œåå‘è¿‡æ¸¡å­¦ä¹ ã€‚</li>
<li>æœ‰æ•ˆçš„æ­£å‘è¿‡æ¸¡æ–¹æ³•æ˜¯æ´—ç‰Œï¼ŒåŸºäºéšæœºæœ‰é™ç¾¤ç†è®ºé€‰æ‹©æ‰©æ•£é•¿åº¦ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¹¿ä¹‰çš„Plackett-Luceåˆ†å¸ƒç”¨äºåå‘è¿‡æ¸¡ï¼Œæé«˜äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†ç†è®ºåŸºç¡€çš„â€œå»å™ªè®¡åˆ’â€ä»¥æé«˜é‡‡æ ·å’Œå­¦ä¹ æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fe3fb9c312fa6db22c4fc69dc7715cd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b34213e60f178f27d4cbc5428729494.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Self-supervised-pre-training-with-diffusion-model-for-few-shot-landmark-detection-in-x-ray-images"><a href="#Self-supervised-pre-training-with-diffusion-model-for-few-shot-landmark-detection-in-x-ray-images" class="headerlink" title="Self-supervised pre-training with diffusion model for few-shot landmark   detection in x-ray images"></a>Self-supervised pre-training with diffusion model for few-shot landmark   detection in x-ray images</h2><p><strong>Authors:Roberto Di Via, Francesca Odone, Vito Paolo Pastore</strong></p>
<p>Deep neural networks have been extensively applied in the medical domain for various tasks, including image classification, segmentation, and landmark detection. However, their application is often hindered by data scarcity, both in terms of available annotations and images. This study introduces a novel application of denoising diffusion probabilistic models (DDPMs) to the landmark detection task, specifically addressing the challenge of limited annotated data in x-ray imaging. Our key innovation lies in leveraging DDPMs for self-supervised pre-training in landmark detection, a previously unexplored approach in this domain. This method enables accurate landmark detection with minimal annotated training data (as few as 50 images), surpassing both ImageNet supervised pre-training and traditional self-supervised techniques across three popular x-ray benchmark datasets. To our knowledge, this work represents the first application of diffusion models for self-supervised learning in landmark detection, which may offer a valuable pre-training approach in few-shot regimes, for mitigating data scarcity. </p>
<blockquote>
<p>æ·±åº¦ç¥ç»ç½‘ç»œå·²å¹¿æ³›åº”ç”¨äºåŒ»ç–—é¢†åŸŸçš„å„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»ã€åˆ†å‰²å’Œå…³é”®ç‚¹æ£€æµ‹ã€‚ç„¶è€Œï¼Œå…¶åœ¨å®é™…åº”ç”¨ä¸­å¸¸å—åˆ°æ•°æ®ç¨€ç¼ºçš„é™åˆ¶ï¼ŒåŒ…æ‹¬å¯ç”¨çš„æ ‡æ³¨å’Œå›¾åƒã€‚æœ¬ç ”ç©¶é¦–æ¬¡å°†å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰åº”ç”¨äºå…³é”®ç‚¹æ£€æµ‹ä»»åŠ¡ï¼Œä¸“é—¨è§£å†³Xå°„çº¿æˆåƒä¸­æ ‡æ³¨æ•°æ®æœ‰é™å¸¦æ¥çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ä¸»è¦åˆ›æ–°ä¹‹å¤„åœ¨äºåˆ©ç”¨DDPMsè¿›è¡Œå…³é”®ç‚¹æ£€æµ‹çš„è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œè¿™æ˜¯è¯¥é¢†åŸŸä¹‹å‰æœªè¢«æ¢ç´¢çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æå°‘çš„æ ‡æ³¨è®­ç»ƒæ•°æ®ï¼ˆä»…50å¼ å›¾åƒï¼‰ä¸‹å®ç°å‡†ç¡®çš„å…³é”®ç‚¹æ£€æµ‹ï¼Œè¶…è¶Šäº†ImageNetç›‘ç£é¢„è®­ç»ƒå’Œä¼ ç»Ÿè‡ªç›‘ç£æŠ€æœ¯åœ¨ä¸‰ä¸ªæµè¡Œçš„Xå°„çº¿åŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™é¡¹å·¥ä½œä»£è¡¨äº†æ‰©æ•£æ¨¡å‹åœ¨å…³é”®ç‚¹æ£€æµ‹è‡ªç›‘ç£å­¦ä¹ ä¸­çš„é¦–æ¬¡åº”ç”¨ï¼Œè¿™å¯èƒ½ä¸ºç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜æä¾›æœ‰ä»·å€¼çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬æƒ…å†µä¸‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.18125v3">PDF</a> Accepted at WACV 2025</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦é¢†åŸŸçš„åº”ç”¨ç ”ç©¶ã€‚æ–‡ç« é’ˆå¯¹Xå°„çº¿æˆåƒä¸­çš„åœ°æ ‡æ£€æµ‹ä»»åŠ¡ï¼Œå¼•å…¥å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰è¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œè§£å†³äº†æ ‡æ³¨æ•°æ®æœ‰é™çš„é—®é¢˜ã€‚æ­¤æ–¹æ³•åœ¨ä»…ä½¿ç”¨å°‘é‡ï¼ˆå¦‚50å¼ ï¼‰æ ‡æ³¨å›¾åƒçš„æƒ…å†µä¸‹ä»å¯å®ç°å‡†ç¡®çš„åœ°æ ‡æ£€æµ‹ï¼Œå¹¶åœ¨ä¸‰ä¸ªå…¬å…±Xå°„çº¿æ•°æ®é›†ä¸Šè¶…è¶ŠImageNetç›‘ç£é¢„è®­ç»ƒå’Œä¼ ç»Ÿè‡ªç›‘ç£æŠ€æœ¯ã€‚æ­¤ä¸ºæ‰©æ•£æ¨¡å‹åœ¨è‡ªç›‘ç£å­¦ä¹ ä¸­çš„é¦–æ¬¡åº”ç”¨äºåœ°æ ‡æ£€æµ‹ï¼Œä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜æä¾›äº†ä¸€ç§æœ‰ä»·å€¼çš„é¢„è®­ç»ƒæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶èƒŒæ™¯æ¶‰åŠæ·±åº¦ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦é¢†åŸŸçš„å¹¿æ³›åº”ç”¨åŠå…¶åœ¨é¢å¯¹æ•°æ®ç¨€ç¼ºæŒ‘æˆ˜æ—¶çš„å±€é™æ€§ã€‚</li>
<li>æå‡ºåˆ©ç”¨å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰è§£å†³åœ°æ ‡æ£€æµ‹ä»»åŠ¡ä¸­æ ‡æ³¨æ•°æ®æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>æ ¸å¿ƒåˆ›æ–°ç‚¹åœ¨äºä½¿ç”¨DDPMsè¿›è¡Œè‡ªç›‘ç£é¢„è®­ç»ƒï¼Œè¿™æ˜¯ä¸€ç§åœ¨åŒ»å­¦å›¾åƒåœ°æ ‡æ£€æµ‹é¢†åŸŸå°šæœªæ¢ç´¢çš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•å¯åœ¨ä»…ä½¿ç”¨å°‘é‡æ ‡æ³¨å›¾åƒçš„æƒ…å†µä¸‹å®ç°å‡†ç¡®çš„åœ°æ ‡æ£€æµ‹ã€‚</li>
<li>æ­¤æ–¹æ³•åœ¨ä¸‰ä¸ªæµè¡Œçš„Xå°„çº¿æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ImageNetç›‘ç£é¢„è®­ç»ƒå’Œä¼ ç»Ÿè‡ªç›‘ç£æŠ€æœ¯ã€‚</li>
<li>è¿™æ˜¯é¦–æ¬¡å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºè‡ªç›‘ç£å­¦ä¹ çš„åœ°æ ‡æ£€æµ‹ï¼Œä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜æä¾›äº†æœ‰ä»·å€¼çš„é¢„è®­ç»ƒæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.18125">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c4fe8ec52f131a12305c13150eef9066.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdf8299076cafa7e4723193ef80f93d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a569e2988b76382c74c267bebcb93089.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e059e518c4b2016e7c6a59a420b98309.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-de552bc34893907d1e9500a7bc2f586a.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-10  RadIR A Scalable Framework for Multi-Grained Medical Image Retrieval   via Radiology Report Mining
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-47d5e33a75041b1a6a71b828244f5394.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-10  Surgical Gaussian Surfels Highly Accurate Real-time Surgical Scene   Rendering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
