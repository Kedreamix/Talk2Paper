<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-03-10  Synthetic Data is an Elegant GIFT for Continual Vision-Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-716fefef52104778416315e065200599.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-03-10
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    42 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-03-10-更新"><a href="#2025-03-10-更新" class="headerlink" title="2025-03-10 更新"></a>2025-03-10 更新</h1><h2 id="Synthetic-Data-is-an-Elegant-GIFT-for-Continual-Vision-Language-Models"><a href="#Synthetic-Data-is-an-Elegant-GIFT-for-Continual-Vision-Language-Models" class="headerlink" title="Synthetic Data is an Elegant GIFT for Continual Vision-Language Models"></a>Synthetic Data is an Elegant GIFT for Continual Vision-Language Models</h2><p><strong>Authors:Bin Wu, Wuxuan Shi, Jinqiao Wang, Mang Ye</strong></p>
<p>Pre-trained Vision-Language Models (VLMs) require Continual Learning (CL) to efficiently update their knowledge and adapt to various downstream tasks without retraining from scratch. However, for VLMs, in addition to the loss of knowledge previously learned from downstream tasks, pre-training knowledge is also corrupted during continual fine-tuning. This issue is exacerbated by the unavailability of original pre-training data, leaving VLM’s generalization ability degrading. In this paper, we propose GIFT, a novel continual fine-tuning approach that utilizes synthetic data to overcome catastrophic forgetting in VLMs. Taking advantage of recent advances in text-to-image synthesis, we employ a pre-trained diffusion model to recreate both pre-training and learned downstream task data. In this way, the VLM can revisit previous knowledge through distillation on matching diffusion-generated images and corresponding text prompts. Leveraging the broad distribution and high alignment between synthetic image-text pairs in VLM’s feature space, we propose a contrastive distillation loss along with an image-text alignment constraint. To further combat in-distribution overfitting and enhance distillation performance with limited amount of generated data, we incorporate adaptive weight consolidation, utilizing Fisher information from these synthetic image-text pairs and achieving a better stability-plasticity balance. Extensive experiments demonstrate that our method consistently outperforms previous state-of-the-art approaches across various settings. </p>
<blockquote>
<p>预训练视觉语言模型（VLMs）需要持续学习（CL）来有效地更新其知识，并适应各种下游任务，而无需从头开始重新训练。然而，对于VLMs而言，除了从下游任务中学到的知识丢失之外，预训练知识在持续微调过程中也会被破坏。由于原始预训练数据不可用，这一问题更加严重，导致VLM的泛化能力下降。在本文中，我们提出了一种新型的持续微调方法GIFT，它利用合成数据来克服VLM中的灾难性遗忘问题。我们借助最新的文本到图像合成技术，利用预训练的扩散模型重新创建预训练和已学习的下游任务数据。通过这种方式，VLM可以通过对匹配的扩散生成图像和相应的文本提示进行蒸馏来回顾以前的知识。凭借合成图像-文本对在VLM特征空间中的广泛分布和高对齐度，我们提出了一种对比蒸馏损失以及图像-文本对齐约束。为了进一步对抗内部分布过拟合并增强有限生成数据的蒸馏性能，我们结合了自适应权重整合，利用这些合成图像-文本对的Fisher信息，实现了更好的稳定性-可塑性平衡。大量实验表明，我们的方法在各种设置下始终优于先前的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04229v1">PDF</a> This work is accepted by CVPR 2025. Modifications may be performed</p>
<p><strong>Summary</strong></p>
<p>本文提出一种新型的持续微调方法GIFT，利用合成数据克服视觉语言模型（VLMs）在持续学习中的灾难性遗忘问题。通过利用预训练的扩散模型重新创建预训练及已学习的下游任务数据，使VLM能够通过匹配生成的扩散图像和相应的文本提示进行知识蒸馏。同时，利用合成图像-文本对在VLM特征空间中的广泛分布和高度对齐特性，提出了对比蒸馏损失和图像-文本对齐约束。为进一步解决内部分布过拟合问题并增强有限生成数据的蒸馏性能，融入自适应权重整合策略，利用这些合成图像-文本对的Fisher信息，实现更好的稳定-可塑性平衡。实验表明，该方法在各种设置下均表现优于先前的主流方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GIFT是一种针对视觉语言模型（VLMs）的持续微调方法，旨在解决灾难性遗忘问题。</li>
<li>利用预训练的扩散模型重新创建预训练及下游任务数据，使VLM能够通过知识蒸馏复习旧知识。</li>
<li>通过合成图像-文本对在VLM特征空间中的广泛分布和高度对齐特性，引入对比蒸馏损失和图像-文本对齐约束。</li>
<li>为解决内部分布过拟合问题并增强有限生成数据的性能，采用自适应权重整合策略。</li>
<li>结合合成图像-文本对的Fisher信息，实现稳定-可塑性之间的平衡。</li>
<li>实验表明，GIFT方法在各种设置下均优于先前的主流方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04229">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a9ef1f14fb63f74c76c82b4f3a845606.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f654ad0a8b76d9af538784bfadb15e64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ece8f619e0c4d9183827ae8a59829a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72a2591551c4ddc7a2b590ef2818d860.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Underlying-Semantic-Diffusion-for-Effective-and-Efficient-In-Context-Learning"><a href="#Underlying-Semantic-Diffusion-for-Effective-and-Efficient-In-Context-Learning" class="headerlink" title="Underlying Semantic Diffusion for Effective and Efficient In-Context   Learning"></a>Underlying Semantic Diffusion for Effective and Efficient In-Context   Learning</h2><p><strong>Authors:Zhong Ji, Weilong Cao, Yan Zhang, Yanwei Pang, Jungong Han, Xuelong Li</strong></p>
<p>Diffusion models has emerged as a powerful framework for tasks like image controllable generation and dense prediction. However, existing models often struggle to capture underlying semantics (e.g., edges, textures, shapes) and effectively utilize in-context learning, limiting their contextual understanding and image generation quality. Additionally, high computational costs and slow inference speeds hinder their real-time applicability. To address these challenges, we propose Underlying Semantic Diffusion (US-Diffusion), an enhanced diffusion model that boosts underlying semantics learning, computational efficiency, and in-context learning capabilities on multi-task scenarios. We introduce Separate &amp; Gather Adapter (SGA), which decouples input conditions for different tasks while sharing the architecture, enabling better in-context learning and generalization across diverse visual domains. We also present a Feedback-Aided Learning (FAL) framework, which leverages feedback signals to guide the model in capturing semantic details and dynamically adapting to task-specific contextual cues. Furthermore, we propose a plug-and-play Efficient Sampling Strategy (ESS) for dense sampling at time steps with high-noise levels, which aims at optimizing training and inference efficiency while maintaining strong in-context learning performance. Experimental results demonstrate that US-Diffusion outperforms the state-of-the-art method, achieving an average reduction of 7.47 in FID on Map2Image tasks and an average reduction of 0.026 in RMSE on Image2Map tasks, while achieving approximately 9.45 times faster inference speed. Our method also demonstrates superior training efficiency and in-context learning capabilities, excelling in new datasets and tasks, highlighting its robustness and adaptability across diverse visual domains. </p>
<blockquote>
<p>扩散模型已经成为图像可控生成和密集预测等任务的重要框架。然而，现有模型往往难以捕捉底层语义（如边缘、纹理、形状），并有效地利用上下文学习，从而限制了其上下文理解和图像生成质量。此外，较高的计算成本和缓慢的推理速度阻碍了其在实时应用中的适用性。为了解决这些挑战，我们提出了底层语义扩散（US-Diffusion），这是一种增强的扩散模型，可以在多任务场景下提高底层语义学习、计算效率和上下文学习能力。我们引入了分离与聚合适配器（SGA），它解耦了不同任务的输入条件，同时共享架构，从而实现了更好的上下文学习和跨不同视觉领域的泛化能力。我们还提出了反馈辅助学习（FAL）框架，该框架利用反馈信号来指导模型捕捉语义细节并根据任务特定的上下文线索进行动态适应。此外，我们还提出了一种即插即用的高效采样策略（ESS），用于在较高噪声水平的时间步长上进行密集采样，旨在优化训练和推理效率，同时保持强大的上下文学习性能。实验结果表明，US-Diffusion在Map2Image任务上的FID平均降低了7.47，在Image2Map任务上的RMSE平均降低了0.026，同时推理速度提高了约9.45倍。我们的方法还展示了卓越的训练效率和上下文学习能力，在新的数据集和任务中表现出色，突显其在不同视觉领域的稳健性和适应性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04050v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>扩散模型已成为图像可控生成和密集预测等任务中的强大框架。然而，现有模型在捕捉底层语义（如边缘、纹理、形状）和有效利用上下文学习方面存在不足，限制了其上下文理解和图像生成质量。针对这些挑战，我们提出Underlying Semantic Diffusion（US-Diffusion）扩散模型增强底层语义学习、计算效率和多任务的上下文学习能力。引入分离聚集适配器（SGA），它允许对不同任务进行解耦输入条件，同时共享架构，实现了更好的上下文学习和跨不同视觉领域的泛化能力。我们还提出了反馈辅助学习（FAL）框架，利用反馈信号指导模型捕捉语义细节并根据任务特定上下文动态调整。此外，我们提出了即插即用的高效采样策略（ESS），针对高噪声水平的时间步长进行密集采样，旨在优化训练和推理效率，同时保持强大的上下文学习能力。实验结果表明，US-Diffusion在Map2Image任务上平均减少了7.47的FID分数，在Image2Map任务上平均减少了0.026的RMSE分数，同时推理速度提高了约9.45倍。我们的方法还展示了出色的训练效率和上下文学习能力，在新数据集和任务中表现出色，凸显其在不同视觉领域的稳健性和适应性。</p>
<p><strong>要点分析</strong></p>
<ul>
<li>扩散模型已成为图像生成和预测任务的重要工具。但现有模型存在语义捕捉和上下文学习的局限性。</li>
<li>US-Diffusion模型旨在解决这些挑战，通过增强底层语义学习、计算效率和上下文学习能力。</li>
<li>引入分离聚集适配器（SGA）以解耦不同任务的输入条件并共享架构，实现更好的上下文学习和跨领域泛化。</li>
<li>反馈辅助学习（FAL）框架利用反馈信号来指导模型捕捉语义细节并适应任务特定的上下文。</li>
<li>提出高效采样策略（ESS），旨在优化训练和推理效率，同时保持上下文学习能力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04050">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0febfdbe65da464440b045c6a534a9f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ccbfecfbb3e0729f06b36743f23d3c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a6b13ee51196819e727ceb99ede0536.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TextDoctor-Unified-Document-Image-Inpainting-via-Patch-Pyramid-Diffusion-Models"><a href="#TextDoctor-Unified-Document-Image-Inpainting-via-Patch-Pyramid-Diffusion-Models" class="headerlink" title="TextDoctor: Unified Document Image Inpainting via Patch Pyramid   Diffusion Models"></a>TextDoctor: Unified Document Image Inpainting via Patch Pyramid   Diffusion Models</h2><p><strong>Authors:Wanglong Lu, Lingming Su, Jingjing Zheng, Vinícius Veloso de Melo, Farzaneh Shoeleh, John Hawkin, Terrence Tricco, Hanli Zhao, Xianta Jiang</strong></p>
<p>Digital versions of real-world text documents often suffer from issues like environmental corrosion of the original document, low-quality scanning, or human interference. Existing document restoration and inpainting methods typically struggle with generalizing to unseen document styles and handling high-resolution images. To address these challenges, we introduce TextDoctor, a novel unified document image inpainting method. Inspired by human reading behavior, TextDoctor restores fundamental text elements from patches and then applies diffusion models to entire document images instead of training models on specific document types. To handle varying text sizes and avoid out-of-memory issues, common in high-resolution documents, we propose using structure pyramid prediction and patch pyramid diffusion models. These techniques leverage multiscale inputs and pyramid patches to enhance the quality of inpainting both globally and locally. Extensive qualitative and quantitative experiments on seven public datasets validated that TextDoctor outperforms state-of-the-art methods in restoring various types of high-resolution document images. </p>
<blockquote>
<p>现实世界文本文件的数字版本通常面临原始文件环境腐蚀、扫描质量低或人为干扰等问题。现有的文档恢复和修复方法通常在推广到未见过的文档风格和处理高分辨率图像时遇到困难。为了解决这些挑战，我们引入了TextDoctor，这是一种新颖的统一文档图像修复方法。TextDoctor受人类阅读行为的启发，首先恢复文本的基本元素，然后对整个文档图像应用扩散模型，而不是在特定文档类型上训练模型。为了处理不同的文本大小并避免高分辨率文档中常见的内存不足问题，我们建议使用结构金字塔预测和补丁金字塔扩散模型。这些技术利用多尺度输入和金字塔补丁来提高全局和局部修复的质量。在七个公共数据集上进行的大量定性和定量实验验证了TextDoctor在恢复各种类型的高分辨率文档图像方面优于最先进的方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04021v1">PDF</a> 28 pages, 25 figures</p>
<p><strong>Summary</strong></p>
<p>文本DocumentDoctor针对数字版现实世界文本文件存在的问题，如原始文档的环境腐蚀、扫描质量低或人为干扰等，提出了一种新的统一文档图像修复方法。该方法受到人类阅读行为的启发，从补丁中恢复基本文本元素，然后应用扩散模型于整个文档图像，而不是针对特定文档类型训练模型。针对高分辨率文档中常见的文本大小不同和内存不足问题，我们提出了结构金字塔预测和补丁金字塔扩散模型等解决方案。这些方法利用多尺度输入和金字塔补丁，提高了全局和局部修复的质量。在七个公共数据集上的广泛定性和定量实验验证了TextDoctor在恢复各种类型的高分辨率文档图像方面优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TextDoctor是一种新的文档图像修复方法，旨在解决数字版现实世界文本文件存在的问题。</li>
<li>该方法能够从补丁中恢复基本文本元素，并应用扩散模型于整个文档图像。</li>
<li>TextDoctor的灵感来源于人类阅读行为，能够处理不同的文档类型。</li>
<li>为了应对高分辨率文档中常见的文本大小不同和内存不足问题，TextDoctor采用了结构金字塔预测和补丁金字塔扩散模型等技术。</li>
<li>这些技术利用多尺度输入和金字塔补丁，提高了修复质量，既全局又局部。</li>
<li>TextDoctor在七个公共数据集上进行了广泛的实验，证明了其在恢复高分辨率文档图像方面的优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04021">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b65f96fbb7e74c9ed8226df787f0f15f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3b1d62e180c2fed98efedc741ecd88e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b7c99730e335994411760dbd58e7fa0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8fec45272ccc6ebae38a1327271af480.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Positive-Unlabeled-Diffusion-Models-for-Preventing-Sensitive-Data-Generation"><a href="#Positive-Unlabeled-Diffusion-Models-for-Preventing-Sensitive-Data-Generation" class="headerlink" title="Positive-Unlabeled Diffusion Models for Preventing Sensitive Data   Generation"></a>Positive-Unlabeled Diffusion Models for Preventing Sensitive Data   Generation</h2><p><strong>Authors:Hiroshi Takahashi, Tomoharu Iwata, Atsutoshi Kumagai, Yuuki Yamanaka, Tomoya Yamashita</strong></p>
<p>Diffusion models are powerful generative models but often generate sensitive data that are unwanted by users, mainly because the unlabeled training data frequently contain such sensitive data. Since labeling all sensitive data in the large-scale unlabeled training data is impractical, we address this problem by using a small amount of labeled sensitive data. In this paper, we propose positive-unlabeled diffusion models, which prevent the generation of sensitive data using unlabeled and sensitive data. Our approach can approximate the evidence lower bound (ELBO) for normal (negative) data using only unlabeled and sensitive (positive) data. Therefore, even without labeled normal data, we can maximize the ELBO for normal data and minimize it for labeled sensitive data, ensuring the generation of only normal data. Through experiments across various datasets and settings, we demonstrated that our approach can prevent the generation of sensitive images without compromising image quality. </p>
<blockquote>
<p>扩散模型是一种强大的生成模型，但经常会生成用户不需要的敏感数据，这主要是因为未标记的训练数据经常包含这种敏感数据。由于在大规模未标记的训练数据中标记所有敏感数据是不切实际的，我们通过使用少量标记的敏感数据来解决这个问题。在本文中，我们提出了正负未标记扩散模型，该模型使用未标记和敏感数据来防止敏感数据的生成。我们的方法可以使用仅未标记的（正常）（阴性）数据来近似正常数据的证据下限（ELBO），因此，即使没有标记的正常数据，我们也可以最大化正常数据的ELBO并最小化标记敏感数据的ELBO，确保只生成正常数据。通过在不同数据集和设置上的实验，我们证明了我们的方法可以防止生成敏感图像，同时不会损害图像质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03789v1">PDF</a> Accepted at ICLR2025. Code is available at   <a target="_blank" rel="noopener" href="https://github.com/takahashihiroshi/pudm">https://github.com/takahashihiroshi/pudm</a></p>
<p><strong>Summary</strong><br>     扩散模型通过少量标记敏感数据生成正向未标记扩散模型，防止生成敏感数据。此方法无需标记正常数据，即可最大化正常数据的证据下限（ELBO），最小化标记敏感数据的ELBO，确保仅生成正常数据。实验证明，该方法能有效防止生成敏感图像，同时保证图像质量不受影响。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型能够生成强大的生成模型，但可能会生成用户不需要的敏感数据。</li>
<li>敏感数据经常出现在未标记的训练数据中。</li>
<li>使用少量标记敏感数据来解决这一问题。</li>
<li>提出正向未标记扩散模型，利用未标记和敏感数据防止生成敏感数据。</li>
<li>通过最大化正常数据的证据下限（ELBO）和最小化标记敏感数据的ELBO来实现。</li>
<li>无需标记正常数据即可实现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03789">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b6c2323f04d89b448c90249a5aa61750.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a6436213afdb231c3be6c2d5fb2671d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Tackling-Few-Shot-Segmentation-in-Remote-Sensing-via-Inpainting-Diffusion-Model"><a href="#Tackling-Few-Shot-Segmentation-in-Remote-Sensing-via-Inpainting-Diffusion-Model" class="headerlink" title="Tackling Few-Shot Segmentation in Remote Sensing via Inpainting   Diffusion Model"></a>Tackling Few-Shot Segmentation in Remote Sensing via Inpainting   Diffusion Model</h2><p><strong>Authors:Steve Andreas Immanuel, Woojin Cho, Junhyuk Heo, Darongsae Kwon</strong></p>
<p>Limited data is a common problem in remote sensing due to the high cost of obtaining annotated samples. In the few-shot segmentation task, models are typically trained on base classes with abundant annotations and later adapted to novel classes with limited examples. However, this often necessitates specialized model architectures or complex training strategies. Instead, we propose a simple approach that leverages diffusion models to generate diverse variations of novel-class objects within a given scene, conditioned by the limited examples of the novel classes. By framing the problem as an image inpainting task, we synthesize plausible instances of novel classes under various environments, effectively increasing the number of samples for the novel classes and mitigating overfitting. The generated samples are then assessed using a cosine similarity metric to ensure semantic consistency with the novel classes. Additionally, we employ Segment Anything Model (SAM) to segment the generated samples and obtain precise annotations. By using high-quality synthetic data, we can directly fine-tune off-the-shelf segmentation models. Experimental results demonstrate that our method significantly enhances segmentation performance in low-data regimes, highlighting its potential for real-world remote sensing applications. </p>
<blockquote>
<p>在遥感领域，由于获取标注样本的成本高昂，数据有限是一个常见问题。在少量分割任务中，模型通常会在基础类别上进行训练，这些基础类别拥有丰富的标注，然后适应具有有限示例的新类别。然而，这通常需要专门的模型架构或复杂的训练策略。相反，我们提出了一种简单的方法，利用扩散模型在给定场景内生成多种新型类别对象的变体，这些变体以新类别的有限示例为条件。通过将问题构造成图像修复任务，我们在各种环境下合成新类别的合理实例，有效地增加了新类别的样本数量，并减轻了过拟合。生成的样本使用余弦相似度度量进行评估，以确保与新类别语义一致。此外，我们还采用任意分割模型（SAM）对生成的样本进行分割，以获得精确标注。通过使用高质量的合成数据，我们可以直接对现成的分割模型进行微调。实验结果表明，我们的方法在数据稀少的情况下显著提高了分割性能，突显其在现实世界遥感应用中的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03785v1">PDF</a> Accepted to ICLRW 2025 (Oral)</p>
<p><strong>Summary</strong></p>
<p>利用扩散模型，针对遥感领域有限数据的问题，提出了一种简单的方法。该方法通过生成场景中新型物体多样化的变体，以有限的样本为条件，解决了样本量较少的问题。通过将问题转化为图像补全任务，合成各种环境下新型物体的合理实例，增加了新型样本的数量，有效缓解了过拟合现象。生成的样本使用余弦相似性度量进行评估，确保与新型类语义的一致性。同时，采用分段模型对生成样本进行精确标注，并可直接对货架上的分割模型进行微调。实验结果证明，该方法在低数据环境下显著提高分割性能，突显其在现实遥感应用中的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型用于生成新型物体多样化的变体，解决遥感领域有限数据问题。</li>
<li>方法将问题转化为图像补全任务，合成各种环境下的新型物体实例。</li>
<li>生成样本通过余弦相似性度量评估，确保与新型类的语义一致性。</li>
<li>采用Segment Anything Model（SAM）对生成样本进行精确标注。</li>
<li>使用高质量合成数据可直接微调货架上的分割模型。</li>
<li>实验结果证明该方法在低数据环境下显著提高分割性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03785">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6713c5cf68538c4917bcdaf08a2892d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c2f1999084763db57955f04b7650749.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe2bb63831237921c2d9b431240e513a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-203caff4fab9bda8d4835d4d9a088b3c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Efficient-Diversity-Preserving-Diffusion-Alignment-via-Gradient-Informed-GFlowNets"><a href="#Efficient-Diversity-Preserving-Diffusion-Alignment-via-Gradient-Informed-GFlowNets" class="headerlink" title="Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed   GFlowNets"></a>Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed   GFlowNets</h2><p><strong>Authors:Zhen Liu, Tim Z. Xiao, Weiyang Liu, Yoshua Bengio, Dinghuai Zhang</strong></p>
<p>While one commonly trains large diffusion models by collecting datasets on target downstream tasks, it is often desired to align and finetune pretrained diffusion models with some reward functions that are either designed by experts or learned from small-scale datasets. Existing post-training methods for reward finetuning of diffusion models typically suffer from lack of diversity in generated samples, lack of prior preservation, and&#x2F;or slow convergence in finetuning. Inspired by recent successes in generative flow networks (GFlowNets), a class of probabilistic models that sample with the unnormalized density of a reward function, we propose a novel GFlowNet method dubbed Nabla-GFlowNet (abbreviated as \methodname), the first GFlowNet method that leverages the rich signal in reward gradients, together with an objective called \graddb plus its variant \resgraddb designed for prior-preserving diffusion finetuning. We show that our proposed method achieves fast yet diversity- and prior-preserving finetuning of Stable Diffusion, a large-scale text-conditioned image diffusion model, on different realistic reward functions. </p>
<blockquote>
<p>通常，人们通过收集目标下游任务的数据集来训练大型扩散模型，但人们常常希望通过与专家设计或从小型数据集中学习得到的奖励函数对齐并微调预训练的扩散模型。现有的用于扩散模型奖励微调的后期训练方法通常存在生成样本缺乏多样性、缺乏先验保留以及微调收敛缓慢等问题。受最近生成流网络（GFlowNets）成功的启发，GFlowNets是一类以奖励函数的未归一化密度进行采样的概率模型。我们提出了一种新的GFlowNet方法，称为Nabla-GFlowNet（简称\methodname），它是第一个利用奖励梯度丰富信号的GFlowNet方法，以及一个称为\graddb的目标及其变体\resgraddb，专为保留先验的扩散微调设计。我们展示，所提方法能够在不同的现实奖励函数上快速且多样、保留先验地对大规模文本条件图像扩散模型Stable Diffusion进行微调。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.07775v2">PDF</a> Technical Report (35 pages, 31 figures), Accepted at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>预训练的扩散模型可以通过与专家设计或从小规模数据集中学习的奖励函数进行对齐和微调，来提高其性能。现有的扩散模型奖励微调的后训练方法通常存在生成样本缺乏多样性、缺乏先验知识保留以及微调收敛缓慢的问题。受生成流网络（GFlowNets）近期成功的启发，我们提出了一种新的GFlowNet方法，名为Nabla-GFlowNet，它是第一个利用奖励梯度丰富信号的GFlowNet方法，配合称为\graddb的目标及其变体\resgraddb，用于先验保留的扩散微调。实验表明，所提方法在不同的真实奖励函数上，实现了对大规模文本条件图像扩散模型Stable Diffusion的快速、多样性和先验知识保留的微调。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型可以通过与奖励函数对齐和微调来提高性能。</li>
<li>现有方法存在生成样本缺乏多样性和先验知识保留的问题。</li>
<li>Nabla-GFlowNet是一种新的GFlowNet方法，利用奖励梯度丰富信号。</li>
<li>Nabla-GFlowNet配合\graddb及其变体\resgraddb，用于先验保留的扩散微调。</li>
<li>所提方法在不同真实奖励函数上实现了对Stable Diffusion模型的快速、多样性和先验知识保留的微调。</li>
<li>Stable Diffusion是一个大规模文本条件图像扩散模型。</li>
<li>Nabla-GFlowNet方法具有潜在的应用价值，可推广到其他领域的扩散模型微调。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.07775">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-566ef6fcce08e6ec39393b0d7128a0c4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1d874aa872307bde9e8a53820468e24f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="UniMLVG-Unified-Framework-for-Multi-view-Long-Video-Generation-with-Comprehensive-Control-Capabilities-for-Autonomous-Driving"><a href="#UniMLVG-Unified-Framework-for-Multi-view-Long-Video-Generation-with-Comprehensive-Control-Capabilities-for-Autonomous-Driving" class="headerlink" title="UniMLVG: Unified Framework for Multi-view Long Video Generation with   Comprehensive Control Capabilities for Autonomous Driving"></a>UniMLVG: Unified Framework for Multi-view Long Video Generation with   Comprehensive Control Capabilities for Autonomous Driving</h2><p><strong>Authors:Rui Chen, Zehuan Wu, Yichen Liu, Yuxin Guo, Jingcheng Ni, Haifeng Xia, Siyu Xia</strong></p>
<p>The creation of diverse and realistic driving scenarios has become essential to enhance perception and planning capabilities of the autonomous driving system. However, generating long-duration, surround-view consistent driving videos remains a significant challenge. To address this, we present UniMLVG, a unified framework designed to generate extended street multi-perspective videos under precise control. By integrating single- and multi-view driving videos into the training data, our approach updates a DiT-based diffusion model equipped with cross-frame and cross-view modules across three stages with multi training objectives, substantially boosting the diversity and quality of generated visual content. Importantly, we propose an innovative explicit viewpoint modeling approach for multi-view video generation to effectively improve motion transition consistency. Capable of handling various input reference formats (e.g., text, images, or video), our UniMLVG generates high-quality multi-view videos according to the corresponding condition constraints such as 3D bounding boxes or frame-level text descriptions. Compared to the best models with similar capabilities, our framework achieves improvements of 48.2% in FID and 35.2% in FVD. </p>
<blockquote>
<p>创建多样且逼真的驾驶场景对于提升自动驾驶系统的感知和规划能力至关重要。然而，生成长时间、全景一致的驾驶视频仍然是一项重大挑战。为了解决这一问题，我们推出了UniMLVG，这是一个统一框架，旨在在精确控制下生成扩展街道多视角视频。我们的方法通过将单视角和多视角驾驶视频融入训练数据，更新了一个配备跨帧和跨视角模块的三阶段DiT扩散模型，通过多训练目标大幅提升了生成视觉内容的多样性和质量。重要的是，我们提出了一种创新的多视角视频生成的显式视点建模方法，有效提高运动过渡的一致性。我们的UniMLVG能够处理各种输入参考格式（如文本、图像或视频），根据相应的条件约束（如3D边界框或帧级文本描述）生成高质量的多视角视频。与具有类似能力的最佳模型相比，我们的框架在FID上提高了48.2%，在FVD上提高了35.2%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.04842v3">PDF</a> </p>
<p><strong>Summary</strong><br>自动驾驶系统的感知和规划能力的提升离不开多样化且真实的驾驶场景的生成。为此，本文推出UniMLVG统一框架，旨在生成扩展街道多视角视频。该框架集成单视角和多视角驾驶视频到训练数据中，通过更新基于DiT的扩散模型并配备跨帧和跨视模块，在三个阶段进行多任务训练，显著提高生成视频内容的多样性和质量。此外，提出明确的多视角视频生成视点建模方法，有效改善动作过渡一致性。UniMLVG能够处理多种输入参考格式，如文本、图像或视频等，并根据条件约束生成高质量的多视角视频。相较于其他类似能力模型，该框架在FID和FVD指标上分别提升了48.2%和35.2%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成多样化且真实的驾驶场景对提升自动驾驶系统的感知和规划能力至关重要。</li>
<li>UniMLVG框架旨在生成扩展街道多视角视频，集成单视角和多视角驾驶视频到训练数据中。</li>
<li>通过更新基于DiT的扩散模型并配备跨帧和跨视模块，提高生成视频内容的多样性和质量。</li>
<li>UniMLVG框架具备处理多种输入参考格式的能力，如文本、图像或视频等。</li>
<li>创新的明确视点建模方法用于多视角视频生成，改善动作过渡一致性。</li>
<li>UniMLVG框架能够根据条件约束生成高质量的多视角视频。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.04842">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b4913200bfc127d729e3c05cae3a95d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2048f20e96d3f85ea60e525c0dc0e152.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e295be432ca5085af6d9aba09bc1c0cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ae96a0df37a5baac06537d9c9e3c66e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c9de545656bd881ff5f400bf9149d14.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="VISION-XL-High-Definition-Video-Inverse-Problem-Solver-using-Latent-Image-Diffusion-Models"><a href="#VISION-XL-High-Definition-Video-Inverse-Problem-Solver-using-Latent-Image-Diffusion-Models" class="headerlink" title="VISION-XL: High Definition Video Inverse Problem Solver using Latent   Image Diffusion Models"></a>VISION-XL: High Definition Video Inverse Problem Solver using Latent   Image Diffusion Models</h2><p><strong>Authors:Taesung Kwon, Jong Chul Ye</strong></p>
<p>In this paper, we propose a novel framework for solving high-definition video inverse problems using latent image diffusion models. Building on recent advancements in spatio-temporal optimization for video inverse problems using image diffusion models, our approach leverages latent-space diffusion models to achieve enhanced video quality and resolution. To address the high computational demands of processing high-resolution frames, we introduce a pseudo-batch consistent sampling strategy, allowing efficient operation on a single GPU. Additionally, to improve temporal consistency, we present pseudo-batch inversion, an initialization technique that incorporates informative latents from the measurement. By integrating with SDXL, our framework achieves state-of-the-art video reconstruction across a wide range of spatio-temporal inverse problems, including complex combinations of frame averaging and various spatial degradations, such as deblurring, super-resolution, and inpainting. Unlike previous methods, our approach supports multiple aspect ratios (landscape, vertical, and square) and delivers HD-resolution reconstructions (exceeding 1280x720) in under 6 seconds per frame on a single NVIDIA 4090 GPU. </p>
<blockquote>
<p>在这篇论文中，我们提出了一种利用潜在图像扩散模型解决高清视频反问题的新型框架。我们的方法建立在最近利用图像扩散模型解决视频反问题的时空优化进展之上，通过利用潜在空间扩散模型来提高视频质量和分辨率。为了解决处理高分辨率帧的高计算需求，我们引入了一种伪批量一致采样策略，该策略可在单个GPU上实现高效操作。此外，为了提高时间一致性，我们提出了伪批量反转初始化技术，该技术结合了测量中的信息潜在因素。通过与SDXL集成，我们的框架在广泛的时空反问题中实现了最先进的视频重建，包括帧平均值的复杂组合和各种空间退化，如去模糊、超分辨率和图像修复。不同于以前的方法，我们的方法支持多种纵横比（横屏、竖屏和正方形），并在单个NVIDIA 4090 GPU上以每秒不到6帧的速度实现高清分辨率重建（超过1280x720）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.00156v3">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://vision-xl.github.io/">https://vision-xl.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>基于图像扩散模型的时空优化进展，提出一种解决高清视频逆问题的新型框架。利用潜在空间扩散模型提高了视频质量和分辨率，并引入伪批次一致采样策略，实现了在单个GPU上的高效操作。结合伪批次反转初始化技术，提高了时间一致性。结合SDXL技术，该框架在多种时空逆问题中实现了视频重建的最佳效果。与传统方法不同，该方法支持多种纵横比，并在单个NVIDIA 4090 GPU上实现了超过1280x720分辨率的重建，每帧处理时间不到6秒。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用潜在图像扩散模型解决高清视频逆问题。</li>
<li>引入伪批次一致采样策略，实现高效单GPU操作。</li>
<li>提出伪批次反转初始化技术，提高时间一致性。</li>
<li>结合SDXL技术实现先进视频重建。</li>
<li>支持多种纵横比的视频重建。</li>
<li>在单个NVIDIA 4090 GPU上实现快速高清分辨率重建（超过1280x720）。</li>
<li>每帧处理时间少于6秒。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.00156">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1c475e92cd99161407e48b33d0641857.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8871c27cdf71b815704de01a55da047e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c167c42cc1be2a5bcb26cc2118e66a5a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-716fefef52104778416315e065200599.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2542f9a161a5efa509201afd5b327014.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MVGenMaster-Scaling-Multi-View-Generation-from-Any-Image-via-3D-Priors-Enhanced-Diffusion-Model"><a href="#MVGenMaster-Scaling-Multi-View-Generation-from-Any-Image-via-3D-Priors-Enhanced-Diffusion-Model" class="headerlink" title="MVGenMaster: Scaling Multi-View Generation from Any Image via 3D Priors   Enhanced Diffusion Model"></a>MVGenMaster: Scaling Multi-View Generation from Any Image via 3D Priors   Enhanced Diffusion Model</h2><p><strong>Authors:Chenjie Cao, Chaohui Yu, Shang Liu, Fan Wang, Xiangyang Xue, Yanwei Fu</strong></p>
<p>We introduce MVGenMaster, a multi-view diffusion model enhanced with 3D priors to address versatile Novel View Synthesis (NVS) tasks. MVGenMaster leverages 3D priors that are warped using metric depth and camera poses, significantly enhancing both generalization and 3D consistency in NVS. Our model features a simple yet effective pipeline that can generate up to 100 novel views conditioned on variable reference views and camera poses with a single forward process. Additionally, we have developed a comprehensive large-scale multi-view image dataset called MvD-1M, comprising up to 1.6 million scenes, equipped with well-aligned metric depth to train MVGenMaster. Moreover, we present several training and model modifications to strengthen the model with scaled-up datasets. Extensive evaluations across in- and out-of-domain benchmarks demonstrate the effectiveness of our proposed method and data formulation. Models and codes will be released at <a target="_blank" rel="noopener" href="https://github.com/ewrfcas/MVGenMaster/">https://github.com/ewrfcas/MVGenMaster/</a>. </p>
<blockquote>
<p>我们介绍了MVGenMaster，这是一个多视图扩散模型，通过加入3D先验知识来应对多样化的新型视图合成（NVS）任务。MVGenMaster利用通过度量深度和相机姿态进行变形的3D先验知识，显著提高了NVS中的通用性和3D一致性。我们的模型采用简单有效的管道，只需一次前向过程，就可以根据可变的参考视图和相机姿态生成多达100个新型视图。此外，我们还开发了一个名为MvD-1M的大规模多视图图像数据集，包含高达160万个场景，配备对齐良好的度量深度以训练MVGenMaster。而且，我们对训练和模型进行了几次修改，以通过扩展数据集来加强模型。在内部和外部基准测试的大量评估表明了我们提出的方法和数据制定的有效性。模型和代码将在<a target="_blank" rel="noopener" href="https://github.com/ewrfcas/MVGenMaster/">https://github.com/ewrfcas/MVGenMaster/</a>发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.16157v3">PDF</a> Accepted by CVPR2025. Models and codes will be released at   <a target="_blank" rel="noopener" href="https://github.com/ewrfcas/MVGenMaster/">https://github.com/ewrfcas/MVGenMaster/</a>. The project page is at   <a target="_blank" rel="noopener" href="https://ewrfcas.github.io/MVGenMaster/">https://ewrfcas.github.io/MVGenMaster/</a></p>
<p><strong>Summary</strong></p>
<p>MVGenMaster是一款利用多视角扩散模型结合3D先验技术的通用型新视角合成（NVS）解决方案。它通过利用基于度量深度和相机姿态的3D先验知识，显著提高了NVS的通用性和3D一致性。MVGenMaster拥有简洁高效的流程，能够在单一前向过程中，根据多种参考视角和相机姿态生成多达100个新视角。此外，研究团队还开发了一个名为MvD-1M的大规模多视角图像数据集，包含160万场景，配备对齐的度量深度以训练MVGenMaster模型。该论文也介绍了一些通过扩展数据集加强模型性能的训练和模型修改方法。评估和实验结果表明，该方法及数据构建方式非常有效。模型和代码将公开发布在<a target="_blank" rel="noopener" href="https://github.com/ewrfcas/MVGenMaster/">链接</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MVGenMaster是一款多视角扩散模型，结合了3D先验技术用于新视角合成（NVS）。</li>
<li>利用了基于度量深度和相机姿态的3D先验知识，增强了模型的通用性和3D一致性。</li>
<li>MVGenMaster可以基于多种参考视角和相机姿态生成多个新视角。</li>
<li>研究团队开发了一个大规模的多视角图像数据集MvD-1M，用于训练MVGenMaster模型。</li>
<li>该模型可以通过扩展数据集来加强性能，包括训练和模型的修改方法。</li>
<li>MVGenMaster经过广泛评估和实验验证，证明其有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.16157">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d48c4968a8b2b02ec517efb3e2c495da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c275c0b529da432422bfd9ca559becbc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d315a6d9e096f61d2aff7c2161218d49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1bb85f145dbd061760d16c7da28768a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e885603cbfe1aa9e3a4f37f0d4c2172e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SymmetricDiffusers-Learning-Discrete-Diffusion-on-Finite-Symmetric-Groups"><a href="#SymmetricDiffusers-Learning-Discrete-Diffusion-on-Finite-Symmetric-Groups" class="headerlink" title="SymmetricDiffusers: Learning Discrete Diffusion on Finite Symmetric   Groups"></a>SymmetricDiffusers: Learning Discrete Diffusion on Finite Symmetric   Groups</h2><p><strong>Authors:Yongxing Zhang, Donglin Yang, Renjie Liao</strong></p>
<p>Finite symmetric groups $S_n$ are essential in fields such as combinatorics, physics, and chemistry. However, learning a probability distribution over $S_n$ poses significant challenges due to its intractable size and discrete nature. In this paper, we introduce SymmetricDiffusers, a novel discrete diffusion model that simplifies the task of learning a complicated distribution over $S_n$ by decomposing it into learning simpler transitions of the reverse diffusion using deep neural networks. We identify the riffle shuffle as an effective forward transition and provide empirical guidelines for selecting the diffusion length based on the theory of random walks on finite groups. Additionally, we propose a generalized Plackett-Luce (PL) distribution for the reverse transition, which is provably more expressive than the PL distribution. We further introduce a theoretically grounded “denoising schedule” to improve sampling and learning efficiency. Extensive experiments show that our model achieves state-of-the-art or comparable performances on solving tasks including sorting 4-digit MNIST images, jigsaw puzzles, and traveling salesman problems. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/DSL-Lab/SymmetricDiffusers">https://github.com/DSL-Lab/SymmetricDiffusers</a>. </p>
<blockquote>
<p>有限对称群$S_n$在组合学、物理学和化学等领域中具有重要意义。然而，学习$S_n$上的概率分布面临着巨大的挑战，主要是由于其难以处理的大小和离散性质。在本文中，我们介绍了SymmetricDiffusers，这是一种新型离散扩散模型，它通过分解学习反向扩散的更简单转换来简化学习$S_n$上复杂分布的任务，这些转换使用深度神经网络完成。我们确定了riffle洗牌作为一种有效的正向转换，并根据有限群上的随机游走理论提供了选择扩散长度的经验指南。此外，我们提出了一种广义的Plackett-Luce（PL）分布用于反向转换，该分布被证明比PL分布更具表现力。我们还引入了一个理论基础的“降噪时间表”，以提高采样和学习效率。大量实验表明，我们的模型在解决包括排序4位MNIST图像、拼图和旅行推销员问题在内的任务时达到了最新或相当的性能。我们的代码已发布在<a target="_blank" rel="noopener" href="https://github.com/DSL-Lab/SymmetricDiffusers%E3%80%82">https://github.com/DSL-Lab/SymmetricDiffusers。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02942v2">PDF</a> ICLR 2025 Oral</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种新的离散扩散模型——SymmetricDiffusers，该模型能够简化学习有限对称群$S_n$上的复杂分布的任务。它通过分解复杂的分布学习过程为更简单反向扩散过渡来实现这一目标，利用深度神经网络进行反向过渡学习。研究过程中采用了有效的正向过渡方法——洗牌，并基于随机有限群理论提供了选择扩散长度的经验指南。此外，提出了一种广义的Plackett-Luce分布用于反向过渡，提高了模型的表达能力。同时引入了理论基础的“去噪计划”以提高采样和学习效率。实验证明，该模型在排序、拼图和旅行商问题等多项任务上取得了领先水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SymmetricDiffusers是一种针对有限对称群$S_n$上的复杂分布的离散扩散模型。</li>
<li>它通过分解复杂分布学习过程为更简单反向扩散过渡来实现学习。</li>
<li>深度神经网络用于进行反向过渡学习。</li>
<li>有效的正向过渡方法是洗牌，基于随机有限群理论选择扩散长度。</li>
<li>提出了一种广义的Plackett-Luce分布用于反向过渡，提高了模型的表达能力。</li>
<li>引入了理论基础的“去噪计划”以提高采样和学习效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02942">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fe3fb9c312fa6db22c4fc69dc7715cd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b34213e60f178f27d4cbc5428729494.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Self-supervised-pre-training-with-diffusion-model-for-few-shot-landmark-detection-in-x-ray-images"><a href="#Self-supervised-pre-training-with-diffusion-model-for-few-shot-landmark-detection-in-x-ray-images" class="headerlink" title="Self-supervised pre-training with diffusion model for few-shot landmark   detection in x-ray images"></a>Self-supervised pre-training with diffusion model for few-shot landmark   detection in x-ray images</h2><p><strong>Authors:Roberto Di Via, Francesca Odone, Vito Paolo Pastore</strong></p>
<p>Deep neural networks have been extensively applied in the medical domain for various tasks, including image classification, segmentation, and landmark detection. However, their application is often hindered by data scarcity, both in terms of available annotations and images. This study introduces a novel application of denoising diffusion probabilistic models (DDPMs) to the landmark detection task, specifically addressing the challenge of limited annotated data in x-ray imaging. Our key innovation lies in leveraging DDPMs for self-supervised pre-training in landmark detection, a previously unexplored approach in this domain. This method enables accurate landmark detection with minimal annotated training data (as few as 50 images), surpassing both ImageNet supervised pre-training and traditional self-supervised techniques across three popular x-ray benchmark datasets. To our knowledge, this work represents the first application of diffusion models for self-supervised learning in landmark detection, which may offer a valuable pre-training approach in few-shot regimes, for mitigating data scarcity. </p>
<blockquote>
<p>深度神经网络已广泛应用于医疗领域的各种任务，包括图像分类、分割和关键点检测。然而，其在实际应用中常受到数据稀缺的限制，包括可用的标注和图像。本研究首次将去噪扩散概率模型（DDPMs）应用于关键点检测任务，专门解决X射线成像中标注数据有限带来的挑战。我们的主要创新之处在于利用DDPMs进行关键点检测的自监督预训练，这是该领域之前未被探索的方法。该方法能够在极少的标注训练数据（仅50张图像）下实现准确的关键点检测，超越了ImageNet监督预训练和传统自监督技术在三个流行的X射线基准数据集上的表现。据我们所知，这项工作代表了扩散模型在关键点检测自监督学习中的首次应用，这可能为缓解数据稀缺问题提供有价值的预训练方法，特别是在小样本情况下。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.18125v3">PDF</a> Accepted at WACV 2025</p>
<p><strong>Summary</strong><br>扩散模型在医学领域的应用研究。文章针对X射线成像中的地标检测任务，引入去噪扩散概率模型（DDPMs）进行自监督预训练，解决了标注数据有限的问题。此方法在仅使用少量（如50张）标注图像的情况下仍可实现准确的地标检测，并在三个公共X射线数据集上超越ImageNet监督预训练和传统自监督技术。此为扩散模型在自监督学习中的首次应用于地标检测，为解决数据稀缺问题提供了一种有价值的预训练方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究背景涉及深度神经网络在医学领域的广泛应用及其在面对数据稀缺挑战时的局限性。</li>
<li>提出利用去噪扩散概率模型（DDPMs）解决地标检测任务中标注数据有限的问题。</li>
<li>核心创新点在于使用DDPMs进行自监督预训练，这是一种在医学图像地标检测领域尚未探索的方法。</li>
<li>该方法可在仅使用少量标注图像的情况下实现准确的地标检测。</li>
<li>此方法在三个流行的X射线数据集上的表现超越了ImageNet监督预训练和传统自监督技术。</li>
<li>这是首次将扩散模型应用于自监督学习的地标检测，为解决数据稀缺问题提供了有价值的预训练方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.18125">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c4fe8ec52f131a12305c13150eef9066.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdf8299076cafa7e4723193ef80f93d6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a569e2988b76382c74c267bebcb93089.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e059e518c4b2016e7c6a59a420b98309.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-09/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-de552bc34893907d1e9500a7bc2f586a.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-03-10  RadIR A Scalable Framework for Multi-Grained Medical Image Retrieval   via Radiology Report Mining
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-09/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-47d5e33a75041b1a6a71b828244f5394.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-03-10  Surgical Gaussian Surfels Highly Accurate Real-time Surgical Scene   Rendering
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
